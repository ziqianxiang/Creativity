Under review as a conference paper at ICLR 2019
Directional Analysis of Stochastic Gradient
Descent via von Mises-Fisher Distributions in
Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Although stochastic gradient descent (SGD) is a driving force behind the recent
success of deep learning, our understanding of its dynamics in a high-dimensional
parameter space is limited. In recent years, some researchers have used the
stochasticity of minibatch gradients, or the signal-to-noise ratio, to better char-
acterize the learning dynamics of SGD. Inspired by these work, we here analyze
SGD from a geometrical perspective by inspecting the stochasticity of the norms
and directions of minibatch gradients. We propose a model of the directional con-
centration for minibatch gradients through von Mises-Fisher distribution and show
that the directional uniformity of minibatch gradients increases over the course of
SGD. We empirically verify our result using deep convolutional networks and
observe a higher correlation between the gradient stochasticity and the proposed
directional uniformity than that against the gradient norm stochasticity, suggesting
that the directional statistics of minibatch gradients is a major factor behind SGD.
1	Introduction
Stochastic gradient descent (SGD) has been a driving force behind the recent success of deep learn-
ing. Despite a series of work on improving SGD by incorporating the second-order information of
the objective function (Roux et al., 2008; Martens, 2010; Dauphin et al., 2014; Martens & Grosse,
2015; Desjardins et al., 2015), SGD is still the most widely used optimization algorithm for training
a deep neural network. The learning dynamics of SGD, however, has not been well characterized
beyond that it converges to an extremal point (Bottou, 1998) due to the non-convexity and high-
dimensionality of a usual objective function used in deep learning.
Gradient stochasticity, or the signal-to-noise ratio (SNR) of the stochastic gradient, has been pro-
posed as a tool for analyzing the learning dynamics of SGD. Shwartz-Ziv & Tishby (2017) identified
two phases in SGD based on this. In the first phase, “drift phase”, the gradient mean is much higher
than its standard deviation, during which optimization progresses rapidly. This drift phase is fol-
lowed by the “diffusion phase”, where SGD behaves similarly to Gaussian noise with very small
means. Similar observations were made by Li & Yuan (2017) and Chee & Toulis (2018) who have
also divided the learning dynamics of SGD into two phases.
Shwartz-Ziv & Tishby (2017) have proposed that such phase transition is related to information
compression. Unlike them, we notice that there are two aspects to the gradient stochasticity. One
is the L2 norm of the minibatch gradient (the norm stochasticity), and the other is the directional
balance of minibatch gradients (the directional stochasticity). SGD converges or terminates when
either the norm of the minibatch gradient vanishes to zeros, or when the angles of the minibatch
gradients are uniformly distributed and their non-zero norms are close to each other. That is, the
gradient stochasticity, or the SNR of the stochastic gradient, is driven by both of these aspects, and
it is necessary for us to investigate not only the holistic SNR but also the SNR of the minibatch
gradient norm and that of the minibatch gradient angles.
In this paper, we use a von Mises-Fisher (vMF hereafter) distribution, which is often used in di-
rectional statistics (Mardia & Jupp, 2009), and its concentration parameter κ to characterize the
directional balance of minibatch gradients and understand the learning dynamics of SGD from the
perspective of directional statistics of minibatch gradients. We prove that SGD increases the direc-
1
Under review as a conference paper at ICLR 2019
tional balance of minibatch gradients. We empirically verify this with deep convolutional networks
with various techniques, including batch normalization (Ioffe & Szegedy, 2015) and residual con-
nections (He et al., 2015), on MNIST and CIFAR-10 (Krizhevsky & Hinton, 2009). Our empirical
investigation further reveals that the proposed directional stochasticity is a major drive behind the
gradient stochasticity compared to the norm stochasticity, suggesting the importance of understand-
ing the directional statistics of the stochastic gradient.
Contribution We analyze directional stochasticity of the minibatch gradients via angles as well
as the concentration parameter of the vMF distribution. Especially, we theoretically show that the
directional uniformity of the minibatch gradients modeled by the vMF distribution increases as train-
ing progresses, and verify this by experiments. In doing so, we introduce gradient norm stochasticity
as the ratio of the standard deviation of the minibatch gradients to their expectation and theoretically
and empirically show that this gradient norm stochasticity decreases as the batch size increases.
Related work Most studies about SGD dynamics have been based on two-phase behavior
(Shwartz-Ziv & Tishby, 2017; Li & Yuan, 2017; Chee & Toulis, 2018). Li & Yuan (2017) investi-
gated this behavior by considering a shallow neural network with residual connections and assuming
the standard normal input distribution. They showed that SGD-based learning under these setups has
two phases; search and convergence phases. Shwartz-Ziv & Tishby (2017) on the other hand inves-
tigated a deep neural network with tanh activation functions, and showed that SGD-based learning
has drift and diffusion phases. They have also proposed that such SNR transition (drift + diffusion)
is related to the information transition divided into empirical error minimization and representation
compression phases. However, Saxe et al. (2018) have reported that the information transition is not
generally associated with the SNR transition with ReLU (Nair & Hinton, 2010) activation functions.
Chee & Toulis (2018) instead looked at the inner product between successive minibatch gradients
and presented transient and stationary phases.
Unlike our work here, the experimental verification of the previous work conducted under limited
settings - the shallow network (Li & Yuan, 2017), the specific activation function (ShWartz-Ziv &
Tishby, 2017), and only MNIST dataset (Shwartz-Ziv & Tishby, 2017; Chee & Toulis, 2018) - that
conform well with their theoretical assumptions. Moreover, their work does not offer empirical
result about the effect of the latest techniques including both batch normalization (Ioffe & Szegedy,
2015) layers and residual connections (He et al., 2015).
2	Preliminaries
Norms and Angles Unless explicitly stated, a norm refers to L2 norm. ∣∣ ∙ ∣∣ andh∙,)thus
correspond to L2 norm and the Euclidean inner product on Rd, respectively. We use xn ⇒ x
to indicate that “a random variable xn converges to x in distribution.” Similarly, xn →P x means
convergence in probability. An angle θ between d-dimensional vectors u and v is defined by
θ = 180 cos-1 (√u⅛).
π	kukkvk .
Loss functions A loss function of a neural network is written as f (W) = n Pn=ι fi(w), where
w ∈ Rd is a trainable parameter. fi is “a per-example loss function” computed on the i-th data
point. We use I and m to denote a minibatch index set and its batch size, respectively. Further, we
call fɪ(w) = ml Pi∈ fi(w) “a minibatch loss function given I”. In Section 3.1, we use gi(W) and
g(w) to denote -Vwfi(w) and -Vw力(w), respectively. In Section 3.3, the index i is used for
the corresponding minibatch index set Ii. For example, the negative gradient of fIi (W) is written as
gi(w). During optimization, we denote a parameter W at the i-th iteration in the t-th epoch as Wi
and W00 is an initial parameter. We use nb to refer to the number of minibatches in a single epoch.
von Mises-Fisher Distribution We use the von Mises-Fisher (vMF) distribution to model the
directions of vectors. The definition of the vMF distribution is as follows:
Definition 1. (Von Mises-Fisher Distribution, Banerjee et al. (2005)) The pdf of the vMF(μ, K) is
given by
fd(x; μ, K) = Cd(K) exp(κμ>x)
2
Under review as a conference paper at ICLR 2019
X
Figure 1: Characteristics of the vMF distribution in a 2-dimensional space. 100 random samples are
drawn from vMF(μ, K) where μ = (1,0)> and K = {0,5, 50}.
on the hypersphere Sd-1 U Rd. Here, the concentration parameter K determines how the samples
from this distribution are concentrated on the mean direction μ and Cd(K) is constant determined
by d and K.
If K is zero, then it is a uniform distribution on the unit hypersphere, and as K → ∞, it becomes a
point mass on the unit hypersphere (Figure 1). The maximum likelihood estimates for μ and K are
μ = k Pi=I X k and K ≈ r(ld-f2)where x∕s are random samples from the VMF distribution and r =
Pn Xik
U 乙黄11. The formula for K is approximate since the exact computation is intractable (Banerjee
et al., 2005).
3 Theoretical Motivation
3.1	Analysis of the Gradient Norm Stochasticity
It is a usual practice for SGD to use a minibatch gradient g(w) = -Vw∕z(w) instead of a full batch
gradient g(w) = -Vw f (w). The minibatch index set I is drawn from {1,...,n} randomly. g(w)
satisfies E[g(w)] = g(w) and Cov(g(w), g(w)) ≈ 房 PnL1 gi(w)gi(w)τ for n》m where n
is the number of full data points and gi(w) = -Vw fi(w) (HOfferetaL,2o17). As the batch size m
increases, the randomness of g(w) decreases. Hence Ekg(W)∣∣ tends to ∣∣g(w)∣∣,and Var(∣∣g(w)k),
which is the variance of the norm of the minibatch gradient, vanishes. The convergence rate analysis
is as the following:
Theorem 1. Let g(w) be a minibatch gradient Inducedfrom the minibatch index set I of batch size
m from {1,...,n} and suppose Y = maxi,j∈{i,...,n1 | hgi(w), gj(w))|. Then
2(n — m)	Y	(n — m)γ
≤ kg(w)k-kg(w)k ≤ m(n - 1) X Ekg(W)k + kg(w)k ≤ m(n - 1)kg(w)k
and
Var(kg(w)k) ≤ m-m γ.
Hence,
,Var(kg(w)k)
Ekg(W)k
≤ /2(n - m) X Y
一V m(n - 1)	∣∣g(w)k2 .
(1)
Proof. See Supplemental A.
□
According to Theorem 1, a large batch size m reduces the variance Ofkg(W) ∣ centered at Ekg(W) ∣
with convergence rate O(1∕m). We empirically verify this by estimating the gradient norm stochas-
ticity at random points while varying the minibatch size, using a fully-connected neural network
(FNN) with MNIST, as shown in Figure 2(a) (see Supplemental E for more details.)
3
Under review as a conference paper at ICLR 2019
(a) ,Var(M(w)∣∣)/EM(W)Il
Figure 2: The directions of minibatch gradients become more important than their lengths as the
batch size increases. (a) The gradient norm Stochasticity of g(w) with respect to various batch sizes
at 5 random points w with mean (black line) and mean±std.(shaded area) in a log-linear scale; (b) If
the gradient norm stochasticity is sufficiently low, then the directions of gi (w),s need to be balanced
to satisfy P3=1 gi(w) ≈ 0.
(b) Directional balance in SGD iterations
This theorem however only demonstrate that the gradient norm stochasticity is (l.h.s. of (1)) is low
at random initial points. It may blow up after SGD updates, since the upper bound (r.h.s. of (1)) is
inversely proportional to kg(w)k. This implies that the learning dynamics and convergence of SGD,
measured in terms of the vanishing gradient, i.e., Pn= 1 gi(w) ≈ 0, is not necessarily explained by
the vanishing norms of minibatch gradients, but rather by the balance of the directions of g%(w)'s,
which motivates our investigation of the directional statistics of minibatch gradients. See Figure 2(b)
as an illustration.
3.2	Uniformity measurement via analysis of angles
In order to investigate the directions of minibatch gradients and how they balance, we start from an
angle between two vectors. First, we analyze an asymptotic behavior of angles between uniformly
random unit vectors in a high-dimensional space.
Theorem 2.	Suppose that u and v are mutually independent d-dimensional uniformly random unit
vectors. Then,
呼 COs-I
π
hu, vi - 90
⇒N
as d → ∞.
Proof. See Supplemental B.
□
According to Theorem 2, the angle between two independent uniformly random unit vectors is nor-
mally distributed and becomes increasingly more concentrated as d grows (Figure 3(a)). If SGD
iterations indeed drive the directions of minibatch gradients to be uniform, then, at least, the dis-
tribution of angles between minibatch gradients and a given uniformly sampled unit vector follows
asymptotically
N
(2)
Figures 3(b) and 3(c) show that the distribution of the angles between minibatch gradients and a
given uniformly sampled unit vector converges to an asymptotic distribution (2) after SGD itera-
tions. Although we could measure the uniformity of minibatch gradients how the angle distribution
between minibatch gradients is close to (2), it is not as trivial to compare the distributions as to
compare numerical values. This necessitates another way to measure the uniformity of minibatch
gradients.
4
Under review as a conference paper at ICLR 2019
(a) Asymptotic densities
(b) Densities at initial epochs
(c) Densities after training
Dimension	635,200
——100,000	1,000,000
300,000	2,000,000
Asymptotic ... Seed	3
---Seed 1	--- Seed 4
…Seed 2	—— Seed 5
Figure 3: (a) Asymptotic angle densities (2) of θ(u, V) = 1∏80 cos-1(u, Vi where U and V are
independent uniformly random unit vectors for each large dimension d. As d → ∞, θ(u, v) tends
to less scattered from 90 (in degree). (b-c) We apply SGD on FNN for MNIST classification with
the batch size 64 and the fixed learning rate 0.01 starting from five randomly initialized parameters.
We draw a density plot θ(u, „/)§ ) for 3,000 minibatch gradients (black) at W = w0 (b) and
w = wfi0nal, with training accuracy of > 99.9%, (c) when u is given. After SGD iterations, the
density of θ(u, gj (W)) converges to an asymptotic density (red). The dimension of FNN is 635,200.
3.3 Uniformity measurement via vMF distribution
To model the uniformity of minibatch gradients, we propose to use the vMF distribution in Defini-
tion 1. The concentration parameter κ measures how uniformly the directions of unit vectors are
distributed. By Theorem 1, with a large batch size, the norm of minibatch gradient is nearly deter-
ministic, and μ is almost parallel to the direction of full batch gradient. In other words, K measures
the concentration of the minibatch gradients directions around the full batch gradient.
The following Lemma 1 introduces the relationship between the norm of averaged unit vectors and
K, the approximate estimator of κ.
Lemma 1. The approximated estimator of κ induced from the d-dimensional unit vectors
{xi, X2, ∙∙∙ , Xnb},
r r(d —尸2)
"1 -尸 2 ,
is a strictly increasing function on [0,1], where r = k Ei=I Xik. Ifwe consider K = h(u) as a
function of U = ∣∣ En= ι Xik, then h(∙) is Lipschitz continuous on [0,nb(1 — e)] for any e > 0.
Moreover, h(∙) and h0(∙) are strictly increasing and increasing on [0, nb), respectively.
Proof. See Supplemental C.1.
□
Consider
S h (IX Hi II),
which is measured from the directions from the current location W to the fixed points pi ’s, where
h(∙) is a function defined in Lemma 1. Since h(∙) is an increasing function, We may focus only
on k Pn= ι kpi-Wk k to see how K behaves with respect to its argument. Lemma 2 implies that
the estimated directional concentration K decreases if we move away from w0 to w0 = w0 +
e Pi kPi-W0k with a small e (Figure 4(a)). In other words, ^(w0) < ^(w0).
Lemma 2. Let pi, p2, ∙∙∙ , Pnb be d -dimensional vectors. Ifall p/s are not on a single ray from
the current location W, then there exists positive number η such that
nb
X
Pj - W - ePn=i ⅞≡1
j =1 Ipj — W — e
Pnb	Pi—w
乙i=i kPi—wk
nb
X Pi — W
=kPi - wk
<
5
Under review as a conference paper at ICLR 2019
(a) From w0 to w0 by e Ei Xi
(b) From w0 to w0 by SGD iterations
Figure 4: (a) If the	point	is slightly moved	from w0 to W0 by	e Ei Xi where Xi	=	(Pi -
w00)/kpi - w00k and	x0i =	(pi - w0)/kpi -	w0k, then k Pix0ik	< k Pi xik which	is	equiva-
lent to κ(w0) < ^(w0); (b) If gi(w0),s are sufficiently parallel to (Pi — w0),s for each i, then
w0 ≈ w0 + η Ei gi (w0). When w0 and w0 are sufficiently close to each other, We also have
^(w0) < κ(w0).
for all ∈ (0, η].
Proof. See Supplemental C.2.
□
We make the connection between the observation above and SGD by first viewing Pi ’s as local
minibatch solutions.
Definition 2. For a minibatch index set Ii, pi (W) = arg minwo ∈N(w;r,)f1i (w0) is a local minibatch
solution of Ii at w, where N(w; ri) is a neighborhood of radius ri at w. Here, ri is determined by
W and Ii forPi(W) to exist uniquely.
Under this definition, Pi (W) is local minimum of a minibatch loss function fIi near W. Then we
reasonably expect that the direction of gi (W) = -Vw fIi (W) is similar to that of pi (W) — w.
Each epoch of SGD with a learning rate η computes a series of Wt = W0 + η Pj=I gi(Wi-1)
for all j ∈ {1,..., nb}. If gi(∙)'s are Lipschitz continuous for all i ∈ {1,..., nb}, then we have
∣gi(WiT)k ≈ ∣gi(w0)k for a small η. Moreover, Theorem 1 implies ∣∣t^i(w0)k ≈ T for all
i ∈ {1, . . . , nb} with a large batch size or at the early stage of SGD iterations. Combining these
approximations, ∣gi(Wqt-、)∣ ≈ T for all i ∈ {1,..., nb}.
For example, suppose that t = 0, nb = 3 and τ = 1, and assume that Pi(W00) = Pi(W10) = Pi for
alli = 1,2,3. Then,
and
^(w0 ) = h
Pi- w0 II
kPi- w0 k Il
0	II * 3 Pj - W00 - η
KM) ≈ h∣lχ=χ jWT；
P3 * *	gi (WOT )	I l ∖
乙i二1 Ilgi(WOT 川	ii
P3	gi(w0-1) Illl
乙i=1 kgi(WOT)k l 1
IfηPi∣p≡⅛ ≈ηPif≡0⅛,then
X Pt-W0 -η P3=1 ⅛⅞⅛ ≈ X Pj-WO -η P3=1 ”
士 IPj- W0 - ηPL ⅛g⅛II	Mj- W0 - ηP3=1 ⅛≡⅛I.
Hence, we have K(w0) < ^(w0) by Lemma 2. A trivial case satisfying this condition would be for
each pair of Pi — W0 and gi(WOT) to be approximately parallel, as illustrated in Figure 4(b).
6
Under review as a conference paper at ICLR 2019
Theorem 3.	Let pι(w0),p2(w0), .…,Pnb(w0) be d-dimensional vectors, and all pi(w0),s are
not on a single ray from the current location wt0 . If
X Pi(WO) - w0 - XX gi(WiT)
i=1 kg(WO)-w0k ⅛ ι∣gi(wi-1)k
≤ξ
(3)
for a sufficiently small ξ > 0, then there exists positive number η such that
X Pj (W?)- W? TP1 号W-⅛
,S"(WO)- wo -Pι k^g⅞
X Pi(WO) — w0
i=1 kPi(WO) — WOk
(4)
<
for all ∈ (0, η].
Proof. See Supplemental C.3.
□
This Theorem 3 asserts that κ(∙) decreases even with some perturbation along the averaged direction
Pi ∣∣Pi(W)-W∣∣. With additional assumptions on each minibatch loss functions, We have a sufficient
condition for (3), summarized in Corollary 3.1.
Corollary 3.1. Let Pi be the local minibatch solution of each fIi. Suppose a region R satisfying:
For all W, W0 ∈ R, Pi(W) = Pi(W0) = Pi
for all i = 1,…,n. Further, assume that Hessian matrices of f□Js are positive definite, Well-
conditioned, and bounded in the sense of matrix L2-norm on R. If SGD moves from WtO to WtO+1
on R with a large batch size and a small learning rate, then K(Wt) > ^(wo+i). Moreover, we can
estimate ^(w°1) and k(w") by minibatch gradients at w01 and Wt+ι, respectively.
Proof. See Supplemental D.	口
Without the corollary above, we need to solve Pi(W01) = argminw∈N(wo；『)fi(w) for all i ∈
{1,..., ns}, where n is the number of samples to estimate κ, in order to compute K(W0). Corol-
lary 3.1 however implies that we can compute ^(wJ?) by using 燃(：0卜 instead of 厚(：0)二0八,
significantly reducing computational overhead.
In Practice Although the number of all possible minibatches in each epoch is nb = mn , it is
often the case to use n0b ≈ n/m minibatches at each epoch in practice to go from WtO to WtO+1 .
Assuming that these n0b minibatches were selected uniformly at random, the average of the n0b nor-
malized minibatch gradients is the maximum likelihood estimate of μ,just like the average of all n
normalized minibatch gradients. Thus, we expect with a large n0b,
X Pi(Wt) — Wt _ XnX gi(WiT) ≤ ξ
⅛ kPi(w?)- Wtk = kgi(wi-1)k -ξ,
and that SGD in practice also satisfies K(Wt) > R(w0+i).
4 Experiments
4.1	Setup
In order to empirically verify our theory on directional statistics of minibatch gradients, we train
various types of deep neural networks using SGD and monitor the following metrics for analyzing
the learning dynamics of SGD:
•	Training loss
7
Under review as a conference paper at ICLR 2019
Figure 5: We show the average K (black curve) ± std. (shaded area), as the function of the number
of training epochs (in log-log scale) across various batch sizes in MNIST classifications using FNN
with fixed learning rate 0.01 and 5 random initializations. Although K with the large batch size
decreases more smoothly rather than the small batch size, We observe that K still decreases well with
minibatches of size 64. We did not match the ranges of the y-axes across the plots to emphasize the
trend of monotonic decrease.
•	Validation loss
•	Gradient stochasticity (GS)↑ kEVw尢(w)k/Ptr(Cov(Vw尢(w), Rwfii(w)) 1
•	Gradient norm stochasticity (GNS) ↑ EkVw加(w)k/PVar(IlVwfii(w)k) 1
•	Directional Uniformity↑ κ 1
The latter three quantities are statistically estimated using n = 3,000 minibatches. We use K to
denote the κ estimate.
We train the following types of deep neural networks (Supplemental E):
•	FNN: a fully connected network with a single hidden layer
•	DFNN: a fully connected network with three hidden layers
•	CNN: a convolutional network with 14 layers (Krizhevsky et al., 2012)
In the case of the CNN, we also evaluate its variant with skip connections (+Res) (He et al., 2015).
As it was shown recently by Santurkar et al. (2018) that batch normalization (Ioffe & Szegedy,
2015) improves the smoothness of a loss function in terms of its Hessian, we also test adding batch
normalization to each layer right before the ReLU (Nair & Hinton, 2010) nonlinearity (+BN). We
use MNIST for the FNN, DFNN and their variants, while CIFAR-10 (Krizhevsky & Hinton, 2009)
for the CNN and its variants.
Our theory suggests a sufficiently large batch size for verification. We empirically analyze how
large a batch size is needed in Figure 5. From these plots, K decreases monotonically regardless
of the minibatch size, but the variance over multiple training runs is much smaller with a larger
minibatch size. We thus decide to use a practical size of 64. With this fixed minibatch size, we use
a fixed learning rate of 0.01, which allows us to achieve the training accuracy of > 99.9% for every
training run in our experiments. We repeat each setup five times starting from different random
initial parameters and report both the mean and standard deviation.
4.2	Directional Uniformity Increases
FNN and DFNN We first observe that K decreases over training regardless of the network's depth
in Figure 6 (a,b). We however also notice that K decrease monotonically with the FNN, but less so
with its deeper variant (DFNN). We conjecture this is due to the less-smooth loss landscape ofa deep
neural network. This difference between FNN and DFNN however almost entirely vanishes when
batch normalization (+BN) is applied (Figure 6 (e,f)). This was expected as batch normalization is
known to make the loss function behave better, and our theory assumes a smooth objective function.
CNN The CNN is substantially deeper than either FNN or DFNN and is trained on a substan-
tially more difficult problem of CIFAR-10. In other words, the assumptions underlying our theory
may not hold as well. Nevertheless, as shown in Figure 6 (c), K eventually drops below its initial
point, although this trend is not monotonic and K fluctuates significantly over training. The addi-
tion of batch normalization (+BN) helps with the fluctuation but K does not monotonically decrease
8
Under review as a conference paper at ICLR 2019
5	20 50	150
Epoch
65 55
000 00
× ××
1	5	20 50 150
Epoch
554 4
10 10 10 10
1	5	20	80 200
Epoch
554 443
101010 1010
1	5	20	80 200
Epoch
(a) FNN
MNIST
98.18 ± 0.04%
(e) FNN+BN
MNIST
98.37 ± 0.08%
(b) DFNN
MNIST
98.30 ± 0.05%
(f) DFNN+BN
MNIST
98.51 ± 0.11%
(c) CNN
CIFAR-10
53.78 ± 3.22%
(d) CNN+Res
CIFAR-10
63.39 ± 1.68%
(g) CNN+BN
CIFAR-10
73.00 ± 1.16%
(h) CNN+Res+BN
CIFAR-10
72.71 ± 1.02%

Figure 6: We show the average K (black curve) ± std. (shaded area), as the function of the number
of training epochs(in log-log scale) for all considered setups. We report the name of architecture,
dataset and maximum valid accuracy(mean±std.) of all epochs. Although K decreases eventually
over time in all the cases, batch normalization (+BN) significantly reduces the variance of the di-
rectional stochasticity ((a-d) vs. (e-h)). We also observe that the skip connections make K decrease
monotonically ((c,g) vs. (d,h)). Note the differences in the y-scales.
(Figure 6 (g)). On the other hand, We observe the monotonic decrease of K when skip connections
(+Res) are introduced (Figure 6 (c) vs. Figure 6 (d)) albeit still with some level of fluctuation espe-
cially in the early stage of learning. When both batch normalization and skip connections are used
(+Res+BN), the behaviour of K matches with our prediction without much fluctuation.
Effect of +BN and +Res Based on our observations that the uniformity of minibatch gradients
increases monotonically, when a deep neural network is equipped with residual connection (+Res)
and trained with batch normalization (+BN), we conjecture that the loss function induced from
these two techniques better satisfies the assumptions underlying our theoretical analysis, such as
its well-behavedness. This conjecture is supported by for instance Santurkar et al. (2018), who
demonstrated batch normalization guarantees the boundedness of Hessian, and Orhan & Pitkow
(2017), who showed residual connections eliminate some singularities of Hessian.
K near the end of training The minimum average K of DFNN+BN, which has 1,920,000 pa-
rameters, is 71, 009.20, that of FNN+BN, which has 636, 800 parameters, is 23, 059.16, and that
of CNN+BN+Res, which has 207,152 parameters, is 20,320.43. These average K are within a
constant multiple of estimated K using 3,000 samples from the vMF distribution with true K = 0
(35, 075.99 with 1, 920, 000 dimensions, 11, 621.63 with 636, 800 dimensions, and 3, 781.04 with
207, 152 dimensions.) This implies that we cannot say that the underlying directional distribution
of minibatch gradients in all these cases at the end of training is not close to uniform (Cutting et al.,
2017). For more detailed analysis, see Supplementary F.
4.3	Directional Uniformity and Other Metrics
The gradient stochasticity (GS) was used by Shwartz-Ziv & Tishby (2017) as a main metric for
identifying two phases of SGD learning in deep neural networks. This quantity includes both the
gradient norm stochasticity (GNS) and the directional uniformity K, implying that either or both
of GNS and K could drive the gradient stochasticity. We thus investigate the relationship among
these three quantities as well as training and validation losses. We focus on CNN, CNN+BN and
CNN+Res+BN trained on CIFAR-10.
9
Under review as a conference paper at ICLR 2019
(a) CNN
(b) CNN+BN
(c) CNN+Res+BN
o SNR-κ
SNR-normSNR
K	——normSNR ——SNR
Train loss Valid loss
Figure 7: (First row) We plot the evolution of the training loss (Train loss), validation loss (Valid
loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR)
and directional uniformity K. We normalized each quantity by its maximum value over training for
easier comparison on a single plot. In all the cases, SNR (orange) and K (red) are almost entirely
correlated with each other, while normSNR is less correlated. (Second row) We further verify this
by illustrating SNR-^ scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales.
These plots suggest that the SNR is largely driven by the directional uniformity.
From Figure 7 (First row), it is clear that the proposed metric of directional uniformity K correlates
better with the gradient stochasticity than the gradient norm stochasticity does. This was especially
prominent during the early stage of learning, suggesting that the directional statistics of minibatch
gradients is a major explanatory factor behind the learning dynamics of SGD. This difference in
correlations is much more apparent from the scatter plots in Figure 7 (Second row). We show these
plots created from other four training runs per setup in Supplemental G.
5 Conclusion
Stochasticity of gradients is a key to understanding the learning dynamics of SGD (Shwartz-Ziv &
Tishby, 2017) and has been pointed out as a factor behind the success of SGD (see, e.g., LeCun et al.,
2012; Keskar et al., 2016). In this paper, we provide a theoretical framework using von Mises-Fisher
distribution, under which the directional stochasticity of minibatch gradients can be estimated and
analyzed, and show that the directional uniformity increases over the course of SGD. Through the
extensive empirical evaluation, we have observed that the directional uniformity indeed improves
over the course of training a deep neural network, and that its trend is monotonic when batch nor-
malization and skip connections were used. Furthermore, we demonstrated that the stochasticity of
minibatch gradients is largely determined by the directional stochasticity rather than the gradient
norm stochasticity.
our work in this paper suggests two major research directions for the future. First, our analysis
has focused on the aspect of optimization, and it is an open question how the directional uniformity
relates to the generalization error although handling the stochasticity of gradients has improved SGD
(Neelakantan et al., 2015; Hoffer et al., 2017; Smith et al., 2017; Jin et al., 2017). Second, we have
focused on passive analysis of SGD using the directional statistics of minibatch gradients, but it
is not unreasonable to suspect that SGD could be improved by explicitly taking into account the
directional statistics of minibatch gradients during optimization.
10
Under review as a conference paper at ICLR 2019
References
Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit hy-
persphere using von mises-fisher distributions. Journal of Machine Learning Research, 6(Sep):
1345-1382, 2005.
Leon Bottou. Online algorithms and stochastic approximations. In David Saad (ed.), Online
Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998. URL
http://leon.bottou.org/papers/bottou-98x. revised, oct 2012.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
George Casella and Roger L Berger. Statistical inference, volume 2. Duxbury Pacific Grove, CA,
2002.
Jerry Chee and Panos Toulis. Convergence diagnostics for stochastic gradient descent with constant
learning rate. In International Conference on Artificial Intelligence and Statistics, pp. 1476-1485,
2018.
Christine Cutting, Davy Paindaveine, and Thomas Verdebout. Tests of concentration for low-
dimensional and high-dimensional directional data. In Big and Complex Data Analysis, pp. 209-
227. Springer, 2017.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et al. Natural neural networks. In Ad-
vances in Neural Information Processing Systems, pp. 2071-2079, 2015.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1729-1739, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape
saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc.,
2012.
11
Under review as a conference paper at ICLR 2019
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks ofthe trade, pp. 9-48. Springer, 2012.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
Kanti V Mardia and Peter E Jupp. Directional statistics, volume 494. John Wiley & Sons, 2009.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and
James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint
arXiv:1511.06807, 2015.
A Emin Orhan and Xaq Pitkow. Skip connections eliminate singularities. arXiv preprint
arXiv:1701.09175, 2017.
Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gra-
dient algorithm. In Advances in neural information processing systems, pp. 849-856, 2008.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch
normalization help optimization?(no, it is not about internal covariate shift). arXiv preprint
arXiv:1805.11604, 2018.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase the
batch size. CoRR, abs/1711.00489, 2017. URL http://arxiv.org/abs/1711.00489.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
12
Under review as a conference paper at ICLR 2019
Supplementary Material
A Proofs for Theorem 1
In proving Theorem 1, we use Lemma A.1. Define selector random variables(Hoffer et al., 2017)
as below:
1, ifi ∈ I
si =	0, if i ∈/ I .
Then we have
1n
g(w) = — fgi(w)Si.
m
i=1
Lemma A.1. Let g(w) be a minibatch gradient induced from the minibatch index set I with batch
size m from {1, . . . , n}. Then
0 ≤ Ekg(W)k2 -kg(W)k2 ≤ 2n■—m)γ.	(5)
m(n - 1)
where γ = maxi,j∈{1,...,n} | hgi	(W), gj (W)i |.
Proof. By Jensen,s inequality, 0 ≤ Ekg(W)k2 — kg(W)k2. Note that
nn 1
Ekg(W)k2 = XX 滔 hgi(w), gj (w)〉E[SiSj].
i=1 j=1 m
Since 旧[“/=答 δj + m(m-⅛ (1 — δj),
Ekg(W)k2- kg(W)k2
—
n
mnn-7 )χ a(W), gi(W)i
m—1
mn(n — 1)
—
m—1
mn(n — 1)
n
hgi (W), gj(W)i
i=1 j=1
n
X hgi (W), gi (W)i
=1
n
+ mnm(n — 1) XX hgi(W)，gj(W)i
i=1 j =1
≤
—
m-1
n- m
mn(n - 1)
2(n - m)
nγ + mn2(n — 1)
n2γ
m(n — 1) T
where γ	= maxi,j∈{1,...,n} | hgi (W), gj(W)i |.
□
Theorem 1.	Let @(w) be a minibatch gradient inducedfrom the minibatch index set I of batch size
m from {1, . . . ,n} and suppose γ = maxi,j∈{1,...,n} | hgi	(W), gj (W)i |. Then

0 ≤ Ekg(W)k-kg(w)k≤
2(n — m)	γ	(n — m)γ
m(n — 1) * Ekg(W)k + kg(W)k ' m(n — 1)kg(W)k
and
Var(kg(W)k) ≤ mn--m γ.
Hence,
PVar(kg(W)k) ≤ [
Ekg(W)k	≤ V
2(n — m)
m(n — 1) X Ilg(W)k2 .

13
Under review as a conference paper at ICLR 2019
Proof. ByJensen's inequality, we have Ilg(W)Il = ∣∣E[g(w)]k ≤ Ekg(w)k and (Ekg(W)k)2 ≤
Ekg(W) k2. From the second inequality and Lemma A.1,
(Ekg(W)k)2 ≤ Ekg(W)k2 ≤ ∣g(w)k2+2(n- m) Y
m(n - 1)
or
(Ekg(W)k - kg(W)k)(Ekg(W)k+ng(W)k)≤ mnn-m) γ.
Hence
2(n - m)	γ	(n - m)γ
g W g W W m m(n - 1) × Ekg(W)k + ∣∣g(W)k	m(n - 1)kg(W)|「
Further,
Var(kg(W)k)= Ekg(W)k2-(Ekg(W)k)2
≤ Ekg(W)k2-kEg(W)k2
≤ kg(W)k2+mnn-m) γ - kg(W)k2
2(n - m)
m(n — 1) Y
□
B Proofs for Theorem 2
For proofs, Slutsky’s theorem and delta method are key results to describe limiting behaviors of
random variables in distributional sense.
Theorem B.1. (Slutsky’s theorem, Casella & Berger (2002)) Let {xn}, {yn } be a sequence of
random variables that satisfies xn ⇒ x and yn →P ρ when n goes to infinity and ρ is constant. Then
xnyn ⇒ cx
Theorem B.2. (Delta method, CaseUa & Berger (2002)) Let \冗 be a Sequence ofrandom variables
that satisfies √n(yn — μ) ⇒ N(0, σ2). For a given smooth function f : R → R, suppose that f z(μ)
exists and is not 0 where f0 is a derivative. Then
√n × (f(yn) - f(μ)) ⇒ N(0,σ2(f0(μ))2).
Lemma B.1. Suppose that U and V are mutually independent d-dimensional uniformly random unit
vectors. Then, √dhu, Vi ⇒ N(0,1) as d → ∞.
Proof. Note that d-dimensional uniformly random unit vectors U can be generated by normalization
of d-dimensional multivariate standard normal random vectors X 〜 N(0, Id). That is,
x
U〜肉.
Suppose that two independent uniformly random unit vector U and V are generated by two indepen-
dent d-dimensional standard normal vector X = (xι, x2,…，Xd) and y = (y「、？,…，yd). Denote
them
xy
U=而 and v=而
By SLLN, we have
kx「八
→ → 1 a.s.
Vd
(Use d Pd=ι X → Ex2 = 1). Since almost sure convergence implies convergence in probability,
∣∣x∣∣∕√d→ 1. SimiIarly, kyk∕√d→ 1. Moreover, by CLT,
hx√yi = √d(d X Xiyi) ⇒N(0,1)∙
i=1
Therefore, by Theorem B.1 (Slutsky’s theorem),
√d hu, Vi ⇒ N(0, 1).
□
14
Under review as a conference paper at ICLR 2019
cos-1 hu, vi
Theorem 2.	Suppose that u and v are mutually independent d-dimensional uniformly random unit
vectors. Then,
90 ⇒
as d → ∞.
Proof. Suppose that μ = 0, σ = 1, and f (∙) = 1∏80 cos-1(∙). Since 1∏80* cos-1(x) = -∏√U~,
We have f0(μ) = -1∏∏0. Hence, by Lemma B.1 and Theorem B.2 (Delta method), the desired
convergence in distribution holds.	□
C Proofs for Theorem 3
C.1 Proof of Lemma 1
Lemma 1. The approximated estimator of κ induced from the d-dimensional unit vectors
{x1, x2, ∙∙∙ , Xnb},
r r(d — f2)
一τ-尸,
where r = k Pi=1 Xik is a StriCt increasing function on [0,1]. Ifwe Consider K = h(u) as function
of U = k En= ι Xik. Then h(∙) is Lipschitz continuous on [0, nb(1 — e)] for any E > 0. Moreover,
h(∙) and h0(∙) are strict increasing and increasing on [0, nb), respectively.
Proof. Note that k Pn= 1 Xik ≤ Pn= 1 ∣∣Xi∣∣ = n> Therefore, we have r ∈ [0,1]. If d = 1, then
K = r and this increases on [0,1]. For d > 1,
dκ	d + r4 + (d — 3 万2)
-------------=------------：-7Γ~τ:-
dr-------------------------(1 — f2)2
and its numerator is always positive for d > 2. When d = 2,
dκ	r4 — 3r2 + 4	(r2 — 3 )2 + 7
df =	(1 — f2)2 =	(1 — r2)2	> .
So K increases as r increases.
The Lipschitz continuity of h(∙) directly comes from the continuity of 警 since
dκ 1 dκ
—=---------.
du	nb drf
Recall that any continuous function on the compact interval [0, nb(1 — E)] is bounded. Hence the
derivative of K with respect to U is bounded. This implies the Lipschitz continuity of h(∙).
h(∙) is strictly increasing since f =黑.Further,
h00(u)
1 d2^
nb2 drf2
2f5 + (4 — 8d)f3 + (8d — 6)f
nb2(1 — f2)4
>0
due to r ∈ [0,1]. Therefore h0(∙) is also increasing on [0, nb).
□
C.2 Proof of Lemma 2
Lemma 2. Let pi, p2,…，Pnb be d -dimensional vectors. IfaU pi.，s are not on a single ray from
the current location w, then there exists positive number η such that
Ii XX Pj - W — EPi=1 kPi-wk Ii < Ii XX Pi— W
“j=1 kPj- W -EPn=I 蕊-副 11 ll ⅛ kpf
for all E ∈ (0, η].
15
Under review as a conference paper at ICLR 2019
Proof. Without loss of generality, we regard w as the origin. Let f()	=
Il Pn= ι kpj ：PnbII * * * * * kpikɪ∣∣, then f(O) = ∣∣ P= 1 尚||. Therefore, We only need to ShoW
f(0) < O. NoW denote Xj = ɪpjɪ, Pj(E) = Pj- E Pn=I Xi and U = - Pn=I Xi. That is,
pj () = pj + u. Since
=DX Pj(E) X Pj(E) E
=j kPj (E)k, j⅛ kPj (E)k /,
We have
2DX Pj(E)	d (X Pj(E) ∖E
j kPj (E)k,dE lj⅛ kPj (E)II 〃
and
d(X Pj(E) λ = X kPj(E)ku - ⅜j⅜Pj(E)
dE Vj⅛ kPj(E)IP = j=1	∣Pj(e)∣2
Hence
f0(,) = 2D X Pj(E) X kPj(E)ku - ⅛⅝⅜Pj(E)
f()= j kPj (E)I , j⅛	kPj (E)I2
Note thatPj(O) = Pj and IXjI = 1. We have
2D X Pj_ X kPj|u - %1 Pj
S 两,M -j-
nb	nb 1
2( X Xj, X 西(u - hu, Xji Xj))
nb 1
，X 扃(U -hu, Xji X力〉
kuk2- hu, Xji2
= 2 - u,
nb
=-2X
j=1
nb
≤-2X
j=1
=O
IPj I
kuk2-kuk2kXjk2
l∣Pjk
Since the equality holds When hU, Xji2 = IUI2 IXj I2 for all j, We have strict inequality When all
Pi's are not located on a single ray from the origin.	□
C.3 Proof of Theorem 3
The proof of Theorem 3 is very similar to that of Lemma 2.
Theorem 3.	Let Pι(w0),P2(w0), ∙…,Pnb(w0) be d-dimensional vectors，and all Pi(w0),s are
not on a single ray from the current location wt0. If
Il X Pi(WO) - w0 _ X gi(wiT) ∣∣ ≤ ξ	(6)
Il = ∣Pi(w0)-wOk - = i@(wi-1)k∣∣一ξ	()
for sufficiently small ξ > O, then there exists positive number η such that
∣∣ X	Pj(WO) - w0 - EPn=1 kgi(Wi-1)k ∣∣ < 11X Pi(WO) -	w0	∣∣	(7)
∣∣ j=ι	∣Pj (w0) - wo -EPn=I 备wt⅛k ∣∣ ∣∣=怔(WO)-	w0k	∣∣
kgi (wt )k
16
Under review as a conference paper at ICLR 2019
for all ∈ (0, η].
Proof. We regard w0 as the origin 0. For simplicity, write Pi(0) and gi(wi-1) as Pi and gi,
respectively. Let fg = IlPn=ι kp 二 Pn=I Ik k『and fg = IIPn=ι
jpn=ι k⅛	∣∣2
kpj-e pnbι k^ik k
Denote U = - Pn= 1 ɪ, t = Pn= 1 ɪ - Pn=I 衢 and pj9 = Pj + e(U + t). Then
I	nb
fg = IlX
i=1
Pj9 I2
kPj (e)kl1 .
Now we differentiate f() with respect to , that is,
nb	〜nb ∣∣75 ∙(f)∣∣(u + t) _ hu+tBjU -()
f0(,) = 2DX Pj 9 X kpj(e)k(u +t)	kPj (e)k Pj(Q E
f( )= ∖j⅛ kPj (e)k，j⅛	kPj (e)k2	/.
Recall that Pj (0) = Pj. Rewrite ∣P^ = χj∙ and use f 0(O) in the proof of Lemma 2
nb
fo(0)
nb
2D-U,X
j=1
nb kPjk(u +1)- hu+⅛p1
kPjk2
U+t -〈u+t,岛〉岛
Pj E
kPj k
2D - U,
2D - U,
nb 1
X 西(U+1 -hu+t，xji X力〉
nb 1	nb 1
X kj (u - hu, xji xj〉+2(- u, X kj tt - ht, xj i xj))
nb	1
=f (O)- 2 X 而■ (hu, ti- ht, Xjihu, xji)
Since f0(0) < 0 by the proof of Lemma 2,
nb	1
fo(O) < 0	0	2X H(ht, xji hu, xji - hu,ti) < |f0(O)|.
j=1 kPj k
By using kxj k = 1 and applying the Cauchy inequality,
nb
2	____
j=1 kPj k
nb
(ht, xj i hu, Xji-hu, ti)≤ 2£
ktkkXj kkukkXj k + kukktk
j=1
=4 X kukktk
=j=1 kPj k
≤ 4nbkukktk
一minjkPjk
kPj k
〉
≤ m⅞‰ Euk ≤ X kxik = nb)
Define r = minj kPj k. If
〜If0(O)Ir
ξ< -4nτ
then
~,、
f9 < 0.
□
17
Under review as a conference paper at ICLR 2019
D Proofs for Corollary 3.1
Corollary 3.1. Let pi be local minibatch solutions for each fIi. Suppose a region R satisfying:
For all w, w0 ∈ R, pi(w) = pi (w0) = pi
for all i = 1,…,n. Further, assume that Hessian matrices of fι∕s are positive definite, Well-
conditioned, and bounded in the sense of matrix L2 -norm on R. If SGD moves wt0 to wt0+1 on
R with a large batch size and a small learning rate, then K(Wt) > ^(w0+ι). Moreover, we Can
estimate ^(w0) and K(w0+J by minibatch gradients at Wt and w0+ι, respectively.
Proof. Recall that w0+ι = Wt + η Pn= 1 gi(wi-1) where η is a learning rate. To prove Corollary
3.1, we need to ShoW ^(w0+ι) < K(Wt) which is equivalent to
Il XX Pj(wt+ι)	- Wt - ηPn= 1 gi(wiT) Il	‹ ∣∣Xnb- Pi(Wt)	- Wt	Il	(8)
"j=1 l∣Pj(w0+i)	- Wt -ηPn= ιgi(WiT)k ll ll i=1	kpi(Wt)- Wtk	l|.
Since ∣∣Vw2fii(∙)k2 is bounded on R, Rwfɪɪ(∙) is LiPschitz continuous on R(Bottou, 2010). If
the batch size is sufficiently large and the learning rate η is sufficiently small, |@(Wi-1)k ≈
llgi(Wt)k ≈ τ for all i by Theorem 1. Therefore, we have
nb
η X gi(WiT)
i=1
nb
≈ τη
i=1
*T)
kgi(Wi-1)k
If we denote τη as , we can convert (8) to (9).
i∣ X	Pj(Wt+l) - Wt - e Pi=1	kgi(wi-1)k i∣	< 11 X	Pi(Wt) — w0 ∣i	(9)
ll 之	kpj(Wt+ι) - Wt - ePn=I	卷号Ik Il Il =	kpi(Wt)- Wtk I∣.
kgi (wt )k
Since both Wtt+1 and Wtt are in R for small learning rate, we have Pi (Wtt+1) = Pi(Wtt) = Pi by the
assumption. That is, (9) is equivalent to (7). In (7), gi(Wi-1)∕kgi(Wi-1)k cannot be replaced by
(Pi - Wtt)/lPi - Wtt l in general. Hence we introduce Definition D.1 and Lemma D.1 to connect
the direction of the minibatch gradient with the corresponding local minibatch solution.
Definition D.1. The condition number c(A) ofa matrix A is defined as
c(A)
σmax(A)
σmin(A)
where σmax(A) and σmin(A) are maximal and minimal singular values ofA, respectively. IfA is
positive-definite matrix, then
C(A) = λmx7A.
λmin (A)
Here λmax(A) and λmin(A) are maximal and minimal eigenvalues ofA, respectively.
Lemma D.1. If the condition number of the positive definite Hessian matrix of fIi at a local mini-
batch solution Pi, denoted by Hi = Vw2fIi(Pi), is close to 1 (well-conditioned), then the direction
to Pi from W is approximately parallel to its negative gradient at W. That is, for all W ∈ R,
Pi- W _ JMwL || 〜0
kpi- Wk	kgi(w)k∣∣~
where gi(W) = -Vw九(w).
Proof. By the second order Taylor expansion,
fii(w) ≈ fii(Pi) + 1(w — Pi)>Hi(w — Pi).
18
Under review as a conference paper at ICLR 2019
Hence,
gi(w) = -Vwfii(W) ≈ -Hi(W - Pi)
Denote pi - w as x. Then, we only need to show
崎-舒『≈ 0
Since Hi is positive definite, we can diagonalize it as Hi = Pi>ΛiPi where Pi is an orthonormal
transition matrix for Hi .
Il_x__Hx∖∖2 = 2-2 x>Hix
n 同	kHiX∣∣∖∖ =	同IIHixk
(Pi x)> ΛiPi x
=-2 kPixkP>ΛiPixk
=2	2 (Pix)>ΛiPiX
=-HPiXkMiPxk
≤ 2 - 2 P N(PiX)2
≤	IIPixkMiPi Xk
≤ 2 - 2	λmink马xk2
一	IIPixkAmaxkPixk
=2 - 2 Nmn ≈ 0
λmax
□
Lemma D.1 suggests that a well-conditioned Hessian matrix of 九 at Pi allows gi(w)/|@(w)k to
be replaced by (pi - W)/kpi - Wk for all W ∈ R. Using this, we prove Lemma D.2.
Lemma D.2. Let W be a parameter in R. If the condition number of Hessian matrix of fIi is
sufficiently close to 1 (well-conditioned) and kw-w0 k is sufficiently close to 0, then
∖∖ Pi - w0 _ gi(w) ∖∖ ≤ ɪ
∖∖ kPi - w0k	kgi(w)k ∖ ^ nb
for all sufficiently small ξ.
Proof. We have
Pi - W0	_ gi(w) ∖∖ l∣Pi - w0k	kgi(w)k ∖	≤	∖∖	Pi	- w0	_	g(w)	∖∖	+	∖∖	g(w)	_	gi(w) ∖∖ 一∖∖ kPi	- W0k	kg(w)k∖∖	+	∖∖ kg(w)k	kgi(w)k∖∖ ≤	∖∖	Pi	- w0	_	g(w)	∖∖	+	∖∖	g(w)	_	Pi	- w0	∖∖ 一∖∖ kPi	- W0k	kg(w)k∖∖	+	∖∖ kg(w)k	kPi	- W0k∖∖ + ∖∖	Pi - w0 _ Pi	- W ∖∖	+ ∖∖ Pi	- W _ gi(w) ∖∖	kPi - W0k kPi	- w∣∖∖	∖∖ kPi	- w∣ kgi(W)k ≤ e + e + Si-F-MOMb + e kPi - Wt0 kkPi - Wk
3 +	2
Pi - Wt0 k2 - hPi - Wt0, W - Wt0i
l∣Pi - w0kkPi - wk
for sufficently small (See Lemma D.1). Now we only need to show
∕2(1	kPi - w0k2 - hPi - w0, W - w0i
V I	kPi - w0kkPi- wk
(10)
19
Under review as a conference paper at ICLR 2019
Since kw-W0k is sufficiently small, We have
∣∣Pi — w0k2 —(Pi — w0, W — w0) = ∣∣Pi — w0k (∣∣Pi — w0k — / TTPi——wt-τ-, W — w0∖ )
i t i	t,	t i t i t	kp - w0 k , t
≥ kPi — WOk(IPi — w0k — kw — WOk)
∣Pi — Wt0 ∣2 1 —
Iw - WOk、
kPi - w0 ∣μ
≥ 0.
By using the above non-negativeness, We have the folloWing inequality.
1 ≥ kPi — Wk2 —〈Pi — w0,W — wO〉
―	kPi — WOkkPi - Wk
≥ kPi — WOk2 —〈Pi — w0, W — W〉
—kPi — WOk(kPi — W?k + kW — WOk)
kPi-w0k _〈 Pi-w0	w-w0 \
= kw-w0k	∖kpi-w0k, kw-w0
1 I kPi-w0k
1+ kw-w0k
kPi-w0k _ 1
≥ kw-w0k
-1 + kPi-w0k .
1+ kw”
(11)
As kw-Wθk → 0+, (11) is monotonically increasing to 1. This implies that (10) holds for suffi-
ciently small e.	口
With a small learning rate, Wti-1’s are in R for all i ∈ {1, . . . , nb}. As a result, by Lemma D.2, We
have
Il Pi — W00	g∕i(Wf-1) Il ξ
I 庙F -面WFII ≤ n	()
for sufficiently small ξ. This implies (6) since
nb	nb	i 1	nb	i 1
II X Pi - W — X gi(W右)∣i ≤ X ∣i Pi ― W — gi(Wt ) i∣
ii = kPi — Wk —i=11@(WiT)k1 - = ii kPi — Wk - |@(WiT)kII.
Then we can apply Theorem 3 and K(w?) > K(W?+i) holds.
For the last statement, 'Moreover we can estimate ^(w01) and ^(w0+ι) by minibatch gradients at
WtO and WtO+1, respectively.”, recall that
K(WO)=h(IIX WTk II)
where h(∙) is increasing and Lipschitz ContinUoUS(Lemma 1). By Lemma D.1, we have
ii Pi(WO) — wO _ gi(wO) II < ξ
ii kPi(WO)- WOk	kgi(wO)k i	nb
for sufficiently small ξ > 0. Therefore,
III X Pi(WO)	— wO	ii	_ ii	X gi(wO) III ≤	ii	X Pi(WO) — wO	_ X	gi(WO) II
"i i=ι kPi(WO)- WOkII ii	i=ι	kgi(wO)k ii -	ii	i=ι kPi(WO)- WOk i=ι	kgi(WO)k ii
where rhs is bounded by ξ. Hence, Lipschitz continuity of h(∙) implies that
Ih(IIX Pi(WO) — wO n h(IIX gi(wO) Hλ∣ , 0
∏i 之 kPi(wO) - WOk D 一项之 BW D1 → 0
as ξ → 0. That is,
E ≈ h(IIX 熬3 II)).
Since t is arbitrary, we can apply this for all W ∈ R including w01+ι.	口
20
Under review as a conference paper at ICLR 2019
E Experimental Details
E.1 Model Architecture
For all cases, their weighted layers do not have biases, and dropout (Srivastava et al., 2014) is not
applied. We use Xavier initializations(Glorot & Bengio, 2010) and cross entropy loss functions for
all experiments.
FNN The FNN is a fully connected network with a single hidden layer. It has 800 hidden units
with ReLU (Nair & Hinton, 2010) activations and a softmax output layer.
DFNN The DFNN is a fully connected network with three hidden layers. It has 800 hidden units
with ReLU activations in each hidden layers and a softmax output layer.
CNN The network architecture of CNN is similar to the network introduced in He et al. (2016)
as a CIFAR-10 plain network. The first layer is 3 × 3 convolution layer and the number of output
filters are 16. After that, we stack of {4, 4, 3, 1} layers with 3 × 3 convolutions on the feature maps
of sizes {32, 16, 8, 4} and the numbers of filters {16, 32, 64, 128}, respectively. The subsampling is
performed with a stride of 2. All convolution layers are activated by ReLU and the convolution part
ends with a global average pooling(Lin et al., 2013), a 10-way fully-conneted layers, and softmax.
Note that there are 14 stacked weighted layers.
+BN We apply batch normalization right before the ReLU activations on all hidden layers.
+Res The identity skip connections are added after every two convolution layers before ReLU
nonlinearity (After batch normalization, if it is applied on it.). We concatenate zero padding slices
backwards when the number of filters increases.
E.2 Data
We use neither data augmentations nor preprocessings except scaling pixel values into [0, 1] both
MNIST and CIFAR-10. In the case of CIFAR-10, for validation, we randomly choose 5000 images
out of 50000 training images.
21
Under review as a conference paper at ICLR 2019
Sample size
1,000
2,000
3,000
Figure 8: We show K estimated from {1,000 (black), 2,000 (blue), 3,000 (red)} random samples of
the vMF distribution with underlying true κ in 10, 000-dimensional space, as the function of κ (in
log-log scale except 0). For large κ, it is well-estimated by K regardless of sample sizes. When the
true κ approaches 0, we need a larger sample size to more accurately estimate this.
Table 1: The value of K (mean±std.) estimated from {1,000, 2,000, 3,000} random samples of
the vMF distribution with K = 0 across various dimensions.
Network	Dimension	K (1,000 samples)	K (2,000 samples)	K (3,000 samples)
CNN	206, 128	6, 527.09 ± 5.98	4, 608.16 ± 6.18	3, 762.45 ± 7.23
CNN+Res	206, 128	6, 527.09 ± 5.98	4, 608.16 ± 6.18	3, 762.45 ± 7.23
CNN+BN	207, 152	6, 563.62 ± 8.68	4, 633.84 ± 5.60	3, 781.04 ± 3.12
CNN+Res+BN	207, 152	6, 563.62 ± 8.68	4, 633.84 ± 5.60	3, 781.04 ± 3.12
FNN	635, 200	20, 111.90 ± 13.04	14, 196.89 ± 14.91	11, 607.39 ± 9.27
FNN+BN	636, 800	20, 157.57 ± 14.06	14, 259.83 ± 16.38	11, 621.63 ± 6.83
DFNN	1, 915, 200	60, 619.02 ± 13.49	42, 849.86 ± 18.90	34, 983.31 ± 15.62
DFNN+BN	1, 920, 000	60, 789.84 ± 17.93	42, 958.71 ± 25.61	35, 075.99 ± 12.39
F SOME NOTES ABOUT THE κ ESTIMATE
We point out that, for a small κ, the absolute value of K is not a precise indicator of the uniformity
due to its dependence on the dimensionality, as was investigated earlier by Cutting et al. (2017). In
order to verify this claim, we run some simulations. First, we vary the number of samples and the
true underlying K with the fixed dimensionality (Unfortunately, we could not easily go over 10, 000
dimensions due to the difficulty in sampling from the vMF distribution with positive K.). We draw
{1, 000, 2, 000, 3, 000} random samples from the vMF distribution with the designated K. We
compute K from these samples.
As can be seen from Figure 8, the K approaches the true K from above as the number of samples
increases. When the true K is large, the estimation error rapidly becomes zero as the number of
samples approaches 3, 000. When the true K is low, however, the gap does not narrow completely
even with 3, 000 samples.
While fixing the true K to 0 and the number of samples to {1, 000, 2, 000, 3, 000}, we vary the
dimensionality to empirically investigate the K. We choose to use 3,000 samples to be consistent
with our experiments in this paper. We run five simulations each and report both mean and standard
deviation (Table 1).
We clearly observe the trend of increasing K's with respect to the dimensions. This suggests that
We should not compare the absolute values of K's across different network architectures due to
the differences in the number of parameters. This agrees well with Cutting et al. (2017) which
empirically showed that the threshold for rejecting the null hypothesis of K = P by using K where P
is a fixed value grows with respect to the dimensions.
22
Under review as a conference paper at ICLR 2019
G Other four training runs in Figure 7
We show plots from other four training runs in Figure 6. For all runs, the curves of GS (inverse of
SNR) and K are strongly correlated while GNS (inverse of normSNR) is less correlated to GS.
(a) CNN, initialization 1
(b) CNN, initialization 1
8 × 104
7 × 104
6 × 104
5 × 104
4 × 104
K3 × 104
(c) CNN+BN, initialization 1
2 × 104
0.05	0.10	0.20
SNR
(d) CNN+BN, initialization 1
Q:NSE」OU
.0 .0 .0
5 2 10
0.75-
o
ra
0 0.50-
0.25-
1.00-
0.00-	ι	ι	ι ι
1	5	20	80	200
Epoch
2 × 105-
105 二
A
κ λ -
5 × 104 -
2 × 104-
(f) CNN+Res+BN, initialization 1
O SNR-K
SNR-normSNR
-0.5
0.05 θ.1θ 0.20	0.50
SNR
-10.0
Q:NSE」OU
.0 .0
52
(e) CNN+Res+BN, initialization 1
---normSNR
SNR
---- Train loss -------- Valid loss
K
Figure 9: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid loss),
inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and di-
rectional uniformity κ. We normalized each quantity by its maximum value over training for easier
comparison on a single plot. In allthe cases, SNR (orange) and K (red) are almost entirely correlated
with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-
K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest
that the SNR is largely driven by the directional uniformity.
23
Under review as a conference paper at ICLR 2019
1.00-
0 075-
比 0.50-
0.25-
0.00-
1
0∙2
0
5 4
00
×
5
Λκ
Oo
2
E
-10
Q:NSE」OU
0∙5
(a) CNN, initialization 2
1.00-
0.75-
O
ra
比 0.50-
0.25-
0.00-	ι	ι	ι ι
1	5	20	80	200
Epoch
(b) CNN, initialization 2
-10.0
-5.0
-2.0
-1.0
-0.5
0.05	0.10	0.20	0.50
frNmE0u
1.00-
0.75-
O
ra
比 0.50-
0.25-
0.00-
1
(c) CNN+BN, initialization 2
Λκ
80
(d) CNN+BN, initialization 2
E^u
ZNSEOU
O 0 0 5
5 2 10
05
O R
N
S
10
O
50
O
(e)	CNN+Res+BN, initialization 2
κ	normSNR
SNR
---- Train loss Valid loss
(f)	CNN+Res+BN, initialization 2
。SNR-K
SNR-normSNR
R
N
S
— 5
"
E
Figure 10: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid
loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and
directional uniformity κ. We normalized each quantity by its maximum value over training for easier
comparison on a single plot. In allthe cases, SNr (orange) and K (red) are almost entirely correlated
with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-
K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest
that the SNR is largely driven by the directional uniformity.
24
Under review as a conference paper at ICLR 2019
1.00-
0 075-
比 0.50-
0.25-
0.00-
1
106 二
5 × 105 :
^2 × 105-
105≡
5 × IO4：
5	20	80	200
Epoch
Q:NSE」OU
5
O
N
S
2<)
(a) CNN, initialization 3
(c) CNN+BN, initialization 3
(b) CNN, initialization 3
1.00-
0.75-
O
ra
比 0.50-
0.25-
0.00-
1	5	20
Epoch
2 × 105-
105 二
^5 × 104-
2 × 104-
80	200
(d) CNN+BN, initialization 3
-10.0
-5.0
-2.0
-1.0
-0.5
0.05	0.10	0.20	0.50
SNR
(f) CNN+Res+BN, initialization 3
。SNR-K
SNR-normSNR
frNmE0u
(e) CNN+Res+BN, initialization 3
κ	normSNR
SNR
---- Train loss Valid loss
2
O
R
N
S
Figure 11: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid
loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and
directional uniformity ^. We normalized each quantity by its maximum value over training for easier
comparison on a single plot. In allthe cases, SNr (orange) and K (red) are almost entirely correlated
with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-
K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest
that the SNR is largely driven by the directional uniformity.
25
Under review as a conference paper at ICLR 2019
(a) CNN, initialization 4
Q:NSE」OU
5^
0 0 5
2 10
1.00-
0.75-
o
ra
比 0.50-
0.25-
0.00-	ι	ι	ι ι
1	5	20	80	200
Epoch
(c) CNN+BN, initialization 4
(b) CNN, initialization 4
-10.0
-5.0
-2.0
-1.0
-0.5
0.05	0.10	0.20
SNR
(d) CNN+BN, initialization 4
frNmEOU
(e) CNN+Res+BN, initialization 4
κ	normSNR SNR
---- Train loss Valid loss
(f) CNN+Res+BN, initialization 4
。SNR-K
SNR-normSNR
ZNSEOU
O 0 0 5
5 2 10
Figure 12: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid
loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and
directional uniformity κ. We normalized each quantity by its maximum value over training for easier
comparison on a single plot. In allthe cases, SNr (orange) and K (red) are almost entirely correlated
with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-
K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest
that the SNR is largely driven by the directional uniformity.
26