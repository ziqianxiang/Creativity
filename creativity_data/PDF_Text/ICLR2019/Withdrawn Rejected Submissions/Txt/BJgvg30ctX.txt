Under review as a conference paper at ICLR 2019
Information Regularized Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We formulate an information-based optimization problem for supervised classifi-
cation. For invertible neural networks, the control of these information terms is
passed down to the latent features and parameter matrix in the last fully connected
layer, given that mutual information is invariant under invertible map. We pro-
pose an objective function and prove that it solves the optimization problem. Our
framework allows us to learn latent features in a more interpretable form while
improving the classification performance. We perform extensive quantitative and
qualitative experiments in comparison with the existing state-of-the-art classifica-
tion models.
1 Introduction
Quantities of information are nonlinear measures capable of describing complex relationship be-
tween unstructured data and they form the basis of the probabilistic algorithms in the literature of
machine learning. Information theoretic methods are also reported to be effective on improving
deep generative models (Chen et al. (2016); Kim & Mnih (2018)) and deep learning models for
classification (Grandvalet & Bengio (2004); Pereyra et al. (2017)). Information Bottleneck (IB)
problem (Tishby et al., 1999) is formulated as:
minimize I(X; T) - λI(Y ; T) ,	(1)
where the solution random variable T is interpreted as a minimal sufficient representation of signal
X for label Y and the mutual information is defined as
I(X; Y )=L ZY p(x,y)log (ppx≡) dydχ.
(2)
The term I(X; T) has its origins in Lossy Compression and Rate-Distortion Theory (Cover &
Thomas, 2006), conveying an simple idea of ”keep only what is relevant”.
However, Saxe et al. (2018) argued that the mutual information I(X; T ) between signal X and fea-
ture T in intermediate layer is infinite, as the transformation from X to continuous random variable
T is deterministic. In addition they showed experimentally that layers equipped with ReLU actually
do not compress too much information, which is supported by many recent work on the invertibility
of the neural network (Dosovitskiy & Brox (2015); Jacobsen et al. (2018)). This motivates us to con-
sider a different problem with similar principle idea: we would like to establish a theoretically valid
objective that allows the neural network to extract only the relevant information for classification
from the data.
We focus on the discrete prediction random variable Y inferred by the probabilistic model P(Y|X)
and introduce the following information optimization problem for supervised classification:
■	" T∕ΛΛ C∖
maximize I(Y ; Y )
J .	T / -IT- T^>∖	1» T -√^>∖	C	C
subject to I(X; Y) - I(Y; Y) < τ , for some τ > 0 .
(3)
The intuition behind this objective lies in twofold:
Information perspective: A good classification model should be robust against irrelevant features
of X, and prevent over-fitting in the learning process. In optimization problem (3) we maximize
the relevant information I(Y; Yb ), while constraining the irrelevant information I(X; Yb ) - I(Y; Yb )
1
Under review as a conference paper at ICLR 2019
^
^
^
^
that X has about Y . Although I(X; Y ) - I(Y ; Y ) converges to zero as I(Y ; Y ) approaching its
maximum (see Figure 1L), in practice it’s never attained due to the limited capacity of the models
or over-fitting. Our proposed constrain addresses the problem of over-fitting: if two models achieve
the similar classification accuracy, this constraint prefers the one that does not overfit to spurious
factors of variation in X (e.g., pixel-level artifact/noise in the image that accidentally correlates to
the labels in the training data).
Prediction confidence perspective: A good classification model should not be certain about its
decision which is in fact wrong. However, modern neural networks are too confident in their predic-
tions (Guo et al., 2017; Szegedy et al., 2015; Pereyra et al., 2017). To be more precise, high capacity
neural networks mostly assign labels of data with prediction confidence near 0 or 1. In particu-
lar, they assign 0 probability to some correct labels and therefore do not have enough flexibility to
correct themselves from making the wrong prediction. We propose to compress the irrelevant infor-
mation I(X; Y ) - I(Y ; Y ), where minimizing I(X; Y ) decreases the confidence on all predictions
but maximizing I(Y ; Y ) increases the confidence on the correct predictions. Therefore the overall
effect reduces the certainty on the false prediction of Y (see Figure 1L).
To solve this optimization problem, we first present some insights on the dynamics of deep neural
network, which can be decomposed into two stages: (i) Transformation stage: samples {Xk }k=1:n
of the high dimensional unstructured signal X are transformed under the deep invertible (informa-
tion preserving) feature map F to become (almost) linearly separable; (ii) Classification stage: the
weight matrix w in the last fully connected layer together with the Softmax function, takes structured
features {F (Xk)}k=1:n as input and gives predictions.
Invertibility of F allows neural networks to pass the control of I(X; Y ) = I(F (X); Y ) towards
F (X) and w in the last layer, where F (X) can be interpreted as transformed signal that preserves
information about the original signal X and the inference model becomes conceptually linear with
classifier w (see Figure 1R). In Section 2 we derive objective function (7) and prove that it solves
the optimization problem (3). We show the classification performance is improved in Section 4.1
and the features F (X) are sculpted into a form with more interpretability entry-wise in Section 4.2.
Figure 1: (L) An information Venn diagram: three disks represent the entropy of X, Y, Yb respec-
tively, the area in red is the relevant information I(Y ; Y ), the area in grey is the irrelevant information
I(X; Y ) - I(Y ; Y ). The optimal solution is obtained when the smaller disks coincide, which is typ-
ically not achieved in practice. In particular, the trained model may be extremely confident in its
prediction (when H(Y ) lies inside of H(X)), but predicts the wrong label (having large grey area).
Our optimization problem explicitly prohibits the growth of grey area throughout the training. (R)
Logic chart of our formulation: our proposed optimization problem only involves F (X) and w,
allowing us to have control over the latent feature F (X) directly.
The invertibility property has been empirically demonstrated for complex non-linear deep neural
networks that are widely used in practice. We will discuss the literature in Section 3. In addition, we
prove in Proposition C.1 that a lower bound of classification error is minimized if neural network is
invertible.
Our contribution: Our contribution lies in the following: (i) we formulated a novel information
optimization problem for supervised classification; (ii) we propose a simple objective function that
improves supervised deep learning with better performance and interpretability; (iii) we formally
2
Under review as a conference paper at ICLR 2019
justify the use of `1 , `2 regularization from an information perspective. Different from the naive
regularization on w, our regularization on wTF (X) is novel and effective.
2 Main Results
Consider a classification problem where the training data D = {(xk, yk)}k=1:n are sampled from
random variable pair (X, Y ) with unknown joint distribution. Each xk is fed into a deep prob-
abilistic model, which outputs probability densities and predicts ybk , a realization of the predic-
tion random variable Yb . Let C denote the label class and X denote the signal space, then the
mutual information between random variables, e.g., continuous X and discrete Yb , is defined as
I(X; Yb) = Pb∈C JX p(x, y) log (PpxxPyb)) dx, and the entropy of Y is defined as H (Y)=
- Py∈C p(y) log p(y).
We first call out our assumptions used throughout our analysis. (I): we assume the marginal densities
of Y, Y are uniform over C ; (II): there exists a unique true label for every sample of X .
Mutual information is bounded and its gradient with respect to logits is approximately zero over a
large domain. In particular if the logits are initially small for true labels, gradient updates cannot
effectively correct them. Therefore it’s not practical to train mutual information terms directly. In
this section we introduce alternative terms and prove that they are feasible for our purpose. We show
in Proposition 2.2 that I(Y; Y) is maximized if the classical cross entropy objective is minimized.
On the other hand, we show in Proposition 2.1 that for invertible F, I(X; Y) = I(F (X); Y) is
minimized if kwTF (X)k is minimized. We derive our objective function (7) in Section 2.2. Our
experimental result in Section 4.1 verifies that the proposed objective function does compress the
irrelevant information I(X; Y) - I(Y; Y).
2.1	S olving Optimization Problem with Feasible Terms
Without loss of generality, we consider the binary classification problem, i.e. the label class C =
{±1}. To tract the population quantities I(F (X); Y) and I(Y; Y), we decompose each of them
into an empirical part and a probabilistic bound, which is negligible if sample size n is large. In
Proposition 2.1, we show that in order to compress I(F (X); Y), we need to compress the norm of
classifier w and feature F(X). In particular, smaller |wTF(X)| represents lower confidence of the
model on its predictions Y, indicating a smaller amount of mutual information I(F (X); Y). The
proof is provided in Appendix A.
Proposition 2.1. I(X; Yb) = I(F (X); Y) is well estimated by its empirical version
∑^n=ι ∑y∈c p(b∣xk )log(2p(b∣xk))∕n With high probability, which shares the same unique (global)
minimum with kn=1 |wT F (xk )| at wTF(xk ) = 0, for all k ∈ {1, ..., n}.
Denote the sigmoid function with σ(a) = 1∕(1 + e-a). Proposition 2.2 establishes the relation-
ship between maximization over mutual information I(Y; Y) and minimization over cross entropy
- Pkn=1 log σ(yk wTF(xk )); higher confidence of the model on its correct predictions over the
samples indicates a larger value of I(Y; Y). The proof is provided in Appendix B.
Proposition 2.2. I(Y; Y) is well estimated by its empirical version Pyb byblog (∏ ：y^) With
high probability, which shares the same unique (global) maximum with kn=1 log σ(yk wTF(xk ))
given that ykWTF(Xk) > 2, for all k ∈ {1,…，n}. Here byb = 21- EnyI σ(ywτF(Xi)) is an
unbiased estimate of p(y, yb) and πby+ = yb∈C πbyyb, πb+yb = y∈C πbyyb.
2.2	Derivation of Objective Function
In Lagrangian form of optimization problem (3), the constant τ can be dropped and the objective
becomes
λ
maximize (1 + λ)I(Y; Y) 一 λI(F(X); Y) ^⇒ maximize I(Y; Y) 一 】+ 入I(F(X); Y) .	(4)
3
Under review as a conference paper at ICLR 2019
Consider a single signal xk and its true label yk, we propose the following objective function for
binary supervised classification problem:
Lk = αR (IwTF(Xk)|) - logσ(ykWTF(Xk)) ,	(5)
where R is some regularizer. According to results in Section 2.1, minimizing (5) allows us to
maximize I(Y ; Y ) while constraining I(X; Y ). We typically choose the hyper-parameter α > 0 to
be a reasonably small number. The intuition comes from the observation that λ∕(1 + λ) is upper
bounded by one. If we compress I(X; Y ) harshly, then neural networks may choose to minimize
I(F(X); Yb) at a cost of minimizing I(Y ; Yb).
Recall from the information theoretic perspective of our proposed optimization problem, our regu-
larizer should prefer a model that does not overfit among all the ones with high training accuracy.
In this case neural networks assign only large logits wyT F(Xk) to true label yk for each signal
Xk, and generalization of (5) to multi-class case for I(F(X); Yb) can be simplified to constraining
wyTkF(Xk) - wjTF(Xk), where wj is the jth column of w, assigning feature F(Xk) a probability to
label j. We propose to simply constrain wyT F(Xk) and does not encourage increasing wjTF(Xk):
ewyTk F(xk)
Lk = αR (|wTkF(Xk)|) - lθg (ewTkF(Xk)+ P行TewjF(Xk)>	⑹
In our experiment, we take the Elastic Net approach by Zou & Hastie (2005) using a combination
of '2 and '1 regularizers: We use Holder's inequality to bound IwTcF(Xk)∣ with both SuPF(X) ∙
∣∣wyfc kι and Ilwykk2 ∙ ∣∣F (Xk )∣∣2. In practice we assume SuP F (X) to be a constant and is absorbed
into the hyper-parameter. Our proposed objective function is of the form:
1 n	ewyTk F(Xk )
L = a1||w||1 + ng∣α2kwyfc ∣2 ∙∣f (Xk )∣2 - log (ewj F (Xk)+ P#	ewjF (Xk) J ;⑺
TLT .∙	.1	.	1	∙	1 .	∙	∙	.1	1	♦	♦	,	1 .1	C T/，r 4≥∖ -I .	1	.
Notice that classical training methods maximize cross entropy and therefore I(Y ; Y ), but do not
compress I(X; Y ) explicitly. In Equation (7), we explicitly encourage the compression of irrelevant
information I(X; Y ) - I(Y ; Y ). As we demonstrate in Section 4.3, the proposed objective function
1	11	. CT/” 4≥∖	T/，广 4≥∖ .1	1	. .t . ∙ ∙
does encourage a smaller amount of I(X; Y ) - I(Y ; Y ) throughout the training process.
3	Related work
Recent experimental work reported that neural networks with invertible structure have better per-
formance. Dosovitskiy & Brox (2015) showed that images can be resconstructed from the latent
features in AlexNet through an inverting process; this reconstruction is further improved by Zhang
et al. (2016), where they built an encoder-decoder structure to encourage invertibility and showed
reconstructive objective is beneficial to the performance of the neural network (e.g., VGGNet).
Shang et al. (2016) proposed an invertible activation scheme named CReLU to preserve informa-
tion; Gilbert et al. (2017) analyzed theoretically the invertibility of CNN; Jacobsen et al. (2018)
built a theoretic invertible structure whose performance is comparable to ResNet He et al. (2015).
Invertibility seems to be an intriguing property or design principle that often emerges in the recent
state-of-the-art deep architectures.
Information theoretic methods are reported to be effective attacking machine learning problems. In
deep learning, IB was first introduced in Tishby & Zaslavsky (2015) and the follow-up experimen-
tal work Shwartz-Ziv & Tishby (2017). They argued that DNN structure forms a markov chain
and information is compressed layer by layer. The theoretical breakthrough by Achille & Soatto
(2017) established a connection between the IB objective and the generalization in deep learning,
carrying along with the notion of sufficiency and invariance of representations. Strouse & Schwab
(2016),Slonim & Tishby (1999) used IB objectives for clustering problems. Alemi et al. (2017),
Gao et al. (2018),Kim & Mnih (2018) established information theoretic approaches based on VAE
to encourage disentangled and informative latent representations; Grandvalet & Bengio (2004) in-
troduced entropy regularizer in semi-supervised learning; Krause et al. (2010) took Regularized
4
Under review as a conference paper at ICLR 2019
NaiveReg ResNet-32	α2 = 0.0005	a? = 0.001	α2 = 0.002	a2 = 0.004
Best Accuracy %	70.06 ± 0.38	69.94 ± 0.33-	69.86 ± 0.32-	68.99 ± 0.52~~
ResNet-32	Original	a2 = 0.01	a2 = 0.03	a2 = 0.05
Best Accuracy %	70.15 ± 0.33	70.34 ± 0.27	70.57 ± 0.20	70.25 ± 0.23
Constrain	0.296 ± 0.044	0.293 ± 0.043	0.280 ± 0.040	0.266 ± 0.038
ResNet-Wide	Original	a2 = 0.01	a2 = 0.05	a2 = 0.09	a2 = 0.15
Best Accuracy %	78.51 ± 0.27	79.37 ± 0.18	79.62 ± 0.13	79.64 ± 0.12	79.45 ± 0.14
Constrain	0.254 ± 0.056	0.240 ± 0.051	0.213 ± 0.049	0.198 ± 0.044	0.174 ± 0.038
Table 1: Performance comparison on CIFAR100, Best Accuracy (%, test set) and the average val-
ues of the constrain I(X; Y ) - I(Y ; Y ) throughout the training process are provided in the tables.
The first table gives the results of naive regularization on w for ResNet-32, with α1 = 0, α2 =
[0.0005, 0.001, 0.002, 0.003, 0.004], note that the choices of hyper-parameters are small because
it regularize the whole matrix w but not to specific columns as proposed; the second table is for
ResNet-32, comparing original ResNet and RegResNets with α1 = 0, α2 = [0.01, 0.03, 0.05];
the third table is for ResNet-28-10-Wide, comparing original ResNet and RegResNets with α1 =
5e-6, α2 = [0.01, 0.05, 0.09, 0.15]. All results are calculated from 10 samples. The constrain de-
creases as α2 increases; the performance can be effectively improved under proper choice of α2 .
Information Maximization(RIM) approach for classification problems; Chen et al. (2016) proposed
to add information ingredients to the objective of GANs, encourage to learn disentangled represen-
tations.
Our framework decomposes deep neural network into a composition of nonlinear transformation
map and a linear probablistic model; this idea was originated in Bell & Sejnowski (1995) where
they considered the blind separation problem and decompose the prediction Yb to be the sum of an
invertible deterministic part and a stochastic part. Amjad & Geiger (2018) and Kolchinsky et al.
(2017) also studied the IB problem in a stochastic setting. Our idea of explicit regularization on w
and F(X) is related to the margin based and stability based interpretations of generalization in deep
learning respectively, studied by Arora et al. (2018), Bartlett & Mendelson (2003), Neyshabur et al.
(2017), Sun et al. (2015).
4	Experiments
In our experiments we build the feature map F with ResNet or InvNet (introduced in Section 4.2).
In Appendix D we prove that ResNet by He et al. (2015) is invertible under mild assumptions. We
prefix the name of the model trained under our objective with ”Reg”, i.e. RegResNet/RegInvNet.
4.1	Performance
We report the accuracy of ResNet on test data of CIFAR100 in Table 1.1
We compare the performance between our proposed regularization on wyT F (xk ) and the naive reg-
ularization on w. In both form of regularization we take α1 = 0 and test over different choices of
α2 . We pick smaller α2 for naive regularization because it’s applied to the full matrix. We observe
the performance of ResNet-32 under naive regularization drops monotonically as α2 increases.
Under a suitable choice of hyperparameters, RegResNet outperforms ResNet by a noticeable margin.
It implies that our proposed constrain on the irrelevant information I(X; Y ) - I(Y ; Y ) is beneficial
to the classification performance. However, if the hyperparameters are too large, the performance
drops, i.e. α2 = 0.05 for ResNet-32 and α2 = 0.15 for ResNet-Wide; this matches the discussion
in Section 2.2 that the model may try to reduce the relevant information I(Y ; Y ). Our approach
addresses the problem of over-fitting; ResNet-Wide is improved by a larger margin compared to
1We use the open source code implemented by Xin Pan at https://github.com/tensorflow/
models/tree/master/research/resnet and build our objective function based upon it. Our results
for the baseline models match the reported ones on the website.
5
Under review as a conference paper at ICLR 2019
ResNet-32 because ResNet-Wide has higher capacity and is therefore over-fits more to the training
data.
4.2	Analysis on Features
We introduce another invertible structured neural network on MNIST dataset and analyze the feature
F(X) learned in the last layer qualitatively. The feature map F is built to be LeNet-300-100 and
the decoder D has the opposite structure. At each step during the training process, we update the
autoencoder F + D and the InvNet F + w alternatively; our regularization is applied to w as usual.
In this section we report our result with α1 = α2 = 0.002.
4.2.1	Learning to Overlook Irrelevant Information
We feed 1k testing samples of digit 9 into the neural network to get 1k samples of features F (X).
Recall that the features F(X) are vectors of dimension 100 and w is a matrix of size 100 × 10. We
calculate the mean and standard deviation of each entry of the features from these 1k samples. From
Figure 2 we see that under our regularization, the entry-wise products w10iTF (X)i becomes sparse
as only a few entries have large values for both feature and weight. This implies that the information
needed to compute the logits for classification is encoded into only a few entries of the feature,
which we regard as relevant entries. Our regularization forces small w10 on irrelevant entries, so the
logits it outputs are not sensitive to variations in these entries; on the other hand, we do encourage
large w10 on the relevant entries. This matches our motivation that a neural network should be robust
•	, ∙	1	, ∙ f	J T / T-I / -rz-∖ -v>∖ T/工厂 -V>∖ FC	,1	1	, ♦ C	J	-v>∖
against irrelevant information I(F (X); Y ) -I(Y ; Y ) and focus on the relevant information I(Y ; Y ).
Index of Feature Entry
Figure 2: Statistics plots for feature entries of digit 9. Recall that the features F (X) are vectors of
dimension 100 and w is a matrix of size 100 × 10. The horizontal coordinate represents the index
of entries of F(X) ranging from 0 to 99, the vertical coordinate represents entry values of the 10th
column of w, the values of mean and standard deviation for each entry of features from samples of
digit 9. For RegInvNet, w10 only assigns large value to representative entries and is more robust
against perturbations in other irrelevant entries.
Index of Feature Entry
4.2.2	Learning to Extract Relevant Information
It had been argued by Szegedy et al. (2013) that it is the space but not individual units in high level
features that encodes interpretable information. Under our regularization, a meaningful basis of the
informative space is found; in particular, the features of 9 are encoded into 10 entries (see Figure 2).
On the other hand, features that have high values in these entries are expected to be the features of
digit 9.
To validate this conjecture, for each digit, we find the entry of its feature mean with the highest
value. We use the micro and macro average ROC metric on these feature entries and compare the
results from InvNet and RegInvNet in Figure 3. The curve with larger area underneath indicates
higher representative power of individual entries learned in the features. We conclude that under our
regularization, relevant information for classification is encoded into only a few key entries of the
features, and these entries are highly indicative and interpretable.
6
Under review as a conference paper at ICLR 2019
wroα ①>ssod Φ≡H
InvNet
0.2	0.4	0.6	0.8
False Positive Rate
micro-average ROC curve (area = 0.81)
macro-average ROC curve (area = 0.81)
wroα ①>ssod Φ≡H
RegInvNet
micro-average ROC curve (area = 0.99)
“ I " macro-average ROC curve (area = 0.99)
0.4	0.6	0.8
False Positive Rate
Figure 3: The macro/micro average ROC curves for representative entries of features genearated
by original and regularized model. The entries of features learned under our regularization strongly
indicate the categories of the digits.
4.3 Relationship to Linear Regression
We trained ResNet-32 on CIFAR10, the feature F (X) is a vector of size 64, the classifier w is
a matrix of size 64 × 10. Note that the product wTF (X) is a vector of size 10 representing the
probability assigned by the model for each class.
Under our framework, deep learning can be conceptually simplified to regularized linear regression
if we regard F(X) as input data. However, F(X) depends on the model parameters in the previous
layers so it’s not fixed like real data. Moreover we observe in our experiments that naive regular-
ization on w alone will upscale the norm of F (X), which neutralizes the effect of regularization.
In Figure 4 We show that as under our regularization, the '2 -norm of W is suppressed while the '2-
norms of feature F(X) remain similar. In addition, several rows of w are trained to be zero, which
implies that many entries of the feature F (X) are regarded by the network as irrelevant information
for classification, since any variance in the entries of F (X) where the corresponding rows ofw are
zero has no influence on the probability assigned to each label class by the model.
Figure 4: Compression of the RegResNet-32 (Blue) and the original ResNet-32 (Orange) on CIFAR-
10 over the training process: the first plot records the average `2 norm of the last layer features F(X)
in a batch; the second plot records the average `2 norm of the columns ofw; the third plot records the
ratio of zero entries among all entries of w ; the plots for trained w with/without regularization after
84000 steps are listed on the right. Best test accuracy are 92.86% and 92.64% for regularized and
original ResNet-32 respectively. Under our regularization, the norm of the feature learned remains
similar and the norm of classifier w is smaller. Therefore w is less sensitive to ”support” and ”outlier”
features.
5	The Role of Invertibility
Invertibility allows us to treat F(X) as transformed data that preserves all the information from X,
and therefore work on the information regularization problem under a linear scheme.
In Appendix D we prove that ResNet is fairly invertible due to the intrinsic invertibility of the opera-
tor I+L given kLk < 1. In this section we build a PlainNet by using only L as the operator for each
building block, so the theoretical guarantee for invertibility is not present for PlainNet. In Table 2,
7
Under review as a conference paper at ICLR 2019
we see that PlainNet-32 can still benefit from our regularization, however, it’s performance is less
stable compared to ResNet-32 if the hyper-parameters are too large. The reason is for PlainNet, the
feature in the last layer F (X) does not preserve information about X very well, so it has a higher
demand on the capacity of the classifer w and is therefore more sensitive to our regularization.
Performance %	Original	α2 = 0.01	a2 = 0.05	α2 = 0.09	a2 = 0.15	α2 = 0.3
ResNet-32	92.49 ± 0.14	92.52 ± 0.30	92.76 ± 0.33	92.45 ± 0.37	88.36 ± 3.12	78.95 ± 4.33
PlainNet-32	90.06 ± 0.21	90.33 ± 0.24	90.25 ± 0.24	90.06 ± 0.31	85.97 ± 3.40	62.76 ± 16.22
Table 2: The performance statistics for ResNet-32 and PlainNet-32 without or under various regu-
larizations. For regularized models we hold α1 = 1e-5 and take α2 = [0.01, 0.05, 0.09, 0.15, 0.3].
The means and standard deviations reported are based on 5 samples. It can be observed that although
PlainNet-32 obtains marginal improvement for small α2, the performance drops dramatically and
becomes unstable as α2 increases.
6	Conclusion
We give an interpretation of the deep learning dynamics by decomposing it into an signal transfor-
mation stage and feature classification stage, where we emphasis importance of the classifier w in
the last fully connected layer given that the feature map F is invertible. Then we take the advantage
of the fact that mutual information quantities are invariance under invertible mapping to attack our
proposed information optimization problem for supervised classification in deep learning. Our the-
ory justifies the use of direct regularization terms on w, F(X) for neural networks with invertibility
property. Our regularization improves the performance of neural networks by a noticeable margin
and is capable of encouraging the interpretability of the entries of features learned in the last layer.
References
Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep
representations. CoRR, abs/1706.01350, 2017.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy.
An information-theoretic analysis of deep latent-variable models. CoRR, abs/1711.00464, 2017.
Rana Ali Amjad and Bernhard C. Geiger. How (not) to train your neural network using the infor-
mation bottleneck principle. CoRR, abs/1802.09766, 2018.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. CoRR, abs/1802.05296, 2018.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3, March 2003.
Anthony J. Bell and Terrence J. Sejnowski. An information-maximization approach to blind sepa-
ration and blind deconvolution. Neural Comput., 7(6), November 1995.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
gan: Interpretable representation learning by information maximizing generative adversarial nets.
CoRR, abs/1606.03657, 2016.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). 2006.
Alexey Dosovitskiy and Thomas Brox. Inverting convolutional networks with convolutional net-
works. CoRR, abs/1506.02753, 2015.
Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Auto-encoding total correla-
tion explanation. CoRR, abs/1802.05822, 2018.
Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, and Honglak Lee. Towards understanding
the invertibility of convolutional neural networks. In Proceedings of the 26th International Joint
Conference on Artificial Intelligence, IJCAI’17, 2017.
8
Under review as a conference paper at ICLR 2019
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In
Proceedings of the 17th International Conference on Neural Information Processing Systems,
NIPS’04, 2004.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. CoRR, abs/1706.04599, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. CoRR, abs/1512.03385, 2015.
Marcus Hutter and Marco Zaffalon. Distribution of mutual information from complete and incom-
Plete data. Computational Statistics & Data Analysis, 48(3):633-657, 2005.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015.
Jom-Henrik Jacobsen, Arnold W. M. Smeulders, and EdoUard Oyallon. i-revnet: Deep invertible
networks. CoRR, abs/1802.07088, 2018.
H. Kim and A. Mnih. Disentangling by Factorising. ArXiv e-prints, 2018.
Artemy Kolchinsky, Brendan D. Tracey, and David H. Wolpert. Nonlinear information bottleneck.
CoRR, abs/1705.02436, 2017.
Andreas Krause, Pietro Perona, and Ryan G. Gomes. Discriminative clustering by regularized in-
formation maximization. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and
A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 775-783. 2010.
P.D. Lax. Functional analysis. Pure and applied mathematics. ISBN 9780471556046.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring gener-
alization in deep learning. CoRR, abs/1706.08947, 2017.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regu-
larizing neural networks by penalizing confident output distributions. CoRR, abs/1701.06548,
2017.
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren-
dan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning.
In International Conference on Learning Representations, 2018.
Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving
convolutional neural networks via concatenated rectified linear units. CoRR, abs/1603.05201,
2016.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. CoRR, abs/1703.00810, 2017.
Noam Slonim and Naftali Tishby. Agglomerative information bottleneck. In Proceedings of the
12th International Conference on Neural Information Processing Systems, NIPS’99, 1999.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 2014.
D J Strouse and David J Schwab. The deterministic information bottleneck. In Proceedings of the
Thirty-Second Conference on Uncertainty in Artificial Intelligence, UAI’16, 2016.
Shizhao Sun, Wei Chen, Liwei Wang, and Tie-Yan Liu. 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.
9
Under review as a conference paper at ICLR 2019
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. CoRR,
abs/1503.02406, 2015.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. 1999.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci-
ence. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press,
2018.
Yuting Zhang, Kibok Lee, and Honglak Lee. Augmenting supervised neural networks with unsu-
pervised objectives for large-scale image classification. CoRR, abs/1606.06582, 2016.
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 2005.
10
Under review as a conference paper at ICLR 2019
A Proof of Proposition 2.1
C	<* 1	. i 1 ∙ 1	i .	T∕" -τ≥∖	ι .ι	F 1	,	ι	i` .λ t
Proposition 2.1 establishes	a connection between	I(X, Y )	and the absolute	value	of the logits
|wT F (X)| for the binary case. Intuitively, decreasing the confidence of the model on its predic-
tions will decrease the mutual information I(X; Y ).
Proposition 2.1. I(X; Yb) = I(F (X); Y ) is well estimated by its empirical version (Monte-
carlo approximation) with high probability, which shares the same unique (global) minimum with
Pkn=1 |wTF(xk)| at wTF(xk) = 0, for all k ∈ {1, ..., n}.
EI	.	1 ♦ C	J τ / ” -τ≥∖ ♦	♦
The mutual information I(X; Y ) is given as
I(X; Y ) = ZX X pi (P⅛≡ )dx.
y∈C
Apply the assumption (II), the marginal distribution of Y is uniformly distributed:
p(x,y) = p®|x) = ‰(blx)
p(χ)p(b) = p(b) = p(y1 ).
Substituting (9) into (8) yields
I(X；Y)= Z p(χ)Xp(b∣χ)iog( p(χ,yb∖Idx
X	yb∈C	p(x)p(y)
=	p(x)	p(yb|x) log(2p(yb|x))dx .
X	yb∈C
(8)
(9)
(10)
According to the Hoeffding’s inequality for bounded random variables [Proposition 2.2.6, Vershynin
(2018)], let M, m denote upper and lower bounds of the integrand of (10) correspondingly, we have
P ]∣χ(χp(y∣Xk)log(2p(b∣Xk))-I(X; Y)j ≥ t} ≤ 2e-nM-m)2 .	(11)
Equivalently, with probability at least 1 - δ,
Pn=I ρb∈cp(b|xk)log(2p(b|xk))	b S log(2)(M - m)2
---------------n------------------I(X； Y) < V---------2n--------.	(12)
Here Pkn=1 Pyb∈C p(yb|xk) log(2p(yb|xk))/n is a Monte carlo estimation of RHS of I(X; Yb).
Recall that, for the binary case p(yb|x) = p(yb|F (x)) can expressed as
p(yb = 1|F (x)) = σ(wT F (x))
p(yb = -1|F (x)) = 1 - σ(wT F (x)) .	(13)
Then we have
X p(yb|F (xk)) log(2p(yb|F (xk))) = σ(wT F (x)) log(2σ(wT F (x)))
yb∈C
+ (1 - σ(wT F (x))) log(2 - 2σ(wT F (x)))) ,
which is bounded by [0, log(2)].
Take M = log(2), m = 0, we have
S log( 2 )log(2)2 j
hold with probability at least 1 - δ.
/ b	Pn=I Pb∈Cp(b|xk)log(2p(b|xk))
I(X ； Y ) =------------------------------+ O
(14)
(15)
n
The conclusion follows from the fact thatPn=ι Pb∈c p(b∣xk)log(2p(b∣xk ))/n has a unique global
minimum at wTF(xk) = 0 for each xk.
11
Under review as a conference paper at ICLR 2019
B Proof of Proposition 2.2
Consider the training samples {(xk, yk)}k=1:n, each xk is fed into a deep probabilistic model which
outputs probability densities and predicts ybk, a realization of the prediction Yb . Let C = {±1} be
the binary class and ny be the counts of observed occurrences of k satisfying yk = y ∈ C, then
n = Py∈C ny = Py ny, where we omit the range over C for convenience.
We denote the true joint probability with πyyb = p(y, yb), the marginal probabilities with πy+
Pyb πyyb and π+yb = Py πyyb. The mutual information I(Y ; Yb) can be expressed as
I(Y; Y )=I(π)=Xπyb log「
(16)
Our empirical mutual information I(πb) is defined as
I(πb) =	πbyyb log
yyb
^
πyyb
^ ^ I ,
πy+π+yb
(17)
where bbyb = 2n1y Pn= 1 σ(ybwτ F (Xi)).
Proposition 2.2 establishes a connection between I(Y; Yb) and the cross-entropy objective for the
binary case. Intuitively, increasing the confidence of the model on its correct predictions will estab-
lish a more
I(Y;Yb).
deterministic relationship between Y and Yb and thus
increase the mutual information
Proposition 2.2. I(Y; Y) is well estimated by I(πb) with high probability, which shares the
same unique (global) maximum with kn=1 logσ(ykwTF(xk)) at ykwT F (xk) → ∞ given that
yk WT F (Xk) > 1 ,for all k ∈ {1,..., n}.
Proposition 2.2 follows from Proposition B.1 and Proposition B.2, where Proposition B.1 shows
that I(Y; Y) is well approximated by I(πb) with high probability and Proposition B.2 shows the
remaining claims in Proposition 2.2.
As shown in Lemma B.1, ∏yy =+ Pn=I σ(yj'wτF(Xi)) is an unbiased estimate of ∏yy. Here σ
to represent the sigmoid function defined by σ(X) = ex/ (ex + 1). By leveraging the concentration
property of bounded variables, i.e., σ(ybwT F (Xi)), the estimation error can be bounded with high
probability (Lemma B.2).
Lemma B.1. The empirical joint probability, defined as
1 ny
πyb = ʒ—Xσ(yw F(Xi)),	(18)
2ny i=1
is an unbiased estimate of the true joint distribution πyyb.
Lemma B.2. With probability at least 1 - δ, we have
IA , ,ʌ	, , 1 Slog(δ)min{4 sUpx(WTF(X))2,1}	门a
@yb| :一 |nyb- nyb| ≤ 2y	^n	.	(19)
Proposition B.1. With probability at least 1 - δ,
I(Y; Y)=I(π) = I(Π) - O (jlog(8)min{4 SUpx(WTF(X))2,1}) .	(20)
Proof. To estimate the empirical mutual information given fixed samples, we use the approach by
Hutter & Zaffalon (2005). In particular, taylor expansion gives
I(b)=I(π) + Xlog(^nb-)∆yb + O(∆2),	(21)
yy	πy+π+yb
12
Under review as a conference paper at ICLR 2019
where ∆yyb = πbyyb-πyyb. Hence, Eq (21) together with Lemma B.2 yield, with probability exceeding
1 - ∣c∣2δ,
I(π) = I(b) - O (Iδ)mm{1 SUpx(WTF(X))2，1J .	(22)
Notice that, in the binary case the cardinality |C| = 2.
□
Next we prove the intermediate results, Lemmas B.1 and B.2.
Proof of Lemma B.1. Direct derivation on the true joint distribution πyyb gives
πyyb = p(y,yb) =	p(y, yb, x)dx =	p(y|yb, x)p(yb|x)p(x)dx
XX
=	p(y |x)p(yb|x)p(x)dx
X
=P p(x∣y)σ(ywτF(x))p(y)dx .
X
(23)
Given assumption (I) which states that the marginal density of Y is uniform over C, for every true
label y ∈ C We have p(y) = 2.
We can therefore rewrite (23) as
∏yb = 2 / σ(bwTF(x))p(x∣y)dx .
(24)
According to assumption (II), p(x|y) is a probability density over space of signal x With true label
y.
The Monte Carlo estimation of (24) gives the empirical joint probability Which is unbiased:
1 ny
byb = ʒ— X σ(bwTF(Xi)) .	(25)
2ny i=1
□
Proof of Lemma B.2. Again, by leveraging the Hoeffding’s inequality for bounded random variables
[Proposition 2.2.6 of Vershynin (2018)], We have
P I I X (σ(bwTF(Xky))- EXy [σ(bwTF(Xy))]) ≥ t > ≤ 2e ny(M-m)2 ,
IIky = I
Where Xy is the data random variable Whose true label is y .
Equivalently, With probability at least 1 - δ,
Pny=I "TF(Xky)) - EXy iF(Xy))] <1 耳字Ξm
ny	I	2ny
(26)
(27)
Where M, mare upper and loWer bounds of random variable σ(ybwT F (Xy)), respectively.
Substitute (24) and (25) into (27), With probability at least 1 - δ,
|byb - nyb| ≤
∕log(2)(M - m)2
V	2ny
(28)
1
2
13
Under review as a conference paper at ICLR 2019
To estimate the upper and lower bounds M, m for σ(ybwT F (X)), we use the Taylor’s theorem:
sup σ(ybwT F (x)) = σ(0) + sup σ0 (c)(ybwT F (x))
xx
≤ 1 + 1SUp IwTF(x)l	(29)
2	4x
and
inf σ(ybwT F (x)) = σ(0) + inf σ0(c)(ybwTF(x))
≥ 2 — 4 sup IwTF(x)I ,	(30)
given that the derivative of sigmoid function is bounded by 4.
It follows that
IM - mI = sup σ(ybwT F (x)) - inf σ(ybwT F (x))
xx
≤ 1sup IwTF(x)∣ .	(31)
Also notice that M, m are the bounds for sigmoid function, so their difference is at most 1.
From derivations above, we can rewrite (28) as
Ibyb- ∏yb∣ ≤ 2 jlθg(2EG SUPx(WTF⅛1} ,	(32)
and the lemma follows.	口
Proposition B.2. The empirical mutual information I(πb) shares the same unique (global) maximum
with Pn=IIogσ(ykWTF(Xk)) as ykWTF(Xk) → ∞ given that ykWTF(Xk) > 2, for all k ∈
{1, ..., n}.
Proof. The empirical information I(πb) is defined by
I(πb) = X πbyyb log
ij
where the empirical joint probability is given by
^
πyyb
^ ^ I ,
πy+π+yb
(33)
1 ny
bbyb = 2— Xσ(yb'wτF(Xi)).
2ny i=1
(34)
It then follows that for any y ∈ C ,
1 ny	1 ny
πby+ =砧 Xσ(wTF(Xi))+ 有 Xσ(-wTF(Xi))
1 ny
=2- X(σ(wTF(Xi)) + σ(-wTF(Xi)))
ny i=1
=L X 1 = 1 .
2ny±	2
(35)
In binary case it means that
πb1(-1)
2- bb11，
O	O
π(-1)1 = 2 - π(-1)(-1)
(36)
14
Under review as a conference paper at ICLR 2019
and the empirical mutual information can decomposed as
I(b) = b11 lοg U +1 -1b(-1)(-1)) + G - b j lοg(2 - b： +I) +
J)(-1)) log (b ； /(；)(T)— ) + b(-i)(-i) log
π	∖bll + 2 - π(-1)(-1)
πb(-1)(-1)
1 - bιι + b(-i)(-i)
+ log(2) .
(37)
We differentiate (37) with respect to πb11 and πb(-1)(-1) , and calculate the critical points over the
domain [0, 2] for both variables, which gives
1
b11 = b(-1)(-1) = 4
(38)
Observe that (38) is a global minimum over [0, 2] X [0, 1 ]. Since this is the unique critical point
where the derivative vanishes, the global maximums can only be obtained on the boundaries. In
particular, if We restrict (bιι,∏(-i)(-i)) on [4, 2] × [4, 2], (37) is a strictly increasing function over
πb11, πb(-1)(-1) and the unique global maximum is obtained at
1
b11 = π(-1)(-1) = 2
(39)
The proposition follows from the definition (34) of πbyyb that (39) is only approached when
ykwT F (xk) → ∞, for all k ∈ {1, ..., n}.
C Invertibility is B eneficial
We show in Proposition C.1 that the lower bound for the classification error is itself lower bounded
by a constant, which is attained if F is invertible. Although the performance of the model also
depends on the classifier w, our bound claims that an invertible feature map F could provide a better
environment for the classifier w to perform well. Intuitively, invertibility preserves the information
of the signal X as it flows through the neural network and reaches the classifier w; on the other hand,
w potentially performs better on the input that preserves all information of the data compared to the
one that doesn’t.
Proposition C.1. (Fano’s Inequality) The classification error is lower bounded as follows:
P(Y = Y) ≥ H(YIF(X))- log(2)
(=) ≥	iog(∣c∣ -1)
(40)
The lower bound satisfies
H(Y|F(X))- lοg(2) ≥ H(Y|X)- lοg(2)
lοg(∣c∣ - 1)	≥ lοg(∣C∣ - 1)
for all F, and the equality is attained if F is invertible.
(41)
Let Z = F (X ) and the machinery of deep learning can be decribed by the following Markov Chain:
Y → X → Z → Yb .
(42)
Lemma C.1 is a technical result that helps to prove Proposition C.1. The information Z = F(X)
has about the true labels Y is maximized when F is invertible, which is beneficial in the sense that
the key information influential for classification can be well preserved.
Lemma C.1 (Chain Rule). Given the Markov Chain assumption equation 42, we have
____ʌ, ________ _、 _______ _
I(Y; Y) ≤ I(Y; Z) ≤ I(Y; X),
and the second equality is attained if F is invertible.
(43)
1
^
2 - π
□
15
Under review as a conference paper at ICLR 2019
Proof. We will only prove the second inequality and the first inequality follows by a similar argu-
ment. Consider the decomposition
I(Y; X,Z) = Z XZ p(x, y, z) log P(X,y,z< dxdz
X y Z	p(y)p(x, z )
=/ χ/ ((χ∖v z)P(v z)loκ P(X|y，z)p(y，z) dxdz
-Jx 勺 JZp(x∖y, z)p(y,z) log p(y)p(χ∖z)p(z)
= I(Y; Z) + Z XZ p(χ∖y,z)p(y, z)log P(X∖y,z) dxdz
X y Z	P(X∖z )
= I(Y; Z) + Z XZ p(χ,y∖z)p(z)log ∕p(x,y∖z)、dxdz
X y Z	P(X∖z)P(y∖z)
= I(Y; Z) + I(X; Y∖Z) .	(44)
Similarly we obtain
I(Y; X, Z) = I(X; Y) + I(Y; Z∖X).	(45)
equation 44 together with equation 45 yields
I(Y; Z) + I(X; Y∖Z) = I(X; Y) + I(Y; Z∖X).	(46)
According to the Markov Chain setting, Y and Z are conditionally independent given X, hence
I(Y; Z∖X) = 0; in addition, the mutual information I(X; Y∖Z) is nonnegative. It follows from (46)
that
I(Y； Z) ≤ I(Y； X) .	(47)
□
Next we present a lower bound for the classification error. This lower bound is negatively related
to the mutual information I(Y; F (X)), and it attains its minimum if F is invertible. Although the
performance also depends on the classifier w, Proposition C.1 implies that an invertible feature map
F allows more chances for the classifier w to perform well.
Proof of Proposition C.1. Consider the random variable E defined as:
E=	10,,
if Y 6= Yb
otherwise .
(48)
By the Chain Rule following from similar arguments presented in Lemma C.1, we have
H(E,Y∖Yb) =H(Y∖Yb)+H(E∖Y,Yb)
H(E,Y∖Yb) = H(E∖Yb) +H(Y∖E,Yb) .	(49)
Note that H (E ∖Y, Y) = 0, since the value of E is determined given the knowledge of Y, Y . It then
follows that
H(Y∖Yb) = H(E∖Yb) +H(Y∖E,Yb)
≤ log(2) + H(Y∖E = 0, Yb)P(E = 0) + H(Y∖E = 1, Yb)P(E = 1)
= log(2) + H(Y∖E = 1, Yb)P(Y 6= Yb)
≤ log(2) + log(∖C∖ - 1)P(Y 6=Yb) .	(50)
On the other hand, Lemma C.1 shows that
. ^.	^. . ^.
H (Y) - H (Y ∖Y )= I(Y ； Y) ≤ I(Z ； Y ) = H (Y) - H (Y ∖Z) ,	(51)
16
Under review as a conference paper at ICLR 2019
which gives
H(Y|Z) ≤ H(Y|Yb) .	(52)
Substitute it into (50) yields the result
H(Y|Z) ≤ log(2) + log(|C| - 1)P(Y 6=Yb) .	(53)
As for the second statement, Lemma C.1 shows that
H(Y) -H(Y|Z) =I(Y;Z) ≤ I(Y;X) =H(Y) - H(Y |X) .	(54)
It then follows that,
H(Y |Z) = H (Y |F(X)) ≥ H(Y X) = 0 ,	(55)
where the equality is attained if F is invertible.	口
D Invertibility of ResNet
ResNet is designed to allow the model to ”learn” the identity map easily. Specifically, the input
vector x and output vector y of a building block are related by
y = L(x) +x = (L+I)(x) ,	(56)
where the operator L could be a composition of activation functions, convolutions, drop-
out(Srivastava et al. (2014)) and batch normalization(Ioffe & Szegedy (2015)). It’s shown in
Lemma D.1 below that if the operator norm |L| < 1, then L + I is theoretically guaranteed to
have an inverse, which enables information preservation among intermediate layers. In Figure 5 we
experimentally verify that |L| < 1 for all building blocks during the training process. In general,
operations such as ReLU, pooling, drop-out are not invertible(Dosovitskiy & Brox, 2015); it’s chal-
lenging to build a strictly invertible network (Jacobsen et al., 2018). From this point of view, the
beauty of ResNet lies in the fact that it’s guaranteed to be invertible regardless how L evolves during
the training process, as long as |L| < 1.
Although the usual design of ResNet does involve non-invertible components such as pooling, we
argue that ResNet still has descent invertible property compared to the majority of other neural
network designs. We also experimentally verify that our regularization does not improve a very
deep ResNet on its performance by a clear margin; we speculate that information will lose more as
it goes deeper.
Lemma D.1. Consider Holder Space C0,1 (U), where U is the closure ofsome bounded open Set U,
with equiped Holder norm |L|
α suPx∈U |L(X)I + suPx,y∈U,x=y { ।L(Xx-L(y)l }, here a is Some
positive scalar. If L ∈ C0,1(U) and ∣L∣ < 1, then there exists B such that B(I+L) = (I+L)B = I.
Proof. It's well known that C(Oj(U) is a Banach space (Lax).
Define
∞
B=X(-L)n.	(57)
n=0
Since |L| < 1, (57) is a Cauchy sequence and coverges in Banach space. Convergence sequences
can be multiplied termwise, it follows that
∞∞
BL = L X(-L)n = - X(-L)n = -(B-I) .	(58)
n=0	n=1
So B(I + L) = I. The other equality (I + L)B = I can be shown to hold in the same fashion. 口
17
Under review as a conference paper at ICLR 2019
Figure 5: We measure the operator norm of L in each building block of ResNet-32 over 80k training
steps. It can be observed that, the operator norms are all bounded by 1 throughout the training
process, which verifies the hypothesis made in Appendix D. We conlude that ResNet is invertible.
E Implementation Details
We implement all models using Tensorflow. We modify the ResNet based on the code provided
at https://github.com/tensorflow/models/tree/master/research/resnet.
We follow the same learning rate scheme proposed in the original code. For the InvNet on MNIST,
we train the network with initial learning rate 0.1, and decay it by 0.7 every 10 epochs. For both
InvNet and ResNet, we apply the `1 norm regularization every 30 iterations.
18
Under review as a conference paper at ICLR 2019
F Results for All Digits on InvNet
40
60
1^- feature mean
----weight
+ product of feature and weight
Figure 6: A reproduction of the feature statistical results of InvNet on MNIST for all digits
---feature mean
—weight
+ product of feature and weight
This is a reproduction of the feature statistical results of InvNet on MNIST for all digits for Figure 2.
19
Under review as a conference paper at ICLR 2019
We observe that each digits have their specific entries with high value assigned to both weights
and feature means. An additional observation is that typically the feature entry with low mean
also has low standard deviation, such entry rarely contributes to the logits for classification if the
corresponding value of weight is also small.
G Results on i-RevNet
We reproduce the results in Section 4.2 on i-RevNet (Jacobsen et al. (2018)). The statistics and
ROC curves of the features produced by i-RevNet and Reg-i-RevNet on CIFAR10 is similar to those
presented in Section 4.2.
Figure 7: Reproduction of Experimental Results in Section 4.2 on i-RevNet. The plots in the first
row are for i-RevNet and the plots in the second row are for Reg-i-RevNet.
H Histogram on Features of Digit 9
Figure 8: The histogram for values of feature entries of Digit 9
20
Under review as a conference paper at ICLR 2019
The histogram for values of feature entries of Digit 9 has a decaying shape but with a heavier tail
compared to that of Gaussian with small variance. The spasity of w depends on our choice of
hyperparameters. For example in Figure 4 we measure the sparsity of the learned w of RegResNet
for CIFAR10: about 60% entries of w are precisely zero.
I The Use of S urro gate Functions to Minimize Mutual
Information
Our objective is
I(X,Yb) =H(Yb)-H(Yb|X) .
(59)
Assume Y is uniformly distributed on C, H(Y ) becomes a constant and we have
-H (Y |X) = - fχ X p(χ, y) log PpxxF
= -	p(x)	p(yb|x) log p(yb|x)
X	yb∈C
≈ -	p(yb|x) log p(yb|x)
x∈Bat yb∈C
(60)
Note that (60) is composed of functions in the form a log a where a is the output of a softmax
function on logits. For simplicity we consider the binary case where a = 1 + e-wT F (x) -1. The
derivative of it with respect to w takes the form
e-wT F (X)F (x)
(1 + e-wTF(X))2 .
(61)
Assume |wT F (x)| is large, if wT F(x) > 0 then the numerator decays exponentially with respect
to |wT F (x)| and the denominator converges to 1; if wF(x) < 0 then the denonimator grows expo-
nentially with respect to |wT F (x)| and dominates the numerator. To conclude, for large |wT F (x)|
the gradient is decaying to zero exponentially. So the effect of punishing large logits |wT F (x)| by
this objective is not clear as the gradient vanishes for large |wT F (x)|. We also analyzed the general
softmax functions for multi-labels and found they exhibit similar properties.
We propose to use a surrogate function wT F(X) to minimize this objective to make the regulariza-
tion effect more clear. We prove that our regularizer achieves the same goal compared to the mutual
information objective but provides a better gradient for training.
21