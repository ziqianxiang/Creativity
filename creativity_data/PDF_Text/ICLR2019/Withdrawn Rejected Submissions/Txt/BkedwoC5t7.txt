Under review as a conference paper at ICLR 2019
Formal Limitations on the Measurement of
Mutual Information
Anonymous authors
Paper under double-blind review
Ab stract
Motivated by applications to unsupervised learning, we consider the problem of
measuring mutual information. Recent analysis has shown that naive kNN estima-
tors of mutual information have serious statistical limitations motivating more re-
fined methods. In this paper we prove that serious statistical limitations are inher-
ent to any measurement method. More specifically, we show that any distribution-
free high-confidence lower bound on mutual information cannot be larger than
O(ln N) where N is the size of the data sample. We also analyze the Donsker-
Varadhan lower bound on KL divergence in particular and show that, when sim-
ple statistical considerations are taken into account, this bound can never pro-
duce a high-confidence value larger than ln N . While large high-confidence lower
bounds are impossible, in practice one can use estimators without formal guaran-
tees. We suggest expressing mutual information as a difference of entropies and
using cross entropy as an entropy estimator. We observe that, although cross en-
tropy is only an upper bound on entropy, cross-entropy estimates converge to the
true cross entropy at the rate of 1/√N.
1	Introduction
Motivated by maximal mutual information (MMI) predictive coding (McAllester, 2018; Stratos,
2018; Oord et al., 2018), we consider the problem of measuring mutual information. A classical
approach to this problem is based on estimating entropies by computing the average log of the
distance to the kth nearest neighbor in a sample (Kraskov et al., 2003). It has recently been shown
that the classical kNN methods have serious statistical limitations and more refined kNN methods
have been proposed (Gao et al., 2014). Here we establish serious statistical limitations on any
method of estimating mutual information. More specifically, we show that any distribution-free
high-confidence lower bound on mutual information cannot be larger than O(lnN) where N is the
size of the data sample.
Prior to proving the general case, we consider the particular case of the Donsker-Varadhan lower
bound on KL divergence (Donsker & Varadhan, 1983; Belghazi et al., 2018). We observe that
when simple statistical considerations are taken into account, this bound can never produce a high-
confidence value larger than lnN. Similar comments apply to lower bounds based on contrastive
estimation. The contrastive estimation lower bound given in Oord et al. (2018) does not establish
mutual information of more than ln k where k is number of negative samples used in the contrastive
choice.
The difficulties arise in cases where the mutual information I(x, y) is large. Since I(x, y) =
H(y) - H(y|x) we are interested in cases where H(y) is large and H(y|x) is small. For example
consider the mutual information between an English sentence and its French translation. Sampling
English and French independently will (almost) never yield two sentences where one is a plausi-
ble translation of the other. In this case the DV bound is meaningless and contrastive estimation is
trivial. In this example we need a language model for estimating H(y) and a translation model for
estimating H(y|x). Language models and translation models are both typically trained with cross-
entropy loss. Cross-entropy loss can be used as an (upper bound) estimate of entropy and we get an
estimate of mutual information as a difference of cross-entropy estimates. Note that the upper-bound
guarantee for the cross-entropy estimator yields neither an upper bound nor a lower bound guarantee
1
Under review as a conference paper at ICLR 2019
for a difference of entropies. Similar observations apply to measuring the mutual information for
pairs of nearby frames of video or pairs of sound waves for utterances of the same sentence.
We are motivated by the problem of maximum mutual information predictive coding (McAllester,
2018; Stratos, 2018; Oord et al., 2018). One can formally define a version of MMI predictive coding
by considering a population distribution on pairs (x, y) where we think of x as past raw sensory sig-
nals (images or sound waves) and y as a future sensory signal. We consider the problem of learning
stochastic coding functions Cx and Cy so as to maximize the mutual information I(Cx (x), Cy (y))
while limiting the entropies H(Cx(x)) and H (Cy (y)). The intuition is that we want to learn repre-
sentations Cx (x) and Cy (y) that preserve “signal” while removing “noise”. Here signal is simply
defined to be a low entropy representation that preserves mutual information with the future. Forms
of MMI predictive coding have been independently introduced in (McAllester, 2018) under the name
“information-theoretic cotraining” and in (Oord et al., 2018) under the name “contrastive predictive
coding”. It is also possible to interpret the local version of DIM (DIM(L)) (Hjelm et al., 2018) as a
variant of MMI predictive coding.
A closely related framework is the information bottleneck (Tishby et al., 2000). Here one again
assumes a population distribution on pairs (x, y). The objective is to learn a stochastic coding
function Cx so as to maximize I (Cx (x), y) while minimizing I (Cx (x), x). Here one does not ask
for a coding function on y and one does not limit H(Cx (x)).
Another related framework is INFOMAX (Linsker, 1988; Bell & Sejnowski, 1995; Hjelm et al.,
2018). Here we consider a population distribution on a single random variable x. The objective is to
learn a stochastic coding function Cx so as to maximize the mutual information I(x, Cx (x)) subject
to some constraint or additional objective.
As mentioned above, in cases where I(Cx(x), Cy(y)) is large it seems best to train a model of the
marginal distribution of P(Cy) and a model of the conditional distribution P(Cy|Cx) where both
models are trained with cross-entropy loss. Section 5 gives various high confidence upper bounds
on cross-entropy loss for learned models. The main point is that, unlike lower bounds on entropy,
high-confidence upper bounds on cross-entropy loss can be guaranteed to be close to the true cross
entropy.
Out theoretical analyses will assume discrete distributions. However, there is no loss of generality in
this assumption. Rigorous treatments of probability (measure theory) treat integrals (either Riemann
or Lebesgue) as limits of increasingly fine binnings. A continuous density can always be viewed as
a limit of discrete distributions. Although our proofs are given for discrete case, all our formal
limitations on the measurement of mutual information apply to continuous case as well. See Marsh
(2013) for a discussion of continuous information theory. Additional comments on this point are
given in section 4.
2	The Donsker-Varadhan Lower Bound
Mutual information can be written as a KL divergence.
I(X,Y) =KL(PX,Y,PXPY)
Here PX,Y is a joint distribution on the random variables X and Y and PX and PY are the marginal
distributions on X and Y respectively. The DV lower bound applies to KL-divergence generally. To
derive the DV bound we start with the following observation for any distributions P , Q, and G on
the same support. Our theoretical analyses will assume discrete distributions.
KL(P,Q) = Ez 〜P
ln Q(z)
Ez〜P
ln Gz) + KL(PG
≥	Ez 〜P
ln
G(Z)
Q(Z)
(1)
2
Under review as a conference paper at ICLR 2019
Note that (1) achieves equality for G(z) = P(z) and hence we have
KL(P,Q)=SUP Ez∈P ln / [	⑵
G	Q(z)
Here we can let G be a parameterized model such that G(z) can be computed directly. However, we
are interested in KL(PX,Y , PXPY ) where our only access to the distribution P is through sampling.
If we draw a pair (x, y) and ignore y we get a sample from PX . We can similarly sample from PY .
So we are interested in a KL-divergence KL(P, Q) where our only access to the distributions P and
Q is through sampling. Note that we cannot evaluate (1) by sampling from P because we have no
way of computing Q(z). But through a change of variables we can convert this to an expression
restricted to sampling from Q. More specifically we define G(z) in terms of an unconstrained
function F (z) as
G(Z) = Z Q(z)eF(Z)	Z = X Q(z)eF(Z)= Ez~Q eF(Z)	(3)
z
Substituting (3) into (2) gives
KL(P, Q) = SUP Ez~p F(Z)- ln Ez~q eF(Z)	(4)
F
Equation (4) is the Donsker-Varadhan lower bound. Applying this to mutual information we get
I(X,Y)	= KL(PX,Y,PXPY)
= sup Eχ,y~Pχγ F(x,y) - ln Eχ~Pχ,y~Pγ eF(x,y)	(5)
F,
This is the equation underlying the MINE approach to maximizing mutual information (Belghazi
et al., 2018). It would seem that we can estimate both terms in (5) through sampling and be able to
maximize I(X, Y ) by stochastic gradient ascent on this lower bound.
3	Statistical Limitations of KL-Divergence Lower B ounds
In this section we show that the DV bound (4) cannot be used to measure KL-divergences of more
than tens of bits. In fact we will show that no high-confidence distribution-free lower bound on KL
divergence can be used for this purpose.
As a first observation note that (4) involves Ez~q eF(Z). This expression has the same form as
the moment generating function used in analyzing large deviation probabilities. The utility of ex-
pectations of exponentials in large deviation theory is that such expressions can be dominated by
extremely rare events (large deviations). The rare events dominating the expectation will never
be observed by sampling from Q. It should be noted that the optimal value for F(Z) in (4) is
ln(P (Z)/Q(Z)) in which case the right hand side of (4) simplifies to KL(P, Q). But for large KL
divergence we will have that F(Z) = ln(P(Z)/Q(Z)) is typically hundreds of bits and this is exactly
the case where Ez~q eF(Z) cannot be measured by sampling from Q. If Ez~q eF(Z) is dominated
by events that will never occur in sampling from Q then the optimization of F through the use of
(4) and sampling from Q cannot possibly lead to a function F(Z) that accurately models the desired
function ln(P(Z)/Q(Z)).
To quantitatively analyze the risk of unseen outlier events we will make use of the following simple
lemma where We write Pz~q(Φ[z]) for the probability over drawing Z from Q that the statement
Φ[Z] holds.
Outlier Risk Lemma: For a sample S 〜QN with N ≥ 2, and a property Φ[z] such that
Pz~q (Φ[z]) ≤ 1/N, the probability over the draw of S that no Z ∈ S satisfies Φ[z] is at least
1/4.
3
Under review as a conference paper at ICLR 2019
Proof: The probability that Φ[z] is unseen in the sample is at least (1 - 1/N)N which is at least 1/4
for N ≥ 2 and where we have limN→∞ (1 - 1/N)N = 1/e. Q.E.D.
We can use the outlier risk lemma to perform a quantitative risk analysis of the DV bound (4). We
can rewrite (4) as
KL(P,Q) ≥ B(P,Q,F)
B(P,Q,F) = Ez〜P F(Z)- lnEz〜Q eF(Z)
We can try to estimate B(P, Q, G) from samples SP and SQ, each of size N, from the population
distributions P and Q respectively.
B(Sp,Sq,F ) = NN X F (Z)- ln N X eF (Z)
z∈SP	z∈SQ
-J-VTl ∙1 7^^>∕7-⅝∙OT^I∖*	1	F	1	T7T∕7-⅝∙O∖,1	1	. ∙	. A，C Γ^1	7-ɪ ∖ ∙	. rɪ-i
While B(P, Q, F) is a lower bound on KL(P, Q), the sample estimate B(SP, SQ, F) is not. To
get a high confidence lower bound on KL(P, Q) we have to handle unseen outlier risk. For a fair
comparison with our analysis of cross-entropy estimators in section 5, we will limit the outlier risk
FF	f T-l / ∖ . .) ∙	1 l^C L 1 EI 1	.	∙1 1	1	Γ∙ -f∖ / Γ^1 C 7-ɪ ∖	1
by bounding F(Z) to the interval [0, Fmax]. The largest possible value of B(SP, Sq, F) occurs when
F(Z) = Fmax for all Z ∈ SP and F(Z) = 0 for all Z ∈ SQ. In this case we get B(SP, SQ, F) =
Fmax . But by the outlier risk lemma there is still at least a 1/4 probability that
Ez〜Q eF(Z) ≥ NeFmax.	⑹
Any high confidence lower bound B(SP, SQ, F) must account for the unseen outlier risk. In partic-
ular we must have
eFm ax
B(Sp,Sq,F )	≤ Fmax - ln -N-
= lnN
Our negative results can be strengthened by considering the preliminary bound (1) where G(Z) is
viewed as a model ofP(Z). We can consider the extreme case of perfect modeling of the population
P with a model G(Z) where G(Z) is computable. In this case we have essentially complete access
to the distribution P. But even in this setting we have the following negative result.
Theorem 1 Let B be any distribution-free high-confidence lower bound on KL(P,Q) computed with
complete knowledge of P but only a sample from Q.
More specifically, let B(P, S, δ) be any real-valued function of a distribution P, a multiset S, and a
confidence parameter δ such that, for any P, Q and δ, with probability at least (1- δ) over a draw
of S from QN we have
KL(P,Q) ≥ B(P, S, δ).
For any such bound, and for N ≥ 2, with probability at least 1- 4δ over the draw of S from QN
we have
B(P, S, δ) ≤ lnN.
Proof. Consider distributions P and Q and N ≥ 2. Define Q by
Q(Z)=(I- N)Q(Z)+NP(Z).
We now have KL(P, Q) ≤ ln N. We will prove that from a sample S 〜 QN We cannot reliably
1∙	. ∙	∙ 1	1	ʌʌ IK
distinguish between Q and Q.
We first note that by applying the high-confidence guarantee of the bound to Q have
, , , , ~ ..
PS〜QN (B(P, S, δ) ≤ KL(P, Q)) ≥ 1 - δ.
The distribution Q equals the marginal on Z of a distribution on pairs (s, Z) where s is the value of
Bernoulli variable with bias 1/N such that if s = 1 then Z is drawn from P and otherwise Z is drawn
4
Under review as a conference paper at ICLR 2019
from Q. By the outlier risk lemma the probability that all coins are zero is at least 1/4. Conditioned
on all coins being zero the distributions QN and QN are the same. Let Pure(S) represent the event
that all coins are 0 and let Small(S) represent the event that B(P, S, δ) ≤ lnN. We now have
PS〜QN (Small(S)) = PS〜QN (Small(S)IPure(S))
PS〜QN (Pure(S) ∧ Small(S))
=	PS〜QN (PUre(S))
〉PS〜QN (Pure(S)) - PS〜QN (-Small(S))
≥	PS 〜QN (PUre(S))
≥ PS〜QN (PUre(S)) — δ
—PS〜QN(PUre(S))
=1------------,δ , 、、
PS〜QN (PUre(S))
≥ 1 - 4δ.
4 Statistical Limitations on Entropy Lower Bounds
Mutual information is a special case of KL-divergence. It is possible that tighter lower bounds can
be given in this special case. In this section we show similar limitations on lower bounding mutual
information. We first note that a lower bound on mutual information implies a lower bound on
entropy. The mutual information between X and Y cannot be larger than information content of X
alone.
I(X,Y) =H(X)-H(XIY) ≤ H(X)
So a lower bound on I(X, Y ) gives a lower bound on H(X). We show that any distribution-free
high-confidence lower bound on entropy requires a sample size exponential in the size of the bound.
The above argument seems problematic for the case of continuous densities as differential entropy
can be negative. However, for the continuous case we have
I(x, y) = sUp I(Cx(x),Cy(y))
Cx,Cy
where Cx and Cy range over all maps from the underlying continuous space to discrete sets (all
binnings of the continuous space). Hence an O(ln N) upper bound on the measurement of mutual
information for the discrete case applies to the continuous case as well.
The type of a sample S, denoted T(S), is defined to be a function on positive integers (counts)
where T(S)(i) is the number of elements of S that occur i times in S. For a sample of N draws
we have N = Pi iT(S)(i). The type T(S) contains all information relevant to estimating the
actual probability of the items of a given count and of estimating the entropy of the underlying
distribution. The problem of estimating distributions and entropies from sample types has been
investigated by various authors (McAllester & Schapire, 2000; Orlitsky et al., 2003; Orlitsky &
Suresh, 2015; Arora et al., 2018). Here we give the following negative result on lower bounding the
entropy of a distribution by sampling.
Theorem 2 Let B be any distribution-free high-confidence lower bound on H(P) computed from a
sample type T (S) with S 〜P N.
More specifically, let B(T, δ) be any real-valued function of a type T and a confidence parameter
δ such that for any P, with probability at least (1 - δ) over a draw of S from PN, we have
H(P) ≥ B(T(S),δ).
For any such bound, and for N ≥ 50 and k ≥ 2, with probability at least 1 - δ - 1.01/k over the
draw of S from PN we have
B(T(S), δ) ≤ ln2kN2.
5
Under review as a conference paper at ICLR 2019
Proof: Consider a distribution P and N ≥ 100. If the support of P has fewer than 2kN2 elements
then H(P) < ln 2kN2 and by the premise of the theorem we have that, with probability at least
1 - δ over the draw of S, B(T(S), δ) ≤ H(P) and the theorem follows. If the support of P has at
least 2kN2 elements then we sort the support ofP into a (possibly infinite) sequence x1, x2, x3, . . .
so that P(xi) ≥ P(xi+1). We then define a distribution P on the elements x1, . . . , x2kN2 by
/ P(Xi)	for i ≤ kN2	∖
P(Xi) =
∖ P(i>kN2)	for kN2 < i ≤ 2kN2 )
kN2
We will let Small(S) denote the event that B(T (S), δ) ≤ ln 2kN2 and let Pure(S) abbreviate the
event that no element Xi for i > kN2 occurs twice in the sample. Since P has a support of size
rʌ 7 ~xtγ) 1	ττ-∕τ~⅛∖	1 rʌ 7 ~xτr) * i	.t	♦ r∙ .t ι	. Γ∖
2kN2 we have H(P) ≤ ln 2kN2. Applying the premise of the lemma to P gives
Ps~Pn(Small(S)) ≥ 1 - δ	(7)
For a type T let P⅛~pN (T) denote the probability over drawing S 〜 PNthat T(S) = T. We now
have
Ps~p N (TIPure(S))= P5~pN (TIPure(S)).
This gives the following.
Ps-pN (Small(S)) ≥ P5~pN (Pure(S) ∧ Small(S))
= Ps~pN (Pure(S)) Ps~pN (Small(S) ∣ Pure(S))
= Ps~pN(Pure(S)) Ps~pn(Small(S) ∣ Pure(S))
≥ Ps~pN (Pure(S)) P5~pN (Pure(S) ∧ Small(S))	(8)
For i > kN2 We have P(Xi) ≤ 1/(kN2) which gives
N-1	j
PS~PN (Pure(S)) ≥ ∏ (1- kN2 )
j=1
Using (1 - P) ≥ e-1.01 P for P ≤ 1/100 we have the following birthday paradox calculation.
ln Ps~Pn (Pure(S))	≥
1.01
kN2
N-1
Xj
j=1
1.01 (N - 1)N
kN2	2
≥
Ps~Pn (Pure(S)) ≥
-.505/k
e-.505/k ≥ 1 - .505/k
(9)
Applying the union bound to (7) and (9) gives.
Ps~Pn (Pure(S) ∧ Small(S)) ≥ 1 — δ — .505/k	(10)
By a derivation similar to that of (9) we get
Ps~pn(Pure(S)) ≥ 1 — .505/k	(11)
Combining (8), (10) and (11) gives
Ps-pN (Small(S)) ≥ 1 - δ - 1.01/k
6
Under review as a conference paper at ICLR 2019
5 Cross Entropy as an Entropy Estimator
Since mutual information can be expressed as a difference of entropies, the problem of measuring
mutual information can be reduced to the problem of measuring entropies. In this section we show
that, unlike high-confidence distribution-free lower bounds, high-confidence distribution-free upper
bounds on entropy can approach the true cross entropy at modest sample sizes even when the true
cross entropy is large. More specifically we consider the cross-entropy upper bound.
H(P)
Ex~Pln P1X)
Ex〜P ln (G(X) P(X)
H(P,G)- K L(P, G)
≤	H (P, G)
For G = P we get H(P, G) = H(P) and hence we have
H(P)=inGf H(P,G)
In practice Pis a population distribution andGis model ofP. For example P might be a population
distribution on paragraphs and G might be an autoregressive RNN language model. In practice G
will be given by a network with parameters Φ. In this setting we have the following upper bound
entropy estimator.
ʌ , .
H(P) = inf H(P,Gφ)
Φ
(12)
The gap between H(P) and H(P) depends on the expressive power of the model class.
The statistical limitations on distribution-free high-confidence lower bounds on entropy do not arise
for cross-entropy upper bounds. For upper bounds we can show that naive sample estimates of
the cross-entropy loss produce meaningful (large entropy) results. We first define the cross-entropy
estimator from a sample S.
1
H(SG = |S| E Tn G(X)
x∈S
We can bound the loss of a model G by ensuring a minimum probability e-Fmax where Fmax is then
the maximum possible log loss in the cross-entropy objective. In language modeling a loss bound
exists for any model that ultimately backs off to a uniform distribution on characters. Given a loss
bound of Fmax we have that H(S, G) is just the standard sample mean estimator of an expectation
of a bounded variable. In this case we have the following standard confidence interval.
Theorem 3 For any population distribution P, and model distribution G with -ln G(x) bounded
to the interval [0, Fmax], with probability at least 1 一 δ over the draw of S 〜P N we have
ʌ/u
ʌ ,
H (P, G) ∈ H(S,G) ± FmaX
It is also possible to give PAC-Bayesian bounds on H(P, GΦ) that take into account the fact that
GΦ is typically trained so as to minimize the empirical loss on the training data. The PAC-Bayesian
bounds apply to“broad basin” losses and loss estimates such as the following.
Hσ(S,GΦ)
ʌ .
Hσ (S,Gφ)
Ex〜P Ee〜N(0,σI) 一 ln GΦ+e (X)
回 ^X Ee〜N(0,σI) 一 ln GΦ+e (X)
x∈S
Under mild smoothness conditions on GΦ(X) as a function of Φ we have
7
Under review as a conference paper at ICLR 2019
lim Hσ (P, GΦ) = H(P, GΦ)
σ→0
ʌ ʌ , .
lim Hσ(S,Gφ)	= H(S,Gφ)
σ→0
An L2 PAC-Bayesian generalization bound (McAllester (2013)) gives that for any parameterized
class of models and any bounded notion of loss, and any λ > 1/2 and σ > 0, with probability at
least 1 - δ over the draw of S from PN we have the following simultaneously for all parameter
vectors Φ.
Hσ(P,Gφ) ≤ -ɪɪ
1 - 2λ
l∣φ∣l2
2σ2
+ ln1
δ
It is instructive to set λ = 5 in which case the bound becomes.
Hσ (P, GΦ) ≤ 10 (Hσ (S, GΦ) + 5Fmax (K +ln 1))
While this bound is linear in 1/N, and tighter in practice than square root bounds, note that there is
a small residual gap when holding λ fixed at 5 while taking N → ∞. In practice the regularization
parameter λ can be tuned on holdout data. One point worth noting is the form of the dependence of
the regularization coefficient on Fmax, N and the basin parameter σ .
It is also worth noting that the bound can be given in terms of “distance traveled” in parameter space
from an initial (random) parameter setting Φ0 .
Hσ(P,Gφ) ≤ 10 (Hσ(S, Gφ) + 5Fmax (JJl-孚E +ln 1))
9	N	2σ2	δ
Evidence is presented in Dziugaite & Roy (2017) that the distance traveled bounds are tighter in
practice than traditional L2 generalization bounds.
6	MMI predictive Coding
Recall that in MMI predictive coding we assume a population distribution on pairs (x, y) where we
think of x as past raw sensory signals (images or sound waves) and y as a future sensory signal. We
then consider the problem of learning stochastic coding functions Cx and Cy that maximizes the
mutual information I(Cx(x), Cy(y)) while limiting the entropies H(Cx(x)) and H (Cy (y)). Here
we propose representing the mutual information as a difference of entropies.
I(Cx(x), Cy(y)) = H(Cy(y)) - H(Cy(y)|Cx(x))
When the coding functions are parameterized by a function Ψ, the above quantities become a func-
tion of Ψ. We can then formulate the following nested optimization problem.
ʌ , , ʌ , .. , .
Ψ = argmax H(Cy(y); Ψ) - H(Cy(y)|Cx(x);W)
Ψ
H(Cy(y); Ψ) = inf H(Cy(y),G&;W)
Θ
H(Cy(y)∣Cχ(x); Ψ) = inf H(Cy(y),Gφ∣Cχ(x); Ψ)
The above quantities are expectations over the population distribution on pairs (x, y). In practice
we have only a finite sample form the population. But the preceding section presents theoretical ev-
idence that, unlike lower bound estimators, upper bound cross-entropy estimators can meaningfully
estimate large entropies from feasible samples.
8
Under review as a conference paper at ICLR 2019
7	Conclusions
Maximum mutual information (MMI) predictive coding seems well motivated as a method of unsu-
pervised pretraining of representations that maintain semantic signal while dropping uninformative
noise. However, the maximization of mutual information is a difficult training objective. We have
given theoretical arguments that representing mutual information as a difference of entropies, and es-
timating those entropies by minimizing cross-entropy loss, is a more statistically justified approach
than maximizing a lower bound on mutual information.
Unfortunately cross-entropy upper bounds on entropy fail to provide either upper or lower bounds on
mutual information — mutual information is a difference of entropies. We cannot rule out the possi-
ble existence of superintelligent models, models beyond current expressive power, that dramatically
reduce cross-entropy loss. Lower bounds on entropy can be viewed as proofs of the non-existence
of superintelligence. We should not surprised that such proofs are infeasible.
References
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and
empirics. ICLR, 2018.
Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. Mine:
mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.
Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separa-
tion and blind deconvolution. Neural computation, 7(6):1129-1159, 1995.
M. Donsker and S. Varadhan. Asymptotic evaluation of certain markov process expectations for
large time, iv. Communications on Pure and Applied Mathematics, 36(2):183-212, 1983.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Efficient estimation of mutual information for
strongly dependent variables. arXiv preprint arXiv:1411.2003, 2014.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and
Yoshua Bengio. Learning deep representations by mutual information estimation and maximiza-
tion. arXiv preprint arXiv:1808.06670, 2018.
Alexander Kraskov, Harald Stoegbaue, and Peter Grassberger. Estimating mutual information. arXiv
preprint arXiv:cond-mat/0305641, 2003.
Ralph Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988.
Charles Marsh. Introduction to continuous entropy. www.crmarsh.com/static/pdf/Charles_Marsh_Continuous_Entropy.pdf,
2013.
David McAllester. A pac-bayesian tutorial with a dropout bound. arXiv preprint arXiv:1307:2118,
2013.
David McAllester. Information Theoretic Co-Training. arXiv preprint arXiv:1802.07572, 2018.
David McAllester and Robert Schapire. On the convergence rate of good-turing estimators. COLT,
2000.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Alon Orlitsky and Ananda Theertha Suresh. Competitive distribution estimation: Why is good-
turing good. NIPS, 2015.
Alon Orlitsky, Narayana Santhanam, and Junan Zhang1. Always good turing: Asymptotically opti-
mal probability estimation. Science, 302(5644), 2003.
9
Under review as a conference paper at ICLR 2019
Karl Stratos. Mutual information maximization for simple and accurate part-of-speech induction.
arXiv preprint arXiv:1804.07849, 2018.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
10