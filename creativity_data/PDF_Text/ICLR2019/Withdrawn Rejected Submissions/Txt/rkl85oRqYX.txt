Under review as a conference paper at ICLR 2019
Exploiting Invariant Structures for Compres-
sion in Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Modern neural networks often require deep compositions of high-dimensional
nonlinear functions (wide architecture) to achieve high test accuracy, and thus
can have overwhelming number of parameters. Repeated high cost in prediction
at test-time makes neural networks ill-suited for devices with constrained memory
or computational power. We introduce an efficient mechanism, reshaped tensor
decomposition, to compress neural networks by exploiting three types of invariant
structures: periodicity, modulation and low rank. Our reshaped tensor decomposi-
tion method exploits such invariance structures using a technique called tensoriza-
tion (reshaping the layers into higher-order tensors) combined with higher order
tensor decompositions on top of the tensorized layers. Our compression method
improves low rank approximation methods and can be incorporated to (is com-
plementary to) most of the existing compression methods for neural networks to
achieve better compression. Experiments on LeNet-5 (MNIST), ResNet-32 (CI-
FAR10) and ResNet-50 (ImageNet) demonstrate that our reshaped tensor decom-
position outperforms (5% test accuracy improvement universally on CIFAR10)
the state-of-the-art low-rank approximation techniques under same compression
rate, besides achieving orders of magnitude faster convergence rates.
1 Introduction
Modern neural networks achieve unprecedented accuracy over many difficult learning problems at
the cost of deeper and wider architectures with overwhelming number of model parameters. The
large number of model parameters causes repeated high cost in test-time as predictions require load-
ing the network into the memory and repeatedly passing the unseen examples through the large
network. Therefore, the model size becomes a practical bottleneck when neural networks are de-
ployed on constrained devices, such as smartphones and IoT cameras.
Compressing a successful large network (i.e., reducing the number of parameters), while maintain-
ing its performance, is non-trivial. Many approaches have been employed, including pruning, quan-
tization, encoding and knowledge distillation (see appendix A for a detailed survey). A complemen-
tary compression technique, on top of which the aforementioned approaches can be used, is low rank
approximation. For instance, singular value decomposition (SVD) can be performed on fully con-
nected layers (weights matrices) and tensor decomposition on convolutional layers (convolutional
kernels). Low rank approximation methods can work well and reduce the number of parameters by
a factor polynomial in the dimension only when the weight matrices or convolutional kernels have
low rank structures, which might not always hold in practice.
We propose to exploit additional invariant structures in the neural network for compression. A
set of experiments on several benchmark datasets justified our conjecture (Section 4): large neural
networks have some invariant structures, namely periodicity, modulation and low rank, which make
part of the parameters redundant. Consider this toy example of a vector with periodic structure
[1,2,3,1,2,3,1,2,3] or modulated structure [1,1,1,2,2,2,3,3,3] in Figure 1. The number of parameters
needed to represent this vector, naively, is 9. However if we map or reshape the vector into a higher
order object, for instance, a matrix [1,1,1;2,2,2;3,3,3] where the columns of the matrix are repeated,
then apparently this reshaped matrix can be decomposed into rank one without losing information.
Therefore only 6 parameters are needed to represent the original length-9 vector.
1
Under review as a conference paper at ICLR 2019
Invariant structures <
[1, 2, 3, 1, 2, 3, 1, 2, 3]
Periodic structure
[1, 1, 1, 2, 2, 2, 3, 3, 3]
Modulated structure
Component
[tl
[1, 1, U
Figure 1: A toy example of invariant structures. The periodic and modulated structures are picked
out by exploiting the low rank structure in the reshaped matrix.
Although the invariant structures in large neural networks allow compression of redundant param-
eters, designing a sophisticated way of storing a minimal representation of the parameters (while
maintaining the expressive power of the network) is nontrivial. To solve this problem, we proposed
a new framework called reshaped tensor decomposition (RTD) which has three phases:
1.	Tensorization. We reshape the neural network layers into higher-order tensors.
• For instance, consider a special square tensor convolutional kernel T ∈ RD ×D ×D ×D , we re-
m
shape T into a higher m-order tensor T' ∈ RD' × ∙ × D, so called tensorization. Tensorization
guarantees that the reshaped tensor has a smaller dimension, i.e., D < D ( as Dm = D4 and
m > 4). Similar ideas apply to general convolutional kernels.
2.	Higher-order tensor decomposition. We deploy tensor decomposition (a low rank approxima-
tion technique detailed in section 3) on the tensorized layers to exploit the periodic, modulated
as well as low rank structures in the original layers.
•	A rank-R tensor decomposition of the above 4-order tensor T will result in R number of
components (each contains 4D parameters), and thus 4DR number of parameters in total —
smaller than the original D4 number of parameters if R is small.
•	A rank-R tensor decomposition of the above reshaped m-order kernel tensor T' maps the layer
into m + 1 narrower layers. The decomposition will result in R number of components with
mDm parameters and thus mDmm R in total — better than the 4DR number of parameters
required by doing tensor decomposition on the original tensor T (D is usually large).
Now the weights of the tensorized neural networks are the components of the tensor, i.e., result of
the tensor decomposition. However, decomposing higher order tensors is challenging and known
methods are not guaranteed to converge to the minimum error decomposition (Hillar & Lim,
2013). Therefore fine tuning is needed to achieve high performance.
3.	Data reconstruction-based sequential tuning. We fine-tune the parameters using a data
reconstruction-based sequential tuning (Seq) method which minimizes the difference between
training output of the uncompressed and compressed, layer by layer. Our Seq tuning is a novel
approach inspired by a sequential training method proved to converge faster and achieve guar-
anteed accuracy using a boosting framework (Huang et al., 2017). Unlike traditional end-to-end
(E2E) backpropagation through the entire network, Seq tunes individual compressed “blocks”
one at a time, reducing the memory and complexity required during compression.
Summary of Contributions
•	Novel compression schemes. We propose new reshaped tensor decomposition methods to ex-
ploit invariant structures for compressing the parameters in neural networks. By first tensorizing
the kernel/weights into a higher-order tensor, our reshaped tensor decomposition discovers extra
invariant structures and therefore outperform existing “low rank approximation methods”.
•	Efficient computational framework. We introduce a system of tensor algebra that enables
efficient training and inference for our compressed models. We show that a tensor decomposition
on the parameters is equivalent to transforming one layer into multiple narrower sublayers in the
compressed model. Therefore, other compression techniques (e.g. pruning) can be applied on top
of our method by further compressing the sublayers returned by our method.
•	Sequential knowledge distillation. We introduce Seq tuning to transfer knowledge to a com-
pressed network from its uncompressed counterpart by minimizing the data reconstruction error
block by block. With our strategy, only one block of the network is loaded into the GPU “at each
2
Under review as a conference paper at ICLR 2019
time”, therefore allowing compression of large networks on moderate devices. Furthermore, we
show empirically that our strategy converges much faster than normal end to end tuning.
•	Comprehensive experiments. We perform extensive experiments to demonstrate that our re-
shaped tensor decomposition outperforms state-of-the-art low-rank approximation techniques (ob-
tains 5% higher accuracy on CIFAR10 under same compression rates). Our experiments also show
that our method scales to deep residual neural networks on large benchmark dataset, ImageNet.
Organization of the paper Section 2 introduces tensor operations, tensor decompositions and
their representations in tensor diagrams. In Section 3, we introduce convolutional layer diagram,
review existing low-rank approximation techniques, and propose three new schemes to exploit addi-
tional invariant structures. In Section 4 and Appendix B, we demonstrate by extensive experiments
that our compression obtains higher accuracy than existing low-rank approximation techniques. Ap-
pendix A surveys compression techniques and discuss how our method is related or complementary
to existing techniques. For simplicity, we will use tensor diagrams throughout the text. However we
provide a detailed appendix where the tensor operations are mathematically defined.
2 Tensor Preliminaries
Notations An m-dimensional array T is defined as an m-order
tensor T ∈ RI0×…×Im-1. Its (io, ∙ ∙ ∙ ,in-1,in+1,…，im-ι)th
mode-n fiber, a vector along the nth axis, is denoted as
T
i i0 ,	, in—1 , ： ,in+ 1 , ,im—1
Tensor Diagrams. Following the convention in quantum
physics Cichocki et al. (2016), Figure 2 introduces graphical
representations for multi-dimensional objects. In tensor diagrams,
an array (scalar/vector/matrix/tensor) is represented as a node in the
graph, and its order is denoted by the number of edges extending
from the node, where each edge corresponds to one mode (whose
©	Θ-I^
(a) Scalar	(b) Vector
M
(c) Matrix (d) Tensor
Figure 2: Diagram of a ∈
R, v ∈ RI, M ∈ RI×J and
T∈ RI×J×K.
dimension is denoted by the number associated to the edge) of the multi-dimensional array.
(a)
I0	J1
I0 = J1
2HX)⅛JK =
Mode-0 tensor product/multiplication
⑼ X × 0 M →T2 : Tj0,iι,i2 = Er Xr,i1,i2 Mjo ,r
Mode-(0,1) tensor contraction X ×01 Y → T1
Ti11 ,i2 ,j0 ,j2 = Σr Xr,il ,i2 Yj0 ,r,j2
(C) Mode-(0,1) tensor convolution X *0 Y →T3
Ti1,i2,jθj2 = x=,i1,i2 * Yj0 ,:,j2
I0
I0
∖I2	J2 /
I1	J0
(d)
Mode-(0,1) tensor partial outer product
Xe)I Y→ T4: T4iι,i2,j0,j2 = Xr,i1,i2 Yj0,r,j2
Figure 3: Tensor operation illustration. Examples of tensor operations in which M ∈ RJ0×J1 ,
X ∈ RI0 ×I1 ×I2 and Y ∈ RJ0×J1×J2 are input matrix/tensors, and T1 ∈ RI1 ×I2 ×J0×J2, T2 ∈
RJ0×I1×I2, T3 ∈ RI’×I1×I2×J0×J2 and T4 ∈ RIo ×I1×I2×J0×J2 are output tensors of correspond-
ing operations. Similar definitions apply to general mode-(i, j) tensor operations.
Tensor Operations. In Figure 3, we use some simple examples to introduce four types of tensor
operations, which are higher-order generalization of their matrix/vector counterparts, on input ten-
sors X and Y and input matrix M. In tensor diagram, an operation is represented by linking edges
from the input tensors, where the type of operation is denoted by the shape of line that connects
the nodes: solid line stands for tensor contraction / tensor multiplication, dashed line represents
3
Under review as a conference paper at ICLR 2019
tensor convolution, and curved line is for tensor partial outer product. The rigorous definitions of
high-order general tensor operations are defined in Appendix D.
Tensor Decompositions. We introduce generalized tensor decomposition as the reverse mapping
of the general tensor operations (detailed in Appendix F): given a set of operations and a tensor,
the generalized tensor decomposition recovers the factors/components such that the operations on
these factors result in a tensor approximately equal to the original one. Several classical types of
tensor decompositions (such as CANDECOMP/PARAFAC (CP), Tucker (TK) and Tensor-train (TT)
decompositions) are introduced in Appendix F, and their applications on the convolutional kernel in
Figure 4a (defined in Section 3) are illustrated as tensor diagrams in Figures 4b, 4c and 4d.
(a) Original kernel
^-(k?^MkO-^MkTK
IW
(c) Tucker (TK)
IS	IH	IW	IT
^S)-R_~(KH)-R-(K^~~Rt~~(KT)
(d) Tensor-train (TT)
H
W
(e) Tensorized kernel
T0
(f) r-CP (m=3)
So
P
Rt0
Q0
Q2
Rt2
T
(g) r-TK (m=3)
∣So
⅛~r0
R1
R2
(h) r-TT (m=3)
IW
H
C
Figure 4: Diagrams of kernel decompositions. Figure (b) (c) and (d) are three types of plain
tensor decomposition for a traditional convolutional kernel K in (a). Figure (f) (g) and (h) are three
types of reshaped tensor decomposition for our tensorized kernel K'in (e) where the reshaping order
m ∈ Z is chosen to be 3 for illustrative simplicity.
3 Compressing convolutional layer by tensor decompositions
A standard convolutional layer in neural networks is parameterized by a 4-order kernel K ∈
RH×W ×S×T where H, W are height/width of the filters, and S, T are the numbers of input/output
channels. The layer maps a 3-order input tensor U ∈ RX×Y ×S (with S number of feature maps of
height X and width Y) to another 3-order output tensor V ∈ RX ×y ×t (with T number of feature
maps of height X' and width Y') according to the following equation:
Vx,y,t
S-1
ΣΣKi,j,s,t Ui+dx,j+dy,s
s=0 i,j
(3.1)
where d is the stride of the convolution. With HWST parameters, it takes O(HWSTXY) opera-
tions (FLOPs) to compute the output V . The diagram of the convolutional layer is in Figure 5a.
Plain Tensor Decomposition (PD) Traditional techniques compress a convolutional layer by di-
rectly factorizing the kernel K using tensor decompositions Jaderberg et al. (2014); Lebedev et al.
(2014); Kim et al. (2015), such as CANDECOMP/PARAFAC (CP), Tucker (TK) and Tensor-train
(TT) decompositions. For example, consider a Tensor-train decomposition on K, the kernel can be
factorized and stored as KS ∈ RS×Rs, KH ∈ RRs×H×R, KW ∈ RR×W×Rt and KT ∈ RRt×T,
which only requires (SRs + HRsR + WRtR + TRt) parameters as illustrated in Figure 4d. The
decomposition is rigorously defined element-wisely as
Rs-1 R-1 Rt-1
Ki,j,s,t = NENKs,rs Krs,i,r Kr,j,rt KrTt ,t	(3.2)
We defer the details of using CPandTKto Appendix G, although their tensor diagrams are illustrated
in Figures 4b, 4c and 4d and their complexities are summarized in Tables 1 and 10.
4
Under review as a conference paper at ICLR 2019
T
(a) Uncompressed
T
(b) Compression via TT
KW)-^MKT
Figure 5: Convolutional layer diagram. Input U is passed through the layer kernel K. The forward
propogation operation of an uncompressed layer, a plain tensor decomposition compressed layer and
our reshaped tensor decomposition compressed layer are illustrated in (a), (b) and (c) respectively.
Motivation of Tensorization - Invariant Structures Consider a matrix M ∈ RL2×L2 with
a modulated version of another
M = a 0 b, a, b ∈ RL2, where a = [c; •…；c] is periodic repetition of the same vector C ∈ RL and
[do1τ; dιl∖ …；dL-ι1τ] is a modulated version of another vector d ∈ RL. Obviously,
M as a rank-1 matrix can be represented by two length-L2 vectors a and b, resulting in a total of
2L2 parameters. However, if we reshape the matrix M into a 4-order tensor T ∈ RL×L×L×L, it
can be factorized by CP decomposition as T = 1 0 c 0 d 0 1 (1 ∈ RL), and represented by four
length-L vectors, requiring only 4L parameters. We refer the process of reshaping an array into a
higher-order tensor as tensorization, and the use of tensor decomposition following tensorization as
reshaped tensor decomposition (RTD). Therefore, the example above demonstrates that RTD dis-
covers additional invariant structures that baseline plain tensor decomposition (PD) fails to identify.
Reshaped Tensor Decomposition (RTD) Inspired by this intuition, we tensorize the convolu-
tional kernel K into a higher-order tensor K ∈H×W×S0 ×…×Sm-1 ×τ0 ×…×τm-1. Correspondingly,
we define an equivalent tensorized convolutional layer to Equation 3.1, by further reshaping input U
and output V into higher-order tensors U' ∈ RX×Y×S0× ×sm-1 and V' ∈ RX'×Y'×τ0×…×τm-1.
S0 -1	Sm-1 -1
Vx	t 1
χ,y,to, , tm-1
∑∙∙∙ E ∑κij
,s0 ∙∙∙ ,sm—1 ,t0 , ∙∙∙ ,tm-1
Mi+dxj+dy,s0,…,Sm-ι
(3.3)
s0=0	sm-1 =0 i,j
Now We can compress the convolutional layer by factorizing the tensorized kernel K by tensor
decompositions, and name the schemes using CP, Tucker and Tensor-train as reshaped CP (r-CP),
reshaped Tucker (r-TK) and reshaped Tensor-train (r-TT) respectively. For example, consider a r-TT
decomposition on K', the tensorized kernel can now be stored in m+1 factors {K0,…，Km-1, KC}
(a special example of m = 3 is illustrated in Figure 4h). The decomposition scheme is rigorously
defined element-wisely as
Ki js0 …S ItCI …t 1
"j,s0,	,sm—1 , t,0 ,	, m——1
R0-1	Rm-1-1
E∙∙∙ Σ K0o,ro,to
Q0,s1,t1,r1 …KCj,rm-1
(3.4)
r0=1	rm-1=0
where K ∈ RR-1×sl×τι×Rι,∀ι ∈ [m] and KC ∈ RRm-1×h×w. Assuming Sl = Sm,Tl =
Tmm, Rl = R,∀l ∈ [m], We require O(m(ST)mR2 + HWR) parameters. We defer the detailed
descriptions of r-CP and r-TK to Appendix I, but we illustrate their tensor diagrams in Figures 4f
and 4g and summarize their complexities in Tables 1 and 12.
Sequential Tuning Tensor decompositions provide weight estimates in the tensorized convolu-
tional layers. However, decomposing higher order tensors is challenging and known methods are
not guaranteed to converge to the minimum error decompositions (Hillar & Lim, 2013). Therefore
fine tuning is needed to restore high performance. Analogous to Huang et al. (2017), our strategy
of data reconstruction-based sequential tuning (Seq) sequentially fine-tunes the parameters layer by
layer, using backpropagation to minimize the difference between the outputs from uncompressed
layer Vi and the tensorized compressed layer Vi.
Computational Complexity As shown in Figure 5c, reshaped tensor decomposition maps a layer
into multiple (and thus deeper) narrower layers, each of which has width Rl that is usually smaller
5
Under review as a conference paper at ICLR 2019
than the original width T . We design efficient algorithms for forward/backward propagations for
prediction/fine-tuning on these modified layers using tensor algebra. (1) Forward propagation:
computing the output V', given the input U' and kernel factors {Kl}m=-1. (2) Backward propaga-
tion: computing the derivative of loss function L with respect to (w.r.t.) the input ∂L∕∂U' and
kernel factors {∂L∕∂Kl}m=01, given the derivative w.r.t. the output ∂L∕∂V'.
A naive forward and backward propagation mechanism is to explicitly reconstruct the original kernel
K using the factors {Kl }m=1, which however makes propagations highly inefficient as shown in
Appendix F, G and I. Alternatively, we propose a framework where both propagations are
evaluated efficiently without explicitly forming or computing the original kernel. The key idea
is to interact the input U' with each of the factors Kl individually. Taking r-TT as an example, We
plug the decomposition 3.4 into 3.3, then the computation of V' is reduced into m + 1 steps:
Rl-1 -1 Sl -1
UX,y,…,rι = E Σ Kr--1,sι ,tl,rι Ux-Isl,…,Sm.ι ,to,…,t-ι ,r-ι, ∀l =1,…，m
rl-1 =0 sl =0
Rm-1 -1
^x,y,t0,∙∙∙ ,tm-1 =	Krmm-1
,i,j Ui+dx,j+dy,to,…,tm-1,rm-1
rm-1 =0 i,j
(3.5)
(3.6)
where Ul is the intermediate result after interacting with Kl-1 , and U0 = U. Each step in 3.5
takes O(max(S,T)1+ mm RXY) operations, while the last step in 3.6 requires O(HWTRXY) 1.
Therefore, the time complexity for the forward pass is O((m max(S, T)1+记R + HWT)RXY),
more efficient than uncompressed O(HWSTXY) as R ≪ S, T . Backpropagation is derived and
analyzed in Appendix I, and the analyses of other decomposition are in Appendix G and I, but we
summarize their computational complexities in Table 1, 10 and 12.
Parallel Computational Complexity Tensor algebra allows us to implement the propagations 3.5
and 3.6 in parallel given enough computational resources, further speeding up prediction. The par-
allel tine complexities of prediction 2 with our RTD implementation is displayed in Table 2. The
prediction time complexity of RTD outperforms the baseline PD, whereas the PD outperforms the
original convolutional layers as R ≪ N and m ≥ 3.
Decomp.	O(# of parameters)	O(# of forward ops.)	O(# of backward ops.)
original	k2N2	,k2 N 2D2 ,	,N2D4	,
-CP-	(k2 + 2N )R	(k2 + 2N )RD2	(D2 +2N )RD2
TK	(k2R +2N )R	(k2R + 2N )RD2	(D2R + 2N )RD2
TT	2(kR + N )R	2(kR + N )RD2	2(DR + N )RD2
r-CP-	(k2 + mN m2 )R	(mN 1+* + k2N )RD2	(mN 1+ * + ND2)RD2
r-TK	(k2R2m-1 +2mN )R	(k2R2m-1 +2mN )RD2	(R2m-1D2 + 2mN )RD2
r-TT	(mN m R + k2)R	(mN 1+ m R + k2N )RD2	(mN 1+m R + ND2)RD2
Table 1: Number of parameters and operations required by a compressed convolutional layer by
various types of tensor decompositions (for the special case of X = Y = X' = Y' = D, S = T =
N , H = W = k and D ≫ k). General settings are summarized in Tables 10 and 12.
Decomp.	O (parallel complexity)	Decomp.	O(parallel complexity)
^^CP-	N + k2 + R	-r-CP-	mN = + k2R
TK	N + k2R + R	r-TK	2mN m1 + k2Rm
TT	N + 2kR + R	r-TT	mN mm R + k2R
Table 2: Parallel time complexity of forward pass using various types of tensor decompositions on
convolutional layers. The uncompressed parallel complexity of forward pass is O(k2N).
1The optimal complexity of tensor algebra is NP complete in general Lam et al. (1997), therefore the com-
plexity presented in this paper is the complexity of our implementation.
2Assuming adding n terms takes n time in parallel for memory efficiency, although it could be O(log n).
6
Under review as a conference paper at ICLR 2019
4	Experiments
We evaluate our reshaped tensor decomposition method on the state-of-art networks for a set of
benchmark datasets: we evaluate convolutional layer compression on ResNet-32 He et al. (2016)
for CIFAR-10; we evaluate fully-connected layer compression on MNIST; and we evaluate the
scalability of our compression method on ResNet-50 He et al. (2016) for ImageNet (2012) dataset.
The baseline we compare against is the state-of-the-art low-rank approximation methods called plain
tensor decomposition (PD), as other compression methods are complementary and can be used on
top of our reshaped tensor decomposition (RTD) method. All types of tensor decomposition (CP,
TK, and TT) in baseline PD will be evaluated and compared with corresponding types of tensor
decomposition (r-CP, r-TK and r-TT) in our RTD method.
Our primary contribution is to introduce a new framework, reshaped tensor decomposition, that
picks out additional invariance structure such as periodicity and modulation, which the low rank
approximation baseline, plain tensor decomposition, fails to find. Now we demonstrate that our
RTD maintains high accuracy even when the networks are highly compressed on CIFAR-10. We
refer to traditional backpropogation-based tuning of the network as end-to-end (E2E) tuning, and to
our proposed approach that trains each block individually as data reconstruction-based sequential
(Seq) tunning.
Decomp.	5%	Compression rate 10%	20%		40%	Decomp.	2%	Compression rate 5%	10%		20%
-SVD-	83.09	87.27	89.58	90.85	r-TR,-	-	80.801	-	90.60
CP	84.02	86.93	88.75	88.75	r-CP	85.7	89.86	91.28	-
TK	83.57	86.00	88.03	89.35	r-TK	61.06	71.34	81.59	87.11
TT	77.44	82.92	84.13	86.64	r-TT	78.95	84.26	87.89	-
,The reshaped tensor ring (r-TR) results			are cited from Wang et al. (2018), and the accuracy of 80.8% is achieved by 6.67% compression rate.						
Table 3: Percentage test accuracy of baseline PD with E2E tuning vs. our RTD with Seq tuning on
CIFAR10. The uncompressed ResNet-32 achieves 93.2% accuracy with 0.46M parameters.
Our algorithm achieves 5% higher accuracy than baseline on ResNet-34 CIFAR10. As in Ta-
ble 3, using baseline CP decomposition with end-to-end tuning, ResNet-34 is compressed to 10% of
its original size, reducing the accuracy from 93.2% to 86.93%. Our reshaped tensor decomposition
using r-CP, paired with Seq tuning, increases the accuracy to 91.28% with the same 10% compres-
sion rate — a performance loss of 2% with only 10% of the number of parameters. It achieves
further aggressive compression — a performance loss of 6% with only 2% of the number of
parameters. We observe similar trends (higher compression and higher accuracy) for Tensor-train
decomposition. The structure of the Tucker decomposition (see section I) makes it less effective
with very high compression, since the “internal structure” of the network reduces to very low rank,
which may lose necessary information. Increasing the network size to 20% of the original provides
reasonable performance on CIFAR-10 for Tucker as well.
Table 3 shows that RTD with Seq tuning outperforms PD
with end-to-end tuning. Now we address the following
question: is one factor (Seq tuning or RTD) primarily re-
sponsible for increased performance, or is the benefit due
to synergy between the two?
Seq tuning, reshaped tensor decomposition, or both?
(1) We present the effect of different tuning methods on
accuracy in Table 4. Other than at very high compres-
sion rate (5% column in Table 4), Seq tuning (Seq) consis-
tently outperforms end-to-end (E2E) tuning. In addition,
Seq tuning is also much faster and leads to more stable
convergence compared to end-to-end tuning. Figure 6 plots the compression error over the number
of gradient updates for various tuning methods. (2) We present the effect of different compression
methods on accuracy in Table 5. Interestingly, if our RTD is used, the test accuracy is restored for
0.
0.
0.
0.
0.
0.
0.
0.
0.
1
.9
.8
.7
.6
.5
.4
.3
.2
.1
0
Seq-CP
Seq-TT
Seq-TK
一 ■ E2E-CP
E2E-TK
E2E-TT
0	0.5	1	1.5	2	2.5	3	3.5
# Gradient Updates (×1011 )
Figure 6: Convergence rate for Seq vs.
E2E tuning on CIFAR10.
7
Under review as a conference paper at ICLR 2019
even very high compression ratios 3. These results confirm the existence of extra invariant struc-
ture in the parameter space of deep neural networks. Such invariant structure is picked up by
our proposed aproach, tensorization combined with low rank approximation (i.e., our RTD), but not
by low rank approximation itself (i.e., baseline PD). Therefore, our results show that RTD and Seq
tuning are symbiotic, and both are necessary to simultaneously obtain a high accuracy and a high
compression rate.
Decomp.	Compression rate							
	5%		10%		20%		40%	
	Seq	E2E	Seq	E2E	Seq	E2E	Seq	E2E
SVD-	74.04	83.09	85.28	87.27	89.74	89.58	91.83	90.85
CP	83.19	84.02	88.50	86.93	90.72	88.75	89.75	88.75
TK	80.11	83.57	86.75	86.00	89.55	88.03	91.3	89.35
TT	80.77	77.44	87.08	82.92	89.14	84.13	91.21	86.64
Table 4: Percentage accuracy of our Seq vs. baseline E2E tuning using PD on CIFAR10.
Decomp.	Compression rate 5%	10%		Decomp.	Compression rate 5%	10%	
-CP-	83.19	88.50	-r-CP-	89.86	91.28
TK	80.11	86.73	r-TK	71.34	81.59
TT	80.77	87.08	r-TT	84.26	87.89
Table 5: Percentage accuracy of our RTD vs. baseline PD using Seq tuning on CIFAR10.
Scalability Finally, we show that our methods scale to state-of-the-art large networks, by evaluat-
ing performance on the ImageNet 2012 dataset on a 50-layer ResNet (uncompressed with 76.05%
accuracy). Table 6 shows the accuracy of RTD (TT decomposition) with Seq tuning compared to
plain tensor decomposition with E2E tuning and the uncompressed network, on ResNet-50 with
10% compression rate. Table 6 shows that Seq tuning of RTD is faster than the alternative. This is
an important result because it empirically validates our hypotheses that (1) our RTD compression
captures the invariance structure of the ResNet (with few redundancies) better and faster than the
baseline PD compression, (2) data reconstruction Seq tuning is effective even on the largest networks
and datasets, and (3) our proposed efficient RTD compression methods scale to the state-of-the-art
neural networks.
# epochs	# samples	Uncompressed #Params. = 25M	TT (E2E) #Params. = 2.5M	r-TT (Seq) # params. = 2.5M
-02-	024^	422	278	44:35
0.3	0.36M	6.23	3.99	46.98
0.5	0.60M	9.01	7.48	49.92
1.0	1.20M	17.3	12.80	52.59
2.0	2.40M	30.8	18.17	54.00
Table 6: Convergence of percentage accuracies of uncompressed vs. PD (TT decomposition) vs.
RTD (r-TT decomposition) achieving 10% compression rate for ResNet-50 ImageNet.
5	Conclusion and Perspectives
We describe an efficient mechanism for compressing neural networks by tensorizing network layers.
We implement tensorized decompositions to find approximations of the tensorized kernel, potentially
preserving invariance structures missed by implementing decompositions on the original kernels.
We extend vector/matrix operations to their higher order tensor counterparts, providing systematic
notations and libraries for tensorization of neural networks and higher order tensor decompositions.
As a future step, we will explore optimizing the parallel implementations of the tensor algebra.
3Note that Tucker remains an exception for aggressive compression due to the low rank internal structure
that we previously discussed.
8
Under review as a conference paper at ICLR 2019
References
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural informa-
tion processing systems, pp. 2654-2662,2014.
Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An
exploration of parameter redundancy in deep networks with circulant projections. In Proceedings
of the IEEE International Conference on Computer Vision, pp. 2857-2865, 2015.
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration
for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.
Frangois Chollet. XcePtion: Deep learning with depthwise separable convolutions. arXiv preprint
arXiv:1610.02357, 2016.
Andrzej Cichocki, Namgil Lee, Ivan V Oseledets, Anh Huy Phan, Qibin Zhao, and D Mandic.
Low-rank tensor networks for dimensionality reduction and large-scale optimization problems:
Perspectives and challenges part 1. arXiv preprint arXiv:1609.00893, 2016.
Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear
structure within convolutional networks for efficient evaluation. In Advances in neural informa-
tion processing systems, pp. 1269-1277, 2014.
Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate tensorization:
compressing convolutional and fc layers alike. arXiv preprint arXiv:1611.03214, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016.
Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM
(JACM), 60(6):45, 2013.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Furong Huang, Jordan Ash, John Langford, and Robert Schapire. Learning deep resnet blocks
sequentially using boosting theory. arXiv preprint arXiv:1706.04964, 2017.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-
pression of deep convolutional neural networks for fast and low power mobile applications. arXiv
preprint arXiv:1511.06530, 2015.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Jean Kossaifi, Aran Khanna, Zachary Lipton, Tommaso Furlanello, and Anima Anandkumar. Ten-
sor contraction layers for parsimonious deep nets. In Computer Vision and Pattern Recognition
Workshops (CVPRW), 2017 IEEE Conference on, pp. 1940-1946. IEEE, 2017a.
Jean Kossaifi, Zachary C Lipton, Aran Khanna, Tommaso Furlanello, and Anima Anandkumar. Ten-
sor regression networks. arXiv preprint arXiv:1707.08308, 2017b.
Chi-Chung Lam, P Sadayappan, and Rephael Wenger. On optimizing a class of multi-dimensional
loops with reductions for parallel execution. Parallel Processing Letters, 7(2):157-168, 1997.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.
Speeding-up convolutional neural networks using fine-tuned cp-decomposition. arXiv preprint
arXiv:1412.6553, 2014.
9
Under review as a conference paper at ICLR 2019
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedingsofthe IEEE, 86(11):2278-2324,1998.
Michael Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks through
ffts. arXiv preprint arXiv:1312.5851, 2013.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural
networks. In Advances in Neural Information Processing Systems, pp. 442-450, 2015.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295-
2317, 2011.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Advances in Neural Information Processing Systems, pp. 3088-3096, 2015.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4,
inception-resnet and the impact of residual connections on learning. In AAAI, volume 4, pp.
12, 2017.
Wenqi Wang, Yifan Sun, Brian Eriksson, Wenlin Wang, and Vaneet Aggarwal. Wide compression:
Tensor ring nets. learning, 14(15):13-31, 2018.
Bichen Wu, Forrest N Iandola, Peter H Jin, and Kurt Keutzer. Squeezedet: Unified, small, low
power fully convolutional neural networks for real-time object detection for autonomous driving.
In CVPR Workshops, pp. 446-454, 2017.
Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for
video classification. arXiv preprint arXiv:1707.01786, 2017.
Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu
Wang. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1476-1483, 2015.
Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun. Efficient and accurate
approximations of nonlinear convolutional networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1984-1992, 2015.
10
Under review as a conference paper at ICLR 2019
Appendix: Exploiting Invariant Structures for Compression in
Neural Networks
A	Related Works
A recent survey Cheng et al. (2017) reviews state-of-the-art techniques for compressing neural net-
works, in which they group the methods into four categories: (1) low-rank factorization; (2) design
of compact filters; (3) knowledge distillation; and 4) parameters pruning, quantization and encoding.
Generally, our decomposition schemes fall into the category of low-rank factorization, but these
schemes also naturally lead to novel designs of compact filters in the sublayers of the compressed
network. On the other hand, our strategy of sequential tuning is an advanced scheme of knowl-
edge distillation that transfers information from pre-trained teacher network to compressed student
network block by block. Furthermore, our method is complementary to the techniques of param-
eters pruning, quantization and encoding, which can be applied on top of our method by further
compressing the parameters in the sublayers returned by tensor decomposition.
•	Low-rank Factorization. Low-rank approximation techniques have been used for a long time
to reduce the number of parameters in both fully connected and convolutional layers. Pioneering
papers propose to flatten/unfold the parameters in convolutional layer into matrices (a.k.a ma-
tricization), followed by (sparse) dictionary learning or matrix decomposition (Jaderberg et al.,
2014; Denton et al., 2014; Zhang et al., 2015). Subsequently in Lebedev et al. (2014); Kim et al.
(2015), the authors show that it is possible to compress the tensor of parameters directly by
standard tensor decomposition (in particular CP or Tucker decomposition). The groundbreak-
ing work (Novikov et al., 2015) demonstrates that the parameters in fully connected layer can be
efficiently compressed by tensor decomposition by first reshaping the matrix of parameters into
higher-order tensor, and the idea is later extended to compress LSTM and GRU layers in recurrent
neural networks (Yang et al., 2017).
Concurrent to Wang et al. (2018), our paper extends this basic idea to convolutional layer by
exploiting the invariant structures among the filters. Different from Wang et al. (2018) that only
focuses Tensor-ring decomposition, we investigates, analyzes and implements a boarder range of
decomposition schemes, besides other benefits discussed below.
•	Design of Compact Filters. These techniques reduce the number of parameters by im-
posing additional constraints on linear layers (fully connected or convolutional). For exam-
ple, the matrix of parameters in fully connected layer is restricted to circular (Cheng et al.,
2015), Toeplitz/Vandermonde/Cauchy (Sindhwani et al., 2015), or multiplication of special ma-
trices (Yang et al., 2015). Historically, convolutional layer is proposed as a compact design of
fully connected layer, where spatial connections are local (thus sparse) with repeated weights.
Recent research further suggests to use more compact convolutional layers, such as 1 × 1 con-
volutional layer (Szegedy et al., 2017; Wu et al., 2017)(where each filter is simply a scalar), and
depthwise convolutional layer (Chollet, 2016) (where connections between the feature maps are
also sparse). In our paper, we show that the sublayers returned by our decomposition schemes are
in fact 1 × 1 depthwise convolutional layers, combing advantages from both designs above.
•	Knowledge Distillation. The algorithms of knowledge distillation aim to transfer information
from a pre-trained teacher network to a smaller student network. In Ba & Caruana (2014);
Hinton et al. (2015), the authors propose to train the student network supervised by the logits
(the vector before softmax layer) of the teacher network. Romero et al. (2014) extends the idea to
matching the outputs from both networks at each layer, up to an affine transformation. Our Seq
tuning strategy is therefore similar to Romero et al. (2014), but we use identical mapping instead
of affine transformation, and train the compressed network block by block.
•	Pruning, Quantization and Encoding. Han et al. (2015) proposes a three-step pipeline to com-
press a pre-trained network, by (1) pruning uninformative connections, (2) quantizing the re-
maining weights and (3) encoding the discretized parameters. Since our decomposition schemes
effectively transform one layer in the original network into the multiple sublayers, this pipeline
can be applied by further compressing all sublayers. Therefore, our method is complementary
(and can be used independently) to the techniques in this pipeline.
11
Under review as a conference paper at ICLR 2019
B	S upplementary experiments
Convergence Rate Compared to end-to-end, an ancillary benefit of Seq tuning is much faster and
leads to more stable convergence. Figure 6 plots compression error over number of gradient updates
for various methods. (This experiment is for PD with 10% compression rate.) There are three salient
points: first, Seq tuning has very high error in the beginning while the “early” blocks of the network
are being tuned (and the rest of the network is left unchanged to tensor decomposition values).
However, as the final block is tuned (around 2 × 1011 gradient updates) in the figure, the errors drop
to nearly minimum immediately. In comparison, end-to-end tuning requires 50-100% more gradient
updates to achieve stable performance. Finally, the result also shows that for each block, Seq tuning
achieves convergence very quickly (and nearly monotonically), which results in the stair-step pattern
since extra tuning of a block does not improve (or appreciably reduce) performance.
Performance on Fully-Connected Layers An extra advantage of reshaped tensor decomposition
compression is that it can apply flexibly to fully-connected as well as convolutional layers ofa neural
network. Table 7 shows the results of applying reshaped tensor decomposition compression to vari-
ous tensor decompositions on a variant of LeNet-5 network LeCun et al. (1998). The convolutional
layers of the LeNet-5 network were not compressed, trained or updated in these experiments. The
uncompressed network achieves 99.31% accuracy. Table 7 shows the fully-connected layers can
be compressed to 0.2% losing only about 2% accuracy. In fact, compressing the dense layers
to 1% of their original size reduce accuracy by less then 1%, demonstrating the extreme efficacy of
reshaped tensor decomposition compression when applied to fully-connected neural network layers.
Method	Compression rate		
	0.2%	0.5%	1%
r-CP	97.21	97.92	98.65
r-TK	97.71	98.56	98.52
r-TT	97.69	98.43	98.63
Table 7: Reshaped tensor decomposition combined with sequential for fully-connected layers on
MNIST. The uncompressed network achieves 99.31% accuracy.
C Notations
Symbols: Lower case letters (e.g. v) are used to denote column vectors, while upper case letters
(e.g. M) are used for matrices, and curled letters (e.g. T) for multi-dimensional arrays (tensors). For
a tensor T ∈ RI0× ×1m-1, We will refer to the number of indices as order, each individual index
as mode and the length at one mode as dimension. Therefore, we will say that T ∈ RI0× ×Im-1
is an m-order tensor which has dimension Ik at mode-k. Tensor operations are extensively used
in this paper: The tensor (partial) outer product is denoted as 0, tensor convolution as *, and
finally × denotes either tensor contraction or tensor multiplication. Each of these operators will
be equipped with subscript and superscript when used in practice, for example ×nm denotes mode-
(m, n) tensor contraction (defined in Appendix D). Furthermore, the symbol ◦ is used to construct
compound operations. For example, (* ◦ 0) is a compound operator simultaneously performing
tensor convolution and tensor partial outer product between two tensors.
Indexing: In this paragraph, we explain the usages of subscripts/superscripts for both multi-
dimensional arrays and operators, and further introduce several functions that are used to alter the
layout of multi-dimensional arrays.
• Nature indices start from 0, but reversed indices are used occasionally, which start from -1.
Therefore the first entry ofa vector v is v0, while the last one is v-1.
• For multi-dimensional arrays, the subscript is used to denote an entry or a subarray within an
object, while superscript is to index among a sequence of arrays. For example, Mi,j denotes the
entry at ith row and jth column of a matrix M, and M(k) is the kth matrix in a set of N matrices
{M(0), M⑴,…M(N-1)}. For operators, as we have seen, both subscript and superscript are
used to denote the modes involved in the operation.
12
Under review as a conference paper at ICLR 2019
•	The symbol colon ’:’ is used to slice a multi-dimensional array. For example, M:,k denotes the
kth column of M, and T:,:,k denotes the kth frontal slice ofa 3-order tensor T.
•	Big-endian notation is adopted in conversion between multi-dimensional array and vectors.
Specifically, the function vec(∙) flattens (a.k.a. vectorize) a tensor T ∈ RI0× ×1m-1 into a vector
V ∈ Rn m-1Il SUChthat T0,…，im-1 = vim-i+im-2lm-i +…+i° Ii ∙∙∙ Im-1 ∙
•	The function SwapaXes(∙) is used to permute ordering of the modes of a tensor as needed. For
example, given two tensors U ∈ RI×J×K and V ∈ RK×J×I, the operation V = swapaxes(U)
convert the tensor U into V such that Vk,j,i = Ui,j,k .
•	The function flipaxis(∙, ∙) flips a tensor along a given mode. For example, given a tensor U ∈
RI×J×K and V = flipaXis(U, 0), the entries in V is defined as Vi,j,k = UI-1-i(mod I),j,k.
D Tensor operations
OPeratOr	NOtatiOn	DefinitiOn
mode-(k, l) Tensor Contraction	T(0)二	X	k ×l	Y	T(O) 7i0,∙∙∙ ,ik-i,ik+i ,∙∙∙ ,im-i,jo,∙∙∙ ,ji-i ,jι + ι ,∙∙∙ ,jn-l =<Xi0,…，ik-1,∖ik+1,…，im-1 , Yj0,…,jl-1,:,jl + 1, …,jn-i) inner product of mode-k fiber of X and mode-l fiber of Y
mode-k Tensor Multiplication	T⑴二	X	×k	M	T(I) lio,∙∙∙ ,ik-ι,r,ik+ι,∙∙∙ ,im-i =<Xio,∙∙∙ ,ik-ι,∖ik+ι,∙∙∙ ,im-i , M:,r) inner product of mode-k fiber of X and rth column of M
mode-(k, l) Tensor Convolution	T⑵二	X	*k	Y	T⑵ ,iθ,…，ik-1,：,ik + 1,…，im-1,jθ,…，jl-1,jl + 1,…，jn-1 =Xiθ,…，ik-1,：,ik + 1,…，im-1 * Yjθ,∙∙∙ ,jl-i ,： ,jl + ι ,∙∙∙ ,jn-1 convolution of mode-k fiber of X and mode-l fiber of Y
mode-(k, l) Partial- Outer Product	T⑶二	X	动	Y	T⑶ i 0Q∙∙, ,ik-i,r,ik + i,∙∙∙ ,im-i,jo,∙∙∙ ,jn-1 =Xio,∙∙∙ ,ik-i,r,ik+i,∙∙∙ ,im-i Yjo,∙∙∙ ,ji-i ,r,jι + ι ,∙∙∙ ,jn-i Hadamard product of mode-k fiber of X
and mode-l fiber of Y
Table 8: Summary of tensor operations. In this table, X ∈ RI0××1mτ, γ∈ RJ0×∙∙∙× Jn—1 and
matrix M ∈ RIk×J. Mode-(k, l) tensor contraction and mode-(k, l) tensor partial-outer product are
legal only if Ik = Jl. T(0) is an (m + n - 2)-order tensor, T(1) is an m-order tensor, T(2) is an
(m + n - 1)-order tensor and T(3) is an (m + n - 1)-order tensor.
In this section, we introduce a number of tensor operations that serve as building blocks of tensorial
neural networks. To begin with, we describe several basic tensor operations that are natural gener-
alization to their vector/matrix counterparts. Despite their simplicity, these basic operations can be
combined among themselves to construct complicated compound operators that are actually used in
all designs. We will analyze their theoretical sequential time complexities in details, and point out
the implementational concerns along the way. Although all these operations can in principle be im-
plemented by parallel programs, the degree of parallelism depends on their particular software and
hardware realizations. Therefore, we will use the sequential time complexity as a rough estimate of
the computational expense in this paper.
Tensor contraction Given a m-order tensor T(0) ∈ RI0× ×1m-1 and another n-order tensor
T(1) ∈ RJ0× ×Jn-1, which share the same dimension at mode-k of T(0) and mode-l of T(1)( i.e.
Ik = Jl), the mode-(k, l) contraction of T(0) and T⑴，denoted as T = T(0) Xk T(1), returns
a (m + n - 2)-order tensor T
∈ rIo ×…×Ik-ι×Ik+i×…×Im-1×J0×…×J1-1×J1 + 1×…×Jn-i whose
13
Under review as a conference paper at ICLR 2019
entries are computed as
Z^0 ,…,ik-1 ,ik + 1 ,…,im-1 ,j0 ,…,jl — 1 ,jl + 1 ,…,jn-1
E琛)一
r=0
,ik-1 ,r,ik + 1 ,''' ,im-1
T(1)
j0 ,... ,jl — 1,r,jl + 1,''' ,jn —1
Tj0,…，jl-1,： ,jl + 1,…，jn-J
(D.1a)
(D.1b)
⑺⑼，i
∖ i0 ,…,ik-1 , ： ,ik + 1 ,…,2m—1
Notice that tensor contraction is a direct generalization of matrix multiplication to higher-order
tensor, and it reduces to matrix multiplication if both tensors are 2-order (and therefore matri-
ces). As each entry in T can be computed as inner product of two vectors, which requires
Ik = Jl multiplications, the total number of operations to evaluate a tensor contraction is there-
fore O(( um=-01Iu)( vn=-01,v=lJv)), taking additions into account.
Notice that the analysis of time complexity only serves as a rough estimate for actual execution time,
because we do not consider the factors of parallel computing and computer systems. In practice,
(1) the modes that are not contracted over can be computed in parallel, and summations can be
computed in logarithmic instead of linear time; (2) The spatial locality of the memory layout plays
a key role in speeding up the computation of tensor operations. These arguments equally apply to
all tensor operations in this paper, but we will not repeat them in the analyses for simplicity.
Tensor multiplication (Tensor product) Tensor multiplication (a.k.a. tensor product) is a special
case of tensor contraction where the second operant is a matrix. Given a m-order tensor U ∈
RI0× ×Im-1 and a matrix M ∈ RIk ×J, where the dimension of U at mode-k agrees with the
number of the rows in M, the mode-k tensor multiplication of U and M, denoted as V = UXk M,
yields another m-order tensor V ∈ RI0× ×Ik-1 ×J×Ik+1×…Im-1, whose entries are computed as
V20,∙∙∙ ,ik-1,j,ik + 1,∙∙∙ ,2m-1
Ik-1
=Ui0,…,ik-1,r,ik+1,--- ,im-1 Mrj	(D.2a)
r=0
=Wi0,…,ik-1,：,ik+1,…,im-1 , M：,j〉	(D.2b)
Following the convention of multi-linear algebra, the mode for J now substitutes the location origi-
nally for Ik (which is different from the definition of tensor contraction). Regardlessly, the number
of operations for tensor multiplication follows tensor contraction exactly, that is O((∏m=-01 Iu)J).
Tensor convolution Given a m-order tensor T(0) ∈ RI0 ×I1 × ×ImT and another n-order tensor
T⑴ ∈ RJ0×J1× ×Jnτ. The mode-(k, l) convolution of T⑼ and T⑴，denoted as T = T⑼ *?
T(1), returns a (m + n — 1)-ordertensor T ∈ RI0 ×---×Ik×…×Im-1×J0×…×Jl-1×Jl+1×…×Jn-1. The
entries of T can be computed using any convolution operation * that is defined for two vectors.
T
*i0j∙∙∙ ,ik-1 ,： ,ik + 1 ,∙∙∙ ,im-1 ,j0 ,∙∙∙ ,jι-1 ,j1 + 1 ,∙∙∙ ,jn-1
T(0)
i0,''' ,ik-1 ,： ,ik + 1 ,''' ,im-1
*T
j0,∙∙∙ ,jl-1,∙,jl + 1,∙∙∙ ,jn-1
T⑴	*
'j0,…，jl — 1,：,jl + 1,…，jn-1 个
∣ik-1,∙,ik+1,∙∙∙ ,im-1
(D.3a)
(D.3b)
Here we deliberately omit the exact definition of vector convolution *, as it can be defined in multiple
forms depending on the user case (Interestingly, the "convolution" in convolutional layer indeed
computes correlation instead of convolution). Correspondingly, the resulted dimension Ik at mode-
k is determined by the chosen type of convolution. For example, the "convolution" in convolutional
layer typically yields Ik = Ik (with same padding) or Ik = Ik 一 Jl + 1 (with valid padding). Notice
that vector convolution itself is generally asymmetric , i.e. u * v = v * u (except for the case of
circular ConVolution). For convenience, we can define its conjugate as * such that U * V = V * u.
With this notations, Equation D.3a can also be written as D.3b.
Generally speaking, Fast Fourier Transform (FFT) plays a critical role to lower the computational
complexities for all types of convolution. In the case of tensor convolution, the number of re-
quired operations without FFT is O((∏U=-o1 Iu)(∏n-1 JV)), while FFT is able to reduce it to
14
Under review as a conference paper at ICLR 2019
O((∏Um-01u=k Iu)(∏n-o,v=ι Jv)max(Ik, Jι)log(max(Ik, Ji))). That being said, FFT is not al-
ways necessary: ifmin(Ik, Jl) < log (max(Ik, Jl)) (which is typical in convolutional layers, where
Ik is the height/width of the feature maps and Jl is the side length of the square filters), computing
the convolution without FFT is actually faster. Furthermore, FFT can be difficult to implement (thus
not supported by popular software libraries) if convolution is fancily defined in neural networks (e.g.
dilated, atrous). Therefore, we will assume that tensor convolutions are computed without FFT in
subsequent sections unless otherwise noted.
Tensor outer product Given a m-order tensor T⑼ ∈ RI0×I1× ×1mτ and another n-order
tensor T⑴ ∈ RJ0×J1×∙∙∙×Jn—1, the outer product of T⑼ and T⑴，denoted T = T⑼ 0
T(1), concatenates all the indices of T(o) and T(1), and returns a (m + n)-order tensor T ∈
RI0 × × Im— 1 × J。× × Jn-I whose entries are computed as
,im-1,j0,∙∙∙ ,jn-1 = i i0,∙∙∙ ,im-1 /jθ ,…,jn-1
(D.4)
It is not difficult to see that tensor outer product is a direct generalization for outer product for two
vectors M = U 0 V = UvT. Obviously, the number of operations to compute a tensor outer product
explicitly is o((∏Um-11u)(∏n-1 Jv)). Tensor outer product is rarely calculated alone in practice
because it requires significant amounts of computational and memory resources.
Tensor partial outer product Tensor partial outer product is a variant of tensor outer product
defined above, which is widely used in conjunction with other operations. Given a m-order tensor
T(0) ∈ RI0×I1× ×Im-1 and another n-order tensor T⑴ ∈ RJ0×J1× ×Jn-1, which share the
same dimension at mode-k of T(o) and mode-l of T(1) (i.e. Ik = Jl), the mode-(k, l) partial outer
product of T⑼ and T⑴，denoted as T = T(0) 0lk T(1), returns a (m + n - 1)-order tensor
T ∈ RI0××Im-1×J0××Jι-1×Jι+1××Jn-1, whose entries are computed as
T
*i0j∙∙∙ ,ik-1 ,r,ik+1 ,∙∙∙ ,im-1 ,j0 ,∙∙∙ ,ji-1 ,jι + 1 ,∙∙∙ ,jn-1
m-1 ,jθ ,…，jl-1,r,jl + 1,…，jn-1
(D.5a)
(D.5b)
The operation bears the name "partial outer product" because it reduces to outer product
once we fix the indices at mode-k of T(o) and mode-l of T(1). Referring to the computa-
tional complexity of tensor outer product, the number of operations for each fixed index is
O(( um=-o1,u=kIu)( vn=-o1,v=lJv )), therefore the total time complexity for the tensor partial outer
product is O(( um=-o1 Iu)( vn=-o1,v=lJv)), the same as tensor contraction.
Precautions of usage
•	Similar to matrix multiplication, the operants in tensor operations are not commutative in general.
For example, neither T(o) ×lk T(1) = T(1) ×lk T(o) nor T(o) ×lk T(1) = T(1) ×lk T(o) holds even
if the dimensions at the specified modes happen to match.
•	Different from matrix multiplication, the law of associative also fails in general. For example,
(T (o) ×lk T (1)) ×qp T (2) = T(o) ×lk (T (1) ×qp T (2)), mainly because tensor operations can change
the locations of modes in a tensor.
•	However, both problems are not fundamental, and can be fixed by adjusting the superscripts and
subscripts of the operators carefully (and further permute ordering of the modes in the result
accordingly). For example, T⑼ Xk T⑴ = SwapaXes(T⑴ Xk T(0)) holds if SwapaXes(∙) is
properly performed. Due to space limits, we can not develop general rules in this paper, and will
derive such identities as needed. In general, the take away message is a simple statement: Given
an expression that contains multiple tensor operations, these operations need to be evaluated from
left to right unless a bracket is explicitly supplied.
Compound operations: As building blocks, the basic tensor operations defined above can further
combined to construct compound operations that perform multiple operations on multiple tensors
simultaneously. For simplicity, we illustrate their usage using two representative examples in this
15
Under review as a conference paper at ICLR 2019
section. More examples will arise naturally when we discuss the derivatives and backpropagation
rules for compound operations in Appendix E.
•	Simultaneous multi-operations between two tensors. For example, given two 3-order tensors
T⑼ ∈ rr×x×s and T⑴ ∈ Rr×h×s, We can define a compound operation(00 ◦*： ◦ ×2) be-
tween T(0) and T (1), where mode-(0, 0) partial outer product, mode-(1, 1) convolution and mode-
(2, 2) contraction are performed simultaneously, which results in a 2-order tensor T of RR×X (it
is indeed a matrix, though denoted as a tensor). The entries of T = T(0)(00 o*1 ◦ ×2) T⑴ are
computed as
S-1
Tr，： = ET(0,S *T(1,S	(D.6)
s=0
For commonly used vector convolution, it is not difficult to show that number of operations re-
quired to compute the result T is O (R max(X, H) log(max(X, H))S) with FFT and O(RXHS)
without FFT, as each of the R vectors in T is computed with a sum of S vector convolutions.
•	Simultaneous operations between a tensor and a set of multiple tensors. For example, given a
3-order tensor U ∈ RR×X×S and a set of three tensors T(0) ∈ RR×P, T(1) ∈ RK×Q and T(2) ∈
Rs×t, we can define a compound operation on U as V = U(00T(0) *0 T⑴ ×2 T⑵),which per-
forms mode-(0, 0) partial outer product with T(0), mode-(1, 0) convolution with T(1) and mode-
(2,0) contraction with T⑵ simultaneously. In this case, a 5-order tensor V ∈ Rr×x ×p×q×t is
returned, with entries calculated as
S-1
Vr，:，p，q，t =	Tr(，0p) Ur，:，s * T:，(q1) Ts(，2t)	(D.7)
s=0
The analysis of time complexity of a compound operation with multiple tensors turns out to be a
non-trivial problem. To see this, let us first follow the naive way to evaluate the output according
to the expression above: (1) each vector in the result Vr，:，p，q，t can be computed with a sum of S
vector convolutions, which requires O(XHS) operations; (2) and with RPQT such vectors in
the result V, the time complexity for the whole compound operation is therefore O(RXHPRST).
However, it is obviously not the best strategy to perform these operations. In fact, the equations
can be equivalently rewritten as
Vr,：,p,q,t =((I-I Ur，：，sT(,2) *Gl) T(P)	(D.8)
If we follows the supplied brackets and break the evaluation into three steps, it is not difficult to
verify that these steps take O(RXST),O(RXHPT) and O(RX'HPT) operations respectively,
and result in a total time complexity of O(RXST + RXHpT + RX'pQT) for the compound
operation, which is far lower than the one with the naive way.
Unfortunately, it is an NP-hard problem to determine the best order (with minimal number of
operations) to evaluate a compound operation over multiple tensors, therefore in practice the order
is either determined by exhaustive search (if there are only a few tensors) or follows a heuristic
strategy (if the number of tensors is large).
The examples provided above are by no mean comprehensive, and in fact more complicated com-
pound operations simultaneously perform multiple operations on multiple tensors can be defined,
and we will see examples of them in the next section when we derive the backpropagation equations
for the compound operations above. Generally, compound operations over multiple tensors are diffi-
cult to flatten into mathematical expressions without introducing tedious notations. Therefore, these
operations are usually described by graphical representations, which are usually called tensor net-
work in the physics literature (not to confuse with tensorial network in this paper). Interested readers
are referred to the monograph Cichocki et al. (2016), which serves a comprehensive introduction to
the application of tensor network in the field of machine learning.
16
Under review as a conference paper at ICLR 2019
E Derivatives and backpropagation of tensor operations
All operations introduced in the last section, both basic and compound, are linear in their operants.
Therefore, the derivatives of the result with respect to its inputs are in principle easy to calculate. In
this section, we will explicitly derive the derivatives for all operations we have seen in Appendix D.
These derivatives can be further combined with classic chain rule to obtain the corresponding back-
propagation equations (i.e. how gradient of the loss function propagates backward through tensor
operations), which are the cornerstones of modern feed-forward neural networks. In the section, we
show that these backpropagation equations can also be characterized by (compound) tensor opera-
tions, therefore their computational complexities can be analyzed similarly as in Appendix D.
Interestingly, the backpropagation equations associated with a tensor operation, though typically
appear to be more involved, share the same asymptotic complexities as in the forward pass (with
tensor convolution as an exception). This observation is extremely useful in the analyses of tensorial
neural networks in Appendix G, H and I, which allows us to reuse the same number in the forward
pass in the analysis of backward propagation.
In this section, we will assume for simplicity the loss function L is differentiable. However, all
derivatives and backpropagation equations equally apply when L is only sub-differentiable (piece-
wise smooth). Also, we will focus on one step of backpropagation, therefore we assume the gradient
of the loss function is known to us in prior.
Tensor contraction Recall the definition of tensor contraction in Equation D.1a, the partial deriva-
tives of the result T with respect to its operants T(0), T(1) can be computed at the entries level:
∂T
U ∕i0,…,ik-1,ik+1,…,im-1 j0,…jl-1 jl + 1,…jn-1
∂T⑼,
i0 ,∙∙∙ ,ik — 1 ,r,ik + 1 , ∙∙∙ ,im —1
∂T
,i0,…,ik-1,ik+1,…,im-1 j0,…jl-1 jl + 1,…jn-1
dT01)∙∙jl-l,rjl + l,…jn-1
T(1)
‘j0,…jl-1,rjl + 1,…jn-1
T(0)
i0,∙∙∙ ,ik-1 ,r,ik+1 ,∙∙∙ ,im-1
(E.1a)
(E.1b)
With classic chain rule, the derivatives of L with respect to T(0) and T(1) can be obtained through
the derivative of Lwith respect to T.
∂L
∂ T ⑼，.，
i0, ,ik—1,r,ik + 1,
∂L
J0-1	Jl-1-1 Jl+1-1	Jn-1-1
——E…E E…E
,im-1	j0 =0	jl-1 =0 jl+1=0	jn-1=0
d Ti0,…，ik-1,ik + 1,…，im-1 j0 ,…，jl-1jl + 1,… jn-1
∙∙∙ jl — 1,r,jl + 1,∙∙∙ ,jn —1
∂L
''/j0 , ∙∙∙ ,jl — 1,r,jl + 1,∙∙∙ ,jn — 1
∂L
I0 -1	Ik-1 -1 Ik+1 -1	Im-1 -1
E-E E-E
i0=0	ik-1=0 ik+1=0	im-1=0
∂T
U ∕i0,…,ik-1,ik + 1,…,im-1 j0,… jl-1 jl + 1,… jn-1
T(0)
i0 ,∙∙∙ ,ik-1 ,r,ik + 1 ,∙∙∙ ,im-1
Though tedious at entries level, it can be simplified with tensor notations in this paper.
∂L
∂T(0y = SwapaXes
∂L
∂T(T7 = SwapaXes
m-1	m+l-2	m+l-1	m+n-3	(1)
(×0	◦•.•”-+	◦ ×l++1	◦…。×n-1	)T( )
(×0。…。×k-ι。**。…。×m-2)τ ⑼)
(E.2a)
(E.2b)
(E.3a)
(E.3b)
where SwapaXes(∙) is used to align the modes of outputs. Notice that the backpropagation equa-
tions are compound operations, even if the original operation is a basic one. It is not dif-
ficult to show that the number of operations required for both backpropagation equations are
O(( um=-01Iu)( vn=-01,v=l Jv)), which are exactly the same as in the forward pass in Equation D.1a.
The result should not surprise us however, since the tensor contraction is a direct generalization to
matrix multiplication (where backward propagation has exactly the same time complexity as the
matrix multiplication itself).
17
Under review as a conference paper at ICLR 2019
Tensor multiplication (Tensor product) As a special case of tensor contraction, the derivatives
and backpropagation equations for tensor multiplication can be obtained in the same manner. To
begin with, the derivatives of V with respect to U and M can be computed from the definition in
Equation D.2a.
dVio,…,ik-ι j,ik+ι,…,im-ι
dUi0,∙∙∙ ,ik-1 ,r,ik+l,∙∙∙ ,im-1
Mr,j
(E.4a)
dVio,∙∙∙ ,ik-1j,ik+1,∙∙∙ ,im-1
∂Mj
=Uio,…
,ik-1 ,r,ik+1 ,∙∙∙ ,im-1
(E.4b)
Subsequently, the derivatives of L with respect to U and M can be computed as with chain rule,
∂L
^^i0,∙∙∙ ,ik-1 ,r,ik + 1 ,∙∙∙ ,im-1
J-1
I=
∂ L
,ik-1,j,ik+1,∙∙∙ ,im-ι
Mr,j
(E.5a)
∂L
∂ Mrj
I0 -1	Ik-1 -1 Ik+1 -1	Im-1 -1
E-E E-E
i0=0 ik-1=0 ik+1=0 im-1=0
_____________∂L
i0i0,0 , ∙∙∙ ,ik — 1 j,ik+1 , ∙∙∙ ,im —1
UiO,∙∙∙ ,ik-1,r,ik + 1,∙∙∙ ,im-1
(E.5b)
Again, the backpropagation equations above can be succinctly written in tensor notations.
∂L ∂L
--=——
∂V ∂ U
Xk MT
∂L
∂M
u(×0。…。×k-1
◦	×k+1
◦	×k+1
◦…OX
m-1
m-1
∂ L
∂V
(E.6a)
(E.6b)
where the time complexities for both equations are O((∩m=01 Iu)J), which is identical to the for-
ward pass in Equation D.2a (obviously since tensor multiplication is a special of tensor contraction).
Tensor convolution Recall in the definition of tensor convolution in Equation D.3a, we deliber-
ately omit the exact definition of vector convolution for generality. For simplicity, we temporarily
limit ourselves to the special case of circular convolution. In this case, tensor convolution can be
concretely defined by either equation below:
T ,i0,…,ik-1,：,ik+1, …,im-1 j0,…jl-1 jl + 1,…jn-1 =Cir fτi(0)	-,	.i,	i ) T•⑴- ɪ	4 i0 ,∙∙∙ ,ik-1 , ： ,ik + 1 ,∙∙∙ ,im-1	j0 ,jl — 1 , ： jl+ 1,jn-1 T ,i0,…,ik-1,∙,ik+1,…,im-1 j0,…jl —1 jl + 1,…jn —1 =Cir (T•⑴-Y	-	) TPi .i	i ∖ j0 ,…jl — 1 , ： jl + 1，…jn—1 J i0,∙∙∙ ,ik-1 ,∙ ,ik + 1 ,∙∙∙ ,im-1	(E.7a) (E.7b)
where Cir(∙) returns a circular matrix of the input vector. Concretely, given a vector V ∈ RI, the
circular matrix Cir(v) is defined as Cir(v)i,j = vi-j(modI). Now, the derivatives of the result tensor
T with respect to T(0) and T(1) can be obtained by matrix calculus.
dTio，…，ik —1，：，ik+1，…，im—1 jo，…jl —Ijl+ 1,…jn—1 = Cir (T(1)	)T rʌzɔ-(0)	I j0，…jl — 1，： jl + 1,…jn—1 J i0，•••，ik —八：，/ + 1，。…，im — 1 'z^-0，…，ik —1，：，ik + 1，…，im—1 j0，…jl — 1 jl + 1，…jn—1   Cir (T" (0)	) 内 ZJ-(I)	y i0,∙∙∙，ik — 1，：，ik + 1，…，im—1 J U'j0，-「jl — 1，：jl + 1，—-jn—1	(E.8a) (E.8b)
18
Under review as a conference paper at ICLR 2019
Applying chain rule to the equations above, we arrive at two lengthy equations:
J0-1	Jl-1-1 Jl+1-1	Jn-1-1 dτ^L	= E …E E …E dTi0,…，ik-1,：,ik+1,…，im-1	j0=0	jl-1=0 jl + 1=0	jn-1=0 Cir (T(I) .	. .	.	)丁 -	dL	 ∖ j0, ,jl-1 ,：,jl+1, ,jn-1 i AT	:	_ -	:	-	-	-	- U ∕i0,…,ik-1 ,∙,ik+1,…,im-1,j0,…,jl-1,jl + 1,…,jn-1 I0 -1	Ik-1 -1 Ik+1 -1	Im-1 -1 dτ<υ-^L——=E …E E …E ∂ Tj0,∙∙∙ ,jl-1,r,jl+1,∙∙∙ ,jn-1	i0=0	ik-1=0 ik+1=0	im-1=0 Cir (Ti(0)∙∙ ifc 1 ： ifc+1∙∙∙ i JT 而	dL	 i0 ,∙∙∙ ,ik-1 ,:,ik+1 ,∙∙∙ ,im-1 i i0 ,…,ik-1,:,ik + 1,…,im-1 ,j0,…,jl-1,jl + 1,…,jn-1 With notations of tensor operations, they can be greatly simplified as	(E.9a) (E.9b)
∂dLL) = SWaPaXes(∂T( ×m …×m+lτ ◦ *k ◦ ×m++l-2。…。×m+T-2)flipaxis(T⑴,l)) ∂∣l⅛=SWaPaXes( ∂T( ×o。…。×k-ι。*k。 ×k+1。…。×m-1)flipaxis(T(O),k))	(E.10a) (E.10b)
Although these backpropagation equations are derived for the special case of circular convolution,
they hold for general convolution if we replace *k by its corresponding adjoint operator (*?)丁 (and
use the unflipped versions of T(0) and T(1)). With adjoint of convolution, Equations E.10a and
E.10b can be rewritten as
M = swapaxes (IT (×m …×m+l-1。(*α。×m+l-2 …×m+Γ2) T(I)) ∂L	∂L ∂T(T) = SWaPaXeS (万(×o。…。Xk—1。(*Q 。×Rι。…。Xm-I) T(D	(E.11a) (E.11b)
where the exact form of the adjoint operator (*?)τ depends on the original definition of vector
convolution. Generally, the trick to start with circular convolution and generalize to general cases is
very useful to derive backpropagation equations for operations that convolution plays a part.
Despite varieties in the definitions of tensor convolution, the analyses of their time complexities
of backpropagation equations are identical, since the numbers of operations only differ by a con-
stant for different definitions (therefore asymptotically the same). With FFT, the number of op-
erations for these two backpropagation equations are O((^m∑^u=k 儿)(“：-1山=1 Jv)max(Ik, Jl)
log(maχ(Ik, Jl))) and O((Rm-o1u=k IU)(Rn-1,v=l Jv) maχ(Ik,Ik)log(max(Ik,Ik/，and with-
out FFT O((Rm=o,u=k Iu)(Rn=0,v=l Jv )Ik Jl) and O((Rm=o,u=k IU)(Rn=0,v=l Jv Xk Ik) respec-
tively. Different from other operations, the time complexities for forward and backward passes
are different (with circular convolution as an exception). This asymmetry can be utilized in neural
networks (WhereIk ≈ Ik ≫ Jl) to accelerate backpropagation with FFT.
Tensor outer product The derivatives ofT with respect to T(0) and T (1) can be directly observed
from the definition of tensor outer product in Equation D.4.
d Tiθ,…,im-1,j0 ,…，jn-1
∂Ti(O)	i
i0 ,…，im-1
∂ Tl「…,j0 ,∙∙∙,jn-1
d 琛 Ln-I
j0,∙∙∙ ,jn-1
∙ , i ?m — 1
(E.12a)
(E.12b)
19
Under review as a conference paper at ICLR 2019
Similar to all previously discussed operations, we first derive the backpropagation equations for T (0)
and T(1) at the entries level using standard chain rule.
∂ L d 琛 L,im-1	J0-1 =E ∙ j0 =0	Jn-1 -1 ∙∙ E :——dL	τ(1) , 乙 ∂T	二	,	,	jO ,…jn-1 jn-1 = 0 J /i0,…,im-1 ,j0,…,jn-1	(E.13a)
∂ L 盯黑jn-1二	I0-1 =E ∙∙ i0 =0	Im-1 -1 • E	F——dL	T ⑼ 乙 ∂T	二	i	i	i0,∙∙∙ ,im-1 im-1=0 Ti0,…,im-1j0,…jn-1	(E.13b)
which can then be converted to tensor notations in this paper:
∂ L	∂L m = ∂T(×m °	∙∙°×m+L) T ⑴	(E.14a)
∂T⑼二			
∂ L	E (×0。-	∙°×m=D T ⑼	(E.14b)
∂T⑴二			
The number of operations required for both equations are O((∏m=o1u=k Iu)(∏n-1 Jv)), which are
again identical to one in the forward pass in Equation D.4.
Tensor partial outer product Finally, the derivatives of T with respect to T(0) and T(1) can be
obtained from the definition of tensor partial outer product in Equation D.5a.
∂T
U ∕i0,…,ik-1,r,ik+1,…,im-1 j0 ,… jl- 1 jl + 1,… jn-1
∂Ti ⑼	i i:	i
i0,	,ik — 1,r,ik + 1,	,im — 1
∂T
U ∕i0,…,ik-1,r,ik+1,…,im-1 j0 ,… jl- 1 jl + 1,… jn-1
d琛―,…jn-1
T(1)
j0 ,∙∙∙ ,jl — 1,r,jl + 1,∙∙∙ ,jn —1
T(0)
i0 ,∙∙∙ ,ik-1 ,r,ik+1 ,∙∙∙ ,im-1
(E.15a)
(E.15b)
Again with chain rule, the backpropagation equations for T(0) and T(1) at the entries level are
	J0-1	Jl-1-1 Jl+1-1	Jn-1-1 dT(0)-^L	= E …E E …E ° TiO,…，ik-1,r,ik + 1,…，im-1	j0 = 0	jl-1=0 jl + 1=0	jn-1=0 ∂L	(1) ∂ T	i	i	i	-	-	TO,…jl-1,rjl + 1,…jn-1	( . 6a) ,i0,∙∙∙ ,ik-1 ,r,ik+1 ,…,im-1 jO ,…jl — 1 jl + 1 ,…jn-1 IO -1	Ik-1 -1 Ik+1 -1	Im-1 -1 dT(υ-^L——=E …E E …E U TjO,…，jl —1,r,jl + 1,…，jn —1	io =0	ik-1=0 ik+1=0	im-1=0 ∂T	; 二	:	二	二 二	二-TiOI ,ik-1,r,ik+1,…，im-1	(E.16b) / iO ,…,ik —1 ,r,ik+1 ,…,im—1 jO ,…jl — 1 jl + 1 ,…jn—1
Though the backpropagation equations above appear very similar to the ones for tensor contraction
in Equations E.1a and E.1b, written in tensor notations, they are almost the same as the ones for
tensor convolution in Equations E.10a andE.10b, except that (*k)「s are now replaced by 汕.
∂L ∂ T⑼ ∂L ∂ T⑴	=swapaxes (dL (Xm。…。×m+1-1。动。×m+l-2。…。×m+n-2) T⑴)(E.17a) -1	+1	n- =swapaxes (^L (×0。…。×k-1 o 0k o ×k÷1 o∙∙∙o ×m-1) T(O))	(E.17b) k-1	k+1	m-
It is not difficult to recognize the time complexity for the two equations above are O((∏U=-1 IU)
(∏n-1 v=ι Jv)), which are identical to the ones in forward pass in Equation D.5a.
Compound operations Up to this point, we have developed the derivatives and backpropagation
equations for all basic operations. In this part, we will continue to show similar techniques above
equally apply to compound operations, though slightly more involved, and derive the backpropaga-
tion equations for the examples we used in Appendix D. Though these equations are not immediately
useful in later sections, the techniques to derive them are useful for all other compound operations.
Furthermore, these induced equations, which are more complicated than their original definitions,
serve as complementary examples of compound operations to the ones in the last section.
20
Under review as a conference paper at ICLR 2019
•	Simultaneous multi-operations between two tensors. In Appendix D, we introduced a com-
pound operation (00 ◦ *1 ◦ ×2) on two tensors T(0) ∈ rr×x×s and T(I) ∈ Rr×h×s, which
returns a tensor T ∈ RR×X . Here, we recap its definitions as follows:
T = T⑼(00。*1 ◦ ×2) T⑴	(E.18a)
S-1
Tr,： = ET(0,S *T(1,S	(E.18b)
s=0
To ease the derivation, we use the trick to start with circular convolution: directly apply the chain
rule, the backpropagation equations at entries level are obtained as follows:
IL 	,-. = ∂ t(o,S	= Cir (T(1,S)T	IL τr^	(E.19a)
IL ITr(,1:,)s	= Cir (T(O,S)T	IL τr^	(E.19b)
Now we convert the equations above to tensor notations, and replace the circular convolutions
with their adjoints to obtain general backpropagation rules:
∂ L
∂ T(O)
∂ L
∂ T⑴
IL (0O ◦ *i)flipaχiS(T (I)) = IL (00 ◦(*I)T)T ⑴
∂T	∂T
IT (0O ◦ *1) flipaXiS(T (O)) = IT (00 ◦(*I)T)T (O)
(E.20a)
(E.20b)
For simplicity, we assume FFT is not used to accelerate the backpropagation equations. In
this case, the derivatives with respect to T(O) and T(I) can be computed in O(RHX'ST) and
O(RXX'ST) operations respectively. Again, the time complexities of forward and backward
passes are not the same when a (compound) tensor operation contains convolution.
•	Simultaneous operations between a tensor and a set of multiple tensors. Another compound
operation presented in Appendix D is defined between a tensor U ∈ RR×X×S anda set of tensors
T(O) ∈ Rr×p, T(I) ∈ Rh×q and T(2) ∈ Rs×t, which returns a tensor V ∈ RR×x'×p×q×t.
Again, we recap its definitions in the following:
V = U(0OT(O) *O T⑴ ×O T⑵)	(E.21a)
S-1
Vr,:,p,q,t = E T(P)(Ur,:,s * 吟)瑶)	(E.21b)
s=O
In order to derive the backpropagation rule for the core tensor U, we follow the standard procedure
to (1) first obtain its entries level representation, and (2) explicitly convert it to tensor notations
subsequently. Concretely, the backpropagation equation in both formats are displayed as follows:
P-1 Q-1 T-1
∣U⅛ = E E E T(P) (Cir ") iɪ)瑶)	(E2冽
1L = 1L ((端◦ ×2)T (O) ((*1)T ◦ ×3) t (I) (×2 ◦ ×4) t ⑵)	(E.22b)
IU	I V
Notice that the equation above is indeed simultaneous multi-operations between a tensor and
a set of multiple tensors, which combines two types of "basic" compound operations introduced
in Appendix D. In principle, we can obtain backpropagation equations for {T (O) , T(1) , T (2)} in
the same manner. However, there is a simpler way to derive them by rewriting the definition as:
V = SwapaχeS U *O1T (1) ×O2 T (2)
0OO T(O) = SwapaχeS U(O) 0OO T(O)	(E.23a)
= SwapaχeS U 0OOT (O) ×O2 T (2) *O1 T (1)
= SwapaχeS (U⑴ *1 T⑴)	(E.23b)
V = SwapaχeS U 0OOT (O) *O1 T (1) ×O2 T (2)
= SwapaχeS U(2) ×22 T(2)	(E.23c)
21
Under review as a conference paper at ICLR 2019
where U⑼,U⑴ and U⑵ are short-hand notations for U(*1T⑴ ×2 T(2)), U(00T(0) ×2 T⑵)
and U(00T⑼ *0 T(1)). With these notations, We are able to reduce these complex expressions
to basic ones, by which we can reuse the backpropagation rules derived in this section:
	∂ L ∂T(O)	∂L	O 1	3 =∂v (0O ◦ ×1 ◦ ×2	◦ X4) U(O)	
		∂L O 1	3 =∂V (0O O×1 o×2	◦ ×4) (U (*OT⑴ ×OT⑵))	(E.24a)
	∂ L ∂T⑴	=∂L (×O ◦ (*1)t ◦	×2 ◦ ×4)U (I)	
		=∂V (×o ◦ (*1)T ◦	×2 ◦ ×4) (u (0OT(O) ×OT⑵))	(E.24b)
(	∂L Y ∂T⑵)	∂L	O	1	2 =而(×O 0×1 °×2	◦ x3) u ⑵	
		∂L	O	1	2 =而(×O O×1 o×2	◦x3) (u (0OT(O) *OT⑴))	(E.24c)
The complicity of tensor operation culminates at this point: the equations above are examples
of simultaneous multi-operations on multiple tensors, which we omitted in the discussion in
Appendix D due to their complexity. Although the expressions themselves suggest particular
orderings to evaluate the compound operations, they are merely the traces of the techniques used
in deriving them. It is completely reasonable to reorganize the equations such that they can be
computed with more efficient strategies: for instance, one can verify that the following set of
equations is actually equivalent to the one above:
∂L
∂ T(O)
∂L
∂ T⑴
T
∂VL (((*O)T o×3) T⑴ ×4 T⑵)(制◦ ×1 o×3) U
∂L ((00 ◦ ×2)T(O) ×4T⑵)(×0 ◦(*I)T ◦ ×2)U
∂ll ((0O ◦ ×2)T(O) ((*O)T ◦ ×3)T(I))(XO ◦ ×1)U
(E.25a)
(E.25b)
(E.25c)
As discussed in Appendix D, the problem to find the optimal order to evaluate a compound op-
eration over multiple tensors is NP-hard in general and usually we need to resort to heuristics to
obtain a reasonably efficient algorithm. Indeed, one can verify that the second set of equations is
more efficient than the first one. For this example, interested readers are encouraged to find the
most efficient way by combinatoric search.
F	Tensor decompositions
Tensor decompositions are natural extensions of matrix factorizations for multi-dimensional ar-
rays. In this section, we will review three commonly used tensor decompositions, namely CANDE-
COMP/PARAFAC (CP) decomposition, Tucker (TK) decomposition and Tensor-train (TT) decompo-
sition. CP and Tucker decompositions are classic methods to factorize a tensor and are thoroughly
reviewed in an early survey Kolda & Bader (2009), while Tensor-train decomposition is recently
proposed in Oseledets (2011), in which the authors show that it has superior numerical stability
and scalability than the classic ones. More advanced schemes of tensor decompositions (known as
tensor networks) are reviewed in Cichocki et al. (2016).
For each of these decompositions, we will present their forms both at the entries level and in tensor
notations introduced in Appendix D. When tensor decompositions are used in neural networks, a
natural question to ask is how the backpropagation algorithm adapts to the decomposition schemes,
i.e. how the gradient of the original tensor backpropagates to its factors. In this section, we will
follow the standard procedure in Appendix E to derive the corresponding backpropagation equation
for each tensor decomposition. Different from previous works (Novikov et al., 2015; Kossaifi et al.,
2017b) that use matrix calculus following matricization, we present the backpropagation equations
directly in tensor notations, which makes our presentation concise and easy to analyze. As we
will see in the analyses, backpropagation equations through the original tensor to its factors are
22
Under review as a conference paper at ICLR 2019
computationally expensive for all decomposition schemes, therefore it is preferable to avoid explicit
computation of these equations in practice.
CP decomposition CP decomposition is a direct generalization of singular value decomposition
(SVD) which decomposes a tensor into additions of rank-1 tensors (outer product of multiple vec-
tors). Specifically, given an m-order tensor T ∈ RI0×lι× ×1m-1, CP decomposition factorizes it
into m factor matrices {M(0)}lm=-01, where M(l) ∈ RR×Il , ∀l ∈ [m], where R is called the canonical
rank of the CP decomposition, which is allowed to be larger than the Il ’s.
R-1
T	ʌ 丁 M⑼…M(m-1)
∕i0,…,im-ι — Mr Mr,i0	Mr,im-1
r=0
R-1
T = E Mf)㊈…㊈ MrmT)
r=0
=1 ×0(M⑼端…端M(mT))
(F.1a)
(F.1b)
where 1 ∈ RR is an all-ones vector of length R. With CP decomposition, T can be represented with
only (Em-I Il)R entries instead of (∏m=^-1 Il) as in the original tensor.
Now we proceed to derive the backpropagation rules for CP decomposition, i.e. the equations relat-
ing ∂L∕∂T to {∂L∕∂M(l)}m-1. In order to avoid deriving these equations from the entries level,
we first isolate the factor of interest and rewrite the definition of CP decomposition as:
T = swapaxes((M(O)…端 M(IT)端 M(I+1)…端 M(mT)) ×0 M(I))
(F.2)
swapaxes
A(l) ×00 M(l)
where we treat the first term as a constant tensor A(l). Once we reduce the compound operation to a
basic one, we can simply refer to the rule derived in Appendix E, which gives us
竺MW
= =
巴1
(×0 …。×lτo×l+ι …×m-i) A(I)
(×0 …。XlT o×l+l …×m-1)
0) 00 ∙∙∙00 M(IT) 00 M(l+1) 00 ∙∙∙00 M(m-1))	(F.3)
The number of operations to compute one such equation both is O((∏m-1 Il )R), therefore a total
number of O(m(∏m-1 Il)R) is required for all m equations. Therefore, evaluating these equations
are computationally expensive (which takes O(mR) order as many operations as the size (∏m-11l)
of the original tensor T), and should be avoided whenever possible.
Tucker decomposition Tucker decomposition provides more general factorization than CP de-
composition. Given an m-order tensor T ∈ RI0 × I1 ××Im-1, Tucker decomposition factors it into
m factor matrices {M(l)}lm=-01, where M(l) ∈ RRl ×Il , ∀l ∈ [m] and an additional m-order core
tensor C ∈ RR0 × R1 × × Rm- 1, where the Tucker ranks Rl,s are required to be smaller or equal than
the dimensions at their corresponding modes, i.e. Rl ≤ Il , ∀l ∈ [m].
R0-1	Rm-1-1
To=Σ ∙∙∙ E Cro,...,rm-1 Mr0,i0 …Mrm-I1L-I	(F.4a)
r0=0	rm-1=0
T = C (×0M(0) ×1 M(I)…×m-1 M(m-1))	(F.4b)
Notice that when Ro = •… = Rm-I = R and C is a super-diagonal tensor with all super-diagonal
entries to be ones (a.k.a. identity tensor), Tucker decomposition reduces to CP decomposition, and
therefore CP decomposition is a special case of Tucker decomposition. With Tucker decomposition,
a tensor is approximately by (∏m-1 Rl + Em-I IlRl) entries.
23
Under review as a conference paper at ICLR 2019
The backpropagation equations relating ∂L∕∂T to ∂L∕∂C and {∂L∕M(l)}m-01 can be derived Sim-
ilarly as in CP decomposition. First, we derive the equation for C at the entries level:
∂ L
dCro,…,rm-ι
I0 -1 Im-1 -1
T …T	dL— M⑼…MgT)
∂T	r0,i0	rm-1,im-
io =0	im-1=0	∕i0,…，im-1
1	(F.5)
The equation above, written in tensor notations, reveals an expression in "reversed" Tucker form:
dC = ∂T (×0(M(0))τ …×m-ι (MgT))[)	(F.6)
Although the number of operations to evaluate the equation depends on the particular order of ten-
Sor multiplications between ∂L∕∂T and {M(l))τ}m-01, the time complexity can be bounded by
O((Em-I Rι)(∏m^o1 Il)), where the worst case is achieved when Rl = Il, ∀l ∈ [m]. Regarding
the backpropagation equations for the factors {M(l)}lm=-01, we again isolate the factor of interest and
rewrite the definition of Tucker decomposition as:
T= C ×0M(0) • ∙ ∙ ×1-1 M(IT) ×l+ι M(I+1) ∙ ∙ ∙ ×m-ι M(mT))) ×l M(I)	(F.7)
= A(l) ×l M(l)
where the first term is abbreviated as a tensor A(l). Subsequently, we apply the standard backpropa-
gation rule of tensor multiplication in Appendix E and obtain the following equation:
(∂M) )τ = ∂TT (×0。…。×1-"×1+1。…。×"1) A(I)
=∂T ( ×0 (M(O))T …×l-ι (M(IT))τ ×1+1 (M(I+1))T
• • • ×m-1 (MgT))τ) (×0。…。×l-1。×l+1。…。×m-1) C (F.8)
where the second expression is equivalent to the first one, but requires fewer operations. Though the
exact number of operations depends on the order of tensor multiplications, it can be (again) bounded
by O((Em-I Rl)(∏m-011l)). Therefore, the total time complexity for all (m +1) equations (m for
{M(l)}m-01 and one for C) is bounded by O((m + 1)(Em-I Rl)(∏m=-01 Il)), which is also highly
inefficient and should be avoided in practice.
Tensor-train decomposition Tensor-train decomposition factorizes a m-order tensor into m inter-
connected low-order core tensors {T (l)}lm=-01, where T(l) ∈ RRl ×Il ×Rl+1 , l = 1, • • • , m - 2 with
T(0) ∈ RI0 ×R0, and T (m-1) ∈ RRm-1 ×Im-1 such that
R0-1	Rm-2-1
T,…,im-ι = ∑ ••• E 琛r.Ts1,r1 …Tmmɪi	(F.9a)
r0=1	rm-2=1
T = T(0) ×-1 T⑴ ×-1 …×-1 T(mT)	(F.9b)
where the Rl’s are known as Tensor-train ranks, which controls the tradeoff between the number of
parameters and accuracy of representation. With Tensor-train decomposition, a tensor is represented
by (R0I0 + Em-2 RlIlRl+1 + Rm-IIm-1) entries.
The backpropagation equations are derived following the paper Novikov et al. (2015), although
we reformat them in tensor notations. To begin with, we introduce two sets of auxiliary tensors
{P(l)}lm=-01 and{Q(l)}lm=-01 as follows:
P(l) = T(0) ×0-1 •• • ×0-1 T (l) = P(l-1) ×0-1 T (l), ∀l = 1, ••• ,m- 1	(F.10a)
Q(l) = T(l) ×0-1 • • • ×0-1 T (m-1) = T(l) ×0-1 Q(l+1), ∀l = 0, • • • ,m - 2	(F.10b)
with corner cases as P(0) = T(0) and Q(m-1) = T (m-1). Note that both {P(l)}lm=-01 and
{Q(l)}lm=-01
can be computed using dynamic programming (DP) using the recursive definitions
24
Under review as a conference paper at ICLR 2019
above. With these auxiliary tensors, the definition of Tensor-train decomposition in Equation F.9b
can be rewritten as:
T = P(l-1) ×0-1 T(l) ×0-1 Q(l+1), ∀l = 0,m- 1
(F.11)
T(0) ×-1 Q(1) = P (m-2) ×-1 T (m-1)
Applying the backpropagation rule for tensor contraction twice, the backpropagation equations can
be obtained in tensor notations as:
-∂⅛ = P (IT) (×0 ◦…。×l-1) ∂L(×2 ◦…。×m-l J Q(I+1), ∀l = 0,m - 1	(F.12)
(l)	0	l-1	1	m-l-1	,	,	.
d∂L) = ∂T (×1。…。×m-i) Q(I), dτ∂L-=P (…(X0 ◦…。×m-2) ∂T
Neglecting the expense of precomputing the auxiliary tensors {P(l)}lm=-01 and {Q(l)}lm=-01, the time
complexity for the lth equation is O(RlRι+ι(∏m=^01 Iι)), therefore the total number of operations
for all m equations is O((Ro + Em-2 RlRl+ι + Rm-I)(∏m=-1 Il)), which is again prohibitively
large for practical use.
Variants of standard decompositions In this paper, tensor decompositions are usually used in
flexible ways, i.e. we will not stick to the standard formats defined in the previous paragraphs.
Indeed, we consider tensor decomposition as a reverse mapping of tensor operations: given a tensor
T and a set of operations, the corresponding tensor decomposition aims to recover the input factors
{T(l)}lm=-01 such that the operations on these factors return a tensor approximately equal to the given
one. In the following, we demonstrate some possibilities using examples:
•	The ordering of the modes can be arbitrary. Therefore, CP decomposition of 3-order tensor
T ∈ Rl0×ll×I2 can be factorized as Ti2 = ER-01 M(0)r M* M(2), or Tɪ2 =
ERoI MrIJO MrIiI Mr2i)2, or even 工。,*=ER-01 M(O)r MrIiI Mi?广 It is easy to observe
these decompositions are equivalent to each other if factor matrices are properly transposed.
•	A tensor may be partially factorized over a subset of modes. For example, we can define a
partial Tucker decomposition which factors only the last two modes of a 4-order tensor T ∈
RI0×I1 ×I2×I3 into a core tensor C ∈ RI0×I1 ×R2×R3 and two factor matrices M(2) ∈ RR2×I2,
1∖M(3) ∈ RR3×I3 Suchthat T	一 ∖ 'R2 1 E R3- 1「.	]\M(2) ]\M(3) or altemativelν
M ∈ R such that Ti0 ,i1,i2,i3 = r2 =o r3=o i0 ,i1 ,r2,r3 Mr2,i2 Mr3 ,i3 or alternatively
T = C ×2M(2) ×3 M(3) if written in our tensor notations.
•	Multiple modes can be grouped into supermode and decomposed like a single mode. For ex-
ample, given a 6-order tensor T ∈ RI0×I1 ×I2×J0×J1×J2 can be factorized into three factors
T(o) ∈ RI0×J0×R0, T(1) ∈ RR0×I1×J1×R1 and T (2) ∈ RR1×I2×J2 such that Ti0,j0,i1,j1,i2,j2 =
ER2-1 ER3-1 τ(O)	T(I)	T(2) Crm CregnrrinrtIV	tT —	(0)(0)y-1 T (1) x-1 T (2)
r2=o r3=o Ti0,j0,r0	Tr0,i1,j1,r1	Tr1,i2,j2, or more succinctly	as T =	T	×o T ×o T .
where IO and JO are grouped into a supermode (IO, JO) and similarly for (I1, J1) and (I2, J2).
Decomp.	Notations	O(# of params.)	O(# of backprop ops.)
original	T	I	0
CP	1 ×0 (M(O)邮…㊈0 M(m-1))	ml m R	mIR
TK	C(×0M(O)…×m-1 M(m-1))	Rm + ml mm R	m2 IR
TT	T(0) ×-1 …×-1 T(m-1)	ml ^m R2	mIR2
Table 9: Summary of tensor decompositions. In this table, we summarize three types of tensor
decompositions in tensor notations, and list their numbers of parameters and time complexities to
backpropagate the gradient of a tensor T ∈ RI0 × I1 × Im -1 to its m factors (and an additional core
tensor C for Tucker decomposition). For simplicity, we assume all dimensions Il’s of T are equal,
and denote the size of T as the product of all dimensions I = ∏m-1 Il. Furthermore, We assume
all ranks Rl’s (in Tucker and Tensor-train decompositions) share the same number R.
25
Under review as a conference paper at ICLR 2019
G Plain tensor decomposition s on convolutional layer
In this section, we will show how tensor decomposition is able to compress (and accelerate) the stan-
dard convolutional layer in neural networks. In order to achieve this, we first represent the operation
of a standard convolutional layer in tensor notations. By factorizing the tensor of parameters (a.k.a.
kernel) into multiple smaller factors, compression is achieved immediately.
As we discussed in Appendix F, learning the factors through the gradient of the original tensor of
parameters is highly inefficient. In this section, we provide an alternative strategy that interacts
the input with the factors individually, in which explicit reference to the original kernel is avoided.
Therefore, our strategy also reduces the computational complexity along with compression.
For simplicity, we assume FFT is not used in computing convolutions, although we show in Ap-
pendix E that FFT can possibly speed up the backward pass Mathieu et al. (2013).
Standard convolutional layer In modern convolutional neural network (CNN), a standard convo-
lutional layer is parameterized by a 4-order kernel K ∈ RH×W ×S×T, where H and W are height
and width of the filters (which are typically equal), S and T are the number of input and output
channels respectively. A convolutional layer maps a 3-order tensor U ∈ RX×Y ×S to another 3-
order tensor V ∈ RX'×Y'×T, where X and Y are the height and width for the input feature map,
while X' and Y' are the ones for the output feature map, with the following equation:
Vx,y,t
S-1
ΣΣKi,j,s,t Ui+dx,j+dy,s
s=0 i,j
(G.1)
where d is the stride of the convolution layer and the scopes of summations over i and j are deter-
mined by the boundary conditions. Notice that the number of parameters in a standard convolutional
layer is HWST and the number of operations needed to evaluate the output V is O(HWSTXY).
With tensor notations in this paper, a standard convolutional layer can be defined abstractly as
V = u (*0 ◦*； ◦ ×2) κ
(G.2)
which states that the standard convolutional layer in fact performs a compound operation of two
tensor convolutions and one tensor contraction simultaneously between the input tensor U and the
kernel of parameters K. Following the standard procedure in Appendix E, we obtain both backprop-
agation equations in tensor notations as follows:
∂L ∂L // 0、丁 / 八丁 2、，L ∂L
∂U = ∂V ((*0) °(*1) °×3) k, ∂k
U
◦
∂L
∂V
(G.3)
It is not difficult to verify that the numbers of operations to compute these two backpropagation
equations are O(HWSTX'Y') and O(XYSTX'Y') respectively.
In the next few paragraphs, we will apply various decompositions in Appendix F as well as singular
value decomposition (SVD) on the kernel K, and derive the steps to evaluate Equation G.1 that inter-
act the input with the factors individually. Interestingly, these steps are themselves (non-standard)
convolutional layers, therefore tensor decomposition on the parameters is equivalent to decoupling
a layer in the original model into several sublayers in the compressed network, which can be imple-
mented efficiently using modern deep learning libraries. For simplicity, we assume in the analyses
of these decomposition schemes that the output feature maps have approximately the same size as
the input ones, i.e. X ≈ X', Y ≈ Y'.
SVD-convolutional layer Many researchers propose to compress a convolutional layer using
singular value decomposition, under the name of dictionary learning (Jaderberg et al., 2014;
Denton et al., 2014; Zhang et al., 2015). These methods differ in their matricization of the tensor
of parameters K, i.e. how to group the four modes into two and flatten the kernel K into a matrix.
By simple combinatorics, it is not difficult to show there are seven different types of matricization
in total. Here, we only pick to present the one by Jaderberg et al. (2014), which groups filter height
26
Under review as a conference paper at ICLR 2019
and input channels as a supermode (H, S) and filter width and output channels (W, T) as another.
R-1
Ki,j,s,t = E K(0s),r K(1r),t
r=0
K
swapaxes
×21 K(1)
(G.4a)
(G.4b)
where K(0) ∈ RH×S×R and K(1) ∈ RW ×R×T are the two factor tensors. It is easy to see an SVD-
convolutional layer has (HS + WT)R parameters in total (HSR in K(0) and WTR in K(1)). Now
we plug the Equation G.4a into G.1, and break the evaluation ofV into two steps such that only one
factor is involved at each step.
S-1
Ux(0,j)+dy,r =	Ki(,0s),r Ui+dx,j+dy,s
s=0 i
Vx,y,t
R-1
EE Kj1r),t UX0j)+dy,r
r=0 j
(G.5a)
(G.5b)
where U(0) ∈ RX ×Y ×R is the intermediate result after the first step. This two-steps procedure re-
quires only requires O((HSXY+WTX'Y)R) operations in the forwardpass (O(HSRXY) at the
first step and O(WTRX'Y) at the second). This number is smaller than O(HWSTXY) as in the
standard convolutional layer, since (HS + WT)R ≤ HWST implies (HSXY + WTX'Y)R ≤
(HS + WT )RXY ≤ HWSTXY. Therefore, SVD-convolutional layer also outperforms the stan-
dard one in efficiency. (Notice that if K is reconstructed explicitly, the forward pass requires at least
O(HWSTXY) operations.) In tensor notations, these steps can be written concisely as follows:
U⑼=U(*0 o×1) K⑼
V = u ⑼(*1 ◦ ×2) K ⑴
(G.6a)
(G.6b)
After decomposition, each operation is still a compound operation of tensor convolution and tensor
contraction, and therefore itself a convolutional layer whose filters have size either H × 1 and 1 × W.
Effectively, SVD-convolutional layer is in fact a concatenation of two convolutional layers without
nonlinearity in between. Now we proceed to derive the corresponding backpropagation equations
for these two steps following the procedure in Appendix E, which are presented in the following:
∂L /, 1、丁 Ολ	∂L∖	∂L „ √∩∖	/ ∩	, 1、丁\	∂ L
而 GO)T	o×2)	K(1), __(__	= U⑼(×0 ◦ (U)T)而
∂L
∂U
募((MT ◦ ×2) K(0)，∂KL) = U (W)To ×1)
∂ L
∂ U(O)
(G.7a)
(G.7b)
∂ L
∂ U⑼
It is not hard to show the number of operations required to obtain the derivatives with respect to U
and U(0) is O((HSX'Y+WTX'Y')R),andtheoneforthefactorsis O((XSX'Y+YTX'Y')R).
CP-convolutional layer Both Lebedev et al. (2014); Denton et al. (2014) propose to decompose
the kernel K using CP decomposition, differing at whether the height H and width W of the filters
are grouped into a supermode. For simplicity, we follow the scheme in Denton et al. (2014):
R-1
Kij,s,t = E _sor Kj _2
r=0
K = 1 ×0 (K(I) 02 K(O) 02k(2))
(G.8a)
(G.8b)
where K(0) ∈ RS×R, K(1) ∈ RH×W×R and K(2) ∈ RR×T are three factor tensors, which contain
(HW + S + T)R parameters in total. Again, plugging Equation G.8a into G.1 yields a three-steps
27
Under review as a conference paper at ICLR 2019
procedure to evaluate V :	S-1
(O) i+dx,j+dy,r =	E KsOr Ui+dx,j+dy,s	(G∙9a) s=O
Ux(1,y),r =	EKi(,1j),r Ui(+O)dx,j+dy,r	(G.9b) i,j R-1
Vx,y,t =	E K2 U(iy,r	(G.9c) r=O
where U(O) ∈ RX×Y×R and U(I) ∈ RX'×Y'×R are two intermediate tensors. Written in tensor
notations, these equations are represented as:
U(0) = U ×2 K(0)	(G.10a)
U⑴=U(O) (*0 o*1 ◦㊈2) K(I)	(G.10b)
V = U(1) ×2 K(2)	(G.10c)
After CP decomposition, the first and third steps are basic tensor multiplications on the in-
put/intermediate tensor, which are usually named (weirdly) as 1 × 1 convolutional layers despite
that no convolution is involved at all, while the second step is a compound operation of two
tensor convolutions and one partial outer product, which is known as depth-wise convolutional
layer (Chollet, 2016). The number of operations for these three steps are O(SRXY ), O(HWRXY )
and O(TRX'Y') respectively, resulting in a time complexity of O((SXY + HWXY + TX'Y')R)
for the forward pass, which is faster than the standard convolutional layer, since (HW+S+T)R ≤
HWST implies (SXY+ HWXY + TX'Y')R ≤ HWSTXY. Now we proceed to obtain their
backpropagation equations following the procedure in Appendix E:
λγ r)r	r)r	r)r
蔡=黑2(K⑵)T，∂⅛ = U⑴*士2	(W)
∂UL)=∂UL) GO)T ◦(*I)To 嗡K(I), ∂KL)=∂UL) ((*0)T ◦国)T ◦乳 2)U(I)
∂L = ∂UL) ×2 (K(O))T, ∂KL0) = U (×0 o×1) ∂UL0)	(GIIb)
The number of operations in all three steps to calculate the derivatives with respect to in-
put/intermediate tensors can be counted as O((SXY + HWX'Y' + TX'Y')R), while the one
for the factors as O((SXY+XYX'Y' + TX'Y')R).
Tucker-convolutional layer The use of Tucker decomposition to compress and accelerate convo-
lutional layers is proposed in Kim et al. (2015). Despite the name of Tucker decomposition, they in
fact suggest a partial Tucker decomposition, which only factorizes the modes over the numbers of
input/output filters and keeps the other two modes for filter height/width untouched.
Rs-1 Rt-1
Ki,j,s,t = EE KsOrsK(,黑,rt Kr?	(G.12a)
rs=O rt=O
K = K(I) (×2(K(O))t ×3 K⑵)	(G.12b)
where K(O) ∈ RS×Rs, K(1) ∈ RH×W ×Rs ×Rt and K(2) ∈ RRt×T are three factor tensors, with a
total of (SRs +HWRsRt +RtT) parameters. All that follow are identical to the ones for SVD and
CP layers. A three-steps forward pass procedure is obtained by plugging Equation G.12a in G.1.
S-1
Ui+dx,j+dy,rs = E^ KsOrs Ui+dx,j+dy,s	(G.13a)
s=O
Rs-1
Ux(1,y),rt =	Ki(,1j),rs,rt Ui(+O)dx,j+dy,rs	(G.13b)
rs=O i,j
Rt-1
Vx,y,t =	Kr(2t,)t Ux(1,y),rt	(G.13c)
rt=O
28
Under review as a conference paper at ICLR 2019
where U(0) ∈ RX×Y ×Rs and U(1) ∈ RX ×Y ×Rt are two intermediate tensors. These three steps,
with number of operations of O(RsSXY),O(HWRsRtXY) and O(RtTX'Y') respectively, leads
to a total time complexity of O(sRsXY + HWRsRtXY + RtTX'Y') for the forward pass. Like
CP and SVD convolutional layers, Tucker-convolutional layer is faster than the standard convo-
lutional layer, since SRs + HWRsRt + RtT ≤ HWST implies SRsXY + HWRsRtXY +
RtTX'Y' ≤ HWSTXY. These equations, again, can be concisely written in tensor notations:
U(0) =U ×2 K(0)
u ⑴=u(0) (*0 ◦*； ◦ ×2) K ⑴
V = U(1) ×2 K(2)
(G.14a)
(G.14b)
(G.14c)
where the first and the third steps are two 1 × 1 convolutional layers, and the second step is itself a
standard convolutional layer, which only differs from CP-convolutional layer at the second step. For
completeness, we summarize all backpropagation equations in the following:
		∂ L	∂L	×2(K(2))T,	∂ L	=U(I) (×0 o×1) IL	(G.15a)
		∂ U⑴	=-- ∂V		∂ K⑵二		
∂L	∂ L	((*0)丁	◦ (*1)T	。*2) K(I),	∂ L	=券（而。而）U	(1)	(G.15b)
∂U(O)二	二∂ U⑴				∂K(I);		
		∂L ——= ∂ U	∂ L ∂ U(O)	×2(K(O))T,	∂ L ∂K(O);	=U (×0 °×1) ∂UL)	(G.15c)
Referring to the CP-convolutional layer, the time complexity for the backward pass is obtained with
slight modification: the number of operations for the input/intermediate tensors is O(SRsXY +
HWRsRtX'Y '+RtTX'Y '), and the one for factors is O(SRsXY +XY HWX'Y '+RtTX'Y ').
Tensor-train-convolutional layer Lastly, we propose to apply Tensor-train decomposition to com-
press a convolutional layer. However, naive Tensor-train decomposition on the kernel K may give
inferior results (Garipov et al., 2016), and careful reordering of the modes is necessary. In this pa-
per, we propose to reorder the modes as (input channels S, filter height H, filter width W, output
channels T), and decompose the kernel as
Rs-1 R-1 Rt-1
Ki,j,s,t = Σ Σ Σ KS0rs Krs,i,r Kj,rtK3,t	(G.16a)
rs=0 r=0 rt=0
K = swapaxes (K(O) ×-1 K⑴ ×-1 K⑵ ×-1 K⑶)	(G.16b)
where K(0) ∈ RS×Rs, K(1) ∈ RRs×H×R, K(2) ∈ RR×W×Rt and K(3) ∈ RRt×T are factors, which
require (SRs+H RsR+W RRt+RtT). Once we plug the decomposition scheme in Equation G.16a
into G.1, the evaluation of V is decoupled into four steps , with number of operations as O(Rs SXY),
O(HRsRXY), O(W RRtX 'Y) and O(RtT X 'Y ') respectively, resulting in a total time complexity
as O(RsSXY + HRsRXY + WRRTX'Y + RtTX'Y') in the forward pass.
S-1
Ui+dx,j+dy,rs =	Ks(0,r)s Ui+dx,j+dy,s	(G.17a)
s=0
Rs-1
Ux,j+dy,r = EEKrs ,i,r Ui+dx,j+dy,rs	(G.17b)
rs=0 i
R-1
UX2) rt = EE Krj 〜U(1,)+⅛ r	(G.17c)
x,y ,rt	r,j,rt x,j+dy,r
r=0 j
Rt-1
Vx,y,t =	K(r3t,)t Ux(2,y),rt	(G.17d)
rt=0
29
Under review as a conference paper at ICLR 2019
where U(0) ∈ RX×Y×Rs, U⑴ ∈ RX'×Y×R and U⑵ ∈ RX'×Y'×Rt are three intermediate tensors.
In tensor notations, these equations can be rewritten as as follows:
U(0) =U ×2 K(0)	(G.18a)
U⑴=U⑼(*1 ◦ ×2) K(I)	(G.18b)
U⑵=U⑴(*1 o×2) K⑵	(G.18c)
V = U(2) ×2 K(3)	(G.18d)
Tensor-train-convolutional layer is concatenation of four sub-layers, where the first and the last
ones are 1 × 1 convolutional layers, while the other two in between are convolutional layers with
rectangular kernels. In fact, Tensor-train-convolutional layer can either be interpreted as (1) a
Tucker-convolutional layer where the second sublayer is further compressed by a SVD, or (2) a
SVD-convolutional layer where both factors are further decomposed again by SVD. Referring to the
previous results, the corresponding backpropagation equations are easily derived as
	∂L	∂L =--	×2	(K(3))τ,	∂ L	=U⑵* =U⑴(×0	1 ∂L O×l)而		(G.19a)
∂L	∂U(2) ∂ L	∂V			∂ K⑶二 ∂ L			∂L	
		((*0)τ	◦	×2) K⑵,			◦ (=1)τ)		(G.19b)
∂U⑴二 ∂L	=∂U(2) ∂ L				∂ K⑵二 ∂ L			∂U(2) ∂L	
		((*0)τ ∂ L	◦ ×2	×2) K(I), (K(O))τ,		=U(O) <7=0	)τ ◦ ×1) <1) U ∂U⑼		(G.19c) (G.19d)
∂U(O)二	=∂ U⑴ ∂L ——=				∂ K⑵二 ∂ L	U (*O =U(×0◦)		∂U(I)	
	∂ U	∂ U(O)			∂K(O)二				
Similar to all previous layers, the time complexities for input/intermediate tensors and factors can be
CalcUIatedas O(SRsXY+HRsRX'Y+WRtTX'Y'+RtTX'Y') and O(SRsXY+XRsRX'Y+
YRtRX'Y' + RtTX'Y') respectively.
Decomp.	O(# of params.) O(# of forward ops.)	O(# of backward ops. for inputs) O(# of backward ops. for params.)
original	HWST HWSTXY	HWSTX 'Y' XYSTX 'Y'
SVD	(HS + WT )R HSXY + WTX 'Y )R	(HSX 'Y + WTX 'Y ')R (XSX 'Y + YTX 'Y ')R
CP	(HW + S + T )R (SXY + HWXY + TX 'Y ')R	-(SXY + HWX 'Y' + TX 'Y ')R (SXY + XYX 'Y' + TX 'Y ')R
TK	(HWRsRt+ SRs + RtT) (HWRsRt XY+ SRsXY + RtTX 'Y')	(HWRsRtX 'Y'+ SRsXY + RtTX 'Y') (XYRsRtX 'Y'+ SRsXY + RtTX 'Y')
TT	(SRs + HRsR+ WRtR + RtT) (SRs	+ HRsRXY+ WRtRX 'Y + RtTX 'Y')	(SRsXY + HRsRX 'Y +- WRtTX 'Y' + RtTX 'Y') (SRsXY + XRsRXY+ YRtRX 'Y' + RtTX 'Y')
Table 10: Summary of plain tensor decomposition on convolutional layer. We list the nUmber
of parameters and the nUmber of operations reqUired by forward/backward passes for varioUs plain
tensor decomposition on convolUtional layer. For reference, a standard convolUtional layer maps a
set of S featUre maps with height X and width Y, to another set of T featUre maps with height X '
and width Y'. All filters in the convolUtional layer share the same height H and width W.
H Reshaped tensor decomposition s on dense layer
The operation of dense layer (a.k.a. fUlly connected layer) in neUral network can be simply charac-
terized by a matrix-vector mUltiplication, which maps a vector u ∈ RS to another vector v ∈ RT,
30
Under review as a conference paper at ICLR 2019
where S and T are the number of units for the input and output respectively.
S-1
vt = Ks,tus
s=0
v=Ku
(H.1a)
(H.1b)
It is easy to see that a dense layer is parameterized by a matrix K with ST parameters, and evaluating
the output v requires O(ST ) operations. With a matrix at hand, the simplest compression is via
singular value decomposition (SVD), which decomposes K into multiplication of two matrices K =
P Q, where P ∈ RS×R, Q ∈ RR×T with R ≤ min(S, T ). With SVD decomposition, the number
of parameters is reduced from ST to ((S+T)R) and time complexity from O(ST) to O((S + T)R).
Inspired by the intuition that invariant structures can be exploited by tensor decompositions in
Section 3, we tensorize the matrix K into a tensor K ∈ RS0 ×∙∙∙×Sm- 1 ×T0 ×∙∙∙×Tm- 1 Such that
S = ∏m-01 Si, T = ∏m-01 Tl and Vec(K) = Vec(K). Correspondingly, We reshape the in-
put/output u, V into U ∈ RS0× ×Sm-1, V ∈ RT0 × ×Tm-1 such that Vec(U) = u, Vec(V) = V and
present an (uncompressed) tensorized dense layer as follows:
Vto,…，tm-1
S0-1	Sm-1-1
T …T Kso …S Ito …t 1 Uso …S 1
/ /	/ / s0, ,sm-1,t0, ,tm-1 s0, ,sm-1
s0 =0 sm-1 =0
V = U(×0。…。×m-1) K
(H.2a)
(H.2b)
Therefore, a tensorized dense layer, parameterized by a 2m-order tensor K, maps an m-order tensor
U to another m-order tensor V . It is straightforward to observe that the tensorized dense layer is
mathematically equivalent is to the dense layer in Equation H.1a. Correspondingly, its backpropaga-
tion equations can be obtained by simply reshaping the ones for standard dense layer:
∂L
∂u
5∂L ∂L	∂∂C∖ T
JC  .   U I   I
∂ V ∂ K <∂ V )
∂L
∂L
∂U
2m-1 ∂L ∂L	∂L
K(Xm …Xm-11)而灰=UX 而
(H.3)
(H.4)
U X K
In the section, we will compress the tensorized dense layer by decomposing the kernel K into mul-
tiple smaller factors. As we will see, the schemes of tensor decompositions used in this section are
not as straightforward as in Appendix F and G, and our principles in their designs are analyzed at
the end of this section. Again, learning the the factors through the gradient of the original tensor is
extremely costly, therefore a multi-steps procedure to compute the output by interacting the input
with the factors individually is desirable. For simplicity of analyses, we will assume for the rest of
the paper that S and T are factored evenly, that is Sl ≈ S ~m, Tl ≈ T 'm, ∀l ∈ [m] and all ranks are
equal to a single number R.
r-CP-dense layer Obviously, the simplest way to factorize K is to perform naive CP decompo-
sition over all 2m modes without grouping any supermode. However, such naive decomposition
leads to significant loss of information, as we discuss at the end of this section. In this paper, we
instead propose to factor the kernel K by grouping (Sl , Tl)’s as supermodes. Concretely, the tensor
of parameters K is decomposed as:
R-1
Ksn ∙∙∙ S	t tn ∙∙∙ t I= T K(0) t …KWIt	(H.5a)
s0, ,sm-1,t0, ,tm —1	/ / r,so,to	r,sm-1,tm-1	∖	'
r=0
K = swapaxes (1 X0 (K⑼ X0 …X0 K(mT))	(H.5b)
where K(l) ∈ RR×Sl ×Tl, ∀l ∈ [m] are m factors and R is the canonical rank that controls the
tradeoff between the number of parameters and the fidelity of representation. Therefore, the total
number of parameters of a r-CP-dense layer is approximately Em-I Sl TlR ≈ m(ST) ^1 R, which is
significantly smaller than ST given that R is reasonably small. The next step to derive the sequential
procedure mirrors the ones in all schemes in Appendix G, by plugging the Equation H.5a into H.2a,
31
Under review as a conference paper at ICLR 2019
we arrive at a multi-steps procedure for the forward pass.
uT,S0,∙∙∙ ,Sm-1	=Uso,…,Sm-		(H.6a)
U (l+1) r,sl + 1 ,…,sm- 1 ,t0,…，tl	Sl -1 —E K(D U(l) =/ r '5,sl ,tl Sr,sl ,•• sl =0	• ,sm —1,t0,∙∙∙ ,tl — 1	(H.6b)
Vt0,tl ,…，tm-1	R-1 =E UrmI)... t 1 r 1 ,t0 ,	,tm—1 r=0		(H.6c)
where the m intermediate results U Ill ∈ RR×sl×…Sm-1×T0×…×τl-1,∀ι ∈ [m] are (m+1)-orderten-
sors, and the (l + 1)th step above (i.e. the equation interacting U(l) with K(I)) requires O((∏k=o Sk)
(Πm=ι1 Tk)R) operations. The number can be bounded by O(max(S, T)1+mmR), and equality is
achieved when Sι = Tl = S~m = T~m. Since the first and last steps require O(SR) and O(TR) that
are negligible compared to the other m steps, therefore the total number of operations is bounded by
O(m max(S, T)1+ m R). These steps in tensor notations are presented as follows:
U(0) = 1 XU
(H.7a)
U(l+1) = U(l)(00 ◦ ×1) K(I)	(H.7b)
V = 1 ×00 U(m)	(H.7c)
Following the procedure in Appendix E, their backpropagation equations are obtained as:
舒=K(I)(00 °×-ι) ∂U‰	(H.8a)
募=U(l)(00 o×2。…。×m-1) ∂U∂-	(H∙8b)
∂L _ l ∂L ∂L _ l o ∂L
∂U(m)= 0 ∂V, ∂U = ×0 ∂U(0)
As we discussed in Appendix E, if a compound operation does not contain convolution, the time
complexity of backpropagation is identical to the forward pass. Therefore, we claim the number of
operations required for backward pass is also bounded by O(m max(S, T)1+ mm R).
r-Tucker-dense layer The application of TK decomposition is rather straightforward, which fac-
tors the tensor of parameters K exactly the same as in Appendix F.
ʌɪʃsθ ,∙∙∙ ,sm — 1 ,t0 ,∙∙∙ ,tm —1
R0s -1	Rsm-1 -1 Rt0 -1	Rtm-1 -1
E…E E-E
r0s=0	rms -1=0 r0t=0	rmt -1=0
P(0)	P(m-1) C	Q(0)	Q(m-1)
PS0 ,r0 …Psm-I ,rm-ι〜s,…，rm-ι,r0,…，*-ι Qrt ,t0 ….Q*->tm-ι
K = C (×0(P(0) )T …×m-1 (P(mT))T Xm Q(0)∙∙∙ ×2m-i Q(m^1))
(H.9a)
(H.9b)
where P(I) ∈ RSl×Rs,∀l ∈ [m] are named as input factors, C ∈ Rr0×…×Rm-ι×R0×…×Rm-1 as
core factor, and lastly Q(l) ∈ RRlt×Tl , ∀l ∈ [m] as output factors. Similar to Tucker-convolutional
layer, the procedure to evaluate the result V can be broken into three steps, by plugging Equa-
tion H.9a in H.2a.
	S0 -1	Sm-1 -1			
UrO)…rs	二 1 0 , ,r m-1	=E -	• E P(O) S … 乙	s0,rs	(m-1) s	rs	s0 • Sm-I，' m-1 0,	•• ,sm-1	(H.10a)
	s0=0	sm-1 =0			
	Rs0 -1	Rsm-1 -1			
UrI).. rt	= 1 0 , ,r m-1	=E ∙	•• Σ Crs，…,rm	-1,rt，…rm-1 uro,∙∙	rs , m-1	(H.10b)
	r0s=0	rms -1=0			
	Rt0 -1	Rtm-1 -1			
Vt0 ,••• ,tm-1	=E ∙	• • E q(0), •• r0t,t0	• Q(mt -1)	U (t1) rmt -1,tm-1 r0t,	••• ,rmt -1	(H.10c)
	r0t=0	rmt -1=0			
32
Under review as a conference paper at ICLR 2019
where the first and last steps are compound operations between a tensor and a set of multiple ten-
sors, while the middle step is a multi-operations between two tensors. Under the assumptions that
Sl ≈ Sm,Ti ≈ Tm,R = Rt = R,∀l ∈ [m], the order to contract the factors in the first and
last steps makes no difference, therefore we assume the order follows the indices without loss of
generality. With this strategy, the contraction with the lth factor takes O((∏k=o Rk)(∏m=l1 Sk))
operations, which is roughly O(Rl+1 Smmml) and can be further bounded by O(SR) since R ≤ Sm1
by the definition of Tucker decomposition: therefore, the time complexity to contract all m factors
is at most O(mSR). Likewise, the number of operations for the last step can also be bounded by
O(mT R). Lastly, it is easy to see the middle step needs O(R2m ) operations, therefore leads to a
total time complexity of O(m(S + T)R + R2m ) for the three-step procedure. In tensor notations,
these equations can be concisely written as
U⑼=U (×oP⑼ …×m-ι P"1)
U⑴=U⑼(×0。…”mm- 1) C
V = U⑴(×oQ⑼…×m-ι Q(m-1))
(H.11a)
(H.11b)
(H.11c)
Though compound in nature, the procedure to derive their backpropagation rules are pretty straight-
forward: notice the equations for the first and last steps have the exactly the same form as standard
Tucker decomposition in Appendix F. Therefore, we can simply modify the variable names therein
to obtain the backpropagation equations for these two steps.
∂ U ⑴=∂V (×0(Q(O)广…×m-1 (Q(mT))T)	(H.12a)
∂ ∂L「一 ∂L / O	i-1 i+1	m-n
(∂QW) = ∂V(×0 ◦…。×l-1 0×l+1 ◦…° ×m-1)
(U⑴(×oQ⑼…×l-ι Q(IT) ×1+1 Q(I+1)…×m-ι QgT)))	(H.12b)
令=∂UL) (×0(P(O) )T …×m-1 (PemT)T)	(H.12c)
∂L	∂L	O	l-1 l+1	m-1
(湎)=∂U(0) (×0 ◦…°×l-1 °×l+1 ◦…° ×mτ)
(U 卜OP(O)…×l-1 P(IT) ×l+1 P(l+1)…×m-1 P(mT)))	(H.12d)
The step in the middle is itself a tensorized layer defined in Equation H.2b, therefore its backpropa-
gation rules can be obtained by renaming the variable in Equations H.4.
∂L	m	2m-1	∂L
∂UW = C(×° ◦…°×m-1)E
∂L	(O)	∂L
∂C = U 0 而
(H.13a)
(H.13b)
Despite their technical complicity, we can resort to conclusion that the complexities for forward
and backward passes are the same for operations without convolution, and claim that the number of
operations required for the backpropagation equations above is bounded by O(m(S + T)R+ R2m ).
r-Tensor-train-dense layer The layer presented in this part follows closely the pioneering work
in compressing network using tensor decompositions (Novikov et al., 2015), except that we replace
the backpropagation algorithm in the original paper (as discussed in Appendix F) with a multi-
step procedure similar to all other layers in this paper. With the replacement, the efficiency of
the backward is greatly improved compared to original design. Similar to r-CP-dense layer, we
will group (Sl, Tl)’s as supermodes and decomposed the kernel K by Tensor-train decomposition
following the order of their indices:
s0,s0 ,''' ,sm — 1 ,t0 ,∙∙∙ ,tm —1
R0-1
E -
r0=O
rm-2=O
K(m-1)
rm-2,sm-1,tm-1
(H.14a)
K = swapaxes (K(O) ×-1 K(I) ×-1 •一 ×-1 K(m-1))
(H.14b)
33
Under review as a conference paper at ICLR 2019
where the factor tensors are Klll ∈ RRι-ι×sι×τι×Rι,∀ι = 1,…,m 一 2, with two corner cases
K(0) ∈ RS0×T0×R0 and K(m-1) ∈ RRm-2×Sm-1 ×Tm-1 . The number of parameters in its lth factor
K(I) is approximately (ST)m1 R2, therefore the layer has O(m(ST) m1 R2) parameters in total. For
aestheticism, we add singleton mode R-ι = 1 to U, K(O) such that U ∈ RS0×∙∙∙×sm-1×R-1, K(O) ∈
RR-1×S0×T0 ×R0, and rename U as U(0). Now insert the Equation H.14a into H.2a and expand
accordingly, we obtain an (m + 1)-steps procedure to evaluate the output V:
uSl+ι1,)
∙∙ ,Sm-1,tθ,∙∙∙ ,tl,ri
Rl-1-1 Sl-1
Kr(ll)-1,sl,tl,rl
rl-1=O sl=O
∙∙ ,sm-1,t0,∙∙∙ ,tl-1,rl-1
(H.15)
where the last result U(m) is equal to the final output V, and the other m tensors U(l)’s are interme-
diate results. Similar to r-CP-dense layer, the number of operations at the lth step is O((∏k=o Sk)
(∏m=-1 TI)RI-ιRι) and can be bounded by O(max(S, T)1+m⅛ R2), therefore the time complexity
for all m-steps is bounded by O(m max(S, T)1+m1 R2). These steps very simple in tensor notations.
U(l+1l = U⑴(×0 ◦ ×-1) K(I)	(H.16)
Observe that the operations in the forward pass are entirely tensor contractions, therefore their back-
propagation equations are easily derived following the procedure in Appendix E.
薪=K(I) (×L of) ∂U‰)	(HW
∂KL) = SWaPaXeS (U (I) (×0。…。×m-2) ∂U‰))	(H.17b)
In the analysis of the backward pass, we can again take advantage of the argument that the forward
and backward passes share the same number of operations. Therefore, we claim the time complexity
for backpropagation is bounded by O(m max(S,T)1+ u R2).
Relation to tensor contraction layer: In Kossaifi et al. (2017a), the authors propose a novel ten-
sor contraction layer, which takes a tensor of arbitrary order as input and return a tensor of the same
order. Formally, a tensor contraction layer, parameterized by a set of m matrices {M(l)})lm=-O1, with
M(I) ∈ RSl×Tl,∀l ∈ [m], maps a m-order tensor U ∈ Rs0×…×sm-1 to another m-order tensor
V ∈ RT0 × …Tm-ISUChthat
S0-1	Sm-1-1
Vt0,…"-I = E^^^ E "SO,…,Sm-1 Ms…Msm-1L-I	(H.侬)
s0 =O	sm-1 =O
V = u(×oM(0)…×m-1 M(m-1))	(H.18b)
Itis not difficult to observe that the tensor contraction layer is in fact special case of r-CP-dense layer
where the kernel is restricted to rank-1, that is KS0,…,sm-i,t0,…,tm-1 = Ms^。…Msm-I号…，or
equivalently K = M(O) 0∙∙∙0 M(m-1) in our tensor notations.
Relation to tensor regression layer: Along with the tensor contraction layer, tensor regression
layer is also proposed in Kossaifi et al. (2017b), which takes a tensor of arbitrary order as input and
maps it to a scalar. Formally, given an m-order tensor U ∈ RS0 × × Sm-1, it is reduced to a scalar V
by contracting of all the modes with another tensor of the same size K ∈ RS0 × ×Sm-1.
S0-1	Sm-1-1
V = ∑∙∙∙ E Us0,…,…
s0=O	sm-1=O
Ks
0 ,∙∙∙ ,sm-1
V=U(XO ◦ ×1。…。×m-i)K
(H.19a)
(H.19b)
where the tensor K is stored in Tucker-format as in Equation F.4a. Therefore, the tensor regression
layer is effectively parameterized by a set of matrices {M(l)}lm=-O1, M(l) ∈ RSl ×Rl , ∀l ∈ [m], with
34
Under review as a conference paper at ICLR 2019
an additional core tensor C ∈ RR0 × × RmT. Therefore the definition of tensor regression layer in
Equation H.19a can also be rephrased as
S0-1	Sm-1-1
V = V … V UseI …S M M(O)rn …M(m-1)	, CrCI …r 1	(H.20a)
/ /	/ / s 0 , ,sm- 1	s0,r0	sm-1 ,rm-1 1 0, ,rm-1	\	'
s0=0	sm-1=0
=U (×0M ⑼…×m-ι M(mT)) (×0。…。×m-1) C	(H.20b)
Now we are able to observe that the tensor regression layer is indeed a special case of r-Tucker-
dense layer where the input factors P(l) = M(l), ∀l ∈ [m], while the output factors Q(l)’s are
simply scalar 1’s with Tl = 1, ∀l ∈ [m].
Comments on the designs: As we can observe, the design of r-Tucker-dense is different from
the other two layers using CP and Tensor-train decompositions, in which (Sl, Tl)’s are first grouped
into supermodes before factorization. Indeed, it is a major drawback in the design of r-Tucker-dense
layer: notice that the first intermediate result U(0) obtained in the forward pass is a tensor of size
Rs XRs ∙∙∙× Rm-ι, which becomes very tiny if the kernel K is aggressively compressed. Therefore,
the size of the intermediate tensor poses an "information bottleneck", causing significant loss during
the forward pass, which is verified by our experimental results in Section app:experiments. There-
fore, the use of r-Tucker-dense layer is not recommended when we expect excessive compression
rate. On the other hand, by grouping (Sl, Tl)’s as supermodes in r-CP-dense layer and r-Tensor-train-
dense layer, all intermediate tensors U(l)’s have similar size as the input, therefore the bottleneck in
r-Tucker-dense layer is completely avoided.
But why do we not group (Sl, Tl)’s together in the design of Tucker-dense layer at the beginning?
In theory, we are for sure able to factorize the kernel as
R0-1	Rm-1-1
K = E …E Crθ,∙∙∙ ,rm-1 Krθ,S0,t0 …Krm-l,Sm-1 ,tm-1	321)
r0=1	rm-1=0
However, the contractions among the input and the factors become problematic: (1) interacting the
input with the factors Ka),s yields an intermediate tensor of size To × ∙∙∙× Tm-I × Ro × ∙∙∙×
Rm-1, which is too large to fit into memory; (2) while reconstructing the kernel K from K(l)’s
and subsequently invoking the Equation H.2a will make the time complexity for backward pass
intractable as we discussed in Appendix F. Therefore, we have to abandon this attempting design, in
order to maintain a reasonable time complexity.
As a compensation for the possible loss, the current design of Tucker-dense layer actually has one
benefit over the other two layers: the numbers of operations for the backward pass remains at the
same order as the number of parameters, while the number of operations required by r-CP-dense and
r-Tensor-train-dense layers are orders higher. As a result, r-Tucker-dense layer is much faster than
r-CP-dense and r-Tensor-train-dense layers at the same compression rate. Therefore, r-Tucker-dense
layer is more desirable if we value speed over accuracy and the compression rate is not too high.
Decomp.	O(# of params.)	O(# of forward/backprop ops.)
original	ST	ST
SVD	(S + T )R		(S + T )R
r-CP-	m(ST) mm R	m max(S, T)1+mm R
r-TK	m(S m1 + T m1 )R + R2m	m(S + T )R + R2m
r-TT	m(ST) mm R2	m max(S,T )1+mm R2
Table 11: Summary of reshaped tensor decomposition on dense layer. In this table, we list
the numbers of parameters and time complexities of forward/backward passes required by various
reshaped tensor decomposition s on dense layer. For simplicity, we assume that the number of input
units S and outputs units T are factorized evenly, i.e. Sl = Smm, Tl = Tm1, ∀l ∈ [m] and all ranks
(in r-TK and r-TT) share the same number R, i.e. Rl = R, ∀l ∈ [m].
35
Under review as a conference paper at ICLR 2019
I Reshaped tensor decomposition s on convolutional layer
In Appendix H, we tensorize the parameters into higher-order tensor in order to exploit their in-
variance structures. It is tempting to extend the same idea to convolutional layer such that simi-
lar structures can be discovered. In the section, we propose several additional convolutional lay-
ers based on the same technique as in Appendix H: the input and output tensors are folded as
U ∈ RX×Y×s0×…×sm-1 and V ∈ Rx'×γ'×T0×∙∙∙×τm-1, while the tensor of parameters are re-
shaped into K ∈ RH × W × S0 ×∙∙∙× Sm- 1 × T0 × × Tm-1. Similar to the tensorized dense layer in EqUa-
tion H.2a, we define a (uncompressed) tensorized convolutional layer equivalent to Equation G.1:
“x,y,t0, ∙∙∙ ,tm-1
S0-1	Sm-1-1
∑∙∙∙ E EKi,j,so …，sm-ι,to,…，tm-1 Ui+dχ,j+dy,so,…,sm-1
s0=0	sm-1 =0 i,j
V = u (*0 ◦*； o×2 ◦…◦ ×m+1) κ
(I.1a)
(I.1b)
Their corresponding backpropagation eqUations are then easily obtained by reshaping the ones for
standard convolUtional layer in EqUation G.3.
dL	O 0( S^∖T C <ι,1 ʌɪ C m m+2 XZ 2m⅛1λ dL
∂U = K Go) ◦ (*I) O×2+ …×m+∏∂V
∂Li((qTC 仙Tl ∂L
∂K = U ((*0) °(*1) )∂V
(I.2a)
(I.2b)
What follows are almost replications of Appendix G and H: applying tensor decompositions in
Appendix F to the kernel K and derive the corresponding mUlti-steps procedUres. As we shall
expect, all layers in this section mimic their coUnterparts in Appendix H (in fact they will redUce to
coUnterpart layers when original layer is 1 × 1 convolUtional layer). Therefore in this section, we
will borrow resUlts from last section whenever possible and only emphasize their differences.
r-CP-convolutional layer In this part, CP decomposition is Used in a similar way as in r-CP-
dense layer, which decomposes the kernel K groUping (Sl , Tl)’s as sUpermodes, and (H, W ) as an
additional sUpermode. Specifically, the tensor K takes the form as
R-1
Kij S.…S 1 tn ∙∙∙ t I= V K(O) t …K(m-1) t	K(m)	(I.3a)
i,j,s0, ,sm- 1 ,t0, ,tm- 1	/ / r,so ,to	r,sm- 1 ,tm- 1 r,i,j	`	'
r=0
K = swapaxes(1 ×0 (K(O) 00 ••• 00 K(m)))	(I.3b)
where K(l) ∈ RR×Sl ×Tl , ∀l ∈ [m] and K(m) ∈ RR×H×W are (m + 1) factors. Notice that EqUa-
tion I.3a only differs from EqUation H.5a in r-CP-dense layer by an additional factor K(m), therefore
has HWR more parameters and reaches O(m(ST)m R + HWR) in total. Accordingly, the multi-
steps procedUre to evalUate the oUtpUt V now has one extra step at the end, and the (m + 2)-steps
algorithm is presented at the entries level as follows:
U(0)	U
ur,i+dx,j+dy,S0,-∙∙ ,sm-1	ui+dχ,j+dy,s0,…，sm-1
Sl-1
U (1+1)	= V K(1)	U (I)
υιi"+,^dj+^dy,s^ + -∙,∙,∙ ,sm-1,to,∙∙∙ ,tι / r	r,sl ,tl υir,i+dx,j+dy,sι,∙∙∙ ,sm-1,to,∙∙∙ ,tι-1
Sl=0
R-1
V	— VV K(m) U(m)
Vx,y,t0,…,tm-1	Z-/ Kr, Kr,i,j Ur,i+dx,j+dy,t0,…，t
r=0 i,j
(I.4a)
(I.4b)
(I.4c)
where U(I) ∈ RR×Sl ×…×Sm+1×T0 ×…×Tl-1 ,∀ι ∈ [m] are m intermediate tensors. Notice that the
order to interact the (m + 1) factors is arbitrary, that is the convolUtional factor U(m) can be convo-
lUted over at any step dUring the forward pass. In this paper, we place the convolUtional factor U(m)
to the end simply for implementational convenience: it is not difficUlt to recognize that the last step
is a 3D-convolutional layer with R inpUt featUre volUmes and one oUtpUt featUre volUme, ifwe treat
the nUmber of featUre maps T as depth of the featUre volUmes. The time complexity in the forward
36
Under review as a conference paper at ICLR 2019
pass are easily obtained through the results of r-CP-dense layer: compared to r-CP-dense layer, each
of the existing m steps will be scaled by a factor of XY , while the additional last step requires
O(HWTRXY ) operations. Therefore, the total number of operations for r-CP-convolutional layer
is O(m max(S,T)1+ m RXY + HWTRXY). In tensor notations, these steps are rephrased as:
U ⑼=1 ㊈ U	(I.5a)
U(I+1)= U(I)传0 ◦ ×3) K(I)	(I.5b)
V = U(m) (×0 0*1 o*2) K(m)	(I.5c)
The backpropagation equations are also very similar to their r-CP-dense layer counterparts. For
completeness, we list all of them in the following:
∂ L	= Km ((*O)T ◦ (*2)T	∂L		(I.6a)
∂U(m)二		∂ ∂V	∂L	
∂ L	=u (m) ((*O)T ◦ (*2)τ	乂3	m+2) ×2 ◦…。×m+1)		(I.6b)
dK(m).			∂V	
∂L	=swapaxes (K(I)(以 <	0×-1) ∂U⅛π)		(I.6c)
∂U(I)二				
∂L	二 U(I)(0°° ◦ 01 ◦ 02 ◦	×3 o∙..o×m+3)	∂L	(I.6d)
∂K(I)二			∂U (l+1)	
∂L	1× ×o 旦 ×o ∂U⑼			
——= ∂ U				(I.6e)
In the analyses of these backpropagation equations, it is again convenient to make connections to
their r-CP-dense layer counterparts: the time complexities for the first m steps are scaled by XY,
while the last step of 3D-convolutional layer requires O(HWTRX'Y') operations for the derivative
with respect to U(m), and O(XYTRX'Y') for the gradient of Klm).
r-Tucker-convolutional layer Incorporating the features from both Tucker-convolutional layer in
Appendix G and r-Tucker-dense layer in Appendix H, we propose to apply partial Tucker decompo-
sition on the tensorized kernel K over all modes except the filter height H and width W. Concretely,
the tensorized kernel K is factorized as:
Ki,j,so,…
,sm — 1,t0,∙∙∙ ,tm —1
Rs0-1	Rsm-1-1 Rt0 -1	Rtm-1 -1
E-E E…E
r0s=0	rms-1=0 r0t=0	rmt -1=0
P(O)	... p(m-1)	C ∙ S S t t Q(O) …Q(m-1)
PS0,r0	Psm-ι,rm-六 i,j,r0,…，rm-i,rt L,*-1 QrO ,to	Qrm-1,tm-1
K = C (×2(P(O) )T …×m+1 (P(m-1))τ ×m+2 Q(O)…×2m+1 Q(m-1))
(I.7a)
(I.7b)
where P(I) ∈ RSl×Rs,∀l ∈ [m], C ∈ Rh×w×r0×…×r"i×r0×…×Rm-ι and Q(I) ∈ RRt×Tl,∀l ∈
[m] are again named as input factors, core factor and output factors respectively. The reason that
(Sl , Tl )’s are not grouped into supermodes follows exactly the same arguments as in Appendix H.
Compared to r-Tensor-train-dense layer, the only difference is that the core tensor now has two extra
modes for filter height and width, and therefore the number of parameters is magnified by a factor of
HW. Similar to Tucker-convolutional and r-Tucker-dense layers, the procedure to evaluate V can
be sequentialized into three steps:
(O)
Ui+dχ,j+dy,r0,…，rm-1
S0-1	Sm-1 -1
E …E	P(O)
乙 乙 s0,
s0=O sm-1=O
(m-1)
.…Psm-ι,rm-1 Ui+dχ,j+dy,s0,…,sm-1
Rs0 -1	Rsm-1 -1
U (1)	=E	E	E C	U (O)
x,y,r0,…，rm-1	=乙…	乙	乙S,j,r0,…，rm-ι,rt,…rm-1	Ui+dx,j+dy,r0,…，rm-1
r0s=O	rms -1=O i,j
Rt0 -1	Rtm-1 -1
vχ,y,"∙,tm-1 = ∑∙∙∙ E	Q^o
r0t=O	rmt -1=O
Qrm-X-I Ux1y,rt ,一,啧一
(I.8a)
(I.8b)
(I.8c)
37
Under review as a conference paper at ICLR 2019
where U ⑼ ∈ RX × Y ×R0×…×Rm-1 and U ⑴ ∈ RX ×Y '×R0×…×Rm-1 are two intermediate tensors.
Referring to r-Tucker-dense layer, the number of operations of the first step is scaled up by XY , the
number for the middle step by HWXY and the last step by X'Y'. Therefore, the time complexity
for the three-steps process is O(mS^1 XY + HWR2mXY + mT^mX'Y'). Subsequently, We can
rewrite these steps concisely in tensor notations.
U⑼=U (×2P⑼…×m+ι P(mT))
u ⑴=U(O)(*0 ◦*; ×2 ◦•••◦ ×m-1) c
0	1 2	m-1
V = U⑴(×2Q(O)…×m+1 QgT))
(I.9a)
(I.9b)
(I.9c)
In principle, the backpropagation rules for the sequential steps can be derived almost identically as
in the r-Tucker-dense layer. For reference, we list all equations for the first and last steps as follows:
∂UL) = If g(Q(O))T …×m+ι (Q(mT)T)	(I.10a)
∂L	∂L	O	l+1 l+3	m+1
(可)=IV (×0 ◦…°×l+1 °×l+3 ◦…。×m+1)
(U⑴(×2Q(O)…×l+1 Q(T) ×l+3 Q(l+1)…×m+1 Q(mT)))	(I.10b)
IL =券 IP(O) )T …"+1 (PemT)T)	(I^0O
IL	IL	O	l+1	l+3	m+1
(府)=∂U(O) (×°	×l+1 ◦ ×l+3 ◦…° ×m+J
(u gp(O)…×l+1 P(IT) ×l+3 P(l+1)…×m+1 P(mT)))	(I.10d)
Notice that the middle step itself is a tensorized convolutional layer defined in Equation I.1b, there-
fore its backpropagation equations are exactly the same as the ones in Equations I.2a and I.2b.
d L — O O ( T、T C / . 1 ∖T C ∖zm+2 C C xz2m+1∖
∂U(O) = C U*O) ° (*I) ° ×2	°∙∙∙° ×m+1 )
∂L
∂ U⑴
∂L
∂C
t∖ ∂ L
°(*1))所
(I.11a)
(I.11b)
The analyses for the backpropagation equations mimic the ones in the forward pass, again by com-
parison against the ones in r-Tucker-dense layer: the time complexity to obtain the derivatives in
the first step is magnified by XY, while the ones for the middle step and the last step are scaled by
HWX'Y' and X'Y' respectively. Therefore, the total number of operations for the derivatives with
respect to input/intermediate results is O(mSmm RXY + HWR2mX'Y' + mT~mRXY), and the
number for the factors is O(mSmm RXY + XYR2mX'Y' + mTmm RX'Y').
r-Tensor-train-convolutional layer Following the r-CP-convolutional layer, we propose to apply
Tensor-train decomposition to the tensorized kernel K by grouping (Sl, Tl)’s and filter height/width
(H, W ) as supermodes. In Tensor-train decomposition, these supermodes are ordered by their in-
dices, with the extra supermode (H, W) appended to the end. Concretely, the tensorized kernel K
is decomposed as:
Ki,j,so ,…,sm
1,tθ,∙∙∙ ,tm-1
R0-1
Σ∙∙
r0=O
K(m)
rm-1 ,i,j
(I.12a)
K = swapaxes (K(O) X-I K⑴ X-I …X-I K(m))
(I.12b)
where K(O) ∈ RS0×T0 ×R0, K(l) ∈ RRl-1×Sl ×Tl ×Rl and K(m) ∈ RRm-1×H×W are (m + 1)
factor tensors. Compared to r-Tensor-train-dense layer, the r-Tensor-train-convolutional layer has
an additional factor K(m) that contains RHW parameters, which leads to a total number of
O((m(ST) mm + HW)R). For conciseness, We follow the preprocessing steps to add singleton mode
38
Under review as a conference paper at ICLR 2019
R-1 = 1 to U and K(O) such that U ∈ RX ×Y ×S0× …×sm-∖×R-∖ and K(O) ∈ Rr-1×s0×t0×r0 and
rename U as U(0). As we shall expect, the multi-steps procedure to evaluate V now has (m + 1)
steps, with the last step as a 3D-convolutional layer:
Rl-1 -1 Sl -1
U(i)	= 丁 L K(L)	u(i)
Si+dx,j+dy,…,rι	，/	/ r Jrl-ι,sl ,tι,rι υτi+dx,j+dy,sι ,∙∙∙ ,Sm-ι ,to,∙∙∙ ,ti — i,ri — i
rl-1=O sl=O
(I.13a)
Rm-1-1
Vx,y,t0,∙∙∙,tm-1 = E EKrm-ι,i,j Ui(X+dy,to,∙∙∙,tm-i,rm-i	(L13b)
rm-1=O i,j
where U(I) ∈ RX ×Y×Sl ×…×Sm- 1 ×T0 ×…×Tl- 1 × Rl- 1, ∀ι ∈ [m] are the intermediate results. The
number of operations for the first m steps is O(m max(S,T)1+mm RXY) by comparing against
the corresponding steps in r-Tensor-train-dense layer, and the last step of 3D-convolutional layer
requires O(HWRTXY) operations in the forward pass. In tensor notations, these steps are nicely
represented as:
U(I) = U(I) (×2 ◦ X-1) K(I)	(I.14a)
V = U(m) (*1 o*2 ◦ X-1) Km)
The backpropagation rules are easily obtained by modifying the ones in r-Tensor-train-dense layer.
For completeness, we list all backpropagation equations in the following:
∂Um) = dL (M)T。宙)D K(m)
dKLm) = dL((*0)T。(*1)T o×2 …×m+i)U (m)
∂L	(l)	2	3	∂L
而=SWaPaXes (K() (×-2。×-ι) ∂U(m))
∂KL) = SWaPaXeS (U(I) (×0 o×1 0X3 …xm+1) ∂UdLi))
(I.14b)
(I.15a)
(I.15b)
(I.15c)
(I.15d)
The analyses of the backward steps again follow by building connections with r-Tensor-train-dense
layer: each of the first m steps is scaled by XY, while the last step requires O(HWTRX'Y') for
U(m) and O(XYTRX'Y') forK(m).
Decomp.	O(# of params) O(# of forward ops.)	O(# of back ops. for inputs) O(# of back ops. for params.)
original	HWST HWSTXY	HWSTX 'Y' XYSTX 'Y'
r-CP	#ParamsCP + HWR #OPsCPXY + HWTRXY	#OPsCPXY + HWTRX 'Y'- #OPsCPXY + XYTRX 'Y'
r-TK	(HW #ParamsTKC + #ParamsTKI + #ParamsTKO (HW #OPsTKC XY+ #OPsTKI XY + #oPsTKO X 'Y')	(HW #OPsTKc X 'Y'+ #OPsTKI XY + #OPsTKO X 'Y') (XY #OPsTKC X 'Y'+ #OPsTKI XY + #OPsTKO X 'Y')
r-TT ,In order to com #ParamsC #OpsCP = #ParamsT #OpsTKI =	#ParamsTT + HWR #OPsTTXY + HWTRXY pare against reshaped tensor decomposition s on dense layer in P = m(ST)m R	#ParamsTK = EL #Para m max(S, T)1 + mm #OPsTK = EL #OPsTKL KI = mSm R	#ParamsTKC = R2m mSR	#OPsTKC = R2m	#OPsTTXY + HWTRX 'Y'- #OPsTTXY + XYTRX 'Y' Table 11, we denote the numbers for the dense layers as: msTKL	#ParamsTT = m(ST) m R2 #	OpsTT = m max(S, T)1+ m R2 1 #	ParamsTKO = mT m R #	OPsTKO = mTR
Table 12: Summary of reshaped tensor decomposition on convolutional layer. In this table, we
list the number of parameters and time complexities required by various reshaped tensor decompo-
sition s on convolutional layer. Recall that a convolutional layer, composed with ST filters of size
H X W, maps a set ofS feature maps of size X X Y to another set ofT feature maps of size X' X Y'.
For simplicity, we assume the numbers of input/output feature maps S, T are factorized evenly, i.e.
Sl = S 前,Ti = T 前，∀l ∈ [m], and all ranks are equal to R, i.e Rl = R, ∀l ∈ [m].
39