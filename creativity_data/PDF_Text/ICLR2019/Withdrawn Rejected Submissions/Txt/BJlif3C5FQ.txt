Under review as a conference paper at ICLR 2019
Learning to Attend On Essential Terms: An
Enhanced Retriever-Reader Model for Open-
domain Question Answering
Anonymous authors
Paper under double-blind review
Ab stract
Open-domain question answering remains a challenging task as it requires models
that are capable of understanding questions and answers, collecting useful infor-
mation, and reasoning over evidence. Previous work typically formulates this task
as a reading comprehension or entailment problem given evidence retrieved from
search engines. However, existing techniques struggle to retrieve indirectly related
evidence when no directly related evidence is provided, especially for complex
questions where it is hard to parse precisely what the question asks. In this paper
we propose a retriever-reader model that learns to attend on essential terms dur-
ing the question answering process. We build (1) an essential term selector which
first identifies the most important words in a question, then reformulates the query
and searches for related evidence; and (2) an enhanced reader that distinguishes
between essential terms and distracting words to predict the answer. We evaluate
our model on multiple open-domain QA datasets where it outperforms the exist-
ing state-of-the-art, notably leading to a relative improvement of 8.1% on the AI2
Reasoning Challenge (ARC) dataset.
1 Introduction
Open-domain question answering (QA) has been extensively studied in recent years. Many existing
works have followed the ‘search-and-answer’ strategy and achieved strong performance (Chen et al.,
2017; Kwon et al., 2018; Wang et al., 2018) spanning multiple QA datasets such as TriviaQA (Joshi
et al., 2017), SQuAD (Rajpurkar et al., 2016), MS-Macro (Nguyen et al., 2016), among others.
However, open-domain QA tasks become inherently more difficult when (1) dealing with ques-
tions with little available evidence; (2) solving questions where the answer type is free-form text
(e.g. multiple-choice) rather than a span among existing passages (i.e., ‘answer span’); or when (3)
the need arises to understand long and complex questions and reason over multiple passages, rather
than simple text matching. As a result, it is essential to incorporate commonsense knowledge or to
improve retrieval capability to better capture partially related evidence (Chen et al., 2017).
As shown in Table 1, the TriviaQA, SQuAD, and MS-Macro datasets all provide passages within
which the correct answer is guaranteed to exist. However, this assumption ignores the difficulty
of retrieving question-related evidence from a large volume of open-domain resources, especially
when considering complex questions which require reasoning or commonsense knowledge. On the
other hand, ARC does not provide passages known to contain the correct answer. Instead, the task of
identifying relevant passages is left to the solver. However, questions in ARC have multiple answer
choices that provide indirect information that can help solve the question. As such an effective
model needs to account for relations among passages, questions, and answer choices.
Figure 1 shows an example of a question in the ARC dataset and demonstrates the difficulties in
retrieval and reading comprehension. As shown for Choice 1 (C1), a simple concatenation of the
question and the answer choice is not a reliable query and is of little help when trying to find
supporting evidence to answer the question (e.g. we might retrieve sentences similar to the question
or the answer choice, but would struggle to find evidence explaining why the answer choice is
correct). On the other hand, a reformulated query consisting of essential terms in the question and
1
Under review as a conference paper at ICLR 2019
Table 1: Differences among popular QA datasets.
Dataset	Open-	Multiple Passage	No ranking domain	choice	retrieval	supervision1
ARC (Clark et al., 2018) SQuAD (Rajpurkar et al., 2016) TriviaQA (Joshi et al., 2017) MS-Macro (Nguyen et al., 2016)	✓	✓	✓	✓ ✓ ✓ ✓
Q: Mercury , the planet nearest to the SUn , has extreme surface temperatures, ranging from 465 C
in sunlight to -180 C in darkness . Why is there such a large range of temperatures on Mercury?
C1: The planet is too small to hold heat.
C4: The planet lacks an atmosphere to hold heat .
QUe Queryl = Q+C1
Query1: Mercury , the planet nearest to the Sun , has extreme surface
temperatures , ranging from 465 C in sunlight to -180 C in darkness .
Why is there such a large range of temperatures on Mercury? The
planet is too small to hold heat.
Query4 = Essential-term(Q)+C4
Query4: Mercury extreme surface temperatures.
The planet lacks an atmosphere to hold heat .
Retrieving evidence
S1:	Other planets such as Mercury has extreme
hot and cold temperatures .
S2:	The planet Mercury is too small and has too
little gravity to hold onto an atmosphere.
Sending evidence
to reader
Retrieving evidence
∖
S1: The lack of atmosphere also contributes to the
planet 's wild temperature extremes .
S2: Mercury is the closest planet to the sun and has a
thin atmosphere, no air pressure and an extremely
high temperature.

MRC
Figure 1:	Example of the retrieve-and-read process to solve open-domain questions. Words related
with the question, C1, and C4 are marked in red, green, and blue, respectively.
Choice 4 can help retrieve evidence explaining why Choice 4 is a correct answer. To achieve this,
the model needs to (1) ensure that the retrieved evidence supports the fact mentioned in both the
question and the answer choices and (2) capture this information and predict the correct answer.
To address these difficulties, we propose an essential-term-aware Retriever-Reader (ET-RR) model
that learns to attend on essential terms during retrieval and reading. Specifically, we develop a
two-stage method with an essential term selector followed by an attention-enhanced reader.
Essential term selector. ET-Net is a recurrent neural network that seeks to understand the question
and select essential terms, i.e., key words, from the question. We frame this problem as a classi-
fication task for each word in the question. These essential terms are then concatenated with each
answer choice and fed into a retrieval engine to obtain related evidence.
Attention-Enhanced Reader. Our neural reader takes the triples (question, answer choice, retrieved
passage) as input. The reader consists of a sequence of language understanding layers: an input
layer, attention layer, sequence modeling layer, fusion layer, and an output layer. The attention and
fusion layers help the model to obtain a refined representation of one text sequence based on the
understanding of another, e.g. a passage representation based on an understanding of the question.
We further add a choice-interaction module to handle the semantic relations and differences between
answer choices. Experiments show that this can further improve the model’s accuracy.
We evaluate our model on the ARC dataset, where our model achieves an accuracy of 36.61% on the
test set, thus ranking first on the official leaderboard. We also adapt two datasets to the open-domain
setting, RACE-Open and MCScript-Open, where we outperform baseline models by a large margin.
Ablation studies show that each of our model’s components contributes to its accuracy.
1For SQuAD and TriviaQA, since their questions are paired with span-type answers, it is convenient to
obtain ranking supervision where retrieved passages are relevant via distant supervision; however free-form
questions in ARC result in a lack of supervision which makes the problem more difficult. For MS-Macro, the
dataset is designed to annotate relevant passages though it has free-form answers.
2
Under review as a conference paper at ICLR 2019
I QUestiOn ]
Essential term
selector
, 〉C Essential
I terms
+ [ ChOiCe 1 ] = [ QUery 1 ∣
A Attention enhanced reader
I IR retriever
+ [ ChOiCe N ] — ∣ QUery N )!
————-
∣==> [ Passage 1 )
∣==> [ Passage N ]
、___________________J
[ QUestiOn ] I ChOiCe 1 I [ Passage 1 ] ”.( QUestiOn ∣ [ ChOiCe N ) ∣ Passage N ]
[ QUestiOn ] ! [ ChOiCe 1 ] + •…+ [ ChOiCe N ] ∣
F- 1	1
T	InPUt layer	∣
I	AttentiOn layer	∣
[ SeqUenCe Modeling layer ]
-------------->Θ
W
[ PrOjeCtiOn layer ]
I 0 .	,	(b) Essential term
(PrediCtiOn )	selector
InPUt layer
InPUt layer
AttentiOn layer ]
AttentiOn layer ∣


]
[ SeqUenCe Modeling layer ]
I	FUsiOn layer	]
[	OUtPUt layer	]
U SCOre 1
I SeqUenCe Modeling layer ]
[	FUsiOn	layer	]
(	OUtPUt	layer	]
…	-JJ-	Score N
SOftmaX	]
叵	叵	叵
θ	O	。2
(c) Fusion layer
w≡	(a)Ret器reader
Figure 2:	Model structure for our essential-term-aware retriever-reader model.
2 Related Work
There has recently been growing interest in building better retrievers for open-domain QA. Wang
et al. (2018) proposed a Reinforced Ranker-Reader model that ranks retrieved evidence and assigns
different weights to evidence prior to processing by the reader. Min et al. (2018) demonstrated that
for several popular MRC datasets (e.g. SQuAD, TriviaQA) most questions can be answered using
only a few sentences rather than the entire document. Motivated by this observation, they built
a sentence selector to gather this potential evidence for use by the reader model. Nishida et al.
(2018) developed a multi-task learning (MTL) method for a retriever and reader in order to obtain a
strong retriever that considers certain passages including the answer text as positive samples during
training. The proposed MTL framework is still limited to the scenario when it is feasible to discover
whether the passages contain the answer span. Although these works have achieved progress on
open-domain QA by improving the ranking or selection of given evidence, few have focused on the
scenario where the model needs to start by searching for the evidence itself.
Scientific Question Answering (SQA) is a representative open-domain task that requires capability
in both retrieval and reading comprehension. In this paper, we study question answering on the
AI2 Reasoning Challenge (ARC) scientific QA dataset (Clark et al., 2018). This dataset contains
elementary-level multiple-choice scientific questions from standardized tests and a large corpus of
relevant information gathered from search engines. The dataset is partitioned into “Challenge” and
“Easy” sets. The challenge set consists of questions that cannot be answered correctly by any of
the solvers based on Pointwise Mutual Information (PMI) or Information Retrieval (IR). Existing
models tend to achieve only slightly better and sometimes even worse performance than random
guessing, which demonstrates that existing models are not well suited to this kind of QA task.
Khashabi et al. (2017) worked on the problem of finding essential terms in a question for solving
SQA problems. They handcrafted over 100 features and used an SVM classifier to uncover essential
terms within a question. They also published a dataset containing over 2,200 science questions
annotated with essential terms. We leverage this dataset to build an essential term selector.
More recently, Boratko et al. (2018) developed a labeling interface to obtain high quality labels for
the ARC dataset. One interesting finding is that human annotators tend to retrieve better evidence
after they reformulate the search queries which are originally constructed by a simple concatenation
of question and answer choice. By feeding the evidence obtained by human-reformulated queries
into a pre-trained MRC model (i.e. DrQA (Chen et al., 2017)) they achieved an accuracy increase
of 42% on a subset of 47 questions. This shows the potential for a “human-like” retriever to boost
performance on this task. Inspired by this work, we focus on selecting essential terms to reformulate
more efficient queries, similar to those that a human would construct.
3
Under review as a conference paper at ICLR 2019
3	Approach
In this section, we introduce the essential-term-aware retriever-reader model (ET-RR). As shown
in Figure 2, we build a term selector to discover which terms are essential in a question. The
selected terms are then used to formulate a more efficient query enabling the retriever to obtain
related evidence. The retrieved evidence is then fed to the reader to predict the final answer.
For a question with q words Q = {wtQ }tq=1 along with its N answer choices C = {Cn }nN=1 where
Cn = {wtC}tc=1, the essential-term selector chooses a subset of essential terms E ⊂ Q, which are
then concatenated with each Cn to formulate a query. The query for each answer choice, E + Cn,
is sent to the retriever (e.g. Elastic Search2), and the top K retrieved sentences based on the scores
returned by the retriever are then concatenated into the evidence passage Pn = {wtP}tp=1.
Next, given these text sequences Q, C, and P = {Pn}nN=1, the reader will determine a matching
score for each triple {Q, Cn, Pn}. The answer choice Cn* with the highest score is selected.
We first introduce the reader model in Section 3.1 and then the essential term selector in Section 3.2.
3.1	Reader Model
3.1.1	Input Layer
To simplify notation, we ignore the subscript n denoting the answer choice until the final output
layer. In the input layer, all text inputs—the question, answer choices, and passages, i.e., retrieved
evidence—are converted into embedded representations. Similar to Wang (2018), we consider the
following components for each word:
Word Embedding. Pre-trained GloVe word embedding with dimensionality dw = 300.
Part-of-Speech Embedding and Named-Entity Embedding. The part-of-speech tags and named
entities for each word are mapped to embeddings with dimension 16.
Relation Embedding. A relation between each word in P and any word in Q or C is mapped to an
embedding with dimension 10. In the case that multiple relations exist, we select one uniformly at
random. The relation is obtained by querying ConceptNet (Speer et al., 2017).
Feature Embeddings. Three handcrafted features are used to enhance the word representations:
1.	Word Match. If a word or its lemma of P exists in Q or C, then this feature is 1 (0
otherwise).
2.	Word Frequency. A logarithmic term frequency is calculated for each word.
3.	Essential Term. For the i-th word in Q, this feature, denoted as wei, is 1 if the word is an
essential term (0 otherwise). Let we = [we1, we2, ..., weq] denote the essential term vector.
For Q, C, P, all of these components are concatenated to obtain the final word representations
WQ ∈ Rq×dQ , WC ∈ Rc×dC , WP ∈ Rp×dP , where dQ , dC, dP are the final word dimensions of
Q, C, and P.
3.1.2	Attention Layer
As shown in Figure 2, after obtaining word-level embeddings, attention is added to enhance word
representations. Given two word embedding sequences WU, WV , word-level attention is calculated
as:
MUV = WUU ∙ (WVV)＞； MUV = softmax(Muv); WU = Muv ∙ (WvV), (1)
where U ∈ RdU ×dw and V ∈ RdV ×dw are two matrices that convert word embedding sequences to
dimension dw, and M0UV contains dot products between each word in WU and WV, and softmax
is applied on M0UV row-wise.
2https://www.elastic.co/products/elasticsearch
4
Under review as a conference paper at ICLR 2019
Three types of attention are calculated using Equation (1): (1) question-aware passage representation
WPQ ∈ Rp×dw , (2) question-aware choice representation WCQ ∈ Rc×dw , and (3) passage-aware
choice representation WPC ∈ Rc×dW .
3.1.3	Sequence Modeling Layer
To model the contextual dependency of each text sequence, we use BiLSTMs to process the word
representations obtained from the input layer and attention layer:
Hq = BiLSTM[WQ]; Hc = BiLSTM[WC; WCP; WCQ];	Hp = BiLSTM[WP; WPQ],
(2)
where Hq ∈ Rq×l, Hc ∈ Rc×l, and Hp ∈ Rp×l are the hidden states of the BiLSTMs, ‘;’ is
feature-wise concatenation, and l is the size of the hidden states.
3.1.4	Fusion Layer
We further convert each question and answer choice into a single vector: q ∈ Rl and c ∈ Rl :
aq = Softmax([Hq; We] ∙ w>q); q = Hq>αq; αc = Softmax(Hc ∙ wɪ); C = Hc>αc, (3)
where the essential-term feature we from Section 3.1.1 is concatenated with Hq, and wsq and wsc
are learned parameters.
Finally, a bilinear sequence matching is calculated between Hp and q to obtain a question-aware
passage representation, which is used as the final passage representation:
ap = Softmax(Hp ∙ q); P = Hp>αp.	(4)
3.1.5	Choice Interaction
When a QA task provides multiple choices for selection, the relationship between the choices can
provide useful information to answer the question. Therefore, we integrate a choice interaction layer
to handle the semantic correlation between multiple answer choices. Given the hidden state Hcn of
choice cn and Hci of other choices ci, ∀i 6= n, we calculate the differences between the hidden
states and apply max-pooling over the differences:
cinter = Maxpool(Hcn - √r17 X Hci),	(5)
N-1
i6=n
where N is the total number of answer choices. Here, cinter characterizes the differences between
an answer choice cn and other answer choices. The final representation of an answer choice is
updated by concatenating the self-attentive answer choice vector and inter-choice representation as
cfinal = [c; cinter].	(6)
3.1.6	Output Layer
For each tuple {q, Pn, cn}nN=1, two scores are calculated by matching (1) the passage and answer
choice and (2) question and answer choice. We use the bilinear form for both matchings. Finally, a
softmax function is applied over N answer choices to determine the best answer choice:
spnc = PnWpccfinnal; sqnc = qWqccfinnal; s = Softmax(spc) + Softmax(sqc),	(7)
where spnc, sqnc are the scores for answer choice 1 ≤ n ≤ N ; spc, sqc are score vectors for all N
choices; and s contains the final scores for each answer choice. During training, we use a cross-
entropy loss.
3.2	Essential Term Selector
Essential terms are key words in a question that are crucial in helping a retriever obtain related
evidence. Given a question Q and N answer choices C1, . . . , CN, the goal is to predict a binary
variable yi for each word Qi in the question Q, where yi = 1if Qi is an essential term and 0
5
Under review as a conference paper at ICLR 2019
Table 2: Example of essential term data.
Question	If an object is attracted to a mag- net, the object is most likely made of (A) wood (B) plastic (C) card- board (D) metal
# annotators	5
Annotation	If,0; an,0; object,3; is,0; at- tracted,5; to,0; a,0; magnet,,5; the,0; object,1; is,0; most,0; likely,0; made,2; of,0
Table 3: Precison, recall and F1 scores of dif-
ferent selectors.
Model	Precision	Recall	F1
MaxPMI	0.88	0.65	0.75
SumPMI	0.88	0.65	0.75
PropSurf	0.68	0.64	0.66
PropLem	0.76	0.64	0.69
ET Classifier	0.91	0.71	0.80
ET-Net	0.74	0.90	0.81
otherwise. To address this problem, we build a neural model, ET-Net, which has the same design
as the reader model for the input layer, attention layer, and sequence modeling layer to obtain the
hidden state Hq for question Q.
In detail, we take the question Q and the concatenation C of all N answer choices as input to ET-
Net. Q and C first go through an input layer to convert to the embedded word representation, and
then word-level attention is calculated to obtain a choice-aware question representation WQC as in
Equation (1). We concatenate the word representation and word-level attention representation of the
question and feed it into the sequence modeling layer:
Hq = BiLSTM[WQ ; WCQ].	(8)
As shown in Figure 2, the hidden states obtained from the attention layer are then concatenated with
the embedded representations of Q and fed into a projection layer to obtain the prediction vector
y ∈ Rq for all words in the question:
y =[Hq; WQ ]∙ws,	(9)
where ws contains the learned parameters, and WQf is the concatenation of the POS embedding,
NER embedding, relation embedding, and feature embedding from Section 3.1.1.
After obtaining the prediction for each word, we use a binary cross-entropy loss to train the model.
During evaluation, we take words with yi greater than 0.5 as essential terms.
4	Experiments
In this section, we first discuss the performance of the essential term selector, ET-Net, on a public
dataset. We then discuss the performance of the whole retriever-reader pipeline, ET-RR, on the
ARC, RACE-Open and MCScript-Open datasets. For both the ET-Net and ET-RR models, we use
96-dimensional hidden states and 1-layer BiLSTMs in the sequence modeling layer. A dropout rate
of 0.4 is applied for the embedding layer and the BiLSTMs’ output layer. We use adamax (Kingma
& Ba, 2014) with a learning rate of 0.02 and batch size of 32. The model is run for 100 epochs.
4.1	Performance on Essential Term Selection
We use the public dataset from Khashabi et al. (2017) which contains 2,223 annotated questions,
each accompanied by four answer choices. Table 2 gives an example of an annotated question.
As shown, the dataset is annotated for binary classification. For each word in the question, the data
measures whether the word is an “essential” term according to 5 annotators. We then split the dataset
into training, development, and test sets using an 8:1:1 ratio and select the the model that performs
best on the development set.
Table 3 shows the performance of our essential term selector and baseline models from Khashabi
et al. (2017). MAXPMI and SUMPMI score the importance of a word w by taking the max or sum
of its PMI p(w, c) scores for all answer choices c. PROPSURF and PROPLEM are baselines that
consider a word as an essential term if it or its lemmatized word appears at least a certain proportion
of times as essential in the dataset. ET Classifier is an SVM-based model from Khashabi et al.
6
Under review as a conference paper at ICLR 2019
Table 4: Examples of essential term prediction (in questions) by ET-Net. True positives are marked
in red while false positives are marked in blue.
Example questions
Which unit of measurement can be used to describe the length of a desk ?
One way animals usually respond to a sudden drop in temperature is by
Organisms require energy to survive. Which of the following processes provides en-
ergy to the body ?
Table 6: Ablation test on attention components of
ET-RR on ARC. ’-'denotes the ablated feature.
Table 5: Statistics on ARC and RACE-Open.
Corpus size is the number of sentences.
Dataset	ARC	RACE- Open	MCScript- Open	Model	Test
				ET-RR	36.61
Train	1,119	9,531	1,036	- inter-choice	36.36
Dev	299	473	156	- passage-choice	35.41
Test	1,172	528	319	- question-choice	34.47
Corpus	1.46M	0.52M	24.2K	- passage-question	34.05
(2017) requiring over 100 handcrafted features. As shown, our ET-Net achieves a comparable result
with the ET Classifier in terms of F1 Score.
Table 4 shows example predictions made by ET-Net. As shown, ET-Net is capable of selecting most
ground-truth essential terms. It rejects certain words such as “organisms” which have a high TF-
IDF in the corpus but are not relevant to answering a particular question. This shows its ability to
discover essential terms according to the context of the question.
4.2	Performance on Open-domain Multiple-choice QA
With the trained essential term selector (ET-Net) from the previous experiment, we train and evaluate
the reader model on three open-domain multiple-choice QA datasets. All datasets are associated
with a sentence-level corpus. In the experiments, ET-RR generates a query for each of the N answer
choices. For each query, ET-RR then obtains the top K sentences returned by the retriever and
considers these sentences as a passage for the reader. We set K = 10 for all experiments and report
results for different K in the ablation test. Detailed statistics are shown in Table 5.
•	ARC (Clark et al., 2018): We consider the ‘Challenge’ set in the ARC dataset and use the
provided corpus during retrieval.
•	RACE-Open: We adapted the RACE dataset (Lai et al., 2017) to the open-domain set-
ting. Originally, each question in RACE comes with a specific passage. To enable passage
retrieval, we concatenate all passages into a corpus with sentence deduplication.3
•	MCScript-Open: The MCScript (Ostermann et al., 2018) dataset is also adapted to the
open-domain setting. Again we concatenate all passages to build the corpus.4 5 6
We compare ET-RR against existing retrieval-reader methods on both datasets. Accuracy is shown
in Table 7. Results for ARC are obtained from the official leaderboard.7 On the ARC dataset,
ET-RR outperforms all previous models with a relative 8.1% improvement over the state-of-the-art
BiLSTM Max-out method. On the RACE-Open and MCScript-Open datasets, ET-RR achieves a
relative improvement of 24.6% and 10.5% on the test set compared with the IR solver respectively.
3As short questions are usually passage-specific and retrieval can rarely find any related passage, we only
keep questions with more than 15 words.
4We keep questions with more than 10 words rather than 15 words to ensure that there is sufficient data.
5IR solver sends question plus each answer choice as query to the search engine, then pick the answer choice
of which the top retrieved sentence has the highest score as the answer
6Different from ET-RR, in the original BiDAF baseline, the sentences returned by each query are mixed
together, then the top N × K sentences are aggregated as a whole passage and passed to the reader.
7Snapshot from http://data.allenai.org/arc/ on September 26, 2018
7
Under review as a conference paper at ICLR 2019
Table 7: Experimental results on ARC, RACE-Open and MCScript-Open. We report the accuracy
of ET-RR on multiple-choice selection.
Model	ARC Test	RACE-Open Test	MCScript-Open Test
IR solver5 (Clark et al., 2018)	20.26	30.70	60.46
Guess All (random) (Clark et al., 2018)	25.02	25.01	50.02
BiDAF6 (Clark et al., 2018)	26.54	26.89	50.81
DGEM (Clark et al., 2018)	27.11	/	/
KG2 (Zhang et al., 2018)	31.70	/	/
TriAN + f(dir)(cs) + f(ind)(cs) (Zhong et al., 2018)	33.39	/	/
BiLSTM Max-out (Mihaylov et al., 2018)	33.87	/	/
ET-RR	36.61	38.26	66.78
Table 8: Comparisons for different query formulation methods and amounts of retrieved evidence
(i.e. top K) on ARC dataset, in terms of percentage accuracy.
Model	ET-RR (Concat)		ET-RR (TF-IDF)		ET-RR	
Top K	Dev	Test	Dev	Test	Dev	Test
5	39.26	33.36	39.93	34.73	39.93	35.59
10	38.93	35.33	39.43	35.24	43.96	36.61
20	41.28	34.56	38.59	33.88	42.28	35.67
4.3	Ablation study
Finally, we investigate how each component contributes to model performance.
Attention components. Table 6 demonstrates how the attention components contribute to the per-
formance of ET-RR. As shown, ET-RR with all attention components performs the best on the ARC
test set. The performance of ET-RR without passage-question attention drops the most significantly
out of all the components. It is worth noting that the choice interaction layer gives a further 0.24%
boost on test accuracy.
Essential term selection. To understand the contribution of our essential-term selector, we introduce
two variants of ET-RR:
•	ET-RR (Concat). Concatenates the original question and answer choice as the query.
•	ET-RR (TF-IDF). We calculate the TF-IDF scores and take top 30% words8 with the highest
scores in the question to concatenate with each answer choice as a query.
Table 8 shows an ablation study comparing different query formulation methods and amount of
retrieved evidence K. As shown, with the essential term selector ET-Net, the model consistently
outperforms other baselines, given different numbers of retrievals K. The performance of all models
works best when K = 10. Furthermore, only using TF-IDF to select essential terms in a question
is not effective. When K = 10, the ET-RR (TF-IDF) method even performs worse than ET-RR
(Concat). This illustrates the challenges in understanding what is essential in a question.
5	Conclusion
We presented a new retriever-reader model (ET-RR) for open-domain QA. Our pipeline has the
following contributions: (1) we built an essential term selector (ET-Net) which helps the model
understand which words are essential in a question leading to more effective search queries when
retrieving related evidence; (2) we developed an attention-enhanced reader with attention and fu-
sion among passages, questions, and candidate answers. Experimental results show that ET-RR
outperforms existing QA models on the ARC, RACE-Open and MCScipt-Open datasets.
8According to the annotated dataset, around 30% of the terms in each question are labelled as essential.
8
Under review as a conference paper at ICLR 2019
References
Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew
McCallum, Maria Chang, Achille Fokoue, Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kar-
tik Talamadupula, and Michael Witbrock. A systematic classification of knowledge, reasoning,
and context within the arc dataset. In QA@ACL, 2018.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-
domain questions. In ACL, 2017.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
CoRR, abs/1803.05457, 2018.
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke S. Zettlemoyer. Triviaqa: A large scale
distantly supervised challenge dataset for reading comprehension. In ACL, 2017.
Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and Dan Roth. Learning what is essential in
questions. In CoNLL, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.
Heeyoung Kwon, Harsh Trivedi, Peter Jansen, Mihai Surdeanu, and Niranjan Balasubramanian.
Controlling information aggregation for complex question answering. In ECIR, 2018.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading
comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,
2018.
Sewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. Efficient and robust question
answering from minimal context over documents. In ACL, 2018.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and
Li Deng. Ms marco: A human generated machine reading comprehension dataset. CoRR,
abs/1611.09268, 2016.
Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako Asano, and Junji Tomita. Retrieve-and-read:
Multi-task learning of information retrieval and reading comprehension. CoRR, abs/1808.10628,
2018.
Simon Ostermann, Michael Roth, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. Semeval-
2018 task 11: Machine comprehension using commonsense knowledge. In SemEval@NAACL-
HLT, 2018.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions
for machine comprehension of text. In EMNLP, 2016.
Robert Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of
general knowledge. In AAAI, 2017.
Liang Wang. Yuanfudao at semeval-2018 task 11: Three-way attention and relational knowledge
for commonsense machine comprehension. In SemEval@NAACL-HLT, 2018.
Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerald Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain
question answering. In AAAI, 2018.
Yuyu Zhang, Hanjun Dai, Kamil Toraman, and Le Song. KG2 : Learning to reason science exam
questions with contextual knowledge graph embeddings. CoRR, abs/1805.12393, 2018.
Wanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Improving question
answering by commonsense-based pre-training. arXiv preprint arXiv:1809.03568, 2018.
9