Under review as a conference paper at ICLR 2019
Understand	the dynamics of GANs via
Primal-Dual Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Generative adversarial network (GAN) is one of the best known unsupervised
learning techniques these days due to its superior ability to learn data distributions.
In spite of its great success in applications, GAN is known to be notoriously hard
to train. The tremendous amount of time it takes to run the training algorithm
and its sensitivity to hyper-parameter tuning have been haunting researchers in
this area. To resolve these issues, we need to first understand how GANs work.
Herein, we take a step toward this direction by examining the dynamics of GANs.
We relate a large class of GANs including the Wasserstein GANs to max-min
optimization problems with the coupling term being linear over the discriminator.
By developing new primal-dual optimization tools, we show that, with a proper
stepsize choice, the widely used first-order iterative algorithm in training GANs
would in fact converge to a stationary solution with a sublinear rate. The same
framework also applies to multi-task learning and distributional robust learning
problems. We verify our analysis on numerical examples with both synthetic and
real data sets. We hope our analysis shed light on future studies on the theoretical
properties of relevant machine learning problems.
1 Introduction
Since it was first invented by Ian Goodfellow in his seminal work Goodfellow et al. (2014), generative
adversarial networks (GANs) have been considered as one of the greatest discoveries in machine
learning community. It is an extremely powerful tool to estimate data distributions and generate
realistic samples. To train its implicit generative model, GAN uses a discriminator since traditional
Bayesian methods that require analytic density functions are no longer applicable. This novel
approach inspired by zero sum game theory leads to a significant performance boost; GANs are
able to generate samples in a fidelity level that is way beyond traditional Bayesian methods. During
the last few years, there have been numerous research articles in this area aiming at improving its
performance (Radford et al., 2015; Zhao et al., 2016; Nowozin et al., 2016; Arjovsky et al., 2017;
Mao et al., 2017). GANs have now become one of most recognized unsupervised learning techniques
and have been widely used in a variety of domains such as image generation (Nguyen et al., 2017),
image super resolution (Ledig et al., 2017), imitation learning (Ho & Ermon, 2016).
Despite the great progress of GANs, many essential problems remain unsolved. Why is GAN so
hard to train? How to tune the hyper-parameters to reduce instability in GAN training? How to
eliminate mode collapse and fake images that show up frequently in training (Arjovsky & Bottou,
2017)? Comparing with many other machine learning techniques, the properties of GANs are far
from being well understood. It is quite likely that the theoretical foundation of GANs will become a
longstanding problem. The theoretical difficulty of GANs mainly lies in the following several aspects.
First, it is a non-convex optimization problem with a complicated landscape. It is unclear how to
solve such optimization problems efficiently. The first-order method widely used in the literature via
updating the generator and discriminator along descent/ascent direction does not seem to converge
all the time. Although some techniques were proposed to stabilize the training performance of the
network, e.g., spectral normalization Miyato et al. (2018), in fact, there is no evidence that these
algorithms guarantee even local optimality. Second, even if there were an efficient algorithm to solve
this optimization problem, we do not know how well they generalize. After all, the optimization
formulation is based only on the samples generated by the underlying distribution but our goal is
to recover this underlying distribution. Of course, this is a problem faced by all machine learning
1
Under review as a conference paper at ICLR 2019
techniques. Last, there are no reliable ways to evaluate the quality of trained models. There are a
number of works in this topic (Salimans et al., 2016; Heusel et al., 2017), but human eyes inspection
remains the primary approach to judge a GAN model.
In the present work, we focus on the first problem and analyze the dynamics of GANs from an
optimization point of view. More precisely, we study the convergence properties of the first-order
method in GAN training. Our contributions can be summarized as follows. 1) We formulate a large
class of GAN problems as a primal-dual optimization problem with a coupling term that is linear
over discriminator (see Section 2 for the exact formulation); 2) We prove that the simple primal-dual
first-order algorithm converges to a stationary solution with a sublinear convergent rate O(1/t).
There have been a number of papers that study the dynamics of GANs from an optimization viewpoint.
These works can be roughly divided into three categories. In the first category, the authors focus
on high level idea using nonparametric models. This includes the original GAN paper Goodfellow
et al. (2014), the Wasserstein GAN papers Arjovsky & Bottou (2017); Arjovsky et al. (2017) and
many other works proposing new GAN structures. In the second category, the authors consider the
unrolled dynamics (Metz et al., 2016), that is, the discriminator remains optimal or almost optimal
during the optimization processes. This is considerably different to the first-order iterative algorithm
widely used in GAN training. Recent works Heusel et al. (2017); Li et al. (2017); Sanjabi et al. (2018)
provide global convergence analysis for this algorithm.
The last category is on the first-order primal-dual algorithm, in which both the discriminator and the
generator update via (stochastic) gradient descent. However, most of the convergence analysis are
local (Daskalakis et al., 2017; Mescheder et al., 2017; Nagarajan & Kolter, 2017; Li et al., 2018).
Other related work including the following: In Qian et al. (2018) the authors consider a gradient
descent/ascent algorithm for a special min-max problem arising from robust learning (min problem is
unconstrained, max problem has simplex constraints); In Yadav et al. (2018) the GANs are treated as
convex-concave primal-dual optimization problems. This formulation is considerably different to our
setup where GANs, as they should be, are formulated as nonconvex saddle point problems. In Farnia
& Tse (2018), the authors investigated the properties of the optimal solutions, which is also different
from our work focusing on convergence analysis of the first-order primal-dual algorithm. In Zhao
et al. (2018), some unified framework covering several generative models, e.g., VAE, infoGAN, were
proposed in the Lagrangian framework. However, the dual variable in their problem is a Lagrangian
multiplier, while in our problem, itis the discriminator of GAN. Besides, the focus of their paper is not
the optimization algorithm. In Chen et al. (2018), the authors related a class of GANs to constrained
convex optimization problems. More specifically, such GANs can be viewed as Lagrangian forms
of these convex optimization problems. The optimization variables in their formulation are the
probability density of the generator and the function values of the discriminator. Many issues like
nonconvexity do not show up. This is essentially a nonparametric model, which doesn’t apply to
cases when the discriminator and the generator are represented by parametric models. On the other
hand, our analysis is carried out on the parametric models directly and we have to deal with the
nonconvexity of neural networks. In Hajinezhad & Hong (2017) a primal-dual algorithm has been
studied for a non-convex linearly constrained problem (which can be reformulated into a min-max
problem, with the max problem being linear and unconstrained, and with linear coupling between
variables); In Hamedani & Aybat (2018), (Chen et al., 2013) and the references therein, first-order
methods have been developed for convex-concave saddle point problems. Compared to these works,
our considered problem is more general, allowing non-convexity and non-smoothness in the objective,
non-convex coupling between variables, and can further include constraints. Moreover, we provide
global convergence rate analysis, which is much stronger than the local analysis mentioned above.
It turns out that the primal-dual framework we study in this paper can also be applied to the
distributional robust machine learning problems (Namkoong & Duchi, 2016) and the multi-task
learning problems (Qian et al., 2018). In multi-task learning, the goal is to train a single neural
network that would work for several different machine learning tasks. Similarly, in distributional
robust learning, the purpose is to have a single model that would work for a set of data distributions.
In both problems, an adversarial layer is utilized to improve the worst case performance, which leads
to a primal-dual optimization structure that falls into the scope of problems we consider.
The rest of the paper is structured as follows. In Section 2 we introduce GAN and its primal-dual
formulation. We provide details of the algorithms with proof sketches in Section 3. The full proofs
2
Under review as a conference paper at ICLR 2019
are relegated to the appendix. We highlight our theoretical results in Section 4 via several numerical
examples, with both synthetic and real datasets.
2	Generative adversarial networks and min-max problems
GAN is a type of deep generative model with implicit density functions. It consists of two critical
components: a generator and a discriminator. The generator takes random variables with known
distribution as input and outputs fake samples. The discriminator is trained to distinguish real samples
and fake samples.
2.1	GANS
In the original form of GAN, the generator is a neural network and the discriminator is a standard
classifier with cross entropy cost. Denote the underlying probability distributions of the data sample
{yi}in=y1 ⊂ Rd and the random seed x (a random variable in Rk) of the generative model by Py and
Px respectively. The original or vanilla GAN is of the form (Goodfellow et al., 2014)
min max{E (log(F (y))) - E(log(1 - F (G(x))))}.
G∈G F∈F
(1)
Here G stands for the set of possible generators and F the set of possible discriminators.
There have been numerous modifications of the original GAN, among which the Wasserstein GAN
(Arjovsky et al., 2017) attracts a lot of attention. It has the form
min max{E(F (y)) -E(F(G(x)))},
G∈G F∈F
(2)
where G is the set of parameterized generators and F is the set of Lipschitz functions with Lipschitz
constant 1. This is a special case of a more general class of GANs with the same structure but different
constraint set F . The inner maximization loop defines different notions of integral probability metrics
(Muller, 1997) depending on the choices of F. Other than Wasserstein GAN, one interesting example
with this structure is the generative moment matching networks (Li et al., 2015; Arjovsky et al., 2017).
Wasserstein GAN can be extended to general optimal transport cost c(x, y), which results in saddle
point formulation
min max{E(ψ(y)) + E(φ(G(x)) | ψ(y) + φ(x) ≤ c(x,y), ∀x,y}.	(3)
G∈G ψ,φ
Note that the discriminator now becomes two functions ψ, φ instead of one. When c(x, y) = kx - yk,
the above reduces to the standard Wasserstein GAN in equation 2.
2.2	Nonconvex Min-Max problems
We observe that, compared to vanilla GAN in equation 1, the Wasserstein GAN in equation 2 and
equation 3 has a special structure. In the nonparametric form, the coupling between the generator G
and the discriminator F is linear on F in Wasserstein GAN while nonlinear in vanilla GAN. Indeed,
replacing F by αF1 + βF2 in the coupling term E(F (G(x))) yields
E(F(G(x))) = E((αF1 + βF2)(G(x))) =αE(F1(G(x)))+βE(F2(G(x))).
This structure motivates us to study the following min-max primal-dual optimization problem
Xm∈inX mY ∈aYx h(X) + hg(X),Yi - l(Y)
(4)
with l being strictly convex. the other two functions h and g can be non-convex. Here Y represents
the discriminator F and X represents the generator G. Note that the coupling term hg(X), Y i is
linear over the discriminator Y , but nonlinear in X.In real applications, we need to parameterize the
discriminator and generator, which may lead to the loss of the property that the coupling is linear
over discriminator. Next, we present several cases where this linear structure stays.
3
Under review as a conference paper at ICLR 2019
2.2.1	Wasserstein GAN in LQG setting
Wasserstein GAN in linear quadratic Gaussian (LQG) setting was proposed in Feizi et al. (2017) to
understand GAN. In this simplified case, the data distribution is Gaussian, the cost is quadratic and
the generator is linear, namely,
c(χ,y) = 1kχ — yk2, y ~N(0,∑y), X-N(0,ik), G(X) = θχ.
It can be shown that it suffice to consider discriminator of the form
φ(X) = 1 Ilxk2 — 1XTAx, ψ(y) = 2Ilyk2 一 ∣yτBy
with A, B being positive definite.
The constraint φ(x) + ψ(y) ≤ 1 ∣∣x 一 y∣2 implies that B 占 A-1. Consequently, the discriminator
can be parametrized by a single variable A 0. Additionally, G(X) is a zero mean Gaussian random
variable with covariance θθT . Therefore, the Wasserstein GAN in equation 3 then reduces to
min maxTr(Σy) + Tr(θθT) - Tr(AθθT) - Tr(A-1Σy),
(5)
which is apparently in the form of equation 4. When only samples {xi}in=1, {yi}in=1 of x, y are
available, this becomes
1n1	1	1n1	1
θmd×maxnX(2kyik 一2yiA-yi) + nX(2kθχik 一2xiθAθxi)∙
∈	i=1	i=1
(6)
2.2.2	GANs with discriminators linear on features
In general quadratic discriminators are not sufficient to distinguish complicate high dimension
distributions. In order to deal with more general data sets, we consider the setting where the
discriminator is a linear combination of predefined basis functions. More specifically, let {Fi}in=1 be
the basis functions, and F = P αi Fi with some constraint on α, then the above formulation becomes
min max
G∈G α∈Rn
αiE(Fi(y)) - X αiE(Fi(G(x)) - λ∣α∣2	.
i=1
(7)
Here the term λkαk2 with λ > 0 is used to regularize α, or equivalently, the discriminator.
Similarly, for GAN structure in equation 3, we can restrict our discriminators φ and ψ to be linear
combinations of basis functions {φi}in=1 and {ψi}in=1, that is, φ = P αiφi, ψ = P βiψi. The
constraint φ(x) + ψ(y) ≤ c(x, y) in equation 3 is difficult to impose precisely. Instead, we use a
regularization term l and obtain
min max
G∈G α,β
αiE{φi(G(X))} + X βiE{ψi(y)} 一 l(α,β) .
(8)
Clearly, both equation 7 and equation 8 are of the form equation 4. We remark that no constraint has
been imposed on the generator G; it can be any general neural network. The requirement that the
discriminator is a linear combination of predefined basis functions could be strong, but in principle,
any function can be approximated to an arbitrary precision with large enough bases.
3	Algorithm design and convergence analysis
In this work, we consider the following general min-max problem,
1n
x∈ix max f(X, Y)，-∑ (hi(X) + hgi(X ),Y i 一 E))
∈	∈	n i=1
(9)
where X is a convex and compact set and the size of kX k is upper bounded by σX ; hi (X) :
Rd → R, ∀i is a non-convex function and has Lipschitz continuous gradient with constant LX ;
4
Under review as a conference paper at ICLR 2019
li (Y ) : Rd → R, ∀i are strongly convex with modulus γ > 0 and Lipschitz gradient constant LY ;
the matrix function gi(X) : X → dim(Y ) can also be non-convex, and it is assumed to be Lipschitz
and Lipschitz gradient continuous with constants Lg,1 and Lg,2. We note that regardless of whether
Y is a bounded set or not, one can show that for all X ∈ X, the maximizer Y * for the maximization
problem lies in a bounded set; see Lemma 3 in the appendix for proof.
We note that by allowing constraints in the form of x ∈ X, and y ∈ Y, one can also include
nonsmooth regularizers in the formulation. As an example, if we add λkX k1 into the objective
function, one can introduce a new variable z, and consider an equivalent problem with the constraints
∣∣X∣∣ι ≤ z, with the objective function changed to n Pn=ι (hi(X) + hgi(X),Yi - li(Y)) + z.
The above formulation is quite general. Compared with the existing convex-concave saddle point
literature (Hamedani & Aybat, 2018), (Chen et al., 2013), our formulation allows non-convexity in the
minimization, which is essential to modelling the neural network structure of generators in GANs and
general non-convex supervised tasks in multi-task learning; Compared with the non-convex linearly
constrained problems considered in (Hajinezhad & Hong, 2017), it further allows non-linear and
non-convex function g(X) to couple with Y. Compared with the robust learning formulation given in
equation 18, equation 9 can further include constraints and nonsmooth objective functions (thus can
include nonsmooth regularizers such as `1 norm).
It is important to note that due to the generality of problem equation 9, developing performance
guaranteed first-order method, which only utilizing the gradient information about functions hi , gi , li
is very challenging. To the best of our knowledge, there has been no such algorithm that can provably
compute even first-order stationary solutions for problem equation 9.
3.1	Algorithm description
Our proposed gradient primal-dual algorithm of solving equation 9 is listed below, in which we
alternatingly perform first-order optimization to update X and Y :
Xr+1 = arg min (1 XX(VXhi(Xr) + VX Tr(gi(Xr)Yr)) ,X - Xr∖ + 号∣∣X - Xr∣∣2,
X∈X n i=1	2
(10)
Y r+1 = argmaχ 卜1XX VY Ii(Yr)+gi(X r+1),Y- Y)- ⅛ρ kγ - Yrk2
To be consistent with the optimization literature, we will refer to the X-step the “primal step” and
the Y-step as the “dual step”. We note that 1∕β and P are two positive parameters, that represent
stepsizes of the two updates, and both of them should be small. A few remarks are ready.
Remark 1 (projected gradient). It can be easily verified that the updates of Xr and Yr can be
written down in closed form using the following alternating projected gradient descent/ascent steps:
Xr+1 = projχ (Xr - β (1 XX (Vχhi(Xr) + Vx Tr(gi(Xr)Yr)))),
Y r+1= projγ 卜 r + pg XX Vγ li (Yr)+ gi(Xr+1))).
Remark 2 (stochastic vs deterministic algorithm). This work will be focused on the deterministic
algorithm given in equation 10, because such an algorithm is representative of the primal-dual
first-order dynamics used in training GANs and optimizing robust ML problems, and it is already
challenging to analyze. However, we do want to remark that, it is relatively straightforward to
build upon our proof, by incorporating the standard technique in stochastic constrained optimization
(Ghadimi et al., 2016) (such as using decreasing stepsizes, and certain randomization rule in picking
the final solutions), to analyze the stochastic version of the algorithm, in which mini-batches of the
component functions are randomly selected to update at each iteration. However, in order to keep the
discussion of the paper simple, we choose not to present such results.
5
Under review as a conference paper at ICLR 2019
3.2	Convergence Analysis
In this section, we present our main convergence results for the primal-dual first-order algorithm
given in equation 10. We first present a few necessary lemmas.
Lemma 1. (Descent Lemma) Let (Xr, Y r) be a sequence generated by equation 10. The descent of
the objective function can be quantified by
f(Xr+1,Yr+1) - f(Xr,Yr) ≤ - (2 - LLX) kXr+1 - Xrk2
1 /	1	Lχ + 1
+ 2	(LY	+ Lg,ι	+ P) kYr+1	-	Yrk2 +	kYr	-	Yr-1k2.	(11)
From Lemma 1, it is not clear whether the objective function is decreased or not, since the primal step
will consistently decrease the objective value while the dual step will increase the objective value.
The key in our analysis is to identify a proper “potential function”, which can capture the essential
dynamics of the algorithm, and will be able to reduce in all iterations.
Lemma 2. When the following conditions are satisfied,
γ
ρ<LY
β ≥ max ∣Lg,ι + (LY + Lg,1 + P)((6；ILlX+ Lg,2 +2ρL2,1) ,Lχ + σγ LgJ
Y	(12)
then there exist c1,c2, c3, d > 0 such that potential function will monotonically decrease, i.e.,
Pr+1 -Pr ≤ -c1 kXr+1 - Xrk2 - c2kY r+1 -Yrk2 -c3kYr - Y r-1k2	(13)
where Pr+1，f (X r+1,Yr+1)+ d Qr+1 and
Qr+1 , (LX[β +1) + 1) kXr+1 - Xrk2 + (2pβ - γ--2βLY) kYr+1 - Yrk2.
To state our main result, let us define the proximal gradient of the objective function as
VL(X, Y)
X - PrOjXX -Vχf (X,Y)]
Y-projY[Y+VYf(X,Y)]
(14)
where Proj denotes the convex Projection oPerator. Clearly, when VL(X, Y) = 0, then a first-order
stationary solution of the Problem equation 4 is obtained.
Theorem 1. Suppose that the sequence (Xr, Yr) is generated by equation 10 and ρ, β satisfy the
conditions equation 12. For a given small constant , let T() denote the iteration index satisfying
the following inequality
T(),min{r| kVL(Xr,Yr)k2 ≤,r≥ 1}.	(15)
Then there exists some constant C > 0 such that
C(PI-P)
e- T (e)
(16)
where P denotes the lower bound of Pr.
The above result shows that our ProPosed algorithm converges to the first-order stationary Point of
the original Problem in a sublinear rate.
4	Experiments
We conduct several exPeriments to illustrate our results. The first two examPles are on GANs with
both synthetic data and MNIST dataset, and the last one (suPPlemental material) is on multi-task
learning with real data. Our intention is by no means to show our algorithm generates suPerior
samPles than other methods. Our main goal is to show that our first-order Primal-dual algorithm
would converge at least to a local solution. All exPeriments are imPlemented on a NVIDIA TITAN
XP.
6
Under review as a conference paper at ICLR 2019
Table 1: Comparison of time elapsed (in seconds) for different dimensions.
	5D	10D	20D	50D
ours	130.27	134.76	135.47	380.98
Sanjabi et al.	524.00	543.52	545.18	1603.68
4.1	GAN in LQG setting
In the LQG setting, as discussed in Section 2.2.1, the generator is modeled by a linear map G(x) = θx
with parameter θ and the discriminator is parametrized by a positive definite matrix A. The seed
x is a zero-mean random variable with unit covariance. We randomly generate a positive definite
matrix Σy as our covariance of the data samples. The solution to this GAN problem satisfies A = I,
θθT = Σy.
We implement our algorithm with different step-sizes. The results are shown in Figure 1 for 20
dimensional data, from which we see that the algorithm converges when the step-size is sufficiently
small, while diverges for a large step-size. We also compare our algorithm with the one proposed
in Sanjabi et al. (2018), which requires solving the inner maximization problem in each iteration.
As can be seen from the last plot of Figure 1, while these two algorithms take a similar number of
iterations to converge, the total time consumption is more for Sanjabi et al. (2018) as it takes more
time to solve the maximization problem than to update the parameter one step along the gradient
direction. A detailed comparison is given in Table 1.
(a) ρ = 0.0001, β = 10000	(b)ρ = 0.005, β = 1000	(c) Sanjabi et al. (2018)
Figure 1:	(a) converge and (b) diverge show the effect of step-sizes; (c) is based on the algorithm in
Sanjabi et al. (2018).
To visualize the effectiveness of LQG GAN, we consider the problem in 2 and 3 dimensional spaces.
We plot the samples corresponding to both the learned and the real covariance matrices in Figure 2.
Clearly, the learned models match the underlying truth.
(a) d = 2	(b) d = 3
Figure 2:	Real (green) vs generated (red) samples
7
Under review as a conference paper at ICLR 2019
4.2	GANs with discriminators linear on features
We test the GANs framework with discriminators linear on features (discussed in Section 2.2.2) on the
MNIST (LeCun et al., 1998) data of size 28 × 28. The network architecture of the generator is same
as DCGAN (Radford et al., 2015). To get the reasonable basis functions for the discriminator, we first
train a Wasserstein GAN model with a subset of the MNIST data for a small number (5k) of iterations.
We then use the last hidden layer of the discriminator as our bases. We implement our algorithm
with a different number of basis and the results are shown in Figures 3 and 4. We have two major
observations here: i) the GANs with discriminators linear on features generate reasonable samples
and the performance improves as we increase the number of bases; ii) the algorithm converges with a
small step-size while diverges for a large step-size.
便zk"嘤€A*^∙∙⅛⅛*zx⅛⅛
庚夕才^君中沪.⅛⅛⅛⅛f疝⅛
酒廿.「曹春吗 温 1'⅛l⅞^⅝ms^ɪf(•<, ^∙
承啰力小窑⑷承-■■>'，电拿`温
承.温@嚏8卷电❖.❖拿虱>
<❖. X序.嬉邃</?2/中
>■■#:"❖■ ⅛ .■/->0 M⅛∙∙r■、
⅛⅛⅛叠⅛Z,女 立❖更雪
⅛d 工流 j¾∙受❖口⅛齿。
尊曹(警方⅛受尊摩叠-∙-e⅛叠
发⅛y.^一若•安一.不嗷X.「<.零脸 重、
,电⅞电承⅛-.*⅛⅛f⅛⅛‹更翼力
771:Tqz?qs£ qπ≡
j7f733Q∙ΓON7/
/7,13RO&34 善少 4
:r 3oγ*2q ;3,明sf3
75 7 * G —74 2 71 \
MY卒4e / & q<5ff&i
d方7⅛⅛ 7©430q —
5 4**■ z / G夕 ?344.4
6 f ð 7 7S4 1 N 7-0
&Zr/ 7万岑ʃa /97
3⅛玄SG 7 9 7 q 4..3 3
W — 3，3c6 b 3r-73 O
J J 界∙⅜ / 3 2 Nc?
24Jqa7,¾d∕7 G⅛
QS 2 w∙sz,7f 4 升ɪ
“34*，，；5。^/二1上;区.曲
5,240s,¾*4-<⅛/
<⅛/ WW ?∙o6⅛* .q∙⅛f
.i 7 D 7 *v<j2 ⅛Λ∙⅛ / .- - 3
*"Λ^ζ⅛ 3 弓与ir6 Wdr 看 C
7 g t 3 夕一" 二；，q3i? 3
4 nσ" 4 01¼7，玷 Z
¢C>⅛7.∙妤 d 里，-N ⅛r / 2
ɔ邑 GaW <½ f，T 7 f 三
(a) 128
(b) 1024
(c) 4096
Figure 3: Generated samples with different number (a) 128, (b) 1024, and (c) 4096 of basis functions.
(a) converge	(b) diverge
Figure 4: Effect of stepsizes: the algorithm converge for small stepsize and diverge for large stepsize
(c) diverge samples
5	Conclusion
In this work, we presented a convergence result for a first-order algorithm on a class of non-convex
max-min optimization problems that arise in many machine learning applications such as generative
adversarial networks and multi-task learning. To the best of our knowledge, this is the first
convergence result for this type of primal-dual algorithms. Our results allow us to analyze GANs
with neural network generator as well as general multi-task non-convex supervised learning problems.
A critical assumption we made is that the inner maximization loop is a strictly convex problem.
For applications in GANs, our assumptions require the discriminator to be a linear combination of
predefined basis functions. Extending this to the most general cases where the discriminator is a
neural network requires further investigations and will be a future research topic.
8
Under review as a conference paper at ICLR 2019
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Xu Chen, Jiang Wang, and Hao Ge. Training generative adversarial networks via primal-dual
subgradient methods: a Lagrangian perspective on gan. In Proc. of International Conference on
Learning Representations (ICLR), 2018.
Y. Chen, G. Lan, and Y. Ouyang. Optimal primal-dual methods for a class of saddle point problems.
SIAMJ. Optim, 24(4):1779-1814, 2013.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with
optimism. arXiv preprint arXiv:1711.00141, 2017.
Farzan Farnia and David Tse. A convex duality framework for gans. In Proc. of Conference on
Neural Information Processing Systems (NIPS), 2018.
Soheil Feizi, Changho Suh, Fei Xia, and David Tse. Understanding GANs: the LQG setting. arXiv
preprint arXiv:1710.10793, 2017.
S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex
stochastic composite optimization. Mathematical Programming, 155(1-2):267305, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. of Advances in Neural
Information Processing Systems (NIPS), pp. 2672-2680, 2014.
D.	Hajinezhad and M. Hong. Perturbed proximal primal dual algorithm for nonconvex nonsmooth
optimization. 2017. Submitted for publication.
E.	Y. Hamedani and N. S. Aybat. A primal-dual algorithm for general convex-concave saddle point
problems. 2018. Submitted for publication, available at: arXiv:1803.01401.
Martin HeuSeL Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium.
arXiv preprint arXiv:1706.08500, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Proc. of Advances in
Neural Information Processing Systems (NIPS), pp. 4565-4573, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. In Proc. of Advances in Neural Information Processing Systems
(NIPS), pp. 1097-1105, 2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic
single image super-resolution using a generative adversarial network. In Proc. of IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 4, 2017.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. Towards understanding the dynamics
of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. On the limitations of first order
approximation in GAN dynamics. 2018.
9
Under review as a conference paper at ICLR 2019
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International
Conference on Machine Learning, pp. 1718-1727, 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In Proc. of International Conference
on Learning Representations (ICLR), 2018.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least
squares generative adversarial networks. In Proc. of International Conference on Computer Vision
(ICCV), pp. 2813-2821, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Proc. of
Advances in Neural Information Processing Systems (NIPS), pp. 1825-1835, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In Proc. of International Conference on Learning Representations
(ICLR), 2018.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429-443, 1997.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. In
Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 5585-5595, 2017.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust
optimization with f-divergences. In Proc. of Advances in Neural Information Processing Systems
(NIPS), pp. 2208-2216, 2016.
Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play
generative networks: Conditional iterative generation of images in latent space. In Proc. of IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 7, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers
using variational divergence minimization. In Proc. of Advances in Neural Information Processing
Systems (NIPS), pp. 271-279, 2016.
Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li. Robust optimization over
multiple domains. arXiv preprint arXiv:1805.07588, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Proc. of Advances in Neural Information Processing
Systems (NIPS), pp. 2234-2242, 2016.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D. Lee. On the convergence and
robustness of training GANs with regularized optimal transport. In Proc. of Advances in Neural
Information Processing Systems (NIPS). 2018.
Abhay Yadav, Sohil Shah, Zheng Xu, David Jacobs, and Tom Goldstein. Stabilizing adversarial
nets with prediction methods. In Proc. of International Conference on Learning Representations
(ICLR), 2018.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv
preprint arXiv:1609.03126, 2016.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. The information autoencoding family: A
Lagrangian perspective on latent variable generative models. In Proc. of Conference on Uncertainty
in Artificial Intelligence (UAI), 2018.
10
Under review as a conference paper at ICLR 2019
A Robust multi-task machine learning
Multi-task machine learning (Qian et al., 2018) aims at learning a single model that would work for
several different machine learning tasks. Let
minE{fι(x1,W)},…，minE{f (xn,W)}
be n supervised learning problems, then a multi-task formulation is
n
minXpiE{fi(xi,W)},
i=1
(17)
where p is a probability vector to weight the tasks. A common choice is the uniform distribution
P = [1/n,..., 1/n]. To improve the worst case performance, one can change P adaptively and attain
the max-min formulation
n
minmax{XpiE{fi(xi, W)} - λD(p,p) + h(W)},	(18)
W p i=1
where D is a distance function to regularize p. Here we have also added an regularization term on W .
A closely related topic is the distributional robustness (Namkoong & Duchi, 2016) problem
n
min max	pif(xi, W),
W p∈P
i=1
(19)
where P is a subset of the space of probability vectors. Relaxing the hard constraint onp points to a
regularized version
n
min max〉Pif (xi, W) — λD(p,p)
Wp
i=1
(20)
While p represents weights on different tasks in multi-task learning, it describes data distribution
in distributional robustness. It beautifully incorporates data uncertainties in the learning problems.
This formulation has also applications in adversarial learning Madry et al. (2018), where pif(xi, W)
denotes the loss given by data xi after adversarial reweighing through the inner maximization problem.
The goal of the outer minimization is to learn a model with parameters W such that the worst case
loss is minimized.
A.1 Numerical example
We consider two supervised learning tasks with MNIST (LeCun et al., 1998) dataset and CIFAR10
(Krizhevsky & Hinton, 2009) data set. We seek a single neural network that works for these two
completely unrelated problems (see Section A). First, we convert the MNIST data from 28 × 28
gray images to 32 × 32 color images so that it is in the same format as CIFAR10. We use a standard
AlexNet (Krizhevsky et al., 2012) as our model. We train the model with the robust multi-task
learning framework and compare it to the results from three other methods: train with MNIST only,
train with CIFAR10 only and train with both data sets but with even weight [0.5, 0.5]. The batch size
we use is 128.
The results are presented in Table 2. The last row is the results for the robust multi-task learning, and
the “even mixture” in the second last row standards for uniform weight [0.5, 0.5] between the two
tasks. We see that the result of using robust multi-task learning framework is better than the one with
even weight. Moreover, its performance on each task is comparable to that trained from a single data
set alone. The optimal p is [0.205, 0.795], where the first value is for the MNIST dataset. We also
implement our algorithm with different levels λ of regularizations. The results are shown in Figure
5, where we display the loss functions in the first two plots and the weight for MNIST in the last
plot. Even though changing λ doesn’t affect the convergence rate that much, it changes the optimal p
significantly.
11
Under review as a conference paper at ICLR 2019
Table 2: Multi-task Learning: Comparison of Accuracies
Method		MNIST				CIFAR10		
	Training	Test	Training	Test
MNIST	99.98	=	99.33	=	9.24	=	9.16	=
CIFAR10	T025	TgTTg	^9975	^7659
Even Mixture	^9996	-99.40	-99.42	■74:97
OPT MixtUre	99.98	—	99.35	—	99.85	—	76.29	—
0.. ɪ00 ...200. 3.00....400
Number of iterations (x 100)
(a) λ = 0.1
Number of iterations (x 100)
(b) λ = 0.005
Number of iterations (x 100)
(c) The change of p across iterations
Figure 5: Effect of regularization. Here p is the weight, W is the parameter and λ is the regularization
constant. The regularizer D is taken to be the Kullback-Leibler divergence.
B Convergence Analysis of the Primal Dual Algorithm
In our convergence analysis of the primal-dual algorithm, we use the optimality conditions of Xr-
and Y r-subproblems repeatedly so that the quantities of measuring the size of the difference of the
iterates, e.g., kXr+1 - Xrk and kY r+1 - Yrk, can be obtained. Also, for the simplicity of the
notation, we have the following definitions:
n	nn
h(χ)，n X h/χ),	g(χ)，n Xgi(χ), i(y)，- Xmy).
n i=1	n i=1	n i=1
Then, we give the optimality condition of χr -subproblem and Yr -subproblem as follows,
(Vh(Xr) + X VXgj,k(Xr)Yrk + β(Xr+1 - Xr),Xr+1 - χ∖ ≤ 0,∀X ∈ X,
j,k
Yr+1 = Yr + ρer+1 + P (g(Xr+1) - Vyl(Yr)).
(21)
(22)
(23)
where -er+1 denotes the subgradient of the convex indicator function 1(yr+1 ∈ Y), Yjr,k denotes the
entry at the j row and kth column of Yr, and gj,k(Xr) denotes the matrix value function mapping
from Xr to the value at thejth row and kth column ofg(Xr). Note that equation 23 is also equivalent
to
(一Vyl(Yr) + g(Xr+1) - -(Yr+1 - Yr),Yr+1 - Y)≥ 0, ∀Y ∈ Y.	(24)
Before going to the details, we will first introduce the following lemma that characterizes the upper
bound of kYrk.
Lemma 3. Let (Xr, Yr) be a sequence generated by equation 10. The size of Yr is upper bounded
by some constant number denoted by σY.
Proof. Letζ(X,Y) , l(Y) - hg(X),Yi. Define:
Y*，arg min Z(Xι,Y) Y*，arg min Z(X2,Y)
(25)
12
Under review as a conference paper at ICLR 2019
From the optimality conditions, we know that
NYZ(Xι,Y*),Y*- Y*i≥ 0 GYZ(X2,Y*),Y*- Y*i≥ 0.	(26)
Adding these two inequalities, we have
GYZ(X1,Y*) - VγZ(X2, Y*), Y* - %*〉≥ 0,	(27)
which implies that
Lg,1kX1 - X2kkY1* -Y2*k
≥ hg(X2) - g(X1), Y1* -Y2*i
= hVYζ(X2,Y2*) -VYζ(X1,Y2*),Y1* -Y2*i
≥ hVYζ(X1,Y1*) -VYζ(X1,Y2*),Y1* -Y2*i
= hVl(Y1*) - Vl(Y2*), Y1* -Y2*i
≥ γkY1* - Y2* k2
where in the first inequality we used the Lipschitz continuity; in the last inequality we used the strong
convexity of function l(Y). Therefore, we have
kY*- Y*k≤ LgikX1- X2k.	(28)
γ
Since X ∈ X, with kXk bounded by σX, we have the claim that the distance between any two
Y* and Y* are bounded by 2Lg,ισχ/γ. In other words, Yχr, ∀Xr ∈ X are within a compact sets,
where YX*r , arg minY ∈Y ζ(Xr, Y) and the radius of the set is upper bounded by 2Lg,1 σX /γ.
Let Yr+1 denote the r + 1th iterate of the projected gradient descent method of solving the dual
problem which is parameterized by Xr+1. Because the dual problem is strongly convex, it is standard
to show that
kY r+1 - YX* r+1k ≤ kYr -YX*r+1k.	(29)
This implies that the distance between the iterate and the set is not increasing. The proof is complete.
□
Throughout our analysis, we used σY ≥ kYrk1, ∀r where kYk1 , i,j |Yi,j|. Note that kYk1 ≥
kY kF . Based on this definition and the previous lemma, it is easy to check that the following holds
kVf(X,Y)-Vf(Z,Y)k ≤ (LX + σY Lg,2)kX - Zk	(30)
where LX and Lg,2 are two constants defined after equation 4.
B.1 Proof of Lemma 1
B.1.1	Primal Problem
Define
f(X,Y),h(X)-l(Y)+hg(X),Yi.
(31)
First, suppose that we choose β large enough such that β ≥ LX + σY Lg,2 . Then we have the
following estimate of the descent of the objective value:
f(Xr+1, Y r) -f(Xr,Yr)
=h(Xr+1) + hg(Xr+1), Y ri -h(Xr) - hg(Xr),Yri
≤)hVχh(Xr) + X VXgi,j(Xr)Yrk,Xr+1 - Xri + LX +σYLg,2 kXr+1 - Xrk2
i,j
(32)
≤) - 2 kXr+1 - Xr k2
when in (a) we used the equation 30; In (b) wej used the optimality condition equation 22, and we
choose β ≥ LX + σY Lg,2 .
13
Under review as a conference paper at ICLR 2019
B.1.2	Dual Problem
For the dual problem, we have
f (Xr+1,Yr+1) - f (Xr+1,Yr)
=-l(Yr+1) + (g(Xr+1),Yr+1 - Yri + l(Yr) - (1(Yr+1 ∈Y) - 1(Yr ∈ Y))
(a)
≤ -(Vyl(Yr) - er, Yr+1 - Yri + (g(Xr+1), Yr+1 - Yr)
=-(VYl(Yr) - er+1,Yr+1 - Yr) - (er+1 - er,Yr+1 - Yr) + (g(Xr+1),Yr+1 - Yr)
=1 (Yr+1 - Yr Yr + 1 - Yr) - (6r⅛1 - Cr Yr+1 - Yr)
ρ'	7	7
≤ P(Yr+1 - Yr ,Yr+1 - Yr) + 1 ( (LY + Lg,1 - P) ∣∣Yr+1 - Y rk2
+ 冬∣∣Xr+1 - Xrk2 + 2 (LY + P) kYr - Yr-1k2)
=1 ((LY + Lg,1 + P)IIYr+1 - Yrk2 + HIlXr+1 - Xrk2
+ 2	IYr - Yr-1k2
where in (a) we used the convexity of both function l(X) and indicator function, i.e., l(Yr+1) +
1(Yr+1 ∈Y) - (l(Yr) + 1(Yr ∈Y) ≥ Ryl(Yr) - J,Yr+1 - Yr)，in (b) we used equation 23
and (c) is true because the following relations:
1.	First, from equation 23 we have
Yr+1 =Yr + ρ(g(Xr+1) - Vl(Yr) + er+1),	(33)
Yr =Yr-1 + ρ(g(Xr) - Vl(Yr-1) + er),	(34)
which imply
er+1-er = 1(Yr+1 - Yr - (Yr - Yr-1)) - (g(Xr+1) - g(Xr)) + Vl (Yr)-Vl(Yr-1).
P	(35)
2.	Second, we have
(er+1 - er ,Yr+1 - Yr)
=1 (Vr+1,Yr+1 - Yr) - (g(Xr+1) - g(Xr),Yr+1 - Yr)
-(VYl(Yr) - Vyl(Yr-1), Yr+1 - Yr)
(a)	1	1
≥ -而(∣Yr - Yr-1k2 -IYr+1 - Yrk2) - D(LY + Lg,1)kYr+1 - Yrk2
-1 Lg,1kXr+1 - Xrk2- 2LYIlYr - Yr-1k2
≥- 1 ((ly + Lg,1 - P) ∣Yr+1 - Y r∣2 -与 IlXr+1 - X r∣2
-1	IYr - Yr-1k2)
where in (a) we used Cauchy-Schwarz inequality.
14
Under review as a conference paper at ICLR 2019
Finally, combing the above results, we have
f (X r+1
,Yr+1) - f (Xr ,Yr)
≤ -2kXr+1- Xrk2 +2 (Ly + Lg,1 + p) ∣∣Yr+1 - Yrk2
primal descent
+ L2L kXr+1 - X rk2 + 1 (LY + ρ) ∣∣y T- Y r-1k2.
(36)
B.2 PROOF OF LEMMA 2
Proof. Let Wr+1，(Xr+1 - Xr) - (Xr - Xr-1). Subtracting equation 22 at iteration r - 1 from
itself at the rth iteration, we have the successive difference of equation 23, i.e.,
〈Vxh(Xr) -Vχh(Xr-1)
EVgj,k (Xr )γr∖k -∑,Vgj,k (X r-1)γr-1	+βW r+1,Xr+1 - Xr〉≤ 0.
(37)
∖ j,k
j,k
^—^^™
，A
Similarly, from equation 23, we also have
Yr+1 - Yr - (Yr - Yr-1) = ρ (Cr+1 - Er
+ P (g(Xr+1) - g(Xr)) - ρ (Vl(Yr) - Vl(Yr-1)).
(38)
B.2.1 Induction of THE SIZE OF THE Successive Iterates
First, we note the following key inequality
(Wr+1,Xr+1 - Xri = 1 ∣∣Xr+1 - Xrk2 - 1 ∣∣Xr - Xr-1k2 + 1 ∣∣Wr+1k2.
(39)
Second, we can get the lower bound of hAr, Xr+1 - Xr)as follows:
VXgj,k(Xr)Y：k - X Vj,k(Xr-1)γr-1,Xr+1 - Xr∖
j,k	/
(40)
v”k (Xr) (Yrk- γr-1) ,x r+1 - Xr)
+ (X(Vgj,k(Xr) - V”k(Xr-1)) Yjr-1, Xr+1 - Xr)
∖ j,k	/
XX Vxm,n "k (χr) (Yrk- Yr-I) (Xmn - χm,n)
m,n j,k
+ XX(VXm,n "k (X r ) - Vχm,n ”k (X r-1)) Yr-I(Xmn - X2,n
m,n j,k
(41)
(42)
XhVXgj,k(Xr),Xr+1 - Xr心k
j,k
X---------------------------
Yr-I
Yj,k
,A：
+ X〈Vxgj,k(Xr) - Vxg∕(Xr-1), Xr+1 - Xr〉Yj-
(43)
j,k
I^
{z^^
，Ar
一A2
+
,
I
{z','∙∕'^^
,V r + 1
,
)
—
,
,
15
Under review as a conference paper at ICLR 2019
In the above derivation, we basically changed the order of summations.
From the mean value theorem, we know that there exits Xr+：) ∈ [Xr ,Xr+1] such that
gm,n(Xr+1) - gm,n(Xr ) = DVX gm,n(XrX)), X	— X')) ∙	⑷)
Hence, Ar becomes
EhVXgj,k(Xr),Xr+1 - Xr)(倏 - Yr-I)
j,k
=XhVXgj,k(Xr) - Vχgj,k(Xr+1), Xr+1 - Xr乂Yl- Yr-I)	(45)
j,k
+ XhVXgj,k(Xr+1), Xr+1 - Xri(Yrk - Yr-I)
j,k
=XhVXgj,k(Xr) -Vxgj,k(Xr+1),Xr+1 - Xri(Yjrk - Yr-I)
j,k
+ XIgj,k(Xr+1) - gj,k(Xr))(Yrk - Yr-I)
j,k
≥ -IYr - Yrτ∣∣1∣∣Xr+1 - Xrk∣∣Xr+1 - Xr∣∣Lg,2
+(Vr+1 + Vyl(Yr) - Vyl(Yr-1), Yr - Yr-1)- (er+1 - er, Yr - Yr-1〉
(a)
≥ -kYr - Yr-1kιkXr+1 -XI2Lg,2 +
-her+1 - er,Yr - Yr-1i
V r+1
-----+ Vyl(Yr) - Vyl(Yr-1), Yr - Yr-1
P
where in (a) we used Cauchy-Schwarz inequality.
Combining Ar and Ar, we can get the lower bound of(Ar, Xr+1 - Xr)，i.e.,
hAr,Xr+1 - Xr)
≥-∣∣Yr - Yr-1∣∣1∣∣Xr+1 - Xr∣∣2Lg,2 +
V r+1
----+ Vyl(Yr) - Vyl(Yr-1),Yr - Yr-1
P
-her+1 - er,Yr - Yr-1i + ∑(Vxgj,k(Xr)) -Vχg∙,k(Xr-1)),Xr+1 - Xr>Yr-1
≥-∣∣Yr - Yrτ∣∣1∣∣Xr+1 - Xr∣∣2Lg,2 +
V r+1
--------er+1 - er, Yr - Yr-1
P
+ (Vyl(Yr) - Vyl(Yr-1), Yr - Yr-1〉
-2∣Yr-1k1Lχ (∣∣Xr - Xr-1 k2 + ∣Xr+1 - Xrk2) ∙
To further bound the above terms, we will give the upper bound of
Vr - Yr-1,Vr+1 /ρ - (er+1 - er)〉and hVγl(Yr) - Vyl(Yr-1),Yr - Yr-1〉separately
in the following two steps.
16
Under review as a conference paper at ICLR 2019
Step 1). First, we have
-Yr-1,Vr+1 - (Er + 1 - Er )
-Yr-1
P
(Yr+1 - Yr) + Yr+1 - Yr, Vr+1 - (Er+1 - Er)
(46)
/V r+1 V r+1
P
=P(ll -
—
-(er+1
1
—
V r+1
--------(Er+1 - Er
-2rι (∣Yr+1 - Yr∣2
2P
P
Yr , Vr+1 - (Er+1 - Er )
P
)2 -∣Er+1 - Er∣2
-IlYr - Yr-1∣2 + IlVr+1∣∣2) + (Yr+1
Yr ,er+1 - er i
_- ,
{z
≤0
(47)
(48)
V r+1
--------(Er+1 - Er)
P
21	21	2
-2pIIy r+1 - y rII2 + 2pIIy r - y r-1ll2
(49)
where in (a) we used
≤P
—
P
2
+
P
—
(-er+1 - (-er),Yr+1 - Yr)≥ 0
(50)
due to the fact that -er+1 is the subgradient of the convex indicator function 1(yr+1 ∈ Y), and also
the equality
(Yr - Yr-i,yr+ι) = 1 ∣∣Yr+ι - Yrk2 - 1∣Yr - Yr-i∣2 - 1 ∣∣yr+i∣2.
(51)
Further, from the optimality condition of the Yr subprolbem shown in equation 23, we know
V r+1
丁 -(C
_r + 1
-er) = g(Xr+1) - g(Xr) - (Vyl(Yr) - Vyl(Yr-1)),
(52)
which gives
P
2
V r+1
P
-(L - Er
2
)≤ PL"∣Xr+1 - Xr∣2 + PLy∣Yr - Yr-1∣2
(53)
where we used the Lipschitz continuity.
Step 2). Next, we need to quantify the lower bound of〈Vy l(Yr) -Vyl(Yr-1), Yr - Yr-1). Since
l(Y) is strongly convex, we have
hVyl(Yr) -Vyl(Yr-1),Yr - Yr-1i ≥ YkYr - Yr-1k2.
(54)
Combining step 1 and step 2 and equation 46, we have the lower bound of (Ar ,Xr+1 - Xr)
(Ar,Xr+1 - Xr)≥ -∣∣Yr - Yr-1kιLg,2∣∣Xr+1 - Xrk2 - 1 ∣∣Yr-1kιLχ∣∣Xr+1 - Xr∣∣2
-1 kYr-1∣Lχ∣Xr - Xr-1k2 + 2P∣Yr+1 - Yrk2 - 2P∣Yr - Yr-1k2
-ρL2,1∣Xr+1 - Xr∣2 - PLy∣∣Yr+1 - Yr∣2 + YkYr - Yr-1∣2. (55)
17
Under review as a conference paper at ICLR 2019
Therefore, dividing equation 37 by β and combining equation 39, equation 55, we have
2∣Xr+1 - Xrk2 + 2pβ ∣Yr+1 - Yrk2
≤ 1 ∣X T- X r-1k2 + 2pβ ∣Y T- Yr-1 k2 -1 ∣W r+1k2
(56)
2L
+
,g,2∣∣Yr - YrT∣∣1 + LX kYrT∣∣1 + LX + 2ρL%.
Lx IY rτ∣∣ι + Lχ
(a)1
≤ 2 ∣Xr-X'
2β
丁-1k2 + —
I +2ρβ
2β
∣∣XT - Xr-1k2 -
口Xr+1 - Xrk2
Y - PLY
∣∣yr - YTT ∣∣2 -Y
β
-PLY
~1-
∣∣Yr - Yr-1k2
∣∣Yr - YTTk2
一 /
5σY LX + Lg,2 + 2PLgJ
2β
'∙^^^^^^^^^^^^^{^^^^^^^^^^^^^^
dual descent
∣∣Xt+1 - XTk2 + LX(σγ +1) ∣∣XT - XTTk2
2β
(57)
+
+
where in (a) we used σγ ≥ ∣∣Yt∣1, ∀r.
B.2.2 POTENTIAL FUNCTION
Rearranging equation 57, we have
QT+1 ,
LX% +1) + 2) kXt+1 - Xtk2 +	- TLY) kYt+1 - Ytk2
2β	2	2β
Lx⅞+jl + 1) kXT -XTTk2 + (2Pβ - T) kYT - YTTk2
Y - PLY k γt+1 - γtk2 + (6σY + I)LX + Lg,1 + 2pLg,1 ∣∣xt+1 - Xt12
=Qt +
(6σY + I)LX + Lg,2 + 2PLg,1
2β
FLY∣Yt+1 - Ytk2 -
2β
∣XT+1 - XT ∣2
I-PLY∣γt - γTTk2.
(58)
≤
—
—
—
Define a potential function PT+1，(Xt+1, YT+1) + dQτ+1. We can obtain
Pτ+1 -PT ≤ -
/ ∖
β _ Lg,1 _ d^6σY + I)LX + Lg,2 + 2PLg,1)
2 - ^-2	2β
S----------------------V-------------------------'
∖	,C1
∣XT+1 - XT ∣2
(
d(Y - PLY)
-2β-
∖
∣Y T+1 - YT ∣2
( ∖
d(Y - PLY) _ LY + P
2β
'------------
∖	Δ
∖	=C3
∣∣YT - Yτ-1k2.
(59)
2
18
Under review as a conference paper at ICLR 2019
B.2.3 Convergence Conditions
If the following conditions hold, i.e., c1 , c2, c3 > 0, then the potential function has a sufficient
decrease at each iteration.
β	Lg,ι	d(6σY + 1)LX + Lg,2 + 2ρLg,ι)	∩	,6a
2 - F	2β	>0,	(60)
4(γ - PLY) - (LY + Lg,ι + -) >0.	(61)
βρ
To ShoW that ∃d ≥ 0 such that
β - Lg,1
0
0
LY + Lg,ι + p _ +一
-1 ((6σY + I)LX + Lg,2 + 2ρL2,J	0
0	1(Y - PLY)
0
by S-procedure it is sufficiently to shoW the set Xe = {x| (β - Lg,1) x2 < LY + Lg,1 + P, ((6σY +
1)LX + Lg,2 + 2PLg2,1)x2 > (γ - PL2Y)} is empty.
Hence, We require that
1.
γ - PL2Y > 0,
(62)
Which gives
γ
P<LY;
(63)
2.
3.
β ≥ Lg,1;
and We also need
β ≥ Lg,1 +
(LY + Lg,ι + P )((6σY + I)LX + Lg,2 + 2ρLg,ι)
γ-PL2y
(64)
Note that We need 焉 一 Y-PLY ≥ 0, since the potential function should be greater than 0, meaning
that
L2Y P2 一 γP + 2 ≥ 0,	(65)
Which is actually satisfied automatically by checking the tWo roots of the equation L2Y P2 一 γP + 2
(note that LY ≥ γ).
Combing the descent lemma shoWn in Lemma 1, We have the conditions of P, β shoWn in Lemma 2
so that the potential function can have a sufficient descent at each iteration.	□
B.3 Proof of Theorem 1
Proof. First, we can get the upper bound of kVL(Xr,Yr)k, which is
∣∣VL(X r ,Yr )k
≤kX r+1 - X r k + kX r+1 - projχ (X r - Vχ f(Xr ,Yr ))k
+ kYr+1 - Yrk + kYr+1 - projγ(Yr + Vyf (Xr, Yr))k
(a)
≤ kXr+1 - Xrk + kprojχ(Xr+1 - Vχf (Xr+1, Yr)) - ProjX(Xr - VXf (Xr, Yr))k
+ kYr+1 - Yrk + kprojγ(Yr+1 + Vγf (Xr+1,Yr+1)) 一 ProjY(Yr + VYf (Xr, Yr))k
(b)
≤kXr+1 一 Xrk + kVXf(Xr+1, Y r)) 一 VXf(Xr, Y r))k
+ kY r+1 - Y r k + kVγ f (X r+1 ,Y r+1)) - Vγ f (X r ,Y r ))k	(66)
(c)
≤kXr+1 - Xrk + kVXh(Xr+1) - VXh(Xr)k + σY kVXg(Xr+1) - g(Xr)k
+kVYl(Yr+1)-VYl(Yr)k
d)
≤(1+LX+σYLg,2)kXr+1-Xrk+(1+LY)kYr+1-Yrk
19
Under review as a conference paper at ICLR 2019
where in (a) we used the optimality condition of Xr-subproblem; in (b) we used nonexpansiveness
of the projection operator; in (c) we take the gradient of f(X, Y ); in (d) we used the Lipschitz
continuous of function h(X) and l(Y ).
Since Xr, Y r are within compact sets, the sizes of Xr, Y r are bounded, which implies that there
exits σ1 such that
INL(Xr, Yr)k2 ≤ σι(kXr+1 - Xrk2 + IlYr+1 - Yr∣∣2).	(67)
'--------------{z-------------}
,Cr
From equation 59, we also know that there exits a constant σ2 , min{c1, c2} such that
Pr - Pr+1 ≥ σ2Cr.	(68)
Combining equation 67, we have
INL(Xr,Yr)k2 ≤ σ1 (Pr -Pr+1) .	(69)
σ2
Summing both sides of the above inequality over r = 1, . . . , T, we have
T
X ∣∣VL(Xr,Yr)k2 ≤ σι(P1 -PT+1) ≤ σι(P1 -P)	(70)
σ	σ
r=1
where in the last inequality We have used the fact that Pr is decreasing and lower bounded by P since
Xr, Yr are within the compact sets. By utilizing the definition T (), the above inequality becomes
T(e)e ≤ σ1 (P 1 -P).	(71)
σ2
Dividing both sides by T(e), and by setting C，σ1∕σ2, the desired result is obtained	□
20