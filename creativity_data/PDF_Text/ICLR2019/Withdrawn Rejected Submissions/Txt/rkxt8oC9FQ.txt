Under review as a conference paper at ICLR 2019
Perfect Match: A Simple Method for Learn-
ing Representations For Counterfactual In-
ference With Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Learning representations for counterfactual inference from observational data is of
high practical relevance for many domains, such as healthcare, public policy and
economics. Counterfactual inference enables one to answer ”What if...?” ques-
tions, such as ”What would be the outcome if we gave this patient treatment t1?”.
However, current methods for training neural networks for counterfactual infer-
ence on observational data are either overly complex, limited to settings with only
two available treatment options, or both. Here, we present Perfect Match (PM), a
method for training neural networks for counterfactual inference that is easy to im-
plement, compatible with any architecture, does not add computational complex-
ity or hyperparameters, and extends to any number of treatments. PM is based on
the idea of augmenting samples within a minibatch with their propensity-matched
nearest neighbours. Our experiments demonstrate that PM outperforms a number
of more complex state-of-the-art methods in inferring counterfactual outcomes
across several real-world and semi-synthetic datasets.
1	Introduction
Estimating individual treatment effects1 (ITE) from observational data is an important problem in
many domains. In medicine, for example, we would be interested in using data of people that
have been treated in the past to predict what medications would lead to better outcomes for new
patients (Shalit et al. (2017)). Similarly, in economics, we would for example want to determine how
effective job programs would be based on results of past job training programs (LaLonde (1986)).
ITE estimation from observational data is difficult for two reasons: Firstly, we never observe all
potential outcomes. If a patient is given a treatment to treat her symptoms, we never observe what
would have happened if the patient was prescribed a potential alternative treatment in the same
situation. Secondly, the assignment of cases to treatments is typically biased such that cases for
which a given treatment is more effective are more likely to have received that treatment. The
distribution of samples may therefore differ significantly between the treated group and the overall
population. A supervised model naively trained to minimise the factual error would ovefit to the
properties of the treated group, and thus not generalise well to the entire population.
To address these problems, we introduce Perfect Match (PM), a simple method for training neural
networks for counterfactual inference that extends to any number of treatment options. PM effec-
tively controls for biased assignment of treatments in observational data by augmenting every sample
within a minibatch with its closest matches by propensity score from the other treatment options.
PM is easy to use with existing neural network architectures, simple to implement, and does not
add any hyperparameters or computational complexity. We perform experiments that demonstrate
that PM outperforms a number of more complex state-of-the-art methods in inferring counterfac-
tual outcomes on several real-world and semi-synthetic datasets. Our experiments also show that
PM is more robust to a high level of treatment assignment bias than existing methods. We believe
that our simple and effective method for ITE estimation that extends to any number of treatment
options could potentially make counterfactual inference more accessible, particularly for medical
applications, where multiple available treatment options are the norm.
1The ITE is sometimes referred to as Conditional Average Treatment Effect (CATE).
1
Under review as a conference paper at ICLR 2019
Contributions. This work contains the following contributions:
•	We introduce Perfect Match, a simple methodology for learning neural representations for
counterfactual inference based on propensity score matching within minibatches.
•	We perform extensive experiments on semi-synthetic and real-world data in both the binary
and multiple treatment setting. The experiments show that PM outperforms a number of
more complex state-of-the-art methods in inferring counterfactual outcomes.
2	Related Work
There are five main categories of approaches to learning to estimate ITEs:
Matching-based Methods. Matching methods estimate the counterfactual outcome of a sample
X with respect to treatment t using the factual outcomes of its nearest neighbours having received t,
with respect to a metric space. k-Nearest-Neighbour (kNN) (Ho et al. (2007)) operates in the poten-
tially high-dimensional covariate space, and therefore might suffer from the curse of dimensionality.
Propensity Score Matching (PSM) (Rosenbaum & Rubin (1983)) matches on the scalar probability
p(t|X) of t given the covariates X. PSM may require under- or oversampling the original data.
Adjusted Regression Methods. Adjusted regression models apply regression models with both
treatment and covariates as input. The simplest case is Ordinary Least Squares, which can either be
used for building one model, with the treatment as an input feature, or multiple separate models, one
for each treatment (Kallus (2017)). More complex regression models, such as Treatment-Agnostic
Representation Networks (TARNET) (Shalit et al. (2017)) may be used to capture non-linear re-
lationships. Methods that make use of propensity scores, and a model of the outcomes, such as
Propensity Dropout (PD) (Alaa et al. (2017)), are referred to as doubly robust (Funk et al. (2011)).
Tree-based Methods. Tree-based methods train many weak learners to build expressive ensemble
models. Examples of tree-based methods are Bayesian Additive Regression Trees (BART) (Chip-
man et al. (2010); Chipman & McCulloch (2016)) and Causal Forests (CF) (Wager & Athey (2017)).
Representation-balancing Methods. Representation-balancing methods seek to learn a high-
level representation for which the covariate distributions are balanced across treatment groups. Bal-
ancing Neural Networks (Johansson et al. (2016)) attempt to find such representations by minimising
the discrepancy distance (Mansour et al. (2009)) between treatment groups. Counterfactual Regres-
sion Networks (CFRNET, Shalit et al. (2017)) use different metrics such as the Wasserstein distance.
Distribution-modeling Methods. Generative Adversarial Nets for inference of Individualised
Treatment Effects (GANITE) (Yoon et al. (2018)) address ITE estimation using counterfactual and
ITE generators. GANITE uses a complex architecture with many hyperparameters and submodels
that may be difficult to implement and optimise. Causal Effect Variational Autoencoders (CEVAEs)
(Louizos et al. (2017)) use a latent variable modeling approach that is robust to hidden confounding,
but does not address biased treatment assignment. Causal Multi-task Gaussian Processes (CMGP)
(Alaa & van der Schaar (2017)) apply a multi-task Gaussian Process to ITE estimation. The optimi-
sation of CMGPs involves a matrix inversion of O(n3) complexity that limits their scalability.
In contrast to existing methods, PM is a simple doubly robust method built on propensity score
matching that can be used to train expressive non-linear neural network models for ITE estimation
from observational data in settings with any number of treatments (Table 1). While the underlying
idea behind PM is simple and effective, it has, to the best of our knowledge, not yet been explored.
Table 1: Comparison of several representative state-of-the-art methods for counterfactual inference
showing whether they were designed for > 2 treatments and their complexity of implementation.
PM CF BART BNN CFRNET PD CEVAE CMGP GANrTE
> 2 Treatments	JJJXX	× XX ✓
Complexity	Low Low Low Low Medium Medium High High High
2
Under review as a conference paper at ICLR 2019
3 Methodology
Problem Setting. We consider a setting in which we are given N observed samples X , where
each sample consists of p covariates xi with i ∈ [0, p). For each sample, the potential outcomes
are represented as a vector Y with k entries yj where each entry corresponds to the outcome when
applying one treatment tj out of the set of k available treatments T = {t0, ..., tk-1} with j ∈ [0, k).
As training data, we receive observed samples X and their factual outcomes yj when applying one
treatment tj . The set of available treatments can contain two or more treatment options. We refer
to the special case of two available treatment options as the binary treatment setting. Given the
observational training data with factual outcomes, we wish to train a predictive model that is able
to accurately produce a predicted potential outcomes vector Y with k entries yj. In literature, this
setting is known as the Rubin-Neyman potential outcomes framework (Rubin (2005)).
Precision in Estimation of Heterogenous Effect (PEHE). The primary metric that we optimise
for when training models to estimate ITE is the PEHE (Hill (2011)). In the binary setting, the PEHE
measures the ability of a predictive model to estimate the difference in effect between two treatments
t0 and t1 for samples X . To compute the PEHE, we measure the mean squared error between the
true difference in effect y1 (n) - y0 (n), drawn from the noiseless underlying outcome distributions
μι and μo, and the predicted difference in effect yι(n) - yo (n) indexed by n over N samples:
1N
QEHE = N X (Eyj(n)〜μj(n)[yi(n) - y0(n)] - [y1(n) - y0
n=0
2
(1)
When the underlying noiseless distributions μj are not known, the true difference in effect yι(n)-
y0 (n) can be estimated using the noisy ground truth outcomes yi (Appendix A). As a secondary
metric, we consider the error ATE in estimating the average treatment effect (ATE) (Hill (2011)). The
ATE measures the average difference in effect across the whole population (Appendix B). The ATE
is not as important as PEHE for models optimised for ITE estimation, but can be a useful indicator
of how well an ITE estimator performs at comparing two treatments across the entire population.
We can neither calculate PEHE nor ATE without knowing the outcome generating process.
Multiple Treatments. Both PEHE and ATE can be trivially extended to multiple treatments by
considering the average PEHE and ATE between every possible pair of treatments. Note that we
lose the information about the precision in estimating ITE between specific pairs of treatments by
averaging over all k2 pairs. However, one can inspect the pair-wise PEHE to get the whole picture.
ʌ
mPEHE
1 k-1 i-1
(k) i=0 j=0 GPEHEaj
(2)
ʌ
mATE
k-1 i-1
亩=j=0 GATEaj
(3)
Perfect Match (PM). We consider fully differentiable neural network models optimised via mini-
batch stochastic gradient descent to predict potential outcomes Y given a sample X. To address the
treatment selection bias inherent in observational data, we propose the use ofa training methodology
based on propensity score matching. A propensity score is the conditional probability p(t|X) of a
given sample X receiving a specific treatment t (Rosenbaum & Rubin (1983); Ho et al. (2007)). The
propensity score is a balancing score, meaning that if the distribution of propensity scores across
treatment groups is the same, then the distribution of their covariates xi is also guaranteed to be the
same across treatment groups (Ho et al. (2007)). By matching samples with their nearest neighbours
by propensity score we can, in the optimal case, balance the covariates xi . This breaks the depen-
dence of treatment assignment on X if all relevant covariates are observed, and therefore effectively
removes any potential treatment assignment bias (Ho et al. (2007)). However, in practice, we do
not have access to the true treatment assignment probability p(t|X). We therefore have to estimate
the propensity score p(t|X) by training a predictive model to predict the likelihood of receiving a
treatment t for a given sample X . Note that propensity scores are not the only balancing score (Ho
et al. (2007)). For example, X itself is another balancing score. However, matching on the one-
dimensional propensity score is preferable because it avoids the curse of dimensionality that would
be associated with matching on the potentially high-dimensional X directly. In PM, we match every
sample within a minibatch with its nearest neighbours by propensity score from all other treatments
(Algorithm 1, more details in Appendix C), taken from the training set. Every minibatch the model
3
Under review as a conference paper at ICLR 2019
is trained on therefore contains the same number of samples for each treatment group, and the
covariates xi of each treatment group are approximately balanced on average. The intuition behind
matching on the minibatch level, rather than the dataset level (Ho et al. (2011)), is that it reduces
the variance during training which in turn leads to better expected performance for counterfactual
inference (Appendix D). In this sense, PM can be seen as a minibatch sampling strategy (Csiba &
Richtarik (2018)) specifically designed to improve learning for CoUnterfactUal inference.
Algorithm 1 Batch Augmentation using Perfect Match (PM). After augmentation, each batch con-
tains an eqUal nUmber of samples from each treatment groUp and the covariates xi across all treat-
ment groUps are approximately balanced.
Input: Batch of B random samples Xbatch with assigned treatments t, training set Xtrain of N sam-
ples, nUmber of treatment options k, propensity score estimator EPS to calcUlate the probability
p(t|X ) of treatment assigned given a sample X
Output: Batch XoUt consisting of B × k matched samples
1:	procedure PERFECT_Match：
2:	Xout — Empty
3:	for sample X with treatment t in Xbatch do
4:	p(t∣X) J EPS(X)
5:	for i = 0 to k - 1 do
6:	if i 6= t then
7:	psi J p(t|X)i
8:	Xmatched J get closest match to propensity score psi with treatment i from Xtrain
9:	Add sample Xmatched to XoUt
10:	Add X to XoUt
Model Selection. Besides accoUnting for the treatment assignment bias, the other major issUe in
learning for coUnterfactUal inference from observational data is that, given mUltiple models, it is not
trivial to decide which one to select. The root problem is that we do not have direct access to the
trUe error in estimating coUnterfactUal oUtcomes, only the error in estimating the observed factUal
oUtcomes. This makes it difficUlt to perform parameter and hyperparameter optimisation, as we do
not know which models are better than others for coUnterfactUal inference on a given dataset. To
rectify this problem, We use a nearest neighbour approximation ^nn-pehe of the ^pehe metric for
the binary (Shalit et al. (2017)) and multiple treatment settings for model selection. The ^nn-pehe
estimates the treatment effect of a given sample by substituting the true counterfactual outcome with
the outcome yj from a respective nearest neighbour NN matched on X using the Euclidean distance.
1N	2
^nn-pehe = N X ([yι(NN(n)) - yo(NN(n))] - [yι(n) - yo(n)f)	(4)
n=0
Analogously to Equations (2) and (3), the ^nn-pehe metric can be extended to the multiple treatment
setting by considering the mean ^nn-pehe between all (2) possible pairs of treatments (Appendix E).
Model Architecture. No different from other applications, the chosen architecture plays a key role
in the performance of neural networks when attempting to learn representations for counterfactual
inference. Shalit et al. (2017) claimed that the naive approach of appending the treatment index
tj may perform poorly if X is high-dimensional, because the influence of tj on the hidden layers
may be lost during training. Shalit et al. (2017) subsequently introduced the TARNET architecture
to rectify this issue. Since the original TARNET was limited to the binary treatment setting, we
extended the TARNET architecture to the multiple treatment setting (Figure 1). We did so by using
k head networks, one for each treatment option over a set of shared base layers, each with L layers.
In TARNET, the jth head network is only trained on samples from treatment tj . The shared layers
are trained on all samples. By using a head network for each treatment, we ensure tj maintains an
appropriate degree of influence on the network output at all points during training.
Figure 1: The TARNET architecture with k heads for the multiple treatment setting.
4
k heads
Under review as a conference paper at ICLR 2019
Table 2: Comparisons of the datasets used in our experiments. We evaluate on three semi-synthetic
datasets with varying numbers of treatments and samples, and a real-world clinical trial dataset.
Dataset	TyPe	# Samples	# Features	# Treatments	Counterfactuals
IHDP	semi-synthetic	747	25	2	available
Jobs	real-world	3212	7	2	not available
News	semi-synthetic	5000	2870	2/4/8/16	available
TCGA	semi-synthetic	9659	20531	4	available
4 Experiments
Our experiments aimed to answer the following questions:
1	What is the comparative performance of PM in inferring counterfactual outcomes in the
binary and multiple treatment setting compared to existing state-of-the-art methods?
2	Does model selection by NN-PEHE outperform selection by factual MSE?
3	How does the relative number of matched samples within a minibatch affect performance?
4	How well does PM cope with an increasing treatment assignment bias in the observed data?
5	How does the presence of hidden confounders influence the performance of PM?
6	How do the learning dynamics of minibatch matching compare to dataset-level matching?
4.1	DATASETS
We performed experiments on four real-world and semi-synthetic datasets (Table 2) with binary and
multiple treatment options in order to gain a better understanding of the empirical properties of PM.
Infant Health and Development Program (IHDP). The IHDP dataset (Hill (2011)) contains
data from a randomised study on the impact of specialist visits on the cognitive development of
children, and consists of 747 children with 25 covariates describing properties of the children and
their mothers. Children that did not receive specialist visits were part of a control group. The
outcomes were simulated using the NPCI package from Dorie (2016). The IHDP dataset is biased
because the treatment groups had a biased subset of the treated population removed (Shalit et al.
(2017)). We used the same simulated outcomes2 as Shalit et al. (2017).
Jobs. The Jobs dataset (LaLonde (1986)) is a blend of data from randomised and observational
studies on the effect of professional training programs on unemployment, and is a commonly used
benchmark dataset for counterfactual inference in the binary setting. Each sample consisted of de-
mographic covariates, such as age, gender and previous income, and the treatment was enrolment
in a job training program. All samples that were not enrolled in a training program were considered
control cases. The task was to predict the potential unemployment given the covariates and treat-
ment. We used the same feature and sample set as described in Dehejia & Wahba (2002); Smith &
Todd (2005); Shalit et al. (2017). A subgroup of the data was randomised and we therefore were
able to estimate the ground truth average treatment effect on the treated (ATT) (Shalit et al. (2017)).
Treatment assignment in Jobs is biased because parts of the data come from an observational study.
2Available at: http://www.mit.edu/~fredrikj/files/IHDP-10 00.tar.gz
Figure 3: Change in error (y-axes) in terms
of precision in estimation of heterogenous ef-
fect (PEHE) and average treatment effect (ATE)
when increasing the percentage of matches in
each minibatch (x-axis). Symbols COrreSPOnd to
the mean value of ^ate (red) and √^pehe (blue)
on the test set of News-8 across 50 repeated runs
with new outcomes (lower is better). Perfor-
mance improves with more matches added.
Figure 2: Correlation analysis of the real PEHE
(y-axis) with the mean squared error (MSE; left)
and the nearest neighbour approximation of the
precision in estimation of heterogenous effect
(NN-PEHE; right) across over 20,000 model
evaluations on the validation set of IHDP. Scat-
terplots show a subsample of 1’400 data points. P
indicates the Pearson correlation. NN-PEHE cor-
relates significantly better with PEHE than MSE.
5
Under review as a conference paper at ICLR 2019
Figure 4: Comparison of several state-of-the-art methods for counterfactual inference on the test
set of the News-8 dataset when varying the treatment assignment imbalance κ (x-axis), i.e. how
much the treatment assignment is biased towards more effective treatments. Symbols correspond to
the mean value of ^ate (left) and √^PEHE (right) across 50 repeated runs with new outcomes (lower
is better). The shaded area indicates the standard deviation. PM handles an increasing treatment
assignment bias K better than existing state-of-the-art methods in terms of both ^ate and √∈Pehe.
News. The News dataset was first proposed as a benchmark for counterfactual inference by Johans-
son et al. (2016) and consists of 5000 randomly sampled news articles from the NY Times corpus3.
The News dataset contains data on the opinion of media consumers on news items. The samples X
represent news items consisting of word counts xi ∈ N, the outcome yj ∈ R is the reader’s opinion
of the news item, and the k available treatment options represent various devices that could be used
for viewing, e.g. smartphone, tablet, desktop, television or others (Johansson et al. (2016)). We
extended the original dataset specification in (Johansson et al. (2016)) to enable the simulation of
arbitrary numbers of viewing devices. To model that consumers prefer to read certain media items
on specific viewing devices, we train a topic model on the whole NY Times corpus and define z(X)
as the topic distribution of news item X . We then randomly pick k + 1 centroids in topic space,
with k centroids zj per viewing device and one control centroid zc . We assigned a random Gaussian
outcome distribution with mean μ,〜 N(0.45,0.15) and standard deviation σ7-〜 N(0.1,0.05) to
each centroid. For each sample, we drew ideal potential outcomes from that Gaussian outcome dis-
tribution yj 〜N(μj,σ7∙) + e with e 〜N(0,0.15). We then defined the unscaled potential outcomes
y∙ = yj * [D(z(X), Zj) + D(Z(X), zj] as the ideal potential outcomes yj weighted by the sum of
distances to centroids zj and the control centroid zc using the Euclidean distance as distance D. We
assigned the observed treatment t using t|x 〜 Bern(SOftmax(Kyj)) with a treatment assignment
bias coefficient κ, and the true potential outcome y§ = Cyj as the unscaled potential outcomes yj
scaled by a coefficient C = 50. We used four different variants of this dataset with k = 2, 4, 8, and
16 viewing devices, and K = 10, 10, 10, and 7, respectively. Higher values of K indicate a higher
expected treatment assignment bias depending on yj. K = 0 indicates no assignment bias.
The Cancer Genomic Atlas (TCGA). The TCGA project collected gene expression data from
various types of cancers in 9659 individuals (Weinstein et al. (2013)). There were four available
clinical treatment options: (1) medication, (2) chemotherapy, (3) surgery, or (4) both surgery and
chemotherapy. We used a synthetic outcome function that simulated the risk of cancer recurrence
after receiving either of the treatment options based on the real-world gene expression data. Before
further processing, we standardised the gene expression data using the mean and standard deviations
of gene expression at each gene locus for normal tissue in the training set. To produce the potential
outcomes Y , we first selected k + 1 gene expression phenotypes as representative phenotypes: One
phenotype Xj for each treatment option and one as a control phenotype Xc . Analogously to the
News dataset, we assigned a random Gaussian outcome distribution with mean μj 〜 N(0.45,0.15)
and standard deviation σt 〜N(0.1,0.05) to each phenotype. For each sample, we drew ideal po-
tential outcomes from that Gaussian outcome distribution yj 〜N(μj, σj) + e with e 〜N(0,0.15).
We then defined the unscaled potential outcomes yj = yj * [D(X, Xtj ) + D(X, Xc)] as the ideal
potential outcomes yj weighted by the sum of distances to phenotype Xtj and the control pheno-
type Xc using the cosine similarity as distance metric D. We assigned the observed treatment t
using t|x 〜Bern(SOftmaX(Kyj)) with a treatment assignment bias coefficient K = 10, and the true
potential outcome yj = Cyj as the unscaled potential outcomes yj scaled by a coefficient C = 50.
All datasets with the exception of IHDP were split into a training (63%), validation (27%) and test
set (10% of samples). For IHDP we used exactly the same splits as previously used by Shalit et al.
(2017). We repeated experiments on IHDP, Jobs, News and TCGA 1000, 10, 50, and 5 times,
respectively. We reassigned outcomes and treatments with a new random seed for each repetition.
3https://archive.ics.uci.edu/ml/datasets/bag+of+words
6
Under review as a conference paper at ICLR 2019
Table 3: Comparison of methods for counterfactual inference with two available treatment options on IHDP, Jobs and News-2. We report the mean value ± the standard deviation of √∈pehe, ∈ate, Rpol and ATT on the test sets over a number of repeated runs. Where available we list the numbers reported by the original authors. n.r. = not reported. * = significantly different from PM (α < 0.05)						
Method	IHDP (1000 repeats)		Jobs (10 repeats)		News-2 (50 repeats)	
	76pehe	ATE	RPol (π)	ATT	，^PEHE	^ATE
PM	0.84 ± 0.61	0.24 ± 0.20	0.18 ± 0.10	0.16 ± 0.07	16.76 ± 1.26	3.99 ± 1.01
+ on X	0.81 ± 0.57	0.24 ± 0.20	0.23 ± 0.11	0.19 ± 0.09	17.06 ± 1.22	4.14 ± 1.27
+ MLP	0.83 ± 0.57	0.23 ± 0.20	0.19 ± 0.12	0.16 ± 0.07	t 18.38 ± 1.46	t 5.90 ± 2.07
kNN	t 4.1 ± 0.2	t 0.79 ± 0.05	t 0.26 ± 0.0	0.13 ± 0.05	t 18.14 ± 1.64	t 7.83 ± 2.55
PSMPM	n.r.	n.r.	0.18 ± 0.12	0.18 ± 0.09	t 17.49 ± 1.49	t 5.02 ± 2.34
PSMMI	t 2.70 ± 3.85	t 0.49 ± 0.81	0.20 ± 0.09	t 0.42 ± 0.22	t 17.40 ± 1.30	t 4.89 ± 2.39
RF	t 6.6 ± 0.3	t 0.96 ± 0.06	t 0.28 ± 0.0	t 0.09 ± 0.04	t 17.39 ± 1.24	t 5.50 ± 1.20
CF	t 3.8 ± 0.2	t 0.40 ± 0.03	0.20 ± 0.0	t 0.07 ± 0.03	t 18.36 ± 1.73	t 8.48 ± 2.46
BART	t 2.3 ± 0.1	t 0.34 ± 0.02	t 0.25 ± 0.0	t 0.08 ± 0.03	t 18.53 ± 2.02	t 5.40 ± 1.53
GANITE	t 2.4 ± 0.4	t 0.49 ± 0.05	0.14 ± 0.01	t 0.06 ± 0.03	t 18.28 ± 1.66	t 4.65 ± 2.12
BNN	t 2.1 ± 0.1	t 0.42 ± 0.03	0.24 ± 0.0	t 0.09 ± 0.04	n.r.	n.r.
PD	n.r.	n.r.	n.r.	n.r.	t 17.52 ± 1.62	4.69 ± 3.17
TARNET	t 0.95 ± 0.02	t 0.28 ± 0.01	0.21 ± 0.0	0.11 ± 0.04	17.17 ± 1.25	t 4.58 ± 1.29
CFRNETWass	t 0.76 ± 0.02	t 0.27 ± 0.01	0.21 ± 0.0	t 0.09 ± 0.03	16.93 ± 1.12	t 4.54 ± 1.48
CEVAE	t 2.7 ± 0.1	t 0.46 ± 0.02	t 0.26 ± 0.0	t 0.03 ± 0.01	n.r.	n.r.
CMGP	t 0.77 ± 0.11	t 0.13 ± 0.12	0.24 ± 0.05	t 0.09 ± 0.07	n.r.	n.r.
4.2	Experimental Setup
Models. We evaluated PM, ablations, baselines, and all relevant state-of-the-art methods: kNN
(Ho et al. (2007)), BART (Chipman et al. (2010); Chipman & McCulloch (2016)), Random Forests
(RF) (Breiman (2001)), CF (Wager & Athey (2017)), GANITE (Yoon et al. (2018)), Balancing
Neural Network (BNN) (Johansson et al. (2016)), TARNET (Shalit et al. (2017)), Counterfactual
Regression Network using the Wasserstein regulariser (CFRNETWass) (Shalit et al. (2017)), CEVAE
(Louizos et al. (2017)), CMGP (Alaa & van der Schaar (2017)), and PD (Alaa et al. (2017)). We
trained a Support Vector Machine (SVM) with probability estimation (Pedregosa et al. (2011)) to
estimate p(t|X) for PM on the training set. We also evaluated preprocessing the entire training set
with PSM using the same matching routine as PM (PSMPM) and the ”MatchIt” package (PSMMI, Ho
et al. (2011)) before training a TARNET (Appendix F). In addition, we trained an ablation of PM
where we matched on the covariates X (+ on X) directly, if X was low-dimensional (p < 200), and
on a 50-dimensional representation of X obtained via principal components analysis (PCA), if X
was high-dimensional, instead of on the propensity score. We also evaluated PM with a multi-layer
perceptron (+ MLP) that received the treatment index tj as an input instead of using a TARNET.
Figure 5: Comparison of several state-of-the-art methods for counterfactual inference on the test
set of the TCGA dataset when varying the percentage of hidden confounding (x-axis), i.e. what
percentage of the features that were relevant for treatment assignment are visible to the model.
Symbols correspond to the mean value of ^ate (left) and √^pehe (right) across 5 repeated runs with
new outcomes (lower is better). The shaded area indicates the standard deviation. PM, TARNET
and CFRNET are relatively stable up to 60% of hidden confounding. The methods degrade strongly
in counterfactual prediction performance at levels of hidden confounding higher than 60%.
7
Under review as a conference paper at ICLR 2019
Table 4: Comparison of methods for counterfactual inference with more than two available treatment
options on the News-4, News-8 and News-16. Numbers are mean value ± the standard deviation of
√^mPEHE and ^mATE over 50 repeated runs. * = significantly different from PM (α < 0.05)
Method	News-4 (κ = 10)		News-8 (κ = 10)		News-16 (κ = 7)	
	V ^mPEHE	ʌ mATE	V ^mPEHE	ʌ mATE	V ^mPEHE	ʌ mATE
PM	21.58 ± 2.58	10.04 ± 2.71	20.76 ± 1.86	6.51 ± 1.66	20.24 ± 1.46	5.76 ± 1.33
+ on X	21.41 ± 1.75 *	8.91 ± 2.00	20.90 ± 2.07	6.36 ± 1.65	20.67 ± 1.42	5.73 ± 1.16
+ MLP	t 25.05 ± 2.80 *	14.92 ± 3.14	* 24.88 ± 1.98 *	14.00 ± 2.45	* 27.05 ± 2.47	* 16.90 ± 2.01
kNN	* 27.92 ± 2.44 *	19.40 ± 3.12	* 26.20 ± 2.18 *	15.11 ± 2.34	* 27.64 ± 2.40	* 17.27 ± 2.10
PSMPM	* 22.74 ± 2.58 *	11.62 ± 2.69	* 22.16 ± 1.79 *	8.81 ± 1.96	* 23.57 ± 2.48	* 11.04 ± 2.50
PSMMI	* 37.26 ± 2.28 *	30.19 ± 2.47	* 30.50 ± 1.70 *	22.09 ± 1.98	* 28.17 ± 2.02	* 18.81 ± 1.74
RF	* 26.59 ± 3.02 *	18.03 ± 3.18	* 23.77 ± 2.14 *	12.40 ± 2.29	* 26.13 ± 2.48	* 15.91 ± 2.00
CF	* 27.28 ± 2.59 *	19.04 ± 3.15	* 25.76 ± 2.05 *	15.12 ± 2.20	* 26.85 ± 2.29	* 17.03 ± 1.99
BART	* 26.41 ± 3.10 *	17.14 ± 3.51	* 25.78 ± 2.66 *	14.80 ± 2.56	* 27.45 ± 2.84	* 17.50 ± 2.49
GANITE	* 24.50 ± 2.27 *	13.84 ± 2.69	* 23.58 ± 2.48 *	11.20 ± 2.84	* 25.12 ± 3.53	* 13.20 ± 3.28
PD	20.88 ± 3.24 *	8.47 ± 4.51	21.19 ± 2.29	7.29 ± 2.97	* 22.28 ± 2.25	* 10.65 ± 2.22
TARNET	* 23.40 ± 2.20 *	13.63 ± 2.18	* 22.39 ± 2.32 *	9.38 ± 1.92	* 21.19 ± 2.01	* 8.30 ± 1.66
CFRNETWass	* 22.65 ± 1.97 *	12.96 ± 1.69	* 21.64 ± 1.82 *	8.79 ± 1.68	* 20.87 ± 1.46	* 8.05 ± 1.40
Architectures. To ensure that differences between methods of learning counterfactual representa-
tions for neural networks are not due to differences in architecture, we based the neural architectures
for TARNET, CFRNETWass, PD and PM on the same, previously described extension of the TAR-
NET architecture (Shalit et al. (2017), Details in Appendix G) to the multiple treatment setting.
Hyperparameters. For the IHDP, Jobs, News and TCGA dataset we respectively used 30, 30, 10,
and 5 optimisation runs for each method using randomly selected hyperparameters from predefined
ranges (Appendix H). We selected the best model across the runs based on validation set ^nn-pehe.
Metrics. We calculated the PEHE and ATE for datasets for which we know the outcome generating
process. For the Jobs dataset, we did not have access to counterfactual outcomes. We therefore
calculated the ATT = ∣^∩e∣ pN=o y1(n) - t∩e P^n=o yo(n) where E was the randomised
subset of the dataset, and reported the error in estimating ATT EATT = |ATT - 历\石| PnN=O y1(n)∣,
and the policy risk RPol(π) (Shalit et al. (2017); Yoon et al. (2018)). The policy risk is the average
loss in value when treating according to the policy π implied by an ITE estimator (Appendix I).
5	Results and Discussion
Counterfactual Inference. We evaluated the counterfactual inference performance of the listed
models in the binary setting (Table 3) and multiple treatment setting (Table 4). On IHDP, PM
reached the third best performance in terms of √PEHE after CFRNET and CMGP, and the second
best EATE after CMGP. On Jobs, PM reached the second best RPol (π) after GANITE. On the binary
News-2, PM outperformed all other methods in terms of √PEHE and ∈ate. On the News-4/8/16
datasets with more than two treatments, PM consistently outperformed all other methods - in some
cases by a large margin - on both metrics with the exception of the News-4 dataset, where PM came
second to PD. The strong performance of PM across a wide range of datasets with varying amounts
of treatment options is remarkable considering how simple it is compared to other, highly specialised
methods. Notably, PM consistently outperformed both CFRNET, with the exception of the IHDP
dataset, which accounted for covariate imbalances between treatments via regularisation rather than
matching, and PSMMI, which accounted for covariate imbalances by preprocessing the entire train-
ing set with a matching algorithm (Ho et al. (2011)). We also found that matching on the propensity
score was, in almost all cases, not significantly different from matching on X directly when X was
low-dimensional, or a low-dimensional representation of X when X was high-dimensional (+ on
X). This indicates that PM is effective with any low-dimensional balancing score. In addition, using
PM with the TARNET architecture outperformed the MLP (+ MLP) in almost all cases, with the ex-
ception of the low-dimensional IHDP and Jobs. We conclude that matching on the propensity score
or a low-dimensional representation of X and using the TARNET architecture are sensible default
configurations, particularly when X is high-dimensional.
8
Under review as a conference paper at ICLR 2019
Figure 6: Comparison of the learning dynamics during training (normalised training epochs; from
start = 0to end = 100 of training, x-axis) of several matching-based methods on the validation set of
News-8. The coloured lines correspond to the mean value of the factual error (√MSE; red) and the
CoUnterfactUaI error (√^pehe; blue) across 10 randomised hyperparameter configurations (lower is
better). The shaded area indicates the standard deviation. All methods used exactly the same model
and hyperparameters and only differed in how they addressed treatment assignment imbalance. The
leftmost figure shows the desired behavior of the counterfactual and factual error jointly decreasing
until convergence. PM exhibits the desired behavior of aligning the optimisation of the factual and
counterfactual error. PSMPM shows a weaker degree of alignment and much higher variance. In
contrast, PSMMI shows the undesired behavior of the counterfactual error increasing as the factual
error decreases, indicating that the models were overfitting to the properties of the treated group.
Model Selection. To judge whether NN-PEHE is more suitable for model selection for counter-
factual inference than MSE, we compared their respective correlations with the PEHE on IHDP. We
found that NN-PEHE correlates significantly better with the PEHE than MSE (Figure 2).
Number of Matches per Minibatch. To determine the impact of matching fewer than 100% of
all samples in a batch, we evaluated PM on News-8 trained with varying percentages of matched
samples on the range 0 to 100% in steps of 10% (Figure 3). We found that including more matches
indeed consistently reduces the counterfactual error up to 100% of samples matched. Interestingly,
we found a large improvement over using no matched samples even for relatively small percentages
(<40%) of matched samples per batch. This shows that propensity score matching within a batch is
indeed highly effective at improving the training of neural networks for counterfactual inference.
Treatment Assignment Bias. To assess how the predictive performance of the different methods
is influenced by increasing amounts of treatment assignment bias, we evaluated their performances
on News-8 while varying the assignment bias coefficient κ on the range of 5 to 20 (Figure 4). We
found that PM handles high amounts of assignment bias better than existing state-of-the-art methods.
Hidden Confounding. To analyse the sensitivity of the different methods to hidden confounding,
we successively retrained the models on a reduced set of covariates xi in increasing steps of 10%
(= 2053 gene loci) in the range of 10% to 90% (Figure 5). We removed the covariates from the
model but kept them for computing assignments and outcomes, i.e. they were hidden confounders.
The results showed that PM was competitive with TARNET and CFRNET in terms of robustness
to hidden confounding. Notably, PM, TARNET and CFRNET were relatively stable up to levels of
60% of hidden confounding, likely because some gene loci are highly correlated.
Comparing Minibatch and Dataset Matching. As outlined previously, it is possible to optimise
the counterfactual error using the observed factual samples if (1) we manage to break the dependence
of treatment assignment on X, and (2) we observe all relevant variables (Ho et al. (2007)). If we were
successful in balancing the covariates, we would expect that the counterfactual error is implicitly
and consistently improved alongside the factual error, despite optimising the neural network for
the observed factual loss only. To elucidate to what degree this is the case when using the various
evaluated matching-based methods, we evaluated the respective training dynamics of PM, PSMPM
and PSMMI (Figure 6). We found that PM better conforms to the desired behavior than PSMPM and
PSMMI. PSMPM, which used the same matching strategy as PM but on the dataset level, showed a
much higher variance than PM. PSMMI was overfitting to the treated group.
Limitations. A limitation of this work is that the theory of matching to balance the covariates only
holds in the strongly ignorable setting, i.e. under the assumption that there are no unobserved con-
founders. However, our experiments (Figure 5) and related works showed that even the presence of
large amounts of hidden confounders may not necessarily decrease the performance of ITE estima-
tors in practice if we observe proxy variables (Montgomery et al. (2000); Louizos et al. (2017)). In
future work, we would like to integrate PM with diagnostic tools that address hidden confounding
(Kallus & Zhou (2018)), and the ability to learn from multiple related tasks (Schwab et al. (2018)).
9
Under review as a conference paper at ICLR 2019
6	Conclusion
We presented PM, a new and simple method for training neural networks for estimating ITEs from
observational data that extends to any number of available treatment options. In addition, we ex-
tended the PEHE metric to settings with more than two treatments, and introduced a nearest neigh-
bour approximation of PEHE and mPEHE that can be used for model selection without having
access to counterfactual outcomes. We performed experiments on several real-world and semi-
synthetic datasets that showed that PM outperforms a number of more complex state-of-the-art
methods in inferring counterfactual outcomes. We also found that the NN-PEHE correlates sig-
nificantly better with real PEHE than MSE, that including more matched samples in each minibatch
improves the learning of counterfactual representations, and that PM handles an increasing treatment
assignment bias better than existing state-of-the-art methods. PM may be used for settings with any
amount of treatment options, is compatible with any existing neural network architecture, simple to
implement, and does not introduce any additional hyperparameters or computational complexity.
References
Ahmed M Alaa and Mihaela van der Schaar. Bayesian inference of individualized treatment effects
using multi-task gaussian processes. In Advances in Neural Information Processing Systems, pp.
3424-3432, 2017.
Ahmed M Alaa, Michael Weisz, and Mihaela van der Schaar. Deep counterfactual networks with
propensity-dropout. arXiv preprint arXiv:1706.05966, 2017.
Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. arXiv preprint
arXiv:1610.01271, 2016.
Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.
Hugh Chipman and Robert McCulloch. BayesTree: Bayesian additive regression trees. R package
version 0.3-1.4, 2016.
Hugh A Chipman, Edward I George, Robert E McCulloch, et al. BART: Bayesian additive regression
trees. The Annals of Applied Statistics, 4(1):266-298, 2010.
Dominik Csiba and Peter Richtarik. Importance sampling for minibatches. Journal of Machine
Learning Research, 19(27), 2018.
Rajeev H Dehejia and Sadek Wahba. Propensity score-matching methods for nonexperimental
causal studies. Review of Economics and statistics, 84(1):151-161, 2002.
Vincent Dorie. NPCI: Non-parametrics for causal inference, 2016. URL https://github.
com/vdorie/npci.
Michele Jonsson Funk, Daniel Westreich, Chris Wiesen, Til Strmer, M. Alan Brookhart, and Marie
Davidian. Doubly robust estimation of causal effects. American Journal of Epidemiology, 173
(7):761-767, 2011.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1):217-240, 2011.
Daniel E Ho, Kosuke Imai, Gary King, and Elizabeth A Stuart. Matching as nonparametric prepro-
cessing for reducing model dependence in parametric causal inference. Political analysis, 15(3):
199-236, 2007.
Daniel E Ho, Kosuke Imai, Gary King, Elizabeth A Stuart, et al. MatchIt: nonparametric prepro-
cessing for parametric causal inference. Journal of Statistical Software, 42(8):1-28, 2011.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual infer-
ence. In International Conference on Machine Learning, pp. 3020-3029, 2016.
Nathan Kallus. Recursive partitioning for personalization using observational data. In International
Conference on Machine Learning, 2017.
10
Under review as a conference paper at ICLR 2019
Nathan Kallus and Angela Zhou. Confounding-robust policy improvement. Advances in Neural
Information Processing Systems, 2018.
Adam Kapelner and Justin Bleich. bartMachine: Machine learning with Bayesian additive regres-
sion trees. arXiv preprint arXiv:1312.2171, 2013.
Robert J LaLonde. Evaluating the econometric evaluations of training programs with experimental
data. The American economic review,pp. 604-620, 1986.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling.
Causal effect inference with deep latent-variable models. In Advances in Neural Information
Processing Systems, pp. 6446-6456, 2017.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Mark R Montgomery, Michele Gragnolati, Kathleen A Burke, and Edmundo Paredes. Measuring
living standards with proxy variables. Demography, 37(2):155-174, 2000.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning
Research, 12:2825-2830, 2011.
Paul R. Rosenbaum and Donald B. Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41-55, 1983.
Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal
of the American Statistical Association, 100(469):322-331, 2005.
Patrick Schwab, Emanuela Keller, Carl Muroi, David J Mack, Christian Strassle, and Walter Karlen.
Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care. In International
Conference on Machine Learning, 2018.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: Gener-
alization bounds and algorithms. In International Conference on Machine Learning, 2017.
Jeffrey A Smith and Petra E Todd. Does matching overcome LaLonde’s critique of nonexperimental
estimators? Journal of Econometrics, 125(1-2):305-353, 2005.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 2017.
John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Mills Shaw, Brad A Ozenberger,
Kyle Ellrott, Ilya Shmulevich, Chris Sander, Joshua M Stuart, Cancer Genome Atlas Research
Network, et al. The cancer genome atlas pan-cancer analysis project. Nature genetics, 45(10):
1113-1120, 2013.
Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GANITE: Estimation of Individualized
Treatment Effects using Generative Adversarial Nets. In International Conference on Learning
Representations, 2018.
11
Under review as a conference paper at ICLR 2019
Supplementary Material for:
”Perfect Match: A Simple Method for Learn-
ing Representations For Counterfactual In-
ference With Neural Networks”
Anonymous authors
Paper under double-blind review
A Precision in Estimation of Heterogenous Effect (PEHE)
When the underlying noiseless distributions μj are not given, PEHE is calculated using:
1N	2
GPEHE = N E ([yi(n) - yo(n)] - [y1(n) - y0(n)])	(A.1)
n=0
B	Average Treatment Effect (ATE)
The ATE measures the average difference in effect across the whole population (Hill (2011)). It can
be useful indicator of how well an ITE estimator performs at comparing two treatments across the
entire population.
1N	1N
EATE =11N £ Ey(n)〜μ(n)[y(n)] - N E y(n)||2	(B∙1)
Similar to Equation (A.1), the counterfactual treatment effect can be estimated using the noisy
ground truth outcomes if the underlying noiseless distributions μj are not given:
1N	1N
aATE = || N∑Sy(n)- NEy(n)"2	(BZ
n=0	n=0
C Details Perfect Match (PM)
Algorithm 1 outlines the procedure of using PM to augment a batch with propensity score matches
for training a neural network with stochastic gradient descent. We trained a Support Vector Machine
(SVM) with probability estimation as propensity score estimator EPS. To speed up recalling nearest
neighbours, we additionally prepared an index per treatment into Xtrain sorted by propensity score.
At augmentation time, we used binary search on this index to find the nearest neighbours by propen-
sity score in O(log Nt) where Nt is the number of samples X in Xtrain assigned to the treatment
group t. To avoid overfitting to specific edge samples when propensity scores are not distributed
evenly in the training set, we chose at random from one of the k=6 closest samples by propensity
score. See Table S1 for a comparison of the effects on predictive performance of choosing varying
values of k . For the ”+ on X” model we matched, using the Euclidean distance, directly on the
covariates X on IHDP and Jobs, and using a low dimensional representation of X , obtained using
principal component analysis (PCA) with 50 principal components, on News.
11
Under review as a conference paper at ICLR 2019
Table S1: Comparison of PM with varying numbers of nearest neighbours k considered for ran-
domised matching on the News-2/4/8/16 datasets (k=1 corresponds to no randomisation; k=6, in
cursive, are the results reported in the main body). We report the mean value ± the standard devia-
tion of √^pehe on the test sets over 50 runs. There was no significant differences between different
levels of k and k=6 (significance level α < 0.05). In particular, there was no significant difference
between using randomisation and not using randomisation.
Method	News-2 λ∕^PEHE	News-4 ∖! ^pehe	News-8 ∖! ^pehe	News-16 λ∕^Pehe
PM (k=1)	16.79 ± 1.16	21.86 ± 2.24	20.70 ± 1.87	20.06 ± 1.43
PM (k=3)	16.82 ± 1.17	21.49 ± 2.43	20.50 ± 1.48	20.28 ± 1.52
PM (k=6)	16.76 ± 1.26	21.58 ± 2.58	20.76 ± 1.86	20.24 ± 1.46
PM (k=9)	16.91 ± 1.18	21.56 ± 2.58	20.75 ± 1.53	20.47 ± 1.94
PM (k=12)	17.11 ± 0.94	21.16 ± 2.20	21.10 ± 2.14	20.28 ± 1.64
PM (k=15)	16.87 ± 1.14	20.94 ± 2.05	20.92 ± 1.80	20.48 ± 1.80
D Reduced Variance is Linked to Improved Performance in
Estimating Treatment Effects
In the standard supervised setting, the variance added by minibatch SGD leads to slower convergence
(Csiba & Richtrik (2018)), but will not typically lead to models that perform worse if they are trained
until a convergence criterion has been met, e.g. early stopping. Itis therefore perhaps surprising that
we observe a difference not just in convergence (Figure 6), but also in counterfactual estimation
performance (Tables 3 and 4) when comparing models trained with and without batch matching.
A potential cause of the observed difference in treatment effect estimation performance is that we
do not have access to the true counterfactual error in observational data to select the best model
encountered during training. We therefore have to resort to using the factual error (e.g. factual
MSE) or an estimator based on the factual error (e.g. NN-PEHE) to select the best encountered
model. However, when the variance during training is high, chances are that we select a model
based on the factual error that at this point during the training happens to be suboptimal in terms
of counterfactual error. Through this mechanism, added variance during training for counterfactual
inference is directly linked to worse expected performance in treatment effect estimation (Figure
S1). We do not observe the same behavior in standard supervised learning tasks because we have
direct access to the underlying objective, and are therefore able to select the best encountered model
regardless of the variance during optimisation, i.e. models that during training do not perform well
momentarily will not be selected because their measured error is high. We confirmed experimentally
that gradient steps are more likely to be in opposing directions when not using PM (Figure S2).
Figure S1: Illustration of the impact of added variance introduced by minibatch SGD during training.
In the supervised setting (left), high variance does in general not impact the final performance in
terms of expected factual error (red), because we can select (dotted black line) the best encountered
model during training using the observed error. In the counterfactual setting (right), we are not able
to compute the true counterfactual error (blue) in observational data, and must therefore resort to
selecting based on the factual error or a measure derived from the factual error. Even though we
encounter models that perform similarly well, we are less likely to select these models under high
variance, because we do not have direct access to the counterfactual error and the minimum of the
factual error in general does not correspond to the minimum of the counterfactual error, particularly
when optimisation of counterfactual and factual error are not well aligned. The dashed blue line
indicates the counterfactual error of the model selected based on factual error.
12
Under review as a conference paper at ICLR 2019
Γr
Desired
rewol
」3usll
ESMΔ
ΔεPEHE higher
PM
lower	higher
ΔεPEHE
rewol
」3usll
ESMΔ
lower	higher
ΔεPEHE
rewol
Figure S2: Comparison of the frequencies of individual gradient steps during training of several
matching-based methods on the validation set of News-8. The heatmap entries show the frequencies
of pairs of differences in factual error (∆MSE; y-axis) and CoUnterfactual error (Δ∈pehe; x-axis)
after individual gradient steps. All methods used the same model and only differed in how they
addressed treatment assignment bias. The leftmost heatmap shows the desired behavior of gradient
steps changing the counterfactual and factual error jointly during training. PM most closely resem-
bles the desired behavior of jointly optimising counterfactual and factual error. In contrast, PSMPM
and PSMMI have a much higher relative frequency of gradient steps that do not move into the same
magnitude or direction (off-diagonal entries) for both factual and counterfactual error.
E Nearest Neighbour Approximation of PEHE for Multiple
Treatments (NN-mPEHE)
The ^nn-pehe metric can be extended to the multiple treatment setting by considering the mean
^nn-pehe between all (2) possible pairs of treatments:
eNN-mPEHE
1 k-1 i-1
百 i=0j=0
PEHE,i,j
(E.1)
F Pseudocode Propensity Score Matching (PSM)
Algorithm S2 outlines the procedure of preprocessing a training set using PSMMI. For PSMPM, we
used the PM matching algorithm (see Algorithm 1) to find matches between samples from the treat-
ment groups. ForPSMMI, we used the MatchIt package (Ho et al. (2011)) on setting ”nearest” to find
matches between sets of samples from two treatment groups (function get_matched_samples).
Algorithm S2 Preprocessing a training set using Propensity Score Matching (PSM). After prepro-
cessing, the training set contains an equal number of samples from each treatment group and the
covariates xi across all treatment groups are approximately balanced.
Input: N training samples Xtrain with assigned treatments t, Number of treatment options k, func-
tion get_matched_samples to find nearest matches by propensity score between sets of
samples
Output: Preprocessed training set Xout consisting of matched samples from each treatment group
1: procedure PREPROcess_Training_Set：
2:	tbase J index of treatment with smallest number of samples in Xtrain
3:	Xbase J all samples in Xtrain with treatment tbase
4:	for i = 0 to k - 1 do
5:	if i = tbase then
6:	Xcurrent J all samples in Xtrain with treatment i
7:	Xmatched J getaatched_samples(XbaSe, XCUlTent)
8:	Add all samples in Xmatched to Xout
9:	Add all samples in Xbase to Xout
13
Under review as a conference paper at ICLR 2019
G TARNET, CFRNET and PD for Multiple Treatments
In TARNET models, we used ELU nonlinearities between the L hidden layers with M hidden units.
L and M were hyperparameters that we optimised during hyperparameter optimisation (Section H).
We did not use batch normalisation (BN). To extend CFRNET to the multiple treatment setting, we
defined the first treatment option as the control treatment and regularised all treatment options to
have the same activation distribution in the topmost shared layer (Shalit et al. (2017)). For PD, we
only used propensity dropout and the propensity estimation network. We did not make use of the
alternating training schedule proposed in (Alaa et al. (2017)). The ”+ MLP” model was a simple
MLP with L hidden layers of M hidden units that received the treatment option index tj as an
additional input along with the covariates X , and output a k-dimensional potential outcome vector
Y. The MLP used ELU nonlinearities between the L hidden layers, and also did not use BN.
H Hyperparameters
To ensure a fair comparison, we used a standardised approach to hyperparameter optimisation for
those methods for which we did not have previously reported performance numbers. In particular,
each method we trained was optimised over the same amount of hyperparameter optimisation runs.
For the methods that used neural network models (TARNET, CFRNET, PD, PSMPM, PSMMI, in-
cluding ”+ on X” and ”+ MLP”, and PM), we chose hyperparameters at random from predefined
ranges (Table S2). For CFRNET, we additionally varied the weight of the imbalance penalty at ran-
dom between 0.1, 1.0, and 10.0. All methods that used a neural network model used the TARNET
architecture to ensure differences in performance are not due to architectural differences. To train
PM for the Jobs dataset, we used fixed hyperparameters: A batch size of 50, 60 hidden units per
layer, and 3 hidden layers. For optimisation, we used the Adam optimiser with a learning rate of
0.001 for a maximum of 100 (News, TCGA) or 400 (IHDP, Jobs) with an early stopping patience
of 30 on the factual MSE. We used the default hyperparameters for BART and CF from the ”bart-
Machine” (Kapelner & Bleich (2013)) and”grf” (Athey et al. (2016)) R-packages. For GANITE,
we used our own implementation since there was no open source implementation available, and - in
addition to the parameters in Table S2 - optimised over the supervised loss weights α and β (Yoon
et al. (2018)) between 0.1, 1, and 10. For the GANITE generators and discriminators, we used MLP
architectures with L hidden layers of M hidden units each.
Table S2:	Hyperparameter ranges used in the performed experiments.
Hyperparameter	IHDP	News/TCGA
Batch size B	4, 8, 50, 100	50
Number of units per hidden layer M	50, 100, 200	40, 60, 80
Number of hidden layers L	1,2,3	2, 3
I Policy Risk RPOL (π)
We define the policy risk RPol(π) as the expected loss in value when treating according to the policy
π implied by an ITE estimator (Shalit et al. (2017)):
Rpo1(∏) = 1 - (E[yι∣∏(X) = 1] ∙p(∏ = 1) + E[yo∣∏(X) = 0] ∙p(∏ = 0))	(I.1)
where the policy π(X) for a sample X implied by an ITE estimator that produces potential outcomes
y is defined as:
)=11 if yι,χ - yo,χ > λ (to treat)
0 otherwise	(to not treat)
(I.2)
14
Under review as a conference paper at ICLR 2019
J Origins of Performance Numbers in Table 3
Where available, performance numbers in Table 3 were taken from the original authors for IHDP
and Jobs (Table S3). For News-2/4/8/16 and TCGA, performance numbers were produced by us.
Table S3:	Origins of the performance results reported in Table 3. Entries show the reference to the
original report of the performance results, or ”ours” if we produced the results. n.r. = not reported
Method	IHDP	Jobs
PM	ours	ours
+ on X	ours	ours
+ MLP	ours	ours
kNN	(ShalitetaL 2017)	(Shalit et al. 2017)
PSMPM	n.r.	ours
PSMMI	ours	ours
RF	(Shalitetal. 2017)	(Shalit et al. 2017)
CF	(Shalit et al. 2017)	(Shalit et al. 2017)
BART	(Shalit et al. 2017)	(Shalit et al. 2017)
GANITE	(Yoon etal. 2018)	(Yoon et al. 2018)
BNN	(Shalit et al. 2017)	(Shalit et al. 2017)
PD	n.r.	n.r.
TARNET	(Shalit et al. 2017)	(Shalit et al. 2017)
CFRNETWass	(Shalit et al. 2017)	(Shalit et al. 2017)
CEVAE	(Louizos et al. 2017)	(Louizos et al. 2017)
CMGP	(Yoon et al. 2018)	(Yoon et al. 2018)
15