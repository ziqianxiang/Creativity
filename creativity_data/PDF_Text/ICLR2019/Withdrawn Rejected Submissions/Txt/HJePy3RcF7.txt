Under review as a conference paper at ICLR 2019
Rethinking learning rate schedules
FOR STOCHASTIC OPTIMIZATION
Anonymous authors
Paper under double-blind review
Ab stract
There is a stark disparity between the learning rate schedules used in the prac-
tice of large scale machine learning and what are considered admissible learning
rate schedules prescribed in the theory of stochastic approximation. Recent re-
sults, such as in the ’super-convergence’ methods which use oscillating learning
rates, serve to emphasize this point even more. One plausible explanation is that
non-convex neural network training procedures are better suited to the use of fun-
damentally different learning rate schedules, such as the “cut the learning rate
every constant number of epochs” method (which more closely resembles an ex-
ponentially decaying learning rate schedule); note that this widely used schedule
is in stark contrast to the polynomial decay schemes prescribed in the stochastic
approximation literature, which are indeed shown to be (worst case) optimal for
classes of convex optimization problems.
The main contribution of this work shows that the picture is far more nuanced,
where we do not even need to move to non-convex optimization to show other
learning rate schemes can be far more effective. In fact, even for the simple case
of stochastic linear regression with a fixed time horizon, the rate achieved by any
polynomial decay scheme is suboptimal compared to the statistical minimax rate
(by a factor of condition number); in contrast the “cut the learning rate every
constant number of epochs” provides an exponential improvement (depending
only logarithmically on the condition number) compared to any polynomial de-
cay scheme. Finally, it is important to ask if our theoretical insights are somehow
fundamentally tied to quadratic loss minimization (where we have circumvented
minimax lower bounds for more general convex optimization problems)? Here,
we conjecture that recent results which make the gradient norm small at a near op-
timal rate, for both convex and non-convex optimization, may also provide more
insights into learning rate schedules used in practice.
1 Introduction
The recent advances in machine learning and deep learning rely almost exclusively on stochastic op-
timization methods, primarily SGD and its variants. Here, these large scale stochastic optimization
methods are manually (and often painstakingly) tuned to the problem at hand (often with paral-
lelized hyper-parameter searches), where there is, as of yet, no class of “universal methods” which
uniformly work well on a wide range of problems with little to no hyper-parameter tuning. This is in
stark contrast to non-stochastic numerical optimization methods, where it is not an overstatement to
argue that the l-BFGS and non-linear conjugate gradient methods (with no hyper-parameter tuning
whatsoever) have provided nearly unbeatable procedures (for a number of decades) on nearly every
unconstrained convex and non-convex problem. In the land of stochastic optimization, there are
two dominant (and somewhat compatible approaches): those methods which often manually tune
learning rate schedules to achieve the best performance (Krizhevsky et al., 2012; Sutskever et al.,
2013; Kingma & Ba, 2014; Kidambi et al., 2018) and those methods which rely on various forms of
approximate preconditioning (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2014).
This works examines the former class of methods, where we seek a more refined understanding of
the issues of learning rate scheduling, through both theoretical analysis and empirical studies.
Learning rate schedules for SGD is a rather enigmatic topic since there is a stark disparity between
what is considered admissible in theory and what is employed in practice to achieve the best re-
1
Under review as a conference paper at ICLR 2019
sults. Let us elaborate on this distinction more clearly. In theory, a vast majority of works starting
with Robbins & Monro (1951); Polyak & Juditsky (1992) consider learning rates that have the form
of ηt = b+ata for some a, b ≥ 0 and 1/2 < ɑ ≤ 1 -We call these polynomial decay schemes.
The key property enjoyed by these polynomial decay schemes is that they are not summable but are
square summable. A number of Works obtain bounds on the asymptotic convergence rates of such
schemes. Note that the focus of these Works is to design learning rate schemes that Work Well for
all large values of t. In contrast, practitioners are interested in achieving the best performance given
a computational budget or equivalently a fixed time horizon T e.g., 100 passes on training dataset
With a batch size of 128.
The corresponding practically best performing learning rate scheme is often one Where the step
size is cut by a constant factor once every feW epochs, or, equivalently, When no progress is made
on a validation set (Krizhevsky et al., 2012; He et al., 2016b) (often called a dev set based decay
scheme). Such schemes are Widely popular to the extent that they are available as schemes in deep
learning libraries such as PyTorch 1 and several such useful tools of the trade are taught on popular
deep learning courses 2. Furthermore, What is (often) puzzling (from a theory perspective) is the
emphasis that is laid on “babysitting” the learning rates 3 to achieve the best performance. Why
do practitioners use constant and cut learning rate schemes While most of the theory Work routinely
Works With polynomial decaying schemes? Of course, implicit to this question is the vieW that both
of these schemes are not equivalent. Indeed if both of these Were equivalent, one could parameterize
the learning rate as b+ata and do hyperparameter search over a, b and a. In practice, this simply
does not give results comparable to the constant and cut schemes.4 One potential explanation for
this could be that, in the context of neural netWork training, local minima found by constant and
cut schemes are of much better quality than those found by polynomial decay schemes, While for
convex problems, polynomial decay schemes are indeed optimal.
The primary contribution of this Work is to shoW that this is simply not the case. We concretely shoW
hoW minimax optimal theoretical learning rates (i.e. polynomial decay schemes for Wide classes of
convex optimization problems) may be misleading (and sub-optimal for locally quadratic problems),
and the story in practice is more nuanced. There important issues at play With regards to this sub-
optimality. First, even for the simple case of stochastic linear regression, With a fixed time horizon,
the rate achieved by any polynomial decay scheme (i.e., any choice of a, b and α) is suboptimal
compared to the statistical minimax rate (i.e., information theoretically best possible rate achievable
by any algorithm) by a factor of condition number κ (see Section 3 for definitions), While there exist
constant and cut schemes that are suboptimal only by a factor of log κ.
Second, this Work shoWs that a factor of κ suboptimality is unavoidable ifWe Wish to bound the error
of each iterate of SGD. In other Words, We shoW that the convergence rate of lim sup of the error,
as t → ∞, has to be necessarily SUboPtimaI by a factor of Ω(κ) compared to the statistical minimax
rate, for any learning rate sequence (polynomial or not). In fact, at least Ω1∕κ fraction of the iterates
have this sUboptimality. With this resUlt, things become qUite clear - all the Works in stochastic
approximation try to bound the error of each iterate of SGD asymptotically (or lim sup of the error
in other words). Since this necessarily has to be SUbOPtlmaI by a factor of Ω(κ) compared to the
statistical minimax rates, the suboptimality of polynomial decay rates is not an issue. HoWever, With
a fixed time horizon, there exist learning rate schemes with much better convergence rates, while
polynomial decay schemes fail to get better rates in this simpler setting (of known time horizon).
Thirdly, the work shows that, for stochastic linear regression, if we consider lim inf (rather than
lim sup) of the error, it is possible to design schemes that are suboptimal by only a factor of log κ
compared to the minimax rates. Variants of the constant and cut schemes achieve this guarantee.
In summary, the contributions of this paper are showing how widely used pratical learning rate
schedules are, in fact, highly effective even in the convex case. In particular, our theory and empirical
results demonstrate this showing that:
1https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.
ReduceLROnPlateau
2http://cs231n.github.io/
3http://cs231n.github.io/neural-networks-3/
4In fact, this work shows an instance where there is a significant (provable) difference between the perfor-
mance of these two schemes.
2
Under review as a conference paper at ICLR 2019
•	For a fixed time horizon, constant and cut schemes are provably, significantly better than
polynomial decay schemes.
•	There is a fundamental difference between fixed time horizon and infinite time horizon.
•	The above difference can be mitigated by considering lim inf of error instead of lim sup.
•	In addition to our theoretical contributions, we empirically verify the above claims for
neural network training on cifar-10.
Extending results on the performance of constant and cut schemes to more general convex optimiza-
tion problems, beyond stochastic linear regression, is an important future direction. However, the
fact that the suboptimality of polynomial decay schemes even for the simple case of stochastic linear
regression, has not been realized after decades of research on stochastic approximation is striking.
In summary, the results of this paper show that, even for stochastic linear regression, the popu-
lar in practice, constant and cut learning rate schedules are provably better than polynomial decay
schemes popular in theory and that there is a need to rethink learning rate schemes and convergence
guarantees for stochastic approximation. Our results also suggest that current approaches to hyper-
parameter tuning of learning rate schedules might not be right headed and further suggest potential
ways of improving them.
Paper organization: The paper is organized as follows. We review related work in Section 2. Sec-
tion 3 describes the notation and problem setup. Section 4 presents our results on the suboptimality
of both polynomial decay schemes and constant and cut schemes. Section 5 presents results on
infinite horizon setting. Section 6 presents experimental results and Section 7 concludes the paper.
2	Related Work
We will split related work into two parts, one based on theory and the other based on practice.
Related efforts in theory: SGD and the problem of stochastic approximation was introduced in
the seminal work of Robbins & Monro (1951); this work also elaborates on stepsize schemes that
are satisfied by asymptotically convergent stochastic gradient methods: we refer to these schemes
as “convergent” stepsize sequences. The (asymptotic) statistical optimality of iterate averaged SGD
with larger stepsize schemes of O(1∕na) with α ∈ (0.5,1) was proven in the seminal works of RUP-
pert (1988); Polyak & Juditsky (1992). The notions of convergent learning rate schemes in stochastic
approximation literatUre has been stUdied in great detail (LjUng et al., 1992; KUshner & Yin, 2003;
Bharath & Borkar, 1999; Lai, 2003). Nearly all of the aforementioned works rely on fUnction valUe
sUb-optimality to measUre convergence and rely on the notion of asymptotic convergence (i.e. in
the limit of the nUmber of Updates of SGD tending to infinity) to derive related “convergent stepsize
schedUles”. Along this line of thoUght, there are several efforts that prove (minimax) optimality
of the aforementioned rates (in a worst case sense and not per problem sense) e.g., Nemirovsky &
YUdin (1983); Raginsky & Rakhlin (2011); Agarwal et al. (2012).
An alternative viewpoint is to consider gradient norm as a means to measUre the progress of an
algorithm. Along this line of thoUght are several works inclUding the stochastic process viewpoint
considered by Polyak & JUditsky (1992) and more recently, the work of Nesterov (2012) (working
with deterministic (exact) gradients). The work of Allen-ZhU (2018) considers qUestions relating to
making the gradient norm small when working with stochastic gradients, and provides an improved
rate. We retUrn to this criterion in Section 7.
In terms of oracle models, note that both this paper, as well as other resUlts (Lacoste-JUlien et al.,
2012; Rakhlin et al., 2012; BUbeck, 2014), work in an oracle model that assUmes boUnded variance
of stochastic gradients or similar assUmptions. There is an alternative oracle model for analyzing
SGD as followed in papers inclUdingBach & MoUlines (2013); Bach (2014); Jain et al. (2017) which
is argUably more reflective of SGD’s behavior in practice. For more details, refer to Jain et al. (2017).
It is an important direction to prove the resUlts of this paper working in the alternative practically
more applicable oracle model.
Efforts in practice: As highlighted in the introdUction, practical efforts in stochastic optimization
have diverged from the classical theory of stochastic approximation, with several deep learning
3
Under review as a conference paper at ICLR 2019
libraries like pytorch 5 providing unconventional alternatives such as cosine/sawtooth/dev set decay
schemes, or even exponentially decaying learning rate schemes. In fact, a natural scheme used in
training convolutional neural networks for vision is where the learning rate is cut by a constant factor
after a certain number of epochs. Such schemes are essentially discretized variants of exponentially
decaying learning rate schedules. We note that there are other learning rate schedules that have been
recently proposed such as sgd with warm restarts (Loshchilov & Hutter, 2016), oscillating learning
rates (Smith & Topin, 2017) etc., that are unconventional and have attracted a fair bit of attention.
Furthermore, exponential learning rates appear to be considered in more recent NLP papers (see for
e.g., Krishnamurthy et al. (2017)) 6.
3	Problem Setup
Notation: We represent scalars with normal font a, b, L etc., vectors with boldface lowercase char-
acters a, b etc. and matrices with boldface uppercase characters A, B etc. We represent positive
semidefinite (PSD) ordering between two matrices using . The symbol & represents that the di-
rection of inequality holds for some universal constant.
Our theoretical results focus on the following additive noise stochastic linear regression problem.
We present the setup and associated notation in thisdsfec1tion. We wish to solve:
def >	>
min f (W) where f (W) = -w HW — W b
for some positive definite matrix H and vector b.7 We denote the smallest and largest eigenvalues
of H by μ > 0 and L > 0. K = L denotes the condition number of H. We have access to
a stochastic gradient oracle which gives Us Vf (w) = Vf (W) + e, where e is a random vector
satisfying8 E [e] = 0 and E ee> = σ2H.
Given an initial point W0 and step size sequence ηt , the SGD algorithm proceeds with the update
Wt = Wt-1 — ηtVf(Wt-1) = Wt-1 — ηt (Vf(Wt-1) +et) ,
where et are independent for various t and satisfy the above mean and variance conditions.
Let W* C=f argmi□w∈Rd f (w). The SUbOPtimality of a point W is given by f (w) — f (w* ). It is
well known that given t accesses to the stochastic gradient oracle above, any algorithm that uses
these stochastic gradients and outputs Wt has suboptimality that is lower bounded by σtd. More
concretely (Van der Vaart, 2000), we have that
E [f (Wt)] — f (w*)
lim
t→∞
σ2d∕t
≥1.
Moreover there exist schemes that achieve this rate of (1 + o(1)) σ2d e.g., constant step size SGD
with averaging (Polyak & Juditsky, 1992). This rate of σ2d∕t is called the statistical minimax rate.
4	Comparison between polynomial decay schemes vs constant
AND CUT SCHEMES
In this section, we will show that polynomial decay schemes are suboptimal compared to the statis-
tical minimax rate by at least a factor ofκ while constant and cut schemes are suboptimal by at most
a factor of log κ.
5see https://pytorch.org/docs/stable/optim.html for a complete list of alternatives
6Refer to their JSON file https://github.com/allenai/allennlp/blob/master/
training_config/wikitables_parser.jsonnet
7Any linear least squares * PNi (x>W — y⅛)2 can be written as above with H d=f n Pi xix> and
b d=f n Pi yixi.
8While this might seem very special, this is indeed a fairly natural scenario. For instance, in stochastic linear
regression with independent additive noise, i.e., yt = x> w* + & where 以 is a random variable independent of
xt and E [t] = 0 and E t2 = σ2, the noise in the gradient has this property. On the other hand, the results in
this paper can also be generalized to the setting where E ee> = V for some arbitrary matrix V. However,
error covariance of σ2 H significantly simplifies exposition.
4
Under review as a conference paper at ICLR 2019
4.1	Suboptimality of polynomial decay schemes
Our first result shows that there exist problem instances where all polynomial decay schemes i.e.,
those of the form ^α，for any choice of a, b and a are SUboPtimal by at least a factor of Ω(κ)
compared to the statistical minimax rate.
Theorem 1. There exists a problem instance such that the initial function value f (w0) ≤ σ2d, and
for any fixed time T satisfying T ≥ κ2, for all a, b ≥ 0 and 0.5 ≤ α ≤ 1, and for the learning rate
scheme η = ^α, we have E [f (wτ)] - f (w*) ≥ 32 ∙年.
4.2	Suboptimality of constant and cut scheme
OUr next resUlt shows that there exist constant and cUt schemes that achieve statistical minimax rate
UPto a mUltiPlicative factor of only log κ log2 T .
Theorem 2.	For any problem and fixed time horizon T > κ log(κ), there exists a constant and cut
learning rate Scheme that achieves E [f (WT)] — f (w*) ≤ f(wO)-f(w ) + 2 log K ∙ log2 T ∙ σTd.
We will now consider an exPonential decay scheme (in contrast to Polynomial ones from Section 4.1)
which is a smoother version of constant and cUt scheme. We show that the same resUlt above for
constant and cUt scheme can also be extended to the exPonential decay scheme.
Theorem 3.	For any problem and fixed horizon T, there exist constants a and b such that learning
rate scheme of η = b ∙ exp (—at) achieves E [f (WT)] — f (w*) ≤ f T-(IfOw) ) + log K ∙ log T ∙ σTd.
The above resUlts show that constant and cUt as well as exPonential decay schemes, that dePend on
the time horizon, are mUch better than Polynomial decay schemes. Between these, exPonential decay
schemes are smoother versions of constant and cUt schemes, and so one woUld hoPe that they might
have better performance than constant and cut schemes - We do see a log T difference in our bounds.
One unsatisfying aspect of the above results is that the rate behaves as loTT, which is asymptotically
worse than the statistical rate of T. It turns out that it is indeed possible to improve the rate to 十
using a more sophisticated scheme. The main idea is to use constant and polynomial schemes in
the beginning and then switch to constant and cut (or exponential decay) scheme later. To the best
of our knowledge, these kind of schemes have never been considered in the stochastic optimization
literature before. Using this learning rate sequence successively for increasing time horizons would
lead to oscillating learning rates. We leave a complete analysis of oscillating learning rates (for
moving time horizon) to future work.
Theorem 4.	Fix K ≥ 2. For any problem and fixed time horizon T/ log T > 5K, there exists a
learning rate SCheme that achieves E [f (WT)] — f (w*) ≤ f(wo)—f(W ) + 50log2 K ∙ σTd.
5 Infinite horizon setting
In this section we show a fundamental limitation of the SGD algorithm. First we will prove that the
SGD algorithm, for any learning rate sequence, needs to query a point with suboptimality more than
Ω(κ/ log K) ∙ σ2d∕T for infinitely many time steps T.
Theorem 5.	There exists a universal constant C > 0 such that for any SGD algorithm with ηt ≤
1 /2κ for all t9, we have lim SuPT→∞ Ef(WT2d(f)(w )
≥______K_____
≥ C log(κ + 1).
Next we will show that in some sense the “fraction” of query points that has value more than τσ2∕T
is at least Ω(1∕τ) when T is smaller than the threshold in Theorem 5.
Theorem 6. There exists universal constants C1,C2 > 0 such thatfor any T ≤ c。】1Og(κ+1) where
C is the constant in Theorem 5, for any SGD algorithm and any number of iteration T > 0 there
exists a T0 ≥ T such thatfor any T ∈ [T0, (1 + ∖∕C2τ)T0] we have 叫"wT2)]∕f (W ) ≥ τ.
Finally, we now show that there are constant and cut or exponentially decaying schemes that achieve
the statistical minimax rate up to a factor of log K log2 T in the lim inf sense.
9Learning rate more than 2∕κ will make the algorithm diverge.
5
Under review as a conference paper at ICLR 2019
Model	err @50K	err @100K	err @200K
optimized for 50κ	0.00018	9.6 × 10-5^	9.6 × 10-L
optimized for 100κ	0.00018	9.6 × 10-5	9.6 × 10-1-
optimized for 200κ	000275	00007 ―	2.5 × 10-5~
Table 1: Does the learning rate sequence optimized for
a given end time generalize to other end times? Observe
how a model optimized to perform for a specific horizon
behaves sub-optimally for other time horizons.
Figure 1: Plot of final error versus condi-
tion number κ for decay schemes 1,2,3 for
the 2-d regression problem.
Theorem 7. There exists an absolute constant C and a constant and cut learning rate scheme that
Ef(WT)]-f (W* ) ,
obtains liminfτ→∞ (二2丁 iog2 t/t) ≤ C log K.
Similar results can be obtained for the exponential decay scheme of Theorems 3 and 4 with moving
time horizon. However the resultant learning rates might have oscillatory behavior. This might
partly explain the benefits of oscillating learning rates observed in practice (Smith & Topin, 2017).
6	Experimental Results
We present experimental validation of our claims through controlled synthetic experiments on a two-
dimensional quadratic objective and on a real world non-convex optimization problem of training a
residual network on the cifar-10 dataset, to illustrate the shortcomings of the traditional stochastic
approximation perspective (and the advantages of non-convergent exponentially decaying and os-
cillating learning rate schemes) for a realistic problem encountered in practice. Complete details of
experimental setup are given in Appendix D.
6.1	Synthetic Experiments: Two-Dimensional Quadratic objective
We consider the problem of optimizing a two-dimensional quadratic objective, similar in spirit
as what is considered in the theoretical results of this paper. In particular, for a two-dimensional
quadratic, we have two eigenvalues, one of magnitude κ and the other being 1. We vary our condi-
tion number κ ∈ {50, 100, 200} and use a total of 200κ iterations for optimization. The results ex-
pressed in this section are obtained by averaging over two random seeds. The learning rate schemes
we search over are:
力—η	门)	D= η	(2)
η 1 + b ∙ t	1 + b√t	ηt = η ∙ exp(-b ∙ t). (3)
For the schemes detailed above, there are two parameters that need to be searched over: (i) the
starting learning rate η0 and, (ii) the decay factor b. We perform a grid search over both these
parameters and choose ones that yield the best possible final error at a given end time (i.e. 200κ).
We also make sure to extend the grid should a best performing grid search parameter fall at the edge
of the grid so that all presented results lie in the interior of our final grid searched parameters.
We will present results for the following experiments: (i) behavior of the error of the final iterate
of the SGD method with the three learning rate schemes (1),(2), and (3) as we vary the condition
number, and (ii) how the exponentially decaying learning rate scheme (3) optimized for a shorter
time horizon behaves for a longer horizon.
For the variation of the final iterate’s excess risk when considered with respect to the condition
number (Figure 1), we note that polynomially decaying schemes have excess risk that scales linearly
with condition number, corroborating Theorem 1. In contrast, exponentially decaying learning rate
scheme admits excess risk that nearly appears to be a constant and corroborates Theorem 3. Finally,
we note that the learning rate schedule that offers the best possible error in 50κ or 100κ steps does
not offer the best error at 200κ steps (Table 1).
6
Under review as a conference paper at ICLR 2019
① n-E> UO-zpunH-.≡E 匕
Figure 2: Plot of the training function value (left) and test 0/1- error (right) comparing the three
decay schemes (two polynomial) 1, 2, (and one exponential) 3 scheme on the classification problem
of cifar-10 with a 44-layer residual net with pre-activation blocks.
6.2	Non-Convex Optimization: Training a Residual Net on cifar- 1 0
We consider here the task of training a 44-layer deep residual network (He et al., 2016b) with
pre-activation blocks (He et al., 2016a) (dubbed preresnet-44) for classifying images in the cifar-
10 dataset. The code for implementing the network employed in this paper can be found here10.
For all the experiments, we use the Nesterov’s Accelerated gradient method (Nesterov, 1983) imple-
mented in pytorch 11 with a momentum set to 0.9 and batchsize set to 128, total number of training
epochs set to 100, `2 regularization set to 0.0005.
Our experiments are based on grid searching for the best learning rate decay scheme on four para-
metric family of learning rate schemes described above 1,2,3; all gridsearches are performed on a
separate validation set (obtained by setting aside one-tenth of the training dataset = 5000 images)
and with models trained on the remaining 45000 images. For presenting the final numbers in the
plots/tables, we employ the best hyperparameters from the validation stage and train it on the entire
50, 000 images and average results run with 10 different random seeds. The parameters for grid-
searches and related details are presented in Appendix D. Furthermore, just as with the synthetic
experiments, we always extend the grid so that the best performing grid search parameter lies in the
interior of our grid search.
Comparison between different schemes: Figure 2 and Table 2 present a comparison of the per-
formance of the three schemes (1)-(3). They clearly demonstrate that the best exponential scheme
outperforms the best polynomial schemes.
Hyperparameter selection using truncated runs: Figure 3 and Tables 3 and 4 present a compari-
son of the performance of three exponential decay schemes each of which has the best performance
at 33, 66 and 100 epochs respectively. The key point to note is that best performing hyperparameters
at 33 and 66 epochs are not the best performing at 100 epochs (which is made stark from the per-
spective of the validation error). This demonstrates that selecting hyper parameters using truncated
runs, which has been proposed in some recent efforts such as hyperband (Li et al., 2017), might
necessitate rethinking.
Decay Scheme	Train Function Value	Test 0/1 error
-O(1∕t) (equation 1)-	0.0713 ± 0.015	10.20 ± 0.7%=
O(1∕√t) (equation 2)	-0.1119 ± 0.036	11.6 ± 0.67%
exp(-t) (equation 3)	0.0053 ± 0.0015	7.58 ± 0.21%
Table 2:	Comparing Train Softmax Function Value and Test 0/1 Error of various learning rate decay
schemes for the classification task on cifar-10 using a 44-layer residual net with pre-activations.
10https://github.com/D-X-Y/ResNeXt-DenseNet
11https://github.com/pytorch
7
Under review as a conference paper at ICLR 2019
(un-R> UOAUUnJ ura匕
Figure 3: Plot of the training function value (left) and test 0/1- error (right) comparing the ex-
ponential decay scheme (equation 3), with parameters optimized for 33, 66 and 100 epochs on the
classification problem of cifar-10 with a 44-layer residual net with pre-activation blocks.
Decay Scheme	Train FVal @33	Train FVal @66	Train FVal @100
exp(-t) [optimized for 33 epochs] (eqn 3)	0.098 ± O00F	0.0086 ± 0.002	0.0062 ± 0.0015-
exp(-t) [optimized for 66 epochs] (eqn 3)	0.107 ± 0.012	0.0088 ± 0.0014	0.0061 ± 0.0011
exp(-t) [optimized for 100 epochs] (eqn 3)	0.3 ± 0.06~~	0.071 ± 0.017	0.0053 ± O00T6-
Table 3:	Comparing Train Softmax Function Value of various learning rate decay schemes for the
classification task on cifar-10 using a 44-layer residual net with pre-activations.
Decay Scheme	Test 0/1 @33	Test 0/1 @66	Test 0/1 @100
exp(-t) [optimized for 33 epochs] (eqn 3)	10.36 ± 0.235%	8.6 ± 0.26*	8.57 ± 0.25*
exp(-t) [optimized for 66 epochs] (eqn 3)	10.51 ± 0.45%	8.51 ± 0.13%	8.46 ± 0.19%
exp(-t) [optimized for 100 epochs] (eqn 3)	14.42 ± 1.47%	9.8 ± 0.66%~	7.58 ± 0.21%
Table 4: Comparing Test 0/1 error of various learning rate decay schemes for the classification task
on cifar-10 using a 44-layer residual net with pre-activations.
7 Conclusions and Discussion
The main contribution of this work shows that the picture of learning rate scheduling is far more
nuanced than suggested by prior theoretical results, where we do not even need to move to non-
convex optimization to show other learning rate schemes can be far more effective than the standard
polynomially decaying rates considered in theory.
Is quadratic loss minimization special? One may ask if there is something particularly special
about why the minimax rates are different for quadratic loss minimization as opposed to more gen-
eral convex (and non-convex) optimization problems? Ideally, we would hope that our theoretical
insights (and improvements) can be formally established in more general cases. Here, an alternative
viewpoint is to consider gradient norm as a means to measure the progress of an algorithm. The
recent work of Allen-Zhu (2018) shows marked improvements for making the gradient norm small
(when working with stochastic gradients) for both convex and non-convex, in comparison to prior
results. In particular, for the strongly convex case, Allen-Zhu (2018) provides results which have
only a logarithmic dependency on κ, an exponential improvement over what is implied by standard
analyses for the gradient norm (Lacoste-Julien et al., 2012; Rakhlin et al., 2012; Bubeck, 2014);
Allen-Zhu (2018) also provides improvements for the smooth and non-convex cases. Thus, for the
case of making the gradient norm small, there does not appear to be a notable discrepancy between
the minimax rate of quadratic loss minimization in comparison to more general strongly convex (or
smooth) convex optimization problems. Interestingly, the algorithm of Allen-Zhu (2018) provides
a recursive regularization procedure that obtains an SGD procedure, where the doubling regulariza-
tion can be viewed as being analogous to an exponentially decaying learning rate schedule. Further
work in this direction may be promising in providing improved algorithms.
8
Under review as a conference paper at ICLR 2019
References
Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright. Information-
theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans-
actions on Information Theory, 2012.
Zeyuan Allen-Zhu. How to make the gradients small stochastically. CoRR, abs/1801.02982, 2018.
Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for
logistic regression. Journal of Machine Learning Research (JMLR), volume 15, 2014.
Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with
convergence rate O(1/n). In NIPS 26, 2013.
B. Bharath and V. S. Borkar. Stochastic approximation algorithms: overview and recent trends.
Sadhana ,1999.
Sebastien Bubeck. Theory of convex optimization for machine learning. CoRR, abs/1405.4980,
2014.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal ofMachine Learning Research, 12:2121-2159, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV (4), Lecture Notes in Computer Science, pp. 630-645. Springer, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016b.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-
ing stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of
existing momentum schemes for stochastic optimization. CoRR, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.
Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. Neural semantic parsing with type con-
straints for semi-structured tables. In EMNLP, pp. 1516-1526, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In NIPS, 2012.
Harold J. Kushner and George Yin. Stochastic approximation and recursive algorithms and applica-
tions. Springer-Verlag, 2003.
Simon Lacoste-Julien, Mark W. Schmidt, and Francis R. Bach. A simpler approach to obtaining
an o(1/t) convergence rate for the projected stochastic subgradient method. CoRR, 2012. URL
http://arxiv.org/abs/1212.2002.
Tze Leung Lai. Stochastic approximation: invited paper, 2003.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:
A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning
Research, 18(1):6765-6816, 2017.
Lennart Ljung, Georg Pflug, and Harro Walk. Stochastic Approximation and Optimization of Ran-
dom Systems. Birkhauser Verlag, Basel, Switzerland, Switzerland, 1992. ISBN 3-7643-2733-2.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with restarts. ”CoRR”,
”abs/1608.03983”, 2016.
Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-
mization. John Wiley, 1983.
9
Under review as a conference paper at ICLR 2019
Yurii Nesterov. How to make the gradients small, 2012.
Yurii E. Nesterov. A method for unconstrained convex minimization problem with the rate of con-
vergence O(1/k2). Doklady AN SSSR, 269, 1983.
Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and Optimization, volume 30, 1992.
Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics in
convex programming. IEEE Transactions on Information Theory, 2011.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In ICML, 2012.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, vol. 22, 1951.
David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Report,
ORIE, Cornell University, 1988.
Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of residual networks
using large learning rates. arXiv preprint arXiv:1708.07120, 2017.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
10
Under review as a conference paper at ICLR 2019
A Proofs of results in Section 4.1
κ
Proof of Theorem 1. The problem instance is simple. Let H
, where the first
1
d diagonal entries are equal to K and the remaining * diagonal entries are equal to 1 and all the off
diagonal entries are equal to zero. Let Us denote by v(i) d=f E ](w(i) — (w*)(i)) ] the variance in
the ith direction at time step t. Let the initialization be such that v(i) = σ2∕κ for i = 1, 2,..., d/2
and v0(i) = σ2 for i = d/2 + 1, ..., d. This means that the variances for all directions with eigenvalue
κ remain equal as t progresses and similarly for all directions with eigenvalue 1. We have
2T	T	T
=f E (WTI)-(W*)⑴)=Y (1 - ηjK)) v(1) + κσ2 X ηj Y (1 - ηiκ)2 and
j=1	j=1	i=j+1
2T	T	T
VTd)=fE I(WTd)-(WFd))	= Y(1-η)2v0d + σ2Xη2 Y (I-ni)2.
j=1	j=1	i=j+1
We consider a recursion for vt(i) with eigenvalue λi (1 or K). By the design of the algorithm, we
know
vt(+i)1 = (1 - ηtλi)2vt(i) + λiσ2ηt2 .
Let s(η, λ) = ι-(σ-1λ)2 be the solution to the stationary point equation X = (1 - ηλ)2 + λσ2η2.
Intuitively if we keep using the same learning rate η, then vt(i) is going to converge to s(η, λi). Also
note that s(η, λ) ≈ σ2η∕2 when ηλ《1.
We first prove the following claim showing that eventually the variance in direction i is going to be
at least s(ηT , λi).
Claim 1. Suppose s(ηt, λi) ≤ v0(i), then vt(i) ≥ s(ηt, λi).
Proof. We can rewrite the recursion as
vt(+i)1 - s(ηt, λi) = (1 - ηtλi)2 (vt(i) - s(ηt, λi)).
In this form, it is easy to see that the iteration is a contraction towards s(ηt, λi). Further, vt(+i)1 -
s(ηt, λi) and vt(i) - s(ηt, λi) have the same sign. In particular, let t0 be the first time such that
s(ηt, λi) ≤ v0(i) (note that ηt is monotone and so is s(ηt, λi)), it is easy to see that vt(i) ≥ v0(i) when
t ≤ t0 . Therefore we know vt(i) ≥ s(ηt0, λi), by the recursion this implies vt(0i)+1 ≥ s(ηt0 , λi) ≥
s(ηt0+1, λi). The claim then follows from a simple induction.	□
If s(ηT , λi) ≥ v0(i) for i = 1 or i = d then the error is at least σ2 d/2 ≥ Kσ2 d/T and we are done.
Therefore We must have s(ητ, K) ≤ VOI) = σ2∕κ, and by Claim 1 We know VTI) ≥ s(ητ, K) ≥
σ2ηT /2. The function value is at least
Ef(WT)]≥ d ∙ KVTI) ≥ P.
dκσ2 η	1
To make sure E [f (WT)] ≤ ―3^T we must have ητ ≤ 苏.Next we will show that when this
happens, VT(d) must be large so the function value is still large.
We will consider two cases, in the first case, b ≥ Tα. Since 8T ≥ ητ = b+Tα ≥ 语，we have
a ≤ 4τ. Therefore VTd ≥ (1 - a)2τv(d) ≥ σ2∕2, so the function value is at least E [f (Wt)] ≥
d VT ≥ dσ2 ≥ κdτ~, and we are done.
11
Under review as a conference paper at ICLR 2019
In the second case, b < Tα. Since 8T ≥ ητ = b+Tα ≥ 2τaα, We have a ≤ 0.25Tα-1. The sum of
learning rates satisfy
T
X ηi
i=1
TT
≤ X -aa ≤ X 0.25i-1 ≈ 0.25 log T.
i=1 i i=1
Here the second inequality uses the fact that Tα-1i-α ≤ i-1 When i ≤ T. Similarly, We also knoW
PT=I η2 ≤ PT=I 0.25i-2 ≤ π2∕24. Using the approximation (1 - η)2 ≥ exp(-2η - 4η2) for
η < 1/4, We get vT(d)
is at least E [f(wt)] ≥
theorem.
≥ exp(-2 PT=I η - 4 PT=I η2)ν(d) ≥ σ2∕5√Τ, so the function value
d v(d)
≥ dσ2	≥ κdσ2
This concludes the second case and proves the
□
B Proofs of results in Section 4.2
Proof of Theorem 2. The learning rate scheme is as folloWs. Divide the total time horizon into
lοg(κ) equal sized phases. In the 'th phase, the learning rate to be used is IogT；净 K. Note that the
learning rate in the first phase depends on strong convexity and that in the last phase depends on
smoothness (since the last phase has ` = log κ). Recall the variance in the kth coordinate can be
upper bounded by
T	TT
VT =f E (w*-(w*)(1))	≤ Y(ι-η∙ λ(k)) v(1) + λ(k) σ2 X η2 Y (ι-ηRk))
j =1	j=1	i=j+1
(T	∖	T(T
-2 X ηjλ(k) I V01) + λ(k)σ2 X η2 exp I -2 X ηiλ(k)
j =1	j =1	i=j+1
We Will shoW that for every k, We have
V把 ≤ log κ,∙ logT ∙ σ2,
vT	λ(k)T	σ ,
which directly implies the theorem. Now choose any k. Let' denote the number satisfying 2'* ∙μ ≤
λ(k) < 2'*+1 ∙ μ. Note that '* depends on k but we suppressed the dependence for notational
simplicity.
T
T
≤ exp -2Xηjλ(k) V0(k) + λ(k)σ2 X ηj2
j=1
exp I-2 X ηi λ(k)
i=j +1
≤ exp (-2lθgTlogK ∙ /)∙二
≤ pV T	log K
j=1
T	'*-1
+ λ(k)σ2 ∙ -T- ∙ X η2
log κ `
g	'=1
logκ
η2* ∙  -1-e+ X γl-
1 - eχp (-ηe* λ(k))	'='*+1 log K
(log K log T A2
I 2'μT )
+ Kσ2
log2 κ log2 T
-μT3
log K log2 T X ɪ
T	'=*+ι 2'μ
+ σ2
+ n`* σ2 + σ2
log K log2 T
T
≤
≤
≤
2log κ ∙ log2 T 2
+ -----TTTw；-----σ
This finishes the proof.
□
12
Under review as a conference paper at ICLR 2019
Proofof Theorem 3. The learning rate scheme We consider is Yt = γo ∙ ct-1 with γo =
logT∕(μTe), Te = T/ log K and C =(1 - 1/Te). Further, just as in previous lemmas, we Con-
sider a specific eigen direction λ(k) and write out the progress made along this direction by iteration
say, rT ≤ T:
T	T	(T	∖
e畤=Y(1 - Ytλ(k))2err0k) + (λ(k))2σ2 X Y ∙	Y (I-Mk))2
t=1	τ=1	t=τ +1
≤ exp f-2λ(k)γo X ct-}e∏T + (Xk)C	X c2τ exp (-2λ^γ0 X CtT)
exp - L
. (1-CT
<2λ(k)γo t∖	(
exp(π--τ∙C『卜xp(-
1-C
+
T
X C2τ exp
τ=1
(λ⑹)2σ2γ
C2^^
』.(一)
1-C
T
X C2τ exp
1-C
∙(cτ)
τ=1
Represent Cτ = x and 2λ(k)γ0∕(1 - C) = α. Substituting necessary values of the quantities, we
have α = 2 logT ∙ λ(k)∕μ ≥ 2. Now, the second term is upper bounded using the corresponding
integral, which is the following:
CT	T
c	c	1	22
x2 exp (-αx) ≤ J	x2 exp(-αx)dx ≤ — ∙(1+-----1---2 J exp(-α).
Substituting this in the previous bound, we have:
—
—
erT) ≤ exp (-α(1 - ct)) ∙ I err0k) +
「•(1+√α2 !2
≤ exp (-α(1 — ct)) ∙
≤ exp (-α(1 — ct)) ∙
exp (-α(1 — ct)) ∙
[ 16(λ(k))2σ2γ2 )
α
l	(λ(k))2σ2 logT
+ 16
+ μ2T2 ∙ 2(λ^∕μ)
,oλ(k)σ2 log Tʌ
+ 一μT2 —)
Now, setting T = Te log(Cλ(k)∕μ), and using 1 - a ≤ exp (-a), with C > 1 being some (large)
universal constant, we have:
λ(k)σ2logT
+ 一μT2—
/	1
≤ T2-(1∕C)
λ(k)σ2 log T
+ 一μT2—
(4)
Now, in order to argue the progress of the algorithm from T + 1 to T, we can literally view the
algorithm as starting with the iterate obtained from running for the first T steps (thus satisfying the
excess risk guarantee in equation 4) and then adding in variance by running for the duration of time
between T + 1 to T. For this part, we basically upper bound this behavior by first assuming that
there is no contraction of the bias and then consider the variance introduced by running the algorithm
from T+1 to T. This can be written as:
_
T-T	T-T
4
4
T
errTO≤ Y (1 - Ytλ(k))2errT≈) + (λ(k))2σ2 X Y (1 - gtλ(k))2
t=T+ι
τ=1	t=τ +1
一 ʌ	一 ʌ
T-T	T-T
≤ errj) + (λ(k))2σ2 X 弋`t Y (1 - gtN
τ=1	t=τ +1
13
Under review as a conference paper at ICLR 2019
Now, to consider bounding the variance of the process with decreasing sequence of learning rates,
we will instead work with a constant learning rate and understand its variance:
ʌ
T-T
(λ(k))2σ2 X Y2(1 - γλ(k))2(T-T-T) ≤
τ=1
γ2σ2*k))2
(2 — λ(k)γ)γλ(k)
≤ γσ2λ(k).
What this implies in particular is that the variance is a monotonic function of the learning rate and
thus the overall variance can be bounded using the variance of the process run with a learning rate
of YT.
T — r^	T — r^	T — r^
(λ(k))2σ2 X	YT+T	Y (1―Yt+Tλ(k))2	≤	(λ(k))2σ2	X	YT(1 — Ytλ(k))2	≤	YTσ2λ(k)
τ=1	t=τ +1	t=τ +1
≤ log T ∙ μ
一 μTe . Cλ(k)
σ2 log T log κ
≤ T
Plugging this into equation 4 and summing over all directions, We have the desired result. □
σ2λ(k)
Proof of Theorem 4. The learning rate scheme is as follows.
We first break T into three equal sized parts. Let A = T/3 and B = 2T /3. In the first T/3 steps,
We use a constant learning rate of 1/L. In the second T/3 steps, We use a polynomial decay learning
rate ηA+t = *(长+〃2). In the third T/3 steps, we break the steps into log2(κ) equal sized phases.
In the 'th phase, the learning rate to be used is 原等,.Note that the learning rate in the first phase
depends on strong convexity and that in the last phase depends on smoothness (since the last phase
has ` = log κ).
Recall the variance in the kth coordinate can be upper bounded by
盟 =fE [(wT0-(w**2] ≤ Y(1 —ηjλ(k))2v(1) + λ(k)σ2Xηj Y (1 —ηRk))2
j=1	j=1	i=j+1
≤ exp —2Xηjλ(k) v0(1) + λ(k)σ2 X ηj2 exp —2 X ηiλ(k) .
j=1	j=1	i=j+1
We will show that for every k, we have
(k)
vTk)≤ 书 + 50⅛κ ∙ σ2∙,	⑸
which directly implies the theorem.
We will consider the first T/3 steps. The guarantee that we will prove for these iterations is: for any
t ≤ A, v(k) ≤ (1 — λ(k)∕L)2tv(k) + 展
This can be proved easily by induction. Clearly this is true when t = 0. Suppose it is true for t — 1,
let’s consider step t. By recursion of vt(k) we know
v(k) = (1 — λ(k)∕L)2v(-)1 + λ(k)σ2∕L2
—λ(k)∕L)2 + X(k)/L)
≤ (1—λ(k)∕L)2tv(k)+σ2.
L
Here the second step uses induction hypothesis and the third step uses the fact that (1 — x)2 +x ≤ 1
when X ∈ [0,1]. Inparticular, since (1 — λ(k)∕L)2τ/3 ≤ (1 — 1∕κ)2τ/3 ≤ (1 — 1∕κ)3κ logT = 1/T3,
we know at the end of the first phase, v(k) ≤ v0k)∕T3 + σ2.
2
≤ (1 - λ(k)∕L)2tv(k) + - ((1
14
Under review as a conference paper at ICLR 2019
In the second T/3 steps, the guarantee would be: for any t ≤ T/3, vA(k+) t ≤ v0(k)/T 3 + 2ηA+tσ2.
We will again prove this by induction. The base case (t = 0) follows immediately from the guarantee
for the first part. Suppose this is true for A + t - 1, let us consider A + t, again by recursion we
know
vA(k+)t = (1 - λ(k)ηA+t-1)2vA(k+)t-1 + λ(k)σ2ηA2 +t-1
≤ v(k)∕τ3 + 2ηA+t-ισ2 ((1 — λ(k)ηA+t-ι)i2 + 2λ(k)ηA+t-ι^
≤ v(fc)/T3 + 2ηA+t-ισ2(1 - 2μηA+t-ι) ≤ v(fc)/T3 + 2ηA+tσ2.
Here the last line uses the fact that 2ηA+t-ι(1 一 2μηA+t-ι) ≤ 2ηA+tσ2, which is easy to verify by
our choice of η. Therefore, at the end of the second part, We have V(^) ≤ v(k)∕T3 + *(长+*/6).
Finally we will analyze the third part. Let T = T∕3log2 κ, we will consider the variance v([,^ at
the end of each phase. We will make the following claim by induction:
Claim 2. Suppose 2 ∙ μ ≤ λ(k, then
vB+ 'T ≤ VB) exp(-3') + 2Tη2λ(k)σ2.
(k)	(k)
Proof. We will prove this by induction. When ` = 0, clearly we have VB ≤ VB so the claim is
true. Suppose the claim is true for ' 一 1, we will consider what happens after the algorithm uses n`
for T steps. By the recursion of the variance we have
v(T ≤ v(k-1)T ∙ exP(-2η` ∙ Nk)T + ^Γη2λ(k)σ2.
Since 2' ∙ μ ≤ λ(k), we know exp(—2ηg ∙ λ(k'T) ≤ exp(—3). Therefore by induction hypothesis
we have
vB+ 'T ≤ 唐 exp(—3') + exp(—3) ∙ 2Tη2-iλ(k) + Tη*k ≤ 唐 exp(—3') + 2Tη2λ(k).
This finishes the induction.	口
By Claim 2, Let '* denote the number satisfying 2'* ∙ μ ≤ λ(k) < 2`* + 1 ∙ μ, by this choice we know
μ∕λ(k) ≥ 2 exp(—3'?) we have
≤ vB+ '*T ≤ VB' exp(-3'*) + 2Tη2* λ(k)σ2
‹ 唆 + 24σ2 + 50 log? K	Q
≤ ~T3 + λ(k)T + 3λ(k)T ^ σ .
v0k)	50log2 K	2
≤ F + λ(k)T ∙σ .
Therefore, the function value is bounded by E [f (wT)] = Pd=1 λ(k)v(k) ≤ ffw + 50 噜2 K ∙
σ2d.	口
C Proofs of results in Section 5
All of our counter-examples in this section are going to be the same simple function. Let H be a
diagonal matrix with d∕2 eigenvalues equal to K and the other d∕2 eigenvalues equal to 1. Intuitively,
we will show that in order to have a small error in the first eigendirection (with eigenvalue K), one
need to set a small learning rate ηt which would be too small to achieve a small error in the second
15
Under review as a conference paper at ICLR 2019
eigendirection (with eigenvalue 1). As a useful tool, we will decompose the variance in the two
directions corresponding to κ eigenvalue and 1 eigenvalue respectively as follows:
vT(1) d=ef E
](wT1)-(w*)⑴)2
T	TT
Y (1 - ηj κ)2 v0(1) + κσ2 Xηj2 Y (1 - ηiκ)2
j=1	j=1	i=j+1
≥ exp
T
-2	ηjκ
j=1
TT
+ κσ2	ηj2 exp -2	ηiκ
j=1
i=j+1
and
(6)
d=ef E
2T
-(w*)⑵)=Y(i-η )2 v02)
j=1
(T ∖	T
-2Xηj v0(2) +σ2Xηj2exp
j=1	j=1
TT
+σ2Xηj2 Y (1 - ηi)2
j=1	i=j+1
T τ \
-2 X ηi).
i=j+1
(7)
T
ProofofTheorem 5. Fix T = κ∕C log(κ + 1) where C is a universal constant that We choose later.
We need to exhibit that the lim sup is larger than τ . For simplicity we will also round κ up to the
nearest integer.
Let T be a given number. Our goal is to exhibit a T > T such that
flin (WT )
FTTJ
≥ τ. Given the step size
sequence ηt, consider the sequence of numbers To = T,Tι,…，T such that Ti is the first number
that
Ti
X	ηt
t=Ti-1+1
3
≤ K.
1
—≤
κ
Note that such a number always exists because all the step sizes are at most 2∕κ. We will also let
∆i be Ti - Ti-1. Firstly, from (6) and (7), we see that Pt ηt = ∞. Otherwise, the bias will never
decay to zero. If f (WTi-ι+∆i) > 7二；工.for Some i = 1, ∙∙∙ ,κ, we are done. If not, we obtain
the following relations:
∆2 ≤ σ2 X ηTo+t ≤ 呼∙ E
1	t=1
wT0+∆1 -(w*)(I)) 2
≤ eχP(3)flin(wTo+∆ι) ≤ exp?；。
T0 + ∆1
⇒ T0 ≤ (exp(3)τ - 1) ∆1.
Here the second inequality is based on (6). We will use C1 to denote exp(3). Similarly, we have
2	∆2
∆2 ≤ σ2 X ηTι+t ≤
2	t=1
生E
κ
WTI)+∆2 -(w* )(1))2
/一	Ciτσ2
≤ CIfiin(WTι+∆2 ) ≤ T] + ∆]
⇒ T1 ≤ (C1 τ - 1) ∆2
⇒	T0 ≤
(C1τ - 1)
C1τ
2
∆2.
Repeating this argument, we can show that
T = To	≤ (CIT -1)i ∆i	and	Ti	≤	(CIT	T)j；	∆j	∀ i < j.
0— (Cιτ 广1	(Ci τ )j-i-i	j
We will use i = 1 in particular, which specializes to
T V(CIT - 1)j 1 A	v ∙	9
11 ≤ ---------：~~∩—∆j ∀ j ≥ 2.
1 -	(C1τ )j-2	j -
Using the above inequality, we can lower bound the sum of ∆j as
κ
κ
∑∆j ≥ Ti ∙∑
j=2
j=2
(Cιτ)j-2	、T 1 X (1 l
(Cιτ - 1)j-1 ≥ T1 ^ 不 ^ j⅛ C +
j=2
j-2
≥ T1 ^ CT ∙ exp("(Cιτ)).
(8)
16
Under review as a conference paper at ICLR 2019
This means that
Ef(WTJ ≥ d ∙E ](wT2)-(W*)(2)) ] ≥ eχp(-6)σ2d ∙ XηT+i
exp(-6)σ2d	exp(-6)σ2d	exp (κ∕(Cγτ) — 3)	σ2d
≥	∆≥ T≥	CT	Pκ=2 ∆j,
where we used (8) in the last step. Rearranging, we obtain
E [f (WTK)] ≥ eχP (K/(CIT) - 3)
(σ2d/TK) —	Cιτ	.
If We choose a large enough C (e.g., 3Cι),the right hand side is at least exp((C/Ci)Kog(K+1)-3)
≥ κ.
□
To prove Theorem 6, We rely on the folloWing key lemma, Which says if a query point WT is bad
(in the sense that it has expected value more than 10τσ2d/T), then it takes at least Ω(T∕τ) steps to
bring the error back doWn.
Lemma 8. There exists universal constants C1,C2 > 0 Such thatfor any T ≤ CCl ιθg(κ+i) where C
is the constant in Theorem 5, suppose at step T, the query point WT satisfies f (WT) ≥ Ciτσ2d∕T,
then for all T ∈ [T, (1 + Cτ )T ] we have E [f (wt )] ≥ τσ2d∕T ≥ τσ2d∕T.
Proof of Lemma 8. Since	f(WT)
d (K (WTI) - (w*)(i)) + (WT2 - (w*)
≥
C1τσ2d∕T	and	f (wt )	=
we know either (WTI) - (w*)(1))	≥
Cιτσ2∕2κT or (WT2) - (w*)(2))	≥ Cιτσ2∕2T. Either way, we have a coordinate i with
eigenvalue λi (κ or 1) such that (WT) - (w*)(i)) ≥ Cιτσ2∕(2Tλi).
Similar as before, choose ∆ to be the first point such that
ηT +1 + ηT+2 + —+ ηT+△ ∈ [i∕λi, 3∕λi].
First, by (6) or (7), we know for any T ≤ T ≤ T + ∆, E ](w(i) - (w*)(i)) ]	≥
exp(-6)C1τ σ2∕(2λiT) just by the first term. When we choose C1 to be large enough the con-
tribution to function value by this direction alone is larger than τσ2∕T. Therefore every query in
[T, T + ∆] is still bad.
We will consider two cases based on the value of S2 := PT=△+1 ηT.
If S2 ≤ C2τ∕(λi2T) (where C2 is a large enough universal constant chosen later), then by Cauchy-
Schwartz we know
T+△
S2 ∙ ∆ ≥ ( X ηT)2 ≥ ι∕λ2.
〜
T=T +1
Therefore ∆ ≥ T∕C2τ, and we are done.
If S2 > C2τ∕(λi2T), by Equation (6) and (7) we know
E
,	21	T+△	/	T+△	∖
wT+δ -	(w*)(i))	≥	σ2	X	ηT eχp I	-2λi	X	ηj	I
'	T=T+1	∖	j=T+ι )
T+△
≥ exp(-6)σ2 X ηT ≥ exp(-6) ∙ C2τσ2∕(λ2T).
~
T=T +1
17
Under review as a conference paper at ICLR 2019
Here the first inequality just uses the second term in Equation (6) or (7), the second inequality is
because PT=+Δ+i ηj ≤ PT=+T+ι ηj ≤ 3∕λi and the last inequality is just based on the value of S2.
In this case as we can see as long as C2 is large enough, T + ∆ is also a point with E [f (wT +∆)] ≥
λiE (WT+δ - (w*)(i)) ] ≥ C1τσ2∕(T + ∆), so we can repeat the argument there. Eventually
we either stop because we hit case 1: S2 ≤ C2τ∕λi2T or the case 2 S2 > C2τ∕λi2T happened more
than T∕C2τ times. In either case We know for any T ∈ [T, (1 + ∖∕C2)T] E [f (w^)] ≥ τσ2∕T ≥
τσ2∕T as the lemma claimed.	□
Theorem 6 is an immediate corollary of Theorem 5 and Lemma 8.
Proof of Theorem 7. This result follows by running the constant and cut scheme for a fixed time
horizon T and then increasing the time horizon to K ∙ T. The learning rate of the initial phase for the
new T0 = κ ∙ T is 1∕μT0 = 1∕μ ∙ κT = 1∕LT which is the final learning rate for time horizon T.
Theorem 2 will then directly imply the current theorem.	□
D Details of experimental setup
D. 1 Synthetic 2-d Quadratic Experiments
As mentioned in the main paper, we consider three condition numbers namely κ ∈ {50, 100, 200}.
We run all experiments for a total of 200κ iterations. The two eigenvalues of the Hessian are κ and
1 respectively, and noise level is σ2 = 1 and we average our results with two random seeds. All our
grid search results are conducted on a 10 × 10 grid of learning rates × decay factor and whenever a
best run lands at the edge of the grid, the grid is extended so that we have the best run in the interior
of the gridsearch.
For the O(1∕t) learning rate, we search for decay parameter over 10-points logarithmically spaced
between {1∕(100κ), 3000∕κ}. The starting learning rate is searched over 10 points logarithmically
spaced between {1∕(20κ), 1000∕κ}.
For the O(1∕ʌ/(t)) learning rate, the decay parameter is searched over 10 logarithmically spaced
points between {100∕κ, 200000.0∕κ}. The starting learning rate is searched between {0.01, 2}.
For the exponential learning rate schemes, the decay parameter is searched between
{exp (-2∕N), exp (-106∕N)}. The learning rate is searched between {1∕5000, 1∕10}.
D.2 Non-Convex experiments on cifar- 1 0 dataset with a 44-layer residual net
As mentioned in the main paper, for all the experiments, we use the Nesterov’s Accelerated gradient
method (Nesterov, 1983) implemented in pytorch 12 with a momentum set to 0.9 and batchsize set
to 128, total number of training epochs set to 100, `2 regularization set to 0.0005.
With regards to learning rates, we consider 10-values geometrically spaced as {1,0.6,…，0.01}.
To set the decay factor for any of the schemes such as 1,2, and 3, we use the following rule. Suppose
we have a desired learning rate that we wish to use towards the end of the optimization (say, some-
thing that is 100 times lower than the starting learning rate, which is a reasonable estimate of what is
typically employed in practice), this can be used to obtain a decay factor for the corresponding decay
scheme. In our case, we found it advantageous to use an additively spaced grid for the learning rate
γt, i.e., one which is searched over a range {0.0001,0.0002,…，0.0009,0.001,…，0.009} at the
80th epoch, and cap off the minimum possible learning rate to be used to be 0.0001 to ensure that
there is progress made by the optimization routine. For any of the experiments that yield the best
performing gridsearch parameter that falls at the edge of the grid, we extend the grid to ensure that
the finally chosen hyperparameter lies in the interior of the grid. All our gridsearches are run such
that we separate a tenth of the training dataset as a validation set and train on the remaining 9∕10th
dataset. Once the best grid search parameter is chosen, we train on the entire training dataset and
12https://github.com/pytorch
18
Under review as a conference paper at ICLR 2019
evaluate on the test dataset and present the result of the final model (instead of choosing the best
possible model found during the course of optimization).
19