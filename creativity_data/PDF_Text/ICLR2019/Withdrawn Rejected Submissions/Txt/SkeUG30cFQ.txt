Under review as a conference paper at ICLR 2019
The Expressive Power of Deep Neural Networks
with Circulant Matrices
Anonymous authors
Paper under double-blind review
Ab stract
Recent results from linear algebra stating that any matrix can be decomposed into products
of diagonal and circulant matrices has lead to the design of compact deep neural network
architectures that perform well in practice. In this paper, we bridge the gap between these
good empirical results and the theoretical approximation capabilities of Deep diagonal-
circulant ReLU networks. More precisely, we first demonstrate that a Deep diagonal-
circulant ReLU networks of bounded width and small depth can approximate a deep ReLU
network in which the dense matrices are of low rank. Based on this result, we provide new
bounds on the expressive power and universal approximativeness of this type of networks.
We support our experimental results with thorough experiments on a large, real world
video classification problem.
1	Introduction
Recent progress in deep neural networks came at the cost of an important increase of model sizes. Nowadays,
state-of-the-art architectures for common tasks such as object recognition typically have tens of millions of
parameters (He et al., 2016) and up to a billion parameters in some cases (Dean et al., 2012). Best performing
(ensemble) models typically combine dozens of such models, and their size can quickly add up to ten or
twenty gigabytes. Large models are often more accurate, but training them requires time and large amounts
of computational resources. Even when they are trained, they remain difficult to deploy, especially on mobile
devices where memory or computational power is limited.
In linear algebra, itis common to exploit structural properties of matrices to speedup computations, or reduce
memory usage. Cheng et al. (2015) have applied this principle in the context of deep neural networks, and
proposed a network architecture in which large unstructured weight matrices have been replaced with more
compact matrices with a circulant structure. Since any n-by-n circulant matrix can be represented in memory
using only a vector of dimension n, the change resulted in a drastic reduction of the model size (from 230MB
to 21MB). Furthermore, Cheng et al. have shown empirically that their network architecture can be almost
as accurate as the original network.
Moczulski et al. (2015) have proposed a more principled approach leveraging a result by Huhtanen &
Peramaki (2015) stating that any matrix A ∈ Cn×n can be decomposed into 2n - 1 diagonal and Cir-
culant matrices. They use this result to design Deep diagonal-circulant ReLU networks. However their
experiments show good results even with a small number of factors (down to 2 factors), suggesting that
Deep diagonal-circulant ReLU networks can achieve good approximation error, even with few factors.
In this paper, we bridge the gap between the good empirical results observed by Moczulski et al. (2015), and
the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. We prove that Deep
diagonal-circulant ReLU networks with bounded width and small depth can approximate any dense neural
network. We obtain this result by showing that any matrix A can be decomposed into 4k + 1 diagonal and
1
Under review as a conference paper at ICLR 2019
circulant matrices where k is the rank of the matrix A. In practice, this result is more useful than the one
by HUhtanen & Peramaki since one can rely on a low rank SVD decomposition of A while controlling the
approximation error.
In addition to this theoretical contribUtion, we also condUct thoroUgh experiments on synthetic and real
datasets. In accordance with the theory, oUr experiments demonstrate that we can easily tradeoff accUracy
for model size by adjUsting the nUmber of factors in the matrix decomposition. Finally we evalUate the
applicability of this approach on state-of-the-art neUral network architectUres trained for video classification
on the YoUtUbe-8m Video dataset (over 1TB of training data). This experiment demonstrates that Deep
diagonal-circUlant ReLU networks can be Used to train more compact neUral networks on large scale, real
world scenarios.
2	Related Work
A variety of techniqUes have been proposed to bUild more compact deep learning models. A first category of
techniqUes aims at compressing a trained network into a smaller model, withoUt compromising the accUracy.
For example model distillation (Hinton et al., 2015) is two step training procedUre: first a large model
is trained to be as accUrate as possible, second a more compact model is trained to approximate the first
one. Other approaches have focUsed on compressing the network by redUcing the memory, at the level of
individUal weights, (for example, Using weight qUantization (Han et al., 2016) or parameter prUning) or at
the level of weight matrices, Using low rank decomposition of the original weight matrix (Sainath et al.,
2013) or Using sparse representations (Collins & Kohli, 2014; Dai et al., 2018; LiU et al., 2015).
Instead of compressing the network a posteriori, several researchers have focUsed on designing models
that are compact by design. This approach has several benefits, bUt most importantly, it redUces memory
footprint, both reqUired dUring training and inference. Chen et al. (2015) have proposed to compress weight
matrices by Using hashing fUnctions to map several matrix coefficients into the same memory cell. The
techniqUes works well in theory, bUt sUffers from poor performance on modern GPU devices dUe to irregUlar
access patterns.
In their paper, Cheng et al. (2015) observed that fUlly connected layers (which typically occUpy 90%1. of
the total nUmber of weights) are often Used to perform simple dimensionality redUction operation between
layers of different dimension. The idea of replacing large weight matrices from fUlly connected layers with
more compact circulant layers comes from a result by Hinrichs & Vyblral (2011) that have demonstrated
that circUlant matrices can be Use to approximate the Johson-LindenstraUss transform, often Used to perform
dimensionality reduction. Building on this result Cheng et al. proposed to replace the weight matrix of a
fully connected layer by a circulant matrix initialized with random weights. The resulting models achieve
good accuracy, with the random circulant matrix, but even better when the weights of the circulant matrix
are trained with the rest of the network using a gradient based optimization algorithm. This suggests that
such layers, often perform more than simple random projections, and that more expressive fully connected
layers are beneficial to the overall accuracy of the model.
Fortunately, more general linear transforms can also be described using circulant matrices or other structured
matrices, at the cost of using more of them. MUller-Quade et al. (1998) and Schmid et al. (2000) have
demonstrated this formally by showing that any matrix can be decomposed into the product of diagonal
and circulant matrices, and Moczulski et al. (2015) have proposed a compact neural network architecture
based on this decomposition that exhibit good accuracy in practice. Other researchers have investigated
using alternative structures such as Toeplitz (Sindhwani et al., 2015), Vandermonde (Sindhwani et al., 2015)
or Fastfood transforms (Yang et al., 2015). Despite demonstrating good empirical results, there have been
1In network such as AlexNet, the last 3 fully connected layers use 58M out of the 62M total trainable parameters.
2
Under review as a conference paper at ICLR 2019
little theoretical insight to explain the good approximation capabilities of deep neural networks based on
structured matrices.
Barron (1993) presented the universal approximation theorem which states that any neural network with
at least 1 hidden layer and sigmoid non linearity can approximate any function. However, the theorem by
Barron (1993) does not bound the width of the neural network and does not consider the training procedure.
Since then, substantial theoretical work has been done to evaluate the expressiveness of a neural network
as a function of the width (i.e. the number of neurons) and the depth of the network Arora et al. (2018);
Mhaskar et al. (2017); Lin et al. (2017); Poole et al. (2016); Raghu et al. (2016); Telgarsky (2016); Mhaskar
& Poggio (2016). In 2000, Hanin (2017) have investigated the approximation capabilities of neural networks
with ReLU activations and demonstrated that such networks can approximate any function.
More recently, Zhao et al. (2017) have provided a theoretical study of Deep diagonal-circulant ReLU net-
works and demonstrated that 2-layers networks of unbounded width are universal approximators. However,
these results are of limited interest because the networks used in practice are of bounded width. Unfortu-
nately, nothing is known about the theoretical properties of Deep diagonal-circulant ReLU networks in this
case.
3	Building compact deep neural networks using circulant matrices
3.1	Preliminaries on circulant matrices
A n-by-n circulant matrix C is a special kind of Toeplitz matrix where each row is a cyclic right shift of the
previous one as illustrated below.
C = circ(c)
c0	cn-1	cn-2
c1	c0	cn-1
c2	c1	c0
cn-1	cn-2	cn-3
c1
c2
c3
.
.
.
c0
Despite their rigorous structure, circulant matrices are expressive enough to model a variety of linear trans-
forms such as random projections (Hinrichs & Vyblral, 2011) and when they are combined together with
diagonal matrices, they can be used to represent an arbitrary transform (Schmid et al., 2000).
Circulant matrices also exhibit several properties that are interesting from a computational perspective. First,
a circulant n-by-n matrix C can be represented using only n coefficients. Thus, it is far more compact that
a full matrix that requires n2 coefficient. Second, the product between a circulant matrix C and a vector x
can be simplified to a simple element-wise product between the vector c and x in the Fourier domain (which
is generally performed efficiently on GPU devices). This results in a complexity reduced from O(n2) to
O(nlog(n)).
In their paper, Huhtanen & Peramaki (2015) have demonstrated that any matrix A ∈ Cn×n can be approxi-
mated with an arbitrary precision by a product of circulant and diagonal matrices:
Theorem 1. (Huhtanen & Peramaki, 2015) For any g^ven matrix A ∈ Cn×n ,let P be the smallest integer
such that A = ip=1 DiSi-1 where D1 . . . Dp are diagonal matrices. Then for any > 0, for any matrix
norm ∣∣∙k, there exists a Sequence of matrices Bi... B2n-1 where Bi is a circulant matrix if i is odd, and a
diagonal matrix otherwise, such that kB1B2 . . . B2n-1 - Ak < , and where S = circ(0, 1, 0, . . . , 0)
Because of their interesting properties, several researchers have considered circulant matrices as a replace-
ment from full weight matrices inside neural networks.
3
Under review as a conference paper at ICLR 2019
3.2	Theoretical Properties of Deep diagonal-circulant ReLU networks
There has already been some recent theoretical work on Deep diagonal-circulant ReLU networks, in which
2-layer networks of unbounded width where shown to be universal approximators. These results are of
limited interest, because the networks used in practice are of bounded width. Unfortunately, nothing is
known about the theoretical properties of Deep diagonal-circulant ReLU networks in this case. In particular,
the following questions remained unanswered up to now: Are Deep diagonal-circulant ReLU networks
with bounded width universal approximators? What kind of functions can Deep diagonal-circulant ReLU
networks with bounded-width and small depth approximate?
In this section, we first define formally diagonal-circulant ReLU networks, and then provide a theoretical
analysis of their approximation capabilities.
Definition 1 (Deep ReLU networks). With ReLU (x) = max(0, x), let fA,b(x) = ReLU (Ax + b) for any
matrices A ∈ Cn×n, and any b ∈ Rn. A Deep ReLU network is a function fAl,bl ◦ . . . ◦ fA1,b1, where
A1 . . . Al are arbitrary n × n matrices and b1 . . . bl ∈ Cn and where l and n are the depth and the width of
the network respectively.
As in Moczulski et al. (2015), Deep diagonal-circulant ReLU networks can be defined as follows:
Definition 2 (Deep diagonal-circulant ReLU networks). A Deep diagonal-circulant ReLU network is a
function fDlCl,bl ◦ . . . ◦ fD1C1,b1 where D1 . . . Dl ∈ Cn×n are diagonal matrices, C1 . . . Cl ∈ Cn×n
are circulant matrices, and where l and n are the depth and the width of the network respectively.
To show that bounded-width Deep diagonal-circulant ReLU networks are universal approximators, we first
need a proposition relating standard deep neural networks to Deep diagonal-circulant ReLU networks.
Proposition 2. Let N : Rn → Rn be a deep ReLU networks of width n and depth l, and let X ⊂ Rn be
a compact set. For any > 0, there exists a deep diagonal-circulant ReLU network N0 of width n and of
depth (2n - 1)l such that kN (x) - N0(x)k2 < for all x ∈ X.
We can now state the universal approximation corrolary:
Corrolary 1. Bounded depth Deep diagonal-circulant ReLU networks are universal approximators on any
compact set X.
Proof. Proposition 2 shows that bounded-width Deep diagonal-circulant ReLU networks can approximate
any Deep ReLU network. It has been shown recently in Hanin (2017) that bounded-width deep ReLU
networks are universal approximators. Together, these two results concludes the proof.	□
It is important to remark that Deep diagonal-circulant ReLU networks are not necessarily more compact than
Deep ReLU networks. Indeed, consider a n-wide Deep ReLU network with l layers having ln2 weights. The
previous corollary tells us that this network can be decomposed in a Deep ReLU networks involving l(2n-1)
matrices, i.e. 2ln(2n - 1) weights.
Despite the lack of theoretical guarantees a number of work provided empirical evidence that bounded
width and small depth Deep diagonal-circulant ReLU networks result in good performance (e.g. Moczulski
et al. (2015); Araujo et al. (2018); Cheng et al. (2015)). The following theorem studies the approximation
properties of these small depth networks.
Proposition 3. Let N : fAl ,bl ◦ . . . ◦ fA1 ,b1 be a deep ReLU network of width n and depth l, such that
each matrix Ai is of rank ki, where ki divides n. Let X ⊂ Rn be a compact set. For any > 0, there
exists a deep diagonal-circulant ReLU network N0 of width n and of depth ( in=1 (4ki + 1)) l such that
kN (x) - N0(x)k2 < for all x ∈ X.
4
Under review as a conference paper at ICLR 2019
This result generalizes Proposition 2, showing that a Deep diagonal-circulant ReLU networks of bounded
width and small depth can approximate a deep ReLU network in which the dense matrices are of low rank.
Note in the proposition, we require that ki divides n. We conjecture that the proposition holds even without
this condition, but we were not able to prove it.
Finally, what if we choose to use small depth network to approximate deep ReLU networks where matrices
are not of low rank ? To answer this question, we first need to show the negative impact of replacing matrices
by their low rank approximators in neural networks:
Proposition 4. Let N = fAl,bl ◦ . . . ◦ fA1,b1 be a deep ReLU network, where Ai ∈ Cn×n, bi ∈ Cn for all
i ∈ [l]. Let Ai be the matrix obtained by a SVD approximation of rank k of matrix Ai. Let σ%,j is the jth
singular value of Ai. Define N = fA. ◦…◦ fw m.Then,forany X ∈ Cn, we have ∣∣N (x) -N(X)Il ≤
(l
(σmax,1
-1)Rσmax,k
σmax,1 -1
maxi σi,j.
where R is an upper bound on norm of the output of any layer in N, and σmax,j
Basically, this proposition shows that we can approximate matrices in a neural network by low rank matrices,
and control the approximation error. In general, the term σml ax,1 could seem large, but in practice, it is likely
that most singular values in deep neural network are small in order to avoid divergent behaviors. We can
now prove the result on Deep diagonal-circulant ReLU networks:
Corrolary 2. Consider any deep ReLU network N = fAl,bl ◦ . . . ◦ fA1,b1 of depth l and width n. Let
σmax,j = maxi σi,j where σi,j is the j th singular value of Ai. Let X ⊂ Rn be a compact set. For any k
dividing n, there exists a deep diagonal-circulant ReLU network N0 = fDmCm,b0 ◦ . . . ◦ fD1C1,b0 of width
n and of depth m = 4(k + 1)n, such that for any X ∈ X, kN (x) — N 0 (x)k < (σmaσ1~I)R；max,k, where
R is an upper bound on the norm of the outputs of each layer in N.
Proof. Let N=于4回◦... ◦ f^ ,比，where each Ai is the matrix obtained by a SVD approximation of rank
k of matrix Ai .
With proposition 4, we have an error bound on ∣∣N (x) - N (x) 11. Now each matrix Ai can be replaced by a
product of k diagonal-circulant matrices. By lemma 1, this product yields a Deep diagonal-circulant ReLU
networks of depth m = 4(k +1)n, strictly equivalent to N on X. The result follows.	□ 4
4 Empirical evaluation
The experiments that we present in this section aim at answering the following questions. First question:
what is the impact of increasing the number of diagonal-circulant factors on the accuracy of the network?
To answer this question, we conduct a series of experiments on a synthetic classification dataset with an
increasing number of factors. As we will show, the results match our theoretical analysis from Section 3.
Second question: can this approach be useful to build more compact models in the context of large scale real-
world machine learning applications. To answer this second question, we build a deep diagonal-circulant
neural network architecture for video classification. The architecture is based on state-of-the-art architecture
initially proposed by Abu-El-Haija et al. (2016b) and later improved by Miech et al. (2017) in involve several
large layers that can be made more compact using circulant matrices as done in Araujo et al. (2018). As we
will show, the approach demonstrate good accuracy and can be used to build a more compact network than
the original one.
5
Under review as a conference paper at ICLR 2019
4.1	Impact of the number of diagonal-circulant factors on accuracy
Experimental setup The dataset is generated using the make classification2 function from Scikit-
Learn (Pedregosa et al., 2011). It is made of 10000 examples, 5 variables, 2 classes and 2 clusters for
each class. We train a neural network with 3 hidden layers of 1024 neurons each. We used a batch size of
50, a learning rate of 5 × 10-2, a learning rate decay of 0.9 every 10 000 examples. We compare the dense
neural network with a Deep diagonal-circulant ReLU networks with several factors. We use the initialization
proposed in Moczulski et al. (2015).
#Factors	#Params	Compress. Rate(%)	Loss
Dense	2107397	-	0.16016
k=2	19461	99.0	0.38847
k=4	35 845	98.2	0.36668
k=8	68 613	96.7	0.33275
k=16	134149	93.6	0.32798
k=32	523 269	75.1	0.32657
Table 1: This table shows the loss obtain on the synthetic dataset given the number of factor used for each
layer.
Results Table 4.1 shows the loss of the dense architecture versus the Deep diagonal-circulant ReLU net-
works with different factors. The table also shows the compression rate obtain with the Deep diagonal-
circulant ReLU networks. We notice that the Deep diagonal-circulant ReLU networks manage to achieve
more than 90% compression rate with a substantial loss in accuracy with factor up to 16. Adding factors
improve the accuracy but make the convergence difficult. We were note able to train a model with more than
32 layers. A solution would be to use the circulant-diagonal ReLU decomposition only on certain layer in
order to trade-off compression with accuracy more precisely.
4.2	Deep diagonal-circulant ReLU networks for large-scale video classification
In this section, we demonstrate the applicability of diagonal-circulant ReLU networks in the context of a
large scale video classification architecture trained on the Youtube-8M dataset. Our architecture is based on
a state-of-the-art architecture that was initially proposed by Abu-El-Haija et al. (2016b) and later improved
by Miech et al. (2017).
4.2	. 1 Experimental Settings
Dataset The dataset is composed of an embedding (each video and audio frames are represented by a
vector of respectively 1024 and 128) of video and audio frames extracted every 1 seconds with up to 300
frames per video.
Model Architecture This architecture can be decomposed into three blocks of layers, as illustrated in
Figure 4.1. The first block of layers, composed of the Deep Bag-of-Frames embedding, is meant to make
an embedding of these frames in order to make a simple representation of each video. The first block
of layers, composed of the Deep Bag-of-Frames embedding, is meant to process audio and video frames
2http://SCikit-learn.org/stable/modules/generated/skleam.datasets.make_classification.html
6
Under review as a conference paper at ICLR 2019
Dim Reduction
Classification
Embedding
FC I——
Concat
Fn —
MoE
Context
Gating
Figure 4.1: This figure shows the architecture used for the training of the YouTube-8M dataset.
independently. The DBoF layer computes two embeddings: one for the audio and one for the video. In
the next paragraph, we will only focus on the describing the video embedding. (The audio embedding is
computed in a very similar way.) A second block of layers reduces the dimensionality of the output of the
embedding and merges the resulting output with a concatenation operation. Finally, the classification block
uses a combination of Mixtures-of-Experts (MoE) Jordan & Jacobs (1993); Abu-El-Haija et al. (2016a) and
Context Gating Miech et al. (2017) to calculate the final probabilities.
Experiment We want to compare the effect on the circulant-diagonal ReLU decomposition only on cer-
tain layer to evaluate the trade-off between compression rate and accuracy. First, we train the architecture
presented in Figure 4.1 without any circulant matrices to serve as a baseline. Then, we used the circulant-
diagonal decomposition on each layer independently.
Hyper-parameters All our experiments are developed with TensorFlow Framework Abadi et al. (2015).
We trained our models with the CrossEntropy loss and used Adam optimizer with a 0.0002 learning rate and
a 0.8 exponential decay every 4 million examples. We used a fully connected layer of size 8192 for the video
DBoF and 4096 for the audio. The fully connected layers used for dimensionality reduction have a size of
512 neurons. We used 4 mixtures for the MoE Layer.
Evaluation Metric We used the GAP (Global Average Precision), as used in Abu-El-Haija et al. (2016b),
to compare our experiments.
4.2.2 Results
This series of experiments aims at understanding the effect of circulant-diagonal ReLU decomposition over
different layers with 1 factors. Table 2 shows the result in terms of number of weights, size of the model
(MB) and GAP. We also compute the compression ratio with respect to the dense model. The compact fully
connected layer achieves a compression rate of 9.5 while having a very similar performance, whereas the
compact DBoF and MoE achieve a higher compression rate at the expense of accuracy. Figure 4.2 shows
that the model with a compact FC converges faster than the dense model. The model with a compact DBoF
shows a big variance over the validation GAP which can be associated with a difficulty to train. The model
with a compact MoE is more stable but at the expense of its performance.
7
Under review as a conference paper at ICLR 2019
Baseline Model	#Weights	Size (MB)	Compress. Rate(%)	GAP@20	Diff.
Dense Model	45 359764	173	-	0.846	-
Compact DBoF	36987 540	141	18.4	0.838	-0.008
Compact FC	41181844	157	9.2	0.845	-0.001
Compact MoE	12668 504	48	72.0	0.805	-0.041
Table 2: This table shows the effect of circulant-diagonal decomposition on different layers.
Comparison of the effect of compactness over
different layers with the base model
Figure 4.2: Validation GAP according to the number of epochs for different compact models.
5 Conclusions
In this paper we provided a theoretical study of the properties of Deep diagonal-circulant ReLU networks
and demonstrated that they are bounded width universal approximators. The bound on this decomposition
allowed us to calculate the error bound on any Deep diagonal-circulant ReLU networks given the depth on
the network and the singular values associated with the weight matrices. Our empirical study demonstrate
that we can trade-off model size for accuracy in accordance with the theory, and that we can use Deep
diagonal-circulant ReLU networks in large scale machine learning applications.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay
Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL
https://www.tensorflow.org/. Software available from tensorflow.org.
Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Apostol (Paul) Natsev, George Toderici, Balakrishnan
Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification bench-
8
Under review as a conference paper at ICLR 2019
mark. In arXiv:1609.08675, 2016a. URL https://arxiv.org/pdf/1609.08675v1.pdf.
Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan,
and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. arXiv
preprint arXiv:1609.08675, 2016b.
Alexandre Araujo, Benjamin Negrevergne, Yann Chevaleyre, and Jamal Atif. Training compact deep learn-
ing models for video classification using circulant matrices. In The 2nd Workshop on YouTube-8M Large-
Scale Video Understanding at ECCV 2018, 2018.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks
with rectified linear units. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=B1J_rgWRW.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930-945, 1993. URL http://pages.cs.wisc.edu/
~brecht∕cs838docs∕93.Barron.Universal.pdf.
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing neural
networks with the hashing trick. In Proceedings of the 32Nd International Conference on International
Conference on Machine Learning - Volume 37, ICML’15, pp. 2285-2294. JMLR.org, 2015. URL http:
//dl.acm.org/citation.cfm?id=3045118.3045361.
Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S. F. Chang. An exploration of parameter
redundancy in deep networks with circulant projections. In 2015 IEEE International Conference on
Computer Vision (ICCV), pp. 2857-2865, Dec 2015.
Maxwell D. Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. CoRR,
abs/1412.1442, 2014.
Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational
information bottleneck. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1143-
1152, Stockholmsmassan, StockhoImSWeden,10-15Jul2018. PMLR. URL http://Proceedings.
mlr.press/v80/dai18d.html.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc aurelio Ranzato,
AndreW Senior, Paul Tucker, Ke Yang, Quoc V. Le, and AndreW Y. Ng. Large scale distributed deep
netWorks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural
Information Processing Systems 25, pp. 1223-1231. Curran Associates, Inc., 2012. URL http://
papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural netWorks With
pruning, trained quantization and huffman coding. International Conference on Learning Representations
(ICLR), 2016.
B. Hanin. Universal Function Approximation by Deep Neural Nets With Bounded Width and ReLU Activa-
tions. ArXiv e-prints, August 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, June 2016. doi: 10.1109/CVPR.2016.
90.
Aicke Hinrichs and Jan Vybiral. Johnson-IindenstraUss lemma for circulant matrices. Random Structures &
Algorithms, 39(3):391-398, 2011.
9
Under review as a conference paper at ICLR 2019
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS
Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/abs/
1503.02531.
Marko HUhtanen and Allan Peramaki. Factoring matrices into the product of CircUlant and diagonal matrices.
Journal ofFourier Analysis and Applications, 21(5):1018-1033, Oct 2015. ISSN 1531-5851. doi: 10.
1007/s00041-015-9395-0. URL https://doi.org/10.1007/s00041-015-9395-0.
M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. In Proceedings
of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 2, pp. 1339-
1344 vol.2, Oct 1993. doi: 10.1109/IJCNN.1993.716791.
Henry W. Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well? Journal
of Statistical Physics, 168(6):1223-1247, Sep 2017. ISSN 1572-9613. doi: 10.1007/s10955-017-1836-5.
URL https://doi.org/10.1007/s10955-017-1836-5.
Baoyuan Liu, Min Wang, H. Foroosh, M. Tappen, and M. Penksy. Sparse convolutional neural networks. In
2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 806-814, June 2015.
doi: 10.1109/CVPR.2015.7298681.
Hrushikesh Mhaskar, Qianli Liao, and Tomaso A Poggio. When and why are deep networks better than
shallow ones? In AAAI, pp. 2343-2349, 2017.
Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory perspec-
tive. Analysis and Applications, 14(06):829-848, 2016.
Antoine Miech, Ivan Laptev, and Josef Sivic. Learnable pooling with context gating for video classification.
CoRR, abs/1706.06905, 2017.
Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. Acdc: A structured efficient
linear layer. arXiv preprint arXiv:1511.05946, 2015.
Jorn MUller-Quade, Harald Aagedal, Th Beth, and Michael Schmid. Algorithmic design of diffractive optical
systems for information processing. Physica D: Nonlinear Phenomena, 120(1-2):196-205, 1998. URL
https://www.sciencedirect.com/science/article/pii/S0167278998000554.
F. Pedregosa, G. VaroqUaUx, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. DUboUrg, J. Vanderplas, A. Passos, D. CoUrnapeaU, M. BrUcher, M. Perrot, and E. DUchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.
Ben Poole, SUbhaneil Lahiri, Maithra RaghU, Jascha Sohl-Dickstein, and SUrya GangUli. Exponential ex-
pressivity in deep neUral networks throUgh transient chaos. In D. D. Lee, M. SUgiyama, U. V. LUxbUrg,
I. GUyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3360-3368.
CUrran Associates, Inc., 2016. URL https://arxiv.org/abs/1606.05340.
Maithra RaghU, Ben Poole, Jon Kleinberg, SUrya GangUli, and Jascha Sohl-Dickstein. On the expressive
power of deep neUral networks. arXiv preprint arXiv:1606.05336, 2016.
T. N. Sainath, B. KingsbUry, V. Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factor-
ization for deep neUral network training with high-dimensional oUtpUt targets. In 2013 IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing, pp. 6655-6659, May 2013. doi:
10.1109/ICASSP.2013.6638949.
10
Under review as a conference paper at ICLR 2019
Michael Schmid, Rainer Steinwandt, Jorn MUller-Quade, Martin Rotteler, and Thomas Beth. Decomposing
a matrix into circulant and diagonal factors. Linear Algebra and its Applications, 306(1-3):131-143,
2000. URL https://core.ac.uk/download/pdf/82406534.pdf.
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-
footprint deep learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 3088-
3096. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/
5869-structured-transforms-for-small-footprint-deep-learning.pdf.
Matus Telgarsky. Benefits of depth in neural networks. In COLT, 2016.
Z. Yang, M. Moczulski, M. Denil, N. d. Freitas, A. Smola, L. Song, and Z. Wang. Deep fried convnets.
In 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1476-1483, Dec 2015. doi:
10.1109/ICCV.2015.173.
Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, and Bo Yuan. Theoretical properties for neural
networks with weight matrices of low displacement rank. In Doina Precup and Yee Whye Teh (eds.),
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pp. 4082-4090, International Convention Centre, Sydney, Australia, 06-11
Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/zhao17b.html.
6 S upplemental Material: Technical Lemmas and Proofs
Lemma 1. Let Al , . . . A1 ∈ Cn×n, b ∈ Cn and let X ⊂ Rn be a compact set. There exists βl . . . β1 ∈ Cn
such that for all x ∈ X we have fAl,βl ◦ . . . ◦ fA1,β1 (x) = ReLU (AlAl-1 . . . A1x + b).
Proof. of lemma 1. Define Ω = maxχ∈χ,j∈[i] IQk=I AkxI . Define hj(x) = AjX + βj. Let βι = Ω1n
where 1n is the n-vector of ones. Clearly, for all x ∈ X we have h1(x) ≥ 0, so ReLU ◦ h1(x) = h1(x).
More generally, for all j < n - 1 define βj+ι = 1nΩ - Aj+ιβj. It is easy to see that for all j < n
we have hj ◦ ... ◦ hι(x) = AjAj-ι... Aix + 1nΩ. This garantees that for all j ‹ n, hj ◦ ... ◦ hι(x)=
ReLUohjQ.. .◦ ReLUohι(x). Finally, defineβι = b-Aιβι-ι . Wehave, ReLU◦hi◦.. .◦ReLU◦hi(x)=
ReLU (Aj ... Aix + b).	□
Proof. of proposition 2. Assume N = fAl,bl ◦. . .◦fA1,b1. By theorem 1, for any 0 > 0, any matrix Ai, there
exists a sequence of 2n-1 matrices Ci,nDi,n-iCi,n-i . . . Di,iCi,i such that Qjn=-0i Di,n-j Ci,n-j - Ai <
0, where Di,i is the identity matrix. By lemma 1, we know that there exists {βij}i∈[l] j∈[n] such that for all
i ∈ [l], fDinCin,βin ◦... ◦ fDi1Ci1,βi1 (x) = ReLU (DinCin... Ciix + bi).
Now if0 tends to zero, kfDinCin,βin ◦ . . . ◦ fDi1Ci1,βi1 - ReLU (Aix + bi)k will also tend to zero for any
x ∈ X, because the ReLU function is continuous and X is compact. LetN0 = fD1nC1n,β1n ◦. . .◦fDi1 Ci1,βi1 .
Again, because all functions are continuous, for all x ∈ X, kN (x) - N0(x)k tends to zero as 0 tends to
zero.	□
Proposition 5. Let A ∈ Cn×n a matrix of rank k. Assume that n can be divided by k. For any > 0,
there exists a sequence of4k + 1 matrices Bi, . . . , B4k+i, where Bi is a circulant matrix ifi is odd, and a
diagonal matrix otherwise, such that
11
Under review as a conference paper at ICLR 2019
4k+1
A- Y Bi
i=1
<
F
Proof. of proposition 5. Let UΣVT be the SVD decomposition of M where U, V and Σ are n × n matrices.
Because M is of rank k, the last n-k columns of U and V are null. In the following, we will first decompose
U into a product of matrices WRO, where R and O are respectively circulant and diagonal matrices, and
W is a matrix which will be further decomposed into a product of diagonal and circulant matrices. Then,
we will apply the same decomposition technique to V . Ultimately, we will get a product of 4k + 2 matrices
alternatively diagonal and circulant.
Let R = circ(r1 . . . rn). Let O be a n × n diagonal matrix where Oi,i = 1 if i ≤ k and 0 otherwise. The k
first columns of the product RO will be equal to that of R, and the n - k last colomns of RO will be zeros.
For example, if k = 2, we have:
/	ri	Irn	0	…0	∖
r	ri
..
RO =Ir3	r2	.	.
..
..
..
∖ rn	rn-1	0	…0 /
Let us define k diagonal matrices Di = diag(dii . . . din) for i ∈ [k]. For now, the values of dij are unknown,
but we will show how to compute them. Let W = Pik=i DiSi-i. Note that the n - k last columns of the
product WRO will be zeros. For example, with k = 2, we have:
-di,i
d2,2	d1,2
W =	d2,3
d2,i
d2,n	di,n
/	rιdii + rnd21	rndii + rn-id21	0
r2d12 + r1d22	ri di2 + rnd22
.
WRO =	.
0
rndin + rn-id2n	rn-idin +rn-2d2n	0
0
We want to find the values of dij such that WRO = U. We can formulate this as linear equation system. In
case k = 2, we get:
12
Under review as a conference paper at ICLR 2019
∖
..
×
d2,1
d1,1
d2,2
dl,2
d2,3
dl,3
U1,1
U1,2
U2,1
U2,2
..
∖ /
The ith bloc of the bloc-diagonal matrix is a Toeplitz matrix induced by a subsequence of length k of
(r1 , . . . rn , r1 . . . rn). Set rj = 1 for all j ∈ {k, 2k, 3k, . . . n} and set rj = 0 for all other values of j. Then
it is easy to see that each bloc is a permutation of the identity matrix. Thus, all blocs are invertible. This
entails that the block diagonal matrix above is also invertible. So by solving this set of linear equations, we
find d1,1 . . . dk,n such that WRO = U. We can apply the same idea to factorize V = W 0.R.O for some
matrix W 0 . Finally, we get
A = UΣVT = WROΣOTRTW0T
Thanks to Theorem 1, W and W0 can both be factorized in a product of 2k - 1 circulant and diagonal
matrices. Note that OΣOT is diagonal, because all three are diagonal. Overall, A can be represented with a
product of 4k + 2 matrices, alternatively diagonal and circulant.	□
Proof. of proposition 3 By proposition 5, each low rank matrix of the neural net can be decomposed in a
small number of diagonal and circulant matrices. By lemma 1, the matrices can be connected to form a
neural net.	□
Proof. of proposition 4 Let xo ∈ Cn and Xo = x0. For all i ∈ [l], define Xi = ReLU (Aixi-I + b) and
Xi = ReLU (AiXi-I + b) .By lemma 2, We have
IlXi - XiIl ≤ σi,k+ι ∣∣Xi-ik + σi,ι ∣∣Xi-i - Xi-Ik
Observe that for any sequence ao, a1 . . . defined reccurently by ao = 0 and ai = rai-1 + s, the reccurence
s(ri -1)
relation can be unfold as follows: a% = [-] '. We can apply this formula to bound our error as follows
IIXl-XlIl ≤
(σmax,1-1)σmax,k maxikxik
□
σmax,1 -1
Lemma 2. Let A ∈ Cn×n with singular values σι... σn,, and let x, X ∈ Cn. Let A be the matrix obtained
by a SVD approximation of rank k of matrix A. Then we have:
∣∣ReLU (AX + b) — ReLU (AX + b) ∣∣ ≤ σk+ι ∣∣x∣ + σι ∣∣X — Xk
Proof. Recall that ∣∣A∣∣2 = SuPz 窄沪=σι = ∣∣A∣∣2, because σι is the greatest singular value of both A
and A. Also, note that ∣∣A - A∣∣? = σk+1. Let us bound the formula without ReLUs:
13
Under review as a conference paper at ICLR 2019
Il(Ax + b) - (Ax + b)∣∣ = I I (Ax + b) - (Ax + b) ∣ ∣
=∣∣ Ax — AX — A(X — x) ∣∣
≤ ∣∣(a-A)x∣∣+1∣A∣∣2l∣x-Xk
≤ IlXIl σk+ι + σι ∣∣x - x∣∣
Finally, it is easy to see that for any pair of vectors α,b ∈ Cn, we have ∣ReLU (a) - ReLU (b)∣ ≤ ∣∣a - b∣.
This concludes the proof.	□
14