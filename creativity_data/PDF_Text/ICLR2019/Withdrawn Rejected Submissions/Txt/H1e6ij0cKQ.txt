Under review as a conference paper at ICLR 2019
Efficient Sequence Labeling with Actor-
Critic Training
Anonymous authors
Paper under double-blind review
Ab stract
Neural approaches to sequence labeling often use a Conditional Random Field
(CRF) to model their output dependencies, while Recurrent Neural Networks
(RNN) are used for the same purpose in other tasks. We set out to establish RNNs
as an attractive alternative to CRFs for sequence labeling. To do so, we address
one of the RNN’s most prominent shortcomings, the fact that it is not exposed
to its own errors with the maximum-likelihood training. We frame the prediction
of the output sequence as a sequential decision-making process, where we train
the network with an adjusted actor-critic algorithm (AC-RNN). We comprehen-
sively compare this strategy with maximum-likelihood training for both RNNs
and CRFs on three structured-output tasks. The proposed AC-RNN efficiently
matches the performance of the CRF on NER and CCG tagging, and outperforms
it on Machine Transliteration. We also show that our training strategy is signifi-
cantly better than other techniques for addressing RNN’s exposure bias, such as
Scheduled Sampling, and Self-Critical policy training.
1	Introduction
Sequence labeling is a canonical structured output problem, with many techniques available to track
its chain-structured output dependencies. Conditional Random Fields (CRF) built over a neural
feature layer have recently emerged as a best practice for sequence labeling tasks in NLP (Huang
et al., 2015). Alternatively, one can track the same output dependencies using a Recurrent Neu-
ral Network (RNN) over the output sequence, similar to the decoder component in neural machine
translation (NMT). Using a decoder RNN instead of a CRF has several potential advantages, such
as simplified implementation, tracking longer dependencies, and allowing for larger output vocab-
ularies. However, shifting from the CRF’s sequence-level training objective to the decoder RNN’s
sequence of token-level objectives may lead to suboptimal performance due to exposure bias.
Exposure bias stems from the token-level maximum likelihood objective typically used in RNN
training, which does not expose the model to its own errors (Bengio et al., 2015). RNNs are typi-
cally conditioned on gold-standard contexts during training (a procedure known as Teacher Forcing
(Goodfellow et al., 2016)), while at test time, the model is conditioned on its own predictions, cre-
ating a train-test mismatch. As sequence-level models, CRFs are immune to exposure bias.
We set out to establish the decoder RNN as an attractive alternative to the CRF for sequence labeling.
We do so by adopting a simple and effective RNN training strategy to counter the exposure-bias
problem, and by providing an experimental comparison to demonstrate that our modified RNN can
match the CRF in accuracy, and surpass it in flexibility. Our chosen training method is Actor-
Critic reinforcement learning (Konda & Tsitsiklis, 2003), which we adapt to the sequence-labeling
scenario by providing immediate rewards after each output tag, and by letting those outputs adjust
the critic scores. Our complete system, dubbed as the AC-RNN, maintains the same neural feature
layer that has proven so successful for the CRF, changing only the output layer. We conduct a
comprehensive analysis comparing the AC-RNN to the CRF under controlled conditions using a
shared implementation. We demonstrate that on Named Entity Recognition (NER) and Combinatory
Categorical Grammar (CCG) supertagging, the AC-RNN can match the performance of the CRF,
while training more efficiently.
To demonstrate its flexibility, we also test our method on machine transliteration, a monotonic
transduction problem that straddles the boundary between sequence labeling and full sequence-to-
1
Under review as a conference paper at ICLR 2019
sequence modeling. Finally, we compare our method with previous techniques proposed to address
exposure bias. On NER tagging, we empirically demonstrate that AC-RNN is significantly better
than the RNN trained with scheduled sampling (Bengio et al., 2015). We also demonstrate that our
approach is more suitable for sequence labeling tasks than other similar policy-gradient methods
such as self-critical training of Rennie et al. (2017).
2	Prior Work
In sequence labeling, several neural methods have recently been shown to outperform earlier systems
that use hand-engineered features. For the tasks of POS tagging, chunking and NER, Huang et al.
(2015) apply a CRF output layer on top of a bi-directional RNN over the source. For the NER task,
Lample et al. (2016) extend the RNN layer with character-level RNNs that capture information about
word prefixes and suffixes. Ma & Hovy (2016) use Convolutional Neural Networks to a similar end.
We build upon Lample et al.’s approach, replacing their CRF with a decoder RNN trained with an
adjusted Actor-Critic objective.
We also apply the AC-RNN to CCG supertagging (Clark & Curran, 2004), where the model labels
each word in a sentence with one of 1,284 morphosyntactic categories from CCGbank (Hockenmaier
& Steedman, 2007). Bidirectional RNNs have been used in a number of recent supertagging systems
(Lewis et al., 2016; Xu, 2016; Vaswani et al., 2016; Kadari et al., 2017; Wu et al., 2017). Among
these, our effort is most similar to Vaswani et al. (2016), who also use an RNN decoder over a
bidirectional encoder; however, they address exposure bias with Scheduled Sampling.
We also consider transduction tasks, which go beyond sequence labeling by allowing many-to-many
monotonic alignments between the source and target symbols. In particular, we focus on translit-
eration, where the goal is to convert a word from a source script to a target script on the basis of
the word’s pronunciation. Many neural transliteration approaches follow the sequence-to-sequence
model originally proposed for NMT (Jadidinejad, 2016; Rosca & Breuel, 2016). Our AC-RNN
transliteration system is similar to these, but with an improved objective to address exposure bias.
Other approaches have been proposed to address exposure bias in RNN, especially for NMT. We re-
view the following major techniques: Scheduled Sampling, Reinforcement Learning, and Imitation
Learning. To our knowledge, none of these prior works has compared its method against CRF.
Scheduled Sampling (SS): Bengio et al. (2015) introduce the notion of scheduled sampling as the
decoder RNN is gradually exposed to its own errors, where a sampling probability is annealed at
every training epoch so that we use gold-standard inputs at the beginning of the training, but while
approaching the end, we instead condition the predictions on the model-generated inputs.
Reinforcement Learning (RL): Ranzato et al. (2016) apply the REINFORCE algorithm (Williams,
1992) to Neural Machine Translation (NMT), to train the network with a reward derived from the
BLEU score of each generated sequence. Bahdanau et al. (2017) apply the actor-critic algorithm in
NMT by applying a reward-reshaping approach to construct intermediate BLEU feedback at each
step. Rennie et al. (2017) introduce a Self-Critical (SC) training approach that does not require a
critic model, which has been shown to outperform REINFORCE in the image captioning task. This
method has also been applied to abstract summarization (Paulus et al., 2018).
Unlike these previous works that apply reinforcement-learning techniques to optimize an available
external metric such as ROUGE in text summarization, or BLEU in translation, giving one reward
at the end of each sequence, we demonstrate that sequence labeling tasks benefit from the binary
rewards that are available at each step. In addition, Bahdanau et al. (2017), and Paulus et al. (2018)
combine the Teacher Forcing maximum-likelihood objective with their proposed RL objectives,
which requires two forward computations in the decoder RNN, one for conditioning on the ground-
truth labels, another for the RL objective without conditioning on the ground-truth labels. In this
work, we will incorporate the supervision of the gold label into the actor-critic algorithm itself
without any extra computation. In contrast to Bahdanau et al. (2017), our method employs a simpler
critic architecture, without any schedules to pre-train the critic model.
Imitation Learning:
Another alternative method to address exposure bias would be imitation learning, where one has
access to a gold-standard policy instead of rewards. In the case of sequence labeling, this policy
2
Under review as a conference paper at ICLR 2019
corresponds to the gold-standard tag sequence, and most imitation-learning algorithms such as Dag-
ger (Ross et al., 2011) reduce to variants of Scheduled Sampling. Recently, the related technique of
learning to search (DaUme In et al., 2009) has been extended to neural network models by SEARNN
(Leblond et al., 2018), but that has been shown to perform slightly worse than the actor-critic training
for NMT.
3	Architecture
3.1	Encoder
Following Huang et al. (2015), the encoder of our sequence labeler employs a bi-directional RNN
over the tokens in the sequence. The bi-directional RNN transforms context-independent token rep-
resentations into representations of tokens-in-context, allowing each position to potentially encode
information from the entire input sequence.
For sequence labeling, we follow Lample et al. (2016), and build our context-independent word
representation by combining an embedding table with the outputs of a bi-directional RNN applied
to each word’s characters. The final states of the forward and backward character RNNs are con-
catenated to the word’s embedding, and then passed through a dropout layer. We also concatenate
capitalization pattern indicators to these feature vectors. Our word embeddings are initialized using
embeddings pre-trained on a large corpus.
For transliteration, where we operate exclusively on the character level, we apply a bi-directional
RNN on the character representations, which are provided by a randomly initialized embedding
table.
3.2	Decoders
Given an input X = (x1 , x2, ..., xl), we look for an output sequence Y = (y1 , y2, ..., yl) where
each yt is an output token. In the encoder (Section 3.1), we transform the input X into a sequence
of hidden vectors H = (h1, h2, ..., hl). Given these vectors, the simplest decoder does not account
for output dependencies at all. Instead, it independently predicts the output at time t by mapping ht
into a probability distribution pINDP(yt|ht) using a softmax layer. This results in a sequence-level
probability of p(Y |X) = QtpINDP(yt|ht).
In sequence labeling tasks, there are typically dependencies between the output tokens; for example,
in English Part of Speech tagging, a determiner is unlikely to follow another determiner. The most
prominent and widely used approach to track such dependencies is the CRF, which uses dynamic
programming over an undirected graphical model to maintain a well-defined probability distribu-
tion over sequences, effectively modeling p(Y |X) = pCRF(Y |H), where pCRF hides fixed-order
Markov dependencies over Y . The CRF can be used as a node in a neural sequence labeler, while
still allowing the system to train end-to-end (Huang et al., 2015).
An alternative technique is to use a decoder RNN on top of the encoder, as is typically done in neural
machine translation (Sutskever et al., 2014), and has been done for CCG super-tagging (Vaswani
et al., 2016). Let dt be the recurrent decoder state, summarizing the output sequence up to time t, and
let ct be the context vector that summarizes the input X for time t. For sequence labeling, the source-
to-target alignment is trivial, and ct is provided directly by the encoder: ct = ht. We can then use the
input-feeding method of Luong et al. (2015) to define dt recursively: dt = RNN(dt-1, yt-1, ct-1),
where RNN is a recurrent unit, in our case, an LSTM (Hochreiter & Schmidhuber, 1997). Finally, a
softmax layer is used to define a probability distribution over output tokens pSM(yt|ct, dt), resulting
in a sequence model of: p(Y |X) = Qt pRNN(yt|ht, yt0<t) = QtpSM(yt|ct,dt). During training,
the gold-standard previous token yt-1 is fed into the decoder at time t, while at test time, we use the
model’s generated output.
For transliteration, where the input and output sequence lengths do not match, we can no longer
simply provide the encoder state ht at time t as the context vector ct for our probability models.
Following standard practice in NMT, for the decoder RNN, an attention mechanism (Bahdanau
et al., 2015) can learn an alignment model that provides a scalar score αt,t0 for each target position t
and source position t0, giving us a context vector ct = Pt0 αt,t0 ht0, which we can use in place of ht
3
Under review as a conference paper at ICLR 2019
Algorithm 1 adjusted Actor-Critic Training
•	Input: Source X, Target Y, and n as hyper-parameter
•	Greedy decode X using θ to get:
•	the output sequence Y = (yι,..., yι)
•	decoder RNN states D = (d1, ..., dl)
•	context vectors C = (c1 , ..., cl )
•	For each output target position t:
•	rt = 1 if yt = yt, 0 otherwise
•	Vθ0 (t) = CriticNetwork(dt, ct, θ0)
•	loss θ = 0; loss θ0 = 0
•	For each output target position t:
•	Gt = Pin=-01 [rt+i] +Vθ0(t+n)
•	δt = Gt - Vθ0 (t)
•	aδt = adjust(yt, yt, δt) × δt
•	lossθ = lossθ - aδtlnPθ(yt∣X,yto<t)
•	loss θ0 = loss θ0 + δt × δt
•	Back-propagate through lossθ as normal to update θ
•	Perform a semi-gradient step along loss θ0 to update θ0
in the RNN models described above. Specifically, we use the global-general attention mechanism
of Luong et al. (2015). To modify the CRF for transliteration, we must allow it to generate output
of a different length from its input. To do so, we pad both sequences with extra end symbols up to a
fixed maximum length, and let CRF decode until the end of the padded source sequence. It controls
its target length by outputting padding tokens.
All of the models described above can be trained with a maximum likelihood objective:
Jml (θ) = Pχ,γln Pθ(Y X)
4 Actor-Critic Training
We adopt the actor-critic algorithm (Sutton & Barto, 1998; Konda & Tsitsiklis, 2003; Mnih et al.,
2016) to fine-tune the decoder RNN. In AC training, the decoder RNN first generates a greedy
output sequence according to its current model, similar to how it would during testing. We calculate
a sequence-level credit (return) for each prediction by comparing it to the gold-standard. The AC
update modifies our RNN to improve credits at each step. This process exposes the decoder to its
own errors, alleviating exposure bias. Algorithm 1 provides pseudo code for the training process,
which we expand upon in the following paragraphs.
We define the token-level reward r as +1 if the generated token yt is the same as the gold token
yt, and as 0 otherwise. We compute the sequence-level credit Gt for each decoding step using the
multi-step Temporal Difference return (Sutton & Barto, 1998):
Gt = Pin=-01 [rt+i] +Vθ0(t+n)
The step count n allows us to control our bias-variance trade-off, with a large n resulting in less
bias but higher variance. The critic Vθ0 (t) is a regression model that estimates the expected return
E [Gt], taking the context vector ct and the decoder’s hidden state dt as input. It is trained jointly
alongside our decoder RNN, using a distinct optimizer (without back-propagating errors through ct
and dt). With this critic in place, the update for the AC algorithm is defined as
∂Jac(θ)
~∂θ~
d Iog(Pe(yt|X,yt0Vt))
∂θ
(δt)
t
where:
δt = Gt - Vθ0(t)
4
Under review as a conference paper at ICLR 2019
76.9 -'
φ
-+- output-adjusted actor-critic
actor-critic + ml
actor-critic
reinforce & baseline
reinforce
Figure 1: The adjusted actor-critic objective compared to alternative policy-gradient objectives on
German NER with 17 possible output tags. All objectives are maximized using Gradient Ascent
with a fixed step size of 0.5 for actor-critic, and 0.01 for REINFORCE objectives. For REINFORCE
objectives, we set n = l in the Temporal Difference credits (i.e. sum all rewards until the end of
sequence). All methods are trained 20 times with different random seeds using the same hyper-
parameters.
The AC update changes the prediction likelihood proportionally to the advantage δt of the token
yt. Therefore, if Gt > Vθ∣ (t), the decoder should increase the likelihood. The AC error Jdθ⑻
back-propagates only through the actor’s prediction likelihood pθ .
Critic Architecture: We employ a non-linear feed-forward neural network as our critic, which uses
leaky-ReLU activation functions (Nair & Hinton, 2010) in the first two hidden layers. In the output
layer, it uses a linear transformation to generate a scalar value. To learn the critic’s parameters θ0,
we use a semi-gradient update (Sutton & Barto, 1998). We do not use the full gradient in the Mean
Squared error to train this regression model. Accordingly, for dl∂θsθ0 = 久嚼0凡),instead of using
2δtd(Gt-Ver)), We use the update 2δtdV(). The Temporal Difference return Gt uses the critic's
estimate Vθ0 (t + n). The full gradient will cause a feedback loop as by doing so, we will allow Gt
to match With Vθ0 (t) in order to reduce the Mean Squared error.
Adjusted Training: Due to the inevitable regression error of the critic, and the fact that it is ran-
domly initialized at the beginning, the advantage δt can undesirably become negative for a correctly-
selected tag, or positive for a Wrongly-selected tag. Optimizing the netWork according to these
invalid advantages Would increase the probability of the Wrong tags, While decreasing the probabil-
ity of the true tag. In such cases, to help critic update itself and form better estimates in the next
iteration, we clip δt to zero by defining the adjusted advantage aδt as adjust(yt, ^t, δt) X δt where:
0
adjust(yt,yt,δt) = < 0
、1
if	yt	= yt	&	δt	< 0
if	yt	= yt	&	δt> 0
otherwise
By setting the advantage aδt to 0, the adjust term effectively switches off the entire actor update
when the advantage has the wrong polarity. Note that the critic is always updated.
Similar to prior RL works (Ranzato et al., 2016; Bahdanau et al., 2017; Paulus et al., 2018), we pre-
train the network using the Teacher Forcing maximum-likelihood objective (Jml). We then continue
training from the best model using our adjusted actor-critic objective. Figure 1 shows development
set performance, starting from the same Jml-pre-trained point, for our adjusted objective, as com-
pared to standard actor-critic, REINFORCE with baseline, and normal REINFORCE on German
NER. We observe that the adjusted training helps the model reach a higher point compared to all
5
Under review as a conference paper at ICLR 2019
other objectives. Unlike prior RL methods, the adjusted actor-critic objective does not require any
schedule for pre-training the critic. It also avoids the necessity of combining the actor-critic ob-
jective with the Teacher Forcing maximum-likelihood training, which is shown as actor-critic + ml
in Figure 1. After the pre-training phase, the related works (Ranzato et al., 2016; Bahdanau et al.,
2017; Paulus et al., 2018) still combine the maximum-likelihood training with their proposed RL
objectives.
5	Experiments
We comprehensively compare AC-RNN, CRF, and RNN on three tasks: NER tagging, CCG su-
pertagging, and machine Transliteration. We then compare the adjusted actor-critic objective with
Scheduled Sampling on NER. Finally, we compare our training strategy to the Self-Critical method
of Rennie et al. (2017).
5.1	Setup
Datasets: To conduct the NER experiments, we use the English and German datasets of the CoNLL-
2003 shared task (Tjong Kim Sang & De Meulder, 2003). Both datasets are annotated with 4
different entity types: ‘Location’, ‘Organization’, ‘Person’, and ‘Miscellaneous’ (e.g. events, na-
tionalities, etc.). As we have multi-word named entities (e.g. ‘University of XYZ’), we employ the
‘BILOU’ tagging scheme (Ratinov & Roth, 2009).
For CCG supertagging, we use the English CCGbank (Hockenmaier & Steedman, 2007), the stan-
dard sections {02-21}, {00}, and {23} as the train, development, and test sets, respectively. We
consider all the 1284 supertags appeared in the train set.
We use pre-trained, 100-dimensional Glove embeddings (Pennington et al., 2014) for all English
word-level tasks, and fine-tune them during training. For German NER, we obtain the embeddings
(64 dimensions) of Lample et al. (2016), which are trained on a German monolingual dataset from
the 2010 Machine Translation Workshop. We apply no preprocessing on the datasets except replac-
ing the numbers and unknown words with the ‘NUM’ and ‘UNK’ symbols.
We conduct the transliteration experiments on the English-to-Chinese (EnCh), English-to-Japanese
(EnJa), English-to-Persian (EnPe), and English-to-Thai (EnTh) datasets of the NEWS-2018 shared
task.1 The training sets contain approximately 40K, 30K, 10K and 30K instances for EnCh, EnJa,
EnPe, and EnTh, respectively, while the development sets have 1K instances. We train the models
on the training sets, and evaluate them on the development sets. We hold out 10% of the training
sets as our internal tuning sets.
Training Details: For the experiments, our different models share the same encoder, using the same
number of hidden units (see the appendix for the hyper-parameters used in our experiments). The
maximum-likelihood training is done with the Adam optimizer (Kingma & Ba, 2015) with a learning
rate of 0.0005. The RL training is done with the mini-batch gradient ascent (θ = θ + α dj∂θ(θ)) using
a fixed step size of 0.5 for NER & CCG, and 0.1 for Transliteration 2. The critic is trained with a
separate Adam optimizer with the learning rate of 0.0005. We employ a linear-chain first-order
undirected graph in the CRF model.
As performance varies depending on the random initialization, we train each model 20 times for
NER and 5 times for CCG using different random seeds which are the same for all models. We
report scores averaged across these runs ± the standard deviations. Due to time constraints, for the
transliteration experiments, we train each model only once.
Evaluation: We compute the standard evaluation metric for each task: entity-level F1-score for
NER, tagging accuracy for CCG, and word-level accuracy for transliteration. For the models with
decoder RNNs, we report the results achieved using a beam search with a beam of size 10. For the
NER and CCG experiments, we conduct the significance tests on the unseen final test sets, using the
Student’s t-test over random replications at the significance level of 0.05.
1http://workshop.colips.org/news2018/shared.html
2We also tried Adam and RMSProp optimizers for the RL training, but both completely diverged.
6
Under review as a conference paper at ICLR 2019
Model	Dev (En)	Test (En)	Dev (De)	Test (De)
INDP	93.63 ±0.13	89.77 ±0.21	75.51 ±0.28	72.15 ±0.57
RNN	94.43 ±0.16	90.75 ±0.23	76.85 ±0.39	73.52 ±0.36
CRF	94.47 ±0.12	90.80 ±0.19	76.27 ±0.35	73.59 ±0.36
AC-RNN	94.54 ±0.12	90.96 ±0.15	77.10 ±0.29	73.82 ±0.29
Lample et al. (2016)		90.94 一		78.76
Table 1: Average entity-level F1-score for English & German NER on the CoNLL-2003 datasets.
Reimers & Gurevych (2017) report 90.81 as the median performance for the CRF model of Lample
et al. (2016) in English.
Model	Dev	Test	Memory (GB)	Time (m)
INDP	94.24 ±0.03	94.25 ±0.11	1.3	5
RNN	94.25 ±0.06	94.28 ±0.09	1.8	11
CRF	94.31 ±0.07	94.15 ±0.11	9.8	50
AC-RNN	94.43 ±0.08	94.39 ±0.06	1.5	10
Vaswani et al. (2016) Kadari et al. (2017)	94.24 94.37	9450 94.49°		
Table 2: Average top-1 accuracy, and the required GPU memory and execution time (one epoch) on
English CCG supertagging. The results are achieved with CRF training.
5.2	Results
Main Comparisons: As our primary empirical study, we compare the AC-RNN to CRF and RNN
models. we also consider independent prediction of the labels as another baseline.
The results of the NER experiments are shown in Tables 1. As expected, we observe that by mod-
elling the output dependencies using either an RNN or a CRF, we achieve a significant improvement
over the baseline INDP, about 1% F1-score on both English and German datasets. With respect to
prior work, our CRF model replicates the reported results on English NER. On German NER, we
cannot replicate the CRF results of Lample et al. (2016), although we obtained their German word
embeddings. We attribute this discrepancy to different preprocessing of the dataset. Moreover, AC-
RNN significantly outperforms both RNN and CRF on both English and German test sets with the
corresponding P values of 0.001 and 0.004 for RNN, and 0.003 and 0.016 for CRF. These results
demonstrate that AC-RNN is successful at overcoming the RNN’s exposure bias, and represents a
strong alternative to CRF for named entity recognition.
On CCG supertagging (Table 2), AC-RNN is significantly better than all other models with the P
values of 0.019, 0.025, and 0.002, respectively, and is competitive with reported state-of-the-art
results. For this task, we had expected the improvements to be larger, because of CCG supertag-
ging’s potential for long-distance output dependencies. Instead, the results show that independent
predictions do surprisingly well.
Table 2 also shows the time and memory requirements for each method on the CCG task. We observe
that, due to the large output vocabulary size of the task (1284 supertags), CRF is five times slower
than AC-RNN during training, while the batched version of its Forward algorithm requires six times
more GPU memory during training. The Forward algorithm of CRF runs out of memory with the
mini-batch size of 16 on a 12-GB Graphical Processing Unit.
The transliteration results in Table 3 show that AC-RNN outperforms CRF (likely due to CRF’s
inability to predict an output of a different length from its input), as well as RNN (likely due to
its exposure bias). The transliteration experiments support our hypothesis that AC-RNN is more
generally-applicable than CRF, and the improvements from the adjusted actor-critic training transfer
to other tasks.
To confirm that our RNN baseline performs reasonably well, we validate our transliteration model
against a standard NMT implementation as provided by the OpenNMT tool (Klein et al., 2017). We
apply the tool “as-is” with its default translation hyper-parameters. Note that our RNN system in
this experiment is also an NMT-style model with an attention mechanism.
7
Under review as a conference paper at ICLR 2019
Model	EnCh	EnJa	EnPe	EnTh
CRF	67.6	45.8	75.6	32.2
RNN	70.6	51.6	76.3	39.7
SC-RNN	70.2	51.8	77.2	41.3
AC-RNN	72.3	52.4	77.8	41.4
OpenNMT	70.1	47.7	70.5	36.3
Table 3: The word-level transliteration accuracy on the development sets of NEWS-2018 shared
task. SC: Self-Critical training
Model	Dev	Test
RNN	76.85 ±0.39	73.52 ±0.36
SS-RNN	76.93 ±0.32	73.65 ±0.29
SC-RNN	76.71 ±0.27	73.50 ±0.42
AC-RNN	77.10 ±0.29	73.82 ±0.29
Table 4: The adjusted actor-critic training compared to Scheduled Sampling (SS-RNN), and Self-
Critical training (SC-RNN) on German NER.
Scheduled Sampling Comparisons: In the next experiment, we compare the adjusted actor-critic
objective to Scheduled Sampling. We implement this approach using the inverse sigmoid schedule
of Bengio et al. (2015) for annealing , and denote it as SS-RNN. Table 4 shows the comparison
of SS-RNN with AC-RNN on German NER. Both systems improve over RNN, but AC-RNN is
significantly better than SS-RNN with the P value of 0.040. This result supports our hypothesis that
the reinforcement-learning solutions should outperform Scheduled Sampling, as the adjusted actor-
critic training considers the entire sequence, whereas Scheduled Sampling addresses only exposure
to the immediately previous token. We also observe that Scheduled Sampling, unlike the adjusted
actor-critic training, is highly sensitive to the choice of sampling schedule (see the appendix for a
quantitative analysis).
Self-Critical Comparisons: In our final experiment, we compare the adjusted actor-critic training
with the Self-Critical policy training of Rennie et al. (2017) which does not require a critic model.
This method is intended to represent the state-of-the-art in reinforcement learning for sequence-
to-sequence models with sequence-level rewards, to be contrasted against our AC-RNN and its
position-level rewards. The transliteration results shown in Table 3 indicate that the Self-Critical
training improves over RNN on EnJa and EnPe, and EnTh, however, it fails to beat AC-RNN across
all the evaluation sets. On German NER, the Self-Critical training cannot improve over RNN as
shown in Table 4. This observation is aligned with our initial hypothesis that the reinforcement-
learning techniques applied to sequence labeling would benefit more from modelling the intermedi-
ate rewards, as is done with the Temporal Difference credits in the adjusted actor-critic training.
6	Conclusion
We have proposed an adjusted actor-critic algorithm to train encoder-decoder RNNs for sequence
labeling tasks. Though related reinforcement-learning algorithms have previously been applied to
sequence-to-sequence tasks, our proposed AC-RNN is specialized to sequence-labeling by taking
advantage of the per-position rewards. To our knowledge, we have presented the first direct, con-
trolled comparison between CRFs and any form of RNN. On NER and CCG supertagging, our
system significantly outperforms both RNN and CRF, establishing the AC-RNN as an efficient al-
ternative for sequence labeling. We have also demonstrated the advantages of the AC-RNN in terms
of its flexibility, fast training, and small memory footprint. Finally, we showed that our proposal for
handling exposure-bias outperforms the related alternatives of Scheduled Sampling and Self-Critical
policy training.
8
Under review as a conference paper at ICLR 2019
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. ICLR, 2015. URL http://arxiv.org/abs/1409.0473.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau,
Aaron C. Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. ICLR,
2017. URL http://arxiv.org/abs/1607.07086.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Proceedings of the 28th International Confer-
ence on Neural Information Processing Systems - Volume 1, NIPS’15, pp. 1171-1179, Cam-
bridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=
2969239.2969370.
Stephen Clark and James R. Curran. The importance of supertagging for wide-coverage ccg pars-
ing. In COLING 2004: Proceedings of the 20th International Conference on Computational
Linguistics, 2004. URL http://www.aclweb.org/anthology/C04-1041.
Hal Daume III, John Langford, and Daniel Marcu. Search-based structured prediction. 2009. URL
http://pub.hal3.name/#daume06searn.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Sepp Hochreiter and JUrgen SChmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Julia Hockenmaier and Mark Steedman. Ccgbank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Comput. Linguist., 33(3):355-396, September 2007.
ISSN 0891-2017. doi: 10.1162/coli.2007.33.3.355. URL http://dx.doi.org/10.1162/
coli.2007.33.3.355.
Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional LSTM-CRF models for sequence tagging.
CoRR, abs/1508.01991, 2015. URL http://arxiv.org/abs/1508.01991.
Amir Hossein Jadidinejad. Neural machine transliteration: Preliminary results.	CoRR,
abs/1609.04253, 2016. URL http://arxiv.org/abs/1609.04253.
Rekia Kadari, Yu Zhang, Weinan Zhang, and Ting Liu. Ccg supertagging via bidirectional lstm-crf
neural architecture. Neurocomputing, 12 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
URL http://arxiv.org/abs/1412.6980.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. Opennmt: Open-
source toolkit for neural machine translation. In Proceedings of ACL 2017, System Demon-
strations, pp. 67-72. Association for Computational Linguistics, 2017. URL http://www.
aclweb.org/anthology/P17-4012.
Vijay R Konda and John N Tsitsiklis. On actor-critic algorithms. SIAM journal on Control and
Optimization, 42(4):1143-1166, 2003.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 260-270. Association for Computational Linguistics, 2016. doi: 10.18653/v1/
N16-1030. URL http://aclweb.org/anthology/N16-1030.
Remi Leblond, Jean-Baptiste Alayrac, Anton Osokin, and Simon Lacoste-Julien. SEARNN: Train-
ing RNNs with global-local losses. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=HkUR_y-RZ.
9
Under review as a conference paper at ICLR 2019
Mike Lewis, Kenton Lee, and Luke Zettlemoyer. Lstm ccg parsing. In Proceedings of the 2016
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pp. 221-231. Association for Computational Linguistics, 2016.
doi: 10.18653/v1/N16-1026. URL http://www.aclweb.org/anthology/N16-1026.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 1412-1421. Association for Computational Linguistics, 2015.
doi: 10.18653/v1/D15-1166. URL http://www.aclweb.org/anthology/D15-1166.
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 1064-1074. Association for Computational Linguistics, 2016. doi: 10.
18653/v1/P16-1101. URL http://aclweb.org/anthology/P16-1101.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
Research, pp. 1928-1937, New York, New York, USA, 20-22 Jun 2016. PMLR. URL http:
//proceedings.mlr.press/v48/mniha16.html.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference on International Conference on Machine
Learning, ICML’10, pp. 807-814, USA, 2010. Omnipress. ISBN 978-1-60558-907-7. URL
http://dl.acm.org/citation.cfm?id=3104322.3104425.
Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. ICLR, 2018. URL http://arxiv.org/abs/1705.04304.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543,
2014. URL http://www.aclweb.org/anthology/D14-1162.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing with recurrent neural networks. ICLR, 2016. URL http://arxiv.org/abs/1511.
06732.
Lev Ratinov and Dan Roth. Design challenges and misconceptions in named entity recognition.
In Proceedings of the Thirteenth Conference on Computational Natural Language Learning
(CoNLL-2009), pp. 147-155, Boulder, Colorado, June 2009. Association for Computational Lin-
guistics. URL http://www.aclweb.org/anthology/W09-1119.
Nils Reimers and Iryna Gurevych. Reporting score distributions makes a difference: Performance
study of lstm-networks for sequence tagging. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing, pp. 338-348, Copenhagen, Denmark, Septem-
ber 2017. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/D17-1035.
Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-critical
sequence training for image captioning. 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 1179-1195, 2017.
Mihaela Rosca and Thomas Breuel. Sequence-to-sequence neural network models for translitera-
tion. CoRR, abs/1610.09565, 2016. URL http://arxiv.org/abs/1610.09565.
Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In AISTATS, 2011.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with
neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp.
3104-3112. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
5346-sequence-to-sequence-learning-with-neural-networks.pdf.
10
Under review as a conference paper at ICLR 2019
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press,
Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter Daelemans and Miles Osborne (eds.), Proceed-
ings ofCoNLL-2003,pp. 142-147. Edmonton, Canada, 2003.
Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. Supertagging with lstms. In Pro-
ceedings of the 2016 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, pp. 232-237. Association for Compu-
tational Linguistics, 2016. doi: 10.18653/v1/N16-1027. URL http://www.aclweb.org/
anthology/N16-1027.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Mach. Learn., 8(3-4):229-256, May 1992. ISSN 0885-6125. doi: 10.1007/
BF00992696. URL https://doi.org/10.1007/BF00992696.
Huijia Wu, Jiajun Zhang, and Chengqing Zong. A dynamic window neural network for ccg su-
pertagging. In AAAI Conference on Artificial Intelligence, 2017.
Wenduan Xu. Lstm shift-reduce ccg parsing. In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing, pp. 1754-1764. Association for Computa-
tional Linguistics, 2016. doi: 10.18653/v1/D16-1181. URL http://www.aclweb.org/
anthology/D16-1181.
11
Under review as a conference paper at ICLR 2019
Hyper-parameter	En/De	CCG	TL
Char _embedding_Size	-32-	32	128
OutPut_embedding_Size	32	128	128
max_gradient_norm	5.0	5.0	10.0
encoder units	256	512	256
decoder units	256	512	256
batch size	32	10	64
n	2/4	8	6
dropout	0.5	0.5	0.5
RNN gate	LSTM	LSTM	LSTM
Table 5: The hyper-parameters used in the experiments.
Test
Models
Figure 2: The sensitivity of Scheduled Sampling on the choice of sampling schedule on German
NER. Higher k results in less sampling.
A Hyper-parameters
Table 5 provides the hyper-parameters used in our experiments.
B Schedule for Scheduled S ampling
Figure 2 illustrates that Scheduled Sampling is highly sensitive to the choice of sampling schedule.
12