Under review as a conference paper at ICLR 2019
Deep Denoising: Rate-Optimal Recovery of
Structured Signals with a Deep Prior
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks provide state-of-the-art performance for image denoising,
where the goal is to recover a near noise-free image from a noisy image. The
underlying principle is that neural networks trained on large datasets have em-
pirically been shown to be able to generate natural images well from a low-
dimensional latent representation of the image. Given such a generator network,
or prior, a noisy image can be denoised by finding the closest image in the range
of the prior. However, there is little theory to justify this success, let alone to
predict the denoising performance as a function of the networks parameters. In
this paper we consider the problem of denoising an image from additive Gaussian
noise, assuming the image is well described by a deep neural network with ReLu
activations functions, mapping a k-dimensional latent space to an n-dimensional
image. We state and analyze a simple gradient-descent-like iterative algorithm
that minimizes a non-convex loss function, and provably removes a fraction of
p1 ´ Opk{nqq of the noise energy. We also demonstrate in numerical experiments
that this denoising performance is, indeed, achieved by generative priors learned
from data.
1	Introduction
We consider the image or signal denoising problem, where the goal is to remove noise from an
unknown image or signal. In more detail, our goal is to obtain an estimate of an image or signal
y* P Rn from
y “ y* ' η,
where η is unknown noise, often modeled as a zero-mean white Gaussian random variable with
covariance matrix σ2{nI.
Image denoising relies on modeling or prior assumptions on the image y*. For example, suppose
that the image y* lies in a k-dimensional subspace of Rn denoted by Y. Then we can estimate the
original image by finding the closest point in '2-distance to the noisy observation y on the subspace
Y. The corresponding estimate, denoted by y, obeys
}y ´ y*}2 W σ2 k,
(1)
with high probability (throughout, }∙} denotes the '2-norm). Thus, the noise energy is reduced by a
factor of k/n over the trivial estimate y “ y which does not use any prior knowledge of the signal.
The denoising rate (1) shows that the more concise the image prior or image representation (i.e.,
the smaller k), the more noise can be removed. If on the other hand the prior (the subspace, in this
example) does not include the original image y* , then the error bound (1) increases as we would
remove a significant part of the signal along with noise when projecting onto the range of the signal
prior. Thus a concise and accurate prior is crucial for denoising.
Real world signals rarely lie in a priori known subspaces, and the last few decades of image denois-
ing research have developed sophisticated and accurate image models or priors and algorithms.
Examples include models based on sparse representations in overcomplete dictionaries such as
wavelets (Donoho, 1995) and curvelets (Starck et al., 2002), and algorithms based on exploiting
1
Under review as a conference paper at ICLR 2019
self-similarity within images (Dabov et al., 2007). A prominent example of the former class of al-
gorithms is the BM3D (Dabov et al., 2007) algorithm, which achieves state-of-the-art performance
for certain denoising problems. However, the nuances of real world images are difficult to describe
with handcrafted models. Thus, starting with the paper (Elad & Aharon, 2006) that proposes to learn
sparse representation based on training data, it has become common to learn concise representation
for denoising (and other inverse problems) from a set of training images.
In 2012, Burger et al. (Burger et al., 2012) applied deep networks to the denoising problem, by
training a deep network on a large set of images. Since then, deep learning based denoisers (Zhang
et al., 2017) have set the standard for denoising. The success of deep network priors can be at-
tributed to their ability to efficiently represent and learn realistic image priors, for example via auto-
decoders (Hinton & Salakhutdinov, 2006) and generative adversarial models (Goodfellow et al.,
2014). Over the last few years, the quality of deep priors has significantly improved (Karras et al.,
2017; Ulyanov et al., 2017). As this field matures, priors will be developed with even smaller latent
code dimensionality and more accurate approximation of natural signal manifolds. Consequently,
the representation error from deep priors will decrease, and thereby enable even more powerful
denoisers.
As the influence of deep networks in inverse problems grows, it becomes increasingly important to
understand their performance at a theoretical level. Given that most optimization approaches for
deep learning are first order gradient methods, a justification is needed for why they do not get stuck
in local minima. The closest theoretical work to this question is Bora et al. (2017), which solves
a noisy compressive sensing problem with generative priors by minimizing empirical risk. Under
the assumption that the network is Lipschitz, they show that if the global optimizer can be found,
which is in principle NP-hard, then a signal estimate is recovered to within the noise level. While
the Lipschitzness assumption is quite mild, the resulting theory does not provide justification for
why global optimality can be reached.
The most related work that establishes theoretical reasons for why gradient methods would not get
stuck in local minima, when using deep generative priors for solving inverse problems, is Hand &
Voroninski (2018). In it, the authors establish global favorability for optimization of the noiseless
empirical risk function. Specifically, they show existence of a descent direction outside a ball around
the global optimizer and a negative multiple of it in the latent space of the generative model. This
work does not provide a specific algorithm which provably estimates the global minimizer, nor does
it provide an analysis of the robustness of the problem with respect to noise.
In this paper, we propose the first algorithm for solving denoising with deep generative priors that
provably finds an approximation of the underlying image. Following the lead of Hand & Voronin-
ski (2018), we assume an expansive Gaussian model for the deep generative network in order to
establish this result.
Contributions: The goal of this paper is to analytically quantify the denoising performance of
deep-prior based denoisers. Specifically, we characterize the performance of a simple and efficient
algorithm for denoising based on a d-layer generative neural network G: Rk → Rn, with k < n,
and random weights. In more detail, we propose a gradient method with a tweak that attempts to
minimize the least-squares loss f (x) “ 2}G(x) — y}2 between the noisy image y and an image in
the range of the prior, Gpxq. While f is non-convex, we show that the gradient method yields an
estimate X obeying
IIGpxq — y*}2 S σ2—,
n
with high probability, where the notation S absorbs a constant factor depending on the number
of layers of the network, and its expansitivity, as discussed in more detail later. Our result shows
that the denoising rate of a deep prior based denoiser is determined by the dimension of the latent
representation.
We also show in numerical experiments, that this rate—shown to be analytically achieved for random
priors—is also experimentally achieved for priors learned from real imaging data.
2
Under review as a conference paper at ICLR 2019
x1
Figure 1: Loss surface f (x) “ }G(x) — G(x*)}, x* = [1,0], of an expansive network G with
ReLu activation functions with k “ 2 nodes in the input layer and n2 “ 300 and n3 “ 784 nodes in
the hidden and output layers, respectively, with random Gaussian weights in each layer. The surface
has a critical point near —x*, a global minimum at x*, and a local maximum at 0.
2	Problem formulation
We consider the problem of estimating a vector y* P Rn from a noisy observation y “ y* ` η. We
assume that the vector y* belongs to the range of a d-layer generative neural network G: Rk → Rn,
with k < n. That is, y* “ G(x*) for some x* P Rk. We consider a generative network of the form
G(x) “ relu(Wd . . . relu(W2 relu(W1x*)) . . .),
where relu(x) “ max(x, 0) applies entrywise, Wi P RniXni´1, are the weights in the i-th layer, n
is the number of neurons in the ith layer, and the network is expansive in the sense that k “ n0 <
nι < ∙∙∙ < nd = n. The problem at hand is: Given the weights of the network Wi... Wd and a
noisy observation y, obtain an estimate y of the original image y* such that }y — y*} is small and y
is in the range of G.
3	Denoising via empirical risk minimization
As a way to solve the above problem, we first obtain an estimate of x*, denoted by X, and then
estimate y* as G(x). In order to estimate x*, we minimize the empirical risk objective
f(xq ：= 2}G(X) ´ y}2.
Since this objective is nonconvex, there is no a priori guarantee of efficiently finding the global min-
imum. Approaches such as gradient methods could in principle get stuck in local minima, instead
of finding a global minimizer that is close to x*.
However, as we show in this paper, under appropriate conditions, a gradient method with a tweak—
introduced next—finds a point that is very close to the original latent parameter x* , with the distance
to the parameter x* controlled by the noise. In order to state the algorithm, we first introduce a useful
quantity. For analyzing which rows of a matrix W are active when computing relu(W x), we let
W',x = diag(WX > 0)W.
For a fixed weight matrix W, the matrix W',χ zeros out the rows of W that do not have a positive
dot product with x. Alternatively put, W',χ contains weights from only the neurons that are active
for the input x. We also define Wι,',χ “ (Wι)',χ “ diag(Wιx > 0)Wi and
Wi,',x = diag(WiWi—i,+,x …W2,',xWι,',χx > 0)Wi.
The matrix Wi,',x consists only of the weights of the neurons in the ith layer that are active if the
input to the first layer is x.
3
Under review as a conference paper at ICLR 2019
We are now ready to state our algorithm: a gradient method with a tweak informed by the loss
surface of the function to be minimized. Given a noisy observation y, the algorithm starts with
an arbitrary initial point x0 ‰ 0. At each iteration i “ 0, 1, . . ., the algorithm computes the step
direction
Vxi =(∏1=dWi,',Xi)t(G(xi) ´ yq,
which is equal to the gradient of f if f is differentiable at xi . It then takes a small step opposite to
Vxi. The tweak is that before each iteration, the algorithm checks whether f (—Xi) is smaller than
f(xiq, and if so, negates the sign of the current iterate xi.
This tweak is informed by the loss surface. To understand this step, it is instructive to examine the
loss surface for the noiseless case in Figure 1. It can be seen that while the loss function has a global
minimum at x*, it is relatively flat close to —x*. In expectation, there is a critical point that is a
negative multiple of x* with the property that the curvature in the +χ* direction is positive, and the
curvature in the orthogonal directions is zero. Further, around approximately —x*, the loss function
is larger than around the optimum x* . As a simple gradient descent method (without the tweak)
could potentially get stuck in this region, the negation check provides a way to avoid converging to
this region. Our algorithm is formally summarized as Algorithm 1 below.
Algorithm 1 Gradient method
Require: Weights of the network Wi, noisy observation y, and step size α > 0
1:	Choose an arbitrary initial point x0 P Rkzt0u
2:	for i “ 0, 1, . . . do
3:	if f (—Xi) V f(xi) then
4:	Xi D——Xi;
5:	end if
6:	Compute Vxi = (π1=dWi,',Xi)t(G(Xi) ´ y)
7:	Xi'1 = Xi ´ αvxi
8:	end for
Other variations of the tweak are also possible. For example, the negation check in Step 2 could be
performed after a convergence criterion is satisfied, and if a lower objective is achieved by negating
the latent code, then the gradient descent can be continued again until a convergence criterion is
again satisfied.
4 Main results
For our analysis, We consider a fully-connected generative network G: Rk → Rn with Gaussian
weights and no bias terms. Specifically, we assume that the weights Wi are independently and iden-
tically distributed as N(0, 2{ni), but do not require them to be independent across layers. Moreover,
we assume that the network is sufficiently expansive:
Expansivity condition. We say that the expansivity condition with constant e > 0 holds if
neee´* 1 2 3 log(1∕e)n-ι logni´i, forall i,
where c is a particular numerical constant.
In a real-world generative network the weights are learned from training data, and are not drawn
from a Gaussian distribution. Nonetheless, the motivation for selecting Gaussian weights for our
analysis is as follows:
1. The empirical distribution of weights from deep neural networks often have statistics con-
sistent with Gaussians. AlexNet is a concrete example (Arora et al., 2015).
2. The field of theoretical analysis of recovery guarantees for deep learning is nascent, and
Gaussian networks can permit theoretical results because of well developed theories for
random matrices.
3. It is not clear which non-Gaussian distribution for weights is superior from the joint per-
spective of realism and analytical tractability.
4
Under review as a conference paper at ICLR 2019
4.	Truly random nets, such as in the Deep Image Prior (Ulyanov et al., 2017), are increas-
ingly becoming of practical relevance. Thus, theoretical advances on random nets is of
independent interest.
We are now ready to state our main result.
Theorem 1. Consider a network with the weights in the i-th layer, Wi P RniXni´1, i.i.d. N(0, 2{n}
distributed, and suppose that the network satisfies the expansivity condition for some e ≤ K{d90.
Also, suppose that the noise variance obeys
ω ≤ }x'f 1, ω :“ ʌ ∕l8σ2- log(ndnd´1 ...nd).
d16	n 1 2
Consider the iterates of Algorithm 1 with stepsize α “ K4 ∙1. Then, there exists a number of steps
N upper bounded by
N & K2 5
d4e }x*}
such that after N steps, the iterates of Algorithm 1 obey
}xi — x*} ≤ K5d9 }x*} ? ' K6d6ω, for all ieN,	(2)
with probability at least 1 — 2ei2k log n —郡“2 8nie-K7ni—2 — 8n1e-K7'2 logp1{eqk. Here, K1, K2,..
are numerical constants, and x0 is the initial point in the optimization.
The error term in the bound (2) consists of two terms—the first is controlled by e, and the second
depends on the noise. The first term is negligible if e is chosen sufficiently small, but that comes
at the expense of the expansivity condition being more stringent. The second term in the bound (2)
is more interesting and controls the effect of noise. Specifically, for e sufficiently small, our result
guarantees that after sufficiently many iterations,
}xi — x*}2 S σ2-,
n
where the notation S absorbs a factor logarithmic in n and polynomial in d. One can show that G is
Lipschitz in a region around x*1,
}G(xi) — G(x*)『S σ2 —.
n
Thus, the theorem guarantees that our algorithm yields the denoising rate of σ2 k{n, and, as a conse-
quence, denoising based on a generative deep prior provably reduces the energy of the noise in the
original image by a factor of k{n. We note that the intention of this paper is to show rate-optimality
of recovery with respect to the noise power, the latent code dimensionality, and the signal dimen-
sionality. As a result, no attempt was made to establish optimal bounds with respect to the scaling
of constants or to powers of d. The bounds provided in the theorem are highly conservative in the
constants and dependency on the number of layers, d, in order to keep the proof as simple as possi-
ble. Numerical experiments shown later reveal that the parameter range for successful denoising are
much broader than the constants suggest. As this result is the first of its kind for rigorous analysis
of denoising performance by deep generative networks, we anticipate the results can be improved in
future research, as has happened for other problems, such as sparsity-based compressed sensing and
phase retrieval.
4.1	The Weight Distribution Condition (WDC)
To prove our main result, we make use of a deterministic condition on G, called the Weight Distri-
bution Condition (WDC), and then show that Gaussian Wi, as given by the statement of Theorem 1
are such that Wi{√2 satisfies the WDC with the appropriate probability for all i, provided the ex-
pansivity condition holds. Our main result, Theorem 1, continues to hold for any weight matrices
such that Wi{?2 satisfy the WDC.
1The proof of Lipschitzness follows from applying the Weight Distribution Condition in Section 4.1.
5
Under review as a conference paper at ICLR 2019
> ≤ e, with Qx,y = π-^OIk + 孚M^0y,	(3)
2π	2π
The condition is on the spatial arrangement of the network weights within each layer. We say that
the matrix W P RnXk satisfies the Weight Distribution Condition with constant e if for all nonzero
x, y P Rk,
n
1 hwi,xi >01 hwi,yi >0 ' Wiwi — Qx,y
i“1
where wi P Rk is the ith row of W; Mχ0y P RkXk is the matrix2 such that x → y, y → x,
and z → 0 for all Z P span({x, y})K; X “ x∕}x}2 and y “ y∕}y}2; θo “ =(x,y); and 1s is
the indicator function on S. The norm in the left hand side of (3) is the spectral norm. Note that an
elementary calculation3 gives that Qχ,y “ ErXi“1 Ihwi,χ)>o1hwi,yi>0∙wiwtS for wi 〜N(0,Ik∕n)∙
As the rows wi correspond to the neural network weights of the ith neuron in a layer given by W,
the WDC provides a deterministic property under which the set of neuron weights within the layer
given by W are distributed approximately like a Gaussian. The WDC could also be interpreted as a
deterministic property under which the neuron weights are distributed approximately like a uniform
random variable on a sphere of a particular radius. Note that if x “ y, Qx,y is an isometry up to a
factor of 1∕2.
5	Applications to Compressed Sensing
In this section we briefly discuss another important scenario to which our results apply to, namely
regularizing inverse problems using deep generative priors. Approaches that regularize inverse prob-
lems using deep generative models (Bora et al., 2017) have empirically been shown to improve
over sparsity-based approaches, see (Lucas et al., 2018) for a review for applications in imaging,
and (Mardani et al., 2017) for an application in Magnetic Resonance Imaging showing a significant
performance improvement over conventional methods.
Consider an inverse problem, where the goal is to reconstruct an unknown vector y* P Rn from
m < n noisy linear measurements:
z = Ay* ` η	P Rm,
where A P RmXn is called the measurement matrix and η is zero mean Gaussian noise with
covariance matrix σ2 ∕nI, as before. As before, assume that y* lies in the range of a genera-
tive prior G, i.e., y* “ G(x*q for some x*. As a way to recover x*, consider minimizing the
empirical risk objective f(x) “ ɪ}AG(x) — z}, using Algorithm 1, with Step 6 substituted by
Vxi “ (A∏1=dWi,',χi)t(AG(χi) — y), to account for the fact that measurements were taken with
the matrix A.
Suppose that A is a random projection matrix, for concreteness assume that A has i.i.d. Gaussian
entries with variance 1∕m. One could prove an analogous result as Theorem 1, but with ω “
Jl8σ2 m log(ndnd´1... nd), (note that n has been replaced by m). This extension shows that,
provided e is chosen sufficiently small, that our algorithm yields an iterate xi obeying
}G(xi)— G(x*)}2 W σ2 —,
m
where again W absorbs factors logarithmic in the ni ’s, and polynomial in d. Proving this result
would be analogous to the proof of Theorem 1, but with the additional assumption that the sensing
matrix A acts like an isometry on the union of the ranges of 口匕/十咫,analogous to the proof
in (Hand & Voroninski, 2018). This extension of our result shows that Algorithm 1 enables solving
inverse problems under noise efficiently, and quantifies the effect of the noise.
2A formula for Mχ0y is as follows. If θo “ =(X, y) P (0, π) and R is a rotation matrix such that X and y
(Cos θo	sin θo	0 ∖
sin θ0 ´ cos θ0	0	R, where
0	0	0k—2)
θk´2 is a k — 2 X k — 2 matrix of zeros. If θo “ 0 or π, then M^0y “ Xxt or ´XXt, respectively.
3To do this calculation, take X = eι and y “ Cos θo - eι ' sin θo - e2 without loss of generality. Then each
entry of the matrix can be determined analytically by an integral that factors in polar coordinates.
6
Under review as a conference paper at ICLR 2019
We hasten to add that the paper (Bora et al., 2017) also derived an error bound for minimizing em-
pirical loss. However, the corresponding result (for example Lemma 4.3) differs in two important
aspects to our result. First, the result in (Bora et al., 2017) only makes a statement about the mini-
mizer of the empirical loss and does not provide justification that an algorithm can efficiently find
a point near the global minimizer. As the program is non-convex, and as non-convex optimization
is NP-hard in general, the empirical loss could have local minima at which algorithms get stuck.
In contrast, the present paper presents a specific practical algorithm and proves that it finds a solu-
tion near the global optimizer regardless of initialization. Second, the result in (Bora et al., 2017)
considers arbitrary noise η and thus can not assert denoising performance. In contrast, we consider
a random model for the noise, and show the denoising behavior that the resulting error is no more
than Opk{nq, as opposed to }η}2 « Op1q, which is what we would get from direct application of
the result in (Bora et al., 2017).
6 Experimental results
In this section we provide experimental evidence that corroborates our theoretical claims that denois-
ing with deep priors achieves a denoising rate proportional to σ2 k{n. We consider both a synthetic,
random prior, as studied theoretically in the paper, as well as a prior learned from data. All our
results are reproducible with the code provided in the supplement.
6.1	Denoising with a synthetic prior
We start with a synthetic generative network prior with ReLu-activation functions, and draw its
weights independently from a Gaussian distribution. We consider a two-layer network with n “
1500 neurons in the output layer, 500 in the middle layer, and vary the number of input neurons,
k, and the noise level, σ. We next present simulations showing that if k is sufficiently small, our
algorithm achieves a denoising rate proportional to σk{n as guaranteed by our theory.
Towards this goal, We generate Gaussian inputs x* to the network and observe the noisy image
y “ G(x*) ' η, η 〜N(0,σ2∕nI). From the noisy image, we first obtain an estimate X of the
latent representation by running Algorithm 1 until convergence, and second we obtain an estimate
of the image as y “ G(X). In the left and middle panel of Figure 3, we depict the normalized mean
squared error of the latent representation, MSE(X, x*), and the mean squared error in the image
domain, MSE(G(X), G(x*)), where we defined MSE(z, z1) “ }z — z1}2. For the left panel, we
fix the noise variance to σ2 “ 0.25, and vary k, and for the middle panel we fix k “ 50 and vary
the noise variance. The results show that, if the network is sufficiently expansive, guaranteed by
k being sufficiently small, then in the noiseless case (σ2 “ 0), the latent representation and image
are perfectly recovered. In the noisy case, we achieve a MSE proportional to σ2k∕n, both in the
representation and image domains.
We also observed that for the problem instances considered here, the negation trick in step 3-4 of
Algorithm 1 is often not necessary, in that even without that step the algorithm typically converges
to the global minimum. Having said this, in general the negation step is necessary, since there exist
problem instances that have a local minimum opposite of X*.
6.2	Denoising with a learned prior
We next consider a prior learned from data. Technically, for such a prior our theory does not apply
since we assume the weights to be chosen at random. However, the numerical results presented in
this section show that even for the learned prior we achieve the rate predicted by our theory pertain-
ing to a random prior. Towards this goal, we consider a fully-connected autoencoder parameterized
by k, consisting of an decoder and encoder with ReLu activation functions and fully connected lay-
ers. We choose the number of neurons in the three layers of the encoder as 784, 400, k, and those
of the decoder as k, 400, 784. We set k “ 10 and k “ 20 to obtain two different autoencoders. We
train both autoencoders on the MNIST (Lecun et al., 1998) training set.
We then take an image y* from the MNIST test set, add Gaussian noise to it, and denoise it using our
method based on the learned decoder-network G for k “ 10 and k “ 20. Specifically, we estimate
7
Under review as a conference paper at ICLR 2019
rorre derauqs nae
0.03
0.02
0.01
0
0	20	40	60
k
Figure 3: Mean square error in the image domain, MSE(Gpx),x*), and in the latent representa-
tion, MSEpx, x*), as a function of the dimension of the latent representation, k, with σ2 = 0.25 (left
panel), and the noise variance, σ2 with k “ 50 (middle panel). As suggested by the theory pertain-
ing to decoders with random weights, if k is sufficiently small, and thus the network is sufficiently
expansive, the denoising rate is proportional to σ2 k{n. Right panel: Denoising of handwritten dig-
its based on a learned decoder with k “ 10 and k “ 20, along with the least-squares fit as dotted
lines. The learned decoder with k “ 20 has more parameters and thus represents the images with a
smaller error; therefore the MSE at σ “ 0 is smaller. However, the denoising rate for the decoder
with k “ 20, which is the slope of the curve is larger as well, as suggested by our theory.
the latent representation x by running Algorithm 1, and then set y “ Gpx). See Figure 2 for a few
examples demonstrating the performance of our approach for different noise levels.
We next show that this achieves a mean squared error (MSE) proportional to σ2 k{n, as suggested
by our theory which applies for decoders with random weights. We add noise to the images with
noise variance ranging from σ2 “ 0 to σ2 “ 6. In the right panel of Figure 3 we show the MSE in
the image domain, MSEpGpx), Gpx*)), averaged over a number of images for the learned decoders
with k “ 10 and k “ 20. We observe an interesting tradeoff: The decoder with k “ 10 has fewer
parameters, and thus does not represent the digits as well, therefore the MSE is larger than that for
k “ 20 for the noiseless case (i.e., for σ “ 0). On the other hand, the smaller number of parameters
results in a better denoising rate (by about a factor of two), corresponding to the steeper slope of the
MSE as a function of the noise variance, σ2 .
References
S. Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for
training. arXiv:1511.05653, 2015.
A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models.
arXiv:1703.03208, 2017.
H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete
with BM3d? In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2392—
2399, 2012.
C.	Clason. Nonsmooth analysis and optimization. arXiv:1708.04180, 2017.
K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-D transform-
domain collaborative filtering. IEEE Transactions on Image Processing,16(8):2080—2095, 2007.
8
Under review as a conference paper at ICLR 2019
D.	L. Donoho. De-noising by soft-thresholding. IEEE Transactions on Information Theory, 41(3):
613-627,1995.
M. Elad and M. Aharon. Image denoising via sparse and redundantd representations over learned
dictionaries. IEEE Transactions on Image Processing, 15(12):3736-3745, 2006.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A Courville, and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems
27, pp. 2672-2680. 2014.
P. Hand and V. Voroninski. Global guarantees for enforcing deep generative priors by empirical risk.
In Conference on Learning Theory, 2018. arXiv:1705.07576.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504-507, 2006.
T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality,
stability, and variation. arXiv: 1710.10196, October 2017.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
A. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos. Using deep neural networks for inverse
problems in imaging: Beyond analytical methods. IEEE Signal Processing Magazine, 35(1):
20-36, 2018.
M. Mardani, H. Monajemi, V. Papyan, S. Vasanawala, D. Donoho, and J. Pauly. Recurrent gen-
erative adversarial networks for proximal learning and automated compressive image recovery.
arXiv:1711.10046, 2017.
Jean-Luc Starck, E. J. Candes, and D. L. Donoho. The curvelet transform for image denoising. IEEE
Transactions on Image Processing, 11(6):670-684, 2002.
D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep Image Prior. arXiv:1711.10925, 2017.
K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a Gaussian denoiser: Residual learning
of deep CNN for image denoising. IEEE Transactions on Image Processing, 26(7):3142-3155,
2017.
9
Under review as a conference paper at ICLR 2019
A	Proofs
In this section we prove our main result, Theorem 1. Instead of proving Theorem 1 as stated, we
will prove the following equivalent rescaled statement for when Wi have i.i.d. Np0, 1{niq entries.
Because of this rescaling, G(X) scales like 2—d{2 }x}, the noise ω is assumed to scale like 2—d{2, Vf
scales like 2d, and α scales like 2d .
Theorem 2. Consider a network with the weights in the i-th layer, Wi P RniXni—1, i.i.d. N(0,1{ni)
distributed, and suppose that the network satisfies the expansivity condition for some e ≤ K{d90.
Also, suppose that the noise variance obeys
ω ≤ }x*'K12——,	ω :“ C 18σ2 k log(ndnd´1 . .. n=).
d16	n
Consider the iterates of Algorithm 1 with stepsize α “ K4d. Then, there exists a number of steps
N upper bounded by
NVKI f(x0)2d
、d4e }x*}
such that after N steps, the iterates of Algorithm 1 obey
∣∣Xi 一 X*} ≤ K5d9}x*}?! ' K6d62d{2ω, forall ieN,	(4)
with probability at least 1 — 2ei2k log n — ]d=2 8nie~K7ni—2 — 8nιe~K7'2 logp1{eqk. Here, K1,K2,..
are numerical constants, and x0 is the initial point in the optimization.
As mentioned in Section 4.1, our proof makes use of a deterministic condition, called the Weight
Distribution Condition (WDC), formally defined in Section 4.1. The following proposition estab-
lishes that the expansivity condition ensures that the WDC holds:
Lemma 3 (Lemma 9 in (Hand & Voroninski, 2018)). Fix e P (0,1). Ifthe entires of Wi P Rnixni´1
are i.i.d. N(0,1{ni) and the expansivity condition n > ee´2 log(l/e)ni´i log nj∖ holds, then Wi
satisfies the WDC with constant e with probability at least 1 — 8nie~Ke ni´1. Here, C and K are
numerical constants.
We note that the form of dependence of ni on e can be read off the proofs of Lemma 10 in (Hand
& Voroninski, 2018). It follows from Lemma 3, that the WDC holds for all Wi with probability at
least 1 — Ed=2 8nie-K7ni-2 — 8n1e-K7e2 logp1{eqk.
In the remainder of the proof we work on the event that the WDC holds for all Wi .
A.1 Preliminaries
Recall that the goal of our algorithm is to minimize the empirical risk objective
f (x) = 1}G(x) — y}2,
where y := G(x*) ' η, with η 〜N(0, σ2/nI).
Our results rely on the fact that outside of two balls around X = x* and X = —ρdX*, with Pd
a constant defined below, the direction chosen by the algorithm is a descent direction, with high
probability. Towards this goal, we use a concentration argument, similar to the arguments used
in (Hand & Voroninski, 2018). First, define Ax := ∏1=dWi,+,χ (with Wi,',χ defined in Section 3)
for notational convenience, and note that the step direction of our algorithm can be written as
Vx =	Vx	`	qx,	with	Vx	:= AxAxX—(Axy(Ax*qx*,	and qx	:=	Ax".	(5)
Note that at points X where G (and hence f) is differentiable, we have that Vx = Vf (x).
The proof is based on showing that Vx concentrates around a particular hx P Rk, defined below,
that is a continuous function of nonzero x, x* and is zero only at X = x* and X = —PdX*. The
10
Under review as a conference paper at ICLR 2019
definition of hx depends on a function that is helpful for controlling how the operator X → W',xχ
distorts angles, defined as:
gpθq “ Cosτ(P ´" +Sinθ).
With this notation, we define
1 .	1 ´d´1	π ´ θi )	1	d´1	sinθi	´ π π ´ θj) }x*}2
hx “- 2d (U	=)x*	+ 2d	IxZ	丁(旦 丁)FX
where θ “ =(x, x*) and θi = g(θ一ι). Note that hχ is deterministic and only depends on x, x*,
and the number of layers, d.
In order to bound the deviation of Vχ from hχ we use the following two lemmas, bounding the
deviation controlled by the WDC and the deviation from the noise:
Lemma 4 (Lemma 6 in (Hand & Voroninski, 2018)). Suppose that the WDC holds with e <
1∕(16πd2q2. Then, for all nonzero x, x* P Rk,
d e
}Vχ — hχ}2 ≤ K-2~ max(}x}2, }x*}2), and	(7)
<(π1=dwi,',x)x, (π1=dWi,',χ* qx*D > 4∏ 2d }x}2 }x*}2, and	⑻
>∏1=dWi,',χ>2 W 2d(1 + 2e)d ≤ *二	(9)
Proof. Equation (7) and (8) are Lemma 6 in (Hand & Voroninski, 2018). Regarding (9), note that
the WDC implies that }Wi,+,χ}2 < 1∕2 + e. It follows that
>gWi,',χ>2 ≤ 2d(1 + 2e)d = 2dedlogpi'2e) ≤ 1+4ed & r´d
where the last inequalities follow by our assumption on e.
□
Lemma 5. Suppose the WDC holds with e < 1∕(16πd2)2 ,that any subset of ni´ 1 rows of Wi are
linearly independent for each i, and that η „ N (0, σ2∕nIq. Then the event
Enoise “ {>(∏1=dWi,',χ)tη> ≤ 2ω{2 , for all x) , ω :“ γ16σk log(ndnd´1 ... nd)	(10)
holds with probability at least 1 — 2ei2k log n.
IimsUPy 一 χ,
defined by
As the cost function f is not differentiable everywhere, we will make use of the generalized sub-
differential in order to reference the subgradients at nondifferentiable points. For a Lipschitz
function f defined from a Hilbert space X to R, the Clarke generalized directional derivative
of f at the point x P X in the direction u, denoted by fo (x; u), is defined by fo (x; u) “
f.	f.
fpy+ t)f(y), and the generalized subdifferential of f at x, denoted by Bf (x), is
Bf(X) = {v P Rk | Xv, uy ≤ f (x; u), for all U P X}.
Since f(x) is a piecewise quadratic function, we have
Bf(x) = conv(v1, v2, . . . , vt),	(11)
where conv denotes the convex hull of the vectors v1, . . . , vt, t is the number of quadratic functions
adjoint to x, and vi is the gradient of the i-th quadratic function at x.
Lemma 6. Under the assumption of Lemma 5, and assuming that Enoise holds, we have that, for any
x ‰ 0 and any vχ P Bf (x),
}vχ ´ hχ} ≤ K 2dr max(}x}2, }x*}2q + 2d/2.
In particular, this holdsfor the subgradient VX = Vχ.
11
Under review as a conference paper at ICLR 2019
Proof. By (11), Bf pxq “ convpv1, . . . vtq for some finite t, and thus vx “ a1v1 ` . . . atvt for some
aι,... ,at20, Xi ai = 1. For each vi, there exists a w such that Vi “ limt∣0 Vχ'tw. On the event
Enoise, we have that for any X ‰ 0, for any Vx P Bf Px)
}VX — hx } “ }vx ' qx — hx }
≤ Ilvx ´ hx} ` }qx}
≤ K 2d max(}x}2, }x*}2q ' 2d{2,
where the last inequality follows from Lemmas 4 and 5 above. The proof is concluded by appealing
to the continuity of hx with respect to nonzero x, and by noting that
}vx ´ hx} ≤ X ai}vi ´ hx} ≤ K 2d maχ(}x}2, }x*}2q ' —[2,
i
where We used the inequality above and that Xi ai = 1.	□
We will also need an upper bound on the norm of the step direction of our algorithm:
Lemma 7. Suppose that the WDC holds with e < 1/(16πd2)2 and that the event Enoise holds with
, 2 一 d{2}x*}
ω ≤ ——8∏ ^11. Then, for all x,
dK
}Vx} ≤ F max(}x},}x*}),	(12)
where K is a numerical constant.
Proof. Define for convenience Zj “ 口工1 K θ∏χ,x* . We have
}Vx} ≤}hx} ' }hx — Vx}
,1	1 /	1 d´ sin θi,x	}x*}
≤ 2d x ´ 2dZoX* ´ 2d⅛ ~^Γ~ζi+1 而 x
i“0
≤ 2d }x}+^ 2d+∏2d) }x*}+K1 ~2?~ maχ(}x}, }x*}q+条
dK
≤ F maχ(}x}, }x*}q,
+ K1 2d max(}x}2, }x*}2q + 2d/2
where the second inequality follows from the definition of hx and Lemma 6, the third inequality
uses |Zj| ≤ 1, and the last inequality uses the assumption ω ≤ 2-8∏x*}.	□
A.2 Proof of Theorem 2
We are now ready to prove Theorem 2. The logic of the proof is illustrated in Figure 4. Recall that
xi is the ith iterate of x as per Algorithm 1. We first ensure that we can assume throughout that xi
is bounded away from zero:
Lemma 8. Suppose that WDC holds with e < 1{(16πd2)2 and that Enoise holds with ω in (10)
2´d∕2}x* }	K,d
obeying ω ≤ ——8∏包.Moreover, suppose that the step size in Algorithm 1 satisfies 0 < α < K-,
where K is a numerical constant. Then, after at most N = P 38πK 0" )2 steps, we have that for all
i > N that xi R B(0, Ko }x*}), Ko “ 忐.
In particular, if α “ K2d{d2, then N is bounded by a constant times d4.
We can therefore assume throughout this proof that xi R B(0, Ko}x*}), Ko “ 忐. We prove
Theorem 2 by showing that if IhxI is sufficiently large, i.e., if the iterate xi is outside of set
Se “ !x P Rk ∣}hx}≤ 2d β max(}x}, }x*})),
12
Under review as a conference paper at ICLR 2019
Figure 4: Logic of the proof: Starting at an arbitrary point, Algorithm 1 moves away from 0,
at least till its iterates are outside the gray ring, as 0 is a local maximum; and once an iterate xi
leaves the gray ring around 0, all subsequent iterates will never be in the white circle around 0 again
(See Lemma 8). Then the algorithm might move towards —ρdχ*, but once it enters the dashed ball
around — ρdχ*, it enters a region where the function value is strictly larger than that of the dashed
ball around x*, by Lemma 10. Thus steps 3-5 of the algorithm will ensure that the next iterate
Xi is in the dashed ball around x*. From there, the iterates will move into the region S', since
outside of S' Y Se the algorithm chooses a descent direction in each step (see the argument around
equation (16)). The region S' is covered by a ball of radius r, by Lemma 9, determined by the noise
and .
with
β “ 4Kd3? + 13ω2d{2{}x*},	(13)
then the algorithm makes progress in the sense that fpxi'1q—fpxiq is smaller than a certain negative
value. The set Se is contained in two balls around x* and —ρχ*, whose radius is controlled by β:
Lemma 9. For any β ≤ ^1^,
Sβ U B(x*, 5000d6β}x*}2) Y B(—PdX*, 500d11aβ}x*}2).	(14)
Here, Pd > 0 is defined in the proof and obeys Pd → 1 as d → 8.
Note that by the assumption ω ≤ }x*}K12—— and Kd45? ≤ 1, our choice of β in (13) obeys
β ≤ 642ldi2 for sufficiently small K1, K, and thus Lemma 9 yields:
Sβ u B(x*,r) Y B(—pdX*,-\Jr}x*} d8)∙
were we define the radius r “ K2d9∕}χ*} + K3d6ω2d/2, where K2, K3 are numerical constants.
Note that hat the radius r is equal to the right hand side in the error bound (4) in our theorem. In
order to guarantee that the algorithm converges to a ball around x*, and not to that around —Pdx*,
we use the following lemma:
Lemma 10. Suppose that the WDC holds with e < 1∕(16πd2)2. Moreover suppose that Enoise holds,
and that ω in the event Enoise obeys z´d/j.* 眄 ≤ K9∕d2, where K9 < 1 is a universal constant.
Then for any φd P rPd, 1s, it holds that
f (χ) < f (y)	(15)
for all x P B(φdX*, K3d~10}x*}} and y P B(一φdx*, K3dT0}x*}), where K3 < 1 is a universal
constant.
In order to apply Lemma 10, define for convenience the two sets:
S' =Sβ x B(x*,r), and
Se =Sβ X B(—Pdx*,
x
13
Under review as a conference paper at ICLR 2019
By the assumption that Kd45?! ≤ 1 and ω ≤ KIdT62—d{2 }x*}, We have that for sufficiently
small K1, K,
S' 三 B(x*,K3dT0}x*}) and Se 三 B(一p.x*,/-叫*}).
Thus, the assumptions of Lemma 10 are met, and the lemma implies that for any X P Se and
y P S', it holds that f (x) > f (y). We now show that the algorithm converges to a point in S'.
This fact and the negation step in our algorithm (line 3-5) establish that the algorithm converges to
a point in Sβ' if we prove that the objective is nonincreasing with iteration number, which will form
the remainder of this proof.
Consider i such that xi R Sβ. By the mean value theorem (Clason, 2017, Theorem 8.13), there is
a t P [0,1S such that for Xi “ Xi — tavxi there is a v^iι P Bf (Xi), where Bf is the generalized
subdifferential of f, obeying
f(xi ´ αVχJ — f (xi) =Xvxi, ´αVχi〉
“ Xvxi , ´ αvXi〉' Xvxi - vxi , ´ αvXi〉
≤ ´ α}vxi}2 + ɑ}vxi ´ vxi}}vxi}
=´ α}vxi}(}vxi} ´ }v^i ´ vxi}q.	(16)
In the next subsection, we guarantee that for any t P [0,1], v^i with Xi “ xi — tavxi is close to vxi:
5 d2
}vxi ´ vxi} W + αK72d)}vxi}, for all vxi P Bf(Xi).	(17)
Applying (17) to (16) yields
f(Xi ´ avxiq ´ f(Xi) ≤ ´Wα}vxi}2,
where we used that αK7d2 ≤ 吉,by our assumption on the stepsize α being sufficiently small.
Thus, the maximum number of iterations for which Xi R Se is f (Xo)12{(α mini }vxi }2). We next
lower-bound } vxi}. We have that on Enoise, for all X R Se, with β given by (13).
∣∣vx}2 ≥ }hx} — }hx — vx}
> 2ed maχ(∣X}, }x*}) (β — Kid3? —
>
2d{2
ω R
2edmax(∣X}, }x*})
^3Kd3?+12ω 7)
(18)
> 2ed}X*}3Kd3?
where the second inequality follows by the definition of Se and Lemma 6, and the third inequality
follows from our definition of β in (13). Thus,
f(Xi — αvxi) — f(Xi) ≤ —αK52e2dd6e}Xφ|2 ≤ —2edd4K6e∣X*}2
where we used α “ K4 22. Hence, there can be at most JfpxOq2	iterations for which Xi R Se.
4 d	K6d4e}x* }2	Te
In order to conclude our proof, we remark that once Xi is inside a ball of radius r around x*, the
iterates do not leave a ball of radius 2r around χ*. To see this, note that by (12) and our choice of
stepsize,
K
α}vxi} ≤ -d maχ(}χi}, }χ*}).
This concludes our proof.
The remainder of the proof is devoted to prove the lemmas used in this section.
14
Under review as a conference paper at ICLR 2019
A.3 Proof of Equation (17)
Our proof relies on hx being Lipschitz, as formalized by the lemma below, which is proven in
Section A.9:
Lemma 11. For any x,y R B(0, K0}x*}), where K° and K4 are numerical constants,
}hχ ´ hy} ≤ K4dd2}x ´ y}.
By Lemma 11, for all t P [0,1] and i > N (recall that by Lemma 8, after at most N steps,
Xi ‰ B(0, K0}χ*})):
}h^i ´ hχi }≤ K4dd2}Xi ´ Xi},	(19)
where Xi “ Xi — tavχ.. Thus, We have that on Enoise, for any Vxi P Bf (Xi) by Lemma 6,
l∣vXi — vxi} ≤}v^i — h^J ' }h^i — hxi} ' }hxi — VxJ
d3 ≠	ω , K4d2	d3 ≠	ω
WKI	2d	maX(}Xi}, }x*}) '	2d{2	'	2d~ }Xi —	Xi}	'	KI	2d	maX(}Xi},	}x*}) '	2d{2
WKI曾 max(}Xi}+ a}Vxi},}X*}) +
K4d2 ,,~ ll 〜d3?	ω
2d α}vxi} + KI 2d max(}Xi}, }x*}) + 22d{2
〜d3?	αdK	K4d2	,,~ ll	^Ko∕d2 ll ll ________
WKI 2d	(2 +- -2d~ J max(}Xi}, }x*}) +	2d~α}vxi} + 2_2d- }x*}	QO)
where the second inequality is from Lemma 6 and Equation 19, and the fourth inequality is from (12)
and the assumption -彘T W Kg∕d2.
2	/ ∣∣x*∣∣2
Combining (20) and (18), we get that
l∣vxi — vxi} W
5	d2
(6 + -K7 巩)
}Vxi},
with the appropriate constants chosen sufficiently small. This concludes the proof of Equation (17).
A.4 Proof of Lemma 8
First suppose that Xi P B(0, 2KQ}x* }). We show that after a polynomial number of iterations N,
we have that Xi`N R B(0,2K}x* }). Below, we prove that
(X,Vxi V 0 and }Vx} >	1	}x*} for all X P B(0, 2Ko}x*}).	(21)
2d 16π
It follows that for any Xi P B(0, 2Ko }x*}), Xi and the next iterate produced by the algorithm,
Xi'1 “ Xi — αvxi, form an obtruse triangle. As a consequence,
}Xi'i}2 >}Xi}2 + ɑ2}Vxi}2
》HXi}2 + α2 (2d 16∏)2 }x*}\
where the last inequality follows from (21). Thus, the norm of the iterates Xi will increase until after
'2K0Od16π)2 iterations, we have Xi'N R B(0, 2Ko}x*}).
The proof of the lemma is concluded by showing that
Xi R B(0,2Ko }x* }) implies xi' 1 R B(0, K0 |x* Il q	(22)
As a consequence, in a polynomial number N of steps, for each iterate, we have that Xi R
B(0, Ko}x*}), for all ieN, as claimed.
15
Under review as a conference paper at ICLR 2019
We next prove the implication (22). Consider Xi R B(0,2KQ }x*}), and note that
}xi'1} “ }xi ´ αvXi} > }xi} ´ α}vXi}
dK
J }xi} ´ αFmaχp}xi}, }x*}q
> }Xi} ´ αdK}xi}
j } i"	2d 2K0
J }xi} ´ 1 }xi}
where the second inequality follows from (12), the third inequality from }xi} J 2K0}x* }, and
finally the last inequality from our assumption on the stepsize α. This concludes the proof of (22).
Proof of (21): It remains to prove (21). We start with proving <x,Vχ) < 0. For brevity of notation,
let Λz = ∏1=d Wi,+,z. We have
xTVx =(ATΛχx ´ ΛTΛχ* x* + ΛTη, x)
WH2-d}x}2 ´ 4∏2d}x}}x*} + }x}2⅛
≤}χ} ^ 122∕χ}+1{2d⅛*b 4∏ 2d }χ*)
≤}x}2d ^2}x} ´ 8∏}x*)
The first inequality follows from (8) and (9), and the second inequality follows from our assumption
on ω. Therefore, for any X P B(0,11∏}x*}),〈x, Vxi < 0, as desired.
We next show that, for any X P B(0,高}x*})
}vx} =}ATAxX ´ ATAx*x* + ATη} J }ATAx*x*} ´ }ATAxx} ´ }ATη}
1 1	13 1	w
J4∏2d}x*}- 122d}x}- 2d∕2
J2d ^8∏ ´ 16∏)}x*}∙
where the second inequality is from (8) and (9). This concludes the proof of (21).
A.5 Proof of Lemma 5
Let Ax = ∏1=dWi,+,x. We have that
}qx}2 = >Axη>2 ≤ }Ax}2}PΛχη}2,
where PΛx is a projector onto the span ofAx. As a consequence, }PΛxη}2 is χ2 -distributed random
variable with k-degrees of freedom scaled by σ{n. A standard tail bound (see (?, p. 43)) yields that,
for any β J k,
P [}PΛχη}2 J 4β] ≤ 2e^.
Next, we note that by applying Lemmas 13-14 from (Hand & Voroninski, 2018, Proof of Lem. 15))4,
with probability one, that the number of different matrices Ax can be bounded as
|tAx|x ‰ 0}∣ = ∣ {∏1=dWi,+,x∣x ‰ 0( | ≤ 10d2PndndT ...nd)k W PndndT ...nd)2k,
where the second inequality holds for logp10q W k{4 logpn1q. To see this, note that
PndndT ...nd)k J i0d2 is implied by k(d log(nι) + (d — 1) log(n2) + ...log(nd)) J
kd2{4 logpn1q J d2 logp10q. Thus, by the union bound,
p[}Pλx η}2 W 16k log(ndnd´1 ...nd), forall x] J 1 — 2ei2k log(n),
4The proof in that argument only uses the assumption of independence of subsets of rows of the weight
matrices.
16
Under review as a conference paper at ICLR 2019
where n “ n&. Recall from (9) that }Λχ} ≤ 12. Combining this inequality with }亮}2 ≤
}Λx}2}PΛx η}2 concludes the proof.
A.6 Proof of Lemma 9
We now show that hχ is away from zero outside of a neighborhood of x* and —ρdχ*. We prove
Lemma 9 by establishing the following:
Lemma 12. Suppose 64d6 ? ≤ L Define
d´1
ρd :“ Σ
i“0
sin θi
π
(h T
∖j=i'1
where θo “ π and θi = g(%_ι). If X P Se, then we have that either
∣θo∣ ≤ 32d4β and ∣}x}2 — }x*}2∣ ≤ 132d6β}x*}2
or	_
θ ´ π∣ ≤ 8πd4√β and	∣}x}2 ´ }x*}2Pd∣ ≤ 200d7-*}2.
In particular, we have
Se U B(x*, 5000d6β}x*}2) Y B(一ρdx*, 500d11aβ}x*}2)∙	(23)
Additionally, Pd → 1 as d → 8.
Proof. Without loss of generality, let }x*} “ 1, x* “ eι and X “ r cos θo ∙ eι ' r Sinθ ∙ e2 for
θo P [0, π]. Let x P Se.
First we introduce some notation for convenience. Let
d—1	d—1	d—1
ξ “ ∏ π—,Z =X sinθi ∏ πz-θj, r =}x}2, M “ max(r,1).
i = 0	穴	i = 0	穴 j=i'1	穴
Thus, hx = — 2dξx0 ' 2d (r — Z)x. By inspecting the components of hx, we have that X P Se
implies
| — ξ ' cosθo(r — Zq∣ ≤ βM	(24)
| sinθo(r — Zq∣ ≤ βM	(25)
Now, we record several properties. We have:
θ P [0, π{2S for i > 1		
θi	< θi´ι for i21	
∣ξ∣	≤ 1	(26)
|Z|	≤ — sin θo	(27)
θqi	π 3π ≤ i+i for i20	(28)
θqi	π > i+j for i20	(29)
d—1	W ξ=∏ T	>上电d-3 π	(30)
i=0 π		
θo = π + Oι(δ)	0 θi = θi + Oι(iδ)	(31)
θo = ∏ ` Oι(δ)	^∣ξ∣≤ δ π	(32)
θo = ∏ ` Oι(δ)	d2 δ =Z = Pd + Oι(3d δ) if	 ≤ 1 π	(33)
17
Under review as a conference paper at ICLR 2019
We now establish (28). Observe 0 < g(θ) ≤ '3∏ ' 1) 1 “： g(θ) for θ P (0,∏]. As g and g are
monotonic increasing, we have q “ g°i(qo) “ goi(∏) ≤ goi(∏) “ '3∏ ' ∏)1 “ 瑞.Similarly,
g(θ)》(∏ ' θ)´1 implies that q》言,establishing (29).
5 T	, T1 1 /C C' T 一	/AC' FK - X	1
We now establish (30). Using (28) and θi ≤ θi, we have
d´1	d´1	Q
n´i ´ 勺i⅛” d´3,
i“1	i“1
where the last inequality can be established by showing that the ratio of consecutive terms with
respect to d is greater for the product in the middle expression than for d´3.
We establish (31) by using the fact that ∣g1(θ)∣ ≤ 1 for all θ P [0, ∏S and using the same logic as
for (Hand & Voroninski, 2018, Eq. 17).
2
We now establish (33). As θo “ ∏ ' Oι(δ), we have θi “ θi ' Oι(iδ). Thus, if d∏δ ≤ 1,
d´1	d´1	d´1
∏ — “ n´T+O1(2∏)) = (n <jHi
j=i ' 1	j=i ' 1	j=i 十 1
So
d´1	d´1
Z=∑(警+O1(∏δq)[C∏ T)+O1(d2δ)l	(34)
“ Pd + O1 (d2δ/π + d3δ{π + d4δ2∕π)	(35)
“ ρd + O1(3d3δq.	(36)
Thus (33) holds.
Next, we establish that X P Se 0 r ≤ 4d, and thus M ≤ 4d. Suppose r > 1. Atleast one of the
following holds: | sinθ0∣21/?2 or | CoSθ0∣21/?2. If | sinθ0∣21/?2 then (25) implies that
|r —	Z|	≤	?2er. Using (27), we get r ≤	d{∏-	≤	d/2 if β <	1/4.	If | cosθ0∣21/?2,	then
1	´ 2β
(24)	implies that |r — Z| ≤ ?2(尸/+ ∣ξ∣). Using (26), (27), and β < 1/4, we get r ≤ 尸?)十Z ≤
1 ´ V 2 β
：?2e ≤ 4d. Thus, we have X P Se = r ≤ 4d = M ≤ 4d.
Next, we establish that we only need to consider the small angle case (θ0 « 0) and the large angle
case (θo « ∏), by considering the following three cases:
(CaseI) sin θo ≤ 16d4β: Wehave θ0 = O1 (32d4βq or θ0 = ∏+O1(32d4β),as 32d4β V 1.
(Case II) |r — Z| V ”M: Applying case II to inequality (24) yields ∣ξ∣ ≤ 2?βM. Using
(30), we get θo = ∏ + θ1(2∏d3∙βM).
(Case In) sin θo > 16d4β and |r — Z|2√βM: Finally, consider Case III. By (25), we
have |r — Z| ≤ SinMM-. Using this inequality in (24), we have ∣ξ∣ ≤ βM + SinM- ≤ Sβθ- <
8d´4M ≤ 2d´3, where the second to last inequality uses sin θo > 16d4β and the last
inequality uses M ≤ 4d. By (30), we have ɪ´θ0 d´3 ≤ ξ ≤ 2 d´3, which implies that
θoen/2. Now, as |r — Z|2√βM, then by (25), we have | sin θ0∣ ≤ √β. Hence,
θo = ∏ + O1(2√β), as θoen/2 and as β V 1.
At least one of the Cases I,II, or III hold. Thus, we see that it suffices to consider the small angle
case θo = O1(32d4β) or the large angle case θ0 = n + O1(8∏d4√β).
Small Angle Case. Assume θo = O1(δ) with δ = 32d4β. As θi ≤ θo ≤ δ for all i, we have
12ξ》(1 — ∏)d = 1 + O1(2∏d) provided 6d/n ≤ 1/2 (which holds by our choice δ = 32d4β by
18
Under review as a conference paper at ICLR 2019
assumption 64d6?e ≤ 1). By (27), We also have Z = Oi(dδ). By (24), We have
| ´ ξ ' cosθo(r - Zq∣ ≤ βM.
一	_	,-2 ,
Thus, as cos θ0 “ 1 ' Oι(θo∕2) “ 1 ' Oι(δ2/2),
´´1 ' O1("/ ' (1 ' OiG^))(r ' Oip-qq “ Oip4dβq,
π	ππ
and r ≤ M ≤ 4d (shown above) provides,
r ´ 1 “ Oι(4dβ + 2δd + δd + 2δd4d + 空2)
π π π	π2
“ Oi(4βd + 4δd2).
(37)
(38)
By plugging in that δ “ 32d4β, We have that r ´ 1 “ Oi(132d6β), Where We have used that
32∏5β & 1/2.
Large Angle Case. Assume θo = ∏ + Oi(δ) where δ = 8∏d4”. By (32) and (33), we have
ξ “ Oι(δ∕π), and we have Z = Pd + Oi(3d3δ) if 8d6?β ≤ 1. By (24), we have
l´ ξ + cos θo(r ´ ζ )∣≤ βM,
so, as cosθo = 1 — Oι(θ0∕2),
Oi(δ∕∏) + (1 + Oi(δ2∕2))(r — ρd + Oι(3d3δ)) “ Oi(βM),
and thus, using r ≤ 4d, ρd ≤ d, and δ = 8πd4yβ ≤ 1,
53
r — ρd = Oi(βM + δ∕π + 3d3δ + -δ2d + -d3δ3)	(39)
“ Oi(4βd + δ(1 + 3d3 + 5d + 3d3))	(40)
“ Oi(200d7aβ)	(41)
To conclude the proof of (23), we use the fact that
}x ´ x*}2 ≤ ∣}x}2 ´ }x*}2∣ + (}x*}2 + ∣}x}2 ´ }x*}2∣)θ0.
This fact simply says that if a 2d point is known to have magnitude within ∆r of some r and is
known to be within angle ∆θ from 0, then its Euclidean distance to the point of polar coordinates
(r, 0) is no more than ∆r + (r + ∆r)∆θ.
qq
Finally, we establish that Pd → 1 as d → 8. Note that pd`i “(1 — θd)ρd +——∏θd and ρo = 0.
It suffices to show Pd → 0, where Pd := 1 — Pd. The following recurrence relation holds: Pd “
q	qq
(1 ——d´1 )Pd—i + "	∏-d´1, with Po = 1. Using the recurrence formula (Hand & Voroninski,
—— 一 ，一 一、、	,,	γ	,
2018, Eq. (15)) and the fact that θ0 = π, we get that
Pd
d
=Σ
i“i
θi´i — Sin θi´i
π
d	θq
"j)
j=i'i
(42)
using (29), we have that
dd
∏J1 -j卜 ∏J1 -
j=i'i	j=i'i
j)
d
“exp(一 Σ j)
j=i'i J
W exp(—「"
i`i
i + 1
d + 1
Using (28) and the fact that θi´i — Sin θi´i ≤ qτ∕6, we have that Pd ≤ Xd=i θ´1 ∙ d' → 0 as
d → 8.
□
19
Under review as a conference paper at ICLR 2019
A.7 Proof of Lemma 10
Consider the function
fη (X) = fo(x)—〈G(x) — G(x*),η),
and note that f (x) = f (x) ' }η}2. Consider X P B(φdx*,夕}x*}), for a 夕 that will be specified
later. Note that
KG(X) ´ G(χ*),ηil W KnI= dWi,+,χχ,η) | + Kn1=dWi,+,x*χ*,η) |
=|〈x, (n1“dwi,',x)tn) | + |〈x*, (n1=dWi,',χ*)tη) |
≤(}χ} + }χ*}) 2d{2
≤ 3lχ*} + }χ*}q2d{2,
where the second inequality holds on the event Enoise, by Lemma 5, and the last inequality holds by
our assumption on x. Thus, for X P B(φdX*,夕}x*})
fη(x) ≤Efo(x) + ∣fo(x) ´ Efo(x)| + |〈G(x) — G(x*),ηi∣
W2d'ι φdd ´ 2φd + K3的)}χ*}2 + 2d'1 }χ*}2
+ ^1⅞4^}x}2 + 矶 + 4e> 48d3?}x}}x*} + ^1⅛4^}x*}2
+ (2 }x*} + }x*}q 2d{2
W2d'ι (φd ´ 2φd + K3如)}x*}2 + 2d'ι }x*}2
+ ¾4^(Φd + 02}x*}2 + - + %: 48d3?(φd + 0}x*}2 + ¾4^}x*}2
+ (2 }x*} + }x*}q 2d{2
W 2d'l (1 + φd — 2φd + Kde + 68d2?) + (2}x*} + }x*}q2d{2	(43)
where the last inequality follows from e < ?e, Pd W 1, 4ed < 1,夕 < 1 and assuming 夕=e.
Similarly, We have that for any y P B(一φdx*, q}x*})
fη (y"E[f (y)S — |f (y) — Ef (y 川— |〈G(x) — G(χ*),ηi∣
22d'i 'φd— 2φdρd— 10d32)}χ*}2 + 2d'i}χ*}2
—^ ^1'4^}y}2 + e(1 + - 48d3? }y}}χ*} + e⅛4e⅛χ*}2)
—(2 }χ*} + }x*}) 2d{2
22d'1	'1 +	φd —	2φdρd —	10d32—68d2√e)—(2}χ*} +	}χ*})2d{2	(44)
Using e < √e, Pd W 1, 4ed < 1,夕 < 1 and assuming 夕 =e, the right side of (43) is smaller than
the right side of (44) if
φd ― Pdφd ― 13}η}2
—W l (125 + K⅛)d3	,
(45)
We can establish that:
Lemma 13. For all d22 ,that
1{ 'Kι(d + 2)2) W 1 — Pd W 250/(d + 1).
Thus, it suffices to have 夕 =e = K and 13}¾ W K W 2 κK'2产 for an appropriate universal
constant K9, and for an appropriate universal constant K3.
20
Under review as a conference paper at ICLR 2019
A.8 Proof of Lemma 13
It holds that
}x - y} N 2sin(θχ,y/2) min(}x}, }y}),
sin(θ∕2) N θ∕4,
而g(θ) P r0,1s
dθ
log(1 + x) W x
log(1 - x) N -2x
@x, y
Ve p ro, ∏s
vθ p ro, ∏s
@x P r-0.5, 1s
@x P r0, 0.75s
where θx,y “ =px, yq. We recall the results (36), (37), and (50) in (Hand & Voroninski, 2018):
3π
θiW -	and
1 - Pd = ∏^1 Il - ∏)
i=1
Therefore, we have for all 0 W i W d - 2,
d´1
+Σ
i=1
θi - sin θi
π
∏1 ^1 - θj)
j=i'1
d´1
d´1
(46)
(47)
(48)
(49)
(50)
∏1(1- jw ∏(1 —"
j“i'1、	J	j“i'1 '
Sd— 1
6乙 j“i'i
log(1-j⅛q W e
Yld— 1	1
乙j=i'1 j'1
eTd+1 s⅛ ds =上
d + 1,
d´1
d´1
∏1(1- j > ∏1(1-j⅛) “
j=i'1	j=i'1 ∖
eɪd´1+! log(1—j'33)2e
γιd-1	6
乙j=i'1 j÷3
e”4ds “ ^片)6，
π
θi N E @i N 0






w
>
where the second and the fifth inequalities follow from (49) and (50) respectively. Since π3{(12(i +
1)3) W @3/12 W θi — Sin θi W θ3∕6 W 27π3/(6(i + 3)3), we have that for all d23
and
d´1
Σ
i=1
27π 3 i + 2
6pi ` 3q3 d`1
3π5
250
d + 1 + 4(d + 1)、d + 1
Pd + 2)
6	d´1
+Σ
i=1
π3
12pi ` 3q3
L + 3)6 >	1
d+2	N Kι(d + 2)2 ,
2
1 - PdW d+1 +
1 - Pd》(
3
w
2
where We use ^8=4 5 W ∏62 and ∑n=ι i3 = O(n4).
A.9 Proof of Lemma 11
To establish Lemma 11, we prove the following:
Lemma 14. For all x, y ‰ 0,
1	6d + 4d2
}hx - hy} w (2d +	π2d max (
R,十)}x*)x- y}
Lemma 11 follows by noting that if x,y R B(0,r}x*}), then }hχ - hy} W (2d + 6∏'4d2) }x - y}.
ProofofLemma 14. For brevity of notation, let ζj,z = "：二 π7θi,z. Combining (46) and (47)
gives lθo,χ - θ0,y | W 4max (}⅛, }⅛) }
for all iej. It follows that


X - y}. Inequality (48) implies |&,x - &,y| W j - j|
Il hx - hy} W 2d }x - y} + 2d lζ0,χ - ζ0,yl}x*}
T1
1
+ 2d
d´1	d´1
vɔ Sin θi,χ	- L Sin θi,y
〉,——-ζi'i,χX - X ——-
i=0 π	i=0	π
l	jh
T2
ζi'1,yy }x*}∙
n
(51)
21
Under review as a conference paper at ICLR 2019
By Lemma 15, we have
TI ≤ ∏l%x ´ θ0,y |
4d
——max
π
6,
}x ´ y}.
(52)
<
Additionally, it holds that
d´ 1
T2= Σ
i“0
sin θi,χ
Ci'1,xX ´
sin θi,χ
ζi'1,xy '
sin θi,χ
d´1
Zi'i,χy ´ ∑
Sin 4,y
Zi'1,y y
i“0
≤d}x ´ y}
d´1
Σ
i = 0
l
sin θi,χ
d´ 1
Ci'1,x - X
i=0
Sin 4,y
ζi'l,y .
(53)
X
T3
π
π
π
π
`
π
π
n
We have
d´ 1
T3 ≤ Σ
i=0
d´1
≤ Σ
i“0
sin L
--------Qi'1
π
1(
,x
d´i´1
sin θi,χ
ζi'i,y
"i ―1,x ´ θi´1,y ∣) '
sin 6i,χ
ζi'i,y ´
s⅛ Zi'i
11 sin θi,χ — sin θi,y |
π
,y
π

π
`
π
π
d2	4d2	1	1
≤Tlθ0,x ´θ0,y| ≤ 丁max (1,nJ }x´y}.
(54)
Using (46) and (47) and noting }X 一 y} ≤ θx,y yield
Ilx ´ y} ≤ θχ,y ≤ 2max (同,而)}x ´ y}.
(55)
Finally, combining (51), (52), (53), (54) and (55) yields the result.
□
Lemma 15. Suppose ai,b P [0, π] for i = 1,...,k, and 旧一bi | ≤ |aj ´ bj |, Viej. Then it holds
that
k
π
i=1
π ´ ai
k
-π
i=1
π ´ bi
k
≤ ∏la1 ´ b1|.
π
π
Proof. Prove by induction. It is easy
inequality holds with k “ t ´ 1. Then
to
verify that the inequality holds if k “ 1. Suppose the
t
π
i“1
π ´ ai
t
-π
i=1
π ´ bi
t
π
i=1
π ´ ai	π ´ at
t´1
π
i=1
π ´ bi
π ´ at
t´1
π
i“1
π ´ bi
t
-π
i=1
π ´ bi
π
π
≤
`

π
π
π
π
π
π
≤----∣aι ´ bι∣ '—|at ´ bt| ≤ -∣aι ´ bι∣.
π
π
π
□
22