Under review as a conference paper at ICLR 2019
Volumetric Convolution:	Automatic Repre-
sentation Learning in Unit Ball
Anonymous authors
Paper under double-blind review
Ab stract
Convolution is an efficient technique to obtain abstract feature representations
using hierarchical layers in deep networks. Although performing convolution in
Euclidean geometries is fairly straightforward, its extension to other topological
spaces—such as a sphere (S2) or a unit ball (B3)—entails unique challenges.
In this work, we propose a novel ‘volumetric convolution’ operation that can
effectively convolve arbitrary functions in B3 . We develop a theoretical framework
for volumetric convolution based on Zernike polynomials and efficiently implement
it as a differentiable and an easily pluggable layer for deep networks. Furthermore,
our formulation leads to derivation of a novel formula to measure the symmetry of
a function in B3 around an arbitrary axis, that is useful in 3D shape analysis tasks.
We demonstrate the efficacy of proposed volumetric convolution operation on a
possible use-case i.e., 3D object recognition task.
1	Introduction
Convolution-based deep neural networks have performed exceedingly well on 2D representation
learning tasks (Krizhevsky et al., 2012; He et al., 2016). The convolution layers perform parameter
sharing to learn repetitive features across the spatial domain while having lower computational cost
by using local neuron connectivity. However, most state-of-the-art convolutional networks can only
work on Euclidean geometries and their extension to other topological spaces e.g., spheres, is an open
research problem. Remarkably, the adaptation of convolutional networks to spherical domain can
advance key application areas such as robotics, geoscience and medical imaging.
Some recent efforts have been reported in the literature that aim to extend convolutional networks
to spherical signals. Initial progress was made by Boomsma & Frellsen (2017), who performed
conventional planar convolution with a careful padding on a spherical-polar representation and its
cube-sphere transformation (Ronchi et al., 1996). A recent pioneering contribution by Cohen et al.
(2018) used harmonic analysis to perform efficient convolution on the surface of the sphere (S2)
to achieve rotational equivariance. These works, however, do not systematically consider radial
information in a 3D shape and the feature representations are learned at specified radii. Specifically,
Cohen et al. (2018) estimated similarity between spherical surface and convolutional filter in S2,
where the kernel can be translated in S2. Furthermore, Weiler et al. (2018) recently solved the more
general problem of SE(3) equivariance by modeling 3D data as dense vector fields in 3D Euclidean
space. In this work however, we focus on B3 to achieve the equivariance to SO(3).
In this paper, we propose a novel approach to perform volumetric convolutions inside unit ball (B3)
that explicitly learns representations across the radial axis. Although we derive generic formulas
to convolve functions in B3, we experiment on one possible use case in this work, i.e., 3D shape
recognition. In comparison to closely related spherical convolution approaches, modeling and
convolving 3D shapes in B3 entails two key advantages: ‘volumetric convolution’ can capture both
2D texture and 3D shape features and can handle non-polar 3D shapes. We develop the theory of
volumetric convolution using orthogonal Zernike polynomials (Canterakis, 1999), and use careful
approximations to efficiently implement it using low computational-cost matrix multiplications. Our
experimental results demonstrate significant boost over spherical convolution and that confirm the
high discriminative ability of features learned through volumetric convolution.
Furthermore, we derive an explicit formula based on Zernike Polynomials to measure the axial
symmetry of a function in B3, around an arbitrary axis. While this formula can be useful in many
1
Under review as a conference paper at ICLR 2019
function analysis tasks, here we demonstrate one particular use-case with relevance to 3D shape
recognition. Specifically, we use the the derived formula to propose a hand-crafted descriptor that
accurately encodes the axial symmetry of a 3D shape. Moreover, we decompose the implementation
of both volumetric convolution and axial symmetry measurement into differentiable steps, which
enables them to be integrated to any end-to-end architecture.
Finally, we propose an experimental architecture to demonstrate the practical usefulness of proposed
operations. We use a capsule network after the convolution layer as it allows us to directly compare
feature discriminability of spherical convolution and volumetric convolution without any bias. In other
words, the optimum deep architecture for spherical convolution may not be the same for volumetric
convolution. Capsules, however, do not deteriorate extracted features and the final accuracy only
depends on the richness of input shape features. Therefore, a fair comparison between spherical and
volumetric convolutions can be done by simply replacing the convolution layer.
It is worth pointing out that the proposed experimental architecture is only a one possible example
out of many possible architectures, and is primarily focused on three factors: 1) Capture useful
features with a relatively shallow network compared to state-of-the-art. 2) Show richness of computed
features through clear improvements over spherical convolution. 3) Demonstrate the usefulness of the
volumetric convolution and axial symmetry feature layers as fully differentiable and easily pluggable
layers, which can be used as building blocks for end-to-end deep architectures.
The main contributions of this work include:
•	Development of the theory for volumetric convolution that can efficiently model functions in B3 .
•	Implementation of the proposed volumetric convolution as a fully differentiable module that can
be plugged into any end-to-end deep learning framework.
•	The first approach to perform volumetric convolution on 3D objects that can simultaneously model
2D (appearance) and 3D (shape) features.
•	A novel formula to measure the axial symmetry of a function defined in B3 , around an arbitrary
axis using Zernike polynomials.
•	An experimental end-to-end trainable framework that combines hand-crafted feature representation
with automatically learned representations to obtain rich 3D shape descriptors.
The rest of the paper is structured as follows. In Sec. 2 we introduce the overall problem and our
proposed solution. Sec. 3 presents an overview of 3D Zernike polynomials. Then, in Sec. 4 and
Sec. 5 we derive the proposed volumetric convolution and axial symmetry measurement formula
respectively. Sec. 6.2 presents our experimental architecture, and in Sec. 7 we show the effectiveness
of the derived operators through extensive experiments. Finally, we conclude the paper in Sec. 8.
2	Problem definition
Convolution is an effective method to capture useful features from uniformly spaced grids in Rn ,
within each dimension of n, such as gray scale images (R2), RGB images (R3), spatio-temporal
data (R3) and stacked planar feature maps (Rn). In such cases, uniformity of the grid within each
dimension ensures the translation equivariance of the convolution. However, for topological spaces
such as S2 and B3, it is not possible to construct such a grid due to non-linearity. A naive approach to
perform convolution in B3 would be to create a uniformly spaced three dimensional grid in (r, θ, φ)
coordinates (with necessary padding) and perform 3D convolution. However, the spaces between
adjacent points in each axis are dependant on their absolute position and hence, modeling such a
space as a uniformly spaced grid is not accurate.
To overcome these limitations, we propose a novel volumetric convolution operation which can
effectively perform convolution on functions in B3. Itis important to note that ideally, the convolution
in B3 should be a signal on both 3D rotation group and 3D translation. However, since Zernike
polynomials do not have the necessary properties to automatically achieve translation equivariance,
we stick to 3D rotation group in this work and refer to this operation as convolution from here
onwards. Fig. 1 shows the analogy between planar convolution and volumetric convolution. In
Sec. 3, we present an overview of 3D Zernike polynomials that will be later used in Sec. 4 to develop
volumetric convolution operator.
2
Under review as a conference paper at ICLR 2019
3	3D Zernike Polynomials
3D Zernike polynomials are a complete and orthogonal set of basis functions in B3, that exhibits
a ‘form invariance’ property under 3D rotation (Canterakis, 1999). A (n, l, m)th order 3D Zernike
basis function is defined as,
Zn,l,m = Rn,l (r)Yl,m (θ, φ)	(1)
where Rn,l is the Zernike radial polynomial (Appendix D.3), Yl,m (θ, φ) is the spherical harmonics
function (Appendix D.1), n ∈ Z+, l ∈ [0, n], m ∈ [-l, l] and n - l is even. Since 3D Zernike polyno-
mials are orthogonal and complete in B3, an arbitrary function f(r, θ, φ) in B3 can be approximated
using Zernike polynomials as follows.
∞n l
f(θ, φ, r) = XX X
Ωn,l,m(f )Zn,l,m(θ, φ,r)	⑵
n=0 l=0 m=-l
where Ωn,ι,m(f) could be obtained using,
f(θ, φ, r)z^,i,mr2sinφdrdφdθ
(3)
where * denotes the complex conjugate. In Sec. 4, we will derive the proposed volumetric convolution.
4	VOLUMETRIC CONVOLUTION OF FUNCTIONS IN B3
When performing convolution in B3, a critical problem which arises is that several rotation operations
exist for mapping a point p to a particular point p0 . For example, using Euler angles, we can
decompose a rotation into three rotation operations R(θ, φ) = R(θ)yR(φ)zR(θ)y, and the first
rotation R(θ)y can differ while mapping p to p0 (if y is the north pole). However, if we enforce the
kernel function to be symmetric around y, the function of the kernel after rotation would only depend
on p and p0 . This observation is important for our next derivations because we can then uniquely
define a 3D rotation on kernel in terms of azimuth and polar angles.
Let the kernel be symmetric around y and f(θ, φ, r), g(θ, φ, r) be the functions of object and kernel
respectively. Then we define volumetric convolution as,
1	2π	π
f *g(α,β) := hf (θ, φ,r), τ(α,β) (g(θ,φ, r)» =/	/	/ f (θ, φ, r), τ(α,β)(g(θ, φ, r)) sin φdφdθdr
00	0
(4)
where τ(α,β) is an arbitrary rotation, that aligns the north pole with the axis towards (α, β) direction
(α and β are azimuth and polar angles respectively). Eq. 4 is able to capture more complex patterns
compared to spherical convolution due to two reasons: 1) the inner product integrates along the radius
and 2) the projection onto spherical harmonics forces the function into a polar function, that can
result in information loss.
In Sec. 4.1 we derive differentiable relations to compute 3D Zernike moments for functions in B3.
4.1	SHAPE MODELING OF FUNCTIONS IN B3 USING 3D ZERNIKE POLYNOMIALS
Instead of using Eq. 3, we derive an alternative method to obtain the set {Ωn,ι,m}. The motivations are
two fold: 1) ease of computation and 2) the completeness property of3D Zernike Polynomials ensures
that limn→∞∣∣f - Pn Pl Pm Ωn,ι,mZn,ι,m∣[ = 0 for any arbitrary function f. However, since n
should be finite in the implementation, aforementioned property may not hold, leading to increased
distance between the Zernike representation and the original shape. Therefore, minimizing the recon-
struction error 工也时中 ∣∕(θ,φ,r) - f(θ,φ,r) ∣ where f(θ, φ, r) = PN PI Pm Ωnj,mZnj,m,
pushes the set {Ω%ι,m} inside frequency space, where {Ωn,ι,mJ has a closer resemblance to the
corresponding shape. Following this conclusion, we derive the following method to obtain {Ωn,ι,m}.
3
Under review as a conference paper at ICLR 2019
Figure 1: Analogy between planar
and volumetric convolutions. Top
(left to right): image, kernel and
planar convolution. Bottom (left to
right): 3D object, 3D kernel and vol-
umetric convolution. In planar convo-
lution the kernel translates and inner
product between the image and the
kernel is computed in (x, y) plane.
In volumetric convolution a 3D rota-
tion is applied to the kernel and the
inner product is computed between
3D function and 3D kernel over B3 .
Since Yl,m(θ, φ) = (-1)m J24+∏1(-m)!Pm(COSφ)dmθ, where Pm(∙) is the associated Legendre
function (Appendix D.2), it can be deduced that, Yl,-m(θ,φ) = (—1)mYjm(θ,φ). Using this
relationship we obtain Zn,l,-m(θ, φ) = (—1)mZ∖ l m(θ, φ) and hence approximate Eq. 2 as,
∞n l
f(θ,φ,r) =ΣΣΣAn,l,mRe{Zn,l,m} + Bn,l,m I mg {Zn,l,m}	(5)
n=0 l=0 m=0
where Re{Zn,l,m} and Img{Zn,l,m} are real and imaginary components of Zn,l,m respectively. In
matrix form, this can be rewritten as,
f(θ,φ,r)=Ua+Vb=f(θ,φ,r)=XC	(6)
where C is the set of 3D Zernike moments Ωn,l,m. Eq. 6 can be interpreted as an overdetermined
linear system, with the set Ωn,l,m as the solution. To find the least squared error solution to the Eq. 6
we use the pseudo inverse of X . Since this operation has to be differentiable to train the model
end-to-end, a common approach like singular value decomposition cannot be used here. Instead, we
use an iterative method to calculate the pseudo inverse of a matrix (Li et al., 2011). It has been shown
that Vn converges to A+ where A+ is the Moore-Penrose pseudo inverse of A if,
Vn+1 = Vn(3I - AVn(3I - AVn)), n ∈ Z+	(7)
for a suitable initial approximation V0 . They also showed that a suitable initial approximation would
be V0 = aAT with 0 < α < 2∕ρ(AAT), where ρ(∙) denotes the spectral radius. Empirically, we
choose α = 0.001 in our experiments. Next, we derive the theory of volumetric convolution within
the unit ball.
4.2 CONVOLUTION IN B3 USING 3D ZERNIKE POLYNOMIALS
We formally present our derivation of volumetric convolution using the following theorem. A short
version of the proof is then provided. Please see Appendix A for the complete derivation.
Theorem 1: Suppose f, g : X -→ R3 are square integrable complex functions defined in B3
so that hf, fi < ∞ and hg, gi < ∞. Further, suppose g is symmetric around north pole and
τ(α, β) = Ry (α)Rz (β) where R ∈ SO(3). Then,
1	2π π	4π ∞ n l
f (θ, φ, r), τ(α,β)(g(θ, φ, r)) sin φdφdθdr ≡4∏∑∑∑ Ωn,l,m(f)Ωn,l,o(g)Yl,m (θ,φ)
0	0	0	3 n=0 l=0 m=-l
(8)
where Ωn,l,m(f), Ωn,l,0(g) and 匕,m(θ,φ) are (n,l,m)tth 3D Zernike moment of f, (n,l,0)th 3D
Zernike moment of g, and spherical harmonics function respectively.
Proof: Completeness property of 3D Zernike Polynomials ensures that it can approximate an arbitrary
function in B3 , as shown in Eq. 2. Leveraging this property, Eq. 4 can be rewritten as,
∞ n l	∞ n0 l
f * g(θ,φ)=hXXX
On,l,m(f )Zn,l,m, τ(θ,φ)(XXX
0n0,l0,m0 (g)Zn0,l0,mO))	(9)
n=0 l=0 m=-l	n0=0 l0=0 m0 =-l
4
Under review as a conference paper at ICLR 2019
However, since g(θ, φ, r) is symmetric around y, the rotation around y should not change the function.
This ensures,
g(r, θ, φ) = g(r, θ - α,φ)	(10)
and hence,
∞ n0 l	∞ n0	l
XXX
◎n0,l0,m0 (g)Rn0,l0 (r)Yl0,m0 =XXX
◎n0 ,l0 ,m0 (g)Rn0 ,l0 (r) Yl0,m0 e 'rn OL
n0=0 l0=0 m0 =-l	n0=0 l0=0 m0=-l
(11)
This is true, if and only if m0 = 0. Therefore, a symmetric function around y, defined inside the unit
sphere can be rewritten as,
0
∞n
ΣΣCn0,l0,0(g)Zn0,l0,0	(12)
n0=0 l0=0
which simplifies Eq. 9 to,
∞ n l	∞ n0
f * g(θ,φ) = hXX X
•n,l,m(f )Zn,l,m, τ(θ,φ)(XXCn0,l0,0(g)Zn0,l0,0)i	(13)
n=0 l=0 m=-l	n0=0 l0=0
Using the properties of inner product, Eq. 13 can be rearranged as,
∞ n ∞ n0	l
f*g(θ,φ) =XXXXX
◎nKmXfX'rn,l0,0 (g) hZn,l,m, τ(θ,φ) (Zn0,l0,0)i	(14)
n=0 l=0 n0=0 l0=0 m=-l
Using the rotational properties of Zernike polynomials, we obtain (see Appendix A for our full
derivation),
∞n l
f * g(θ,φ)=4∏ xxxΩr,l,m(f )Ωr,1,0(g) 匕,m(θ,φ)	(15)
n=0 l=0 m=-l
Since We can calculate Ωr,ι,m(f) and Ωr,1,0(g) easily using Eq. 6, f * g(θ, φ) can be found using
a simple matrix multiplication. It is interesting to note that, since the convolution kernel does not
translate, the convolution produces a polar shape, which can be further convolved-if needed-using the
q 4∏ P P
E 7m⅛i
relationship f * g(θ, φ)
∖ ʌ / 7	/∕k I ∖	1	∖	1 ʌ / 7	∖
f (l,m)g(l,m)Y(i,m)(θ,φ) where, f (l,m) and g(l,m)
are the (l, m)th frequency components of f and g in spherical harmonics space. Next, we present a
theorem to show the equivariance of volumetric convolution with respect to 3D rotation group.
4.3 Equivariance to 3D rotation group
One key property of the proposed volumetric convolution is its equivariance to 3D rotation group. To
demonstrate this, we present the following theorem.
Theorem 1: Suppose f, g : X -→ R3 are square integrable complex functions defined in B3 so that
hf, fi < ∞ and hg, gi < ∞. Also, let ηα,β,γ be a 3D rotation operator that can be decomposed into
three Eular rotations Ry(α)Rz(β)Ry(γ) and τα,β another rotation operator that can be decomposed
into Ry(α)Rz(β). Suppose ηα,β,γ (g) = τα,β(g). Then, η(α,β,γ) (f) * g(θ, φ) = τ(α,β) (f * g)(θ, φ),
where * is the volumetric convolution operator.
The proof to our theorem can be found in Appendix B. The intuition behind the theorem is that if a 3D
rotation is applied to a function defined in B3 Hilbert space, the output feature map after volumetric
convolution exhibits the same rotation. The output feature map however, is symmetric around north
pole, hence the rotation can be uniquely defined in terms of azimuth and polar angles.
5	AXIAL SYMMETRY OF FUNCTIONS IN B3
In this section we present the following proposition to obtain the axial symmetry measure of a
function in B3, around an arbitrary axis using 3D Zernike polynomials.
5
Under review as a conference paper at ICLR 2019
Figure 2: Kernel representations of spherical convolution
(left) vs Volumetric convolution (right). In volumetric
convolution, the shape is modeled and convolved in B3
which allows encoding non-polar 3D shapes with texture.
In contrast, spherical convolution is performed in S2 that
can handle only polar 3D shapes with uniform texture.
Proposition: Suppose g : X -→ R3 is a square integrable complex function defined in B3 such
that hg, gi < ∞. Then, the power of projection of g in to S = {Zi } where S is the set of Zernike
basis functions that are symmetric around an axis towards (α, β) direction is given by,
nl
∣∣sym(α,β)Il= XXlI X Ωn,l,m Ym,l (α,β )112	(16)
n l=0 m=-l
where α and β are azimuth and polar angles respectively.
The proof to our proposition is given in Appendix C.
6 A Case Study: 3D Object Recognition
6.1	3D OBJECTS AS FUNCTIONS INB3
A 2D image is a function on Cartesian plane, where a unique value exists for any (x, y) coordinate.
Similarly, a polar 3D object can be expressed as a function on the surface of the sphere, where any
direction vector (θ, φ) has a unique value. To be precise, a 3D polar object has a boundary function
in the form of f : S2 → [0, ∞].
Translation of the convolution kernel on (x, y) plane in 2D case, extends to movements on the surface
of the sphere in S2 . If both the object and the kernel have polar shapes, this task can be tackled by
projecting both the kernel and the object onto spherical harmonic functions (Appendix E). However,
this technique suffers from two drawbacks. 1) Since spherical harmonics are defined on the surface of
the unit sphere, projection of a 3D shape function into spherical harmonics approximates the object to
a polar shape, which can cause critical loss of information for non-polar 3D shapes. This is frequently
the case in realistic scenarios. 2) The integration happens over the surface of the sphere, which is
unable to capture patterns across radius.
These limitations can be addressed by representing and convolving the shape function inside the unit
ball (B3). Representing the object function inside B3 allows the function to keep its complex shape
information without any deterioration since each point is mapped to unique coordinates (r, θ, φ),
where r is the radial distance, θ and φ are azimuth and polar angles respectively. Additionally, it
allows encoding of 2D texture information simultaneously. Figure 2 compares volumetric convolution
and spherical convolution. Since we conduct experiments only on 3D objects with uniform surface
values, in this work we use the following transformation to apply a simple surface function f (θ, φ, r)
to the 3D objects:
f(θ,φ,r)= r0,,
if surface exists at (θ, φ, r)
otherwise
(17)
6.2	An experimental architecture
We implement an experimental architecture to demonstrate the usefulness of the proposed operations.
While these operations can be used as building-tools to construct any deep network, we focus on three
key factors while developing the presented experimental architecture: 1) Shallowness: Volumetric
convolution should be able to capture useful features compared to other methodologies with less
number of layers. 2) Modularity: The architecture should have a modular nature so that a fair
comparison can be made between volumetric and spherical convolution. We use a capsule network
after the convolution layer for this purpose. 3) Flexibility: It should clearly exhibit the usefulness
6
Under review as a conference paper at ICLR 2019
Irl
Figure 3: Experimental ar-
chitecture: An object is
first mapped to three view
angles. For each angle, ax-
ial symmetry and volumet-
ric convolution features are
generated for P+ and P-.
These two features are then
separately combined using
compact bilinear pooling.
Finally, the features are
fed to two individual cap-
sule networks, and the de-
cisions are max-pooled.
of axial symmetry features as a hand-crafted and fully differentiable layer. The motivation is to
demonstrate one possible use case of axial symmetry measurements in 3D shape analysis.
The proposed architecture consists of four components. First, we obtain three view angles, and
later generate features for each view angle separately. We optimize the view angles to capture
complimentary shape details such that the total information content is maximized. For each viewing
angle ‘k’, we obtain two point sets Pk+ and Pk- consisting of tuples denoted as:
Pk = {(xi , yi , zi) : yi > 0}, and Pk = {(xi , yi , zi) : yi < 0},	(18)
such that y denotes the horizontal axis. Second, the six point sets are volumetrically convolved
with kernels to capture local patterns of the object. The generated features for each point set are
then combined using compact bilinear pooling. Third, we use axial symmetry measurements to
generate additional features. The features that represent each point set are then combined using
compact bilinear pooling. Fourth, we feed features from second and third components of the overall
architecture to two independent capsule networks and combine the outputs at decision level to obtain
the final prediction. The overall architecture of the proposed scheme is shown in Fig. 3.
6.3	Optimum view angles
We use three view angles to generate features for better representation of the object. First, we
translate the center of mass of the set of (x, y, z) points to the origin. The goal of this step is
to achieve a general translational invariance, which allows us to free the convolution operation
from the burden of detecting translated local patterns. Subsequently, the point set is rearranged
as an ordered set on x and z and a 1D convolution net is applied on y values of the points. Here,
the objective is to capture local variations of points along the y axis, since later we analyze point
sets P+ and P - independently. The trained filters can be assumed to capture properties similar
to ∂ny∕∂xn and ∂ny∕∂zn, where n is the order of derivative. The output of the 1D convolution
net is rotation parameters represented by a 1 X 9 vector ~ = {r1,r2,…，r9}. Then, We compute
R1 = Rx(r1)Ry(r2)Rz(r3), R2 = Rx(r4)Ry(r5)Rz(r6) and R3 = Rx(r7)Ry(r8)Rz(r9) where
R1 , R2 and R3 are the rotations that map the points to three different view angles.
After mapping the original point set to three view angles, we extract the Pk+ and Pk- point sets from
each angle k that gives us six point sets. These sets are then fed to the volumetric convolution layer
to obtain feature maps for each point set. We then measure the symmetry around four equi-angular
axes using Eq. 16, and concatenate these measurement values to form a feature vector for the same
point sets.
6.4	Feature fusion using compact bilinear pooling
Compact bilinear pooling (CBP) provides a compact representation of the full bilinear representation,
but has the same discriminative power. The key advantage of compact bilinear pooling is the
significantly reduced dimensionality of the pooled feature vector.
We first concatenate the obtained volumetric convolution features of the three angles, for P + and P -
separately to establish two feature vectors. These two features are then fused using compact bilinear
7
Under review as a conference paper at ICLR 2019
pooling (Gao et al., 2016). The same approach is used to combine the axial symmetry features. These
fused vectors are fed to two independent capsule nets.
Furthermore, we experiment with several other feature fusion techniques and present results in
Sec. 7.2.
6.5	Capsule Network
Capsule Network (CapsNet) (Sabour et al., 2017) brings a new paradigm to deep learning by modeling
input domain variations through vector based representations. CapsNets are inspired by so-called
inverse graphics, i.e., the opposite operation of image rendering. Given a feature representation,
CapsNets attempt to generate the corresponding geometrical representation. The motivation for
using CapsNets in the network are twofold: 1) CapsNet promotes a dynamic ‘routing-by-agreement’
approach where only the features that are in agreement with high-level detectors are routed forward.
This property of CapsNets does not deteriorate extracted features and the final accuracy only depends
on the richness of original shape features. It allows us to directly compare feature discriminability
of spherical and volumetric convolution without any bias. For example, using multiple layers of
volumetric or spherical convolution hampers a fair comparison since it can be argued that the optimum
architecture may vary for two different operations. 2) CapsNet provides an ideal mechanism for
disentangling 3D shape features through pose and view equivariance while maintaining an intrinsic
co-ordinate frame where mutual relationships between object parts are preserved.
Inspired by these intuitions, we employ two independent CapsNets in our network for volumetric
convolution features and axial symmetry features. In this layer, we rearrange the input feature vectors
as two sets of primary capsules—for each capsule net—and use the dynamic routing technique
proposed by Sabour et al. (2017) to predict the classification results. The outputs are then combined
using max-pooling, to obtain the final classification result. For volumetric convolution features, our
architecture uses 1000 primary capsules with 10 dimensions each. For axial symmetry features,
we use 2500 capsules, each with 10 dimensions. In both networks, decision layer consist of 12
dimensional capsules.
6.6	Hyperparameters
We use n = 5 to implement Eq. 15 and three iterations to calculate the Moore-Penrose pseudo inverse
gstep
using Eq. 7. We use a decaying learning rate lr = 0.1 X 0.9询00, where gstep is incremented by one
per each iteration. For training, we use the Adam optimizer with β1 = 0.9, β2 = 0.999, = 1 × 10-8
where parameters refer to the usual notation. All these values are chosen empirically. Since we
have decomposed the theoretical derivations into sets of low-cost matrix multiplications, specifically
aiming to reduce the computational complexity, the GPU implementation is highly efficient. For
example, the model takes less than 15 minutes for an epoch during the training phase for ModelNet10,
with a batchsize 2, on a single GTX 1080Ti GPU.
7 Experiments
In this section, we discuss and evaluate the performance of the proposed approach. We first compare
the accuracy of our model with relevant state-of-the-art work, and then present a thorough ablation
study of our model, that highlights the importance of several architectural aspects. We use Model-
Net10 and ModelNet40 datasets in our experiments. Next, we evaluate the robustness of our approach
against loss of information and finally show that the proposed approach for computing 3D Zernike
moments produce richer representations of 3D shapes, compared to the conventional approach.
7.1	Comparison with the state-of-the-art
Table 1 illustrates the performance comparison of our model with state-of-the-art. The model
attains an overall accuracy of 92.17% on ModelNet10 and 86.5% accuracy on ModelNet40,
which is on par with state-of-the-art. We do not compare with other recent work, such as
Kanezaki et al. (2016); Qi et al. (2016); Sedaghat et al. (2016); Wu et al. (2016); Qi et al.
(2016); Bai et al. (2016); Maturana & Scherer (2015) that show impressive performance on
8
Under review as a conference paper at ICLR 2019
Method	Trainable layers	# Params	M10	M40	Table 1: Comparison with state-of-the-
					
SO-Net (Li et al., 2018)	11FC	60M	95.7%	93.4%	art methods on
Kd-Networks (Klokov & Lempitsky, 2017)	15KD	-	94.0%	91.8%	
VRN (Brock et al., 2016)	45Conv	90M	93.11%	90.8%	ModelNet10 and
Pairwise (Johns et al., 2016) MVCNN (Su et al., 2015)	23Conv 60Conv + 36FC	143M 200M	92.8% -	90.7% 90.1%	ModelNet40 datasets
Ours	3Conv + 2Caps	4.4M	92.17%	86.5%	(ranked according to
PointNet (Qi et al., 2017)	2ST + 5Conv	80M	-	86.2%	performance). Ours
ECC (Simonovsky & Komodakis, 2017)	4Conv + 1FC	-	-	83.2%	
DeepPano (Shi et al., 2015)	4Conv + 3FC	-	85.45%	77,63%	achieve a competitive
3DShapeNets (Wu et al., 2015)	4-3DConv + 2FC	38M	83.5%	77%	performance with the
PointNet (Garcia-Garcia et al., 2016)	2Conv + 2FC	80M	77.6%	-	least network depth.
ModelNet10 and ModelNet40. These are not comparable with our proposed approach, as we
propose a shallow, single model without any data augmentation, with a relatively low number
of parameters. Furthermore, our model reports these results by using only a single volumetric
convolution layer for learning features. Fig. 4 demonstrates effectiveness of our architecture
by comparing accuracy against the number of trainable parameters in state-of-the-art models.
7.2	Ablation study
Table 3 depicts the performance comparison between
several variants of our model. To highlight the effective-
ness of the learned optimum view points, we replace the
optimum view point layer with three fixed orthogonal
view points. This modification causes an accuracy drop
of 6.57%, emphasizing that the optimum view points
indeed depends on the shape. Another interesting—
perhaps the most important—aspect to study is the
performance of the proposed volumetric convolution
against spherical convolution. To this end, we replace
the volumetric convolution layer of our model with
spherical convolution and compare the results. It can
be seen that our volumetric convolution scheme out-
performs spherical convolution by a significant margin
of 12.56%, indicating that volumetric convolution cap-
tures shape properties more effectively.
Table 2: Ablation study of the proposed
architecture on ModelNet10 dataset
Method	Accuracy
Final Architecture (FA)	92.17%
FA + Orthogonal Rotation	85.60%
FA - VolCNN + SphCNN	79.53%
FA -MaxPool + MeanPool	87.27%
FA + Feature Fusion (Axial + Conv)	86.53%
Axial Symmetry Features	66.73%
VolConv Features	85.3%
SphConv Features	71.6%
FA - CapsNet + FC layers	87.3 %
FA - CBP + Feature concat	90.7%
FA - CBP + MaxPool	90.3%
FA - CBP + Average-pooling	85.3 %
Furthermore, using mean-pooling instead of max-
pooling, at the decision layer drops the accuracy to
87.27%. We also evaluate performance of using a sin-
gle capsule net. In this scenario, we combine axial sym-
metry features with volumetric convolution features
using compact bilinear pooling (CBP), and feed it a sin-
gle capsule network. This variant achieves an overall
accuracy of 86.53%, is a 5.64% reduction in accuracy
compared to the model with two capsule networks.
Moreover, we compare the performance of two fea-
ture categories—volumetric convolution features and
axial symmetry features—individually. Axial symme-
try features alone are able to obtain an accuracy of
66.73%, while volumetric convolution features reach a
significant 85.3% accuracy. On the contrary, spherical
100
90
Curs
SO-Net
VRN
PoinltNet
80
3DShapeNet
Pai印ise
MVCNN
20 40 60 80 100 120 140 160 180 200
Number of trainable parameters
Figure 4: Accuracy vs number of trainable
params (in millions) trend (ModelNet40)
convolution attains an accuracy of 71.6%, which again highlights the effectiveness of volumetric
convolution.
Then we compare between different choices that can be applied to the experimental architecture.
We first replace the capsule network with a fully connected layer and achieve an accuracy of 87.3%.
This is perhaps because capsules are superior to a simple fully connected layer in modeling view-
9
Under review as a conference paper at ICLR 2019
5%	10%	20%	30%	50%
Percentage loss of points
Figure 5: The robustness of the proposed model Figure 6: The mean reconstruction error Vs ‘n’.
against missing data. The accuracy drop is less Our Zernike frequencies computation approach
than 30% at a high data loss rate of 50%.	has far less error than the conventional approach.
point invariant representations. Then we try different substitutions for compact bilinear pooling and
achieve 90.7%, 90.3% and 85.3% accuracies respectively for feature concatenation, max-pooling
and average-pooling. This justifies the choice of compact bilinear pooling as a feature fusion tool.
However, it should be noted that these choices may differ depending on the architecture.
7.3	Robustness against information loss
One critical requirement of a 3D object classification task is to be robust against various information
loss. To demonstrate the effectiveness of our proposed features in this aspect, we randomly remove
data points from the objects in validation set, and evaluate model performance. The results are
illustrated in Fig. 5. The model shows no performance loss until 20% of the data is lost, and only
gradually drops to an accuracy level of 66.5 at a 50% data loss, which implies strong robustness
against random information loss.
7.4	Effectiveness of the proposed method for calculating 3D Zernike moments
In Sec. 4.1, we proposed an alternative method to calculate 3D Zernike moments (Eq. 5, 6), instead
of the conventional approach (Eq. 3). We hypothesized that moments obtained using the former has
a closer resemblance to the original shape, due to the impact of finite number of frequency terms.
In this section, we demonstrate the validity of our hypothesis through experiments. To this end, we
compute moments for the shapes in the validation set of ModelNet10 using both approaches, and
compare the mean reconstruction error defined as: 1 PTIIf (t) - Pn Pl Pm Ωn,ι,mZn,ι,m(t) ∣∣,
where T is the total number of points and t ∈ S3 . Fig. 6 shows the results. In both approaches, the
mean reconstruction error decreases as n increases. However, our approach shows a significantly
low mean reconstruction error of 0.0467% at n = 5 compared to the conventional approach, which
has a mean reconstruction error of 0.56% at same n. This result also justifies the utility of Zernike
moments for modeling complex 3D shapes.
8	Conclusion
In this work, we derive a novel ‘volumetric convolution’ using 3D Zernike polynomials, which can
learn feature representations in B3 . We develop the underlying theoretical foundations for volumetric
convolution and demonstrate how it can be efficiently computed and implemented using low-cost
matrix multiplications. Furthermore, we propose a novel, fully differentiable method to measure
the axial symmetry of a function in B3 around an arbitrary axis, using 3D Zernike polynomials.
Finally, using these operations as building tools, we propose an experimental architecture, that gives
competitive results to state-of-the-art with a relatively shallow network, in 3D object recognition
task. An immediate extension to this work would be to explore weight sharing along the radius of the
sphere.
10
Under review as a conference paper at ICLR 2019
References
Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. Gift: A real-time
and scalable 3d shape search engine. In Computer Vision and Pattern Recognition (CVPR), 2016
IEEE Conference on,pp. 5023-5032. IEEE, 2016.
Wouter Boomsma and Jes Frellsen. Spherical convolutions and their application in molecular
modelling. In Advances in Neural Information Processing Systems, pp. 3436-3446, 2017.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Generative and discriminative
voxel modeling with convolutional neural networks. arXiv preprint arXiv:1608.04236, 2016.
N Canterakis. 3d zernike moments and zernike affine invariants for 3d image analysis and recognition.
In In 11th Scandinavian Conf. on Image Analysis. Citeseer, 1999.
Taco S Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Spherical cnns. International
Conference on Learning representations (ICLR), 2018.
Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell. Compact bilinear pooling. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 317-326, 2016.
Alberto Garcia-Garcia, Francisco Gomez-Donoso, Jose Garcia-Rodriguez, Sergio Orts-Escolano,
Miguel Cazorla, and J Azorin-Lopez. Pointnet: A 3d convolutional neural network for real-time
object class recognition. In Neural Networks (IJCNN), 2016 International Joint Conference on, pp.
1578-1584. IEEE, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Edward Johns, Stefan Leutenegger, and Andrew J Davison. Pairwise decomposition of image
sequences for active multi-view recognition. In Computer Vision and Pattern Recognition (CVPR),
2016 IEEE Conference on, pp. 3813-3822. IEEE, 2016.
Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object catego-
rization and pose estimation using multiviews from unsupervised viewpoints. arXiv preprint
arXiv:1603.06208, 2016.
Roman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks for the recognition of
3d point cloud models. In 2017 IEEE International Conference on Computer Vision (ICCV), pp.
863-872. IEEE, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Hou-Biao Li, Ting-Zhu Huang, Yong Zhang, Xing-Ping Liu, and Tong-Xiang Gu. Chebyshev-type
methods and preconditioning techniques. Applied Mathematics and Computation, 218(2):260-270,
2011.
Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud analysis.
arXiv preprint arXiv:1803.04249, 2018.
Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time
object recognition. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International
Conference on, pp. 922-928. IEEE, 2015.
Charles R Qi, Hao Su, Matthias Nieβner, Angela Dai, MengyUan Yan, and Leonidas J Guibas.
Volumetric and multi-view cnns for object classification on 3d data. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 5648-5656, 2016.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE, 1(2):4, 2017.
11
Under review as a conference paper at ICLR 2019
C Ronchi, R Iacono, and Pier S Paolucci. The “cubed sphere”: a new method for the solution of
partial differential equations in spherical geometry. Journal of Computational Physics, 124(1):
93-114,1996.
Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In
Advances in Neural Information Processing Systems, pp. 3859-3869, 2017.
Nima Sedaghat, Mohammadreza Zolfaghari, Ehsan Amiri, and Thomas Brox. Orientation-boosted
voxel nets for 3d object recognition. arXiv preprint arXiv:1604.03351, 2016.
Baoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. Deeppano: Deep panoramic representation
for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339-2343, 2015.
Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neural
networks on graphs. In Proc. CVPR, 2017.
Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional
neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on
computer vision, pp. 945-953, 2015.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. arXiv preprint arXiv:1807.02547,
2018.
Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilis-
tic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural
Information Processing Systems, pp. 82-90, 2016.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1912-1920, 2015.
12
Under review as a conference paper at ICLR 2019
Supplementary Material
Volumetric Convolution: Automatic Representation Learning in
Unit Ball
A Convolution within unit sphere using 3D Zernike polynomials
Theorem 1: Suppose f, g : X -→ R3 are square integrable complex functions defined in B3
so that hf, fi < ∞ and hg, gi < ∞. Further, suppose g is symmetric around north pole and
τ(α, β) = Ry (α)Rz (β) where R ∈ SO(3). Then,
1 2π π	4π ∞ n l
f(θ,φ,r),τ(α,β)(g(θ,φ,r))sinφdφdθdr ≡ ɪ V E E Ω%ι,m(f )Ω%1,0(g)匕,m(θ,φ)
0	0	0 3n=0 l=0 m=-l
(19)
where Ωn,ι,m(/), Ωn,1,0(g) and Ylm(θ, φ) are (n,l, m)th 3D Zernike moment of f, (n,l, 0)th 3D
Zernike moment of g, and spherical harmonics function respectively.
Proof: Since 3D Zernike polynomials are orthogonal and complete in B3, an arbitrary function
f(r θ φ) in B3 can be approximated using Zernike polynomials as follows.
∞n l
f(θ φ r) = XX X
Ωn,l,m(f )Zn,l,m(θ, φ,r)	(20)
n=0 l=0 m=-l
where Ωn,l,m(f) could be obtained using,
1 2π	π
Ωn,l,m(f) = J J J f (θ, Φ, r)zl,i,mr2sinφdrdφdθ
(21)
where * denotes the complex conjugate.
Leveraging this property (Eq. 20) of 3D Zernike polynomials Eq. 4 can be rewritten as,
∞ n l	∞ n0	l
f * g(θ,φ)= h	◎n,l,m(f )Zn,l,m, τ(θ,φ) (	◎n0,l0,m0 (g)Zn0,l0,m0 )i (22)
n=0 l=0 m=-l	n0=0 l0=0 m0=-l
But since g(θ, φ, r) is symmetric around y, the rotation around y should not change the function.
Which ensures,
g(r, θ, φ) = g(r, θ - α, φ)
and hence,
(23)
∞	n0	l	∞ n0 l
XXX
◎n0,l0,m0 (g)Rn0,l0 (r)Yl0,m0 =XXX
◎n0,l0,m0 (g)Rn0,l0 (r)Yl0,m0e 'rn OL
n0=0 l0=0 m0 =-l	n0=0 l0=0 m0=-l
(24)
This is true, if and only if m0 = 0. Therefore, a symmetric function around y, defined inside the unit
sphere can be rewritten as,
∞ n0
ΣΣΩn0,l0,θ(g)Zn0,l0,0	(25)
n0=0 l0=0
which simplifies Eq. 22 to,
13
Under review as a conference paper at ICLR 2019
0
∞n l	∞n
f * g(θ,φ) = hΣΣ E Ωn,l,m(f)Zn,l,m, τ(θ,φ)(EE Ωn0,10,0(g)Zn0,10,0)i	(26)
n=0 l=0 m=-l	n0=0 l0=0
Using the properties of inner product, Eq. 26 can be rearranged as,
0
∞n∞n l
f*g(θ,φ) =ΣΣΣΣΣΩn,l,m(f )Ωn0,ιo,0 (g) (Zn,l,m,T(θ,φ)(。城”,0)〉	(27)
n=0 l=0 n0=0 l0=0 m=-l
Consider the term τ(θ,φ) (Zn0,l0,0). Then,
l0
τ(θ,φ) (Zn0,l0,0) = τ(θ,φ) (Rn0,l0Yl0,0) = Rn0,l0 τ(θ,φ) (Yl0,0) = Rn0,l0	Yl0,m00Dml 00,0(θ, φ)
m00=-l0
(28)
where Dml ,m0 is the Wigner-D matrix. But we know that Dml0 00,0(θ, φ) = Yl0,m00 (θ, φ). Then Eq. 27
becomes,
00
∞n∞n l	l
f*g(θ,φ) =ΣΣΣΣΣCn,l,m(f Wn0,l0HgO X γi0,m00 (θ, φ)hZn,l,m, Zn0,l0,m00 i
n=0 l=0 n0=0 l0=0 m=-l	m00=-l0
(29)
4∞n l
f * g(θ,Φ) = 4∏ΣΣΣΩn,l,m(f )Ωn,1,0(g)匕,m(θ,φ)	(30)
n=0 l=0 m=-l
B Equivariance of volumetric convolution to 3D rotation group
Theorem 1: Suppose f, g : X -→ R3 are square integrable complex functions defined in B3 so that
hf, fi < ∞ and hg, gi < ∞. Also, let ηα,β,γ be a 3D rotation operator that can be decomposed into
three Eular rotations Ry(α)Rz(β)Ry(γ) and τα,β another rotation operator that can be decomposed
into Ry(α)Rz(β). Suppose ηα,β,γ (g) = τα,β(g). Then,
η(α,β,γ)(f) * g(θ, φ) = τ(α,β) (f * g)(θ, φ)	(31)
where * is the volumetric convolution operator.
Proof: Since η(α,β,γ) ∈ SO(3), we know that η(α,β,γ)(f (x)) = f (η(-α1,β,γ)(x)). Also we know that
η(α,β,γ) : R3 → R3 is an isometry.
Define,
hη(α,β,γ)f, η(α,β,γ)gi =	f (η(-α1,β,γ)(x))g(η(-α1,β,γ)(x))dx	(32)
S3
Consider the Lebesgue measure λ(S3) = S3 dx. It can be proven that a lebesgue measure invariant
under the isometries, which gives us dx = dη(α,β,γ) (x) = dη(-α1,β,γ) (x), ∀x ∈ S3. Therefore,
< η(α,β,γ)f, η(α,β,γ)g >=	f(η(-α1,β,γ)(x))g(η(-α1,β,γ)(x))d(η(-α1,β,γ)x) =< f,g >	(33)
14
Under review as a conference paper at ICLR 2019
Let f(θ, φ, r) and g(θ, φ, r) be the object function and kernel function (symmetric around north pole)
respectively. Then volumetric convolution is defined as,
f * g(θ, φ) =< f,T(θ,φ)g >	(34)
Applying the rotation η(α,β,γ) to f, we get,
η(α,β,γ) (f) * g(θ, φ) =< η(α,β,γ)(f), τ(θ,φ)g >	(35)
By the result 33, we have,
η(α,β,γ) (f) * g(θ, φ) =< f, η(-α1,β,γ)(τ(θ,φ)g) >	(36)
However, since ηα,β,γ (g) = τα,β (g) we get,
η(α,β,γ) (f) * g(θ, φ) =< f, τ(θ-α,φ-β,)g >	(37)
We know that,
∞n l
f * g(θ, φ) =< f, τ(θ,φ)g >= XX X
Ωn,l,m(f )Ωn,1,0(g)Y1,m (θ,φ)	(38)
n=0 l=0 m=-l
Then,
∞n l
η(α,β,γ)(f) * g(θ, φ) =< f, τ(θ-α,φ-β)g >=ΣΣΣ◎n,l,m(f )0~1,0 (g)γi,m(θ - α,φ - β}
n=0 l=0 m=-l
(39)
η(α,β,γ)(f) * g(θ, φ) = (f * g)(θ - α, φ - β)	(40)
η(α,β,γ)(f) * g(θ, φ) = τ(α,β) (f *g)	(41)
Hence, we achieve equivariance over 3D rotations.
C AXIAL SYMMETRY MEASURE OF A FUNCTION IN B3 AROUND AN
arbitrary axis.
Proposition: Suppose g : X -→ R3 is a square integrable complex function defined in B3 such
that hg, gi < ∞. Then, the power of projection of g in to S = {Zi} where S is the set of Zernike
basis functions that are symmetric around an axis towards (α, β) direction is given by,
nl
||sym(a,e)Il= XXlI X Ωn,1,mYm,1(α,β)∣∣2	(42)
n l=0 m=-l
where α and β are azimuth and polar angles respectively.
Proof: The subset of complex functions which are symmetric around north pole is S = {Zn,l,0 }.
Therefore, projection of a function into S gives,
n
symy(θ, φ) = X Xhf, Zn,l,0izn,l,0(θ, φ)	(43)
n l=0
15
Under review as a conference paper at ICLR 2019
To obtain the symmetry function around any axis which is defined by (α, β), we rotate the function
by (-α, -β), project into S, and final compute the power of the projection.
sym(α,β) (θ, φ) =	hτ(-α,-β)(f), Zn,l,0izn,l,0(θ, φ)	(44)
n,l
For any rotation operator U, and for any two points defined on a complex Hilbert space, x and y,
hU(x), U(y)iH = hx, yiH	(45)
Applying this property to 44 gives,
sym(α,β)(θ, φ) =	hf, τ(α,β)(Zn,l,0)izn,l,0(θ, φ)	(46)
n,l
Using Eq. 20 we get,
n	n0	l0
sym(α,β)(θ,φ)=	h	Cn0l0m0 Zn0,l0,m0 ,T(a,e)(Zn,l,0y)zn,l,0(θ, φ)	(47)
n l=0 n0 l0=0 m0=-l0
Using properties of inner product Eq. 47 further simplifies to,
00
n	nl
sym(α,β)(θ,φ)=	.n01 m0 hZn0 ,l0 ,m∕ ,τ(α,β) (Zn,l,0)'i zn,l,0(θ, φ)	(48)
n l=0 n0 l0=0 m0=-l0
Using the same derivation as in 28,
n	n0	l0	l
sym(α,β)(θ,φ)=	◎n’l,m0 X : Yl,m00 (α,β) < Zn0,l0,m0 , Zn,l,m00 > zn,l,0(θ, φ)
n l=0 n0 l0=0 m0=-l0	m00 =-l
(49)
Since 3D Zernike Polynomials are orthogonal we get,
4π n l
sym(α,β)(θ,φ) = 3-ΣΣΣΩn,l,mYm,l(α, β)z%l,0(θ, φ)	(50)
n l=0 m=-l
In signal theory the power of a function is taken as the integral of the squared function divided by the
size of its domain. Following this we get,
n l	n0	l0
llsym(α,β)Il= h(∑∑ ∑ ωn,l,mYm,l (α, β))zn,l,0(θ,φ),(	S'n0 ,l0 ,m0 γm0,l0 (α, β )zn0,l0,θ(θ, φ)), i
n l=0 m=-l	n0 l0=0 m0=-l0
(51)
We drop the constants here since they do not depend on the frequency. Simplifying Eq. 51 gives,
nl l
l|sym(a,e)||=	ωn,l,mYm,l (α, e)Cn,l,m0Ym0,l (α, β)	(52)
n l=0 m=-l m0=-l
which leads to,
nl
llsym(α,β)Il= XXlI X Ωn,l,mYm,l(α,β)∣∣2	(53)
n l=0 m=-l
16
Under review as a conference paper at ICLR 2019
D Function definitions
D.1 Spherical harmonics
Spherical harmonics are a set of complete orthogonal functions, which are defined on S2 .
匕,m(θ,φ) = (-1)m[∕2l + 1(l - m)!Pm(COsφ)eimθ
4π (l + m)!
(54)
where l is an integer, m is an integer, |m|< l, and Pm(∙) is the associated Legendre function (See
appendix D.2).
D.2 Associated Legendre function
Associated Legendre functionPlm(x) is defined as,
2 m/2 l+m
Pm(X) = (-1)m ( -2ιl!)	E(χ2- 1)l
where l is an integer, m is an integer, |m|< l, and x is a real number.
(55)
D.3 Zernike radial polynomial
(n-m)/2
Rn,m (r) =
k=0
(-1)k(n - k)!
k! ((n + m)/2 - k)! ((n - m)/2 - k)!
rn-2k
(56)
E S PHERICAL CONVOLUTION ON S2
Let the f and g be the shape functions of the object and kernel respectively. Then f and g can be
expressed as,
ll
f(θ,φ)=XX
f(l,m)Yι,m(θ,Φ) and, g(θ,φ)=XX
g(t,m)Ylm (θ,φ)	(57)
l m=-l	l m=-l
where Yl,m is the (l, m)th spherical harmonics function and f(l, m) and g(l, m) are (l, m)th fre-
quency components of f and g respectively. Then the frequency components of convolution f * g
can be easily calculated as,
4π
f * g(l, m) = ↑j 2τ+ι f(l, m)g(l, 0)t
(58)
where t denotes the complex conjugate.
F Rotation Parameters
17
Under review as a conference paper at ICLR 2019
Table 3: Average rotation parameter values across classes of ModelNet10. The values are reformatted
to be positive angles between 0 and 360.
Class	r1	r2	r3	r4	r5	r6	r7	r8	r9
Bathtub Bathtub	319.2	100.5	57.8	185.2	223.4	98.3	350.6	167.4	14.2
Bed	264.3	196.3	103.7	208.5	186.2	194.4	267.9	246.3	81.2
Chair	198.6	91.2	243.7	47.4	161.2	87.9	240.5	47.3	203.4
Desk	88.4	80.2	130.9	206.6	86.5	112.8	291.7	233.2	351.4
Dresser	58.0	145.7	353.1	148.4	346.4	125.3	47.0	2.2	35.4
Monitor	218.9	279.0	58.1	10.4	30.3	331.4	90.7	285.6	346.1
Night stand	85.3	336.1	175.9	246.4	169.4	278.7	317.0	137.6	302.9
Sofa	306.1	86.9	109.2	311.1	22.5	321.4	96.9	47.0	76.2
Table	299.8	85.2	126.5	215.1	221.9	245.5	237.1	50.6	128.4
Toilet	277.0	325.3	215.5	255.6	192.2	19.8	278.4	193.4	348.2
Average	211.6	172.6	157.4	183.4	164.1	182.4	221.8	141.1	189.7
18