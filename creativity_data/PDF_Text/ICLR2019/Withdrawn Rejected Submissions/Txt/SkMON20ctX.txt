Under review as a conference paper at ICLR 2019
On the Trajectory of Stochastic Gradient De-
scent Learning in the Information Plane
Anonymous authors
Paper under double-blind review
Ab stract
Studying the evolution of information theoretic quantities during Stochastic Gra-
dient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained
popularity in recent years. Nevertheless, these type of experiments require es-
timating mutual information and entropy which becomes intractable for moder-
ately large problems. In this work we propose a framework for understanding
SGD learning in the information plane which consists of observing entropy and
conditional entropy of the output labels of ANNs. Through experimental results
and theoretical justifications it is shown that, under some assumptions, the SGD
learning trajectories appear to be similar for different ANN architectures. First,
the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy
tends to increase to the maximum. Then, it is shown that the SGD learning trajec-
tory appears to move close to the shortest path between the initial and final joint
distributions in the space of probability measures equipped with the total variation
metric. Furthermore, it is shown that the trajectory of learning in the information
plane can provide an alternative for observing the learning process, with poten-
tially richer information about the learning than the trajectories in training and
test error.
1	Introduction
How do information theoretic quantities behave during the training of ANNs? This question was
addressed by Shwartz-Ziv & Tishby (2017) in an attempt to explain the learning through the lens
of the information bottleneck method (Tishby et al., 1999). In that work, the layers of an ANNs
are considered random variables forming a Markov chain. The authors constructed a 2D informa-
tion plane by estimating the mutual information values between hidden layers, inputs, and outputs
of ANNs. Using this approach it was observed that the information bottleneck method provides
an approximate explanation for SGD learning. In addition, their experiments showed the role of
compression in learning. That initial paper motivated further work on this line of research (Saxe
et al., 2018; Gabrie et al., 2018). The main practical limitation of that type of experiments is that
it requires estimating mutual information between high dimensional continuous random variables.
This becomes prohibitive as soon we move to moderately large problems, such as the CIFAR-100
dataset, where the large ANNs are employed. Other works dealing with information theoretic quan-
tities tend to have these experimental limitations. For instance, Russo & Zou (2015); Xu & Raginsky
(2017); Asadi et al. (2018) used generic chaining techniques to show that generalization error can
be upper bounded by the mutual information between the training dataset and output of the learning
algorithm. Nevertheless, estimating that mutual information to verify those results experimentally
becomes intractable. Furthermore, in our previous work (Anonymous, 2018) we defined a novel 2D
information plane that only requires to estimate information theoretic quantities between the correct
and estimated labels. Since these random variables are discrete and one-dimensional, this framework
can be used to study learning in large recognition problems as well. Moreover, that work provides a
preliminary empirical study on the behavior of those information theoretic quantities during learning
along with some connections between error and conditional entropy.
In this work, we extend the experiments from Anonymous (2018) to more general scenarios and aim
to characterize the observed behavior of SGD. Our main contributions are as follows:
1
Under review as a conference paper at ICLR 2019
•	We define a 2D-information plane, inspired by the works of Shwartz-Ziv & Tishby (2017),
and use it to study the behavior of ANNs during SGD learning. The main quantities are
entropy of the output labels and its conditional entropy given true labels.
•	It is shown that if the learning is done perfectly and under some other mild assumptions,
the entropy tends to increase to its maximum.
•	It is additionally shown that SGD learning trajectory follows approximately the shortest
path in the space of probability measures equipped with the total variation metric. The
shortest path is characterized well by a Markov chain defined on probabilities of estimate
labels conditioned on true labels. To that end we provide theoretical and experimental
justifications for constructing a simple Markovian model for learning, and compare it with
SGD through experiments. These experiments are conducted using various datasets such as
MNIST (LeCun et al., 1998), CIFAR-10/ CIFAR-100, spirals (Anonymous, 2018), as well
as different ANN architectures like Fully Connected Neural Networks (FCNNs), LeNet-5
(LeCun et al., 1999), and DenseNet (Huang et al., 2017).
•	The trajectory, however, is not universal. Through a set of experiments, it is shown that
SGD learning trajectory differs significantly for different learning strategies, noisy labels,
overfitting, and underfitting. We show examples where this type of trajectories provide a
richer view of the learning process than conventional training and test error, which allows
us to spot undesired effects such as overfitting and underfitting.
The paper is organized as follows: Section 2 introduces the notation as well as elementary notions
from information theory. Section 3 formulates learning as a trajectory on the space of probability
measures, defines the notion of shortest learning path, and provides a connection to Markov chains.
Section 4 constructs a simple Markov chain model for gradient based learning that moves along the
shortest learning path. Finally, Section 5 performs an empirical evaluation of the proposed model.
2	System Model
Let x ∈ X be a random vector belonging to some set X of possible inputs. We assume that there
exists a function, known as “oracle”, that maps x to one of K ∈ N classes. Formally, there exists a
deterministic mapping c : X → Y where Y = {0, . . . , K - 1} is the set of possible classes. Then,
let y ∈ Y denote the random variable y = c(x). One common assumption, that is present in popular
datasets such as MNIST, CIFAR-10, CIFAR-100, and Imagenet, is that y is uniformly distributed.
We assume this to be true throughout this paper. Note that the designer of the dataset has control
over the marginal distribution of yy.
We model the effect of having error-prone labels, denoted by the random variable y, in the data by
introducing discrete independent random noise z ∈ Y to yy in the form of modulo addition1, that is
y = y ㊉ Z ∈ Y. Let θ ∈ Θ be the vector, possibly random, containing of all tunable parameters
in the hypothesis space Θ. Then a classifier is a deterministic function g : Θ × X → Y that aims
to approximate c. Further, ^ = g(θ, x) is defined to be the random variable of the label predicted
by the classifier g(θ, ∙). Using this notation We define the three types of error: the dataset error
P = P(y = y), the test error ε = P(y = y), the true error y = P(^ = y). A summary of this system
model is provided in Figure 1. We shortly revieW some elementary concepts from information theory
such as entropy and mutual information. The entropy of a discrete random variable y ∈ Y is defined
as2
H(y) = -	P(y= y)logP(y= y) = -ElogP(y) .
y∈Y
The entropy is bounded by 0 ≤ H(y) ≤ log |Y| and it measures the amount uncertainty present in
y. Similarly, the conditional entropy between two random variables y and ^ is
H(y∖y') = -E log P(y∖y')
and it quantifies the uncertainty about y given that y is known. Finally, the mutual information
I(y; y) between y and ^ measures how much information does one random variable carry about the
1We use ㊉ to denote the modulo K addition.
2In this paper we assume log to be the natural logarithm.
2
Under review as a conference paper at ICLR 2019

dataset
Xoooooo
Figure 1: System model.
other. It may be defined in terms of entropies as
I(y; y) = H(y) - H(y∣^) = H(y) - H(y|y).
Moreover, the following proposition is a well-known result from information theory, known as
Fano’s inequality, that relates test error ε and conditional entropy.
Proposition 1. Fano's Inequality (CSiSzar & Korner (2011), Lemma 3.8)
The value of H (y∣^) and H (^∣y) is upper bounded by a function of the expected error as
maχ{H(y|V),H(Wy)} ≤ ψ(ε),
where the function Ψ : [0, 1] → R iS defined aS
Ψ(x) = x log(K - 1) + hb(x), x ∈ [0, 1]
and hb(x) = -x log(x) - (1 - x) log(1 - x) iS the binary entropy function.
This results provides an upper bound on conditional entropy in terms of ε, that is known to be sharp.
In the works of Feder & Merhav (1994) it has been shown that I(y; ^) gives an upper and lower
bound on the minimal error 3 between y and y. In addition, the minimal error is minimized when
I(y; y) reaches its maximum. Therefore, learning can be modeled as finding θ such that I(y; y) is
maximized. This can be written in terms of entropies as
max H(y) - H(VIy).
(1)
As in our previous work (Anonymous, 2018) we are interested on characterizing the trajectory in the
2D information plane, composed by H(^) and H(^∣y), during the learning process of artificial neural
networks (ANNs). In Figure 2 we observe that learning trajectory for the DenseNet architecture of
100 layers as it learns to classify data from the CIFAR-100 dataset, for p = 0. Intuitively, when
solving equation 1, maximizing H(^) is more related with the unsupervised component of learning
since it does not depend y. On the other hand, keeping H(^∣y) low while H(y) increases can be
seen as the supervised component of equation 1. From this point of view it would be interesting to
characterize the inflection point from which H(y|y) starts decreasing, since it allows us to observe
at which point SGD starts paying more attention to assigning labels correctly than to learn about
the distribution of the input. One also may wonder if this increasing-decreasing trajectory is an
accidental result for that occurs only on this particular experimental setup, or if it is a fundamental
property of SGD. In Anonymous (2018) we showed that this behavior seems to appear regardless
of the activation function employed (see Appendix C) on the spirals and MNIST dataset. Moreover,
in further sections we provide a justification for this type of trajectory and show that it remains in
other datasets. As Z is independent, minimizing ε amounts to g(θ, ∙) learning c(∙), regardless of the
value of p (Angluin & Laird, 1988). For more information about error and entropy relations in the
presence of noisy labels see Appendix A.
3The minimal error is the error obtained by a maximum likelihood classifier that predicts y from ^.
3
Under review as a conference paper at ICLR 2019
(a) Information Plane
Figure 2: Learning trajectory of DenseNet on the CIFAR-100 dataset. The markers in the black
dashed lines represent the ideal values of H(y), H(y|y) and ε when ɛ = 0. (a): The dashed lines
correspond to the maximum entropy value. (b): The dashed lines are the upper bound given by
Fano’s inequality.
(b) EntroPy-ErTor plane
3	Connection Between SGD and Markov Chains
In gradient based training of ANNs, the tunable parameters of the networks θ are changed in time
by the gradient updates of the loss function, in order to minimize the learning error for a particular
problem at hand. Let θn ∈ Θ denote the tunable parameters of an ANN after n ≥ 1 training steps
of SGD. The parameters are initialized as θ0, which can be random or deterministic. The set Θ can
be seen as a high dimensional Euclidean space with the network parameter θ as its vector.
At the training step n, the outcome of the learning algorithm is captured by the random variable
yn which is modulated by the network function g(θn, ∙) applied to the random input data x, i.e.,
yn = g (θn, x). Therefore the SGD learning gives rise the following sequence of random variables
g(θ0, x), . . . , g(θn, x), .
As n grows large with a successful training, the sequence of random variables converges approxi-
mately to the true labels y which itself follows a joint distribution with x. Note that the above random
variables are coupled through the common random variable x and the sequence of parameter updates
θn.
If the probability distribution of yn is denoted by Pn, a first question is to see how SGD methods
modify pn on the space of probability measures defined on Y. As a consequence, one can determine
the trajectory of H(yn), which will be plotted later. However it is additionally important in learning
that the random variable yn approximates the true labels. Therefore, a second question would be
how SGD methods change the joint distribution of (yn, yn). The answer could determine instead
the trajectory of the conditional entropy H(yn ∣yn). We first study the trajectory of H(yn).
The random variables yn are defined as g(θn, x). Consider the sequence of random variables {θn}.
Let T denote the set of training samples (x, y) that are obtained prior to training and independently.
In addition, let Tn be a subset of T that is used at the step n for SGD update. Tn is assumed to be
independent from (θ0, . . . , θn-1) and it is either deterministic and known all n or randomly chosen
at each step. These variations correspond to the variants of SGD.
In pure gradient based methods without momentum based techniques, the network parameters obey
the following recursive relation
θn = f(θn-1,Tn),	(2)
where f denotes the update rule of SGD. The model assumes that the SGD updates only depend on
the parameters in the last step and the training set used in the current iteration. We can assume that
Tn are i.i.d. random variables if we neglect the effect of reusing training data in different batches.
The first conclusion is that the sequence of random variables {θn } is a Markov chain.
Proposition 2. The sequence of random variables {θn } defined as equation 2 with i.i.d. random
variables Tn is a Markov Chain.
4
Under review as a conference paper at ICLR 2019
Proof. IfTn’s are i.i.d. random variables, the proof follows directly from Serfozo (2009, Proposition
11) on equation 2.	□
The transition probability of this Markov chain can be obtained only from f and T1. In that sense,
the random process {θn} is a homogeneous Markov chain. Throughout this work, it is assumed that
the Markov chain {θn} has a stationary distribution which corresponds to the learned ANN.
This proposition shows that SGD updates induce Markov property for weights of an ANN. The
sequence of random variables ^n = g(θn, x) however is in general not a Markov chain, particularly
because they are coupled through a common random variable x. Since We are interested in H(^n)
and the distribution pn , these random variables can be decoupled by considering the random vari-
ables g(θn , xn ) where xn are i.i.d. random variables with the same distribution as x. Note that the
value of the entropy function remains unchanged after decoupling, namely H&) = H(g(θn, Xn)).
The new sequence is a function ofa Markov chain and i.i.d. random variables. The question whether
the resulting sequence is a Markov chain has been addressed in Spreij (2001); Gurvits & Ledoux
(2005) showing that 勺门 is not a Markov chain in general unless certain conditions are met by the
function g(∙). Unfortunately the function g(∙) is not injective and a non-injective function of a
Markov chain is not Markov chain in general. However the random variable g(θn, xn) can be seen
as the observation of the Markov process {θn} through a noisy memoryless channel g(∙, Xn). There-
fore the random process {g(θn, xn)} is a HMP. See Ephraim & Merhav (2002) for an information
theoretic survey.
If the learning is done perfectly, the HMP {g(θn, xn)} converges to the uniform distribution of
correct labels. Since the random variables are discrete, entropy is a continuous function of the distri-
bution pn. Therefore as the correct labels are uniformly distributed, the entropy H(y^ approaches
its maximum log K. The instantaneous entropy H(yQ would converge monotonically to the en-
tropy of the stationary distribution log K if the sequence were to be a Markov chain. This could
explain the monotonicity of H(^n) in the experiments. Although the sequence not a Markov chain
but it is indeed a HMP, the following proposition shows that the entropy is lower-bounded by an
increasing function.
Proposition 3. Suppose that the Markov process {θn} with the probability distribution qn has a
stationary distribution q. We have
H(yn) ≥ log |K| - Dgnkq).
The proof follows from data processing inequality for Kullback-Leibler divergence and is found in
Appendix B. Note that since {θn} is a Markov process, D(qnkq) is non-increasing.
4	SGD on Joint Probability Measures
In the previous section, the non-decreasing property of H (In was investigated by modeling the
network output as an HMP. In the same spirit let us define yn = C(Xn) to be i.i.d. realizations of
y. In the ideal situation where the network manages to learn successfully the true labels, we can say
yn converges to yn almost surely4. The joint distribution of (∕n yn)) specifies a point on the space
of probability measures on Y × Y. The task of learning consists tuning the parameters θn in a way
that the joint distribution approaches the distribution of (yyn, yyn). Therefore the gradient descent
steps corresponds to a sequence of points, that is joint distributions of (^n, y^, on the space of
probability measures on Y × Y with the end point ideally being the joint distribution of (yyn, yyn). In
this section, we investigate the gradient descent algorithm by exploring the path it takes on the space
of probability measures on Y X Y5. The trajectory of conditional entropy H(^n∣yn) is determined
for the trajectory of probability distributions on the space of joint measures.
However it is in general difficult to precisely characterize this path. Instead, one might ask how
the gradient descent trajectory compares with a certain natural path on the space of distributions. A
4A weaker notion of convergence can be used although this choice is not crucial for the next results.
5The problem of finding the optimal way to change the distribution is connected to the problem of optimal
transport. We would like to thank (removed for anonymity) for mentioning this connection.
5
Under review as a conference paper at ICLR 2019
Experiment
Markov Process (α-SMLC)
Training Steps (%)
10080604020
Steps (%)
ISlNn
Training Steps (%)
10080604020
Steps (%)
Figure 3: Experiments for FCNN and LeNet-5. This figures follow the same format as Figure 2(a).
The shape of the makers differentiate between different values of p. For the α-SMLC model the
white star shows the inflection point of H(y|y) and α = 0.85.
relevant question is to ask what the shortest path between these probability measures is on this space
and how similar is the trajectory of SGD compared to this shortest path. To be able to formally ad-
dress this issue we require to define curves and lengths on the metric space of probability measures.
The space of probability distributions defined on the discrete space Y × Y, denoted by P(Y × Y),
with total variation metric dTV(∙, ∙) is a simplex in a finite dimensional Euclidean space. The total
variation metric is equivalent to the L1-distance between the points in the corresponding Euclidean
space. A curve in this space is defined by a continuous function σ : [0, 1] → P(Y × Y). The curve
is called a shortest path if it has minimal length among all curves with endpoints σ(0) and σ(1).
Note that the length is measured in this space using L1-norm. The following theorem guarantees
that there is a shortest path on this space between probability measures and it can be traveled using
a Markov chain.
Theorem 1.	The shortest path between two probability measures μ and V on the space of discrete
probability measures P(Y X Y) is given by tμ +(1 - t)ν for t ∈ [0,1]. Furthermore if the Prob-
ability measures are represented by row vectors there is a transition matrix Π with the stationary
distribution V such that μn = μ∏n is on the line segment between μ and V and limn→∞ μn = ν.
Proof. Not that the space of probability distributions here is a bounded compact metric space with
each two points connected by a rectifiable curve. The existence of shortest path follows from
(Burago et al., 2001, Corollary 2.5.20,Theorem 2.5.23). The transition matrix in Theorem 1 is
given simply by
Π= (1-α)I+α1TV
where 1 = (1,1,..., 1).	□
To see the implication of previous theorem more precisely, consider conditional distributions
P(yn ∈ ∙∣yn = l) and let Pl(n) ∈ RK be the following vector for l ∈ Y, that is
Pl(n) = (P口 = 0∣yn = l), P(yn = E = l),…，P。= K -旧=l)) .
6
Under review as a conference paper at ICLR 2019
We use the compact notation P(n) ∈ RK×K for the matrix with po(n),...,PK-ι(n) as rows.
Note that, with the assumption that 晨 is uniformly distributed, the joint distribution of	晨)is
fully determined by P(n) since P(y, Nn) = KKP(^∣Yn). Suppose that the initialization θo is such
that g(θo, ∙) initially maps all inputs to the same class (the first class is assumed for simplicity).
Therefore the initial distribution matrix P (0) assumes no knowledge about the input and is given by
PN(0) = 1e1T .
Ideally, in a learning algorithm, the matrix P (0) converges to an optimal distribution P * as n → ∞,
that is P* = limn→∞ P(n). If P"、= yn) = 1, We have
PN* = I .
Now that we set the initial distribution and stationary distributions, the following transition matrix
provides a way to pass from the initial distribution to the stationary one.
Definition 1. α- Simple Markov Learning Chain (α-SMLC)
Given 0 < α < 1, the Sequence ofrandompairs {(^n, Nn)} is an α-SMLC if Pl (n) = Pl(n 一 1)∏ι,
for every n ≥ 0, l ∈ Y with
∏l = 1eT + α(I - 1eT),
PN(0) = 1elT .
The above construction provides a different transition matrix for each PNl(n) depending on l. The
following theorem describes how an α-SMLC moves P(n) in the space of stochastic matrices. It
actually shows this construction leads to points on the shortest path between the measures.
Theorem 2.	If (In yn) is the n-th random pair generated of an α-SMLC, then
PN (n) = (1 一 t)PN (0) + tI , with t = (1 一 αn).	(3)
Proof. c.f. Appendix B
□
This shows that P(n) belongs to the continuous curve from equation 3, regardless of the choice ofα.
Moreover, let ^(t) be a version of y、parametrized by t ∈ [0,1] such that its conditional distribution
with Nyn corresponds to P(n) = (1 一 t)P (0) + tI.
Proposition 4. If {(y、，y、)} is an α-SMLC and Z follows the distribution
P(z = i)
1 一 p, i = 0
Kp-1,	i ∈{1,..∙,k - 1} ，
then
dH(∂tt)ly) = K [p log(1 一 tp) + (1 一 K + P) log(1 一 t + tKp- 1)
一 (K 一 1)(1 一P) log(t(1 一 P)) — (K - 1)plog(t Np 1 )i
K 一 1 J
Proof. c.f. Appendix B	□
Corollary 1. In the setting of Proposition 4, if P → 0 then H (^(t)∣y) has one maximum at t = 11,
H (y( 1 )∣y) = K-1 iog2.
This result allows us to characterize the shape of the 2D curves (H(^(t)),H(^(t)∣y)) for the above
construction.
We now consider the implication of Markov assumption for error. Define y： ∈ Y for all l ∈ Y to be
random variables distributed according to the conditional probabilities P(y： = k) = P(g(θn x)=
k|Ny、 = l) for all k ∈ Y. Note that, since Ny、 is uniformly distributed, one can compute the joint dis-
tribution of (y：, Nn) from the marginal distributions ofg：,..., ^K-1 and vice-versa. The following
propositions shows that the Markov trajectory implies the reduction in error as well.
7
Under review as a conference paper at ICLR 2019
Training Steps (%)
Step (%)
Oooo
1 8 6 4 2 0
一 _l_
HSINn
3.0
2.5
^2.0
卮
≥T1.5
1. 0
0.5
0.0
Training Steps (%)
W*
Step (%)
∞ O O O O
1 8 6 4 2
Figure 4: Experiments for FCNN and LeNet-5. This figures follow the same format as Figure 2(b),
with α = 0.85. The shape of the makers differentiate between different values ofp.
Proposition 5. If {y； } is a stationary Markov Chain converging to the distribution p* = el then
P(yn = l|yn = l) ≤ P(^n+1 = l|yn = l)
Corollary 2. If {y；} is a stationary Markov Chain converging to the distribution p* = el for all
l ∈ Y then
P(yn=yn ) ≥ P(^n+1=yn ) ∙
This last results shows that ε is non-increasing with n (i.e., no over-fitting), which is a desirable
property for any learning algorithm. We will show through numerical simulations how a comparable
behavior is observed for gradient descent methods.
5 Experiments
In this section we compare gradient based learning to the α-SMLC model through empirical simula-
tions. First, We use the datasets where SGD is extremely successful at the classification task (ε ≈ 0),
that is the MNIST dataset and the spirals dataset (Anonymous, 2018). The spirals dataset constitutes
a 2D-spiral classification task constructed as
X = ((√a + b) Cos (2πa + 2∏y) , (√a + b) sin (2πa + 2∏y)),
where a ∈ [0,1], b ∈ [0,0.1] and y ∈ {0,1,..., K - 1} are independent uniformly distributed
random variables and K = 3. This dataset is divided into a training set of 50 000 samples and a
test set of 2 000. Furthermore, we train the FCNN of Anonymous (2018) for the spirals dataset and
use LetNet-5 (LeCun et al., 1999) for MNIST, achieving an average accuracy above 99% in both
cases. For the sake of completeness we perform more experiments on the CIFAR-10 and CIFAR-
100 datasets using the DenseNet architecture from Huang et al. (2017) with 40 and 100 layers
respectively. FCNN and LetNet-5 are trained using Adam’s optimizer, while DenseNet is trained
with SGD. More detailed explanation about the experimental setup is provided in Appendix C. We
estimate use a naive estimator of entropy which consists on computing the empirical distribution of
8
Under review as a conference paper at ICLR 2019
0 8 6
L00
(Z云H
slaripS
2 0 8 6 4 2 0
- - - - - - -
Iiooooo
(W) H
Training Steps (％)
Training Steps (%)
o
6
ɛ
(a) Underfitting (P = 0.1)	(b)OVerfitting
Figure 5: Experiments for FCNN on the spirals dataset. This figures follow the same format as
Figures 3 and 4.
(y, y) and directly calculating entropy afterwards. This method is known to have an approximation
error of K2/N (Miller, 1955), which is good enough for our experiments since N is much larger
than K2 in our datasets. For cases with larger K one could use more sophisticated methods, such as
SChurmann (2004); Archer et al. (2014). For our experiments we introduce i.i.d. noise to the dataset
labels before training according to P(z = 0) = 1 - p and P(z 6= 0) = p/(K - 1). We keep fixed
α = 0.85 for all simulations.
In Figure 3 we show the similarity between the α-SMLC model and ANNs on the spirals and MNIST
dataset. This figure is obtained by averaging the entropy values over 100 realizations of training.
We observe ANNs move along the information plane in a similar way as the α-SMLC model, and
converge to the optimal distribution. In addition, We display the inflection point of H (y|y) of the α-
SMLC model for different values of p. Interestingly, as labels get noisier the inflection point occurs
at a larger H(^) value. This seems to be the case for SGD learning as well. This phenomena suggests
that, as labels get noisier, a learning algorithm needs to know more about the input distribution before
it can start assigning labels efficiently. Similar conclusions can be drawn form Figure 4, which is
also obtained by averaging over 100 realizations of training. An interesting result is that, regardless
of the value of p, a good learning algorithm should converge to a point that lies on the upper bound
provided by Fano’s inequality. These Figures artificially mitigate the randomness induced by SGD
on the trajectory by averaging over several realizations of training 6. How much SGD oscillates
seems to depend on the experimental setup, such as the learning rate, dataset, and the structure of
the classifier. See Appendix C for examples of highly oscillating trajectories.
Our last experiment consists on investigating how underfitting and overfitting affects the trajectory
of SGD. To that end in Figure 5(a) we include an `1 norm regularization term into the loss func-
tion (details in Appendix C) controlled by a parameter λ. As expected for sufficiently small λ the
minimum error is attained. Interestingly, as we induce underfitting by increasing this regularization
coefficient the obtained models move away from Fano’s bound. This naturally leads to increased
error values. On the other hand, in Figure 5(b) we increase the number of parameters of FCNN and
reduce the dataset size in order to induce overfitting (details in Appendix C). While overfitting leads
to larger error as well as underfitting, it can be distinguished by its trajectory in the entropy-error
plane. Underfitting seems to push models away from Fano’s bound while overfitting happens when
an ANN is at the bound. This experiment shows that the information plane provides a richer view
beyond train and test error that allow us to observe effects that were previously hidden. Further
understanding about desired trajectories is interesting since it may allow practitioners to monitor
models during training, spot undesired behaviors, and possibly tune hyperparameters accordingly.
6The realizations that did not converge are discarded.
9
Under review as a conference paper at ICLR 2019
References
Dana AnglUin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343-370,
1988.
AnonymoUs. This reference is not disclosed in order to mantain anonymity. 2018.
Evan Archer, Il Memming Park, and Jonathan W Pillow. Bayesian entropy estimation for coUntable
discrete distributions. The Journal of Machine Learning Research ,15(1):2833-2868, 2014.
Amir R. Asadi, EmmanUel Abbe, and Sergio Verd. Chaining MUtUal Information and Tightening
Generalization Bounds. arXiv preprint arXiv:1806.03803, June 2018.
Dmitri Burago, IU D. Burago, and Serge Ivanov. A course in metric geometry. Number v. 33 in
Graduate studies in mathematics. American Mathematical Society, Providence, RI, 2001. ISBN
978-0-8218-2129-9.
T. M. Cover and Joy A. Thomas. Elements of information theory. Wiley-Interscience, Hoboken,
N.J, 2nd ed edition, 2006. ISBN 978-0-471-24195-9. OCLC: ocm59879802.
Imre Csiszar and JanoS Korner. Information theory: coding theorems for discrete memoryless ^ys-
tems. Cambridge University Press, Cambridge ; New York, 2nd ed edition, 2011. ISBN 978-0-
521-19681-9.
Y. Ephraim and N. Merhav. Hidden Markov processes. IEEE Transactions on Information Theory,
48(6):1518-1569, June 2002. ISSN 0018-9448. doi: 10.1109/TIT.2002.1003838. URL http:
//ieeexplore.ieee.org/document/1003838/.
M. Feder and N. Merhav. Relations between entropy and error probability. IEEE Transactions on
Information Theory, 40(1):259-266, January 1994. ISSN0018-9448. doi: 10.1109/18.272494.
Marylou Gabrie, Andre Manoel, Clement Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala,
and Lenka Zdeborova. Entropy and mutual information in models of deep neural networks. arXiv
preprint arXiv:1805.09785, 2018.
Leonid Gurvits and James Ledoux. Markov property for a function of a markov chain: A linear
algebra approach. Linear algebra and its applications, 404:85-117, 2005.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, volume 1, pp. 3, 2017.
Varun Jog and Venkat Anantharam. The entropy power inequality and Mrs. Gerber’s lemma for
groups of order 2n. IEEE transactions on information theory, 60(7):3773-3786, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Patrick Haffner, Leon Bottou, and Yoshua Bengio. Object recognition with gradient-
based learning. In Shape, contour and grouping in computer vision, pp. 319-345. Springer, 1999.
George A Miller. Note on the bias of information estimates. Information theory in psychology:
Problems and methods, 2(95):100, 1955.
Daniel Russo and James Zou. How much does your data exploration overfit? Controlling bias via
information usage. arXiv preprint arXiv:1511.05219, November 2015.
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren-
dan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning.
In International Conference on Learning Representations, 2018.
Thomas Schurmann. Bias analysis in entropy estimation. Journal of Physics A: Mathematical and
General, 37(27):L295, 2004.
Richard Serfozo. Basics of applied stochastic processes. Springer Science & Business Media, 2009.
10
Under review as a conference paper at ICLR 2019
Ravid Shwartz-Ziv and Naftali Tishby. Opening the Black Box of Deep Neural Networks via Infor-
mation. arXiv preprint arXiv:1703.00810, March 2017.
Peter Spreij. On the markov property of a finite hidden markov chain. Statistics & probability letters,
52(3):279-288,2001.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The Information Bottleneck Method. In
Proc. 37th Annu. Allerton Conf. Commun., Control, Comput., pp. 368-377, 1999.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. In Advances in Neural Information Processing Systems 30, pp. 2521-2530. 2017.
11
Under review as a conference paper at ICLR 2019
A Collection of Results for Learning with Random Labels
For ε < 1 - -K, proposition 1 can be written as a lower bound on ε through the following Proposition
of Anonymous (2018).
Proposition 6. (Anonymous, 2018) Φ(H(Z)) ≤ ε, where Φ : [0, log |K|] → [0,1 — ] is the inverse
function of Ψ in the interval [0,1 - -K].
Proof. For independent noise z, we have
Haly) ≥ H (y|y, W = H(Zly, y) = H(Z).
By Proposition 1,it follows that H(z) ≤ Ψ(ε). Applying the inverse Φ(x), X ∈ [0,1- -K], completes
the proof.	口
In Anonymous (2018) it is shown that this bound is sharp when Z is distributed such that Φ(H(Z)) =
p, thus p ≤ ε. We generaliZe this result in the following theorem for an arbitrary distribution of Z,
under some mild conditions, and show that p ≤ ε is in fact a sharp lower bound for arbitrary Z.
Lemma 1. Let Z ∈ Y be a random variable with P (Z = 0) = 1 - p, then
H(Z) ≤ Ψ(p) .
Proof. Let us define βk，P(z = k) and the auxiliary random variable Z ∈ {1,...,K - 1} with
P (Z = k) = βk∕p for all k = 1,...,K - 1.
H(Z) = -(1 - p) log(1 - p)
= -(1 - p) log(1 - p)
= -(1 - p) log(1 - p)
= -(1 - p) log(1 - p)
=-(1 — p)log(1 — P) — P log P + PH (Z)
=hb(p) + pH (Z)
≤ hb(P) + P log(K - 1)
= Ψ(P)
□
Theorem 3. IfP(Z = 0) = 1 - P and P(Z = 0) > P(Z = k) for all k = 1, . . . , K - 1, then
P≤ε,
and equality is attained if and only if P (^ = y) = 1.
Proof. of Theorem 3 For the sake of notation let us define
δk，P(^ = y ㊉ k)
βk , P(Z = k) for all k ∈ {0, . . . , K - 1}
βmax ,	max βk .
k=1,...,K -1
From Proposition 1 we know that ε ≥ Φ(H (Z)). Since Ψ is an increasing function in the interval
[0,1 -%]and Φ is its inverse in that interval, We have that Φ is an increasing function as well. From
Lemma 1 (c.f. Appendix B) we know that H(Z) ≤ Ψ(P), this leads to
Φ(H(Z)) ≤ Φ(Ψ(P)) = P .
K-1
-	βk log βk
k=1
K-1
-PX
k=1
K-1
-PX
k=1
K-1
βk log βkp
PP
Bk 1	V- β 1 β
—log P - P	log —
-	βk logP + PH(yZ)
k=1
12
Under review as a conference paper at ICLR 2019
Then, if the bound from Proposition 1 were to be sharp, ε could reach values strictly lower than p.
We will show that this is not possible.
1 - ε = P(y = y)	(4)
K-1
=X P(y = y|z = k)P(z = k)	(5)
k=0
K-1
=X P(y = y ㊉ k)P(z = k)	(6)
k=0
K-1
= X δkβk	(7)
k=0
K-1
= (1 -p)δ0+ X βkδk	(8)
k=1
K-1
≤ (1 -p)δ0+ X βmaxδk	(9)
k=1
= (1 - p)δ0 + βmax(1 - δ0)	(10)
≤ (1 - p)δ0 + (1 - p)(1 - δ0)	(11)
= (1-p).	(12)
If ε < p we obtain (1 - p) < 1 - ε ≤ (1 - p) which is a contradiction, hence it must hold that
ε ≥ p.
Finally, if ε = p then equation 10 yields
1 - p ≤ (1 - p)δ0 + βmax(1 - δ0) .
Since βmaχ < (1 - p), this inequality holds if and only if δo = 1, that is P(y = y) = 1.	□
This theorem shows that the minimum expected error ε can only be attained if g(θ, ∙) manages to
denoise the labels, hence εy = 0. We extend this result by deriving bounds for εy, given ε and p, in
the following theorem.
Theorem 4. (Angluin & Laird, 1988)
Given ε < 1 — K^ and p < 2, if P(z = 0) = 1 — P, and P(z = k) < 1 for all k = 1,...,K — 1,
then εyis bounded by
ε-p	ε-p
-----≤ ε ≤ -------.
1 - p 1 - 2p
Proof. of Theorem 4
1 - ε = P(y = y)
K -1
X P (y ㊉ k = ^) p (z = k)
k=0 X {z	}x{	}
δk	βk
K -1
(1-p)(1-εy)+ Xδkβk
k=1
K -1
≤ (1 - p)(1 - εy) + βmax X δk
k=1
= (1 - p)(1 - εy) + βmaxεy
= (1 - p) - εy((1 - p) - βmax ) ,
(13)
(14)
13
Under review as a conference paper at ICLR 2019
(I - P) - (I - ε
(1 - P) - βmax
ε ≤
≤ (1 - P) - (1 - ε) = ε - P
一 (1 — P) — p 1 — 2p
Applying PK=-II δkβk ≥ 0 in equation 13 leads to 1 - ε ≥ (1 - p)(1 - ε) thus
(I - P) - (I - D
(1 - P)
ε-P
1-P
≤ ε,
≤ ε
⇒
which completes the proof.
□
Corollary 3. If P < 2, ε < 1 — K, and 1—2p < 1 — K then
max{H (yly),H(而}≤ ψ(≡P).
Proof. Since Ψ is an increasing function in the interval [0,1 - K], the proof follows from applying
Theorem 4 on Proposition 1.	□
In information theory there is a result of this kind, known as Mrs. Gerber’s Lemma (MGL), that does
not require knowledge about P and ε. MGL provides an upper bound on H (y|y) given H (y|y) for the
case of K = 2. This result also states that the minimum H(y|y) is attained when H(y∣^) = 0. Since
we assumed yy to be uniformly distributed, this corresponds to εy = 0, up to permutation ambiguities.
Generalizing MGL for arbitrary K is still an open question in information theory. Nevertheless,
Jog & Anantharam (2014) successfully proved MGL for the cases where K is a power of 2. We
summarize that result in the following proposition.
Proposition 7. Generalized Mrs. Gerber’s Lemma for K = 2n(Jog & Anantharam, 2014)
with
f2n (yy, z)
min
H(y∣y)=y,H(z)=z
H(y ㊉ZIy)
f2n(yy,z)
f2(yy- klog2,z - klog2) + klog2
max(yy, z)
ifklog2 ≤yy,z ≤ (k+1)log2,
otherwise
where
f2(x, y) = h(h-1(x) ? h-1(y))
and k is an arbitrary positive integer and a ? b , a(1 - b) + b(1 - a).
We have derived inequalities that relate entropies and error values. Then we showed that in the
presence of corrupted labels, the best g(θ, ∙) can do for minimizing ε is to learn c, regardless of the
value of P.
B Deferred Proofs
Proof of Proposition 3: The proof follows from the following general theorem.
Theorem 5. Let {yn} be an HMP defined as the observation of a Markov process {xn} through
an arbitrary stationary memoryless channel with values in the state space Y. Suppose that the
probability distributions on the respective state spaces of {xn} and {yn} are given by {qn} and
{pn } with the stationary distribution q and p. Then
D(pn kp) ≤ log D(qnkq).
Proof. Based on the assumption above, xn and yn are related according to the conditional distribu-
tion characterized by the conditional probabilities {r(∙∣χ) : X ∈ X}. Denote the joint distribution of
Xn and ∖n by μn defined on X X Y and given by qn (x) X r(∙∣x). The stationary joint distribution
μ is defined by q X r. Using the chain rule (Cover & Thomas, 2006, Theorem 2.5.3), We have:
D(μnkμ) = D(qnkq) + D(H∣r) = D(qn∣q)∙
On the other hand, the chain rule and the non-negativity of Kullback-Leibler divergence shows that:
D(μnkμ) ≥ D(Pn kp),
which implies the theorem.	□
14
Under review as a conference paper at ICLR 2019
2.00
1.75
1.50
1.25
经 1.00
W
♦ 0.75
0.50
0.25
0.00
HSINn
Training Steps (%)
Training Steps (%)
Training Steps (%)
Training Steps (%)
Figure 6: Mrs Gerber’s Lemma in action.
Using the fact that the stationary distribution of yn is equal to the uniform distribution, We have:
D(yn∣y) = iog ∣κ∣- H。).
This fact along With Theorem 5 proves Proposition 3.
Proof. of Theorem 2
Since 1elT1elT = 1elT We get
(∏ι)2 = IeTleT +α 1eT(I - 1eT) +α (I - 1eT)1eT +α2(I - 1eT)
I -- } I —{Z	_}	{_—	_}
IeT	0	0
= 1elT + α2(I - 1elT).
By induction We get
(Πl)n = 1elT + αn(I - 1elT) = (1 - αn)1elT + αnI.
Replacing t = (1 - αn) ∈ [0, 1] yields
(Πl)n =t1elT+(1-t)I.
Finally,
Pl (n)= Pl (0)(∏1 )n
=Pl(0)(t1eT + (1- t)I)
=(1 - t)pl (0)+ tel,
thus
Pl (n) = (Po(n)T,..., PK-1 (n)T)T = (1 -埒Pl (0)+ tI.
□
Proof. of Proposition 4
Since y = y ㊉ z the distribution of y can be expressed as a circular convolution between the dis-
tributions of Z and y. Informally, we express this as P (y) = P (y ㊉ Z) = P(y) ~ P (z). The same
holds true if we condition by y, that is P(y|y = l) = P(y ㊉ z|y = l) = P(y|y = l) ~ P(z) for all
15
Under review as a conference paper at ICLR 2019
l = 0, . . . , K - 1. Then for uniformly distributed y, and z distributed according to equation 4, we
can express this relation in matrix form as
P(t) = ((1 - P - -p-)I +	11T) P(t).
K-1 K-1
、--------------{z------------}
circular convolution matrix of P (z)
Therefore, the (i, j)-th element of P(t) is denoted as Pi,j (t) and given by
Pij ⑴=eTP (t)ej =(I- t)eTej+t(I- P-K-I )eTej+tK-1
Differentiating Pi,j (t) with respect to t yields
* = -eTej+(i-p - K- )eTej+K-
For particular choices of i, j these expressions boil down to
(1 - t + t(1 - p),	j =	1,i	= j	(-P,	j = 1,i = j
J 1 - t + t K-1,	j =	1,i= j	and dPi,j ⑴=J	K-1 -	1, j = 1,i = j
I t(1 - P),	j =	1,i	= j	∂t I	1 - p, j = 1,i = j	.
ItK-1,	j =	1,i= j	J K-1,	j = 1,i = j
Finally, We make Use of these to derive a closed form expression for H(y⅛t"y), that is
H(y(t)∣y) _ ∂ X 1 XPWl p#
—∂t — = ∂t [- ^ KzPij (t)g Pij (t)
1	K K ∂Pij (t)
=-kΣΣ -ɪ2(1+logPi,j⑻
j =1 i=1
=-K XX ∑[ d¾≡(1 + iog Pij (t))
j =1 i=j
-	K XX X[ 3(1+log Pij (t))
j=2 i=j
-K XX X[ d¾≡(1 + iog Pij (t))
j =1 i6=j
-K X ∑[ 54+iog Pij (t))
j=2 i6=j
=⅛pIOg(I - tP) + W(K - I)( KP 1 - I)(I + IOg(I -1 +t-Jp 1 ))
K	K	K- 1	K- 1
-	⅛(K - I)(I -P)(I+log(t(I -P))) - ⅛(K - I)P(I+log(t P ))
K	K	K- 1
K (P-(K-1)(KpT - I)-(K- I)(I-P)-(K- I)P
'---------------------------------
{z
=0
=K IPlog(1 - tP) + (1 - K + p) log(1 - t + tK- 1)
-	(K - 1)(1 -p) log(t(1 - p)) - (K - 1)plog(t p )i .
K- 1
}
Remark: For P → 0 we now that the maximum the curve is at t = 1 since
lim H 吧Iy) = 0 ⇒ ⅛[(1 - K)log(1-1) - (1 - K )log t]=0 ⇒ log= =0 ⇒
p→0	∂ t	K	t
1
2
16
□
Under review as a conference paper at ICLR 2019
Table 1: Simulation Parameters for Figure 7 (Anonymous, 2018). The models with highest accuracy
are used in the main paper.
Dataset	Activation	Batch Size	γmax	γmin	Test Accuracy
	tanh	128	10-1	10-2	99.7%
Spirals	sigmoid	128	10-1	10-5	99.6%
	ReLU	700	10-1	10-5	97.8%
	tanh	128	10-2	10-2	97.1%
MNIST	sigmoid	128	10-2	10-4	96.3%
	ReLU	128	10-2	10-4	99.1%
Table 2: Simulation Parameters for DenseNet on CIFAR datasets
Dataset	Activation	Batch Size	γmax	γmin	Test Accuracy
CIFAR-10	-ReLU^^	64	10-1	10-1	80.2%
CIFAR-100	-ReLU^^	64	10-1	10-1	80.2%
C Experimental Details and Complementary Experiments
A fully connected ANN with four hidden layers of five neurons each, as FCNN, is trained on the
spirals dataset. For the MNIST dataset, the popular convolutional network called LeNet-5 LeCun
et al. (1999) is used. To train these networks we let the learning rate γ ∈ R start at a given γmax ∈ R
and then decay by 40% per epoch until reaching some given minimum learning rate γmin < γmax,
that is γ = max{γmax0.6bepochc , γmin}. For the CIFAR-10 dataset we train a 100 layer DenseNet
architecture as done in Huang et al. (2017), but we stop the training after 10 epochs instead of
the original 300 used by the authors. The different configurations used for these experiments are
summarized in Table 1. For instance, Figure 2 shows 1 realization of SGD training for DenseNet on
CIFAR-100. In that figure we observe a rather stable trajectory, with not much oscillation. However,
in Figure 8 we average over 2 realizations of SGD learning for DenseNet on CIFAR-10 and obtain
highly oscillating trajectories. As expected, both trajectories follow a similar behavior as the α-
SMLC model.
In Figure 5(a) the same model as in Figure 3 is used but an additional regularization term is added
to the loss function, that is λkw k1 where w is a vector containing all the weights in the network.
In Figure 5(b) a FCNN with a single hidden layer of size 100, and tanh activations, is used and the
dataset sizes are reduced according to 3. Other parameters are γmax = 10-1, γmax = 10-2, and the
number of epochs is 10 000.
Table 3: Dataset Sizes for Figure 5(b)
Dataset Size
200
p
0.1
0.2	400
0.4	1000
17
Under review as a conference paper at ICLR 2019
MNIST
Training StePS (％)
f c4
Z L
(W) H
Training StePS (％)
方A
1
Γ V
0∙
O
-
O
(W) H

Training StePS (％)
(Z国)H
Training StePS (％)
(A国)H
pom"IS
Training StePS (％)
Training StePS (％)
Training StePS (％)
Figure 8: Experiments for DenseNet on CIFAR-10. This figures follow the same format as Figures
3 and 4.
Training StePS (％)
100
(W) H
Figure 7: Information plane trajectory during the learning process (Anonymous, 2018).
5 O 5
Z Z L
(W) H
01-RAFIC
H (y)
80
18