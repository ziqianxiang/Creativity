Under review as a conference paper at ICLR 2019
Exploring and Enhancing the Transferability
of Adversarial Examples
Anonymous authors
Paper under double-blind review
Ab stract
State-of-the-art deep neural networks are vulnerable to adversarial examples,
formed by applying small but malicious perturbations to the original inputs. More-
over, the perturbations can transfer across models: adversarial examples generated
for a specific model will often mislead other unseen models. Consequently the
adversary can leverage it to attack deployed systems without any query, which
severely hinders the application of deep learning, especially in the safety-critical
areas. In this work, we empirically study how two classes of factors those might
influence the transferability of adversarial examples. One is about model-specific
factors, including network architecture, model capacity and test accuracy. The
other is the local smoothness of loss surface for constructing adversarial examples.
Inspired by these understandings on the transferability of adversarial examples, we
then propose a simple but effective strategy to enhance the transferability, whose
effectiveness is confirmed by a variety of experiments on both CIFAR-10 and
ImageNet datasets.
1	Introduction
Recently Szegedy et al. (2013) showed that an adversary is able to fool deep neural network models
into producing incorrect predictions by manipulating the inputs maliciously. The corresponding
manipulated samples are called adversarial examples. More severely, it was found that adversarial
examples have cross-model generalization ability, i.e., that adversarial examples generated from
one model can fool another different model with a high probability. We refer to such property
as transferability. By now, more and more deep neural network models are applied in real-world
applications, such as speech recognition, computer vision, etc.. Consequently, an adversary can
employ the transferability to attack those deployed models without querying the systems (Papernot
et al., 2016b; Liu et al., 2017), inducing serious security issues.
Understanding the mechanism of transferability could potentially provide various benefits. Firstly,
for the already deployed deep neural network models in real systems, it could help to design better
strategies to improve the robustness against the transfer-based attacks. Secondly, revealing the mystery
behind the transferability of adversarial examples could also expand the existing understandings on
deep learning, particularly the effects of model capacity (Madry et al., 2018; Fawzi et al., 2015) and
model interpretability (Dong et al., 2017; Ross and Doshi-Velez, 2018). Therefore, studying the
transferability of adversarial examples in the context of deep networks is of significant importance.
In this paper, we numerically investigate two classes of factors that might influence the adver-
sarial transferability, and further provide a simple but rather effective strategy for enhancing the
transferability. Our contributions are summarized as follows.
•	We find that adversarial transfer is not symmetric, which means that adversarial examples
generated from model A can transfer to model B easily does not implies the reverse is also
natural. Second, we find that adversarial examples generated from a deep model appear
less transferable than those from a shallow model. We also explore the impact of the non-
smoothness of the loss surface. Specifically, we find that the local non-smoothness of loss
surface harms the transferability of generated adversarial examples.
•	Inspired by previous investigations, we propose the smoothed gradient attack to improve
the adversarial transferability, which employs the locally averaged gradient instead of the
1
Under review as a conference paper at ICLR 2019
original one to generate adversarial examples. Extensive numerical validations justify the
effectiveness of our method.
2	Related work
The phenomenon of adversarial transferability Was first studied by Szegedy et al. (2013). By
utilizing the transferability, Papernot et al. (2016b;a) proposed a practical black-box attack by training
a substitute model With limited queried information. Liu et al. (2017) first studied the targeted
transferability and introduced the ensemble-based attacks to improve the transferability. More
recently, Dong et al. (2018) shoWed that the momentum can help to boost transferability significantly.
MeanWhile, there exist several Works trying to explain the adversarial transferability. Papernot et al.
(2016a) attributed the transferability to the similarity betWeen input gradients of source and target
models. Liu et al. (2017) proposed to use the visualization technique to see the large-scale similarity
of decision boundaries. HoWever, our numerical investigations imply that these similarity-based
explanations have their intrinsic limitation that they cannot explain the non-symmetric property of
adversarial transferability.
Our smoothed gradient attacks that enhance the transferability by utilizing the information in the small
neighborhood of the clean example is inspired by the Works on shattered gradients (Balduzzi et al.,
2017) and model interpretability (Smilkov et al., 2017). Recently, similar strategies are also explored
by Athalye et al. (2018a) and Warren He (2018) for White-box attacks, Whereas We focus on the
black-box settings. Our method is also related to the Work by Athalye et al. (2018b), Which introduced
the expectation over transformation (EOT) method to increase robustness of adversarial examples.
The EOT formulation is similar to our objective (7), but they did not study the transferability. Also
our motivations are totally different from theirs.
3	Background
3.1	Adversarial attack
Let f(x) : Rd 7→ RK denote the classifier. In deep learning, it is found that for almost any sample x
and its label ytrue, there exists a small perturbation η that is nearly imperceptible to human such that
argmax fi(x) = ytrue,	argmax fi(x + η) 6= ytrue.
ii
(1)
We call η the adversarial perturbation and correspondingly xadv := x + η the adversarial example.
The attack (1) is called a non-targeted attack since the adversary has no control over Which class the
input x Will be misclassified to. In contrast, a targeted attack aims at fooling the model to produce a
Wrong label specified by the adversary, i.e. argmaxi fi (x + η) = ytarget.
In this paper, We consider the pure black-box attacks. This means the adversary has no knoWledge of
the target model (e.g. architecture and parameters) and is also not alloWed to query any input-output
pair from the target model. On the contrary, the white-box attack means that the target model is
available to the adversary.
3.2	Generating adversarial examples
Modeling In general, crafting adversarial examples can be modeled as the folloWing optimization
problem,
maximizex0	J (x0) := J(f(x0), ytrue)
s.t. ∣x0 - x∣∞ ≤ ε,
(2)
where J is some loss function measuring the discrepancy between the model prediction and ground
truth; the '∞ norm ∣∣ ∙ k∞ is used to quantify the magnitude of the perturbation. Other norm is also
possible, but We focus on '∞ norm in this paper. To improve the strength of adversarial transferability,
instead of using a single model, Liu et al. (2017) proposed the ensemble attack, which generates
2
Under review as a conference paper at ICLR 2019
adversarial examples against a model ensemble f 1(x), f 2(x),…，f M(x):
maximize#，J	X fk(x0), ytrue)
s.t. kx - xk∞ ≤ ε.
(3)
Optimizer We use the following iteration to solve (2) and (3),
xt+1 = ProjD (Xt + α sign(Vχ J(Xt))).	(4)
where D = [0, 255]d ∩ {x0 | kx0 - xk ≤ ε} and α is the step size. We call the attack that evolves (4)
for T steps iterative gradient sign method (IGSM) attack (Kurakin et al., 2017; Madry et al., 2018).
Furthermore, the famous fast gradient sign method (FGSM) is a special case with α = ε, T = 1.
Dong et al. (2018) recently proposed the momentum-based attack as follows
gt+1 = μgt + VJ (xt)∕kVJ (xt )kι
Xt+1 = projD (Xt + α sign(gt)) ,
(5)
where μ is the decay factor of momentum. By using this method, they won the first-place in NIPS
2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions. We will call it
mIGSM attack in this paper.
4	Evaluation of Adversarial Transferability
Datasets To evaluate the transferability, three datasets including MNIST, CIFAR-10 and ImageNet
are considered. For ImageNet, evaluations on the whole ILSVRC2012 validation dataset are too time-
consuming. Therefore, in each experiment we randomly select 5, 000 images that can be correctly
recognized by all the examined models to form our new validation set.
Models (i) For MNIST, we trained fully connected networks (FNN) ofD hidden layers, with the width
of each layer being 500. (ii) For CIFAR-10, we trained five models: lenet,resnet20, resnet44, resnet56,
densenet. The test accuracies of them are 76.9%, 92.4%, 93.7%, 93.8% and 94.2%, respectively. (iii)
For ImageNet, the pre-trained models provided by PyTorch are used. The Top-1 and Top-5 accuracies
of them can be found on website1. To increase the reliability of experiments, all the models have
been tested. However, for a specific experiment we only choose some of them to present since the
findings are consistent among all the tested models.
Criteria Given a set of adversarial pairs, {(Xiadv, yitrue)}iN=1, we calculate the Top-1 success rate (%)
fooling a given model f (x) by 詈 PN=I 1[argmaxi fi(χadv) = ytrue]. For targeted attacks, each
image Xadv is associated with a pre-specified label ytarget 6= ytrue. Then we evaluate the performance
of the targeted attack by the following Top-1 success rate:詈 PN=I 1[argmaXi f (Xadv) = ykrget].
The corresponding Top-5 rates can be computed in a similar way.
5	How Model-specific Factors Affect Transferability
Previous studies on adversarial transferability mostly focused on the influence of attack methods (Liu
et al., 2017; Dong et al., 2018; Tramer et al., 2017; Kurakin et al., 2017). However it is not clear how
the choice of source model affects the success rate transferring to target models. In this section, we
investigate this issue from three aspects including architecture, test accuracy and model capacity.
5.1	Architecture
We first explore how the architecture similarity between source and target model contributes to the
transferability. This study is crucial since it can provide us guidance to choose the appropriate source
models for effective attacks. To this end, three popular architectures including ResNet, DenseNet and
VGGNet are considered, and for each architecture, two networks are selected. Both one-step and
multi-step attacks are performed on ImageNet dataset. Table 1 presents the experiment results, and
the choice of hyper-parameters is detailed in the caption.
1http://pytorch.org/docs/master/torchvision/models.html
3
Under review as a conference paper at ICLR 2019
Table 1: Top-1 success rates(%) of FGSM and IGSM attacks. The row and column denote the source and target
models, respectively. For each cell, the left is the success rate of FGSM (ε = 15), while the right is that of IGSM
(T = 5, α = 5, ε = 15). The dashes correspond to the white-box cases, which are omitted.
	resnet18	resnet101	vgg13_bn	vgg16,bn	densenet121	densenet161
resnet18	-	36.9 / 43.4	51.8/58.0	45.1/51.7	41.1 / 49.2	30.0/35.8
resnet101	48.5 / 57.2	-	38.9/41.6	33.1/40.0	33.2 / 46.9	28.7/43.2
vgg13_bn	35.5/26.8	14.8 / 10.8	-	58.8/90.7	19.1 / 15.9	13.8/11.7
vgg16,bn	35.2/26.1	15.6 / 11.1	61.9/91.1	-	21.1 / 16.8	15.8/13.2
densenet121	49.3/63.8	34.4 / 50.7	47.6/58.7	41.0/57.8	-	38.5/73.6
densenet161	45.7 / 56.3	33.8 / 54.6	48.6 / 56.0	41.3/55.9	43.4 / 78.5	-
We can see that the transfers between two models are non-symmetric, and this phenomenon is more
obvious for the models with different architectures. For instance, the success rates of IGSM attacks
from densenet121 to vgg13_bn is 58.7%, however the rate from vgg13-bn to densenet121 has only
15.9%. The lack of symmetry implies that previous similarity-based explanations of adversarial
transferability are quite limited.
Another interesting observation is that success rates between models with similar architectures are
always much higher. For example the success rates of IGSM attacks between vgg13Jbn and vgg16bn
are higher than 90%, nearly twice the ones of attacks from any other architectures.
5.2	Model Capacity and Test Accuracy
We first study this problem on ImageNet dataset. A variety of models are used as source models to
perform both FGSM and IGSM attacks against vgg19-bn, and the results are displayed in Figure 1.
The horizontal axis is the Top-1 test error, while the vertical axis is the number of model parameters
that roughly quantifies the model capacity.
We can see that the models with powerful attack capability concentrate in the bottom left corner,
whereas for those models with either large test error or large number of parameters, the fooling rates
are much lower. We also tried other models with results shown in Appendix C , and the results show
no difference.
70
o 60
三50
S
40 40
+->
E
30 30
e
d
j
o
ɑʒ
q
E
n
N
20
10
37.38
34.56
IGSM: vgg19 bn
49.68
42.42
.47.44 ⅛-43∙3
.46.04
53.76
48.98
25
30
35
40
26.06
45
Top-1 Error (%)
70-
60-
50-
40-
30-
20-
10-
0-
*31-5
FGSM: vgg19 bn
36.1
34.4
.35.9	*40∙1
.37.3
40.3
★42.0
.30.2
★ resnet
■ densenet
• Vgg
• alexnet
♦ SqUeeZenet
25	30	35	40	45
Top-1 Error (%)
0
Figure 1: Top-1 success rates of IGSM (T = 20,α = 5,ε = 15) and FGSM(ε = 15) attacks against vgg19bn
for various models. The annotated value is the success rate transferring to the vgg_19. Here, the models of
vgg-style have been removed to exclude the influence of architecture similarity. For the same color, the different
points corresponds networks of different depths.
We suspect that the impact of test accuracy is due to that the decision boundaries of high-accuracy
models are similar, since they all approximate the ground-truth decision boundary of true data very
well. On the contrary, the model with low accuracy has a decision boundary relatively different
from the high-accuracy models. Here the targeted network vgg19bn is a high-accuracy model.
Therefore it is not surprising to observe that high-accuracy models tend to exhibit stronger attack
capability. This phenomenon implies a kind of data-dependent transferability, which is different from
the architecture-specific transfers observed in the previous section.
4
Under review as a conference paper at ICLR 2019
It is somewhat strange that adversarial examples generated from deeper model appear less trans-
ferable. To further confirm this observation, we conduct additional experiments on MNIST and
CIFAR-10. Table 2 shows the results, which is basically consistent. This observation suggests us not
to use deep models as the source models when performing transfer-based attacks, although we have
not fully understand this phenomenon.
Table 2: Top-1 success rates (%) of attack from the source model (row) to the target model (column). (a) FGSM
attack for MNIST with ε = 40 and D denotes the depth of the fully connected network. (b) FGSM attack for
CIFAR-10 with ε = 10.
(a) MNIST
	D = 0	D = 2	D=4	D = 8
D = 0	-	62.9	62.9	64.4
D = 2	52.9	-	48.3	49.4
D = 4	47.3	43.1	-	44.8
D = 8	31.2	29.2	29.0	-
(b) CIFAR-10
	resnet20 resnet44		resnet56	densenet
resnet20	-	70.4	64.0	71.6
resnet44	65.4	-	57.1	65.8
resnet56	66.2	62.9	-	40.3
6	Non-smoothness of the Loss S urface
In this section, we consider that how the smoothness of loss surface J (x) affects the transferability.
For simplicity, let g(χ) = NxJ (x) denote the gradient. Smilkov et al. (2017) showed that gradient
g(x) is very noisy and uninformative for visualization, though the model is trained very well. Balduzzi
et al. (2017) studied a similar phenomenon that gradients of deep networks are extremely shattered.
Both of them implies that the landscape is locally extremely rough. We suspect that this local
non-smoothness could harm the transferability of adversarial examples.
6.1	Intuition
For simplicity, assume model A and B are the source and target models
which are well trained with high test accuracies, respectively. Previous
methods use gA(x) to generate adversarial perturbations, so the transfer-
ability mainly depends on how much sensitivity of gA for model A can
transfer to model B. As illustrated in Figure 2, where three curves de-
note the level sets of three models, we can see that the non-smoothness
can hurt the transferability, since both model A and B have very high
test accuracy, their level sets should be similar globally, and JB(x) is
probably unstable along gA . As illustrated, the local oscillation of gA
makes the sensitivity less transferable. One way to alleviate this is to
smooth the landscape JA, thereby yielding a more transferable gradient
Ga, i.e.hGa, gBi > h^A, ^b). Here we use G denote the gradient of
the smoothed loss surface.
6.2	Justification
Figure 2: Illustration of how
the non-smoothness of the loss
surface hurts the transferabil-
ity. For any a, let a denote the
unit vector O-.
kak2
To justify the previous arguments, we consider smoothing the loss surface by convolving it with a
Gaussian filter, then the smoothed loss surface is given by Jσ(x) := Eξ〜N(o,i)[J(X + σξ)]. The
corresponding gradient can be calculated by
Gσ (x) = Eξ 〜N (0,I) [g(x + σξ)].
(6)
The extent of smoothing is controlled by σ, and please refer to Appendix A to see the smoothing
effect. We will show that Gσ (x) is more transferable than g(x) in the following.
Gradient similarity We first quantify the cosine similarity between gradients of source and target
models, respectively. Two situations are considered: vgg13±n→vgg16Jbn, densenet121 →vgg13Jbn,
which correspond the within-architecture and cross-architecture transfers, respectively. We choose
σ = 15, and the expectation in (6) is estimated by using ml Pm=I g(χ + ξi). To verify the averaged
gradients do transfer better, we plot the cosine similarity against the number of samples m. In
Figure 3a, as expected, we see that the cosine similarity between GA and gB are indeed larger than
5
Under review as a conference paper at ICLR 2019
the one between gA and gB . Moreover, the similarity increases with m monotonically, which further
justifies that GA is more transferable than gA .
Visualization In Figure 3b we visualize the transferability by comparing the decision boundaries
of model A (resnet34 ) and model B (densenet121). The horizontal axis represents the direction of
GA of resnet34, estimated by m = 1000, σ = 15, and the vertical axis denotes orthogonal direction
7	； 入 ∖ A El ∙	j 1	1	j 1	1 ∙ j ∙ j
hA := gA - hgA, GAiGA. This process means that we decompose the gradient into two terms:
gA = αGA + βhA with hGA , hAi = 0 . Each point in the 2-D plane corresponds to the image
j 1	11	1	1	1	1 •	1 ∙	/	,	z^∕	,	1' /ʌ CLL∖ 1	∙ j 1	1	•
perturbed by u and v along each direction, clip(x + u GA + v hA, 0, 255), where x is the clean image.
The color corresponds to the label predicted by the target model.
It can be easily observed that for model A, a small perturbation in both directions can produce wrong
classification. However, when applied to model B, the sensitivities of two directions dramatically
change. The direction of hA becomes extremely stable, whereas to some extent GA preserves the
sensitivity, i.e. GA do transfer to model B better than hA . This suggests that for gradient gA , the
noisy part hA is less transferable than the smooth part GA . We also tried a variety of other models
shown in Appendix C, and the results are the same.
m
(a)
Model A: resnet34	Model B: densenetl21
(b)
Figure 3: (a) Cosine similarity between the gradients of source and target models. (b) Visualization of decision
boundaries. The origin corresponds to the clean image shown in Figure 10 of Appendix. The same color denotes
the small label, and the gray color corresponds to the ground truth label.
7	Smoothed Gradient Attack
7.1	Method
Inspired by the previous investigations, enhancing the adversarial transferability can be achieved by
smoothing the loss surface. Then our objective becomes,
maximize Jσ(x0) := Eg-N(o,i)[J(x0 + σξ)]
s.t.	kx0 - xk ≤ ε.
(7)
Intuitively, this method can also be interpreted as generating adversarial examples that are robust to
Gaussian perturbation. Expectedly, the generated robust adversarial examples can still survive easily
in spite of the distinction between source and target model.
If use the iterative gradient sign method to solve (7), we have the following iteration:
Gt
1m
mm X VJ(Xt+ξi),	& 〜N(0,σ21)
i=1
(8)
xt+1 = projD (xt + αsign(Gt)) ,
where Gt is a mini-batch approximation of the smoothed gradient (6). Compared to the standard
IGSM, the gradient is replaced by a smoothed version, which is endowed with stronger transferability.
Therefore we call this method sg-IGSM. The special case T = 1, α = ε, is accordingly called
sg-FGSM. Any other optimizer can be used to solve the (7) as well, and we only need to replace the
original gradient with the smoothed one.
6
Under review as a conference paper at ICLR 2019
7.2	Choice of Hyper Parameters
We first explore the sensitivity of hyper parameters m, σ when applying our smoothed gradient
technique. We take ImageNet dataset as the testbed, and sg-FGSM attack is examined. To increase
the reliability, four attacks are considered here. The results are shown in Figure 4.
We see that sg-FGSM consistently outperforms FGSM for all distortion level, although the improve-
ment varies for different ε. Furthermore, larger m leads to higher success rate due to the better
estimation of the smooth part of gradient, and the benefit starts to saturate after m ≥ 30. For the
smoothing factor σ, we find neither too large nor too small value can work well, and the optimal σ is
about 15. Overly large σ will introduce a large bias in (8), and extremely small σ is unable to smooth
the landscape enough.
Therefore, in the subsequent sections, we will use m = 20, σ = 15 to estimate the smoothed gradient
and only report the result for one distortion level.
Ooooo
8 6 4 2
(δ2e-l sseɔɔns I01
6o4o2oo
(δ2e-l sseɔɔnsdol
0	5	10	15	20	25
Perturbation ε
0	5	10	15	20	25
Perturbation ε
(a)
Figure 4: (a) Success rates (%) for sg-FGSM attacks with different m. Here we use σ = 15. (b) The sensitivity
of the hyper parameter σ.
m = 30, ε = 10
60-
s巴 ssəɔɔns'dol
0	5	10	15	20	25
Noise level: σ
(b)
7.3	Effectiveness
Single-model attack We first test the effectiveness of our method for single-model based attacks
on non-targeted setting. To make a fair comparison, we fix the number of gradient calculation
per sample at 100. Specifically, for sg-IGSM we have T = 5 due to m = 20, whereas T = 100
for IGSM. The results are shown in Table 3. We see that smoothed gradients do enhance the
transferability dramatically for all the attacks considered. Please especially note those bold rates,
where the improvements have reached about 30%.
Table 3: Top-1 success rates(%) of non-targeted IGSM and sg-IGSM attacks. The row and column denote the
source and target modesl, respectively. For each cell, the left is the success rate of IGSM (T = 100, α = 1),
while the right is the that of sg-IGSM (T = 5, α = 5). In this experiment, distortion level ε = 15.
	densenet121	resnet152	resnet34	vgg13,bn	vgg19_bn
densenet121	-	50.1/80.6	59.9 / 87.2	62.2 / 82.2	56.5 /84.3
resnet152	52.5/81.3	-	57.2 / 85.6	47.7/71.1	42.9 / 72.6
resnet34	51.5/76.4	46.5/73.1	-	53.8/74.8	49.1 / 74.5
vgg13_bn	24.1/49.2	14.3/33.5	25.1 / 54.1	-	90.6 / 96.4
vgg19,bn	27.1/57.5	16.7/41.6	27.6 / 60.7	92.0/96.1	-
Ensemble attack In this part, we examine the ensemble-based attack on the targeted setting2. For
targeted attacks, generating an adversarial example predicted by target models as a specific label is
2Compared to non-targeted attack, we find that a larger step size α is necessary for generating strong targeted
adversarial examples. Readers can refer to Appendix B for more details on this issue, though we cannot fully
understand it. Therefore a much larger step size than the non-target attacks is used in this experiment.
7
Under review as a conference paper at ICLR 2019
too hard, resulting in a very low success rate. We instead adopt both Top-1 and Top-5 success rates
as our criteria to better reflect the improvement of our method. A variety of model ensembles are
examined, and the results are reported in Table 4.
Table 4: Top-5 success rates (%) of ensemble-based approaches for IGSM (T = 20, α = 15) versus sg-IGSM
(T = 20, α = 15). The row and column denote the source and target models, respectively. The Top-1 rates can
be found in Appendix C, which show the same trend as the Top-5 rates.
Ensemble	resnet152 resnet50	vgg16_bn
resnet101+densenet121	28.1/56.8 26.2/52.4 8.1/29.7
resnet18+resnet34+resnet101+densenet121	50.4/70.4 54.7/72.4 28.1/52.6
vgg11 _bn+vgg13_bn+resnet18 +resnet34+densenet121	24.3/55.8 36.9/65.9 62.2/83.5
As we can see, it is clear that smoothed gradient attacks outperform the corresponding normal ones
by a remarkable large margin. More importantly, the improvement never be harmed compared to
single-model case in Table 3, which implies that smoothed gradient can be effectively combined with
ensemble method without compromise.
Momentum attack In this experiment, three networks of different architectures are selected. As
suggested in Dong et al. (2018), We choose μ = 1, and all attacks are iterated for T = 5 with step size
α = 5. In Table 5, we report the results of non-targeted attacks of three attacks including mIGSM,
sg-IGSM and mIGSM with smoothed gradient (sg-mIGSM). It is shown that our method clearly
outperforms momentum-based method for all the cases. Moreover, by combining with the smoothed
gradient, the effectiveness of momentum attacks can be further improved significantly.
Table 5: Top-1 success rates (%) of momentum attacks and smoothed gradient attacks. The row and column
denote the source and target model, respectively. Each cell contains three rates corresponding to mIGSM,
sg-IGSM and sg-mIGSM attacks, respectively.
	resnet18	densenet121	vgg13_bn
resnet18	-	65.6/73.1/865	70.4 / 77.7 / 86.7
densenet121	72.7/84.5/91.1	-	68.7/80.3/86.7
vgg13_bn	43.1/58.6/74.3	28.4/44.7/60.9	-
7.4	Robustness
Smoothed gradient attacks can be viewed as generating adversarial examples robust against Gaussian
noise perturbations. Therefore, we are interested in how robust is the adversarial example against
other image transformations. To quantify the influence of transformations, we use the notion of
destruction rate defined by Kurakin et al. (2017). The lower is this rate, the more robust are the
adversarial examples.
Densenet121 and resnet34 are chosen as our source and target model, respectively; and four image
transformations are considered: rotation, Gaussian noise, Gaussian blur and JPEG compression.
Figure 5 displays the results, which show that adversarial examples generated by our methods are
more robust than those generated by vanilla methods. This numerical result is interesting, since we
only explicitly increase the robustness against Gaussian noise in generating adversarial examples. We
speculate that the robustness can also transfer among different image transforms.
JPEG ComPression
Quality
----FGSM
----IGSM: T=10
----IGSM: T=30
——Sg-FGSM
----Sg-IGSM:T=10
----Sg-IGSM:T=30
Figure 5: Destruction rates of adversarial examples for various methods. For smoothed gradient attacks, we
choose m = 20, σ = 15. The distortion ε = 15.
8
Under review as a conference paper at ICLR 2019
8	Conclusion
In this paper, we first investigated the influence of model-specific factors on the adversarial trans-
ferability. It is found that the model architecture similarity plays a crucial role. Moreover models
with lower capacity and higher test accuracy are endowed with stronger capability for transfer-based
attacks. we second demonstrate that the non-smoothness of loss surface hinders the transfer of
adversarial examples. Motivated by these understandings, we proposed the smoothed gradient attack
that can enhance the transferability of adversarial examples dramatically. Furthermore, the smoothed
gradient can be combined with both ensemble and momentum based approaches rather effectively.
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International
Conference on Machine Learning, volume 80, pages 274-283. PMLR, 2018a.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In Proceedings of the 35th International Conference on Machine Learning, volume 80,
pages 284-293. PMLR, 2018b.
David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
The shattered gradients problem: If resnets are the answer, then what is the question? In
Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 342-
350. PMLR, 2017.
Yinpeng Dong, Hang Su, Jun Zhu, and Fan Bao. Towards interpretable deep neural networks by
leveraging adversarial examples. arXiv preprint arXiv:1708.05493, 2017.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Fundamental limits on adversarial robustness.
In Proc. ICML, Workshop on Deep Learning, 2015.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
ICLR Workshop, 2017.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples
and black-box attacks. In International Conference on Learning Representations, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against deep learning systems using adversarial examples.
arXiv preprint arXiv:1602.02697, 2016b.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability
of deep neural networks by regularizing their input gradients. In Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
9
Under review as a conference paper at ICLR 2019
Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of
transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017.
Dawn Song Warren He, Bo Li. Decision boundary analysis of adversarial examples. In International
Conference on Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2019
A Visualization of Smoothed Gradient
In this section, we provide an visualization understanding how the local average has the smoothing
effect. We choose densenet121 as the model and visualize the Saliency map of gradient NxJ (x)
and the smoothed versions for varying m. Two images are considered, and the results are shown in
Figure 6. We can see that local average can smooth the gradient significantly. Please refer to the work
by Smilkov et al. (2017) for more details.
Figure 6: Visualization of gradients. The leftmost is the examined image. The second corresponds to the original
gradient, whereas the remaining two images corresponds to the smoothed gradients estimated by different m.
m = 200, σ = 0.10
B Influence of step size for targeted attacks
When using IGSM to perform targeted black-box attacks, there are two hyper parameters including
number of iteration T and step size α. Here we explore their influence to the quality of adversarial
examples generated. The success rates are calculated on 5, 000 images randomly selected according
to description of Section 4. resnet152 and vgg16bn are chosen as target models. The performance
are evaluated by the average Top-5 success rate over the three ensembles used in Table 4.
Figure 7: Average success rates over three ensembles for different step size α and number of iteration k. The
three ensembles are the same with those in Table 4. Distortion ε = 20.
Figure 7 shows that for the optimal step size α is very large, for instance in this experiment it is
about 15 compared to the allowed distortion ε = 20. Both too large and too small step size will yield
harm to the performances. It is worth noting that with small step size α = 5, the large number of
iteration provides worse performance than small number of iteration. One possible explanation is
that more iterations lead the optimizer to converge to a more overfitted solution. In contrast, a large
11
Under review as a conference paper at ICLR 2019
step size can prevent it and encourage the optimizer to explore more model-independent area, thus
more iteration is better.
C Additional Results
How Model Capacity and Test Accuracy Affect Transferability In this part, we additionally
explore the impact of model capacity and test accuracy by using resnet152 as our target model and
perform transfer-based attacks against from a variety of models and the results shown in Figure 8.
The observations are consistent with the observation in Section 5.2.
IGSM: resnet152
FGSM: resnet152
仑 140∙
三 120-
S 100-
■M
E
æ
ω
d
<-»—
80-
60-
40-
12.02
140
120
100
80
60
resnet
densenet
Vgg
alexnet
squeezenet
14.8
q
E
ZJ
N
20-
0-
51.4
■51.18
44.98
46.0
40
20
25
30
35
40
.7.62
45
29.5
29.4
26.8
29.4
25
30
35
.10.6
40	45
0
★

Top-1 Error (%)
Top-1 Error (%)
Figure 8: Top-1 success rates of IGSM (T = 20, α = 5, ε = 15) and FGSM(ε = 15) attacks against resnet152
for various models. The annotated value is the success rate transferring to the resnet152. Here, the models
of resnet-style have been removed to exclude the influence of architecture similarity. For the same color, the
different points corresponds networks of different widths.
Visualization of Decision Boundary In this part, we provide additional results on the visualization
of the decision boundary. Different Figure 3b, we here consider the sign of two directions, since the
attack method actually updates using the sign (g) rather than g. In Figure 9, we show the decision
boundary. Each point corresponds to perturbed image:
clip(x + usign (GA) + vsign (hA) , 0, 255).
Here the model A is resnet34, and the definitions of GA and hA are the same as before. The color
denotes the label predicted by the target model, and the gray color corresponds to the ground-truth
label. We can see that the smoothed gradient GA is indeed more transferable than the noise part hA .
Figure 9: Visualization of decision boundaries. The source model is resnet34, and 9 target models are considered.
The horizontal and vertical direction corresponds to u and v , respectively.
12
Under review as a conference paper at ICLR 2019
Additional Results for the Effectiveness of Smoothed Gradient Attacks In this part, we present
some additional results to show the effectiveness of smoothed gradient attack.
Table 6: Top-1 success rates (%) of ensemble-based non-targeted IGSM and sg-IGSM attacks. The
row and column denote the source and target model, respectively. The left is the success rate of IGSM
(T = 100, α = 3), while the right is that of sg-IGSM (T = 50, α = 3). The distortion ε = 20.
Ensemble	densenet121	resnet152	resnet50	vgg13_bn
resnet18+resnet34+resnet101	87.8 / 97.8	94.6 / 98.9	97.4 / 99.4	84.1/96.1
vgg11 _bn+densenet161	86.8/97.2	62.9 / 89.7	80.3 / 94.8	94.9 / 98.4
resnet34+vgg16_bn+alexnet	68.9/91.3	54.6 / 87.2	77.9 / 96.2	98.1 / 99.1
Table 7: Top-1 success rates (%) of ensemble-based targeted IGSM and sgd-IGSM attacks. The row
and column denote the source and target model, respectively. The left is the success rate of IGSM
(T = 20, α = 15), while the right is that of sg-IGSM (T = 20, α = 15). The distortion ε = 20.
Ensemble	resnet152 resnet50 vgg13_bn vgg16_bn
resnet101+densenet121	11.6/37.1 11.9/34.5 2.6/ 105 2.6/14.1
resnet18+resnet34+resnet101+densenet121	30.3/ 55.2 36.8/ 57.3 10.8/29.1 12.8/35.0"
vgg11_bn+vgg13 _bn+resnet18+ resnet34+densenet121	10.1/35.1 22.2/47.9	-	42.1/72.1
D The Examined Image for Visualization of Decision B oundary
Figure 10: The image used to perform decision boundary visualization. Its ID in ILSVRC2012
validation set is 26, with ground truth label being table lamp.
13