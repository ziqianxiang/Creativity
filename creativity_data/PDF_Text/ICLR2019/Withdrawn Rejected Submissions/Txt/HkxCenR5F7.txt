Under review as a conference paper at ICLR 2019
Variational recurrent models for
REPRESENTATION LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
We study the problem of learning representations of sequence data. Recent work
has built on variational autoencoders to develop variational recurrent models for
generation. Our main goal is not generation but rather representation learning
for downstream prediction tasks. Existing variational recurrent models typically
use stochastic recurrent connections to model the dependence among neighbor-
ing latent variables, while generation assumes independence of generated data per
time step given the latent sequence. In contrast, our models assume independence
among all latent variables given non-stochastic hidden states, which speeds up
inference, while assuming dependence of observations at each time step on all la-
tent variables, which improves representation quality. In addition, we propose and
study extensions for improving downstream performance, including hierarchical
auxiliary latent variables and prior updating during training. Experiments show
improved performance on several speech and language tasks with different levels
of supervision, as well as in a multi-view learning setting.
1 Introduction
Modeling sequence data is a central problem in domains such as speech and natural language pro-
cessing. In this work, we study the problem of learning representations (features) of sequence data,
with varying degrees of supervision, that can be helpful for downstream sequence labeling tasks. We
take a generative approach, inspired by variational autoencoders (VAEs) Kingma & Welling (2014);
Rezende et al. (2014); Doersch (2016) for non-sequence data. We assume that the sequence data
is generated by a series of latent variables, parameterize the observation likelihood with recurrent
neural networks, and maximize (a lower bound on) the observation likelihood. If labels are avail-
able during training, a discriminative loss can also be imposed on the latent variables. The learned
posterior distribution of the latent variables then provides features for the downstream tasks. This
intuitively simple approach, however, requires non-trivial extensions of the deep generative models
that have been successful for non-sequence data.
For a (non-sequence) observation x, a VAE assumes a prior distribution, p(z), on the latent vari-
able z which is often simple (e.g., N(0, I)), and consists of a generation network parameterizing
p(x|z) with weights θ and an inference network parameterizing the approximate posterior q(z|x)
with weights φ. Learning the VAE is done by maximizing an evidence lower bound (ELBO) on the
observation log-likelihood logpθ(x) = log Jp(z)pθ(χ∣z)dz, defined as
ELBO ：= Eqφ(z∣x) [logPθ(x∣z)] — DKL (qφ(z∣x)∣∣p(z)) ≤ logpθ(x).	⑴
VAEs can be viewed as autoencoders: The encoder q(z|x) maps the input to a latent variable z ac-
cording to a (also typically Gaussian) posterior distribution, and the decoderp(x|z) reconstructs the
input from samples of z drawn from this posterior. The output of the encoder (most commonly, the
mean of q(z|x)) is often used as a learned representation (features) for downstream tasks. Maximiz-
ing the ELBO is equivalent to minimizing the reconstruction loss of the output x with respect to the
input, plus a regularizer based on the prior distribution of the latent variable. In some variants the im-
portance of the KL term is tuned by weighting the regularizer by a hyperparameter β Higgins et al.
(2016); Alemi et al. (2016).
VAEs have been extended to model sequence data in a number of ways. Fabius & van Amersfoort
(2014) learn a single representation for the entire sequence, while Hsu et al. (2017) learn both a
1
Under review as a conference paper at ICLR 2019
whole-sequence representation and a set of representations for pre-defined segments within the se-
quence. For many tasks, such as the ones we consider here, it is desirable to represent a length-T
input sequence x1:T with a corresponding length-T latent sequence z1:T so as to fit directly into
typical recurrent network-based prediction models.
Several recent approaches fit this criterion Krishnan et al. (2015); Archer et al. (2015); Chung et al.
(2015b); Fraccaro et al. (2016); Goyal et al. (2017a); Chen et al. (2018). Learning is again done by
maximizing an ELBO, with the main differences among approaches being the specific forms of the
prior p(z1:T) (typically parameterized so as to capture dynamics in the latent space), the genera-
tion distribution pθ(XLT|zi：T), and the approximate posterior qφ(z±T|xi：T). For example, direct
recurrent connections between stochastic variables Fraccaro et al. (2016), as shown in Figure 1(a),
or indirect recurrent connections, e.g. ht-1 → zt-1 → ht → zt Chung et al. (2015b); Goyal et al.
(2017a), are often introduced in p(z1:T) to model the dependence between neighboring latent vari-
ables. While this is more powerful than a simpler prior for the purpose of generation, it poses
challenges for designing the approximate posteriors due to the dependencies among zt’s. On the
other hand, given z1:T, the generation model is often fully factorized into T independent terms:
Pθ (xi:T ∣Z1:T) = ∏T=1 Pθ (xt∣zt).
Most prior work on variational recurrent models has focused on generation quality and likelihood
evaluation. It is not clear, however, that the learned representations are useful for downstream tasks.
The goal of our work is to fill this gap by developing variational recurrent models that produce
high-quality representations for prediction tasks, and to investigate several modeling choices.
We take an approach, similarly to Chen et al. (2018), where the latent distribution p(z1:T) fac-
tors over time conditioned on a deterministic hidden state sequence, while the generation model
Pθ(xi：T|zi：T) is factored as ∏T=ιpθ(xtlzi：T), as shown in Figures 1(b,d). Also similarly
to Chen et al. (2018), we use a simple Gaussian prior which is updated during training, and find that
this can be very helpful. Unlike Chen et al. (2018), however, we consider a wide variety of tasks
and levels of supervision. Compared to variational recurrent models with recurrent connections
among the stochastic latent variables, this class of models is more efficient and easier to implement,
since there are no dependencies between stochastic latent variables and therefore no nested sampling
procedure.
One key novelty in this work is that, while we have no recurrent connections among the z1:T, we
require each zj to generate each xk with some probability, for all 1 ≤ j, k ≤ T , so that the fea-
tures are predictive of a large context of observations. We propose a “stochastic generation” model
that approximates this idea, and we find that this extension is important for improving downstream
performance.
In addition, we study the proposed models on a range of speech and NLP prediction tasks, in a variety
of supervision settings, finding improved performance in all cases. We consider fully supervised,
semi-supervised, and weakly supervised (using multi-view training data) settings. We believe this is
the first attempt at a broad study of recurrent variational representation learning that is applicable to
a variety of downstream tasks and levels of supervision.
2	A Variational Recurrent Representation Learning Model
Our variational recurrent representation learning approach, shown in Figure 1(b-d), aims to learn in-
formative representations while keeping inference simple. As in other recurrent variational models,
we optimize an ELBO of the same form as in Equation 1 but where the variables are all sequences
of length T; that is, the prior is p(z1:T), the posterior is qφ (z1:T |x1:T), and the generation model is
pθ (x1:T |z1:T).
2.1 Inference
Given a length T input x1:T, the model first maps from the input sequence to a sequence of de-
terministic recurrent hidden state vectors h1:T, produced by one or more stacked recurrent neural
network layers. Conditioned on ht (1 ≤ t ≤ T), the posterior distribution of the latent random
variable zt is a spherical Gaussian
ZtIht 〜qφ(ZtIht) ≡ N(μφt, diag(σφt)),	[μφt, logσΦt] = FΦ(ht)	(2)
2
Under review as a conference paper at ICLR 2019
Figure 1: a) A variational recurrent model with stochastic recurrent connections (red arrows); (b)
Proposed model with no latent stochastic variables dependencies and with stochastic generation;
(c) An example of stochastic generation, with a particular choice of Gaussian stochastic generation
distribution; (d) The complete generation process of our model, where each xk is generated with
some probability by each zt . Throughout, dashed lines indicate generation given samples of latent
variables, and double-headed arrows indicate bidirectional recurrent connections. In (a,b), only one
layer of deterministic hidden states ht is shown, but any number can be used.
Here, qφ is the inference network with parameters φ and Fφ is either a linear transformation or a
feedforward neural network with a linear final layer. The full posterior and prior are
TT
qφ (zi:T ∣hi:T ):= Y[qφ(zt∖ht),	p(zi:T )：=	p(zt)	(3)
t=1	t=1
Finally, a reconstruction of the input sequence is generated from the latent stochastic sequence z1:T.
The learned representation of xt, which can be used for downstream tasks, is taken to be the mean
μφt of qφ(zt∖ht).
Note that no direct dependence is assumed among different time steps of z1:T given h1:T.
This greatly simplifies inference over prior work that directly models dependence between the
zt Goyal et al. (2017a); Fraccaro et al. (2016); Chung et al. (2015b); Krishnan et al. (2015), since
the distribution of each zt (given ht) is a single spherical Gaussian rather than a more complex mul-
timodal distribution that would arise with dependence modeling (e.g., using recurrent connections
among stochastic nodes). Consider a model with stochastic recurrent connections as in Figure 1(a),
which models the conditional probability of zt given the past z1:t-1. The marginal distribution of
each zt is multimodal, and requires nested sampling. In order to obtain good estimates, a large num-
ber of samples of z1:t-1 may need to be used. In addition, in a model with multimodal marginal
distributions of zt, it is not clear what to consider to be the learned representation for use in a down-
stream tasks; in particular the mean of zt may have very small probability and thus may not be a
good choice.
2.2 Stochastic generation
Whereas our approach simplifies inference, the generation of each xk for 1 ≤ k ≤ T in our model
involves samples of all z1:T, with each zt for 1 ≤ t ≤ T having a different effect on the generation
of xk, as shown in Figures 1(c,d). Intuitively, this generation approach reintroduces some of the
relationships between the zt that may have been lost by having no direct dependence modeling.
Given a particular choice of k, t, the generative model for xk given zt is also a spherical Gaussian:
Xk∖zt 〜N(μθt,diag(σθt)) = Pθ(Xk ∖zt),	[μθt, log σθt] = Fθ(Zt)	(4)
where again Fθ , with parameters θ, can be linear or a feedforward neural network with a linear
final layer. Note that the full generative distribution (Figure 1(d)) is multimodal, although we never
explicitly evaluate it, as described below.
3
Under review as a conference paper at ICLR 2019
Figure 1(d) shows our full generation process. However, this generation process is a bit expensive;
instead we employ stochastic generation (Figure 1(c)), a technique that aims to approximate the
generation process. In stochastic generation, we first randomly select a frame xk from x1:T for time
step t. During training we evaluate 旧勺力^&)[logp(Xk|zt) for time step t. The way we sample
the target to be reconstructed for time step t affects the way zt and ht encode x1:T. The higher the
probability that a distant time frame is selected, the more temporal information should be encoded
in the latent variables. Next we describe stochastic generation more formally.
Definition 2.1. α(δt,T,k) Given x1:T and Gaussian distribution p(t) (s)≡ N(t, δ), where 1 ≤ t ≤ T.
Let c0:T = {-∞, 1.5, 2.5, ..., T - 0.5, +∞}. We define α(δt,T,k) for 1 ≤ t, k ≤ T as
(5)
Definition 2.2. Stochastic Generation Given δ and sequence x1:T , stochastic generation recon-
structs xk using samples of zt with probability α(δt,T,k). That is, the conditional log-likelihood for
time step t is Eqφ(zt∣ht)[logPθ(xk∣zt)] With probability αδtTk).
In stochastic generation, each zt contributes to xk differently based on the delay between time steps
t and k. If δ is chosen such that only nearby neighbor xks are produced, it becomes a weighted
window generation. Remark 1 shows how a particular instance of stochastic generation relates to
a full generation model, with graphical model as described in Figure 1(d).
Remark 1. Given z1:T , φ and θ, stochastic generation in expectation is evaluating
TT
Σ Eqφ(.ztt∣h-t) Σ αδ,Tk) logPθ(Xk |zt)
t=1	k=1
1}
(6)
Equation 6 is equivalent to computing the expectation of logpθ (X1:T |z1:T ), where
T
TT
Pθ(xi:T ∣Z1:T) = " Pθ (Xtlzi：T) = "口Pθ(χk ∣zt)αδ,T)
t=1	t=1 k=1
(7)
It is straightforward to show that the ELBO from the two perspectives is identical (see the supple-
mentary material). Through stochastic generation, although we are always using simple Gaussian
distributions, we are implicitly creating a more complex generative model. Note that, for purposes of
generation and likelihood evaluation, we directly use the generative model described in Equation 7.
More detailed derivations can be found in the supplementary material.
3	Extensions
Thus far we have described a basic recurrent variational representation learning model that uses
stochastic generation to account for context instead of recurrent stochastic connections. We next
describe a few extensions aimed at improving the learned representations for use in downstream
tasks. First, we embed our model in a multitask training approach, in order to utilize different levels
and types of supervision (full supervision, semi-supervision, and weak supervision in a multi-view
learning setting). Next, similarly to Chen et al. (2018), we consider models with a hierarchy of latent
variables per time step and specify a different functionality for each one, which allows some of the
latent variables to focus on encoding task-specific/view-invariant information; and we study the use
of prior updating in order to improve the quality of inference with our factored posterior.
3.1	Learning with supervision
If we have a task-specific prediction model with loss F and labels l, then we can maximize the
following objective
(1 - α) Lδ,β (X1:T , z1:T, θ, φ) - αF (z1:T, l)	(8)
4
Under review as a conference paper at ICLR 2019
Figure 2: a) A model with hierarchical latent variables. The additional latent variable yt is task-
specific. b) A multi-view recurrent variational model, RecVCCAP. This is a recurrent extension of
variational canonical correlation analysis with private variables (VCCAP, Wang et al., 2016). c) The
hierarchical version ofb, RecVCCAP+H.
where Lδ,β is our representation learning loss (ELBO), with a given stochastic generation parameter
δ and KL term weight β, and α > 0 is a trade-off parameter. This is a multitask loss, where the
latent variable zt needs to perform well with respect to the unsupervised loss (reconstruction of the
input and similarity to the prior) as well as the supervised task-related loss.
In some settings labels are not available, but a second view of the data—that is, a second input se-
quence u1:T paired with each primary input sequence x1:T —is available at training time but not test
time. For example the second view may be from another modality. In such a setting it is often possi-
ble to learn a better representation of the primary view by using the second view as a form of weak
supervision Ngiam et al. (2011); Srivastava & Salakhutdinov (2012); Sohn et al. (2014); Wang et al.
(2015). Here, we extend the variational canonical correlation analysis with private variables (VC-
CAP) model of Wang et al. (2016) to a variational recurrent multi-view model with stochastic gen-
eration, named RecVCCAP for short.
In RecVCCAP, a latent representation y1:T is inferred from x1:T only, but used to reconstruct both
modalities. In addition, z1:T conditioned on deterministic hidden state g1:T , and also o1:T condi-
tioned on deterministic n1:T, are also introduced to model information specific (“private”) to x and
u respectively, as shown in Figure 2(b). The ELBO of this model is a lower bound on the joint
distribution of data in the two views:
log pθ (x1:T , u1:T)
≥	Eqφ (zi：T ,yi：T |gi：T ,hi：T) [ logpθ (X 1:TIZLT,y1：T)] + Eqφ(oi:T ,yi：T |ni：T ,hi：T) [logpθ (ULT |o上T,y1:T)]
-	DKL (qφ(o1:T|n1：T)∣∣p(o1:T)) — DKL (qφ(z1:T|g1:T)∣∣p(z1:T)) - DKL ® (y1:T|h1:T)∣∣p(y1:T))
ς=i ∣Eqφ(ytlht)qφ(ZtIgt)
ςt=i {αδ,T logPθ(Xk lzt)} + Eqe(yt|ht)qe(ot|ntKT=1 {αδ,T logPθ(Uk lot)}
]}
— Dkl®(oi：T |n1:T )∣∣p(o1:T)) — DκL(qφ(z1:T |g1:T )∣∣p(z1:T)) — DκL[qφ(y1:T |h1:T )∣∣p(y1:T))	(9)
Here the generation model includes	Pθ(χ1:T |z1:T, y1:T )p(z1:T )p(y1:T)	and
Pθ(u1:T|o1:T,y1:T)p(o1:T)p(y1:T) for the two modalities respectively. Detailed derivation of
the ELBOs can be found in supplementary material.
3.2	Hierarchical latent variables
In supervised training, it may be beneficial to separate the latent variables into task-specific and task-
independent components, and to apply the supervised loss only to the task-specific latent variables.
The multitask model of Equation 8 can be easily extended with an auxiliary task-specific latent
variable yt, as shown in Figure 2(a). The supervised loss is applied only to yt while the unsupervised
loss (reconstruction + prior) is applied to zt . The multitask loss (negative ELBO+supervised loss)
5
Under review as a conference paper at ICLR 2019
of this hierarchical model is
-(I - α){Lδ,β(XLT, z1:T, yi:T, θ, φ)} + αF(yi:T, I)
≡ -(1 - α)ΣtT=1
Eqφ (yt∣ht)qφ(zt∣yt,ht)
∑T=ι{α(^T log Pθ(xk ∣zt)}
-β{DκL(qφ(zt∖yt, ht)∣∣Pθ(Zt)) + Dkl(qφ(yt∣ht)∣∣Pθ(yt))}} + αF(yi:T,l)	(10)
Another benefit of such a hierarchical model, over the basic one, is that it increases the complexity
of the latent representation——Zt 〜 {qφ(zt) = J qφ(zt,yt)dyt} is multi-modal even though yt and
zt|yt are assumed to be Gaussian—while keeping inference relatively simple since each of the latent
distributions is conditionally a single Gaussian. In this sense the motivation is similar to that of ex-
isting work by Rezende & Mohamed (2015) and Maal0e et al. (2016). A final motivation, similarly
to S0nderby et al. (2016); Zhao et al. (2017), is that this 2-layer model can directl represent a hier-
archical feature space. Unlike prior work, however, a central goal here is to keep the task-specific
latent variable uni-modal so that its mean can be used as the learned representation. The multi-view
recurrent model of Equation 9 can also be extended with hierarchical latent variables, as shown in
Figure 2(c). The resulting ELBO and derivation is given in the supplementary material.
3.3 Prior updating
A simple prior p(Z) ≡ N(0, I) is most common in vanilla VAEs and other variational ap-
proaches (Higgins et al., 2016; Alemi et al., 2016). Some recent work (Serban et al., 2017;
Tomczak & Welling, 2017; Goyal et al., 2017b) has considered more complex prior distributions
in order to better match the true distribution of the assumed latent space. In models of sequence
data, existing variational approaches use priors on Zt that are conditioned on various parts of the
input and latent variables in other frames (Goyal et al., 2017a; Fraccaro et al., 2016; Chung et al.,
2015b).
Motivated by the view of the prior as a regularizer, we propose an approach where the en-
coder/decoder (φ and θ) and the prior are alternately updated during training. In particular, we
set the prior in a given iteration i to be the posterior from the previous iteration: p(πi) (Z1:T) :=
q@(i)(zi：T|xi：T), where p∏0)(zi:T) ≡ NT(0,I) and φ(i), θ(i) are the solutions maximizing the
ELBO with respect to p(i-1) (Z1:T). It is straightforward to show that
Imax EiΦqz1:τιτ |xi：t ) [log(Pθ(x1:T |z1:T ))] - βDKL(qφ(z1:T |x1:T )||Pni) (z1:T )) }
≤
max Eqφ(iz1zτ∣x |xi：T) [log(Pθ(XLT |z1:T ))] - βDKL(qφ(zi:T∣X1:T)∣∣P∏i+i)(zi:T))卜11)
The intuition of this approach is that the KL regularization term should have a large effect at the be-
ginning of training, and the effect should progressively diminish during training. More motivations
and illustrations are given in the supplementary material.
4	Experiments
The main goal of our work is to learn good representations for downstream tasks.
We compare models on several tasks: (1) the CoNNL 2003 named entity recogni-
tion task Tjong Kim Sang & De Meulder (2003) and (2) the CoNNL 2000 text chunking
task Tjong Kim Sang & Buchholz (2000); (3) phonetic recognition on the TIMIT speech cor-
pus Zue et al. (1990) and the University of Wisconsin X-ray Microbeam (XRMB) acoustic-
articulatory data set Westbury et al. (1990) (the latter used for multi-view representation learn-
ing), (4) character-level speech recognition on the Wall Street Journal (WSJ) data set Paul & Baker
(1992). The first of these tasks was also studied by Chen et al. (2018), but only in a semi-supervised
setting. In order to save space, detailed information on all data sets and hyperparameter tuning is
included in the supplementary material.
In the single-view setting, we compare the following models: 1) Variational model with forward
stochastic recurrent connections (StocCon, Figure 1(a)), 2) StocCon with prior updating (Stoc-
Con+P), 3) StocCon with both forward and backward (with the dependence path ZT → ZT-1 → ∙∙∙)
6
Under review as a conference paper at ICLR 2019
Table 1: F1 score of NER on CoNLL 2003 and Chunking on CoNLL 2000. “Baseline” = Two layer
bidirectional GRU recognizer without any representation learning loss.
Model		NER DEV	NER TEST	Chunking DEV	Chunking TEST
1.Baseline	933^	893^	94T	93ΓΓ
2.StocCon	92.4	-	92.8	-
3.StocCon+P	93.2	-	94.0	-
4.RecRep+P	93.6	-	94.1	-
5.RecRep+H+P	93.7	89.8	94.5	93.7
Table 2: TIMIT phonetic error rates (%).
“Baseline” = CTC recognizer without rep-
resentation learning loss.
Model		DEV	TEST
1.Baseline	17.5	19.4
2.StocCon	17.6	-
3.StocCon+P	17.5	-
4.StocCon+B+P	16.9	-
5.RecRep (δ = 0)	18.0	-
6.RecRep+H	17.4	-
7.RecRep+P	17.2	-
8.RecRep+H+P	16.7	19.0
Table 3: WSJ character error rates (%). “Baseline”
= CTC recognizer without any representation learn-
ing loss. “#lab./unlab.” refers to the number of la-
beled/unlabeled utterances used for training.
Model		#lab.	/ #unlab.	DEV	TEST
1.Baseline1	-5K	0^	25.3	228
3.StocCon+P	5K	0	25.5	-
4.RecRep+P	5K	0	24.8	-
6.RecRep+H+P	5K	0	23.1	-
5.RecRep+P	-5K	18K	22.2	-
7.RecRep+H+P	5K	18K	20.7	18.1
stochastic recurrent layers (StocCon+B+P), , 4) Our model, variational recurrent model for repre-
sentation learning, RecRep for short, 5) RecRep with prior updating (RecRep+P), 6) RecRep with
hierarchical latent variables (RecRep+H) and 7) RecRep with both hierarchical latent variables and
prior updating (RecRep+H+P).
In the multi-view setting, the models are our recurrent extensions of variational canonical correlation
analysis (VCCAP) Wang et al. (2016): 1) Recurrent VCCAP with stochastic recurrent connections
with prior updating (StocConVCCAP+P), 2) Our model, recurrent VCCAP without stochastic re-
current connections (RecVCCAP), 3) RecVCCAP with prior updating (RecVCCAP+P), 4) RecVC-
CAP with latent hierarchy (RecVCCAP+H) and 5) RecVCCAP with hierarchy and prior updating
(RecVCCA+H+P, Figure 2(b)).
4.1	Overview of results
The results below follow the same general pattern across tasks. Models with (unidirectional)
stochastic recurrent connections (StocCon) struggle to produce representations that improve over
baselines, even when enhanced with prior updating. Bidirectional stochastic recurrent models (Stoc-
Con+B) improves performance, but at the cost of even more complex inference (for this reason we
only attempted this model on one task, Table 2). Our RecRep models, without stochastic recurrent
connections, always improve over the baselines at least when using prior updating and/or hierar-
chical latent variables. In some experiments we set δ = 0 in the stochastic generation model (i.e.,
generating only xt from zt); this always significantly hurts performance, showing the benefit of
stochastic generation. Depending on the task, either prior updating or hierarchical latent variables is
particularly useful. In all cases the best results are produced by RecRep+H+P models.
4.2	Supervised setting: CoNLL 2003 named entity recognition and CoNLL 2000
CHUNKING
For named entity recognition (NER) and text chunking (Table 1) we follow the setting
of Peters et al. (2017). The input is a pretrained word embedding (GloVe 100-dimensional em-
bedding Pennington et al. (2014)) concatenated with the output of a character RNN embedding.
The baseline prediction model is a 2-layer bidirectional GRU RNN Cho et al. (2014); Chung et al.
(2015a). . The decoder distribution pθ(x|z) is a softmax rather than Gaussian, as in Miao et al.
(2016). A more detailed generation model description is in the supplementary material.
7
Under review as a conference paper at ICLR 2019
4.3	Supervised setting: TIMIT phonetic recognition
For phonetic recognition on the TIMIT data set, we use the same data processing and train/dev/test
split as in Tang et al. (2017).1 The baseline recognizer is a 3-layer stacked bidirectional
LSTM Hochreiter & Schmidhuber (1997) network with pyramidal subsampling as in Chan et al.
(2016); Lu et al. (2016), where the output of each layer (except the topmost layer) is reduced in
length by a factor of two. The supervised phonetic recognition loss is the connectionist temporal
classification (CTC) loss Graves et al. (2013). The representation learning loss (ELBO) is applied to
the second layer. For all speech models, the decoder distribution pθ(x|z) is modeled as a spherical
Gaussian, with diagonal covariance matrix σ2I, where σ is a hyperparameter.
4.4	Semi-supervised setting: WSJ character-level speech recognition
For character-level speech recognition on the Wall Stree Journal data set (WSJ), we use the stan-
dard dev (503 utterances) and test (330 utterances) sets. For semi-supervised experiments, we use
about 5K utterances out of the full WSJ training set as labeled data and an additional subset of
16K utterances as unlabeled data. The recognizers are two-layer bidirectional GRU (BiGRU) net-
works trained with character-level CTC loss as in Miao et al. (2015). The representation learning
loss (ELBO) is applied to the first layer BiGRU output. The results (Table 3) show that our models
can improve performance both in the fully supervised setting (with a small amount of labeled data)
and further in the semi-supervised setting (with some additional unlabeled data).
4.5	Multi-view setting: XRMB phonetic recognition
We use the same setup as in prior work on XRMB Tang et al. (2018): We use the acous-
tic+articulatory data from 35 speakers for representation learning and the acoustic-only data from
12 speakers for phonetic recognition experiments in a 6-fold setup (Table 4). The recurrent mod-
els are 2-layer bidirectional LSTMs with 256 units per layer. We also include a result from prior
work using basic VCCAP with non-recurrent fully connected feedforward networks, which operate
on large overlapping fixed-size segments of the input Tang et al. (2018). Interestingly, here all of
the representation learning approaches improve significantly over baseline, and the non-recurrent
feedforward models perform quite well, although the best performance is again produced by the
proposed RecVCCAP models with stochastic generation. In anecdotal experiments, we find that we
can further improve phonetic error rates to < 7% with either feedforward or RecVCCAP models by
increasing their complexity (e.g., by further increasing the window size in the feedforward models);
however, the memory and computation time requirements of feedforward models quickly become
prohibitive while the recurrent models remain fairly efficient.
Table 4: XRMB phonetic error rates (%) with multi-view representation learning models.
Model	6-fold PER
1.Baseline	12.9
2.Baseline (4-layer)	11.1
3.StocConVCCAP+P	9.9
4.Feed-Forward Tang etal.(2018)	9.4
5.RecVCCAP (δ = 0)	9.7
6.RecVCCAP	9.2
7.RecVCCAP+P	8.9
8.RecVCCAP+H	8.8
9.RecVCCAP+H+P	7.3
4.6	Visualization of learned representations
We visualize the features we learned on XRMB in the multi-view setting via t-SNE embed-
dings Maaten & Hinton (2008). We compare three types of features in Figure 3, computed on 443
randomly selected speech frames each corresponding to one of the 6 vowels [aa,ae,uw,ih,eh,ow]
1We do not, however, use a second validation set as in Tang et al. (2017).
8
Under review as a conference paper at ICLR 2019
Input acoustic features	Model with stochastic recurrent
connections (StocConVCCAP+P)
Our model (RecVCCAP+H+P)
Figure 3: t-SNE plots of several XRMB speech representations, corresponding to rows 1, 3, and 9
of Table 4.
from the 12 test speakers. Qualitatively, these visualizations show that representations learned with
the proposed models form tighter label-specific clusters.
5	Conclusion
We have proposed an approach for learning sequence representations of sequence inputs via vari-
ational recurrent neural models. Unlike most prior work, our approach focuses on the quality of
the learned representation for a variety of downstream tasks rather than on generation quality, and
the representations are learned in a multitask setting along with a supervised, semi-supervised, or
weakly supervised multi-view loss. We use simple, efficient inference networks where the mean of
the latent variables naturally serves as a representation, and we introduce stochastic generation as an
approach for modeling context without a complex inference model. We have studied the proposed
models in the context of several speech and language tasks. We find that the proposed recurrent rep-
resentations consistently improve task performance, and that hierarchical latent variables and prior
updating are useful in a variety of settings.
References
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box
variational inference for state space models. arXiv preprint arXiv:1511.07367, 2015.
William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural
network for large vocabulary conversational speech recognition. In Acoustics, Speech and Signal
Processing (ICASSP), 2016 IEEE International Conference on,pp. 4960-4964. IEEE, 2016.
Mingda Chen, Qingming Tang, Karen Livescu, and Kevin Gimpel. Variational sequential labelers
for semi-supervised learning. In Proc. of EMNLP, 2018.
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent
neural networks. In International Conference on Machine Learning, pp. 2067-2075, 2015a.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems, pp. 2980-2988, 2015b.
Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.
9
Under review as a conference paper at ICLR 2019
Otto Fabius and Joost R van Amersfoort. Variational recurrent auto-encoders. arXiv preprint
arXiv:1412.6581, 2014.
Marco Fraccaro, S0ren Kaae S0nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in neural information processing Systems, pp. 2199-2207,
2016.
Anirudh Goyal, Alessandro Sordoni, Marc-Alexandre C6te, Nan Rosemary Ke, and YoshuaBengio.
Z-forcing: Training stochastic recurrent networks. In Advances in Neural Information Processing
Systems, pp. 6716-6726, 2017a.
Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric Xing. Nonparametric varia-
tional auto-encoders for hierarchical representation learning. arXiv preprint arXiv:1703.07027,
2017b.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international
conference on, pp. 6645-6649. IEEE, 2013.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable
representations from sequential data. In Advances in neural information processing systems, pp.
1876-1887, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint
arXiv:1511.05121, 2015.
Liang Lu, Lingpeng Kong, Chris Dyer, Noah A Smith, and Steve Renals. Segmental recurrent neural
networks for end-to-end speech recognition. arXiv preprint arXiv:1603.00223, 2016.
Lars Maal0e, Casper Kaae S0nderby, S0ren Kaae S0nderby, and Ole Winther. Auxiliary deep gen-
erative models. International Conference on Machine Learning, 2016.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Yajie Miao, Mohammad Gowayyed, and Florian Metze. Eesen: End-to-end speech recognition using
deep rnn models and wfst-based decoding. In Automatic Speech Recognition and Understanding
(ASRU), 2015 IEEE Workshop on, pp. 167-174. IEEE, 2015.
Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In Interna-
tional Conference on Machine Learning, pp. 1727-1736, 2016.
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multi-
modal deep learning. In Proceedings of the 28th international conference on machine learning
(ICML-11), pp. 689-696, 2011.
D. B. Paul and J. M. Baker. The design for the Wall Street Journal-based CSR corpus. In HLT
Workshop on Speech and Natural Language, 1992.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. Semi-supervised
sequence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108, 2017.
10
Under review as a conference paper at ICLR 2019
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Iulian Vlad Serban, Alexander G Ororbia, Joelle Pineau, and Aaron Courville. Piecewise latent vari-
ables for neural variational text processing. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing,pp. 422-432,2017.
Kihyuk Sohn, Wenling Shang, and Honglak Lee. Improved multimodal deep learning with variation
of information. In Advances in Neural Information Processing Systems, pp. 2141-2149, 2014.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in neural information processing systems, pp. 3738-3746,
2016.
Nitish Srivastava and Ruslan R Salakhutdinov. Multimodal learning with deep boltzmann machines.
In Advances in neural information processing systems, pp. 2222-2230, 2012.
Hao Tang, Liang Lu, Lingpeng Kong, Kevin Gimpel, Karen Livescu, Chris Dyer, Noah A Smith,
and Steve Renals. End-to-end neural segmental models for speech recognition. IEEE Journal of
Selected Topics in Signal Processing, 11(8):1254-1264, 2017.
Qingming Tang, Weiran Wang, and Karen Livescu. Acoustic feature learning using cross-domain
articulatory measurements. arXiv preprint arXiv:1803.06805, 2018.
Erik F Tjong Kim Sang and Sabine Buchholz. Introduction to the conll-2000 shared task: Chunking.
In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on
Computational natural language learning-Volume 7, pp. 127-132. Association for Computational
Linguistics, 2000.
Erik F Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings of the seventh conference on Natural lan-
guage learning at HLT-NAACL 2003-Volume 4, pp. 142-147. Association for Computational Lin-
guistics, 2003.
Jakub M Tomczak and Max Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120, 2017.
W. Wang, X. Yan, H. Lee, and K. Livescu. Deep variational canonical correlation analysis. arXiv
preprint arXiv:1610.03454, 2016.
Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view representation
learning. In International Conference on Machine Learning, pp. 1083-1092, 2015.
John Westbury, Paul Milenkovic, Gary Weismer, and Raymond Kent. X-ray microbeam speech
production database. The Journal of the Acoustical Society of America, 88(S1):S56-S56, 1990.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from generative
models. arXiv preprint arXiv:1702.08396, 2017.
V. Zue, S. Seneff, and J. Glass. Speech database development at MIT: TIMIT and beyond. Speech
Comm., 9(4):351-356, 1990.
11
Under review as a conference paper at ICLR 2019
S upplementary Material: Variational recur-
RENT MODELS FOR REPRESENTATION LEARNING
Anonymous authors
Paper under double-blind review
1 Derivation for stochastic generation
As mentioned in the main text, we are not calculating the full generative model shown in Figure
1(d) of the main paper. Instead, we use the process referred to as stochastic generation. Here, we
illustrate the connection between using stochastic generation for training and directly evaluating the
multi-modal generative model corresponding to Figure 1(d) in the main paper.
Assume we have N sequences in our training set, and assume each sequence has been visited (via
stochastic generation) K times. We denote the tth latent variable of sequence x1:T as zt. Suppose zt
has tried to reconstruct xk for Mt,k times. So on average, by using stochastic generation in training,
the log-likelihood computed for the sequence x1:T given the posterior is as follows:
巩i{ Eqφ(zt∣ht)
log Pθ(χk |zt
(1)
with E[MKk] = αδtTkk given the definition of stochastic generation.
If we directly evaluate the generation distribution
Pθ (xi:T ∣zi:T) = ∏T=ιPθ (xt ∣Z1:T)
=∏T=1∏T=1Pθ (xk ∣zt)αδT
(2)
then we can derive the ELBO as follows:
logPθ(x1：T)	≥	Eqφ(zi：t |hi：T)
Eqφ (z1:T |h1:T)
Eqφ (z1:T |h1:T)
log pθ (x1:T |z1:T) - DKL qφ (z1:T |h1:T)||p(z1:T)
ςT=I卜τ=ι{αδkTt) logPθ(xt|zk)}}]- DκL(qφ (zi：T |hi：T )∣∣p(zi:T))
∑T=1∣∑T=ι{αδ,Tk) logPθ(Xk∣zt)}}] - DκL(qφ(zi：T|hi：T)||p(zi：T3)
ςT=i {Eqφ(zt∣ht)
∑τ=ι{α(gT logPθ(Xk∣zt)}
-DKL((Iφ(ztlht)llp(zt))}
(4)
The log-likelihood of X1:T given the posterior is then as follows:
ςt=1 {Eqφ(zt∣ht)
∑T=ι{αδ竽) log Pθ (Xk ∣zt)}
(5)
According to Equation 1, in expectation, the conditional log-likelihood computed using stochas-
tic generation is also equal to Equation 5. This equality shows the connection between stochastic
generation and the full generation process.
1
Under review as a conference paper at ICLR 2019
2 Proof and illustration of prior updating
Given the prior p∏ (zi：T) parameterized by ∏, we have the ELBO
logpθ,π (x1:T)
log
Pθ (xi:T |zi:T )p∏(zi:T )qφ(zi:T |xi:T )
qφ(zi:T |xi:T)pθ (zi:T |xi:T)
Eqφ (z1:T |x1:T ) log
Pθ (xi:T |zi:T)p∏ (zi:T)qφ (zi:T |xi:T)
qφ(zi:T |xi:T)pθ (zi:T |xi:T)
≥	Eqφ (z1:T |x1:T) logpθ (xi:T |zi:T) - DKL(qφ(zi:T |xi:T )||Pn (zi:T))	⑹
As described in the paper, when using prior updating, We simply set pii)(zi:T) ：= q@(i)(zi:T|xi:T).
Here, p^(zi:t) is the prior after i updates, where p∏0)(zi:T) ≡ NT(0,I); φ(i) and θ(i) are the
solutions maximizing the lower bound with respect to p(i-i) (zi:T), so we have
≤
≤
mφax {Eqφ(zLτ |xi：t )[log(Pθ (Xi:T |zi:T ))] - βDKL(qφ(zi:T |xi:T ^成*上T )) }
Eqφ(i+1) (zi：T |xi：T) [log(Pθ(i+ι)(xi:T |zi:T))] - eDKL(q@(i+i) (zi:T |xi:T*Pni)(Zi:T)
Eqφ(i+1) (zi：T|xi：T) [log(PΘ(i+1)(Xi:T|zi:T))]-卜Dkl (qφ(i+D (zi:T|xi:T)“Pni+i)(zi:T) }
mφax {Eqφ (ZLT |xi：T )[log(Pθ (Xi:T |zi:T ))] - βDKL(qφ(zi:T |xi:T )||Pni+i)(zi:T))}
0
(7)
It is not difficult to show that the equality holds when and only when we have qφ(i+1) (zi:T |Xi:T) =
P(ni)(zi:T) ≡ qφ(i) (zi:T |Xi:T); that is, observing the evidence does not change our understanding
of the latent space. An illustration of prior updating from a regularization perspective is given in
Figure 1.
3 ELBO of recurrent multi-view (hierarchical) models
We now derive the ELBO for RecVCCAP.
log(Pθ (Xi:T, ui:T))
E	[ ι	(Pθ(xi:T, ui:T|zi:T, yi:T, oi:T)pθ(zi:T, yi:T, oi:T)
Eqφ (z1:T ,yi:T ,o1:T |gl：T ,h1:T ,n1:T) [log(	~TZ ^	7	∣∑	^	∖
Pθ(zi:T, yi:T, oi:T|xi:T, ui:T)
(8)
Assume the prior distribution is given and factorizes as
Pθ (zi:T, yi:T, oi:T) = P(zi:T, yi:T, oi:T) = P(zi:T) × P(yi:T) × P(oi:T)
(9)
2
Under review as a conference paper at ICLR 2019
Figure 1: Illustrative explanation of our prior updating strategy from a regularization perspective.
We then have
log（pθ （x1:T , u1:T））
Eqφ （z1:T ,y1:T ,o1:T
|g1:T ,h1:T ,n1:T
1：T, ui：t|zi：T, yi：T, oi：T）p（zi:T, yi：T, oi：T）qφ（zi：T, yi：T, oi：T|gi：T, hi：T, nι
Pθ（zi：T, yi：T, oi：T |xi：T, ui：T）qφ（zi：T, yi：T, oi：T|gi：T, hi：T, ni：T）
+
+
Eqφ （z1:T ,y1:T ,o1:T
Eqφ（zi:T ,yi：T ,oi： T
Eqφ （z1:T ,y1:T,o1:T
|g1:T,h1:T,n1:T） log pθ （xi：T, ui：T |zi：T, yi：T, oi：T）
|g1：T ,h1：T ,n1：T ） log
|g1：T,h1：T,n1：T） log
q@（zi：T, yi：T, oi：T|gi：T, hi：T, ni：T）
pθ （zi：T, yi：T, oi：T |xi：T, ui：T）
_________p（zi：T,yi：T, oi：T）__________
qφ（zi∙.T,yi：T, oi：Tlgi：T, hi：T,ni：T）
）］
）］
Eqφ（zi:T ,yi：T ,oi：T
gi：T,hi：T,ni：T） [log pPθ（xi：T, ui：T|zi：T, yi：T, oi：T））]
+	DκL（qφ（zi∙.T, yi：T, oi：Tlgi：T, hi：T, ni：T）∣∣Pθ（zi：T, yi：T, oi：Tlxi：T, ui：T））
-	DKL（qφ（zi∙.T, yi：T, oi：Tlgi：T, hi：T, ni：T）IIp（zi：t, yi：T, oi：T））
≥	Eqφ（zi:T ,yi:T ,oi:T |gi：T ,hi:T ,ni:T） [log p0（ （X i：T, ui：T lzi：T, yi：T, oi：T ））]
-	DKL（q«（zi：T, yi：T, oi：Tlgi：T, hi：T, ni：T）IIp（zi：t, yi：T, oi：T））
Eqφ （zi：T,yi：T |gi：T,hi：T） log pθ （xi：T |zi：T, yi：T） + Eqφ （oi：T,yi：T |ni：T,hi：T） log pθ （ui：T |oi：T, yi：T）

DKL（qψ（oitTlni：T）IIP（oi：t）） - DKLlqΦ（zi：Tlgi：T）llP（zi：T）） - DKL（qΦ（yi：Tlhi：T）llP（yi：T））
ς=i {Eqφ（yt∖ht）qφ（zt∖gt）
ςt=1 ^δk^r log Pθ （xk lzt）}+ Eqcl4>^t）cl4>{Ot\r^^=i, αδ,tkTT^r log Pθ（uk lot）}
]}

DKL qφ （oi：T |ni：T）||p（oi：T） - DKL qφ （zi：T |gi：T）||p（zi：T）	- DKL qφ （yi：T |hi：T）||p（yi：T）
3
Under review as a conference paper at ICLR 2019
Similarly, we can derive the ELBO for the hierarchical version of RecVCCAP as follows:
logPθ(XLT,u1:T) ≥ Eqφ(zi:T,yi:T|gi：T ,hi:T) [logθ (XLT |z1：T)] + Eqφ(oi:T,yi：T |ni：T,hi：T) [logθ (u1:T|o1:T)]
	
DKL(qφ(oi:T|yi：T)∣∣p(oi:T)) - DKL(q@(z\：T|yi：T)∣∣p(zi:T)) - DKL("(yi：T|hi：T)∣∣p(yi:T))
ΣtT=1
Eqφ(ytlht)qφ (ztWt,gD
ςt=1 {αδ,T logPθ(XkIzt)} + Eqe(yt|ht)qe(Ot|yt,ntKT=i {θδ',T logPθ(Uk |
ot)}]}
	
DKL qφ (o1:T |y1:T)||p(o1:T) - DKL qφ (z1:T |y1:T)||p(z1:T) - DKL qφ (y1:T |h1:T)||p(y1:T)	(11)
4	Decoder details
For speech tasks we use a Gaussian distribution with diagonal covariance as the decoder distribution,
that is
logPθ(x∣zt) ≡N(μ∣zt,σ2I)	(12)
where σ is tuned.
For NLP tasks we use a softmax decoder as described in Equation (6) and (7) of Miao et al. (2016).
In our tasks (NER and chunking), the input is a character RNN output concatenated with a pre-
trained word embedding, and the generation network tries to reconstruct the word corresponding to
the given embedding by matching the reconstructed embedding with the (pre-trained) embedding of
the target word.
5	Data sets
In this section, we give more details for the five data sets we used in the paper: TIMIT, XRMB,
WSJ, CoNLL2003 and CoNLL2000.
For TIMIT, there are 3696, 400 and 192 train/dev/test utterances respectively. The per frame input
are speaker-normalized 40-dimensional log filter bank features (without energy) and their first and
second derivatives. 39 phone labels are used.
For XRMB, there are in total 47 speakers and roughly 2300 acoustic utterances. 35 speakers (roughly
1700 utterances), together with their paired articulatory measurements, are used to train multi-view
models. The remaining 12 speakers are used to test the quality of the learned representations by
testing on phonetic recognition in a K -fold experimental setup. We use 6-fold validation here,
where the 12 speakers (separate from the 35 speakers used for multi-view representation learning)
are separated into 6 groups, and each time we pick one group as development set, one group as test
set, and the remaining 4 groups (8 speakers) as phonetic recognizer training set. For XRMB, the
input features are 13-dimensional MFCCs, also with first and second derivatives, making the feature
vectors 39-dimensional.
For WSJ, we start with the standard split, with 37416 train, 503 dev, and 330 test utterances. How-
ever, in this paper we don’t use the full training set. For WSJ we again use log filter bank features,
but with energy, and also their first and second derivatives. So the per-frame input feature vectors are
123-dimensional. For the purpose of semi-supervised learning, we split the training into 24 parts,
and use 3 of them (roughly 5K utterances) as labeled data and treat the other 21 splits as unlabeled
data.
For the CoNLL2003 and CoNLL2000 tasks, we refer the reader to Peters et al. (2017), specifically
the second and fourth paragraphs of the experimental section.
6	Hyperparameter tuning and training
For all speech models, the decoder distribution pθ(x|z) is a spherical Gaussian, with covariance
matrix σ2I. σ is a hyperparameter selected among {0.1,1.0,10.0}. The KL term weight β is
selected among {0.1,1.0,10.0}, a is selected among {0.3,0.5,0.7,0.9,0.99}, and √δ is selected
among {0, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20}. The tuning strategy applied to all of the tasks in this work.
4
Under review as a conference paper at ICLR 2019
For learning, we typically use Adam Kingma & Ba (2015), with initial learning rate chosen from
{0.001, 0.0005, 0.0001}. For TIMIT, we also use stochastic gradient descent, following the opti-
mization strategy described in Tang et al. (2017).
References
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In Interna-
tional Conference on Machine Learning, pp. 1727-1736,2016.
Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. Semi-supervised
sequence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108, 2017.
Hao Tang, Liang Lu, Lingpeng Kong, Kevin Gimpel, Karen Livescu, Chris Dyer, Noah A Smith,
and Steve Renals. End-to-end neural segmental models for speech recognition. IEEE Journal of
Selected Topics in Signal Processing, 11(8):1254-1264, 2017.
5