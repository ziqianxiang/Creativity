Under review as a conference paper at ICLR 2019
Hierarchically Clustered
Representation
Learning
Anonymous authors
Paper under double-blind review
Ab stract
The joint optimization of representation learning and clustering in the embedding
space has experienced a breakthrough in recent years. In spite of the advance, clus-
tering with representation learning has been limited to flat-level categories, which
oftentimes involves cohesive clustering with a focus on instance relations. To over-
come the limitations of flat clustering, we introduce hierarchically clustered repre-
sentation learning (HCRL), which simultaneously optimizes representation learn-
ing and hierarchical clustering in the embedding space. Specifically, we place a
nonparametric Bayesian prior on embeddings to handle dynamic mixture hier-
archies under the variational autoencoder framework, and to adopt the generative
process of a hierarchical-versioned Gaussian mixture model. Compared with afew
prior works focusing on unifying representation learning and hierarchical cluster-
ing, HCRL is the first model to consider a generation of deep embeddings from
every component of the hierarchy, not just leaf components. This generation pro-
cess enables more meaningful separations and mergers of clusters via branches in
a hierarchy. In addition to obtaining hierarchically clustered embeddings, we can
reconstruct data by the various abstraction levels, infer the intrinsic hierarchical
structure, and learn the level-proportion features. We conducted evaluations with
image and text domains, and our quantitative analyses showed competent likeli-
hoods and the best accuracies compared with the baselines.
1	Introduction
Clustering is one of the most traditional and frequently used machine learning tasks. Clustering
models are designed to represent intrinsic data structures, such as latent Dirichlet allocation (Blei
et al., 2003). The recent development of representation learning has contributed to generalizing
model feature engineering, which also enhances data representation (Bengio et al., 2013). Therefore,
representation learning has been merged into the clustering models, e.g., variational deep embedding
(VaDE) (Jiang et al., 2017). Besides merging representation learning and clustering, another critical
line of research is structuring the clustering result, e.g., hierarchical clustering. This paper introduces
a unified model enabling nonparametric Bayesian hierarchical clustering with neural-network-based
representation learning.
Autoencoder (Rumelhart et al., 1985) is a typical neural network for unsupervised representa-
tion learning and achieves a non-linear mapping from a high-dimensional input space to a low-
dimensional embedding space by minimizing reconstruction errors. To turn the low-dimensional
embeddings into random variables, a variational autoencoder (VAE) (Kingma & Welling, 2014)
places a Gaussian prior on the embeddings. The autoencoder, whether it is probabilistic or not, has
a limitation in reflecting the intrinsic hierarchical structure of data. For instance, VAE assuming a
single Gaussian prior needs to be expanded to suggest an elaborate clustering structure.
Due to the limitations of modeling the cluster structure with autoencoders, prior works combine the
autoencoder and the clustering algorithm. While some early cases pipeline just two models, e.g.,
Huang et al. (2014), a typical merging approach is to model an additional loss, such as a clustering
loss, in the autoencoders (Xie et al., 2016; Guo et al., 2017; Yang et al., 2017; Nalisnick et al.,
2016; Chu & Cai, 2017; Jiang et al., 2017). These suggestions exhibit gains from unifying the
encoding and the clustering, yet they remain at the parametric and flat-structured clustering. A more
recent development releases the previous constraints by using the nonparametric Bayesian approach.
1
Under review as a conference paper at ICLR 2019
(a) Hierarchically clustered embeddings
(b) Reconstructed images
Figure 1: Example of hierarchically clustered embeddings on MNIST with three levels of hierarchy,
the reconstructed digits from the hierarchical Gaussian mixture components, and the extracted level
proportion features. We marked the mean of a Gaussian mixture component with the colored square,
and the digit written inside the square refers to the unique index of the mixture component.
	Level 1	Level 2	Level 3
B	0.4300	0.3642	0.2058
B	0.4500	0.2942	0.2558
B	0.4303	0.4360	0.1337
Ξ	0.4271	0.4600	0.1130
Ξ	0.4234	0.4550	0.1216
S	0.2120	0.0197	0.7683
B	0.2745	0.0258	0.6998
H	0.2387	0.0670	0.6944
(c) Level propor-
tion features of
real images
For example, the infinite mixture of VAEs (IMVAE) (Abbasnejad et al., 2017) explores the infinite
space for VAE mixtures by looking for an adequate embedding space through sampling, such as the
Chinese restaurant process (CRP). Whereas IMVAE remains at the flat-structured clustering, VAE-
nested CRP (VAE-nCRP) (Goyal et al., 2017) captures a more complex structure, i.e., a hierarchical
structure of the data, by adopting the nested Chinese restaurant process (nCRP) prior (Griffiths et al.,
2004) into the cluster assignment of the Gaussian mixture model.
This paper proposes hierarchically clustered representation learning (HCRL) that is a joint model
of 1) nonparametric Bayesian hierarchical clustering, and 2) representation learning with neural
networks. HCRL extends a previous work on merging flat clustering and representation learning,
i.e., VaDE, by incorporating inter-cluster relation modelings. Unlike a previous work of VAE-nCRP,
HCRL learns the full spectrum of hierarchical clusterings, such as the level assignment and the level
proportion of generating a component hierarchy. These level assignments and proportions were not
modeled in VAE-nCRP, so each data instance cannot be analyzed from the perspective of general-
ization and specialization in a hierarchy. On the contrary, by adding level assignment and proportion
modeling, a data instance can be generated from an internal component of the hierarchy, which is
limited to the leaf component in VAE-nCRP. Hierarchical mixture density estimation (Vasconcelos
& Lippman, 1999), where all internal and leaf components are directly modeled to generate data, is a
flexible framework for hierarchical mixture modeling, such as hierarchical topic modeling (Mimno
et al., 2007; Griffiths et al., 2004), with regard to the learning of the internal components.
Specifically, HCRL jointly optimizes soft-divisive hierarchical clustering in an embedding space
from VAE via two mechanisms. First, HCRL includes a hierarchical-versioned Gaussian mixture
model (HGMM) with a mixture of hierarchically organized Gaussian distributions. Then, HCRL
sets the prior of embeddings by adopting the generative processes of HGMM. Second, to handle a
dynamic hierarchy structure dealing with the clusters of unequal sizes, we explore the infinite hierar-
chy space by exploiting an nCRP prior. These mechanisms are fused as a unified objective function;
this is done rather than concatenating the two distinct models of clustering and autoencoding. The
quantitative evaluations focus on density estimation quality and hierarchical clustering accuracy,
which shows that HCRL has competent likelihoods and the best accuracies compared with the base-
lines. When we observe our results qualitatively, we visualize 1) the hierarchical clusterings, 2) the
embeddings under the hierarchy modeling, and 3) the reconstructed images from each Gaussian
mixture component, as shown in Figure 1. These experiments were conducted by crossing the data
domains of texts and images, so our benchmark datasets include MNIST, CIEAR-100, RcVLv2,
and 20Newsgroups.
2	Preliminaries
2.1	Variational Deep Embedding
Figure 2 presents a graphical representation and a neural architecture of VaDE (Jiang et al., 2017).
The model parameters of κ, μ1K, and b2：K, which are a proportion, means, and covariances of
2
Under review as a conference paper at ICLR 2019
Figure 2: Graphical representation of VaDE (Jiang et al., 2017) (left), VAE-nCRP (Goyal et al.,
2017) (center), and neural architecture of both models (right). In the graphical representation, the
white/shaded circles represent latent/observed variables. The black dots indicate hyper or variational
parameters. The solid lines represent a generative model, and dashed lines represent a variational
approximation. A rectangle box means a repetition for the number of times denoted by the bottom
right of the box.
mixture components, respectively, are declared outside of the neural network1. VaDE trains model
parameters to maximize the lower bound of marginal log likelihoods via the mean-field variational
inference (Jordan et al., 1999). VaDE uses the Gaussian mixture model (GMM) as the prior, whereas
VAE assumes a single standard Gaussian distribution on embeddings. Following the generative pro-
cess of GMM, VaDE assumes that 1) the embedding draws a cluster assignment, and 2) the embed-
ding is generated from the selected Gaussian mixture component.
VaDE uses an amortized inference as VAE, with a generative and inference networks; L(x) in Equa-
tion 1 denotes the evidence lower bound (ELBO), which is the lower bound on the log likelihood. It
should be noted that VaDE merges the ELBO of VAE with the likelihood of GMM.
lθED(x) ≥ L(X) =	E	[log	P(C,z,X) 1 = E boa YK K N ⑵μc,σ2IJ)	+lo£D(x|z)]	(1)
logP(X) ≥ L(X) =	Eq	[log	q(c,z∣x) J = Eq [log口	p(c∖z)N(z∣μ, σ2Ij)	+logP(XIz)]	(1)
2.2	Variational Autoencoder nested Chinese Restaurant Process
VAE-nCRP uses the nonparametric Bayesian prior for learning tree-based hierarchies, the nCRP
(Griffiths et al., 2004), so the representation could be hierarchically organized. The nCRP prior
defines the distributions over children components for each parent component, recursively in a top-
down way. The variational inference of the nCRP can be formalized by the nested stick-breaking
construction (Wang & Blei, 2009), which is also kept in the VAE setting. The distribution over paths
on the hierarchy is defined as being proportional to the product of weights corresponding to the nodes
lying in each path. The weight, πi , for the i-th node follows the Griffiths-Engen-McCloskey (GEM)
distribution (Pitman et al., 2002), where ∏ is constructed as ∏ = Vi Qj=I(1 — Vj), Vi 〜Beta(1, Y)
by a stick-breaking process. Since the nCRP provides the ELBO with the nested stick-breaking
process, VAE-nCRP has a unified ELBO of VAE and the nCRP in Equation 2.
rg∖ F L σ P(V)工 lnσ p(αPar(P) ∣α*)p9p∣αpar(p),σN) P(ZIv) P(z ∖αp, C,σD) ,
L(X) = EqKg E + log----------------q(αp, αpar(p) ∣x)-----M	q(z∣X)	+ logP(XIZ)
V{z} X}
(3.1)	(3.2)	(2)
Given the ELBO of VAE-nCRP, we recognized a number of potential improvements. First, term
(3.1) is for modeling the hierarchical relationship among clusters, i.e., each child is generated from
its parent. VAE-nCRP trade-off is the direct dependency modeling among clusters against the mean-
field variational approach. This modeling may reveal that the higher clusters in the hierarchy are
more difficult to train. Second, in term (3.2), leaf mixture components generate embeddings, which
implies that only leaf clusters have direct summarization ability for sub-populations. Additionally, in
term (3.2), variance parameter σD2 is modeled as the hyperparameter shared by all clusters. In other
words, only with J -dimensional parameters, α, for the leaf mixture components, the local density
modeling without variance parameters has a critical disadvantage.
For all of these weaknesses, we were able to compensate with the level proportion modeling and
HGMM prior. The level assignment generated from the level proportion allows a data instance to
1 Appendix D enumerates the symbols in this paper.
3
Under review as a conference paper at ICLR 2019
P(Zn)
=∑R P(G)Pan)"(%。油)
=∑ζlι nCRP(3)%W("s。汕)
* Example
ɪ - Path, ζ2 = [l, 1,2]
Q - Level proportion,
η2 = [0.3,0.5,0.2]
Figure 3: A simple depiction (left) of the key notations, where each numbered circle refers to the
corresponding Gaussian mixture component. The graphical representation (center) and the neural
architecture (right) of our proposed model, HCRL. The neural architecture of HCRL consists of two
probabilistic encoder networks, gφη and gφz , and one probabilistic decoder network, fθ.
select among all mixture components. We do not need direct dependency modeling between the
parents and their children because all internal mixture components also generate embeddings.
3 Methodology
3.1	Generative Process
The generative process of HCRL resembles the generative process of hierarchical clusterings, such
as the hierarchical latent Dirichlet allocation (Griffiths et al., 2004). In detail, the generative process
departs from selecting a path ζ, from the nCRP prior (phase 1). Then, we sample a level proportion
(phase 2) and a level, l (phase 3), from the sampled level proportion to find the mixture component
in the path, and this component of ζl provides the Gaussian distribution for the latent representation
(phase 4). Finally, the latent representation is exploited to generate an observed datapoint (phase
5). The below formulas are the generative process with its density functions. In addition, Figure 3
illustrates a graphical representation corresponding to the described generative process. The gener-
ative process also presents our formalization of corresponding prior distributions, denoted as p(∙),
and variational distributions, denoted as q(∙), by generation phases. The variational distributions are
used in our inference methods called mean-field variational inference (MFVI) (Jordan et al., 1999)
as detailed in Section 3.3.
1.	Choose a path Z 〜nCRP(Z∣γ)
• p(ζ) = QlL=1 π1,ζ2,...,ζl whereπ1,ζ2,...,ζl = Qll0=1{v1,ζ2,...,ζl0 (Qjζl=0 -1 1(1 - v1,ζ2,...,j))},
q (Z Ix)区 SZ , Σ2 ζ∈ child © Sζ
2.	Choose a level proportion η 〜DiriChlet(η∣α)
• p(η) = Dir(η∣α), qφη (η∣x) = DiriChlet(η∣α) ≈ LogistiCNormal(n|"n,万；IL)
C	T	-	-e”.	-77
where [μη ；log ση] = gφn (X), α =亳(I - L + ⅛- P e nl0)
3.	Choose a level l 〜Multinomial(l∣η)
• p(l) = Multinomial(η), q(l∣x) = Multinomial(l∣ω)
whereωι X exp ]ψ(α∕ - ψ(αο) + PZ SZ (PJ=I - 1 log(2πσ2l,j)-
4.	Choose a latent representation Z 〜 N (z∖μzι, σ2ι ij )
• P(Z) = pζ,ιP(Z∖γ) ∙ηι ∙N(Z∖μζι,σ2ι1J),
qφz (z∖x) = N(z∖μz, σZIJ) where μ ;log σZ] = gφ, (x)
e2j
2σ2ι,j
(ezj -μζl,j )
5.	Choose an observed datapoint x - N (x∖μχ, σXID) where [μχ; log σX] = fθ(z)2 *
—
3.2	Neural Architecture
The neural architecture of HCRL consists of two probabilistic encoders on Z and η, and one proba-
bilistic decoder on Z as shown in the right part of Figure 3. This unbalanced architecture originates
2We introduce the sample distribution for the real-valued data instances, and Appendix F provides the binary
case as well, which we use for MNIST.
4
Under review as a conference paper at ICLR 2019
from our modeling assumption of p(x|z), notp(x|z, η). The reconstruction design of x depending
on the two stochastic variables of z and η may lead to a large variance of the reconstruction on x.
Additionally, we cannot guarantee that both z and η contribute to the the reconstruction on x (Chen
et al., 2016). Although the decoding structure of η is not included explicitly in the neural network
architecture of HCRL, We provide the formalization of p(η∣z) in Table 1 according to our gener-
ative assumptions. We call this reconstruction process, which is inherently a generative process of
the traditional probabilistic graphical model (PGM), PGM reconstruction (see the decoding neural
netWork part of Figure 3).
Table 1: Encoding and decoding structure on z and η in HCRL. (s) indicates the s-th sample.
	Encoding	Decoding
z	^∏qφzIIjiXz!)=gφzil(s)[X∏	X 〜Pθ (x∖z), X(S) = fθ (Z(S))
η	n 〜qΦηS∖x), n⑻=gφη(C(S), X)	P(XIn) (X Rv,z pζ,ιP(XIz)p(z∖ζ,l)p(l∖n)p(ζ∖V)P(V)
3.3	Mean-Field Variational Inference
The formal specification can be a factorized probabilistic model as Equation 3, Where Φ =
{v, ζ, η, l, z} denotes the set of latent variables, and MT denotes the set of all nodes in tree T .
N
P(Φ, X)= Π p(vj ∣γ) ∏ p(vi∣γ) Y[p(Zn∖v')p(ηn∖α)p(ln∖ηn)p(Zn∖Zn ,ln)pθ (Xn∣Zn) (3)
j∈Mτ	i∈Mτ	n=1
The proportion and assignment on the mixture components for the n-th data instance are modeled by
ζn as a path assignment; ηn as a level proportion; and ln as a level assignment. v is a Beta draW used
in the stick-breaking construction. The latent variables are inferred through MFVI, and therefore We
assume the variational distributions are as Equation 4:
N
q(Φ∖x) =	p(vj ∖γ)	q(vi∖ai,bi)	q(ζn∖xn)qφη (ηn∖xn)q(ln∖ωn, xn)qφz(zn∖xn) (4)
j∈Mτ	i∈Mτ	n=1
Where qφη (ηn∖xn) and qφz (zn∖xn) should be noted because these tWo variational distributions
follow the amortized inference of VAE. q(Z∖x) 8 SZ，£《三电同© SZ is the variational distribution
over path Z, where Child(Z) means the set of all full paths that are not in T but include Z as a sub
path. Because we specified both generative and variational distributions, we define the ELBO of
HCRL, L = Eq [log p(φ,χ", in Equation 5. Appendix F enumerates the full derivation in detail.
We report that the Laplace approximation with the logistic normal distribution is applied to model
the prior, α, of the level proportion, η. We choose a conjugate prior of a multinomial, so p(ηn∖α)
follows the Dirichlet distribution. To configure the inference network on the Dirichlet prior, the
Laplace approximation is used (MacKay, 1998; Srivastava & Sutton, 2017; Hennig et al., 2012).
p( p w Γ1 P(V) , 1 p(η) , 1 πp(ζ∖v)P(IIn)P(Z∖μZι,σ2J ɪ, ( l J _
L(X) = Eq 悭港后 + log 诉)+ logU 加而 於⑻ + logP(XIz)「5)
3.4	Training Algorithm of Clustering Hierarchy
This model is formalized according to the stick-breaking process scheme. Unlike the CRP, the stick-
breaking process does not represent the direct sampling of the mixture component at the data in-
stance level. Therefore, it is necessary to devise a heuristic algorithm for operations, such as GROW,
PRUNE, and MERGE, to refine the hierarchy structure. Appendix C provides details about each
operation, together with the overall training algorithm of HCRL. In the below description, an inner
path and a full path refer to the path ending with an internal node and a leaf node, respectively.
• GROW expands the hierarchy by creating a new branch under the heavily weighted internal node.
Compared with the work of Wang & Blei (2009), we modified GROW to first sample a path, Z ,
一	.—.	,. 一*、 一 一	一 一 .一 一	一 一 一. .	一
proportional to	n q(Zn = Z ), and then to grow the path if the sampled path is an inner path.
5
Under review as a conference paper at ICLR 2019
• PRUNE cuts a randomly sampled minor full path, Z*, satisfying IP 虱'：=7) < δ, where δ is
Tn,Z Q(ζn = ζ)
the pre-defined threshold. If the removed leaf node of the full path is the last child of the parent
node, we also recursively remove the parent node.
MERGE combines two full paths, Z(i) and Z(j), with
similar posterior probabilities, measured
by J(Z(i), ζCj)) = qiqj/∖qi∖∖qj|, where qi = [q(Zι = ζ(i)),…，q(ZN = ζ(i))].
4 Experiments
4.1	Datasets and Baselines
Datasets: We used various hierarchically organized benchmark datasets as well as MNIST.
•	MNIST (LeCun et al., 1998): 28x28x1 handwritten image data, with 60,000 train images and
10,000 test images. We reshaped the data to 784-d in one dimension.
•	CIFAR-100 (Krizhevsky & Hinton, 2009): 32x32x3 colored images with 20 coarse and 100
fine classes. We used 3,072-d flattened data with 50,000 training and 10,000 testing.
•	RCVLv2 (Lewis et al., 2004): The preprocessed text of the Reuters Corpus Volume. We prepro-
cessed the text by selecting the top 2,000 tf-idf words. We used the hierarchical labels up to the
4-level, and the multi-labeled documents were removed. The final preprocessed corpus consists
of 11,370 training and 10,000 testing documents randomly sampled from the original test corpus.
•	20Newsgroups (Lang, 1995): The benchmark text data extracted from 20 newsgroups, consisting
11,314 training and 7,532 testing documents. We also labeled by 4-level following the annotated
hierarchical structure. We preprocessed the data through the same process as that OfRCVLv2.
Baselines: We completed our evaluation in two aspects: 1) optimizing the density estimation, and
2) clustering the hierarchical categories. First, we evaluated HCRL from the density estimation per-
spective by comparing it with diverse flat clustered representation learning models, and VAE-nCRP.
Second, we tested HCRL from the accuracy perspective by comparing it with multiple divisive
hierarchical clusterings. The below is the list of baselines. We also added the two-stage pipeline
approaches, where we trained features from VaDE first and then applied the hierarchical clusterings.
We reused the open source codes3 provided by the authors for several baselines, such as IDEC,
DCN, VAE-nCRP, and SSC-OMP.
1.	Variational Autoencoder (VAE) (Kingma & Welling, 2014)
2.	Variational Deep Embedding (VaDE) (Jiang et al., 2017)
3.	Improved Deep Embedded Clustering (IDEC) (Guo et al., 2017): improves DEC (Xie et al.,
2016) by attatching decoder structure. We use the code by the authors.
4.	Deep Clustering Network (DCN) (Yang et al., 2017): optimizes the K-means-related cost de-
fined on the embedding space. We used the open source code provided by the authors.
5.	Infinite Mixture of Variational Autoencoders (IMVAE) (Abbasnejad et al., 2017): searches
for the infinite embedding space by using a Bayesian nonparametric prior.
6.	Variational Autoencoder - nested Chinese Restaurant Process (VAE-nCRP) (Goyal et al.,
2017): We used the open source code provided by the authors.
7.	Hierarchical K-means (HKM) (Nister & Stewenius, 2006): performs K-means (Lloyd, 1982)
recursive in a top-down way.
8.	Mixture of Hierarchical Gaussians (MOHG) (Vasconcelos & Lippman, 1999): infers the
level-specific mixture of Gaussians.
9.	Recursive Gaussian Mixture Model (RGMM): runs GMM recursively in a top-down manner.
10.	Recursive Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit (RSS-
COMP): performs SSC-OMP (You et al., 2016) recursively for hierarchical clustering. SSC-
OMP is a well-known methods for image clustering, and we used the open source code.
4.2	Quantitative Analysis
We used two measures to evaluate the learned representations in terms of the density estimations:
1) negative log likelihood (NLL), and 2) reconstruction errors (REs). Autoencoder models, such as
3https://github.com/XifengGUo/IDEC (IDEC); https://github.com/boyangumn/DCN (DCN);
https://github.com/prasoongoyal/bnp-vae (VAE-nCRP); http://vision.jhu.edu/code/ (SSC-OMP)
6
Under review as a conference paper at ICLR 2019
IDEC and DCN, were tested only for the REs. The NLL is estimated with 100 samples. Table 2 indi-
cates that HCRL is best in the NLL and is competent in the REs which means that the hierarchically
clustered embeddings preserve the intrinsic raw data structure.
Table 2: Test set performance of the negative log likelihood (NLL) and the reconstruction errors
(REs). RePlicated ten times, and the best in bold. P * < 0.05 (Student,s t-test). Model- L# means
that the model trained with the #-depth hierarchy.
MNIST	CIFAR-100	RCVLv2	20Newsgroups
Model	NLL	REs	NLL	REs	NLL	REs	NLL	REs
VAE	230.71	10.46	1960.06	-^57.54	2559.46	1434.59	2735.80	1788.22
VaDE	217.20	10.35	1921.85	53.60	2558.32	1426.38	2733.46	1782.86
IDEC	N/A	12.75	N/A	64.09	N/A	1376.26	N/A	1660.61*
DCN	N/A	11.30	N/A	44.26	N/A	1361.98	N/A	1691.17
IMVAE	296.57	10.69	1992.83	40.45*	2566.01	1387.02	2722.81	1718.08
VAE-nCRP-L3	718.78	32.67	2969.62	198.66	2642.88	1538.42	2712.28	1680.56
VAE-nCRP-L4	721.00	32.53	2950.73	198.97	2646.48	1542.81	2713.58	1680.71
HCRL-L3	203.24*	8.70*	1843.40*	―^50.44	2554.50*	1395.05	2726.75	1828.71
HCRL-L4	203.91*	8.16*	1849.13*	50.47	2535.43*	1353.34	2702.88	1711.30
VaDE generally Performed better than VAE did, whereas other flat clustered rePresentation learning
models tended to be slightly different for each dataset. HCRL showed overall comPetent Perfor-
mance and better results with a deePer hierarchy of level four than of level three, which imPlies that
caPturing the deePer hierarchical structure is likely to be useful for the density estimation.
Additionally, we evaluated hierarchical clustering accuracies by following Xie et al. (2016), excePt
for MNIST that is flat structured. Table 3 Points out that HCRL has significantly better micro-
averaged F-scores comPared with every baseline. HCRL is able to reProduce the ground truth hier-
archical structure of the data, and this trend is consistent when HCRL comPared with the PiPelined
model, such as VaDE with a clustering model. The result of the comParisons with the clustering
models, such as HKM, MOHG, RGMM, and RSSCOMP, is interesting because it exPerimentally
Proves that the joint oPtimization of hierarchical clustering in the embedding sPace imProves hier-
archical clustering accuracies. HCRL also Presented better hierarchical accuracies than VAE-nCRP.
We conjecture the reasons for the modeling asPect of VAE-nCRP: 1) the simPlified Prior modeling
on the variance of the mixture comPonent as just constants, and 2) the non-flexible learning of the
internal comPonents.
Table 3: Hierarchical clustering accuracies with F-scores, on CIFAR-100 with a dePth of three,
RCV1_v2 with a depth of four, and 20NeWSgroUPS with a depth of four. Replicated ten times, and a
confidence interval with 95%. Best in bold.
Model	CIFAR-100	RCVLv2	20Newsgroups
HKM	0.1620±o.o077	0.2564±0.0679	0.4088±0.0426
MOHG	0.0846±0.0378	0.1026±0.0135	0.0402±0.0119
RGMM	0.1686±0.0115	0.2743±0.0521	0.4351±0.0369
RSSCOMP	0.1461±0.0228	0.2657±0.0545	0.2953±0.0474
VAE-nCRP	O.2O11±0.0076	0.4128±0.0242	0.5584±0.0267
VaDE+HKM	0.1637±o.oii6	0.3308±0.0664	0.4850±0.0558
VaDE+MOHG	0.1659±0.0155	0.4227±0.0927	0.4915±0.0713
VaDE+RGMM	0.1806±0.0132	0.3858±0.0615	0.4095±0.0651
VaDE+RSSCOMP	0.1923±o.02ii	0.2718±0.0444	0.2905±0.0431
HCRL	0.2245±o.oi37	0.4553±0.0295	0.6008±0.0973
4.3	Qualitative Analysis
MNIST: In Figure 1, the digits {4, 7, 9} and the digits {3, 8} are grouped together with a clear hier-
archy, which was consistent between HCRL and VaDE. Also, some digits {0, 4, 2} in a round form
are grouped, together, in HCRL. In addition, among the reconstructed digits from the hierarchical
mixture components, the digits generated from the root have blended shapes from 0 to 9, which is
natural considering the root position.
7
Under review as a conference paper at ICLR 2019
CIFAR-100: Figure 4 shows the hierarchical clustering results on CIFAR-100. Given that there
were no semantic inputs from the data, the color was dominantly reflected in the clustering criteria.
However, if one observes the second hierarchy, the scene images of the same sub-hierarchy are
semantically consistent, although the background colors are slightly different.
Figure 4: Example extracted sub-hierarchies on CIFAR-100
RCV1.v2: Figure 5 shows the embedding of RCVLV2. VAE and VaDE show no hierarchy, and
close sub-hierarchies are distantly embedded. VAE-nCRP guides the internal mixture components
to be agglomerated at the center, and the cause of agglomeration is the generatiVe process of VAE-
nCRP, where the parameter of the internal components are inferred without direct information from
data. HCRL shows a clear separation between the sub-hierarchy without the agglomeration.
(a) VAE (Kingma &
Welling, 2014)
(b) VaDE
(Jiang et al., 2017)
(c) VAE-nCRP
(Goyal et al., 2017)
(d) HCRL
Figure 5: Comparison of embeddings on RCV1,v2, plotted using t-SNE (Maaten & Hinton, 2008).
We mark the mean of a mixture component with a numbered square, colored in {red} for VaDE,
{red (root), green (internal), blue (leaf)} for VAE-nCRP and HCRL. The first-level sub-hierarchies
are indicated with four colors.
20Newsgroups: Figure 6 shows the example sub-hierarchies on 20Newsgroups. We enumerated
topic words from documents with top-five likelihoods for each cluster, and we filtered the words by
tf-idf values. We observe relatively more general contents in the internal clusters than in the leaf
clusters of each internal cluster.
Ist subhierarchy on icomputer,
bios pictures picture hardware screen
___brand scsi drive connect computers
___floppy interface transfer words hd
___mac moving floppy screen black
2nd subhierarchy on ipolitics,
topic movement war noise majority
____court criminals law crypto encryption
____claims investigation percent adam wall
____party conflict industry majority unit
3rd subhierarchy on ivehicles&sports,
series vehicle day finally afraid
____hit bunch rule playing station
____excellent hospital insurance game pick
____pictures teams guys shot daily
Figure 6: Example extracted sub-hierarchies on 20Newsgroups
5 Conclusion
In this paper, we have introduced a hierarchically clustered representation learning framework for the
hierarchical mixture density estimation on deep embeddings. HCRL aims at encoding the relations
among clusters as well as among instances to preserve the internal hierarchical structure of data. The
main differentiated features of HCRL are 1) the crucial assumption regarding the internal mixture
components for having the ability to generate data directly, and 2) the unbalanced autoencoding
neural architecture for the level proportion modeling as the encoding structure, and the probabilistic
model as the decoding structure. From the modeling and the evaluation, we found that HCRL enables
the improvements due to the high flexibility modeling compared with the baselines.
8
Under review as a conference paper at ICLR 2019
References
M Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel. Infinite variational autoencoder for
semi-supervised learning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR),pp. 781-790. IEEE, 2017.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine
Learning research, 3(Jan):993-1022, 2003.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Wenqing Chu and Deng Cai. Stacked similarity-aware autoencoders. In Proceedings of the 26th
International Joint Conference on Artificial Intelligence, pp. 1561-1567. AAAI Press, 2017.
Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric Xing. Nonparametric varia-
tional auto-encoders for hierarchical representation learning. arXiv preprint arXiv:1703.07027,
2017.
Thomas L Griffiths, Michael I Jordan, Joshua B Tenenbaum, and David M Blei. Hierarchical topic
models and the nested chinese restaurant process. In Advances in neural information processing
systems, pp. 17-24, 2004.
Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with
local structure preservation. In International Joint Conference on Artificial Intelligence (IJCAI-
17), pp. 1753-1759, 2017.
Philipp Hennig, David Stern, Ralf Herbrich, and Thore Graepel. Kernel topic models. In Artificial
Intelligence and Statistics, pp. 511-519, 2012.
Peihao Huang, Yan Huang, Wei Wang, and Liang Wang. Deep embedding network for clustering.
In Pattern Recognition (ICPR), 2014 22nd International Conference on, pp. 1532-1537. IEEE,
2014.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: An unsupervised and generative approach to clustering. In Proceedings of the 26th
International Joint Conference on Artificial Intelligence, pp. 1965-1972. AAAI Press, 2017.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the
Second International Conference on Learning Representations (ICLR 2014), April 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Ken Lang. Newsweeder: Learning to filter netnews. In Machine Learning Proceedings 1995, pp.
331-339. Elsevier, 1995.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for
text categorization research. Journal of machine learning research, 5(Apr):361-397, 2004.
Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129-137, 1982.
9
Under review as a conference paper at ICLR 2019
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
David JC MacKay. Choice of basis for laplace approximation. Machine learning, 33(1):77-86,
1998.
David Mimno, Wei Li, and Andrew McCallum. Mixtures of hierarchical topics with pachinko allo-
cation. In Proceedings of the 24th international conference on Machine learning, pp. 633-640.
ACM, 2007.
Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep latent gaussian
mixtures. In NIPS Workshop on Bayesian Deep Learning, volume 2, 2016.
David Nister and Henrik Stewenius. Scalable recognition with a vocabulary tree. In Computer vision
and pattern recognition, 2006 IEEE computer society conference on, volume 2, pp. 2161-2168.
Ieee, 2006.
Jim Pitman et al. Combinatorial stochastic processes. Technical report, Technical Report 621, Dept.
Statistics, UC Berkeley, 2002. Lecture notes for St. Flour course, 2002.
Abel Rodriguez, David B Dunson, and Alan E Gelfand. The nested dirichlet process. Journal of the
American Statistical Association, 103(483):1131-1154, 2008.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations
by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive
Science, 1985.
Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. arXiv
preprint arXiv:1703.01488, 2017.
Naonori Ueda, Ryohei Nakano, Zoubin Ghahramani, and Geoffrey E Hinton. Smem algorithm for
mixture models. In Advances in neural information processing systems, pp. 599-605, 1999.
Nuno Vasconcelos and Andrew Lippman. Learning mixture hierarchies. In Advances in Neural
Information Processing Systems, pp. 606-612, 1999.
Chong Wang and David M Blei. Variational inference for the nested chinese restaurant process. In
Advances in Neural Information Processing Systems, pp. 1990-1998, 2009.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In International conference on machine learning, pp. 478-487, 2016.
Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces:
Simultaneous deep learning and clustering. In International Conference on Machine Learning,
pp. 3861-3870, 2017.
Chong You, Daniel Robinson, and Rene Vidal. Scalable sparse subspace clustering by Orthogo-
nal matching pursuit. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3918-3927, 2016.
10
Under review as a conference paper at ICLR 2019
A Synthetic Demo
We created a synthetic dataset that has a hierarchical structure and is sampled from the 50-
dimensional Gaussian distributions, presented in Figure 7. The hierarchy, which has a branch factor
of two and a depth of four, has a total of eight leaf clusters. Figure 7a shows the raw synthetic dataset
in the input space of R50 , and after running HCRL, we plot the hierarchically clustered embeddings
in the latent space in Figure 7b. In addition to the embeddings, we also present a confidence ellipse
with dashed lines for each learned Gaussian mixture component. Because the root component is
involved in generating all of the data, it forms a large ellipse, while the leaf component summarizes
the local density, so the small ellipse is learned.
-60 •----------'-----------t-----------•----------•-
-40	-20	0	20	40
(a) Raw synthetic data ∈ R50
(b) Hierarchically clustered embeddings ∈ R2
Figure 7: Synthetic data in the input space of R50 (left), which is visualized via t-SNE (Maaten
& Hinton, 2008), and hierarchically clustered embeddings in the latent space of R2 (right). We
additionally show a 95% confidence ellipse with a dashed line for each Gaussian mixture component.
We show how the above embeddings learned to be hierarchically clustered in the latent space during
training in Figure 8. In the learning mechanism of HCRL, we can observe the hierarchically clustered
embeddings from a major deviation to a minor deviation in the data over iterations.
(a) Epoch 0
(b) Epoch 10
(c) Epoch 20
(d) Epoch 25
(e) Epoch 30
Figure 8: The process by which the embeddings of the synthetic data are learned. The dashed ellipse
corresponds to the 95% contour of the learned Gaussian mixture component, whose mean is marked
as the gray circle.
(f) Epoch 50
11
Under review as a conference paper at ICLR 2019
B Experimental Settings
We conducted experiments for all autoencoder-based models with a neural architecture whose en-
coder network was set as fully connected layers with dimensions D-2000-2000-500-J for z, and
D-10-10-L for η, and the decoder network is a mirror of the encoder network for z. The hyperpa-
rameters of HCRL given by users, γ and α, was set to 1.0, and a vector of all entries 1 sized of L,
respectively. We used the Adam optimizer (Kingma & Ba, 2014) with an init learning rate of 0.001
for MNIST dataset and 0.0001 for other datasets. Meanwhile, VAE-nCRP is targeted for grouped
data. For experiments with our non-grouped datasets, we treated the group instance as a group in-
stance having a single data instance. For parametric hierarchical clustering models, we gave the
branch factor as the input parameter, [1,20,5], [1,4,7,9], and [1,6,4,3], for CIFAR-100, RCVLv2,
and 20Newsgroups, respectively. For VaDE, we set the number of clusters to the number of leaf
clusters; 100 for CIFAR-100, 252 for RCVLv2, and 72 for 20NeWsgroups.
C Algorithms
C.1 Training Algorithm
Algorithm 1 summarizes the overall algorithm for HCRL. The tree-based hierarhcy T is defined as
(N, P), Where N and P denote a set of nodes and paths, respectively. We refer to the node at level l
lying on path Z, as N(Zi：i) ∈ N. The defined paths, P, consist of full paths (ending at a leaf node),
Pfull, and inner paths (ending at an internal node), Pinner, as a union set.
Algorithm 1 selects an operation out of three operations: GROW, PRUNE, and MERGE. The GROW
algorithm is executed for every specific iteration period, tG. After ellapsing tb iterations since per-
forming the GROW operation, We begin to check Whether the PRUNE or MERGE operation should
be performed. We prioritize the PRUNE operation first, and if the condition of performing PRUNE
is not satisfied, We check for the MERGE operation next. After performing any operation, We ini-
tialize nb to 0, Which is for locking the changed hierarchy during minimum tb iterations to be fitted
to the training data.
Algorithm 1 Training for Hierarchically Clustered Representation Learning
Input: Training examples x; the tree-based hierarchy depth, L; period of performing GROW, tgroW;
minimum number of epochs locking the hierarchy, tlock; operation-related thresholds δprune,
δmerge; a queue Whose element is the set of changed paths, Q; the number of training epochs, E;
maximum length of Q, Qmax; groW scale, sgroW
Output: T(E), φz, Φη, θ, ω, {ai, b, μi, σ2}i∈MT (E)
1: μζ , σQ2	J Initialize L Gaussian mixture components
ZL* L 11 Z1:L
2: T⑼ J Initialize the tree-based hierarchy having a single path with μζι L, σ2
3: nlock J 0 // for counting the number of epochs, Where the hierarchy has not ch:anged
4: for each epoch e = 1, ∙∙∙ ,E do
5:	φz, φη, θ J Update the network weight parameters using gradients Vφz,力小®L(X)
6:	{ai, bi, μi, σ2}i∈Mʃ(e-i)J Update node-specific params. using gradients Vα,b,μ,σ2L(X)
7:	Update other variational parameters using gradients VL(x)
8:	if mod(e, tgrow) = 0 then
9:	T (e), Q J GROW(T (e-1), Q, sgrow, Qmax) // See Algorithm 2
10:	end if
11:	if T(e) = T (e-1) and nlock ≥ tlock then
12:	T(e),Q J PRUNE(T (e-1), Q, δprune) // See Algorithm 3
13:	if T(e) = T (e-1) thenT(e),QJ MERGE(T (e-1), Q, δmerge, Qmax) // See Algorithm 4
14:	end if
15:	if T(e) 6= T (e-1) then nlock J 0 else nlock J nlock + 1
16: end for
12
Under review as a conference paper at ICLR 2019
C.2 Algorithm for Grow Operation
Figure 9: The illustration of GROW operation
The GROW operation expands the hierarchy by creating a new branch under the heavily weighted
internal node. Compared with the work from Wang & Blei (2009), we modify GROW to firstly
sample a path according to PN=I q(Zn = Z), and then grow the path if the sampled path is an
inner path. When we create the new Gaussian mixture component, we initialize the parameters of a
corresponding Gaussian distribution depending on the mean and the variance of the parent node, as
shown in line 10 of Algorithm 2.
Algorithm 2 GROW Operation
1:	function GROW(T Q, Sgrow, QmaX)
2:	J(Z) — PN=1 q(Zn = Z) for Z ∈ P // Calculate the measure
3:	Sample a path Z* with probability JZ,)、 Z J(Z)
4:	Q0 — φ // Temporary set of changed paths in this epoch
5:	if Z* ∈ Pinner and Z* ∈ Q s.t. Q ∈ Q then
6:	lo — |Z |
7:	for l0 = l0,…，L — 1 do
8:	* j0 — MaXimum indeX for the child node whose parent path is Z1:l0
9:	Z*:l0 + 1 — [Z*:l0 ,j0 + 1]
10:	N(Z*：io+ι) -N3ζ]l0 + e,diag02； J) where e 〜N(0,ngIj)
11:	Q - Q ∪ {Z*:l0 + 1}
12:	ifl0 < L— 1 then
13:	τ* Pinner - Pinner ∪ {Cl：" + 1}
14:	else
15:	,*	、 Pfull - Pfull ∪ {Z 1：L}
16:	end if
17:	end for
18:	end if
19:	enqueue Q0 to Q
20:	while QmaX < |Q| do dequeue Q
21:	P — Pfull ∪ Pinner
22:	T— (N, P)
23:	return T, Q
24:	end function
13
Under review as a conference paper at ICLR 2019
C.3 Algorithm for Prune Operation
Figure 10: The illustration of PRUNE operation
Qφo
⅛r
The PRUNE operation cuts a minor path, which is sampled according to PN=Iq(Zn = Z) among
the full paths satisfying PN=I q(Zn = Z) < δ, where δ is the pre-defined threshold parameter. If
the removed leaf node of the full path is the last child of the parent node, we also recursively remove
the parent node as shown in the upper case of Figure 10.
Algorithm 3 PRUNE Operation
1: function PRUNE(T, Q, δprune)
2:	J(Z) . PN=Iq(Zn = Z), Z ∈ P // Calculate the measure
3:	Ω 一 {Z | Z ∈ Pfull, PJZZO) < δprune}
4:	Randomly sample a full path Z * * 〜Ω
5:	if |Pfull | > 1 and Z* / Q s.t. Q ∈ Q then
*	、
6:	Pfull — Pfull\{Z i：l }
7:	for l0 = L — 1,…，1 do
8:	N(Zιi0+ι) J φ
9:	if l0 < L — 1 then
*
10:	P inner J P inner\{Z 1：l0 + 1}
11:	end if
,*
12:	nc J Number of the children nodes whose parent path is Z1：l0
13:	if nc > 0 then
14:	break
15:	end if
16:	end for
17:	end if
18:	P J Pfull ∪ Pinner
19:	T J (N, P)
20:	return T, Q
21: end function
14
Under review as a conference paper at ICLR 2019
C.4 Algorithm for Merge Operation
Figure 11: The illustration of MERGE operation
The MERGE operation combines two full paths with similar posterior probabilities, measured by
J(Z(i),Z(j)) = qiqj/∖qi||qj|, where qi = [q(Zι = Z(i)),…,q(0N = Z(i))]. We merged two
Gaussian components by following Ueda et al. (1999). The specific meaning of combining the two
paths is merging the paired two Gaussian distributions lying on the two paths by level, if the two
GaUSSian distribtions are different. The estimation of merged GaUSSian parameters, μ and σ, is the
weighted summation of two subject Gaussian parameters. The propbability of the node at level l
lying on a Path Z given x, p(Zι ∖x), is proportional to Pn{q(In = l) ∙ PZ∈λ q(Zn = ζ)}, where
Λ={Z0∖ζl0=ζlandZ0 ∈ PfUll}.
Algorithm 4 MERGE Operation
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
function MERGE(T, Q, δmerge , Qmax)
J (Z (i), Z (j))一
TqnTqjT
s.t. qi
[q(Z1 = Z(i)),..., q(ZN = Z(i))] // Calculate themeasUre
Ω J{(Z(i), Z (j)) ∖ J (Z (i), Z (j)) ≥^merge,{Z(i), Wj)} ⊂ Pfull}
Randomly sample a pair of paths (Z(1), Z⑵)~ Ω
Q0 J φ // Temporary set of changed paths in this epoch
if {Z(1), Z⑵} * Q s.t. Q ∈ Q then
(1) -(2)
l J Maximum level of nodes shared by Z , Z
Z *：i j Z 11ι)
for l0 = l,…，L — 1 do
22	22
μ(1) J μζ(I)+ɪ, σ⑴ J σζ⑴	，μ⑵ J μζ(2),+ɪ, σ⑵ J σζ(2)
w(1) J P(Z(01+1 ∖x), w(2) J P(Z(2-+1 ∖x)
J w(i)μ(i)+w(2)μ(2)	2 J
μ* J	w(1) +w(2)	, σ* J
w(1)σ2i) +w ⑵ σ22)
w(1)+w(2)
*
j0 一 MaximUm index for the child node whose parent path is Z1:l0
—*	「二*
Z1:l0+1 一 Z1:l0 , j0 + 1
N(Z *：10+1) JN (μ*, diag(σ2))
N(Z 11)0+ι) — Φ ,N(Z 12)0+ι) - φ
if l0 < L - 1 then
Pinner — Pinner ∪ {CLl0 + l}∖{C 1：l0 + 1, Cl：l0 + l}
else
PfUllJ Pfull ∪{Z*:L }\{Z 11L, Z12L}
end if
*
Q J Q ∪ {Z 1:10 + 1}
end for
end if
enqueue Q0 to Q
while Qmax < ∖Q∖ do dequeue Q
P J PfUll ∪ Pinner
T J (N, P)
return T, Q
end function
15
Under review as a conference paper at ICLR 2019
D Notations
The following Table 4 lists the notations used throughout this paper.
Table 4: Table of symbols
Models	Symbol	Definition
All	x/x0 z D/J g*(x) fθ(z) θ 2 μ Z, σ N 2 μx, σx		An observed / reconstructed datapoint A latent representation The input / latent dimensionality A encoder network parametrized by *, whose input is X A decoder network parametrized by θ, whose input is z The variational parameters and weights of the decoder network fθ The variational mean and variance for Gaussian distribution qφz (z|x) The prior parameters, mean and variance, for Gaussian distribution pθ (x|z)
VaDE & VAE-nCRP	φ N μ, σ	The variational parameters and weights of the encoder network gφ The variational mean and variance for Gaussian distribution qφ (z|x)
VaDE & HCRL	N xn = 1,…，N zn = 1,…，N	The number of datapoints n-th observed datapoint n-th latent representation corresponding to Xn
VAE-nCRP & HCRL	L	The height of the tree-based hierarchy
VaDE	K cn = 1,…，N κ μc,σN	The number of (finite) clusters The cluster assignment of zn, ∈ {1, ..., K} The prior parameter for multinomial distribution p(c) The prior parameters, mean and variance, for Gaussian distribution of c-th cluster, P(Z)
VAE-nCRP	M Nm =1,…,M xm,n=1 ,∙∙∙ ,N zm,n=1 ,∙∙∙ ,N vmp Y (0)	(1) γmp , γmp ζmn Sm n αpar(p) α* N μρar(p), σpar(p) αp N σN N μp , σp σDN	The number of sequences The number datapoints in m-th sequence n-th observed datapoint in m-th sequence n-th latent representation corresponding to Xmn The Beta draws of m-th sequence on node p, for the tree-based stick- breaking construction The prior parameter for Beta distribution p(vmp) The variational parameters, for Beta distribution q(vmp|Xm) The path assignment of zmn The variational parameter for multinomial distribution q(ζmn |Xmn) The J -dimensional parameter vector for the parent node ofp The prior parameter for Gaussian distribution p(αp) for the root node The variational mean and variance for Gaussian distribution q(αpar(p) |X) The J -dimensional parameter vector for node p The prior parameter, variance, for Gaussian distribution ρ(αp∣αpar(p)) The variational mean and variance for Gaussian distribution q(αp∣x) The prior parameter, variance, for Gaussian distribution p(zmn∣Zmn, ap)
HCRL	2∙2	0	W 2Z 2	The variational parameters and weights of the encoder network gφz The variational parameters and weights of the encoder network gφη The variational mean and variance for Gaussian distribution qφz (z|X) The variational mean and variance for logistic normal distribution qφη(n|x) The variational parameter for Dirichlet distribution qφη (η∣x) The Beta draws for the tree-based stick-breaking construction of node i The prior parameter for Beta distribution p(vi) The variational parameters, for Beta distribution q(vi |X) The path assignment of zn The variational parameter for multinomial distribution q(Zn∣Xn) The level proportion of zn The prior parameter for Dirichlet distribution p(ηn ) The level assignment of zn, ∈ {1, ..., L} The variational parameter for multinomial distribution q(ln|Xn) The prior parameters, mean and variance, for Gaussian distribution of node
i, P(Zn|Zn, ηn)
16
Under review as a conference paper at ICLR 2019
E Generative and Inference Model for HCRL
HCRL assumes the generative process as described in Section 3.1. Section E.1 describes the joint
probability distribution, and Section E.2 presents the corresponding variational distributions. We
adopt the much notation-related conventions from Wang & Blei (2009), especially on paths.
E.1	Generative Model
N
Pθ (v,Z, η, l, z, x) = p(v∣γ) ∏ P(Zn∣v)p(ηn∣α)p(ln∣ηn)p(Zn∣Zn, ln, "L∞, σ2∞)pθ (Xn∣Zn)
n=1
N
=∏ P(Vj ∣γ) ∏ P(Vim ∏P(Zn∣v)p(ηn∣α)p(ln∣ηn)p(Zn∣Zn,ln)pθ(Xn∣Zn)
j ∈/ MT	i∈MT	n=1
•	MT : Set of all nodes in truncated tree T
•	For j ∈ MT,p(vj ∣γ) =Beta(Vj ∣1,γ)
•	For i ∈ MT,p(vi∣γ) = Beta(vi∣1,γ)
•	P(Zn = [1,Z2 ,…,Zl]∣v)
L
P(Zn = [1,ζ2 ,…,ZL]|V) = Y π1,Z2,…,Zl
l=1
L	ζl -1
=	π1,ζ2,...,ζl-1 V1,ζ2,...,ζl	(1 - V1,ζ2,...,j)
l=1	j=1
=YL Yl	V1,ζ2,...,ζl0 ζYl0-1(1 - V1,ζ2,...,j)
l=1 l0=1	j=1
-π1,ζ2,...,ζ1 = Qlo=i{vi,Z2,…,Go(QZ=-I(I-V1,Z2,…,j))}
•	p(ηn∣ɑ) = Dirichlet(ηn∣ɑ)
•	p(ln∣ηn) = Multinomial(ηn)
•	P(Zn IZn = Z,ln = I) = N(Zn lμZι, σZι IJ)
•	pθ(xn∣Zn) : Probabilistic decoding of Xn parametrized by θ, whose input is Zn
•	Tree-based stick-breaking construction
- We will denote all Beta draws as v, each of which is an independent draw from Beta(v∣1, Y)
(except for root V1 = 1)
* Vi ~ Beta(Vi|1, Y)
-	The root nodes stick length: π1 = V1 ≡ 1
-	Stick length at second level: π1i = π1V1i Qij-=11(1 - V1j), Pi∞=1 π1i = π1 = 1
-	For the segment π1k, the stick lengths of its children are π1ki = π1kV1ki Qij-=11(1 - V1kj), for
i = 1, 2, ..., ∞,	i=1 π1ki = π1k
E.2 Inference Model
As VAE, we infer the random variables via the mean-field approximation, where the variational
distribution, qφη,φz (v, Z, η, l, Z|x), approximates the intractable posterior. We model the variational
distributions as follows:
N
qφη,φz (v, Z, η,l, z|x) = q(v∣a, b, x) ∏ q(Zn∣Xn)qφη (ηn∣Xn)q(ln∣ωn, Xn)qφz (Zn∣Xn )
n=1
N
=ɪ ɪ P(Vj IY) ɪ ɪ q(Vi lai, bi) ]] q (ZnIxn) qφη (ηn |xn )q(ln lωn, xn)qφz (ZnIxn)
j∈Mτ	i∈Mτ	n=1
17
Under review as a conference paper at ICLR 2019
•	For j ∈ MT,p(vj ∣γ) =Beta(Vj ∣1,γ)
•	For i ∈ MT, q(vi∣ai, bi) H VaiT(I - Vi)bi-1 = Beta(ViIai, bi)
—	ai	= 1 + (L -	Ii	+ 1) PN=1	Pzi0+i,...,zl q(Zn = [1,Z2,…，Z10,Z1o+i,...,Zl])
-bi	= ] +(L -	li	+ 1) PN=I	Pj,Z10+1,..,ζLj>Z10 q(Zn = [1,Z2,…,Zi0-i,j,Zi0+1,…,Zl])
* li : The level of the mixture component i
• q(ZnIxn) h SnZ , Pζ∈child(Z) SnZ
- Z: a path in the truncated tree T, either an inner path (a path ending at an internal node) or a full
path (a path ending at a leaf node)
-child(Z): the set of all full paths that are not in T but include Z as a sub path
* As a special case, if Z is a full path, child(Z) just contains itself
-	In the case of a full path,
expEqXXlogV1,ζ2,...,ζl0 + ζXl0-1
log(1 - V1,ζ2,...,j)	+ Z0
l=1 l0=1	j=1
exp{Eq χ(χ(log v1,Z2,…,Z10 + X log(1 - v1,Z2,...,j)) +logN(Zn Eni ,σ2ni
l=1	l0=1	j=1
exp	- l +1) (Eq [log vl,Z2,...,Zi] + X Eq [log(I-V1,Z2,…,j )]+ log N(Zn Mni
j=1
-In the case of an inner path, Z，[1, Z2 ,…，Zk]⊂ MT
*	child(Z) , {[Z,ZI0+1,∙∙∙,Zl]: ZI0+1 >j0}	_
*	jo : maximum index for the child node whose parent path is Z
SnZ =	X	SnZ
Z∈child(Z)
X expEqXL (Xl (log
V1,ζ2,...,ζi0 + ζXi0-1
Iog(I - v1,Z2,…,j )) +log N (znlμZnl
Z∈child(Z)	[lI=I ∖,1=∖'	j=ι	)
exp{(Eq[P3ο+ιlogNlZn∖μζni,σ2nilj)]+ PL=I0+1 log(L - l +1) + (L - l0)(ψ(l) - ψ(1+ γ)))}
exp
L-l+1)
(1 - exp{ψ(γ) - ψ(1 + γ)})L-l0
v1,Z2,…,Zi]+ X Eq [log(1 - v1,Z2,…,j )]) +log Nlzn∖μζnl ,σ2niIJ )
j=1
(L - l0) log(1 - V1,ζ2,...,ζl0,j)
-	qφη(ηn∖xn) = Dirichlet(ηn∖αen)
* enl = e1y(1 - L2 + e Ln PL=I e-μηnl0 )
- q(ln∖ωn,xn) = Multinomial(ln ∖ωn)
* ωn H exp{ψ(eni) - ψ(dnθ)+ PZ SnZ(PJ=I -2 log(2∏σ2nιj) - 2^zn- - (“飞—*加1") )}
ζnl ,j	ζnl ,j
L
• αn0 = / √i=i αni
18
Under review as a conference paper at ICLR 2019
* Derivation for ωn
Lωnl =	3nl (ψ (αenl ) — ψ(αen0 )) — 3nl log 3nl
l
l
+	Snζ0
ζ0
ωnl
1	σez2
-2log(2πσ2n A- 2σj
ζnl ,j
—
(%一μζnι,j产
=3nl(ψ(anl) — ψ(αnθ)) + ^X SnZ03nl (^X — 2 log(2πσ∣0Gj)—
— 3nl log 3nl + λ(	3nl — 2)
l
,j
eZnj	(μZnj — MM l,j )2
2σ2n ι,j
—
2σ2n ι,j
∂Lω
ωnl
∂ωnl
-2 log(2πσ2n Ij)-
eZnj	(eZnj— μζn l,j )2
2σ2n ι,j
—
2σ2n l,j
- (log ωnl + 1) + λ
3ni = exp{ψ(Sni) — ψ(Sno) + X S^，(X — 2 log(2πσ∣0i,j)—
eZnj	(μZnj - μζn l,j )2
3ni H exp I ψ(e⅛ni) — ψ(eenθ) + X SnZ，(X — 2 log(2πσ∣0 K)—
-------- - ------------------
'吟,j	2σ20, j
nl ,j	nl ,j
eZnj	(eznj - 〃* l,j )2
—
—1 + λ
2σ2n ι,j
—
2σ2n ι,j
-qφz (zn|xn) = N (zn[μzn , σLIJ )
F Evidence Lower B ound
In this section, we present the detailed derivation of the ELBO in Equation 6, which is the objective
function for learning HCRL.
logP(X) ≥ LELBO(X) = Eq [logP£ZrnIlZzlx
q(v, ζ ,η, ,z|x)
Eq log
Qi∈MT P(Vi∣Y ) Qn=I P(Zn∣v)p(ηn∣ɑ)p(ln∣ηn)p(Zn∣Zn,ln)Pθ (Xn∣Zn) ^
Qi∈MT q(vi∣ai,bi) Qn=I q(Zn ∣Xn)qφη (ηn∣Xn)q(ln gn, Xn)qφz (Zn∣Xn).
N
E Eq [log p(vi∣γ)] + EEq [log P(Zn∣v) + log P^nl。)+ log p(ln∣ηn) + log P(Zn|Zn,ln )
i∈MT
n=1
N
+ log Pθ (Xn∣Zn)] — E Eq [log 9也旧也)]—EEq [log q(Zn∣Xn) + log 9φη (ηn∣Xn)
i∈MT
+ log q(ln∣ωn, Xn) + log qφz(Zn ∣Xn)]
n=1
(6)
F.1 Detailed Derivation for ELBO
The followings are additional notations used for the detailed derivation:
• ψ : The digamma function
LL
• αn0 = i=1 αni, α0 = i=1 αi
19
Under review as a conference paper at ICLR 2019
Eq [log P(Vi ∣γ)]
LU XX jYJvj
N
v0) Y q(vi = v0) Y q(ζn = ζ0)q(ηn = η0)
i∈MT
n=1
q(ln = l0)q(zn = z0) log P(vi = v0)dη0dz0dv0
P(vj = v0)	q(vi = v0) log P(vi = v0)dv0
v0 j ∈/ MT	i∈MT
q(vi = v0) log P(vi = v0)dv0
v0
/ Beta(v0∣ai, bi) ∙ logBeta(v0∣1, γ)dv0
v0
log Γ(1 +γ) - log Γ(1) - log Γ(γ) + (1 - 1)ψ(ai) + (γ - 1)ψ(bi)
+(-1 - γ + 2)ψ(ai + bi)
log Γ(1 +γ) - logΓ(γ) + (γ - 1)ψ(bi) + (1 - γ)ψ(ai + bi)
logΓ(1+γ)-logΓ(γ)+(γ-1)(ψ(bi)-ψ(ai+bi))
log(γΓ(γ)) -logΓ(γ) + (γ - 1)(ψ(bi) -ψ(ai + bi))
log γ + log Γ(γ)) -logΓ(γ) + (γ - 1)(ψ(bi) -ψ(ai + bi))
logγ+ (γ- 1)(ψ(bi) -ψ(ai +bi))
Eq [log P(Zn∣v)]
exp{logΓ(L - lo + 1) + (L - lo)(ψ(1) - ψ(1 + γ))}
(1 - exp{ψ(γ) - ψ(1 + γ)})L-l0
×
exp
L-l+1)
v1,ζ2,...,ζl] + Eq[log(1 - v1,ζ2,...,j)]
j=1
j0
×
exp Eq (L-l0)	log(1 - v1,ζ2,...,ζl0,j)
j=1
Eq [logP(ηn∣α)]
XX Y
P(vj = v ) Y q(vi = v )q(ζn = ζ )q(ηn = η )
JEJzJB l Z0 j∈Mτ	i∈Mτ
q(ln = l0)q(zn = z)log p(nn =d ∣ a) dn0dz0dv0
q q(nn = n0)logp(nn = n0∣a)dn0 = D Dir(n0∣an) ∙ logDir(n0∣a)dn0
η0	η0
L
L
logΓ(α0) -	logΓ(αi) +	(αi - 1)(ψ(αeni) - ψ(αen0))
i=1
i=1
Eq [logP(ln∣ηn)]
XX Y
p(vj = v0) Y q(vi = v0)q(ζn = ζ)q(ηn = η0)
JU JzJn l0 ζ0 j∈Mτ	i∈Mτ
q(ln = l )q(zn = Z ) logP(In = IInn)dηdz dv
q E q(nn = n0)q(in = i)logP(In = ι∣nn = n0)dn0
η0 l0
X q(in = ι0) / q(nn = n0)logMult(i0In0)dn0
l0	η0
X ωnio / Dir(n01an)log η0odn0
l0	η0
X ωni0(ψ(αeni0 ) - ψ(αen0))
i0
20
Under review as a conference paper at ICLR 2019
Eq [log P(Zn|Zn,ln)]
XX Y
p(vj = v0) Y q(vi = v0)q(ζn = ζ)q(ηn = η0)
JV，JzJnIO zo j∈Mτ	i∈Mτ
q(ln = l )q(Zn = z) logp(Zn = Z | Cn = Z,ln = l )dη dz dv
/	ΣΣq(Zn	=	Z)q(ln	=	l0)q(Zn = z)lθgP(Zn	= Z IZn	= ZNn	=	l0)dz0
z0 l0 ζ0
XX
q(Zn = Z)q(ln = l0)	q(Zn = Z) log p(Zn = Z|Zn = Z, ln = l0)dZ0
XXq(Zn =Z0)q(ln = l0) [ N(Z∣μzo, e2rIJ) ∙ logN(Z∖μζ , σ2l Ij)dZ0
l0 ζ0	z 0	nl0	nl0
X Snzo X"n，0 (X -2log(2∏σ2nj") 一
ζ0	l0	j=1
e2nj	(Knj-仁 l，" )2
2σ2n I，,j
2σ2n y,j
—
Eq [log Pθ (Xn∣Zn)]
XX Y
p(v" = v0) Y q(vi = v )q(Zn = Z)q(ηn = η )
JIVJzJnI z j∈Mτ	i∈Mτ
q(ln = l0)q(Zn = Z)log Pθ (xn∣Zn = Z°)dη, dZ dV
I q(Zn = Z)log Pθ (Xn∣Zn = Z0)dZ0
z0
1S
≈ S ElogPΘ(xn∣ZnS)), where Z(i,S)=
s=1
J S PS=I PD=1 Xnd log μXSn)d + (1 -
=]1 PS=I PLT log(2∏σX%,
μ(X + σ(i Θ 七⑸ and W(S)〜N(0, Ij)
-χnd)log(1 — μXs)d )
(Xnd-Rd)2
2σs)2
if xn is binary
if xn is real-valued
Eq[log q(vi|ai, bi)]
XX Y
p(v" = v0) Y q(vi = v0)q(Zn = Z)q(ηn = η0)
JV，JzJnIO zo j∈Mτ	i∈Mτ
q(ln = l0)q(Zn = Z) log q(vi = v0)dη0dZ0dv0
p(v" = v0)	q(vi = v0) logq(vi = v0)dv0
Jv，j∈Mτ	i∈Mτ
J q(vi = v0) log q® = v0)dv0 = J Beta(V0%,%) ∙ logBeta(v0∣ai, bi)dv0
log Γ(ai + bi) - logΓ(ai) - logΓ(bi) + (ai - 1)ψ(ai) + (bi - 1)ψ(bi)
+(-ai - bi + 2)ψ(ai + bi)
Eq [log q(Zn∣Xn)]
XX Y
p(v" = v0) Y q(vi = v )q(Zn = Z)q(ηn = η )
JvJzJlη l Z j∈Mτ	i∈Mτ
q(ln = l )q(Zn = Z) log q(Zn = Z)dη dZ dv
X q(Zn = Z) log q(Zn = Z)
ζ0
ΣSnZ0	1	SnZ0
z Pzoo Snzoo og Pzoo Snzoo
21
Under review as a conference paper at ICLR 2019
Eq [log qφη (ηn∣Xn)]
XX Y
p(vj = v0) Y q(vi = v0)q(ζn = ζ)q(ηn = η0)
JeJzJny Z0 j∈Mτ	i∈Mτ
q(ln = l0)q(zn = z) log q(ηn = η0)dη0dz0dv0
q q(ηn = η0)logq(ηn = η0)dη0 = D Dir(η0∣ɑn ∙logDir(η0∣αn)dη0
η0	η0
L
L
Eq [log q(ln∣3n, Xn )]
logΓ(αen0) -	logΓ(αeni) +	(αeni - 1)(ψ(αeni) - ψ(αen0))
i=1
i=1
XX Y
p(vj = v0) Y q(vi = v )q(ζn = ζ)q(ηn = η )
JlvJzJnIo ζ j∈Mτ	i∈Mτ
q(ln = l0)q(zn = z) log q(ln = l0)dη0dz0dv0
X q(ln = l0)log q(ln = l0) = X Mult(l0∣ωn)log Mult(l0∣ωn)
l0
fωnio ∙ log ωn
l0
l0
Eq [log qφz(Zn∣Xn)]
XX Y
p(vj = v0) Y q(vi = v )q(ζn = ζ)q(ηn = η )
JIvJzJn l ζ j∈Mτ	i∈Mτ
q(ln = l0)q(zn = z) log q(zn = z)dη0dz0dv0
q(zn = z) log q(zn = z)dη0dz0
q q(zn = z)logq(zn = z)dz0 = / N(z|”z0, σ2oIJ) ∙ logN(z∣μz∕, σ20Ij)dz0
z0	z0
-2 lOg(Zn) - 2 X(I +log eZnj )
j=1
22