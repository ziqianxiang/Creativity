Under review as a conference paper at ICLR 2019
Entropic GANs meet VAEs:
A Statistical Approach to Compute Sample
Likelihoods in GANs
Anonymous authors
Paper under double-blind review
Ab stract
Building on the success of deep learning, two modern approaches to learn a prob-
ability model of the observed data are Generative Adversarial Networks (GANs)
and Variational AutoEncoders (VAEs). VAEs consider an explicit probability
model for the data and compute a generative distribution by maximizing a vari-
ational lower-bound on the log-likelihood function. GANs, however, compute a
generative model by minimizing a distance between observed and generated prob-
ability distributions without considering an explicit model for the observed data.
The lack of having explicit probability models in GANs prohibits computation of
sample likelihoods in their frameworks and limits their use in statistical inference
problems. In this work, we show that an optimal transport GAN with the entropy
regularization can be viewed as a generative model that maximizes a lower-bound
on average sample likelihoods, an approach that VAEs are based on. In particular,
our proof constructs an explicit probability model for GANs that can be used to
compute likelihood statistics within GAN’s framework. Our numerical results on
several datasets demonstrate consistent trends with the proposed theory.
1	Introduction
Learning generative models is becoming an increasingly important problem in machine learning
and statistics with a wide range of applications in self-driving cars (Santana & Hotz, 2016), robotics
(Hirose et al., 2017), natural language processing (Lee & Tsao, 2018), domain-transfer (Sankara-
narayanan et al., 2018), computational biology (Ghahramani et al., 2018), etc. Two modern ap-
proaches to deal with this problem are Generative Adversarial Networks (GANs) (Goodfellow et al.,
2014) and Variational AutoEncoders (VAEs) (Kingma & Welling, 2013; Makhzani et al., 2015;
Rosca et al., 2017; Tolstikhin et al., 2017; Mescheder et al., 2017b).
VAEs (Kingma & Welling, 2013) compute a generative model by maximizing a variational lower-
bound on average sample likelihoods using an explicit probability distribution for the data. GANs,
however, learn a generative model by minimizing a distance between observed and generated dis-
tributions without considering an explicit probability model for the data. Empirically, GANs have
been shown to produce higher-quality generative samples than that of VAEs (Karras et al., 2017).
However, since GANs do not consider an explicit probability model for the data, we are unable
to compute sample likelihoods using their generative models. Computations of sample likelihoods
and posterior distributions of latent variables are critical in several statistical inference. Inability to
obtain such statistics within GAN’s framework severely limits their applications in such statistical
inference problems.
In this paper, we resolve these issues for a general formulation of GANs by providing a theoretically-
justified approach to compute sample likelihoods using GAN’s generative model. Our results can
open new directions to use GANs in massive-data applications such as model selection, sample
selection, hypothesis-testing, etc (see more details in Section 5).
Now, we state our main results informally without going into technical conditions while precise
statements of our results are presented in Section 2. Let Y and Y ：= G(X) represent observed (i.e.
real) and generative (i.e. fake or synthetic) variables, respectively. X (i.e. the latent variable) is
the randomness used as the input to the generator G(.). Consider the following explicit probability
1
Under review as a conference paper at ICLR 2019
low
densit
high
generative variable
^
Y= G*(X)
Figure 1: A statistical framework for GANs. By training a GAN model, we first compute optimal
generator G* and optimal coupling between the observed variable Y and the latent variable X. The
likelihood of a test sample ytest can then be lower-bounded using a combination of three terms: (1)
the expected distance of ytest to the distribution learnt by the generative model, (2) the entropy of
the coupled latent variable given ytest and (3) the likelihood of the coupled latent variable with ytest.
model of the data given a latent sample X = x:
fγ∣x=x(y) 8 exp(-'(y, G(x))),	(1.1)
where `(., .) is a loss function. fY∣X=x(y) is the model that we are considering for the underlying
data distribution. This is a reasonable model for the data as the function G can be a complex
function. Similar data models have been used in VAEs. Under this explicit probability model, we
show that minimizing the objective of an optimal transport GAN (e.g. Wasserstein GAN Arjovsky
et al. (2017)) with the cost function `(., .) and an entropy regularization (Cuturi, 2013; Seguy et al.,
2017) maximizes a variational lower-bound on average sample likelihoods. I.e.
average sample likelihoods ≥ - (entropic GAN objective) + constants.	(1.2)
If '(y, y) = Ily - y ∣∣2, the optimal transport (OT) GAN simplifies to WGAN (ArjoVsky et al., 2017)
while if '(y,y) = Ily 一 y∣2, the OT GaN simplifies to the quadratic GAN (or, w2gAN) (Feizi
et al., 2017). The precise statement of this result can be found in Theorem 1. This result provides
a statistical justification for GAN’s optimization and puts it in par with VAEs whose goal is to
maximize a lower bound on sample likelihoods. We note that the entropy regularization has been
proposed primarily to improve computational aspects of GANs (Genevay et al., 2018). Our results
provide an additional statistical justification for this regularization term. Moreover, using GAN’s
training, we obtain a coupling between the observed variable Y and the latent variable X . This
coupling provides the conditional distribution of the latent variable X given an observed sample
Y = y. The explicit model of equation 1.1 acts similar to the decoder in the VAE framework, while
the coupling computed using GANs acts as an encoder.
Connections between GANs and VAEs have been investigated in some of the recent works as well
(Hu et al., 2018; Mescheder et al., 2017a). In Hu et al. (2018), GANs are interpreted as methods
performing variational inference on a generative model in the label space. In their framework,
observed data samples are treated as latent variables while the generative variable is the indicator of
whether data is real or fake. The method in Mescheder et al. (2017a), on the other hand, uses an
auxiliary discriminator network to rephrase the maximum-likelihood objective of a VAE as a two-
player game similar to the objective ofa GAN. Our method is different from both these approaches
as we consider an explicit probability model for the data, and show that the entropic GAN objective
maximizes a variational lower bound under this probability model, thus allowing sample likelihood
computation in GANs similar to VAEs.
Of relevance to our work is Wu et al. (2016), in which annealed importance sampling (AIS) is used
to evaluate the approximate likelihood of decoder-based generative models. More specifically, a
Gaussian observation model with a fixed variance is used as the generative distribution for GAN-
based models on which the AIS is computed. Gaussian observation models may not be proper
2
Under review as a conference paper at ICLR 2019
specially in high-dimensional spaces. Our approach, on the other hand, makes a connection between
GANs and VAEs by constructing a theoretically-motivated model for the data distribution in GANs.
We then leverage this approach in computing sample likelihood estimates in GANs.
Another key question that we address here is how to estimate the likelihood of a new sample ytest
given the generative model trained using GANs. For instance, if we train a GAN on stop-sign
images, upon receiving a new image, one may wish to compute the likelihood of the new sample
ytest according to the trained generative model. In standard GAN formulations, the support of the
generative distribution lies on the range of the optimal generator function. Thus, if the observed
sample ytest does not lie on that range (which is very likely in practice), there is no way to assign a
sensible likelihood score to that sample. Below, we show that using the explicit probability model of
equation 1.1, we can lower-bound the likelihood of this sample ytest. This is similar to the variational
lower-bound on sample likelihoods used in VAEs. Our numerical results show that this lower-bound
well-reflect the expected trends of the true sample likelihoods.
Let G* and Pγ,χ be the optimal generator and the optimal coupling between real and latent vari-
ables, respectively. The optimal coupling P^X can be computed efficiently for entropic GANs as
we explain in Section 3. For other GAN architectures, one may approximate such couplings as we
explain in Section 4. The log likelihood of a new test sample ytest can be lower-bounded as
logfY(ytest) ≥ - EPXY=ytest ['(ytest, G*(x))] + H(PXIY=ytest) + EPXY=ytest [-ɪf2].	(1.3)
'---~^.------Z 、__________________V_______________Z 、--------V------，	L 一
loglikelihood	distancetothegenerativemodel	couplingentropy	likelihood；VntvariabZ
We present the precise statement of this result in Corollary 2. This result combines three components
in order to approximate the likelihood of a sample given a trained generative model:
•	The distance between ytest to the generative model. If this distance is large, the likelihood
of observing ytest from the generative model is small.
•	The entropy of the coupled latent variable. If the entropy term is large, the coupled latent
variable has a large randomness. This contributes positively to the sample likelihood.
•	The likelihood of the coupled latent variable. If latent samples have large likelihoods, the
likelihood of the observed test sample will be large as well.
Figure 2a provides a pictorial illustration of these components. In what follows, we explain the
technical ingredients of our main results. In Section 3, we present computational methods for GANs
and entropic GANs, while in Section 4, we provide numerical experiments on benchmark datasets.
2	Main Results
Let Y ∈ Rd represent the real-data random variable with a probability density function fY (y).
GAN's goal is to find a generator function G : Rr → Rd such that Y ：= G(X) has a similar
distribution to Y . Let X be an r-dimensional random variable with a fixed probability density
function fX (x). Here, we assume fX (.) is the density of a normal distribution. In practice, we
observe m samples {yι,…,Ym} from Y and generate m′ samples from Y, i.e., {yι,..., ym'} where
y = G(xi) for 1 ≤ i ≤ m. We represent these empirical distributions by Pγ and PY, respectively.
Note that the number of generative samples m′ can be arbitrarily large.
GAN computes the optimal generator G* by minimizing a distance between the observed distribu-
tion Pγ and the generative one Pγ. Common distance measures include optimal transport measures
(e.g. Wasserstein GAN (Arjovsky et al., 2017), WGAN+Gradient Penalty (Gulrajani et al., 2017),
GAN+Spectral Normalization (Miyato et al., 2018), WGAN+Truncated Gradient Penalty (Petzka
et al., 2017), relaxed WGAN (Guo et al., 2017)), and divergence measures (e.g. the original GAN’s
formulation (Goodfellow et al., 2014), f -GAN (Nowozin et al., 2016)), etc.
In this paper, we focus on GANs based on optimal transport (OT) distance (Villani, 2008; Arjovsky
et al., 2017) defined for a general loss function `(., .) as follows
W'(Pγ, Pγ) ：= min E ['(Y,Y)].	(2.1)
py,Y
3
Under review as a conference paper at ICLR 2019
PYY is the joint distribution whose marginal distributions are equal to Pγ and PY, respectively.
If '(y, y) = Ily - yg, this distance is called the first-order Wasserstein distance and is referred to
by Wι(.,.), while if '(y,y) = Ily - y∣2, this measure is referred to by W2(.,.) where W2 is the
second-order Wasserstein distance (Villani, 2008).
The optimal transport (OT) GAN is formulated using the following optimization (Arjovsky et al.,
2017; Villani, 2008):
min W'(Pγ, Pγ),
G∈G
(2.2)
where G is the set of generator functions. Examples of the OT GAN are WGAN (Arjovsky et al.,
2017) corresponding to the first-order Wasserstein distance W1(., .) 1 and the quadratic GAN (or,
the W2GAN) (Feizi et al., 2017) corresponding to the second-order Wasserstein distance W2(., .).
Note that optimization 2.2 is a min-min optimization. The objective of this optimization is not
smooth in G and it is often computationally expensive to obtain a solution (Sanjabi et al., 2018).
One approach to improve computational aspects of this optimization is to add a regularization term
to make its objective strongly convex (Cuturi, 2013; Seguy et al., 2017). The Shannon entropy
H(Pγ,γ) ：= -E[logPγ,γ].
function is defined as
The negative Shannon entropy is a common
strongly-convex regularization term. This leads to the following optimal transport GAN formulation
with the entropy regularization, or for simplicity, the entropic GAN formulation:
m∈n min E['(Y,Y)] - λH (Pγ,γ),
(2.3)
where λ is the regularization parameter.
There are two approaches to solve the optimization problem 2.3. The first approach uses an iterative
method to solve the min-min formulation (Genevay et al., 2017). Another approach is to solve
an equivelent min-max formulation by writing the dual of the inner minimization (Seguy et al.,
2017; Sanjabi et al., 2018). The latter is often referred to as a GAN formulation since the min-max
optimization is over a set of generator functions and a set of discriminator functions. The details of
this approach are further explained in Section 3.
In the following, we present an explicit probability model for entropic GANs under which their
objective can be viewed as maximizing a lower bound on average sample likelihoods.
Theorem 1 Let the lossfunction be shift invariant, i.e., '(y, y) = h(y - y). Let
fγX=x(y) = Cexp(-'(y, G(x))∕λ),	(2.4)
be an explicit probability model for Y given X = x for a well-defined normalization
, ∫y∈Rd eχp(-'(y,G(χ))∕λ).
Then, we have
Epy [logfγ(Y)] ≥ - 1 {Epy,y ['(Y,Y)] - λH (Pγ,γ)} +constants.
'	Γ7T^:^^"s---------------------------V----------------Z
ave. sample likelihoods	entropic GAN objective
(2.5)
(2.6)
In words, the entropic GAN maximizes a lower bound on sample likelihoods according to the explicit
probability model of equation 2.4.
The proof of this theorem is presented in Section A. This result has a similar flavor to that of VAEs
(Makhzani et al., 2015; Rosca et al., 2017; Tolstikhin et al., 2017; Mescheder et al., 2017b) where a
generative model is computed by maximizing a lower bound on sample likelihoods.
Having a shift invariant loss function is critical for Theorem 1 as this makes the normalization term
C independent from G and X (to see this, one can define y' ：= y - G(x) in equation 2.6). The most
1 Note some references (e.g. (Arjovsky et al., 2017)) refer to the first-order Wasserstein distance simply as the
Wasserstein distance. In this paper, we distinguish between different Wasserstein distances explicitly.
4
Under review as a conference paper at ICLR 2019
standard OT GAN loss functions such as the L2 for WGAN (Arjovsky et al., 2017) and the quadratic
loss for W2GAN (Feizi et al., 2017) satisfy this property.
One can further simplify this result by considering specific loss functions. For example, we have the
following result for the entropic GAN with the quadratic loss function.
Corollary 1 Let '(y, y) = Ily - y ∣∣2∕2. Then, fγ∣χ =χ(.) of equation 2.4 CorresPonds to the multi-
1
variate Gaussian density function and C =
(2πλ)d
. In this case, the constant term in equation 2.6
is equal to - log(m) - d log(2πλ)∕2 - r∕2 - log(2π)∕2.
Let G* and P^X be optimal solutions of an entroPic GAN optimization 2.3 (note that the optimal
coupling can be computed efficiently using equation 3.7). Let ytest be a newly observed sample. An
important question is what the likelihood of this sample is given the trained generative model. Using
the explicit probability model of equation 2.4 and the result of Theorem 1, we can (approximately)
compute sample likelihoods as explained in the following corollary.
Corollary 2 Let G* and PYY (or, alternatively P^X) be optimal solutions of the entropic GAN
equation 2.3. Let ytest be a new observed samPle. We have
logfγ(yytes) ≥ -λ {EPX∣Y=ytest ['(ytest, G*(x))] - λH (P*X∣Y =ytest)}	(2.7)
IxI2
+ Ep+----------------+ constants.
PX ∣Y =ytest	2
The inequality becomes tight iff DKL (P*X∣Y=ytest∣∣fX∣Y=ytest) = 0 where DKL(.∣∣.) is the Kullback-
Leibler divergence between two distributions.
3	GAN’s Dual Formulation
In this section, we discuss dual formulations for OT GAN (equation 2.2) and entropic GAN (equa-
tion 2.3) optimizations. These dual formulations are min-max optimizations over two function
classes, namely the generator and the discriminator. Often local search methods such as alternating
gradient descent (GD) are used to compute a solution for these min-max optimizations.
First, we discuss the dual formulation of OT GAN optimization 2.2. Using the duality of the inner
minimization, which is a linear program, we can re-write optimization 2.2 as follows (Villani, 2008):
min max E [D1(Y)] - E [D2(G(X))],	(3.1)
G∈G D1,D2
where Di (y) - D2(y) ≤ '(y, y) for all (y, y). The maximization is over two sets of functions Di
and D2 which are coupled using the loss function. Using the Kantorovich duality Villani (2008), we
can further simplify this optimization as follows:
min max E[D(Y)] - E [D⑻(G(X))],	(3.2)
G∈G D：'-convex
where D(')(Y) ：= infY '(Y,Y) + D(Y) is the '-conjugate function of D(.) and D is restricted
to '-convex functions (Villani, 2008). The above optimization provides a general formulation for
OT GANs. If the loss function is I.I2, then the optimal transport distance is referred to as the first
order Wasserstein distance. In this case, the min-max optimization 3.2 simplifies to the following
optimization (Arjovsky et al., 2017):
min max E[D(Y)]-E[D(G(X))].	(3.3)
G∈G D:1-Lip
This is often referred to as Wasserstein GAN, or WGAN (Arjovsky et al., 2017). If the loss function
is quadratic, then the OT GAN is referred to as the quadratic GAN (or, W2GAN) (Feizi et al., 2017).
Similarly, the dual formulation of the entropic GAN equation 2.3 can be written as the following
optimization (Cuturi, 2013; Seguy et al., 2017) 2:
2Note that optimization 3.4 is dual of optimization 2.3 when the terms λH(Pγ) + λH(PY) have been added
to its objective. Since for a fixed G (fixed marginals), these terms are constants, they can be ignored from the
optimization objective without loss of generality.
5
Under review as a conference paper at ICLR 2019
min max E [Dι(Y)] - E[D2(G(X))] - λEpγ×p^ [exp (v(y, y)∕λ)],	(3.4)
G∈G D1,D2	Y
where
v(y, y) ：= Dι(y) - D2(y) - '(y, y).	(3.5)
Note that the hard constraint of optimization 3.1 is being replaced by a soft constraint in optimization
3.2. In this case, optimal primal variables PY Y can be computed according to the following lemma
(Seguy et al., 2017):	,
Lemma 1 Let D； and Dg be the optimal discriminatorfunctionsfor a given generatorfunction G
according to optimization 3.4. Let
v*(y,y) ：= D；(y) - D；(y) - '(y,y).	(3.6)
Then,
PY,Y (y, y) = Pγ(y)Pγ (y) exp (v* (y, y )∕λ).	(3.7)
This lemma is important for our results since it provides an efficient way to compute the optimal
coupling between real and generative variables (i.e.
PYY) using the optimal generator (G*) and
discriminators (D1； and D2；) of optimization 3.4. It is worth noting that without the entropy reg-
ularization term, computing the optimal coupling using the optimal generator and discriminator
functions is not straightforward in general (unless in some special cases such as W2GAN (Villani,
2008; Feizi et al., 2017)). This is another additional computational benefit of using entropic GAN.
4	Experimental Results
In this section, we supplement our theoretical results with experimental validations. One of the main
objectives of our work is to provide a framework to compute sample likelihoods in GANs. Such
likelihood statistics can then be used in several statistical inference applications that we discuss in
Section 5. With a trained entropic WGAN, the likelihood of a test sample can be lower-bounded
using Corollary 2. Note that this likelihood estimate requires the discriminators D1 and D2 to be
solved to optimality. In our implementation, we use the algorithm presented in Sanjabi et al. (2018)
to train the Entropic GAN. It has been proven (Sanjabi et al., 2018) that this algorithm leads to a
good approximation of stationary solutions of Entropic GAN.
To obtain the surrogate likelihood estimates using Corollary 2, we need to compute the density
PX∣y=ytest (x). As shown in Lemma 1, WGAN with entropy regularization provides a closed-
form solution to the conditional density of the latent variable (equation 3.7). When G； is in-
jective, PXY=ytest (x) can be obtained from equation 3.7 by change of variables. In general case,
PX∣y=ytest (x) is not well defined as multiple X can produce the same ytest. In this case,
PYY=ytest (y) =	∑	PX∣Y=ytest (x).	(4.1)
x∣G* (x)=y
Also, from equation 3.7, we have
PYIY=ytest(y) =	∑	PX(x) exp (v* (ytest,G*(x))∕λ).	(4.2)
x∣G*(x)=y
One solution (which may not be unique) that satisfies both equation 4.1 and 4.2 is
PXY=ytest(X) = PX (X) exp (v* (ytest,G*(x))∕λ).	(4.3)
Ideally, We would like to choose PXY=丫3(x) satisfying equation 4.1 and 4.2 that maximizes the
lower bound of Corollary 2. But finding such a solution can be difficult in general. Instead we use
equation 4.3 to evaluate the surrogate likelihoods of Corollary 2 (note that our results still hold in
this case). In order to compute our proposed surrogate likelihood, we need to draw samples from the
distribution PXY=ytest(x). One approach is to use a Markov chain Monte Carlo (MCMC) method
to sample from this distribution. In our experiments, however, we found that MCMC demonstrates
poor performance owing to the high dimensional nature ofX. A similar issue with MCMC has been
reported for VAEs in Kingma & Welling (2013). Thus, we use a different estimator to compute
the likelihood surrogate which provides a better exploration of the latent space. We present our
sampling procedure in Alg. 1 of Appendix.
6
Under review as a conference paper at ICLR 2019
Figure 2: (a) Distributions of surrogate sample likelihoods at different iterations of entropic
WGAN’s training using MNIST dataset. (b) Distributions of surrogate sample likelihoods of
MNIST, MNIST-1 and SVHM datasets using a GAN trained on MNIST-1.
Surrogate likelihood
(b)
4.1	Likelihood Evolution in GAN’s Training
In the experiments of this section, we study how sample likelihoods vary during GAN’s training.
An entropic WGAN is first trained on MNIST dataset. Then, we randomly choose 1, 000 samples
from MNIST test-set to compute the surrogate likelihoods using Algorithm 1 at different training
iterations. Surrogate likelihood computation requires solving D1 and D2 to optimality for a given
G (refer to Lemma. 2), which might not be satisfied at the intermediate iterations of the training
process. Therefore, before computing the surrogate likelihoods, discriminators D1 and D2 are
updated for 100 steps for a fixed G. We expect sample likelihoods to increase over training iterations
as the quality of the generative model improves.
Fig. 2a demonstrates the evolution of sample likelihood distributions at different training iterations
of the entropic WGAN. At iteration 1, surrogate likelihood values are very low as GAN’s generated
images are merely random noise. The likelihood distribution shifts towards high values during the
training and saturates beyond a point. Details of this experiment are presented in Appendix E.
4.2	Likelihood Comparison Across different datasets
In this section, we perform experiments across different datasets. An entropic WGAN is first trained
on a subset of samples from the MNIST dataset containing digit 1 (which we call the MNIST-1
dataset). With this trained model, likelihood estimates are computed for (1) samples from the entire
MNIST dataset, and (2) samples from the Street View House Numbers (SVHN) dataset (Netzer
et al., 2011) (Fig. 2b). In each experiment, the likelihood estimates are computed for 1000 samples.
We note that highest likelihood estimates are obtained for samples from MNIST-1 dataset, the same
dataset on which the GAN was trained. The likelihood distribution for the MNIST dataset is bimodal
with one mode peaking inline with the MNIST-1 mode. Samples from this mode correspond to digit
1 in the MNIST dataset. The other mode, which is the dominant one, contains the rest of the digits
and has relatively low likelihood estimates. The SVHN dataset, on the other hand, has much smaller
likelihoods as its distribution is significantly different than that of MNIST. Furthermore, we observe
that the likelihood distribution of SVHN samples has a large spread (variance). This is because
samples of the SVHN dataset is more diverse with varying backgrounds and styles than samples
from MNIST. We note that SVHN samples with high likelihood estimates correspond to images
that are similar to MNIST digits, while samples with low scores are different than MNIST samples.
Details of this experiment are presented in Appendix E.
4.3	Approximate Likelihood Computation in Un-regularized GANs
Most standard GAN architectures do not have the entropy regularization. Likelihood lower bounds
of Theorem 1 and Corollary 2 hold even for those GANs as long as we obtain the optimal coupling
PYY in addition to the optimal generator G* from GAN's training. Computation of optimal Cou-
7
Under review as a conference paper at ICLR 2019
(a)	(b)
Figure 3: (a) Sample likelihood estimates of MNIST, Office and CIFAR datasets using a GAN
trained on the CIFAR dataset. (b) Sample likelihood estimates of MNIST, Office and LSUN datasets
using a GAN trained on the LSUN dataset.
Pling PI ^ from the dual formulation of OT GAN can be done When the loss function is quadratic
Y,Y
(Feizi et al., 2017). In this case, the gradient of the oPtimal discriminator Provides the oPtimal
coupling between Y and Y (Villani, 2008) (see Lemma. 2 in Appendix C).
For a general GAN architecture, hoWever, the exact
computation of optimal coupling P]γ may be
difficult. One sensible approximation is to couple Y = ytest with a single latent sample X (we are
assuming the conditional distribution PXIY=ytest is an impulse function). To compute X correspond-
ing to a ytest, we sample k latent samples {x；}k=1 and select the Xi whose G* (Xi) is closest to ytest.
This heuristic takes into account both the likelihood of the latent variable as well as the distance
between ytest and the model (similarly to equation 3.7). We can then use Corollary 2 to approximate
sample likelihoods for various GAN architectures.
We use this approach to compute likelihood estimates for CIFAR-10 (Krizhevsky, 2009) and LSUN-
Bedrooms (Yu et al., 2015) datasets. For CIFAR-10, we train DCGAN while for LSUN, we train
WGAN (details of these experiments can be found in Appendix E). Fig. 3a demonstrates sample
likelihood estimates of different datasets using a GAN trained on CIFAR-10. Likelihoods assigned
to samples from MNIST and Office datasets are lower than that of the CIFAR dataset. Samples
from the Office dataset, however, are assigned to higher likelihood values than MNIST samples.
We note that the Office dataset is indeed more similar to the CIFAR dataset than MNIST. A similar
experiment has been repeated for LSUN-Bedrooms (Yu et al., 2015) dataset. We observe similar
performance trends in this experiment (Fig. 3b).
5	Conclusion
In this paper, we have provided a statistical framework for a family of GANs. Our main result shows
that the entropic GAN optimization can be viewed as maximization of a variational lower-bound on
average log-likelihoods, an approach that VAEs are based upon. This result makes a connection
between two most-popular generative models, namely GANs and VAEs. More importantly, our
result constructs an explicit probability model for GANs that can be used to compute a lower-bound
on sample likelihoods. Our experimental results on various datasets demonstrate that this likelihood
surrogate can be a good approximation of the true likelihood function. Although in this paper we
mainly focus on understanding the behavior of the sample likelihood surrogate in different datasets,
the proposed statistical framework of GANs can be used in various statistical inference applications.
For example, our proposed likelihood surrogate can be used as a quantitative measure to evaluate
the performance of different GAN architectures, it can be used to quantify the domain shifts, it can
be used to select a proper generator class by balancing the bias term vs. variance, it can be used
to detect outlier samples, it can be used in statistical tests such as hypothesis testing, etc. We leave
exploring these directions for future work.
8
Under review as a conference paper at ICLR 2019
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing Systems, pp. 2292-2300, 2013.
Soheil Feizi, Changho Suh, Fei Xia, and David Tse. Understanding GANs: the LQG setting. arXiv
preprint arXiv:1710.10793, 2017.
AUde Genevay, Gabriel Peyre, and Marco Cuturi. Sinkhom-autodiff: Tractable Wasserstein learning
of generative models. arXiv preprint arXiv:1706.00292, 2017.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models With sinkhorn di-
vergences. In Amos Storkey and Fernando Perez-Cruz (eds.), Proceedings of the Twenty-First
International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of
Machine Learning Research, pp. 1608-1617, Playa Blanca, Lanzarote, Canary Islands, 09-11
Apr 2018. PMLR.
Arsham Ghahramani, Fiona M Watt, and Nicholas M Luscombe. Generative adversarial netWorks
uncover epidermal regulators and predict single cell perturbations. bioRxiv, pp. 262501, 2018.
Ian GoodfelloW, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.
Xin Guo, Johnny Hong, Tianyi Lin, and Nan Yang. Relaxed Wasserstein With applications to GANs.
arXiv preprint arXiv:1705.07164, 2017.
Noriaki Hirose, Amir Sadeghian, Patrick Goebel, and Silvio Savarese. To go or not to go? a near
unsupervised learning approach for robot navigation. arXiv preprint arXiv:1709.05439, 2017.
Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P. Xing. On unifying deep genera-
tive models. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=rylSzl- R-.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive groWing of gans for im-
proved quality, stability, and variation. CoRR, abs/1710.10196, 2017. URL http://arxiv.
org/abs/1710.10196.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Hung-yi Lee and Yu Tsao. Generative adversarial netWork and its applications to speech signal and
natural language processing. IEEE International Conference on Acoustics, Speech and Signal
Processing, 2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian GoodfelloW, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
L. Mescheder, S. NoWozin, and A. Geiger. Adversarial variational bayes: Unifying variational
autoencoders and generative adversarial netWorks. In Proceedings of the 34th International Con-
ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research. PMLR,
August 2017a.
9
Under review as a conference paper at ICLR 2019
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722,
2017b.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems,pp. 271-279, 2016.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs.
arXiv preprint arXiv:1709.08894, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987,
2017.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. Solving approximate Wasserstein
GANs to stationarity. Neural Information Processing Systems (NIPS), 2018.
Swami Sankaranarayanan, Yogesh Balaji, Carlos D. Castillo, and Rama Chellappa. Generate to
adapt: Aligning domains using generative adversarial networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Eder Santana and George Hotz. Learning a driving simulator. arXiv preprint arXiv:1608.01230,
2016.
Vivien Seguy, Bharath BhUshan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and
Mathieu Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint
arXiv:1711.02283, 2017.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. arXiv preprint arXiv:1711.01558, 2017.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse. On the quantitative analysis
of decoder-based generative models. CoRR, abs/1611.04273, 2016.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of
a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015.
Appendix A Proof of Theorem 1
Using the Baye’s rule, one can compute the log-likelihood of an observed sample y as follows:
log fY (y) = log fY ∣X=x(y) +logfX(x) -logfX∣Y=y(x)
=log C - '(y,G(x)) - log √2∏ - ɪXL - log fχ∣γ=y(χ),
(A.1)
where the second step follows from equation 2.4.
10
Under review as a conference paper at ICLR 2019
Consider ajoint density function Pχ,γ such that its marginal distributions match PX and PY. Note
that the equation A.1 is true for every x. Thus, we can take the expectation ofboth sides with respect
to a distribution Pχ∣γ=y. This leads to the following equation:
log fY (y) =EPXY=y
= EPXIV=y
.......__ _ _ 1 _	UxU2 _	,,
G(x))∕λ + logc - 2 log 2π 2 log fχ∣γ=y(x)
[-'(y, G(x))∕λ + log C - 1 log 2π - uxL - log fχ∣γ=y(x)
+ log (PX∣Y =y(x)) TOg(PXIY=y(x))
-EPXI Y=y ['(y, G(x))∕λ] - 1 log 2π + log C + EPXY=y
+ kl (PX∣Y=y ∣∣fX∣Y =y ) + H (PXIY=y),
UxU2 一
2
where H(.) is the Shannon-entropy function.
Next we take the expectation of both sides with respect to PY :
E[logfY(Y)] = - vePx,v ['(y,G(x))] - wlog2π + logC + EfX -^~
λ	2	2
+ EPγ [kL (PXIY=y∣∣fX∣Y=y)] + H (PX,Y) - H (PY) ∙
(A.2)
(A.3)
Here, we replaced the expectation over PX with the expectation over fX since one can generate
an arbitrarily large number of samples from the generator. Since the KL divergence is always non-
negative, we have
E [log fY(Y)] ≥ - 1 {Epx,y ['(y, G(x))] - λH (Px,y)} + logC - log(m) - T + log2π (A.4)
λ	2
Moreover, using the data processing inequality, we have H(Pχ,γ) ≥ H(PG(X),γ) (Cover &
Thomas, 2012). Thus,
E [log fγ(Y)] ≥ - 1 {Epx,y ['(y,G(x))] - λH (Pγ,γ)} + log C - log(m) - r + l；g 2” (A.5)
-	—Y- J 、	V	，
sample likelihood	GAN objective with entropy regularizer
This inequality is true for every Pχ,γ satisfying the marginal conditions. Thus, similar to VAEs, we
can pick Pχ,γ to maximize the lower bound on average sample log-likelihoods. This leads to the
entropic GAN optimization 2.3.
Appendix B	Tightness of the bound
In Theorem 1, we showed that the Entropic GAN objective maximizes a lower-bound on the average
sample log-likelihoods. This result is in the same flavor of variational lower bounds used in VAEs,
thus providing a connection between these two areas. One drawback of VAES in general is about the
lack of tightness analysis of the employed variational lower bounds.
In this section, we aim to understand the tightness of the entropic GAN lower bound for
some generative models. Corollary 2 shows that the entropic GAN lower bound is tight when
KL (Pχ∣γ=y∣∣fχ∣γ=y) approaches 0. Quantifying this term can be useful for assessing the qual-
ity of the proposed likelihood surrogate function. We refer to this term as the approximation gap.
Computing the approximation gap can be difficult in general as it requires evaluating fχ∣γ=y. Here
we perform an experiment for linear generative models and a quadratic loss function (same setting
of Corrolary 1). Let the real data Y be generated from the following underlying model
11
Under review as a conference paper at ICLR 2019
Figure 4: A visualization of density functions of PX∣Y=ytest and fX∣Y =ytest for a random two-
dimensional ytest. Both distributions are very similar to one another making the approximation gap
(i.e. KL (PX∣Y=ytest∣∣fX∣Y=ytest)) very small. Our other experimental results presented in Table 1
are consistent with this result.
fYX =X ~ N(Gχ, λI)
where X Z N(0, I)
Using the Bayes rule, we have
fX∣ytest ZN(Rytest,I-RG)
where R = GT (GGT + λI)-1
Since we have a closed-form for fX∣Y, KL (PX∣Y=y∣∣fX∣Y=y) can be computed efficiently.
The matrix G to generate Y is chosen randomly. Then, an entropic GAN with a linear generator
and non-linear discriminators are trained on this dataset. PX∣Y=y is then computed using equa-
tion 4.3. Table 1 reports the average surrogate log-likelihood values and the average approximation
gaps computed over 100 samples drawn from the underlying data distribution. We observe that the
approximation gap is orders of magnitudes smaller than the log-likelihood values.
Additionally, in Figure 4, we demonstrate the density functions of PX∣Y=y and fX∣Y=y for a random
y and a two-dimensional case (r = 2) . In this figure, one can observe that both distributions are very
similar to one another making the approximation gap very small.
Architecture and hyper-parameter details: For the generator network, we used 3 linear layers without
any non-linearities (2 → 128 → 128 → 2). Thus, it is an over-parameterized linear system. The
discriminator architecture (both D1 and D2) is a 2-layer MLP with ReLU non-linearities (2 → 128
→ 128 → 1). λ = 0.1 was used in all the experiments. Both generator and discriminator were trained
using the Adam optimizer with a learning rate 10-6 and momentum 0.5. The discriminators were
trained for 10 steps per generator iteration. Batch size of 512 was used.
Table 1: The tightness of the entropic GAN lower bound. Approximation gaps are orders of magni-
tudes smaller than the surrogate log-likelihood values. Results are averaged over 100 samples drawn
from the underlying data distribution.
Noise Dimension	Approximation Gap	Surrogate Log-Likelihood
2	9.3 X 10-4	-4.15
5	4.7 X 10-2	-15.35
10	6.2 X 10-2	-46.3	—
12
Under review as a conference paper at ICLR 2019
Algorithm 1 Estimating sample likelihoods in GANs
1:	Sample N points Xi i±d Pχ (x)
2:	Compute Ui ：= Pχ(Xi) exp (v* (ytest, G*(xJ)∕λ)
3:	Normalize to get probabilities Pi = ^NUi—
∑i=1 ui
4:	Compute L = - 1 [∑n1 pil(ytest,G*(xi)) + λ ∑n1 Pi log pi] - ∑n1 Pi ^x2^-
5:	Return L
Appendix C	Optimal Coupling for W2GAN
Optimal coupling Pγ, Y for the W2GAN (quadratic GAN (Feizi et al., 2017)) can be computed using
the gradient of the optimal discriminator (Villani, 2008) as follows.
Lemma 2 Let PY be absolutely continuous whose support contained in a convex set in Rd. Let Dopt
be the optimal discriminator for a given generator G in W2GAN. This solution is unique. Moreover,
we have
Y
dist Y - VDopt(Y),
(C.1)
where d=ist means matching distributions.
Appendix D	S inkhorn Loss
In practice, it has been observed that a slightly modified version of the entropic GAN demonstrates
improved computational properties (Genevay et al., 2017; Sanjabi et al., 2018). We explain this
modification in this section. Let
W',λ(Pγ, Pγ) ：= min E['(Y,Y)] + λDκL (Pγ,γ),	(D.1)
where DKL(.∣∣.) is the KullbackLeibler divergence. Note that the objective of this optimization
differs from that of the entropic GAN optimization 2.3 by a constant term λH(PY) + λH(PY). A
sinkhorn distance function is then defined as (Genevay et al., 2017):
W^λ(Pγ, PY) ：= 2W',λ(Pγ, PY) - W',λ(Pγ, Pγ) - W',λ(Pγγ, PY).	(D.2)
W is called the Sinkhorn loss function. Reference Genevay et al. (2017) has shown that as λ → 0,
IV',λ(Pγ, PY) approaches W',λ(Pγ, PY). For a general λ, We have the following upper and lower
bounds:
Lemma 3 For a given λ > 0, we have
W',λ(Pγ, Pγ) ≤ 2W',λ(Pγ, Pγ) ≤ W',λ(Pγ, Pγ) + λH(Pγ) + λH(Pγ).	(D.3)
Proof From the definition equation D.2, we have W',λ(Pγ, PY) ：≥ W',λ(Pγ, Pγ)/2. Moreover,
since W',λ(Pγ, Pγ) ≤ H(Pγ) (this can be seen by using an identity coupling as a feasible so-
lution for optimization D.1) and similarly %,λ(Pγ, PY) ≤ H(PY), we have %,λ(Pγ, PY) ≤
1W',λ(Pγ, Pγ)/2 + λ∕2H(Pγ) + λ∕2H(Pγ). ’	'	■
Since H(Pγ) + H(PY) is constant in our setup, optimizing the GAN with the Sinkhorn loss is
equivalent to optimizing the entropic GAN. So, our likelihood estimation framework can be used
with models trained using Sinkhorn loss as well. This is particularly important from a practical
standpoint as training models with Sinkhorn loss tends to be more stable in practice.
Appendix E	Training Entropic GANs
In this section, we discuss how WGANs with entropic regularization is trained. As discussed in
Section 3, the dual of the entropic GAN formulation can be written as
13
Under review as a conference paper at ICLR 2019
min max E [Dι(Y)] - E[D2(G(X))] - λEpγ×p^ [exp (v(y, y)∕λ)],
G∈G D1,D2	Y
where
v(y, y) ：= Dι(y) - D2(y) - '(y, y).
We can optimize this min-max problem using alternating optimization. A better approach would be
to take into account the smoothness introduced in the problem due to the entropic regularizer, and
solve the generator problem to stationarity using first-order methods. Please refer to Sanjabi et al.
(2018) for more details. In all our experiments, we use Algorithm 1 of Sanjabi et al. (2018) to train
our GAN model.
E.1 GAN’S TRAINING ON MNIST
MNIST dataset constains 28 ×28 grayscale images. As a pre-processing step, all images were resized
in the range [0, 1]. The Discriminator and the Generator architectures used in our experiments are
given in Tables. 2,3. Note that the dual formulation of GANs employ two discriminators - D1 and
D2, and we use the same architecture for both. The hyperparameter details are given in Table 4.
Some sample generations are shown in Fig. 5
E.2 GAN’S TRAINING ON CIFAR
We trained a DCGAN model on CIFAR dataset using the discriminator and generator architecture
used in Radford et al. (2015). The hyperparamer details are mentioned in Table. 5. Some sample
generations are provided in Figure 7
Table 2: Generator architecture
Layer	Output Size	FilterS
Input	128	-
Fully connected	4.4.256	128 → 256
Reshape	256 X 4 X 4	-
BatchNorm+ReLU	256 X 4X 4	-
Deconv2d (5 × 5, Str 2)	128 X 8 X 8	256 → 128
BatchNorm+ReLU	128X8X8	-
Remove border row and CoL	128X7X7	-
Deconv2d (5 X 5, Str 2)	64X 14X 14	128 → 64
BatchNorm+ReLU	128X8X8	-
Deconv2d (5 X 5, Str 2)	1X28X28	64 → 1
	Sigmoid	1X28X28	-
Table 3: Discriminator architecture
Layer	Output size	FilterS
Input	1 X 28 X 28=	-
Conv2D(5 X 5, Str 2)	32 X 14 X 14	1 → 32
LeakyReLU(0.2)	32 X 14 X 14	-
Conv2D(5 X 5, Str 2)	64 X 7 X 7	32 → 64
LeakyReLU(0.2)	64 X 7 X 7	-
Conv2d (5 X 5, Str 2)	128 X 4 X 4	64 → 128
LeakyRelU(0.2)	128X4X4	-
ReShape	128.4.4	-
Fully connected	1	一	2048 → 1
14
Under review as a conference paper at ICLR 2019
5623，夕 7√
∕3S5∕J∕7
0o77√3i∖v
J3∕zr34/0
1夕J 9Q∕夕夕
33z63^^y
OOG 7/q 2
q / /ac23 /
7331596 T
6qygq 夕 0,1
71∕Q417HΩ/
6∕y077/〃
KG / 7 13/^ I
J3G>6CF47
IjF 4356
Figure 5: Samples generated by Entropic GAN trained on MNIST
Figure 6: Samples generated by Entropic GAN trained on MNIST-1 dataset
E.3 GAN’s Training on LSUN-Bedrooms dataset
We trained a WGAN model on LSUN-Bedrooms dataset with DCGAN architectures for generator
and discriminator networks (Arjovsky et al., 2017). The hyperparameter details are given in Table. 6,
and some sample generations are provided in Fig. 8
Table 4: Hyper-parameter details for MNIST experiment
Parameter	Config
λ	5
Generator learning rate	0.0002
Discriminator learning rate	0.0002
Batch size	100
Optimizer	Adam
Optimizer params	β1 = 0.5, β2 = 0.9
Number of critic iters / gen iter	5
Number of training iterations	10000
Table 5: Hyper-parameter details for CIFAR-10 experiment
Parameter
Generator learning rate
Discriminator learning rate
Batch size
Optimizer
Optimizer params
Number of training epochs
Config
0.0002
0.0002
64
Adam
β1 = 0.5, β2 = 0.99
100
15
Under review as a conference paper at ICLR 2019
Figure 8: Samples generated by WGAN model trained on LSUN-Bedrooms dataset
16
Under review as a conference paper at ICLR 2019
Table 6: Hyper-parameter details for LSUN-Bedrooms experiment
Parameter	Config
Generator learning rate	0.00005
Discriminator learning rate	0.00005
Clipping parameter c	0.01
Number of critic iters per gen iter	5
Batch size	64
Optimizer	RMSProp
Number of training iterations	70000
17