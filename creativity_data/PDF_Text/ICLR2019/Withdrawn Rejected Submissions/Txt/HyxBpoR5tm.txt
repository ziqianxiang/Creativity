Under review as a conference paper at ICLR 2019
Adversarially Robust Training through
Structured Gradient Regularization
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel data-dependent structured gradient regularizer to increase the
robustness of neural networks vis-a-vis adversarial perturbations. Our regularizer
can be derived as a controlled approximation from first principles, leveraging the
fundamental link between training with noise and regularization. It adds very little
computational overhead during learning and is simple to implement generically
in standard deep learning frameworks. Our experiments provide strong evidence
that structured gradient regularization can act as an effective first line of defense
against attacks based on long-range correlated signal corruptions.1
1	Introduction
Deep neural networks, in particular convolutional neural networks (CNNs), have been used with great
success for perceptual tasks such as image classification (Simonyan & Zisserman, 2014) or speech
recognition (Hinton et al., 2012). However, it has been shown that the accuracy of models obtained by
standard training methods can dramatically deteriorate in the face of so-called adversarial examples
(Szegedy et al., 2013; Goodfellow et al., 2014), i.e. small perturbations in the input signal, typically
imperceptible to humans, that are sufficient to induce large changes in the output. This apparent
vulnerability is worrisome as CNNs start to proliferate in the real-world, including in safety-critical
deployments.
Although the theoretical aspects of vulnerability to adversarial perturbations are not yet fully under-
stood, a plethora of methods has been proposed to find adversarial examples. These often transfer or
generalize across different architectures and datasets (PaPernot et al., 2016a; Liu et al., 2016; Tramer
et al., 2017), enabling black-box attacks even for inaccessible models. The most direct and commonly
used strategy of Protection is to use adversarial examPles during training, see e.g. (Goodfellow
et al., 2014) or (Miyato et al., 2015). This raises the question, of whether one can generalize across
such examPles in order to become immune to a wider range of Possible adversarial Perturbations.
Otherwise there aPPears to be a danger of overfitting to a sPecific attack, in Particular if adversarial
examPles are sParsely generated from the training set.
In this PaPer, we Pursue a different route, starting from the hyPothesis that the weakness revealed
by adversarial examPles first and foremost hints at a data scarcity Problem. InsPired by the classical
work of BishoP (1995) and similar in sPirit to Miyato et al. (2017), we thus ProPose regularization
as the Preferred remedy to increase model robustness. Our regularization aPProach comes without
rigid a priori assumPtions on the model structure and filters (Bruna & Mallat, 2013) as well as strong
(and often overly drastic) requirements such as layer-wise LiPschitz bounds (Cisse et al., 2017) or
isotroPic smoothness (Miyato et al., 2017). Instead, we will rely on the ease of generating adversarial
examPles to learn an informative regularizer, focusing on the correlation structure of the adversarial
noise. Our regularizer consistently outPerforms structure-agnostic baselines on long-range correlated
Perturbations - achieving over 25% higher classification accuracy. We thus consider it an effective
first line of defense on toP of which other defense mechanisms may be built.
In summary, we make the following contributions:
•	We Provide evidence that current adversarial attacks act by Perturbing the short-range
covariance structure of signals.
1Code will be made available
1
Under review as a conference paper at ICLR 2019
•	We propose Structured Gradient Regularization (SGR), a data-dependent regularizer in-
formed by the covariance structure of adversarial perturbations.
•	We present an efficient SGR implementation with low memory footprint.
•	We introduce a long-range correlated adversarial attack against which SGR-regularized
classifiers are shown to be robust.
2	Adversarial Robust Learning
2.1	Virtual Examples
Imagine we had access to a set of transformations τ ∈ T of inputs that leave outputs invariant. Then
we could augment our training data by expanding each real example (xi , yi) into virtual examples
(τ(xi), yi). This approach is simple and widely-applicable, e.g. for exploiting known data invariances
(SchGlkoPf et al., 1996), and can greatly increase the statistical power of learning. A similar stochastic
method has been suggested by Maaten et al. (2013): a noisy channel or perturbation Q corrupts
x → X with probability Q(x|x), leading to an altered (more precisely: convolved) joint distribution
Pq(X, y) = P(y) R P(χ∣y)Q(X∣χ)dχ.
Where could such a corruption model come from? We propose to use a generative mechanism to
learn adversarial corruptions from examples. In fact, we will use a simple additive noise model in
this paper,
Q(X∣χ) = Q(ξ：=X -χ), E[ξξ>] = ∑.	(1)
Our focus will be on leveraging the correlation structure of the noise, captured by the matrix of
raw second moments Σ, aggregated as a running exponentially weighted batch estimate computed
from adversarial perturbations ξ, obtained by methods such as FGSM (Goodfellow et al., 2014),
PGD (Madry et al., 2017) or DeepFool (Moosavi Dezfooli et al., 2016).
2.2	Robust Multiclass Learning Objective
Once we have an estimate of Q available, we propose to optimize any standard training criterion
(e.g. a likelihood function) in expectation over a mixing distribution between the uncorrupted
and the corrupted data. For the sake of concreteness, let us focus on the multiclass case, where
y ∈ {1, . . . , K}. Define a probabilistic classifier
φ : Rd → 4K,	4K := {π ∈ R+K : 1>π = 1} .	(2)
For a given loss function ' such as the negative log-loss, '(y, π) = - log πy, our aim is to minimize
L(φ) = (1 -λ)Ep ['(y, φ(χ))]+λEpQ ['(y, φ(χ))],	⑶
where P denotes the empirical distribution and PQ the noise corrupted distribution. The hyper-
parameter λ gives us a degree of freedom to pay more attention to the training data vs. our (imperfect)
corruption model.
2.3	From Virtual Examples to Regularization
Approaches relying on the explicit generation of adversarial examples face three statistical challenges:
(i) How can enough diverse samples be generated to cover all attacks? (ii) How can we avoid a
computational blow-up by adding too many virtual examples? (iii) How can we prevent overfitting to
a specific attack? A remedy to these problems is through the use of regularization. The basic idea is
simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in
closed form, at least under reasonable approximations.
The general connection between regularization and training with noise-corrupted examples has been
firmly established by Bishop (1995). The benefits of regularization over adversarially augmented
training in terms of generalization error have been hypothesized by Galloway et al. (2018). In (Maaten
et al., 2013), properties of the loss function are exploited to achieve efficient regularization techniques
for certain types of noise. Here, we follow the approach pursued in (Roth et al., 2017) in using an
approximation that is accurate for small noise amplitudes.
2
Under review as a conference paper at ICLR 2019
3	Structured Gradient Regularization
3.1	From Correlated Noise to Structured Gradient Regularization
To approximate the expectation with regard to the noisy channel, we generalize a recent approach for
gradient regularization of Generative Adversarial Networks (GANs) (Roth et al., 2017), from binary
classification to multiclass classification and from white noise to (adversarially) structured noise.
For the sake of simplicity of derivations we assume adversarial perturbations to be centered. Let us
Taylor expand the log-component functions, making use of the shortcut ψy := log φy,
ψy (∙ + ξ) = ψy + Vψ> ξ + 2 ξ> 4[ψy ]ξ + O(kξ∣∣3)	(4)
For any zero-mean distribution Q with second moments Σ,
EQ[Ψy (∙ + ξ)] = ψy +1 Tr(4[ψy ]Σ) + O(E[kξk3]).	(5)
The Hessian is calculated via the chain rule,
4(ψy) = V [vφy] = 4φφy - vψy ∙ vψ> .	⑹
We will make a similar argument to (Roth et al., 2017) to show that it is reasonable to neglect the
terms involving Hessians 4φy. Let Us therefore consider the Bayes-optimal classifier φy, for which
up to a normalization constant
φy(χ) B P(χ∣y)P(y),	(7)
sUch that
EP [4φXχ)卜ZX4%(χ)dχ.	(8)
Exchanging sUmmation and differentiation, we however have as a conseqUence of the normalization
X4φy(χ)=4 Xφy(χ) =41=0.	(9)
yy
Thus, under the assumption that φ ≈ φ* and of small perturbations (such that we can ignore higher
order terms in Eq. (4)), we get
Epq [ψy(x)] = EP [ψy(x) - 2Tr [Vψy Vψ>∑]] + O(E[∣∣ξk3]).	(10)
We can turn this into a regularizer by taking the leading terms (Roth et al., 2017). Identifying P = P
with the empirical distribution, we arrive at the following robust learning objective:
Structured Gradient Regularization (SGR)	
minφ → LSGR(φ) = Ep [- logφy(x)] + λΩ∑(φ)	(11)
Ω∑(φ):= 1EP [Vx log φy(x)> Σ Vχlogφy(x)i	
For uncentered adversarial perturbations EQ [ξ] = μ it is easy to see that the corresponding regularizer
is given by
Ωμ∑ (φ) = -EP V log φ>(x) μ +Ω∑(Φ).
(12)
In practice, we however observed this correction to be small.
3
Under review as a conference paper at ICLR 2019
Algorithm 1 Adversarial SGR. Default values: λ ∈ [0, 1], β = 0.1
Input: Regularization strength λ, exponential decay rate β, batch size m
Initial classifier parameters (weights and biases) ω0
while ωt not converged do
Sample minibatch of data {(x(1),y⑴)，..., (x(m),y(m))} ~ P.
Compute (normalized) adversarial perturbations ξ(1), ..., ξ(m)
Compute covariance matrix or covariance function Cov(ξ(1), ..., ξ(m))
Update running average Σt — (1 -β) Σt-ι + β Cov(ξ⑴,…,ξ(m))
mK
L(ω) = m X [X-Jk* i (ii) (iii) *logφk(X(i)；ω)]
i=1 k=1
mK
Ω∑t (ω) = 2m X ]χ y(i Vx log φk (x(i); ω)> Σt Vx log φk(x(ii; ω)
ωt - ωt-i — Vω (L(ω) + λ Ω∑t (ω) ʌ I
t	ωt-1
return ωt
The gradient-descent step can be performed with any learning rule. We used Adam in our experiments.
See Sec. 3.3 for an efficient implementation of the regularizer.
3.2 Properties
There are a few facts that are important to point out about the derived SGR regularizer.
(i) As the regularizer provides an efficiently computable approximation for an intractable expectation,
it is clearly data-dependent. Ω∑ penalizes loss-gradients evaluated at training points. This is different
from standard regularizers that penalize some norm of the parameter vector, such as L2 -regularization.
In the latter case, we would expect the regularizer to have the largest effect in the empty parts of the
input space, where it should reduce the variability of the classifier outputs. On the contrary, Ω∑ has its
main effect around the data manifold. In this sense, it is complementary to parameter regularization.
(ii) The SGR regularizer is intrinsic in the sense that it does not depend on the parameters of the
classifier (for a good reason, we have not mentioned any parameterization), but instead directly acts
on the function realized by the classifier φ (the relevant gradients measure the sensitivity of φ with
regard to the input x and not with regard to parameters). Thus, it is parameterization invariant and
can be naturally applied to any function space, whether it is finite-dimensional or not.
(iii) We can gain complementary insights into SGR by explicitly computing it in terms of classifier
logits ψy(x). As outlined in Sec 7.1 in the Appendix, We obtain the following expression for the
structured gradient regularizer
ω∑2) = 2 EP h(V3y — hV0)>£(Vgy- hv0)] , hV0 (X)= X V^y (X)φy (X)	(13)
y
We can therefore see that SGR is penalizing large variations of the class-conditional logit-gradients
Vgy around their data-dependent average hVgi. For simple one-layer softmax classifiers we obtain
Ω∑(φ) = 1/2 EP [(ωy - hωi)> Σ (ωy - <ω>)]. This suggests an intriguing connection to variance-
based regularization. Weight-decay regularization, on the other hand, simply penalizes large norms.
3.3 Implementation
The implementation of SGR training is outlined in Algorithm 1. The matrix of raw second moments
is aggregated through a running exponentially weighted average
∑t 一 (1-β) ∑t-1 + β Cov(ξ⑴，…,ξ(mi)	(14)
4
Under review as a conference paper at ICLR 2019
with the decay rate β as a tuneable parameter trading off weighting between current (β→ 1) and past
(β → 0) batch averages. For low-dimensional data sets, we can directly compute the full matrix
m
Cov(ξ⑴,…,ξ(m)) = ɪ X荣］.
m
i=1
(15)
For high-dimensional data sets, we have to resort to more memory-efficient representations leveraging
the sparseness of the covariance matrix and covariance-gradient matrix-vector product. For image
data sets, the covariance matrix can be estimated as a function of the displacement between pixels, as
illustrated in Fig. 2,
Cov(ξ(1), ...,ξ(m))
ij
' CovFun (||i -j||2)
(16)
where i = (ir , ic ) denotes the 2D pixel location and i is the covariance matrix index obtained by
flattening the 2D pixel location into a 1D array such that ir and ic denote the row and column numbers
of pixel i, and similar for j respectively. Rounding pixel displacements ||i - j||2 to the nearest integer,
the covariance function simply becomes an array of real numbers storing the current estimate of the
covariance between two pixels displaced by the difference in array indices.
In our experiments, we normalize the perturbations such that the average diagonal entry of the
covariance matrix is one, thus allowing for more intuitive optimization over the regularization
strength parameter λ. We note also that it is not necessary to update Σ at every training step and it is
even possible to train with a fixed “expert”-designed covariance matrix.
The other crucial quantities needed to evaluate the regularizer are logφk(∙) for training points
(x(i), y(i)). This is simply the per-sample cross-entropy loss, which is often available as a highly
optimized callable operation in modern deep learning frameworks, reducing the implementation of
the SGR regularizer to merely a few lines of code at very little computational overhead.
4	Experiments
4.1	Experimental Setup
Classifier architectures & data preprocessing. We trained Convolutional Neural Networks (CNNs)
with seven hidden layers (and different numbers of filters for each data set), on CIFAR10 (Krizhevsky
& Hinton, 2009) and MNIST (LeCun, 1998). Our models are identical to those used in (Carlini
& Wagner, 2017; Papernot et al., 2016b). For both data sets, we adopt the following standard
preprocessing and data augmentation scheme (He et al., 2016): each training image is zero-padded
with four pixels on each side2, randomly cropped to produce a new image with the original dimensions
and horizontally flipped with probability one half3 . We also standardize each image to have zero
mean and unit variance when passing it to the classifier. Further details can be found in the Appendix.
Training methods. We train each classifier with a number of different training methods: (i) clean, i.e.
through the standard softmax cross-entropy objective, (ii) clean + L2 weight decay regularization,
(iii) adversarially augmented training, i.e. training the classifier on a mixture of clean and adversarial
examples (iv) GN (gradient-norm) regularized (corresponding to SGR with identity covariance matrix)
and (v) SGR regularized. The SGR covariance matrix was computed from either: (a) loss gradients,
(b) sign of loss gradients (corresponding to FGSM) or (c) PGD perturbations. The performance of
these SGR variants was identical in all our experiments, hence we collectively report them as SGR.
Adversarial attacks. We evaluate the classification accuracy against several adversarial attacks: Fast
Gradient Sign Method (FGSM) (Goodfellow et al., 2014), Projected Gradient Descent (PGD) (Madry
et al., 2017) and DeepFool (Moosavi Dezfooli et al., 2016). The attack strength is reported in
units of 1/255. The SNR reported for the DeepFool attack is computed according to Eq. (2) in
(Moosavi Dezfooli et al., 2016). The attacks were implemented with the open source CleverHans
Library (Papernot et al., 2017). Further details and attack hyper-parameters can be found in Sec. 7.2
in the Appendix.________________
2We shifted the CIFAR10 data by -0.5 so that padding adds mid rgb-range pixels.
3We do not flip MNIST digits.
5
Under review as a conference paper at ICLR 2019
Figure 1: CoVariance matrices of PGD, FGSM and DeePFool perturbations as Wen as CIFAR10
training set (for comparison). The short-range structure of the perturbations is clearly visible. It is
also apparent that the first two attack methods yield perturbations with almost identical covariance
structure. We are showing a center-crop for better visibility (25% trimmed on each side). For
comparison the matrices were rescaled such that their maximum element has an absolute value of one.
Figure 2: PGD covariance functions: (Left) intra-channel covariance functions, measuring correla-
tions between identical color channels, (the coloring in the plot matches the corresponding channel),
(right) inter-channel covariance functions, measuring correlations between distinct color channels.
The rapid decay with pixel displacement, indicating very short decay length, is clearly visible.
4.2	Covariance Structure of Adversarial Perturbations
To investigate the covariance structure of adversarial perturbations, we trained a clean classifier for 50
epochs and computed adversarial perturbations for every data point in the test set. The perturbations
were flattened into 1D arrays before computing the full covariance matrix4 as in Eq. (15).
The short-range correlation structure of the perturbations, shown in Fig. 1, is clearly visible: the
correlations decay much faster than those of the data set covariance matrix. Figure 2 shows the
corresponding PGD covariance functions. The rapid decay with pixel displacement, indicating very
short decay length5 , is again evident. It thus seems that an unregularized classifier vulnerable to
adversarial perturbations gives too much weight to short-range correlations (low-level patterns) and
not enough weight to long-range ones (globally relevant high-level features of the data).
4.3	Long-range Correlated Noise Attack
To investigate the effect and potential benefit of using a structured covariance matrix in the SGR regu-
larizer versus an “unstructured” diagonal covariance, corresponding to gradient-norm regularization,
we compute the classification accuracy against a worst-of-100 attack in which the adversary samples
perturbations from a long-range correlated multivariate Gaussian with covariance matrix specified
through an exponentially decaying covariance function parametrized by a variable decay length ζ .
Inspired by the PGD covariance function, we chose intra-channel CovFun f (r) = exp(-r∕Z) and
inter-channel CovFun f (r) = 0.5exp(-r/Z). The corresponding covariance matrices are depicted
in Fig. 5 in the Appendix. LRC samples are shown in Fig. 3.
The worst-of-100 attack then consists in perturbing every test set data point 100 times and evaluating
the classifier accuracy against the worst of these perturbations. We tested for several decay lengths in
the range ζ ∈ [1, 2, 4, 8, 16] and attack strengths ∈ [0.01, 0.7] with which the perturbations were
scaled. Figure 4 in the Appendix shows the accuracy of different models as a function of . As
a baseline, we also trained a model on LRC-augmented input. For SGR and GN we performed a
hyper-parameter search over λ (at fixed ζ = 8) and report results for the best performing model.
4
4 Strictly speaking, Σ denotes the matrix of raw second moments. We will refer to Σ as the covariance matrix
however, since the difference between the two is negligible.
5Decay length is defined as the displacement over which the covariance function decays to 1/e of its value.
6
Under review as a conference paper at ICLR 2019
CL
frog
Figure 3: (Left) Accuracy of different models as a function of the LRC attack decay length ζ ∈ [1, 16]
at fixed = 0.3 (averaged over five runs). The plot clearly demonstrates the superiority of using a
structured covariance matrix in the SGR regularizer. (Right) LRC perturbed samples with increasing
decay lengths ζ ∈ [1, 2, 4, 8, 12] (two examples per decay length). The top row shows original test
set images, the middle row shows adversarially perturbed samples, while the bottom row shows the
adversarial perturbations rescaled to fit the full range of rgb values (their actual range and hence
saliency is smaller). The footer indicates the classifier predictions on adversarial input.
deer deer frog frog deer cat truck
The LRC attack is a natural prototype for low frequency perturbations, as opposed to existing
attacks which we have shown to mainly corrupt the short range (high frequency) structure of signals.
As shown in Fig. 3 (left), the low frequency range is precisely the regime where the benefit of
structured regularization comes into play: SGR clearly and consistently outperforms GN on long-
range correlated perturbations, achieving over 25% higher classification accuracy on LRC-perturbed
input With Z 〜16. As the decay length goes to zero, the synthetic covariance matrix converges to the
identity matrix and SGR performance approaches GN performance.
Finally, to verify that those effects are not a result of our synthetically chosen covariance function, We
also performed the same experiments With the covariance matrix computed from the CIFAR10 training
set, shoWn in Fig. 1. The accuracies for = 0.3 are: SGR 65.8%, GN 48.7%, LRC noise 57.9% and
clean 46.5%, again confirming the benefit of using a structured covariance in the regularizer.
4.4	White-box & Transfer Attack Accuracy
A summary of the White-box and transfer attack accuracies can be found in Table 1. We train all
models to achieve the highest possible White-box / transfer attack accuracies, averaged over in the
range [0, 32] for the MNIST and [0, 8] for the CIFAR10 test set, i.e. the best performing models Were
selected according to the integrated area under the attack curve. The corresponding hyper-parameters
are listed in Sec. 7.2 in the Appendix.
On MNIST, SGR/GN achieve PGD White-box accuracies of 96.5% compared to the 78.3% accuracy
achieved by the clean model. On CIFAR10, SGR/GN trained models achieve an accuracy of 53.4%
against targeted T-PGD attack (target is selected uniformly at random), Whereas the clean model
achieves 24.9% and PGD-trained model achieves 69.7% accuracy. SGR/GN trained models thus
achieve White-box accuracies that are intermediate betWeen those of the clean and adversarially
trained models. The higher White-box accuracies achieved by adversarially trained models, hoWever
comes at the expense of a drop in test set accuracies. On CIFAR10, the test set accuracy of the
PDG-trained model drops to 77.7%, While SGR/GN achieve 83.4% and 83.7% respectively.
An interesting observation can be made from the transfer attack accuracies (bold-face entries) on
CIFAR10. As can be seen, the PGD adversarial perturbations obtained from SGR/GN trained
models hurt the clean model more (60.3%) than the PGD perturbations obtained from PGD/FGSM
adversarially trained models (77.0%/81.9%). Moreover, SGR achieves significantly higher accuracies
against PGD transfer attacks derived from the clean model (80.6%) than the clean model does on
PGD perturbations obtained from the SGR-trained model (60.3%), While accuracies achieved by
SGR models on PDG samples from PGD-trained models (75.8%) are comparable to those in the
other direction (74.0%).
As expected, the difference betWeen SGR and GN in terms of White-box and transfer attack accuracies
is less significant in this experiment, Which can be explained by the evident short-range nature of
current adversarial attacks (see discussion in Sec. 4.2). Reassuringly, We did not observe any loss of
7
Under review as a conference paper at ICLR 2019
MNIST = 32
Train. M.	TEST	RAND	PGD	t-pgd	CLEAN	PGD	FGSM	GN	SGR	FOOL
CLEAN	99.4	99.3	78.3	74.1	98.3	98.9	98.9	97.9	98.3	0.144
WDECAY	98.8	98.7	92.5	95.0	98.4	98.4	98.4	96.9	97.6	0.229
PGD	99.5	99.5	99.1	99.3	99.4	99.3	99.4	99.3	99.3	0.453
FGSM	99.4	99.4	98.5	98.0	99.3	99.2	99.2	99.2	99.2	0.339
GN	99.5	99.5	96.5	97.6	99.3	99.2	99.2	98.0	98.4	0.309
SGR	99.6	99.5	96.6	96.5	99.4	99.3	99.4	98.5	98.7	0.303
				CIFAR10 =		8				
Train. M.	TEST	RAND	PGD	t-pgd	CLEAN	PGD	FGSM	GN	SGR	FOOL
CLEAN	84.7	83.2	28.8	24.9	72.2	77.0	81.9	61.9	60.3	0.010
WDECAY	81.4	79.2	27.1	32.3	69.7	71.7	68.9	54.1	52.9	0.015
PGD	77.7	77.7	62.4	69.7	77.1	71.5	76.2	74.0	74.0	0.061
FGSM	81.9	82.0	55.2	66.4	81.1	75.0	78.0	76.5	76.4	0.046
GN	83.7	83.7	41.9	54.1	81.6	76.9	82.0	68.1	67.9	0.039
SGR	83.4	82.8	41.5	53.4	80.6	75.8	81.2	66.8	65.5	0.039
Table 1: Test set, white-box and PGD-transfer attack accuracies. Each row corresponds to a model
trained with a different training method, evaluated against the respective white-box (non-bold entries)
and PGD-transfer attacks (bold-face entries) indicated above each column (T-PGD refers to targeted
attack). The white-box and transfer attack accuracies are averaged over attack strengths in the range
∈ [0, 32] for MNIST and ∈ [0, 8] for CIFAR10, i.e. the reported accuracies represent the integrated
area under the attack curve. For the transfer attack accuracies of a training method against itself, we
trained a separate model with the same hyper-parameters to obtain the perturbations.
performance in all our experiments when using the covariance function instead of the full covariance
matrix in the SGR regularizer.
5	Related Work
A large body of related work on distributionally robust optimization (DRO) (Sinha et al., 2017;
Gao & Kleywegt, 2016) seeks a classifier that minimizes the worst-case loss against an adversary
that can shift the entire training set within an uncertainty ball around the empirical distribution.
Adversarial training can be viewed as a special case of DRO against a pointwise adversary that
independently perturbs each example. While these methods aim at identifying the worst-case
distribution, we propose to efficiently approximate expectations over corrupted distributions through
structure-informed regularization.
Coincidentally, Ross & Doshi-Velez (2017) and Simon-Gabriel et al. (2018) also suggested to use
gradient-norm regularization to improve the robustness against adversarial attacks. While (Simon-
Gabriel et al., 2018) investigated the effect of different gradient-norms induced by constraints on the
magnitude of the perturbations, we focused on a structured generalization of gradient regularization.
Similar in spirit to our work is also the recent work on adversarial training vs. weight decay regular-
ization (Galloway et al., 2018). While weight decay acts on the parameters, our regularizer acts on the
function realized by the classifier. As discussed in Sec. 3.2, the two approaches are complementary to
each other and can also be combined.
From a Bayesian perspective, regularization can be understood as imposing a prior distribution over
the model parameters. As parameters in neural networks are non-identifiable, we ultimately prefer to
impose priors in the space of functions directly (Teh, 2017; Neal, 2012; 1996). As pointed out in
Sec. 3.2, this is indeed one of the key properties of our regularizer.
6	Conclusion
The fact that adversarial perturbations can fool classifiers while being imperceptible to the human
eye, hints at a fundamental mismatch between how humans and classifiers try to make sense of the
data they see. We provided evidence that current adversarial attacks act by perturbing the short-range
correlations of signals. We proposed a novel structured gradient regularizer (SGR) informed by
the covariance structure of adversarial noise and presented an efficient SGR implementation with
low memory footprint. Devising further (e.g. gradient-based) low frequency attacks is an interesting
direction of future research.
8
Under review as a conference paper at ICLR 2019
References
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation, 7:
108-116,1995.
Joan BrUna and StePhane Mallat. Invariant scattering convolution networks. IEEE transactions on
pattern analysis and machine intelligence, 35(8):1872-1886, 2013.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Security and Privacy (SP), 2017 IEEE Symposium on, PP. 39-57. IEEE, 2017.
MoustaPha Cisse, Piotr Bojanowski, Edouard Grave, Yann DauPhin, and Nicolas Usunier. Parseval
networks: ImProving robustness to adversarial examPles. In International Conference on Machine
Learning, PP. 854-863, 2017.
Angus Galloway, Thomas Tanay, and Graham W Taylor. Adversarial training versus weight decay.
arXiv preprint arXiv:1804.03308, 2018.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic oPtimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. ExPlaining and harnessing adversarial
examPles. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image
recognition. In In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), PP.
770-778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, NavdeeP Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. DeeP neural networks
for acoustic modeling in sPeech recognition: The shared views of four research grouPs. IEEE
Signal Processing Magazine, 29(6):82-97, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiPle layers of features from tiny images.
UToronto, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
YanPei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examPles
and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.
Laurens Maaten, Minmin Chen, StePhen Tyree, and Kilian Weinberger. Learning with marginalized
corruPted features. In International Conference on Machine Learning, PP. 410-418, 2013.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris TsiPras, and Adrian Vladu.
Towards deeP learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a reg-
ularization method for suPervised and semi-suPervised learning. arXiv preprint arXiv:1704.03976,
2017.
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeePfool: a simPle and
accurate method to fool deeP neural networks. In Proceedings of 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016.
9
Under review as a conference paper at ICLR 2019
Radford M Neal. Priors for infinite networks. In Bayesian Learningfor Neural Networks, pp. 29-53.
Springer, 1996.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP),
2016 IEEE Symposium on, pp. 582-597. IEEE, 2016b.
Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri, Alexander
Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan Sheatsley, Abhibhav
Garg, and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine learning library. arXiv preprint
arXiv:1610.00768, 2017.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability
of deep neural networks by regularizing their input gradients. arXiv preprint arXiv:1711.09404,
2017.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training
of generative adversarial networks through regularization. In Advances in Neural Information
Processing Systems 30, pp. 2015-2025, 2017.
Bernhard Scholkopf, Chris Burges, and Vladimir Vapnik. Incorporating invariances in support vector
learning machines. In International Conference on Artificial Neural Networks, pp. 47-52. Springer,
1996.
Carl-Johann Simon-Gabriel, Yann Ollivier, Bernhard Scholkopf, L6on Bottou, and David Lopez-Paz.
Adversarial vulnerability of neural networks increases with input dimension. arXiv preprint
arXiv:1802.01421, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2014.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Yee Whye Teh. On bayesian deep learning and deep bayesian learning (breiman lecture nips17).
http://csml.stats.ox.ac.uk/news/2017-12-08-ywteh-breiman-lecture/, 2017.
Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of
transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017.
10
Under review as a conference paper at ICLR 2019
7 Appendix
7.1 SGR properties
(iii) We can gain complementary insights into SGR by explicitly computing it in terms of logits
φy(x)
exPWy(X))
Py exP 停y(X))
Defining the class average of the logit-gradients as
"0 (X) := EV% (x)φy (X)
y
we obtain the following expression for the structured gradient regularizer
(17)
(18)
Ω∑(φ) = 2 EP (V^y -hV0)>Σ(V^y -(V0)
(19)
We can therefore see that SGR is penalizing large fluctuations of the class-conditional logit-gradients
V夕y around their data-dependent class averagehV夕).For simple one-layer Softmax classifiers
ψy (x) = ω>x + by, we obtain
ω∑2) = 2ep (ωy - hωi)> ς (ωy - hωi)
(20)
This suggests an intriguing connection to variance-based regularization. Weight-decay regularization,
on the other hand, simply penalizes large norms.
7.2 Hyper-parameters
Attack Hyper-parameters
FGSM	: in units of 1/255
PGD	: in units of 1/255 iter = /5 nb_iter = 10 (wb) , 40 (t)
DEEP FOOL	overshoot : 0.02 max_iter= 100
Table 2: Attack hyper-parameters. wb stands for white-box, t for transfer attacks.
An overview of the attack hyper-parameters can be found in Table 2. Untargeted attacks find
adversarial perturbations to an arbitrary one of the K - 1 other classes. Both PGD and DeepFool
are iterative attacks: they use 10 / 40 (for the white-box / transfer attack tables) and 100 iterations
respectively. The numbers reported for the DeepFool attack are computed according to Eq. (2)
in (Moosavi Dezfooli et al., 2016). The adversarial example construction processes use the most
likely label predicted by the classifier in order to avoid label leaking (Kurakin et al., 2016) during
adversarially augmented training. The perturbed images returned by the attacks were clipped to lie
within the original rgb range, as is commonly done in practice. The attacks were implemented with
the open source CleverHans Library (Papernot et al., 2017).
For SGR/GN we performed a hyper-parameter search over the regularization strength λ and SGR
decay parameter β. For adversarial training, we trained models with in the full range ∈ [0, 32]
for MNIST and ∈ [0, 8] for CIFAR10 and report results for the best performing one. The hyper-
parameters of the best performing models are: CIFAR10: SGR λ= 0.005, β = 0.1, GN γ= 0.025,
PGD =8, FGSM =4, weight-decay 0.005. MNIST: SGR λ=3.0, β=0.1, GNγ=10.0, PGD
=32,FGSM=32, weight-decay 0.005.
11
Under review as a conference paper at ICLR 2019
Figure 4: Long Range Correlated Noise. Classification accuracy of different models as a function of
the attack strength for an LRC attack with a fixed decay length ζ = 8 on CIFAR10. The SGR/GN
hyper-parameters are reported in Sec. 4.3. See Fig. 3 for a plot of the accuracy of different models as
a function of the LRC attack decay length ζ ∈ [1, 16] at fixed = 0.3
Figure 5: Long-range structured covariance matrices corresponding to the covariance functions
defined in Sec. 4.3 for increasing decay lengths ζ ∈ [1, 2, 4, 8], as well as covariance matrix of
CIFAR10 training set (for comparison). As an alternative to the covariance function representation
described in Sec. 3.3, we can also leverage the block-structure of the full covariance matrix to obtain
a sparse representation: as can be seen in the figures, different equally distant pairs of rows are often
nearly identically correlated. Hence it is enough to estimate the covariance between a few pairs of
rows rm and rn > rm (until the correlations become sufficiently small). This amounts to for instance
estimating the covariance matrix by a diagonal and a few off-diagonal blocks.
7.3	Further Experimental Results
Smoothness of Regularized Classifier At the most basic level of abstraction, adversarial vulnera-
bility is related to an overly-steep classifier gradient, such that a tiny change in the input can lead to a
large perturbation of the classifier output. In order to analyze this effect, we visualize the softmax
activations along linear trajectories between various clean and corresponding adversarial examples.
As can be seen in Fig. 6, the regularized classifier consistently leads to smooth classifier outputs.
Label leaking. Label leaking can cause models trained with adversarial examples to perform better
on perturbed than on clean inputs, as the model can learn to exploit regularities in the adversarial
example construction process (Kurakin et al., 2016). This hints at a danger of overfitting to the
specific attack when training with adversarial examples. SGR regularized classifiers, on the other
hand, do not suffer from label leaking, as the adversarial examples are never directly fed to the
classifier.
7.4	Classifier Architectures
Networks were trained for 50/75 epochs on MNIST/CIFAR10 using the Adam optimizer (Kingma &
Ba, 2014) with learning rate 0.001 and minibatch size 128.
12
Under review as a conference paper at ICLR 2019
SGR regularized model
Clean model
Figure 6: Softmax activations along a linear trajectory between a clean sample and plus/minus five
times the = 4 PGD adversarial perturbation for an SGR regularized classifier (top) and a clean
classifier (bottom) on CIFAR10.
Classifier	
Feature Block	Conv2D (filter size: 3 X 3, feature maps: Params[0], stride: 1 X 1) ReLU
Feature Block	Conv2D (filter size: 3 X 3, feature maps: Params[1], stride: 1 X 1) ReLU
MaXPooling	MaxPool (pool size: (2,2))
Feature Block	Conv2D (filter size: 3 X 3, feature maps: params[2], stride: 1 X 1) 	ReLU	
Feature Block	Conv2D (filter size: 3 X 3, feature maps: params[3], stride: 1 X 1) 	ReLU	
MaxPooling	MaxPool (pool size: (2,2))
Fully-Connected	Dense (units: params[4])
Fully-Connected	Dense (units: params[5])
Fully-Connected	Dense (units: params[6]) Softmax
Table 3: Classifier Architectures: CIFAR10 params=[64, 64, 128, 128, 256, 256, 10], MNIST
params= [32, 32, 64, 64, 200, 200, 10]. Our models are identical to those used in (Carlini & Wagner,
2017; Papernot et al., 2016b).
13