Under review as a conference paper at ICLR 2019
Variational Domain Adaptation
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes variational domain adaptation, a unified, scalable, simple
framework for learning multiple distributions through variational inference. Un-
like the existing methods on domain transfer through deep generative models, such
as StarGAN (Choi et al., 2017) and UFDN (Liu et al., 2018), the variational do-
main adaptation has three advantages. Firstly, the samples from the target are not
required. Instead, the framework requires one known source as a prior p(x) and
binary discriminators, p(Di|x), discriminating the target domain Di from others.
Consequently, the framework regards a target as a posterior that can be explicitly
formulated through the Bayesian inference, p(x∣Di) 8 p(D∕x)p(x), as exhibited
by a further proposed model of dual variational autoencoder (DualVAE). Sec-
ondly, the framework is scablable to large-scale domains. As well as VAE encodes
a sample X as a mode on a latent space: μ(χ) ∈ Z, DualVAE encodes a domain
Di as a mode on the dual latent space μ*(Di) ∈ Z*, named domain embedding.
It reformulates the posterior with a natural paring h,)： ZXZ* → R, which can
be expanded to uncountable infinite domains such as continuous domains as well
as interpolation. Thirdly, DualVAE fastly converges without sophisticated auto-
matic/manual hyperparameter search in comparison to GANs as it requires only
one additional parameter to VAE. Through the numerical experiment, we demon-
strate the three benefits with multi-domain image generation task on CelebA with
up to 60 domains, and exhibits that DualVAE records the state-of-the-art perfor-
mance outperforming StarGAN and UFDN.
1	Introduction
“...we hold that all the loveliness of this world comes by communion in Ideal-Form. All shapelessness
whose kind admits of pattern and form, as long as it remains outside of Reason and Idea, is ugly
from that very isolationfrom the Divine-Thought” ——Plato (427 — 347 bc)
Agents that interact in various environments have to handle multiple observation distributions . Do-
main adaptation (Bengio, 2012) is a methodology employed to exploit deep generative models,
such as adversarial learning (Goodfellow et al., 2014) and variational inference (Kingma & Welling,
2013), that can handle distributions that vary with environments and other agents. Further, multi-task
learning and domain transfer are examples of how domain adaptation methodology is used. We focus
on domain transfer involving transfers across a distribution between domains. For instance, pix2pix
(Isola et al., 2017) outputs a sample from the target domain that corresponds to the input sample from
the source domain. This can be achieved by learning the pair relation of samples from the source
and target domains. CycleGAN (Zhu et al., 2017a) transfers the samples between two domains us-
ing samples obtained from both domains. Similarly, UNIT (Liu et al., 2017), DiscoGAN(Kim et al.,
2017), and DTN(Taigman et al., 2016) have been proposed in previous studies.
However, the aforementioned method requires samples that are obtained from the target domains,
and because of this requirement, it cannot be applied to domains for which direct sampling is ex-
pensive or often impossible. For example, the desired, continuous, high-dimensional action in the
environment, intrinsic reward (e.g., preference and curiosity) and the policy of interacting agents
other than itself cannot be sampled from inside, and they can only discriminate the proposed input.
Even for ourselves, the concept of beauty or interest in our conscious is subjective, complex, and
difficult to be sampled from the inside, although it is easy to discriminate on the outside.
1
Under review as a conference paper at ICLR 2019
Figure 1: The key concept of variational domain adaptation. a) Given the proposal drawn from the
prior, the discriminator discriminates the target domain from the others. Each domain is posterior for
the prior N (z|0, 1); further, the distribution in the latent space is observed to be a normal distribution
using the conjugate likelihood. b) Domain transfer is represented by the mean shift in the latent
space. C) Domain embedding: After training, all the domains can only be represented by vectors μ2.
In this study, we propose variational domain adaptation, which is a framework for targets that pose
challenges with respect to direct sampling. One solution is multi-domain semi-supervision, which
converts the problem to semi-supervised learning, thereby making is possible to perform variational
inference. In this supervision, a source domain is regarded as a prior p(x) and a target domain is
considered to be a posterior p(x|Di) by referring to the label given by a supervised discriminator
p(Di|x) that distinguishes the target domain from others. Our model imitates the behavior of the
discriminator and models the target domain using a simple conclusion of the Bayesian theorem,
Pθ(XIDi) ex pθ(Di∣x)pθ(x). The end-to-end learning framework also makes it possible to learn
good prior pθ (x) with respect to all the domains. After the training was completed, the posterior
pθ (x|Di) succeeded in deceiving the discriminator p(Di|x). This concept is similar to rejection
sampling in the Monte Carlo methods. Further, variational domain adaptation is the first important
contribution from this study.
The second contribution from this study is a model of dual variational autoencoder (DualVAE),
which is a simple extension of the conditional VAE (Kingma et al., 2014), employed to demonstrate
our concept of multi-domain semi-supervision. DualVAE learns multiple domains in one network
by maximizing the variational lower bound of the total negative KL-divergence between the target
domain and the model. DualVAE uses VAE to model the prior p(x) and an abstract representation for
the discriminator p(Di|x). The major feature of DualVAE is domain embedding that states that all
the posteriors are modeled as a normal distribution N(z∣μ%, σ2) in the same latent space Z using the
conjecture distribution of the prior. Here, μ% is the domain embedding that represents the domain Di.
This enables us to sample from pθ(x|Di). Our major finding was that the discriminator of DualVAE
was a simple inner product between the two means of domain embedding and the VAE output:
Pθ (Di|x)
log Er
log
N(z∣μi, σ2)N(Z∣μφ(x), σ2)
N(z|0, I)
dz
σ2
that acts as a natural paring between the sample and the domain. The probabilistic end-to-end model
learns multiple domains in a single network, making it possible to determine the effect of transfer
learning and to learn data that multi-domains cannot observe from sparse feedback. Domain embed-
ding is a powerful tool and allows us to use VAEs instead of GANs.
The third contribution of this study is that DualVAE was validated for use in a recommendation task
using celebA (Liu et al., 2015). In the experiment, using celebA and face imaging data obtained
based on evaluations by 60 users, an image was generated based on the prediction of user evalua-
tion and an ideal image that was determined to be good by multiple users. We demonstrated that
the image could be modified to improve the evaluation by interpolating the image, and the image
was evaluated using the domain inception score (DIS), which is the score of the model that has
learned the preference of each user. We present the beauty inside each evaluator by simply sampling
pθ (x|Di). The DIS of DualVAE is higher than that of a single domain, and the dataset and code are
available online.
2
Under review as a conference paper at ICLR 2019
2	Related Work
The existing literature related to the domain transfer is based on the assumption that the samples are
obtained from the target domain. For example, pix2pix(Isola et al., 2017) can output the samples
from the target domain that corresponds to the input samples from the source domain by learning
the pair relation between the samples of the source and target domains. CycleGAN (Zhu et al.,
2017a), which differs from pix2pix, does not require sample pairs from both domains. Similarly,
UNIT (Liu et al., 2017), DiscoGAN(Kim et al., 2017), and DTN(Taigman et al., 2016) also do not
require sample pairs. Furthermore, because there are few cases in which samples from the source and
target domains form a one-to-one pair in real world research after being extended to the conversion
of one-to-many relationships, including BicycleGAN(Zhu et al., 2017b) and MUNIT(Huang et al.,
2018).
Several studies were conducted to model multiple distributions in a semi-supervised manner. Star-
GAN(Choi et al., 2017), UFDN(Liu et al., 2018), and RegCGAN(Mao & Li, 2018) are extensions
of the aforementioned models and are frameworks that can convert the source domain samples into
samples for various target domains with a single-network structure. However, the problem with these
methods is associated with hyperparameter tuning, which arises from the characteristics of adver-
sarial learning. DualVAE is a simple extension of a conditional VAE in a multi-domain situation.
Conditional VAEs utilizes VAE for semi-supervised learning. Although the model is quite simple,
it is powerful and scalable making it possible to learn multiple distributions with domain embed-
ding. In fact, we demonstrated that DualVAE quickly converged for more than 30 domains without
sophisticated hyperparameter tuning. In the experiment conducted in this study, Eω [J(θ∣ω)] was
evaluated instead of J(θ∣ω) to demonstrate that our method required less hyperparameter tuning.
3	Method
3.1	Problem Definition
With regard to n domains D1 , . . . , Dn, and a sample x on an observation space X, the objective
of unsupervised domain adaptation is to minimize the KL-divergence between the target distribu-
tion and the model, DKL p(i) (x)kp(i)(x, θ) , over all the domains Di. From the perspective of
optimizing θ, minimizing the KL divergence is equivalent to maximizing the cross-entropy. As
p(i)(χ, θ) = p(i)(χ∣θ)p(θ), the unsupervised domain adaptation can be formulated as a maximiz-
ing problem for the weighted average of cross-entropy over the domains:
1n
Maximizeθ : J(θ) = ~X Yi Ex〜p(i)[logpθi)(x)] + Ylogp(θ),	(1)
n i=1
where pθi)(χ) = p(i)(χ∣θ), Yi ∈ [0,1] is the importance of each domain Di and Y = Pn=ι Yi/n.
If Yi = 1 for all the i’s, the objective function is simply the mean, and if Yi = 0 for certain i’s, the
domain Di is ignored.
The difficulty arises from the fact that it is not possible to directly sample x from p(i) x can be
directly sampled from the likelihood p(Di|x). This challenge was the motivation for considering
multi-domain semi-supervision.
3.2	Multi-domain Semi-supervision
Multi-domain semi-supervision assumes a prior p(x) and models each the domain as a posterior
P(Z) = p(χ∣Di). As the Bayesian inference, We reformulate the cross-entropy Ex〜p(i)[logpθ(x|Di)]
in Eq. (1) as follows:
Ex〜p(i) [logPθ(x|Di)] = Zp(x∣Di)logPθ(x∣Di)dx = Z PDx)PxIlOg Pp(DiIxpθ(x) dχ
p(Di)	pθ (Di)
_ κ ∣"P(Di|x)i Pθ (Di|x)"| , κ ∣"P(Di|x)[ t /
=Ex~p [TWlog TpwJ + Ex~p [T(Dr gPP (X)一
=Ex〜P [f (Di∣x)logfθ(Di∣x)] + Ex〜P [f (Di∣x)logPθ(x)],	⑵
3
Under review as a conference paper at ICLR 2019
where f (Di|x) = p(Di∣x)/p(Di) and fθ(Di|x) = p(D∕x)∕pθ(Di). By letting Yi = P(Di), the
objective is identical to:
J(θ) =
Ex 〜p,i 〜[n] [p(Di∣x)log fθ(Di|x)] + Ex〜P [logpθ(x)] + Ylogp(θ),
X------------------------------} '----------------/ X---------}
(3)
^^^^™{^^^^^™
discriminator
{zy^^
prior
"^^"{^^^""^
regularizer
where [n] is a uniform distribution over {1,...,n} and f (D|x) = Ei〜皿[f (D∕x)]. The first term
is the likelihood from the discriminator; the second term is the prior learned by a generative model,
including VAE; and the last term is the regularizer.
Because the equation is intractable, we use Monte Carlo sampling to estimate the function. During
the estimation, we initially sample x1, . . . , xm from the prior p(x) and subsequently obtain the
binary labels yi7- ∈ {0,1} from each discriminator yi7-〜 p(D∕χj). Since the number of labels
from supervises is nm, the situation that the sparse labels: k << nm is considered. Further, some
discriminators only provide parts of the labels. In the situation, the missing values are 0-padded:
yij = 0.
1nm	1m
J(θ) ≈ 1∑∑yij log fθ (yj ∣χj) + — ɪ^log pθ (Xj) + yl°g p(θ),	(4)
n i=1 j=1	m j=1
where ≈ indicates Monte Carlo estimation and y = PZi Pm=I yj/k.In the limit of n → ∞, the
right side of the equation is identical to the left side.
3.3	Dual Variational Autoencoder (DualVAE)
We extended the VAE for multi-domain transfer to demonstrate our concept of multi-domain semi-
supervision. Our proposed model, dual variational autoencoder (DualVAE), models each domain
pi(x) as a posterior distribution p(x|Di) that is similar to that observed in a conditional VAE. Fig. 2
depicts the VAE and DualVAE graphical models.
The major feature of DualVAE is domain embedding, where all the domains and the prior share the
same latent space Z. For the prior distribution, P(Z) = N(z∣0,I) and p(z∣Di) = N(z∣μi,σ2I),
where μi ∈ Z is an embedding and I is a unit matrix in Z. In the following, We denote σ2I = σ2
without loss of generality. The domain Di is characterized only by its embedding μi. Here, μo is the
embedding of the prior that can be assumed to be μo = 0.
Training DualVAE is virtually equivalent to simultaneously training (n + 1) VAEs which share a
parameter, including the prior. Using conjecture distribution for the prior P(z), the posterior dis-
tribution is observed to be a normal distribution. Therefore, all the posteriors are VAEs. The joint
distribution can be given as follows:
3.3.1	VAE: THE PRIOR Pθ (x)
A VAE (Kingma & Welling, 2013) is used to model the prior P(x), a deep generative model that
employs an autoencoder to model the hidden variable as random variable. The benefit of a VAE is
that it can be used to model each distribution as a normal distribution in Z, achieved by maximizing
the variational lower bound of log P(x) as follows:
logPθ(X) ≥ Lθ(X) = Ez〜qφ(∙∣x) [logPw(x|z)] - DKL (qφ(zlx)kP(Z)),	⑸
where φ, w ∈ θ is a parameter of the encoder and the decoder, respectively. The objective is to
learn a pair of the encoder Pw(χ∣z) and the decoder qφ(z∣χ) to maximize L(x). Z acts as a prior
P(Z) = N(Z|0, I).
The lower bound Lθ(X) is derived using the reconstruction error and penalty term as the KL diver-
gence between the model and the prior P(Z). Further, the gradient of the reconstruction term can be
calculated using the Monte Carlo method, and because the construction term is the KL divergence
between two normal distributions, it can be analytically calculated.
4
Under review as a conference paper at ICLR 2019
Figure 2: Left: Graphical models of the probabilistic models of VAE and DualVAE. The gray and
white circles indicate the observed variables and latent variables, respectively. Symbols without
circles indicate the constants. Arrows between the symbols indicate probabilistic dependency (e.g.,
X generates Y). A rectangle with suffixes indicates a block, which comprises multiple elements.
Right: The network structure of DualVAE. The label is structured as the inner product of latent zθ
and domain embedding zi .
Domain
feature
matrix
3.3.2	Discriminator fθ(Di|x)
Using the definition and the Bayesian theorem, log fθ(Di |x) can be written as follows:
logfθ(Di|x) = log P pθ(DilzMzIx) dz = log P PPSi)pθ(ZIx) dz
pθ (Di)	pθ (z)
= Iog /N(Z|"i,σ2)N(z|"°(χ),σ2)dz =4&。(x)
=log]	N (ζ∣O,I)	= IL.
The equation above indicates log fθ (DiIx) can be written simply as the inner product between μi
and μφ(x), and the objective can be written as follows:
Ei~[n] [yi log fp(Di|x)] = FUφ(X) = αμU(y)μφ(x),	⑺
where U = (μ1,…，μn)t, μU = yτU/n and α = σ-2. Interestingly, it only requires one ad-
ditional parameter U except a hyperparameter α. Uis named as a domain embedding matrix, rep-
resenting the set of the domain prototypes. Domain embedding makes it possible to extend our
method to infinite domains such as a continuous domain. In fact, μU (y) ∈ Z* represents a proto-
type of mixed domains indicated by y in a domain latent space Z*, a dual space of Z. Note that
dimZ = dimZ*.
3.3.3	REGULARiZER p(θ)
The overall parameters of DualVAE is θ = (w, φ, U), where w is the encoder’s , parameterφ is the
decoders’s parameter, and U is the domain embedding matrix. While a typical VAE does not assume
any distribution ofw, φ,p(U) is set as an exponential distribution with an additional hyperparameter
β ∈ (0, ∞) to obtain sparse representation: p(U) Y exp(一β∣∣U∣∣1 /γ) where ∣∙∣∣1 is a 1-norm, thus,
log p(θ) = log p(U) = — — ∣U ∣1 — m dim Z log 2 + log β — log γ.	(8)
γ
As the terms except for the first are independent of θ , we ignore them later as constants.
3.3.4	The Final Form
By putting together the prior, the discriminator, and the regularizer, the variational lower bound of
the point-wise objective of DualVAE J(θIx, y) can be written as a surprisingly simple form:
J(θ∣x,y) ≥ Lp(x) + αh μφ(x),μu(y)〉一 β∣∣U∣∣ι,	⑼
where hu, v)= VTu. Consequently, a DualVAE maximizes a duality paring〈•，•)： ZXZ* → R
between the sample latent space Z = Zφ(X ) and the domain latent space Z* = ZU* (Y) where
5
Under review as a conference paper at ICLR 2019
Y = {0, 1}n. Note that the objective requires only two additional hyperparameters in addition to the
VAE. If α,β → 0, it is equivalent to a single VAE. Intuitively, 1∕α and 1∕β control variance and
bias of the domain embeddings, respectively.
The training algorithm of the DualVAE is shown in Algorithm 1.
Algorithm 1 Variational domain adaptation through DualVAE
Require: observations (xj)jm=1, batch size M, VAE/encoder optimisers: g, ge, hyperparameters
α, β, and the label matrix Y = (yj)jm=1.
Initialize encoder, decoder and domain embedding parameters: φ, w, U
repeat
Randomly select batch (xj)j∈B of size M
Sample Zj 〜qφ(z∖χj) ∀j ∈ B
φ,w — g(Vφ,w Pj∈B [log Pw (Xj Izj ) - DKL (qφ(z∖χj)Ilp(Z))D
φ, U J ge(Vφ,u Pj∈B[α(Kj- UTzj)2 + βkUk1])
until convergence of parameters θ = (φ, w, U)
4	Experiment
Based on an original numerical experiment in domain adaptation, we confirmed that the DualVAE
learns multiple distributions both qualitatively and quantitatively. Similar to the case of the existing
methods, domain adaptation was confirmed via an image-generation task in this study. First, we
performed A facial image recommendation task, which is a content-based recommendation task for
generating the preferences of users. Second, we performed the standard domain transfer task with
40 domains in CelebA (Liu et al., 2015) and we showed that DualVAE outperformed two state-of-
the-art methods through GAN and VAE.
The objective of the first task was to generate an image that was preferred by a specific user. We set
the input space X as the raw image, the prior p(x) as faces, and the domain Di as a user. We used
the dataset of CelebA and SCUT-FBP5500 as the samples from the prior. The objective of the task
was to generate samples from pθ(x∖Di), exhibiting the images that were preferred by a user. We
used label yi 〜p(Di∖χ) as the existing dataset of SCUT-FBP5500 with 5,500 faces and 60 users for
the content-based recommendation.
The purpose of the second task was to transfer samples from p(x) into samples frompθ(x∖Di). We
set the prior p(x) as face images and the posterior pθ (x∖Di) as face images with certain attributes of
CelebA. We used label yi 〜p(Di∖χ) as the attribute of CelebA.
The results revealed that the DualVAE successfully learned the model of the target distribution
pθ(x∖Di) both quantitatively and qualitatively. Quantitatively, we confirmed that the discriminator
learned the distribution by evaluating the negative log-likelihood loss, - logpθ(Di∖x). We evaluated
the samples using the domain inception score (DIS), which is the score for evaluating the transfor-
mation of images into multiple target domains. Notably, the DIS of the DualVAE was higher than
several models. Qualitatively, we demonstrated that the image could be transferred to improve the
evaluation by interpolating the image. We further exhibited several beautiful facial images that the
users were conscious of by decoding each domain embedding μ%, which can be considered as the
projection of the ideal from inside the users. In addition, 40 domain-transferred images using the
dataset of CelebA by the proposed method was better than the images by other models.
4.1	Dataset
CelebA CelebA(Liu et al., 2015) comprises approximately 200,000 images of faces of celebrities
with 40 attributes.
SCUT-FBP5500 SCUT-FBP5500(Liang et al., 2018) comprises 5500 face images and employs
a 5-point scale evaluation by 60 people in terms of beauty preference. The face images can be
categorized as Asian male, Asian female, Caucasian male, and Caucasian female, with 2000, 2000,
750, 750 images, respectively.
6
Under review as a conference paper at ICLR 2019
4.2	Result
The quantitative result of the experiment can be demonstrated by evaluating the generated images
by several models using a Domain Inception Score (DIS). Although the Inception Score (Salimans
et al., 2016) is a score for measuring generated images, it can only measure the diversity of the
images, and it is not for evaluating domain transfer of the images. Therefore, we proposed using a
DIS, which is a score for evaluating the transformation of images into multiple target domains.
The DIS is a scalar value using the output of Inceptionv3(Szegedy et al., 2016) pretrained to output
the domain label, and it is evaluated by the sum of two elements. The first is whether the domain
transfer of the original image has been successful (transfer score), and the second is whether the
features other than the transferred domain are retained (reconstruction score). A more detailed ex-
planation of the DIS is provided in the appendix.
Comparison of a DualVAE and a single-domain VAE A DualVAE can transform the image of
the source domain into images of multiple target domains with one model. However, considering a
simpler method, it is also possible to transfer the image of the source domain to the images of the
multiple target domains by creating multiple models. We will call each of these models a Single Do-
main VAE (SD-VAE). Since an SD-VAE is a model that converts the image of one source domain to
the image of one target domain, models corresponding to the number of target domains are required,
and thus, 60 models required training. We demonstrated that the DualVAE performance was equal
to or higher than that of the SD-VAE using the DIS. With respect to the output images of these two
models, the one with a higher DIS value was considered to be capable of outputting ideal images.
We calculated the DIS of 200 test images transferred by these two model. The DIS of the DualVAE
was -0.0185, whereas that of the SD-VAE was -0.0282. Thus, the DIS of the DualVAE was 0.01
higher than that of SD-VAE.
Comparison of DualVAE and several models The DualVAE was compared with several models
capable of performing image-to-image translations for multiple domains using a single model. In this
experiment, only the celebA dataset and the attributes of the dataset were used as the domain. Also,
the input image was resized to 128 × 128. In each model, the dimension of the latent variable and
the learning rate were randomly changed, the DIS was calculated several times, and the average and
the standard deviation were obtained. The DualVAE obtained a higher DIS than the other models.
Table 1: Average DISs for three domain adaptation methods employing random hyperparameter
search, which demonstrates DualVAE outperforms several models based on the DIS as the hyperpa-
rameter search is not necessary. Typical generated images are shown in the Appendix.
Method	5 domains	10 domains	20 domains	40 domains
CVAE(Kingma et al., 2014)	-0.055±0.011	-0.108±0.017	-0.112±0.007	-0.152±0.006
UFDN (Liu et al., 2018)	0.251±0.011	0.160±0.013	0.075±0.008	-0.002±0.003
StarGAN (Choi et al., 2017)	0.239±0.261	-0.094±0.346	0.068±0.188	0.050±0.032
DualVAE	0278±0.026	0.180±0.011	0.163±0.025	0.140±0.020
4.3	Visualization of domain transfer
We transferred the images by interpolating between the original and the target domain images. We
calculated the following vector wi :
Wi = Z + λμi.
(10)
Here, Wi was constrained by giving it the same norm as z to retain as much of the original features
as possible.
By changing λ and decoding Wi , five images were determined to represent unideal to ideal recon-
structions for each of the three sample users (i = 14, 18, and 32), and interpolation was performed
to approach the ideal image xi in Figure 3. In addition, we have visualized transferred images of the
40 attributes by the proposed method and other models in Figure 4.3. Although StarGAN and UFDN
7
Under review as a conference paper at ICLR 2019
retained the characteristics of the original image considerably, it Was qualitatively understood that
domain transfer Was not good especially When the number of domains Was large like 40 attributes.
Figure 3: Images obtained from our model by decoding wi (i = 14, 18, and 32) While changing the
value of λ. The reconstructed images are present in the center.
(Sl)①」。US」①⅞ue匕
Figure 4: Scatter plot of DVAE, UFDN, StarGAN and CVAE when we change several parameters.
Different color denotes different models. All 40 domains transferred images are in subsection C.1.
5	Conclusion
Variational domain adaptation, which is a unified framework for learning multiple distributions in a
single network, is proposed in this study. Our framework uses one known source as a prior p(x) and
binary discriminatorp(Di|x), thereby discriminating the target domain Di from the others; this is in
contrast with the existing frameworks in which samples undergo domain transfer through deep gen-
erative models. Consequently, our framework regards the target as a posterior that is characterized
through Bayesian inference, P(XDi) Y p(Di |x)p(x). This Was exhibited by the proposed DualVAE.
The major feature of the DualVAE is domain embedding, which is a powerful tool that encodes all
the domains and the samples obtained from the prior into normal distributions in the same latent
space as that learned by a unified netWork through variational inference. In the experiment, We ap-
plied our frameWork and model to a multi-domain image generation task. celebA and face image
data that Were obtained based on evaluation by 60 users Were used, and the result revealed that the
DualVAE method outperformed StarGAN and UFDN.
Several directions should be considered for future research. First, We intend to expand DualVAEs
for learning in complex domains, such as high-resolution images With several models, for example,
gloW(Kingma & DhariWal, 2018). Second, We Will perform an experiment to consider Wider domains
With respect to beauty. We expect that our proposed method Will contribute to society in a number
of Ways and Will help to deal With the paradigm of multiple contexts—multimodal, multi-task, and
multi-agent contexts.
8
Under review as a conference paper at ICLR 2019
References
Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Pro-
Ceedings of ICML Workshop on Unsupervised and Transfer Learning, pp. 17-36, 2012.
Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Star-
gan: Unified generative adversarial networks for multi-domain image-to-image translation. arXiv
preprint, 1711, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-
image translation. arXiv preprint arXiv:1804.04732, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. arXiv preprint, 2017.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover
cross-domain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192,
2017.
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems,
pp. 3581-3589, 2014.
Lingyu Liang, Luojun Lin, Lianwen Jin, Duorui Xie, and Mengru Li. Scut-fbp5500: A diverse
benchmark dataset for multi-paradigm facial beauty prediction. 2018.
Alexander Liu, Yen-Chen Liu, Yu-Ying Yeh, and Yu-Chiang Frank Wang. A unified feature disen-
tangler for multi-domain image translation and manipulation. arXiv preprint arXiv:1809.01361,
2018.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks.
In Advances in Neural Information Processing Systems, pp. 700-708, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Xudong Mao and Qing Li. Unpaired multi-domain image generation via regularized conditional
gans. arXiv preprint arXiv:1805.02456, 2018.
Leland McInnes and John Healy. Umap: Uniform manifold approximation and projection for di-
mension reduction. arXiv preprint arXiv:1802.03426, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Andriy Mnih and Ruslan R Salakhutdinov. Probabilistic matrix factorization. InJ. C. Platt, D. Koller,
Y. Singer, and S. T. Roweis (eds.), Advances in Neural Information Processing Systems 20, pp.
1257-1264. Curran Associates, Inc., 2008.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
2234-2242. Curran Associates, Inc., 2016.
9
Under review as a conference paper at ICLR 2019
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. arXiv
preprint arXiv:1611.02200, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. arXiv preprint, 2017a.
Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and
Eli Shechtman. Toward multimodal image-to-image translation. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 465-476. Curran Associates, Inc., 2017b.
A Latent space
We visualized the latent space Z of VAE and DualVAE. VAE differs from DualVAE methodology
because evaluation regression is not conducted during training. For each model, we can achieve
5500 latent vectors of 63 dimensions by encoding 5500 images from SCUT-FBP5500. We obtained
a scatter plot after using UMAP (McInnes & Healy, 2018) to reduce the number of dimensions to
two. The average score is indicated by colors ranging from red to blue. As can be observed from the
UMAP of DualVAE, the gradient of the score is learned, and it represents the user vector(domain
embedding vector) in Figure 5.
Figure 5: Latent visualization of VAE (left) and DualVAE (right) demonstrates that DualVAE learns
a good prior to model the domains. The heat map indicates the mean score of all the users.
B Domain Inception Score (DIS)
Although the Inception Score (Salimans et al., 2016) is a score for measuring generated images,
it can only measure the diversity of the images, and it is not for evaluating domain transfer of the
images. Therefore, we proposed using a DIS, which is a score for evaluating the transformation of
images into multiple target domains.
DIS is a scalar value, and it is evaluated by the sum of two elements. The first is whether the domain
transfer of the original image has been successful (transfer score), and the second is whether the
features other than the transferred domain are retained (reconstruction score).
We calculated the DIS using Algorithm 2. First, we assumed that there were N domains and we knew
which domain each image belongs to. We fine-tuned Inceptionv3 (Szegedy et al., 2016) using images
X as inputs and domains as outputs. To enable the model to classify the images as the domains, we
replaced the last layer of the model in a new layer which had N outputs. Second, we transferred test
10
Under review as a conference paper at ICLR 2019
images into N domains using Equation 10 and loaded the transferred images into the Inceptionv3
pretrained above. Through this process we got N × N matrix for every original image, because
one image was transferred into N domains and each domain image was mapped to N-dim vector.
We then mapped the original image into N-dim vector using Inceptionv3, and subtracted this vector
from each row of the abobe N × N matrix. We named this matrix M. The key points are (1) the
diagonal elements of M should be large because we transferred the original image into the diagonal
domains, and (2) the off-diagonal elements of M should be small because the transferred images
should preserve original features as possible. In a later subsection, we will directly visualize these
two elements and evaluate models.
Algorithm 2 Domain Inception Score (DIS)
Require: observation X ∈ X, Inceptionv3 f, domain transfer model m.
X J m(x)
M J f(x0)- f(x)
ts J average(diag(M))
rs J -average(abs(notdiag(M)))
DIS J ts+rs
In the Algorithm, abs denotes taking the absolute value, diag denotes taking the diagonal elements
of the matrix, notdiag denotes taking the non-diagonal elements, avg denotes taking the mean of
multiple values.
C Adaptation over many domains
This section shows further results of Table 1, the experimental result for domain adaptation over 40
domains made from CelebA. In the experimental setting above, we use attributes in CelebA as a
domain, the setting is used by several studies with domain adaptation (Choi et al., 2017). The result
shows DualVAE only learns 40 domains in one network, which indicates DualVAE is an easy way
to learn over 10 domains.
Next, we show several experimental results when we change the parameters of the models. Because
StarGAN uses GAN, the learning rate parameter is not robust, thus the learning is not conducted
well. Moreover, celebA has 40 domains which are too many for StarGAN, and this can also be
considered as one of the reasons that learning is not conducted well. Because reconstruction is
conducted well, rs in Algorithm 2 becomes larger than that of DualVAE. On the other hand, domain
transfer is not conducted properly, ts in Algorithm 2 becomes extremely small compares to that of
DualVAE. Therefore, as we can see from Table 1, DIS becomes a very small value.
C.1 CelebA
(a) DualVAE
11
Under review as a conference paper at ICLR 2019
(b) CVAE
Figure 6: Comparing domain transfer by several methods. (a) Since the image is blurry compared
to StarGAN, although the original features change significantly, domain transfer is still conducted
properly. (b) Although the characteristics of the original image are well preserved, domain transfer
and reconstruction is not conducted. (c) Although the characteristics of the original image are well
preserved, domain transfer is not conducted well. (d) Keeping the characteristic and being able to
transfer a small amount of the domain.
12
Under review as a conference paper at ICLR 2019
C.2 MNIST (10 DOMAINS)
Next, we conduct domain transfer experiments using the MNIST dataset. In this experiment, we
demonstrated that it is possible to transfer the image into another label (domain), while not compro-
mising the style of the original image. We also plotted the relation with DIS when labels are sparse.
Moreover, we showed in subsection I.1 it is possible to transfer to another domain step by step.
Figure 7: Scatter plot of the missing ratio of MNIST’s label and DIS of the DualVAE. Variable s is
the missing ratio. The original image is shown in the top right of the figure. The labels of the original
images are transformed to zero, one and two. The vertical axis is ts of Algorithm 2, the horizontal
axis is rs of Algorithm 2. DIS grows in the upper right corner.
q
9
2
(a)
(b)
DIS=0.72f ts=0.82, rs=-0.10, λ=6.0f s=0.9

4
(C)
DIS=0.08, ts=O.ll, rs=-0.03,λ=2.0, s=0.99
1
(d)
666
3 3 3
d IIΛI
3 0
公Q
5 Q
7
2

q

q q
。0
71
Figure 8: Domain transfer by varying λ. (a) Good example: domain transfer to different label is
successful while keeping the characteristic of the reconstruction image. (b) Bad example: although
the characteristic of reconstruction images are kept, domain transfer to different labels is not enough.
(c) Bad example: although domain transfer to different label is successful, the characteristic of re-
construction images is lost a little bit. (d) Bad example: domain transfer to different labels is not
successful.
13
Under review as a conference paper at ICLR 2019
D Domain embeddings
By reducing the dimensions of the 60 domain embedding vectors from 63 to 2 using
UMAP(McInnes & Healy, 2018), the domain embedding vectors were visualized by means of a
scatter plot. Furthermore, xi was visualized by decoding samples from the domain distribution.
posterior P(x∖Di)
prior P(X)
Figure 9: Scatter plot of the domain embedding vectors, and several decoded images of the sam-
ples from each domain. Six zi from the target domain distribution and output xi were decoded.
Furthermore, z0 from the source domain data distribution and output x0 was also decoded.
E Domain mixing
In this chapter, we show it is possible to conduct arithmetic operations among domains. For example,
suppose we learned the embedding vector of a charming image domain for each single person. We
can output the charming image for the group of people as an entity without learning simply by taking
the average value of the domain embedding vectors. Denote Community preference as fI, personal
evaluation model as f (= μTz(x)),
fI(X) = |1| X fi(X) = |1| X μTZ(X) = μTZ(X),
|I| i∈I	|I| i∈I
(11)
where, μ = (1/|I|) Ei∈i μ, , which is the average of domain embedding vectors. Moreover, i is
the index denoting the domain (person), I is the number of domains, and z(x) is the latent vector of
image x.
As shown in Equation 11, since the domain embedding vectors are linearly functional, by taking
the inner product of the average of these vectors μ and the latent vector z, the average of personal
evaluation (evaluation of the community) can be obtained.
Therefore, by substituting μi for μ in Equation 10, We can reconstruct the face images with high a
high degree of community evaluation. We reconstructed for higher (and lower) evaluation using 10
face images from both genders. Each image enjoys higher evaluation to the right. We can see that
gradually the caving becomes deep, the beard disappears, the eyes become bigger and the outline
becomes sharp Figure 10.
14
Under review as a conference paper at ICLR 2019
original
original
worse
better
worse
better
Figure 10: Images decoded to get closer (or further) to the average of domain embedding vectors.
The middle is the reconstructed original image.
F Connection to the history of matrix factorization
The section tells the proposed method, DualVAE, is a natural generalization from probabilistic Ma-
trix Factorization (PMF)(Mnih & Salakhutdinov, 2008), proposed in ten years ago.
F.1 Probabilistic matrix factorization (PMF)
PMF is used in several application area, mainly collaborative filtering algorithm, which are typical
recommendation algorithms. PMF learns the user matrix U ∈ RK ×N and the item matrix V ∈
RK ×J that can restore the evaluation matrix. Here, rij is the evaluation value of item j by user i, the
evaluation matrix is denoted as R ∈ RI×J. Moreover, the column vector of the user matrix U and
the item matrix V are denoted as ui ,vj respectively. K is the dimension of these vectors, N is the
number of users, J is the number of items. Iij is the indicator function that takes the value 1 when
evaluation rij exists and 0 otherwise.
The log likelihood of PMF is
logP(R|U, V,σ2) = XXIijlogN(TijIUTVj,σ2).	(12)
ij
Our objective is to find the ui , vj that maximizes the above.
Relationship to DualVAE DualVAE is an end-to-end coupling of VAE and PMF. We could see
DualVAE as PMF extended to a generative model. ui in Equation 12 corresponds to the domain
embedding vector in DVAE, vj corresponds to the latent vector in DVAE, rij corresponds to the
likelihood that item j belongs to domain i.
F.2 Experimental Analysis
F.2.1 Effect of end-to-end coupling
We experimentally show that the DualVAE outperformed the non-end-to-end coupling. We com-
pared two models. One is the model trained to regress evaluation of the image end-to-end by cal-
culating inner product of hidden representation of VAE and domain embedding (DVAE). The other
is the model which learns hidden representation of VAE followed by learning to regress evaluation
by inner product like above (VAE-PMF). We used SCUTFBP-5500 Figure 17 dataset, and validated
it into 5000 images with 60 evaluators and 500 test images with 60 evaluators. We quantitatively
compared these two models in terms of Root Mean Square Error (RMSE) of model prediction and
reconstruction error of test images. The result suggests that DualVAE achieved a much smaller
15
Under review as a conference paper at ICLR 2019
RMSE. Moreover, though DualVAE constrained its hidden representation to regress evaluation, the
reconstruction error was almost the same as VAE-PMF. This suggests that DualVAE can generate as
clear images as vanilla VAE.
Table 2: compare DUalVAE with VAE
Method RMSE Reconstruction loss
VAE + PMF0.423	8.19 X 104
DualVAE	0.356	8.20 X 104
Figure 11: RMSE and Reconstruction loss. DualVAE is far superior to VAE in classification accu-
racy, and there is almost no difference in reconstruction error between them.
----DVAE(proposed)
----V⅛E+PMF
----DVAE(proposed)
----VAE+PMF
F.2.2 Robustness to sparsity
In addition to generalization capability, another benefit from PMF is robustness to sparsity as PMF
is robust to a matrix with many missing values. We will experimentally demonstrate that DualVAE
is also robust with respect to sparse labels. We calculate the rs and ts when applying Algorithm 2
on 160 celebA test images, and plot the below figure when we change the missing ratio of celeA’s
domain labels and the λ in Equation 10.
Figure 12: Scatter plot of the missing ratio of celeA’s label and DIS of DualVAE. Variable s is the
missing ratio. The original image is shown on the left top of the figure. The attributes of the original
images are transformed to blond hair, eyeglasses and mustache. The vertical axis is ts of Algorithm 2,
the horizontal axis is rs of Algorithm 2. DIS grows in the upper right corner.
16
Under review as a conference paper at ICLR 2019
(a) s = 0. Keeping the characteristic and being able to domain transfer.
(b) s = 0.9. Although the sparseness of the labels is high, domain transfer is was still conducted
rather well.
Figure 13: Sparsity analysis through various sparsity.
(c) s = 0.99 (bad example). Image quality is poor, domain transfer is not conducted properly.
17
Under review as a conference paper at ICLR 2019
From Figure 13, keeping the characteristic of the upper right plots, it is possible to conduct domain
transfer at the same time. Moreover, the method is strong on the sparseness of domain labels, and
DIS does not drop even when 90 of the labels are missing.
On the other hand, we show that StarGAN is not as robust as DualVAE with respect to sparseness.
When 90 of domain labels are missing, StarGAN cannot learn at all and generates identical images.
original image
DIS=-0.10⅛s=0.9
Blond Hair	Tyegtasses-_ _ _ Mustache
DIS=0.035, s=0.5
Eyeglasses
BIondH air
Figure 14: Scatter plot of the missing ratio of celeA’s label and DIS of StarGAN. Variable s is the
missing ratio. The vertical axis is ts of Algorithm 2, the horizontal axis is rs of Algorithm 2. DIS
grows at the upper right corner.
18
Under review as a conference paper at ICLR 2019
DIS=-O-Il, ts=-0.01, rs=-0.09r s=0.9
5o-Ckκk_5hadow ⅛ched-Eyebrαws Attractive	GagS-Undef-EyeS	Bald	Bangs	Eig-LiPS	Big-NoSe	ElaCk_Hair	GkHlLHair
BIUrry	BVWn-Hair	BUShy.Eyebrows	Chubby	DDUbIe-Chin	Eyeglasses	Goatee	Giay-Hair	HBaVy_Makeu<p	I⅞gh-Cheeibαιιe5
Male Mo<ιth-Sligħtty-Oρeπ Mustache	Narr□w-Eyes	Na Beard	Ozal Face	I⅛∣e Skin	ft)mty_Nose	ReCeding-Hairiine	Rαsy-Cheeks
Sdebums	Smiling	Strai gM-Hair	Ubvy-Hair	Wearing-EarfingS	Ufeariπg-Hat	WfearingJJPStiCk	UfearingsNecMace	Vfearing-Neektie	,l⅛ung
(b) s = 0.9. All identical images are generated, and domain transfer is not properly conducted.
Figure 15: Sparsity analysis through StarGAN.
G FURTHER EXPERIMENTS WITH VARIOUS HYPERPARAMETER (α)
We conducted a comparison experiment with the existing methods when changing α(= σ-2) in
Equation 9. Here, the number of domains was set to 40. As you can see from the results below, it
turns out that the performance of DualVAE is robust to α.
0.00 -
10β	10j	IO2	IO3	IO4	IO3
a
Figure 16: Plot of DVAE, UFDN, and StarGAN when we change alpha. The performance of DVAE
is rubust to α and DVAE outperforms the existing methods based on the DIS.
H Model Details
The section shows three models used in tasks of domain adaptation over three types of domains:
environment, attribute and class.
Environment First, we describe the experimental setting for domain transfer to the ideal image of
each individual. We assumed that the beauty criterion required for evaluating the facial images de-
19
Under review as a conference paper at ICLR 2019
pends on the gender of a person in the target image. Therefore, we added the gender information to
the images. For this purpose, we applied CGAN (Mirza & Osindero, 2014) to VAE. We normalized
the scoring in [-1, 1] to accelerate the learning. Subsequently, we considered the specific model
structure of DualVAE. Both the input and output images were RGB images, x ∈ R256×256×3 .
We used convolution networks for the encoder and stride 2 for convolution and no pooling. Con-
volution, batch normalization(Ioffe & Szegedy, 2015), and LeakyReLU were repeated four times
and were subsequently connected to fully connected layers. Further, after batch normalization and
LeakyReLU layers, a 63-dimensional latent variable was obtained. The decoder exhibited a com-
pletely symmetric shape with deconvolution layers instead of convolution layers. Furthermore, as
the gender attribute, we set 0 as female and 1 as male. We added an image x ∈ R256×256×1 com-
prising 0 or 1 data as the input of the encoder and a scalar of 0 or 1 for gender to the latent variable,
which was the input to the decoder. The detailed structure is in Structure A of Table 3. We optimized
DualVAE on SCUT-FBP5500. Because there were no face evaluation data in celebA, we only used
it to optimize VAE. Learning was alternatively realized using these two datasets. We show the image
example of SCUT-FBP5500 (Liang et al., 2018). From Figure 17, we can see the evaluation value
depends on each person.
Attribute Next, in comparative experiment with several models, domain transfer was performed
with only celebA data and domain number of 40, 20, 10, and 5. We experimented with several
parameters of the models. In particular, the dimensions of the latent variable and the learning rates
were randomly selected. Both the input and output images were RGB images, x ∈ R128×128×3. The
detailed structure is in Structure B of Table 3.
Class Finally, we describe the experimental setting of domain transfer in the MNIST dataset. This
experimental result is stated in the subsection C.2. Both the input and output images were gray
images, x ∈ R28×28×1. The detailed structure is in Structure C of Table 3.
Table 3: The model structures of DualVAE used in our experiment. Conv stands for Convolution,
Deconv stands for Deconvolution, FC stands for Full Connected, and numbers in each parenthesis
are input and output channels. The kernel sizes of convolution are all 4 × 4, and stride sizes of that
are all two. Also, Batch2d represents batchnormalization in two dimensions, Batch1d represents
batchnormalization in one dimension, and LReLU stands for LeakyReLU.
Preference	Attribute	Class
Conv(3+1, 32)	Conv(3, 64)	Conv(1, 64)
Batch2d,LReLU	Batch2d,LReLU	Batch2d,LReLU
Conv(32, 64)	Conv(64, 128)	Conv(64, 128, 4, 4)
Batch2d,LReLU	Batch2d,LReLU	Batch2d,LReLU
Conv(64, 128)	Conv(128, 256)	FC(128*7*7, 1024)
Batch2d,LReLU	Batch2d,LReLU	Batch1d,LReLU
Conv(128, 256)	FC(256*16*16, 1024)	FC(1000, 100*2)
Batch2d,LReLU	Batch1d,LReLU	FC(100, 1024)+FC(100, 10)
FC(256*16*16, 1024)	FC(1024, dim*2)	Batch1d,LReLU
Batch1d,LReLU	FC(dim, 1024)+FC(dim, 40)	FC(1024, 128*7*7)
FC(1024, 63*2)	Batch1d,LReLU	LReLU
FC(63+1, 1024)+FC(63+1, 60)	FC(1024, 256*16*16)	Deconv(128, 64)
Batch1d,LReLU	LReLU	Batch2d,LReLU
FC(1024, 256*16*16)	Deconv(256, 128)	Deconv(64, 1)
LReLU	Batch2d,LReLU	Sigmoid
Deconv(256, 128)	Deconv(128, 64)	
Batch2d,LReLU	Batch2d,LReLU	
Deconv(128, 64)	Deconv(64, 3)	
Batch2d,LReLU	Sigmoid	
Deconv(64, 32)		
Batch2d,LReLU		
Deconv(32, 3)		
Sigmoid		
20
Under review as a conference paper at ICLR 2019
(a)
Figure 17: (a) Distribution of the respective scores within [1, 5] rated by 60 people. (b) The images
of SCUT-FBP5500.
(b)
21
Under review as a conference paper at ICLR 2019
I Further examples of domain transfer through DualVAE
The results below shows result from domain adaptation performed by DualVAE by randomly-
sampled images from two datasets: MNIST and CelebA.
I.1 MNIST (10 domains)
乙乙乙乙乙乙乙乙乙
。I 23 9 56 780
。I33956780
σ1 ə 3 V y 6 7 y 7
σ— ə 3 V y 6 7y7
^v)33y7⅛∙ 7y7
7 ^7777
7777777777
7777777777
7777777777
7777777777
1
S
6
Λli3nπ>ofo >/006
Λli3DΠ>of6 >/006
Λli3Λ∙3fo ɔ 06 6

3 3 3
V V ⅛
5 5 5

6 6
O I a3q54 7 Do Ol
O I 0*89
O I 23 4 56 7 Do O/
g1034667£弓
GC3√5^, gc 夕
Gc 6 466 Vrs夕
GKICKKKI<I<I<K
rκllclGκlκlul<ll<lκl
GG G GG GGG G G
Ola 3454 989
Ola 3456 980
Ola 3454 28 O
O I 23 4 5g 7 Do 9
6IA3hl56 ɔ 8 Al
OIqr 3 夕夕4 7⅛ 9
。彳Rcn"夕4 "夕夕
-Wf of Q∙
qqqqqqqqqq
∂I33⅛56 7 Da ʌv
ð I ⅜d 3 ⅛ 5 6 7 Av 9
Ola 3 夕66 ?8夕
Ola 3 9 66 7 Λo Az
0JJ3"66 0*6 夕
OJD3"86 009
OId ∂ O ∂6 OOO
O 6 ð δo∂6 OoO
OO6。00600。
O。。。Oo6000
G — 33^1^06 7000/
0la3v^o6 7 Oo 7
^Ia3v^o6 VOOOZ
0l33vy6 7 Oo ?
5>la3 0 56 Og 0
^I33u∕y⅛ V y 7
σza3yy4* 7y7
7Y⅛9V∙YV7Y7
7Y70J7YWY7
YYyYYYYYYY
∂i2345γ6 7 Qo A/
∂l a 3 夕& 6 4>89
el23"ir6 A7
∂I23" 8 6 勺89
∂123"er6 个&?
e>123“。。eðof
2?当白。
6 ,4 2 2。匕 GS
S S SGCS
VSVSVSVSVSVSVSVSVS。
(0 — 33^^56 7 Do 9
OIrO3 夕 56 7 Do 9
OIrO3 夕 56 7 Do 9
。—23 9 56 7 Do 9
。103956 9 DQ 9
0JO3"66。8。
。。。。。0。。。。
Odoooooooo
Oooooooooo
Ddoddooddo
。—a3 3 56 73 9
e>la3v$6 739
C>1a3y夕 6 73 夕
夕夕夕
t f 1
夕夕m
? ? 3
y 7 y
夕夕夕
夕,6
7 7 7
，夕夕
夕夕夕
7，7CK /7>7??
7<κ7>>7>?CK ?
777777777?
Figure 18: DualVAE stably transfers samples across 10 domains while domain-irrelevant features
(e.g., style) are kept.
22
Under review as a conference paper at ICLR 2019
I.2 CelebA (40 domains)
CtoCk.Shadow ArChed.Eyebrows Attractive Bags Under Eyes Bald	Bangs	Big_LiPS	Big Nose	BIack Hair	BIond Hair
点点fia宜H宜点£!■
Blurry	Brown Hair Busħy Eyebrows OIUbby	D。IJbie_Chin	Eyeglasses	Goatee	Gray_Hair	Heavy Makeup High Cheekbones
BrownJdair
Eyeglasses
Goatee
Male M。Uth_5Iightly_0pen Mustache	Narrow_Eyes	No_Beard	OVaLFaCe	Pale_Skin	POinty_N。Se RecedingJairIine Rosy_Cheeks
RfI点直自点AfI位a
SidebUES	Smiling	Straigh=Hair Wavy_Hair Weartng Earrings IAfearing Hat IAfearing Lipstick	Wearing_MeCklaCe	IAfearing Necktie Voung
IjCtoCk.Shadow ArchedEyebrows Attractive BagsUnderEyes Bald	Bangs	Big_ LiPS	BigNose	BIackHair	BlondHair
Tl 个 m	：
Blurry	Brown Hair Busħy Eyebrows ChUMIy	D。IJbie_Chin	Eyeglasses	Goatee	Gray_Hair	Heavy Makeup High Cheekbones
BrownJdair
Eyeglasses
Goatee
Male Mouth_5lightly_Open Mustache Narrow_Eyes NJBeard	O⅛al-Face	Pale-Skin	POinty_N。Se RecedingsHairIine Rosy-Cheeks
>9、TlTiTl 4 F C F
Sidebums	Smiling	Straigh=Hair Wavy_Hair Weartng Earrings IAfearing Hat IAfearing Lipstick Wearing_MeCklaCe IAfearing Necktie YoUng
∖qf 今令 rτ∣τ∣0∣τ∣
5 o Clock Shadow Arched EyeUrows Pttractive Bags Under Eyes Bald	BangS	Big Ups	Blg No^e	Black Hair	Blond Hair
GanRRC&戏。
Blurry	BrOWn_Hair BUShy_Eyebrows	Giubby	Double-Chin	Eyeglasses	Goatee	Gray-Hair Heavy-Makeup High_Cheekbones
RllLeiRBElftlSLJ La 豆
Male Mouth SIightIy Open Mustache Namw Eyes No Beard	Oval Face	Pale_Skin	Pointy Nose Reeeding_Hairune Rosy Cheeks
氏a炕CA兔AnSLR
23
Under review as a conference paper at ICLR 2019
5_o_CtoCk.Shadow	ArChed_Eyebrows	Attractive	Bags_U RderEyes	Bald	Bangs	BigjJPS	BigNose	BIaCk_ Hair	BIondJair
Goatee
Blurry	BrownJair Bushy_Eyebrows	QIUbby	D。IJbljChin	Eyeglasses	Goatee	Grayfair Heavy_Makeup High∕heekb OneS
Ad©aCAaA
MaJe
Mouth_Slightly_Open
Mustache
Nanw_Eyes
NJBeard
Owa LFaCe
PaljSkin
Pointy-Nose
RecwdingJdairIinE
RoSy_Cheeks

Sidebums	Smiling	Straight_Hair	Wavy_Hair	IAfea ring_Earrings	IAfea ring_Hat	IAfearingJJPStiCk Weartng_Necklace	IAfea ring_Necktie	Young
£。_CtoCk.Shadow	Arcħed-Eye brows
Attractive
3
∖i*r	，
Eyeglasses

Blurry	Brown Hair	Bushy Eyebrows	Chubby	DoubIe Chin Eyeglasses	GQatee	Gray Hair	HeaVy_MakeUP High Cheekbones
,Kh	I. c、I j
Male	MOUth_5IightIy_Open	Mustache Narrow_Eyes NjBeard	OVaLFaCe	Pale-Skin	Pointy_Nose	RecedingsHairIine Rosy-Cheeks
I I I ɔ I 广、，f' Ib C q，、力I
SdetMjrns	Smiling	StraightJair Wavy Hair Irtfearing Earrings WearingJIat IAfeartng Lipstick Wearing_Mecklace WfearingJIecktie Voung
∣∣	9 .QI Iφ)∣
^QΓ	⅜tf,
24
Under review as a conference paper at ICLR 2019
25