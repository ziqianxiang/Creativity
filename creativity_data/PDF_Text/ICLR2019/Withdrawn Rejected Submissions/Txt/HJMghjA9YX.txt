Under review as a conference paper at ICLR 2019
Model Comparison for Semantic Grouping
Anonymous authors
Paper under double-blind review
Ab stract
We introduce a probabilistic framework for quantifying the semantic similarity
between two groups of embeddings. We formulate the task of semantic similarity
as a model comparison task in which we contrast a generative model which jointly
models two sentences versus one that does not. We illustrate how this framework
can be used for the Semantic Textual Similarity tasks using clear assumptions
about how the embeddings of words are generated. We apply information criteria
based model comparison to overcome the shortcomings of Bayesian model com-
parison, whilst still penalising model complexity. We achieve competitive results
by applying the proposed framework with an appropriate choice of likelihood on
the STS datasets.
1	Introduction
The problem of Semantic Textual Similarity (STS), measuring how closely the meaning of one piece
of text corresponds to that of another, has been studied in the hope of improving performance across
various problems in Natural Language Processing (NLP), including information retrieval (Zheng
& Callan, 2015). Recent progress in the generation of word embeddings (Mikolov et al., 2013)
has allowed the encoding of words using distributed vector representations, which capture semantic
information through their location in the learned embedding space. Despite the extent to which
semantic relations between words are captured in this space, it remains a challenge to adapt these
individual word embeddings to express semantic similarity between word groups, like documents,
sentences, and other textual formats.
Recent methods for STS rely on additive composition of word vectors (Arora et al., 2016; Blacoe
& Lapata, 2012; Mitchell & Lapata, 2008; 2010) or deep learning architectures (Kiros et al., 2015),
which summarise a sentence through a single embedding. The resulting sentence vectors are then
compared using cosine similarity — a choice that is not theoretically justified, stemming only from
the fact that cosine similarity gives good empirical results. It is difficult for a practitioner to uti-
lize word vectors efficiently if the underlying assumptions in the similarity measure are not well
understood.
The main contribution of our work is the proposal of a framework that addresses these issues by ex-
plicitly deriving the similarity measure through a chosen generative model of embeddings, instead
of empirically motivating it. Via this design process, a practitioner can encode suitable assumptions
and constraints that may be favourable to the application of interest. Furthermore, this framework
puts forward a new research direction that could help understand semantic similarity, in which prac-
titioners can study suitable embedding distributions and assess how these perform.
The secondary contribution of our work is a derivation of a similarity measure that performs well in
an online setting1. Online settings are both practical and key to use-cases that involve information
retrieval in dialogue systems. For example, in a chat-bot application new queries will arrive one at
a time and methods such as the one proposed in Arora et al. (2016) will not perform as strongly
as they do on the benchmark datasets. This is because one cannot perform the required data pre-
processing on the entire query dataset, which will not be available a priori in online settings. Whilst
our framework produces an online similarity metric, it remains competitive to offline methods such
as Arora et al. (2016). We achieve results comparable to Arora et al. (2016) on the STS dataset in
1The difference between an online and an offline setting is whether or not one has access to the entire dataset
at evaluation time. That is, in an online setting, one has access to only a single query pair at a time.
1
Under review as a conference paper at ICLR 2019
Figure 1: The generation of embeddings of words in a sentence through a latent variable (the se-
mantic group). Above we visualize the semantic group space of embeddings (with Di , Dj , Dk
representing different semantic groups).
O(nd) time — compared to the O(nd* 2) average complexity of their method (where n is the number
of words in a sentence, and d is the embedding size).
2	Background
The compositional nature of distributed representations demonstrated in Mikolov et al. (2013) and
Pennington et al. (2014) indicate the presence of semantic groups in the representation space of word
embeddings; an idea which has been further explored in Athiwaratkun & Wilson (2017). Under this
assumption, the task of semantic similarity can be formulated as the following question: “Are the
two sentences (as groups of words) partitions of the same semantic group?” Figure 1 illustrates this
concept. Utilising this rephrasing, this work formulates the task of semantic similarity between
two arbitrary groups of objects as a model comparison problem. Taking inspiration from Ghahra-
mani & Heller (2006) and Marshall et al. (2006) we propose the generative models for groups (e.g.
sentences) D1, D2 seen in Figure 2.
The Bayes Factor for this graphical model is then formally defined as
P(D1, D2M1) _ 1	P(D1, D2MI)
ιm( 1, 2)— og pa, D2∣M2)- og p(Di ∣M2)p(D2 ∣M2).
(1)
Figure 2: On the left, M1 assumes that both datasets are generated i.i.d. from the same parametric
distribution. On the right, M2 assumes that the datasets are generated i.i.d. from distinct parametric
distributions.
2
Under review as a conference paper at ICLR 2019
To obtain the evidence p(D|Mi) the parameters of the model must be marginalised
p(Dι, D2∣M1) = ∕p(D1, D2 ∣θ)p(θ)dθ =/ Y	p(wk∣θ)p(θ)dθ,
wk∈D1 ㊉ D2
P(DiM2)= / Y p(wk∣θ)p(θ)dθ,
wk∈Di
where ㊉ denotes concatenation.
Computing the semantic similarity score of the two groups D1 , D2 under the Bayesian framework
requires selecting a reasonable model likelihoodp(wk∣θ), prior density on the parameters p(θ), and
computing the marginal evidence specified above. Computing the evidence can be computationally
intensive and usually requires approximation. What is more, the Bayes factor is very sensitive to
the choice of prior and can result in estimates that heavily underfit the data (especially under a
vague prior, as shown in Appendix E), having the tendency to select the simpler model; which is
argued further by M. S. Bartlett (1957) and Akaike et al. (1981). This is handled in Ghahramani &
Heller (2006) by using the empirical Bayes procedure, a shortcoming of which is the issue of double
counting (Berger, 2000) and thus being prone to over-fitting. We address these issues by choosing to
work with information criteria based model comparison as opposed to using the Bayes factor. The
details are described in Section 3.
We are not aware of any prior work on sentence similarity that uses our approach. We employ a
generative model for sentences similar to Arora et al. (2016), but our contribution differs from theirs
in that our similarity is based on the aforementioned model comparison test, whilst theirs is based on
the inner product of sentence embeddings derived as maximum likelihood estimators. The method
by Marshall et al. (2006) applies the same score as in Equation 1 in the context of merging datasets
whilst we focus on information retrieval and semantic textual similarity.
3	Methodology
We address the shortcomings of the Bayes Factor described in Section 2 by proposing model com-
parison criteria that minimise Kullback-Leibler (KL) divergence (denoted in equations as DKL)
across a candidate set of models. This results in a penalised likelihood ratio test which gives com-
petitive results. This approach relies on Information Theoretic Criteria to design a log-ratio-like test
that is prior free and robust to overfitting.
We seek to compare models M1 and M2 using Information Criteria (IC) to assess the goodness of
fit of each model. There are multiple IC used for model selection, each with different settings to
which they are better suited. The IC which we will be working with have the general form
IC(D, M) = - (αL(例D, M) + Ω (D, M)),
where L(θ∣D, M) = Pi L(θ∣Wi, M) is the maximised value of the log likelihood function for
model M, α is a scalar derived for each IC, and Ω (D, M) represents a model complexity penalty
term which is model and IC specific. Using this general formulation for the involved information
criteria yields the following similarity score:
sim(D1,D2) = - IC({D1, D2}, M1) + IC({D1, D2}, M2)
L(Θ1,2∣M1) - (L(Θ1M2) + L(θ2 |M2)))
-Ω({D1,D2}, Mi) + Ω ({Dι, D2}, M2).
The paragraph surrounding Equation (9) of Tenenbaum & Griffiths (2001) (Tversky’s contrast
model) describes what a good similarity is from a cognitive science perspective. Interestingly, the
semantic interpretation of the similarity we have derived is similar to the one described in that work.
We want to contrast the commonalities of the two datasets (through shared parameters) to the dis-
tinctive features of each dataset (through independently fit parameters).
Examples of these criteria can be put into two broad classes. The Bayesian Information Crite-
rion (BIC) is an example of an IC that approximates the model evidence directly, as defined in
3
Under review as a conference paper at ICLR 2019
Schwarz et al. (1978). Empirically it has been shown that the BIC is likely to underfit the data, es-
pecially when the number of samples is small (Dziak et al., 2012). We provide additional empirical
results in Appendix E, showing BIC is not a good fit for the STS task as sentences contain a rela-
tively small number of words (samples). Thus we focus on the second class - Information Theoretic
Criteria.
3.1 Information Theoretic Criteria
The Information Theoretic Criteria (ITC) are a family of model selection criteria. The task they
address is evaluating the expected quality of an estimated model specified by L(θ∣w) when it is
used to generate unseen data from the true distribution G(w), as defined in Konishi & Kitagawa
(2008b). This family of criteria perform this evaluation by using the KL divergence between the
true model G(W) and the fitted model L(θ∣w), With the aim of selecting the model (from a given
set of models) that minimizes the quantity
DKL (G(W)I ∣p(w∣θ)) = Eg ln
∖ U P	p(w∣θ)
-HG(W)- EG [lnp(w∣θ)].
The entropy of the true model HG(W) will remain constant across different likelihoods. Thus, the
quantity of interest in the definition of the information criterion under consideration is given by
the expected log likelihood under the true model Eg[lnp(w∣θ)]. The goal then, is to find a good
estimator for this quantity. One such estimator is given by the normalized maximum likelihood:
EG [lnp(w|。)]
1n
—£ln P(wi∣θ),
n
i=1
where G represents the empirical distribution. This estimator introduces a bias that varies with
respect to the dimension of the model’s parameter vector θ and requires a correction in order to
carry out a fair comparison of information criteria between models. A model specific correction
is derived resulting in the following IC, called Takeuchi Information Criterion (TIC) in Takeuchi
(1976)
1n	I
J= -n ∑VθL(θ∣Wi)	§
1n	I
I, = — VVθ L(θ∣Wi )Vθ L>(θ∣Wi)
n i=1	θ=θ
TIC(D, M) = -2(L(0|D, M) - tr(IJT)).
Then, under the TIC we have the following similarity (full derivation in Appendix A)
Sim(D1,D2) = 2(L(θ1,2∣M1) - L(θ1∣M2) - L(θ2M2)
-tr (I1,2 J-1) + tr (Iι Ji-1) + tr (I2/2-)).
For the case where we assume our model has the same parametric form as the true model and as
n → ∞, the equality I = J holds resulting in a penalty of tr (IJ-1) = tr(Ik) = k where k is the
number of model parameters, as shown in Konishi & Kitagawa (2008a). This results in the Akaike
Information Criterion (AIC) (Akaike, 1974)
, ʌ.
AIC(D, M) = -2(L(θ∣D, M) - k).
The AIC simplification of TIC relies on several assumptions that only hold true in the big data
limit. AIC also assumes our model M has the same parametric form as the true model (Konishi
& Kitagawa (2008a)). In general, TIC is a more robust approximation. However, as shown in
Appendix F, for models with a high number of parameters, TIC may prove unstable and thus AIC
will generally perform better. In this study we will consider and contrast both.
4
Under review as a conference paper at ICLR 2019
4 Likelihoods
In this section we will illustrate how to derive a similarity score under our ITC framework by choos-
ing a likelihood function that incorporates our prior assumptions about the generating process of the
data. Adopting the viewpoint of a practitioner, we would like to compare the performance of two
models — one that ignores word embedding magnitude, and one that makes use of it. Our modelling
choices for each assumption are the von Mises-Fisher (vMF) and Gaussian likelihoods respectively.
The comparison between the two likelihoods we provide in Section 5 provides empirical evidence as
to which approach is better suited to modelling word embeddings. The rest of this section outlines
the details of each of the two modelling choices.
4.1 Von Mises-Fisher Likelihood
Word embeddings are d dimensional vectors of real numbers, that are traditionally learned by opti-
mizing a dot product between target words and context vectors (Mikolov et al., 2013). Due to this
training setup, cosine similarity is often used to measure the semantic similarity of words in various
information retrieval tasks. Thus, we want to explore a distribution induced by the cosine similarity
measure. We model our embeddings as vectors lying on the surface of the d - 1 dimensional unit
hyper-sphere w ∈ Sd-1 and i.i.d. according to a vMF likelihood (Fisher et al. (1993))
p(w∣μ,κ)
K 2-ι	1
(2π) 21d-1 (K)exp (κμ>w) = Z(K)exp (κμ>w),
where μ is the mean direction vector and K is the concentration, with supports ∣∣μ∣∣ = ||w|| =
1, K ≥ 0. The term Iν (K) Corresponds to a modified Bessel function of the first kind with order ν.
In this work we reparameterise the random variable to polar hypersphericals w(φ) (φ =
(φ1, ..., φd-1)>) as adopted in Mabdia (1975). Further details can be found in Appendix B.
The first and second order partial derivatives of the vMF log likelihood are derived in Appendix C.
We prove (in Appendix C) that the mixed derivatives of the vMF log likelihood are a constant (with
respect to φ) times ∂L(θ, κ∣φ)∕∂θk. Thus, evaluated at the MLE, these entries are zero. Assuming
l<k
∂2L(θ, K|D)
∂K∂θk
_ ∂2L(θ,κ∣D)
θ=θ,κ=^ =	dθkdθι
= 0.
θ=θ,κ = ^
Thus, J is a diagonal matrix, with diagonal j = (J11, ..., Jdd)>
d	2d
tr(IJT) = X ZiTIii = Jl1 (∂KL(θ,κ∣D)) + X J- 1
i=1	K	i=2
(念 LGKID)Y
(2)
This quantity only requires O(nd) operations to compute and thus does not increase the asymptotic
complexity of the algorithm.
The closed form of the similarity measure for two sentences D1, D2 of length m and l respectively
under this model is then
sim(D1, D2) =(m + l)^1,2R1,2 — mK1R1 — l^2 是
-(m + l)logZ(K1,2) + mlogZ(K1) + llog Z(K2)
-1	-1
-tr(I1,2J1,2 ) + tr(IIJI	) + tr(I2J2	),
where the Jacobian terms (from the reparametrisation) cancel out. The subscripts indicate the sen-
tence, with 1, 2 meaning the concatenation of the two sentences.
4.2 Gaussian likelihood
Schakel & Wilson (2015) show that some frequency information is contained in the magnitude of
word embeddings. This motivates a choice of a likelihood function that is not constrained to the unit
5
Under review as a conference paper at ICLR 2019
hyper-sphere and possibly the simplest such choice is the Gaussian likelihood. Due to the small size
of sentences2 we choose a diagonal covariance Gaussian models. The Gaussian likelihood is then
P (w∣μ, ∑) = N (w∣μ, ∑),
where Σ is a diagonal matrix.
As with the vMF likelihood, we prove in Appendix D that the Hessian of the log likelihood evaluated
at the MLE is diagonal. The TIC correction is a sum of O(d) terms, similar to the form in Equation
2. This can be nicely written in terms of biased sample kurtosis (denoted K)
tr(IJT) = I (d + X
d
d+χ Ki
i=1
The closed form of the similarity measure for two sentences D1, D2 of length m and l respectively
under this model is then
d
sim(Dι, D2) = X-(m + l)ln(σ 1,2)i + m ln(σ ι)i + l ln(σ 2)i
i=1
d
+ 2 +
1d
2 E -(κ1,2)i + (KI)i + (κ2)
i=1
where the subscripts indicate the sentence, with 1, 2 meaning the concatenation of the two sentences.
5	Experiments
We assess our methods’ performance on the Semantic Textual Similarity (STS) datasets3 4 (Agirre
et al., 2012; 2013; 2014; 2015; 2016). The objective of these tasks is to estimate the similarity
between two given sentences, validated against human scores. In our experiments, we assess on
the pre-trained GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2016), and Word2Vec
(Mikolov et al., 2013) word embeddings. For the vMF distribution, we centre the distribution of
word embeddings and normalize them to be of length 1. Some of the sentences are left with a single
word after querying the word embeddings, making the MLE of the κ parameter of the vMF and Σ
parameter of the Gaussian ill-defined. We overcome this issue by padding each sentence with an
arbitrary embedding ofa word or punctuation symbol (i.e. ’.’ or ’?’).
We first compare our methods: vMF likelihood with TIC correction (vMF+TIC) and diagonal Gaus-
sian likelihood with AIC correction (Diag+AIC) against each other. Then, the better method is
compared against mean word vector (MWV), word mover’s distance (WMD) (Kusner et al., 2015)
4, smooth inverse frequency (SIF), and SIF with principal component removal as defined in Arora
et al. (2016) 5. We re-ran these models under our experimental setup, to ensure a fair comparison.
The metric used is the average Spearman correlation score over each dataset, weighted by the num-
ber of sentences. The choice of Spearman correlation is given by its non-parametric nature (assumes
no distribution over the scores), as well as measuring any monotonic relationship between the two
compared quantities.
5.1	Embedding magnitude
A practitioner may want to learn more about a given set of word embeddings, and the way these
embeddings were trained may not allow the user to understand the importance of certain features
— say embedding magnitude. This is where our framework can be used to help build intuition by
comparing a likelihood that implicitly incorporates embedding magnitude to one that does not.
2The covariance matrix of n samples with d dimensions such that n < d results in a low rank matrix.
3The STS13 dataset does not include the proprietary SMT dataset that was available with the original release
of STS.
4https://github.com/mkusner/wmd
5https://github.com/PrincetonML/SIF
6
Under review as a conference paper at ICLR 2019
Embedding	Method	STS12	STS13	STS14	STS15	STS16	Average
FastText	vMF+TIC	0.5578	0.5532	0.5897	0.6660	0.6596	0.6023
	Diag+AIC	0.6193	0.6335	0.6721	0.7328	0.7518	0.6765
GloVe	vMF+TIC	0.5516	0.5873	0.5865	0.6605	0.6083	0.5977
	Diag+AIC	0.6031	0.6132	0.6445	0.7171	0.7346	0.6564
Word2Vec	vMF+TIC	0.5161	0.4997	0.5661	0.6477	0.5984	0.5683
	Diag+AIC	0.5630	0.5799	0.6291	0.6951	0.6701	0.6265
Table 1: Comparison of Spearman correlations on the STS datasets between the two similarity
measures we introduce in the text. The average is weighted according to dataset size.
We present the comparison between the similarities derived from the vMF and Gaussian likelihoods
in Table 5.1. We note that the Gaussian is a much better modelling choice, beating the vMF on every
dataset by a margin of at least 0.05 (5%) on average, with each of the three word embeddings. This
is strong evidence that the information encoded in an embedding’s magnitude is useful for tasks such
as semantic similarity. This further motivates the conjecture that frequency information is contained
in word embedding magnitude, as explored in Schakel & Wilson (2015).
5.2	Online scenario
In Section 1, it was mentioned that in an online setting, one cannot perform the principal component
removal described in Arora et al. (2016), as that pre-processing requires access to the entire query
dataset a priori. This scenario arises in the context of information retrieval, e.g. when creating a
chat-bot.
The comparison against the baseline methods are presented in Table 5.2. We are able to out-perform
the standard weighting induced by MWV, as well as the WMD approach on all datasets, with each of
the three word embeddings considered. We outperform SIF using the GloVe embeddings by 0.0278
(2.78%), effectively tie when using the FastText embeddings with a difference of 0.0021 (0.21%)
and we are marginally below using Word2Vec by 0.0028 (0.28%). The latter two differences are
small enough to be considered insignificant.
5.3	Offline scenario
There are use-cases in which the entire dataset of sentences is available at evaluation time — for
example, in clustering applications. For this scenario, we compare against the SIF weightings,
Embedding	Method	STS12	STS13	STS14	STS15	STS16	Average
FastText	Diag+AIC	0.6193	0.6335	0.6721	0.7328	0.7518	0.6765
	SIF	0.6003	0.6921	0.6729	0.7473	0.7012	0.6777
	MWV	0.5994	0.6494	0.6473	0.7114	0.6814	0.6542
	WMD	0.5576	0.5146	0.5915	0.6800	0.6402	0.5997
GloVe	Diag+AIC	0.6031	0.6132	0.6445	0.7171	0.7346	0.6564
	SIF	0.5754	0.6269	0.6113	0.6899	0.6699	0.6286
	MWV	0.5526	0.5643	0.5625	0.6314	0.5804	0.5784
	WMD	0.5516	0.5007	0.5811	0.6704	0.6246	0.5896
Word2Vec	Diag+AIC	0.5630	0.5799	0.6291	0.6951	0.6701	0.6265
	SIF	0.5400	0.6352	0.6386	0.7027	0.6413	0.6293
	MWV	0.5329	0.5796	0.5886	0.6385	0.5776	0.5846
	WMD	0.5162	0.4966	0.5778	0.6630	0.6022	0.5755
Table 2: Comparison of Spearman correlations on the STS datasets between our best model
(Diag+AIC) and SIF,WMD and MWV for three different word vectors. The average is weighted
according to dataset size.
7
Under review as a conference paper at ICLR 2019
Word vectors	Method	STS12	STS13	STS14	STS15	STS16	Average
FastText	Diag+AIC	0.6193	0.6335	0.6721	0.7328	0.7518	0.6765
	SIF+PCA	0.5893	0.7121	0.6790	0.7498	0.7142	0.6810
GloVe	Diag+AIC	0.6031	0.6132	0.6445	0.7171	0.7346	0.6564
	SIF+PCA	0.5681	0.6844	0.6546	0.7166	0.6931	0.6552
Word2Vec	Diag+AIC	0.5630	0.5799	0.6291	0.6951	0.6701	0.6265
	SIF+PCA	0.5324	0.6486	0.6510	0.7031	0.6609	0.6347
Table 3: Comparison of Spearman correlations on the STS datasets between our best model
(Diag+AIC) and SIF+PCA for three different word vectors.
augmented with the additional pre-processing technique seen in Arora et al. (2016). We need only
consider this baseline, as it outperforms all others by a large margin.
The results are shown in Table 5.3. We remaing competitive with SIF+PCA on all three word
embeddings, being able to match very closely on GloVe embeddings. On the FastText and Word2Vec
embeddings, our method is less than 0.01 (1%) lower on average than SIF+PCA.
6	Conclusion
We’ve presented a new approach to similarity measurement that achieves competitive performance
to standard methods in both online and offline settings. Our method requires a set of clear choices
— model, likelihood and information criterion. From that, a comparison framework is naturally
derived, which supplies us with a statistically justified similarity measure (by utilizing ITC to re-
duce the resulting model-comparison bias). This framework is suitable for a variety of modelling
scenarios, due to the freedom in specifying the generative process. The graphical model we employ
is adaptable to encode structural dependencies beyond the i.i.d. data-generating process we have as-
sumed throughout this study — for example, an auto-regressive (sequential) model may be assumed
if the practitioner suspects that word order matters (i.e. compare ”Does she want to get pregnant?”
to ”She does want to get pregnant.”)
In this study, we conjecture that the von Mises-Fisher distribution lends itself to representing word
embeddings well, if their magnitude is disregarded and a unimodal distribution over individual sen-
tences is assumed. Relaxing the former assumption, we also model word embeddings with a Gaus-
sian likelihood. As this improves results, this is empirical evidence that word embedding magni-
tude carries relevant information, which agrees with prior intuition built from Schakel & Wilson
(2015). We hope that this framework could be a stepping stone in using more complex and accurate
generative models of text to assess semantic similarity. For example, relaxing the assumption of
unimodality is an interesting area for future research.
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and
Computational Semantics-Volume 1: Proceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pp.
385-393. Association for Computational Linguistics, 2012.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. * sem 2013 shared
task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Se-
mantics (* SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic
Textual Similarity, volume 1, pp. 32-43, 2013.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the 8th international workshop on semantic evalu-
ation (SemEval 2014), pp. 81-91, 2014.
8
Under review as a conference paper at ICLR 2019
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al. Semeval-2015 task 2:
Semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the
9th international workshop on semantic evaluation (SemEvaI 2015), pp. 252-263, 2015.
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea,
German Rigau, and Janyce Wiebe. Semeval-2016 task 1: Semantic textual similarity, monolingual
and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016), pp. 497-511, 2016.
Hirotugu Akaike. A new look at the statistical model identification. IEEE transactions on automatic
control, 19(6):716-723, 1974.
Hirotugu Akaike et al. Likelihood of a model and information criteria. Journal of econometrics, 16
(1):3-14, 1981.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. International Conference on Learning Representations, 2017, 2016.
Ben Athiwaratkun and Andrew Gordon Wilson. Multimodal word distributions. arXiv preprint
arXiv:1704.08424, 2017.
Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit hy-
persphere using von mises-fisher distributions. Journal of Machine Learning Research, 6(Sep):
1345-1382, 2005.
James O. Berger. Bayesian analysis: A look at today and thoughts of tomorrow. Journal of the
American Statistical Association, 95(452):1269-1276, 2000. ISSN 01621459. URL http:
//www.jstor.org/stable/2669768.
William Blacoe and Mirella Lapata. A comparison of vector-based representations for semantic
composition. In Proceedings of the 2012 joint conference on empirical methods in natural lan-
guage processing and computational natural language learning, pp. 546-556. Association for
Computational Linguistics, 2012.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors
with subword information. CoRR, abs/1607.04606, 2016. URL http://arxiv.org/abs/
1607.04606.
John J Dziak, Donna L Coffman, Stephanie T Lanza, and Runze Li. Sensitivity and specificity
of information criteria. The Methodology Center and Department of Statistics, Penn State, The
Pennsylvania State University, 16(30):140, 2012.
Nicholas I Fisher, Toby Lewis, and Brian JJ Embleton. Statistical analysis of spherical data. Cam-
bridge university press, 1993.
Zoubin Ghahramani and Katherine A Heller. Bayesian sets. In Advances in neural information
processing systems, pp. 435-442, 2006.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Ur-
tasun, and Sanja Fidler. Skip-thought vectors. CoRR, abs/1506.06726, 2015. URL http:
//arxiv.org/abs/1506.06726.
Sadanori Konishi and Genshiro Kitagawa. Information criteria and statistical modeling. Springer
Science & Business Media, 2008a.
Sadanori Konishi and Genshiro Kitagawa. Information criteria and statistical modeling. Springer
Science & Business Media, 2008b.
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to docu-
ment distances. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pp. 957-966, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.
press/v37/kusnerb15.html.
9
Under review as a conference paper at ICLR 2019
Maurice S M. S. Bartlett. A comment on d. v. lindley’s statistical paradox. Biometrika, 44(3/4):
533-534,1957.
KV Mabdia. Distribution theory for the von mises-fisher distribution and ite application. Statistical
Distributions for Scientific Work, 1:113-30, 1975.
Phil Marshall, NUtan Rajguru, and Anze Slosar. Bayesian evidence as a tool for comparing datasets.
Physical Review D, 73(6):067302, 2006.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013.
Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. proceedings of
ACL-08: HLT, pp. 236-244, 2008.
Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive
science, 34(8):1388-1429, 2010.
Kevin P Murphy. Conjugate bayesian analysis of the gaussian distribution. def, 1(2σ2)口6, 2007.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Adriaan MJ Schakel and Benjamin J Wilson. Measuring word significance using distributed repre-
sentations of words. arXiv preprint arXiv:1508.02297, 2015.
Gideon Schwarz et al. Estimating the dimension ofa model. The annals of statistics, 6(2):461-464,
1978.
Kei Takeuchi. The distribution of information statistics and the criterion of goodness of fit of models.
Mathematical Science, 153:12-18, 1976.
Joshua B Tenenbaum and Thomas L Griffiths. Generalization, similarity, and bayesian inference.
Behavioral and brain sciences, 24(4):629-640, 2001.
Guoqing Zheng and Jamie Callan. Learning to reweight terms with distributed representations. In
Proceedings of the 38th International ACM SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’15, pp. 575-584, New York, NY, USA, 2015. ACM. ISBN 978-
1-4503-3621-5. doi: 10.1145/2766462.2767700. URL http://doi.acm.org/10.1145/
2766462.2767700.
10
Under review as a conference paper at ICLR 2019
A DERIVATION OF TIC FOR M2
The MLE for p(φ1,2 ∣θ, M2) can be derived by simply estimating the separate MLE solutions for
p(Φι∣θι,M2) and p(φ2∣θ2,M2). What is not as obvious is that the penalty term follows the
estimation pattern.
Gradient vectors for M2 are given by (where ㊉ is concatenation)
VθL(θ∣φ1,2) = VθιL(θ1∣φ1)㊉ Vθ2L(θ2∣φ2),
and Hessian results in a block diagonal matrix
vθL(θlφ1,2)= [vθ1 L01lφ1) vθ2L(0θ2∣φ2),
with inverse
vθ L(θlφ1,2 )-1 = [vθ1 LB1"1)	Vθ2 L(θ2∣φ2)-1
Computing tr(IJ-1) then yields
tr(IJ-1) = tr (VθL(θ∣φ1,2)VθL(θ∣φ1,2)>VθL(θ∣φ1,2)-1)
tr
tr
(Jl1ιVθι L(θ1∣φ1)-1 I12Vθ2 L(θ2∣φ2 )-11、
I[I12VΘ1 L(θ1∣φ1)-1 I22Vθ2 L(θ2∣φ2 )-1])
(I11vθ1 L(θ1∣φ1)-1) +tr (I22VΘ2L(θ2∣φ2)-1)
B Reparametrisation of the vMF distribution
We reparametrise the random variable to polar hypersphericals w(φ) (φ = (φ1, ..., φd-1)>) as
adopted in Mabdia (1975)
p(φlθ,κ)=((2π)κ I d-1(κ))愣卜xp(κμ(θ)>w(φ))，
where
i-1
wi(φ) = ((1 - δid) cos φi + δid)	sin φk ,
k=1
i-1
μi(θ) = ((I - δid) Cos θi + δid) ] ] Sin θk,
k=1
∂w
∂φ
d-2
Y(sin φk)d-k-1.
k=1
This reparametrisation simplifies the calculation of partial derivatives. The maxima of the likelihood
remains unchanged since ∣∂w∕∂φ∣ does not depend on θ thus the MLE estimate in the hyper-
shperical coordinates parametrisation is given by applying the map from the cartesian MLE to the
polars.
n
ΣX1 Wi
Il Pn=1 Will,
Ad(κ)
ʌ	1	,..
θ = μ-1(μ),
R= Ii pn=1 Will
n
Id/2
Id/2-1
R(d - R2)
1 — R2
μ
11
Under review as a conference paper at ICLR 2019
where both the derivation and approximation for the MLE estimates are derived in Banerjee et al.
(2005). Let D = {φi}in=1 be the dataset. The log likelihood is then
∂ w
∂ φ
L(θ, κ∣φ) = κw(φ)Tμ(θ) — log Z(K) + log
n
L(θ,κ∣D) = X L(θ,κ∣φi).
i=1
C Partial Derivative Calulations (vMF likelihood)
We first show the following result, which is useful for the full derivation. For k ≤ j
∂∂
∂θk μj (θ) = ∂θk((I- δkd)
j-1
cos θk + δkd)	sin θi
(1 - δkj)号
sin θk
i=1
sin θk
δkj	7Γ~ I
cos θk
i-1
((1 - δkd)cosθk + δkd)	sin θi
i=1
—
=((I - δkj ) Cot θk - δkj tan θk )μj (θ),
where the second line comes from the fact that sin θk (or cosθk) gets transformed into a cos θk (or
—Sin θk), and thus We can revert to the original definition of μj by multiplying with a cot θk (or
- tanθk). If k > j, this derivative is 0. Thus, for a single data point w(φ)
∂
∂∂	∂
∂θ^L(θ,κ∖φ = ∂θJw(φ) μ(θ) - Λ∕Γ log Z(κ)
∂θ
k
公 > ∂μ(θ)
=κw(φ) F
d
=K): Wj (φ)μj (θ)((1 - δkj ) Cot θk - δkj tan θk ),
j=k
where the sum starts from k, as for j < k, the derivative is zero.
The derivative with respect to K is derived as follows
d-L(θ,κ∣φ) = ,κw(φ)>μ(θ) - ɪ log Z(κ)
∂K	∂K	∂K
Id (κ)
=w(φ)>μ(θ) - τ 2(、,
I d-1(κ)
where the derivative of the second term is a known result.
We next focus on second order derivatives
∂2	∂2	j-1
∂θ2 μj (θ) = ∂θ2 ((1 - δid) Cos θi + δid) ɪɪ sin θi.
k	k	i=1
Unless this derivative is zero, we notice that we take the derivative
which result in the negative of the original function. Thus
∂2	d
∂θ2 L(θ,κlφ) = -KEwj (φ)μj (θ).
k	j=k
d∂d θk or d2∂n2θi, both of
The below result is given
∂2	∂	Id (K)
万~2L(θ, κ|φ) = 77-(- ʃ 2 / J
∂K2	∂K	Id-1 (K)
Id (K)(Id-2(K)+Id (K))- Id-1(K)(Id
=	2Id-1(K)2-
-1 (K)+ I d + 1(K))
12
Under review as a conference paper at ICLR 2019
Next, we show that the second order mixed derivatives are a constant (with respect to φ) times
∂L(θ, κ∖φ)∕∂θk, i.e.
∂2L(θ, K∖φ)	∂2
∂κ∂θk
∂κ∂θk
∂
κw(φ) μ⑻-E log Z(K)
w(φ)> 转
∂θk
κ
-1 ∂L(θ, κ∖φ)
∂θk
Evaluated at the MLE by definition κ
-1 ∂L(θ,κ∣D)
ðθk
=0
_ ʌ
θ=θ,κ = ^
Assuming l < k (Hessian is symmetric)
∂L(θ, κ∣φ)
∂θk∂θl
公T ∂μ(θ)
κw(φ)瓯瓯
d
K E	Wj(φ)μj(θ)((1 — δkj)cot θk — δkj tan θk)((1 — δj) cot 仇—δj tan θι)
j=max(k,l)
d
KEwj(φ)μj(θ) Cot θι((1 — δkj) Cot θk — δkj tan θk)
j=k
d
Cot θικfwj(φ)μj(θ)((1 — δkj) cot θk — δkj tanθk)
j=ι
∂ L(θ,κ∖φ)
=cot θl ——而------.
∂θk
Which is also zero evaluated at the MLE cot θι
∂L(θ,κ∣D)
∂θk
= 0.
θ=θ,κ=κ
D Partial Derivatives Calculation (Gaussian Likelihood)
The partial derivatives for the diagonal Gaussian likelihood are (we take derivatives with respect to
precision λk = 1∕σ2 )
n
厢L(μ, λ∖W) = X λ"xki) - 〃，,
∂2	2
∂μ2 L(μ, λ∖W) = -nλk,
∂n
碣 L(μ, λ∖w) = 2λk
—
n
2 X(Xki)-〃k)
i=1
∂2
∂W
∂2
■2L(μ, λ W)
—
n
双,
∂λk∂μk
Evaluating at the MLE we get
i=1
∂2
福 LM λlD)
∂2
■2 L(μ, λ∣D)
μ=μ,λ=λ
-nλk
—
—
∂2
μ=μ,λ=λ
n
忘
—
n
2^2 ,
Qk
4
nσ
■—
2
∂λk∂μk
L(μ, λ∣D)
n
ʌ = X (Xki)- a，= 0.
μ=μ,λ=λ	i=1
13
Under review as a conference paper at ICLR 2019
n ι . ∙ .	. ι	ι ♦ . ∙ ∙ .ι ι f` ∙ . ∙	i`	ι τ	.
Substituting these derivatives into the definition of I and J we get
ʌ
I
上μk,μk
^
Jμk ,μk
Iλ2k,λ2k
T
Jλ2k,λ2k
(μ 4)k
4
Finally, computing the model complexity penalty we get
1
tr(I JT) = 2
d
d+X
i=1
E Bayes Factor and Bayesian Information Criterion
We first define the Bayes Factor for the Gaussian likelihood with parameters (μ, Λ = Σ-1). We
assume a Wishart prior
p(μ, Λ) = N (μ∣μo, (κoΛ)-1) Wi(Λ∣ν°, T-1),
which yields the following Normal-Wishart posterior (Murphy, 2007)
p(μ, Λ∣D) = N (μ∣μn, (κnΛ)-1) Wi(Λ”, T-1).
The posterior parameters are
νn = ν0 + n,
κn = κ0 + n,
_ κoμo + nW
μn =	,
κn
n
S = E(Wk - w) (wk - w)> ,
k=1
Tn = S + To + ɪɪ-0 (W - μo)(w - μo)> .
2κn
The evidence (Murphy, 2007) is
(D) =	1	( κ0 A d JT置 Γ Mn/2)
p( ) = ∏"d2∖κn)	∣τn∣Vn Γd(νo∕2).
Using p(Dι, D2∣M1) = p(D1 ㊉ D2 |M 1) We compute the Bayes factor for M1, M2 in closed form
sim(D1, D2)
KnKm ∖ d |Tn| VnITjVm Γ ( V ) Γ ( V )
κ0κ1) 阳性∣τo∣ν0 γ (Vn)rd(Vm)
(3)
where ∣Dι ∣ = n, ∣S21 = m and ∣Dι ㊉ D2∣ = m + n = l.
The BIC is defined as
, ʌ. ,
BIC(D, M) = -2L(θ∣D, M) + klogn ≈ -p(D∣M),
and acts as a direct approximation to the model evidence (Schwarz et al., 1978). Thus, the similarity
under the BIC is
sim(D1, D2)
2
. , ʌ . , ʌ . .
∣Mι) -L(θ1∣M2) -L(θ2∣M2)
n+m
一klog ---
nm
(4)
14
Under review as a conference paper at ICLR 2019
	AIC	Bayes Factor	BIC
STS-12	0.6031	0.5183	0.5009
STS-13 (-SMT)	0.6132	0.6297	0.6011
STS-14	0.6445	0.6428	0.5926
STS-15	0.7171	0.7429	0.6625
STS-16	0.7346	0.6328	0.5826
Table 4: Spearman correlations using GloVe embeddings and Gaussian likelihood. The AIC and
BIC use a diagonal covariance matrix, while the Bayes Factor uses a full covariance matrix.
where n, m are defined as above.
Equations 3 and 4 represents our similarity score under a Gaussian likelihood, for the Bayes Factor
and BIC respectively.
Table E compares BIC and the Bayes Factor using a Gaussian likelihood to the approach presented
in the paper. We see that while these approaches are competitive on STS-13, STS-14 and STS-15,
they both give severely worse results on STS-12 and STS-16, with more than 0.08 difference. This
motivates our choice to do a penalised likelihood ratio test instead of doing full Bayesian inference
of the evidences.
F	TIC Robustness
In this section, we compare the TIC and AIC on the two likelihoods described in the main text. Table
F presents the results of that comparison. As we can see, with each word embedding the Gaussian
AIC correction outperforms the TIC correction on average. Looking at Figure 3, it becomes apparent
why — the more parameters a Gaussian has, the more dependent its correction is on the number of
words in the sentence. This is reminiscent of the linear scaling with number of words in the BIC
penalty discussed in Appendix E, which was shown to perform badly. On the other hand, looking at
Figure 4, we see that the TIC for the vMF distribution has very low variance, and is generally not
dependent on the number of word embeddings in the word group. This gives intuition why the AIC
and TIC for the vMF give very similar results.
Embedding	Method	STS12	STS13	STS14	STS15	STS16	Average
FastText	vMF+TIC	0.5578	0.5532	0.5897	0.6660	0.6596	0.6023
	vMF+AIC	0.5443	0.5445	0.5853	0.6636	0.6654	0.5966
	Diag+TIC	0.5883	0.6553	0.6678	0.7197	0.7052	0.6626
	Diag+AIC	0.6193	0.6335	0.6721	0.7328	0.7518	0.6764
GloVe	vMF+TIC	0.5516	0.5873	0.5865	0.6605	0.6083	0.5977
	vMF+AIC	0.5541	0.5739	0.5824	0.6576	0.6599	0.5997
	Diag+TIC	0.5802	0.6420	0.6535	0.7179	0.6966	0.6534
	Diag+AIC	0.6031	0.6132	0.6445	0.7171	0.7346	0.6564
Word2Vec	vMF+TIC	0.5161	0.4997	0.5661	0.6477	0.5984	0.5683
	vMF+AIC	0.5176	0.5045	0.5696	0.6495	0.6072	0.5716
	Diag+TIC	0.5366	0.5932	0.6258	0.6814	0.6215	0.6127
	Diag+AIC	0.5630	0.5799	0.6291	0.6951	0.6701	0.6265
Table 5: Comparison of Spearman correlations on the STS datasets between the TIC and AIC cor-
rections for the diagonal covariance Gaussian and vMF likelihood functions.
15
Under review as a conference paper at ICLR 2019
LUJ9^EU8d
299
601
600
O
— 一
Oooo
Oooo
5 4 3 2
LUJ9^EU8d
Oo
25	50	75	100	125	150	175	200
Sentence length
Figure 3: TIC Correction penalty for varying sample sizes, samples generated from standardised
normal N(0, Id).
100 parameters
-300 parameters
-600 parameters
599
2	4	6
8	10	12	14	16	18	20
Sentence length
Figure 4: TIC Correction penalty for varying sample sizes, samples generated from Uniform distri-
bution on the unit hypersphere U (Sd-1).
16