Under review as a conference paper at ICLR 2019
Stability of Stochastic Gradient Method
with Momentum for Strongly Convex Loss
Functions
Anonymous authors
Paper under double-blind review
Ab stract
While momentum-based methods, in conjunction with the stochastic gradient de-
scent, are widely used when training machine learning models, there is little theo-
retical understanding on the generalization error of such methods. In practice, the
momentum parameter is often chosen in a heuristic fashion with little theoretical
guidance. In this work we use the framework of algorithmic stability to provide an
upper-bound on the generalization error for the class of strongly convex loss func-
tions, under mild technical assumptions. Our bound decays to zero inversely with
the size of the training set, and increases as the momentum parameter is increased.
We also develop an upper-bound on the expected true risk, in terms of the number
of training steps, the size of the training set, and the momentum parameter.
1	Introduction
A fundamental issue for any machine learning algorithm is its ability to generalize from the training
dataset to the test data. A classical framework used to study the generalization error in machine
learning is PAC learning (Vapnik and Chervonenkis, 1971; Valiant, 1984). However, the associated
bounds using this approach can be conservative. Recently, the notion of uniform stability, introduced
in the seminal work of Bousquet and Elisseeff (Bousquet and Elisseef, 2002), is leveraged to analyze
the generalization error of the stochastic gradient method (SGM) (Hardt et al., 2016). The result
in (Hardt et al., 2016) is a substantial step forward, since SGM is widely used in many practical
systems. This method is scalable, robust, and widely adopted in a broad range of problems.
To accelerate the convergence of SGM, a momentum term is often added in the iterative update of
the stochastic gradient (Goodfellow et al., 2016). This approach has a long history, with proven
benefits in various settings. The heavy-ball momentum method was first introduced by Polyak
(Polyak, 1964), where a weighted version of the previous update is added to the current gradient
update. Polyak motivated his method by its resemblance to a heavy ball moving in a potential
well defined by the objective function. Momentum methods have been used to accelerate the back-
propagation algorithm when training neural networks (Rumelhart et al., 1986). Intuitively, adding
momentum accelerates convergence by circumventing sharp curvatures and long ravines of the sub-
level sets of the objective function (Wilson et al., 2018). For example, Ochs et al. has presented
an illustrative example to show that the momentum can potentially avoid local minima (Ochs et al.,
2015). Nesterov has proposed an accelerated gradient method, which converges as O(1/k2) where
k is the number of iterations (Nesterov, 1983). However, the Netstrov momentum does not seem to
improve the rate of convergence for stochastic gradient (Goodfellow et al., 2016, Section 8.3.3). In
this work, we focus on the heavy-ball momentum.
Although momentum methods are well known to improve the convergence in SGM, their effect
on the generalization error is not well understood. In this work, we first build upon the frame-
work in (Hardt et al., 2016) to obtain a bound on the generalization error of SGM with momentum
(SGMM) for the case of strongly convex loss functions. Our bound is independent of the number
of training iterations and decreases inversely with the size of the training set. Secondly, we develop
an upper-bound on the optimization error, which quantifies the gap between the empirical risk of
SGMM and the global optimum. Our bound can be made arbitrarily small by choosing sufficiently
many iterations and a sufficiently small learning rate. Finally, we establish an upper-bound on the
1
Under review as a conference paper at ICLR 2019
expected true risk of SGMM as a function of various problem parameters. We note that the class of
strongly convex loss functions appears in several important machine learning problems, including
linear and logistic regression with a weight decay regularization term.
Other related works: convergence analysis of first order methods with momentum is studied in
(Nesterov, 1983; Ochs et al., 2014; Su et al., 2014; Ghadimi et al., 2015; Lessard et al., 2016; Yang
et al., 2016; Loizou and Richtarik, 2018; Gadat et al., 2016). Most of these works consider the
deterministic setting for gradient update. Only a few works have analyzed the stochastic setting
(Yang et al., 2016; Loizou and Richtarik, 2018; Gadat et al., 2016). Our convergence analysis
results are not directly comparable with these works due to their different assumptions regarding the
properties of loss functions. In particular, we analyze the convergence of SGMM for a smooth and
strongly convex loss function as in (Hardt et al., 2016), which is new.
First-order methods with noisy gradient are studied in (Kidambi et al., 2018) and references therein.
In (Kidambi et al., 2018), the authors show that there exists linear regression problems for which
SGM outperforms SGMM in terms of convergence.
Our main focus in this work is on the generalization, and hence true risk, of SGMM. We are aware
of only one similar work in this regard, which provides stability bounds for quadratic loss functions
(Chen et al., 2018). In this paper, we obtain stability bounds for the general case of strongly convex
loss functions. In addition, unlike (Chen et al., 2018), our results show that machine learning models
can be trained for multiple epochs of SGMM with bounded generalization errors.
Notation: We use EH to denote the expectation and ∣∣ ∙ ∣∣ to represent the Euclidean norm of a
vector. We use lower-case bold font to denote vectors. We use sans-serif font to denote random
quantities. Sets and scalars are represented by calligraphic and standard fonts, respectively.
2	Generalization error and stability
We consider a general supervised learning problem, where S = {zι,…，Zn} denotes the set of
samples of size n drawn i.i.d. from some space Z with an unknown distribution D . We assume a
learning model described by parameter vector w. Let f(w; z) denote the loss of the model described
by parameter w on example z ∈ Z . Our ultimate goal is to minimize the true or population risk:
R(W)= EZ 〜D f(w; z).	(1)
Since the distribution D is unknown, we replace the objective by the empirical risk, i.e.,
1n
RS (w) = nɪ^f (w; Zi).	⑵
We assume W = A(S) for a potentially randomized algorithm A(∙). In order to find an upper-bound
on the true risk, we consider the generalization error, which is the expected difference of empirical
and true risk:
g =∆ ES,A [R(A(S)) - RS(A(S))].	(3)
Finally, to upper bound g, we consider uniform stability:
Definition 1 Let S and S0 denote two data sets from space Zn such that S and S0 differ in at most
one example. Algorithm A is s-uniformly stable if for all data sets S, S0, we have
sup EA [f (A(S); z) - f(A(S0); z)] ≤ s.	(4)
z
It is shown in (Hardt et al., 2016) that uniform stability implies generalization in expectation:
Theorem 1 (Hardt et al., 2016) If A is an s-uniformly stable algorithm, then the generalization
error of A is upper-bounded by s.
Theorem 1 shows that it is enough to control the uniform stability of an algorithm to upper bound
the generalization error.
2
Under review as a conference paper at ICLR 2019
2.1	Assumptions on the loss function
In our analysis, we will assume that the loss function satisfies the following properties.
Definition 2 A function f : Ψ → R is L-Lipschitz if for all u, v ∈ Ψ we have |f (u) - f (v)| ≤
Lku - vk.
Definition 3 A function f : Ψ → R is β-smooth if for all u, V ∈ Ψ we have ∣∣Vf (u) 一 ▽/(V)Il ≤
β ku - vk.
Definition 4 A function f : Ψ → R is γ -strongly convex if for all u, V ∈ Ψ we have f(u) ≥
f(v) + Vf(V)T(U — v) + 2 ∣u — v∣2.
We assume that the parameter space Ω is a convex set. Furthermore, for the loss function to be
L-LiPschitz and and strongly convex, We further assume that Ω is compact. Since Ω is compact, the
SGMM update requires projection.
2.2	Stochastic gradient method with momentum
The update rule for projected SGMM is given by:
Wt+1 = P (Wt + μ(wt - Wt-i) 一 αVwf(wt; Zit))	(5)
where P denotes the Euclidean projection onto Ω, a > 0 is the learning rate1, μ > 0 is the momen-
tum parameter, it is a randomly selected index, and f(wt; Zit) is the loss evaluated on sample Zit. In
SGMM, we run the update (5) iteratively for T steps and let wT denote the final output. Note that
there are two typical approaches to select it. The first approach is to select it ∈ {1,…，n} uniformly
at random at each iteration. The second approach is to permutate {1, ∙∙∙ ,n} randomly once and
then select the examples repeatedly in a cyclic manner. Our results are valid for both approaches.
The key quantity of interest in this paper is the generalization error for SGMM given by:
Eg = Es,a[R(WT) - RS(WT)] = Es,io,…,iτ-ι [R(wτ) - RS(WT)]
since the randomness in A arises from the choice of io, ∙∙∙ , iτ-ι.
3	Main results
In the following, we assume that the loss function f (∙; z) is β-smooth, L-Lipschitz, and γ-strongly
convex for all Z.
Theorem 2 (Stability bound) Suppose that the SGMM update (5) is executed for T steps with con-
Stant learning rate a and momentum μ. Provided that βα++γ 一 2 ≤ μ < 3*')and α ≤ j+γ,
SGMM satisfies Es-uniform stability where
F ≤	2αL2(β + Y)	(6)
S — n(αβγ ― 3μ(β + Y))
The result in Theorem 2 implies that the stability bound decreases inversely with the size of the
training set. It increases as the momentum parameter μ increases. These properties are also verified
in our experimental evaluation.
Theorem 3 (Convergence bound) Suppose that the SGMM update (5) is executed for T steps with
constant learning rate a and momentum μ. Then we have
ES,A[RS(WT) — RS(WS)] ≤
μW0	. (1 — μ)Wι
(1 — μ)T	2aT
γW2	μγW3	αL2
--------------+-—：------
2	2(1 — μ)	2(1 — μ)
(7)
1In the following, we assume α is constant over time. In practice, time-decaying α is used in some applica-
tions. Please note that our results hold for time-decaying α.
3
Under review as a conference paper at ICLR 2019
where WT denotes the average of T steps of the algorithm, i.e., WT = τ++ι PT=O Wt, RS(W)=
1 Pn=I f(w; Zi), WS = argminw RS(w), Wo = Es,a[Rs(wo) - RS(WT)], W1 = Es,a[∣∣wo -
WSk2], W2 = Es,a[∣∣Wt — WSk2], and W3 = T+1 PT=O Es,A[kwt - Wt-Ik2].
Theorem 3 bounds the optimization error, i.e., the expected difference between the empirical risk
achieved by SGMM and the global minimum. UPon setting μ = 0 and Y = 0 in (7), We can recover
the classical bound on optimization error for SGM (Nemirovski and Yudin., 1983), (Hardt et al.,
2016, Theorem 5.2). The first tWo terms in (7) vanish as T increases. The terms With negative
sign imProve the convergence due to the strongly convexity. The last term dePends on the learning
rate, α, the momentum parameter μ, and the Lipschitz constant L. This term can be controlled by
selecting α sufficiently small.
Proposition 1 (Upper-bound on true risk) Suppose that the SGMM update (5) is executed for T
steps with constant learning rate a and momentum μ, satisfying the conditions in Theorem 2 and
μ(β + Y)《ɑβγ. Then, setting α = 1-μ∖∕WT1, we have:
Es,a[R(Wt)] ≤ Es,a[Rs(WS)] +
μWθ	叵	γW2	μγW3	2L2 (β + Y)
(1 — μ)T + LV TC	2	2(1 — μ) + nβγC
(8)
where C =∆
1 _ 3μL(β+γ)√T
(1-μ)βγ√W1
and Wt as well as the constants Wo, ∙ ∙ ∙ , W3 are defined in Theorem 3.
Proposition 1 provides a bound on the expected true risk of SGMM in terms of the global minimum
of the empirical risk. The bound in (8) is obtained by combining Theorem 2 and Theorem 3 and
minimizing the expression over a. The choice of a simplifies considerably when μ is sufficiently
small, as stated in Proposition 1. Due to the page constraint, the proof of this result is provided in
the supplementary material. Note that the first two terms in (8) vanish as T increases. The last term
in (8) vanishes as the number of samples n increases.
4 Proof of Theorem 2 ( S tab ility bound)
Following (Hardt et al., 2016), we track the divergence of two different iterative sequences of up-
date rules with the same starting point. However, our analysis is more involved as the presence of
momentum term requires a more careful bound on the iterative expressions.
To keep the notation uncluttered, we first consider SGMM without projection and defer the discus-
sion of projection to the end of this proof. Let S = {zι,…，Zn} and S0 = {z1,…，z" be two
samples of size n that differ in at most one example. Let WT and W0T denote the outputs of SGMM
on S and S0, respectively. We consider the updates Wt+ι = Gt(Wt) + μ(wt 一 Wt-I) and w；+i =
Gt(Wt) + μ(wt — wt-ι) with Gt(Wt) = Wt — αVwf (wt； Zit) and Gt(Wt) = Wt — Kwf (wt; z0t),
respectively, for t = 1,…，T. We denote δt δ ∣∣Wt — Wt∣∣. Suppose Wo = wO, i.e., δo = 0. We
first establish an upper-bound on EA[δt+1] in terms of EA [δt] and EA[δt-1] in the following lemma,
whose proof is provided in the supplementary document.
Lemma 1 Provided that ɑ ≤ β+γ, an upper-bound on Ea[δt+ι] is given by
EA[δt+ι] ≤ (1 + μ - Q B] )EA∣δt] + μEA[δt-ι] +----.	(9)
β+Y	n
Using the result of Lemma 1, in the following, we develop an upper bound on EA [δT]. Let us
consider the recursion
EA[^t+1] = (1 + μ - yΓ~~'^EA[δt] + μEA[^t-1] H----	(IO)
β+Y	n
with δo = δo = 0. Upon inspecting (10) it is clear that
Ea[£] ≥ (1 + μ — ^∣βτ)EaBi],	∀t ≥ 1,	(11)
β + Y
4
Under review as a conference paper at ICLR 2019
as we simply drop the remainder of positive terms. Substituting (11) into (10), we have
Er二、	(	μ	αβγ ∖τπ - .l 2αL
EAMt+1] ≤	(1	+ μ + ι + μ -	αβγ	- β+γ E EAMt]	+ -
I μ	β+γ	/
αβγ	2αL
≤ (1 + 3μ - R	) EA 修]+
β+γ	n
where the second inequality holds due to μ ≥ β+γ - ɪ.
(12)
Noting that EA [δt] ≥ EA [δt] for all t including T , we have
EA [δT] ≤
+ 3μ 一
αβγ Y ≤
β + Y J 一
2αL(β + Y)
n(αβγ - 3μ(β + Y))
(13)
where the second expression holds since 0 ≤ μ < 3αβγy) is assumed.
Applying the L-LiPSchitz property on f (∙, z), it follows that
EA[∣f(wτ;Z)- f(wT;z)|] ≤ LEA[δτ] ≤ n(：；-Y；；+ Y)).
(14)
Since this bound holds for all S, S0 and z, we obtain an upper-bound on the uniform stability and
the proof is complete. Our stability bound in Theorem 2 holds for the projected SGMM update (5)
because Euclidean projection does not increase the distance between projected points (the argument
is essentially analogous to (Hardt et al., 2016, Lemma 4.6)). In particular, note that Lemma 1 holds
for the projected SGMM.
5 Proof of Theorem 3 (Convergence bound)
Again, we first consider SGMM without projection and discuss the extension to projection at the end
of this proof. Our proof is inspired by the convergence analysis in (Yang et al., 2016; Ghadimi et al.,
2015) for a convex loss function with bounded variance and time-decaying learning rate. Different
from these works, we analyze the convergence of SGMM for a smooth and strongly convex loss
function with constant learning rate. To facilitate the convergence analysis, we define:
Pt =	(Wt - wt-ι)	(15)
1 — μ
with P0 = 0. Substituting into the SGMM update, the parameter recursion is given by
α
wt+1 + Pt+1 = Wt + Pt - 1--Vwf (wt; Zit)	(16)
1 — μ
It follows that
kwt+ι + Pt+1 - wk2 =kwt + Pt - wk2 + (τɑ-)2kvwf (Wt;zit)k2
1 — μ
-2α-(wt + Pt - W)TVwf (wt; Zit).	(17)
1 — μ
Substituting Pt (15) into (17), the recursion (16) can be written as
kwt+ι + Pt+1 - wk2 =kwt + Pt - wk2 + (γɑ-)2kvwf (Wt; zit)k2
1	— μ
-/产晨2 (wt - Wt-I)TVwf(Wt; Zit) - 2α-(wt - w)TVwf(Wt; Zit).
(I - μ)2	1 - μ
(18)
Upon taking the expectation with respect to it in (18) we have
EitkWt+1 + Pt+1- wk2 ≤ kWt + Pt-Wk2 +(τ-rμ )2L2 - (1¾2 (Wt-Wt-I)T vw RS (Wt)
—
2α
E (Wt-W) vwRS (Wt)
(19)
5
Under review as a conference paper at ICLR 2019
where We use the fact that ∣∣Vwf (wt； zQk ≤ L, due to L-Lipschitz, and that EiJVwf (wt； zQ]=
PwRS(wt). Furthermore, since RS(∙) is a Y-strongly convex function, for all Wt and wt-ι, we
have
(wt — w)TVWRS(Wt) ≥ RS(Wt) — RS(w) + 2∣∣wt — w∣2,
(wt - Wt-I)TVWRS(Wt) ≥ RS(wt) — RS(Wt-I) + 2∣∣wt — Wt-1∣2.	(20)
Substituting (20) in (19), we have
Eit [lwt+ι + pt+1 — wk2] ≤kwt + pt — wk2 — 1 αγ kwt — wI2 —门 2αμ	(RS(Wt) — RS(Wt-I))
+	1 — μ	(I- μ)2
— ι2αμ (RS (Wt)- RS (W)) + (⅛7 — (iaμγ⅛ kwt - Wjk2.
(21)
Taking expectation over io, ∙∙∙ , it for a given S, summing (21) for t = 0,…，T, and rearranging
terms, we have
TT
1--XEa[Rs(wt)	— RS(w)]	≤ ——^^Ea[Rs(wo)	— RS(WT)]	— ：；~J XEa[∣∣Wt	— w∣2]
1	— μ	t=0	(I- μ)	1 — μ t=0
— (10μY)2 X EA[kwt — Wt-Ik2] + EA[kw0 — wk2] + α 彳).	(22)
(μ) t=o	(	μ)
Since ∣∙∣ is a convex function, for all WT and w, we have
1T
kw t - wk2 ≤ τ+ιJ2 kwt — wk2.	(23)
Furthermore, due to convexity of RS(∙), we have
1T
Rs(Wt) — Rs(w) ≤ t~i E (RS(Wt)- RS(W)).	(24)
T+ 1 t=o
Taking expectation over S, applying inequalities (23) and (24) into (22), and substituting W = WS,
we obtain (7) and the proof is complete.
Our convergence bound in Theorem 3 can be extended to projected SGMM (5). Let use denote
yt+ι = Wt + μ(wt — Wt-ι) — OVWf (wt； zQ. Then, for any feasible W ∈ Ω, (17) holds for yt+ι,
i.e.,
kyt+ι + ^~y (yt+ι — Wt) — wk2 =kwt + Pt — wk2 +(-0- )2kVwf (wt； Zit)k2
+	1 — μ +	1 — μ
— 12Oμ (Wt + Pt — w)T Vwf (Wt； Zit).	(25)
Note that the LHS of (25) can be written as
kyt+ι +	(yt+ι — Wt) — wk2 = γ-i- kyt+ι — (〃wt +(1 — μ)w)k∙
1 - μ	1 - μ
We note that μwt + (1 — μ)w ∈ Ω for any W ∈ Ω and Wt ∈ Ω since Ω is convex.
Now in projected SGMM, we have
kwt+ι — (μwt + (1 — μ)w)k2 = llp(yt+ι) — (Zt + (1 — μ)w)k2
≤ kyt+1 — (μwt + (I- μ)w)k2	(26)
since projection a point onto Ω moves it closer to any point in Ω. This shows inequality (19) holds,
and the convergence results do not change.
6
Under review as a conference paper at ICLR 2019
J0t9 U-∙Q ■ J0t9-s
0	100	200	300	400	500
n (number Oftralnlng samples)
(a) Generalization error with respect to cross entropy
0.00	1 ---- 1	-------1------------1-----------1------------1—
0	100	200	300	400	500
n (number Oftralnlng samples)
(b) Training error (cross entropy)
0.175
0.150
0.125
0.100
0.075
£ 0.050
£
0.025
0.000
Figure 1: Generalization performance (cross entropy) of logistic regression for notMNIST dataset
with T = 1000 iterations and minibatch size 10.
XOeJnUUe~s∂~
100	200	300	400	500
n (number of training samples)
(a) Generalization error with respect to classification
accuracy
1.000
0.995
0.990
0.985
0.980
0.975
0.970
XOeJnUUe U"6
(b) Training accuracy
Figure 2: Generalization performance (classification accuracy) of logistic regression for notMNIST
dataset with T = 1000 iterations and minibatch size 10.
6	Experimental evaluation
In this section, we validate the insights obtained in our theoretical results in experimental evalua-
tion. Our main goal is to study how adding momentum affects the convergence and generalization
of SGM. We study the performance of SGMM when applied to the notMINIST dataset. Please note
that similar results are provided for the MNIST dataset in the supplementary document. We train a
logistic regression model with the weight decay regularization using SGMM for binary classification
on the two-class notMNIST dataset that contains the images from letter classes “C” and “J”, which
leads to a smooth and strongly convex loss function. We set the learning rate α = 0.01. The weight
decay coefficient and the minibatch size are set to 0.001 and 10, respectively. We use 100 SGMM
realizations to evaluate the average performance. We compare the training and generalization per-
formance of SGM without momentum with that of SGMM under μ = 0.5 and μ = 0.9, which are
common momentum values used in practice (Goodfellow et al., 2016, Section 8.3.2).
The generalization error (with respect to cross entropy) and training error versus the number of
training samples, n, under SGMM with fixed T = 1000 iterations are shown in Figures 1a and 1b,
respectively, for μ = 0,0.5,0.9. In Figures 2a and 2b, We plot the generalization error (with respect
to classification accuracy) and the training accuracy as a function of the number of training samples
for the same dataset. First, we observe that the generalization error (with respect to both cross
entropy and classification accuracy) decreases as n increases for all values of μ, which is suggested
by our stability upper-bound in Theorem 2. In addition, for sufficiently large n, we observe that the
generalization error increases with μ, consistent with Theorem 2. On the other hand, the training
error increases as n increases, which is expected. We can observe that adding momentum reduces
training error as it improves the convergence rate. The training accuracy also improves by adding
momentum as illustrated in Fig. 2b.
7
Under review as a conference paper at ICLR 2019
(a) Training error versus epoch
(b) Test error versus epoch
epoch
(a) Training accuracy versus epoch
Figure 4: Training and test accuracy of logistic regression for notMNIST dataset with n = 500
training samples and minibatch size 10.
Figure 3: Training and test error of logistic regression (cross entropy loss) for notMNIST dataset
with n = 500 training samples and minibatch size 10.
epoch
(b) Test accuracy versus epoch
In order to study the optimization error of SGMM, we show the training error and test error versus the
number of epochs, under SGMM trained with n = 500 samples in Figures 3a and 3b, respectively.
We plot the classification accuracy for training and test datasets in Figures 4a and 4b, respectively.
We observe that the training error decreases as the number of epochs increases for all values of μ,
which is consistent with the convergence analysis in Theorem 3. Furthermore, as expected, we see
that adding momentum improves the training error and accuracy. However, as the number of epochs
increases, we note that the benefit of momentum on the test error and accuracy becomes negligible.
This happens because adding momentum also results in a higher generalization error thus penalizing
the gain in training error.
7	Conclusions
We study the generalization error and convergence of SGMM for the class of strongly convex loss
functions, under mild technical conditions. We establish an upper-bound on the generalization er-
ror, which decreases with the size of the training set, and increases as the momentum parameter
is increased. Secondly, we analyze the convergence of SGMM during training, by establishing an
upper-bound on the gap between the empirical risk of SGMM and the global minimum. Our pro-
posed bound reduces to a classical bound on the optimization error of SGM (Nemirovski and Yudin.,
1983) for convex functions, when the momentum parameter is set to zero. Finally, we establish an
upper-bound on the expected difference between the true risk of SGMM and the global minimum of
the empirical risk, and illustrate how it scales with the number of training steps and the size of the
training set. Although our results are established for the case when the learning rate is constant, they
can be easily extended to the case when the learning rate decreases with the number of iterations.
We also present experimental evaluations on the notMNIST dataset and show that the numerical
plots are consistent with our theoretical bounds on the generalization error and the convergence gap.
8
Under review as a conference paper at ICLR 2019
References
V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events
to their probabilities. TheoryofProbabiUIyandItsAPPUcations,16(2):264-280,1971.
L.	G. Valiant. A theory of the learnable. Communications oftheACM, 27(11):1134-1142, 1984.
O. Bousquet and A. Elisseef. Stability and generalization. Journal of Machine Learning Research,
2:499-526, 2002.
M.	Hardt, B. Recht, and Y Singer. Train faster, generalize better: Stability of stochastic gradient
descent. Preprint available at arXiv:1509.01240v2, 2016.
I. Goodfellow, Y. Bengio, and A. Courville. DeeP Learning. Cambridge, USA: The MIT Press,
2016.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR ComPuta-
tional Mathematics and Mathematical Physics, 4(5):1-17, 1964.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by backpropagating
errors. Nature, 323:533-536, 1986.
A. C. Wilson, B. Recht, and M. I. Jordan. A lyapunov analysis of momentum methods in optimiza-
tion. Preprint available at arXiv:1611.02635v4, 2018.
P. Ochs, T. Brox, and T. Pock. IPIASCO: Inertial proximal algorithm for strongly convex optimiza-
tion. Journal of Mathematical Imaging and Vision, 53(2):171-181, 2015.
Y. Nesterov. A method of solving a convex programming problem with convergence O(1/k2).
Soviet Mathematics Doklady, 27(2):372-376, 1983.
P. Ochs, Y. Chen, T. Brox, and T. Pock. IPIANO: Inertial proximal algorithm for nonconvex opti-
mization. SIAM Journal on Imaging Sciences, 7(2):1388-1419, 2014.
W. Su, S. Boyd, and E. Candes. A differential equation for modeling Nesterov's accelerated gradi-
ent method: Theory and insights. In Proc. Advances in Neural Information Processing Systems
(NIPS), 2014.
E. Ghadimi, H. R. Feyzmahdavian, and M. Johansson. Global convergence of the heavy-ball method
for convex optimization. In Proc. EuroPean Control Conference (ECC), Linz, Austria, 2015.
L. Lessard, B. Recht, and A Packard. Analysis and design of optimization algorithms via integral
quadratic constraints. SIAM Journal on OPtimization, 26(1):57-95, 2016.
T. Yang, Q. Lin, and Z Li. Unified convergence analysis of stochastic momentum methods for
convex and non-convex optimization. Preprint available at arXiv:1604.03257v2, 2016.
N. Loizou and P. Richtarik. Momentum and stochastic momentum for stochastic gradient, newton,
proximal point and subspace descent methods. Preprint available at arXiv:1712.09677v2, 2018.
S. Gadat, F. Panloup, and S. Saadane. Stochastic heavy ball. Preprint available at
arXiv:1609.04228v2, 2016.
R. Kidambi, P. Netrapalli, P. Jain, and S. M. Kakade. On the insufficiency of existing momentum
schemes for stochastic optimization. In Proc. International Conference on Learning RePresenta-
tions (ICLR), 2018.
Y. Chen, C. Jin, and B. Yu. Stability and convergence trade-off of iterative optimization algorithms.
Preprint available at arXiv:1804.01619, 2018.
A. Nemirovski and D. B. Yudin. Problem ComPlexity and Method Efficiency in OPtimization. Wiley
Interscience, 1983.
9