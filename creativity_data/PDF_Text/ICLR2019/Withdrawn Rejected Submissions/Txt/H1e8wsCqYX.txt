Under review as a conference paper at ICLR 2019
Laplacian Networks: Bounding Indicator
Function Smoothness for Neural Networks
Robustness
Anonymous authors
Paper under double-blind review
Abstract
For the past few years, Deep Neural Network (DNN) robustness has become
a question of paramount importance. As a matter of fact, in sensitive
settings misclassification can lead to dramatic consequences. Such misclas-
sifications are likely to occur when facing adversarial attacks, hardware
failures or limitations, and imperfect signal acquisition. To address this
question, authors have proposed different approaches aiming at increas-
ing the robustness of DNNs, such as adding regularizers or training using
noisy examples. In this paper we propose a new regularizer built upon the
Laplacian of similarity graphs obtained from the representation of training
data at each layer of the DNN architecture. This regularizer penalizes
large changes (across consecutive layers in the architecture) in the distance
between examples of different classes, and as such enforces smooth variations
of the class boundaries. Since it is agnostic to the type of deformations
that are expected when predicting with the DNN, the proposed regularizer
can be combined with existing ad-hoc methods. We provide theoretical
justification for this regularizer and demonstrate its effectiveness to improve
robustness of DNNs on classical supervised learning vision datasets.
1	Introduction
Deep Neural Networks (DNNs) provide state-of-the-art performance in many challenges
in machine learning (He et al., 2016; Wu et al., 2016). Their ability to achieve good
generalization is often explained by the fact they use very few priors about data (LeCun
et al., 2015). On the other hand, their strong dependency on data may lead to focus on
biased features of the training dataset, resulting in a nonrobust classification performance.
In the literature, authors have been interested in studying the robustness of DNNs in various
conditions. These conditions include:
•	Robustness to isotropic noise, i.e., small isotropic variations of the input (Mallat,
2016), typically meaning that the network function leads to a small Lipschitz
constant.
•	Robustness to adversarial attacks, which can exploit knowledge about the network
parameters or the training dataset (Szegedy et al., 2013; Goodfellow et al., 2014).
•	Robustness to implementation defects, which can result in only approximately correct
computations (Hubara et al., 2017).
To improve DNN robustness, three main families of solutions have been proposed in the
literature. The first one involves enforcing smoothness, as measured by a Lipschitz constant,
in the operators and having a minimum separation margin (Mallat, 2016). A similar approach
has been proposed in (Cisse et al., 2017), where the authors restrict the function of the
network to be contractive. A second class of methods use intermediate representations
obtained at various layers to perform the prediction (Papernot and McDaniel, 2018). Finally,
in (Kurakin et al., 2016; Pezeshki et al., 2016; Madry et al., 2018), the authors propose to
1
Under review as a conference paper at ICLR 2019
train the network using noisy inputs so that it better generalizes to this type of noise. This
has been shown to improve the robustness of the network to the specific type of noise used
during training, but it is not guaranteed that this robustness would be extended to other
types of deformations.
In this work, we introduce a new regularizer that does not focus on a specific type of
deformation, but aims at increasing robustness in general. As such, the proposed regularizer
can be combined with other existing methods. It is inspired by recent developments in Graph
Signal Processing (GSP) (Shuman et al., 2013). GSP is a mathematical framework that
extends classical Fourier analysis to complex topologies described by graphs, by introducing
notions of frequency for signals defined on graphs. Thus, signals that are smooth on the
graph (i.e., change slowly from one node to its neighbors) will have most of their energy
concentrated in the low frequencies.
The proposed regularizer is based on constructing a series of graphs, one for each layer of the
DNN architecture, where each graph captures the similarity between all training examples
given their intermediate representation at that layer. Our proposed regularizer penalizes
large changes in the smoothness of class indicator vectors (viewed here as graph signals) from
one layer to the next. As a consequence, the distances between pairs of examples in different
classes are only allowed to change slowly from one layer to the next. Note that because
we use deep architectures, the regularizer does not prevent the smoothness from achieving
its maximum value, but constraining the size of changes from layer to layer increases the
robustness of the network function by controlling the distance to the boundary region, as
supported by experiments in Section 4.
The outline of the paper is as follows. In Section 2 we present related work. In Section 3
we introduce the proposed regularizer. In Section 4 we evaluate the performance of our
proposed method in various conditions and on vision benchmarks. Section 5 summarizes our
conclusions.
2	Related work
DNN robustness may refer to many different problems. In this work we are mostly interested
in the stability to deformations (Mallat, 2016), or noise, which can be due to multiple factors
mentioned in the introduction. The most studied stability to deformations is in the context
of adversarial attacks. It has been shown that very small imperceptible changes on the
input of a trained DNN can result in missclassification of the input (Szegedy et al., 2013;
Goodfellow et al., 2014). These works have been primordial to show that DNNs may not be
as robust to deformations as the test accuracy benchmarks would have lead one to believe.
Other works, such as (Recht et al., 2018), have shown that DNNs may also suffer from drops
in performance when facing deformations that are not originated from adversarial attacks,
but simply by re-sampling the test images.
Multiple ways to improve robustness have been proposed in the literature. They range
from the use of a model ensemble composed of k-nearest neighbors classifiers for each
layer (Papernot and McDaniel, 2018), to the use of distillation as a mean to protect the
network (Papernot et al., 2016a). Other methods introduce regularizers (Gu and Rigazio,
2014), control the Lipschitz constant of the network function (Cisse et al., 2017) or implement
multiple strategies revolving around using deformations as a data augmentation procedure
during the training phase (Goodfellow et al., 2014; Kurakin et al., 2016; Moosavi Dezfooli
et al., 2016).
Compared to these works, our proposed method can be viewed as a regularizer that penalizes
large deformations of the class boundaries throughout the network architecture, instead of
focusing on a specific deformation of the input. As such, it can be combined with other
mentioned strategies. Indeed, we demonstrate that the proposed method can be implemented
in combination with (Cisse et al., 2017), resulting in a network function such that small
variations to the input lead to small variations in the decision, as in (Cisse et al., 2017),
while limiting the amount of change to the class boundaries. Note that our approach does
2
Under review as a conference paper at ICLR 2019
not require using training data affected by a specific deformation, and our results could be
further improved if such data were available for training, as shown in the Appendix.
As for combining GSP and machine learning, this area has sparked interest recently. For
example, the authors of (Gripon et al., 2018) show that it is possible to detect overfitting by
tracking the evolution of the smoothness of a graph containing only training set examples.
Another example is in (Anirudh et al., 2017) where the authors introduce different quantities
related to GSP that can be used to extract interpretable results from DNNs. In (Svoboda
et al., 2018) the authors exploit graph convolutional layers (Bronstein et al., 2017) to increase
the robustness of the network.
To the best of our knowledge, this is the first use of graph signal smoothness as a regularizer
for deep neural network design.
3	Methodology
3.1	Similarity preset and postset graphs
Consider a deep neural network architecture. Such a network is obtained by assembling layers
of various types. Of particular interest are layers of the form x' → x'+1 = h'(W'x' + b'),
where h' is a nonlinear function, typically a ReLU, W' is the weight tensor at layer ', x' is
the intermediate representation of the input at layer ` and b` is the corresponding bias tensor.
Note that strides or pooling may be used. Assembling can be achieved in various ways:
composition, concatenation, sums. . . so that we obtain a global function f that associates an
input tensor x0 to an output tensor y = f(x0).
When computing the output y associated with the input x0, each layer ` of the architecture
processes some input x' and computes the corresponding output y' = h'(W'x' + b').
For a given layer ` and a batch of b inputs X = {x1, . . . , xb}, we can obtain two sets
X' = {x'1 , . . . , x'b}, called the preset, and Y' = {y1' , . . . , yb'}, called the postset.
Given a similarity measure s on tensors, from a preset we can build the similarity preset
matrix: M'pre [i, j] = s(xi', x'j), ∀1 ≤ i, j ≤ b, where M[i, j] denotes the element at line i and
column j in M. The postset matrix is defined similarly.
Consider a similarity (either preset or postset) matrix M'. This matrix can be used to build
a k-nearest neighbor similarity weighted graph G' = hV, A'i, where V = {1, . . . , b} is the set
of vertices and A' is the weighted adjacency matrix defined as:
(M'[i,j] if M'[i,j] ∈ argmaxio=j (M'[i0,j],k)
A'[i, j] = <	U arg maxj0 6=i (M [i, j ], k) , ∀i, j ∈ V, (1)
(0	otherwise
where arg maxi (ai, k) denotes the indices of the k largest elements in {a1, . . . , ab}. Note that
by construction A' is symmetric.
3.2	Smoothness of label signals
Given a weighted graph G' = hV, A'i, we call Laplacian of G' the matrix L' = D' - A' ,
where D' is the diagonal matrix such that: D'[i, i] = j A'[i,j], ∀i ∈ V . Because L' is
symmetric and real-valued, it can be written:
L' = F'Λ'F'>,	(2)
where F is orthonormal and contains eigenvectors of L' as columns, F> denotes the transpose
of F, and Λ is diagonal and contains eigenvalues of L' is ascending order. Note that the
constant vector 1 ∈ Rb is an eigenvector of L' corresponding to eigenvalue 0. Moreover, all
eigenvalues of L' are nonnegative. Consequently, 1∕√n can be chosen as the first column in
F.
Consider a vector S ∈ Rb, we define S the Graph Fourier Transform (GFT) of S on G' as
(Shuman et al., 2013):
S = F>s.	⑶
3
Under review as a conference paper at ICLR 2019
Because the order of the eigenvectors is chosen so that the corresponding eigenvalues are
in ascending order, if only the first few entries of S are nonzero that indicates that S is low
frequency (smooth). In the extreme case where only the first entry of S is nonzero We have
that s is constant (maximum smoothness). More generally, smoothness σ'(s) of a signal S
can be measured using the quadratic form of the Laplacian:
bb
σ'(s) = s>L's = X A'[i,j](s[i] -Sj])2 = XΛ'[i,i]S[i]2,	(4)
i,j=1	i=1
where We note that S is smoother when σ'(S) is smaller.
In this paper we are particularly interested in smoothness of the label signals. We call label
signal Sc associated with class c a binary ({0, 1}) vector whose nonzero coordinates are the
ones corresponding to input vectors of class c. In other words, Sc [i] = 1 ⇔ (xi is in class
c), ∀1 ≤ i ≤ b. Using Equation (4), we obtain that the smoothness of the label signal Sc is
the sum of similarities between examples in distinct classes. Thus a smoothness of 0 means
that examples in distinct classes have 0 similarity.
Denote u the last layer of the architecture: yiu = yi , ∀i. Note that in typical settings, where
outputs of the networks are one-hot-bit encoded and no regularizer is used, at the end of
the learning process it is expected that yi>yj ≈ 1 if i and j belong to the same class, and
yi> yj ≈ 0 otherwise.
Thus, assuming that cosine similarity is used to build the graph, the last layer smoothness
for all c would be σpuost (Sc) ≈ 0, since edge weights between nodes having different labels will
be close to zero given Equation (4). More generally, smoothness of Sc at the preset or postset
of a given layer measures the average similarity between examples in class c and examples in
other classes (σ (Sc) decreases as the weights of edges connecting nodes in different classes
decrease). Because the last layer can achieve σ(Sc) ≈ 0, we expect the smoothness metric σ
at each layer to decrease as we go deeper in the network. Next we introduce a regularization
strategy that limits how much σ can decrease from one layer to the next and can even prevent
the last layer from achieving σ(Sc) = 0. This will be shown to improve generalization and
robustness. The theoretical motivation for this choice is discussed in Section 3.4.
3.3	Proposed regularizer
3.3.1	Definition
We propose to measure the deformation induced by a given layer ` in the relative positions
of examples by computing the difference between label signal smoothness before and after
the layer, averaged over all labels:
δσ =X [σPost(Sc)- σPre(Sc)].	⑸
c
These quantities are used to regularize modifications made to each of the layers during the
learning process.
Remark 1: Since we only consider label signals, we solely depend on the similarities between
examples that belong to distinct classes. In other words, the regularizer only focuses on the
boundary region, and does not vary if the distance between examples of the same label grows
or shrinks. This is because forcing similarities between examples of a same class to evolve
slowly could prevent the network to train appropriately.
Remark 2: Compared with (Cisse et al., 2017), there are three key differences that characterize
the proposed regularizer:
1.	Not all pairwise distances are taken into account in the regularization; only distances
between examples corresponding to different classes play a role in the regularization.
2.	We allow a limited amount of both contraction and dilation of the metric space.
Experimental work (e.g. (Gripon et al., 2018; Papernot and McDaniel, 2018)) has
4
Under review as a conference paper at ICLR 2019
Class domains boundary
Initial problem: OOO XXX OO O
i)	No regularization:
Figure 1: Illustration of the effect of our proposed regularizer. In this example, the goal is to
classify circles and crosses (top). Without use of regularizers (bottom left), the resulting
embedding may considerably stretch the boundary regions (as illustrated by the irregular
spacing between the tics). Forcing small variations of smoothness of label signals (bottom
right), we ensure the topology is not dramatically changed in the boundary regions.
ii)	Proposed regularization:
, J O O O
shown that the evolution of metric spaces across DNN layers is complex, and thus
restricting ourselves to contractions only could lead to lower overall performance.
3.	The proposed criterion is an average (sum) over all distances, rather than a stricter
criterion (e.g. Lipschitz), which would force each pair of vectors (xi, xj) to obey the
constraint.
Illustrative example:
In Figure 1 we depict a toy illustrative example to motivate the proposed regularizer. We
consider here a one-dimensional two-class problem. To linearly separate circles and crosses, it
is necessary to group all circles. Without regularization (setting i)), the resulting embedding
is likely to increase considerably the distance between examples and the size of the boundary
region between classes. In contrast, by penalizing large variations of the smoothness of label
signals (setting ii)), the average distance between circles and crosses must be preserved in
the embedding domain, resulting in a more precise control of distances within the boundary
region.
3.4	Motivation: label signal bandwidth and powers of the Laplacian
Recent work (Anis et al., 2017) develops an asymptotic analysis of the bandwidth of label
signals, BW (s), where bandwidth is defined as the highest non-zero graph frequency of s,
i.e., the nonzero entry of S with the highest index. An estimate of the bandwidth can be
obtained by computing:
BWm(s)
(s>Lms ʌ
〈s>s )
(1/m)
(6)
for large m. This can be viewed as a generalization of the smoothness metric of (4). (Anis
et al., 2017) shows that, as the number of labeled points x (assumed drawn from a distribution
p(x)) grows asymptotically, the bandwidth of the label signal converges in probability to the
supremum of p(x) in the region of overlap between classes. This motivates our work in three
ways.
First, it provides theoretical justification to use σ'(s) for regularization, since lower values of
σ'(s) are indicative of better separation between classes. Second, the asymptotic analysis
suggests that using higher powers of the Laplacian would lead to better regularization, since
estimating bandwidth using BWm (s) becomes increasingly accurate as m increases. Finally,
this regularization can be seen to be protective against specializing by preventing σ'(s)
5
Under review as a conference paper at ICLR 2019
Figure 2: Sample of a Laplacian and squared Laplacian of similarity graphs in a trained
vanilla architecture. Examples of the batch have been ordered so that those belonging to a
same class are consecutive. Dark values correspond to high similarity.
000 000 000
,,,
642
ssenhtoomS
Layer depth
ssenhtoomS
5	10	15
Layer depth
2
Figure 3: Evolution of smoothness of label signals as a function of layer depth, and for
various regularizers and choice of m, the power of the Laplacian matrix.
from decreasing “too fast”. For most problems of interest, given a sufficiently large amount
of labeled data available, it would be reasonable to expect the bandwidth of s not to be
arbitrarily small, because the classes cannot be exactly separated, and thus a network that
reduces the bandwidth too much can result in being biased by the training set.
3.5 Analysis of the Laplacian powers
In Figure 2 we depict the Laplacian and squared Laplacian of similarity graphs obtained
at different layers in a trained vanilla architecture. On the deep layers, we can clearly see
blocks corresponding to the classes, while the situation in the middle layer is not as clear.
This figure illustrates how using the squared Laplacian helps modifying the distances to
improve separation. Note that we normalize the squared Laplacian values by dividing them
by the highest absolute value.
In Figure 3, we plot the average evolution of smoothness of label signals over 100 batches, as
a function of layer depth in the architecture, and for different choices of the regularizer. In
the left part, we look at smoothness measures using the Laplacian. In the right part, we use
the squared Laplacian. We can clearly see the effectiveness of the regularizer in enforcing
small variations of smoothness across the architecture. Note that for model regularized
with L2 , changes in smoothness measured by L are not easy to see. This seems to suggest
that some of the gains achieved via L2 regularization come in making changes that would
be “invisible” when looking at the layers from the perspective of L smoothness. The same
normalization from Figure 2 is used for L2 .
4 Experiments
In the following paragraphs we evaluate the proposed method using various tests. We use
the well known CIFAR-10 (Krizhevsky and Hinton, 2009) dataset made of tiny images. As
far as the DNN is concerned, we use the same PreActResNet (He et al., 2016) architecture
for all tests, with 18 layers. All inputs, including those on the test set, are normalized based
on the mean and standard deviation of the images of the training set. In all figures, P are
6
Under review as a conference paper at ICLR 2019
SNR ≈ ∞
100
80
60
40
20
0
R	PR	V	P
SNR ≈ 20
SNR ≈ 15
Figure 4: Test set accuracy under Gaussian noise with varying signal-to-noise ratio.
Parseval trained networks, R are networks trained with the proposed regularizer and V are
vanilla networks. More details and experiments can be found at the Appendix.
We depict the obtained results using box plots where data is aggregated from 10 different
networks corresponding to different random seeds and batch orders. In the first experiment
(left most plot) in Figure 4, we plot the baseline accuracy of the models on the clean test set
(no deformation is added at this point). These experiments agree with the claim from (Cisse
et al., 2017) where the authors show that they are able to increase the performance of the
network on the clean test set. We observe that our proposed method leads to a minor
decrease of performance on this test. However, we see in the following experiments that this
is mitigated with increased robustness to deformations. Such a trade-off between robustness
and accuracy has already been discussed in the literature (Fawzi et al., 2018).
4.1	Isotropic deformation
In this scenario we evaluate the robustness of the network function to small isotropic variations
of the input. We generate 40 different deformations using random variables N(0, 0.25) which
are added to the test set inputs. Note that they are scaled so that SNR ≈ 15 and SNR ≈ 20.
The middle and right-most plots from Figure 4 show that the proposed method increases the
robustness of the network to isotropic deformations. Note that in both scenarios the best
results are achieved by combining Parseval training and our proposed method (lower-most
box on both figures).
4.2	Adversarial Robustness
We next evaluate robustness to adversarial inputs, which are specifically built to fool the
network function. Such adversarial inputs can be generated and evaluated in multiple
ways. Here we implement two approaches: first a mean case of adversarial noise, where the
adversary can only use one forward and one backward pass to generate the deformations, and
second a worst case scenario, where the adversary can use multiple forward and backward
passes to try to find the smallest deformation that will fool the network.
For the first approach, we add the scaled gradient sign (FGSM attack) on the input (Kurakin
et al., 2016), so that we obtain a target SNR = 33. Results are depicted in the left and center
plots of Figure 5. In the left plot the noise is added after normalizing the input whereas on
the middle plot it is added before normalizing. As in the isotropic noise case, a combination
of the Parseval method and our proposed approach achieves maximum robustness.
In regards to the second approach, where a worst case scenario is considered, we use the
Fo olbox toolbox (Rauber et al., 2017) implementation of DeepFool (Moosavi Dezfooli et al.,
2016). Due to time constraints We sample only a of the test set images for this test. The
conclusions are similar (right plot of Figure 5) to those obtained for the first adversarial
attack approach.
4.3	Implementation robustness
Finally, in a third series of experiments We evaluate the robustness of the netWork functions
to faulty implementations. As a result, approximate computations are made during the
7
Under review as a conference paper at ICLR 2019
60
40
20
FGSM after norm
V	P	R	PR
FGSM before norm
DeepFool
Figure 5: Robustness against an adversary measured by the test set accuracy under FGSM
attack in the left and center plots and by the mean L2 pixel distance needed to fool the
network using DeepFool on the right plot.
100
25% dropout
80
60
40
20
0	V	PR	P	R
100
40% dropout
80
60
40
20
0	PR	P	V	R
80
60
40
20
5 bit quantization
100
0	V	PR	P	R
0
Figure 6: Test set accuracy under different types of implementation related noise.
test phase that consist of random erasures of the memory (dropout) or quantization of the
weights (Hubara et al., 2017).
In the dropout case, we compute the test set accuracy when the network has a probability of
either 25% or 40% of dropping a neuron’s value after each block. We run each experiment
40 times. The results are depicted in the left and center plots of Figure 6. It is interesting to
note that the Parseval trained functions seem to drop in performance as soon as we reach
40% probability of dropout, providing an average accuracy smaller than the vanilla networks.
In contrast, the proposed method is the most robust to these perturbations.
For the quantization of the weights, we consider a scenario where the network size in memory
has to be shrink 6 times. We therefore quantize the weights of the networks to 5 bits (instead
of 32) and re-evaluate the test set accuracy. The right plot of Figure 6 shows that the
proposed method is providing a better robustness to this kind of deformation than the tested
counterparts.
5 Conclusion
In this paper we have introduced a new regularizer that enforces small variations of the
smoothness of label signals on similarity graphs obtained at intermediate layers of a deep
neural network architecture. We have empirically shown with our tests that it can lead
to improved robustness in various conditions compared to existing counterparts. We also
demonstrated that combining the proposed regularizer with existing methods can result in
even better robustness for some conditions.
Future work includes a more systematic study of the effectiveness of the method with regards
to other datasets, models and deformations. Recent works shown adversarial noise is partially
transferable between models and dataset (Moosavi-Dezfooli et al., 2017; Papernot et al.,
2016b) and therefore we are confident about the generality of the method in terms of models
and datasets.
One possible extension of the proposed method is to use it in a fine-tuning stage, combined
with different techniques already established on the literature. An extension using a combi-
nation of input barycenter and class barycenter signals instead of the class signal could be
interesting as that would be comparable to (Zhang et al., 2017). In the same vein, using
random signals could be beneficial for semi-supervised or unsupervised learning challenges.
8
Under review as a conference paper at ICLR 2019
References
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep
residual networks. In European Conference on Computer Vision, pages 630-645. Springer,
2016.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural
machine translation system: Bridging the gap between human and machine translation.
arXiv preprint arXiv:1609.08144, 2016.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436,
2015.
Stephane Mallat. Understanding deep convolutional networks. Phil. Trans. R. Soc. A, 374
(2065):20150203, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Quantized neural networks: Training neural networks with low precision weights and
activations. Journal of Machine Learning Research, 18:187-1, 2017.
Moustapha Cisse, Piotr Bo janowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier.
Parseval networks: Improving robustness to adversarial examples. In International
Conference on Machine Learning, pages 854-863, 2017.
Nicolas Papernot and Patrick D. McDaniel. Deep k-nearest neighbors: Towards confident,
interpretable and robust deep learning. CoRR, abs/1803.04765, 2018. URL http://arxiv.
org/abs/1803.04765.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale.
arXiv preprint arXiv:1611.01236, 2016.
Mohammad Pezeshki, Linxi Fan, Philemon Brakel, Aaron Courville, and Yoshua Bengio.
Deconstructing the ladder network architecture. In International Conference on Machine
Learning, pages 2368-2376, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=rJzIBfZAb.
David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.
The emerging field of signal processing on graphs: Extending high-dimensional data analysis
to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83-98,
2013.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10
classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Security and
Privacy (SP), 2016 IEEE Symposium on, pages 582-597. IEEE, 2016a.
Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to
adversarial examples. arXiv preprint arXiv:1412.5068, 2014.
9
Under review as a conference paper at ICLR 2019
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple
and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
Vincent Gripon, Antonio Ortega, and Benjamin Girault. An inside look at deep neural
networks using graph signal processing. In Proceedings of ITA, February 2018.
Rushil Anirudh, Jayaraman J Thiagarajan, Rahul Sridhar, and Timo Bremer. Influential
sample selection: A graph signal processing approach. arXiv preprint arXiv:1711.05407,
2017.
Jan Svoboda, Jonathan Masci, Federico Monti, Michael M Bronstein, and Leonidas
Guibas. Peernets: Exploiting peer wisdom against adversarial attacks. arXiv preprint
arXiv:1806.00088, 2018.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18-42, 2017.
Aamir Anis, Aly El Gamal, Salman Avestimehr, and Antonio Ortega. A sampling theory
perspective of graph-based semi-supervised learning. arXiv preprint arXiv:1705.09518,
2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf, 2009.
Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classifiers’ robustness to
adversarial perturbations. Machine Learning, 107(3):481-508, 2018.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to
benchmark the robustness of machine learning models. arXiv preprint arXiv:1707.04131,
2017. URL http://arxiv.org/abs/1707.04131.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Uni-
versal adversarial perturbations. arXiv preprint, 2017.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine
learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277, 2016b.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond
empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.
Jelena Kovacevic and Amina Chebira. An introduction to frames. Foundations and Trends
in Signal Processing, 2(1):1-94, 2008.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks, 2016.
A Parseval Training and implementation
We compare our results with those obtained using the method described in (Cisse et al., 2017).
There are three modifications to the normal training procedure: orthogonality constraint,
convolutional renormalization and convexity constraint.
For the orthogonality constraint we enforce Parseval tightness (Kovacevic and Chebira, 2008)
as a layer-wise regularizer:
Re (W') = 2 kw'>w'-1 k2,	⑺
where W' is the weight tensor at layer '. This function can be approximately optimized with
gradient descent by doing the operation:
W' 一 (1 + β)W' - βW'W'>W'.
(8)
10
Under review as a conference paper at ICLR 2019
Given that our network is smaller we can apply the optimization to the entirety of the W ,
instead of 30% as per the original paper, this increases the strength of the Parseval tightness.
For the convolutional renormalization, each matrix W' is reparametrized before being applied
to the convolution as √W , where k§ is the kernel size.
2ks +1
For our architecture the inputs from a layer come from either one or two different layers. In
the case where the inputs come from only one layer, α the convexity constraint parameter
is set to 1. When the inputs come from the sum of two layers we use α = 0.5 as the value
for both of them, which constraints our Lipschitz constant, this is softer than the convexity
constraint from the original paper.
B Hyperparameters
We train our networks using classical stochastic gradient descent with momentum (0.9),
with batch size of b = 100 images and using a L2-norm weight decay with a coefficient of
λ = 0.0005. We do a 100 epoch training. Our learning rate starts at 0.1. After half of the
training (50 epochs) the learning rate decreases to 0.001.
We use the mean of the difference of smoothness between successive layers in our loss function.
Therefore in our loss function we have:
L = CategoricalCrossEntropy + λW eightDecay + γ∆
(9)
where ∆ = d--i Pd=ι 星|. We perform experiments using various powers of the Laplacian
m = 1, 2, 3, in which case the scaling coefficient γ is put to the same power as the Laplacian.
We tested multiple parameters of β , the Parseval tightness parameter, γ the weight for the
smoothness difference cost and m the power of the Laplacian. We found that the best values
for this specific architecture, dataset and training scheme were: β = 0.01, γ = 0.01, m =
2, k = b.
C Depiction of the network
Figure 7 depicts the network used on all experiments of sections 3 and 4. f = 64 is the filter
size of the first layer of the network. Conv layers are 3x3 layers and are always preceded by
batch normalization and relu (except for the first layer which receives just the input). The
smoothness gaps are calculated after each ReLU.
Figure 7: Depiction of the studied network
Strided Conv layer, 4f
Conv layer, f
,ided Conv layer, 2f
Conv layer, 2f
Conv layer, 2f
Conv layer, 2f
Conv layer, f
Conv layer, f
Conv layer, f
Conv layer, f
Global Avg pooling, 8f
Strided Conv layer, 8f
Linear+ Softmax ：
Conv layer, 8f
Conv layer, 8f
Conv layer, 8f
Conv layer, 4f
Conv layer, 4f
Conv layer, 4f
10
11
Under review as a conference paper at ICLR 2019
D Additional experiments
Given suggestions from the reviewers, we performed additional experiments to further
demonstrate the capabilities of the proposed regularizer. Due to the lack of space they could
not be added to the main paper. We consider the effects of the regularizer when applied
on another datasets. We also consider the effects of adding adversarial data augmentation
methods while minimizing the amount of other influencing factors. We first look at the
results when using the same architecture as for the CIFAR-10 dataset, which inevitably
results in far from state-of-the-art accuracy on CIFAR-100. Then, we perform experiments
using a different architecture (namely WideResnet 28-10, with dropout) for CIFAR-100.
D.1 CIFAR-10
We add two types of tests for the CIFAR-10 dataset: adversarial data augmentation during
training and black-box FGSM.
D.1.1 Tests with FGSM adversarial data augmentation
In this section we consider tests adding adversarial data augmentation as suggested in (Ku-
rakin et al., 2016). To be more precise we use the method they advise which is called
"step1.1" using e =蔡.The results presented in the figures below are obtained by running
10 experiments with random initializations. We first perform the same tests as in Section 4.
As expected, we observe in Figure 8 that training with adversarial examples help in the case
of Gaussian noise, as it adds more variation to the training set, while reducing the accuracy
on the clean set. Note that combining our method with adversarial training results in the
best median accuracy. Combining the three methods is less successful than expected, which
could indicate that a better hyperparameter search would be needed.
SNR ≈ ∞
100
80
60
40
20
0 AR APR A AP R PR V P
100
80
60
40
20
0
SNR ≈ 20
P V R PR A AP APR AR
SNR ≈ 15
100
80
60
40
20
0
P V R PR APR A AP AR
Figure 8:	Test set accuracy under Gaussian noise with varying Signal-to-Noise Ratio (SNR).
A is for Adversarial, P is for Parseval, R is for the proposed Regularizer and V is for Vanilla
network.
Considering adversarial robustness, the obtained results are depicted in Figure 9. We observe
that adding FGSM adversarial training does not generalize well to other types of attack
(which is readily seen in the literature Madry et al. (2018)). Overall, the models using the
proposed regularizer are the most robust.
Finally, when considering implementation related perturbations, the results depicted in
Figure 10 are consistent with the ones from Section 4.3, in which is shown that the proposed
regularizer helps improving robustness.
In summary, even when adding adversarial training, the proposed regularizer is either the
most robust in median, or capable of improving the robustness when used combined with
the other methods.
12
Under review as a conference paper at ICLR 2019
FGSM after norm
100
80
60
40
20
0
V A AP P R PR AR APR
FGSM before norm
100
80
60
40
20
0 V R P PR A AP AR APR
6
4
2
•10-5	DeepFool
0 AP A V AR APR P R PR
Figure 9:	Robustness against an adversary measured by the test set accuracy under FGSM
attack in the left and center plots and by the mean L2 pixel distance needed to fool the
network using DeepFool on the right plot.
0
25% dropout
100
80
60
40
20
V PR A AP P APR R AR
40% dropout
100
80
60
40
20
0 PR P V R AP A APR AR
5 bit quantization
100
80
60
40
20
A V AP APR PR AR P R
0
Figure 10:	Test set accuracy under different types of implementation related noise.
D.1.2 Tests with black box FGSM
To further verify that the obtained results are not only due to gradient masking, we perform
tests with black box FGSM, where the target attacked network is not the same as the source
of the adversarial noise.
For this test we set the SNR of FGSM to 33. We chose the network with the best performance
for each of the tested methods. The results are depicted in Table 1. In our experiments,
we found that the combination of our method with Parseval is the most robust to noise
coming from other sources, while the noise created by both Parseval and our method did not
generalize as well as the one created by Vanilla. This demonstrates that the improvements
are not caused by gradient masking, but are caused by the increased robustness of the
proposed method and Parseval’s.
Table 1: Black box FGSM applied to the different methods. The most robust target for a
given source is bolded, while the strongest source for a target is in italic.
Target	Source	J			
	Vanilla	Parseval	RegulariZer	Parseval + Regularizer
Vanilla	-X-	60.74-	6149	7251
Parseval	57.82	X	68.21	73.87
Regularizer	69.72	74.96	X	73.56
Parseval + Regularizer	75.35	76.11	702		X	
D.1.3 Tests with PGD adversarial data augmentation
Most of our adversarial tests are performed with FGSM because of its simplicity and speed,
even though it has already been shown (e.g: Madry et al. (2018)) that FGSM is weak as
an attack and as a defense mechanism. Despite the fact we do not only target adversarial
13
Under review as a conference paper at ICLR 2019
defense, we further stress the ability of the proposed regularizer to improve it and to combine
with other methods. To this end we perform experiments against the PGD (Pro jected
Gradient Descent) attack.
PGD is an iterative version of FGSM, which run for a maximum number of iterations it or
until convergence. For each iteration it moves by a distance of step in the direction of the
gradient provided it does not go at a distance greater than from the original image.
Our experiments show that the proposed regularizer increases robustness against a weak PGD
attack (similar epsilon as our FGSM with SNR=33), but it is almost completely defeated by
the PGD with the parameters from (Madry et al., 2018). The results are depicted in table 2.
We also show that, as expected, FGSM training does not add significant robustness against
the stronger PGD attack.
Table 2: Test set accuracy on the CIFAR-10 dataset against the PGD attack with different
parameters.
Model	it =	二 20, step = 0.002, e = 0.01	it	:20, Step = 225 ,e = 255
Vanilla		095%		0.02%
Proposed Regularizer		11.18%		0.09%
FGSM		5.78%		0.09%
FGSM + Regularizer		12.91%		0.55%
As the proposed regularizer can be combined with FGSM defense, it is natural to also test it
alongside PGD training. We use the parameters advised in (Madry et al., 2018): 7 iterations
with step = 2/255, and = 8/255. The results depicted in Table 3 show that using our
regularizer increases robustness of networks trained with PGD. Note that Dropout and
Gaussian Noise were applied ten times to each of the networks and the results are displayed
as the mean test set accuracy under these perturbations. A rate of 40% was used for dropout.
The PGD attack uses the following parameters: it = 20, step =募,E =蔡.
Table 3: Results on the CIFAR-10 with PGD training and the hyperparameters from
Appendix B.
Robustness	Isotropic	Adversarial	Implementation
Model/Test Type	SNR ≈ ∞ SNR ≈ 15	PGD	Dropout
PGD Training PGD Training + Regularizer	-76.39%	71.25%- 76.36%	72.26%	-32.78%- 33.72%	35.20% 55.63%
D.2 CIFAR-100
We test the generality of the method using the CIFAR-100 dataset. Results are shown in
Table 4 as the mean over three different initializations. Dropout and Gaussian Noise are
applied ten times to each of the networks for a total of 30 different runs. An SNR of 33 is
used for FGSM, and a rate of 25% is used for dropout. Images are normalized in the same
way as the experiments with CIFAR-10. Due to time constraints We sample only 吉 of the
images from the test set for the Deep Fool test.
The proposed regularizer is the most robust on all categories, while Parseval has problems with
the perturbations, despite yielding the best accuracy on the clean test set. The combination
of the proposed regularizer and the parseval training method is not able to reproduce the
good results from the CIFAR-10 dataset.
The results shown in Table 4 are obtained using an architecture that is not performing
very well on the clean test set for the CIFAR-100 dataset. We thus performed additional
experiments using the WideResNet 28-10 (Zagoruyko and Komodakis, 2016) architecture,
and we added standard data augmentation (random crops and random horizontal flipping)
and dropout with probability of 30% after the first convolution of each residual block. We
train for 200 epochs, starting with a learning rate of 0.1 and divide the learning rate by 5 in
epochs 60, 120 and 160. Momentum of 0.9 is used and weight decay of 5e-4. We use the
14
Under review as a conference paper at ICLR 2019
Table 4: Results on the CIFAR-100 dataset with the hyperparameters from Appendix B.
Bolded value represent the best model on the test.
Robustness	Isotropic		Adversarial		Implementation
Model/Test Type	SNR ≈ ∞	SNR ≈ 15	FGSM	Deep Fool	Dropout
Vanilla	62.38%	-12.78%-	5.70%	1.7E-5	866%
Parseval	63.61%	10.11%	5.85%	1.5E-5	10.61%
Proposed RegulariZer	60.06%	21.14%	6.15%	2.9E-5	21.40%
Proposed + Parseval	56.64%	20.01%	4.07%	1.8E-5	9.41%
value from the Parseval paper (β = 0.0003) as in this case it provided better results than the
one described in Section B.
Results on the WideResNet 28-10 architecture using data augmentation are shown in Table 5.
We observe that the proposed method (sometimes with combinations with other methods) is
still the most robust.
Table 5: Results on the CIFAR-100 dataset with WideResNet 28-10.
Robustness	Isotropic		Adversarial		Implementation
Model/TeSt TyPe	SNR ≈ ∞	SNR ≈ 15	FGSM	Deep Fool	Quantization
Vanilla	78.42%	-11.68%-	21.38%	5.3E-5	12.56%
Parseval	77.71%	12.75%	22.73%	5.7E-5	1.58%
Proposed Regularizer	77.33%	14.46%	23.27%	5.8E-5	17.01%
Proposed + Parseval	76.72%	20.24%	25.85%	6.9E-05	1.0%
E Impact of the proposed regularizer on the boundary
We look at the impact of the proposed regularizer on the boundary region. To this end, we
choose 10 pairs of points in distinct classes that are the most similar (i.e. their distance is
minimal) in the input space and we look at the decision of the network function along the
segment between them. The average is depicted in Figure 11. Note that the point to the left
is always chosen to be the one corresponding to the decision of the network at the middle of
the segment, so that the average curve is asymmetric.
1(+xλ(
1
0.5
0
-0.5
-1
0	0.2	0.4	0.6	0.8	1
λ
Figure 11: F (λx + (1 - λ)x0) for different methods.
Interestingly, we observe that the proposed regularizer is the one for which the boundary is
closest to the middle of the segments, thus proving our claim that the proposed regularizer
control the boundary region.
15
Under review as a conference paper at ICLR 2019
F Regularizer pseudo-code
Below in Algorithm 1 we describe how we use the proposed regularizer to compute the loss
as a pseudo-code. This function receives five inputs:
1.	listactivations : the list of the intermediate features right after each call of the ReLU
activation function of the network. We call these intermediate features activations'
where ` represents the depth of the network;
2.	y: the output of the network;
3.	s: the label signal of the batch. Otherwise said, the ground truth labels of the
examples of the batch;
4.	m: the power of the Laplacian for which we wish to compute the smoothness;
5.	γ: the scaling coefficient of the regularizer loss.
Algorithm 1: Loss function of the regularized network
1:
2:
3:
4:
5:
6:
7:
8:
L
9:
procedure Smoothness (activations', s, m)
A' — Pairwise cosine similarity of activations'
d' — Diagonal degree matrix of A'
L' — D' - A'
σ' — TraCe(sl(L')ms)
return σ'
procedure Loss(listactivations, y, s, m, γ)
for activations' ∈ listactivations do
σ' J Smoothness(activations', s, m)
∆ J P⅛ax ∣σi-σi-1∣
A J	'max-1
10:	return CategoricalCrossEntropy(s, y) + γm ∆
16