Under review as a conference paper at ICLR 2019
Learning with Reflective Likelihoods
Anonymous authors
Paper under double-blind review
Ab stract
Models parameterized by deep neural networks have achieved state-of-the-art re-
sults in many domains. These models are usually trained using the maximum
likelihood principle with a finite set of observations. However, training deep prob-
abilistic models with maximum likelihood can lead to the issue we refer to as input
forgetting. In deep generative latent-variable models, input forgetting corresponds
to posterior collapse—a phenomenon in which the latent variables are driven in-
dependent from the observations. However input forgetting can happen even in
the absence of latent variables. We attribute input forgetting in deep probabilis-
tic models to the finite sample dilemma of maximum likelihood. We formalize
this problem and propose a learning criterion—termed reflective likelihood—that
explicitly prevents input forgetting. We empirically observe that the proposed cri-
terion significantly outperforms the maximum likelihood objective when used in
classification under a skewed class distribution. Furthermore, the reflective like-
lihood objective prevents posterior collapse when used to train stochastic auto-
encoders with amortized inference. For example in a neural topic modeling ex-
periment, the reflective likelihood objective leads to better quantitative and quali-
tative results than the variational auto-encoder and the importance-weighted auto-
encoder.
1	Introduction
Learning deep probabilistic models with maximum likelihood has led to many successes in den-
sity estimation, variational inference, and text and image generation (Dinh et al., 2016; Kingma &
Dhariwal, 2018; Kingma & Welling, 2013; Rezende et al., 2014; Oord et al., 2016). However, max-
imum likelihood learning of deep models often causes the problem of input forgetting—so called
because it corresponds to ignoring the input. We refer to an input here as any variable being condi-
tioned upon—for example a covariate in supervised learning or a latent variable in deep generative
latent variable models. Input forgetting corresponds to latent variable collapse in the context of
latent variable models and has been discussed in several works (Burda et al., 2015; Bowman et al.,
2015; Hoffman & Johnson, 2016; Chen et al., 2016; S0nderby et al., 2016; Zhao et al., 2017; Alemi
et al., 2018; Dieng et al., 2018a). It also occurs when learning with Restricted Boltzmann Machines
(RBMs) where all the hidden units of the RBM collapse by learning to only capture the bias in the
visible units (Cho et al., 2011). Input forgetting makes posterior inference in deep generative models
very difficult. In this paper we study input forgetting when the input is a latent variable and when
the input is a covariate.
1.1	Contributions
In this paper, we propose a learning criterion that does not suffer from the input forgetting problem
induced by maximum likelihood when fitting deep probabilistic models. This criterion naturally
arises from what we call the finite sample dilemma of maximum likelihood.
The finite sample dilemma of maximum likelihood. Consider a supervised learning setting where
we have two random variables x and y. Our goal is to learn the relationship between these two ran-
dom variables by fitting a conditional parametric model pθ (y | x). The learning problem corresponds
to finding θ* that minimizes the risk function,
R(θ)= Ex 〜p*(χ) [Ey 〜p*(y | χ) [l(y,fθ (X))]],	⑴
1
Under review as a conference paper at ICLR 2019
where p* denotes the population distribution and l(∙, ∙) isaloss function that measures how different
fθ(x) is from the true y. Maximum likelihood corresponds to a log loss,
l(y,fθ(x)) = - logpθ(y | x) and Rmie(θ) = -Ex〜p*(x)[Ey〜p*(y | x) [logPθ(y | x)]] . (2)
In practice we approximate Rmle(θ) using a finite dataset D = (xn, yn)nN=1 that contains N realiza-
tions of the random variables x and y. The sample-based maximum likelihood objective is
1N
L(θ; D) = N ɪ^log Pθ (yn|xn).	(3)
Now consider the risk of maximum likelihood under the same conditional model pθ(y | x) when x
and y are independent,
R mle(θ) = -Ex〜p*(x) [Ey〜p*(y) [logpθ(y | x)] .	(4)
The corresponding sample-based maximum likelihood objective is
1N1N
L0(θ;D) = NflN Ilogpθ(yn|xn).	⑸
-	_	.	~ Z ,	-、	-	~, Z ,—、.	.	,	.	.	...
The objectives L(θ; D) and L0(θ; D) in Eq. 3 and Eq. 5 are related as follows
NN
L(θ; D) = N ∙ L0(θ; D) - Nff log pθ (yn∣xn).	(6)
n=1 n0 6=n
Although there is no relationship between the two population risks Rmle(θ) and R0mle(θ), there is
a relationship between their sample-based estimates.
When maximizing the likelihood under finite data—i.e. maximizing L(θ; D)—the term L0(θ; D) is
also being maximized. However maximizing L0(θ; D) leads to input forgetting as the underlying
data being used do not encode any relationship between x and y. To see this consider maximum
likelihood with the same data but where each label y is also paired with all possible inputs x. This
version of the data does not contain any relationship between x and y. Maximum likelihood using
this data where x and y are independent corresponds to maximizing L0(θ; D).
A new learning criterion for supervised learning. In light of the relationship in Eq. 6, we propose
to explicitly discourage the independence behavior induced by maximizing L0(θ; D). We do so by
maximizing the reflective likelihood (RLL). Maximizing the RLL simultaneously maximizes the
likelihood of the outputs under their true associated inputs and minimizes the average log-probability
of these outputs when paired with all other possible inputs. On a classification under imbalance
with the MNIST dataset, the classifiers trained with RLL outperform those trained with the usual
maximum likelihood objective in all the instances of class imbalance. The performance were similar
when the class distribution is uniform.
A new family of stochastic auto-encoders for unsupervised learning.
We extend the proposed RLL to unsupervised learning with stochastic auto-encoders and amor-
tized inference. We call stochastic auto-encoders trained with the RLL objective Reflective Auto-
Encoders (RAEs). RAEs do not suffer from the issue of posterior collapse—a phenomenon in which
the estimated approximate posterior distribution of the latent variables reduces to the prior. We eval-
uate RAEs on a neural topic modelling experiment and found they outperform models learned with
the maximum likelihood objective both quantitatively and qualitatively.
1.2	Related Work.
Our work is closely related to two lines of work: penalized maximum likelihood and posterior
inference in deep generative models.
Traditional penalized maximum likelihood methods such as the Lasso and L2-norm regularization
directly operate on the parameters ofa model (Tibshirani, 1996; Hinton, 1987; Louizos et al., 2017).
2
Under review as a conference paper at ICLR 2019
These regularizers penalize the magnitude of the parameters and correspond to specific prior distri-
butions on the parameters from the Bayesian perspective. In contrast several data-dependent regu-
larizers have been proposed. As early as 1995, Bishop (1995) proposed to add noise to the input
when training a neural network with stochastic gradient descent for maximum likelihood learning
and showed this corresponds to a form of Tikhonov regularization. Several works have extended this
to noise injection in the hidden units ofa neural network (Srivastava et al., 2014; Maaten et al., 2013;
Gal & Ghahramani, 2016; Wager et al., 2013; Dieng et al., 2018b). Our work relates to those data-
dependent regularizers that are explicit—in the sense that the corresponding objective function can
be written as the sum of the initial objective function and an additional regularization term.
Our work also relates to the literature on posterior inference in deep generative models. Latent
variable models are ubiquitous in machine learning. They often involve intractable integrals that
are approximated with Markov chain Monte Carlo or variational inference. Our work relates to
variational methods for deep generative models (Kingma & Welling, 2013; Rezende et al., 2014).
The problem that often occurs in these settings is the latent variables are driven independent to
the observations thus rendering posterior inference meaningless. Several works have discussed this
issue (Burda et al., 2015; Bowman et al., 2015; Hoffman & Johnson, 2016; Chen et al., 2016; S0n-
derby et al., 2016; Zhao et al., 2017; Alemi et al., 2018; Dieng et al., 2018a). In this paper we
identify a possible cause to this problem and adopt a regularization approach to fix it.
2	Supervised Learning with Reflective Likelihoods
We consider the same setting as Section 1.1. We have observations (xn, yn)nN=1 and our goal is to
fit a parametric model pθ (y | x) to this data. As stated in Section 1.1, the usual maximum likelihood
objective might lead to parameters θ that do not capture the true relationship between x and y.
In this section we propose the RLL criterion that encourages parameter settings under which y is
more likely when conditioning on its corresponding x than under the average of all the conditional
probabilities of y given all possible inputs. The RLL criterion has two forms.
Weak reflective likelihood. The weaker form of RLL minimizes L0 (θ; D) while maximizing the
usual sample-based approximation L(θ; D),
1N
LRLL(θ; D) = nn X
n=1
N
log Pθ(fn | Xn) - -n E log Pθ Wn | XnO),
n0=1
(7)
where αn ≥ 0 is a hyperparameter trading off these two terms. We will discuss how to set αn
at the end of this section. The objective LRLL (θ; D) provides Us with a way to explicitly prefer
Rmie(θ) in Eq.(1) over R0mie(θ) in Eq.(4), over an entire training set. The objective LRLL(θ; D),
however, may capture the dependence between the input and the output for some examples but not
necessarily for all examples. This is because the second term in Eq. 7 can be driven to its minimal
value by setting the term inside the log to zero for one example. We tackle this issue by proposing a
stronger version of RLL.
Strong reflective likelihood. The stronger version of RLL favors parameters that capture the rela-
tionship between X and y by discouraging the independence behavior induced by maximization of
~.. .
L0(θ; D),
1N
L船(θ; D) = IN X
n=1
1N
log Pθ (yn | Xn) - an log NE Pθ Bn | X〃)∙
n0=1
(8)
The second term comes from an application of Jensen’s inequality,
R0mle(θ) ≤ Ey~p*(y) [log Ex〜p*(x) [pθ(y I χ)]],
and using the dataset D to approximate this upper bound. The inner term of this upper bound is what
we call a reflective probability,
pθfl(y) = Ex〜p*(x)Pθ (y I x).	(9)
3
Under review as a conference paper at ICLR 2019
This reflective probability can be used as a diagnostic measure of whether a trained model has cap-
tured the dependence between the input and output for each pair. The sample-based approximation
ofprθefl(y) is what appears inside the logarithm in the second term in Eq. 8.
Unlike LRLL,this stronger objective function LRLL explicitly decreases the reflective probability of
each individual example (x, y). From here on out we focus on LRLL and refer to it as RLL.
Stochastic Approximation of RLL. The objective in Eq. 8 is not amenable to large datasets.
First, when the sample size N is large computing LRLng (θ; D) is intractable. Second even if we use
a small subset of data we are still left with approximating prθefl(y) which is also intractable for large
N. To enable maximizing RLL with large datasets, we propose to maximize
LRLL(θ; M, M0) = ∣mM∣ X	logpθ(y i X)-
(x,y)∈M
αn log ∣M10)r	X	pθ(y I x0)
(x0,y0)∈M0
(10)
where M and M0 are two minibatch of data samples. In practice we improve the efficiency by
constructing the second minibatch M0 using the first minibatch M. For each pair (X, y) ∈ M, we
uniformly select K examples from M at random to construct M0 .
Choice of penalty αn. We investigate two strategies to set the cofficient αn . The first strategy
simply sets αn to a global fixed coefficient αn = α0 , where α0 is a hyperparameter. The second
strategy on the other hand dynamically adapts αn for each training example (Xn, yn) according
to:
αn = α(Xn, yn,θ) = k0 OfthorwiseynIXn)	PrMyn)
(11)
The second strategy is motivated by the intuition that the probability assigned to a correctly paired
example (Xn , yn ) by a model pθ should be higher than the average of the probabilities assigned
to incorrectly paired examples (x, Nn) for all X 〜p*(x). It is however unclear how much higher
the probability of a correctly paired example pθ (yn I Xn) should be over the reflective probability
prθefl(yn), and we do not want to increase the difference if the former is already greater than the
latter.
3	Unsupervised Learning with Reflective Likelihoods
We extend RLL to unsupervised learning. We do so using the auto-encoding framework (Hinton &
Salakhutdinov, 2006; Vincent et al., 2008; Kingma & Ba, 2014; Rezende et al., 2014). We first dis-
cuss stochastic auto-encoders and posterior collapse before proposing RAEs—a family of stochastic
auto-encoders that do not suffer from the posterior collapse issue.
Stochastic auto-encoders and posterior collapse. We position ourselves in the setting where
there are global parameters θ and one latent variable z for every observation y. An observation y
is generated by first drawing a latent variable z from some prior distribution p(z)—that we assume
fixed—and then sampling y from the conditional distribution of y given z. This conditional dis-
tribution pθ(y I z) is parameterized by θ. We are concerned with learning the parameters θ in the
presence of the latent variables. Maximum likelihood corresponds to maximizing
L(θ; p*) = Ey〜p*(y) logPθ (y) = Ey〜p*(y)
log
pθ(y I z)p(z)dz
(12)
z
The likelihood pθ (y I z) is represented as a powerful deep neural network that takes z as input. This
objective Eq. 12 is intractable. Stochastic auto-encoders posit a distribution qφ(z I y) over the latents
and either maximize (Burda et al., 2015)
L(θ,φ; p") = Ey 〜p*(y) log EZ 〜qφ(z | y) Pθ∖ ：(Z ； y) ) [] ,	(13)
4
Under review as a conference paper at ICLR 2019
or its lower bound (Kingma & Ba, 2014; Rezende et al., 2014)
L(θ, φ; p*) = Ey 〜p*(y) EZ 〜qφ(z | y) log(Z ； y)，]] .	(14)
The distribution qφ(z | y) is parameterized by a deep neural network that takes the observation yas
input.
As discussed in several works, the objective in Eq. 14 is prone to posterior collapse. It can be
rewritten as
Lvae(θ,φ;p*) = Ey〜p*(y) [Ez〜qφ(z | y) logPθ( | Z)- KL(qφ(z | y)∣∣p(z))] ,	(15)
What effectively happens is that the KL term quickly collapses to zero early on during optimization,
leading to an approximate posterior that does not capture the data.
The importance-weighted auto-encoder (IWAE, Burda et al., 2015) was introduced to learn more
expressive approximate posteriors by directly optimizing the objective in Eq. (13) using K samples
from the approximate posterior,
Liwae(θ;P*) = Eye[log K XX [p⅛⅛⅛F]	(16)
Burda et al. (2015) show this leads to less posterior collapse. However, the IWAE can still lead
to posterior collapse due to the finite sample dilemma of maximum likelihood discussed in Sec-
tion 1.
Reflective auto-encoders. We propose a new class of stochastic auto-encoders that prevent pos-
terior collapse by maximizing the RLL. We reconsider the original population risk functions for
maximum likelihood under both paired data,
Rmle(θ) = -Ex〜p*(x) [Ey〜p*(y | x) [logpθ(y | X)]]
and its upper bound under independent data,
Rmle(θ) ≤ Ey~p*(y) [log Eχ~p*(x) [pθ(y | X)]] .
The RLL in Eq. 8 is a sample-based approximation of
LRLL (θ; P*) = Ex 〜p*(x)Ey 〜p*(y | x) [log Pθ ( | x) - α log Ex，〜p*(x，)[Pθ (y | X0)]].
Note this is simply the difference between the original maximum likelihood objective and the log
reflective probability. In the case of unsupervised learning this corresponds to
LRnLIp(θ) = Ey〜p*(y) [logPθ(y) - αlogEy，〜p*(y) [Pθ(y | y0)]] ,	(17)
where we define pθ (y | y0) as
pθ (y I y0) = / pθ (y I z)pθ (Z | y0)dz = Epθ (z । yο)[Pθ (y | z)].
Using this definition we rewrite the objective as
LRLUp(θ) = Ey〜p*(y) [logEZ〜p(z)Pθ(y | z) - αlogEy，〜p*(y)Epθ(z ∣ y0)[Pθ(y I z)]] .	(18)
The expectations in Eq. 18 are intractable. In this paper, we propose to approximate them using
importance weighting (Burda et al., 2015). For that we define a parametric proposal distribution
qφ(z I y)—we make the conditioning on y explicit to account for recognition networks as proposal
distributions. We now write the different expectations as
1K
EZ〜p(z) [Pθ(y I z)] ≈ Pθ(y) = κE"(y,zk)pθ(y∣zk) where zk 〜qφ(z ∣ y);
k=1
and Ey，〜p* (y)EZ〜pθ(z | y0) [pθ (y | z)] ≈ Ey，〜p* (y) ()： v(y', Zk )pθ (y | Zk ))
5
Under review as a conference paper at ICLR 2019
where Zk 〜 qφ(z | y0). The expensive expectation over p*(y0) is dealt with as before by using
a small random subset of training examples. The importance weights ω(y, zk) and v(y0, zk) are
computed as
ω(y,zk) = exp(ω(y, Zk)) and ω(y, Zk) = logP(Zk) - log qφ(zk | y).
v(y0,zk)= LKXp(V(y *))、、and V(y0,zk) = logpθ(y01 zk)+logp(zk) - logq°(zk | y0).
Es=I eχp(v(y0,zs))
Maximizing the objective in Eq. 18 encourages the proposal qφ to output a distinct approximate
posterior distribution for each input y. We conjecture this helps avoid posterior collapse. We verify
this later in our empirical study.
4	Connections to ranking
Collobert et al. (2011) proposed a pair-wise ranking loss for natural language processing. This
ranking loss is defined as
Lranking = - m-fθ(spos)+fθ(sneg))+,
(19)
where fθ is a parametric scoring function, and spos and sneg are positive and negative examples
respectively. spos is often simply a training example such as a sentence, whereas sneg is a sample
from a distribution conditioned on spos. The hyperparameter m is the margin.
Consider the RLL objective in Eq. (8). Consider the choice of αn in Eq. (11). Using stochastic
approximation with a single sample (x, y) for the likelihood term and a single sample (x0, y) for the
reflective probability We can rewrite the objective LRLng (θ; D) as
LRLng(θ; D)=Iogpθ(y | X)- αo logpθ(y | x0)
=(1 - α0) log pθ (y |x) - α0 [0 - log pθ (y | x) + log pθ (y | x0)]+ .
This reveals that RLL is a convex sum between the usual maximum likelihood objective and the
ranking loss in Eq. (19), modulated by the global coefficient α0. It directly tackles the finite sample
dilemma discussed in Section 1 by maximizing the likelihood while ensuring that the correctly
paired examples are better scored than incorrectly paired examples. This in turn maximizes the
dependence between the inputs and outputs.
5	Applications
In this section we apply our proposed method to two different problems: MNIST digit classification
under imbalance and neural topic modeling. In classification under imbalance it is hard to learn
features of rare classes. Because RLL promotes a stronger dependence between inputs and outputs
we expect it to perform better than the maximum likelihood objective in the presence of imbalance.
We also consider neural topic models that use stochastic auto-encoders. Posterior collapse is partic-
ularly severe in text modeling. Neural topic models suffer from this problem (Srivastava & Sutton,
2017). One manifestation of posterior collapse in neural topic modeling is that all the dimensions
of the topic matrix collapse to the same topic which in turn contains words that are not necessarily
related to each other (Miao et al., 2016; Srivastava & Sutton, 2017). On both applications we found
RLL yields better performance both quantitatively and qualitatively. It learns more useful features
for rare classes in classification under imbalance on MNIST as evidenced by higher F1 scores (See
fig. 1.) Finally it learns more meaningful latent variables as evidenced by lower perplexity on docu-
ment completion (See Table 2.) We also found that the choice ofαn is dependent on the application.
For classification we found a fixed schedule for alpha to perform best. For neural topic modeling
we found an adaptive schedule to perform best. The reported results for RLL correspond to the best
schedule for αn .
6
Under review as a conference paper at ICLR 2019
Figure 1: Histogram of classification F1 scores for MLE and RLL. Left: Uniform distribution D1.
Right: Imbalanced distribution D10. Performance of MLE and RLL on D1 is similar. However RLL
outperforms MLE by a significant margin for the imbalanced distribution. This gain in performance
comes from how well RLL performs on rare classes. For digits 2, 6, and 8 both MLE and RLL have
0 F1 scores.
1.2
-RLL
-MLE
1.2
-RLL
-MLE
1.0
0.8
0.6
0.4
0.2
0.0
1.0
IIllllllllJ ι∣ι I...
4	6
Digit Label
0.0
4	6
Digit Label
0
2
8
0
2
8
Table 1: F1 scores and accuracies on (a) the original test set and (b) matched test sets. Across
the board, the classifiers trained with the proposed reflective log-likelihood perform similarly to or
outperform those trained with maximum log-likelihood.
(a) Original Test Set
Method	Metric	D1	D2	D3	D4	D5	D6	D7	D8	D9	D10
MLE	Acc	98.5	89.0	77.7	68.5	65.4	55.9	45.5	31.9	28.0	21.1
RLL	Acc	98.6	92.0	83.3	70.2	71.7	59.1	48.8	35.6	33.8	31.1
MLE	F1	98.5	84.5	69.3	57.9	57.8	43.9	32.2	19.5	20.6	17.5
RLL	F1	98.6	92.6	80.9	60.7	65.9	48.4	37.2	22.8	27.4	27.0
(b) Matched Test Set
Method	Metric	D1	D2	D3	D4	D5	D6	D7	D8	D9	D10
MLE	Acc	99.2	90.9	80.8	72.4	66.1	55.7	45.1	32.6	29.2	19.5
RLL	Acc	99.0	93.9	85.1	74.5	74.1	57.3	48.1	37.5	34.1	28.8
MLE	F1	99.2	87.0	73.1	62.7	57.5	43.7	31.2	20.7	22.8	15.9
RLL	F1	99.0	94.3	83.3	66.2	68.8	45.9	36.1	24.1	27.8	24.8
5.1	Classification under Class Imbalance
We start with MNIST which has 5,000 training examples per class, totalling 50,000 training exam-
ples. We refer to this original dataset as D0. We build 9 additional training sets. Each additional
training set Dk leaves only 5 training examples for each class j < k. See Table 3 in the Appendix
for all the class distributions. The validation set in each case is constructed by collecting all the data
points that are left from the training set. We train a classifier on each of these constructed training
sets and test it on both the original test set, in which all the classes are uniformly represented, and
the matched test set, in which the class distribution matches that of the corresponding training set.
We measure both F1 score and accuracy for each setting.
As a classifier, we use a deep convolutional network (LeCun et al., 1995). It has two convolutional
layers and two fully connected layers. Each convolutional layer consists of 5 × 5 convolution with
stride 1 followed by 2 × 2 max pooling and a rectified linear unit (ReLU, Glorot et al., 2011). The
output from the final convolutional layer is flattened and processed by two fully-connected layers of
50 units. We train the classifier using the Adam optimizer with dropout regularization. We find the
number of samples to estimate the second term in RLL by maximizing accuracy on the validation
set. We found that in most cases one sample suffices. We choose α0 to maximize accuracy on the
validation set. We found α0 = 0.5 to be the best.
In Table 1, we see that the classifiers trained with RLL always outperform those trained with the
usual maximum likelihood criterion when the class distribution is imbalanced. This is the case both
in terms of F1 score and accuracy on both the original (balanced) test set as well as the matched test
set.
7
Under review as a conference paper at ICLR 2019
Table 2: Perplexity and KL divergence computed from the neural topic model trained on the
20NewsGroup dataset. K denotes the number of posterior samples used during training. (?) The
difference between VAE (K = 1) and IWAE (K = 1) comes from the fact that we use an analytical
solution to the KL divergence in the case of VAE and sample-based approximation in the case of
IWAE.
Criterion	Autoencoder	K I FUllPPLl		Completion PPLl	KL(qφ(z |y) k p(z))↑
MLE	VAE	1	841	911	2.3
MLE	IWAE	1	820	884	2.8
RLL	RAE	1	809	875	3.0
MLE	VAE	100	818	886	3.1
MLE	IWAE	100	780	838	3.5
RLL	RAE	100	763	822	4.0
5.2	Neural Topic Modelling
We use the 20NewsGroup (Joachims, 1996) for topic modelling. 20NewGroup consists of 11,314
training and 7,531 test articles. The vocabulary size is 2,000. We follow the standard preprocess-
ing steps which include tokenization and the removal of non-UTF-8 characters and English stop
words (Miao et al., 2016; Srivastava & Sutton, 2017; Card et al., 2017).
We train a neural topic model, which is an extension of latent Dirichlet allocation (LDA, Blei et al.,
2003) with amortized inference. This is effectively a stochastic autoencoder with a bag-of-word
input and output. The inference network qφ of this auto-encoder is a two-layer feed-forward neural
network with 100 tanh units each. The output of this network is projected to a 15-dimensional vector
normalized by softmax to compute the topic proportion. The likelihood pθ is a three-layer feed-
forward neural network that maps this topic proportion to a bag-of-words. Our architecture choice
is based on the earlier observation by Srivastava & Sutton (2017) that this model is particularly prone
to posterior collapse. We test both the VAE (see Eq. (15)) and the IWAE (see Eq. (16)) as baselines.
These are two auto-encoders trained with different forms of the maximum likelihood objective. We
use Adam (Kingma & Ba, 2014) and train each model for 200 epochs with a fixed learning rate of
0.002. For RLL, we use 5 samples to estimate the reflective probability. We choose an adaptive
schedule for αn .
We measure both perplexity and the KL divergence between the estimated posterior distribution qφ
and the prior distribution p(z) = N(z; 0, 12). The KL has been used as a metric for measuring
posterior collapse. For perplexity, we report both the perplexity on the test set (Full PPL) and the
document completion perplexity (Completion PPL, Wallach et al., 2009). Both types of perplexities
are computed using importance sampling. Document completion consists in holding out some words
for each document in the test set to compute the topic proportions and then evaluating perplexity on
the remaining words of each document using the topic proportions learned with the held out words.
It is a good way to assess the quality of the learned latent variables in topic modeling. We held
out the first half of each document to compute the topic proportions and evaluated perplexity on the
second half.
Similarly to the classification experiment, we observe that RAE—the stochastic auto-encoder trained
with RLL—always outperforms the neural topic models trained with a maximum likelihood objec-
tive (VAE and IWAE) (see Table 2.) In addition to having a lower perplexity, RLL yields slightly
higher KL divergence scores—which suggests that RLL indeed improves upon VAE and IWAE in
terms of posterior collapse. We verify this further by looking at the learned topics; RLL the topics
learned by RLL are as good as the topics learned by Latent Dirichlet Allocation (LDA). This is not
the case for the VAE. IWAE also yields good topics but they are less sharp than the ones learned by
RAE. These topics can be found in the appendix.
8
Under review as a conference paper at ICLR 2019
6	Conclusion
In this paper, we identified the finite sample dilemma when fitting probabilistic models with maxi-
mum likelihood using a finite set of observations. We conjectured this to be the cause of the input
forgetting issue often encountered when fitting deep probabilistic models using maximum likeli-
hood. We then proposed RLL—a new learning criterion that mitigates the input forgetting issue.
RLL encourages a strong dependence between observations and their conditioning variables. This
in turn results in better performance both in classification under imbalance and neural topic modeling
with stochastic auto-encoders.
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing
a broken elbo. In International Conference on Machine Learning, pp. 159-168, 2018.
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation,
7(1):108-116, 1995.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine
Learning research, 3(Jan):993-1022, 2003.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Dallas Card, Chenhao Tan, and Noah A Smith. A neural framework for generalized topic models.
arXiv preprint arXiv:1705.09296, 2017.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016.
KyungHyun Cho, Tapani Raiko, and Alexander T Ihler. Enhanced gradient and adaptive learning
rate for training restricted boltzmann machines. In Proceedings of the 28th International Confer-
ence on Machine Learning (ICML-11), pp. 105-112, 2011.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Re-
search, 12(Aug):2493-2537, 2011.
Adji B Dieng, Yoon Kim, Alexander M Rush, and David M Blei. Avoiding latent variable collapse
with generative skip models. arXiv preprint arXiv:1807.04863, 2018a.
Adji B Dieng, Rajesh Ranganath, Jaan Altosaar, and David M Blei. Noisin: Unbiased regularization
for recurrent neural networks. arXiv preprint arXiv:1805.01500, 2018b.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059,
2016.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp.
315-323, 2011.
Geoffrey E Hinton. Learning translation invariant recognition in a massively parallel networks. In
International Conference on Parallel Architectures and Languages Europe, pp. 1-13. Springer,
1987.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
9
Under review as a conference paper at ICLR 2019
Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the varia-
tional evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS,
2016.
Thorsten Joachims. A probabilistic analysis of the Rocchio algorithm with TFIDF for text catego-
rization. Technical report, Carnegie-Mellon University, 1996.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361(10):1995, 1995.
Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through
l_0 regularization. arXiv preprint arXiv:1712.01312, 2017.
Laurens Maaten, Minmin Chen, Stephen Tyree, and Kilian Weinberger. Learning with marginalized
corrupted features. In International Conference on Machine Learning, pp. 410-418, 2013.
Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In Interna-
tional Conference on Machine Learning, pp. 1727-1736, 2016.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
arXiv preprint arXiv:1601.06759, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther.
How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint
arXiv:1602.02282, 2016.
Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. arXiv
preprint arXiv:1703.01488, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 267-288, 1996.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
Advances in neural information processing systems, pp. 351-359, 2013.
Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. Evaluation methods for
topic models. In Proceedings of the 26th annual international conference on machine learning,
pp. 1105-1112. ACM, 2009.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational
autoencoders. arXiv preprint arXiv:1706.02262, 2017.
10
Under review as a conference paper at ICLR 2019
7	Appendix
Table 3: Class distributions using the MNIST dataset. There are 10 class—one class for each of
the 10 digits in MNIST. The distribution D1 is uniform and the other distributions correspond to
different imbalance settings as given by the proportions in the table. Note these proportions might
not sum to one exactly because of rounding.
Dist	0	1	2	3	4	5	6	7	8	9
D1	0.1	0.1	0.1	0.1	0.1	0.1	0.1	0.1	0.1	0.1
D2	1e-3	0.11	0.11	0.11	0.11	0.11	0.11	0.11	0.11	0.11
D3	1e-3	1e-3	0.12	0.12	0.12	0.12	0.12	0.12	0.12	0.12
D4	1e-3	1e-3	1e-3	0.14	0.14	0.14	0.14	0.14	0.14	0.14
D5	1e-3	1e-3	1e-3	1e-3	0.17	0.17	0.17	0.17	0.17	0.17
D6	1e-3	1e-3	1e-3	1e-3	1e-3	0.20	0.20	0.20	0.20	0.20
D7	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	0.25	0.25	0.25	0.25
D8	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	0.33	0.33	0.33
D9	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	0.49	0.49
D10	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3	0.99
Table 4: Top ten words of five randomly selected topics for different models on the 20NewsGroup
dataset. Overall RAE learns topics as good as LDA—a non-neural network based model that does not
suffer from posterior collapse. The VAE suffers from collapse and learns topics that are meaningless.
The IWAE learns better topics than the VAE but has two topics (topics 3 and 5) that are worse than
RAE.
Setting I	Topics
LDA
VAE
IWAE
RAE
israel israeli jews arab state peace land jewish write policy
pay tax insurance money write care year article health rate
car drive engine buy write speed article light dealer driver
offer sale condition mouse best old excellent tape month trade
game team year win play season player fan write baseball
write article thanks want try need help buy work really
write article thanks thing really look buy drive gun problem
thanks run work write problem program software drive buy computer
write article buy drive thanks problem car try want help
armenians armenian kill turkish say child war government live attack
game play team player year write article better win baseball
god christian jesus faith bible truth church christ believe christianity
write article car thanks buy problem game bike team player
armenians armenian turks turkish government israel state genocide attack serve
thanks buy write car article phone appreciate drive sale run
god christian jesus life faith church believe bible christ christianity
game team play player win score year season hit toronto
gun weapon say kill police come carry crime health criminal
card drive windows mode driver problem pc disk printer scsi
government key clipper chip secure encryption escrow law enforcement security
11