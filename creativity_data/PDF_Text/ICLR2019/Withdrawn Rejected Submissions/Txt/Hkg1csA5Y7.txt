Under review as a conference paper at ICLR 2019
A fast quasi-Newton-type method for large-
SCALE STOCHASTIC OPTIMISATION
Anonymous authors
Paper under double-blind review
Ab stract
During recent years there has been an increased interest in stochastic adaptations
of limited memory quasi-Newton methods, which compared to pure gradient-
based routines can improve the convergence by incorporating second order in-
formation. In this work we propose a direct least-squares approach conceptu-
ally similar to the limited memory quasi-Newton methods, but that computes the
search direction in a slightly different way. This is achieved in a fast and numer-
ically robust manner by maintaining a Cholesky factor of low dimension. This is
combined with a stochastic line search relying upon fulfilment of the Wolfe con-
dition in a backtracking manner, where the step length is adaptively modified with
respect to the optimisation progress. We support our new algorithm by provid-
ing several theoretical results guaranteeing its performance. The performance is
demonstrated on real-world benchmark problems which shows improved results
in comparison with already established methods.
1	Introduction
In learning algorithms we often face the classical and hard problem of stochastic optimisation where
we need to minimise some non-convex cost function f (x)
min f (x),	(1)
x∈Rd
when we only have access to noisy evaluations of the function and its gradients. We take a particular
interest in situations where the number of data and/or the number of unknowns d is very large.
The importance of this problem has been increasing for quite some time now. The reason is simple:
many important applied problems ask for its solution, including most of the supervised machine
learning algorithms when applied to large-scale settings. There are two important situations where
the non-convex stochastic optimisation problem arise. Firstly, for large-scale problems it is often
prohibitive to evaluate the cost function and its gradient on the entire dataset. Instead, it is divided
into several mini-batches via subsampling, making the problem stochastic. This situation arise in
most applications of deep learning. Secondly, when randomised algorithms are used to approxi-
mately compute the cost function and its gradients the result is always stochastic.
Our contributions are: 1. A new stochastic line search algorithm allowing for adaptive step-lengths
akin to what is done in the corresponding state-of-the-art deterministic optimisation algorithms. This
is enabled via a stochastic formulation of the first Wolfe condition. 2. We provide anew and efficient
way of incorporating second-order (curvature) information into the stochastic optimiser via a direct
least-squares approach conceptually similar to the popular limited memory quasi-Newton methods.
3. To facilitate a fast and numerically robust implementation we have derived tailored updating of
a small dimension Cholesky factor given the new measurement pair (with dimension equal to the
memory length). 4. We support our new developments by establishing several theoretical properties
of the resulting algorithm. The performance is also demonstrated on real-world benchmark problems
which shows improved convergence properties over current state-of-the-art methods.
2	Related work
Due to its importance, the stochastic optimisation problem is rather well studied by now. The first
stochastic optimisation algorithm was introduced by Robbins & Monro (1951). It makes use of first-
1
Under review as a conference paper at ICLR 2019
order information only, motivating the name stochastic gradient (SG), which is the contemporary
term (Bottou et al., 2018) for these algorithms, originally referred to as stochastic approximation.
Interestingly most SG algorithms are not descent methods since the stochastic nature of the update
can easily produce a new iterate corresponding to an increase in the cost function. Instead, they are
Markov chain methods in that their update rule defines a Markov chain.
The basic first-order SG algorithms have recently been significantly improved by the introduction of
various noise reduction techniques, see e.g. (Johnson & Zhang, 2θl3; Schmidt et al., 2013; Konecny
& Richt处ik, 2017; Defazio et al., 2014).
The well-known drawback of all first-order methods is the lack of curvature information. Analo-
gously to the deterministic setting, there is a lot to be gained in extracting and using second-order
information that is maintained in the form of the Hessian matrix. The standard quasi-Newton method
is the BFGS method, named after its inventors (Broyden, 1967; Fletcher, 1970; Goldfarb, 1970;
Shanno, 1970). In its basic form, this algorithm does not scale to the large-scale settings we are
interested in. The idea of only making use of the most recent iterates and gradients in forming the
inverse Hessian approximation was suggested by Nocedal (1980) and Liu & Nocedal (1989). The
resulting L-BFGS method is computationally cheaper with a significantly reduced memory foot-
print. Due to its simplicity and good performance, this has become one of the most commonly
used second-order methods for large-scale problems. Our developments makes use of the same trick
underlying L-BFGS, but it is carefully tailored to the stochastic setting.
Over the past decade we have witnessed increasing capabilities of these so-called stochastic quasi-
Newton methods, the category to which our developments belong. The work by Schraudolph et al.
(2007) developed modifications of BFGS and its limited memory version. There has also been a
series of papers approximating the inverse Hessian with a diagonal matrix, see e.g. Bordes et al.
(2009) and Duchi et al. (2011). The idea of exploiting regularisation together with BFGS was
successfully introduced by Mokhtari & Ribeiro (2014). Later Mokhtari & Ribeiro (2015) also de-
veloped a stochastic L-BFGS algorithm without regularisation. The idea of replacing the stochastic
gradient difference in the BFGS update with a subsampled Hessian-vector product was recently
introduced by Byrd et al. (2016), and Wang et al. (2017) derived a damped L-BFGS method.
Over the past five years we have also seen quite a lot of fruitful activity in combining the stochas-
tic quasi-Newton algorithms with various first-order noise reduction methods (Moritz et al., 2016;
Gower et al., 2016). A thorough and forward-looking overview of SG and its use within a modern
machine learning context is provided by Bottou et al. (2018). It also includes interesting accounts
of possible improvements along the lines of first-order noise reduction and second-order methods.
It is interesting—and perhaps somewhat surprising—to note that it is only very recently that stochas-
tic line search algorithms have started to become available. One nice example is the approach pro-
posed by Mahsereci & Hennig (2017) which uses the framework of Gaussian processes and Bayesian
optimisation. The step length is chosen that best satisfies a probabilistic measure combining reduc-
tion in the cost function with satisfaction of the Wolfe conditions. Conceptually more similar to our
procedure is the line search proposed by Bollapragada et al. (2018), which is tailored for problems
that are using sampled mini-batches, as is common practice within deep learning.
3	Stochastic Line Search
In deterministic line search algorithms we first compute a search direction pk and then decide how
far to move along that direction according to
xk+1 = xk + αkpk ,	(2)
where αk > 0 is referred to as the step length. The search direction is of the form
pk = -Hkgk ,	(3)
where Hk denotes an approximation of the inverse Hessian matrix. The question of how far to move
in this direction can be framed as the following scalar minimisation problem
min f (xk + αpk),	α > 0.	(4)
α
2
Under review as a conference paper at ICLR 2019
The most common way of dealing with this problem is to settle for a possibly sup-optimal solution
that guarantees at least a sufficient decrease. Such at solution can be obtained by selecting a step
length αk that satisfies the following inequality
f(xk + αpk) ≤ f(xk) + cαkgkTpk ,	(5)
where c ∈ (0, 1). The above condition is known as the first Wolfe condition (Wolfe, 1969; 1971) or
the Armijo condition (Armijo, 1966).
While deterministic line search algorithms have been well established for a long time, their stochas-
tic counterparts are still to a large extent missing. Consider the case when the measurements of the
function and its gradient are given by
fk = f (xk ) + ek ,	gbk = gk + vk ,	(6)
where gk，Vf (χ)∣χ=χk, and ek ∈ R and Vk ∈ Rd×1 denote noise on the function and gradient
evaluations, respectively. Furthermore we assume that
E [ek] = b, Cov[ek] = σf2,	E [vk]= 0,	Cov[vk] = σg2I.	(7a)
The challenge is that since fbk and gbk are random variables it is not obvious how to select a step
length αk that—in some sense—guarantees a descent direction.
We explore the idea of requiring equation 5 to be fulfilled in expectation when the exact quantities
are replaced with their stochastic counterparts,
E
fb(xk + αpbk) - fb(xk) - cαkgbkTpbk ≤ 0,
(8)
where pbk = -Hkgbk. This is certainly one way in which we can reason about the Wolfe condition
in the stochastic setting we are interested in. Although satisfaction of equation 8 does not leave any
guarantees when considering a single measurement, it still serves as an important property that could
be exploited to provide robustness for the entire optimisation procedure. To motivate our proposed
algorithm we hence start by establishing the following results.
lemma 1 (Stochastic Wolfe condition 1) Assume that i) the gradient estimates are unbiased , ii)
the cost function estimates are possibly biased, and iii) a descent direction is ensured in expectation
E [pbTgk ] < 0. Then (for small a:s)
E hf(Xk + αpk)i ≤ E hf(Xk) + Cak bTPk] , where 0 <c<c= F-----------------Pk g"订、.	(9)
pkTgk - σg2 Tr (H)
Proof 1 See Appendix A.1.
lemma 2 There exists a step length αk > 0 such that E fb(xk + αkpbk) - fb(xk) < 0.
Proof 2 See Appendix A.2
Relying upon these results we propose a line search based on the idea of repeatedly decreasing the
step length αk until the stochastic version of the first Wolfe condition is satisfied. Pseudo-code for
this procedure is given in Algorithm 1. An input to this algorithm is the search direction pbk, which
can be computed using any preferred method. The step length is initially set to be the minimum of
the "natural" step length 1 and the iteration dependent value ξ∕k. In this way the initial step length
is kept at 1 until k > ξ, a point after which it is decreased at the rate 1/k. Then we check whether
the new point Xk + αpbk satisfies the stochastic version of the first Wolfe condition. If this is not
the case, we decrease αk with the scale factor ρ. This is repeated until the condition is met, unless
we hit an upper bound max{0, τ - k} on the number of backtracking iterations, where τ > k is a
positive integer. With this restriction the decrease of the step length is limited, and when k ≥ τ we
use the initial step length no matter if the Wolfe condition is satisfied or not. The purpose of ξ is to
facilitate convergence by reducing the step length as the minima is approached. The motivation of
τ comes from arguing that the backtracking loop does not contribute as much when the step length
is small, and hence it is reasonable to provide a limit on its reduction.
3
Under review as a conference paper at ICLR 2019
Algorithm 1 Stochastic backtracking line search
Require: Iteration index k, spatial point xk, search direction pbk, scale factor ρ ∈ (0, 1), reduction
limit ξ ≥ 1, backtracking limit τ > 0.
1:	Set the initial step length ak = min{ 1 ,ξ∕k}
2:	Seti = 1
3:	while f(xk + αpbk) > f(xk) + cαkgbkTpbk and i ≤ max{0, τ - k} do
4:	Reduce the step length αk J Pak
5:	Set i J i + 1
6:	end while
7:	Update xk+1 = xk + αkpbk 4
4 Computing the search direction
In this section we address the problem of computing a search direction based on having a limited
memory available for storing previous gradients and associated iterates. The approach we adopt
is similar to limited memory quasi-Newton methods, but here we employ a direct least-squares
estimate of the inverse Hessian matrix rather than more well-known methods such as damped L-
BFGS and L-SR1. In contrast to traditional quasi-Newton methods, this approach does not strictly
impose neither symmetry or fulfilment of the secant condition. It may appear peculiar to relax
these requirements. However, in this setting is not obvious that enforced symmetry necessarily
produces a better search direction. Furthermore, the secant condition relies upon the rather strong
approximation that the Hessian matrix is constant between two subsequent iterations. Treating the
condition less strictly might be helpful when that approximation is poor, perhaps especially in a
stochastic environment. We construct a limited-memory inverse Hessian approximation in Section
4.1 and show how to update this representation in Section 4.2. In Section 4.3 we discuss a particular
design choice associated with the update and Section 4.4 provides a means to ensure that a descent
direction is calculated. The complete algorithm combining the procedure presented in this section
with the line search routine in Algorithm 1 is given in Appendix C.
4.1 Quasi-Newton Inverse Hessian Approximations
According to the secant condition (see e.g. Fletcher (1987)), the inverse Hessian matrix Hk should
satisfy
Hkyk = sk,	(10)
where yk = gk - gk-1 and sk = xk - xk-1. Since there are generally more unknown values in Hk
than can be determined from yk and sk alone, quasi-Newton methods update Hk from a previous
estimate Hk-1 by solving regularised problems of the type
Hk = arg min kH - Hk-1k2F,W	s.t. H = HT, Hyk = sk,	(11)
H,
where kX k2F,W = kXW k2F = trace(WTXTXW) and the choice of weighting matrix W results
in different algorithms (see Hennig (2015) for an interesting perspective on this). Examples of the
most common quasi-Newton methods are given in Appendix B.
We employ a similar approach and determine Hk as the solution to the following regularised least-
squares problem
Hk = argmn ∣∣HK - Sk kF + λ∣∣H - Hk kF,	(12)
H
where Yk and Sk hold a limited number of past ybk’s and sk’s according to
Yk , [ybk-m+1, . . . , ybk] ,	Sk , [s
k-m+1 , . . . , sk] .	(13)
Here, m << d is the memory limit and ybk = gbk - gbk-1 is an estimate of yk. The regulator matrix
Hk acts as a prior on H and can be modified at each iteration k. The parameter λ > 0 is used to
control the relative cost of the two terms in equation 12. It can be verified that the solution to the
above least-squares problem (12) is given by
Hk = (λHk + Sk YT )(λI + KYT)— 1,	(14)
4
Under review as a conference paper at ICLR 2019
where I denotes the identity matrix. The above inverse Hessian estimate can be used to generate a
search direction in the standard manner by scaling the negative gradient, that is
pbk = -Hkgbk.	(15)
However, for large-scale problems this is not practical since it involves the inverse of a large matrix.
To ameliorate this difficulty, we adopt the standard approach by storing only a minimal (limited
memory) representation of the inverse Hessian estimate Hk . To describe this, note that the dimen-
sions of the matrices involved are
Hk ∈ Rd×d,	Yk ∈ Rd×m,	Sk ∈ Rd×m.	(16)
We can employ the Sherman-Morrison-WoodbUry formula to arrive at the following equivalent
expression for Hk
Hk = (Hk + λ-1SkYT) [i - Yk (λI + YTK)T yt] .	(17)
Importantly, the matrix inverse (λI + YkrYk) 1 is now by construction a positive definite matrix of
size m × m. Therefore, we construct and maintain a Cholesky factor ofλI + YkT Yk since this leads
to efficient solutions. In particular, if we express this matrix via a Cholesky decomposition
RkTRk = λI+YkTYk,	(18)
where Rk ∈ Rm×m is an upper triangular matrix, then the search direction pbk = -Hkgbk can be
computed via
Pbk = -H k Zk - λ-1 Sk(Ykrzk ), Zk = Gk - Ykwk , Wk = R-1 (R-T(YkTbk)) .	(19)
Note that the above expressions for pbk involve first computing wk, which itself involves a compu-
tationally efficient forward-backward substitution (recalling that Rk is an m × m upper triangular
matrix and the memory length m is typically 10-50). Furthermore, Hk is typically diagonal so that
Hk zk is also efficient to compute. The remaining operations involve four matrix-vector products
and two vector additions. Therefore, for problems where d >> m then the matrix-vector products
will dominate the computational cost.
Constructing Rk can be achieved in several ways. The so-called normal-equation method constructs
the (upper triangular) part of λI + YkTYk and then employs a Cholesky routine, which produces Rk
in O(nm(m+1) + m3/3) operations. Alternatively, we can compute Rk by applying Givens rotations
or Householder reflections to the matrix Mk = [√λI YkT] T . This costs O(2m2((n + m) - m/3))
operations, and is therefore more expensive, but typically offers better numerical accuracy (Golub
& Van Loan, 2012).
4.2	Fast and robust inclusion of new measurements
In order to maximise the speed, we have developed a method for updating a Cholesky factor given
the new measurement pair (sk+1, ybk+1). Suppose we start with a Cholesky factor Rk at iteration k
such that
RkTRk = λI + YkTYk .	(20)
Assume, without loss of generality, that Yk and Sk are ordered in the following manner
Yk , [Y1, ybk-m+1, Y2] ,	Sk , [S1, sk-m+1, S2] ,	(21)
where Y1, Y2, S1 and S2 are defined as
YI ,	[yk-m+'+1, ..., yk] ,	Y2	,	[yk—m+2,..∙,	yk-m+'] ,	(22a)
S1 ,	[sk-m+'+1, . . .	, sk] ,	S2	,	[sk—m+2, . . .	,	sk-m+'∖ ,	(22b)
and ` is an appropriate integer so that Yk and Sk have m columns. The above ordering arises from
“wrapping-around” the index when storing the measurements. We create the new Yk+1 and Sk+1 by
replacing the oldest column entries, ybk—m+1 and sk—m+1, with the latest measurements ybk+1 and
sk+1, respectively, so that
Yk+1 , [Y1, ybk+1, Y2] ,	Sk+1 , [S1, sk+1, S2] .	(23)
5
Under review as a conference paper at ICLR 2019
The aim is to generate a new Cholesky factor Rk+1 such that
RkT+1Rk+1 = λI + YkT+1Yk+1.	(24)
To this end, let the upper triangular matrix Rk be written conformally with the columns of Yk as
R1	r1 R2
Rk =	∙	r	r3
L ∙	•冗4」
(25)
so that R1 and R2 have the same number of columns as Y1 and Y2, respectively. Furthermore, r1 is
a column vector, r2 is a scalar and r3 is a row vector. Therefore,
R1TR1	R1Tr1	R1TR2
RTRk =	∙ r2 + rT ri	rτR2 + TlT3
_	∙	∙	RTR4 + RT R2 + rTT3_
'λi + YTYi	γTbk-m+ι	CYTY2	-
=	∙	λ + yT-m+1yk-m+1	yT-m+i Y2 .
_	∙	∙	λi + YTY2_
By observing a common structure for the update λI + YkT+iYk+i it is possible to write
BI + YT Yi	YTbk+1	YT Y2	-
λi + yT+iK+i=	∙	λ + bT+ιbk-m+ι	bT+iY2
_	∙	∙	λI + YTY2_
-RlRi	RTr4	RTR2	-
=	∙	r5 + rTr4	rT R2 + T5T6	,
_ ∙	∙	RTR6 + RTR2 + TtT6.
(26)
(27)
where r4 , r5 and r6 are determined by
r4 = R-T(YTbk +1),	r5 = (λ + yT+ibk+i - rT2)"2 ,	r6 = — (bT+iY2 -rTR2)∙ (28)
r5
The final term R6 can be obtained by noticing that
R6TR6 + R2TR2 + r6Tr6 = R4TR4 + R2TR2 + r3Tr3 ,	(29)
which implies
R6TR6 = R4TR4 - r6Tr6 + r3Tr3 .	(30)
Therefore R6 can be obtained in a computationally very efficient manner by down-dating and updat-
ing the Cholesky factor R4 with the rank-1 matrices r6Tr6 and r3Tr3, respectively (see e.g. Section
12.5.3 in Golub & Van Loan (2012)).
4.3	Selecting Hk
There is no magic way of selecting the prior matrix Hk. However, in practise it has proved very
useful to employ a simple strategy of Hk，Yk I, where the positive scalar Yk > 0 is adaptively
chosen in each iteration. As a crude measure of progress we adopt the following rule
κYk-i,	if αk-i =—,
Yk = < Yk-i∕κ,	if ak-i < l∕ρq,	(31)
Yk-i,	otherwise,
where κ ≥ — is a scale parameter, and q corresponds to the number of backtracking loops in the
line search; the values κ = —.3 and q = 3 were found to work well in practise. The intuition
behind equation 31 is that if no modification of the step length αk is made, we can allow for a
more "aggressive" regularisation. Note that a low Yk is favouring small elements in Hk. Since
pbk = -Hgbk, this limits the magnitude of pbk and the change kxk+i - xk k is kept down. Hence, it is
good practice to set the initial scaling Y0 relatively small and then let it scale up as the optimisation
progresses. Furthermore, We should point out that a diagonal Hk comes with an efficiency benefit,
since the product Hk Zk in equation 19 then is obtained as the element-wise product between two
vectors.
6
Under review as a conference paper at ICLR 2019
4.4 Ensuring a descent direction
In deterministic quasi-Newton methods, the search direction pk must be chosen to ensure a descent
direction such that pkTgk < 0, since this guarantees reduction in the cost function for sufficiently
small step lengths αk. Since pk = -Hgk, we have thatpkTgk = -gkTHkgk which is always negative
if the approximation Hk of the inverse Hessian is positive definite. Otherwise, we can modify the
search direction by subtracting a multiple of the gradient Pk J Pk - βkgk. This is motivated by
noticing that
(Pk - βgk)Tgk = PkTgk - βkgkTgk,
which always can be made negative by selecting βk large enough, i.e. if
βk R.
gkTgk
(32)
(33)
In the stochastic setting, the condition above does not strictly enforce a descent direction. Hence
the search direction Pbk as determined by equation 15 is not a descent direction in general. However,
ensuring that the condition is fulfilled in expectation is necessary since this is one of the assumptions
made in lemma 1. Hence, we now establish the following result.
lemma 3 If
β > PTgk - σ Tr (H)
k	gTgk+ dσg	,
then
E (Pbk - βkgbk)Tgbk < 0.
(34)
(35)
Proof 3 See Appendix A.3.
In the practical setting we can not use equation 34 as it is, since we do not have access to gk and
Pk , nor the noise variance σg2 . Instead we suggest a pragmatic approach in which these quantities
are replaced by their estimates gbk, Pbk and σg2. The noise estimate σg2 could either be regarded a
design parameter or empirically calculated from one or more sets of repeatedly collected gradient
measurements. Nevertheless, picking βk sufficiently large ensures fulfilment of equation 32 and
equation 35 simultaneously.
5	Numerical experiments
Let us now put our new developments to the test on a suite of problems from different categories
to exhibit different properties and challenges. In Section 5.1 we study a commonly used bench-
mark, namely the collection of logistic classification problems described by Chang & Lin (2011)
in the form of their library for support vector machines (LIBSVM). In Section 5.2 we consider an
optimisation problem arising from the use of deep learning to solve the classical machine learning
benchmark MNIST1, where the task is to classify images of handwritten digits. Also, we test our
method on training a neural network on the CIFAR-10 dataset (Krizhevsky, 2009).
In our experiments we compare against relevant state-of-the-art methods. All experiments were run
on a MacBook Pro 2.8GHz laptop with 16GB of RAM using Matlab 2018b. All routines where
programmed in C and compiled via Matlab’s mex command and linked against Matlab’s Level-1,2
BLAS libraries. More details about the experiments are available in Appendix D. The source code
used to produce the results will be made freely available.
5.1	Logistic loss and a 2-norm regulariser
The task here is to solve eight different empirical risk minimisation problems using a logistic loss
function with an L2 regulariser (two are shown here and all eight are profiled in Appendix D).
1yann.lecun.com/exdb/mnist/
7
Under review as a conference paper at ICLR 2019
(a) covtype	(b) spam
(c) CIFAR
(d) MNIST
Figure 1: Performance on two classification tasks using a logistic loss with a two-norm regulariser
((a) and (b)), and two deep convolutional neural networks (CNNs) used for recognising images
of handwritten digits from the MNIST data (c), and classification of the images in the CIFAR-10
data (d). Lighter shaded lines indicate individual runs, whereas the darker shaded line indicates the
average.
The data is taken from Chang & Lin (2011). These problems are commonly used for profiling
optimisation algorithms of the kind introduced in this paper, facilitating comparison with existing
state-of-the-art algorithms. More specifically, we have used a similar set-up as Gower et al. (2016),
which inspired this study. The chosen algorithm parameters for each case is detailed in Appendix D.
We compared our limited memory least-squares approach (denoted as LMLS) against two existing
methods from the literature, namely, the limited memory stochastic BFGS method after Bollapra-
gada et al. (2018) (denoted as LBFGS) and the stochastic variance reduced gradient descent (SVRG)
by Johnson & Zhang (2013) (denoted SVRG). Figures 1a and 1b show the cost versus time for 50
Monte-Carlo experiments.
5.2	Deep learning
Deep convolutional neural networks (CNNs) with multiple layers of convolution, pooling and non-
linear activation functions are delivering state-of-the-art results on many tasks in computer vision.
We are here borrowing the stochastic optimisation problems arising in using such a deep CNN to
solve the MNIST and CIFAR-10 benchmarks. The particular CNN structure used for the MNIST
example employs 5 × 5 convolution kernels, pooling layers and a fully connected layer at the end.
We made use of the publicly available code provided by Zhang (2016), which contains all the im-
plementation details. For the CIFAR-10 example, the network includes 13-layers with more than
150,000 weights, see Appendix D for details. The MATLAB toolbox MatConvNet (Vedaldi & Lenc,
2015) was used in the implementation. Figures 1c and 1d show the average cost versus time for
10 Monte-Carlo trials with four different algorithms: 1. the method developed here (LMLS), 2.
a stochastic limited memory BFGS method after Bollapragada et al. (2018) (denoted LBFGS), 3.
Adam developed by Kingma & Ba (2015), and 4. stochastic gradient (denoted SG). Note that all
algorithms make use of the same gradient code.
6	Conclusion and future work
In this work we have presented a least-squares based limited memory optimisation routine that ben-
efits from second order information by approximating the inverse Hessian matrix. The procedure
is conceptually similar to quasi-Newton methods, although we do not explicitly enforce symmetry
or satisfaction of the secant condition. By regularising with respect to an inverse Hessian prior, we
allow for an adaptive aggressiveness in the search direction update. We have shown that the com-
putations can be made robust and efficient using tailored Cholesky decompositions, with a cost that
scales linearly in the problem dimension. Our method is designed for stochastic problems through
a line search that repeatedly decreases the step length so as to satisfy the first Wolfe condition in
expectation. Theoretical results have been established that support the proposed procedure. The
method shows improved convergence properties over existing algorithms when applied to bench-
mark problems of various size and complexity.
8
Under review as a conference paper at ICLR 2019
References
L. Armijo. Minimization of functions having Lipschitz continuous first partial derivatives. Pacific
Journal OfMathematics, 16(1):1-3,1966.
R. Bollapragada, D. Mudigere, J. Nocedal, H.-J. M. Shi, and P. T. P. Tang. A progressive batching
L-BFGS method for machine learning. In Proceedings of the 35th International Conference on
Machine Learning (ICML), Stockholm, Sweden, 2018.
A. Bordes, L. Bottou, and P. Gallinari. SGD-QN: Careful quasi-Newton stochastic gradient descent.
Journal of Machine Learning Research (JMLR), 10:1737-1754, 2009.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning.
SIAM Review, 60(2):223-311, 2018.
C.	G. Broyden. Quasi-Newton methods and their application to function minimization. Mathematics
of Computation, 21:368-381, 1967.
C. G. Broyden. The convergence of a class of double-rank minimization algorithms. Journal of the
Institute of Mathematics and Its Applications, 6(1):76-90, 1970.
R. H. Byrd, S. L. Hansen, J. Nocedal, and Y. Singer. A stochastic quasi-Newton method for large-
scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):27:1-27:27, 2011.
A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: a fast incremental gradient method with support
for non-strongly convex composite objectives. In Advances in Neural Information Processing
Systems (NIPS), Montreal, Canada, 2014.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research (JMLR), 12:2121-2159, 2011.
R. Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):317-322,
1970.
R. Fletcher. Practical methods of optimization. John Wiley & Sons, Chichester, UK, second edition,
1987.
D. Goldfarb. A family of variable metric updates derived by variational means. Mathematics of
Computation, 24(109):23-26, 1970.
G. H. Golub and C. F. Van Loan. Matrix Computations. John Hopkins University Press, Baltimore,
fourth edition, 2012.
R. M. Gower, D. Goldfarb, and P. Richtarik. Stochastic block BFGS: squeezing more curvature out
of data. In Proceedings of the 33rd International Conference on Machine Learning (ICML), New
York, NY, USA, 2016.
P. Hennig. Probabilistic interpretation of linear solvers. SIAM Journal on Optimization, 25(1):
234-260, 2015.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance re-
duction. In Advances in Neural Information Processing Systems (NIPS), Lake Tahoe, NV, USA,
2013.
D.	P. Kingma and J. Ba. Adam: a method for stochastic optimization. In Proceedings of the 3rd
international conference on learning representations (ICLR), San Diego, CA, USA, 2015.
J. Konecny and P. Richtðrik. Semi-stochastic gradient descent methods. Frontiers inApplied Math-
ematics and Statistics, 3(9), 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
9
Under review as a conference paper at ICLR 2019
D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503-528, 1989.
M. Mahsereci and P. Hennig. Probabilistic line searches for stochastic optimization. Journal of
Machine Learning Research (JMLR), 18(119):1-59, 2017.
A. Mokhtari and A. Ribeiro. RES: regularized stochastic BFGS algorithm. IEEE Transactions on
Signal Processing, 62(23):6089-6104, 2014.
A. Mokhtari and A. Ribeiro. Global convergence of online limited memory BFGS. Journal of
Machine Learning Research (JMLR), 16:3151-3181, 2015.
P. Moritz, R. Nishihara, and M. I. Jordan. A linearly-convergent stochastic L-BFGS algorithm.
In The 19th International Conference on Artificial Intelligence and Statistics (AISTATS), Cadiz,
Spain, 2016.
J. Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35
(151):773-782, 1980.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research.
Springer, New York, USA, second edition, 2006.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,
22(3):400-407, 1951.
M.	Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient.
Technical Report arXiv:1309.2388, arXiv preprint, 2013.
N.	N. Schraudolph, J. Yu, and S. Gunter. A stochastic quasi-Newton method for online convex
optimization. In Proceedings of the 11th international conference on Artificial Intelligence and
Statistics (AISTATS), 2007.
D. F. Shanno. Conditioning of quasi-Newton methods for function minimization. Mathematics of
Computation, 24(111):647-656, 1970.
A. Vedaldi and K. Lenc. Matconvnet - convolutional neural networks for matlab. In Proceeding of
the ACM Int. Conf. on Multimedia, 2015.
X. Wang, S. Ma, D. Goldfarb, and W. Liu. Stochastic quasi-Newton methods for nonconvex stochas-
tic optimization. SIAM Journal on Optimization, 27(2):927-956, 2017.
P. Wolfe. Convergence conditions for ascent methods. SIAM Review, 11(2):226-235, 1969.
P. Wolfe. Convergence conditions for ascent methods II: some corrections. SIAM Review, 13(2):
185-188, 1971.
Z. Zhang. Derivation of backpropagation in convolutional neural networks (CNN). github.com/
ZZUTK/An-Example-of-CNN-on-MNIST-dataset, 2016.
10
Under review as a conference paper at ICLR 2019
A Proofs of the lemmas
A.1 The first stochastic Wolfe condition
In the interest of a simple notation we note that we can drop the sub-index k from all variables since
all reasoning in this proof is for iteration k. We also drop x from the arguments and write f rather
than f (x), etc. However, we do make the randomness explicit, since this is crucial for arriving at the
correct answer. For example we write fbz to indicate that the random variable z was used in comput-
ing the estimate of the function value at the iterate xk . This means that the noise contaminating the
measurement corresponds to the realisation of z .
Consider the stochastic version of the Wolfe condition
Ez,z0 fbz0 (x + αpbz) - fbz(x) - cαpbzTgbz ≤ 0,
(36)
where the expected value is w.r.t. the randomness used in computing the required estimators as
indicated by z and z0. Using Taylor series we can for small step lengths α express fz0 (x + αpbz)
according to
T
fz0 (x + αpbz) ≈ fz0 + αpbzTgbz0.
(37)
Here it is crucial to note that the randomness in the above estimate stem from two different sources.
The search direction pbz enters in the argument of the Taylor expansion which is why it has to be
computed before actually performing the Taylor expansion, implying that the randomness is due to
z and not z0 for the search direction. Based on equation 36 we have the following expression for the
stochastic Wolfe condition
Ez,z0 fbz0 - fbz - αpbzT gbz0 + cαpbzTgbz ≤ 0.
(38)
Let b denote a possible bias in the cost function estimator, then we have that Ez0 fbz0 = f + b and
Ez fbz = f + b. Hence, the first two terms in equation 38 cancel and we have
Ez,z0 [PTbzθ] = Ez [bT] Ez0 [bzθ] = PTg
c ≤ Ezn [bTbz] =	Ez [bTbz]	= EnpTbi.
The denominator can be written as
Ez [bTbz] = Ez [-bTHgz] = -Ez [Tr (HbZgT)] = - Tr (HEz 伉gT]).
Using the definition of covariance we have that
Ez [gbzgbzT] = Ez [gbz]Ez [gbzT] + Covgb = ggT + σg2I,
(39)
(40)
(41)
where we in the last equality made use of the assumption that the gradients are unbiased and that
Covgb = σgI. Summarising we have that
Ez [bTbz] = - Ir (HggT)- σg Tr (H)= PTg - σg Tr (H),
and hence we get
pTg
C ≤	------------.
一PTg - σg Tr (H)
(42)
(43)
Recalling the assurance given in lemma 3 with the associated proof in Appendix A.3, we can always
modify p to guarantee that this bound is positive.
A.2 Reduction in cost function
In this section we show that there exists an α > 0 such that
Ez0 hfbz0 (x + αp)i < Ez hfbz(x)i .
(44)
11
Under review as a conference paper at ICLR 2019
We do this by studying the difference
Ez0 hfbz0 (x + αp)i - Ez hfbz(x)i = Ez,z0 hfbz0 (x + αp) - fbz(x)i ,	(45)
in which the first term can be expressed using a Taylor series according to
fbz0(x + αpbz) = fbz0 + αpbzT gbz0 + O(αkpbz k2).	(46)
For small values of α we can discard the term O(αkpbz k2) from this expression. Hence we have that
Ez,z0 hfbz0	+ αpbzTgbz0	-	fbzi	= α Ez,z0	pbzTgbz0	= αEz	pbzT	Ez0	[gbz0]	= αpTg.	(47)
Following from Section 4 we can ensure pTg to be negative, and hence equation 44 holds as long as
α is chosen small enough for the Taylor expansion to be a valid approximation.
A.3 Ensuring a descent direction
Note that
Ez	(pbz-βgbz)Tgbz	=Ez	pbzTgbz	-βEz	gbzTgbz	=Ez	-gbzTHgbz	-βEz	gbzTgbz	.	(48)
Using equation 42 we have
Ez [-bTHgz] = - Tr (HggT) - σg Tr (H) = PTg - σg Tr (H),	(49)
and it directly follows from this that
Ez [gbzTgbz] = Ez [-gbzTIgbz] = gTg + dσg2.	(50)
Hence
Ez [(pz - βbz)Tbz] = PTg - σg Tr(H) - β (gτg + dσg) ,	(51)
and we see that
PTg - σ2 Tr (H)
Ez [(Pz - βbz )Tbz] < 0 ⇔ β> " "t g，2，.	(52)
gTg + dσg2
B Quasi-Newton methods
The historically most popular quasi-Newton method is given by the BFGS algorithm (Broyden,
1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970)
Hk+1
(I - ykST ʌ + SksT
I	yTskJ yTsk
(53)
A closely related version is obtained if the optimisation in equation 11 is done with respect to the
Hessian matrix rather than to its inverse. This results in the so-called DFP formula
Hk+1 = Hk -
Hk ykyTHk + Sk ST
ykTHkyk	ykT sk
(54)
Both of these are rank 2 updates that ensure Hk to be positive definite, an attractive feature in the
sense that it guarantees a descent direction. However, it may cause problems in regions where the
true inverse Hessian is indefinite. Another well-known alternative is the symmetric rank 1 (SR1)
method
Hk = Hk-1 +
(Sk - Hk-Iyk ) (Sk - Hk-Iyk )T
(Sk - Hk-1yk )Tyk
(55)
Apart from BFGS and DFP above, this is a rank 1 update that in general does not preserve positive
definiteness. For this reason it has received a particular interest within the so-called trust-region
framework, where its ability of producing indefinite inverse Hessian approximations is being re-
garded as a major strength (Nocedal & Wright, 2006).
12
Under review as a conference paper at ICLR 2019
A main drawback of quasi-Newton methods is that they scale poorly to large problems, since the
number of elements in Hk equals the square of the input dimension d. This has resulted in devel-
opments of so-called limited memory algorithms, which rely upon the idea of forming the search
direction pk directly without the need of storing the full matrix Hk. To that end a number m << d
of past differences y and s are being stored in memory. A notable member of this family is the
L-BFGS algorithm (Nocedal, 1980), which has been widely within large-scale optimisation. During
recent years, not at least due to the growing number of deep learning applications, there has been
an increasing interest in adapting deterministic limited memory methods to the stochastic setting
(Wang et al., 2017; Moritz et al., 2016; Gower et al., 2016; Schraudolph et al., 2007; Bordes et al.,
2009; Mokhtari & Ribeiro, 2015; Byrd et al., 2016; Bollapragada et al., 2018).
C Resulting algorithm
We summarise our ideas from Section 3 and 4 in Algorithm 2. The if-statement on line 5 is included
to provide a safe-guard against numerical instability.
D Experiment details
Details for the datasets used in Section 5 are listed in Table 1, including the parameter choices we
made in our algorithm. Here, b denotes the mini-batch size. The results of all logistic regression
experiments are collected in Figure 2 (including those already shown in Figure 1), and the neural
network results in Figures 1d and 1c are provided in a more readable size in Figure 3a and 3b.
Detailed information of the network structure in the CIFAR experiment is provided in the printout
shown in Figure 4.
The adaptive procedure of selecting the inverse Hessian prior Hk did not have much impact in
most of the logistic regression examples, and thus it was kept constant Hk = γoI except for in the
covtype and URL problems. In the CIFAR and MNIST problems, however, the procedure was
found to be of significant importance.
Table 1: List of the datasets used in the experiments, where n denotes the size of the dataset (column
2) and d denotes the number of variables in the optimisation problem (column 3). The remaining
columns list our design parameters, including the mini-batch size b.
Problem	n	d	b	m	λ	ξ	τ	γ0
gisette	6 000	5000	770	20	10-4	50	10	10
covtype	581 012	54	7 620	55	10-4	100	5	100
HIGGS	11 000 000	28	66 340	28	10-4	20	5	100
SUSY	3 548 466	18	5 000	18	10-4	50	10	100
epsilon	400 000	2000	1000	20	10-4	150	1	100
rcv1	20 242	47 236	710	10	10-4	50	10	600
URL	2396 130	3 231 961	1548	5	10-4	200	50	100
spam	82 970	823 470	2048	2	10-4	150	10	200
MNIST	60 000	3898	1000	50	8∙10-4	150	50	1
CIFAR	50 000	150 000	200	20	10-2	100	10	0.5
13
Under review as a conference paper at ICLR 2019
Algorithm 2 Stochastic quasi-Newton with line search
Require: An initial estimate x1 , a maximum number of iterations kmax , memory limit m, regular-
isation parameter λ, scale factor ρ ∈ (0, 1), reduction limit ξ ≥ 1, backtracking limit τ > 0
1: Set k = 1
2: while k < kmax do
3: Obtain a measurement of the cost function and its gradient
fk = f (xk ) + ek ,
gbk = gk + vk .
4:	Set Hk = Yk I where
κγk-1,
Yk = Y Yk-i∕κ,
γk-1,
if αk-1 = 1,
if αk-1 < 1∕ρq,
otherwise.
5:	if ybkTsk > kskk* 1 2 3 42 then
6:	if k > m then
7:	Form Yk and Sk by replacing the oldest vector-pairs in Yk-1 and Sk-1 with ybk and sk
8:	else
9:	Form Yk and Sk by adding ybk and sk to Yk-1 and Sk-1
10:	end if
11:	else
12:	Set Yk = Yk-1 and Sk = Sk-1
13:	end if
14:	Select pbk as
Pk = -Hk Zk - λ 1Sk (YTzk),
zk = gbk - Yk wk ,
wk = Rk 1 (Rk T (YkTgk))，
with details provided in Section 4.2.
15:	SetPk — Pk - βkbk with βk where
p、PTgk - c Tr (H)
βk >	.
gbkTgbk + dσcg2
16:	Set αk = min{1, ξ∕k}
17:	Set i = 1
18:	while f(xk + αPbk) > f(xk) + cαk gbkT Pbk and i ≤ max{0, τ - k} do
19:	Reduce the step length αk J Pak
20:	Set i J— i + 1
21:	end while
22:	Update xk+1 = xk + αkPbk
23:	Setk J k+ 1
24:	end while
14
Under review as a conference paper at ICLR 2019
(b) covtype
(a) gisette
(d) SUSY
(c) HIGGS
(e) epsilon
(g) URL
Figure 2: Performance on classification tasks using a logistic loss with a two-norm regulariser.
(f) RCV1
Time (sec)
(h) spam
15
Under review as a conference paper at ICLR 2019
(b) CIFAR
Figure 3: Solving the optimisation problem used in training a state-of-the-art deep convolutional
neural network (CNN) used for recognising images in the (a) MNIST and (b) CIFAR-10 dataset.
LMLS refers to limited memory least-squares approach developed in this paper, SG refers to basic
stochastic gradient and Adam refers to Kingma & Ba (2015).
16
Under review as a conference paper at ICLR 2019
layer∣ type Iɪ name I	0∣ nput I n∕a∣	1∣ conv I Layerl ∣ 	I _	2∣ IIIPooll Layer2∣ 	I	relu I layer3∣	4∣ conv I Iayer4∣	relu I Layer51	ðl apool I IayerG∣	7∣ conv I layer7∣I	8∣ relu I ayer8∣	9∣ apoolI Layer9∣	101 conv∣ layerl01 Ic	HI relu I ιyerll ∣	121 conv∣ layerl2∣	softmxlI layerl3∣
support I	n∕a∣		I - 5∣		I 3∣	二		Y	ʒl	5∣	二	-"Bl	4∣	^^υ	ι∣	1∣
filt dim I	n∕a∣	3∣	∏∕a∣	n∕a∣	32 I	n∕a∣	n∕a∣	321	n∕a∣	n∕a∣	641	∏∕a∣	641	n/a I
filt dilatI	n∕a∣	1∣	∏∕a∣	n∕a∣	1∣	∏∕a∣	∏∕a∣	ɪl	n∕a∣	n∕a∣	ɪl	∏∕a∣	ɪl	n/a I
num filts∣	n∕a∣	32 I	n∕a∣	n∕a∣	32 I	n∕a∣	n∕a∣	641	n∕a∣	n∕a∣	641	n∕a∣	101	n/a I
stride∣	n∕a∣	1∣	2∣	H	H	H	2∣	H	H	2∣	H	H	H	1∣
Padl	n∕a∣	2∣0×l×θxl∣ 	I	I		0∣	2∣	0∣	3xlx0xlI	2∣	0∣	3xlx0×l∣	0∣	0∣	0∣	0∣
rf size I	n∕a∣	5∣	7∣	7∣	15 I	15 I	191	351	351	431	671	67 I	671	671
rf offset I	n∕a∣	1|	2∣	2∣	2∣	2∣	4∣	4∣	4∣	8∣	201	201	201	201
rf stride∣	n∕a∣	I∣	2∣	2∣	2∣	2∣	4∣	4∣	4∣	8∣	8∣	8∣	8∣	8∣
data size∣	321	32 I	16 I	16 I	16 I	16 I	δl	8|	8|	4∣	1|	ι∣	1|	1∣
data depth∣	3∣	32 I	32 I	32 I	32 I	32 I	32 I	641	641	64 I	641	64 I	10]	1∣
data num I	200 I	200 I	200 I	200 I	200 I	200 I	200 I	2001	200 I	200 I	2001	200 I	2001	1∣
data mem∣	2MB I	25MBI	6MB I	6MB I	6MB I	6MB I	2MB I	3MBI	3MBI	800KB I	50KBI	50KB I	8KBI	4B∣
param mem∣	n∕a∣	10KBI	0B∣	0B∣	100KB I	0B∣	0B∣	200KBI	0B∣	0B∣	256KBI	0B∣	3KBI	0B∣
parameter memory∣		569KB (1	.5e+05	parameters)∣										
data memory I		61MB (for batch size			200) I									
Figure 4: Detailed network strucure in the CIFAR-10 experiment.
17