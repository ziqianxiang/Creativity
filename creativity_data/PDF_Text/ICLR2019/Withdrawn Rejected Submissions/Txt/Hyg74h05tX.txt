Under review as a conference paper at ICLR 2019
Flow++: Improving flow-based generative
MODELS WITH VARIATIONAL DEQUANTIZATION AND
ARCHITECTURE DESIGN
Anonymous authors
Paper under double-blind review
Ab stract
Flow-based generative models are powerful exact likelihood models with efficient
sampling and inference. Despite their computational efficiency, flow-based mod-
els generally have much worse density modeling performance compared to state-
of-the-art autoregressive models. In this paper, we investigate and improve upon
three limiting design choices employed by flow-based models in prior work: the
use of uniform noise for dequantization, the use of inexpressive affine flows, and
the use of purely convolutional conditioning networks in coupling layers. Based
on our findings, we propose Flow++, a new flow-based model that is now the
state-of-the-art non-autoregressive model for unconditional density estimation on
standard image benchmarks. Our work has begun to close the significant perfor-
mance gap that has so far existed between autoregressive models and flow-based
models.
1	Introduction
Deep generative models - latent variable models in the form of variational autoencoders (Kingma &
Welling, 2013), implicit generative models in the form of GANs (Goodfellow et al., 2014), and exact
likelihood models like PixelRNN/CNN (van den Oord et al., 2016b;c), Image Transformer (Parmar
et al., 2018), PixelSNAIL (Chen et al., 2017), NICE, RealNVP, and Glow (Dinh et al., 2014; 2016;
Kingma & Dhariwal, 2018) - have recently begun to successfully model high dimensional raw
observations from complex real-world datasets, from natural images and videos, to audio signals
and natural language (Karras et al., 2017; Kalchbrenner et al., 2016b; van den Oord et al., 2016a;
Kalchbrenner et al., 2016a; Vaswani et al., 2017).
Autoregressive models, a certain subclass of exact likelihood models, achieve state-of-the-art density
estimation performance on many challenging real-world datasets, but generally suffer from slow
sampling time due to their autoregressive structure (van den Oord et al., 2016b; Salimans et al.,
2017; Chen et al., 2017; Parmar et al., 2018). Inverse autoregressive models can sample quickly
and potentially have strong modeling capacity, but they cannot be trained efficiently by maximum
likelihood (Kingma et al., 2016). Non-autoregressive flow-based models (which we will refer to
as “flow models”), such as NICE, RealNVP, and Glow, are efficient for sampling, but have so far
lagged behind autoregressive models in density estimation benchmarks (Dinh et al., 2014; 2016;
Kingma & Dhariwal, 2018).
In the hope of creating an ideal likelihood-based generative model that simultaneously has fast sam-
pling, fast inference, and strong density estimation performance, we seek to close the density esti-
mation performance gap between flow models and autoregressive models. In subsequent sections,
we present our new flow model, Flow++, which is powered by an improved training procedure for
continuous likelihood models and a number of architectural extensions of the coupling layer defined
by Dinh et al. (2014; 2016).
2	Flow Models
A flow model f is constructed as an invertible transformation that maps observed data x to a stan-
dard Gaussian latent variable z = f (x), as in nonlinear independent component analysis (Bell &
1
Under review as a conference paper at ICLR 2019
SejnoWski, 1995; Hyvarinen et al., 2004; Hyvarinen & Pqunen, 1999). The key idea in the de-
sign of a flow model is to form f by stacking individual simple invertible transformations (Dinh
et al., 2014; 2016; Kingma & DhariWal, 2018; Rezende & Mohamed, 2015; Kingma et al., 2016;
Louizos & Welling, 2017). Explicitly, f is constructed by composing a series of invertible floWs as
f (X) = f ι ◦•••◦ fL (x), with each f having a tractable inverse and a tractable Jacobian determinant.
This way, sampling is efficient, as it can be performed by computing f T(Z) = f-1 ◦…◦ f-1(z)
for z 〜N(0, I), and so is training by maximum likelihood, since the model density
log p(X) = log N (f1
◦…fL(X)；ο, I)+X log I ∂fʃiɪ
(1)
is easy to compute and differentiate with respect to the parameters of the flows fi .
3	Flow++
In this section, we describe three modeling inefficiencies in prior work on flow models: (1) uni-
form noise is a suboptimal dequantization choice that hurts both training loss and generalization;
(2) commonly used affine coupling flows are not expressive enough; (3) convolutional layers in the
conditioning networks of coupling layers are not powerful enough. Our proposed model, Flow++,
consists of a set of improved design choices: (1) variational flow-based dequantization instead of
uniform dequantization; (2) logistic mixture CDF coupling flows; (3) self-attention in the condition-
ing networks of coupling layers.
3.1	Dequantization via variational inference
Many real-world datasets, such as CIFAR10 and ImageNet, are recordings of continuous signals
quantized into discrete representations. Fitting a continuous density model to discrete data, how-
ever, will produce a degenerate solution that places all probability mass on discrete datapoints (Uria
et al., 2013). A common solution to this problem is to first convert the discrete data distribution into
a continuous distribution via a process called “dequantization,” and then model the resulting contin-
uous distribution using the continuous density model (Uria et al., 2013; Dinh et al., 2016; Salimans
et al., 2017).
3.1.1	Uniform dequantization
Dequantization is usually performed in prior work by adding uniform noise to the discrete data over
the width of each discrete bin: if each of the D components of the discrete data X takes on values
in {0, 1, 2, . . . , 255}, then the dequantized data is given by y = X + u, where u is drawn uniformly
from [0, 1)D. Theis et al. (2015) note that training a continuous density model pmodel on uniformly
dequantized data y can be interpreted as maximizing a lower bound on the log-likelihood for a
certain discrete model Pmodel on the original discrete data X:
Pmodel (X) :=	pmodel (X + u) du
(2)
The argument of Theis et al. (2015) proceeds as follows. Letting Pdata denote the original distri-
bution of discrete data and pdata denote the distribution of uniformly dequantized data, Jensen’s
inequality implies that
Ey~Pdata [log pmodel(y)] = X Pdata(X)	logpmodel(X+ u) du
≤ X Pdata(X) log	pmodel (X + u) du
=Ex~Pdata [log Pmodel(X)]
(3)
(4)
(5)
Consequently, maximizing the log-likelihood of the continuous model on uniformly dequantized
data cannot lead to the continuous model degenerately collapsing onto the discrete data, because its
objective is bounded above by the log-likelihood of a discrete model.
2
Under review as a conference paper at ICLR 2019
3.1.2	Variational dequantization
While uniform dequantization successfully prevents the continuous density model pmodel from col-
lapsing to a degenerate mixture of point masses on discrete data, it asks pmodel to assign uniform
density to unit hypercubes x + [0, 1)D around the data x. It is difficult and unnatural for smooth
function approximators, such as neural network density models, to excel at such a task. To sidestep
this issue, we now introduce a new dequantization technique based on variational inference.
Again, We are interested in modeling D-dimensional discrete data X 〜 Pdata using a continuous
density model pmodel, and we will do so by maximizing the log-likelihood of its associated discrete
model Pmodel (X) := [0,1)D pmodel (X + u) du. NoW, hoWever, We introduce a dequantization noise
distribution q(u|X), With support over u ∈ [0, 1)D. Treating q as an approximate posterior, We have
the folloWing variational loWer bound, Which holds for all q:
Ex~Pdata [lOg Pmodel (X)] = Ex-Pdata log /0 J)D q(UIX) PmodeuX+ U) 对
pmodel (X + U)
≥ EX-Pdata [/[0,1)D q(U|X) lOg	q(u|x)	dU
pmodel (X + U)
=Ex 〜PdataEu 〜q(∙∣x) lθg --7-i-)—
(6)
(7)
(8)
We Will choose q itself to be a conditional floW-based generative model of the form U = qx(),
where W 〜p(e) = N(e; 0, I) is Gaussian noise. In this case, q(u∣x) = p(q-1(u)) ∙ Idq-I/∂u∣, and
thus We obtain the objective
Ex~pdata [logPmodei(x)] ≥ Ex~Pdat-logMdxx+d”)	⑼
which we maximize jointly over pmodel and q. When pmodel is also a flow model X = f-1(z) (as itis
throughout this paper), it is straightforward to calculate a stochastic gradient of this objective using
the pathwise derivative estimator, as f(X + qx(W)) is differentiable with respect to the parameters of
f and q .
Notice that the lower bound for uniform dequantization - eqs. (3) to (5) - is a special case of our
variational lower bound - eqs. (6) to (8), when the dequantization distribution q isa uniform distribu-
tion that ignores dependence on X. Because the gap between our objective (8) and the true expected
log-likelihood Ex〜尸壮^ [logPmOdel(x)] is exactly Ex〜PdataDKL (q(u∣x) k Pmodei(u∣x))], using a
uniform q forces pmodel to unnaturally place uniform density over each hypercube X + [0, 1)D to
compensate for any potential looseness in the variational bound introduced by the inexpressive q.
Using an expressive flow-based q, on the other hand, allows pmodel to place density in each hyper-
cube x + [0, 1)D according to a much more flexible distribution q(U|x). This is a more natural task
for pmodel to perform, improving both training and generalization loss.
3.2	Improved coupling layers
Recent progress in the design of flow models has involved carefully constructing flows to increase
their expressiveness while preserving tractability of the inverse and Jacobian determinant computa-
tions. One example is the invertible 1 × 1 convolution flow, whose inverse and Jacobian determinant
can be calculated and differentiated with standard automatic differentiation libraries (Kingma &
Dhariwal, 2018). Another example, which we build upon in our work here, is the affine coupling
layer (Dinh et al., 2016). It is a parameterized flow y = fθ(x) that first splits the components of x
into two parts x1, x2, and then computes y = (y1, y2), given by
yι = xι, y = x2 ∙ exp(aθ(xι)) + bθ(xι)
(10)
Here, aθ and bθ are outputs of a neural network that acts on x1 in a complex, expressive manner, but
the resulting behavior on x2 always remains an elementwise affine transformation - effectively, aθ
and bθ together form a data-parameterized family of invertible affine transformations. This allows
the affine coupling layer to express complex dependencies on the data while keeping inversion and
3
Under review as a conference paper at ICLR 2019
log-likelihood computation tractable. Using ∙ and exp to respectively denote elementwise multipli-
cation and exponentiation,
∂y	>
xι = yι,	X2 = (y2 - bθ(yι)) ∙ exp(-aθ(yι)), log ∂χ = 1 aθ(xι)	(11)
The splitting operation x 7→ (x1, x2) and merging operation (y1, y2) 7→ y are usually performed
over channels or over space in a checkerboard-like pattern (Dinh et al., 2016).
3.2.1	Expressive coupling transformations with continuous mixture CDFs
We found in our experiments that density modeling performance of these coupling layers could be
improved by augmenting the data-parameterized elementwise affine transformations by more gen-
eral nonlinear elementwise transformations. For a given scalar component x of x2 , we apply the
cumulative distribution function (CDF) for a mixture of K logistics - parameterized by mixture
probabilities, means, and log scales π, μ, S - followed by an inverse sigmoid and an afine transfor-
mation parameterized by a and b:
X 1——→ σ-1 (MixLogCDF(x; π, μ, S)) ∙ exp(a) + b	(12)
K
where MixLogCDF(x; π, μ, s) := S^^niQ ((x — μi) ∙ exp(-si))	(13)
i=1
The transformation parameters π, μ, s, a, b for each component of x2 are produced by a neural
network acting on x1 . This neural network must produce these transformation parameters for each
component of x2, hence it produces vectors a&(xι) and bθ(xι) and tensors ∏θ(xι), μθ(xι), sθ(xι)
(with last axis dimension K). The coupling transformation is then given by:
yι = xι,	y = σ-1 (MixLogCDF(x2; ∏θ(xι), μθ(xι), sθ(xι))) ∙ exp(aθ(xι)) + bθ(xι)
(14)
where the formula for computing y2 operates elementwise.
The inverse sigmoid ensures that the inverse of this coupling transformation always exists: the range
of the logistic mixture CDF is (0, 1), so the domain of its inverse must stay within this interval. The
CDF itself can be inverted efficiently with bisection, because it is a monotonically increasing func-
tion. Moreover, the Jacobian determinant of this transformation involves calculating the probability
density function of the logistic mixtures, which poses no computational difficulty.
3.2.2	Expressive conditioning architectures with self-attention
In addition to improving the expressiveness of the elementwise transformations on x2, we found it
crucial to improve the expressiveness of the conditioning on xι - that is, the expressiveness of the
neural network responsible for producing the elementwise transformation parameters π, μ, s, a, b.
Our best results were obtained by stacking convolutions and multi-head self attention into a gated
residual network (Mishra et al., 2018; Chen et al., 2017), in a manner resembling the Transformer
(Vaswani et al., 2017) with pointwise feedforward layers replaced by 3 × 3 convolutional layers. Our
architecture is defined as a stack of blocks. Each block consists of the following two layers connected
in a residual fashion, with layer normalization (Ba et al., 2016) after each residual connection:
Conv = Input → Nonlinearity → Conv3×3 → Nonlinearity → Gate
Attn = Input → Conv1×1 → MultiHeadSelfAttention → Gate
where Gate refers to a 1 × 1 convolution that doubles the number of channels, followed by a
gated linear unit (Dauphin et al., 2016). The convolutional layer is identical to the one used by
PixelCNN++ (Salimans et al., 2017), and the multi-head self attention mechanism we use is identi-
cal to the one in the Transformer (Vaswani et al., 2017). (We always use 4 heads in our experiments,
since we found it to be effective early on in our experimentation process.)
With these blocks in hand, the network that outputs the elementwise transformation parameters is
simply given by stacking blocks on top of each other, and finishing with afinal convolution that
increases the number of channels to the amount needed to specify the elementwise transformation
parameters.
4
Under review as a conference paper at ICLR 2019
4	Experiments
Here, we show that Flow++ achieves state-of-the-art density modeling performance among non-
autoregressive models on CIFAR10 and 32x32 and 64x64 ImageNet. We also present ablation ex-
periments that quantify the improvements proposed in section 3, and we present example generative
samples from Flow++ and compare them against samples from autoregressive models.
Our experiments employed weight normalization and data-dependent initialization (Salimans &
Kingma, 2016). We used the checkerboard-splitting, channel-splitting, and downsampling flows
of Dinh et al. (2016); we also used before every coupling flow an invertible 1x1 convolution flows
of Kingma & Dhariwal (2018), as well as a variant of their “actnorm” flow that normalizes all acti-
vations independently (instead of normalizing per channel). Our CIFAR10 model used 4 coupling
layers with checkerboard splits at 32x32 resolution, 2 coupling layers with channel splits at 16x16
resolution, and 3 coupling layers with checkerboard splits at 16x16 resolution; each coupling layer
used 10 convolution-attention blocks, all with 96 filters. More details on architectures, as well as
details for the other experiments, will be given in a source code release.
4.1	Density modeling results
In table 1, we show that Flow++ achieves state-of-the-art density modeling results out of all non-
autoregressive models, and it is competitive with autoregressive models: its performance is on par
with the first generation of PixelCNN models (van den Oord et al., 2016b), and it outperforms
Multiscale PixelCNN (Reed et al., 2017). As of submission, our models have not fully converged
due to computational constraint and we expect further performance gain in future revision of this
manuscript.
Table 1: Unconditional image modeling results
Model family	Model	CIFAR10 bits/dim	ImageNet 32x32 bits/dim	ImageNet 64x64 bits/dim
Non-autoregressive	RealNVP (Dinh et al., 2016)	3.49	4.28	—
	Glow (Kingma & DhariWaL 2018)	3.35	4.09	3.81
	IAF-VAE (Kingma et al., 2016)	3.11	一	—
	Flow++ (ours)	3.09	3.86	3.69
Autoregressive	MultisCale PixelCNN (Reed et al., 2017)	一	3.95	3.70
	PixelCNN (van den Oord et al., 2016b)	3.14	一	—
	PixelRNN (van den Oord et al., 2016b)	3.00	3.86	3.63
	Gated PixelCNN (van den Oord et al., 2016c)	3.03	3.83	3.57
	PixelCNN++ (Salimans et al., 2017)	2.92	一	—
	Image Transformer (Parmar et al., 2018)	2.90	3.77	—
	PixelSNAIL (Chen et al., 2017)	2.85	3.80	3.52
4.2	Ablations
We ran the following ablations of our model on unconditional CIFAR10 density estimation: varia-
tional dequantization vs. uniform dequantization; logistic mixture coupling vs. affine coupling; and
stacked self-attention vs. convolutions only. As each ablation involves removing some component
of the network, we increased the number of filters in all convolutional layers (and attention layers,
if present) in order to match the total number of parameters with the full Flow++ model.
In fig. 1 and table 2, we compare the performance of these ablations relative to Flow++ at 400 epochs
of training, which was not enough for these models to converge, but far enough to see their relative
performance differences. Switching from our variational dequantization to the more standard uni-
form dequantization costs the most: approximately 0.127 bits/dim. The remaining two ablations
both cost approximately 0.03 bits/dim: switching from our logistic mixture coupling layers to affine
5
Under review as a conference paper at ICLR 2019
coupling layers, and switching from our hybrid convolution-and-self-attention architecture to a pure
convolutional residual architecture. Note that these performance differences are present despite
all networks having approximately the same number of parameters: the improved performance of
Flow++ comes from improved inductive biases, not simply from increased parameter count.
The most interesting result is probably the effect of the dequantization scheme on training and gener-
alization loss. At 400 epochs of training, the full Flow++ model with variational dequantization has
a train-test gap of approximately 0.02 bits/dim, but with uniform dequantization, the train-test gap is
approximately 0.06 bits/dim. This confirms our claim in Section 3.1.2 that training with variational
dequantization is a more natural task for the model than training with uniform dequantization.
epoch
Figure 1: Ablation training (light) and validation (dark) curves on unconditional CIFAR10 density
estimation. These runs are not fully converged, but the gap in performance is already visible.
Table 2: CIFAR10 ablation results after 400 epochs of training. Models not converged for the
purposes of ablation study.
Ablation	bits/dim parameters
uniform dequantization	3.292	32.3M
affine coupling	3.200	32.0M
no self-attention	3.193	31.4M
Flow++ (not converged for ablation)	3.165	31.4M
4.3	Samples
We present the samples from our trained density models of Flow++ on CIFAR10, 32x32 ImageNet,
64x64 ImageNet, and 5-bit CelebA in figs. 2 to 5. The Flow++ samples match the perceptual quality
of PixelCNN samples, showing that Flow++ captures both local and global dependencies as well
as PixelCNN and is capable of generating diverse samples on large datasets. Moreover, sampling
is fast: our CIFAR10 model takes approximately 0.32 seconds to generate a batch of 8 samples
in parallel on one NVIDIA 1080 Ti GPU, making it more than an order of magnitude faster than
PixelCNN++ with sampling speed optimizations (Ramachandran et al., 2017). More samples are
available in the appendix (section 7).
5	Related Work
Likelihood-based models constitute a large family of deep generative models. One subclass of such
methods, based on variational inference, allows for efficient approximate inference and sampling,
but does not admit exact log likelihood computation (Kingma & Welling, 2013; Rezende et al.,
2014; Kingma et al., 2016). Another subclass, which we called exact likelihood models in this
work, does admit exact log likelihood computation. These exact likelihood models are typically
6
Under review as a conference paper at ICLR 2019
(a) PixelCNN
Figure 2: CIFAR 10 Samples. Left: samples from van den Oord et al. (2016b). Right: samples from
Flow++, which captures local dependencies well and generates good samples at the quality level of
PixelCNN, but with the advantage of efficient sampling.
Figure 3: 32x32 ImageNet Samples. Left: samples from van den Oord et al. (2016b). Right: samples
from Flow++. Note that diversity of samples from Flow++ matches the diversity of samples from
an autoregressive model on this dataset, which is much larger than CIFAR10.
specified as invertible transformations that are parameterized by neural networks (Deco & Brauer,
1995; Larochelle & Murray, 2011; Uria et al., 2013; Dinh et al., 2014; Germain et al., 2015; van den
Oord et al., 2016b; Salimans et al., 2017; Chen et al., 2017).
There is prior work that aims to improve the sampling speed of deep autoregressive models. The
Multiscale PixelCNN (Reed et al., 2017) modifies the PixelCNN to be non-fully-expressive by in-
troducing conditional independence assumptions among pixels in a way that permits sampling in a
logarithmic number of steps, rather than linear. Such a change in the autoregressive structure al-
lows for faster sampling but also makes some statistical patterns impossible to capture, and hence
reduces the capacity of the model for density estimation. WaveRNN (Kalchbrenner et al., 2018)
7
Under review as a conference paper at ICLR 2019
Figure 4: 64x64 ImageNet Samples. Left: samples from Multi-Scale PixelRNN (van den Oord et al.,
2016b). Right: samples from Flow++. The diversity of samples from Flow++ matches the diversity
of samples from PixelRNN with multi-scale ordering.
Figure 5: Samples from Flow++ trained on 5-bit 64x64 CelebA, without low-temperature sampling.
improves sampling speed for autoregressive models for audio via sparsity and other engineering
considerations, some of which may apply to flow models as well.
There is also recent work that aims to improve the expressiveness of coupling layers in flow models.
Kingma & Dhariwal (2018) demonstrate improved density estimation using an invertible 1x1 con-
volution flow, and demonstrate that very large flow models can be trained to produce photorealistic
faces. Muiller et al. (2018) introduce piecewise polynomial couplings that are similar in spirit to our
mixture of logistics couplings. They found them to be more expressive than affine couplings, but
reported little performance gains in density estimation. We leave a detailed comparison between our
coupling layer and the piecewise polynomial CDFs for future work.
6	Conclusion
We presented Flow++, a new flow-based generative model that begins to close the performance
gap between flow models and autoregressive models. Our work considers specific instantiations
8
Under review as a conference paper at ICLR 2019
of design principles for flow models - dequantization, flow design, and conditioning architecture
design - and we hope these principles will help guide future research in flow models and likelihood-
based models in general.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separa-
tion and blind deconvolution. Neural computation, 7(6):1129-1159, 1995.
Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autore-
gressive generative model. arXiv preprint arXiv:1712.09763, 2017.
Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. arXiv preprint arXiv:1612.08083, 2016.
Gustavo Deco and Wilfried Brauer. Higher order statistical decorrelation without information loss.
Advances in Neural Information Processing Systems, pp. 247-254, 1995.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv
preprint arXiv:1605.08803, 2016.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. arXiv preprint arXiv:1502.03509, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Aapo Hyvarinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and
uniqueness results. Neural Networks, 12(3):429-439, 1999.
Aapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent component analysis, volume 46. John
Wiley & Sons, 2004.
Nal Kalchbrenner, Lasse Espheholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. eural machine translation in linear time. arXiv preprint arXiv:1610.00527, 2016a.
Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex
Graves, and Koray Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527, 2016b.
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lock-
hart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient
neural audio synthesis. arXiv preprint arXiv:1802.08435, 2018.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. Proceedings of the 2nd
International Conference on Learning Representations, 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
autoregressive flow. arXiv preprint arXiv:1606.04934, 2016.
Hugo Larochelle and Iain Murray. The Neural Autoregressive Distribution Estimator. AISTATS,
2011.
9
Under review as a conference paper at ICLR 2019
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural
networks. arXiv preprint arXiv:1703.01961, 2017.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In International Conference on Learning Representations (ICLR), 2018.
Thomas Muller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Novak. Neural im-
portance sampling. arXiv preprint arXiv:1808.03856, 2018.
Niki Parmar, AShiSh Vaswani, Jakob Uszkoreit, Eukasz Kaiser, Noam Shazeer, and Alexander Ku.
Image transformer. arXiv preprint arXiv:1802.05751, 2018.
Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang,
Yang Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast genera-
tion for convolutional autoregressive models. arXiv preprint arXiv:1704.06001, 2017.
Scott E. Reed, Aaron van den Oord, Nal Kalchbrenner, Sergio Gomez, Ziyu Wang, Dan Belov, and
Nando de Freitas. Parallel multiscale autoregressive density estimation. In Proceedings of The
34th International Conference on Machine Learning, 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings
ofThe 32nd International Conference on Machine Learning, pp. 1530-1538, 2015.
Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
imate inference in deep generative models. In Proceedings of the 31st International Conference
on Machine Learning (ICML-14), pp. 1278-1286, 2014.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to ac-
celerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint
arXiv:1701.05517, 2017.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.
Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive
density-estimator. In Advances in Neural Information Processing Systems, pp. 2175-2183, 2013.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016a.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
International Conference on Machine Learning (ICML), 2016b.
Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-
ray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint
arXiv:1606.05328, 2016c.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,
2017.
10
Under review as a conference paper at ICLR 2019
7	Appendix A: Samples
Figure 6: Samples from Flow++ trained on CIFAR10

Ce昵■离
11
Under review as a conference paper at ICLR 2019
Figure 7: Samples from Flow++ trained on 32x32 ImageNet
12
Under review as a conference paper at ICLR 2019
Figure 8: Samples from Flow++ trained on 64x64 ImageNet
13
Under review as a conference paper at ICLR 2019
Figure 9: Samples from Flow++ trained on 5-bit 64x64 ImageNet
14
Under review as a conference paper at ICLR 2019
Figure 10: Samples from Flow++ trained on 5-bit 64x64 CelebA
15
Under review as a conference paper at ICLR 2019
Figure 11: Samples from Flow++ trained on 3-bit 64x64 CelebA
16