Table 1: Comparison of Backplay with related work: Behavioral Cloning (BC), Generative Adversar-ial Imitation Learning (GAIL), and Reverse Curriculum Generation (RCG).
Table 2: Results after 2000 epochs on 100 mazes. Note that for Backplay and Uniform, the Map Setis also the type of demonstrator, where N-optimal has demonstrations N steps longer than the shortestpath. From left to right, the table shows: the percentage of mazes on which the agent optimallyreaches the goal, percentage on which it reaches in at most five steps more than optimal, and theaverage and standard deviation of extra steps over optimal. Both Backplay and Uniform succeedon almost all mazes and, importantly, can outperform the expertsâ€™ demonstrations. On the otherhand, Standard does not learn a useful policy and Florensa fails to learn more than 50 - 70% of themaps, which is why its sub-optimality mean and std is so high. Results for Backplay were generallyconsistent across all seeds. For others, we report their best score. See A.4 for further details.
Table 3: Backplay hyperparameters for Maze.
Table 4: Backplay hyperparameters for Pommerman 4 maps.
Table 5: Backplay hyperparameters for Pommerman 100 maps.
Table 6: Aggregate per-map win rates of the model trained with Backplay on 100 Pommerman maps.
