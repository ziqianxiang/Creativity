Table 2: Transfer evaluation tasks. N = number of training examples; C = number of classes ifapplicable. h1, h2 are sentence representations, fm,s a composition function from section 4.
Table 3: SentEval and base task evaluation results for the models trained on natural language infer-ence (T = NLI ); AllNLI is used for training. All scores are accuracy percentages, except STS14,which is Pearson correlation percentage.
Table 4: SentEval and base task evaluation results for the models trained on discourse connectiveprediction (T = Disc). All scores are accuracy percentages, except STS14, which is Pearsoncorrelation percentage.
Table 5: Comparison models from previous work. InferSent represents the original results fromConneau et al. (2017), SkipT is SkipThought from Kiros et al. (2015), and BoW is our re-evaluationof GloVe Bag of Words from Conneau et al. (2017).
Table 6: Results for sentence relation tasks using an alternative composition function (fCÎ²,-) duringthe evaluation step.
