Table 1: Basic MP theory, and the spiked and Heavy-Tailed extensions we use, including known,empirically-observed, and conjectured relations between them. Boxes marked “*” are best describedas following “TW with large finite size corrections” that are likely Heavy-Tailed (23), leading to bulkedge statistics and far tail statistics that are indistinguishable. Boxes marked “**” are phenomeno-logical fits, describing large (2 < μ < 4) or small (0 < μ < 2) finite-size corrections on N → ∞behavior. See (24; 23; 25; 26; 27; 28; 29; 30; 19; 31) for additional details.
Table 2: The 5+1 phases of learning we identified in DNN training. We observed Bulk+Spikesand Heavy-Tailed in existing trained models (LeNet5 and AlexNet/InceptionV3, respectively; seeSection 3); and we exhibited all 5+1 phases in a simple model (MiniAlexNet; see Section 6).
Table 1: Definitions of acronyms used in the text.
Table 2: Definitions of notation used in the text.
Table 3: Basic MP theory, and the spiked and Heavy-Tailed extensions We use, including knoWn,empirically-observed, and conjectured relations between them. Boxes marked “*” are best de-scribed as folloWing “TW With large finite size corrections” that are likely Heavy-Tailed [20],leading to bulk edge statistics and far tail statistics that are indistinguishable. Boxes marked“**” are phenomenological fits, describing large (2 < μ < 4) or small (0 < μ < 2) finite-sizecorrections on N → ∞ behavior. See [38, 20, 19, 111, 7, 40, 8, 26, 22, 21] for additional details.
Table 4: Description of main DNNs used in our analysis and the key observations about theESDs of the specific layer weight matrices using RMT. Names in the “Key observation” columnare defined in Section 5 and described in Table 7.
Table 5: Fit of PL exponents for the ESD of selected (2D Linear) layer weight matrices Wl inpre-trained models distributed with pyTorch. Layer is identified by the enumerated id of thepyTorch model; Q = N/M ≥ 1 is the aspect ratio; (M × N) is the shape of WlT ; α is the PLexponent, fit using the numerical method described in the text; D is the Komologrov-Smirnovdistance, measuring the goodness-of-fit of the numerical fitting; and “Best Fit” indicates whetherthe fit is better described as a PL (Power Law) or TPL (Truncated Power Law) (no fits werefound to be better described by Exponential or LogNormal).
Table 6: Allen NLP Models and number of Linear Layers per model examined.
Table 7: The 5+1 phases of learning we identified in DNN training. We observed Bulk+Spikesand Heavy-Tailed in existing trained models (LeNet5 and AlexNet/InceptionV3, respectively;see Section 4); and we exhibited all 5+1 phases in a simple model (MiniAlexNet; see Section 7).
