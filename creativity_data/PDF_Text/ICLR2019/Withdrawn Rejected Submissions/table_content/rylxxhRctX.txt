Table 1: Comparison of GAN and VAE baselines, our model using both losses (CQ), and our modelusing the additional flow-based layers (CQF).
Table 2: Evaluation of more advanced architectures.
Table 3: Evaluation on CIFAR-10 of different architectures of the invertible layers of the model.
Table 4: Comparison of our models on CIFAR-10 and STL-10 (48Ã—48) with state-of-the-art genera-tive likelihood-based and adversarial models, as well as the hybrid SVAE.
Table 5: Evaluation of our CQF model on additional datasets. IS is not reported on CelebA andLSUN because it is not informative on these datasets.
Table 6: Residual architectures for experiments from Table 2 and Table 313Under review as a conference paper at ICLR 2019B Qualitative influence of the feature space flexibility in amaximum-likelihood settingIn Figure 6 we show samples obtained using VAE models trained with MLE. The models includeone without invertible decoder layers, and with NVP layers using one, two and three scales. Thesamples illustrate the dramatic impact of using invertible NVP layers in these autoencoders.
