Table 1: Perplexity of trained models by number of training sentences. All but the language modelsare 1000D BiLSTMs (500D per direction). The 500D forward and backward language models arecombined into a single bidirectional language model for analysis experiments.
Table 2: Here we display results for training on all of auxiliary task data. Word-conditional mostfrequent class baselines for this amount of training data are 89.9% for POS tagging and 71.6% forCCG supertagging. For each task, we underline the best performance for each training dataset sizeand bold the best overall performance.
Table 3: Here we display results for training on 10% of auxiliary task data. Word-conditional mostfrequent class baselines for this amount of training data are 88.6% for POS tagging and 68.3% forCCG supertagging. For each task, we underline the best performance for each training dataset sizeand bold the best overall performance.
Table 4: Here we display results for training on 1% of auxiliary task data. Word-conditional mostfrequent class baselines for this amount of training data are 81.8% for POS tagging and 62.3% forCCG supertagging. For each task, we underline the best performance for each training dataset sizeand bold the best overall performance.
