Table 1: Training and Evaluation DatasetsTensorflow Abadi et al. (2015) using a standard convolutional VGG-16 (Simonyan & Zisserman,2015) architecture. The only difference is that the fully connected layers have a dimensionality of2048, rather than 4096, and that Leaky-ReLU activations were used instead of ReLU. The dropoutkeep probability for the convolution layers was 0.3+ the keep probability for fully connected layer.
Table 2: Training ConfigurationThe classification error rates on the test data for all models are given in the table 3, which shows thatall models have comparable error rates and that adversarial training has improved the error rate ofPN-ADV relative to PN.
Table 3: Classification Error rates on CIFAR-10 test data.
Table 4: Out-of-Distribution Detection11Under review as a conference paper at ICLR 2019Appendix B	Uncertainty for DNNsThe simplest approach to modeling uncertainty for classification is to use a DNN to parameterize adiscrete distribution over class labels conditioned on the input (eq. 20):P(y∣χ*; θ) = f (χ*; θ)(20)which is the standard approach to classification using neural networks. This model will capture datauncertainty when trained via maximum likelihood. Specifically, consider the derivation in eq. 21,which shows that the expected negative log-likelihood of a model, given the real underlying datadistribution Ptr(y, x) can be expressed as the expected KL divergence between the model P(y|x; θ)3and the true conditional distribution Ptr(y|x), which is the reducible loss, and the entropy ofPtr(y|x),which is the irreducible loss. The irreducible loss represents the data uncertainty - uncertainty due to,for example, class overlap. Thus, as the reducible loss is minimized, the model learns not only toyield the correct classifications, but also to capture the uncertainty inherent in the data.
