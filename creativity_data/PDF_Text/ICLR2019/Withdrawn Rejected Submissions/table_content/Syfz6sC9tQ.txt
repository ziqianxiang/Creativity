Table 1: CIFAR10 results for GFMN with different feature extractors.
Table 2: Impact of the number of layers/features used for feature matching in GFMN (1K=210).
Table 3: Inception Score and FID of different generative models for CIFAR10 and STL10.
Table 4:	GeneratorS14Under review as a conference paper at ICLR 2019RGB image x ∈ Rm×m×3^^4 X 4,stride=2 Conv 64 ReLU3 × 3,stride= 1 Conv 64 ReLU3 × 3,stride=1 Conv 64 ReLU4 × 4,stride=2 Conv 128 ReLU4 × 4,stride=2 ConV 256 ReLUdense (FC) 7 100 一DCGAN like EncoderRGB image x ∈ Rm×m×3			3 × 3, 64 3 × 3, 64	× 1maxpool			3 × 3, 128 3 × 3, 128	× 1maxpool			3 × 3, 256 3 × 3, 256	× 2maxpool			3 × 3, 512 3 × 3, 512	× 2maxpool		
Table 5:	Feature extractor architectures. m = 32 for MNIST, CIFAR10 and STL10. m = 64 forCelebA and ImageNetA.3 Pretraining of ImageNet ClassifiersBoth VGG19 and Resnet18 networks are trained with SGD with fixed 10-1 learning rate, 0.9momentum term, and weight decay set to 5 × 10-4. We pick models with best top 1 accuracy on thevalidation set over 100 epochs of training; 29.14% for VGG19 (image size 32×32), and 39.63% forResnet18 (image size 32×32). When training the classifiers we use random cropping and randomhorizontal flipping for data augmentation. When using VGG19 and Resnet18 as feature extractors inGFMN, we use features from the output of each ReLU that follows a conv. layer, for a total of 16layers for VGG and 17 for Resnet18.
Table 6: Cross-domain results for STL10 and CIFAR10 autoencoder features.
