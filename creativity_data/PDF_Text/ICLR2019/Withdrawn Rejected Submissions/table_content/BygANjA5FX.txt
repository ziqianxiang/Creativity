Table 1: Test set mean error rates and standard deviation in percentage on the MNIST dataset withdifferent configurations of the tested model architecture. m = 1 indicates regular CNNs and m = 3indicates a CNN with IEA that consists of three replications of a convolutional layer.
Table 2: Test set error rates in percentage of average ensemble models on the MNIST dataset withdifferent configurations of the tested model architecture. k represents the number of individualnetworks within an ensembleModel depth	m= 1,k=	3	m = 3, k =	31 layer	1.32		1.32	2 layers	1.39		0.68	3 layers	1.48		0.38	Table 3 describes the rotated-MNIST test dataset results. In this case, the usage of IEA of convo-lutional layers still leads to significant improvements over convolutional-layer-only models. It canbe seen in the case of the one-layer model that the IEA of convolutional layers model significantlyimproves the accuracy compared to convolutional-layer-only models. The performance increaseswhen more layers are added (deeper model). We can see a drop of the mean error rate by 0.41% inthe case of two layers deep model, and a drop of 0.68% with three layers deep one. These resultsshow the capability of IEA in decreasing the total model variance. Table 4 shows the case of averageensemble of three IEA of convolutional layer models and convolutional-layer-only models. In the caseof a one layer deep model, the ensemble of convolutional layer models is better than the ensembleof IEA of convolutional layer models. When more layers are considered, the ensemble of IEA ofconvolutional layer models outperforms the ensemble of convolutional-layer-only models, except inthe case of two layers model were the ensemble of convolutional-layer-only models has a slightlybetter performance. We attribute this behavior to the selection of hyper-parameters. Also, an error
Table 3: Test set mean error rates and standard deviation in percentage on the rotated-MNIST datasetwith different configurations of the tested model architecture.
Table 4: Test set error rates in percentage of average ensemble models on the rotated-MNIST datasetwith different configurations of the tested model architecture.
Table 5: Test set mean error rates and standard deviation in percentage on the CIFAR-10 dataset.
Table 6: Test set average ensemble error rates in percentage on the CIFAR-10 dataset.
Table 7: Test set mean error rates and standard deviation in percentage on the CIFAR-100 dataset.
Table 8: Test set average ensemble error rates in percentage on the CIFAR-100 dataset.
Table 9: Inference time for models trained on CIFAR-100 in milliseconds per image using NVIDIAGeForce GTX 1080Ti GPU.
Table 10: Features generated by vanilla CNN with IEA components and regular convolutional layers.
Table 11: The mss-scores of features generated by convolutional layers and IEAs. The score ismeasured for the first layer only in the models. The models were trained on rotated-MNIST dataset.
Table 12: Features generated by vanilla CNN with IEA components and the same model usingMaxOut. The models were trained on the rotated-MNIST dataset. The input images are shown ina gray scale while the features are shown using a heat map color. The black shade in the heat mapindicates a minimum pixel value, while the white color indicates a maximum pixel value.
