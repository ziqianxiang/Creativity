Table 1: Average reward and penalty for the different control benchmark tasks and policies trained inthe constrained, unconstrained and original reward setup. In all cases, the constraint-based approachresults in the lowest average penalty. While the lower bound was set to 0.9 of a maximum of 1, weobtain the same average reward as the unconstrained case for the cartpole swingup and humanoidstand tasks.
Table 2: Results for models trained to achieve a fixed lower bound on the velocity. Reported numbersare average per-step (velocity overshoot [m/s], penalty [W]), except for the unbounded case wherewe report actual velocity. Each entry is an average over 4 seeds. We highlight the best constant Î±,in terms of smallest overshoot, for each target bound.
Table 3: Results of models that are conditioned on the target velocity, evaluated for for differentvalues. Reported numbers are average per-step (velocity overshoot [m/s], penalty [W]). Each rowis an average over 4 seeds. The highlighted numbers mark the best individual alpha for each targetvelocity (in terms of velocity overshoot).
Table 4: Overview of the hyperparameters used for the experiments.
Table 5: Overview of the different model variations and noise models in the Minitaur domain.
