Table 1: Quantitative and human evaluations for different mask rates and number of segments.
Table 2: An example from the Yelp data where the template contains two missing portions and 40%of the tokens are masked out.
Table 3: BLEU scores on the Grimm test set for infilling prepositionsTemplate	__m__ old woman went __m__ , but saw __m__ one on the stairsGround Truth	the old woman went out, but saw no one on the stairsSeq2Seq	the old woman went With , but saw at one on the stairsGAN	the old woman went for , but saw no one on the stairsSelf-attn	the old woman went in , but saw that one on the stairsTable 4: An example from the Grimm Tales data where prepositions are masked out.
Table 4: An example from the Grimm Tales data where prepositions are masked out.
Table 5: Perplexity on the train/test sets of Grimm Tales for language models with anchor words.
Table 6: Examples for language models with anchor words.
Table 7: Perplexity on the train/test sets of the NBA scripts for language models with anchor words.
Table 8: Examples of the NBA scripts for language models with anchor words.
Table 9: Examples for language models with anchor words on Grimm Tales.
Table 10: Examples of the NBA scripts for language models with anchor words.
