Table 1: Performance comparison: bold type marks the best performance, and italics type marksthe second from the best performanceMNIST	Original	FGSM =0.5	DIST c=0.01	PGD = 0.3, 50 iterations	Mixup α = 0.12, 10×data	Ours r = 8, 10×dataCW rob	2.442	2.390	2.5010	2.343	2.803	3.554Test acc	0.9818	0.9817	0.9873	0.9869	0.9904	0.9904FGSM1 acc	0.5382	0.6375	0.8542	0.7511	0.8323	0.9292FGSM3 acc	0.2606	0.8963	0.1169	0.5840	0.2623	0.5558FGSM5 acc	0.1423	0.9390	0.0244	0.1340	0.1344	0.2878PGD 3 acc	0.0126	0.0258	0.0065	0.2534	0.0180	0.1281GAU 5 acc	0.6358	0.6316	0.5735	0.5886	0.5813	0.9556CIFAR-10	Original	FGSM	DIST	PGD	Mixup	Ours		=0.5	c=0.01	= 0.3, 50 iterations	α = 0.12, 16×data	r = 10, 16×dataCW rob	38.010	38.210	38.503	38.108	37.648	38.746Test acc	0.8395	0.7995	0.7935	0.7791	0.8521	0.8249FGSM1 acc	0.4922	0.4927	0.3825	0.4588	0.7483	0.6853FGSM3 acc	0.4463	0.6517	0.2241	0.3848	0.7287	0.6806FGSM5 acc	0.4093	0.7572	0.1998	0.3405	0.7192	0.6721PGD 3 acc	0.2987	0.2233	0.1871	0.5291	0.5018	0.4111GAU 5 acc	0.3701	0.6356	0.6169	0.5390	0.3371	0.6961timization Sinha et al. (2017) can improve the CW robustness of the DNN model and demonstrate
Table 2: Performance comparison on ImageNet: Tested on ResNet-18 (He et al. (2016)) modelafter 90 epochs training. Comparison only done between Bamboo and Mixup (Zhang et al. (2017))against FGSM, due to the lack of support of the effectiveness and the lack of open-sourced imple-mentations of other defending and attacking methods on ImageNet	Original	Mixup	OursTop-1 acc	57.336	58.213	60.520Top-5 acc	80.647	81.452	83.216Top-1 FGSM	11.342	12.947	14.062Top-5 FGSM	22.860	26.400	26.5629Under review as a conference paper at ICLR 2019We also note that the overall performance of this method on CIFAR-10 dataset is not as good asthat on MNIST, possibly due to the scalability issue of the min-max optimization as elaborated inEquation (6). A large-scale CNN and larger input space for the CIFAR-10 experiment may be toocomplicated to efficiently find an optimal solution. Although not specially designed against adver-sarial attack, the performance of Mixup Zhang et al. (2017) is promising on robustness gain and theaccuracy against adversarial attack with small strength. However, the overall robustness achievedby Mixup, indicated by the CW robustness, is not as good as what is achieved by Bamboo. TheImageNet experiment results showed in Table 2 show the same trend as well.
