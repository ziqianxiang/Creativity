Table 1: Dataset description. Cifar-10 is transformed into 10 anomaly detection tasks, whereby oneclass is used as the normal class, the remaining classes are the anomalies. Further, note that thetrain & validation dataset contains only normal data samples. This scenarios resembles the typicalsituations where anomalies are extremely rare and not available at training time.
Table 2: Anomaly detection on CIFAR-10, performance measured in AUROC. Each class is consid-ered as the normal class once with all other classes being considered as anomalies, resulting in 10one-vs-nine classification tasks. Performance is averaged for all ten tasks and over three runs each.
Table 3: Anomaly detection performance on Metal Anomaly dataset. To reduce noisy residuals dueto the high-dimensional input domain, only 10% of maximally abnormal pixels with the highestresiduals are summed to form the total anomaly score. AUROC is computed on an unseen test set, acombination of normal and anomaly data. For more detailed results, refer to attachment H. Anomalydetection performance of plain MHP rapidly breaks down with increasing number of hypotheses.
Table 4: CIFAR-10 anomaly detection: AUROC-performance of different approaches. The columnindicates which class was used as in-class data for distribution learning. Note that random perfor-mance is at 50% and higher scores are better. Top-2-methods are marked. Our ConAD approachoutperforms traditional methods and vanilla MHP-approaches significantly and can benefit from anincreasing number of hypotheses. Furthermore, Mixture Density Networks perform similarly touni-modal output distributions of VAEs.
Table 5: Anomaly detection performance on the Metal Anomaly dataset, measured in AUROC,showing how different multiple hypothesis approaches perform with increasing number of hypothe-ses. Vanilla single-hypothesis approaches such as VAE and VAE+GAN under-perform on this task.
