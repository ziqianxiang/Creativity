Table 1: Percent reward obtained when the algorithm (column) is used to infer the bias of the demonstrator(row). The optimal and Boltzmann algorithms assume a fixed model of the demonstrator and train the VIN tomimic the model before performing reward inference (and were used in figure 3). We also include the fourflavors of Algorithm 2 that were plotted in figure 4. The VI algorithm uses a differentiable implementation ofsoft value iteration as the planner instead of a VIN (used in section 5.2). The demonstrators are the optimalagent, the biased agents of figure 1, and versions of each of these agents with Boltzmann noise.
Table 2: Accuracy when predicting the demonstratorâ€™s actions (row) on new gridworlds using the planner andreward inferred by the algorithm (column). Algorithms and demonstrators are the same as in Table 1.
