Table 1: Network architectures used for experiments on CIFAR-10 and STL-10. Images are assumedtobe of size 32 × 32 for CIFAR-10 and 64 × 64 for STL-10. We setM = 512 for CIFAR-10 andM = 1024 for STL-10. Layers in parentheses are only included for STL-10. The noise-generatornetwork follows the generator architecture with the number of channels reduced by a factor of 8.
Table 2: We perform ablation experiments on CIFAR-10 and STL-10 to demonstrate the effective-ness of our proposed algorithm. Experiments (a)-(c) show results where only noisy examples are fedto the discriminator. Experiment (c) corresponds to previously proposed noise-annealing and resultsin an improvement over the standard GAN training. Our approach of feeding both noisy and cleansamples to the discriminator shows a clear improvement over the baseline.
Table 3: We apply our proposed method to various previous GAN models trained on CIFAR-10 andCelebA. The same network architectures and hyperparameters as in the original works are used. Wecan observe that using our approach increases performance in most cases even with the suggestedhyperparameter settings. Note that our algorithm also allows successful training with the originalminimax GAN loss as opposed to the commonly used heuristic (e.g., in DCGAN).
Table 4: Hyperparameter settings used to evaluate the robustness of our proposed GAN stabilizer.
