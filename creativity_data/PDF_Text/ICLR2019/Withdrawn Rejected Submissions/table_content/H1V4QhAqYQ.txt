Table 1: Validation accuracy (Top1) results for Cifar, ImageNet models. Bottom: test perpleXityresult on Penn-Tree-Bank (PTB) datasetNetwork	Dataset	Baseline	BatchAugmentResNet44 (He et al., 2016)	Cifar10	93.07%	93.65% (M=10)ResNet44 + cutout	Cifar10	93.7%	95.27% (M=40)VGG + cutout (Simonyan & Zisserman, 2014)	Cifar10	93.82%	95.32% (M=32)Wide-ResNet28-10 + cutout (Zagoruyko, 2016)	Cifar10	96.6%	97.15% (M=6)DARTS (Liu et al., 2018)	Cifar10	97.11%	97.64% (M=10)ResNet44 + cutout	Cifar100	72.97%	74.13% (M=40)VGG + cutout	Cifar100	73.03%	75.5% (M=32)Wide-ResNet28-10 + cutout	Cifar100	79.85%	80.13% (M=10)DenseNet100-12 (Huang et al.)	Cifar100	77.73%	78.8% (M=4)AlexNet (Krizhevsky et al., 2012)	ImageNet	58.25%	62.31% (M=8)MobileNet (Howard et al., 2017)	ImageNet	70.6%	71.4% (M=4)ResNet50 (He et al., 2016)	ImageNet	76.3%	76.8% (M=4)Word-level LSTM (Merity et al., 2017)	PTB	58.8 ppl	58.6 ppl (M=10)5Under review as a conference paper at ICLR 20193.3	Dropout as intermediate augmentationWe also wished to test the ability of batch augmentation to improve results in tasks where no ex-
Table 2: The speed and time for AlexNet-BN. Table from You et al. (2017)Batch Size	Stable Accuracy	4-GPU speed	4-GPU time512	0.602	6627 img/sec	5h 22m 30s=4096	0.604	6585 img/sec	5h 24m 44s ä¸€ReferencesAntreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarialnetworks. arXiv preprint arXiv:1711.04340, 2017.
