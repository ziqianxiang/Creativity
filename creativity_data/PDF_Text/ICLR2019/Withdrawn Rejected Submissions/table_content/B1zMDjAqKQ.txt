Table 1: Mean accuracy, in percentage, and standard deviation of our experiments: the baselinesresults, training the network with half and all the samples of the training set and reconstructing theabsent modality using the expectation mechanism.
Table 2:	Training parameters of the Vision channelParameterEpochsBatch sizeOptimizerInitial learning rateADAM beta1ADAM beta2Value20032ADAM (Adaptive Learning rate)0.050.90.9999.1.2 Auditory channelTable 3 exhibits all the important parameters used to train our auditory channel. We follow the sametraining procedure as the vision channel, and also use ADAM optimizer with an adaptive learningrate.
Table 3:	Training parameters of the Auditory channelParameterEpochsBatch sizeOptimizerInitial learning rateADAM beta1ADAM beta2Value25032ADAMADAM (Adaptive Learning rate)0.90.9999.1.3 Self-organizing binding layerTable 4 exhibits all the important parameters used to train our gamma Growing-When-Required(GWR) network. We use a small insertion threshold, which helps the network to maintain a limitednumber of neurons, reinforcing the generation of high-abstract clusters.
Table 4: Training parameters of the Auditory channelParameter	ValueEpochs	50Insertion threshold	0.01Context size	4Initial Gamma Weights	0.64391426, 0.23688282, 0.08714432, 0.0320586Î²b	0.5b	0.2n	0.003Table 5: Detailed results in accuracy (in %) and standard deviation for our baseline experiments.
Table 5: Detailed results in accuracy (in %) and standard deviation for our baseline experiments.
Table 6: Detailed accuracy (in %) for our baseline experiments.
