Table 1: Planning Error of the forward prediction learned on the data collected by the explorationpolicy for multi-object scenarios.
Table 2:	Multi-step Learning (Simulation)OUrs	REINFORCE	DQN11%	6.0%	6.5%Table 3:	Real-world Robot TestOurs REINFORCE RANDOM91%	1%	17%Figure 3: (Left) Interaction Rate vs. Number of Samples for real-world robot. (Right) Table 2 showsinteraction rate with 250 steps and Table 3 shows the interaction rate of final robot model.
Table 3:	Real-world Robot TestOurs REINFORCE RANDOM91%	1%	17%Figure 3: (Left) Interaction Rate vs. Number of Samples for real-world robot. (Right) Table 2 showsinteraction rate with 250 steps and Table 3 shows the interaction rate of final robot model.
Table 1: Prediction Loss of the forward prediction learned on the data collected by the explorationpolicy for multi-object scenarios on real robot.
Table 2: Discussing the behavior of exploration reward for count-based method, curiosity (dynamics-based) and our differentiable exploration. +ve and -ve refers to the gradient direction for the actionin our case. *,* refers to the cases when our objective of differentiable exploration behaves differentlythan count-based and curiosity.
