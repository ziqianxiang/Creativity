Table 1: Adapted Behaviors Distance (ABD), the lower the better5.4	Efficiency Analysis-DO: -14.04	-	D5: -9.59一 DI: -14.07	-	06: -4.ββ-D2:-12.3	=	D7:	-11.16-D3: -10.66	-	DB: -8.01D4: -4.79	-	D9：	-10-77Figure 4: Heuristics Scores for imitation learning from scratch at 500 iterations18	1»	2jβ 3<K 3Sβ ««	450	∞0Iterations(a) Martial Arts10β	1∞	∞	2∞	3««	3∞ 4ββ 4∞	«9«Iterations(b) HandstandIn order to verify the efficiency as compared to demonstration judging via training from scratch, Weset a limited quota be the total number of iterations required for meta-training the adaptive parametersθ plus the number of gradient updates during adaptation to each demonstration, which sums upto around 6,000 and 7,500 for our martial arts and handstand environments respectively. We thenevenly distribute this quota to train different agents imitating each of the demonstration from thenoisy set, and output their top choices. In Fig.4, the heuristics curves generated up to 500 iterations
Table 2: Setups of our simulation environmentsA.2 Model ArchitecturesWe build the policy networks for each of the simulation environments using a 2-layer MLP. Theposterior network Q used in MAML + MI is implemented as a siamese 1-layer Long-Short TermMemory (LSTM) (Hochreiter & Schmidhuber, 1997) network followed by a two consecutive fully-connected networks to produce the 1-dimensional embedding as the prediction of the encodeddemonstration feature. In this paper, we simply use l2-norm of observations throughout the entireepisode of a demonstration as its encoded feature.
Table 3: Network architectures used in this paper, the MLP(h1,h2) are two-layer multi-layer perceptrons withhidden sizes h1 and h2. Siamese-LSTM(h1, L) indicates how L layers of LSTM cells with hidden szie h1.
