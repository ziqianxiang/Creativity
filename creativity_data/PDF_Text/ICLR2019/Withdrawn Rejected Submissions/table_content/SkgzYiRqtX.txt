Table 1: Hyper-parameters settings.
Table 2: Results on human annotated datasetDataset MetriC	Distantly Labeled Test Set P@5% P@10%	P@15% P@20%				Dense Distantly Labeled Test Set P@5%	P@10%	P@15%	P@20%			Multi-Window CNN	78.9	78.4	76.2	72.9	86.2	83.4	81.4	79.1PCNN	73.0	65.4	58.1	51.2	85.3	79.1	72.4	68.1Context-Aware RE	90.8	89.9	88.5	87.2	93.5	93.0	93.8	93.0GP-GNN (#layers=1)	90.5	89.9	88.2	87.2	97.4	93.5	92.4	91.9GP-GNN (#layers=2)	92.5	92.0	89.3	87.1	95.0	94.6	95.2	94.2GP-GNN (#layers=3)	94.2	92.0	89.7	88.3	98.5	97.4	96.6	96.1Table 3: Results on distantly labeled test set5.3	Effectiveness of Reasoning MechanismFrom Table 2 and 3, we can see that our best models outperform all the baseline models significantlyon all three test sets. These results indicate our model could successfully conduct reasoning on thefully-connected graph with generated parameters from natural language. These results also indicatethat our model not only performs well on sentence-level relation extraction but also improves onbag-level relation extraction. Note that Context-Aware RE also incorporates context informationto predict the relation of the target entity pair, however, we argue that Context-Aware RE onlymodels the co-occurrence of various relations, ignoring whether the context relation participatesin the reasoning process of relation extraction of the target entity pair. Context-Aware RE mayintroduce more noise, for it may mistakenly increase the probability of a relation with the similartopic with the context relations. We will give samples to illustrate this issue in Sect. 5.5. Another
Table 3: Results on distantly labeled test set5.3	Effectiveness of Reasoning MechanismFrom Table 2 and 3, we can see that our best models outperform all the baseline models significantlyon all three test sets. These results indicate our model could successfully conduct reasoning on thefully-connected graph with generated parameters from natural language. These results also indicatethat our model not only performs well on sentence-level relation extraction but also improves onbag-level relation extraction. Note that Context-Aware RE also incorporates context informationto predict the relation of the target entity pair, however, we argue that Context-Aware RE onlymodels the co-occurrence of various relations, ignoring whether the context relation participatesin the reasoning process of relation extraction of the target entity pair. Context-Aware RE mayintroduce more noise, for it may mistakenly increase the probability of a relation with the similartopic with the context relations. We will give samples to illustrate this issue in Sect. 5.5. Anotherinteresting observation is that our #layers=1 version outperforms CNN and PCNN in these threedatasets. One probable reason is that sentences from Wikipedia corpus are always complex, which7Under review as a conference paper at ICLR 2019may be hard to model for CNN and PCNN. Similar conclusions are also reached by Zhang & Wang(2015).
Table 4: Sample predictions from the baseline models and our GP-GNN model. Ground truth graphsare the subgraph in Wikidata knowledge graph induced by the sets of entities in the sentences. Themodels take sentences and entity markers as input and produce a graph containing entities (coloredand bold) and relations between them. Although “No Relation” is also be seen as a type of relation,we only show other relation types in the graphs.
