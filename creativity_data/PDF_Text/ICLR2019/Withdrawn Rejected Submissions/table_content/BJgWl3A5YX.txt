Table 1: Node parameters	x rotation	y rotation	z rotation	Scale	Parent position	Parent rotation	ReflectionFront hip	π∕2	0	π∕2	1.0	0.5	3π∕4	TrueRear hip	π∕2	0	π∕2	1.0	0.5	π∕4	TrueKnee	π∕4	0	0	1.0	1.0	-π∕2	FalseTable 2: Edge parameters6	Appendix: Formal Definition of Multi-Agent RL and FullEnvironment S pecificationOur analysis is based on independent multi-agent RL, in a physics simulator environment. At ahigh level, we use multiple independent learners in a partially-observable Markov game, often calledpartially-observable stochastic games (Hansen et al., 2004). In every state, agents take actions givenpartial observations of the true world state, and each agent obtains an individual reward. Agents re-ceive a shaping reward to encourage them to approach their opponent, +100 if they win and -100if they lose. Through their individual experiences interacting with one another in the environment,agents learn an appropriate behavior policy. More formally, consider an N -player partially observ-able Markov game M (Shapley, 1953a; Littman, 1994) defined on a finite state set S. An observa-tion function O : S × {1, . . . , N} → Rd gives each agent’s d-dimensional restricted view of thetrue state space. On any state, the agents may apply an action from A1, . . . , AN (one per agent).
Table 2: Edge parameters6	Appendix: Formal Definition of Multi-Agent RL and FullEnvironment S pecificationOur analysis is based on independent multi-agent RL, in a physics simulator environment. At ahigh level, we use multiple independent learners in a partially-observable Markov game, often calledpartially-observable stochastic games (Hansen et al., 2004). In every state, agents take actions givenpartial observations of the true world state, and each agent obtains an individual reward. Agents re-ceive a shaping reward to encourage them to approach their opponent, +100 if they win and -100if they lose. Through their individual experiences interacting with one another in the environment,agents learn an appropriate behavior policy. More formally, consider an N -player partially observ-able Markov game M (Shapley, 1953a; Littman, 1994) defined on a finite state set S. An observa-tion function O : S × {1, . . . , N} → Rd gives each agent’s d-dimensional restricted view of thetrue state space. On any state, the agents may apply an action from A1, . . . , AN (one per agent).
Table 3: Shapley computation for hypothetical exampleSection 3.2 analyzes the contribution of individual body changes using the Shapley value. Itis basedon computing the Shapley value similarly to the computation in Table 3 for the simple hypotheticalexample. Such a direct computation simply averages the marginal contribution of each componentacross all permutations (as given in the formula for the Shapley value in Section 1). Althoughthis direct computation is straighforward, it is intractable in for two reasons. First, our definitionof the cooperative game in Section 3.2 uses the probability of a certain agent (with a hybrid bodyand the evolved controller) beating another agent (with the original body and a baseline controller).
