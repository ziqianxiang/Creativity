Table 1: Validation accuracy results on classification tasks. GBN stands for Ghost-BN. Fisher Trace+GBNstands for LB + Ghost-BN with isotropic Gaussian noise scaled by square-root of trace of Fisher. Diag-F+GBNstands for LB + Ghost-BN with diagonal Fisher noise. All methods in each row are trained with the samenumber of epochs. Confidence interval is computed over 3 random seeds. LB with full Fisher requires samenumber of updates as SB, which renders it impractical for most of the models for which we work with. Whilethis is indeed infeasible to compute for all the models, we note that LB with full Fisher reaches roughly thesame validation accuracy (93.22) as SB in the case of ResNet44 (CIFAR-10).
Table 2: Validation accuracy results on classification tasks using BatchChange, Multiplicative, K-FAC. Forreader’s convenience, we report again the result of Diag-F+GBNDataset	Network	SB I BatchChange		Multiplicative ∣ K-FAC		Diag-F+GBNCIFAR-10	VGG16	93.25	93.18	90.98	93.06	93.15 ± 0.05CIFAR-100	-VGG16-	72.83	72:44	68.77	71.86	71.94 ± 0.14CIFAR-10	ResNet44	93.42	93：02	91.28	92.81	92.72 ± 0.15CIFAR-100	ResNet44x2	75.55	75:16	71.98	73.84	74.10 ± 0.18We give the training plots over epochs, the experimental results with the square root learning scalingscheme in the following table and plots,16Under review as a conference paper at ICLR 2019Table 3: Validation accuracy results on classification tasks where Diag F+GBN uses square-root scaling learn-ing rate.
Table 3: Validation accuracy results on classification tasks where Diag F+GBN uses square-root scaling learn-ing rate.
