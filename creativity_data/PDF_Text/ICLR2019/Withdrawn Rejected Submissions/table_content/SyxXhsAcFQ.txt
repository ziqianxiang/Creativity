Table 1: MNIST - Accuracies - rotated, unrotated combinations	Samples used to learn W	R/R	R / NR	NR / NR	NR/RPlanar 1	-	23.00	-	98.00	1100Spherical CNN 1	-	95.00	-	96.00	94.00FFS2CNN 2	-	95.80	-	96.00	95.86Ours	12000	96.73 (0.35)	96.90 (0.23)	97.68 (0.08)	98.48 (0.06)28 /14 Tensor	12000	96.17 (0.35)	96.51 (0.27)	97.10 (0.70)	97.51 (0.09)28 /14 Scale	12000	94.79 (0.59)	95.56 (0.55)	93.86 (0.87)	91.66(1.36)Ours	500	96.40 (0.09)	96.64 (0.06)	97.41 (0.09)	98.24 (0.05)28 /14 Tensor	500	95.78 (0.12)	95.98 (0.09)	96.53 (0.12)	97.03 (0.07)28 /14 Scale	500	94.37 (0.39)	95.34 (0.23)	92.67 (1.02)	89.62(1.51)1 Values as reported in Table 1 of Cohen et al. (2018)2 Values as reported in Kondor et al. (2018c)3.	Results on Classification, Fashion-MNIST. In Table 2 we report mean classification accu-racy(and stdev) on the Fashion-MNIST data set(Xiao et al. (2017)). We rotate each data pointaround the origin by an angle chosen uniformly between 0 and 2Ï€ to create F-MNIST-rot. TheCW basis was learned in the AE architecture with Fashion-MNIST as input. The classifier is afour layer network (96K parameters). We compared with a Depth 5 CNN (102K parameters).
Table 2: Fashion MNIST - Accuracies - rotated, unrotated combinations	Samples used to learn W	R/R	R/NR	NR/NR	NR/RCNN Ours	- 20000	80.86 (0.57)^^79.83 (0.66)^^90.68 (0.31)^^20.86 (0.46) 86.34(0.18)	84.67(0.27)	86.70(0.29) 85.42(0.18)3.4	ImplementationOur autoencoders and classifiers were implemented as neural nets in Python (TensorFlow). We usedthe Adam optimizer.
