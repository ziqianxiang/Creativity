Table 1: Performance comparison on IWSLT2014 German-English dataset.
Table 2: Performance comparison on IWSLT2014 English-to-German dataset.			Table 3: Performance comparison on IWSLT2015 English-to-Vietnamese dataset.					Method	BLEU Greedy Beam	Method	BLEU					Greedy	Beam						Hard monotonic (Raffel et al 2017)	23.0	N/A			ar monoonc a e e a.,		NPMT (Huang et al., 2018)	23.62	25.08	Luong & Manning (2015)	N/A	23.3NPMT+LM (Huang et al., 2018)	N/A	25.36	NPMT (Huang et al., 2018)	26.91	27.69Seq2Seq (Bahdanau et al., 2014)	21.26	22.59	NPMT+LM (Huang et al., 2018)	N/A	28.07Graph2Seq (Gildea et al., 2018)	20.32	22.39	Seq2Seq (Bahdanau et al., 2014)	25.50	26.10—			Graph2Seq (Gildea et al., 2018)	22.70	24.73GraphSeq2Seq (Ours)	26.02	27.32	—		GraphSeq2Seq-Variant (Ours)	25.78	27.00	GraphSeq2Seq (Ours)	28.44	29.25—			GraphSeq2Seq-Variant (Ours)	28.48	29.624.2	IWSLT 2014 ENGLISH-TO-GERMANImplementation For the IWSLT 2014 English-to-German machine translation benchmark, follow-ing the setup presented in Section 4.1, the same dataset is used but with the opposition direction.
Table 4: Performance comparison on WMT 2016 English-Czech dataset.
Table 5: Quantitative analysis of our GraphSeq2Seq on BLEU scores with random graph and se-quence noises based on IWSLT2014 German-English dataset. The random noises change from 0%to 75%. Note that 75% indicates that 75% of graph and sequence information are noises.
Table 6: The impact of highway layers on the performance (BLEU) of GraphSeq2Seq onIWSLT2014 German-English dataset. GraphSeq2Seq is training with different numbers of high-way layers.
Table 7: The impact of the weight for graph encoding on the performance (BLEU) of GraphSeq2Seq.
