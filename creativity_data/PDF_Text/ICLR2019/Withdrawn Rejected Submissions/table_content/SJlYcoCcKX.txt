Table 1: We validate different state-of-the-art knowledge transfer methods applied on MNIST. Top-1 indicate the original validation set. Top-5-C is the result of top 5 accuracy on our Customizedvalidation set.
Table 2: VGG-19 to AlexNet, run time in second pre-epoch7Under review as a conference paper at ICLR 2019KD Method	Kernel Size	Kernel Run Time	UtilizationAT	167MB	^T27s	0.82FitNet	455MB	644s	0.34NMT	104MB	269s	0.89Table 3: Kernel Size Comparison for different knowledge transfer methodshow the different transfer method performance on CIFAR-100. When class increases, from 10 to100, our NMT show its preponderance. NMT can reach small error in first 20 epochs and improvethe student performance into 10%. If we keep training to 200 epoch, NMT can have best result.
Table 3: Kernel Size Comparison for different knowledge transfer methodshow the different transfer method performance on CIFAR-100. When class increases, from 10 to100, our NMT show its preponderance. NMT can reach small error in first 20 epochs and improvethe student performance into 10%. If we keep training to 200 epoch, NMT can have best result.
