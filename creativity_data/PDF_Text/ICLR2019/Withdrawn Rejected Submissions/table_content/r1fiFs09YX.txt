Table 1: Rewards for policies after meta-training (MOA, MO and MA) against 100 new opponentsMethods	Chasing game	Blocking game	Recommending gameMOA	8.70	6.00	9.25MO	0.00	-9.6	0.00MA	4.70	-10.00	-0.08NM	0.90	-9.00	2.724 purple objects to player 2. When player 1 reaches one of the objects or the game is played 16steps, the game ends. Player 1 only gets rewards when it reaches an object. Assume that the verticalcoordinate for the goal of player 2 is y2 and that of the recommended object is y1 . Then the rewardfor player 1 is a sample from GaUssian distribution N(μ, 1), where μ = 10 - 3/2 * ∣y1 - y21.
