Table 1: Pearson’s r× 100 on STSBnormalized sentence vectors. Since our model is non-parameterized, it does not utilize any informa-tion from the dev set when evaluating on the test set and vice versa. Hyper-parameters are chosen atm = 7, h = 17, K = 45, and t = 3 by conducing hyper-parameters search on dev set. Results onthe dev and test set are reported in Table 1. As shown, on the test set, our model has a 5.5% higherscore compared with another non-parameterized model SIF, and 25.5% higher than the baseline ofaveraging L.F.P word vectors. It also outperforms most parameterized models including GRAN,InferSent, and Sent2Vec. Of all evaluated models, our model only ranks second to Reddit + SNLI,which is trained on the Reddit conversations dataset (600 million sentence pairs) and SNLI (570ksentence pairs). In comparison, our proposed method requires no external data and no training.
Table 3: Results on supervised tasks. Sentence embeddings are fixed for downstream supervisedtasks. Best results for each task are underlined, best results from models in the same category are inbold. SIF results are extracted from Arora et al. (2017) and Ruckle et al. (2018), and some trainingtime is collected from Logeswaran & Lee (2018).
Table 4: Comparison of different configurations demonstrates the effectiveness of our model onSTSB dev set and SUBJ. SDR stands for sentence-dependent principal component removal in Sec-tion 2.4.2. SIR stands for sentence-independent principal component removal, i.e. directly removingtop h corpus principal components from the sentence embedding.
Table 5: Running time of GEM, InferSent and SkipThought on encoding sentences in STSB test set.
