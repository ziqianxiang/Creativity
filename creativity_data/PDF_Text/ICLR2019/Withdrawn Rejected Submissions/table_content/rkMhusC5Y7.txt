Table 1: MAP scores on the test sets of the document retrieval datasets. Similar results hold for othermetrics (see Appendix A). ?The weights of the agents are initialized from a single model pretrainedfor ten days on the full training set.
Table 2: Main result on the question-answering task (SearchQA dataset). We did not include thetraining cost of the aggregator (0.2 days, 0.06 Ã—1018 FLOPs).
Table 3: Diversity scores of reformulations from different methods. For pBLEU and pCos, lowervalues mean higher diversity. Notice that higher diversity scores are associated with higher F1 andoracle scores.
Table 4: Results on more metrics on the test set of the TREC-CAR dataset.
Table 5: Results on more metrics on the test set of the Jeopardy dataset.
Table 6: Results on more metrics on the test set of the MSA dataset.
Table 7: Multiple reformulators vs. aggregator contribution. Numbers are MAP scores on the devset. Using a single reformulator with the aggregator (RL-RNN Greedy/Sampled/Beam + Aggrega-tor) improves performance by a small margin over the single reformulator without the aggregator(RL-RNN). Using ten reformulators with the aggregator (RL-10-Sub) leads to better performance,thus indicating that the pool of diverse reformulators is responsible for most of the gains of theproposed method.
Table 8: Comparison of different aggregator functions on TREC-CAR. The reformulators are fromRL-10-Sub.
Table 9: Partitioning strategies and the corresponding evaluation metrics. We notice that the randomstrategy generally results in the best quality sub-agents, leading to the best scores on both of thetasks.
Table 10: Examples for the qualitative analysis on SearchQA. In bold are the reformulations andanswers that had the highest scores predicted by the aggregator. We only show the top-5 reformu-lations of each method. For a detailed analysis of the language learned by the reformulator agents,see Buck et al. (2018a).
