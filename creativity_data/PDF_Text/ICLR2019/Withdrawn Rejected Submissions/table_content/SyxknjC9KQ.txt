Table 1: Training accuracy achieved on the circle dataset by different networksMethods	Hidden nodes	Parameters	Training accuracyNN-ReLU	2	12	68.87NN-tanh	2	12	69.10Maxout Network (h=2)	2	18	87.17DenMo-Net	2	12	91.64.1	Visualization with a toy datasetFor visualizing the decision boundaries learned by the classifiers, we have generated data on twoconcentric circles belonging to two different classes with center at the origin. We compare theresults when only two neurons are taken in the hidden layer in all the networks. It is observed thatclassical neural network fails to classify this data with two hidden neurons as it learns one hyperplaneper one hidden neuron. The boundaries learned by the network with ReLU activation function (NN-ReLU) is shown in figure 3a. The result of maxout network is better (87.17% training accuracy)as it introduces extra parameters with max function to achieve non-linearity. In the maxout layerwe have taken maximum among h = 2 features. As we see in the figure 3b the network learns(2 * h =)4 straight lines when trying to classify these data. For the same data and two neurons indilation-erosion layer, our DenMo-Net has learned 6 lines to form the decision boundary (figure 3c).
Table 2: Accuracy on MNIST dataset with different architecturesNeurons in dilation-erosion layer (l)Test Accuracy10	50	100	20076.35	93.38	95.51	96.854.3	Fashion-MNIST DatasetThe Fashion-MNIST dataset (Xiao et al., 2017) has been proposed with the aim of replacing thepopular MNIST dataset. Similar to the MNIST dataset this also contains 28 × 28 images of 10classes and 60,000 training and 10,000 testing samples. While MNIST is still a popular choicefor benchmarking classifiers, the authors’ claim that MNIST is too easy and does not represent themodern computer vision tasks. This dataset aims to provide the accessibility of the MNIST datasetwhile posing a more challenging classification task.
Table 3: Achieved accuracy in the test setDataset	Test accuracy		DenMo-Net	State of the artMNIST	98.43 (l = 400)	99.79 (Wan et al., 2013)Fashion-MNIST	89.87 (l = 800)	89.70 (Xiao et al., 2017)7Under review as a conference paper at ICLR 2019nodes)Figure 5: Test accuracy over epochs on CIFAR-10 datasetTable 4: Test accuracy achieved on CIFAR-10 dataset by different networksArchitecture		l=200				l=400				l=600			parameters	accuracy	parameters accuracy		parameters a	ccuracyNN-tanh	616,610	^^48.88	1,233,210	49.39	1,849,810	51.24NN-ReLU	616,610	49.28	1,233,210	50.43	1,849,810	52.25Maxout-Network	1,231,210	49.51	2,462,410	50.10	3,693,610	51.51DenMo-Net	616,610	51.84	1,233,210	53.41	1,849,810	54.49happens because our network is able to learn more hyperplanes with number of parameters similarto normal artificial neural networks. When using only a single type of neurons in our network, wesee a different result for this dataset (Figure 5b). The network takes time to learn with only erosionneurons. The situation improves a little when using only dilation neurons. When using both type of
Table 4: Test accuracy achieved on CIFAR-10 dataset by different networksArchitecture		l=200				l=400				l=600			parameters	accuracy	parameters accuracy		parameters a	ccuracyNN-tanh	616,610	^^48.88	1,233,210	49.39	1,849,810	51.24NN-ReLU	616,610	49.28	1,233,210	50.43	1,849,810	52.25Maxout-Network	1,231,210	49.51	2,462,410	50.10	3,693,610	51.51DenMo-Net	616,610	51.84	1,233,210	53.41	1,849,810	54.49happens because our network is able to learn more hyperplanes with number of parameters similarto normal artificial neural networks. When using only a single type of neurons in our network, wesee a different result for this dataset (Figure 5b). The network takes time to learn with only erosionneurons. The situation improves a little when using only dilation neurons. When using both type ofmorphological neurons, the network is able to perform better by leveraging the power of both theoperations.
Table 5: Comparison with Baseline CIFAR100Architecture	l=200		l=400			l=600			parameters accuracy		parameters accuracy		parameters a	ccuracyNN-tanh	634,700	19.50	1,269,300	19.62	1,903,900	20.46NN-ReLU	634,700	17.83	1,269,300	19.63	1,903,900	20.77Maxout-Network	1,249,300	21.58	2,498,500	21.49	3,747,700	21.69DenMo-Net	634,700	23.65	1,269,300	25.89	1,903,900	26.93Therefore, only a single element of the structuring element is updated. So, the learning can be slow.
Table 6: Test accuracy achieved on CIFAR-10 and CIFAR-100 dataset by different networksNetworkNN-tanh (400D-200D-100D)NN-ReLU (400D-200D-100D)DenMo-Net (400DE-100FC)DenMo-Net (400DE-200DE-100FC)DenMo-Net (400DE-200FC-100DE-100FC)AccuracyCIFAR-10 CIFAR-10051.652.7953.4151.239.224.3724.3725.8923.3116.32Figure 7: Train and Test accuracy achieved over epochs in the CIFAR-10 dataset
