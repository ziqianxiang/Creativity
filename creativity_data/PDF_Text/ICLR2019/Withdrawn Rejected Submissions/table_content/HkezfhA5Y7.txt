Table 1: Test accuracy (Test (%)) and generalization gap (Gap (%)) for A: the baseline, B: A +weight decay (WD), and C: B + batch normalization (BN). There is no large-batch generalizationgap (LBGG) for B; however, a small LBGG appears when we introduce BN in C. Not shown: Theloss gap is one order of magnitude less for B vs. A. Training accuracy is omitted for brevity. ModelA achieves 100±0% training accuracy. We report the mean accuracy and standard error of the mean,assuming the error is normally distributed and independent over 5 seeds.
Table 2: Models A, B, and C from Table 1, but trained instead with random labels sampled uni-formly. Clearly, model B has the least information capacity, yet also happens to achieve the highesttest accuracy for the original labels in Table 1. Notice the 25±1% LBGG for C, and the 54±2% GGbetween C and B for batch size 128. There is a 3 ± 4% LBGG in A.
Table 3: Basic fully-convolutional CNN architecture. We respectively denote h, w, cin , cout , s asconvolution kernel height, width, number of input and output channels w.r.t. each layer, and stride.
