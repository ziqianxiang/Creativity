Table 1: MNIST Accuracy (in %) of ConvNet1 target model for different scenarios. = 0.1/0.2/0.3 forSGD, ExL, SGDens, ExLens; = 0.3/0.4 for SGDPGD, ExLPGD. For PGD attack, we report accuracyfor 40-/100-step attacks. Note, SGDPGD, ExLPGD have stronger BB attacks than remaining scenarios4.
Table 2: CIFAR10/ CIFAR100 Accuracy (in %) of ResNet18/ ResNext-29 target model for different sce-narios. e = ɪ/τ16/:32 for ExL, SGD, ExLens, SGDens, ExLpgd, SGDpgd. For PGD attack, we255 255 255report accuracy for 7-/20-step attacks. Note, SGDpGD , ExLpGD have stronger BB attacks than remainingscenarios4. Accuracy < 5%, in most places, have been omitted and marked as ‘-’.
Table A1: Hyperparameter Table for training ResNet18 models on CIFAR10 dataModel Type	Training Method	EpoChs η∕ηadv		η, ηadv deCay/step- size	ηnoise ∕ηnoise adv	ηnoise, ηnoise adv deCay/step- A size	Test CuraCy in (%)	SGD	120	0.1/-	0.1/30	-	-	88.8	ExL	120	0.1/-	0.1/30	0.001/-	0.1/30	87.1	SGDens	80	0.1/0.05	0.1/30	-	-	86.3Target	ExLens	120	0.1/0.05	0.1/30	0.001/0.0005	0.1/30	86.4	SGDPGD	122	0.1/0.1	0.1/20	-	-	83.2	ExLPGD	122	0.1/0.1	0.1/20	0.001/0.0005	0.1/20	83	SGD	300	0.1/-	0.1/100	-	-	89SourCe	P GDAdv	122	0.1/0.1	0.1/20	-	-	83EnsAdv	SGD	31	0.1/-	0.1/30	-	-	81Table A2: Hyperparameter Table for training ResNext29 models on CIFAR100 dataModel Type	Training Method	EpoChs η∕ηadv		η, ηadv deCay/step- size	ηnoise ∕ηnoise adv	ηnoise, ηnoise adv deCay/step- A size	Test CCuraCy in (%)	SGD	100	0.1/-	0.1/40	-	-	71	ExL	58	0.1/-	0.1/20	0.001/-	0.1/20	69.4	SGDens	42	0.1/0.05	0.1/20	-	-	69.8Target	E xLens	48	0.1/0.05	0.1/20	0.001/0.0005	0.1/20	67.3	SGDPGD	52	0.1/0.05	0.1/20	-	-	71.6	ExLPGD	52	0.1/0.05	0.1/20	0.001/0.0005	0.1/20	69	SGD	34	0.1/-	0.1/10	-	-	67.2
Table A2: Hyperparameter Table for training ResNext29 models on CIFAR100 dataModel Type	Training Method	EpoChs η∕ηadv		η, ηadv deCay/step- size	ηnoise ∕ηnoise adv	ηnoise, ηnoise adv deCay/step- A size	Test CCuraCy in (%)	SGD	100	0.1/-	0.1/40	-	-	71	ExL	58	0.1/-	0.1/20	0.001/-	0.1/20	69.4	SGDens	42	0.1/0.05	0.1/20	-	-	69.8Target	E xLens	48	0.1/0.05	0.1/20	0.001/0.0005	0.1/20	67.3	SGDPGD	52	0.1/0.05	0.1/20	-	-	71.6	ExLPGD	52	0.1/0.05	0.1/20	0.001/0.0005	0.1/20	69	SGD	34	0.1/-	0.1/10	-	-	67.2SourCe	P GDAdv	48	0.1/0.05	0.1/20	-	-	68.4EnsAdv	SGD	45	0.1/-	0.1/20	-	-	71.3For MNIST, we use 2 different arChiteCtures as sourCe/ target models. ConvNet1: 32C-M-64C-M-1024FC is the model used as target. ConvNet2: 10C-M-20C-M-320FC is the model used assourCe. Here, we use mini-batCh SGD with momentum of 0.5, batCh size 64, for training the weightparameters. Table A3 shows the hyperparameters used to train the models in Table 1 of main paper.
Table A3: Hyperparameter Table for training ConvNet1/ConvNet2 models on MNIST dataModel Type	Training Method	Epochs	η∕ηadv	η, ηadv decay/step- size	ηnoise ∕ηnoise adv	ηnoise , ηnoise adv decay/step- A size	Test ccuracy in (%)	SGD	100	0.01/-	0.1/50	-	-	99.1Target	ExL	150	0.01/-	0.1/50	0.001/-	0.1/50	99.2ConvNet1	SGDens	64	0.01/0.005	0.1/30	-	-	99	ExLens	32	0.01/0.005	0.1/30	0.001/3.3e-5	0.1/30	99.1	SGDPGD	142	0.01/0.01	0.1/30	-	-	97.9	ExLPGD	162	0.01/0.01	0.1/30	1e-4/1e-5	0.1/30	98Source	SGD	15	0.01/-	-/-	-	-	98.6(ConvNet2)	P GDAdv	128	0.01/0.01	0.1/30	-	-	97EnsAdv ConvNet1	SGD	15	0.01/-	-/-	-	-	98.8Table A4: Hyperparameter Table for training ResNet18 models on CIFAR10 data for different types ofnoise modeling (X + N,X × N) with all/ only negative gradient NLNNoise Modeling Type	Gradient ▽LN	Epochs	η	η decay/step- size	ηnoise	ηnoise decay/step- size	Test Accuracy in (%)X+N	Negative	120	0.1	0.1/30	0.001	0.1/30	78.1X+N	All	120	0.1	0.1/30	0.001	0.1/30	77.1X×N	Negative	120	0.1	0.1/30	0.001	0.1/30	87.1X×N	All	120	0.1	0.1/30	0.001	0.1/30	85.1SGD	-	120	0.1	0.1/30	-	-	88.9D Appendix D: Implicit generative modeling of noise acquires
Table A4: Hyperparameter Table for training ResNet18 models on CIFAR10 data for different types ofnoise modeling (X + N,X × N) with all/ only negative gradient NLNNoise Modeling Type	Gradient ▽LN	Epochs	η	η decay/step- size	ηnoise	ηnoise decay/step- size	Test Accuracy in (%)X+N	Negative	120	0.1	0.1/30	0.001	0.1/30	78.1X+N	All	120	0.1	0.1/30	0.001	0.1/30	77.1X×N	Negative	120	0.1	0.1/30	0.001	0.1/30	87.1X×N	All	120	0.1	0.1/30	0.001	0.1/30	85.1SGD	-	120	0.1	0.1/30	-	-	88.9D Appendix D: Implicit generative modeling of noise acquiresADVERSARIAL KNOWLEDGEIntuitively, we can justify adversarial robustness inherited with noise modeling in two ways: First, byintegrating noise during training, we allow a model to explore multiple directions within the vicinityof the data point (thereby incorporating more off-manifold data) and hence inculcate that knowledgein its underlying behavior. Second, we note that noise learnt with ExL inherits the input data charac-teriStics (i.e. N ⊂ X) and that the noise-modeling direction (▽ N L) is aligned with the loss gradient,▽xL (that is also used to calculate the adversarial inputs, Xadv = X + esignEXL)). This ensuresthat the exploration direction coincides with certain adversarial directions improving the model’sgeneralization capability in such spaces. Note, for fully guaranteed adversarial robustness as perEqn. 1 in main paper, the joint input/output distribution (p(X |Y ), p(Y )) has to be realized in addi-tion to the noise modeling and N should span the entire space of adversarial/off-manifold data.
