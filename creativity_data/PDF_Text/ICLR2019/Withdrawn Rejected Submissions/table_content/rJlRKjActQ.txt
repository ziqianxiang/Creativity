Table 1: Supervised Classification Results on CIFAR-10 (a) and CIFAR-100 (b). We note significantimprovement with Manifold Mixup especially in terms of Negative log-likelihood (NLL). Pleaserefer to Appendix C for details on the implementation of Manifold Mixup and Manifold Mixup Alllayers and results on SVHN. f and 去 refer to the results reported in (Zhang et al., 2018) and (GUoet al., 2016) respectively.
Table 3: CIFAR-10 Test Accuracy Results on white-box FGSM (Goodfellow et al., 2015) adversarialattack (higher is better) using PreActResNet18 (left). SVHN Test Accuracy Results on white-boxFGSM using WideResNet20-10 (Zagoruyko & Komodakis, 2016). Note that our method achievessome degree of adversarial robustness, against the FGSM attack, despite not requiring any additional(significant) computation. f refers to results reported in (Madry et al., 2018)CIFAR-10 Models	FGSM		Adv. Training (PGD 7-step) f	56.10	SVHN Models	FGSMAdversarial Training			+ Fortified Networks	81.80	Baseline	21.49Baseline (Vanilla Training)	36.32	Input Mixup 1 r⅛∖	c/z noInput Mixup (α = 1.0)	71.51	(α = 1.0) Manifold Mixup	56.98Manifold Mixup (α = 2.0)	77.50	(α = 2.0)	65.91CIFAR-100 Models	FGSM	Adv. Training			(PGD 7-step)	rc on			72.80Input Mixup (α = 1.0)	40.7	—	Manifold Mixup (α = 2.0)	44.96		tive study of this, we trained a small decoder convnet (with upsampling layers) to predict an imagefrom the Manifold Mixup classifier’s hidden representation (using a simple squared error loss in thevisible space). We then performed mixup on the hidden states between two random examples, and
Table 4: Results on synthetic data generalization task with an idealized Manifold Mixup (mixing inthe true latent generative factors space). Note that in all cases visible mixup significantly improvedlikelihood, but not to the same degree as factor mixup.
Table 5: Models trained on the normal CIFAR-100 and evaluated on a test set with novel deforma-tions. Manifold Mixup (ours) consistently allows the model to be more robust to random shearing,rescaling, and rotation even though these deformations were not observed during training. For therotation experiment, each image is rotated with an angle uniformly sampled from the given range.
Table 6: Results on SVHN dataset with PreActResNet18 architectureModel		Test Error ( in %)PreActResNet18		No Mixup		2.22Input Mixup (α =	0.01)	2.30Input Mixup (α =	0.05)	2.28Input Mixup (α =	0.2)	2.29Input Mixup (α =	0.5)	2.26Input Mixup (α =	1.0)	2.37Input Mixup (α =	1.5)	2.41Manifold Mixup (α = 1.5)		1.92Manifold Mixup (α = 2.0)		1.9017Under review as a conference paper at ICLR 20190.24-0.22 -0.20-0.18 -0.16 -0.14 -
Table 7: Test accuracy (in %) of Manifold Mixup for a range of values for hyperparameter α forCifar10 dataset on PreActReseNet18α	Input Mixup	Manifold Mixup0.5	95.75	96.121.0	95.84	96.101.2	96.09	96.291.5	96.06	96.351.8	95.97	96.452.0	95.83	96.73Manifold Mixup outperformed Input Mixup for all alphas in the set (0.5, 1.0, 1.2, 1.5, 1.8, 2.0) - indeed thelowest result for Manifold Mixup is better than the worst result with Input Mixup. Note that Input Mixup’sresults deteriorate when using an alpha that is too large, which is not seen with manifold mixup.
Table 8: Test accuracy (in %) of Manifold Mixup when the mixing is performed in different subsetsof layers, for PreActReseNet18 on Cifar10 dataset.
