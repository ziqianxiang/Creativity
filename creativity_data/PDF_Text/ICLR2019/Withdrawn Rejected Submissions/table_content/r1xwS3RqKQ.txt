Table 1: Comparison of performance on MNIST by activation. Compactness DifEN performance ison par with significantly larger fixed activation networksActivation	ReLU	LReLU	SeLU	SWish	DifENAccuracy	0.9891	0.9890	0.9903	0.9918	0.9919equipped with fixed activations to that with DifEN neurons. Again, we use 3-fold cross validation andreport the average performance. The following experiments were conducted using the open sourcediabetes regression data.
Table 2: Comparison of performance on diabetes regression dataset by network size and activationsize	DifEN	ReLU	LReLU	SeLU	Swish1	2490.781	7391.783	6977.1	6289.9	4298.4932	2446.003	3759.527	4562.308	3793.336	3249.6084	2412.504	2931.891	2720.555	2912.025	2839.3238	2313.98	2465.16	2398.2	2488.361	2664.85416	2117.47	2334.454	2357.557	2165.465	2236.137We ran the test in Figure 3, numerous times with different initializations, each time the differentialtransformed itself to have a sine-like solution. This supports that DifEN activation functions transformduring the training process. We also demonstrated the capability of DifENs to learning complexconcepts, and with a significantly reduced network size. Table 2 shows a DifEN can perform on par,or better than a network with over 2x the number of parameters when compared to a fixed activationnetwork. Moreover, DifENs can learn a better approximation when compared to a network withfixed activations throughout. The ability of DifENs to achieve top-notch performance with a compactrepresentation makes them a good candidate for on-device applications. Moreover, DifENs seem wellsuited to applications that require the capabilities of a big neural network, but are currently limited bymemory, or where latency issues are a factor, such as space applications and robotics.
