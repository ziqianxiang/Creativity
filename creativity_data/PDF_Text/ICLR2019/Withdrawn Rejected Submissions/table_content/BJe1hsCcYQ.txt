Table 1: Evaluation of Taxonomy embeddings. MR: Mean Rank (lower is better). MAP: MeanAverage Precision (higher is better). ρ: Spearman’s rank-order correlation (higher is better).
Table 2: Percentage of node representations that have smaller Euclidean norm than their children inthe tree when λ = 0Method	Wordnet Nouns		Wordnet Verbs	EuroVoc	ACM	MeSHOurs (β =	0.01)	97.2%	96.3%	98.6%	98.3%	97.7%Ours (β =	0.1)	97.9%	96.0%	98.4%	98.0%	97.5%Ours (β =	1)	94.3%	92.0%	98.3%	94.3%	93.2%We learn the representations {fi ∈ Fd}in=1 that minimize the following problem:Lneighborhood + λ	X	max(kfuk2 - kfvk2,0)	(14){(u,v):rank(u)<rank(v)}where λ ≥ 0 is a regularization parameter. The first term defined in Eq. (12) tries to get similarexamples close to each other. The second term tries to satisfy the order of the embedding norms tomatch the normalized ranks. Using Theorem 3.1, we optimize Euclidean norms in Fd.
Table 3: Test F1 classification scores for four different subtrees of WordNet noun tree.
Table 4: Examples of clusters extracted from the hierarchical complete-linkage clustering algorithmComputer vision (California)	Malik J, Belongie S, Fowlkes C, Martin D, Torresani L, Hertzmann A, Bregler C, Omohundro S, Stolcke A, Allinson N, Moon KMachine learning (MIT & California)	Jordan M, Tenenbaum J, Russell S, El Ghaoui L, Ng A, Blei D Bhattacharyya C, Nilim A, Lanckriet G, Xing EKernel methods (cluster 1)	Scholkopf B, Smola A, Weston J, Bousquet O, Chapelle O, Gretton AKernel methods (cluster 2)	Shawe-Taylor J, Cristianini N, Platt J, Campbell CWhen learning 10-dimensional representations, the MR is 1.0 and the MAP is 100%. When λ = 0,the embeddings with smallest Euclidean norm are in that order: Sejnowski T. (52), Jordan M. (45),Tishby N. (29), Dayan P. (25), Koch C. (46), Singh S. (27), Hinton G. (32), Scholkopf B. (36), MozerM. (21), Bialek W. (27), Singer Y. (19), Vapnik V. (23), Bengio Y. (33), Shawe-Taylor J. (22), ZemelR. (13). The authors with a large number of co-authors then tend to have smallest Euclidean norms.
