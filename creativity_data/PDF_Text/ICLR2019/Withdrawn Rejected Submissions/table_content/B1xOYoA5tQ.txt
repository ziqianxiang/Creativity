Table 1: This table presents the effect of increasing the dimension (10, 20, ..., 3000) of the outputencoding layer on the classification accuracy (%) of a model that uses RO multi-way encoding forthe MNIST dataset on (1) data perturbed using an FGSM black-box attack with = 0.2 by a modelthat uses 1ofK encoding, and (2) clean data. As the dimension increases, accuracy increases up toa certain point; We use 2000 for the length of our multi-way encoding layer.
Table 2: This table presents the classification accuracy (%) of MNIST on black-box and white-boxFGSM attacks of strength = 0.2 using architectures A and C. Every cell in this table generatesattacks from a substitute model f(x) for a target model g(x). We conclude: a) g(x) is more vulnera-ble to attacks when f(x) uses the same encoding, hence the lower reported accuracy. b) Even whenthe source and target models are the same and use the same encoding (*), i.e. white-box attacks,RO encoding leads to better accuracy compared to 1ofK. c) In brackets is the Pearson correlationcoefficient of the gradients of g(x) and f(x) with respect to the input x. Gradients are less correlatedwhen the source and target models use different encodings. In addition, if the same encoding is usedin the source and target models, RO results in a lower correlation compared to 1ofK.
Table 3: RO (target model) consistently results in a significantly higher classification accuracy foruntargeted attacks, and a significantly lower attack success rate compared to 1ofK for all fourbenchmark datasets. The numbers reported in this table are the average classification and attacksuccess rate over all classes of each dataset. We note that the clean accuracy for MNIST, CIFAR-10,CIFAR-100, and SVHN is, 99.1, 94.3, 74.5, 96.2, respectively (Â±0.1 for RO or 1ofK).
Table 4: Comparison against state-of-the-art defense approaches on white-box and black-box PGDattacks, and on clean data. We observe that our approach is more resilient to both types of attacks,while simultaneously improving accuracy on clean data. (*) indicates our replication of Kannanet al. (2018) using the experimental setting of Madry et al. (2017) on MNIST, also used by Ours,that uses only 90% of the training set.
Table 5: Our attack is capable of fooling the watermarking detection algorithm. Fine-tuning a stolenmodel using RO encoding remarkably reduces the watermarking detection accuracy, and makes itcomparable to the accuracy of models trained from scratch and do not use the stolen model. Theaccuracy of fine-tuned models benefits significantly from the pre-trained weights of the stolen model.
Table 6: This table presents black-box attacks from the substitute model A1ofK on various tar-get models. RO achieves the highest accuracy and the lowest input gradient correlation with thesubstitute model among the different target models.
Table 7: This table presents black-box attacks from the substitute model C1ofK on various targetmodels. RO achieves the highest accuracy and the lowest input gradient correlation with the substi-tute model among the different target models.
Table 8: Correlation between A1ofK and A01ofKLayer	Input	Conv1	Conv2Correlation Coefficient	0.1	0.008	0.13Table 9: Correlation between ARO and A0ROLayer	Input	Conv1	Conv2Correlation Coefficient	0.02	0.005	0.01Table 10: Correlation between ARO and A1ofKIn order to measure proper correlations, we average gradients of convolutional layers over channelssimilar to the way used to generate a gradient-based saliency map Selvaraju et al. (2017). Other-wise, the order of convolutional filters affects the correlations and makes it hard to measure propercorrelations between models. In this sense, the correlations at FC1 (before the last layer) may notgive meaningful information since neurons in the FC layer do not have a strict ordering.
Table 9: Correlation between ARO and A0ROLayer	Input	Conv1	Conv2Correlation Coefficient	0.02	0.005	0.01Table 10: Correlation between ARO and A1ofKIn order to measure proper correlations, we average gradients of convolutional layers over channelssimilar to the way used to generate a gradient-based saliency map Selvaraju et al. (2017). Other-wise, the order of convolutional filters affects the correlations and makes it hard to measure propercorrelations between models. In this sense, the correlations at FC1 (before the last layer) may notgive meaningful information since neurons in the FC layer do not have a strict ordering.
Table 10: Correlation between ARO and A1ofKIn order to measure proper correlations, we average gradients of convolutional layers over channelssimilar to the way used to generate a gradient-based saliency map Selvaraju et al. (2017). Other-wise, the order of convolutional filters affects the correlations and makes it hard to measure propercorrelations between models. In this sense, the correlations at FC1 (before the last layer) may notgive meaningful information since neurons in the FC layer do not have a strict ordering.
