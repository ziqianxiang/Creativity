Table 1: The probabilities found after solving the empirical payoff matrix with sub-policiesas strategiessub-policies	1	2	3	4	5	6	7	8	9	10	~032^	~026~	~O5~	~00~	~Q7^	~Q~	~Q~	~00~	0.0	~Q~Table 2: Performance on iterated RPS. R = rock_ only, P = paper_only, S = scissors_only,c = “counter”, ci = “counter i”, min denotes the minimum performance along a rowwinrate (%)	R	P	S	repeat	C	c2	c3	c4	minHASP	49	47T~	-50-	-49-	~2Γ	^5T	48	^48^	~^Γ~Random	42	50	45	45	51	56	48	49	42Self-Play	92	20	100	70	43	20	66	60	20Self-Play	75	100	10	62	39	41	77	47	10Self-Play	89	13	59	53	78	45	28	51	13opponent using “counter”, so chooses the counter to that (“counter2”) instead. The reasoningcontinues until it loops back to “counter6” = “repeat”. If our algorithm is successful andthe initial policy is (“repeat”), it should autonomously discover all these sub-policies in thestrategy space.
Table 2: Performance on iterated RPS. R = rock_ only, P = paper_only, S = scissors_only,c = “counter”, ci = “counter i”, min denotes the minimum performance along a rowwinrate (%)	R	P	S	repeat	C	c2	c3	c4	minHASP	49	47T~	-50-	-49-	~2Γ	^5T	48	^48^	~^Γ~Random	42	50	45	45	51	56	48	49	42Self-Play	92	20	100	70	43	20	66	60	20Self-Play	75	100	10	62	39	41	77	47	10Self-Play	89	13	59	53	78	45	28	51	13opponent using “counter”, so chooses the counter to that (“counter2”) instead. The reasoningcontinues until it loops back to “counter6” = “repeat”. If our algorithm is successful andthe initial policy is (“repeat”), it should autonomously discover all these sub-policies in thestrategy space.
Table 3: Reward on 12x12 Generals.io 1v1 mode.
Table 4: Reward on 12x12 Generals.io 1v1 mode. expand_left, expand_right, and ex-pand_more are all trained using self-play along with "achievement" rewardsreward achieved	expand_left	expand_right	expand_more	FloBot	minHASP Random-Ens Self-Play	-0.63[0.04] -0.64 -0.75[0.11]	-0.44[0.02] -0.46 -O64[0.11]	-0.33[0.08] -0.36 -0.34[0.04]	--0.23[0.1]- -0.22405 -0.034[0.05]	-0.63[0.4] -0.64 -0.77[0.08]{1	win-1	loss-0.005 elseWe initialize the first phase of HASP with an agent trained to prefer owning territory on thetop half of the map by adding the achievement reward described in Section 3.2. Runningphase 1 of HASP , we obtained 6 different sub-policies that are diversely distributed in thesub-policy space. From observation, we find that the sub-policies are different from eachother in terms of playing styles. We also find that, as we expected, each of them is trying tocounter the previous self by a large margin, although it is exploitable itself.
Table 5: The frequency at which our policy selects each sub-policy when playing againstexpand_leftsub-policies	1	2	3	4	5	6	0.14	"pɪ	~Q8~	0.22	0.12	0.14Table 4 shows the performance of our HASP agent as well as the baselines against severalheld-out agents that we trained using self-play with an additional "achievement" reward:expand-left, expand-right, and expand-more. The agent trained using HASP achieves a higherreward against all of the held-out agents that we tested on. In contrast with our IteratedRock-Paper-Scissors results, randomly selecting a sub-policy at each turn is competitive withour policy that can change the probability of sampling a sub-policy with the current state.
