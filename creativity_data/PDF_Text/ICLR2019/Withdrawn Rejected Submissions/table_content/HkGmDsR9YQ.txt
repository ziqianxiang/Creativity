Table 1: Direct policy evaluation. Each game was initiallytrained in the default mode for 50M frames then evaluatedin each listed game flavour. Reported numbers are the av-erage over 5 runs. Standard deviation is reported betweenparentheses.
Table 2: Policy evaluation using regularization. Each game wasinitially trained in the default mode for 50M frames with dropoutand `2 regularization then evaluated on each listed flavour. Re-ported numbers are the average over 5 runs. Standard deviationis reported between parentheses.
Table 3: Experiments fine-tuning the entire network with and without regularization (dropout + '2).
Table 4: Experiments fine-tuning early layers of the network trained with regularization. An agentis trained with dropout + `2 regularization in the default flavour of each game for 50M frames, thenDQN’s parameters θ were used to initialize the corresponding layers to be further fine-tuned on eachnew flavour. Remaining layers were randomly initialized. Compared against fine-tuning the entirenetwork from Table 3. Standard deviation reported between parenthesis.
Table 5: Baselines using vanilla DQN for all tested game variants.
Table 6: Baselines using dropout + `2 regularization for each default flavour.
Table 7: Comparison of baseline results with and without regularization in the default flavour. Thebaseline agent with regularization was trained with dropout and `2 regularization.
