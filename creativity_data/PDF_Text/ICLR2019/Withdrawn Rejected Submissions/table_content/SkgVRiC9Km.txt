Table 1: Results on white-box MNIST and CIFAR with FGSM . The ResNet model provided in(Papernot et al., 2016a) was used in the CIFAR FGSM experiments.
Table 2: CIFAR-10 PGD Results with (non-resnet) CNNs. In these experiment we used a fortifiedblock (single convolutional autoencoder) following each convolutional layer. Both experiments wererun for 200 epochs and with all hyperparameters and architecture kept the same with the exception ofthe fortified layer being added. We considered different types of baselines: ‘Baseline - no new layers’means we simply removed the fortified block. ‘Baseline - extra layers’ means that we added extralayers to match the capacity of the fortified layers, but only gave half of these extra layers activationsas the fortified block has two layers but only one activation. ‘Baseline - extra activations’ meansthat we added an activation following each layer, giving more activations in total than the FortifiedNetwork.
Table 3: CIFAR-10 PGD Results with ResNets. In this experiment we used a single fortified layerfollowing the 2nd resblock, and the baseline consists of the same network but with the fortified layerremoved. Both experiments were run for 200 epochs and with all hyperparameters and architecturekept the same with the exception of the fortified layer being added.
Table 4: Accuracies against white-box attacks on Fashion MNIST. For PGD we used ε = 0.1 and forFGSM we used ε = 0.1 and ε = 0.31. Compared with DefenseGAN (Samangouei et al., 2018).
Table 5: Left: Accuracies against blackbox MNIST attacks with adversarial training (FGSM).
Table 6: Hyperparameter combinations (Fashion-MNIST; FGSM; ε=0.3)Reconstruction Loss Weight	n/a	0.01	0.1	1.0	1.0	1.0	1.0Autoencoder Noise (N (0, σ))	n/a	0.01	0.01	0.01	0.001	0.01	0.1Accuracy	88.00	89.70	90.01	91.00	90.78	91.00	91.31Table 7: Fashion MNIST, PGD (ε = 0.1), 40 attack iterations, 50 epochs; experiments based on a 2hidden layer MLP with 512 units per layer and leaky relu activation. ± standard deviation reportedover last 5 epochs. All setups improve over the baseline (72.36%).
Table 7: Fashion MNIST, PGD (ε = 0.1), 40 attack iterations, 50 epochs; experiments based on a 2hidden layer MLP with 512 units per layer and leaky relu activation. ± standard deviation reportedover last 5 epochs. All setups improve over the baseline (72.36%).
Table 8: More detailed version of table 1, but with more detailed ablation experiments for ourmethod included. Accuracies against white-box MNIST attacks with FGSM, where the model isa convolutional net. We used the standard FGSM attack parameters with an ε of 0.3 and compareagainst published adversarial training defenses. We also performed ablations where we consideredremoving the reconstruction error on adversarial examples Ladv as well as switching the activationfunction in the fortified layers from leaky relu to tanh, which we found to slightly help in this case.
Table 9: PGD, attack run for 100 iterationsSteps	Baseline	Baseline (extra layers)	Fortified Networks7 steps	33.0	34.2	45.050 steps	31.6	32.5	42.1200 steps	31.4	32.2	41.5Table 10: More attack steps to uncover gradient masking effects.
Table 10: More attack steps to uncover gradient masking effects.
