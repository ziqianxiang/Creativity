Table 1: Performance of MobileNet-CIFAR on CIFAR-10 dataset with different rand-d, as a ratioto input channel size (d). Our adaptive mixture method provides consistent performance gain withnegligible FLOPs increase.
Table 2: Performance for different networks on ImageNet. With negligible FLOPs increase, adaptivelow-rank factorizations outperforms regular ones.
Table 3: MobileNet-CIFAR: Each line describes a sequence of 1 or more identical (modulo stride)layers, repeated for n times. All layers in the same sequence have the same number c of outputchannels. The first layer of each sequence has a stride s and all others use stride 1. A mobile cellconsists of a depth-wise convolution and a point-wise 1 × 1 convolution. All spatial convolutions use3 × 3 kernels.
Table 4: MobileNet-ImageNet: Each line describes a sequence of 1 or more identical (modulo stride)layers, repeated for n times. All layers in the same sequence have the same number c of outputchannels. The first layer of each sequence has a stride s and all others use stride 1. A mobile cellconsists of a depth-wise convolution and a point-wise 1 × 1 convolution. All spatial convolutions use3 × 3 kernels.
Table 5: MobileNet-ImageNet-LR: Each line describes a sequence of 1 or more identical (modulostride) layers, repeated n times. All layers in the same sequence have the same number c of outputchannels. The first layer of each sequence has a stride s and all others use stride 1. All spatialconvolutions use 3 × 3 kernels. The expansion factor t is always applied to the input size.
Table 6: Inference time / FLOPs of low-rank decomposition.
