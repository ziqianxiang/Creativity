Table 1: Correspondence between NRM and CNN.
Table 2: Error rate percentage on CIFAR-10 over 3 runs.
Table 3: Error rate percentage on CIFAR-100 over 3 runs.
Table 4: Error rate percentage on SVHN in comparison with other state-of-the-art methods. Allresults are averaged over 2 runs (except for NRM+RPN when using all labels, 1 run)	250 labels 73257 images	500 labels 73257 images	1000 labels 73257 images	73257 labels 73257 imagesALI Dumoulin et al. (2017)			7.42 ± 0.65	Improved GAN (Salimans et al., 2016)		18.44 ± 4.8	8.11 ± 1.3	+ Jacob.-reg + Tangents (Kumar et al., 2017)		4.87 ± 1.60	4.39 ± 1.20	Π model (Laine & Aila, 2017)	9.69 ± 0.92	6.83 ± 0.66	4.95 ± 0.26	2.50 ± 0.07Temporal Ensembling (Laine & Aila, 2017)		5.12 ± 0.13	4.42 ± 0.16	2.74 ± 0.06Mean Teacher (Tarvainen & Valpola, 2017)	4.35 ± 0.50	4.18 ± 0.27	3.95 ± 0.19	2.50 ± 0.05VAT+EntMin (Miyato et al., 2018)			3.86	DRM (Nguyen et al., 2016)		9.85	6.78	Supervised-only	27.77 ± 3.18	16.88 ± 1.30	12.32 ± 0.95	2.75 ± 0.10NRM without RPN	9.78 ± 0.24	7.42 ± 0.61	5.64 ± 0.13	3.46 ± 0.04NRM+RPN	9.28 ± 0.01	6.56 ± 0.88	5.47 ± 0.14	3.57NRM+RPN+Max-Min+MeanTeacher	3.97 ± 0.21	3.84 ± 0.34	3.70 ± 0.04	2.87 ± 0.05Rendering Process in NRMAlgorithm 1 Rendering Process in NRMInput: Object category y .
Table 5: Table of notations for NRMVariablesx , input image of size D(0)y , object categoryz(') = {s('), t(')}	， all latent variables of size D(') in level's(`, p)	,	switching latent variable at pixel location p in level `t(`, p)	,	local translation latent variable at pixel location p in level	`h(y, z;')	= h(')	，	intermediate rendered image of size D(') in level'h(y, z;	0)	= h(0)	,	rendered image of size D(0) from NRM before adding noiseψ(')	， corresponding feature maps in layer ' in CNNs.
Table 6: The network architecture used in all of semi-supervised experiments on CIFA10, CIFAR100and SVHN.
Table 7: The network architecture used in our MNIST semi-supervised training.
