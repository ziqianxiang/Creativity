Table 1: Performance of Context Mover’s Barycenters (CoMB) and related baselines on the STStasks using Toronto Book Corpus3. The numbers are average Pearson correlation x 100 (with respectto groundtruth scores). CoMB outperforms the best SIF baseline on 3 out of4 tasks in the test set andalso leads to an overall improvement on average for several hyperparameter settings. It is also 1.5xbetter than the SIF with no PC removed and Skip-thought, and twice as better than BoW. Here, α, β, sdenote the PPMI smoothing, column normalization exponent (Eq. 10) and k-shift. Skip-thoughtscores are taken from Arora et al. (2017).
Table 2: Comparison of the entailment vectors from Henderson (2017) used alone (DHend.), and whenused together with our Context Mover’s Distance, (CMDK, where K is the number of clusters),in the form of ground cost DHend. . The two listed CMD variants are the ones with best validationperformance, when K is fixed to 200 and 250. For reference, this table also includes state-of-the-artmethods, like Gaussian embeddings with cosine similarity (GE+C) or KL-divergence (GE+KL), andDIVE. Scores for GE+C, GE+KL, and DrVE + C∙∆S are taken from Chang et al. (2017) as We usethe same evaluation setup6. The scores are AP@all (%). More details about the training setup andresults on other datasets (along With the effect of PPMI parameters) can be found in Section A.1,Section A.7.1 & Table A.8. Numbers in bold indicate the best score for that dataset and the onesUnderlined denote the second best.
Table 3: Top 10 closest neighbors for CoMB and SIF (no PC removed) found across the vocabulary,and sorted in ascending order of distance from the query sentence. Words in italics are those whichin our opinion would fit well when added to one of the places in the query sentence. Note that, bothCoMB (under current formulation) and SIF don’t take the word order into account.
Table 4: Comparison of the entailment vectors alone (Hend.) and when used together with ourContext Mover’s Distance, CMD(K) (where K is the number of clusters), in the form of ground costDHend.. We also indicate the average gain in performance across these 10 datasets by using CMDalong with the entailment vectors. All scores are AP at all (%).
Table 5: Dataset sizes. N is the number of word pairs in the dataset, and OOV denotes how manyword pairs are not processed.
Table 6: Detailed test set performance of Context Mover’s Barycenters (CoMB) and related baselineson the STS12, STS13, and STS14 tasks using Toronto Book Corpus. The numbers are average Pearsoncorrelation x 100 (with respect to groundtruth scores). Here, α, β, s denote the PPMI smoothing,column normalization exponent (Eq. 10), and k-shift.
Table 7: (continued from Table 6) Detailed test set performance of Context Mover’s Barycenters(CoMB) and related baselines on the STS15 using Toronto Book Corpus. The numbers are averagePearson correlation x 100 (with respect to groundtruth scores). Here, α, β, s denote the PPMIsmoothing, column normalization exponent (Eq. 10), and k-shift.
Table 8: Detailed validation set performance of Context Mover’s Barycenters (CoMB) and relatedbaselines on the STS16 using Toronto Book Corpus. The numbers are average Pearson correlation x100 (with respect to groundtruth scores). Note that, STS16 was used as the validation set to obtainthe best hyperparameters for all the methods in these experiments. As a result, high performance onSTS16 may not be indicative of the overall performance. Here, α, β, s denote the PPMI smoothing,column normalization exponent (Eq. 10), and k-shift.
Table 9: Comparison of the entailment vectors alone (Hend.) and when used together with ourContext Mover’s Distance, CMD(α, s) (where α and s are the PPMI smoothing and shift parameters),in the form of ground cost DHend.. All of the CMD variants use K = 100 clusters. We observe thatusing our method with the entailment vectors performs better on 8 out of 9 datasets in comparsionto just using these vectors alone. Avg. gain refers to the average gain in performance relative tothe entailment vectors. Avg. gain w/o Baroni refers to the average performance gain excluding theBaroni dataset. The hyperparameter α refers to the smoothing exponent and s to the shift in the PPMIcomputation. All scores are AP at all (%).
Table 10: STS ground scores and their implied meanings, as taken from Agirre et al. (2015)Ground-truth details. The ground-truth scores (can be fractional) and range from 0 to 5, and themeaning implied by the integral score values can be seen in the Table 10. In the case where differentexamples have the same ground-truth score, the ground-truth rank is then based on lexicographicalordering of sentences for our qualitative evaluation procedure. (This for instance means that sentencepairs ranging from 62 to 74 would correspond to the same ground-truth score of 4.6). The ranking isdone in the descending order of sentence similarity, i.e., most similar to least similar.
Table 11: Examples of some indicative sentence pairs, from News dataset in STS14, with ground-truthscores and ranking as obtained via (best variants of) CoMB and SIF. The total number of sentences is300 and the ranking is done in descending order of similarity. The method which ranks an examplecloser to the ground-truth rank is better and is highlighted in blue. CoMB ranking is the one producedwhen representing sentences via CoMB and then using CMD to compare them. SIF ranking is whensentences are represented via SIF and then employing cosine similarity.
Table 12: Analysis of sentence lengths in each of the datasets from STS12, STS13, STS14, and STS15.
Table 13: Examples of some indicative sentence pairs, from Images dataset in STS15, with ground-truth scores and ranking as obtained via (best variants of) CoMB and SIF. The total number ofsentences is 750 and the ranking is done in descending order of similarity. The method which ranksan example closer to the ground-truth rank is better and is highlighted in blue. CoMB ranking is theone produced when representing sentences via CoMB and then using CMD to compare them. SIFranking is when sentences are represented via SIF and then employing cosine similarity.
Table 14: Examples of some indicative sentence pairs, from WordNet dataset in STS14, with ground-truth scores and ranking as obtained via (best variants of) CoMB and SIF. The total number ofsentences is 750 and the ranking is done in descending order of similarity. The method which ranksan example closer to the ground-truth rank is better and is highlighted in blue. CoMB ranking is theone produced when representing sentences via CoMB and then using CMD to compare them. SIFranking is when sentences are represented via SIF and then employing cosine similarity.
Table 15: The top word pairs with maximum positive difference in ranks (CMD rank - Hendersonrank). The rank is given out of 1635.
Table 16: The top word pairs with maximum negative difference in ranks (CMD rank - Hendersonrank). The rank is given out of 1635.
