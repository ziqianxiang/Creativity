Table 1: Average scores of various RL algorithms on CartPole and Pendulum with noisy rewards â‘ºand surrogate rewards under known (^) or estimated (r) noise rates. Note that the results for last twoalgorithms DDPG (rand-one) & NAF (rand-all) are on Pendulum, but the others are on CartPole.
Table 2: Average scores of PPO on five selected games with noisy rewards (r) and surrogate rewards(r). The experiments are repeated three times with different random seeds.
Table 3: RL algorithms utilizedin the robustness evaluation.
Table 4: Average performance of Q-Learning on Perturbed MDP example (Figure 4a) with noisyrewards (r), surrogate rewards under known (r) or estimated (r) noise rates. Note that the successmeans the agents can find the optimal policy at every initial state according to learned Q-function.
Table 5: Average scores of various RL algorithms on CartPole with sample-mean reward usingvariance reduction technique (VRT), surrogate rewards (ours) and the combination of them (ours +VRT). Note that the reward confusion matrices are unknown to the agents and the experiments arerepeated three times with different random seeds.
Table 6: Complete average scores of various RL algorithms on CartPole and Pendulum with noisyrewards (r) and surrogate rewards under known (r) or estimated (T) confusion matrices. Note thatthe results for last two algorithms DDPG (rand-one) & NAF (rand-all) are on Pendulum, but theothers are on CartPole. The experiments are repeated three times with different random seeds.
Table 7: Complete average scores of PPO on five selected Atari games with noisy rewards (r) andsurrogate rewards (r). The experiments are repeated three times with different random seeds.
