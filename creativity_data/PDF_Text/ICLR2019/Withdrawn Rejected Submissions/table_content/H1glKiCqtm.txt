Table 1:	Examples showing how the semantics of the variable names within a method can be usedto reason about the name of the method bodyThis semantic knowledge is available to us as even though computers do not need to understand thesemantic meaning of a method or variable name, they are mainly chosen to be understood by otherhuman programmers (Hindle et al., 2012).
Table 2:	Rank 1 F1 scores for each of the embeddings.
Table 3: Results relative to random embeddings for each project-embedding combination. Overlapis the percentage of tokens within the embedding vocabulary that also appear in the project. Speedupis relative speedup of convergence compared to random embeddings. Improvement is relative im-provement in test loss compared to random embeddings.
Table 4: Results relative to random embeddings for each of the pre-trained embeddings, averagedacross all 10 projects.
Table 5: Python versions of the Java examples from table 1We also look at the amount of over-fitting on each project. From figures 1c, 1d, 1f, 1g and 1i we cansee how the random embeddings show a large amount of over-fitting compared to the pre-trainedcode embeddings. We measure how much a project over-fits as:O = Lb/LfLb is the best validation loss achieved and Lf is the final validation loss achieved. We dub this termthe over-fit factor, where an O = 1 would imply the final loss is equal to the lowest loss and thushas not over-fit at all (this could also mean the model is still converging, however from figure 1 wecan see all project-embedding combinations converge before 25 epochs).
Table 6: Over-fit factor for each project-embedding combination. Higher is better.
Table 7: Over-fit factor for each embedding averaged across all 10 projects. Higher is better.
