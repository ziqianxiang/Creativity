Table 1: Accuracy on miniImageNet for 5-way 1-shot and 5-shot classification. For GTR episodictraining, we use initial learning rate 0.01, and decrease by 10 times when encountering validationperformance plateau. Feature extractor is fixed during episodic training. The weight on prior edgesλ (in Equation 4) is chosen by validation (see Table 6).
Table 2: Averaged AUC of abnormality classification.
Table 3: Averaged AUC of disease classification.
Table 4: Graph classification accuracy in percent.			4.3	Graph classificationTo probe the ability of Graph Transformer on transforming same graph features from low-levelsemantics to high-level semantics, we conduct the experiment on two benchmark protein datasets forgraph classification: PROTEINS (Borgwardt et al., 2005; Feragen et al., 2013) and D&D (Dobson& Doig, 2003). PROTEINS contains 1113 graphs and 43472 nodes in total. Each graph has nodesrepresenting secondary structure elements (SSEs), and edges representing neighboring relations inthe amino acid sequence or in 3D space (Borgwardt et al., 2005). The maximum number of nodesin a graph is 620, the average number of nodes is 39.06, and the average number of edges is 72.8.
Table 5: Memory and time complexity of Graph Transformer. N is the number of input graph nodes,M is the number of output graph nodes, d is the models hidden state dimension. GTR scales linearlywith graph size, and uses constant time for training and testing on graph outputs.
Table 6: Accuracy on miniImageNet for 5-way 1-shot and 5-shot classification using different λvalues. GT R(λ = x) indicates GTR using x importance weight on prior edges, and thus (1 - x)importance weight on learned edges. For GTR episodic training, we use initial learning rate 0.01,and decrease by 10 times when encountering validation performance plateau. Feature extractor isfixed during episodic training.
Table 7: Accuracy on miniImageNet for 5-way 1-shot classification using different feature and hid-den feature dimensions. We use initial learning rate 0.01, and decrease by 10 times when encoun-tering validation performance plateau. All other hyper-parameters are fixed. We use 0.5 prior edgeweights. We compare GTR with its direct baseline Gidiaris et al. (Gidaris & Komodakis, 2018).
