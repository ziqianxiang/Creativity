Table 1: The average elapsed time(min) and iterations(k) of different networks till task solved ingiven 100k iterations. The wall clock training time for 100k iterations and the average loss of final10 validations (denoted in underlines) are shown for the unsolved tasks.
Table 2: Bits-per-character on Penn Treebank and enwik8 test set.
Table 4: Comparison for performance and cost ofdifferent setups on Penn Treebank dataset. Table 3: Comparison of proposal generation per-							formance in terms of recall at 1000 proposals.			Model	ARMIN		LSTM	We choose tIoU=0.6 and 0.8 to compare for con-			Setup	1	2	1	2sistency with Buch et al. (2017).		Our MA-SST	Hidden size	500	550		1kachieves highest recall at tIoU=0.8.			nparam (M)	4.02	4.81	4.79	4.79			nmem	5	10	一	—Model	tIoU=0.6	tIoU=0.8	Ttrunc	50	50	100	150SST (Paper)	0.920	0.672	batch size	384	300	128	128SST (We implement)	0.897	0.732	Memory(GB)	3.49	3.56	2.36	3.27MA-SST(Ours)	0.911	0.759	Speed (chars/s)	98k	75k	71k	70k			BPC	1.238	1.226	1.27	1.24The average recall under different proposal numbers and tIoUs3 are depicted in figure 2. MA-SSToutperforms the original SST and our SST simultaneously. To the best of our knowledge, MA-SSTachieves highest performance of recall at around tIoU=0.8 (see table 3), even if other competitivenon-RNN networks (Escorcia et al., 2016; Caba Heilbron et al., 2016; Gao et al., 2017; Guo et al.,2018) are taken into account. The MA-SST tends to precisely generate proposals that has a highoverlapping area with the ground truth action intervals, leading to high recall performance at high3tIoU denotes temporal Intersection over Union between a proposal and its maximally overlapped groundtruth action. Applying bigger tIoU threshold make the generated proposals higher in quality but less in quantity.
Table 5: Hyperparameters of the models for algorithmic tasks.
Table 6: Hyperparameters of the algorithmic tasks.
Table 7: Ablation on Penn Treebank (dh=800).
