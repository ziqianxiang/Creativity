Table 1: Compression Ratios (CR) of Different ArchitecturesMethod	Architecture	Original Error	Final Error	∆ Error	CRRandom	LeNet-300-100	176%	2.25%	0.49%	8%OBD	LeNet-300-100	1.76%	1.96%	0.20%	8%LWC	LeNet-300-100	1.64%	1.59%	-0.05%	8%DNS	LeNet-300-100	2.28%	1.99%	-0.29%	1.8%L-OBS	LeNet-300-100	1.76%	1.96%	0.20%	1.5%MLPrune(Ours)	LeNet-300-100	1.86%	1.94%	0.08%	1.3%OBD	LeNet-5	1.27%	2.65%	1.38%	8%LWC	LeNet-5	0.80%	0.77%	-0.03%	8%DNS	LeNet-5	0.91%	0.91%	0.00%	0.9%L-OBS	LeNet-5	1.27%	1.66%	0.39%	0.9%MLPrune(Ours)	LeNet-5	0.85%	0.89%	0.04%	0.5%LWC	Cifar-Net	18.57%	19.36%	0.79%	9%L-OBS	Cifar-Net	18.57%	18.76%	0.19%	9%MLPrune(Ours)	Cifar-Net	18.43%	18.60%	0.17%	6.4%DNS	AlexNet	43.42%	43.09%	-0.33%	5.7%LWC	AlexNet	42.78%	42.77%	-0.01%	11%L-OBS	AlexNet	43.30%	43.11%	-0.19%	11%MLPrune(Ours)	AlexNet	43.17%	43.14%	-0.03%	4.0%
Table 2: AlexNet Per-layer Compression RatiosArchitecture	Layer	Parameters	Han et al.(2015b)	Guo et al. (2016)	MLPrune (Ours)	conv1	35K	84%	53.8%	67.2%	conv2	307K	38%	40.6%	37.8%	conv3	885K	35%	29.0%	27.7%	conv4	663K	37%	32.3%	33.2%AlexNet	conv5	442K	37%	32.5%	38.6%	Tc!-	38M	9%	3.7%	1.5%	fc2	17M	9%	6.6%	3.2%	fc3	4M	25%	4.6%	13.7%	total-	61M	11%	5.7%	4%5.3	Does MLPrune prune similar parameters as magnitude-based pruning ?In section 5.2, we’ve seen MLPrune can automatically adjust how much to prune from differentlayers. In this section, we’ll investigate what parameters does MLPrune prune within one layer. Sinceboth MLPrune and magnitude-based pruning can achieve good compression results, it’s intriguing7Under review as a conference paper at ICLR 2019(a) Weight distribution before pruningto explore if MLPrune prune similar parameters as magnitude-based pruning, or they prune totallydifferent parameters.
Table 3: Retraining SetupArchitecture	Pruning Phase	Init LR	Decay Epochs	Decay Rate	Dropout	Weight Decay	Stopping Epoch	pretrain 100% to 50%	01 -	40 -	0.1 -	0.5 -	5e-4 -	120 -	50% to25%	0.1	40	0.1	0.5	2e-4	120AlexNet	25%to12.5%	0.1	40	0.1	0.4	2e-4	120	12.5% to 6.25%	0.1	40	0.1	0.4	2e-4	120	6.25% to 5%	0.1	40	0.1	0.4	2e-4	120	5% to 4%	0.1	40	0.1	0.3	2e-4	120	Pretrain 100% to 50%	-0.05- -	20 -	0.1 -	0.5 -	5e-4 -	60 -	50% to25%	0.05	20	0.1	0.5	2e-4	60VGG16	25%to12.5%	0.05	20	0.1	0.5	2e-4	60	12.5% to 6.25%	0.05	20	0.1	0.5	2e-4	60	6.25% to 5%	0.05	20	0.1	0.5	2e-4	60	5% to 4%	0.05	20	0.1	0.4	2e-4	60The AlexNet is first pre-trained using similar settings as in Caffe model zoo. A training image isfirst rescale so that the shorter edge is 256 with aspect ratio unchanged. Then a 227x227 image israndomly croped, followed by randomly flip horizontally or not. We do not apply color augmentationas in original paper (Krizhevsky et al., 2012). During testing, a center cropped image is fed into thenetwork. SGD with momentum is applied for training. The batch-size is 256 and momentum is 0.9for pre-training and all retraining stages.
Table 4: Per-layer Compression RatiosArchitecture	Layer	Parameters	Han et al.(2015b)	Guo et al. (2016)	MLPrune (Ours)	TC1	-235K^^	8%	1.8%	0.73%LeNet-300-100	fc2	30K	9%	1.8%	4.74%	fc3	1K	26%	5.5 %	39.4%	total	-267K^^	8%	1.8%	1.3%	conv1	-05K	66%	14.2%	41.2%	conv2	25K	12%	3.1%	3.1%LeNet-5	Tc1	-400K^^	8%	0.7%	0.2%	fc2	5K	19%	4.3%	8.9%	total	-431K^^	8%	0.9%	0.5%	conv1	35K	84%	53.8%	67.2%	conv2	307K	38%	40.6%	37.8%	conv3	885K	35%	29.0%	27.7%	conv4	663K	37%	32.3%	33.2%AlexNet	conv5	442K	37%	32.5%	38.6%	TcI	38M	9%	3.7%	1.5%	fc2	17M	9%	6.6%	3.2%	fc3	4M	25%	4.6%	13.7%	total	61M	1T%	5.7%	4%
