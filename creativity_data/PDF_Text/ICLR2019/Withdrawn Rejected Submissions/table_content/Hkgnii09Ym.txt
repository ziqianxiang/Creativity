Table 1: Time complexity of various set operations. n is the number of items, d is the dimensionalityof each item, and m is the number of inducing points.
Table 2: Mean absolute errors on the max re-gression task.
Table 3: Error rates on the unique charactercounting task.
Table 4: Meta clustering results. The number inside parenthesis indicates the number of inducingpoints used in ISABs of encoders. We show average likelihood per data for the synthetic dataset andthe adjusted rand index (ARI) for the CIFAR-100 experiment. LL1/data, ARI1 are the evaluationmetrics after a single EM update step. The oracle for the synthetic dataset is the log likelihood ofthe actual parameters used to generate the set, and the CIFAR oracle was computed by running EMuntil convergence.
Table 5: Meta set anomaly results. Each architecture is evaluated using average of test area underreceiver operating characteristic curve (AUROC) and test area under precision-recall curve (AUPR).
Table 6: Test accuracy for the point cloud classification task using 1,000 points.
Table 7: Detailed architectures used in the max regression experiments.
Table 8: Detailed results for the unique character counting experiment.		Architecture	Accuracy	rFF + Pooling rFF + PMA rFFp-mean + Pooling rFFp-max + Pooling rFF + Dotprod SAB + Pooling SAB + Dotprod Set Transformers (SAB + PMA (1)) Set Transformers (SAB + PMA (2)) Set Transformers (SAB + PMA (4)) Set Transformers (SAB + PMA (8))	0.4366 ± 0.0071 0.4617 ± 0.0073 0.4617 ± 0.0076 0.4359 ± 0.0077 0.4471 ± 0.0076 0.5659 ± 0.0067 0.5888 ± 0.0072 0.6037 ± 0.0072 0.5806 ± 0.0075 0.5945 ± 0.0072 0.6001 ± 0.0078	Table 9: Detailed architectures used in the unique character counting experiments.		Encoder	Decoder	rFF	SAB	Pooling	PMAConv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) FC(64, ReLU)	SAB(64, 4) FC(64, ReLU)	SAB(64, 4) FC(64, ReLU) FC(64, -)	mean FC(64, ReLU) FC(1, softplus)	PMA1 (8, 8) FC(1, softplus)The loss function We optimize, as previously mentioned, is the log likelihood logp(χ∣γ) =x log(γ) - γ - log(x!). We chose this loss function over mean squared error or mean absolute errorbecause it seemed like the more logical choice When trying to make a real number match a targetinteger. Early experiments shoWed that directly optimizing for mean absolute error had roughly thesame result as optimizing Y in this way and measuring ∣γ - x|. We train using the Adam optimizerWith a constant learning rate of 10-4 for 200, 000 batches each With batch size 32.
Table 9: Detailed architectures used in the unique character counting experiments.		Encoder	Decoder	rFF	SAB	Pooling	PMAConv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) Conv(64, 3, 2, BN, ReLU) FC(64, ReLU)	SAB(64, 4) FC(64, ReLU)	SAB(64, 4) FC(64, ReLU) FC(64, -)	mean FC(64, ReLU) FC(1, softplus)	PMA1 (8, 8) FC(1, softplus)The loss function We optimize, as previously mentioned, is the log likelihood logp(χ∣γ) =x log(γ) - γ - log(x!). We chose this loss function over mean squared error or mean absolute errorbecause it seemed like the more logical choice When trying to make a real number match a targetinteger. Early experiments shoWed that directly optimizing for mean absolute error had roughly thesame result as optimizing Y in this way and measuring ∣γ - x|. We train using the Adam optimizerWith a constant learning rate of 10-4 for 200, 000 batches each With batch size 32.
Table 10: Detailed architectures used in 2D synthetic experiments.
Table 11: Average log-likelihood/data (LL0/data) and average log-likelihood/data after single EMiteration (LL1/data) the clustering experiment. The number inside parenthesis indicates the numberof inducing points used in the SABs of encoder. For all PMAs, four seed vectors were used.
Table 12: Average log-likelihood/data (LL0/data) and average log-likelihood/data after single EMiteration (LL1/data) the clustering experiment on large-scale data. The number inside parenthesisindicates the number of inducing points used in the SABs of encoder. For all PMAs, six seed vectorswere used.
Table 13: Detailed architectures used in CIFAR-100 meta clustering experiments.
Table 14: Average clustering accuracies measured by Adjusted Rand Index (ARI) for CIFAR100clustering experiments. The number inside parenthesis indicates the number of inducing points usedin the SABs of encoder. For all PMAs, four seed vectors were used.
Table 15: Detailed architectures used in CelebA meta set anomaly experiments. Conv(d, k, s, r, f)is a convolutional layer with d output channels, k kernel size, s stride size, r regularization method,and activation function f. If d is a list, each element in the list is distributed. FC(d, f, r) denotesa fully-connected layer with d units, activation function f and r regularization method. If d is alist, each element in the list is distributed. SAB(d, h) denotes the SAB with d units and h heads.
Table 16: Detailed architectures used in the point cloud classification experiments.
Table 17: Additional point cloud experiments using 100 points.
Table 18: Additional point cloud experiments using 5000 points.	Architecture	AccuracyrFF + Pooling	0.8933 ± 0.0156rFF + PMA (1)	0.8628 ± 0.0136ISAB (16) + Pooling	0.9040 ± 0.0173Set Transformer (16)	0.8779 ± 0.0122rFF + Pooling + tricks (Zaheer et al., 2017)	0.90 ± 0.003Figure 5: Runtime of a single SAB/ISAB block on dummy data. x axis is the size of the input setand y axis is time (seconds). Note that the x-axis is log-scale.
