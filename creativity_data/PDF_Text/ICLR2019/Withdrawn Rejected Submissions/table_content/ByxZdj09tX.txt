Table 1: table of prisoner’s dilemmaIn the same manner, we first train a neural network according to the strategies-to-payoff table forplayer A and B as illustrated in Figure 4 below.
Table 2: permuted tables of prisoner’s dilemma (with NE underline)A StrategiesB Strategies	[0]	[1]∏qT	[7, 7]	[0,10][1]	[10,0]	[5, 5]A StrategiesB Strategies	[0]	[1]iɑr	[0,10]	[5, 5][1]	[7, 7]	[10,0]With a 3-layered deep feedforward neural network with size (1+1)*30*(10+10) with learning anddeducing rate at 0.01 and learning and deducing epochs at 10000, the trajectories of arbitrarilyrandomized starting points for strategy inputs for player A and B in each corresponding table inTable 2 can be shown in the left part of Figure 15.
