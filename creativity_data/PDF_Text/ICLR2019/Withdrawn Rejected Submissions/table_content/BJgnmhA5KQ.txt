Table 1: Results on the WMT datasets. The En-De, En-Fr, Zh-En datasets have 10, 10, 3 human referencesrespectively, and we generate the same number of hypotheses as the number of references.
Table 2: Results of various latent variable models on the IWSLT’14 De-En dataset. For Variational NMT thehypotheses are generated by first sampling z followed by greedy decoding. Since we only have one referencehere, we report pairwise BLEU and BLEU of each hypothesis (K = 2) with respect to the reference.
Table 3: Examples of generations by different methods on the WMT’17 Zh-En dataset. Our Hard-MoE modelshows considerable diversity compared to beam search and diverse beam search.
Table 4: Examples of generations by Hard-MoE on the WMT’17 Zh-En dataset. Different latent values learnto specialize for different translation styles consistently across examples, such as past tense vs. present tense,this vs. that, and per cent vs. %.
Table 5: Results on the WMT’17 En-De dataset with various numbers of generations (K). We compare:multinomial sampling (Sampling); sampling restricted to the top-k candidates at each step (Filteredsampling (top-2)); beam search with varying beam widths (Beam); diverse beam search (Vijayakumaret al., 2016) with varying number of outputs (Diverse beam; note that the number of groups G and diversitystrength are tuned separately for each value of K); and our hard mixture of experts approach with K latentcategories (Hard-MoE).
