Table 1: Program accuracy data for synthetic tasks. The tree-transformer has clear advantages overthe sequence-transformer for larger tasks, indicating that the custom positional encodings may beproviding useful structural information.
Table 2: Program accuracy data for CoffeeScript-JavaScript translation tasks. Here, the tree-transformer is compared to Chen et al.â€™s tree-to-tree model (Chen et al. (2018)) which have pre-viously produced state-of-the-art results. The tree-transformer has improved results on over half thedatasets, and particularly shows improved performance on larger datasets.
Table 3: Metrics for semantic parsing tasks. The sequence-to-tree transformer outperforms thebaseline transformer on most tasks. This demonstrates that the induced bias of explicit tree structureoutweighs the additional hurdle of converting between positional encoding schemes. Transformerarchitectures in general, however, do not yet compete with state-of-the-art results.
