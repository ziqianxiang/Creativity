Table 1: OpenAI-Gym Pendulum policy success rate (higher is better). We evaluate multiple policies trainedand tested on different OpenAI-Gym Pendulum-v0 environment settings. Rows represent performance forpolicies trained on a specific mass, columns correspond to specific test masses. Success rate is computed asthe fraction of 8 trials with an average maximum deviation of less than 15 degrees, over 6 tests per test massper trial, from the vertical steady point over the last 100 steps of a 300-step episode. Darker shading indicatesworse performance.
Table 2: Unity Ball-pushing policy error (lower is better). Performance evaluation of multiple policies trainedand tested on different custom Unity ball-pushing environment settings. Rows represent performance for poli-cies trained on a specific ball mass, columns correspond to specific test masses. Errors are computed as themean Euclidean distance of the ball from the goal evaluated on 6 separate trials, with 50 pre-defined tests pertrial (to ensure fair comparison between policies).
Table 3: Pendulum policy success rate comparisons (higher is better)Test Mass	Policy					Oracle	Joint	PER	TD-error-guided Curriculum	Reward-guided Curriculum (ours)1	1.00	1.00	1.00	100	1.002	1.00	1.00	0.93	100	1003	1.00	1.00	1.00	100	1004	1.00	1.00	1.00	100	1005	1.00	1.00	0.93	100	1006	1.00	1.00	0.79	085	1007	1.00	1.00	0.64	07	1008	1.00	0.93	0.57	069	1009	1.00	0.86	0.29	069	10010	1.00	0.57	0.14	0.46	0.93	—AVg	1.00	0.94	0.73	0.85	0.99Table 4: Cart-Pole policy success rate comparisons (higher is better)Cart Mass	Pole Mass	Policy				Oracle	Joint	Reward-guided Curriculum (ours)1.0	0.10	1.00	0.83 0.83 0.83 0.83	100 1.00 1.00 0.93	0.25	0.83			0.50	1.00		
Table 4: Cart-Pole policy success rate comparisons (higher is better)Cart Mass	Pole Mass	Policy				Oracle	Joint	Reward-guided Curriculum (ours)1.0	0.10	1.00	0.83 0.83 0.83 0.83	100 1.00 1.00 0.93	0.25	0.83			0.50	1.00			1.00	0.83		3.0	0.10	0.83 0.83 0.83 0.83	1.00 1.00	086 0.79 0.79 0.86	0.25				0.50		0.83 0.67		1.00			50	0.10	0.83 0.83 0.83 0.67	0.33 0.33	0.57 0.64 0.64 0.64	0.25				0.50		0.17 0.17		1.00			AVg	一		0.85	0.65	0.81	一Table 5: Ball pushing policy error rate comparisons (lower is better)Test Mass	Policy				Oracle	Joint	PER	Reward-guided Curriculum (ours)2	0.21	0.55	4.85	0.44
Table 5: Ball pushing policy error rate comparisons (lower is better)Test Mass	Policy				Oracle	Joint	PER	Reward-guided Curriculum (ours)2	0.21	0.55	4.85	0.444	0.21	0.64	4.77	0.446	0.22	0.68	4.71	0.458	0.22	0.68	4.66	0.4510	0.22	0.69	4.65	0.45	一AVg	0.22	0.65	4.72	0.45	一It is immediately clear that our method outperforms policies built on joint sampling, PER (wheretested) and the TD-error-guided curriculum (where tested), and achieves a performance closest toour oracle, of all the methods tested - with the Pendulum and Cart-Pole getting within 1% and 4% oftheir oracles’ success rate respectively (noting that the Pendulum’s oracle achieved a 100% successrate). In the case of the ball-pushing task, where we did not have a binary definition of success,7Under review as a conference paper at ICLR 2019it can be noted that the average error is improved over joint sampling, being within 2× of theoracle’s error, as opposed to 3×. Our method also has a significantly lower computational cost thanthe oracle, needing to train only a single policy as opposed to Qm∈M Nm policies. Additionally, asevidenced by Figure 1, the curricula developed appear to address the needs of each learner, adjusting
Table 6: Unity-pendulum success rate for policies trained on individual environment settings (higheris better)Train∖Test	1	2	3	4	5	6	7	8	9	10	AVG2	0.83	0.83	0.50	0.17	0.17	0.00	0.00	0.00	0.00	0.00	0.254	1.00	1.00	1.00	1.00	1.00	1.00	1.00	0.67	0.83	0.67	0.926	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	0.67	0.67	0.938	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.0010	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00Table 7: Cart-pole success rate for policies trained on individual environment settings (higher isbetter). Note that the column and row headings contain the trained/tested pole and cart massesrespectively within bracketsTrain/Test	(0.1,1)	(0.1,3)	(0.1,5)	(0.25,1)	(0.25,3)	(0.25,5)	(0.5,1)	(0.5,3)	(0.5,5)	(1,1)	(1,3)	(1,5)	AVG(0.1,1)	1.00	0.00	0.00	-083-	-000-	-000~	0.67	0.00	0.00	0.00	0.00	0.00	0.21(0.1,3)	-020-	-020-	-000-	-020-	-060-	-000~	-020~	-040-	-000~	0.20	0.20	0.00	0.18(0.1,5)	-100-	-100-	-100-	-100-	-100-	-083-	-^067-	-^067-	-^067-	1.00	0.50	0.50	0.82(0.25,1)	-083-	-000-	-000-	-083-	-000-	-000-	-^067-	-^000-	-^000-	0.00	0.00	0.00	0.19-(0.25,3)-	-080-	-080-	-000-	-080-	-080~	-000~	-080-	-060-	-000~	0.80	0.20	0.00	0.47-(0.25,5)-	-i≡-	-075-	-100-	-100-	-075~	-100~	-100~	-075-	-050~	0.75	0.75	0.50	0.81(0.5,1)	-100-	-000-	-000-	-100-	-000-	-000-	-^083-	-^000-	^^000-	0.17	0.00	0.00	0.25(0.5,3)	-067-	-083-	-033-	-067-	-083-	-033-	-^067-	-^083-	^^017-	0.67	0.50	0.17	0.56
Table 7: Cart-pole success rate for policies trained on individual environment settings (higher isbetter). Note that the column and row headings contain the trained/tested pole and cart massesrespectively within bracketsTrain/Test	(0.1,1)	(0.1,3)	(0.1,5)	(0.25,1)	(0.25,3)	(0.25,5)	(0.5,1)	(0.5,3)	(0.5,5)	(1,1)	(1,3)	(1,5)	AVG(0.1,1)	1.00	0.00	0.00	-083-	-000-	-000~	0.67	0.00	0.00	0.00	0.00	0.00	0.21(0.1,3)	-020-	-020-	-000-	-020-	-060-	-000~	-020~	-040-	-000~	0.20	0.20	0.00	0.18(0.1,5)	-100-	-100-	-100-	-100-	-100-	-083-	-^067-	-^067-	-^067-	1.00	0.50	0.50	0.82(0.25,1)	-083-	-000-	-000-	-083-	-000-	-000-	-^067-	-^000-	-^000-	0.00	0.00	0.00	0.19-(0.25,3)-	-080-	-080-	-000-	-080-	-080~	-000~	-080-	-060-	-000~	0.80	0.20	0.00	0.47-(0.25,5)-	-i≡-	-075-	-100-	-100-	-075~	-100~	-100~	-075-	-050~	0.75	0.75	0.50	0.81(0.5,1)	-100-	-000-	-000-	-100-	-000-	-000-	-^083-	-^000-	^^000-	0.17	0.00	0.00	0.25(0.5,3)	-067-	-083-	-033-	-067-	-083-	-033-	-^067-	-^083-	^^017-	0.67	0.50	0.17	0.56(0.5,5)	-100-	-083-	-083-	-100-	-067-	-067-	-100-	-083-	-083-	1.00	0.83	0.50	0.83(1,1)	-080-	-000-	-000-	-080-	-000~	-000~	-080-	-000~	-000~	0.60	0.00	0.00	0.25(1,3)	-083-	-083-	-017-	-083-	-083-	-0:17-	-^083-	-^083-	-^017-	0.83	0.67	0.17	0.60(1,5)	1.00	0.83	0.83	0.83	0.83	0.83	1.00	0.83	0.83	0.83	0.83	0.67	0.85D Pendulum Curriculum EvolutionFigure 6: Additional Sample Curricula for Pendulum training by Reward-guided Stochastic Cur-riculum13
