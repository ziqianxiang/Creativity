Table 1: Predictive errors in linear regression based on a test set considering different v0 and σMAE / MSE	v0=10-3, σ=0.4	vo=10-3, σ=0.5	v0=10-2, σ=0.4	v0 =10-2 , σ=0.5SGLD-SA	1.32 / 2.85	1.34 / 2.90	1.34 / 2.92	1.37 / 2.93EMVS	1.43 / 3.19	3.04 / 13.6	3.40 / 18.8	1.33 / 2.95SGLD	4.98 / 42.6	4.98 / 42.6	4.98 / 42.6	4.98 / 42.63.4	Posterior ApproximationThe posterior average given decreasing learning rates can be approximated through the weightedsample average E[ψ(β)] = PTPT)：(?)) (Welling and Teh, 2011) to avoid over-emphasizing thetail end of the sequence and reduce=the variance of the estimator. Teh et al. (2015); Chen et al. (2015)showed a theoretical optimal learning rate c(k)H k-1/3 for SGLD and k-1/5 for SGHMC to achievefaster convergence for posterior average, which are used in Sec. 4.1 and Sec. 4.2 respectively.
Table 2: Classification accuracy on MNIST and Fashion MNIST using small networksDataset	MNIST	aMNIST	aMNIST-5	FMNIST	aFMNIST	aFMNIST-5Vanilla	99.31	99.54	99.75	92.73	93.14	94.48Dropout	99.38	99.56	99.74	92.81	93.35	94.53SGHMC	99.55	99.71	99.77	92.93	94.29	94.64SGHMC-SA	99.60	99.75	99.79	93.01	94.38	94.78(a) ζ = ..., 0.3,  	(b) MNISTFigure 3: Adversarial test accuracies based on adversarial images of different levelsover-tuning; however, as the degree of adversarial attacks increases, the performance decreasessharply. In contrast, more robust models should be less affected by these adversarial attacks.
Table 3: Predictive errors in logistic regression based on a test set considering different v0 and σMAE / MSE	v0 =10-2 , σ=0.6	v0=10-1, σ=0.6	v0 =10-2 , σ=2	v0 =10-3 , σ=2SGLD-SA	0.182 / 0.0949	0.195 / 0.1039	0.146 / 0.1049	0.165 / 0.0890SGLD	0.311 / 0.2786	0.304 / 0.2645	0.333 / 0.2977	0.331 / 0.3037ESM	0.240 / 0.0982	0.454 / 0.2080	0.182 / 0.0882	0.172 / 0.1102For any integrable function f , set F as the Fourier transform defined byFɪ=√2∏∕f(x) exp(isx)dx.
