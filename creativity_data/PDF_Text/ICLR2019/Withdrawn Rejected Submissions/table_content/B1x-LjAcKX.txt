Table 1: Average test accuracy (%) of backpropagation (BP), DNI (Jaderberg et al., 2017), critictraining (Czarnecki et al., 2017a), and proposed local critic training (LC). The numbers of localnetworks used are shown in the parentheses. The standard deviation values are also shown.
Table 2: Average test accuracy (%) with respect to the number of layers in the local critic networks.
Table 3: Average test accuracy (%) with respect to the update frequency of local critic networks.
Table 4: Average test accuracy (%) of the sub-models produced by local critic training and thenetworks trained by regular backpropagation.
Table 5: FLOPs required for a feedforward pass and numbers of model parameters in the sub-modelsand main model for CIFAR-10. Note that sub-model 2 has less FLOPs and parameters than sub-model 1 due to the pooling operation in sub-model 2.
Table 6: Average FLOPs and accuracy of progressive inference for test data of CIFAR-10 when thethreshold is set to 0.9 or 0.95.
Table 7: Average test accuracy (%) of Sobolev local critic training (Sob LC), Sobolev critic training(Czarnecki et al., 2017a), and deep supervision (Wang et al., 2015). The numbers of local networksused are shown in the parentheses. The standard deviation values are also shown.
Table 8: Test accuracy (%) of backpropagation (BP), and local critic training (LC) for ResNet-50and ResNet-101. The numbers of local networks used are shown in the parentheses.
Table 9: Test accuracy (%) of ResNet-50 trained with backpropagation (BP) and local critic training(LC) for the ImageNet dataset.
