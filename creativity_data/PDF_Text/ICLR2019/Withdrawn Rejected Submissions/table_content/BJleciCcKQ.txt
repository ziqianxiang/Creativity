Table 1: WER and CER (%) evaluated on WSJ eval92. The models are trained on SI-284.
Table 2: WER and CER (%) evaluated on WSJ eval92 with 1-D convolution.
Table 3: WER and CER (%) on WSJ eval92. The models are trained on WSJ SI-ALL.
Table 4: Frame-wise phoneme classification accuracy (%) on TIMIT.
Table 5: Frame-wise phoneme classification accuracy (%) of different Gated ConvNets on TIMIT.
Table 6: Execution time (sec) for models for 1 sec speech, function of TPModels	TP = 1	TP = 4	TP = 8	TP = 164x600 LSTM	1.46			6x700 QRNN (k = 1), 1-D conv	1.21	0.39	0.20	0.156x700 Diagonal LSTM	1.55	0.47	0.25	0.1830x300 Gated ConvNet, 1-D conv	1.85	0.47	0.29	0.204	Related WorksNeural networks that contain recurrent paths have been of great interest for a long time becauseof their ability to sequence modeling. However, the simple RNN, or so-called vanilla RNN (El-man, 1990), has the vanishing and exploding gradient problems because the recurrent path containsmatrix-vector multiplication operations and the eigenvalues of the matrix cannot be easily controlledduring the training (Bengio et al., 1994). The most successful RNN, so far, is the LSTM RNN, whichemploys the memory cells for storing the information, and the cell states are updated by the use ofelement-wise multiplications (Hochreiter & Schmidhuber, 1997). Since the element-wise operationcorresponds to multiple independent first-order recurrent equations, and the magnitude can be eas-ily controlled by limiting the coefficients of the recurrent equations, LSTM RNN shows fairly goodtrainability and long-term memorization capability. However, LSTM RNN is based on two recurrentpaths, one is the cell state update and the other is the output feedback, and, as a result, the behavior ishard to interpret. Gated recurrent unit (GRU) is also a successful RNN structure; the operation alsoemploys two different recurrences, cell state update, and output feedback to the input (Cho et al.,
