Table 1: BN plus LN final validation performance (ResNet-34-v2, batch size 128)Normalization	CIFAR-10 Accuracy	CIFAR-10 Cross EntropyBN, LN	0.9259	0.3087LN, BN	0.9245	0.3389BN (Ioffe & Szegedy, 2015)	0.9209	0.3969LN (Ba et al., 2016)	0.9102	0.3548the main trunk. The improved residual branch constrains the gradients returning from the residualmappings to be zero-centered and decorrelated with respect to some activations inside the branch.
Table 2: Streaming regularization is less affected by small batch sizes (ResNet-34-v2, batch size 2)Normalization	CIFAR-10 Accuracy	CIFAR-10 Cross EntropyOur Best Hyperparameter	0.9091 Our Worst Hyperparameter	0.9005	0.3627 0.4086BN (IOffe & Szegedy, 2015)^^0.8903 Renorm (Ioffe, 2017)	0.9033 Identity	0.9086	0.4624 0.3823 0.6934 (0.4229 at best point)inputs in batch b. In our work, we keep track of am exponential running estimates across batches,α* 〜EbEi∂LX=X(b)and β 〜EbEi∂LZi ∂ziX=X(b)that marginalize the (B, H, W) di-mensions into accumulators of shape C. The b subscript of the outer expectation is slightly abusivenotation indicating that α* and β* are running averages across recent batches With momentum asa hyperparameter that determines the weighting. We regularize the gradient regression with virtualactivations and virtual gradients, defined as folloWs. We append tWo virtual batch items, broadcastto an appropriate shape, x+ = μb + σb and X- = μb - σb Here, μb and σb are batch statistics ofthe real activations. The concatenated tensor undergoes standard BN, Which outputs the usual {zi }for the real activations, but z+ = 1 and z- = -1 for the virtual items. The z+ and z- do not affect
