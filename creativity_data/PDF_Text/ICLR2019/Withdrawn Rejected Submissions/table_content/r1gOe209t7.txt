Table 1: Test errors (%) of various network structures and methodsStructure and method	DePth	Params	C10	C100FitNet (Romero et al., 2014)	19	-	8.39	35.04Deeply Supervised Net (Lee et al., 2015)	-	-	7.97	34.57Highway Network (Srivastava et al., 2015)	-	-	7.72	32.39ResNet v1 with Stochastic Depth (Huang et al., 2016b)	110	1.7M	5.23	24.58ResNet v2 (He et al., 2016b)	164	1.7M	5.46	24.33ResNet v2 (He et al., 2016b)	1001	10.2M	4.92	22.71SWaPoUt v2 W X 2 (Singh et al., 2016)	20	1.09M	5.68	25.86Swapout v2 W Ã— 4 (Singh et al., 2016)	32	7.46M	4.76	22.72DenseNet-BC	76	0.5M	5.21	24.09DenseNet-BC(standard droPoUt)	76	0.5M	5.56	24.75DenseNet-BC(oUr sPecialized droPoUt)	76	0.5M	4.94	23.90DenseNet-BC	100	0.8M	4.73	23.22DenseNet-BC(standard droPoUt)	100	0.8M	5.01	23.80DenseNet-BC(oUr sPecialized droPoUt)	100	0.8M	4.51	22.33DenseNet-BC	148	1.5M	4.31	20.76DenseNet-BC(standard droPoUt)	148	1.5M	4.60	22.28DenseNet-BC(oUr sPecialized droPoUt)	148	1.5M	3.90	19.754 Experiments
Table 2: Unit-wise pre-dropout vs standard dropoutStructure	Test error (%)		standard dropout	Pre-dropoutDenseNet-40	5.83	5.75DenseNet-BC-76	5.56	5.25From Table 2, pre-dropout structure achievesbetter accuracy than standard dropout on bothof the two DenseNet structures. Such resultscoincide with our analysis that pre-dropoutcould incur better feature-reuse and stimulatevarious features in the network. Meanwhile according to the analysis in Section 3.2 one factor thatcould disadvantage pre-dropout here is that correlated features can compensate for dropped featuresin standard dropout method.
Table 3: Comparisons of different dropout granularityMethod	Test error (%)		C10	C100Unit-wise dropout	5.25	24.52Layer-wise dropout	5.46	25.28Channel-wise dropout	5.09	24.36discards some useful features at one time, as a result a loss of accuracy is observed.
Table 4: Comparisons of various probability schedulesMethod	Error (%)Channel-wise dropout (uniform 0.5)	5.09Channel-wise dropout with v1	5.02Channel-wise dropout with v2	5.13Channel-wise dropout with v3	4.94Table 4 gives an example result for DenseNet-BC-76 on CIFAR10 augmentation dataset. Recall thatthe number of feature maps to shallow layers in DenseNet is very limited and schedule v2 applieslower survival probabilities on these layers, thus from the results we can see although v2 reduces thetotal randomness in the model, a loss of relatively larger quantity of low-level features could stillhurt the accuracy.
Table 5: Effectiveness of the specialized dropoutmethod on other state-of-the-art CNN models. Themodel names in bold denote models with the special-ized dropout method. Results are reported as test errors.
