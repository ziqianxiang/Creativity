Table 1: Summary of Results5	ConclusionThis analysis has only been done on activation outputs for convolutional layers before the applica-tion of non-linearities such as ReLU. Non-linearities introduce more dimensions, but those are nota function of the number of filters in a layer. Hence we recommend not to perform ReLU in-placewhile performing this analysis. The number of samples to be taken into account for PCA are recom-mended to be around 2 orders of magnitudes more than the number of filters we are trying to findredundancy in. This is particularly of importance in the later layers where the activation map sizesare small. We need to collect these activations over many batches to make sure we have enough datato run PCA analysis on. While the percentage variance one would like to retain depends on the ap-plication and acceptable error tolerance, empirically we have found that preserving 99.9% puts us ata sweet spot for most cases with less than half a percentage point in accuracy degradation and a con-siderable gain in computational cost. This analysis comes in handy in three cases: While designingnew network for new data; while adapting given network for new data; and while optimizing currentnetwork for faster runtimes or reduced power consumption during training or inference in hardwareimplementations. Another benefit of this analysis is that not only does it deliver an optimal point, butenables an interpretable, graceful exploration of accuracy-energy trade-off with negligible overheadof compute cost and time. This method is orthogonal to other model compression techniques.
