Table 1: AIN architectures for kaggle competition of Furniture-128layers	Output Size	AIN-121	AIN-169Input	M×N×3	-	Cov	M × N × 64	kernel_size7 ×7,stride2	LAIL(1)	M × N × 64	kernel_size3 × 3,stride2	Dense Block(1)	M × N × 256	(3×3COV) × 6	(3×3COV) × 6LAIL(2)	M × N × 64	kernel_size3 × 3,stride2	Dense Block(2)	M × N × 256	(31Xvɔ × 12	(3×3COV) × 12LAIL(3)	M6 × N6 × 128	kernel_size3 × 3,stride2	Dense Block(3)	M6 × N ×512	(3×3COV) × 24	(3×3COV)× 32LAIL(4)	M × N ×256	kernel_size3 × 3,stride2	Dense Block(4)	M × N ×1024	(1×3COV) × 16	(1×3COV)× 32GAIL	1 × 1 × 512	kerneLSize M × N	FC,Softmax	128		1https://www.kaggle.com/c/imaterialist- challenge- furniture- 20184Under review as a conference paper at ICLR 2019Local Attention Incorporate Layer. The size of input tensors will decrease by - (S=Stride)through LAIL, matrix Weights produce by Cov+Sigmoid which gives a value between 0-1 as anattention gate, matrix X produce by standard Cov+Relu which contains the key information of the
Table 2: Error rates (%) on CIFARMotheds	Depth	Params	C10	C10+AlexNet	-8^^	4.5M	35.8	26.4ResNet-110	110	1.7M	-	6.43DenseNet(k=12)	40^^	1.04M	7.00	5.36DenseNet(k=12)	100	7.08M	5.84	4.21AIN(CNNBlock)	10^^	4.7M	21.8	14.2AIN(ResBlock)	110	2.31M	-	6.26AIN(DenseBlock)	40^^	1.57M	6.72	5.15AIN(DenseBlock)	100	10.6M	5.61	4.02associated with the standard data augmentation scheme (mirroring/shifting) as (He et al., 2016;Huang et al., 2016), we name this dataset as ”furniture128+”. The second way is to scale the imageinto the maximum slide size of224(e.g.:350x 350 → 224× 224, 640 ×480 → 224× 168,268 ×400 →150×224). With random mirroring and zoom fact between 0.7 and 1.3. The input data still remainthe size distinctive, We name this dataset as ”furniture128*”.
Table 3: Error rates (%) on furniture-128Motheds	Depth	Params	furniture-128+	furniture128*ResNet-101	101	42.9M	20.1	-ResNet-152	152	58.6M	18.8	-DenseNet121	121	7.2M	18.1	-DenseNet169	169	12.9M	16.4	-AIN(ResBlock)	101	44.1M	193	187AIN(ResBlock)	152	59.9M	18.2	18.0AIN(DenseBlock)	121	7.56M	17.1	165AIN(DenseBlock)	169	14.8M	16.2	15.30.50.40	10	20	30	40	50	60	70	80epochFigure 8: Furniture-128 loss and epochs0.7	0.8	0.9	1.0	1.1	1.2	1.3	1.4	1.5parameters	le7Figure 9: Furniture-128 error and parameters7Under review as a conference paper at ICLR 2019
Table 4: The top-1 and top-5 error rates(%) on the ImageNet validation set, with single-crop / 10- crop testing for ImageNet+, without zoom / 10- zoom testing ImageNet*			Motheds	Params	topi	top5DenseNet121(ImageNet+)	8.06M	25.02/23.61	7.71/6.66DenseNet169(ImageNet+)	14.31M	23.80/22.08	6.85/5.92DenseNet201(ImageNet+)	20.24M	22.58/21.46	6.34/5.54AIN121(ImageNet*)	8.20M	23.77/23.23	6.73/6.61AIN169(ImageNet*)	15.99M	22.24/21.99	6.06/5.90AIN201(ImageNet*)	22.48M	21.45/21.23	5.54/5.43It’s obvious to find that 10-crop achieve a lower error rate than single-crop in previous previousliterature such as (Huang et al., 2016; Chollet, 2016; He et al., 2016). Table. 4 shows that thegap between without-zoom/10-zoom is much smaller than the gap of single-crop/10-crop. ForDenseNet201 top1 error, single-crop give an error rate of 22.58 ,and 21.46 for 10-crop. There isa gap of 1.12 between them. The top1 error gap between without-zoom and 10-zoom of AIN201 is0.22 which is much smaller than 1.12. Crop for 10 times almost certainly get the global vision ofthe image statistically which increase the 10 times computational cost in forward pass. AIN can getthe global vision of the image with single forward pass, as a result the improvement of 10-zoom isnot so significant than 10-crop. It infers that random crop might lost some information of the image,and AIN is a better way to deal with the various image size comparing with random crop.
