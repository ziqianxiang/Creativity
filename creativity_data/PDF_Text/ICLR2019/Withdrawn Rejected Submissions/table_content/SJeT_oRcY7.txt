Table 1: Test accuracies (%) on MNIST for nh = 5000 hidden neurons and receptive field size p =10. The reference algorithm l-BP is a rate model trained with backpropagation, see subsection 3.2and subsection 3.3. The rate models were trained with 107 iterations (pattern presentations), thespiking LIF model with 106 iterations.
Table 2: Test accuracies (%) on MNIST and CIFAR10. PCA uses full connectivity and 700 (2500)PCs for MNIST (CIFAR10). All other methods use nh = 5000 hidden neurons and receptive fieldsize p = 10. Note that CIFAR10 has d = 32×32×3 = 3072 input channels (the third factor is due tothe color channels), MNIST only d = 28×28 = 784. The models were trained for 107 iterations (≈167 epochs).
Table 3: (Hyper-)Parameters for SC. Best performing parameters in bold.
Table 4: (Hyper-)Parameters for RP. Best performing parameters in bold.
Table 5: (Hyper-)Parameters for BP, FA and SH. Best performing parameters in bold.
Table 6: (Hyper-)Parameters for the spiking LIF l-RP model. Input and target amplitudes are im-plausibly high due to the arbitrary convention R = 1 Ω. Best performing parameters in bold.
Table 7: (Hyper-)Parameters for the LIF rate l-RP model. Parameters are the same as in the spikingmodel; only a was converted from α according to Equation 26. 107 iterations were used for trainingthe rate model (compared to 106 in the spiking model). Input and target amplitudes are implausiblyhigh due to the arbitrary convention R =1 Ω. Best performing parameters in bold.
Table 8: MNIST benchmarks for bio-plausible models of deep learning compared with modelsin this paper (bold). Models are ranked by accuracy (rightmost column). Accuracy refers to theclassification accuracy on the MNIST test set. Parts of this table are taken from (Diehl & Cook,2015) and (Kheradpisheh et al., 2018). Models involving convolutional/pooling layers are markedin blue. Note that the simple models in the l-RP class (l-RP, LIF rate & spiking l-RP), marked inred, perform better than several more elaborate models. For conventional ANN/DNN/CNN MNISTbenchmarks see Table at http://yann.lecun.com/exdb/mnist/).
