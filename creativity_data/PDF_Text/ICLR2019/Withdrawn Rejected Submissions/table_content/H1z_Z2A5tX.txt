Table 1: MNIST - GRU	DeCU		VoCu	NaiveTraining Set	Null Digits	100% 97.5 ± 0.2%	100% 94.3 ± 0.8%	100% 0%Test Set	NUll Digits	100% 96.5 ± 0.3%	100% 93.8 ± 0.7%	100% 0%	Table 2: MNIST - LSTM					DeCu	VoCu	NaiveTraining Set	NUll Digits	100% 97.3 ± 0.1%	100% 95.2 ± 0.3%	100% 0%Test Set	NUll Digits	100% 97.3 ± 0.1%	100% 95.2 ± 0.2%	100% 0%	Table 3: CIFAR -		GRU			DeCu	VoCu	NaiveTraining Set	NUll Digits	100% 94.3 ± 0.9%	100% 88.8 ± 2.0%	100% 0%Test Set	NUll Digits	100% 72.4 ± 1.0%	100% 65.7 ± 1.2%	100% 0%	Table 4: CIFAR - LSTM					DeCu	VoCu	NaiveTraining Set	NUll Digits	100% 95.1 ± 0.4%	100% 92.9 ± 0.8%	100% 0%Test Set	NUll Digits	100% 66.5 ± 0.4%	100% 67.7 ± 1.2%	100% 0%B	Accuracies when requiring a second retrievalAccuracies when extending the nominal task to two trigger setting. Specifically, after the first triggeris introduced, we add another one and observe the decay in accuracy upon that second trigger.
Table 5: Second Retrieval MNISTDeCu	VoCuGRU	First trigger at t = Second trigger at t Difference	12 = 12	97.6 ± 0.1% 97.2 ± 0.1% 0.4 ± 0.2%	93.6 ± 0.6% 79.8 ± 5.4% 13.9 ± 5.3%LSTM	First trigger at t = Second trigger at t Difference	12 = 12	97.4 ± 0.1% 97.3 ± 0.2% 0.2 ± 0.3%	94.9 ± 0.3% 86.3 ± 5.1% 8.7 ± 5.1%Table 6: Second Retrieval - CIFARDeCu	VoCuGRU	First trigger at t = Second trigger at t Difference	12 =12	93.2 ± 0.2% 93.2 ± 0.3% 0.1 ± 0.3%	89.7 ± 0.4% 85.9 ± 0.3% 3.8 ± 3.2%LSTM	First trigger at t = Second trigger at t Difference	12 =12	95.7 ± 0.1% 95.7 ± 0.3% 0.03 ± 0.4%	93.2 ± 0.1% 92.91 ± 0.3% 0.3 ± 0.3%C Hyper-parametersC.1 MNISTThe network was trained for a total of 14 ∙ 104 Gradient Decent steps, using the ADAM optimizers.
Table 6: Second Retrieval - CIFARDeCu	VoCuGRU	First trigger at t = Second trigger at t Difference	12 =12	93.2 ± 0.2% 93.2 ± 0.3% 0.1 ± 0.3%	89.7 ± 0.4% 85.9 ± 0.3% 3.8 ± 3.2%LSTM	First trigger at t = Second trigger at t Difference	12 =12	95.7 ± 0.1% 95.7 ± 0.3% 0.03 ± 0.4%	93.2 ± 0.1% 92.91 ± 0.3% 0.3 ± 0.3%C Hyper-parametersC.1 MNISTThe network was trained for a total of 14 ∙ 104 Gradient Decent steps, using the ADAM optimizers.
