Table 1: Inception scores on unlabelled CIFAR with and without the chain generator approachtrained with WGAN+GP. d refers to the depth of the first conv layer in DCGAN architectures. Seesection 7.4 for details about architecture.
Table 2: Comparison of inception scoresMethod (unlabelled)	Inception scoresReal Data	11.24±0.12WGAN (Arjovsky et al., 2017)	3.82±0.06MIX + WGAN (Arora et al., 2017)	4.04±0.07DCGAN - WGAN+GP *	5.18±0.05ALI (Dumoulin et al., 2016)	5.35±0.05BEGAN (Berthelot et al., 2017)	5.62Small DCGAN (WGAN+GP) + Editors*	6.05±0.04Small ResNet (WGAN+GP) + Editors*	6.70±0.02ResNet - WGAN+GP *	6.86±0.04EGAN-Ent-VI (Dai et al., 2017)	7.07±0.10MGAN (Hoang et al., 2017)	8.33±0.10The variant of ChainGAN using a ResNet base generator also uses a smaller network (model 8),while achieving comparable scores. While performing slightly worse than base on the standardinception score (IV2-TF), we outperform the base model in the IV3-Torch inception model used inBarratt & Sharma (2018). We also see the same effect as above: results from the base generatorare better when in chain and trained alongside editors (model 8) rather than being trained by itself(model 7). Figure 4b shows random samples from Editor 3 of the Small ResNet + Editors model.
