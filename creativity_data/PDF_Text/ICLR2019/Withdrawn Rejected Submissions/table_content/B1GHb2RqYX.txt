Table 1: Classification accuracy (%). PolyCNN rows only show the best performing (single-seed) model andthe Baseline row shows the particular CNN counterpart.
Table 2: Classification accuracy (%) on CIFAR-10 with 20 convolution layers and 512 filters in each layer.
Table 3: Classification accuracy (%) on	CIFAR-10 with 20 convolution layers and 512 filters in each layer.					# Seed Filters	1	2	4	8	16	32	64	128	256	512PolyCNN (early fan-out) 87.24	88.06	88.76	88.98	89.35	90.02	90.78	91.89	92.28	92.48PolyCNN (late fan-out)	81.73	83.42	85.50	86.95	88.91	90.34	91.48	92.09	92.21	92.334.3.2	Varying the Number of Seed FiltersHere we report CIFAR-10 accuracy by varying the number of seed filters in Table 3. The network has20 PolyCNN layers, and the total number of filters per layer is set to 512. We now vary the numberof seed filters from 1 to 512, by a factor of 2. So when the number of seed filters is approaching512, PolyCNN reduces to standard CNN. As can be seen, as we increase the number of seed filters,we are essentially increase the model complexity and the performance is rising monotonically. Thisexperiment will provide insight into trading-off between performance and model complexity.
Table 4: Classification accuracy (%) on 100-class ImageNet with varying convolutional filter sizes.
Table 5: Comparison of the number of learnable parameters in convolutional layers in AlexNet and AlexNetwith PolyCNN modules. The proposed method saves 6.4873Ã— learnable parameters in the convolutional layers.
Table 6: Top-1 classification accuracy (%) on full ImageNet with AlexNet.
Table 7: Top-1 classification accuracy (%) on full ImageNet with native ResNet family.
