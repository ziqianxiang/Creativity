Table 1: A description of the problem configurations and training strategies used in this paper. η0 is the initiallearning rate, W is the number of epochs used for warm-up in the linear scaling rule, E is the total number ofepochs trainedDataset	Task	Architecture	Training Strategy	BS rangeMNIST	IC	ResNet34	BLR, LSR (no = 0.1, W = 10,E =	200)	26 - 214CIFAR-10	IC	AlexNet, MobileNetV2 ResNet34, VGG16	BLR, LSR, SRSR (no = 0.1,W = 10,E = 200)	26 - 214CIFAR-100	IC	ResNet34	BLR, LSR (no = 0.1, W = 10,E =	200)	26 - 214SVHN	IC	ResNet34	BLR, LSR (no = 0.1, W = 10,E =	200)	26 - 214WikiText-2	NLP	LSTM	BLR, LSR (no = 20,W = 3,E =	40)	23 - 21oCityscapes	IS	DRN-D-22	BLR, LSR (no = 0.01, W = 10,E =	二 100)	23 - 211Figure 2: On the left: speedup curves when applying several popular techniques to avoid the generalizationgap. Base LR uses a single learning rate for all batch sizes. On the right: the effect of the linear approximationerror on final test accuracy when using the linear LR scaling rule.
