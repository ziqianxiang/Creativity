Table 1: Likelihood-based benchmark and time statistics for synthetic Turing test. ‘-(MLE)’ meansthe best performance is acquired during MLE pre-training.
Table 2: N-gram-level quality benchmark: BLEU on test data of EMNLP2017 WMT NeWsModel/Algorithm	BLEU-2	BLEU-3	BLEU-4	BLEU-5MLE	0.781	0.482	0.225	0.105SeqGAN (Yu et al., 2017)	0.731	0.426	0.181	0.096RankGAN (Lin et al., 2017)	0.691	0.387	0.178	0.095MaliGAN (Che et al., 2017)	0.755	0.456	0.179	0.088LeakGAN (Guo et al., 2017)	0.835	0.648	0.437	0.271TextCoT-basic (ours)	0.785	0.489	0.261	0.152TextCoT-strong (ours)	0.800	0.501	0.273	0.200TextCoT-strong (α = 1.5) (ours)	0.856	0.701	0.510	0.310Table 3: Diversity benchmark: estimated Word Mover Distance (eWMD) and NLLtestModel/Algorithm	eWMDtest	eWMDtrain			NLLtestMLE	1.015(σ =	0.023)	0.947 (σ	0.019)	2.365SeqGAN (Yu et al., 2017)	2.900 (σ =	0.025)	3.118 (σ	0.018)	3.122RankGAN (Lin et al., 2017)	4.451 (σ =	0.083)	4.829 (σ	0.021)	3.083MaliGAN (Che et al., 2017)	4.891 (σ =	0.061)	4.962 (σ	0.020)	3.240LeakGAN (Guo et al., 2017)	1.803 (σ =	0.027)	1.767 (σ	0.023)	2.327TextCoT-basic (ours)	0.766 (σ =	0.031)	0.886 (σ	0.019)	2.247TextCoT-strong (ours)	0.923 (σ =	0.018)	0.941 (σ	0.016)	2.1444.2 TextCoT: Zero-prior Long & Diverse Text Generation
Table 3: Diversity benchmark: estimated Word Mover Distance (eWMD) and NLLtestModel/Algorithm	eWMDtest	eWMDtrain			NLLtestMLE	1.015(σ =	0.023)	0.947 (σ	0.019)	2.365SeqGAN (Yu et al., 2017)	2.900 (σ =	0.025)	3.118 (σ	0.018)	3.122RankGAN (Lin et al., 2017)	4.451 (σ =	0.083)	4.829 (σ	0.021)	3.083MaliGAN (Che et al., 2017)	4.891 (σ =	0.061)	4.962 (σ	0.020)	3.240LeakGAN (Guo et al., 2017)	1.803 (σ =	0.027)	1.767 (σ	0.023)	2.327TextCoT-basic (ours)	0.766 (σ =	0.031)	0.886 (σ	0.019)	2.247TextCoT-strong (ours)	0.923 (σ =	0.018)	0.941 (σ	0.016)	2.1444.2 TextCoT: Zero-prior Long & Diverse Text GenerationAs an important sequential data modeling task, zero-prior text generation, especially long anddiversified text generation, is a good testbed for evaluating the performance of a generative model.
Table 4: WMT News Samples from Different Models	Sources	ExampleLeakGAN	(1)	It,s a big advocate for therapy is a second thing to do, and I,m creating a relationship with a nation. (2)	It,s probably for a fantastic footage of the game, but in the United States is already time to be taken to live. (3)	It,s a sad House we have a way to get the right because we have to go to see that," she said. (4)	I,m not sure if I thank a little bit easier to get to my future commitment in work, " he said. (5)	“ I think it was alone because I can do that, when you're a lot of reasons, " he said. (6)	Ifs the only thing We do, We spent 26 and $35(see how you do is We lose it," said both sides in the summer.
Table 5: N-gram-level quality benchmark: BLEU on test data of EMNLP2017 WMT News (New Split)						Model/Algorithm	BLEU-2	BLEU-3	BLEU-4	BLEU-5	eWMD	CoT-basic (ours)	0.850	0.571	0.316	0.169	1.001 (σ =	0.020)Reverse KL (ours)	0.860	0.590	0.335	0.181	1.086 (σ =	0.014)			11			Under review as a conference paper at ICLR 2019Although under evaluation of weak metrics like BLEU, if successfully trained, the model trainedvia Reverse KL seems to be better than that trained via CoT, the disadvantage of Reverse KL underevaluation of more strict metric like eWMD indicates that Reverse KL does fail in learning someaspects of the data patterns e.g. completely covering the data mode.
