Table 1: Raw communication bits per worker (Kbits per iteration of training) for different networksMethod	I Baseline	DQSGD	QSGD	TernGrad	One-BitFC300-100	I 8531.5	422.8	422.8	426.2	342.6Lenet	I 53227.8	2636.7	2636.7	2641.2	1897.8CifarNet	I 34185.5	1690	1690	1692	1251Table 2: Resulting bit stream per worker (Kbits per iteration of training) after entropy coding fordifferent networks, 32 workersMethod	I DQSGD	QSGD	TernGrad	One-BitFC300-100	I 38.6	38.2	48.23	330Lenet	I 299.7	307.3	438.2	1889CifarNet	I 192.7	197	281	1241Figure 4 shows the accuracy of the final trained model vs different number of workers for FC-300-100and Lenet models. Table 3 shows the results for CifarNet model after 50 epochs ot training. From thesimulations, it is seen that our proposed algorithm performs much better than the one-bit quantizationmethod and is close to the baseline performance (non-quantized communication).
Table 2: Resulting bit stream per worker (Kbits per iteration of training) after entropy coding fordifferent networks, 32 workersMethod	I DQSGD	QSGD	TernGrad	One-BitFC300-100	I 38.6	38.2	48.23	330Lenet	I 299.7	307.3	438.2	1889CifarNet	I 192.7	197	281	1241Figure 4 shows the accuracy of the final trained model vs different number of workers for FC-300-100and Lenet models. Table 3 shows the results for CifarNet model after 50 epochs ot training. From thesimulations, it is seen that our proposed algorithm performs much better than the one-bit quantizationmethod and is close to the baseline performance (non-quantized communication).
Table 3: Accuracy of CifarNet after 50 epochs of training, Adam training algorithmMethod I	Baseline	DQSG	QSG	TernGrad	One-Bit4 workers ∣	68.2	65.6	64.7	64.7	49.68 workers ∣	68.2	64.1	64.1	64	47.80. 0. 0. 0. 0.
