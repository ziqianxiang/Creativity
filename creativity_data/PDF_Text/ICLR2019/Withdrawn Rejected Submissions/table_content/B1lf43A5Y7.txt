Table 1: The accuracy of MemNet on the task of Path Finding with 1k and 10k examples. Super-vising the attention layers helps substantially. We also report the performance of the 2-hop memorynetwork in Sukhbaatar et al. (2015) for reference.
Table 2: Sentence-level attention weights placed on gold chains on both first and second step on thebAbI development set. Models are trained on 1k examples. Here, “> a ” denotes the percentageof samples which place more than an a-fraction of the weight on the gold sentence at that step.
Table 3: The performance of different models on development and test set. Our simple memory-network based model outperforms recent prior work and nearly equals the performance of the Entity-GCN (De Cao et al., 2018).
Table 4: Percentages of samples with attention above or below the given threshold. AvgMax denotesthe average of the max weight over sentence of the whole development set.
Table 5: Accuracy of models with attention weight above or below the given threshold. Here weonly pick the the attention weight on the third step as an illustration since it has similar behaviors onall three steps.
Table 6: Some representative examples from two error categories. Here, we picked up the sentencewith the highest attention weight at each step. ID denotes the actual example ID in the developmentset of WikiHop. The number at the end of each sentence denotes the aggregate attention weight forthe sentence.
