Table 1: Performance of variants of proposed listener architecture (image-modality, attention, andcontext-incorporation alternatives) on different generalization tasks and subpopulations of the testset (chance is 33%; mean accuracy Â±1 standard error). Bottom table uses best-performing modelfrom top table. Averages taken over five random seeds that controlled the data splits and neural-netinitializations.
Table 2: Testing the part-awareness of neural listener by lesioning different parts of the objects.
Table 3: Speaker evaluations. For the neural listeners, five random seeds controlling the weightinitialization and speaker-listener data splits were used.
Table 4: Optimal hyper parameters for neural listener architectures using both images and pointclouds and word attention.
Table 5: Optimal hyper parameters for the context-aware neural-speaker.
Table 6: Performance of different listeners in specific subpopulations in the earlier languagegeneralization task. Averages over five random seeds that controlled the data splits and the neural-netinitializations.
Table 7: Most distinctive words in each triplet type according to point-wise mutual information(excluding tokens that appeared fewer than 30 times in the dataset). Lower numbers are moredistinctive of far and higher numbers are more distinctive of close .
