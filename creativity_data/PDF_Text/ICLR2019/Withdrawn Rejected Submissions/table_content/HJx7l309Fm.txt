Table 1: Comparison of various methods for multi-agent RL	Base Algorithm	Attention	Centralized Critic(s)	Number of Critics	Multi-task Learning of Critics	Multi-Agent AdvantageMAAC (OUrS)	-	SaC	X	X	N	X	XMAAC (Uniform) (ours)	SAC	-	uniform	X	N	X	XCOMA*	Actor-Critic (On-Policy)		X	1		X	-MADDPGt	DDPG		X	N		COMA+SAC	SaC		X	1		XMADDPG+SAC	SaC		X	N		X	-DDPG*	-	DDPG	-			N	N/A	N/A 一Centralized Critic(s): each agent’s estimate of Qi takes the actions and observations of the other agents into account. Number of Critics:number of separate networks used for predicting Qi for all N agents. Multi-task Learning of Critics: all agents’ estimates of Qi shareinformation in intermediate layers, benefiting from multi-task learning. Multi-Agent Advantage: cf. SeC 3.2 for details. * (Foerster et al.,2018), t (Lowe et al., 2017), * (LiUiCrap et al., 2016)spaCe and the rover’s observation spaCe), rather than being expliCitly part of the model, and is limitedto a few disCrete signals.
Table 2: MAAC improvesover MADDPG+SAC# agents	4	8	16Percentage	^20^	"49^	-33-our approach scales better when the number of agents increases. In future research we will continueto improve the scalability when the number of agents further increases by sharing policies amongagents, and performing attention on sub-groups (of agents).
