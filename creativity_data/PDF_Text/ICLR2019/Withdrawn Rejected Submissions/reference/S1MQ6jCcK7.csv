title,year,conference
 Robust optimizationfor deep regression,2015, In Proc
 Mixture density networks,1994, 1994
 Robust learning from demonstration using lever-aged Gaussian processes and sparse constrained opimization,2016, In Proc
 Uncertainty-aware learning fromdemonstration using mixture density networks with sampling-free variance modeling,2017, arXivpreprint arXiv:1709
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Scalablekernel methods via doubly stochastic gradients,2014, In Proc
 A geometric perspective onthe robustness of deep networks,2017, IEEE Signal Processing Magazine
 Robust loss functions under label noise for deepneural networks,2017, In Proc
 Training deep neural-networks using a noise adaptationlayer,2017, In Proc
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, arXiv preprint arXiv:1802
 Mentornet: Regularizing verydeep neural networks on corrupted labels,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Temporal ensembling for semi-supervised learning,2017, In Proc
 Pseudo-label: The simple and efficient semi-supervised learning method for deepneural networks,2013, In Workshop on Challenges in Representation Learning
 Self-error-correcting convolutionalneural network for learning with noisy labels,2017, In Proc
" Decoupling"" when to update"" from"" how to update""",2017, InAdvances in Neural Information Processing Systems
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Learning withnoisy labels,2013, In Proc
 A survey of motionplanning and control techniques for self-driving urban vehicles,2016, IEEE Transactions on IntelligentVehicles
 Makingdeep neural networks robust to label noise: a loss correction approach,2017, In Proc
 Estimating accuracy fromunlabeled data: A bayesian approach,2016, In Proc
 Infinite mixtures of gaussian process experts,2002, InAdvances in Neural Information Processing Systems
 Gaussian processes for machine learning,2006, 2006
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In Conference on Computer Vision andPattern Recognition
 Wide residual networks,2016, In BMVC
 Understandingdeep learning requires rethinking generalization,2016, In Proc
 mixup: Beyond empiricalrisk minimization,2017, In Proc
