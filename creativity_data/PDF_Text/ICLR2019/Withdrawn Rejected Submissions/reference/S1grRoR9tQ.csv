title,year,conference
 Adaptive Algorithms and StochasticApproximations,1990, Berlin: Springer
 Weight uncertainty inneural networks,2015, In Proc
 On the convergence of stochastic gradient MCMCalgorithms with high-order integrators,2015, In Proc
 Stochastic gradient Hamiltonian Monte Carlo,2014, InProc
 Bayesian convolutional neural networks with Bernoulli approxi-mate variational inference,2016, In ICLR workshop
 Dropout as a Bayesian approximation: Representing modeluncertainty in deep learning,2016, In Proc
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Proc
 Generative adversarial nets,2014, In Proc
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proc
 What is the best multi-stage architecture forobject recognition? In Proc,2009, of the International Conference on Computer Vision (ICCV)
 Adam: A method for stochastic optimization,2014, In Proc
 Preconditioned stochasticgradient Langevin dynamics for deep neural networks,2016, In Proceedings of the Thirtieth AAAIConference on Artificial Intelligence
 Trajectory averaging for stochastic approximation MCMC algorithms,2010, The Annals ofStatistics
 A complete recipe for stochastic gradient mcmc,2015, In NIPSAutodiff Workshop
 cleverhans v2,2016,0
 Automatic differentiation inpytorch,2017, In NIPS Autodiff Workshop
 Bayesian gan,2017, In Proc
 Approximation analysis of stochastic gradient Langevin dynamicsby using fokker-planck equation and ito process,2014, In Proc
 Regularization of neuralnetworks using dropconnect,2013, In Proc
 Bayesian learning via stochastic gradient Langevin dynamics,2011, InProc
 Stochastic pooling for regularization of deep convolutional neuralnetworks,2013, In Proc
 A hitting time analysis of stochastic gradientLangevin dynamics,2017, In Proc
2 General Stochastic ApproximationIn contrast to Eq,2019,(21) and Eq
