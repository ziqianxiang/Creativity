title,year,conference
 Variance reduction for faster non-convex optimization,2016, InProceedings of The 33rd International Conference on Machine Learning
 Stochastic optimization with variance reduction for infinite datasetswith finite sum structure,2017, In Advances in Neural Information Processing Systems 30 (NIPS 2017)
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, Advances in Neural InformationProcessing Systems 27 (NIPS 2014)
 Competing with the empirical riskminimizer in a single pass,2015, In Proceedings of The 28th Conference on Learning Theory
 Densely connectedconvolutional networks,2017, In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the 32nd International Conference on MachineLearning
 Accelerating stochastic gradient descent using predictive variancereduction,2013, Advances in Neural Information Processing Systems 26 (NIPS2013)
 On the insufficiency ofexisting momentum schemes for stochastic optimization,2018, International Conference on LearningRepresentations (ICLR)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Gradient-based learning applied to document recog-nition,1998, Proceedings of the IEEE
 Less than a Single Pass: Stochastically Controlled Stochastic Gradi-ent,2017, In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics
 Non-convex finite-sum optimization viascsg methods,2017, In Advances in Neural Information Processing Systems 30
 Incremental majorization-minimization optimization with application to large-scalemachine learning,2014, Technical report
 Minimizing finite sums with the stochasticaverage gradient,2017, F
 Accelerated proximal stochastic dual coordinate ascent forregularized loss minimization,2014, Proceedings of the 31st International Conference on MachineLearning
 Wide residual networks,2016, arXiv preprintarXiv:1605
