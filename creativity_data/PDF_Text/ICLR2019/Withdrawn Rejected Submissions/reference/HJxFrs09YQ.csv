title,year,conference
 Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep NeuralNetwoks,2018, arXiv preprint arXiv:1806
 On the Convergence of A Class of Adam-Type Algorithms forNon-Convex Optimization,2018, arXiv:1808
 Adam: A Method for Stochastic Optimization,2017, arXiv preprint arXiv:1412
 Rectified Linear Units Improve Restricted Boltzmann Machines,2010, In Proceedings ofthe 27th International Conference on Machine Learning
 Unified Convergence Analysis of Stochastic Momentum Methods for Convex andNon-Convex Optimization,2015, arXiv:1409
 On The Convergence of Adam and Beyond,2018, In International conference onLearning Representations (ICLR)
 Theory of Multiobjective Optimization,1985, Elsevier Science
 Very Deep Convolutional Networks for Large-Scale Image Recognition,2016, InInternational conference on Learning Representations (ICLR)
 Unified Convergence Analysis of Stochastic Momentum Methods for Convex andNon-Convex Optimization,2016, arXiv:1604
 On the Convergence of Adaptive Gradient Methods for NonconvexOptimization,2018, arXiv:1808
