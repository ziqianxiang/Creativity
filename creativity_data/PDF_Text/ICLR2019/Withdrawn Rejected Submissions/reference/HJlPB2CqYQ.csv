title,year,conference
 Optimization Algorithms on Matrix Manifolds,2007, PrincetonUniversity Press
 Information Geometry and Its Applications,2016, Springer
 Practical Gauss-Newton Optimisation forDeep Learning,2017, In International Conference on Machine Learning
 Large-scale machine learning with stochastic gradient descent,0082, In Proceedings ofCOMPSTATâ€™2010
 Optimization Methods for Large-Scale MachineLearning,1606, arXiv:1606
 Riemannian approach to batch normalization,2017, In Advances inNeural Information Processing Systems
 ParsevalNetworks: Improving Robustness to Adversarial Examples,1704, arXiv:1704
 Deep Residual Learning for ImageRecognition,1512, arXiv:1512
 Recurrent Orthogonal Networks and Long-MemoryTasks,1602, arXiv:1602
 Adam: A Method for Stochastic Optimization,1412, arXiv:1412
 Deep Neural Networks as Gaussian Processes,1711, arXiv:1711
 Projections of Probability Distributions: A Measure-Theoretic Dvoretzky The-orem,2012, In Geometric Aspects of Functional Analysis
 Inferring single-trialneural population dynamics using sequential auto-encoders,2018, Nature Methods
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,1711, arXiv:1711
 The Emergence of Spectral Univer-sality in Deep Networks,1802, arXiv:1802
 Deep Amortized Inference for ProbabilisticPrograms,1610, arXiv:1610
 Weight Normalization: A Simple Reparameterization toAccelerate Training of Deep Neural Networks,1602, arXiv:1602
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural networks,1312, arXiv:1312
 Deep InformationPropagation,1611, arXiv:1611
 Spectral Theory of Block Operator Matrices and Applications,2008, IMPERIALCOLLEGE PRESS
 On orthogonality and learningrecurrent networks with long term dependencies,2017, arXiv:1702
 Full-Capacity Unitary Recurrent Neural Networks,2016, arXiv:1611
 All You Need is Beyond a Good Init: Exploring BetterSolution for Training Extremely Deep Convolutional Neural Networks with Orthonormality andModulation,1703, arXiv:1703
