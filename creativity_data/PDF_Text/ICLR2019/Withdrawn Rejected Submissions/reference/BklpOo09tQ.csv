title,year,conference
 Towards evaluating the robustness of neural networks,2016, arXivpreprint arXiv:1608
 Analysis of classifiers robustness to adversarialperturbations,2018, Machine Learning
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 Machine vs machine: Minimax-optimal defense against adversarial examples,2018, 2018
 Learning with a strong adver-sary,2015, arXiv preprint arXiv:1511
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Magnet: a two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 Analysis of universal adversarial perturbations,2017, arXiv preprint arXiv:1705
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS workshop on deep learningand unsupervised feature learning
 Towards the science ofsecurity and privacy in machine learning,2016, arXiv preprint arXiv:1611
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Improving the adversarial robustness and in-terpretability of deep neural networks by regularizing their input gradients,2017, arXiv preprintarXiv:1711
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 One pixel attack for fooling deepneural networks,2017, arXiv preprint arXiv:1710
 Intriguing properties of neural networks,2014, 2014
 Detecting adversarial samples for deepneural networks through mutation testing,2018, arXiv preprint arXiv:1805
