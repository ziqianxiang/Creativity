title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, CoRR
 Ockham’s Razor: A Historical and Philosophical Analysis of Ockham’s Principle ofParsimony,1976, PhD thesis
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, CoRR
 Neural networks and principal component analysis: Learning from exampleswithout local minima,1989, Neural Networks
 Theloss surface of multilayer networks,2014, CoRR
 Flat minima,1997, Neural Computation
 Dogs vs,2018, cats
 Deep learning,2015, Nature
 The dynamics of learning: A random matrix approach,2018, InJennifer G
 Convergenceof gradient descent on separable data,2018, CoRR
 In search of the real inductive bias:On the role of implicit regularization in deep learning,2014, CoRR
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Isabelle Guyon
 Deep learning generalizes because theparameter-function map is biased towards simple functions,2018, CoRR
 General conditions for predic-tivity in learning theory,2004, Nature
 Svcca: Singular vec-tor canonical correlation analysis for deep understanding and improvement,2017, arXiv preprintarXiv:1706
 Learning hierarchical categories indeep neural networks,2013, In Markus Knauff
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2013, cite arxiv:1312
 The implicit bias of gradient descent on separabledata,2017, CoRR
 On orthogonality and learningrecurrent networks with long term dependencies,2017, CoRR
 Convergence of sgd in learning relu modelswith separable data,2018, CoRR
 Understandingdeep learning requires rethinking generalization,2016, CoRR
