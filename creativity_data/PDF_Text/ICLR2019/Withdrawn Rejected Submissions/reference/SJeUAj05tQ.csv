title,year,conference
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 Non-stationary stochastic optimization,2015, OperationsResearch
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Matrix analysis,1990, Cambridge university press
 Collaborative deep learning infixed topology networks,2017, In Advances in Neural Information Processing Systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Distributed online convex optimization over jointly Con-nected digraphs,2014, IEEE Transactions on Network Science and Engineering
 Adaptive bound optimization for online convex opti-mization,2010, arXiv preprint arXiv:1002
 Communication-efficientlearning of deep networks from decentralized data,2016, arXiv preprint arXiv:1602
 Online optimizationin dynamic environments: Improved regret rates for strongly convex problems,2016, In Decision andControl (CDC)
 Distributed subgradient methods for multi-agent optimiza-tion,2009, IEEE Transactions on Automatic Control
 A relaxed nonmonotone adaptive trust region methodfor solving unconstrained optimization problems,2015, Computational Optimization and Applications
 Distributed optimization in sensor networks,2004, In Proceedings ofthe 3rd international symposium on Information processing in sensor networks
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 An online optimization approach for multi-agent trackingof dynamic parameters in the presence of adversarial noise,2017, In American Control Conference(ACC)
 Distributed online optimization in dynamic environmentsusing mirror descent,2018, IEEE Transactions on Automatic Control
 Online learning and online convex optimization,2012, Foundations andTrendsR in Machine Learning
 Distributed asynchronous deterministicand stochastic gradient optimization algorithms,1986, IEEE transactions on automatic control
 On nonconvex decentralized gradient descent,2018, IEEE Transactions onSignal Processing
 Improved dynamic regret fornon-degenerate functions,2017, In Advances in Neural Information Processing Systems
 Online convex programming and generalized infinitesimal gradient ascent,2003, 2003
 Thiscompletes the proof,2019,	â–¡28Under review as a conference paper at ICLR 20196
