title,year,conference
 Information dropout: Learning optimal representationsthrough noisy computation,2018, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Deep variational informationbottleneck,2016, arXiv preprint arXiv:1612
 Learning stochastic recurrent networks,2014, arXiv preprintarXiv:1411
 Representation learning: A review and newperspectives,2013, IEEE transactions on pattern analysis and machine intelligence
 Quasi-recurrent neural net-works,2016, arXiv preprint arXiv:1611
 Understanding disentangling in Î²-VAE,2018, arXiv preprintarXiv:1804
 InfoGan:Interpretable representation learning by information maximizing generative adversarial nets,2016, InAdvances in neural information processing systems
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 A recurrent latent variable model for sequential data,2015, In Advances in neural informationprocessing systems
 Deep reinforce-ment learning with macro-actions,2016, arXiv preprint arXiv:1606
 A framework for the quantitative evaluation of disen-tangled representations,2018, 2018
 Sequential neural modelswith stochastic layers,2016, In Advances in neural information processing systems
 DRAW:A recurrent neural network for image generation,2015, arXiv preprint arXiv:1502
 beta-VAE: Learning basic visual concepts with aconstrained variational framework,2016, 2016b
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 Long short-term memory,1997, Neural computation
 Disentangling by factorising,2018, arXiv preprint arXiv:1802
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational Bayes,2013, arXiv preprintarXiv:1312
 A deeP generative model for disentangled rePresentations ofsequential data,2018, arXiv preprint arXiv:1803
 Learning hierarchical features from generativemodels,2017, arXiv preprint arXiv:1702
