title,year,conference
 A progressive batchingL-BFGS method for machine learning,2018, In Proceedings of the 35th International Conference onMachine Learning (ICML)
 Quasi-Newton methods and their application to function minimization,1967, Mathematicsof Computation
 LIBSVM: A library for support vector machines,2011, ACM Transactions onIntelligent Systems and Technology
 SAGA: a fast incremental gradient method with supportfor non-strongly convex composite objectives,2014, In Advances in Neural Information ProcessingSystems (NIPS)
 A family of variable metric updates derived by variational means,1970, Mathematics ofComputation
 Matrix Computations,2012, John Hopkins University Press
 Stochastic block BFGS: squeezing more curvature outof data,2016, In Proceedings of the 33rd International Conference on Machine Learning (ICML)
 Accelerating stochastic gradient descent using predictive variance re-duction,2013, In Advances in Neural Information Processing Systems (NIPS)
 Adam: a method for stochastic optimization,2015, In Proceedings of the 3rdinternational conference on learning representations (ICLR)
 Learning multiple layers of features from tiny images,2009, Technical report
 RES: regularized stochastic BFGS algorithm,2014, IEEE Transactions onSignal Processing
 A stochastic approximation method,1951, Annals of Mathematical Statistics
 A stochastic quasi-Newton method for online convexoptimization,2007, In Proceedings of the 11th international conference on Artificial Intelligence andStatistics (AISTATS)
 Conditioning of quasi-Newton methods for function minimization,1970, Mathematics ofComputation
 Matconvnet - convolutional neural networks for matlab,2015, In Proceeding ofthe ACM Int
 Convergence conditions for ascent methods,1969, SIAM Review
 Convergence conditions for ascent methods II: some corrections,1971, SIAM Review
