title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, In Advances inNeural Information Processing Systems
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Few-shot text classification with pre-trained word embeddingsand a human in the loop,2018, arXiv preprint arXiv:1804
 Efficient vector representation for documents through corruption,2017, arXiv preprintarXiv:1707
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 One-shot visual imitationlearning via meta-learning,2017, In Conference on Robot Learning
 Neural turing machines,2014, arXiv preprint arXiv:1410
 Teaching machines to read and comprehend,2015, In Advances in NeuralInformation Processing Systems
 Universal language model fine-tuning for text classification,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 A meta-learning approach for text categorization,2001, In Proceedings ofthe 24th annual international ACM SIGIR conference on Research and development in informationretrieval
 The effect of background knowledge on the readingcomprehension of second language learners,1985, Foreign Language Annals
 Learning to optimize neural nets,2017, arXiv preprint arXiv:1703
 A structured self-attentive sentence embedding,2017, arXiv preprint arXiv:1703
 Rectifier nonlinearities improve neural networkacoustic models,2013, In Proc
 Meta-learning with temporalconvolutions,2017, arXiv preprint arXiv:1707
 Meta networks,2017, In International Conference on MachineLearning
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Feed-forward networks with attention can solve some long-termmemory problems,2015, arXiv preprint arXiv:1512
 Optimization as a model for few-shot learning,2016, 2016
 Bidirectional recUrrent neUral networks,1997, IEEE Transactionson Signal Processing
 Bi-directional block self-attention for fast and memory-efficient seqUence modeling,2018, arXiv preprint arXiv:1804
 End-to-end memory networks,2015, In Advancesin neural information processing systems
 Lifelong learning algorithms,1998, Learning to learn
 Attention is all yoU need,2017, In Advances in Neural InformationProcessing Systems
 A perspective view and sUrvey of meta-learning,2002, ArtificialIntelligence Review
 Matching networks for one shotlearning,2016, In Advances in Neural Information Processing Systems
