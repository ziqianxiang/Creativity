title,year,conference
 Learning to learn by gradient descent bygradient descent,2016, In NIPS
 On the optimization of a synapticlearning rule,1992, In Preprints Conf
 Meta-learning and universality: Deep representations and gradientdescent can approximate any learning algorithm,2018, In ICLR
 Model-Agnostic Meta-Learning for Fast Adapta-tion of Deep Networks,2017, In ICML
 Probabilistic Model-Agnostic Meta-Learning,2018, InNIPS
 Recasting gradient-based meta-learning as hierarchical bayes,2018, In ICLR
 Adam: A method for stochastic optimization,2015, In ICLR
 Siamese neural networks for one-shotimage recognition,2015, In ICML Deep Learning Workshop
 Gradient-Based Meta-Learning with Learned Layerwise Metricand Subspace,2018, In ICML
 Learning to Optimize,2016, In ICLR
 Meta-sgd: Learning to learn quickly for few-shot learning,2017, arXiv preprint arXiv:1707
 Visualizing data using t-sne,2008, In JMLR
 A simple neural attentive meta-learner,2018, In ICLR
 Recurrent models of visualattention,2014, In NIPS
 Meta networks,2017, In ICML
 Reptile: a scalable metalearning algorithm,2018, arXiv preprintarXiv:1803
 TADAM: Task dependent adaptivemetric for improved few-shot learning,2018, In NIPS
 FiLM: VisualReasoning with a General Conditioning Layer,2017, In AAAI 2018
 Optimization as a Model for Few-Shot Learning,2016, In ICLR
 Meta-learning with memory-augmented neural networks,2016, In ICML
 Trust regionpolicy optimization,2015, In ICML
 Prototypical networks for few-shot learning,2017, InNIPS
 Attention is all you need,2017, In NIPS
 Matching networks for oneshot learning,2016, In NIPS
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 Self-attention generativeadversarial networks,2018, arXiv preprint arXiv:1805
