title,year,conference
 Designing neural network architectures usingreinforcement learning,2016, arXiv preprint arXiv:1611
 Deep learning of representations: Looking forward,2013, In International Conference on StatisticalLanguage and Speech Processing
 Smash: one-shot model architecturesearch through hypernetworks,2017, arXiv preprint arXiv:1708
 Efficient architecture search by networktransformation,2018, AAAI
 Imagenet: A large-scale hierarchicalimage database,2009, In Computer Vision and Pattern Recognition
 Long short-term memory,1997, Neural computation
 Densely connected convolutionalnetworks,2017, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Condensenet: An efficientdensenet using learned group convolutions,2018, group
 Learning multiple layers of features from tiny images,2009, Technicalreport
 Pruning filters for efficientconvnets,2016, arXiv preprint arXiv:1608
 Progressive neural architecture search,2017, arXiv preprint arXiv:1712
 Darts: Differentiable architecture search,2018, arXiv preprintarXiv:1806
 Learningefficient convolutional networks through network slimming,2017, In Computer Vision (ICCV)
 Shufflenet v2: Practical guidelines for efficientcnn architecture design,2018, arXiv preprint arXiv:1807
 Automatic differentiation in pytorch,2017, 2017
 Efficient neural architecture search viaparameter sharing,2018, arXiv preprint arXiv:1802
 Large-scale evolution of image classifiers,2017, arXiv preprint arXiv:1703
 Reinforcement learning: An introduction,1998, MIT press
 Blockdrop: Dynamic inference paths in residual networks,2018, 2018
 Genetic cnn,2017, In ICCV
 Shufflenet: An extremely efficient convolutional neural network formobile devices,2017, arxiv 2017
 Practical network blocks design with q-learning,2017, arXiv preprintarXiv:1708
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
 Learning transferable architectures forscalable image recognition,2017, arXiv preprint arXiv:1707
2	Training configurationsTraining configurations for CIFAR,1000, Based on the pretrained DenseNet
