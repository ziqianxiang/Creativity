title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Conditional computation inneural networks for faster models,2015, arXiv preprint arXiv:1511
 Strategies for training large vocabulary neurallanguage models,2015, arXiv preprint arXiv:1512
 A note on the group lasso and a sparse grouplasso,2010, arXiv preprint arXiv:1001
 Learning to forget: Continual predictionwith lstm,1999, 1999
 Classes for fast maximum entropy training,2001, In Acoustics
 Efficientsoftmax approximation for gpus,2016, arXiv preprint arXiv:1609
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Structuredoutput layer neural network language model,2011, In Acoustics
 Deep learning,2015, nature
 Lsh softmax: Sub-linear learning and inference of thesoftmax layer in deep architectures,2018, 2018
 Casia online and offline chinesehandwriting databases,2011, In Document Analysis and Recognition (ICDAR)
 Stanford neural machine translation systems forspoken language domain,2015, In International Workshop on Spoken Language Translation
 A* sampling,2014, In Advances in Neural InformationProcessing Systems
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Extensionsof recurrent neural network language model,2011, In Acoustics
 A scalable hierarchical distributed language model,2009, In Advancesin neural information processing systems
 Hierarchical probabilistic neural network language model,2005, InAistats
 Fast amortized inference and learning in log-linear models with randomly perturbed nearest neighbor search,2017, arXiv preprint arXiv:1707
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Group sparse regular-ization for deep neural networks,2017, Neurocomputing
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Svd-softmax: Fastsoftmax approximation on large vocabulary neural networks,2017, In Advances in Neural InformationProcessing Systems
 A new unbiased and efficient class of lsh-based sam-plers and estimators for partition function computation in log-linear models,2017, arXiv preprintarXiv:1703
 Topic modeling: beyond bag-of-words,2006, In Proceedings of the 23rd internationalconference on Machine learning
 Learning structured sparsity indeep neural networks,2016, In Advances in Neural Information Processing Systems
 Navigating withgraph representations for fast and scalable decoding of neural language models,2018, arXiv preprintarXiv:1806
