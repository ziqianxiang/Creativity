title,year,conference
 Multimodal recurrentneural networks with information transfer layers for indoor scene labeling,2018, IEEE Transactions onMultimedia
 Ensemble methods in machine learning,2000, In International workshop on multipleclassifier systems
 Deep Learning,2016, MIT Press
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Paraphrasing complex network: Network compres-sion via factor transfer,2018, arXiv preprint arXiv:1802
 Efficient neural architecturesearch via parameter sharing,2018, arXiv preprint arXiv:1802
 Data distillation:Towards omni-supervised learning,2017, arXiv preprint arXiv:1712
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Transparent model distillation,2018, arXivpreprint arXiv:1801
 Recurrent neural network training with dark knowl-edge transfer,2016, In Acoustics
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Aggregated residualtransformations for deep neural networks,2017, In Computer Vision and Pattern Recognition (CVPR)
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Deep mutual learning,2017, arXivpreprint arXiv:1706
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
