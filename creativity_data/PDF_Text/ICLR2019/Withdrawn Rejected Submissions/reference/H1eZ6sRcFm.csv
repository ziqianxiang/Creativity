title,year,conference
 Neural machine translation by jointlylearning to align and translate,0147, In International Conference on Learning Representations (ICLR)
 Semi-supervised sequence learning,2015, In Conference on NeuralInformation Processing Systems (NIPS)
 Deep Unsupervised Clustering with Gaussian MixtureVariational Autoencoders,2016, 2016
 Multi-task learning for multiple language translation,2015, 2015
 Learning distributed representations of sentencesfrom unlabelled data,2016, 2016
 Categorical Reparameterization with Gumbel-Softmax,2016, InInternational Conference on Learning Representations (ICLR)
 Discrete Autoencoders for Sequence Models,2018, 2018
 Fast Decoding in Sequence Models using Discrete Latent Variables,2018, In InternationalConference on Machine Learning (ICML)
 Adam: A Method for Stochastic Optimization,2014, In InternationalConference on Learning Representations (ICLR)
 Auto-Encoding Variational Bayes,2013, In International Conferenceon Learning Representations (ICLR)
 A structured self-attentive sentence embedding,2017, In International Conference on LearningRepresentations (ICLR)
 Effective Approaches to Attention-based Neural Machine Translation,2015, In Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Neural Variational Inference for Text Processing,2015, InInternational Conference on Machine Learning (ICML)
 Recurrentneural network based language model,2010, In Conference of the International Speech CommunicationAssociation (INTERSPEECH)
 A Hybrid Convolutional VariationalAutoencoder for Text Generation,2017, In Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 VAE with a VampPrior,2017, In International Conference onArtificial Intelligence and Statistics (AISTATS)
 Multitask Learning with Low-LevelAuxiliary Tasks for Encoder-Decoder Based Speech Recognition,2017, 2017
 Neural Discrete RepresentationLearning,2017, In Conference on Neural Information Processing Systems (NIPS)
 Attention Is All You Need,2017, In Conference on Neural InformationProcessing Systems (NIPS)
 A Learning Algorithm for Continually Running Fully RecurrentNeural Networks,1989, NeuralComputation
 DataNoising as Smoothing in Neural Network Language Models,2017, In International Conference onLearning Representations (ICLR)
 Improved VariationalAutoencoders for Text Modeling using Dilated Convolutions,2017, In International Conference onMachine Learning (ICML)
 InfoVAE: Information Maximizing VariationalAutoencoders,2017, 2017
 Thesize of the hidden state of LSTM is 256 for both for LSTM and self-attention,2017, The size of the wordembedding is 256 and the size of the latent variables is 128
