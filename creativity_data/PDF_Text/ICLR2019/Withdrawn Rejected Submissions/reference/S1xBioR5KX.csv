title,year,conference
 Deep Rewiring: Trainingvery sparse deep networks,2017, nov 2017
 SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,2017, oct 2017
 CompressingNeural Networks with the Hashing Trick,2015, apr 2015
 The LossSurfaces of Multilayer Networks,2014, nov 2014
 The loss landscape of overparameterized neural networks,2018, apr 2018
 BinarizedNeural Networks: Training Deep Neural Networks with Weights and Activations Constrained to+1 or -1,2016, feb 2016
 NeST: A Neural Network Synthesis Tool Based on aGrow-and-Prune Paradigm,2017, pp
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, arXiv
 PredictingParameters in Deep Learning,2013, jun 2013
 Sharp Minima Can GeneralizeFor Deep Nets,1938, 2017
 Qualitatively characterizing neural networkoptimization problems,2014, dec 2014
 Learning both Weights and Connections forEfficient Neural Networks,2015, jun 2015
 Deep Residual Learning for ImageRecognition,1664, Arxiv
 Channel Pruning for Accelerating Very Deep NeuralNetworks,2017, jul 2017
 MobileNets: Efficient Convolutional Neural Networks forMobile Vision Applications,2017, apr 2017
 An empirical analysis of the optimization ofdeep network loss surfaces,2016, dec 2016
 Speeding up Convolutional Neural Networkswith Low Rank Expansions,2014, may 2014
 Flexpoint: An Adaptive Numerical Format for Efficient Training of DeepNeural Networks,2017, nov 2017
 ImageNet Classification with DeepConvolutional Neural Networks,2012, Technical report
 Fast ConvNets Using Group-wise Brain Damage,2015, jun 2015
 Gradient-based learning applied todocument recognition,1998, Proceedings ofthe IEEE
 Measuring the Intrinsic Dimensionof Objective Landscapes,2018, apr 2018
 Pruning Filters forEfficient ConvNets,2016, aug 2016
 Theory of Deep Learning II: Landscape of the Empirical Risk inDeep Learning,2017, arXiv
 LearningEfficient Convolutional Networks through Network Slimming,2017, aug 2017
 Rethinking the Value ofNetwork Pruning,2018, oct 2018
 ThiNet: A Filter Level Pruning Method for Deep NeuralNetwork Compression,2017, jul 2017
 Recovering fromRandom Pruning: On the Plasticity of Deep Convolutional Neural Networks,2018, jan 2018
 ACDC: A StructuredEfficient Linear Layer,2015, nov 2015
 Exploring Sparsity in RecurrentNeural Networks,2017, apr 2017
 Theory of Deep Learning III: explaining the non-overfittingpuzzle,2017, 2017
 Weight Normalization: A Simple Reparameterization toAccelerate Training of Deep Neural Networks,2016, feb 2016
 Very Deep Convolutional Networks for Large-Scale ImageRecognition,2014, sep 2014
 Structured Transforms for Small-FootprintDeep Learning,2015, oct 2015
 Network Compression using CorrelationAnalysis of Layer Responses,2018, jul 2018
 Going Deeper with Convolutions,2014, sep 2014
 Learning invariance withcompact transforms,2018, pp
 Low-Cost Parameteri-zations of Deep Convolution Neural Networks,2018, may 2018
 LearningStructured Sparsity in Deep Neural Networks,0028, 2016
 Deep Fried Convnets,2014, dec 2014
 Wide Residual Networks,2016, may 2016
 Theory of Deep Learning IIb: Optimization Properties of SGD,2018, jan 2018a
