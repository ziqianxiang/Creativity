title,year,conference
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks ofthe trade
 The effects of hyperparameters on sgd training of neural networks,2015, arXiv preprintarXiv:1508
 Sample size selection in opti-mization methods for machine learning,2012, Mathematical programming
 Big batch sgd: Automated inferenceusing adaptive batch sizes,2016, arXiv preprint arXiv:1610
 Extracting certainty from uncertainty: Regret bounded by variation incosts,2010, Machine learning
 Non-stochastic best arm identification and hyperparameteroptimization,2016, In Artificial Intelligence and Statistics
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Systematic evaluation of convolution neuralnetwork advances on the imagenet,2017, Computer Vision and Image Understanding
 Online learning and online convex optimization,2012, Foundations andTrendsR in Machine Learning
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 The general inefficiency of batch training for gradientdescent learning,2003, Neural Networks
