title,year,conference
 Advances in optimizing re-current networks,2013, In IEEE International Conference on Acoustics
 Largescale distributed deep networks,2012, In Advances in Neural Information Processing Systems 25: 26thAnnual Conference on Neural Information Processing Systems
 AdaBatch: Adaptive batch sizes fortraining deep neural networks,2017, CoRR
 Incorporating Nesterov momentum into Adam,2016, ICLR Workshop
 HyPerParameter oPtimization: a sPectral aPProach,2018, InInternational Conference on Learning Representations
 DeeP residual learning for image recog-nition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Learning curve Prediction with Bayesianneural networks,2017, In International Conference on Learning Representations
 Scaling distributed machine learning with theParameter server,2014, In OSDI
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Asynchronous decentralized parallel stochasticgradient descent,2018, In Proceedings of the 35th International Conference on Machine Learning
 Deep gradient compression: Reducingthe communication bandwidth for distributed training,2018, In International Conference on LearningRepresentations
 Linearly convergent stochastic heavy ball method for minimiz-ing generalization error,2017, CoRR
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in Neural Information Processing Systems
 On the importance of ini-tialization and momentum in deep learning,2013, In Proceedings of the 30th International Conferenceon Machine Learning
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
 Petuum: A new platform for distributed machinelearning on big data,2015, IEEE Transactions on Big Data
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference
 Hogwild++: A new mechanism for decentral-ized asynchronous stochastic gradient descent,2016, In IEEE 16th International Conference on DataMining
