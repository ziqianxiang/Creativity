title,year,conference
 A closer look at memorization in deep netWorks,2017, arXiv preprintarXiv:1706
 Identifying and eliminating mislabeled training in-stances,1996, In Proceedings ofthe National Conference OnArtificial Intelligence
 Entropy-SGD: Biasing gradi-ent descent into Wide valleys,2016, arXiv preprint arXiv:1611
 Sharp minima can generalize for deep nets,2017, arXivpreprint arXiv:1703
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, arXiv preprint arXiv:1806
 Robust loss functions under label noise for deepneural netWorks,2017, In AAAI
 Deep residual learning for image recog-nition,2015, arXiv preprint arXiv:1512
 Delving deep into rectifiers: SurpassingHuman-Level performance on ImageNet classification,2015, arXiv preprint arXiv:1502
 SIMPLIFYING NEURAL NETS BY DISCOVERINGFLAT MINIMA,1995, In G Tesauro
 MentorNet: LearningData-Driven curriculum for very deep neural networks on corrupted labels,2017, arXiv preprintarXiv:1712
 On Large-Batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2017, arXiv preprint arXiv:1712
 SGDR: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Stochastic gradient descent as approximatebayesian inference,2017, arXiv preprint arXiv:1704
 Implicit regularization in deep learning,2017, arXiv preprint arXiv:1709
 Learning with confident examples: Rankpruning for robust classification with noisy labels,2017, arXiv preprint arXiv:1705
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, arXiv preprint arXiv:1707
 On the theory of the brownian motion,1930, Physicalreview
 Learn-ing from noisy Large-Scale datasets with minimal supervision,2017, In CVPR
 Learning from massive noisylabeled data for image classification,2015, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Wide residual networks,2016, arXiv preprintarXiv:1605
 mixup: Beyond empiricalrisk minimization,2017, October 2017
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, arXiv preprint arXiv:1805
42 Â± 0,2019,20	23
