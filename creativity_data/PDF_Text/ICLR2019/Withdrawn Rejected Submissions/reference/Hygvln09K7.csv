title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, NIPS
 Fac-tors of transferability for a generic convnet representation,2016, TPAMI
 Theoretical models of learning to learn,1998, Learning to learn
 The evolution of color vision in insects,2001, Annual review ofentomology
 Learning to learn without gradient descent by gradientdescent,2017, ICML
 Rl 2: Fastreinforcement learning via slow reinforcement learning,2017, arxiv
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, ICML
 One-shot visual imita-tion learning via meta-learning,2017, CoLR
 Low-shot visual recognition by shrinking and hallucinatingfeatures,2017, ICCV
 The formation of learning sets,1949, Psychological review
 Deep residual learning for image recog-nition,2016, CVPR
 Learning to learn using gradient descent,2001, InArtificial Neural Networks ICANN
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, ICML
 Adam: A method for stochastic optimization,2014, ICLR
 Siamese neural networks for one-shotimage recognition,2015, ICML Workshop
 Imagenet classification with deep convo-lutional neural networks,2012, In NIPS
 One shot learningof simple visual concepts,2011, In CogSci
 Learning to optimize,2017, ICLR
 Meta-learning with temporalconvolutions,2017, arxiv
 Meta networks,2017, ICML
 Optimization as a model for few-shot learning,2017, ICLR
 One-shot learning with memory-augmented neural networks,2016, ICML
 A neural network that embeds its own meta-levels,1993, ICNN
 Prototypical networks for few-shot learning,2017, NIPS
 A perspective view and survey of meta-learning,2002, ArtificialIntelligence Review
 Match-ing networks for one shot learning,2016, NIPS
 Learning to reinforcement learn,2017, arxiv
 Learned optimizers that scale andgeneralize,2017, ICML
 Active one-shot learning,2017, NIPS Deep Reinforcement LearningWorkshop
