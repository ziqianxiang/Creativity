title,year,conference
 Singularities affect dynamics of learning in neuromanifolds,2006, Neuralcomputation
 Layer normalization,2016, arXiv preprint arXiv:1607
 Universal approximation bounds for superpositions of a sigmoidal function,1993, IEEETransactions on Information Theory
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Gradient descent learns one-hidden-layer cnn:Donâ€™t be afraid of spurious local minima,2017, arXiv preprint arXiv:1712
 Local minima and plateaus in hierarchical structures of multilayerperceptrons,2000, Neural networks
 How to start training: The effect of initialization and architecture,2018, arXivpreprint arXiv:1803
 Approximating continuous functions by relu nets of minimal width,2017, arXivpreprint arXiv:1710
 Deep residual learning for image recognition,2016, In IEEEConference on Computer Vision and Pattern Recognition
 Multilayer feedforward networks are universal approxi-mators,1989, Neural Networks
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In International Conference on Machine Learning
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2015, In International Conferenceon Learning Representations
 Self-normalizing neural networks,2017, InAdvances in Neural Information Processing Systems
 Efficient backprop,1998, In Neural networks: Tricksof the trade
 Rectifier nonlinearities improve neural network acousticmodels,2013, In International Conference on Machine Learning
 Neural networks for optimal approximation of smooth and analytic functions,1996, NeuralComputation
 All you need is a good init,2016, In International Conference on LearningRepresentations
 Neural networks should be wide enough to learn discon-nected decision regions,2018, In International Conference on Machine Learning
 Numerical Optimization,2006, Springer
 Optimal approximation of piecewise smooth functions using deeprelu neural networks,2018, In Conference on Learning Theory
 Exponential expressivity in deepneural networks through transient chaos,2016, In Advances in Neural Information Processing Systems
 Exact solutions to the nonlinear dynamics of learningin deep linear neural networks,2014, In International Conference on Learning Representations
 Training a single sigmoidal neuron is hard,2002, Neural computation
 Training very deep networks,2015, In Advances in NeuralInformation Processing Systems
 On the importance of initialization and momentumin deep learning,2013, In International Conference on Machine Learning
 Group normalization,2018, arXiv preprint arXiv:1803
 Error bounds for approximations with deep relu networks,2017, Neural Networks
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
