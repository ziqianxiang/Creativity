title,year,conference
 Theloss surfaces of multilayer networks,2015, In Proceedings of the Eighteenth International Conferenceon Artificial Intelligence and Statistics
 Deep Learning,2016, MIT Press
 Dynamic network surgery for efficient DNNs,2016, InAdvances in Neural Information Processing Systems (NIPS)
 Network sketching: exploiting binary struc-ture in deep CNNs,2017, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Effectivequantization methods for recurrent neural networks,2016, CoRR
 Distilling the knowledge in a neural network,2015, InNIPS Deep Learning and Representation Learning Workshop
 Simplifying neural nets by discovering flat minima,1995, InAdvances in Neural Information Processing Systems (NIPS)
 Long short-term memory,1997, Neural Comput
 Optimal brain damage,1990, In Advances in NeuralInformation Processing Systems (NIPS)
 Deep learning,2015, Nature
 Viterbi-based pruningfor sparse matrix with fixed and high index compression ratio,2018, In International Conference onLearning Representations (ICLR)
 Building a large annotatedcorpus of English: The Penn Treebank,1993, Comput
 Variational dropout sparsifies deepneural networks,2017, In International Conference on Machine Learning (ICML)
 Exploring sparsity in recurrentneural networks,2017, In International Conference on Learning Representations (ICLR)
 Model compression via distillation and quanti-zation,2018, In International Conference on Learning Representations (ICLR)
 On the compression ofrecurrent neural networks with an application to LVCSR acoustic modeling for embedded speechrecognition,2016, In ICASSP
 XNOR-Net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 Regularization of neuralnetworks using DropConnect,2013, In International Conference on Machine Learning (ICML)
 Training and inference with integers in deepneural networks,2018, In International Conference on Learning Representations (ICLR)
 Alternating multi-bit quantization for recurrent neural networks,2018, In International Conferenceon Learning Representations (ICLR)
 DoReFa-Net: train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv:1606
 Trained ternary quantization,2017, InInternational Conference on Learning Representations (ICLR)
