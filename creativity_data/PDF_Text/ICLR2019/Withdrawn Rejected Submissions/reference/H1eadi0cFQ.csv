title,year,conference
 Efficient approaches for escaping higher order saddle pointsin non-convex optimization,2016, In Conference on Learning Theory
 Convexneural networks,2006, In Advances in neural information processing systems
 Boosting for high-dimensional linear models,2006, The Annals of Statistics
 Path-level network transformationfor efficient architecture search,2018, arXiv preprint arXiv:1806
 Net2net: Accelerating learning via knowledgetransfer,2015, arXiv preprint arXiv:1511
 Trust region methods,2000, SIAM
 Local minima and plateaUs in hierarchical strUctUres ofmUltilayer perceptrons,2000, Neural networks
 Escaping from saddle pointsâ€”online stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 How to escape saddlepoints efficiently,2017, arXiv preprint arXiv:1703
 Deep learning withoUt poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 SUb-sampled cUbic regUlarization for non-convex opti-mization,2017, In International Conference on Machine Learning
 Compnet: NeUral networks growing via the compact networkmorphism,2018, arXiv preprint arXiv:1804
 A generic approach for escaping saddle points,2017, arXiv preprintarXiv:1709
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 First-order stochastic algorithms for escaping from saddle points in almostlinear time,2017, arXiv preprint arXiv:1711
 ADADELTA: An Adaptive Learning Rate Method,2012, arXiv
