title,year,conference
 Natural gradient works efficiently in learning,1998, Neural COmputatiOn
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in Neural InfOrmatiOn PrOcessing Systems (NIPS)
 Onlinelearning rate adaptation with hypergradient descent,2017, arXiv preprint arXiv:1703
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural NetwOrks: Tricks Of the Trade
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in Neural InfOrmatiOn PrOcessing Systems (NIPS)
 Sharp minima can generalizefor deep nets,2017, arXiv preprint arXiv:1703
 DeeP residual learning for image recog-nition,2016, In COnference On COmputer VisiOn and Pattern RecOgnitiOn (CVPR)
 Norm matters: Efficient and accuratenormalization schemes in deeP networks,2018, arXiv preprint arXiv:1803
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 PoPulation-based train-ing of neural networks,2017, arXiv preprint arXiv:1711
 Adam: A method for stochastic oPtimization,2015, In InternatiOnalCOnference On Learning RepresentatiOns (ICLR)
 Learning to oPtimize,2016, arXiv preprint arXiv:1606
 Learning to oPtimize neural nets,2017, arXiv preprint arXiv:1703
 HyPer-band: Bandit-based configuration evaluation for hyPerParameter oPtimization,2016, arXiv preprintarXiv:1603
 Stochastic gradient descent as aPProximateBayesian inference,2017, The JOurnal Of Machine Learning Research
 Optimizing neural networks with Kronecker-factored approxi-mate curvature,2015, In International Conference on Machine Learning (ICML)
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Reflections on random kitchen sinks,2017, 2017
 Local gain adaptation in stochastic gradient descent,1999, In Ninth InternationalConference on Artificial Neural Networks (ICANN)
 Trust regionpolicy optimization,2015, In International Conference on Machine Learning (ICML)
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Practical Bayesian optimization of machinelearning algorithms,2012, In Advances in Neural Information Processing Systems (NIPS)
 Scalable Bayesian optimization using deepneural networks,2015, In International Conference on Machine Learning (ICML)
 On the importance of initializa-tion and momentum in deep learning,2013, In International Conference on Machine Learning (ICML)
 Freeze-thaw Bayesian optimization,2014, arXivpreprint arXiv:1406
 Lecture 6,2012,5â€”RMSprop: Divide the gradient by a runningaverage of its recent magnitude
 Understanding short-horizon bias instochastic meta-optimization,2018, In International Conference on Learning Representations (ICLR)
 Fashion-MNIST: A novel image dataset for bench-marking machine learning algorithms,2017, arXiv preprint arXiv:1708
