title,year,conference
 TensorFlow : Large-Scale Machine Learning on HeterogeneousDistributed Systems,2016, arXiv preprint arXiv:1603
 Analyzing the Performance of Multilayer NeuralNetworks for Object Recognition,2014, In ECCV
 Unitary Evolution Recurrent Neural Networks,2016, InICML
 A Closer Look at Memorization in Deep Networks,2017, In ICML
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 ImageNet: A Large-ScaleHierarchical Image Database,2009, In CVPR
 DeCAF : A Deep Convolutional Activation Feature for Generic Visual Recognition,2014, InProceedings of the 31st International Conference on Machine Learning
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In AISTATS
 Beyond convexity: Stochastic quasi-convex opti-mization,2015, In NIPS
 Deep Residual Learning for ImageRecognition,2016, In CVPR
 The vanishing gradient problem during learning recurrent neural nets and problemsolutions,1998, IJUFKS
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, arXiv:1803
 Binarized Neural Networks,2016, InNIPS
 Three Factors Influencing Minima in SGD,2017, arXiv:1711
 Adam: A method for stochastic optimization,2015, In ICLR
 Gradient-based learning applied todocument recognition,0018, Proceedings of the IEEE
 On the difficulty of training recurrent neuralnetworks,2013, In ICML
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,1365, In NIPS
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2014, In ICLR
 Layer-specificadaptive learning rates for deep networks,2015, In ICMLA
 Super-Convergence: Very Fast Training of Residual NetworksUsing Large Learning Rates,2017, arXiv:1708
 A bayesian perspective on generalization and stochastic gradientdescent,2017, In Proceedings of Second workshop on Bayesian Deep Learning (NIPS 2017)
 Lecture 6,2012,5RmsProp: Divide the gradient by a runningaverage of its recent magnitude
 TheMarginal Value of Adaptive Gradient Methods in Machine Learning,2017, In NIPS
 Normalized gradient withadaptive stepsize method for deep neural network training,2017, arXiv:1707
 Wide Residual Networks,2016, In BMVC
 Understandingdeep learning requires re-thinking generalization,2017, In ICLR
