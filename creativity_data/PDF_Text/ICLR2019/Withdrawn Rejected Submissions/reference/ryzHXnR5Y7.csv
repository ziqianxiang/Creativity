title,year,conference
 Variancereduction in sgd by distributed importance sampling,2015, arXiv preprint arXiv:1511
 Online variance reduction for stochastic optimiza-tion,2018, arXiv preprint arXiv:1802
 Bayesian coreset construction via greedy iterative geodesicascent,2018, arXiv preprint arXiv:1802
 Active bias: Training more accu-rate neural networks by emphasizing high variance samples,2017, In Advances in Neural InformationProcessing Systems
 Very deep convolutional net-works for text classification,2016, arXiv preprint arXiv:1606
 Very deep convolutional net-works for text classification,2017, In Proceedings of the 15th Conference of the European Chapterof the Association for Computational Linguistics: Volume 1
 Language modeling with gatedconvolutional networks,2017, In Doina Precup and Yee Whye Teh (eds
 Learning to teach,2018, In InternationalConference on Learning Representations
 The unreasonable effectiveness of data,2009, IEEEIntelligent Systems
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Deep networks withstochastic depth,2016, In European Conference on Computer Vision
 Densely connectedconvolutional networks,2017, In CVPR
 Coresets for scalable bayesian logisticregression,2016, In Advances in Neural Information Processing Systems
 Bag of tricks for efficienttext classification,2016, arXiv preprint arXiv:1607
 Exploring thelimits of language modeling,2016, arXiv preprint arXiv:1602
 Biased importance sampling for deep neural networktraining,2017, arXiv preprint arXiv:1706
 Not all samples are created equal: Deep learning withimportance sampling,2018, arXiv preprint arXiv:1803
 Curriculum learning and minibatch bucketing in neural machinetranslation,2017, In Proceedings of the International Conference Recent Advances in Natural LanguageProcessing
 Heterogeneous uncertainty sampling for supervised learning,1994, InMachine Learning Proceedings 1994
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS workshop on deep learningand unsupervised feature learning
 Active learning for convolutional neural networks: A core-setapproach,2018, 2018
 From theories to queries: Active learning in practice,2011, In Isabelle Guyon
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In Computer Vision (ICCV)
 Learning mixtures ofsubmodular functions for image collection summarization,2014, In Advances in neural informationprocessing systems
 Dynamic data selection for neuralmachine translation,2017, In Proceedings of the 2017 Conference on Empirical Methods in NaturalLanguage Processing
 Dynamic sentence sampling for efficient trainingof neural machine translation,2018, arXiv preprint arXiv:1805
 Data dropout: Optimizing training data for convolutionalneural networks,2018, arXiv preprint arXiv:1809
 Using document summarization techniquesfor speech data subset selection,2013, In Proceedings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Submodular subset selectionfor large-scale speech training data,2014, In Acoustics
 Aggregated residual trans-formations for deep neural networks,2017, In Computer Vision and Pattern Recognition (CVPR)
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
 Stochastic optimization with importance sampling for regularized lossminimization,2015, In international conference on machine learning
