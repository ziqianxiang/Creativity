title,year,conference
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 Learning multiple layers of features from tiny images,2009, 2009
 Deep gradient compression: Re-ducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Sparse binarycompression: Towards distributed deep learning with minimal communication,2018, arXiv preprintarXiv:1805
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 A work-efficient step-efficient prefixsum algorithm,2006, In Workshop on edge computing using new commodity architectures
 Scan primitives for gpu com-puting,2007, In Graphics hardware
 Scalable distributed dnn training using commodity gpu cloud computing,2015, In SixteenthAnnual Conference of the International Speech Communication Association
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
