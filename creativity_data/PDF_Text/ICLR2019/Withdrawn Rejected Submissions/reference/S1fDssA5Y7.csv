title,year,conference
 Localized rademacher complexities,2002, InInternational Conference on Computational Learning Theory
 Vicinal risk minimization,2001, InAdvances in neural information processing systems
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 Sharp minima can generalizefor deep nets,2017, arXiv preprint arXiv:1703
 Escaping from saddle pointsonline stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Fairness withoutdemographics in repeated loss minimization,2018, arXiv preprint arXiv:1806
 Densely connectedconvolutional networks,2017, In CVPR
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Deeply-supervised nets,2015, In Artificial Intelligence and Statistics
 On the method of bounded differences,1989, Surveys in combinatorics
 Variance-based regularization with convex objectives,2017, InAdvances in Neural Information Processing Systems
 Stochastic gradient descent for non-smooth optimization: Conver-gence results and optimal averaging schemes,2013, In International Conference on Machine Learning
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
