title,year,conference
 Natural gradient works efficiently in learning,1998, Neural Computation
 Distributed second-order optimization usingkronecker-factored approximations,2017, In International Conference on Learning Representations
 Neural networks and principal component analysis: Learning fromexamples without local minima,1989, Neural networks
 Complex-valued autoencoders,2012, Neural Networks
 Open problem: The landscape of the losssurfaces of multilayer networks,2015, In Conference on Learning Theory
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, In International Conference on Machine Learning
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations
 New insights and perspectives on the natural gradient method,2014, arXiv:1412
 Fast large-scale optimization by unifyingstochastic gradient and quasi-newton methods,2014, In International Conference on Machine Learning
 A differential equation for modeling nesterovsaccelerated gradient method: Theory and insights,2014, In Advances in Neural Information ProcessingSystems
