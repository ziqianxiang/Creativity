title,year,conference
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International conference on machine learning
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Learning Multiple Layers of Features from Tiny Images,2009, Technical report
 Efficient backprop,1998, InNeural networks: Tricks of the trade
 Batch size matters: A diffusion approx-imation frameWork on nonconvex stochastic gradient descent,2017, arXiv preprint arXiv:1705
 Stochastic modified equations and adaptive stochastic gradient algo-rithms,2015, arXiv preprint arXiv:1511
 Stochastic gradient descent as approximatebayesian inference,2017, arXiv preprint arXiv:1704
 Empirical analysis ofthe hessian of over-parametrized neural netWorks,2017, arXiv preprint arXiv:1706
 Opening the black box of deep neural netWorks via informa-tion,2017, arXiv preprint arXiv:1703
 Very deep convolutional netWorks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The regularization effects ofanisotropic noise in stochastic gradient descent,2018, arXiv preprint arXiv:1803
