title,year,conference
 Character-level lan-guage modeling with deeper self-attention,2018, arXiv preprint arXiv:1808
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 An empirical evaluation of generic convolutionaland recurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Hierarchical multiscale recurrent neural net-works,2016, arXiv preprint arXiv:1609
 Language modeling with gatedconvolutional networks,2016, arXiv preprint arXiv:1612
 Topicrnn: A recurrent neural networkwith long-range semantic dependency,2016, arXiv preprint arXiv:1611
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in neural information processing systems
 Efficientsoftmax approximation for gpus,2016, arXiv preprint arXiv:1609
 Improving neural language models with acontinuous cache,2016, arXiv preprint arXiv:1612
 Generating sequences with recurrent neural networks,2013, arXiv preprintarXiv:1308
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Long short-term memory,1997, Neural computation
 An improved relative self-attention mech-anism for transformer with application to music generation,2018, arXiv preprint arXiv:1809
 Tying word vectors and word classifiers: Aloss framework for language modeling,2016, arXiv preprint arXiv:1611
 Document contextlanguage models,2015, arXiv preprint arXiv:1511
 Exploring thelimits of language modeling,2016, arXiv preprint arXiv:1602
 Neural machine translation in linear time,2016, arXiv preprint arXiv:1610
 Sigsoftmax: Reanalysisof the softmax bottleneck,2018, arXiv preprint arXiv:1805
 A clockwork rnn,2014, arXivpreprint arXiv:1402
 Factorization tricks for lstm networks,2017, arXiv preprintarXiv:1703
 A simple way to initialize recurrent networksof rectified linear units,2015, arXiv preprint arXiv:1504
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Pushing the bounds of dropout,2018, arXiv preprint arXiv:1805
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Regularizing and optimizing lstm lan-guage models,2017, arXiv preprint arXiv:1708
 An analysis of neural language modelingat multiple scales,2018, arXiv preprint arXiv:1803
 Recurrentneural network based language model,2010, In Eleventh Annual Conference of the International SpeechCommunication Association
 Hierarchical probabilistic neUral network langUage model,2005, InAistats
 Fast-slow recUrrent neUral networks,2017, In Advancesin Neural Information Processing Systems
 Understanding the exploding gradient prob-lem,2012, CoRR
 Efficient neural architecturesearch via parameter sharing,2018, arXiv preprint arXiv:1802
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 Self-attention with relative position representa-tions,2018, arXiv preprint arXiv:1803
 Skip-gram language modeling using sparsenon-negative matrix probability estimation,2014, arXiv preprint arXiv:1412
 Learning longer-term dependenciesin rnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 Larger-context language modelling,2015, arXiv preprintarXiv:1511
 Topic compositional neural language model,2017, arXiv preprint arXiv:1712
 Memory networks,2014, arXiv preprintarXiv:1410
 On multi-plicative integration with recurrent neural networks,2016, In Advances in neural information processingsystems
 Breaking the softmaxbottleneck: A high-rank rnn language model,2017, arXiv preprint arXiv:1711
 Recurrenthighway networks,2016, arXiv preprint arXiv:1607
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
