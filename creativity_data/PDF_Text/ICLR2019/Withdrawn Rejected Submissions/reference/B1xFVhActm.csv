title,year,conference
 Cross-view training for semi-supervised learning,2018, 2018
 Super-vised learning of universal sentence representations from natural language inference data,2017, arXivpreprint arXiv:1705
 Whatyou can cram into a single vector: Probing sentence embeddings for linguistic properties,2018, arXivpreprint arXiv:1805
 Generative adversarial nets,2014, In Z
 Learning distributed representations of sentencesfrom unlabelled data,2016, arXiv preprint arXiv:1602
 Skip-thought vectors,2015, In Advances in neural information processingsystems
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 An efficient framework for learning sentence representa-tions,2018, arXiv preprint arXiv:1803
 Encoding sentences with graph convolutional networks forsemantic role labeling,2017, arXiv preprint arXiv:1703
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Towards universal paraphrasticsentence embeddings,2015, arXiv preprint arXiv:1511
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
