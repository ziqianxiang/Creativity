title,year,conference
 Tensorflow: A system for large-scale machine learning,2016, In OSDI
 Stop wasting my gradients: Practical svrg,2015, arXiv preprint arXiv:1511
 signsgd:compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 On the convergence ofa class of adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 Automated inference with adaptivebatches,2017, In Artificial Intelligence and Statistics
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing systems
 Stronger baselines for trustable results in neural machinetranslation,2017, arXiv preprint arXiv:1706
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Draw: Arecurrent neural network for image generation,2015, arXiv preprint arXiv:1502
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2017, arXiv preprint arXiv:1711
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing Systems
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 On the insufficiency of ex-isting momentum schemes for stochastic optimization,2018, In International Conference on LearningRepresentations
 Adam: A method for stochastic optimization,2014, arxiv
 Skip-thought vectors,2015, In Advances in neural information processingsystems
 Training deep autoencoders for collaborative filtering,2017, arXivpreprint arXiv:1708
 On the convergence of stochastic gradient descent with adaptivestepsizes,2018, arXiv preprint arXiv:1805
 Aggregated momentum: Stability through passivedamping,2018, arXiv preprint arXiv:1804
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Behavior of accelerated gradient methods near critical pointsof nonconvex problems,2017, arXiv preprint arXiv:1706
 Momentum and optimal stochastic search,1994, In Proceedings ofthe 1993 Connectionist Models Summer School
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, arXiv preprint arXiv:1511
 Critical points of an autoencoder can provably recover sparsely usedovercomplete dictionaries,2017, arXiv preprint arXiv:1708
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2016, arXiv preprint arXiv:1604
 Adaptive meth-ods for nonconvex optimization,2018, In Advances in Neural Information Processing Systems
 On the convergence ofadaptive gradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
