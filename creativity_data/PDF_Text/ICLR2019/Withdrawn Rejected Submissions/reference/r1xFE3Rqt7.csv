title,year,conference
 Variational networkquantization,2018, In International Conference on Learning Representations
 XcePtion: DeeP learning with depthwise separable convolutions,2016, arXiv preprint
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 Long short-term memory,1997, Neural computation
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Squeeze-and-excitation networks,2017, arXiv preprint arXiv:1709
 Adaptive mixtures oflocal experts,1991, Neural computation
 Speeding up convolutional neural networkswith low rank expansions,2014, arXiv preprint arXiv:1405
 Dynamic filter networks,2016, In Advancesin Neural Information Processing Systems
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Learning compact recurrent neural networks,2016, InAcoustics
 Compressing deep neu-ral networks using a rank-constrained topology,2015, In Sixteenth Annual Conference of the InternationalSpeech Communication Association
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Training and inference with integers in deepneural networks,2018, In International Conference on Learning Representations
 Breaking the softmax bottle-neck: A high-rank RNN language model,2018, In International Conference on Learning Representations
 On compressing deep models by lowrank and sparse decomposition,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Recurrent neural network regularization,2014, arXivpreprint arXiv:1409
 Learning transferable architecturesfor scalable image recognition,2017, arXiv preprint arXiv:1707
