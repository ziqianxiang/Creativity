title,year,conference
 An empirical evaluation of generic convolutional andrecurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Skip rnn: Learning toskip state updates in recurrent neural networks,2017, arXiv preprint arXiv:1708
 Language modeling with gated convolu-tional networks,2016, arXiv preprint arXiv:1612
 Recurrent neural network gram-mars,2016, arXiv preprint arXiv:1602
 A theoretically grounded application of dropout in recurrent neuralnetworks,2016, In Advances in neural information processing Systems
 Efficient sequence learning withgroup recurrent networks,2018, In Proceedings of the 2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Few-shot learning with graph neural networks,2018, In Proceedings of ICLR
 Neural messagepassing for quantum chemistry,2017, In Proceedings of ICML
 Improving neural language models with a continuouscache,2016, arXiv preprint arXiv:1612
 Long short-term memory,1997, Neural computation
 Learning graphical state transitions,2017, In Proceedings of ICLR
 Encoding sentences with graph convolutional networks for semanticrole labeling,2017, In Proceedings EMNLP
 Regularizing and optimizing lstm languagemodels,2017, arXiv preprint arXiv:1708
 The graphneural network model,2009, IEEE Transactions on Neural Networks
 Neural speed reading via skim-rnn,2017, arXivpreprint arXiv:1711
 Structured sequence model-ing with graph convolutional recurrent networks,2016, arXiv preprint arXiv:1612
 Neural language modeling by jointlylearning syntax and lexicon,2017, arXiv preprint arXiv:1711
 Recurrent neural network regularization,2014, arXivpreprint arXiv:1409
 Recurrent highwaynetworks,2016, arXiv preprint arXiv:1607
