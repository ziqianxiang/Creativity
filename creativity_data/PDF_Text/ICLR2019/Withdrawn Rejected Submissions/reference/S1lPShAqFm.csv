title,year,conference
 Deep Speech 2: End-to-EndSpeech Recognition in English and Mandarin,2016, In Proceedings of The International Conferenceon Machine Learning (ICML)
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 One Billion Word Benchmark for Measuring Progress in Statistical Language Model-ing,2013, arXiv preprint arXiv:1312
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 Escaping from saddle pointsonline stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Connectionist Tem-poral Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,2006, InProceedings of the International Conference on Machine Learning (ICML)
 Deep Speech: Scaling Up End-to-EndSpeech Recognition,2014, arXiv preprint arXiv:1412
 Exploring theLimits of Language Modeling,2016, arXiv preprint arXiv:1602
 Second order properties of error surfaces: Learning timeand generalization,1991, In Advances in neural information processing systems
 Explorations on high dimen-sional landscapes,2014, arXiv preprint arXiv:1412
 Universal halting times in optimization andmachine learning,2015, arXiv preprint arXiv:1511
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Gradient diversity: a key ingredient for scalable distributed learning,2017, arXiv preprintarXiv:1706
 RecurrentHighway Networks,2017, In Proceedings of The International Conference on Machine Learning(ICML)
 Letâ€™s also assume that the input is scalar x,2019, This is basically same aslinear regression
