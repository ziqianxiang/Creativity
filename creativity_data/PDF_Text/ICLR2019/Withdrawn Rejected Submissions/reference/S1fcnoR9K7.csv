title,year,conference
 Natural gradient works efficiently in learning,0899, Neural Comput
 Designing neural network architec-tures using reinforcement learning,2016, arXiv preprint arXiv:1611
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks of the trade
 Making a science of model search: Hyper-parameter optimization in hundreds of dimensions for vision architectures,2013, 2013
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Stronger baselines for trustable results in neural machinetranslation,2017, arXiv preprint arXiv:1706
 A brief review ofthe ChaLearn AutoML challenge: any-time any-dataset learning without human intervention,2016, InWorkshop on Automatic Machine Learning
 Learning both Weights and Connections forEfficient Neural Networks,2015, In Advances in Neural Information Processing Systems
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Tracking the best expert,1998, Machine learning
 Long short-term memory,1997, Neural computation
 Mobilenets: Efficient convolutional neural networks formobile vision aPPlications,2017, arXiv preprint arXiv:1704
 Densely connectedconvolutional networks,2017, In CVPR
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 An emPirical exPloration of recurrentnetwork architectures,2015, In International Conference on Machine Learning
 ExPloring thelimits of language modeling,2016, arXiv preprint arXiv:1602
 Improving generalization performance by switching fromAdam to SGD,2017, arXiv preprint arXiv:1712
 Adam: A Method for Stochastic Optimization,2015, In InternationalConference on Learning Representations
 Combining expert advice efficiently,2008, arXiv preprintarXiv:0802
 Learning Multiple Layers of Features from Tiny Images,2009, 2009
 Optimal brain damage,1990, In D
 Efficient backprop,1998, InNeural Networks: Tricks of the Trade
 Measuring the Intrinsic Dimen-sion of Objective Landscapes,2018, arXiv preprint arXiv:1804
 Progressive neural architecture search,2017, arXiv preprintarXiv:1712
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Gradient-based hyperparameter optimiza-tion through reversible learning,2015, In International Conference on Machine Learning
 Tuning-free step-size adaptation,2012, In Acoustics
 Building a large annotatedcorpus of english: The penn treebank,1993, Comput
 Speed learning on the fly,2015, arXivpreprint arXiv:151L02540
 Training deep networks without learning rates throughcoin betting,2017, In Advances in Neural Information Processing Systems
 Large-scale evolution of image classifiers,2017, arXiv preprintarXiv:1703
 Local gain adaptation in stochastic gradient descent,1999, 1999
 Compression of Neural MachineTranslation Models via Pruning,2016, arXiv preprint arXiv:1606
 Very deep convolutional networks for large-scale image recogni-tion,2014, CoRR
 Evolving neural networks through augmenting topolo-gies,2002, Evolutionary computation
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Machine learning: a Bayesian and optimization perspective,2015, Academic Press
 Catching UP faster in Bayesian model selec-tion and model averaging,2008, In J
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
