title,year,conference
 signsgd :compressed optimisation for non-convex problems,2018, arXiv preprint arXiv :1802
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT’2010
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 A new optimizer using particle swarm theory,1995, In MicroMachine and Human Science
 Image-to-image translation withconditional adversarial networks,2017, arXiv preprint
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv :1712
 Convolutional neural networks for sentence classification,2014, arXiv preprintarXiv :1408
 Adam : A method for stochastic optimization,2014, arXiv preprintarXiv :1412
 Imagenet classification with deep convo-Iutional neural networks,2012, In Advances in neural information processing systems
 Automatic learning rate maximization byon-line estimation of the hessian’s eigenvectors,1993, In Advances in neural information processingsystems
 On the convergence of adam and beyond,2018, 2018
 Regularized logistic regression is strictly convex,2005, Unpublished manuscript
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv :1609
 Convolutional neuralnetwork for automatic detection of sociomoral reasoning level,2017, pp
