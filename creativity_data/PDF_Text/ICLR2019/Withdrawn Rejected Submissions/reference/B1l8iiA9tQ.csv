title,year,conference
 Optimization methods for large-scale machinelearning,2018, SIAM Review
 Training deep nets with sublinearmemory cost,2016, CoRR
 Distributed deep lear-ning using synchronous stochastic gradient descent,2016, CoRR
 An exactalgorithm for f-measure maximization,2011, In J
 On the stochastic approximation,1965, Theory Probab
 Memory-efficientbackpropagation through time,2016, CoRR
 Deep learning,2016, Book in preparation for MITPress
 Online and stochastic gradient methodsfor non-decomposable loss functions,2014, CoRR
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, CoRR
 Fader networks: Manipulating images by sliding attributes,2017, CoRR
 On the statistical consistency of plug-inclassifiers for non-decomposable performance measures,2014, In Z
 A conVergence theorem for nonnegatiVe almost supermartingales andsome applications,1971, Optimizing methods in statistics
 Machine learning action parameters inlattice quantum chromodynamics,2018, 2018
 StriVing forsimplicity: The all conVolutional net,2014, CoRR
 Disentangled representations in neural models,2016, CoRR
 The general inefficiency of batch training for gradientdescent learning,2003, NeuralNetw
 Optimizing f-measures: A tale oftwo approaches,2012, In Proceedings of the 29th International Coference on International Conference onMachine Learning
