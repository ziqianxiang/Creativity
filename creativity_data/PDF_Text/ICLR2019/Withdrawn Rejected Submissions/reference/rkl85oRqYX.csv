title,year,conference
 Anexploration of parameter redundancy in deep networks with circulant projections,2015, In Proceedingsof the IEEE International Conference on Computer Vision
 A survey of model compression and accelerationfor deep neural networks,2017, arXiv preprint arXiv:1710
 XcePtion: Deep learning with depthwise separable convolutions,2016, arXiv preprintarXiv:1610
 Ultimate tensorization:compressing convolutional and fc layers alike,2016, arXiv preprint arXiv:1611
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Learning deep resnet blockssequentially using boosting theory,2017, arXiv preprint arXiv:1706
 Speeding up convolutional neural networkswith low rank expansions,2014, arXiv preprint arXiv:1405
 Tensor decompositions and applications,2009, SIAM review
 Ten-sor contraction layers for parsimonious deep nets,2017, In Computer Vision and Pattern RecognitionWorkshops (CVPRW)
 Ten-sor regression networks,2017, arXiv preprint arXiv:1707
 On optimizing a class of multi-dimensionalloops with reductions for parallel execution,1997, Parallel Processing Letters
 Gradient-based learning applied todocument recognition,1998, Proceedingsofthe IEEE
 Fast training of convolutional networks throughffts,2013, arXiv preprint arXiv:1312
 Tensorizing neuralnetworks,2015, In Advances in Neural Information Processing Systems
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Structured transforms for small-footprint deeplearning,2015, In Advances in Neural Information Processing Systems
 Wide compression:Tensor ring nets,2018, learning
 Tensor-train recurrent neural networks forvideo classification,2017, arXiv preprint arXiv:1707
 Deep fried convnets,2015, In Proceedings of the IEEE International Conference on ComputerVision
 Efficient and accurateapproximations of nonlinear convolutional networks,2015, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
