title,year,conference
 Spectrally-normalized margin bounds forneural networks,2017, In The 31st Conference on Neural Information Processing Systems (NIPS)
 Prediction games and arcing algorithms,1999, Neural computation
 Support-vector networks,1995, Machine Learning
 Eigenvalue computation in the 20th century,2001, In Numericalanalysis: historical developments in the 20th century
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR)
 Empirical margin distributions and bounding thegeneralization error of combined classifiers,2002, The Annals of Statistics
 Learning multiple layers of features from tiny images,2009, Tech￾nical report
 Probability in Banach Spaces: Isoperimetry and Processes,1991, Springer￾Verlag
 A surprisinglinear relationship predicts test performance in deep networks,2018, MIT CBMM memo
 Spectral normalizationfor generative adversarial networks,2018, In The 6th International Conference on Learning Represen￾tations (ICLR)
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2018, In The 6th International Conferenceon Learning Representations (ICLR)
 On convergence proofs on perceptrons,1962, In Proceedings of the Symposium on theMathematical Theory of Automata
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The implicit bias of gradient descent on separabledata,2018, In The 6th International Conference on Learning Representations (ICLR)
 Matching networks for oneshot learning,2016, In Advances in Neural Information Processing Systems
 High-Dimensional Statistics: A Non-Asymptotic Viewpoint,2019, Cambridge Seriesin Statistical and Probabilistic Mathematics
 On early stopping in gradient descent learn￾ing,2007, Constructive Approximation
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Boosting with early stopping: Convergence and consistency,2005, Annals ofStatistics
 For Bi-bounded difference functionsh : X → R s,2019,t
