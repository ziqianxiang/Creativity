title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Thermometer encoding: One hotway to resist adversarial examples,2018, 2018
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Stochastic activation pruning for robust adversarial de-fense,2018, arXiv preprint arXiv:1803
 Boost-ing adversarial attacks with momentum,2018, arXiv preprint
 Analysis of classifiers robustness to adversarialperturbations,2018, Machine Learning
 Adversarialdefense based on structure-to-signal autoencoders,2018, arXiv preprint arXiv:1803
 Countering adversarialimages using input transformations,2017, arXiv preprint arXiv:1711
 Early methods for detecting adversarial images,2016, arXiv preprintarXiv:1608
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 Characterizing adversarial subspaces using localintrinsic dimensionality,2018, arXiv preprint arXiv:1801
 Magnet: a two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 V-net: Fully convolutional neural net-works for volumetric medical image segmentation,2016, In 3D Vision (3DV)
 Distilla-tion as a defense to adversarial perturbations against deep neural networks,2015, arXiv preprintarXiv:1511
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia Conference on Computer and Communications Security
 Foolbox: A python toolbox to benchmark therobustness of machine learning models,2017, arXiv preprint arXiv:1707
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 Defending against adversarial images using basis functionstransformations,2018, arXiv preprint arXiv:1803
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Vulnerability analysis of chestx-ray image classification against adversarial attacks,2018, arXiv preprint arXiv:1807
 A boundary tilting persepective on the phenomenon of adversarialexamples,2016, arXiv preprint arXiv:1608
 Ensemble adversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 Using radial basis function networks for functionapproximation and classification,2012, ISRN Applied Mathematics
 Mitigating adversarialeffects through randomization,2017, arXiv preprint arXiv:1711
 Adversarialexamples for semantic segmentation and object detection,2017, 2017b
 Feature squeezing: Detecting adversarial examples in deepneural networks,2017, arXiv preprint arXiv:1704
 Adversarial examples:Attacks and defenses for deep learning,2018, arXiv preprint arXiv:1712
 Efficient defenses against adver-sarial attacks,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
