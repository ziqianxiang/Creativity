title,year,conference
 Wasserstein gan,2017, arXiv preprintarXiv:1701
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Hashing hyperplane queries tonear points with applications to large-scale active learning,2010, In Advances in Neural InformationProcessing Systems
 Multi-class active learning for imageclassification,2009, 2009
 Adam: A method for stochastic optimization,2015, In Proceedings ofthe International Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 ImageNet Classification with Deep Convo-lutional Neural Networks,2012, In F
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, In Proceedings of the International Conference onLearning Representations
 Very Deep Convolutional Networks for Large-Scale ImageRecognition,2015, In International Conference on Learning Representations (ICRL)
 Striving forsimplicity: The all convolutional net,2014, In Proceedings of the International Conference on LearningRepresentations Workshops
 Improving the improved trainingof wasserstein gans: A consistency term and its dual effect,2018, In Proceedings of the InternationalConference on Learning Representations
 Incorporating diversity and density in active learning forrelevance feedback,2007, In European Conference on Information Retrieval
 Combining active learning and semi-supervised learning using gaussian fields and harmonic functions,2003, In ICML 2003 workshop on thecontinuum from labeled to unlabeled data in machine learning and data mining
