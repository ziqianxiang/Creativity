title,year,conference
 Relevant sparse codes with variational informa-tion bottleneck,2016, In Advances in Neural Information Processing Systems
 Convergence diagnostics for stochastic gradient descent with constantstep size,2017, arXiv preprint arXiv:1710
 Optimal manifold representation of data: an informationtheoretic approach,2004, In Advances in Neural Information Processing Systems
 Entropy and mutual information in models of deep neural networks,2018, arXivpreprint arXiv:1805
 The information bottleneck revisited or how to choose a gooddistortion measure,2007, In Information Theory
 On the diffusion approximation of noncon-vex stochastic gradient descent,2017, arXiv preprint arXiv:1705
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 How to escapesaddle points efficiently,2017, arXiv preprint arXiv:1703
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Handwritten digit recognition with a back-propagation net-work,1990, In Advances in neural information processing systems
 Deep learning,2015, Nature
 Stochastic modified equations and adaptive stochastic gradient algo-rithms,2015, arXiv preprint arXiv:1511
 Self-organization in a perceptual network,1988, Computer
 Bregman divergence bounds and the universality of thelogarithmic loss,2018, arXiv preprint arXiv:1810
 On the universality of the logistic loss function,2018, arXivpreprint arXiv:1805
 Generalized independent component analysisover finite alphabets,2016, IEEE Transactions on Information Theory
 An information-theoretic framework for non-linear canonical correlation analysis,2018, arXiv preprint arXiv:1810
 Estimation of entropy and mutual information,0899, Neural Comput
 Modeling by shortest data description,1978, Automatica
 Empirical analysis ofthe hessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Learning and generalization with the informa-tion bottleneck,0304, Theoretical Computer Science
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
 Deep learning and the information bottleneck principle,2015, InInformation Theory Workshop (ITW)
 Theory of deep learning iib: Optimization properties of SGD,2018, CoRR
 Energy-entropy competition andthe effectiveness of stochastic gradient descent in machine learning,2018, Molecular Physics
