Figure 1: A basic buildingblock of vanilla RNNsTo establish the generalization bound, We need to define the “model complexity” of vanilla RNNs.
Figure 2: A basic building block of MGU RNNswhere Wr, Wh ∈ Rdh×dx, Ur, Uh ∈ Rdh ×dh, V ∈ Rdy ×dh, and rt ∈ Rdh. The notation de-notes the Hadamard product (entrywise product) of vectors. Denote by Fg,t the class of mappingsfrom the first t inputs to the t-th output computed by gated (MGU or LSTM) RNNs. For simplicity,We consider σ being the sigmoid function, i.e., σ(x) = (1 + exp(-x))-1, σ%(∙) = tanh(∙), andσy being ρy-Lipschitz with σy (0) = 0. Extensions to general Lipschitz activation operators as inAssumption 3 are straightforward. Suppose we have h0 = 0 and the following assumption.
Figure 3: A basic building block of LSTM RNNswhere Wg, Wr, Wo, Wc ∈ Rdh×dx,	Ug,Ur, Uo, Uc ∈	Rdh×dh, and	gt,rt,ot	∈ Rdh.	For sim-plicity, we also consider σ being the	sigmoid function,	and σc(∙) =	tanh(∙).	The t-th	output isyt = σy(Vht), where V ∈ Rdy×dh, and σy is Py-Lipschitz with σy(0) = 0. Suppose we haveho = co = 0 and the following assumption.
Figure 4: A comparison between general-ization bounds for the same vanilla RNNtrained on the wikitext dataset. The verti-cal axis is the logarithmic scale of the cor-responding bounds.
Figure 5: Illustration of input x ∈ R6 convolvingwith 3-channel 3-dimensional convolutional filtersI1 , I2, and I3, followed by an average pooling.
