Figure 1: EnGAN model overviewwhere Gω is the Generator network, Tφis the Statistics network used for MI es-timation and Eθ is the energy networkOne option to generate samples is simply to use the usualGAN approach of sampling a Z 〜Pz from the latent priorand then output X = G(z), i.e., obtain a sample X 〜pg.
Figure 2: Top: Training samples for the 3 toy datasets - 25gaussians, swissroll and 8gaussians.
Figure 3: Left: 64x64 samples from the CelebA dataset Right: 28x28 samples from the 3-StackedMNIST dataset. All samples are produced by the generator in a single step, without MCMCfine-tuning (see Fig. 4 for that).
Figure 4: Left: Samples at the beginning of the chain (i.e. simply from the ordinary generator, Z 〜N (0, I )). Right: Generated samples after 100 iterations of MCMC using the MALA sampler. Wesee how the chain is smoothly walking on the image manifold and changing semantically meaningfuland coherent aspects of the images.
Figure 5: Samples from the beginning, middle and end of the chain performing MCMC sampling invisible space. Initial sample is from the generator (pG) but degrades as we follow MALA directly indata space. Compare with samples obtained by running the chain in latent space and doing the MHrejection according to the data space energy (Fig. 4). It can be seen that MCMC in data space haspoor mixing and gets attracted to spurious modes.
