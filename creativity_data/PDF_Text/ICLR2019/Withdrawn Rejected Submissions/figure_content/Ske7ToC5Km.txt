Figure 1: The G2S-RNN neural network architecture. The quences output of GRAPH2SEQ and sum-GRU units process the representation output by GRAPH2SEQ, marizes it into one vector per vertex. (3)operating independently on each vertex’s time-series.	Q-Network takes the vector summary ofeach vertex a ∈ V and outputs the esti-mated Q(G, S, a) value. The overall architecture is illustrated in Fig. 1. To make the networkpractical, we truncate the sequence outputs of Graph2Seq to a length of T . However the value of T isnot fixed, and is varied both during training and testing according to the size and complexity of thegraph instances encountered; see § 5 for details. We describe each module below.
Figure 2: Scalability of G2S-RNN and GCNN in (a) Erdos-Renyi graphs (left), random bipartite graphs (right),and (b) on much larger graphs. The neural networks have been trained over the same graph types as the testgraphs. Error bars show one standard deviation.
Figure 3: Minimum vertex cover in worst-case graphs for greedy (left), random regular graphs (center), andrandom bipartite graphs (right) using models trained on size-15 Erdos-Renyi graphs. Each point shows thataverage and standard deviation for 20 randomly sampled graphs of that type. The worst-case greedy graph type,however, has only one graph at each size; hence, the results show outliers for this graph type.
Figure 4: (a) Max cut in Erdos-Renyi graphs (left), Grid graphs (right). (b) Maximum independent set inErdos-Renyi graphs (left), structured bipartite graphs (right). Some approximation ratios are less than 1 due tothe time cut-off of the Gurobi solver.
Figure 5: Example to illustrate k-LOCAL-GATHER algorithms are insufficient for computing certainfunctions. Corresponding vertices in the two trees in (a) and (b) above have similar k = 2 localneighborhoods, but the trees have minimum vertex cover of different sizes (Figure 6). Similarlythe trees in (c) and (d) contain nodes with similar k = 3 local neighborhoods, but have differingminimum vertex cover sizes (Figure 6).
Figure 6: Minimum vertex cover solution for the trees shown in Fig. 5. The vertices that belong tothe vertex cover are shaded in yellow.
Figure 7: Minimum vertex cover in (a) random Erdos-Renyi graphs, (b) random regular graphs, (c) randombipartite graphs, (d) greedy example, under Erdos-Renyi graph and adversarial graph training strategies.
Figure 8: (a) Erdos-Renyi graph of size 10 considered in Figures (b) and (c), (b) vertex-wise principalcomponent scores at each layer, and (c) projection of the principal direction at each iteration on theprincipal direction of iteration 10. These experiments are performed on our trained model.
Figure 9: (a) Approximation ratio of GRAPH2Seq With varying number of layers, (b) y(∙) vectors ofGraph2Seq in the intermediate layers seen using the Q-function, (c) x(∙) vectors of the fixed-depthmodel seen using the Q-function. Figure (b) and (c) are on planted vertex cover graph With optimumcover of vertices {0, 1, 2, 3, 4}.
