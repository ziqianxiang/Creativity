Figure 1:	Results of various DDPG algorithms on safe robot locomotion tasks, with x-axis inthousands of episodes. We include runs from DDPG (red), DDPG-Lagrangian (magenta), SD-DPG (blue), SDDPG-modular (green) on HalfCheetah-Safe and Point-Gather. We discover thatthe Lyapunov-based approaches can perform safe learning, despite the fact that the environment dy-namics model and cost functions are not known, control actions are continuous, and deep functionapproximations are necessary.
Figure 2:	Results of various PPO algorithms on safe robot locomotion tasks, with x-axis in thou-sands of episodes. We include runs from PPO (red), PPO-Lagrangian (magenta), SPPO (blue),SPPO-modular (green) on HalfCheetah-Safe and Point-Gather. Similar to Figure 1, the Lyapunov-based approaches can perform safe learning in the control tasks when function approximations onpolicies and value functions are necessary.
Figure 3:	Constraint violations (logarithmic scale, i.e., log(1 + ccv), where ccv represents the areaunder constraint learning curve that is above the constraint threshold) of various PG algorithms onsafe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG(red), DDPG-Lagrangian (magenta), SDDPG (blue), SDDPG-modular (green), PPO (red), PPO-Lagrangian (magenta), SPPO (blue), SPPO-modular (green) on HalfCheetah-Safe and Point-Gather.
Figure 4: The Robot Locomotion Control TasksAnt-GatherPoint-GatherOn top of the GAE parameter Î», in all numerical experiments and for each algorithm (SPPO, SD-DPG, SPPO-modular, SDDPG-modular, CPO, Lagrangian, and the unconstrained PG counterparts),we systematically explored different parameter settings by doing grid-search over the following fac-tors: (i) learning rates in the actor-critic algorithm, (ii) batch size, (iii) regularization parameters ofthe policy relative entropy term, (iv) with-or-without natural policy gradient updates, (v) with-or-without the emergency safeguard PG updates (see Appendix A.1 for more details). Although eachalgorithm might have a different parameter setting that leads to the optimal performance in training,the results reported here are the best ones for each algorithm, chosen by the same criteria (which isbased on value of return plus certain degree of constraint satisfaction). To account for the variabilityduring training, in each learning curve a 95% confidence interval is also computed over 10 separaterandom runs (under the same parameter setting).
Figure 5: Constraint violations (logarithmic scale, i.e., log(1 + ccv)) of various PG algorithms onsafe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG(red), DDPG-Lagrangian (magenta), SDDPG (blue), SDDPG-modular (green), PPO (red), PPO-Lagrangian (magenta), SPPO (blue), SPPO-modular (green) on Ant-Gather and Point-Circle.
Figure 6: Results of various PG algorithms on safe robot locomotion tasks, with x-axis in thousandsof episodes. We include runs from PPO (red), PPO-Lagrangian (magenta), SPPO (blue), SPPO-modular (green) on Ant-Gather and Point-Circle.
Figure 7: Results of various PG algorithms on safe robot locomotion tasks, with x-axis in thousandsof episodes. We include runs from DDPG (red), DDPG-Lagrangian (magenta), SDDPG (blue),SDDPG-modular (green) on Ant-Gather and Point-Circle.
