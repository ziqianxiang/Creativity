Figure 1: Visualization of sample trajectories inour learned embedding space.
Figure 2: Computational architecture for estimating IS(JSD) and IA(JSD) for image-based observations.
Figure 3: Example sample paths in our learned embedding representations. Note the embeddingdimensionality d is 2, and thus we did not use any dimensionality reduction techniques.
Figure 4: Performance of EMI on locomotion tasks with sparse rewards compared to baseline meth-ods (TRPO, EX2, ICM). The solid line is the mean reward (y-axis) of 5 different seeds at eachiteration (x-axis) and the shaded area represents one standard deviation from the mean.
Figure 5: Performance of EMI on sparse reward Atari environments compared to the baseline meth-ods (TRPO, EX2, ICM). EMI in (a), (b), (d), (e) uses relative diversity intrinsic rewards. Predictionerror intrinsic rewards are used in (c), (f). The solid line is the mean reward (y-axis) of 5 differentseeds at each iteration (x-axis) and the shaded area represents one standard deviation from the mean.
Figure 6:	Example transitions that entail large or small instances of the error term S(s, a), in Mon-tezuma’s Revenge.
Figure 7: Ablation study of loss terms in EMI on SparseHalfCheetah environment. Each solid linerepresents the mean reward of 5 random seeds.
Figure 8:	Study of intrinsic reward coefficient α in EMI on SparseHalfCheetah environment. Eachsolid line represents the mean reward of 5 random seeds.
