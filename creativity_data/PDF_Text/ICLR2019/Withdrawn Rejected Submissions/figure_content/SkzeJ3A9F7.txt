Figure 1: Schematic Explanation: In general formulation, the reward function r is a function ofenvironment function TE. This makes reward function non-differentiable with respect to action at. Inour approach, We use local constancy approximation to assume xt+i is a constant. This immediatelyturns reward function differentiable and hence trainable via gradient descent.
Figure 2: Comparing the performance of our proposed sample efficient exploration with other baselinemethods in simulated manipulation environment.
Figure 3: (Left) Interaction Rate vs. Number of Samples for real-world robot. (Right) Table 2 showsinteraction rate with 250 steps and Table 3 shows the interaction rate of final robot model.
Figure 4: Used external reward from environment to learn policyIn this section, we tried to analyze how policy performs if it is provided with external reward fromthe environment. The environment gives +1 to the agent if it moves the objects, otherwise 0.
Figure 5: Robot Interacting with objects based on curiosity12Under review as a conference paper at ICLR 2019(a) real world robot setting(b) simulation settingAppendix B	Prediction LossPrediction Loss We compare the prediction loss of the forward model learned on the data collectedby different exploration schemes. We use L2 distance in the ImageNet feature space. As the result inthe Table 1 indicate the data collected using our model performs better.
Figure 7: Comparison to recent state-of-the-art exploration strategy (Burda et al., 2018) on Atari.
