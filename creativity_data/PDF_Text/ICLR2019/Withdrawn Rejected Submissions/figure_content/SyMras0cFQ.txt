Figure 1: Role of homeostasis in learning sparse representations: We show the results of SparseHebbian Learning using different homeostasis algorithms at convergence (1024 learning epochs).
Figure 2: Histogram Equalization Homeostasis and its role in unsupervised learning: (A) Non-linear homeostatic functions zi , ∀i learned using Hebbian learning. These functions were computedfor different homeostatic strategies (None, OLS or HEH) but only used in HEH. Note that for ourchoice of N0 = 13, all cumulative functions start around 1 - N0/N ≈ .970. At convergence ofHEH, the probability of choosing any filter is equiprobable, while the distribution of coefficientsis more variable for None and OLS. As a consequence, the distortion between the distributions ofsparse coefficients is minimal for HEH, a property which is essential for the optimal representation ofsignals in distributed networks such as the brain. (B) Effect of learning rate η (eta) and homeostaticIearning rate 加(eta_homeo) on the final cost as computed for the same learning algorithms butwith different homeostatic strategies (None, OLS or HEH). Parameters were explored around a defaultvalue, on a logarithmic scale and over 4 octaves. This shows that HEH is robust across a wide rangeof parameters.
Figure 3: Homeostasis on Activation Probability (HAP) and a quantitative evaluation of home-ostatic strategies: (A) 18 from the 441 dictionaries learned for the two heuristics EMP and HAPand compared to the optimal homeostasis (see Figure 1-A, HEH). Again, the upper and lower rowrespectively show the least and most probably selected atoms. (B) Comparison of the cost F duringlearning and cross-validated over 10 runs: The convergence of OLS is similar to EMP. The simplerHAP heuristics gets closer to the more demanding HEH homeostatic rule, demonstrating that thisheuristic is a good compromise for fast unsupervised learning.
Figure 4: Extension to Convolutional Neural Networks (CNNs): We extend the HAP algorithmto a single layered CNN with 20 kernels and using the ATT face database. We show here the kernelslearned without (None, top row) and with (HAP, bottom row) homeostasis (note that we used thesame initial conditions). As for the simpler case, we observe a heterogeneity of activation countswithout homeostasis, that is, in the case which simply normalizes the energy of kernels (see (A)).
