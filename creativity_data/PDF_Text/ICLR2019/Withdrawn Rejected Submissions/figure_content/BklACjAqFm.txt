Figure 1: Binary tree MDP; in each state, agent moves either up or down. States withodd indices are terminal. Reward is only obtained after a large fixed number of up actions.
Figure 2: Architecture diagram for network used for Atari experiments.
Figure 3: The Chain MDP illustratedWe present the return achieved by the three methods in these tabular problems withinfigure 4. The poor performance of BDQN follows from its lack of propagation of uncertainty.
Figure 4: Test reward on Tree (left) and Chain (right), both with L=10. Episodes onx-axis. Average over 250 runs plotted; 95% CI shaded.
