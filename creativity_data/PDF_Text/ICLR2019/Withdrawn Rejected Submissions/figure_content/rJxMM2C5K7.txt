Figure 1: Nested one-dimensional quantizers, finequantizer (blue) with ∆1 = 1∕3 and coarse quan-tizer (green) with ∆2 = 1.
Figure 2: Schematic overview of thedistributed training.
Figure 4:	Accuracy of distributed training vs number of workersBaseline--QSGDDQSGDOne-Bit100	200	300	400	500Iteration0.20.10ycaruccA(b) 8 workers(a) 4 workersFigure 5:	Comparison of convergence rate of distributed training of CifarNet with Adam algorithmNext, we compare our nested dithered quantizer with the dithered quantization scheme. To have faircomparison, we chose the same expected accuracy for both quantization schemes. For DQSG, wechose M = 2, hence ∆ = 0.5 and the output of quantizer would be in {-2, . . . , -2}. In NDQSG,for half of the workers, we divided the workers to two groups, half of the workers use DQSG with thesame ∆ and the other half, uses NDQSG with ∆1 = 1/3 and ∆2 = 1. Hence, the output of NDQSGquantizer is in {-1, 0, 1}. In Fig. 6 we compared the accuracy of NDQSG with DQSG and baseline
Figure 5:	Comparison of convergence rate of distributed training of CifarNet with Adam algorithmNext, we compare our nested dithered quantizer with the dithered quantization scheme. To have faircomparison, we chose the same expected accuracy for both quantization schemes. For DQSG, wechose M = 2, hence ∆ = 0.5 and the output of quantizer would be in {-2, . . . , -2}. In NDQSG,for half of the workers, we divided the workers to two groups, half of the workers use DQSG with thesame ∆ and the other half, uses NDQSG with ∆1 = 1/3 and ∆2 = 1. Hence, the output of NDQSGquantizer is in {-1, 0, 1}. In Fig. 6 we compared the accuracy of NDQSG with DQSG and baselinetraining during training. As seen, the learning curve of NDQSG is almost the same as DQSG and thebaseline. However, the communication bits are much less. For example, in training FC-300-100, with2 level quantizers, QSG and DQSG requires 619.2 Kbits per worker to communicate, while NDQSG9Under review as a conference paper at ICLR 2019reduces that to 422.8 Kbits, more than 30% reduction in number of bits to communicate. The Sameis true for the other considered neural networks.
Figure 6: Accuracy of nested dithered quantization at each iteration of training for 8 workers5 ConclusionIn this paper, first, we introduced DQSG, dithered quantized stochastic gradient, and showed that howit can reduce communication bits per training iteration both theoretically and via simulations, withoutaffecting the accuracy of the trained model. Next, we explored the correlation that exists amongthe SGs computed by workers in a distributed system and proposed NDQSG, a nested quantizationmethod for the SGs. Using theoretical analysis as well as simulations, we showed that NDSQGperforms almost the same as DQSG in terms of accuracy and training speed, but with much fewernumber of communication bits.
