Figure 1: The architecture for our Neuron Selectivity Transfer: the student network is not onlytrained from ground-truth labels, but also mimics the distribution of the activations from interme-diate layers in the teacher network. Each dot or triangle in the figure denotes its correspondingactivation map of a filter.
Figure 2: Neuron activation heat map of two selected images.
Figure 3: Different knowledge transfer methods on CIFAR10 and CIFAR100. Test errors are inbold, while train errors are in dashed lines. Our NST improves final accuracy observably with a fastconvergence speed. Best view in color.
Figure 4: Top-1 validation error of differentknowledge transfer methods on ImageNet. Bestview in color.
Figure 5: t-SNE (Maaten & Hinton (2008)) visual-ization shows that our NST Transfer reduces the dis-tance between teacher and student activations distri-bution.
