Figure 1: Left: for each iteration, a randomly selected encoder-decoder pair is used. Right: inexpectation, all encoders are trained with all decoders, and vice versa.
Figure 2: The reconstruction error per pixel on MNIST (left) and CIFAR-100 (right).
Figure 3: Set-up for assessing the quality of the representations. Purple boxes represent trainableevaluation modules; blue boxes represent trained encoders and decoders that are kept fixed duringthe evaluation training. Left: linear classifiers; centre: new encoders; right: new decoders.
Figure 4: The sample complexity gain relative to the traditional AE setup when training a linearclassifier, on MNIST (left) and CIFAR-100 (right).
Figure 5: The sample complexity gain relative to the traditional AE setup when training new de-coders, on MNIST (left) and CIFAR-100 (right), while training new encoders.
Figure 6: The sample complexity gain relative to the traditional AE setup when training new en-coders, on MNIST (left) and CIFAR-100 (right), while training new encoders.
