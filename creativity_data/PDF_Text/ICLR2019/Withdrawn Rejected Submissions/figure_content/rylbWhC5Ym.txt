Figure 1: Comparison of performance of HR-Q with a neural network on the Mountain Car domain.
Figure 2: Comparison of performance of HR-Q with a neural network on the Acrobot domain7Under review as a conference paper at ICLR 2019the network and hyperparameters are in Appendix B. We take 20 independent runs, with a differentseed in each run used to initialize Tensorflow, NumPy, and the OpenAI Gym environment. Each runis taken over 1000 episodes. In both these experiments, we see HR-TD starts to learn a useful policybehavior before either of the other techniques. Interesting to note is that in Fig. 1b, HR-TD learnsa state representation that causes the target value to change less than DQN but does not restrict itas much as TC. But in Fig. 2b we see that HR-TD is able to find a representation that is better atkeeping the target value separate than TC is. However, in both these cases, the value function that islearned seems to be quickly useful for learning a better policy.
Figure 3: Experimental Evaluation on Atari Pong domainWe also validate the applicability of this technique to a more complex domain and a more complexnetwork. We apply the HR-Q to DDQN on the Atari domain to verify that the technique is scalableand that the findings and trends we see in the first two experiments carry forward to this challengingtask. We use the network architecture specified in Mnih et al. (2015), and the hyper-parameters forTC as specified in Pohlen et al. (2018). Experimental details are specified in Appendix B. From theresults, we see that HR-TD does not interfere with learning on the complex network, and does aboutas well as DDQN.
Figure 4: Comparison on the Mountain Car domain.
Figure 5: Comparison of policy evaluation on the Mountain Car domain.
