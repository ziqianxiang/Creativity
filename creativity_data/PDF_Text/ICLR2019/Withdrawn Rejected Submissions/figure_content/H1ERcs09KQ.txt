Figure 1: Example of hierarchically clustered embeddings on MNIST with three levels of hierarchy,the reconstructed digits from the hierarchical Gaussian mixture components, and the extracted levelproportion features. We marked the mean of a Gaussian mixture component with the colored square,and the digit written inside the square refers to the unique index of the mixture component.
Figure 2: Graphical representation of VaDE (Jiang et al., 2017) (left), VAE-nCRP (Goyal et al.,2017) (center), and neural architecture of both models (right). In the graphical representation, thewhite/shaded circles represent latent/observed variables. The black dots indicate hyper or variationalparameters. The solid lines represent a generative model, and dashed lines represent a variationalapproximation. A rectangle box means a repetition for the number of times denoted by the bottomright of the box.
Figure 3: A simple depiction (left) of the key notations, where each numbered circle refers to thecorresponding Gaussian mixture component. The graphical representation (center) and the neuralarchitecture (right) of our proposed model, HCRL. The neural architecture of HCRL consists of twoprobabilistic encoder networks, gφη and gφz , and one probabilistic decoder network, fθ.
Figure 4: Example extracted sub-hierarchies on CIFAR-100RCV1.v2: Figure 5 shows the embedding of RCVLV2. VAE and VaDE show no hierarchy, andclose sub-hierarchies are distantly embedded. VAE-nCRP guides the internal mixture componentsto be agglomerated at the center, and the cause of agglomeration is the generatiVe process of VAE-nCRP, where the parameter of the internal components are inferred without direct information fromdata. HCRL shows a clear separation between the sub-hierarchy without the agglomeration.
Figure 5: Comparison of embeddings on RCV1,v2, plotted using t-SNE (Maaten & Hinton, 2008).
Figure 6: Example extracted sub-hierarchies on 20Newsgroups5 ConclusionIn this paper, we have introduced a hierarchically clustered representation learning framework for thehierarchical mixture density estimation on deep embeddings. HCRL aims at encoding the relationsamong clusters as well as among instances to preserve the internal hierarchical structure of data. Themain differentiated features of HCRL are 1) the crucial assumption regarding the internal mixturecomponents for having the ability to generate data directly, and 2) the unbalanced autoencodingneural architecture for the level proportion modeling as the encoding structure, and the probabilisticmodel as the decoding structure. From the modeling and the evaluation, we found that HCRL enablesthe improvements due to the high flexibility modeling compared with the baselines.
Figure 7: Synthetic data in the input space of R50 (left), which is visualized via t-SNE (Maaten& Hinton, 2008), and hierarchically clustered embeddings in the latent space of R2 (right). Weadditionally show a 95% confidence ellipse with a dashed line for each Gaussian mixture component.
Figure 8: The process by which the embeddings of the synthetic data are learned. The dashed ellipsecorresponds to the 95% contour of the learned Gaussian mixture component, whose mean is markedas the gray circle.
Figure 9: The illustration of GROW operationThe GROW operation expands the hierarchy by creating a new branch under the heavily weightedinternal node. Compared with the work from Wang & Blei (2009), we modify GROW to firstlysample a path according to PN=I q(Zn = Z), and then grow the path if the sampled path is aninner path. When we create the new Gaussian mixture component, we initialize the parameters of acorresponding Gaussian distribution depending on the mean and the variance of the parent node, asshown in line 10 of Algorithm 2.
Figure 10: The illustration of PRUNE operationQφo⅛rThe PRUNE operation cuts a minor path, which is sampled according to PN=Iq(Zn = Z) amongthe full paths satisfying PN=I q(Zn = Z) < δ, where δ is the pre-defined threshold parameter. Ifthe removed leaf node of the full path is the last child of the parent node, we also recursively removethe parent node as shown in the upper case of Figure 10.
Figure 11: The illustration of MERGE operationThe MERGE operation combines two full paths with similar posterior probabilities, measured byJ(Z(i),Z(j)) = qiqj/∖qi||qj|, where qi = [q(Zι = Z(i)),…,q(0N = Z(i))]. We merged twoGaussian components by following Ueda et al. (1999). The specific meaning of combining the twopaths is merging the paired two Gaussian distributions lying on the two paths by level, if the twoGaUSSian distribtions are different. The estimation of merged GaUSSian parameters, μ and σ, is theweighted summation of two subject Gaussian parameters. The propbability of the node at level llying on a Path Z given x, p(Zι ∖x), is proportional to Pn{q(In = l) ∙ PZ∈λ q(Zn = ζ)}, whereΛ={Z0∖ζl0=ζlandZ0 ∈ PfUll}.
