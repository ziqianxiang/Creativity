Figure 1: Spatio-temporal attention for video action recognition. The convolutional features areattended over both spatially, in each frame, and subsequently temporally. Both attentions are soft,meaning that the effective final representation at time t of an RNN, used to make the prediction, isa spatio-temporally weighted aggregation of convolutional features across the video along with thepast hidden state from t - 1. For details please refer to Sec. 3.
Figure 2: Spatial attention component. We use several layers of convolutional network to learn theimportance mask Mi for the input image feature Xi , the output is the element-wise multiplicationXi = Xi Î˜ Mi. Details please refer to Sec. 3.2.
Figure 3: Temporal attention component. The temporal attention learns a temporal attentionweight wti at each time step t. The final feature map Yt at time t to the ConvLSTM is a weightedsum of the feature from all the previous masked frames. Details please refer to Sec. 3.3.
Figure 4: Examples of spatial temporal attention. (Best viewed in color.) A frame sequencefrom a video of Drink action in HMDB51. The original images are shown at the top row, spatialattention is shown as heatmap (red means important) in the middle row, and temporal attention scoreis shown as the gray image (the brighter the frame is, the more crucial the frame is) at the bottomrow. It shows that spatial attention can focus on important areas while temporal attention can attendto crucial frames. The temporal attention also shows a unimodal distribution for the entire actionfrom starting to drink to completing the action.
Figure 5: Examples of spatial attention for action localization. (Best viewed in color.) Bluebounding boxes represent ground truth while the red ones are predictions from our learned spatialattention. (a) long jump, (b) rope climbing, (c) skate boarding, (d) soccer juggling, (e) walking withdog, (f) biking.
Figure 6: Examples temporal localization with temporal attention from THUMOS14. Theupper two rows show Volleyball action original images and imposed with temporal attention weightsrespectively. The lower two rows show Throw Discus action. Our temporal attention module canautomatically highlight important frames and avoid irrelevant frames corresponding to non-actionposes or background.
Figure 7: Multiple actions in one image for video action recognition The Sit action fromHMDB51. In the first two frames, there is no sitting action while the spatial attention capturethe important area, but the temporal attention can effectively ignore them as the background infor-mation. It is interesting that in the last few frames, there is another person trying to sit down, but thevisual attention can only capture one sitting person.
Figure 8: Examples of spatial attention for action localization. (Best viewed in color.) Bluebounding boxes represent ground truth while the red ones are predictions from our learned spatialattention. Our spatial attention mechanism is able to focus on important part of the action, while theground truth bounding boxes labels focus on the entire human pose. As in the training stage, theground truth bounding boxes are not used, and the model can only depend on crucial spatial arearather than the entire action to make prediction. For actions with object interactions, such as HorseRiding and Pole Vault, the ground truth box focuses on human pose while the model focuses onobjects (such as Horse, Pole) as well.
Figure 9: Failure cases for spatial localization. (Best viewed in color.) In the ground truth bound-ing boxes, there is only one bounding box in human action for each frame but there may be morethan one person performing the same action. Typical IOU=0 case is that our attention focuses onthe unlabeled human action, such as Fencing and Biking shown here. Strong motion blur also leadsto failure cases, such as the Diving and Floor Gymnastics frames shown here. The diving and gym-nastics poses are highly motion blurred so the spatial attention focuses on the swimming pool andaudiences respectively. Some of the important background information also leads to failure cases,such as the basketball frame in the Basketball Dunk shown here.
Figure 10: Failure example for temporal attention localization. A sequence of Tennis Swingaction from one video of THUMOS14. All temporal localization is correctly localized except theframe (b). The labeled action starts from frame (b) but our temporal attention module still assigns alow importance score.
