Figure 1: (a) shows the mappings relation in our proposed method. (b) shows the architecutre of theour networks.
Figure 2: (a) shows the density of h for 1 and 2 MNIST, (b) shows the mean of h for entire MNISTclassesZ also have smoother density. Therefore, H and Z have greater convexity than X and it permitslinear combination on these two manifolds in the feature spaces H and Z, are the two manifoldsare more convex than the manifold in the pixel space X. By manifold hypothesis, different classmanifolds are well-separated by regions of very low density (Cayton, 2005). The feature spaces arehigh-level representation spaces so that the density is unfolded and a linear combination of featuresgenerates amore natural result in the pixel space Bengio (2014).
Figure 3: (a), (b) the generate images using the indicator vectors of Laplacian on C for MNISTand CIFAR10, (c) describes the linear combination using class 2 to generate 1 but failed. (d) is thegenerated image by the linear combination of the right features h for 1the hypotheses (Bengio et al., 2013), we know that each class manifold has the compactness and theconvexity. Now we can assume the following assumption.
Figure 4: a shows theoretical result of the estimated eigengap using the overlapped area of block-diagonal matrices and Figure (b) and (C) shows the eigengap ∣λk 一 λk-ι∣ of the MNIST and CI-FAR10.
Figure 5: In the some range of noise, we can always generate the same class image. The magnitudesof noise for each class are different.
Figure 6: We sample Z from the normal distribution, so the norm of z is smaller than h. In additino,we can observe that C changes not only the magnitude of the vector but also the angle.
Figure 7: The left figure is the cosine values between the mean vector of each classes hi, Zj. Theright figure is the cosine values between hi, hj.
Figure 8: The left figure is the initial similarity matrix W, The right figure is the right permutatedsimilarity matrix.
