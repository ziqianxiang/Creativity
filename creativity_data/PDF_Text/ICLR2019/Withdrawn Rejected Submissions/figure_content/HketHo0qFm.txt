Figure 1: RL algorithms for CartPole. x-axis is the iterations of episodes and y-axis is the rewards of thecorresPonding ePisode. The goal of the game is to keeP the Pole standing on the cart as long as Possible. Themaximum number of stePs , i.e., the sum of the rewards, of each ePisode is 200.
Figure 2: The results of Q-learning, double Q-learning and on-policy actor-critic algorithms, on games ofCartPole, Mountain Car and Pendulum. The comparison of algorithms in three variants: the traditional, theinverse policy and the hybrid policy. The x-axis is the iterations of episodes and y-axis is the 100-step averagerewards around the corresponding episode. In game Mountain Car and Pendulum, we use log scaled y-axis,which is computed as y0 = -log(-y).
Figure 3:	The correction of policies on Mountain Car experiment, with deep Q-learning, double Q-learning andactor-critic. x-axis and y-axis are the the velocity and the position of the car, respectively.
