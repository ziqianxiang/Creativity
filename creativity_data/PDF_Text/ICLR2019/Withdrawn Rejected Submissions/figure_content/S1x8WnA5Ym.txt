Figure 1: We draw inspiration from DPP to model a subset diversity using a kernel. During training, we extractthe feature representation of real and fake batches φreal and φf ake . Then, we construct their diversity kernels:LSB , LDB . Our loss encourages G to synthesize data ofa diversity LSB similar to the real data diversity LDB .
Figure 2: Scatter plots of the true data(green dots) and generated data(blue dots) from different GAN methodstrained on mixtures of Gaussians arranged in a ring (top) or a grid (bottom).
Figure 3: Evaluating the models on the 2D Ring and Grid datasets in terms of (a) data-efficiency and (b)time-efficiency. GDPP-GAN tends to converge faster and require the least amount of training data.
Figure 4: Real images and their nearest generations ofCIFAR-10. Nearest generations are obtained by opti-mizing the input noise to minimize the reconstructionerror of the generated image.
Figure 5: Adding GDPP loss to DCGAN Sta-bilizes adversarial training and generates highquality samples earliest on CIFAR-10.
Figure 6: (a) Architectures employed in the synthetic experiments. (b) Architectures employed in our imagegeneration experiments.
Figure 7: The effect of poor initialization on generations: GDPP-GAN models true manifold structure evenwith poor initializations, while WGAN-GP maps noise to disperse distribution covering the modes with lowquality samples.
Figure 8: Random Samples generated on Stacked-MNIST by GDPP-GAN and GDDP-VAE respectively.
Figure 9: Random Samples generated by GDPP-GAN after 100K iterations.
Figure 10: Fixed noise qualitative samples progression for different models.
Figure 11: Comparing the performance of our loss when compared to DCGAN and WGAN-GP loss, usingProgressive-Growing GANs (Karras et al., 2017).
