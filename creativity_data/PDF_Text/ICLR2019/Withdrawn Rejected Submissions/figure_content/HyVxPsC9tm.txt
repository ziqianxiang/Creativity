Figure 1: The subtle difference in the feature map.
Figure 2: DynCNN ArchitectureojɔlŋɔuəjoJJIaəuirjh一► Prediction-► Pres ervation--⅜- Control一► Updation3.1	Frame DifferencingThe frame differencing method (Liu & Hou (2012)) is often used in the moving object detection andsegmentation methods. In this work, we employ it to inspect the inter-frame variation between twoadjacent frames. The basic concept of frame differencing is to subtract two frames to calculate thedifference of each pixel. However, in most surveillance video sequences there exist speckle noisewhich severely affects the difference value. Accordingly, the difference map, as referred to inputdifference map (IDM), is derived by thresholding the result of frame differencing with the followingstatement:D(i, j) =	10∣∆I(i,j)∣ < Θidmotherwise(1)where ΘIDM denotes the threshold of the statement. The pixel at (i,j) is denoted as a changed pixel
Figure 3: The diagram of data transfer. In the pixel-based method, the amount of needed pixels forone cell (e.g. 2 × 2 pixels) is 4 × 9 while the cell-based method requires 4 × 4 needed pixels, whichgreatly reduces the amount of data transfer.
Figure 4: The average precision and calculation amount on 4 dataset over different threshold of inputdifference mapIt is observed from Fig. 4a that the calculation amount exists a significant drop in the thresholdinterval of 5 to 15 over 4 datasets, which indicates the threshold greater or equal to 15 can filter thespeckle noise efficiently in four datasets. However, the precision will be affected as the threshold isgreater or equal to 25 as shown in Fig. 4b. Therefore, by trading the calculation amount off againstprecision, 20 is the best choice of threshold applicable to various surveillance videos and is selectedas the default in experiments.
Figure 5: The distribution of each test surveillance videosIn Fig. 5, we reveal the distribution of each test surveillance videos on calculation amount and FPSwith respect to the average scene similarity. The triangle mark denotes the average of videos in thesame dataset while the purple line represents the value of baseline. It is obvious that the higher the8Under review as a conference paper at ICLR 2019average scene similarity the more the calulation amount decreases. In addition, the FPS also tendsto increase as the average scene similarity is higher, especially in the dataset VIRAT. In datasetVIRAT, the most scene of surveillance videos is parking lot where most of time are the still scene,resulting in 99% average scene similarity. The high scene similarity prunes the calculation amountof baseline model up to 75.7% (90.5 GFLOPs to 22.0 GFLOPs) and improves 2.2 times (32.5 to71.7) of FPS. From the distribution map shown in Fig. 5, it is implied that the surveillance videosgenerally exist high scene similarity. Furthermore, the proposed dynamic convolution is applicableto all the videos. Therefore, it is believed that dynamic convolution can be applied to the practicalapplications on surveillance videos.
Figure 6: SSD512 modelFigure 7: DynSSD512 model11Under review as a conference paper at ICLR 20197.2	GPU Execution TimeFor a better understanding of the proposed method, we list the average execution time of each layerin detail. Take the PETS2009 dataset for example.
Figure 7: DynSSD512 model11Under review as a conference paper at ICLR 20197.2	GPU Execution TimeFor a better understanding of the proposed method, we list the average execution time of each layerin detail. Take the PETS2009 dataset for example.
Figure 8: Example detections on 4 test datasets(f) DynCNN on VIRAT0.98(h) DynCNN on YouTube13Under review as a conference paper at ICLR 20197.4	Calculation results on other datasetsTable 9: Average FLOPs result on AVSS2007Layer-type	MaPs	Baseline		DynCNN (w/o)			DynCNN (w/)				WXh	FLOPs	wxh	FLOPs	%Pruned	WXh	FLOPs	%PrunedConv1-1	512x512	64	4.5E+08	1704 X 16	4.7E+07-	89.6%	1931 X18	6.3E+07	85.9%Conv1-2	512x512	64	9.6E+09	1916x16	1.1E+09	88.2%	1929x16	1.2E+09	87.5%Conv2-1	256 X256	128	4.8E+09	1276x8	7.5E+08-	84.4%	1507 X10	1.1E+09	77.1%Conv2-2	256 x256	128	9.6E+09	1385x8	1.6E+09	83.0%	1505 x 8	2.2E+09	77.0%Conv3-1	128 X128	256	4.8E+09	532x8	1.2E+09-	74.0%	753 X 12	2.6E+09	44.6%Conv3-2	128 x128	256	9.6E+09	578x8	2.7E+09	71.7%	751 x 10	4.4E+09	53.9%Conv3-3	128 X128	256	9.6E+09	619x8	2.9E+09	69.7%	749x8	3.5E+09	63.2%Conv4-1	64x64	512	4.8E+09	236x8	2.2E+09-	54.2%	377 x 12	4.7E+09	1.7%Conv4-2	64x64	512	9.6E+09	254x8	4.7E+09	50.5%	375x 10	7.8E+09	18.5%Conv4-3	64x64	512	9.6E+09	273x8	5.1E+09	46.8%	373 x8	6.2E+09	35.3%
