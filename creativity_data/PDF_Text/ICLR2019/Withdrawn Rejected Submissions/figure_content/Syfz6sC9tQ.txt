Figure 1: Generated samples from GFMN using pretrained encoder as feature extractor.
Figure 2: Generated samples from GFMN that uses as feature extractor a VGG-19 net pretrainedon ImageNet. (2a) is a sample from the (real) CIFAR10 dataset. (2d - 2f) show the impact of usingdifferent number of layers to perform feature matching.
Figure 3: Generated images from GFMN trained with either simple Moving Average (MA) (3a, 3b,3c and 3d) or Adam Moving Average (AMA) (3e and 3f), and different minibatch sizes (mbs). Whilesmall minibatch sizes have a big negative effect for MA, it is not an issue for AMA.
Figure 4: Loss as a function of training epochs with example of generated faces.
Figure 5: Generated samples from GFMN trained for two different portions of ImageNet dataset.
Figure 6: Random samples from GFMN models using different portions of ImageNet as training set.
Figure 7:	Generated samples from GFMN using pretrained encoder as feature extractor. Visualcomparison of models generated without (top row) and with (bottom row) initialization of thegeneration.
Figure 8: Generated images from GFMN trained with a different number of VGG19 layers for featureextraction.
Figure 9: Generated images by WGAN-GP with pretrained VGG19 as a discriminator.
Figure 10: Generated images from GFMN trained with either simple moving average (MA) or Adammoving average (AMA). VGG19 ImageNet classifier is used as feature extractor.
Figure 11: Generated images from GFMN (11a and 11b) and GMMN (11c and 11d). GMMN imageswere obtained from Li et al. (2017).
Figure 12: Random samples from the decoders used to initialize GFMN generators (Figs. 12a, 12band 12c), and from the generators after GFMN training (Figs. 12d, 12e and 12f) .
Figure 13: Generated images from GFMN trained with either simple Moving Average (MA) (13a,13b, 13d and 13e) or Adam Moving Average (AMA) (13c and 13f). For MA, two minibatch sizes(mbs) are used: 64 and 128. Images in the top row were generated by models that perform featurematching using the minibatch-wise mean of the features from the real data, while the models thatgenerated the images in the bottom row used global mean (gm) features computed in the wholeCelebA dataset.
Figure 14: Generated images from GFMN trained with either VGG19 features (top row) or autoen-coder (AE) features (bottom row). We show images generated by GFMN models trained with simplemoving average (MA) and Adam moving average (AMA). Although VGG19 features are from across-domain classifier, they perform much better than AE features, specially for the MA case.
