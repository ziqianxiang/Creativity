Figure 1: Recursively constructing higher layer MLMDPs. The base LMDP (LEFT) is specifiedas a set of interior and boundary states Si , Sb , along with their corresponding instantaneous rewardsRi , Rb, and state transition dynamics Pi , Pb . The recursion is realized by first augmenting thebase LMDP with an addition set of subtask states St (CENTER). In order to successfully integratethese new states, the corresponding reward function Rt and transition dynamics [Pt, Pbl+1, Pil+1]are suitably specified. The resulting construction is itself an LMDP, forming the basis for recursion(RIGHT).
Figure 2: Adding an additional subtask state. The representational capacity of the hierarchy isincrementally increased by periodically adding a new subtask state. This state is suitably integratedinto the problem, by specifying the associated transition and reward structures. Note that thisprocedure is defined layer-wise, and that additional subtask states are added at all levels of thehierarchy.
Figure 3: Learning the hierarchy online can improve exploration. a) The online policy is ableto more reliably reach further than either the exploration policy, or the hierarhical policy (withoutexploration). b) With hierarchy, the agent is able to see more of the space earlier on. Rather thandithering at early states, the agent utilizes the higher layer policies to jump to the right boundaryof its experience before exploring locally. c) The number of subtasks grows consistently with thenumber of steps taken by the agent. This suggests that the agent is being pushed to the frontier of itsexperience, seeing new states, and instantiating new subtasks. d) Heatmap plots of the exponentiatedvalue function for the created subtasks, show how the subtasks tile the space. Notice that the numberof subtasks grows as the agent takes more steps in the domain.
Figure 4: Exploration with hierarchical policies a) Our agent operates in a corridor-of-roomsdomain. At any point in time, the task is specified as a single goal room, in which 50% of thestates are rewarded. When the agent reaches a rewarded state, the goal room is reset randomly. Thiscorresponds to a foraging task in which the agent must explore both globally and locally. b) Thenumber of goals reached by the agent in a finite horizon task of 50,000 steps. The hierarchical agentis better suited to the task owing to its ability to operate over an abstracted representation of thedomain.
