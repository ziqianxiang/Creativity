Figure 1: Causaldiagram of agentA’s effect on B’saction. We conditionon each agent’s viewof the environmentand LSTM stateu (shaded nodes),and intervene on atA(blue).
Figure 2: The communication model has twoA2C heads, which learn a normal policy, πe, anda policy for emitting communication symbols,πc. Other agents’ communication messages mtare input to the LSTM.
Figure 3: The Model of Other Agents (MOA) archi-tecture learns both an RL policy πe, and a supervisedmodel that predicts the actions of other agents, at+1.
Figure 4: Causal diagram in the MOA case.
Figure 5: The two SSD environments, Cleanup (left) and Harvest (right). Agents can exploit other agentsfor immediate payoff, but at the expense of the long-term collective reward of the group.
Figure 6: Total collective reward obtained in all experiments. Error bars show a 99.5% confidence interval(CI) over 5 random seeds, computed within a sliding window of 200 agent steps. The models trained withinfluence reward (red) significantly outperform the baseline and ablated models.
Figure 7: A mo-ment of high influ-ence when the pur-ple influencer sig-nals the presence ofan apple outside theyellow influencee’sfield-of-view (yellowoutlined box).
Figure 8: Metrics describing the quality of learned communication protocols.
Figure 9: Schelling diagrams for the two social dilemma tasks show that an individual is almost alwaysmotivated to defect, even though the group will get higher reward if there are more cooperators.
Figure 10: The Box trapped environment in whichthe teal agent is trapped, and the purple agent canrelease it with a special open box action.
Figure 11: The Box trapped proof-of-concept experiment reveals that an agent gets high influence forletting another agent out of a box in which it is trapped.
Figure 12: A moment ofhigh influence between thepurple influencer and ma-genta influencee.
Figure 13: Total collective reward times equality, R*(1-G), obtained in all experiments. Error bars showa 99.5% confidence interval (CI) over 5 random seeds, computed within a sliding window of 200 agentsteps. Once again, the models trained with influence reward (red) significantly outperform the baseline andablated models.
Figure 14: Total collective reward over the top 5 hyperparameter settings, with 5 random seeds each, forall experiments. Error bars show a 99.5% confidence interval (CI) computed within a sliding window of200 agent steps. The influence models still maintain an advantage over the baselines and ablated models,suggesting the technique is robust to the hyperparameter settings.
Figure 15: Total collective reward obtained by agents trained to optimize for the collective reward, for the 5best hyperparameter settings with 5 random seeds each. Error bars show a 99.5% confidence interval (CI)computed within a sliding window of 200 agent steps.
