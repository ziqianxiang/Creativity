Figure 1: Overall canvas-drawer architecture.
Figure 2: Canvas network architecture and training setup.
Figure 3: Drawer network expanded for n=3 timesteps.
Figure 4: A sliding drawer net-work slides its receptive fieldacross small sections of a largerimage, producing action se-quences for each section inde-pendently.
Figure 5: Structure of a singlebezier curve.
Figure 6: MNIST and Omniglot symbol recreation with bezier curve draw-ing networks. Black-background images refer to pixel outputs, while white-background images showcase parametrized bezier curves rendered in red.
Figure 7: Training curves of various Canvas-Drawer settings. Canvas-Drawer regularly outperformsa typical RL agent in sample efficiency and accuracy, even when accounting for the training time ofthe canvas network.
Figure 8: High-resolution sketch recreation using sliding drawer. Black backgrounds representpixel output, and white backgrounds showcase produced bezier curves in red. Examples fromAO (2018); Kushidama Minaka (2017)(a)	Hierarchical drawer results.
Figure 9: Hierarchical drawers learn smoother, more connected drawingbehaviors than non-hierarchical variants.
Figure 10: Out-of-distribution test cases on a drawer network trained on the sketches dataset.
Figure 11: Canvas-drawer pairs are able to translate between grayscaleMNIST pixels and colored bezier curves, and between architectural floor-plans and bounding-box room segmentations.
Figure 12: Drawer networks can recreate rectangular prisms in 3D space from a set of 2D observa-tions, without paired data.
