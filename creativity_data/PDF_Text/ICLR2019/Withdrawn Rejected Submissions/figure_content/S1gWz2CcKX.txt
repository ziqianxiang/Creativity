Figure 1: Neural MMO pipeline. We alternate between collecting experience across 100 procedu-rally generated worlds and updating agents’ parameters via policy gradients. Test time visualizationprovides insight into the learned policies through value function estimates, map tile visitation distri-bution, and agent-agent dependencies.
Figure 2: Left: Foraging agents learn to efficiently balance their food and water levels while com-peting with other agents for resources. Center: Combat agents learn the latter while also balancingmelee, range, and mage attack styles to engage with and outmaneuver other agents. Right: Graphicskey for tiles and agents.
Figure 3: Maximum population size at train time varies in 16, 32, 64, 128. At test time, we mergethe populations learned in pairs of experiments and evaluate lifetimes at a fixed population size. Re-gardless of population size at test time, we find that agents trained in larger populations outperformagents trained in small populations-4.JU ① EBUJrIOl U-① LU-①ΞiOpponent Population Size at Train TimeAOUφnbaJLLUoAElω>Figure 4: Population size magnifies exploration: agents spread out to avoid competition.
Figure 4: Population size magnifies exploration: agents spread out to avoid competition.
Figure 5: Populations count (number of species) magnifies niche formation. Visitation maps areoverlaid over the game map; different colors correspond to different species. Training a singlepopulation tends to produce a single deep exploration path. Training 8 populations results in manyshallower paths: populations spread out to avoid competition among species.
Figure 6: Exploration maps in the environment randomized settings. From left to right: populationsize 8, 32, 128. All populations explore well, but larger populations with more species develop morerobust policies, utilize the map more efficiently, and thereby do better in tournaments.
Figure 8: Attack maps and niche formation quirks. Left: combat maps from automatic and learnedtargeting. The left two columns in each figure are random. Agents with automatic targeting learn tomake effective use of melee combat (denoted by higher red density). Right: noisy niche formationmaps learned in different combat settings with mixed incentives to engage in combat.
Figure 7: Agents learn to depend on other agents. Each square map shows the response of an agentof a particular species, located at the square’s center, to the presence of agents at any tile aroundit. Random: dependence map of random policies. Early: ”bulls eye” avoidance maps learned afteronly a few minutes of training. Additional maps correspond to foraging and combat policies learnedwith automatic targeting (as in tournament results) and learned targeting (experimental, discussed inAdditional Insights). In the learned targeting setting, agents begin to fixate on the presence of otheragents within combat range, as denoted by the central square patterns.
Figure 9: We procedurally generate maps by thresholding an 8 octave Perlin (Perlin, 1985) ridgefractal. We map tile type to a fixed range of valuesAppendix7	FrameworkThis section contains full environment details that are useful but not essential to understandingthe base experiments. All parameters in the following subsystems are configurable; we provideonly sane defaults obtained via balancing.
Figure 10: Agents observe their local environment. The model embeds this observations and com-putes actions via corresponding value, movement, and attack heads. These are all small fully con-nected networks with 50-100k parameters.
