Figure 1: The support of the activation matrix is determined by ReLU threshold. (a) When allthe rowS of the SUPPort are identical, there is a sub-weight-matrix such that the layer is fully linearW ith res pec 11o th e inpu t batch. (b, c) As the support becomes more complex, which we characterizeb y th e increase in its rec tan gle cover number, the layer becomes more non-linear.
Figure 2: Memorization mechanism of CNNs trained on CIFAR-10, with increasing level of labelrandomization p (i.e., p = 0 is the unmodified dataset). We analyze each layer by applying NMFcompression to its activation matrix with increasing rank k, while observing the impact on classi-fication performance. In (a) and (b) we show the k vs. accuracy curves at an layers of differentdepth. We can immediately see that in deep layers, networks with high memorization are signif-icantly less robust to NMF compression, indicating higher degrees of non-linearity. Furthermore,networks trained on fully randomized labels (p = 1) behave differently than networks with partial orno randomization. By summarizing each curve in (a) and (b) by its area under the curve (AuC), weshow in (c) a birds-eye view over all layers. All networks with p < 1 pass through distinct phasesconsisting of a feature extraction phase until conv3_1, followed by a memorization phase untilconv4_2, followed by a final clustering phase. Interestingly, the case p = 1 shifts the process intoearlier layers, explaining why layer-by-layer it appears as an outlier.
Figure 3: Detecting memorization via compression. We demonstrate this on networks trainedwith different levels of label randomization (p), and hence of memorization. (a) Due to its sensitiv-ity to the non-linearity of ReLU activations, NMF successfully captures the level of memorizationpresent in neural networks. (b) PCA compression is able to regain sufficient variance for goodaccuracy even with small values of k, which renders it less effective for detecting memorization.
Figure 4: Memorization across various datasets and network architectures. We show thatNMF-based compression is sensitive to memorization in diverse settings. Each column shows re-sults fora specific dataset and network architecture. (a, b, d, e) We show that network layers are con-siderably more linear with respect to single-class batched than with respect to multi-class batches.
Figure 5: Detecting generalization via compression. While all three methods show correlationwith generalization error, NMF is most correlated with a Pearson correlation of -0.82, followed byPCA with -.064 and random ablation with -0.61.
Figure 6: Early stopping for CNN training on CIFAR-10. (a, b) The test loss is (in blue) starts toincrease after about the 5th epochs, indicating the start of overfitting. Using our proposed single-class NMF approach, we can detect the test loss turning point. We show the area under the curve(AuC) for the single-class NMF approach (in green) for the accuracy measures as discussed inSection 4.1.2. Similarly, we show the AuC when performing random ablations (in orange). (c) TheNMF AuC curve and test loss curve consistently have near extrema, as seen over several runs.
Figure 7: NMF and PCA directions are more important for networks with induced-memorization. Each column shows results for a specific dataset and network architecture. Com-pared to random ablation as in Figures 3 and 4, where networks with induced-memorization aremost robust, in all cases we see that ablation in NMF and PCA directions hurts their performancemore, compared to memorizing networks.
Figure 8: NMF compression on VGG-19. (a) Deep VGG layers are highly linear with respect tosingle-class batches, as indicated by high accuracy for small dimensions of k. Compression can havea denoising effect and improve upon the baseline accuracy of the batch (dashed line). (b) RemovingNMF directions causes a dramatic drop in accuracy, more so on single-class batches. (c) Per-classtest set accuracy is significantly correlated with the area under the k vs. accuracy curve (NMF AuC).
Figure 9: NMF early stopping for few-shot learning of MNIST digits with only 20 samples. Byobserving the training set accuracy under NMF k = 1 compression, we are able to correctly guessthe gradient of the test set accuracy. We use this to perform early stopping with the simple heuristicof stopping at the first peak, which leads to improved accuracy as shown in Tabel 2.
Figure 10:	NMF reconstruction error and extreme memorization. Layer-by-layer view of (a)raw and (b) normalized NMF reconstruction errors, which NMF is trying to minimize. As we againnotice the outlier behavior of networks trained with label randomization p = 1, in (c) we localizethe transition between the two regimes to around p = 0.9.
Figure 11:	NMF runtime on a typical ImageNet batch. Thanks to GPU acceleration, NMF withmultiplicative updates can be run to convergence in reasonable time.
