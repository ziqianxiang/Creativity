Figure 1: Network structure diagram for applying multi-scale stylization. (a) The cascade networkscheme transfers style of an image into that of a target style image by using single scale transformers(SST) scale by scale. (b) Our multi-scaled style transformer (MST) transfers multi-scaled styles ina feed-forward process by using internal SSTs and a skip-connected decoder.
Figure 2: Correlation between channels in the multi-scaled feature map of the input image (a) ex-tracted from the pre-trained VGG16 Simonyan & Zisserman (2014). The area corresponding to eachscale of feature map is divided into red lines. In case of intra-scale feature transform, the diagonalrectangles on the correlation matrix are used. In case of inter-scale feature transform, entire regionof the correlation matrix is considered.
Figure 3: Each diagram shows the process of merging multi-scale features and dividing them intothe original size for inter-scale feature transform. (a) Merging multi-scale features is performedas upsampling each scale feature by using nearest neighborhood interpolation to the largest sizefollowed by concatenating them along the channel axis. (b) After feature transform, the transformedfeature is split into multi-scale features and downsampled into its original size. Each split feature isinserted into the decoder layer of the corresponding scale by using skip-connectionthe input and a target style image. Their method can transform an image in any target style by atime consuming online optimization process. Li et al. Li et al. (2017a) interpreted the process ofgenerating a stylized image by matching Gram matrix Gatys et al. (2016) as a problem of maximummean discrepancy (MMD) specifically with a second-order polynomial kernel.
Figure 5: Output stylized images according to the number of skip-connections: The (content, style)losses of output images are (c) (0.910, 0.535), (d) (0.707, 0.512), and (e) (0.814, 0.497). The moreskip-connections are used, the less style loss is acheived. And there is a trade-off between style andcontent losses.
Figure 6: Amplitude of loss gradients with resprct to the convolution weights in the skip-connecteddecoder layers during the learning process: The gradients are drawn every 500 iterations. Theformer half of the channels are for decoded feature from the previous scale and the latter are forskip-connected feature from transformer. Based on the gradients of 1st skip-connected layer (a) and2nd skip-connected layer (b), the skip-connected (transformed) feature highly seems to affect to thedecoder in initial interations but both decoded and transformed features samely affect as iterationgoes. And the latter decoder layer (b) is less affected by the skip-connected feature than the formerlayer (a).
Figure 7: Output stylized images of the existing methods and our method: The target style imagesare not used for network learning.
Figure 8: Comparison of single-network-multi-scale transform methods (Avatar-Net Sheng et al.
