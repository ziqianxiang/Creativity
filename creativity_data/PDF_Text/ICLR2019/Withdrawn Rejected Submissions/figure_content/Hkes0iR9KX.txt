Figure 1: The architecture of the network we used in the numerical experiments. mlp stands formulti-layer perceptron, numbers in bracket are layer sizes. Batchnorm is used for all layers withReLU. Dropout layers are used for the last mlp.
Figure 2: Left: The distribution of the nodes of a graph with 6 nodes in the feature space. Right:This figure represents two graphs. The circles represent clusters (not nodes). Each cluster containsmany nodes. The clusters in the right graph form a loop but in the left graph they do not.
Figure 3: The architecture of the network we used in the numerical experiments for graph clus-tering. The feature vector of each node after the third Feature Aggregation is concatenated withthe global feature obtained with the Global Pooling layer. Therefore, the network is aware of therole of each node in the global structure of the graph. The last mlp block classifies all the nodesindependently.
