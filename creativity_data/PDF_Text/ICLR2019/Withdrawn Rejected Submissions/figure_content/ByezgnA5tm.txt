Figure 1: No-ID-dithering constraint: .* ('r)2action sequences in order to learn to avoid constraint violations while maximizing discounted reward;and (3) experimental results comparing our approaches (trained with reward shaping) to a baselinemethod trained without knowledge of the constraints. We found that our approaches are effective insignificantly reducing, and even eliminating, constraint violations while maintaining high reward.
Figure 2: Architecture used in experiments.
Figure 3: Results for Breakout: (left) no-1D-dithering; (right) no-overactuating.
Figure 4: Results for Space Invaders: (left) no-1D-dithering; (right) no-overactuation.
Figure 5: Results for Seaquest: (left) no-2D-dithering; (right) no-overactuation.
Figure 6: Architecture used in our experiments.
