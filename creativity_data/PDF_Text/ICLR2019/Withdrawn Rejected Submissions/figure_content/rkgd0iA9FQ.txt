Figure 1: Optimally tuned parameters for different ξ values. 1 hidden layer network of 1000 nodes;Left: Loss on training set; Middle: Loss on test set; Right: Gradient norm on training setScipy library function scipy.sparse.linalg.eigsh that can use a function that computesthe matrix-vector products to compute the eigenvalues of the matrix (Lehoucq et al., 1998). Thus,for finding the eigenvalues of the Hessian, it is sufficient to be able to do Hessian-vector products.
Figure 2: Tracking the smallest eigenvalue of the Hessian on a 1 hidden layer network of size 300.
Figure 3: Full-batch experiments on a 3 hidden layer network with 1000 nodes in each layer; Left:Loss on training set; Middle: Loss on test set; Right: Gradient norm on training setFigure 4: Mini-batch experiments on a network with 5 hidden layers of 1000 nodes each; Left: Losson training set; Middle: Loss on test set; Right: Gradient norm on training set5.4	Corroborating the full-batch behaviors in the mini-batch settingIn Figure 4, we show how training loss, test loss and gradient norms vary when using mini-batchesof size 100, on a 5 hidden layer autoencoder with 1000 nodes in each hidden layer trained on thefull MNIST dataset. The same phenomenon as here has been demonstrated in more such mini-batchcomparisons on autoencoder architectures with varying depths and widths in Appendix D.3 and onVGG-9 with CIFAR-10 in Appendix E.
Figure 4: Mini-batch experiments on a network with 5 hidden layers of 1000 nodes each; Left: Losson training set; Middle: Loss on test set; Right: Gradient norm on training set5.4	Corroborating the full-batch behaviors in the mini-batch settingIn Figure 4, we show how training loss, test loss and gradient norms vary when using mini-batchesof size 100, on a 5 hidden layer autoencoder with 1000 nodes in each hidden layer trained on thefull MNIST dataset. The same phenomenon as here has been demonstrated in more such mini-batchcomparisons on autoencoder architectures with varying depths and widths in Appendix D.3 and onVGG-9 with CIFAR-10 in Appendix E.
Figure 5: Fixed parameters with changing ξ values. 1 hidden layer network of 1000 nodes(c) Gradient norm on training set24Under review as a conference paper at ICLR 2019D Additional ExperimentsD.1 ADDITIONAL FULL-BATCH EXPERIMENTS ON 22 × 22 SIZED IMAGESIn Figures 6, 7 and 8, we show training loss, test loss and gradient norm results for a variety ofadditional network architectures. Across almost all network architectures, our main results remainconsistent. ADAM with β1 = 0.99 consistently reaches lower training loss values as well as bettergeneralization than NAG.
Figure 6: Loss on training set; Input image size 22 × 22(b) 3(a) 1 hidden layer; 1000 nodeshidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodes(d) 3 hidden layers; 300 nodes(e) 3 hidden layer; 3000 nodes(f) 5 hidden layer; 300 nodesFigure 7: Loss on test set; Input image size 22 × 2225Under review as a conference paper at ICLR 2019(a) 1 hidden layer; 1000 nodes(b) 3 hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodeseach	each(d) 3 hidden layers; 300 nodes(e) 3 hidden layer; 3000 nodesFigure 8: Norm of gradient on training set; Input image size 22 × 22(f) 5 hidden layer; 300 nodes26Under review as a conference paper at ICLR 2019D.2 Are the full-batch results consistent across different input dimensions?
Figure 7: Loss on test set; Input image size 22 × 2225Under review as a conference paper at ICLR 2019(a) 1 hidden layer; 1000 nodes(b) 3 hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodeseach	each(d) 3 hidden layers; 300 nodes(e) 3 hidden layer; 3000 nodesFigure 8: Norm of gradient on training set; Input image size 22 × 22(f) 5 hidden layer; 300 nodes26Under review as a conference paper at ICLR 2019D.2 Are the full-batch results consistent across different input dimensions?To test whether our conclusions are consistent across different input dimensions, we do two experi-ments where we resize the 22 × 22 MNIST image to 17 × 17 and to 12 × 12. Resizing is done usingTensorFlow's tf.image.resize_images method, which uses bilinear interpolation.
Figure 8: Norm of gradient on training set; Input image size 22 × 22(f) 5 hidden layer; 300 nodes26Under review as a conference paper at ICLR 2019D.2 Are the full-batch results consistent across different input dimensions?To test whether our conclusions are consistent across different input dimensions, we do two experi-ments where we resize the 22 × 22 MNIST image to 17 × 17 and to 12 × 12. Resizing is done usingTensorFlow's tf.image.resize_images method, which uses bilinear interpolation.
Figure 9:	Full-batch experiments with input image size 17 × 17D.2.2 INPUT IMAGES OF SIZE 12 × 12Figure 10 shows results on input images of size 12 × 12 on a 3 layer network with 1000 hiddennodes in each layer. Our main results extend to this input dimension as well. ADAM with β1 =0.99 converges the fastest as well as generalizes the best, while NAG does better than ADAM withβ1 = 0.9.
Figure 10:	Full-batch experiments with input image size 12 × 1227Under review as a conference paper at ICLR 2019D.3 ADDITIONAL MINI-BATCH EXPERIMENTS ON 22 × 22 SIZED IMAGESIn Figure 11, we present results on additional neural net architectures on mini-batches of size 100with an input dimension of 22 × 22. We see that most of our full-batch results extend to the mini-batch case.
Figure 11:	Experiments on various networks with mini-batch size 100 on full MNIST dataset withinput image size 22 × 22. First row shows the loss on the full training set, middle row shows theloss on the test set, and bottom row shows the norm of the gradient on the training set.
Figure 12: Mini-batch image classification experiments with CIFAR-10 using VGG-9(b) Test set accuracyTo test whether these results might qualitatively hold for other datasets and models, we train animage classifier on CIFAR-10 (containing 10 classes) using VGG-like convolutional neural networks(Simonyan & Zisserman, 2014). In particular, we train VGG-9 on CIFAR-10, which contains 7convolutional layers and 2 fully connected layers, a total of 9 layers. The convolutional layerscontain 64, 64, 128, 128, 256, 256, 256 filters each of size 3 × 3, respectively. We use batchnormalization (Ioffe & Szegedy, 2015) and ReLU activations after each convolutional layer, andthe first fully connected layer. Table 1 contains more details of the VGG-9 architecture. We useminibatches of size 100, and weight decay of 10-5. We use fixed step sizes, and all hyperparameterswere tuned as indicated in Section B.
