Figure 1: Fully connected neural networks on MNIST-Back-Rand dataset. Left panel: training loss vs. number ofiterations. Right panel: training loss only for 5 epochs. One epoch means all training data points are used once. Forthis dataset, since the training size is 12000 and the mini-batch size is 128, one epoch means 93.75 iterations. Thisallows us to better visualize the speed in the first few epochs. In large applications, data-loading is often the bottleneck.
Figure 2: CIFAR10 c96-c96-c96-c192-c192-c192-c192-c192-c10 ConVNet with dropout. Left panel: training loss vs.
Figure 3: Embedding-LSTM100-f100 RNN: on the left is the training loss against number of iterations. On the rightis the plot for first 5 epochs.
Figure 4: The training performance on two datasets for different r values.
Figure 5: Training loss of new-optimistic-adam. Left panel: MNIST-Back-Rand. Right panel: CIFAR10 (CNN).
