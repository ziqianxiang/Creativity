Figure 1: Unambiguous classification of non-orthogonal quantum data sampled from different prob-ability distributions. The data Were averaged over 10 repeated trails starting With random initial-izations and the bars indicate the standard deviations. The training, validation and test data Weresampled from the corresponding distributions. The data size indicates the size of the training andthe validation set. The test data Was fixed at a size of 104 for each family and each distribution.
Figure 2:	With different penalties we observed a trade-off between the error rate and the in-conclusive rate. Compared with the point αerr = αinc = 0 (bottom left corner), the added penaltiesimproved the success probability or the inconclusiveness respectively. (a)-(c): The gradual transi-tion from the unambiguous classification (near-zero error rate, top left corner) to a minimal errorclassification (near-zero inconclusiveness, bottom right corner) with changes in the error penaltyαerr and the inconclusiveness penalty αinc. We observed that the gain in the success rate was around0.32 when we made a sacrifice of only 0.1 in the error rate. The data was tested on a ∈ [0, 1],and averaged over 50 repeated trails with random initializations. (d): Standard deviation for Psuc.
Figure 3:	Unambiguous discrimination of data sampled from different probability distribu-tions with higher success rate. (a) Trained quantum circuits were capable of classifying quantumdata which was sampled from a variety of different mixed probability distributions for ρ1 (a) andρ2(b). The classification was done in an unambiguous manner (with error rate < 0.01). (b) Forcomparison, we included here the theoretical result mentioned in Table 2.
Figure 4: The cost function after 5000 iterations. The result obtained using exact probabilitiesis shown by the horizontal dashed line. For a smaller step size (ε) for gradient calculations, wefound that more repetitions were required to give a consistent result. However, a combination ofε = 10-2 and 105 repetitions gave a result which well approximated the result obtained using exactprobabilities. Here repeat is the number of repeated measurements that were made each time tocalculate the cost function. The cost function values were averaged over 50 repeated runs of thetraining process, and the bars indicate the standard deviations.
Figure 5: Small learning rates with high number of iterations.
