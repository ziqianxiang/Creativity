Figure 1: A, B , C represent differenttasks. Vθ L denotes the gradients with respectto θ. (a). Gradient Passing within a task. (b).
Figure 3: Gradient Passing for Multi-task Communica-tion. The superscript s and p represent parameters whichcan be shared or not (private).
Figure 4: Performances of SR (Star-network), ASR (Adversarial Star-network) and LGP-SR (Star-network with list-wise gradient passing) models with different numbers (from 100 to 1000) of trainingsamples on five tasks. The darker grey horizontal line shows the performances in single-task setting(with 1600 training samples).
Figure 5: Evolution of parameters during training phase projected into 3D space using PCA forSR and PGP-SR models. z-axis describes different epochs. Cyan and green lines represent theevolution of private parameters in task “electronics” and “books”, while the red line denotesthe evolution of parameters shared by these two tasks.
Figure 6: (a) Single task Models. (b) Hard-sharing Multi-task Models. (c) Soft-sharing Multi-taskModels.
