Figure 1: Examples of the decision axis Λ2, shown here in green, for different data manifolds.
Figure 2: As the dimension increases, the rch2 (Λ∞; Si ∪ S2) decreases, and so an ∣∣ ∙ ∣∣∞ robustclassifier is less robust to ∣∣ ∙ ∣2 attacks. The dashed lines are placed at 1∕√d, where our theoreticalresults predict We should start finding ∣∣ ∙ ∣∣2 adversarial examples. We use the robust ∣∙∣∞ loss ofWong & Kolter (2018)We say a decision boundary Df for a classifier f is e-robust in the ∣∣ ∙ ∣∣p norm if e < rchp Df. Inwords, starting from any point x ∈ M, a perturbation ηx must have p-norm greater than rchp Dfto cross the decision boundary. The most robust decision boundary to ∣∣ ∙ ∣∣p-perturbations is Λp. InTheorem 1 we construct a learning setting where Λ2 is distinct from Λ∞. Thus, in general, no singledecision boundary can be optimally robust in all norms.
Figure 3: Left: To construct an δ-cover We place sample points, shown here in black, along aregular grid with spacing ∆. The blue points are the furthest points of Π from the sample. To coverΠ we need ∆ = 2δ∕√k. Right: An illustration of the lower bound technique used in Equation 3.
Figure 4: We plot the upper bound in Equation 4 on the left. As the codimension increases, thepercentage of volume of Π1 covered by 1-balls around the 1-sample approaches 0. On the right Weplot the number of samples necessary to cover Π, shown in blue, against the number of samplesnecessary to cover Π1, shown in orange, as the codimension increases.
Figure 5: As the codimension increases the robustness of decision boundaries learned by Adam onnaturally trained networks decreases steadily. Left: Effectiveness of FGSM attacks as codimensionincreases. Center: BIM attacks. Right: Training using the adversarial training procedure of Madryet al. (2018) is no guarantee of robustness; as the codimension increases it becomes easier to findadversarial examples using BIM attacks. Appendix B.4 shows the performance on nearest neighboron this data, which is essentially perfect accuracy for all e.
Figure 6: Robustness of nearest neighbors on MNIST. Left: Performance on l∞ BIM attack againsta naturally trained model. Center: The same for the adversarially trained convolutional models ofMadry et al. (2018). Right: Performance of the robust model and nearest neighbors on examplesgenerated by a custom attack on nearest neighbors.
Figure 7: As in the case of training with Adam, we see a steady decrease in robustness on theCircles dataset as the codimension increases when training with SGD.
Figure 8: Adverarial training with a PGD adversary, as in Figure 5, using SGD. Similarly we see adrop in robustness as the codimension increases.
Figure 9: Robustness of nearest neighbors against the naturally trained (left) and adversariallytrained (right) convolutional models of Madry et al. (2018) against FGSM attacks under the ∣∣∙∣∣∞norm on MNIST.
Figure 10: Histograms of the angle deviations of FGSM perturbations from the normal space forthe Circles dataset in codimensions 1 (upper right), 10 (upper left), 100 (lower left), 500 (lowerright). Nearly all perturbations make an angle of less than 20。with the normal space.
Figure 11:	The FGSM (left) and BIM (right) perturbations that fooled our deep networks are cor-rectly classified by a nearest neighbor classifier. Nearest neighbor classifiers are robust in highcodimension settings because their decision boundaries are elongated in the directions normal to thedata manifold.
Figure 12:	The BIM perturbations that fooled the adversarially trained model using the proceduresuggested by Madry et al. (2018) are correctly classied by a nearest neighbor classifier.
Figure 13: Adversarial training OfMadry et al. (2018) on the Planes dataset with a 1-cover (left),consisting of 450 samples, a 0.5-cover (center), 1682 samples, and a 0.25-Cover (right), 6498 sam-ples. Increasing the sampling density improves robustness at the same codimension. However eventraining on a significantly denser training set does not produce a classifier as robust as a nearestneighbor classifier on a much sparser training set, Figure 12for each point. If y ∈ Xtest projected to a point y0 that the model classified differently than x, wetake ηx = y0 - x, otherwise ηx = 0. This incredibly simple attack reduces the accuracy of thepretrained robust model of Madry et al. (2018) for ∣∣ ∙ k∞ and e = 0.3 to 90.6%, less than twopercent shy of the current SOTA for whitebox attacks, 88.79% (Zheng et al. (2018)).
Figure 14: (Left) The normalized gradient field of the loss for an adversarially trained network.
Figure 15: Adverarial examples generated using PGD (left), FGSM (center), and BIM (right). Whilethe network is robust to PGD attacks, FGSM and BIM attacks are more effective because they ignorethe magnitude of the gradient. For PGD we draw arrows from the test sample to the adversarialexample generated from that point to aid the reader.
Figure 16: Three different perspectives on our upper bound in Equation 12. (Left, Center) In eachcase the percentage of S£ covered by X£ goes to 0. (Right) The number of points necessary to coverS' by X' grows exponentially with the dimension.
Figure 17: The Voronoi diagram ofa set of points in R2 (left) and its dual the Delaunay triangulation(right).
Figure 18: We create two synthetic datasets which allow us to perform controlled experiments onthe affect of codimension on adversarial examples.
Figure 19: The training set lies entirely in the xy-plane, shown here at Z = 0. We visualize cross sec-tions of the decision boundary for z ∈ [-5, 5] for various optimization algorithms training for differ-ent lengths of time. The results show how various optimization algorithm learn decision boundariesthat extend into the normal directions in which there is no data provided. We average the decisionboundary over 20 retrainings, so faded results indicated how frequently a point was labeled a specificclass.
Figure 20: Comparison of adversarial examples for nearest neighbor with adversarial examplesfor Madry et al. (2018). The top row is the original data that the examples were generated from.
