Figure 1: An illustration of the first QRNN layer for language modeling. In this visualization, aQRNN layer with a window size of two convolves and pools using embeddings from the input. Notethe absence of recurrent weights.
Figure 2: Full experimental results on Penn Treebank and WikiText-103. We illustrate thePerplexity-efficiency tradeoff space on the test set obtained before applying the single-rank update.
Figure 3: Illustration depicting pruning on a truncated subset of the first layerâ€™s weights from thePTB model, where each row corresponds to a different technique, and each column a differentoperating point. From left to right, the operating points are 100%, 80%, 70%, 60%, and 50% FLOPs.
Figure 4: Visualization of the relative changes in per-word R@3 on the test set in the pruned models.
