Figure 1: The ARMIN structure. At each time-step, the ARMIN performs read operation, cellprocessing and write operation in chronological order: (a) It reads out a historical hidden state rtfrom memory with an one-hot read vector produced via passing Xt and ht-ι to a fully connectedlayer followed by a gumbel-softmax function. (b) The ARMIN cell receives xt, ht-ι and rt asinputs and outputs ot and ht. ot is passed to output layers, and ht is passed to next time-step. (c)ht is written to the previous location of rt.
Figure 2: Comparison of proposal generation performance. For data generation, we use the codeoffered in Buch et al. (2017) for consistency. (Left) The average number of proposals v.s. averagerecall for tIoU ≥ 0.5. The MA-SST outperforms our SST when average number of proposals is lessthan 500, and outperforms the original SST when average number of proposals is more than 500.
Figure 3: The running speed and memory consumption at the training and inference stages(nmem = 20). The solid blocks shows the memory consumption and the curves denote the run-ning speed in characters/s. (a) shows the training stage, and (b) shows the inference stage (notethat ARMIN+TARDIS-addr has basically the same memory consumption with ARMIN, so it,s notshown in the graph).
Figure 4: Copy task.
Figure 5: Repeat copy task.
Figure 6: Associative recall task.
Figure 7: Priority sort task.
