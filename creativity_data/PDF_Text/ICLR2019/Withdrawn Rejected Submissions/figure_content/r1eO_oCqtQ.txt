Figure 1: Comparison of the LSTM and g-LSTM models. K is the computational block for the timegate in (b).
Figure 2: Test loss and test label error rates across training epochs for LSTM (black) and g-LSTM(blue) networks on different tasks.
Figure 3: Time gate behavior pre and post training, demonstrating the ability of the netWork to learnfrom extreme initialization parameters. Here, kn is plotted as a function of time (x-axis) With blackvalues corresponding to a fully closed gate (value 0) and White values corresponding to a fully opengate (value 1). Note that loWer values of σ ensure that the unit is processed only if the time input isμ, while higher σ values lead to the unit processed like in the standard LSTM, at all times.
Figure 4: Time gate behavior of (a) g-LSTM and (b) budgeted g-LSTM for 110 units post training.
Figure 5: Reduction in computes as a func- Figure 6: Speed up in convergence of LSTM us-tion of threshold for budgeted g-LSTM. ing the temporal curriculum learning schedule.
Figure 7: Average gradient norm through time steps for g-LSTM and LSTM.
Figure 8: Additional experiments: timing gate openness for non-optimal initializations(e) Experiment A3: before training12Under review as a conference paper at ICLR 2019C	Comparing time gate parameter trainability in g-LSTM andPLSTMNetwork	Initialization	Final MSE Lossg-LSTM	μ 〜U(300, 700), σ = 40	7.7 ∙ 10-5PLSTM T = 1000, S 〜U(250, 650), r = 0.10	2.4 ∙ 10-4Table 4: Adding task (T=1000): Comparing 110 unit g-LSTM and PLSTM networks with similarinitializations, MSE computed after training for 500 epochs.
Figure 9: g-LSTM(a) before trainings-!m UOPPIq(b) after training (500 epochs)Figure 10:	PLSTM13Under review as a conference paper at ICLR 2019D Hyperparameter InvestigationWe look at the network performance for different hyperparameter values, focusing on the sMNISTtask.
Figure 10:	PLSTM13Under review as a conference paper at ICLR 2019D Hyperparameter InvestigationWe look at the network performance for different hyperparameter values, focusing on the sMNISTtask.
Figure 11:	Results for sMNIST with various optimizers and initializations.
Figure 12: Comparative results on sMNIST with chrono initialization and constant initialization.
Figure 13: Results for sMNIST using two different network sizes.
Figure 14: Label error rate on sMNIST for different λ values (shown in legend).
Figure 15: Gate openness for different λ values of the budgeted g-LSTM.
