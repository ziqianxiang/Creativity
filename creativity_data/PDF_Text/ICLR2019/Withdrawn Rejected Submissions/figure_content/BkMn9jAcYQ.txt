Figure 1: Diagram of our communication game.
Figure 2: Learning curves for PG, PG+LM and PG+LM+G. En LM NLL curves show the NLL of Englishmessages, computed by a language model trained on WikiText103. Lower En BLEU indicates more languagedrift, and higher En LM NLL indicates more language drift.
Figure 3: Token frequency analysis on three different models (PG, PG+LM, PG+LM+G) as well as the pre-trained model before any fine-tuning (Pretrained). We show the word frequency curves (sorted in decreasingorder) for each model, after subtracting the reference English frequency statistics (also sorted). Positive y valuesindicate higher frequency values than the English reference, and negative y values indicate lower frequencyvalues than English. Note that y-axis is the frequency difference in thousands, and x-axis shows the vocabularyindex (sorted with frequency) in log scale.
Figure 4: Token frequency analysis similar to Figure 3, but with the x-axis fixed to the token indices sortedwith respect to English reference, in decreasing order.
Figure 5: Token frequency curves (before subtracting the reference frequencies). Both x (vocabulary index)and y (frequency) axes are in log scale.
