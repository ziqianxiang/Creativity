Figure 1: Overall architecture of NaNTPs: the two main contributions consist in a faster inferencemechanism (represented by the K-NN OR component, discussed in Section 3) and two dedicatedencoders, one for KB facts and rules, and another for text (discussed in Section 4).
Figure 2: Given the Countries dataset, we replaced a varying number of training triples with mentions(see Appendix B.1.2 for details) and integrated the mentions using two different strategies: byencoding the mentions using the encoder introduced in Section 4 (Facts and Mentions) and by simplyadding them to the KB (Facts).Experiments were conducted with the attention mechanism proposedin Section 3 (right) and the standard rule-learning procedure (left), each with 10 different randomseeds. We can see that, on each of the datasets, using the encoder yields consistently better AUC-PRvalues than simply adding the mentions to the KB.
Figure 3: Run-time and memory performance of NaNTP in comparison with NTP. Run-time speedupcalculated as the ratio of examples per second of NaNTP and NTP. Memory efficiency calculated asa ratio of the memory use of NTP and NaNTP. Dashed line denotes equal performance - above it(green) NaNTP performs better, below it (red) performs worse.
