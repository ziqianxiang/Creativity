Figure 1: An overview of the architecture for our experiments how client code communicates with a PVar andhow the model for the PVar is trained and updated.
Figure 2: The cost of different variants of binary search (top left), cumulative regret compared to vanilla binarysearch (right), and initial function usage (bottom).
Figure 3:	Results from using a PVar for selecting the number of pivots in QuickSort. (a) shows the overallcost for the different baseline methods and for the variant with a PVar over training episodes. (b) shows thecumulative regret of the PVar method compared to each of the baselines over training episodes.
Figure 4:	Fraction of pivots chosen by the PVar in QuickSort after 5000 episodes. The expected approximationerror of the median is given in the legend, next to the number of samples.
Figure 5:	The architecture of the neural networks for TD3 with key embedding network.
Figure 6:	Cache performance for power law access patterns. Top: α = 0.1, bottom: α = 0.5. (a) Hit Ratio(w/o exploration) and (b) Cumulative Regret (with exploration)learned to predict constant offsets (which is trivial), however the discrete implementation actuallylearned to become an LRU CRP which is non-trivial. The continuous implementation with frequen-cies quickly outperforms the LRU baseline, making the cost/benefit worthwhile long-term (negativecumulative regret after a few hundred episodes).
