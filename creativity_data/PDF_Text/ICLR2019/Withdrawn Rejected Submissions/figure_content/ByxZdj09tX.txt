Figure 1: example of a simplified brain of a mosquitoImagine the brain of a mosquito consists of a canonical deep feedforward neural network with onlyone input layer, one hidden layer, and one output layer. The input layer consists of strategy input andenvironment input. And the output layer consists of payoff out. A list of neurons in the strategy inputrecords the strategies (denoted is) that the mosquito has adopted in response to certain environmentinformation (denoted in), which environment information is also simultaneously recorded by theother list of the neurons in the environment input as well. Meanwhile the neurons in the payoffoutput also instantaneously record the payoff (denoted o) to the mosquito when it adopts a certainstrategy is under a certain environment in . The concatenation of is and in is denoted as i.
Figure 2: example of the learning process in a mosquito’s brain2Under review as a conference paper at ICLR 2019When the brain (the neural network) of the mosquito is adequately trained (by using StochasticGradient Descent) according to the table in Figure 2, upon seeing a human’s hand at presence, thebrain (the neural network) of the mosquito must figure out and make deduction about what strategyit shall adopt in order to stay alive. In another word, how could the neural network figure outa strategy that, when put into the neural network along with environment input [1, 0, 0], cangenerate the payoff [1]?The answer that Deep Deducing gives is rather simple - JUst randomize the strategy input in theinput layer of the trained neural network and let the neurons that represent the strategy input toadjust itself gradually to render the neural network to reach output [1] by propagating the errorbetween the generated output and the desired output [1] back to the neurons in the strategy inputitself for fixed epochs or as long as the generated output (of the adjusted strategy input obtained inthe last epoch) does not adhere to [1] in a reasonable deviation.
Figure 3: example of the deduction process in a mosquito’s brain3Under review as a conference paper at ICLR 2019Finally, after fixed epochs, according to the preliminary experimental result shown in Figure 3, themosquito will obtain a strategy input that approximate [1, 0, 0] when argmaxed and one-hotted,which means the mosquito better just flies away. This final strategy information will later be sentto other parts of the mosquito to initiate the procedure of flying such as twitching wing tensors.
Figure 4: example of the learning process for prisonersSecond, since it is not a zero-sum game, we will tune (by one epoch) the neurons which representthe strategy input for player A to generate the desired payoff output for player A, namely [1, *](where “*” stands for whatever the payoff for player B already is), and then we will tune (by oneepoch) the neurons which represent the strategy input for player B to generate the desired output forplayer B, namely [*, 1] (where “*” stands for whatever the payoff for player A already is). We willkeep this procedure for fair epochs as shown in Figure 5.
Figure 5: example of the deduction process for prisonerscan also be dissected into two phases, namely the training or learning phase and the deduction phase.
Figure 6: trajectories of strategy inputs for player A and B corresponding to Table 2 (left)trajectories of strategy inputs for player B under fixed strategy corresponding to Table 2 (right)7Under review as a conference paper at ICLR 2019As we can see in the left part of Figure 15, regardless of the initially randomized starting points(denoted red dots) for the strategy inputs, the strategy inputs for player A and B converge to NE(denoted blue dots) in each table under Deep Deducing.
Figure 7: the learning or training process to smooth the optimization geometry for player B underlower left table in Table 2We can see the smoothness of geometry for the strategy inputs increases as the neural networklearns more about a game table and further lessens the chance of getting stuck at local minimumfor gradient descent and other types of optimization skills (see Figure 9). This explains why DeepDeducing can be attached to Deep Learning.
Figure 8: the trajectories of strategy inputs under different error surface for player A and B underlower left table in Table 28Under review as a conference paper at ICLR 2019The second possible explanation is that, In Kakutani’s fixed point theorem, a topology space keptmapped into itself (if closed, continuous) will converge to a point. This point is the fixed point underevery mapping. By Nash theorem, if the mapping is the pursuit of the best interest of each player,this point will be the Nash Equilibrium (as in “Introduction to Topology: Pure and Applied (Adams& Franzosa, 2008).”). In Figure 15, we can observe that the trajectory of strategy inputs for player Aand B under Deep Deducing algorithm converge to a certain point regardless of their initial differentposition. Therefore, by Kakutani’s fixed point theorem, there is a fixed point under every mapping.
Figure 9: accuracy for tracking down NEs in different learning epochsAs we can see, Deep Deducing is almost 100 % accurate in tables of size 2x2, however the precisionin tables of size 4x4 drops to 70%. This is because the learning process only lessens the chanceof getting stuck at local minimum, we are only using plain and modest gradient descent withBack Propagation, and the strategy inputs for player A and B might get stuck at ”common” localminimums in 4x4 tables, which minimum is the common minimum in the error surface of playerA and B respectively (so we call it ”common”) in which case we cannot expect ant A and B topull each other out of their local minimums. To solve this problem, we will apply a simplifiedgradient-based particle swarm optimization and transform Equation 2 into Equation 3a and 3b forplayer A and 4a and 4a for player B:mA — iA - iA-1	Ga)iA+1 — iA — αJΑE(odA - o；A) + αmA + α(iAoptimal - iA)	(3b)∂itmB ~ iB - iB-1	(4a)∂0iB+1 -" α∂iBE(OdB - OB) + αmB + Mit …al- iB)	(4b)where itAoptimal is the optimal solution in the strategy swarm for player A at time t, and vice versa.
Figure 10: accuracy for tracking down NEs in tables 4x4 in different learning epochs coupled withsimplified gradient-based swarm optimizationAs we can see in Figure 10, the accuracy improves to 80 % for 4x4 tables . Deep Deducing tracksdown NEs despite the fact that there is only one uncoupled algorithm.
Figure 11: game tree for sequential gameA strategy inlayer 1[1.0,0.0,00][1.070.0,0.0][0.0, 0.0,1.0][0.0,0.0,1.0]B strategy inlayer 2[1.0,0.0,0.0][0.070.0,1.0][1.0,0.0,0.0][0.070.0,1.0]actual payoff for Aand B[0.2, 0.5][05 0.2][0.0,00][θ-3, 0.1]Figure 12:	training or learning phase in Sequential Discrete Game
Figure 12:	training or learning phase in Sequential Discrete GameIn the deduction phase, we first randomize strategy input for A and B. Then, by using the notionof “Back Induction” in Game Theory (as in “Games of Strategy, Second Edition (Dixit & Skeath,2004).”), we tune the neurons in the strategy input for player B in layer 2 to meet the desired output[*,1] for player B, and then we tune the strategy input for player A in layer 1 to meet the desiredoutput [1,*] for player A. At last we repeat the process again, and the strategy inputs obtained in thelast epoch will be kept to the next epoch. The whole procedure can be shown in Figure 13.
Figure 13:	deduction phase in Sequential Discrete GameFinally, the final strategy input for player A in layer 1 in figure Figure 13 is the optimal strategy forplayer A in layer 1 in the real world. After player A chooses its strategy, in the next layer of the gamein the real world, player B can exploit the same technic to obtain the optimal strategy for himself orherself in layer 2. Both players will converge to the Nash Equilibrium (route) in Sequential DiscreteGame in Game theory in the end. The same methodology also applies to Sequential Discrete Gamewith arbitrary numbers of layers.
Figure 14:	the states of the play in experiment with Deep DeducingAs we can see, if adequately tuned, in some tricky first-hand advantages, Deep Deducing can avoidbad moves in order to survive.
Figure 15: transforming Feed-forward Neural Network into Recurrent Neural Network to performcrow reasoning by Deep DeducingTo testify the functionality of this method, we will start from teaching a machine to move points Ato points B in a 5x5 grid. We first train a deep feed-forward neural network to learn what positionin the grid the points will achieve when it pushes the present points to left, up, right, down or re-mains the points still. Then we will permute the deep feed-forward neural network into a recurrentneural network by Figure 15, making the hidden as well as the output layers as the states of the gridand the input layer as the direction where it moves the points. Then we set the position of pointsB as the goal where the machine is expected to push points A to match the status of points B. Bydeep deducing, the machine starts a crow-like reasoning and find the shortest route to push points15Under review as a conference paper at ICLR 2019A to where points B are located. The whole source code can be seen at: https://github.
Figure 16:	the recorded states of the movement of DFNN-transformed RNN under Deep DeducingInterestingly, for a pre-fixed number of steps taken into consideration in the future (which neednot to be the exact shortest number of steps to where points B is located), the machine mimics therationale of a crow and moves points A to the location of points B in the shortest route. The machinecan choose to remain still to avoid wasting unnecessary steps (to save space, only ”still” strategy isnot recorded in Figure 16). The DFNN-transformed RNN, when back-propagating error back to theinput layers sequentially, can make sequential deduction and combine the knowledge pieces learnedfrom the environment into a logical, functional and sequential reasoning to achieve its goal, whichis way too similar to that of a crow.
Figure 17:	the recorded states of the movement of DFNN-transformed RNN under Deep Deducingwith partial noiseInterestingly, DFNN-transformed RNN under Deep Deducing with partial noise can still find a routeto push points A to points B despite that fact that there are noise in the initial status of the board.
