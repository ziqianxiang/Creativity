Figure 1: Fraction of the gradient in the top subspace ftop, along with training loss and accuracy.
Figure 2: Overlap of top Hessian subspaces Vt(otp1) and Vt(otp2). (a) Top 10 subspace of fully-connectednetwork trained on MNIST. (b) Subspace spanned by the next 10 Hessian eigenvectors. (c) Top10 subspace of convolutional network trained on CIFAR10. (d) Subspace spanned by the next 10Hessian eigenvectors. (e) Top 10 subspace of ResNet-18 trained on CIFAR10. (f) Subspace spannedby the next 10 Hessian eigenvectors. The network architectures are the same as in Figure 1.
Figure 3: Subspace overlap of top Hessian subspaces Vt(otp1) and Vt(otp2) for different top subspacedimensions with different initial number of steps t1 averaged over the interval t2 - t1 for (a)fully-connected two-layer network trained on MNIST and (b) ResNet-18 architecture trained onCIFAR10. Note the kink around subspace dimension equal to one less than the number of classes inthe dataset.
Figure 4: Eigenvalues of the Hessian ofa fully-connected network with two hidden layers, each with32 neurons, trained on MNIST for 40 epochs. The top 10 largest eigenvalues are labeled and clearlyform a nontrivial tail at the right edge of the spectrum.
Figure 5: Histogram of eigenvalue density on the right edge of the Hessian spectrum for a fully-connected two-layer (256, 256) model trained on CIFAR100 averaged over 200 realizations.
Figure 6: Eigenvector corresponding to the maximal eigenvalue for the fully-connected (100,100)architecture trained on MNIST after (a) 0 steps, (b) 100 steps, (c) 200 steps, and (d) 400 steps.
Figure 7: Eigenvector corresponding to the maximal eigenvalue for the fully-connected (100,100)architecture trained on MNIST after (a) 0 steps and (b) 400 steps zoomed in on the top layer weightsand biases. These plots are strong evidence that eigenvector is clearly not dominated by any partic-ular parameter and is meaningfully changing in time.
Figure 8: Evolution of the maximal eigenvalue for (a) fully-connected (100,100) architecture trainedon MNIST and (b) ResNet-18 architecture trained on CIFAR10. Note the second plot has a log scaleon the y-axis.
Figure 9: Overlap of top Hessian subspaces Vt(otp1) and Vt(otp2) for fully-connected network trained onMNIST using the same architecture in Figure 1. (a) Top 2 subspace. (b) Top 5 subspace. (c) Top 9subspace. (d) Top 11 subspace. (e) Top 15 subspace. (f) Top 20 subspace.
Figure 10: Overlap of top Hessian subspaces Vt(otp1) and Vt(otp2) for ResNet-18 architecture trained onCIFAR10 as in in Figure 1. (a) Top 2 subspace. (b) Top 5 subspace. (c) Top 9 subspace. (d) Top 11subspace. (e) Top 15 subspace. (f) Top 20 subspace.
Figure 11: The overlap squared ci2 of the gradient with the ith eigenvector of the Hessian. Data isfor a fully-connected (100,100) architecture trained on MNIST for (a) 0 steps, (b) 50 steps, (c) 100steps, and (d) 200 steps. After 0 steps, we have Pi2=0 1 ci2 = .20 compared with Pi2=0 1 ci2 = .94 after50 steps. Also, note that after 50 steps we have Pi1=0 1 ci2 = .93 vs. Pi2=0 11 ci2 = .01. Together, theseresults show that that the gradient dynamically evolves to lie mostly in the top subspace and is notsimply an eigenvector of the Hessian.
Figure 12: Fraction of the gradient in the top subspace ftop . In experiments (a)-(e), we use a fully-connected network trained on MNIST, and in (f) we use a CovNet trained on CIFAR10. The changesfrom the setup described in Figure 1 are: (a) changed learning rate, η = .01 instead of η = 0.1. (b)changed batch size, 256 instead of 64. (c) no hidden layers, just softmax. (d) changed activation:softplus instead of ReLU. (e) random labels on MNIST. (f) changed optimizer, Adam instead ofSGD.
