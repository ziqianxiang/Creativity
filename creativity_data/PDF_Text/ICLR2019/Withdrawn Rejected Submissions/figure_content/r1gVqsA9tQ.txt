Figure 1: Chain generator model, with noise z, base generator Gb , and editors, Ei .
Figure 2: In this example, the first few editors significantly enhance the image of a car, but afterEditor 3, they tend to more subtle.
Figure 3: Randomly selected MNIST examples from our DCGAN with editors model at differentepochs.
Figure 4: (a) Inception scores across epochs of model 4 vs model 1 (b) Randomly selected CIFAR-10examples from editor 3 of Model 8.
Figure 5: Whereas backpropagation for traditional GANs (top) require retaining the entire generatorgraph, ChainGAN requires the graph of one randomly selected editor per iteration (bottom), whichhas a fraction of the number of parameters. Orange boxes represent the parameters involved in thebackpropagation of loss gradients.
Figure 6: CelebA samples. Editors progressively smooth irregularities of the initial generated im-ages.
Figure 7: Multiple critics share convolution parameters and differ only in the last linear layer. CriticDo is responsible for evaluating base generator Gb's output Xo, Di is responsible for evaluatingeditor Eis output X%, and so on.
Figure 8: Selected outputs from using ResNets for both the generator and 5 editors.
