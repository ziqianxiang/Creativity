Figure 1: HyPerGAN architecture. The mixer transforms S ~ S into latent codes {qι,..., qN}. Thegenerators each transform a latent subvector qi into the parameters of the corresponding layer in thetarget network. The discriminator forces Q(q|s) to be well-distributed and close to Pshort term memory store. Meta Networks (Munkhdalai & Yu, 2017) build on this aPProach byusing an external neural memory store in addition to multiPle sets of fast and slow weights. (Baet al., 2016) augment recurrent networks with fast weights as a more biologically Plausible memorysystem than Neural Turing Machines. Unfortunately, the generation of each Predicting networkrequires querying the base (slow) learner many times. Many of the methods Presented there, alongwith hyPerParameter learning (Lorraine & Duvenaud, 2018), and the original hyPernetwork, ProPoselearning target weights which are deterministic functions of the training data. Our method insteadcaPtures a distribution over Parameters, and Provides a cheaP way to directly samPle full networksfrom a learned distribution.
Figure 2: Results of HyperGAN on the 1D regression task. From left to right, we plot the predictivedistribution of 10, 100, and 1000 sampled models from a trained HyperGAN. Within each image,the blue line is the target function x3 , the red circles show the noisy observations, the grey line isthe learned mean function, and the light blue shaded region denotes ±3 standard deviations5	classes as out of distribution examples. To build an estimate of the predictive entropy we samplemultiple networks from HyperGAN per example, and measure their predictive entropy..
Figure 3: Empirical CDF of the predictive entropy on out of distribution datasets notMNIST, and5 classes of CIFAR-10 unseen during training. Solid lines denote tests on the respective out ofdistribution data, while the dashed lines denote entropy on inlier examples (MNIST and CIFAR-10).
Figure 4: Diversity of predictions on adversarial examples. FGSM and PGD examples are createdagainst a network generated by HyperGAN, and tested on 500 more generated networks. FGSMtransfers better than PGD, though both attacks fail to cover the distribution learned by HyperGANAdversarial examples have been shown to successfully fool ensembles (Dong et al., 2017), but withHyperGAN one can always generate significantly more models that can be added to the ensemblefor the cost of one forward pass, making it hard to attack against. In Fig. 4 we test HyperGANagainst adversarial examples generated to fool one network. It is shown that while those examplescan fool 50% - 70% of the networks generated by HyperGAN, they usually do not fool all of them.
Figure 6: Convolutional filters from MNIST classifiers sampled from HyperGAN. For each imagewe sample the same 5x5 filter from 25 separate generated networks. From left to right: figures aand b show the first samples of the first two generated filters for layer 1 respectively. Figures c andd show samples of filters 1 and 2 for layer 2. We can see that qualitatively, HyperGAN learns togenerate classifiers with a variety of filters.
Figure 7: Top: MNIST examples to which HyperGAN assigns high entropy (outlier). Bottom:Not-MNIST examples which are predicted with low entropy (inlier)A.3 Diversity without Mixer or DiscriminatorWe run experiments on both MNIST and CIFAR-10 where we remove both the mixer and the dis-criminator. Tables 3 and 4 show statistics of the networks generated by HyperGAN using onlyindependent Gaussian samples to the generators. In this setting, HyperGAN learns to generate onlya very small distribution of parameters.
