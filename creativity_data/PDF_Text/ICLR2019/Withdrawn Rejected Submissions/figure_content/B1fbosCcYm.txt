Figure 1: The two layer Hebb-Rosenblatt memory architecture. This is a novel, differentiable moduleinspired by Rosenblatts perceptron that can be used to ‘learn’ a Hebbian style memory representationover a sequence and can subsequently be projected through to obtain a latent space.
Figure 2: The attentional working memory model which produces an update to the memory. SeeSection 3.2 for a description of the structure and function of each module.
Figure 3: The classification model which uses the terminal state of the memory.
Figure 4: The drawing model which uses intermediate states of the memory.
Figure 5: CIFAR-10 reconstructions for the baseline β-VAE and the drawing model with N = 8,Sg = 16. Top: baseline results. Middle: drawn results. Bottom: target images. Best viewed in colour.
Figure 6: Output from the drawing model for CelebA with N = 8, Sg = 32. Top: the drawn results.
Figure 7: Top: sketchpad results and associated predictions for a sample of misclassifications. Bottom:associated input images and target classes.
Figure 8: Canvas updates for the drawing model on MNIST with 12 glimpses of size 8, 6 and 4. Thefirst 6 rows give the drawing sequence (we omit the first 6 steps for brevity) and the bottom rowshows the target image. Best viewed in colour.
Figure 9: Canvas updates for the drawing model on CIFAR-10 with 8 glimpses of size 16. The first 8rows give the drawing sequence and the bottom row shows the target image. Best viewed in colour.
Figure 10: Canvas updates for the drawing model on CelebA with 8 glimpses of size 32. The first 8rows give the drawing sequence and the bottom row shows the target image. Best viewed in colour.
