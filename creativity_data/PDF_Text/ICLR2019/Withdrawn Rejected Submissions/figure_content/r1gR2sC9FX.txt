Figure 1: Left (a, b): Evolution of the spectrum (x-axis for frequency) during training (y-axis). The colorsshow the measured amplitude of the network spectrum at the corresponding frequency, normalized by thetarget amplitude at the same frequency (i.e. |九 |/Ai) and the colorbar is clipped between 0 and 1. Right (a,b): Evolution of the spectral norm (y-axis) of each layer during training (x-axis). Figure-set (a) shows thesetting where all frequency components in the target function have the same amplitude, and (b) where higherfrequencies have larger amplitudes. Gist: We find that even when higher frequencies have larger amplitudes,the model prioritizes learning lower frequencies first. We also find that the spectral norm of weights increasesas the model fits higher frequency, which is what we expect from Theorem 1.
Figure 2: Functions learned by two identical networks (up to initialization) to classify the binarized value ofa sine wave of frequency k = 200 defined on a γL=20 manifold. Both yield close to perfect accuracy for thesamples defined on the manifold (scatter plot), yet they differ significantly elsewhere. The shaded regions showthe predicted class (Red or Blue) whereas contours show the confidence (absolute value of logits).
Figure 3: (a,b,c,d): Evolution of the network spectrum (x-axis for frequency, colorbar for magnitude) duringtraining (y-axis) for the same target functions defined on manifolds γL for various L. Since the target functionhas amplitudes Ai = 1 for all frequencies ki plotted, the colorbar is clipped between 0 and 1. (e): Correspond-ing learning curves. Gist: Some manifolds (here with larger L) make it easier for the network to learn higherfrequencies than others.
Figure 4: Heatmap of training accuracies of a network trained to predict the binarized value of a sine wave ofgiven frequency (x-axis) defined on γL for various L (y-axis).
Figure 5: Normalized spectrum of the model (x-axis for frequency, colorbar for magnitude) with perturbedparameters as a function of parameter perturbation (y-axis). The colormap is clipped between 0 and 1. Observethat the lower frequencies are more robust to parameter perturbations than the higher frequencies.
Figure 6: The learnt function (green) overlayed on the target function (blue) as the training progresses. Thetarget function is a superposition of sinusoids of frequencies κ = (5, 10, ..., 45, 50), equal amplitudes andrandomly sampled phases.
Figure 7: Loss curves averaged over multiple runs. (cf. Experiment 1)A	Experimental DetailsA.1 Experiment 1We fit a 6 layer ReLU network with 256 units per layer fθ to the target function λ, which is asuperposition of sine waves with increasing frequencies:λ : [0,1] → R, λ(z) = ^X Ai sin(2πkiz + 夕i)iwhere k = (5,10,15,…，50), and Wi is sampled from the uniform distribution U(0, 2π). In the firstsetting, we set equal amplitude for all frequencies, i.e. Ai = 1 ∀ i, while in the second setting weassign larger amplitudes to the higher frequencies, i.e. Ai = (0.1, 0.2, ..., 1). We sample λ on 200uniformly spaced points in [0, 1] and train the network for 80000 steps of full-batch gradient descentwith Adam (Kingma & Ba, 2014). Note that we do not use stochastic gradient descent to avoid thestochasticity in parameter updates as a confounding factor. We evaluate the network on the same 200point grid every 100 training steps and compute the magnitude of its (single-sided) discrete fouriertransform at frequencies ki which we denote with |fki |. Finally, we plot in figure 1 the normalized~Umagnitudes ^k^ averaged over 10 runs (with different sets of sampled phases Wi). We also recordthe spectral norms of the weights at each layer as the training progresses, which we plot in figure 1for both settings (the spectral norm is evaluated with 10 power iterations). In figure 6, we show an
Figure 9: Evolution with training iterations (y-axis) of the Fourier spectrum (x-axis for frequency, and col-ormap for magnitude) for a network with varying depth, width = 16 and weight clip = 10. The spectrum ofthe target function is a constant 0.005 for all frequencies.
Figure 8: The target function used in Experiment 5.
Figure 10: Evolution with training iterations (y-axis) of the Fourier spectrum (x-axis for frequency, and col-ormap for magnitude) for a network with varying width, depth = 3 and weight clip = 10. The spectrum ofthe target function is a constant 0.005 for all frequencies.
Figure 11: Evolution with training iterations (y-axis) of the Fourier spectrum (x-axis for frequency, and col-ormap for magnitude) for a network with varying weight clip, depth = 6 and width = 64. The spectrum ofthe target function is a constant 0.005 for all frequencies.
Figure 12: Evolution with training iterations (y-axis) of the network prediction (x-axis for input, and colormapfor predicted value) for a network with varying weight clip, depth = 6 and width = 64. The target function isa δ peak at x = 0.5.
Figure 13: Loss curves of two identical networks trained to regress white-noise under identical conditions,one on MNIST reconstructions from a DAE with 64 encoder features (blue), and the other on 64-dimensionalrandom vectors (green).
Figure 14: Path between CIFAR-10 adversarial examples (e.g. “frog” and “automobile”, such thatall images are classified as “airplane”).
Figure 15: Each row is a path through the image space from an adversarial sample (right) to a true trainingimage (left). All images are classified by a ResNet-20 to be of the class of the training sample on the right withat least 95% softmax certainty. This experiment shows we can find a path from adversarial examples (right, Eg.
Figure 16: (a,b,c,d): Heatmaps of training accuracies (L-vs-k) of KNNs for various K. Whencomparing with figure 4, note that the y-axis is flipped. (e): The frequency spectrum of KNNs withdifferent values of K , and a DNN. The DNN learns a smoother function compared with the KNNsconsidered since the spectrum of the DNN decays faster compared with KNNs.
