Figure 1: The overall structure of the proposed IPE schemeasynchronous update of parameters (Mnih et al. (2016), Babaeizadeh et al. (2017)), sharing an ex-perience replay buffer (Horgan et al. (2018)), GPU-based parallel computation (Babaeizadeh et al.
Figure 2: (a) the conceptual search coverage in the policy parameter space by parallel learners(the proper individual search area by each learner may be larger than that in the figure) and (b) anillustration of search comparison: single search (blue) versus the proposed interactive parallel searchguided by the policy of best learner at each search interval (pink - policies of best learners duringsearch)Now, the overall procedure for the proposed IPE scheme is explained with the diagram in Fig. 1. Thevalue function parameter and policy parameter of each learner are initialized. The chief distributesthe parameter β and the reference policy parameter φb , which is the policy information of the bestlearner over the previous M time steps, to all learners. At each time step, each learner interactswith its own environment copy by taking its action and receiving the reward and the next state, andstores its experience to the shared common replay buffer D . Then, the i-th learner updates its valuefunction parameter θi by minimizing its own value loss function L(θ,) which is the same as that ofthe base algorithm, and updates the policy parameter φi by minimizing the augmented loss functionL(φi) in (1) for N times by drawing N mini-batches from the shared common replay buffer D.
Figure 3: Performance for PPO (brown), ACKTR (orange), SQL (purple), SAC (red), TD3 (green),and IPE-TD3 (proposed method, blue) on MuJoCo tasks.
Figure 4: Ablation study: Different parallel methods for Ant-v1 task4.3	Ablation studyIPE-TD3 has several components to improve the performance based on parallelism: 1) sharingexperiences from multiple policies, 2) using the best policy information, and 3) fusing the bestpolicy information in a soft manner based on the augmented loss function. Thus, we investigatedthe impact of each component on the performance improvement. For comparison we considered thefollowing parallel policy exploration methods gradually incorporating more techniques:1.	TD3 The original TD3 with one learner2.	Distributed RL TD3 (DRL-TD3) N actors obtain samples from N environment copies.
Figure 5: Performance of IPE-SAC (blue), Re-SAC (purple), and SAC (red) on Humanoid (rllab)1The simulation is still running, and we will change the graph when the simulation is finished.
