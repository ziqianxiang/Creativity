Figure 1: Architecture of UG-WGAN. The amount of languages can be trivially increased by in-creasing the number of language agnostic segments kjα and ejα .
Figure 2: Ablation study of λ. Both Wasserstein and Perplexity estimates were done on a held outtest set of documents.
Figure 3: T-SNE Visualization of u(∙). Same colored dots represent the same language.
Figure 4: Cross-Lingual Generalization gap and performance6	DiscussionUniversal Grammar also comments on the learnability of grammar, stating that statistical informa-tion alone is not enough to learn grammar and some form of native language faculty must exist,sometimes titled the poverty of stimulus (POS) argument (Chomsky, 2010; Lewis & Elman, 2001).
Figure 5: The figure on the left shows perplexity calculations on a held out test set for UG-WGANtrained on a varying number of languages. The figure on the right shows the perplexity gap betweentwo languages trained with UG-WGAN λ = 0.1 and UG-WGAN λ = 0.0.
