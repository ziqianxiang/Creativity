Figure 1: The evolution of the learning state vector st during an annotation episode starting from the samestate for (a) random sampling, (b) uncertainty sampling, and (c) our learnt strategy. Every column representsst at iteration t, With |V| = 30 . Yellow corresponds to values of yt that predict class 1 and blue - class 0.
Figure 2: Adapting the DQN architecture. Left: In standard DQN, the Q-function takes the state vector asinput and yields an output for each discrete action. Right: In our version, actions are represented by vectors.
Figure 3: Example of non-myopic be-haviour of a learnt RL strategyBaseline	Rs	Us	OURSLogReg-100	32.07	-28.80%	-34.71%LogReg-200	80.06	-29.61%	-39.96%LogReg-500	51.59	-31.49%	-37.75%SVM	30.87	-7.81%	-28.35%Table 2: Increasing the number of annotations still using logis-tic regression (first three rows) and using SVM instead of logisticregression as the base classifier (fourth row). We report the aver-age number of annotations required using Rs and the percentagesaved by either Us or OURS.
Figure 4: Comparing the behavior of Rs, Us and OURS. (a) Histogram of pt for Rs in blue, Us in cyan, andOURS in purple. (b) Evolution over time for random. (c) Evolution over time for OURS.
Figure 5: Performance of all the strategies on 0-adult dataset.
Figure 6: Results of experiment from Sec. 4.2. Performance of all baseline strategies on 3 first datasets.
Figure 7: Results of experiment from Sec. 4.3. Performance of 3 top strategies from experiment of Sec. 4.2and a random sampling on the next 3 datasets.
