Figure 1: The bias/variance tradeoff of our gradient estimate as a function of , comparing toGumbel-Softmax gradient estimate as a function of its temperature τ . The parameters were learnedusing REINFORCE and from its optimal parameters we estimate the gradient randomly for 500times. Left: the average difference from REINFORCE gradient. Right: the average standard devia-tion of the gradient estimate.
Figure 2: Right: Test log-loss bound of VAEs with different categorial variables z = (z1 , ..., zn)with z ∈ {1, ..., k}. Left: Test log-loss bound of discrete VAE with 20 binary units as a function oftraining epochs.
Figure 3: semi-supervised VAE on MNIST with 50/100/300/600 labeled examples out of the50, 000 training examples. Direct loss minimization combined with VAE improves the performanceeven with weak supervision, e.g., with only 50 examples direct loss minimization with VAE achievesbetter accuracy than semi-supervised Gumbel-Softmax VAE.
Figure 4: Learning attribute representation in CelebA, using our semi-supervised setting, by cali-brating our arg max prediction using a loss function. These images here are generated while settingtheir attributes to get the desired image. The i-th row consists the generation of the same continuouslatent variable for all the attributesFollowing Kingma et al. (2014), we conducted a quantitive experiment with weak supervision onMNIST with 50/100/300/600 labeled examples out of the 50, 000 training examples. For labeledexamples, We set the perturbed label zeθ+φ+γ+' to be the true label. This is equivalent to usingthe indicator function over the space of correct predictions. A comparison of our method withGumbel-Softmax appears in Figure 3. We can see that direct loss minimization combined withVAE improves the performance even with weak supervision, e.g., with only 50 examples direct lossminimization with VAE achieves better accuracy than semi-supervised Gumbel-Softmax VAE. Wenote that we cannot compare the objective function of both methods, as our objective considersdirect loss minimization, however, we are able to compare the test log-loss bound, see Figure 3.
Figure 5: Test log-loss bound of discrete VAE with n × k binary units as a function of trainingepochs.
Figure 6: comparing direct and GSM for deep decoder and encoder over discrete product spacesz = (z1, ..., zn) where each zi ∈ {1, ..., k} and nk = 60. The encoder is built over a 28 × 28image and has four layers (including input) of (784, 400, 200, 60) units respectively. The decoderarchitecture has four layers with (60, 200, 400, 784) units respectively, including the 28 × 28 outputlayer. Between each layer we have a ReLU function and the output layer used tanh to calibrate thepixel value. The experiment is performed over non-binarized MNIST with the MSE loss.
Figure 7: Comparing unsupervised to semi-supervised VAE on MNIST, for which the discrete la-tent variable has 10 values, i.e., z ∈ {1, ..., 10}. One can see that semi-supervised helps the VAEto capture the class information and consequently improve the image generation process. Left: gen-erated images from an unsupervised VAE. Right: generated images from semi-supervised mixturemodel. The j-th column consists of images generated for the j-th discrete class. The i-th rowconsists the generation of the same continuous latent variable for all the 10 classes. One can see thatthe discrete latent space is able to learn the class representation. The continuous latent space learnsthe variability within the class. Comparing to unsupervised image generation, we observe that somesupervision improves the image generation per class.
Figure 8: Extending Figure 4. The i-th row consists the generation of the same continuous latentvariable for all the attributes.
