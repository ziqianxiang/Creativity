Figure 1: Toy examples of binary classification problem with blue circles and red rhombus repre-senting two classes of data. The blue dash lines and solid lines show true boundary and estimatedboundary respectively. (a) The original data; (b) Adversarial examples added into training data; (c)Virtual data interpolated between classes added in training data; (d) Gaussian random noise addedinto input.
Figure 2: Mean estimated probabilities (solid lines) with 99% confidence intervals (dash lines)obtained from 1000 realizations for regularization methods, with N = 300 data points sampled from(a) U nif orm([-1, -0.9] ∪ [0.9, 1]), w = 4, b = 0. (b) U nif orm([0, 1]), w = 8, b = 4.
Figure 3: Mean variance of estimated parameters obtained from 1000 realizations with true wincreasing from 1 to 6. x ∈ U nif orm([-1, -0.9] ∪ [0.9, 1]. The legend ‘regularizers’ represents allregularization methods, since they produce almost identical plots. Here we use δ = λ = 0.1 for labelsmoothing and logit squeezing; β = 0.01 for weight decay.
Figure 4: Mean estimated probabilities (solid lines) with 95% confidence intervals (dash lines)obtained from 1000 realizations for augmentation methods, with N = 300 data points sampled from(a) U nif orm([-1, -0.9] ∪ [0.9, 1]), w = 4, b = 0. (b) U nif orm([0, 1]), w = 8, b = 4. In bothcases, we use α = 0.3 for feature smoothing and mixup, and σ = 0.3 for generating random noise.
Figure 5: The confidence interval of averaged ,boundary, -K PK=I b/wi With X ∈ R2 and K = 3.
Figure 6: Mean estimated probabilities (solid lines) With 95% confidence intervals (dash lines)obtained from 1000 realizations in the case of in-symmetric data. Data Were generated from model 1With w = 4, b = 0. (a) N1 = 200 and N2 = 100 data points Were sampled from [-1, -0.9] and[0.9, 1] respectively; (b) both N1= 150 and N2 = 150 data points Were sampled from [-1, -0.9]and [0.6, 0.7] respectively.
Figure 7: Mean estimated probabilities (solid lines) with 95% confidence intervals (dash lines)obtained from 1000 realizations in the case of in-symmetric data. x ∈ [-1, -0.9] ∪ [0.9, 1] with size300. y = 1 J p > 0.5. Then 10 another input x0 = 0.1, y0 = 1 or 10 input of x0 = —0.1, y0 = 0 areadded randomly with probability 0.5 to the training.
