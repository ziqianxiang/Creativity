Figure 1: When distilling the teacher distribution (red) with a student distribution (blue) by min-imizing the KL divergence, the student receives two counteracting gradient signals from the twocorresponding likelihood terms. The probability of receiving a signal to push out the student distri-bution to fill the teacher is proportional to the relative amount of space occupied by the shaded area,which vanishes when the covariance matrix of the teacher has trivial eigenvalues.
Figure 2: We distill a Gaussian teacher with a Gaussian student. x-axis: likelihood under the teacher; y-axis:count of samples drawn from the teacher (real samples) and the learned student (generated samples). (a-d) inthe subfigures correspond to {4, 16, 32, 64}- dimensional multivariate Gaussians.
Figure 3: Density distillation of teacher models trained on MNIST (first row) and Fashion-MNIST(secondrow). Column 1: samples from teacher network T. Column 2: samples from student trained with the KLloss SKL. Column 3: samples from student trained with the z-reconstruction loss Sz. Column 4: samplesfrom student trained with the x-reconstruction loss Sx . Column 5: samples from student trained with thex-reconstruction loss where x is sampled from the teacher SO .
Figure 4: Experiments with a ResNet student using (a) l2 x-reconstruction loss, (b) l1 x-reconstruction loss,(c) adding N(0, 0.5) noise to the encodings z and using l2 x-reconstruction loss, (d) adding N(0, 0.5) noiseto z and using l1 x-reconstruction loss, and (e) adding N(0, 0.5) noise to z and using a mixed loss l2 + l1. Ingeneral, we observe that adding noise significantly improves sample quality, and training with l1 losses lead tosharper samples.
Figure 5: We randomly sample z from N(0, I784) (for each row) and rescale the vector such that ithas norm r ∙ √784, where r ∈ [0.700, 0.750, 0.800, 0.850, 0.900, 0.920, 0.940, 0.960, 0.980, 0.990,0.995, 1.000, 1.005, 1.010, 1.020, 1.040, 1.060, 1.080, 1.100, 1.150, 1.200, 1.250, 1.300], whichcorrespond to the change along the horizontal axis. We observe that directional information doesnot preserve digit identity in the data space, and the manifold per digit can be stretched around theorigin on different level sets of norm.
Figure 6: (left) Teacher samples with a PixelCNN on CIFAR-10, (right) Student samples usingx-reconstruction under the l1 loss, with noise injection.
