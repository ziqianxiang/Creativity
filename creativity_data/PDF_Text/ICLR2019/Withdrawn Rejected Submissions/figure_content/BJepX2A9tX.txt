Figure 1: Comparison of convolution schemes. The domain of filter ‘F’ in the input and its corre-sponding outputs in the feature map are colored red. That of the rotation of ‘F’ by 180 degrees iscolored blue. The local support on the domain for the convolution at a few points for each schemeare shown in gray. Conic convolution, with rotations of 90 degrees in this example, encodes rotationequivariance without introducing distortion to the support of the filter in the original domain (unlikethe log-polar transform) and without requiring additional storage for feature maps (unlike groupconvolution). The example shown for group convolution is the first layer of a G-CNN, mappingfrom Z2 to the roto-translation group.
Figure 2: The overall architecture of the proposed CFNet. (a) Filtering the image by various filtersat rotations in corresponding conic regions preserves rotation-equivariance. (b) Subsequent convo-lutional feature maps are filtered similarly. Rotation-invariance is encoded by the transition fromconvolutional to fully-connected layers, which consists of (c) element-wise multiplication and sum,denoted by , with rotated weight tensors, transforming rotation to circular shift, and (d) applica-tion of the magnitude response of the 2D-DFT to encode invariance to such shifts. (e) This output isreshaped and passed through the final, fully-connected layers.
Figure 3: Comparison of CFNet CNet, G-CNN, G-CNN+DFT, and a standard CNN on the GMMsynthetic biomarker images and on images of protein localization. (a,b) Example images, shown asheat maps for detail, showing inter- and intra-class variation. (e) Example images from yeast cellphenotype classes. Testing classification accuracy of methods on synthetic GMM images (c,d) andprotein localization (f,g) with varying numbers N of training examples per class.
