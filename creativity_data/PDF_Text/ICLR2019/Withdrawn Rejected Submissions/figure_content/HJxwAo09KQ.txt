Figure 1: Learned optimizers outperform existing optimizers on training loss (a) and validation loss(b). (a,b) Training and validation curves for a three layer CNN, trained on a subset of 32x32 ima-genet classes not seen during outer-training of the optimizer. Dashed lines indicate the best achievedperformance over an additional 130 seconds. We show two learned optimizers - one trained tominimize training loss, and the other trained to minimize validation loss on the inner-problem. Wecompare against Adam, RMSProp, and SGD+Momentum, individually tuned for the train and vali-dation loss (Panel (a) and (b), respectively). On training loss (a), our learned optimizer approacheszero training loss, and achieves it’s smallest loss values in less than one quarter the wall-clock time.
Figure 2:	Top: Schematic of unrolled optimization. Bottom Definition of terms used in this paper.
Figure 3:	Outer-problem optimization landscapes become increasingly pathological with increas-ing inner-problem step count. (a) A toy 1D inner-problem loss surface with two local minimum.
Figure 4:	Large biases can result from reducing the number of steps per truncation in unrolledoptimization. (a) Each line represents an experiment with the same total number of inner-steps(10k) but a different number of unrolling steps per truncation. In all cases, the initial learning rateis 1e - 3 (dashed line). We find low truncation amounts move away from the optimal learningrate (〜4e - 3). (b) Outer-gradient at each truncation over the course of inner-training. Initialouter-gradients are highly negative, trying to increase the learning rate, while later outer-gradientsare slightly above zero, decreasing learning rate. The total outer-gradient is the sum of these twocompeting directions. (c) Cumulative value of outer-gradient vs inner-training step. When a linecrosses the zero point, the outer-learned learning rate is at an equilibrium. For low unrolling steps,the cumulative sum is positive after a small number of steps, resulting in a increasing learning rate,and decreasing outer-loss. As more inner-steps are taken, the bias increases, eventually flipping theouter-gradient direction resulting in increasing outer-loss.
Figure 5: (a) As outer-training progresses, the variance of the reparameterization gradient estimatorgrows, while the variance of the ES estimator remains constant. (b,c) Performance after 10k iter-ations of inner-training for different outer-parameters over the course of outer-training. Each linerepresents a different random initialization of outer-parameters (θ). Models are trained targeting thetrain outer-objective (b), and the validation outer-objective (c). Dashed lines indicate performanceof hyperparameter tuned Adam. Model’s in orange are the best performing and used in §4.
Figure 6: Despite outer-training only targeting optimizer performance on a three layer convolu-tional network trained on 10 way classification problems derived from 32x32 Imagenet, the learnedoptimizers demonstrate some ability to generalize to new architectures and datasets. Plot shows trainand validation performance on a six layer convolutional neural network trained on 28x28 MNIST.
Figure 7: We compare the model described in §4.1 with different features removed. Shown aboveis the distribution of outer-loss performance across random seed averaged between 380k and 420kinner problem trained. For full learning curves, see Appendix G. First, with regard to gradientestimators we find the combined estimator is more stable and out performs both the analytic gradient,and evolutionary strategies. With regard to a curriculum, we find poor convergence with lowernumbers of unrolling steps (10, 100) and high variance with larger numbers of truncation’s steps(1000). Finally with regard to optimizer features, we find our learned optimizers perform nearly aswell without the RMS terms and without the time horizon input, but fails to converge when not givenaccess to momentum based features.
Figure 8: Additional outer-validation problems.
Figure 9: Outer-training problems.
Figure 10: Inner problem: 2 hidden layer fully connected network. 32 units per layer with reluactivations trained on 14x14 MNIST.
Figure 11: Inner problem: 3 hidden layer fully connected network. 128 units per layer with reluactivations trained on 14x14 MNIST.
Figure 12: Inner problem: 6 convolutional layer network. 32 units per layer, strides: [2,1,2,1,1,1]with relu activations on 28x28 MNIST.
Figure 13: Inner problem: 3 convolutional layer network. 32 units per layer, strides: [2,2,1] withrelu activations on 28x28 MNIST.
Figure 14: Inner problem: 3 convolutional layer network. 128 units per layer, strides: [2,2,1] withrelu activations trained on a 10 way classification sampled from 32x32 Imagenet (using holdoutclasses).
Figure 15: Training curves for ablations described in §4.5. The thick line bordered in black is themedian performance, with the shaded region containing the 25% and 75% percentile. Thinner solidlines are individual runs.
