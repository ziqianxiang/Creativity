Figure 1: An example of first-person view in the 3D Doom environment with sample instructions and ques-tions. The test set consists of unseen instructions and questions. The dataset evaluates a model for cross-taskknowledge transfer between Semantic Goal Navigation (SGN) and Embodied Question Answering (EQA).
Figure 2: Overview of our proposed architecture, described in detail in Section 4.
Figure 5: Architecture of the Dual-Attention unit with example intermediate representations and operations.
Figure 6: Example auxiliary tasklabels for the red channel.
Figure 7: Training accuracy of all models trained with auxiliary tasks for Easy (left) and Hard (right).
Figure 8: Training accuracy of proposed Dual-Attention model with all ablation models trained without (left)and with (right) auxiliary tasks for the Easy environment.
Figure 9: Visualizations of convolutional output channels. We visualize the convolutional channels corre-sponding to 7 words (one in each row) for the same frame (shown in the rightmost column). The first columnshows the auxiliary task labels for reference. The second column and third column show the output of thecorresponding channel for the proposed Dual-Attention model trained without and with auxiliary tasks, respec-tively. As expected, the Aux model outputs are very close to the auxiliary task labels. The convolutional outputsof the No Aux model show that words and objects/properties in the images have been properly aligned evenwhen the model is not trained with any auxiliary task labels. We do not provide any auxiliary label for words‘smallest’ and ‘largest’ as they are not properties of an object and require relative comparison of objects. Thevisualizations in row 5 (corresponding to ‘smallest’) indicate that both models are able to compare the sizes ofobjects and detect the smallest object in the corresponding output channel even without any aux labels for thesmallest object.
Figure 10: Spatial Attention and Answer Prediction Visualizations. An example EQA episode with thequestion “Which is the smallest blue object?”. The sentence embedding of the question is shown on the top(xsent). As expected, the embedding attends to object type words (’torch’, ’pillar’, ’skullkey’, etc.) as the ques-tion is asking about an object type (’Which object’). The rows show increasing time steps and columns showthe input frame, the input frame overlaid with the spatial attention map, the predicted answer distribution, andthe action at each time step. As the agent is turning, the spatial attention attends to small and blue objects. Timesteps 1, 2: The model is attending to the yellow skullkey but the probability of the answer is not sufficientlyhigh, likely because the skullkey is not blue. Time step 3: The model cannot see the skullkey anymore so itattends to the armor which is next smallest object. Consequently, the answer prediction also predicts armor, butthe policy decides not to answer due to low probability. Time step 4: As the agent turns more, it observes andattends to the blue skullkey. The answer prediction has high probability for skullkey as it’s small and blue andthe policy decides to answer the question.
Figure 11: Architecture of the policy module.
Figure 12: Plot showing the training accuracy of all the models without auxiliary tasks for Doom Easy andHard environments.
Figure 13: Plot showing the training accuracy of 3 models across 3 training runs with different seeds with andwithout auxiliary tasks for Doom Easy environment without any smoothing.
Figure 14: Objects of various colors and sizes used in the ViZDoom environment.
Figure 15: Example first-person vieWs of the House3D environment With sample objects of various colors.
