Figure 1: The TARNET architecture with k heads for the multiple treatment setting.
Figure 3: Change in error (y-axes) in termsof precision in estimation of heterogenous ef-fect (PEHE) and average treatment effect (ATE)when increasing the percentage of matches ineach minibatch (x-axis). Symbols COrreSPOnd tothe mean value of ^ate (red) and √^pehe (blue)on the test set of News-8 across 50 repeated runswith new outcomes (lower is better). Perfor-mance improves with more matches added.
Figure 2: Correlation analysis of the real PEHE(y-axis) with the mean squared error (MSE; left)and the nearest neighbour approximation of theprecision in estimation of heterogenous effect(NN-PEHE; right) across over 20,000 modelevaluations on the validation set of IHDP. Scat-terplots show a subsample of 1’400 data points. Pindicates the Pearson correlation. NN-PEHE cor-relates significantly better with PEHE than MSE.
Figure 4: Comparison of several state-of-the-art methods for counterfactual inference on the testset of the News-8 dataset when varying the treatment assignment imbalance κ (x-axis), i.e. howmuch the treatment assignment is biased towards more effective treatments. Symbols correspond tothe mean value of ^ate (left) and √^PEHE (right) across 50 repeated runs with new outcomes (loweris better). The shaded area indicates the standard deviation. PM handles an increasing treatmentassignment bias K better than existing state-of-the-art methods in terms of both ^ate and √∈Pehe.
Figure 5: Comparison of several state-of-the-art methods for counterfactual inference on the testset of the TCGA dataset when varying the percentage of hidden confounding (x-axis), i.e. whatpercentage of the features that were relevant for treatment assignment are visible to the model.
Figure 6: Comparison of the learning dynamics during training (normalised training epochs; fromstart = 0to end = 100 of training, x-axis) of several matching-based methods on the validation set ofNews-8. The coloured lines correspond to the mean value of the factual error (√MSE; red) and theCoUnterfactUaI error (√^pehe; blue) across 10 randomised hyperparameter configurations (lower isbetter). The shaded area indicates the standard deviation. All methods used exactly the same modeland hyperparameters and only differed in how they addressed treatment assignment imbalance. Theleftmost figure shows the desired behavior of the counterfactual and factual error jointly decreasinguntil convergence. PM exhibits the desired behavior of aligning the optimisation of the factual andcounterfactual error. PSMPM shows a weaker degree of alignment and much higher variance. Incontrast, PSMMI shows the undesired behavior of the counterfactual error increasing as the factualerror decreases, indicating that the models were overfitting to the properties of the treated group.
Figure S1: Illustration of the impact of added variance introduced by minibatch SGD during training.
Figure S2: Comparison of the frequencies of individual gradient steps during training of severalmatching-based methods on the validation set of News-8. The heatmap entries show the frequenciesof pairs of differences in factual error (∆MSE; y-axis) and CoUnterfactual error (Δ∈pehe; x-axis)after individual gradient steps. All methods used the same model and only differed in how theyaddressed treatment assignment bias. The leftmost heatmap shows the desired behavior of gradientsteps changing the counterfactual and factual error jointly during training. PM most closely resem-bles the desired behavior of jointly optimising counterfactual and factual error. In contrast, PSMPMand PSMMI have a much higher relative frequency of gradient steps that do not move into the samemagnitude or direction (off-diagonal entries) for both factual and counterfactual error.
