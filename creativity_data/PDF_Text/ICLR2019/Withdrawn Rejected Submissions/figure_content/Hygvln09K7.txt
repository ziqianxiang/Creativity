Figure 1: The computational graph of our fast/slow learning model: to optimize the cost functionf (θ, θ0), we apply different strategies on θ, θ0 separately. We leverage a lifelong-slow learningscheme (SGD) on θ0 to acquire generic knowledge and a fast learning scheme g(.) on task-specificθ. [Photo: inspired by (Andrychowicz et al. (2016))]Figure 2: Decoupling the optimization of a CNN into the slow module (generic bottom convolutionallayers parametrized by θ0 in the red dashed box) and the fast module (top FC layers parametrized byθ in the blue dashed box).
Figure 2: Decoupling the optimization of a CNN into the slow module (generic bottom convolutionallayers parametrized by θ0 in the red dashed box) and the fast module (top FC layers parametrized byθ in the blue dashed box).
Figure 3: Case study: a two-layer linear MLP with two FC layers.
Figure 4: Experiments on the two-layer linear MLP. (a) x-axis: fast/slow training epochs; vertical-axis: Relative loss L(Wf,1)/L(Wf,0) after one-step fast module update with a large learning rateη = 0.2; (b) x-axis: fast/slow training epochs; vertical-axis: Frobenius-norm of the estimationdifference between slow module θ0 = {Ws} and the true whitening precision matrix Σx-1.
Figure 5: FullImageNet: 1-shot Classification Accuracy w.r.t. class number. Red line: our meta-learning; blue line: CNN metric-learning.
