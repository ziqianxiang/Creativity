Figure 1: Toy MDP with two states s0 and s1, and two actions a0 and a1. Agent receives reward of 1 forselecting a1 at s0 and 0 otherwise.
Figure 2: We examine the performance of DDPG in three off-policy settings. Each individual trial is plottedwith a thin line, with the mean in bold. Straight lines represent the average return of episodes contained in thebatch. An estimate of the true value of the off-policy agent, evaluated by Monte Carlo returns, is marked by adotted line. In the fixed buffer experiment, the off-policy agent learns from a large, diverse dataset, but exhibitspoor learning behavior and value estimation. In the concurrent learning setting the agent learns alongside abehavioral agent, with access to the same data, but suffers in performance. In the imitation learning setting, theagent receives data from an expert policy but is unable to learn, and exhibits highly divergent value estimates.
Figure 3: We evaluate BCQ and several baselines following the experiments from Section 3.2. The shaded arearepresents half a standard deviation. Value estimates include a plot of each trial, with the mean in bold. Straightlines represent the average return of episodes contained in the batch. An estimate of the true value of BCQ,evaluated by Monte Carlo returns, is marked by a dotted line. Unlike any other algorithm, BCQ matches oroutperforms the performance of the behavioral policy in all three tasks, while exhibiting a stable value function.
Figure 4: We examine the effectiveness of BCQ when learning from a highly noisy demonstrator. 100k transi-tions are provided by an expert policy with 0.3 probability of a random action and N (0, 0.3) added to remainingactions. Average returns include a plot of each trial, with the mean in bold. BCQ greatly outperforms behavioralcloning algorithms, as well as the behavioral policy, demonstrating robustness to suboptimal data.
Figure 5: We evaluate BCQ and DDPG on a batch collected by a random behavioral policy. The shaded arearepresents half a standard deviation. Value estimates include a plot of each trial, with the mean in bold. Anestimate of the true value of BCQ and DDPG, evaluated by Monte Carlo returns, is marked by a dotted line.
Figure 6:	Default Network Architecture. All actor networks are followed by a tanh ∙ max action sizeBCQ. BCQ uses four networks: a perturbation model ξφ (s, a), a critic Qθ (s, a), a value networkVψ(s), and a state-conditioned VAE Gω(s), along with a target value network Vψ0 (s). Each networkin BCQ follows the default architecture (Figure 6) and the default hyper-parameters (Table 1). Forξφ(s, a, Φ), the constraint Φ is implemented through a tanh activation multiplied by I ∙ Φ followingthe final layer.
Figure 7:	DDPG Critic Network Architecture.
Figure 8:	DQN Network Architecture.
