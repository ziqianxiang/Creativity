Figure 1: Degenerate optimisation problems with known solutions. Left: Trajectories on theStochastic Rosenbrock function for different solvers (darker shaded regions denote higher functionvalues). Right: evolution of the loss per iterations for the plotted trajectories.
Figure 2: Comparison with the different optimisers for various datasets and networks. Theevolution of the training error is shown, as it is the quantity being optimised. CurveBall performswell under a variety of realistic settings, including large-scale datasets (ImageNet), the presence ofbatch-normalization, and severely over-parameterised models (ResNet).
Figure 3: (Left) Results of 50 randomly-generated architectures on CIFAR10. The median (thicklines) and 25th-75th percentiles (shaded region) are shown. Numbers in the legend represent learningrates (fixed for CURVEBALL). (Right) Training error vs. wall clock time (basic CIFAR-10 model).
Figure 4: Convergence rate as a function of hyper-parameters ρ, β, and Hessian eigenvalue hi . Lowervalues (brighter) are better. The white areas show regions of non-convergence.
Figure 5: Hyper-parameter evolution during training. Average momentum ρ (left), learning rateβ (middle), and trust region λ (right), for each epoch for the basic CNN on CIFAR10, with andwithout batch normalisation (BN). To make their scales comparable, we plot λ divided by its initialvalue (which is λ0 = 1 with batch normalisation and λ0 = 10 without).
Figure 6: Gradient evolution during training. Average gradient norm during each epoch for thebasic CNN on CIFAR-10, with and without batch normalisation (BN).
Figure 7: Training error vs. wall clock time (basic CIFAR-10 model). The time axis is logarithmicto show a comparison with conjugate-gradient-based Hessian-free optimisation. Due to the CGiterations, it takes an order of magnitude more time to converge than first-order solvers and ourproposed second-order solver, despite the efficient GPU implementation.
Figure 8: Training with fixed ρ = 1. Basic CNN architecture on CIFAR-10 without and with batchnormalisation, respectively. Both settings use automatic tuning of the remaining hyper-parameters(by adapting eq. 18).
