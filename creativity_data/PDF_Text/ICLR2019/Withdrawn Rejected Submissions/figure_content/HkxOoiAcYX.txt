Figure 1: I(X; Bin(Te)) vs. epochs for different bin sizes and the model in Shwartz-Ziv & Tishby(2017). The curves converge to ln(212) ≈ 8.3 for small bins, per the 12-bit uniformly distributed X.
Figure 2: kth noisy neuron in layer 1with nonlinearity σ; WEk) and bg(k)are the kth row/entry of the weightmatrix and the bias, respectively.
Figure 3: Histograms of cosine similarities between internal representations of deterministic, noisy,and dropout MNIST CNN models. To encourage comparable internal representations, all modelswere initialized with the same random weights and accessed the training data in the same order.
Figure 4: Single-layer tanh network: (a) the density pT (k) at epochs k = 250, 2500; (b) pT (k) and (c)I X; T(k) as a function of k; and (d) mutual information as a function of weight w with bias -2w.
Figure 5: (a) Evolution of I(X; TQ) and training/test losses across training epochs for the SZTmodel with β = 0.005 and tanh nonlinearities. The scatter plots show the values of Layer 5 (d5 = 3)at the arrow-marked epochs on the mutual information plot. The bottom plot shows H(Bin(Te))across epochs for bin size B= 10β. (b) Same setup as in (a) but with regularization that encouragesorthonormal weight matrices. (c) SZT model with β = 0.01 and linear activations.
Figure 6: (a) Histogram of within- and between-class pairwise distances for SZT model with tanhnon-linearities and additive noise β = 0.005. (b) Same as (a) but training with weight normalization.
Figure 7: H(Bin(T^)) for the MNISTCNN, computed using two bins: [-1,0]and (0,1]. The tiny range of the y axisshows the near injectivity of the model.
Figure 8: Histograms of within-class and between-class pairwise distances from the MNIST CNN.
Figure 9: Two-layer leaky ReLU network: (a) network parameters as a function of epoch, (b,c) thecorresponding PDFs pT1(k) and pT2(k), and (d) the mutual information for both layers.
Figure 10: SZT model with (a) tanh nonlinearity and additive noise β = 0.01 without weightnormalization, (b) ReLU nonlinearity and β = 0.01 without weight normalization, (c) ReLUnonlinearity and β = 0.01 with weight normalization. Test classification accuracy is 97%, 96%, and97%, respectively.
Figure 11: Generated spiral data for binary classification problem.
Figure 12: (a) Evolution of I(X; TQ) and training/test losses across training epochs for Spiral datasetwith β = 0.005 and tanh nonlinearities. The scatter plots on the right are the values of Layer 5(d5 = 3) at the arrow-marked epochs on the mutual information plot. The bottom plot shows theentropy estimate H(Bin(TQ)) across epochs for bin size B = 10β. (b) Same setup as in (a) but witha regularization that encourages orthonormal weight matrices.
Figure 13: H (Bin(TQ)) estimate for deterministic net using spiral data. Bin size was set to B = 0.001.
Figure 14: Estimation results for the SP estimator compared to state-of-the-art kNN-based andKDE-based differential entropy estimators. The differential entropy of S + Z is estimated, where Sis a truncated d-dimensional mixture of 2d Gaussians and Z 〜N(0, σ2Id). Results are shown as afunction of n, for d = 5, 10 and σ = 0.1, 0.5. The SP estimator presents faster convergence rates,improved stability and better scalability with dimension compared to the two competing methods.
