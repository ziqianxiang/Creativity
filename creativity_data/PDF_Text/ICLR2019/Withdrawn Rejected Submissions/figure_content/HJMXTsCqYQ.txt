Figure 1: The SMILES and one-hot encoding for benzene. For simplicity only the characters presentin Benzene are shown in the one-hot encoding. In reality there would be a column for each characterin the SMILES alphabet.
Figure 2: The SMILES Variational Autoencoder with the learned constraint function illustrated bya circular feasible region in the latent space.
Figure 3: Experiments on 5 disjoint sets comprising 50 latent points each. Very small (VS) Noise aretraining data latent points with approximately 1% noise added to their values, Small (S) Noise have10% noise added to their values and Big (B) Noise have 50% noise added to their values. All latentpoints underwent 500 decode attempts and the results are averaged over the 50 points in each set.
Figure 4: a) The percentage of latent points decoded to drug-like molecules. The results are from 20iterations of Bayesian optimization with batches of 50 data points collected at each iteration (1000latent points decoded in total). The standard error is given for 5 separate train/test set splits of 90/10.
Figure 5: The best scores for new molecules generated from the baseline model (blue) and the modelwith constrained Bayesian optimization (red). The vertical lines show the best scores averaged over5 separate train/test splits of 90/10. For reference, the histograms are presented against the backdropof the top 10% of the training data in the case of Composite LogP and QED, and the top 20% of thetraining data in the case of Composite QED.
Figure 6: The best scores for novel molecules generated by the constrained Bayesian optimizationmodel optimizing for PCE. The results are averaged over 3 separate runs with train/test splits of90/10.
Figure 7: Constrained Bayesian optimization of the 2D Branin-Hoo Function.
Figure 8: a) Data points collected over 40 iterations of sequential Bayesian optimization. b) Con-tour plot of the predictive mean of the sparse GP used to model the objective function. Lightercolours indicate lower values of the objective. c) The contour learned by the BNN giving the prob-ability of constraint satisfaction.
Figure 9: Performance of Parallel Bayesian Optimization with EIC against Random Sampling.
