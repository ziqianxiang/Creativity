Figure 1: Object-Contrastive Networks (OCN): by attracting embedding nearest neighbors and repulsingothers using metric learning, continuous object representations naturally emerge. In a video collected by arobot looking at a table from different viewpoints, objects are extracted from random pairs of frames. Giventwo lists of objects, each object is attracted to its closest neighbor while being pushed against all other objects.
Figure 2: Models and baselines: for comparison purposes all models evaluated in Sec. 5 share the samearchitecture of a standard ResNet50 model followed by additional layers. While the architectures are shared,the weights are not across models. While the unsupervised model (left) does not require supervision labels, the’softmax’ baseline as well as the supervised evaluations (right) use attributes labels provided with each object.
Figure 3: We use 187 unique object instance in the real world experiments: 110 object for training (left), 43objects for test (center), and 34 objects for evaluation (right).
Figure 4: An OCN embedding organizes objects along their visual and semantic features. For example, a redbowl as query object is associated with other similarly colored objects and other containers. The leftmost object(black border) is the query object and its nearest neighbors are listed in descending order. The top row showsrenderings of our synthetic dataset, while the bottom row shows real objects.
Figure 6: The robot experiment of pointing to the best match to a query object (placed in front of the roboton the small table). The closest match is selected from two sets of target object list, which are placed on thelong table behind the query object. The first and the second row respectively correspond to the experiment forthe first and second target object lists. Each column also illustrates the query objects for each object category.
Figure 7: Robot experiment of grasping the object that is closest to the query object (held by hand). Imageson the left are captured by the robot camera, and the images on the right are the video frames from a thirdperson view camera. The leftmost object (black border) is the query object and its nearest neighbors are listedin descending order. The top row and the bottom row show the robot successfully identifies and grasps theobject with similar color and shape attribute respectively.
Figure 8: Consecutive frames captured with our robotic setup. At each run we randomly select 10 objects andplace them on the table. Then a robot moves around the table and take snapshots of the table at different angles.
Figure 9: Synthetic data: two frames of a synthetically generated scene of table-top objects (a) and a subsetof the detected objects (c). To validate our method against a supervised baseline, we additionally render colormasks (b) that allow us to identify objects across the views and to associate them with their semantic attributesafter object detection. Note that objects have the same color id across different views. The color id’s allow usto supervise the OCN during training.
Figure 10: A result showing the organization of real bowls based on OCN embeddings. The query object (blackborder, top left) was taken from the validation all others from the training data. As the same object is used inmultiple scenes the same object is shown multiple times.
Figure 11: A result showing the organization of real bowls based on OCN embeddings. The query object (blackborder, top left) was taken from the validation all others from the training data. As the same object is used inmultiple scenes the same object is shown multiple times.
Figure 12: A result showing the organization of bottles from synthetic data based on OCN embeddings. Thequery object (black border, top left) was taken from the validation all others from the training data.
Figure 13: A result showing the organization of vases from synthetic data based on OCN embeddings. Thequery object (black border, top left) was taken from the validation all others from the training data.
Figure 14: A result showing the organization of chairs from synthetic data based on OCN embeddings. Thequery object (black border, top left) was taken from the validation all others from the training data.
Figure 15: The robot experiment of pointing to the best match to a query object (placed in front of the robot onthe small table). The closest match is selected from two sets of target object list, which are placed on the longtable behind the query object. The first and the last three rows respectively correspond to the experiment forthe first and second target object lists. Each column also illustrates the query objects for each object category.
Figure 16: An OCN embedding organizes objects along their visual and semantic features. For example, a redbowl as query object is associated with other similarly colored objects and other containers. The leftmost object(black border) is the query object and its nearest neighbors are listed in descending order. The top row showsrenderings of our synthetic dataset, while the bottom row shows real objects. For real objects we removed thesame instance manually.
