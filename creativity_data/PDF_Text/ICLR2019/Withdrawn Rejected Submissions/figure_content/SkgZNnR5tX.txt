Figure 1: Navigation task. (left) Example maze from the training distribution together with thepath taken by the agent from spawn (cyan) to goal (magenta). (right) Frames from top left to bottomright correspond to agent observations as it takes the path from spawn to goal. Note that whilethe navigation task may look simple given a top down view, the agent only receives very partialinformation about the maze at every step, making navigation a difficult task.
Figure 2: Example of search procedure. First, We generate a set of 10 initial candidate mazes bysampling from the training distribution. We then Evaluate each with the agent over 30 episodes,select the best maze (i.e. lowest agent score), and Modify this maze by randomly moving two wallsto form the next set of candidates (Iteration 1). This process is repeated for 20 iterations, leadingto a maze where the agent score is 0.09 in this example (i.e. the agent finds the goal once in 11episodes). In Appendix A.2.1 we detail the computational requirements of this search procedure.
Figure 3: The search algorithm is able to rapidly find mazes where agents fail to find thegoal. (a) The objective used for the optimizer is average agent score. The dashed line correspondsto average goals reached on randomly perturbed mazes. (b) Minimizing score also leads to a lowprobability of at least one goal retrieval in an episode. The dashed line corresponds to average prob-ability of reaching a goal on randomly perturbed mazes. The blue lines are computed by averagingacross 50 optimization runs.
Figure 4: Example mazes leading to low scores and example trajectories on these mazes. (a)Maze with randomly perturbed walls. Despite being out of distribution, agents find the goal in 98%of episodes on such mazes and are able to get the goal on average 45 times per episode. (b) Mazeobtained after 20 iterations, moving two walls at each iteration to minimize reward. All agents findthe goal on such mazes in less than 20% of episodes. (c) Maze obtained through additional iterationsof removing walls. All agents find the goal on such mazes in less than 40% of episodes. (d) Humantrajectory on the same maze as in (c). Humans are able to consistently find the goal on such mazes.
Figure 5: Adversarial mazes are robust to change of spawn positions. The probability of goalretrieval (shown with the color bar) remains low across large portions of the simplified maze as the(a) agent spawn locations and (b) goal locations are moved for each episode..
Figure 6: Mazes that lead to failures in one agent lead to failure in other agents as well. Thisis the case for agents of the same architecture with different hyperparameters, and is also the casefor transfer across agents of different architecture. We note, however, that transfer across agentswith different architectures is weaker than among agents with the same architecture, and that theperformance of agents with the same architecture but with different hyperparameters is slightlyhigher than for the agents used to originally find the mazes.
Figure 7: Richer training distributions did not lead to robust agents. Adversarial optimizationfor agents trained with adversarial mazes (red) and for agents trained with randomly perturbed mazes(yellow). Compared against the Standard training method from Figure 3 for 50 iterations (blue).
Figure 8: Mazes used for human experiments. For each maze, the agent that performed best foundthe goal less than 50% of the time. In contrast, humans always found the goal, usually within lessthan a third of the episode. Note that humans played at the same resolution as agents, 96x72 pixels.
Figure 9: Trajectories taken by Human 3 on mazes leading to agent failure.
Figure 10: Pairwise transfer scores. Lower number indicates more transfer. ‘A’ corresponds to ourA2CV agent, and ‘M’ corresponds to MERLIN.
