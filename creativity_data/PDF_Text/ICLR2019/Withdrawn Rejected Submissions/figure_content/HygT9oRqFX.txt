Figure 1: Class distributions in the learned latent spaces, where A and B respectively denotethe distributions of classes A and B while Ai and Bj respectively denote examples in classes Aand B. The mixed features are distributed such that it is possible to distinguish each class in themixup method and the mixed features are distributed such that it is easier to distinguish each classin MixFeat. Because MixFeat repeatedly mixes training examples in each latent space to extenddiversity, the obtained feature space is able to distinguish each class more easily.
Figure 2: Computational passes of MixFeat. Let ㊉ denote the addition operator and Θ denotethe sample-wise product. Vector r is a random-value vector sampled from Gaussian distributionN(0, σ2), whereas vector θ is a random-value vector sampled from uniform distribution U(-π, π),where π indicates the circular constant. Each element of r or θ is associated with the feature map ofeach sample in the mini-batch. Furthermore, F denotes the random sort operation along the exampleaxis to the input tensor, F-1 denotes the restoring order operation along the example axis to the inputtensor, and (copy) denotes copying the vector from the forward training pass. The inference phasereturns the input tensor without modification.
Figure 3: Training and test error curves for PyramidNet (depth = 272, α = 200). Left: Trainingcurve on CIFAR-10. Right: Training curve on CIFAR-100. The discrepancy between the trainingand test curves is suppressed by MixFeat.
Figure 4: Results with incorrect labels in the training dataset without mixing, with MixFeat, withmixup, and with the method combined MixFeat and mixup on CIFAR-10 using 20-layer ResNets(pre-activation). Left: Test error (%) results for an increasing number of incorrect labels in the train-ing dataset. The increase in the error rate as the number of incorrect labels increases is suppressedby MixFeat and mixup. Additionally, the method combined those two has better performance thanof each method. Center: Test error curves for the training dataset with 50% incorrect labels. Right:Training and test loss curves for the training dataset with 50% incorrect labels. The test curves de-grade drastically after the peak without mixing whereas they are kept low by MixFeat and mixup.
Figure 5: Left: Test error (%) results when reducing the training dataset size without mixing,with MixFeat, with mixup, and the method combined MixFeat and mixup on CIFAR-10 using 20-layer ResNets (pre-activation). The increase in the error rate with data reduction is suppressedwith MixFeat and mixup. Additionally, the method combined those two has better performancethan of each method. Center: Comparison of the results of original MixFeat, i.i.d.-MixFeat, 1D-MixFeat, inner-MixFeat, and Gaussian noise with changing hyperparameters σ or α on CIFAR-10using 20-layer ResNets (pre-activation). The original MixFeat has the highest performance. Right:Comparison of the results for various hyperparameter σ values in MixFeat for various settings. Thebest value is σ = 0.2 in each setting.
Figure 6: Distribution of various perturbations: (a) MixFeat / i.i.d.-MixFeat, (b) 1D-MixFeat, (c)inner-MixFeat, and (d) Gaussian noise.
Figure 7: Visualization of feature distributions after 1,200-epoch training with a six-layer neuralnetwork on two-dimensional toy data. Top row shows the results without MixFeat, Bottom rowshows the results with MixFeat. (a) The input distribution, (b) intermediate distributions after thesecond layers, (c) intermediate distributions after fourth layers, and (d) output distribution. In thedistributions obtained with learning using MixFeat, it is easier to distinguish each class at eachdepth.
