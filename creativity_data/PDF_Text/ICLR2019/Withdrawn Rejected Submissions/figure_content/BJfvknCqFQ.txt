Figure 1: Examples of adversarial transformations and their predictions in the standard, ”blackcanvas”, and reflection padding setting.
Figure 2: Loss landscape of a random example for each dataset when performing left-right transla-tions and rotations. Translations and rotations are restricted to 10% of the image pixels and 30°, re-spectively. We observe that the landscape is significantly non-concave, rendering first-order methodsto generate adversarial example ineffective. Figure 11 in the appendix shows additional examples.
Figure 3:	MNIST. Successful adversarial examples for the models studied in Section 4. Rotationsare restricted to be within 30° of the original image and translations UP to 3 pixels per direction(image size 28 × 28). Each example is visualized along with its predicted label in the original andpertUrbed versions.
Figure 4:	CIFAR10. Successful adversarial examples for the models studied in Section 4. Rotationsare restricted to be within 30° of the original and translations UP to 3 pixels per directions (imagesize 32 × 32). Each example is visualized along with its predicted label in the original and perturbedversion.
Figure 5: ImageNet. Successful adversarial examples for the models studied in Section 4. Rotationsare restricted to be within 30° of the original and translations UP to 24 pixels per directions (imagesize 299 × 299). Each example is visualized along with its predicted label in the original andpertUrbed version.
Figure 6:	Sample adversarial transformations for the ”black-canvas” setting for the standard modelson CIFAR10 and ImageNet.
Figure 7:	Sample adversarial transformations for the reflection padding setting for the standardmodels on CIFAR10.
Figure 8: Fine-grained dataset analysis. For each model, we visualize what percent of the test setcan be fooled via various methods. We compute how many examples can be fooled with eithertranslations or rotations (”any”), how many can be fooled only by one of these, and how manyrequire a combination to be fooled (”both”).
Figure 9: Visualizing which angles fool the classifier for 50 random examples. For each dataset andmodel, we visualize one example per row. Red corresponds to misclassification of the images. Weobserve that the angles fooling the models form a highly non-convex set.
Figure 10: Cumulative Density Function plots. For each fraction of grid points p, we plot the per-centage of correctly classified test set examples that are fooled by at least p of the grid points. Forinstance, we can see from the first plot, MNIST Translations and Rotations, that approximately 10%of the correctly classified natural examples are misclassified under 1/5 of the grid points transfor-mations.
Figure 11:	Loss landscape of 4 random examples for each dataset when performing left-right trans-lations and rotations. Translations and rotations are restricted to 10% of the image pixels and 30°respectively. We observe that the landscape is significantly non-concave, making rendering FOmethods for adversarial example generation powerless.
Figure 12:	Accuracy of different classifiers against '∞-bounded adversaries with various values ofε and spatial transformations. For each value of ε, We perform PGD to find the most adversarial '∞-bounded perturbation. Additionally, we combine PGD with random rotations and translations andwith a grid search over rotations and translations in order to find the transformation that combineswith PGD in the most adversarial way.
