Figure 1: Face completion results of our method on CelebA-HQ (Karras et al., 2017). Top: ourapproach can complete face images at high resolution (1024 × 1024). Middle and Bottom: thefrequency-attention filters of readers and writers of the top left image. While the resolution increasesfrom 8 × 8 to 1024 × 1024, the model attends on higher frequency information. Regions with richdetails (e.g. eyes) get more attention, especially at high resolutions. Best viewed in magnification.
Figure 2: Overview of our method. See text fordetails.
Figure 3: The proposed FAM used in growing GANs progressively. Here, We show the example of increasiresolutions from 32 × 32 to 64 × 64. See text for details. Best viewed in color and magnification.
Figure 4: Comparison with Context Encoder (CE) on high-resolution face completion. While increasingresolution, CE generated more distorted images while our method produced sharper faces with more details.
Figure 5: Sample results of our approach. The left two groups are completion results with center andirregular hand-drawn masks at 1024 × 1024. For each group, from left to right columns: cropped,synthesized and real images. The third group shows the performance of the attribute controller, inwhich the first and third row are corrupted images and source actors whose facial landmarks are usedto control the expressions of synthesized faces (row two and four). The right most two columns areconditioned on the “Male” attribute while column two and three are with “Not Male”. The leftmostcolumn depends on their ground-truth landmarks and attributes.
Figure 6: Comparisons on the naturalness: ours and CTX (Yu et al., 2018). The leftmost bar chartshows the average percentage that the images generated by our method look more natural than CTX.
Figure 7: Some failure cases of our approach.
Figure 8: Ablation Study. First row: cropped images; Second row: results from a model that istrained with only regular L1 loss (unweighted) and Ladv . The vanishing gradient problem preventsthe networks from making progress when the synthesized content is still blurry. Third row: thetraining process is stabilized by adopting the progressive training methodology and a set of designedloss functions. However, the already-learned structures can be distorted while the network is growing(e.g. first column); Fourth row, FAM helps prevent the coarse structures from being altered whileencouraging the model to attend to regions with rich details. For instance, the eyes are sharper, morevivid and realistic when the model is trained with FAM; Fifth row: the ground-truth samples.
Figure 9: High-resolution face completion results with center masks. All images are resized from1024 × 1024. For each group, from left to right: cropped, synthesized, and real images.
Figure 10: High-resolution face completion results with random and hand-drawn masks. All imagesare resized from 1024 × 1024. For each group, from left to right: cropped, synthesized, and realimages.
Figure 11: Face completion results with attribute controller. In this example, only attribute vectors([“Smiling”, “Male”]) are used to control the properties of generated images. The facial expressionsare controlled with the latent variables, rather than landmarks. From column two to five, the attributesare: [0, 0], [0, 1], [1, 0], [1, 1]. “1” denotes an attribute is turned on, otherwise not.
Figure 12: Face completion results with attribute controller. Attribute “Male” is used to control theappearances (“Male” for column two and three; “Not Male” for column four and five). Landmarksfrom source actors (row one and three) are used to control expressions of synthesized images (rowtow and four). The leftmost column shows cropped images and faces generated with ground-truthattributes and landmarks.
Figure 13: More examples of the attentive read/write filters while the resolution grows from 8 × 8 to1024 × 1024. The leftmost column are cropped images while the rightmost are synthesized images.
Figure 14: The progressive training process of our approach. The training of the completion network(or the “generator” G) and the discriminator D starts at low resolution (4 × 4). Higher layers areadded to both G and D progressively to increase the resolution of the synthesized images. TheIr X r I cubes in the figure represent convolutional layers that handle resolution r. For the conditionalversion, attribute labels Aobs are concatenated to the latent vectors. The discriminator D splits intotwo branches in the final layers: Dcls that classifies if an input image is real, and Dattr that predictsattribute vectors. Note that X G and XD are both a set of inputs as defined in the paper. We useimages in this Figure as a simplified illustration.
Figure 15: Illustrations of a single layer of our architecture. There are skip connections betweenmirrored encoder and decoder layers. Left: the structure of the completion network; the skipconnection is a copy-and-concatenate operation. This structure helps preserve the identity informationbetween the synthesized images and real faces, resulting in little deformation. Right: the structureof the conditional completion network; residual connections are added to the encoder, and the skipconnections are residual blocks instead of direct concatenation. The attributes of the synthesizedcontents can be manipulated more easily with this structure. Each blue rectangle represents a set ofConvolutional, Instance Normalization and Leaky Rectified Linear Unit (LeakyReLU) (Maas et al.,2013) layers.
