Figure 1: Computation flow of our hierarchical memory network on an example from WikiHop(Welbl et al., 2017). The question is encoded to produce a query q1, which produces sentence-level attention α and word-level attention β for each sentence. This attention computes a passagerepresentation m1 from which we form the query q2 for the next step of inference.
Figure 2: An example from the Path Finding task (Weston et al., 2015). The bold sentences denotethe gold reasoning chain. The arrows illustrates the reasoning process.
Figure 3: An example of graph search. A graph is formed based on entity relationships betweenthe document sentences. The bold path is the path from query to answer obtained by breadth-firstsearch.
