Figure 1: Visual representation of the Deep GA encoding method. From a randomly initializedparameter vector θ0 (produced by an initialization function φ seeded by τ0), the mutation functionψ (seeded by τ1) applies a mutation that results in θ1. The final parameter vector θg is the resultof a series of such mutations. Recreating θg can be done by applying the mutation steps in thesame order. Thus, knowing the series of seeds τ0...τg that produced this series of mutations isenough information to reconstruct θg (the initialization and mutation functions are deterministic).
Figure 2: GA and random search performance across generations on Atari 2600 games. Theperformance of the GA and random search compared to DQN, A3C, and ES depends on the game.
Figure 3: Example of high-performing individual on Frostbite found through random search.
Figure 4: How different algorithms explore the deceptive Image Hard Maze over time. Tradi-tional reward-maximization algorithms do not exhibit sufficient exploration to avoid the local op-timum (of going up into Trap 2, as shown in Fig. 5). In contrast, a GA optimizing for noveltyonly (GA-NS) explores the entire environment and ultimately finds the goal. For the evolutionaryalgorithms (GA-NS, GA, ES), blue crosses represent the population (pseudo-offspring for ES), redcrosses represent the top T GA offspring, orange dots represent the final positions of GA elites andthe current mean ES policy, and the black crosses are entries in the GA-NS archive. All 3 evolu-tionary algorithms had the same number of evaluations, but ES and the GA have many overlappingpoints because they revisit locations due to poor exploration, giving the illusion of fewer evaluations.
Figure 5: Image Hard Maze Domain and Results. Left: A small wheeled robot must navigate tothe goal with this bird’s-eye view as pixel inputs. The robot starts in the bottom left corner facingright. Right: novelty search can train deep neural networks to avoid local optima that stymie otheralgorithms. The GA, which solely optimizes for reward and has no incentive to explore, gets stuckon the local optimum of Trap 2. The GA optimizing for novelty (GA-NS) is encouraged to ignorereward and explore the whole map, enabling it to eventually find the goal. ES performs even worsethan the GA, as discussed in the main text. DQN and A2C also fail to solve this task. For ES, theperformance of the mean θ policy each iteration is plotted. For GA and GA-NS, the performanceof the highest-scoring individual per generation is plotted. Because DQN and A2C do not have thesame number of evaluations per iteration as the evolutionary algorithms, we plot their final medianreward as dashed lines. SI Fig. 4 shows the behavior of these algorithms during training.
