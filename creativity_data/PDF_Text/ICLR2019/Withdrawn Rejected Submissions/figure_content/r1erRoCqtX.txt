Figure 1:	Precision at k = 10 with different arrangement methods in the course of training (d = 50,b = 64). Using 104 × 104 stochastic blocks matrices with B ∈ {10, 20, 50, 100}. The switch pointfor the MIX method are shown in blue (to COO+LSH) and green (to IND). The solid lines are forJaccard LSH and the dashed lines are for angular LSH.
Figure 2:	Cosine gap with different arrangement methods in the course of training (d = 50, b = 64).
Figure 3: Expected increase inCosSim(fι, f2) for fi 〜Nd aftergradient update to same randomcontext C 〜NdEffectiveness of gradient updates When updateS on correSpond-ing aSSociationS of two entitieS are proceSSed in the Same minibatchthen the coSine Similarity of their embedding vectorS increaSeS. ThiSholdS alSo in early training when the embedding vectorS are randomlyinitialized. Similar entitieS have more correSponding aSSociationS(fraction equalS the Jaccard Similarity) and benefit more from thiSproperty. In particular, the SGNS loSS term for a poSitive example iSL+(f, C) = logσ(f, C) = log (1+exp(-f∙C)) . The gradient withrespect to f is Vf (L+(f, C)) = C中乂工/© and the respectiveupdate of f - f + η ι+ex11(f© C clearly increases CosSim(f, c).
Figure 4: Square 104 × 104 stochastic blocks when training with few example interactions (inde-pendent and coordinated samples), with minibatch size b = 4. Left to right: (T = 5, B = 10);(T = 5,B = 100), (T = 20,B = 10), (T = 20,B = 100).
Figure 5:	Precision at k = 10 with different arrangement methods in the course of training (d = 50,b = 4). Using 104 × 104 stochastic blocks matrices with B ∈ {10, 20, 50, 100}. The switch pointfor the MIX method are shown in blue (to COO+LSH) and green (to IND). The solid lines are forJaccard LSH and the dashed lines are for angular LSH.
Figure 6:	Precision at k = 10 with different arrangement methods in the course of training (d = 50,b = 256). Using 104 × 104 stochastic blocks matrices with B ∈ {10, 20, 50, 100}. The switch pointfor the MIX method are shown in blue (to COO+LSH) and green (to IND). The solid lines are forJaccard LSH and the dashed lines are for angular LSH.
Figure 7: Cosine gap with different arrangement methods in the course of training (d = 50, b = 4).
Figure 8:	Cosine gap with different arrangement methods in the course of training (d = 50, b = 256).
Figure 9:	Training (IND with b = 64) with different dimensions. From left: MOVIELENS 1 M (cosinegap and precision for k = 50) and AMAZON (cosine gap and precision for k = 50).
Figure 10:	Training (IND b = 64) with different dimensions on 104 × 104 Stochastic blocks. Fromleft: B = 10 (cosine gap and precision at k = 10) and B = 100 (cosine gap and precision at k = 10).
