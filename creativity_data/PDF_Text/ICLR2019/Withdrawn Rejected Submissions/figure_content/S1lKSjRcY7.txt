Figure 1: Continuous relaxations at β = 2 to a Bernoulli random variable Ber(qφ = 0.8): (a) showsthe GSM approximation and (b) shows a piece-wise linear approximation to the discontinuous CDF.
Figure 2: Convex, single binary-variable toy example: (a) The relaxed objectivef (ζ) = (ζ - 0.45)2.
Figure 3: NELBO of the RAM and sampled RAM estimators on MNIST trained using the 200H -784V architecture having a linear decoder: (a) plots the decrease in training NELBO for RAM andtwo variants of sampled RAM. (b) shows the computational savings of sampled RAM.
Figure 4: MNIST training on the linear architecture 200H - 784V: (a) compares training NELBOof CR estimators (all estimators use β = 2). Solid/dashed lines correspond to one/two-pass training.
Figure 5: MNIST training on the non-linear architecture 200H 〜784V: (a) compares trainingNELBO of CR estimators (all estimators use β = 2). Solid/dashed lines correspond to one/two-passtraining. (b) Final training NELBO for different β, (c) Final training NELBO using KL annealing;explicit annealing erases the gains of GSM.
Figure 6: MNIST training with REBAR using GSM and PWL relaxations, dependence of the finaltrained NELBO on β : (a) the linear architecture. (b) the non-linear architecture.
Figure 7:	Encoder training the with fixed pre-trained decoder on MNIST: (a) the linear architecture.
Figure 8:	Finding maximal cliques in the C1000.9 graph: (a) and (b) show the course of optimizationfor two settings of κ and different gradient estimators. (c) plots the final maximal clique size acrossκ for the estimators.
Figure 9: Single binary variable concave toy example. (a) Relaxed function f(ζ) = -(ζ - 0.45)2(b) Probability qφ(z = 1) during optimizationI.2	Categorical Toy ExampleWe consider a categorical example with convex and concave functions of a single categorical vari-able y having 10 values. We take f(y) = ± Pa(ga - ya)2 such that g0 = 0.9, g1 = 1.1, andgi>1 = 1. The convex function has a minimum at y1 = 1, while the concave function is minimizedat y0 = 1. We compare 4 estimators for minimizing this function: RAM of Eq. (5), PWL of Eq. (18),GSM of Eq. (19) and IGSM of Eq. (20). The probability of the true minimum is shown in Fig. 10.
Figure 10:	The probability of the true minimum for a single categorical-variable toy example with(a) concave function f(y) = - Pa(ga - ya)2 and (b) convex function f(y) = Pa(ga - ya)2I.3 Discrete Variational AutoencodersFig. 11 shows the results for all 4 architectures 200H - 784V, 200H - 200H - 784V, 200H 〜784V,and 200H 〜200H 〜784V at β = 2. Hierarchical models with two layers of latent units inFig. 11(b),(d) exhibit similar trends to the factorial case considered in the main text. The GSMestimator converges to a higher NELBO in the linear case and becomes unstable in the non-linearcase.
Figure 11:	MNIST training (CR estimators use β = 2). Solid/dashed lines correspond to one/two-pass training.
Figure 12:	MNIST training on categorical linear architecture 20 × 10H - 784V (CR estimators useβ = 2). Solid/dashed lines correspond to one/two-pass training.
Figure 13:	OMNIGLOT training (CR estimators use β = 2). Solid/dashed lines correspond toone/two-pass training.
