Figure 1: Depictions for different models of sequential data: (a) Graphical model for an RNN. â™¦ denotes adeterministic intermediate representation, (b) Graphical model for an HMM. denotes probabilistic states, (c)Graphical model for the proposed attentive state space model. (c) Unrolled instance-wise graphical depictionfor an attentive state space. Thickness of the arrows reflect the attention weights.
Figure 2: Illustration of phased attention mechanism. Left: Architecture of the phased attention network. Thefollow-up data (augmented with the static features) are fed in a reversed order into the phased LSTM, withreversed and shifted timestamps for the hospital visits. Right: Illustration for the operation of the phasedLSTM. The time gate gt of 4 neurons are depicted; each has different oscillatory parameters. The contents ofthe cell state ct decay as we go backwards in time, implying smaller attention weights for older hospital visits.
Figure 3: Rearranged super states for the attentive model in Figure 1c with truncated attention and Q = 1. Theresulting graphical model (right) corresponds to a standard Hidden Markov model.
Figure 4: Depiction for the distributions of clinical observations and generated attention in the 3 learned pro-gression stages.
Figure 6: Illustration for the structure of the EHR data.
Figure 7: Depiction for the attentive state dynamics.
