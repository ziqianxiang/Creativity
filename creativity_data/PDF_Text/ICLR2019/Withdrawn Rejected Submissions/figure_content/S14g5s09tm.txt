Figure 1: Illustration of the encoder models used to learn a joint embedding space. Videos andsentences are mapped into a low-dimensional space by applying CNNs and temporal attention. Thenseveral fully-connected layers map into the joint embedding space. The decoders follow this samearchitecture with the weights transposed.
Figure 2: Visualization of several constrains on the shared embedding space. Circles are video data,ovals are reconstructed video. Diamonds are text data, and pentagons are reconstructed text. (a) Thereconstruction (Eq. 1) and joint (Eq. 3) losses. (b) Mapping from text to video using the cross-domain(Eq. 4) loss.
Figure 3: Visualization of the adversarial formulation to learn with unpaired data. We create 3discriminators, (1) Dz learns to discriminate examples of text/video in the latent space. (2) DV learnsto discriminate video generated from text compared to video. (3) DT learns to discriminate generatedtext compared to text.
Figure 4: Example video sequences from the MLB-YouTube dataset with the commentary caption.
Figure 5: Example captions for unseen activities. Left: Using a joint embedding space allows themodel to correctly caption this video as basketball, despite never seeing an example of basketballduring training. Right: An example of a caption for the unseen water-ski activity. Here the modelfails to correctly caption the activity.
Figure 6: t-SNE mapping of (a) fixed text representation and (b) joint embedding with all paired lossesfor the MLB-YouTube dataset. The joint embedding space provides most distinct representations forthe activities. Each color represents the activity class of the video (e.g., swing, hit, foul ball, etc.).
