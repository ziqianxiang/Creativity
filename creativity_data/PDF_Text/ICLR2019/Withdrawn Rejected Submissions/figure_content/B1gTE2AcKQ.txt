Figure 1: For VAEs and Wake-sleep, the recognition model q also serves as the variational dis-tribution that trains p. WSR distinguishes these, learning a recognition model r and a categoricalvariational posterior q which is separate from r. This means that like VAEs, WSR jointly trains pand q using an unbiased estimate of the variational objective (blue). Like wake-sleep, the recogni-tion model can train self-supervised (green), allowing WSR to handle discrete latent variables. Tooptimise the finite support of q, WSR incorporates a memory module M that remembers the bestvalues of zi found by r(zi ; xi) across iterations.
Figure 2: What trains what? r is a recogni-tion network trained self-supervised on samplesfrom the p. The memory Mi for each task i is aset of the ‘best’ z values ever sampled by r, se-lected according their joint probability p(z, xi).
Figure 3: One-shot generalisations produced by each algorithm on each the cellular automatadatasets. For each input image we sample a program z from the variational distribution q, thensynthesize a new image in the same style from p(z|x) using the learned .
Figure 4: Quantitative results on all variants of the cellular automata dataset. In all cases WSR learnsprograms which generalise to unseen images of the same concepts, achieving > 99% accuracy on a100-way classification task (second row). WSR also best recovers the true noise parameter = 0.01(third row). Note: x-axis is wallclock time on a single Titan-X GPU to allow a fair comparison, asWSR requires several times more computation per iteration.
Figure 5: Kernels inferred by the WSR for various real time series in the UCR dataset. Blue (left) isa 256-timepoint observation, and orange (right) is a sampled extrapolation using the inferred kernel(top, simplified where possible). The explicit compositional structure of this latent representationallows each discovered concept to be easily translated into natural language.
Figure 6: Quantitative comparison of models trained on the Text-Concepts dataset.
Figure 7: Comparison of ELBo between WSRand Wake-Sleep models for every example in thedataset. Examples in red are shown in Table 4.3ordered by WSR ELBo (descending).
