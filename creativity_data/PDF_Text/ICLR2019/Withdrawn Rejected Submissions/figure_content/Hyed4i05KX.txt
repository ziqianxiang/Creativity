Figure 1: An example of two hidden layer units withthe same function. The corresponding feature vectorsare the same, except that their signs are opposite.
Figure 2: Input image examples of MNIST data set.
Figure 3: Hierarchical clusters of an LNN (MNIST data set).
Figure 4: Representative input-output mappings of extracted clusters.
Figure 5: Hierarchical clusters of an LNN (food consumer price index data set).
Figure 6: Representative input-output mappings of extracted clusters.
Figure 7: Cluster structure of an LNN acquired by non-negative matrix factorization (MNIST dataset).
Figure 8: Representative input-output mappings of extracted clusters.
Figure 9:	Cluster structure of an LNN acquired by non-negative matrix factorization (food con-sumer price index data set).
Figure 10:	Representative input-output mappings of extracted clusters.
Figure 11: Left: Feature vectors of Definition 3. Each row corresponds to a feature vector for ahidden layer unit. Center: Feature vectors after the alignment of the signs. Right: Sum of thecosine similarities of all the pairs of feature vectors (MNIST data set).
Figure 12: Dendrograms of the hierarchical clustering results with the original feature vectors ofDefinition 3 (top) and with the feature vectors after the alignment of the signs (bottom).
Figure 13: Left: Feature vectors of Definition 3. Each row corresponds to a feature vector for ahidden layer unit. Center: Feature vectors after the alignment of the signs. Right: Sum of thecosine similarities of all the pairs of feature vectors (food consumer price index data set).
Figure 14: Dendrograms of the hierarchical clustering results with the original feature vectors ofDefinition 3 (top) and with the feature vectors after the alignment of the signs (bottom).
