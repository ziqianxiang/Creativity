Figure 1: Entropy of belief for a 3-state POMDP.
Figure 2: The belief reachabil-ity tree. The circles are be-lief points while squares de-pict branchings based on ac-tions. Addition of perceptionactions leads to combinatorialexpansion of number of beliefpoints in each layer.
Figure 3:	The robot moves in a grid while communicating with the cameras to localize itself. Thereis a camera at each state on the perimeter. The accuracy of measurements made by each cameradepends on the distance of the camera from that state. The robot’s objective is to reach the goalstate, labeled by star, while avoiding the obstacles.
Figure 4:	Results of 1-D simulation for a map of size 12, averaged over 1000 runs for each perceptionpolicy. Left: The average discounted cumulative reward. The solid lines depict the correspondingstandard deviations. Right: The average belief entropy over the time horizon of 25.
Figure 5:	The frequency of visiting states when using different perception methods for a 2-D mapof size 5*5 according to Figure 3-(b).
Figure 6: The probability simplex, ∆B , for a 3-state POMDP. Gray area illustrates the projection ofhypograph of entropy, corresponding to posterior belief after greedy perception action, onto ∆B .
Figure 7: Comparing random and greedy selection for 2-D robotic navigation in terms of the averagebelief entropy over the time horizon of 25.
Figure 8: The value function of a subset of size 10 from sampled belief points for 1-D navigation.
