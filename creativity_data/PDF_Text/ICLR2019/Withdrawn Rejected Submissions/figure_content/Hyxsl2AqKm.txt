Figure 1: Our model architecture includes a two-channel CNN followed by an LSTM video encoder,an action classifier, and an LSTM decoder for caption generation.
Figure 2: Our encoder includes a two-channel CNN followed by an LSTM for aggregating features.
Figure 3: Captioning examples:Model Outputs: [Piling coins up], [Removing mug, revealing cup behind].
Figure 4: 20bn-kitchenware samples: Using a knife to cut something (left), Trying but failing topick something up with tongs(right).
Figure 5: 20bn-kitchenware transfer learning results: averaged scores obtained using a VGG16, anInflated ResNet34, as well as two-channel models trained on four aforementioned tasks. We reportresults using 1 training sample per class, 5 training samples per class or the full training set.
Figure 6: Ground truth and model prediction for classification examples.
Figure 9: Ground truth captions and model outputs for video examples.
Figure 10: Grad-CAM for M(256-0) on video examples predicted correctly during fine-grainedaction classification. We can see that the model focuses on different parts of different frames in thevideo in order to make a prediction.
Figure 11: Grad-CAM on video example with ground truth caption Pretending to pick mouse up.
Figure 12: Grad-CAM on video example with ground truth caption Moving toy closer to toy. Wecan see that the model focuses on the gap between toys when using “Moving” token. It also looksat both toy objects when using the token “Closer”.
Figure 13: Grad-CAM on video example with ground truth caption Bottle being deflected fromball during captioning process. The model focuses on the collision between bottle and ball, whenusing token “Deflected”.
