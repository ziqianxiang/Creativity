Figure 2: Exemplary representation of the architecture of our proposed model. On the left, anexpensive pretrained DNN on ImageNet is trained on a specific dataset (transfer learning). Then, the(uncalibrated) output of such DNN is the input to the BNN calibration stage. This stage is trainedby the maximization of the Expected Lower Bound (ELBO) and predictions are done using MonteCarlo integration. The inputs and outputs of the Bayesian stage have same dimension (given by thenumber of classes), and lie in the so-called logit space. Orange Gaussians on each arrow representvariational distributions on parameters. We do not plot all the arrows for clarity. This Bayesian stageis independent of the previous one as we only require access to the logits of an already trained model.
Figure 3: This figures compares the ECE performance for TS and BNN in test and validation. On theleft (CIFAR10) we show the performance of different training parameters. For example 30MC_500means that the ELBO was optimized using 30 MC steps to estimate the expectation and 500 epochsof Adam optimization. On the right (CIFAR100) we show the performance of a BNN trained withdifferent number of epochs up to 2000, showing the robustness against the course of learning.
Figure 4: Bimodal distribution of 2-dimensional training data (orange and blue points) with MaximumLikelihood estimation of a Gaussian distribution (red contour) and other possible likelihood modelsexplaining the data (dashed plots). Green and black dots represent data not seen in the training data,for which densities are to be assigned. Best viewed in color.
