Figure 1: Examples of the dense block and the DenseNet with standard dropout method. Whitespots denote dropped neurons. The right figure shows that the same feature maps (orange color)after dropout layer will be directly sent to later layers, which makes the dropped features alwaysunavailable to later layers.
Figure 2: Examples illustrating data flow in one dense block of standard dropout method and pre-dropout method. The blue boxes represent dropout layers, while the yellow boxes represent com-posite functions. Xã€‚, Xi and Xi are the data tensors. W stands for the output of the dense block.
Figure 4: Different probability schedules. The numbers besides blue boxes represent survival proba-bilities. Left figure shows the linearly decaying schedule v1 which applies linearly decaying proba-bilities on different layers of DenseNet. Right figure shows v3 schedule which applies various prob-abilities on different portions of the input to a convolutional layer depending on distances betweenlayers generating those portions and the input layer.
Figure 5: Training on CIFAR100. Thin curves de-note training error, and bold curves denote test error.
