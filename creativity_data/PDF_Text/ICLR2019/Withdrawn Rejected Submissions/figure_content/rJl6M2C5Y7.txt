Figure 1: Experiments on toy examples. (a) Rosenbrock objective values during training; (b) Learning ratesfor RMSprop compared to the adaptive learning rate of RMSprop-APO on Rosenbrock; (c) Loss on the badly-conditioned regression problem; (d) Learning rate adaptation on the badly-conditioned regression problem.
Figure 2: Experiments on MNIST and CIFAR-10, with and without APO. Upper row: mean loss over thetraining set. Middle row: accuracy on the test set. Bottom row: learning rate per iteration. We use ResNet34for CIFAR-10, and train for 200 epochs with learning rate decayed by a factor of 10 every 60 epochs.
Figure 3: K-FAC results on CIFAR-10. We compare K-FAC with a fixed learning rate and a manual learningrate schedule to APO, used to tune 1) the learning rate; and 2) both the learning rate and damping coefficient.
Figure 4: Comparison of SGD and SGD-APO on CIFAR-100.
Figure 5:	Comparison of RMSprop and RMSprop-APO used to optimize a ResNet on SVHN.
Figure 6:	SGD with weight decay compared to SGD-APO without weight decay, on CIFAR-10.
Figure 7: (a) Tuning multiple RMSprop parameters from {η, ρ, } on MNIST. (b) Tuning per-layer learningrates for SGD on MNIST. (c) Tuning per-layer learning rates for RMSprop on MNIST. (d) K-FAC and RMSpropon FashionMNIST.
Figure 8: Adaptation of ρ and using RMSprop-APO on MNIST.
Figure 9: RMSprop-APO convergence from different initializations on the Rosenbrock surface. Wefind that RMSprop-APO achieves very low training loss starting from any of the points (x, y) ∈{(-1, 1.5), (-2, 2), (0, 0), (0.5, 0.5), (1, -1)}.
Figure 10: RMSprop-APO performance on Rosenbrock,optimizer.
Figure 11: Robustness to initial learning rates on MNIST and CIFAR-10. Top row: RMSprop-APO tuningan MLP on MNIST. Bottom row: RMSprop-APO tuning ResNet34 on CIFAR-10. (a) Training loss; (b) testaccuracy; (c) learning rate adaptation.
Figure 13: Loss and learning rate for APO.
Figure 12: Loss and learning rate for SGD.
Figure 14: Adam results on CIFAR-10. We compare Adam with a fixed learning rate and a manual learningrate schedule to Adam-APO.
Figure 15: Training loss and test accuracy of APO and PBT, as a function of wall-clock time.
