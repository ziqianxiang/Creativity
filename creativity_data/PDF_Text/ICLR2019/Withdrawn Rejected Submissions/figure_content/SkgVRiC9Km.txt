Figure 1: An investigation showing why fortified networks improve robustness. The illustrations onthe left show how the deeper layers of a network can have a simpler manifold and statistical structureas compared to in the visible space. The plot on the right shows direct experimental evidence forthis hypothesis: we added fortified layers with different capacities to MLPs trained on MNIST, anddisplay the value of the total reconstruction errors for adversarial examples divided by the totalreconstruction errors for clean examples. A high value indicates success at detecting adversarialexamples. Our results support the central motivation for fortified networks: that off-manifold pointscan much more easily be detected in the hidden space (as seen by the relatively constant ratio for theautoencoder in hidden space) and are much harder to detect in the input space (as seen by this ratiorapidly falling to zero as the input-space autoencoderâ€™s capacity is reduced).
Figure 2: Diagram illustrating a one-layer fortified network. A network is evaluated with a datasample x and its corresponding adversarial example xe. Hidden units hk and hk are corrupted withnoise, encoded with the encoder enc., and decoded with the decoder dec. The autoencoder (denotedby the red color) is trained to reconstruct the hidden unit hk that corresponds to the clean input.
