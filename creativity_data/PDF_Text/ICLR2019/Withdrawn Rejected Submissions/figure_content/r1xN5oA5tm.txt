Figure 1: Homogeneous multi-head attention, where each attention head features one n-gram type.
Figure 2: Heterogeneous n-gram attention for each attention head. Attention weights and vectorsare computed from all n-gram types simultaneously.
Figure 3: Interleaved phrase-to-phrase heterogeneous attention. The queries are first transformedinto unigram and bigram representations, which in turn then attend independently on key-valuepairs to produce unigram and bigram attention vectors. The attention vectors are then interleavedbefore passing through another convolutional layer.
Figure 5: Attention heat maps at layer 3 and layer 6 of QueryK heterogeneous model fora samplesentence pair in English-German newstest2014 test set. The left half in each figure indicates token-to-token mappings, while the right half indicates token-to-phrase mappings.
Figure 6: Attention heat maps at layer 3 and layer 6 of QueryK interleaved heterogeneous modelfor another sample from English-German newstest2014 test set. Upper-left, upper-right, lower-left,lower-right quadrants of each figure show token-to-token, token-to-phase, phrase-to-token, phrase-to-phrase alignments respectively.
