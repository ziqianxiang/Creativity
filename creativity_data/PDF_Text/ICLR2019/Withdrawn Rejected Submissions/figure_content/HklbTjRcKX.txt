Figure 1: Samples generated using a PixelCNN++ decoder model, conditioned on hidden activationscreated by processing an image of a horse (a) in a ResNet (h3, see Section 5) in classifier training.
Figure 2: Information curves for comparison with Shwartz-Ziv & Tishby (2017) and Saxe et al.
Figure 3: Mutual Information tracking in the forward direction - I(y; hj), (a) and (b) - and in-verse direction - I(x; hj), (c) and (d). The classifier always increases MI with the target data (a),while the autoencoder’s bottleneck layer compresses label-information. The orange curve in (a) iscomputed from the classifier’s log-likelihood throughout training. Information compression is evi-denced in both training regimes, the degree to which is listed as ∆c in nats on the right of (c) and(d). Increasing linear separability is also shown in (a). For forward decoding of the autoencoder (b),I(y; h4) = I(y; h3) since the difference in decoding model is only an average pooling operation,which is applied during encoding for h4 and decoding for h3 .
Figure 4: Samples generated using PixelCNN++, conditioned on h3 in the classifier training set-up.
Figure 5: Samples generated using PixelCNN++, conditioned on h4 in the classifier training set-up.
Figure 6: The ResNet model architecture (encoder) used to generate hidden activations in eithera classification or autoencoder set-up. Each convolution block (inner central blocks) consists of:convolution → BatchNorm → leaky ReLU non-linearity. Additional convolution → BatchNormblocks are used at necessary skip connections (‘/2’ blocks). The hidden representations (h1, . . . , h4)are taken at the ends of ‘hyper layers’, which are the three grouped and separately coloured series ofblocks. This encoder is a 21 layer ResNet architecture, accounting for the convolutions in both skipconnections.
Figure 7: Samples generated using PixelCNN++, conditioned on h2 in the classifier training set-up.
Figure 8: PixelCNN++ decoder models’ loss curves for estimating I(x; h2), for classifier training.
Figure 9:	PixelCNN++ decoder models’ loss curves for estimating I(x; h3), for classifier training.
Figure 10:	Unconditional PixelCNN++ loss curves when trained on the encoder dataset of CINIC-10. Since this is only using one third of CINIC-10, it may be possible to achieve a lower losswhen using a larger portion of CINIC-10. The best evaluation loss here corresponds to 3.58 bits perdimension, as opposed to the 2.92 bits per dimension on CIFAR-10 (Salimans et al., 2017).
Figure 11: Unconditional PixelCNN++ generated samples when trained on the encoder dataset ofCINIC-10. These samples have good local qualities but are not particularly convincing as realimages. This is a known pitfall of autoregressive explicit density estimators.
