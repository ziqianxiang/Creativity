Figure 1: In traditional GANs, f *(x) is only defined on the supports of Pg and Pr and its values donot reflect any information about the locations of other points in Pg and Pr . Therefore, they have noguarantee on the convergence. Wasserstein distance in a compacted dual form suffers from the sameproblem. GANs under LiPSchitz constraint builds connection between Pg and Pr, where Nxf *(x)pulls Pg towards Pr .
Figure 2: The source of Mode Collapse. In traditional GANs, f * (x) is a function of the localdensities Pg (x) and Pr (x). Given f *(x) is an increasing function of Pr (x) and decreasing functionof Pg (x), when fake samples get close to a mode of the Pr, Nxf *(χ) move them towards the mode.
Figure 3: Verifying the objective family(d) exp(x)Figure 4: Vxf *(x) gradation with CIFAR-10We verify a set of φ and φ satisfying Eq. (12): (a) φ(x) = 夕(一x) = x; (b) φ(x) = 夕(一x) =—log(σ(—x)); (c) φ(x) = φ(-x) = X + xχ+ + 1; (d) φ(x) = φ(-x) = exp(x). As shown inFigure 3, the gradient of each generated sample is towards a real sample.
Figure 4: Vxf *(x) gradation with CIFAR-10We verify a set of φ and φ satisfying Eq. (12): (a) φ(x) = 夕(一x) = x; (b) φ(x) = 夕(一x) =—log(σ(—x)); (c) φ(x) = φ(-x) = X + xχ+ + 1; (d) φ(x) = φ(-x) = exp(x). As shown inFigure 3, the gradient of each generated sample is towards a real sample.
Figure 5: f *(x) in new objective is more stable.
Figure 6: Training curves on CIFAR-10.
Figure 7:	ADAM optimizer with lr=1e-2, beta1=0.0, beta2=0.9. MLP with RELU activations, #hid-den units=1024, #layers=1.
Figure 8:	ADAM optimizer with lr=1e-2, beta1=0.0, beta2=0.9. MLP with RELU activations, #hid-den units=1024, #layers=4.
Figure 9:	ADAM optimizer with lr=1e-5, beta1=0.0, beta2=0.9. MLP with RELU activations, #hid-den units=1024, #layers=4.
Figure 10:	SGD optimizer with lr=1e-3. MLP with SELU activations, #hidden units=128, #lay-ers=64.
Figure 11:	SGD optimizer with lr=1e-4. MLP with SELU activations, #hidden units=128, #lay-ers=64.
Figure 12: Various φ(x) and D(x) that satisfies Eq. 12.
Figure 13: FiD and iCP (incePtion score) training curves of different objectives on Tiny imageNet.
Figure 14: Random Samples of Lipschitz GAN trained of different objectives on Oxford 102.
Figure 15: Random Samples of Lipschitz GAN trained of different objectives on Cifar-10.
Figure 16: Random Samples of Lipschitz GAN trained of different objectives on Tiny Imagenet.
Figure 17: The gradient of Lipschitz constraint based GANs with real world data, where Pr consistsof ten images and Pg is Gaussian noise. Up: Each odd column are X 〜Pg and the nearby column aretheir gradient Vxf *(x). Down: the leftmost in each row are X 〜 Pg, the second are their gradientsVxf *(x),the interior are X + e ∙ Vxf *(x) with increasing e, and the rightmost are the nearest y 〜Pr.
Figure 18: Comparison between gradient penalty and maximum gradient penalty, with Pr and Pgconsist of ten real and noise images, respectively. The leftmost in each row is a X 〜Pg and thesecond is its gradient Vxf *(x). The interior are X + e ∙ Vxf *(x) with increasing e, which will passthrough a real sample, and the rightmost is the corresponding y 〜Pr.
