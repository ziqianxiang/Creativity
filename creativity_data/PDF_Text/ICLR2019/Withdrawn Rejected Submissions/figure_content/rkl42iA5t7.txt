Figure 1: Results of different SimpleCNN compressed networks. Accuracy change in the y axis isreported in percentage points (error bars show the standard deviation of multiple runs). Note how allPFA algorithms lie close to the upper bound while the random pruning severely degrades accuracy.
Figure 2: Results for VGG-16 compressed networks. Accuracy change in the y axis is reported inpercentage points. Notice how the accuracy obtained by PFA is comparable to the state of the artwhile achieving much better compression ratios.
Figure 3: Domain adaption from CIFAR-100 and from CIFAR-10. (a,c) PFA fine matches theaccuracy of PFA full while using architectures more than 4x smaller. PFA fine largely outperformsthe full model trained from scratch Full scratch. The vertical percentage labels show the PFAcompression ratio. In (b,d), recipes for VGG-16 trained on CIFAR-100 and CIFAR-10 using PFA-KLwith data from different target domains. Note how PFA exploits the knowledge of the target domain(b), creating recipes adapted to the task complexity.
