Figure 1: Overview of our model. Our modelconsists of two components: a model-basedand gradient-based meta-learner. The for-mer strives to identify the task from a fewsamples and modulate the prior accordingly;the latter performs gradient updates to effec-tively adapt to the specific task.
Figure 2: Qualitative Visualization of Regression on Three-modes Simple Functions Dataset. (a): We comparethe predicted function shapes of modulated MuMoMAML against the prior models of MAML and Multi-MAML, before gradient updates. Our model can fit the target function with limited observations and no gradientupdates. (b): The predicted function shapes after five steps of gradient updates, MUMOMAML is qualitativelybetter. More visualizations in Supplementary Material.
Figure 3: (a) Comparing the models’ performance with respect to the number of gradient updates applied. ForMUMOMAML, we report the performance after modulation for gradient step 0. (b) A demonstration of themodulation on prior model by our model-based meta-learner. With the FiLM modulation, MuMoMAML canadapt to different priors before gradient-based adaptation.
Figure 4: tSNE plots of the task embeddings produced by our model from randomly sampled tasks; markercolor indicates different modes of a task distribution. The plots (a) and (c) reveal a clear clustering accordingto different task modes, which demonstrates that MuMoMAML is able to identify the task from a few sam-ples and produce a meaningful embedding υ . (a) Regression: the distance among distributions aligns with theintuition of the similarity of functions (e.g. a quadratic function can sometimes be similar to a sinusoidal or alinear function while a sinusoidal function is usually different from a linear function) (b) Few-shot image clas-sification: we observe a embedding manifold with some sub-structures appearing. However, it is not intuitive tounderstand them directly. (c) Reinforcement learning: the embeddings for 2D navigation goals sampled fromtwo Gaussian distributions environment are cleanly separated.
Figure 5: (a) A comparison between MUMOMAML (orange) and MAML (blue) on a 2D navigation task.
Figure 6: Sample trajectories in the 2D navigation environment. In the before modulation plot on the left, theMuMoMAML policy (line) moves randomly around one of the modes. In the middle plot, before gradientupdates are applied, MuMoMAML navigates rapidly to the correct modes, while MAML policy (dashed line)has not yet observed any rewards. On the right, after three gradient updates, both have found good policies forthe goals, while MuMoMAML converges on the goals in fewer steps.
Figure 7:	Additional qualitative results of the regression tasks (a): MUMOMAML after modulation vs.
Figure 8:	Additional qualitative results of the regression tasks (b): MUMOMAML after adaptation vs. otherposterior models.
Figure 9: Additional trajectories sampled from the 2D navigation environment with MuMoMAML.
