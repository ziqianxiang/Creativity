Figure 1: The Differentiable Greedy Network (DGN). The overall structure of the network on theleft shows that the input features X are run through a linear encoding layer and a ReLU activationfunction. The resulting non-negative features H are then run through successive greedy layers alongwith the state vector which encodes the progression of sentence selections. The right diagram detailsa greedy layer. In the left branch, for greedy iteration i, the network enumerates the possible nextstates, {si ∪ {v} : ∀v ∈ V \si}. The network then evaluates these potential next states in the contextof the current state si and finds the sentence that provides the biggest increase in terms of fα(∙),which is then added to the current state to form the new state si+1. During training, the selectionis done through a softmax with temperature τ to differentiably approximate the argmax, and at testtime we use the argmax to ensure the submodular guarantee of the model.
Figure 2: Recall@7 on the validation set. The DGN seems to fit very quickly to the full set interms of cross entropy, but the recall improves consistently. Decreasing the learning rate results in asmoother and consistently decreasing loss, but lower recall.
