Figure 1: 2 vs. 2 accuracy for theactivation layers of ResNet-50.
Figure 2: 2 vs. 2 accuracy for the layers ofInception-v3. Inception modules introduce par-allel paths each containing activation nodes.
Figure 3: The emergence of semantic information during training of FractalNet on CIFAR-100. Left:training and test error for the first 60 epochs training. Right: the 2 vs. 2 accuracy for layers ofFractalNet during training. We show 2 vs. 2 accuracy at the first convolutional layer, the end of everysecond fractal network block, and the last fully connected layer. The initial layers of the CNN learnsemantics before the later layers, but later layers continue to improve in later epochs.
Figure 4: 1 vs. 2 accuracy for misclassifiedimages, averaged within modules of ResNet-50.
Figure 5: 1 vs. 2 accuracy for misclassi-fied images, averaged within modules ofInception-v3.
Figure 6: 1 vs. 2 accuracy through layers of Inception-v3 for adversarial examples. When the accuracyis above 50, there is more evidence for the true class than the adversarial class. Left: Average 1 vs.
Figure 7: A sample of eight targeted adversarial examples.
Figure 8:	The 2 vs. 2 accuracy through architecture diagram of ResNet-50. The architecture diagramof ResNet-50 is annotated with 2 vs. 2 accuracy of layers against Skip-Gram word-vectors (He et al.,2015). This is a high resolution image and can be zoomed/viewed in a pdf.
Figure 9:	2 vs. 2 accuracy through architecture diagram of Inception-v3. The architecture diagram ofInception-v3 is annotated with 2 vs. 2 accuracy of layers against Skip-Gram word-vectors (Szegedyet al., 2015b). This is a high resolution image and can be zoomed/viewed in a pdf.
Figure 10: The emergence of semantic information during training of FractalNet on CIFAR-100.
