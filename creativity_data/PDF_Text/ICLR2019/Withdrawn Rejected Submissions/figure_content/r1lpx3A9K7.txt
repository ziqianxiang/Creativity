Figure 1: (a) The semantic features of images should be unchanged before and after the adversarialperturbation. Via FBGAN, original, adversarial and reconstructed images are encoded to similarsemantic codes. Each column stands for the ten-categorical code that related to the classificationof an image (see section 3 for details). Here all three images are classified as “7” from categoricalcodes. (b) Besides a discriminator D and a generator G in the vanilla GAN, we add an encoderE mapping from the data space to the latent space, and the discriminator D takes a tuple (x, z)as input. There are three types of tuple (x, z): (x, E(x)) for X 〜 Pχ, (G(z), Z) for Z 〜 Pz and(G(E(x)), E(x)) for x 〜PX; the discriminator D treats the first type as real and the other twoas fake. Mutual information between latent codes z and generated G(z) is maximized in order todisentangle the semantic features.
Figure 2: Implementation The encoder E(x)is a convolutional network and the generator G(z)is a deconvolutional network. The discriminatorD(x, z) shares parameters with the auxiliary func-tion g(x). Z = (Zc, ZLn) stands for the Categori-cal and continuous codes.
Figure 3: Manipulating semantic codes on MNIST and FMNIST Images generated by oneten-dimensional categorical code and eight continuous codes. (a) and (c) demonstrate that we cangenerate any category of images by changing the categorical codes. (b) and (d) are the effects ofcontinuous codes: each row shows how the generated image changes when tuning one continuouscodes with all other codes fixed.
Figure 4: Reconstruction of MNIST and FMNIST The first two rows are the original testset images and their reconstructions; the middle two rows are the gray-box adversaries and theirreconstructions; the last two rows are the white-box adversaries and their reconstructions. All theadversaries are from PDG with purtabation ε = 0.3.
Figure 5: Generation and reconstruction of SVHN (a) and (c) are generated images by changingthe categorical codes and continuous codes respectively, similar to Figure 3. We observe that thecontinuous codes shown in (c) control: the blurriness (from clear to blurry), brightness (from brightto dark), background color (from green to brown) and the feature on the edge. (b) and (d) are theadversarial defense results. (b) shows the accuracy on clean, adversarial and reconstructed images,similar to Table 1. In (d), the first two rows are the clean images and their reconstructions, and thelast two rows are the gray-box adversaries (PGD, ε = 0.1) and their reconstructions. The semanticcodes consist 4 ten-categorical codes and 128 continuous codes.
Figure 6: Performance of vanilla BiGAN (a) illustrates MI gap (5) of the categorical code,where FBGAN converges fast but BiGAN does not. (b) and (c) are generated images by changingthe categorical code and continuous codes, similar to Figure 3. The semantic features are entangledin the latent codes of BiGAN.
