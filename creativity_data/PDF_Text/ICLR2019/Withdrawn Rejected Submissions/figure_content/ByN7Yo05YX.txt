Figure 1: (Left). An example of an ANT architecture. Data is passed through transformers (blackcircles on edges), routers (white circles on internal nodes), and solvers (gray circles on leaf nodes).
Figure 2: Visualisation of class distributions (red) and path probabilities (blue) at respective nodesof an example ANT (a) before and (b) after the refinement phase. (a) shows that the learned modelcaptures an interpretable hierarchy, grouping semantically similar images on the same branches. (b)shows that the refinement phase polarises path probabilities, pruning a branch.
Figure 3: Using CIFAR-10, in (Left), we assess the performance of ANTs for varying amountsof training data. (Middle) The complexity of the grown ANTs increases with dataset size. (Right)Global refinement improves the generalisation; the dotted lines show the epochs at which the modelsenter the refinement phase.
Figure 4: Effect of patience level on the validation accuracy trajectory during training. Each curveshows the validation accuracy on CIFAR-10 dataset.
Figure 5: Illustration of discovered ANT architectures. (i) ANT-MNIST-A, (ii) ANT-MNIST-B, (iii)ANT-MNIST-C, (iv) ANT-CIFAR10-A, (v) ANT-CIFAR10-B, (vi) ANT-CIFAR10-C. Histogramsin red and blue show the class distributions and path probabilities at respective nodes. Small blackcircles on the edges represent transformers, circles in white at the internal nodes represent routers,and circles in gray are solvers. The small white circles on the edges denote specific cases wheretransformers are identity functions.
