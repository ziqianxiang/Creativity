Figure 1: (Top) Pooling does not preserve Shift-equivariance. It is functionally equivalent to densely evaluatedpooling followed by naive downsampling. The latter operation ignores the Nyquist sampling theorem andloses shift-equivariance. (Bottom) We low-pass filter between the operations. This keeps the original poolingoperation, while antialiasing the appropriate signal. This equivalent analysis and modification can be appliedto any strided layer, such as convolution.
Figure 2: Classification stability for selected images. Predicted probability of the correct class changes whenshifting the image. The baseline (black) exhibits chaotic behavior, which is stabilized by our method (blue).
Figure 3: Toy example of sensitivity to shifts. We illustrate how downsampling affects Shift-equivarianceWith a toy example. (Top-Left) An input toy signal is in light gray; max-pooled (k = 2, S = 2) toy signal isin blue. (Top-Right) Simply shifting the input and then max-pooling provides a completely different answer(red). (Bot-Left) The blue and red points are inherently sampled from densely max-pooled (k = 2, S = 1)intermediate signal (thick black). (Bot-Right) We instead sample from the low-passed intermediate signal,shown in green and magenta, better preserving shift-equivariance.
Figure 4: Shift-equivariance throughout the network. We compute feature distance between left and right-hand sides of the shift-equivariance condition in Equation 1. Each point in each heatmap is a shift (∆h, ∆w).
Figure 5: Classification consistency vs. classification. Networks trained (left) without and (right) with shift-based data augmentation, using various filters. UP (more consistent) and to the right (more accurate) is better.
Figure 6: Distribution of per-image classification variation. We show the distribution of classificationvariation in the test set, (left) without and (right) with data augmentation at training. Lower variation meansmore consistent classifications (and increased shift-invariance). Training with data augmentation drasticallyreduces variation in classification. Adding filtering further decreases variation.
Figure 7: Total Variation (TV) by layer.
Figure 8: Classification consistency vs. classification for DenSeNet Same test in as in Fig. 5, but withDenseNet (Huang et al., 2017) instead of VGG13 (Simonyan & Zisserman, 2014). We show networks trained(left) without and (right) with shift-based data augmentation, using various blurring filters. Consistency iscomputed by computing classification of an image with two random shifts, and checking for agreement. Up(more consistent) and to the right (more accurate) is better. Number of sides corresponds to number of filtertaps used (e.g., triangle for 3-tap filter); colors correspond to different methods for generating FIR filters.
Figure 9: Robustness to shift-based adversarial attack. Classification accuracy as a function of the numberof pixels an adversary is allowed to shift the image. Applying our proposed filtering increases robustness, bothwithout (left) and with right data augmentation.
Figure 10: Blurring before pooling. Blurring before the max-pooling (gray points) for different filters, ascompared to their PoolBlurDownsample counterparts (colored polygons). The poorer performing filters, whentraining without data augmentation, observe an increase in performance. For almost all filters when trainingwith data augmentation, and for the higher-performing filters training without data augmentation, performanceis significantly reduced, often in both accuracy and consistency. Directly blurring the downsampled signal(after the pooling layer), as proposed in the main paper, is more effective.
Figure 11: Average accuracy as a function of shift. (Left) We show classification accuracy across the testset as a function of shift, given different filters. (Right) We plot accuracy vs diagonal shift in the input image,across different filters. Note that accuracy degrades quickly with the baseline, but as increased filtering is added,classifications become consistent across spatial positions.
