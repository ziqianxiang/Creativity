Figure 1: Illustration of the proposed zero-shot learning framework. Each utterance is first mappedinto acoustic space H. Then we transform each point in the acoustic space into phonetic spaceP with a linear transformation V . Finally phoneme distributions can be obtained by applying asignature matrix Sas stripped, black and white. Inspired by approaches in computer vision research, we propose theUniversal Phonetic Model (UPM) to apply zero-shot learning to acoustic modeling. In this model wedecompose the phoneme into its attributes and learn to predict a distribution over various phoneticattributes. This can then be used to infer the unseen phonemes for the test language. For example,the phoneme /a/ can be decomposed into its attributes: vowel, open, front and unrounded.
Figure 2: Illustration of the sequence model for zero-shot learning. The input layer is first processedwith a Bidirectional LSTM acoustic model, and produces a distribution over phonetic attributes.
