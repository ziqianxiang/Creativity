Figure 1:	Analysis on hard batch selection strategy: (a) shows the true sample distribution accordingto the difficulty computed by Eq. (1) at the training accuracy of 60%. An easy data set (MNIST) doesnot have “too hard” sample but “moderately hard” samples colored in gray, whereas a relatively harddata set (CIFAR-10) has many “too hard” samples colored in black. (b) shows the result of SGDon a hard batch. The moderately hard samples are informative to update a model, but the too hardsamples make the model overfit to themselves.
Figure 2:	Key idea of Ada-Boundary: (a) shows the sampling process of Ada-Boundary, (b) showsthe results of an SGD iteration on the boundary samples.
Figure 3: Classification of CIFAR-10 samples using the softmax distribution obtained fromWideResNet 16-8 when training accuracy is 90%. If the prediction probability of the true labelis the highest, the prediction is correct; otherwise, incorrect. If the highest probability dominates thedistribution, the model’s confidence is strong; otherwise, weak.
Figure 4: Sample distribution according to the normalized diSt(xi, yi; θt) at the training accuracyof 80%, when training LeNet-5 (Se = 100) with the Fashion-MNIST data set. The distributions ofmini-batch samples selected by the rank-based and quantization-based approaches, respectively, areplotted together with the true sample distribution.
Figure 5: The distributions of mini-batch samples selected by the three variants in the same config-uration as Figure 4.
Figure 6: Convergence curves of five batch selection strategies with SGD on three data sets.
Figure 7: Convergence curves of Ada-Boundary(History) with SGD on three data sets.
Figure 8:	Convergence curves of Ada-Boundary(History) with momentum on three data sets.
Figure 9:	Convergence curves of Ada-Boundary with varying se on two hard data sets.
Figure 10: Convergence curves using the momentum optimizer for Figure 6.
