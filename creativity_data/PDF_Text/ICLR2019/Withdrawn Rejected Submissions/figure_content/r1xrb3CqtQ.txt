Figure 1: Architecture and training. Our method aims at transfer from one domain to another domainsuch that the correct semantics (e.g., label) is maintained across domains and local changes in thesource domain should be reflected in the target domain. To achieve this, we train a model to transferbetween the latent spaces of pre-trained generative models on source and target domains. (a) Thetraining is done with three types of loss functions: (1) The VAE ELBO losses to encourage modelingof z1 and z2, which are denoted as L2 and KL in the figure. (2) The Sliced Wasserstein Distanceloss to encourage cross-domain overlapping in the shared latent space, which is denoted as SWD. (3)The classification loss to encourage intra-class overlap in the shared latent space, which is denoted asClassifier. The training is semi-supervised, since (1) and (2) requires no supervision (classes) whileonly (3) needs such information. (b) To transfer data from one domain x1 (an image of digit “0”) toanother domain x2 (an audio of human saying “zero”, shown in form of spectrum in the example),We first encode xι to zι ~ q(zι∣xι), which We then further encode to a shared latent vector Z usingour conditional encoder, Z ~ q(zl∖zι,D = 1), where D donates the operating domain. We thendecode to the latent space of the target domain z2 = g(z|z0, D = 2) using our conditional decoder,which finally is used to generate the transferred audio x2 = g(x2∖Z2).
Figure 2: Synthetic data to demonstrate latent transformations. Both the low-level latent spaces andthe shared latent space are two dimensional. Best viewed in color. (a) Synthetic data. The left redeclipse denotes the domain 1, while the right green eclipse denotes domain 2. The color gradientdenotes the continuity of local changes. There are two classes, A (denoted as dots) and B (denotedas crosses), for both domains. Also note that for domain 1, label A and B are arranged up-and-down, while for domain 2 they are left-and-right. This is intentionally designed to force the modelto learn a rotation instead of “cheating” by squeezing the shape of ellipses. (b) Reconstructionsusing shared VAE. (c) Domain 1 transferred to domain 2. Note that the transfer correctly handlesclasses as well as continuity of local changes (d) Domain 2 transferred to domain 1. Observationis similar to (c). (e) The shared latent space where the blue line is the decision boundary of theclassifier. Here, the points from both domains are overlapping, class-separated, spread evenly, andmaintain the continuity of color gradient.
Figure 3:	Qualitative Results for Reconstruction. Images are divided into three groups, representingMNIST, Fashion MNIST and SC09 reconstruction respectively from left to right. Specifically, forSC09 we show the log magnitude spectrum of the audio. Within each group, the left is the originaland the right is the reconstruction. We see that the bridging autoencoder is able to archive high-quality reconstructions for MNIST and Fashion MNIST, and reasonable reconstructions for SC09.
Figure 4:	Qualitative Results for Domain Transfer. Images are divided into three groups, repre-senting MNIST - MNIST , MNIST - Fashion MNIST and MNIST - SC09 transfer respectivelyfrom left to right. Within each group, on the left is the data in the source domain and on the right isthe data in the target domain. We see that transfer maintains the label, yet still maintains diversityof samples, reflecting the transfer of a broad range of attributes.
Figure 5: Intra-Class Interpolation. Interpolations are divided into three groups, representing MNIST→ MNIST , MNIST → Fashion MNIST and MNIST → SC09 transfer respectively from top tobottom. Images in a red square are fixed points in the interpolations, which means interpolationhappens between two neighboring fixed points. In each group, there are three rows of interpolations:(1) Interpolate in source domain between fixed data points, (2) Transfer fixed data points in sourcedomain to target domain and interpolate between transferred fixed points there, (3) Transfer allpoints in first row to the target domain. Note that in this intra-class setting, transferring preservesthe smoothness of data when interpolating within one class.
Figure 6: Inter-class Interpolation. The arrangement of images is the same as Figure 5, except thatinterpolation now happens between classes. It can be shown that, unlike regular generative model(row 1 and row 2 in each group) that exhibits pixel (data) level interpolation, especially the blurrinessand distortion half way between instances of different labels, our proposed transfer (row 3) resortsto produce high-quality, in-domain data. This is an expected behavior since our proposed methodlearns to model the marginalized posterior of data distribution.
Figure 7: Synthetic data to demonstrate the transfer between 2-D latent spaces with 2-D shared latentspace. Better viewed with color and magnifier. Columns (a) - (e) are synthetic data in latent space,reconstructed latent space points using VAE, domain 1 transferred to domain 2, domain 2 transferredto domain 1, shared latent space, respectively, follow the same arrangement as Figure 2. Each rowrepresent a combination of our proposed components as follows: (1) Regular, unconditional VAE.
Figure 8: Model Architecture for our Conditional VAE. (a) Gated Mixing Layer, or GML, as animportant building component. (b) Our conditional VAE with GML.
Figure 9: Qualitative results from applying Pix2Pix (Isola et al., 2016) on the left and Cycle-GAN (ZhU et al., 2017) on the right, on the same settings used in Figured 4. Visually, both ex-isting transfer approaches suffer from less desirable overall visual quality and less diversity in localchanges, compared to our proposed approach. Particularly, Pix2Pix more or less makes semanticlabels correct but suffers from mode collapses in each label, while CycleGAN has slightly betterquality but suffers from label collapse, which is observable here that most of digits are transferredto Dress and leads to bad transfer accuracy.
