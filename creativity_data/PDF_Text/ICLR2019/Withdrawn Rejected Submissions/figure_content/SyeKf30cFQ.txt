Figure 1: Problem setting. (a) We use Greek letters {α,β,... ω} to represent receptive fields.
Figure 2: (a) Multiple nodes (neurons) share the same receptive field a. Note that na is the numberof nodes sharing the receptive field α. (b) Grouping nodes with the same receptive fields together. Byabuse of notation, α also represents the collection of all nodes with the same receptive field.
Figure 3: Batch Normalization (BN) as a projection. (a) Add BN by inserting a new node jbn. (b)ForWard/backward pass in BN and relevant quantities. (c) The gradient g that is propagated down isa projection of input gradient gbn onto the orthogonal complementary space spanned by {f, 1}.
Figure 4: Disentangled representation. (a) Nodes are grouped according to regions. (b) An exampleof one parent region α (2 nodes) and two child regions β1 and β2 (5 nodes each). We assumefactorization property of data distribution P. (C) disentangled activations, (d) Separable weights.
Figure 5: Overfitting ExampleHere is one example. Suppose there are two different kinds of events at two disjoint reception fields:Za and zγ. The class label is zω, which equals Za but is not related to zγ. Therefore, we have:~, . . ~, . .
Figure 6: Notation used in Thm. 1.
Figure 7: Original BN rule from (Ioffe & Szegedy, 2015).
Figure 8: Batch Normalization (BN) as a projection. (a) Add BN by inserting a new node jbn. (b)ForWard/backward pass in BN and relevant quantities. (c) The gradient g that is propagated down isa projection of input gradient gbn onto the orthogonal complementary space spanned by {f, 1}.
Figure 9: Disentangled representation. (a) Nodes are grouped according to regions. (b) An exampleof one parent region α (2 nodes) and two child regions β1 and β2 (5 nodes each). We assumefactorization property of data distribution P. (C) disentangled activations, (d) Separable weights.
