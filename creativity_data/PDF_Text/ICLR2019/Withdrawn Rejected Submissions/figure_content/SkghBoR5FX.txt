Figure 1: Model under attack, rePresenting the motion stream of a two-stream action recognitionclassifier. The model inPuts a stack of video frames, internally calculates and classifies an oPticalflow stack, then outPuts a Prediction.
Figure 2: Detailed diagram of the MUA. Text in black relates to the forward pass and text in bluerelates to the backward pass. Optical flow is calculated between frame pairs and concatenated toform an optical flow stack which is classified with the CNN.
Figure 3: Visualization of the perturbations required from each attack to cause a misclassificationfor a sample video clip. The adversarial examples shown are with = 0.025.
Figure 4: Plots showing how the accuracy of the classifier changes as epsilon changes for the threeattack variants. Specifically, (a) shows how the stack-level accuracy changes and (b) shows thevideo-level accuracy.
Figure 5: Percent of successful adversarial examples versus number of frames perturbed for iterativeattack variants. Each bar represents an average of results from all tested epsilons from 0.005 to 0.035.
Figure 6: This plot shows stack level transferability of attacks by attacking black-box models withexamples generated in the white-box setting. The prefix of the label represents the attack variant,OS=one-shot and IRG=it-saliency-RG. The suffix is the system being attacked. The FlowNet2 sys-tem is the baseline as it is the white-box system. TVL1 and Farn represent black-box systems withTV-L1 and Farneback optical flow algorithms and CNN trained models, respectively.
Figure 7: Examples of perturbations at each tested epsilon.
