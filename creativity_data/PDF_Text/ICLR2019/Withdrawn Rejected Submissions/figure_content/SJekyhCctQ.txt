Figure 1: Detecting adversarial examplesusing NeuralFP with N = 2 fingerprints,for K-class classification. 夕(x) is themodel output. NeuralFP separates realdata x (top) from adversarial data x0 =x + η (bottom) by comparing the sensi-tivity of the model to certain predefinedperturbations around unseen inputs with areference sensitivity encoded around themanifold of real images during training.
Figure 2: Geometry of fingerprints for SVMs with lin-early separable data. Let d(x) be the distance of x tothe decision boundary (see Thm 1). δ±max (δ±min) de-note the maximal (minimal) distances of the positive(x+) and negative (x-) examples to the separating hy-perplane hw, xi + b = 0. The fingerprint ∆x1 with(△x1,W〉= δmin will have f (x- + ∆x) < 0 andf(x- ) < 0 for all x- in the data distribution (redregion). Hence, △x1 will flag all x0 in the regions-δ-min < d(x0) < 0 as not real (d(x0) is the signeddistance of x0 from the boundary), since for those x0 itwill always see a change in predicted class. Similarly,△x2 with h∆x2 ,Wi = δmax always sees a class changefor real x-, flagging all x0 : d(x0) < -δ-max as not real.
Figure 3: Left: decision boundary without fingerprints. Center: with fingerprints, red arrows indicatefingerprint-directions. The decision boundary is significantly more non-linear. Right: contour plot offingerprint loss. NeuralFP detects dark regions as “real”, while lighter regions are “fake” (tunablethrough τ). Fingerprinting create valleys of low-loss delineating the data-distribution from outliers.
Figure 4: Fingerprint losses (mean overN fingerprints) on 100 random test(blue) and adversarial (red) CIFAR-10images. We see a clear separation inloss, illustrating that NeuralFP is effec-tive across many thresholds τ .
Figure 6: AUC-ROC mean μ and Standard-deviation σ for 32 randomly sampled finger-prints (including randomizing N ) for CIFAR-10. The AUC-ROC across all PWAs varies little(σ < 1%), with σ highest for CW-L2.
Figure 5: AUC-ROC with varying adversarialdistortion (300 pre-test images). Unlike robustprediction(Madry et al., 2017; Kannan et al.,2018), NeuralFP shows no performance declinewith increasing distortion (AUC-ROC >99%).
Figure 7: AUC-ROC for different hyperparameters (left, middle) and ROC curves (right) on CIFAR-10for partial-whitebox attacks. For analysis on MNIST, see Appendix. NeuralFP is robust across attacks& hyperparameters with an AUC-ROC between 95 - 100%. Increasing N improves performance,indicating more fingerprints are harder to fool. Increasing the magnitude ε decreases AUC on CW-L2only, suggesting that as adversarial perturbations become of smaller magnitude, NeuralFP requires ε.
Figure 8: Geometry of fingerprints for SVMs withlinearly separable data. Let d(x) be the distanceof x to the decision boundary (see Thm 1). δ±max(δ±min) denote the maximal (minimal) distances ofthe positive (x+) and negative (x-) examples tothe separating hyperplane hw, xi + b = 0. Thefingerprint ∆x1 with h∆x1, ei = δ-min will havef(x- + ∆x) < 0 and f(x-) < 0 for all x- inthe data distribution (red region). Hence, ∆x1 willflag all x0 in the regions -δ-min < d(x0) < 0 as“fake”, since for those x0 it will always see a changein predicted class. Similarly, ∆x2 with h∆x2 , ei =δ-max always sees a class change for real x-, thusflagging all x0 with d(x0) < -δ-max as “fake”.
Figure 9: AUC-ROC performance for different hyperparameter settings (left, middle) and ROCCurves (right) on MNIST. We see that the performanCe of NeuralFP is robust aCross attaCks andhyperparameters, with the AUC-ROC between 90 - 100% for most settings. The AUC-ROC is lowestversus CW-L2, whiCh is one of the strongest known attaCk.
Figure 10: Histograms depicting the distribution of losses for randomly sampled adversarial examplesand test-data. Randomly sampled adversarial examples are well separated from unseen test examples.
