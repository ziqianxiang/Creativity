Figure 1: A simple schematic of skip connections between two successive layers. The dimensionalityQassmer Kof the network does not change with depth. The variable y(t) represents the values of the residuals atlayer t, and x(t) is the activation of neurons at layer t.
Figure 2: Mean and standard deviation of the residuals for all 10 classes for a 1000 layer deepResNet with shared weights (top), a 15 layer ResNet with variable weights (middle), and mean andstandard deviation of the residuals of the MLP network for inputs corresponding to 2000 samplesfrom all classes. Top: The weights are from a 15-layer deep network. The stable residuals aresparse in activities. Middle: The network with 15 different weight matrices has more residuals attheir maximum values which correspond to many saturated neurons in the final layer. The standarddeviation of the residuals are zero after the 4th layer, showing that the residuals have no transientdynamics in the following layers. Bottom: Residual dynamics in MLP are oscillatory and the standarddeviation between residuals corresponding to a single input class is high.
Figure 3: Residuals (A) and their cumulative (B) for neurons that have the largest contribution in theclassifierâ€™s output for the softmax layer. In all cases, but class 8, the cumulative with the highest finalvalue at layer 15 corresponds to the class that has the highest sensitivity to that neuron.
Figure 4: Average noise to signal ratio (A) and cosine similarity (B) for final hidden layers of ResNet,and MLP network. The horizontal axis is in logarithmic scale, and shows the maximum amplitude ofthe noise signal.
Figure 5: Mean and standard deviation of the residuals, for the ResNet with shared weights (A), andthe MLP network (B), corresponding to the 10 different classes of the MNIST test data. The meanof the residuals are different in few dimensions for each class in the ResNet, and there are morevariabilities in the MLP.
Figure 6: Clustering properties of internal dynamics for the ResNet (with shared weights) and theMLP network considered in the paper. A: distance between residuals corresponding to differentclasses. Residuals that belong to the same input class are closer to each other than those belonging todifferent classes. B: Average cosine similarity between the final hidden layers of the ResNet for 200samples per class. C: Average cosine similarity between the final hidden layers of the MLP networkfor 200 samples per class.
