Figure 2: vRNNA vRNN induces memory by encoding the past information in its hidden state units ht, whichare continuously changing and forced to be used every time; thus, the memory of the vRNNis called state memory or internal memory. Fig.2(b) shows the state transition diagram of avRNN, where s0, s1, ..., s4 represent the state at times t0, t1, ..., t4, respectively. The arrows showthe dependency of the variables. Here, state s1 is decided by only s0, s2 is decided by only s1 and soon. (All the memory visualization figures in this paper ignore the current input.)As the number of hidden units is limited in practice, a compromise between memory depth andmemory resolution always exists in the vRNN De Vries & Principe (1992). For long memory depthsequences, a vRNN requires a very large number of hidden units to achieve acceptable accuracy. Ifthe sequences are composed of symbols or discrete numbers, the limitation of vRNN is even easierto see: the process can be viewed from a Markov transition model perspective. Specifically, thevRNN attempts to learn a first-order Markov transition model (with transition probability 1), wherethe current state is decided by only current input and the state in the previous step. Thus, since thestate space is not very large (the number of states is less than the size of input symbols’ alphabet) forfirst-order Markov sequences, the vRNN always performs well. However, for higher-order Markovsequences or sequences that do not satisfy the Markov property, the vRNN still attempts to build afirst-order Markov state model, which results in a very large state space (several old states must becombined into a new state). The compromise between memory depth and memory resolution (whichare related to the number and temporal resolution of the states) would make vRNN not suitable for
Figure 3: LSTM: The blue belt, M0, rep-resents the external memory. At t1, memoryM00 is generated and stored; attimet9, M00is updated to M01. The black dashed arrowsrepresent the effect of the current state on theexternal memory. The state index is also thetime index.
Figure 4: Neural stack: the network first savesstate M00 in belt M0 and updates it to M01.
Figure 5: Neural RAMi)	all the reading weights except that of the topmost memory slot are set to zero, wtr (i) = 0, if i 6= 0;ii) only the writing weight for the topmost memory slot is learned, and all others are copied from it,wtr(i) = wtr(0),ifi 6= 0;iii)	in the writing process, instead of learning all the contents to be written to the stack, as in Eq.(11),only the content ofM0 is learned as ct(0) = t(wh0 Tc ht-1 + bc) + γmt-1(1), and all other contentsare calculated as ct(i) = mt-1(i - 1) + γmt-1(i + 1), if i 6= 0.;6Under review as a conference paper at ICLR 2019iv)	only the writing and erasing weights for the topmost element are learned, and all others arecopied from the values of the topmost element, wtr (i) = wtr (0), et(i) = et(0).
Figure 6: Learning curves for four synthetic tasksThe learning curves for the four tasks when using different networks are shown in Fig.6. Theperformance is measured in terms of MSE for first two tasks and output entropy for the last twotasks. We use the same number of units in all the architectures to ensure a fair comparison. Theresults indicate that for counting, all four networks can achieve an almost zero error; for countingwith interference, all the networks except vRNN can complete the task; for sequence reversing, neuralstack and neural RAM are suitable networks; and for repeat copying, neural RAM is the only networkthat can solve the problem. We also apply different parameter settings, for instance, varying thenumber of hidden units from 5 to 1000. The performances are the same as shown in Fig.6, except adifferent nonzero error value is observed when the network is not capable of accomplishing the task.
Figure 7: Task1: vRNN: Internal memory contentFigure 8: Task1: LSTM: External memory contentFigure 9: Task1: Neural stack: stack contentD.2 counting with interferenceThe experiment settings are same as "counting" task. Fig.11 to Fig.13 shows the memory usageof LSTM, neural stack and neural RAM. Fig.11 shows that when a is received, the third elementof the memory content would increase by around 0.2. Fig.12 and Fig.13 also show the similarincremental patterns of neural stack and neural RAM. An notable difference between Fig.11-Fig.12and Fig.8-Fig.9 is the usage of the memory. When dealing with counting task, the output gates arealways 1, however, when dealing with counting with interference task, the output gates are 0 wheninputting b and c, this helps to cut off the interference from the memory. Similarly, the read vector16Under review as a conference paper at ICLR 2019Figure 10: Task1: Neural RAM: Memory bank content and corresponding read and write operationFigure 11: Task2: LSTM: External memory contentInput abaab c ebb aacc at=1 2	3	4	5	6	7	8	9 10 11 12 13 14Figure 12: Task2: Neural stack: stack content17Under review as a conference paper at ICLR 2019
Figure 8: Task1: LSTM: External memory contentFigure 9: Task1: Neural stack: stack contentD.2 counting with interferenceThe experiment settings are same as "counting" task. Fig.11 to Fig.13 shows the memory usageof LSTM, neural stack and neural RAM. Fig.11 shows that when a is received, the third elementof the memory content would increase by around 0.2. Fig.12 and Fig.13 also show the similarincremental patterns of neural stack and neural RAM. An notable difference between Fig.11-Fig.12and Fig.8-Fig.9 is the usage of the memory. When dealing with counting task, the output gates arealways 1, however, when dealing with counting with interference task, the output gates are 0 wheninputting b and c, this helps to cut off the interference from the memory. Similarly, the read vector16Under review as a conference paper at ICLR 2019Figure 10: Task1: Neural RAM: Memory bank content and corresponding read and write operationFigure 11: Task2: LSTM: External memory contentInput abaab c ebb aacc at=1 2	3	4	5	6	7	8	9 10 11 12 13 14Figure 12: Task2: Neural stack: stack content17Under review as a conference paper at ICLR 2019Figure 13: Task2: Neural RAM: Memory bank content and corresponding read and write operation
Figure 9: Task1: Neural stack: stack contentD.2 counting with interferenceThe experiment settings are same as "counting" task. Fig.11 to Fig.13 shows the memory usageof LSTM, neural stack and neural RAM. Fig.11 shows that when a is received, the third elementof the memory content would increase by around 0.2. Fig.12 and Fig.13 also show the similarincremental patterns of neural stack and neural RAM. An notable difference between Fig.11-Fig.12and Fig.8-Fig.9 is the usage of the memory. When dealing with counting task, the output gates arealways 1, however, when dealing with counting with interference task, the output gates are 0 wheninputting b and c, this helps to cut off the interference from the memory. Similarly, the read vector16Under review as a conference paper at ICLR 2019Figure 10: Task1: Neural RAM: Memory bank content and corresponding read and write operationFigure 11: Task2: LSTM: External memory contentInput abaab c ebb aacc at=1 2	3	4	5	6	7	8	9 10 11 12 13 14Figure 12: Task2: Neural stack: stack content17Under review as a conference paper at ICLR 2019Figure 13: Task2: Neural RAM: Memory bank content and corresponding read and write operationare always around [0.3, 0.3, 0.3] in Fig.10, however, in Fig.13, the read vector’s elements are almost
Figure 10: Task1: Neural RAM: Memory bank content and corresponding read and write operationFigure 11: Task2: LSTM: External memory contentInput abaab c ebb aacc at=1 2	3	4	5	6	7	8	9 10 11 12 13 14Figure 12: Task2: Neural stack: stack content17Under review as a conference paper at ICLR 2019Figure 13: Task2: Neural RAM: Memory bank content and corresponding read and write operationare always around [0.3, 0.3, 0.3] in Fig.10, however, in Fig.13, the read vector’s elements are almostzeros when encountering b and c. The read vector here works as the output gate in LSTM and neuralstack. It also shows why neural RAM does not need an output gate.
Figure 11: Task2: LSTM: External memory contentInput abaab c ebb aacc at=1 2	3	4	5	6	7	8	9 10 11 12 13 14Figure 12: Task2: Neural stack: stack content17Under review as a conference paper at ICLR 2019Figure 13: Task2: Neural RAM: Memory bank content and corresponding read and write operationare always around [0.3, 0.3, 0.3] in Fig.10, however, in Fig.13, the read vector’s elements are almostzeros when encountering b and c. The read vector here works as the output gate in LSTM and neuralstack. It also shows why neural RAM does not need an output gate.
Figure 12: Task2: Neural stack: stack content17Under review as a conference paper at ICLR 2019Figure 13: Task2: Neural RAM: Memory bank content and corresponding read and write operationare always around [0.3, 0.3, 0.3] in Fig.10, however, in Fig.13, the read vector’s elements are almostzeros when encountering b and c. The read vector here works as the output gate in LSTM and neuralstack. It also shows why neural RAM does not need an output gate.
Figure 13: Task2: Neural RAM: Memory bank content and corresponding read and write operationare always around [0.3, 0.3, 0.3] in Fig.10, however, in Fig.13, the read vector’s elements are almostzeros when encountering b and c. The read vector here works as the output gate in LSTM and neuralstack. It also shows why neural RAM does not need an output gate.
Figure 14: Task3: Neural stack: stack contentFig.14 sho	ws how neural stack utilizes its stack memory to solve this problem. Since each memorybank’s word size is 16, here we only use colors instead of the specific numbers to show the values ofcontents in memory. Different from the first two tasks, the function of the stack is finally exploited.
Figure 15:	Task3: Neural RAM: Memory bank content and corresponding read and write operationto make space for new stuffs. Another feature of the memory bank for neural RAM is the memorybanks are not used in order such as M0, M1, M2...In this example, the memory banks are used inthe order M0, M2, M7, M13. But as long as the network knows the writing order, the task can beaccomplished. Fig15(b)(c) shows the reading and writing weights, we can see that the second half ofthe reading weights is the mirror of the first half of the sequence of the writing weights, which meansthe network learns to reverse.
Figure 16: Task4: Neural RAM: Memory bank content and corresponding read and write operationnetwork whether to continue repeating or not. We can see from Fig.16(b), at time t=22, after readingfrom M2 and M5, the network stops reading from M4 to M9 and turns to M0.
