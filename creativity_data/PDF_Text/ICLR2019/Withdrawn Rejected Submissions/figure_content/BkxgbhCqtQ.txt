Figure 1: DLVMs havelayers of stochastic la-tent variables.
Figure 2: A dis-tribution qφ is op-timized to approxi-mate pθ*.
Figure 3: SQUAD quantizes thedomain of z to model a flexible andtractable variational distribution.
Figure 4: The left diagram visualizes the computational graph of SQUAD at training time, providinga detailed view on how an individual latent variable is sampled. The right diagram visualizes howthe proposed matrix-factorization variant improves the parameter efficiency of the model.
Figure 5: In the IB bound, the marginal p(z) is approximated with a fixed distribution r(z). Usingour proposed SQUAD distribution we can impose a variety of interesting forms for r(z) via thespacing v and weighting r(zk = vc) of the quantization bins. For the values v, we compare linearlyspaced bins (left) versus bins with equal probability mass under a normal distribution (right). Fur-thermore, we explore the effect of allowing the bin values to be optimized with SGD on a per-neuronor per-layer basis, to allow the model to optimize the quantization scheme with the highest fidelity.
Figure 6: Risk/coverage curve (with log-axes for discernibility) of 2-layer models on notMNIST.
Figure 7: By analyzing the emerging predicted distributions of individual neurons in a convergedSQUAD model, we find that the flexible variational distribution is used to its full advantage. Figure(A) visualizes a subset of interesting stereotypical distributions we hope to find in the model. Figure(B) summarizes distributions predicted by the model similar to stereotypes, discovered by looking atpredicted distributions with low KL. Figure (C) shows how often distributions similar to stereotypesarise, as measured by the KL distance (lower KL is closer to stereotypes).
Figure 8: By using a 1-dimensional matrix factorization for a SQUAD-factorized distribution, wecan visualize the type of (stochastic) activation functions learned by the method. After training themodel as usual on fashion-MNIST, we take a random neuron from the first layer. We visualize howthe predicted distribution of the output changes as a function of the 1-dimensional input. The lefty-axis indicates the prob(ability per value as shown using the green line. The right y-axis indicatesthe value and in blue the most likely value is shown, and the gray dots represent samples from theneuron. The red line depicts the expected output of the neuron. The shape of the expected output isakin to a peaky sigmoid activation, and similar shapes are found in the other neurons of the networkas well. This provides food for thought on the design of activation functions for conventional neuralnetworks.
Figure 9: This figure serves to provide intuition on how a variety of distributions come about in ourmodel. We show the set of weights used to predict the probability for the C bins of a randomlyselected latent variable zl=1,k=12 from the first layer in a converged 2-layer SQUAD model (re-shaped to a 28x28 squares for comparison with the data). We then present 5 data-points for whichthe neuron predicts a stereotypical distribution, as visualized in the last bar-plot.
Figure 10: This figure visualizes the pairwise relationship between hyper-parameters of SQUADand the effect on coverage. The top-60 configurations are highlighted. Green values are good, redvalues are bad. We have filtered on the optimal settings for bin values and prior to reduce clutter.
