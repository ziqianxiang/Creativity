Figure 1: Investigations on the robustness of an A2C agent with respect to discounting, rewardscaling and action repetitions. We report the average reward per environment step, after 5000 steps oftraining, for each of 20 distinct seeds. Each parameter study compares different fixed configurationsof a specific hyper-parameter to the corresponding adaptive solution. In all cases the performanceof the adaptive solutions is competitive with that of the best tuned solutionexpensive, and combinatorially so when there are many different hyper-parameters because oftenthe hyper-parameters interact in subtle ways. None of these dimensions is binary, and differentalgorithms may satisfy each of them to a different degree, but keeping them in mind can be helpfulto drive research towards more general RL solutions. We first discuss the first three in isolation, inthe context of simple toy environments, to increase the understanding about how adaptive solutionscompare to the corresponding heuristics they are intended to replace. We then use the 57 Atarigames in the Arcade Learning Environment (Bellemare et al., 2013) to evaluate the performanceof the different methods at scale. Finally, we investigate how well the methods generalize to newdomains, using 28 continuous control tasks in the DeepMind Control Suite (Tassa et al., 2018).
Figure 2: Comparison of inductive biases to RL solutions. All curves show mean episode return asa function of the number of environment steps. Each plot compares the same fully general agentto 2 alternative. a) tuned action repeats, and no action repeats. b) tuned discount factor, and nodiscounting. c) reward clipping, and learning from raw rewards with no rescaling of the updates. d)learning from the raw observation stream of Atari, and the standard preprocessing.
Figure 3: In a separate experiment on the 28 tasks in the DeepMind Control Suite, we comparedthe general solution agent with an A2C agent using all the domain heuristics previously discussed.
Figure 4: Performance on all Atari games of the fully adaptive agent (in red), the agent with fixedaction repeats (in green), and the agent acting at the fastest timescale (in blue).
Figure 5: For each Atari game, the associated plot shows the time horizon T = (1 - γ)-1 for thediscount factor γ that was adapted across the 200 million training frames.
Figure 6:	For each Atari game, the associated plot shows the time horizon T = (1 - λ)-1 for theeligibility trace λ that was adapted across the 200 million training frames.
Figure 7:	Performance on all control suite tasks for the fully adaptive agent (in red), the agent trainedwith the Atari heuristics (in green), and an A3C agent (black horizontal line) as tuned specificallyfor the Control Suite by Tassa et al. (2018).
