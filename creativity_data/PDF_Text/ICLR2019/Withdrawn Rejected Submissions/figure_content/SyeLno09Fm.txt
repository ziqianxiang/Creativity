Figure 1: A diagram of our meta-inverse RL approach. Our approach attemptsto remedy over-fitting in few-shot IRL by learning a “prior” that constraintsthe set of possible reward functions to lie within a few steps of gradient de-scent. Standard IRL attempts to recover the reward function directly from theavailable demonstrations. The shortcoming of this approach is that there islittle reason to expect generalization as it is analogous to training a densitymodel with only a few examples.
Figure 2: Our approach can be un-derstood as approximately learninga distribution over the demonstra-tions τ, in the factor graph p(τ) =ZZ QT=I Φr(Φτ, St, at)Φdyn(st+1, St, at)(above) where we learn a prior over φT,which during meta-test is used for MAPinference over new expert demonstrations.
Figure 3: An example task: When learning atask, the agent has access to the image (left)and demonstrations (red arrows). To evaluate theagent’s learning (right), the agent is tested for itsability to recover the reward for the task when theobjects have been rearranged. The reward struc-ture we wish to capture can be illustrated by con-sidering the initial state in blue. An policy actingoptimally under a correctly inferred reward shouldinterpret the other objects as obstacles, and prefera path on the dirt in between them.
Figure 4: Meta-test performance with varyingnumbers of demonstrations (lower is better):held-out tasks test performance (top), and testperformance on held-out tasks with novel sprites(bottom). All methods are capable are overfit-ting to the training environment (See AppendixA). However, in both test settings, MandRILachieves comparable performance to the train-ing environment, while the other methods over-fit until they receive at least 10 demonstrations.
Figure 5: Meta-test performance comparison with varying number of demonstrations using pre-trained weightsfine-tuned over either a single task or all tasks. We find that pre-training on the full set of tasks hurts adaptation,while pre-training on a single task does not improve performance (comparable to random initialization). Ourapproach ManDRIL outperforms all these methods, which shows that explicitly optimizing for initial weightsfor fine-tuning robustly improves performance. Shaded regions show 95% confidence intervals.
Figure 6: Meta-test “training” performance with varying numbers of demonstrations (lower is better). Thisis the performance on the environment for which demonstrations are provided for adaptation. As the numberof demonstrations increase, all methods are able to perform well in terms of training performance as they cansimply overfit to the training environment without acquiring the right visual cues that allow them to generalize.
