Figure 1: The Recurrent Kalman Network. An encoder network extracts latent features wt fromthe current observation ot . Additionally, it emits an estimate of the uncertainty in the features viathe variance σtobs. The transition model At is used to predict the current latent prior zt- , Σt-using the last posterior zt+-1, Σt+-1 and subsequently update the prior using the latent observation(wt, σtobs). As we use a factorized representation of Σt, the Kalman update simplifies to scalaroperations. The current latent state zt consists of the observable units pt as well as the correspondingmemory units mt . Finally, a decoder produces either st+ , σt+ , a low dimensional observation andan element-wise uncertainty estimate, or ot+, a noise free image.
Figure 2: Results of the multiple pendulum experiments. To evaluate the quality of our uncertainty(j)	(j),+prediction We compute the normalized error S σ-S,+for each entry j of S for all time steps in alltest sequences. This normalized error should follow a Gaussian distribution with unit variance if theprediction is correct. We compare the resulting error histograms With the a unit variance Gaussian.
Figure 3: Predicted sine value of the tree links With 2 times standard deviation (green). Ground truthdisplayed in blue. The crosses visualize the current visibility of the link With yelloW correspondingto fully visible and red to fully occluded. If there is no observation for a considerable time thepredictions become less precise due to transition noise, hoWever the variance also increases.
Figure 4: Each of (a) and (b) shows from left to right: true images, input to the models, imputationresults for RKF, imputation results for KVAE(Fraccaro et al., 2017).
Figure 5: Example images for the multiple pendulum experiments. Left: Noise free image. Middle:sequence of images showing how the noise affects different pendulums differently. Right: Imagewithout useful information.
