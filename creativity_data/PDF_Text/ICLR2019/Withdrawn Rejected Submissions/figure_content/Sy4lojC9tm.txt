Figure 1: Dataset Distillation: we distill the knowledge of tens of thousands of images into a few synthetictraining images called distilled images. (a): On MNIST, 10 distilled images can train a standard LENET with aparticular fixed initialization to 94% test accuracy (compared to 99% when fully trained). On CIFAR10, 100distilled images can train a deep network with fixed initialization to 54% test accuracy (compared to 80% whenfully trained). (b): Using pre-trained networks for SVHN, we can distill the domain difference between twoSVHN and MNIST into 100 distilled images. These images can be used to quickly fine-tune networks trainedfor SVHN to achieve high accuracy on MNIST. (c): Training for a malicious objective, our formulation can beused to create adversarial attack images. If well-optimized networks retrained with these images for one singlegradient step, they will catastrophically misclassify a particular targeted class.
Figure 2: Distilled images trained for fixed initialization. MNIST distilled images use 1 GD step and 3 epochs(10 images in total). CIFAR10 distilled images use 10 GD steps and 3 epochs (100 images in total). ForCIFAR10, only selected steps are shown. At left, we report the corresponding learning rates for all 3 epochs.
Figure 3: Distilled images trained for random initialization with 10 GD steps and 3 epochs. We show imagesfrom selected GD steps and corresponding trained learning rates for all 3 epochs.
Figure 4: Hyperparameter sensitivity studies on random initialization: (a) average test accuracy w.r.t. the numberof gradient descent steps. The number of epochs is fixed to be 2. (b) average test accuracy w.r.t. the number ofepochs. The number of steps is fixed to be 10, with each containing 10 images (one per category).
Figure 5: Comparison between applying the same number of images in one versus multiple GD steps on randominitialization, with the number of epochs fixed to 1. N denotes the total number of images per category. Formultiple steps runs, each of the N steps applies one image per category.
Figure 6: Performance for our method and baselines with random pre-trained initialization and a maliciousobjective. Distilled images are trained for one GD step. For baselines, we use the same numbers of imageswith incorrect labels and also apply one GD step, and report the result that achieves the highest accuracy w.r.t.
