Figure 1: Our common model design: Dur-ing pretraining, we train the shared encoderand the task-specific model for each pretrain-ing task. We then freeze the shared encoderand train the task-specific model anew foreach target evaluation task. Tasks may in-volve more than one sentence.
Figure 2: Top: Pretraining learning curves for GLUE score without ELMo (left) and with ELMo(right). Bottom: Target-task training Learning curves for each GLUE task with three encoders: Therandom encoder without ELMo (left) and with it (center), and Outside MTL without ELMo (right).
