Figure 1: Visual comparison of our result to that obtained by other codecs. Note that even when us-ing more than twice the number of bytes, all other codecs are outperformed by our method visually.
Figure 2: Visual example of images produced by our GC network with C = 4 along with thecorresponding results for BPG, and a baseline model with the same architecture (C = 4) but trainedfor MSE only (MSE bl.), on Cityscapes. The reconstruction of our GC network is sharper andhas more realistic texture than those of BPG and the MSE baseline, even though the latter two havehigher PSNR (indicated in dB for each image) than our GC network. In particular, the MSE baselineproduces blurry reconstructions even though it was trained on the Cityscapes data set, demonstratingthat domain-specific training alone is not enough to obtain sharp reconstructions at low bitrates.
Figure 3: Visual example of images from RAISE1k produced by our GC network with C = 4 alongwith the corresponding results for BPG.
Figure 4: User study results evaluating our GC models on Kodak, RAISEIK (top) and Cityscapes(bottom). For Kodak and RAISE1K, We use GC models trained on Open Images, without anysemantic label maps. For Cityscapes, we used GC (D+), using semantic label maps only for D andonly during training. The standard error is computed over per-user mean preference percentages.
Figure 5: Sampling codes W uniformly (left), and generating them with a WGAN-GP (right).
Figure 6: Mean IoU as a function of bpp on theCityscapes validation set for our GC and SC networks,and for the MSE baseline. We show both SC modes: RI(inst.), RB (box). D+ annotates models where instancesemantic label maps are fed to the discriminator (onlyduring training); EDG+ indicates that semantic labelmaps are used both for training and deployment. Thepix2pixHD baseline (Wang et al., 2018) was trainedfrom scratch for 50 epochs, using the same downsam-pled 1024 X 512px training images as for our method.
Figure 7: Synthesizing different classes using our SC network with C = 8. In each image exceptfor no synthesis, we additionally synthesize the classes vegetation, sky, sidewalk, ego vehicle, wall.
Figure 8: Structure of the proposed SC network. E is the encoder for the image X and the semanticlabel map s. q quantizes the latent code W to W. The subsampled heatmap multiplies W (pointwise)for spatial bit allocation. G is the generator/decoder, producing the decompressed image X, and Dis the discriminator used for adversarial training. F extracts features from S .
Figure 9: First 5 images of the Kodak data set, produced by our GC model with C = 4 and BPG.
Figure 10: First 5 images of RAISE1k, produced by our GC model with C = 4 and BPG.
Figure 11: First 5 images of Cityscapes, produced by our GC model with C = 4 and BPG.
Figure 12: Our model loses more texture but has less artifacts on the knob. Overall, it looks compa-rable to the output of Rippel & Bourdev (2017), using significantly fewer bits.
Figure 13:	Notice that compared to Rippel & Bourdev (2017), our model produces smoother linesat the jaw and a smoother hat, but proides a worse reconstruction of the eye.
Figure 14:	Notice that our model produces much better sky and grass textures than Rippel & Bourdev(2017), and also preserves the texture of the light tower more faithfully.
Figure 15: Notice that our model yields sharper grass and sky, but a worse reconstruction of the fenceand the lighthouse compared to Minnen et al. (2018). Compared to BPG, Minnen et al. producesblurrier grass, sky and lighthouse but BPG suffers from ringing artifacts on the roof of the secondbuilding and the top of the lighthouse.
Figure 16: Our model produces an overall sharper face compared to Minnen et al. (2018), but thetexture on the cloth deviates more from the original. Compared to BPG, Minnen et al. has a lessblurry face and fewer artifacts on the cheek.
Figure 17: Here we obtain a significantly worse reconstruction than Minnen et al. (2018) and BPG,but use only a fraction of the bits. Between BPG and Minnen et al., it is hard to see any differences.
Figure 18: Here we obtain a significantly worse reconstruction compared to Minnen et al. (2018) andBPG, but use only a fraction of the bits. Compared to BPG, Minnen et al.has a smoother backgroundbut less texture on the birds.
Figure 19: We uniformly sample codes from the (discrete) latent space w^ of our generative com-pression models (GC with C = 4) trained on Cityscapes and Open Images. The Cityscapes modeloutputs domain specific patches (street signs, buildings, trees, road), whereas the Open Images sam-ples are more colorful and consist of more generic visual patches.
Figure 20: We train the same architecture with C = 4 for MSE and for generative compressionon Cityscapes. When uniformly sampling the (discrete) latent space W of the models, We see starkdifferences between the decoded images G(W). The GC model produces patches that resembleparts of Cityscapes images (street signs, buildings, etc.), Whereas the MSE model outputs looks likelow-frequency noise.
Figure 21: We experiment with learning the distribution of w^ = E(x) by training an improvedWasserstein GAN (Gulrajani et al., 2017). When sampling form the decoder/generator G of ourmodel by feeding it with samples from the improved WGAN generator, we obtain much sharperimages than when we do the same with an MSE model.
Figure 22: Synthesizing different classes for two different images from Cityscapes, using our SCnetwork with C = 4. In each image except for no synthesis, we additionally synthesize the classesvegetation, sky, sidewalk, ego vehicle, wall.
Figure 23: Example images obtained by our SC network (C = 8) preserving a box and synthesizingthe rest of the image, on Cityscapes. The SC network seamlessly merges preserved and generatedimage content even in places where the box crosses object boundaries.
Figure 24: Reconstructions obtained by our SC network using semantic label maps estimated fromthe input image via PSPNet (Zhao et al., 2017).
Figure 25: When disabling the distortion losses (i.e. λ = 0), such that only LGAN remains, weobserve that the training ”collapses” and produces repetitive textures, both for OpenImages andCitycapes.
Figure 26: We show convergence plots for the generator and discriminator losses from training ourGC C = 8) channel model on OPenImages.
Figure 27: We show convergence plots for the distortion losses from training our GC C = 8) channelmodel on OpenImages.
