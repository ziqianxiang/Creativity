Figure 1: (a) The additive white Gaussian noise (AWGN) channel (Cover & Thomas, 1991). Theadditive noise Z, originating from various sources in the environment, is assumed to be independentofX. (b) Structural causal model of a pattern recognition task, with the intended message as a latentrandom variable Φ, causal mechanisms f, g, and noise variables ZX, ZY ; adapted from Peters et al.
Figure 2: The SNR for the stochastic gradient of the cross-entropy loss L w.r.t. the weights ver-SUs epochs, with ∣∣ mean(VwL)∣∣2∕∣∣w∣∣2 as signal and ∣∣ std(VwL)k2 as noise. We use constantlearning rate (1e-3) SGD with a mini-batch size of 100, relu and tanh activation functions, with andwithout batch normalization layers on all but the last layer. Best viewed in colour.
Figure 3: Information plane visualization for the model from Section 4.1 with the relu activationfunction trained on the original labels (a), and random labels (b) for 104 epochs. Intuitively, we onlysee compression for the original labels, whereas the model memorizes in the random label case. Thedegree of memorization is roughly the difference in I (X; T ) values between the two cases.
Figure 4: Demonstration of regularized generative modeling via an adversarial attack. (a) Samplesfrom our preprocessed version of the SVHN dataset for each class Y ∈ {0, . . . , 9} arranged fromleft to right, with the predicted class (argmax softmax probability) and confidence. (b) Transmittinga message across the channel in the reverse direction (i.e., from Y to X) by minimizing the loss foreach y ∈ Y w.r.t. X 〜N(μ, σ2) with step size 1e-2. We set μ equal to the population mean, andσ2 as two orders of magnitude less than the population standard deviation. We show the pattern Xobtained after iterating until full confidence to within 3 decimal places, which took ≈ 100 steps.
Figure 5: Characterizing fault tolerance in terms of information transmitted, I(T; Y ) for 10K sam-ples from the SVHN test set, as a function of the SNR for two kinds of adversarial noise (“L2” and“L∞"), and white Gaussian noise ("Gauss"). Model (a) is regularized with weight decay, while (b)uses BN and is less tolerant to all sources of noise. Accuracy is eventually reduced to 0% for theunbounded attacks and ≈ 10% for Gaussian noise, however the images with model dependent ad-versarial “noise” contain more information (as expected) as the SNR → 0.
Figure 6: Sphere packing in a two-neuron (n = 2) layer over 500 epochs. Architecture is a fully-connected MLP [(a) (784-392-196-2-49-10) and (b) (784-392-BN-196-BN-2-49-10)] with reluunits, batch size 128, constant learning rate 1e-2, weight decay λ=1e-3, and β=5e-2. All othersettings left as default from Kolchinsky et al. (2017). The plots have a fixed x- and y-axis range of±10. All samples from the MNIST training set are plotted and colour coded by label.
Figure 7: The model with prolonged weight decay training on SVHN (Model B+) has well calibratedpredictions in a transfer learning setup to MNIST. (Top) Histogram of prediction margins for zero-shot (left), and one-shot (right) transfer. (Bottom) Test accuracy versus binned prediction margins.
