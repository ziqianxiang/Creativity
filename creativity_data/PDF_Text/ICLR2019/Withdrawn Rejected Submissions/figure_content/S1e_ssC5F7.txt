Figure 1: Convergence performances of RKL1 , RKL2, and RKL3 on the database of MNIST inonline learning setting, up: performances at different initial learning rates, and down: the wholetraining process with the given initial learning rate in the bracket for each algorithm.
Figure 2: Convergence performances of algorithms on MNIST in the full batch setting. left: thetrain loss of the last training epoch at different initial learning rates, right: the whole training processwith the given initial learning rate in the bracket for each algorithm.
Figure 3: Convergence performances of algorithms on CIFAR-10 in the online learning setting, left:the train loss of the last training epoch at different initial learning rates, right: the whole trainingprocess with the given initial learning rate in the bracket for each algorithm.
Figure 4: Convergence performances of KL2 and KL3 on the database of MNIST in online learningsetting, up:at different initial learning rates, and down:the whole training process with the giveninitial learning rate in the bracket.
Figure 5: Convergence performances of H2 and H3 on the database of MNIST in online learningsetting, up:at different initial learning rates, and down:the whole training process with the giveninitial learning rate in the bracket.
Figure 6: Convergence performances of χ22 and χ23 on the database of MNIST in online learningsetting, up:at different initial learning rates, and down:the whole training process with the giveninitial learning rate in the bracket.
Figure 7: The values of the maximum βt at each training step for various algorithms.
