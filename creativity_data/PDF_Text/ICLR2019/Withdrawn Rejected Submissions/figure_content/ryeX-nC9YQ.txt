Figure 1: (a) Quantization error bounds; (b) Convergence of full-precision (fp) SGD and LP-SGD.
Figure 2: Plots of the asymptotic loss gap from Figure 1(b) as a function of model size d and Ïƒ1.
Figure 3:	A figure showing the actual quantization variance E kQ(w) - wk22 and the tight upperbound that we introduced in one dimension. Similarly to 1(a) we plot this bound when taking theminimum over all possible z.
Figure 4:	Plots of noise ball size vs. be when running SGD with 16 bits FPQ on synthetic data setand MNIST. Note the use of two y-axes in Figure 4(b) to make the series fit in one figure.
