Figure 1: Learning curves from five RL algorithms on CartPole game with true rewards (r) ■, noisyrewards (r) ■ and estimated surrogate rewards (r) (η = 1) ■. Note that reward confusion matricesC are unknown to the agents here. Full results are in Appendix D.2 (Figure 6).
Figure 2: Learning curves from DDPG and NAF on Pendulum game With true rewards (r) ■, noisyrewards (r) and surrogate rewards (r) (η = 1) . Both symmetric and asymmetric noise areconduced in the experiments. Full results are in Appendix D.2 (Figure 8).
Figure 3: Learning curves from PPO on Pong-v4 game with true rewards (r) ■, noisy rewards (F) Iand surrogate rewards (η = 1) (r)	. The noise rates increase from 0.1 to 0.9, with a step of 0.1.
Figure 4: Perturbed-Reward MDP ExampleThere are two experiments conducted in this setting: 1) performance of Q-Learning under differentnoise rates (Table 4); 2) robustness of estimation module in time-variant noise (Figure 4b). As shownin Table 4, Q-Learning achieved better results consistently with the guidance of surrogate rewardsand the confusion matrix estimation algorithm. For time-variant noise, we generated varying amountof noise at different training stages: 1) e- = 0.1, e+ = 0.3 (0 to 1e4 steps); 2) e- = 0.2, e+ = 0.1(1e4 to 3e4 steps); 3) e- = 0.3, e+ = 0.2 (3e4 to 5e4 steps); 4) e- = 0.1, e+ = 0.2 (5e4 to 7e4steps). In Figure 4b, we show that Algorithm 1 is robust against time-variant noise, which dynam-ically adjusts the estimated C after the noise distribution changes. Note that we set a maximummemory size for collected noisy rewards to let the agents only learn with recent observations.
Figure 5: Learning curves from five reward robust RL algorithms (See Algorithm 3) on CartPolegame with true rewards (r) ■, noisy rewards ⑺(η = 1) ■, sample-mean noisy rewards (η = 1) ■,estimated surrogate rewards (T) ■ and sample-mean estimated surrogate rewards ■. Note thatconfusion matrices C are unknown to the agents here. From top to the bottom, the noise rates are0.1, 0.3, 0.7 and 0.9. Here we repeated each experiment 10 times with different random seeds andplotted 10% to 90% percentile area with its mean highlighted.
Figure 6:	Complete learning curves from five reward robust RL algorithms (see Algorithm 3) onCartPole game with true rewards (r) ■, noisy rewards ⑺(η = 1) ■ and estimated surrogaterewards (r) ■. Note that confusion matrices C are unknown to the agents here. From top to thebottom, the noise rates are 0.1, 0.3, 0.7 and 0.9. Here we repeated each experiment 10 times withdifferent random seeds and plotted 10% to 90% percentile area with its mean highlighted.
Figure 7:	Estimation analysis from five reward robust RL algorithms (see Algorithm 3) on CartPolegame. The upper figures are the convergence curves of estimated error rates (from 0.1 to 0.9), wherethe solid and dashed lines are ground truth and estimation, respectively; The lower figures are theabsolute difference between the estimation and ground truth of confusion matrix C (normalizedmatrix norm).
Figure 8: Complete learning curves from DDPG and NAF on Pendulum game with true rewards(r) B, noisy rewards (r) ■ and surrogate rewards (^) (η = 1) ■. Both symmetric and asymmetricnoise are conduced in the experiments. From top to the bottom, the noise rates are 0.1, 0.3, 0.7 and0.9, respectively. Here we repeated each experiment 6 times with different random seeds and plotted10% to 90% percentile area with its mean highlighted.
