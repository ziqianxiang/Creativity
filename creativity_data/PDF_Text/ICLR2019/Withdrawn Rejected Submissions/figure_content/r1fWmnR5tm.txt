Figure 1: An illustration of layer-wise pruning method based on vanilla DenseNet. For one layer, not all connections arerequired and each layer has its unique connections. After being pruned some dense connections, the network can stillpredict correctly.
Figure 2: Illustration of our proposed framework. In each iteration, the output of the i-th time step makes keeping ordropping decisions for the i-th layer. All outputs of the LSTM controller generate a child network by sampling fromK × K -dimensional Bernoulli distribution. Then, the child network forwards propagation with mini-batch samples andthe reward function can be evaluated with the predictions and FLOPs. The controller is optimized with policy gradient.
Figure 3: Quantitative results on DenseNet-40-12 wth LWP. Lef t: the number of input channel in vanilla DenseNet-40-12 and the learned child network. Right: the connection dependency between any two layers is represented as theaverage absolute wights of convolution layer.
Figure 4: Hyperparameters search on CIFAR-10 to explore the trade off between FLOPs and accuracy by changing theexponential power of FLOPs ratio and penalty -γ .
