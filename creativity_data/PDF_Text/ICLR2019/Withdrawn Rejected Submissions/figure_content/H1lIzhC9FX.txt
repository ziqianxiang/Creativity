Figure 1: Dynamic generative memory: The AC-GAN Odena et al. (2016) architecture allows simul-taneous adversarial and auxiliary training, in which auxiliary output is trained on the real samplesof the current task and synthesized sample of previously seen tasks. During the adversarial train-ing with real and fake samples of current class a connection plasticity in the generator is learnedsimulataniously with the weights of the base network. Connections plasticity is represented by taskspecific, trainable binary masks for base netwrokâ€™s activations.
Figure 4: Evolution of Frechet Inception Dis-tance (FID), the lower the better. FID decreasesduring the absence for forgetting is evidenced bythe fact that FID does not increase during thelearning subsequent task.
Figure 3: Evolution of the classification accuracyover the 10 tasks of the CIFAR10 dataset. Per-formance of the currently learned task is alwayshigh (real samples are available), it drops duringthe subsequent tasks due to increasing problemcomplexity.
Figure 5: Samples generated for CIFAR10 after 5th taskFinally, on the CIFAR-10 dataset we provide a Frechet Inception Distance (Heusel et al., 2017)metric of the generated images over the first 5 classes in order to asses the perceptual quality of thegeneration. Figure 4 provides the dynamics of this metric while training across the different tasks.
Figure 6: Neural capacity analysis8Under review as a conference paper at ICLR 2019Figure 7: Neuron plasticity during the training of a task t. The standard deviation of the mask valuesis shown as a blue shadow.
Figure 7: Neuron plasticity during the training of a task t. The standard deviation of the mask valuesis shown as a blue shadow.
