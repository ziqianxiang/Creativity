Figure 1: Two draws of outputs for ReLU and Tanh networks with (σb, σw) = (1, 1) ∈ Dφ,var ∩Dφ,corr. The output functions are almost constant.
Figure 2: A draw from the outputfunction of a ReLu network with20 layers, 100 neurons per layer,(σb2,σw2 ) = (0, 2) (edge of chaos)This class of activation functions has the interesting property of preserving the variance across layerswhen the network is initialized on the EOC. However, we show in Proposition 3 below that, even inthe EOC regime, the correlations converge to 1 but at a slower rate. We only present the result forReLU but the generalization to the whole class is straightforward.
Figure 3: Impact of the initialization on the EOC for a ReLU networkrecommendation of He et al. (2015) whose objective was to make the variance constant as the inputpropagates but did not analyze the propagation of the correlations. Klambauer et al. (2017) alsoperformed a similar analysis by using the "Scaled Exponential Linear Unit" activation (SELU) thatmakes it possible to center the mean and normalize the variance of the post-activation φ(y). Thepropagation of the correlation was not discussed therein either. In the next result, we present thecorrelation function corresponding to ReLU networks. This was first obtained in Cho & Saul (2009).
Figure 4:	Correlation function and a draw of the output for a Swish networkLemma 2. Under the conditions of Proposition 4, the only change being limσb→0 q > 0, the resultof Proposition 4 holds if only if the activation function is linear.
Figure 5:	Impact of the initialization on the edge of chaos for Swish networkReLU versus Tanh We proved in Section 3.2 that the Tanh activation guarantees better informationpropagation through the network when initialized on the EOC. However, Tanh suffers>us⊃ou< UO-4->ep=e>(a) width = 10, depth = 5Tanh with (σ≡j 而)=(1.722,0.04){ErOC)ReLU with (σ≡, σQ = (2,0)(EoC)Figure 6: Comparaison of ReLu and Tanh learning curves for different widths and depths(b) width = 80, depth = 407Under review as a conference paper at ICLR 2019from the vanishing gradient problem. Consequently, we expect Tanh to perform better than ReLU forshallow networks as opposed to deep networks, where the problem of the vanishing gradient is notencountered. Numerical results confirm this fact. Figure 6 shows curves of validation accuracy withconfidence interval 90% (30 simulations). For depth 5, the learning algorithm converges faster forTanh compared to ReLu. However, for deeper networks (L ≥ 40), Tanh is stuck at a very low testaccuracy, this is due to the fact that a lot of parameters remain essentially unchanged because thegradient is very small.
Figure 6: Comparaison of ReLu and Tanh learning curves for different widths and depths(b) width = 80, depth = 407Under review as a conference paper at ICLR 2019from the vanishing gradient problem. Consequently, we expect Tanh to perform better than ReLU forshallow networks as opposed to deep networks, where the problem of the vanishing gradient is notencountered. Numerical results confirm this fact. Figure 6 shows curves of validation accuracy withconfidence interval 90% (30 simulations). For depth 5, the learning algorithm converges faster forTanh compared to ReLu. However, for deeper networks (L ≥ 40), Tanh is stuck at a very low testaccuracy, this is due to the fact that a lot of parameters remain essentially unchanged because thegradient is very small.
Figure 8: Convergence of the variance for three different inputs with (σb, σw) = (1, 1)Lemma 1. Let (σb, σw) ∈ Dφ,var ∩ Dφ,corr such that q > 0, a, b ∈ Rd and φ an activation functionsuch that supx∈K E[φ(xZ)2] < ∞ for all compact sets K. Define fl by cla+,b1 = fl(cla,b) and f byf(x) = σ2+σw E[φ(√qZ1)φ(√q(XZ1+√1-χ2z2)) .Then limι→∞ supχ∈[0j] ∣fι(x) - f(x)∣ = 0.
Figure 9: Graphs of ReLU and SwishProof. To abbreviate notation, we note φ := φSwish = xex/(1 + ex) and h := ex/(1 + ex) is theSigmoid function. This proof should be seen as a sketch of the ideas and not a rigourous proof.
Figure 10: graphs of E[φ00(U1)φ00(U2(x))] for different values of q (from 0.1 to 10, the darkest linecorresponds to q = 10)We observe that f00 has very small values when q is large, this is a result of the fact that φ00is concentrated around 0.
Figure 11: The correlation function on the edge of order-to-chaos for a Tanh network with smallvalues of σbWe deal with the first part fi(x) = σWE[1-α<Zι<α X 1-a<xZ1+√1-χ2z2]，we have that ：f1 (x) = σwE[1-α<Z1 <α ×2=w— E[1-a<Zι <α2π√1-X2(ZI - √T-X2Z2)δ-α=xzι+√1-χ2z2]1	Z1 + xα	(xZ1 + α)2√Γ-^2 T-m exp(-项ET)]2σW—x2α-αz1 + xα1 — x2z12 + 2xαz1 + α2exp(—-2(1—χ2))dz1
Figure 12: Experimental results for ELU activationFigure 12a shows the EOC curve (condition (ii) is satisfied). Figure 12b shows that is non-decreasingand Figure 12d illustrates the fact that limσb→0 q = 0. Finally, Figure 12c shows that function f isconvex. Although the figures of F and f are shown just for one value of (σb, σw), the results are truefor any value of (σb, σw) on the EOC.
