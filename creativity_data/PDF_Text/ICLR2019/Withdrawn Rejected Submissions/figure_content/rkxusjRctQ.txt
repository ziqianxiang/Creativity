Figure 1: The Minecraft random walk dataset for localization in 3D scenes. We generate randomtrajectories in the Minecraft environment, and collect images along the trajectory labelled by camerapose coordinates (x,y,z yaw and pitch). Bottom: Images from random scenes. Top: The localizationproblem setting - for a new trajectory in a new scene, given a set of images and corresponding cameraposes (the ‘context’), predict the camera pose of an additional observed image (the ‘target’, in green).
Figure 2: Localization as probabilistic inference (a). The observed images X depend on the environ-ment E and the camera pose P. To predict Pr(P |X), in the generative approach, we use a model ofPr(X|P) (green) and apply Bayes’ rule. In the discriminative approach we directly train a modelof Pr(P |X) (red). In both cases the environment is implicitly modeled given a context of (image,camera pose) pairs C = {xi , pi}. In the GQN model (b), the context is processed by a representationnetwork and the resulting scene representation is fed to a recurrent generation network with L layers,predicting an image given a camera pose.
Figure 3: (a) Instead of conditioning on a parametric representation of the scene, all patches fromcontext images are stored in a dictionary, and each layer of the generation network can access themthrough an attention mechanism. In each layer, the key is computed as a function of the generationnetwork’s recurrent state, and the result v is fed back to the layer (b).
Figure 4: The loss and predictive MSE for both the generative direction and the discriminativedirection. The attention model results in lower loss and lower MSE in both directions.
Figure 5: Generated samples from the generative model (middle), and the whole output distributionfor the discriminative model (bottom). Both were computed using the attention models, and eachimage and pose map is from a different scene conditioned on 20 context images. The samples capturemuch of the structure of the scenes including the shape of mountains, the location of lakes, thepresence of large trees etc. The distribution of camera pose computed with the reversed GQN showsthe model’s uncertainty, and the distribution’s mode is usually close to the ground truth (green dot).
Figure 6: Attention over the context images in the generative and discriminative directions. Thetotal attention weights are shown as a red overlay, using the same context images for both directions.
Figure 7: Localization with the generative and discriminative models - target image, a sample drawnusing the ground-truth pose, and probability maps for x,y position and yaw angle (bright=high prob.).
Figure 8: The reversed GQN model. A similar architecture to GQN, where the generation networkis ‘reversed’ to form a localization network, which is queried using a target image, and predicts itscamera pose.
Figure 9: Localization with the generative and discriminative models - target image, a sample drawnusing the ground-truth pose, and probability maps for the x,y position and yaw angles. The generativemaps are computed by querying the model with all possible pose coordinates, and the discriminativemaps are simply the model’s output. The poses of the context images are shown in cyan, target imagein green and maximum likelihood estimates in magenta. The generative maps are free from the priorin the training data (that target poses are close to the context poses) giving higher probability tounexplored areas and lower probabilities to areas with context images which are dissimilar to thetarget.
