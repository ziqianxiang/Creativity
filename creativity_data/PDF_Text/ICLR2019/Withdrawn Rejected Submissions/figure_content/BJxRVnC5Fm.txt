Figure 1: Mean Replacement illustrated in three steps. In step (1) the units to be pruned are selected(highlighted in red). In step (2) mean activations are multiplied with outgoing weights. In step (3)the product is added to the bias of corresponding units.
Figure 2: (left) Results from a single experiment, where MEDIUM_CONV (a 5 layer convolutionalnetwork) trained for 35k iterations with batch size 64. We calculate pruning penalties at every 250iteration after pruning 10% of the units at each layer . All the measurements are made on a copiedmodel and the mean values over 8 runs reported with 80% confidence intervals. (right) We plot theaverage loss on validation and test sets using the left axis and accuracies on the right.
Figure 3: scatter plots aggregating all measurements, where the two pruning penalties (with andwithout bias propagation) for the same scoring function are plotted in the opposite axes.(left) Exper-iments with cifar-10 dataset: pruning penalties are calculated every 250 training iterations. (right)Experiments with Imagenet-2012 dataset: pruning penalties are calculated every 10000 trainingiterations.
Figure 4: Pruning penalties after retraining the network with batch size 64 and learning rate 1e-3for N fine tuning steps. Pruning penalties are measured after fine tuning steps and aggregated ina scatter plot. The data used originate from pruning experiments on Cifar-10 made after 25000training steps (second half of the training). (left) N=10. (middle) N=100. (right) N=5004.2	Mean Replacement Reduces the Loss After Pruning And RetrainingOne could argue that training the pruned network could quickly compensate for the damage causedby zeroing the units without bias propagation. In other words, the networks pruned without meanreplacement might end up learning the correct bias quickly through fine tuning, achieving the sameloss as the network pruned with mean replacement after N fine tuning steps. To assess this claim,we repeat our basic experiments but perform a specific number of retraining steps before measuringthe post-pruning loss. In order to eliminate the unstable effects observed during the early stages oftraining, in this experiment we only consider the pruning-and-retraining penalties measured after atleast 25,000 training iterations on the Cifar-10 dataset. Most of the networks we train have near zerolosses by that time (see Figure 2 (right)). Figure 4 shows the scatter plots for 3 different values offine tuning iterations. Although the effect of Mean Replacement diminishes when we increase thenumber of fine tuning steps, we can still see a difference after 500 fine tuning steps, which is almostone full epoch. This observation supports our claim that the immediate improvement on pruningpenalty helps the future optimization.
Figure 5: Performance profiles of scoring functions calculated from all experiments we ran forCifar-10. The y-axis denotes the probability for a particular scoring function to have a pruningpenalty smaller than the threshold t = min(∆Loss)i * T + max(∆Loss)i * (1 一 T) where themin and max are calculated separately among the scoring functions for each time step i. The x-axisdenotes the interpolation constant T that determines the exact threshold ti used for specific pruningmeasurements. Bias propagation improves the performance of every scoring function considered.
Figure 6: Figure 6a shows the pruning penalties for the specific experiment setting averaged over8 seeds. We use MEDIUM_conv network and perform pruning experiments on the second convolu-tional layer using a pruning fraction of 0.1. Figure 6b is the histogram of the squared activation’sof the pruned units from the same experiment. The distribution for mrs is includes many sampleswith high squared norm suggesting a high error term for the approximation.
Figure 7: Fraction of units selected at least once by the scoring algorithm accumulated throughoutthe training. We use the same experimental setting as Figure 6 and show how many different unitsthe scoring algorithm selects throughout the training. Random scorer selects all units at least onceby the iteration number 15000. Successful scoring functions somehow stay consistent with theirchoices and choose a small subset of units in the convolutional layer. In Figure 7b we repeat thesame plot discarding the measurements taken before step 10000. The set of units chosen by thescoring function decreases later in the training.
Figure 8:	Scatter plots generated similar to the ones in Section 4.1. However the entire set of resultsare partitioned according to the layer/s pruned.
Figure 9:	Pruning penalties measured after N fine tuning steps. For fine tuning steps a batch sizeof 64 and a learning rate of 1e-3 are used. The data is gathered from the first half of Cifar-10experiments.
Figure 10:	Performance of various pruning methods in iterative pruning setting. Networks arepruned iteratively starting from epoch 60000. We prune 1% of each layer and fine-tune with learningrate of 1e-3 for 100 iterations in between, until pruning targets are reached.
Figure 11: Pruning penalties from the experiments in Section 4.2 grouped by various pruning meth-ods. We do 2 comparisons in 1 graph by using different colours. For a label ‘Y VS X‘ x-axis repre-sents pruning penalties when pruning method X is used and y-axis represents the pruning penaltieswhen the method Y is used. Difference among different pruning methods diminishes with increasednumber of fine tuning steps.
