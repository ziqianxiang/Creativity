Figure 1: Example results. While the SV2P method (Babaeizadeh et al., 2018) produces blurry images, ourmethod maintains sharpness and realism through time. The prior SVG-LP method (Denton & Fergus, 2018)produces sharper predictions, but still blurs out objects in the background (left) or objects that interact with therobot such as the baseball (right).
Figure 2: Our proposed video prediction model. (a) During testing, we synthesize new frames by samplingrandom latent codes z from a prior distribution p(z) independently at each time step. The generator G takesa previous frame and a latent code to synthesize a new frame. (b) During training, the generator is optimizedto predict videos that match the distribution of real videos, using learned discriminators. The discriminatorsoperate on entire sequences. We sample latent codes from two distributions: (1) the prior distribution, and (2)a posterior distribution approximated by a learned encoder E. For the latter, the regression L1 loss is used.
Figure 3:	Qualitative Results (BAIR action-free dataset). Unless labeled otherwise, we show the closestgenerated sample to ground truth using VGG cosine similarity. SV2P immediately produces blurry results. OurGAN and VAE-based variants, as well as SVG-LP produce sharper results. However, SVG-LP still blurs outthe jar on the right side of the image when it is touched by the robot, while our GAN-based models keep thejar sharp. We show three results for our SAVP model: using the closest, furthest, and random samples. Thereis large variation between the three samples in the arm motion, and even the furthest sample from the groundtruth looks realistic. (bottom) We show a failure case where the arm disappears.
Figure 4:	Qualitative Results (KTH dataset). We show qualitative comparisons to ablations of our model.
Figure 5:	Realism vs Diversity. We measure realism using a real vs fake Amazon Mechanical Turk (AMT)test, and diversity using average VGG cosine distance. Higher is better on both metrics. Our VAE variantachieves higher realism and diversity than the SV2P (Babaeizadeh et al., 2018) and SVG (Denton & Fergus,2018) methods based on VAEs. Our GAN variant achieves higher realism than the pure VAE methods, at theexpense of significantly lower diversity. Our SAVP model, based on VAE-GANs, improves along the realismaxis compared to a pure VAE method, and improves along the diversity axis compared to a pure GAN method.
Figure 6:	Similarity of the best sample (BAIR action-free dataset). We show the similarity (higher is better)between the best sample (of 100) as a function of prediction time step across different methods and evaluationmetrics. (top) Although SV2P produces blurry and unrealistic images, it achieves the highest PSNR. BothSAVP and SVG-LP outperform SV2P on VGG similarity. We expect our GAN-based variants to underperformon PSNR and SSIM since GANs prioritize matching joint distributions of pixels over per-pixel reconstructionaccuracy. (bottom) We compare to ablated versions of our model. Our VAE variant achieves higher scores thanour SAVP model, which in turn achieves significantly higher VGG similarities compared to our GAN-onlymodel. Note that the models were only trained to predict 10 future frames (indicated by the vertical line), butis being tested on generalization to longer sequences.
Figure 7:	Similarity of the best sample (KTH dataset). We evaluate the similarity between the best predictedsample (out of a 100 samples) and the ground truth video. (top) As in the case of the robot dataset, SV2Pachieves high PSNR values, even though it produces blurry and unrealistic images. Although all three methodsachieve comparable VGG similarities for the first 10 future frames (which is what the models were trained for,and indicated by the vertical line), our SAVP model predicts videos that are substantially more realistic, asshown in our subjective human evaluation, thus achieving a desirable balance between realism and accuracy.
Figure 8: Architecture of our generator network. Our network uses a convolutional LSTM (Hochreiter &Schmidhuber, 1997; Xingjian et al., 2015) with skip-connection between internal layers. As proposed by Finnet al. (2016), the network predicts (1) a set of convolution kernels to produce a set of transformed input images(2) synthesized pixels at the input resolution and (3) a compositing mask. Using the mask, the network canchoose how to composite together the set of warped pixels, the first frame, previous frame, and synthesizedpixels. One of the internal feature maps is given to a fully-connected layer to compute the kernels that specifypixel flow. The output of the main network is passed to two separate heads, each with two convolutionallayers, to predict the synthesized frame and the composite mask. These two outputs use sigmoid and softmaxnon-linearities, respectively, to ensure proper normalization. We enable stochastic sampling of the model byconditioning the generator network on latent codes. These are first passed through a fully-connected LSTM,and then given to all the convolutional layers of the the convolutional LSTM.
Figure 9: Similarity of the best sample (BAIR action-conditioned dataset). We show the similarity betweenthe predicted video and the ground truth, using the same evaluation as in Fig. 14. We compare our deterministicand VAE variants when trained with L1 and L2 losses, and observe that they have a significant impact on thequality of our predictions. The models trained with L1 produce videos that are qualitatively better and achievehigher VGG similarity than the equivalent models trained with L2.
Figure 10: Realism vs Diversity (BAIR action-conditioned dataset). The SV2P method (Babaeizadeh et al.,2018) from prior work produces images with low realism, whereas our GAN, VAE, and SAVP models fool thehuman judges at a rate of around 35-40%. Our VAE-based models also produce videos with higher diversity,though lower diversity than other datasets, as this task involves much less stochasticity. The trend is the sameas in the other datasets. Our SAVP model improves the realism of the predictions compared to our VAE-onlymodel, and improves the diversity compared to our GAN-only model.
Figure 11: Similarity of the best sample (BAIR action-conditioned dataset). We show the similarity betweenthe predicted video and the ground truth, using the same evaluation as in Fig. 14, except that we condition onrobot actions. (top) We compare to prior SV2P (Babaeizadeh et al., 2018) and ours ablations. Our VAEand deterministic models both outperform SV2P, even though it is VAE-based. However, notice that the gapin performance between our VAE and deterministic models is small, as the dataset is less stochastic whenconditioning on actions. Our SAVP model achieves much lower scores on all three metrics. We hypothesize thatour SAVP model, as well as SV2P, is underutilizing the provided actions and thus achieving more stochasticityat the expense of accuracy. (bottom) We compare deterministic models—SNA (Ebert et al., 2017) and ours—and our VAE model when trained with L1 and L2 losses. As in the action-free case, we observe that thechoice of the pixel-wise reconstruction loss significantly affects prediction accuracy. Models trained with L1are substantially better in SSIM and VGG cosine similarity compared to equivalent models trained with L2.
Figure 12: Example generations of MoCoGAN. We use the unconditional version of MoCoGAN (Tulyakovet al., 2018) to generate videos from the BAIR robot pushing dataset. We chose this model as a representativerecent example of purely GAN-based unconditioned video generation. MoCoGAN produces impressive resultson various applications related to human action, which are focused on a actor in the middle of the frame.
Figure 13:	Qualitative visualization of diversity. We show predictions of our models, averaged over 100samples. A model that produces diverse outputs should predict that the robot arm moves in random directionsat each time step, and thus the arm should “disappear” over time in these averaged predictions. Consistent withour quantitative evaluation of diversity, we see that both our SAVP model and our VAE variant produces diversesamples, whereas the GAN-only method is prone to mode-collapse.
Figure 14:	Similarity of the best sample (BAIR action-free dataset), updated plot. We show the similarity(higher is better) between the best sample (of 100) as a function of prediction time step across different methodsand evaluation metrics. The three leftmost plots show the similarity (higher is better) between the best sample(of 100) as a function of prediction time step across different methods and evaluation metrics. Besides the stan-dard metrics, we also use the Learned Perceptual Image Patch Similarity (LPIPS) metric (Zhang et al., 2018),which has been shown to correlate well with human perception. This distance is measured in the AlexNet fea-ture space (pretrained on ImageNet classification) with linear weights calibrated to match human judgements.
Figure 15:	Similarity of the best sample and diversity (KTH dataset), updated plot. The three leftmostplots show the similarity (higher is better) between the best sample (of 100) as a function of prediction timestep. The plot on the right shows the diversity (higher is better) as a function of prediction time step, computedas the LPIPS distance between pairs of samples. The top and bottom plots show results when conditioning on10 and 2 frames, respectively. Among the VAE methods, our VAE-only model achieves substantially highersimilarities and diversities than the SVG model from prior work (Denton & Fergus, 2018). The GAN-onlymodel mode-collapses and generates samples that lack diversity. Our SAVP method, which incorporates thevariational loss, improves both sample diversity and similarities, compared to the GAN-only model. Our SAVPmodel also achieves higher accuracy than SVG.
