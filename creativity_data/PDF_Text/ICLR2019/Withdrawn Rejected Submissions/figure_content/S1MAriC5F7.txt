Figure 1: Promotion scheme for SHA with n = 9, r = 1, R = 9, and η = 3. (Left) Visual depictionof the promotion scheme for bracket s = 0. (Right) Promotion scheme for different brackets s. sincreases the starting budget per configuration r0 ≤ R by a factor of η per increment of s. Hence, ittakes more resources to explore the same number of configurations for higher s. Note that for a givens, the same budget is allocated to each rung but is split between fewer configurations in higher rungs.
Figure 2: Comparison of promotion schemes for SHA and ASHA. We show the promotion schemesfor Bracket 0 from Figure 1 (i.e. r = 1, R = 9, η = 3, and s = 0). Each bar represents a jobalong with the associated training budgets. The jobs submitted by SHA and ASHA are shown inchronological order going from left to right. The bars are labeled with the configuration numberand the color gradient corresponds to performance, with lighter bars having lower error; as inFigure 1(left), configurations 1, 6, and 8 are promoted to rung 1 and configuration 8 is promoted torung 2. SHA must complete all jobs in a rung before moving the next rung. In contrast, ASHA strivesto maintain the simple rule that each rung should always have about 1/n = 1/3 of the configurationsas the rung below it, while new configurations are added to the bottom rung as needed.
Figure 3: Sequential experiments (1 worker). Average across 10 trials is shown for each hyperpa-rameter optimization method. Gridded lines represent top and bottom quartiles of trials.
Figure 4: Limited-scale distributed experiments with 25 workers. For each searcher, the averagetest error across 5 trials is shown in each plot. The light dashed lines indicate the min/max ranges.
Figure 5: Large-scale ASHA benchmark that takes on the order of weeks to run with 500 workers.
Figure 6: Modern LSTM benchmark with DropConnect (Merity et al., 2018) using 16 GPUs. Theaverage across 5 trials is shown, with dashed lines indicating min/max ranges.
Figure 7:	Average number of configurations trained on the maximum resource R within 2000 timeunits for different standard deviations in training time and drop probabilities.
Figure 8:	Average time before a configuration is trained on the maximum resource R for differentstandard deviations in training time and drop probabilities.
Figure 9: Sequential Experiments (1 worker) with Hyperband running synchronous SHA. Hyper-band (by rung) records the incumbent after the completion of a SHA rung, while Hyperband (bybracket) records the incumbent after the completion of an entire SHA bracket. The average test erroracross 10 trials of each hyperparameter optimization method is shown in each plot. Dashed linesrepresent min and max ranges for each tuning method.
