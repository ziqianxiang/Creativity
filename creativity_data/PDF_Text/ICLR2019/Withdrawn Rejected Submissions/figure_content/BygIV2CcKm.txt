Figure 1: A generic data augmentation framework for the classification task. An input training sam-Ple X is transformed to X by a transformation model which is parameterized by τ. The conventionalmethod usually defines G as a composition of predefined transformations with randomly sampledcorresponding ranges τ (solid line path). In this paper, G is a differentiable parametric model, and alearnable network E for estimating T from X is also considered to obtain the transformed sample Xthat maximizes generalization of the classification model (dashed line path).
Figure 2: The proposed transformation models for spatial and appearance transformation. (a) Spatialtransformation model Gs takes an input τg and then generates sw and sb . Flow field s denotes thecoordinates to be transformed and is obtained by operations like the point-wise affine transform bySw and sb. The warp operation transforms X to X by interpolating X based on the coordinates in s.(b)Appearance transformation model Ga takes an input τα and then generates Cw and cb X is obtainedby filtering X by a filter with weights cw and bias cb .
Figure 3: Toy example on a synthetic binary classification dataset. Red and blue dots representdata points of class red and blue. A contour plot of predictive probability is overlaid. Each columnrepresents: (i) training samples and the predictive probability contour plot from model parameterslearned using training samples, (ii) validation samples and the predictive probability contour plotfrom model parameters learned using training sample, (iii) augmented samples to maximize influ-ence and the predictive probability contour plot from model parameters learned using augmentedsamples, (iv) validation samples with the predictive probability contour plot from model parameterslearned using augmented samples.
Figure 4: Images are transformed by applying linearly interpolated ten τ ’s to the same trainingsample in CIFAR-10 dataset. Each row represents: (i) the training image, (ii) spatial transformationmodel outputs, (iii) spatially transformed images, (iv) appearance transformation model outputs and(v) final transformed images.
