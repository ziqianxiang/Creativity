Figure 1: ModelsTo alleviate the pernicious loss function of IRGAN, we propose a model which uses two discrimi-nators in a co-operative setup influenced by Co-training (Blum & Mitchell (1998)). Instead of usingtwo different views (x1, x2) as mentioned in the work, we use the same views for both the discrim-inators but let them influence each other in a feedback loop. Training is similar to Model 1 with theonly difference being that each discriminator decreases the likelihood of documents relevant to theother discriminator rather than itself, as shown in the equation below. This model achieves betterperformance than IRGAN.
Figure 2: Performance curvesThe loss curves in Figure 2 picked from IRGAN’s work show deteriorating performance of the gen-erator, which is in contrast to what is observed in actual adversarial training. In the minimax setting,since the generator is expected to capture the real data distribution, its performance is supposed toimprove and this can indirectly be seen in GANs and DCGANs where the samples generated lookmore and more like real-world data points. Further, a deteriorating generator implies that the dis-criminator’s improvement in performance is only because of the first term of JD , which hints thatour proposed models might be able to do better than IRGAN. The reason offered in the paper is that“A worse generator could be the result of the sparsity of document distribution, i.e., each questionusually has only one correct answer”. But this reason does not seem plausible, given that DCGANshave been able to model very high dimensional data, where the probability distribution is only a tinypart of the real space.
Figure 3: Discriminator Loss for Content-Recommendation(2010)), where each documents is an arm and the context xq,d can be used to determine the action-value function fθ (xq,d). In previous works (Li et al. (2010)) f has been considered to be linear, butrecent studies Collier & Llorens (2018) have modeled them as deep neural networks.
