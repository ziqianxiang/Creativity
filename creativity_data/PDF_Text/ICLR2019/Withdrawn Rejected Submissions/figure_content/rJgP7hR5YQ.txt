Figure 1: An example of composition and decomposition for example 1.
Figure 2: An example of composition and decomposition for example 2.
Figure 3: Examples of MNIST-MB dataset. 5x5 grid on the left shows examples of MNIST digits(first component), middle grid shows examples of monochromatic backgrounds (second compo-nent), grid on the right shows examples of composite images.
Figure 4: Examples of MNIST-BB dataset. 5x5 grid on the left shows examples of MNIST digits(first component), middle grid shows examples of monochromatic backgrounds with shifted, rotated,and scaled boxes (second component), grid on the right shows examples of composite images withdigits transformed to fit into appropriate box.
Figure 5: Given component generators and composite data, decomposition can be learned.
Figure 6: Training composition and decomposition jointly can lead to “incorrect” decompositionsthat still satisfy cyclic consistency. Results from the composition and decomposition network. Wenote that decomposition network produces inverted background (compare decomposed backgroundsto original), and composition network inverts input backgrounds during composition (see back-grounds in re-composed image). Consequently decomposition and composition perform inverseoperations, but do not correspond to the way the data was generated.
Figure 7:	Given one component, decomposition function and the other component can be learned.
Figure 8:	Some results of chain learning on MNIST-BB. First we learn a background generator givenforeground generator for “1” and composition network, and later we learn the foreground generatorfor digit “2” given background generator and composition network.
Figure 9:	Composition network can be trained to edit for coherence. Only the model trained usingloss ld-cyc + lc achieves coherence in this example. For this model, we show illustrative failuremodes. In the first example, the network removes specificity of the sentences to make them coherent.
Figure 10: A simple example illustrating a bijective composition and corresponding resolving ma-trix.
Figure 11: Knowing the composition function is not sufficient to learn components and decompo-sition. Instead, the model tends to learn a “trivial” decomposition whereby one of the componentgenerators tries to generate the entire composed example.
Figure 12: Given one component, decomposition function and the other component can be learned.
Figure 13: Some results of chain learning on MNIST to Fashion-MNIST. First we learn a back-ground generator given foreground generator for digit “1” and composition network, and later welearn the foreground generator for T-shirts given background generator.
