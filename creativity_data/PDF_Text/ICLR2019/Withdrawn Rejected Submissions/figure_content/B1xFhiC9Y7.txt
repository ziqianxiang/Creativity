Figure 1: Illustration of the proposed patch-level alignment against the global alignment thatconsiders the spatial relationship between grids. We first learn discriminative representations forsource patches (solid symbols) and push a target representation (unfilled symbol) close to thedistribution of source ones, regardless of where these patches are located in the image.
Figure 2: Overview of the proposed framework. (a) A baseline model that utilizes global alignmentin (Tsai et al., 2018). (b) Our algorithm combines global and patch-level alignments in a generalframework that better preserves local structure. Note that Os, Ot ∈ (0, 1)H×W×C are distributionsand the figures used here are only for illustration purposes.
Figure 3: The proposed network architecture that consists of a generator G and a categorizationmodule H for learning discriminative patch representations. In the clustered space, solid symbolsdenote source representations and unfilled ones are target representations pulled to the sourcedistribution.
Figure 4: Visualization of patch-level feature representations via t-SNE. In each figure, two thousandpatches are sampled from each source and target domain.
Figure 5: Example results for GTA5-to-Cityscapes. Our method often generates the segmentationwith more details (e.g., sidewalk and pole) while producing less noisy regions.
Figure 6: Visualization of patch-level representations. We first shoW feature representations via t-SNEof our method and compare With the one Without the proposed patch-level alignment. In addition,We shoW patch examples in the clustered space. In each group, patches are similar in appearancebetWeen the source and target domains.
Figure 7: Example results of adapted segmentation for the Cityscapes-to-OxfordRobotCar setting.
Figure 8: Example results of adapted segmentation for the Cityscapes-to-OxfordRobotCar setting.
Figure 9: Example results of adapted segmentation for the GTA5-to-Cityscapes setting. For eachtarget image, we show results before adaptation, output space adaptation Tsai et al. (2018), and theproposed method.
Figure 10: Example results of adapted segmentation for the GTA5-to-Cityscapes setting. For eachtarget image, we show results before adaptation, output space adaptation Tsai et al. (2018), and theproposed method.
Figure 11: Example results of adapted segmentation for the SYNTHIA-to-Cityscapes setting. Foreach target image, we show results before adaptation, output space adaptation Tsai et al. (2018), andthe proposed method.
