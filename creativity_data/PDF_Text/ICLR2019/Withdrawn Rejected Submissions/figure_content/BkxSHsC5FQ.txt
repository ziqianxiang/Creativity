Figure 1: Illustration of class incremental learning. After we train a base model using all the avail-able data at a certain time point (e.g., classes 1 〜N1), new data belonging to new classes may con-tinuously appear (e.g., classes N2 〜N3, classes N4〜N5, etc) and We need to equip the model withthe ability to handle the new classes.
Figure 2: Overview of our framework. The basic idea is to incrementally train a deep learningmodel efficiently using the new class data and the support data of the old classes. We divide thedeep learning model into two parts, the mapping function (all the layers before the last layer) and thesoftmax layer (the last layer). Using the learned representation produced by the mapping function,we train an SVM, with which we can find the support vector index and thus the support data ofold classes. To stabilize the learned representation of old data, we apply two novel consolidationregularizers to the network.
Figure 3: Main results. (A)-(F): Performance comparison between SupportNet and the competingmethods on the six datasets in terms of accuracy. For the SupportNet and iCaRL methods, we setthe support data (examplar) size as 2000 for MNIST, CIFAR-10, CIFAR-100 and enzyme data, 80for the HeLa dataset, and 1600 for the breast tumor dataset.
Figure 4: (A): The accuracy deviation of SupportNet from the “All Data” method with respect tothe size of the support data. The x-axis shows the support data size. The y-axis is the test accuracydeviation of SupportNet from the “All Data” method after incrementally learning all the classes ofthe HeLa subcellular structure dataset. (B): The accumulated running time comparison betweenSupporNet and “All Data” method on MNIST.
Figure 5: The confusion matrix of different methods on the 6-class classification task for EC predic-tion: (A) the “Random Guess” method, (B) the “Fine Tune” method, (C) iCaRL, and (D) Support-Net. The data from the first five classes were given as the old data, and the ones from the sixth classwere given as the new data.
Figure 6: The accuracy matrices of different methods on MNIST. Those matrices show the perfor-mance composition of Fig 3 (A), considering those methods’ performance on the classes belongingto different class batch (CB) separately. In the matrix, each row represents the performance of thedeep learning model at each incremental training time point. Each column represents the perfor-mance of the deep learning model on each class batch’s test data. (A) GEM’s accuracy matrix onMNIST. (B) iCaRL’s accuracy matrix on MNIST. (C) VCL’s accuracy matrix on MNIST. (D) VCLwith K-center Coreset’s accuracy matrix on MNIST. (E) SupportNet’s accuracy matrix on MNIST.
Figure 7: The t-SNE visualization ofEC dataset’s feature representation at each incremental trainingtime point. (A1-A4). t-SNE result of the feature representation learned from SupportNet withoutany regularizers. (B1-B4). t-SNE result of the feature representation learned from SupportNet withthe consolidation regularizers.
Figure 8: The t-SNE visualization of MNIST dataset’s feature representation at each incrementaltraining time point. (A1-A5). t-SNE result of the feature representation learned from SupportNetwithout any regularizers. (B1-B5). t-SNE result of the feature representation learned from Support-Net with consolidation regularizers.
Figure 9: Performance of SupportNet with less support data. The experiment setting is the same asFig. 3, except for that We use less support data. ’SupportNet_500' means that We use only 500 datapoints as support data.
Figure 10: Performance comparison of SupportNet and iCaRL on tiny ImageNet dataset. The ex-periment setting is the same as Fig. 3, except for that we have more classes. ‘Difference’ shows theperformance difference between SupportNet and iCaRL along the training process.
