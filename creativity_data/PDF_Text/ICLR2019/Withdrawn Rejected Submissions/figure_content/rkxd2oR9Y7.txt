Figure 1: Sketch of how GGT performs fast full-matrix preconditioning. Note that the inverse ma-trices are understood here to be Moore-Penrose pseudoinverses; see Section 2.1 for a full treatment.
Figure 2: Synthetic experiments on convex loss functions, demonstrating the value of adaptive regu-larization and attenuation of gradient history. Left: An ill-conditioned instance of logistic regression.
Figure 3: Results of CNN and RNN experiments. GGT dominates in training loss across both tasks,and generalizes better on the RNN task. Top: CIFAR-10 classification with a 3-branch ResNet.
Figure 4: Evolution of the spectrum of the gradient matrix during training. Each vertical slice is adensity heatmap of the eigenvalues of G>t Gt . The black lines indicate the minimum and maximumeigenvalues, smoothed in time by a median filter. Top: CNN training. Approaching the end oftraining, the gradients become more anisotropic. Bottom: RNN training. Within the first few epochs,the gradients become more isotropic, then stabilize. (Truncated to 5 epochs; the density was visuallystable for the remainder of training.)We also tried using GGT as a drop-in replacement for Adam in the state-of-the-art word-level lan-guage modeling code accompanying (Merity et al., 2017; 2018). Although we were competitivewith Adam, We only observed an improvement in the first 〜20 epochs. We hypothesize that theadvantage of full-matrix regularization in this setting is more marginal, as the gradients in the em-bedding layers are naturally sparse in the vocabulary (“one-hot”) basis. On a similar note, we foundthat Adam outperformed GGT on attention-based architectures for NLP; refer to Appendix B for anexperiment and discussion.
Figure 5: Plot of experiments from Sections 3.2 and 3.3, with wall clock time on horizontal axisinstead of epoch count (as in Figure 3). Top: CIFAR-10 classification with a 3-branch ResNet.
Figure 6: Plot of experiments from Sections 3.2 and 3.3, with wall clock time on horizontal axisinstead of epoch count (as in Figure 3). Top: CIFAR-10 classification with a 19-layer vanilla CNN.
