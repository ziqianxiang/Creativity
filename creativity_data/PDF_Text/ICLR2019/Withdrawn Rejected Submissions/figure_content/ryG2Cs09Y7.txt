Figure 1: Overview of the proposed model. The top and bottom networks are the same copy thatshare all network parameters. Both the clean and adversarial images are forwarded through thenetwork to produce the corresponding attention features. The L2 regularization loss is defined as theEuclidean distance between the two sets of attention features. The final model loss is a combinationof the L2 regularization loss and the cross-entropy loss for only the adversarial input.
Figure 2: The relationship between attention weights and feature robustness. The horizontal axis isthe robustness rank, with 0 being the most robust and 63 the least robust, and the vertical axis is thecorresponding attention weights. Left plot shows the results for training set and right plot is for testset of CIFAR-10.
Figure 3: The learned attention maps of our model. The first row are the input images and the secondare the attention maps learned at residual block 4.
Figure 4: Gradient maps generated by compute gradient of model’s cross-entropy loss with respectto the input images. Top row are the original CIFAR-10 images, midrow are the gradient maps ofMadry et al. (2017), and bottom row are the gradient maps of our model. The raw gradients areclipped to within ±3 standard deviation and rescaled to lie in the [0, 1] range for visualization. Noother preprocessing is applied.
