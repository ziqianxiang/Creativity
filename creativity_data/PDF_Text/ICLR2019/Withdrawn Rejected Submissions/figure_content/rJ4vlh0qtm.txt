Figure 1: An example to demonstrate the message flow of spontaneous and self-organizing commu-nication (SSoC) architecture on a multi-agent system. The communication path is not predestined.
Figure 2: Structure of SSoC network.
Figure 3: An illustration of the multi-camera intelligent surveillance task.
Figure 4: The experiment of multi-camera intelligent surveillance taskin Fig.4 (b). SSoC performs best by getting highest rewards on average, as in table 1. We find thatthe agents learn a smart strategy that once the agent captures the ball, it can capture the ball allthe time until the ball moves outside its visual field. In addition, once an agent captures a ball, itsneighbor agent will search around by turning back and forth to capture the coming ball earlier. Thiscollaborative strategy is enabled by useful messages passing among the agents. When necessary, theagent learns to speak ”1” meaning that it decides to transfer the useful message to next agent. Whenthere is no effective message, the agent chooses to be silent. For better illustration, we visualizeSSoC’s learned policy in the video against other algorithms. The video is in demo. We recommendreaders to have a look at the video for better understanding of the policy learned by SSoC.
Figure 5: Emerged collaboration eabled by self-organizing communication on one test running.
Figure 6: Performance comparison on large-scale battle taskCommnet Meanfield Independent SSoCMean rewards	197.6	236.3	25.6	312.4Mean Kill 49	52	36	5T~Table 2: mean rewards on larget-scale battle task5 ConclusionIn this paper, we propose a SSoC network for MARL tasks. Unlike previous methods which oftenassume a predestined communication structure, the SSoC agent learns when to start a communi-cation or transfer its received message via a novel “Speak” action. Similar to the agent’s originalaction, this “Speak” can also be learned in a reinforcement manner. With such a spontaneous com-munication action, SSoC is able to establish a dynamic self-organizing communication structureaccording to the current state. Experiments have been performed to demonstrate better collaborativepolicies and improved on communication efficiency brought by such a design. In future work, wewill continue to enhance the learning of “Speak” action e.g. encoding a temporal abstraction tomake the communication flow more stable or develop a specific reward for this “Speak” action.
Figure 1: Illustrate the reward defined for multi-camera intelligent surveillancetask2.	Training details of SSoC on large-scale battle taskThe following algorithm gives the process of one round training on the large-scale battle task. The reward is provided by the MAgent[22] platform.
