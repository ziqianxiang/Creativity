Figure 1: Geometric intuition for DPPs: let φi , φj be two feature vectors of Φ such that the DPPkernel verifies L = ΦΦT; then，PL({ij}) 8 V0l(φi, φj). Increasing the norm of a vector(quality) or increasing the angle between the vectors (diversity) increases the volume spanned by thevectors (Kulesza & Taskar, 2012, Section 2.2.1).
Figure 3: Sampling on the unit square with a DppNet (1 hidden layer with 841 neurons) trained on asingle Dpp kernel. Visually, DppNet gives similar results to the full Dpp (left). As evaluated by DppNLL, the DppNet’s mode achieves superior performance to the full Dpp, and DppNet samplingoverlaps completely with Dpp sampling (right).
Figure 4: Digits sampled from a DPPNET (3 layer of 365 neurons) trained on MNIST encodings.
Figure 5: Results for the Nystrom approximation experiments, comparing DPPNET to the fast MCMCsampling method of Li et al. (2016c) according to root mean squared error (RMSE) and wallclocktime. We observe that subsets selected by DPPnet achieve comparable and lower RMSE than a DPPand the MCMC method respectively while being significantly faster.
Figure 6: Digits and VAE reconstructions from the MNIST training setCelebA encodings were generated by a VAE using a Wide Residual Network (Zagoruyko & Ko-modakis, 2016) encoder with 10 layers and filter-multiplier k = 4, a latent space of 32 full-covarianceGaussians, and a deconvolutional decoder trained end-to-end using an ELBO loss. In detail, thedecoder architecture consists of a 16K dense layer followed by a sequence of 4 × 4 convolutions with[512, 256, 128, 64] filters interleaved with 2× upsampling layers and a final 6 × 6 convolution with 3output channels for each of 5 components in a mixture of quantized logistic distributions representingthe decoded image.
