Figure 1: (a) The information flow in the unified Model-Free Hierarchical Reinforcement LearningFramework. (b) Temporal abstraction in the meta-controller/controller framework.
Figure 2: (a) The rooms task with a key and a box. (b) The results of the unsupervised subgoaldiscovery algorithm with anomalies marked with black Xs and centroids with colored ones. (c)Reward over an episode, with anomalous points corresponding to the key (r = +10) and the car(r = +40). (d) The average success of the controller in reaching subgoals during intrinsic motivationlearning in the pretraining phase. (e) The average episode return for the unified model-free HRLmethod. (f) The success of the unified model-free HRL in solving the rooms task.
Figure 3: (a) A sample screen from the ATARI 2600 game Montezuma’s Revenge. (b) The CNNarchitecture for the controller’s value function. (c) The CNN architecture for the meta-controller’svalue function. (d) The results of the unsupervised subgoal discovery algorithm. The blue circlesare the discovered anomalous subgoals and the red ones are the centroid subgoals. (e) The averageof return over 10 episode during the second phase of the learning. (f) The success of the controllerin reaching to the subgoals during the second phase of learning.
