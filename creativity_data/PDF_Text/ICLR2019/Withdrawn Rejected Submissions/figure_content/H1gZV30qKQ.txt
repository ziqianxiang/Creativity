Figure 1: an MDPWe design an MDP with 5 states as in Fig. 1: an initial state (s0), twointermediate states (s1, s2), a goal state (Sg), and a failure state (Sf). Sgand Sf are terminal states, so their state values are both 0. The task for theagent is to reach the goal state through one of the two paths. We denotethe path passing s1 as path1, and the path passing s2 as path2. Let fi > 0be environment parameters. At some state s, the agent takes an actiona ∈ [一1,1]. The reward is designed in a goal-based style with penaltiesto large |a|. If the agent transits from the initial state to an intermediatestate, it receives a reward r = 1 — |a|. Every time the agent visits thefailure state, it will be punished by a negative reward r = -5. The optimal policy for this task isaS = -fi if fi < f2； aS = f2 if fi ≥ f2； af(sι) = -fi； a&2) = f2.
Figure 2: The training curve of π(s0) during fine-tuning.
Figure 3: The training curves of training from scratch and transfer learning of MVC, TRPO, andDDPG. Thick lines correspond to mean episode rewards, and shaded regions show standard deviationsof 3 random seeds. Our method (MVC) achieves comparable performance with the baselines whilesignificantly outperforms them on transfer learning.
Figure 4: Train from scratch40003000<t:20001000200 Intt points, 4 opt steps200 Intt points, 0 opt steps1000 Inlt points, 0 opt step;O-L0	25	50	75 100 125 150 175 200Steps (xlQ00)Figure 5: Transfer learningHyperparameters We verify the influence of the initialization and the number of optimizationsteps in the action search process. Fig. 4 shows that calling Adam optimizer is critical to stabilize thetraining. In transfer learning scenario (Fig. 5), the optimization shows a more significant impact onthe performance and sample complexity. The agent that searches action with 200 initial points and 4optimization steps is enough to win the agent using only 1000 initial points by a large margin.
Figure 5: Transfer learningHyperparameters We verify the influence of the initialization and the number of optimizationsteps in the action search process. Fig. 4 shows that calling Adam optimizer is critical to stabilize thetraining. In transfer learning scenario (Fig. 5), the optimization shows a more significant impact onthe performance and sample complexity. The agent that searches action with 200 initial points and 4optimization steps is enough to win the agent using only 1000 initial points by a large margin.
Figure 6: The training curve during fine-tuning.
Figure 7: Density plot shows the estimated Q versus observed returns sampled from 5 test trajectories.
Figure 8: Comparison with more baselines.
