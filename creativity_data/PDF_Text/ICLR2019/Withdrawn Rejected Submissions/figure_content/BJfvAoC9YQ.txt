Figure 1: Visual depiction of feature transformation process on new episodes.
Figure 2: (a) Performance evolution over first 25 episodes and (b) Average performance acrossmultiple runs at the end of lifelong experimentsS.No	Method	Validation Accuracy1	Naive Leamer	30%2	Cumulative Training	99%3	Feature Transformers	96.4%(b)(a) Comparison with state-of-the-art methodsFigure 3: Comparison of proposed approach in conventional multi-task setting(b) Evolution of 1st task’s accuracy over timenamely - post the two pooling layers and fully connected layers. Feature transformer network es-sentially had one additional dense layer per step and was optimized for (equation 7).
Figure 3: Comparison of proposed approach in conventional multi-task setting(b) Evolution of 1st task’s accuracy over timenamely - post the two pooling layers and fully connected layers. Feature transformer network es-sentially had one additional dense layer per step and was optimized for (equation 7).
Figure 4: (a) Performance Comparison on validation dataset and (b) Comparison of feature trans-formers from different base layersS.No	Base Feature Extraction Layer	Validation Accuracy]	block3.p∞l	86.94%2	block4.p∞l	85.84%3	fc」	84.6%4	fc_2	83.2%(b)Fig. 4a captures the performance of feature transformer with the base features being extractedfrom first pooling layer - block3_pool. After fourth batch of data, feature transformer result almostmatches the performance of cumulative training. This performance is achieved despite not havingaccess to the full images but only the stored features. Table. 4b also presents the performanceof feature transformer depending upon the base features used. It can be noted that performanceis lowest for the layer that is closer to the classification layer - fc_2. This is intuitively satisfyingbecause, the further layers in a deep neural network will be more finely tuned towards the specifictask and deprives the feature transform of any general features.
Figure 5: Comparison with state-of-the-art methods - iCIFAR100 dataset4.3	ICIFAR100 DATASETWe present the 100 classes from CIFAR100 dataset in a sequence of 20 tasks comprising of 5 classeseach. Similar to Lomonaco & Maltoni (2017), we use the definitions for MT and single-incrementaltask (SIT). In an MT setting, evaluation is performed only on the new tasks exposed to the learnerin the current episode. We start with a VGG type architecture, pretrained on iCIFAR10 dataset assuggested by Lopez-Paz et al. (2017) and Lomonaco & Maltoni (2017). Base features are extractedfrom flatten layer (before the fully connected layers) and our feature transformers included twodense layers with feature length = 256. Similar to our earlier experiments, the feature transformermodule from the previous episode initializes the current episode transformer and is optimized forthe cumulative loss, with NoEPochs = 30 and batch_size = 32.
