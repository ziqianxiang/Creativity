Figure 1: Learning to detect an object from 3 min of video. Left to right: training video of a pen in hand,negative video without pen, two test videos with per frame detections shown as black dots. [video link]of changes between training and test videos, e.g. due to different lighting or changes in the background.
Figure 2: NEMO overview. (a) Spatial encoder architecture with n residual blocks, c channels, and kernelsize k. (b)-(d) Different losses for object detection; the target object is illustrated as a blue rectangle; thedetected object location z is shown as a black dot. (b) The variation loss based on two frames at times t andt+d enforces variation between detected object locations, resulting in a gradient that pushes z(t) and z(t+d)apart. (c) The slowness loss enforces object detections in frames t and t + 1 to be close, which produces agradient that pulls z(t) and z(t+1) together. (d) The presence loss enforces that the object is detected in thepositive frame t rather than in the negative frame t-, which creates a gradient that increases activations inthe positive frame and decreases them in the negative frame.
Figure 3: Learning to detect a moving object with a moving camera. Left to Right: Object detection infour random test frames; a set of image crops centered at the detected locations for random test frames. Topto bottom: Three separately trained object detectors for a Roomba in a living room, a drone in a hallway, anda toy car on a balcony. The last row shows toy car detector generalizing to a different scene. [video link]Figure 4: Learning to detect multiple objects: a USB cable (orange), a toy cow (green), a Rubik’s Cube(purple), a whiteboard marker (red), and a phone (blue). Top: Random frames in a multi-object test videoafter training on single object videos. Middle: Image crops centered at the detected object locations inrandom test frames. Bottom: Generalization to a test video from a different viewpoint. [video link]7Under review as a conference paper at ICLR 2019Figure 5: Demonstrating manipulation tasks with learned object representations. Left: Four demon-strations with overlayed first and the last frames. First frame object detections (black circles) are connectedto last frame detections (white circles). Right: Mean and three standard deviations of last frame detectionsacross all demonstrations reveal the goal of the tasks; white circle indicates task-relevance based on if aver-age object motion is at least 10 pixels. Top to bottom: Three manipulation tasks: 1) Move the cow (green) toa goal location. 2) Move all objects to a goal location. 3) Arrange objects in a vertical line ordered by size.
Figure 4: Learning to detect multiple objects: a USB cable (orange), a toy cow (green), a Rubik’s Cube(purple), a whiteboard marker (red), and a phone (blue). Top: Random frames in a multi-object test videoafter training on single object videos. Middle: Image crops centered at the detected object locations inrandom test frames. Bottom: Generalization to a test video from a different viewpoint. [video link]7Under review as a conference paper at ICLR 2019Figure 5: Demonstrating manipulation tasks with learned object representations. Left: Four demon-strations with overlayed first and the last frames. First frame object detections (black circles) are connectedto last frame detections (white circles). Right: Mean and three standard deviations of last frame detectionsacross all demonstrations reveal the goal of the tasks; white circle indicates task-relevance based on if aver-age object motion is at least 10 pixels. Top to bottom: Three manipulation tasks: 1) Move the cow (green) toa goal location. 2) Move all objects to a goal location. 3) Arrange objects in a vertical line ordered by size.
Figure 5: Demonstrating manipulation tasks with learned object representations. Left: Four demon-strations with overlayed first and the last frames. First frame object detections (black circles) are connectedto last frame detections (white circles). Right: Mean and three standard deviations of last frame detectionsacross all demonstrations reveal the goal of the tasks; white circle indicates task-relevance based on if aver-age object motion is at least 10 pixels. Top to bottom: Three manipulation tasks: 1) Move the cow (green) toa goal location. 2) Move all objects to a goal location. 3) Arrange objects in a vertical line ordered by size.
Figure 6: Baseline comparisons. Mean squared error (MSE) is computed in image coordinates normalizedto [0,1]. (a-c) MSE over time for three test videos. NEMO outperforms all baselines in (b) and (c). (d) MSEfor a given object shows substantial advantage compared to best baselines; methods are colored as in (a).
