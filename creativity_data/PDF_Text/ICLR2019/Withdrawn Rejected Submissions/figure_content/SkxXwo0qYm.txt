Figure 1: Operation batching for an affine transformation followed by a tanh element-wise activationfunctionFigure 1 illustrates the basic concept of operation batching. The function y = tanh(W x + b)is called three times for the independent input vector x1 , x2 , x3 . For efficient execution, we canconcatenate the three vectors into a matrix X and broadcast the bias vector b into a matrix B,then execute the affine transformation as Y = tanh(W X + B). The result Y consists of threecolumn vectors y1, y2, y3, which are the corresponding results of the function calls for x1, x2, x3.
Figure 2: Two computation graphs of calculating the loss on a mini-batch consisting of three traininginstances for a neural network using vanilla RNN.
Figure 3: Part of the generated computation graph of RNN after applying operation batching. Eachblue rectangle represents the batched operations of an RNN cell while the orange one represents theparameter matrix Whh. Each yellow rectangle represents a term of the summation that calculatesthe derivative of loss w.r.t Whh. In each rectangle, the variables with black color represent the pa-rameters and the intermediate results calculated in the forward propagation and those with red colorrepresent the derivatives calculated in the backward propagation. The black solid arrows representthe dependencies in the forward propagation while the red ones represent those in the backwardpropagation. The arrows of dashes represent the dependencies in the backward propagation, too.
