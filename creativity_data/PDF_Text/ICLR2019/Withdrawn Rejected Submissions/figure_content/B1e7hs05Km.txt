Figure 1: Thompson Sampling vs ε-greedy and Boltzmann exploration. (a) ε-greedy is wastefulsince it assigns uniform probability to explore over 5 and 6, which are obviously sub-optimal whenthe uncertainty estimates are available. Boltzmann exploration randomizes over actions even if theoptimal action is identifies. (b) Boltzmann exploration does not incorporate uncertainties over theestimated action-values and chooses actions 5 and 6 with similar probabilities while action 6 issignificantly more uncertain. Thomson Sampling is a simple remedy to all these issues.
Figure 2: BDQN deploys Thomp-son Sampling to, sample Wa ∀a ∈A around the empirical mean Wawith Wa the underlying parameterof interest.
Figure 3: The comparison between DDQN and BDQNFor the game Atlantis, DDQN+ gives score of 64.67k during the evaluation phase, while BDQNreaches score of 3.24M after 20M interactions. As it is been shown in Fig. 3, BDQN saturatesfor Atlantis after 20M interactions. We realized that BDQN reaches the internal OpenAIGym limitof max_episode, where relaxing it improves score after 15M steps to 62M, Appendix A.7. Weobserve that BDQN immediately learns significantly better policies due to its efficient explore/exploitin a much shorter period of time. Since BDQN on game Atlantis promise a big jump around time step20M, we ran it five more times in order to make sure it was not just a coincidence Appendix A.7Fig. 7. For the game Pong, we ran the experiment for a longer period but just plotted the beginning ofit in order to observe the difference. Due to cost of deep RL methods, for some games, we run theexperiment until a plateau is reached.
Figure 4: Effect of learning rate on DDQNA.4 Computational and sample cost comparison:For a given period of game time, the number of the backward pass in both BDQN and DQN are thesame where for BDQN it is cheaper since it has one layer (the last layer) less than DQN. In the senseof fairness in sample usage, for example in duration of 10 ∙ TBayes target = 100k, all the layers ofboth BDQN and DQN, except the last layer, sees the same number of samples, but the last layer ofBDQN sees 16 times fewer samples compared to the last layer of DQN. The last layer of DQN for aduration of 100k, observes 25k = 100k/4 (4 is back prob period) mini batches of size 32, which is16 ∙ 100k, where the last layer of BDQN just observes samples size of B = 100k. As it is mentionedin Alg. 1, to update the posterior distribution, BDQN draws B samples from the replay buffer andneeds to compute the feature vector of them. Therefore, during the 100k interactions for the learningprocedure, DDQN does 32 * 25k of forward passes and 32 * 25k of backward passes, while BDQN15Under review as a conference paper at ICLR 2019Table 4: The comparison of BDQN, DDQN, Dropout-DDQN and random policy. Dropout-DDQNas another randomization strategy provides a deficient estimation of uncertainty and results in poorexploration/exploitation trade-off.
Figure 5: The comparison between DDQN, BDQN and Dropout-DDQNA.7 Further investigation on Atlantis:After removing the maximum episode length limit for the game Atlantis, BDQN gets the score of62M. This episode is long enough to fill half of the replay buffer and make the model perfect for thelater part of the game but losing the crafted skill for the beginning of the game. We observe in Fig. 6that after losing the game in a long episode, the agent forgets a bit of its skill and loses few gamesbut wraps up immediately and gets to score of 30M . To overcome this issue, one can expand thereplay buffer size, stochastically store samples in the reply buffer where the later samples get storedwith lowest chance, or train new models for the later parts of the episode. There are many possiblecures for this interesting observation and while we are comparing against DDQN, we do not want toadvance BDQN structure-wise.
Figure 6: BDQN on Atlantis after removing the limit on max of episode length hits the score of 62Min 16M samples.
Figure 7: A couple of more runs of BDQN where the jump around 15M constantly happensA.8 Further discussion on ReproducibilityIn Table 2, we provide the scores of bootstrap DQN (Osband et al., 2016) and NoisyNet5Fortunatoet al. (2017) along with BDQN. These score are directly copied from their original papers and we didnot make any change to them. We also desired to report the scores of count-based method (Ostrovskiet al., 2017), but unfortunately there is no table of score in that paper in order to provide them here.
Figure 8: Two actions, with the same mean, but the one with higher variance on the return might beless safe than the one with narrower variance on the return.
