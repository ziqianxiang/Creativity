Figure 1: Neural architecture of DySAT: we employ structural attention layers followed by temporalattention layers. Dashed black arrows indicate new links and dashed blue arrows refer to neighbor-based structural-attention.
Figure 2: Performance comparison of DySAT with different models across multiple time steps: thesolid line represents DySAT; dashed lines represent static graph embedding models; and dotted linesrepresent dynamic graph embedding models. We truncate the y-axis to avoid visual clutter.
Figure 3: Heatmap visualizing mean and standard deviation of temporal attention weights over allnodes in Enron dataset.
Figure 4: Performance comparison of DySAT with different models on multi-step link predictionfor 6 future time steps on all datasetsD Impact of unseen nodes on Dynamic Link PredictionIn this section, we analyze the sensitivity of different graph representation learning on link predictionfor previously unseen nodes that appear newly at time t.
Figure 5: Performance comparison of DySAT with different models on link prediction restricted tonew nodes at each time step3	'	5	'	7	'	9	'	11	'	13Time steps14Under review as a conference paper at ICLR 2019Figure 6: Neural architecture of IncSAT: the components that are excluded from DySAT are wrappedby dashed blue rectangles. The intermediate node representations are directly loaded from savedmodels trained previously.
Figure 6: Neural architecture of IncSAT: the components that are excluded from DySAT are wrappedby dashed blue rectangles. The intermediate node representations are directly loaded from savedmodels trained previously.
