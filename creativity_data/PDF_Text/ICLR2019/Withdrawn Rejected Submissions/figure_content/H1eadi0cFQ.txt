Figure 1: Left: Training loss of a single-hidden-layer CNN on MNIST. Blue: Original network stuckin a flat area. Green: We add two filters at step 50 using our tunnel opening technique and we removethem again at step 300. Orange: We add and remove the same two filters, but instead of our gradientmaximization technique, we add the filters in a random fashion. Right: Resulting full gradient normafter tunnel opening in a fully optimized single-hidden-layer CNN on MNIST against the choice ofhyperparameter t. Each curve refers to a different number of tunnels opened (i.e. filters added).
Figure 2: Left Training loss of a single-hidden-layer CNN on MNIST. Blue: Original Networkstuck in a flat area. Orange: Random weight reorganization at step 50. Purple: Proposed weightreorganization at step 50. Shaded areas represent one standard deviation around the mean of 10repeated experiments. Right: Training loss of a five-layer CNN on ImageNet. Blue: OriginalNetwork stuck in a flat area. Orange: Random tunnel opening at step 5000. Purple: Proposed tunnelopening at step 5000. Shaded areas represent one standard deviation around the mean of 10 repeatedexperiments. Baselines: Green: Perturbed GD (Jin et al., 2017). Red: NEON (Xu & Yang, 2017).
Figure 3: Distance between new filter u0 and filter u from which it was copied during tunnel openingin MNIST experiment. Note that symmetry is broken faster using our proposed method, which isdue to the network escaping the flat area more quickly. Uniformly sampled λt , as well as fixedλt escape more slowly and therefore break symmetry more slowly. Note that in the special caseXt = 21, symmetry is never broken. Shaded areas are one standard deviation from the mean over 10experiment repetitions.
Figure 4: Full experiment runs. Top Left: 1-layer MNIST CNN opening and closing. Top Right:1-layer MNIST CNN weight reorganization. Bottom: 5-layer ImageNet CNN tunnel opening.
