Figure 1: Example of targeted adversarial attack on speech to text systems in practicewhen the adversarial example is nearing its target. The combination of these two approaches providesa 89.25% average targeted attack similarity with a 94.6% audio file similarity after 3000 generations.
Figure 2: Diagram of Baidu’s DeepSpeech model Hannun et al. (2014)layered, highly nonlinear decoder model that has the ability to decode phrases of arbitrary length.
Figure 3: Diagram of our genetic algorithmFigure 4: Overlapping of adversarial(blue) and original (orange) audio sam-ple waveforms. The perturbation isbarely noticeablefor each sample in the population to determine which samples are the best. Our scoring functionwas the CTC-Loss, which as mentioned previously, is used to determine the similarity between aninput audio sequence and a given phrase. We then form our elite population by selecting the bestscoring samples from our population. The elite population contains samples with desirable traits thatwe want to carry over into future generations. We then select parents from the elite population andperform Crossover, which creates a child by taking around half of the elements from parent1 andthe other half from parent2. The probability that we select a sample as a parent is a function of thesample’s score. With some probability, we then add a mutation to our new child. Finally, we updateour mutation probabilities according to our momentum update, and move to the next iteration. Thepopulation will continue to improve over time as only the best traits of the previous generations aswell as the best mutations will remain. Eventually, either the algorithm will reach the max number ofiterations, or one of the samples is exactly decoded as the target, and the best sample is returned.
Figure 4: Overlapping of adversarial(blue) and original (orange) audio sam-ple waveforms. The perturbation isbarely noticeablefor each sample in the population to determine which samples are the best. Our scoring functionwas the CTC-Loss, which as mentioned previously, is used to determine the similarity between aninput audio sequence and a given phrase. We then form our elite population by selecting the bestscoring samples from our population. The elite population contains samples with desirable traits thatwe want to carry over into future generations. We then select parents from the elite population andperform Crossover, which creates a child by taking around half of the elements from parent1 andthe other half from parent2. The probability that we select a sample as a parent is a function of thesample’s score. With some probability, we then add a mutation to our new child. Finally, we updateour mutation probabilities according to our momentum update, and move to the next iteration. Thepopulation will continue to improve over time as only the best traits of the previous generations aswell as the best mutations will remain. Eventually, either the algorithm will reach the max number ofiterations, or one of the samples is exactly decoded as the target, and the best sample is returned.
Figure 5: Histogram of levenshtein distances of attacks.
