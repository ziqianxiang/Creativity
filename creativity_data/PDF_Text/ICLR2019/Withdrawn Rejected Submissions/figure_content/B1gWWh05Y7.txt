Figure 1: Simulation results for the true reward landscape and SR(πθ) with different values ofτ in a bandit setting with 10,000 actions. Each action is represented by a feature ω ∈ R. LetΩ = (ωι,..., ω10,000) be the feature vector. The policy is parameterized by a weight scalar θ ∈ R.
Figure 2: Results of MENT (red), UREX (green), and REPMD (blue) on synthetic bandit problemand algorithmic tasks. Plots show average reward with standard error during training. Syntheticbandit results averaged over 5 runs. Algorithmic task results averaged over 25 random training runs(5 runs × 5 random seeds for neural network initialization). X-axis is number of sampled trajectories.
Figure 3: Learning curves of DDPG (red), TD3 (yellow), SAC (green) and PMAC (blue) on Mujocotasks (with SAC+R (gray) added on Humanoid). Plots show mean reward with standard error duringtraining, averaged over five different instances with different random seeds. X-axis is millions ofenvironment steps.
Figure 4: Ablation Study of REPMD and PMAC on ReversedAddition and Ant.
