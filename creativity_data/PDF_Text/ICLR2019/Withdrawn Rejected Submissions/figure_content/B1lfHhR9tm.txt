Figure 1: Overview of the decaNLP dataset with one example from each decaNLP task in the orderpresented in Section 2. Each task is framed as a form of question answering. Answer words in redare generated by pointing to the context, in green from the question, and in blue if they are generatedfrom a classifier over the full output vocabulary.
Figure 2: Overview of the MQAN model. It takes in a question and context document, encodesboth with a BiLSTM, uses dual coattention to condition representations for both sequences on theother, compresses all of this information with another two BiLSTMs, applies self-attention to collectlong-distance dependency, and then uses a final two BiLSTMs to get representations of the questionand context. The multi-pointer-generator decoder uses attention over the question, context, andpreviously output tokens to decide whether to copy from the question, copy from the context, orgenerate from a limited vocabulary.
Figure 3: An analysis of how the MQAN chooses to output answer words. When p(generation) ishighest, the MQAN places the most weight on the external vocab. When p(context) is highest, theMQAN places the most weight on the pointer distribution over the context. When p(question) ishighest, the MQAN places the most weight on the pointer distribution over the question.
Figure 4: MQAN pretrained on decaNLP outperforms random initialization when adapting to newdomains and learning new tasks. Left: training on a new language pair - English to Czech, right:training on a new task - Named Entity Recognition (NER).
Figure 5: Visualization of encoder activations for a set of 6 (question, answer) pairs in the order: ques-tion answering, machine translation, summarization, natural language inference, and commonsensereasoning. x-axis for each block represents time, and y-axis denotes neurons in the layer.
Figure 6: Visualization of attention weights over the context for a set of 6 (question, answer) pairs inthe order: question answering, machine translation, summarization, natural language inference, andcommonsense reasoning. x-axis for each block represents time.
Figure 7: Visualization of attention weights over the question for a set of 6 (question, answer) pairsin the order: question answering, machine translation, summarization, natural language inference,and commonsense reasoning. x-axis for each block represents time.
