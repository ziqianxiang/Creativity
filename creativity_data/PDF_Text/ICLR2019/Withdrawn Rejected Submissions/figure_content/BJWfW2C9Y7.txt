Figure 1: Predictive Local Smoothness3.1 PLS methodPLS is an adaptive learning rate method based on the local smoothness. The local smoothnessvaries based on the updating parameters {xt}t≥0 in stochastic gradient algorithms. According to thedefinition 1, the local smoothness sequence {L(xt)}t≥0 are ideally used to adjust the learning rate inthe neighborhood sequence {Nrxt (x*)}t≥o∙ However, it is difficult to compute {L(χt)}t≥o becauseof the unknown x*. Since there is a common point sequence on {χt}t≥ι in both {Nrxt_ 1 (xt)}t≥ιand {Nrxt (x*)}t≥o, {L(xt)}t≥ι can be predicted by {L(χ∕}t≥ι, which is easily computed by usingthe current gradient and the latest gradient. In this paper, our key idea is to use the predictive localsmoothness sequence {L(xt)}t≥1 instead of the unknown {L(xt)}t≥0 to adjust automatically thelearning rate ηt . PLS is described as the following three steps.
Figure 2: Logistic regression (convex) and least squares regression (convex) on mushroom usingSGD, SQN, adaQN, PLS-SGD and PLS-SQN.
Figure 3: Performance comparison of SGD, AMSGrad, AccSGD, PLS-SGD, PLS-AMSGrad andPLS-AccSGD on MNIST using neural networks with two fully-connected hidden layers. The lefttwo columns show the training loss and test loss for classification, while right two columns show thetraining loss and test loss for reconstruction.
Figure 4: Adaptive learning rates of three layers of neural networks for classification task on MNISTdataset using PLS-SGD, PLS-AMSGrad and PLS-ACCSGD.
Figure 5: Performance comparison of SGD, AMSGrad, AccSGD, PLS-SGD, PLS-AMSGrad andPLS-AccSGD on CIFAR10 using neural network with two fully-connected hidden layers. The left,middle, and right columns show the training loss, test loss, and adaptive learning rate, respectively.
Figure 6: Adaptive learning rates of different layers of neural network for reconstruction usingPLS-SGD, PLS-AMSGrad and PLS-ACCSGD on MNIST. The down row shows the adaptive learningrate in the first 200 iterations.
