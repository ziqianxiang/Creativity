Figure 1: (a) is the full autoencoder (step 1 to 4) , (b) is expanding the steps (3 for the LSTMand 4 for the factorization and node decoding) of the generator G of the autoencoder and (c)is the conditional setting a discriminator D that assesses the outputs and gives its feedbackto the generator G. Lrec (resp. Lcond) refers to the reconstruction (resp. condtional) lossdescribed in section 3.3.
Figure 2:	Conditional generation: The initial LogP value of the query molecule is spec-ified as IL and the Pearson correlation coefficient is specified as c. We report on the y-axisthe conditional value given as input and on the x-axis the true LogP of the generated graphwhen translated back into molecule (we have of course only reported the values when thedecoded graphs correspond to valid molecules, which explains the difference in the totalnumber of points for each query molecule).
Figure 3:	We report here a comparison of the abilities of previous recent models involvingmolecular graph generation and optimizationWe are interested in the following features of the models :•	Inference : If the model is equipped or not with an inference network. To encodesome target molecule like we do in the conditional setting.
Figure 4: Partial graph Autoencoder used for the pre-training part12Under review as a conference paper at ICLR 2019B.2	Mutual information maximizationFor the conditional setting we choose a simple mutual information maximization formula-tion. The objective is to maximize the MI I(X; Y ) between the target property Y and thedecoder’s output X = Gθ(Y ) under the joint pθ (X, Y ) defined by the decoder Gθ. In theconditional setting Gθ is also conditioned on the encoded molecule z but for simplicity wetreat it as a parameter of the decoder (and thus reason with one target molecule from whichwe want to modify attributes). We define the MI as:I ( y ； Gθ ( y ))= E X 〜Gθ( y )[E y，〜Pθ( y | X) [log Pθ ^ X )]] + H ⑻=Ex~Gθ(y) [DKL (pθ (」x) ||Q(∙ |x))+ Ey…Pθ(y |X)[log Q(y!\x)]] + H(y)≥ EX〜Gθ(y)[Ey，〜Pθ(y|X) [log Q(9，\X)]] + H⑻In our conditional setting we pre-trained the discriminators (parametrized by Q in thelower bound derivation) to approximate pdata(y\X) which makes the bound tight only whenPθ (ypaired\x) is close to Pdata (y\x) and this corresponds to a stage where the decoder hasmaximized the log-likelihood of the data well enough (i.e. when it is able to reconstructinput graphs properly when z and y are paired). Thus, in the conditional setting we aremaximizing the following objective:
Figure 5: Accuracy score as a function of the number of heavy atoms in the molecule(x axis)for different size of the latent codeNotice that as we make use of a simple LSTM to encode a graph representation, thereis a risk that for the largest molecules the long term dependencies of the embeddings arenot captured well resulting in a bad reconstruction error. We capture this observation infigure 4. One possible amelioration could be to add other at each step other non-sequentialaggregation of the embeddings (average pooling of the emebeddings for example) or to makethe encoder more powerful by adding some attention mechanisms. We leave those for futurework.
Figure 6: LogP increasing task visual example. The original molecule is circled in red.
