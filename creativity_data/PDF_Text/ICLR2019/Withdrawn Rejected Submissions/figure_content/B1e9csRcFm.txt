Figure 1: Average l2 Norm during SGD Optimization(c) YouTubeSGD Optimization Results in Small Vector Norm: Figure 1 shows the average l2 norm of theembedding vectors during the first 50 SGD epochs (with varying value of λr). As we can see, theaverage norm of embedding vectors increases consistently after each epoch, but the increase rate getsslower as time progresses. In practice, the SGD procedure is often stopped after 10 〜50 epochs(especially for large scale graphs with millions of vertices4), and the relatively early stopping timewould naturally result in small vector norm.
Figure 2: Average Norm of Stochastic Gradients during SGD OptimizationThe Vanishing Gradients: Figure 2 shows the average l2 norm of the stochastic gradients ∂L∕∂xuduring the first 50 SGD epochs:∂L∂xu-	σ(-xuTxv)xv +	σ(xuTxv)xv + 2λrxuv∈N+ (u)	v∈N- (u)(3)From the figure, we can see that the stochastic gradients become smaller during the later stage ofSGD, which is consistent with our earlier observation in Figure 1. This phenomenon can be intuitivelyexplained as follows: after a few SGD epochs, most of the training data points have already been wellfitted by the embedding vectors, which means that most of the coefficients σ(±xuTxv) in Eqn (3) willbe close to 0 afterwards, and as a result the stochastic gradients will be small in the following epochs.
Figure 3: Generalization Performance During SGDRegularization and Early Stopping: Figure 3 shows the generalization performance of embeddingvectors during the first 50 SGD epochs, in which we depicts the resulting average precision (AP)score5 for link prediction and F1 score for node label classification. As we can see, the generalizationperformance of embedding vectors starts to drop after 5 〜20 epochs when λr is small, indicatingthat they are overfitting the training dataset afterwards. The generalization performance is worst nearthe end of SGD execution when λr = 0, which coincides with the fact that embedding vectors in thatcase also have the largest norm among all settings. Thus, Figure 3 and Figure 1 collectively suggestthat the generalization of linear graph embedding is determined by vector norm.
Figure 4: Impact of Embedding DimensionEmbedding Dimension(d) λr = 14 Demonstrating the Importance of Norm Regularization viaHinge-Loss Linear Graph EmbeddingIn this section, we present the experimental results for a non-standard linear graph embeddingformulation, which optimizes the following objective:L =λ+1	X	h(xTuxv)	+	λ-1	x	h(-χTXv)+λ2r	X ||xv||2	⑷(u,v)∈E+	(u,v)∈E-	v∈VBy replacing logistic loss with hinge-loss, it is now possible to apply the dual coordinate descent(DCD) method (Hsieh et al., 2008) for optimization, which circumvents the issue of vanishinggradients in SGD, allowing us to directly observe the impact of norm regularization. More specifically,consider all terms in Eqn (4) that are relevant to a particular vertex u:L(U)=	X	λyi max(1 - yiXTXi,。) + 1|匕『.	(5)λr	2(xi,yi)∈Din which we defined D = {(Xv, +1) : v ∈ N+(u)} ∪ {(Xk, -1) : k ∈ N-(u)}. Since Eqn (5)takes the same form as a soft-margin linear SVM objective, with Xu being the linear coefficientsand (Xi , yi ) being training data, it allows us to use any SVM solver to optimize Eqn (5), and thenapply it asynchronously on the graph vertices to update their embeddings. The pseudo-code for the
Figure 5: Generalization Average Precision with Varying λr (D = 100)Impact of Regularization Coefficient: Figure 5 shows the generalization performance of embeddingvectors obtained from DCD procedure (〜20 epochs). As We can see, the quality of embeddingsvectors is very bad when λr ≈ 0, indicating that proper norm regularization is necessary forgeneralization. The value of λr also affects the gap betWeen training and testing performance, Whichis consistent With our analysis that λr controls the model capacity of linear graph embedding.
Figure 6: Generalization Average Precision with Varying D (λr = 3)5	DiscussionSo far, we have seen many pieces of evidence supporting our argument, suggesting that the general-ization of embedding vectors in linear graph embedding is determined by the vector norm. Intuitively,it means that these embedding methods are trying to embed the vertices onto a small sphere centeredaround the origin point. The radius of the sphere controls the model capacity, and choosing properembedding dimension allows us to control the trade-off between the expressive power of the modeland the computation efficiency.
