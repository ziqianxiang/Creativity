Figure 1: Combination of our proposed strategies during FL training. We reduce the size of themodel to be communicated by (1) constructing a sub-model via Federated Dropout, and by (2)lossily compressing the resulting object. This compressed model is then sent to the client, who (3)decompresses and trains it using local data, and (4) compresses the final update. This update is sentback to the server, where it is (5) decompressed and finally, (6) aggregated into the global model.
Figure 2: Federated Dropout applied to two fully-connected layers. Notices activation vectors a, b =σ(U a) and c = σ(V b) in (I). In this example, we randomly select exactly one activation from eachlayer to drop, namely a1 , b2 , and c3, producing a sub-model with 2 × 2 dense matrices, as in (II).
Figure 3: Effect of varying our lossy compression parameters on CIFAR-10 and EMNIST.
Figure 4: Results for Federated Dropout, varying the percentage of neurons kept in each layer.
Figure 5: Effect of using both compression and Federated Dropout on CIFAR-10 and EMNIST.
Figure 6: Compressing an already trained MNIST model with linear transform + subsampling +uniform quantization.
Figure 7: Effect of varying our lossy compression parameters on the convergence MNIST.
Figure 8: Effect of varying the percentage of neurons kept in each layer on MNIST.
Figure 9: Effect of using both lossy compression and Federated Dropout on MNIST.
