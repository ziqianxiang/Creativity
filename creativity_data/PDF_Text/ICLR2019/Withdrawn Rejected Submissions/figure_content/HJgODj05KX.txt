Figure 1: Convex cost function: evolution of the least squares cost for (left) the best performingparameter configuration of the tested algorithms; (right) several choices of acceleration parameterμo for PA-SGD. The new PA-SGD converges faster and achieves an almost optimal cost functionvalue after 10 - 20 epochs. ADAM with β1,0 = 0.9 and α0 = 10-1 has similar convergence asAMSGrad and was omitted. We also include a zoom-in, in log-log scale, of the gap between theleast squares cost and that of the other methods.
Figure 2: Convex cost function: evolution of the logistic regression cost for (left) the best performingparameter configuration of the tested algorithms; (right) several choices of acceleration parameterμo for PA-SGD. The new PA-SGD exhibits faster convergence than both ADAM and AMSGrad.
Figure 3: Neural network training: (left) evolution of the mean squared error cost; we also includea zoom-in, in log-log scale; (right) evolution of the multinomial logistic regression cost. The bestperforming parameter configurations are presented. For low value of the acceleration parameterμo, PA-SGD exhibits comparable convergence with respect to ADAM and AMSGrad. Our methodshows benefit when a larger acceleration parameter μ° is used, which allows for a lower final costalbeit with slower initial convergence speed. AMSGrad has a comparable behavior with ADAMwith all parameter choices. For the logistic regression we note that towards the final stages ofconvergence all methods exhibit large fluctuations of the cost function values.
