Figure 1: A toy example of a IMV-LSTM with a two-variable input sequence and the hidden matrixof 4-dimensions per variable. Circles represent one dimensional elements. Purple and blue colorscorrespond to two variables. Blocks containing rectangles with circles inside represent input data andhidden matrix. Panel (a) exhibits the derivation of hidden update jt . Grey areas represent transitionweights. Panel (b) demonstrates the mixture attention process. (best viewed in color)To distinguish from the hidden state and gate vectors in a standard LSTM, hidden state and gatematrices in IMV-LSTM are denoted with tildes. Specifically, we define the hidden state matrix attime step t as ht = [h1 , ∙ ∙ ∙ , hN]>, where hit ∈ RNXd, h[ ∈ Rd. The overall size of the layer isderived as D = N ∙ d. The element htn of ht is the hidden state vector specific to n-th input variable.
Figure 2: (a) Histograms of overall variable attention at different epochs. (b) Histograms of temporal attentionof variable “P-temperature”. (c) Prior and posterior attention histograms of two example variables.
Figure 3: Variable and temporal importance interpretation during the training of IMV-Full and IMV-Tensor onPLANT dataset. (Best viewed in color)4.5 Variable importance for predictionIn this group of experiments, we evaluate the efficacy of variable importance through the lens ofprediction tasks. We focus on IMV-LSTM family and RNN baselines, i.e. DUAL and RETAIN.
Figure 4: Q-Q plot of testing errors16Under review as a conference paper at ICLR 2019RMSE	RMSE	RMSEBO	1000.000.1750.1500.125 ■0.1000.0750.0500.02520	40	60Epoch0.00IMV-TensorBO	100
Figure 5: Testing error curves over the training phase17Under review as a conference paper at ICLR 20196.2.3 Model InterpretationIn this part, we exploit the domain knowledge in the literature corresponding to the dataset, so asto qualitatively explain the importance learned by IMV-LSTM. Fig. 6-9 show the quantitativevariable-wise temporal and variable importance values of IMV-LSTM on different datasets. Notethat not all datasets in the experiments have associated study of variable importance in the literature.
Figure 6:	IMV-Full on SML dataset. (Best viewed in color)----Temp, dlnnlιχfFwecasttemp.
Figure 7:	IMV-Tensor on SML dataset. (Best viewed in color)In the following Table 5, 6, and 7, we list the full ranking of variables of the datasets by each approach.
Figure 8:	IMV-Full on NASDAQ dataset. (Best viewed in color)19Under review as a conference paper at ICLR 2019Variable Importance——AAL	——CELG——AAPL	  CERN——ADBE ——CMCSA——AQI	——COST——ADP	——CS∞——ADSK ——CSX--AKAM	——CIRP——ALXN ---- CTSH--AMAT	——DISCA——AMGN ---- DISH——AMZN --------- DLTR——ATVl ---- EA——AVGO	——EBAY——BBBY	——ESRX——BIDU	——EXPE——BIIB	- FAST
Figure 9: IMV-Tensor on NASDAQ dataset. (Best viewed in color)20Under review as a conference paper at ICLR 2019Table 5: Variable importance ranking by IMV-Full and IMV-Tensor on NASDAQ dataset.
