Figure 1: Comparison of Proximal Policy Optimization (PPO) and our proposed PPO-CMA method.
Figure 2: Comparing policy gradient variants in the didactic example of Figure 1, when doing mul-tiple minibatch updates with the data from each iteration. Actions with positive advantage estimatesare shown in green, and negative advantages in red. Top: Basic policy gradient is highly unstable.
Figure 3: Our didactic example solved by CMA-ES using x ≡ a and f (x) ≡ r(a). Red denotespruned samples that have zero weights in updating the exploration distribution. CMA-ES expandsthe exploration variance while progressing on an objective function slope, and shrinks the variancewhen reaching the optimum.
Figure 4: The difference between joint and separate updating of mean and covariance, denoted bythe black dot and ellipse. A) sampling, B) pruning and weighting of samples based on fitness, C)EMNA-style update, i.e., estimating mean and covariance based on weighted samples, D) CMA-ESupdate, where covariance is estimated before updating the mean.
Figure 5: Means and standard deviations of training curves in OpenAI Gym MuJoCo tasks. Thehumanoid results are from 5 runs and others from 10 runs. Red denotes our vanilla PPO imple-mentation with the same hyperparameters as PPO-CMA and PPO-CMA-m, providing a controlledcomparison of the effect of algorithm changes. Gray denotes OpenAI’s baseline PPO implementa-tion using their default hyperparameters and training scripts for these MuJoCo tasks.
Figure 6: Comparing PPO and PPO-CMA in 20 training runs with the Hopper-V2 environment. Redtrajectories denote clear failure cases where the reward plateaus and the agent only falls forward ortakes one or a few steps. PPO’s variance decreases faster and there is also some instability withvariance peaking suddenly, associated with a decrease of rewards. PPO-CMA has no similar plateausand adapts variance robustly and consistently.
Figure 7:	Same as Figure 6 but with the automatic state observation normalization described inAppendix B. PPO-CMA is more robust to the normalization. PPO failure cases are associated withlower variance.
Figure 8:	Boxplot of PPO results of 20 training runs of Hopper-V2 with different entropy lossweights and lower clipping limits for policy’s standard deviation. The plots are from the last iterationwhere a limit of 1M total simulation steps was reached. The upper standard deviation limit is 1 inall plots and the entropy loss weight plots use a lower clipping limit of 0.01. Large values of eitherparameter can help escaping the local optimum of simply falling forward or taking only 1 or fewsteps (rewards below 1000), but at the same time, too large values impair average and best-caseperformance.
Figure 9: Comparing the effect of PPO variance clipping and entropy loss. Even with a fairly lowweight, the entropy loss can lead to worse results and cause unstable increases in variance that yieldlow rewards (the red trajectory in the rightmost images).
Figure 10: Algorithm differences in the didactic example of Figure 1. Here, instead of showingthe results of each iteration, we show how the policy is updated in the minibatch gradient stepsinside a single iteration. Actions with positive advantage estimates are shown in green, and negativeadvantages in red. The black ellipses denote the policy mean and standard deviation. The greennon-filled circles visualize the negative-advantage actions converted to positive-advantage actionsthrough the mirroring trick of PPO-CMA-m.
Figure 11: Training curves and final reward boxplots of PPO-CMA in 20 training runs of Hopper-V2 with different history lengths H . The orange lines show medians and the whiskers show thepercentile range 10...90. With H = 1, there are outliers below a reward of 500, in which case theagent only lunges forward and falls without taking any steps.
