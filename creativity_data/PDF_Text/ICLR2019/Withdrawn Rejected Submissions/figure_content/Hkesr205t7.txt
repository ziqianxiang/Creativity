Figure 1: An overview of shared manifold learning with MIVAE to achieve GZSL. At training time, MIVAElearns a mapping to latent space z, qφ(z∣x, ay), so that it integrates image X and class-attributes ay , andproperly places even unseen data. Note that mappings of each of the image and the attributes, qλ(z∣x) andqλ(z|ay), are also learned to approximate this mapping, qφ(z∣x, ay). At test time, We can properly arrangethe attributes of the unseen classes on the latent space using the learned mapping of the attributes, qλ(z∣ay).
Figure 2: Explanation of biased problem in GZSL. Each circle represents the distribution in the representationspace of each class, and the dotted circles are the unseen classes. In GZSL, if the representations of the unseenclasses overlap the seen classes, all unseen data might be predicted in any of the seen classes. On the otherhand, ZSL does not have such a problem as there are no seen classes at test time. Note that in this explanation,we ignore the difficulty of learning the generalized relations between images and attributes, but actually, it isnecessary to consider the influence of that as well.
Figure 3: Learning curves in GZSL with joint representation learning methods. The left and middle plotrespectively present the accuracy of seen and unseen classes in the test data. The right shows the harmonicmean evaluation.
Figure 4: t-SNE plots of joint latent representations. After selecting five seen classes and five unseen classesfrom AWA2, we embedded their images and attributes. Circle plots are the embedding of the images. Trianglesrepresent embedding of the attributes (4 denotes a seen class and 5 is an unseen class). Each color of the plotcorresponds to a different class: (a) MED-VAE, (b) VZSL, and (c) MIVAE, where λmap = 1.
