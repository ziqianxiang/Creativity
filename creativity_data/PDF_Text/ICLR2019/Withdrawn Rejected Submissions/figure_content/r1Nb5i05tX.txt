Figure 1: To measure the MI between the features at layer i and the target labels Y , we use the MINEestimator of (Belghazi et al., 2018). To measure the MI between a noisy version of the features atlayer i and the input X, we use our AA-MINE, which contains an internal copy of layers 1, . . . , i(see zoom-in on the right).
Figure 2: Estimating the (noise-regularized) MI between the input and last layer of a MLP (784 -512 - 512 - 10) with random weights using a standard MINE (Belghazi et al., 2018) and our AA-MINE (see Sec. 4). For zero noise level (σ2 = 0), the MI is theoretically infinite, which is capturedby our AA-MINE but not by the standard MINE. This demonstrates the advantage of incorporatingprior-knowledge into the MI estimation scheme. When enforcing stronger regularization (increasingnoise level), our AA-MINE is stable and estimates decreasing values of MI, as expected.
Figure 3: Information plane dynamics with conventional training. In this experiment, a 3-layerMLP with ReLU activations was trained to classify MNIST digits and MI with input/target weremeasured with AA/standard MINE. (a) When using only the cross-entropy loss, both the MI withthe input and the MI with the output tend to increase throughout the entire training process, aligningwith the observations of Saxe et al. (2018). (b) When adding weight regularization, a compressionphase emerges for the first two layers, where their MI with the input begins to decrease after theirMI with the target reaches high values. Note that, as observed by Saxe et al. (2018), the noise-regularized complexity term does not satisfy the data-processing inequality, so that I(X, Li + ε)need not necessarily be larger than I(X, Lj + ε) for j > i.
Figure 4: Information plane dynamics for layer-by-layer training explicitly with the IB functional.
Figure 5: Visualizing the effect of training using the IB principle with t-SNE embeddings. Left: Theembedding of L1 when setting the trade-off coefficient to (a) β = 102, and (b) β = 10-3. Whenincreasing β , the training procedure attempts to discretize the latent representation, which indeedacts as a form of compression. Right: The embedding of L3 when (c) training end-to-end with across-entropy loss, and (d) training layer-by-layer with the IB functional. With the IB functional,we obtain better separated and concentrated clusters.
Figure 6: CIFAR-10 test accuracy as function of training steps for for various combinations ofloss terms. Here we used a CNN with 6 convolutional layers and 3 fully connected layers (seeAppendix C for details). In all cases, faster convergence to a superior result is obtained whenusing IB regularization relative to using only the cross-entropy loss. In particular, note that thisdemonstrates the superiority of explicitly enforcing high values of I(Y, Y) in conjunction with lowcross-entropy values, over using cross-entropy alone.
