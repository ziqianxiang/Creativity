Figure 1: 2-D toy example. Compared dynamics are defined in Table 1, k = 2, σt2 is tuned to keep noise ofall dynamics sharing same expected squared norm, 0.01. All dynamics are run by 500 iterations with learningrate 0.005. (a) The trajectory of each compared dynamics for escaping from the sharp minimum in one run.
Figure 2: One hidden layer neural net-works with fixed output layer parameters.
Figure 3:	FashionMNIST experiments. (a) The first 400 eigenvalues of Hessian at θGD, the sharp minimafound by GD after 3000 iterations. (b) The projection coefficient estimation a = u1；：； H, as shown inProposition 3. (C)Tr(Ht∑t) versus Tr(Ht∑t) during SGD optimization initialized from θGd，Σt = τD∑t Idenotes the isotropic noise with same expected squared norm as SGD noise.
Figure 4:	FashionMNIST experiments. Compared dynamics are initialized at θGD found by GD, marked bythe vertical dashed line in iteration 3000. The learning rate is same for all the compared methods, ηt = 0.07,and batch size m = 20. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration. (c) Expectedsharpness versus iteration. Expected sharpness is measured as EV〜N(0户1)[L(θ + ν)] — L(θ), and δ = 0.01,the expectation is computed by average on 1000 times sampling.
Figure 5: L? norm of gradient mean, kVL(θt) k, and the expected norm of noise √ηtE[eTet]∕m dur-ing the training using SGD. The dataset and model are same as the experiments of FashionMNISTin main paper, or as in Section C.2B.2 THE FIRST 50 ITERATIONS OF FASHIONMNIST EXPERIMENTS IN MAIN PAPERFigure 6 shows the first 50 iterations of FashionMNIST experiments in main paper. We observethat SGD, GLD 1st eigvec(H), GLD Hessian and GLD leading successfully escape from the sharpminima found by GD, while GLD diag, GLD dynamic, GLD const and GD do not.
Figure 6:	The fisrt 50 iterations of FashionMNIST experiments in main paper. Compared dynamics areinitialized at θGD found by GD. The learning rate is same for all the compared methods, η = 0.07, and batchsize m = 20. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration.
Figure 7:	CIFAR-10 experiments. Compared dynamics are initialized at ΘgD found by GD, marked by thevertical dashed line in iteration 3000. The learning rate is same for all the compared methods, ηt = 0.05, andbatch size m = 100. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration. (c) Expectedsharpness versus iteration. Expected sharpness is measured as EV〜N(o,δ2i) [L(θ + ν)] — L(θ), and δ = 0.01,the expectation is computed by average on 100 times sampling. All observations consist with main paper.
Figure 8: Constructed 2-dimensional surface in main paper.
