Figure 1: An overview of the steps used to obtain a context-dependent hierarchical explanation. Step1 inputs a data instance of interest (e.g. a sentence) into a complex model, in this case a classifier.
Figure 2: An illustration of the hypothe-sis that certain local interactions, whichare similar (left), are represented at acommon manifold (right) in a model.
Figure 3: Results of synthetic experiments with Mahe and different baselines explaining base mod-els XGBoost (tree), MLP, LSTM trained on F1-4 (Table 2) at σ = 0.6 (max= 3.2) are shown. (a)ShoWS the average local fit in MSE of Mah´ and baselines on the base models' representations ofrespective interactions. (b) shows the average R-precision of interaction rankings from each base-line. *Shap-Tree cannot detect or fit to a three-way interaction. fWe assume interaction order isunknown, and ACD-MLP and ACD-LSTM require exhaustive search of all possible interactions.
Figure 4: ExamPle of exPlanations that Mechanical Turk users choose from for a sentiment analysistask. (a) is linear LIME, (b) is Mah´. LIME explanations are shown as positive and negativecontributions of each feature (word) to the prediction, and Mahe explanations are shown similarlywith one of the contributions belonging to a single interaction or group of words.
Figure 5: Interaction polarity con-sistency in Sentiment-LSTM be-fore and after model modification.
Figure 6: Examples of context-dependent explanations in hierarchical format for ResNet152, whereimages come from the ImageNet test set. Interaction attributions of {gi(∙)}K=ι are show at eachK-1 level, K ≥ 1 (§4.1). Colors in superpixels represent attribution scores and their polarity. Cyanregions positively contribute to the predicton, and red regions negatively contribute. Boundariesbetween overlapping interactions are merged when their attribution polarities match.
Figure 7: Context-free explanations of interactions in ResNet152 for each image, evaluated over40 tests before and after modification. Each test superimposes the interaction of interest onto arandomly selected background image from the test set of ImageNet. Corresponding predictionsare shown above each image, and the percentage of consistent interaction polarity before and aftermodification are shown below in that order. The red color indicates a negative interaction attributionpolarity before modification. After modification, the polarities become positive.
Figure 8: Example of Mechanical Turk interface used by workers to select between explanationsprovided by linear LIME and MahC4https://github.com/marcotcr/lime14Under review as a conference paper at ICLR 2019Text with highlighted wordsDirecM Rob Maι-shallText with highlighted wordsText with highlighted wordsText with highlighted wordsup, Ballistic is Odd15up, Ballistic 国 OddIyThe issue of faith is not explored verydeeplyA that respects the Marvel versionwithout becoming ensnared by it.
Figure 9: Randomly selected comparisons between (a) linear LIME and (b) Mahe explanations ofSentiment-LSTM used in Mechanical Turk experiments (§5.2.2).
Figure 10: Average runtime of linear LIME versus Mahe on context-dependent explanations. RUn-times for experiments in Table 3 are shown. “local inference” is the runtime for sampling in thelocal vicinity of a data instance and running inference though a black-box model for every sampledpoint. “NID” is the runtime for running NID interaction detection. “linear model” is the runtimefor training a linear model (Eq. 1) to get linear attributions with LIME. “interaction model(s)” isthe runtime for sequentially training interaction models (Eq. 3) to get interaction attributions withMahe.
Figure 11: Runtime of Mahe for determining whether an interaction is context-free for a randomlyselected interaction and 40 different contexts, run sequentially. Runtimes for checking interactionconsistency before and after model retraining (fine-tuning) are shown, resulting in tests on 80 con-texts total. DNA-CNN takes longer here because we needed to relax the cutoff criteria of identifyingthe last hierarchical level to find the CACGTG interaction. For context-free experiments, a cutoff pa-tience (§5.1) for Sentiment-LSTM, ResNet152, and Transformer was not needed in our experimentsand is excluded in this runtime analysis. The patience for DNA-CNN was 2.
Figure 12:	Comparisons of Mah´ to baselines for identifying consistent interaction polarity beforemodifying models. Results for explaining Sentiment-LSTM are shown on the same interactionsidentified in Figure 5. The baselines are GLM and GA2M. GLM is a lasso-regularized generalizedlinear model with all pairs of multiplicative interaction terms (Bien et al., 2013), and GA2M is atree-based generalized additive model with pairwise non-additive interactions (Lou et al., 2013).
Figure 13:	Comparisons of Mahe to baselines for identifying consistent negated interaction polarityafter using the same baselines to locally modify Sentiment-LSTM on the interactions from Figure 12.
