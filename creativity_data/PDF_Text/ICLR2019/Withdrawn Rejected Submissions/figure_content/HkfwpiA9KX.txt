Figure 1 : FSA for (a) φ1 = ♦a ∧ ♦b. (b) φ2 = ♦c. (c) φ∧ = φ1 ∧ φ2 .
Figure 2 : Policies for (a) φ1 = ♦a ∧ ♦b. (b) φ2 = ♦c. (c) φ∧ = φ1 ∧ φ2 . The agent moves in a 8 × 10gridworld with 3 labeled regions. The agent has actions [up, down, left, right, stay] where the directional actionsare represented by arrows, stay is represented by the blue dot.
Figure 3 : The FSA for (a):φtraverse = Q(ψr ∧ Q(ψg ∧ Qψb)) b φinterrupt = (ψhand-insight ⇒Qψh) U ψb. The subscripts φι and φ2 are dropped for clarity.
Figure 4 : (a): The upper figure shows our simulation environment in V-REP Rohmer et al. (2013) and thelower shows the corresponding experimental environment. (b) An execution trace of policy πφ∧ where φ∧ =φtraverse ∧ φinterrupt .
Figure 5 : (left) Learning curve of discounted return (discount factor 0.98). The plot shows mean and onestandard deviation calculated over 5 episodes. (right) Mean and standard deviation of episode length (steps)averaged over 5 episodes(a smaller number means faster accomplishment of the task).
