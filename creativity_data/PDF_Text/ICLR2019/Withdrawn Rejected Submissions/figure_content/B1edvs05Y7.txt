Figure 1:	One communication round of DSGD: a) Clients synchronize with the server. b) Clientscompute a weight update independently based on their local data. c) Clients upload their local weightupdates to the server, where they are averaged to produce the new master model.
Figure 2:	Sources of noise in SGD (illustration): Left: Optimization noise, caused by GradientDescent overshooting. Bouncing between the walls of the ravine results in negatively correlated noise.
Figure 3: Validation Error for ResNet32 trained onCIFAR at different levels of temporal and gradientsparsity (the error is color-coded, brighter meanslower error). The prior approaches of GradientDropping and Federated Averaging can be embed-ded in a two-dimensional compression framework.
Figure 4: Step-by-step explanation of techniques used in Sparse Binary Compression: (a) Illustratedis the traversal of the parameter space with regular DSGD (left) and Federated Averaging (right).
Figure 5: Left: Top-1 validation accuracy vs number of epochs. Right: Top-1 validation error vsnumber of transferred bits (log-log). Epochs 30 and 60 at which the learning rate is reduced aremarked in the plot. ResNet50 trained on ImageNet.
Figure 6: Perplexity for different levels of gradient sparsity and temporal sparsity at different stagesof training. WordLSTM trained on PTB.
