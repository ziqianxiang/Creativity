Figure 1: Each column shows variation between two selected flavours of each game. From left toright: Freeway, Hero, Breakout, and Space Invaders.
Figure 2: Performance of an agent that wastrained in the default mode of Freeway andevaluated at every 500, 000 frames in each cor-responding mode. Results are averaged overfive seeds. The y-axis is log scaled.
Figure 3: Performance of an agent thatwas evaluated every 500, 000 frames af-ter being trained in the default flavour ofFREEWAY with dropout and `2 regular-ization. Results are averaged over fiveseeds. The y-axis is log scaled.
Figure 8:	Neural network architecture used by DQN to predict state-action values.
Figure 9:	Performance curves for policy evaluation results. The x-axis is the number of framesbefore we evaluated the -greedy policy from the default flavour on the target flavour. The y-axis isthe cumulative reward the agent incurred.
