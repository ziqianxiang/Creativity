Figure 1: Positive example optimization for L1(θ1, θ2) = θ12 +θ22, L2(θ1, θ2) = (θ1 - 1)2 + (θ2 - 1)2and V(θ1,θ2) = [- θ2+θ2 - 2θι, θ∣+⅛∣ - 2θ2] where the proposed method speeds UP the process(compared on all runs). Each colored trajectory represents one optimization run with random initialposition. Star represents the convergence point. All experiments Use steepest descent method andrUn 600 iterations with a constant step size of 0.01. Convergence time is defined as nUmber of stepsneeded to get below 0.1 loss of L1 (gray region). Color of each point represents its alignment withVLi (green—positive alignment, red—negative alignment, white—directions are perpendicular).
Figure 2: Multi-task learning setup on ImageNet class pairs. (a): gradient cosine similarity is higherfor near pairs and lower for far pairs. (b) and (c): testing accuracy on single task (dotted), naivemulti-task (dashed), and our method (solid). Naive multi-task learning helps in near pairs (see (b))but hurts in far pairs (see (c)) because of its lack of the ability to prevent negative effects from theauxiliary task to the main task. Our method can overcome this limitation by dropping the auxiliarytask when its gradient direction disagrees with the main task, thus achieving the best of both worlds:matching the multi-task performance on near pairs (see (b) where our method and multi-task learninglearn faster than single task only) and the single task performance in far pairs (see (c) where multi-tasklearning performs poorly, but our method automatically recovers single task performance).
Figure 3: Top row: expected learning curves for cross-environment distillation experiments, averagedover 1, 000 partially observable gridworlds. Teacher’s policy is based on Q-Learning, its performancein a new environment (with modified positive rewards) is represented by the top dotted line. Thebottom dotted line represents random policy score. Each column represents a different temperatureapplied to the teacher policy. 0 temperature refers to the original deterministic greedy policy given byQ-Learning. We report five methods: reward using just policy gradient in the new task; distill usingjust distillation cost towards the teacher; add adding the two above; cos using the weighted versionof our method (Algorithm 2); strict cos using the unweighted version of our method (Algorithm 1).
Figure 4: Results on Breakout.
Figure 5: Results on Breakout and Ms. PacMan (averaged over 3 seeds). The two plots to the leftshow performance on Breakout and Ms. PacMan respectively. The third plot shows how the gradientcosine similarity between the two tasks changes during training. The last plot shows an averagescore of the multi-task performance (normalized independently for each game based on the best scoreachieved across all experiments). Our method is able to learn both games without forgetting andachieves the best average performance.
Figure 6: Visualization of the counterexample from Proposition 3, stars denote starting (green) andend (black) points. Dotted and dashed lines correspond to paths A and B respectively. Blue arrowsrepresent gradient vector field of the main loss, while the violet ones the merged vector field.
Figure 7: Illustration of cosine similarity between gradients on synthetic loss surfaces.
Figure 8: Negative example optimization for Lι(θ) = (θι < 0)(θ2 +θ2) + (θι > 0) (l-exp(-2(θ2 +θ22) and L2 (θ) = (θ1 -2)2 + (θ2 - 0.5)2 where the proposed method slows down the process (com-pared on red runs). For the ease of presentation, we choose L1 , which is non-differentiable/smoothwhen θ = 0. But one can create any smooth functions with analogous properties. The core ideais, when there exists a flat region on the loss surface, the auxiliary lost tends to push the iteratesto this region. Even though this move still decreases the loss (i.e., convergence is guaranteed), theoptimization process will be slowed down.
Figure 9: LCA (x-axis) versus FID (y-axis) as a ground truth for class similarity. The measurementsreflect human intuition of class similarity; trimaran and catamaran (bottom-left) are similar bothvisually and conceptually, whereas rock python and traffic light (top-right) are dissimilar both visuallyand conceptually.
Figure 10: Left most: Initial task Tmain , yellow border denotes starting point and violet onesterminating states. Red states are penalizing with the value in the box while the green ones providepositive reward. Middle Left: Solution found by a single run of Q-learning with uniform explorationpolicy. Middle Right: Transformed task Taux . Right most: Solution found by gradient cosinesimilarity driven distillation with policy gradient.
Figure 11: Cosine similarity as a function of dimensionality. On the left, we generate two randomvectors θ1 and θ2 from a Gaussian distribution with zero mean and variance σ2 and as expected, thecosine similarity drops to zero very quickly as the number of dimensions increases. On the right, wemimic a scenario where the true gradients of the main and auxiliary are aligned, however we observeonly corrupted noisy gradients which are noisy copies of the true underlying vector; we generateμ 〜N(0, Id) and generate θ1 〜N(μ, σId) and θ2 〜N(μ, σId). In this case, cosine similarity islarger in higher dimensions (as the inner product of the corruption noise goes to zero).
