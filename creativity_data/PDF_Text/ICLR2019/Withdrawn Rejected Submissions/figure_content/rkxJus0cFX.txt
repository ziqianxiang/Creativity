Figure 1: Performance of fourcommunication-set selection meth-ods under message sizes. Elements inthe data list are generated randomlyfrom a standard uniform distribution.
Figure 2: Left : top-1 val-idation accuracy Vs number ofepochs of training VGG16 on Ci-far10 (4 GPUs, total batch size =256). Center : top-1 validationaccuracy VS number of epochs oftraining ResNet50 on ImageNet(8 GPUs, total batch size = 256).
Figure 3:	Scalability of RedSync for CNNs training on ImageNet using Muradin.
Figure 4:	Scalability of RedSync for LSTM on PTB and Wiki2 datasets. Scalability of RedSync forLSTM VGG16 on Muradin.
Figure 5:	Scalability of RedSync for CNNs with ImageNet and LSTM with PTB on Piz Daint.
Figure 6: The time cost of different parts in RedSync on Piz Daint. Time is the average 10 iterationscost. For each two column group, the left column illustrates time decomposition for RedSync andright column illustrates time decomposition for quantized RedSync.
Figure 7: Communication pattern of sparse synchronization with allgather and dense synchroniza-tion with allreduce.
Figure 8: Two different schemes to overlap communication with computation for CNNs and RNNs.
