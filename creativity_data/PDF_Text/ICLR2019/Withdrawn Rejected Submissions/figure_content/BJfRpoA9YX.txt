Figure 1: (a) Current work (adversarial information factorization) This figure shows our modelwhere the core, shown in blue, is a VAE with information factorization. Note that Eφ is split in two,Ez,φ and Ey,φ, to obtain both a latent encoding, ^, and the label, y, respectively. Dθ is the decoderand Aψ the auxiliary network. The pink blocks show how a GAN architecture may be incorporatedby placing a discriminator, Cχ, after the encoder, Eφ, and training Cχ to classify decoded samplesas “fake” and samples from the dataset as “real”. For simplicity, the KL regularization is not shownin this figure. (b) Previous work: cVAE-GAN (Bao et al., 2017) Architecture most similar to ourown. Note that there is no auxiliary network performing information factorization and a label, y, ispredicted only for the reconstructed image, rather than for the input image (y).
Figure 2: Facial Attribute Classification. We compare the performance of our classifier, Ey,φ, to astate of art classifier (Zhuang et al., 2018).
Figure 3: Reconstructions, ‘Smiling’ and ‘Not Smiling’. The goal here was to reconstruct theface, changing only the desired ‘Smiling’ attribute. This demonstrates how other conditional models(Bao et al., 2017) may fail at the image attribute editing task, when high quality reconstructions arerequired. Both models are trained with the same optimizers and hyper-parameters.
Figure 4: Editing other attributes. We obtain a z, the identity representation, by passing an image,X through the encoder. We append Z with a desired attribute label, y J y, and pass this through thedecoder. We use y = 0 and y= 1 to synthesize samples in each mode of the desired attributeWe have presented the novel IFcVAE-GAN model, and (1) demonstrated that our model learns tofactor attributes from identity, (2) performed an ablation study to highlight the benefits of usingan auxiliary classifier to factorize the representation and (3) shown that our model may be used toachieve competitive scores on a facial attribute classification task. We now discuss this work in thecontext of other related approaches.
Figure 5: Reconstructions, ‘Smiling’ and ‘Not Smiling’, with and without Laux. The goal herewas to reconstruct the face, changing only the desired ‘Smiling’ attribute. This figure demonstratesthe need for the Laux term in our model. Both models are trained with the same optimizers andhyper-parameters.
Figure 6: Reconstructions, ‘Smiling’ and ‘Not Smiling’, without Lgan. This figure demonstratesthat without Lgan , reconstructions are blurred. To achieve sharp reconstructions it is necessary toincorporate Lgan in the loss function.
Figure 7: Facial Attribute Classification. We compare the performance of our classifier, Ey,φ, toa linear classifier trained on latent representations extracted from a trained DIP-VAE (Kumar et al.,2018). DIP-VAE is one of the best models for learning disentangled (, or factored,) representationsfrom unlabelled data.
