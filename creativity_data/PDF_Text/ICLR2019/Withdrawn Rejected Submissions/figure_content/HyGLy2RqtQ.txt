Figure 1: Over-parameterization improves generalization in the XORD problem. The network in Eq. 1 andFig. 4 is trained on data from the XORD problem (see Sec. 3). The figure shows the test error obtained fordifferent number of channels k. The blue curve shows test error when restricting to cases where training errorwas zero. It can be seen that increasing the number of channels improves the generalization performance.
Figure 2: Inductive bias in XORD and OBD problems. (a) Over-parameterization improves generalization inthe OBD problem (b) Pattern detection phenomenon in the XORD problem (c) Pattern detection phenomenonin the OBD problem. In both (b) and (c) we see that as the number of channels increase, gradient descent isbiased towards %0 training error solutions with more detected patterns.
Figure 3: Model compression in MNIST. The plot shows the test error of the small network (4 channels) withstandard training (red), the small network that uses clusters from the large network (blue), and the large network(120 channels) with standard training (green). It can be seen that the large network is effectively compressedwithout losing much accuracy.
Figure 4: Network architecture used for training in the XORD problem.
Figure 5: Higher confidence of hinge-loss results in better performance in the XORD problem.
Figure 6: Demonstration of generalization gap for values not included in Theorem 5.3. The experimentalsetup is the same as in Section 8.2.1 with the following exceptions. For all number of channels we changed thestandard devation σg and only for k = 2 we changed the learning rate η, as described next for each subfigure.
