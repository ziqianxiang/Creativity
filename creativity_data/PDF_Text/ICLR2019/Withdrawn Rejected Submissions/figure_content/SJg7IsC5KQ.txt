Figure 1: Compare of BNGD and GD on OLS model. The results are encoded by four differentcolors: whether εk is close to the optimal step size ε°pt of GD, characterized by the inequality0.8εopt < εk < εopt/0.8, and whether loss OfBNGD is less than the optimal GD. Parameters: H =diag(logspace(0,log10(κ),100)), u is randomly chosen uniformly from the unit sphere in R100, w0is set to H u/kH uk. The GD and BNGD iterations are executed for k = 2000 steps with the samew0. In each image, the range ofεa (x-axis) is 1.99 * logspace(-10,0,41), and the range ofε (y-axis)is logspace(-5,16,43).
Figure 2: Tests of scaling property of the 2CNN+2FC network on MNIST dataset. BN is performedon all layers, and e=1e-3 is added to variance √σ2 + e. All the trainable parameters (except the BNparameters) are randomly initialized by the Glorot scheme, and then multiplied by a same parameterη.
Figure 3: Performance of BNGD and GD method on MNIST (network-(1), 1FC), Fashion MNIST(network-(2), 2CNN+2FC) and CIFAR-10 (2CNN+3FC) datasets. The performance is characterizedby the loss value at ephoch=1. In the BNGD method, both the shared learning rate schemes andseparated learning rate scheme (learning rate lr_a for BN parameters) are given. The values areaveraged over 5 independent runs.
Figure 4: The geometric meaning of the separation propertyCorollary A.16. If lim kwk+1 - wk k = 0, and lim (wkT g)2qk = 0, then either lim (wkT g)2 = 0k→∞	k→∞	k→∞or lim qk = 0.
Figure 5: Plot of 300 random initial tests. H = diag(logspace(0,log10(2000),5)).
Figure 6: Plot of 500 random initial tests. H = diag(linspace(1,2000,100)).
Figure 7: H = diag(linspace(1,2000,d)).
Figure 8: H = diag(linspace(1,cond,100)).
