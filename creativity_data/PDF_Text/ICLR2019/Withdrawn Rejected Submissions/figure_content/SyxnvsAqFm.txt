Figure 1: (a) Quantization Training algorithm (b) Convergence of accuracy using step training(Phase1) for ResNet32 on CIFAR-10 dataset.
Figure 2: (left) Histogram of bit flips for all the weights of ResNet32 for CIFAR-10. (right) weightsflipping their signs over the course of step training. Weight is randomly chosen from layer 30 ofResNet32 for CIFAR-10. Larger learning rate allows for more exploration and flipping of weightswhile small learning rate allows for fine-tuning and final convergence.
Figure 3: Comparing different skewed and default quantization mode (Rastegari et al., 2016) withmultiple tables. Skewed quantization mode converts a 4d tensor W ∈ Rk×c×f×f intoRf×kcf, anddefault quantization mode convert into Rk×cff (where k is number of output features, c is numberof input features, fxf is the filter size). For a kernel of shape 128x128x3x3, skewed mode with 42tables has a memory footprint equivalent to default quantization mode.
Figure 4: Performing node shuffling for a layer in DNN. (a) Node shuffling for a layer in between2 layers is performed by switching the row and columns of the previous and next weight matrix.
Figure 5:	Convergence of weight when training using Step Training. Compares two scenarios where(a) bit flips once and (b) bit does not flips during the course of step training. Step training wasperformed for ResNet32 using CIFAR-10 dataset.
Figure 6:	(a) Shows the convergence of quantization error with decrease in learning rate. Thedistance between consecutive quantization weights (QSS iterations apart) and distance between cor-responding full precision weights is also shown. Distance between consecutive quantization weightsis directly correlated with the number of weight flips (Figure 2). (b) Shows the movement of losswith step training for ResNet32 using CIFAR-10 dataset. The loss rises for the last learning rate asdistortion causes more damage compared what training with the small learning rate can repair.
Figure 7:	(a) Shows the same behavior of histogram of total bit flips for step training with or withoutpre-trained model for ResNet32 with CIFAR-10 dataset. (b) Parametric 1-D plots as described in(Goodfellow et al., 2015; Keskar et al., 2017). Qi denote the weight set after performing quantized-distortion for the ith time while performing step training. FPi correspond to the full precisionweight set just before performing the ith quantized-distortion. The plot is for cross entropy along aline segment containing the two points. Specifically for a ∈ [0, 1], we plot f (aQi + (1 - a)FPi).
Figure 8:	(a) Parametric 1-D plots quantized weight sets at different learning rates while performingstep training. Step training is performed with 160k iterations, with learning rate decayed by 0.1every 40k steps to investigate the effects of 4 different learning rates. All the quantized points aresampled randomly at different iterations at different learning rate. Q1 is obtained at learning rate0.1, Q2 at 0.01, Q3 at 0.001 and Q4 at 0.0001. (b) Starting from these quantized weight sets, themodel is retrained using full precision training method for the remainder of the iterations. Thisretraining gives us FP1, FP2, FP3, FP4. The rise in the loss function along the path between twofull precision points shows the existence of full precision weights in different local minimum.
Figure 9:	Distribution of weights of the WikiText-2 LSTM model for full precision and quantizedtrained model.
