Figure 1: An example value of yi-i+n in Eq.(10). As described in the text, effective values for y.+nthat can contribute to the final BLEU score are the set of n-grams in the reference y*. Here, i = 1,n = 2, and yi,i+n takes value of “i am” which occurs twice in y* (j = 1 and j = 8). The probabilityp(yi,i+n = “i am”) is thus counted twice when enumerating j, and hence in Eq.(10) we divideC(“i am”, y*) to avoid such duplicate count.
Figure 2:	An illustration of decoding with teacher masks. The red lines denote masked steps, forwhich the corresponding one-hot ground-truth token is used for both DEBLEU evaluation and nextstep decoding. For unmasked steps, the output Gumbel-softmax distribution is used as a soft tokenand fed to the next step. Left panel illustrates a mask pattern of 2:2, and the right panel illustrates a4:2 pattern. As the training proceeds, annealing between the patterns is performed. See section 3.3for more details.
Figure 3:	The curves of test-set BLEU score when training on the German-to-English (de-en)and English-to-French (en-fr) datasets, respectively. The starting point (step=0) is the model afterpretraining.
