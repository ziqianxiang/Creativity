Figure 1: Pong. (left) Empirical distributions of the Q function for a single action obtained fromQR-DQN-1 during training for 20 millions of frames. y-axis is the training step number. (right)Standard deviation of the empirical distribution of the Q function for a single action.
Figure 2: Comparison of performance of QUCB and QUCB+ in multi-armed bandits with 10 arms.
Figure 3: Atari 2600 Venture game. Online training curves for QUCB with vanishing schedule andQUCB with constant schedule. Curves are averaged over 3 runs. Shaded area represents corre-sponding standard errors.
Figure 4: Cumulative rewards performance comparison of DQN-QUCB+ and QR-DQN-1. The barsrepresent relative gain/loss of DQN-QUCB+ over QR-DQN-1.
Figure 5: Online learning curves for DQN-QUCB+ and QR-DQN-1 averaged over 3 runs for 40millions of frames. Bands represent standard errors.
