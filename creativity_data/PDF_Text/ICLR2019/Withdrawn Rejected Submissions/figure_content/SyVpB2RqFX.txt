Figure 1: Summarization of relevant work. β-VAE modifies ELBO by increasing the penalty on theKL divergence terms. InfoVAE drops the mutual information terms from ELBO. JointVAE seeksto control the mutual information by pushing the their upper bounds (the associated KL divergenceterms) towards progressively increased values, Cy &Cz. We drop the subscripts θ and φ hereafter.
Figure 2: IMAE on MNIST (a) Illustration of Proposition 1. (b)-(d) Latent traverse on the continu-ous representations Z . The rows are conditioned on the discrete representations y learnt by IMAE,and the initial value of Z for each row is obtained by feeding the encoder with randomly selecteddata corresponds with y. We then manipulate each selected Zk within [-2, 2] while keeping all otherdimensions fixed. (e) & (f) Discrete representations learnt by IMAE with different β values.
Figure 3: Tracking the key quantities for different models by sweeping β for all different methods.
Figure 4: For each image, the first row is the digit type learnt by the model, where each entry isobtained by feeding the decoder with the averaged z values corresponding with the learnt y. Thesecond row is obtained by traversing the ”angle” latent factor within [-2, 2] on digit 6. IMAE iscapable of uncovering the underlying discrete factor over a wide range of β values. More inter-pretable continuous representations can be obtained when the method is capable of learning discreterepresentations, since less overlap between the mainfolds of each category is induced.
Figure 5: Disentanglement comparison on dSprites. The results are reported by training eachmethod with β ∈ [1,10], and we set β = γ∕2 with Y ∈ [1,10] for IMAE. For each β value, everymethod is trained over 8 random initializations. Shade regions indicate the 80% confidence intervals.
Figure 6: Prevent over confidence predictions by encouraging local smoothnessE Disentanglement quality with respect to both continuous anddiscrete representations on 2D shapesSee figure 7.
Figure 7: Disentanglement comparison on dSprites with respect to both y and z. The results arereported by training each method with β ∈ [1,10], and we set β = γ∕2 with Y ∈ [1,10] for IMAE.
Figure 8: JointVAE with different sets of target vlues (Cy, Cz). For each β value, we train JointVAEwith 10 different random seeds. We augment JointVAE with VAT.
