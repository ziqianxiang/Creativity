Figure 1: A visualisation of the graphical models, including both Generative and Discriminativeones, as well as Fully connected and Bottleneck ones. The last character indicates the first node inthe topological order of the graph. The colour encoding is the same as those in experiments.
Figure 2: Visualising detection mechanisms. The scattered dots are training data points, with dif-ferent classes shown in different colours (red for c = 0 and blue for c = 1). Same labels aremanually assigned for inputs when the detection method requires y . Decision regions are shown inthe corresponding colours. Input points in the shaded area are rejected by detection.
Figure 3: Accuracies (column 1), detection rates (columns 2-4) and minimum `inf perturbation(column 6) against white-box zero-knowledge '∞ attacks on MNIST. The higher the better. Thesecond from right most column visualises crafted adversarial examples on an image of digit “7”,with '∞ distortion e growing from 0.1 to 0.5.
Figure 4: Visualising the clean inputs and theCW adversarial examples crafted on GFZ, digitsin red rectangles show significant ambiguity.
Figure 5: Accuracy and detection rates of DBX, GBY, and GBZ against PGD-based perfect-knowledge attack ( = 0.2) on MNIST. The solid area denotes accuracy and the hatched areadenotes detection rate with each considered detector. Zero knowledge attacks are labelled ’-ZK’,other attack labels are described in the main text.
Figure 6: Accuracy and detection rates against distillation-based attacks on MNIST. The higher thebetter. We only present generative classifiers’ results here, for full results see appendix E.
Figure 7: Results on cross-model transfer attacks on MNIST. The horizontal axis corresponds tothe source victim that the adversarial examples are crafted on, and the vertical axis corresponds tothe target victim that the attacks are transferred to. The higher (i.e. the lighter) the better.
Figure 8: Accuracy, detection rates and minimum `inf perturbations against white-box zero-knowledge '∞ attacks on the CIFAR Plane-Vs-frog dataset. The higher the better. The secondfrom right most column visualises crafted adversarial examples on an image of a plane, with '∞distortion ∈ {0.01, 0.02, 0.05, 0.1, 0.2}.
Figure 9:	Accuracy and detection rates against white-box zero-knowledge CW attacks on the CI-FAR plane-vs-frog dataset. The higher the better.
Figure 10:	Accuracy and detection rates against white-box zero-knowledge '∞ attacks on CIFAR-10. The higher the better. Note that results for the DBX classifiers are almost identical.
