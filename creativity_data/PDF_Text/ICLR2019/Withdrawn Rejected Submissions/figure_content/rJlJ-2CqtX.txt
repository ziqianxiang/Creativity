Figure 1: The continuous control environments used in the experiments. Cart-pole swingup (a) andhumanoid stand and walk (b) are from the DM control suite (Tassa et al., 2018). The Minitaur robot(c) is similarly simulated in MuJoCo. The red dot denotes the IMU.
Figure 2: Representative results of the executed policies in the control benchmark tasks. Plots (a), (b)and (c) show the mean and standard deviation as output by the policy trained on cart-pole swingupin the constrained, unconstrained and original reward setting respectively, following a trajectorygenerated using actions sampled from this distribution. In all three cases, we observe high controlinput during the first 2 seconds, corresponding to the swingup phase. Figure (d) shows the controlnorm during the episode rollout of policies trained in humanoid stand. Note that in all cases theactual return between the thee methods is almost identical.
Figure 3: Comparison of policies trained on the humanoid stand task in the constrained, uncon-strained and original reward setup. Figures show the average frame of the final 50% of the episode.
Figure 4: Comparison of a single versus a state-dependent λ multiplier for models trained to achievea minimum velocity of 0.5m/s. A single multiplier results in large swings in reward and on averagehigher values of λ. In b, policies start off at 0m/s and first learn to satisfy the constraint beforeoptimizing the penalty. In c, for the state-dependent case, we show the mean and standard deviationof λ across the training batch.
Figure 5: Comparison of the constrained optimization approach with baselines using a fixed penalty.
