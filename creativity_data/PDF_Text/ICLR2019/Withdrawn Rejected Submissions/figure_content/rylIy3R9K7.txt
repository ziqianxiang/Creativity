Figure 1:	(a) converge and (b) diverge show the effect of step-sizes; (c) is based on the algorithm inSanjabi et al. (2018).
Figure 2:	Real (green) vs generated (red) samples7Under review as a conference paper at ICLR 20194.2	GANs with discriminators linear on featuresWe test the GANs framework with discriminators linear on features (discussed in Section 2.2.2) on theMNIST (LeCun et al., 1998) data of size 28 × 28. The network architecture of the generator is sameas DCGAN (Radford et al., 2015). To get the reasonable basis functions for the discriminator, we firsttrain a Wasserstein GAN model with a subset of the MNIST data for a small number (5k) of iterations.
Figure 3: Generated samples with different number (a) 128, (b) 1024, and (c) 4096 of basis functions.
Figure 4: Effect of stepsizes: the algorithm converge for small stepsize and diverge for large stepsize(c) diverge samples5	ConclusionIn this work, we presented a convergence result for a first-order algorithm on a class of non-convexmax-min optimization problems that arise in many machine learning applications such as generativeadversarial networks and multi-task learning. To the best of our knowledge, this is the firstconvergence result for this type of primal-dual algorithms. Our results allow us to analyze GANswith neural network generator as well as general multi-task non-convex supervised learning problems.
Figure 5: Effect of regularization. Here p is the weight, W is the parameter and λ is the regularizationconstant. The regularizer D is taken to be the Kullback-Leibler divergence.
