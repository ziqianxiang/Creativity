Figure 1: Learning rate sensitivity study in the Random Walk Markov Chain with buffer size n =15000, batch Size b = 16 For simplicity We write the policies as follows: μ = [μ(left∣∙), μ(nght∣).
Figure 2: left and center Buffer size study for the random walk markov chain and four roomsdomain respectively. We select the best settings for each buffersize and report the average RMSVEright Four rooms domain learning rate sensitivities parameter study.
Figure 3: left Markov Chain Random Walk with n = 15000, b = 16, μ = [0.99,0.01], π =[0.01, 0.99], optimal learning rates from above parameter study center Four rooms environmentn = 10000, b = 32 with optimal settings from below parameter studies. We report 70% confidencebands for ease of comparison. right Torcs racing car simulator learning curves of RMSRE calculatedover a pre-collected evaluation set. αBC-IR = 1e-4, αIR = 1e-4, αIS = 1e-6 selected from aparameter study using RMSProp and the NVIDIA network designed for self-driving cars (Bojarskiet al., 2016)walk and four rooms domains we see WIS-Buffer and WIS-Batch perform approximately the sameas ER+IS with IR outperforming the competitors. This can be attributed to IR’s resampling scheme,where we see more samples important to training. This is especially apparent in the four roomsexperiments, where we may only get a few chances to train from certain hard to reach states. Theuniform sampling methods will more likely miss out on rare examples, or only see them once makinglearning slow. We also note that WIS-Batch is unable to learn in the hardest of settings for theMarkov chain, potentially due to the bias incurred from only performing WIS on a subsample ofthe entire data. One interesting observation is in the Baird’s Star Problem where IR results in betterweights and lower value error (see appendix).
Figure 4: Markov Chain results for V-Trace and Sarsa. Four clipping parameters were chosenwith different amounts of aggressiveness. They were calculated from the known max importancesampling value, multiplied by the scalars 0.5 and 0.9. We also included a clipping value of 1.0which is consistent with the recommendations made for retrace. The error for Sarsa was calculatedby deriving the state-value function from the current action-state value function, and following thesame error calculation as with the other methods.
Figure 6: Four rooms experiments for new random states every run: left Learning rate sensitivitycenter Buffer Size Sensitivity right Learning CurvesB.4	Mountain CarWe use the standard mountain car domain described in (Sutton and Barto, 2018). To make thesimulations more realistic we collect experience from behavior policy learned through Q-learning(Sutton and Barto, 2018) with a -greedy exploration strategy. We code a GVF to predict whetherthe agent will hit the back wall within a horizon of γ = 0.9 while following the the persistent policyaccelerating forward. We use two exploration parameters = {0.1, 0.5} with maximum importancesampling ratio values at ρmax = {6, 30} respectively.
Figure 7: Mountain Car: Mini-batch Gradient Descent with constant learning rate π[0, 0, 1.0], |B| = 50000, Left = 0.1, Right = 0.5B.5	Sampling according to intermediate policiesWhile intuitively it might seem sampling according to the target policy will produce the best valuefunctions, when making multiple predictions with the same training network it would be more con-venient to sample from the experience replay buffer with an intermediate policy. We get the benefitsof less variant importance sampling ratios with the ability to use one IR buffer for multiple policies.
Figure 8: Mountain Car: Mini-batch Gradient Descent with Adam Optimizer. x-axis: learningrates, y-axis: RMSVE π = [0, 0, 1.0], |B| = 50000, Left = 0.1, Right = 0.5Figure 9: Mountain Car left Learning curve for optimal settings for Adam optimizer x-axis:time(104) y-axis: RMSVE center Early example of learned predictions x-axis: Time y-axis: Pre-diction right Late example of learned predictions x-axis: Time, y-axis: PredictionTo combine the IR and importance sampling with a uniform experience replay we sample from thebuffer according to the PMFρi	πsample (at |P(SamPling di) = PP,	Pi = ʒRRWe would then us off-Policy temPoral difference methods with the effective imPortance samPlingratio used as Peffective = ∏ 冗(：：(二‘|§,). We show initial results for this extension in the markov chainrandom walk in figure 10.
Figure 9: Mountain Car left Learning curve for optimal settings for Adam optimizer x-axis:time(104) y-axis: RMSVE center Early example of learned predictions x-axis: Time y-axis: Pre-diction right Late example of learned predictions x-axis: Time, y-axis: PredictionTo combine the IR and importance sampling with a uniform experience replay we sample from thebuffer according to the PMFρi	πsample (at |P(SamPling di) = PP,	Pi = ʒRRWe would then us off-Policy temPoral difference methods with the effective imPortance samPlingratio used as Peffective = ∏ 冗(：：(二‘|§,). We show initial results for this extension in the markov chainrandom walk in figure 10.
Figure 10: Intermediate sampling policies in a random walk markov chain. (left) π = [0.1,0.9], μ[0.9,0.1], (right) π = [0.01,0.99], μ = [0.99,0.01]C More Theoretical ResultsC.1 Consistency of the IR estimator with growing buffer sizeWe show the consistency of the IR estimator with n → ∞ for convenience, but our approach closelyfollows that of (Smith et al., 1992).
