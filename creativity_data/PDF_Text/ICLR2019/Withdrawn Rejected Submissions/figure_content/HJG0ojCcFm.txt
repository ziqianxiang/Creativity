Figure 1: An overview of our methodology. We examine a team formation task defined by anunderlying cooperative game. Applying different negotiation protocols to the same underlying taskgenerates different environments (Markov games). Instead of hand-crafting negotiation bots to eachsuch environment, We train independent RL negotiation agents. We compare the agreements RLagents arrive at to game theoretic solutions to the underlying cooperative game.
Figure 2: The Team Patches environment for a game with threshold q = 15 and a total reward ofr = 7. Agents are represented by the squares containing their weights. In this example, we haven = 5 agents, with the weights v = [5, 6, 7, 8, 9], and three patches where agents can form teams(red (left), green (top), and blue (right)). (a) At the start of an episode, the agents randomly begin inthe center of the map with no assigned team or demands. (b) At the end of an episode, the agentshave moved inside the grid-world to each of the patches. Agents in red (7 and 8) form a viable teamwith a valid reward allocation as their weights are above the required threshold (7 + 8 ≥ 15) andtheir demands are equal to the availability (3 + 4 ≤ 7). The team in green is not viable as the totalweight is not sufficient (5 15), and the blue team has an invalid reward allocation as their demandsare higher than the availability (4 + 4	7). Agents 7 and 8 receive 3 and 4 reward respectively.
Figure 3: Share of reward received by agents in both environments. Shapley points (left) and cor-responding density-estimation contours (right). The x-axis is the fair share prediction using theShapley value, and the y-axis is the empirical reward share of the RL co-trained agents. We includey = x (black dashed) to show the reward our agents would receive if they exactly matched thepredictions, as well as a trend line (in red).
Figure 4: Influence of spatial changes (per-turbed, blue) versus no changes (unperturbed,red) on the highest-weighted agent’s reward.
Figure 5: Prediction of the Shapley values fromthe game weights and threshold using super-vised learning.
Figure 6: Team Patches - (a) Example RGB observation an agent receives in an episode. The whitesquare is the observing agent (centered). Other agents are observed as maroon, cyan, purple, andnavy, patches as red, green, and blue, and the environment’s border is observed as gray. In additionto these observations, the agent also receives its index as a one-hot vector of size number of players,the weights of all agents, and their current demands. (b) Visualization of spatial perturbations wherethe agent with the highest weight is initialized at 0 to N squares from the nearest patch.
Figure 7: Shapley Correspondance on Boards With Reduced VarianceF	Letting Agents Observe their Negotiation PositionIn Section 4.4 we discuss potential reasons for the empirical gains of our RL agents deviating fromthe Shapley value. The analysis there shows that given a direct supervision signal (boards labeledwith the Shapley values), a small neural net can approximate the Shapley value well. Our RL agentshave a more challenging task for two reasons: (1) They have to take into account not only theirnegotiating position but also protocol details. (2) Their RL supervision is weaker: they only knowhow successful a whole sequence of actions was, and not the “correct” action they should have takenat every timestep.
Figure 8: Shapley Aware Propose AcceptFigure 8 shows the same pattern as Figure 3a. The deviation from game theoretic predictions thusstems not from being unable to identify the “high-level” negotiation position of agents, but ratherfrom the RL procedure we applied; RL agents attempt to maximize their own share of the gains(rather than for obtaining their “fair” share of the gains, as captured by the Shapley value), and areforced to deal with a the need to find strong policies taking into account the specific negotiationprotocol (under a non-stationary environment, where their peers constantly adapt their behavior).
