Figure 1: (L) An information Venn diagram: three disks represent the entropy of X, Y, Yb respec-tively, the area in red is the relevant information I(Y ; Y ), the area in grey is the irrelevant informationI(X; Y ) - I(Y ; Y ). The optimal solution is obtained when the smaller disks coincide, which is typ-ically not achieved in practice. In particular, the trained model may be extremely confident in itsprediction (when H(Y ) lies inside of H(X)), but predicts the wrong label (having large grey area).
Figure 2: Statistics plots for feature entries of digit 9. Recall that the features F (X) are vectors ofdimension 100 and w is a matrix of size 100 × 10. The horizontal coordinate represents the indexof entries of F(X) ranging from 0 to 99, the vertical coordinate represents entry values of the 10thcolumn of w, the values of mean and standard deviation for each entry of features from samples ofdigit 9. For RegInvNet, w10 only assigns large value to representative entries and is more robustagainst perturbations in other irrelevant entries.
Figure 3: The macro/micro average ROC curves for representative entries of features genearatedby original and regularized model. The entries of features learned under our regularization stronglyindicate the categories of the digits.
Figure 4: Compression of the RegResNet-32 (Blue) and the original ResNet-32 (Orange) on CIFAR-10 over the training process: the first plot records the average `2 norm of the last layer features F(X)in a batch; the second plot records the average `2 norm of the columns ofw; the third plot records theratio of zero entries among all entries of w ; the plots for trained w with/without regularization after84000 steps are listed on the right. Best test accuracy are 92.86% and 92.64% for regularized andoriginal ResNet-32 respectively. Under our regularization, the norm of the feature learned remainssimilar and the norm of classifier w is smaller. Therefore w is less sensitive to ”support” and ”outlier”features.
Figure 5: We measure the operator norm of L in each building block of ResNet-32 over 80k trainingsteps. It can be observed that, the operator norms are all bounded by 1 throughout the trainingprocess, which verifies the hypothesis made in Appendix D. We conlude that ResNet is invertible.
Figure 6: A reproduction of the feature statistical results of InvNet on MNIST for all digits---feature mean—weight+ product of feature and weightThis is a reproduction of the feature statistical results of InvNet on MNIST for all digits for Figure 2.
Figure 7: Reproduction of Experimental Results in Section 4.2 on i-RevNet. The plots in the firstrow are for i-RevNet and the plots in the second row are for Reg-i-RevNet.
Figure 8: The histogram for values of feature entries of Digit 920Under review as a conference paper at ICLR 2019The histogram for values of feature entries of Digit 9 has a decaying shape but with a heavier tailcompared to that of Gaussian with small variance. The spasity of w depends on our choice ofhyperparameters. For example in Figure 4 we measure the sparsity of the learned w of RegResNetfor CIFAR10: about 60% entries of w are precisely zero.
