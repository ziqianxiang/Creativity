Figure 1: High level overview of 1D-CNN encoder-decoder system2	Related works2.1	Architecture SearchThere have been recent works in architecture search such as Zoph & Le (2016), Baker et al. (2016),Miikkulainen et al. (2017), Real et al. (2017), that use reinforcement learning or evolutionaryalgorithms to traverse the space of architectures. These approaches characterize architectures asdiscrete entities in a non-differentiable space and use either a constrained state space or heuristics todiscover architectures. These approaches have been subject to criticism from the machine learningcommunity for using large amounts of computation.
Figure 2: Structure of architecture compression spaces with forward (blue) and backward (red)mappings visualized3.	Pooling layers reduce dimensions of input image by some factorLemma 3.1. For a spatial input of dimension d0, the number of pooling layers possible in a validarchitecture is finite.
Figure 3: Joint training loss on (from top-bottom) FMNIST, SVHN, CIFAR-10, CIFAR-100In this section, we empirically evaluate our approach, showing that it is capable of compressingnetworks by upwards of 10x by benchmarking them on modern classification tasks such as CIFAR-10,CIFAR-100, FMNIST and SVHN.We show that our approach outperforms current state of the artarchitecture compression techniques by comparing the results to several baselines.
Figure 4: First 100 layer 1 filters for encoder (left) and decoder (right). Each column represents aninput channel and rows represent filters.
