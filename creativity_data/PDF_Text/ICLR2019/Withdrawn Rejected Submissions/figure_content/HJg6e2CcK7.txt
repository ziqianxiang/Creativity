Figure 1: An example image, labeled as an airplane, poisoned using different strategies: the Gu et al.
Figure 2: Left: After training a model on a small, clean dataset, We examine the training examples thatwere assigned the lowest probability on their labels. The 300 lowest label probability training samplescontain over 20 of the 100 poisoned samples. Right: The Gu et al. (2017) attack, but restricted toonly clean labels (only images from the target class are poisoned). The attack is ineffective; even at25% poisoning, only one class exceeds 50% attack success. The attack success rate is defined as thepercentage of test examples not labeled as the target that are classified as the target class When thebackdoor pattern is applied.
Figure 3: GAN-based interpolation from a frog to a horse. Natural images of a frog and horse areshown on the far left and right, respectively. Interpolated images are shown in between, where τ isthe degree of interpolation from one class to the next. τ = 0.0 and 1.0 represent the best possiblereproduction of the original frog and horse, respectively.
Figure 4: An image of an airplane converted into adversarial examples of different maximal perturba-tions (ε). Left: the original image (i.e. ε = 0). Top row: '2-bounded with ε = 300,600,1200 (left toright). Bottom row: '∞ norm-bounded with ε = 8,16, 32 (left to right).
Figure 5: Comparing attack performance with varying magnitude. Left: Varying degrees of GAN-based interpolation for the deer class. Interpolation for T < 0.2 has similar performance to thebaseline. T ≥ 0.2 has substantially improved performance at6 % poisoning. Right: Attacks usingadversarial perturbations resulted in substantially improved performance on the airplane class relativeto the baseline, with performance improving as ε increases. Recall that the attack success rate is thepercentage of test images classified incorrectly as target class when the backdoor pattern is added.
Figure 6: Attack performance on all classes. Left: The T = 0.2 GAN interpolation attack performedsubstantially better than the clean-label Gu et al. (2017) baseline (Figure 2), especially for the 1.5%and 6% poisoning percentages. Right: The `2 norm-bounded attack with ε = 600 resulted in highattack success rates on all classes when poisoning a 1.5% or greater proportion of the target labeldata. Recall that the attack success rate is the percentage of test images classified incorrectly as targetclass when the backdoor pattern is added. A per-class comparison can be found in Appendix C.2.
Figure 7: Reproducing the Gu et al. (2017) attack on CIFAR-10. The attack is Very effective. Abackdoor is injected with just 75 (0.15%) training examples poisoned.
Figure 8: Left: Reducing the backdoor pattern's amplitude (to 16, 32 and 64) still results in successfulpoisoning when poisoning 6% or more of the dog class. Right: Poisoning using a maximum backdoorpattern amplitude of 32 was successful on all classes for poisoning proportions of 6% or greater.
Figure 9: Lower backdoor pattern amplitudes render the backdoor pattern much less noticeable. Here,an image of a dog is poisoned with '2-bounded adversarial perturbations (ε = 600) and varyingbackdoor pattern amplitudes. From left to right: backdoor pattern amplitudes of 0 (no backdoorpattern), 16, 32, 64, and 255 (maximal backdoor pattern).
Figure 10: Left: An example image of the cat class after application of the four-corner backdoorpattern (at amplitude 32). Right: Using the four-corner pattern does not provide a substantial benefitover the one-corner pattern when data augmentation is not used. When data augmentation is used,however, the difference in performance is stark, with the one-corner pattern achieving much lowerattack success rates.
Figure 11: Left: the one-corner reduced amplitude pattern usually fails to poison the network Whendata augmentation is performed. Right: The four-corner reduced amplitude pattern, on the other hand,successfully poisons the network for the majority of classes.
Figure 12: Using Gaussian noise to increase classification difficulty results in some improvementwhen the standard deviation of the noise is low. At higher standard deviations, the performancereduces dramatically.
