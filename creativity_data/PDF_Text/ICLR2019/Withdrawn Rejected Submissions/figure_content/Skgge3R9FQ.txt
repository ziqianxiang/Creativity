Figure 1: Illustration of the influence of different sets of dustbin training samples on the over-generalized regions. Two-moon classification dataset: (a) naive MLP trained only with in-distributionsamples, (b-d) augmented MLPs trained with different out-distribution sets as dustbin. The MLP ismade of three layers and ReLU activation functions.
Figure 2:	Misclassification distribution over original classes: (a) CIFAR-1001 Vs SVHN provided bya naive VGG trained on CIFAR-10; (b) DS-ImageNett VS LSUN provided by a naive ReSnet-164trained on CIFAR-100.
Figure 3:	Interpolated samples for CIFAR10 and MNIST. Third row for every dataset represents theinterpolated samples that are composed of images from first (source) and second rows (target).
Figure 4: Visualization of data distribution in last convolution layer (i.e., feature space) of anaugmented CNN trained on CIFAR-10 and CIFAR-100 as in-distribution and out-distribution sets,respectively. For visualization purposes, these feature spaces are reduced to 3D using PCA. For moreresults, refer to Fig 7 of the Appendix.
Figure 5: Church window plots for various data instances. Black dot corresponds to the clean sampleposition.
Figure 6: Robustness of naive CNNs (LeNet for MNIST (left) and VGG-16 for CIFAR-10 (right)) andtheir augmented counterparts under different adversarial attack algorithms. Robustness to white-boxattacks is measured by the percentage of visiting fooling classes and dustbin class by moving inadversarial directions.
Figure 7: Visualization of some randomly selected test samples and their corresponding adversaries(FGS and T-FGS) in the feature spaces (the penultimate layer) learned by a naive CNN and anaugmented CNN. To produce and manipulate the 3D plots refer to https://github.com/mahdaneh/Out-distribution-learning_FSvisulization(TPR) 91% and 95% for CIFAR-10 and CIFAR-100, respectively. As it can be seen from Table 5, the error ratesof our augmented CNN on most of the various types of black-box adversaries are lower than ODIN due to itshigher adversaries rejection (classifying as dustbin) rates. For example, our method on I-FGS adversaries ofCIFAR-100, which are highly transferable (it can be perceived from naive ResNet-164â€™s low accuracy on I-FGS(i.e. 22.20% from Table 1)), has error rate 37.45% as it rejects 45.75% of this I-FGS adversaries, while ODINrejects only 5.75% of adversaries, resulting to error rate 73.7%.
