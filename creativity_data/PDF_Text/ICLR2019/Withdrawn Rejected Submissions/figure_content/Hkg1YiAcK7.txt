Figure 1: Suppose the data distribution is a mixture of Gaussian (MoG), i.e., p(x) = 0.5N(—3,1) +0.5N (3, 1), and the model distribution is a MoG with learnable means a and b, i.e., pG(x) =0.5N (a, 1) + 0.5N (b, 1). The figure demonstrate the contour of the two divergences with the x-axisand y-axis representing the value of a and b respectively. The JS-divergence allows a collapsed localoptima with a = —3.25, b = —2.69 (numerical results).
Figure 2: Density plots of the true distributions and the generator distributions of different methodstrained on the ring data (Top) and the grid data (Bottom).
Figure 3: Three different metrics evaluated on the generator distributions of different methodstrained on the ring data (Top) and the grid data (Bottom). The metrics from left to right are: Numberof modes covered (the higher the better); Averaged intra-mode KL divergence (the lower the better);Percentage of high quality samples (the higher the better).
Figure 4: Generated samples of DCGANs and LBT-GANs with different size of discriminators.
Figure 5: Generated samples on CIFAR10 (a) and CelebA (b) of DCGANs and LBT-GANs.
Figure 6:	Sensitivity analysis of the unrolling steps K and the inner update iterations M.
Figure 7:	The training process and learned distribution of GAN and LBT-GAN for a simple toyexample.
Figure 8:	The generated samples of LBT-GAN with a smaller VAE (left) and the samples from thesmaller VAE (right).
