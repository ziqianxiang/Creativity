Figure 1: (a) A simple network architecture; the output evaluated using Eq. (1) is x5(z1 , z2) =f5(w53f3(w31z1 +w32z2 + b3) +w54f4(w41z1 +w42z2 + b4) + b5). (b) Highlight of the structureof neuron 4 (encircled in the dashed line) of (a): The activation function f4 of the neuron is computedas an expansion over the training set. Each neuron 4j, j = 1, . . . , N in the figure corresponds to theterm g(a4 - aj4) in Eq. (4).
Figure 2: XOR. The plots show the activation functions learned by the simplest KBDN whichconsists of one unit only for the 2-dim (2a) and 4-dim (2b) XOR. The first/second row refer toexperiments with without/with regularization, whereas the three columns correspond with the chosennumber of point for the expansion of the Green function d = 50, 100, 300.
Figure 3: Charging Problem. The plot shows the accuracy obtained by recurrent nets with classicsigmoidal unit, LSTM cell, and KB unit. The horizontal axis is in logarithmic scale.
Figure 4: Capturing Long-Term dependencies. Number of successful trials and average numberof iterations for a classification problem when the ∨, ∧,㊉ and ≡ functions are used to determine thetarget, given the first two discriminant bits.
Figure 5: Activation functions. The 20 activation functions corresponding to the problem ofcapturing long-term dependencies in sequences that are only discriminated by the first two bit (≡function). All functions are plotted in the interval [-4, 4]. The functions with a dashed frame are theones for which |f0| > 1 in some subset of [-4, 4].
Figure 6: Capturing Long-Term dependencies. Number of successful trials and average numberof iterations when facing the ≡ problem with sequences of length ranging from 5 to 200, when thedistinguishing information is located in the first two bits.
