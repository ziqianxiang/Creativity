Figure 1: Illustration of the agentâ€™s network architecture. This diagram shows DRC(2,3) for twotime steps. Square boxes denote ConvLSTM modules and the rectangle box represents an MLP.
Figure 2:	Examples of Sokoban levels from the (a) unfiltered, (b) medium test sets, and from the (c)hard set. Our best model is able to solve all three levels.
Figure 3:	a) Learning curves for various configurations of DRC in Sokoban-Unfiltered. b) Compar-ison with other network architectures tuned for Sokoban. Results are on test-set levels.
Figure 4: Forcing extra computation stepsafter training improves the performance ofDRC on Sokoban-Medium set (5 networks,each tested on the same 5000 levels). Stepsare performed by overriding the policy withno-op actions at the start of an episode.
Figure 5: Comparison of DRC(3,3) (Top, Large network) and DRC(1,1) (Bottom, Small network)when trained with RL on various train set sizes (subsets of the Sokoban-unfiltered training set). Leftcolumn shows the performance on levels from the corresponding train set, right column shows theperformance on the test set (the same set across these experiments).
Figure 6: (a) Generalization results from a trained model on different training set size (Large,Medium and Small) for Sokoban. Left figure shows results on the unfiltered test set, right figureshows results on the medium test set. (b) Similar generalization results for trained models in Box-world.
Figure 7: Sample observations for each of the planning environments at the beginning of an episode.
Figure 8: (a) Food in dark blue offers a reward of 1. After it is eaten, the food disappears, leaving ablack space. Power pills in light blue offer a reward of 2. The player is in green, and ghosts in red.
Figure 9: Learning curves in Sokoban for DRC architectures tested on the medium-difficulty testset. The dashed curve shows DRC(3,3) trained on the easier unfiltered dataset. The solid curvesshow DRC(3,3) and DRC(9,1) trained directly on the medium-difficulty train set.
Figure 10: Learning curves comparing the DRC(3,3) and DRC(1,1) network configurations in 5Atari 2600 games. Results are averaged over two independent runs. We also provide ApeX-DQN(no-op regime) results from Horgan et al. (2018) as a reference.
Figure 11: Ablation studies on the performance of our baseline DRC(3,3) agent on Sokoban. Allcurves show test-set performance. a) We replace the ConvLSTM for simpler memory modules. b)We remove our extra implementation details, namely pool-and-inject, the vision shortcut, and thetop-down skip connection, from the model.
Figure 12: Generalization performance on BoxWorld when model is trained on different datasetsizesFigure 13: (a) Extrapolation to larger number of boxes than those seen in training. The model wastrained on 4 boxes. (b) Generalization gap computed as the difference between performance (in %of level solved) on train set against test set. Smaller bars correspond to better generalization.
Figure 13: (a) Extrapolation to larger number of boxes than those seen in training. The model wastrained on 4 boxes. (b) Generalization gap computed as the difference between performance (in %of level solved) on train set against test set. Smaller bars correspond to better generalization.
Figure 14: Performance curves comparing DRC(3,3) against baseline architectures on (a) Gridworld(32x32) and (b) Sokoban. The Sokoban curves show the test-set performance. For VIN we averagethe top five performing curves from the parameter sweep on each domain. For the other architec-tures we follow the approach described in Appendix I, averaging not the top five curves but fiveindependent replicas for fixed hyperparameters.
Figure 15: Performance curves comparing DRC(3,3) and DRC(1,1) against the best ResNet ar-chitectures we found for (a) MiniPacman and (b) Boxworld. DRC(3,3) reaches nearly 100% onBoxworld at 1e9 steps.
