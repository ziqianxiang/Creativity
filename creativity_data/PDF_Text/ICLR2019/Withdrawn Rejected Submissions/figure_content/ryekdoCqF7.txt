Figure 1: Different cases of the distribution projection on Π,s convex hullIn case 1, one can find the oPtimal distribution in Π to aPProximate real data set in CONV Π. Butin case 2, using standard GANs with a single generator, one can only find the distribution in Π1Under review as a conference paper at ICLR 2019that is nearest to the projection. It then makes sense to train multiple generators and use a convexcombination of them to better approximate the data distribution (than using a single generator in thenon-convex case (see figure 1(c))).
Figure 2: A framework for training GN+1Based on the former definition, we obtain V(GN +1) = R ω +>[一必常1'；+1+，.。*+1+*] dx, where,	N+1 x	(ΦN+1)2preal	,A = 2(φN +1 ∙ Preal - φN ∙ Ppre) and B = (φN +1 + φN)ppre - 2φN +1 ∙ Ppre ∙ Preal.
Figure 3: We train 4 generators to catch the modes(i.e., 8 Gaussian distributions).
Figure 4: Wasserstein distance between prealand pnow as the number of generators increases.
Figure 5: The mean Iog(DX(Preal∣∣Pnow)) of Figure 6: Theminimum Iog(DX(Preal||pn°w))30 repetition experiments .	of 30 repetition experiments .
Figure 7: W-distance between mixture distribu-tion and the real data.
Figure 8: W-distance between distributionGi (z, θ) and the real data .
