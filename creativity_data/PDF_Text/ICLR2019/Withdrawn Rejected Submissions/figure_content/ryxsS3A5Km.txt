Figure 1: Illustration of different approaches for continuous learning. a) All but the task specificlayer are shared, catastrophic forgetting is countered by techniques that prevents parameters to moveto lower energy regions of previous tasks. b) Each task will add some fixed task specific parameters,all layers’ original weights are not tuned, and thus prevents forgetting. c) Our approach, wherenetwork structure is determined by architecture search. In this example, the search results decideto “reuse” the first two layer, do “adaptation” for the third layer and allocate “new” weight for thefourth layer.
Figure 2: Illustration of the training pipeline of our framework. a) Current state of super model. Inthis example, the 1st and 3rd layers have single copy of weight, while the 2nd and 4th has two andthree respectively. b) During search, each copy of weight for each layer will have a “reuse” and an“adaptation” options plus a “new” option, thus totally 2|Sl | + 1 choices. α is the weight parametersfor the architecture. c) Parameter optimization with selected architecture on the current task k. d)Update super model to add the newly created S30 .
Figure 3: Results on permutated MNIST dataset. a) The base network structure with 3 fully-connected layers and 1 output layer. b) The searched architecture results from running 10 permutatedMNIST sequentially. The first layer always choose to “new”, while other layers choose to “reuse”.
Figure 5: Visualization of searched architecturewith learning two tasks sequentially. The searchare based on the super model obtained by train-ing ImageNet as first task. (a) and (b) showssearched architecture on CIFAR100 and Om-niglot task respectively.
Figure 6: Comparison catastrophic forgettingeffect between our proposed approach and base-line on visual decathlon dataset.
Figure 4: Distance between the tuned parameter at each taskand the parameter of the very first task on VDD dataset. a)First layer parameter distance, and b) Last layer parameterdistance. Baseline indicates the result from tuning all layersusing SGD.
Figure 7: Comparative performance on a) permuted MNIST and b) split CIFAR-100 dataset. Meth-ods include Kirkpatrick et al. (2017, EWC), Lee et al. (2017b, IMM), Fernando et al. (2017, PathNet(PN)), RUsU et al. (2016, Progressive Net (PG)), Serra et al. (2018, HAT), Lee et al.(2017b, DEN),Nguyen et al. (2018, VCL), ours (w/o reg) denotes the case where finetuning for current tasks isdone withoUt Using any regUlarization to prevent forgetting, and oUs represents the case where the`2 regUlarization is Used.
