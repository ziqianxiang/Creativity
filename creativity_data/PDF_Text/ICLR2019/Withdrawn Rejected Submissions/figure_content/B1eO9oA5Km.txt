Figure 1: The primary idea of our proposal (right) and dual learning (left)Encoder'shiddenDecoder'shiddenSameinformationspace(OUr proposal)guide the training of NMT.1 Note that GN has a flexible structure, which can reconstruct the inputsource sentence not only from the encoder but also from the decoders of NMT systems for differentlanguages. In that case, as shown in the right panel in Figure 1, GN keeps the hidden states of encoderand decoder consistent and forces the NMT system to map them to the same information space. Wellmatched encoder and decoder will bring a better word alignment and yield better translation as wewill see in Section 3. And GN thus offers a better updating direction for the NMT model in the duallearning process. Further, benefiting from the flexible structure of GN, we expand GLF to addressmultilingual translation completed by guiding all the decoders of different languages at the meantimethrough GN. Ensemble output of different languages can be regard as a normalization operation thatcan accelerate deep network training according to Ioffe & Szegedy (2015).
Figure 2: Experimental observationof the bi-direction attention entropy.
Figure 3: Overview of GLF and GNNote that BDE doesnâ€™t consider the word orders in translation, thus we need to use language modelsto ensure a fluent translation. Based on the properties of information space, we develop a novelguided learning framework called multi-dual learning framework to use both monolingual data andthird party resources to improve the performance of neural machine translation (see Section 3.2).
Figure 4: Accuracy increasing comparison. f denotes With RSM. one unit of X axis denotes 1750steps. (a),(b) are from trilingual translation While (c),(d) are from loW resource translation in Whichonly 10k data for fr - es.
