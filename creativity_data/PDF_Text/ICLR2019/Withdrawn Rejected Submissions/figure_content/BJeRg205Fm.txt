Figure 1: The choice of activation function in the output layer of a neural network affects the shape ofthe objective function. To stabilize learning in a Dirichlet output layer, we propose that the activationfunction f should approach 0 asymptotically as X → -∞, and that ∂χψ(f (x)) = ψ(f(χ))f0(χ)should be bounded. Two such functions are the Inverse-Linear (left) and Exponential-Linear (right)piecewise functions.
Figure 2: Example evolution of the probability density function for a simple neural network with aBeta output, trained to optimize the log-likelihood of a single data point with y = 0.1. The densityfunctions are given by Pθ1,θ2(y) = yɑ-1(1 - y)β-1∕B(α, β) where α = EL(θι) and β = EL(O2).
Figure 3: Left: Example prediction from the XENON1T network with a Beta output layer. Thetarget is at 0.95, and the Beta distribution predicted by the network has mean 0.95 and mode 0.96.
Figure 4: Validation set performance on the CIFAR-100 distillation task using a softmax outputlayer with categorical cross-entropy loss, a Dirichlet output layer as parameterized in Equation 1,or a Dirichlet output layer as parameterized in Equation 7. Rather than compare the NLL objective,we compare the more intuitive MSE loss, using the mean of the Dirichlet distributions as pointestimates. Each network was also trained using an MSE objective (using the mean of the Dirichletas a point-estimate); this had little effect on the softmax output, but hurt the final performance of theDirichlet outputs.
Figure 5: A deep Dirichlet-multinomial autoencoder was used to learn a two-dimensional embed-ding of simulated samples from 100-dimensional multinomials. The 10 different clusters are readilyapparent in the embedding of the validation set examples. The samples shown are colored by theirtrue cluster identity.
