Figure 1: Hierarchical Vanilla Attention Mechanismreference but different from the existing Multi-Level Attention Mechanisms. Our Ham-V focuson all the intermediate attention results rather than just the result of the last attention level. Asshown in Figure1, given the query and the input sequence which consists of n keys, we calculate theVanilla Attention Mechanism result of them and get Query 1. And then we continue to calculatethe attention result of Query 1 and the keys and get Query 2. Repeat this calculation d times. Thus,we form a d-depth attention. Finally, the output of our Ham-V is the weighted sum of the above dattention results, where the d weights are the softmax values of d trainable parameters. The softmaxis used to convert these d weights into the probabilities. These weights can tell us the relativeimportance of the d intermediate attention results Query i. In other words, the relative importance ofthe d attention levels.
Figure 2: Hierarchical Self Attention Mechanismweighted linear combination of these d intermediate attention results, our model takes every level offeatures into consideration.
Figure 3: Three quatrains generated by 10-level Ham PPG model7	Concluding RemarksIn this paper we have developed Hierarchical Attention Mechanism (Ham). So far as we know,This is the first attention mechanism which introduces hierarchical mechanisms into attention mecha-nisms and takes the weighted sum of different attention levels as the output, so it combines low-levelfeatures and high-level features of input sequences to output a more suitable intermediate result fordecoders.
