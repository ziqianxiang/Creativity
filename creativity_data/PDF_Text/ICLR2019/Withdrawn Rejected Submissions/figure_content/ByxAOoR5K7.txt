Figure 1: Top left: A gridworld environment where an agent must learn to navigate from a startinglocation (green asterisk) with the goal of reaching a target location indicated by the red flag. Crossingeach open square has a reward of -1; colliding with walls (black squares) has a reward of -10. Theoptimal deterministic policy is shown for each state. Top right: Each point along the curve representsan optimal policy that achieves the maximal expected value at a given rate of information. Bottom: 3policies illustrated at different points along the information rate distortion tradeoff curve. Colors inthe plots illustrate the entropy of the policy in each state of the maze.
Figure 2: A) Average accumulated reward across 100 training episodes. Horizontal red line showsthe performance of the standard (tabular) Actor-Critic algorithm. Black plot markers show theperformance of CL-AC as a function of the information rate of the policy channel. B) Expected valueof the policy learned at the end of 100 episodes, compared to the optimal policy for the task, as afunction of the information rate of the policy. C) Comparison of the entropy of the policies learnedby AC and CL-AC after one episode. Both algorithms were constrained to take the same sequence ofactions and observed the same rewards. The learned policies are illustrated by line segments in eachstate that indicate the probability of taking each of the four actions (north, south, east, west).
Figure 3: Average probability of occupying each state for learned policies using three differentinformation rate constraints, averaged over 1,000 episodes.
Figure 4: Left: Agents were trained on a randomly constructed maze, and evaluated on a mazewith additional walls randomly placed. Right: Average generalization performance for CL-AC, as afunction of the trade-off parameter Î². Generalization performance of the standard AC algorithm isshown by the horizontal red line.
