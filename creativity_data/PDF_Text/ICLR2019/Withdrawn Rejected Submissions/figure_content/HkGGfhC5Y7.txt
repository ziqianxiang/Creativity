Figure 1: VQ-VAE model adapted to conditional supervised translation as described inKaiser et al. (2018). We use x and y to denote the source and target sentence respectively.
Figure 2: VQ-VAE model as described in van den Oord et al. (2017) for image reconstruction.
Figure 3: Samples of original and reconstructed images from CIFAR-10 using EM trainedwith a code-book of size 28 .
Figure 4: On the left are reconstructions from a model trained with VQ-VAE (van den Oordet al., 2017) and the right figure shows reconstructions from EM training, our approach.
Figure 5: Comparison of VQ-VAE (green curve) vs EM with different number of samples(yellow and blue curves) on the WMTâ€™14 English-German translation dataset with a code-book size of 214, with the encoder of the discrete autoencoder attending to the outputof the encoder of the source sentence as in Kaiser et al. (2018). The y-axis denotes theteacher-forced BLEU score on the test set, which is used only for evaluation while training.
