Figure 1: Illustration of the 3 pacing functions used below, showing the different hyper-parametersthat define each of them (see text). The values of the hyper-parameters used in this illustration werechosen arbitrarily, for illustration only.
Figure 2: Results in case 1, with Inception-based transfer scoring function and fixed exponentialpacing function. Inset: bars indicating the average final accuracy in each condition over the lastfew iterations. Error bars indicate the STE (STandard Error of the mean) after 50 repetitions. Thecurriculum method (in blue) reaches higher accuracy faster, and converges to a better final solution.
Figure 3: Results in cases 2 and 3, with Inception-based transfer scoring function and fixed expo-nential pacing function. Inset: zoom-in on the final iterations, for better visualization. Error barsshow STE after 5 repetitions. (a) CIFAR-10 dataset, (b) CIFAR-100 dataset.
Figure 4: (a) Curriculum learning using a competitive VGG-based network running on the entireCIFAR-100 dataset. (b) Results in case 1 with the Inception-based transfer scoring function, show-ing both the self-taught scoring function and the self-paced scoring function. Inset: bars indicatingthe average final accuracy in each condition over the last few iterations. Error bars indicate the STEafter 3 repetitions in (a) and 50 repetitions in (b).
Figure 5: Results in case 5, with Inception-based transfer scoring function and fixed exponentialpacing function. Inset: zoom-in on the final iterations, for better visualization. Error bars show STEafter 25 repetitions.
Figure 6: (a) Distance between the mean gradient direction of preferred examples under differentscoring functions. Each bar corresponds to a pair of mean gradients in two different conditions, seedetails in the main text. (b) The total variance of each set of gradients.
Figure 7: Results in case 1, with Inception-based transfer scoring function. (a) Single step pacingfunction, (b) varied Exponential pacing function. Inset: bars indicating the average final accuracy ineach condition over the last few iterations. Error bars indicate the STE after 50 repetitions.
Figure 8: Results in case 1, showing the learning curves for all different test conditions. (a) Inset:bars indicating the average final accuracy in each condition over the last few iterations. (b) Barsindicate the final accuracy when picking the parameters which maximize the area under the curve.
Figure 9: Results under the same conditions as in Fig. 8b, using instead the ‚Äùaquatic mammalsCIFAR-100 superclass. Error bars show STE after 50 iterations.
Figure 10: Results in case 1, comparing different variants of the transfer scoring function. The insetbars show the final accuracy of the learning curves. The error bars shows STE after 50 repetitions forthe vanilla and Inception conditions with RBF kernel SVM, and 5 repetitions for the Resnet, VGG-16and the Linear SVM conditions.
Figure 11: Results under conditions similar to empirical case 3 as shown in 3b, using cyclic schedul-ing for the learning rate as proposed by Smith (2017).
