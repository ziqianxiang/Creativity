Figure 1: The input image is converted into a spike map over time. At each time step neurons spikewith a probability proportional to the corresponding pixel value at their location. These spike maps,when summed over several time steps, resemble the original inputThe neuron model is that of a leaky integrate-and-fire (LIF) neuron. The membrane potential (Vmem)is the internal state of the neuron that gets updated at each time step based on the input of the neuron,Z[t] (eq. 1).The output activation (A[t]) of the neuron depends on whether Vmem reaches a threshold(Vth) or not. At the time instant when Vmem ≥ Vth, the neuron spikes (eq. 2). At any time instant,the output of the neuron is 0 if it has not spiked, or 1 if it has spiked. The leak factor is determinedby a constant α. After a neuron spikes, it’s membrane potential is reset to 0. Fig. 2b illustrates atypical neuron’s behavior over time in an SNN.
Figure 2: The operation of a Spiking Neural Network:(a) The input vector is mapped one-to-one tothe input neurons(layer(0)). The input value governs the firing rate of the neuron, i.e. number oftimes the neuron output is 1 in a given duration. (b) The membrane potential of the neuron integratesover time (with leak). As soon as it crosses Vth, the neuron output changes to 1, and Vmem is resetto 0. For taking derivative during backpropagation, a sigmoid approximation is used for the neuronactivationthe Poisson spike trains. These autoencoders later form the building blocks of the audio-to-imagesynthesis network. The description of the network and the notation used throughout the paper isgiven in Fig. 2a.
Figure 3:	The AE-SNN (784-196-784) is trained over MNIST (60,000 training samples, batch size= 100) and we study the impact of (a) leak, (b) mask, and (c) input spike train duration.
Figure 4:	AE-SNN trained on MNIST (training examples = 60,000, batch size = 100)function, the neurons that are not supposed to fire, are just being trained to not have a membranepotential that exceeds the threshold, which is a more relaxed rule to enforce. Increasing the durationof the input spike train improves the performance as shown in Fig.3c. However, it comes at thecost of increased training time as backpropagation is done at each time step, as well as increasedinference time. We settle for an input time duration of 15 as a trade-off between MSE and time takento train and infer for the next set of simulations.
Figure 5:	AE-SNN trained on Fashion-MNIST (training examples = 60,000, batch size = 100)would be the spike image resolution. For a duration of 60 time steps, a neuron can spike anywherebetween zero to 60 times, thus allowing 61 gray-scale levels. Some of the generated images by AE-SNN-1024 are displayed in Fig. 5b. The AE-ANNs are trained for 1 epoch, batch size 100, learningrate 5e-3 and weight decay 1e-4. For Fashion-MNIST, the AE-SNNs exhibited better performancethan AE-ANNs as shown in Fig. 5a. This is an interesting observation, where the better performancecomes at the increased effort of per-batch training. Also it exhibits such behavior on only this dataset,and not on MNIST (4a). The spatio-temporal nature of training over each time step could possiblytrain the network to learn the details in an image better. We also observed that, for both datasets,MNIST and Fashion-MNIST, the AE-SNN converges faster than AE-ANNs trained without Adam,and converges at almost the same time as an AE-ANN trained with Adam. The proposed spike-based backpropagation algorithm is able to bring the AE-SNN performance at par, or even better,than AE-ANNs.
Figure 6: Audio to Image synthesis model using an Autoencoder trained on MNIST images, andan Audiocoder trained to convert TI-46 digits audio samples into corresponding hidden state of theMNIST images.
Figure 7:	The audiocoder (AC-SNN/AC-ANN) is trained over Dataset A, while the autoencoder(AE-SNN/AE-ANN) is fixed. MSE is reported on the overall audio-to-image synthesis model com-posed of AC-SNN/ANN and AE-SNN/ANN50505050506655443322CiSciS SSSSSS(φs ɪ ωω-≥Dataset Aone-image-per-class"Zero""One""Two"“Three”"Four""Five""Six"“Seven”“Eight”2	4	6	8	10 12 14 16 18 20Number of Epochs	"Nine"
Figure 8:	The performance of the Audio to Image synthesis model on the two datasets - A and BThe duration (Th) of stored “hidden state” spike train was varied from 15 to 10, 5, 2, and 1. Aspike map at a single time step is a 1-bit representation. The AE-SNN compresses an 784×8 bitrepresentation into 196×Th-bit representation. For Th = 15, 10, 5, 2, and 1, the compression is2.1 ×, 3.2×, 6.4×, 16× and 32× respectively. Even when the AC-SNN is trained with a muchsmaller “hidden state”, the AE-SNN is able to reconstruct the images without much loss. In Fig.
