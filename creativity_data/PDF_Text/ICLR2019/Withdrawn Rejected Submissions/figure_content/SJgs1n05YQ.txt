Figure 1: Visualization of learned semantic prior of M(ψ): the most and least likely nearby roomsfor dining room (L), bedroom (M) and outdoor (R), with numbers denoting ψz , i.e., the probabilityof two rooms connecting to each other.
Figure 2: Example of a successful trajectory. The agent is spawned inside the house, targeting“outdoor”. Left: the 2D top-down map with sub-target trajectories ("outdoor” - orange; “garage” -blue; “living room” - green); Right, 1st row: RGB visual image; Right, 2nd row: the posterior ofthe semantic graph and the proposed sub-targets (red arrow). Initially, the agent starts by executingthe sub-policy "outdoor" and then "garage" according to the prior knowledge (1st graph), but bothfail (top orange and blue trajectories in the map). After updating its belief that garage and outdoorare not nearby (grey edges in the 2nd graph), it then executes the "living room" sub-policy withsuccess (red arrow in the 2nd graph, green trajectory). Finally, it executes “outdoor” sub-policyagain, explores the living room and reaches the goal (3rd graph, bottom orange trajectory).
Figure 3: Comparison in success rate with model-free baselines (Sec. 6.2). We evaluate performanceof random policy (blue), model-free RL baseline (pure μ(θ), green) and our LEAPS agent (red),with increasing horizon H from left to right (left: H = 300; middle: H = 500; right: H = 1000).
Figure 4: Comparison in success rate with semantic-aware policies (Sec. 6.3). We evaluate perfor-mance of the semantic augmented model-free agent ("aug. μs(θ)”, blue), the HRL agent with thesame sub-policies as LEAPS but with an LSTM controller (“RNN control.”, green) and our LEAPSagent (red), with increasing horizon H from left to right (left: H = 300; middle: H = 500; right:H = 1000). Top row: success rate (y-axis) w.r.t. the distance in meters from birthplace to target(x-axis); middle row: success rate with confidence interval (y-axis) w.r.t. the shortest planningdistance in the ground truth semantic model (x-axis); bottom row: relative improvements of LEAPSover the baselines (y-axis) w.r.t. the optimal plan distance (x-axis). Our LEAPS agent outperformsboth of the baselines for targets requring planning computations (i.e., plan-steps > 1). For farawaytargets with plan-steps > 2 in longer horizons (H ≥ 500), LEAPS improves over augmented policyby 80% and over RNN controller by 10% in success rate. Note that even though the LSTM controllerhas two orders of magnitudes more parameters than our semantic model M, our LEAPS agent stillperforms better.
Figure 5: Metrics of Success Rate(%) / SPL(‰) evaluating the performances of LEAPS and baselineagents. Our LEAPS agents have the highest success rates for all the cases requiring planningcomputations, i.e., plan-steps larger than 1. For SPL metric, LEAPS agents have the highest overallSPL value over all baseline methods (rightmost column). More importantly, as the horizon increases,LEAPS agents outperforms best baselines more. LEAPS requires a relatively longer horizon for thebest practical performances since the semantic model is updated every fixed N = 30 steps, whichmay potentially increase the episode length for short horizons. More discussions are in Sec. 6.4.
Figure 6:	Metrics of Success Rate(%) / SPL(‰) evaluating the performances of LEAPS and baselinesagents using the ground truth oracle semantic signals provided by the environments. We also includethe performance of LEAPS agent using CNN detector as a reference. Note that even using an CNNdetector, LEAPS agents outperforms all baselines in both metrics of success rate and SPL. Notably,the performance of LEAPS-CNN agents is comparable to LEAPS-true agents and sometimes evenbetter. This indicates that our semantic model can indeed tolerate practical errors in CNN detectors.
Figure 7:	Averaged successful episode length for different approaches. The length of shortest pathreflects the strong difficulty of this task.
