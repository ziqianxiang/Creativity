Figure 1: Training process for a Q-map agent updating the prediction towards all goals at once.
Figure 2: Left: For each room, the layout is shown (L), with some observations (O), ground-truthQ-frames (G) and predicted Q-frames (maximized over the action dimension) (P). Right: Meansquared error (MSE) between the predictions and ground truth, for Q-maps using the proposedconvolutional architecture and a simple multilayer perceptron (MLP) in millions of transitions.
Figure 3: Illustration of the action-decision pipeline of a DQN + Q-map agent.
Figure 4: Example of learned Q-frames (maximized over the action dimension) on Montezuma’sRevenge and how a simple random-goal walk allows to explore most of the first room. Goals areshown with circles. Most of the room is visited quickly with the proposed exploration method.
Figure 5: Top: Coordinates visited after 2 million steps on the first level of Super Mario All-Stars.
Figure 6: Performance comparison between ε-greedy exploration (red), and the proposed exploration(green) with confidence intervals of 99%. Vertical bars indicate flags reached (end of the level). Theproposed agent significantly outperforms the baseline, reaching earlier and more frequently the flag.
