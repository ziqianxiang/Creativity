Figure 1: An overall framework of considered resizable mini-batch gradient descent algorithm(RMGD). The RMGD samples a batch size from a probability distribution, and parameters areupdated by mini-batch gradient using the selected batch size. Then the probability distribution isupdated by checking the validation loss.
Figure 2: The probability distribution vs epoch using the RMGD. (top) The early stages of thetraining. (bottom) The later stages of the training. The white dot represents the selected batch sizeat each epoch. In the early stages of the training, RMGD updates the probabilities to search variousbatch sizes (exploration), and in the later stages, RMGD increases the probability of successful batchsize (exploitation).
Figure 3: The probability distribution and selected batch size. The white dot is selected batch size atepoch. (top) The case that small batch size performs better. (middle) The case that large batch sizeperforms better. (bottom) The case that best performing batch size varies.
Figure 4: The results of test accuracy for the MNIST dataset. The error bar is standard error. (left)The test accuracy of 100 times repeated experiments with AdamOptimizer. (right) The test accuracyof 100 times repeated experiments with AdagradOptimizer. In both cases, most RMGD settingsoutperform all fixed MGD algorithms.
Figure 5: The results of test accuracy for the CIFAR10 and CIFAR100 dataset. The error bar isstandard error. (left) The test accuracy of 25 times repeated experiments on CIFAR10. (right) Thetest accuracy of 10 times repeated experiments on CIFAR100. In both cases, all RMGD settingsoutperform all fixed MGD algorithms.
