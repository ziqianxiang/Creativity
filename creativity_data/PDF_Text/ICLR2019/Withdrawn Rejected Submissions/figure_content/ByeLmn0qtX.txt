Figure 1: The key concept of variational domain adaptation. a) Given the proposal drawn from theprior, the discriminator discriminates the target domain from the others. Each domain is posterior forthe prior N (z|0, 1); further, the distribution in the latent space is observed to be a normal distributionusing the conjugate likelihood. b) Domain transfer is represented by the mean shift in the latentspace. C) Domain embedding: After training, all the domains can only be represented by vectors μ2.
Figure 2: Left: Graphical models of the probabilistic models of VAE and DualVAE. The gray andwhite circles indicate the observed variables and latent variables, respectively. Symbols withoutcircles indicate the constants. Arrows between the symbols indicate probabilistic dependency (e.g.,X generates Y). A rectangle with suffixes indicates a block, which comprises multiple elements.
Figure 3: Images obtained from our model by decoding wi (i = 14, 18, and 32) While changing thevalue of λ. The reconstructed images are present in the center.
Figure 4: Scatter plot of DVAE, UFDN, StarGAN and CVAE when we change several parameters.
Figure 5: Latent visualization of VAE (left) and DualVAE (right) demonstrates that DualVAE learnsa good prior to model the domains. The heat map indicates the mean score of all the users.
Figure 6: Comparing domain transfer by several methods. (a) Since the image is blurry comparedto StarGAN, although the original features change significantly, domain transfer is still conductedproperly. (b) Although the characteristics of the original image are well preserved, domain transferand reconstruction is not conducted. (c) Although the characteristics of the original image are wellpreserved, domain transfer is not conducted well. (d) Keeping the characteristic and being able totransfer a small amount of the domain.
Figure 7: Scatter plot of the missing ratio of MNIST’s label and DIS of the DualVAE. Variable s isthe missing ratio. The original image is shown in the top right of the figure. The labels of the originalimages are transformed to zero, one and two. The vertical axis is ts of Algorithm 2, the horizontalaxis is rs of Algorithm 2. DIS grows in the upper right corner.
Figure 8: Domain transfer by varying λ. (a) Good example: domain transfer to different label issuccessful while keeping the characteristic of the reconstruction image. (b) Bad example: althoughthe characteristic of reconstruction images are kept, domain transfer to different labels is not enough.
Figure 9: Scatter plot of the domain embedding vectors, and several decoded images of the sam-ples from each domain. Six zi from the target domain distribution and output xi were decoded.
Figure 10: Images decoded to get closer (or further) to the average of domain embedding vectors.
Figure 11: RMSE and Reconstruction loss. DualVAE is far superior to VAE in classification accu-racy, and there is almost no difference in reconstruction error between them.
Figure 12: Scatter plot of the missing ratio of celeA’s label and DIS of DualVAE. Variable s is themissing ratio. The original image is shown on the left top of the figure. The attributes of the originalimages are transformed to blond hair, eyeglasses and mustache. The vertical axis is ts of Algorithm 2,the horizontal axis is rs of Algorithm 2. DIS grows in the upper right corner.
Figure 13: Sparsity analysis through various sparsity.
Figure 14: Scatter plot of the missing ratio of celeA’s label and DIS of StarGAN. Variable s is themissing ratio. The vertical axis is ts of Algorithm 2, the horizontal axis is rs of Algorithm 2. DISgrows at the upper right corner.
Figure 15: Sparsity analysis through StarGAN.
Figure 16: Plot of DVAE, UFDN, and StarGAN when we change alpha. The performance of DVAEis rubust to α and DVAE outperforms the existing methods based on the DIS.
Figure 17: (a) Distribution of the respective scores within [1, 5] rated by 60 people. (b) The imagesof SCUT-FBP5500.
Figure 18: DualVAE stably transfers samples across 10 domains while domain-irrelevant features(e.g., style) are kept.
