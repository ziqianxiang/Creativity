Figure 1: The conditional transformation unit Φ constructs a collection of mappings {Φk} in thelatent space which produce high-level attribute changes to the decoded outputs. Conditioning infor-mation is used to select the appropriate convolutional weights ωk for the specified transformation;the encoding lx of the original input image x is transformed to lyk = Φk (lx) = conv(lx , ωk) andprovides an approximation to the encoding lyk of the attribute-modified target image yk .
Figure 2: Selected methods for incorporating conditioning information; the proposed LTNN methodis illustrated on the left, and six conventional alternatives are shown to the right.
Figure 3: Comparison of CVAE-GAN (top) with proposed LTNN model (bottom) using the noisyNYU hand dataset (Tompson et al., 2014). The input depth-map hand pose image is shown to thefar left, followed by the network predictions for 9 synthesized view points. The views synthesizedusing LTNN are seen to be sharper and also yield higher accuracy for pose estimation (see Figure 6).
Figure 4: Qualitative evaluation for multi-view reconstruction of hand depth maps using the NYUdataset.
Figure 5: Quantitative evaluation for multi-view hand synthesis. (a) Evaluation with state-of-the-artmethods using the real NYU dataset. (b) LTNN ablation results and comparison with alternativeconditioning frameworks using synthetic hand dataset. Our models: conditional transformation unit(CTU), conditional discriminator unit (CDU), task-divide and RGB balance parameters (TD), andLTNN consisting of all previous components along with consistency loss. Alternative concatenationmethods: channel-wise concatenation (CH Concat), fully connected concatenation (FC Concat), andreshape concatenation (RE Concat).
Figure 7: Qualitative evaluation for multi-view reconstruction of real face using the stereo facedataset (Fransens et al., 2005).
Figure 6: Quantitative comparison of model performances for experiment on the real face dataset.
Figure 8: Unseen objectsOf note is the fact that the LTNN framework is capable of effectively performing the specifiedrigid-object transformations using only a single image as input, whereas most state-of-the-art viewsynthesis methods require additional information which is not practical to obtain for real datasets.
Figure 9: Generated 360° views for chair dataset. A single, gray-scale image of the chair at the farleft (shown in box) is provided to the network as input.
