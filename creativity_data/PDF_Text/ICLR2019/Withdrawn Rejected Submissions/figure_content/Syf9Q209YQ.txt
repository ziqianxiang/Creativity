Figure 1: Issues with GAN-derived manifold gradients. (a) Left: Samples (green dots) and generatedsamples (red dots) from a GAN trained on the two circles dataset. Middle: Manifold gradients fromthe trained GAN. Right: Manifold gradients normalized to unit norm. The GAN approximatesthe data manifold well in this toy example and normalizing gradients mitigates issues with noisygradients. (b) Effect of perturbations on the latent code of a GAN trained on MNIST. Each pair ofrows shows the generated example from the latent code (top) and the generated example from theperturbed latent code (bottom). Random perturbations of equal norm in the latent space can havealmost no effect (blue box) or a large effect (red box) on generated examples.
Figure A1: Results including data augmentation (4x4 random translations and random horizontalflips similar to previous works).
Figure A2: Manifold regularization With knoWn tangent directions on a synthetic dataset. Left:Semi-supervised classification of the disjoint manifold. Middle: Magnitude of the regularizationterm for a batch of generated samples. Darker fill color reflects larger magnitude. Generated data-points near the decision boundary are highly penalized. Right: KnoWn tangent directions.
Figure A3: Behavior of manifold regularization With a separate classifier on toy examples. Left:Classification boundaries of trained classifier. Labeled examples are shoWn as points, unlabeledexamples Were draWn from distributions over the curves shoWn. The classifier achieves perfect ac-curacy on the tWo datasets. Middle: Magnitude of the regularization term for a batch of generatedsamples. Darker fill color reflects larger magnitude. Generated data-points near the decision bound-ary are highly penalized. Right: Direction of invariance promoted by our norm. The trained GANsare able to approximate the data distribution and manifold gradients. In this example γ = 6 and =0.15.
Figure A4: GANs trained on a disjoint manifold. Visualization of the non continuous mapping.
Figure A5: Generated images from the coupled experiments trained on CIFAR-10. (a) ImprovedGAN (b) Ambient regularization (c) Manifold regularization. Mode collapse is visible for the Im-proved GAN images with no regularization.
Figure A6: Generated images from the coupled experiments trained on SVHN. (a) Improved GAN(b) Ambient regularization (c) Manifold regularization. Mode collapse is visible for the ImprovedGAN images with no regularization.
Figure A7: Generated images for decoupled experimentsF Tangent imagesWe show samples from the decoupled generator trained on the unlabeled examples on CIFAR10, inFigure A7. The model has an inception score of 6.68 and a FID of 32.32In this section, for a better understanding of our method, we provide examples of tangent images,approximated by our trained GAN. We shows the influence of the different hyperparameters η, inFigure A9 and A8 on the visual aspect of the images.
Figure A8: Effect of hyperparameter for approximating manifold regularization. Generated imageswith varying perturbations as per the gradient approximation (g(z) + ef) are shown for η = 1. Weused ( = 20, η = 1) in our experiments.
Figure A9: Effect of hyperparameter η for approximating manifold regularization. Moving η in therange [10-4, 10-3, 10-2, 10-1,1, 10] with = 20 fixedFigure A10: Unsupervised learning of the two moon dataset. No labels are used for the training. Theobjective function is made of the Laplacian norm, a l2 norm and the entropy of the output predictionQ(f) =YL Ilf kL + YK Ilf k2K - γhH(f). We added the entropy to avoid degenerated solutions. Inthis example = 0.15 , Y = 3 ,YK = 1 , Yh = 0.1G Unsupervised learning of simple manifolds by minimizing theLaplacian normSimilarly to Belkin et al. (2006) we show that this is possible to go even further with the mani-fold regularizer. Under certain conditions, we noticed that it was possible to cluster without anysupervision (Figure A10).
