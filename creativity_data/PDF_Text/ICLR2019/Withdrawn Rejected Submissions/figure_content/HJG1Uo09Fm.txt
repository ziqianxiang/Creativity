Figure 1: A diagram of our method. We aim to optimize for a fast reinforcement learning procedure throughimitation. We train for a set of initial parameters θ such that only one or a few steps of gradient descentproduces a policy that matches the actions from expert demonstrations. At meta-test time, we run a few steps ofreinforcement learning to efficiently acquire a policy for a new task.
Figure 2: Illustration of a reaching task (left) and an ant locomotion tasks (middle) in our experimentalevaluation. The range of ant goals is shown on the right. Our experiments consider a sparse reward variant ofeach task, where the reward is only provided when the robot moves its gripper next to the cyan ball and when theant comes near the target location.
Figure 3: Reinforcement learning performance at meta-test time, comparing our approach with the MAML RLmethod, learning from a policy initialized from demonstrations and a random initialization. Our method is able tomatch the performance of meta-reinforcement learning and slightly outperform initializing from demonstrationsin these experimental domainsWe compare our MRI approach to the original MAML RL method (Finn et al., 2017a), using onestep of policy gradient for the inner optimization and trust region policy optimization for the outeroptimization. We additionally compare to two baselines. To provide a lower bound on learningperformance that indicates the difficulty of the task, we compare to a randomly-initialized policy. Wealso compare to a policy initialized via behavior cloning on all of the demonstrations. Note that thiscorresponds to our approach with an inner learning rate of α = 0. This comparison measures theeffect of explicitly training for fast adaptation, rather than simply training for a policy that attempts torepresent all of the meta-training tasks at once.
Figure 4: Reinforcement learning performance, when only sparse rewards are available at meta-test time. Ourmethod is able to learn when only sparse rewards are available, whereas prior methods struggle.
