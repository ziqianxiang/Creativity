title,year,conference
    Transformer-xl:   Attentive  language  models  beyond  a  fixed-length  context,2019,    CoRR
  BERT: pre-training of deepbidirectional  transformers  for  language  understanding,2018,    CoRR
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
     Sequence-level  knowledge  distillation,2016,     arXiv  preprintarXiv:1606
  Adam:  A method for stochastic optimization,2014,  arXiv preprintarXiv:1412
  Actor-critic algorithms,2000,  In Advances in neural informationprocessing systems
   Improved image cap-tioning  via  policy  gradient  optimization  of  spider,2017,   In  Proceedings  of  the  IEEE  internationalconference on computer vision
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Sequence level train-ing with recurrent neural networks,2015, arXiv preprint arXiv:1511
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
   Attention is all you need,2017,   CoRR
