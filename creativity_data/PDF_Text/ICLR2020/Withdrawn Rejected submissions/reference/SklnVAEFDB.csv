title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 The best of both worlds: Combin-ing recent advances in neural machine translation,2018, arXiv preprint arXiv:1804
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Teaching machines to read and comprehend,2015, In Advances inneural information processing systems
 Rouge: A package for automatic evaluation of summaries,2004, In Text summarizationbranches out
 Coarse-to-fine attention models for document summarization,2017, InProceedings of the Workshop on New Frontiers in Summarization
 Generating wikipedia by summarizing long sequences,2018, In International Conference onLearning Representations
 Fine-tune bert for extractive summarization,2019, arXiv preprint arXiv:1903
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Language models with transformers,2019, CoRR
 R-transformer: Recurrent neural networkenhanced transformer,2019, arXiv preprint arXiv:1907
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
