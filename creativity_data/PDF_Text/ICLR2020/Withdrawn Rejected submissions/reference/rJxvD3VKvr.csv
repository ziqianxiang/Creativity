title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 On the convergence rate of training recurrent neuralnetworks,2018, arXiv preprint arXiv:1810
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Overfitting or perfect fitting? risk bounds forclassification and regression rules that interpolate,2018, In Advances in Neural Information ProcessingSystems
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 The effects of optimization on generalization in infinitely wide neural net-works,2019, ICML Workshop
 Sgd learns the conjugate kernel class of the network,2017,	In I
 Essentially no barri-ers in neural network energy landscape,2018, arXiv preprint arXiv:1803
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Deep learning,2016, MIT press
 Training dynamics of deep networks usingstochastic gradient descent via neural tangent kernel,2019, arXiv preprint arXiv:1905
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Deep neural networks as gaussian processes,2017, arXiv preprint arXiv:1711
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Just interpolate: Kernel” ridgeless” regression can gener-alize,2018, arXiv preprint arXiv:1808
 Global minima of dnns: The plenty pantry,2019, arXiv preprintarXiv:1905
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 On the spectral bias of neural networks,2018, arXiv preprintarXiv:1806
 Kernel anddeep regimes in overparametrized models,2019, arXiv preprint arXiv:1906
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 A type of generalization error induced byinitialization in deep neural networks,2019, arXiv preprint arXiv:1905
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
