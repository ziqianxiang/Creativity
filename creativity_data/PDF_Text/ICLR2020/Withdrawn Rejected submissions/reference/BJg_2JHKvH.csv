title,year,conference
 Invertible residual networks,2018, arXivpreprint arXiv:1811
 MixMatch: A holistic approach to semi-supervised learning,2019, arXiv preprintarXiv:1905
 Residual flows forinvertible generative modeling,2019, arXiv preprint arXiv:1906
 Good semi-supervised learning that requires a bad GAN,2017, In Advances in Neural Information ProcessingSystems 30
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 NICE: Non-linear independent componentsestimation,2014, arXiv preprint arXiv:1410
 Density estimation using Real NVP,2016, arXivpreprint arXiv:1605
 On calibration of modern neuralnetworks,2017, CoRR
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational Bayes,2013, arXiv preprintarXiv:1312
 Glow: Generative flow with invertible 1Ã—1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Feature visualization,2017, Distill
 Realis-tic evaluation of deep semi-supervised learning algorithms,2018, In Advances in Neural InformationProcessing Systems
 Masked autoregressive flow for densityestimation,2017, In Advances in Neural Information Processing Systems
 The infinite gaussian mixture model,2000, In Advances in neural informationprocessing systems
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Interpolation con-sistency training for semi-supervised learning,2019, arXiv preprint arXiv:1903
