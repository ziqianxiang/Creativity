title,year,conference
 Cnvlutin: Ineffectual-neuron-free deep neural network computing,2016, ACM SIGARCHComputer Architecture News
 Compression-aware training of deep networks,2017, In Advancesin Neural Information Processing Systems
 Scalable methods for 8-bit training ofneural networks,2018, In Advances in Neural Information Processing Systems
 Mixed precision training of convolutional neural networks us-ing integer operations,2018, In International Conference on Learning Representations
 High-accuracy low-precision training,2018, arXiv preprintarXiv:1803
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Ultimate tensoriza-tion: compressing convolutional and fc layers alike,2016, arXiv preprint arXiv:1611
 Deep learning withlimited numerical precision,2015, In International Conference on Machine Learning
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In Advances in neural information processing systems
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Algorithm 65: Find,0001, Commun
 Loss-aware binarization of deep networks,2016, arXivpreprint arXiv:1611
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Learning to quantize deep networks by optimizing quantizationintervals with task loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Learning multiple layers of features from tiny images,2009, Technical report
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Fixed point quantization of deep convolu-tional networks,2016, In International Conference on Machine Learning
 Towards accurate binary convolutional neural network,2017, InAdvances in Neural Information Processing Systems
 Deep gradient compression: Re-ducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Dynamic sparsegraph for efficient deep learning,2019, In International Conference on Learning Representations
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Re-laxed quantization for discretized neural networks,2019, In International Conference on LearningRepresentations
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Discovering low-precision networks close to full-precision networks for efficient embedded inference,2018, arXiv preprint arXiv:1809
 Ternary neural networks with fine-grained quantization,2017, arXiv preprintarXiv:1705
 Introspective sorting and selection algorithms,1997, Software: Practice andExperience
 Tensorizing neuralnetworks,2015, In Advances in neural information processing systems
 Automatic differentiation inpytorch,2017, 2017
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European Conference on ComputerVision
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In C
 Per-tensor fixed-point quantization of the back-propagationalgorithm,2019, In International Conference on Learning Representations
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Sparsified sgd with memory,2018, InAdvances in Neural Information Processing Systems
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Train-ing deep neural networks with 8-bit floating point numbers,2018, In Advances in neural informationprocessing systems
 Minimal effort back propagation forconvolutional neural networks,2017, arXiv preprint arXiv:1709
 Learning structured sparsity indeep neural networks,2016, In Advances in neural information processing systems
 Quantized convolutionalneural networks for mobile devices,2016, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Training and inference with integers in deepneural networks,2018, In International Conference on Learning Representations
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Lq-nets: Learned quantization forhighly accurate and compact deep neural networks,2018, In Proceedings of the European Conferenceon Computer Vision (ECCV)
 Improving neuralnetwork quantization using outlier channel splitting,2019, arXiv preprint arXiv:1901
 Explicit loss-error-aware quantizationfor low-bit deep neural networks,2018, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
