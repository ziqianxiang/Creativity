title,year,conference
  Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 A large annotatedcorpus for learning natural language inference,2015, arXiv preprint arXiv:1508
 Groupreduce: Block-wise low-rankapproximation for neural language model shrinking,2018, In Advances in Neural Information ProcessingSystems
  Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Ensemble distillation for neural machinetranslation,2017, arXiv preprint arXiv:1702
  The state of sparsity in deep neural networks,2019,  arXivpreprint arXiv:1902
  Deep Learning,2016,  MIT Press
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
  Long short-term memory,1997,  Neural computation
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, Transactions of the Association for Computational Linguistics
 Natural language inference,2009, Citeseer
 Low rank approximation,2012, Springer
   Quaternion  recurrent  neural  networks,2018,   arXiv  preprintarXiv:1806
 A decomposable attentionmodel for natural language inference,2016, arXiv preprint arXiv:1606
  Glove:  Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, 2019
   Attention is all you need,2017,   In Advances in neural informationprocessing systems
  Bilateral multi-perspective matching for naturallanguage sentences,2017, arXiv preprint arXiv:1702
   Dialogue  natural  languageinference,2018, arXiv preprint arXiv:1811
  A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
   Aggregated residualtransformations for deep neural networks,2017,  In Proceedings of the IEEE conference on computervision and pattern recognition
