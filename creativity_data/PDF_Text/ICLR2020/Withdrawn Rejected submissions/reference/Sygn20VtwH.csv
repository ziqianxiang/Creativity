title,year,conference
 Tree-structured decoding with doubly-recurrent neuralnetworks,2016, 2016
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 An actor-critic algorithm for sequence prediction,2016, arXiv preprintarXiv:1607
 An empirical evaluation of generic convolutionaland recurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Trellis networks for sequence modeling,2018, arXivpreprint arXiv:1810
 Modeling temporal depen-dencies in high-dimensional sequences: Application to polyphonic music generation and tran-scription,2012, arXiv preprint arXiv:1206
 Recursive neural networks canlearn logical semantics,2014, arXiv preprint arXiv:1406
 A fast unified model for parsing and sentence understanding,2016, arXiv preprintarXiv:1603
 Quasi-recurrent neural net-works,2016, arXiv preprint arXiv:1611
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Hierarchical multiscale recurrent neural net-works,2016, arXiv preprint arXiv:1609
 Language to logical form with neural attention,2016, arXiv preprintarXiv:1601
 Recurrent neural networkgrammars,2016, arXiv preprint arXiv:1602
 Hierarchical recurrent neural networks for long-term dependen-cies,1996, In Advances in neural information processing systems
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Learning hierarchical in-formation flow with recurrent neural modules,2017, In Advances in Neural Information ProcessingSystems
 Cooperative learning of disjoint syntaxand semantics,2019, arXiv preprint arXiv:1902
 Long short-term memory,1997, Neural computation
 Towards neural phrase-based machine translation,2017, arXiv preprint arXiv:1706
 Learning hierarchicalstructures on-the-fly with a recurrent-recursive model for sequences,2018, In Proceedings of The ThirdWorkshop on Representation Learning for NLP
 A clockwork rnn,2014, arXivpreprint arXiv:1402
 Simple recurrent units for highly par-allelizable recurrence,2018, In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing
 Towards binary-valued gates for robust lstm training,2018, arXiv preprint arXiv:1806
 Latent predictor networks for code generation,2016, arXiv preprintarXiv:1603
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Stanford neural machine translation systems forspoken language domains,2015, 2015
 Abstract syntax networks for code generationand semantic parsing,2017, arXiv preprint arXiv:1704
 Sequence level train-ing with recurrent neural networks,2015, arXiv preprint arXiv:1511
 Relational recurrent neuralnetworks,2018, In Advances in Neural Information Processing Systems
 Neural language modeling byjointly learning syntax and lexicon,2017, arXiv preprint arXiv:1711
 Ordered neurons: Integratingtree structures into recurrent neural networks,2018, arXiv preprint arXiv:1810
 Bivariate beta lstm,2019, arXiv preprintarXiv:1905
 Improved semantic representationsfrom tree-structured long short-term memory networks,2015, arXiv preprint arXiv:1503
 Recurrently controlled recurrent networks,2018, In Advancesin Neural Information Processing Systems
 The importance of being recurrent for modelinghierarchical structure,2018, arXiv preprint arXiv:1803
 Learning longer-term depen-dencies in rnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Tensor2tensor for neural machine translation,2018, CoRR
 Tree transformer: Integrating tree structuresinto self-attention,2019, arXiv preprint arXiv:1909
 Tranx: A transition-based neural abstract syntax parser forsemantic parsing and code generation,2018, arXiv preprint arXiv:1810
 Learning tocompose words into sentences with reinforcement learning,2016, arXiv preprint arXiv:1611
