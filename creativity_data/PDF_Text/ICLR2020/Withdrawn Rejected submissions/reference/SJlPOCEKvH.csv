title,year,conference
 Tensorflow:Large-scale machine learning on heterogeneous distributed systems,2016, CoRR
 Stronger generalization bounds fordeep nets via a compression approach,2018, CoRR
 KERMIT: gen-erative insertion-based modeling for sequences,2019, CoRR
 Sparse networks from scratch: Faster training withoutlosing performance,2019, ArXiv
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 The difficulty of training sparseneural networks,2019, CoRR
 The state of sparsity in deep neural networks,2019, CoRR
 Eie: Efficient inference engine on compressed deep neural network,2016, In Proceedings of the43rd International Symposium on Computer Architecture
 Using pre-training can improve model robust-ness and uncertainty,2019, In ICML
 Scalable syntax-aware language models using knowledge distillation,2019, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Rethinking the valueof network pruning,2019, In International Conference on Learning Representations
 Learning sparse neural networks through l-0 regularization,2018, In International Conference on Learning Representations
 Deep contextualized word representations,2018, CoRR
 Languagemodels are unsupervised multitask learners,2019, 2019
 Low-memory neural network training: A technical report,2019, CoRR
 Energy and policy considerations for deeplearning in NLP,2019, CoRR
 Distilling task-specific knowledge from BERT into simple neural networks,2019, CoRR
 Luck matters: Understandingtraining dynamics of deep relu networks,2019, CoRR
 Attention is all you need,2017, CoRR
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, CoRR
