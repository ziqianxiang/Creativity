title,year,conference
   Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
  Keeping notes:Conditional natural language generation with a scratchpad encoder,2019,  In Proceedings of the 57thConference of the Association for Computational Linguistics
 Fast abstractive summarization with reinforce-selected sentencerewriting,2018, CoRR
  Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Language to logical form with neural attention,2016, CoRR
   Neural turing machines,2014,   CoRR
   Incorporating copying mechanism insequence-to-sequence learning,2016,   CoRR
   Teaching  machines  to  read  and  comprehend,2015,   In  Advances  inneural information processing systems
  Abstractive summarization of reddit postswith multi-level memory networks,2018, arXiv preprint arXiv:1811
  Adam:  A method for stochastic optimization,2014,  arXiv preprintarXiv:1412
 Ask me anything: Dynamic memory networks fornatural language processing,2015, CoRR
      A   structured   self-attentive   sentence   embedding,2017,      arXiv   preprintarXiv:1703
  Effective approaches to attention-based neural machine translation,2015, arXiv preprint arXiv:1508
  A taxonomy for neural memory networks,2018,  CoRR
  Key-value memory networks for directly reading documents,2016,  CoRR
   A  read-write  memory  network  for  moviestory understanding,2017, CoRR
 Abstractive text summarizationusing sequence-to-sequence rnns and beyond,2016, arXiv preprint arXiv:1602
  Ranking sentences for extractive summariza-tion with reinforcement learning,2018,  CoRR
  Attend to you:  Personalized imagecaptioning with context sequence memory networks,2017, CoRR
 On the difficulty of training recurrent neuralnetworks,2013, pp
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
   A neural attention model for abstractivesentence summarization,2015,   CoRR
 Commentaryreconsolidation: Strengthening the shaky trace through retrieval,2000, NatureReviews Neuroscience
   Get  to  the  point:  Summarization  withpointer-generator networks,2017,  CoRR
  Hybrid memnet for extractive sum-marization,2017,   In  Proceedings  of  the  2017  ACM  on  Conference  on  Information  and  KnowledgeManagement
    On  extractive  andabstractive neural document summarization with transformer language models,2019,   arXiv preprintarXiv:1909
  Weakly supervised memorynetworks,2015, CoRR
 Coverage-based neural machinetranslation,2016, CoRR
   Pointer  networks,2015,   In  Advances  in  NeuralInformation Processing Systems
   Memory-enhanced decoder for neuralmachine translation,2016,  CoRR
 No training required: Exploring random encoders for sentence clas-sification,2019, CoRR
 Memory architectures in recurrent neural network language models,2018, 2018
  The dimensionality oftoken embeddings is 128 and embeddings are trained from scratch,2014,  The vocabulary size is limitedto 50
