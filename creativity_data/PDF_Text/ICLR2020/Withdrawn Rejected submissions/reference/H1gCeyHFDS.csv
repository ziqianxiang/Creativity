title,year,conference
 Towards characterizing divergence in deepq-learning,2019, arXiv preprint arXiv:1903
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Improving the convergence of back-propagation learning withsecond order methods,1988, 1988
 Exact natural gradient in deep linearnetworks and its application to the nonlinear case,2018, In Advances in Neural Information ProcessingSystems
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, arXiv preprint arXiv:1905
 Training neural networks as learning data-adaptive kernels:Provable representation and approximation benefits,2019, arXiv preprint arXiv:1901
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Escaping from saddle points - online stochasticgradient for tensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 Fastapproximate natural gradient descent in a kronecker factored eigenbasis,2018, In Advances in NeuralInformation Processing Systems
 Numerical methods for solving linear least squares problems,1965, Numerische Mathematik
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, In International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 How to escapesaddle points efficiently,2017, In Proceedings of the 34th International Conference on Machine Learning
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2017, arXiv preprint arXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Kronecker-factored curvature approximations forrecurrent neural networks,2018, 2018
 Revisiting small batch training for deep neural networks,2018, arXivpreprint arXiv:1804
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, arXiv preprint arXiv:1902
 Generalization bounds of sgld for non-convexlearning: Two theoretical viewpoints,2017, arXiv preprint arXiv:1707
 Ordinal regression with multipleoutput cnn for age estimation,2016, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Efficient subsampled gauss-newton and natural gradient methods fortraining neural networks,2019, arXiv preprint arXiv:1906
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 High-Dimensional Statistics: A Non-Asymptotic Viewpoint,2019, CambridgeSeries in Statistical and Probabilistic Mathematics
 Scalable trust-regionmethod for deep reinforcement learning using kronecker-factored approximation,2017, In Advances inneural information processing Systems
 Group normalization,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Fast convergence of natural gradient descent foroverparameterized neural networks,2019, arXiv preprint arXiv:1905
 Fixup initialization: Residual learning withoutnormalization,2019, arXiv preprint arXiv:1901
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from minima and regularization effects,2018, arXiv preprintarXiv:1803
 An improved analysis of training over-parameterized deep neuralnetworks,2019, arXiv preprint arXiv:1906
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
