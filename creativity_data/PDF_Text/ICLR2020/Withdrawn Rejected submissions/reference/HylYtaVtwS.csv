title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Visual long-term memory has amassive storage capacity for object details,2008, Proceedings of the National Academy of Sciences
 Adversarial examples that fool both computer vision andtime-limited humans,2018, In Advances in Neural Information Processing Systems
 Generalisation in humans and deep neural networks,2018, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Adversarial spheres,2018, arXiv preprint arXiv:1801
 Improving neural language models with acontinuous cache,2016, arXiv preprint arXiv:1612
 Unbounded cache model for onlinelanguage modeling with open vocabulary,2017, In Advances in Neural Information Processing Systems
 Using videos to evaluateimage model robustness,2019, arXiv preprint arXiv:1904
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, In International Conference on Learning Representations (ICLR)
 Billion-scale similarity search with gpus,2017, arXivpreprint arXiv:1702
 A simple cache model for image recognition,2018, In Advances in Neural InformationProcessing Systems
 Foolbox: A python toolbox to benchmarkthe robustness of machine learning models,2017, arXiv preprint arXiv:1707
 Adver-sarially robust generalization requires more data,2018, In Advances in Neural Information ProcessingSystems
 Intriguing properties of neural networks,2013, 2013
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 Retrieval-augmented convolutional neural netWorks for improvedrobustness against adversarial examples,2018, arXiv preprint arXiv:1802
