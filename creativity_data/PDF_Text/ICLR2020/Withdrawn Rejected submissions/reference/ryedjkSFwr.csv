title,year,conference
 QSGD:communication-efficient SGD via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems
 A linear speedup analysis of distributed deep learning with sparseand quantized communication,2018, In Advances in Neural Information Processing Systems
 Error feedbackfixes signsgd and other gradient compression schemes,2019, In Proceedings of the 36th InternationalConference on Machine Learning
 Adam: A method for stochastic optimization,2015, In Proceedingsof the 3rd International Conference on Learning Representations
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 Scaling distributed machine learning withthe parameter server,2014, In Proceedings of the 11th Symposium on Operating Systems Design andImplementation
 Deep gradient compression: Reducingthe communication bandwidth for distributed training,2018, In Proceedings of the 6th InternationalConference on Learning Representations
 A stochastic approximation method,1951, Annals of MathematicalStatistics
 Sparsified SGD with memory,2018, InAdvances in Neural Information Processing Systems
 On the importance of ini-tialization and momentum in deep learning,2013, In Proceedings of the 30th International Conferenceon Machine Learning
 Doublesqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In Proceedings of the 36thInternational Conference on Machine Learning
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
