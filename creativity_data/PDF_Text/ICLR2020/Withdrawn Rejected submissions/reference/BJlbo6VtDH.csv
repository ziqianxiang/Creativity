title,year,conference
 Efficiency of Pseudolikelihood estimation for simPle gaussian fields,1977, 1977
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Constant-time machinetranslation with conditional masked language models,2019, arXiv preprint arXiv:1904
 Non-autoregressiveneural machine translation,2018, CoRR
 Insertion-based decoding with automatically inferredgeneration order,2019, CoRR
 Bridging nonlinearities and stochastic regularizers with gaussianerror linear units,2016, arXiv preprint arXiv:1606
 Adam: A method for stochastic optimization,2014, arXiv preprint1412
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Deterministic non-autoregressive neural sequencemodeling by iterative refinement,2018, arXiv preprint arXiv:1802
 Flowseq: Non-autoregressive conditional sequence generation with generative flow,2019, arXiv preprint 1909
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Unsupervised pretraining for sequence to sequencelearning,2017, Proceedings of the 2017 Conference on Empirical Methods in Natural LanguageProcessing
 Neural machine translation of rare words withsubword units,2016, In ACL
 Latent-variable non-autoregressiveneural machine translation with deterministic inference using a delta posterior,2019, arXiv preprint1908
 Insertion transformer: Flexiblesequence generation via insertion operations,2019, CoRR
 Bidirectional beam search: Forward-backward inference inneural sequence models for fill-in-the-blank image captioning,2017, 2017
 Attention is all you need,2017, In NIPS
 Semi-autoregressive neural machine translation,2018, arXivpreprint arXiv:1808
 Non-autoregressivemachine translation with auxiliary regularization,2019, arXiv preprint 1902
 Non-monotonic sequential textgeneration,2019, CoRR
