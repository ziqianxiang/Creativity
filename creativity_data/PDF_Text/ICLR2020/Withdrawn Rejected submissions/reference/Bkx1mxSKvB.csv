title,year,conference
 Onexact computation With an infinitely Wide neural net,2019, arXiv preprint arXiv:1904
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 A mean field theory of quantized deep networks:The quantization-depth trade-off,2019, arXiv preprint arXiv:1906
 Dynamical isometry and a mean fieldtheory of RNNs: Gating enables signal propagation in recurrent neural networks,2018, In InternationalConference on Machine Learning
 Autoaugment:Learning augmentation strategies from data,2019, In The IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 Dynamical isometry and a mean field theory of lstms and grus,2019, CoRR
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In International Conference on Artificial Intelligence and Statistics
 Finite depth and width corrections to the neural tangent kernel,2019, arXivpreprint arXiv:1909
 On the selection of initialization and activa-tion function for deep neural networks,2018, arXiv preprint arXiv:1805
 Dynamics of deep neural networks and neural tangent hier-archy,2019, arXiv preprint arXiv:1909
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems 31
 Universal statistics of fisher information indeep neural networks: mean field approach,2018, arXiv preprint arXiv:1806
 Deep neural networks as gaussian processes,2018, In International Conference on LearningRepresentations
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Bayesian deep convolutional networks withmany channels are gaussian processes,2019, In International Conference on Learning Representations
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In Advances In NeuralInformation Processing Systems
 Transfusion: Understandingtransfer learning with applications to medical imaging,2019, arXiv preprint arXiv:1902
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Deep informationpropagation,2017, International Conference on Learning Representations
 Mean field residual networks: On the edge of chaos,2017, In Advancesin Neural Information Processing Systems
 Amean field theory of batch normalization,2019, arXiv preprint arXiv:1902
