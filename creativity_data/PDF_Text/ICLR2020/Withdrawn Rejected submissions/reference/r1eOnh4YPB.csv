title,year,conference
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National AcademyOfSciences
 Understanding Batch Normal-ization,2018, In NIPS
 Adaptive subgradient methods for online learning andstochastic optimization,2011, JMLR
 Caltech-256 object category dataset,2007, 2007
 Deep residual learning for image recog-nition,2016, In CVPR
 Densely connectedconvolutional networks,2017, In CVPR
 Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift,2015, In ICML
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InICLR
 Adam: A Method for Stochastic Optimization,2015, In ICLR
 Second Order Properties of Error Surfaces: LearningTime and Generalization,1991, In NIPS
 Towards Explaining the Regularization Effect of InitialLarge Learning Rate in Training Neural Networks,2019, In NeurIPS
 Rethinking the value ofnetwork pruning,2019, In ICLR
 Challenging Common Assumptions in the Unsupervised Learn-ing ofDisentangled Representations,2019, In ICML
 Sgdr: Stochastic gradient descent with warm restarts,2017, In ICLR
 Adaptive Gradient Methods with DynamicBound of Learning Rate,2019, In ICLR
 SGD on Neural Networks Learns Functions of Increasing Complex-ity,2019, In NeurIPS
 Learning and Transferring Mid-LevelImage Representations using Convolutional Neural Networks,2014, In CVPR
 On the difficulty of training recurrent neuralnetworks,2013, InICML
 Recognizing indoor scenes,2009, In CVPR
 Transfusion: UnderstandingTransfer Learning for Medical Imaging,2019, In NeurIPS
 On the Convergence of Adam and Beyond,2018, InICLR
 Imagenet large scale visualrecognition challenge,2015, IJCV
 Cyclical learning rates for training neural networks,2017, In WACV
 Sequence to Sequence Learning with Neural Net-works,2014, In NIPS
 The MarginalValue of Adaptive Gradient Methods in Machine Learning,2017, In NIPS
 Hessian-based Anal-ysis of Large Batch Training and Robustness to Adversaries,2018, In NIPS
 Wide Residual Networks,2016, In BMVC
 ADADELTA: An Adaptive Learning Rate Method,2012, arXiv:1212
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
