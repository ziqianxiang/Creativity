title,year,conference
 The early phase of neural network training,2020, In Submitted to International Conferenceon Learning Representations
 Picking winning tickets before training by preserving gradient flow,2020, In Submittedto International Conference on Learning Representations
 Winning the lottery with continuous sparsification,2020, In Submitted to InternationalConference on Learning Representations
 Essentially no bar-riers in neural network energy landscape,2018, In International Conference on Machine Learning
 Gradient descent happens in a tiny subspace,2018, arXivpreprint arXiv:1812
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InInternational Conference on Learning Representations
 Optimal brain damage,1990, In Advances in NeuralInformation Processing Systems
 SNIP: Single-shot Network Pruningbased on Connection Sensitivity,2019, In International Conference on Learning Representations
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 An exponential learning rate schedule for deep learning,2019, arXivpreprint arXiv:1910
 Rethinking the Value ofNetwork Pruning,2019, In International Conference on Learning Representations
 One ticket to win them all:generalizing lottery ticket initializations across datasets and optimizers,2019, 2019
 Pruning algorithms-a survey,1993, IEEE transactions on Neural Networks
 A bayesian perspective on generalization and stochastic gra-dient descent,2018, In International Conference on Learning Representations
