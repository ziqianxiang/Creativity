title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Vicinal risk minimization,2001, InAdvances in neural information processing systems
 Invariance reduces variance: Understanding dataaugmentation in deep learning and beyond,2019, arXiv preprint arXiv:1907
 Akernel theory of modern data augmentation,2018, arXiv preprint arXiv:1803
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Graph neural tangent kernel: Fusing graph neural networks with graph kernels,2019, ArXiv
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Deep convolutional net-works as shallow gaussian processes,2019, In International Conference on Learning Representations
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv preprint arXiv:1806
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Network in network,2013, arXiv preprint arXiv:1312
 Bayesian deep convolutional networks with many chan-nels are gaussian processes,2019, In International Conference on Learning Representations
 Creating artificial neural networks that generalize,1991, Neuralnetworks
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
