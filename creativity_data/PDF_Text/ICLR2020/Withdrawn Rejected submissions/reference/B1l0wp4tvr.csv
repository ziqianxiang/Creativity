title,year,conference
 Learning representations for neural network-based classificationusing the information bottleneck principle,0162, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Infinitely divisible matrices,1930, The American Mathematical Monthly
 Adaptive estimators show informationcompression in deep neural networks,2019, In International Conference on Learning Representations
 On kernel-target align-ment,2002, In Advances in neural information processing systems
 Measures of entropy fromdata using infinitely divisible kernels,2012, IEEE Transactions on Information Theory
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Yee Whye Teh and Mike Titterington (eds
 Estimating information flow in deep neural networks,2019, In KamalikaChaudhuri and Ruslan Salakhutdinov (eds
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, 2015 IEEE International Conference onComputer Vision (ICCV)
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Caveats for information bottleneckin deterministic scenarios,2019, In International Conference on Learning Representations
 Input feature selection by mutual information based on parzen win-dow,0162, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Kernel meanembedding of distributions: A review and beyond,2017, Foundations and Trends in Machine Learning
 Estimation of entropy and mutual information,0899, Neural Comput
 On estimation of a probability density function and mode,1962, Ann
 On the information bottleneck theory of deeplearning,2018, In International Conference on Learning Representations
 Normalized cuts and image segmentation,2000, IEEE Transactions onPattern Analysis and Machine Intelligence
 Data spectroscopy: Eigenspaces of convolution operatorsand clustering,2009, The Annals of Statistics
 Opening the black box of deep neural networks via informa-tion,2017, CoRR
 A kernel-based framework totensorial data analysis,2011, Neural networks
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Deep learning and the information bottleneck principle,2015, In 2015 IEEEInformation Theory Workshop (ITW)
 Multivariate extension of matrix-based renyiâ€™s -order entropy functional,2019, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Understanding autoencoders with information theoretic con-cepts,0893, Neural Networks
 Understanding Convo-lutional neural network training with information theory,2018, CoRR
	Batch normalization layer4,1024,	Fully connected layer with 1024 inputs and 20 outputs
