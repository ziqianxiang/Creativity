title,year,conference
 Learning from noisy examples,1988, Machine Learning
 A closer lookat memorization in deep netWorks,2017, In ICML
 ToWards instance-dependent label noise-tolerant classification: a probabilistic approach,2018, Pattern Analysis and Applications
 On Symmetric Losses forLearning from Corrupted Labels,2019, ICML
 Learning Withbounded instance-and label-dependent label noise,2017, stat
 Are humans good intuitive statisticians after all? rethinking someconclusions from the literature on judgment under uncertainty,1996, Cognition
 Modelling class noise With symmetric and asymmetric distributions,2015, InAAAI
 Robust loss functions under label noise for deepneural netWorks,2017, In AAAI
 Training deep neural-netWorks using a noise adaptationlayer,2017, In ICLR
 Who said What: Modelingindividual labelers improves classification,2018, In AAAI
 On calibration of modern neuralnetWorks,2017, In ICML
 Co-teaching: Robust training of deep neural netWorks With extremely noisy labels,2018, InNeurIPS
 Binary classification from positive-confidencedata,2018, In NeurIPS
 Mentornet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In ICML
 Learning from noisy singly-labeleddata,2018, In ICLR
 Temporal Ensembling for Semi-Supervised Learning,2017, ICLR
 Dimensionality-driven learning with noisy labels,2018, In ICML
 Noise tolerance under risk minimization,2013, IEEE Transactions onCybernetics
 Learning from cor-rupted binary labels via class-probability estimation,2015, In ICML
 Learning from binarylabels with instance-dependent corruption,2016, arXiv preprint arXiv:1605
 Learning from binary labelswith instance-dependent noise,2018, Machine Learning
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Learning withNoisy Labels,2013, In NeurIPS
 Predicting good probabilities with supervised learn-ing,2005, In ICML
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Supervised learning from multiple experts: whom to trustwhen everyone lies a bit,2009, In ICML
 Training deep neural networks on noisy labels with bootstrapping,2015, ICLR
 Classification with asymmetric label noise:Consistency and maximal denoising,2013, In COLT
 Learning with bad training data via iterative trimmed loss mini-mization,2019, In ICML
 Cheap and fast - but is it good?evaluating non-expert annotations for natural language tasks,2008, In EMNLP
 Learning SVMs from sloppily labeled data,2009, In InternationalConference on Artificial Neural Networks
 Trainingconvolutional networks with noisy labels,2015, ICLR workshop
 Joint optimization frame-work for learning with noisy labels,2018, In CVPR
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In NeurIPS
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 Modeling annotator expertise: Learning when everybody knows a bit ofsomething,2010, In AISTATS
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 mixup: Beyond empiricalrisk minimization,2018, ICLR
 Generalized Cross Entropy Loss for Training Deep Neural Net-works with Noisy Labels,2018, In NeurIPS
