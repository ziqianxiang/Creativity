title,year,conference
 Massively multilingual sentence embeddings for zero-shotcross-lingual transfer and beyond,2019, Transactions of the Association for Computational Linguistics
 Enhanced LSTMfor natural language inference,2017, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 XNLI: Evaluating cross-lingual sentence representations,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Cross-language knowledge transfer using multilin-gual deep neural network with shared hidden layers,2013, In 2013 IEEE International Conference onAcoustics
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Exploiting similarities among languages for ma-chine translation,2013, arXiv preprint arXiv:1309
 Automatic differentiation inpytorch,2017, 2017
 Learning joint multilingual sentence representations withneural machine translation,2017, arXiv preprint arXiv:1704
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 What do you learn fromcontext? probing for sentence structure in contextualized word representations,2019, In InternationalConference on Learning Representations
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 The united nations parallel cor-pus v1,2016, 0
