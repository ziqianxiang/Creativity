title,year,conference
 Convergence guarantees for rm-sprop and adam in non-convex optimization and their comparison to nesterov acceleration onautoencoders,2018, arXiv preprint arXiv:1807
 On the convergence of a class of adam-typealgorithms for nonconvex optimization,2018, arXiv preprint arXiv:1808
 Stochastic algorithms with geo-metric step decay converge linearly on sharp functions,2019, arXiv preprint arXiv:1907
 Imagenet: A large-scalehierarchical image database,2009, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 Incorporating nesterov momentum into adam,2016, 2016
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Long short-term memory,1997, Neural computation
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 On the convergence of stochastic gradient descent with adaptivestepsizes,2018, arXiv preprint arXiv:1805
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Variants of rmsprop and adagrad with logarithmicregret bounds,2017, In International Conference on Machine Learning
 Stochastic variancereduction for nonconvex optimization,2016, In International conference on machine learning
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In Advances in Neural Information Processing Systems
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Understanding short-horizon bias instochastic meta-optimization,2018, In International Conference on Learning Representations
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR)
 Accelerate stochastic subgradient method by leveraginglocal groWth condition,2016, arXiv preprint arXiv:1607
 Wide residual netWorks,2016, arXiv preprintarXiv:1605
 Adaptivemethods for nonconvex optimization,2018, In Advances in Neural Information Processing Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Normalized direction-preserving adam,2017, arXivpreprint arXiv:1709
 On the convergence of adagrad with momentum for training deep neuralnetworks,2018, arXiv preprint arXiv:1808
