title,year,conference
 Greedy layer-wise trainingof deep networks,2007, In Advances in neural information processing systems
 Quora question pairs,2018, Quora
 What does bert lookat? an analysis of bertâ€™s attention,2019, arXiv preprint arXiv:1906
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Efficient sequence learningwith group recurrent networks,2018, In Proceedings of the 2018 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Limits of end-to-end learning,2017, arXiv preprint arXiv:1704
 Bridging nonlinearities and stochastic regularizers with gaussianerror linear units,2016, arXiv
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 A fast learning algorithm for deep beliefnets,2006, Neural computation
 Searching for mobilenetv3,2019, arXivpreprint arXiv:1905
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Tensorized embed-ding layers for efficient model compression,2019, arXiv preprint arXiv:1901
 Sequence-level knowledge distillation,2016, arXiv preprintarXiv:1606
 Factorization tricks for lstm networks,2017, arXiv preprintarXiv:1703
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, arXiv preprint arXiv:1808
 Large memory layers with product keys,2019, CoRR
 Roberta: A robUstly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Face model compression bydistilling knowledge from neUrons,2016, In Thirtieth AAAI Conference on Artificial Intelligence
 On the downstream performance of com-pressed word embeddings,2019, arXiv preprint arXiv:1909
 Deep contextUalized word representations,2018, arXiv preprint arXiv:1802
 LangUagemodels are UnsUpervised mUltitask learners,2019, OpenAI Blog
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Q-bert: Hessian based ultra low precision quantization of bert,2019, arXiv preprintarXiv:1909
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Smalland practical bert models for sequence labeling,2019, arXiv preprint arXiv:1909
 Well-read students learn better:The impact of student initialization on knowledge distillation,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Extracting andcomposing robust features with denoising autoencoders,2008, In Proceedings of the 25th internationalconference on Machine learning
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Large batch optimization for deep learning: Trainingbert in 76 minutes,2019, arXiv preprint arXiv:1904
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
