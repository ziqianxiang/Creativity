title,year,conference
 Towards an adversarially robust normalization approach,2020, In Submitted to InternationalConference on Learning Representations
   Obfuscated  gradients  give  a  false  senseof  security:  Circumventing  Defenses  to  Adversarial  Examples,2018,   In  Proceedings  of  the  35thInternational Conference on Machine Learning
  Understanding Batch Nor-malization,2018,  In Advances in Neural Information Processing Systems 31
  A Quantitative Analysis of the Effect of BatchNormalization on Gradient Descent,2019,  In Proceedings of the 36th International Conference onMachine Learning
 Towards evaluating the robustness of neural networks,2017, In IEEESymposium on Security and Privacy
 AdverTorch v0,2019,1: An Adversarial RobustnessToolbox based on PyTorch
 Adversarial Spheres,2018, In International Conference on Learning RepresentationsWorkshop Track
 Delving Deep into Rectifiers: SurpassingHuman-Level Performance on ImageNet Classification,2015, In International Conference on ComputerVision
  Benchmarking Neural Network Robustness to CommonCorruptions and Perturbations,2019, In International Conference on Learning Representations
  Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Excessive InvarianceCauses Adversarial Vulnerability,2019,   In International Conference on Learning Representations
  Why the logistic function?  A tutorial discussion on probabilities and neuralnetworks,1995, MIT Computational Cognitive Science Report
 Characterizing Well-Behaved vs,2019, Pathological Deep Neural Networks
  Towards Explaining the Regularization Effect of InitialLarge Learning Rate in Training Neural Networks,2019,  In Neural Information Processing Systems
   MNIST-C:  A  robustness  benchmark  for  computer  vision,2019,   InInternational Conference on Machine Learning Workshop on Uncertainty and Robustness in DeepLearning
 Practical Black-Box Attacks Against Machine Learning,2017, In Asia Conference on Computerand Communications Security
 Very Deep Convolutional Networks for Large-Scale ImageRecognition,2015, In International Conference on Learning Representations
  Deep Inside Convolutional Networks:Visualising Image Classification Models and Saliency Maps,2014,  In International Conference onLearning Representations Workshop
 The Implicit Bias of GradientDescent on Separable Data,2018, In International Conference on Learning Representations
 Is Robustness theCost of Accuracy? – A Comprehensive Study on the Robustness of 18 Deep Image ClassificationModels,2018, In Computer Vision – ECCV 2018
 Intriguing properties of neural networks,2014, In International Conference on LearningRepresentations
   A Boundary Tilting Persepective on the Phenomenon ofAdversarial Examples,2016, arXiv:1608
 Ro-bustness May Be at Odds with Accuracy,2019, In International Conference on Learning Representations
 L2 Regularization versus Batch and Weight Normalization,2017, arXiv:1706
 Three Mechanisms of Weight DecayRegularization,2019, In International Conference on Learning Representations
 Residual Learning Without Normalization viaBetter Initialization,2019, In International Conference on Learning Representations
