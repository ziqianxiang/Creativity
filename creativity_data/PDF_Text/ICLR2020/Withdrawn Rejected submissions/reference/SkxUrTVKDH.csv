title,year,conference
 Structural compression of convolutional neural networks based ongreedy filter pruning,2018, arXiv preprint arXiv:1705
 Learning the number of neurons in deep networks,2016, In NIPS
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Convergence of the linearized bregman iteration forl1-norm minimization,2009, Mathematics of Computation
 Memory bounded deep convolutional networks,2014, In arXivpreprint arXiv:1412
 An introduction to o-minimal geometry,2001, RAAG Notes
 Gradient descent finds globalminima of deep neural networks,2018, 2018
 Visualizing higher-layerfeatures of a deep network,2009, University of Montreal
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In ICCV
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 A unified dynamic approach to sparse model selection,2018, In The 21stInternational Conference on Artificial Intelligence and Statistics (AISTATS)
 Split lbi: An iterative regularization pathwith structural sparsity,2016, In D
 Boosting with structural sparsity: Adifferential inclusion approach,2018, Applied and Computational Harmonic Analysis
 Speeding up convolutional neural networkswith low rank expansions,2014, In BMVC
 Adam: A method for stochastic optimization,2015, In ICLR
 A primer of real analyticfunctions,2012, Birkhauser
 Pruning filters forefficient convnets,2017, In ICLR
 Learningefficient convolutional networks through network slimming,1965, In ICCV
 A mean field view of the landscape oftwo-layers neural network,2018, Proceedings of the National Academy of Sciences (PNAS)
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, Conference on Learning Theory (COLT)
 Variational analysis and generalized differentiation I: Basic Theory,2006, Springer
 The roleof over-parametrization in generalization of neural networks,2019, In International Conference onLearning Representations (ICLR)
 Sparse recovery via differentialinclusions,2016, Applied and Computational Harmonic Analysis
 Variational analysis,2013, Grundlehren Math
 Striving forsimplicity: The all convolutional net,2017, arXiv preprint arXiv:1412
 Learning the number of neurons indeep networks,2016, In NIPS
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, In IJCAI 2018
 Global convergence of block coordinatedescent in deep learning,2017, In Proceedings of the 36th International Conference on Machine Learning
 Accelerating very deep convolutionalnetworks for classification and detection,2016, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Msplit lbi: Realizing featureselection and dense estimation simultaneously in few-shot and zero-shot learning,2018, In InternationalConference on Machine Learning (ICML)
 On model selection consistency of lasso,2006, J
 Trained ternary quantization,2017, ICLR
