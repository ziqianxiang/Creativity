title,year,conference
 Deep variational informationbottleneck,2016, arXiv preprint arXiv:1612
 Infogan:Interpretable representation learning by information maximizing generative adversarial nets,2016, InAdvances in neural information processing systems
 Meta-learning and universality: Deep representations and gradientdescent can approximate any learning algorithm,2017, arXiv preprint arXiv:1710
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, arXiv preprint arXiv:1703
 Transferringknowledge across learning processes,2018, arXiv preprint arXiv:1812
 Infobot: Transfer and exploration via the informationbottleneck,2019, arXiv preprint arXiv:1901
 Uncertainty autoencoders: Learning compressed representationsvia variational information maximization,2018, arXiv preprint arXiv:1812
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Gradient-based meta-learning with learned layerwise metric andsubspace,2018, 2018
 Transductive propagation networkfor few-shot learning,2018, arXiv preprint arXiv:1805
 Meta-learning updaterules for unsupervised representation learning,2018, arXiv preprint arXiv:1804
 A simple neural attentive meta-learner,2017, arXiv preprint arXiv:1707
 Nofuss distance metric learning using proxies,2017, In Proceedings of the IEEE International Conferenceon Computer Vision
 Reptile: a scalable metalearning algorithm,2018, arXiv preprintarXiv:1803
 Cartesian k-means,2013, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 Optimization as a model for few-shot learning,2016, 2016
 Generating diverse high-fidelity images withvq-vae-2,2019, arXiv preprint arXiv:1906
 Discrete variational aUtoencoders,2016, arXiv preprint arXiv:1609
 Meta-learning with latent embedding optimization,2018, arXiv preprintarXiv:1807
 Learning and generalization with the informationbottleneck,2010, Theoretical Computer Science
 Prototypical networks for few-shot learning,2017, InAdvances in Neural Information Processing Systems
 Improved deep metric learning with multi-class n-pair loss objective,2016, In Advances inNeural Information Processing Systems
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Deep learning and the information bottleneck principle,2015, In 2015IEEE Information Theory Workshop (ITW)
 Meta-dataset: A datasetof datasets for learning to learn from few examples,2019, arXiv preprint arXiv:1903
 Neural discrete representation learning,2017, In Advances inNeural Information Processing Systems
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Sampling matters indeep embedding learning,2017, In Proceedings of the IEEE International Conference on ComputerVision
 Caml:Fast context adaptation via meta-learning,2018, arXiv preprint arXiv:1810
