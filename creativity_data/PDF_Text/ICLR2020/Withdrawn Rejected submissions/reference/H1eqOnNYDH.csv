title,year,conference
 Data augmentation generative adversarialnetworks,2017, arXiv preprint arXiv:1711
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Dropout as data augmen-tation,2015, arXiv preprint arXiv:1506
 Multitask learning,1998, In Learning to learn
 Invariance reduces variance: Understanding dataaugmentation in deep learning and beyond,2019, arXiv preprint arXiv:1907
 Big neural networks waste capacity,2013, In International Conferenceon Learning Representations
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Dataset augmentation in feature space,2017, In InternationalConference on Learning Representations
 Regularization theory and neural networksarchitectures,1995, Neural computation
 Deep learning,2016, MIT press
 Fractional max-pooling,2014, arXiv preprint arXiv:1412
 Mixup as locally linear out-of-manifold regulariza-tion,2018, arXiv preprint arXiv:1809
 Comparing biases for minimal network construction withback-propagation,1989, In Advances in Neural Information Processing Systems
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Further advantages of data augmentation on convolutionalneural networks,2018, In International Conference on Artificial Neural Networks
 Deep neural networks trained with heavier data augmentation learn features closer torepresentations in hIT,2018, In Conference on Cognitive Computational Neuroscience
 Deep networks withstochastic depth,2016, In European Conference on Computer Vision
 Densely connectedconvolutional networks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Fractalnet: Ultra-deep neural networkswithout residuals,2016, arXiv preprint arXiv:1605
 Smart augmentation-learning an optimaldata augmentation strategy,2017, IEEE Access
 Regularization techniques to improve generalization,2012, In Neural Networks:Tricks of the Trade
 In search of the real inductive bias: Onthe role of implicit regularization in deep learning,2014, In International Conference on LearningRepresentations
 The effectiveness of data augmentation in image classification usingdeep learning,2017, arXiv preprint arXiv:1712
 Tangent prop-a formalism forspecifying selected invariances in an adaptive network,1992, In Advances in Neural InformationProcessing Systems
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Striving forsimplicity: The all convolutional net,2014, In International Conference on Learning Representations
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 On the uniform convergence of relative frequencies of eventsto their probabilities,1971, Theory of Probab
 Dropout training as adaptive regularization,2013, InAdvances in neural information processing systems
 Wide residual networks,2016, In British Machine VisionConference
 Understand-ing deep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations
9 and learning rate of 0,2015,1
