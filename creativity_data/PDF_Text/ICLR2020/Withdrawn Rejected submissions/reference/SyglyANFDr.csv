title,year,conference
 On the convergence rate of training recurrent neuralnetworks,2018, arXiv preprint arXiv:1810
 A convergence theory for deep learning via over-parameterization,2019, In ICML
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks of the trade
 An adaptive sampling schemeto efficiently train fully convolutional networks for semantic segmentation,2018, In Annual Conferenceon Medical Image Understanding and Analysis
 Optimization methods for large-scale machinelearning,2018, Siam Review
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Active bias: Training more accu-rate neural networks by emphasizing high variance samples,2017, In Advances in Neural InformationProcessing Systems
 General risk measures for robustmachine learning,2019, arXiv preprint arXiv:1904
 Information theory and statistics: A tutorial,2004, Foundations andTrendsR in Communications and Information Theory
 Statistics of robust optimization: A generalizedempirical likelihood approach,2016, arXiv preprint arXiv:1610
 Niftynet: a deep-learning platform for medicalimaging,2018, Computer methods and programs in biomedicine
 Robust empirical optimization is almostthe same as mean-variance optimization,2018, Operations research letters
 Minmax optimization: Stable limit points ofgradient descent ascent are locally optimal,2019, arXiv preprint arXiv:1902
 On gradient descent ascent for nonconvex-concaveminimax problems,2019, arXiv preprint arXiv:1906
 Proximite et dualite dans Un espace hilbertien,1965, Bulletin de la Societemathematique de France
 Non-convex min-max optimization:Provable algorithms and applications in machine learning,2018, arXiv preprint arXiv:1810
 Minimizing the maximal loss: How and why,2016, In ICML
 Training region-based object detectorswith online hard example mining,2016, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Certifying some distributional robustness withprincipled adversarial training,2017, arXiv preprint arXiv:1710
 Distributionally robust deep learning as a generalization ofadversarial training,2017, In NIPS workshop on Machine Learning and Computer Security
 Wide residual networks,2016, arXiv preprintarXiv:1605
 An improved analysis of training over-parameterized deep neuralnetworks,2019, arXiv preprint arXiv:1906
 Similarly to Theorem B,2019,1
1 Proof of technical lemma 2Using Lemma 4,2019,2 and Lemma 4
