title,year,conference
 Extremely large minibatch sgd: training resnet-50on imagenet in 15 minutes,2017, arXiv preprint arXiv:1711
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Understanding batch normaliza-tion,2018, In Advances in Neural Information Processing Systems
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 Big batch sgd: Automated inferenceusing adaptive batch sizes,2016, arXiv preprint arXiv:1610
 Why momentum really works,2017, Distill
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Stochastic gradientdescent escapes saddle points efficiently,2019, arXiv preprint arXiv:1902
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 On the insufficiency of existingmomentum schemes for stochastic optimization,2018, In 2018 Information Theory and ApplicationsWorkshop (ITA)
 One Weird trick for parallelizing convolutional neural netWorks,2014, arXiv preprintarXiv:1404
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Mass: an accelerated stochastic method for over-parametrizedlearning,2018, arXiv preprint arXiv:1810
 The poWer of interpolation: Understanding theeffectiveness of sgd in modern over-parametrized learning,2017, arXiv preprint arXiv:1712
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 Momentum and optimal stochastic search,1994, In Proceedings of the1993 Connectionist Models Summer School
 The effect of netWorkWidth on stochastic gradient descent and generalization: an empirical study,2019, arXiv preprintarXiv:1905
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Measuring the effects of data parallelism on neural network training,2018, arXiv preprintarXiv:1811
 A tail-index analysis of stochastic gradientnoise in deep neural networks,2019, arXiv preprint arXiv:1901
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Image classification atsupercomputer scale,2018, arXiv preprint arXiv:1811
 Imagenet training inminutes,2018, In Proceedings of the 47th International Conference on Parallel Processing
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Recurrent neural network regularization,2014, arXivpreprint arXiv:1409
 Which algorithmic choices matter at which batch sizes? insightsfrom a noisy quadratic model,2019, arXiv preprint arXiv:1907
 Equation 5 holds so long as the gradient of each training example is anindependent and uncorrelated sample from an underlying short tailed distribution,1024, Additionally
