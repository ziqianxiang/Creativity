title,year,conference
 The fifth pascal recognizingtextual entailment challenge,2009, In TAC
 What does bert lookat? an analysis of bertâ€™s attention,2019, arXiv preprint arXiv:1906
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Cognitive graph for multi-hopreading comprehension at scale,2019, In Proceedings of the 57th Conference of the Association forComputational Linguistics
 Compressing deep convolutional net-works using vector quantization,2014, arXiv preprint arXiv:1412
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Sequence-level knowledge distillation,2016, In Proceedings of the2016 Conference on Empirical Methods in Natural Language Processing(EMNLP)
 Revealing the dark secretsof bert,2019, arXiv preprint arXiv:1908
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing(EMNLP)
 Patient knowledge distillation for bert modelcompression,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing(EMNLP)
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Attention is all you need,2017, In Advances in neural informationprocessing systems(NeurIPS)
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 Conference on Empirical Methods in Natural Language Processing(EMNLP)
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics(NAACL)
