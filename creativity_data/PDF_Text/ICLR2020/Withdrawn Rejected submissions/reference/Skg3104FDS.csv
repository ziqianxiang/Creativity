title,year,conference
 Parameter adaptation in stochasticoptimization,1998, In D
 Unsupervised learning,1989, Neural Computation
 Online learning rateadaptation with hypergradient descent,2017, CoRR
 Towards a discrete newton method with memory for large-scale optimization,1996, Nonlinear Optimization and Applications
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Openai baselines,2017, GitHub
 Adaptive subgradient methods for online learning and stochasticoptimization,1532, J
 Reinforcement learning for improving agent design,2018, arXiv preprint arXiv:1810
 Deep residual learning for image recognition,2015, CoRR
 Linear convergence of gradient and proximal-gradientmethods under the Polyak-IojasieWicz condition,2016, CoRR
 Learning multiPle layers of features from tiny images,2009, Technical rePort
 MNIST handWritten digit database,2010, 2010
 Random synaPtic feedback WeightssuPPort error backProPagation for deeP learning,2016, Nature Communications
 Second-Order Optimization for Neural Networks,2016, PhD thesis
 OPtimizing neural netWorks With kronecker-factored aPProximatecurvature,2015, CoRR
 Gradient methods for the minimisation of functionals,1963, USSR Computational Mathematicsand Mathematical Physics
 Some methods of sPeeding uP the convergence of iteration methods,1964, Ussr ComputationalMathematics and Mathematical Physics
 Proximal Policy oPtimizationalgorithms,2017, arXiv preprint arXiv:1707
 Recurrent deterministic Policy gradient method forbiPedal locomotion on rough terrain challenge,2018, In 2018 15th International Conference on Control
 Reinforcement Learning: An Introduction,2018, The MIT Press
 Lecture 6,2012,5â€”RmsProP: Divide the gradient by a running average of itsrecent magnitude
 A theory of maximizing sensory information,1992, Biological Cybernetics
