title,year,conference
  Label refinery:Improving imagenet classification through label progression,2018,  arXiv preprint arXiv:1805
  Curriculum learning,2009,  InProceedings of the 26th annual international conference on machine learning
   Hard negative mining for metric learningbased zero-shot classification,2016,  In Gang Hua and Herve´ Je´gou (eds
  Distributed deep learning using syn-chronous stochastic gradient descent,2016, arXiv preprint arXiv:1602
  Thedifficulty of training deep architectures and the effect of unsupervised pre-training,2009,  In ArtificialIntelligence and Statistics
  Learning to teach,2018,  arXiv preprintarXiv:1805
  Reverse cur-riculum generation for reinforcement learning,2017, arXiv preprint arXiv:1707
  Densely connectedconvolutional networks,2017,  In Proceedings of the IEEE conference on computer vision and patternrecognition
    Mentornet:   Learningdata-driven  curriculum  for  very  deep  neural  networks  on  corrupted  labels,2017,    arXiv  preprintarXiv:1712
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 An empir-ical evaluation of deep architectures on problems with many factors of variation,2007,  In Proceedingsof the 24th international conference on Machine learning
  Boot-strapping visual categorization with relevant negatives,2013, IEEE Transactions on Multimedia
  Drop-activation:  Implicit parameter reductionand harmonic regularization,2018, arXiv preprint arXiv:1811
   Distributionalsmoothing with virtual adversarial training,2015, arXiv preprint arXiv:1507
 Regularizingneural networks by penalizing confident output distributions,2017,  arXiv preprint arXiv:1701
   icarl:Incremental classifier and representation learning,2017,   In Proceedings of the IEEE conference onComputer Vision and Pattern Recognition
  Training deep neural networks on noisy labels with bootstrapping,2014,  arXiv preprintarXiv:1412
  Experiencereplay for continual learning,2018, arXiv preprint arXiv:1811
 Reinforcement today,1958, American Psychologist
  Rethink-ing the inception architecture for computer vision,2016,   In Proceedings of the IEEE conference oncomputer vision and pattern recognition
  Curriculum learning by transfer learning:  Theoryand experiments with deep networks,2018, arXiv preprint arXiv:1802
   Disturblabel:  Regularizingcnn on the loss layer,2016,  In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
  Shakedrop regularizationfor deep residual learning,2018, arXiv preprint arXiv:1802
   Wide  residual  networks,2016,   In  British  Machine  VisionConference 2016
  Residual networks ofresidual networks: Multilevel residual networks,2017,  IEEE Transactions on Circuits and Systems forVideo Technology
0Early Exaggeration           12,1000,0Lr                                      200
