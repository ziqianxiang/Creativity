title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 State-of-the-artspeech recognition with sequence-to-sequence models,2018, In 2018 IEEE International Conference onAcoustics
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Long short-term memory,1997, Neural computation
 Deep reinforcement learning: An overview,2017, arXiv preprint arXiv:1701
 Generating wikipedia by summarizing long sequences,2018, arXiv preprint arXiv:1801
 Scaling neural machine translation,2018, InProceedings of the Third Conference on Machine Translation: Research Papers
 Improving language under-standing by generative pre-training,2018, Technical report
 Languagemodels are unsupervised multitask learners,2019, Technical report
 Token-levelensemble distillation for grapheme-to-phoneme conversion,2019, arXiv preprint arXiv:1904
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Tensor2tensor for neural machine translation,2018, CoRR
 Non-local neural networks,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Reducing bertpre-training time from 3 days to 76 minutes,2019, arXiv preprint arXiv:1904
 Relational deep reinforcementlearning,2018, arXiv preprint arXiv:1806
 Self-attention generativeadversarial networks,2018, arXiv preprint arXiv:1805
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
