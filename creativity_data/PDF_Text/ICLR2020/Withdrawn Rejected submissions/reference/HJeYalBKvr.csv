title,year,conference
   Neural machine translation by jointlylearning to align and translate,2015, In ICLR
  Bert:  Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Graphrel: Modeling text as relational graphs for jointentity and relation extraction,2019,  In Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
   Building a large annotatedcorpus of english: The penn treebank,1993, 1993
  Languagemodels are unsupervised multitask learners,2019, 2019
 Linguistically-informed self-attention for semantic role labeling,2018,   In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, EMNLP 2018
  Graph convolution over pruned dependencytrees improves relation extraction,2018, In Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing
