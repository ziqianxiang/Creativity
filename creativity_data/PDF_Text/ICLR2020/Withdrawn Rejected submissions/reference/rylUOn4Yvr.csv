title,year,conference
 Variancereduction in sgd by distributed importance sampling,2016, In ICLR Workshop
 A closer look at memorization in deep netWorks,2017, In ICML
 Robust loss functions under label noise for deepneural netWorks,2017, In AAAI
 Training deep neural-netWorks using a noise adaptationlayer,2017, In ICLR
 Adaptive sampling for sgd by exploiting side information,2016, In ICML
 Curriculumnet: Weakly supervised learning from large-scale Web images,2018, InECCV
 Co-teaching: Robust training of deep neural netWorks With extremely noisy labels,2018, InNeurIPS
 Deep residual learning for image recog-nition,2016, In CVPR
 Using trusted data to traindeep netWorks on labels corrupted by severe noise,2018, In NeurIPS
 Batch normalization: Accelerating deep netWork training byreducing internal covariate shift,2015, In ICML
 Caffe: Convolutional architecture for fast feature embed-ding,2014, In ACMMM
 Mentornet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In ICML
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification With deep convo-lutional neural netWorks,2012, In NeurIPS
 Deep nets donâ€™t learn via mem-orization,2017, In ICLR Workshop
 Self-paced learning for latent variablemodels,2010, In NeurIPS
 Design of robust neural netWorkclassifiers,1998, In ICASSP
 Deep spectral clustering learning,2017, In ICML
 Deep learning,2015, Nature
 Cleannet: Transfer learning forscalable image classifier training with label noise,2018, In CVPR
 Learning to learn from noisylabeled data,2019, In CVPR
 Diversity regularized spatiotemporalattention for video-based person re-identification,2018, In CVPR
 Learning fromnoisy labels with distillation,2017, In ICCV
 Focal loss for dense objectdetection,2017, In ICCV
 Quality aware network for set to set recognition,2017, In CVPR
 Online batch selection for faster training of neural networks,2016, InICLR Workshop
 Dimensionality-driven learning with noisy labels,2018, In ICML
" Decoupling ""when to update"" from ""how to update""",2017, InNeurIPS
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE Transactions on PatternAnalysis and Machine Intelligence
 Nofuss distance metric learning using proxies,2017, In ICCV
 Learning withnoisy labels,2013, In NeurIPS
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Training deep neural networks on noisy labels with bootstrapping,2015, In ICLR Workshop
 Training region-based object detectorswith online hard example mining,2016, In CVPR
 Parsing natural scenes and naturallanguage with recursive neural networks,2011, In ICML
 Learning from noisy labels with deep neural networks,2014, arXivpreprint arXiv:1406
 Going deeper with convolutions,2015, InCVPR
 Joint optimization frame-work for learning with noisy labels,2018, In CVPR
 Combating label noise in deep learning using abstention,2019, In ICML
 Learning with symmetric labelnoise: The importance of being unhinged,2015, In NeurIPS
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In CVPR
 Deep metriclearning by online soft mining and class-aware attention,2019, In AAAI
 Improving MAE against CCEunder label noise,2019, arXiv preprint arXiv:1903
 Symmetric crossentropy for robust learning with noisy labels,2019, In ICCV
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 mixup: Beyond empiricalrisk minimization,2018, ICLR
 Heated-up softmaxembedding,2018, arXiv preprint arXiv:1809
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
 Mars: Avideo benchmark for large-scale person re-identification,2016, In ECCV
