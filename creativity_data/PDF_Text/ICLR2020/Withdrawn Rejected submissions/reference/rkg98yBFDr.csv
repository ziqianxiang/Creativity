title,year,conference
 Mine: mutual information neural estimation,2018, arXiv preprintarXiv:1801
 Decision-based adversarial attacks: Reliableattacks against black-box machine learning models,2017, arXiv preprint arXiv:1712
 Large scale gan training for high fidelity naturalimage synthesis,2018, arXiv preprint arXiv:1809
 Proxylessnas: Direct neural architecture search on target taskand hardware,2018, arXiv preprint arXiv:1812
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Generative ensembles for robust anomaly detection,2018, arXiv preprintarXiv:1810
 Asymptotic evaluation of certain markov processexpectations for large time,1983, iv
 Conditional generative models are not robust,2019, arXivpreprint arXiv:1906
 Selective classification for deep neural networks,2017, In Advancesin neural information processing systems
 Adversarial spheres,2018, arXiv preprint arXiv:1801
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Towards deep neural network architectures robust to adversarialexamples,2014, arXiv preprint arXiv:1412
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conferenceon Artificial Intelligence and Statistics
 Deep pyramidal residual networks,2017, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational Bayes,2013, In International Conferenceon Learning Representations
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Simple black-box adversarial perturbationsfor deep networks,2016, arXiv preprint arXiv:1612
 Estimating divergence functionalsand the likelihood ratio by convex risk minimization,2010, IEEE Transactions on Information Theory
 f-gan: Training generative neural samplersusing variational divergence minimization,2016, In Advances in neural information processing systems
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Pixelcnn++: Improving thepixelcnn with discretized logistic mixture likelihood and other modifications,2017, arXiv preprintarXiv:1701
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Ensemble adversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 The constant K in the likelihoodmargin loss JLM is 10,2014, All models are trained 500 epochs
 There are two types of implementations in cleverhans,2016, By defaultrand_init is set to True
