title,year,conference
 Sanitychecks for Saliency maps,2018, In Advances in Neural Information Processing Systems
 Towards robust interpretability with self-explainingneural networks,2018, In Advances in Neural Information Processing Systems
 Interpreting black box models via hypothesistesting,2019, arXiv:1904
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 PCANet: A simpledeep learning baseline for image classification,2015, IEEE Transactions on Image Processing
 Interpreting neuralnetwork classifications with variational dropout saliency maps,2017, In Advances in Neural InformationProcessing Systems
 Explaining imageclassifiers by counterfactual generation,2019, In International Conference on Learning Representations
 Real time image saliency for black box classifiers,2017, In Advances inNeural Information Processing Systems
 Interpretable explanations of black boxes by meaningful perturba-tion,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Counterfactual visualexplanations,2019, International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, In International Conference on Learning Representations (ICLR)
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in Neural Information Processing Systems
 The mythos of model interpretability,2016, arXiv:1606
 Universaladversarial perturbations,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Not just a black box:Learning important features through propagating activation differences,2016, arXiv:1605
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv:1409
 Deep inside convolutional networks:Visualising image classification models and saliency maps,2013, arXiv:1312
 Understanding impacts of high-order lossapproximations and features in deep learning interpretation,2019, arXiv:1902
 Smoothgrad:removing noise by adding noise,2017, arXiv:1706
 Gradients of counterfactuals,2016, arXiv:1611
 Intriguing properties of neural networks,2013, arXiv:1312
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on ComputerVision and Pattern Recognition
 Interpreting neural networks using flip points,2019, arXivpreprint arXiv:1903
 Generative imageinpainting with contextual attention,2018, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
