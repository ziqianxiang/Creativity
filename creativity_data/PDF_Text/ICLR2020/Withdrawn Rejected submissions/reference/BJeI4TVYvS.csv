title,year,conference
 Discriminative learning for differing training and testdistributions,2007, In Proceedings of the 24th international conference on Machine learning
 Two-sample statistics based on anisotropickernels,2017, arXiv preprint arXiv:1709
 Fast two-sample testing withanalytic representations of probability measures,2015, In Advances in Neural Information Processing Systems
 Gradient descent provably optimizes over-parameterized neural networks,2018, arXiv preprint arXiv:1810
 On multivariate goodness-of-fit and two-sample testing,2004, Technical report
 Introduction to modern nonparametric statistics,2003, 2003
 A linear-time kernelgoodness-of-fit test,2017, In Advances in Neural Information Processing Systems
 f -divergence estimation and two-sample homo-geneity test under semiparametric density-ratio models,2011, IEEE Transactions on Information Theory
 f -divergence estimation and two-sample homo-geneity test under semiparametric density-ratio models,2012, IEEE Transactions on Information Theory
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Generative moment matChing networks,2015, In International Conferenceon Machine Learning
 StatistiCal model CritiCism using kernel two sample tests,2015, In Advancesin Neural Information Processing Systems
 Revisiting Classifier two-sample tests,2016, arXiv preprint arXiv:1610
 Linking losses for density ratio and Class-probability estimation,2016, InInternational Conference on Machine Learning
 Learning in impliCit generative models,2016, arXiv preprintarXiv:1610
 f-gan: Training generative neural samplers usingvariational divergenCe minimization,2016, In Advances in neural information processing systems
 On the ConvergenCe of adam and beyond,2019, arXiv preprintarXiv:1904
 Provable approximation properties for deep neuralnetworks,2018, Applied and Computational Harmonic Analysis
 StoChastiC gradient desCent for non-smooth optimization: ConvergenCe resultsand optimal averaging sChemes,2013, In International Conference on Machine Learning
 Density-difference estimation,2013, Neural Computation
 Generative models and model criticism via optimized maximum mean discrepancy,2016, arXivpreprint arXiv:1611
 On the importance of initialization andmomentum in deep learning,2013, In International conference on machine learning
 Error bounds for approximations with deep relu networks,2017, Neural Networks
 Optimal approximation of continuous functions by very deep relu networks,2018, arXiv preprintarXiv:1802
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
