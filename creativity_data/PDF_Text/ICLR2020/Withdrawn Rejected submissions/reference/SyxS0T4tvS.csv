title,year,conference
 Cloze-drivenpretraining of self-attention netWorks,2019, arXiv preprint arXiv:1903
 The second PASCAL recognising textual entailment challenge,2006, 2006
 Thefifth PASCAL recognizing textual entailment challenge,2009, 2009
 A large anno-tated corpus for learning natural language inference,2015, In Empirical Methods in Natural LanguageProcessing (EMNLP)
 KERMIT: Genera-tive insertion-based modeling for sequences,2019, arXiv preprint arXiv:1906
 Xnli: Evaluating cross-lingual sentence representations,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 The PASCAL recognising textual entailmentchallenge,2006, In Machine learning challenges
 Unified language model pre-training for natural language understandingand generation,2019, arXiv preprint arXiv:1905
 neWs-please: A genericneWs craWler and extractor,2017, In Proceedings of the 15th International Symposium of InformationScience
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Race: Large-scale readingcomprehension dataset from examinations,2017, arXiv preprint arXiv:1704
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Improving multi-task deep neu-ral networks via knowledge distillation for natural language understanding,2019, arXiv preprintarXiv:1904
 Multi-task deep neural networksfor natural language understanding,2019, arXiv preprint arXiv:1901
 Mixedprecision training,2018, In International Conference on Learning Representations
 Deep contextualized word representations,2018, In North American Association forComputational Linguistics (NAACL)
 Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks,2018, arXiv preprint arXiv:1811
 Collecting diverse natural language inference problems for sentencerepresentation evaluation,2018, In Proceedings of EMNLP
 Improving language under-standing with unsupervised learning,2018, Technical report
 Languagemodels are unsupervised multitask learners,2019, Technical report
 Choice of plausible alternatives:An evaluation of commonsense causal reasoning,2011, In 2011 AAAI Spring Symposium Series
 Neural machine translation of rare words withsubword units,2016, In Association for Computational Linguistics (ACL)
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Empirical Methods in Natural Language Processing (EMNLP)
 MASS: Masked sequence to sequencepre-training for language generation,2019, In International Conference on Machine Learning (ICML)
 ERNIE: Enhanced representation through knowledgeintegration,2019, arXiv preprint arXiv:1904
 A simple method for commonsense reasoning,2018, arXiv preprintarXiv:1806
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 SuperGLUE: A stickier benchmark for general-purpose languageunderstanding systems,2019, arXiv preprint 1905
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Reducingbert pre-training time from 3 days to 76 minutes,2019, arXiv preprint arXiv:1904
 Defending against neural fake news,2019, arXiv preprint arXiv:1905
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In arXiv preprint arXiv:1506
