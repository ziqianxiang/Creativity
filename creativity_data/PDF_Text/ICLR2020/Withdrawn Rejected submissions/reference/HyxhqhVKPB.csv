title,year,conference
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 A brief tutorial on distributed and concurrent machine learning,2018, In Proceedings of the2018 ACM Symposium on Principles of Distributed Computing
 Tensorflow: a system for large-scalemachine learning,2016, In OSDI
 Mxnet: A flexible and efficient machine learning library forheterogeneous distributed systems,2015, arXiv preprint arXiv:1512
 Scaling distributed machine learning with the parameterserver,2014, In OSDI
 Communication efficient distributedmachine learning with the parameter server,2014, In Advances in Neural Information Processing Systems
 Using MPI-2: Advanced features of the messagepassing interface,1999, MIT press
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Asynchronous decentralized parallel stochasticgradient descent,2017, arXiv preprint arXiv:1710
 D2: Decentralized training overdecentralized data,2018, arXiv preprint arXiv:1803
 Accelerated decentralized optimizationwith local updates for smooth and strongly convex objectives,2018, arXiv preprint arXiv:1810
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in Neural Information ProcessingSystems
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing systems
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 On the convergence of distributed subgra-dient methods under quantization,2018, In 2018 56th Annual Allerton Conference on Communication
 Atomo: Communication-efficient learning via atomic sparsification,2018, In Advances inNeural Information Processing Systems
 Communication compression fordecentralized training,2018, In Advances in Neural Information Processing Systems
 Gradient sparsification for communication-efficient distributed optimization,2018, In Advances in Neural Information Processing Systems
 High-accuracy low-precision training,2018, arXiv preprintarXiv:1803
 Distributed learning over unreliable networks,2018, arXiv preprint arXiv:1810
 Deepsqueeze:Parallel stochastic gradient descent with double-pass error-compensated compression,2019, arXivpreprint arXiv:1907
 Decentralized stochastic optimizationand gossip algorithms with compressed communication,2019, arXiv preprint arXiv:1902
 Decentralized double stochastic averaging gradient,2015, InSignals
 Consensus optimization with delayed and stochastic gradients ondecentralized networks,2016, In Big Data (Big Data)
 Communication-efficient algorithms for decentralized andstochastic optimization,2017, arXiv preprint arXiv:1701
 Decentralized consensus optimizationwith asynchrony and delays,2018, IEEE Transactions on Signal and Information Processing overNetworks
 Cola: Decentralized linear learning,2018, In Advances in NeuralInformation Processing Systems
 Dadam: A consensus-baseddistributed adaptive gradient method for online optimization,2019, arXiv preprint arXiv:1901
 Asynchronous decentralized optimization in directed networks,2019, arXivpreprint arXiv:1901
 Stochastic gradient push fordistributed deep learning,2018, arXiv preprint arXiv:1811
 Sparsified sgd with memory,2018, InAdvances in Neural Information Processing Systems
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems
 Synchronous multi-gpu deep learning with low-precision communication: An experimental study,2018, Proceedings of the EDBT 2018
 A linear speedup analysis of distributed deep learning with sparseand quantized communication,2018, In Advances in Neural Information Processing Systems
 Distributed learningwith sublinear communication,2019, arXiv preprint arXiv:1902
 Quantizeddecentralized consensus optimization,2018, CoRR
 Error compensated quantizedsgd and its applications to large-scale distributed optimization,2018, arXiv preprint arXiv:1806
 Trainingquantized nets: A deeper understanding,2017, In Advances in Neural Information Processing Systems
 Deep learning withlimited numerical precision,2015, In International Conference on Machine Learning
 Understanding andoptimizing asynchronous low-precision stochastic gradient descent,2017, In ACM SIGARCH ComputerArchitecture News
 Local sgd converges fast and communicates little,2018, arXiv preprint arXiv:1805
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
