title,year,conference
 Sparse communication for distributed gradient descent,2017, arXivpreprint arXiv:1704
 Equivalent-accuracy acceleratedneural-network training using analogue memory,2018, Nature
 Scalable methods for 8-bit training ofneural networks,2018, In Advances in Neural Information Processing Systems
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Solving linear least squares problems by gram-schmidt orthogonalization,1967, BrTNUmericalMathematics
 Convex optimization,2004, Cambridge university press
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Acceleration of Deep Neural Network Training with ResistiveCross-Point Devices,1662, Frontiers in Neuroscience
 Resistive ram endurance: Array-levelcharacterization and correction techniques targeting deep learning applications,2019, rEEE Transactionson Electron Devices
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the rEEE internationalconference on computer vision
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Federated optimization:Distributed machine learning for on-device intelligence,2016, CoRR
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep gradient compression:Reducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 A recursive modified gram-schmidt algorithmfor least-squares estimation,1986, IEEE transactions on acoustics
 Approximating real-time recurrent learning withrandom kronecker factors,2018, In Advances in Neural Information Processing Systems
 Automatic differentiation inpytorch,2017, 2017
 Memristor-Based Multilayer NeuralNetworks With Online Gradient Descent Training,2015, IEEE Transactions on Neural Networks andLearning Systems
 Unbiased online recurrent optimization,2017, arXiv preprintarXiv:1702
 Rotation matrix in arbitrary dimension to align vector,2013, MathematicsStack Exchange
 A learning algorithm for continually running fully recurrentneural networks,1989, Neural computation
 Binary neural network with 16Mb RRAM macro chip for classification and online training,2016, In 2016 IEEE International ElectronDevices Meeting (IEDM)
 Neuro-inspired computing with emerging nonvolatile memorys,2018, Proceedings of theIEEE
 Manhattan rule training for memristivecrossbar circuit pattern classifiers,2015, In 2015 IEEE 9th International Symposium on Intelligent SignalProcessing (WISP) Proceedings
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 In Section 4,2020,1
