title,year,conference
 Online continual learning with notask boundaries,2019, arXiv preprint arXiv:1903
 On the generalization ability of on-linelearning algorithms,2004, IEEE Transactions on Information Theory
 Riemannianwalk for incremental learning: Understanding forgetting and intransigence,2018, In Proceedings of theEuropean Conference on Computer Vision (ECCV)
 Efficientlifelong learning with a-gem,2018, arXiv preprint arXiv:1812
 Gradnorm: Gra-dient normalization for adaptive loss balancing in deep multitask networks,2017, arXiv preprintarXiv:1711
 Orthogonal gradient descent for contin-ual learning,2019, arXiv preprint arXiv:1910
 Catastrophic forgetting in connectionist networks,1999, Trends in cognitive sciences
 Overcoming catastrophic interference using conceptor-aided backprop-agation,2018, 2018
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Learning multiple layers of features from tiny images,2009, Technical report
 Overcoming catastrophic forgetting withunlabeled data in the wild,2019, In Proceedings of the IEEE International Conference on ComputerVision
 Gradient episodic memory for continual learning,2017, In Advances in NeuralInformation Processing Systems
 Continuallifelong learning with neural networks: A review,2019, Neural Networks
 Connectionist models of recognition memory: constraints imposed by learning andforgetting functions,1990, Psychological review
 Learning to learn without forgetting by maximizing transfer and minimizing interfer-ence,2018, arXiv preprint arXiv:1810
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Multi-task learning as multi-objective optimization,2018, In Advancesin Neural Information Processing Systems
 Lifelong robot learning,1995, Robotics and autonomous systems
 Continual learning of context-dependent pro-cessing in neural networks,2019, Nature Machine Intelligence
 1	Permuted MNISTMEGA0,2020,9613 0
