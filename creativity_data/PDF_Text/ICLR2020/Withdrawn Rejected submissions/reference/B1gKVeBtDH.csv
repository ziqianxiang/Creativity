title,year,conference
 Groupreduce: Block-wise low-rank approximation for neural language model shrinking,2018, In Advances in Neural InformationProcessing Systems 
 Dynamic network surgery for efficient dnns,2016, InAdvances In Neural Information Processing Systems
 Race: Large-scale readingcomprehension dataset from examinations,2017, arXiv preprint arXiv:1704
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Atensorized transformer for language modeling,2019, arXiv preprint arXiv:1906
 On bayesian methods for seeking the extremum,1975, In Optimization Techniques IFIPTechnical Conference
 Convolutional neural networks with low-rank regularization,2015, arXiv preprint arXiv:1511
 Distilling task-specific knowledge from bert into simple neural networks,2019, ArXiv
 Bert rediscovers the classical nlp pipeline,2019, Proceedingsof the 57th Annual Meeting of the Association for Computational Linguistics
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Efficient and accurateapproximations of nonlinear convolutional networks,2015, In Proceedings of the IEEE Conference onComputer Vision and pattern Recognition
