title,year,conference
 Weighted transformer network for machinetranslation,2017, CoRR
 Character-level languagemodeling with deeper self-attention,2018, arXiv preprint arXiv:1808
 Neural machine translation by jointlylearning to align and translate,2014, CoRR
 Image captioning as neural machine translationtask in sockeye,2018, arXiv preprint arXiv:1810
 Image captioning as neural machine translationtask in SOCKEYE,2018, CoRR
 The best of both worlds: Combiningrecent advances in neural machine translation,2018, In Proceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics
 Microsoft cococaptions: Data collection and evaluation server,2015, arXiv:1504
 Microsoft COCO captions: Data collection and evaluation server,2015, CoRR
 Gated feedback recurrentneural networks,2015, In Proceedings of the 32nd International Conference on Machine Learning
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Latent alignmentand variational attention,2018, In Advances in Neural Information Processing Systems 31: AnnualConference on Neural Information Processing Systems 2018
 Meteor universal: Language specific translation evaluationfor any target language,2014, In Proceedings of the Ninth Workshop on Statistical Machine Translation
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Classicalstructured prediction losses for sequence to sequence learning,2018, In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Neural phrase-to-phrase machine translation,2018, CoRR
 Convolutionalsequence to sequence learning,2017, In ICML 2017
 Towards neural phrase-based machine translation,2017, arXiv preprint arXiv:1706
 Learning when to concentrate or divertattention: Self-adaptive attention temperature for neural machine translation,2018, In Proceedings of the2018 Conference on Empirical Methods in Natural Language Processing
 Knowing when to look: Adaptiveattention via a visual sentinel for image captioning,2017, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Neural baby talk,2018, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Effective approaches to attention-basedneural machine translation,2015, In Proceedings of the 2015 Conference on Empirical Methods inNatural Language Processing
 From Softmax to SParsemax: A sparse modelof attention and multi-label classification,2016, In ICML 2016
 Fast-slow recurrent neural networks,2017, In Advancesin Neural Information Processing Systems
 Bleu: a method for automaticevaluation of machine translation,2002, In ACL 2002
 Sparse sequence-to-sequence models,2019, Proceedingsof the 57th Annual Meeting of the Association for Computational Linguistics
 Self-criticalsequence training for image captioning,2017, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics
 Reinforcedself-attention network: a hybrid of hard and soft attention for sequence modeling,2018, In IJCAI 2018
 Adaptive attentionspan in transformers,2019, Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Attention is all you need,2017, In NIPS 2017
 Cider: Consensus-based imagedescription evaluation,2015, In CVPR 2015
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, In Machine Learning
 Fixup initialization: Residual learning withoutnormalization,2019, CoRR
