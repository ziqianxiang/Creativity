title,year,conference
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010: 19th International Conference on Computational Statistics
 Optimization methods for large-scale machine learning,2018, SIAMReview
 Mxnet: A flexible and efficient machine learning library forheterogeneous distributed systems,2015, CoRR
 Incorporating Nesterov momentum into Adam,2016, In International Conference onLearning Representations
 Deep residual learning for image recognition,2016, In 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, CoRR
 Acceleratingstochastic gradient descent for least squares regression,2018, In Sebastien Bubeck
 On the insufficiency ofexisting momentum schemes for stochastic optimization,2018, In International Conference on LearningRepresentations
 Adam: A method for stochastic optimization,2015, In 2015 International Conferenceon Learning Representations (ICLR 2015)
 Gradient-based learning applied to document recognition,2009, Tech Report
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep learning via Hessian-free optimization,2010, In International Conference on MachineLearning (ICML 2010)
 Adaptive bound optimization for online convexoptimization,2010, CoRR
 Rectified linear units improve restricted boltzmann machines,2010, InInternational Conference on Machine Learning
 Some methods of speeding up the convergence of iteration methods,1964, Ussr ComputationalMathematics and Mathematical Physics
 On the convergence of Adam and beyond,2018, InInternational Conference on Learning Representations
 A stochastic approximation method,1951, Annals of MathematicalStatistics
 On the importance of initialization and momentumin deep learning,2013, In International conference on machine learning
 Rmsprop: Divide the gradient by a running average of its recentmagnitude,2012, COURSERA: Neural Networks for Machine Learning
 Sadam: A variant of adam for stronglyconvex functions,2019, 2019
 ADADELTA: an adaptive learning rate method,2012, CoRR
