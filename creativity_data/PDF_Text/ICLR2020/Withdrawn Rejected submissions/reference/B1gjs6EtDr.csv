title,year,conference
 Character-levellanguage modeling with deeper self-attention,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Layer normalization,2016, arXiv preprintarXiv:1607
 Neural machine translation by jointlylearning to align and translate,2014, CoRR
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Convergence properties of the k-means algorithms,1995, In Advances inneural information processing systems
 Pixelsnail: An improved autore-gressive generative model,2017, arXiv preprint arXiv:1712
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Monotonic chunkwise attention,2017, arXiv preprintarXiv:1712
 Exponentially increasing the capacity-to-computation ratio forconditional computation in deep learning,2014, arXiv preprint arXiv:1406
 Learning phrase representations using RNN encoder-decoder for statistical machinetranslation,2014, CoRR
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Deep sequential neural network,2014, arXiv preprintarXiv:1410
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 On the equivalence of nonnegative matrix factorizationand spectral clustering,2005, In Proceedings of the 2005 SIAM International Conference on DataMining
 Learning factored representations in a deepmixture of experts,2013, arXiv preprint arXiv:1312
 Large-scale matrix factorizationwith distributed stochastic gradient descent,2011, In SIGKDD International Conference on KnowledgeDiscovery and Data Mining
 Improving neural language models with acontinuous cache,2016, arXiv preprint arXiv:1612
 Draw: Arecurrent neural network for image generation,2015, arXiv preprint arXiv:1502
 Musictransformer: Generating music with long-term structure,2018, 2018
 Look harder: A neural machine transla-tion model with hard attention,2019, In Proceedings of the 57th Conference of the Association forComputational Linguistics
 Aneural transducer,2015, arXiv preprint arXiv:1511
 Sparse nonnegative matrix factorization for clustering,2008, Technical report
 Adam: A method for stochastic optimization,2014, CoRR
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Algorithms for non-negative matrix factorization,2001, In Advancesin neural information processing systems
 Generating wikipedia by summarizing long sequences,2018, arXiv preprint arXiv:1801
 Multi-task deep neural networks fornatural language understanding,2019, arXiv preprint arXiv:1901
 Least squares quantization in pcm,1982, IEEE transactions on information theory
 Effective approaches to attention-basedneural machine translation,2015, arXiv preprint arXiv:1508
 Balanced k-means for clustering,2014, In Joint IAPR InternationalWorkshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and SyntacticPattern Recognition (SSPR)
 Generating high fidelity images with subscale pixel networksand multidimensional upscaling,2018, arXiv preprint arXiv:1812
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 An analysis of neural language modelingat multiple scales,2018, arXiv preprint arXiv:1803
 Image transformer,2018, arXiv preprint arXiv:1802
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Adaptive attentionspan in transformers,2019, arXiv preprint arXiv:1905
 Conditionalimage generation with pixelcnn decoders,2016, In Advances in neural information processing systems
 Attention is all you need,2017, CoRR
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
