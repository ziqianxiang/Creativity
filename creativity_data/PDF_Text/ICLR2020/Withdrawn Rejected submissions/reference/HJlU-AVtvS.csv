title,year,conference
 Towards Characterizing Divergence in DeepQ-Learning,2019, arXiv:1903
 On the Convergence Rate of Training RecurrentNeural Networks,2018, arXiv:1810
 A Convergence Theory for Deep Learning viaOver-Parameterization,2018, arXiv:1811
 OnExact Computation with an Infinitely Wide Neural Net,2019, arXiv:1904
 Universal approximation bounds for superpositions of a sigmoidal function,0018, IEEETransactions on Information Theory
 On spherical expansions of zonalfunctions on Euclidean spheres,1420, Archiv der Mathematik
 On the Inductive Bias of Neural Tangent Kernels,2019, arXiv:1905
 A gaussian process perspective on convolutional neural netWorks,2018, arXivpreprint arXiv:1810
 Harmonic Analysis OfNeural Networks,1063, Applied and Computational HarmonicAnalysis
 Kernel methods for deep learning,2009, In Advances in NeuralInformation Processing Systems
 Deep gaussian processes,2013, In Artificial Intelligence andStatistics
 Gradient Descent Provably OptimizesOver-parameterized Neural Networks,2018, arXiv:1810
 Deep ConvolutionalNetworks as shallow Gaussian Processes,2018, arXiv:1808
 Linearized two-layersneural networks in high dimension,2019, arXiv:1904
 Understanding the difficulty of training deep feedforwardneural networks,2010, In Yee Whye Teh and Mike Titterington
 Strictly and non-strictly positive definite functions on spheres,1350, Bernoulli
 On the Selection of Initialization andActivation Function for Deep Neural Networks,2018, arXiv:1805
 Steps Toward Deep Kernel Methods from Infinite NeuralNetworks,2015, arXiv:1508
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision
 Neural Tangent Kernel: Convergence andGeneralization in Neural Networks,0000, arXiv:1806
 Universal Statistics of Fisher Information inDeep Neural Networks: Mean Field Approach,2018, arXiv:1806
 Deep Gaussian Processes withConvolutional Kernels,2018, arXiv preprint arXiv:1806
 Hierarchical Gaussian process latent variable models,2007, InProceedings of the 24th International Conference on Machine Learning
 Continuous neural networks,2007, In Artificial Intelligence andStatistics
 Deep Neural Networks as Gaussian Processes,2018, In International Conference onLearning Representations
 Wide Neural Networks of Any Depth Evolve as Linear Models Under GradientDescent,2019, arXiv:1902
 BAYESIAN LEARNING FOR NEURAL NETWORKS,1995, PhD Thesis
 Bayesian Deep Convolutional Networks with Many Channels areGaussian Processes,2018, arXiv preprint arXiv:1810
 Analysis of Boolean Functions,2014, Cambridge University Press
 Geometry of Neural Network Loss Surfaces via RandomMatrix Theory,0000, In Doina Precup and Yee Whye Teh
 Nonlinear random matrix theory for deep learning,0000, In Advancesin Neural Information Processing Systems
 The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network,2018, In Advances in Neural Information Processing Systems 31
 Resurrecting the sigmoid in deeplearning through dynamical isometry: Theory and practice,0000, In I
 Resurrecting the sigmoid in deeplearning through dynamical isometry: Theory and practice,0000, arXiv:1711
 The Emergence of Spectral Univer-sality in Deep Networks,2018, arXiv:1802
 The Nonlinearity Coefficient - Predicting Overfitting inDeep Neural Networks,0000, arXiv:1806
 Ex-ponential expressivity in deep neural networks through transient chaos,0004, In Advances In NeuralInformation Processing Systems
 On the Spectral Bias of Neural Networks,2018, arXiv:1806
 Early stopping and non-parametric regression:An optimal data-dependent stopping rule,2013, arXiv:1306
 Deep InformationPropagation,2017, 2017
 Neural Network with Unbounded Activation Functions is UniversalApproximator,2017, Applied and Computational Harmonic Analysis
 An Introduction to Matrix Concentration Inequalities,2015, arXiv:1501
 Computing with Infinite Networks,1997, In Advances in Neural InformationProcessing Systems
 Stochastic VariationalDeep Kernel Learning,2016, In Advances in Neural Information Processing Systems
 Training ultra-deep CNNswith critical initialization,2017, In NIPS Workshop
 Strictly Positive Definite Functions on Spheres,0002, Proceedings of theAmerican Mathematical Society
 Training behavior of deep neural network infrequency domain,2018, arXiv:1807
 Deep Mean Field Theory: Layerwise Variance and WidthVariation as Methods to Control Gradient Explosion,2018, February 2018
 Mean Field Residual Network: On the Edge of Chaos,2017, InAdvances in Neural Information Processing Systems
 AMean Field Theory of Batch Normalization,2019, arXiv:1902
 On Early Stopping in Gradient DescentLearning,1432, Constructive Approximation
 Explicitizing an Implicit Bias of theFrequency Principle in Two-layer Neural Networks,2019, arXiv:1905
 Stochastic Gradient Descent OptimizesOver-parameterized Deep ReLU Networks,2018, arXiv:1811
 By Eqs,2020, (8) and (5)
