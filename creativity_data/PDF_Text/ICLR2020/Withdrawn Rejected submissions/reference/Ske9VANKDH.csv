title,year,conference
 Understanding batch normalization,2018, InProc
 On the convergence of a class of Adam-type algorithms fornon-convex optimization,2019, In Proc
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Accelerated gradient methods for nonconvex nonlinear and stochasticprogramming,2016, Mathematical Programming
 Deep sparse rectifier neural networks,2011, In Proc
 Identity matters in deep learning,2016, arXiv:1611
 Deep residual learning for image recognition,2015, In Proc
 Densely connected convolutional networks,2016, In Proc
 Adam: A method for stochastic optimization,2015, In Proc
 Learning multiple layers of features from tiny images,2009, Technical report
 Visualizing the losslandscape of neural nets,2018, In Proc
 Rectifier nonlinearities improve neural network acoustic models,2013, InProc
 Skip connections eliminate singularities,2018, In Proc
 On the momentum term in gradient descent learning algorithms,1999, Neural Networks
 On the convergence of Adam and beyond,2018, In Proc
 A stochastic approximation method,1951, The Annals of Mathematical Statistics
 Learning Representations by Back-propagatingErrors,1986, Nature
 How does batch normalization help optimization?In Proc,2018, Advances in Neural Information Processing Systems (NeurIPS)
 Highway networks,2015, ArXiv 1505
 Going deeper with convolUtions,2015, In Proc
 Low-rank solUtions of linearmatrix eqUations via ProcrUstes flow,2016, In Proc
 Understanding deep learning reqUiresrethinking generalization,2017, In Proc
 Recovery gUarantees for one-hidden-layerneUral networks,2017, In Proc
 Characterization of gradient dominance and regUlarity conditions for neUralnetworks,2017, ArXiv:1710
 Geometrical properties and accelerated gradient solvers of non-convex phase retrieval,2016, In Proc
 SGD converges to global minimUm in deep learn-ing via star-convex path,2019, In Proc
