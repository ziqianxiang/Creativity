title,year,conference
 On the convergence ofa class of adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Beyond convexity: Stochastic quasi-convex opti-mization,2015, In Advances in Neural Information Processing Systems
 ADAM: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 On the convergence of stochastic gradient descent with adaptivestepsizes,2018, arXiv preprint arXiv:1805
 Regularizing and optimizing LSTMlanguage models,2018, In International Conference on Learning Representations
 Non-gaussianity ofstochastic gradient noise,2019, arXiv preprint arXiv:1910
 Introduction to optimization,1987, optimization software
 On the convergence of ADAM and beyond,2019, arXivpreprint arXiv:1904
 A stochastic approximation method,1951, Annals of Mathematical Statistics
 A tail-index analysis of stochastic gradientnoise in deep neural networks,2019, arXiv preprint arXiv:1901
 Escaping saddle pointswith adaptive gradient methods,2019, arXiv preprint arXiv:1901
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Analysis of gradient clipping andadaptive scaling with a relaxed smoothness condition,2019, arXiv preprint arXiv:1905
 On the convergence ofadaptive gradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
 A sufficient condition for conver-gences of adam and rmsprop,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
