title,year,conference
 Ellipsoidal trust region methods andthe marginal value of Hessian information for neural network training,2019, arXiv preprintarXiv:1905
 Linear coupling: An ultimate unification of gradient andmirror descent,2014, arXiv preprint arXiv:1407
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 Convex optimization,2004, Cambridge University Press
 Entropy-SGD: Biasing gra-dient descent into wide valleys,2017, In 5th International Conference on Learning Representations(ICLR)
 An investigation into neural net optimizationvia Hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Stochastic gradient methods with layer-wise adaptive moments for training of deep networks,2019, arXiv preprint arXiv:1905
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Problem complexity and methodefficiency in optimization,1983, Wiley
 Smooth minimization of non-smooth functions,2005, Mathematical programming
 On stochastic sign descent methods,2019, arXiv preprintarXiv:1905
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, In Fifteenth Annual Conferenceof the International Speech Communication Association
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Block-normalized gradient method: An empirical study for training deep neural network,2017, arXiv preprintarXiv:1707
 Analysis of gradient clipping andadaptive scaling with a relaxed smoothness condition,2019, arXiv preprint arXiv:1905
 To compute the smoothness constant w,2000,r
