title,year,conference
 Convergence rate of expected spectral distributions of large random matrices part i:Wigner matrices,2008, In Advances In Statistics
  On the principal components ofsample covariance matrices,2016, Probability theory and related fields
 Limits of spiked random matrices ii,2016, The Annals of Probability
 Cleaning large correlation matrices: tools fromrandom matrix theory,2017, Physics Reports
 Entropy-SGD: Biasing gradient descentinto wide valleys,2016, arXiv preprint arXiv:1611
  Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Open problem: The landscape of the losssurfaces of multilayer networks,2015, In Conference on Learning Theory
 Gpytorch:Blackbox matrix-matrix Gaussian process inference with GPU acceleration,2018, In Advances in NeuralInformation Processing Systems
 An investigation into neural net optimizationvia Hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Semicircle law for a class of random matrices withdependent entries,2012, arXiv preprint arXiv:1211
 Entropic determinants of massive matrices,2017, In 2017 IEEEInternational Conference on Big Data
   Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Flat minima,1997, Neural Computation
 Three factors influencing minima in SGD,2017, arXiv preprint arXiv:1711
 Sparse Principal Components Analysis,2004, Unpublished manuscript
  On large-batch training for deep learning:  Generalization gap and sharp minima,2016,  arXivpreprint arXiv:1609
 Limitations of the empirical Fisher approxima-tion,2019, arXiv preprint arXiv:1905
 Noise dressing of financialcorrelation matrices,1999, Physical review letters
   Bayesian methods for adaptive models,1992,   PhD thesis
 Optimizing neural networks with Kronecker-factored approximatecurvature,2015, In International conference on machine learning
   The Lanczos and conjugate gradient algorithms in finiteprecision arithmetic,2006, Acta Numerica
 The full spectrum of deep net Hessians at scale: Dynamics with sample size,2018, arXivpreprint arXiv:1811
   Automatic differentiation inPytorch,2017, 2017
 Fast exact multiplication by the Hessian,1994, Neural computation
 Improved bounds on sample size for implicit matrix traceestimators,2015, Foundations of Computational Mathematics
 Eigenvalues of the Hessian in deep learning: Singularityand beyond,2016, arXiv preprint arXiv:1611
 Empirical analysis of theHessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Towards understanding generalization of deep learning: Perspective ofloss landscapes,2017, arXiv preprint arXiv:1706
 Hessian-based analysisof large batch training and robustness to adversaries,2018, In Advances in Neural Information ProcessingSystems
59                                        0,2020,52                                       5
