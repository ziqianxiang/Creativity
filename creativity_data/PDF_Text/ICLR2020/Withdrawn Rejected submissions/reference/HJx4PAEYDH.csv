title,year,conference
 Character-level lan-guage modeling with deeper self-attention,2018, arXiv preprint arXiv:1808
 Layer normalization,2016, arXiv preprintarXiv:1607
 An empirical evaluation of generic convolutionaland recurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Modeling temporal depen-dencies in high-dimensional sequences: Application to polyphonic music generation and tran-scription,2012, arXiv preprint arXiv:1206
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 A convolutional encoder modelfor neural machine translation,2016, arXiv preprint arXiv:1611
 Towards end-to-end speech recognition with recurrent neuralnetworks,2014, In International conference on machine learning
 Session-based rec-ommendations with recurrent neural networks,2015, arXiv preprint arXiv:1511
 Deep neural networksfor acoustic modeling in speech recognition,2012, IEEE Signal processing magazine
 Long short-term memory,1997, Neural computation
 Character-aware neural languagemodels,2016, In Thirtieth AAAI Conference on Artificial Intelligence
 Zoneout: Regularizing rnnsby randomly Preserving hidden activations,2016, arXiv preprint arXiv:1606
 A simPle way to initialize recurrent networksof rectified linear units,2015, arXiv preprint arXiv:1504
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Recurrentneural network based language model,2010, In Eleventh annual conference of the international speechcommunication association
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Danq: a hybrid convolutional and recurrent deep neural networkfor quantifying the function of dna sequences,2016, Nucleic acids research
 Lstm neural networks for language mod-eling,2012, In Thirteenth annual conference of the international speech communication association
 Wavenet: A generative model forraw audio,2016, SSW
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Architectural complexity measures of recurrent neural networks,2016, In Advancesin neural information processing systems
