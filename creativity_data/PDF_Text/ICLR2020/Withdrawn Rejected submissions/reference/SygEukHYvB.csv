title,year,conference
 Deep Variational InformationBottleneck,2017, In International Conference on Learning Representations
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Adversarial transformation networks: Learning to generate adver-sarial examples,2017, arXiv preprint arXiv:1703
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Robust physical-world attacks on deep learningmodels,2017, arXiv preprint arXiv:1707
 The Conditional Entropy Bottleneck,2018, Open Review
 Explaining and harnessing adversarialexamples,2015, In CoRR
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, arXiv preprint arXiv:1903
 Natural adversarialexamples,2019, arXiv preprint arXiv:1907
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Improving ro-bustness without sacrificing accuracy with patch gaussian augmentation,2019, 2019
 On Variational Bounds of MutualInformation,2019, ICML2019
 Intriguingproperties of neural networks,2013, arXiv: 1312
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Learnability for the information bottle-neck,2019, Uncertainty in AI
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv: 1708
 Feature denoisingfor improving adversarial robustness,2019, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Wide Residual Networks,2016, arXiv: 1605
