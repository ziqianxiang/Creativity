title,year,conference
 Statistical mechanics of optimal convex inference in highdimensions,2016, Phys
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 Large sample covariance matrices without independence structures incolumns,2008, Statistica Sinica
 On the limiting spectral distribution for alarge class of symmetric random matrices with correlated entries,2015, Stochastic Processes and theirApplications
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, In Advances in neural information processing systems
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Proceedings of the 32Nd International Conference on NeuralInformation Processing Systems
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 An analytic theory of generalization dynamics and transferlearning in deep linear networks,2018, arXiv preprint arXiv:1809
 The dynamics of learning: A random matrix approach,2018, In 35thInternational Conference on Machine Learning
 On the Spectrum of Random Features Maps of High DimensionalData,2018, In International Conference on Machine Learning (ICML 2018)
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings ofthe National Academy of Sciences
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, arXiv preprint arXiv:1902
 Geometry of neural network loss surfaces via random matrixtheory,2017, In International Conference on Machine Learning
 Nonlinear random matrix theory for deep learning,2017, In Advancesin Neural Information Processing Systems
 The spectrum of the fisher information matrix ofa single-hidden-layer neural network,2018, In Advances in Neural Information Processing Systems
 Global convergence of neuronbirth-death dynamics,2019, arXiv preprint arXiv:1902
 Neural networks as interacting particle systems:Asymptotic convexity of the loss landscape and universal scaling of the approximation error,2018, arXivpreprint arXiv:1805
 Outrageouslylarge neural language models using sparsely gated mixtures of experts,2017, ICLR
