title,year,conference
 Pattern recognition and machine learning,2006, Springer
 Conditioning as disintegration,1997, Statistica Neerlandica
 Dimensionality reduction flows,2019, arXivpreprint arXiv:1908
 Nice: Non-linear independent components esti-mation,2014, arXiv preprint arXiv:1410
 Density estimation using real NVP,2016, arXivpreprint arXiv:1605
 A RAD approach todeep mixture models,2019, arXiv preprint arXiv:1903
 Transport Monte Carlo,2019, arXiv preprint arXiv:1907
 Made: Masked autoencoder fordistribution estimation,2015, In International Conference on Machine Learning
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Flow++: Improving flow-based generative models with variational dequantization and architecture design,2019, In InternationalConference on Machine Learning
 Neural autoregressiveflows,2018, In International Conference on Machine Learning
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Sum-of-squares polynomial flow,2019, In InternationalConference on Machine Learning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Auto-encoding variational Bayes,2013, arXiv preprintarXiv:1312
 Im-proved variational inference with inverse autoregressive flow,2016, In Advances in neural informationprocessing systems
 Auxiliary deep gen-erative models,2016, In International Conference on Machine Learning
 The concrete distribution: A continuousrelaxation of discrete random variables,2016, arXiv preprint arXiv:1611
 Masked autoregressive flow for densityestimation,2017, In Advances in Neural Information Processing Systems
 Computational optimal transport,2019, Foundations and TrendsÂ® inMachine Learning
 Laddervariational autoencoders,2016, In Advances in neural information processing systems
 A note on the evaluation of generativemodels,2015, arXiv preprint arXiv:1511
 Sylvester normal-izing flows for variational inference,2018, In UAI 2018: The Conference on Uncertainty in ArtificialIntelligence (UAI)
 Factoring variations in natural images with deepGaussian mixture models,2014, In Advances in Neural Information Processing Systems
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
