title,year,conference
 Generating sentences from a continuous space,2015, arXiv preprint arXiv:1511
 Understanding disentangling in Î²-vae,2018, arXiv preprintarXiv:1804
 Variational lossy autoencoder,2016, arXiv preprint arXiv:1611
 Avoiding latent variable collapsewith generative skip models,2018, arXiv preprint arXiv:1807
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Pixelvae: A latent variable model for natural images,2016, arXiv preprintarXiv:1611
 beta-vae: Learning basic visual concepts with aconstrained variational framework,2017, ICLR
 Introvae: Introspective variational autoen-coders for photographic image synthesis,2018, In Advances in Neural Information Processing Systems
 Semi-amortizedvariational autoencoders,2018, arXiv preprint arXiv:1802
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Im-proved variational inference with inverse autoregressive flow,2016, In Advances in neural informationprocessing systems
 Autoen-coding beyond pixels using a learned similarity metric,2015, arXiv preprint arXiv:1512
 Preventing posterior collapse withdelta-vaes,2019, arXiv preprint arXiv:1901
 Stochastic backpropagation andapproximate inference in deep generative models,2014, arXiv preprint arXiv:1401
 Howto train deep variational autoencoders and probabilistic ladder networks,2016, In 33rd InternationalConference on Machine Learning (ICML 2016)
 Vae with a vampprior,2017, arXiv preprint arXiv:1705
 Neural discrete representation learning,2017, In Advances inNeural Information Processing Systems
 Towards deeper understanding of variationalautoencoding models,2017, arXiv preprint arXiv:1702
 Infovae: Balancing learning and inference invariational autoencoders,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
