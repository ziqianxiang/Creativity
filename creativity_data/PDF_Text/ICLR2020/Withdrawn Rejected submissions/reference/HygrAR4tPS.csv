title,year,conference
 Ellipsoidal trust region methods andthe marginal value of Hessian information for neural network training,2019, arXiv preprintarXiv:1905
 Improving the convergence of the backpropagation learning with secondorder methods,1988, Morgan Koufmann
 signSGD:Compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 NAMSG: An efficient method for training neuralnetworks,2019, arXiv preprint arXiv:1905
 On the convergence ofa class of Adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 Convergence guarantees for RMSProp andADAM in non-convex optimization and an empirical comparison to Nesterov acceleration,2018, arXivpreprint arXiv:1807
 Incorporating Nesterov momentum into Adam,2016, In ICLR Workshops
 DeeP residual learning for image recog-nition,2016, In Conference on Computer Vision and Pattern Recognition
 Identity maPPings in deeP residualnetworks,2016, In European Conference on Computer Vision
 Rainbow: Combining imProvements in deeP reinforcementlearning,2017, 2017
 Long short-term memory,1997, Neural Computation
 Batch normalization: accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Improving generalization performance by switching fromAdam to SGD,2017, arXiv preprint arXiv:1712
 Adam: a method for stochastic optimization,2015, In ICLR
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Quasi-hyperbolic momentum and Adam for deep learning,2018, arXivpreprint arXiv:1810
 Optimizing neural networks with Kronecker-factored approxi-mate curvature,2015, arXiv preprint arXiv:1503
 On the convergence of Adam and beyond,2019, InICLR
 DeepOBS: a deep learning optimizer bench-mark suite,2019, arXiv preprint arXiv:1903
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Striving forsimplicity: the all convolutional net,2014, arXiv preprint arXiv:1412
 On the importance of initializa-tion and momentum in deep learning,2013, In ICML
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems 30
 Fashion-MNIST: a novel image dataset for bench-marking machine learning algorithms,2017, arXiv preprint arXiv:1708
 Adaptive meth-ods for nonconvex optimization,2018, In Advances in Neural Information Processing Systems 31
 Yellowfin and the art of momentum tuning,2017, arXiv preprintarXiv:1706
 A sufficient condition for conver-gences of Adam and RMSProp,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
