title,year,conference
 Pretrained languagemodels for sequential sentence classification,2019, CoRR
 Transformer-XL: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Long document classification from local wordglimpses via recurrent attention learning,2019, IEEE Access
 Long short-term memory,1997, Neural computation
 Sparse attentive backtracking: Temporal credit assignment throughreminding,2018, In Advances in Neural Information Processing Systems
 Convolutional neural networks for sentence classification,2014, arXiv preprintarXiv:1408
 PatentBERT: Patent classification with fine-tuning a pre-trainedBERT model,2019, CoRR
 DeepPatent: patent classification with convolutionalneural networks and word embedding,2018, Scientometrics
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Approximating real-time recurrent learning withrandom kronecker factors,2018, In Advances in Neural Information Processing Systems
 Understanding the exploding gradient prob-lem,2012, CoRR
 Maps of Meaning: The Architecture of Belief,1999,	Routledge
 Lan-guage models are unsupervised multitask learners,2019, 2019
 Development of a patentdocument classification and search platform using a back-propagation network,2006, Expert Systemswith Applications
 Unsupervised word embeddings capturelatent knowledge from materials science literature,2019, Nature
 An efficient gradient-based algorithm for on-line training ofrecurrent network trajectories,1990, Neural computation
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
