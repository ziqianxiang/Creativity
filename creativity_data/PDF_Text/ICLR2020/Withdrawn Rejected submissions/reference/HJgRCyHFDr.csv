title,year,conference
  The case for full-matrix adaptive regularization,2019,  In Proceedings of the 36th International Conference on MachineLearning
  Adaptive stochasticnatural gradient method for one-shot neural architecture search,2019,  In International Conference onMachine Learning
  A simple but tough-to-beat baseline for sentence embeddings,2017,  InProceedings of the 5th International Conference on Learning Representations
  Mirror descent and nonlinear projected subgradient methods for convexoptimization,2003, Operations Research Letters
 The relaxation method of finding the common point of convex sets and its applica-tion to the solution of problems in convex programming,1967, USSR Computational Mathematics andMathematical Physics
   ProxylessNAS: Direct neural architecture search on target task andhardware,2019, In International Conference on Learning Representations
   The loss surface of multilayernetworks,2015,   In  Proceedings  of  the  18th  International  Conference  on  Artificial  Intelligence  andStatistics
  sharpdarts:  Faster and more accurate differentiable architecturesearch,2019, 2019
 Multi-fidelity bayesian optimisation withcontinuous approximations,2017, In International Conference on Machine Learning
   An experimental and theoretical comparison ofmodel selection methods,1997, Machine Learning
  Adam:  A method for stochastic optimization,2015,  In Proceedings of the 3rdInternational Conference on Learning Representations
 Statistical machine learning,2010, 2010
   Random  search  and  reproducibility  for  neural  architecture  search,2019,   InUncertainty in Artificial Intelligence
  DARTS: Differentiable architecture search,2019,  In InternationalConference on Learning Representations
  Sgdr:  Stochastic gradient descent with warm restarts,2016,  arXiv preprintarXiv:1608
 Foundations of Machine Learning,2012, MIT Press
 Generalization in deep networks: The role of distance from initializa-tion,2017, arXiv
 Uniform convergence may be unable to explain generalization in deeplearning,2019, In Advances in Neural Information Processing Systems
 Xnas: Neural architecturesearch with expert advice,2019, 2019
 Efficient neural architecture search via parameterssharing,2018, In International Conference on Machine Learning
  Random features for large-scale kernel machines,2008,  In Advances in NeuralInformation Processing Systems
  Regularized Evolution for Image Classifier Archi-tecture Search,2018, 2018
  Generalization properties of learning with random features,2017,  In Advancesin Neural Information Processing Systems
 Fast rates for regularized objectives,2008, In Advancesin Neural Information Processing Systems
  Going deeper with convolutions,2015,  In Proceedings of the IEEE conference on computervision and pattern recognition
 Feature hashing for large scalemultitask learning,2009,  In Proceedings of the 26th International Conference on Machine Learning
  SNAS: stochastic neural architecture search,2019,  In InternationalConference on Learning Representations
 Shakedrop regularization,2018, 2018
 On the convergence rate of stochastic mirror descent for nonsmooth nonconvexoptimization,2018, arXiv
   Learning transferable architectures for scalableimage recognition,2018, In Conference on Computer Vision and Pattern Recognition
 ThenAlgorithm 2 will return an expected ε-stationary point of f under the projected stationarity measurein a number of stochastic oracle calls bounded by,2020, βFb² 
4 and scheduledpath dropout of 0,2016,2
