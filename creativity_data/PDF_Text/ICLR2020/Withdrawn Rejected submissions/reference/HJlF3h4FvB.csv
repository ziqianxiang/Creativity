title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Label refinery:Improving imagenet classification through label progression,2018, arXiv preprint arXiv:1805
 Saas: Speed as a supervisor for semi-supervisedlearning,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Gradient descent finds globalminima of deep neural netWorks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural netWorks,2018, arXiv preprint arXiv:1810
 Shake-shake regularization,2017, CoRR
 Co-teaching: Robust training of deep neural netWorks With eXtremely noisy labels,2018, InAdvances in Neural Information Processing Systems
 Distilling the knoWledge in a neural netWork,2015, arXivpreprint arXiv:1503
 Understanding generalization of deep neural netWorks trainedWith noisy labels,2019, arXiv preprint arXiv:1905
 Mentornet: Learningdata-driven curriculum for very deep neural netWorks on corrupted labels,2017, arXiv preprintarXiv:1712
 Deep learning,2015, nature
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2019, CoRR
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In NeurIPS
 Sgdr: Stochastic gradient descent with warm restarts,2017, In ICLR
 Dimensionality-driven learning with noisy labels,2018, arXiv preprintarXiv:1806
 Foundations of machine learning,2018, MITpress
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, arXiv preprintarXiv:1805
 On the spectral bias of deep neural networks,2018, arXiv preprintarXiv:1806
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Learning theory estimates via integral operators and theirapproximations,2007, Constructive approximation
 Joint optimization frameworkfor learning with noisy labels,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Iterative methods by space decomposition and subspace correction,1992, SIAM review
 Iterative regularization and nonlinear inverse scale space applied towavelet-based denoising,2007, IEEE Transactions on Image Processing
 Knowledge distillation in generations:More tolerant teachers educate better students,2018, arXiv preprint arXiv:1805
 Snapshot distillation: Teacher-studentoptimization in one generation,2018, arXiv preprint arXiv:1812
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in Neural Information Processing Systems
