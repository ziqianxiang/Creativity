title,year,conference
 The Security of MachineLearning,2010, Machine Learning
 Poisoning Attacks against Support Vector Ma-chines,2012, In International Conference on Machine Learning
 Explaining and Harnessing AdversarialExamples,2015, In International Conference on Learning Representations
 Badnets: Identifying Vulnerabilities in theMachine Learning Model Supply Chain,2017, arXiv preprint arXiv:1708
 AdversarialMachine Learning,2011, In Workshop on Security and Artificial Intelligence
 Understanding Black-box Predictions via Influence Functions,2017, InInternational Conference on Machine Learning
 Stronger Data Poisoning Attacks Break DataSanitization Defenses,2018, arXiv preprint arXiv:1811
 Learning Multiple Layers of Features from Tiny Images,2009, Dataset
 Gradient-based Learning Appliedto Document Recognition,1998, Proceedings of the IEEE
 Using Machine Teaching to Identify Optimal Training-Set Attacks onMachine Learners,2015, In AAAI
 Conditional Generative Adversarial Nets,2014, arXiv preprintarXiv:1411
 Towards Poisoning of Deep Learning Algorithms with Back-Gradient Optimization,2017, In Workshop on Artificial Intelligence and Security
 Detection of Ad-versarial Training Examples in Poisoning Attacks through Anomaly Detection,2018, arXiv preprintarXiv:1802
 Label Sanitization against Label FlippingPoisoning Attacks,2018, In Nemesisâ€™18 Workshop on Recent Advancements in Adversarial MachineLearning
 Antidote: Understanding and Defending against Poisoning ofAnomaly Detectors,2009, In SIGCOMM Conference on Internet Measurement
 Certified Defenses for Data PoisoningAttacks,2017, In Advances in Neural Information Processing Systems
 Fashion-MNIST: a Novel Image Dataset for Bench-marking Machine Learning Algorithms,2017, arXiv preprint arXiv:1708
 Generative Poisoning Attack Method againstNeural Networks,2017, arXiv preprint arXiv:1703
