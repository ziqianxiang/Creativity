title,year,conference
 Advanced Calculus,2010, Springer
 Essentially no barriersin neural network energy landscape,2018, In ICML
 Gradient descent finds globalminima of deep neural networks,2019, In ICML
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Gradient descent happens in a tiny subspace,2018, arXivpreprint arXiv:1812
 Identity mappings in deep residualnetworks,2016, In ECCV
 Flat minima,1997, Neural Computation
 Densely connectedconvolutional networks,2017, In CVPR
 Three factors influencing minima in sgd,2018, arXiv preprint arXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In ICLR
 On connected sublevel sets in deep learning,2019, In ICML
 Eigenvalues of the hessian in deep learning: Singularityand beyond,2017, arXiv preprint arXiv:1611
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1803
 A tail-index analysis of stochastic gradientnoise in deep neural networks,2019, arXiv preprint arXiv:1901
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In ICLR
 Re-thinking the inception architecture for computer vision,2016, In CVPR
 Fluctuation-dissipation relations for stochastic gradient descent,2019, In ICLR
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Energy-entropy competition andthe effectiveness of stochastic gradient descent in machine learning,2018, Molecular Physics
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, In ICML
