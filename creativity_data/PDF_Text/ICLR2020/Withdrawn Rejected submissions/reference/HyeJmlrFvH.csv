title,year,conference
 QSGD: Communication-efficientSGD via gradient quantization and encoding,2017, In Proc
 Scaling up machine learning: Parallel and distributedapproaches,2011, Cambridge University Press
 Hogwild: A lock-free approach to parallelizing stochasticgradient descent,2011, In Proc
 Large scale distributed deep networks,2012, In Proc
 Project adam: Building an efficient andscalable deep learning training system,2014, In Proc
 Scaling distributed machine learning with the parameter server,2014, In Proc
 1-bit stochastic gradient descent and its application todata-parallel distributed training of speech DNNs,2014, In Proc
 Deep learning with limited numericalprecision,2015, In Proc
 Dorefa-net: Training low bitwidth convolutionalneural networks with low bitwidth gradients,2016, arXiv:1606
 TernGrad: Ternary gradients to reducecommunication in distributed deep learning,2017, In Proc
 signSGD: Compressed optimisa-tion for non-convex problems,2018, In Proc
 Binarized neural networks,2016, InProc
 Faster CNNs with direct sparseconvolutions and guided pruning,2017, In Proc
 Principles of pulse code modulation,1969, Iliffe
 Horovod: fast and easy distributed deep learning intensorflow,2018, arXiv preprint arXiv:1802
 Convex optimization: Algorithms and complexity,2015, Foundations and Trends in MachineLearning
 Deep residual learning for image recognition,2016, In Proc
 ImageNet: A large-scale hierarchicalimage database,2009, In Proc
 Learning multiple layers of features from tiny images,2009, Technical report
 Error feedback fixes SignSGD and othergradient compression schemes,2019, In Proc
 Deep speech: Scaling up end-to-endspeech recognition,2014, arXiv preprint arXiv:1412
 Universal codeword sets and representations of the integers,1975, IEEE Transactions onInformation Theory
 Convex Optimization,2004, Cambridge Univ
 Note that dj's dependon quantization intervals,2017, In the following
