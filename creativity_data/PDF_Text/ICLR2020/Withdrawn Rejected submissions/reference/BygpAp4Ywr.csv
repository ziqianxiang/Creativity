title,year,conference
 Wasserstein gan,2017, arXiv preprintarXiv:1701
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Towards evaluating the robustness of neural networks,2017, InSecurity and Privacy (SP)
 Zoo: Zeroth order opti-mization based black-box attacks to deep neural networks without training substitute models,2017, InProceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Ead: elastic-net attacks todeep neural networks via adversarial examples,2018, In AAAI
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 On thesensitivity of adversarial robustness to input data distributions,2019, arXiv preprint arXiv:1902
 Maximum likelihood estimation of intrinsic dimension,2005, InAdvances in neural information processing systems
 Nattack: Learning the dis-tributions of adversarial examples for an improved black-box attack on deep neural networks,2019, InICML
 Towards robust neural networksvia random self-ensemble,2017, arXiv preprint arXiv:1712
 Magnet: a two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Universaladversarial perturbations,2017, arXiv preprint
 Cascade adversarial machine learning regu-larized with a unified embedding,2017, arXiv preprint arXiv:1708
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In Security and Privacy (SP)
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia Conference on Computer and Communications Security
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 Adversarial vulnerability of neural networks increases with input dimension,2018, arXiv preprintarXiv:1802
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Wasserstein auto-encoders,2017, arXiv preprint arXiv:1711
 Mitigating adversarialeffects through randomization,2017, arXiv preprint arXiv:1711
