title,year,conference
 Surprising effectiveness of few-imageunsupervised feature learning,2019, arXiv preprint arXiv:1904
 Rieman-nian walk for incremental learning: Understanding forgetting and intransigence,2018, arXiv preprintarXiv:1801
 Superpositionof many models into one,2019, CoRR
 NICE: Non-linear independent componentsestimation,2014, arXiv preprint arXiv:1410
 Towards robust evaluations of continual learning,2018, arXiv preprintarXiv:1805
 Deep learning,2016, 2016
 Overcoming catastrophic forgetting via model adaptation,2018, 2018
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Training confidence-calibrated classifiersfor detecting out-of-distribution samples,2017, arXiv preprint arXiv:1711
 SupportNet:solving catastrophic forgetting in class incremental learning with support data,2018, arXiv preprintarXiv:1806
 Learning without forgetting,2018, IEEE Transactions on Pattern Analysisand Machine Intelligence
 Enhancing the reliability of out-of-distributionimage detection in neural networks,2017, arXiv preprint arXiv:1706
 Gradient episodic memory for continual learning,2017, In Advances in NeuralInformation Processing Systems
 Continuallifelong learning with neural networks: A review,2018, arXiv preprint arXiv:1802
 Automatic differentiation inpytorch,2017, 2017
 Encoder based lifelonglearning,2017, pp
 iCaRL:Incremental classifier and representation learning,2017, pp
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Continual learning with deep generativereplay,2017, pp
 Feature selection for classification: A review,2014, Dataclassification: Algorithms and applications
 Three scenarios for continual learning,2019, CoRR
 Matching networks for oneshot learning,2016, pp
 Lifelong learning with dynamicallyexpandable networks,2017, arXiv preprint arXiv:1708
 Lifelong learning with dynamicallyexpandable networks,2018, 2018
002 Number of epochs 200 Weight decay	0,2017,0 Patience	20	Learning Rate	0
