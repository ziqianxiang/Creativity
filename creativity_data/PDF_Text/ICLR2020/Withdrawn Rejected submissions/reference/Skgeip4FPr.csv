title,year,conference
 Deep neural networks are biased towardssimple functions,2018, arXiv preprint arXiv:1812
 Input-output maps are strongly biasedtowards simple outputs,2018, Nature communications
 Deep convolutional net-works as shallow gaussian processes,2018, arXiv preprint arXiv:1808
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Deep learning,2015, nature
 On the complexity of finite sequences,1976, IEEE Transactions oninformation theory
 Some pac-bayesian theorems,1999, Machine Learning
 Generalization and parameter estimation in feedforward nets:Some experiments,1990, In Advances in Neural Information Processing Systems
 Bayesian convolutional neural networks with many channels aregaussian processes,2018, arXiv preprint arXiv:1810
 On the spectral bias of neural networks,2018, arXiv preprintarXiv:1806
 The perceptron: a probabilistic model for information storage and organizationin the brain,1958, Psychological review
 Discovering neural nets with low kolmogorov complexity and high general-ization capability,1997, Neural Networks
 Deep informationpropagation,2017, In International Conference on Learning Representations
 Deep learning generalizes because theparameter-function map is biased towards simple functions,2018, arXiv preprint arXiv:1805
 A fine-grained spectral perspective on neural networks,2019, arXiv preprintarXiv:1907
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
