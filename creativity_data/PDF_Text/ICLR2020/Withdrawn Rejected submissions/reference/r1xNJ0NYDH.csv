title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 On exponential convergence of sgd in non-convexover-parametrized learning,2018, arXiv preprint arXiv:1811
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 Concentration inequalities: A nonasymptotictheory of independence,2013, Oxford university press
 Sgd learns over-Parameterized networks that Provably generalize on linearly seParable data,2017, arXiv preprintarXiv:1710
 EntroPy-sgd: Biasing gradient descentinto wide valleys,2016, arXiv preprint arXiv:1611
 The ef-fect of network width on the Performance of large-batch training,2018, arXiv preprint arXiv:1806
 The loss landscaPe of overParameterized neural networks,2018, arXiv preprintarXiv:1804
 Towards faster stochastic gradient search,1992, In Advances in neuralinformation processing systems
 Gradient descent Provably oPtimizesover-Parameterized neural networks,2018, arXiv preprint arXiv:1810
 Stiffness: A new perspective ongeneralization in neural networks,2019, arXiv preprint arXiv:1901
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Second-order optimization for neural networks,2016, University of Toronto (Canada)
 Non-asymptotic analysis of stochastic approximation algorithmsfor machine learning,2011, In Advances in Neural Information Processing Systems
 Generalization in deep networks: The role of distance frominitialization,2019, arXiv preprint arXiv:1901
 Convergence rate of incremental subgradient algorithms,2001, InStochastic optimization: algorithms and applications
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, arXiv preprintarXiv:1805
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, arXiv preprint arXiv:1308
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 The singular values of convolutional layers,2018, arXivpreprint arXiv:1805
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Optimistic rates for learning with a smoothloss,2010, arXiv preprint arXiv:1009
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Benefits of depth in neural networks,2016, arXiv preprint arXiv:1602
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2018, arXiv preprint arXiv:1810
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Towards understanding generalization of deep learning: Perspective ofloss landscapes,2017, arXiv preprint arXiv:1706
 Amean field theory of batch normalization,2019, arXiv preprint arXiv:1902
 Gradient diversity: a key ingredient for scalable distributed learning,2017, arXiv preprintarXiv:1706
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Fixup initialization: Residual learning withoutnormalization,2019, arXiv preprint arXiv:1901
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
