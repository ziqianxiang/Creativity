title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 On the inductive bias of neural tangent kernels,2019, arXiv preprintarXiv:1905
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, arXiv preprint arXiv:1905
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Disentangling feature and lazylearning in deep neural networks: an empirical study,2019, arXiv preprint arXiv:1906
 Implicit regularization of discrete gradientdynamics in deep linear neural networks,2019, arXiv preprint arXiv:1904
 Complexity of linear regions in deep networks,2019, arXiv preprintarXiv:1901
 Training dynamics of deep networks usingstochastic gradient descent via neural tangent kernel,2019, arXiv preprint arXiv:1905
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Sgd on neural networks learns functions of increasing complexity,2019, arXivpreprint arXiv:1905
 Implicit regularization in deep learning,2017, arXiv preprint arXiv:1709
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 On the spectral bias of deep neural networks,2018, arXiv preprintarXiv:1806
 A sober look at neural network initializations,2019, arXiv preprint arXiv:1903
 High-dimensional probability: an introduction with applications in data sci-ence,2018, Number 47 in Cambridge series in statistical and probabilistic mathematics
 Regularization matters: Generalizationand optimization of neural nets v,2018,s
 Kernel anddeep regimes in overparametrized models,2019, arXiv preprint arXiv:1906
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
