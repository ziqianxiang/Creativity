title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Towards understanding generalization in gradient-based meta-learning,2019, arXiv preprint arXiv:1907
 Flat minima,1997, Neural Computation
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 Automatic differentiation inPyTorch,2017, In NIPS Autodiff Workshop
 Local gain adaptation in stochastic gradient descent,1999, 1999
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Identifying generalizationproperties in neural networks,2018, arXiv preprint arXiv:1809
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 A walk with sgd,2018, arXiv preprintarXiv:1802
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv preprint arXiv:1705
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
