title,year,conference
   Neural machine translation by jointlylearning to align and translate,2015, ICLR
  Modelinglocalness for self-attention networks,2018, EMNLP
   Behavior analysis of nli models:Uncovering the influence of three factors on robustness,2018, NAACL
  Bert:  Pre-training of deepbidirectional transformers for language understanding,2019, NAACL
 Efficient training of bertby progressively stacking,2019, ICML
  Informationaggregation for multi-head attention with routing-by-agreement,2019, NAACL
  Non-entailed subsequences as a challenge for natural languageinference,2019, SCiL
 Right for the wrong reasons: Diagnosing syntacticheuristics in natural language inference,2019, ACL
 Distributed representa-tions of words and phrases and their compositionality,2013, NIPS
 Key-value memory networks for directly reading documents,2016, 2016
  Glove: Global vectors for wordrepresentation,2014, EMNLP
 Deep contextualized word representations,2018, NAACL
  Improving language under-standing by generative pre-training,2018, 2018
 Attention is all you need,2017, NIPS
 Visual concepts and compositional voting,2017, Annals of Mathematical Sciences andApplications
   Convolutionalself-attention networks,2019, NAACL
