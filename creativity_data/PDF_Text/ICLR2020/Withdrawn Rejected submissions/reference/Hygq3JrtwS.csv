title,year,conference
 Stronger generalization bounds for deep nets via acompression approach,2018, arXiv preprint arXiv:1802
 The tradeoffs of large scale learning,2008, In Advances in neural informationproceSSing SyStemS
 Use of some sensitivity criteria for choosing networks withgood generalization ability,1995, Neural ProceSSing LetterS
 Sensitivity analysis for input vector in multilayer feedforward neural networks,1993, InNeural NetworkS
 Deep learning,2016, MIT press
 Delving deep into rectifiers: Surpassing human-levelperformance on imagenet classification,2015, In ProceedingS of the IEEE international conference on computerviSion
 Deep residual learning for image recognition,2016, InProceedingS of the IEEE conference on computer viSion and pattern recognition
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, arXiv preprint arXiv:1502
 Predicting the generalization gap in deepnetworks with margin distributions,2018, arXiv preprint arXiv:1810
 Generalization in deep learning,2017, arXiv preprintarXiv:1710
 Onlarge-batch training for deep learning: Generalization gap and sharp minima,2016, arXiv preprint arXiv:1609
 Imagenet classification with deep convolutional neuralnetworks,2012, In AdvanceS in neural information proceSSing SyStemS
 Deep learning,2015, nature
 On the number of linear regions ofdeep neural networks,2014, In Advances in neural information processing systems
 A modern take on the bias-variance tradeoff in neural networks,2018, arXiv preprintarXiv:1810
 Exploring generalization in deeplearning,2017, In Advances in Neural Information Processing Systems
 Sensitivityand generalization in neural networks: an empirical study,2018, arXiv preprint arXiv:1802
 The nonlinearity coefficient-predicting overfitting in deep neuralnetworks,2018, arXiv preprint arXiv:1806
 Robust large margin deep neuralnetworks,2017, IEEE Transactions on Signal Processing
 meprop: Sparsified back propagation for accelerateddeep learning with reduced overfitting,2017, arXiv preprint arXiv:1706
 Learning sparse neural networks viasensitivity-driven regularization,2018, In Advances in Neural Information Processing Systems
 Benefits of depth in neural networks,2016, arXiv preprint arXiv:1602
 Mathematics of deep learning,2017, CoRR
 Towards robust deep neuralnetworks,2018, arXiv preprint arXiv:1810
 Wider or deeper: Revisiting the resnet model for visualrecognition,2019, Pattern Recognition
 Robustness and generalization,2012, Machine learning
 Effective neural network ensemble approach forimproving generalization performance,2013, IEEE transactions on neural networks and learning systems
 Understanding deep learningrequires rethinking generalization,2016, arXiv preprint arXiv:1611
1	Experimental DetailsThe CIFAR-10 dataset5 is used for the experiments presented in the paper,1000, The fully connected neural networkshave the same number of units in the hidden layers
