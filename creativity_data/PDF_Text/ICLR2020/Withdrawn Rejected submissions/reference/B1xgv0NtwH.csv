title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Detecting backdoor attacks on deep neural networks byactivation clustering,2018, arXiv preprint arXiv:1811
 Dual pathnetworks,2017, In Advances in Neural Information Processing Systems
 Analyzing and improving representationswith the soft nearest neighbor loss,2019, arXiv preprint arXiv:1902
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Badnets: Identifying vulnerabilities in themachine learning model supply chain,2017, arXiv preprint arXiv:1708
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Densely connectedconvolutional networks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Stronger data poisoning attacks break datasanitization defenses,2018, arXiv preprint arXiv:1811
 Delving into transferable adversarial exam-ples and black-box attacks,2016, arXiv preprint arXiv:1611
 Mal-ware classification with recurrent networks,2015, In International Conference on Acoustics
 Adversarial robustness through local linearization,2019, arXivpreprint arXiv:1907
 Transfer learning forhate speech detection in social media,2019, In Conference on Artificial Intelligence (AAAI)
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, arXiv preprint arXiv:1801
 Going deeper With convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Neural cleanse: Identifying and mitigating backdoor attacks in neural netWorks,2019, NeuralCleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks
 Feature denoisingfor improving adversarial robustness,2019, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Aggregated residual trans-formations for deep neural netWorks,2017, In Computer Vision and Pattern Recognition (CVPR)
