title,year,conference
  A convergence theory for deep learning viaover-parameterization,2019,  In International Conference on Machine Learning
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
   Theoretical analysis of auto rate-tuning bybatch normalization,2019,  In International Conference on Learning Representations
  The benefits of over-parameterization at initialization indeep ReLU networks,2019, In NeurIPS
 Layer normalization,2016, Deep LearningSymposium
  Understanding batchnormalization,2018, In Advances in Neural Information Processing Systems
 A quantitative analysis of the effect of batchnormalization on gradient descent,2019, In International Conference on Machine Learning
  A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
  Generalization bounds of stochastic gradient descent for wideand deep neural networks,2019, In NeurIPS
 On lazy training in differentiable program-ming,2019, In NeurIPS
   Gradient descent findsglobal minima of deep neural networks,2019,  In Kamalika Chaudhuri and Ruslan Salakhutdinov
   Gradient  descent  provablyoptimizes  over-parameterized  neural  networks,2019,   In  International  Conference  on  LearningRepresentations
   Understanding the difficulty of training deep feedfor-ward neural networks,2010, In Proceedings of the thirteenth international conference on artificialintelligence and statistics
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018,   In Advances in Neural Information ProcessingSystems
 Batch normalization: Accelerating deep network trainingby reducing internal covariate shift,2015, In International Conference on Machine Learning
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in neural information processing systems
  Learning overparameterized neural networks via stochasticgradient descent on structured data,2018,  In Advances in Neural Information Processing Systems
   Convergence analysis of two-layer neural networks with reluactivation,2017, In Advances in Neural Information Processing Systems
 Understanding regularization inbatch normalization,2018, arXiv preprint arXiv:1809
  Towards moderate overparameterization:  globalconvergence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
  Weight normalization:  A simple reparameterization toaccelerate training of deep neural networks,2016,  In Advances in Neural Information ProcessingSystems
  Wngrad:  Learn the learning rate in gradientdescent,2018, arXiv preprint arXiv:1803
 Global convergence of adaptive gradient methodsfor an over-parameterized neural network,2019, arXiv preprint arXiv:1902
  A mean field theory of batch normalization,2019,  In International Conference on LearningRepresentations
  Fast convergence of natural gradientdescent for over-parameterized neural networks,2019, In NeurIPS
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In NeurIPS
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2019, Machine Learning
