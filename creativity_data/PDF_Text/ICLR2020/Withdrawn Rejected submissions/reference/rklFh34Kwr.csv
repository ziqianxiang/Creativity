title,year,conference
 Bayesian posterior sampling viastochastic gradient fisher scoring,2012, arXiv preprint arXiv:1206
 Bayesiandark knowledge,2015, In Advances in Neural Information Processing Systems
 Weight un-certainty in neural network,2015, In International Conference on Machine Learning
 On the convergence of stochastic gra-dient mcmc algorithms with high-order integrators,2015, In Advances in Neural InformationProcessing Systems
 Bayesian sampling using stochastic gradient thermostats,2014, In Z
 Deep residual learning forimage recognition,2016, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network trainingby reducing internal covariate shift,2015, In International Conference on Machine Learning
 Variational dropout and the local repa-rameterization trick,2015, In Advances in Neural Information Processing Systems
 Preconditioned stochas-tic gradient langevin dynamics for deeP neural networks,2016, In Thirtieth AAAI Conferenceon Artificial Intel ligence
 A simPle baseline for bayesian uncertainty in deeP learning,2019, arXiv preprintarXiv:1902
 Mcmc using hamiltonian dynamics,2011, Handbook of markov chain montecarlo
 Mcmc-based inference in the era of big data: A funda-mental analysis of the convergence comPlexity of high-dimensional chains,2015, arXiv preprintarXiv:1508
 Weight normalization: A simPle reParameterization toaccelerate training of deeP neural networks,2016, In Advances in Neural Information ProcessingSystems
 Bayesian uncertainty estimation forbatch normalized deeP networks,2018, arXiv preprint arXiv:1802
 Privacy for free: Posterior samplingand stochastic gradient monte carlo,2015, In International Conference on Machine Learning
 Wngrad: learn the learning rate in gradientdescent,2018, arXiv preprint arXiv:1803
 Fixup initialization: Residual learningwithout normalization,2019, arXiv preprint arXiv:1901
 Cyclical stochastic gradient mcmc for bayesian deep learning,2019, arXiv preprintarXiv:1902
