title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Optimal convergence ratesfor nesterov acceleration,2018, arXiv preprint arXiv:1805
 Optimization methods for large-scale machinelearning,2018, Siam Review
 On the curved geometry of accelerated optimization,2018, arXiv preprintarXiv:1812
 Reducing the dimensionality of data with neuralnetworks,2006, science
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Aggregated momentum: Stabilitythrough passive damping,2018, arXiv preprint arXiv:1804
 Deep learning via hessian-free optimization,2010, In ICML
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Accelerating nesterovâ€™s method for strongly convex functions withlipschitz gradient,2011, arXiv preprint arXiv:1109
 A benchmark of kriging-based infill criteriafor noisy optimization,2013, Structural and Multidisciplinary Optimization
 On the convergence of adam and beyond,2018, 2018
 A differential equation for modeling nesterovsaccelerated gradient method: Theory and insights,2014, In Advances in Neural Information ProcessingSystems
 Virtual library of simulation experiments: Test functions anddatasets,2019, Retrieved April 14
 On the importance ofinitialization and momentum in deep learning,2013, ICML (3)
 A variational perspective on acceleratedmethods in optimization,2016, proceedings of the National Academy of Sciences
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
