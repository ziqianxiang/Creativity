title,year,conference
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, preprint arXiv:1806
 On the convergence of a class of ADAM-type algorithms for non-convex optimization,2019, In Proceedings of International Conference onLearning Representation (ICLR)
 Sadagrad: Strongly adaptive stochastic gradi-ent methods,2018, In International Conference on Machine Learning
 Forecasting trends in time series,1985, Management Science
 Forecasting: Principles and Practice,2018, OTexts
 ADAM: A method for stochastic optimization,2015, In Proceedingsof International Conference on Learning Representation (ICLR)
 Readingdigits in natural images with unsupervised feature learning,2011, NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 On the convergence of ADAM and beyond,2018, InProceedings of International Conference on Learning Representation (ICLR)
 An overview of gradient descent optimization algorithms,2016, preprintarXiv:1609
 Fashion-MNIST: a novel image dataset for bench-marking machine learning algorithms,2017, preprint arXiv:1708
 Adadelta: an adaptive learning rate method,2012, preprint arXiv:1212
 On the convergence ofadaptive gradient methods for nonconvex optimization,2018, preprint arXiv:1808
 Online convex programming and generalized infinitesimal gradient ascent,2003, InProceedings of the 20th International Conference on Machine Learning
 A sufficient condition for conver-gences of ADAM and RMSProp,2019, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
