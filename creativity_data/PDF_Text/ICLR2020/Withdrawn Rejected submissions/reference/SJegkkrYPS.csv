title,year,conference
 Deep rewiring:Training very sparse deep networks,2017, CoRR
 Nest: A neural network synthesis tool based on agrow-and-prune paradigm,2017, arXiv preprint arXiv:1711
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Deep residual learning for imagerecognition,2015, CoRR
 Channel pruning for accelerating very deep neuralnetworks,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Explor-ing the granularity of sparsity in convolutional neural networks,2017, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition Workshops
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, CoRR
 Exploring sparsity in recur-rent neural networks,2017, arXiv preprint arXiv:1704
 Principal filter analysis forguided network compression,2018, arXiv preprint arXiv:1807
 Resnet on tiny imagenet,2016, Submitted on
 Adversarialrobustness of pruned neural networks,2018, ICLR 2018
 Learning structured sparsity indeep neural networks,2016, In Advances in neural information processing systems
