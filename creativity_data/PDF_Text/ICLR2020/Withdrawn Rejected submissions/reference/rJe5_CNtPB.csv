title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 An actor-critic algorithm for sequence prediction,2016, arXiv preprintarXiv:1607
 Deriving machine attention from humanrationales,2018, arXiv preprint arXiv:1808
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 A pulse model in log-domainfor a uniform synthesizer,2016, In Acoustics
 Hierarchical rnns forwaveform-level speech synthesis,2018, In 2018 IEEE Spoken Language Technology Workshop (SLT)
 Efficientneural audio synthesis,2018, arXiv preprint arXiv:1802
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Professor forcing: A new algorithm for training recurrent networks,2016, InAdvances In Neural Information Processing Systems
 Close to human qualitytts with transformer,2018, arXiv preprint arXiv:1809
 Neural machine translationwith supervised attention,2016, In Proceedings of COLING 2016
 Robust universal neural vocoding,2018, arXiv preprint arXiv:1811
 Effective approaches to attention-based neural machine translation,2015, arXiv preprint arXiv:1508
 Samplernn: An unconditional end-to-end neural audiogeneration model,2016, arXiv preprint arXiv:1612
 Neural machine translation and sequence-to-sequence models: A tutorial,2017, arXivpreprint arXiv:1703
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Sequence level train-ing with recurrent neural networks,2015, arXiv preprint arXiv:1511
 Natural tts synthesis by con-ditioning wavenet on mel spectrogram predictions,2018, In 2018 IEEE International Conference onAcoustics
 Highway networks,2015, arXivpreprintarXiv:1505
 Tacotron: Towards end-to-endspeech synthesis,2017, arXiv preprint arXiv:1703
 Su-pervising neural attention models for video captioning by human gaze data,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
