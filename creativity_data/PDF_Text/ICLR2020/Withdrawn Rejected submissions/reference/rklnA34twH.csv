title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 A new look at an old problem: A universal learningapproach to linear regression,2019, arXiv preprint arXiv:1905
 Deep pnml: Predictive normalized maximum likelihoodfor deep neural networks,2019, arXiv preprint arXiv:1904
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 On evaluating adversarial robustness,2019, arXivpreprint arXiv:1902
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Universal batch learning with log-loss,2018, In 2018 IEEE InternationalSymposium on Information Theory (ISIT)
 Universal supervised learning for individual data,2018, arXiv preprintarXiv:1812
 Deep learning,2016, MIT press
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Countering adversarialimages using input transformations,2017, arXiv preprint arXiv:1711
 Adversarial examples detection in deep networks with convolutional filterstatistics,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Magnet: a two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 Universal sequential coding of single messages,1987, Problemy PeredachiInformatsii
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Exploring the space of adversarial images,2016, In 2016 InternationalJoint Conference on Neural Networks (IJCNN)
 A theory of the learnable,1984, In Proceedings of the sixteenth annual ACM symposiumon Theory of computing
 Wide residual networks,2016, arXiv preprintarXiv:1605
