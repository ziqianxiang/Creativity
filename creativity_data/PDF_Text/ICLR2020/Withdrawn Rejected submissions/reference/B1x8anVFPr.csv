title,year,conference
 Character-level lan-guage modeling with deeper self-attention,2018, arXiv preprint arXiv:1808
 Neural machine translation by jointlylearning to align and translate,2017, 2017
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Language modeling with gatedconvolutional networks,2017, In International Conference on Machine Learning
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Understanding back-translation atscale,2018, arXiv preprint arXiv:1808
 Convolutionalsequence to sequence learning,2017, In International Conference on Machine Learning
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Mask r-cnn,2017, In Proceedings oftheIEEE international conference on computer vision
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Layer normalization,2016, arXiv preprintarXiv:1607
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Training tips for the transformer model,2018, The Prague Bulletin ofMathematical Linguistics
 Languagemodels are unsupervised multitask learners,2019, 2019
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Neural machine translation of rare words withsubword units,2016, In ACL
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Tensor2tensor for neural machine translation,2018, CoRR
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In arXiv preprint arXiv:1506
 This example comes fromExample 2,2019,5 in (Wainwright
