title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, In ICML
 A closer lookat memorization in deep networks,2017, In ICML
 Auxiliary image regularizationfor deep cnns with noisy labels,2016, In ICLR
 Webly supervised learning of convolutional networks,2015, In ICCV
 Robust neural machine translation with doublyadversarial inputs,2019, In ACL
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Training deep neural-networks using a noise adaptationlayer,2017, In ICLR
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Deep residual learning for image recog-nition,2016, In CVPR
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In ICLR
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In NeurIPS
 Using pre-training can improve model robustnessand uncertainty,2019, In ICML
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Self-paced cur-riculum learning,2015, In AAAI
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, ICML
 The unreasonable effectiveness of noisy data for fine-grainedrecognition,2016, In ECCV
 Learning to learn from noisylabeled data,2019, In CVPR
 Learning fromnoisy labels with distillation,2017, In ICCV
 Dimensionality-driven learning with noisy labels,2018, In ICML
 Learning to label aerial images from noisy data,2012, In ICML
 Regularizing deep neuralnetworks by noise: its interpretation and optimization,2017, In NeurIPS
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In CVPR
 Better generalization with on-the-flydataset denoising,2018, arXiv preprint
 Rethinkingthe inception architecture for computer vision,2016, In CVPR
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, In ICML
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In CVPR
 Matching networks for oneshot learning,2016, In NeurIPS
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 On early stopping in gradient descent learn-ing,2007, Constructive Approximation
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 mixup: Beyond empiricalrisk minimization,2018, In ICLR
