title,year,conference
 Autoaugment:Learning augmentation policies from data,2018, arXiv:1805
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv:1708
 Shake-shake regularization,2017, arXiv:1705
 Dropblock: A regularization method for convolutionalnetworks,2018, In Annual Conference on Neural Information Processing Systems (NIPS)
 Deep networks withstochastic depth,2016, arXiv:1603
 Data augmentation by pairing samples for images classification,2018, arXiv:1801
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv:1502
 Adam: A method for stochastic optimization,2014, arXiv:1412
 Imagenet classification with deep convolu-tional neural networks,2012, In Annual Conference on Neural Information Processing Systems (NIPS)
 Fractalnet: Ultra-deep neural networkswithout residuals,2017, In International Conference on Learning Representation (ICLR)
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv:1409
 Going deeper with convolutions,2015, InConference on Computer Vision and Pattern Recognition (CVPR)
 Rethinkingthe inception architecture for computer vision,2016, In Conference on Computer Vision and PatternRecognition (CVPR)
 Efficient objectlocalization using convolutional networks,2014, arXiv:1411
 Wide residual networks,2016, arXiv:1605
 mixup: Beyond empiricalrisk minimization,2017, arXiv:1710
