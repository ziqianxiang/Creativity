title,year,conference
  Convex Optimization,0521,  Cambridge University Press
  Support-vector networks,1995,  Machine learning
   Why  relu  units  sometimes  die:  Analysis  of  single-unit  errorbackpropagation in neural networks,2018,   CoRR
  Understanding the difficulty of training deep feedforward neu-ral networks,2010,   In In Proceedings of the International Conference on Artificial Intelligence andStatistics (AISTATS10)
  When neurons fail,2017,  In 2017 IEEE International Parallel and DistributedProcessing Symposium (IPDPS)
  Principles of riemannian geometry in neural networks,2017,  In NIPS
  Delving deep into rectifiers:  Surpass-ing human-level performance on imagenet classification,2015,  CoRR
 Deep residual learning for image recog-nition,2015, CoRR
  Untersuchungen zu dynamischen neuronalen netzen,1991,  Diploma
  Batch normalization:  Accelerating deep network training byreducing internal covariate shift,2015,  CoRR
    Adam:   A  method  for  stochastic  optimization,2014,    CoRR
 Learning multiple layers of features from tiny images,2009, Technical report
 MNIST handwritten digit database,2010, 2010
 A tutorial on energy-based learning,2006, 2006
  Deep learning,0028,  Nature
   Rethinking the valueof network pruning,2018,  CoRR
   Dying  relu  and  initialization:Theory and numerical examples,2019, arXiv preprint arXiv:1903
 Rectifier nonlinearities improve neural network acoustic models,2013, 2013
  Understanding and improvingconvolutional neural networks via concatenated rectified linear units,2016,   CoRR
  Trainability and data-dependent initialization of over-parameterized  relu  neural networks,2019,   CoRR
   Efficientnet:  Rethinking model scaling for convolutional neuralnetworks,2019, CoRR
  Wide residual networks,2016,  CoRR
