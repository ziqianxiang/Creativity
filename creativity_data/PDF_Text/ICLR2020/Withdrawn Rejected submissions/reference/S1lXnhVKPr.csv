title,year,conference
 Sparse communication for distributed gradient descent,2017, InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing systems
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Tradingredundancy for communication: Speeding up distributed sgd for non-convex optimization,2019, InICML
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Error feedback fixessignsgd and other gradient compression schemes,2019, In ICML
 Convolutional neural networks for sentence classification,2014, In Proceedings of the 2014Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Deep gradient compression: Reducingthe communication bandwidth for distributed training,2018, In International Conference on LearningRepresentations
 Sarah: A novel method for machinelearning problems using stochastic recursive gradient,2017, In ICML
 Automatic differentiation inpytorch,2017, 2017
 Glove: Global vectors for wordrepresentation,2014, In Empirical Methods in Natural Language Processing (EMNLP)
 Parallel training of dnns with natural gradi-ent and parameter averaging,2014, arXiv preprint arXiv:1410
 Faster distributed deepnet training: Computation and communication decoupled stochastic gradient descent,2019, In IJCAI
 Local sgd converges fast and communicates little,2019, In ICLR 2019 ICLR 2019International Conference on Learning Representations
 Experiments on parallel training of deep neural network using modelaveraging,2015, arXiv preprint arXiv:1507
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 D2: Decentralized training overdecentralized data,2018, In ICML
 Doublesqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In ICML
 Cooperative sgd: A unified framework for the design and analysis ofcommunication-efficient sgd algorithms,2018, arXiv preprint arXiv:1808
 On the linear speedup analysis of communication efficient mo-mentum sgd for distributed non-convex optimization,2019, In ICML
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 On the convergence properties of a k-step averaging stochastic gradientdescent algorithm for nonconvex optimization,2018, In Proceedings of the 27th International JointConference on Artificial Intelligence
