title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 On the convergence rate of training recurrent neuralnetworks,2018, arXiv preprint arXiv:1810
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems 30
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, arXiv preprint arXiv:1905
 Robust statistical learning with Iipschitzand convex loss functions,2019, Probability Theory and Related Fields
 Gradient descent provably optimizesover-parameterized neural networks,2019, International Conference on Learning Representations 7
 Greedy function approximation: a gradient boosting machine,2001, The Annals ofStatistics
 Approximation capabilities of multilayer feedforward networks,1991, Neural networks
 Online learning with kernels,2004, IEEEtransactions on signal processing
 Empirical margin distributions and bounding thegeneralization error of combined classifiers,2002, The Annals of Statistics
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems 31
 Boosting algorithms as gradientdescent,1999, In Advances in Neural Information Processing Systems 12
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Foundations of machine learning,2012, MITpress
 Stochastic particle gradient descent for infinite ensembles,2017, arXivpreprint arXiv:1712
 Towards moderate overparameterization: global con-vergence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 Understanding machine learning: From theory toalgorithms,2014, Cambridge university press
 Online learning algorithms,2006, Foundations of computational mathematics
 Learning relus via gradient descent,2017, In Advances in Neural InformationProcessing Systems 30
 Neural network with unbounded activation functions is universalapproximator,2017, Applied and Computational Harmonic Analysis
 Early stopping for kernel boosting algorithms:A general analysis with localized complexities,2017, In Advances in Neural Information ProcessingSystems 30
 Global convergence of adaptive gradient methods for anover-parameterized neural network,2019, arXiv preprint arXiv:1902
 Online regularized classification algorithms,2006, IEEE Transactions onInformation Theory
 Fast convergence of natural gradient descent foroverparameterized neural networks,2019, arXiv preprint arXiv:1905
 Learning one-hidden-layer relunetworks via gradient descent,2018, arXiv preprint arXiv:1806
 An improved analysis of training over-parameterized deep neuralnetworks,2019, arXiv preprint arXiv:1906
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
