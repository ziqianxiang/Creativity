title,year,conference
 Linear coupling: An ultimate unification of gradient andmirror descent,2017, In ITCS
 On the curved geometry of accelerated optimization,2018, arXiv preprintarXiv:1812
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing systems
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Long short-term memory,1997, Neural computation
 Accelerated gradient methods for stochastic optimiza-tion and online learning,2009, In Advances in Neural Information Processing Systems
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing Systems
 Algorithms ofrobust stochastic optimization based on mirror descent method,2019, arXiv preprint arXiv:1907
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Validation analysis of mirror descentstochastic approximation method,2012, Mathematical programming
 A unified variance-reduced accelerated gradient method forconvex optimization,2019, arXiv preprint arXiv:1905
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Aggregated momentum: Stabilitythrough passive damping,2019, In International Conference on Learning Representations
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Regularizing and optimizing lstm lan-guage models,2017, arXiv preprint arXiv:1708
 Gradient methods for minimizing composite functions,2013, Mathematical Programming
 Proximal algorithms,2014, Foundations and TrendsR in Optimization
 Automatic differentiation inpytorch,2017, 2017
 On the convergence of adam and beyond,2018, InInternational Conference on Learning RePresentations
 On accelerated proximal gradient methods for convex-concave optimization,2008, 2008
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Wide residual networks,2016, arXiv PrePrintarXiv:1605
 A simple stochastic variance reduced algorithmwith fast convergence rates,2018, In International conference on machine learning
 DirectAcceleration of SAGA using Sampled Negative Momentum,2019, In AISTATS
