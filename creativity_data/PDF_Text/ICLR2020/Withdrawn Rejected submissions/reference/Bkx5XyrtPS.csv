title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv
 A convergence analysis of gradient de-scent for deep linear neural networks,2019, In International Conference on Learning Representations
 Gradient descent with identity initialization effi-ciently learns positive definite linear transformations,2018, In ICML
 Gradient descent findsglobal minima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Identiy matters in deep learning,2017, In International Conference onLearning Representations
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Deep linear networks with arbitrary loss: All local minimaare global,2018, In International Conference on Machine Learning
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, NeurIPS
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 Spurious valleys in two-layer neural networkoptimization landscapes,2018, arXiv preprint arXiv:1802
 Small nonlinearities in activation functions create badlocal minima in neural networks,2019, In International Conference on Learning Representations
 Solving large scale linear prediction problems using stochastic gradient descent algo-rithms,2004, In ICML
 Critical points of linear neural networks: Analytical forms and land-scape properties,2018, In International Conference on Learning Representations
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
