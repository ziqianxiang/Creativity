title,year,conference
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Adaptively sparse transformers,2019, arXivpreprint arXiv:1909
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Long short-term memory,1997, Neural computation
 Gpipe: Efficient training of giant neural networks using pipeline parallelism,2018, arXivpreprint arXiv:1811
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In ACL’ 17
 Bert for coreference resolution:Baselines and analysis,2019, arXiv preprint arXiv:1908
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting,2019, arXiv preprint arXiv:1907
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Document-level neuralmachine translation with hierarchical attention networks,2018, In EMNLP’ 18
 Passage re-ranking with bert,2019, arXiv preprintarXiv:1901
 Languagemodels are unsupervised multitask learners,2019, 2019
 Biological structure and function emerge from scaling unsupervised learningto 250 million protein sequences,2019, bioRxiv
 Matching theblanks: Distributional similarity for relation learning,2019, arXiv preprint arXiv:1906
 Low-memory neural network training: A technical report,2019, arXiv preprintarXiv:1904
 Adaptive attentionspan in transformers,2019, arXiv preprint arXiv:1905
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Newsqa: A machine comprehension dataset,2017, In Proceedings of the 2ndWOrkshop on Representation Learningfor NLP
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
1Description	Base Large	Attention dropout	0,1000,1	Λλ∕r∩rτnιιπ sfp∏Q	10K	armup seps	B Batch size	256	256	Weight decay	0
