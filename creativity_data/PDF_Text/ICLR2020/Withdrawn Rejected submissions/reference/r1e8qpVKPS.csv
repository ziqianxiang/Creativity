title,year,conference
 Alpha maml: Adaptive model-agnostic meta-learning,2019, 36th International Conference on Machine Learning
 The effects of negative adaptation in Model-Agnostic Meta-Learning,2018, arXiv preprint arXiv:1812
 Gradient agreement as an optimizationobjective for meta-learning,2018, arXiv preprint arXiv:1810
 On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms,2019, arXiv preprint arXiv:1908
 Model-agnostic metalearning for fast adaptationof deep networks,2017, International Conference on Machine Learning (ICML)
 Probabilistic Model-Agnostic Meta-Learning,2018, NeuralInformation Processing Systems 2018
 Online Meta-Learning,2019, arXivpreprint arXiv:1902
 Recasting Gradient-Based Meta-Learning as Hierarchical Bayes,2018, arXiv preprint arXiv:1801
 Towards understanding generalization in gradient-based meta-learning,2019, arXiv preprint arXiv:1907
 Unsupervised Meta-Learning for Reinforcement Learning,2018, arXiv:1806
 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,2015, arXiv:1502
 Reconciling meta-learningand continual learning with online mixtures of tasks,2019, arXiv:1812
 Provable Guarantees for Gradient-Based Meta-Learning,2019, arXiv preprint arXiv:1902
 Adam: A Method for Stochastic Optimization,2014, arXivpreprint arXiv:1412
 One shot learningof simple visual concepts,2011, In Proceedings of the 33th Annual Meeting of the Cognitive ScienceSociety
 Efficient backprop,1998, Neuralnetworks: Tricks of the trade
 Deep Learning Theory Review: An Optimal ControlAnd Dynamical Systems Perspective,2019, arXiv preprint arXiv:1908
 Optimization as a Model for Few-shot Learning,2017, The InternationalConference on Learning Representations 2017
 Learning to learn,1998, Springer
 Match-ing networks for one shot learning,2016, In Advances in Neural Information Processing Systems 29
 Toward Multimodal Model-AgnosticMeta-Learning,2018, arXiv preprint arXiv:1812
2 and 5,2014,3
