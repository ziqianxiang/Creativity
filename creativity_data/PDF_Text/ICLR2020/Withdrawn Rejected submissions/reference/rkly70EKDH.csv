title,year,conference
 Finding approxi-mate local minima faster than gradient descent,2017, In Proceedings of the 49th Annual ACM SIGACTSymposium on Theory of Computing
 On the convergence rate of training recurrent neuralnetworks,2019, In Advances in neural information processing systems
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Overfitting or perfect fitting? risk bounds forclassification and regression rules that interpolate,2018, In Advances in Neural Information ProcessingSystems
 Global optimality of local search forlow rank matrix recovery,2016, In Advances in Neural Information Processing Systems
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 Distributional and lq norm inequalities for polynomials overconvex bodies in rn,2001, Mathematical research letters
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Advances in neural information processingsystems
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Escaping from saddle pointsonline stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Matrix completion has no spurious local minimum,2016, InAdvances in Neural Information Processing Systems
 No spurious local minima in nonconvex low rank problems: Aunified geometric analysis,2017, In International Conference on Machine Learning
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2017, arXiv preprint arXiv:1711
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Polynomial-time tensor decompositions with sum-of-squares,2016, In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 Towards moderate overparameterization: global conver-gence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 Non-squarematrix sensing without spurious local minima via the burer-monteiro approach,2016, arXiv preprintarXiv:1609
 Learning relus via gradient descent,2017, In Advances in Neural InformationProcessing Systems
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 Recovery guaranteesfor one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
 An improved analysis of training over-parameterized deep neuralnetworks,2019, arXiv preprint arXiv:1906
 The proof here is similar to that of Lemma 11,2020, Let mÎ¹
