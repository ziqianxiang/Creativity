title,year,conference
 A simple but tough-to-beat baseline for sentenceembeddings,2017, In Proceedings of the International Conference on Learning Representations
 Massively multilingual sentence embeddings for zero-shotcross-lingual transfer and beyond,2018, arXiv preprint arXiv:1812
 A large anno-tated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing
 Universal sentence encoder,2018, arXivpreprint arXiv:1803
 A multi-task approach for dis-entangling syntax and semantics in sentence representations,2019, arXiv preprint arXiv:1904
 Senteval: An evaluation toolkit for universal sentence represen-tations,2018, arXiv preprint arXiv:1803
 Whatyou can cram into a single vector: Probing sentence embeddings for linguistic properties,2018, arXivpreprint arXiv:1805
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Lagging inferencenetworks and posterior collapse in variational autoencoders,2019, arXiv preprint arXiv:1901
 Learning distributed representations of sentencesfrom unlabelled data,2016, In Proceedings of the 2016 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Long short-term memory,1997, Neural computation
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Skip-thought vectors,2015, In Advances in Neural Information ProcessingSystems 28
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, arXiv preprint arXiv:1808
 Distributed repre-sentations of words and phrases and their compositionality,2013, In Advances in Neural InformationProcessing Systems
 Glove: Global vectors for wordrepresentation,2014, Proceedings of Empirical Methods in Natural Language Processing (EMNLP2014)
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Training tips for the transformer model,2018, The Prague Bulletin ofMathematical Linguistics
 Sentence-bert: Sentence embeddings using siamese bert-networks,2019, arXiv preprint arXiv:1908
 Filtering and mining parallel data in a joint multilingual space,2018, arXiv preprintarXiv:1805
 Learning joint multilingual sentence representations withneural machine translation,2017, arXiv preprint arXiv:1704
 Learning generalpurpose distributed sentence representations via large scale multi-task learning,2018, arXiv preprintarXiv:1804
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 ParaNMT-50M: Pushing the limits of paraphrastic sentence em-beddings with millions of machine translations,2018, In Proceedings of the 56th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers)
 No training required: Exploring random encoders for sentenceclassification,2019, arXiv preprint arXiv:1901
 Towards universal paraphrastic sen-tence embeddings,2016, In Proceedings of the International Conference on Learning Representations
 Charagram: Embedding words andsentences via character n-grams,2016, In Proceedings of the 2016 Conference on Empirical Methodsin Natural Language Processing
 Beyond bleu: Trainingneural machine translation with semantic similarity,2019, In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics
 Simple and effectiveparaphrastic similarity from parallel translations,2019, Proceedings of the ACL
 Overview of the third bucc shared task:Spotting parallel sentences in comparable corpora,2018, In Proceedings of 11th Workshop on Buildingand Using Comparable Corpora
