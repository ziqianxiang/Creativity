title,year,conference
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in Neural Information Processing Systems
 Deep frank-wolfe for neural networkoptimization,2018, arXiv preprint arXiv:1811
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning to optimize,2016, arXiv preprint arXiv:1606
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Learning gradient descent: Better generalization and longerhorizons,2017, arXiv preprint arXiv:1703
 On the convergence of adam and beyond,2018, 2018
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Meta-learning with backpropagation,2001, InNeural Networks
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
