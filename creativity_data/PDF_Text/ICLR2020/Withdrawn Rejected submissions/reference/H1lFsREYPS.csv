title,year,conference
  Unified language model pre-training for natural language understandingand generation,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Two-stage synthesis networks for transferlearning in machine comprehension,2017,  In Proceedings of the conference on Empirical Methods inNatural Language Processing (EMNLP)
   Korquad1,2019,0: Korean qa dataset for machinereading comprehension
   Neural  models  for  key  phrase  extraction  and  question  generation,2018,   In  Proceedingsof Annual Meeting of the Association for Computational Linguistics (ACL)
   Xlnet: Generalized autoregressive pretraining for language understanding,2019,   arXiv preprintarXiv:1906
 Paragraph-level neural question generationwith maxout pointer and gated self-attention networks,2018,  In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing (EMNLP)
