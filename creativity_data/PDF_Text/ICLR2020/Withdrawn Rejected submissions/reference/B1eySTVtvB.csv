title,year,conference
 On the validityof self-attention as explanation in transformer models,2019, arXiv preprint arXiv:1908
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 What does BERT lookat? an analysis of BERTâ€™s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Xnli: Evaluating cross-lingual sentence representations,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing(EMNLP)
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)
 Grammar induction with neural languagemodels: An unusual replication,2018, In Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing (EMNLP)
 Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting,2019, In Proceedings of the 33rd Conference on Neural Information Processing Systems(NeurIPS)
 Hierarchical transformers for multi-document summarization,2019, InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational Linguistics
 Pointer sentinel mixturemodels,2017, In Proceedings of the 5th International Conference on Learning Representations (ICLR)
 Automatic differentiation inPyTorch,2017, In NeurIPS Autodiff Workshop
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Modeling by shortest data description,1978, Automatica
 Buildingend-to-end dialogue systems using generative hierarchical neural network models,2016, In Proceedingsof the 30th AAAI Conference on Artificial Intelligence (AAAI)
 Neural language modelingby jointly learning syntax and lexicon,2018, In Proceedings of the 6th International Conference onLearning Representations (ICLR)
 Ordered neurons: Integratingtree structures into recurrent neural networks,2019, In Proceedings of the 7th International Conferenceon Learning Representations (ICLR)
 Novel positional encodings to enable tree-based trans-formers,2019, In Proceedings of the 33rd Conference on Neural Information Processing Systems(NeurIPS)
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, arXiv preprint arXiv:1909
 Mass: Masked sequence to sequencepre-training for language generation,2019, In Proceedings of the 36th International Conference onMachine Learning (ICML)
 Learn-ing to extract cross-session search tasks,2013, In Proceedings of the 22nd international conference onWorld Wide Web (WWW)
 Acomprehensive survey on graph neural networks,2019, arXiv preprint arXiv:1901
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
