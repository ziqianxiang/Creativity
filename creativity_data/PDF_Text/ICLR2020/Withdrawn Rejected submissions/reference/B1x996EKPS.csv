title,year,conference
 A little is enough: Circumventingdefenses for distributed learning,2019, arXiv preprint arXiv:1902
 Machinelearning with adversaries: Byzantine tolerant gradient descent,2017, In Neural InformationProcessing Systems
 Online learning and stochastic approximations,1998, Online learning in neuralnetworks
 Sgd:Decentralized byzantine resilience,2019, arXiv preprint arXiv:1905
 The hidden vulnerabilityof distributed learning in Byzantium,2018, In Jennifer Dy and Andreas Krause
 Multivariate estimation with high breakdown point,1985, Mathematicalstatistics and applications
 Phocas: dimensional byzantine-resilient stochastic gradient descent,2018, arXiv preprint arXiv:1805
 Zeno: Byzantine-suspicious stochasticgradient descent,2018, arXiv preprint arXiv:1805
