title,year,conference
 Modular multitask reinforcement learning with policysketches,2017, In International Conference on Machine Learning
 Representation learning: A review and newperspectives,2013, IEEE transactions on pattern analysis and machine intelligence
 Learningaction representations for reinforcement learning,2019, arXiv preprint arXiv:1902
 Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectoryembeddings,2018, In International Conference on Machine Learning
 Quantifying generalizationin reinforcement learning,2018, arXiv preprint arXiv:1812
 Unsupervised learning of disentangled representations from video,2017, In Advancesin neural information processing systems
 Deep reinforcementlearning in large discrete action spaces,2015, arXiv preprint arXiv:1512
 Self-supervised visual planning withtemporal skip connections,2017, In Conference on Robot Learning
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In International Conference on Machine Learning
 Shapestacks: Learning vision-based physical intuition for generalised object stacking,2018, In Proceedings of the European Conferenceon Computer Vision (ECCV)
 The principle of maximum entropy,1985, The mathematical intelligencer
 Deepreinforcement learning with a natural language action space,2015, arXiv preprint arXiv:1511
 Darla: Improving zero-shot transfer inreinforcement learning,2017, In International Conference on Machine Learning
 Information theory and statistical mechanics,1957, Physical review
 EMI:Exploration with mutual information,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 AUto-encoding variational bayes,2014, 2014
 CUriosity driven explo-ration of learned disentangled goal spaces,2018, In AUde Billard
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Challenging common assUmptions in the UnsUpervised learning of dis-entangled representations,2019, In International Conference on Machine Learning
 Learning latent plans from play,2019, arXiv preprint arXiv:1903
 Human-level controlthrough deep reinforcement learning,2015, Nature
 Asynchronous methods for deep reinforcementlearning,2016, In International Conference on Machine Learning
 Visualreinforcement learning with imagined goals,2018, In Advances in Neural Information ProcessingSystems
 Gotta learn fast: Anew benchmark for generalization in rl,2018, arXiv preprint arXiv:1804
 Continuallifelong learning with neural networks: A review,2018, arXiv preprint arXiv:1802
 Learning to controlself-assembling morphologies: A study of generalization via modularity,2019, In arXiv preprintarXiv:1902
 Recogym:A reinforcement learning environment for the problem of product recommendation in onlineadvertising,2018, arXiv preprint arXiv:1808
 Graph networks as learnable physics engines for inference andcontrol,2018, In International Conference on Machine Learning
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Bidirectional recurrent neural networks,1997, IEEE Transactions onSignalProcessing
 Improving generalization for ab-stract reasoning tasks using disentangled feature representations,2018, arXiv preprint arXiv:1811
 Reinforcement learning: An introduction,2018, MIT press
 Policy gradient meth-ods for reinforcement learning with function approximation,2000, In Advances in neural informationprocessing systems
 The natural language of actions,2019, arXiv preprint arXiv:1902
 Principles of risk minimization for learning theory,1992, In Advances in neuralinformation processing systems
 Nervenet: Learning structured policy withgraph neural networks,2018, In International Conference on Learning Representations
 In I,2017, Guyon
 Neuraltask programming: Learning to generalize across hierarchical tasks,2017, In International Conferenceon Robotics and Automation
 Maximum entropy inversereinforcement learning,2008, 2008
 This process keeps the earlier layers of the policy but fine-tunes theactual action selection in the final layer by re-initializing the output layer,2020, This process is able toadapt to an unseen action set of any size
 A ball is launched at a given tool from various angles andspeeds and then interacts with the tool,1024, The properties of the tool will determine the deflection pathof the ball
