title,year,conference
 signSGD: Compressed optimi-sation for non-convex problems,2018, In Proceedings of the International Conference on MachineLearning
 On the generalization ability of on-line learningalgorithms,2004, IEEE Transactions on Information Theory
 Closing the generalization gap of adaptive gradient methods in training deepneural networks,2018, arXiv preprint arXiv:1806
 The Cambridge dictionary of statistics,2006, Cambridge University Press
 Gluoncv and gluonnlp: Deep learning in computer vision andnatural language processing,2019, arXiv preprint arXiv:1907
 Deep residual learning for image recognition,2016, In Proceedingsof the International Conference on Computer Vision and Pattern Recognition
 SGDR: Stochastic gradient descent with warm restarts,2017, In Proceedingsof the International Conference on Learning Representations
 Adaptive gradient methods with dynamic bound of learningrate,2019, Proceedings of the International Conference on Learning Representations
 Adaptive bound optimization for online convex optimization,2010, InProceedings of the Annual Conference on Computational Learning Theory
 Pointer sentinel mixture models,2017, In Proceedingsof the International Conference on Learning Representations
 Regularizing and optimizing LSTM language models,2018, InProceedings of the International Conference on Learning Representations
 Problem Complexity and Method Efficiency in Optimization,1983, Wiley
 On the convergence of adam and beyond,2018, In Proceedings of theInternational Conference for Learning Representations
 No more pesky learning rates,2013, In Proceedings of the InternationalConference on Machine Learning
 Stochastic gradient descent for non-smooth optimization: Convergence re-sults and optimal averaging schemes,2013, In Proceedings of the International Conference on MachineLearning
 Layer-specific adaptive learning ratesfor deep networks,2015, In Proceedings of the International Conference on Machine Learning andApplications
 On the importance of initialization and momentumin deep learning,2013, In Proceedings of the International Conference on Machine Learning
 The marginal value of adaptivegradient methods in machine learning,2017, In Advances in Neural Information Processing Systems
 Large batch training of convolutional networks,2017, arXiv preprintarXiv:1707
 Normalized gradient with adaptive stepsizemethod for deep neural network training,2017, arXiv preprint arXiv:1707
 Adaptive methods for nonconvex optimiza-tion,2018, In Advances in Neural Information Processing Systems
 Recurrent neural network regularization,2014, arXiv preprintarXiv:1409
 ADADELTA: An adaptive learning rate method,2012, Preprint arXiv:1212
 Theory ofdeep learning iii: Generalization properties of sgd,2017, Technical report
 Adashift: Decorrelation and convergenceof adaptive learning rate methods,2018, arXiv preprint arXiv:1810
 A sufficient condition for convergences of adam andrmsprop,2019, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
	â–¡D	Proof of Proposition 2Proof,2004, Let (xit
