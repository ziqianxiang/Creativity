title,year,conference
 What does BERT lookat? an analysis of bertâ€™s attention,2019, CoRR
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Automatically constructing a corpus of sen-tential paraphrases,2005,	In Third International Workshop on Paraphrasing(IWP2005)
 Hotflip: White-box adversarial examplesfor NLP,2017, CoRR
 Is BERT really robust? naturallanguage attack on text classification and entailment,2019, CoRR
 Textbugger: Generating adversarial textagainst real-world applications,2018, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Deep neural networks are easily fooled: Highconfidence predictions for unrecognizable images,2014, 2015 IEEE Conference on Computer Visionand Pattern Recognition (CVPR)
 Probing neural network comprehension of natural languagearguments,2019, CoRR
 Lan-guage models are unsupervised multitask learners,2018, 2018
 Matching theblanks: Distributional similarity for relation learning,2019, CoRR
 BERT rediscovers the classical NLP pipeline,2019, CoRR
 Attention is all you need,2017, CoRR
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, CoRR
 Generating natural adversarial examples,2017, CoRR
