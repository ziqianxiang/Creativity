title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, In Advances inNeural Information Processing Systems
 In Proc,2018, of the 6th Int
 Efficient optimization of loops and limits with randomizedtelescoping sUms,2019, ArXiv
 Learning long-term dependencies with gradientdescent is difficUlt,1994, IEEE transactions on neural networks
 Generic Methods for Optimization-Based Modeling,2012, In AISTATS
 Understanding the difficUlty of training deep feedforward neUralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Using deep q-learning to control optimization hyperparameters,2016, arXiv preprintarXiv:1602
 Densely connectedconvolUtional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Increased rates of convergence through learning rate adaptation,1988, Neural networks
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Sgdr: Stochastic gradient descent with warm restarts,2017, ICLR
 Scalable gradient-based tuningof continuous regularization hyperparameters,2016, In International conference on machine learning
 Self-tuningnetworks: Bilevel optimization of hyperparameters using structured best-response functions,2019, arXivpreprint arXiv:1903
 Gradient-based hyperparameter opti-mization through reversible learning,2015, In Proceedings of the 32nd International Conference onMachine Learning
 Coin betting and parameter-free online learning,2016, In Advances inNeural Information Processing Systems
 Fast exact multiplication by the hessian,1994, Neural computation
 Hyperparameter optimization with approximate gradient,2016, In Proceedings of the33rd International Conference on Machine Learning
 No more pesky learning rates,2013, In InternationalConference on Machine Learning
 Local gain adaptation in stochastic gradient descent,1999, In 1999 Ninth InternationalConference on Artificial Neural Networks ICANN 99
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Accelerating the convergence ofthe back-propagation method,1988, Biological cybernetics
 Learned optimizers that scale and generalize,2017, InInternational Conference on Machine Learning
 Understanding short-horizon bias instochastic meta-optimization,2018, arXiv preprint arXiv:1803
 Wide residual networks,2016, arXiv preprint arXiv:1605
