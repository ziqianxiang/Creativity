title,year,conference
 Learning representations by maximizingmutual information across views,2019, arXiv preprint arXiv:1906
 Greedy layer-wise trainingof deep networks,2007, In Advances in neural information processing systems
 Large scale gan training for high fidelity naturalimage synthesis,2018, arXiv preprint arXiv:1809
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Large scale adversarial representation learning,2019, arXiv preprintarXiv:1907
 Adversarial feature learning,2016, arXiv preprintarXiv:1605
 Data-efficientimage recognition with contrastive predictive coding,2019, arXiv preprint arXiv:1905
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Deep learning,2015, nature
 Generating high fidelity images with subscale pixel networksand multidimensional upscaling,2018, arXiv preprint arXiv:1812
 Distributed representa-tions of words and phrases and their compositionality,2013, In C
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 Image transformer,2018, arXiv preprint arXiv:1802
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Stand-alone self-attention in vision models,2019, arXiv preprint arXiv:1906
 Generating diverse high-fidelity images withvq-vae-2,2019, arXiv preprint arXiv:1906
 Selfie: Self-supervised pretraining for imageembedding,2019, arXiv preprint arXiv:1906
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Non-local neural networks,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Self-attention generativeadversarial networks,2018, arXiv preprint arXiv:1805
