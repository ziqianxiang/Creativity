title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 On regularization and robustness of deep neuralnetworks,2018, arXiv preprint arXiv:1810
 Evasion attacks against machine learning at test time,2013, In Joint Europeanconference on machine learning and knowledge discovery in databases
 Adversarial examples from computationalconstraints,2018, arXiv preprint arXiv:1805
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Parsevalnetworks: Improving robustness to adversarial examples,2017, In International Conference on MachineLearning
 Generalizable adversarial training via spectralnormalization,2018, arXiv preprint arXiv:1811
 Adversarial vulnerability for any classifier,2018, arXivpreprint arXiv:1802
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Distributionally robust stochastic optimization with wassersteindistance,2016, arXiv preprint arXiv:1604
 Adversarial spheres,2018, arXiv preprint arXiv:1801
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Towards deep neural network architectures robust to adversarialexamples,2014, arXiv preprint arXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Deep learning,2015, nature
 A unified gradient regularization family foradversarial examples,2015, In 2015 IEEE International Conference on Data Mining
 On detecting adversarialperturbations,2017, arXiv preprint arXiv:1702
 Distributionalsmoothing with virtual adversarial training,2015, arXiv preprint arXiv:1507
 Virtual adversarial training: a reg-ularization method for supervised and semi-supervised learning,2017, arXiv preprint arXiv:1704
 Spectral normalization forgenerative adversarial networks,2018, arXiv preprint arXiv:1802
 Deepfool: a simple andaccurate method to fool deep neural networks,2016, In Proceedings of 2016 IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Universaladversarial perturbations,2017, arXiv preprint
 Variance-based regularization with convex objectives,2017, InAdvances in Neural Information Processing Systems
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Sensitivity and generalization in neural networks: an empirical study,2018, arXiv preprintarXiv:1802
 Transferability in machine learning: fromphenomena to black-box attacks using adversarial samples,2016, arXiv preprint arXiv:1605
 Certified defenses against adversarialexamples,2018, arXiv preprint arXiv:1801
 The odds are odd: A statistical test for detectingadversarial examples,2019, arXiv preprint arXiv:1902
 Adversarial manipulation of deeprepresentations,2015, arXiv preprint arXiv:1511
 Adver-sarially robust generalization requires more data,2018, arXiv preprint arXiv:1804
 Understanding adversarial training: Increasinglocal stability of neural nets through robust optimization,2015, arXiv preprint arXiv:1511
 Very deep convolutional networks for large-scale imagerecognition,2014, In International Conference on Learning Representations (ICLR)
 Certifiable distributional robustness withprincipled adversarial training,2017, arXiv preprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Lipschitz-margin training: Scalable certificationof perturbation invariance for deep neural networks,2018, In Advances in Neural Information ProcessingSystems
 Feature squeezing: Detecting adversarial examples in deepneural networks,2017, arXiv preprint arXiv:1704
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv preprint arXiv:1705
