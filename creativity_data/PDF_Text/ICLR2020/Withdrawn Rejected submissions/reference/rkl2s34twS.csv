title,year,conference
 A closer look at memorization in deep networks,2017, In ICML
 A theory of learning fromdifferent domains,2010, MLJ
 Evolving culture versus local minima,2014, In Growing Adaptive Machines
 Imagenet: A large-scale hierarchical image database,2009, InCVPR
 Unsupervised domain adaptation by backpropagation,2015, In ICML
 Scatter component analysis : A unified frameworkfor domain adaptation and domain generalization,2017, TPAMI
 Geodesic flow kernel for unsupervised domain adaptation,2012, InCVPR
 Domain adaptation with conditional transferablecomponents,2016, In ICML
 Causal generative domainadaptation networks,2018, CoRR
 A kernel two-sample test,2012, JMLR
 Caltech-256 object category dataset,2007, Technical report
 Cross language text classification via subspace co-regularized multi-view learning,2012, InICML
 Co-teaching: Robust trainingof deep neural networks with extremely noisy labels,2018, In NeurIPS
 Cycada: Cycle-consistent adversarial domain adaptation,2018, In ICML
 Mentornet: Learning data-driven curriculum for very deepneural networks on corrupted labels,2018, In ICML
 Minimax statistical learning with wasserstein distances,2018, In NeurIPS
 Cleannet: Transfer learning for scalable image classifier training withlabel noise,2018, In CVPR
 Deep domain generalization via conditionalinvariant adversarial networks,2018, In ECCV
" Decoupling ""when to update"" from ""how to update""",2017, In NeurIPS
 Few-shot adversarial domain adaptation,2017, In NeurIPS
 Making deepneural networks robust to label noise: A loss correction approach,2017, In CVPR
 Asymmetric tri-training for unsupervised domain adaptation,2017, In ICML
 Maximum classifier discrepancy for unsupervised domainadaptation,2018, In CVPR
 Simultaneous deep transfer across domains and tasks,2015, InICCV
 Adversarial discriminative domain adaptation,2017, In CVPR
 SUN database: Large-scale scene recognitionfrom abbey to zoo,2010, In CVPR
 Learning from massive noisy labeled data for imageclassification,2015, In CVPR
 Domain adaptation under target and conditional shift,2013, InICML
 Learning rate is set to 0,1000,01 forsimulated tasks and 0
