title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Neural network learning: Theoretical foundations,2009, cambridgeuniversity press
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Spectrally-normalized margin bounds for neuralnetworks,2017, 06 2017
 Almost linear vc dimension bounds for piecewisepolynomial networks,1999, In Advances in Neural Information Processing Systems
 Reconciling modern machine learningand the bias-variance trade-off,2018, arXiv preprint arXiv:1812
 Overfitting or perfect fitting? risk bounds forclassification and regression rules that interpolate,2018, 06 2018b
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Entropy-sgd: Biasing gradient descentinto wide valleys,2016, arXiv preprint arXiv:1611
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, arXiv preprint arXiv:1805
 Training neural networks as learning data-adaptive kernels:Provable representation and approximation benefits,2019, arXiv preprint arXiv:1901
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, Proceedings of the 36th International Conference on MachineLearning
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Linearized two-layersneural networks in high dimension,2019, arXiv preprint arXiv:1904
 Learning neural networks with two nonlinear layers in polynomialtime,2017, arXiv preprint arXiv:1709
 Learning one convolutional layer with overlappingpatches,2018, arXiv preprint arXiv:1802
 Size-independent sample complexity ofneural networks,2017, arXiv preprint arXiv:1712
 Understanding generalization through visualizations,2019, 2019
 A hessian based complexity measure fordeep networks,2019, arXiv preprint arXiv:1905
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 The concentration of measure phenomenon,2001, volume 89 of Mathematical Surveys andMonographs
 Arpack users guide: Solution of large scale eigenvalueproblems by implicitly restarted arnoldi methods,1998, SIAM
 Visualizing the loss landscapeof neural nets,2018, In Advances in Neural Information Processing Systems
 Gradient descent with early stopping is prov-ably robust to label noise for overparameterized neural networks,2019, arXiv preprint arXiv:1903
 Hessian based analysisof sgd for deep nets: Dynamics and generalization,2019, arXiv preprint arXiv:1907
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, NeurIPS
 A comparative analysis of the optimization and generalization propertyof two-layer neural network and random feature models under gradient descent dynamics,2019, arXivpreprint arXiv:1904
 A vector-contraction inequality for rademacher complexities,2016, In InternationalConference on Algorithmic Learning Theory
 A mean field view of the landscape oftwo-layers neural networks,2018, arXiv preprint arXiv:1804
 Deterministic pac-bayesian generalization bounds for deepnetworks via generalizing noise-resilience,2019, arXiv preprint arXiv:1905
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprintarXiv:1707
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Refined generalization analysis of gradient descent for over-parameterized two-layer neural networks with smooth activations on classification problems,2019, arXivpreprint arXiv:1905
 Towards moderate overparameterization: global con-vergence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 The full spectrum of deep net hessians at scale: Dynamics with sample size,2018, arXivpreprint arXiv:1811
 The full spectrum of deepnet hessians at scale: Dynamics with sgd training andsample size,2019, arXiv preprint arXiv:1811
 Measuring the spectrum of deepnet hessians,2019, 2019b
 Neural networks as interacting particle systems:Asymptotic convexity of the loss landscape and universal scaling of the approximation error,0520, 052018
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Perturbation bounds for matrix square roots and pythagorean sums,1992, Linearalgebra and its applications
 Mean field analysis of neUral networks: A centrallimit theorem,2018, 08 2018
 A mean field View of the landscape of two-layers neUralnetworks,2018, In Proceedings of the National Academy of Sciences
 On learning oVer-parameterized neUral networks: A fUnctional approxi-mation prospectiVe,2019, arXiv preprint arXiv:1905
 Rademacher complexity for adVersarially robUstgeneralization,2018, arXiv preprint arXiv:1810
 A UsefUl Variant of the daVis-kahan theorem forstatisticians,2014, Biometrika
 Understandingdeep learning reqUires rethinking generalization,2016, arXiv preprint arXiv:1611
