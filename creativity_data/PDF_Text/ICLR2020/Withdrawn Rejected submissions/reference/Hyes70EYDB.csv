title,year,conference
 Synthesizing robust adversarialexamples,2017, arXiv preprint arXiv:1707
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, ICML
 Towards evaluating the robustness of neural networks,2017, In IEEESymposium on Security and Privacy (SP)
 Grad-CAM++: Generalized gradient-based visual explanations for deep convolutional networks,2018, In2018 IEEE Winter Conference on Applications of Computer Vision (WACV)
 Ead: elastic-net attacks todeep neural networks via adversarial examples,2018, AAAI
 Learning perceptually-aligned representations via adversarial robustness,2019, arXiv preprintarXiv:1906
 Robust physical-world attacks on deep learning visual classification,2018, In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Interpretation of neural networks is fragile,2019, InProceedings of the AAAI Conference on Artificial Intelligence
 Explaining and harnessing adversarialexamples,2015, ICLR
 Testing robustness againstunforeseen adversaries,2019, arXiv preprint arXiv:1908
 Towards deep learning models resistant to adversarial attacks,2018, ICLR
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Rise: Randomized input sampling for explanation ofblack-box models,2018, arXiv preprint arXiv:1806
 Grad-CAM: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Deep inside convolutional networks: Vi-sualising image classification models and saliency maps,2013, arXiv preprint arXiv:1312
 Smoothgrad:removing noise by adding noise,2017, arXiv preprint arXiv:1706
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Is robustnessthe cost of accuracy?-a comprehensive study on the robustness of 18 deep image classificationmodels,2018, arXiv preprint arXiv:1808
 Towards hiding adversarial exam-ples from network interpretation,2018, arXiv preprint arXiv:1812
 Intriguing properties of neural networks,2014, ICLR
 Interpreting adversarial examples by activation promotion and suppression,2019, arXiv preprintarXiv:1904
 Structured adversarial attack: Towards general implementation and betterinterpretability,2019, In International Conference on Learning Representations
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
 Interpretable deep learningunder fire,2018, CoRR
 Learning deep features for discrim-inative localization,2016, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
