title,year,conference
 The importance of better models in stochastic optimization,0027, Proceedingsof the National Academy of Sciences
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, CoRR
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprint arXiv:1910
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identifying key algorithm parameters andinstance features using forward selection,2013, In International Conference on Learning and IntelligentOptimization
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Are gans createdequal? a large-scale study,2018, In Advances in neural information processing Systems
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Associationfor Computational Linguistics: Human Language Technologies
 An empirical study on hyperparametertuning of decision trees,2018, arXiv preprint arXiv:1812
 On the state of the art of evaluation in neural languagemodels,2018, In International Conference on Learning Representations
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 DeepOBS: A deep learning optimizerbenchmark suite,2019, In International Conference on Learning Representations
 On the importance of initializationand momentum in deep learning,2013, In Sanjoy Dasgupta and David McAllester (eds
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a running average of itsrecent magnitude
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
