title,year,conference
 Deep Frank-Wolfe for neural networkoptimization,2019, International Conference on Learning Representations
 A large annotatedcorpus for learning natural language inference,2015, Conference on Empirical Methods in NaturalLanguage Processing
 A generalized subgradient method with relaxation step,1995, Mathematical Programming
 Convex optimization: Algorithms and complexity,2015, Foundations and Trends inMachine Learning
 Padam: Closing the generalization gap of adaptive gradient methodsin training deep neural networks,2018, arXiv preprint
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, International Conference on Learning Representations
 Supervisedlearning of universal sentence representations from natural language inference data,2017, Conferenceon Empirical Methods in Natural Language Processing
 Adabatch: Efficient gradient aggregation rules for sequentialand parallel stochastic gradient methods,2017, arXiv preprint
 An algorithm for quadratic programming,1956, Naval ResearchLogistics Quarterly
 Densely connectedconvolutional networks,2017, Conference on Computer Vision and Pattern Recognition
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations
 Block-coordinate Frank-Wolfe optimization for structural SVMs,2013, International Conference on Machine Learning
 On the convergence of stochastic gradient descent with adaptivestepsizes,2019, International Conference on Artificial Intelligence and Statistics
 A unified optimizationview on generalized matching pursuit and frank-wolfe,2017, International Conference on ArtificialIntelligence and Statistics
 Adaptive gradient methods with dynamicbound of learning rate,2019, International Conference on Learning Representations
 Variants of rmsprop and adagrad with logarithmicregret bounds,2017, International Conference on Machine Learning
 Scale-free algorithms for online linear optimization,2015, InternationalConference on Algorithmic Learning Theory
 Automatic differentiation inpytorch,2017, NIPS Autodiff Workshop
 Minimization of unsmooth functionals,1969, USSR Computational Mathematicsand Mathematical Physics
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 No more pesky learning rates,2013, International Conferenceon Machine Learning
 DeepOBS: A deep learning optimizer benchmarksuite,2019, International Conference on Learning Representations
 Accelerated proximal stochastic dual coordinate ascent forregularized loss minimization,2016, Mathematical Programming
 Minimization methods for non-differentiable functions,1985, Springer Series inComputational Mathematics
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2019, International Conference on ArtificialIntelligence and Statistics
 The marginalvalue of adaptive gradient methods in machine learning,2017, Neural Information Processing Systems
 Adaptive methodsfor nonconvex optimization,2018, Neural Information Processing Systems
 ADADELTA: an adaptive learning rate method,2012, arXiv preprint
 Bpgrad: Towards global optimality in deeplearning via branch and pruning,2017, Conference on Computer Vision and Pattern Recognition
 Follow the moving leader in deep learning,2017, International Conferenceon Machine Learning
 Sgd converges toglobal minimum in deep learning via star-convex path,2019, International Conference on LearningRepresentations
