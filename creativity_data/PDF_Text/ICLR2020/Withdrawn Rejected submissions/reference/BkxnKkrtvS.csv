title,year,conference
 Generating sentences from a continuous space,2015, arXiv preprint arXiv:1511
 Learning a named entity tagger from gazetteerswith the partial perceptron,2009, In AAAI Spring Symposium: Learning by Reading and Learning toRead
 Grn:Gated relation network to enhance convolutional neural network for named entity recognition,2019, InProceedings of AAAI
 Semi-supervised se-quence modeling with cross-view training,2018, arXiv preprint arXiv:1809
 Differentiable perturb-and-parse: Semi-supervised parsing with a struc-tured variational autoencoder,2018, arXiv preprint arXiv:1807
 Latent alignment andvariational attention,2018, In Advances in Neural Information Processing Systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Allennlp: A deep semantic natural languageprocessing platform,2018, arXiv preprint arXiv:1803
 Marginal likelihood train-ing of bilstm-crf for biomedical named entity recognition from disjoint label sets,2018, In Proceedingsof the 2018 Conference on Empirical Methods in Natural Language Processing
 beta-vae: Learning basic visual concepts with aconstrained variational framework,2017, In International Conference on Learning Representations
 Training products of experts by minimizing contrastive divergence,2002, Neuralcomputation
 Categorical reparameterization with gumbel-softmax,2016, arXivpreprint arXiv:1611
 Semi-supervisedconditional random fields for improved sequence segmentation and labeling,2006, In Proceedings ofthe 21st International Conference on Computational Linguistics and the 44th annual meetingof the Association for Computational Linguistics
 Better modeling of incompleteannotations for named entity recognition,2019, In Proceedings of NAACL
 An introductionto variational methods for graphical models,1999, Machine learning
 Semi-amortizedvariational autoencoders,2018, arXiv preprint arXiv:1802
 UnsUPer-vised recurrent neural network grammars,2019, arXiv preprint arXiv:1904
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Conditional random fields: Proba-bilistic models for segmenting and labeling sequence data,2001, 2001
 Towards improving neural named entity recognitionwith gazetteers,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 End-to-end sequence labeling via bi-directional lstm-cnns-crf,2016, arXivpreprint arXiv:1603
 The concrete distribution: A continuousrelaxation of discrete random variables,2016, arXiv preprint arXiv:1611
 Differentiable dynamic programming for structured predictionand attention,2018, arXiv preprint arXiv:1802
 Language as a latent variable: Discrete generative models for sen-tence compression,2016, arXiv preprint arXiv:1609
 Monte carlo analysis of reactivity coefficients in fast reactors general theory andapplications,1967, Technical report
 Monte carlo gradient esti-mation in machine learning,2019, arXiv preprint arXiv:1906
 Perturb-and-map random fields: Using discrete optimizationto learn and sample from energy models,2011, In 2011 International Conference on Computer Vision
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Training tips for the transformer model,2018, The Prague Bulletin ofMathematical Linguistics
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Black box variational inference,2014, In ArtificialIntelligence and Statistics
 Design challenges and misconceptions in named entity recognition,2009, InProceedings of the thirteenth conference on computational natural language learning
 Learning representations byback-propagating errors,1988, Cognitive modeling
 Cheap and fastâ€”but is it good?:evaluating non-expert annotations for natural language tasks,2008, In Proceedings of the conference onempirical methods in natural language processing
 Fast and accurate entityrecognition with iterated dilated convolutions,2017, arXiv preprint arXiv:1702
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 Distantly super-vised ner with partial annotation learning and reinforcement learning,2018, In Proceedings of the 27thInternational Conference on Computational Linguistics
 Semi-supervised structuredprediction with neural crf autoencoder,2017, In Proceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing
