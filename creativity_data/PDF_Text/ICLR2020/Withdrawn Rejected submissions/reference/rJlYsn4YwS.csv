title,year,conference
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 Convex optimization,2004, Cambridge university press
 Distributed optimization of deeply nested systems,2014, InArtificial Intelligence and Statistics
 Deep learning with python,2017, Manning Publications Co
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Global convergencein deep learning with variable splitting via the kurdyka-ojasiewicz property,2018, arXiv preprintarXiv:1803
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 A proximal block coordinate descentalgorithm for deep neural network training,2018, 2018
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Rectifier nonlinearities improve neural networkacoustic models,2013, 30(1):3
 Recurrentneural network based language model,2010, In Eleventh Annual Conference of the International SpeechCommunication Association
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Learning representations byback-propagating errors,1986, nature
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Trainingneural networks without gradients: A scalable admm approach,2016, In International Conference onMachine Learning
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Efficient vlsi implementation of neural networks withhyperbolic tangent activation function,2014, IEEE Transactions on Very Large Scale Integration (VLSI)Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 A convergence analysis of nonlinearly constrained admmin deep learning,2019, arXiv preprint arXiv:1902
 Efficient training of very deep neural networksfor supervised hashing,2016, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
