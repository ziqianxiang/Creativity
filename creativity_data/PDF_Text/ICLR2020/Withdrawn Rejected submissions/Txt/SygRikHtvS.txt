Under review as a conference paper at ICLR 2020
Coresets for Accelerating
Incremental Gradient Methods
Anonymous authors
Paper under double-blind review
Ab stract
Many machine learning problems reduce to the problem of minimizing an expected
risk. Incremental gradient (IG) methods, such as stochastic gradient descent and
its variants, have been successfully used to train the largest of machine learning
models. IG methods, however, are in general slow to converge and sensitive to
stepsize choices. Therefore, much work has focused on speeding them up by
reducing the variance of the estimated gradient or choosing better stepsizes. An
alternative strategy would be to select a carefully chosen subset of training data,
train only on that subset, and hence speed up optimization. However, it remains an
open question how to achieve this, both theoretically as well as practically, while
not compromising on the quality of the final model. Here we develop CRAIG,
a method for selecting a weighted subset (or coreset) of training data in order to
speed up IG methods. We prove that by greedily selecting a subset S of training
data that minimizes the upper-bound on the estimation error of the full gradient,
running IG on this subset will converge to the (near)optimal solution in the same
number of epochs as running IG on the full data. But because at each epoch the
gradients are computed only on the subset S, we obtain a speedup that is inversely
proportional to the size of S. Our subset selection algorithm is fully general and
can be applied to most IG methods. We further demonstrate practical effectiveness
of our algorithm, CRAIG, through an extensive set of experiments on several
applications, including logistic regression and deep neural networks. Experiments
show that CRAIG, while achieving practically the same loss, speeds up IG methods
by up to 10x for convex and 3x for non-convex (deep learning) problems.
1 Introduction
Mathematical optimization lies at the core of training large-scale machine learning systems, and
is now widely used over massive data sets with great practical success, assuming sufficient data
resources are available. Achieving this success, however, also requires large amounts of (often GPU)
computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019).
Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one
of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019).
Training machine learning models often reduces to the problem of optimizing a regularized empirical
risk function. Given a convex loss l, and a μ-strongly convex regularize] r, one aims to find model
parameter vector x* over the parameter space X that minimizes the loss f over the training data V:
x* ∈ argmin f (x), f(x) :=	fi(x) + r(x),	fi(x) = l(x, (ai, yi)),
x∈X	i∈V
(1)
where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd → R are associated
with training examples (ai, yi), where ai ∈ Rd is the feature vector, and yi is the point i’s label.
The standard Gradient Descent can find the minimizer of this problem, but requires repeated com-
putations of the full gradient Vf (x)—sum of the gradients over all training data points/functions
i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of
deep neural networks where gradient computations (backpropagation) are expensive. Incremental
Gradient (IG) methods, such as Stochastic Gradient Descent and its accelerated variants, including
SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014),
1
Under review as a conference paper at ICLR 2020
SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient
on random subsets/batches of training data. While this provides an unbiased estimate of the full
gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015),
and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013;
Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused
on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson &
Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad
(Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)).
However, the direction that remains largely unexplored is how to carefully select a small subset
S ⊆ V of the full training data V , so that the model can only be trained on the subset S while still
(approximately) converging to the globally optimal solution (i.e., the model parameters that would
be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this
would directly lead to a speedup of |V |/|S| (which can be very large if |S|	|V |) per epoch of IG.
There are four main challenges in finding such a subset S. First is that a guiding principle to select
S is unclear. For example, selecting training points close to the decision boundary might allow the
model to fine tune the decision boundary, while picking the most diverse set of data points would
allow the model to get a better sense of the training data distribution. Second is that finding S must
be fast as otherwise identifying the set S may take longer than the actual optimization, and so no
overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has
to decide on an ordering over S and a gradient stepsize for each data point in S, as they affect the
convergence. And last, while the method might work well empirically on some data sets, one also
requires theoretical understanding and mathematical convergence guarantees.
Here we develop CoResets for Accelerating Incremental Gradient descent (CRAIG), for selecting a
subset of training data points to speed up training of large machine learning models. Our key idea is to
directly approximate the gradient. That is, we aim to find a weighted and ordered subset S of training
data V that is representative of the full gradient of V . We prove that the subset S that minimizes
an upper-bound on the error of estimating the full gradient of V maximizes a submodular facility
location function. As a result, the subset S can be efficiently found using a fast greedy algorithm. A
further benefit of our approach is that set S is created incrementally which induces a natural ordering
over data in S. Thus, rather than processing data points in a random or arbitrary order, CRAIG
processes them using in the order provided by the procedure, which we show further speeds up the
convergence of the method.
We also provide theoretical analysis of CRAIG and prove the convergence of our approach. In
particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the
full gradient by an error of at most e, We prove that IG method with diminishing stepsize ak = α∕kτ
at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Re∕μ neighborhood of the optimal
solution at rate O(1∕√k). Here, R = min{do, (rγmaχC + e)∕μ} where do is the initial distance to
the optimum, C is an upper-bound on the norm of the gradients, r = |S |, and γmax is the largest
weight for the elements in the subset obtained by CRAIG. Moreover, we prove that ifin addition
to the strong convexity, component functions have smooth gradients, IG with the same diminishing
stepsize on subset S converges to a 2e∕μ neighborhood of the optimum solution at rate O(1∕kτ).
The above convergence rates are the same as convergence rate of IG on V for a strongly convex
risk (and smooth component functions), and therefore IG on S converges in the same number
epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires
fewer gradient computations and thus leads to a | V∣∕∣S| speedup over traditional IG methods, while
still (approximately) converging to the optimal solution. Furthermore, CRAIG only requires the
knowledge of estimated gradient differences and does not involve any (exact) gradient calculations.
Therefore, CRAIG can be used as a simple preprocessing step before IG starts and no additional
storage or gradient calculations are required during IG, which makes CRAIG extremely practical. As
such CRAIG can be used to speed up any existing IG methods, including IG, Adam, SAGA, SVRG
as we show in the experiments section.
We demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regres-
sion (a convex optimization problem) as well as training neural networks (non-convex optimization
problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA,
SVRG, Adam, Adagrad, and NAG. In particular, CRAIG while achieving practically the same loss
2
Under review as a conference paper at ICLR 2020
and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by
up to 10x for convex and 3x for non-convex loss functions. We also demonstrate that the deliberate
ordering scheme of the CRAIG algorithm significantly improves convergence time.
2	Related Work
Convergence of IG methods has been long studied under various conditions (Zhi-Quan & Paul, 1994;
Mangasariany & Solodovy, 1994; Bertsekas, 1996; Solodov, 1998; Tseng, 1998), however IG’s
convergence rate has been characterized only more recently (See (Bertsekas, 2015) for a survey).
In particular, NediC & Bertsekas (2001) provides a O(1∕√k) convergence rate for diminishing
stepsizes αk per epoch k under a strong convexity assumption, and Gurbuzbalaban et al. (2015)
proves a O(l∕kτ) convergence rate with diminishing stepsizes ɑk = Θ(1∕kτ) for T ∈ (0,1] under
an additional smoothness assumption for the components. While these works provide convergence
on the full dataset, our analysis provides the same convergence rates on subsets obtained by CRAIG.
It has been empirically observed that ordering of data significantly affects the convergence rate of IG.
However, finding a favorable ordering for IG has been a long standing open question. Among the few
results are that of (Recht & Re, 2012) showing that without-replacement random sampling improves
convergence ofIG for least means squares problem, and the very recent result of (Gurbuzbalaban et al.,
2017) showing that a Random Reshuffling (RR) method with iterate averaging and a diminishing
stepsize Θ(1∕kτ) for T ∈ (1/2,1) converges at rate Θ(1∕k2τ) with probability one in the suboptimality
of the objective value, thus improving upon the Ω(1∕k) rate of SGD. Contrary to the above randomized
analysis, we propose the first deterministic ordering on the data points and empirically show that the
ordering provided by CRAIG provides a significant speedup for the convergence of IG.
Techniques for speeding up SGD, are mostly focused on variance reduction techniques (Roux et al.,
2012; Shalev-Shwartz & Zhang, 2013; Johnson & Zhang, 2013; Hofmann et al., 2015; Allen-Zhu
et al., 2016), and accelerated gradient methods when the regularization parameter is small (Frostig
et al., 2015; Lin et al., 2015; Xiao & Zhang, 2014). Very recently, Hofmann et al. (2015); Allen-Zhu
et al. (2016) exploited neighborhood structure to further reduce the variance of stochastic gradient
descent and improve its running time. Our CRAIG method and analysis are complementary to
variance reduction and accelerated methods. CRAIG can be applied to these methods as well to
speed them up (as we show in experiments).
3	Coresets for Incremental Gradient Descent (CRAIG)
We proceed as follows: First, we define an objective function L for selecting an optimal set S of size r
that best approximates the gradient of the full training dataset V of size n. Then, we show that L can
be turned into a submodular function F and thus S can be efficiently found using a greedy algorithm.
Crucially, we also show that the approximation error between the estimated and the true gradient can
be efficiently minimized in a way that is independent of the actual optimization procedure and thus
CRAIG can simply be used as a preprocessing step before the actual optimization starts.
Incremental gradient methods aim at estimating the full gradient Pi∈v Nfi(X) over V by iteratively
making a step based on the gradient of every function fi . Our key idea in CRAIG is that if we can
find a small subset S such that the weighted sum of the gradients of its elements closely approximates
the full gradient over V, we can apply IG only to the set S (with stepsizes equal to the weight of the
elements in S), and we should still converge to the (approximately) optimal solution, but much faster.
Specifically, our goal in CRAIG is to find the smallest subset S ⊆ V and corresponding per-element
stepsizes γj > 0 that approximate the full gradient with an error at most > 0 for all the possible
values of the optimization parameters x ∈ X .1
S* = argmin |S|, s.t. maχ IlENfi(x)-EYj Nfj (x)k≤ e.	⑵
S⊆V,γj≥0 ∀j	x∈X i∈V	j∈S
Given such an S* and associated weights {γ}j, we are guaranteed that gradient updates on S* will
be similar to the gradient updates on V regardless of the value of x.
1Note that in the worst case we may need |S* | ≈ | V | to approximate the gradient. However, as we show in
experiments, in practice we find that a small subset is sufficient to accurately approximate the gradient.
3
Under review as a conference paper at ICLR 2020
Unfortunately, directly solving the above optimization problem is not feasible, due to two problems.
Problem 1: Eq. (2) requires us to calculate the gradient of all the functions fi over the entire space
X , which is too expensive and would not lead to overall speedup. In other words, it would appear
that solving for S * is as difficult as solving problem (1), as it involves calculating Pi∈v Vfi(x) for
various x ∈ X . And Problem 2: even if calculating the normed difference between the gradients in
Eq. (2) would be fast, as we discuss later finding the optimal subset S* in NP-hard. In the following,
we address the above two challenges and discuss how we can quickly find a near-optimal subset S.
3.1 Upper-bound on the Estimation Error
We first address Problem 1, i.e., how to quickly estimate the error/discrepancy of the weighted sum
of gradients of functions fj associate with data points j ∈ S, vs the full gradient, for every x ∈ X .
Let S be a subset of r data points. Furthermore, assume that there is a mapping ςx : V → S that for
every x ∈ X assigns every data point i ∈ V to one of the elements j in S, i.e., ςx (i) = j ∈ S. Let
Cj = {i ∈ [n]∣ς(i) = j} ⊆ V be the set of data points that are assigned to j ∈ S, and γj = |Cj |
be the number of such data points. Hence, {Cj}jr=1 form a partition of V. Then, for any arbitrary
(single) x ∈ X we can write
X Vfi(X) = X(Vfi(X)-Vfςχ(i)(X) + vfς(i)(X))	⑶
i∈V	i∈V
=E(Vfi(X)-Vfςχ(i) (X)) + EYj Vfj(X).	⑷
i∈V	j∈S
Subtracting and then taking the norm of the both sides, we get an upper bound on the error of
estimating the full gradient with the weighted sum of the gradients of the functionsfj forj ∈ S. I.e.,
kXVfi(X)-XγjVfj(X)k ≤XkVfi(X)-Vfςx(i)(X)k,	(5)
i∈V	j∈S	i∈V
where the inequality follows from the triangle inequality. The upper-bound in Eq. (5) is minimized
when ςx assigns every i ∈ V to an element in S with most gradient similarity at X, or minimum
Euclidean distance between the gradient vectors at X. That is: ςx(i) ∈ argminj∈SkVfi(X) - Vfj(X)k.
Hence,
min k	Vfi(X) -	γjVfj(X)k ≤	minkVfi(X) - Vfj(X)k.	(6)
S⊆V i∈V	j∈S	i∈V j∈S
The right hand side of Eq. (6) is minimized when S is the set of r medoids (exemplars) for all the
components in the gradient space. So far, we considered upper-bounding the gradient estimation
error at a particular X ∈ X. To bound the estimation error for all X ∈ X, we consider a worst-case
approximation of the estimation error over the entire parameter space X . Formally, we define a
distance metric dij between gradients of fi and fj as the maximum normed difference between
Vfi(X) and Vfj (X) over all X ∈ X:
dij , max kVfi(X) - Vfj(X)k.	(7)
x∈X
Thus, by solving the following minimization problem, we obtain the smallest weighted subset S*
that approximates the full gradient by an error of at most for all X ∈ X :1
S* = argmin |S|,	such that L(S) ,	min dij ≤ .
S⊆V	i∈V j∈S
(8)
Note that Eq. (8) requires that the gradient error is bounded over X . However, we show (Appendix
B) for several classes of convex problems, including linear regression, ridge regression, logistic
regression, and regularized support vector machines (SVMs), the normed gradient difference between
data points can be efficiently boundedly approximated by (Allen-Zhu et al., 2016; Hofmann et al.,
2015):
∀X,i, j	IlVfi(X)- Vfj(X)k	≤	dij	≤ maχO(IlXIl)	∙	∣∣ai	-	ajII= const. |旧—aj∣∣.	(9)
Note when kXk is bounded for all X ∈ X, i.e., maxx∈X O(kXk) < ∞, upper-bounds on the Euclidean
distances between the gradients can be pre-computed. This is crucial, because it means that estimation
error of the full gradient can be efficiently bounded independent of the actual optimization problem
(i.e., point X). Thus, these upper-bounds can be computed only once as a pre-processing step before
any training takes place, and then used to find the subset S by solving the optimization problem (8).
We address upper-bounding the normed difference between gradients for deep models in Section 3.4.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 CRAIG (CoResets for Accelerating Incremental Gradient descent)
Input: Set of component functions f for i ∈ V = {1, ∙∙∙ , n}.
Output: Subset S ⊆ V with corresponding per-element stepsizes {γ}j∈S, and an ordering σ.
1:	So — 0, so = 0, i = 0.
2:	while F (S) < L({s0}) - do
3:	j ∈ argmaxe∈V \Si-1 F(e|Si-1)
4:	Si = Si-1 ∪ {j}
5:	σi = j
6:	i = i + 1
7:	end while
8:	for j = 1 to |S | do
9：	Yj = Pi∈v Ij = argm⅛s∈s maXχ∈x∣∣Vfi(x) - Vfs(x)k]
10: end for
3.2	The CRAIG Algorithm
Optimization problem (8) produces a subset S of elements with their associated weights {γ}j∈S or
per-element stepsizes that closely approximates the full gradient. Here, we show how to efficiently
approximately solve the above optimization problem in order to find a near-optimal subset S.
The optimization problem (8) is NP-hard as it involves calculating the value of L(S) for all the 2|V|
subsets S ⊆ V. We show, however, to transform it into a submodular set cover problem, for which
efficient approximation algorithms exist.
Formally, F is submodular if F(S ∪ {e}) - f(S) ≥ F(T ∪ {e}) - F(T), for any S ⊆ T ⊆ V
and e ∈ V \ T. We denote the marginal utility of an element s w.r.t. a subset S as F (e|S) =
F(S∪ {e}) - F(S). Function F is called monotone if F (e|S) ≥ 0 for any e∈ V\S and S ⊆ V. The
submodular cover problem is defined as finding the smallest set S that achieves utility ρ. Precisely,
S * = argmin |S|,	such that F(S) ≥ ρ.	(10)
S⊆V
Although finding S* is NP-hard since it captures such well-known NP-hard problems as Minimum
Vertex Cover, for many classes of submodular functions (Nemhauser et al., 1978; Wolsey, 1982), a
simple greedy algorithm is known to be very effective. The greedy algorithm starts with the empty
set So = 0, and at each iteration i, it chooses an element e ∈ V that maximizes 4(e|Si-1), i.e.,
Si = Si-1 ∪ {argmaXe∈V F (e|Si-1) - F (Si-1)}. Greedy gives us a logarithmic approximation,
i.e. |S| ≤ 1 + ln(maXe F (e|0)) |S* |. The computational complexity of the greedy algorithm
is O(|V| ∙ |S|). However, its running time can be reduced to O(|V|) using stochastic algorithms
(Mirzasoleiman et al., 2015a) and further improved using lazy evaluation (Minoux, 1978), and
distributed implementations (Mirzasoleiman et al., 2015b; 2016).
Given a subset S ⊆ V, the facility location function quantifies the coverage of the whole data set
V by the subset S by summing the similarities between every i ∈ V and its closest element j ∈ S.
Formally, facility location is defined as Ffl (S) = Pi∈V maXj∈S si,j, where si,j is the similarity
between i, j ∈ V. The facility location function has been used in a number of applications, including
scene and documents summarization (Simon et al., 2007; Lin & Bilmes, 2012).
By introducing an auxiliary element so we can turn L(S) in Eq. (8) into a monotone submodular
facility location function,
F(S) = L({so}) - L(S ∪ {so}),	(11)
where L({so}) is a constant. In words, F measures the decrease in the estimation error associated
with the set S versus the estimation error associated with just the auxiliary element. It is easy to see
that for suitable choice of so, maximizing F is equivalent to minimizing L. Therefore, we apply the
greedy algorithm to approximately solve the following problem to get the subset S defined in 8:
S* = argmin |S|,	such that F(S) ≥ L({so}) - .	(12)
S⊆V
At every step, the greedy algorithm selects an element that reduces the upper bound on the estimation
error the most. In fact, the size of the smallest subset S that estimates the full gradient by an error of
at most depends on the structural properties of the data. Intuitively, as long as the marginal gains
5
Under review as a conference paper at ICLR 2020
of facility location are considerably large, we need more elements to improve our estimation of the
full gradient. Having found S, the weight γj of every element j ∈ S is the number of components
that are closest to it in the gradient space, and are used as stepsize of element j ∈ S during IG. The
pseudocode for CRAIG subset selection method is outlined in Algorithm 1.
3.3	Ordering on the elements of the subset
Notice that CRAIG creates subset S incrementally one element at a time, which produces a natural or-
der σ to the elements in S. Adding the element with largest marginal gain j ∈ argmaxe∈V F(e|Si-1)
improves our estimation from the full gradient by an amount bounded by the marginal gain. Formally,
at every step i, we have F(Si) ≥ (1 - e-i/|SI)F(S*), and hence
k X Vfi(X)- X YjVfj(x)k ≤ Cnt -(1-e-i/1SI)L(S*).	(13)
Intuitively, the first elements of the ordering contribute the most to provide a close approximation
of the full gradient and the rest of the elements further refine the approximation. Hence, the first
incremental gradient updates gets Us close to x*, and the rest of the updates further refine the solution.
We show experimentally that processing data points in the order of S leads to faster convergence than
when we consider elements in S in a random order. We defer the formal proof to future work.
3.4	Application of CRAIG to Deep Networks
Incremental gradient methods, including SGD with momentum (Qian, 1999), Adam (Kingma &
Ba, 2014) and Adagrad (Duchi et al., 2011) are widely used to train deep networks. As discussed,
CRAIG selects a subset that closely approximates the full gradient, and hence can be also applied for
speeding up training deep networks. The challenge here is that we cannot use inequality (9) to bound
the normed difference between gradients for all x ∈ X and find the subset as a preprocessing step.
However, it has been shown that for neural networks, the variation of the gradient norms is mostly
captured by the gradient of the loss w.r.t. the last layer (see Section 3.2 of (Katharopoulos & Fleuret,
2018)) that is often not expensive or only slightly more expensive than calculating the loss. In many
cases, where we have cross entropy loss with soft-max as the last layer, the gradient of the loss w.r.t.
the i-th input to the soft-max is simply pi - yi, where pi are logits (dimension p - 1 forp classes)
and y is the one-hot encoded label. In this case, CRAIG does not need any backward pass or extra
storage. That is, CRAIG can be applied at the beginning of every epoch to find a subset for that
epoch. Note that, although CRAIG needs an additional O(|V| ∙ |S|) complexity (or O(|V|) using
stochastic greedy) to find the subset S at the beginning of every epoch, this complexity does not
involve any (exact) gradient calculations and is negligible compared to the cost of backpropagations
performed during the epoch. Hence, as we show in the experiments CRAIG is practical and salable.
4	Convergence Rate Analysis of CRAIG
The idea of CRAIG is to selects a subset that closely approximates the full gradient, and hence can be
applied to speed up most IG variants as we show in our experiments. Here, we briefly introduce the
original IG method, and then prove the convergence rate of IG applied to subsets found by CRAIG.
4.1	Incremental Gradient Methods (IG)
Incremental gradient (IG) methods are core algorithms for solving Problem (1) and are widely used
and studied. IG aims at approximating the standard gradient method by sequentially stepping along
the gradient of the component functions fi in a cyclic order. Starting from an initial point x01 ∈ Rd, it
makes k passes over all the n components. At every epoch k ≥ 1, it iteratively updates xik-1 based
on the gradient of f for i = 1, ∙∙∙ ,n using stepsize ak > 0. Formally,
xk = Xk-I- Q∙kVfi(xk-i),	i = 1,2,…，n,	(14)
with the convention that x0k+1 = xkn . Note that for a closed and convex subset X of Rd, the results
can be projected onto X , and the update rule becomes
Xk = PX(Xk-I- akVfi(Xk-I)),	i = 1, 2,…，n,	(15)
6
Under review as a conference paper at ICLR 2020
where PX denotes projection on the set X ⊂ Rd .
IG with diminishing stepsizes converges at rate O(1∕√k) for strongly convex sum function (Nedic
& Bertsekas, 2001). If in addition to the strong convexity of the sum function, every component
function f is smooth, IG with diminishing stepsizes ɑk = Θ(1∕ks), S ∈ (0,1] converges at rate
O(1∕ks) (Gurbuzbalaban et al., 2015).
The convergence rate analysis of IG is valid regardless of order of processing the elements. However,
in practice, the convergence rate of IG is known to be quite sensitive to the order of processing the
functions (Bertsekas & Scientific, 2015; Gurbuzbalaban et al., 2017). If problem-specific knowledge
can be used to find a favorable order σ (defined as a permutation {σι, ∙∙∙ , σn} of {1,2,…,n}), IG
can be updated to process the functions according to this order, i.e.,
Xk = Xk-I- αkVfσa(Xk-1),	i = 1, 2,…，n.	(16)
In general a favorable order is not known in advance, and a common approach is sampling the
function indices with replacement from the set {1,2,…，n} and is called the Stochastic Gradient
Descent (SGD) method, a.k.a. the Robbins-Monro algorithm (Robbins & Monro, 1951) (also see
(Bottou, 1998; Bertsekas, 2015; Nemirovski et al., 2009; Shalev-Shwartz & Srebro, 2008)).
4.2	Convergence Rate of IG on Subsets Found by CRAIG
Next we analyze the convergence rate of IG applied to the weighted and ordered subset S found by
CRAIG. In particular, we show that (1) applying IG to S converges to a close neighborhood of the
optimal solution and that (2) this convergence happens at the same rate (same number of epochs) as
IG on the full data. Formally, every step of IG on the subset becomes
Xk = Xk-I- ak Ysσi VfSσi (Xk-1)，	i = 1, 2,…，r,	Si ∈ S, |S| = r. (17)
Here, σ is a permutation of {1,2,…,r}, and the per-element stepsize Ysi for every function 于$匕 is
the weight of the element Si ∈ S and is fixed for all epochs.
4.3	Convergence Rate for Strongly Convex Functions
We first provide the convergence analysis for the case where the sum function in Problem (1) is
strongly convex, i.e. ∀x, y ∈ Rd we have that f (y) ≥ f (x) + Ef (x), y - Xi + 2∣∣x - y∣∣2.
Theorem 1. Assume that i∈V fi(X) is strongly convex, and S is a weighted subset of size r such
that L(S) = i∈V minj∈S dij ≤ . Then for the iterates {Xk =X0k} generated by applying IG to S
with per-ePoch stepsize ak = α∕kτ with a > 0 and T ∈ [0,1], we have
(i)	if τ = 1, then IlXk — x*∣∣2 ≤ 2eR∕μ + r2γ21aχC2∕kμ,
(ii)	if 0 < τ < 1, then IlXk — x*∣∣2 ≤ 2eR∕μ,	for k → ∞
(iii)	if τ = 0, then IlXk — x*∣∣2 ≤ (1 — αμ)k+1∣∣Xo — x*∣∣2 + 2eR∕μ + αr2YmaXC2∕μ,
where C is an upper-bound on the norm of the component function gradients, i.e.
maxi∈v supχ∈χ ∣Nfi(X)Il ≤ C, YmaX = maxj∈s Yj is the largest per-element step size, and
R = min{do, (rγmaχC + e)∕μ}, where do = ∣∣xo — x*∣∣ is the initial distance to the optimum x*.
All the proofs can be found in the Appendix. The above theorem shows that IG on S converges at
the same rate O(1∕√k) of IG on the entire data set V. However, compared to IG on V, the |V∣∕∣S∣
speedup of IG on S comes at the price of getting an extra error term, 2eR∕μ.
4.4	Convergence Rate for Smooth and Strongly Convex Functions
If in addition to strong convexity of the expected risk, each component function has a Lipschitz
gradient, i.e. ∀x ∈ X, i ∈ [n] we have ∣∣Vfi(x) — Vfi(y)∣ ≤β∕∣x — y∣, then we get the following
results about the iterates generated by applying IG to the weighted subset S returned by CRAIG.
7
Under review as a conference paper at ICLR 2020
①一e」」0」」①一SB-L
Figure 1: Loss residual and error rate of IG, SVRG, SAGA for Logistic Regression on Covtype data
set with 581,012 data points. We compare performance of CRAIG (10% selected subset) vs. entire
data set. We achieve the average speedup of 7x for achieving similar loss residual and error rate
across the three optimization methods.
O 50 IOO 150 200 250
Time (sec)
0	50	100 150 200 250
Time (sec)
0	50	100 150 200 250
Time (sec)
Theorem 2. Assume that E,∈v fi(x) is StrongIy convex and let fi(x), i = 1, 2,…，n be convex
and twice continuously differentiable component functions with Lipschitz gradients on X. Given a
subset S such that L(S) = i∈V minj∈S dij ≤ . Then for the iterates {xk = x0k} generated by
applying IG to S with per-epoch stepsize ak = α∕kτ with α > 0 and T ∈ [0,1], we have
(i)	ifτ = 1, then ∣∣Xk - x* ∣∣ ≤ 2e∕μ + βCrγ2aχ∕kμ,
(ii)	if 0 < τ < 1, then ∣∣Xk — x*∣ ≤ 2e∕μ,	for k → ∞
(iii)	if τ = 0, then ∣∣Xk - x*∣ ≤ (1 — αμ)k ∣∣xo — x*∣ + 2e∕μ + αβCrγ^21aχ∕μ,
where β = in=1 βi is the sum of gradient Lipschitz constants of the component functions.
The above theorem shows that for τ > 0, IG applied to S converges to a 2e∕μ neighborhood of the
optimal solution, with a rate of O(1∕kτ) which is the same convergence rate for IG on the entire data
set V . As shown in our experiments, in real data sets small weighted subsets constructed by CRAIG
provide a close approximation to the full gradient. Hence, applying IG to the weighted subsets
returned by CRAIG provides a solution of the same or higher quality compared to the solution
obtained by applying IG to the whole data set, in a considerably shorter amount of time.
5	Experiments
In our experimental evaluation we wish to address the following questions: (1) How do loss and
accuracy of IG applied to the subsets returned by CRAIG compare to loss and accuracy of IG applied
to the entire data; (2) How small is the size of the subsets that we can select with CRAIG and still
get a comparable performance to IG applied to the entire data; (3) how does the ordering affect
the performance of IG on the subset; and (4) how well does CRAIG scale to large data sets, and
extend to non-convex problems. To this end, we apply CRAIG to several convex and non-convex
problems. In our experiments, we report the run-time as the wall-clock time for subset selection with
CRAIG, plus minimizing the loss using IG or other optimizers with the specified learning rates. For
the classification problems, we separately select subsets from each class while maintaining the class
ratios in the whole data, and apply IG to the union of the subsets.
5.1	Convex Experiments
In our convex experiments, we apply CRAIG to IG, as well as variance reduction methods SVRG
(Johnson & Zhang, 2013), and SAGA (Defazio et al., 2014), that try to reduce the variance of SGD
either based on computations of full gradients at pivot points, or on keeping per data point corrections
8
Under review as a conference paper at ICLR 2020
0.75 -
0.70 -
EnEAdo Oj ①uuelsQ
Figure 2: (top) Speedup of CRAIG applied to to get similar loss residual as IG after 50 epoch, and
(bottom) distance to the optimal solution vs various subset sizes for (left) Covtype, (middle) SensIT,
and (right) Ijcnn1. Smaller subsets provides larger speedups, but may converge farther away from the
optimal solution.
EnE⅛o，SSO- 6uc-s,l.L
Figure 3: Loss residual of CRAIG for Logistic Regression on (left) Covtype, (middle) SensIT, and
(right) Ijcnn, where we process the points of the subset according to the ordering provided by CRAIG
vs three random permutations of the same subset. Notice convergence is significantly faster when we
process the points in CRAIG order.
in memory. We apply L2-regularized logistic regression: fi(x) = ln(1 +exp(-xTaiyi)) + 0.5λxT x
to classify the following three datasets from LIBSVM: (1) covtype.binary including 581,012 data
points of54 dimensions, (2) SensIT including 78,823 training and 19,705 test data points of dimension
100, and (3) Ijcnn1 including 49,990 training and 91,701 test data points of 22 dimensions. As
covtype do not come with labeled test data, we randomly split the training data into halves to make
the training/test split (training and set sets are consistent for different methods). For the convex
experiments, λ is set to 10-5.
Figure 1 compares training loss residual and test error rate of IG, SVRG, and SAGA on the subsets
of size 10% of covtype selected by CRAIG (with corresponding per-element stepsizes) to that of IG,
SVRG, and SAGA on the entire data set. We used a constant learning rate of α for SAGA and SVRG,
and α/√k for IG, where α = 10-3 for Covtype and Ijcnn, and 3 X 10-5 for SensIT. It can be seen
that subsets obtained by CRAIG achieve a similar loss and error rate as the entire data sets, but much
faster. In particular, we obtained a speedup of 8.3x, 8.3x, 4.5x from applying IG, SVRG and SAGA
on the subsets of size 10% obtained by CRAIG.
Figure 2 top row, compares the speedup achieved by CRAIG to reach a similar loss residual as that
ofIG after 50 epochs for subsets of size 5% to 25%. The bottom row compares the L2-norm of the
distance to optimal solution (estimated by running IG for a long time) for IG applied to the subsets of
various size obtained by CRAIG. We observe that while smaller subsets provide a larger speedup, IG
on smaller subsets may converge to a point farther away from the optimal solution.
Finally, Figure 3 shows the loss residual vs time for IG when it processes the elements of the subsets
according to the ordering obtained by CRAIG compared to random permutations of the same subsets.
We observe that the greedy ordering significantly improves the rate of convergence of IG.
9
Under review as a conference paper at ICLR 2020
O IOOO 2000 o 2000 4000 6000 O 2000 4000 6000 O 2000 4000 6000 O 2000 4000 6000
Time (sec)	Time (SeC)	Time (sec)	Time (sec)	Time(sec)
(a) MNIST	(b)CIFAR10
Figure 4: Training loss and test accuracy of CRAIG applied to (a) SGD on MNIST with a 1-layer
neural network, and (b) SGD, Adam, Adagrad, NAG, on CIFAR-10 with ResNet-56. CRAIG
provides 2x to 3x speedup and a better generalization performance.
5.2	Non-convex Experiments
Our non-convex experiments involves applying CRAIG to train the following two neural networks:
(1) Our smaller network is a fully-connected hidden layer of 100 nodes and ten softmax output nodes;
sigmoid activation and L2 regularization with λ = 0.0001 and mini-batches of size 10 on MNIST
dataset of handwritten digits containing 60,000 training and 10,000 test images. (2) Our large neural
network is ResNet-56 for CIFAR10 with convolution, average pooling and dense layers with softmax
outputs and L2 regularization with λ = 2 × 10-4 CIFAR 10 includes 50,000 training and 10,000 test
images from 10 classes, and we used mini-batches of size 128. Both MNIST and CIFAR10 data sets
are normalized into [0, 1] by division with 255.
We apply CRAIG to several popular methods for training neural networks, including SGD, Nesterov
Accelerated Gradient (NAG) method (Nesterov, 1983), Adagrad (Duchi et al., 2011), and Adaptive
Moment Estimation (Adam) (Kingma & Ba, 2014). Momentum is a method that helps accelerate SGD
in the relevant direction and dampens oscillations. NAG adapts momentum updates to the slope of
our error function and speed up SGD. Adagrad adapts the learning rate to the parameters, performing
smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features,
and larger updates (i.e. high learning rates) for parameters associated with infrequent features. Adam
computes individual adaptive learning rates for different parameters from estimates of first and second
moments of the gradients. Fig. 4a shows loss, and error rate for training a 1-layer neural net on
MNIST. Fig. 4b show similar quantities for training ResNet-56 on CIFAR10. For both problems, we
used a constant learning rate of 10-2. Here, we apply CRAIG to select a subset of 30%-40% of the
data at the beginning of every epoch and train only on the selected subset with the corresponding
per-element stepsizes. Interestingly, in addition to achieving a speedup of 2x to 3x for training neural
networks, the subsets selected by CRAIG provide a better generalization performance compared to
models trained on the entire data set.
6	Conclusion
We developed a method, CRAIG, for selecting an ordered subset (coreset) of data points with their
corresponding per-element stepsizes to speed up iterative gradient (IG) methods. In particular, we
showed that weighted subsets that minimize the upper-bound on the estimation error of the full
gradient, maximize a submodular facility location function. Hence, we can obtain an ordered subset
of data points with their corresponding learning rates using a greedy algorithm. We showed that
IG on subsets S returned by CRAIG converges at the same rate as IG on the entire data set V ,
while providing a |V |/|S | speedup. In our set of experiments, we showed that various IG methods,
including SAGA, SVRG, NAG, Adagrad and Adam runs up to 10x faster on convex and up to 3x on
non-convex problems on subsets found by CRAIG while achieving practically the same trining loss
and test error. Finally, we empirically demonstrated the effect of the ordering found by the greedy
algorithm on the convergence rate of IG methods.
10
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. The Journal of
Machine Learning Research, 18(1):8194-8244, 2017.
Zeyuan Allen-Zhu, Yang Yuan, and Karthik Sridharan. Exploiting the structure: Stochastic gradient methods
using raw clusters. In Advances in Neural Information Processing Systems, pp. 1642-1650, 2016.
Hilal Asi and John C Duchi. The importance of better models in stochastic optimization. arXiv preprint
arXiv:1903.08619, 2019.
Dimitri P Bertsekas. Incremental least squares methods and the extended kalman filter. SIAM Journal on
Optimization, 6(3):807-822, 1996.
Dimitri P Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimization: A
survey. arXiv preprint arXiv:1507.01030, 2015.
Dimitri P Bertsekas and Athena Scientific. Convex optimization algorithms. Athena Scientific Belmont, 2015.
LCon Bottou. Online algorithms and stochastic approximations. In David Saad (ed.), Online Learning and
Neural Networks. Cambridge University Press, Cambridge, UK, 1998. URL http://leon.bottou.
org/papers/bottou-98x. revised, oct 2012.
Kai Lai Chung et al. On a stochastic approximation method. The Annals of Mathematical Statistics, 25(3):
463-483, 1954.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support
for non-strongly convex composite objectives. In Advances in neural information processing systems, pp.
1646-1654, 2014.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal point
and faster stochastic algorithms for empirical risk minimization. In International Conference on Machine
Learning, pp. 2540-2548, 2015.
Mert Gurbuzbalaban, ASU Ozdaglar, and Pablo Parrilo. Why random reshuffling beats stochastic gradient descent.
arXiv preprint arXiv:1510.08560, 2015.
Mert Gurbuzbalaban, Asuman Ozdaglar, and Pablo A Parrilo. On the convergence rate of incremental aggregated
gradient algorithms. SIAM Journal on Optimization, 27(2):1035-1048, 2017.
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance reduced stochastic
gradient descent with neighbors. In Advances in Neural Information Processing Systems, pp. 2305-2313,
2015.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In
Advances in neural information processing systems, pp. 315-323, 2013.
Angelos Katharopoulos and FrangOiS Fleuret. Not all samples are created equal: Deep learning with importance
sampling. arXiv preprint arXiv:1803.00942, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization. In Advances
in Neural Information Processing Systems, pp. 3384-3392, 2015.
Hui Lin and Jeff A Bilmes. Learning mixtures of submodular shells with application to document summarization.
arXiv preprint arXiv:1210.4871, 2012.
OL Mangasariany and MV Solodovy. Serial and parallel backpropagation convergence via nonmonotone
perturbed minimization. 1994.
Michel Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization
techniques, pp. 234-243. Springer, 1978.
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrdk, and Andreas Krause. Lazier
than lazy greedy. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015a.
11
Under review as a conference paper at ICLR 2020
Baharan Mirzasoleiman, Amin Karbasi, Ashwinkumar Badanidiyuru, and Andreas Krause. Distributed submod-
ular cover: Succinctly summarizing massive data. In Advances in Neural Information Processing Systems, pp.
2881-2889, 2015b.
Baharan Mirzasoleiman, Morteza Zadimoghaddam, and Amin Karbasi. Fast distributed submodular cover:
Public-private data summarization. In Advances in Neural Information Processing Systems, pp. 3594-3602,
2016.
Angelia Nedic and Dimitri Bertsekas. Convergence rate of incremental subgradient algorithms. In Stochastic
optimization: algorithms and applications, pp. 223-264. Springer, 2001.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. An analysis of approximations for maximizing submodular set
functions—i. Mathematical Programming, 14(1):265-294, 1978.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-1609, 2009.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o (1∕k^
2). In Doklady AN USSR, volume 269, pp. 543-547, 1983.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):145-151,
1999.
Benjamin Recht and Christopher Re. Beneath the valley of the noncommutative arithmetic-geometric mean
inequality: conjectures, case-studies. Technical report, and consequences. Technical report, University of
Wisconsin-Madison, 2012.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics,
pp. 400-407, 1951.
Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential
convergence _rate for finite training sets. In Advances in neural information processing systems, pp. 2663-
2671, 2012.
Shai Shalev-Shwartz and Nathan Srebro. Svm optimization: inverse dependence on training set size. In
Proceedings of the 25th international conference on Machine learning, pp. 928-935. ACM, 2008.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-
tion. Journal of Machine Learning Research, 14(Feb):567-599, 2013.
Ian Simon, Noah Snavely, and Steven M Seitz. Scene summarization for online image collections. In 2007 IEEE
11th International Conference on Computer Vision, pp. 1-8. IEEE, 2007.
Mikhail V Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Computational
Optimization and Applications, 11(1):23-35, 1998.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in
nlp. arXiv preprint arXiv:1906.02243, 2019.
Paul Tseng. An incremental gradient (-projection) method with momentum term and adaptive stepsize rule.
SIAM Journal on Optimization, 8(2):506-531, 1998.
Laurence A Wolsey. An analysis of the greedy algorithm for the submodular set covering problem. Combinator-
ica, 2(4):385-393, 1982.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM
Journal on Optimization, 24(4):2057-2075, 2014.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
Luo Zhi-Quan and Tseng Paul. Analysis of an approximate gradient projection method with applications to the
backpropagation algorithm. Optimization Methods and Software, 4(2):85-101, 1994.
12
Under review as a conference paper at ICLR 2020
A Convergence Rate Analysis
We firs proof the following Lemma which is an extension of the [Chung et al. (1954), Lemma 4].
Lemma 3. Let Uk ≥ 0 be a SeqUenCe ofreal numbers. Assume there exist k0 such that
C、 e d
Uk+1 ≤ (1 - k)Uk + k + ⅛p+T， Vk ≥ ko，
where e > 0,d > 0,c > 0 are given real numbers. Then
Uk
Uk
Uk
≤ (dk-1 + e)(c - p + 1) ―1 k—p+1 + o(k-p+1)
=O(k-c log k)
=O(k-c)
for c > p
for c = P
for c < p
1,P
1,p
1,p
(18)
(19)
(20)
(21)
—
—
—
〉
>
>
1
1
1
Proof. Let c > p - 1 and vk = kp—1Uk -
(1 + k)p = (1 + P) + o(1) we can write
k(c—p+1)	c—p+1.
Then, using Taylor approximation
d
—
e
d
e
vk + 1 = (k + 1)p 1 uk + 1 -
(k + 1)(c -p + 1)
≤ kp-1(1 + I)PT((I- C)Uk + keep +
c-p+ 1
dd
(22)
kp+1	(k + 1)(c - p + 1) c - p + 1
(23)
—
—
—
e
p 1	c-p+ 1	1 e p-1	1
k Uk (1	k — + o( k)) + k (1 + 丁 + o( k))
(24)
d p-1	1
+ 后(1 + ~JΓ + o( k))
d
vk + k(c — p + 1)
(k + 1)(c - p + 1)
+ e	)(1 - C-P + 1
c-p+ 1	k
c-p+1
+o(小)
(25)
(26)
—
d
e
—
+e (1 + ―+o(1)) + E (1 + ― + o(1))
k	k k k2 k k
de
—
—
(27)
(k + 1)(c — p + 1) C — p + 1
(28)
vk ( 1 —
d/(c - P + 1)	e(p - 1)	d(p - 1)
k(k + 1)	+ —万一+
k2
k3
+
(29)
c — P + 1
k
+ o( f) +
Note that for vk, we have
—
c-P+ 1 , / 1八
~ΠΓ~ + o( k))= ∞
and
p+1)
+1)
Therefore, limk→∞
e(p - 1) + d(p - 1)
k2
k3
+ o
c — p + 1
vk ≤ 0, and we get Eq. 18. For P
into the region U ≤ C, with ratio 1 - C.
Moreover, for P - 1 ≥ C we have
k
1, we have Uk
1	—1
+ o( k)) T 0∙
≤ C. Hence, Uk converges
vk+1 = Uk+1 (k + 1)c ≤ [(I - k)Uk
(	c2
e d 17 c 乙 C
+ k + k+∖k (1 + k+
=I1 -页 + o
e0
≤ vk + k—
vk + kp—c+1
1 + O( %)+ k—
c2 ,
页+ o
1 + c + O
k
(30)
(31)
(32)
+
d
for sufficiently large k. Summing over k, we obtain that vk is bounded for P — 1 > c (since the
series P∞1(1∕kα) converges for α > 1) and vk = O(log k) for P = C + 1 (since Pk=1(1∕i)=
O(log k)).^	一 □
13
Under review as a conference paper at ICLR 2020
In addition, based on [Chung et al. (1954), Lemma 5] for uk ≥ 0, we can write
uk+1 ≤ (I - T7)uk + y∑ + yr,	0 <s< 1, s ≤ p <t.	(33)
ks	kp	kt
Then, we have
e1	1
Uk ≤ - J + o( h	).	(34)
c kp-s	kp-s
A. 1 Convergence Rate for S trongly Convex Functions
Proof of Theorem 1
We now provide the convergence rate for strongly convex functions building on the analysis of Nedic
& Bertsekas (2001). For non-smooth functions, gradients can be replaced by sub-gradients.
Let xk = x0k. For every IG update on subset S we have
kxk -x*k2 = I∣xk-1 - akYjVfj(Xk-I)-x*k2	(35)
=kxk-ι - x*k2 - 2αkYjVfj(Xk-I)(Xk-I -x*) + αkkYj-Vfj(Xk-ι)k2	(36)
≤ kxk-1 - x*k2 - 2αk(fj(Xk-I)- fi(x*)) + αk∣γiVfj(Xk-I)∣2.	(37)
Adding the above inequalities over elements of S we get
l∣Xk+ι - X*k2 ≤ l∣Xk - X*k2 - 2αk X(fi(Xk-i) - fj(x*))+ 睚 X l∣YjVfj(Xk-I)∣∣2	(38)
j∈S	j∈S
=IlXk - x*∣∣2 - 2αk〉：(fj (Xk) - fi (XG)
j∈S
+2αkX(fj(Xjk-1)-fj(Xk))+α2kXlYjVfj(Xjk-1)l2	(39)
j∈S	j∈S
Using strong convexity we can write
∣∣Xk+ι - X*k2 ≤ IlXk - X*k2 - 2αk (E Yj Vfj (x*) ∙ (Xk 一 x*) + 2 ∣∣Xk 一 x*∣∣2)
j∈S
+2αkX(fj(Xjk-1)-fj(Xk))+α2kXlYjVfj(Xjk-1)l2	(40)
j∈S	j∈S
Using Cauchy-Schwarz inequality, we know
| XYjVfj(X*) ∙ (Xk- X*)| ≤ Il XYjVfj(X*)l∣∙ llXk - x*II∙	(41)
j∈S	j∈S
Hence,
-EYjVfj(x*) ∙ (Xk - X*) ≤ k EYjVfj(x*)k∙ IlXk - X*k∙	(42)
j∈S	j∈S
From reverse triangle inequality, and the facts that S is chosen in a way that ∣∣ Pi∈v Vfi(X*)-
Pj∈s YjVfj(X*)∣∣ ≤ a and that P» Vfig = 0 we have ∣∣ j YjVfj(χ*)∣∣ ≤
Il pi∈v Vfi(X*)∣∣ + E = e. Therefore
Il X YjVfj(x*)∣∣∙ ∣∣Xk - x*∣∣ ≤ 〜l∣Xk - χ*ll	(43)
j∈S
For a continuously differentiable function, the following condition is implied by strong convexity
condition
lxk -χ*ll ≤ 1 Il XYjVfj(χk)l∣.
(44)
14
Under review as a conference paper at ICLR 2020
Assuming gradients have a bounded norm maxχ∈χ, kVfj(x)k ≤ C, and the fact that ∑j∈s Yj = n
we can write
k EYjVfj(Xk)k≤ n ∙ C.	(45)
j∈S
Thus for initial distance ∣∣xo 一 x* k = do, We have
Ilxk 一 x*k ≤ min(n ∙ C, do) = R	(46)
Putting Eq. 42 to Eq. 46 together We get
kxk+1 一 χ*k2 ≤ (I 一 αkμ) kxk 一 χ*k2 + 2αk ER∕μ
+2αkX(fj(xj-1,k) 一 fj(xk)) + αk2rYm2 axC2.	(47)
j∈S
NoW, from convexity of every fj for j ∈ S We have that
fj(Xk) - fj(Xj-I) ≤ ∣Yjvfj(Xk)k ∙ kxk-ι —xkk.	(48)
In addition, incremental updates gives us
j-1
kXjk-1 一 Xkk ≤ αk	kYiVfi(Xik-1)k ≤ αk(j 一 1)YmaxC.	(49)
i=1
Therefore, We get
2αkX(fj(Xk) 一 fj(Xjk-1)) + α2krYm2axC2
j∈S
r
≤ 2αk X : YmaXC ∙ αk(j 一 I)YmaXC + αkrγmaχC^	(50)
i=1
= α2kr2Ym2 aXC2	(51)
Hence,
∣∣Xk+l - x*k2 ≤ (1 — αkμ)∣∣Xk — x*k2 + 2αk∈R∕μ + (Okr2YmyaχC2.	(52)
Where Ymax is the size of the largest cluster, and C is the upperbound on the gradients.
For 0 < s ≤ 1, the theorem follows by applying Lemma 3 to Eq. 52, with C = μ, e = 2eR∕μ, and
d = r2Y2 C2.
maX
For S = 0, where we have a constant step size (k = ( ≤ ɪ, we get
kxk + 1 - x* k2 ≤ (I-aM)k+1 kxo — χ* k2
kk
+ 2eaeR £(1 一 (μ)j∕μ + α2r2γmγaχC2 £(1 一 (μ)j	(53)
j=o	j=o
SinCe Pk=O(I - (μ)j ≤ αμ ,we get
∣Xk+l — X*k2 ≤ (1 — (μ)k+1kxo — x*∣2 + 2eR∕μ + αr2γmnaχC2∕μ,	(54)
and therefore,
∣∣Xk+ι — x*∣∣2 ≤ (1 — (μ)k+1kxk — x*∣2 + 2eR∕μ + αr2YmmaxC2∕μ.	(55)
A.2 Convergence Strongly Convex and Smooth Component Functions
Proof of Theorem 2
IG updates for cycle k on subset S can be written as
Xk+1 = Xk — (k(	Yj Vfj (Xk) — ek)	(56)
j∈S
ek = X Yi(Vfj (Xk) — Vfj (Xjk-1))	(57)
j∈S
15
Under review as a conference paper at ICLR 2020
Building on the analysis of Gurbuzbalaban et al. (2015), for convex and twice continuously differen-
tiable function, we can write
X YjVfj(Xk) - X YjVfj(XJ= Ak(Xk- XJ	(58)
j∈S	j∈S
where Ak = R1 V2 f (x* + T(Xk 一 χ*))dτ is average of the Hessian matrices corresponding to the r
(weighted) elements of S on the interval [Xk ,x*].
From Eq. 58 we have
X(Vfi(Xk) - Vfi(X*)) - XYj(Vfj(Xk) - Vfj(X*)) = Ak(Xk - X*) - Ark(Xk - X*), (59)
i∈V	j∈S
where Ak is average of the Hessian matrices corresponding to all the n component functions on
the interval [Xk, X*]. Taking norm of both sides and noting that Pi∈V fi(X*) = 0 and hence
k Pj∈SYjfj(X*)k ≤ , we get
k(Ak - Ark)(Xk - X*)k = kXVfi(Xk)-XYjVfj(Xk) +XYjfj(X*)k ≤2,	(60)
where is the estimation error of the full gradient by the weighted gradients of the elements of the
subset S, and we used k	i∈V Vfi(Xk) - j∈SYjVfj(Xk)k ≤ .
Substituting Eq. 58 into Eq. 56 we obtain
Xk+1 - X* = (I - αkArk)(Xk - X*) + αkek	(61)
Taking norms of both sides, we get
kXk+1 - X*k ≤ k(I - αkArk)(Xk - X*k) + αkkekk	(62)
Now, we have
k(I - αk Ark )(Xk - X* )k =	kI (Xk - X* ) - αk Ark (Xk - X* )k	(63)
=	kI (Xk - X* ) - αk (Ark - Ak )(Xk -	X* )	- αk Ak (Xk	- X* )k	(64)
≤	k(I - αk Ak)(Xk - X*)k + αk k(Ak	- Ark)(Xk	- X*)k	(65)
≤	k(I - αkAk)(Xk - X*)k +2αk	(66)
Substituting into Eq. 62, we obtain
kXk + 1 一 X*k ≤ kI 一 αkAk k ∙ IlXk - X*k + 2αke + αk IlekIl	(67)
From strong convexity of Pi∈V fi(X), and gradient smoothness of each component fi(X) we have
μIn W X V2fi(X), Ak W βIn,	X ∈ X,	(68)
i∈V
where β = Pi∈V βi In addition, from the gradient smoothness of the components we can write
IekI ≤ XYjβjIXk 一 XjkI	(69)
j∈S
j-1
≤ X Yjej X kXk-1-Xk k	(70)
j∈S i=1
j-1
≤	Yjβjαk	IYiVfi(Xik)I	(71)
j∈S	i=1
≤ αkβCrYm2 ax,	(72)
16
Under review as a conference paper at ICLR 2020
where in the last line we used |S | = r. Therefore,
I∣xk+1	- x*k	≤ max(∣∣1 - αkμ∣∣, ||1 - αkβ∣∣)∣∣xk - x*∣∣ + 2α*E + (OkβCrγ2aχ	(73)
≤ (I - αkμ)kxk - x* k +2(kC + αkβCrYmaχ	if αkβ ≤ 1.	(74)
For	0	< s ≤	1, the	theorem follows by applying Lemma 3 to Eq.	73 with C =	μ, e = 2c,
d =	βCrγ21aχ.	For S = 0, where We have a constant step size (k = ( ≤	β, we get
kk
kxk+1 - x* k ≤ (I - (μ)k + 1 kxk - χ* k + 2e ^X(I - αμ)i + α2 ^X(I - αμ^βCrYmaX	(75)
i=0	i=0
≤ (1 - αμ)k+1∣Xk - x*k +2c∕μ + αβCrγ2maχ∕μ,	(76)
≤ (1 - αμ)k + 1kxk - x*k + 2"μ + CrYmaχ∕μ,	(77)
where the inequality in Eq. 76 follows since Pk=0(1 - (μ)i ≤ Oμμ.
B Norm of the Difference B etween Gradients
For ridge regression fi(x) = ɪ(ai, X - yi)2 + 2 ∣∣x∣∣2, we have Nfi(Xl = ai(hai,X — yi) + λx.
Therefore,
INfi(x) - Nfj(x)I	= (Iai	- ajI.IxI	+ Iyi	-yjI)IajI	(78)
For IaiI ≤ 1, and yi = yj we get
INfi(x)-Nfj(x)I ≤ Iai-ajIO(IxI)	(79)
For reguralized logistic regression with y ∈ {-1, 1}, we have Nfi(x) = yi∕(1 + eyihai,xi ). For
yi = yj we get
ekai -aj k.kxk	1
INfi(X) - ▽力(X)Il = ι + e-haiχ Ilajk.	(80)
For kaik ≤ 1, using Taylor approximation ex ≤ 1 + x, and noting that +，…)≤ 1 we get
kVfi(X)- Vfj(x)k ≤ kai二⅞k kajk ≤ kai - ajkO(kxk).	(81)
For classification, we require yi = yj , hence we can select subsets from each class and then merge
the results. On the other hand, in ridge regression we also need |yi - yj | to be small. Similar results
can be deduced for other loss functions including square loss, smoothed hinge loss, etc.
Assuming IXI is bounded for all X ∈ X, upper-bounds on the euclidean distances between the
gradients can be pre-computed.
C Additional Experiments
17
Under review as a conference paper at ICLR 2020
EnE⅛0 - SSO
Figure 5: Loss residual vs. time for IG on 10 subsets of size 10%, 20%, 30%, ..., 100% selected by
CRAIG, random subsets, and random subsets weighted by |V |/|S |. Stepsizes are tuned for every
subset separately, by preferring smaller training loss from a large number of parameter combinations
for two types of learning scheduling: exponential decay η(t) = noabt/nc with parameters no and a to
adjust and t-inverse η(t) = η0(1 + bbt/nc)-1 with η0 and b to adjust.
Figure 6: Training loss and test accuracy for SGD applied to full MNIST vs. subsets of size 60%
selected by CRAIG and random subsets of size 60%. Both the random subsets and the subsets found
by CRAIG change at the beginning of every epoch.
18