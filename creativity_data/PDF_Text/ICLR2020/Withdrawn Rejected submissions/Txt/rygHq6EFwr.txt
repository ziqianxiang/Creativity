Under review as a conference paper at ICLR 2020
GResNet: Graph Residual Network for Reviv-
ing Deep GNNs from Suspended Animation
Anonymous authors
Paper under double-blind review
Ab stract
The existing graph neural networks (GNNs) based on the spectral graph convolu-
tional operator have been criticized for its performance degradation, which is es-
pecially common for the models with deep architectures. In this paper, we further
identify the suspended animation problem with the existing GNNs. Such a prob-
lem happens when the model depth reaches the suspended animation limit, and
the model will not respond to the training data any more and become not learn-
able. Analysis about the causes of the suspended animation problem with existing
GNNs will be provided in this paper, whereas several other peripheral factors that
will impact the problem will be reported as well. To resolve the problem, we in-
troduce the GResNet (Graph Residual Network) framework in this paper, which
creates extensively connected highways to involve nodes’ raw features or interme-
diate representations throughout the graph for all the model layers. Different from
the other learning settings, the extensive connections in the graph data will render
the existing simple residual learning methods fail to work. We prove the effec-
tiveness of the introduced new graph residual terms from the norm preservation
perspective, which will help avoid dramatic changes to the node’s representations
between sequential layers. Detailed studies about the GResNet framework for
many existing GNNs, including GCN, GAT and LoopyNet, will be reported in
the paper with extensive empirical experiments on real-world benchmark datasets.
1 Introduction
Graph neural networks (GNN), e.g., graph convolutional network (GCN) Kipf & Welling (2016)
and graph attention network (GAT) VeliCkovic et al. (2018), based on the approximated spectral
graph convolutional operator Hammond et al. (2011), can learn the representations of the graph data
effectively. Meanwhile, such GNNs have also received lots of criticism, since as these GNNs’ ar-
chitectures go deep, the models’ performance will get degraded, which is similar to observations on
other deep models (e.g., convolutional neural network) as reported in He et al. (2015). Meanwhile,
different from the existing deep models, when the GNN model depth reaches a certain limit (e.g.,
depth ≥ 5 for GCN with the bias term disabled or depth ≥ 8 for GCN with the bias term enabled
on the Cora dataset), the model will not respond to the training data any more and become not learn-
able. Formally, we name such an observation as the GNNs’ suspended animation problem, whereas
the corresponding model depth is named as the suspended animation limit of GNNs. Here, we need
to add a remark: to simplify the presentations in this paper, we will first take vanilla GCN as the
base model example to illustrate our discoveries and proposed solutions in the method sections.
Meanwhile, empirical tests on several other existing GNNs, e.g., GAT VeliCkoviC et al. (2018) and
LoopyNet Zhang et al. (2018), will also be studied in the experiment section of this paper.
As illustrated in Figure 1, we provide the learning performance of the GCN model on the Cora
dataset, where the learning settings (including train/validation/test sets partition, algorithm imple-
mentation and fine-tuned hyper-parameters) are identical to those introduced in Kipf & Welling
(2016). The GCN model with the bias term disable of seven different depths, i.e., GCN(1-layer)-
GCN(7-layer), are compared. Here, the layer number denotes the sum of hidden and output layers,
which is also equal to the number of spectral graph convolutional layers involved in the model.
For instance, besides the input layer, GCN(7-layer) has 6 hidden layer and 1 output layer, both of
which involve the spectral graph convolutional operations. According to the plots, GCN(2-layer)
and GCN(3-layer) have comparable performance, which both outperform GCN(1-layer). Mean-
while, as the model depth increases from 3 to 7, its learning performance on both the training set
1
Under review as a conference paper at ICLR 2020
O 200	400	600	800 IOOO
epoch (iter, over training set)
GCN(I-Iayer)
—GCN(2-layer)
----GCN(3-layer)
----GCN(4-layer)
----GCN(5-layer)
----GCN(6-layer)
一 GCN(7-layer)
0	200	400	600	800	1000
epoch (iter, over training set)
----GCN(I-Iayer)
—GCN(2-layer)
----GCN(3-layer)
——GCN(4-layer)
----GCN(5-layer)
——GCN(6-layer)
-GCN(7-layer)
(a) Training Accuracy	(b) Testing Accuracy
Figure 1: The learning performance of GCN (bias disabled) with 1-layer, 2-layer, . . . , 7-layer on
the Cora dataset. The x axis denotes the iterations over the whole training set. The y axis of the left
plot denotes the training accuracy, and that of the right plot denotes the testing accuracy.
and the testing set degrades greatly. It is easy to identify that such degradation is not caused by over-
fitting the training data. What’s more, much more surprisingly, as the model depth goes deeper to 5
or more, it will suffer from the suspended animation problem and does not respond to the training
data anymore. (Similar phenomena can be observed for GCN (bias enabled) and GAT as illustrated
by Figures 9 and 10 in the appendix of this paper, whose suspended animation limits are 8 and 5,
respectively. Meanwhile, on LoopyNet, we didn’t observe such a problem as shown in Figure 11
in the appendix, and we will state the reasons in Section 6 in detail.)
In this paper, we will investigate the causes of the GNNs’ suspended animation problem, and analyze
if such a problem also exists in all other GNN models or not. GNNs are very different from the tradi-
tional deep learning models, since the extensive connections among the nodes render their learning
process no longer independent but strongly correlated. Therefore, the existing solutions proposed
to resolve such problems, e.g., residual learning methods used in ResNet for CNN He et al. (2015),
cannot work well for GNNs actually. In this paper, several different novel graph residual terms will
be studied for GNNs specially. Equipped with the new graph residual terms, we will further intro-
duce a new graph neural network architecture, namely graph residual neural network (GRESNET),
to resolve the observed problem. Instead of merely stacking the spectral graph convolution layers
on each other, the extensively connected high-ways created in GResNet allow the raw features or
intermediate representations of the nodes to be fed into each layer of the model. We will study the
effectiveness of the GRESNET architecture and those different graph residuals for several existing
vanilla GNNs. In addition, theoretic analyses on GResNet will be provided in this paper as well to
demonstrate its effectiveness from the norm-preservation perspective.
The remaining parts of this paper are organized as follows. In Section 2, we will introduce the
related work of this paper. The suspended animation problem with the spectral graph convolutional
operator will be discussed in Section 3, and the suspended animation limit will be analyzed in
Section 4. Graph residual learning will be introduced in Section 5, whose effectiveness will be
tested in Section 6. Finally, we will conclude this paper in Section 7.
2	Related Work
Graph Neural Network: Graph neural networks Monti et al. (2017); Atwood & Towsley (2016);
Masci et al. (2015); Kipf & Welling (2016); Battaglia et al. (2018); Bai et al. (2018); Scarselli et al.
(2009); Zhou et al. (2018); Niepert et al. (2016) have become a popular research topic in recent
years. Traditional deep models cannot be directly applied to graph data due to the graph inter-
connected structures. Many efforts have been devoted to extend deep neural networks on graphs
for representation learning. GCN proposed in Kipf & Welling (2016) feeds the generalized spectral
features into the convolutional layer for representation learning. Similar to GCN, deep loopy graph
neural network Zhang (2018) proposes to update the node states in a synchronous manner, and it
introduces a spanning tree based learning algorithm for training the model. LoopyNet accepts
nodes’ raw features into each layer of the model, and it can effectively fight against the suspended
animation problem according to the studied in this paper. GAT Velickovic et al. (2018) leverages
masked self-attentional layers to address the shortcomings of GCN. In this year, we have also wit-
nessed some preliminary works on heterogeneous graph neural networks Wang et al. (2019); Liu
et al. (2018). Similar to GCN, GEM Liu et al. (2018) utilizes one single layer of attention to cap-
ture the impacts of both neighbors and network heterogeneity, which cannot work well on real-world
complex networks. Based on GAT, HAN Wang et al. (2019) learns the attention coefficients between
2
Under review as a conference paper at ICLR 2020
the neighbors based on a set of manually crafted meta paths Sun et al. (2011), which may require
heavy human involvements. DifNN Zhang et al. (2018) introduce a diffusive neural network for the
graph structured data specifically, which doesn’t suffer from the oversmoothing problem due to the
involvement of the neural gates and residual inputs for all the layers. Due to the limited space, we
can only name a few number of the representative graph neural network here. The readers are also
suggested to refer to page1 , which provides a summary of the latest graph neural network research
papers with code on the node classification problem.
Residual Network: Residual learning Srivastava et al. (2015); He et al. (2015); Bae et al. (2016);
Han et al. (2016); Gomez et al. (2017); Tai et al. (2017); Yu et al. (2017); Ahn et al. (2018); Li
et al. (2018a); Behrmann et al. (2019) has been utilized to improve the learning performance, espe-
cially for the neural network models with very deep architectures. To ease gradient-based training
of very deep networks, Srivastava et al. (2015) introduces the highway network to allow unimpeded
information flow across several layers. Innovated by the high-way structure, He et al. (2015) in-
troduces the residual network to simplify highway network by removing the fusion gates. After
that, residual learning has been widely adopted for deep model training and optimization. Bae et al.
(2016) introduces residual learning for image restoration via persistent homology-guided manifold
simplification; Tai et al. (2017); Li et al. (2018a) introduces a recursive residual network for image
resolution adjustment. Some improvement of residual network has also been proposed in recent
years. A reversible residual network is introduced in Gomez et al. (2017), where each layer’s activa-
tions can be reconstructed exactly from the next layer’s; Han et al. (2016) improves the conventional
model shape with a pyramidal residual network instead; Yu et al. (2017) introduce the dilated resid-
ual network to increases the resolution of output feature maps without reducing the receptive field
of individual neurons; and Ahn et al. (2018) studies the cascading residual network as an accurate
and lightweight deep network for image super-resolution. Readers can also refer to He (2016) for a
detailed tutorial on residual learning and applications in neural network studies.
3	Suspended Animation Problem with GCN Model
In this part, we will provide an analysis about the suspended animation problem of the spectral
graph convolutional operator used in GCN to interpret the causes of the observations illustrated
in Figure 1. In addition, given an input network data, we will provide the theoretic bound of the
suspended animation limit for the GCN model.
3.1	Vanilla Graph Convolutional Network Revisit
To make this paper self-contained, we will provide a brief revisit of the vanilla GCN model in this
part. Formally, given an input network G = (V, E), its network structure information can be denoted
as an adjacency matrix A = {0, 1}n×n (where |V| = n). GCN defines the normalized adjacency
11
matrix of the input network as A = D-2 AD-2, where A = A + In×n and D is the diagonal
∙-v	∙-v	∙-v
matrix of A with entry D(i, i) = j A(i, j). Given all the nodes in V together with their raw
feature inputs X ∈ Rn ×dx (dχ denotes the node raw feature length), GCN defines the spectral
graph convolutional operator to learn the nodes’ representations as follows:
H = SGC (X; G, W) = ReLU (AXW),
(1)
where W ∈ Rdx × dh is the variable involved in the operator.
Furthermore, GCN can involve a deep architecture by stacking multiple spectral graph convolu-
tional layers on each other, which will be able to learn very complex high-level representations of
the nodes. Here, let’s assume the model depth to be K (i.e., the number of hidden layers and output
layer), and the corresponding node representation updating equations can be denoted as:
H(0)	= X,
H(k) = ReLU (AH(kT)W(kT)) , ∀k ∈ {1, 2,…，K - 1},
Y = softmax (AH(KT)W(KT)).
(2)
https://paperswithcode.com/task/node-classification
3
Under review as a conference paper at ICLR 2020
3.2	Suspended Animation Problem with GCN
By investigating the spectral graph convolutional operator defined above, we observe that it actually
involves two sequential steps:
H(k) =ReLU (AH(kT)W(kT)ʌ ⇔ (MCLayer:	T(k)= AH(k 1)
= U (A	⇔ ⇔ [fC Layer:	H(k) = ReLU (T(k)W(I)),()
where the first term on the right-hand-side defines a 1-step Markov chain (MC or a random walk)
based on the graph and the second term is a fully-connected (FC) layer parameterized by variable
W(k-1). Similar observations have been reported in Li et al. (2018b) as well, but it interprets the
spectral graph convolutional operator in a different way as the Laplacian smoothing operator used
in mesh smoothing in graphics instead.
Therefore, stacking multiple spectral graph convolutional layers on top of each other is equivalent to
the stacking of multiple 1-step Markov chain layers and fully-connected layers in a crosswise way.
Considering that the variables W(k-1) for the vector dimension adjustment are shared among all
the nodes, given two nodes with identical representations, the fully-connected layers (parameterized
by W(k-1)) will still generate identical representations as well. In other words, the fully-connected
layers with shared variables for all the nodes will not have significant impacts on the convergence
of Markov chain layers actually. Therefore, in the following analysis, we will simplify the model
structure by assuming the mapping defined by fully-connected layers to be the identity mapping.
We will investigate the Markov chain layers closely by picking them out of the model to compose
the Markov chain of multiple steps:
T(0)
T(k)
X,
A T(k-1), ∀k ∈ {1, 2,…，K}.
(4)
Meanwhile, the Markov chain layers may converge with k layers iff T(k) = T(k-1), i.e., the repre-
sentations before and after the updating are identical (or very close), which is highly dependent on
the input network structure, i.e., matrix A, actually.
DEFINITION 1. (Irreducible and Aperiodic Network): Given an input network G = (V, E), G is
irreducible iff for any two nodes vi , vj ∈ V, node vi is accessible to vj. Meanwhile, G is aperiodic
iff G is not bipartite.
LEMMA 1. Given an unweighted input graph G, which is irreducible, finite and aperiodic, if its
corresponding matrix is asymmetric, Startingfrom any initial distribution vector X ∈ n×ry(L (X ≥ 0
and kxk1 = 1), the Markov chain operating on the graph has one unique stationary distribution
vector ∏ such that limt→∞ Atx = ∏, where π*(i) = d(vi). Meanwhile, ifmatrix A is symmetric,
the stationary distribution vector ∏ will be a uniform distribution over the nodes, i.e., π*(i) = ɪ.
Based on the above Lemma 1, we can derive similar results for the multiple Markov chain layers in
the GCN model based on the nodes’ feature inputs, which will reduce the learned nodes’ represen-
tations to the stationary representation matrix.
THEOREM 1. Given a input network G = (V, E), which is unweighted, irreducible, finite and ape-
riodic, if there exist enough nested Markov chain layers in the GCN model, it will reduce the nodes’
representations from the column-normalized feature matrix X ∈ n×^dχ to the stationary repre-
sentation Π* = [π*, π*, •…,π*] ∈ Rn×dx. Furthermore, if G is undirected, then the stationary
representation will become Π* = n ∙ 1n×dx.
Theorem 1 illustrates the causes of the GNNs’ suspended animation problem. Proofs of Lemma 1
and Theorem 1 are provided in the appendix attached to this paper at the end.
4 Suspended Animation Limit Analysis
Here, we will study the suspended animation limit of the GCN model based on its spectral convo-
lutional operator analysis, especially the Markov chain layers.
4.1	Suspended Animation Limit based Input Network S tructure
Formally, we define the suspended animation limit of GCN as follows:
4
Under review as a conference paper at ICLR 2020
Definition 2. (Suspended Animation Limit): The suspended animation limit of GCN on network
G is defined as the smallest model depth τ such that for any nodes’ column-normalized featured
matrix input X in the network G the following inequality holds:
kGCN(X；τ) - Π*∣∣ι ≤ 6.	(5)
For representation convenience, we can also denote the suspended animation limit of GCN defined
on network G as ζ(G) (or ζ for simplicity if there is no ambiguity problems).
Based on the above definition, for GCN with identity FC mappings, there exists a tight bound of the
suspended animation limit for the input network.
Theorem 2. Let 1 ≥ λι ≥ λ? ≥ ∙∙∙ ≥ λn be the eigen-values of matrix A defined based on
network G, then the corresponding suspended animation limit of the GCN model on G is bounded
Z≤o l Iogmini∏⅜)
_	yi - max{λ2, ∣λn∣}
(6)
In the case that the network G is a d-regular, then the suspended animation limit of the GCN model
on G can be simplified as
log n
1 - max{λ2, ∣λn∣}
ζ≤O
The suspended animation limit bound derived in the above theorem generally indicates that the
network structure G determines the maximum allows depth of GCN. Among all the eigen-values
of A defined on network G, λ? measures how far G is from being disconnected; and λn measures
how far G is from being bipartite. In the case that G is reducible (i.e., λ2 = 1) or bipartite (i.e.,
λn = -1), we have ζ → ∞ and the model will not suffer from the suspended animation problem.
In the appendix of Kipf & Welling (2016), the authors introduce a naive-residual based variant of
GCN, and the sepctral graph convolutional operator is changed as follows (the activation function
is also changed to sigmoid function instead):
H(k) = σ (AH(kT)W(kT)) + H(kT),	(8)
For the identity fully-connected layer mapping, the above equation can be reduced to the following
lazy Markov chain based layers
H(k) = 2 ∙ &AH(k-1) + 1 H(k-1)).
(9)
Such a residual term will not help resolve the problem, and it will still suffer from the suspended
animation problem with the following suspended animation limit bound:
Corollary 1. Let 1 ≥ λι ≥ λ2 ≥ ∙∙∙ ≥ λn be the eigen-values of matrix A defined based on
network G, then the corresponding suspended animation limit of the GCN model (with lazy Markov
chain based layers) on G is bounded
ζ≤O
(10)
Proofs to Theorem 2 and Corollary 1 will be provided in the appendix as well.
4.2 Other Practical Factors Impacting the Suspended Animation Limit
According to our preliminary experimental studies, GCN is quite a sensitive model. Besides the
impact of input network explicit structures (e.g., network size n, directed vs undirected links, and
network eigenvalues) as indicated in the bound equation in Theorem 2, many other factors can also
influence the performance of GCN model a lot, which are summarized as follows:
• Network Degree Distribution: According to Theorem 1, if the input network G is directed
and unweighted, the Markov chain layers at convergence will project the features to Π* =
5
Under review as a conference paper at ICLR 2020
Table 1: A Summary of Graph Residual Terms and Physical Meanings.
Name	Residual Term	Description	
naive resid- ual	R (H(I), X; G)	= H(k-1)	Node residual terms are assumed to be independent and determined by the current state only.
graph-naive residual	R (H(I), X; G)	=A H(kT)	Node residual terms are correlated based on network structure, and can be determined by the current state.
raw resid- ual	R (H(I), X; G)	=X	Node residual terms are assumed to be independent and determined by the raw input features only.
graph-raw residual	R (H(kT), X; G)	ʌ =A X	Node residual terms are correlated based on network structure, and are determined by the raw input features.
[π*, π*, ∙∙ ∙ , π*], where π*(i) is determined by the degree of node Vi in the network. For
any two nodes vi, vj ∈ V, the differences of their learned representation can be denoted as
d(vi,vj) = k∏(i,:)- Π(j, :)ki = X ∣∏(i,k)- Π(i,k)∣ = dx Id(Vi) 1d(v)| .(11)
k=1	2|E|
According to Newman (2003), the node degree distributions in most complex networks fol-
low the power-law Faloutsos et al. (1999), i.e., majority of the nodes are of very small de-
grees. Therefore, for massive nodes in the input network with the same (or close) degrees,
the differences between their learned representations will become not distinguishable.
• Raw Feature Coding: Besides the input network, the raw feature coding can also affect the
learning performance greatly. Here, we can take the GCN with one single Markov chain
layer and one identity mapping layer. For any two nodes Vi , Vj ∈ V with raw feature vectors
X(i, :) and X(j, :), we can denote the differences between their learned representations as
follows:
d(vi,Vj) = kT(i,:)- T(j, :)ki = ∣∣(A(i,:)- A(j,:)) X∣∣1.	(12)
Different from Equation (11), analysis in the above equation is not based on the stationary
representation of the nodes, and it is based on the MC-layer representation as indicated in
Equation (3) and Equation (4). For the one-hot feature coding used in the source code of
GCN Kipf & Welling (2016) and other GNNs, matrix X can be also very sparse as well.
Meanwhile, vector A(i, :) - A(j, :) is also a sparse vector, which renders the right-hand-
side term to be a very small value.
•	Training Set Size: Actually, the nodes have identical representation and the same labels
will not degrade the learning performance of the model. However, if such node instances
actually belong to different classes, it will become a great challenge for both the training
and test stages of the GCN model.
•	Gradient Vanishing/Exploding: Similar to the existing deep models, deep GNNs will
also suffer from the gradient vanishing/exploding problems Pascanu et al. (2012), which
will also greatly affect the learning performance of the models.
Although these factors mentioned above are not involved in the suspended animation limit bound
representation, but they do have great impacts on the GCN model in practical applications. In the
following section, we will introduce graph residual network GRESNET, which can be useful for
resolving such a problem for GCN.
5 Graph Residual Network
Different from the residual learning in other areas, e.g., computer vision He et al. (2015), where the
objective data instances are independent with each other, in the inter-connected network learning
setting, the residual learning of the nodes in the network are extensively connected instead. It renders
the existing residual learning strategy less effective for improving the performance of GCN.
5.1	Graph Residual Learning
Residual learning initially introduced in He et al. (2015) divides the objective mapping into two
parts: the inputs and the residual function. For instance, let H(x) be the objective mapping which
projects input x to the desired domain. The ResNet introduced in He et al. (2015) divides H(x) as
F(x) + R(x) (where R(x) = x is used in He et al. (2015)). This reformulation is motivated by the
counterintuitive phenomena about the degradation problem observed on the deep CNN. Different
6
Under review as a conference paper at ICLR 2020
Figure 2: A comparison of vanilla GCN and GRESNET (GCN) with different graph residual terms.
The vanilla GCN has two layers, and the GResNet models have a deep architecture which involves
seven layers of SGC operators. The intermediate ReLU activation functions used between sequential
layers in GResNet are omitted for simplicity. Notation [ɪ] denotes the normalized adjacency
matrix of input network G, which indicates the correlated graph residual terms.
from the learning settings of CNN, where the data instances are assumed to be independent, the
nodes inside the input network studied in GCN are closely correlated. Viewed in such a perspective,
new residual learning mechanism should be introduced for GCN specifically.
Here, We need to add a remark: For the presentation simplicity in this paper, given the objective
H(x) = F(x) + R(x), We will misuse the terminologies here: We will name F(x) as the approxi-
mated mapping of H(x), and call R(x) as the graph residual term. Formally, by incorporating the
residual learning mechanism into the GCN model, the node representation updating equations (i.e.,
Equation 2) can be rewritten as follows:
H(0)	= X
H(k) = ReLU (AH(kT)W(kT) + R (H(kT), X; G)) , ∀k ∈ {1, 2,…，K - 1},	(13)
Y = softmax (AH(KT)Wg + R (H(KT), X; G)).
The graph residual term R(H(k-1), X; G), ∀k ∈ {1,2, •一，K} can be defined in different ways.
We have also examined to put R(H(k-1), X; G) outside of the ReLU(∙) function for the hidden
layers (i.e., H(k) = ReLU (AH(k-1)W(k-1∕) + R (H(k-1), X; G)), whose performance is not as
good as what we show above. In the appendix of Kipf & Welling (2016), by following the ResNet
(CNN) He et al. (2015), the graph residual term R(H(k-1), X; G) is simply defined as H(k-1),
which is named as the naive residual term in this paper (Here, term “naive” has no disparaging
meanings). However, according to the studies, such a simple and independent residual term for the
nodes fail to capture information in the inter-connected graph learning settings.
5.2	GResNet Architecture
In this paper, we introduce several other different representations of the graph residual term, which
are summarized in Table 1. If feature dimension adjustment is needed, e.g., for raw residual term,
an extra variable matrix Wadj can be added to redefine the terms (which are not shown in this
paper). For the graph residual term representations in Table 1, naive residual and raw residual are
based on the assumption that node residuals are independent and determined by either the current
state or the raw features. Meanwhile, the graph naive residual and graph raw residual assume the
residual terms of different nodes are correlated instead, which can be computed with the current
state or raw features. We also illustrate the architectures of vanilla 2-layer GCN and the 7-layer
GRESNETs (taking GCN as the base model) with different graph residual terms in Figure 2. We
7
Under review as a conference paper at ICLR 2020
epoch (iter, over training set)
epoch (iter, over training set)
(c) Training Accuracy
GResNet(graph-naive)
(d) Testing Accuracy
GResNet(graph-naive)
(a) Training Accuracy
GRESNET(naive)
(e) Training Accuracy
GResNet(raw)
(b) Testing Accuracy
GRESNET(naive)
(f) Testing Accuracy
GResNet(raw)
epoch (iter, over training set)
(g) Training Accuracy
GResNet(graph-raw)
阳碉懒麻椰W
----GResNet(GCNfgraph-raw,l-layer)
----GReSNet(GCN,gr叩h_raw,2-layer)
----GReSNet(GCN,gr叩h_raw,3-layer)
----GResNet(GCNfgraph-raw,4-layer)
----GReSNet(GCN,gr叩h_raw,5-layer)
----GResNet(GCNfgraph-raw,6-layer)
----GReSNet(GCN,gr叩h_raw,7-layer)
200	400	600	800	1000
epoch (iter, over training set)
(h) Testing Accuracy
GResNet(graph-raw)
Figure 3: The learning performance of GResNet with GCN as the base model and different
graph residual terms on the Cora dataset: (a)-(b) GResNet(GCN, naive); (c)-(d) GResNet(GCN,
graph-naive); (e)-(f) GResNet(GCN, raw); (g)-(h) GResNet(GCN, graph-raw). For plot (b), the
curves corresponding to the 5-layer, 6-layer and 7-layer models are hidden by the legend. For plot
(c) and (d), the curve of 7-layer model is hidden by the legend.
need to add a remark here that similar solutions to the raw residual terms have been studied in the
latest research papers Wang et al. (2018); Wang & Gupta (2018) as well. However, in this paper,
we aim to generalize such techniques as a unified framework, which can accept different types of
residual terms not just the raw residual term.
Vanilla GCN: For the vanilla GCN network used in Kipf & Welling (2016), i.e., the left plot of
Figure 2, given the inputs G and X, it employs two SGC layers to project the input to the objective
labels. For the intermediate representations, the hidden layer length is 16, and ReLU is used as the
activation function, whereas softmax is used for output label normalization.
GResNet Network: For the GRESNET network, i.e., the right four models in Figure 2, they
accept the identical inputs as vanilla GCN but will create graph residual terms to be added to the
intermediate representations. Depending on the specific residual term representations adopted, the
corresponding high-way connections can be different. For the hidden layers involved in the models,
their length is also 16, and ReLU is used as the activation function for the intermediate layers.
By comparing the output Y learned by the models against the ground-truth Y of the training in-
stances, all the variables involved in the model, i.e., Θ, can be effectively learned with the back-
ʌ
propagation algorithm to minimize the loss functions '(Y, Y; Θ) (or '(Θ) for simplicity). In the
following part, we will demonstrate that for the GResNet model added with graph residual terms.
It can effectively avoid dramatic changes to the nodes’ representations between sequential layers.
5.3	Graph Residual Learning Effectiveness Analysis
In this part, we will illustrate why the inclusion of the graph residual learning can be effective for
learning deep graph neural networks. Here, we assume the ultimate model that we want to learn
as H : X → Y , where X and Y denote the feature and label spaces, respectively. For analysis
simplicity, we have some assumptions about the function H Zaeemzadeh et al. (2018).
ASSUMPTION 1. Function H is differentiable, invertible and satisfies the following conditions:
•	∀x, y, z ∈ X with bounded norm, ∃α > 0, k(H0(x) - H0(y))z∣∣ ≤ α ∙ ∣∣x - yk ∙ ∣∣z∣∣,
•	∀x, y ∈ X with bounded norm, ∃β > 0, ∣∣H-1(x) — H-1 (y) ∣∣ ≤ β ∙ ∣∣x — y k,
•	∃x ∈ X with bounded norm such that Det(H0 (x)) > 0.
In the above conditions, terms α and β are constants.
To model the function H, the GRESNET actually defines a sequence ofK sequential mappings with
these K layers:
x(k) =F(k-1)(x(k-1)) +R(k-1)(x(k-1)),	(14)
where x(k-1) and x(k) denote the intermediate node representations serving as input and output
of the kth layer. F(k-1) (∙) and R(k-1) (∙) denote the function approximation and residual term
8
Under review as a conference paper at ICLR 2020
Table 2: Best performance (accuracy) and model depth summarization of GResNet with different
residual terms on the benchmark datasets (we take GCN, GAT and LoopyNet as the base models).
Methods		Datasets (Accuracy & Model Depth)					
Base Models	Residuals	Cora		Citeseer		Pubmed	
vanilla GCN (KiPf & Wening (2016))		0.815	2-layer	0.703	2-layer	0.790	2-layer
GCN	naive	0.814	3-layer	0.710	3-layer	0.814	3-layer
	graph-naive	0.833	2-layer	0.715	3-layer	0.811	2-layer
	raw	0.826	4-layer	0.727	4-layer-	0.810	3-layer
	graph-raw	0.843	5-layer-	0.722	4-layer	0.817	7-layer-
vanilla GAT (VeIiCkOviC et al.(2018))		0.830	2-layer	0.725	2-layer	0.790	2-layer
GAT	naive	0.844	5-layer	0.735	5-layer	0.809	3-layer
	graph-naive	0.855	3-layer-	0.732	4-layer	0.815	5-layer
	raw	0.842	3-layer	0.733	3-layer	0.814	4-layer
	graph-raw	0.847	3-layer	0.729	5-layer	0.822	4-layer-
vanilla LOOPYNET(Zhang (2018))		0.826	2-layer	0.716	2-layer	0.812	2-layer
LoopyNet	naive	0.833	2-layer	0.728	3-layer	0.830	4-layer
	graph-naive	0.832	2-layer	0.728	3-layer	0.819	2-layer
	raw	0.836	2-layer	0.730	5-layer	0.828	4-layer
	graph-raw	0.839	4-layer-	0.737	5-layer-	0.814	4-layer
learned by the k - 1th layer of the model. When training these K sequential mappings, we have the
following theorem hold for the representation gradients in the learning process.
THEOREM 3. Let H denote the objective function that we want to model, which satisfies Assump-
tion 1, in learning the K -layer GRESNET model, we have the following inequality hold:
(1 - δ) I 螺 Ik I” ∣2
≤ (1+δ)
i ∂'(Θ i
1荷l2
(15)
where δ ≤ C ∙ 'ogKK)and C = ci ∙ max{ɑ ∙ β ∙ (1 + β),β ∙ (2 + α) + α} for some ci > 0.
Proof of Theorem 3 will be provided in the appendix. The above theorem indicates that in the
learning process, the norm of loss function against the intermediate representations doesn’t change
significantly between sequential layers. In other words, GResNet can maintain effective represen-
tations for the inputs and overcome the suspended animation problem. In addition, we observe that
the bound of the gap term δ, i.e., C ∙ (OgKK), decreases as K increases (when K ≥ 2). Therefore, for
deeper GResNet, the model will lead to much tighter gradient norm changes, which is a desired
property. In the following section, we will provide the experimental results of GResNet compared
against their vanilla models on several graph benchmark datasets.
6	Experiments
To demonstrate the effectiveness of GResNet in improving the learning performance for graph
neural networks with deep architectures, extensive experiments will be done on several graph
benchmark datasets. Similar to the previous works on node classification Kipf & Welling (2016);
Velickovic et al. (2018), the graph benchmark datasets used in the experiments include Cora, Cite-
seer and Pubmed from Sen et al. (2008). For fair comparison, we follow exactly the same experi-
mental settings as Kipf & Welling (2016) on these datasets.
In this paper, we aim at studying the suspended animation problem with the existing graph neural
networks, e.g., GCN Kipf & Welling (2016), GAT Velickovic et al. (2018) and LOOPYNET Zhang
(2018), where LoopyNet is not based on the spectral graph convolutional operator. We also aim
to investigate the effectiveness of these proposed graph residual terms in improving their learning
performance, especially for the models with deep architectures. In addition, to make the experiments
self-contained, we also provide the latest performance of the other baseline methods on the same
datasets in this paper, which include state-of-the-art graph neural networks, e.g., APPNP Klicpera
et al. (2019), GOCN Jiang et al. (2019) and GraphNAS Gao et al. (2019), existing graph embedding
models, like DeepWalk Perozzi et al. (2014), Planetoid Yang et al. (2016) and MoNet Monti et al.
(2016) and representation learning approaches, like ManiReg Belkin et al. (2006), SemiEmb Weston
et al. (2008), LP Zhu et al. (2003) and ICA Lu & Getoor (2003).
9
Under review as a conference paper at ICLR 2020
Table 3: Learning result accuracy of node classification methods. In the table, ‘-’ denotes the results
of the methods on these datasets are not reported in the existing works. Performance of GCN, GAT
and LoopyNet shown in Table 2 are not provided here to avoid reporting duplicated results.
Methods	Datasets (Accuracy)		
	Cora	Citeseer	Pubmed
LP (Zhu et al. (2003))	0.680	0.453	0.630
ICA (Lu & Getoor (2003))	0.751	0.691	0.739
ManiReg (Belkin et al. (2006))	0.595	0.601	0.707
SemiEmb (Weston et al. (2008))	0.590	0.596	0.711
DeepWalk (Perozzi et al. (2014))	0.672	0.432	0.653
Planetoid (Yang et al. (2016))	0.757	0.647	0.772
MoNet (Monti et al. (2016))	0.817	-	0.788
APPNP (Klicpera et al. (2019))^^	0.851	0.757	0.797
GOCN (Jiang et al. (2019))	0.848	0.718	0.797
GraphNAS (Gao et al. (2019))	-	0.731	0.769
GRESNET(GCN)	0.843	0.727	0.817
GRESNET(GAT)	0.855	0.735	0.822
GResNet(LoopyNet)	0.839	0.737	0.830
Reproducibility: Both the datasets and source code used in this paper can be accessed via link2.
Detailed information about the server used to run the model can be found at the footnote3.
6.1	Effectiveness of the Graph Residual Terms
In addition to Figure 1 for GCN (bias disabled) on the Cora dataset, as shown in Figures 9-11
in the appendix, for the GCN (bias enabled) and GAT with deep architectures, we have observed
similar suspended animation problems. Meanwhile, the performance of LoopyNet is different.
Since LoopyNet is not based on the spectral graph convolution operator, which accepts nodes’
raw features in all the layer (it is quite similar to the raw residual term introduced in this paper).
As the model depth increase, performance of LoopyNet remains very close but converge much
more slowly. By taking GCN as the base model, we also show the performance of GResNet with
different residual terms in Figure 3. By comparing these plots with Figure 1, both naive and graph-
naive residual terms help stabilize the performance of deep GRESNET(GCN)s. Meanwhile, for
the raw and graph-raw residual terms, their contributions are exceptional. With these two residual
terms, deep GResNet(GCN)s can achieve even better performance than the shallow vanilla GCNs.
Besides the results on the Cora dataset, in Table 2, we illustrate the best observed performance
by GResNet with different residual terms based on GCN, GAT and LoopyNet base models
respectively on all the datasets. Both the best accuracy score and the achieved model depth are
provided. According to the results, for the vanilla models, GCN, GAT and LoopyNet can all
obtain the best performance with shallow architectures. For instance on Cora, GCN(2-layer) obtains
0.815; GAT(2-layer) gets 0.830; and LOOPYNET(2-layer) achieves 0.826, respectively. Added
with the residual terms, the performance of all these models will get improved. In addition, deep
GRESNET(GCN), GRESNET(GAT) and GRESNET(LOOPYNET) will be able to achieve much
better results than the shallow vanilla models, especially the ones with the graph-raw residual terms.
The best scores and the model depth for each base model on these datasets are also highlighted. The
time costs of learning the GResNet model is almost identical to the required time costs of learning
the vanilla models with the same depth, which are not reported in this paper.
6.2	A Complete Comparison with Existing Node Classification Methods
Besides the comparison with GCN, GAT and LoopyNet shown in Table 2, to make the experi-
mental studies more complete, we also compare GResNet(GCN), GResNet(GAT) and GRes-
Net(LoopyNet) with both the classic and the state-of-the-art models, whose results are provided
in Table 3. In the table, we didn’t indicate the depth of the GResNet models and results of GCN,
GAT and LoopyNet (shown in Table 2 already) are not included. According to the results, com-
pared against these baseline methods, GResNets can also outperform them with great advantages.
Without the complex model architecture extension or optimization techniques used by the latest
methods APPNP Klicpera et al. (2019), GOCN Jiang et al. (2019) and GraphNAS Gao et al. (2019),
2https://github.com/anonymous-sourcecode/GResNet
3GPU Server: ASUS X99-E WS motherboard, Intel Core i7 CPU 6850K@3.6GHz (6 cores), 3 Nvidia
GeForce GTX 1080 Ti GPU (11 GB buffer each), 128 GB DDR4 memory and 128 GB SSD swap. For the deep
models which cannot fit in the GPU memory, we run them with CPU instead.
10
Under review as a conference paper at ICLR 2020
adding the simple graph residual terms into the base models along can already improve the learning
performance greatly.
7	Conclusion
In this paper, we focus on studying the suspended animation problem with the existing graph neural
network models, especially the spectral graph convolutional operator. We provide a theoretic anal-
ysis about the causes of the suspended animation problem and derive the bound for the maximum
allowed graph neural network depth, i.e., the suspended animation limit. To resolve such a problem,
we introduce a novel framework GResNet, which works well for learning deep representations
from the graph data. Assisted with these new graph residual terms, we demonstrate that GResNet
can effectively resolve the suspended animation problem with both theoretic analysis and empirical
experiments on several benchmark node classification datasets.
11
Under review as a conference paper at ICLR 2020
References
N. Ahn, B. Kang, and K. Sohn. Fast, accurate, and, lightweight super-resolution with cascading
residual network. CoRR, abs/1803.08664, 2018.
J.	Atwood and D. Towsley. Diffusion-convolutional neural networks. In NIPS, 2016.
W. Bae, J. Yoo, and J. Ye. Beyond deep residual learning for image restoration: Persistent homology-
guided manifold simplification. CoRR, abs/1611.06345, 2016.
Y. Bai, H. Ding, Y. Sun, and W. Wang. Convolutional set matching for graph similarity. arXiv
preprint arXiv:1810.10866, 2018.
P. Bartlett, S. Evans, and P. Long. Representing smooth functions as compositions of near-identity
functions with implications for deep network optimization. CoRR, abs/1804.05012, 2018.
P. Battaglia, J. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tac-
chetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer,
G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli,
M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu. Relational inductive biases, deep learning, and
graph networks. arXiv preprint arXiv:1806.01261, 2018.
J. Behrmann, W. Grathwohl, R. Chen, D. Duvenaud, and J. Jacobsen. Invertible residual networks.
In ICML, 2019.
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for
learning from labeled and unlabeled examples. J. Mach. Learn. Res., 2006.
J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via impor-
tance sampling. CoRR, abs/1801.10247, 2018.
M. Faloutsos, P. Faloutsos, and C. Faloutsos. On power-law relationships of the internet topology.
In SIGCOMM, 1999.
Y. Gao, H. Yang, P. Zhang, C. Zhou, and Y. Hu. Graphnas: Graph neural architecture search with
reinforcement learning. CoRR, abs/1904.09981, 2019.
A. Gomez, M. Ren, R. Urtasun, and R. Grosse. The reversible residual network: Backpropagation
without storing activations. In NIPS. 2017.
W. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. CoRR,
abs/1706.02216, 2017.
D. Hammond, P. Vandergheynst, and R. Gribonval. Wavelets on graphs via spectral graph theory.
Applied and COmpUtatiOnal Harmonic Analysis, 30(2):129 - 150, 2011.
D. Han, J. Kim, and J. Kim. Deep pyramidal residual networks. CoRR, abs/1610.02915, 2016.
K. He. Deep residual networks: Deep learning gets way deeper. https://icml.cc/2016/
tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf,
2016. [Online; accessed 15-September-2019].
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CORR,
abs/1512.03385, 2015.
O. Holder. Ueber einen mittelwertsatz. Nachrichten von der KOnigL, 2, 1889.
B. Jiang, Z. Zhang, J. Tang, and B. Luo. Graph optimized convolutional networks. CORR,
abs/1904.11883, 2019.
T. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv
preprint arXiv:1609.02907, 2016.
J. Klicpera, A. Bojchevski, and S. Gunnemann. Combining neural networks with personalized
pagerank for classification on graphs. In ICLR, 2019.
12
Under review as a conference paper at ICLR 2020
J. Li, F. Fang, K. Mei, and G. Zhang. Multi-scale residual network for image super-resolution. In
ECCV, 2018a.
Q. Li, Z. Han, and X. Wu. Deeper insights into graph convolutional networks for semi-supervised
learning. CoRR, abs/1801.07606, 2018b.
Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for
malicious account detection. In CIKM, 2018.
Q. Lu and L. Getoor. Link-based classification. In ICML, 2003.
J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural networks
on riemannian manifolds. In CVPR workshops, 2015.
F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. Bronstein. Geometric deep learning
on graphs and manifolds using mixture model cnns. CoRR, abs/1611.08402, 2016.
F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. Bronstein. Geometric deep learning
on graphs and manifolds using mixture model cnns. In CVPR, 2017.
M. Newman. The structure and function of complex networks. SIAM REVIEW, 45(2):2003, 2003.
M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In
ICML, 2016.
J. Norris. Markov chains. Cambridge University Press, 1998.
R. Pascanu, T. Mikolov, and Y. Bengio. Understanding the exploding gradient problem. CoRR,
abs/1211.5063, 2012.
B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In
KDD, 2014.
F. Scarselli, M. Gori, A. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network
model. Trans. Neur. Netw., 2009.
P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T Eliassi-Rad. Collective classification
in network data. AI Magazine, 2008.
R. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. CoRR, abs/1505.00387, 2015.
Y. Sun, R. Barber, M. Gupta, C. Aggarwal, and J. Han. Co-author relationship prediction in hetero-
geneous bibliographic networks. In ASONAM, 2011.
Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep recursive residual network. In CVPR,
2017.
P. Velickovic, C. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph Attention Net-
works. In ICLR, 2018.
N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y. Jiang. Pixel2mesh: Generating 3d mesh models
from single RGB images. CoRR, abs/1804.01654, 2018.
X. Wang and A. Gupta. Videos as space-time region graphs. CoRR, abs/1806.01810, 2018.
X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. Yu. Heterogeneous graph attention network.
In WWW, 2019.
J. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In ICML,
2008.
Z. Yang, W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embed-
dings. CoRR, abs/1603.08861, 2016.
F. Yu, V. Koltun, and T. Funkhouser. Dilated residual networks. CoRR, abs/1705.09914, 2017.
13
Under review as a conference paper at ICLR 2020
A. Zaeemzadeh, N. Rahnavard, and M. Shah. Norm-preservation: Why residual networks can
become extremely deep? CoRR, abs/1805.07477, 2018.
J. Zhang. Deep loopy neural network model for graph structured data representation learning. CoRR,
abs/1805.07504, 2018.
J. Zhang, B. Dong, and P. Yu. Fake news detection with deep diffusive network model. CoRR,
abs/1805.08751, 2018.
J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun. Graph neural networks: A review of
methods and applications. CoRR, abs/1812.08434, 2018.
X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian fields and har-
monic functions. In ICML, 2003.
14
Under review as a conference paper at ICLR 2020
8 Rebuttal New Materials
8.1	Experimental Results on other Larger Datasets
We added the experimental results on the PPI and Reddit datasets in the following table. Detailed
information about datasets is as follows:
•	PPI: Node #: 14,755; Edge #: 225,270; Feature #: 50; Class #: 121
•	Reddit: Node #: 232, 965; Edge #: 11,606,919; Feature #: 602; Class #: 41
We need to mention that GAT used in GResNet is very slow on large datasets (it is not our problem
but the problem with the GAT base model), we cannot get the results of GResNet(GAT) out on
Reddit during the rebuttal period. Since PPI and Reddit are not common used in other graph neural
network papers, so many of the entries in Table 4 are not provided. We will add these results in the
camera-ready version of this paper later.
Table 4: Learning result on larger datasets.
Methods	Datasets (Accuracy)			Dataset (micro-averaged F1)	
	Cora	Citeseer	Pubmed	PPI	Reddit
LP (Zhu et al. (2003))	0.680	^^0.453^^	0.630	-	-
ICA (Lu & Getoor (2003))	0.751	0.691	0.739	-	-
ManiReg (Belkin et al. (2006))	0.595	0.601	0.707	-	-
SemiEmb (Weston et al. (2008))	0.590	0.596	0.711	-	-
DeepWalk (Perozzi et al. (2014))	0.672	^^0.432^^	0.653	-	0.324
Planetoid (Yang et al. (2016))	0.757	0.647	0.772	-	-
MoNet (Monti et al. (2016))	0.817	-	0.788	-	-
APPNP (Klicpera et al. (2019))	0.851	^^0.757^^	0.797	-	-
GOCN (Jiang et al. (2019))	0.848	0.718	0.797	-	-
Graph-S (Gao et al. (2019))	-	0.731	0.769	-	-
GCN (Kipf & Welling (2016))	0.815	^^0.703^^	0.790	0.500	0.930
GAT (Velickovic et al. (2018))	0.830	0.725	0.790	0.973	-
LOOPYNET (Zhang (2018))	0.826	0.716	0.812	-	-
GraphSAGE (Hamilton et al. (2017))	-	-	-	0.612	0.954
FastGCN (Chen et al. (2018))	0.850	-	0.880	0.513	0.937
GRESNET(GCN)	0.843	^^0.727^^	0.817	0.832	0.950
GRESNET(GAT)	0.855	0.735	0.822	0.987	-
GResNet(LoopyNet)	0.839	0.737	0.830	0.980	0.955
15
Under review as a conference paper at ICLR 2020
8.2	Experimental Results on Deeper Models
8.2	. 1 Deeper Model Performance S ummary
As suggested by the reviewers, we also add the studies of GCN, GResNet(GCN, naive), GRes-
Net(GCN, graph-naive), GResNet(GCN, raw) and GResNet(GCN, graph-raw) with deeper
architectures (more than 7 layers) on the Cora dataset in the following table and plots
As illustrated in Table 7, we show the experimental results on Cora obtained by the models with 2,
10, 20, 30, 40, 50 layers, respectively. According to the numbers, GCN cannot really work well for
all the layers other than 2, and the performance of the naive and graph-naive residual terms is not
good neither. However, the performance of the raw and graph-raw residual terms is exceptionally
great. For instance, for the model with 40 layers, GResNet(GCN, raw) and GResNet(GCN,
graph-raw) can still achieve 0.790 and 0.838 as the accuracy scores.
Table 5: Learning result by models with deeper architectures.
Methods	Model Depth (Accuracy)					
	2	10	20	30	40	50
GCN	0.815	0.174	0.072	0.072	0.072	0.072
GResNet(GCN, naive)	0.804	0.416	0.126	0.057	0.057	0.057
GResNet(GCN, graph-naive)	0.833	0.169	0.057	0.057	0.057	0.057
GResNet(GCN, raw)	0.815	0.814	0.802	0.812	0.798	0.800
GResNet(GCN, graph-raw)	0.813	0.823	0.823	0.818	0.838	0.799
8.2.2 Deeper Model Learning Loss on Training and Testing Sets per Epoch
In addition to Table 7, we also illustrate the performance of the models in the learning process in
Figures 4-8, where both the training accuracy and testing accuracy on each epoch are provided. Ac-
cording to the results, GCN and GResNet(GCN, graph-naive) will fail to work when the model
architecture goes deeper. GResNet(GCN, naive) can slightly revive the models for 10 layers, but
will also fail to work for deeper architectures, e.g., 20, 30, 40 and 50 layers. However, GRes-
Net(GCN, raw) and GResNet(GCN, graph-raw) work very great for all these different depth in
the plot, which also illustrate the effectiveness of the raw and graph-raw residual terms proposed in
this paper.
(a) Training Accuracy
(b) Testing Accuracy
Figure 4:	Deeper GCN on Cora.
16
Under review as a conference paper at ICLR 2020
0
,kl
——GResNet(GCN,na
—GResNet(GCN,na
——GResNet(GCN,na
——GResNet(GCN,na
——GResNet(GCN,na
ve,2-layer)
ve,10-layer)
ve,20-layer)
ve,30-layer)
ve,40-∣ayer)
----GResNet(GCN,naive,50-layer)
0
----GResNet(GCN,naive,2-layer)
GResNet(GCN, naive, 10-layer)
----GResNet(GCN,naive,20-layer)
----GResNet(GCN,naive,30-layer)
----GResNet(GCN,naive,40-∣ayer)
----GResNet(GCN,naive,50-layer)
200	400	600	800	1000
epoch (iter, over training set)
200	400	600	800	1000
epoch (iter, over training set)

(a) Training Accuracy
(b) Testing Accuracy
Figure 5:	Naive Residual: deeper GResNet(GCN, naive) on Cora.
GResNetfGCN,graph-naιve,2-layer)
GResNet(GCN,graph_naive, 10-layer)
----GResNet(GCN,graph_naive,20-layer)
----GResNet(GCN,graph_naive,30-layer)
I --- GResNet(GCN,graph_naive,40-layer)
L---- GResNet(GCN,graph_naive,50-layer)
200	400	600	800	1000
epoch (iter, over training set)
0 8 6 4 2
LSSSS
% Ausnuue 6u⊂-eh
(a)	Training Accuracy
(b)	Testing Accuracy
Figure 6: Graph-Naive Residual: deeper GResNet(GCN, graph-naive) on Cora.
----GResNetfGCN,raw,2-layer)
—GResNet(GCN,raw,10-layer)
----GResNet(GCN,raw,20-layer)
----GResNet(GCN,raw,30-layer)
----GResNet(GCN,raw,40-∣ayer)
----GResNet(GCN,raw,50-layer)
O
200	400	600	800 IOOO
epoch (iter, over training set)
(a)	Training Accuracy
(b)	Testing Accuracy
Figure 7: Raw Residual: deeper GResNet(GCN, raw) on Cora.
0
----GResNet(GCN,graph_raw,2-layer)
—GResNet(GCN,graph_raw,10-layer)
----GResNet(GCN,graph_raw,20-layer)
----GResNet(GCN,graph_raw,30-layer)
----GResNet(GCN,graph_raw,40-layer)
----GResNet(GCN,graph_raw,50-layer)
----GResNet(GCN,graph_raw,2-layer)
—GResNet(GCN,graph_raw,10-layer)
----GResNet(GCN,graph_raw,20-layer)
----GResNet(GCN,graph_raw,30-layer)
----GResNet(GCN,graph_raw,40-layer)
----GResNet(GCN,graph_raw,50-layer)
200	400	600	800	1000
epoch (iter, over training set)
(a) Training Accuracy
(b) Testing Accuracy
Figure 8: Graph-Raw Residual: deeper GResNet(GCN, graph-raw) on Cora.
We have also studied even deeper model architectures on GResNet(GCN, raw) and GRes-
Net(GCN, graph-raw) to 100 layers, which can still perform well.
17
Under review as a conference paper at ICLR 2020
9	Studies on Other Impacting Factors with Residual Terms
By changes to the original raw data, we will be able to get different samples of the data with different
properties, e.g., raw features, and the training set size. We will illustrate the learning results based
on these different settings on the Cora dataset as follows for the reviewers’ information.
9.1	Training Set Size
Table 6: Learning result by models with deeper architectures.
Methods	Training Set Size		(Regular Size: 140)	
	140	500	1000	1500
GCN(2-layer)	0.815	0.863	0.922	0.933
GCN(5-layer)	0.315	0.331	0.403	0.324
GRESNET(GCN, naive, 2-layer)	0.797	0.842	0.936	0.956
GRESNET(GCN, naive, 5-layer)	0.596	0.748	0.813	0.958
GRESNET(GCN, graph-naive, 2-layer)	0.833	0.862	0.927	0.956
GRESNET(GCN, graph-naive, 5-layer)	0.749	0.797	0.904	0.946
GRESNET(GCN, raw, 2-layer)	0.809	0.799	0.918	0.979
GResNet(GCN, raw, 5-layer)	0.816	0.861	0.932	0.976
GRESNET(GCN, graph-raw, 2-layer)^^	0.809	0.853	0.929	0.966
GResNet(GCN, graph-raw, 5-layer)	0.843	0.859	0.917	0.963
9.2	Raw Feature Encoding
Table 7: Learning result by models with deeper architectures.
Methods	Feature Coding (Regular Coding: bag-of-word)	
	bag-of-word	one-hot
GCN(2-layer)	0.815	0.795
GCN(5-layer)	0.315	0.199
GRESNET(GCN, naive, 2-layer)	0797	0.700
GRESNET(GCN, naive, 5-layer)	0.596	0.592
GResNet(GCN, graph-naive, 2-layer)	0833	0.797
GResNet(GCN, graph-naive, 5-layer)	0.749	0.748
GRESNET(GCN, raw, 2-layer)	0809	0.714
GRESNET(GCN, raw, 5-layer)	0.816	0.721
GRESNET(GCN, graph-raw, 2-layer)	0.809	0.746
GRESNET(GCN, graph-raw, 5-layer)	0.843	0.798
18
Under review as a conference paper at ICLR 2020
10	Appendix
10.1	Extra Experimental Results
GCN(I-Iayer)
GCN(2-layer)
GCN(3-layer)
GCN(4-layer)
GCN(5-layer)
GCN(6-layer)
GCN(7-layer)
GCN(8-layer)
GCN(9-layer)
GCN(IO-Iayer)
200	400	600	800	1000
epoch {iter, over training set)
——GCN(I-Iayer)
一GCN(2-layer)
——GCN(3-layer)
——GCN(4-layer)
——GCN(S-Iayer)
——GCN(6-layer)
—GCN(7-layer)
—GCN(S-Iayer)
一GCN(9-layer)
—GCN(IO-Iayer)
200	400	600	800	1000
epoch (iter, over training set)
87654321
660.60.0.0.6
% XUSDUUro 6ucroh
(a)	Training Accuracy
Figure 9: The learning performance of GCN
----GResNet(GAT,none,I-Iayer)
—GResNet(GAT,none,2-layer)
----GResNet(GAT,none,3-layer)
----GResNet(GAT,none,4-∣ayer)
----GResNet(GAT,none,5-layer)
----GResNet(GAT,none,6-layer)
—GResNet(GAT,none,7-layer) _
200	400	600	800	1000
epoch (iter, over training set)
(b)	Testing Accuracy
(bias enabled) on the Cora dataset.
0.8-舲m***∙**s**we*βMM>⅜Wl*fl≡l**
----GResNet(GAT,none,I-Iayer)
—GResNet(GAT,none,2-layer)
----GResNet(GAT,none,3-layer)
----GResNet(GAT,none,4-∣ayer)
----GResNet(GAT,none,5-layer)
----GResNet(GAT,none,6-layer)
—GResNet(GAT,none,7-layer)
0.11
0	200	400	600	800	1000
epoch (iter, over training set)
(b) Testing Accuracy (GAT)
the Cora dataset
(a) Training Accuracy (GAT)
Figure 10: The learning performance of GAT on
----LoopyNet(I-Iayer)
—LoopyNet(2-layer)
----LoopyNet(B-Iayer)
----LoopyNet(4-layer)
----LoopyNet(S-Iayer)
----LoopyNet(G-Iayer)
—LoopyNet(7-layer)

200	400	600	800	1000
epoch (iter, over training set)
LoopyNet(I-Iayer)
LoopyNet(2-layer)
LoopyNet(S-Iayer)
LoopyNet(4-layer)
LoopyNet(S-Iayer)
LoopyNet(G-Iayer)
LoopyNet(7-layer)
(a)	Training Accuracy (LoopyNet)
(b)	Testing Accuracy (LoopyNet)
Figure 11: The learning performance of LoopyNet on the Cora dataset
19
Under review as a conference paper at ICLR 2020
10.2 Proofs of Theorem and Lemma
10.2.1	Proof of Lemma 1
LEMMA 1. Given an irreducible, finite and aperiodic graph G, starting from any initial distribution
vector X ∈ Rn×1 (X ≥ 0 and ∣∣xkι = 1), the Markov chain operating on the graph has one unique
stationary distribution vector ∏ such that limt→∞ Atx = ∏, where ∏ (i)=第孑.Ifmatrix A is
symmetric (i.e., G is undirected), ∏ will be a uniform distribution over the nodes, i.e., π*(i) = ɪ.
Proof. The stationary distribution vector existence and uniqueness has been proved in Norris (1998).
Here, We need to identify on vector π at convergence such that Aπ = π, i.e.,
∏(i) = £A(i,j)n(j).
j
(16)
According to the definition of A, it is easy to have
XX dw⅛ ∏(j)
π(i).
(17)
Where wi,j denotes the initial connection Weight betWeen vi and vj. For the unWeighted netWork,
wi,j Will be in {0, 1} indicating if vi and vj are connected or not. Notation dw(i) denotes the rough
degree of node Vi in the network subject to the weight W : E ∈ R, which sums the weight of edges
connected to the nodes in the network. So, it is enough to have π(j) 8 dw (j). More precisely, we
can set
π(j)
dw (j)
Pk dw(j)
dw(j)
西.
(18)
In this case,
X A(ij)πj) = X wi,j dw(j) = X Wii = dw⑶=π⑴	(19)
j (,j) (j)= j dw (j) 2|E| = j 2|E| = 2|E| = ().	(9)
Meanwhile, for the symmetric and normalized adjacency matrix A, we can prove the stationary
distribution π*(i) = n in a similar way, which concludes the proof.	□
10.2.2	Proof of Theorem 1
THEOREM 1. Given a input network G = (V, E), which is unweighted, irreducible, finite and ape-
riodic, if there exist enough nested Markov chain layers in the GCN model, it will reduce the nodes’
representations from the column-normalized feature matrix X ∈ n×^dχ to the stationary repre-
sentation Π* = [π*, π*, •…,π*] ∈ Rn×dx. Furthermore, if G is undirected, then the stationary
representation will become Π* = ɪ ∙ 1n×dx.
Proof. This theorem can be proved based on Lemma 1. For any initial state distribution vector
x ∈ R∣V∣×1, for the Markov chain at convergence, we have
lim Atx = π*.
t→∞
We misuse the notation A * = limt→∞ At. In this case,
ʌ ʌ ʌ ʌ
A*X = [A*X(:, 1), A*X(:, 2),…，A*X(:, dχ)]
=[∏*,∏*,…，∏*],
(20)
(21)
which together with Lemma 1 conclude the proof.
□
20
Under review as a conference paper at ICLR 2020
10.2.3	Proof of Theorem 2
Prior to introducing the proof of Theorem 2, we will introduce the following lemma first.
LEMMA 2. For any vector X ∈ Rn,thefollowing inequality holds:
kxkp ≤ (n)P-q kxkq .	(22)
Proof. According to Holder,s inequality Holder (1889), for ∀a, b ∈ Rn×1 and r > 1,
n
X |a(i)||b(i)| ≤
i=1
(23)
Let |a(i)| = ∣Xi∣p, |b(i)| = 1 andr = P,
nn
X ∣Xilp = X ∣Xilp ∙ 1
(24)
Therefore,
(25)
Theorem 2. Let 1 ≥ λι ≥ λ? ≥ ∙∙∙ ≥ λn be the eigen-values of matrix A defined based on
network G, then the corresponding suspended animation limit of the GCN model on G is tightly
bounded
Z ≤o l Iogmini π⅛
一11 - max{λ2, ∣λn∣}
(26)
□
ζ≤O
(27)
In the case that the network G is a d-regular, then the suspended animation limit of the GCN model
on G can be simplified as
log n
1 - max{λ2, ∣λn∣}
Proof. Instead of proving the above inequality directly, we propose to prove that ζ is suspended
animation limit by the following inequality instead
IA ZX -πt ≤√√n,	(28)
which can derive the following inequality according to Lemma 2:
IlAζX - ∏*( ≤ e.	(29)
21
Under review as a conference paper at ICLR 2020
ʌ
Let vι, v2,…，Vn be the eigenvectors of A and ∀x
	x =	hx, vii vi =	αivi, ii		(30)
where αi = hx, vii. Therefore, we have	A ZX = Eai λζ Vi.		(31)
			□
Considering that λζ1 =	=1 and vι = [√，√，…，√]>,then		
	αι = hχ, vιi = X χ(i) √n = √n kχkι i	_ 1 =√n,	(32)
and	aιλfVi = ɪvi = [ɪ,—,…，口 = n nn n	∏*,	(33)
where			
Therefore, we have
IAζ x — π*	22 =	αiλiζvi i=2	2 = X αi2λi2ζ 2	i=2
	≤ X αi2λ2ζ i max		= X αi2vi>viλ2mζax max
	i=2		i=2
	≤k	χk22 λ2mζax	
	≤ λ2mζax ,		
(34)
where λmaχ = max{∣λ2∣, ∣λ3∣,…，∣λn∣} = max{λ2, ∣λn∣}.
Therefore, to ensure
2
max{λ2, lλnl}≤ n
we can have
ζ≤
log - log n
2 log λmaχ
≤O
(35)
(36)
10.2.4 Proof of Corollary 1
Corollary 1. Let 1 ≥ λι ≥ λ2 ≥ ∙∙∙ ≥ λn be the eigen-values of matrix A defined based on
network G, then the corresponding suspended animation limit of the GCN model (with lazy Markov
chain based layers) on G is tightly bounded
ζ≤O
(37)
Proof. For the lazy Markov chain layer, we have its updating equation as follows
T = 2 A H(k-1) + 2 H(kT) = 1(A + diag({dw (i)}vi∈v))H(I) = A H(I).	(38)
It is easy to show that A is positive definite and We have its eigen-values λι ≥ λ? ≥∙∙∙≥ λn ≥ 0.
Therefore,
λmax = max{|12 |, lλ3 |, ∙∙∙ , lλn |} = max{12, |》n|} = λ2,	(39)
which together with Theorem 2 conclude the proof.	□
22
Under review as a conference paper at ICLR 2020
10.3 Proof of Theorem 3
Prior to introducing the proof of Theorem 3, we first introduce the following lemma.
LEMMA 3. For any non-singular matrix I + M, we have
1 - σmax(M) ≤ σmin(I + M) ≤ σmax(I + M) ≤ 1 + σmax(M),	(40)
where σmaχ(∙) and σmin(∙) denote the maximum and minimum singular values of the input matrix,
respectively.
Proof. Due to the triangle inequality, the upper bound is easy to prove:
σmax(I + M) = kI + Mk2 ≤ kIk2 + kMk2 = 1 + σmax(M).	(41)
In the case that σmax(M) ≥ 1, the lower bound is trivial to prove since I + M is non-singular, we
have
σmin(I + M) >0.	(42)
Meanwhile, in the case that σm,aχ(M) < 1, it is easy to know that ∣λmaχ(M) | < 1, where λmaχ(∙)
denotes the latest eigenvalue of the input matrix.
σmin(I + M) = (I + M)-12-1
(-1)kMk
k=1	2
≥
≥
X(-1)kMk2
X∞ kMk2k!-1
(τ-k1M2)	=1-σmax(M)
(43)
which concludes the proof for the lower bound.
□
THEOREM 3. Let H denote the objective function that we want to model, which satisfies Assump-
tion 1, in learning the K -layer GRESNET model, we have the following inequality hold:
(1 - δ) I 普 ∣2 ≤ I* ∣2 ≤ (1+δ) I 普 ∣2
where δ ≤ C'ogKK) and C = ci max{αβ(1 + β), β(2 + α) + α} for some ci > 0.
Proof. We can represent the Jacobian matrix J of x(k) with x(k-1). Therefore, we have
∂'(Θ) = ∂'(Θ) ∂x(k)= j ∂'(Θ)
∂χ(k-i) ∂χ(k) ∂χ(k-i)	∂χ(k).
(44)
(45)
Matrix J can be rewritten as J = I + VF(k-i)(x(k-i)), where
VF (k-i)(χ(k-i)) = t→lim0+
F (k-i)(χ(k-i) + tv) - F (k-i)(χ(k-i))
(46)
Meanwhile, it is easy to know that
σ
min (J)
I 煨 ∣2≤
J∂'(Θ)I <rτ (JI∂'(Θ)I
J^XW∣∣2≤ σmax(J)I^XW∣∣2
(47)
□
23
Under review as a conference paper at ICLR 2020
Based on the above lemma, we have
…)I*卜I "卜(1+ σ)/L
(48)
where σ = σm°χ(VF (I)(X(I))).
Furthermore, we know that
σmax(VF (k-1)(X(k-1))
supv
lVF(kτ)(χ(kτ))vl2
Ilvk2
lim supv
t→0+
||F(kT)(X(k-D + tv) — F(kT)(X(kT)) I?
t Ilvk2
(49)
≤ IIF(k-1)IIL
where ∣∣∙∣l denotes the LiPschitz seminorm of the input function and it is defined as
F(k-1)IIL
IF(I)(X)- F(I)(y)112
IX - yI2
(50)
Meanwhile, according to the Theorem 1 in Bartlett et al. (2018) (whose proof will not be introduced
here), we know that
|F(I) Il ≤ C厘
which concludes the proof. In the above equation, K denotes the layer depth of the model.
(51)
24