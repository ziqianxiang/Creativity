Under review as a conference paper at ICLR 2020
Pareto Optimality in No-Harm Fairness
Anonymous authors
Paper under double-blind review
Ab stract
Common fairness definitions in machine learning focus on balancing various no-
tions of disparity and utility. In this work we study fairness in the context of risk
disparity among sub-populations. We introduce the framework of Pareto-optimal
fairness, where the goal of reducing risk disparity gaps is secondary only to the
principle of not doing unnecessary harm, a concept that is especially applicable
to high-stakes domains such as healthcare. We provide analysis and methodology
to obtain maximally-fair no-unnecessary-harm classifiers on finite datasets. We
argue that even in domains where fairness at cost is required, no-unnecesary-harm
fairness can prove to be the optimal first step. This same methodology can also
be applied to any unbalanced classification task, where we want to dynamically
equalize the misclassification risks across outcomes without degrading overall per-
formance any more than strictly necessary. We test the proposed methodology on
real case-studies of predicting income, ICU patient mortality, classifying skin le-
sions from images, and assessing credit risk, demonstrating how the proposed
framework compares favorably to other traditional approaches.
1	Introduction
Machine learning algorithms play an important role in decision making in society. When these al-
gorithms are used to make high-impact decisions such as hiring, credit-lending, predicting mortality
for intensive care unit patients, or classifying benign/malign skin lesions, it is paramount to guar-
antee that these decisions are both accurate and unbiased with respect to sensitive attributes such as
gender or ethnicity. A model that is trained naively may not have these properties by default; see,
for example Barocas & Selbst (2016).
In these critical applications, it is desirable to impose some fairness criteria. Much of the fairness in
machine learning literature attempts to produce algorithms that satisfy Demographic Parity, which
aims to make algorithm’s predictions independent of the sensitive populations (Louizos et al. (2015);
Zemel et al. (2013); Feldman et al. (2015)); or Equality of Odds or Equality of Opportunity, which
aims to produce predictions that are independent of the sensitive attributes given the ground truth
(Hardt et al. (2016); Woodworth et al. (2017)). Notions of Individual Fairness have also been ad-
vanced (Dwork et al. (2012); Joseph et al. (2016); Zemel et al. (2013)). These notions of fairness
can be appropriate in many scenarios, but in domains where quality of service is paramount, such
as healthcare, we argue that it is necessary to strive for models that are as close to fair as possi-
ble without introducing any unnecessary harm to any subgroup (Ustun et al. (2019)). Even if the
overall fairness goal is a potentially harmful, zero-gap classifier, pursuing first a Pareto-fair classi-
fier and later applying other harmful methodologies ensures that all possible non-harmful trade-offs
are covered before explicitly degrading performance, therefore minimal harm is introduced to the
decision.
In this work we make use of the concept of Pareto optimality to measure and analyze discrimination
(unfairness) in terms of the difference in predictive risks across sub-populations defined by our
sensitive attributes, a fairness metric that has been explored in other recent works such as Calders
& Verwer (2010); Dwork et al. (2012); Feldman et al. (2015); Chen et al. (2018); Ustun et al.
(2019). We examine the subset of models from our hypothesis class that have the best trade-offs
between sub-population risks, and select from this set the one with the smallest risk disparity gap.
This is in direct contrast to common post-hoc correction methods like the ones proposed in Hardt
et al. (2016); Woodworth et al. (2017), where noise is potentially added to the decisions of the best
performing sub-population. While this latter type of approach diminishes the risk-disparity gap, it
does so by degrading performance on advantaged groups, the previously disadvantaged groups do
not directly benefit from this treatment. Since our proposed methodology does not require test-time
1
Under review as a conference paper at ICLR 2020
access to sensitive attributes, and can be applied to any standard classification or regression task, it
can also be used to reduce risk disparity between outcomes, acting as an adaptive risk equalization
loss compatible with unbalanced classification scenarios.
Main Contributions. We formalize the notion of no-harm risk fairness1 using Pareto optimality
(Mas-Colell et al. (1995)), a state of resource allocations from which it is impossible to reallocate
without making one subgroup worse. We show that finding a Pareto-fair classifier is equivalent to
finding a model in our hypothesis class that belongs to the Pareto front (the set of all Pareto opti-
mal allocations) with respect to the sub-population risks with the smallest possible risk disparity.
This general notion is already amenable to non-binary sensitive attributes. We analyze the fairness
performance trade-offs that can be expected from different approaches with an illustrative exam-
ple. We provide a concrete algorithm that promotes fair solutions belonging to the Pareto front;
this algorithm can be applied to any standard classifier or regression task that can be trained using
(Stochastic) Gradient Descent. We show that if the goal is to obtain a zero-gap classifier, first recov-
ering the fairest Pareto optimal solution and then adding harmful post-hoc corrections ensures the
lowest risk levels across all subgroups. We demonstrate how our methodology performs on synthetic
and real tasks such as inferring income status in the Adult dataset Dua & Graff (2017a) irrespective
of their ethnicity or gender, predicting ICU mortality rates in the MIMIC-III dataset from hospital
notes Johnson et al. (2016), classifying skin lesions in the HAM10000 dataset Tschandl et al. (2018),
and assessing credit risk on the German Credit dataset Dua & Graff (2017b).
2	Related Work
There is an extensive body of work on fairness in machine learning. Following Friedler et al. (2019),
we compare our methodology against the works of Feldman et al. (2015); Kamishima et al. (2012);
Zafar et al. (2017). Our method shares conceptual similarities with Zafar et al. (2017); Woodworth
et al. (2017); Agarwal et al. (2018), with differences on how we define our fairness objective and
adapt it to work with standard neural networks. Although optimality is often discussed in the fairness
literature, itis usually in the context of utility-discrimination tradeoffs. To the best of our knowledge,
this is the first work to discuss optimality with respect to subgroup risks on a unified classifier, a
distinction that disallows extreme performance degradation in the pursuit of fairness.
The work presented in Hashimoto et al. (2018) discusses decoupled classifiers as a way of minimiz-
ing group-risk disparity, but simultaneously cautions against this methodology when presented with
insufficiently large datasets. The works of Chen et al. (2018); Ustun et al. (2019) also empirically
report the disadvantages of decoupled classifiers as a way to mitigate risk disparity. Here we argue
for the use of a single classifier because it allows transfer learning between diverse sub-populations.
We do not need access to the sensitive attribute during test time, but in cases where this is possible,
we instead choose to incorporate it as part of our observation features.
The work of Chen et al. (2018) uses the unified bias-variance decomposition advanced in Domin-
gos (2000) to identify that noise levels across different sub-populations may differ, making per-
fect fairness parity impossible without explicitly degrading performance on one subclass. Their
methodology attempts to bridge the disparity gap by collecting additional samples from high-risk
sub-populations. Here we modify our classifier loss to bridge the disparity gap without inducing
unnecessary harm, which could prove to be synergistic with their methodology.
3	Problem Statement
Consider we have access to a dataset D = {(xi, yi, ai)}in=1 containing n independent triplet samples
drawn from ajoint distribution (xi, yi, ai)〜P(X, Y, A) where Xi ∈ X are our input features (e.g.,
images, tabular data, etc.), yi ∈ Y is our target variable, and ai ∈ A indicates group membership or
sensitive status (e.g., ethnicity, gender); our input features X may or may not explicitly contain A.
Let h ∈ H be a classifier from a compact hypothesis class H trained to infer y from x, h : X → Y ;
and a loss function ` : Y × Y → R. We define the class-specific risk of classifier h on subgroup a as
Ra(h) = Eχ,γ∣A=a['(h(X), Y)]. The risk discrimination gap between two subgroups a,a0 ∈ A is
1Inthis paper we use “no-harm” to mean “no-unnecessary-harm,” in other words, the system doesn’t degrade
performance on any class unless it is strictly necessary to improve performance in a disadvantaged class.
2
Under review as a conference paper at ICLR 2020
measured as Γa,a0 (h) = |Ra(h) - Ra0 (h)|, and we define the pairwise discrimination gap vector as
~ΓA(h) = {Γa,a0 (h)}a,a0∈A. Our goal is to obtain a classifier h ∈ H that minimizes this gap without
causing unnecessary harm to any particular group in A. To formalize this notion, we define:
Definition 3.1. Dominant risk vector: A vector r0 ∈ Rk is said to dominate r ∈ Rk, noted as r	r0,
if ri ≥ ri0, ∀i = 1, ..., k and ∃j : rj > rj0 (i.e., strict inequality on at least one component).
Definition 3.2. Dominant risk classifier: Classifier h0 is said to dominate h, noted as h	h0 , if the
risks vector r0 = {Ra(h0)}|aA=|1 dominates r = {Ra(h)}|aA=|1.
Definition 3.3. Pareto front: We define the Pareto front as P(H, A) = {h ∈ H : @h0 ∈ H | h
h0}. This means that there is no other classifier in H that is at least as good in all risks and strictly
better in at least one of them. It is the set of classifiers such that improving one group’s risk comes
at the cost of increasing other’s.
The Pareto front defines the best achievable trade-offs between population risks Ra(h). This def-
inition is already suited for classification and regression tasks where the sensitive attributes are
categorical. Constraining the classifier to be in the Pareto front disallows laziness, there exists no
other classifier in the hypothesis class H that is at least as good on all class-specific risks and strictly
better in one of them. As shown in Chen et al. (2018); Domingos (2000), the risk can be decom-
posed in bias, variance and noise for some loss functions, where the noise is the smallest achievable
risk for infinitely large datasets (Bayes-optimal risk). If the noise differs between sensitive groups,
zero-discrimination (perfect fairness) can only be achieved by introducing bias or variance.
Literature on fairness has focused on putting constraints on the norm of discrimination gaps (Zafar
et al. (2017; 2015); Creager et al. (2019); Woodworth et al. (2017)). We follow a similar criteria in
Definition 3.4 and define the Pareto-fair classifier as the classifier in the Pareto front that minimizes
l∣Γ∕(h) ∣∣∞ (the maximum risk discrimination gap). Note that one could alternatively choose to find
the Pareto classifier that minimizes the maximum subgroup risk.
Definition 3.4. Pareto-fair classifier and Pareto-fair vector: A classifier h is an optimal Pareto-
fair classifier if it minimizes the discrimination gap among all Pareto front classifiers, h =
arg min ∣∣Γ∕(h)∣∣∞. The Pareto-fair vector r* ∈ R1A1 is defined as r* = {R0(h* )}a∈A.
h∈P(H,A)
Even when perfect equality of risk is desirable, Pareto classifiers still serve as useful intermedi-
aries. To this end, Lemma 3.1, shows that applying a mechanism for reaching equality of risk on
a dominated classifier h ∈ H leads to equal or worse risks than applying it to a Pareto classifier
hp ∈ P(H, A) that dominates h.
Lemma 3.1. Ifh 6∈ P(H, A)	→ ∃hp	∈	P(H, A) :	hp	h ∧	Ra(hpER)	≤	Ra(hER)∀a,	with hER
an equal-risk classifier : Ra(hER) = max Ra0 (h), ∀a and hpER : Ra(hpER) = max Ra0 (hp).
a0∈A a	a0∈A a
To exemplify these notions graphically, Figure 1 shows a scenario with binary sensitive attributes a
and binary output variable y where none of the Pareto front classifiers achieve equality of risk. Here
the noise level differs between subgroups, and the Pareto-fair vector r* is not achieved by either a
Naive classifier (minimizes expected global risk), or a classifier where subgroups are re-sampled to
appear with equal probability (rebalanced Naive classifier). Note that the amount of performance
degradation required to enforce perfect fairness starting from the Naive classifier is higher than when
starting from the Pareto-fair vector.
Our objective is to find the Pareto-fair classifier h* as in Definition 3.4. In particular, we will give
regularity conditions on the hypothesis class H and the loss function '(h(X), Y). It can be shown
that for a sufficiently rich class of hypothesis functions H and for risk functions Ra (h) that are
convex with respect to h, the space of risk vectors is convex (see Geoffrion (1968); Koski (1985);
Miettinen (2012)). Under these conditions, we can find an auxiliary loss function φ : R|A| → R
defined in terms of the subgroup risks {Ra(h)}a∈A (denoted from here on as r ∈ R|A|) that has a
global minima on r*.
To prove the existence of loss function φ with the desired global minimum r*, it is convenient to
think of the convex set of risk vectors as the intersection ofa convex Pareto set (defined by a convex
Pareto front and all risks that are dominated by it), and an additional convex set Ω. We further
require both this Pareto set and Ω to be smooth, so that We can apply standard tools from smooth
convex optimization and prove that for every risk vector r0 in the Pareto front there exists a function
φ that has r0 as its global optima. This is formalized in Lemma 3.3.
Definition 3.5. Pareto field: A function P : Ω → R is a Pareto field over a convex set Ω ⊂ Rk if
P ∈ C1 is a continuously differentiable function such that NiP(r) > 0∀r ∈ Ω, ∀i = 1,...,k.
3
Under review as a conference paper at ICLR 2020
Figure 1: Example of achievable risk tradeoffs for a binary regression problem with two unbalanced
sensitive groups a = {0, 1} (Pa=1 > Pa=0). Bottom left and top left figures show conditional
distributions of observation variable pa(x), and target variable p(y = 1|x, a) respectively. p(y =
1|x, a) are simple piece-wise constant functions with levels ρlow and ρhigh. Middle figure shows
the Pareto front defined by subgroup risks; we observe that noise levels differ across subgroups and
perfect Pareto fairness is unattainable. Equality of Risk requires pure degradation of service for
group a = 1. Both Naive and Rebalanced Naive classifiers do not attain the best possible no-harm
classifier in this case. The proposed Pareto-fair point is shown in green, with its corresponding
optimal Equal risk classifier shown in purple. The utopic point (Ro0pt,Ro1pt) can only be attained
when the classifier also has access to the sensitive attribute (h(X, A)). Rightmost figure shows the
trade-offs attainable between discrimination and mean risks, an alternative perspective to the risk
trade-off figure.
Lemma 3.2. Let Ω ⊂ Rk be a convex set, and P : Ω → R a Pareto field. Then the set D = {r ∈
Ω : P (r) = 0} is a Pareto set in Ω, and the set D+ = {r ∈ Ω : P (r) > 0} is the set of dominated
points, i.e., D+ = {r ∈ Ω : ∃r0 ∈ D | r A r0}
Lemma 3.3. Let Ω ⊂ Rk be a convex set defined by Ω = {r ∈ Rk : gC(r) ≥ 0 ∀ C ∈
{1,..., C},gC continuously differentiable}; let P : Ω → R be a convex Pareto field with cor-
responding proper Pareto set D = {r ∈ Ω : P(r) = 0}. Let r ∈ D and φ(r, μ)=
Pk=I ri + μi(ri - c)+2, with c < r, where we denote (x)+2 = max2(x, 0). There exists a set
of vectors μ A 0 such that:
r = arg min φ(r, μ) s.t. : P (r) ≥ 0, gC (r) ≥ 0 ∀c ∈ {1,..., C}.
r∈Rk
Where C is the number of constraints that characterized Ω. Proofs for all lemmas are given in
the supplementary material, Section A.1. Lemma 3.3 motivates our loss function to take the form
φ(r, μ) = PiAI ri + μi (ri — c)+2, with |A| the number of sensitive groups. The challenge is to find
μ* such that our Pareto-fair vector r* minimizes φ(r, μ*). In Section 4 We provide an algorithm to
approximately recover the Pareto-fair classifier h* by adaptively searching for μ* and optimizing a
classifier on φ(∙, μ) using tools from standard Stochastic Gradient Descent.
4 Optimization Methods
Recall that we wish to recover the Pareto-fair classifier h* within our hypothesis class. From Lemma
3.3, there exists a loss function of the form
|A|
φ(r; μ,C) = Σ ri + μi (ri ―C)+2,	八、
i=1	(1)
φ(h; μ,c) = P Ra(h) + μa(Ra(h) — c)+2,
a∈A
such that h* = arg min φ(h; μ, c). Note that We can state the loss function directly on the risk
h∈H
vectors r, or implicitly on the classifier h. Since Ra (h) is differentiable with respect to h, φ(h; μ, c)
can be directly minimized using gradient descent on h for any choice of values μ, c. Let DTr =
4
Under review as a conference paper at ICLR 2020
{x, y, a}iN=T1, DVal = {x, y, a}iN=V1 be our training and validation datasets respectively. The proposed
implementation of the Pareto-fair framework is formalized in Algorithm 1 where we specify how to
update the penalty coefficients ~, c.
Algorithm 1: ParetoFairOptimization
Given： hθ,',DTr,DVa,nμ,np,nmaχ,γ > 0,k ≥ 1,ξ,Z ∈ (0,1),lr,B
μ J 1, μ* J 1, μcount J 0, ecount J 0, C J 0, Γ* — ∞, h* J hθ
while ecount ≤
nmax and μCount ≤ nμ do
μcount J- μcount + 1, ecount J- ecount + 1
hθ, rVαl J AdaptiveOptimize(hθ,', μ,c, DTr, DVal,np,lr, B)// Optimize current loss
// Check that solution is Pareto efficient and reduces fairness gap
if ∣∣~A(h)∣∣∞ < Γ* and r Val is not dominated by previous validation risks then
h* J hθ, Γ* J ||~A(h)∣∣∞, Cold J c, c J mina -f-
μ* J μ ∙ ^rV-cCld+ , a0 J argmaXa *, μcount J 0
else
I lr J Z lr, μ J μ*, γ J γξ, hθ J h*
end
μa0 J (1 + γ)μa0
end
// Exit loop due to excessive iterations or no improvement in fairness
Return: h*
We regularly check that reductions in the fairness gap generalize to the validation set; we addi-
tionally check if the trade-offs are in the non-dominated solution set (i.e., we have not observed a
universally better classifier during training). Algorithm 2 (AdaptiveOptimize, shown in Section A.2)
summarizes how we perform stochastic gradient descent steps with early stopping in-between μ, C
updates. Lemma A.5 (shown in supplementary material, Section A.1) shows that this algorithm is
convergent for |A| = 2 when the minimization step for a fixed μ, C is performed exactly.
To conclude this section, we stress that the proposed framework is independent of the desired al-
gorithm class H and loss function `; these are kept from the original application. The Pareto-fair
classifier uses the same inputs as the Naive classifier, with parameters that have been optimized
towards Pareto fairness. Code will be made available.
5	Experiments and Results
We applied the methodology described in Section 4 to learn a Pareto-fair classifier (classifier in
the group-risk Pareto front with the smallest risk disparity). We first validate our methodology on
synthetic data with known Pareto-fair classifiers. Observations are drawn from a Gaussian mixture
model where each sensitive attribute is encoded by a corresponding Gaussian mode, and target
attributes are binary. We demonstrate our methodology on publicly available fairness datasets, and
show how the risk disparity gaps are subsequently reduced. Where applicable, we compare our
results against the methodologies proposed in Zafar et al. (2017); Kamishima et al. (2012); Hardt
et al. (2016); Feldman et al. (2015).
5.1	Synthetic Data
We tested our approach on synthetic data where the observations are drawn from conditional Gaus-
sian distributions X|A = a 〜 N(μA, 1), the target variable y is a conditional Bernoulli variable
with distribution Y|X = x, A = a 〜Berfa(X) with fa(x) = Paow 1[x ≤ Ca] + ρhlgh1[x > c。]),
and |A| = 3. We used Brier Score (BS, refer to Section A.4) as our loss function. Under
these conditions, the Bayes-optimal classifier for each subgroup is h(x) = fa(x). The sub-
group risk function for any classifier h can be computed as Ra (h) = EX|A=a[Y-h(X))2] =
EX|A=a[fa(X)(1 - h(X))2 + (1 - fa(X))h(X)2]. Network details are given in the supplemen-
tary material, Section A.6. Figure 2 shows the analytically-derived Pareto-fair classifier, as well
as the trade-off obtained by the proposed algorithm. We remark that the Pareto-fair classifier in
this scenario cannot be achieved using linear logistic regression, something that can be seen from
the functional form of the Bayes Optimal classifier derived in Section A.8 in supplementary mate-
rial. From the center graph in Figure 2 we can see that the approximated Pareto-Fair risks (dashed
5
Under review as a conference paper at ICLR 2020
lines) are close to the theoretical best (solid lines).The achieved maximum risk discrimination is also
close to the theoretical best. The algorithm is able to correctly trade risks from the two advantaged
subgroups to reduce the risk of the worst performing subgroup.
Figure 2: Synthetic data experiment. Bottom left figure shows the conditional distribution of the observa-
tion variables for each of the three subgroups, while upper left shows the distribution of the target variable
conditioned on both the observation and sensitive attribute; it additionally shows the theoretical and empirical
Pareto-fair classifier. Center figure shows the empirical validation risks (dashed lines) as a function of epochs
during the optimization procedure, the theoretical Pareto-Fair risks are also shown (solid lines). Rightmost
shows how the maximum discrimination shrinks with the number of epochs.
5.2	Real Datasets
We evaluate the performace of our algorithm on mortality prediction (MIMIC-III), skin lesion clas-
sification (HAM10000), income prediction (Adult), and credit lending (German). We also compared
with several state of the art methods (Zafar et al. (2017); Kamishima et al. (2012); Hardt et al. (2016);
Feldman et al. (2015)) and with a model trained to minimize the average risk (Naive) and one that
undersamples the minority class (Naive Balanced or NaiveB). A description of these methods is
provided in Section A.3 of Supplementary material. Metrics used for evaluation include accuracy
(Acc), confidence (Conf), expected and maximum calibration error (ECE and MCE respectively),
Brier score (BS) and cross-entropy (CE). Values for every metric are reported per sensitive attribute.
The dataset average is reported as (sample mean), and the group-normalized average as (group
mean). The best and worst performing groups are also shown as (group max) and (group min), and
the worst case difference between subgroups is reported as (disc). For an in-depth description of
the the metrics and datasets used for evaluation, refer to Sections A.4 and A.5 of Supplementary
material. Here we present a comparison of the performance in terms of accuracy and ECE but an
extensive comparison is available in additional results Section A.7 in supplementary material.
Similarly to Chen et al. (2018); Zafar et al. (2017); Hardt et al. (2016); Woodworth et al. (2017), we
omit the sensitive attribute a ∈ A from our observation features. Our method trains a single classifier
for the entire dataset to avoid needing test-time access to sensitive attributes whenever possible. All
classifiers shown in this section are implemented using neural networks and/or logistic regression,
we used either cross-entropy or Brier score as our training loss depending on the dataset. For details
on the architecture and hyperparameters used on each dataset, refer to the supplementary material,
Section A.6. We show how the proposed Pareto-fair approach produces well calibrated models that
reduce group disparities across several metrics.
5.2.1	Predicting Mortality in Intensive Care Patients
Medical decisions in general and mortality prediction in particular are examples where notions of
fairness among sub-populations are of paramount importance, and where ethical considerations
make no-harm fairness a very attractive paradigm. To that end, we used clinical notes collected
from adult ICU patients at the Beth Israel Deaconess Medical Center (MIMIC-III dataset) Johnson
et al. (2016) to predict patient mortality. We study fairness with respect to age (adult or senior),
ethnicity (white or nonwhite), and outcome (alive/deceased). This leads to a total of 8 sensitive
groups. We included outcome (alive/deceased) as a sensitive sub-population criteria to demonstrate
a case where sensitive attributes would not be available at test-time, and because in our experiments
patients who ultimately passed away on ICU were under-served by a Naive classifier. Table 1 shows
empirical accuracies and expected calibration errors of all tested methodologies. It is important to
note how imbalanced these groups are by looking at the ’ratio’ column. Here one can see that 56.7%
of the samples correspond to the majority class (alive, white, senior) and only 0.4% to the minority
6
Under review as a conference paper at ICLR 2020
(a) ACCuraCy CompariSon
type	ratio	Naive	NaiveB	ZafarCE	ParetoFair CE	ParetoFair BS
Sample mean	-	89.5±0.2%	78.9±0.7%	78.0±0.7%	74.3±0.8%	75.7±1.1%
group mean	12.5%	61.8±1.5%	77.3±1.3%	69.4±1.3%	76.2±0.5%	77.5±0.7%
group max	56.7%	99.2±0.3%	88.1±1.3%	93.0±1.4%	86.4±2.2%	87.1±2.0%
group min	0.4%	18.1±2.5%	65.7±1.2%	43.7±3.9%	67.9±2.5%	69.9±2.2%
diSC	56.3%	81.1±2.5%	22.5±2.5%	49.3±4.4%	18.5±4.2%	17.1±2.6%
(b) ACCuraCy CompariSon after Hardt PoSt-ProCeSSing
type	ratio	NaiveB+H	Zafar+H	ParetoFair CE+H	ParetoFair BS+H
Sample mean	-	75.1±1.8%	66.3±1.0%	70.6±1.3%	71.9±1.2%
group mean	12.5%	71.2±1.1%	62.4±1.3%	70.8±0.7%	72.3±1.0%
group max	56.7%	80.0±2.5%	70.0±2.6%	77.6±4.1%	79.9±5.6%
group min	0.4%	61.4±3.4%	52.0±3.6%	65.7±1.8%	66.8±2.1%
diSC	56.3%	18.6±3.2%	18.1±4.3%	11.9±5.0%	13.1±5.6%
(C) ExpeCted Calibration error (ECE)
type	ratio	Naive	NaiveB	ZafarCE	ParetoFair CE	ParetoFair BS
Sample mean	-	0.115±0.004	0.037±0.003	0.219±0.007	0.062±0.021	0.052±0.012
group mean	12.5%	0.288±0.016	0.068±0.006	0.305±0.013	0.094±0.023	0.082±0.021
group max	56.7%	0.593±0.028	0.166±0.047	0.561±0.04	0.172±0.05	0.156±0.05
group min	0.4%	0.042±0.002	0.025±0.003	0.069±0.014	0.039±0.013	0.032±0.011
diSC	56.3%	0.552±0.028	0.141±0.045	0.492±0.045	0.133±0.043	0.125±0.044
Table 1: Performance comparison on MIMIC dataset. We show accuracies and expected calibration
error on teSt Set. Standard deviationS are Computed aCroSS 5 SplitS.
ClaSS (deCeaSed, nonwhite, adult). Our methodology produCeS low diSCrimination ClaSSifierS with
high group aCCuraCieS, for example, ParetoFair trained with brier SCore loSS (BS) inCreaSed the ClaS-
SifiCation aCCuraCy of the moSt under-Served group by over 50% while reduCing the aCCuraCy of the
beSt-Served group by 12% when Compared to the Naive ClaSSifier.
5.2.2	Skin Lesion Classification
The HAM10000 dataSet TSChandl et al. (2018) ColleCtS over 10, 000 dermatoSCopiC imageS of Skin
leSionS over a diverSe population. LeSionS are ClaSSified aCCording to diagnoStiC CategorieS. We
found that a Naive ClaSSifier exhibited almoSt no meaSurable diSCrimination baSed on age or raCe on
thiS dataSet. We inStead ChoSe to uSe the diagnoSiS ClaSS aS both the target and SenSitive variable,
CaSting balanCed riSk minimization aS a partiCular uSe-CaSe for Pareto fairneSS. ThiS iS poSSible SinCe
our methodology doeS not require teSt-time aCCeSS to SenSitive labelS. It waS not poSSible to Show
CompariSonS againSt Hardt et al. (2016) SinCe the SenSitive attribute iS perfeCtly prediCtive of the
outCome. Table 3 ShowS aCCuraCieS and ECE for all teSted methodologieS. The Pareto-fair ClaSSi-
fier haS the overall beSt Calibration reSultS and SmalleSt aCCuraCy diSparitieS aCroSS methodologieS.
CompariSonS againSt other methodS were not poSSible beCauSe the target labelS are non-binary.
GroupS	ratio	ParetoFair	NaiveB	Naive	ParetoFair	NaiveB	Naive
Sample mean	-	78.6%	84.8%	83.6%	0.055	0.065	0.095
group mean	14.3%	63.4%	64.1%	32.3%	0.206	0.223	0.416
group max	81.0%	83.5%	90.9%	96.0%	0.452	0.57	0.816
group min	0.5%	51.9%	33.3%	0.0%	0.025	0.034	0.032
diSC	80.4%	31.7%	57.5%	96.0%	0.426	0.535	0.784
(a) ACCuraCy	(b) ECE
Table 3: ACCuraCieS (left) and ExpeCted Calibration error (right) on teSt Set for HAM10000 dataSet.
5.2.3	Income Prediction and Credit Risk Assesment
We teSted the propoSed method on the Adult UCI dataSet Dua & Graff (2017a) and on the Ger-
man Credit dataSet Dua & Graff (2017b). In the Adult UCI dataSet the goal iS to prediCt a perSon’S
7
Under review as a conference paper at ICLR 2020
(a) Adult gender comparison							
type	ratio	Naive	NaiveB	Feldman	KamiShima	Zafar	ParetoFair
Male	67.9%	92.3±0.4%	92.3±0.3%	92.4±0.3%	92.6±0.4%	92.2±0.7%	92.4±0.3%
Female	32.1%	80.5±0.4%	80.3±0.7%	80.8±0.3%	80.9±0.4%	80.8±0.1%	81.0±0.5%
sample mean	-	84.3±0.3%	84.2±0.5%	84.5±0.3%	84.6±0.3%	84.4±0.3%	84.7±0.3%
group mean	50.0%	86.4±0.2%	86.3±0.4%	86.6±0.3%	86.7±0.2%	86.5±0.4%	86.7±0.2%
disc	35.9%	11.9±0.7%	12.0±0.7%	11.6±0.2%	11.7±0.7%	11.4±0.7%	11.4±0.6%
	(b) Adult ethnicity and gender CompariSon								
type	ratio	Naive	NaiveB	Feldman	KamiShima	Zafar	ParetoFair
White male	39.7%	90.8±0.3%	90.8±0.3%	90.6±0.3%	90.4±1.1%	90.5±0.6%	90.7±0.3%
Other	60.3%	79.8±0.5%	79.7±0.7%	80.4±0.6%	79.4±2.3%	80.1±0.1%	80.6±0.6%
Sample mean	-	84.1±0.2%	84.1±0.4%	84.5±0.3%	83.8±1.8%	84.2±0.2%	84.6±0.4%
group mean	50.0%	85.3±0.1%	85.2±0.3%	85.5±0.3%	84.9±1.6%	85.3±0.3%	85.6±0.4%
diSc	20.5%	11.0±0.8%	11.1±0.8%	10.2±0.7%	11.0±1.4%	10.3±0.6%	10.2±0.6%
(c) German compariSon							
type	ratio	Naive	NaiveB	Feldman	KamiShima	Zafar	ParetoFair
Female	29.5%	64.4±6.3%	64.1±7.5%	74.2±7.7%	68.8±6.8%	72.7±6.1%	65.4±5.1%
Male	70.5%	68.9±5.0%	70.4±5.3%	72.0±3.7%	72.7±2.6%	71.6±3.9%	68.7±6.0%
Sample mean	-	67.6±4.5%	68.6±4.5%	72.7±4.1%	71.6±3.2%	71.9±3.2%	67.0±4.4%
group mean	50.0%	66.6±4.7%	67.3±4.8%	73.1±4.9%	70.8±4.0%	72.1±3.5%	67.5±5.1%
diSc	41.0%	7.6±2.0%	10.4±3.2%	6.6±3.3%	6.0±4.4%	6.3±4.3%	5.3±2.8%
Table 5: Performance comparison between methods for Adult and German datasets. We show
accuracies on test set. Standard deviations are computed across 5 splits.
income, which can be an important factor on meaningful decisions such as credit lending. In the
German Credit dataset the goal is predicting credit risk. We select gender and ethnicity as our sen-
sitive attributes. To compare ourselves against state of the art methods Zafar et al. (2017); Feldman
et al. (2015); Kamishima et al. (2012) we binarize the sensitive attributes into White-Male and Other
when dealing with ethnicity and gender simultaneously, or Male and Female when dealing with gen-
der, and use the unified testbed provided in Friedler et al. (2019). We limit our hypothesis class H
to linear logistic regression to compare evenly against these standard baselines. Results on both
datasets are shown in Table 5.
6	Discussion
There exists a rich literature of fairness in machine learning in general, and risk-based fairness in
particular. Here we explore a relatively untapped sub-problem where the goal is to reduce risk
disparity gaps in the most ethical way possible (i.e., minimizing unnecessary harm). Unlike other
works in the area, our problem investigates on how to reduce this disparity gap without collecting
additional data samples, using the entirety of the available training data to produce an algorithm that
is maximally fair with respect to sub-populations, and that does not necessarily require test-time
access to sensitive attributes.
We provide a concrete algorithmic adaptation to any standard classification or regression loss to
bridge this disparity gap at no unnecessary harm, and demonstrate its performance on several real-
world case studies. Even for applications where the need for strict fairness outweighs the need for
no-harm classifiers, this methodology can be applied before any post-hoc corrections to ensure that
the risk disparity gap is closed in the most risk-efficient way for all involved sub-populations. The
proposed algorithm does not sweep through different disparity constraint values, as previously done
in related works, making it a simpler alternative.
As an avenue of future research, it could be of interest to analyze if we can automatically identify
high-risk sub-populations as part of the learning process and attack risk disparities as they arise,
rather than relying on preexisting notions of disadvantaged groups or populations. We strongly
believe that no-unnecessary-harm notions of fairness are of great interest for several applications,
especially so on domains such as healthcare and lending, where decisions are highly impactful.
8
Under review as a conference paper at ICLR 2020
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reduc-
tions approach to fair classification. arXiv preprint arXiv:1803.02453, 2018.
Solon Barocas and Andrew D Selbst. Big data’s disparate impact. Calif. L. Rev., 104:671, 2016.
Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification.
Data Mining and Knowledge Discovery, 21(2):277-292, 2010.
Irene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? In
Advances in Neural Information Processing Systems, pp. 3539-3550, 2018.
Elliot Creager, David Madras, Jorn-Henrik Jacobsen, Marissa A Weis, Kevin Swersky, Toniann
Pitassi, and Richard Zemel. Flexibly fair representation learning by disentanglement. arXiv
preprint arXiv:1906.02589, 2019.
Pedro Domingos. A unified bias-variance decomposition. In Proceedings of 17th International
Conference on Machine Learning, pp. 231-238, 2000.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017a. URL http://archive.
ics.uci.edu/ml.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017b. URL https://
archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data).
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214-226. ACM, 2012.
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 259-268. ACM, 2015.
Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P
Hamilton, and Derek Roth. A comparative study of fairness-enhancing interventions in machine
learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pp.
329-338. ACM, 2019.
Arthur M Geoffrion. Proper efficiency and the theory of vector maximization. Journal of mathe-
matical analysis and applications, 22(3):618-630, 1968.
Moritz Hardt, Eric Price, Nathan Srebro, et al. Equality of opportunity in supervised learning. In
Advances in neural information processing systems, pp. 3315-3323, 2016.
Tatsunori B Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. arXiv preprint arXiv:1806.08010, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii,
a freely accessible critical care database. Scientific data, 3:160035, 2016.
Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Rawlsian fairness
for machine learning. arXiv preprint arXiv:1610.09559, 1(2), 2016.
Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier
with prejudice remover regularizer. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 35-50. Springer, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
9
Under review as a conference paper at ICLR 2020
Juhani Koski. Defectiveness of weighting method in multicriterion optimization of structures. Com-
munications in applied numerical methods, 1(6):333-337, 1985.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair
autoencoder. arXiv preprint arXiv:1511.00830, 2015.
Andreu Mas-Colell, Michael Dennis Whinston, Jerry R Green, et al. Microeconomic theory, vol-
ume 1. Oxford university press New York, 1995.
Kaisa Miettinen. Nonlinear multiobjective optimization, volume 12. Springer Science & Business
Media, 2012.
Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection
of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5:
180161, 2018.
Berk Ustun, Yang Liu, and David Parkes. Fairness without harm: Decoupled classifiers with prefer-
ence guarantees. In International Conference on Machine Learning, pp. 6373-6382, 2019.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-
discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classification. arXiv preprint arXiv:1507.05259, 2015.
Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, and Adrian Weller.
From parity to preference-based notions of fairness in classification. In Advances in Neural In-
formation Processing Systems, pp. 229-239, 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
10
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Proofs
Here we restate the Lemmas shown in Section 3 along with a sketch of the proofs.
Lemma 3.1 Ifh 6∈ P(H,A) → ∃hp	h ∈ P(H,A) : Ra(hpER) ≤ Ra (hER), ∀a, where hER
is a equality of risks classifier in H such that Ra(hER) = max Ra0 (h), ∀a and hER : Ra(hER) =
a0∈A a	p	p
max Ra0 (hp), ∀a.
a0∈A
Proof. If hp dominates h → Ra(h'R) = max R。，(hp) ≤ max R.0(h) = Ra(hER) ∀a ∈ A.	□
a0∈A	a0∈A
Definition A.1. Dominant vector: A vector r0 ∈ Rk is said to dominate r ∈ Rk, noted as r0 r, if
ri ≥ ri0, ∀i = 1, ..., k and ∃j : rj > rj0 (i.e., strict inequality on at least one component).
Definition A.2. Pareto field: A function P : Ω → R is defined as a Paretofield over a convex set
Ω ⊂ Rk if P ∈ C1 is a continuously differentiable function such that ViP(r) > 0 ∀r ∈ Ω, ∀i =
1,...,k.
Lemma A.1. Let Ω ⊂ Rk be a convex set, and P : Ω → R a Pareto field. Then the set D = {r ∈
Ω : P(r) = 0} is a proper Pareto set in Ω, and the set D+ = {r ∈ Ω : P(r) > 0} is the set of
dominated points, i.e., D+ = {r ∈ Ω : ∃r0 ∈ D | r * r0}.
Proof First We prove that D = {r ∈ Ω : P(r) = 0} is a Pareto set. Assume by contradiction
that there exists r, r0 ∈ D | r0 * r, then r0 - r = c = Pi δiei δi ≥ 0∀i ∈ {1, . . . , k}, ∃j ∈
{1, . . . , k } | δj > 0. ei is a standard basis vector.
Using the Gradient theorem We have
P (r0) = P(r) +	VP (λ)dλ,
= P(r) + Z h(δ1, . . . ,δk), (VP0(r + λc), . . . , VPk(r + λc))idλ,
0
P(r) +
X δi /1 yPi(r + λc)dλ,
^{z
>0
> P (r),
Which directly contradicts r0 , r ∈ D. No point in set D is dominated by another point in set D,
making set D a proper Pareto set.
To show that the set D+ is the set of dominated points, we first note that forall r ∈ Ω | r0 ∈ D, r *
r0 We have P(r) > P (r0) using the same arguments as above, meaning that the set of dominated
points is included in D+. Similarly, for all r ∈ Ω | r0 ∈ D, r0 占 r we have P(r0) ≥ P(r),
meaning that non-dominated points are not a part of set D+.	□
Lemma A.2. Let Ω ⊂ R1A1 be a convex set defined by Ω = {r ∈ R1A1 : gc(r) ≥ 0∀C ∈
{1,...,C},gc continuously differentiable}; let P : Ω → R be a convex Pareto field with corre-
sponding Pareto set D = {r ∈ Ω : P(r) = 0}. Let r* ∈ D and φ(r, μ) = PiAl r + μi(r 一 c)2+,
with c < r*. There exists a set of μ* * 0 such that:
r* = arg min φ(r, μ*) s.t. : P(r) ≥ 0,gc(r) ≥ 0 ∀c ∈ {1,...,C}
r∈RlAl
Proof. By hypothesis, both Ω and {r ∈ Ω : P(r) ≥ 0} are convex, so the intersection is also
convex. For μ * 0, φ(r, μ) is a convex function of r. Under these hypothesis, the Karush-Kuhn-
Tucker (KKT) conditions are necessary and sufficient to recover a global minimizer. So it suffices
to find a set μ* such that the KKT conditions are exactly satisfied at r*.
11
Under review as a conference paper at ICLR 2020
Let J* = {j : gj(r*) = 0} be the set of indices of active gc constraints at r*, note that by
hypothesis, r* ∈ D and therefore P(r*) is always active. Focusing on the first KKT condition on
the active set we get Vφ(r*, ~) = λVP(r*) + Pj∈j* η Vgj(r*), in matrix form:
■11	∣^2(r* - c)+	0	1	「I |
.+	...	μ = VP VgjI
1	0	2(rA* - c)+	|	|
λ
I ]	ηι
VgjJ*∣	.
I J .
ηc*
By hypothesis, we have ri - c > 0 and therefore
μi
λVPi(r*) + Pj∈j* ηjVgj(r*) - 1
2(r* — c)
In particular, for ηj = 0 ∀j and μ1 > [ VPi(r*) — 1] 2(r*-c)+ ∀i 6= 1 we get
μi=[(μ12(r*—C)++I) "Pi[*)—1]2(r*—c)+ > 0∀i=j,
μι > 0,
μi2(r* - c)+ + 1
VPι(r*)
> 0,
which satisfies the KKT conditions and produces the required minimizer r* for all points r * ∈ D.
□
Lemma A.3. Let Ω ⊂ R2 be a convex set defined by Ω = {r ∈ R2 : gc(r) ≥ 0∀C ∈
{1,..., C}, gc continuously differentiable}; let P : Ω → R be a ConVex Pareto field with corre-
sponding Pareto set D = {r ∈ Ω : P(r) = 0}. Let e > 0, r* = arg min[maxi ri — minj rj]
r∈D
then
C = min(r1, r2) — < min(r1*, r2*) ∀(r1, r2) ∈ D.
Proof. case r1* = r2* : Assume r ∈ D : mini ri > r1*. Then r r*, which contradicts the
hypothesis r ∈ D.
case r1* 6= r2*: Without loss of generality, we can take r1* > r2*. We can then prove by couterpositive
that r ∈ D implies r1 > r2. Assume by contrapositive that there exists r0 ∈ D : r20 > r10 . By
hypothesis D is a continuous curve, and We can find a path R(t) : R(0) = r*, R(1) = r0, R(t) ∈
D∀t ∈ [0,1], which would imply there exists t : RG) = (r, r), in which case r* = r2, which
contradicts r1* 6= r2* .
We then have that for r ∈ D, r1 > r2. Assume by counterpositive that r2 = min(r1, r2) >
min(r1*, r2*) = r2* then r1 < r1* (since both r, r* ∈ D), but then Ir1 — r2 I < Ir1* — r2* I which
contradicts the hypothesis.
Putting both statements together, we proved that if r1* > r2*, then ∀r ∈ D, min(r1, r2) = r2 ≤
min(r1*, r2*) = r2*.
□
Lemma A.4. Let Ω ⊂ R2 be a convex set defined by Ω = {r ∈ R2 : gc(r) ≥ 0 ∀ C ∈
{1,..., C}, gc continuously differentiable}; let P : Ω → R be a convex Pareto field with corre-
sponding Pareto set D = {r ∈ Ω : P(r) = 0}. Let μ = (μ1,μ2) > 0, μ0 = (μι, μ2); μ2 > μ2.
Define φ(r,μ, c) = P2=ι ri + μi(ri - c)2+ and
r = arg min φ(r, μ, c),
s. t. P (r)≥0
r∈Ω
r0 = arg min φ(r, μ0, C).
s. t. P (r)≥0
r∈Ω
With c < r2, then, rɪ ≥ ri, r2 ≤ r, with equality ifand only if r0 = r = arg min r
s. t. r∈{P(r)≥0}∩Ω
12
Under review as a conference paper at ICLR 2020
Proof. By hypothesis, we have
φ(r,μ0,c) ≥ φ(r0,μ0,c)
φ(r,μ, c) + (μ2 - μ2)(r2 - c)2+ ≥ φ(r,μ,c) + (μ2 - μ2)(r2 - c)2+
φ(r, μ, C) ≥ φ(r, μ, C) + (μ2 - μ2)[(r2 - C)2+ - (r - C)2+]
since by hypothesis φ(r, μ, C) ≤ φ(r0, μ, c), it follows r2 ≤ r2, and therefore r1 ≥ r1, because they
both belong to the Pareto set.
Case r0 = r. To analyze when the equality arises, note that the tangent to D at r is τ (r) =
(▽2P(r), -VιP(r)) and that r + δτ(r), δ > 0 is a valid search direction if r = r (r does not
minimize r2). Also note that Vφ(r,μ0,C) = Vφ(r,μ, c) + (0, 2(μ2 - μ2)(r2 - c)+)
By contradiction, assume r = r, then r + δτ(r), δ > 0 is a valid search direction and
h(δτ(r)), Vφ(r, μ, C > 0, which implies r0 = arg min φ(r, μ0,C) and contradicts the hypoth-
s. t. P (r)≥0
r∈Ω
esis.
Therefore, if r0 = r it follows r0 = r = r
□
Lemma A.5. Let Ω ⊂ R1A1 be a convex set defined by Ω = {r ∈ R1A1 : gc(r) ≥ 0 ∀ C ∈
{1,..., C}, gc continuously differentiable}; let P : Ω → R be a convex Pareto field with corre-
sponding Pareto set D = {r ∈ Ω : P(r) = 0}. Let r* = arg min(maxi r 一 min7- r7-) and
r∈D
φ(r, μ, c) = piA1 r + μi(r% — c)2+. Let μ0 > 0 be an initial penalty vector, r0 = arg min Pi r,
r∈D
> 0, C0 = mini ri0 - ,γ > 0, ξ ∈ (0, 1). Define the following auxiliary variables
Ck+1 = min rik -
i
Γk = max rik - min rik
ii
a* = arg max(rk)
a
μ∖∖ = (Y + 1屋
μ∖at = μkat	⑵
r* = arg min φ(r, μ*,Ck+1)
s. t. P (r)≥0
r ∈Ω
C* = min ri* -
ii
Γ* = max ri* - min r*
ii jj
We define the following iteration procedure
compute Eq 2
If Γk = Γ*
terminate
While Γ* > Γk
Y — ξY
Recompute Eq 2
End While
μk+1
μ* ∙
(r* 一 Ck)+
(r* — c*)+
rk+1 = r*
The procedure is convergent to r* for |A| = 2.
13
Under review as a conference paper at ICLR 2020
Proof. From Lemma A.3 We have Ck < mini W ∀k, We can then apply Lemma A.2 and state that
there exists a set μ*k such that r* = arg min φ(r, μ*k, Ck).
s. t. P (r)≥0
r∈Ω
We split the proof in two scenarios, one where Γ* = 0 and then Γ* > 0, since they show different
convergence properties.
case Γ* = 0. Assume without loss of generality that rk = argmaxg ri, from Lemma A.4 we have
μk < μ1*, and further, for all μ[ ∈ (μi, μi*), we have ri ‹ rɪ ‹ rɪ. Since |A| = 2 and rk, r^,r*
all belong to D, we also have rk > r22 > r2, which implies Γk > Γ* > Γ* = 0. The subiteration
procedure is always successful, and we get a sequence of iterates such that Γk > Γk+1, a strictly
decreasing sequence bounded below by 0, which implies asymptotic convergence to r*. Procedure
never terminates with probability 1 (event μk ∈ μk has probability 0).
case Γ* > 0 Assume without loss of generality that r1* < r2*. Following the arguments from Lemma
k	μk + 1
A.3 we observe that ri < r ∀(rι, r2) ∈ D. In these conditions, we have that μ⅛ < μ2k+r and
μι	μι
Ck+1 > Ck. We will re-derive the KKT conditions in this scenario to show that convergence is
guaranteed for μk > μ2*, and that this μ2* is independent of Ck, meaning that convergence to the
optimal r* can be achieved in a finite number of steps.
Let VP(r*) = (dP1,dP2) > 0, with corresponding tangent vector τ* = (dP2, —dPi), for r*
to be the risk with minimal gap, r* + δτ * must be an infeasible descent direction for δ > 0.
Therefore, there is an active constraint from Ω, g(r), with Vg(r*) = (dg1,dg2) such that
h(dgi,dg2),(dP2,—dPi)i <0.
Deriving the first KKT conditions, we get
1 + μι2(r* — C) = λdP1 + ηdgι,
1 + μ22(r* — C) = λdP2 + ηdg2,
and in terms of λ, η
1 + μι2(r1 - C) - ηdgι
λ =-----------------------
η
dP1
dPι(1 + μz2(rg — C) — dP2(l + μι2(r ɪ — C))
dg2dP1 - dP2dg1
Note that dg2dPi — dP2dgi > 0 from h(dgi, dg2), (dP2, —dPi)i < 0. To recover a valid set of
Lagrange multipliers η, λ > 0 , it suffices to have
dPi(1 + μ22(r* — c)) > dP2(l + μι2(r* — c)),
μ2 >
μ2 >
(I + μi2(r* - C)) dp2 — 1
2(r* - C)	,
(I + μι2(r* 一 CO))dP2 — 1
2e
where we used that c0 < Ck < e ∀k. Therefore, for μ⅛ large enough, rk = r* and the algorithm
μi	J
terminates.	□
A.2 Algorithmic details
Here we provide details on how we optimize our adaptive loss in-between penalty update (μ0) steps,
shown in Algorithm 2.
A.3 Methods
We compare the performance of the following methods:
14
Under review as a conference paper at ICLR 2020
Algorithm 2: AdaptiveOptimize
Given: hθ, L, ~, c, DTr, DVal,np, lr, B
P — 0, φ* — ∞ h* — hθ
while p ≤ np	// Making loss progress
do
for {(xi, yi, ai}iB=1 ∈ DTr	// Run one epoch of SGD on training set
do
Ia = {i ∈{1,...,B}∧ ai = a}, RB(hθ )=“ P L(hθ (xi),yi) // Empirical risks
a i∈Ia
ΦB(hθ ) = Pa∈A RB(hθ ) + μa(R (hθ) - c)+2
θ 一 θ — lrVθφB(hθ)	// Gradient step
end
if φVal(hθ) < φ*	// Evaluate improvement on Val and update target risks
then
h* — he； φ* — φVαl (hθ); p — 0
else
P — P + 1；
end
RVaI _ RVal (h)
end
Return: h*,RVal
Kamishima. Kamishima et al. (2012) uses logistic regression as a baseline classifier, and requires
numerical input (observations), and binary sensitive attribute and target variable. Fairness is con-
trolled via a regularization term with a tuning parameter η that controls trade-off between fairness
and overall accuracy. η is optimized via grid search with η ∈ (0, 300) as in the original paper.
We report results on the hyperparameter configuration that produces the smallest accuracy disparity
between sensitive subgroups.
Feldman. Feldman et al. (2015) provides a preprocessing algorithm to sanitize input observations.
It modifies each input attribute so that the marginal distribution of each coordinate is independent
on the sensitive attribute. The degree to which these marginal distributions match is controlled by
a λ parameter between 0 and 1. It can handle numerical and categorical observations, as well as
non-binary sensitive attributes, and arbitrary target variables. Following Friedler et al. (2019), we
train a linear logistic regressor on top of the sanitized attributes. λ is optimized via grid search with
increments of 0.05. We report results on the hyperparameter configuration that produces the smallest
accuracy disparity between sensitive subgroups.
Zafar. Zafar et al. (2017) Addresses disparate mistreatment via a convex relaxation. Specifically,
in the implementation provided in Friedler et al. (2019), they train a logistic regression classifier
with a fairness constraint that minimizes the covariance between the sensitive attribute and the clas-
sifier decision boundary. This algorithm can handle categorical sensitive attributes and binary target
variables, and numerical observations. The maximum admissible covariance is handled by a hyper-
parameter c, tuned by logarithmic gridsearch with values between 0.001 and 1. We report results
on the hyperparameter configuration that produces the smallest accuracy disparity between sensitive
subgroups.
Hardt. Hardt et al. (2016) proposes a post-processing algorithm that takes in an arbitrary predictor
and the sensitive attribute as input, and produces a new, fair predictor that satisfies equalized odds.
This algorithm can handle binary target variables, an arbitrary number of sensitive attributes, and
any baseline predictor, but requires test-time access to sensitive attributes. it does not contain any
tuning parameter. We apply this method on top of both the Naive Classifier and our Pareto Fair
classifier.
Naive Classifier (Naive). Standard classifier, trained to minimize an expected risk h =
arg min EX,A,Y [L(h(X), Y )]. The baseline classifier class H is implemented as a neural network
h∈H
and varies by experiment as described in Section A.6, the loss function also varies by experiment
and is also described in Section A.6. Optimization is done via stochastic gradient descent.
15
Under review as a conference paper at ICLR 2020
Naive Balanced (NaiveB). Baseline classifier designed to address undersampling
of minority classes, trained to mimimize a class-rebalanced expected risk h =
arg min EA〜U[i,...,∣a∣],(x,y)〜p(x,y∣a) [L(h(X), Y)].	Like the Naive classifier, it is imple-
h∈H
mented as a neural network and optimized via stochastic gradient descent. The sole difference
with the Naive classifier is that, during training, training samples are drawn from the new input
distribution A 〜 U[1,..., |A|]; X,Y|A 〜 P(X, Y|A), which is achieved by re-weighted
sampling of the original training dataset.
Pareto Fair. Our proposed methodology, trained to minimize an adaptive loss function using the
procedure described in Algorithm 1. Addresses risk disparity minimization without introducing
unnecessary harm to any subgroup. The baseline classifier class H is implemented as a neural
network and varies by experiment as described in Section A.6, the loss function also varies by
experiment and is also described in Section A.6.
A.4 Evaluation Metrics
Here we describe the metrics used to evaluate the performance of all tested methods. We are given a
set of test samples Dt = {(xi, yi)}iN=1 where xi ∈ X is a realization of our model input and yi ∈ Y
the corresponding objective. We assume that Y is a finite alphabet, as in a classification problem,
and we will represent the one-hot encoding of yi as ~ei. Given a trained model h : X → [0, 1]|Y| the
predicted output for a an input xi is a vector h(xi) = p~i such that (p~i)j ∈ [0, 1], ∀j ∈ {1, ..., |Y|}
and | Pj=ι 3)j = 1 (e.g.: output of a softmax layer). The predicted class is y^i = argmaxj (~)j
and its associated confidence is Pi = maxj (p~j. Ideally y^i should be the same as y》Using these
definitions, we compute the following metrics.
Accuracy (AC):需 PN=I 1(yi = yi). Fraction of correct classifications in dataset.
Confidence (CO): N PN=I pi. Average magnitude of the predicted class probability.
Brier Score (BS):需 PN=I ||ei - ~i||2 where ~ is the one-hot representation of the categorical
ground truth value yi. This quantity is also known as Mean square error (MSE).
Cross-Entropy (CE): -N PN=I PjY1(ei)jlog(~i)j also known as negative log-likelihood (NLL)
of the multinomial distribution.
Expected Calibration Error (ECE): N PM=J Pii∈Bm [1(yi = yi) - ~i] ∣ where M is the number
of bins to divide the interval [0,1] such that Bm = {i ∈ {1,.., N} : ~ ∈ (mɪ-ɪ, M ]} are the group
of samples that our model assigns a confidence Si) in the interval (m-1, M]. Measures how closely
the predicted probabilities match the true base rates.
Maximum Calibration Error (MCE): maxm∈{i,...,M }∣	Pi∈Bw, [1(yi = yi) - ~i]∣. Measures
worst-case miscalibration errors.
These metrics are computed independently for each sensitive subgroup on the test set and reported
in Section A.7.
A.5 Experiments on real data
The following is a description of the data and experiments for each of the real datasets. The infor-
mation present here is summarized in Table 7.
16
Under review as a conference paper at ICLR 2020
Dataset	Outcome/ Objective	Sensitive Attribute	Train/Val/Test	splits
Adult	Income	Gender(F/M)		
Dua & Graff (2017a)	2 categories	Ethnicity(W/NW) 2x2 categories	60/20/20	5
German	Credit	Gender (F/M)	60/20/20	5
Dua & Graff (2017a)	2 categories	2 categories		
MIMIC-III Johnson et al. (2016)	Mortality (A/D) 2 categories	Mortality(A/D), Age (A/S) Ethnicity (W/NW) 2x2x2 categories	60/20/20	5
HAM10000	Type of lesion	Type of lesion	80/10/10	1
Tschandl et al. (2018)	7 categories	7 categories		
Table 7: Basic characteristics of real datasets
MIMIC-III. This dataset consist of clinical records collected from adult ICU patients at the Beth
Israel Deaconess Medical Center (MIMIC-III dataset) Johnson et al. (2016). The goal is predicting
patient mortality from clinical notes. We follow the pre-processing methodology outlined in Chen
et al. (2018), where we analyze clinical notes acquired during the first 48 hours of ICU admission;
discharge notes where excluded, as where ICU stays under 48 hours. Tf-idf statistics on the 10, 000
most frequent words in clinical notes are taken as input features.
We identify 8 sensitive groups as the combination of age (under/over 55 years old), ethnicity as
determined by the majority group (white/nonwhite) and outcome (alive/deceased). Here we will use
the term adult to refer to people under 55 years old and senior otherwise. This dataset shows large
sample disparities since 56.7% of corresponds to the overall majority group (alive-senior-white) and
only 0.4% to the overall minority group (deceased-adult-nonwhite).
We used a fully connected neural network as described in table 8 as the baseline classifier for our
proposed Pareto Fair algorithm. We compare our results against both the Naive and Naive Balanced
algorithms using the same neural network architecture, and crossentropy (CE) as our training loss.
We also evaluate the performance of Zafar applied on the feature embeddings learned by the Naive
Balanced classifier (Results for Zafar over the original input features were not promising).
We report the performance across a 5-fold split of the data, we used a 60/20/20 train-validation-test
partition as described on Table 7 and report results over the test set. We denote the overall sensitive
attribute as the combination of outcome (A:alive/D:deceased), age (A:adult/S: senior) and ethnic-
ity (W:white, NW:nonwhite) with shorthand notation of the form D/A/W to denote, for example
deceased, white adult. We also note that results on both Zafar and Hardt were done over only the
sensitive attributes Adult/Senior and White/Nonwhite, outcome was not considered as a sensitive
attribute for both methods since Hardt requires test-time access to sensitive attributes, which would
not be possible in this case, and Zafar attempts to decorrelate sensitive attributes and classification
decision boundaries, which is counterproductive when the sensitive attribute includes the correct
decision outcome.
HAM10000. This dataset contains over 10, 000 dermatoscopic images of skin lesions over a di-
verse population Tschandl et al. (2018). Lesions are classified in 7 diagnostic categories, and the
goal is to learn a model capable of identifying the category from the lesion image. The dataset is
highly unbalanced since 81% of the samples correspond to a melanocytic nevi lesion (nv), and 0.5%
to dermatofibroma (df).
Here we chose to use the diagnosis class as both the target and sensitive variable, casting balanced
risk minimization as a particular use-case for Pareto fairness.
We load a pre-trained DenseNet121 network Huang et al. (2017) and train it to classify skin lesions
from dermatoscopic images using our Pareto fairness algorithm. We compared against the Naive and
the Naive balanced training setup. Note that in the naive balance approach we use a batch sampler
where images from each class have the same probability, this can be seen as a naive oversampling
technique. Table 8 shows the details of the implementation.
17
Under review as a conference paper at ICLR 2020
We used the original train-validation-test (80/10/10) split, and report results on the test set. Nota-
tion for each group follows the original notation: Actinic keratoses and intraepithelial carcinoma /
Bowen’s disease (akiec), basal cell carcinoma (bcc), benign keratosis-like lesions (bkl), dermatofi-
broma (df), melanoma (mel), melanocytic nevi (nv) and vascular lesions (vasc).
Adult. The Adult UCI dataset Dua & Graff (2017a) is based on the 1994 U.S. Census and contains
data on 32, 561 adults. The data contains 105 binarized observations representing education status,
age, ethnicity, gender, and marital status and a target variable indicating income status (binary at-
tribute representing over or under $50, 000). Following Friedler et al. (2019), we take ethnicity and
gender as our target sensitive attributes, defining two subgroups (White Males and Other). We also
present results taking just the gender as sensitive attribute (Male/Female). To compare our Pareto
Fair algorithm evenly against the other methods, we limit our hypothesis class to linear logistic
regression.
German. The German credit dataset Dua & Graff (2017a) contains 20 observations collected
across 1000 individuals, and a binary target variable assessing the individual’s credit score as good
or bad. We take gender (Male/Female) as the sensitive attribute, which is not included in the data but
can be inferred. As in the Adult dataset, we limit our hypothesis class to linear logistic regression to
compare evenly across methodologies.
A.6 Neural Architectures and Parameters
Table 8 summarizes network architectures and loss functions for all experiments in Section 5. Note
that all networks have a standard dense softmax as their final layer. The training optimizer is standard
ADAM Kingma & Ba (2014), loss functions were either crossentropy (CE) or brier score (BS), also
known as categorical mean square error (MSE).
Dataset	Network Body	Gate	Loss type	Parameters
Synthetic	FullyConnected 64x64	ELU	BS	batch size=32 lr=5e-4
Adult - German Dua & Graff (2017a)	Logistic Regression (LR)	-	CE	batch size=32 lr=5e-4
MIMIC-III Johnson et al. (2016)	FullyConnected 2048x2048	ELU	CE/BS	batch size=512 lr=1e-6/5e-6
HAM10000 Tschandl et al. (2018)	DenseNet121 Huang et al. (2017)	ReLU	BS	batch size=32 lr=5e-6
Table 8: Summary of network architectures and losses. All networks have a final softmax dense layer as the
output layer.. CE refers to crossentropy and BS to Brier score or categorical MSE. Training was done using
ADAM optimizer with the learning rates (lr) specified in the table.
A.7 Supplementary Results
The following tables show a quantitative performance comparisons between the various fairness
methods described in Section A.3 over the datasets described in Section A.5. All metrics presented
are described in Section A.4 and computed per sensitive attribute. We additionally provide results
for the overall mean across the dataset (sample mean), as well as results on the group-balanced
mean across groups (group mean). We use group max and group min to denote the maximum and
minimum metric values attained over all sensitive groups. We denote group max - group min as
Discrimination, which is the worst case difference in performance across groups. When the dataset
contains multiple splits, we report both split mean and split standard deviation.
18
Under review as a conference paper at ICLR 2020
type	ratio	Naive	NaiveB	ZafarCE	ParetoFair CE	ParetoFair BS
A/A/NW	5.7%	99.1±0.4%	86.3±1.5%	93.0±1.4%	80.4±2.8%	83.4±2.6%
A/A/W	13.3%	98.8±0.5%	86.3±1.1%	90.0±1.3%	81.0±1.5%	83.2±1.5%
A/S/NW	12.9%	97.5±0.6%	76.5±1.7%	81.8±1.7%	69.5±2.2%	71.4±3.0%
A/S/W	56.7%	97.9±0.3%	79.0±0.6%	77.4±0.7%	73.5±1.3%	74.6±1.6%
D/A/NW	0.4%	23.4±9.4%	76.1±8.5%	47.7±9.6%	76.6±5.3%	78.6±6.1%
D/A/W	0.9%	32.6±3.7%	80.1±3.3%	60.5±6.3%	85.0±3.8%	83.3±4.2%
D/S/NW	1.8%	21.4±2.2%	66.9±2.4%	48.2±2.0%	72.6±5.1%	73.3±2.9%
D/S/W	8.3%	23.4±2.2%	67.4±1.9%	57.1±2.2%	71.0±4.2%	72.5±3.6%
sample mean	-	89.5±0.2%	78.9±0.7%	78.0±0.7%	74.3±0.8%	75.7±1.1%
group mean	12.5%	61.8±1.5%	77.3±1.3%	69.4±1.3%	76.2±0.5%	77.5±0.7%
group max	56.7%	99.2±0.3%	88.1±1.3%	93.0±1.4%	86.4±2.2%	87.1±2.0%
group min	0.4%	18.1±2.5%	65.7±1.2%	43.7±3.9%	67.9±2.5%	69.9±2.2%
disc	56.3%	81.1±2.5%	22.5±2.5%	49.3±4.4%	18.5±4.2%	17.1±2.6%
type	ratio	NaiveB H	ZafarCE H	ParetoFair CE H	ParetoFair BS H
A/A/NW	5.7%	76.3±1.9%	68.4±2.2%	69.9±2.5%	71.6±3.2%
A/A/W	13.3%	76.7±1.6%	67.8±1.3%	70.8±2.4%	71.8±1.9%
A/S/NW	12.9%	76.4±2.2%	67.9±2.1%	70.1±2.5%	71.3±3.1%
A/S/W	56.7%	76.2±2.2%	67.6±1.1%	70.6±1.2%	72.1±1.2%
D/A/NW	0.4%	66.6±9.9%	59.7±8.9%	71.2±7.5%	74.1±9.2%
D/A/W	0.9%	66.4±2.4%	57.0±5.0%	72.9±3.8%	73.6±4.3%
D/S/NW	1.8%	64.8±2.1%	55.9±3.1%	70.2±4.3%	71.2±2.9%
D/S/W	8.3%	66.2±2.9%	55.3±1.9%	70.9±3.8%	72.4±3.5%
sample mean	-	75.1±1.8%	66.3±1.0%	70.6±1.3%	71.9±1.2%
group mean	12.5%	71.2±1.1%	62.4±1.3%	70.8±0.7%	72.3±1.0%
group max	56.7%	80.0±2.5%	70.0±2.6%	77.6±4.1%	79.9±5.6%
group min	0.4%	61.4±3.4%	52.0±3.6%	65.7±1.8%	66.8±2.1%
disc	56.3%	18.6±3.2%	18.1±4.3%	11.9±5.0%	13.1±5.6%
Table 9: Accuracies on test set for MIMIC dataset. Standard deviations are computed across 5 splits.
type	ratio	Naive	NaiveB	ZafarCE	ParetoFair CE	ParetoFair BS
A/A/NW	5.7%	0.027±0.004	0.198±0.019	0.138±0.029	0.291±0.04	0.252±0.033
A/A/W	13.3%	0.027±0.005	0.196±0.013	0.2±0.026	0.285±0.022	0.247±0.019
A/S/NW	12.9%	0.055±0.007	0.311±0.017	0.364±0.036	0.383±0.018	0.363±0.029
A/S/W	56.7%	0.049±0.003	0.283±0.005	0.449±0.014	0.355±0.014	0.338±0.017
D/A/NW	0.4%	1.007±0.133	0.362±0.055	1.042±0.189	0.335±0.03	0.34±0.041
D/A/W	0.9%	0.922±0.068	0.292±0.041	0.785±0.121	0.287±0.022	0.277±0.031
D/S/NW	1.8%	1.085±0.019	0.417±0.018	1.032±0.041	0.371±0.033	0.365±0.03
D/S/W	8.3%	1.043±0.021	0.419±0.023	0.857±0.044	0.379±0.032	0.367±0.035
sample mean	-	0.159±0.003	0.285±0.006	0.437±0.014	0.347±0.013	0.327±0.013
group mean	12.5%	0.527±0.022	0.31±0.009	0.608±0.026	0.336±0.009	0.319±0.002
group max	56.7%	1.098±0.028	0.431±0.015	1.122±0.078	0.402±0.017	0.394±0.017
group min	0.4%	0.025±0.003	0.188±0.016	0.138±0.029	0.261±0.023	0.233±0.022
disc	56.3%	1.073±0.03	0.243±0.025	0.985±0.087	0.141±0.017	0.161±0.025
Table 10: Brier Score (BS) on test set for MIMIC dataset. Standard deviations are computed across
5 splits.
19
Under review as a conference paper at ICLR 2020
type	ratio	Naive	NaiveB	ZafarCE	ParetoFair CE	ParetoFair BS
A/A/NW	5.7%	0.068±0.005	0.329±0.027	2.803±0.659	0.459±0.051	0.409±0.043
A/A/W	13.3%	0.067±0.007	0.322±0.017	4.079±0.601	0.45±0.031	0.4±0.027
A/S/NW	12.9%	0.121±0.009	0.471±0.02	7.697±0.91	0.564±0.02	0.538±0.034
A/S/W	56.7%	0.111±0.003	0.434±0.007	9.238±0.363	0.531±0.019	0.508±0.021
D/A/NW	0.4%	1.573±0.169	0.567±0.078	21.617±3.25	0.515±0.038	0.523±0.052
D/A/W	0.9%	1.363±0.112	0.459±0.05	17.284±2.502	0.46±0.023	0.445±0.033
D/S/NW	1.8%	1.58±0.046	0.604±0.027	22.097±0.961	0.552±0.037	0.544±0.037
D/S/W	8.3%	1.532±0.046	0.609±0.033	18.446±0.9	0.563±0.036	0.547±0.041
sample mean	-	0.266±0.005	0.437±0.007	9.113±0.4	0.522±0.018	0.495±0.017
group mean	12.5%	0.802±0.032	0.474±0.009	12.908±0.233	0.512±0.013	0.489±0.004
group max	56.7%	1.65±0.055	0.635±0.014	23.343±1.058	0.587±0.019	0.578±0.019
group min	0.4%	0.065±0.005	0.313±0.021	2.803±0.659	0.426±0.031	0.388±0.032
disc	56.3%	1.585±0.059	0.323±0.03	20.54±1.089	0.162±0.025	0.191±0.037
Table 11: Cross Entropy (CE) on test set for MIMIC dataset. Standard deviations are computed
across 5 splits.
type	ratio	Naive	NaiveB	ZafarCE	ParetoFair CE	ParetoFair BS
A/A/NW	5.7%	0.046±0.004	0.063±0.007	0.069±0.014	0.109±0.016	0.101±0.004
A/A/W	13.3%	0.043±0.002	0.051±0.006	0.1±0.013	0.092±0.027	0.083±0.015
A/S/NW	12.9%	0.068±0.007	0.033±0.007	0.182±0.018	0.045±0.017	0.038±0.01
A/S/W	56.7%	0.065±0.002	0.027±0.006	0.225±0.007	0.052±0.025	0.041±0.016
D/A/NW	0.4%	0.55±0.094	0.164±0.049	0.522±0.095	0.149±0.037	0.146±0.06
D/A/W	0.9%	0.43±0.057	0.09±0.03	0.394±0.061	0.155±0.057	0.123±0.053
D/S/NW	1.8%	0.57±0.017	0.068±0.022	0.516±0.021	0.09±0.029	0.07±0.02
D/S/W	8.3%	0.535±0.023	0.048±0.012	0.429±0.022	0.061±0.027	0.052±0.026
sample mean	-	0.115±0.004	0.037±0.003	0.219±0.007	0.062±0.021	0.052±0.012
group mean	12.5%	0.288±0.016	0.068±0.006	0.305±0.013	0.094±0.023	0.082±0.021
group max	56.7%	0.593±0.028	0.166±0.047	0.561±0.04	0.172±0.05	0.156±0.05
group min	0.4%	0.042±0.002	0.025±0.003	0.069±0.014	0.039±0.013	0.032±0.011
disc	56.3%	0.552±0.028	0.141±0.045	0.492±0.045	0.133±0.043	0.125±0.044
Table 12: Expected calibration error (ECE) on test set for MIMIC dataset. Standard deviations are
computed across 5 splits.
type	ratio	Naive	NaiveB	ZafarCE	ParetoFair CE	ParetoFair BS
A/A/NW	5.7%	0.322±0.096	0.125±0.02	0.425±0.301	0.147±0.017	0.166±0.016
A/A/W	13.3%	0.251±0.061	0.088±0.015	0.229±0.133	0.127±0.026	0.124±0.019
A/S/NW	12.9%	0.223±0.038	0.058±0.013	0.323±0.198	0.077±0.03	0.07±0.013
A/S/W	56.7%	0.203±0.007	0.046±0.011	0.496±0.184	0.091±0.034	0.069±0.02
D/A/NW	0.4%	0.828±0.115	0.341±0.094	0.522±0.095	0.293±0.048	0.422±0.266
D/A/W	0.9%	0.879±0.07	0.183±0.074	0.509±0.201	0.206±0.033	0.191±0.072
D/S/NW	1.8%	0.822±0.052	0.158±0.082	0.553±0.067	0.152±0.027	0.129±0.029
D/S/W	8.3%	0.865±0.027	0.11±0.038	0.563±0.182	0.136±0.044	0.112±0.041
sample mean	-	0.294±0.013	0.067±0.007	0.441±0.113	0.104±0.029	0.089±0.015
group mean	12.5%	0.549±0.031	0.139±0.007	0.453±0.094	0.154±0.024	0.16±0.047
group max	56.7%	0.914±0.045	0.364±0.061	0.7±0.105	0.293±0.048	0.429±0.259
group min	0.4%	0.196±0.007	0.042±0.008	0.146±0.04	0.071±0.025	0.059±0.013
disc	56.3%	0.719±0.044	0.322±0.065	0.554±0.097	0.222±0.062	0.37±0.262
Table 13: Maximum calibration error (MCE) on test set for MIMIC dataset. Standard deviations are
computed across 5 splits.
20
Under review as a conference paper at ICLR 2020
Groups	ratio	ParetoFair	NaiveB	Naive	ParetoFair	NaiveB	Naive
akiec	2.5%	51.9%	55.6%	3.7%	55.4%	66.3%	55.3%
bcc	2.7%	56.7%	76.7%	56.7%	57.6%	77.5%	66.7%
bkl	7.2%	59.5%	58.2%	36.7%	64.5%	71.6%	64.3%
df	0.5%	66.7%	33.3%	0.0%	59.8%	68.1%	81.6%
nv	81.0%	83.5%	90.9%	96.0%	81.7%	89.1%	93.7%
vasc	1.3%	71.4%	85.7%	0.0%	66.9%	74.7%	76.1%
mel	4.8%	53.8%	48.1%	32.7%	63.6%	32.1%	63.6%
sample mean	-	78.6%	84.8%	83.6%	78.0%	85.6%	88.1%
group mean	14.3%	63.4%	64.1%	32.3%	64.2%	73.6%	71.6%
group max	81.0%	83.5%	90.9%	96.0%	81.7%	89.1%	93.7%
group min	0.5%	51.9%	33.3%	0.0%	55.4%	66.3%	55.3%
disc	80.4%	31.7%	57.5%	96.0%	26.3%	22.8%	38.4%
(a) Accuracy	(b) Confidence
Table 14: Accuracies (left) and confidence (right) on test set for HAM10000 dataset.
type	ratio	ParetoFair	NaiveB	Naive	ParetoFair	NaiveB	Naive
akiec	2.5%	0.741	0.671	1.289	1.547	1.638	3.879
bcc	2.7%	0.549	0.341	0.613	1.095	0.678	1.172
bkl	7.2%	0.62	0.606	0.931	1.345	1.206	2.258
df	0.5%	0.536	0.898	1.721	0.989	1.674	6.486
nv	81.0%	0.241	0.128	0.054	0.58	0.28	0.115
vasc	1.3%	0.36	0.246	1.604	0.747	0.499	4.564
mel	4.8%	0.586	0.657	0.941	1.213	1.28	2.137
sample mean	-	0.308	0.213	0.234	0.708	0.449	0.579
group mean	14.3%	0.519	0.507	1.022	1.074	1.037	2.944
group max	81.0%	0.741	0.898	1.721	1.547	1.674	6.486
group min	0.5%	0.241	0.128	0.054	0.58	0.28	0.115
disc	80.4%	0.501	0.769	1.667	0.967	1.394	6.371
(a) Brier Score	(b) Cross Entropy
Table 16: Brier Score (left) and Cross Entropy (right) on test set for HAM10000 dataset.
type	ratio	ParetoFair	NaiveB	Naive	ParetoFair	NaiveB	Naive
akiec	2.5%	0.249	0.254	0.516	0.754	0.447	0.966
bcc	2.7%	0.147	0.134	0.161	0.269	0.408	0.418
bkl	7.2%	0.171	0.156	0.304	0.731	0.294	0.744
df	0.5%	0.452	0.57	0.816	0.718	0.853	0.985
nv	81.0%	0.025	0.034	0.032	0.143	0.248	0.273
vasc	1.3%	0.256	0.202	0.761	0.557	0.539	0.956
mel	4.8%	0.14	0.212	0.323	0.277	0.289	0.836
sample mean	-	0.055	0.065	0.095	0.219	0.27	0.367
group mean	14.3%	0.206	0.223	0.416	0.493	0.44	0.74
group max	81.0%	0.452	0.57	0.816	0.754	0.853	0.985
group min	0.5%	0.025	0.034	0.032	0.143	0.248	0.273
disc	80.4%	0.426	0.535	0.784	0.611	0.605	0.712
(a) ECE	(b) MCE
Table 18: Expected Calibration Error (left) and Maximum Calibration Error (right) on test set for
HAM10000 dataset.
21
Under review as a conference paper at ICLR 2020
type	ratio	Naive	NaiveB	Feldman	Kamishima	Zafar	ParetoFair LR
Male	67.9%	92.3±0.4%	92.3±0.3%	92.4±0.3%	92.6±0.4%	92.2±0.7%	92.4±0.3%
Female	32.1%	80.5±0.4%	80.3±0.7%	80.8±0.3%	80.9±0.4%	80.8±0.1%	81.0±0.5%
sample mean	-	84.3±0.3%	84.2±0.5%	84.5±0.3%	84.6±0.3%	84.4±0.3%	84.7±0.3%
group mean	50.0%	86.4±0.2%	86.3±0.4%	86.6±0.3%	86.7±0.2%	86.5±0.4%	86.7±0.2%
disc	35.9%	11.9±0.7%	12.0±0.7%	11.6±0.2%	11.7±0.7%	11.4±0.7%	11.4±0.6%
type	ratio	Naive	NaiveB	Feldman	Kamishima	Zafar	ParetoFair LR
White Male	39.7%	90.8±0.3%	90.8±0.3%	90.6±0.3%	90.4±1.1%	90.5±0.6%	90.7±0.3%
Other	60.3%	79.8±0.5%	79.7±0.7%	80.4±0.6%	79.4±2.3%	80.1±0.1%	80.6±0.6%
sample mean	-	84.1±0.2%	84.1±0.4%	84.5±0.3%	83.8±1.8%	84.2±0.2%	84.6±0.4%
group mean	50.0%	85.3±0.1%	85.2±0.3%	85.5±0.3%	84.9±1.6%	85.3±0.3%	85.6±0.4%
disc	20.5%	11.0±0.8%	11.1±0.8%	10.2±0.7%	11.0±1.4%	10.3±0.6%	10.2±0.6%
Table 20: Accuracies on test set for Adult dataset. Standard deviations are computed across 5 splits.
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
Male	32.1%	0.116±0.004	0.117±0.005	0.115±0.004	0.11±0.005	0.115±0.013	0.114±0.003
Female	67.9%	0.268±0.004	0.272±0.007	0.263±0.005	0.258±0.003	0.26±0.003	0.261±0.006
sample mean	-	0.22±0.003	0.222±0.006	0.215±0.004	0.21±0.003	0.214±0.006	0.214±0.004
group mean	50.0%	0.192±0.003	0.194±0.006	0.189±0.003	0.184±0.003	0.188±0.008	0.188±0.003
disc	35.9%	0.152±0.005	0.155±0.005	0.148±0.005	0.147±0.005	0.144±0.011	0.147±0.007
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
White Male	39.7%	0.139±0.003	0.137±0.004	0.135±0.004	0.14±0.02	0.138±0.013	0.133±0.004
Other	60.3%	0.28±0.004	0.28±0.008	0.271±0.005	0.284±0.038	0.269±0.002	0.267±0.007
sample mean	-	0.224±0.002	0.223±0.005	0.217±0.003	0.227±0.031	0.217±0.006	0.214±0.005
group mean	50.0%	0.21±0.002	0.208±0.005	0.203±0.003	0.212±0.029	0.204±0.007	0.2±0.005
disc	20.5%	0.142±0.007	0.143±0.007	0.136±0.007	0.144±0.019	0.131±0.012	0.133±0.008
Table 21: Brier Score (BS) on test set for Adult dataset. Standard deviations are computed across 5
splits.
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
Male	67.9%	0.204±0.009	0.204±0.011	0.2±0.005	0.189±0.006	0.201±0.025	0.207±0.017
Female	32.1%	0.411±0.006	0.416±0.011	0.402±0.007	0.395±0.004	0.406±0.01	0.41±0.014
sample mean	-	0.345±0.006	0.348±0.011	0.337±0.005	0.329±0.004	0.34±0.01	0.345±0.013
group mean	50.0%	0.308±0.007	0.31±0.011	0.301±0.005	0.292±0.004	0.303±0.013	0.309±0.013
disc	35.9%	0.207±0.007	0.211±0.005	0.203±0.008	0.206±0.007	0.206±0.027	0.202±0.016
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
White Male	39.7%	0.238±0.005	0.233±0.008	0.227±0.006	0.254±0.075	0.237±0.022	0.233±0.013
Other	60.3%	0.428±0.007	0.428±0.011	0.413±0.007	0.463±0.116	0.418±0.007	0.415±0.015
sample mean	-	0.352±0.004	0.35±0.009	0.339±0.005	0.38±0.099	0.346±0.009	0.343±0.013
group mean	50.0%	0.333±0.003	0.33±0.008	0.32±0.004	0.359±0.095	0.327±0.011	0.324±0.013
disc	20.5%	0.19±0.009	0.194±0.009	0.187±0.01	0.209±0.042	0.181±0.024	0.183±0.013
Table 22: Cross Entropy (CE) on test set for Adult dataset. Standard deviations are computed across
5 splits.
22
Under review as a conference paper at ICLR 2020
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
Male	67.9%	0.026±0.008	0.023±0.007	0.024±0.003	0.011±0.003	0.021±0.023	0.017±0.007
Female	32.1%	0.013±0.004	0.014±0.005	0.01±0.004	0.012±0.002	0.014±0.005	0.013±0.004
sample mean	-	0.017±0.005	0.017±0.005	0.014±0.003	0.011±0.002	0.016±0.011	0.014±0.003
group mean	50.0%	0.019±0.006	0.019±0.005	0.017±0.002	0.011±0.002	0.018±0.014	0.015±0.003
disc	35.9%	0.013±0.004	0.01±0.006	0.014±0.006	0.003±0.002	0.011±0.016	0.009±0.005
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
White Male	39.7%	0.026±0.006	0.024±0.008	0.019±0.002	0.016±0.012	0.026±0.02	0.019±0.006
Other	60.3%	0.018±0.005	0.016±0.004	0.014±0.005	0.021±0.022	0.019±0.008	0.015±0.004
sample mean	-	0.021±0.004	0.019±0.006	0.016±0.003	0.019±0.018	0.022±0.013	0.017±0.002
group mean	50.0%	0.022±0.004	0.02±0.006	0.016±0.003	0.019±0.017	0.023±0.014	0.017±0.003
disc	20.5%	0.008±0.005	0.007±0.005	0.007±0.003	0.006±0.01	0.01±0.009	0.008±0.005
Table 23: Expected calibration error (ECE) on test set for Adult dataset. Standard deviations are
computed across 5 splits.
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
Male	67.9%	0.064±0.012	0.065±0.034	0.064±0.017	0.072±0.01	0.077±0.03	0.062±0.022
Female	32.1%	0.027±0.013	0.031±0.009	0.03±0.01	0.031±0.008	0.035±0.008	0.024±0.008
sample mean	-	0.039±0.009	0.042±0.016	0.041±0.006	0.044±0.008	0.049±0.011	0.036±0.01
group mean	50.0%	0.045±0.008	0.048±0.021	0.047±0.008	0.052±0.009	0.056±0.016	0.043±0.013
disc	35.9%	0.037±0.019	0.034±0.027	0.034±0.024	0.041±0.004	0.042±0.03	0.038±0.02
type	ratio	Naive	NaiveB	Feld	Kam	Zafar	ParetoFair LR
White Male	39.7%	0.043±0.018	0.053±0.015	0.045±0.008	0.061±0.019	0.08±0.033	0.052±0.014
Other	60.3%	0.05±0.011	0.035±0.007	0.035±0.019	0.039±0.034	0.044±0.01	0.032±0.011
sample mean	-	0.047±0.011	0.042±0.01	0.039±0.011	0.048±0.025	0.058±0.017	0.04±0.009
group mean	50.0%	0.047±0.011	0.044±0.011	0.04±0.009	0.05±0.023	0.062±0.02	0.042±0.009
disc	20.5%	0.016±0.014	0.017±0.009	0.023±0.009	0.032±0.02	0.037±0.029	0.023±0.014
Table 24: Maximum calibration error (MCE) on test set for Adult dataset. Standard deviations are
computed across 5 splits.
23
Under review as a conference paper at ICLR 2020
A.8 Analysis of Pareto-optimal classifiers in the infinite data and model
CAPACITY REGIME
In this section we analyze the form of Pareto-optimal solutions to classification (and regression)
tasks in the asymptotically ideal case where we have infinite capacity hypothesis classes, that is,
our hypothesis class H contains every function mapping points from the observation space X to the
classification or regression space R|Y|.
We additionally assume that the joint distributions between target variables Y and observation vari-
ables X given every sensitive attribute are known (i.e., P(Y, X|A) is known), and that the loss
function L(h(x), y) is convex with respect to h(x).
Any joint loss of the form
φPA({Ra(h)}) = EX,Y,A[L(h(X), Y] = XPaRa(h),	(3)
a∈A
produces solutions which are already in the Pareto front of {R0(h)} for any distribution of A 〜 Pa.
Since the loss function is convex with respect to h(X) and the hypothesis class is complete, it can
also be shown that for any point in the Pareto front, there exists a value of PA such that that point is
reached by minimizing φPA ({Ra(h)}) Geoffrion (1968); Koski (1985); Miettinen (2012)
We can therefore analyze the Bayes-optimal classifier hPA which minimizes the Naive risk
φPA ({Ra(h)}), and analytically compute the sub-population risks Ra(hPA ) induced. In general,
we can write
E[Y |x]	= P E[Y |x, a]P (a|x),
a∈A
P E[Y |x,a]P (x|a)Pa
__ a∈A
= P P(x∣a)Pα-,
a∈A
hPA(X)	= arg min E[L(η, Y)|X],
η
Ra(hPA)	= EX,Y [L(hPA (X), Y)|A = a],
(4)
and in the particular case where target variable Y is categorical, and the classifier loss is an L2 loss
against the one-hot encoding of variable Y the equations reduce to
E[Y |x]
hPA(X)
Ra(hPA)
P P~ [Y |x,a]P (x|a)Pa
__ a∈A
= P P(x∣a)Pα-,
a∈A
= E[Y |x],
EX[Py P (y|a, X) Py0(hyP0A(X) - δ[y - y0])2].
(5)
24