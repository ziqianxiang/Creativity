Under review as a conference paper at ICLR 2020
Wyner VAE: A Variational Autoencoder with
Succinct Common Representation Learning
Anonymous authors
Paper under double-blind review
Ab stract
A new variational autoencoder (VAE) model is proposed that learns a succinct
common representation of two correlated data variables for conditional and joint
generation tasks. The proposed Wyner VAE model is based on two information the-
oretic problems—distributed simulation and channel synthesis—in which Wyner’s
common information arises as the fundamental limit of the succinctness of the
common representation. The Wyner VAE decomposes a pair of correlated data
variables into their common representation (e.g., a shared concept) and local rep-
resentations that capture the remaining randomness (e.g., texture and style) in
respective data variables by imposing the mutual information between the data
variables and the common representation as a regularization term. The utility of the
proposed approach is demonstrated through experiments for joint and conditional
generation with and without style control using synthetic data and real images.
Experimental results show that learning a succinct common representation achieves
better generative performance and that the proposed model outperforms existing
VAE variants and the variational information bottleneck method.
1	Introduction
This paper aims to develop a new probabilistic framework for generation tasks (i.e., learning the
distribution of given data and sampling from the learned distributions) for two high-dimensional
random vectors. To motivate the main idea, consider the following cooperative game between Alice
and Bob. Suppose that given an image of a child’s photo, Alice sends its description Z to Bob who
draws a portrait of how the child will grow up based on it. The objective of this game is to draw a
nice portrait, and thus Alice needs to help Bob in the process by providing a good description of
the child’s photo — any redundant information in the description may confuse Bob in his guessing
process. What description does Alice need to generate and send from the child’s photo?
P. Cuff (2013) formulated this game of conditional generation as the channel synthesis problem in
network information theory depicted in Fig. 1. Given a joint distribution q(x, y) = q(x)q(y|x),
Alice and Bob want to generate Y according to q(y|x) based on a sample from q(x). In this problem,
Alice wishes to find the most succinct description Z of X (a child’s photo) such that Y (her adulthood
portrait) can be simulated by Bob according to the desired distribution using this description and
local randomness V (new features to draw a portrait of adults that are not contained in photos of
children). The minimum description rate for such conditional generation is characterized by Wyner’s
common information (Wyner, 1975; El Gamal and Kim, 2011) denoted by J(X; Y) and defined as
the optimal value of the optimization problem
minimize	Iq (X, Y; Z)
subject to X → Z → Y	(1)
variables q(z|x, y),
where X → Z → Y denotes a Markov chain from X to Z to Y and Iq (X, Y; Z) denotes the mutual
information between (X, Y) and Z, where (X, Y, Z)〜q(x, y)q(z∣x, y).
The same quantity J(X; Y) arises as the fundamental limit of the distributed simulation of correlated
sources studied originally by A. Wyner (1975) in which two distributed agents wish to simulate a
target distribution q(x, y) (i.e., joint generation of (X, Y)) based on the least possible amount of
1
Under review as a conference paper at ICLR 2020
Xn
(a) Channel synthesis
v
(b) Single-letter characterization of (a)
Un
yn
(c) Distributed simulation
U
X
v
(d) Single-letter characterization of (c)
Figure 1: Schematics for channel synthesis from X to Y (a,b), and distributed simulation of (X, Y)
(c,d). (a,c) and (b,d) correspond to the operational definition and the single-letter characterization of
each problem, respectively. The local randomness U and V make the decoders stochastic.
shared common randomness. (See Fig. 1 (c,d).) In this sense, the joint distribution q(x, y) and the
conditional distributions q(y∣x), q(x∣y) have the same common information structure characterized
by the optimization problem (1), which involves learning the joint distribution in its nature. We call
thejoint encoder q(z∣x, y) (or equivalently, the corresponding random variable Z) as the common
representation of (X, Y), and the mutual information Iq(X, Y; Z) then can be viewed as a measure
of the complexity of Z.
The goal of this paper is to propose a new probabilistic model based on these information theoretic
observations, to achieve a good performance in joint and conditional generation tasks. We apply
the idea of learning succinct common representation to design a new generative model for a pair of
correlated variables: seeking a succinct representation Z in learning the underlying distribution based
on its sample may also help reduce the burden on the decoder,s side and thereby achieve a better
generative performance.
The rest of the paper gradually develops our framework as follows. We first define a probabilistic
model based on the motivating problems, which we aim to train and use for generation tasks
(Section 2.1), and then establish a general principle for learning the model based on the optimization
problem (1) (Section 2.2). We propose one instantiation of the principle with a standard variational
technique by introducing additional encoder distributions (Section 2.3). The proposed model with its
training method can be viewed as a variant of variational autoencoders (VAEs) (Kingma and Welling,
2014; Rezende et al., 2014), and is thus called Wyner VAE. (See Appendix A for a brief introduction
on VAEs.) The new encoder components introduced in Wyner VAE allow us to decompose a data
vector into the common representation and the local representation, which can be used for sampling
with style manipulation (Section 2.4). We carefully compare our model and show its advantages
over the existing VAE variants (Vedantam et al., 2018; Suzuki et al., 2016; Sohn et al., 2015; Wang
et al., 2016) and the information bottleneck (IB) principle (Tishby et al., 1999) (Section 3), which
is a well-known information theoretic principle in representation learning. In the experiments, we
empirically show the utility of our model in various sampling tasks and its superiority over existing
models and that learning a succinct common representation achieves better generative performance
in generation tasks (Section 4).
2	Wyner variational autoencoder
2.1	Probabilistic model
We first define a probabilistic model for joint and conditional sampling tasks based on the single-letter
characterizations of the motivating problems (Fig. 1 (b,d)). We assume that all distributions to be
2
Under review as a conference paper at ICLR 2020
introduced below belong to some standard parametric families such as Gaussians, and use (q, φ) (or
(p, θ)) to denote the parameters and the distribution of encoders (or decoders and priors).
In both channel synthesis and distributed simulation, Z 〜pθ (Z) signifies the common randomness
fed into the deterministic decoders xθ(z, U) and y (z, v), while U 〜pθ(U) and V 〜pθ(V)
signify the local randomnesses for each decoder. We take the sources of randomness (or priors)
pθ(z),pθ(U),pθ(v) as standard parametric distributions such as Gaussian from which a sample can
be easily drawn. U (or V) is interpreted as a local description of X (or Y) given the common
description Z. We can also view U (or V) as an intrinsic randomness of the stochastic decoder
pθ(x|z) (or pθ(y∣z)), which is the induced distribution by the decoder xθ(z, U) (or y (z, V)) and the
prior pθ (U) (orpθ(v)). To perform joint sampling, we need the priors pθ(z),pθ(U),pθ(v) and the
decoders xθ(z, U) and yθ(z, V) (Fig. 1 (d)). For conditional sampling of Y given X = x, we need
the marginal encoder qφ(z∣x) to be defined in addition to the prior pθ(V) and the decoder y (z, v)
(Fig. 1 (b)). We focus on how to learn qφ(z∣x) in what follows, since q0(Z卜)can be dealt with in the
same way by the symmetry of the Markov chain X → Z → Y.
After Wyner, we name the entirety of all the components (the marginal encoders qφ(z∣x), qφ(z∣y),
the priors pθ(z),pθ(u),pθ(v), and the decoders pθ(x|z, u),pθ(y|z, v)) thatis essential forjointand
conditional sampling tasks as the Wyner common representation model or the Wyner model in short.
2.2	Objective function
We now propose an objective function for learning each component in the Wyner model based on
the optimization problem (1). In practice, the data distribution q(x, y) is replaced by the empirical
distribution qemp(x, y) defined by the given samples. To train the components for joint sampling
in the Wyner model, we solve the optimization problem (1) by incorporating a new consistency
constraint
q(χ,y)qφ(z∣χ,y) ≡pθ(z)Pθ(XIz)Pθ(yIz),
(2)
with the priorspθ(z),pθ(U),pθ(V) and the decoders χθ(z, U), yθ(z, V) as new optimization variables.
The Markov condition X → Z → Y can be removed under the new constraint (2). We then relax the
constraint (2) with an inequality constraint
Lχy→χy := D(pθ(z)pθ(x∣z)pθ(y Iz),q(x, y)qφ(z∣x, y)) ≤ e,
for some > 0, and convert the problem (1) into an unconstrained form as in Zhao et al. (2018):
minimize	Lxy→xy + λIq
variables	qφ(zIx, y), pθ(z), pθ(U), pθ(V), xθ(z, U), yθ(z, V)
(3)
Here we use a shorthand notation Iq := Iq(X, Y; Z), and use λ > 0 to denote the reciprocal of the
Lagrange multiplier. We can choose D(p, q) as any proper distance or divergence measure between
distributions such as f -divergences, Jensen-Shannon divergence, Wasserstein distance, or maximum
mean discrepancy (Zhao et al., 2018).
To find the marginal encoder qφ(zIx) that is consistent to q(yIx)qφ(zIx, y) for conditional gen-
eration, we aim to match a joint distribution induced by the Markov chain X → Z → Y, i.e.,
q(x)qφ(z∣x)pθ(y∣z), with the decoder distribution pθ(z)pθ(x∣z)pθ(y∣z) and the encoder distribu-
tions q(x, y)qφ(zIx, y) of the joint model. That is, we wish to find qφ(zIx) that minimizes
Lχ→χ ：= D(Pθ(z)pθ(x|z), q(x)qφ(z∣x))	(4)
and
Lχy→y := D(q(x)qφ(z∣x)pθ(y∣z),q(x,y)qφ(z∣x,y)).	(5)
The objective functions Ly→y and Lxy→x for qφ(zIy) can be defined in a symmetric manner. The
final objective function for training the Wyner model with succinct common representation learning
then becomes
L := Lxy→xy + λIq + αx→x Lx→x + αxy→y Lxy→y + αy→yLy→y + αxy→x Lxy→x ,
(6)
where the weights α's are nonnegative hyperparameters.
Yet, to minimize the objective function in practice, we need to choose which divergence/distance
metric D(p, q) to use, and also need to address computationally intractable terms—the induced
distributions pθ(x∣z),pθ(y∣z) and the mutual information term Iq-in (3), (4), and (5).
3
Under review as a conference paper at ICLR 2020
2.3	Variational relaxation
We propose an instantiation of the objective function with a specific choice of a divergence function
with a standard variational technique. We choose the metric D(p, q) as the reverse KL divergence
DKL(qkp), which is a common choice in the variational inference literature (see, e.g., Blei et al.
(2017)). To remove the intractable induced distributions, we relax the objective function (3) by
introducing variational encoders qφ(u∣z, x) and qφ(v∣z, y):
Lχy→χy = DκL(q(x, y)qφ(z∣x, y)∣∣Pθ(z)pθ(x∣z)pθ(y |z))	(7)
≤ DκL(q(χ,y)qφ(z∣χ,y)qφ(u∣z,χ)qφ(v|z,y)kPθ(z)pθ(x∣z)Pθ(y|z)pθ(u, V|x,y,Z)) (8)
=DκL(q(χ, y)qφ(z∣χ, y)qφ(u∣z, χ)qφ (v | z, y)kpθ (z)pθ (U)pθ (v)pθ (x| z, u)pθ (y |z, V)) (9)
=: Lxy→xy,	(10)
where (8) follows from the chain rule and nonnegativity of KL divergence (see, e.g.,
Cover and Thomas (2006)), pθ(u, v|x, y, Z) denotes the induced conditional distribution by
Pθ(u),pθ(v),pθ(x|z, u),pθ(y|z, v), andpθ(x|z, u) (orpθ(y|z, v)) denotes the distribution induced
by the decoder xθ(z, u) (or y(z, v)). Note that the intractable distributions pθ(x∣z),pθ(y|z) no
longer appear in the upper bound (9).
For the mutual information term Iq = Iq(X, Y; Z) with (X, Y, Z)〜q(x, y)qφ(z∣x, y), We use the
following standard upper bound (see, e.g., Zhao et al. (2018)):
Iq = Eq(x,y)[DκL(qφ(z | X, Y)府 φ(z))] ≤ Eq(χ,y)[DκL(qφ(z∣X, Y)kPθ (z))] =： 了 q,	(□)
where ⅞φ(z) denotes the induced marginal distribution by q(x, y) and qφ(z∣x, y). Note here that the
relaxation gap is DKL(⅞φ(z)∣∣Pθ(z)), which is again upper bounded by thejoint KL divergence (7).
Therefore, all relaxation steps in (8) and (11) become tight when the joint distributions induced by
data distribution, encoders, priors, and decoders are perfectly consistent with each other, i.e.,
q(χ,y)qφ(z∣χ,y)qφ(u∣z,χ)qφ(v|z,y) ≡pθ(z)pθ(u)pθ(v)pθ(x|z,u)pθ(y|z, v).
We note that these relaxation techniques are standard in the literature, the tightness of which deserves
a separate future study; for recent related work, refer to Cremer et al. (2018); Poole et al. (2019).
To sum up, the objective function for the joint model (3) is relaxed as
Lxy→xy + λIq ∙	(12)
Note that the additional information regularization with λ > 0 is only on the common representation
Z, but not on U, V, which we call local representation. By increasing λ > 0 to a proper degree that
does not impedes fitting, we can “reroute” the information flow from (X, Y) through the common
representation qφ(z∣x, y) to the local representations qφ(u∣z, x), qφ(v∣z, y).
Following similar steps in (7), (8), (9), the objective function in (4) can also be upper bounded as
Lχ→x ≤Lχ→x ：= DκL(q(x)qφ(z∣x)qφ(u∣z, x)kpθ(z)pθ(u)pθ(x|z, u)).	(13)
For (5), we choose Lχy→y ：= DκL(q(x, y)qφ(z∣x, y)kq(x)qφ(z∣x)pθ(y∣z)), which can be viewed
as the expected conditional ELBO (Sohn et al., 2015). It can be also subsequently relaxed as
Lxy→y ≤ Lxy→y ：=DκL(q(x, y)qφ(z∣x, y)qφ(v|z, y)kq(x)qφ(z∣x)pθ(v)pθ(y |z, v)).	(14)
After all, the final relaxed objective function is given as follows:
L ：
L 、
xy→xy
+ λI q + αx→x Lx→x + αy→yLy→y
+ αxy→y Lxy→y + αxy→xLxy→x.
(15)
See Figure 2 for an overview of each term in the objective function.
We call the overall framework which consists of all the components in the Wyner model and the
additional encoders qφ(z∣x, y), qφ(u∣z, x), q@(v|z, y) together with its training objective (15) as
Wyner common representation VAE or Wyner VAE in short. After parameterizing each distribution
component in Wyner VAE as standard parametric distributions such as Gaussians, whose parameters
are again parameterized by deep neural networks, Wyner VAE can be trained efficiently by the stan-
dard reparameterization trick (Kingma and Welling, 2014) as in the standard VAE. (See Appendix B
for the Gaussian parameterization and the corresponding objective functions.)
4
Under review as a conference paper at ICLR 2020
Figure 2: A summary of the training objective for the Wyner VAE (15).
In practice, We can either train the objective function (15) jointly with some positive α's (joint
training), or train the joint model first letting a's be 0 and then train the marginal encoder in a
retrospective manner by freezing the joint model (two-stage training) as in Vedantam et al. (2018).
The hyperparameter a’s may be chosen with cross-validation with validation sets. We elaborate the
training schemes used for each experiment in Appendix F.
We remark the role of the encoders qφ(z|x, y), qφ (u|z, x), qφ(v|z, y). Thejoint encoder qφ(z∣x, y)
appears in the optimization problem (1) and plays as a reference distribution in learning the com-
ponents of the Wyner model. The variational encoders qφ(u|z, x) and qφ(v|z, y) are introduced
to remove the intractable induced distributions pθ(z|x) and pθ(z|y), satisfying the correct condi-
tional independence structure implied by the decoder model pθ(z)pθ(u)pθ(v)pθ(x|z, u)pθ(y|z, v),
that is, qφ(z, u, v|x, y) = qφ(z|x, y)qφ(u|z, x)qφ(v|z, y), or equivalently UX(V, Y)|Z and
V 11 (U, X)|Z. If We learn a succinct common representation qφ (z|x, y) (e.g., a shared concept)
from (X, Y), then qφ(u|z, x) would capture the remaining randomness U of X (e.g., texture and
style). We call this decomposition of the pair (X, Y) into the common representation Z and the local
representation U,V as common-local information decomposition of (X, Y). We refer to (Z, U, V)
as the joint representation of (X, Y), to distinguish it from the common representation Z. Provided
that Wyner VAE achieves a good information decomposition, the variational encoders then can be
used to explicitly in finding the local representations U and V from the data variables X and Y .
2.4	Sampling with style control
As alluded to above, the variational encoders qφ(u|z, x), qφ(v|z, y) can be used in sampling tasks
with style control as a local representation (i.e., style) extractor. We illustrate how to perform
conditional sampling with style control (Fig. 3 (e)). Suppose that (X, Y) is a pair of correlated
images generated from the common concept but from different domains. Given an image y0 , we can
extract the style information V0 from y0 by sampling (Z0, V0) from qφ(z∣y)qφ(v|z, y) (Fig. 3 (d)).
We then generate Yj from an image xj similar to conditional sampling (Fig. 3 (c)), while replacing
the randomly drawn local representation V 〜pθ (V) with the previously extracted style V0, thereby
the generated images Y0,j’s are of the same style as the reference image y0. In a similar manner,
we can also perform joint sampling with a fixed style given a style reference data pair (x0, y0), by
mixing a randomly drawn common representation Z from the prior pθ (z) with the extracted style
variables (u0, v0).
2.5	Degenerate cases in Wyner VAE
Two degenerate cases may arise in Wyner VAE. The first case is where the common variable Z
captures all information of (X, Y), while U and V capture none. Joint VAE (JVAE) (Vedantam et al.,
2018) and joint multimodal VAE (JMVAE) (Suzuki et al., 2016) inherently assume this degenerate
case, as it is discssued in the next section. Wyner VAE is able to avoid such degeneracy by explicitly
5
Under review as a conference paper at ICLR 2020
(a) Joint sampling
(b) Joint stochastic
reconstruction
(C) Conditional sampling
(d) Style extraction
(e) Conditional sampling
with style control
Figure 3: Schematics for selected sampling tasks. Double arrows denote deterministic mapping.
Table 1: Summary of related work. (J:joint generation, C: conditional generation, S: style control.)
	C	J	S
JVAE (Vedantam et al., 2018), JMVAE (Suzuki et al., 2016)	O	O	F
CVAE (Sohn et al., 2015)	O	X	O
VCCA-private (Wang et al., 2016)	O	O	O
VIB (Alemi et al., 2017)	O	X	X
Wyner VAE	O	O	O
having the local variables and controlling the common regularization parameter λ > 0. On the other
extreme, Z may capture no information, while U and V capture all the information of X and Y,
respectively. It may happen in Wyner VAE if the regularization parameter λ is too large. To avoid the
degeneracy, we need to choose a proper λ by cross-validation.
3	Related Work
In this section, we compare the proposed Wyner VAE to the existing models, deferring a detailed
description of the encoder, decoder, prior components and objective functions of each model to
Appendix C. We provide a summary for capabilities of each model in Table 1.
VAE models. Wyner VAE can be viewed as a generalization of the probabilistic model (i.e., encoder
and decoder) assumed in two existing joint VAEs— JVAE (Vedantam et al., 2018) and JMVAE (Suzuki
et al., 2016)—as alluded in the previous paragraph. These models implement a similar idea of
performing joint and conditional generation tasks via a symmetric Markov chain X → W → Y,
where W is the joint representation of (X, Y). In other words, these models can be derived by
removing the local variables U and V in Wyner VAE. In Section 4, we demonstrated that the local
variables in Wyner VAE help generate a variety of samples compared to JVAE.
The same decoder structure of Wyner VAE with the “shared” (Z) and the “private” (U, V) latent
variables has been also studied in the context of multi-view learning (Shon et al., 2006; Ek et al., 2008;
Salzmann et al., 2010; Damianou et al., 2012) mostly based on a linear analysis such as canonical
correlation analysis (CCA). More recently, variational CCA-private (VCCA-private) (Wang et al.,
2016) was proposed to learn the decoder model with variational encoders qφ(z∣x), qφ(u∣x), and
qφ(v|y), with the encoder model qφ(z, u, v|x, y) = qφ(z∣x)qφ(u∣x)qφ(v∣y) to directly capture the
conditional model from X to Z to Y. On the other hand, Wyner VAE relies on the conditional inde-
pendence structure qφ(z, u, v|x, y) = qφ(z∣x, y)qφ(u∣z, x)qφ(v∣z, y), which is naturally induced
by the decoder model. We argue that this choice of encoder model in Wyner VAE may capture
better semantic meaning of the local (private) random variables U and V, thereby leading a better
generative performance; see, e.g., Fig. 6.
Conditional VAE (CVAE) (Sohn et al., 2015) directly models the conditional distribution q(y|x),
obtained by simply conditioning every component in the vanilla VAE for q(y) with the conditioning
variable X. If Y is an image and X is an attribute of the image, a latent representation V in
CVAE needs to capture the redundant information of Y, which is not contained in X, i.e., style
information of Y given X. Wyner VAE can be viewed as a combination of two CVAEs with Z as
a common conditioning variable, being capable of bidirectional sampling in its nature. Yet, if X
is high-dimensional, the conditional models like CVAE in general tend to overfit the input data of
6
Under review as a conference paper at ICLR 2020
Table 2: Wyner model vs. the IB principle (Tishby et al., 1999).
	Wyner model	IB principle
Motivating problem	Channel synthesis, distributed simulation	Lossy compression
Probabilistic model	X→Z→Y	Z→X→Y
Direction of inference	Bidirectional	Unidirectional
Measure of succinctness	I(X, Y; Z)	I(X; Z)
Measure of fit/relevance	D(p, q)	I(Y; Z)
Optimal quantity	J (X; Y)	N/A
X (Dutordoir et al., 2018). To address this problem, a subsequent related work, bottleneck conditional
density estimation (BCDE) (Shu et al., 2017), proposed to learn joint and conditional VAE models
simultaneously by softly tying the parameters of the two models for regularization. We note that
Wyner VAE naturally addresses such problem by using a unified single probabilistic model for
both joint and conditional distribution learning, finding a succinct common representation Z for
regularization.
Information bottleneck principle. The information bottleneck (IB) principle (or method) (Tishby
et al., 1999) is a widely known information theoretic approach in representation learning especially
for discriminative tasks, i.e., when the target variable Y is a function of X and/or even discrete.
Motivated by lossy compression, the IB principle proposes to find a compressed representation Z
from the input variable X (i.e., qφ(z∣x)) while maximizing the relevance of Z in predicting the target
variable Y as the minimizer of the optimization problem minimize qφ(z∣x) βIq(X; Z) — Iq(Y; Z),
where (X, Y, Z)〜q(x, y)q0(z∣x).
The foremost difference between the IB principle and our approach is in the underlying Markov
chains: our symmetric Markov assumption X → Z → Y is more natural than Z → X → Y of
IB, when guessing Y based on Z as a representation of X. Further, our framework aims to find a
certain common information structure characterized by Wyner’s common information with proper
analogies to generation tasks of our interest, whereas the compressed-from-X yet relevant-to-Y
representation Z in the IB principle lacks its operational meaning, relying on a rather weak analogy
to lossy compression. We summarize other differences in various aspects in Table 2. In particular, we
compare Wyner VAE with variational IB (VIB) (Alemi et al., 2017) in the experiments, which is an
instantiation of the IB principle based on a variational technique that can be implemented with neural
networks. As empirically shown below, VIB is not suitable for conditional generative tasks if the
target variable Y is high-dimensional.
4	Experiments
We empirically demonstrate that Wyner VAE outperforms JVAE, CVAE, VCCA-private, and VIB,
for joint/conditional generation tasks and style manipulation on various datasets. We defer the
implementation details and training schemes used for each experiment to Appendix F.
Synthetic data. We first performed an experiment with a mixture of Gaussians (MoG) dataset
as a toy example. We considered a pair of 10-dim. MoG random vectors (X, Y) only correlated
through a label Z 〜Unif([1,2, 3,4,5]) (common information) and 5-dim. Gaussian random vectors
U, V 〜N(0,I5)(local randomness in each variable). We used the Gaussian latent variables
(Z, U, V) of dimensions (10, 10, 10), trained each model for 500 epochs (separate 50 epochs for
each marginal encoder for JVAE and Wyner VAE) with a training data of size 50k, and summarized
the numerical results in Fig. 4 and Table 3, which were evaluated with a test data of size 10k. See
Fig. 9 in Appendix E.1 for some visualizations.
In Fig. 4 (a,b), Wyner VAE with λ = 0 performed best for fitting joint distributions, but did not excel
in conditional log-likelihoods. We observe that the performance of Wyner VAE on the test data gets
improved throughout training by increasing λ: λ = 0.05 achieved a good conditional performance
without too much sacrifice in the joint performance, while a larger value of λ(= 0.1) interfered fitting
to the distribution, failing to capture the essential common information structure. Overall, Wyner
VAE with λ control outperformed the other models. CVAE tends to overfit quickly as noted earlier.
7
Under review as a conference paper at ICLR 2020
Test conditional negative Iog-Iikelihood
50 IOO 150 200 250 300 350 400 450 500
# epochs
Figure 4:	Numerical evaluations for the MoG experiment. For each point of the plots, we trained 10
different models and plotted average values with the shaded region that shows the standard deviation.
(Two largest and smallest outliers were dropped for each point.)
Table 3: Best negative log-likelihood (nll) values during 500 epochs of training for MoG dataset.
	Joint nll	Conditional nll
JVAE (Vedantam et al., 2018)	32.82 ± 0.14	32.80 ± 0.76
Wyner VAE (λ = 0)	32.54 ± 0.11	16.16 ± 0.14
Wyner VAE (λ = 0.05)	32.64 ± 0.18	16.03 ± 0.07
VCCA-private (Wang et al., 2016)	32.77 ± 0.04	15.96 ± 0.04
CVAE (Sohn et al., 2015)	-	16.11 ± 0.06
VIB (Alemi et al., 2017) (β = 0.1)	-	503.91 ± 2.79
JVAE and VIB performed extremely worse in test conditional log-likelihoods compared to others as
in Table 3. JVAE failed to capture the common information structure as the training epochs increased,
while VIB was only able to capture the average behaviors — we demonstrate how these models failed
in this toy dataset in Appendix E.1. Although VCCA-private achieved the best conditional likelihood
with a comparable joint likelihood performance, this model fails to learn more complex distributions
as illustrated in Fig. 6 and Appendix E.2, E.3.
Fig. 4 (c) shows that λ can control the common mutual information Iq(X, Y; Z) in Wyner VAE —
in particular, λ ∈ {0.05, 0.1} kept the (estimated) mutual information at a constant level. We remark,
however, that the MoG dataset has J(X; Y) = log 5, while the mutual information estimates are not
around the true value (Fig. 4). We attribute this mismatch to modeling the underlying discrete latent
variable with a continuous, Gaussian vector. Note that we deliberately used the standard Gaussian
model assuming that we do not have a prior knowledge on the dataset generating process.
MNIST and SVHN datasets. We performed experiments with image datasets MNIST (Le-
Cun, 1998) and SVHN (Netzer et al., 2011), by randomly pairing digit images only through
their labels. In particular, We constructed two dataset, MNIST-MNIST add-1 dataset, where
label(Yi) = label(Xi) + 1 (mod 10), and MNIST-SVHN dataset, where label(Yi) = Iabel(Xi).
For MNIST-MNIST add-1 dataset, we trained Wyner VAE with different choices of λ ∈
{0.0, 0.1, . . . , 0.5} to show the effect of λ on the generative performance of Wyner VAE. Fig. 5 (a)
corroborates our main claim that λ > 0 helps learning succinct representation in terms of small
Iq (X, Y; Z) and there exists a sweet spot (λ = 0.1) that strikes the balance between the fitting and
the succinctness.
We also evaluated the label accuracy using the pre-trained Le-Net5 (LeCun et al., 1998) classification
network of accuracy 99.1%, and the per-pixel variance of samples from conditional generation. Note
that both high accuracy and high variance are desired for good generative models. Fig. 5 (b) shows
that by rerouting the information flow through Z to U, V, λ > 0 helps U, V capture the style of
images with a small sacrifice in label accuracy.
We present image samples to visualize the effect of λ in Wyner VAE and the superiority of Wyner VAE
over the existing models. We performed four sampling tasks—conditional generation, conditional
generation with style control, joint stochastic generation, and joint sampling with style control; see
Fig. 3—for both MNIST-MNIST add-1 and MNIST-SVHN datasets, but here we present only a few
8
Under review as a conference paper at ICLR 2020
—Conditional negative log likelihood
—Mutual information/(X,Y;Z)
u。一⅛E-lαu 二 en¾w
100
Accuracy (%)
Variance
(求)X3en8<
80604020
0.06
O
O
Figure 5:	Numerical evaluations of Wyner VAE for conditional generation of MNIST-MNIST add-1
dataset. The plots were generated similarly as Fig. 4. See also Table 5 in the Appendix.
Input
A.
3
±
s^
/4395
/ a 3 q 5
IJeOɪ 6
J B 3 夕r
— J 3 γ√
/ 彳 3 4 5
1 ∖ I ?■/ /
Q J a
3 3 3 M 3 3
4” 4∣V V
5 rr^∣5∙ 5^
/ 3 3 / S
∕l9
1 o 1
7 9 3
a 0飞
S ¾ ʃ
(a1) WynerVAE
("0)
(b1) WynerVAE	(c1) WynerVAE	(d1) JVAE
(λ = 0.1)
(λ = 0.2)
(e1)
(f1) VCCA-PriVate
4 7 0 3。
& 7 PQ αi O
6 700 90
/0 7 Po of O
¥6 7Sq
∕A
N
，/;，
G c+g cr Q
q「g q Q
,7⅛qD
4 7g Q>o
QQQqQ
U B 9 0
(e2) CVAE
GlαR α Q
Crc-Ooqq
4 ? $ 9 3
Input
(label ref.)
(a2) WynerVAE
/ = 0)
(b2) WynerVAE
(λ = 0.1)
(c2) WynerVAE
(λ = 0.2)
(d2) JVAE
(f2) VCCA-
private

“ 345
a3 夕 5
/，5/ 5
I ΠΛ 3 夕 S
1q"
* q"
Figure 6:	Samples from Wyner VAE and the other models for MNIST-MNIST add-1 dataset. (a1-f1)
Conditional sampling. (a2-f2) Conditional sampling with style control. For both tasks, the leftmost
column denotes the conditioning input to the models.
samples for illustration. We refer the interested reader to Appendix E.2, E.3 for a full comparison.
We defer all the samples from VIB to the appendix, as it only generated same “average” images.
Fig. 6 presents samples of MNIST-MNIST pairs by conditional generation with and without style
control. Fig. 6 (a1-c1) and Fig. 6 (a2-c2) demonstrated how the variations in the generated images
and the style information captured in V are affected by varying λ, respectively. (We illustrate how we
performed conditional generation with style control for CVAE and VCCA-private in Appendix C.)
λ = 0.1 showed the best conditional generation results, consistent with Fig. 5 (a). JVAE generated
images without much variation. VCCA-private erred frequently in guessing the labels, which implies
that the shared representation Z in VCCA-private does not capture the “common information”.
Fig.	7 presents samples of MNIST-SVHN pair from conditional sampling with style control
(Fig. 7 (a,b)) and two variations of joint sampling tasks (Fig. 7 (c-f)). Fig. 7 (c,d) show joint
stochastic reconstruction: Z is inferred from the label reference data, and samples are generated
jointly by drawing local randomness (U, V)〜pθ(u)pθ (v); see Fig. 3 (b). Fig. 7 (e,f) show joint
sampling with style control: similarly to conditional sampling with style control, we generated joint
samples by drawing common randomness Z 〜pθ (Z) with a specified local information from the
style reference. In all cases, we observe that λ = 0.1 achieves better style manipulation over λ = 0,
indicating that λ > 0 helps separating style information from common information.
MNIST quadrant prediction dataset. We performed a quadrant prediction task (Sohn et al., 2015)
with a static, binary MNIST dataset (Larochelle and Murray, 2011), using the Bernoulli observation
model for decoders. Specifically, we split each digit image into two parts into left (X; conditioning)
9
Under review as a conference paper at ICLR 2020
Figure 7: Samples from Wyner VAE for MNIST-SVHN dataset. (a,b) Conditional generation With
style control. (c,d) Joint stochastic reconstruction. (e,f) Joint generation with style control. λ = 0.1
helps local latent variables capture style information, and the generated samples exhibit the effect
compared to λ = 0.
Table 4: Best nll values during 1000 epochs of training for MNIST left-right prediction task. The
conditional nll value for CVAE is taken from Sohn et al. (2015).
	Joint nll	Conditional nll
JVAE (Vedantam et al., 2018)	91.05 ± 0.03	45.22 ± 0.03
Wyner VAE (λ = 0)	87.74 ± 0.05	43.87 ± 0.03
Wyner VAE (λ = 0.15)	89.66 ± 0.03	43.85 ± 0.03
CVAE (Sohn et al., 2015)	-	44.73
and right (Y ; target). In this case, the most succinct common representation Z is a nontrivial object
in contrast to a label (or its bijection) in the previous experiments. Table 4 summarizes negative
log-likelihoods for Wyner VAE, JVAE, and CVAE on the test dataset. Note that Wyner VAE with with
λ = 0 performs best, while an additional regularization parameter λ = 0.15 only gives a marginal
improvement in conditional performance. For this specific case, having the local variables U and
V is enough to capture the local information, as the underlying data seems to have much complex
common information structure compared to a discrete label in the previous cases.
CelebA dataset. We performed an experiment with CelebA dataset (Liu et al., 2015), which is
a degenerate case in the sense that X is a function of Y . While any bijection of X is common
representation Z attaining J (X; Y) in this case, we demonstrated some merits of using Wyner VAE
over CVAE in terms of better common representation learning in Appendix E.4.
5 Concluding Remarks
Cuff’s channel synthesis and Wyner’s distributed simulation are another manifestation of Occam’s
razor by finding the simplest probabilistic structure that connects one random object to another. The
proposed Wyner VAE finds this succinct structure in a disciplined yet efficient manner, and provides
a theoretically sound alternative to the information bottleneck principle. The experimental results
demonstrated the potential of our approach as a new way of learning joint and conditional generation
tasks with optimal representation learning that can be further developed and refined for more complex
dataset such as auditory, text, or a pair of those.
A few remaining questions from experiments are in order. First, this paper does not address how
close the estimated common information from a learned Wyner VAE model is to the true Wyner’s
common information J(X; Y), and it may be interesting to devise a better estimation technique of
Wyner’s common information from data. We emphasize that, however, the main goal of this paper is
to demonstrate the advantages of the probabilistic structure of Wyner VAE and the regularization
parameter λ in various joint and conditional generation tasks. Second, it would be interesting
to investigate the relation between the variational approximation gaps and the quality of learned
representations in Wyner VAE (Cremer et al., 2018; Poole et al., 2019).
10
Under review as a conference paper at ICLR 2020
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. In Int. Conf. Learn. Repr., 2017. URL https://openreview.net/forum?
id=HyxQzBceg.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
J. Am. Stat. Assoc.,112(518):859-877, 2017.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley, New York, second
edition, 2006.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders.
In Proc. Int. Conf. Mach. Learn., pages 1078-1086, 2018.
Paul Cuff. Distributed channel synthesis. IEEE Trans. Inf. Theory, 59(11):7071-7096, November
2013.
Andreas Damianou, Carl Ek, Michalis Titsias, and Neil Lawrence. Manifold relevance determination.
In Proc. Int. Conf. Mach. Learn., 2012.
Vincent Dutordoir, Hugh Salimbeni, James Hensman, and Marc Deisenroth. Gaussian process
conditional density estimation. In Advances in Neural Information Processing Systems, pages
2385-2395, 2018.
Carl Henrik Ek, Jon Rihan, Philip HS Torr, Gregory Rogez, and Neil D Lawrence. Ambiguity
modeling in latent spaces. In Int. Workshop Mach. Learn. Multimodal Interaction, pages 62-73.
Springer, 2008.
Abbas El Gamal and Young-Han Kim. Network Information Theory. Cambridge University Press,
Cambridge, 2011.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a con-
strained variational framework. In Int. Conf. Learn. Repr., 2017. URL https://openreview.
net/forum?id=Sy2fzU9gl.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Int. Conf. Learn. Repr.,
2014. URL https://openreview.net/forum?id=33X9fd2-9FyZd.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Adv. Neural Info. Proc. Syst., pages 3581-3589, 2014.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Int. Conf. Artif.
Int. Stat., pages 29-37, 2011.
Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/
mnist/, 1998.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., pages 3730-3738, 2015.
Lars Maal0e, Casper Kaae S0nderby, S0ren Kaae S0nderby, and Ole Winther. Auxiliary deep
generative models. In Proc. Int. Conf. Mach. Learn., pages 1445-1453, 2016.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
11
Under review as a conference paper at ICLR 2020
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In Proc. Int Conf. Mach. Learn., pages 5171-5180, 2019.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proc. Int.
Conf. Mach. Learn., pages 1530-1538, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proc. Int. Conf. Mach. Learn., pages 1278-
1286, 2014.
Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo method, volume 10. John
Wiley & Sons, 2016.
Mathieu Salzmann, Carl Henrik Ek, Raquel Urtasun, and Trevor Darrell. Factorized orthogonal latent
spaces. In Int. Conf. Artif. Int. Stat., pages 701-708, 2010.
Aaron Shon, Keith Grochow, Aaron Hertzmann, and Rajesh P Rao. Learning shared latent structure
for image synthesis and robotic imitation. In Adv. Neural Info. Proc. Syst., pages 1233-1240, 2006.
Rui Shu, Hung H. Bui, and Mohammad Ghavamzadeh. Bottleneck conditional density estimation. In
Proc. Int. Conf. Mach. Learn., pages 3164-3172, 2017.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep
conditional generative models. In Adv. Neural Info. Proc. Syst., pages 3483-3491, 2015.
Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Joint multimodal learning with deep
generative models. arXiv preprint arXiv:1611.01891, 2016.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. In
Proc. 37th Ann. Allerton Conf. Comm. Control Comput., pages 368-377, 1999.
Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of
visually grounded imagination. In Int. Conf. Learn. Repr., 2018. URL https://openreview.
net/forum?id=HkCsm6lRb.
Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonical correlation
analysis. arXiv preprint arXiv:1610.03454, 2016.
Aaron Wyner. The common information of two dependent random variables. IEEE Trans. Inf. Theory,
21(2):163-179, 1975.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. The information autoencoding family: A
lagrangian perspective on latent variable generative models. In Proc. Uncertain. Artif. Intell., pages
1031-1041, 2018.
12
Under review as a conference paper at ICLR 2020
A A Quick Overview on Variational Autoencoders
Variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014) is a class of
deep generative models that aim to simulate the unknown distribution q(x) underlying the data
x1 , . . . , xN to generate new samples from this distribution efficiently. Let qemp(x) be the empirical
distribution defined by the sample. Assume a generative latent variable model pθ(z)pθ (x|z) to model
the underlying distribution q(x). One of the standard approach to learn each component in the model,
the prior pθ(Z) and the decoder pθ(x|z), is the maximum likelihood approach that aims to solve
N
maximize	log pθ (xi),	(16)
i=1
or equivalently,
minimize DκL(qemp(x)kPθ(x)),	(17)
θ
where pθ(x) is the induced distribution characterized by pθ(z)pθ(x|z), i.e., pθ(x) :=
ppθ(z)pθ(x|z) dz. However, it is often computationally hard to solve the optimization problem
directly due to the induced distribution pθ (x) that involves an integration over a high-dimensional
space.
In variational Bayesian learning approach (see, e.g., (Blei et al., 2017)), an approximate posterior
qφ(z|x) (also called as an encoder) is introduced to relax the objective (16). Here we present a short
derivation of the well-known VAE objective function. Note that the objective in (16) can be upper
bounded as
DKL (qemp(x)kpθ (x)) ≤ DKL^emp (x) kPθ (x)) + EqemP(X) [DκL(qφ(z | X) kPθ (z|X))]	(18)
=DκL(qemp(x)qφ(z∣x)kPθ (x)pθ (z | x))	(19)
=DκL(qemp(x)qφ(z∣x)kPθ (z)Pθ (x | z)),	(20)
where pθ(z|x) is the induced posterior characterized by pθ(z)pθ(x|z). (18) follows from the non-
negativity of the KL divergence, and (19) follows from the chain rule of the KL divergence (see, e.g,
Cover and Thomas (2006)). Note that the final relaxed form (20) does not contain the intractable
term pθ (x). The variational relaxation is tight if and only if DκL(qφ(z∣x)kPθ (ZIX)) = 0 for all x.
The upper bound (20) is the objective function for the standard VAE model (Kingma and Welling,
2014; Rezende et al., 2014). That is, the standard VAE model aims to solve the following optimization
problem:
minimize DκL(qemp(x)qφ(zIx)kpθ(z)pθ(xIz)).	(21)
θ,φ
To express the objective function in a more standard form in the literature, we add a constant
h(qemp(x)), the differential entropy of qemp(x), and then derive
DκL(qemp(x)qφ(zIx)kpθ(z)pθ(xIz)) + h(qemp(x))	(22)
=EqemP(X) DκL(qφ(z∣X)kpθ(Z)) + / qφ(z∣X)log P^X^ dz =: Eqemp(X) [Lθ,φ(X)]. (23)
The loss function Lθ,φ(x) in (23) with negation is called the evidence lower bound (ELBO) in the
literature, since -Lθ,φ(x) lower bounds the evidence log pθ (x) for each x. The KL divergence term
and the expected log loss term are called as the regularization term and the reconstruction term,
respectively.
Assume that the observed variable X is continuous for simplicity. The most standard parameterization
of the components in the VAE model is the diagonal Gaussian parameterization
pθ(z) = N(zI0,diag(σ20,θ)),	(24)
pθ(xIz) = N (xIxθ(z), diag(σθ2(z))),	(25)
qφ(zIx) = N(zIzφ(x),diag(σ2φ(x))),	(26)
where each function may be parameterized by a neural network. Here, σ2 denotes a vector of a proper
dimension and diag(σ2) denotes a diagonal matrix with diagonal entries σ2. Often, the covariance
13
Under review as a conference paper at ICLR 2020
of the prior is taken to be isotropic and constant such as σ20,θ = 1, but σ02,θ can also be set as an
independent trainable parameter as in this work. Note that the diagonal Gaussian parameterization
Pθ(x|z) is a formal modeling assumption to have a tractable density, which is required to evaluate
the log loss logl/pθ(x|z) in the reconstruction loss. This formal Gaussian noise also plays a role
in estimating likelihoods. However, after training, the decoder variance σ2θ (z) is dropped and the
resulting decoder is used as deterministic: z 7→ xθ (z).
With this parameterization, the loss function Lθ,φ (x) in (23) can be estimate efficiently for each x
via a Monte Carlo approximation by sampling Z 〜qφ(z∣x). The overall objective function then
can be minimized with a gradient based optimization algorithm like Adam (Kingma and Ba, 2014)
based on the reparameterization trick (Kingma and Welling, 2014; Rezende and Mohamed, 2015)
and backpropagation.
B S tandard implementation of Wyner VAE
B.1	Gaussian parameterization
As elaborated in Appendix A on VAEs, we use a standard Gaussian parameterization for Wyner VAE
in our experiments. Concretely, we let
pθ(z)	N (zI0, diag(σ20,θ)),	(27)
pθ(U)	N(UI0,diag(σ21,θ)),	(28)
pθ(v)	N (vI0, diag(σ22,θ)),	(29)
Pθ(x|z, u)=	N (xIxθ(z, U), diag(σ2θ(z, U))),	(30)
pθ (y I z, v)=	N (yIyθ(z, v), diag(σ2θ(z, v))),	(31)
qφ(z | x, y)=	N (zIz012,φ(x, y), diag(σ2012,φ(x, y))),	(32)
qφ(u∣z, x)=	N(UIUφ(z,x),diag(σ21,φ(z,x)),	(33)
qφ(v I z, y)=	N (vIvφ(z, y), diag(σ22,φ(z, y)),	(34)
qφ(z|x)=	N (zIz01,φ(x), diag(σ021,φ(x)),	(35)
qφ(zIy)	N (zIz02,φ(y), diag(σ021,φ(y)),	(36)
where each function may be parameterized by a neural network.
B.2	Objective functions
We can rewrite the objective function Lχy→χy in (12) in terms of the expectation of ELBO as in (23):
DKL(qφ(x, y, z, u, v)kpθ(x, y, z, u, v)) + h(q(x, y)) = Eq(x,y) hLrθe,cφ(X,Y) + Lrθe,gφ(X,Y)i,
where
Lθ,φ(X, y) := Eqφ(z∣χ,y) ∕qφ(Uz X)IOg Pd (X1Z, U) du + ∕qφ(V | z, y)log pe(y1z, V) dv ,
Lθeφ(x, y) := DκL(qφ(z∣x, y)qφ(u∣z, x)qφ(v|z, y)kpθ(z)pθ(u)pθ(v))
=DκL(qφ(z | χ, y)kpθ (z))
+ Eqφ(z∣x,y) hDKL(qφ(u | z, x))kPθ (U)) + DKL(qφ(V |Z, y))kPθ(V))i.
We remark that a β-VAE (Higgins et al., 2017) type regularization in our joint model corresponds
to imposing an additional weight β > 1 on Lrθe,gφ (x, y) such that the objective function becomes
Eq(x,y)[Lrθe,cφ(X, Y) + βLrθe,gφ(X, Y)]. We note that
Iq (X, Y; Z, U, V) ≤ Iq (X, Y; Z, U, V)+ DKL 伍 φ(z, U, v)kpθ (z)pθ (u)pθ (v))	(37)
=Eq(x,y)DKL(qφ(z∣X, Y)qφ(u | z, X)qφ(v |z, Y)kpθ (z)pθ (u)pθ (v))] (38)
= Eq(x,y)[Lrθe,gφ(X, Y)].	(39)
14
Under review as a conference paper at ICLR 2020
Therefore, applying β-VAE type regularization imposes an additional regularization on
Iq (X, Y; Z, U, V), which corresponds to the entire latent bottleneck (U, V, Z) for the information
flow from (X, Y).
B.3 Estimation of mutual information
With the typical Gaussian parameterization as presented above, we have an easy estimate for
Iq (X, Y; Z). After training, given a test dataset {xi, yi}iN=1, the mutual information Iq(X, Y; Z)
can be estimated as
Iq(X,Y;Z) = hq(Z) - hq(Z|X, Y)
≈ h(pθ(z)) -	q(x, y)h(qφ(z | x, y)) dz
1N
≈ h(Pθ(Z)) - Nfh(qφ(z∖xiyi))
|Z|	N |Z|
=2 Xlog σ2,θ,j- 2N XX log σ0i2,Φ,j(Xi, yi).	(4O)
j=1	i=1 j=1
We can also estimate the variational upper bound of Iq(X, Y; Z) in (11) as follows:
Iq (X, Y； Z) ≤ I q (X, Y； Z)= Eq(x,y)[DκL d⑶ X, Y)kPθ (z))]
1N
≈ NEDKL(qφ(z∣Xi,yi)kPθ(z)).	(41)
i=1
In the last expression, each KL divergence can be explicitly obtained from the Gaussian parameters
Of qφ(z∣Xi, yi) andp(z).
C A Deeper Look on Related Work
For the standard VAE (Kingma and Welling, 2014; Rezende et al., 2014), we refer the interested
reader to Appendix A. In what follows, we revisit and decompose each VAE-type model into its
encoder/prior/decoder components, and express the objective function in the form of the reverse
KL divergence DKL(qφkpθ), where qφ is the joint distribution over the data variables and the latent
variables defined by the data distribution and the encoders, and pθ is defined by the priors and the
decoders.
JVAE/JMVAE JVAE (Vedantam et al., 2018) and JMVAE (Suzuki et al., 2016) consist of
•	the joint encoder qφ(w∣x, y), the marginal encoders qφ(w∣x),qφ (W 卜)，
•	the prior pθ(w),
•	the decoders pθ(x∣w),pθ(y∣w).1
They share the same joint model objective
DκL(q(χ, y)qφ (w |x, y)kpθ (w)pθ (χ | w)pθ (y | w)),	(42)
but differ in training the marginal encoders. JMVAE trains the marginal encoder qΦ(z|x) via
minimizing
Eq(x,y)[DκL(qφ(w |x, y)kqφ(w |x))]	(43)
together with the joint model objective (42) (by adding two objective functions with an additional
weight as a hyperparameter), while JVAE trains the marginal encoders separately from the joint
model by training the marginal VAE as we proposed, i.e.,
minimize DκL(q(x)qφ(w∣x)∣∣Pθ(w)Pθ(x|w)).	(44)
qφ (w|x)
1We remark that pθ (x|w) and pθ (y|w) are fully characterized by the deterministic decoders xθ (w) and
yθ(w).
15
Under review as a conference paper at ICLR 2020
It is worthwhile to compare the regularization term Iq(X, Y; Z) in Wyner VAE with the idea of β-
VAE (Higgins et al., 2017), which empirically showed that an additional weight on the regularization
term finds a disentangled representation in a VAE model. If we apply a β-VAE type regularization in
our joint model, then it corresponds to an additional weight β > 1 on the mutual information on the
joint representation (Iq (X, Y; Z, U, V)), not only on the common representation (Iq (X, Y; Z)) as
in Wyner VAE. (See Appendix B.2.) In words, Wyner VAE provides a finer control on the information
flow from (X, Y), by manipulating the common path qφ(z∣x, y), while β-VAE blocks the entire
latent bottleneck qφ(z∣x, y)qφ(u∣z, x)qφ(v∣z, y).
CVAE CVAE (Sohn et al., 2015) for modeling q(y|x) consists of
•	the encoder qφ (v|y, x),
•	the prior pθ(v|x),
•	the decoder pθ (y|v, x).
The objective function is then given as
Eq(X) [DκL(q(y ∣x)qφ(v |y, x)kpθ (V ∣x)pθ (y |v, x))].	(45)
Note that without the conditioning variable x, this model boils down to the vanilla VAE for modeling
q(y). If we assume a Markov chain X - V - Y, the decoder pθ(y|v, x) can be replaced with
pθ (y|v).
We performed conditional generation with style control with CVAE as follows: Given a style reference
yo and its corresponding x°, we sample and keep v via the encoder qφ(v∣y, x). Given a new xι,
we take y。/=y(v0, xi) as a new sample. Note that this scheme assumes that v° 〜qφ(v∣y0, x°)
captures only the remaining information in y0 excluding the information on x0 .
SSVAE/ADGM Semi-supervised VAE (SSVAE) (Kingma et al., 2014) proposed a similar con-
ditional VAE model for modeling q(y|x) especially when the conditioning variable x is discrete.
SSVAE consists of
•	the encoders qφ(v∣y, x), qφ(x∣y) (classifier),
•	the priors pθ(v), pθ(x),
•	the decoder pθ (y|v, x).
Note that it has an additional encoder qφ(x∣y) (classifier) on top of qφ(v∣y, x) from CVAE, and as-
sume the priors pθ(v) andpθ(x), which replace the conditional prior pθ(v|x) and the data distribution
q(x) in CVAE, respectively. With a paired (i.e., labelled) data, SSVAE minimizes
DκL(q(χ, y)qφ (V | y, χ)kpθ(χ)pθ N)pθ (y|v, x)).	(46)
With an unlabeled data, SSVAE minimizes
DκL(q(y)qφ(χ∣y)qφ(v |y, x)kpθ (x)pθ (V)pθ(y | v, x)).	(47)
Note that the label information q(x) in (46) is replaced with qφ(x∣y) in (47) as the label information
is missing in the unlabeled data.
Formally, in this degenerate case, identifying the conditioning variable X as the common latent
variable Z in Wyner VAE recovers SSVAE. Yet, SSVAE was proposed in the context of (semi-
supervised) classification problem which aims to learn a classifier qφ(x∣y), and for a high-dimensional
X, it is not feasible to directly model the conditional distribution qφ(x∣y).
Auxiliary deep generative model (ADGM) (Maal0e et al., 2016) adds an auxiliary latent variable Z to
SSVAE to improve the performance. ADGM consists of
•	the encoders qφ(z∣y), qφ(v∣z, y, x), qφ(x∣z, y) (classifier),
•	the priors pθ(v), pθ(x),
•	the decoders p(y|v, x), p(z|v, x, y).
Note the new components qφ(z∣y) and pθ(z|v, x, y) on top of the SSVAE, and the original en-
coder/decoder components have additional condition on the auxiliary variable Z. However, as ADGM
does not impose any additional conditional independence with Z, it is not directly comparable to
Wyner VAE.
16
Under review as a conference paper at ICLR 2020
VCCA-private As noted earlier, VCCA-private (Wang et al., 2016) consists of
•	the encoders qφ(z∣x), (and/or qφ(z∣y)), qφ(u∣x), qφ(v∣y),
•	the priors pθ(z), pθ(u), pθ(v),
•	the decoders pθ(x|z, u), pθ(y|z, v).
Note that the prior and the decoder components are same with Wyner VAE. Hence, the objective for
VCCA-Private for the marginal encoder qφ(z∣x) can be expressed as
DκL(qφ(z∣x)qφ(u∣x)qφ(v∣y)kpθ(u)pθ(v)pθ(z)pθ(x|z, u)pθ(y |z, v)).	(48)
To model the other direction of the marginal encoder qφ(y∣z), they minimize
DκL(qφ(z∣y)qφ(u∣x)qφ(v|y)kpθ(u)pθ(v)pθ(z)pθ(x|z, u)pθ(y |z, v)).	(49)
To learn qφ(z∣x), qφ(z∣y) simultaneously, BiVCCA-Private minimizes a convex combination of the
two KL divergence terms.
We Performed conditional generation with style control with VCCA-Private as follows: Given a style
reference yo, We sample and keep v° via the encoder qφ(v∣y). Given a new attribute xι, we take
y0,1 = yθ(v0, x1) as anew samPle.
VIB VIB (Alemi et al., 2017) proposed a variational relaxation of the following minimization
problem posed by the information bottleneck principle (Tishby et al., 1999):
minimize βIq (X; Z) - Iq (Y; Z),	(50)
qφ(ZIx)
where (X, Y, Z) 〜q(x, y)qφ(z∣x). VIB introduces two variational distributions pθ(z) andpθ(y|z)
that approximate qΦ(z) and ⅞φ(y∣z). Then, based on standard variational bounds on the mutual
information terms (see, e.g., Zhao et al. (2018)), we obtain a variational upper bound on βIq(X; Z) -
Iq(Y;Z)as
β	Eq(x)[D(qφ(z | x)kPθ (z))] - Eq(x,y)qφ(z∣x) log Pd (YIZ) + hSCy)),	(51)
which is the objective function for VIB. Note that the relaxation gap is given by
βD(7φ(z)kPθ (z))+ Egφ(z) [D(qφ(y ∣Z)kPθ (y|Z))].	(52)
D Likelihood Estimation
For each likelihood of our interest, we derive a naive Monte Carlo (MC) estimator and an MC
estimator with importance sampling (see, e.g., Rubinstein and Kroese (2016)). Here we only present
the estimators for Wyner VAE, but the estimators for other models can be derived in the same manner.
D.1 Joint likelihood
We wish to estimate the joint log-likelihood of the model with respect to given
{(x(i),y(i))}iN=1, i.e., PiN=1 log pθ(x(i), y(i)), where
Pθ(x, y) = /Pθ(z)pθ(u)pθ(v)pθ(x∣z, u)pθ(y ∣z, v)dz dudv.
(1)	Monte Carlo estimator: Let (Z(S), U(S), V(S))〜pθ(z)pθ(u)pθ(v) for S = 1,...,S.
1S
Pθ (x, y) = SEpθ (x∣ Z(S), U(S))pθ (y IZ(S), V(S)).
S=1
test data
(53)
(54)
(2)	Importance sampling: For each (x, y), let (Z(S), U(S), V(S))〜 qφ(z∣x, y)qφ(u∣z, x)qφ(v∣z, y)
for s = 1, . . . , S.
)=1 XX Pθ(Z(S))Pθ(U(S))Pθ(V(S))Pθ(x|z(S), U(S))pθ(y|z(S), V(S))
Pp x,y S M	qφ(z(S)|x,y)qφ(U(S)IZ(S),x)qφ(V(S)IZ(S),y)
(55)
17
Under review as a conference paper at ICLR 2020
D.2 Conditional likelihood
We wish to estimate the conditional log-likelihood of the conditional path of Wyner VAE from x to
y, i.e., qφ(z∣x)pθ(v)pθ(y|z, v), With respect to given test data {(x⑴,y(i))}N=ι, i.e.,
N
X log rθ,φ(y(i) |x(i)),	(56)
i=1
Where
rθ,φ(y | X) = / qφ(z∣ x)pθ (V)Pθ (y |z, v)dv dz.	(57)
(1)	Monte Carlo estimation: Given x, let (Z(S), V(S))〜qφ(z∣x)pθ (v) for S = 1,...,S.
1S
rθ,φ(y |x) = S £pe(y IZ(S), V(S)).	(58)
S=1
(2)	Importance sampling: For each (x, y), let (Z(S), V(S))〜qφ(z∣x, y)qφ(v∣z, y) for S = 1,...,S.
1 XX qφ(Z(S)∣x)pθ(V(S))Pθ(y|z(S), V(S))
rθ,φ(y |X) = S Z1	qφ(z(S)∣x, y)qφ(V(S)IZ(S), y)	.
(59)
E	Additional Experimental Results
E.1 Mixture of Gaussians
Here We present the test log-likelihood evaluation of JVAE and VIB in conditional generation tasks of
the mixture of Gaussians (MoG) dataset; recall Fig. 4. Both JVAE and VIB performed Worse as the
training epochs increased, that is, they overfit to the training data (Fig. 8). We visually illustrate hoW
they failed in Fig. 9. Fig. 9(a) shoWs the outlook of the MoG dataset in our experiment. Fig. 9 (e1,e2)
shoW that JVAE captured all the components at the beginning, but then collapsed to a feW components
afterWards. On the other hand, Fig. 9 (f1,f2) shoW that VIB only captured the average behaviors,
although gradually adapting to the underlying data.
Test conditional negative Iog-Iikelihood
# epochs
Figure 8: Conditional nll values of JVAE and VIB for MoG dataset. For each point of the plots, We
trained 10 different models and plotted average values With the shaded region that shoWs the standard
deviation. (TWo largest and smallest outliers Were dropped for each point.)
2All the scatter plots Were generated based on the Gaussian kernel density estimation.
18
Under review as a conference paper at ICLR 2020
(a) Training data
(d1) VCCA-PriVate
after 50 epochs
(f1) VIB (β = 0.1)
after 50 epochs
(CI) CVAE
after 50 epochs
(e1) JVAE
after 50 epochs
(b1) WynerVAE (λ= 0.05)
after 50 epochs
Figure 9: Visualization of conditionally generated samples for MoG dataset. Each axis of the scatter
plots corresponds to the first coordinate of Xi and Yi, respectively.2 The X data points were from
the test data, and the Y data points were generated from the conditional models based on the test
data. One sample was generated for each data point.
E.2 MNIST-MNIST ADD-1
Table 5: Accompanying table for Fig. 5: Summary of numerical evaluations of MNISi-MNIST
add-1 experiments. For each row, we trained 10 different models and dropped two outliers for each
average and standard deviation.
	Joint nll	Conditional nll	Iq(X,Y;Z)	Accuracy (%)	Variance
JVAE (Vedantam et al., 2018)	1173.40 ± 13.98	514.26 ± 12.03	65.89 ± 0.51	98.84 ± 0.11	0.0032 ± 0.0005
Wyner VAE (λ = 0)	1198.95 ± 5.20	172.90 ± 45.18	41.35 ± 0.78	98.01 ± 0.15	0.0254 ± 0.0036
Wyner VAE (λ = 0.05)	1195.09 ± 10.35	58.86 ± 21.78	33.57 ± 0.84	97.46 ± 0.86	0.0353 ± 0.0027
Wyner VAE (λ = 0.10)	1212.66 ± 10.61	-0.51 ± 6.94	24.52 ± 0.89	91.33 ± 1.31	0.0454 ± 0.0009
Wyner VAE (λ = 0.15)	1220.64 ± 10.71	3.35 ± 6.04	19.73 ± 0.78	84.90 ± 2.68	0.0480 ± 0.0006
Wyner VAE (λ = 0.20)	1230.32 ± 9.30	16.37 ± 7.06	16.04 ± 0.50	79.94 ± 2.83	0.0494 ± 0.0006
VCCA-private (Wang et al., 2016)	1254.89 ± 7.17	90.64 ± 5.50	-	59.63 ± 1.25	0.0548 ± 0.0003
CVAE (Sohn et al., 2015)	-	15.39 ± 6.41	-	97.69 ± 0.28	0.0404 ± 0.0004
VIB (Alemi et al., 2017) (β = 0.001)	-	733.86 ± 13.88	-	96.54 ± 0.24	0.0000 ± 0.0000
19
Under review as a conference paper at ICLR 2020
/23夕567gqo
/23夕567gqo
/23夕567gqo
/23夕567gqo
/&345G 7g,。
/a3 4 5g 7go-。
/ S 3 V S 6 7009。
/2345。789。
az7H5 卜。¾3y
7/ qls一 IPt 2lg,l5rl
夕夕 7。99
343U5I363 r
∕23√s∙ 7 8^0
/ 934W6 70⅞ 夕”
/93 25X3 7990
—Λy 3√S 6 7 g70
∙7∕3:TsG7才??
—23γ,l 7 50 夕 O
1J3引，2 3
7Q 3“56 7Q 9。
∕J 3Ts6 7r9〃
l∙ ɛb 3 VʃAu 7^Q/。
I ʒ ¥ ʃ 6 r-3。。
，彳345。7g 9。
/ 4 3 V 5/to 7U3O
/333-66 78 3 0
/4 3ys6 783。
/*34-56729。
b∖λ3⅛丁6 7*3Q
VIB
JVAE
Wyner VAE	Wyner VAE	CVAE	VCCA-Private
(λ = 0.1)	(λ = 0.2)
Figure 10: Conditional generation.
Wyner VAE
(λ = 0.0)
Input
N/A
N/A
Style ref.
QQaqLrqqqqq
3I÷SU8I⅜3
/，，夕ʃl//ʃl夕”
1Q aq SG qa QQ
IqmgQqqqQ
Ql⅜34SG∖l⅞qo
Z23usG
I q 3415 q 7 00 0l Q
+ d 3 4 3, 7⅛qo
/L;二〃
∕a2√sc 7gq。
—£3qsQ Qgq。
Id345b r∙8qo
/4346幺7夕彳O
∕33q5G 7 Da QJ O
/2 34SO 7g G-O
/2 3 Ul ∕fo 7000-0
OW3⅛72q
VIB
JVAE
VCCA-Private
CVAE
Wyner VAE
U = 0. 2)
Wyner VAE
伍=0.1)
Wyner VAE
U = 0.0)
Input
(label ref.)
Figure 11: Conditional generation with style control.
Style ref.
Joint generation With style control
TT£

14 5 2
Ugqa
ʃ H
W
W
a ?名
TTT

1 C S s∖ζ∖2 3 4
ʃ
N/A
Label ref.	Joint stochastic reconstruction
Figure 12: Joint generation.
Wyner VAE
U = 0. 0)
Wyner VAE
U = 0.1)
Wyner VAE
U = 0. 2)
VCCA-Private
JVAE
20
Under review as a conference paper at ICLR 2020
E.3 MNIST-SVHN
Figure 13: Conditional generation.
Input	Wyner VAE	Wyner VAE	Wyner VAE	CVAE
(A = O, O)	伍=0.05)	伍=0.1)
VCCA-Private
JVAE	VIB
Style ref.

'0
I
Input
(label ref.)
Wyner VAE
(A = O. O)
Wyner VAE
(λ = O, O5)
Wyner VAE
(λ = 0.1)
VCCA-Private
JVAE
0
1
0
2
3


Λ
ŋ
7
S
9
6
8

■ O 1 2 * J1- 5 67 S Ml
OV234,∙789
OIZ7 ▲ $6 7bt9

N/A
N/A
Wyner VAE	Wyner VAE	Wyner VAE
(λ = O, O)	伍=0.05)	伍=0.1)
CVAE	VCCA-Private
JVAE
VIB
21
Under review as a conference paper at ICLR 2020
E.4 (Face, attribute) pairs from CelebA dataset
CelebA dataset (Liu et al., 2015) consists of pairs of a face image and a 40-dim. binary vector that
contains attributes information of the face. We performed conditional generation of face images (Y)
given an attribute vector (X). Since an attribute X is a function of a given face image Y, we let the
dimension of the local variable U be 0 in this case. Fig. 16 presents samples of CelebA faces from
Wyner VAE, JVAE, and CVAE — Wyner VAE with λ = 0.1 generated a variety of faces with the
correct attributes, while JVAE generated images with little variations as previously observed and
CVAE generated diverse images but often with wrong attributes. See also Appendix E.4.2 for the
results with style control and a numerical evaluation of the performance of Wyner VAE on CelebA
dataset.
Input attributes
Arched eyebrows
Attractive
Big lips
Brown hair
Bushy eyebrows
Heavy makeup
Mouth slightly open
No beard
Wavy hair
Wearing lipstick
Young
Sample image
from dataset
(a) Wyner VAE
(A = 0)
(b) Wyner VAE
(λ = 0.1)
(c) JVAE
(d) CVAE
Figure 16: Samples from Wyner VAE, JVAE, and CVAE for CelebA dataset. Multiple face image
samples were conditionally sampled given an attribute vector listed at the leftmost column.
Note that this is a special case where the target variable Y is a function of a conditioning variable
X, which is a degenerate case in the sense that any random variable Z that is bijective with X can
serve as an optimal common representation that achieves J(X; Y), and Wyner VAE and CVAE may
have comparable conditional generation performance. In practice, however, Wyner VAE outperforms
CVAE since Wyner VAE learns a good representation Z of X that is helpful in generating Y
conditionally, while CVAE directly uses the raw X for conditioning.
For a qualitative evidence, we present a few Attribute→Face samples in Fig. 17 from models trained
with more data as in the paper, from a truly unseen attribute (likely-female-features + bald)—Wyner
VAE can produce plausible images as it finds a good representation Z of an unseen attribute, while
CVAE fails.
E.4. 1	Numerical evaluation
We present an additional numerical evaluation of Wyner VAE to corroborate the effect of λ >
0. With CelebA models, we evaluated both conditional paths, i.e., face to attribute classification
(face2attribute) and attribute to face generation (attribute2face). For the attribute classification, we
counted the number of corrected classified binary attributes out of 40. The image variance is evaluated
with per pixel, while all pixel values were normalized between 0 and 1. Table 6 summarizes the
results. We can observe that both the accuracy and the variance for conditional generation from a
given attribute were maximized the around λ 〜0.2. Note that JVAE performs worse in classification
22
Under review as a conference paper at ICLR 2020
Table 6: Numerical evaluation of Wyner VAE and JVAE for CelebA dataset.
	Face2attribute accuracy (%) Attribute2face variance
Wyner VAE (λ = 0) Wyner VAE (λ = 0.05) Wyner VAE (λ = 0.1) Wyner VAE (λ = 0.15) Wyner VAE (λ = 0.2) JVAE (Vedantam et al., 2018)	89.26	0.0482 89.21	0.0493 89.29	0.0516 89.32	0.0527 89.28	0.0544 88.11	0.0073
accuracy with a comparably very small per-pixel variance implying much less variations in the
generated samples.
E.4.2 Additional attribute2face generation results
Here we present additional conditional generated samples (with and without style control) from
CelebA models. We used the sample images and attributes shown in Fig. 18 for these experiments.
For conditional generation in Fig. 19, samples were generated only based on the sample attributes.
For conditional generation with style control in Fig. 20, Wyner VAE and CVAE first extracted style
information from sample images in the leftmost column, and then generated new samples from the
original attribute added with a new binary attribute specified in the topmost row for each column.
Hence, the second column corresponds to the reconstruction of the style reference images from the
faces and the corresponding attributes. We remark that JVAE is not capable of style manipulation,
and the results from conditional generation with JVAE are given as a reference.
Sample image
from dataset
Attributes
Arched_Eyebrows, Attractive, Big_Lips, Big_Nose, High_Cheekbones, No_Beard, Pale_Skin, Smiling, Wavy_Hair,
Wearing_Lipstick, Wearing_Necklace
Big_Lips, Blond_Hair, Blurry, Narrow_Eyes, No_Beard, Smiling, Wearing_Lipstick
Bags_Under_Eyes, Eyeglasses, Male, Receding_Hairline, Sideburns, Wearing_Necktie, Young
Arched_Eyebrows, Attractive, Big_Lips, Brown_Hair, Heavy_Makeup, Mouth_Slightly_Open, No_Beard, Smiling, Wavy_Hair,
Wearing_Lipstick, Wearing_Necklace, Young
5_o_Clock_Shadow, Attractive, Bags_Under_Eyes, Big_Nose, Male, Pointy_Nose, Wearing_Necktie
Figure 18: Sample images and their attribute vectors from CelebA dataset.
23
Under review as a conference paper at ICLR 2020
Wyner VAE
（入=0. 0）
Wyner VAE
（入=0.1）
Wyner VAE
（入=0.2）
CVAE
JVAE
Sample image	Conditional generation of CelebA faces
from dataset	from the attribute vector of the sample image
Figure 19: Conditional generation （attribute2face）.
24
Under review as a conference paper at ICLR 2020
Conditional generation of CelebA faces With style tranfer and attribute addition
Added attribute
Wyner VAE
(λ = 0. 0)
Wyner VAE
(λ = 0.1)
Wyner VAE
(λ = 0.2)
Style ref.
CVAE
JVAE
X0-1-⅜
Figure 20: Conditional generation with style control (attribute2face). Note that JVAE is not capable
of style manipulation, and the results were simply generated from attribute2face generation and are
given as a reference. Hence, the leftmost column is the sample image as in the conditional generation
experiment for JVAE.

25
Under review as a conference paper at ICLR 2020
F Experiment Details
We used the same parameterization, same latent dimensions, and the same network architecture
across the different models to be a fair comparison. For simplicity, we used the standard Gaussian
parameterization of each component for all the implemented models as in the standrad VAE. (See
Appendix A and Appendx B.) For the prior distributions pθ(z),pθ(u),pθ(v), we let the isotropic
variances σ20,θ , σ12,θ, σ22,θ be trainable. With this degree of freedom, the neural networks select
necessary dimensions in the latent spaces over Z, U, V by assigning small variances to unused
dimensions. For the MoG experiment, we used a constant 1/2 for the decoder variance, so that the
log-loss corresponds to the l2-squared loss. For the rest of the experiments, we allowed the diagonal
variances to be trainable: we allocated one trainable decoder variance per channel, independent from
the latent inputs. We found that this trick results in sharper images across the models.
We set the dimension of the latent variable W in JVAE as the sum of the dimensions of Z, U, V as
W corresponds the joint representation. Similarly, since the latent variable of CVAE corresponds to
the local randomness in Wyner VAE, we let the dimension of V in CVAE be equal to the dimension
of V in Wyner VAE. We used (10,10,10), (32,32,32), (128,128,128), and (128,0,128) as the latent
dimensions of (Z, U, V) for MoG, MNIST-MNIST add-1, MNIST-SVHN, and CelebA, respectively.
All log-likelihood values in the experiments were estimated by importance sampling; see Appendix D.
We used S = 100 importance samples for each data point. The mutual information Iq(X, Y; Z) was
estimated in a straightforward manner with the test dataset under the Gaussian parameterization; see
Appendix B.
Computing infrastructure We used NVIDIA TITAN X (Pascal) for our experiments.
Implementation We implemented all models using Keras3 with tensorflow backend.
Datasets For the mixture of Gaussians dataset, we generate the paired dataset (Xi, Yi) ∈ R10 ×R10
as follows. Let μ: {1,2, 3,4,5} → R X R be a function defined as
μ⑴=(0,0),μ(2) = (4,4),μ(3) = (-4,4),μ(4) = (-4, -4),μ(5) = (4, -4).
Then, we let
μι(Zi)15 + U/ γ _ 2 Γμ2(Zi)i5 + Vi
μi(Zi)15 - Ui , i	μ2(Zi)15 - Vi
(60)
where Zi ~ Unif({1,2, 3,4, 5}), Ui, Vi ~ N(0,I5) are drawn independently. Here, I5 ∈ R5
denotes the all-1 vector.
For MNIST-MNIST add-1, we constructed 50k add-1 pairs from the MNIST training dataset. For
MNIST-SVHN domain adapation, we constructed 50k MNIST-SVHN pairs from MNIST and SVHN
training datasets. For testing, we similarly constructed 1k paired images from MNIST and SVHN
test datasets in each case. For CelebA experiments, we set aside 5k samples for test dataset, and used
the rest in training.
Network architectures Let c5s1-k-{activation} denote a 5 × 5 Convolution-
BatchNorm-activation with k filters and stride 1 × 1. Let d3s2-k-{activation} /
u3s2-k-{activation} denote 3 × 3 Convolution / Deconvolution-BatchNorm-activation with
k filters and stride 2 × 2, respectively. Let res-k be a residual block that contains two 3 × 3
convolutional layers with k filters in each (i.e., c3s1-k-LReLU, c3s1-k) and a skip connection
from the input to the output. Let fc-k-{activation} be a fully-connected layer with k units
and a non-linear activation.
For MNIST-MNIST add-1, MNIST-SVHN, and CelebA, each Wyner VAE consisted ofan outer
encoder/decoder pair and a core joint Wyner model. The outer encoder/decoder pair was introduced
to pre-process raw input data. We summarized the network architectures for the Wyner models in
Table 7, Table 8, and Table 10. Note that we padded zeros around the 28 × 28 MNIST images to
make them of size 32 × 32.
3https://keras.io
26
Under review as a conference paper at ICLR 2020
Table 7: Network architecture for MoG experiments.
Core Wyner VAE
qφ's	Pθ (xlz, u),Pθ (y|z, V)
fc-256-ReLU	fc-256-ReLU
fc-256-ReLU	fc-256-ReLU
fc-256-ReLU	fc-256-ReLU
(fc-10,fc-10)	fc-10
Table 8: Network architecture for MNIST-MNIST and MNIST-SVHN experiments.
	χ, y	Core Wyner VAE	
Outer encoder	Outer decoder	qφ's	pθ(X|z, u),Pθ(ylz, v)
c5s1-32-LReLU	u3s2-128-LReLU	fc-512-LReLU	fc-512-LReLU
d3s2-64-LReLU	res-128	(fc-32, fc-32) / (fc-128, fc-128)	fc-4096
d3s2-128-LReLU	res-128		
res-128	res-128		
res-128	res-128		
res-128	u3s2-64-LReLU		
res-128	u3s2-32-LReLU		
d3s2-256	c5s1-1-Sigmoid / c5s1-3-Sigmoid		
Table 9: Network architecture for MNIST quadrant prediction experiments.
Core Wyner VAE
qφ(zlx,y)	qφ(ulz,X), qφ(V|z, y)	pθ(X|z, U),pθ(y|z, V)
fc-500-ELU	fc-500-ELU	fc-500-ELU
fc-500-ELU	fc-500-ELU	fc-500-ELU
(fc-20,fc-20)	(fc-15,fc-15)	fc-392
Table 10: Network architecture for CelebA experiments.
X (attribute)		y (image)		Core Wyner VAE	
Outer encoder	Outer decoder	Outer encoder	Outer decoder	qφ's	pθ (X|z, u), pθ (y|z, V)
fc-512-LReLU fc-512	fc-512-LReLU fc-40-Sigmoid	c5s1-32-LReLU d3s2-64-LReLU d3s2-128-LReLU res-128 res-128 res-128 res-128 d3s2-256	u3s2-128-LReLU res-128 res-128 res-128 res-128 u3s2-64-LReLU u3s2-32-LReLU c5s1-3-Sigmoid	fc-1024-LReLU fc-1024-LReLU fc-1024-LReLU (fc-256, fc-256)	fc-1024-LReLU fc-1024-LReLU fc-1024-LReLU fc-512 / fc-16384
Training We used the Adam optimizer (Kingma and Ba, 2014) with learning rate 10-4 in training
MoG, MNIST-MNIST, MNIST-SVHN and CelebaA models. We used learning rate 5 ∙ 10-4 in
training MNIST quadrant prediction experiment.
For MoG dataset, we trained each model for 500 epochs with batch size 100 and trained each marginal
encoder for separate 50 epochs for JVAE and Wyner VAE. For MNIST-MNIST, MNIST-SVHN,
and CelebA experiment, each model was trained for 100 epochs with batch size 128. For MNIST
quadrant prediction experiment, each model was trained for 1000 epochs with batch size 20.
For the marginal encoder qφ(z∣x) in JVAE or Wyner VAE, we trained the joint models by setting
αx→x = αxy→y = αxy→x = αy→y = 0 and trained only the marginal encoder for every 50
epochs by minimizing Lχ→χ with freezing all the components in thejoint model for 20 epochs. For
MNIST-MNIST, MNIST-SVHN, and CelebA experiments, we trained qφ(z∣x) for 1 epoch after
every 1 epoch of the joint model training as suggested in Vedantam et al. (2018). Note that the outer
encoder/decoder pairs in Tables 8 and 10 were trained with the core joint Wyner model, and they
were fixed in the marginal encoder training. For MNIST quadrant prediction experiment, we trained
thejoint model and the marginal encoders qφ(z∣x) and qφ(z∣y) by minimizing the final objective (15)
by setting all α's to be 1 for simplicity without fine-tuning.
We empirically observed that for some experiments the two-stage training with αxy→y = αxy→x = 0
is still effective. Note that we applied the joint training scheme and used αxy→y , αxy→x > 0 only for
MNIST quadrant prediction experiment to achieve a better conditional log-likelihood performance.
27