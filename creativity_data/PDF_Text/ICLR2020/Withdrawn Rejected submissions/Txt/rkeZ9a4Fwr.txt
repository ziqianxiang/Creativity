Under review as a conference paper at ICLR 2020
Disentangling Improves VAEs’ Robustness to
Adversarial Attacks
Anonymous authors
Paper under double-blind review
Ab stract
This paper is concerned with the robustness of VAEs to adversarial attacks. We
highlight that conventional VAEs are brittle under attack but that methods recently
introduced for disentanglement such as β-TCVAE (Chen et al., 2018) improve
robustness, as demonstrated through a variety of previously proposed adversarial
attacks (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)).
This motivated us to develop Seatbelt-VAE, a new hierarchical disentangled VAE
that is designed to be significantly more robust to adversarial attacks than existing
approaches, while retaining high quality reconstructions.
1	Introduction
Unsupervised learning of disentangled latent variables in generative models remains an open research
problem, as is an exact mathematical definition of disentangling (Higgins et al., 2018). Intuitively, a
disentangled generative model has a one-to-one correspondence between each input dimension of the
generator and some interpretable aspect of the data generated.
For VAE-derived models (Kingma & Welling, 2013; Rezende et al., 2014) this is often based around
rewarding independence between latent variables. Factor VAE (Kim & Mnih, 2018), β-TCVAE
(Chen et al., 2018) and HFVAE (Esmaeili et al., 2019) have shown that the evidence lower bound
can be decomposed to obtain a term capturing the degree of independence between latent variables
of the model, the total correlation. By up-weighting this term, we can obtain better disentangled
representations under various metrics compared to β-VAEs (Higgins et al., 2017a).
Disentangled representations, much like PCA or factor analysis, are not only human-interpretable but
also offer more informative and robust latent space representations. In addition, information theoretic
interpretations of deep learning show that having a disentangled hidden layer within a discriminative
deep learning model increases robustness to adversarial attack (Alemi et al., 2017).
Adversarial attacks on deep generative models, more difficult than those on discriminative models
(Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018), attempt to fool a model into
reconstructing a chosen target image by adding distortions to the original input image. Generally,
the most effective attack mode involves making the latent-space representation of the distorted input
match that of the target image (Gondim-Ribeiro et al., 2018; Kos et al., 2018). This kind of attack is
particularly relevant to applications where the encoder’s output is used downstream.
Projections of data from VAEs, disentangled or not, are used for tasks such as: text classification
(Xu et al., 2017); discrete optimisation (Kusner et al., 2017); image compression (Theis et al., 2017;
Townsend et al., 2019); and as the perceptual part of a reinforcement learning algorithm (Ha &
Schmidhuber, 2018; Higgins et al., 2017b), the latter of which uses a disentangled VAE’s encoder to
improve the robustness of the agent to domain shift.
Here we demonstrate that β-TCVAEs are significantly more robust to ‘latent-space’ attack than
standard VAEs, and are generally more robust to attacks that act to maximise the evidence lower
bound for the adversarial input. The robustness of these disentangled models is highly relevant
because of the use-cases for VAEs highlighted above.
However, imposing additional disentangling constraints on a VAE training objective degrades the
quality of resulting drawn or reconstructed images (Higgins et al., 2017a; Chen et al., 2018). We
sought whether more powerful, expressive models, can help ameliorate this and in doing so built
1
Under review as a conference paper at ICLR 2020
(a) Vanilla VAE
(b) β-TCVAE	(c) β-TCDLGM, L = 5 (d) Seatbelt-VAE, L = 5
Original Original rec. Adversarial	Original Original rec. Adversarial	Original Original rec. Adversarial	Original Original rec. Adversarial
Target Adversarial rec. Distortion	Target Adversarial πβc. Distortion	Target Adversarial rec. Distortion	Target Adversarial ibc. Distortion
(e) Vanilla VAE
(f) β-TCVAE	(g) β-TCDLGM, L = 3 (h) Seatbelt-VAE, L = 3
(i) Vanilla VAE
(j) β-TCVAE	(k) β-TCDLGM, L = 4 (l) Seatbelt-VAE, L = 4
Figure 1: Latent-space adversarial attacks on Chairs, 3D Faces and CelebA for different models,
including our proposed Seatbelt-VAE. β = 10 for β-TCVAE, β-TCDLGM and Seatbelt-VAE. L is the
number of stochastic layers. Clockwise within each plot we show the initial input, its reconstruction,
the adversarial input, the adversarial distortion added to make it (shown normalised), the adversarial
input’s reconstruction, and the target image. Following Tabacof et al. (2016); Gondim-Ribeiro et al.
(2018) we attack with different degrees of penalisation on the magnitude of the adversarial distortion;
in choosing the distortion to show, we pick the one with the penalisation that resulted in the value of
the attack objective just better than the mean. See Section 5 for more details.
a hierarchical disentangled VAE, Seatbelt-VAE, drawing on works like Ladder VAEs (S0nderby
et al., 2016) and BIVA (Maal0e et al., 2019). We demonstrate that Seatbelt-VAEs are more robust to
adversarial attacks than β-TCVAEs and β-TCDLGMs (the latter a simple generalisation we make of
β-TC penalisation to hierarchical VAEs). See Figure 1 for a demonstration.
Rather than being concerned with human-interpretable controlled generation by our models, which
has been the focus of much research into disentangling, instead we are interested in the robustness
afforded by disentangled representations.
Thus our key contributions are:
•	A demonstration that β-TCVAEs are significantly more robust to adversarial attacks via
their latents than vanilla VAEs.
•	The introduction of Seatbelt-VAE, a hierarchical version of the β-TCVAE, designed to
further increase robustness to various types of adversarial attack, while also giving better
perceptual quality of reconstructions even when regularised.
2	Variational Autoencoders
Variational autoencoders (VAEs) are a deep extension of factor analysis suitable for high-dimensional
data like images (Kingma & Welling, 2013; Rezende et al., 2014). They have a joint distribution
2
Under review as a conference paper at ICLR 2020
over data X and latent variables z: pθ(x, Z) = pθ(x∣z)p(z) where p(z) = N(0,I) and pθ(x|z) is
an appropriate distribution given the form of the data, the parameters of which are represented by
deep nets with parameters θ. As exact inference is intractable for this model, in a VAE we perform
amortised stochastic variational inference. By introducing an approximate posterior distribution
qφ(z|x) = N(μφ(χ), Σφ(χ)), We can perform gradient ascent on the evidence lower bound (ELBO)
L(X) = -DκL(qφ(z∣x)∣∣Pθ(x,z)) = Eqφ(z∣x) logPθ(x|z) - Dkl(qφ(z|x)∣∣p(z)) ≥ logp(x)
w.r.t.both θ and φ jointly, using the reparameterisation trick to take gradients through Monte Carlo
samples from qφ(z∣x).
2.1	Disentangling VAEs
In a β-VAE (Higgins et al., 2017a), a free parameter β multiplies the DKL term in L(X) above. This
objective Lβ (X) remains a lower bound on the evidence.
Decompositions of L(X) shed light on its meaning. As shown in Hoffman & Johnson (2016);
Makhzani et al. (2016); Kim & Mnih (2018); Chen et al. (2018); Esmaeili et al. (2019), one can
define the evidence lower bound not per data-point, but instead write it over a dataset D of size N,
D = {Xn}, so we have L(θ, φ, D).
Esmaeili et al. (2019) gives a decomposition of this dataset-level evidence lower bound:
L(θ, Φ,D) = - DκL(qφ(z,x)∣∣Pθ(x,z))
=Eq…[log Pθ⅞M - log X
qφ(z,X) L 3 Pθ(x)	3 qφ(z)
S---V----} '---{---}
① ②
(1)
-DKL(q(x)||pe(X)) - DKL(q0(z)||p(Z))
V---------{------} `--------{-------'
③ ④
(2)
where under the assumption thatP(Z) factorises we can further decompose ④:
dklSφ(Z)IIp(Z))
Eqφ(z) log
(qφ(Zj)IIp(Zj))
_ - /
{z
⑥
(3)
where j indexes over coordinates in z. qφ(Z, x) = qφ(Z∣χ)q(χ) and q(χ) := N PN=I δ(x - Xn)
is the empirical data distribution. qφ(Z) := NN PN=I qφ(Z∣xn) is called the average encoding
distribution following Hoffman & Johnson (2016).
@ is the total correlation (TC) for qΦ(Z), a generalisation of mutual information to multiple variables
(Watanabe, 1960). With this mean-field p(Z), Factor and β-TCVAEs upweight this term, so we have
an objective:
LeTC (θ,φ,D)=① + ② + ③ + ® + β (A	(4)
Chen et al. (2018) gives a differentiable, stochastic approximation to Eqφ(z) log qφ(Z), rendering
this decomposition simple to use as a training objective using stochastic gradient descent. We also
note that @, the total correlation, is also the objective in Independent Component Analysis (Bell &
Sejnowski, 1995; Roberts & Everson, 2001).
2.2	Hierarchical VAEs
We now have a set of L layers of Z variables: z = [Z1, Z2, ..., ZL]. The evidence lower bound for
models of this form is:
LDLGM(θ, φ, D) = Eqφ(z,χ) log PP(x,z) = Eqφ(z,χ)[logpθ(x|z)] - Eq(χ)[DκL(qφ(z,x)∣∣pθ(z))]
qφ(z, X)
(5)
The simplest VAE with a hierarchy of conditional stochastic variables in the generative model is the
Deep Latent Gaussian Model (DLGM) of Rezende et al. (2014). The forward model factorises as a
chain:
L-1
pP(X, z) = pP(XIZ1) Y pP(ZiIZi+1)p(ZL)	(6)
i=1
3
Under review as a conference paper at ICLR 2020
Each Pθ(zi∣zi+1) is a Gaussian distribution with mean and variance parameterised by deep nets.
p(zL ) is a unit isotropic Gaussian.
We can understand this additional expressive power as coming from having a richer family of
distributions for the likelihood over data X marginalising out all intermediate layers: pθ(x|zL)=
R QiL=-11 dzi pθ (x, z) is a non-Gaussian, highly flexible, distribution.
To perform amortised variational inference one introduces a recognition network, which can be any
directed acyclic graph where each node, each distribution over each zi , is Gaussian conditioned on
its parents. This could be a chain, as in Rezende et al. (2014):
L-1
qφ(z∣χ) = Y qφ(zi+1∣zi)qφ(z1∣χ)	⑺
i=1
Again, marginalising out intermediate Zi layers, We see qφ(zL∣χ) = RQL-LI dzi qφ(z∣x) is a
non-Gaussian, highly flexible, distribution.
However, training DLGMs is challenging: the latent variables furthest from the data can fail to learn
anything informative (S0nderby et al., 2016; Zhao et al., 2017). Due to the factorisation of qφ(z∣χ)
andpθ(x, z) in a DLGM, it is possible for a single-layer VAE to train in isolation within a hierarchical
model: eachpθ(zi∣zi+1) distribution can become a fixed distribution not depending on zi+1 such
that each DKL divergence present in the objective between corresponding zi layers can still be driven
to a local minima. Zhao et al. (2017) gives a proof of this separation for the case where the model is
perfectly trained, i.e. DκL(qφ(z,χ)∖∖pθ(χ,z)) = 0.
This is the hierarchical version of the collapse of z units in a single-layer VAE (Burda et al., 2016),
but now the collapse is over entire layers Zi. It is part of the motivation for the Ladder VAE (S0nderby
et al., 2016) and BIVA (Maal0e et al., 2019).
3 SEATBELT-VAE: HIERARCHICAL β-TCVAE WITH SKIP CONNECTIONS
(a) Generative
Model
Figure 2: L = 2 Seatbelt-VAE.
Shaded lines indicate β-TC fac-
torisation in a given node.
pθ(x, z) = pθ(x∖z) Y pθ(Zi∖Zi+1)p(ZL)	(8)
i=1
(b) Approx.
Posterior
We propose novel hierarchical disentangled VAEs where we aim
to disentangle only in the top-most latent variables ZL . Following
the Factor and β-TCVAEs we upweight the term of the form of
@ for zl. Empirically we find models of this type are unable
to converge when disentangling at the bottom most layer, or
when disentangling at each layer. Intuitively, we want to capture
high-level disentangled information at the top, but leave lower
layers free to learn rich entangled representations. Ifpθ(x∖z) =
pθ(x∖Z1), we obtain the generalisation of β-TC penalisation to a
DLGM and call it β-TCDLGM. It suffers from the problems of
collapse described above.
Inspired by BIVA (Maal0e et al., 2019), we choose instead to
condition our likelihood on all Zi layers:
Combining Eqs (7, 5, 8) and applying β-TC penalisation to the DKL term over ZL :
4
Under review as a conference paper at ICLR 2020
LSB(θ, φ, D, β) = Eqφ(z,x) logPθ(XIZ)- Eq(X) log q(x) - Eq(x,z2) [DκL(qφ(z1 |x)∣∣Pθ(z1∣z2))]
L-1
-	X Eqφ(zm-ι,zm+i)[DκL(qφ(ZTzmT)帆(zm∣zm+1))]
m=2
-	DκL(qφ(zL,zL-1)||qφ(zL)qφ(zL-1)) - βDκL(qφ(zL)∣∣ Y qφ(zL))
j=1
-	X DKL Sφ(ZL)IIp(ZL))	⑼
j
=Eqφ(z,x) logpθ (X∣Z) - ©	(10)
where j is indexing over the coordinates in zL . See Appendix for the derivation. We call this model
Seatbelt-VAE, as with the extra conditional dependencies and nodes we increase the safety of our
model to adversarial attacks, to noise, and to decreases in perceptual quality as β increases. We
find that using free-bits regularisation (Kingma et al., 2016) greatly ameliorates the optimisation
challenges associated with DLGMs. For L = 1 this reduces to a β-TCVAE, and for L > 1, β = 1 it
produces a DLGM with our augmented likelihood function.
For completeness, note that for β-TCDLGM:
LeTCDLGM(θ, φ, D, β) = Eqφ(z,χ) logpθ(χ∣z1) - ©	(11)
3.1	Minibatch Training
VAEs and derived models are commonly trained using stochastic gradient ascent on the ELBO, on
minibatches of the training data. With the ELBO in Eq (9), this would be challenging because of the
presence of average encoding distributions, which depend on the entire dataset.
To avoid having to handle large mixture distributions in our objective functions, we derive minibatch
estimators that are a simple generalisation to disentangled hierarchical VAEs of the Minibatch
Weighted Sampling estimator proposed in Chen et al. (2018) in the context of β-TCVAEs. See
Appendix for further details.
4	Robustness of VAEs to Adversarial Attacks
Most adversarial attack research has focused on discriminative models (Akhtar & Mian, 2018; Gilmer
et al., 2018) and recently VAEs have found use in protecting discriminative models against attack
(Schott et al., 2019; Ghosh et al., 2019). Currently, two adversarial modes have been proposed for
attacking VAEs (Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018). In both attack
modes the adversary wants draws from the model to be close to a target image xt, when given a
distorted image x* = X + d as input. When attacking a discriminative model the aim is to manipulate
the comparatively low-dimensional output layer of the network, commonly aiming with the attack
to diminish or increase only a handful of the output units. However, for a generative model, the
attacker is aiming to change a large number of pixel values in the output, changing the content of the
reconstruction. Intuitively this is a harder task, and the attacks proposed in the above papers do not
always result in adversarial examples that are very close to the initial image in appearance.
The first mode of attack, which we call the output attack, aims to reward draws from the decoder
conditioned on z 〜qφ(z∣χ*) that are close to Xt via the ELBO.
For a vanilla VAE, this attack’s adversarial objective is:
∆output(x, d, xt; λ) = Eqφ(z∣χ+d)[logp(xt∣z)] - DκL(qφ(z∣x + d)∣∣p(z)) + λ∣∣d∣∣	(12)
The second mode of attack, the latent attack, aims to find X* = X + d such that qφ(zIX*) ≈ qφ(zIXt)
under some similarity measure r(∙, ∙), which implicitly means that the likelihoodpθ(XtIz) is high
when conditioned on draws from the posterior of the adversarial example. This attack is important if
5
Under review as a conference paper at ICLR 2020
one is concerned with using the encoder network of a VAE as part of downstream task. For a single
stochastic layer VAE, the latent-space adversarial objective is:
△latent(x, d, xt； λ) = r(qφ(z∣x + d), qφ(z∣xt)) + λ∣∣d∣∣	(13)
Note that both modes of attack penalise the L2 norm of d, prioritising smaller distortions. We denote
samples from qφ(z∣χ + d) as Z.
For Tabacof et al. (2016); Gondim-Ribeiro et al. (2018) r(∙, ∙) is DκL(qφ(z∣x + d)∣∣qφ(z∣x)) and
for Kos et al. (2018) it is the L? distance ||Z - z*∣∣2,Z 〜qφ(z∣χ + d), z* 〜qφ(z∣χ) between draws
from the corresponding posteriors or ∣∣μφ(x) - μφ(x + d) ∣∣2 between their means. We follow the
former papers and use the DKL formulation. All three papers find that the latent attack mode is as
or more effective than the output attack for single layer VAEs both under perceptual evaluation and
various proposed metrics (Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018).
For latent attacks, the choice of which layers to attack depends on model architecture. For DLGMs
and β-TCDLGMs the attacker only needs to match at the bottom latent layer as pθ(x|z) = pθ(χ∣z1),
see Eq (7). See Appendix for plots showing how effective this attack is regardless of β and L.
Even though the decoder is conditioned on all latent layers, one could choose to attack individual
layers for Seatbelt-VAE. For example, one could attack just the first layer z1. If one were able to
find a perfect latent-space attack in z1, DκL(qφ(z1 |x + d)∖∖qφ(z1∖χt')) = 0, then the variational
posteriors in higher layers would also be well matched. Attacks that do not perfectly match the target
z 1 may have their mismatch with the target posterior amplified in higher layers. In Seatbelt-VAE the
likelihood over data is conditioned on all z layers, being off-target in these higher layers matters. In
the Appendix we show that targeting the top or base layers individually is not as effective as attacking
all layers. Hence:
ΔD⅛gm(x, d, xt； λ) =DκL(qφ(z1∖x + d), qφ(z1∖xt)) + λ∖∖d∖∖	(14)
L
∆SaBent(x, d,xt; λ) = X DκL(qφ(zi∖x + d),qφ(zi∖xt)) + λ∖∖d∖∖	(15)
i=1
5	Experiments
Here we perform four tranches of experiments. Firstly, we demonstrate that the reconstructions
given by Seatbelt-VAEs (and β-TCDLGMs) degrade much less strongly as β is increased than in
β-TCVAEs. Secondly, we perform a variety of adversarial attacks on all models. We demonstrate
that increasing β makes β-TCVAEs more robust to adversarial attacks than vanilla VAEs, and that
Seatbelt-VAEs are more robust still. Thirdly, we show that these disentangled models are most robust
than vanilla VAEs to unstructured noise distorting their inputs, with Seatbelt-VAEs again the most
robust. Finally, we study the effect of disentangling on the sparsity of model weights.
We perform these experiments on Chairs (Aubry et al., 2014), 3D faces (Paysan et al., 2009), and
CelebA (Liu et al., 2015). Additional results for dSprites (Higgins et al., 2017a) can be found in the
Appendix. We used the same encoder and decoder architectures as Chen et al. (2018) for each dataset.
For the details of neural network architectures and training, see Appendix and accompanying code.
To show the degree to which our models are disentangling, the Appendix also contains the Mutual
Information Gap (MIG) (Chen et al., 2018) at the top layer of each model. Though our models obtain
high MIG at zL, this does not imply that decoding from latent traversals in zL will result in the
generation of images with human-interpretable factors of variation. This is made abundantly clear
in the latent space traversal plots, also shown in the Appendix. As such, we do not believe existing
disentangling metrics directly apply to hierarchical models.
5.1	ELBO AND RECONSTRUCTION QUALITY: β-TCVAES TO SEATBELT-VAES
We trained β-TCVAEs, β-TCDLGMs, and Seatbelt-VAEs for a range of β penalisations. In Figure 3
we plot the final ELBO of our trained models, but calculated without the additional β penalisation
that was applied during training. The ELBO for β-TCVAE [Eq (4)] declines with β much more
quickly than Seatbelt VAEs [Eq (10)] or β-TCDLGMs [Eq (11)]. In the Appendix we also show
that increasing β reduces DκL collapse. This is interesting, as it shows that we can increase the β
6
Under review as a conference paper at ICLR 2020
penalisation for Seatbelt-VAEs, without a large degradation in the quality of the model as measured
by the ELBO.
In Figure 4 we see the effect of depth and disentangling on reconstructions of CelebA. The bottom row,
showing the reconstructions from a Seatbelt-VAE with L = 4 and β = 20 clearly maintains facial
identity better than those from a β-TCVAE in the middle row. The effect is clearest for the 3rd , 4th
and 7th columns, where many of the individuals’ finer facial features are lost by the β-TCVAE but
maintained by the Seatbelt-VAE. This fits with the results in Figure 3, and shows that resistance of the
quality of the reconstructions of Seatbelt to increasing β is visually perceptible as well as measurable.
β	P
(a) Chairs ELBO	(b) 3D Faces ELBO
Figure 3: Plots showing the effect of varying β under various datasets on the ELBO of β-TCVAEs,
β-TCDLGMs and Seatbelt-VAEs [Eqs (4), (11) and (10) respectively]. Shading corresponds to the
95% CI over variation due to variation of ||z|| and L.
Figure 4: Top row shows CelebA input data. Below are reconstructions from β-TCVAE, β = 20 and
then Seatbelt VAE, L = 4, β = 20.
4 3
O O
SSO-Iraμesjφ>pq
(a) Chairs Losses	(b) 3D Faces Losses
Figure 5: ∆iatent∕output for (a) Chairs (b) 3D Faces, for β-TCVAE for different β values. Shading
corresponds to the 95% CI over variation due to our stable of images and our values of ||z|| and λ.
5.2	Adversarial Attack
We apply attacks minimising each of ∆output and ∆latent on: vanilla VAEs, β-TCVAEs, β-
TCDLGMs and Seatbelt-VAEs; trained on: Chairs (Aubry et al., 2014), 3D faces (Paysan et al.,
2009), and CelebA (Liu et al., 2015); for a range of β, L and λ values.
7
Under review as a conference paper at ICLR 2020
We randomly sampled 10 input-target pairs for each dataset. As in Tabacof et al. (2016); Gondim-
Ribeiro et al. (2018), for each pair of images used λ takes 50 geometrically-distributed values from
2-20 to 220. Thus each model undergoes 500 attacks for each attack mode. Like Tabacof et al. (2016);
Gondim-Ribeiro et al. (2018), we used L-BFGS-B for gradient descent (Byrd et al., 1995),
We prefer to avoid classifier based metrics (Kos et al., 2018) as in general we think that such
analysis can be hard to interpret given the many available choices of classifier. Instead, we evaluate
the effectiveness of adversarial attacks from the values reached by - logpθ(χt∣Z), by the attack
objectives {∆output , ∆latent } and by visually appraising the adversarial input (x + d) and the
adversarial reconstruction. Note that higher values of - logpθ(χt∣Z), ∆output, ∆iatent indicate less
effective attacks.
Figure 1 shows latent space attacks and demonstrates that they are less effective on disentangled
models. As in Gondim-Ribeiro et al. (2018), we are showing the attack for the λ that gives us an attack
objective just better than the average objective over all attacks tried. Note that for Seatbelt-VAEs, for
high values of β and L latent attacks often result in the outputs from adversarial attack resembling
the original inputs. See Appendix for more examples of the attacks for {∆latent, ∆output} for the
models trained on dSprites (a toy dataset for disentangling), Chairs, 3D Faces and CelebA; each over
a range of values for β , L, and λ. Note that we rarely observe perceptually effective output attacks
regardless of model or settings, though vanilla VAEs are the most susceptible.
One might expect that adversarial attacks targeting a single factor of the data would be easier for the
attacker. However, we find that disentangled models protect effectively against these attacks as well.
See the Appendix for plots showing an attacker attempting to rotate a dSprites heart.
Figure 5 quantitatively shows that β-TCVAEs become harder to attack as β increases. The values of
∆latent for β-TCVAEs are ≈ 103 times higher than for a standard VAE on Chairs, and still greater
than a factor of 10 for 3D faces. ∆output attack is also less effective, by a smaller factor ≈ 1.2.
Figure 6 shows - log pθ (xt∣Zgtent/OUtPUt) and Figure 7 shows ∆gtent/OUtPUt over a range of datasets
for β-TCDLGMs and Seatbelt-VAEs, varying L and β. Larger values of these metrics correspond to
less successful adversarial attacks. Generally, β-TCDLGMs are very sensitive to latent attack, as we
expect. Like β-TCVAEs, Seatbelt-VAEs offer significant protection to latent attacks, and somewhat
increased protection to output attacks compared to vanilla VAEs. For Seatbelt-VAEs, as we go to the
largest values of β and L for both Chairs and 3D Faces, ∆latent grows by a factor of ≈ 107.
The bottom rows of Figures 6 & 7 (c) (d) have L = 1, and thus correspond to β-TCVAEs. They
contain relatively low values of the adversarial objectives compared to L > 1. Similarly the first
column, corresponding to β=1 models, contains relatively low values. These results tell us that depth
and disentangling together offer the most effective protection from the adversarial attacks studied.
In the Appendix we also calculate the L2 distance between target images and adversarial outputs and
show that the loss of effectiveness of adversarial attacks is not due to the degradation of reconstruction
quality from increasing β. By these metrics too Seatbelt-VAEs outperform other models.
5.3	Robustness to Noise
In addition to studying the robustness of these models to highly structured distortion, we can also
consider robustness to random noise. We add E 〜N(0, I) to the datasets, which are scaled to
-1 ≤ X ≤ 1, and then evaluate Eqφ(z∣χ+e) pθ(x|z*), where z* corresponds to the encoder embedding
of x + E and x is the original (non-noisy) data. See Figure 8 for smoothed histogram plots of this for
different models for different degrees of β. Both β-TC and Seatbelt-VAEs are effectively denoising
autoencoders. They become more robust to noise with increasing β, while β-TCDLGMs get worse.
See Appendix for plots showing the robustness of these models to smaller magnitude noise.
Some of the robustness of disentangled models to adversarial attacks may be conferred by their
robustness to random perturbations of their inputs.
5.4	Total Correlation Penalisation as Regularisation
In the auto-encoder view of these models, the DKL terms in L(θ, φ, D) are associated with a form
of regularisation of the model (Doersch, 2016). Recent work shows that for linear autoencoders,
8
Under review as a conference paper at ICLR 2020
1.95
1.80
1.65
1.50
1.35
(a) 3D Faces
1 2 4 6 8 10
β
(b) Chairs
(d) Chairs
(c) 3D Faces
Figure 6: - logpθ (Xt ⑶ for (a) (b) β-TCDLGMS and (C) (d) Seatbelt-VAES for Chairs and 3D Faces;
over β and L (total number of stochastic layers) values and for latent and output attacks. Larger
values of — log pθ (xt∣Z) correspond to less successful adversarial attacks.
BB	β	β
(a) 3D Faces	(b) Chairs
(c) 3D Faces	(d) Chairs
Figure 7:	{∆latent,∆output} for (a) (b) β-TCDLGMs and (c) (d) Seatbelt-VAEs for Chairs and 3D
Faces; under varying β and L (total number of stochastic layers) values.
9
Under review as a conference paper at ICLR 2020
(a) β-TCVAE
(b) β-TCDLGM
(c) Seatbelt-VAE
Figure 8:	Robustness of logpθ(x|z) to Gaussian noise e 〜N(0,1) scaled by different magnitudes
and added to x on CelebA; for β-TCVAE, β-TCDLGM, Seatbelt-VAE; β = 0, 10 Best viewed
digitally.
Table 1: Relative change of the L2 of Encoders and Decoders by dataset for β-TCVAE and Seatbelt-
VAE (L = 4) when increasing β from 1 to 10.
		Chairs	3D Faces	CelebA
		β :1 → 10	β : 1 → 10	β : 1 → 10
Encoder	β-TCVAE	+5.0%	+19.5%	+73.7%
	Seatbelt-VAE, L = 4	+ 1.0%	+2.7%	+40.2%
Decoder	β-TCVAE	-19.4%	-15.0%	-6.8%
	Seatbelt-VAE, L = 4	-7.6%	-6.0%	-11.4%
L2 regularisation of the weights corresponds to orthogonality of the latent projections (Kunin et al.,
2019). For deep models we expect that disentangling is associated with regularised decoders and more
complex encoders. The decoder receives a simpler representation, but building this representation
requires more calculation. Here we measure the L2 norm of the weights of our networks as a function
of β, shown in Table 1. See Appendix for results for β-TCDLGM.
As we increase β for β-TCVAEs and Seatbelt-VAEs for Chairs, 3D Faces, and CelebA the L2 norm
increases for the encoder and decreases for the decoder. A more complex encoder is more difficult to
match in the latent space and regularised decoders may be contributing to the denoising properties
seen in Figure 8. That the changes are generally greater for β-TCVAE than Seatbelt-VAE makes
sense, as the encoder and decoder of the former interact directly with the disentangled representation.
For the latter the decoder receives inputs from all zi , of varying degrees of disentanglement.
6 Conclusion
We have presented the increases in robustness to adversarial attack afforded by β-TCVAEs. This
increase in robustness is strongest for attacks via the latent space. While disentangled models are
often motivated by their ability to provide interpretable conditional generation, many use cases for
VAEs centre on the learnt latent representation of data. Given the use of these representations as
inputs for other tasks, the latent attack mode is the most important to protect against.
Recent work by Shamir et al. (2019) gives a constructive proof for the existence of adversarial inputs
for deep neural network classifiers with small Hamming distances. The proof holds with deterministic
defence procedures that work as additional deterministic layers of the networks, and in the presence
of adversarial training (Szegedy et al., 2014; Ganin et al., 2016; Tramer et al., 2018; Shaham et al.,
2018). Shamir et al. (2019) thus give a theoretical grounding for using stochastic methods to defend
against adversarial inputs. As VAEs are already used to defend deep net classifiers (Schott et al.,
2019; Ghosh et al., 2019), more robust VAEs, like β-TCVAEs, could find use in this area.
We introduce Seatbelt-VAE, a particular hierarchical VAE disentangled on the top-most layer with
skip connections down to the decoder. This model further increases robustness to adversarial attacks,
while also increasing the quality of reconstructions. The performance of our model under adversarial
attack to robustness is mirrored in robustness to uncorrelated noise: these models are effective
denoising autoencoders as well. We hope this work stimulates further interest in defending and
attacking VAEs.
10
Under review as a conference paper at ICLR 2020
References
Naveed Akhtar and Ajmal Mian. Threat of Adversarial Attacks on Deep Learning in
Computer Vision: A Survey. IEEE Access, 6:14410-14430, 2018. ISSN 21693536.
doi:10.1109/ACCESS.2018.2807385.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep Variational Information
Bottleneck. In ICLR, 2017. ISBN 1612.00410v5. URL https://arxiv.org/pdf/1612.
00410.pdfhttp://arxiv.org/abs/1612.00410.
Mathieu Aubry, Daniel Maturana, Alexei A Efros, Bryan C Russell, and Josef Sivic. Seeing 3D chairs:
Exemplar part-based 2D-3D alignment using a large dataset of CAD models. In Proceedings of the
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3762-3769,
2014. ISBN 9781479951178. doi:10.1109/CVPR.2014.487. URL https://www.di.ens.
fr/willow/research/seeing3Dchairs/texts/Aubry14.pdf.
Anthony J Bell and Terrence J Sejnowski. An information-maximisation approach to blind separation
and blind deconvolution. Neural Computation, 7(6):1004-1034, 1995. URL http://www.inf.
fu-berlin.de/lehre/WS05/Mustererkennung/infomax/infomax.pdf.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. In ICLR,
2016. URL https://arxiv.org/pdf/1509.00519.pdf.
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A Limited Memory Algorithm
for Bound Constrained Optimization. SIAM J. Sci. Comput., 16(5):1190-1208, 9 1995. ISSN
1064-8275. doi:10.1137/0916069. URL http://dx.doi.org/10.1137/0916069.
Ricky T Q Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating Sources of Disentan-
glement in Variational Autoencoders. In NeurIPS, 2018. URL https://arxiv.org/pdf/
1802.04942.pdfhttp://arxiv.org/abs/1802.04942.
Carl Doersch. Tutorial on Variational Autoencoders. Technical report, Carnegie Mellon University,
2016. URL https://arxiv.org/pdf/1606.05908.pdfhttp://arxiv.org/abs/
1606.05908.
Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, N Siddharth, Brooks Paige, Dana H Brooks,
Jennifer Dy, and Jan-Willem van de Meent. Structured Disentangled Representations. In AISTATS,
2019. URL https://arxiv.org/pdf/1804.02086.pdfhttp://arxiv.org/abs/
1804.02086.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, Victor Lempitsky, Urun Dogan, Marius Kloft, Francesco Orabona,
and Tatiana Tommasi. Domain-Adversarial Training of Neural Networks. Journal of Machine
Learning Research, 17:1-35, 2016. URL https://arxiv.org/pdf/1505.07818.pdf.
Partha Ghosh, Arpan Losalka, and Michael J Black. Resisting Adversarial Attacks Using Gaussian
Mixture Variational Autoencoders. In AAAI, 2019. URL www.aaai.org.
Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E Dahl. Motivating the
Rules of the Game for Adversarial Example Research. CoRR, 2018. URL https://arxiv.
org/pdf/1807.06732.pdfhttp://arxiv.org/abs/1807.06732.
George Gondim-Ribeiro, Pedro Tabacof, and Eduardo Valle. Adversarial Attacks on Variational
Autoencoders. CoRR, 2018. URL https://arxiv.org/pdf/1806.04646.pdf.
David Ha and Jurgen Schmidhuber. World Models. In NeurIPS,2018. doi:10.5281/zenodo.1207631.
URL https://worldmodels.github.iohttp://arxiv.org/abs/1803.10122%
0Ahttp://dx.doi.org/10.5281/zenodo.1207631.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning Basic Visual Concepts with a
Constrained Variational Framework. In ICRL, 2017a. doi:10.1177/1078087408328050. URL
http://journals.sagepub.com/doi/10.1177/1078087408328050.
11
Under review as a conference paper at ICLR 2020
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: Improving Zero-Shot Transfer in
Reinforcement Learning. In ICML, 2017b. URL https://arxiv.org/pdf/1707.08475.
pdf.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner Deepmind. Towards a Definition of Disentangled Representations. CoRR,
2018. URL https://arxiv.org/pdf/1812.02230.pdf.
Matthew D Hoffman and Matthew J Johnson. ELBO surgery: yet another way to carve up the varia-
tional evidence lower bound. In NeurIPS, 2016. URL http://approximateinference.
org/accepted/HoffmanJohnson2016.pdf.
Hyunjik Kim and Andriy Mnih. Disentangling by Factorising. In NeurIPS, 2018. URL https:
//arxiv.org/pdf/1802.05983.pdf.
Diederik P Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimisation. In ICLR,
2015. URL https://arxiv.org/pdf/1412.6980.pdf.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In NeurIPS, 2013. ISBN
1312.6114v10. doi:10.1051/0004-6361/201527329. URL http://arxiv.org/abs/1312.
6114.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improving Variational Inference with Inverse Autoregressive Flow. In NeurIPS, 2016. ISBN
9781611970685. URL https://arxiv.org/pdf/1606.04934.pdf.
J Kos, I Fischer, and D Song. Adversarial Examples for Generative Models. In IEEE Security and
Privacy Workshops,pp. 36-42, 5 2018. doi:10.1109/SPW.2018.00014.
Tejas D Kulkarni, Will Whitney, Pushmeet Kohli, and Joshua B Tenenbaum. Deep Convolutional
Inverse Graphics Network. In NeurIPS, 2015. doi:10.1063/1.4914407. URL https://arxiv.
org/pdf/1503.03167.pdfhttp://arxiv.org/abs/1503.03167.
Daniel Kunin, Jonathan M Bloom, Aleksandrina Goeva, and Cotton Seed. Loss Landscapes of
Regularized Linear Autoencoders. In ICML, 2019. URL https://arxiv.org/pdf/1901.
08168.pdf.
Matt J Kusner, Brooks Paige, and Jose Miguel Hernandez-Lobato. Grammar Variational Autoencoder.
In ICML, 2017. URL http://opensmiles.org/spec/open-smiles-2-grammar.
html.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Lars Maal0e, Marco Fraccaro, Valentin Lievin, and Ole Winther. BIVA: A Very Deep Hierarchy of
Latent Variables for Generative Modeling. CoRR, 2019. URL https://arxiv.org/pdf/
1902.02102.pdf.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
Autoencoders. In ICLR, 2016. ISBN 0928-4931. doi:10.1016/j.msec.2012.07.027. URL https:
//arxiv.org/pdf/1511.05644.pdf.
Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3D face
model for pose and illumination invariant face recognition. In 6th IEEE International Conference
on Advanced Video and Signal Based Surveillance, AVSS 2009, pp. 296-301, 2009. ISBN
9780769537184. doi:10.1109/AVSS.2009.58. URL http://faces.cs.unibas.ch/.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and
Approximate Inference in Deep Generative Models. In ICML, 2014. ISBN 9781634393973.
doi:10.1051/0004-6361/201527329. URL https://arxiv.org/pdf/1401.4082.pdf.
12
Under review as a conference paper at ICLR 2020
S Roberts and R Everson. Independent Component Analysis: Principles and Practice. Cambridge
University Press, 2001. ISBN 9780521792981. URL https://books.google.at/books?
id=LLLNxrKQiPkC.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Toward the First Adversarially
Robust Neural Network Model on MNIST. In ICLR, 2019. URL https://arxiv.org/pdf/
1805.09190.pdf.
Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of supervised models through robust optimization. NeurocomPuting, 307:195-204,
2018. ISSN 18728286. doi:10.1016/j.neucom.2018.04.027. URL https://arxiv.org/
pdf/1511.05432.pdf.
Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman. A Simple Explanation for the Existence
of Adversarial Examples with Small Hamming Distance. CoRR, 2019. URL https://github.
com/anishathalye/obfuscated-gradients.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Lad-
der Variational Autoencoders. In NeurIPS, 2016. URL https://arxiv.org/pdf/1602.
02282.pdf.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. CoRR, 2014. URL https://arxiv.
org/pdf/1312.6199.pdf.
Pedro Tabacof, Julia Tavares, and Eduardo Valle. Adversarial Images for Variational Autoencoders.
In NIPS WorkshoP on Adversarial Training, 2016. URL https://arxiv.org/pdf/1612.
00155.pdfhttp://arxiv.org/abs/1612.00155.
Lucas Theis, Wenzhe Shi, Andrew CUnningham&, and Ferenc HUSZ疝.Lossy Image Compression
with Compressive Autoencoders. In ICLR, 2017. URL https://arxiv.org/pdf/1703.
00395.pdf.
James Townsend, Tom Bird, and David Barber. Practical Lossless Compression with Latent Vari-
ables using Bits Back Coding. ICLR, 2019. URL https://github.com/bits-back/
bits-back.http://arxiv.org/abs/1901.04866.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick MC-
Daniel. Ensemble Adversarial Training: Attacks and Defenses. In ICLR, 2018. URL https:
//arxiv.org/pdf/1705.07204.pdfhttp://arxiv.org/abs/1705.07204.
Satosi Watanabe. Information Theoretical Analysis of Multivariate Correlation. IBM Journal of
Research and DeveloPment, 4(1):66-82, 1960. ISSN 0018-8646. doi:10.1147/rd.41.0066.
Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan. Variational Autoencoder for Semi-supervised
Text Classification. In AAAI, pp. 3358-3364, 2017. ISBN 9781450329569. doi:10.1051/0004-
6361/201527329.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning Hierarchical Features from Generative
Models. In ICML, 2017. URL https://arxiv.org/pdf/1702.08396.pdf.
13
Under review as a conference paper at ICLR 2020
Supplementary Material for: Disentangling
Improves VAEs’ Robustness to Adversarial At-
TACKS
Anonymous authors
Paper under double-blind review
Contents
A Derivation of ELBO for Seatbelt-VAEs	3
B Minibatch Weighted Sampling for zi	5
B.1 MWS for qφ(z1): β-TCVAEs .................................................. 5
B.2 Minibatch Weighted Sampling for qφ(zi), i > 1: β-TCGLGMs and Seatbelt-VAEs	5
C Implementation Details	7
C.1 Encoder and Decoder Architectures ......................................... 7
D L2 norm of β-TCDLGM	7
E Robustness to Noise	8
F Activation of z	9
F.1 β-TCVAEs .................................................... 9
F.2 Seatbelt-VAEs ................................................................ 12
G Aggregate Analysis of Adversarial Attack	15
G.1	β-TCVAE .............................................................. 15
G.2	β-TCDLGMs ............................................................ 16
G.3	Seatbelt-VAEs ........................................................ 17
G.4	Seatbelt-VAE layerwise attacks ....................................... 18
H Adversarial Attack Plots	19
H.1	dSprites Adversarial Attack on a Single Factor ....................... 19
H.2	dSprites Adversarial Attack .......................................... 20
H.2.1	β-TCVAEs ...................................................... 20
H.2.2	β-TCDLGMs ..................................................... 22
H.2.3	Seatbelt-VAEs ................................................. 24
H.3	Chairs Adversarial Attack ............................................ 26
H.3.1	β-TCVAEs ...................................................... 26
1
Under review as a conference paper at ICLR 2020
H.3.2 β-TCDLGMs .................................................... 28
H.3.3 Seatbelt-VAEs ................................................ 30
H.4	3D Faces Adversarial Attack ....................................... 32
H.4.1 β-TCVAEs ..................................................... 32
H.4.2 β-TCDLGMs .................................................... 34
H.4.3 Seatbelt-VAEs ................................................ 36
H.5	CelebA Adversarial Attack ......................................... 38
H.5.1 β-TCVAEs ..................................................... 38
H.5.2 β-TCDLGMs .................................................... 38
H.5.3 Seatbelt-VAEs ................................................ 38
I Data Generation from Models	39
J Mutual Information Gap	41
2
Under review as a conference paper at ICLR 2020
A Derivation of ELBO for Seatbelt-VAEs
Start with Eq (5) cf. Eq (7) in the main paper. The likelihood is conditioned on all Z layers: pθ(x|z).
L(θ, φ, D) = Eqφ(z,x) log
Pθ(x, z)
log qφ(z,x)
Eqφ(z,x) [log pθ (XIZ)] - Eq(X) [DKL(qφ(z, X)Mpθ (Z))]
(A.1)
pθ(z)
=Eq(z,x) lθgPθ (X∣Z) — Eq(X) log q(x) + Eq(z,x) log ;3⑻
=Eq(z,x) log pθ (x|z) + H(q(χ))
(A.2)
(A.3)
/
+
L
dx dz1 Y(dziqφ(zi∣ziT))qφ(z1∣x)q(x)log
i=2
P(ZL QL-IPθ(ZkIzk+1)
qφ(ZIIX)Qm=I qφ(zm+1lzm)
z
W = Z dx Ymzi)qφ(zlx)q(X)IOg q°(PLzzL-1)
X---------------------------------------'
{z^
©
+ Z dx Y(dzi)qφ(zlx)q(X)Iog qφ(ziQ⅛⅞Xm+ζm),
X---------------------------------------------------}
(A.4)
①=- Eqo(zLT)DKL(q°(ZLIzLT)||P(ZL))
(A.5)
® = Z dX Y(dzi)qφ(z∣X)q(X)log P[[∣X))
'----------------------------------/
L-1	L
+	dX	(dZi)qφ(zIX)q(X) log
m=2	i=1
X--------------------------------
pθ(ZmIZm+1)
qφ(ZmIZm-1)
(A.6)
^{z
®
}
Ra) = — Eqφ(ζ2,χ) DκL(qφ(ζ1∣x)∣∣Pθ(z1∣z2))
L-1	L	L-1
(Rb) = X IdX Y(dzi)qφ(z1∣X)q(X)	Y	(qφ(zk+1 ∣zk))qφ(zm∣zm-1) log
m=2	i=1	k=1,k6=m
(A.7)
pθ (zm∣zm+1)
qφ(zm∣zm-1)
(A.8)
L-1	L	L-1
—	X	dX Y(dZi)qφ(Z1 IX)q(X)	Y (qφ(Zk+1IZk))DKL(qφ(ZmIZm-1)IIPθ(ZmIZm+1))
m=2	i=1	k=1,k6=m
(A.9)
L-1
—	X Eqφ(zm+1,zm-1) DKL(qφ(ZmIZm-1)IIPθ(ZmIZm+1))	(A.10)
m=2
3
Under review as a conference paper at ICLR 2020
Now we have:
L(θ,Φ,D) = Eq(z,x) logPθ(x|z) + H(q(x)) + (Ra) + (Rb) + ①
(A.11)
(A.12)
Apply βTC decomposition to ① as in Chen et al. (2018). j indexes over units in zl.
© = - Eqφ(zLT) [ Eqφ(zL∣zL-i)[lθg qφ (zLIZLT) - log P(ZL) + log qφ(ZL)
- log qφ(ZL) +logYqφ(ZjL) - logYqφ(ZjL)]
j
j
(A.13)
- Eqφ(zL,zL-1 ) [log
- Eqφ(zL,zL-1) [log
-DKL(qφ(zL, zL-
qφ(zLlzLT)]	[l	qφ(ZL) ] _E	[l	Qjqφ(zj ) ]
qφ(zL	] - Eqφ(ZL) [lOg Qj qφ(zL) ] - Eqφ(ZL) [lOg P(ZL ]
(A.14)
,qφ(zLlzLT)qφ(ZLT)]	n	qφ(ZL) ] XF rloff qΦ(zL)1
qφ(zL)qφ(zL-1)	]-	jj] - j )^ ^]
(A.15)
1)Hqφ(ZL)qφ(ZLT))-DKL(q°(ZL)|| Y qφ(ZL))- X dklSφ(ZL)HP(ZL))
j
八-------------
|
}
(A.16)
Where we have used p(zL) = Qj p(zjL) for our chosen generative model. As in Chen et al. (2018),
we choose to weight (Tb), the total correlation for qφ(zL) by a prefactor β.
LSB(θ, φ, D, β) = Eq(z,x) logPθ(x|z) + H(q(x)) + (Ra) + (Rb) + ® + β(Tb) + (To (A.。)
Giving us the ELBO for Seatbelt-VAEs, Eq (10).
4
Under review as a conference paper at ICLR 2020
B MINIBATCH WEIGHTED SAMPLING FOR zi
As in Chen et al. (2018), applying β-TC decomposition requires us to calculate terms of the form:
Eqφ(zi) log qφ(zi)	(B.1)
The i = 1 case is covered in the appendix of Chen et al. (2018). First we will repeat the argument for
i = 1 as made in Chen et al. (2018), but in our notation, and then we cover the case i > 1 for models
with factorisation of qφ(z∣x) as in Eq 7 in the main paper.
B.1	MWS FOR qφ(z1): β-TCVAES
Introduce BM = {x1, x2, ..., xM}, a minibatch of datapoints drawn uniformly iid from q(x) =
1/N PnN=I δ(x - xn). For for any minibatch we have P(BM)= 克M. Chen et al. (2018) introduce
r(BM |x), the probability of a sampled minibatch given that one member is x and the remaining
M - 1 points are sampled iid from q(x), so r(BM |x) = NM 1.
Eqφ(zi) log qφ(Zi) = Eqφ(zi,x)[log Eq(x) 0φ(ZIIx)]]
1M
=Eqφ(z1,x) [log Ep(BM )[而 5S qΦ(z1 lxm )]]
m=1
p(BM) 1 M 1
≥ Eiqφ(^z1 X^)[l-og Er(BM |x) [ r(B—∣χ) M ^X qφ(Z Ixm)]]
M	m=1
1M
Eqφ(z1,x) [log Er(BM |x) [NM〉： qφ(Z Ixm)]]
m=1
(B.2)
(B.3)
(B.4)
(B.5)
(B.6)
So then during training, one samples a minibatch {x1, x2, ..., xM} and can estimate Eqφ(z1) log qφ(Z1)
as:
1M M
Eqφ(zi) logq。(ZI) ≈ M X[logXq。(ZIIxj) - logNM	(B.7)
i=1	j=1
and Zi is a sample from qφ(Z1∣xi).
B.2	Minibatch Weighted Sampling for qφ(Zi), i > 1: β-TCGLGMs and
Seatbelt-VAEs
Here we have that q(z,x) = QL=2[qφ(Zl∣Zl-1)]qφ(Z1Ix)q(x). Now instead of having a minibatch of
datapoints, we have a minibatch of draws of Zi-1: BMi-1 = {Z1i-1, Z2i-1, ..., ZMi-1}. Each member of
which is the result of sequentially sampling along a chain, starting with some particular datapoint
xm 〜q(x).
For i > 2, members of BMi-1 are drawn:
Zji-1 〜qφ(Zi-1IZj-2)	(B.8)
and for i = 2:
Zj 〜qφ(Z1Ixj)	(B.9)
Thus each member of this batch BMi-1 is the descendant of a particular datapoint that was sampled
in an iid minibatch BM as defined above. We similarly define r(BMi-1 IZi-1) as the probability of
selecting a particular minibatch BMi-1 of these values out from our set {Zni-1} (of cardinality N)
given that we have selected into our minibatch one particular Zi-1 from these N values. Like above,
r(BM-1IZi-1) = N M —1
5
Under review as a conference paper at ICLR 2020
Now we can consider Eqφ(zi) log qφ(zi) for i > 1:
Eqφ(zi) log qφ(zi) = Eqφ(zi,zi-1) [log Eqφ(z1 ) 0φ3 |Zi-1 )]]	(B/0)
1M
=Eqφ(zi,zi-i)[logEp(Bi-I)[mm E qφ(zi∣zm-1)]]	(B.11)
m=1
≥ Eqφ (Zi,ziT) [log Er(BM-1∣zi-1)[ SB jtMX) M X qφ(ZiIzm-I)]]	(B.12)
r M x m=1
1M
=Eqφ(zi,zi-1)[log Er(BM-1∣zi-1)[ NM Σ qφ (ZIzm 1)]]	(B.13)
m=1
Where we have followed the same steps as in the previous subsection.
During training, one samples a minibatch {z1i-1, z2i-1, ..., zMi-1}, where each is constructed by
sampling ancestrally. Then one can estimate Eqφ(zi) log qφ(zi) as:
1M	M
Eqφ(zi) logqφ(zi) ≈ 方 X[logX qφ(zkIzj-1) - logNM]	(B.14)
M k=1	j=1
and zk is a sample from qφ(zi∣zk-1). In our model we only need terms of this form for i = L, so we
have:
1M	M
Eqφ(.ζL) logqφ(zL) ≈ M X[logXqφ(zLlzLT)- lognm]	(B.15)
k=1	j=1
and zL is a sample from qφ(zL∣zL-1).
6
Under review as a conference paper at ICLR 2020
C Implementation Details
All runs were done on the Azure cloud system on NC6 GPU machines.
C.1 Encoder and Decoder Architectures
We used the same architectures as Chen et al.	2018.
See	file	src/stochastic_layers/encoders.py	and
src/stochastic_layers/decoders.py in the accompanying repository.
For β-TCVAE the range of ||z|| values used was {4, 6, 8, 16, 32, 64, 128}. For β-TCDLGMs and
Seatbelt-VAEs the number of units in each layer zi decreases sequentially. There is a list zsizes for
each dataset, and for a model of L layers that the last L entries to give ||zi||, i ∈ {1, ..., L}.
{||z||}dSprites ={96, 48,	24,	12, 6}	(C.1)
{||z||}Chairs ={96, 48,	24,	12, 6}	(C.2)
{||z||}3DFaces ={96, 48,	24,	12, 6}	(C.3)
{||z||}CelebA ={256, 128, 64, 32}	(C.4)
(C.5)
For β-TCDLGMs and Seatbelt-VAEs we also have the mappings qφ(zi+1 |zi) andpθ(zi∣zi+1). These
are amortised as MLPs with 2 hidden layers with batchnorm and Leaky-ReLU activation. The
dimensionality of the hidden layers also decreases as a function of layer index i:
∣∣h∣∣(qφ(zi+1 ∣Zi)) =hsizes[i]	(C.6)
∣∣h∣∣(pθ (zi∣zi+1)) =hsizes[i]	(C.7)
hsizes = [1024, 512, 256, 128, 64]	(C.8)
To train the model we used ADAM (Kingma & Ba, 2015) with default parameters and a learning
rate of 0.001. All data was preprocessed to fall on the interval -1 to 1. CelebA and Chairs were both
downsampled and cropped as in (Chen et al., 2018) and (Kulkarni et al., 2015) respectively.
D L2 NORM OF β-TCDLGM
Table D.1: L2 of Encoders and Decoders by dataset for β-TCDLGM (L = 4) showing the proportional
change from increasing β from 1 to 10.
	I Chairs	I 3D Faces	I CelebA
	I β :1 → 10	I β : 1 → 10	I β : 1 → 10
Encoder	β-TCDLGM I +3.1%	I~+5.7%~	I	+1.22%
Decoder	β-TCDLGM I	-4.8%	I~-3.5%~	I~+7.3%~
7
Under review as a conference paper at ICLR 2020
E Robustness to Noise
(a) Chairs β-TCVAE
(b) Chairs β-TCDLGM
(c) Chairs Seatbelt-VAE
(d) 3D Faces β-TCVAE
(e) 3D Faces β-TCDLGM
(f) 3D Faces Seatbelt-VAE
∕og(x∣z*) β=l
0.0025
0.0020
0.0015
0.0010
0.0005
0.0000
0.0025
0.0020
0.0015
0.0010
0.0005
0.0000
0.0025
0.0020
0.0015
0.0010
0.0005
0.0000
-2000	0	2000
∕og(x∣z*) β=10
(g)	CelebA β-TCVAE
(h)	CelebA β-TCDLGM
(i)	CelebA Seatbelt-VAE
Figure E.1: Robustness of logp(x|z) to Gaussian noise e 〜N(0,1) scaled by different magnitudes
and added to x: on Chairs, 3D Faces and CelebA by row; for β-TCVAE, β-TCDLGM, Seatbelt-VAE
by column. Within each plot a range of β values are shown. Best viewed digitally
8
Under review as a conference paper at ICLR 2020
F Activation of Z
F.1 β-TCVAES
For this subsection. within each subplot We order the units of Z by the values of their DKL divergence.
dSprites
5
0
β=ι,l∣z∣l=4
片 8J∣z∣∣=4
5
5
ll⅛
5
1MB
5
i!≡8
0
0
0
0
llMll=δ
5
5
5
o mill IlM
°	β=1,∣∣z∣∣=32
°	β=4,∣∣z∣∣=32
0
lIiIL
5
0
β=10,∣∣z =32
5
5
5
5
0
IlllL
β=1,∣∣z∣∣=64
0
0
β=10,∣∣z∣∣=64
5
0
l⅛OT28
5
0
ll1现IiK
0
β=8,∣∣z∣∣=64
5
5
β=8,∣∣z∣∣=128
0
Illll
β=10, ∣z =128
Figure F.2: Eq(X) DκL(qφ(zj∖x)∖∖p(zj)) over dSprites for β-TCVAE over values of ||z|| and β. Best
viewed digitally.
9
Under review as a conference paper at ICLR 2020
5
5
5
°	β=8,∣∣z∣∣=4
5
0

β=4,∣∣z∣∣=4
5
°	β=4,∣∣z∣∣=8
0
Ilii
β=8,∣∣z∣∣=8
0	β=10,∣∣z∣∣=8
5
5
iii≡!!16
° β=4,∣∣z∣∣=16
5
Iiiiiiiiiiiiii
β=8,∣∣z∣∣=16
5
0
0
0 β=10,∣∣z∣∣=16
5
5
5
5
0
IIIiiiiM
β=1,∣∣z∣∣=64
0
llN!Su64
0
β=8,∣∣z∣∣=64
0
O5
z∣∣=64
5
0
∣∣∣∣∣∣∣∣∣∣∣∣∣.
β=1,∣∣z∣∣=128
5
0
IIW28
5
0
HIlI
β=8,∣∣z∣∣=128
5
0
Iiiiiiii
0,
∣z∣∣=128
Figure F.3: Eq(X) DκL(qφ(zj∖χ)∖∖p(zj)) over Chairs for e-TCVAE over values of ||z|| and β. Best
viewed digitally.
10
Under review as a conference paper at ICLR 2020
3D Faces
∩lllllll∣∣
° β=8,∣∣z∣∣=16
β=10,∣∣z∣∣=4
β=10,∣∣z∣∣=8
β=10,∣∣z∣∣=16
0
Figure F.4: Eq(X) DKL(q≠(zj∖x)∖∖p(zj)) over 3D Faces for e-TCVAE over values of ||z|| and β. Best
viewed digitally.
11
Under review as a conference paper at ICLR 2020
F.2 Seatbelt-VAEs
Here the DKL divergences are calculated per layer. dSprites
β=8,L=2
Figure F.5: Eiqφ^zi-ι疗十1)DκL(qφ(zi∣zi-1)∣∣p(zi∣zi+1)) where X = z0 andp(zL∣zL+1) = P(ZL),
over dSprites for Seatbelt-VAEs over values of L and β . Best viewed digitally.
12
Under review as a conference paper at ICLR 2020
Chairs
30
20
10
0
β=10,L=3
30
20
10
0
β=10,L=4
Figure F.6: Eiqφ^zi-ι疗十1)DκL(qφ(zi∣zi-1)∣∣p(zi∣zi+1)) where X = z0 andp(zL∣zL+1) = P(ZL),
over Chairs for Seatbelt-VAEs over values of L and β. Best viewed digitally.
13
Under review as a conference paper at ICLR 2020
3D Faces
Figure F.7: Eiqφ^zi-ι疗十1)DκL(qφ(zi∣zi-1)∣∣p(zi∣zi+1)) where X = z0 andp(zL∣zL+1) = P(ZL),
over 3D Faces for Seatbelt-VAEs over values of L and β. Best viewed digitally.
14
Under review as a conference paper at ICLR 2020
G Aggregate Analysis of Adversarial Attack
G.1 β-TCVAE
----- Latent Space Attack
-----Output Attack
φ 6.0
I 5.5
□
850
φ
E 4.5
(D
S)
(CO 4.0
3.5
10
5
β
5
β
(b) dSprites Losses
1 2	4	6	8	10
β
1 2	4	6 θ 10
β
----- Latent Space Attack
-----Output Attack
(a) dSprites Distances
(c) Chairs Distances
SSo-IC5μα5sj(υ>p4
70
6 5 4
Ooo
111
30
1 2	4	6	8	10
1 2	4	6 θ 10
β
Latent Spa∞ Z
Output Attack
(e) 3D Faces Distances
(d) Chairs Losses
(f) 3D Faces Losses
Figure G.8: Plots showing the effect of varying β in a β-TCVAE for dSprites and 3D Faces on:
(a),(d) the L2 distance from xt to its reconstruction when given as input and the L2 distance from the
adversarial input x* and its reconstruction; (b),(e) the adversarial objectives ∆iatnet∕output
15
Under review as a conference paper at ICLR 2020
G.2 β-TCDLGMS
4 3 2
=UOOeaJePBl=
depth
(c) dSprites Negative Log Likelihood
(a)	dSprites Distances
(b)	dSprites Losses
Latent X104	OUtPUt	X103
1 2 4 6 8 10
β
g qcoe
I4dφp
1 2 4 6 8 10
β
(d) Chairs Distances
1.530
300
3
depth
2.40
<D
O
re 2.38
S
⊂2.36
8
⅛2.34
⅛
P2.32
I-
2.30
depth
2	3	4	5
(e) Chairs Losses
β	β
(h) 3D Faces Losses
(f) Chairs Negative Log Likelihood
(i) 3D Faces Negative Log Likelihood
(g) 3D Faces Distances
Figure G.9: Plots showing the effect of varying L on β-TCDLGMs for dSprites 3D Faces and
Chairs, on: (a),(d),(g) the L2 distance between xt and its reconstruction when given as input and the
same between the adversarial input x* and its reconstruction; (b),(e),(h) the adversarial objectives
△output/image; (c),(f),(i) - logPθ(xt∣z), Z 〜qφ(z|x*) and the MIG.
For a DLGM (Rezende et al., 2014) with 2-5 Z layers, with qφ(z|x) factorised as in Eq (7), pθ(x, Z)
factorised as in Eq (6), and βTC penalisation applied to the top layer, we find that latent attacks
targeted at Z 1 are highly effective and remain so as L and β each increase. These models are, however,
slightly more robust to output attacks and this attack becomes less effective as β increases, but more
effective as L increases.
The ease of attacking via Z 1 is consistent with its separation out from the rest of the model.
16
Under review as a conference paper at ICLR 2020
G.3 Seatbelt-VAEs
Oo
--φPBl-LBμBSK>pv--
2000
2
depth
----- Latent Space Attack
Output Attack
2	3	4	5
depth
Latent XIolI
2.5
1.5
1.0
0.5
1 2 4 6 8 10
β
20
l0.0
--φPBl-LBμBSK>pv- _
Output ×103
5.6
<θ
4.0
3.2
2.4
1.6
1 2 4 6 8 10
β
(b) dSprites Losses
(a) dSprites Distances
650
625
600
2	3	4	5
2	3	4
depth
1 2 4 6 8 10
depth
(e) Chairs Losses
8 10
宴d事
(d) Chairs Distances
1 前 LatentSPaCeZ
-----Output Attack
4 , /
--φE5Bl-LBμBSK>pv--
Latent
β
(g)	3D Faces Distances
((Z×)d)60τ
0
-20000
-40000
-60000
-80000
2	3	4	5
2	3	4	5
depth
(c) dSprites Negative Log Likelihood
2400
2	4	5
depth
2	3	4	5
(f) Chairs Negative Log Likelihood
β
(h)	3D Faces Losses
2	3	4
depth
Latent Space Attack
(i)	3D Faces Negative Log Likelihood

Figure G.10: Plots showing the effect of varying L, β on Seatbelt-VAEs trained on dSprites, 3D
Faces and Chairs on: (a),(d),(g) the L2 distance between xt and its reconstruction when given as
input and the same between the adversarial input x* and its reconstruction; (b),(e),(h) the adversarial
objectives ∆°utput∕image; (c),(f),(i) - logpθ(xt∣z), Z 〜qφ(z|x*).
17
Under review as a conference paper at ICLR 2020
G.4 Seatbelt-VAE layerwise attacks
(a) 3D Faces
(b) Chairs
Figure G.11: - logpθ (xt∣Z) for Seatbelt-VAEs for (a) 3D Faces and (b) Chairs; over β and L values
for latent attacks. We attack the bottom layer (z1), the top layer (zL), and finally show the effect when
attacking all layers (z). Larger values of — logpθ (xt∣Z) correspond to less successful adversarial
attacks.
18
Under review as a conference paper at ICLR 2020
H Adversarial Attack Plots
H. 1 dSprites Adversarial Attack on a Single Factor
Latent Attack
Target Adversarial rec. Distortion
Tarqet Adversarial rec. Distortion
β-TCVAE, β = 1, ||z|| = 64
β-TCVAE, β = 2, ||z|| = 64
Target Adversarial rec. Distortion
β-TCDLGM, β = 1, L = 2
β-TCDLGM, β = 2, L = 2
Seatbelt-VAE, β = 1, L = 2
Seatbelt-VAE, β = 2, L = 2
Figure H.12: Latent space attacks on rotation only of a heart-shaped dSprite for β-TCVAEs, β-
TCDLGMs and Seatbelt-VAEs for β = {1, 2}.
19
Under review as a conference paper at ICLR 2020
H.2 dSprites Adversarial Attack
H.2. 1 β-TCVAES
Output Attack
aa≡ ■■■ ■■■ ■■赠 aa≡ aa≡
τ⅛ra<t ⅛⅛raar⅛lg D⅛to⅛n	TarB«1 ⅛⅛raar⅛lg D⅛to⅛n	TarB«1 ⅛⅛raar⅛lg D⅛to⅛n	TarB«1 ⅛⅛raar⅛lg D⅛to⅛n	TarB«1 ⅛⅛raar⅛lg D⅛to⅛n	τ⅛a<t ⅛⅛raar⅛lg r⅛tatl∞
BB≡ BBS BK BB≡ BBH
β= 1,||z|| =4	β = 2,||z|| =4	β = 4,||z|| =4	β = 6,||z|| =4	β = 8,||z|| =4 β = 10,||z|| =4
β = 1,||z|| = 6	β = 2,||z|| = 6	β = 4,||z|| = 6	β = 6,||z|| = 6	β = 8,||z|| = 6 β = 10,||z|| = 6
■■园 aaa aa≡ ■■因 aa≡ ■■■
事■因 BBH BB≡ BEH BBH B≡Ξ
β = 1,||z|| = 8	β = 2,||z|| = 8	β = 4,||z|| = 8	β = 6,||z|| = 8	β = 8,||z|| = 8 β = 10,||z|| = 8
Or⅛Ml OrWnalg ⅛⅛w∞⅛l	0ι⅛⅛l OiwnaI 3 AdwraarW	<⅝⅛ιal <⅝⅛nalg ⅛⅛raar⅛l	Orta⅛l OiwnaI 3 Adwnarial	Ort⅛n⅛ OrtJnalg ⅛⅛w∞⅛l	OrWnal OrtJnalg ⅛⅛w∞⅛l
■■用 aa≡ ■■■ aa≡ ■■■ ■■■
τ⅛ra<t ⅛⅛raar⅛lg r⅜to⅛n	7⅛a⅛ ⅛⅛ww⅛lg Otstortlgn	7⅛a⅛ ⅛⅛ww⅛lg Dtatortlon	7⅛a⅛ ⅛⅛ww⅛lg Dtstortlon	τ⅛a<t ⅛⅛raar⅛lg r⅜to¾∞	τ⅛a<t ⅛⅛raar⅛lg D⅛taB∞
eæ BB≡ BB≡ BK BBE
β = 1,||z|| = 16 β = 2,||z|| = 16 β = 4,||z|| = 16 β = 6,||z|| = 16 β = 8,||z|| = 16 β = 10,||z|| = 16
aαa aαa aæ aao MH a≡
7⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg DMortpn	1⅛ra⅛ ⅛⅜ww⅛lg D⅛to⅛n	⅝a⅛ ⅛⅜ww⅛lg Otstortlon
BK BBH BEΞ BEE BBB BH≡
β = 1,||z|| = 32 β = 2,||z|| = 32 β = 4,||z|| = 32 β = 6,||z|| = 32 β = 8,||z|| = 32 β = 10,||z|| = 32
β = 1,||z|| = 64 β = 2,||z|| = 64 β = 4,||z|| = 64 β = 6,||z|| = 64 β = 8,||z|| = 64 β = 10,||z|| = 64
Figure H.13: Output attacks on dSprites on β-TCVAEs for β = {1, 2, 4, 6, 8, 10} and ||z||
{4, 6, 8, 16, 32, 64}.
20
Under review as a conference paper at ICLR 2020
Latent Attack
β= 1,||z|| =4	β = 2,||z|| =4	β = 4,||z|| =4	β = 6,||z|| =4	β = 8,||z|| =4 β = 10,||z|| =4
β = 1,||z|| = 6	β = 2,||z|| = 6	β = 4,||z|| = 6	β = 6,||z|| = 6	β = 8,||z|| = 6 β = 10,||z|| = 6
aaa ■■超■■■ aa≡ ■■出■■■
7⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	⅝a⅛ ⅛⅜ww⅛lg D⅛to⅛n	⅝a⅛ ⅛⅜ww⅛lg Dtstatlon
Bn B9≡ BBE BB≡ BHΞ BB≡
β = 1,||z|| = 8	β = 2,||z|| = 8	β = 4,||z|| = 8	β = 6,||z|| = 8	β = 8,||z|| = 8 β = 10,||z|| = 8
<⅝⅛⅜l OlWn>lιw∙ MwIWW	<⅝⅛⅜l OlWn«13 MwIWW	<⅝⅛⅜l OlWn«13 Mmartal	<⅝⅛⅜l OlWn«13 MwIWW	OlWZ OlWn«13 MwIWW	OlWn«1 OlWn«13 MwIWW
aaa ■■■ aa≡ ■■■■■■ ■■■
⅛⅛πartalg D⅛to⅛n
7⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	⅝a⅛ ⅛⅜ww⅛lg Otstortlon	τ⅛ra⅛ ⅜⅜ww⅛lg □⅛tort⅛n	1⅛roιt
BHB BB≡ BUB BB≡ 1
• ■*
β = 1,||z|| = 16 β = 2,||z|| = 16 β = 4,||z|| = 16 β = 6,||z|| = 16 β = 8,||z|| = 16 β = 10,||z|| = 16
aaa aaκ aaa ■■■ aa≡ aas
τ⅛ra<t ⅛⅛raar⅛lg D⅛to⅛n	τ⅛ra⅛ ⅜⅜ww⅛lg Dtstgrtlgn	τ⅛ra⅛ ⅜⅜ww⅛lg M*rtkχ1	TarB«1 ⅛⅛raar⅛lg D⅛to⅛n	⅝a⅛ ⅛⅜ww⅛lg Otsfcrtkxi	τ⅛a<t ⅛⅛raar⅛l g D⅛taB∞
BBB BOB BK BBE BBΞ
β = 1,||z|| = 32 β = 2,||z|| = 32 β = 4,||z|| = 32 β = 6,||z|| = 32 β = 8,||z|| = 32 β = 10,||z|| = 32
■■■ ■■■ ■■阳■■电■■明 ■■学
^τ⅛rg^^ A<h*πartalrκ. Distortion τ⅛a<t ⅛⅛raar⅛lg r⅛tatl∞ τ⅛a<t ⅛⅛raar⅛lg r⅛tatl∞ τ⅛a<t ⅛⅛raar⅛lg r⅛tatl∞ ⅝a⅛ ⅛⅜ww⅛lg Dtstartlon τ⅛ra⅛ ⅜⅜ww⅛l3 OtotoMn
电■题 >■■ Bn Bbo Bk Bam
β = 1,||z|| = 64 β = 2,||z|| = 64 β = 4,||z|| = 64 β = 6,||z|| = 64 β = 8,||z|| = 64 β = 10,||z|| = 64
Figure H.14: Latent attacks on dSprites on β-TCVAEs for β = {1, 2, 4, 6, 8, 10} and ||z|| =
{4, 6, 8, 16, 32, 64}.
21
Under review as a conference paper at ICLR 2020
H.2.2 β-TCDLGMS
Output Attack
β= 1,L= 1
β = 2,L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
AdEaial
Advaraartal
AdEaial
Advaraartal
AdVtealfel
TarBat
AdVtrSartaI rec. Dtetorttal
AdversartaJrec. Distortion	AdversaWrec. Dbtortfon
BEO n 以
TarBat
⅛⅛eraa⅛lrec. DtetortIOn
Adwrsartairec. MStortcn
β= 1,L=2
β = 2, L = 2
β = 4, L = 2
β = 6, L = 2
β=8,L= 2
β = 10,L = 2
β = 1, L = 3
β = 2, L = 3
β = 4, L = 3
β = 6, L = 3
β=8,L=3
β = 10,L = 3
Ot⅛⅛al O⅜lnalr⅜c. MVeraatel	O⅜hal Ot⅛lnalιac. M√eraa⅛l	Ot⅛lnal Ot⅛lndrec. Adveeartal _
■■■ ■■■ ■■■ i
Oi
TWflet ⅛⅛eraa⅛lrec. DletCrtIOn
电•国 BBB ■■■ B
⅛⅛eraa⅛lrec. DtetortIOn
.人十
^θɪfalnal OjfalMir^_ A≤wg≤<
■■四
Talwt AdW⅝aι<alrec. Dfatortai
EBΞ
，■宜
β= 1,L=4
β=2,L=4
β = 4, L = 4
β = 6, L = 4
β = 8, L = 4
β = 10, L = 4
■■■ ■■圉■■■ ■■目■■图 M≡
TmMt Adveraarialrec. Dtotertol	Tgt AdEa⅛1 ∣⅜c. Distortion	Tanwt AtKeeaiialrec. D⅛⅛⅝tton	T⅛n⅜t MveraafaJ rec. DktCraMl	Tgt M⅛eraa⅛l ∣⅜c. DtetortlOn	Talwt Adwsaiialrec. DtetoriCTi
■■的 BE^ ■■因 BH5 BB≡ BH 疆
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β = 6, L = 5
β = 8, L = 5
β = 10,L = 5
Figure H.15: Output attacks on dSprites on β-TCDLGMs for β = {1, 2, 4, 6, 8, 10} and L
{1,2,3,4,5}.
22
Under review as a conference paper at ICLR 2020
Latent Attack
β = 1, L = 1
β = 2, L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
Advaraaitel rec. DtetortlOn
MversafaJ rec. Dktcrtlon
⅛i√eraa⅛l rec. DtetortlOn
Ma
Mvtrsartal rec.	Dtetorttal
β = 1, L = 2
β = 2, L = 2
β = 4, L = 2
β = 6, L = 2
β = 8, L = 2
β = 10, L = 2
Adversaial
AdVerSSrid
Advtrsartd
β = 1, L = 3
β = 2, L = 3
β = 4, L = 3
β = 6, L = 3
β = 8, L = 3
β = 10, L = 3
Ml
AdversateJ rec. Dbtortfon
BH昭
β = 6, L = 4
β = 1, L = 4
β = 2, L = 4
β = 4, L = 4
β = 8, L = 4
β = 10, L = 4
β = 1, L = 5	β = 2, L = 5	β = 4, L = 5	β = 6, L = 5	β = 8, L = 5	β = 10, L = 5
Figure H.16: Latent attacks on dSprites on β-TCDLGMs for β = {1, 2, 4, 6, 8, 10} and L =
{1,2,3,4,5}.
23
Under review as a conference paper at ICLR 2020
H.2.3 SEATBELT-VAES
Output Attack
■■囹 MB ■■■ MB
∏∙
___
C⅛'
Ot⅛lnal Oriflhalrec.	AdEa⅛∣
Taraet Adversarial rec. DbtMtlon
β= 1,L= 1
β = 2,L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
Dtetoftlg
β= 1,L=2
β = 2, L = 2
β = 4, L = 2
β = 6, L = 2
β=8,L= 2
β = 10,L = 2
β = 1, L = 3
β = 2, L = 3
β = 4, L = 3
β = 6, L = 3
β=8,L=3
β = 10,L = 3
β= 1,L=4
β=2,L=4
β = 4, L = 4
β = 6, L = 4
β = 8, L = 4
β = 10, L = 4
β = 1, L = 5	β = 2, L = 5	β = 4, L = 5	β = 6, L = 5	β = 8, L = 5	β = 10, L = 5
Figure H.17: Output attacks on dSprites on Seatbelt-VAEs for β = {1, 2, 4, 6, 8, 10} and L =
{1,2,3,4,5}.
24
Under review as a conference paper at ICLR 2020
Latent Attack
Taraet Adverssrtalrec. Distottlon	Terget Adversaial rec. Dlstcrtlon	Target Advetssrtal rec. Distortion
Taraet Advtrsarid rec. MStorI6
β = 1, L = 1
β = 2, L = 1
β = 4, L = 1
β = 6, L = 1	β = 8, L = 1
β = 10, L = 1
β = 8, L = 2	β = 10, L = 2
β = 1, L = 2
β = 2, L = 2
β = 4, L = 2
β = 6, L = 2
Target Advetssrtalrec. Distcrtlon	Target Adversarial rec. DiStOrtkn	Taraet Adversarial rec. Disto<⅞lon
β = 6, L = 3	β = 8, L = 3	β = 10, L = 3
β = 1, L = 3	β = 2, L = 3	β = 4, L = 3
Advecsanal rec.
DIskrUori
Adversanal rec.
Adversanai rec.
Ta¾et Advaraatel rec. □ Istcrtlon
∏ I. ∙ n oHol ruz<
AdufWfmriM nv*
I ；. ； I


β = 1, L = 4
β = 2, L = 4
β = 4, L = 4
β=6,L=4
β = 8, L = 4
β = 10, L = 4
β = 1, L = 5	β = 2, L = 5
β = 4, L = 5
β=6,L=5
β = 8, L = 5
β = 10, L = 5
Figure H.18: Latent attacks on dSprites for Seatbelt-VAEs for β = {1, 2, 4, 6, 8, 10} and L
{1,2,3,4,5}.
25
Under review as a conference paper at ICLR 2020
H.3 Chairs Adversarial Attack
H.3.1 β-TCVAES
Output
β = 1,||z|| =4
β = 6,||z|| =4
CMdnaIrK. AAwraarW
β = 8,||z|| =4
CMdnaIrK. Adnπarial
β= 10,||z|| =4
β = 1,||z|| = 6
β = 2,||z|| = 6
β = 4,||z|| =6
β = 6,||z|| = 6
Wdnd CMdnaIrK. ⅛⅛raar⅛l
β = 8,||z|| = 6
β = 10,||z|| = 6
Origlnelw. . ⅛⅛raa⅛ι
1⅛r0∣t ⅛Mπartalg □⅛tc*⅛∏
CMdnalrK. AAwraarW
β = 2,||z|| = 8
CMdnaIrK. AAwraarW
β = 4,||z|| =8
Oittial CMdnaIrK. ⅛⅛raar⅛l	Ortdnd CMdnaIrK. ⅛⅛raar⅛l
β = 6,||z|| = 8
β = 8,||z|| = 8
Wdnal CMdnaIrK. ⅛⅛raar⅛l
β = 10,||z|| = 8
β = 1,||z|| = 8
CMdnaIrw. AAwraarW
AdwwWrK. ^ogjrtgɪ
β = 2,||z|| = 16
CMdnaIrK. AAwraarW
β = 4,||z|| = 16
β = 6,||z|| = 16
β = 8,||z|| = 16
β = 10,||z|| = 16
β = 1,||z|| = 16
CMdnalrw. AAwraarW
AdwwWrK. ^ogjrtgɪ
τ Q
β = 2,||z|| = 32
CMdnaIrK. Adnπarial
β = 4,||z|| = 32
β = 6,||z|| = 32
β = 8,||z|| = 32
CMdnaIrK. AAwraarW
β = 10,||z|| = 32
β = 1,||z|| = 32
CMdnalrw. AAwraarW
β = 1,||z|| =64
CMdnaIrK. AAwraarW
β = 2,||z|| = 64
CMdnaIrK. AAwraarW
β = 4,||z|| = 64
β = 6,||z|| = 64
CMdnaIrK. AAwraarW
β = 8,||z|| = 64
β = 10,||z|| = 64
* Q
Figure H.19: Output attacks on Chairs for β-TCVAEs for β = {1, 2, 4, 6, 8, 10} and ||z||
{4, 6, 8, 16, 32, 64}.
26
Under review as a conference paper at ICLR 2020
Latent
CMdnalrw. AAwraarW
β = 1,||z|| =4
CMdnaIrK. AAwraarW
β = 2,||z|| = 4
β = 4,||z|| =4
CMdnaIrK. AAwraarW
β = 8,||z|| =4
Wdnal CMdnaIrK. ⅛⅛raar⅛l
β = 6,||z|| =4
β= 10,||z|| =4
CMdnaIrw. AAwraarW
β = 1,||z|| = 6
CMdnaIrK. AAwraarW
β = 2,||z|| = 6
CMdnaIrK. AAwraarW
β = 4,||z|| =6
CMdnaIrK. AAwraarW
⅛⅛1w⅛lrβa DtotCftIcn
β = 6,||z|| = 6
Wdnd CMdnaIrK. ⅛⅛raar⅛l
β = 8,||z|| = 6
CMdnaIrK. AAwraarW
β = 10,||z|| = 6
⅛Mπartalg r⅛ta⅛n
β = 2,||z|| = 8
⅛⅛raβι⅛lr∙o. DtJfcrtwi
β = 6,||z|| = 8
Wdnd CMdnaIrK. ⅛h*πarial
β = 10,||z|| = 8
β = 1,||z|| = 8
β = 4,||z|| =8
β = 8,||z|| = 8
β = 1,||z|| = 16 β = 2,||z|| = 16
CMdnaIrK. AAwraarW
β = 4,||z|| = 16
β = 6,||z|| = 16
CMdnaIrK. AAwraarW
β = 8,||z|| = 16
β = 10,||z|| = 16
CMtfnaIrK. AAwraarW
β = 4,||z|| = 32
β = 6,||z|| = 32
CMtfnd gnalrκ, Mmartal
β = 8,||z|| = 32
β = 10,||z|| = 32
β = 2,||z|| = 32
CMdnaIrw. AAwraarW
⅛⅛raβι⅛lr∙o. DtJfcrtwi
CMdnaIrK. AAwraarW
β = 2,||z|| = 64
5 *> 
1⅛r0∣t ⅛hww⅛lr∙tt DWg⅛n
，9H
β = 6,||z|| = 64
β = 8,||z|| = 64
β = 10,||z|| = 64
β = 1,||z|| =64
β = 4,||z|| = 64
Figure H.20: Latent attacks on Chairs for β-TCVAEs for β = {1, 2, 4, 6, 8, 10} and ||z||
{4, 6, 8, 16, 32, 64}.
27
Under review as a conference paper at ICLR 2020
H.3.2 β-TCDLGMS
Output Attack
β = 1, L = 1
β = 2, L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
Ofighal Orighd rec. Adwrsartei
Taraet Advtrsaridrec.
β = 1, L = 2	β = 2, L = 2	β = 4, L = 2
β = 6, L = 2
β = 8, L = 2
β = 10, L = 2
β = 1, L = 3
β = 2, L = 3
β = 4, L = 3
β = 6, L = 3
β = 8, L = 3
10, L = 3
β = 1, L = 4
β = 2, L = 4
β = 4, L = 4
β=6,L=4
β = 8, L = 4
10, L = 4
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β=6,L=5
β = 8, L = 5
10, L = 5
β
β
β
Output attacks on Chairs for β-TCDLGMs for β = {1, 2, 4, 6, 8, 10} and L = {1, 2, 3, 4, 5}.
28
Under review as a conference paper at ICLR 2020
Latent Attack
β = 1, L = 1
β = 2, L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1	β = 10, L = 1
O<ynal Orighal rec. Adversatal
Orighal Orighal rec. Adveraartal
Target Advetssrtal rec. Mstcrtkxi
Target Adversarial rec. Dtetortkm
β = 1, L = 2
β = 2, L = 2
β = 4, L = 2
β = 6, L = 2
β = 8, L = 2	β = 10, L = 2
β = 1, L = 3
β = 2, L = 3	β = 4, L = 3
β = 6, L = 3
β = 8, L = 3
10, L = 3
β = 1, L = 4
β = 2,L
β = 4, L = 4
β=6,L=4
β = 8, L
10, L = 4
Orighal Orighal rec. Adveraartal	Orighal Orighd rec. AdVtrSaHd
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β=6,L=5
β = 8, L = 5	β = 10, L = 5
β
4
4
β
Figure H.21: Latent attacks on Chairs for β-TCDLGMs for β = {1, 2, 4, 6, 8, 10} and L
{1,2,3,4,5}.
29
Under review as a conference paper at ICLR 2020
H.3.3 SEATBELT-VAES
Output Attack
Target Advetssrtal rec. Dtetortkxi
Ofighal Orighd rec. Adwrsartei
β = 1, L = 1
β = 2, L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
β = 1, L = 2	β = 2, L = 2	β = 4, L = 2
β = 6, L = 2
β = 8, L = 2
10, L = 2
Ofighal	Ori 声Ial rec.	Adversatel
Orighal Orighd rec. Adwraartei
β = 1, L = 3
β = 2,L
β = 4, L = 3
β = 6, L = 3
β = 8, L = 3
β = 10, L = 3
β = 1, L = 4
β = 2, L = 4
β = 4, L = 4
β=6,L=4
β = 8, L = 4	β = 10, L = 4
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β=6,L=5
β = 8, L = 5
10, L = 5
β
3
β
Figure H.22: Output attacks on Chairs for Seatbelt-VAEs for β = {1, 2, 4, 6, 8, 10} and L
{1,2,3,4,5}.
30
Under review as a conference paper at ICLR 2020
Latent Attack
β = 1, L = 1
β = 2, L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
O<⅛hal
Ori的al rec.
Advarsaial
β = 1, L = 2
β = 2,L
β = 4, L = 2
β = 6, L = 2
β = 1, L = 3	β = 2, L = 3	β = 4, L = 3
β = 6, L = 3
β = 1, L = 4
β = 2,L
β = 4, L
β=6,L=4
β = 8, L = 4
β = 10, L = 4
2
4
4
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β=6,L=5
β = 8, L = 5
β = 10, L = 5
Figure H.23: Latent attacks on Chairs for Seatbelt-VAEs for β = {1, 2, 4, 6, 8, 10} and L
{1,2,3,4,5}.
31
Under review as a conference paper at ICLR 2020
H.4 3D Faces Adversarial Attack
H.4. 1 β-TCVAES
Output Attack
β= 1,||z|| =4	β = 2,||z|| =4	β = 4,||z|| =4	β = 6,||z|| =4	β = 8,||z|| =4	β = 10,||z|| =4
6≠W	6≠0lg. AAwraarW	gnal CMSaIg ⅛h*πarial	。岫01	CMgMne- Attanartal	6≠W CMBlne13	Adnearial	。岫01	OHBheI3	⅛Mπa>ial	6≠W	CMtfndno- Adnearial
9$堂,•畿 99∙ ∙∙9 9∙SP DV嚼
TtrgΛ Athmartalm □⅛x⅛∞	Tegit ⅛⅛raβr⅛lmc. Dtstatlw1	Tergit	MnnaMg □⅛x⅛∞	⅞rgιt AthvwWg	r⅛ta⅛n	TB	AΛ∙raw⅛lιw.	αstatl∞	⅞rgιt	Ath*πartal rκ. Dtatatkr1
H Z溜 X图	♦•阳
β = 1,||z|| = 6	β = 2,||z|| = 6	β = 4,||z|| = 6	β = 6,||z|| = 6	β = 8,||z|| = 6	β = 10,||z|| = 6
6≠W	6≠0lg. AAwraarW	gnal CMSeI3 Adnearial	OHR 6^alg AtfaI	6≠0l CMBlne13 Adnearial	OHR OHBheI3 MnnaM	6≠W CMtfndno- MHnartaI
ff« ∙fφ	∙^llf∙≡
⅞rgιt Athmartal rκ. □⅛x⅛∞	Tegit Adwnarial 3 Bstatlon	TE Adnnartal rκ. □⅛x⅛∞	⅞rgιt Athvw⅞alr∙c. otjtattaɪ	Tegit MnnsVno- attaVcn	TvgΛ AtlWnartalg r⅛ta⅛r1
H	♦。图 ''幽 7'国
β = 1,||z|| = 8	β = 2,||z|| = 8	β = 4,||z|| = 8	β = 6,||z|| = 8	β = 8,||z|| = 8	β = 10,||z|| = 8
6≠W	6≠0lg. AAwraarW	0>Vnal	CMSeI3 NhEriaI	。岫01	6^alg Athmartal	6≠W CMBlna13	Mmartal	。岫W	OHBheI3	A<Mπartal	6≠W	2Mg■	Adwnarial
・ ,事■。卷,。糖
⅞rgιt	Athmartalm □⅛x⅛∞	Tegit	⅛⅛raβr⅛lmc. Dtj⅛rt∞	TE	MnnaMg □⅛x⅛n	⅞rgιt AthvwWg	r⅛ta⅛n	TB	AΛ∙raw⅛lιwo.	αstatl∞	⅞rgιt	Ath*πartal rκ.	Dtjtatkr1
♦龄・ 7目♦・■ ♦犯M H ⅜⅜H
β = 1,||z|| = 16 β = 2,||z|| = 16	β = 4,||z|| = 16 β = 6,||z|| = 16 β = 8,||z|| = 16 β = 10,||z|| = 16
P≡⅛
β = 1,||z|| = 32 β = 2,||z|| = 32 β = 4,||z|| = 32 β = 6,||z|| = 32 β = 8,||z|| = 32 β = 10,||z|| = 32
6≠W 6≠0lg. AAwraarW 0>Vnal CMSeI3 MHnartaI 。岫01 6^alg Athmartal 6≠W CMBlne13 Mmartal 。岫W OHBheI3 ⅛Mπa>ial 6≠W CMtfndno- Mmartal
∙∙t fte os舞 ft® e∙* f∙b
Taraat Adwπβdβi raa □rfcrtcn TaroBt ⅛ħwτar1ai ιao. Dtaterilcn Tanat Advaπβ11βi raa □rfcrtcn Taraat ⅛ħwτwW ran. Dtahiikii TaroBt AdzarsvW mα. Dlrfcrilcn Taraat AdMrmIai raα. Dtahiikii
β = 1,||z|| = 64 β = 2,||z|| = 64 β = 4,||z|| = 64 β = 6,||z|| = 64 β = 8,||z|| = 64 β = 10,||z|| = 64
Figure H.24: Output attacks on 3D Faces for β-TCVAEs for β = {1, 2, 4, 6, 8, 10} and ||z|| =
{4, 6, 8, 16, 32, 64}.
32
Under review as a conference paper at ICLR 2020
Latent Attack
ɑvɔal Ori≠ιal3	MHnartaI	gnal	CMgneI3 ⅛⅛raar⅛l	OHR	CMgheI 3	Adnnatal	W≠nl	CMBlne13 MHnartaI	OHA	OHgheI3 Adnnartal	0ri≠nl	CMtfndno-	MHnartaI
∙∙K	•9«	OG徵■♦趟9号翳爨
⅞rgιt Athmartalm	□⅛x⅛∞	Tegit	⅛⅛raβr⅛lmc.	Dtj⅛rt∞	TE	AtMnartaIg □sta⅛n	⅞rgιt	Mmartalg	DhtorVon	TB	⅛MπE3	αstatl∞	⅞rgιt	AdHnaM3	r⅛ta⅛n
♦「囱♦田露♦。陶⅛u^，》■	,仍图
β= 1,||z||	=4	β	=	2,||z||	=4	β	=	4,||z||	=4	β	=	6,||z||	=4	β	=	8,||z|| =4	β =	10,||z||	=4
β = 1,||z|| = 6	β = 2,||z|| = 6	β = 4,||z|| = 6	β = 6,||z|| = 6	β = 8,||z|| = 6	β = 10,||z|| = 6
q ” 0 3号.ff c巧翻,一.巍.手F
⅞rgιt Ath*πartalrκ. □statl∞	Tegit ⅛⅛raβr⅛lno. r⅛tatl∞	TE	AtMnpaIg □⅛x⅛n	⅞rgιt MHnartal3 r⅛ta⅛r1	Tegit Adnnstalna- astatl∞	⅞rgιt AtMnadaIrac. Dtjtatkr1
• 园	V tB	⅜e≡	>t≡	◎，物
β = 1,||z|| = 8	β = 2,||z|| = 8	β	= 4,||z|| = 8	β = 6,||z|| = 8	β = 8,||z|| = 8	β = 10,||z|| = 8
β = 1,||z|| = 16 β = 2,||z|| = 16 β = 4,||z|| = 16 β = 6,||z|| = 16 β = 8,||z|| = 16 β = 10,||z|| = 16
β = 1,||z|| = 32 β = 2,||z|| = 32 β = 4,||z|| = 32 β = 6,||z|| = 32 β = 8,||z|| = 32 β = 10,||z|| = 32
β = 1,||z|| = 64 β = 2,||z|| = 64 β = 4,||z|| = 64 β = 6,||z|| = 64 β = 8,||z|| = 64 β = 10,||z|| = 64
Figure H.25: Latent attacks on 3D Faces for β-TCVAEs for β = {1, 2, 4, 6, 810} and ||z||
{4, 6, 8, 16, 32, 64}.
33
Under review as a conference paper at ICLR 2020
H.4.2 β-TCDLGMS
Output Attack
Otynal
CMghalrec. Adveraaial
O⅛Ma∣
CMghalrec. Adveraartal
Adveraartal rec. Mstcrtkw	Target Adversarial i«c. Distortion	TaIaet Acfveraartal rec. Dtetortloci	Target Adveraatel rec.
'赢、>il ∙d∙ *■
β= 1,L= 1
β = 2,L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
Tarset Adveraartal rec. Dlstcrtloci	Target Adversarial ∣βc. Distortion	TaIaet AdversartaJrec. Dtetortkw	Target Adveraatel rec. Dlstcrtloci	Target Adveraartal rec.
WS S沙图♦。逊“「圜♦♦
DtetortIOn
O<⅛kιal Origkisi rec.	Adwrsaitei
Target	AdVtrSarid rec.	Distortcn
β= 1,L=2
β = 2, L = 2
β = 4, L = 2
β = 6, L = 2
β=8,L= 2
β = 10,L = 2
Ofynal Origkial rec. Adveraatel	Origkial O<⅛kιal rec. Adveraartal	O<⅛kιal Origkid rec. AdVeraalId	O<⅛hal OII的al rec. Advwsatel	Origkial OriShal rec. Advaraartal	O<⅛kιal Origkid rec. Adwrsaitei
f ff f fBff≡ff≡
β = 1, L = 3
β = 2, L = 3
β = 4, L = 3
β = 6, L = 3
β=8,L=3
β = 10,L = 3
β= 1,L=4
β=2,L=4
β = 4, L = 4
β = 6, L = 4
β = 8, L = 4
β = 10, L = 4
Ofynal Origkial rec. Adveraaial	Origkial O<⅛kιal rec. Adveraartal	O<⅛kιal Origkid rec. AdVeraalId	O<⅛hal OII的al rec. Advwsatel	Origkial OriShal rec. Adveraartal	O<⅛kιal Origkid rec. Adwrsaitei
ftf fe≡^ff fft
Taraa
Adveraartal rec. Distcrtlon
Target
Adversarial rec. DtetortiCn
Talget
Acfveraartalrec. DtetortlOn
Tjraet
Adveraatel rec. Dktcrtlon
Target
Adveraartal rec. DtetortlOn

A哂9蜡
Talget
Advtrsarid rec. DtetoriCTl
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β = 6, L = 5
β = 8, L = 5
β = 10,L = 5
Figure H.26: Output attacks on 3D Faces for β-TCDLGMs for β = {1, 2, 4, 8, 10} and L
{1,2,3,4,5}.
34
Under review as a conference paper at ICLR 2020
Latent Attack
Otynal
OrighsI rec.
Adversaial
CMgMaI
Ofighal rec.
Advetsartal
Advetssrtal rec. Distcrtlon
Taraet
Adversarial rec. Dtetortkm
Taraet
β= 1,L
β = 2,L
Orighal Orighal rec. Advwsartal	Orighal Orighd rec. Advtrsartd
■・繇9•等
Target
Advetssrtal rec. Dteto<⅛yι
β = 8, L
Talaet
β = 10, L
01⅛nal
Orighal rec.
Adversaial
Origkisl
Orighal rec.
Advetssrtal
O⅛kιal
Ofighd rec.
AdVerSSrid
Talaet
Adversarial rec. DtetortIOn
Oriflhal
Ori 的 al rec.
Adversaial
Origkial
Oriatial rec.
Advetssrtal
Target
Advetssrtal rec. DtetortlOn
Tjrget
Adversaial rec. Dktcrtlon
AdVtrSarid rec.
O⅛kιal
Ofighd rec.
Advtrsartd
Talaet
AdVtrSaridrec.	DtetoriCTl
⅜υa ⅜⅜H *⅜H
1
1
1
1
111
β= 1,L
β = 2,L
β = 4, L
β = 6, L
β = 8, L
β = 10, L
2
2
2
2
2
2
Oridnal
Orighal rec.
Adversaial
Target
Advetssrtal rec. DutCrtlOn
θʤkial
Ofighal rec.
Target
Advetssrtal
Orighal
Orighalrec.
AdVerSSrid
θʤhal
Orimal rec.
Adversaial
Tjrget
Adversaial rec. Dbtcrtlon
θʤkial
Orighal rec.
Advetssrtal
Target
Adversarial rec. DtetortlOn
O<⅛kιal
Ofighd rec.
Advtrsartd
Talaet
AdVtrSaridrec.	DtetorICn
Adversarial rec. Dtstortkn
⅜⅜B ⅜⅜
Talaet
Adverssrtalrec.
♦速■、、图
f t e
β= 1,L
β = 2,L
β = 4, L
β = 6, L
β = 8, L
β = 10, L
3
3
3
3
3
3
< t f
Otidnal
Orighal rec.
Adversaial
Target
Advetssrtal rec. DietertlOn
Origkial
Ofighal rec.
Advetssrtal
Target
Adversarial rec. Dtetorttal
O<⅛tιa∣
Ofighd rec.
AdverSarid
O⅛hal
Ori的al rec.
Adversaial
Tjrget
Adversaial rec. Dktcrtlon
Origkial
OrighsI rec.
Advetssrtal
O⅛kιal
Ofighd rec.
Advtrsartd
AdVtrSarid rec. Distorlcn
Talaet
Adversarial rec. DtetortIOn
Target
Adversarial rec. Distortion
Taioet

β= 1,L
β = 2,L
β = 4, L
β = 6, L
β = 8, L
β = 10, L
4
4
4
4
4
4
Otynal
OrighaI rec.
Adversatel
θʤkial
Ofighal rec.
Advetssrtal
O<⅛kιal
Ofighd rec.
AdVerSSrid
Talaet
Adverssrtalrec.
⅛⅛t>al
Ori 的 al rec.
Adversatel
CMSMaI
Origtial rec.
Adveissrtal
O<⅛kιal
Ofighd rec.
Advtrsartd
Talaet
AdVtrSaridrec.	DtetorICn
Advetssrtal rec. Distcrtlon
Taraet
Dtstortkri
Tjrget
Target
Advetssrtal rec. DtejOrtjgn
⅜⅜a v⅛a ⅜⅛α
Adversaial rec. Dbtytjgn
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β=6,L=5
β = 8, L = 5
β = 10, L = 5
Figure H.27: Latent attacks on 3D Faces for β-TCDLGMs for β = {1, 2, 4, 8, 10} and L
{1,2,3,4,5}.
35
Under review as a conference paper at ICLR 2020
H.4.3 Seatbelt-VAEs
Output Attack
Advetssrtal rec.
Dtetortlg
Dbtorlcn
β= 1,L= 1
β = 2,L = 1
β = 4, L = 1
β = 6, L = 1
β = 8, L = 1
β = 10, L = 1
Ofynal Origkial rec. Adveraaial	Origkial O<⅛kιal rec. Adveraartal	O<⅛kιal Origkid rec. Adveraailai	0<⅛hal OII的al rec. Adversatal	Origkial OriShal rec. Adveraartal	O<⅛kιal Origkid rec. Adwrsaitei
C闺>>
Tarset Adverssrtalrec. DutCrtlOn	Target Adversarial rec. Dtetorttal
Taiaet AdversartaJrec. MStorton	Target Adveraatel rec. Dlstcrtlon	Target Adveraartal rec. DIStoftton	Taiaet Adwrsaitdrec. Mstoricm
⅜tn ⅜υ≡ 、倒
β= 1,L=2
β = 2, L = 2
β = 4, L = 2
β = 6, L = 2
β=8,L= 2
β = 10,L = 2
Ofynal Origkial rec. Adversatal	Origkial O<⅛kιal rec. Adveraartal	O<⅛kιal Origkid rec. AdVwSaIid	0<⅛hal	OII的al rec.	Adveraaial	Origkial OriShal rec. Adveraartal	O<⅛kιal Origkid rec. AdWrSartai
9•解■/铸
Tarset Adveraartal rec. Mstcrtlon	Target AdwrsartaJrec. Distortion
”豳∙ τ圈
Target Adverssrtalrec. Distortion	Target	Adversaial rec.
、■园.♦
Dbkrtion
β = 1, L = 3
β = 2, L = 3
β = 4, L = 3
β = 6, L = 3
β=8,L=3
β = 10,L = 3
Tarset Adveraartal rec. Dlstortlan	Target Advtrsarialw:. Distortion	Taiaet Acfveraartalrec. Mstortkw	Target AdversaWrec. Dlstcrtkxi	Target Adveraarialrec. Distortlan	Taiaet AdEarid rec. Distortcn
、,第A国♦♦翻、园、' a ♦•爵
β= 1,L=4
β=2,L=4
β = 4, L = 4
β = 6, L = 4
β = 8, L = 4
β = 10, L = 4
Ofynal Origkial rec. Adveraaial	Origkial O<⅛kιal rec. Adveraartal	O<⅛kιal Origkid rec. AdVwSalid	0<⅛hal	OII的al rec.	Adveraatel	Origkial OriShal rec. Adveraartal	O<⅛kιal Origkid rec. Adwrsaitei
■	■•藤■•翳••蹩 9•簟∙∙9
Target AdVerSSrtal rec.
断0
DkttrtiHl
β = 1, L = 5
β = 2, L = 5
β = 4, L = 5
β = 6, L = 5
β = 8, L = 5
β = 10,L = 5
Figure H.28: Output attacks on 3D Faces for Seatbelt-VAEs for β = {1, 2, 4, 6, 8, 10} and L
{1,2,3,4,5}.
36
Under review as a conference paper at ICLR 2020
Latent Attack
Otynal
OrighsI rec.
Adversaial
CMgMaI
OfighsI rec.
Advetsartal
Orighal
Origheirec.
AdVerSSrid
Orighal
Ori 的 SI rec.
Adversaial
CMgMaI
OrighsI rec.
Advetsartal
⅛⅛tιal
OrigMd rec.
Adversaria
Target
Advetssrtal rec. DUtCrtlOn
Target
Adversarial rec. Dteto<⅜n
Talaet
Adversarialrec. DtetortlOn
Tjrget
Adversaial rec. Dktcrtlon
Target
Advetssrtal rec. DtetortlOn
Talaet
Adversartairec 0⅛W⅜<
β= 1,L
β = 2,L
β = 4, L
β = 6, L
β = 8, L
β = 10, L
1
1
1
1
1
1
01⅛nal
OrighsI rec.
Adversaial
Origkisl
OfighaI rec.
Advetssrtal
O<⅛kιal
Ofighd rec.
Adverssrid
Oriflhal
Ori 的 al rec.
Adversaial
Origkisl
Origtial rec.
Adveraartal
O<⅛kιal
Ofighd rec.
Advtrsartd
f f t f f ≡f ft:f f ≡f f≡f ff
Target
Advetssrtal rec. DietCrtIOn
Target
Adversarial rec. DtetortiCn
Talaet
Adverssrtalrec. DtetortlOn
Tjrget
Adversaial rec. Dktcrtlon
Target
Advetssrtal rec. DtetortIOn
Talaet
AdVtrSaridrec.	Dlatorlcn
β= 1,L
β = 2,L
β = 4, L
β = 6, L
β = 8, L
β = 10, L
2
2
2
2
2
2
Otynal
OrighsI rec.
Ad⅛
CMgMaI
OfighsI rec.
Adveissrtal
Orighal
Ofighd rec.
AdvwSalld
Orighal
Ori 的 SI rec.
Atlvarsatel
CMgMaI
OrighsI rec.
Atlvwsartal
⅛⅛tιal
O1⅛⅛∣g∣rec.
Advtrssrtd
Target
Advetssrtal rec. Dutytjgn
Target
Adversarial rec. Dteh^n
Talaet
Adversarialrec. DtetortlOn
Tjrget
Adversaial rec. Dktcrtlon
Advetssrtal rec. DtetortlOn
Talaet
Advtrsartei rec. DtetoriCTl
β = 1, L = 3
β = 2, L = 3
β = 4, L = 3
β = 6, L = 3
β = 8, L = 3
β = 10, L = 3
Oridnal
Orighal rec. Adversaial
Advetssrtal rec.
Target
θʤkial
Ofighal rec.
Advetssrtal
O<⅛kιal
Ofighd rec
θʤhal
Orimal rec.
Target
Adversarial rec. Dteto<⅜n
Adversarial rec. Dtetortkxi
Adversaial
Origkial
Orighal rec
O<⅛kιal
Ofighd rec.
Advtrsaifei
Target
AdVtrSarid rec. Dtetorlcm
Advetssrtal rec. DtetortlOn
∏i Rtfvtkin
Taioet
Tjraet
Adversaial rec. Dlstcrtlon
Taioet
β = 1, L = 4
β = 2, L = 4
β = 4, L = 4
β=6,L=4
β = 8, L = 4
β = 10, L = 4
f f f
Otynal
OrighaI rec.
Adversatel
Target
Advetssrtal rec. DutCrtlOn
β= 1,L
5
Figure H.29: Latent attacks on 3D Faces for Seatbelt-VAEs for β = {1, 2, 4, 6, 8, 10} and L
{1,2,3,4,5}.
37
Under review as a conference paper at ICLR 2020
H.5 CelebA Adversarial Attack
H.5.1 β-TCVAES
Output and Latent Attacks
Figure H.30: Output (a) (b) and Latent (c) (d) attacks on CelebA on β-TCVAEs for β = {1, 10} and
||z|| =32.
H.5.2 β-TCDLGMS
Output and Latent Attack
(a) Output, β = 1
(d) Latent, β = 10
(b) Output, β = 10	(c) Latent, β = 1
Figure H.31: Output (a) (b) and Latent (c) (d) attacks on CelebA on L = 4 β-TCDLGMs for
β={1,10}.
H.5.3 Seatbelt-VAEs
Output and Latent Attack
(a) Output, β = 1
(b) Output, β = 10	(c) Latent, β = 1
(d) Latent, β = 10
Figure H.32: Output (a) (b) and Latent (c) (d) attacks on CelebA on L = 4 Seatbelt-VAEs for
β={1,10}.
38
Under review as a conference paper at ICLR 2020
I Data Generation from Models
Ancestral Sampling in CelebA
(a) L = 1,β = 1	(b) L = 1,β = 10
(c) L = 4, β = 1
(d) L = 4, β = 10
Figure I.33:	Means of the decoder from ancestral sampling in z, for Seatbelt-VAEs with L = {1, 4},
β = {1, 10}. Note that there is a reduction in diversity of the samples for L = 1 (ie a β-TC VAE),
β = 10, which is not the case for the samples from the β = 10 L = 4 Seatbelt-VAE.
39
Under review as a conference paper at ICLR 2020
Latent Traversals for dSprites
(a) β-TC DLGM
(b) Seatbelt-VAE
Figure I.34:	Latent traversals in the |z| = 6 top layer of L = 2, β = 2 for a β-TC DLGM and a
Seatbelt-VAE trained on dSprites. Note that the traversals do not capture the ground-truth factors of
variation.
40
Under review as a conference paper at ICLR 2020
J Mutual Information Gap
The Mutual Information Gap (Chen et al., 2018) is average over ground truth factors of variation
of the entropy-normalised difference between the greatest mutual information between the any of
the units in z and a given ground-truth factor of variation ν and the second-greatest such mutual
information:
K1
MIG = ɪj H(Vk) [I(zj*，Vk)- maxl(zj=j*,νk)]
where zj = argmaxj I(zj,Vk).
(J.1)
Table J.2: MIG in zL: for L = 2; for β-TC DLGMs and Seatbelt-VAEs; for a range of β values; for
dSprites, 3D Faces and Chairs.
dSprites	3D Faces	Chairs
β	β-TC DLGM	Seatbelt-VAE	β-TC DLGM	Seatbelt-VAE	β-TC DLGM	Seatbelt-VAE
1	-0.0411	0.0475	-0.0211	0.0300	-0.0381	0.0134
2	0.3294	0.3589	0.4038	0.2200	0.2641	0.5366
4	0.2751	0.3235	0.2904	0.2349	0.7660	0.4963
6	0.3213	0.3258	0.1806	0.1890	0.1929	0.3111
8	0.3076	0.3182	0.2046	0.2301	0.2526	0.3329
10	0.3547	0.3415	0.1440	0.1053	0.1625	0.2802
41
Under review as a conference paper at ICLR 2020
(a) dSprites
(b) 3D Faces
Z
(c) Chairs
Figure J.35: MIG for β-TC VAEs as a function of |z| for different values of β. Note that MIG
decreases as we increase |z|, indicating that we get degenerate latent representations - that is different
units in z end up with similar mutual information to the same ground truth factors. The red line in a)
is at |z | = 6, the number of ground-truth factors of variation for dSprites.
42