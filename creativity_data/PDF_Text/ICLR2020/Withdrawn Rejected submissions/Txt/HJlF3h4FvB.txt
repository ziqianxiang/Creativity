Under review as a conference paper at ICLR 2020
DISTILLATION ≈ EARLY STOPPING?
Harvesting	Dark	Knowledge	Utilizing
Anisotropic Information Retrieval For
Overparameterized Neural Network
Anonymous authors
Paper under double-blind review
Ab stract
Distillation is a method to transfer knowledge from one model to another and often
achieves higher accuracy with the same capacity. In this paper, we aim to provide a
theoretical understanding on what mainly helps with the distillation. Our answer is
"early stopping". Assuming that the teacher network is overparameterized, we ar-
gue that the teacher network is essentially harvesting dark knowledge from the data
via early stopping. This can be justified by a new concept, Anisotropic Information
Retrieval (AIR), which means that the neural network tends to fit the informative
information first and the non-informative information (including noise) later. Mo-
tivated by the recent development on theoretically analyzing overparameterized
neural networks, we can characterize AIR by the eigenspace of the Neural Tangent
Kernel(NTK). AIR facilities a new understanding of distillation. With that, we
further utilize distillation to refine noisy labels. We propose a self-distillation algo-
rithm to sequentially distill knowledge from the network in the previous training
epoch to avoid memorizing the wrong labels. We also demonstrate, both theoret-
ically and empirically, that self-distillation can benefit from more than just early
stopping. Theoretically, we prove convergence of the proposed algorithm to the
ground truth labels for randomly initialized overparameterized neural networks
in terms of `2 distance, while the previous result was on convergence in 0-1 loss.
The theoretical result ensures the learned neural network enjoy a margin on the
training data which leads to better generalization. Empirically, we achieve better
testing accuracy and entirely avoid early stopping which makes the algorithm more
user-friendly.
1	Introduction
Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language
processing LeCun et al. (2015). Among these tasks, image classification is considered as one of
the fundamental tasks since classification networks are commonly used as base networks for other
problems. In order to achieve higher accuracy using a network with similar complexity as the base
network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network
to guide the training of another (student) network. In Hinton et al. (2015), the authors suggested to
generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student
network. More interestingly, Furlanello et al. (2018); Bagherinezhad et al. (2018) proposed to train a
student network parameterized identically as the teacher network. Surprisingly, the student network
significantly outperforms the teacher network. Later, it was suggested by Zagoruyko & Komodakis
(2016a); Huang & Wang (2017); Czarnecki et al. (2017) to transfer knowledge of representations,
such as attention maps and gradients of the classifier, to help with the training of the student network.
In this work, we focus on the distillation utilizing the network outputs Hinton et al. (2015); Furlanello
et al. (2018); Yang et al. (2018a); Bagherinezhad et al. (2018); Yang et al. (2018b).
To explain the effectiveness of distillation, Hinton et al. (2015) suggested that instead of the hard
labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide
extra information, which is called the "Dark Knowledge". The "Dark knowledge" is the knowledge
1
Under review as a conference paper at ICLR 2020
encoded by the relative probabilities of the incorrect outputs. In Hinton et al. (2015); Furlanello
et al. (2018); Yang et al. (2018a), the authors pointed out that secondary information, i.e the semantic
similarity between different classes, is part of the "Dark Knowledge", and Bagherinezhad et al. (2018)
observed that the "Dark Knowledge" can help to refine noisy labels. In this paper, we would like
to answer the following question: can we theoretically explain how neural networks learn the
Dark Knowledge? Answering this question will help us to understand the regularization effect of
distillation.
In this work, we assume that the teacher network is overparameterized, which means that it can
memorize all the labels via gradient descent training Du et al. (2018b;a); Oymak & Soltanolkotabi
(2018); Allen-Zhu et al. (2018). In this case, if we train the overparameterized teacher network until
convergence, the network’s output coincides exactly with the ground truth hard labels. This is because
the logits corresponding to the incorrect classes are all zero, and hence no "Dark knowledge" can
be extracted. Thus, we claim that the core factor that enables an overparameterized network to
learn "Dark knowledge" is early stopping.
What’s more, Arpit et al. (2017); Rahaman et al. (2018); Xu et al. (2019) observed that "Dark
knowledge" represents the discrepancy of convergence speed of different types of information during
the training of the neural network. Neural network tends to fit informative information, such as simple
pattern, faster than non-informative and unwanted information such as noise. Similar phenomenon
was observed in the inverse scale space theory for image restoration Scherzer & Groetsch (2001);
Burger et al. (2006); Xu & Osher (2007); Shi & Osher (2008). In our paper, we call this effect
Anisotropic Information Retrieval (AIR).
With the aforementioned interpretation of distillation, We further utilize AIR to refine noisy labels
by introducing a new self-distillation algorithm. To extract anisotropic information, we sequentially
extract knowledge from the output of the network in the previous epoch to supervise the training
in the next epoch. By dynamically adjusting the strength of the supervision, we can theoretically
prove that the proposed self-distillation algorithm can recover the correct labels, and empirically
the algorithm achieves the state-of-the-art results on Fashion MNIST and CIFAR10. The benefit
brought by our theoretical study is twofold. Firstly, the existing approach using large networks Li
et al. (2019); Zhang & Sabuncu (2018) often requires a validation set to early terminate the network
training. However, our analysis shows that our algorithm can sustain long training without overfitting
the noise which makes the proposed algorithm more user-friendly. Secondly, our analysis is based on
an '2-loss of the clean labels which enables the algorithm to generate a trained network with a bigger
margin and hence generalize better.
1.1	Contributions
We summarize our contributions as follows
•	This paper aims to understand distillation theoretically (i.e. understand the regularization
effect of distillation). Distillation works due to the soft targets generated by the teacher
network. Based on the observation that the overparameterized network can exactly fit the
one-hot labels which contain no dark knowledge, we theoretically justify that early stopping
is essential for an overparameterized teacher network to extract dark knowledge from the
hard labels. This provides a new understanding of the regularization effect of distillation.
•	This is the first attempt to theoretically understand the role of distillation in noisy label
refinery using overparameterized neural networks. Inspired by Li et al. (2019), we utilize
distillation to propose a self-distillation algorithm to train a neural network under label
corruption. The algorithm is theoretically guaranteed to recover the unknown correct
labels in terms of the `2 -loss rather than the previous 0-1-loss. This enables the algorithm
to generate a trained network whose output has a bigger margin and hence generalizes
better. Furthermore, our algorithm does not need a validation set to early stop during
training, which makes it more hyperparameter friendly. The theoretical understanding of the
overparameterized networks encourages us to use large models which empirically produce
better results.
2
Under review as a conference paper at ICLR 2020
2	DISTILLATION ≈ EARLY STOPPING?
2.1	No Early Stopping, No Dark Knowledge
As mentioned in the introduction, an overparameterized teacher network is able to extract dark
knowledge from the on-hot hard labels because of early stopping. In this section we present an
experiment to verify this effect of early stopping, where we use a big model as a teacher to teach a
smaller model as the student.
In this experiment, we train a WRN-28 Zagoruyko & Komodakis (2016b) on CIFAR100 Krizhevsky
& Hinton (2009) as the teacher model, and a 5-layers CNN as the student. The experimental details
are in the supplementary material. The teacher model was trained by 40, 80, 120, 160 and 200 epochs
respectively. The results of knowledge distillation by the student model is shown in Fig. 1.
Figure 1: A good teacher may not be able to produce a good student. To maximize the effectiveness
of distillation, you should early stop your epoch at a proper time.
As we can see, the teacher model does not suffer from overfitting during the training, while it does
not always educate a good student model either. As suggested by Yang et al. (2018a), a more tolerant
teacher educates better students. The teacher model trained with 80 epochs produces the best result
which indicates that early stopping of the bigger model can extract more informative information for
distillation.
2.2	Anisotropic Information Retrieval
In this paper, we introduce a new concept called Anisotropic Information Retrieval (AIR), which
means to exploit the discrepancy of the convergence speed of different types of information during the
training of an overparameterized neural network. An important observation of AIR is that informative
information tends to converge faster than non-informative and unwanted information such as noise.
Selective bias of an iterative algorithm to approximate a function has long been discovered in different
areas. For example, Xu (1992) observed that iterative linear equation solver fits the low frequency
component first, and they proposed a multigrid algorithm to exploit this property. In image processing,
Scherzer & Groetsch (2001); Burger et al. (2006); Xu & Osher (2007); Shi & Osher (2008) proposed
inverse scale space methods that recover image features earlier in the iteration and noise comes back
later. In kernel learning, early stopping of gradient descent is equivalent to the ridge regression Smale
& Zhou (2007); Yao et al. (2007); Vito et al. (2005), which means that bias from the eigenspace
corresponding to the larger eigenvalues is reduced quicker by the gradient descent.
Under the neural network setting, Rahaman et al. (2018); Xu et al. (2019) observed that neural
networks find low frequency patterns more easily. Cicek et al. (2018) discovered that noisy labels can
slow down the training. Arpit et al. (2017); Rolnick et al. (2017) studied the memorization effects
of the deep networks, and revealed that, during training, neural networks first memorize the data
with clean labels and later with the wrong labels. In the next subsection, we will characterize AIR of
overparametrized neural networks using the Neural Tangent Kernel Jacot et al. (2018).
3
Under review as a conference paper at ICLR 2020
2.3	AIR and Neural Tangent Kernel
In Jacot et al. (2018), the authors introduced the Neural Tangent Kernel to characterize the trajectory
of gradient descent algorithm learning infinitely wide neural networks. Denote f (θ, x) ∈ R as the
output of neural network with θ ∈ Rn being the trainable parameter and x ∈ Rp the input. Consider
training the neural network using the '2-loss on the dataset {(xi, yi)}n=1 ∈ Rp X R:
1n
'(θ) = 2∑(f (θ,χi)-%)2.
i=1
We use the gradient descent θt+1 = θt — ηVl(θt) to train the neural network. Let Ut =
(f (θt , xi))i∈[n] ∈ Rn be the network outputs on all the data {xi} at iteration t and y = (yi)i∈[n].It
was shown by Du et al. (2018b); Oymak & Soltanolkotabi (2018); Li et al. (2019) that the evolution
of the error ut — y can be formulated in a quasi-linear form,
/	∖	/ T	TT ∖ /	∖
(Ut — y) = (I — ηHt) ∙ (ut — y),
where
Ht =df("喝+1-θt - dα, d叱Xj)))
It is known that Ht — Ht = o(1) with respect to d, where Ht = (<f∂θx)
ft产))is
∂θ
i,j
an n × n positive semi-definite Gram matrix and d is the width of the neural network Oymak &
Soltanolkotabi (2018); Li et al. (2019). This means that Ht = Ht when the neural network is
infinitely wide. It was further shown by Oymak & Soltanolkotabi (2018); Li et al. (2019); Du et al.
(2018a) that when the neural network is infinitely wide, the Gram matrix is static, i.e. Ht = H *. The
static Gram matrix H * is referred to as the Neural Tangent Kernel (NTK)Jacot et al. (2018).
Note that H* is a symmetric positive semi-definite matrix. Assume that λ1 > ∙ ∙ ∙ > λn ≥ 0 are its n
eigenvalues and e1 , e2 , ∙ ∙ ∙ , en are the corresponding eigenvectors. The eigenvectors are orthogonal
< ei, ej >= 0 and H* = in=1 λieieiT. Consider the evolution of the projection of the loss function
in different eigenspaces
h(ut — y),ei〉= h(I — ηH*)(ut — y), ei〉= h(ut — y), (I — ηH*)ei〉= (1 — ηλi) h(ut — y),ei〉.
We can see that the component lies in the eigenspace with a larger eigenvalue converges faster.
Therefore, AIR describes the phenomenon that the gradient descent algorithm searches for information
components corresponding to different eigenspaces at different rates. Rahaman et al. (2018); Arpit
et al. (2017); Arora et al. (2019); Zhang & Sabuncu (2018) has shown that that one possible reason of
neural network’s good generalization property is that neural network fits useful information faster.
Thus in our paper, we regard informative information as the eigenspaces associated with the largest
few eigenvalues of NTK.
Figure 2: Components of label noise in the largest five eigensapces of NTK are decreasing.
We denote the projection of supervision signal to the eigenspace with a larger eigenvalue as useful
information. In Figure 2, We calculate the ratio of the172norm of the label vector provided by the
4
Under review as a conference paper at ICLR 2020
self-distillation algorithm lies in the top-5 eigenspace as a representative of informative information.
We can see that the informative information decreases when the noise level increases. This motivates
us to further explore how "Dark Knowledge" helps with label refinery.
3	Noisy Label Refinery
Supervised learning requires high quality labels. However, due to noisy crowd-sourcing platforms
Khetan et al. (2017) and data augmentation pipeline Bagherinezhad et al. (2018), it is hard to acquire
entirely clean labels for training. On the other hand, successive deep models often have huge
capacities with millions or even billions of parameters. Such huge capacity enables the network
to memorize all the labels, right or wrong Zhang et al. (2016), which makes learning deep neural
networks with noisy labels a challenging task. Arpit et al. (2017); Rolnick et al. (2017); Han et al.
(2018) pointed out that neural networks often fit the clean labels before the noisy ones during training.
Recently, Li et al. (2019) theoretically showed that early stopping can clean up label noise with
overparametrized neural networks. This, together with our understanding of distillation with AIR,
inspired us to use distillation for noisy label refinery. We shall introduce a new self-distillation
algorithm with theoretically guaranteed recovery of the clean labels under suitable assumptions.
3.1	Related Works
Training deep models on datasets with label corruption is an important and challenging problem
that has attracted much attention lately. In Ma et al. (2018), the authors proposed to regularize the
Local Intrinsic Dimensionality of deep representations to detect label noise. Tanaka et al. (2018)
proposed a joint optimization framework to simultaneously optimize the network parameters and
output labels. Zhang & Sabuncu (2018) introduced a loss function generalizing the cross entropy
for robust learning. The approach that is most relevant to ours is called learning with a mentor. For
example, Jiang et al. (2017) used a mentor network to learn the curriculum for the student network.
Han et al. (2018); Yu et al. (2019) introduced two networks that can teach each other to reject wrong
labels. Hu et al. (2019) designed a new regularizer to restrict every weight vector to be close to its
initialization for all iterations thus enforcing the same regularization effect as early stopping.
In the literature of distillation, Bagherinezhad et al. (2018) first utilized distillation to refine noisy
labels of ImageNet, while Yang et al. (2018b) introduced a distillation method to complete teacher-
student training in one generation. The latter is most related to our proposed self-distillation algorithm.
However, the difference is that their model aims to ensemble diverse models in one training trajectory
but ours aims to utilize AIR to refine noisy labels during the training.
3.2	The Self-distillation Algorithm
It is known that the label noise lies in the eigenspaces associated to small eigenvalues Arora et al.
(2019); Li et al. (2019). Thus, Li et al. (2019) used early stopping to remove label noise. However,
early stopping is hard to tune and sometimes leads to unsatisfactory results. In this section, we
proposed a self-distillation algorithm with an excellent empirical performance and a theoretical
guarantee to recover the correct labels under certain conditions but without the requirement of early
stopping.
From the perspective of AIR, we observe that the knowledge learned in early epochs is informative
information (i.e. the eigenspaces associated with the largest few eigenvalues of NTK) and can be used
to refine the training for later epochs. In other words, the algorithm distills knowledge sequentially to
guide the training of the model in later epochs by the knowledge distilled by the model from earlier
epochs. The informative information learned during early epochs is, in some sense, “low frequency
information", which is the core factor to enable the model to generalize well. A nice property of
the self-distillation algorithm is that it generates the final model in one generation (i.e. single-round
training), which has almost no additional computational cost compared to normal training. The
proposed self-distillation algorithm is given by Algorithm 1.
Here, the function h(∙) in the algorithm is the label function. It can either be a hard label function
such as hardmax, or a soft label function such as softmax with a certain temperature. The choice
of h(∙) and interpolation coefficient at depends on the usage of the self-distillation algorithm. If
5
Under review as a conference paper at ICLR 2020
Figure 3: Comparison of error flow among MentorNet Jiang et al. (2017), Co-teaching Han et al.
(2018) and our algorithm. Our algorithm does not require another teacher network and hence does
not impose any additional computation burden.
Algorithm 1 Self-Distillation
Randomly initialize the network. t = 0
repeat
Fetch data (χι,yι),…，(χn, yn) from training set.
Set the label Mt = αtyi + (1 - α∕h(N(x” ω∕)
Detach yi from the computational graph
UPdate ωt+1 = ωt ― η PPi=ι R ωI(N (Xi, ωt), y t,i).
t=t+1
until training converged =0
we want to clean up label noise, we normally choose h(∙) to be hardmax or Softmax with a low
temperature. The weight αt is chosen to be adaptively decreasing corresponding to the increase of
our confidence on the learned model at current epoch. The introduction of h(∙) helps to boost AIR
and the information gained from the previous epoch.
3.3	Theoretical Foundation of Self-Distillation
In this section, we provide a theoretical justification of the performance of the self-distillation
algorithm with overparameterized neural networks. Here, we only consider binary classification task
with label ∈ {-1, +1}.
Definition 1. (Noisy Clusterable Dataset DescriptionsLi et al. (2019))
•	We consider a dataset with n data: {(xi,yi,yi)}n=ι ∈ Rd × {-1, +1} × {-1, +1}. (xi,yi)
are the input data and its associated label seen by the model while yji is the unobserved
ground truth label. The pair (xi, y。with y% = yi is called a clean data, otherwise it is
called a corrupted data.
•	We assume that {xi}i∈[n] contains points with unit Euclidean norm and has K clusters.
The data is randomly i.i.d sampled from a distribution P. Let nl be the number of points in
the lth cluster. In our analysis, we ssume that number of data in each cluster is balanced
in the sense that n ≥ ClowKn for constant Clow > 0 and all of the data is bounded, i.e.
kxik22 ≤ R,∀i ∈ [n].
6
Under review as a conference paper at ICLR 2020
•	For each of the K clusters, we assume that all the input data lie within the Euclidean ball
B(cl, ), where cl is the center with unit Euclidean norm and > 0 is the radius.
•	Assume that the data in the same cluster has the same ground truth label y. For the Ith cluster,
we denote ρl the proportion of the data with wrong labels. Let ρ = max{ρi : i ∈ [K]} and
assume that ρ < 2.
•	A dataset satisfying the above assumptions is called an (, ρ) dataset.
The above definition of dataset follows that of the previous work Li et al. (2019); Li & Liang (2018).
It is reasonable to assume that ρ < 1 in order to ensure the correct labels dominate each cluster. In
this work, we consider two-layers neural networks. For input data x ∈ Rd, the output of the neural
network f is:
f (W, x) = vT φ(W x),
where W ∈ Rk×d is the weight matrix and φ is the activation function applied to Wx entry-wise.
We suppose k is even and fix the output layer by assigning half of the entries 我 and the others 一 √⅛.
Given a data matrix X = [x1, x2, . . . , xn]T, we simply denote the output vector on the data matrix as
f(W,X) = [vTφ(WXT)]T = (f(W,x1),f(W,x2),...,f(W,xn))T ∈R.
Following the previous work on training overparameterized neural network Du et al. (2018b), we
consider the MSE loss	ɪ
Definition 2. Fora data matrix D ∈m×d, we denote λ(D) the small eigenvalue of the neural network
covariance matrix
Σ(D) = (DDT)θ Eg〜N(o,id)[φ0(Dg)φ0(Dg)T].
The above definition reveals the matching score of the model and data. We denote C =
[c1, c2, . . . , cK]T the matrix composed by the center of the cluster. We denote Λ = min(λ(C), λ(X))
for simplification.
We are now ready to present our main theorem that establishes the convergence of the proposed
self-distillation algorithm to the ground truth labels under certain conditions.
Theorem 1. Assume that ∣φ(0)∣, ∣φ0(∙)∣ and ∣φ00(∙)∣ are bounded with upper bound Γ ≥ 1. Wefix
a learning rate η = 2r⅛n for the gradient descent. Assume that the SeqUenCe at monotonically
decreases to 0. Furthermore, we have two slow-decay conditions on αt
•	max2√n(αt — αt+ι) ≤ d送λ(2 (1 — 2ρ), max2√n(αs — a§+i) ≤ 5⅛⅛K(1 — 2ρ),
t<T2	s≥T2
•	ατι ≥ max(1 — ⅛Γ⅛1(1 — 2ρ), ⅜⅛),
where TI = dcSK) log( rv⅛Pog 8 升 and T2
labelfunction h(∙)
h( ] J 1 — 2ρx,
h(x) =
I sgn (x),
inf [t : at < 24√n}. We choose the following
I I 1 一 2ρ
|x| ≤
I I 1 一 2ρ
|x| >
For the self-distillation algorithm, if the following two conditions for the radius and the width k are
satisfied
e = O (√dn≡⅛} k=ω (max
K3
c3ow A3
1 nT2 K	n3 T24	1 n n
log WF, (T-IPAlog δ, Alog δ
then for random initialization Wo 〜N (0,1)k×d, With probability 1 — δ, we have:
Iim kf(Wt,X) — yk2 =0,
t→∞
where Wt is the parameter generated by the full-batch self-distillation algorithm at iteration t.
7
Under review as a conference paper at ICLR 2020
Theorem 2. Combining Theorem 1 and Neyshabur et al. (2018), with failure probability δ ∈ (0, 1),
using the self-distillation algorithm and neural network described in Theorem 1 and under the same
condition, we have
E(x,y)〜Pk lim f(Wt,x)-yk2 ≤ O((1 +
t→∞
KΓ2)
clow Λ
Compared to previous result on noisy label Li et al. (2019), our results made the following improve-
ments. Firstly, our algorithm fits the ground truth labels without the help of early stopping as long as
a mild condition on αt is satisfied. Secondly, while previous work only ensures the algorithm to yield
correct class labels on training set, our results state the `2 convergence of the outputs to the ground
truth labels. As a result, the solution that our algorithm finds tends to have larger margin which leads
to better generalization Mohri et al. (2018). This is shown in Theorem 2. and will be supported by
our empirical studies in the next subsection.
3.4	Noisy Label Refinery
In this section, we conduct experiments
on the self-distillation algorithm. In the
experiments, we applied our algorithm
on corrupted Fashion MNIST and CI-
FAR10. At noise level p, every data in
the original training dataset is chosen
and assigned a symmetric noisy label
with probability p. We test our algo-
rithm forp = 0.2, 0.4, 0.6 and 0.8. The
test accuracy is calculated with respect
to the ground truth labels.
We adopted the shake-shake network of
Gastaldi (2017) with 32 channels and
cross entropy loss. We trained the net-
work by momentum stochastic gradient
descent (SGD) with batch size 128, mo-
mentum 0.9 and a weight decay of 1e-4.
We schedule the learning rate following
cosine learning rate Loshchilov & Hut-
ter (2017) with a maximum learning rate
0.2 and minimum learning rate 0. In or-
der to ensure convergence, we trained
the models for 600 epochs. Following
Lee et al. (2015), mean subtraction, hor-
Figure 4: Training on CIFAR10 with 40% noise injection.
The normal training suffers from over-fitting, while self-
distillation does not. (Note that we are conducting cosine
learning rate scheduling. The learning rate is extremely
small at the end of learning.)
izontal random flip, 32 × 32 random crops after padding with 4 pixels on each side from the padded
image is performed as the data augmentation process. For testing, We only do evaluation on the
original 32 X 32 image. In self-distillation, We adaptively adjust at by setting 1 - at = λ * accuracy,
where accuracy is the accuracy calculated on the current batch. The direct ratio λ is the only tuning
parameter. For CIFAR10, We simply set λ as 1 When the noisy level is loW, such as p = 0, 0.2 and
0.4. When the noisy level p = 0.6 and 0.8, We take λ = 1.5. For Fashion MNIST, We simply set
λ as 0.6, 1, 1, 1.4, 1.6 respectively When p = 0, 0.2, 0.4, 0.6, 0.8. We report the average final test
accuracy of 3 runs for Fashion MNIST and CIFAR10 in Table 1.
8
Under review as a conference paper at ICLR 2020
	CIFAR10	一					FaShiOn MNIST	一				
Noise Rate	-0^^	0.2	-^04^^	0.6	0.8	-0^^	0.2	0.4	0.6	0.8
CoTeachingHan et al. (2018)	90.12	86.19	80.87	-	-	94.28	91.24	86.83	-	-
D2LMa etal.(2018)	91.29	86.64	73.12	-	-	94.47	89.12	78.98	-	-
WATDamodaran et al. (2019)	91.88	89.12	84.55	-	-	94.70	93.37	90.41	-	-
GCEZhang & SabUnCU (20Γ8T	-	89.7	87.62	82.70	67.92	-	93.21	92.60	91.56	88.33
Ours	94.31	93.75	90.82	86.72	73.91	95.21	94.58	93.78	92.63	89.21
Table 1: Results of Self-distillation.
3.5	Distillation ' Early Stopping
Distillation can benefit from more than just
early stopping. Distillation has the abil-
ity to enhance the AIR and thus can ex-
tract more dark knowledge from the data
via early stopping. For example, Hinton
et al. (2015) enhanced AIR by adjusting
the temperature in the softmax layer. In
self-distillation, the label function h(∙) is
proposed to amplify the information gained
in the earlier epochs so that the knowledge
gained in the earlier epochs is preserved.
Thus, self-distillation does not require early
stopping which makes the algorithm more
user-friendly. On the other hand, the self-
distillation algorithm dynamically enhances
AIR which enables it to achieve `2 conver-
gence and thus better generalization. Again,
we use the ratio of the norm of the label
Figure 5: Self-distillation always gains information.
vector which lies in the top-5 eigenspace as a representative of informative information. We call the
subtraction of informative information corresponding to the label vector provided by self-distillation
algorithm and original label vector as information gain. From 5 we can see that the information
gain is mostly larger than zero during the training of self-distillation algorithm. This phenomenon
indicates that the supervision signal of self-distillation algorithm gains more information than directly
using the noisy label. To sum up, a well-designed distillation algorithm can enjoy a regularization
effect beyond early stopping and is able to gain more knowledge from the data.
4 Conclusion and Discussion
This paper provided an understanding of distillation using overparameterized neural networks. We
observed that such neural networks posses the property of Anisotropic Information Retrieval (AIR),
which means the neural network tends to fit the infomrative information (i.e. the eigenspaces
associated with the largest few eigenvalues of NTK) first and the non-informative information later.
Through AIR, we further observed that distillation of the Dark Knowledge is mainly due to early
stopping. Based on this new understanding, we proposed a new self-distillation algorithm for noisy
label refinery. Both theoretical and empirical justifications of the performance of the new algorithm
were provided.
Our analysis is based on the assumption that the teacher neural network is overparameterized. When
the teacher network is not overparameterized, the network will be biased towards the label even
without early stopping. It is still an interesting and unclear problem that whether the bias can provide
us with more information. For label refinery, our analysis is mostly based on the symmetric noise
setting. We are interested in extending our analysis to the asymmetric setting.
9
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Devansh Arpit, StanislaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, MaXinder S
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at
memorization in deep netWorks. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 233-242. JMLR. org, 2017.
Hessam Bagherinezhad, MaXWell Horton, Mohammad Rastegari, and Ali Farhadi. Label refinery:
Improving imagenet classification through label progression. arXiv preprint arXiv:1805.02641,
2018.
Martin Burger, Guy Gilboa, Stanley Osher, Jinjun Xu, et al. Nonlinear inverse scale space methods.
Communications in Mathematical Sciences, 4(1):179-212, 2006.
Safa Cicek, Alhussein FaWzi, and Stefano Soatto. Saas: Speed as a supervisor for semi-supervised
learning. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 149-163,
2018.
Wojciech M Czarnecki, Simon Osindero, MaX Jaderberg, Grzegorz SWirszcz, and Razvan Pascanu.
Sobolev training for neural netWorks. In Advances in Neural Information Processing Systems, pp.
4278-4287, 2017.
Bharath Bhushan Damodaran, Kilian Fatras, Sylvain Lobry, Remi Flamary, Devis Tuia, and Nicolas
Courty. Pushing the right boundaries matters! Wasserstein adversarial training for label noise.
arXiv preprint arXiv:1904.03936, 2019.
Simon S Du, Jason D Lee, Haochuan Li, LiWei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural netWorks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural netWorks. arXiv preprint arXiv:1810.02054, 2018b.
Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural netWorks. arXiv preprint arXiv:1805.04770, 2018.
Xavier Gastaldi. Shake-shake regularization. CoRR, abs/1705.07485, 2017.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural netWorks With eXtremely noisy labels. In
Advances in Neural Information Processing Systems, pp. 8527-8537, 2018.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knoWledge in a neural netWork. arXiv
preprint arXiv:1503.02531, 2015.
Wei Hu, Zhiyuan Li, and Dingli Yu. Understanding generalization of deep neural netWorks trained
With noisy labels. arXiv preprint arXiv:1905.11368, 2019.
Zehao Huang and Naiyan Wang. Like What you like: KnoWledge distill via neuron selectivity transfer.
arXiv preprint arXiv:1707.01219, 2017.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural netWorks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
data-driven curriculum for very deep neural netWorks on corrupted labels. arXiv preprint
arXiv:1712.05055, 2017.
10
Under review as a conference paper at ICLR 2020
Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data.
arXiv preprint arXiv:1712.04577, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised
nets. In Artificial Intelligence and Statistics, pp. 562-570, 2015.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. CoRR, abs/1903.11680,
2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NeurIPS, 2018.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR,
2017.
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. arXiv preprint
arXiv:1806.02612, 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. arXiv preprint
arXiv:1805.12076, 2018.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? arXiv preprint arXiv:1812.10004, 2018.
Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht,
Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint
arXiv:1806.08734, 2018.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise. arXiv preprint arXiv:1705.10694, 2017.
Otmar Scherzer and Chuck Groetsch. Inverse scale space theory for inverse problems. In International
Conference on Scale-Space Theories in Computer Vision, pp. 317-325. Springer, 2001.
Jianing Shi and Stanley Osher. A nonlinear inverse scale space method for a convex multiplicative
noise model. SIAM Journal on imaging sciences, 1(3):294-321, 2008.
Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their
approximations. Constructive approximation, 26(2):153-172, 2007.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework
for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5552-5560, 2018.
Ernesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini, and Francesca
Odone. Learning from examples as an inverse problem. Journal of Machine Learning Research, 6
(May):883-904, 2005.
Jinchao Xu. Iterative methods by space decomposition and subspace correction. SIAM review, 34(4):
581-613, 1992.
Jinjun Xu and Stanley Osher. Iterative regularization and nonlinear inverse scale space applied to
wavelet-based denoising. IEEE Transactions on Image Processing, 16(2):534-544, 2007.
11
Under review as a conference paper at ICLR 2020
Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle:
Fourier analysis sheds light on deep neural networks. arXiv preprint arXiv:1901.06523, 2019.
Chenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan Yuille. Knowledge distillation in generations:
More tolerant teachers educate better students. arXiv preprint arXiv:1805.05551, 2018a.
Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation: Teacher-student
optimization in one generation. arXiv preprint arXiv:1812.00123, 2018b.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning.
Constructive Approximation, 26(2):289-315, 2007.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does
disagreement benefit co-teaching? arXiv preprint arXiv:1901.04215, 2019.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-
mance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928,
2016a.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016b.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In Advances in Neural Information Processing Systems, pp. 8778-8788, 2018.
12
Under review as a conference paper at ICLR 2020
A Proof Details
A.1 Neural Network Properties
As preliminaries, we first discuss some properties of the neural network. We begin with the jacobian
of the one layer neural network X → VTφ(Wx), the Jacobian matrix with respect to W takes the
form
JT(W) = (diag(v)φ0(WX|)) * X|
Thus
J(W)J(W)T = (φ0(XW T)diag(v)diag(v)φ0(XW T))	(XXT)
First we borrow Lemma 6.6, 6.7, 6.8 from Oymak & Soltanolkotabi (2018) and Theorem 6.7, 6.8
from Li et al. (2019).
Lemma 1. Let X = [x1, x2, . . . , xn]T be a data matrix made up of data with unit Euclidean norm.
Assuming that λ(X ) > 0, the following properties hold.
•	k J (w,χ)- J(w,x )k≤ r√kn kw - W kF,
•	kJ(w,x)k ≤ γ√n,
•	AS long as k ≥ ^]nlog δ, at random Gaussian initialization Wo 〜N(0, l)k×d, With
probability at least 1 - δ, we have
σmin(J(W0, X)) ≥
λ(X)
(1)
Lemma 2. Let X = [x1, x2, . . . , xn]T be the data matrix of a -clusterable dataset. Set X =
[Xi, X2,..., Xn]T in which Xi corresponds to the center ofcluster including Xi. What's more, we
define the matrix of cluster center C = [c1, c2, . . . , cK]T. Assuming that λ(C) > 0, the following
properties hold.
•	kJ(W,^X) -J(W,X)k ≤ r√√upnkW- WkF ≤ r√knkW- WkF,
•	kJ(W, X)k ≤ Γ√cupn ≤ Γ√n,
•	As long as k ≥ ^r Kog 丁, at random Gaussian initialization W0 〜N(0,1)k×d, with
probability at least 1 - δ, we have
σmin(J(Wθ, X), S+) ≥ ∖卜。WRC ,	⑵
2K
/ T∕TT7- 6∖∖ .— cr	.	. ■ TTT-
•	range(J(W, X)) ⊂ S+ for any parameter matrix W.
Then, we gives out the perturbation analysis of the Jacobian matrix.
Lemma 3. Let X be a E-clusterable data matrix with its center matrix X. For parameter matrices
W, W, we have
k J(W,X) - J(W, X) k ≤ √k (IlW - WkF + IIWkE + √kE)	⑶
.. , ,~ ~ ...
Proof. Webound ∣∣J(W, X) - J(W, X)k by
.. , ~ ~ . .. .. , ~ . ..	.. , ~ ~ ~ ...
kJ(W,X) - J(W,X)k ≤ kJ(W,X) - J(W,X)k + kJ(W,X) - J(W,X)k
The first term is bounded by Lemma 1. As to the second term, we bound it by
13
Under review as a conference paper at ICLR 2020
J(W,X) - j(W,X)k = √kkφ0(WXT) * XT - φ0(WXT) * XTk
≤√=(kφ0(WXT) * XT - φ0(WXT) * XTk
k
+ kφ0(W XT) * XT - φ0(W XT) * XT k)
≤√f(kφ0 (W X T) - φ0(W XT )k
k
+ kΦ0(WXT) * (X - X)Tk)
≤卞(ΓkWkkX - Xk +Γ√kkX - Xk)
≤
kWk + √k)
Combining the inequality above, we get
.. . .〜 ~ .
kJ(W,X) - J(W,X)k ≤
kW - WkF + kWke + √ke)
Lemma 4. Let X be a -clusterable data matrix with its center matrix X. We assume kW1k, kW2k
have a upper bound c k. Then for parameter matrices W1, W2, W1, W2, we have
.. . . .~ ~ ~	,—,
kJ(W1,W2,x) - J(Wι,w2,x)k ≤ r√n(
kW1 - W 1kF + kW2 - W2 kF
+ (c + 1))	(4)
Proof. By the definition of average Jacobian, we have
.. , ~ ~ ~ .
kJ(W1,W2,X) - J(W1,W2,X)k
1
≤ / kJ(W2 + α(W1 - W2),X) - J(W2 + α(W1 - W2),X)∣∣dα
0
≤
≤
≤Γ √n(
,.. . ... 一 一
(kα(Wι - WI) + (1 - α)(W2 - W2)kF + ∣∣W2 + α(W1 -
W2)∣∣E + √ke)da
.. ... ..
(akWι - WιkF + (1 - α)∣∣W2 - W2∣∣f + (α∣∣W1 k + (1 -
kW1 - WIkF + kW2 - W2 kF
+ (c+ 1))
A.2 Prove of the theorem
First, we introduce the proof idea of our theorem. Our proof of the theorem divides the learning
process into two stages. During the first stage, we aim to prove that the neural network will give out
the right classification, i.e. the 0-1-loss converges to 0. The proof in this part is modified from Li
et al. (2019). Furthermore, we proved that training 0-1-loss will keep 0 until the second stage starts
and the margin at the first stage will larger than 1-2ρ. During the second stage, We prove that the
neural networks start to further enlarge the margin and finally the `2 loss starts to converge to zero.
Following Oymak & Soltanolkotabi (2018); Li et al. (2019); Du et al. (2018b), we directly analysis
the dynamics of each individual prediction f (W, Xi) for i = 1,2, ∙∙∙ , n.Oymak & Soltanolkotabi
(2018); Li et al. (2019) has shown that this dynamic can be illustrated by the average Jacobian.
Definition 3. We define the average Jacobian for two parameters W1 and W2 and data matrix X as
J(W1,W2,X)
Z1
0
J(W2 + α(W1
- W2 ), X)dα.
(5)
□
〜
〜
〜
〜

〜
〜
〜
□
14
Under review as a conference paper at ICLR 2020
< .   _ / T ∙ , τ/CCTC∖r	∕C∖∕ι∙	1 ∙ . 1	. ∙ .	. A 八	L7 T / ∕A∖ 1 C
Lemma 5. (Li et al. (2019) Lemma 6.2) Given gradient descent iterate θ = θ 一 ηVL(θ), define
.. ,ʌ _,
C(θ) = J (θ,θ)Jl(θ)
The residual r = f (θ) — y, r = f (θ) — y obey the following equation
r = (I — ηC(θ))r
In our proof, we project the residual to the following subspace
Definition 4. Let {xi}n=ι be a E-clusterable dataset and {Xi}n=ι be the associated cluster centers,
that is, Xi = Cl iff Xi is from lth cluster We define the support subspace S+ as a subspace of
dimension K, dictated by the cluster membership as follows. Let Λl ⊂ {1,2, ∙∙∙ ,n} be the set of
coordinates i such that = cl. Then S+ is characterized by
S+ = {v ∈ Rn|vi1 =vi2∀i1,i2 ∈Λl,1 ≤ l ≤ K}
Definition 5. We define the minimum eigenvalue ofa matrix B on a subspace S
σmin (B, S) =	min	kvTUTBk2,
kvk2 =1,U U T =PS
where PS is the projection to the space S.
Recall the generation process of the dataset
Definition 6. (Clusterable Dataset Descriptions)
•	We assume that {xi}i∈[n] contains points with unit Euclidean norm and has K clusters. Let
nl be the number of points in the lth cluster. Assume that number of data in each cluster is
balanced in the sense that nl ≥ ClowKn for constant Clow > 0.
•	For each of the K clusters, we assume that all the input data lie within the Euclidean ball
B(Cl, E), where Cl is the center with unit Euclidean norm and E > 0 is the radius.
•	A dataset satisfying the above assumptions is called an E-clusterable dataset.
A.2.1 The First Stage: Fitting the label
First, we reduct the dataset to its cluster center, i.e.
Lemma 6. We fix the label function
E = 0 for E-clusterable dataset.
(4
I ---x
h(x)=	1 - 2P
I Sgn (x)
|x| ≤
|x| >
1 — 2ρ
1 — 2ρ
Let {xi}n=ι be a E-clusterable dataset and {Xi}n=ι
be the associated cluster centers, that is, Xi = Cl
iff xi is from lth cluster. We denote the data matrix X and X. We denote α
Jy(C),β = Γ√n
and L = r√n. We Set the learning rate η = min(春,L^), where Θ is maximum ofthe residual
norm during the optimization. We suppose along the optimization path we have α ≤ ∣∣ J (W, X)v∣∣ ≤ β
1-2ρ
for all V ∈ S+. We set Ti =dlog]-no2 8-2p ], where ro = Ps+ (f (Wo, X) — yo) is the projected
residual on the space S+. Then ∀t ≥ T1, we have
•	The neural network can learn the true label
,,, ~.
sgn (f)(Wt,X) = y.
•	The weight vector will be close to its initialization for all iterations
∞∞
X kWt+1 — WtkF ≤ ηβX 忻t∣2 ≤ α(4忻o∣2 + 8√n).
t=0
t=0
15
Under review as a conference paper at ICLR 2020
Proof. We denote Ci，max2√n(αt - αt+ι). We denote Jt = J(Wt,X). Follow the step of
gradient descent, we have
kWt+ι - WtkF = ηkjTrtk2 ≤ ηkjTrtk2 ≤ ηβkrt∣∣2	⑹
We denote Gt = J(Wt+1, Wt, X)J(Wt, X)T. Then the dynamic of gradient descent Can be written
by
~ . ~ .
f(Wt+ι,X)= f(Wt,X)- ηGtrt.	(7)
We consider the dynamic of the residual rt
rt+1 = (I - ηGt)rt + yt - yt+1.
We project the residual on S+
rt+i = (I 一 ηGtkt + yt 一 yt+ι.
Thus, the norm of the residual can be bounded by
...	... ... ~.. ~....
k尸t+1k2 ≤k(I — ηGt)尸tk2 +(I- α/Ilhf(Wt,X))- h(f(Wt+ι,X))Il2	(8)
... ~.
+ (at- αt+ι)ky 一 h(f (Wt+ι,X))k2	(9)
≤k(I — ηGt)尸tk2 + (1 - αt)ηLhβ Mk2 + 2√n(αt - αt+ι)	(10)
Utilizing the following lemma
Lemma 7. (Claim2. Li et al. (2019)) Let PS+ be the projection matrix to S+, then the following
inequality holds
β2PS+ 占 Gt 占 1 JtJT 占 CaPS+
on the condition that η ≤ Lek k2 ∙
Therefore, as long as η ≤ Le黑八? holds, we have
2
忻t+1k2 ≤ (1 ——2-)忻tk2 + (1 - αt)ηLhβ2kftk2 + 2√n(αt - αt+ι).
(11)
2
When at ≥ 1 - 4 02 L , we have
4e Lh
2
k尸t+1k2 ≤ (1------^)k^tk2 + Ci.
By simple calculation, we have
krtk2 ≤(I- ηα- )tkr0k2 + W C1.
After Ti = log]-处 ⅛¾ iterations, we have
1 - 2ρ
krTιk2 ≤ 一4一
2	7 - 3 P
as long as Ci ≤ 哈(1 - 2ρ). On the condition that α^ ≥ 1 - ---ρ, we have
3
yτι(i)y(Z) ≥ 4(I - 2ρ) i = 1,2,...n.
As a result, all the data have been classified correctly at iteration Ti and the following inequality
holds
1 - 2ρ
f(WTι,xi)y(i) ≥ —2-,	i = 1,2,∙∙∙n∙
For the next step, we use induction to prove that
h(ft) = y, k九k2 ≤ 1 42ρ, t ≥ Ti.
(12)
16
Under review as a conference paper at ICLR 2020
For t = T1 , it has already been proved. We assume the claim is correct for arbitrary t, we establish
the induction for t + 1. By applying projection to S+ on equation 7, we have
f (Wt+1, X)- yt = (I - ηGt)rt.
By applying lemma 7, we have
1 - 2ρ
kf(Wt+1,X)-ytk2 ≤ 忻tk2 ≤ --4-ρ.
Given that h(ft) = y, We have
yt(i)y(i) ≥ - - 2p,	i = -, 2,...,n.
Thus, for ft+1 We have
3
f (Wt+1,Xi),Ui ≤ 4(- - 2P), % = 1, 2,...,n.
By the definition of h(∙), We deduce that h(ft+ι) = y. Back to equation 8, We have	
.., ... 	 ~.. ~.... k尸t+1∣∣2 ≤II(I - ηGt)rtk2 + (- - αt)∣∣h(f(wt,x)) - h(f(wt+1,x))∣∣2	(13)
... ~.... + (at - αt+1)kyJ - h(f(wt+1,X))∣∣2	(14)
2 ≤(-	2-)忻tk2 + 2√n(αt - ɑt+1)	(15)
2 ≤(-	2^)kftk2 + CI	(16)
ηα2 - - 2p ηα2 ≤(--亏)—^ + J- 2P)	(17)
≤ -- 2p _	4	(18)
Finally, We estimate the total variation of the parameter. Combining equation 11 and 12, We can
conclude that inequality
2
k尸t+1k2 ≤ (1------^)k尸tk2 + 2√n(αt - αt+1)
holds for all t ≥ 0. After taking sum on both sides for t = 0, -, 2, . . . , We have
∞	2∞
X 忻 tk2 ≤ (-- η0-) X 忻 k2 + 2√n.
t=1	t=0
By simple calculation, We get
X 忻 tk2 ≤ 4 忻 k2 + 86 .
ηα2
t=0
Combining equation 6, We have
∞∞
X kWt+1 - WtkF ≤ ηβX 忻k2 ≤ α(4忻0k2 + 8√n).
t=0	t=0
□
A.2.2 Second Stage: Enlarge The Margin
Then We further to adapt the above theorem to the -clusterable dataset by a pertubation analysis.
Since We have a simple inequality αa ≥ αb, in the folloWing discussion, We simply replace the αa in
the previous conclusion by αb, the conclusion also holds.
17
Under review as a conference paper at ICLR 2020
Lemma 8. Let {xi}n=ι be a E-clusterable dataset and {Xi}n=ι be the associated cluster centers, that
is, Xi = Cl Iff Xi is from lth cluster. We denote the data matrix X and X. For the same initialization
Wo = Wo, we run the self-distillation algorithm on X and tX respectively. We denote the parameter
matrix Wt and Wt for t ≥ 0. We denote a = JClownA,β = Γ√n and L = r√n. We set the learning
rate η = min(泰,l0θ ), where Θ is maximum of the residual norm during the optimization. We
denote c√k the upper bound of the Frobenius norm of the parameter matrxi and we set M = C + 1.
We set T2 = inf {t : at < 24√n}. Then ifthefollowing conditions hold
e ≤___________1-2P___________
€ — 4Mηβ(2 + Lh )Γ√nT2(l + Θ)
k ≥ 4η2Γ2Θ2n(2β2 (2 + Lh)2T2 + 1)2T22,
we have
kf (Wt, X) - f (Wt, X)k2 ≤ 4ηβ(2 + Lh)ΘMΓ√net
kWt - WtkF ≤ 2ηMΓΘ√n(2β2(2 + Lh)2T2 + 1)Et
for all t ≤ T2.
Proof. We introduce the following notations.
, ~
rt = f(Wt, X) - yt, Trt = f(Wt, X) - yt
~ ~ . ~ ~ .
Jt = Jt (Wt, X), Jt = Jt(Wt,X)
, , ~ , ~ ~ ~ .
Jt+1,t =J(WT+1,Wt,X),JJt+1,t =J(WJT+1,WJt,XJ)
dt = kWt-WJtkF,pt= kf(Wt,X)-f(WJt,XJ)k2.
We can conclude the following inequalities from lemma 3 and lemma 4
IIJt- JtIl ≤ Ldt + MΓ√ne,
kJt+ι,t - jt+ι,tk ≤ L~~~2~+~ + Mr√ne.
Thus the parameters are updated by gradient descent, we have
dt+1 = kWt+1 - WJ t+1 kF ≤ kWt - WJ tkF + kηJtT rt - ηJJt Trt k2
~ ....	....
≤ dt + ηJt - Jtkk Trt I∣2 + ηkJtkkrt - Trt I∣2
≤ dt + η(LΘdt + MΓΘ√ne + β(1 + Lh)Pt)
Also, we have
... ,~ ~ ....
pt+1 =kf(Wt+1,X)-f(WJt+1,XJ))k2
T
≤kf(Wt,X)-f(WJt,XJ)-ηJJt+1,tJJtT(rt-Trt)k2
+ ηk(Jt+1,t - JJt+1,t)JtTrtk2 + ηkJJt+1,t(JtT - JJt )rtk2
≤kf(Wt,X)-f(WJt,XJ)-ηJJt+1,tJJtT(rt-Trt)k2
+ ηβkrtk2(L3dt ∖dt+1 +2MΓ√⅛)
TT
≤k(1 - ηJJt+1,tJJtT)(f(Wt,X) - f(WJt,XJ))k2 + ηkJJt+1,tJJtT(yt - yJt)k2
+ ηβkrtk2(L3dt ∖dt+1 +2MΓ√⅛)
2
≤(1 - η2-)Pt + ηβ2(1 - at)kh(f(Wt,X)) - h(f(Wt,X))k2
+ ηβkrtk2(L 3dt +i dt+1 +2M Γ√⅛)
18
Under review as a conference paper at ICLR 2020
If Pt ≤ 1-42ρ holds for t ≤ T2, then for T1 ≤ t ≤ T2, We have f(Wt,X) = y. Under such
circumstance, We have ∣∣h(f(Wt, X)) - h(f(Wt, X))k2 = 0. For t < Ti, We have
2
(1 - η2-)Pt + ηβ2(1 - at)∣h(f(Wt,X)) - h(f(Wt,X))∣2
2
≤(1----2-)pt + ηβ2(1 - at)LhPt
22
≤(1 - ~Tppt + ηβ24β2LhLhPt
≤pt .
To sum up, if We can guarantee that Pt ≤1-2ρ holds for t ≤ T2, We have
Pt+i ≤ Pt + ηβ∣rt∣∣2(L3dt +2dt+1 + 2MΓ√ne).
For ∣rt∣2, We have
∣rt∣2 ≤∣ Trt ∣2 + ∣rt - Trt ∣2
≤k Trt ∣∣2 +(I + Lh)Ilf(Wt,χ) - f(Wt,X)Il2
≤∣ Trt ∣2 + (1 + Lh)Pt
≤Θ + (1 + Lh)Pt.
Thus We have the folloWing inequality for Pt
Pt+i ≤ Pt + ηβ(Θ + (1 + Lh)pt)(L3dt -2dt+1 + 2MΓ√ne).
We claim that if the folloWing conditions for and k hold
≤ _________1 - 2ρ_________
e ≤ 4Mηβ(2 + Lh)Γ√nT2(l +⑼
k ≥ 4η2Γ2Θ2n(2β2(2+ Lh)2T2 + 1)2T22,
one can shoW that
Pt ≤ 4ηβ(2 + Lh)ΘMΓ√net
dt ≤ 2ηMΓΘ√n(2β2(2 + Lh)2T2 + 1)et
for t ≤ T2 by induction. It is obviously When t = 0. We further suppose the inequalities hold for an
arbitrary t satisfying t < T2, We have
dt+i ≤ dt + η(LΘdt + MΓΘ√ne + β(1 + Lh)Pt)
≤ dt + η(2MΓΘ√n∈ + 4ηβ(2 + Lh)ΘMΓ√⅛tβ(1 + Lh))
≤ dt + η(2MΓΘ√n∈ + 4ηβ2(2 + Lh)2ΘMΓ√⅛T⅛)
≤ 2ηMΓΘ√n(2β2(2 + Lh)2T2 + 1)e(t + 1)
because of the condition on k ensures that LΘdt ≤ MΓΘ√ne. When it comes to Pt+i, we have
Pt+i ≤ Pt + ηβ(Θ + (1 + Lh)Pt)(L3dt +2dt+i + 2MΓ√ne)
≤ Pt + ηβ(Θ + (1 + Lh)Θ)(4LηMΓΘ√n(2β2(2 + Lh)2T2 + 1)e(t + 1) + 2MΓ√ne)
≤ Pt + ηβ(Θ + (1 + Lh)Θ)(2Ldτ2 + 2MΓ√ne)
≤ Pt + ηβ(2 + Lh)Θ(2MΓ√ne + 2MΓ√ne)
≤ Pt +4ηβ(2 + Lh)ΘMΓ√ne
≤ 4ηβ(2 + Lh)ΘMΓ√ne(t + 1)
because of the conditions on E and k ensure that Ldτ2 ≤ MΓ√ne and Pt ≤ Θ.
□
19
Under review as a conference paper at ICLR 2020
Now we are ready to finalize the proof of our main theorem.
Proof. We denote C2，max2√n(αs - a§+i). We take Θ = (C3「，log 8 + 1)√n. C3 is the
s≥T2	δ
constant in HOeffding's inequality. By lemma 6, We have Θ ≥ maxt≥o ∣∣T⅛t∣∣2 with probability 1 - 4.
Combining lemma 6 and 8, we have f (Wt, X) = y for Ti ≤ t ≤ T2 and
∣rT2 ∣2 = ∣f (WT2 , X) - yT2 ∣2
... . ,~ ~... . ~ ~. — ..— ..
≤ kf(Wτ2,X) -f(WT2,X)∣2 + kf(WT2,X)-近212 + kyτ2 -yτ212
.. , ,~ ~ ...	..—
≤ kf(WT2 ,X) - f(WT2 ,X)∣2 + kTrT2k2 + αT2 ky - yk2
≤ q + i-aρ + ɪ
一 4	4	24
Similar to the proof in 6, we consider the gradient descent on original dataset X after T2. We proof
the following claim by induction
51
h(f (Ws), X )= y, krsk2 ≤ /I - 2P) + 五,S ≥ T2.
8	24
For s = T2, it has already been proved. We assume the claim is correct for arbitrary s, we establish
the induction for s + 1. By equation 7, we have
f(Ws+i,X) - ys = (I - ηGsKs.
By applying lemma 7, we have
kf(Ws+1,X)-ys∣2 ≤ krs∣2 ≤ 5(1 - 2ρ) + ⅛.
8	24
Given that h(f (Ws,X)) = y, we have
ys(i)y(i) ≥ 1 - 2ατ2, i = 1, 2,...,n.
Thus, for f(Ws+1, xi) we have
f(Ws+1,xi)y(i) ≥ 1 - 2αT2 - 6(I - 2ρ) —五 ≥ -^T^P.
8	24	4
As a result, we have f (Ws+ι,X) = y. Furthermore, we can bound ∣rs+ι ∣∣2 by
krs+1k2 ≤k(I - ηGs)rsk2 + (1 - αs)kh(f (Ws, X)) - h(f (Ws+1, X))k2
+ (as - αs+ι)ky - h(f (Ws+1,X))∣∣2
2
≤(1 - ηr)Ksk2 + C2
≤(1 - ηα2)(5(1 - 2ρ) + ɪ) + ηα2(1 - 2ρ)
2	8	24	32
51
≤8(I- 2ρ)+24.
To sum up, we have the following inequality holds for all s ≥ T2
2
krs+1∣∣2 ≤ (1-2-)krsk2 + 2√n(αs - αs+1).
After taking sum on both sides for s ≥ T2, we have
∞
X krsk2 ≤
s=T2
2krT2ks + 1
ηα2
4 - 2ρ
ηα2
≤
Furthermore, we have
∞
s=T2
kWs+1 - WskF ≤ηβ	krsk2
s=T2
β5
≤ θɪ(4 - 2ρ).
∞
20
Under review as a conference paper at ICLR 2020
Finally, we check all the conditions for α and η.
For η, We require η ≤ 2β2 and η ≤ Le minT (k⅛, 1⅛). For k ≥
,s 2
O( nK loΛ 1 )，we have
clow
-α min (^1—,^^) ≥ ɪ.
Le t≥0,s≥T2(kTrt∣∣2 , krs k2) ≥ 2β2
2n(C3ΓvΛ0g 8 + 1)2K
clow λ
On such condition, we can choose η = 嘉 =2j⅛n. The distance between the intermediate parameter
matrix and the initial parameter matrix can be bounded by
_	…	「一 -	. β.... 一、	β ,5^
R=tm≥o(kWs- W0kF, kWt -W0kF) ≤ α^(4kr0k2 + 8√n)+ dT2+ α2(4- 2ρ) (19)
8K γ	I^^8
C1OWΛ (4c3rVlog δ + 13) + dT2.
(20)
By lemma 1 and lemma 2, as long as k ≥ 20γ “og 丁, with probability 1 - 2, we have
,___ ____ , _______ ʌ. _ . .
σmin (J(W0,X )), σmin(J(W0, X), S+) ≥ 2ɑ
To ensure α lower bounding the eigenvalue of the gram matrix, we need to verify that
RL
≤α
8K Γ2R2
clow Λ
That is to say
k≥
(21)
≤
Another condition related to R is the condition on M . We require
(M - 1)瓜 ≥ kW0∣∣F + R.
By Bernstein’s inequality, we have
kW0 kF ≤
4d + C4 log 8 √k
with probability 1 - δ∕4. On the condition that k ≥ R2(acutually one can show that 8Kr2R2 ≥ R2),
We can choose M = Jd + C4 log 8 + 2.
We take these constants to the lemma 8. Firstly, for we have
≤____________1 - 2ρ___________
E ≤ 4Mηβ(2 + Lh)Γ√nT2(1 + Θ)
_	1 - 2ρ
2(Jd + C4 log δ + 2)(2 + ι-42ρ)(1 + (C3 γ Jlog 8 + 1)√n)
=O( B- 2ρ)21).
√ndT2 log 1
For k we have
k ≥ 4η2Γ2Θ2n(2β2(2 + Lh)2T2 + 1)2T22
=2Θ2(2Γ2n(2 + -ɪ-)2^ + 1)2T2
1 - 2ρ
=n3T4 log 1
=(1 - 2ρ)2
21
Under review as a conference paper at ICLR 2020
We point out that
/ n3T4 log 1、	/ nK log 1、
O( (⅛) ≥ O(").
Secondly we bound dT2 by
dτ2 ≤ 2〃MΓΘ√n(2β2 (2 + Lh)2T2 + l)eT2
≤ (1 - 2ρ)2ηMΓΘ√n(2β2(2 + LhYT2 + 1)72
≤	4Mηβ(2 + Lh )Γ√nT2(1 + Θ)
≤ (I- 2ρ)(β(2 + Lh)T2 + 2β(2 + Lh))
=O(√nT2).
(22)
(23)
(24)
(25)
We combine equation 20, 21 and 25, we deduce the last condition for k
k = Ω
nKT2
clow Λ
To sum up, we require k to satisfy
k = Ω max
K3
c3ow A3
1 nKT2	n3T24	1 n n
log δ,F, (1flog δ, Alog δ
and require to satisfy
e = o √ B- 切21!
y√ndT2 log δ)
We substitute C1 and C2 by the value of α in lemma 6 and lemma 8 respectively and then get the
condition for αt
•	max2√n(αt - αt+1) ≤ 鸵准) (1 - 2ρ), max2√n(αs - a§+i) ≤ 5⅛⅛K(1 - 2ρ),
t<T2	s≥T2
•	ατι ≥ max(1 - ⅛(f (1 - 2ρ), ^-P).
22
Also, We substitute Ti by the value of α in lemma 6, and combine the fact that %=昼 ≤ 1, we
have
Ti = Iogi-ηα2 8>0∣
≤
log
8kr0k2
i-2ρ
log τ⅛2
i 4~
≤
工log冲
ηα2	1 - 2ρ
r 80Γ2K 1 Γ√32nlog 8
≤ d CIowZC Og —1 - 2ρ 一 )e.
□
22
Under review as a conference paper at ICLR 2020
B	Experiment Detials
Experiment In Section2.1 The network structure of the small student network is demonstrated in
Table 2.
Type	Kernel	Dilation	Stride	Outputs	Remark
conv.	3×3	1	1×1	32	bn
maxpool.	2×2	-	-	-	
conv.	3×3	1	1×1	64	bn
maxpool.	2×2	-	-	-	
conv.	3×3	1	1×1	128	bn
maxpool.	2×2	-	-	-	
Flatten					
fc.	-	-	-	512	bn
fc.	-	-	-	10	dropout
Table 2: Architecture of the student network. After each convolution layer, there is a Rectified Linear
Unit(ReLU) layer.
Experiment In Section2.3 and Section3.5 For these two experiments, we modify CIFAR10 to a
binary classification task. We choose class 2, 7 to be the positive class and the others to be negative.
We train resnet56 with MSE loss. We set batch size 128, momentum 0.9, weight decay 5e - 4 and
learning rate 0.1.
In the experiment in section2.3, we fetch a batch of data (batch size=128) from the testset and
randomly corrupted the label by noise level 0, 0.1, 0.2, 0.3, 0.4, 0.5. We plot the ratio of the norm of
the label vector which lies in the subspaces corresponding to top-5 eigenvalues of NTK.
In the experiment in section2.3, we calculate the ratio of the norm of the label vector which lies
in the subspaces corresponding to top-5 eigenvalues of NTK firstly. We calculate the ratio of the
norm of the label vector provided by the self-distillation algorithm lies in the top-5 eigenspace. We
calculate the difference of the latter ratio and the former ratio and called it information gain. We plot
the information gain of the first 1500 iterations.
23