Under review as a conference paper at ICLR 2020
Semi-Supervised Boosting via Self Labelling
Anonymous authors
Paper under double-blind review
Ab stract
Attention to semi-supervised learning grows in machine learning as the price to
expertly label data increases. Like most previous works in the area, we focus on
improving an algorithm’s ability to discover the inherent property of the entire
dataset from a few expertly labelled samples. In this paper we introduce Boosting
via Self Labelling (BSL), a solution to semi-supervised boosting when there is
only limited access to labelled instances. Our goal is to learn a classifier that is
trained on a data set that is generated by combining the generalization of different
algorithms which have been trained with a limited amount of supervised training
samples. Our method builds upon a combination of several different components.
First, an inference aided ensemble algorithm developed on a set of weak classifiers
will offer the initial noisy labels. Second, an agreement based estimation approach
will return the average error rates of the noisy labels. Third and finally, a noise-
resistant boosting algorithm will train over the noisy labels and their error rates to
describe the underlying structure as closely as possible. We provide both analyti-
cal justifications and experimental results to back the performance of our model.
Based on several benchmark datasets, our results demonstrate that BSL is able to
outperform state-of-the-art semi-supervised methods consistently, achieving over
90% test accuracy with only 10% of the data being labelled.
1	Introduction
The rise of the Internet has made it easy to collect massive amounts of data to perform machine
learning tasks. However, providing quality labels to each of the samples collected within these large
datasets is a long and expensive process. There is a rich literature aiming to alleviate this issue,
including using techniques from unsupervised machine learning and crowdsourcing. In this paper,
we approach the problem by combining concepts from crowdsourcing, learning with noisy data and
boosting to propose a novel framework: Boosting via Self Labelling (BSL).
Our aim is to develop and leverage i) machine learning and estimation approaches to create self-
labels for unlabelled instances, and ii) a noise-resistant learning procedure to speed up the perfor-
mance of the seminal AdaBoost algorithm. The framework consists of mainly three steps:
1.	Accurately predict labels for the unlabelled part of the data using a set of supervised classifiers
trained upon a small labelled dataset. Each classifier in our set is analogous to an agent in a
crowdsourcing setting. As a result, an inference method can be used to aggregate each of the
classifiers predictions and output an accurate noisy label for each of the unlabelled data points.
2.	The second step aims to estimate the noise rate of the generated noisy labels by checking how
often the noisy labels generated in step 1 would agree with a particular classifier. This second
order statistic suffices to return us the error rates.
3.	The third step of our approach looks at producing a robust boosting method that is trained over the
generated noisy data. Classically AdaBoost does not perform well under noisy data compounding
errors for each point and progressively creating a worse classifier. The third step introduces a noise
resistant version of AdaBoost which relies on the noise and error rate estimated in step 2. This
results in a final classifier which can be compared against different semi-supervised algorithms.
BSL builds upon mainly two lines of similar works on boosting without cleanly labelled data:
1.	Semi-supervised boosting (Fujino et al., 2005; Blum & Mitchell, 1998; Laine & Aila, 2016;
Grandvalet & Bengio, 2005) uses semi-supervised algorithms (e.g., clustering) to generate ar-
1
Under review as a conference paper at ICLR 2020
tificial or proxy labels and boost accordingly (which we compare with). Existing methods that
apply self-generated labels directly to boosting will fail as the noises in the labels will accumulate
while boosting, especially when the noise rates are high - this is what we observed in our experi-
ment results. Our idea is a couple of steps further: we introduce a bias correction procedure into
boosting, and explicitly estimate the noises in generated labels. We also introduced an inference
framework for generating these labels at first place.
2.	Noise resistant boosting (Bootkrajang & Kaban, 2013) addresses boosting algorithms which are
susceptible to noisy data and proposing variants which can perform under a certain noisy con-
ditions. A set of noisy labels, as well as the knowledge of the noises, are often assumed to be
known. We do not require neither - we will self-generate the labels for unlabelled instances and
learn their error rates.
Our contributions summarize as follows:
1.	We propose a novel self-labelling boosting algorithm (BSL) which is able to outperform present
state-of-the-art semi-supervised algorithms.
2.	As two key components of our self-labelling framework, we contribute i) a new formulation of a
noise resistant Adaboost algorithm which corrects the noises in the labels - this is important in a
boosting process, because otherwise the label noises will accurate while boosting; ii) a label error
estimation procedure without accessing the ground truth labels.
3.	We offer both theoretical guarantees, as well as experimental evidence to the performance of
our framework. We conducted an extensive set of experiments to verify the effectiveness of
BSL. In the different datasets we ran our algorithm on, our method consistently outperforms
many other algorithms by more than 20%. In the cancer dataset when 10% of the data was
labelled, the second best algorithm (Semiboost) under performed our algorithm by 66% (relative
performance). Theoretically we are able to show the convergence of our noise-resistant AdaBoost
subroutine under symmetric error rate assumption.
The rest of the paper organizes as follows. We survey the most relevant work in the rest of the
section. Preliminaries are introduced in Section 2. Section 3 presents a noise-resistant AdaBoost
algorithm. Our Boosting via Self Labelling framework is introduced in Section 4. Section 5 presents
our experiment results. Section 6 concludes our paper. All missing details can be found in Appendix.
1.1	Related Works
Our work has been inspired by three different lines of research:
Semi-Supervised Learning: Research in this avenue looked at creating accurate labels in the presence
of limited labelled data. Work started with some basic algorithms (Cortes & Vapnik, 1995; Fujino
et al., 2005; Demiriz et al., 1999; Blum & Mitchell, 1998), but escalated to complex systems (Lee;
Miyato et al., 2018; Laine & Aila, 2016; Grandvalet & Bengio, 2005). A good survey in this area
was done by Zhu (Zhu, 2005).
Crowdsourcing: Inference crowdsourcing methods have played a huge part in uncovering true labels
from multiple noisy labels. Work in the area has ranged from EM algorithms (Dawid & Skene, 1979;
Raykar et al., 2010; Smyth et al., 1995; Karger et al., 2011) to variational inference methods (Liu
et al., 2012; Karger et al., 2014; Whitehill et al., 2009; Welinder et al., 2010; Raykar &Yu, 2012).
(Chon et al., 2012) wrote a good survey for this topic. In an ensemble setting, crowdsourcing has
appeared in numerous works include, most famously, (Kim & Ghahramani, 2012). However, many
of these results do not consider the noise present in the final aggregated value, which can lead to
accumulated errors in a boosting setting.
Boosting: Starting with the work (Freund & Schapire, 1999), research in AdaBoost has expanded
to all areas of machine learning. Some of the recent work in the area has looked into improving
the performance and pitfalls the original algorithm faced (RatSch et al., 2001; HaStie et al., 2009;
Domingo et al., 2000; Schapire, 2013; Bootkrajang & Kaban, 2013).
Our work builds upon previous work done in these areas, using similar ideas to construct a unique
method. One notable piece of work is Semiboost (Zhu, 2005). This algorithm takes a similar ap-
proach, introducing a boosting framework that improves upon existing classifiers to provide a good
2
Under review as a conference paper at ICLR 2020
classifier in the semi-labelled setting. However, Semiboost uses an unsupervised learning approach
using a similarity matrix based on labelled points and unlabelled points. Semiboost requires a sim-
ilarity equation to run, and this can be hard to achieve in practice. Pseudo-labelling is one of many
neural network approaches to the limited labelled dataset problem. Pseudo-labelling assigns labels
to the unlabelled data and then trains a neural network on the combination of the clean and noisy
labels. However, Pseudo-labelling and algorithms like it, face many different requirements. Algo-
rithms in this class require a sufficient amount of supervised data to work optimally. They also need
the classes to be clustered within the data, and the labelled data to not adhere to the same distribution
as the unlabelled data (Oliver et al., 2018). Learning in noisy data has also been a research focus
that runs parallel to our work. In (Natarajan et al., 2013), Natarajan et al. propose a noisy learning
algorithm which performs significantly better than other noise resistant algorithms. However, the
method requires knowledge on the amount of noise inside of the dataset before training and in a lim-
ited label dataset, which is hard to get in practice. In contrast to these related works, the algorithm
we introduce in this paper does not need prior knowledge about the dataset for it to perform and the
number of labelled points does not adversely affect the accuracy of the classifier generated.
2	Preliminary and Problem Formulation
Assume that D is the underlying true distribution generating n iid examples (xi, yi)in=1, where
each example consists of a feature vector xi = [xi,1, xi,2, . . . , xi,d] ∈ X ⊆ Rd, and a label yi ∈
{-1, +1}(〜Y). We assume the true labels of T examples is available where τ < n such that
(xi,yi)T=ι, denoted as {(x1,y1), (x2,y2), ∙∙∙ , (x∣Nl∣,y∣NL∣)} ：= NL ; while the rest of n - T
samples (χi)n=τ is unlabelled, {χ1,χ2,…，χ∣Nu∣} := NU. Let NU and NL be the set of indices
that make up NU and NL respectively.
Assume that after an initial classifier f : X → {-1, +1} is trained on NL and applied on the
unlabelled dataset such that f (Xi) → y%, Xi ∈ NU. Then NUnoisy denotes the noisy data set pro-
duced (χi,yi)i=U|. Let Nnoisy be the combination of NL and NUnoisy such that for i = 1 to
| Nnoisy |: {(Xi ,yi), if (Xi, yi ) ∈ NUnoisy ; (xi, yi), if (xi, yi ) ∈ NL}. Let Nnoisy be the set of
all indices within Nnoisy . Assume that Nnoisy follows as class-conditional random noise model
such that: ∀n = 1, 2, ..., ∣Nnoisy |: P(yi = -1|yi = +1, Xn) = P+, P(Ri = +1∣yi = -1, Xn)=
ρ- and ρ+ + ρ- < 1. Our goal is to learn a classifier f : X → {-1, +1} trained on Nnoisy that
minimizes the risk of f w.r.t to the 0-1 loss function RD(f)=旧(方^#^〜D [l(f (Xi) = yi)].
2.1	Our problem: Boosting via Self Labelling
We introduce the setting for AdaBoost (Freund & Schapire, 1999). The key idea is, at step t,
•	Maintain a weight Di(t) for each data instance (Xi, yi) ∈ NL.
•	Train a weak learner ft according to the weighted data distribution.
•	The final hypothesis F is a linear combination of each ft trained at every step t.
Let Nmiss be the set of all (Xi, yi) ∈ NL, such that ft(Xi) 6= yi. The goal is to increase the weight
of mis-classified points Di(t), (x^ yi) ∈ Nmiss to encourage classifier ft+1 (∙) to focus on correctly
classifying Nmiss :
n z. l n	Dt(i) ∙exp(-αt ∙ ft(Xi) ∙yi)
Dt(i + 1) =-------------z-------------,
where Zt = £髭仍 Dt(i) ∙ exp^^—(tt ∙ ft (Xi) ∙ yi) is a normalization factor. AdaBoost creates a final
hypothesis in the additive form: F (Xi) = PtT=1 αtft(Xi), where Xi is a test sample.
Our goal, and a short coming with AdaBoost, is classifying a dataset where some of the yi in
(Xi , yi ) ∈ Nnoisy are noisy. Because AdaBoost uses a exponential loss function it is inherently
susceptible to noisy labels. We propose a new loss function that removes bias:
D 小—Dt(i) ∙ exp(-αt ∙ O(Xi,y) ∀i L N
Dt+1(i) =	Tj	, ∀i ∈ Nnoisy
Zt
Our goal is to define a function '(∙) that can help us evaluate an unlabelled instance. This function
'(∙) will allow us to adjust to noisy labels within Nnoisy. Our algorithm runs in two main stages. It
3
Under review as a conference paper at ICLR 2020
first applies noisy labels for unlabelled instances in our dataset, and then creates a final hypothesis
F using a noise-resistant variant of AdaBoost where We define '(∙).
3	Noise-Resistant AdaBoost
We first extend a learning with noisy data approach to the boosting setting, following the work in
(Natarajan et al., 2013). Suppose the examples have the following homogeneous error rates:
ρ+ := Pxi(yi = -1|yi = +1),ρ- := Pxi(yi = +1|yi = -I)
Suppose we know these error rates in this section. Boosting over the above noisy examples will lead
to a biased training process when the label noises are sufficiently large. Our approach is a straight-
forward adaptation from a noise correction mechanism adopted in supervised learning (Natarajan
et al., 2013): defining surrogate loss function ` on noisy labels (for an arbitrary loss function `)
'(f (xi), yi = +1) := (I-P-)'("；)'+1)-p+'(f (XM-I),	(1)
1 - ρ+ - ρ-
'(f(xi), yi = -1) := (I - P+)'(f (Xi), -1) - ρ-`f (Xi), +1).	(2)
1 - ρ+ - ρ-
A nice property of above estimator is its unbiasedness (Natarajan et al., 2013):旧岁科3 ['(f (xi), yi)]=
'(f (χi),yi). We adapt this idea to AdaBoost. Replace '(∙) with the following loss measure as
adopted in AdaBoost: '(f(xi), yi) = ft(xi) ∙ yi. Define ω+ := 二--：；,ω-:= 二--；；and
2+ := Px|y=+Ift(X)= y), e- := PxIy=-Ift(X) = y),
and ^t := max{^+, ^-} and the following αts
+ _ 1 1 1 -得 -_ 1 1 1 - 3
αT =ln , αi- = ln
t 2ω+	^t , t 2ω-	^
Then update Di (t) as follows:
Di(t + 1)
Di(t ∙ exp(-αsign(W)"f (Xi), y，))
Z
(3)
(4)
(5)
where Zt is again the normalization factor. The reason that we need to define two learning rate
αts is because the losses are weighted differently for the noisy labels. and ω := max{ω+, ω-},
Yt := 1 - ^t, and e(δ, n, ρ+,ρ-) := ω ∙ Jnl； 2 . Since we have defined two αts on the training
data based on the noisy labels, we define αt := α++α- , and let the final output classifier be F(X) =
sign(EtT Tft(X) > 0). We prove the following performance guarantee when ρ+ = ρ-:
Theorem 1 With probability at least 1 - δ,
X	I(F(Xi) = yi) ≤ exp (-2 X Yt ) + e(δ,N,P+,P-).
i∈Nnoisy	t=1
Though our above results are proved under the symmetric error setting, we experimentally verified
the performance of our error-resistant boosting procedure.
4 A Self Labelling framework for boosting
We now introduce our Self Labelling framework for boosting. The framework can be broken into
two major components: the noisy label generation and the noisy Adaboost algorithm. Section 3
already described the formation and proof of the noisy Adaboost algorithm. In the next two sections
go into detail over i) the inference method to generate noisy label and ii) the procedure to estimating
the noise levels within the generated labels as inputs into the Adaboost algorithm. More specifi-
cally, Section 4.1 delves deeper into detailing the processes of noisy label generation particularly
addressesing matrix L and the aggregation inside of L that forms the noisy dataset. While Section
4.2 explores how the noise levels within the dataset (ρ-,ρ+) is calculated and used as inputs to the
noisy Adaboost algorithm.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Boosting via Self Labelling
1:	Input: Labelled Data NL, Umabelled Data NU
2:	Output: Final Hypothesis F (x)
3:	For i = 1,…，M :
•	Train fi on (xi , yi ) ∈ NL
•	Get hypothesis ht = fi(xi) → {-1, +1}|NU|, ∀xi ∈ NU
•	Add ht to matrix L
4:	RUn inference method to generate noisy labels for NU: I(L) → {-1, +1}|NU|. Denote the
noisy dataset as NUnoisy. Let Nnoisy= NL ∪ NUnoisy.
5:	Estimate (ρ+, ρ-) Using Eqn. (6, 7).
6:	Initialize: Di(1) = 1/N for i ∈ Nnoisy
7:	for t = 1,…，T:
•	Train weak classifier gt on data distribUtion D(t)
•	Get weak hypothesis ht ：=gt(x,i), (χi,yi) ∈ Nnoisy → {-1, +1}
•	Get weighted error + , - Using Eqn.(3)
•	Calculate ω+, ω- and α+, α- (Eqn.(4)), using estimated p+,ρ-.
•	Compute at := α++αt .
•	For i ∈ NU : Update weight Di(t + 1) according to Eqn.(5). Else for i ∈ NL, update
weight Di(t + 1) according to Eqn.(5) with ρ+ = ρ- = 0.
8:	Final Hypothesis:
F (X) = sign(EL Ttft(x) > °)
4.1 Self Labelling
We generate noisy labels for the unlabelled dataset via a crowdsourcing perspective based on the
works of (Dawid & Skene, 1979; Liu et al., 2012). On line 3 in Algorithm 1, M classifiers
{f1(∙),f2(∙),….fM (∙)} are trained on labelled data NL = {χι ,X2,..., x∣Nl∣}, with the goal to
classify all NU data points, {x1, x2, . . . , x|NU | } as closely as possible to their true unknown binary
labels {yι, y2,..., y∣Nu ∣} ∈ {-1, +1}. Each data point Xi ∈ NU will receive a noisy label yij
which denotes classifier f (∙) ∈ M prediction on x&.
Let each row in matrix L ∈ {-1, +1}* lNUi×m, in the final step on line 3, represent {yi};∀i ∈ NU
for each f (∙) St Lij = f (Xi) = yi,j. Assuming f (∙) = fk(∙)Rj,k ∈ M St j = k each f (∙)
will learn a different distribution. This will allow for a variation that gives a better prediction on
the underlying structure of NU allowing for some f (∙) to be closer to capturing the true underlying
distribution of NU compared to others. Assigning weight qj to each f (∙) according to its perceived
accuracy in respect to other classifiers allows for a more accurate aggregation of all f (∙) responses.
More formally, in step 4 in Algorithm 1, for each of the M classifiers {fι(∙), f2(∙),..., fM(∙)},
I (variational inference method) assigns an optimal weight {q1, q2, . . . , qM} such that taking the
aggregation of each classifiers prediction for point Xi ∈ NU against its respective weight qj will
result in a value yi that will be as close as possible to the unknown true label yi. The set of Xi and
aggregated y ∀i ∈Nu will form NUnoisy.
Conceptually, I uses qj to classify f (∙) as an expert if qj > 0.5, a spammer if qj ≈ 0.5 or an
adversary if qj- < 0.5. If fj-(∙) is designated a spammer then fj-(∙) is randomly guessing and does not
provide any useful prediction. If fj-(∙) is denoted as an adversary, fj (∙) is believed to be “purposely”
picking the incorrect label and iji,j = fj(xi). As the number of labelled data gets smaller, the
number of spammers and adversaries found in M classifiers increases.
Assuming conditional independence among classifiers I can predict the optimal weight as following:
IfP(fj(X), fk(X)|y) = P(fj (X)|y)P(fk (X)|y) where j, k ∈ |M|S.t.j 6= k (treating each classifier
as an independent labeler), we can apply inference approaches to aggregate and infer the true label:
q = arg max log P(q∣L, θ) = log ^X P(q,y∣L, θ)
y
5
Under review as a conference paper at ICLR 2020
Expectation maximization can be used to solve for this maximum a posteriori estimator q by treat-
ing the true labels yi as the hidden variable. Assuming a Beta(σ, β) distribution, an EM can be
formulated as follows:
E Step: μi(yi) W(I- qj尸ij	M Step: qj=HL+二-1
where δi,j = E[Li,j = y/ and yi is estimated by yi = argmaxyi μi(yi) given μ% is the estimated
likelihood of yi. Nj stands for all labels observed by classifier j: Nj = ({fj (χi)}, ∀i ∈ 1 ∙∙∙∣ NU |).
Finally, step 5 of Algorithm 1 shows the noisy dataset (xi, yi), ∀i ∈ Nnoisy where yi is the noisy
label provided by the inference algorithm if Xi ∈ NU, else yi = yi if (xi, yi) ∈ NL. Nnoisy then
feeds into the Noisy Adaboost Algorithm introduced in Section 3.
4.2 Error estimation
Let A(.) represent the noise resistant AdaBoost algorithm we introduced in section 3. LetI represent
the inference method we will use to aggregate matrix L. Inference method I outputs a noisy labelled
data set: NUnoisy = {χi,yi}, Nxi ∈ NU. In order to run A We need to accurately estimate the error
rates ρ-,ρ+: P- = (yi = +1∣yi = -1), ρ+ = (yi = -1|yi = +1) within NUnoisy. Assuming
homogeneous error rates, we can derive an accurate estimation for the error rates of NUnoisy from
the confusion matrices of the the classifiers fj ∈ M.
Denote the false positive and false negative of f (∙) by ρ+,f and ρ-,f respectively. The inference
algorithm I will output ρ-,fj and ρ+,fj for each fi ∈ M by comparing {xi, fj (xi)} against the
aggregated label from the inference algorithm. P(yi = -1) and P(yi = +1) are the marginal
distribution of positive and negative labels within the dataset. If P(yi = -1) and P(yi = +1) are
known we show the below equation uniquely identify the error rates given any fj ∈ M:
Lemma 1 ρ+, ρ- can be determined by the following set of equations:
=	(P-,fjP(yi = 1) - P(yi,fj = yi = 1)
ρ+	P(yi =+1)(I-P+,%) - P(yi = -I)(P-,fl
= -P(yi = I)(I - p+,Q + P(yi,fj = yi = 1)
P-	—P 3 = +1)(I-P+,%)+ P(yi = T)(P-,%)
(6)
(7)
In practice we can balance the dataset to make P(yi = -1) = P(yi = +1) = 0.5. The probability
terms P(yi = 1) and P(yi,f = Ui = 1)n can be estimated through the data. The estimated
parameters are plugged into Eqn.(6,7) to approximate P+,P-. Note this estimation is done without
using ground truth labels.
5	Experimental results
The focus of BSL is to provide a framework which can generate an accurate classifier given very
little labelled data. In this section, we conduct extensive experiments to verify the effectiveness of
BSL and compare with benchmark algorithms.
5.1	Datasets
8 UCI datasets were used to evaluate Boosting via Self-Labelling (BSL). Since Boosting via Self-
Labelling works on binary classification problems, we chose datasets which only contained two
class labels or turned linear regression datasets into binary labels. The first column of Table 1 has
the name of the dataset used and the percentage of data labelled. The Cancer dataset had 567 samples
and 30 features. The Diabetes dataset had 768 samples and 8 features. The Thyroid dataset had 215
samples and 5 features. The Heart dataset had 303 samples and 13 features. The German dataset has
1000 samples and 20 features. The Image dataset has 2310 samples and 19 features. The Housing
dataset has 506 samples and 13 features. And the Sonar dataset has 208 samples and 60 features.
6
Under review as a conference paper at ICLR 2020
5.2	Experimental Setup
The goal of our experiments was to show the performance improvement we achieved by using the
BSL compared to other semi-supervised algorithms. We use classification error rate (measuring the
fraction of mis-classified sample points) each model faced as the evaluation measure. Table 1 reports
the mean of 20 different runs of the experiment. To measure the performance of each trial, we split
the data into 40% test and 60% train. We then broke up the training into increasing percentages of
unlabelled/labelled data points. Table with all results can be found in Appendix.
The pre-processing step started by taking out any points that had missing features or missing labels
in the entire dataset. As a result, the dataset was completely labelled and each sample had all of its
features. Before creating the unlabelled dataset, we split the data into testing and training set. The
unlabelled data was created by removing the labels from a designated percentage of the training set.
Each of the features was normalized between 0 and 1 to allow for a better approximation of the data.
At the end of the pre-processing step, three separate lists were outputted: the testing set, the set that
contained the unlabelled data points and the set that contained the labelled data points.
During the first step of our framework, the labelled data was passed to a set of supervised machine
learning algorithms. Although the number of classifiers could have been theoretically infinite, we
chose to limit the number to 10. As a result, the experiment was able to be conducted in a reasonable
time. The supervised algorithms were all implemented using the Scikit-learn (Pedregosa et al., 2011)
and consisted of KNN, Decision Tree, Gaussian Mixture Model, Naive Bayes, SVM and Logistic
Regression. Some of the models were repeated in their use but took on different initialization values
to create different classifiers.
The second step of the framework called a basic inference algorithm abbreviated (D&S) (Dawid &
Skene, 1979). The crowdsourcing algorithm outputted the noisy labels for each of the unlabelled
data points.
The final part of the framework, our noise-resistant variant of the AdaBoost algorithm is used. The
alpha value for each classifier was optimised to compensate for the increase in values from the loss
functions. We limited the algorithm to running only 20 decision stumps as the base classifier.
Table 1 shows the performance of BSL compared to other state of the art algorithms which try to
create optimised classifiers within the label limited dataset. Table 1 only reports the classification
errors for 10% of the training data labelled. Each algorithm bench marked a different part of the
framework. C-SVM(Liu et al.) showed the improvement noise resistant version of AdaBoost gave
over other noise-resistant algorithms. Semiboost, S3VM, Label Propagation, NN, and Logitboost
tested against the final classifier produced by the framework. CSVM, Semiboost, S3VM, and Log-
itboost were all implemented using different external libraries. Label Propagation was implemented
using Scikit-Learn (Pedregosa et al., 2011) And the Supervised Neural Network (NN) was was im-
plemented with Keras (Chollet et al., 2015).
5.3	Results
Performance Comparison of BSL with Benchmark Algorithms We compared BSL’s perfor-
mance to seven benchmark algorithms, specifically: DS+AdaBoost, Semiboost, S3VM, CSVM,
Label Propagation, NN, Logitboost. DS + AdaBoost applied inference method to assign noisy la-
bels to the unlabelled data and used a standard AdaBoost algorithm (Pedregosa et al., 2011) to create
a final classifier. The Supervised Neural Network (NN) is a sequential model with two hidden layers
each with 100 nodes. If DS was not specified then the model performed without any noise labelled
data outputted by the crowdsourcing algorithm. BSL’s improvement in performance compared to the
other algorithms is significant. BSL consistently had a 20% - 30% increase in performance com-
pared to most of the competing algorithms. While BSL is able to outperform most of the algorithms
on each of the 8 datasets, it was outperformed by the DS + AdaBoost for trials in the thyroid and
sonar dataset. Semiboost was also able to produce better results against BSL in the image dataset. It
is important to note that although DS+AdaBoost and Semiboost outperform BSL, the difference in
improvement was not significant. This shows that the loss function that we introduced to the noise
resistant algorithm might overfit on the data by taking out more noise than was necessary. The loss
of performance could also indicate that the error rates passed into the algorithm were not always
close to the actual noise in the dataset or were too high to be effective. Table 2 (located in the
7
Under review as a conference paper at ICLR 2020
Appendix) shows the estimated error rate the inference crowdsourcing method outputted for each
experiment. It is important to notice the noise that exists within the dataset after the crowdsourcing
step. Our handling of this noise allows our algorithm to perform on average better than the other
benchmark algorithms. When noise was close to 50%, the noise resistant Adaboost loss function
becomes unbounded and this creates unstable classifications. Similarly, the low error rates present
within the thyroid dataset allowed for a non-noise resistant version of Adaboost to outperform BSL.
Performance with increment in the percentage of unlabelled data With Figure 1 (located in the
Appendix), we also show the performance of BSL on 4 UCI datasets we used to compare framework
against the baseline algorithms with increasing amount of unlabelled data instances. In the experi-
ment, we increase the percentage of unlabelled data one at each step, starting from 50% going up to
99% (or as far as possible). Under each step of the experiment, we ran BSL 20 times randomising
at every turn which points were used in training, testing, being labelled or unlabelled. The solid line
shows the mean of running the algorithm 20 times at each step. The graph shows that despite the
decrease of labelled data available to train on, the framework can maintain relatively similar classifi-
cation error rate. This feature is significant because it shows that having a lot of unlabelled data does
not restrict the performance of the framework and therefore is not a prerequisite for it to perform
well. One can note that all the graphs don’t go to 0.99% unlabelled data. Since the UCI datasets
were not significantly big, the closer we are to 0.99% our training data became single-classed la-
belled. As a result, our framework could not create a final classifier as the ensemble we used was
not able to train on the single-classed dataset. As a result, we stopped the experiment at the points
where we started getting a lot of single-class labelled data warnings.
% Labelled	D&S+AdaB	BSL	D&S+CSVM	Semiboost	S3VM	LP	NN	Logitboost
CanCer-10%~	39.1	10.22	3816	29.65	37.63	32.68	35.94	34.65
Diabetes - 10%	30:19	27.79	3498	-32.97-	33.9	35.24	35.63	-31.90-
Thyroid -10%	14:51	23.94	31.32	-30.83-	27.21	27.91	35.18	-2425-
Heart-10%-	32:82	23.21	45.05	-35.25-	47.05	29.47	35.38	-29.81-
German -10%	27.5	26.75	3225	-3225-	32.25	32.49	35.82	-30.28-
Image -10%	27.99	27.06	26:17	-25.31-	29.74	42.98	41.71	-37.28-
Housing - 10%	2900	24.35	27.31	-26.08-	26.50	34.74	42.14	-27.13-
Sonar-10%-	25.60	-	26.24	40.42 —	40.53	38.19	44.68	48.81	45.12
Table 1: Classification Error Rate of BSL and the seven benchmark algorithms. The first column
shows the dataset and percent of data labelled. The columns D&S+CSVM, Semiboost, S3VM and
LP (label propagation), Logitboost, NN show the performance of the six benchmark algorithms.
Each entry shows the mean classification error rate over 20 trials. Full table available in the Ap-
pendix.
6	Discussion and concluding remarks
Our goal in this paper is to present a novel and efficient boosting algorithm for semi-labelled datasets
and show its effectiveness in providing an accurate classifier. The usefulness of BSL stems in its
ability to produce high-quality noisy labels to unlabelled instances and its ability to handle the noises
in labels, despite having severely limited amount of labelled data.
Our results over the 8 UCI datasets reveal the performance improvement BSL brings compared to
other algorithms currently in use. Our experiments also show how impervious BSL can be to noise
by showing constant performance despite increases in unlablled data. A natural direction would be
considering datasets that are fully unlabelled. Instead of having an ensemble of supervised classifiers
BSL could consider using a consensus clustering approach and view each clustering algorithm as a
potential agent within the crowdsource setting. Another important extension of this project would be
using a more proficient inference method to extract the labels less nosier than those produced during
our experiments. It is also a very interesting question to further study the theoretical guarantees of
BSL in more sophisticated settings. Finally, another aspect of the paper we wish to further pursue is
looking at non-homogeneous error rates.
8
Under review as a conference paper at ICLR 2020
References
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Pro-
Ceedings ofthe eleventh annual conference on Computational learning theory, pp. 92-100. ACM,
1998.
Jakramate Bootkrajang and Ata Kaban. Boosting in the presence of label noise. arXiv preprint
arXiv:1309.6818, 2013.
Francois CholletetaL Keras. https://keras.io, 2015.
Yohan Chon, Nicholas D Lane, Fan Li, Hojung Cha, and Feng Zhao. Automatically characterizing
places with opportunistic crowdsensing using smartphones. In Proceedings of the 2012 ACM
Conference on Ubiquitous Computing, pp. 481-490. ACM, 2012.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273-297,
1995.
Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates
using the em algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics), 28
(1):20-28, 1979.
Ayhan Demiriz, Kristin P Bennett, and Mark J Embrechts. Semi-supervised clustering using genetic
algorithms. 1999.
Carlos Domingo, Osamu Watanabe, et al. Madaboost: A modification of adaboost. In COLT, pp.
180-189, 2000.
Yoav Freund and Robert Schapire. A short introduction to boosting. Journal-Japanese Society For
Artificial Intelligence, 14(771-780):1612, 1999.
Akinori Fujino, Naonori Ueda, and Kazumi Saito. A hybrid generative/discriminative approach
to semi-supervised classifier design. In Proceedings of the National Conference on Artificial
Intelligence, volume 20, pp. 764. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT
Press; 1999, 2005.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad-
vances in neural information processing systems, pp. 529-536, 2005.
Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. Multi-class adaboost. Statistics and its Inter-
face, 2(3):349-360, 2009.
David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing
systems. In Advances in neural information processing systems, pp. 1953-1961, 2011.
David R Karger, Sewoong Oh, and Devavrat Shah. Budget-optimal task allocation for reliable
crowdsourcing systems. Operations Research, 62(1):1-24, 2014.
Hyun-Chul Kim and Zoubin Ghahramani. Bayesian classifier combination. In Artificial Intelligence
and Statistics, pp. 619-627, 2012.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep
neural networks.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and S Yu Philip. Building text classifiers using positive
and unlabeled examples. Citeseer.
Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In F. Pereira,
C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Process-
ing Systems 25, pp. 692-700. Curran Associates, Inc., 2012. URL http://papers.nips.
cc/paper/4627-variational-inference-for-crowdsourcing.pdf.
9
Under review as a conference paper at ICLR 2020
Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 2018.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Avital Oliver, Augustus Odena, Colin Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic
evaluation of semi-supervised learning algorithms. 2018. URL https://arxiv.org/pdf/
1804.09170.pdf.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Gunnar Ratsch, Takashi Onoda, and K-R Muller. Soft margins for adaboost. Machine learning, 42
(3):287-320, 2001.
Vikas C Raykar and Shipeng Yu. Eliminating spammers and ranking annotators for crowdsourced
labeling tasks. Journal of Machine Learning Research, 13(Feb):491-518, 2012.
Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca
Bogoni, and Linda Moy. Learning from crowds. Journal of Machine Learning Research, 11
(Apr):1297-1322, 2010.
Robert E Schapire. Explaining adaboost. In Empirical inference, pp. 37-52. Springer, 2013.
Padhraic Smyth, Usama M Fayyad, Michael C Burl, Pietro Perona, and Pierre Baldi. Inferring
ground truth from subjective labelling of venus images. In Advances in neural information pro-
cessing systems, pp. 1085-1092, 1995.
Peter Welinder, Steve Branson, Pietro Perona, and Serge J Belongie. The multidimensional wisdom
of crowds. In Advances in neural information processing systems, pp. 2424-2432, 2010.
Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R Movellan, and Paul L Ruvolo. Whose
vote should count more: Optimal integration of labels from labelers of unknown expertise. In
Advances in neural information processing systems, pp. 2035-2043, 2009.
Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical report, University of
Wisconsin-Madison Department of Computer Sciences, 2005.
10
Under review as a conference paper at ICLR 2020
A Proof for Theorem 1
For our error-corrected AdaBoost we first prove
T
Z ：= ∏ Zt= X exp(-aF(xi),yi))
t=1	记 NnOiSy
(8)
Following standard argument of AdaBoost:
Di(t + 1)
Di(t)exp(-αt"∕t(xi),y))
Zt
Di(t- 1)exp(-αt%(∕t(xi),y) - αt-il(ft-i(xi),Vi))
ZtZt-I
,Di(1)exp(- PT ατ'(f式xi),6))
=	Z
Since '(ft(xi), y) is linear in ft we know
tt
X -αt0(fτ(xi),yi) = -"X ατfτ(xi),y) = -0(F(xi),y)
τ =1	τ=1
Therefore
1= E	Di(t + 1)
i∈Nnoisy
^^i∈NnOiSyDi(1) ∙ exp(-PT=1。(F(xi),y))
Z
Multiple Z on both sides, we have proved Eqn. (8).
Define
(1-ρ-)l(f(xi) = +1)-ρ+ l(f(xi) = -1)
1 - ρ+ - P-
(1-ρ+)l(f(xi) = -1)-P-l(f(xi) = +1)
1 - P+ - P-
(9)
(10)
Next we show that
E	exp(-。(F(Xi),y)) ≥ E G(F(Xi),y)
i∈Nnoisy	i∈Nnoisy
When F(Xi) = y = +1, '(F(Xi),y) = 1 - ωι < 0, but exp(-。(F(Xi),y)) > 0. When
F(Xi) = -1,y = +1, M(F(Xi),y) = ωι, butexp(-。(F(Xi),电)> exp(ωι) > ω> The case for
y = 1 is symmetric.
Via Hoeffding inequality we know that with high probability at least 1 - δ
X 2(F(XiMyi) ≥ X l(f(Xi)= yi) - e(δ,N,P-,P+)
i∈Nnoisy	i∈Nnoisy
where e(δ, N, ρ+, ρ-) := max(ω+, ω-) ∙ JN2 δ .
Therefore
f l(f(g) = yi)	(II)
i∈Nnoisy
≤ X M(F (Xi),yi) + e(δ,N,ρ-,ρ+)	(12)
i∈Nnoisy
≤ X exp(-。(F (Xi),yi)) + e(δ,N,ρ-,ρ+)	(13)
i∈Nnoisy
=Z + e(δ, N,ρ-,ρ+)	(14)
11
Under review as a conference paper at ICLR 2020
Now we prove that
T
Z ≤ exp(-2 X γ2)
t=1
First we notice the following: at time t
〜
'(f(Xi)=	二 十1, yi 二	+1)	二 exp(-ω+αt)	(15)
~, '(f(Xi)=	二一1,yi 二	二 +1) = exp(ω+αt)		(16)
~, '(f(Xi)=	二一1,yi 二	-1)	二 exp(-ω-αt)	(17)
~, '(f(Xi)二	二 十1, yi 二	-1)	二 exp(ω-at)	(18)
Then when y = +1, taking derivatives of Zt w.r.t. at
∂Zt
∂αt
E -ω+exp(-ω+αt) + E ω+ exp(ω+αt)
χ∈A+	χ∈A+
(19)
Here we have defined four sets:
A+ := the set of correctly classified data when yi = +1,
A- := the set of correctly classified data when yi = -1,
(20)
(21)
and A+, A- are their complement sets. Set derivative in Eqn. (19) to 0 we have
at =	ln
1 - ^t
Gt
where Wt is defined as follows:
ε+ := Py=+ι(ft(x) = y)
Similarly when jji = -1
转:=Py=-I(ft(x) = y)
Define Wt = max{^:, ^-} and when ρ+ = ρ-, we have ω+ = ω-. And consequently, α = α+
α- Next follows standard argument in boosting we are ready to prove
Without loss of generality, consider the negative label case yi = -1. Then
Zt = d⅛t+(1 - ιg-)dr⅛
Mtnɪ+(1 - 2rs
—
The inequality is due to the fact Jɪ-^t > Jɪ-t^. Therefore
Zt ≤ √^t(1 - ^t) =，1 - 4γt, Yt := 1 - ^t
Further
J1 - 4γ ≤ exp(-2γ2)
This completes the proof.
12
Under review as a conference paper at ICLR 2020
Proof for Lemma 1
Given yi,fj = fj(Xi) ∈ {±1}, Xi ∈ NU, y is true labels of Xi ∈ NU, and y ∈ (xi, yi) ∈ NUnoisy.
P(yi,fj = yi = I)
=P(yi,fj = yi = Lyi = O) + P(yi,fj = y = 1,yi = I)
=P(yi,fj= yi = 1|Y = 0) ∙ P(yi = 0)
+ P(yi,fj = yi = 1|yi = 1) ∙ P(yi = 1)
=P(yi,fj = 1|yi = O)P Yyi = 1尿=0) ∙ P(yi = 0)
+ P(yi,fj = 1|yi = 1) ∙ P(Ri = 1|yi = 1) ∙ P(yi = 1)
=P(yi = 0) ∙ ρ- ∙ ρ-,fj + P(yi = 1) ∙ (1 - ρ+) ∙ (1 - ρ+,fj)
Further we have
P(yi = 1) = P(yi = 0) ∙ P- + P(yi = 0)(1 - p+)	(22)
Solving above linear equations completes the proof.
Additional experimental results
Percent Unlabelled
Percent Unlabdled
Figure 1: Top left: Cancer Dataset, Top Right: Diabetes Dataset, Bottom Left: Heart Dataset,
Bottom Right: Thryoid Dataset. All these graphs represent an increasing % of unlabelled data
within the dataset.
13
Under review as a conference paper at ICLR 2020
% Unlabelled	Estimated Noise Rate	Actual Noise Rate
Cancer -10%	5.57	9.68
CanCer - 20%	3.51	5.79
CanCer - 30%	322	4.17
CanCer - 40%~	293	3.64
Diabetes -10%	27.39	30.38
Diabetes - 20%	23.47	25:88
Diabetes - 30%	20.00	19.03
Diabetes - 40%	22:39	27:46
Thyroid -10%	8.52	6.15
Thyroid - 20%	775	9.55
Thyroid - 30%	4.65	6.75
Thyroid - 40%	4.51	8.63
Heart - 10%	17:67	16:03
Heart - 20%	14:91	13:86
Heart- 30%	15:81	17:20
Heart- 40%-	14:91	18.52
German - 10%	24:83	25:81
German - 20%	2436	2T.56
German - 30%	T8T6	20:84
German - 40%	15:83	16.09
Image - 10%	66.01	57.57
Image - 20%	57.50	65:23
Image - 30%	50.43	53.72
Image - 40%	4271	41:04
Housing - 10%	2574	26.92
Housing - 20%	14:15	9.57
Housing - 30%	56	8.81
Housing - 40%	4.62	8.82
Sonar-10%	23:78	28:26
Sonar- 20%	26:61	29:69
Sonar- 30%	20.53	19.35
Sonar- 40%-	15.32	10.62
Table 2: Estimated noise within the dataset and the actual noise found within the dataset
14
Under review as a conference paper at ICLR 2020
% Labelled	D&S+AdaB	BSL	D&S+CSVM	Semiboost	S3VM	LP	NN	Logitboost
CanCer-10%	39.1	10.22	38T6	29∙65	37∙63	32∙68	3594	34∙65
CanCer- 20%	33:42	8.9	3781	-1965-	37∙81	29∙08	31∙62	-3182-
CanCer- 30%	3673	9.34	37:02	-1092-	38∙03	21∙56	3097	-2653-
CanCer- 40%~	35:42	9.17	3925	-1075-	36∙84	18∙68	28∙03	-1781-
Diabetes - 10%	30:19	27.79	3498	-3297-	339	35∙24	35∙63	-3190-
Diabetes - 20%	28:18	26.69	3563	-33?18-	32∙92	32∙92	3191	-28:29-
Diabetes - 30%	27.27	25.32	3409	-33.86-	35∙32	30∙71	3138	-2739-
Diabetes - 40%	2796	26.61	34:33	-3194-	34∙68	30∙31	3043	-28:81-
Thyroid -10%	14:51	2394	3132	-3083-	27∙21	27∙91	35∙18	-2425-
Thyroid - 20%	10:51	2L09	3074	-2984-	32∙09	26∙74	3431	-2534-
Thyroid - 30%	9.49	2058	3105	-30:08-	29∙53	25∙58	26∙82	-2746-
Thyroid - 40%	891	2L12	3163	30	30,7	22∙09	2341	-2978-
Heart - 10%	3282	23.21	4505	-3525-	47∙05	29∙47	3538	-2981-
Heart - 20%	26.23	20.49	4426	-2951-	459	29∙26	3440	-3056-
Heart- 30%	25:41	20.49	4508	-2705-	45∙74	27∙88	23∙87	-3667-
Heart- 40%-	20:49	15.57	4386	-2295-	459	26∙31	23∙82	-4132-
German -10%	27.5	26.75	3225	-3225-	32∙25	32∙49	35∙82	-30:28-
German - 20%	26.5	25.8	2700	-2700-	27∙00	26∙24	3133	-29:12-
German - 30%	24:25	22.47	2654	-2542-	27∙18	26∙34	29∙17	-28:55-
German - 40%	2475	20.59	2584	-25Γ1-	25∙39	26∙81	28∙11	-2731-
Image - 10%	27:99	27.06	26:17	-25.31-	29∙74	42∙98	41∙71	-3728-
Image - 20%	2775	2648	24:17	-24:67-	28∙44	37∙38	3822	-3619-
Image - 30%	25Γ0	25∙83	2385	-22.83-	25∙15	35∙06	31∙06	-3345-
Image - 40%	2353	24.64	2480	-2194-	22∙87	32∙51	2757	-29:44-
Housing - 10%	2900	24.35	2731	-2608-	26∙50	34∙74	42∙14	-2713-
Housing - 20%	2164	20.42	2331	-2596-	24∙50	35∙35	39∙45	-2562-
Housing - 30%	2164	19.64	234	-2553-	23∙41	36∙79	31∙87	-2529-
Housing - 40%	2016	19.16	2209	-24:34-	21∙04	33∙89	26∙71	-2348-
Sonar-10%	2560	26.24	4042	-4053-	38∙19	44∙68	48∙81	-45?12-
Sonar - 20%	1660	1839	3687	-3394-	37∙73	38∙07	31∙72	-3169-
Sonar - 30%	13.34	14.1	2900	-2787-	30∙32	33∙33	25∙83	-2588-
Sonar - 40%-	10.63	-	1343	26∙17 —	25:78	26∙17	26∙24	21∙64	24:68
Table 3: Classification Error Rate of BSL and the seven benchmark algorithms. The first column
shows the dataset and percent of data labelled. The columns D&S+CSVM, Semiboost, S3VM and
LP (label propagation), Logitboost, NN show the performance of the six benchmark algorithms.
Each entry shows the mean Classifcation error rate over 20 trials.
15