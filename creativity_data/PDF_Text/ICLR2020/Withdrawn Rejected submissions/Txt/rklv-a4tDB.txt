Under review as a conference paper at ICLR 2020
Mesh-Free Unsupervised Learning-Based PDE
Solver of Forward and Inverse problems
Anonymous authors
Paper under double-blind review
Ab stract
We introduce a novel neural network-based partial differential equations solver
for forward and inverse problems. The solver is grid free, mesh free and shape
free, and the solution is approximated by a neural network. We employ an unsu-
pervised approach such that the input to the network is a points set in an arbitrary
domain, and the output is the set of the corresponding function values. The net-
work is trained to minimize deviations of the learned function from the strong
PDE solution and satisfy the boundary conditions. The resulting solution in turn
is an explicit smooth differentiable function with a known analytical form.
Unlike other numerical methods such as finite differences and finite elements, the
derivatives of the desired function can be analytically calculated to any order. This
framework therefore, enables the solution of high order non-linear PDEs. The
proposed algorithm is a unified formulation of both forward and inverse problems
where the optimized loss function consists of few elements: fidelity terms of L2
and L∞ norms that unlike previous methods promote a strong solution. Robust
boundary conditions constraints and additional regularizers are included as well.
This setting is flexible in the sense that regularizers can be tailored to specific
problems. We demonstrate our method on several free shape 2D second order
systems with application to Electrical Impedance Tomography (EIT), diffusion
and wave equations.
1	Introduction
Partial differential equations are fundamental in science and mathematics with wide applications in
medical imaging, signal processing, computer vision, remote sensing, electromagnetism, economics
and more. Classical methods such as finite differences, finite volume and finite elements are numer-
ical discretization-based methods where the domain is divided into a uniform grid or polygon mesh.
The differential equation is then reduced to a system of algebraic equations. These methods may
have some limitations: the solution is numeric and may suffer from high condition number, highly
dependent on the discretization and even the second derivative is sensitive to noise, see for example
Garcia (2017); LeVeque (2007); Thomas (1995); Reddy (2005) and references therein.
In the last few years, deep learning and neural network-based algorithms are extensively used in
pattern recognition, image processing, computer vision and more. Recently, the deep learning ap-
proach had been adopted to the field of PDEs as well by converting the problem into a machine
learning one. In Supervised learning, the network maps an input to an output based on example
input-output pairs. This strategy is used in inverse problems, where the input to the network is a
set of observations/measurements (e.g. x-ray tomography, ultrasound) and the output is the set of
parameters of interest (tissue density etc.) Feigin et al. (2018); Lucas et al. (2018); McCann et al.
(2017); Seo et al. (2019). Unsupervised learning on the other hand is a self-learning mechanism
where the natural structure present within a set of data points is inferred.
Algorithms for forward and inverse problems in partial differential equations via unsupervised learn-
ing were recently introduced. The indirect approach utilizes a neural network as a component in the
solution. Li et al. (2018) for example, proposed the NETT (Network Tikhonov) approach to inverse
problems. NETT considers regularized solutions having a small value of a regularizer defined by a
trained neural network. Khoo & Ying (2018) introduced a novel neural network architecture, Switch-
Net, for solving the wave equation based inverse scattering problems via providing maps between
1
Under review as a conference paper at ICLR 2020
the scatterers and the scattered field. Han et al. (2018) developed a deep learning-based approach
that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated
using backward stochastic differential equations. The latter is solved by a temporal discretization
and the gradient of the unknown solution at each time step is approximated by a neural network. Li
et al. (2019) approximate the inverse solution map of linear and nonlinear problems directly by a
deep network.
Direct algorithms solve the forward and inverse problem PDEs by directly approximating the solu-
tion with a deep neural network. The network parameters are determined by the optimization of a
cost function such that the optimal solution satisfies the PDE, boundary conditions and initial con-
ditions. Chiaramonte & Kiener (2017) addressed the forward problem by constructing a one layer
network which satisfies the PDE within the domain. The boundary conditions were analytically
integrated in the cost function. They demonstrated their algorithm on the Laplace and hyperbolic
conservation law PDEs. Sirignano & Spiliopoulos (2017) proposed a deep learning forward problem
solver for high dimensional PDEs. Their algorithm was demonstrated on the American option free-
boundary equation. Raissi et al. (2017) focused on continuous time models and solved the Burgers
and Schrodinger equations, and XU & Darve (2019) introduced a novel direct method for the inverse
problem and demonstrated their algorithm on various PDEs.
In this work we address the forward and inverse PDE problems via a direct unsupervised method.
Our key contributions are four fold: (1) inverse problems can be solved in the same framework as
the forward problems. (2) In the both forward and inverse parts we extend the standard L2-based
fidelity term in the cost function by adding L∞-like norm. Moreover, (3) some regularization terms
which impose a-priori knowledge on the solution can be easily incorporated. (4) Our construction
exemplifies the ability to handle free-form domain in a mesh free manner.
Point (1) is essential for full tomography-like solutions as will be explained in the sequel. The
extension of the loss function by the L∞-like norm is fundamental. This term promotes a strong
solution of the PDE. The L2 term, used in recent studies, aims only for weak solutions of the PDE.
Weak solutions may differ from the strong solutions by a set of isolated points where the function
is not continuous. This is not a merely theoretical issue but strongly affects the quality of the result
as we empirically demonstrate. In unsupervised learning of ill-posed problems regularization is
crucial. Choosing the right regularizer and the ability to incorporate it in the formulation is of prime
importance. Our formalism integrates such regularizations in a natural way. We demonstrate our
algorithm by a second order elliptic equation, in particular the Electrical Impedance Tomography
(EIT) application on a circular and three other arbitrary domains. We additionally solve the inverse
problem of diffusion and wave equations.
2	Mathematical Formulation
Let Ω be a bounded open and connected subset of Rd where d is the dimension. A differential
operator L acting on a function u(x) : Rd → R is defined as
Lu(x):= (an(x) ∙ Dn + an-i(x) ∙ DnT + …+ aι(x) ∙ D + a0(x))u(x),	(1)
where Dn is the nth order d-dimensional derivative and a0(x), . . . , an(x) are the coefficients. Con-
sider the partial differential problem with Dirichlet boundary conditions
Lu(X) = f (x),	X ∈ Ω ⊂ Rd
U(X) = uo(x), x ∈ ∂Ω,
(2)
where f(X) : Rd → R is a given function. The forward problem solves u(X) given the coefficients
θ := {a0(X), . . . , an(X)} while the inverse problem determines the coefficients set θ given u(X).
The proposed algorithm approximates the solutions in both problems by neural networks u(X; wu),
{aj (X; waj )} such that the networks are parameterized by wu, {waj }, and the input to the network
is X ∈ Rd. Figure 1 depicts a network architecture of u in R2 . The network consists of few fully
connected layers with tanh activation and linear sum in the last layer. The network is trained to
satisfy the PDE with boundary conditions by minimizing a cost function. In the forward problem
F(U) = λkLu - fk2 + μkLu - f k∞ + ∣∣u - uokι,∂Ω + RF(U),
(3)
2
Under review as a conference paper at ICLR 2020
Figure 1: Network architecture: the point (x, y) ∈ R2 serves as an input and the value u(x, y) as the
output.
and in the inverse problem
I(θ) = λ∣∣Lu - f k2 + μ∣∣Lu - f k∞ + kθ - θokι,∂Ω + RI(。).	(4)
The first two terms enforce the solution to satisfy the equation. The first term minimizes the error in
the L2 sense while the second term minimizes the maximal error. Ideally, the solution to the PDE
has to be satisfied at every point x. This is considered as a strong solution. In the deep learning
framework the optimization is carried out on mini batches of points. The first term minimizes the
equation in the integral or average sense and is therefore insensitive to a point jump in the value of
the integrand. It therefore yields a weak solution. This manifests itself by point discontinuities of
u(x) that are empirically seen as well. The L∞ term promotes a strong solution since it handles
possible discontinuous points where the value of the week solution is different than the strong one.
We consider this discrepancy as an error. Practically, we use a relaxed version of the L∞ norm where
we take the top-K errors. Then, at every mini batch the training procedure minimizes the equation at
K different points having the largest errors. The third term imposes boundary conditions where u0
and θo are the boundary values of u(x) and θ(x). We used the Li norm on the boundary so that the
solution is robust to measurements noise. The last term is a regularizer which can be tailored to the
application. There are few advantages of this setting. First, the solution is a network and therefore
an explicit function. It is a smooth analytic function and is therefore analytically differentiable. It
was recently proven that for such functions, e.g. Lipchitz with constant ν, the approximation of the
function by a fully connected network Γ is given by
l∣u(x) - Γ∣∣LP([0,i]d) ≤ 40ν√dN-2/dL-2/d,
where p ∈ [0, ∞), N number of neurons in a layer, and L number of layers Shen et al. (2019).
In addition, the proposed framework enables setting a prior physical knowledge on the solution by
designing the regularizers RF and RI . Moreover, the training procedure is mesh free. Unlike finite
differences or finite elements methods, we use random points in the domain and its boundary in the
course of the optimization of equation 3 and equation 4, see Figure 2. This means that the solution
does not depend upon a coordinate mesh and We can also define an arbitrary regular domain Ω.
3	Application to Electrical Impedance Tomography
Consider the folloWing elliptic equation Which is a special case of equation 1,
V ∙ (σ(x)Vu(x)) = 0,	X ∈ Ω ⊂ R2	(§)
U(X) = uo(x),	x ∈ ∂Ω,
where V := (∂∕∂χ,∂∕∂y). We assume that 0 < σ(x) ∈ C 1(Ω), which guarantees existence and
uniqueness of a solution U ∈ C2(Ω) Evans (2010).
3
Under review as a conference paper at ICLR 2020
Figure 2: Left to right: finite differences grid, finite elements mesh and random points samples used
in the proposed algorithm.
The elliptical system equation 5 was addressed by Siltanen et al. (2000) in the context of Electrical
Impedance Tomography (EIT) which is a reconstruction method for the inverse conductivity prob-
lem. The function σ indicates the electrical conductivity density, and u is the electrical potential.
An electrical current
∂un	1
/ L= √2∏ cθs(nκ ”I n ∈ Z
(6)
is applied on electrodes on the surface ∂Ω, where K is the angle in polar coordinate system along the
domain boundary, n is the current frequency,夕 is the phase and V is the normal unit. The resulting
voltage u∣∂Ω = uo is measured through the electrodes. The conductivity σ is determined from the
knowledge of the Dirichlet-to-Neumann map or voltage-to-current map
λy ： u | ∂Ω
∂un
→ σ"dɑ ,
Mueller & Siltanen (2012); Alsaker & Mueller (2018); Fan & Ying (2019).
We demonstrate our framework by solving the forward and inverse problem of equation 5 which is a
first step towards a full tomography. Following Mueller & Siltanen (2012), we simulate the voltage
measurement u∣∂ω by the Finite Elements Method (FEM) given three variants of a conductivity
phantom σ(x) depicted in Figure 3. We calculate the FEM solution with different triangle mesh
densities and select as ground truth the one such that finer meshes do not improve the numerical
solution. With our suggested method, the forward problem determines the electrical potential u in
the whole domain Ω given σ, while the inverse problem uses the approximated U and calculates the
conductivity σ given that σ∣∂Ω = σ0.
4 Forward Problem
In the forward problem the conductivity σ(xi) and boundary conditions u0(xb) are given for random
points set {χi} ∈ Ω ⊂ R2, {xb} ∈ ∂Ω ⊂ R2 with sets size of Ns and Nb respectively. A neural
network having the architecture shown in Figure 1 approximates u(x). Let
Li ：= V ∙ (σ(xi)Vu(xi)).
(7)
The cost function equation 3 is then rewritten as
Ns	Nb
F (U(X； Wu))= N- X |Li|2 + K X	ILk | + N Xlu(Xb) - u0 (xb)l + αkwuk2. (8)
s i=1	k∈topK (|Li |)	b b=1
The first term is the L2 norm of the differential operator, the second term is a relaxed version of
the infinity norm where we take the mean value of the top-K values of |Li|. There is some balance
between small K which accounts for few error points and can have negligible effect in the training
procedure, and a large K which resembles the effect of the L2 norm. In all our experiments we em-
pirically selected K = 40 as an intermediate value. The third term imposes the boundary conditions
and the last term serves as a regularizer of the network parameters.
4
Under review as a conference paper at ICLR 2020
Figure 3: Synthetic conductivities σ(x) referred to as phantoms. Left to right: phantom 1, phantom
2 and phantom 3
Table 1: Forward problem results for phantom 1 compared with the DGM method Sirignano &
SPilioPoulos (2017)
	n	φ	u: DGM		u: Proposed		Ux: DGM		ux: Proposed	
			MSE	PSNR	MSE	PSNR	MSE	PSNR	MSE	PSNR
丁	1	~0	2.50e-1	-902	2∙38e-4	39.23	6.01e-6	11.17	7.11e-9	40.43
2	1	π∕8	4.20e-2	14.90	1.13e-4	40.60	7.95e-7	19.27	5.92e-9	40.55
3	1	π∕4	5.46e-2	13.54	1.23e-4	40.03	1.87e-6	13.36	5.62e-9	38.58
4	2	0	5.12e-2	16.90	4.50e-5	47.46	1.64e-6	14.28	2.81e-9	41.94
5	2	π∕4	6.11e-2	6.51	831e-5	35.18	1.99e-6	11.08	2.74e-9	39.69
The first phantom is shown in Figure 3 left. The impedance values associated with the background
elliPses and circle were 1, 5 and 2 resPectively. The original Piecewise constant function σ was
slightly smoothed by a Gaussian kernel.
Figure 10 in the Appendix shows the forward problem results for current ψ with n = 1 and φ = π∕8.
The left column is the FEM solution which is referred to as ground truth, where the toP row indicates
the solution u(x) and the bottom row the derivative of u(x) with respect to the first coordinate x
calculated as the finite difference approximation of the FEM result. The middle column depicts
the outcome of the proposed method where ∂u∕∂x is an analytical derivative of our result. The
right column shows the outcome of the DGM method Sirignano & Spiliopoulos (2017) which is
a special case of equation 3 with λ = 1, μ = 0, L2 norm of the boundary conditions constraint
with no regularizers. Quantitative results of the mean square error (MSE) and peak signal-to-noise
ratio (PSNR) are summarized in Table 1. Clearly, the proposed method outperforms the DGM
method since the weighting parameters, the L∞ norm and the network weights regularization play
a significant role in the loss function.
Table 2: Forward problem results of phantom 2 given a circular domain Ω, and phantom 3 with
domains Ω1, Ω2 and Ω3 as defined in Figure 5
	phantom	n	φ	U MSE	PSNR	u MSE	C PSNR
1	^17Ω	丁	~0	2.86e-4	47.43	1.70e-8	40.61
2	2, Ω	1	π∕2	1.74e-3	38.90	1.03e-8	33.79
3	2, Ω	2	0	1.29e-4	45.52	3.26e-9	41.18
4	2, Ω	2	π∕2	1.30e-4	45.49	8.63e-4	37.07
5	3,Ω1	1	π∕4	6.42e-5	47.16	5.64e-9	39.43
6	3,Ωι	2	π∕4	1.08e-4	34.03	2.61e-9	41.32
7	3,Ω2	1	π∕4	1.08e-4	44.91	4.51e-9	40.39
8	3,Ω2	2	π∕4	5.93e-5	36.64	2.74e-9	41.11
9	3,Ω3	1	π∕4	1.22e-4	44.37	9.17e-9	37.31
10	3,Ω3	2	π∕4	1.74e-4	31.96	2.24e-9	41.99
5
Under review as a conference paper at ICLR 2020
Figure 4: Reconstructed conductivity σ(x) using the generalized inverse problem. Left: phantom 1,
Right: phantom 2. MSE and PSNR are summed up in rows 1-2 of Table 3.
Table 3: Reconstruction of σ(x) using the generalized inverse problem
	phantom	MSE	PSNR
丁	T,Ω	4.06e-2	27.90
2	2, Ω	9.04e-5	40.44
3	3,Ωι	1.07e-2	33.67
4	3,Ω2	5.9e-3	36.27
5	3,Ω3	1.12e-2	33.51
The forward problem was repeated using phantom 2 where the background and circle conductivities
were 1 and 0.2 respectively (Figure 3, middle). Four different current combinations were applied.
Quantitative results are summarized in rows 1-4 of Table 2. Figure 11 in the Appendix shows the
results for both U and ∂u∕∂x for n = 2 and φ = π∕2. The right column presents the relative error
defined as e(x, y) = (ufem(x, y) - u(x, y))/max(ufem).
5 Inverse Problem for EIT
In the inverse problem, the electrical potential u(x) is known while σ(x) is unknown. Since we have
a network which approximates u(x), we can evaluate it at any point x. The objective function equa-
tion 4 then takes the form
λ Ns
z(σ(X； wσD= N X 1Li 12 + K X	ILk 1
S i=1	k∈topK(|Li|)
1 Nb	β Ns
+ N 工卜(Xb) - σ0 (Xb )| + αkwσ l∣2 + N 工 Vσ (Xi)|P
b b=1	s i=1
(9)
As in the forward problem, the first two terms enforce σ to satisfy the PDE, where Li is defined
in equation 7. The third term imposes the boundary conditions, and the fourth regularizes the net-
work parameters. A physical a priori knowledge regarding σ is exploited in this application. The
conductivity is assumed to have well defined sub-regions. We therefore design the fifth term to have
sparse edges via the total variation regularization (p = 1).
Additional inverse problem generalization may exploit multiple u approximations for several cur-
rents ψj . The σ calculation thus, simultaneously relies on all {uj }, resulting a more stable solution.
Let
Lij := ▽• (σ(Xi)VUj (Xi))	(10)
Then equation 9 is generalized to
、 λ N μ μ 一
Jj (σ (X； wσ)) = N X ILijI + K X lLkj |,	(11)
s i=1	k∈topK (|Lij |)
6
Under review as a conference paper at ICLR 2020
Figure 5: Free form shapes. Left to right: Ω1, Ω2, Ω3 and sample points of Ω3. The red points
indicate the boundary.
Figure 6: Reconstructed conductivity σ(x) using the generalized inverse problem given phantom 3
applied on different domains. Left to right: Ω1, Ω2, Ω3. MSE and PSNR are given in rows 3-5 of
Table 3
and
1 Nb	β Ns
I σ(x; wσ = XJj σ(x;wσ D + N X ∣σ (Xb) - σ0 (Xb )| + αkwσ k2 + N X Vσ (Xi)|p.
j	b b=1	s i=1
(12)
Adequate reconstruction results by the generalized inverse problem are shown in Figure 4 and rows
1-2 in Table 3. In both phantoms we used four current combinations ψn,φ: (1, 0),(1, π∕4), (2,0),
(2, π∕4), and σ0 = 1.
6	Free Shape Geometry
We applied the proposed method to arbitrary domains Ω1, Ω2 and Ω3, see Figure 5. The random
sample points within the domain and along its boundary can be easily obtained as can be seen
in Figure 5 right. Forward problem results applied on phantom 3 are presented in rows 5-10 of
Table 2. Figure 12 in the Appendix shows the results for n = 2 and φ = π∕4 for the three domains.
The outcome of the generalized inverse problem equation 12 is shown in Figure 6 and rows 3-5 of
Table 3.
7	Additional Inverse Problem Examples
In this section we exemplify the proposed algorithm with additional inverse problem setting intro-
duced by Xu & Darve (2019). Unknown parameters which are approximated by neural networks are
inferred out of few measurements. The differential operators are approximated in finite differences
schemes discretized by ∆t and ∆h, the temporal and spatial grid spacings. In the two following
examples we applied the cost function equation 4 with total variation and network parameters regu-
larizations.
7.1	Diffusion Equation
Diffusion PDE is given by
du∂t^ = C(x)V2u(x,t) + f(x,t) (x,t) ∈ [-1,1]2 X [0, T],
7
Under review as a conference paper at ICLR 2020
where t denotes the time and the conductivity C(x) is the unknown parameter. In the following we
solve the equation in 2D where x := (x, y). Given two measurements at adjacent times u(x, y, t1)
and u(x, y, t1 +∆t), the conductivity C(x, y) is approximated as a neural network and is determined
as the minimizer of equation 4 with
L(u)-f =
u(x, y, t1 + ∆t) - u(x, y, t1)
∆t
-C(x, y)
D2u(x, y, t1) + D2u(x, y, t1 + ∆t)
2(∆h)2
-f (x, y, t1),
where D2 is the central scheme approximation for the Laplacian on 3x3 stencil. The analytical 2D
conductivity function is given by
C(x, y) = 1 + e-(12xy)2,
and	u(x,y,t) = e-π2t sin ∏x cos(∏y + π∕4),
f(x, y, t) = π2u(x, t)(2C(x, y) - 1).
The measurements were taken at t1 = 0.1 with ∆t = 0.001 and ∆h = 0.002. The result is shown in
Figure 7. As can be seen the smoothness regularizer and the L∞ terms increase the reconstruction
accuracy. In addition, we added few Gaussian noise levels to the measurements. Quantitative results
are summarized in rows 1 - 4 of Table 4.
Figure 7: Reconstructed conductivity C(x, y). Left: ground truth, Middle: proposed method,
Right: Xu & Darve (2019). MSE and PSNR are given in rows 1-4 of Table 4
7.2	Wave Equation
The second example is the wave equation given by
d Udt2y,t) = η2(χ,y)v2u(χ,y,t)	((χ,y),t) ∈ [-1,1]2 X [0,τ].
The unknown parameter is the velocity field η(x, y). Its analytical value is given by
η(x, y) = 0.1 + e-((x-0.5)2)+(y-0.1)2),
and the two initial conditions are
u(x, y, 0) = e-10x2-3y2,
and
u(x,y,∆t)=e-10(x-0.001)2-3(y-0.001)2.
Solutions for all time steps are recursively calculated using the finite differences scheme using the
correct η(x, y). Given three consecutive values of u(x, y, t) the velocity field η(x, y) is extracted by
the minimization of equation 4 where
L(u) =
u(x, y,t1 + ∆t) - 2u(x, y,t1) + u(x, yt1 - ∆t)
(∆t)2
- η2(x, y)
D2 u(χ,y,tι)
(∆h)2
Reconstruction result is depicted in Figure 8 showing the advantage of the proposed method. Addi-
tional quantitave results of noisy measurements are given in rows 5-8 of Table 4.
8
Under review as a conference paper at ICLR 2020
Figure 8: Reconstructed velocity field η(x, y). Left: ground truth, Middle: proposed method,
Right: Xu & Darve (2019). MSE and PSNR are given in rows 5-8 of Table 4
Table 4: Inverse problem results for diffusion and wave equations
	Equation	σ noise	Xu & Darve (2019)		Proposed	
			MSE	PSNR	MSE	PSNR
丁	Diffusion	-0	4.90e-4	39.12	2.04e-4	42.92
2	Diffusion	1e-7	0.259	11.89	0.249	12.06
3	Diffusion	5e-7	0.335	10.77	0.316	11.02
4	Diffusion	1e-6	0.448	9.50	0.438	9.60
5	Wave	0	0.015	19.01	0.013	19.53
6	Wave	3e-9	0.049	13.92	0.015	19.16
7	Wave	3.5e-9	0.056	13.34	0.018	18.32
8	Wave	4e-9	0.059	13.11	0.029	16.19
8	Implementation Details
The network architecture had 4 layers having 26, 26, 26 and 10 neurons. The algorithm was im-
plemented by TensorFlow Ten (2015) using the ADAM optimizer which is a variant of the SGD
algorithm. We used almost the same hyper parameters set in our experiments. Batch size=1000,
decaying learning rate starting at 5e - 3. The learning rate was factored by 0.8 every 200 epochs,
Ns = 80000, Nb = 1200, λ = 0.01, α = 1e - 8, K = 40, μ = 0.01, and β = 0.01. In the diffusion
and wave equation case we set λ = 1, β = 1e - 4, μ = 1e - 4 and α = 1e - 5. Running time, for
the EIT, on Intel i7-8650u CPU was about 15 minutes for the forward problem and 13 minutes for
the generalized inverse problem. For the diffusion and wave equation the running time was about 5
minutes.
Finally, we performed a sensitivity analysis of the algorithm with respect to the number of sample
points Ns. Figure 9 shows the sensitivity of the forward problem algorithm. As expected, with large
number of points we obtain a plateau while with small and intermediate number (up to 50000) the
error is not monotonic due to local minima.
9	Discussion
Deep networks by their nature use compositions of simple functions such as matrix multiplication
and non-linear activations like sigmoid or tanh. This structure (i) enables the approximation ofan ar-
bitrary function Hornik (1989) and (ii) is inherently differentiable. The network architecture dictates
the number of degrees of freedom which in turn enables the expressibility of complex functions. In
this work we present a unified framework for the solution of forward and inverse problems in partial
differential equations in an arbitrary domain. The algorithm relies on direct approximation of the
unknown function by a neural network which yields an analytical smooth solution. The network is
trained to satisfy the PDE and boundary conditions in an unsupervised fashion by the minimization
of a cost function. The optimization procedure depends on random points set within the domain and
its boundary. The problem is therefore mesh free with free-form domain.
9
Under review as a conference paper at ICLR 2020
Figure 9: Sensitivity analysis of the algorithm with respect to the number of sample points.
While recent works formulate the cost function in both forward and inverse problems in the L2 sense,
we introduce a broadened framework. The relaxed L∞ fidelity term was designed to approximate
a strong solution and control possible discrepancies between the weak and the strong solutions.
The L1 norm on the boundary conditions is more robust to measurement noise than the L2 norm.
The additional regularizer (total variation in our examples) has a significant role in the optimization
process where known physical properties like smoothness are imposed on the solution. Lastly,
the network regularizer controls the network weights and prevents over fitting. These additional
terms make the proposed algorithm more accurate and robust compared to other learning-based
methods. We also stress the robustness of our approach exemplified by having almost the same
hyper parameters set in our experiments.
This framework alleviates several problems of the finite differences and finite elements methods. In
particular meshing, discretization and derivatives approximation are solved in a simple and natural
way. Numerical solutions of PDEs in an arbitrary domain are of extreme importance, in particular in
medical imaging applications. The framework is demonstrated by an elliptic system in R2 applied
to Electrical Impedance Tomography for both forward and inverse problems, diffusion and wave
equations. Promising results were achieved for complex and non monotonic functions. Rigorous
analysis of the approximation error and its relation to the network architecture and design are under
current study. Future research includes also full tomography solution, higher dimensional problems
and other classes of PDEs such as dynamic non-linear equations (Burgers, Navier-Stokes etc.)
References
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://
tensorflow.org/. Software available from tensorflow.org.
M. Alsaker and J. L. Mueller. Use of an optimized spatial prior in D-bar reconstructions of EIT tank
data. Inverse Problems and Imaging,12:883-901, 2018.
T. K. Bera, S. K. Biswas, K. Rajan, and J. Nagaraju. Improving image quality in electrical impedance
tomography (eit) using projection error propagation-based regularization (pepr) technique: A sim-
ulation stud. Journal of Electrical Bioimpedance, 2:2-12, 2011.
M. M. Chiaramonte and M. Kiener. Solving differential equations using neural networks, 2017.
http://cs229.stanford.edu/proj2013.
L.C. Evans. Patial Differential Equations. the American Mathematical Society, 2010.
10
Under review as a conference paper at ICLR 2020
Y. Fan and L. Ying. Solving electrical impedance tomography with deep learning. arXiv
1906.03944v1, 2019.
M. Feigin, D. Freedman, and B. W. Anthony. A deep learning framework for single-sided sound
speed inversion in medical ultrasound. arXiv 1810.00322v3, 2018.
A. L. Garcia. Numerical Methods for Physics (Python). CreateSpace Independent Publishing Plat-
form, 2017.
J.	Han, A. Jentzen, and E. Weinan. Solving high-dimensional partial differential equations using
deep learning. arXiv 1707.02568, 2018.
K.	Hornik. Multilayer feedforward networs are universal approximators. Neural Networks, 2:359-
366, 1989.
Y. Khoo and L. Ying. Switchnet: A neural network model for forward and inverse scattering prob-
lems. arXiv 1810.09676v1, 2018.
R. J. LeVeque. Finite Difference Methods for Ordinary and Partial Differential Equations: Steady-
State and Time-dependent Problems. SIAM, 2007.
H. Li, J. Schwab, S. Antholzer, and M. Haltmeier. NETT: Solving inverse problems with deep neural
networks. arXiv 1803.00092, 2018.
Y. Li, J. Lu, and A. Mao. Variational training of neural network approximations of solution maps
for physical models. arXiv 1905.02789v1, 2019.
A. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos. Using deep neural networks for inverse
problems in imaging. IEEE Signal Processing Magazine, 2018.
M. T. McCann, K. H. Jin, and M. Unser. A review of convolutional neural networks for inverse
problems in imaging. arXiv 1710.04011, 2017.
J. Mueller and S. Siltanen. Linear and Nonlinear Inverse Problems with Practical Applications.
SIAM, 2012.
M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics informed deep learning (part i): Data-driven
solutions of nonlinear partial differential equations. arXiv 1711.10561v1, 2017.
J. N. Reddy. An Introduction to the Finite Element Method. McGraw-Hill Education, 2005.
J.K. Seo, K. C. Kim, A. Jargal, K. Lee, and B. Harrach. A learning-based method for solving ill-
posed nonlinear inverse problems: A simulation study of lung EIT. SIAM journal on Imaging
Sciences, 12:1275-1295, 2019.
Z. Shen, H. Yang, and S. Zhang. Nonlinear approximation via compositions. arXiv:1902.10170,
2019.
S. Siltanen, J. Mueller, and D. Isaacson. An implementation of the reconstruction algorithm of a
nachman for the 2d inverse conductivity problem. Inverse Problems, 16(3):681-699, 2000.
J.	Sirignano and K. Spiliopoulos. DGM: A deep learning algorithm for solving partial differential
equations. arXiv 1708.07469v3, 2017.
J.	W. Thomas. Numerical Partial Differential Equations: Finite Difference Methods. Springer, 1995.
K.	Xu and E. Darve. The neural network approach to inverse problems in differential equations.
arXiv 1901.07758v1, 2019.
11
Under review as a conference paper at ICLR 2020
A Appendix
Figure 10: Forward problem results of U(X) and ∂u(x)∕∂x for current frequency n = 1 and phase
φ = π∕8 given phantom 1. Left column: ground truth (FEM). Middle column: proposed method.
Right column: DGM method. MSE and PSNR are reported in row 2 of Table 1
12
Under review as a conference paper at ICLR 2020
(c) u: relative error
(a) u: FEM
(d) ∂u∕∂x: FEM
(b) u: proposed
(e) ∂u∕∂x: proposed
(f) ∂u∕∂x: relative error
Figure 11: Forward problem results of U(X) and ∂u(x)∕∂x for current frequency n = 2 and phase
φ = π∕2 given phantom 2. Left column: ground truth (FEM). Middle column: proposed method.
Right column: relative error. MSE and PSNR are reported in row 4 of Table 2
(a) u: proposed, Ωι	(b) u: proposed, Ω2	(c) u: proposed, Ω3
(d) ∂u∕∂x: proposed, Ωι
(e) ∂u∕∂x: proposed, Ω2
(f) ∂u∕∂x: proposed, Ω3
Figure 12: Forward problem results of u(x) and ∂u(x)∕∂x for current frequency n = 2 and phase
φ = π∕4 given phantom 3 applied on different domains. Left to right: Ω1, Ω2, Ω3. MSE and PSNR
are reported in rows 6, 8 and 10 of Table 2.
13