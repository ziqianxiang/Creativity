Under review as a conference paper at ICLR 2020
Learning Boolean Circuits with Neural Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
Training neural-networks is computationally hard. However, in practice they are
trained efficiently using gradient-based algorithms, achieving remarkable perfor-
mance on natural data. To bridge this gap, we observe the property of local cor-
relation: correlation between small patterns of the input and the target label. We
focus on learning deep neural-networks with a variant of gradient-descent, when
the target function is a tree-structured Boolean circuit. We show that in this case,
the existence of correlation between the gates of the circuit and the target label
determines whether the optimization succeeds or fails. Using this result, we show
that neural-networks can learn the (log n)-parity problem for most product distri-
butions. These results hint that local correlation may play an important role in
differentiating between distributions that are hard or easy to learn.
1	Introduction and Motivation
It is well known (e.g. Livni et al. (2014)) that while deep neural-networks can express any function
that can be run efficiently on a computer, in the general case, training them is computationally
hard. Despite this theoretic pessimism, in practice, deep neural networks are successfully trained on
real world datasets. Bridging this theoretical-practical gap seems to be the holy grail of theoretical
machine learning nowadays. Maybe the most natural direction to bridge this gap is to find a property
of data distributions that determines whether training is computationally easy or hard. The goal of
this paper is to propose such a property.
To motivate this, we first recall the k-parity problem: the input is n bits, there is a subset of k relevant
bits (which are unknown to the learner), and the output should be 1 if the number of 1’s among the
relevant bits is even and -1 otherwise. It is well known (e.g. Shalev-Shwartz et al. (2017)) that the
parity problem can be expressed by a fully connected two layer network or by depth log(n) locally
connected 1 network. We observe the behavior of a one hidden-layer neural network trained on the
k-parity problem, in two different instances: first, when the underlying distribution is the uniform
distribution (i.e. the probability to see every bit is 1); and second, when the underlying distribution
is a slightly biased product distribution (the probability for every bit to be 1 is 0.6). As can be clearly
seen in figure 1, adding a slight bias to the probability of each bit dramatically affects the behavior
of the network: while on the uniform distribution the training process completely fails, in the biased
case it converges to a perfect solution.
This simple experiment shows that a small change in the underlying distribution can cause a dramatic
change in the trainability of neural-networks. A key property that differentiates the uniform from the
biased distribution is the correlation between input bits and the target label. While in the uniform
distribution, the correlation between each bit and the label is zero, in the biased case every bit of the
k bits in the parity has a non-negligible correlation to the label (we show this formally in section 5).
So, local correlations between bits of the input and the target label seems tobe a promising property
which separates easy and hard distributions.
In this paper, we analyze the problem of learning tree-structured Boolean circuits with neural-
networks. The key property that we assume is having sufficient correlation between every gate
in the circuit and the label. We show that a variant of gradient-descent can efficiently learn such
1i.e. every two adjacent neurons are only connected to one neuron in the upper layer.
1
Under review as a conference paper at ICLR 2020
Figure 1: Trainig ReLU networks with one hidden-layer of size 128 with Adam optimizer, on both
instances of the k-Parity problem (k = 5, n = 128). The figure shows the accuracy on a test set.
circuits for some families of distributions, where at the same time, without correlation, gradient-
descent is likely to fail. More concretely, we discuss specific target functions and distributions that
satisfy the local correlation requirement. We show that for most product distributions, gradient-
descent learns the (log n)-parity problem (parity on log n bits of an input with dimension n). We
further show that for every circuit with AND/OR/NOT gates, there exists a generative distribution,
such that gradient-descent recovers the Boolean circuit exactly.
Admittedly, as the primary focus of this paper is on theoretical analysis, the distributions we study
are synthetic in nature. However, to explain the empirical success of neural-networks, we need to
verify whether the local correlation property holds for natural datasets as well. To confirm this, we
perform the following simple experiment: we train a network with two hidden-layers on a single
random patch from images in the ImageNet dataset. We observe that even on a complex task such as
ImageNet, a network that gets only a 3 × 3 patch as an input, achieves 2.6% top-5 accuracy — much
better than a random guess (0.5% top-5 accuracy). The full results of the experiment are detailed
in the appendix. This experiment highlights that, to some extent, natural datasets display a local
correlation property: even a few “bits” of the input already have some non-negligible information
on the target label.
2	Related Work
In recent years, the success of neural-networks has inspired an ongoing theoretical research, trying
to explain empirical observations about their behavior. Some theoretical works show failure cases of
neural-networks. Other works give different guarantees on various learning algorithms for neural-
networks. In this section, we cover the main works that are relevant to our paper.
Failures of gradient-based algorithms. Various works have shown different examples demonstrat-
ing failures of gradient-based algorithm. The work of Shamir (2018) shows failures of gradient
descent, both in learning natural target functions and in learning natural distributions. The work of
Shalev-Shwartz et al. (2017) shows that gradient-descent fails to learn parities and linear-periodic
functions under the uniform distribution. In Das et al. (2019), a hardness result for learning random
deep networks is shown. Other similar failure cases are also covered in Abbe & Sandon (2018);
Malach & Shalev-Shwartz (2019). While the details of these works differ, they all share the same
key principal - if there is no local correlation, gradient-descent fails. Our work complements these
results, showing that in some cases, when there are local correlations to the target, gradient-descent
succeeds to learn the target function.
Learning neural-networks with gradient-descent. Recently, a large number of papers have pro-
vided positive results on learning neural-networks with gradient-descent. Generally speaking, most
of these works show that over-parametrized neural-networks, deep or shallow, achieve performance
that is competitive with kernel-SVM. Daniely (2017) shows that SGD learns the conjugate kernel
associated with the architecture of the network, for a wide enough neural-network. The work of
Brutzkus et al. (2017) shows that SGD learns a neural-network with good generalization, when
the target function is linear. A growing number of works show that for a specific kernel induced
2
Under review as a conference paper at ICLR 2020
by the network activation, called the Neural Tangent Kernel (NTK), gradient-descent learns over-
parametrized networks, for target functions with small norm in the reproducing kernel Hilbert space
(see the works of Jacot et al. (2018); Xie et al. (2016); Oymak & Soltanolkotabi (2018); Allen-Zhu
et al. (2018a;b); Oymak & Soltanolkotabi (2019); Arora et al. (2019); Du et al. (2018); Ma et al.
(2019); Lee et al. (2019)). While these results show that learning neural-networks with gradient-
descent is not hopeless, they are in some sense disappointing —in practice, neural-networks achieve
performance that are far better than SVM, a fact that is not explained by these works. A few results
do discuss success cases of gradient-descent that go beyond the kernel-based analysis (Brutzkus &
Globerson, 2017; 2019; Allen-Zhu &Li, 2019; Yehudai & Shamir, 2019). However, these works still
focus on very simple cases, such as learning a single neuron, or learning shallow neural-networks
in restricted settings. In this work we deal with learning deep networks, going beyond the common
reduction to linear classes of functions.
Layerwise optimization algorithms. In this paper, we analyze the behavior of layerwise gradient-
descent — optimizing one layer at a time, instead of the common practice to optimize the full
network end-to-end. We do so since such algorithm greatly simplifies our theoretical analysis. While
layerwise training is not a common practice, recent works (Belilovsky et al., 2018; 2019) have
shown that such algorithms achieve performance that are competitive with the standard end-to-end
approach, scaling up to the ImageNet dataset. We note that other theoretical works have studied
iterative algorithms that learn neural-networks layer-by-layer (Arora et al., 2014; Malach & Shalev-
Shwartz, 2018). However, our work focuses specifically on layerwise gradient-descent, considering
the problem of learning Boolean circuits.
Learning Boolean Circuits. The problem of learning Boolean circuits has been studied in the
classical literature of theoretical machine learning. The work of Kearns et al. (1987) gives various
positive and negative results on the learnability of Boolean Formulas, including Boolean circuits.
The work of Linial et al. (1989) introduces an algorithm that learns a constant-depth circuit in quasi-
polynomial time. Another work by Kalai (2018) discusses various properties of learning Boolean
formulas and Boolean circuits. Our work differs from the above in various aspects. Our main focus
is learning deep neural-networks with gradient descent, where the target function is implemented
by a Boolean circuit, and we do not aim to study the learnability of Boolean circuits in general.
Furthermore, we consider Boolean circuits where a gate can take any Boolean functions, and not
only AND/OR/NOT, as is often considered in the literature of Boolean circuits. On the other hand,
we restrict ourselves to the problem of learning circuits with a fixed structure of full binary trees.
We are not aware of any work studying a problem similar to ours.
3	Problem Setting
We consider the problem of learning binary classification functions over the Boolean cube. So, let
X = {±1}n be the instance space and Y = {±1} be the label set. Throughout the paper, we
assume the target function is given by a Boolean circuit. In general, such assumption effectively
does not limit the set of target functions, as any computable function can be implemented by a
Boolean circuit. We define a circuit C to be a directed graph with n input nodes and a single output
node, where each inner node has exactly two incoming edges, and is labeled by some arbitrary
Boolean function f : {±1}2 → {±1}, which we call a gate 2. For each node v in C we denote by
Y(V) ∈ {f : {士1}2 → {±1}} its gate. We recursively define hv,c : {±1}n → {±1} to be:
hv,C(x) = γ(v) (hu1,C(x), hu2,C(x))
where u1 , u2 are the two nodes with outcoming edges to v. Finally, define hC = ho,C, where o is
the output node.
We study the problem of learning the target function hC, when C is a full binary tree, and n = 2d,
where d is the depth of the tree. The leaves of the tree are the input bits, ordered by x1, . . . xn.
Admittedly, such assumption greatly limits the set of target functions, but still gives a rather rich
family of functions. For example, such circuit can calculate the parity function on any k bits of the
input (the function calculated by f(x) = Qi∈I xi for some set of indexes I). We note that the total
number of functions calculated by such tree grows like 6n, as shown in Farhoodi et al. (2019).
2Note that in the literature on Boolean circuits it is often assumed that the gates are limited to being AND/OR
and NOT. We allow the gates to take any Boolean function, which makes this model somewhat stronger.
3
Under review as a conference paper at ICLR 2020
We introduce a few notations that are used in the sequel. Fix some tree structured binary circuit C .
This circuit has d levels, and we denote vi,j the j-th node in the i-th level of the tree, and denote
γi,j = γ(vi,j). Fix some i ∈ [d], let ni := 2i, and denote by Γi : {±1}ni → {±1}ni/2 the function
calculated by the i-th level of the circuit:
ri(X) =(Yi-1,1(X1,x2b...,Yi-1,ni∕2(Xni-1, Xni))
For i < i0, we denote: Γi...io := Γi ◦••• ◦ Γio. So, the full circuit is given by he(x) = Γι...d(x).
As noted, our goal is to learn Boolean circuits with neural-networks. To do so, we use a network
architecture that aims to imitate the Boolean circuits described above. We replace each Boolean gate
with a neural-gate: a one hidden-layer ReLU network, with a hard-tanh3 activation on its output.
Formally, let σ be the ReLU activation, and let φ be the hard-tanh activation, so:
(—1 x ≤ —1
σ(X) = max(X, 0), φ(X) = X X ∈ (-1, 1)
[l x ≥ 1
Define a neural-gate to be a neural-network with one hidden layer, input dimension 2, with ReLU
activation for the hidden-layer and hard-tanh for the output node. Namely, denote gw,v : R2 → R
such that:
gw,v(x) = φ(	viσ(hwl, xi))
l=1
Notice that a neural-gate gw,v of width 4 or more can implement any Boolean gate. That is, we
can replace any Boolean gate with a neural-gate, and maintain the same expressive power. To
implement the full Boolean circuit defined above, we construct a deep network of depth d (the
depth of the Boolean circuit), with the same structure as the Boolean circuit. We define d blocks,
each block has neural-gates with the same structure and connectivity as the Boolean circuit. A block
BW (i),V (i) : R2i → R2i-1, is defined by:
BW(i),V(i)(x) = [gw(i,1),v(i,1) (X1, X2), gw(i,2),v(i,2) (X3, X4), . . . , gw(i,2i-1),v(i,2i-1) (X2i-1, X2i)]
We consider the process of training neural-networks of the form NWN = BW(i),v(i) ◦ ∙∙∙ ◦
BW(d),V(d) . Notice that indeed, a network NW,V can implement any tree-structured Boolean cir-
cuit of depth d. In practice, neural-networks are trained with gradient-based optimization algorithm,
in an end-to-end fashion. That is, the weights of all the layers are optimized together, with gradient
updates on a given sample. To simplify the analysis, we instead consider a layerwise optimization
algorithm, that performs gradient updates layer-by-layer. While this approach is much less popu-
lar, it has been recently shown to achieve performance that are comparable to end-to-end training,
scaling up to the ImageNet dataset (Belilovsky et al., 2018).
Denote by P the average-pooling operator, defined by P(xi,..., Xn) = ɪ Pn=ι Xi. Denote the
hinge-loss by '(y,y) = max(1 — yy, 0) and denote the loss on the distribution by LD(f) =
E(χ,y)〜D ['(f (x),y)]. For a sample S ⊆ X × Y, denote the loss on the sample by LS(f) =
啬 P(X y)∈s '(f (x), y). The layerwise gradient-descent algorithm for learning deep networks is
described in algorithm 1.
For simplicity, we assume that the second layer of every neural-gate is fixed, such that v ∈ {±1}.
Notice that this does not limit the expressive power of the network. Algorithm 1 iteratively opti-
mizes the output of the network’s layers, starting from the bottom-most layer. For each layer, the
average-pooling operator is applied to reduce the output of the layer to a single bit, and this output
is optimized with respect to the target label. Note that in fact, we can equivalently optimize each
neural-gate separately and achieve the same algorithm. However, we present a layerwise training
process to conform with algorithms used in practice.
3We chose to use the hard-tanh activation over the more popular tanh activation since it simplifies our
theoretical analysis. However, we believe the same results can be given for the tanh activation.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Layerwise Gradient-Descent
input:
Sample S ⊆ X × Y, number of iterations T ∈ N, learning rate η ∈ R.
Let Nd — id
for i = d . . . 1 do
Initialize W0(i) , V0(i) .
for t = 1 . . . T do
Update W(i) - Wt-)ι - ηGLS(P(BW⑸ V⑸。岫)
∂ Wt-1	t-1 , 0
end for
UpdateNi-I — BWp,V⑶。Ni
end for
Return N0
4	Main Results
Our main result shows that algorithm 1 can learn a function implemented by the circuit C, when
running on “nice” distributions, with the local correlation property. We start by describing the
distributional assumptions needed for our main results. Let D be some distribution over X × Y . For
some function f : X → X0, We denote by f (D) the distribution of (f (χ),y) where (x, y)〜D. Let
D(i) be the distribution Γ(i+1)...d(D). Denote by ci,j the correlation between the output of the j-th
gate in the i-th layer and the label, so: ci,j := ED(i) [xj y].
Define the influence of the j-th gate in the i-th layer with respect to the uniform distribution (U) by:
Zij= Px〜U [ri—1(X) = ri—1(X ㊉ ej)] := Px〜U [ri —1(X) = ri-1(x1, ..., -χj,..., Xn)]
Now, we introduce the main assumption on the distribution D. We assume the following:
Assumption 1. (local correlation) There exists some ∆ ∈ (0, 1) such that for every layer i ∈ [d]
and for every gate j ∈ [2i] with Ii,j 6= 0, the value of ci,j satisfies |ci,j | > |ED [y]| + ∆.
Another technical assumption we need to make is the following:
Assumption 2. (label bias) There exists some β ∈ (0, 1) such that |ED [y]| > β.
Before we present the main results, we wish to discuss the distributional assumptions given above.
Assumption 1 is the key assumption required for the algorithm to succeed in learning the target func-
tion. Essentially, this assumption requires that the output of every gate in the circuit will “explain”
the label slightly better then simply observing the bias between positive and negative examples.
Clearly, gates that have no influence on the target function never satisfy this property, so we require
it only for influencing gates. While this is a strong assumption, in section 5 we discuss examples
of distributions where this assumption typically holds. Furthermore, the experiment described in
section 1 hints that this assumption may hold for natural data. Assumption 2 is a simple technical
assumption, that requires that the distribution of positive and negative examples is slightly biased.
In a sense, we expect that “most” distributions would not be exactly balanced, so this assumption is
easy to satisfy.
Now, consider the case where D (limited to X) is a product distribution: for every j 6= j0, the
variables Xj and Xjo are independent, for (x, y)〜D. A simple argument shows that any product
distribution D that satisfies assumptions 1, satisfies the following properties:
Property 1. There exists some ∆ ∈ (0, 1) such that for every layer i ∈ [d] and for every gate
j ∈ [2i], the output of the j-th gate in the i-th layer satisfies one of the following:
•	The value of the gate j is independent of the label y, and its influence is zero: Ii,j = 0.
•	The value of ci,j satisfies |ci,j | > |ED [y]| + ∆.
Property 2. For every layer i ∈ [d], and for every gate j ∈ [2i—1], the value of (X2j—1, X2j) (i.e, the
input to the j-th gate of layer i - 1) is independent of the label y given the output of the j-th gate:
P(x,y)〜D(i) [(X2j -1,x2j ) = p,y = y0|Yi—1,j (X2j —1,x2j)]
=P(x,y)〜D⑸[(X2j -1, x2j) = plYi—1,j (x2j—1, x2j)] ∙ P(x,y)〜D⑸ [y = y0|Yi—1,j(x2j—1, x2j)]
5
Under review as a conference paper at ICLR 2020
Property 1 is immediate from assumption 1. The following lemma shows that property 2 is satisfied
as well for any product distribution:
Lemma 1. Assume D (restricted to X) is a product distribution (i.e., for every j 6= j0 we have that
Xj and Xjo are independent, for (x, y)〜D). Then D satisfies property 2.
Notice that properties 1, 2 may hold for distributions that are not product distribution (as we show
in the next section). Specifically, property 2 is a very common assumption in the field of Graphical
Models (see Koller & Friedman (2009)). For our results to hold in a more general setting, we use
properties 1 and 2, instead of assuming that D is a product distribution satisfying assumption 1. So,
given a distribution satisfying properties 1, 2 and assumption 2, we show that algorithm 1 achieves
an arbitrarily good approximation with high probability, with sample complexity and run-time quasi-
polynomial in the dimension n:
Theorem 1. Let D be a distribution satisfying properties 1, 2 and assumption 2. Assume that
for every i we initialize Wf) such that IWf)I ≤ 4√k. Fix some e, δ > 0 and as-
sume that k ≥ log-1(4)log(竿),and that η ≤ 焉. Assume we SamPle S 〜 D, with
|S| > m min{∆ 2力产 n11+4log n-2logmm{A2e} log(竿).Then, with probability at least 1 - δ, when
running algorithm 1 on the sample S, the algorithm returns the a function such that:
E(x,y)〜D [N0(x) = he(x)] ≤ C
when running T > 叮 mijɛfzek n6.5+2log nTogmin{A2e} stepsfor each layer.
The above shows a learnability result in the standard PAC setting (given our distributional assump-
tions), where we only guarantee approximation of the target function under the given distribution.
In fact, we can get a stronger result, and show that the algorithm learns the function hC exactly, with
run-time and sample complexity polynomial in n. To get this result, we need to require that there is
no feasible pattern (pair of bits) in the Boolean circuit that is extremely rare:
Assumption 3. There exists some C ∈ (0, 1) such that for every layer i ∈ [d], for every gate
j ∈ [2i-1] and for ^very P ∈ {±1}2 such that P(χ,y)〜D(i) [(x2j-1, x2j) = p] = 0, it holds that:
P(x,y)〜D(i) [(x2j-1,x2j) = p] ≥ c.
In section 5 we discuss distributions that satisfies assumption 3. Given all the above assumptions,
we get the following:
Theorem 2. Let D be a distribution satisfying properties 1, 2 and assumptions 2, 3. Assume
that for every i we initialize Wf) such that IWf)I ≤ 4√2k. Fix some δ > 0 and as-
sume that k ≥ log-1(4)log( 2nd), and that η ≤ 焉. Assume we SamPle S 〜 D, with
|S| > 声 min{∆ 2β}2 log(8δd). Then, with probability at least 1 — δ, when running algorithm 1
on the sample S, the algorithm returns a function such that N0(x) = hC (x) for all x ∈ X, when
running T > 叮 mi：{14e}e StePSfOr each layer.
We give the full proof of the theorems in the appendix, and give a sketch of the argument here.
Observe that the input to the (i, j)-th neural-gate is a pattern of two bits. The target gate (the
(i, j)-th gate in the circuit C) identifies each of the four possible patterns with a single output bit.
For example, if the gate is OR, then the patterns {(1, 1), (-1, 1), (1, -1)} get the value 1, and the
pattern (-1, -1) gets the value -1. Fix some pattern p ∈ {±1}2, and assume that the output of
the (i,j)-th gate on the pattern p is 1. Since we assume the output of the gate is correlated with the
label, the loss function draws the output of the neural-gate on the pattern p toward the correlation of
the gate. In the case where the output of the gate on p is -1, the output of the neural-gate is drawn
to the opposite sign of the correlation. All in all, the optimization separates the patterns that evaluate
to 1 from the patterns that evaluate to -1. In other words, the neural-gate learns to implement the
target gate. This way, we can show that the optimization process makes the network recover all the
influencing gates, so that at the end of the optimization the network implements the circuit.
Observe that when there is no correlation, the above argument fails immediately. Since the label is
slightly biased, when there is no correlation the output of the neural-gate is drawn towards the bias
of the label for all the input patterns, regardless of the value of the gate. If the gate is not influencing
6
Under review as a conference paper at ICLR 2020
the target function (i.e. Ii,j = 0), then this clearly doesn’t effect the overall behavior. However, if
there exists some influencing gate with no correlation to the label, then the output of the neural-gate
will be constant on all its input patterns. Hence, the algorithm will fail to learn the target function.
This shows that assumption 1 is in fact critical for the success of the algorithm.
5	Distributions
In the previous section we showed that algorithm 1 can learn tree-structured Boolean circuits in
polynomial run-time and sample complexity. These results require some non-trivial distributional
assumptions. In this section we study specific families of distributions, and show that they satisfy
the above assumptions.
First, we study the problem of learning a parity function on logn bits of the input, when the under-
lying distribution is a product distribution. The problem of learning parities was studied extensively
in the literature of machine learning theory (Feldman et al., 2006; 2009; Blum et al., 2003; Shalev-
Shwartz et al., 2017; Brutzkus et al., 2019), and serves as a good case-study for the above results.
In the (log n)-parity problem, we show that in fact most product distributions satisfy assumptions
1-3, hence our results apply to most product distributions. Next, we study distributions given by
a generative model. We show that for every circuit with gates AND/OR and NOT, there exists a
distribution that satisfies the above assumptions, so algorithm 1 can learn any such circuit exactly.
5.1	Product Distributions
We observe the k-Parity problem, where the target function is f(x) = j∈I xj some subset I ⊆ [n]
of size |I | = k. A simple construction shows that f can be implemented by a tree structured circuit
as defined previously. We define the gates of the first layer by:
γd-1,j (z1 , z2)
z1z2	x2j -1	, x2j ∈ I
z1	x2j -1	∈ I, x2j
z2	x2j ∈	I, x2j -1
1	o.w	
And for all other layers i < d - 1, we define: γi,j(z1, z1) = z1z2. Then we get the following:
Lemma 2. Let C be a Boolean circuit as defined above. Then: hC (x) = j∈I xj = f (x).
Now, let DX be some product distribution over X, and denote pj := PDX [xj = 1]. Let D be the
distribution of (x, f (x)) where X 〜DX. Then for the circuit defined above We get the following
result:
Lemma 3. Fix some ξ ∈ (0,1). For ^very product distribution D with Pj ∈ (ξ, 1 —ξ) ∪ (1+ξ, 1—ξ)
for ^very j, it holds that if Zij	= 0 then	|cij| —	|E	[y]|	≥	ξk	and P(z,y)〜「(…d(D)	[zj	= 1] ∈
(ξ,1-ξ).
The above lemma shows that every product distribution that is far enough from the uniform distri-
bution, or from a constant distribution, satisfies assumptions 1 and 2 with β, ∆ = (2ξ)k. Using the
fact that at each layer, the output of each gate is an independent random variable (since the input
distribution is a product distribution), we get that assumption 3 is satisfied with = ξ2 . This gives
us the following result:
Corollary 1. Let D be a product distribution with Pj ∈ (ξ, 1 — ξ) ∪ (1 + ξ, 1 — ξ) for every j,
with the target function being the (log n)-Parity (i.e., k = log n). Then, when running algorithm 1
as described in Theorem 2, with probability 1 — δ the algorithm returns the true target function hC,
with run-time and sample complexity polynomial in n.
5.2	Generative Models
Next, we move beyond product distributions, and observe families of distributions given by a gener-
ative model. We limit ourselves to circuits where each gate is chosen from the set{∧,∨,「∧,「∨}.
For every such circuit, we define a generative distribution as follows: we start by sampling a label
7
Under review as a conference paper at ICLR 2020
for the example, from a slightly imbalanced distribution (to satisfy assumption 2). Then iteratively,
for every gate, we sample uniformly at random a pattern from all the pattern that give the correct
output. For example, if the label is 1 and the topmost gate is OR, we sample a pattern uniformly
from {(1, 1), (1, -1), (-1, 1)}. The sampled pattern determines what should be the output of the
second topmost layer. For every gate in this layer, we sample again a pattern that will result in the
correct output. We continue in this fashion until reaching the bottom-most layer, which defines the
observed example.
Formally, for a given gate Γ ∈ {∧, ∨, -∧, -∨},we denote the following sets of patterns:
SΓ = {v ∈ {±1}2 : Γ(v1,v2) = 1}, SΓc = {±1}2 \ SΓ
We recursively define D(0), . . . , D(d), where D(i) is a distribution over {±1}2i × {±1}:
•	D(O) is a distribution supported on {(1,1),(-1, -1)} such that Pd(o)[(1,1)] = 2 + ξ and
pd(0) [(-1, - I)] = 2 - ξ, for some 0 <ξ< ι2 (2).
•	To sample (x,y)〜D(i), first sample (z,y)〜D(i-1). Then, for all j ∈ [2i-1], if
Zj = 1 sample Xj 〜 U ni(Sγi,j ), and if Zj = -1 sample Xj 〜Uni(SYij) Set X =
[x01 , . . . , x02i-1] ∈ {±1}2i, and return (x, y).
Then we have the following results:
Lemma 4. For every i ∈ [d] and every j ∈ [2i], denote qj = E(x,y)〜D(i)[xjy]. Then we have:
∣Ci,j∣-∣E[y]∣ > 3 (3)d = 3nlog(2/3)
We also need the following simple observation:
Lemma 5. For every i ∈ [d] we have Γi(D(i)) = D(i-1).
By definition, we have E [y] = 2ξ, so D(d) satisfies assumption 2 with β = ξ. Notice that from
Lemma 4, the distribution D(d) satisfies property 1 with ∆ = 3nlog(2/3) (note that since we re-
strict the gates to AND/OR/NOT, all gates have influence). By its construction, the distribution also
satisfies property 2, and it satisfies assumption 3 with e = (4)d = n2. Therefore, we can apply The-
orem 2 on the distribution D(d), and get that algorithm 1 learns the circuit C exactly in polynomial
time. This leads to the following nice corollary:
Corollary 2. With the assumptions and notations of Theorem 2, for every circuit C with gates in
{∧, ∨, -∧, -∨}, there exists a distribution D such that when running algorithm 1 ona SamPlefrom
D, the algorithm returns hC with probability 1 - δ, in polynomial run-time and sample complexity.
Note that the fact that for every circuit there exists a distribution that can be learned in the PAC
setting is trivial: simply take a distribution that is concentrated on a single positive example, and
approximating the target function on such distribution is achieved by a classifier that always returns
a positive prediction. However, showing that there exists a distribution on which algorithm 1 exactly
recovers the circuit, is certainly non-trivial.
6	Discussion
In this paper we suggested the property of local corrleation as a possible candidate for differenti-
ating between hard and easy distributions. We showed that on the task of learning tree-structured
Boolean circuits, the existence of local correlations between the gates and the target label allows
layerwise gradient-descent to learn the target circuit. Furthermore, we showed specific tasks and
distributions which satisfy the local correlation property. These results raise a few open questions,
which we leave for future work. The most immediate research problem is showing similar results
for more general structures of Boolean circuit, and on a wider range of distributions (beyond product
distributions or generative models). More generally, we suggest that the local correlation property
may be important in a broader context, beyond Boolean circuits. For example, examining whether an
equivalent property exists when the target function is a convolutional network is an extremely inter-
esting open problem. Needless to say, finding other properties of natural distribution that determine
whether gradient-based algorithms succeed or fail is another promising research direction.
8
Under review as a conference paper at ICLR 2020
References
Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. arXiv preprint
arXiv:1812.06369, 2018.
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some
deep representations. In International Conference on Machine Learning, pp. 584-592, 2014.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale
to imagenet. arXiv preprint arXiv:1812.11446, 2018.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Decoupled greedy learning of cnns.
arXiv preprint arXiv:1901.08164, 2019.
Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and the
statistical query model. Journal of the ACM (JACM), 50(4):506-519, 2003.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
605-614. JMLR. org, 2017.
Alon Brutzkus and Amir Globerson. Why do larger models generalize better? a theoretical per-
spective via the xor problem. In International Conference on Machine Learning, pp. 822-830,
2019.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Alon Brutzkus, Amit Daniely, and Eran Malach. Id3 learns juntas for smoothed product distribu-
tions. arXiv preprint arXiv:1906.08654, 2019.
Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2422-2430, 2017.
Abhimanyu Das, Sreenivas Gollapudi, Ravi Kumar, and Rina Panigrahy. On the learnability of deep
random networks. arXiv preprint arXiv:1904.03866, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Roozbeh Farhoodi, Khashayar Filom, Ilenna Simone Jones, and Konrad Paul Kording. On functions
computed on trees. arXiv preprint arXiv:1904.02309, 2019.
Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami. New results for
learning noisy parities and halfspaces. In 2006 47th Annual IEEE Symposium on Foundations of
Computer Science (FOCS’06), pp. 563-574. IEEE, 2006.
Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami. On agnostic
learning of parities, monomials, and halfspaces. SIAM Journal on Computing, 39(2):606-645,
2009.
9
Under review as a conference paper at ICLR 2020
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing Systems, pp. 8571-
8580, 2018.
Gil Kalai. Boolean functions: Influence, threshold and noise. In European Congress of Mathematics,
pp. 85-110, 2018.
Michael Kearns, Ming Li, Leonard Pitt, and Leslie Valiant. On the learnability of boolean formulae.
In Annual ACM Symposium on Theory of Computing: Proceedings of the nineteenth annual ACM
conference on Theory of computing, volume 1987, pp. 285-295. Citeseer, 1987.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
press, 2009.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, fourier transform, and
learnability. In 30th Annual Symposium on Foundations of Computer Science, pp. 574-579. IEEE,
1989.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in neural information processing systems, pp. 855-863, 2014.
Chao Ma, Lei Wu, et al. A comparative analysis of the optimization and generalization property
of two-layer neural network and random feature models under gradient descent dynamics. arXiv
preprint arXiv:1904.04326, 2019.
Eran Malach and Shai Shalev-Shwartz. A provably correct algorithm for deep learning that actually
works. arXiv preprint arXiv:1803.09522, 2018.
Eran Malach and Shai Shalev-Shwartz. Is deeper better only when shallow is good? arXiv preprint
arXiv:1903.03488, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? arXiv preprint arXiv:1812.10004, 2018.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. arXiv:1902.04674 [cs, math, stat],
February 2019. URL http://arxiv.org/abs/1902.04674. arXiv: 1902.04674.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3067-
3075. JMLR. org, 2017.
Ohad Shamir. Distribution-specific hardness of learning neural networks. The Journal of Machine
Learning Research, 19(1):1135-1163, 2018.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv
preprint arXiv:1611.03131, 2016.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. arXiv preprint arXiv:1904.00687, 2019.
10
Under review as a conference paper at ICLR 2020
A Experiments
Figure 2 details the results of the ImageNet experiment discussed in the introduction.
)%( ycarucc
Top-5 (random)
Top-5 (single-patch)
Top-1 (random)
Top-1 (single-patch)
1
0 I	I	I	I
3579
patch size (px)
Figure 2: Training a ReLU neural network, with two hidden-layers of size 512, on a single patch of
size k × k from the ImageNet data. The patch is randomly chosen from inside the image. We train
the networks with Adam, with batch size of 50, for 10k iterations.
B Proofs of Section 4
We assume w.l.o.g that for every i, j such that Ii,j = 0, the (i, j) gate is constant γi,j ≡
sign(ED [y]). Since the output of this gate has no influence on the output y, we can choose it
freely without changing the target function. To prove Theorem 1 and Theorem 2, we observe the
behavior of the algorithm on the i-th layer. Let ψ : {±1}ni → {±1}ni be some mapping such that
ψ(x) = (ξι ∙ xι,... ,ξni ∙ Xni) for ξι,... ,ξni ∈ {±1}. WealsodefineO : {±1}ni/2 → {±1}ni/2
such that:
ψi(z) = (V1z1, ... , vni/2zni/2)
where νj := sign(ci-1,j ) ci-1,j 6= 0
1	Ii-1,j = 0
Fix some 0 > 0. We need to handle “bad” examples - examples that “rare” patches appear in them.
For every (i, j) gate, we observe all the input patterns p to the (i, j) gate that appear with probability
at most 0. Denote the following set of triplets:
Pee0 := {(ij, p) : P(χ,y)〜D(i) [(X2j-1,x2j) = p] < e0}
Denote the following set of “bad” examples:
Xee0 := x ∈ X : ∃(i, j, p) ∈ Pee0 s.t. (z2j-1, z2j) =p for z = Γ(i+1)...d(x)
We have the following important result, which we prove in the sequel:
Lemma 6. Fix e > 0 and let e0 ≤ e SUCh that P(χ,y)〜D [x ∈ Xee，] < ξ√^ min{∆, 2β}. Assume
we initialize w(0) such that ∣∣w(0) ∣∣ ≤ 告.Fix δ > 0. Assume we sample S 〜D, with |S| >
e2 min{∆,2β}2 log( 8ni). Assume that k ≥ log-1( 3 )log( 8δni), and that η ≤ Ink. Let Ψ : X →
[-1, 1]ni/2 such that for every x ∈/ Xe0 we have Ψ(x) = ψ ◦ Γ(i+1)...d(x) for some ψ as defined
above. Assume we perform the following updates:
Wt(i) J Wt-)1 - η∂WpξLS(P(BWf)[Mi)))
Then with probability at least 1-δ ,for t > 容年；]；{△ 2§} we have: BW ⑸ V (i)(x)= 夕 i ◦□ oψ(x)
for every x ∈/ Xee.
11
Under review as a conference paper at ICLR 2020
Given these results, we can prove the main theorems:
Proof. of Theorem 1 and Theorem 2. Fix δ0 = d. Let €0 ≥ ∙∙∙ ≥ Ed > 0 such that for every i ∈ [d]
we have: P(χ,y)〜D [x ∈ XeJ < ei-18√nx,2β} (We will note the exact value of EiIater). We show
that for every i ∈ [d], w.p at least 1 - (d - i + 1)δ0, after the i-th step of the algorithm we have
Ni-1(X) = Wi ◦ Γi...d(X) for every X ∈/ Xei-1 . By induction on i:
•	For i = d, we get the required using Lemma 6 with ψ, Ψ = id and = d-1, 0 = d.
•	Assume the above holds for i, and We shoW it for i - 1. By the assumption, W.p at least
1 - (d - i + 1)δ0 we have Ni-I(X) = Wi ◦ Γi…d(x) for every X ∈ X"-ι. Observe that:
∂LD
dw(i-i) (P(BW匕1,Mj) ClM-D)=
So using Lemma 6 with ψ = Wi , Ψ = Ni and E
1 - δ0 we have BW(i-1),V(i-1) (X) = Wi-1 ◦ Γi
dLNi-ι(D)
∂ W(iT)
Ei—2 , E =
(P (BW (i-1),V (i-1) ))
i-1 we get that w.p at least
-ι ◦夕i(x) for every X ∈ X"—. In this
case, since ψi ◦夕i = id, We get that for every X ∈ X"_:
Ni-2(x) = BW (i-1),V (i-1) ◦Ni-1(x)
=(ψi-1 ◦ ri-1 ◦ ψi) ◦ (ψi ◦ ri...d)(x) = ψi-1 ◦ r(i—1)…d(X)
and using the union bound gives the required.
Notice that 夕 1 = id: by definition of D(O) = Γι...d(D), for (z, y)〜D(O) we have Z = Γι…m(x)
and also y = Γ1...d(x) for (x, y)〜 D Therefore, we have c0,1 = E(χ,y)〜d(o)[xy] = 1, and there-
fore Wi(z) = sign(cO,1)z = z. Now, choosing i = 1, the above result shows that with probability at
least 1 - δ, the algorithm returns NO such that NO(X)= 夕 1 ◦「1 ◦•••◦ Γm(x) = he(x) for every
x ∈/ X0 .
To prove Theorem 2, it is enough to observe that when taking 0
Ed = E, assumption


3	implies that Pe = 0 and therefore Xe = 0, so the theorem follows. To prove Theorem 1, we
take €0 = E and inductively define Ei = "-谓出金2e}. Notice that ∣Ri∣ ≤ Pi∈[d] ni ∣{±1}2∣ =
4	Plio=g0n 2i—1 = 4n. So, using the union bound We get:
P(x,y)〜D [x ∈ Xej = P(x,y)〜D [∪(i,j,p)∈pq「(i+1)…d(X)(2j —1,2j) = p
≤	P(x,y)~D [r(i+1)∙∙∙d(X)(2j-1,2j) = p]
(i,j,p)∈Pei
≤ X	P(x,y)〜D(i) [(X2j -1,x2j)= p] < IPee0 I ∙ Ei ≤ \£乳,
(i,j,p)∈Pei	i
NoW observing that Q	— min{∆,2β}de —	nlogmin{d2e}e	加VeS	the	re。Uired
NoW, ODSerVing that Ed	=	25.5dn2d =	-n5.5 + 2 log n-	gives	the	required.
□
In the rest of this section we prove Lemma 6. Fix some i ∈ [d] and letj ∈ [ni/2]. With slight abuse
of notation, we denote by w(t) the value of the weight w(i,j) at iteration t, and denote v := v(i,j)
and gt ：= gw(t),v. Recall that we defined Ψ(x) = (ξι ∙ xi ,...,ξ^ ∙ Xni) for ξι... ξm ∈ {土1}.
Denote D(i) := ψ(D(i)) the distribution of (ψ(X),y), where (X,y)〜D(i). Let Y := γi-ι,j, and let
Y such that e(χ1,χ2]= γ(ξ2j-1 ∙ X1,ξ2j∙ X2). Forevery P ∈ {±1}2,denote P := (ξ2j-1P1, ξ2jP2),
so we have γ(pe) = γe(p). Then we have the following:
Lemma 7. Fix some P ∈ {±1}2 such that (i, j, Pe) ∈/ Pee. For every l ∈ [k] such that hwl(t), Pi > 0
and gt(P) ∈ (-1, 1), the following holds:
-ee(P)VlVjhd[D) ,Pi > √2e min{∆, 2β}
∂wl	ni
12
Under review as a conference paper at ICLR 2020
Proof. Observe the following:
IDi) (P(BW⑸,VB)= %,y)~D(i)
∂	2 ni/2
'0(P(BW(i),V(i" (X)) ∙ ~~(t) — EgW(i,j0),v(i,j0) (X2j0-1,x2j0 )
∂ wl	ni j0=1
-y∖ gt(χ2j-ι,χ2j)
∂ wl
n E(χ,y)〜D⑸
ni
2	〜
n	(x,y)~D(i)
[-yvιl{gt(χ2j-ι,χ2j) ∈ (-1,1)} ∙ l{hw(t), (x2j-1,X2j)i > 0} ∙ (x2j-1,X2j)]
We use the fact that '0(P(BW(i),v⑸)(x)) = -y, unless P(BW(i),v⑸)(x) ∈ {±1},in which case
gt(x2j-ι,X2j) ∈ {±1}, so -^(t)gt(x2j-ι,X2j) = 0. FixSomeP ∈ {±1}2 SUchthathw(t),Pi > 0.
∂wl
Note that for every P 6= P0 ∈ {±1}2 we have either hP, P0i = 0, or P = -P0 in which case
hwl(t) , P0i < 0. Therefore, we get the following:
h a Dt)) , pi = nE(x,y)~D(i) h-yvl1{gt(x2j-1,x2j) ∈ (-1, 1)} ∙ 1{hw(t), (x2j-1,x2j)i ≥ 0} ∙ h(x2j-1,x2j), Pii
2
=—E(χ,y)〜D(i) [-yvl1{gt(x2j-1,x2j ) ∈ (-1, 1)} ∙ 1{(x2j-1,x2j) = P} kpk]
ni	,
Denote qp ：= P(x,y)〜D(i) [(x2j-1, X2j) = P∣γ(x2j-1, X2j) = γ(p)]. Using property 2, we have:
P(x,y)〜D(i) [(x2j-1,x2j ) = Plry = y0]
=P(x,y)〜D(i) [(x2j-1,x2j) = P,y = y0,Y(x2j-1,x2j) = Y(P)]
=P(x,y)〜D(i) [(x2j-1,x2j) = P,y = y0|Y(x2j-1 ,x2j) = Y(P)] P(x,y)〜D(i) [γ(x2j-1,x2j) = Y(P)]
=qpP(x,y)〜D(i) [γ(χ2j-ι, χ2j) = γ(P), y = y0]
=qPP(z,y)~D(iT) [zj = Y(P), y = y0]
And therefore:
E(x,y)〜D(i) [y1{(x2j-1,x2j) = PH= E y0P(x,y)〜D(i) [(x2j-1,x2j) = P,V = y]
y0∈{±1}
=qp	yP(z,y)〜D(iT) [zj = Y(P),y = y0]
y0∈{±1}
=qpE(z,y)〜D(iT) [y1{zj = Y(P)H
Assuming gt(P) ∈ (-1, 1), using the above we get:
h(t) , Pi = n E(x,y)〜D(i) [-y1{(x2j-1, x2j) =P}]
=	^E(x,y)〜D(i) [-y1{(ξ2j-1x2j-1, ξ2j x2j ) = PH
ni
=--"2 E(x,y)〜D(i) [y1{(x2j-1,x2j) = Pe}]
ni
=-2√2vιqpE(z,y)〜D(i-i) [y1{zj = Y(P)H
Now, we have the following cases:
13
Under review as a conference paper at ICLR 2020
• If Ii-1,j = 0, then by property 1 zj and y are independent, so:
h dLD(i) Pi = h ∂w(t , Pi	-—26：Aq E(z,y)〜D(i-1) [y1{zj' = e(P)}] ni ---"2vlq E(%y)〜D(i-1) [y] P(z,y)〜D(i-1) [zj = e(P)] ni 二	E(z,y)~D(i-1) [y] P(z,y)~D(i-1) [(X2j-1,x2j) = pe] ni
Since we assume γe(p) = sign(E [y]), and using the fact that (i, j, pe) ∈/ P, we get that:
-e(P)VlVjh dL*, Pi = ∂wl • Otherwise, observe that:	∂L 二-Sign(E [y])vlh —D^, Pi ∂wl(t) 2√2 E …K	「,	、〜2√2c 二——IE [y]I P(z,y)〜D(iT)[(X2j-1, x2j) = pe] > Hee ni	ni
h dLDi pi = h ∂w(t , pi	-2√2vl% E(z,y)〜D(i-1) [y1{zj = e(P)}] ni
=	-S"1 % E(z,y)〜D(iT) y I(Zj ∙ e(P) + 1) ni	2
=	--~ l每(e(P)Ci-1,j + E(z,y)〜D(i-1) [y]) ni
And therefore, using property 1, since Ii-1,j 6= 0, we get:
-e(p)vι Sign(Ci-ι,j)hHILDi,pi = 2qe( (Ici-ι,jI +sign(Ci-ι,j)e(p)E [y])
∂wl	ni
≥√2qpe (Ici-ι,jI-IE [y]∣) > 苧∆
ni	ni
where we use the fact that (i, j, pe) ∈/ P.
□
We introduce the following notation: for a sample S ⊆ X0 × Y, and some function f : X0 → X0,
denote by f(S) the sample f(S) := {(f (x), y)}(x,y)∈S.
Lemma 8. Fix δ > 0. Assume we sample S 〜 D, with ⑸ > 声 min{2∆ 2β}2 log 4∙ Then, With
probability at least 1 - δ, for every p ∈ {±1}2 such that hwl(t),pi > 0 it holds that:
h 3, pi-h *,pi∣≤ 2√ni mw, 2β}
14
Under review as a conference paper at ICLR 2020
Proof. Fix some p ∈ {±1}2 with hwl(t) , pi > 0. Similar to what we previously showed, we get
that:
h皿⑸Pi
h ∂w(t) , Pi
2	(t)
nE(x,y)〜Ψ(S)卜yVl1{gt(x2j-1,X2j) ∈ ( — 1, 1)} ∙ 1{hw1 , (^2j-1,^2j)i ≥。卜〈(必j-1, ^2j), P〉]
2
-E(x,y)〜Ψ(S) [-yvl1{gt(x2j-1,x2j) ∈ (-1, 1)} ∙ 1{(x2j-1,x2j) = P} ∣∣P∣∣]
ni
2√2τπ	r 「/	、/、、「/	、	「
----E(x,y)〜Ψ(S) [-yVl1{gt(x2j-1,X2j) ∈ ( — 1, 1)}• 1{(讶2j-1, ^2j) = p}]
ni
Denote f(x,y) = -yvι1{gt(χ2j-ι,χ2j) ∈ ( — 1,1)} ∙ I{(χ2j-1,χ2j) = p}, and notice that
f(x, y) ∈ [—1, 1]. Now, from Hoeffding’s inequality we get that:
PS [∣Eψ(s) [f(x,y)] — Eψ(d) [f (x,y)]∣ ≥ τ] ≤ exp (-1 |S∣τ2)
So, for ∣S∣ > T log 4 We get that with probability at least 1 一 4 We have:
dLψ(D)	dLψ(s)	_ 2√2 1	I	2√2
h n (t) , Pi 一 h n (t) , Pi =	IEΨ(S) [f (X,y)] — EΨ(D) [f (X,y)]| < ——τ
∣ ∂wl	∂wl	∣	ni	ni
Taking T = j min{∆, 2β} and using the union bound over all P ∈ {±1}2 completes the proof. □
Lemma 9. Fix δ > 0. Assume P(χ,y)〜D [x ∈ XeJ < 8√2n min{∆, 2β}. Assume we sample
S 〜D, with ∣S∣ > j2 min{∆ 2β}2 log 4. Then，With probability at least 1 一 δ ,for every P ∈ {±1}2
such that (i, j, Pe) ∈/ Pej0, and for every l ∈ [k] such that hwl(t), Pi > 0 and gt (P) ∈ (—1, 1), the
following holds:
-e(P)vi Vjh d∂WF,Pi > 总 min。, 2β}
Proof. Denote α
folloWing:
dLD(i)
∂ WSt
dLΨ(D)
∂w(t)
P(χ,y)〜D [x ∈ XeJ, and denote B⑶：=BW⑸,V (i). Then, We have the
二'(PB⑴。Ψ ◦ Γ(i+i)...d(x)) — 3'(PB⑴◦ Ψ(x)) I
∂wl	∂wl
高 '(PB⑺。Ψ ◦ Γ(i+i)...d(x)) — 焉 '(PB ⑴◦ Ψ(x))j ∙ l{x ∈ XT O
≤ E(x,y)〜D [llψ。r(i+1)…d(X)(2j-1,2j) - ψ(X)(2j-1,2j)∣∣ , 1 {x ∈ Xj0}]
≤ 2√2P(χ,y)〜D IX ∈ XJ = 2√2α
So We get, using Lemma 7 and Lemma 8, With probability at least 1 一 δ:
一ee(P)VIVjh dLψ((S), Pi = -e(P)VIVj ( h dLD)) , Pi + h
∂wl	∂wl
∂LΨ(S)
, Pi — h
∂LΨ(D)
, Pi + h
∂LΨ(D)
≥
—e(P)VIVjh dLDi), Pi 一
∂wl
h ∂Lψ≡ , Pi-h
∂wl
∂LΨ(D)
∂ w(t)
, Pi∣∣ — ∣∣h
∂w(t)
∂LΨ(D)
∂ w(t)
, Pi —
, Pi —
h ∂⅛, Pi
∂wl
h dLDi Pi
h ∂w(t) , Pi
>
≥
巫 min{∆, 2β} —	^	min{∆, 2β} 一
ni	2 2ni
点一 min{∆, 2β} 一 4α
2 2ni
dLD(i)	dLΨ(D)
∂w(t)	∂w(t)
kPk
So for α < 8√jn min{∆, 2β} we get the required.
□
15
Under review as a conference paper at ICLR 2020
We want to show that if the value ofgt gets “stuck”, then it recovered the value of the gate, multiplied
by the correlation ci-1,j. We do this by observing the dynamics of hwl(t) , pi. In most cases, its
value moves in the right direction, except for a small set that oscillates around zero. This set is the
following:
At = [(l,p)	: (i,j,p)	∈ Pe ∧ e(p)vιVj	< 0 ∧hw(t),Pi	≤	4η	∧ (e(-p)vιVj	< 0 ∨	(i,j,-p)	∈
ni
We have the following simple observation:
Lemma 10. With the assumptions of Lemma 9, with probability at least 1 - δ, for every t we have:
At ⊆ At+1.
Proof. Fix some (l,P) ∈ At, and We need to show that hw(t+1),Pi ≤ 4η. If hw(t),Pi = 0 then4
hw(t+1), Pi = hw(t), Pi ≤ n and we are done. If hw(t), Pi > 0 then, since (i,j, P / Pe we have
from Lemma 9, w.p at least 1 - δ:
-hdLψ(S), Pi < e(P)vιVj-7∣- min{∆, 2β} < 0
∂wl	2ni
Where we use the fact that γe(P)vlνj < 0. Therefore, we get:
hw(t+1), Pi = hw(t), Pi - IrIh dLψ((S), Pi ≤ hw(t), Pi ≤ 4η
Otherwise, we have hwl(t), Pi < 0, so:
hw(t+1),Pi = hw(t),Pi - ηhdLψ(S),Pi ≤ hw(t),Pi + 4η ≤ 4η
∂wl	ni	ni
□
Now, we want to show that all hwl(t), Pi with (l, P) ∈/ At and (i, j, Pe) ∈/ Pee move in the direction of
Y(P) ∙ νj:
Lemma 11. With the assumptions of Lemma 9, with probability at least 1 - δ, for every l,t and
P ∈ {±1}2 such that hwl(t), Pi > 0, (i, j, Pe) ∈/ Pee and (l, P) ∈/ At, it holds that:
(σ(hw(t),Pi) - σ(hw(tτ),Pi)) ∙ e(P)vιVj ≥ 0
Proof. Assume the result of Lemma 9 holds (this happens with probability at least 1 -δ). We cannot
have hwl(t-1), Pi = 0, since otherwise we would have hwl(t), Pi = 0, contradicting the assumption.
If hwl(t-1),Pi > 0, since we require hwl(t), Pi > 0 we get that:
σ(hw(t),Pi) - σ(hw(tτ),Pi) = hw(t) - w(t-1),Pi = -ηh 匕(S)),Pi
∂wl
and the required follows from Lemma 9. Otherwise, we have hwl(t1), Pi < 0. We observe the
following cases:
•	If γe(P)vlνj ≥ 0 then we are done, since:
σ(hwl(t),Pi) - σ(hwl(t1)
Pi)) ∙ e(P)Vj = σ(hw(t),Pi) G(P)VlVj ≥ 0
4Formally, there is no gradient, but we’ll just take the sub-gradient zero.
16
Under review as a conference paper at ICLR 2020
•	Otherwise, we have γe(p)vlνj < 0. We also have:
hw(t),pi = hw(t-1),pi - ηhdLψ((S),pi ≤ hw(t-1),pi + 4η ≤ 4η
∂wl	ni	ni
Since we assume (l, p) ∈/ At, we must have (i, j, -pe) ∈/ P and γe(-p)vlνj ≥ 0. There-
fore, from Lemma 9 we get:
dLψ(S)
h Iw^
,-pi <-e(-P)VlVj 总 mm。, 2β}
And hence:
0 <	hw(t),pi	=	hw(t-1),pi	+ ηh	dLψ(S)),	-pi	≤ -ηe(-p)vιVjmin{∆, 2β} < 0
∂wl(t-1)	2ni
and we reach a contradiction.
□
From the above, we get the following:
Corollary 3. With the assumptions of Lemma 9, with probability at least 1 - δ, for every l,t and
p ∈ {±1}2 such that hwl(t), pi > 0, (i, j, pe) ∈/ Pe and (l, p) ∈/ At, the following holds:
(σ(hw(t), pi) - σ(hw(0), Pi)) ∙e(P)VIVj ≥ 0
Proof. Notice that for every t0 ≤ t we have (l, p) ∈/ At0 ⊆ At. Therefore, using the previous
lemma:
(σ(hw(t),pi) - σ(hw(0),pi)) ∙ e(p)vινj = X (σ(hw(t),pi) - σ(hw(e∖pi)) ∙ e(P)VIVj ≥ 0
1≤t0≤t
□
Finally, we need to show that there are some “good” neurons, that are moving strictly away from
zero:
Lemma 12. Fix δ > 0. Assume P(χ,y)〜D [x ∈ Xe/ < ξ√^ min{∆, 2β}. Assume we sample
S 〜D, with |S| > 声 min{∆ 2β}2 log 4. Assume that k ≥ log-1( 3) log(4). Then with probability
at least 1 - 2δ, for every p ∈ {±1}2 such that (i, j, pe) ∈/ Pe, there exists l ∈ [k] such that for every
t with gt-1(p) ∈ (-1, 1), we have:
σ(hw(t), pi) f(PyvIVj ≥ ηt^=~ min{∆, 2β}
2ni
Proof. Assume the result of Lemma 9 holds (happens with probability at least 1 - δ). Fix some
p ∈ {±1}2 such that (i,j,p) ∈ Pee. For l ∈ [k], with probability 4 We have both Vl = Y(P)Vj
and hwl(0), pi > 0. Therefore, the probability that there exists l ∈ [k] such that the above holds is
1 - (3)k ≥ 1 - 4. Using the union bound, w.p at least 1 - δ, there exists such l ∈ [k] for every
p ∈ {±1}2. In such case, we have hw(t), pi ≥ ηt√2L- min{∆, 2β}, by induction:
•	For t = 0 this is true since hwl(0), pi > 0.
•	If the above holds for t - 1, then hwl(t-1),pi > 0, and therefore, using Vl = γe(p)Vj and
Lemma 9:
17
Under review as a conference paper at ICLR 2020
-h dLψF, Pi >e(P)VlVj √fc min。,2β}
And we get:
hw(t∖ pi = hw(t-1)，pi - ηh dLψ(D), Pi
∂wl
> hw(t-1), pi + ηγ(p)vlVj-4— min{∆, 2β}
2ni
≥ η(t - 1) -7=— min{∆, 2β} + η~^~ min{∆, 2β}
2ni	2ni
□
Using the above results, We can analyze the behavior of gt(p):
Lemma 13. Assume we initialize w(0) such that ∣∣w(0) ∣∣ ≤ 表. Fix δ > 0. As-
sume P(x,y)〜D [x ∈ Xee，] < 8√2n min{∆, 2β}. Assume we SamPle S 〜 D, with |S| >
声 min{∆ 26产1°g 4 ∙ Assume that k ≥ log-1( 3) log( 4). Then with probability at least 1 一 2δ,
for every p ∈ {±1}2 such that (i, j, pe) ∈/ Pee, fort > √ηe mn{∆,2β} wehave:
gt(p) = γe(p)Vj
Proof. Using Lemma 12, W.p at least 1 - 2δ, for every such p there exists lp ∈ [k] such that for
every t Withgt-1(p) ∈ (-1, 1):
Vlpσ(hw(p),pi) 巧(P)Vj ≥ ηt√2n min{∆，2e}
Assume this holds, and fix some p ∈ {±1}2 With (i, j, pe) ∈/ Pee. Let t, such that gt-1(p) ∈ (-1, 1).
Denote the set of indexes J = {l : hwl(t), pi > 0}. We have the folloWing:
gt(p) = XVlσ(hwl(t),pi)
l∈J
= Vlpσ(hwl(pt),pi) +	X	Vlσ(hwl(t),pi) + X	Vlσ(hwl(t), pi)
l∈J ∖{lp},(l,p)∈At	l∈J∖{lp},(l,p)∈At
From Corollary 3 we have:
e(P)Vj ∙	X	Vlσ(hw(t),pi) ≥ -kσ(hw(0),pi) ≥
l∈J∖{lp },(l,p)∕At
By definition of At and by our assumption on η we have:
1
—
4
1
4
γe(P)νj ∙ X	vlσ(hw(t),Pi) ≥ -k4η ≥ 一
ni
l∈ J ∖{lp},(l,P)∈At
Therefore, we get:
This shows that for t >
ee(p)νj ∙ gt(p) ≥ ηt√n min{∆，2β} - 1
√2ηe mn{∆,2β} We get the required.
□
Proof. of Lemma 6. Using the result of Lemma 13, with union bound over all choices of j ∈ [ni/2].
The required follows by the definition of γe(x2j-1 , x2j) = γi-1,j (ξ2j-1x2j-1, ξ2j x2j), and using
一 _ _ _ ~ —
the definition of Xee	□
18
Under review as a conference paper at ICLR 2020
Proof. of Lemma 1. Fix some i ∈ [d], j ∈ [ni/2],p ∈ {±1}2, y0 ∈ {±1}, such that:
P(x,y)〜D(i) [Yi-1,j (X2j-1,x2j ) = Yi-1,j (P)] > 0
Assume w.l.o.g. that j = 1. Denote by W the set of all possible choices for x3, . . . , xni, such that
when (x1 , x2 ) = p, the resulting label is y0. Formally:
W := {(x3, . . . ,xni) : Γi...d(p1, p2, x3, . . . ,xni) = y0}
Then we get:
PD(i) [(x1,x2) = p,y = y0, γi-1,j (x1, x2) = γi-1,j(p)]
= PD(i)	[(x1,x2)	=p, (x3, . . . ,xni) ∈ W,γi-1,j(x1,x2)	= γi-1,j(p)]
=PD(i)	[(X1,X2)	= P,Yi-1,j (X1,X2) = Yi-1,j (P)] ∙ PD(i)	[(X3, ...,Xni)	∈ W ]
=PD(i)	[(X1,x2)	= P|Yi-1,j (X1,x2) = Yi-1,j (P)] ∙ PD(i)	[Yi-1,j (x1,x2)	= Yi-1,j (P), (x3,	.	.	.,xm ) ∈ W]
=PD(i)	[(x1,x2)	= P|Yi-1,j (x1,x2) = Yi-1,j (P)] ∙ PD(i)	[y = y0,Yi-1,j (X1,x2)	= Yi-1,j (P)]
And dividing by PD(i) [γi-1,j(X1, X2) = γi-1,j (P)] gives the required.
□
C Proofs of Section 5
Proof. of Lemma 2.
For every gate (i, j), let Ji,j be the subset of leaves in the binary tree whose root is the node (i, j).
Namely, Ji,j := {(j - 1)2d-i + 1, . . . , j2d-i}. Then we show inductively that for an input x ∈
{±1}n, the (i, j) gate outputs: Ql∈I∩Ji,j Xl:
•	For i = d - 1, this is immediate from the definition of the gate γd-1,j.
•	Assume the above is true for some i and we will show this for i - 1. By definition of the
circuit, the output of the (i - 1, j ) gate is a product of the output of its inputs from the
previous layers, the gates (i, 2j - 1), (i, 2j). By the inductive assumption, we get that the
output of the (i - 1, j) gate is therefore:
I Y	χι I ∙ (Y	Xl)=	Y	χι = Y χι
l∈Ji,2j-1∩I	l∈Ji,2j∩I	l∈(Ji,j2-1∪Ji,2j)∩I	l∈Ji-1,j
From the above, the output of the target circuit is Ql∈jo i∩i Xl = Ql∈∕ Xl, as required.	□
Proof. of Lemma 3.
By definition we have:
Ci,j = E(χ,y)〜D [Γ(i+1)…d(x)jy] = E(χ,y)〜D [Γ(i+1)…d(x)jy] = E(χ,y)〜D [Γ(i+i)...d(x)jXi …Xk]
Since we require Ii,j 6= 0, then we cannot have Γ(i+1)...d(x)j ≡ 1. So, from what we showed
previously, it follows that Γ(i+i)…d(xj = Qjo∈∕o Xj0 for some 0 = I0 ⊆ I. Therefore, we get that:
ci,j = ED Y Xj0 = Y ED [Xj0] = Y (2Pj0 - 1)
j0∈I∖I0	_|	j0∈I∖I0	j0∈I∖I0
Furthermore, we have that:
ED [y] = ED Y Xj0 = Y ED [Xj0] = Y (2pj0 — 1)
j0∈I	j0∈I	j0∈I
19
Under review as a conference paper at ICLR 2020
And using the assumption on Pj we get:
∖ci,jITED [y]∣ =	∏	I2Pj0 - 1∣- ∏ I2Pj0 - 1I
j0∈[k]∖I0	j0∈[k]
Y	∖2pj0 - 1H (1 - Y ∖2pj0 -1∖
j0∈[k]∖I0	j ∖	j0∈I0
≥ I Y	∣2pj0-1∣) (1-(1- 2ξ)lI1)
j∈[k]∖I0	)
≥ (2ξ)k-∣10I (1 - (1 - 2ξ)) ≥ (2ξ)k
Now, for the second result, we have:
P(Zy)〜Γi...d(D) [zj = 1]= E(x,y)〜D [1{Γchi)…d(x)j = 1}
=E(χ,y)〜D 1( Y xj0 + 1)
_ j0∈I0	_
=2 Y E(χ,y)~D [xj0] + 2
j0∈I0
And so we get:
P(z,y)〜Γ...d(D) [zj = 1] - 1 = 2 Y IE(x,y)〜D [xj0]∣
j0∈I0
< 2(I- 2ξ)∣ι'∣ ≤ 2- ξ
□
Proof. of Lemma 4 For every i ∈ [d] and j ∈ [2i], denote the following:
p+j = P(x,y)〜D(i)[xj = 1Iy = 1] , pi,j = P(x,y)〜D(i)[xj = 1Iy = -1]
Denote D(i)∣z the distribution D(i) conditioned on some fixed value Z sampled from D(i-1). We
prove by induction on i that ∣p+j - p--1 = (∣)i:
•	For i = 0 we have p+j = 1 and p- = 0, so the required holds.
Ci—1
•	Assume the claim is true for i - 1, and notice that we have for every Z ∈ {±1}∣
P(x,y)〜D(i) [xj = 1∣y = 1] = P(x,y)〜D(i)∣z [xj = 1∣zj7∣] = 1] - P(Z ,y)〜Dd [zj∕∣] = 1∣y = 1]
+ P(X,y)〜D(i)|z [xj = 1∣zdj7∣] = -1] ∙ P(Z , y)〜D(iT) [zj/l] = -1∣y = 1]
{pM,j7∣] + 3(1 - Pi-I,「j/2])if γi-1,j∕∣] = ∧
IPi-I,dj/2]	if YiT,j7∣] = ∨
3Pi-I,「j/2] + (1 - Pi-I,「j/2]) if γi-ι,j7∣] = -∧
I (1 - Pi-I,dj/I])	if YiT,「j/I] = -V
Ij,+	_ 1
3 Pi-1,「j/I]	3
3 Pi-I,「j/I]
1 - 3Pi-i,「j/I]
I _ ∣n+
3 - 3Pi-1,「j/I]
if Yi-1,「j/2] = ∧
if Yi-1,dj∕∣] = V
if Yi-1,dj∕∣] = -∧
if Yi-1,dj∕∣] = -V
20
Under review as a conference paper at ICLR 2020
Similarly, we get that:
3PirI,j/2] - 3
3 Pi-1,j∕2]
1	- 3Pi-1,j∕2]
2	— 2 P--
3 3 Pi-1,j∕2]
if Yi-1,dj∕2] = ∧
if Yi-1,j∕2] = V
if Yi-1,dj∕2] = -∧
if Yi-1,dj∕2] = -V
Therefore, we get:
l +	— l 2l +	—
晡—P-j| = 3 K-I,「j/21 — Pi-1,「j/2]
I=(2 )i
From this, we get:
IE(x,y)〜D⑸ [xjy] I = IE(x,y)〜D(i) [(21{xj = 11 - 1)y] |
=|2E(x,y)〜D(i) [1{xj = 1}y] - e [y] i
=I2 (PD(i)[xj = 1,y = 1] 一 PD(i)[xj = 1,y = -1]) - E [y]l
=I 2 (p+∙P [y = 1] - P-jP [y = -1]) - E [y] ∣
=2 (2(P+j- P-,j)+ ξ(P+ + P-j)) - E [y]
≥ 1 P+ - P- 1 - 2ξ i P+ + P-,j i - lE [y]|
≥ i P+- P-/- 6ξ> 2 (3)
And hence:
I E(x,y)〜D⑸[xjy] | - 1 E(x,y)〜D⑸[y] 1 ≥ | (3) - 2ξ > | (3)
□
Proof. of Lemma 5 Fix some z! ∈ {i1}ni/2 and y0 ∈ {±1}. Then we have:
P(x,y)〜Γi(D(i)) [(x,y) = (ZIy0)] = P(x,y)〜。⑶[C(X),j) = (ZO")]
=P(x,y)〜D(i) [∀j γi-1,j (x2j-1,x2j ) = Zj and y = y ]
=P(z,y)〜D(i-1) [(Z, y) = (Zl y O)]
By the definitions of D(i) and D(i-1).
□
21