Under review as a conference paper at ICLR 2020
LocalGAN: Modeling Local Distributions for
Adversarial Response Generation
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a new methodology for modeling the local semantic distribu-
tion of responses to a given query in the human-conversation corpus, and on this
basis, explores a specified adversarial learning mechanism for training Neural Re-
sponse Generation (NRG) models to build conversational agents. The proposed
mechanism aims to address the training instability problem and improve the qual-
ity of generated results of Generative Adversarial Nets (GAN) in their utilizations
in the response generation scenario. Our investigation begins with the thorough
discussions upon the objective function brought by general GAN architectures
to NRG models, and the training instability problem is proved to be ascribed to
the special local distributions of conversational corpora. Consequently, an en-
ergy function is employed to estimate the status of a local area restricted by the
query and its responses in the semantic space, and the mathematical approxima-
tion of this energy-based distribution is finally found. Building on this foundation,
a local distribution oriented objective is proposed and combined with the original
objective, working as a hybrid loss for the adversarial training of response genera-
tion models, named as LocalGAN. Our experimental results demonstrate that the
reasonable local distribution modeling of the query-response corpus is of great
importance to adversarial NRG, and our proposed LocalGAN is promising for
improving both the training stability and the quality of generated results.
1	Introduction
End-to-End generative conversational agents (a.k.a., generative Chat-bots) are believed to be prac-
ticable on the basis of the Sequence-to-Sequence (Seq2Seq) architecture (Sutskever et al., 2014)
trained with large amounts of human-generated conversation sessions (Shang et al., 2015a; Sordoni
et al., 2015), and this task is named as Neural Response Generation (NRG). Similar to the Neural
Machine Translation (NMT) approaches (Bahdanau et al., 2014; Wu et al., 2016), the deep Seq2Seq
models are expected to directly generate appropriate and meaningful responses according to the in-
put query. Compared to the success of NMT systems, the application progress of NRG models is
not satisfying at present due to the “safe response” problem (Li et al., 2016). That is, most of the
generated responses are boring and meaningless, which blocks the continuation of conversations.
Indeed, eliminating “safe responses” is the essential task of NRG models. Thus, various methods
have been considered to address this problem (Li et al., 2016; Xu et al., 2017; Li et al., 2017; Xing
et al., 2017; Pandey et al., 2018; Zhang et al., 2018a; Du et al., 2018).
More recently, Generative Adversarial Nets (GAN) (Goodfellow et al., 2014) have been introduced
to eliminate “safe responses” (Li et al., 2017; Xu et al., 2017). Basically, this methodology is
reasonable since the GAN framework involves an adversarial discriminator that helps NRG models
leap out of the shortsighted state of minimizing the empirical risk on word distribution, by providing
feedback on real samples from the model generated ones. Despite the improvement on the diversity,
the adversarial training process of GAN based response generation models is generally unstable and
sensitive to the training strategy (Yu et al., 2017).
The unstable convergence problem is largely ascribed to the complicated data distribution in practi-
cal scenarios (Arora et al., 2017; Arora & Zhang, 2017). For the response generation oriented GAN
models, in particular, the data distribution appears to be much more complicated. Fundamentally,
an essential characteristic of conversation data is that, for each given query, there always exists a
1
Under review as a conference paper at ICLR 2020
group of semantically-diverse responses, rather than the semantically-unified ones. Furthermore,
the response groups of two different queries tend to keep great divergences in the semantic space.
In this scenario, the discriminator needs to consider the distributions of the generated result in the
semantic space, rather than simply examining whether one single sample comes from the generator
or the original dataset, so as to make the generator sense the distribution of conversation dataset in
the adversarial training procedure.
This paper aims at presenting a specific adversarial training schema for neural response generation.
Beginning with the investigation on the reason for the unsatisfying performance of GANs on the
NRG task, we find the upper-bound of the current GAN learning strategies taking query-response
pairs as the independent training samples. On this basis, we claim that the training schema, including
the actual adversarial strategy and the overall loss function, should be re-defined to agree with the
distribution of NRG training samples in the semantic space, rather than roughly adopting the GAN
framework designed for generating images.
Consequently, we describe the distributional state of the given query and the corresponding re-
sponses with the free energy defined on the basis of Deep Boltzmann Machines (DBM) (Salakhutdi-
nov & Hinton, 2009). In this way, we can quantify the formation process of generating the response
set with the topic restriction of the given query. From the perspective of free energy, this paper
proposes a new cost function to measure the expansion degree of the responses in the local area
of the real-valued semantic space. Cooperating with the traditional implicit density discriminating
loss of GAN, the proposed cost actually provides an explicit density approximation for the local
distribution of each response cluster. Thus, the adversarial learning procedure can be expected to be
more stable with better response generation results obtained1.
2	The limitation of General GAN in the NRG Scenario
According to (Goodfellow et al., 2014), the standard GAN framework contains a generator G and
a discriminator D, which are trained by an iterative adversarial learning procedure based on the
following objective function:
J(D)= Eχ~pd [logD(x)] + Ez~pz [log(1 - D(G(Z)))]	(1)
J(G)= Ez~pz[log D(G(Z))]	⑵
where pz denotes the prior on input noise variables, and pd is the true data distribution.
It should be noted that the GAN tries to learn the manifold of a given dataset (Khayatkhoei et al.,
2018; Kumar et al., 2017), and the discriminator D actually provides a metric for judging whether
the results generated according to Z fits the expected manifold or not. In the NRG scenario, a
naive Seq2Seq model without the guidance signal from D can not capture the data manifold of
the real query-response corpus, which is one of the major facts the safe-response problem can be
ascribed to. Assuming that there exists an oracle discriminator with the ability of distinguishing the
generated fake samples from the ground-truth ones, by mapping each query-response pair (q, r)2 to
a confidence score s, and meanwhile, it can be assume that any practically existing discriminator of
GAN gives the confidence S to (q, r). If the practical discriminator can make S → s, the generator
will obtain more meaningful guidance for the better generation. That is, to improve the capability
of GAN-NRG, it is wise to construct more powerful discriminators for more reasonable J(G).
Now let’s pay attention to the actual change of NRG models brought by GAN. In the generative
conversation agent scenario, G(Z) is corresponding to a generated response rS to a given query q.
Thus, the objective of the generator in the GAN based NRG model can be simply formulated as:
J(G) = %,r)~pg [logD(q,r)]	⑶
The training of the generator is actually the procedure to maximize J(G) toward
E(q,r)~pd [log D(q,r)], So as to generate realistic responses according to given queries. Thus, in
1 The code of our proposed model LocalGAN can be found in https://github.com/Kramgasse49/
local_gan_4generation
2Here q and r represent the vectorized query and its response. We take the simple embedding-averaging
based method to transform texts into vectors. Besides, due to our adopted text vectorizing method, the pre-
training phase of the Deep Boltzmann Machine takes some specified trick detailed in Appendix A.
2
Under review as a conference paper at ICLR 2020
the context of adversarial learning, J(G) should satisfy the following inequality:
J(G) 6 E(q,r)~pd [logD(q,r)]	(4)
However, it is well known that a conversational dataset should not be simply taken as a collection
{(q, r)} composed of independent query-response pairs. Instead, to each given query q, there exists
a finite set of corresponding responses Rq = {ri}. In this case, it is of great necessity to consider
the whole training dataset as a collection of Rq, which takes the form of a number of clusters with
their own local distributions in the semantic space. And we can rewrite the joint distribution pd
in (4) as p(q)p(r|q) and assume every corresponding response to a query follows equal-probability
distribution, which means that p(r∣q) = ∣R1γ. Thus, in the real NRG scenario, on the basis of
Equation 3 and Inequation 4, J(G) follows the inequality as below:
J(G) 6 £E(q,r)~q,Rq [log Dmr)]
q
=XXp(q) ∣R1-∣ [logD(q,r)] 6 XPg)IOg ∣⅛ X D(q, r)
q r∈Rq	q	q	q r∈Rq
(5)
where p(q ) denotes the probability of the query q, and Rq is defined above.
According to Equation 5, the upper bound of J(G) is obtained, in which the
log [ply Pr∈Rq D(q,r)] part is the essence. Apparently, the expression	Pr∈Rq D(q,r)
indicates the mean value of the confidence scores given by the discriminator to each member of the
response set Rq to a given query q. Moreover, it should be noted that current studies tend to utilize
semantic relevance oriented models to build the discriminators of GAN (Xu et al., 2017; Li et al.,
2017). Consequently, D(q, r) can be actually considered as the spatial relationship of q and r in the
semantic space. Thus,	Pr∈Rq D(q, r) stands for the spatial center of all the responses within
Rq. That is, the optimization process of adversarial learning upon conversational datasets will make
the generated responses approach to the center of each local distribution of Rq corresponding to
each given dependent query.
The practical value of this change lies in that, intuitively, the GAN architecture forces the generation
to pay attention to the local distributions of the individual response clusters, rather than taking the
(q, r)-pairs as an entirety. According to the thorough studies on the safe responses of NRG mod-
els (Li et al., 2016; Xu et al., 2017; Zhang et al., 2018a; Pandey et al., 2018), it can be inferred that
the general Seq2Seq will fall into the divergence state of generating the patterns with the maximum
probabilities taking account of the entire dataset, ignoring the individual-difference of each query.
By introducing the implicit loss focusing on the response clusters, GAN makes the divergence of
the generator much closer to the ‘local patterns’ rather than the general patterns, and thus the higher
diversity can be expected and observed (Xu et al., 2017; Li et al., 2017).
The problem turns to: Is the upper bound in Equation 5 powerful enough? Apparently, there exists
an obvious gap between the ‘local patterns’ and the vivid and interesting generated responses. The
upper bound only focuses on the mean of each response set, but the local distribution (or the actual
‘shape’) of each cluster has not been taken into account. This situation does not change when the
cost function of adversarial training is defined by Wasserstein GANs (WGAN) (Arjovsky et al.,
2017) with 1-Lipschitz function f :
JWG)= E(q,r)~pg [f (q, r)]
(6)
Intuitively, it is of paramount importance to estimate both the ‘location’ and the ‘shape’ of the
response set Rq in the semantic space (indeed, R^ Pq 丁 D(q, r) is only relative with ‘location,),
so as to determine the optimization objective of adversarial training. Consequently, we have two
critical problems to be discussed and addressed in the following sections:
•	How to describe the state of the response set Rq with a given query q in the semantic space?
•	Taking account of the reasonable state modeling of (q, Rq), what is the loss function for
adversarial training to generate responses?
3
Under review as a conference paper at ICLR 2020
3 Modeling the State of the Local Distribution for Responses
As mentioned above, the semantic one-to-many relationship between queries and responses makes
it necessary to model the local distribution of the response cluster Rq corresponding to each query
in the space, and it is paramount to turn to the fitting of each local distribution in the adversarial
learning procedure, rather than considering each (q, r)-pair as an independent sample. Basically,
this issue equals to the task of reasonably modeling the state of (q, Rq) in the semantic space,
by considering each (q, Rq) as a systematic entirety and assigning the state of the entirety with
probabilistic distribution. The additional major challenge of this task is, indeed, we have to infer the
state of a local area in the semantic space from a group of finite samples, since it is impossible to
sample all the possible responses to a query from the given corpus, regardless of the corpus size.
3.1	Representing Local Distributions with Query-Response oriented Free
Energy
In this part, we typically take an energy based statistical model, the Average Free Energy (Hinton &
Zemel, 1994; Friston et al., 2006; Ngiam et al., 2011; Friston et al., 2012), to describe the state of the
local distribution of (q, Rq) in the semantic space, for the reasons that: a) energy based models are
considered as a promising avenue towards learning explicit generative models (LeCun et al., 2006;
Le Roux & Bengio, 2008), by representing data distributions without any prior assumptions; and
b) energy based models can be trained in the unsupervised way, and the energy functions of such
models have the potential to estimate the state of generative models (Zhao et al., 2016).
At first, the free energy of a given query-response pair (q, r) can be defined as following:
F(q,r) = -log	exp(E(q, r, H))
H
(7)
where E(q, r, H) stands for the energy function defined according to the relationship of the query q
and its response r via the hidden variable H .
We employ the Deep Boltzmann Machine
(DBM) (Salakhutdinov & Larochelle, 2010; Smolensky,
1986; Hinton & Salakhutdinov, 2006) to implement
E(q, r, H), as illustrated by Figure 1. The reason for
this choice lies in that, from the view of conversational
agents, the meaningful query and the corresponding
response are generally considered to maintain strong
semantic relevance. Thus, the query and response can be
mutually transformed into each other, which is supported
by the considerable amount of studies on response gener-
ation (Shang et al., 2015b; Shao et al., 2017; Zhang et al.,
2018a; Baheti et al., 2018) and question generation (Du
et al., 2017; Zhao et al., 2018; Sun et al., 2018). Without
loss of generality, the pairwise semantic relationship of
the query q and the corresponding response r can be
Figure 1: The DBM for modeling
the semantic relationship of Query-
Response pairs.
modeled by a two-layer DBM. The bottom layer is actually an abstract version of Seq2Seq models,
in which a response r can be generated based on a hidden variable hq, and hq depends on the given
query q theoretically. In the top layer, hq conditionally depends on a hyper hidden variable h.
Figure 1 illustrates the Deep Boltzmann Machine for modeling the semantic relationship of a query
and its responses.
Following (Salakhutdinov & Hinton, 2009), on the basis of the DBM in Figure 1, the energy of the
state {(q, r), H} is defined as:
E(q, r, H) = E (q, r, hq, h) = -hqTWqrr - hqT Wqq q - hT Wqhhq	(8)
where q denotes the query, r stands for the response and H = {hq , h} represents the hidden units.
Wqr, Wqq and Wqh stand for the weights on the corresponding connections of the query, response
and the hidden variables respectively in the graph model shown by Figure 1.
4
Under review as a conference paper at ICLR 2020
Consequently, we can define the average free energy of the query q and its response set Rq as
follows:
F(q,Rq)
∣Rq∣ rXXR F(q,ri)
ri ∈ q
(9)
For better conducting the following discussion, we further define the energy difference between
response ri and rj as:
Definition 3.1 (Scaled Energy Difference).
∆q,ri,rj
F(q,ri) - F(q,r)
F(q, Rq)
(10)
Meanwhile, it is necessary to assign a spatial intuition to Rq in the semantic space by defining:
Definition 3.2 (Response Cluster). In the semantic space, the meaningful responses to the given
query q lie in a restricted region (e.g., a hyper sphere), which can be named as the Response Cluster,
in which F(q, r) can be taken as the distance from a response r to the cluster center.
3.2	ESTIMATION OF F(q, Rq)
Basically, the DBM in Figure 1 provides the definition of the energy function E(q, r, H) of the free
energy given in Equation 8. Thus, the local distribution state of the responses Rq with the given
query q can be mathematically described by F(q, Rq) based on Equation 7 - 9. In practice, however,
this procedure is not operable yet because the computation of F(q, Rq) requires all the response in
Rq, and it is impractical to perform the exhaustive enumeration over all the possible responses of
the given query, regardless of the amount of the training query-response pairs. Consequently, it is
highly necessary to approximate F(q, Rq) under some reasonable assumptions.
Considering the response cluster defined in Definition 3.2, we can naturally assume that the response
random variable r to a given query q follows multivariate normal distribution with mean rc and
covariance matrix Σ, where rc and Σ is only determined by query q. Afterwards, the observed Rq
can be considered as a realization of the |Rq|-variate random variable (ri, ∙∙∙ , rR∣) for ri, 1 ≤
i ≤ |Rq|, i.i.d random variables drawn from the distribution N(rc, Σ). Consequently, an executable
approximation of F(q, Rq) can be obtained based on the following Lemmas and Theorem3.
Lemma 1. If E[F(q, r)] < ∞, F(q, Rq) converges almost surely to the expected value E[F(q, r)],
as |Rq| goes to infinity. That is,
P (Jim F (q,Rq ) = E[F (q,r)D = 1.	(11)
|Rq ∣→∞
Lemma 1 indicates that for any > 0 there exists N() such that if |Rq| > N() then the inequality
|F(q, Rq) - E[F (q, r)]| < holds. In other words, F(q, Rq) can be very close to E[F (q, r)] if the
responses are sufficient.
Lemma 2.
lim	|E[f(r)]-f(rc)|=0	(12)
Ekr-rc k2→0
Practically, the expected Euclidean distance between random variable r and rc can not be zero.
Thus, the fact conveyed by Lemma 2 is that, actually, if the expected Euclidean distance is small
enough, the difference between E[F (q, r)] and F(q, rc) can be controllable (or even close to zero).
Based on the lemmas we have:
Theorem 1. Given E[F (q, r)] < ∞, we have
|F(q, Rq) — F(q, rc)| -→ 0 when |Rq| → ∞ and Ekr —鹿口2 → 0	(13)
where ^ is the estimation of rc based on the well-trained DBM.
3The proof of Lemma 1, Lemma 2 and Theorem 1 is given in Appendix B.
5
Under review as a conference paper at ICLR 2020
proof of theorem 1.
|F(q,Rq) - F(q√^c)l ≤ |F(q,Rq) -E[F(q,r)]∣ + |E[F(q,r)] - F(q,rC)l + |F(q,rc) - F(q,rC)l
(14)
Following lemma 1, the first term converges almost surely to zero when |Rq | → ∞.
And following lemma 2, the second term goes to zero when Ekr - rc k2 → 0.
Finally, given the well-trained DBM with the capability of mapping a query q to rc (Wang et al.,
2010; Srivastava & Salakhutdinov, 2012), ^ can be taken as the estimation of r「Meanwhile, since
the function F(q, r) is the composition and combination of simple continuous functions, we have
|F(q,rc)- F(q∕c)l→ 0.	口
4 The Hybrid Loss of Adversarial Response Generation
As discussed in Section 2, the ability of the response generator in the general GAN architecture
is limited to learning the dense distribution around ∣r^ Pr∈Rq D(q, r) (See Equation 5), which is
composed of the most frequent patterns in the semantic space. By contrast, it is difficult for the
general architecture to sense the remaining sparse space containing high-quality diverse responses.
Therefore, reasonably describing the local distribution of the responses to a given query is highly
necessary. According to the analysis in Section 3, the average free energy can be taken to model the
state of the local area of the responses to a query, and such energy can be reasonably approximated
via the DBM defined on the query-response pairs. On the basis of the previous sections, this section
will finally propose the new hybrid loss function to force the generator to produce responses with
better diversity through the more stable adversarial training process.
4.1	The Radial Distribution Function of the Response
The analysis in Section 3 have shown that the local distribution state of the responses Rq to the given
query q can be modeled by the average free energy F(q, Rq). On this basis, it is possible to propose
the description of the spatial state ofa single response r in the semantic space, and consequently, we
can give a new adversarial loss indicating the cost of simulating the local distribution of Rq .
According to Definition 3.1 and 3.2, in each response cluster Rq, the distance from a response r to
the cluster center rc is actually equivalent to the scaled energy difference between them, that is,
∆q,r,rc
F(q,r) - F(q,rC)
F(q,Rq)
(15)
Meanwhile, on the basis of Theorem 1, F(q, Rq) can be approximated by F(q, rc), and ^ is mod-
eled from training data, and thus we have:
人 〜F(q,r) -F(q,rc)	“a、
∆q,r,rc ≈ ----F(q,rc)----- = α(q,r)	(16)
Here we approximate ∆q,r,rc with α(q,r), and formally call α(q,r) as the Radial Distribution Func-
tion (RDF), indicating the relative cost ratio to F(q, rc) for obtaining r from a given q (also the
distinctiveness of r, actually).
4.2	The Hybrid Objective Function
Based on the previous discussions, for the adversarial response generation methodology, the essence
is to reasonably describe the state of the local distribution of the response cluster given by Defini-
tion 3.2, and further more, to take this important element into account in the final optimization.
Especially, in Subsection 4.1, we have defined the Radial Distribution Function in Equation 16 to
quantify the distinctiveness of a response, the very basis of which is the description of the local state
F(q, Rq) in the semantic space. Thus, we can further build a mechanism to quantify the difference
between the generated response and the golden response as follows:
δα = α(q,r) - α(q,r)
(17)
6
Under review as a conference paper at ICLR 2020
where r is the generated response given by the generator and r comes from the original data. If δɑ
moves toward zero, a@r)would be close to a(q,r)sharing the same F(q, rj.
Consequently, a new expectation comes out. That is, the generator needs to provide results that can
minimize δα, so as to fit the local distribution of the existing responses to a given query. Thus, a
hybrid objective of the generator can be finally defined as:
minJG = -E [log D(q, r)] + ReLU (δα)	(18)
A hinge loss, conducted by the ReLU function ReLU (δα) = max(0, δα), is especially introduced
to reform δα. The primary reason of this operation is that the ReLU function has positive output
only if δα > 0, according to the definition of ReLU (δα). Apparently, δα < 0 indicates that the
generated response r is too far from the center of the response cluster in the semantic space, so that
its relevance may be highly questionable. Meanwhile, minimizing a negative variable is against the
optimization direction. After the ReLU transformation, there remains valid loss only when δα > 0,
and thus both the diversity and the relevance of generated results are taken into account.
4.3	The Phase-wise Optimization
According to the analysis in Section 2, the trivial adversarial training directed by -E [log D(q, r)]
can only determine the form of general responses to a given query. From the spatial perspective in
the semantic space, the original adversarial objective is helpful to roughly locate the response cluster
to be generated. However, the local distribution can not be captured by this procedure.
By contrast, according to the discussions above, the proposed hybrid objective actually provides
a way to force the generated responses, originally gathering around the general form, to expand
into the expected local shape described by the golden truth, by conducting a phase-wise optimizing
operation. This mechanism can be detailed in an intuitive way:
Foundation: Once the DBM in Figure 1 is well-trained with the query-response corpus, the semantic
center rc of a Response Cluster can be determined by the given query q.
Phase-1: In the early stage of the adversarial training, a generated response r is not semantically
relevant to the query q. Thus, it can be inferred that r is radially farther from the cluster center %
than the golden response r. In this situation, according to Equation 16 and Equation 17, we can
claim that δα ≤ 0. In this phase, the hyper objective goes back to the general adversarial objective
due to the ReLU function. Thus, the model is trying to force the generated samples to approach the
center of each cluster, ignoring local distributions.
Phase-2: During the adversarial training in Phase-I, the generated result r will go approaching to
the cluster center rc, which means a(q,r)→ 0 . It should be noted that, for any meaningful existing
training sample r, α(q,r) > 0. Therefore, at some point, it turns to δα > 0 and the right part of the
hybrid objective in Equation 18 takes effect. Consequently, for each given query, the distribution of
the generated results will expand to fit the local distribution of the golden samples.
5	Experiments
5.1	Experimental Setups
Datasets. Our experiments are conducted on two main stream open-access conversation corpora:
The Opensubtitles corpus and the Sina Weibo corpus. The OpenSubtitles dataset contains 5,200,000
movie dialogues, where we extract query-response pairs following (Xu et al., 2018; Li et al., 2016).
The Sina Weibo Corpus (Shang et al., 2015a) contains 2,500,000 single-turn Chinese dialogues, in
which the length of the query and response ranges from 4 to 30. We sample 100,000, and 2,000
unique query-response pairs as validation and testing dataset respectively from both of the corpora4.
Baselines. For meaningful comparison, we introduce the following models as the baselines:
(1)	Seq2Seq: a sequence-to-sequence model trained with maximum likelihood estimation (MLE);
4Both the English and the Chinese datasets used in our experiments are uploaded to https://www.
dropbox.com/sh/k8i079gd2111lsb/AACLLtlNAzile543Da8Qs9tFa?dl=0.
7
Under review as a conference paper at ICLR 2020
Table 1: Performances of LocalGAN and Baselines on the OPensUbtitles and Weibo Datasets.
Opensubtitle	Weibo
Model	Dist-1	Dist-2	Ent4	Rel.	Dist-1	Dist-2	Ent4	Rel.
Seq2Seq	0.025	0.081	5.650	1.090	0.055	0.153	6.400	0.315
Seq2Seq-MMI	0.027	0.086	5.698	1.067	0.059	0.172	6.860	0.309
Adver-REGS	0.0296	0.098	5.701	1.113	0.061	0.181	7.658	0.320
GAN-AEL	0.030	0.100	5.733	1.106	0.062	0.183	7.765	0.318
AIM	0.0292	0.095	5.783	1.120	0.064	0.189	7.833	0.321
DAIM	0.031	0.103	5.873	1.098	0.067	0.195	8.042	0.316
LocalGAN	0.036	0.110	6.073	1.132	0.071	0.212	8.561	0.327
(2)	Seq2Seq-MMI: the NRG model with a MaximUm MUtUal Information criterion (Li et al., 2016);
(3)	Adver-REGS: the NRG model trained Using adversarial framework, in which the Policy gradient
was emPloyed to transfer the reward of the discriminator to the generator (Li et al., 2017);
(4)	GAN-AEL: an adversarial framework with an aPProximate embedding layer for connecting the
generator with the discriminator directly (XU et al., 2017).
(5)	AIM / DAIM: the adversarial training strategy allowing distribUtional matching of synthetic and
real resPonses and exPlicitly oPtimizing a variational lower boUnd on Pairwise mUtUal information
between the qUery and resPonse, so as to imProve the informativeness and diversity of generated
resPonses (Zhang et al., 2018b)5.
Evaluation Metrics. To evalUate diversity of resUlts, we adoPt three widely-aPPlied metrics:
Distinct-1 (Dist-1), Distinct-2 (Dist-2), and EntroPy (Ent4) (Li et al., 2016; Zhang et al., 2018b;
Jost, 2006). Besides, the relevance (Rel.) is measUred by sUmming three embedding-based similar-
ities (greedy, average, extreme) (LiU et al., 2016) UPon the groUnd-trUth and generated resPonses.
Training Details. The vocabUlary size of both datasets is 40,000. The embedding layer of OPenSUb-
titles and Sina Weibo is initialized Using 200-dimensional Glove vectors (Pennington et al., 2014)
and 300-dimensional Weibo vectors (Li et al., 2018) resPectively. All the models are first Pre-trained
by MLE, and then the models inclUding Adver-REGS, GAN-AEL, AIM, DAIM and LocalGAN are
trained with adversarial learning. The discriminator of Adver-REGS and GAN-AEL are based on
CNN following (YU et al., 2017; XU et al., 2017), in which the filter sizes are set to (1,2,3,4) and the
filter number is 128, while that of LocalGAN adopts DBM with (2xembedding_size, 128, 128) to
rePresent the semantic of qUeries and resPonses. The hidden size of the generator is set to 256 and
512 in GAN-based models and Seq2Seq respectively. To guarantee the performance consistency of
AIM and DAIM, we adopt the recommended parameter settings given by Zhang et al. (2018b). The
experiments are conducted on the Tesla K80 GPU.
5.2	Results & Analysis
Table 1 lists quantitative results on the diversity and relevance of generated responses on both
datasets. As shown by the results, compared to Seq2Seq and Seq2Seq-MMI, the GAN-based meth-
ods give better results on the diversity oriented metrics, including Dist-1, Dist-2 and Ent4. This
observation indicates that adversarial learning does provide the meaningful guidance to NRG mod-
els to avoid some of the safe-responses.
It can be observed that LocalGAN outperforms the baselines with adversarial learning architecture
(Adver-REGS, GAN-AEL, AIM and DAIM) on both the diversity metrics and the relevance metrics.
Generally, a notable improvement on diversity may lead to some negative influence on relevance, and
thus promoting the diversity of generated response while maintaining their relevance is essentially
desired for any methodologies, which has been achieved by our LocalGAN. The performances of
LocalGAN can be attributed to the fact that LocalGAN has taken the local distribution of responses
to a given query into account. By adopting the hybrid objective function, the proposed adversarial
5We have taken the codes of AIM and DAIM from https://github.com/dreasysnail/
converse_GAN implemented by the authors of this work for comparisons.
8
Under review as a conference paper at ICLR 2020
Figure 2: The Entropy Trend of adversarial learning based models in the Training Process.
model gets to capture the spatial characteristics of response clusters, and the generation process is
consequently forced to fit the semantic distributions of response clusters.
The training stability is a tough issue to be addressed for adversarial learning (Yu et al., 2017), and
as discussed in the previous sections, one of the motivations of our LocalGAN is to make adversarial
learning more stable. To verify this aspect, we track the changing of the Entropy (Ent4) of results
given by GAN-AEL, Adver-REGS, AIM, DAIM and LocalGAN, as shown in Figure 2.
It can be observed that the training of LocalGAN and Adver-REGS is stable. By contrast, there
exist obvious fluctuations on the curves of AIM and DAIM, and GAN-AEL rapidly gets out of
control after 1000 batch, which makes it rather difficult to grasp the models with the best status.
This group of results indicates the necessity of introducing additional restrictions into adversarial
learning processes. For this purpose, Adver-REGS introduces a teacher-forcing loss (Li et al., 2017),
while AIM and DAIM have taken the informativeness oriented constraints to partially control the
stability (Zhang et al., 2018b). However, GAN-AEL only takes the Wasserstein distance as the
objective (Xu et al., 2017), and thus the entropy goes down rapidly. Compared to the Adver-REGS,
our LocalGAN achieves better diversity with even a more smooth entropy curve. The training of
LocalGAN benefits from the phase-wise optimization driven by the hybrid loss, and its stability also
indicates the meaningfulness of modeling and utilizing local distributions of responses.
5.3	Human Evaluation
To further conduct intuitive comparisons
among the NRG models, we perform human
evaluations on 500 testing samples. Five anno-
tators are asked to judge whether a response is
relevant to the given query and whether the re-
sponse is informative or not respectively. Both
human metrics “Relevance” and “Informative-
ness” solely have two labels 0 and 1 (0: irrel-
evant or non-informative; 1: relevant or infor-
mative) for simplicity. The human evaluation
results are listed in Table 2, and the results ba-
Method	Relevance	Informativeness
Seq2Seq	-~0738~-	0.25
Seq2Seq-MMI	0.67	0.336
Adver-REGS	~~0.702~~	0.398
GAN-AEL	0.696	0.41
AIM	0.746	0.294
DAIM	0.768	0.45
LocalGAN	0.784	0.536
Table 2: Human Evaluation Results of Models.
sically match the observations in Table 1. The
Kappa of relevance and informativeness annotations are 0.734 and 0.561 respectively.
5.4	Case Study
The effectiveness of the NRG models can be illustrated by the responses generated by them intu-
itively. Table 3 lists the sampled results given by all the baselines and our proposed LocalGAN,
trained on the OpenSubtitle corpus (The samples based on the Sina Weibo dataset are given in Ap-
pendix C). It can be observed that, compared to the basic Seq2Seq and Seq2Seq-MMI, the models
incorporating adversarial learning have generated results with varying degrees of semantic diversity,
9
Under review as a conference paper at ICLR 2020
as expected. Among the baselines, DAIM produces more diverse responses, which matches the
quantified evaluation results in Table 1, The samples from LocalGAN have shown better semantic
relevance, and meanwhile the diversity and informativeness are also guaranteed.
Table 3: Sample results given by different models trained on the OpenSubtitle corpus.
Query: you know you’re a strong woman and i admire that. Ground-truth: i’m sorry for what i’m about to do.	Query: you are a perfect girlfriend. Ground-truth: some times i wish that you would go out more.
Seq2Seq: you know what i mean. Seq2Seq-MMI: you know what you’re doing.	Seq2Seq: What are you talking about? Seq2Seq-MMI: you have no idea what you’re talking about.
Adver-REGS: i don’t know how to do that. GAN-AEL: what are you talking about? AIM: what are you doing here? DAIM: i had a plan for you.	Adver-REGS: do you have a girlfriend? GAN-AEL: what are you doing here? AIM: you are the only one. DAIM: you are a good person.
LocalGAN: i thought it was your idea.	LocalGAN: you are a part of me.
Query: maybe cause you keep marrying for money. Ground-truth: i did not marry for money.	Query: you,re just my roommate eric. Ground-truth: millie is much much more than that.
Seq2Seq: what do you mean? Seq2Seq-MMI: what do you want me to do?	Seq2Seq: i don,tknow what you,re talking about. Seq2Seq-MMI: i need to talk to you.
Adver-REGS: what do you want to say? GAN-AEL: what is the difference? AIM: what are you talking about? DAIM: what is the problem?	Adver-REGS: i want to talk to you about this. GAN-AEL: can you help me please? AIM: what are you talking about. DAIM: its a pleasure to meet you.
LocalGAN: i don’t have a choice.	LocalGAN: no you are my friend.
Query: he says some kind of sea creatures washed up on his beach. Ground-truth: how could it be a sea creature?	Query: would you excuse us forjust a minute? Ground-truth: ted i,m gonna be honest.
Seq2Seq: i don’t know what to do. Seq2Seq-MMI: what are we supposed to do?	Seq2Seq: why are you doing this? Seq2Seq-MMI: i don,t want to see you again.
Adver-REGS: he wants to go to the sandglass. GAN-AEL: he told us to come back. AIM: i thought what was going to happen. DAIM: he said it was a gift.	Adver-REGS: what are you going to do? GAN-AEL: i have to go to the bathroom. AIM: what are you supposed to do? DAIM: why are you calling me?
LocalGAN: i thought it looks like a fish.	LocalGAN: please give me your reason for ab- sence.
6 Conclusions
This paper has given the theoretical proof of the upper bound of the adversarial training leveraged
models on the Seq2Seq-based neural response generation task. The proof indicates that, due to the
local distribution nature of query-response corpora, the GAN based NRG models will converge to
the states mostly generating specialized patterns corresponding to given queries. To address this
issue, we proposed to model the local distribution of queries and their response in the semantic
space by adopting energy-based function, and found the approximation of this function. According
to this approximated distribution representation, a new loss function describing the local expansion
cost in the fitting of response distribution is presented and finally combined with the traditional
GAN loss to form a hybrid training objective for the GAN based NRG model. This paper provides a
reasonable explanation to the unstable training process and unsatisfying results of GAN based NRG
approaches, and meanwhile gives a different perspective to leverage the local data distribution to
enhance classic GAN approaches.
10
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv
preprint arXiv:1706.08224, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilib-
rium in generative adversarial nets (GANs). In Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 224-232.
PMLR, 06-11 Aug 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv e-prints, abs/1409.0473, September 2014. URL https:
//arxiv.org/abs/1409.0473.
Ashutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan. Generating more interesting responses in
neural conversation models with distributional constraints. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language Processing, pp. 3970-3980, Brussels, Belgium,
October-November 2018. Association for Computational Linguistics.
Jiachen Du, Wenjie Li, Yulan He, Ruifeng Xu, Lidong Bing, and Xuan Wang. Variational autore-
gressive decoder for neural response generation. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing, pp. 3154-3163, Brussels, Belgium, October-
November 2018. Association for Computational Linguistics.
Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading
comprehension. arXiv preprint arXiv:1705.00106, 2017.
G. B. Folland. Higher-order derivatives and taylors formula in several variables. https:
//sites.math.washington.edu/~folland∕Math4 25∕taylor2.pdf, 2005. Ac-
cessed Nov 8, 2019.
Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of
Physiology-Paris, 100(1-3):70-87, 2006.
Karl Friston, Christopher Thornton, and Andy Clark. Free-energy minimization and the dark-room
problem. Frontiers in psychology, 3:130, 2012.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems 27, pp. 2672-2680, 2014.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimeoality of data with neural net-
works. science, 313(5786):504-507, 2006.
Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length and helmholtz
free energy. In Advances in neural information processing systems, pp. 3-10, 1994.
Lou Jost. Entropy and diversity. Oikos, 113(2):363-375, 2006.
Mahyar Khayatkhoei, Maneesh K Singh, and Ahmed Elgammal. Disconnected manifold learning
for generative adversarial networks. In Advances in Neural Information Processing Systems, pp.
7343-7353, 2018.
Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746-1751,
2014.
Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Semi-supervised learning with gans: Mani-
fold invariance with improved inference. In Advances in Neural Information Processing Systems,
pp. 5534-5544, 2017.
11
Under review as a conference paper at ICLR 2020
Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann machines and
deep belief networks. Neural computation, 20(6):1631-1649, 2008.
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. Predicting structured data, 1(0), 2006.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. In Proceedings of NAACL-HLT, pp. 110-119,
2016.
Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural
dialogue generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing, pp. 2157-2169, 2017.
Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and Xiaoyong Du. Analogical reasoning on
chinese morphological and semantic relations. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pp. 138-143, 2018.
Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.
How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics
for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2122-2132, 2016.
L. Mirsky. A trace inequality of john Von neumann. Monatshefte fur Mathematik, 79(4):303-306,
Dec 1975. ISSN 1436-5081. doi: 10.1007/BF01647331. URL https://doi.org/10.
1007/BF01647331.
Jiquan Ngiam, Zhenghao Chen, Pang W Koh, and Andrew Y Ng. Learning deep energy models.
In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 1105-
1112, 2011.
Gaurav Pandey, Danish Contractor, Vineet Kumar, and Sachindra Joshi. Exemplar encoder-decoder
for neural conversation generation. In Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 1329-1338, Melbourne, Australia,
July 2018. Association for Computational Linguistics.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
K. B. Petersen and M. S. Pedersen. The matrix cookbook, nov 2012. URL http://www2.imm.
dtu.dk/pubdb/p.php?3274. Version 20121115.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep boltzmann machines. In International Confer-
ence on Artificial Intelligence and Statistics, pp. 448-455, 2009.
Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann machines. In
Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp.
693-700, 2010.
Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for short-text conversation.
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pp.
1577-1586, 2015a.
Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for short-text conversation.
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
volume 1, pp. 1577-1586, 2015b.
Yuanlong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. Gen-
erating high-quality and informative conversation responses with sequence-to-sequence models.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
pp. 2210-2219, Copenhagen, Denmark, September 2017. Association for Computational Lin-
guistics. doi: 10.18653/v1/D17-1235.
12
Under review as a conference paper at ICLR 2020
P. Smolensky. Parallel Distributed Processing: Foundations, volume 1, pp. 194-281. MIT Press,
Cambridge, 1986.
Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings of the 14th Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT), pp. 196-205, 2015.
Nitish Srivastava and Ruslan R Salakhutdinov. Multimodal learning with deep boltzmann machines.
In Advances in neural information processing systems, pp. 2222-2230, 2012.
Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, and Shi Wang. Answer-focused and
position-aware neural question generation. In Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing, pp. 3930-3939, Brussels, Belgium, October-
November 2018. Association for Computational Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems 27, pp. 3104-3112, 2014.
Baoxun Wang, Xiaolong Wang, Chengjie Sun, Bingquan Liu, and Lin Sun. Modeling semantic
relevance for question-answer pairs in web social communities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pp. 1230-1238. Association for
Computational Linguistics, 2010.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-
son, Xiaobing Liu, ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,
Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,
Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine translation. CoRR,
abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.
Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. Topic aware
neural response generation. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Jingjing Xu, Xuancheng Ren, Junyang Lin, and Xu Sun. Diversity-promoting gan: A cross-entropy
based generative adversarial network for diversified text generation. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pp. 3940-3949, 2018.
Zhen Xu, Bingquan Liu, Baoxun Wang, SUN Chengjie, Xiaolong Wang, Zhuoran Wang, and Chao
Qi. Neural response generation via gan with an approximate embedding layer. In Proceedings of
the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 628-637, 2017.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI, pp. 2852-2858, 2017.
Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, and Xueqi Cheng. Learning to control
the specificity in neural response generation. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 1108-1117, Melbourne,
Australia, July 2018a. Association for Computational Linguistics.
Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill Dolan.
Generating informative and diverse conversational responses via adversarial information maxi-
mization. In Advances in Neural Information Processing Systems, pp. 1810-1820, 2018b.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network.
arXiv preprint arXiv:1609.03126, 2016.
Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question genera-
tion with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language Processing, pp. 3901-3910, Brussels, Belgium,
October-November 2018. Association for Computational Linguistics.
13
Under review as a conference paper at ICLR 2020
A	The Graphical Model for Response Distribution Modeling
Different from the computer vision related scenario, training a DBM on a set of text vectors is not
trivial, since the training procedure is difficult to converge due to the value scale of text vectors is
much larger than image vectors. Moreover, the simple scaling methods are not effective enough
for this issue. For this purpose, this paper adopts standard-scaler6 to remove the mean and scale to
unit variance. To valid the effectiveness of standard-scaler, We conduct experiments on the query-
response matching task using normalized vectors. The experiment result show that the matching
performance based normalized vectors is similar to that of CNN based architecture (Kim, 2014).
B Detailed Proof of Lemmas
Proof of Lemma 1. As mentioned before, ri is assumed to be independent and identically dis-
tributed (i.i.d) normal variable, thus F (q, ri) can be seen as i.i.d random variable for a fixed query
q. Based on the strong law of large numbers,
F(q, Rq) = ∣R1-∣ X F(q,ri) -→ E[F(q,r)]	when IRqI → ∞
q ri ∈Rq
□
Proof of Lemma 2. For a fixed query, F(q, r) can be seen as the the scalar function of vector r. For
simplicity, we denote F(q, r) as f (r).
Taylor expansions for the first moment of function of random variables are as follows.
E[f(r)]]=E[f(rc+(r- rc))]
=E f (r) + (r - rc)TDf (rc) + 2(r - rc)> {D2f(鹿)} (r - rj + Rrc,2(r - rj
where Df (rc) is the gradient of f evaluated at rc, D2f(rc) is the Hessian matrix and Rrc,2(r - rc)
is the Lagrange remainder. Since Er = rc, the second term (r - rc)T Df (rc) disappears.
Then we will try to find the upper bound of the third term and the remainder term. The relevant
theorems used in the proof are listed as follows.
•	According to (Petersen & Pedersen, 2012), assume A is symmetric, c = E[x] and Σ =
Var[x], then
E xT Ax = Tr(AΣ) + cT Ac.
•	(Mirsky, 1975) states following theorem: If A, B are complex n × n matrices with singular
values αι ≥ α2 ≥ ∙∙∙ ≥ an and βι ≥ β2 ≥ ∙∙∙ ≥ βn respectively, then
n
I Tr(AB)I ≤ Xαiβi
Firstly, since r —r 〜N(0, Σ) and Σ is positive semi-definite matrix, the third term can be simplified
as follows.
IE[(r - rc)> D2f(rc)} (r - rc)]I = I Tr({D2f(rc)}Σ) + 0>D2f(rc)0I
n
≤	αiβi
i=1
≤ α1Tr(Σ) = α1E kr - rc]k22
where aι ≥ a2 ≥ ∙∙∙ ≥ an and βι ≥ β2 ≥ ∙∙∙ ≥ βn denote the singular value of matrix
{D2f (rc)} and Σ respectively.
6https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
14
Under review as a conference paper at ICLR 2020
Meanwhile, according to the Corollary 1 in (Folland, 2005), we have that
lRrc,2(r - rC)I ≤ M! Ilr - rck3
where M is the upper bound for absolute value of third-order partial derivatives of f.
Next, we show that the α1 and third-oder partial derivatives can be bounded by M , where M is
max |b(hq, i)| (hq follows multinomial distribution) and n is the dimension of r.
hq,1≤i≤n
Substituting the definition of E(q, r, H) into F(q, r), we have following equation:
f(r) = F(q,r) = -log	exp(hqT Wqr r + hqTWqqq + hT Wqhhq).
hq,h
After that, its first-order, second-order and third-order partial derivative are calculated as follows:
∂f (r) _ Phq,h exP(hTWqrr + hTWqqq + hTWqhhq) X (hTWqr)i
∂r	Phq ,h exp(hT Wqr r + hTWqqq + hTWqhhq )
drf∂r) = X a(hq, h) × b(hq, i) ×
i j	hq,h
a(hq, h) × b(hq, j) - b(hq, j)
∂ri∂rj ∂rk =Xc(hq,h,k)×b(hq,i)×
Ea(hq,h) × b(hq,j) - b(hq, j)
hq,h
+ T a(hq,h) × b(hq,i) × Ec(hq, h,k) × b(hq,j)
hq,h	Lhq ,h
where
exp(hqTWqrr + hqTWqqq + hTWqhhq)
a( hq ,h) = ----H---------H---------H-----H——
P exp(hTWqrr + hTWqqq + hTWqhhq)
7 7
hq h
c(hq ,h,k) = d⅞qa
a(hq, h)b(hq, k) - a(hq, h)	a(hq, h)b(hq, k)
hq,h
and b(hq, i) = (hqTWqr)i representing the i-th element of the vector hqT Wqr. According to the
definition of a(hq, h), it is obvious that a(hq, h) > 0 and Ph ,h a(hq, h) = 1.
The upper bounds for the second-order and third-order partial derivative are shown as follows.
∂f
∂ri∂rj
≤	|a(hq,h)| × |b(hq,i)| ×
hq,h
a(hq, h) × b(hq, j)
+ |b(hq,j)|
≤ 2M 2
X
|a(hq, h)| × ∣b(hq, k)| ≤ 2M∣a(hq, h)|
7	7
hq ,h
lfkl ≤ X 1c(hq, h,k)1 × M × M + M] + M 2 X |c(hq, h,k)1 ≤ 6M3
J	hq ,h	hq ,h
1 ʌ 1 .1	I	1 1	. 1 Τl~Γ F CTl /Γiλ Λ . 1	Λ	1 i'	Λ
Based on the upper bound above, we can see that M can be 6M3 and the upper bounds for α1 and
|Rrc,2(r - rc)| are as follows.
α1 = σmax({D2f(rc)}) ≤ I{D2f(rc)}IF
∖
nn 2
XX∣∂∂⅛∣2 ≤ 2nM2
i=1 j=1
15
Under review as a conference paper at ICLR 2020
∖Rrc,2(r - rc)∣ ≤ w∣∣r — rck3 = M3kr - rck3 ≤ (√nMkr 一鹿心)3
Hence, we have
∖E[f (r)] - f (rc) ∖ = ∖gE[(r - r0)> {D2f (r°)} (r - r0)] + E[M0,2(r - r0)] ∖
≤ 1 a1Ekr - rc]∣∣2 + EIRrc,2(r - rc)∖
≤ E(√nM∣∣r - rc]∣∣2)2 + E(√^M∣∣r -鹿心)3
Therefore,
lim
E∣∣r-rck2→0
∖E[f(r)] - f (rc) ∖
0
□
16
Under review as a conference paper at ICLR 2020
C Additional Cases
Query：凡事不要想的太复杂，手握的太紧，东西会碎，手会疼。
You don’t need to make things too complicated, and it’s
just like something you are holding, if you use too much power,
it will break and hurt you.
Ground-truth:没办法啊。为什么我那么敏感呢？
I feel very helpless. Why am I so sensitive?
Seq2Seq:这句话说的很好!
That was spoken well.
Seq2Seq-MMI:这句话说得太好了。
That was spoken quite well.
Adver-REGS:这句话很有道理。
It quite make sense.
GAN-AEL:这句话是什么意思？
What does that mean?
AIM:这句话说得太对了！
These words speak the truth.
DAIM:那是我自己想要的东西。
That’s what I want.
LoCalGAN:放下，放下，立地成佛。
Let it go and free yourself.
Query:给所有的胖纸讲个鬼故事，夏天来了。
A terrible story to all the fatties: the summer is coming.
Ground-truth:现在也就这只肥猫能让我笑一笑。
This fatty cat looks so funny.
Seq2Seq:我只想说这句话我很喜欢
I just wanna to say that I like this sentence.
Seq2Seq-MMI:我只想说这句话是什么意思？
I just wanna say what do you mean?
Adver-REGS:我只能说，你是我的骄傲
I can only say that I am proud of you.
GAN-AEL:我不会也有这样的一天吧。
May I have such a day ?
AIM:我只能说，我也是这样的。
I can only say so do I.
DAIM:你是我的唯一，我的瘦子。
You are my only, skinny.
LoCalGAN:这就是传说中的瘦子的心声。
This must come from the heart of a skinny guy.
Query:只要生命还在，就可以从头再来。
Never give up as long as you are alive.
Ground-truth:有些东西.不能重来。
But there always exist something not repeatable.
Seq2Seq:加油，永远支持你!
Cheer up, and I am always supporting you.
Seq2Seq-MMI:每一天都是最好的。
Every day is the best day.
Adver-REGS:新的一天，加油!
A new day, cheer up!
GAN-AEL:新的一天，新的起点！
New day, new beginning!
AIM:加油，继续加油！
Cheer up, forever!
DAIM: 一切都会过去的。
Everything will pass.
LoCalGAN:淡定吧，一切都会好起来的。
Be calm, and everything will be OK.
Query:河豚君。生气的样子真呀么真可爱。
The globefish looks so cute when it gets angry.
Ground-truth:我还以为他吞了一个球呢。
I thought it had swallowed up a ball.
Seq2Seq:我也是这样的啊。
So am I.
Seq2Seq-MMI:原来是这样的啊！
So that’s it!
Adver-REGS:我也有一个这样的。
I also have one like this.
GAN-AEL:我也有这样的感觉。
I also have the same feeling.
AIM:是啊，我也是这样的。
Yeah, so am I.
DAIM:好可爱的样子！
It looks so cute!
LoCalGAN:这就是传说中的河豚?
So this is the legendary globefish?
Query:远离城市，才得安宁。
You can feel peaceful only if you are far from cities.
Ground-truth:我也是这么想的，但真正有几个人能做到呢。
Agree with that, but I think few people can really do it.
Seq2Seq:我也是这么认为
I also think so .
Seq2Seq-MMI:我也有这种感觉
I have the same feeling as well.
Adver-REGS:这是什么意思？
What does this mean?
GAN-AEL:又不是一个人的城市。
This is not a city for any single person.
AIM:我也想，但我不会去。
I also want it, but I will not take action.
DAIM:这就是我们的生活！
This is our life!
LoCalGAN:心若不静，耳根亦不清静。
If you don’t get inner peace, you will hear the boring noise.
Query:喜欢这样被婴儿的小手抓住手指的童鞋举手!
Just let me know if you also love to be holding on
your finger by the tiny hand of a baby!
Ground-truth:喜欢那带着稚气声音喊我姐姐、娜姨
I love to be called as the sister or auntie with baby voice.
Seq2Seq:我也想要这样的!
I also want this!
Seq2Seq-MMI:我喜欢这种感觉！
I love this feeling!.
Adver-REGS:我家也是这样的!
The one in my home is like this as well!
GAN-AEL:有木有想要一个？
Do you want one like this?
AIM:好喜欢这种感觉。
Love this feeling so much.
DAIM:我也想要一个这样的男朋友
I also want a boyfriend like this.
LoCalGAN:好有爱啊，我也想要一个宝宝。
So cute, I also want a baby.
Figure 3: Sample Results given by different models trained on the Sina Weibo corpus.
17