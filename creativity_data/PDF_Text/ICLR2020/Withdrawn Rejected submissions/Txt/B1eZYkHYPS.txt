Under review as a conference paper at ICLR 2020
Shifted Randomized Singular Value Decompo-
SITION
Anonymous authors
Paper under double-blind review
Ab stract
We extend the randomized singular value decomposition (SVD) algorithm (Halko
et al., 2011) to estimate the SVD of a shifted data matrix without explicitly con-
structing the matrix in the memory. With no loss in the accuracy of the original
algorithm, the extended algorithm provides for a more efficient way of matrix
factorization. The algorithm facilitates the low-rank approximation and principal
component analysis (PCA) of off-center data matrices. When applied to differ-
ent types of data matrices, our experimental results confirm the advantages of the
extensions made to the original algorithm.
1	Introduction
The singular value decomposition (SVD) is one of the most used matrix decompositions in many
areas of science. Among the typical applications of SVD are the low-rank matrix approximation
and principal component analysis (PCA) of data matrices (Jolliffe, 2002). Using SVD to accurately
estimate a low-rank factorization or the principal components of a data matrix, a mean-centering
step should be carried out before performing SVD on the matrix. Despite its simplicity, the mean-
centering can be very costly if the data matrix is large and sparse. This cost is because the mean
subtraction of a sparse matrix turns it to a dense matrix which requires a considerable amount of
memory and CPU time to be analyzed. This motivates us to extend the randomized SVD algorithm
introduced by (Halko et al., 2011) to estimate the singular value decomposition of a mean-centered
matrix without explicitly forming the matrix in the memory.
More generally, we introduce a shifted randomized SVD algorithm that provides for the SVD esti-
mation of a data matrix shifted by any vector in the eigenspace of its column vectors. The proposed
algorithm facilitates the low-rank matrix approximation and the principal component analysis of a
data matrix through merging the mean-centering and the SVD steps. The mean-centering is crucial
to obtain the minimum PCA reconstruction error through a deterministic SVD. We experimentally
show that it plays an essential role in case of using the randomized SVD algorithm too. Our exper-
iments with different types of data matrices show that the extended algorithm performs better than
the original algorithm when both are applied to a center-off data matrix.
In the followings, we briefly introduce the principal component analysis and its connection with
the singular value decomposition. Then, we introduce the shifted randomized SVD algorithm and
provide an analysis of its performance. Finally, we report our experimental results obtained from the
principal component analysis of different types of data matrices using the extended and the original
randomized SVD algorithms.
2	Principal Component Analysis
Principal component analysis (PCA) is a method to study the variance of a random vector. PCA
projects a random vector to the eigenspace of its covariance matrix. Let x be an m-dimensional
random vector with the mean vector 0. PCA projects x to a latent random vector y as below:
y=ATx	(1)
where the square matrix A is composed of the eigenvectors of the covariance matrix Σx . The
elements of y are called the principal components of x. In many use cases of PCA, A contains
1
Under review as a conference paper at ICLR 2020
a subset of the eigenvectors of Σx . The minimum PCA reconstruction error is obtained from the
eigenvectors corresponding to the top eigenvalues of the covariance matrix (Jolliffe, 2002).
The matrix of eigenvectors A in Equation 1 can be efficiently estimated from the singular value
decomposition (SVD) of a sample matrix X . To this end, the sample matrix should be first centered
around its mean vector:
X = X - μx1T	(2)
The fact that the matrix of left singular vectors of X is equal to the eigenvectors of the covariance
matrix of X , the PCA projection of X is:
Y = U T X = SV T	(3)
where X = USV T.
The mean-centering step in Equation 2 can be very costly if X is a large sparse matrix and μχ = 0.
In this case, X is a dense matrix that requires a vast amount of memory and cannot be processed in
a reasonable time. In the next section, we introduce a randomized SVD algorithm to estimate the
SVD of X without explicitly performing the mean subtraction step.
3	Shifted Singular Value Decomposition
Let X be an m X n (m ≤ n) matrix and μ be an m dimensional vector in the space of the column
vectors ofX. Algorithm 1, extends the randomized matrix factorization method introduced by Halko
et al. (2011) to return a rank-k approximation of the singular value decomposition of the matrix
X = X 一 μ1T without explicitly forming X. The differences between the extended algorithm and
the original one are in lines 6, 9, 10, and 12. In the followings, we explain the entire algorithm with
a more in-depth focus on the modified parts.
The Shifted-Randomized-SVD algorithm works in three major steps:
1.	Estimate a basis matrix for X
2.	Project X to the space of the basis matrix
3.	Estimate the SVD factors of X from its projection
In the first step (lines 2to 11), arankK basis matrix Q1 (k < K m) that spans the column vectors
of X is computed. In Line 2, a random matrix is drawn from the standard Gaussian distribution.
This matrix is then used in Line 3 to form the sample matrix X1 whose columns are independent
random points in the range of X. This sample matrix is used to estimate a basis matrix for X in
two steps. In Line 4, a basis matrix Q1 is computed through QR-factorization of X1. Since X1 is
sampled from X , the basis matrix is considered as an approximation of the basis of X . Then in
Line 6, the basis of X is estimated from the Qi by the QR-update algorithm proposed by Golub &
Van Loan (1996, p. 607). For a given QR factorization such as Q1R1 = X1 and two vectors u, and
v , the QR-update algorithm computes the QR-factorization in Equation 4 by updating the already
available factors Q1 and R1.
QR = X1 + uvT	(4)
The computational complexity of the QR-update of the m × K matrix X1 is O(m2).1 Replacing u
with -μ and V with 1, the QR-update in Line 6 returns the basis matrix Q that span the range of the
matrix
X = X - μ1T	(5)
In other words, X can be approximated from Q:
X ≈ QQTX	(6)
Note that the basis matrix of X is computed without explicitly constructing the matrix X itself. The
if statement in Line 4 controls the useless performance of the QR-update step with the null vector.
The for loop starting at Line 8, estimates a basis matrix for B = (X X T )q X using the basis of X,
Q. The matrix B with a positive integer power has the same singular vectors as X, but with a sharper
1The computational complexity of the QR-update of an m × n matrix in O(N 2) where N = max(m, n).
2
Under review as a conference paper at ICLR 2020
decay in its singular values since Sj(B) = Sj(X)2q+1, where Sj(.) returns the jth singular vectors
of its input matrix. The sharp decay in singular values improves the accuracy of the randomized
SVD when the singular values of X decay slowly. This effect is because the reconstruction error
of the randomized SVD is directly proportional to the first top unused singular vector of X (see
Equation 12).
The basis of (XXT)qX is computed viaalternative applications of matrix product on XT and X.
For q = 1, in Line 9, a basis matrix of XTX is estimated through QR-factorization of XTQ. To
avoid forming X explicitly, instead of direct multiplication XTQ, We use the distributive property
of multiplication over addition:
XTQ = (X - μ1T)TQ = XTQ - 1(μTQ)	(7)
where 1 is a vector of ones. The product 1(μTQ) can be efficiently computed in O(nK) memory
space if a higher priority is given to the parentheses. In Line 10, a basis matrix of XXTX is esti-
mated through QR-factorization of XQ0 where Q0 is a basis matrix of XTQ. Similar to Equation 8,
the product XQ0 is computed as:
XQ0 = (X - μ1T)Q0 = XQ0 - μ(1TQ0)	(8)
with the same amount of memory space, O(nK). The matrix multiplication loop iterates q times.
At this stage, We have the basis matrix Q that approximates a basis for X.
In the second major step of the algorithm, the matrix X is projected to the space spanned by Q:
Y = QT X	(9)
This step in done in Line 12 using the same trick as in Equation 8:
Y = QT(X - μ1T) = QTX -(QTμ)1T	(10)
Finally, in the third step, the SVD factors of X are estimated from the K X n matrix Y in two
steps. First, a rank-k approximation of Y is computed using a standard method of singular value
decomposition, i.e., Y = U1ΣVT (Line 13). Then, the left singular vectors are updated by U =
QUi resulting in UΣVT = QY (Line 14). Replacing Y with QTX and using Equation 6 (X ≈
QQTX), we have the rank-k approximation of X:
UΣVT ≈ X	(11)
Algorithm 1 The rank-k singular value decomposition of the m × n matrix X 一 μ1T = UΣVT
with (m ≤ n) using the sampling parameter K (k < K	m) and q ∈ {0, 1, 2, . . . }.
1	Procedure SHIFTED-RANDOMIZED-S VD(X, μ, k, K, q)
2	Draw an n × K standard Gaussian matrix Ω
3	Form the sample matrix Xi J XΩ
4	:	Compute the QR factorization X1 = Q1R1
5	if μ = 0 then
6	Compute QR = QiRi — μ1T using the QR-update algorithm
7	:	end if
8	:	for i = 1, 2, . . . , q do
9	Compute the QR-factorization Q0R0 = XTQ - 1(μTQ)
10	Compute the QR-factorization Qr = XQ0 一 μ(1TQ0)
11	:	end for
12	Form Y J QTX — (QTμ)1T
13	:	Compute the singular value decomposition of Y = Ui ΣVT
14	:	U J QUi
15	:	return (U, Σ, V)
16	: end procedure
The shifting vector μ in Algorithm 1 can be any vector in the space of the column vectors of X. If
it is set to the null vector 0, then the algorithm reduces to the original randomized SVD algorithm
of Halko et al. (2011). If it is set to the mean vector of X , then the algorithm estimates the singular
vectors of the mean-centered matrix X. In this case, the algorithm facilitates the principal compo-
nent analysis of a data matrix X through merging the centering step in Equation 2 and the SVD step
in Equation 3.
3
Under review as a conference paper at ICLR 2020
4	Performance Analysis
The Shifted-Randomized-SVD algorithm explained in the previous section approximates the
SVD of a shifted data matrix X = X - μ1T without explicitly constructing the matrix in the
memory. In this section, we study the performance of the algorithm based on the accuracy and the
efficiency of the original randomized SVD algorithm of Halko et al. (2011).
To estimate the singular value decomposition of a sifted matrix X using the original randomized
SVD algorithm, X should be explicitly formed and passed to the algorithm. Since SHIFTED-
Randomized-SVD adds no extra randomness to the original algorithm, we have the same re-
construction error bound as if the original algorithm factorized the shifted matrix X:
1
-	Γ2m~2q 2q+1
E[∣∣X - USVt∣∣] ≤ 1+4,口	σk+ι	(12)
where σk+ι is the (k + 1)th singular value of the m X n matrix X with m ≤ n, 2 ≤ k ≤ mm is the
decomposition rank, and q ∈ Z+ is a power value as explained in Algorithm 1.
In the followings, We study the computational complexity of the SVD factorization of X using the
original randomized SVD algorithm and its extended version in Algorithm 1. For an m× n matrix
X, the computational complexity of the original randomized SVD algorithm is:
O(αk + (m+ n)k2)	(13)
where α is the cost of the matrix-vector multiplication with the input matrix X. If X is a dense
matrix then α = mn , and if X is a sparse matrix then α = T, a small constant value.
Algorithm 1 adds a QR-update step (Line 6) and three matrix-matrix multiplications (lines 9, 10,
and 12) to the original algorithm. The matrix multiplications do not affect the computational com-
plexity of the original algorithm since their computational complexity is equal to the complexity of
computing QTX in the original algorithm. The QR-update step, running in O(m2), however, can
affect the computational complexity of the algorithm.
Assuming that μ = 0, if both X and X are dense matrices then both algorithms have the same
computational complexity as:
O(mnk + (m+ n)k2)	(14)
This equality is because the computational complexity of the QR-update step, O(m2), is dominated
by the complexity of the original algorithm (i.e., m2 ≤ mn for every m≤ n where m, n ∈ N). In
addition, the construction of X to be use by the original algorithm takes O(mn) time which is greater
than or equal to the complexity of the QR-update step. Hence, the added operations do not affect the
computational complexity of the original algorithm. Halko et al. (2011) show that for a dense input
matrix, the randomized SVD algorithm can be performed in O(mn log k + (m+ n)k2) if instead of
the random normal matrix Ω in Line 2, a structured random matrix such as the sub-sampled random
Fourier transform is used. This improvement can be considered for the S hifted-Randomized-
SVD algorithm too.
If the input matrix X is sparse, then X is a dense matrix for every μ = 0. In this case, the computa-
tional complexity of the Shifted-Randomized-SVD algorithm is:
O(Tk + m2 + (m+ n)k2)	(15)
where the constant T is the cost of multiplying a sparse matrix to a vector, and the parameter m2 is
related to the complexity of the QR-update step. On the other hand, since X is a dense matrix, the
complexity of the original algorithm is O(mnk + (m+ n)k2) which is higher than the complexity
of the extended algorithm.
In a special case where X is a dense matrix and X is a sparse matrix, the original algorithm can
factorize X in O(Tk + (m+n)k2), but SHIFTED-RANDOMIZED-SVD needs O(mnk + (m+n)k2)
time. In this case, if Algorithm 1 is applied to X with μ = 0, the factorization can be performed in
the same processing time as the original algorithm. As a summary, we showed that the Shifted-
Randomized-SVD algorithm as illustrated in Algorithm 1 is as efficient as the randomized SVD
algorithm proposed by Halko et al. (2011) if the input matrix is dense, and more efficient than it if
the input matrix is sparse.
4
Under review as a conference paper at ICLR 2020
5 Experiments
We experimentally study the difference between performing PCA with the randomized SVD algo-
rithm (RSVD) proposed by Halko et al. (2011) and its extended version in Algorithm 1 (S-RSVD).
The fact that a minimum PCA reconstruction error is obtained from the deterministic SVD of a
mean-centered data matrix, the performance of S-RSVD on an off-center data matrix with its mean
vector as the shifting vector is expected to be more accurate than the performance of RSVD on the
same data matrix. Since the two algorithms are randomized in nature, it is important to test if this
expectation is valid for different types of data matrices.
We experimentally compare the two algorithms based on the mean of the squared L2 norm of PCA
reconstruction error (MSE). The same parameters K = 2k and q = 0 are used for both S-RSVD and
RSVD unless it is clearly mentioned. The shifting vector μ for S-RSVD is set to the mean vector
of data matrices. The experiments are carried out on different types of data matrices including
randomly generated data, a set of images, and word co-occurrence matrices. The characteristics of
the data matrices are illustrated in the corresponding sections.
5.1	Random Data
In this section, we examine how the two SVD algorithms are affected by the parameters such as
the number of principal components, the size and the distribution of a data matrix, and the power
iteration scheme. Our experiments are based on two comparison metrics, 1) an MSE value obtained
from a fixed number of principal components, and 2) the sum of MSE values obtained from different
number of principal components ranging from 1 to 100.
Figure 1a represents the effect of the number of principal components on the MSE values obtained
from a 100 × 1000 matrix sampled from a 100-dimensional random vector uniformly distribution in
the range 0, 1. The results show that mean-centering leads to substantial reduction to the reconstruc-
tion error when the number of principal components is small. This observation is in line with the fact
that the contribution of the mean-centering is mostly to the accuracy of top principal components.
The effect of the sample size on the two factorization algorithms is represented in Figure 1b in
which the X-axis is the sample size and the Y-axis is the sum of the MSE values obtained from
different number of principal components ranging from 1 to 100. The data matrices are generated
by a 100-dimensional uniform random vector in the range 0, 1. The results show that S-RSVD
is more accurate and stable than RSVD. Despite the fact that both algorithms are randomized, the
stability of S-RSVD is less sensitive to the sample size.
Figure 1c compares the performance of the two algorithms with respect to the data distribution. The
Y-axis is the sum of MSE values.Regardless of the data distribution, S-RSVD is more accurate than
RSVD. This observation is in line with the fact that PCA does not make any assumption about the
data distribution.
To examine whether both algorithms are equally accurate for the factorization of a mean-centered
data matrix X, a comparison between the two algorithms is provided inFigure 1d. The Y-axis is
the sum of the MSE values.In the experiments with RSVD, the matrix X is explicitly constructed
and factorized, but in the S-RSVD experiments the singular factors of X are implicitly estimated
from X . The results show that S-RSVD is as accurate as RSVD applied to an already centered data
matrix X. This observation supports Equation 11.
An important parameter of the randomized SVD algorithm is the power value q that has a positive
effect on the accuracy of the algorithm, . Figure 1e shows the MSE values obtained from each of the
factorization algorithms with different values of q . The data matrix in this experiment is sampled
from a 100-dimensional uniform distribution. The Y-axis is the sum of MSE values and the X-axis
represents q. The importance of mean-centering is clear when the value of q is small. The accuracy
of RSVD is significantly improved as the values of q increases, while the accuracy of S-RSVD
is only slightly improved. This observation on a set of uniformly distributed vectors suggest that
RSVD with a positive value of q (1 or 2 as suggested by Halko et al. (2011)) can be as accurate as
S-RSVD.
To test whether RSVD with a large value of q can be as accurate as S-RSVD, we run the same
experiment as above but on data with different distributions. Figure 1f shows the difference between
5
Under review as a conference paper at ICLR 2020
(a)
——LS-RSVD
八八、
'\/
RSVD
0
5,000
10,000
15,000
20,000
(b)
(d)
(C)
100
90
80
70
50
100
(e)
-----S-RSVD
RSVD
150
200
-600
0
-200
-400
Normal
Zipf
0
50
100
(f)
Uniform
Poisson
150	200
0
Figure 1: A comparison between S-RSVD and RSVD based on (a) number of principal components,
(b) sample size, (c) data distribution, (d) explicit versus implicit mean-centering, (e) the power value
q, and (f) the difference of their accuracies with respect to the power value q and the data distribution.
the sum of MSE values obtained from each of the algorithms (i.e., Y-axis is MSE-SUM(S-RSVD) -
MSE-SUM(S-RSVD)) with respect to the parameter q. Being all the results negative means that S-
RSVD is more accurate than RSVD. Except for the data with Zipfian distribution, the difference
between the accuracy of the two algorithms approaches to zero as the value of q increases. The
Zipfian graph fluctuates widely for small values of q, but it becomes flat as q becomes larger. In
the best case, the difference between the two algorithms on Zipfian data is -64 at q = 200. This
indicates that the power iteration scheme cannot fully recover the reconstruction loss of an off-
center data matrix, but depending on the data distribution, it can be helpful. The power iteration
in Algorithm 1 is a computationally heavy step which can negatively affect the efficiency of the
algorithm when the value of q is large.
5.2	Image Data
In this section, we experiment with handwritten digits and facial image matrices. The handwritten
digits are a copy of the test set of the UCI ML hand-written digits datasets consisting of 1979 images
of size 8 × 8.2 We vectorize individual image matrices and stack them into a single 16 × 1979 data
matrix. The facial images consisting of 13233 images each of size 250 × 250 are downloaded from
Labeled Faces in the Wild (LFW).3 The facial image matrix after vectorizing and stacking all of the
images matrices is a 62500 × 13233 matrix.
The left side of Table 1 summarizes the results obtained from 10-dimensional PCA of the image
matrices.The MSE values represented in the first two rows of the table show that S-RSVD is more
accurate than RSVD. To ensure that the results are not due to chance, we run the experiment 30
times and perform two t-tests with the following null hypotheses:
•	H01:there is no difference between the MSE of S-RSVD and RSVD.
•	H02 :there is no difference between the individual column reconstruction errors of S-RSVD
and RSVD.
2https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+
Handwritten+Digits
3http://vis-www.cs.umass.edu/lfw/lfw.tgz
6
Under review as a conference paper at ICLR 2020
	image data		word data			
	digits	faces	n = 1e3	n = 1e4	n = 1e5	n = 3e5
MSE of S-RSVD	415.7	15.3e7	195e-5	235e-5	763e-5	994e-5
MSE of RSVD	430.6	16.1e7	200e-5	236e-5	765e-5	998e-5
p1 -value	0.00	0.00	0.00	0.00	0.00	0.00
p2-value	0.00	0.00	0.00	0.00	0.00	0.00
WR of S-RSVD	66%	82%	71%	73%	77%	70%
WR of RSVD	34%	18%	29%	27%	23%	30%
Table 1: The reconstruction error statistics of image and word data.
b]∣IE4MH∣a[1MΠLa
276	432	397	275	482	434	258	502	67 1	670
MiiElME⅞t1[1HE-lt]
288	453	427	296	493	487	297	521	725	661
UliElMElk1[1HMtl
(a)
(b)
Figure 2: The effect of mean-centering on (a) handwritten digits and (b) facial data. In each sub-
figure, the top row is the original image, the rows in the middle and bottom are the S-RSVD and
RSVD reconstructed images, respectively. The reconstruction errors are shown on top of each image.
The former hypothesis is validated on the 30 MSE pairs obtained from the SVD methods, but the
later hypothesis is validate on the pairs of the reconstruction error of individual images. The p-values
represented in Table 1 reject both hypotheses and confirm that the better results obtained from S-
RSVD are not by chance. The rejection of H02 indicates that S-RSVD results in not only lower MSE
for the entire image matrices, but also for individual images.
To provide a better picture of how well the SVD algorithms perform on individual images, we plot
the first 10 images of each data matrix and estimate the win-rates of the algorithms. Figure 2 shows
the examples of the original handwritten and facial images (the top rows), and their reconstructions
using S-RSVD (the middle rows) and RSVD (the bottom rows) with the reconstruction error values
on top of each image. For most images, S-RSVD is more accurate than RSVD. To generalize this
observation, we estimate the win-rate (WR) of the algorithms (i.e., the number of images for which
one algorithm is more accurate than the other algorithm out of the total number of images). The
results shown in Table 1 indicate that 66% of the handwritten images and 82% of the facial images
are reconstructed more accurately by S-RSVD than RSVD.
7
Under review as a conference paper at ICLR 2020
5.3	WORD DATA
In this section, we experiment with word probability co-occurrence matrices whose elements are
the probability of seeing a target word in the context of another word. Our experiments are based
on the word co-occurrences probabilities estimated from the English Wikipedia corpus used in the
CoNLL-Shared task 2017.4 For each target word wi, we estimate the probability of seeing the
word conditioned on the occurrence of another word wj, called the context word (i.e., p(wi|wj) ≈
nnj,wi)). The ith column of a probability co-occurrence matrix associated with the word Wi is a
distributional representation of the word.
Due to the Zipfian distribution of words and relatively large number of words, a word probability
co-occurrence matrix is a large and sparse matrix with a high degree of sparsity. A mean subtraction
turns the matrix to a dense matrix that needs huge amount of memory and processing time to be
analyzed. The Shifted-Randomized-SVD algorithm can substantially improve the performance
of analyzing a mean-centered co-occurrence matrix.
We estimate 100-dimensional PCA representations of different m × n word probability co-
occurrence matrices formed with m = 1000 most frequent context words and n most frequent target
words with different values of n. Each experiment is run 30 times with different random seeds. The
right side of Table 1 represents the statistics of the reconstruction errors obtained from each of the
factorization algorithms. The first two rows of the table confirm that S-RSVD is more accurate than
RSVD. To see whether the difference between MSEs is statistically significant, a t-test with the null
hypothesis H01:there is no difference between the MSE of S-RSVD and RSVD is performed. Using
the 30 pairs of MSE values obtained from each of the factorization algorithms, the test rejects the
null hypothesis H01 with a high confidence level (see p1 -value in Table 1).
We study the effect of the mean-centering on the reconstruction of the distributional representation
of individual words (i.e., each column of the co-occurrence matrix). A t-test is performed to validate
the null hypothesis H02:there is no difference between the individual column reconstruction errors of
S-RSVD and RSVD. The acceptance probabilities of H02 shown as p2-values in Table 1 confirm that
the differences between the reconstruction errors of individual words is indeed significant. The win-
rates (WR) of each of the algorithms shows that the mean-centering is beneficial to the reconstruction
of the majority of words.
Conclusion
We extend the randomized singular value decomposition algorithm of Halko et al. (2011) to fac-
torize a shifted data matrix (i.e., a data matrix whose columns vectors are shifted by a vector in
their eigenspace) without explicitly constructing the matrix in the memory. With no harm to the
performance of the original algorithm on dense matrices, the extended algorithm leads to substantial
improvement to the accuracy and efficiency of the algorithm when used for low-rank approximation
and principal component analysis of sparse data matrices. The algorithm is tested on different types
of data matrices including randomly generated data, image data, and word data, with their mean
vector as the shifting vector. The experimental results show that the extended algorithm results in
lower mean squared reconstructions error in all experiments through successfully incorporating the
mean-centering step to SVD.
References
G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press, Baltimore,
MD, USA, third edition, 1996.
N. Halko, P.G. Martinsson, and J.A. Tropp. Finding structure with randomness: Probabilistic al-
gorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217-288,
2011.
I.T. Jolliffe. Principal Component Analysis. Springer Series in Statistics. Springer-Verlag, 2002.
4https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1989
8