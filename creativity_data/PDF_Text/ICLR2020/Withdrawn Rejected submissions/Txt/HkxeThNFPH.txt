Under review as a conference paper at ICLR 2020
Safe Policy Learning for Continuous Control
Anonymous authors
Paper under double-blind review
Ab stract
We study continuous action reinforcement learning problems in which it is crucial
that the agent interacts with the environment only through safe policies, i.e., policies
that keep the agent in desirable situations, both during training and at convergence.
We formulate these problems as constrained Markov decision processes (CMDPs)
and present safe policy optimization algorithms that are based on a Lyapunov
approach to solve them. Our algorithms can use any standard policy gradient
(PG) method, such as deep deterministic policy gradient (DDPG) or proximal
policy optimization (PPO), to train a neural network policy, while guaranteeing
near-constraint satisfaction for every policy update by projecting either the policy
parameter or the selected action onto the set of feasible solutions induced by
the state-dependent linearized Lyapunov constraints. Compared to the existing
constrained PG algorithms, ours are more data efficient as they are able to utilize
both on-policy and off-policy data. Moreover, our action-projection algorithm often
leads to less conservative policy updates and allows for natural integration into an
end-to-end PG training pipeline. We evaluate our algorithms and compare them
with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as
a real-world robot obstacle-avoidance problem, demonstrating their effectiveness
in terms of balancing performance and constraint satisfaction.
1	Introduction
The field of reinforcement learning (RL) has witnessed tremendous success in many high-dimensional
control problems, including video games (Mnih et al., 2015), board games (Silver et al., 2016), robot
locomotion (Lillicrap et al., 2016), manipulation (Levine et al., 2016; Kalashnikov et al., 2018),
navigation (Faust et al., 2018), and obstacle avoidance (Chiang et al., 2019). In RL, the ultimate goal
is to optimize the expected sum of rewards/costs, and the agent is free to explore any behavior as
long as it leads to performance improvement. Although this freedom might be acceptable in many
problems, including those involving simulators, and could expedite learning a good policy, it might be
harmful in many other problems and could cause damage to the agent (robot) or to the environment
(objects or people nearby). In such domains, it is absolutely crucial that while the agent optimizes
long-term performance, it only executes safe policies both during training and at convergence.
A natural way to incorporate safety is via constraints. A standard model for RL with constraints
is constrained Markov decision process (CMDP) (Altman, 1999), where in addition to its standard
objective, the agent must satisfy constraints on expectations of auxiliary costs. Although optimal
policies for finite CMDPs with known models can be obtained by linear programming (Altman,
1999), there are not many results for solving CMDPs when the model is unknown or the state and/or
action spaces are large or infinite. A common approach to solve CMDPs is to use the Lagrangian
method (Altman, 1998; Geibel & Wysotzki, 2005), which augments the original objective function
with a penalty on constraint violation and computes the saddle-point of the constrained policy
optimization via primal-dual methods (Chow et al., 2017). Although safety is ensured when the
policy converges asymptotically, a major drawback of this approach is that it makes no guarantee
with regards to the safety of the policies generated during training.
A few algorithms have been recently proposed to solve CMDPs at scale while remaining safe during
training. One such algorithm is constrained policy optimization (CPO) (Achiam et al., 2017). CPO
extends the trust-region policy optimization (TRPO) algorithm (Schulman et al., 2015a) to handle the
constraints in a principled way and has shown promising empirical results in terms scalability, perfor-
mance, and constraint satisfaction, both during training and at convergence. Another class of these
algorithms is by Chow et al. (Chow et al., 2018). These algorithms use the notion of Lyapunov func-
tions that have a long history in control theory to analyze the stability of dynamical systems (Khalil,
1996). Lyapunov functions have been used in RL to guarantee closed-loop stability (Perkins & Barto,
2002; Faust et al., 2014). They also have been used to guarantee that a model-based RL agent can
be brought back to a “region of attraction” during exploration (Berkenkamp et al., 2017). Chow et
al. (Chow et al., 2018) use the theoretical properties of the Lyapunov functions and propose safe
approximate policy and value iteration algorithms. They prove theories for their algorithms when
1
Under review as a conference paper at ICLR 2020
the CMDP is finite with known dynamics, and empirically evaluate them in more general settings.
However, their algorithms are value-function-based, and thus are restricted to discrete-action domains.
In this paper, we build on the problem formulation and theoretical findings of the Lyapunov-based
approach to solve CMDPs, and extend it to tackle continuous action problems that play an important
role in control theory and robotics. We propose Lyapunov-based safe RL algorithms that can handle
problems with large or infinite action spaces, and return safe policies both during training and at
convergence. To do so, there are two major difficulties that need to be addressed: 1) the policy update
becomes an optimization problem over the large or continuous action space (similar to standard
MDPs with large actions), and 2) the policy update is a constrained optimization problem in which
the (Lyapunov) constraints involve integration over the action space, and thus, it is often impossible to
have them in closed-form. Since the number of Lyapunov constraints is equal to the number of states,
the situation is even more challenging when the problem has a large state space. To address the first
difficulty, we switch from value-function-based to policy gradient (PG) algorithms. To address the
second difficulty, we propose two approaches to solve our constrained policy optimization problem
(a problem with infinite constraints, each involving an integral over the continuous action space)
that can work with any standard on-policy (e.g., proximal policy optimization (PPO) (Schulman
et al., 2017)) and off-policy (e.g., deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016))
PG algorithm. Our first approach, which we call policy parameter projection or θ-projection, is
a constrained optimization method that combines PG with a projection of the policy parameters
onto the set of feasible solutions induced by the Lyapunov constraints. Our second approach, which
we call action projection or a-projection, uses the concept of a safety layer introduced by (Dalal
et al., 2018) to handle simple single-step constraints, extends this concept to general trajectory-
based constraints, solves the constrained policy optimization problem in closed-form using Lyapunov
functions, and integrates this closed-form into the policy network via safety-layer augmentation. Since
both approaches guarantee safety at every policy update, they manage to maintain safety throughout
training (ignoring errors resulting from function approximation), ensuring that all intermediate
policies are safe to be deployed. To prevent constraint violations due to function approximation
errors, similar to CPO, we offer a safeguard policy update rule that decreases constraint cost and
ensures near-constraint satisfaction.
Our proposed algorithms have two main advantages over CPO. First, since CPO is closely connected
to TRPO, it can only be trivially combined with PG algorithms that are regularized with relative
entropy, such as PPO. This restricts CPO to on-policy PG algorithms. On the contrary, our algorithms
can work with any on-policy (e.g., PPO) and off-policy (e.g., DDPG) PG algorithm. Having an
off-policy implementation is beneficial, since off-policy algorithms are potentially more data-efficient,
as they can use the data from the replay buffer. Second, while CPO is not a back-propagatable
algorithm, due to the backtracking line-search procedure and the conjugate gradient iterations for
computing natural gradient in TRPO, our algorithms can be trained end-to-end, which is crucial for
scalable and efficient implementation (Hafner et al., 2017). In fact, we show in Section 3.1 that CPO
(minus the line search) can be viewed as a special case of the on-policy version (PPO version) of our
θ-projection algorithm, corresponding to a specific approximation of the constraints.
We evaluate our algorithms and compare them with CPO and the Lagrangian method on several
continuous control (MuJoCo) tasks and a real-world robot navigation problem, in which the robot
must satisfy certain constraints, while minimizing its expected cumulative cost. Results show that our
algorithms outperform the baselines in terms of balancing the performance and constraint satisfaction
(during training), and generalize better to new and more complex environments.
2	Preliminaries
We consider the RL problem in which the agent’s interaction with the environment is modeled as a
Markov decision process (MDP). A MDP is a tuple (X , A, γ, c, P, x0), where X and A are the state
and action spaces; γ ∈ [0, 1) is a discounting factor; c(x, a) ∈ [0, Cmax] is the immediate cost function;
P(∙∣χ, a) is the transition probability distribution; and xo ∈ X is the initial state. Although We consider
deterministic initial state and cost function, our results can be easily generalized to random initial
states and costs. We model the RL problems in Which there are constraints on the cumulative cost
using CMDPs. The CMDP model extends MDP by introducing additional costs and the associated
constraints, and is defined by (X, A, γ, c, P, x0 , d, d0), Where the first six components are the same as
in the unconstrained MDP; d(x) ∈ [0, Dmax] is the (state-dependent) immediate constraint cost; and
d0 ∈ R≥0 is an upper-bound on the expected cumulative constraint cost.
To formalize the optimization problem associated With CMDPs, let ∆ be the set of Markovian
stationary policies, i.e., ∆ = {∏ : X × A → [0,1], Pa ∏(a∣x) = 1}. At each state X ∈ X, we
define the generic Bellman operator W.r.t. a policy π ∈ ∆ and a cost function h as Tπ,h [V](x) =
Pa∈A π(a∣x) [h(x, a) + Y Pχθ∈χ P(x0∣x, a) V(x0)]. Given a policy ∏ ∈ ∆, we define the expected
2
Under review as a conference paper at ICLR 2020
cumulative cost and the safety constraint function (expected cumulative constraint cost) as Cπ (x0) :=
E[Pt∞=0 γtc(xt, at) | π, x0] and Dπ(x0) := E[Pt∞=0 γtd(xt) | π, x0], respectively. The safety constraint
is then defined as Dπ (x0) ≤ d0. The goal in CMDPs is to solve the constrained optimization problem
π* ∈ arg min {C∏(xo) : Dn(xo) ≤ do} .	(1)
π∈∆
It has been shown that if the feasibility set is non-empty, then there exists an optimal policy in the
class of stationary Markovian policies ∆ (Altman, 1999, Theorem 3.1).
2.1	Policy Gradient Algorithms
Policy gradient (PG) algorithms optimize a policy by computing a sample estimate of the gradient
of the expected cumulative cost induced by the policy, and then updating the policy parameter
in the gradient direction. In general, stochastic policies that give a probability distribution over
actions are parameterized by a κ-dimensional vector θ, so the space of policies can be written
as {πθ , θ ∈ Θ ⊂ Rκ }. Since in this setting a policy π is uniquely defined by its parameter θ,
policy-dependent functions can be written as a function of θ or π interchangeably.
DDPG (Lillicrap et al., 2016) and PPO (Schulman et al., 2017) are two PG algorithms that have
recently gained popularity in solving continuous control problems. DDPG is an off-policy Q-learning
style algorithm that jointly trains a deterministic policy πθ (x) and a Q-value approximator Q(x, a; φ).
The Q-value approximator is trained to fit the true Q-value function and the deterministic policy
is trained to optimize Q(x, πθ (x); φ) via chain-rule. The PPO algorithm we use in this paper is a
penalty form of TRPO (Schulman et al., 2015a) with an adaptive rule to tune the DKL penalty weight
βk . PPO trains a policy πθ (x) by optimizing a loss function that consists of the standard policy
gradient objective and a penalty on the KL-divergence between the current θ and previous θ0 policies,
i.e., Dkl(Θ,Θ0) = E[Pt YtDkl(∏θo(∙E)∣∣∏θ(∙∣xt))∣∏θo,χo].
2.2	Lagrangian Method
Lagrangian method is a straightforward way to address the constraint Dπθ (x0) ≤ d0 in CMDPs. La-
grangian method adds the constraint costs d(x) to the task costs c(x, a) and transform the constrained
optimization problem to a penalty form, i.e., minθ∈θ maχλ≥o E[P∞=o c(χt, at) + λd(xt)∣∏θ,xo] - λdo.
The method then jointly optimizes θ and λ to find a saddle-point of the penalized objective. The
optimization of θ may be performed by any PG algorithm on the augmented cost c(x, a) + λd(x),
while λ is optimized by stochastic gradient descent. As described in Sec. 1, although the Lagrangian
approach is easy to implement (see Appendix A for the details), in practice, it often violates the
constraints during training. While at each step during training, the objective encourages finding a
safe solution, the current value of λ may lead to an unsafe policy. This is why the Lagrangian method
may not be suitable for solving problems in which safety is crucial during training.
2.3	Lyapunov Functions
Since in this paper, we extend the Lyapunov-based approach to CMDPs of (Chow et al., 2018) to PG
algorithms, we end this section by introducing some terms and notations from (Chow et al., 2018)
that are important in developing our safe PG algorithms. We refer readers to Appendix B for details.
We define a set of Lyapunov functions w.r.t. initial state x0 ∈ X and constraint threshold d0 as
LπB(x0,d0) = {L : X → R≥0 | TπB,d[L](x) ≤ L(x), ∀x ∈ X, L(x0) ≤ d0}, where πB is a
feasible policy of (1), i.e., DπB (x0) ≤ d0. We refer to the constraints in this feasibility set as
the Lyapunov constraints. For an arbitrary Lyapunov function L ∈ LπB (x0 , d0), we denote by
FL = {∏ ∈ ∆ : T∏,d[L](x) ≤ L(x), ∀x ∈ X}, the set of L-induced Markov stationary policies. The
contraction property of Tπ,d, together with L(x0) ≤ d0, imply that any L-induced policy in FL is
a feasible policy of (1). However, FL(x) does not always contain an optimal solution of (1), and
thus, it is necessary to design a Lyapunov function that provides this guarantee. In other words,
the main goal of the Lyapunov approach is to construct a Lyapunov function L ∈ LπB (x0, d0), such
that FL contains an optimal policy ∏*, i.e., L(X) ≥ T∏* ,d[L](χ). Chow et al. (2018) show in their
Theorem 1 that without loss of optimality, the Lyapunov function that satisfies the above criterion
can be expressed as LnB ,e(χ) := E[P∞=0 γt(d(χ±) + c(χt)) | ∏b , x], in which e(x) ≥ 0 is a specific
immediate auxiliary constraint cost that keeps track of the maximum constraint budget available for
policy improvement (from ∏b to ∏*). They propose ways to construct such e, as well as an auxiliary
constraint cost surrogate e, which is a tight upper-bound on and can be computed more efficiently.
They use this construction to propose the safe (approximate) policy and value iteration algorithms,
whose objective is to solve the following LP (Chow et al., 2018, Eq. 6) during policy improvement:
∏+(∙∣x) = arg min
n∈∆
/
a∈A
QVnB (x,a)∏(a∣x),
s.t.
/
a∈A
QLnB (x,a) (∏(a∣x) - ∏B (a∣x)) ≤ e(x),
(2)
3
Under review as a conference paper at ICLR 2020
where VnB(x) = T∏b,c[V∏b](x) and QVnB (χ,a) = c(χ,a) + YPxo P(Xlx,a)V∏B(χ0) are the value
and state-action value functions (w.r.t. the cost function c), and QLπ (x, a) = d(x) + e(x) +
γ Px0 P (x0 |x, a)LπB ,e(x0 ) is the Lyapunov function. In any iterative policy optimization method,
such as those studied in this paper, the feasible policy πB at each iteration can be set to the policy
computed at the previous iteration (which is feasible).
In LP (2), there are as many constraints as the number of states and each constraint involves an integral
over the entire action space. When the state space is large, even if the integral in the constraint has a
closed-form (e.g., for finite actions), solving (2) becomes numerically intractable. Chow et al. (Chow
et al., 2018) assumed that the number of actions is finite and focused on value-function-based RL
algorithms, and addressed the large state issue by policy distillation. Since in this paper, we are
interested in problems with large action spaces, solving (2) will be even more challenging. To address
this issue, in the next section, we first switch from value-function-based algorithms to PG algorithms,
then propose an optimization problem with Lyapunov constraints, analogous to (2), that is suitable
for PG, and finally present two methods to solve our proposed optimization problem efficiently.
3	Safe Lyapunov-based Policy Gradient
We now present our approach to solve CMDPs in a way that guarantees safety both at convergence
and during training. Similar to (Chow et al., 2018), our Lyapunov-based safe PG algorithms solve
a constrained optimization problem analogous to (2). In particular, our algorithms consist of two
components, a baseline PG algorithm, such as DDPG or PPO, and an effective method to solve the
general Lyapunov-based policy optimization problem, the analogous to (2), i.e,
θ+ = arg min Cπθ (x0),
θ∈Θ
s.t.
/
a∈A
πθ (a|x) - πB (a|x) QLπB (x, a) da ≤ e(x),
∀x ∈ X .
(3)
In the next two sections, we present two approaches to solve (3) efficiently. We call these approaches
1) θ-projection, a constrained optimization method that combines PG with projecting the policy
parameter θ onto the set of feasible solutions induced by the Lyapunov constraints, and 2) a-projection,
in which we embed the Lyapunov constraints into the policy network via a safety layer.
3.1	THE θ-PROJECTION APPROACH
The θ-projection approach is based on the minorization-maximization technique in conservative
PG (Kakade & Langford, 2002) and Taylor series expansion, and can be applied to both on-
policy and off-policy algorithms. Following Theorem 4.1 in (Kakade & Langford, 2002), We
first have the following bound for the cumulative cost: -BDkl(Θ,Θb) ≤ C∏θ(χo) - C∏θb (χo)-
Ex〜μθB,xo,a^∏θ[QVθB (χ,a) - ⅞b(x)] ≤ B^Dkl(Θ,Θb), where μθB刖 is the γ-visiting distribution of
πθB starting at the initial state x0, and β is the weight for the entropy-based regularization.1 * Using this
result, we denote by Cn@ (xo； ∏θb) = C∏θr (xo) + βDκL(θ, Θb) + Ex〜μθB,xo ,a〜∏θ [Qv@b (x, a) — V‰ (x)]
the surrogate cumulative cost. It has been shown in Eq. 10 of (Schulman et al., 2015a) that replacing
the objective function Cnθ (x0) with its surrogate Cn0 θ (x0； πθB) in solving (3) will still lead to policy
improvement. In order to effectively compute the improved policy parameter θ+, one further approxi-
mates the function Cn0 θ (x0； πθB) with its Taylor series expansion around θB. In particular, the term
Ex〜μθB,xcι ,a-∏θ [Qvθb (x, a) — VΘb (x)] is approximated UP to its first order, and the term Dkl(Θ, Θb)
is approximated up to its second order. These altogether allow us to replace the objective function
in (3) with h(θ — Θb), VθEx〜μθB,χ0,a~∏θ[QvθB (x,a)]i + 2h(θ — Θb),大Dkl(Θ,Θb) ∣θ=θb (θ — Θb)〉.
Similarly, regarding the constraints in (3), we can use the Taylor series expansion (around θB)
to approximate the LHS of the Lyapunov constraints as Ra∈a(∏θ(a∣x) — ∏b (a∣x)) Ql(x, a) da ≈
h(θ — Θb), VθEa〜∏θ[Qlθr (x,a)] ∣θ=θbi. Using the above approximations, at each iteration, our safe
PG algorithm updates the policy by solving the following constrained optimization problem with
semi-infinite dimensional Lyapunov constraints:
θ+ ∈ arg min <(θ — θB ), vθ Ex 〜μθB X0 ,a 〜∏θ [QVθB (x, a)]) + 以(θ — θB ), v2DKL(θ, θB ) ∣Θ=Θb (θ — θB )),
θ∈Θ	B, 0	B	2
s.t. <(θ — Θb ), Vθ Ea 〜∏θ [Qlθb (x,a)] ∣θ=θB〉≤ e(x), ∀x ∈X.
(4)
It can be seen that if the errors resulted from the neural network parameterizations of QVθ and QLθ ,
and the Taylor series expansions are small, then an algorithm that updates the policy parameter by
solving (4) can ensure safety during training. However, the presence of infinite-dimensional Lyapunov
constraints makes solving (4) intractable. A solution to this is to write the Lyapunov constraints in (4)
(without loss of optimality) as maχx∈χh(θ — Θb), VθEa〜∏θ[Qlθr (x,a)] ∣θ=θbi — e(x) ≤ 0. Since
1Theorem 1 in (Schulman et al., 2015a) provides a recipe for computing β such that the minorization-
maximization inequality holds. But in practice, β is treated as a tunable parameter for entropy regularization.
4
Under review as a conference paper at ICLR 2020
the above max-operator is non-differentiable, this may still lead to numerical instability in gradient
descent algorithms. Similar to the surrogate constraint in TRPO(to transform the max DKL constraint
to an average DKL constraint, see Eq. 12 in (Schulman et al., 2015a)), a more numerically stable way
is to approximate the Lyapunov constraint using the average constraint surrogate
1M	1M
<(θ- Θb),MM X VθEa~∏θ[QLθB (Xi,a)] ∣θ=θB >≤ MM Xe(Xi),	(5)
i=1	i=1
where M is the number of on-policy sample trajectories of πθB . In order to effectively compute the
gradient of the Lyapunov value function, consider the special case when the auxiliary constraint sur-
rogate is chosen as e = (1 - γ)(d0 - Dπθ (x0)) (see Appendix B for justification). Using the fact that
e is θ-independent, the gradient term in (5) can be written as Ja ∏θ(a|x) Vθ log ∏θ(a∣χ) Qwθb (χi,a)da,
where WθB (x) = TπB,d [WθB](x) and QWθ (x, a) = d(x) + γPx0 P(x0|x, a)WθB (x0) are the con-
straint value functions, respectively. Since the integral is equal to Ea^∏θQwθe (xi, a)], the average
constraint surrogate (5) can be approximated (approximation is because of the choice of e) by the
inequality D∏fjβ (xo) + 七 h(θ — Θb ),告 PM=iVθ Ea 〜∏θ[Qwθb (χi,a)]∣θ=θB i≤ do, which is equivalent
to the constraint used in CPO (see Section 6.1 in (Achiam et al., 2017)). This shows that CPO (minus
the line search) belongs to the class of our Lyapunov-based PG algorithms with θ-projection. We
refer to the DDPG and PPO versions of our θ-projection safe PG algorithms as SDDPG and SPPO.
Derivation details and the pseudo-code (Algorithm 4) of these algorithms are given in Appendix C.
3.2 THE a-PROJECTION APPROACH
The main characteristic of the Lyapunov approach is to break down a trajectory-based constraint into
a sequence of single-step state dependent constraints. However, when the state space is infinite, the
feasibility set is characterized by infinite dimensional constraints, and thus, it is counter-intuitive to
directly enforce these Lyapunov constraints (as opposed to the original trajectory-based constraint)
into the policy update optimization. To address this, we leverage the idea of a safety layer from (Dalal
et al., 2018), that was applied to simple single-step constraints, and propose a novel approach to
embed the set of Lyapunov constraints into the policy network. This way, we reformulate the CMDP
problem (1) as an unconstrained optimization problem and optimize its policy parameter θ (of the
augmented network) using any standard unconstrained PG algorithm. At every given state, the
unconstrained action is first computed and then passed through the safety layer, where a feasible
action mapping is constructed by projecting unconstrained actions onto the feasibility set w.r.t.
Lyapunov constraints. This constraint projection approach can guarantee safety during training.
We now describe how the action mapping (to the set of Lyapunov constraints) works2. Recall from
the policy improvement problem in (3) that the Lyapunov constraint is imposed at every state x ∈ X .
Given a baseline feasible policy πB = πθB , for any arbitrary policy parameter θ ∈ Θ, we denote
by Ξ(πB , θ) = {θ0 ∈ Θ : QLπ (x, πθ0 (x)) — QLπ (x, πB (x)) ≤ e(x), ∀x ∈ X}, the projection of θ
onto the feasibility set induced by the Lyapunov constraints. One way to construct a feasible policy
∏ξ(∏b ,θ) from a parameter θ is to solve the following '2-projection problem:
12
∏ξ(∏b ,θ)(x) ∈ arg min 5∣∣a 一 ∏θ(x)k , s.t. QLnB (x,a) - QLnB (x,∏B (x)) ≤ e(x).	(6)
a∈A 2	B	B
We refer to this operation as the Lyapunov safety layer. Intuitively, this projection perturbs the
unconstrained action as little as possible in the Euclidean norm in order to satisfy the Lyapunov
constraints. Since this projection guarantees safety, if we have access to a closed form of the
projection, we may insert it into the policy parameterization and simply solve an unconstrained policy
optimization problem, i.e., θ+ ∈ arg minθ∈Θ CπΞ(n ,θ) (x0), using any standard PG algorithm.
To simplify the projection (6), we can approximate the LHS of the Lyapunov constraint with its
first-order Taylor series (w.r.t. action a = πB (x)). Thus, at any given state x ∈ X, the safety layer
solves the following projection problem:
∏Ξ(∏B,θ)(x) ∈ argmin 1 7(X) ||a一∏θ(x)∣∣2 + η(x)||a—∏b(x)k2, s.t. (a—∏b(x))>gL∏R(x) ≤ e(x), (7)
a∈A 2	2
where η(X) ∈ [0, 1) is the mixing parameter that controls the trade-off between projecting on un-
constrained policy (for return maximization) and on baseline policy (for safety), and gLn (X) :=
VaQLnB (x，a) ∣a=∏B(χ) is the action-gradient of the state-action Lyapunov function.
Similar to the analysis of Section 3.1, if the auxiliary cost e is state-independent, one can readily find
gLnβ (x) by computing the gradient of the constraint action-value function VaQwθβ (x, a) ∣a=πB(x).
2In our experiments, we use stochastic (Gaussian) policies with parameterized mean and fixed variance. We
leave extension of the a-projection approach to policies in which variance is also parameterized as future work.
5
Under review as a conference paper at ICLR 2020
Figure 1: DDPG (red), DDPG-Lagrangian (cyan), SDDPG (blue), SDDPG a-projection (green) on HalfCheetah-
Safe and Point-Gather. SDDPG and SDDPG a-projection perform stable and safe learning, although the dynamics
and cost functions are unknown, control actions are continuous, and deep function approximations are used. Unit
of x-axis is in thousands of episodes. Shaded areas represent the 1-SD confidence intervals (over 10 random
seeds). The dashed purple line in the two right figures represents the constraint limit.
(d) Point-Gather, Constraint
Note that the objective function in (7) is positive-definite and quadratic, and the constraint approxima-
tion is linear. Therefore, the solution of this (convex) projection problem can be effectively computed
by an in-graph QP-solver, such as OPT-Net (Amos & Kolter, 2017). Combined with the above
projection procedure, this further implies that the CMDP problem can be effectively solved using an
end-to-end PG training pipeline (such as DDPG or PPO). When the CMDP has a single constraint
(and thus a single Lyapunov constraint), the policy πΞ(πB,θ)(x) has the following analytical solution.
Proposition 1. At any given state x ∈ X, the solution to the optimization problem (7)
has the form ∏ξ(∏b,θ)(x) = (1 - η(x))∏θ(x) + η(χ)∏B(x) - λ*(x) ∙ gLπB (x), where λ*(x)=
(((I -η (X)) ∙ gL∏B (X)T (πθ (X)-πB (X))- e(X))∕gL∏B (X)T gL∏B (X)) +.
The closed-form solution is essentially a linear projection of the unconstrained action πθ (x) onto
the Lyapunov-safe hyper-plane with slope gLπ (x) and intercept e(x) = (1 - γ)(d0 - DπB (x0)). It is
possible to extend this closed-form solution to handle multiple constraints, if there is at most one
constraint active at a time (see Proposition 1 in (Dalal et al., 2018)).We refer to the DDPG and PPO
versions of our a-projection safe Lyapunov-based PG algorithms as SDDPG a-projection and SPPO
a-projection. Derivation and pseudo-code (Algorithm 5) of these algorithms are in Appendix C.
4	Experiments on MuJoCo Benchmarks
We empirically evaluate3 4 our Lyapunov-based safe PG algorithms to assess their: (i) performance
in terms of cost and safety during training, and (ii) robustness w.r.t. constraint violation. We use
three simulated robot locomotion continuous control tasks in the MuJoCo simulator (Todorov et al.,
2012). The notion of safety in these tasks is motivated by physical constraints: (i) HalfCheetah-Safe:
this is a modification of the MuJoCo HalfCheetah problem in which We impose constraints on the
speed of Cheetah in order to force it to run smoothly. The video shows that the policy learned by
our algorithm results in slower but much smoother movement of Cheetah compared to the policies
learned by PPO and Lagrangian4; (ii) Point-Circle: the agent is rewarded for running in a wide circle,
but is constrained to stay within a safe region defined by |x| ≤ Xlim; (iii) POint-Gather & Ant-Gather:
the agent is rewarded for collecting target objects in a terrain map, while being constrained to avoid
bombs. The last two tasks were first introduced in (Achiam et al., 2017) by adding constraints to the
original MuJoCo tasks: Point and Ant. Details of these tasks are given in Appendix D.
We compare our algorithms with two state-of-the-art unconstrained algorithms, DDPG and PPO,
and two constrained methods, Lagrangian with optimized Lagrange multiplier (Appendix A) and
on-policy CPO. We use the CPO algorithm that is based on PPO (unlike the original CPO that is
based on TRPO) and coincides with our SPPO algorithm derived in Section 4.1. SPPO preserves
the essence of CPO by adding the first-order constraint and relative entropy regularization to the
policy optimization problem. The main difference between CPO and SPPO is that the latter does not
perform backtracking line-search in learning rate. We compare with SPPO instead of CPO to 1) avoid
the additional computational complexity of line-search in TRPO, while maintaining the performance
of PG using PPO, 2) have a back-propagatable version of CPO, and 3) have a fair comparison with
other back-propagatable safe PG algorithms, such as our DDPG and a-projection based algorithms.
Comparison with baselines: Figures 1a, 1b, 2a, 2b, 8a, 8b, 9a, 9b show that our Lyapunov-based
PG algorithms are stable in learning and all converge to feasible policies with reasonable performance.
Figures 1c, 1d, 2c, 2d, 8c, 8d, 9c, 9b show the algorithms in terms of constraint violation during
3Videos of MuJoCo experiments can be found at https://drive.google.com/file/d/
1FwbuEnKN2lLWFMKvDCydo2EQVdc14O1a/view?usp=sharing.
4We also imposed constraint on the torque at the Cheetah’s joints in order to force it to run more smoothly
and obtained similar results as imposing constraint on its speed.
6
Under review as a conference paper at ICLR 2020
(a) HalfCheetah-Safe, Return
(c) HalfCheetah-Safe, Con-
straint
(d) Point-Gather, Constraint
Figure 2: PPO (red), PPO-Lagrangian (cyan), SPPO (blue), SPPO a-projection (green) on HalfCheetah-Safe
and Point-Gather. SPPO a-projection perform stable and safe learning, when the dynamics and cost functions
are unknown, control actions are continuous, and deep function approximation is used.
(a) Noisy Lidar observation in a corridor
"G Reward: Distance to reach goal -
Constraint: Total energy on collision
Observations: Noisy 1D lidar + Goal + Robot orientation
(a) Navigation, Success %
(b) Navigation, Constraint
Figure 4: DDPG (red), DDPG-Lagrangian (cyan),
SDDPG (blue), DDPG a-projection (green) on Robot
Navigation. Ours (SDDPG, SDDPG a-projection)
balance between reward and constraint learning. Unit
of x-axis is in thousands of steps. The shaded areas
represent the 1-SD confidence intervals (over 50 runs).
The dashed purple line represents the constraint limit.
(b) SDDPG for point to point task
Figure 3: Robot navigation task details.
training. These figures indicate that our algorithms quickly stabilize the constraint cost below the
threshold, while the unconstrained DDPG and PPO violate the constraints, and Lagrangian tends to
jiggle around the threshold. Moreover, it is worth-noting that the Lagrangian method can be sensitive
to the initialization of the Lagrange multiplier λ0 . If λ0 is too large, it would make policy updates
overly conservative, and if it is too small, then we will have more constraint violation. Without further
knowledge about the environment, we treat λ0 as a hyper-parameter and optimize it via grid-search.
See Appendix D for more details and for the experimental results of Ant-Gather and Point-Circle.
a-projection vs. θ-projection: The figures indicate that in many cases DDPG and PPO with a-
projection converge faster and have lower constraint violation than their θ-projection counterparts
(i.e., SDDPG and SPPO). This corroborates with the hypothesis that a-projection is less conservative
during policy updates than θ-projection (which is what CPO is based on) and generates smoother
gradient updates during end-to-end training.
DDPG vs. PPO: In most experiments (HalfCheetah, PointGather, and AntGather) the DDPG algo-
rithms tend to have faster learning than their PPO counterparts, while the PPO algorithms perform
better in terms of constraint satisfaction. The faster learning behavior is due to the improved data-
efficiency when using off-policy samples in PG, however, the covariate-shift 5 in off-policy data
makes tight constraint control more challenging.
5	Safe Policy Gradient for Robot Navigation
We now evaluate safe policy optimization algorithms on a real robot task - a map-less navigation
task (Chiang et al., 2019) - where a noisy differential drive robot with limited sensors (Fig. 3a) is
required to navigate to a goal outside of its field of view in unseen environments while avoiding
collision. The main goal is to learn a policy that drives the robot to goal as efficiently as possible, while
limiting the impact energy of collisions, since the collision can damage the robot and environment.
Here the CMDP is non-discounting and has a fixed horizon. The agent’s observations consist of
the relative goal position, agent’s velocity, and Lidar measurements (Fig. 3a). The actions are the
linear and angular velocity at the robot’s center of the mass. 6 The transition probability captures the
noisy robot’s dynamics, whose exact formulation is unknown to the robot. The robot must navigate
5Here covariate shift is due to the fact that training data is generated by a policy that is different from the
current policy that is being optimized.
6The first dimension is the robot’s desired linear velocity (speed at which the robot should go straight). The
second dimension is the robot’s angular velocity - speed at which the robot should turn. Both velocity vectors
are applied on the center of the mass of the robot.
7
Under review as a conference paper at ICLR 2020
(a) Lagrangian
(b) SDDPG a-
projection
(c) SDDPG a-
projection
(a) Navigation, Success %
Figure 6: Generalization over success rate (d) and
constraint satisfaction (e) on a different environment.
The average success rate and cumulative (trajectory-
based) constraint cost are over 100 tasks (randomly
sampled start and goal robot positions). A task is con-
sidered successful if the robot reaches the goal in the
map-less navigation task, regardless of the constraint.
Goal Distance Ranqe (m)
(b) Navigation, Constraint
Figure 5: Navigation routes of two learned policies
in the simulator (a) and (b). On-robot experiment (c).
to arbitrary goal positions collision-free in a previously unseen environment, and without access to
the indoor map and any work-space topology. We reward the agent for reaching the goal, which
translates to an immediate cost that measures the relative distance to the goal. To measure the total
impact energy of obstacle collisions, we impose an immediate constraint cost to account for the speed
during collision, with a constraint threshold d0 that characterizes the agent’s maximum tolerable
collision impact energy to any object. Different from the standard approach, where a constraint on
collision speed is explicitly imposed to the learning problem at each time step, we emphasize that a
CMDP constraint is required here because it allows the robot to lightly brush off the obstacle (such as
walls) but prevent it from ramming into any objects. Other use cases of CMDP constraints in robot
navigation include collision avoidance (Pfeiffer et al., 2018) or limiting total battery usage of the task.
Experimental Results: We evaluate the learning algorithms on success rate and constraint control
averaged over 100 episodes with random initialization. The task is successful if the robot reaches
the goal before the constraint threshold (total energy of collision) is exhausted. While all methods
converge to policies with reasonable performance, Figure 4a and 4b show that the Lyapunov-based PG
algorithms have higher success rates, due to their robust abilities of controlling the total constraint, as
well minimizing the distance to goal. Although the unconstrained method often yields a lower distance
to goal, it violates the constraint more frequently leading to a lower success rate. Lagrangian approach
is less robust to initialization of parameters, and therefore it generally has lower success rate and higher
variability than the Lyapunov-based methods. Unfortunately due to function approximation error
and stochasticity of the problem, all the algorithms converged pre-maturely with constraints above
the threshold, possibly due to the overly conservative constraint threshold (d0 = 100). Inspection
of trajectories shows that the Lagrangian method tends to zigzag and has more collisions, while the
SDDPG chooses a safer path to reach the goal (Figures 5a and 5b).
Next, we evaluate how well the methods generalize to (i) longer trajectories, and (ii) new environments.
The tasks are trained in a 22 by 18 meters environment (Fig. 7) with goals placed within 5 to 10
meters from the robot initial state. In a much larger evaluation environment (60 by 47 meters) with
goals placed up to 15 meters away from the goal, the success rate of all methods degrades as the
goals are further away (Fig. 6a). The safety methods (a-projeCtion - SL-DDPG, and θ-projeCtion -
SG-DDPG) outperform unconstrained and Lagrangian (DDPG and LA-DDPG), while retaining the
lower Constraints even when the task beComes more diffiCult (Fig. 6b).
Finally, we deployed the SL-DDPG poliCy onto the real FetCh robot (Wise et al., 2016) in an everyday
offiCe environment. 7 FetCh robot weights 150 kilograms, and reaChes maximum speed of 7 km/h
making the Collision forCe a safety paramount. Figure 5C shows the top down view of the robot log.
Robot travelled, through narrow Corridors and around people walking through the offiCe, for a total of
500 meters to Complete five repetitions of 12 tasks, eaCh averaging about 10 meters to the goal. The
robot robustly avoids both statiC and dynamiC (humans) obstaCles Coming into its path. We observed
additional ”wobbling” effeCts, that was not present in simulation. This is likely due to the wheel
slippage at the floor that the poliCy was not trained for. In several oCCasions when the robot Could not
find a Clear path, the poliCy instruCted the robot to stay put instead of narrowly passing by the obstaCle.
This is preCisely the safety behavior we want to aChieve with the Lyapunov-based algorithms.
6	Conclusions and Future Work
We used the notion of Lyapunov funCtions and developed a Class of safe RL algorithms for Continuous
aCtion problems. EaCh algorithm in this Class is a Combination of one of our two proposed projeCtions:
7Videos of FetCh robot navigation Can be found in the following link: https://drive.google.com/
file/d/1FwbuEnKN2lLWFMKvDCydo2EQVdc14O1a/view?usp=sharing
8
Under review as a conference paper at ICLR 2020
θ-projection and a-projection, with any on-policy (e.g., PPO) or off-policy (e.g., DDPG) PG algorithm.
We evaluated our algorithms on four high-dimensional simulated robot locomotion MuJoCo tasks
and compared them with several baselines. To demonstrate the effectiveness of our algorithms in
solving real-world problems, we also applied them to an indoor robot navigation problem, to ensure
that the robot’s path is optimal and collision-free. Our results indicate that our algorithms 1) achieve
safe learning, 2) have better data-efficiency, 3) can be more naturally integrated within the standard
end-to-end differentiable PG training pipeline, and 4) are scalable to tackle real-world problems. Our
work is a step forward in deploying RL to real-world problems in which safety guarantees are of
paramount importance. Future work includes 1) extending a-projection to stochastic policies and 2)
extensions of the Lyapunov approach to model-based RL and use it for safe exploration.
9
Under review as a conference paper at ICLR 2020
References
J. Achiam, D. Held, A. Tamar, and P. Abbeel. Constrained policy optimization. arXiv preprint
arXiv:1705.10528, 2017.
E. Altman. Constrained Markov decision processes with total cost criteria: Lagrangian approach and
dual linear program. Mathematical methods of operations research, 48(3):387-417, 1998.
E. Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
B. Amos and Z. Kolter. Optnet: Differentiable optimization as a layer in neural networks. arXiv
preprint arXiv:1703.00443, 2017.
F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learning
with stability guarantees. In Advances in Neural Information Processing Systems, pp. 908-918,
2017.
D. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
D. Bertsekas. Dynamic programming and optimal control, volume 1-2. Athena scientific Belmont,
MA, 2005.
H. Chiang, A. Faust, M., and A. Francis. Learning navigation behaviors end-to-end with autorl. IEEE
Robotics and Automation Letters, 4(2):2007-2014, 2019.
Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with
percentile risk criteria. Journal of Machine Learning Research, 18(1):6070-6120, 2017.
Y. Chow, O. Nachum, E. Duenez-Guzman, and M. Ghavamzadeh. A Lyapunov-based approach
to safe reinforcement learning. In Proceedings of Advances in Neural Information Processing
Systems 32, pp. 8103-8112, 2018.
G. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa. Safe exploration in
continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
A. Faust, P. Ruymgaart, M. Salman, R. Fierro, and L. Tapia. Continuous action reinforcement
learning for control-affine systems with unknown dynamics. Acta Automatica Sinica Special
Issue on Extensions of Reinforcement Learning and Adaptive Control, IEEE/CAA Journal of, 1(3):
323-336, July 2014.
A. Faust, O. Ramirez, M. Fiser, K. Oslund, A. Francis, J. Davidson, and L. Tapia. PRM-RL: Long-
range robotic navigation tasks by combining reinforcement learning and sampling-based planning.
In IEEE International Conference on Robot Automation, pp. 5113-5120, 2018.
P. Geibel and F. Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints.
Journal of Artificial Intelligence Research, 24:81-108, 2005.
D. Hafner, J. Davidson, and V. Vanhoucke. TensorFlow Agents: Efficient batched reinforcement
learning in tensorflow. arXiv preprint arXiv:1709.02878, 2017.
S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In ICML,
volume 2, pp. 267-274, 2002.
D. Kalashnikov, A. Irpan, P. Sampedro, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakr-
ishnan, V. Vanhoucke, and S. Levine. QT-Opt: Scalable deep reinforcement learning for vision-
based robotic manipulation. 2018. URL https://arxiv.org/pdf/1806.10293.
H. Khalil. Noninear systems. Prentice-Hall, New Jersey, 2(5):5-1, 1996.
S.	Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuo-motor policies.
Journal of Machine Learning Research, 17:1-40, 2016.
T.	Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. In International Conference on Learning Representations,
2016.
V.	Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
10
Under review as a conference paper at ICLR 2020
V.	Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller,
A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,
D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
T. Perkins and A. Barto. Lyapunov design for safe reinforcement learning. Journal of Machine
Learning Research, 3(Dec):803-832, 2002.
M. Pfeiffer, S. Shukla, M. Turchetta, C. Cadena, A. Krause, R. Siegwart, and J. Nieto. Reinforced
imitation: Sample efficient deep reinforcement learning for mapless navigation by leveraging prior
demonstrations. IEEE Robotics and Automation Letters, 3(4):4423-4430, 2018.
T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.
J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel. Trust region policy optimization. In
Proceedings of the 32nd International Conference on Machine Learning, pp. 1889-1897, 2015a.
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
D.	Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,
I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the
game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learn-
ing with function approximation. In Proceedings of Advances in Neural Information Processing
Systems 12, pp. 1057-1063, 2000.
E.	Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intelligent
Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-5033. IEEE,
2012.
M.	Wise, M. Ferguson, D. King, E. Diehr, and D. Dymesich. Fetch & Freight: Standard platforms for
service robot applications. In Workshop on Autonomous Mobile Service Robots held at the 2016
International Joint Conference on Artificial Intelligence, 2016.
11
Under review as a conference paper at ICLR 2020
A The Lagrangian Approach to Safe RL
We first state a number of mild technical and notational assumptions that we make throughout this
section.
Assumption 1 (Differentiability). For any state-action pair (x, a), πθ (a|x) is continuously differen-
tiable in θ and Vθ∏θ(a|x) is a Lipschitzfunction in θ for ^very X ∈ X and a ∈ A.
Assumption 2 (Strict Feasibility). There exists a transient policy ∏θ (∙∣x) such that D∏θ (x0) < do in
the constrained problem.
Assumption 3 (Step Sizes). The step size schedules {α3,k}, {α2,k}, and {α1,k} satisfy
α1,k =	α2,k =	α3,k = ∞,	(8)
kk	k	
Xα12,k,	Xα22,k,	α23,k < ∞,	(9)
kk	k	
α1,k = o α2,k ,	α2,k	= o α3,k .	(10)
Assumption 1 imposes smoothness on the optimal policy. Assumption 2 guarantees the existence of
a local saddle point in the Lagrangian analysis. Assumption 3 refers to step sizes corresponding to
policy updates and indicates that the update corresponding to {α3,k} is on the fastest time-scale, the
updates corresponding to {α2,k } is on the intermediate time-scale, and the update corresponding to
{α1,k } is on the slowest time-scale. As this assumption refers to user-defined parameters, they can
always be chosen to be satisfied.
To solve the CMDP, we employ the Lagrangian relaxation procedure (Bertsekas, 1999) to convert it
to the following unconstrained problem:
max min ( L(θ,λ) 4 C∏θ (xo) + λ ① ∏ (xo) - do) ),	(11)
λ≥0 θ
where λ is the Lagrange multiplier. Notice that L(θ, λ) is a linear function in λ. Then, there exists a
local saddle point (θ*, λ*) for the minimax optimization problem maxλ≥o minθ L(θ, λ), such that
for some r > 0, ∀θ ∈ RK ∩ Bg*(r), and ∀λ ∈ [0, λmaχ], We have
L(θ,λ*) ≥ L(θ*,λ*) ≥ L(θ*,λ),	(12)
where Bθ*(r) is a hyper-dimensional ball centered at θ* with radius r > 0.
In the folloWing, We present a policy gradient (PG) algorithm and an actor-critic (AC) algorithm.
While the PG algorithm updates its parameters after observing several trajectories, the AC algorithm
is incremental and updates its parameters at each time-step.
We now present a policy gradient algorithm to solve the optimization problem (11). The idea of the
algorithm is to descend in θ and ascend in λ using the gradients of L(θ, λ) w.r.t. θ and λ, i.e.,
vθL(θ,λ) = vθ (C∏θ (XO) + λD∏θ (XO)) ,	vλL(θ,λ) = D∏θ (XO)- do.	(13)
The unit of observation in this algorithm is a system trajectory generated by following the current
policy πθk . At each iteration, the algorithm generates N trajectories by following the current policy
πθk , uses them to estimate the gradients in (13), and then uses these estimates to update the parameters
θ, λ. Let ξ = {Xo, ao, co, X1, a1, c1, . . . , XT-1, aT-1, cT-1, XT} be a trajectory generated by follow-
ing the policy θ, where XT = XTar is the target state of the system and T is the (random) stopping
time. The cost, constraint cost, and probability of ξ are defined as C(ξ) = PkT=-o1 γkc(Xk, ak),
D(ξ) = PT-O γkd(xk), and Pθ(ξ) = Po(xo) QT-； ∏θ(ak∣Xk)P(xk+ι∣Xk", respectively.
Based on the definition of Pθ(ξ), one obtains Vθ log Pθ(ξ) = PT- V log∏θ(ak∣Xk).
Algorithm 1 contains the pseudo-code of our proposed PG algorithm. What appears inside the
parentheses on the right-hand-side of the update equations are the estimates of the gradients of
L(θ, λ) w.r.t. θ, λ (estimates of the expressions in (13)). Gradient estimates of the Lagrangian
function are given by
VθL(θ, λ) = X Pθ(ξ) ∙Vθ log Pθ(ξ) (C∏θ(ξ) + λD∏θ(ξ)), VλL(θ, λ) = -do + X Pθ(ξ) ∙D(ξ),
12
Under review as a conference paper at ICLR 2020
Algorithm 1 Lagrangian Trajectory-based Policy Gradient Algorithm
Input: parameterized policy π3∙;θ)
Initialization: policy parameter θ = θ0 , and the Lagrangian parameter λ = λ0
for i = 0, 1, 2, . . . do
for j = 1, 2, . . . do
Generate N trajectories {ξj,i}jN=1 by starting at x0 and following the policy θi.
end for θ Update: θi+ι = θi - α2,iNN X Vθ log Pθ(ξj,i)∣θ=g (C(ξj,i) + λiD(ξj,i))
j=1
λ Update: " = Γλ λi + αι,i ( - do + N X D(ξj,i))
end for	j=1
where the likelihood gradient is
(T-1
Elog P (xk+ι∣Xk ,ak) + log ∏θ (ak |xk )+log1{x0 = x0}
k=0
T-1	T-1	1
=E Vθ log∏θ(ak|xk) = E -,-1―yVθ∏θ(ak|xk).
k=0	k=0 πθ(akIxk)
T * 1	∙ . <	∙	. . Γz-> ʌ	1	♦	11 ʌ	ʌ 11 9
In Algorithm 1, Γλ is a projection operator to [0, λmaχ], i.e., Γλ(X) = argmm^∈[0 入	]∣∣λ 一 λ∣∣2,
which ensures the convergence of the algorithm. Recall from Assumption 3 that the step-size
schedules satisfy the standard conditions for stochastic approximation algorithms, and ensure that the
policy parameter θ update is on the fast time-scale {α2,i}, and the Lagrange multiplier λ update is
on the slow time-scale {α1,i}. This results in a two time-scale stochastic approximation algorithm
that has been shown to converge to a (local) saddle point of the objective function L(θ, λ). This
convergence proof makes use of standard results in stochastic approximation theory, because in
the limit when the step-size is sufficiently small, analyzing the convergence of PG is equivalent to
analyzing the stability of an ordinary differential equation (ODE) w.r.t. its equilibrium point.
In PG, the unit of observation is a system trajectory. This may result in high variance for the gradient
estimates, especially when the length of the trajectories is long. To address this issue, we propose two
actor-critic algorithms that use value function approximation in the gradient estimates and update the
parameters incrementally (after each state-action transition). We present two actor-critic algorithms
for optimizing (11). These algorithms are still based on the above gradient estimates. Algorithm 2
contains the pseudo-code of these algorithms. The projection operator ΓΛ is necessary to ensure the
convergence of the algorithms. Recall from Assumption 3 that the step-size schedules satisfy the
standard conditions for stochastic approximation algorithms, and ensure that the critic update is on
the fastest time-scale α3,k , the policy update α2,k is on the intermediate timescale, and finally
the Lagrange multiplier update is on the slowest time-scale α1,k . This results in three time-scale
stochastic approximation algorithms.
Using the PG theorem from (Sutton et al., 2000), one can show that
1
1 一 Y
VθL(θ, λ) = VθVθ(x0)
^μθ(x, a∣xo) V log∏(a∣x) Qθ(x, a),
x,a
(20)
where μθ is the discounted visiting distribution and Qθ is the action-value function of policy θ. We
can show that y-γ V log ∏θ(ak∣xk) ∙ δk is an unbiased estimate of VθL(θ, λ), where
c	/	∖,τV∕	∖	-∖r(	∖
δk = cλ(xk,ak) +γVθ(xk+1) 一 Vθ(xk)
is the temporal-difference (TD) error, and V⅞ is an estimator of the value function %.
Traditionally, for convergence guarantees in actor-critic algorithms, the critic uses linear approxima-
tion for the value function Vq(x) ≈ v>ψ(x) = V⅞,v (x), where the feature vector ψ(∙) belongs to a
low-dimensional space Rκ2. The linear approximation V⅞,v belongs to a low-dimensional subspace
13
Under review as a conference paper at ICLR 2020
Algorithm 2 Lagrangian Actor-Critic Algorithm
Input: Parameterized policy ∏(∙∣∙; θ) and value function feature vector φ(∙)
Initialization: policy parameters θ = θ0 ; Lagrangian parameter λ = λ0 ; value function weight v = v0
while TRUE do
for k = 0, 1, 2, . . . do
Sample ak 〜π(∙∣Xk; θk); cλk (xk,ak) = c(xk,ak) + λkd(xk); Xk+ι 〜P(∙∣Xk,ak);
// AC Algorithm:
TD Error:	δk(vk)	= cλk(xk,ak) + γVφk(xk+1) — Vφk (xk)	(14)
Critic Update:	vk+1 =	vk + ζ3 (k)δk (vk)ψ(xk)	(15)
θ Update:	θk+1 =	θk — Z2(k)Vθ log∏θ(ak|xk) ∙ δk(vk)/1 — Y	(16)
λ Update: // NAC Algorithm:	λk+1 =	N 二 γλ (λk + Q(k)( — do + N X D(EjK))) j=1	(17)
Critic Update: wk+1 =	I — Z3(k)Vθ log∏θ(ak∣Xk)∣θ=θk (Vθ log∏θ(ak∣xk)∣θ=θk)		wk
+ Z3(k)δk(vk)Vθ log∏θ(ak∣Xk)∣θ=θk			(18)
θk+1 = θk - ζ2 (k)wk /1 - γ
Follow from Eqs. 14, 15, and 17.
θ Update:
Other Updates:
(19)
end for
end while
SV = {Ψv∣v ∈ Rκ2}, where Ψ is a short-hand notation for the set of features, i.e., Ψ(χ) = ψ>(χ).
Recently with the advances in deep neural networks, it has become increasingly popular to model the
critic with a deep neural network, based on the objective function of minimizing the MSE of Bellman
residual w.r.t. Vθ or Qθ (Mnih et al., 2013).
14
Under review as a conference paper at ICLR 2020
B The Lyapunov Approach to Solve CMDPs
In this section, we revisit the Lyapunov approach to solving CMDPs that was proposed by (Chow
et al., 2018) and report the mathematical results that are important in developing our safe policy
optimization algorithms. To start, without loss of generality, we assume that we have access to a
baseline feasible policy of (1), πB; i.e., πB satisfies DπB (x0) ≤ d0. We define a set of Lyapunov
functions w.r.t. initial state x0 ∈ X and constraint threshold d0 as
LπB (x0, d0)={L : X →R≥0 : TπB,d[L](x) ≤ L(x), ∀x ∈ X; L(x0) ≤ d0},
and call the constraints in this feasibility set the Lyapunov constraints. For any arbitrary Lyapunov
function L ∈ LπB (x0 , d0), we denote by
FL(X) = {∏(∙∣x) ∈ ∆ : T∏,d[L](x) ≤L(x)},
the set of L-induced Markov stationary policies. Since Tπ,d is a contraction mapping (Bertsekas,
2005), any L-induced policy π has the property Dπ (x) = limk→∞ Tπk,d[L](x) ≤ L(x), ∀x ∈ X .
Together with the property that L(x0) ≤ d0, they imply that any L-induced policy is a feasible policy
of (1). However, in general, the set FL(x) does not necessarily contain an optimal policy of (1), and
thus, it is necessary to design a Lyapunov function (w.r.t. a baseline policy πB) that provides this
guarantee. In other words, the main goal is to construct a Lyapunov function L ∈ LπB (x0, d0) such
that
L(X) ≥ T∏*,d[L](χ),	L(χo) ≤ do.	(21)
Chow et al. (Chow et al., 2018) show in their Theorem 1 that 1) without loss of optimality, the
Lyapunov function can be expressed as
∞
L(X) := E	γt(d(Xt) + (Xt)) | πB, X ,
t=0
where (X) ≥ 0 is some auxiliary constraint cost uniformly upper-bounded by
e*(x) := 2DmaχDτv(∏*II∏b)(x)∕(1 - γ),
and 2) if the baseline policy πB satisfies the condition
*，、L 口	∙ d d	d d0 - DnB (XO)	DmaX -(I- Y)D 1
max C(X) ≤ DmaX ∙ min ( (I - Y)-D--------,下--7；---------3
x∈X	DmaX	DmaX + (1 - γ)D
where D = maxχ∈χ max∏ D∏ (x) is the maximum constraint cost, then the Lyapunov function
candidate L* also satisfies the properties of (21), and thus, its induced feasible policy set FL*
contains an optimal policy. Furthermore, suppose that the distance between the baseline and optimal
policies can be estimated efficiently. Using the set of L* -induced feasible policies and noting
that the safe Bellman operator T [V](X) = minπ∈FL * (x) Tπ,c[V](X) is monotonic and contractive,
one can show that T[V](x) = V(x), ∀x ∈ X, has a unique fixed point V*, such that V*(x0)
is a solution of (1) and an optimal policy can be constructed via greedification, i.e., π*(1χ) ∈
argmin∏∈Fl * (χ) T∏,c[V*](x). This shows that under the above assumption, (1) can be solved
using standard dynamic programming (DP) algorithms. While this result connects CMDP with
Bellman’s principle of optimality, verifying whether πB satisfies this assumption is challenging when
a good estimate of DTV(∏*∣∣∏b) is not available. To address this issue, Chow et al. (Chow et al.,
2018) propose to approximate e* with an auxiliary constraint cost e, which is the largest auxiliary
cost satisfying the Lyapunov condition Le(X) ≥ TπB,d[Le](X), ∀X ∈ X, and the safety condition
Le(X0) ≤ d0. The intuition here is that the larger eC, the larger the set of policies FLe. Thus, by
choosing the largest such auxiliary cost, we hope to have a better chance of including the optimal
policy ∏* in the set of feasible policies. Specifically, e is computed by solving the following linear
program (LP):
e ∈ argmax ʃ X c(x) ： do -D∏b (xo) ≥ 1(x°)>(l - γ {P (x0∣x,∏b (x))}xxo∈χ)	/, (22)
:X →R≥0 x∈X	x,x∈
where 1(Xo) represents a one-hot vector in which the non-zero element is located at X = Xo. When
πB is a feasible policy, this problem has a non-empty solution. Furthermore, according to the
derivations in (Chow et al., 2018), the maximizer of (22) has the following form:
e(x) =	(do -DnB(XO)) ∙ 1{x = X}	≥ o
15
Under review as a conference paper at ICLR 2020
Algorithm 3 Safe Policy Iteration (SPI)
Input: Initial feasible policy π0 ;
for k = 0, 1, 2, . . . do
Step 0: With πb = πk , evaluate the Lyapunov function Lk , where k is a solution of (22)
Step 1: Evaluate the cost value function Vπk (x) = Cπk (x); Then update the policy by solving the following
problem: ∏k+ι(∙∣x) ∈ argmin∏∈FL* ⑺ T∏,c[V∏k](x), ∀x ∈ X
end for
Return Final policy ∏k*
where X ∈ argminχ∈χ E [P∞=0 γt1{xt = x} | xo,∏b] . They also show that by further restricting
e(x) to be a constant function, the maximizer is given by
E(X) = (I - Y) ∙ (do - DnB (XO)), ∀x ∈ X.
Using the construction of the Lyapunov function Le, (Chow et al., 2018) propose the safe policy itera-
tion (SPI) algorithm (see Algorithm 3) in which the Lyapunov function is updated via bootstrapping,
i.e., at each iteration Le is recomputed using (22) w.r.t. the current baseline policy. At each iteration
k, this algorithm has the following properties: 1) Consistent Feasibility, i.e., if the current policy πk
is feasible, then πk+1 is also feasible; 2) Monotonic Policy Improvement, i.e., Cπk+1 (X) ≤ Cπk (X)
for any X ∈ X ; and 3) Asymptotic Convergence. Despite all these nice properties, SPI is still a
value-function-based algorithm, and thus, it is not straightforward to use it in continuous action
problems. The main reason is that the greedification step becomes an optimization problem over the
continuous set of actions that is not necessarily easy to solve. In Section 3, we show how we use SPI
and its nice properties to develop safe policy optimization algorithms that can handle continuous
action problems. Our algorithms can be thought as combinations of DDPG or PPO (or any other
on-policy or off-policy policy optimization algorithm) with a SPI-inspired critic that evaluates the
policy and computes its corresponding Lyapunov function. The computed Lyapunov function is then
used to guarantee safe policy update, i.e., the new policy is selected from a restricted set of safe
policies defined by the Lyapunov function of the current policy.
16
Under review as a conference paper at ICLR 2020
C Technical Details of the Safe Policy Gradient Algorithms
In this section, we first provide the details of the derivation of the θ-projection and a-projection
procedures described in Section 3, and then provide the pseudo-codes of our safe PG algorithms.
C.1 DERIVATION OF θ-PROJECTION IN LYAPUNOV-BASED SAFE PG
To derive our θ-projection algorithms, we first consider the original Lyapunov constraint in (3) that is
given by
I	(∏θ(a|x) — ∏b (a∣x)) QLn (x, a) da ≤(T(x), ∀x ∈ X,
a∈A	B
where the baseline policy is parameterized as πB = πθB . Using the first-order Taylor series expansion
w.r.t. θ = Θb, at any arbitrary X ∈ X, the term Ea〜∏o [Qlo^ (x, a)] = fα∈A ∏θ (a|x) QLnB (x, a) da
on left-hand-side of the above inequality can be written as
Ea〜Πθ [qLob (x, a)] = Ea〜∏θB [QLoB (X, a)] + <(θ-θB), VEa〜∏o [QLoB (x, a)] ∖θ=Θb )+O(kθ-θB 112),
which implies that
/	(∏θ(a∣X)-∏B (a∣X)) QLnB (x, a) da =((。一Θb), VθEa^,∏θ [qLob (X,a)] ∣θ=Θb E+O(kθ-θB∣∣2).
Note that the objective function of the constrained minimization problem in (4) contains a regulariza-
tion term: β2 ((θ — Θb), V2Dkl(Θ,Θb) ∣θ=Θb (θ 一 Θb))that controls the distance ∣∣θ — Θb ∣∣ to be
small. For most practical purposes, here one can assume the higher-order term O(∣θ - θB ∣2) to be
much smaller than the first-order term ((θ — Θb), VEa〜∏o [Qlθb (x, a)] ∣θ=Θb } Therefore, one
can approximate the original Lyapunov constraint in (3) with the following constraint:
((θ — θB), VθEa〜∏θ [Qlob (x, a)] ∣θ=θB) ≤ Rx), ∀x ∈ X.
Furthermore, following the same line of arguments used in TRPO (to transform the max DKL
constraint to an average DKL constraint, see Eq. 12 in (Schulman et al., 2015a)), a more numerically
stable way is to approximate the Lyapunov constraint using the average constraint surrogate, i.e.,
1M	1M
((θ — θB), MX vθEa〜∏o [QLOB (Xi, a)] 1Θ=Θb) ≤ MXe(Xi)∙
i=1
i=1
Now consider the special case when auxiliary constraint surrogate is chosen as a constant, i.e.,
e = (1— Y) d0 — Dπo (X0) . The justification of such choice comes from analyzing the solution of
optimization problem (22). Then, one can write the Lyapunov action-value function QLo (X, a) as
∞
Qlθb (x,a) = E EYtd(Xt)∣∏B,xo = x,ao = a
t=0
+ 1-^
Since the second term is independent of θ, for any state x ∈ X, the gradient term
VθEa〜∏θ QLLob (x, a)] can be simplified as
VθEa^∏θ [Qlθb (x, a)] = I ∏θ(a|x) Vθ log ∏θ(a|x) Qw@b (x,a)da = VθEa^∏o QWwθb (x,a)],
a
where WθB (x) = TπB,d[WθB](x) and QwθB (x, a) = d(x) + γPx0 P (x0 |x, a)WθB (x0) are the
constraint value function and constraint state-action value function, respectively. The second equality
is based on the standard log-likelihood gradient property in PG algorithms (Sutton et al., 2000).
Collectively, one can then re-write the Lyapunov average constraint surrogate as
1M
((θ — θB ), M X : vθEa~∏θ [qWθb (Xi, a)] 1Θ=Θb) ≤ e
i=1
where e is the auxiliary constraint cost defined specifically by the Lyapunov-based approach, to
guarantee constraint satisfaction. By expanding the auxiliary constraint cost e on the right-hand-side,
the above constraint is equivalent to the constraint used in CPO, i.e.,
1	1M
d∏Θb (XO) + 1	Y h(θ - θB ), M〉：V6Ea~∏θ[QWθB (Xi, a)[ 1Θ=Θb i ≤ d0.
17
Under review as a conference paper at ICLR 2020
C.2 DERIVATIONS OF a-PROJECTION IN LYAPUNOV-BASED SAFE PG
For any arbitrary state x ∈ X, consider the following constraint in the safety-layer projection problem
given in (6):
QLπB (x, a) - QLπB (x, πB (x)) ≤ e(x).
Using first-order Taylor series expansion of the Lyapunov state-action value function QLπ (x, a)
w.r.t. action a = πB(x), the Lyapunov value function QLπ (x, a) can be re-written as
QLπB (x, a) = QLπB(x,πB(x)) + (a - πB(x))>gLπB(x) + O(ka - πB(x)k2).
Note that the objective function of the action-projection problem in (7) contains a regularization
term η(2χ)∣∣a - ∏b(x)k2 that controls the distance ∣∣a - ∏b(x)k to be small. For most practical
purposes, here one can assume the higher-order term O(ka - πB(x)k2 ) to be much smaller than the
first-order term (a - πB (x))>gLπ (x). Therefore, one can approximate the original action-based
Lyapunov constraint in (6) with the constraint (a - ∏b (x))>gLπB(X) ≤e(x) that is the constraint
in (7). Similar to the analysis of the θ-projection approach, if the auxiliary cost e is state-independent,
the action-gradient term gLπ (x) is equal to the gradient of the constraint action-value function
VaQwθβ (x, a) ∣a=∏B(x), where Qwθb is the state-action constraint value function w.r.t. the baseline
policy. The rest of the proof follows the results from Proposition 1 in (Dalal et al., 2018). This
completes the derivations of the a-projection approach.
C.3 Pseudo-codes of Our Safe PG Algorithms
Algorithms 4 and 5 contain the pseudo-code of our safe Lyapunov-based policy gradient (PG)
algorithms with θ-projection and a-projection, respectively.
C.4 Practical Implementation of Our Safe PG Algorithms
Due to function approximation errors, even with the Lyapunov constraints, in practice a safe PG algo-
rithm may take a bad step and produce an infeasible policy update and cannot automatically recover
from such abad step. To address this issue, similar to (Achiam et al., 2017), we propose the following
safeguard policy update rule to decrease the constraint cost: θk+ι = θk - αsg,kVθD∏θ (x0)θ=θk,
where αsg,k is the learning rate for the safeguard update. If αsg,k >> αk (learning rate of PG), then
with the safeguard update, θ will quickly recover from the bad step, however, it might be overly
conservative. This approach is principled because as soon as πθk is unsafe/infeasible w.r.t. the CMDP
constraints, the algorithm uses a limiting search direction. One can directly extend this safeguard
update to the multiple-constraint scenario by doing gradient descent over the constraint that has the
worst violation.
Another remedy to reduce the chance of constraint violation is to do constraint tightening on the
constraint cost threshold. Specifically, instead of do, one may pose the constraint based on do ∙ (1 - δ),
where δ ∈ (0, 1) is the factor of safety for providing additional buffer to constraint violation.
Additional techniques in cost-shaping have been proposed in (Achiam et al., 2017) to smooth out
the sparse constraint costs. While these techniques can further ensure safety, construction of the
cost-shaping term requires knowledge of the environment, which makes the safe PG algorithms more
complicated.
18
Under review as a conference paper at ICLR 2020
Algorithm 4 Lyapunov-based Safe PG with θ-projeCtion (SDDPG and SPPO)
Input: Initial feasible policy π0 ;
for k = 0, 1, 2, . . . do
Step 0: With πb = πθk, generate N trajectories {ξj,k}jN=1 of T steps by starting at x0 and following the
policy θk
Step 1: Using the trajectories {ξj,k}jN=1, estimate the critic Qθ (x, a) and the constraint critic QD,θ (x, a);
•	For DDPG, these functions are trained by minimizing the MSE of Bellman residual, and one can
also use off-policy samples from replay buffer (Schaul et al., 2015);
•	For PPO these functions can be estimated by the generalized advantage function technique from
Schulman et al. (2015b)
Step 2: Based on the closed form solution of a QP problem with an LP constraint in Section 10.2 of Achiam
et al. (2017), calculate λk with the following formula:
∖* - -βk-- (VθQθ(x,a) ∣θ=θk)> H(θkTRQD,θ(x,a) ∣θ=θ%
λk = I ------------------γ-----------------------
∖	(VθQd,θ(x,a) ∣θ=θk) H(θk)-1VθQd,θ(x,a) ∣θ=θk
+
where
1	T-1
v θ qθ (X, a) = N	γtVΘ log∏θ(α∣x)Qθ(x,α),
x,a∈ξj,k,1≤j≤N t=0
1	T-1
VθQD,θ (x,%) = N	γtV
θ log∏Θ(a∣x)Qθ(x,α),
x,a∈ξj,k,1≤j ≤N t=0
βk is the adaptive penalty weight of the DKL (∏ || ∏θk) regularizer, and H (θk) = V2 DKL (∏ || ∏θ) | θ=θk is
the Hessian of this term
Step 3: Update the policy parameter by following the objective gradient;
•	For DDPG
θk+ι J θk-αk∙N~τ	X	Vθ ∏θ(x) ∣θ = θk ∙(VaQθk (x,a) + λk VaQD,θk (x,a)) ∣a = ∏θk (χ)
x∈ξj,k,1≤j≤N
•	For PPO,
T-1
θk+1 j θk- N^k^ (H(θk)) 1 X	XYt • vθ logπθ(aj,t|xj,t) lθ=θk ∙
k	xj,t,aj,t∈ξj,k,1≤j≤N t=0
(Qθk (xj,t,aj,t) + λk QD,θk (xj,t,aj,t))
Step 4: At any given state X ∈ X, compute the feasible action probability a*(x) via action projec-
tion in the safety layer, that takes inputs Va QL (x, a) = VaQD,θk (x, a) and (x) = (1 - γ)(d0 -
QD,θk (X0, πk(X0))), for any a ∈ A.
end for
Return Final policy ∏θχ*,
D Experimental Setup in MuJoCo Tasks
Our experiments are performed on safety-augmented versions of standard MuJoCo domains (Todorov
et al., 2012).
HalfCheetah-Safe. The agent is a the standard HalfCheetah (a 2-legged simulated robot rewarded
for running at high speed) augmented with safety constraints. We choose the safety constraints to
be defined on the speed limit. We constrain the speed to be less than 1, i.e., constraint cost is thus
1[|v| > 1]. Episodes are of length 200. The constraint threshold is 50.
Point Circle. This environment is taken from (Achiam et al., 2017). The agent is a point mass
(controlled via a pivot). The agent is initialized at (0, 0) and rewarded for moving counter-clockwise
along a circle of radius 15 according to the reward --√χ∙y+=yX^f, for position x, y and velocity
dx, dy. The safety constraint is defined as the agent staying in a position satisfying |x| ≤ 2.5. The
constraint cost is thus 1[|x| > 2.5]. Episodes are of length 65. The constraint threshold is 7.
19
Under review as a conference paper at ICLR 2020
Algorithm 5 Lyapunov-based Safe PG (SDDPG and SPPO)With a-projeCtion
Input: Initial feasible policy ∏o;
for k = 0, 1, 2, . . . do
Step 0: With πb = πθk, generate N trajectories {ξj,k}jN=1 of T steps by starting at x0 and folloWing the
policy θk
Step 1: Using the trajectories {ξj,k}jN=1, estimate the critic Qθ (x, a) and the constraint critic QD,θ (x, a);
•	For DDPG, these functions are trained by minimizing the MSE of Bellman residual, and one can
also use off-policy samples from replay buffer (Schaul et al., 2015);
•	For PPO these functions can be estimated by the generalized advantage function technique from
Schulman et al. (2015b)
Step 2: Update the policy parameter by folloWing the objective gradient;
•	For DDPG
θk + 1 ^-	θk	― αk	∙	N^T	〉：	NBπθ(X) lθ = θk	∙VaQθ%	(x, a)	[a=∏θk (x))；
x∈ξj,k,1≤j≤N
•	For PPO,
T-1
θk + 1 — θk - Ne (H (θk ))	Σ	Σ Yt∙Vθ log πθ(aj,tlxj,t) ∣θ=θk ∙Qθk (xj,t,aj,t)
k	xj,t,aj,t∈ξj,k,1≤j≤N t=0
where βk is the adaptive penalty weight of the DKL (π ∣∣πθk) regularize], and H(θk) =
V2DKL(∏∣∣∏θ) ∣θ=θk is the Hessian of this term
Step 3: At any given state x ∈ X, compute the feasible action probability a* (x) via action projec-
tion in the safety layer, that takes inputs VaQL(x, a) = VaQD,θk (x, a) and (x) = (1 - γ)(d0 -
QD,θk(x0,πk(x0))), for any a ∈ A.
end for
Return Final policy πθk*,
Point Gather. This environment is taken from (Achiam et al., 2017). The agent is a point mass
(controlled via a pivot) and the environment includes randomly positioned apples (2 apples) and
bombs (8 bombs). The agent given a reward of 10 for each apple collected and a penalty of -10 for
each bomb. The safety constraint is defined as the number of bombs collected during the episode.
Episodes are of length 15. The constraint threshold is 4 for DDPG and 2 for PPO.
Ant Gather. This environment is the same as Point Circle, only with an Ant agent (quadrapedal
simulated robot). Each episode is initialized with 8 apples and 8 bombs. The agent receives a reward
of 10 for each apple collected, a penalty of -20 for each bomb collected, and a penalty of -20 if
the episode terminates prematurely (because the Ant falls). Episodes are of length at most 500. The
constraint threshold is 10 and 5 for DDPG and PPO, respectively.
Figure 7 shows the visualization of the above domains used in our experiments.
HalfCheetah-Safe
Figure 7: The Robot Locomotion Control Tasks
Point-Gather
In these experiments, there are three different agents: (1) a point-mass (X ⊆ R9, A ⊆ R2); an
ant quadruped robot (X ⊆ R32 , A ⊆ R8); and (3) a half-cheetah (X ⊆ R18, A ⊆ R6). For
all experiments, we use two neural networks with two hidden layers of size (100, 50) and ReLU
activation to model the mean and log-variance of the Gaussian actor policy, and two neural networks
with two hidden layers of size (200, 50) and tanh activation to model the critic and constraint critic.
To build a low variance sample gradient estimate, we use GAE-λ (Schulman et al., 2015b) to estimate
20
Under review as a conference paper at ICLR 2020
(a) Ant-Gather, Return
(b) Point-Circle, Return
(c) Ant-Gather, Constraint
(d) Point-Circle, Constraint
Figure 8:	DDPG (red), DDPG-Lagrangian (cyan), SDDPG (blue), SDDPG a-projection (green) on Ant-Gather
and Point-Circle. Ours SDDPG and SDDPG a-projection algorithms perform stable and safe learning, although
the dynamics and cost functions are unknown, control actions are continuous, and deep function approximation
is used. Unit of x-axis is in thousands of episodes. Shaded areas represent the 1-SD confidence intervals (over
10 random seeds). The dashed purple line in the two right figures represents the constraint limit.
4」
2 -
0-
—2 -
-4-
10
20 30 40 50 60 70 80
-10-l
0
90
≡, J∣w⅛
500 - ∕?-
w°-∕r
0 1	I	I	I	I	Γ
0	20	40	60	80	100
(c) Ant-Gather, Constraint
(d) Point-Circle, Constraint
(a) Ant-Gather, Return
(b) Point-Circle, Return
Figure 9:	PPO (red), PPO-Lagrangian (cyan), SPPO (blue), SPPO a-projection (green) on Ant-Gather and
Point-Circle. SPPO a-projection performs stable and safe learning, when the dynamics and cost functions are
unknown, control actions are continuous, and deep function approximation is used.
the advantage and constraint advantage functions, with a hyper-parameter λ ∈ (0, 1) optimized by
grid-search.
On top of GAE-λ, in all experiments and for each algorithm (SDDPG, SPPO, SDDPG a-projection,
SPPO a-projection, CPO, Lagrangian, and the unconstrained PG counterparts), we systematically
explored different parameter settings by doing grid-search over the following factors: (i) learning
rates in the actor-critic algorithm, (ii) batch size, (iii) regularization parameters of the policy relative
entropy term, (iv) with-or-without natural policy gradient updates, (v) with-or-without the emergency
safeguard PG updates (see Appendix C.4 for more details). Although each algorithm might have
a different parameter setting that leads to the optimal performance in training, the results reported
here are the best ones for each algorithm, chosen by the same criteria (which is based on the value of
return plus certain degree of constraint satisfaction). To account for the variability during training,
in each learning curve, a 1-SD confidence interval is also computed over 10 separate random runs
(under the same parameter setting).
D.1 More Explanations on MuJoCo Results
In all numerical experiments and for each algorithm (SPPO θ-projection, SDDPG θ-projection, SPPO
a-projection, SDDPG a-projection, CPO, Lagrangian, and the unconstrained PG counterparts), we
systematically explored various hyper-parameter settings by doing grid-search over the following
factors: (i) learning rates in the actor-critic algorithm, (ii) batch size, (iii) regularization parameters
of the policy relative entropy term, (iv) with-or-without natural policy gradient updates, (v) with-or-
without the emergency safeguard PG updates (see Appendix C.4 for more details). Although each
algorithm might have a different parameter setting that leads to the optimal training performance, the
results reported in the paper are the best ones for each algorithm, chosen by the same criteria (which
is based on value of return + certain degree of constraint satisfaction).
In our experiments, we compare the two classes of safe RL algorithms, one derived from θ-projection
(constrained policy optimization) and one from the a-projection (safety layer), with the unconstrained
and Lagrangian baselines in four problems: PointGather, AntGather, PointCircle, and HalfCheetah-
Safe. We perform these experiments with both off-policy (DDPG) and on-policy (PPO) versions of
the algorithms.
In PointCircle DDPG, although the Lagrangian algorithm significantly outperforms the safe RL
algorithms in terms of return, it violates the constraint more often. The only experiment in which
Lagrangian performs similarly to the safe algorithms in terms of both return and constraint violation is
PointCircle PPO. In all other experiments that are performed in the HalfCheetahSafe, PointGather and
21
Under review as a conference paper at ICLR 2020
AntGather domains, either (i) the policy learned by Lagrangian has a significantly lower performance
than that learned by one of the safe algorithms (see HalfCheetahSafe DDPG, PointGather DDPG,
AntGather DDPG), or (ii) the Lagrangian method violates the constraint during training, while the
safe algorithms do not (see HalfCheetahSafe PPO, PointGather PPO, AntGather PPO). This clearly
illustrates the effectiveness of our Lyapunov-based safe RL algorithms, when compared to Lagrangian
method.
22
Under review as a conference paper at ICLR 2020
E Experimental Setup in the Robot Navigation Problem
Mapless navigation task is a continuous control task with a goal of navigating a robot to any arbitrary
goal position collision-free and without memory of the workspace topology. The goal is usually
within 5 - 10 meters from the robot agent, but it is not visible to the agent before the task starts, due
to both limited sensor range and the presence of obstacles that block a clear line of sight. The agent’s
observations, X = (g, g, l) ∈ R68, consists of the relative goal position, the relative goal velocity,
and the Lidar measurements. Relative goal position, g , is the relative polar coordinates between
the goal position and the current robot pose, and g is the time derivative of g, which indicates the
speed of the robot navigating to the goal. This information is available from the robot’s localization
sensors. Vector l is the noisy Lidar input (Fig. 3a), which measures the nearest obstacle in a direction
within a 220° field of view split in 64 bins, UP to 5 meters in depth. The action is given by a ∈ R2,
which is linear and angular velocity vector at the robot’s center of the mass. The transition probability
P : X × A → X captures the noisy differential drive robot dynamics. Without knowing the full non-
linear system dynamics, we here assume knowledge of a simplified blackbox kinematics simulator
operating at 5Hz in which Gaussian noise, N(0, 0.1), is added to both the observations and actions in
order to model the noise in sensing, dynamics, and action actuations in real-world. The objective of
the P2P task is to navigate the robot to reach within 30 centimeters from any real-time goal. While the
dynamics of this system is simpler than that of HalfCheetah. But unlike the MuJoCo tasks where the
underlying dynamics are deterministic, in this robot experiment the sensor, localization, and dynamics
noise paired with partial world observations and unexpected obstacles make this safe RL much more
challenging. More descriptions about the indoor robot navigation problem and its implementation
details can be found in Section 3 and 4 of (Chiang et al., 2019). Fetch robot weights 150 kilograms,
and reaches maximum speed of 7 km/h making the collision force a safety paramount.
Here the CMDP is non-discounting and has a finite-horizon of T = 100. We reward the agent
for reaching the goal, which translates to an immediate cost of c(x, a) = kgk2, which measures
the relative distance to goal. To measure the impact energy of obstacle collisions, we impose an
immediate constraint cost of d(x, a) = kg∣∣ ∙ 1{k1∣ ≤ rimpact}∕T, where rimpact is the impact radius
w.r.t. the Lidar depth signal, to account for the speed during collision, with a constraint threshold d0
that characterizes the agent’s maximum tolerable collision impact energy to any objects. (Here the
total impact energy is proportional to the robot’s speed during any collisions.) Under this CMDP
framework (Fig. 3b), the main goal is to train a policy π* that drives the robot along the shortest
path to the goal and to limit the average impact energy of obstacle collisions. Furthermore, due to
limited data any intermediate point-to-point policy is deployed on the robot to collect more samples
for further training, therefore guaranteeing safety during training is critical in this application.
(a) Training, 23 by 18m
Figure 10: (a) Training and (b) evaluation environments, generated from real office building plans. The
evaluation environment is an order of magnitude bigger.
(b) Building 2, 60 by 47m
23
Under review as a conference paper at ICLR 2020
(a) Lagrangian policy
(b) SDDPG (a-proj.)
Figure 11: Navigation routes of two policies on a similar setup (a) and (b). Log of on-robot experiments (c).
(c) SDDPG (a-proj.) on robot
24