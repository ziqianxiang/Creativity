Under review as a conference paper at ICLR 2020
Masked Translation Model
Anonymous authors
Paper under double-blind review
Ab stract
We introduce the masked translation model (MTM) which combines encoding and
decoding of sequences within the same model component. The MTM is based on
the idea of masked language modeling and supports both autoregressive and non-
autoregressive decoding strategies by simply changing the order of masking. In
experiments on the WMT 2016 Romanian→English task, the MTM shows strong
constant-time translation performance, beating all related approaches with com-
parable complexity. We also extensively compare various decoding strategies sup-
ported by the MTM, as well as several length modeling techniques and training
settings.
1	Introduction
Neural machine translation (NMT) has been developed under the encoder-decoder framework
(Sutskever et al., 2014) with an intermediary attention mechanism (Bahdanau et al., 2015). The
encoder learns contextualized representations of source tokens, which are used by the decoder to
predict target tokens. These two components have individual roles in the translation process, and
they are connected via an encoder-decoder attention layer (Bahdanau et al., 2015). Many advances
in NMT modeling are based on changes in the internal layer structure (Gehring et al., 2017; Wang
et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Dehghani et al., 2019; Yang et al., 2019), tweak-
ing the connection between the layers (Zhou et al., 2016; Shen et al., 2018; Bahar et al., 2018; Li
et al., 2019a), or appending extra components or latent variables (Gu et al., 2016; Zhang et al., 2016;
Shah & Barber, 2018; Hao et al., 2019) - all increasing the overall architectural complexity of the
model while keeping the encoder and decoder separated.
Our goal is to simplify the general architecture of machine translation models. For this purpose, we
propose the masked translation model (MTM) - a unified model which fulfills the role of both the
encoder and decoder within a single component. The MTM gets rid of the conventional decoder
as well as the encoder-decoder attention mechanism. Its architecture is only a sequence encoder
with self-attention layers, trained with an objective function similar to masked language modeling
(Devlin et al., 2019). In order to model the translation problem, the MTM is given the concatenation
of the source and target side from a parallel sentence pair. This approach is similar to the translation
language model presented by Lample & Conneau (2019), but focuses on the target side, i.e. the
masking is applied to some selected positions in the target sentence. The MTM is trained to predict
the masked target words relying on self-attention layers which consider both the source sentence
and a masked version of the target sentence. Trained in this way, the model is perfectly suitable for
non-autoregressive decoding since the model learned to predict every position in parallel, removing
the dependency on decisions at preceding target positions.
Within its extremely simple architecture, one can realize various decoding strategies, e.g., using
left-to-right, non-autoregressive, or iterative decoding by merely adjusting the masking schemes in
search. We present a unified formulation of the MTM for different decoding concepts by factorizing
the model probability over a set of masked positions.
The MTM has several advantages over the conventional encoder-decoder framework:
•	A simpler architecture
•	Source and target representations interact in every layer
•	A variety of decoding strategies including constant-time approaches (Section 3.3.1)
1
Under review as a conference paper at ICLR 2020
On the WMT 2016 Romanian→English translation task, our MTM achieves a better performance
than comparable non-autoregressive/constant-time methods while keeping its simple architecture.
Using our general formulation of the MTM, we compare the translation performance of various
decoding strategies. Moreover, we show that this model allows for decoding speed-up by merely
adjusting the number of iterations at the small cost of translation performance.
2	Related Work
There have been some attempts to combine the encoder and decoder into a single component for
simplified translation modeling. He et al. (2018) share the encoder and decoder parameters of a
Transformer translation model (Vaswani et al., 2017) and allow the encoder-decoder attention to
access the inner layers of the encoder as well. Fonollosa et al. (2019) extend this idea by adding
locality constraints in all attention layers. Radford et al. (2019) train a Transformer decoder on a
large monolingual corpus as a language model and use it as an unsupervised translation model on
pairs of source and target sentences. Similarly to our work, all these approaches couple the encoding
and decoding on the self-attention level. However, their decoding considers only left-side target con-
text, enabling only left-to-right autoregressive translation. Furthermore, their encoding of a source
sentence is limited to the source side itself, while our MTM can refine the source representations
according to a partial target hypothesis represented in the decoder states. Both aspects hinder their
methods from making the best use of the bidirectional representation power of the combined model.
Non-autoregressive NMT, which predicts all target words in parallel, potentially exploits full bidi-
rectional context in decoding. To make the parallel decoding produce a reasonable hypothesis, Gu
et al. (2018) reuse the source words as inputs to the decoder and insert an additional attention module
on the positional embeddings. Lee et al. (2018) use a separate decoder to revise the target hypotheses
iteratively, where Ghazvininejad et al. (2019) train a single decoder with MLM objectives for both
the first prediction and its refinements. To improve the integrity of the hypotheses, one could also
employ an autoregressive teacher to guide the states of the non-autoregressive decoder (Wei et al.,
2019; Li et al., 2019b), apply sentence-level rewards in training (Shao et al., 2019), or integrate gen-
erative flow latent variables (Ma et al., 2019). The self-attention layers of their decoders attend to
all target positions, including past and future contexts. However, all these methods still rely on the
encoder-decoder framework. In this work, we collapse the boundary of the encoding and decoding
of sequences and realize non-autoregressive NMT with a unified model. Regardless of the encoding
or decoding, the self-attention layers of our MTM attend to all available source and target words for
flexible information flow and a model of simplicity.
A common problem in non-autoregressive sequence generation is that the length of an output should
be predefined beforehand. The problem has been addressed by averaging length difference (Li et al.,
2019b), estimating fertility (Gu et al., 2018), dynamically inserting blank outputs via connectionist
temporal classification (CTC) (Libovicky & Helcl, 2018), or directly predicting the total length from
the encoder representations (Lee et al., 2018; Ghazvininejad et al., 2019; Wei et al., 2019; Ma et al.,
2019). In this work, we train a separate, compact length model on given bilingual data.
The training of an MTM is based on the MLM objective (Devlin et al., 2019), developed for pre-
training representations for natural language understanding tasks (Wang et al., 2019). Lample &
Conneau (2019) concatenate source and target sentences and use them together as the input to an
MLM, where both source and target tokens are randomly masked for the training. This improves
cross-lingual natural language inference in combination with the original MLM objective, but has
not been applied to translation tasks. We use the concatenated input sentences but selectively mask
out only target tokens to implement source→target translation. As for inference with an MLM, Wang
& Cho (2019) and Ghazvininejad et al. (2019) propose to build up an output sequence iteratively
by adjusting the input masking for each iteration. In this work, the generation procedures of both
works are tested and compared within our MTM, along with other autoregressive/non-autoregressive
decoding strategies (Section 4.1).
3	Masked Translation Model
We introduce the masked translation model (MTM) focusing on three aspects: model architecture,
training, and decoding. In the corresponding sections, we show how the MTM 1) relaxes the con-
2
Under review as a conference paper at ICLR 2020
Xej∙∣∕f,e{)
I SoftmaX~~∣
A
Figure 1: Visualization of an MTM model during training. The architecture is equivalent to a stan-
dard Transformer encoder with bidirectional self-attention. Additionally, the corruption decisions
for each position are illustrated in an oval above the model input. The corrupted positions M are
indicated before the softmax layer by a filled diamond. Non-corrupted positions (blank diamond)
are not predicted in training.
ventional architectural constraint of the encoder-decoder framework, 2) learns to perform both non-
autoregressive translation and its refinements, and 3) does translation with various decoding strate-
gies in a variable number of iterations.
3.1	Model
Given a source sentence f1J = f1, ..., fj, ..., fJ (source input) the goal of an MTM pθ is to generate
the correct translation eI1 = e1, ..., ei, ..., eI (target output) by modeling:
II
P(MIfJ) = Y p(eilfJ ,ei-1,e1+ι) = Y pθ (eiIfJ ,瓦)	⑴
i=1	i=1
where Ei := E^(e1-1, e∣+1) (target input) is a corrupted version - a subset - of the surrounding Con-
text in the target sentence eI1 . Therefore, the MTM models the true target sentence eI1 independently
for each position, given both the true source and a noisy version of the target context.
Figure 1 illustrates the MTM network architecture which is the same as the MLM presented by
Devlin et al. (2019) or the encoder ofa Transformer network (Vaswani et al., 2017) with an output
softmax layer on top of all corrupted positions. In particular, an MLM consists of N transformer
encoder layers each containing two blocks: a self-attention layer with full bidirectional context
as well as two linear layers with a ReLU activation in between. Layer normalization is applied
before every block, and a residual connection is used to combine the output of a block with its input
(Vaswani et al., 2017).
The source fJ and the noisy target sentence el are concatenated with a special boundary token
‘</T>’ as separator and given to the model as a single input sequence. The tokens of this sequence
are converted to embedding vectors x1J+I+1, whose space is shared over the source and target lan-
guages by a joint subword vocabulary. A positional encoding vector (Vaswani et al., 2017) is added
where the position index is reset at the first target token. Similarly to He et al. (2018) and Lam-
ple & Conneau (2019), we add language embeddings to distinguish source and target vocabularies
efficiently. The embedded representations pass through N transformer encoder layers, where the at-
tention has no direction constraints; this allows the model to use the full bidirectional context beyond
3
Under review as a conference paper at ICLR 2020
the language boundary. Note that the hidden representations of source words can also attend to those
of (partially hypothesized) target tokens, which is impossible in the encoder-decoder architecture.
3.2 Training
During training the target side input 2 1 may be 1) fully masked out, resembling the initial stage
of translation before hypothesizing any target words, 2) partially corrupted, simulating interme-
diate hypotheses of the translation process that need to be corrected, 3) or even the original target
sentence eI1, representing the final stage of translation which should not be refined further. The dif-
ferent levels of corruptions are used to model all plausible cases which we encounter in the decoding
process - from primitive hypotheses to high-quality translations (Section 3.3).
Given bilingual training data D = {(f1J, eI1)}, the first scenario can be easily simulated by masking
out all positions of the target input, i.e. & = </M> for all i, while the target output eI remains
unchanged. In this case, the model effectively describes the conditional distribution p(eI1|f1J, I).
Note that a model for length prediction is trained separately and the details of which can be found
Appendix A.
We cannot expect a single step of parallel decoding to output an optimal translation from only the
source context. Therefore, we consider the second scenario of refining the hypothesis, where we
simulate a partial hypothesis in training by artificially corrupting the given target sentence in the
input. For training an MTM, we formulate this corruption by probabilistic models to 1) select the
positions to be corrupted (ps) and 2) decide how such a position is corrupted (pc). The goal of the
MTM is now to reconstruct the original target sentence eI1 . This leads to the training loss:
L(θ; D) = - E	E	E
(fJ ,e1 )∈D C~Ps(CIeI) 针wi~pc3i∣ea)
∀i∈C
ICCI XlogPθ(ei∣fJ,eI)
i∈C
(2)
where C is a set of positions to be corrupted. The corrupted target input eeI1 is generated in two steps
of random decisions (Devlin et al., 2019):
1.	Target positions i ∈ C for the corruption are sampled from a uniform distribution until
dρs ∙ Ie samples are drawn, with hyperparameter PS ∈ [0,1]. We denote this selection
process by C ~ps(c∣e1) = ps(C II) as a simplified notation.
2.	The specific type of corruption for each selected position i ∈ C is chosen independently
by sampling eei frompc(eei∣ei).
Note that we train the model to reconstruct the original token ei only for the corrupted positions
i ∈ C . For the remaining positions i0 ∈/ C , the corrupted sentence is filled with the original word,
i.e. eei0 = ei0. These uncorrupted positions provide the context to the network for the denoising, and
no loss is applied for these positions to prevent a bias towards copying. We optimize this criterion in
a stochastic way, where C and the eei are sampled anew for each epoch and each training instance.
In principle, the MTM is a denoising autoencoder of the target sentence conditioned on a source
sentence.
The MTM training can be customized by selecting ps and pc appropriately. Following Devlin et al.
(2019), the probability ps is defined as a uniform distribution over all target positions, without con-
sidering the content of the sentence. For the corruption model pc, we define a set of operations and
assign a probability mass ρo ∈ [0, 1] to each operation o. We use the three operations presented by
Devlin et al. (2019):
•	Replace with a mask token:
Mask:	pc(ei = </M>|ei) = ρmask	(3)
•	Replace with a random word e* uniformly sampled from the target vocabulary Ve:
Random:	Pc© = e* 〜VeIei)= Prand	(4)
•	Keep unchanged:
Keep:	pc (eei = ei ∣ei ) = Pkeep	(5)
4
Under review as a conference paper at ICLR 2020
Original:	eI =	Thanks	for	reading	this
Operation:	NoOp.	Keep	Mask	Random
Corrupted:针=	Thanks	for	</M>	cat
Loss:	X	X	X	X
Figure 2: Example of the MTM corruption process on the target input sentence “Thanks for
reading this" with resulting loss P4=2 logpθ(e∕fJ, e4).
Figure 2 shows an example of corrupting a target input sentence in the MTM training. Here all
positions except 1 are corrupted, i.e. C = {2, 3, 4}. At Position 2 the original word is kept by the
Keep operation of pc but in contrast to Position 1 (No Operation) there is a training loss added for
Position 2.
3.3 Decoding
As described above, the MTM is designed and trained to deal with intermediate hypotheses of vary-
ing quality as target input. Accordingly, decoding with an MTM consists of multiple iterations
τ = 1, ..., T: A non-autoregressive generation of the initial hypothesis (for τ = 1) and several steps
of iterative refinements (inspired by Lee et al. (2018)) of the hypothesis (for τ > 1). In the con-
text of this work, an iteration of the MTM during decoding refers to one forward pass of the model
based on a given source and target input, followed by the selection of a target output based on the
predictions of the MTM. To simplify the notation, we denote the given source sentence by F = f1J
and the generated target translation by E = e1.
Similar to traditional translation models, the goal of decoding with an MTM is to find a hypothesis E
that maximizes the translation probability:
— ʌ, — , — _, , ______________________________________ , —,
F → E(F) = arg max {p(E, I|F)} = arg max {p(E|I, F) ∙ p(I|F)}	(6)
E,I	E,I
We approximate this by a two stage maximization process where we first determine the most likely
.	,1,1?	r /Tl T^I∖、 C 11	1 1
target length I := arg maxI {p(I |F)} followed by:
E(F,I) = arg max {p(E∣jT, F)}	(7)
E
Instead of a left-to-right factorization of the target sentence which is common in autoregressive
decoding, we perform a step-wise optimization on the whole sequence. For this we define the
sequence E(T) starting from E(0) :=</M>,...,</M> by selecting the best hypothesis given the
predecessor E(TT), i.e.:
E(T) := arg maxp(E(T) |E(T-I),F,I).	(8)
E(τ)
Introducing an intermediate representation E allows for a more fine-grained control of the decoding
process:
P(E(T)|E(TT),F,I) := X Pθ(E(T)|E(TT),F,I) ∙Pm(E(TT)IE(TT),F,I)	(9)
E(TT)
where the probability pθ is modeled by a neural network with parameters θ and the masked sequence
E(t-1) is modelled by pm, which defines a specific search strategy (Section 3.3.1). In most scenar-
ios, the search strategy is defined to be deterministic, which has the effect that all probability mass
of Pm is concentrated on one masked sequence E(T-1) and We can reformulate Equation (8) as:
E(T) = arg max pθ (E(T )|E (TT),F,I).	(10)
E (τ)
and thus the score is defined solely by the MTM network.
The iterative procedure presented in Equation (10) describes a greedy optimization, as it selects the
currently best hypothesis in each iteration. This does not provide any guarantee for the quality of
5
Under review as a conference paper at ICLR 2020
Algorithm 1: MTM Decoding
Input: Source sentence F = f1J
T	C / 7^ I T-l∖、
I — argmax∕ {p(I∣F)}
E⑼—</M>,…,</M>
for T - 1,...,T do
Sample a masked sequence: E(TT)〜Pm(E(TT)IE(TT))
Compute the model output greedily for every position:
e(τ) — arg max pθ (ei∣E(τ-1> ,F, I)	∀i ∈ {1,…,I}
ei
E(τ) 一 ^1τ),…,e^τ)
Output： E 一 E(T)
the final output. However, if we assume that the model score improves in each iteration, i.e. if we
can show that:
max P(E(T +1)∣E(T ),F,I) ≥ max P(E(T )∣E(TT),F,I) ∀τ =1,...,T - 1	(11)
E(τ+1)	E(τ)
then we know that the maximum score is obtained in the last iteration T. To the best of our knowl-
edge, it is not possible to provide a theoretical proof for this property, yet we will show empirical
evidence that it holds in practice (see Section 4.1).
Thus, in order to find the best hypothesis, it is sufficient to follow the recursive definition presented
in Equation (10), which can be computed straight forward resulting in an iterative decoding scheme.
Algorithm 1 describes this process of MTM decoding. In short, 1) generate a hypothesis (E), 2)
select positions to be masked, and 3) feed the masked hypothesis (E) back to the next iteration.
(T)
Note that the output for each position ei is computed in the same forward pass without a de-
pendency on other words ei(0T) (i0 6= i) from the same decoding step. This means that the first
iteration (τ = 1) is non-autoregressive decoding (GU et al., 2018; Libovicky & HelcL 2018). Non-
autoregressive models tend to suffer from the multimodality problem (Gu et al., 2018), where con-
ditionally independent models are inadequate to capture the highly multimodal distribution of target
translations.
Our MTM decoding prevents this problem making iterative predictions E(τ) each conditioned on
the previous sequence. Each E(τ) results from one forward pass, yielding a complexity of O(T)
decoding steps instead of O(I) as in traditional autoregressive NMT. Thus the MTM decoding can
potentially be much faster than conditional decoding ofa standard encoder-decoder NMT model, as
the number of iterations is not dictated by the target output length I. Furthermore, compared to the
pure non-autoregressive decoding with only one iteration, our decoding algorithm may collapse the
multimodal distribution of the target translation by conditioning on the previous output (Lee et al.,
2018).
3.3.1 Decoding S trategies
The masking probability Pm introduced in Equation (9) resembles the two-step corruption of the
training (Equation (2)):
Pm(E(T)∣E(T)) =PS(C(T)∣c(TT)) Y PC谭)∣e(τ))
i∈C(τ)
(12)
where C is a set of positions to be masked. Similarly to the training, the corruption is performed
only for i ∈ C* (T) and the remaining positions i0 ∈/ C(T) are kept untouched. For the corruption
model Pc in decoding, only the Mask operation is activated, i.e. ρmask = 1 and ρo = 0 for o 6= mask.
This leads to the following simple decisions:
G(T) =卜/M>, if i ∈ C(T)
ɛi	[e(τ),	otherwise
(13)
6
Under review as a conference paper at ICLR 2020
The resulting masked sequence E(T) is supposed to shift the model's focus towards a selected num-
ber of words, chosen by the decoding strategy ps .
Given this definition of pc above, a masked (intermediate) hypothesis in decoding is determined
solely by the position selection ps, which differs by decoding strategy. Each decoding strategy starts
from a fully masked target input, i.e. C(0) = {1, ..., I}, and uncovers positions incrementally in
each iteration.
Fully unmasking The simplest solution is to simply feed back the completely unmasked se-
quence in each iteration (Lee et al., 2018):
Ps(C(T )|C (TT))=PS(C(T ))= ʃl,	if C (τ ).= 0	(14)
0, otherwise
This method works with the richest context from the output of the previous iteration. This may,
however, hurt the model’s performance as the first output is often quite poor, and the focus of the
model is spread across the whole sentence.
Random Instead of unmasking all positions from the beginning, one can unmask the sequence
randomly one position at a time (Wang & Cho, 2019), inspired by Gibbs sampling (Geman & Ge-
man, 1984):
“ (C (τ )|C (T-I)) - ʃ 1,	if C(T) = C (TT)∖{i} With i ∈ {1,...,I}
s	0, otherwise
(15)
Note that this method is nondeterministic and it takes at least I iterations before the output is condi-
tioned on the completely unmasked sequence.
Left-to-Right (L2R) A deterministic alternative to the random strategy is to unveil the sequence
step-wise, starting from the left-most position in the target sequence. In every decoding iteration
τ = 1, ..., T , the index i = τ - 1 is removed from the set of masked positions:
PS(C(T )|C (TT)) = ʃ1,	if C(T) = C (TT)\{T- 1}	(16)
s	0,	otherwise
This corresponds to the traditional autoregressive NMT, but the parallel nature of our MTM decoding
inherently enables to update the prediction for any position at any time, e.g. the prediction for the
first position can change in the last iteration. Furthermore, it allows for different step-wise strategies
一 revealing the sequence right-to-left (R2L) or starting from the middle in both directions (middle-
out) 一 without re-training the model.
Confidence-based L2R decoding ignores a huge advantage of the MTM, which is the property
that the fully bidirectional model can predict sequence elements in any given order. This character-
istic is leveraged by masking a decreasing number of K(τ) positions in each iteration:
(T)	(T -1)	1, if C T = {i1, ..., iK(T)}
s	0, otherwise
At each iteration, K(τ) positions with the lowest model score Pθ (confidence) remain masked:
ik = arg min	{pθ (^l )|E(TT),F))	∀k ∈{1,…，K (T)}
i∈{i1,...,ik-1} I	J
(17)
(18)
where the number of masked positions K(T) is chosen to be linearly decreasing over the number of
iterations T (Ghazvininejad et al., 2019):
K(T) = I - τ ∙ T	(19)
One can also unmask positions one by one, i.e. K(T) = I - T, sacrificing potential improvements
in decoding speed.
7
Under review as a conference paper at ICLR 2020
Table 1: Translation results on the WMT 2016 Romanian→English task on the test set
(newstest2016). (I = target hypothesis length, T = number of iterations for the hypothesis
generation/refinements)
Bleu [%]
Category	Decoding Complexity	Method	Length	
			Predicted	Gold
		Fertility-based (GU et al., 2018)	29.1	-
		CTC (Libovicky & Helcl, 2018)	24.7	-
Non-autoregressive	O(1)	Imitation learning (Wei et al., 2019)	28.9	-
		Reinforcement learning (Shao et al., 2019)	27.9	-
		Generative flow (Ma et al., 2019)	30.2	-
Non-autoregressive	O(T)	Extra refinement model (Lee et al., 2018)	30.2	31.3
+ Refinements		MTM (This Work)	30.5	32.0
Autoregressive	O(I)	Enc-Dec Transformer	32.9	-
4	Experiments
We implemented the MTM in the RETURNN framework (Doetsch et al., 2017) and evaluate the
performance on the WMT 2016 Romanian→English translation task1. All data used in the experi-
ments are preprocessed using the Moses (Koehn et al., 2007) tokenizer and frequent-casing (Vilar
et al., 2010). We learn a joint source/target byte pair encoding (BPE) (Sennrich et al., 2016) with
20k merge operations on the bilingual training data. Unless mentioned otherwise, we report results
on the newstest2016 test set, computed with case sensitivity and tokenization using the software
SacreBLEU2 (Post, 2018).
The MTMs in our experiments follow the base configuration of Vaswani et al. (2017), however, with
a depth of 12 layers and 16 attention heads. They are trained using Adam (Kingma & Ba, 2015)
with an initial learning rate of 0.0001 and a batch size of 7,200 tokens. Dropout is applied with a
probability of 0.1 to all hidden layers and word embeddings. We set a checkpoint after every 400k
sentence pairs of training data and reduce the learning rate by a factor of 0.7 whenever perplexity
on the development set (newsdev2016) does not improve for nine consecutive checkpoints. The
final models are selected after 200 checkpoints based on the development set perplexity.
During training, we select a certain percentage ρs of random target tokens to be corrupted. This
parameter is selected randomly from a uniform distribution PS 〜 U [0,1] in every training step.
We further deviate from Devlin et al. (2019), by selecting the hyperparameters for corruption to be
ρmask = 0.6, ρrand = 0.3, and ρkeep = 0.1, which performed best for MTMs in our preliminary
experiments.
The main results of the MTM are given in Table 1 along with comparative baselines. In total,
our MTM, despite its extremely simple architecture, outperforms comparable constant-time NMT
methods which do not depend on sequence lengths. Compared to the conventional autoregressive
baseline, the MTM falls behind by only -2.4% Bleu with a constant number of decoding steps
and the lower model complexity. Furthermore, a control experiment using the gold length instead
of a predicted length improves the results from the same MTM model by 1.5% Bleu. This result
minimizes the gap between our MTM and a comparable encoder-decoder model down to 0.9%
Bleu, while our model has the ability to improve the decoding speed without retraining by simply
reducing the number of iterations, thus trading in performance against speed.
Note that all other methods shown in Table 1 are based on the encoder-decoder architecture, which
is more sophisticated. Moreover, the performance of Gu et al. (2018), Lee et al. (2018), Wei et al.
(2019) and Shao et al. (2019) relies heavily on knowledge distillation from a well-trained autore-
gressive model, which involves building an additional NMT model and translating the entire training
data with that model. This causes a lot more effort and computation time in training, while the MTM
requires no such complicated steps, and its training is entirely end-to-end.
1http://www.statmt.org/wmt16/translation-task.html
2SacreBLEU configuration string: BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.17
8
Under review as a conference paper at ICLR 2020
Decoding Iterations
Figure 3: Average value of the log-probability over newsdev2016 for each decoding iteration i.e.
each refinement step in decoding with a maximum of 160 iterations.
Decoding Iterations
Figure 4: Comparison of different decoding strategies for the MTM for different amounts of decod-
ing iterations T (on newsdev2016).
Ghazvininejad et al. (2019) demonstrate in their work that even better results could be possible by
computing multiple translations in parallel for a set of most likely length hypotheses. This approach
or even a beam-search variant of our iterative unmasking, will be a focus in our future work.
4.1	Decoding
As described in Section 3.3.1, the flexibility of the MTM allows us to easily implement different
decoding strategies within a single model. Pure non-autoregressive decoding, i.e., a single forward
pass to predict all target positions simultaneously, yields poor translation performance of 13.8%
Bleu on the validation set (newsdev2016), which implies that several decoding iterations are
needed to produce a good hypothesis. We know this to be true if the inequality in Equation (11)
holds, i.e. ifwe see our model score improving in every iteration. Figure 3 shows that we can actually
observe this property by monitoring the average model score throughout all iterations. Outputs for
individual sentences might still worsen between two iterations. The overall score, however, shows a
steady improvement in each iteration.
In Figure 4, we compare various decoding strategies and plot their performance for different number
of decoding iterations T . “Fully unmasking”, i.e. re-predicting all positions based on the previous
hypothesis, improves the hypothesis fast in the early iterations but stagnates at 22% Bleu.
L2R, R2L, and confidence-based one-by-one all unmask one position at a time and show a very
similar tendency with confidence-based one-by-one decoding reaching the strongest final perfor-
mance of 31.9% BLEU. Confidence-based fixed-T unmasks several target positions per time step
9
Under review as a conference paper at ICLR 2020
and achieves similar performance. In contrast to position-wise unmasking, the decoding with a fixed
number of T (and linear unmasking) only needs ten decoding iterations to reach close to optimal
performance.
We test “middle-out” a variation of the L2R strategy to see whether the generation order is negligible
as long as the sentence is generated contiguously. While this improves the hypothesis faster - most
likely due to its doubled rate of unmasking - the final performance is worse than those of L2R or
R2L decoding. Random selection of the decoding positions shows comparably fast improvements up
to the 10th iterations, keeping up with middle-out, even though it reveals the hypothesis for a single
position per iteration, however its performance saturates below most other decoding strategies.
Overall the best result for a low iteration count is obtained with confidence-based decoding with a
fixed number of iterations. This shows that it is possible and sometimes even beneficial to hypoth-
esize several positions simultaneously. We conclude that the choice of the decoding strategy has
substantial impacts on the performance and hypothesize that a good decoding strategy relies on the
model score to choose which target positions should be unmasked.
5	Conclusion
In this work we simplify the existing Transformer architecture by combining the traditional encoder
and decoder elements into a single component. The resulting masked translation model is trained by
concatenating source and target and applying BERT-style masking to the target sentence. The novel
training strategy introduced with the MTM requires a rethinking of the search process and allows
for various new decoding strategies to be applied in the theoretical framework we developed in this
work. A detailed comparison shows that unmasking the sequence one-by-one gives the overall best
performance, be it left-to-right, right-to-left, or confidence-based. Unveiling a constant number of
tokens based on confidence in each decoding step, however, can achieve reasonable performance
with a fixed, much smaller number of iterations.
We show that there is a potential ofat least 1.5 % Bleu improvement that can be achieved by more
elaborate length models, which yields itself as a good start for further research. Furthermore, we
plan to extend the decoding strategies to work with beam search and verify our observations on
further language pairs.
References
Parnia Bahar, Christopher Brix, and Hermann Ney. Towards two-dimensional sequence to sequence
model in neural machine translation. In Proceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pp. 3009-3015, 2018.
Dzmitry Bahdanau, KyunghyunF Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1409.0473.
Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster,
Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, et al. The best of both worlds: Combin-
ing recent advances in neural machine translation. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 76-86, 2018.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019, 2019. URL https://openreview.net/forum?id=
HyzdRiR9Y7.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
and Short Papers), pp. 4171-4186, 2019. URL https://www.aclweb.org/anthology/
N19-1423/.
10
Under review as a conference paper at ICLR 2020
Patrick Doetsch, Albert Zeyer, Paul VOigtlaender, Ilia Kulikov, Ralf Schluter, and Hermann Ney.
RETURNN: The RWTH extensible training framework for universal recurrent neural networks.
In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 5345-5349. IEEE, 2017.
Jose AR Fonollosa, Noe Casas, and Marta R Costa-jussa. Joint source-target self attention with
locality constraints. arXiv preprint arXiv:1905.06596, 2019.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
sequence to sequence learning. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1243-1252. JMLR. org, 2017.
Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Transactions on Pattern Analysis & Machine Intelligence, (6):721-
741, 1984.
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Constant-time machine
translation with conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.
Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1631-1640, 2016.
Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. Non-
autoregressive neural machine translation. In 6th International Conference on Learning Rep-
resentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings, 2018. URL https://openreview.net/forum?id=B1l8BtlCb.
Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling
recurrence for transformer. In Proceedings of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), 2019.
Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Layer-wise co-
ordination between encoder and decoder for neural machine translation. In Advances in Neural
Information Processing Systems, pp. 7944-7954, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source
toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL
on interactive poster and demonstration sessions, pp. 177-180. Association for Computational
Linguistics, 2007.
Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint
arXiv:1901.07291, 2019.
Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural se-
quence modeling by iterative refinement. arXiv preprint arXiv:1802.06901, 2018.
Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R Lyu, and Zhaopeng Tu. Information
aggregation for multi-head attention with routing-by-agreement. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pp. 3566-3575, 2019a.
Zhuohan Li, Zi Lin, Di He, Fei Tian, Tao Qin, Liwei Wang, and Tie-Yan Liu. Hint-based training
for non-autoregressive machine translation. In 2019 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2019), Hong Kong, China, November 2019b.
11
Under review as a conference paper at ICLR 2020
Jindrich Libovicky and Jindfich HelcL End-to-end non-autoregressive neural machine translation
with connectionist temporal classification. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing,pp. 3016-3021, 2018.
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. Flowseq: Non-
autoregressive conditional sequence generation with generative flow. In 2019 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP 2019), Hong Kong, China, November
2019.
Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference
on Machine Translation: Research Papers, pp. 186-191, Belgium, Brussels, October 2018. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
W18-6319.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. Technical report, 2019.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1, pp. 1715-1725, 2016.
Harshil Shah and David Barber. Generative neural machine translation. In Advances
in Neural Information Processing Systems 31:	Annual Conference on Neural In-
formation Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal,
Canada., pp. 1353-1362,	2018. URL http://papers.nips.cc/paper/
7409-generative-neural-machine-translation.
Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, Xilin Chen, and Jie Zhou. Retrieving
sequential information for non-autoregressive neural machine translation. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 3013-3024, Florence,
Italy, July 2019.
Yanyao Shen, Xu Tan, Di He, Tao Qin, and Tie-Yan Liu. Dense information flow for neural machine
translation. In Proceedings of the 2018 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),
pp. 1294-1303, 2018.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neu-
ral networks. In Advances in Neural Information Processing Systems 27: Annual Con-
ference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal,
Quebec, Canada, pp. 3104-3112, 2014. URL http://papers.nips.cc/paper/
5346-sequence-to-sequence-learning-with-neural-networks.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. Jane: Open source hierarchical trans-
lation, extended with reordering and lexicon models. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR, pp. 262-270, 2010.
Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov random
field language model. CoRR, abs/1902.04094, 2019. URL http://arxiv.org/abs/1902.
04094.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7.
Mingxuan Wang, Zhengdong Lu, Jie Zhou, and Qun Liu. Deep neural machine translation with
linear associative unit. In Proceedings of the 55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pp. 136-145, 2017.
12
Under review as a conference paper at ICLR 2020
Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, and Xu Sun. Imitation learning for non-
autoregressive neural machine translation. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pp.1304-1312, Florence, Italy, July 2019.
Baosong Yang, Longyue Wang, Derek Wong, Lidia S Chao, and Zhaopeng Tu. Convolutional self-
attention networks. arXiv preprint arXiv:1904.03107, 2019.
Biao Zhang, Deyi Xiong, Hong Duan, Min Zhang, et al. Variational neural machine translation. In
Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp.
521-530, 2016.
Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward
connections for neural machine translation. Transactions of the Association for Computational
Linguistics, 4:371-383, 2016.
13
Under review as a conference paper at ICLR 2020
Appendix
In this section we present several ablation studies as well as a deeper look into the decoding to
further investigate the MTM.
A Length Modeling
Autoregressive models determine the output length by stopping the generation process once a special
token that marks the end of the sentence (e.g. ’</S>’) is predicted. This approach is not compatible
with the MTM decoding as target tokens for all positions are predicted in parallel. Therefore we
assume a given output length I and a train length model p(I |f1J) on the bilingual training data. In
training, the true length is used, and in decoding, we choose argmaxIp(I|f1J) as the target length.
To model p(I |f1J) and determine the impact of length model quality on the MTM, we test length
models of different complexity:
•	Count-based Table. For the unseen source lengths in training, we assume I = J.
•	Poisson Distribution: a more smoothly parametrized model. For J’s not appearing
in the training data, we back off to a global distribution with the parameter λ that is
learned via maximum likelihood overall source/target length pairs in the training data, i.e.
{(I,J)∣(fJ,e1) eD}.
•	Recurrent Neural Network (RNN): We take the last hidden state as the input to a target
length classifier, i.e. a linear projection with a softmax layer over the possible target lengths
I∈ {1, 2,..., 200}.
•	Bidirectional RNN: a variation of the above which employs a BiLSTM and uses the last
hidden state of the forward and the backward LSTM for length prediction.
Table 2 verifies that the translation performance depends highly on the length model. We also report
the averaged absolute difference of the predicted length and the reference length.
Table 2: Comparison of target hypothesis length modeling for the MTM on the development set
(newsdev2016).
Length Model	Avg. ∆I	Bleu [%]
Count-based	0.70	30.4
Poisson distribution	0.83	31.5
RNN	0.53	31.7
Bidirectional RNN	0.23	31.9
Reference	-	33.5
The simplest count-based model shows the worst performance, where the target length is merely
determined by a frequency table. The Poisson distribution models the length more systematically,
giving +1.1% Bleu against the count-based model. RNN models consider not only the source
length but also the whole sentence semantics, and slightly outperform the Poisson distribution. The
bidirectional RNN model predicts the target length even better, but the translation performance only
improves marginally. Note that using the Reference length improves even further by +1.6% Bleu
over our strongest system, which shows that a good length model is a crucial component of the
MTM system.
B Model Hyperparameters
As discussed earlier, the MTM architecture is equivalent to a traditional transformer encoder. Nev-
ertheless, the way it is applied to the task at hand differs very much, even compared to an MLM.
Thus it was crucial to do a thorough hyperparameter search to obtain an optimal model performance.
The baseline MTM configuration we present here is already the product of many preliminary exper-
iments, setting the number of attention heads h = 16, dropout Pdrop = 0.1, and the learning rate
reduction scheme. The ablation study presented in table 3 highlights the importance of both the
14
Under review as a conference paper at ICLR 2020
Table 3: Comparison for a selected set of hyperparameters: number of layers N, hidden size dmodel,
number of attention heads h, attention key size dk, attention value size dv , dropout probability Pdrop
I N (⇔ NenC + Ndec)	dmodel	h	dk	dv	Pdrop	BLEU [%]	Ter [%]
MTM I	12	512	16	32	32	0.1	31.9	55.1
		4	128	128		31.6	55.5
		8	64	64		31.9	54.9
		32	16	16		31.3	55.8
6						30.6	56.4
18						32.0	54.9
24						32.4	54.6
	256		16	16		29.4	57.8
	1024		64	64		32.2	54.8
					0.0	29.2	58.1
					0.2	31	56.1
Enc-Dec ∣	6+6	512	8	64	64	0.2	34.7	52.3
3+3						35.0	51.8
9+9						35.7	51.4
12+12						35.6	51.6
	256		32	32		33.6	53.1
	1024		128	128		35.5	51.5
number of attention heads and especially dropout. It also shows that it was crucial to increase the
model depth N compared to the standard transformer encoder, by matching the total number of
layers N = 12 as they are used in an encoder-decoder model.
C Derivation of Decoding
In this section, we show a detailed derivation of the decoding process which justifies our iterative
optimization procedure and modularizes the unmasking procedure to apply the application of various
decoding strategies.
A	♦	1	1	. 1 ∙ 1 .	. 1	.< τi .1	1 ∙ . r∙ 1	. ∙	ι .	.	Qi
Assuming we have a hypothesized target length I , the goal is to find an optimal target sequence E
given length I and source sentence F :
E(F,I) = arg max {p(E∖I, F)}	(20)
E
The MTM decoding to find such a sequence is performed in T iterations, whose intermediate hy-
potheses are introduced as latent variables E(1) , . . . , E(T-1) :
X	P(E(T) ,...,E ⑴ ∖F,I)]	(21)
E(1),...,E(T -1)	
arg max
E(T)
where the last iteration should provide the final prediction, i.e. E(T) := E. Applying the chain rule
and a first-order Markov assumption on E(τ) yields:
X YY [p(E(T)∖E(TT),F,I)]]	(22)
E(1),...,E(T -1) τ=1	
arg max
E(T)
with E0 := </M>,..., </M> a sequence of length I. In a next step, We approximate the sum by a
maximization and subsequently apply a logarithm to get:
≈ arg max
E(T)
max
E(1),...,E(T -1)
T
Xhlog P(E(T )∖E(TT),F,I)i
τ =1、	{z	"
=:Q(E(T)IE(T-1),F,I)
(23)
15
Under review as a conference paper at ICLR 2020
To simplify further derivations, we introduce the score function Q here and do another approxima-
tion by considering only the score Q from a single maximum timestep instead of the full sum over
τ = 1,...,T.
≈ arg maX maX
E(T)	E(1),...,E(T-1)
JmaXT Q(E(T)|E(TT),F,I)
(24)
Even with this approximation, we are still trying to find a value for each E(T) that optimizes
the score of another iteration T via the connection of dependencies in Q. As this is impracti-
cal to compute, we alleviate the problem by focusing on a step-wise maximization. For this we
define the sequence E(T) (with E(O) := E(O)) as the best hypothesis of iteration T given E(T-1), i.e.:
E(T) := E(T)(E(TT),F,I) = argmaxQ(E(T)∣E(TT),F,I)	(25)
E(τ)
If We use E(T-1) instead of E(T-1) as the dependency in Q, We restrict the optimization to maxi-
mizing each step independently, given its predecessors optimum:
E(F,I) ≈ arg max < max max Q(E(T )∖E (TT),F,I) ∖	(26)
E(T)	E(1),...,E(T-1) T=1,...,T
Ideally, this iterative procedure should improve the score Q in each iteration, i.e.:
max Q(E(T+1)∖E(T ),F,I) ≥ max Q(E(T) ∖E (TT),F, I) ∀τ = 1,...,T — 1	(27)
E(τ+1)	E(τ)
While Equation (27) is not true for the general case We observe empirically that this the statement
is true on average (see Figure 3). This means that the maximum score to be obtained in the last
iteration T :
E(f,I) ≈ arg max max	[q(E (T )∖E(T-1),F,I)]}	(28)
E(T)	E(1),...,E(T-1)
Which can be simplified to:
=arg max Q(E(T )∖E(T -1),F,I) = E(T)	(29)
E(T)
Thus, in order to approximate the best hypothesis it is sufficient to folloW the recursive definition
presented in Equation (25), Which can be computed straight forWard resulting in an iterative decod-
ing scheme.
To alloW a more fine-grained control of the decoding process, We introduce a (partially) masked
version of hypothesis E(t-1) as a latent variable E(t-1) in the score computation:
Q(E(T )∖E (TT),F,I) = log p(E(τ )∖E(TT),F,I)	(30)
= log X p(E(τ),E(TT)IE(TT),F,I)	(31)
ETT)	_
= log X Pθ(E(T)∖E(TT),F,I) ∙Pm(E(TT)∖E(T-I),I)	(32)
E(T-I)
Where the probability of E(T) is modeled by the neural netWork With parameters θ and the masked
sequence E(τ-I) is given by pm, which defines a specific search strategy (Section 3.3.1). Note here
that, to remove redundant information in the modeling, pθ drops the direct dependency on E(τ-I)
and pm ignores the source sentence F. As the last simplifying step, the intractable sum over the
masked sequence E(T T) is replaced by a point sample, i.e. E (τ T)〜Pm(E (τ T)IE (τ T)) for each
T = 1, . . . , T. This may even provide the exact result ifpm is designed for a deterministic search
strategy.
Q(E(τ )∖E (TT),F,I)= log Pθ (E(T )∖E(TT),F,I)+log Pm(E(TT)IE (TT),I)	(33)
= log Pθ (E(T )∖E(TT),F,I)	(34)
16
Under review as a conference paper at ICLR 2020
D Decoding strategies
For completeness we report the strongest result for each decoding strategy from Figure 4 in Table 4.
Table 4: Comparison of different decoding strategies in greedy decoding for the MTM on the devel-
opment set (newsdev2016) of the Romanian→English task.
	Decoding		
Category	Complexity	Decoding Strategy	Bleu [%]
Non-autoregressive	O(1)	Predict all positions	13.8
Non-autoregressive + Refinements	O(T)	Fully unmasking Confidence-based (fixed T)	22.0 31.9
	O(I)	Confidence-based (one-by-one)	31.9
			
		Left-to-right	31.5
Autoregressive	O(I)	Right-to-left Middle-out	31.5 30.6
		Random	29.2
17