Under review as a conference paper at ICLR 2020
Learning Human Postural Control with Hier-
archical Acquisition Functions
Anonymous authors
Paper under double-blind review
Ab stract
Learning control policies in robotic tasks requires a large number of interactions
due to small learning rates, bounds on the updates or unknown constraints. In
contrast humans can infer protective and safe solutions after a single failure or
unexpected observation. In order to reach similar performance, we developed a
hierarchical Bayesian optimization algorithm that replicates the cognitive infer-
ence and memorization process for avoiding failures in motor control tasks. A
Gaussian Process implements the modeling and the sampling of the acquisition
function. This enables rapid learning with large learning rates while a mental re-
play phase ensures that policy regions that led to failures are inhibited during the
sampling process. The features of the hierarchical Bayesian optimization method
are evaluated in a simulated and physiological humanoid postural balancing task.
We quantitatively compare the human learning performance to our learning ap-
proach by evaluating the deviations of the center of mass during training. Our
results show that we can reproduce the efficient learning of human subjects in
postural control tasks which provides a testable model for future physiological
motor control tasks. In these postural control tasks, our method outperforms stan-
dard Bayesian Optimization in the number of interactions to solve the task, in the
computational demands and in the frequency of observed failures.
1 INTRODUCTION
Autonomous systems such as anthropomorphic robots or self-driving cars must not harm cooperat-
ing humans in co-worker scenarios, pedestrians on the road or them selves. To ensure safe interac-
tions with the environment state-of-the-art robot learning approaches are first applied to simulations
and afterwards an expert selects final candidate policies to be run on the real system. However, for
most autonomous systems a fine-tuning phase on the real system is unavoidable to compensate for
unmodelled dynamics, motor noise or uncertainties in the hardware fabrication.
Several strategies were proposed to ensure safe policy exploration. In special tasks like in robot arm
manipulation the operational space can be constrained, for example, in classical null-space control
approaches Baerlocher & Boulic (1998); Slotine (1991); Choi & Kim (2000); Gienger et al. (2005);
Saab et al. (2013); Modugno et al. (2016) or constraint black-box optimizer Hansen et al. (2003);
Wierstra et al. (2008); Kramer et al. (2009); Sehnke et al. (2010); Arnold & Hansen (2012). While
this null-space strategy works in controlled environments like research labs where the environmental
conditions do not change, it fails in everyday life tasks as in humanoid balancing where the priorities
or constraints that lead to hardware damages when falling are unknown.
Alternatively, limiting the policy updates by applying probabilistic bounds in the robot configuration
or motor command space Bagnell & Schneider (2003); Peters et al. (2010); Rueckert et al. (2014);
Abdolmaleki et al. (2015); Rueckert et al. (2013) were proposed. These techniques do not assume
knowledge about constraints. Closely related are also Bayesian optimization techniques with mod-
ulated acquisition functions Gramacy & Lee (2010); Berkenkamp et al. (2016); Englert & Toussaint
(2016); Shahriari et al. (2016) to avoid exploring policies that might lead to failures. However, all
these approaches do not avoid failures but rather an expert interrupts the learning process when it
anticipates a potential dangerous situation.
1
Under review as a conference paper at ICLR 2020
rewards
1
1
policies
Figure 1: Illustration of the hierarchical BO algorithm. In standard BO (clock-wise arrow), a map-
ping from policy parameters to rewards is learned, i.e., φ 7→ r ∈ R1. We propose a hierarchical
process, where first features κ are sampled and later used to predict the potential of policies condi-
tioned on these features, φ∣κ → r. The red dots show the first five successive roll-outs in the feature
and the policy space of a humanoid postural control task.
features
All the aforementioned strategies cannot avoid harming the system itself or the environment with-
out thorough experts knowledge, controlled environmental conditions or human interventions. As
humans require just few trials to perform reasonably well, it is desired to enable robots to reach sim-
ilar performance even for high-dimensional problems. Thereby, most approaches are based on the
assumption of a ”low effective dimensionality”, thus most dimensions of a high-dimensional prob-
lem do not change the objective function significantly. In Chen et al. (2012) a method for relevant
variable selection based on Hierarchical Diagonal Sampling for both, variable selection and function
optimization, has been proposed. Randomization combined with Bayesian Optimization is proposed
in Wang et al. (2013) to exploit effectively the aforementioned ”low effective dimensionality”. In Li
et al. (2018) a dropout algorithm has been introduced to overcome the high-dimensionality problem
by only train onto a subset of variables in each iteration, evaluating a ”regret gap” and providing
strategies to reduce this gap efficiently. In Rana et al. (2017) an algorithm has been proposed which
optimizes an acquisition function by building new Gaussian Processes with sufficiently large kernel-
lengths scales. This ensures significant gradient updates in the acquisition function to be able to use
gradient-dependent methods for optimization.
The contribution of this paper is a computational model for psychological motor control experiments
based on hierarchical acquisition functions in Bayesian Optimization (HiBO). Our motor skill learn-
ing method uses features for optimization to significantly reduce the number of required roll-outs.
In the feature space, we search for the optimum of the acquisition function by sampling and later use
the best feature configuration to optimize the policy parameters which are conditioned on the given
features, see also Figure 1. In postural control experiments, we show that our approach reduces the
number of required roll-outs significantly compared to standard Bayesian Optimization. The focus
of this study is to develop a testable model for psychological motor control experiments where well
known postural control features could be used. These features are listed in Table 3. In future work
we will extend our model to autonomous feature learning and will validate the approach in more
challenging robotic tasks where ’good’ features are hard to hand-craft.
2 METHODS
In this section we introduce the methodology of our hierarchical BO approach. We start with the
general problem statement and afterwards briefly summarize the concept of BO which we use here
2
Under review as a conference paper at ICLR 2020
as a baseline. We then go through the basic principles required for our algorithm and finally we
explain mental replay.
2.1	Problem Statement
The goal in contextual policy search is to find the best policy ∏* (θ∣c) that maximizes the return
T
J(θ) = E X {rt(xt, Ut) | ∏(θ∣c)}	,	(1)
t=0
with reward rt(xt, ut) at time step t for executing the motor commands ut in state xt.
For learning the policy vector and the context, we collect samples of the return J (θ[k] ) ∈ R1, the
evaluated policy parameter vector θ[k] ∈ Rm and the observed contextual features modeled by
c[k] ∈ Rn. All variables used are summarized in Table 1. The optimization problem is defined as
hθ*, c*i = argmaxE [ J(θ) | π(θ∣c)] .	(2)
θ,c
The optimal parameter vector and the corresponding context vector can be found in an hierarchical
optimization process which is discussed in Section 2.3.
2.2	Bayesian Optimization (baseline)
Bayesian Optimization (BO) has emerged as a powerful tool to solve various global optimization
problems where roll-outs are expensive and a sufficient accurate solution has to be found with only
few evaluations, e.g. Lizotte et al. (2007); Martinez-Cantin et al. (2007); Calandra et al. (2016). In
this paper we use the standard BO as a benchmark for our proposed hierarchical process. Therefore,
we now briefly summarize the main concept. For further details refer to Shahriari et al. (2016).
The main concept of Bayesian Optimization is to build a model for a given system based on the
so far observed data D = {X, y}. The model describes a transformation from a given data point
x ∈ X to a scalar value y ∈ y, e.g. from the parameter vector θ to the return J (θ). Such model
can either be parametrized or non-parametrized and is used for choosing the next query point by
evaluating an acquisition function α(D). Here, we use the non-parametric Gaussian Processes
(GPs) for modeling the unknown system which are state-of-the-art model learning or regression
approaches Williams & Rasmussen (1996; 2006) that were successfully used for learning inverse
dynamics models in robotic applications Nguyen-Tuong et al. (2009); Calandra et al. (2015). For
comprehensive discussions we refer to Rasmussen (2003); Nguyen-Tuong & Peters (2011).
GPs represent a distribution over a partial observed system in the form of
y
y*
m(X)
m(X*)
'k(x,x )	κ (x,x≠)B
K(X*,X) K(X*,X*)]),
(3)
〜N
where D = {X, y} are the so far observed data points and D* = {X*, y*} the query points. This
representation is fully defined by the mean m and the covariance K. We chose m(x) = 0 as mean
Table 1: Variable definitions used in this paper.
J(θ)	R1	return of a rollout
rt(xt, ut)	R1	intermediate reward give at time t
xt	Rd	state of the system
ut	Rl	motor commands of the system
π(θlC)		unknown control policy
θ	Rm	policy vector
c	Rn	context vector
s[k]	{0, 1}	flag indicating the success of rollout k
3
Under review as a conference paper at ICLR 2020
function and as covariance function a Matern kernel Matern (1960). It is a generalization of the
squared-exponential kernel that has an additional parameter ν which controls the smoothness of the
resulting function. The smoothing parameter can be beneficial for learning local models. We used
Mateern kernels
k(xp, Xq) = σ 2ν-1「(v) A HV A + σy δpq ,
with V = 5 / 2 and the gamma function Γ, A = (2√v∣∣Xp - Xq||) /l and modified Bessel function
Hν Abramowitz & Stegun (1965). The length-scale parameter of the kernel is denoted by σ, the
variance of the latent function is denoted by σy and δpq is the Kronecker delta function (which
is one if p = q and zero otherwise). Note that for ν = 1/2 the Mateern kernel implements the
squared-exponential kernel. In our experiments, we optimized the hyperparameters θ = [σ, l, σy]
by maximizing the marginal likelihood Williams & Rasmussen (2006).
Predictions for a query points D* = {x* ,y*} can then be determined as
E(y∕y,X, x*) = μ(x*) = m* + K>K-1(y - m)
var(y*∣y,X, x*)= σ(x*) = K** - K>K-1K*.
(4)
The predictions are then used for choosing the next model evaluation point Xn based on the acqui-
sition function α(X; D). We use expected improvement (EI) Mockus et al. (1978) which considers
the amount of improvement
α(x;D) = (μ(χ) -T)φ ("[x； + ξ
+ σ(x)Φ (巫I),
(5)
where τ is the so far best measured value max(y), Φ the standard normal cumulative distribution
function, φ the standard normal probability density function and ξ 〜σξU (-0.5,0.5) a random
value to ensure a more robust exploration. Samples, distributed over the area of interest, are evalu-
ated and the best point is chosen for evaluation based on the acquisition function values.
2.3	Hierarchical Sampling from Acquisition Functions in Bayesian
Optimization
We learn a joint distribution p(J(θ[k] ), θ[k] , c[k] ) over k = 1, ..., K roll-outs of observed triples.
This distribution is used for as a prior of the acquisition function in Bayesian optimization. However,
instead of directly conditioning on the most promising policy vectors using αBO = α(θ; D), we
propose an iterative conditioning scheme. Therefore, the two acquisition functions
αc = α(c; D),	(6)
αθ = α(θ; c, D),	(7)
are employed, where for Equation (7), the evaluated mean μ(θ; c) and variance σ(θ; c) for the
parameter θ are conditioned on the features c. The hierarchical optimization process works then as
follows:
In the first step we estimate the best feature values based on a GP model using the acquisition
function from Equation (6)
c[k+1] = max α(c; D[1:k]).	(8)
c
These feature values are then used to condition the search for the best new parameter θ[k+1] using
Equation (7)
θ[k+1] = max α(θ; c[k+1], D[1:k]).	(9)
θ
We subsequently continue evaluating the policy vector θ[k+1] using the reward function presented
in Equation (1). Finally, the new data point hJ(θ[k+1] ), θ[k+1] , c[k+1] i can be added to the set of
data points D.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Hierarchical Acquisition Function Sampling for Bayesian Optimization (HiBO)
1:	Initialize the dataset D[1:k] = hJ (θ[k]), θ[k], c[k]i with K rollouts of sampled policies θ.
2:	for k = K, K+1, ... do
3：	c[k+1] = argmaxc a(D[1:k]) : D → R1 using Eq. 6.
4：	θ[k+1] = argmaχθ α(D[1ik^; c[k+1]) using Eq. 7.
5:	Evaluate the policy vector θ[k+1] using Eq. 1.
6： Augment D = [D[1:k] , hJ (θ[k+1]), θ[k+1] , c[k+1]il].
7： end for
2.4 Mental Replay
To ensure robustness for Bayesian Optimization, mental replays can be generated. Therefore, the
new training data set hJ (θ[k+1] ), θ[k+1] , c[k+1]i, generated by the policy parameter θ[k+1], will be
enlarged by augmenting perturbed copies of the policy parameter θ[k+1] . These l copies are then
used for generating the augmented training data sets
D[k+1] = hJ (θ[k+1]), θ[k+1], c[k+1]il.
(10)
Here, the transcript〈•〉1 denotes l perturbed copies of the given set. Hence, perturbed copies of the
parameters θ[k+1] and features c[k+1] are generated keeping the objective J (θ[k+1] ) constant. In
Algorithm (1) the complete method is summarized. We evaluate different replay strategies in the
result Section in 3.3.
3 RESULTS
In this section we first present observations on human learning during perturbed squat-to-stand
movements. We compare the learning results ofa simulated humanoid to the learning rates achieved
by the human participants. Second, we evaluate our hierarchical BO approach in comparison to our
baseline, the standard BO. Third we evaluate the impact of mental replays on the performance of
our algorithm.
3.1	Human postural balancing
To observe human learning, we designed an experiment where 20 male participants were sub-
jected to waist pull perturbation during squat-to-stand movements, see Figure 2(a). Participants
had to stand up from a squat position without making any compensatory steps (if they made a
step, such trial was considered a fail). Backward perturbation to the centre of mass (CoM) was
applied by a pulling mechanism and was dependent on participants’ mass and vertical CoM velocity.
On average, participants required 6 trials (σhuman = 3.1) to successfully complete the motion. On
the left panel of Figure 3, a histogram of the required trials before the first success is shown. On
the right panel, the evaluation results for the simulated humanoid are presented (details on the
implementation are discussed in the subsequent Subsection 3.2). The human learning behavior is
faster and more reliable than the learning behavior of the humanoid. However, humans can exploit
fundamental knowledge about whole body balancing whereas our humanoid has to learn everything
from scratch. Only the gravity constant was set to zero in our simulation, as we are only interested
in the motor adaptation and not in gravity compensation strategies.
Adaptation was evaluated using a measure based on the trajectory area (TA) at every episode as
defined in ?. The Trajectory area represents the total deviation of the CoM trajectory with respect to
a straight line. The trajectory area of a given perturbed trajectory is defined as the time integral of
the distance of the trajectory points to the straight line in the sagittal plane：
tend
TA(ex) = /	x(t)∣y∕(t)∣dt
t0
(11)
5
Under review as a conference paper at ICLR 2020
(a)
Figure 2: (a) Psychological postural control setup for the squat-to-stand movements. (b) Illustration
of the simulated postural control task. An external perturbation is applied during the standing up
motion and the robot has to learn to counter balance. The perturbation is proportional to the CoM
velocity in the superior direction in the sagittal plane.
Figure 3: Histogram showing the number of required trials until the first successful episode for both,
the human experiments and the simulated humanoid, With μhuman = 6.15, σhuman = 3.1, μhumanoid =
8.3 and σhumanoid = 6.38.
A positive sign represents the anterior direction While a negative sign represents the posterior direc-
tion. The mean and standard deviation for the trajectory area over the number of training episodes
for all participants are depicted in Figure 4. Comparing these results With the simulation results of
our humanoid shoWs that the learning rate using our approach is similar to the learning rate of real
humans.
Figure 4: Mean and standard deviaton of the trajectory area (T A) With regard to the number of
episodes for both, the human experiments and the simulated humanoid. For the humanoid the x-
coordinates have been shifted about -0.5 to account for the stretched arms. In addition, the trajec-
tory area of the humanoid has been scaled With the factor 0.1 and shifted about -0.2 to alloW easier
comparison.
6
Under review as a conference paper at ICLR 2020
Figure 5: Comparison of the rewards of the proposed HIBO algorithm and the state-of-the-art ap-
proach Bayesian Optimization. Shown are average statistics (mean and standard deviation) over 20
runs.
3.2	Humanoid postural balancing
To test the proposed algorithm we simulated a humanoid postural control task as shown in Fig-
ure 2(b). The simulated humanoid has to stand up and is thereby exposed to an external pertubation
proportional to the velocity of the CoM in the superior direction in the sagittal plane. The pertuba-
tion is applied during standing up motion such that the robot has to learn to counter balance. The
simulated humanoid consist of four joints, connected by rigid links, where the position of the first
joint is fixed onto the ground. A PD-controller is used with KP,i and KD,i fori = 1, 2, 3, 4 being the
proportional and derivative gains. In our simulations the gains are set to KP,i = 400 and KD,i = 20
and an additive control noise e 〜 N(0,1) has been inserted such that the control input for a certain
joint becomes
ui = KP,i eP,i + KD,i eD,i + e,	(12)
where eP,i, eD,i are the joint errors regarding the target position and velocity. The control gains can
also be learned. The goal positions and velocities for the joints are given. As parametrized policy,
We use a via point [φi, φi], where φi is the position of joint i at time tvia and φi the corresponding
velocity. Hence, the policy is based on 9, respectively 17 parameters (if the gains are learned),
which are summarized in Table 2. For our simulations we handcrafted 7 features, namely the overall
success, the maximum deviation of the CoM in x and y direction and the velocities of the CoM for
the x and y directions at 200 ms respectively 400 ms. In Table 3 the features used in this paper are
summarized. Simultaneously learning of the features is out of scope of this comparison to human
motor performance but part of future work.
We simulated the humanoid in each run for a maximum of tmax = 2 s with a simulation time step
of dt = 0.002 s, such that a maximum of N = 1000 simulation steps are used. The simulation
has been stopped at the simulation step Nend if either the stand up has been failed or the maximum
simulation time has been reached. The return of a roll-out J(θ) is composed according to J(θ) =
-(cbalance + ctime + ccontrol) with the balancing costs cbalance = 1/Nend Pi=en1 ||xCoM,target - xCoM,i || ,
the time dependent costs ctime = (N - Nend) and control costs of ccontrol = 10-8 PiN=en1d Pj4=1 ui2j.
We compared our approach with our baseline, standard Bayesian Optimization. For that we used
the features 4, 5 in 3 and set the number of mental replays to l = 3. We initialized both, the BO and
the HiBO approach with 3 seed points and generated average statistics over 20 runs. In Figure 5 the
comparison between the rewards of the algorithms over 50 episodes is shown. In Figure 6 (a) the
Table 2:	Policy parameter description

,i
,i
proportional gain for joint i
derivative gain for joint i
angle of joint i at the via point
angular velocity of joint i at the via point
time for switching from the via point to goal position
7
Under review as a conference paper at ICLR 2020
(a)	(b)	(c)
Figure 6: Comparison of the number of successful episodes of the proposed HIBO algorithm and
the state-of-the-art approach Bayesian Optimization for different internal experience replay itera-
tions. The last three algorithms implemented an automatic relevance determination of the Gaussian
Process features and policy parameters. Shown are average statistics (mean and standard deviation)
over 20 runs and the true data values are denoted by the black dots.
number of successful episodes is illustrated. Our approach requires significantly fewer episodes to
improve the reward then standard Bayesian Optimization (10±3 vs 45±5) and has a higher success
quote (78% ± 24% vs 60% ± 7%).
We further evaluated the impact of the different features on the learning behavior. In Figure 6 (b)
the average statistics over 20 runs for different selected features with 3 mental replays are shown.
All feature pairs generate better results on average then standard BO, whereas for the evaluated task
no significant difference in the feature choice was observed.
3.3 Exploiting Mental Replays
We evaluated our approach with additional experience replays. For that we included an additive
noise of e^p 〜N(0,0.05) to perturb the policy parameters and features. In Figure 6 (C) average
statistics over 20 runs of the success rates for different number of replay episodes are shown (rep3 =
3 replay episodes). Our proposed algorithm works best with a number of 3 replay episodes. Five or
more replays in every iteration steps even reduce the success rate of the algorithm.
4 CONCLUSION
We introduced HiBO, a hierarchical approach for Bayesian Optimization. We showed that HiBO
outperforms standard BO in a complex humanoid postural control task. Moreover, we demonstrated
the effects of the choice of the features and for different number of mental replay episodes. We
compared our results to the learning performance of real humans at the same task. We found that the
learning behavior is similar. We found that our proposed hierarchical BO algorithm can reproduce
the rapid motor adaptation of human subjects. In contrast standard BO, our comparison method, is
about four times slower. In future work, we will examine the problem of simultaneously learning
task relevant features in neural nets.
Table 3:	Feature description
Feature 1 success
Feature 2 maximum deviation of the CoM in x direction
Feature 3 maximum deviation of the CoM in y direction
Feature 4 velocity of the CoM in x direction at 200 ms
Feature 5 velocity of the CoM in y direction at 200 ms
Feature 6 velocity of the CoM in x direction at 400 ms
Feature 7 velocity of the CoM in y direction at 400 ms
8
Under review as a conference paper at ICLR 2020
References
Abbas Abdolmaleki, Rudolf Lioutikov, Jan R Peters, Nuno Lau, Luis Pualo Reis, and Gerhard
Neumann. Model-based relative entropy stochastic search. In Advances in Neural Information
Processing Systems, pp. 3537-3545, 2015.
Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas,
graphs, and mathematical tables, volume 55. Courier Corporation, 1965.
Dirk V Arnold and Nikolaus Hansen. A (1+ 1)-cma-es for constrained optimisation. In Proceedings
of the 14th annual conference on Genetic and evolutionary computation, pp. 297-304. ACM,
2012.
Paolo Baerlocher and Ronan Boulic. Task-priority formulations for the kinematic control of highly
redundant articulated structures. In IROS, pp. 13-17, 1998.
J Andrew Bagnell and Jeff Schneider. Covariant policy search. In Proceedings of the 18th interna-
tional joint conference on Artificial intelligence, pp. 1019-1024. Morgan Kaufmann Publishers
Inc., 2003.
Felix Berkenkamp, Angela P Schoellig, and Andreas Krause. Safe controller optimization for
quadrotors with gaussian processes. In Robotics and Automation (ICRA), 2016 IEEE Interna-
tional Conference on, pp. 491-496. IEEE, 2016.
Roberto Calandra, Serena Ivaldi, Marc Peter Deisenroth, Elmar Rueckert, and Jan Peters. Learning
inverse dynamics models with contacts. In 2015 IEEE International Conference on Robotics and
Automation (ICRA), pp. 3186-3191. IEEE, 2015.
Roberto Calandra, Andre Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimization
for learning gaits under uncertainty. Annals of Mathematics and Artificial Intelligence, 76(1-2):
5-23, 2016.
Bo Chen, Rui Castro, and Andreas Krause. Joint optimization and variable selection of high-
dimensional gaussian processes. arXiv preprint arXiv:1206.6396, 2012.
Su Il Choi and Byung Kook Kim. Obstacle avoidance control for redundant manipulators using
collidability measure. Robotica, pp. 143-151, 2000.
Peter Englert and Marc Toussaint. Combined optimization and reinforcement learning for manipu-
lation skills. In Robotics: Science and Systems, 2016.
Michael Gienger, Herbert Janssen, and Christian Goerick. Task-oriented whole body motion for
humanoid robots. In IEEE-RAS, pp. 238-244, 2005.
Robert B Gramacy and Herbert KH Lee. Optimization under unknown constraints. arXiv preprint
arXiv:1004.4027, 2010.
N. Hansen, S.D. Muller, and P. Koumoutsakos. Reducing the time complexity of the derandomized
evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation, 11
(1):1-18, 2003.
Oliver Kramer, Andre Barthelmes, and Gunter Rudolph. Surrogate constraint functions for Cma
evolution strategies. In KI, pp. 169-176. Springer, 2009.
Cheng Li, Sunil Gupta, Santu Rana, Vu Nguyen, Svetha Venkatesh, and Alistair Shilton. High
dimensional bayesian optimization using dropout. arXiv preprint arXiv:1802.05400, 2018.
Daniel J Lizotte, Tao Wang, Michael H Bowling, and Dale Schuurmans. Automatic gait optimization
with gaussian process regression. In IJCAI, volume 7, pp. 944-949, 2007.
Ruben Martinez-Cantin, Nando de Freitas, Arnaud Doucet, and Jose A Castellanos. Active policy
learning for robot planning and exploration under uncertainty. In Robotics: Science and Systems,
volume 3, pp. 321-328, 2007.
9
Under review as a conference paper at ICLR 2020
B Matern. Spatial variation: Meddelanden fran statens Skogsforskningsinstitut. Lecture Notes in
Statistics, 36:21, 1960.
Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of bayesian methods for
seeking the extremum. Towards global optimization, 2(117-129):2, 1978.
Valerio Modugno, Gerard Neumann, Elmar Rueckert, Giuseppe Oriolo, Jan Peters, and Serena
Ivaldi. Learning soft task priorities for control of redundant robots. In Robotics and Automa-
tion (ICRA), 2016 IEEE International Conference on, pp. 221-226. IEEE, 2016.
Riccardo Moriconi, KS Kumar, and Marc P Deisenroth. High-dimensional bayesian optimization
with manifold gaussian processes. arXiv preprint arXiv:1902.10675, 2019.
Duy Nguyen-Tuong and Jan Peters. Model learning for robot control: a survey. Cognitive process-
ing, 12(4):319-340, 2011.
Duy Nguyen-Tuong, Matthias Seeger, and Jan Peters. Model learning with local gaussian process
regression. Advanced Robotics, 23(15):2015-2034, 2009.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Proceedings
of the Twenty-Fourth AAAI Conference on Artificial Intelligence, pp. 1607-1612. AAAI Press,
2010.
Santu Rana, Cheng Li, Sunil Gupta, Vu Nguyen, and Svetha Venkatesh. High dimensional bayesian
optimization with elastic gaussian process. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 2883-2891. JMLR. org, 2017.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine
Learning, pp. 63-71. Springer, 2003.
Elmar Rueckert, Gerhard Neumann, Marc Toussaint, and Wolfgang Maass. Learned graphical mod-
els for probabilistic planning provide a new class of movement primitives. Frontiers in Computa-
tional Neuroscience, 6(97), 2013. doi: 10.3389/fncom.2012.00097.
Elmar Rueckert, Max Mindt, Jan Peters, and Gerhard Neumann. Robust policy updates for stochastic
optimal control. In Humanoid Robots (Humanoids), 2014 14th IEEE-RAS International Confer-
ence on, pp. 388-393. IEEE, 2014.
Elmar Rueckert, Jernej Camernik, Jan Peters, and Jan Babic. Probabilistic movement models show
that postural control precedes and predicts volitional motor control. Nature Publishing Group:
Scientific Reports, 6(28455), 2016. doi: 10.1038/srep28455.
Layale Saab, Oscar E Ramos, Francois Keith, Nicolas Mansard, Philippe Soueres, and Jean-Yves
Fourquet. Dynamic whole-body motion generation under rigid contacts and other unilateral con-
straints. IEEE Transactions on Robotics, 29(2):346-362, 2013.
Frank Sehnke, Christian Osendorfer, Thomas RUCkStieβ, Alex Graves, Jan Peters, and Jurgen
Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551-559, 2010.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the
human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 1(104):
148-175, 2016.
Siciliano B Slotine. A general framework for managing multiple tasks in highly redundant robotic
systems. In proceeding of 5th International Conference on Advanced Robotics, volume 2, pp.
1211-1216, 1991.
Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando De Freitas. Bayesian opti-
mization in high dimensions via random embeddings. In Twenty-Third International Joint Con-
ference on Artificial Intelligence, 2013.
Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Episodic reinforcement learning
by logistic reward-weighted regression. In International Conference on Artificial Neural Net-
works, pp. 407-416. Springer, 2008.
10
Under review as a conference paper at ICLR 2020
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for regression. In Ad-
Vances in neural information processing Systems, pp. 514-520, 1996.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
volume 2. MIT Press Cambridge, MA, 2006.
11