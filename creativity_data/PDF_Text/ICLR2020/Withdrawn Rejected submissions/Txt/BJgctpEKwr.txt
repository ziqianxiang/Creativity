Under review as a conference paper at ICLR 2020
RPGAN: random paths as a latent space for
GAN interpretability
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we introduce Random Path Generative Adversarial Network (RP-
GAN) — an alternative design of GANs that can serve as a tool for generative
model analysis. While the latent space of a typical GAN consists of input vec-
tors, randomly sampled from the standard Gaussian distribution, the latent space
of RPGAN consists of random paths in a generator network. As we show, this de-
sign allows to understand factors of variation, captured by different generator lay-
ers, providing their natural interpretability. With experiments on standard bench-
marks, we demonstrate that RPGAN reveals several interesting insights about the
roles that different layers play in the image generation process. Aside from inter-
pretability, the RPGAN model also provides competitive generation quality and
allows efficient incremental learning on new data.
1	Introduction
Nowadays, deep generative models are an active research direction in the machine learning com-
munity. The dominant methods for generative modeling, such as Generative Adversarial Networks
(GANs), are currently able to produce diverse photorealistic images (Brock et al., 2019; Karras
et al., 2019). These methods are not only popular among academicians, but are also a crucial com-
ponent in a wide range of applications, including image editing (Isola et al., 2017; Zhu et al., 2017),
super-resolution (Ledig et al., 2017), video generation (Wang et al., 2018) and many others.
Along with practical importance, a key benefit of accurate generative models is a more complete
understanding of the internal structure of the data. Insights about the data generation process can
result both in the development of new machine learning techniques as well as advances in industrial
applications. However, most state-of-the-art generative models employ deep multi-layer architec-
tures, which are difficult to interpret or explain. While many works investigate interpretability of
discriminative models (Zeiler & Fergus, 2014; Simonyan et al., 2013; Mahendran & Vedaldi, 2015),
only a few (Chen et al., 2016; Bau et al., 2019) address the understanding of generative ones.
In this work, we propose the Random Path GAN (RPGAN) — an alternative design of GANs that
allows natural interpretability of the generator network. In traditional GAN generators, the stochastic
component that influences individual samples is a noisy input vector, typically sampled from the
standard Gaussian distribution. In contrast, RPGAN generators instead use stochastic routing during
the forward pass as their source of stochasticity. In a nutshell, the RPGAN generator contains
several instances of the corresponding layer. For each sample, only one random instance of each
layer is activated during generation. The training of the RPGAN can then be performed in the same
adversarial manner as in traditional GANs. In the sections below, we show how RPGAN allows to
understand the factors of variation captured by the particular layer and reveals several interesting
findings about the image generation process, e.g. that different layers are “responsible for” coloring
or objection location. As a practical advantage, RPGANs can be efficiently updated to new data
via the simple addition of new instances to the bucket, avoiding re-training the full model from
scratch. Finally, we observe that RPGANs allow the construction of generative models without
nonlinearities, which can significantly speed up the generation process for fully-connected layers.
In summary, the main contributions of our paper are the following:
1
Under review as a conference paper at ICLR 2020
•	We introduce RPGAN — GAN with an alternative source of stochasticity, based on random
routing. While being close to traditional GANs in terms of generation quality, RPGAN
allows natural interpretability and efficient model updates with new data.
•	With extensive experiments on standard benchmarks we reveal several insights about the
image generation process. Many of our insights confirm and extend recent findings from
Bau et al. (2019). Note, that our scheme is more general compared to the technique from
Bau et al. (2019) as RPGAN does not require labeled datasets or pretrained segmentation
models.
•	We open-source the PyTorch implementation of RPGAN with common generator architec-
tures1.
The rest of this paper is organized as follows. In Section 2 we review relevant ideas from prior art.
The proposed Random Path GAN design is described in Section 3 and experimentally evaluated in
Section 4. Section 5 concludes the paper and discusses possible directions for future work.
2	Related work
In this section we briefly describe connections of RPGAN to existing ideas from prior works
Generative adversarial networks. GANs are currently one of the main paradigms in generative
modelling. Since the seminal paper on GANs by Goodfellow et al. (2014), a plethora of alternative
loss functions, architectures, normalizations, and regularization techniques were developed (Kurach
et al., 2019). Today, state-of-the-art GANs are able to produce high-fidelity images, often indis-
tinguishable from real ones (Brock et al., 2019; Karras et al., 2019). In essence, GANs consist of
two networks - a generator and a discriminator, which are trained jointly in an adversarial manner.
In standard GANs, the generation stochasticity is provided by the input noise vector. In RPGANs,
we propose an alternative source of stochasticity by using a fixed input but random routes during
forward pass in the generator.
Specific GAN architectures. Many prior works investigated different design choices for GANs,
but to the best of our knowledge, none of them explicitly aimed to propose an interpretable GAN
model. Hoang et al. (2018) proposed the use of several independent generators to address the mode
collapse problem. Chavdarova & Fleuret (2018) employ several auxiliary local generators and dis-
criminators to improve mode coverage as well. Huang et al. (2017) use layer-wise generators and
discriminators to enforce hidden representations produced by layers of the generator to be similar to
the corresponding representations produced by a reversed classification network. Important differ-
ences of RPGAN compared to the works described above is that it uses random routes as its latent
space and does not enforce to mimic the latent representations of pretrained classifiers.
Interpretability. While interpretability of models based on deep neural networks is an important
research direction, most existing work addresses the interpretability of discriminative models. These
works typically aim to understand the internal representations of networks (Zeiler & Fergus, 2014;
Simonyan et al., 2013; Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2016) or explain deci-
sions produced by the network for particular samples (Sundararajan et al., 2017; Bach et al., 2015;
Simonyan et al., 2013). However, only a few works address interpretability of generative models.
A related work by Bau et al. (2019) develops a technique that allows to identify which parts of the
generator are responsible for the generation of different objects. In contrast, we propose GANs with
alternative source of stochasticity that allows natural interpretation by design. Some of our findings
confirm the results from Bau et al. (2019), which provides stronger evidence about the responsibili-
ties of different layers in the generation process. Note, that the technique (Bau et al., 2019) requires a
pretrained segmentation network and cannot be directly applied to several benchmarks, e.g. CIFAR-
10 or MNIST. In contrast, RPGAN does not require any auxiliary models or supervision and can be
applied to any data.
1https://github.com/rpgan-ICLR2020/RPGAN
2
Under review as a conference paper at ICLR 2020
several blocks.
Figure 1: The RPGAN generator. During forward pass, only one block in each bucket is activated.
3	Random Path GAN
3.1	Motivation
Before the formal description, we provide an intuition behind the RPGAN model. Several prior
works have demonstrated that in discriminative convolutional neural networks different layers are
“responsible” for different levels of abstraction (Zeiler & Fergus, 2014; Babenko et al., 2014). For
instance, earlier layers aim to detect small texture patterns, while activations in deeper layers typi-
cally correspond to semantically meaningful concepts. Similarly, in our paper we aim to understand
the roles that different GAN layers play in image generation. Thus, we propose an architecture that
provides a direct way to interpret the impact of individual layers. For a given generator architecture,
we construct several copies of each layer in its architecture. During the forward pass we randomly
choose a layer instance that will be used when generating a particular image. Therefore, we can
analyze the role of each RPGAN layer by visualizing how different instances of that layer affect the
generated image.
3.2	Model
Here we formally describe the structure of the RPGAN model. The model is highly flexible to
the choice of generator and discriminator architectures as well as to the loss function and learning
strategy. Similarly to the standard GAN architectures, our model consists of two networks - a
generator and a discriminator. The RPGAN discriminator operates exactly like discriminators in
common GANs, hence below we focus on the generator description.
Unlike existing GANs, the RPGAN generator always receives a fixed input vector Z during for-
ward pass and aims to produce an image from the real image distribution. The generator con-
sists of several consequent buckets B1 , . . . , Bn. Each bucket is a union of independent blocks:
Bi={Bi1, . . . , Bimi}, where each block is an arbitrary computational unit and mi=|Bi|. A typical
example of a block is a ResNet block (He et al., 2015), a convolutional layer with a nonlinearity or
any other (see Figure 1a) layer type. In our experiments, all the units from the same bucket have the
same architecture.
For each i=1, . . . , n - 1 a block from the bucket Bi produces an intermediate output tensor that
is passed to a block from the next bucket Bi+1. Typically we associate each bucket with a layer
(or several layers) in the generator architecture, which we aim to interpret or analyze. A block
from the first bucket B1 always receives a fixed input vector Z, which is the same for different
forward passes. The stochasticity of the generator arises from a random path that goes from Z to an
output image, using only a single block from each bucket. Formally, during each forward pass, we
randomly choose indices s1, . . . , sn with 1 ≤ si ≤ mi. The generator output is then computed as
Bnsn ◦••• B2s2 0 Bisi (Z) (see Figure 1b). Thus, the generator defines a map from the Cartesian
producthmii X(m^)×∙∙∙× {mn to the image space. Note that we can take an arbitrary existing
GAN model, group its generator layers into buckets and replicate them into multiple blocks. In these
terms, the original model can be treated as the RPGAN model with a single block in each bucket and
3
Under review as a conference paper at ICLR 2020
random input noise. Note that during image generation we perform the same number of operations
as in the standard GAN generator.
By its design, RPGAN with buckets B1 , . . . , Bn and a constant input Z is able to generate at most
∣Bι∣×∙∙∙× |Bn| different samples were |Bk| is the number ofblocks in the bucket Bk. Nevertheless,
this number is typically much larger compared to the training set size. We argue that the probability
space of random paths can serve as a latent space to generate high-quality images, as confirmed by
the experiments below.
Block diversity loss. To guarantee that blocks in a particular bucket are different, we also add
a specific diversity term in the generator loss function. The motivation for this term is to prevent
blocks Bki, Bkj from learning the same weights. Let W be the set of all parameters of the generator.
For each parameter w ∈ W there is a set of its instances {w(1) , . . . w(mw) } in the RPGAN model.
Then we enforce the instances to be different by the loss term
-	MSE
w∈W,i6=j
Here we also normalize by the standard deviation sw of all parameters from different blocks that
correspond to the same layer. This normalization effectively guarantees that all buckets contribute
to the diversity term.
4	Experiments
Architecture. In all the experiments in this section, we use ResNet-like generators with spectral
normalization and the hinge loss (SN-ResNet) described in Miyato et al. (2018). The blocks in the
first bucket are fully-connected layers, the blocks in the last bucket are convolutional layers and
blocks in all other buckets are residual blocks with two convolutions and a skip connection. If not
stated otherwise, all the buckets have the same number of blocks. Additional experiments with other
architectures are provided in Appendix.
Datasets. We performed experiments on CIFAR-10 (Krizhevsky et al., 2009), LSUN-bedroom (Yu
et al., 2015) and Anime Faces (Jin et al., 2017) datasets. For different datasets we use different
numbers of discriminator steps per one generator step dsteps and different numbers of blocks in a
bucket nblocks. We summarize the main parameters used for three datasets in Table 1. In the last
column we also report Coverage, which is the ratio of the latent space cardinality (which equals the
number of buckets to the power nblocks) to the dataset size. Intuitively, large coverage guarantees
that RPGAN has a sufficiently rich latent space of generator routes to capture the reference dataset.
In the experiments below, we demonstrate that even moderate coverage is sufficient to generate
high-fidelity images (see the LSUN-bedroom dataset with coverage ≈ 3.3).
Dataset	Image size	Number of buckets	nblocks	dsteps	Batch size	Coverage
-CIFAR-10-	32 × 32	5	-40-	-5-	64	-2048-
Anime Faces	64 × 64	6	20	1	32	≈ 2970
LSUN-bedroom	128 × 128		7		10	1	16	≈ 3.3
Table 1: The details of architectures and training protocols used for different datasets.
Training details. We use the Adam optimizer with learning rate equal to 0.25 × 10-3, β1, β2
equal to 0.5, 0.999 and train the model for 45 × 104 generator steps for CIFAR-10 and 25 × 104
generator steps for Anime Faces and LSUN-bedroom datasets. During training we also learn the
unique input vector Z. We observed that a learnable Z slightly improves the final generation quality
and stabilizes the learning process. During the training step, we pass Z through N independent
random paths. Formally, let {x1, . . . , xN} be a batch of samples received from a bucket Bk. To pass
this batch through the bucket Bk+1 we take random blocks Bki1 , . . . , BkiN and form a new batch
{Bki1 (x1), . . . , BkiN (xN)}. In all the experiments, we use the same training protocols for both
the RPGAN and the standard GAN of the generator architecture. Note, that despite larger number
of learnable parameters, RPGAN does not require more data or training time to achieve the same
quality, compared to standard GANs.
4
Under review as a conference paper at ICLR 2020
2.5
AllSjəAIP D-UE日əs
1	2	3	4	5
4.5
4
Red channel
Green
Blue
AllSjəAlP J-0u
1	2	3	4	5
varying bucket
varying bucket
(a) Averaged relative inception impact (b) Averaged relative coloring impact
Figure 4: Layer diversity compared to the first layer on CIFAR-10.
Bl generate four images s(1l), . . . , s(4l), varying blocks in Bl. In other words, rather then taking all
possible pairs, we take four random samples from each line of the table in the Figure 3. Then we
measure diversity w.r.t. dimg captured by bucket Bl as a ratio
iP6=jdimg(si(l),s(jl))
Pj dimg(s(Y))
Intuitively, we compute the relative diversity with respect to the first layer, which typically captures
the smallest amount of variations in our experiments. We then average these ratios over 100 inde-
pendent evaluations. High values of averaged ratio Dl→1,dimg imply higher diversity of the bucket
Bl compared to the first bucket in terms of the metric dimg . For dimg we experimented with the
following two metrics, capturing semantics and color differences correspondingly.
Inspired by the well-known Frechet Inception Score concept (HeUsel et al., 2017), We consider
the Euclidean distance between the outputs of the last layer of the pretrained InceptionV3 network
(Szegedy et al., 2016) for semantic distance evalUation. Namely, we define dsemantic(img1, img2) as
kIv3(img1) - Iv3(img2)k2 were Iv3(img) is the InceptionV3 model activations for image img.
To measUre differences in color, we take the Hellinger distance between the color histograms of gen-
erated samples. Namely, for each color channel, we split the range [0, . . . , 255] into 25 eqUal seg-
ments and evalUate the discrete distribUtion defined by the freqUencies the sample’s pixel intensities
appear in a given segment. Then the Hellinger distance between two quantified color distributions is
25
defined as √12 ∖ P (√pi - √qi)2 . We compute this metric for each color channel independently.
The average values of Dl→1,dimg with the standard deviations are shown on Figure 4. It demonstrates
that the semantic diversity is the largest for the intermediate layers. On the contrary, the last buckets,
which are closer to the output, do not influence semantics but have higher impact in terms of color.
Note that the first layer always shows the smallest variability in terms of both semantics and colors.
The last bucket seems to be responsible for color correction and color inversion and has a lower
pallet variability impact. Note, that the plots from Figure 4 reaffirm the findings coming from the
Figure 3. Note that the similar empirical results also hold for other datasets (see figures 13, 16 in
appendix).
Overall, we summarize the main findings common for CIFAR-10, LSUN and Anime Faces datasets
as:
•	The earlier layers have a smaller variability and seem to be responsible for the viewpoint
and the position of the object on the image.
6
Under review as a conference paper at ICLR 2020
•	The semantic details of the image content are mostly determined by the intermediate layers.
•	The last layers typically affect only coloring scheme and do not affect content semantics or
image geometry.
Note, that these conclusions can differ for other datasets or other generator architectures. For in-
stance, for the four-bucket generator and MNIST (Figure 6, left) or randomly colored MNIST ( Fig-
ure 8, left) the semantics are mostly determined by the first two buckets.
4.2	RPGAN vs standard GAN
In this subsection, we argue that the interpretations of different layers, obtained with RPGAN, are
also valid for the standard GAN generator of the same architecture. First, we demonstrate that
both standard GAN and RPGAN trained under the same training protocol provide almost the same
generation quality. As a standard evaluation measure, We use the Frechet Inception Distance (FID)
introduced in Heusel et al. (2017). We also compute precision-recall curve as defined in Sajjadi et al.
(2018). For evaluation on CIFAR-10, We use 50000 generated samples and the Whole train dataset.
We also take ten independently trained generators and report minimal and average FID values. See
Table 2 for FID comparison and Figure 11 in appendix for precision-recall. RPGAN and SN-ResNet
perform With the same quality both in terms of FID and precision-recall curves.
model	min FID	average FID
Five-bucket RPGAN	-169-	20.8
SN-ReSNet	16.75	18.7
Table 2: FID values for CIFAR-10.
To confirm that the layers of the standard GAN generator can be interpreted in the same Way as the
corresponding layers of its RPGAN counterpart, We perform the folloWing experiment. We take a
standard SN-ResNet GAN, consisting of five layers associated With the correspondent buckets in
RPGAN, and train it on CIFAR-10. Then for each layer, We add normal noise to its Weights. In-
tuitively, We expect that the noise injection in the particular layer Would change generated samples
in terms of characteristics, influenced by this layer. For instance, noise in the last tWo layers is ex-
pected to harm coloring scheme, While noise in the intermediate layers is expected to bring maximal
semantic damage. Several samples, produced by perturbed layers, are presented on Figure 5. The
images support the intuition described above and confirm that RPGAN may serve as an analysis tool
for the corresponding generator model. Note, hoWever, that injecting noise per se is not sufficient
for interpretability. The perturbed generators produce poor images, Which are difficult to analyze.
MeanWhile, RPGAN alWays generates good-looking images, Which alloWs to identify the factors of
variation, corresponding to the particular layer. For instance, see Figure 8 for the colored MNIST
dataset. Figure 8 (left) shoWs plausible images, generated by varying RPGAN blocks. In contrast,
Figure 8 (right) demonstrates images from generators perturbed With small and large noise. For
both noise magnitudes, these images are difficult to interpret. Of course, given the interpretations
obtained via RPGAN, one can perceive similar patterns in the noisy generations, but noise injection
alone is not sufficient for interpretability.
4.3	Incremental learning with RPGAN
In the next experiment, We demonstrate that the RPGAN model is also a natural fit for the generative
incremental learning task (see e.g., Wu et al. (2018)). Let us assume that the Whole train dataset D
is split into tWo disjoint subsets D = D1 ∪ D2 . Suppose that initially We have no samples from
D2 and train a generative model to approximate a distribution defined by the subset D1. Then,
given additional samples from D2, We aim to solve an incremental learning task — to update the
model With neW data Without re-training it from scratch. The RPGAN model alloWs solving this task
naturally. First We train a generator With buckets B1, . . . , Bn to reproduce the subset D1. Once We
Want to extend the generator With samples from D2, We add several neW blocks to the buckets that are
responsible for the features that capture the difference betWeen D1 and D2 . Then We optimize the
generator to reproduce both D1 and D2 by training only the neW blocks. Thus, instead of training a
neW generator from scratch, We exploit the pretrained blocks that are responsible for features, Which
7
q2 sʃ
4 J3q
。彳，千
。3 7Γ
O 6 J Ml
□〃5 /
/ ¥
ʃ Xo H J
/765
金S5 5
力5 6 5
0 5 6 5
Under review as a conference paper at ICLR 2020
i, j, k are random indices of blocks from the buckets B3 , B4 , B5 of the original generator. Thus
instead of performing three multiplications of features vector from the second layer by matrices of
the shapes 256 × 512, 512 × 1024, 1024 × 784 we perform a single multiplication by a 256 × 784
matrix. In our experiments, we achieved ×2.2 speed up. Note, however, that after the compression,
the latent space cardinality can decrease if a small subset of tuples (i, j, k) is used to populate the
new bucket. Nevertheless, as random products of joining buckets are used, we expect that the gener-
ated images would be uniformly distributed in the space of images, produced by the uncompressed
generator (see Figure 24 for samples comparison).
5 Conclusion
In this paper, we address the interpretability of generative models. In particular, we have introduced
RPGAN, an alternative design of generative adversarial networks, which allows natural interpreta-
tion of different generator layers via using random routing as a source of stochasticity. With experi-
ments on several datasets, we provide evidence that different layers are responsible for the different
factors of variation in generated images, which is consistent with findings from previous work. As a
possible direction of future research, one can use the RPGAN analysis to construct efficient models,
e.g., via identification of redundant parts of the generator for pruning or inference speedup.
References
Artem Babenko, Anton Slesarev, Alexandr Chigorin, and Victor Lempitsky. Neural codes for image
retrieval. In European conference on computer vision, pp. 584-599. Springer, 2014.
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick KIaUschen, KIaUs-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140, 2015.
David BaU, JUn-Yan ZhU, Hendrik Strobelt, ZhoU Bolei, JoshUa B. TenenbaUm, William T. Freeman,
and Antonio Torralba. Gan dissection: VisUalizing and Understanding generative adversarial net-
works. In Proceedings of the International Conference on Learning Representations (ICLR),
2019.
Andrew Brock, Jeff DonahUe, and Karen Simonyan. Large scale GAN training for high fidelity
natUral image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Tatjana Chavdarova and Francois Fleuret. Sgan: An alternative training of generative adversarial
networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), JUne
2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based
on deep networks. In Advances in neural information processing systems, pp. 658-666, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, NIPS’17, pp. 5769-5779, USA, 2017. Curran Asso-
ciates Inc. ISBN 978-1-5108-6096-4. URL http://dl.acm.org/citation.cfm?id=
3295222.3295327.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-
778, 2015.
9
Under review as a conference paper at ICLR 2020
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems,pp. 6626-6637, 2017.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. MGAN: Training generative adversarial
nets with multiple generators. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=rkmu5b0a-.
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative
adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Y Jin, J Zhang, M Li, Y Tian, and H Zhu. Towards the high-quality anime characters generation
with generative adversarial networks. In Proceedings of the Machine Learning for Creativity and
Design Workshop at NIPS, 2017.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4401-4410, 2019.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. Technical report,
Citeseer, 2009.
Karol Kurach, Mario LuCiC, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study
on regularization and normalization in gans. In International Conference on Machine Learning,
pp. 3581-3590, 2019.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1989.
Christian Ledig, Lucas Theis, FerenC Huszar, Jose Caballero, Andrew Cunningham, AIejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 4681-4690, 2017.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188-5196, 2015.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=B1QRgziT-.
Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing
Generative Models via Precision and Recall. In Advances in Neural Information Processing
Systems (NeurIPS), 2018.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Vi-
sualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319-
3328. JMLR. org, 2017.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. arXiv preprint arXiv:1808.06601, 2018.
10
Under review as a conference paper at ICLR 2020
Figure 7: The FID values for RPGAN with different number of blocks.
Chenshen Wu, Luis Herranz, Xialei Liu, Joost van de Weijer, Bogdan Raducanu, et al. Memory
replay gans: Learning to generate new categories without forgetting. In Advances In Neural
Information Processing Systems,pp. 5962-5972, 2018.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of
a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223-2232, 2017.
A	Additional figures and experiments
A. 1 Ablation on number of blocks
Here we investigate the impact of the number of blocks in each RPGAN bucket on the generation
quality. We train RPGAN with the SN-ResNet generator on CIFAR-10 with a different numbers of
blocks in each bucket. For simplicity, all buckets have the same number of blocks. The resulting
FID values are presented on Figure 7
If the number of blocks is too low, the resulting latent space appears to have insufficient cardinality
to cover the dataset. On the other hand, a too high number of blocks results in a difficult training
procedure and also fails.
A.2 LSUN bedroom dataset
To generate LSUN-bedroom-like images, we use the 7-bucket RPGAN with ResNet-like generator
with five residual blocks, the first fully-connected layer, and the last convolutional layer. Similarly
to CIFAR-10 experiments, during the generation, we freeze a random path and vary blocks in a
single bucket to investigate its responsibility. See Figure 12 for blocks variations and Figure 13 for
buckets responsibility analysis. Note that similarly to CIFAR-10, the central buckets have a maximal
semantic impact. Last two buckets are mostly responsible for coloring. The first two buckets are
responsible for local geometrical features. Note that here we face mode collapse for the third bucket:
mainly, it affects only tiny local features. See Figure 14 for samples generated by the model.
11
Under review as a conference paper at ICLR 2020
Figure 8: Left: images produced by varying blocks in a particular bucket of RPGAN. Right: images
produced by the standard GAN after parameters perturbation in a particular generator layer, with
low and high normal noise variance.
A.3 Anime faces dataset
Though this dataset is not standard for GANs, we use it in the experiments as it nicely reveals the
RPGAN analysis tools. Here we use the 6-bucket RPGAN with ResNet-like generator with four
residual blocks, the first fully-connected layer, and the last convolutional layer. See Figure 15 for
block variations and Figure 16 for bucket responsibility analysis. Again, the content semantics
is mostly defined by the intermediate buckets. The last two buckets are mostly responsible for
coloring: the fifth bucket has the maximal impact on coloring, and the last bucket varies tones. The
first buckets are responsible for small details (one can note the hair on the character’s forehead). See
Figure 17 for samples generated by the model.
A.4 Wasserstein GAN
Here we show that the concept of RPGAN works well with different generator architectures and
learning strategies. Here we present plots for DCGAN-like generators consisted of consequent
convolutional layers without skip connections. All the models were trained with the same parameters
as described in Section 4. Despite of Spectral Normalization, we train these models as WGANs with
weight penalty (Gulrajani et al., 2017). On the Figure 19 we show plots for a four-bucket generator
trained on CIFAR-10. We also train four-bucket generator on colored MNIST, see Figure 8, left.
Finaly, we show plots for the five-bucket generator and CelebA-64x64 dataset on Figure 21. See
Figure 18, Figure 21, Figure 22 for buckets analysis.
B RPGAN interpretability vs weights noising
In this section we show that injecting noise in the generator weights cannot be used as a stand-alone
interpretability method. Namely, we compare images produces by RPGAN and noise injection
for models trained on randomly colored MNIST samples. We train both RPGAN and standard
generators as a Wasserstein GAN with weights penalty.
12
Under review as a conference paper at ICLR 2020
Q-8-6-42
Ioooo
0.0 ɪ ɪ ɪ
0.00	0.25	0.50	0.75
Recall
1.00
Figure 11: Comparison of precision-recall of RPGAN and its backbone SN-ResNet trained on CI-
FAR10.
Figure 12: Frozen paths individual blocks variation in 7-bucket RPGAN.
14
Under review as a conference paper at ICLR 2020
AJISjəAIP-JUEInəs
AJISjəaip JSOD
6 5 4 3 2 1 0
2	4	6
varying bucket
2	4	6
varying bucket
(a) Averaged relative inception impact (b) Averaged relative coloring impact
Figure 13: LSUN bedroom buckets specification. See Section 4.1 for details.
Figure 14: 128 × 128 samples generated by 7-bucket RPGAN
15
Under review as a conference paper at ICLR 2020
Figure 15: Frozen paths individual blocks variation in 6-bucket RPGAN.
AllSjəAlP DlIUE日əs
2	4	6
varying bucket
Red channel
Green
AllSjəAlP J-0u
2	4	6
varying bucket
(a) Averaged relative inception impact (b) Averaged relative coloring impact
Figure 16: Anime Faces buckets specification. See Section 4.1 for details.
16
Under review as a conference paper at ICLR 2020
Figure 17: 64 × 64 samples generated by 6-bucket RPGAN.
12	3	4
varying bucket
32119876
LLL0000
asjəAIPJUEInəs
4
2	3
varying bucket
1
(a) Averaged relative inception impact (b) Averaged relative coloring impact
Figure 18: Buckets specification of RPGAN with DCGAN backbone trained on colored MNIST.
See Section 4.1 for details.
bucket 4
Figure 19: Frozen paths individual blocks variation in 4-bucket RPGAN with a DCGAN backbone.
Generated images size is 32 × 32.
17
AllSjəAlP J-oD
1
2
3
4
varying bucket
1
2
3
4
varying bucket
Under review as a conference paper at ICLR 2020
Red channel
Green
Blue
4.5
4
1	2	3	4	5
varying bucket
1	2	3	4	5
varying bucket
(a)	Averaged relative inception impact
(b)	Averaged relative coloring impact
Figure 22: Buckets specification of RPGAN with DCGAN backbone trained on CelebA dataset. See
Section 4.1 for details.
Figure 23: Samples from a 5-bucket ResNet-like generator without nonlinearities.
rtrf ⅞ / of ?
*7S 0 / 夕
/5a,⅛3
5。7√7
δ/ Moa
G⅛l∙7o
oʃg CK-J-
夕37z O
X3 CXJ..&A
-54 8彳r
Figure 24: Digits generated by RPGAN without nonlinearities (left) and by its ×2.2 faster compres-
sion (right).
19
Under review as a conference paper at ICLR 2020
Figure 25: SN-ResNet generator with different layers noising. Left: low normal noise variance.
Right: high normal noise variance
Original	Compressed
Z ∈ R128
fc, 32blocks, 128
fc, 32 blocks, 256
fc, 32blocks,512
fc, 16blocks, 1024 fc, 128 blocks,784
fc,16blocks, 784
Tanh, reshape to 28 × 28
Table 3: Fully connected RPGAN without nonlinearities and its compressed modification.
20