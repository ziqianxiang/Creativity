Under review as a conference paper at ICLR 2020
Wasserstein-Bounded Generative Adversar-
ial Networks
Anonymous authors
Paper under double-blind review
Ab stract
In the field of Generative Adversarial Networks (GANs), how to design a stable
training strategy remains an open problem. Wasserstein GANs have largely pro-
moted the stability over the original GANs by introducing Wasserstein distance,
but still remain unstable and are prone to a variety of failure modes. In this paper,
we present a general framework named Wasserstein-Bounded GAN (WBGAN),
which improves a large family of WGAN-based approaches by simply adding
an upper-bound constraint to the Wasserstein term. Furthermore, we show that
WBGAN can reasonably measure the difference of distributions which almost
have no intersection. Experiments demonstrate that WBGAN can stabilize as well
as accelerate convergence in the training processes of a series of WGAN-based
variants.
1	Introduction
Over the past few years, Generative Adversarial Networks (GANs) have shown impressive results
in many generative tasks. They are inspired by the game theory, that two models compete with
each other: a generator which seeks to produce samples from the same distribution as the data,
and a discriminator whose job is to distinguish between real and generated data. Both models are
forced stronger simultaneously during the training process. GANs are capable of producing plausible
synthetic data across a wide diversity of data modalities, including natural images (Karras et al., 2017;
Brock et al., 2018; Lucic et al., 2019), natural language (Press et al., 2017; Lin et al., 2017; Rajeswar
et al., 2017), music (Yang et al., 2017; Mogren, 2016; Dong et al., 2017; Dong & Yang, 2018), etc.
Despite their success, it is often difficult to train a GAN model in a fast and stable way, and
researchers are facing issues like vanishing gradients, training instability, mode collapse, etc. This
has led to a proliferation of works that focus on improving the quality of GANs by stabilizing the
training procedure (Radford et al., 2015; Salimans et al., 2016; Zhao et al., 2016; Nowozin et al.,
2016; Chen et al., 2016; Qi, 2017; Deshpande et al., 2018). In particular, Arjovsky et al. (2017)
introduced a variant of GANs based on the Wasserstein distance, and releases the problem of gradient
disappearance to some extent. However, WGANs limit the weight within a range to enforce the
continuity of Lipschitz, which can easily cause over-simplified critic functions (Gulrajani et al., 2017).
To solve this issue, Gulrajani et al. (2017) proposed a gradient penalty method termed WGAN-GP,
which replaces the weight clipping in WGANs with a gradient penalty term. As such, WGAN-GP
provides a more stable training procedure and succeeds in a variety of generating tasks. Based
on WGAN-GP, more works (Wei et al., 2018; Petzka et al., 2017; Wu et al., 2018; Mescheder
et al., 2018; Thanh-Tung et al., 2019; Kodali et al., 2017; Kim et al., 2018) adopt different forms
of gradient penalty terms to further improve training stability. However, it is often observed that
such gradient penalty strategy sometimes generate samples with unsatisfying quality, or even do not
always converge to the equilibrium point (Mescheder et al., 2018).
In this paper, we propose a general framework named Wasserstein-Bounded GAN (WBGAN), which
improve the stability of WGAN training by bounding the Wasserstein term. The highlight is that the
instability of WGANs also resides in the dramatic changes of the estimated Wasserstein distance
during the initial iterations. Many previous works just focused on improving the gradient penalty term
for stable training, while they ignored the bottleneck of the Wasserstein term. The proposed training
strategy is able to adaptively enforce the Wasserstein term within a certain value, so as to balance the
Wasserstein loss and gradient penalty loss dynamically and make the training process more stable.
1
Under review as a conference paper at ICLR 2020
WBGANs are generalized, which can be instantiated using different kinds of bound estimations, and
incorporated into any variant of WGANs to improve the training stability and accelerate the conver-
gence. Specifically, with Sinkhorn distance (Cuturi, 2013; Genevay et al., 2017) for bound estimation,
we test three representative variants of WGANs (WGAN-GP (Gulrajani et al., 2017), WGAN-
div (Wu et al., 2018), and WGAN-GPReal (Mescheder et al., 2018)) on the CelebA dataset (Liu
et al., 2015). As shown in Fig. 1, WBGANs outperform the corresponding counterparts, which
demonstrates that the bounded strategy results in more stable training and accelerates the convergence.
2	Backgrounds
Wasserstein GANs (WGANs). WGANs (ArjoVsky
et al., 2017) were primarily motivated by unstable train-
ing caused by the gradient vanishing problem of the
original GANs (Goodfellow et al., 2014). They pro-
posed to use I-Wasserstein distance Wi (Pr, Pg) to
measure the difference between Pr and Pg, the real
and generated distributions, given that Wi (Pr, Pg)
is continuous everywhere and differentiable almost
everywhere under mild assumptions. The objec-
tive of WGAN is formulated using the Kantorovich-
Rubinstein duality (Villani, 2008):
min max Ex〜pr [D(x)] - Ex〜Pg [D(x)], (1)
G D∈L1	r	g
Figure 1: Instability in the training process
of three variants of WGANs, i.e., WGAN-
GP, WGAN-div and WGAN-GPReal.
where Li is the function space of all D satisfying the 1-Lipschitz constraint kDkL ≤ 1. D is
a critic and G is the generator, both of which are parameterized by a neural network. Under an
optimal critic, minimizing the objective with respect to G is to minimize Wi (Pr, Pg). To enforce
the 1-Lipschitz constraint on the critic, WGAN used a weight clipping on the critic to constrain the
weights within a compact range, [-c, c], which guarantees the set of critic functions is a subset of
the k-Lipschitz functions for some k. With weight clipping, the critic tends to learn over-simplified
functions (Gulrajani et al., 2017), which may lead to unsatisfying results. Gulrajani et al. (2017); Wei
et al. (2018); Petzka et al. (2017); Wu et al. (2018) proposed different forms of gradient penalty as a
regularization term, so that a generalized loss function with respect to the critic can be written as:
LD = -[Ex〜p[D(x)] - Ex〜Pg [D(X)]]+ λGP ∙ GP,	(2)
where Ex〜pr [D(x)] - Ex〜Pg [D(X)] stands for the Wasserstein term, and GP for the gradient penalty
term. LD is actually posing a tradeoff between these two objectives.
Wasserstein Distance between Empirical Distributions. In practice, we approximate Wi (Pr , Pg )
using Wi Pr , Pg , where Pr and Pg denote the empirical version of Pr and Pg with N samples,
i.e., Pr = Nn PN=ι δyi, and Pg = & PN=i Ag®). Here, yi is randomly sampled from the real
image dataset, and δy% is the Dirac delta function at location yi. Computing Wi (Pr, P J is a typical
problem named discrete optimal transport. We denote B as the set of probabilistic couplings between
two empirical distributions defined as:
B ：= {Γ ∈ RN×n | Γ1n = Pg,Γ>1n = Pr},	(3)
where 1N is a N -dimensional all-one vector. Then we have Wi(Pr, Pg) = minγ∈BhΓ, CiF, where
h∙, ∙iF is the Frobenius dot-product and C is the cost matrix, with each element Ci,j = c(G(zi), y,)
denoting the cost to move a probability mass from G(zi ) to yj. The optimal coupling is the solution
of this minimization problem: Γ0 = arg minΓ∈B hΓ, CiF.
The Sinkhorn Algorithm. Despite Wasserstein distance has appealing theoretical properties in
measuring the difference between distributions, its computational costs for linear programming are
often high in particular when the problem size becomes large. To alleviate this burden, Sinkhorn
distance (Cuturi, 2013) was proposed to approximate Wasserstein distance:
_ ^ ^ . . _
dα(Pr, Pg )：=	min	hP,C i,	(4)
P∈Uα (Pr ,Pg )
2
Under review as a conference paper at ICLR 2020
一 _ , , ʌ ʌ .. 一 一一一 一 一.
where Ua(Pr, Pg) is a subset of B defined m Eq. 3:
ʌ ʌ . - . . ʌ , ʌ -
Ua(Pr, Pg )：= {Γ ∈ B∣H(Γ) > H(Pr)+ H(Pg) - α} ⊂ B,	(5)
where H(∙) is the entropy defined as H(Γ) = - PNj=I Γi,jlogΓi,j and H(Pr) = - PN=I Pn log Pn
where Pn is the probability of the n-th sample. Compared to Wasserstein distance, Sinkhorn distance
restricts the search space of joint probabilities to those with sufficient smoothness. To compute
Sinkhorn distance, a Lagrange multiplier was used:
dλ(Pr, Pg) = (Γλ, C ,	Γλ = argmin{Γ,Ci - 1 H(Γ).	(6)
Γ∈B	λ
Each α corresponds a λ ∈ [0, ∞) such that da(Pr, Pg) = dλ (Pr, Pg) holds for that pair (Pr, Pg ).
dλ(∙, ∙) can be computed with a much cheaper cost than the original Wasserstein distance using matrix
scaling algorithms. For λ > 0, the solution Γλ is unique and has the form Γλ = diag(u)K diag(v),
where K is the element-wise exponential of -λC. u and v are two non-negative vectors uniquely
defined up to a multiplicative factor (Cuturi, 2013).
3	Wasserstein-Bounded GANs
3.1	B ound Constraint on 1 -Wasserstein Distance
We start with Eq. 2 and denote W = Ex〜pr [D(x)] - Ex〜Pg [D(X)]. W is often referred to as the
Wasserstein term, which is unbounded during the training process. In a wide range of WGAN’s
variants such as WGAN-GP (Gulrajani et al., 2017), the critic defined by LD is to maximize the
Wasserstein term W while satisfying the gradient penalty GP. However, in practice, we find that
W often rises rapidly to a tremendous value which is far from rational during the initial training
procedure. A possible reason may lie in that the critic function does not satisfy the Lipschitz constraint
during the initial training stage. As shown in Fig. 2, this leads to dramatic instability in optimization
and finally results in unsatisfying performance in image generation.
Our idea is thus straightforward, i.e., setting an upper-bound for W. The modified critic loss function
is written as:
LD = -W + [W -m]+ + λGP ∙ GP,	(7)
where [•]+ = max{∙, 0} is the ramp function thatignores negative inputs, and W denotes the upper
bound of W, and will be discussed later. If W 6 W, theɪerm [W - W]+ simply vanishes and Eq. 7
is equivalent to Eq. 2; otherwise, we have -W + [W - w]+ = -W, which implies that the modified
Wasserstein term is bounded by WW.
Our formulation brings a benefit to the numerical stability of the Wasserstein term. In practice, it
remains comparable to the other term, Xgp ∙ GP, so that both W and GP can be optimized in a 'mild’
manner, i.e., without any one of them dominating or being ignored during training. Note that the WW
term cannot be chosen arbitrarily. Setting it too small, WW will limit the capacity of the critic function,
resulting in a poor generation. Setting it too large, there will be no effect of bounding the W term.
The proposed bounded strategy is a general framework. We name it general in two folds: First,
WBGAN can be applied to almost all gradient penalty based WGANs, such as WGAN-GP (Gulrajani
et al., 2017), WGAN-GPReal (Mescheder et al., 2018), etc. Moreover, there are different ways to
estimate the value of WW . For example, the linear programming was applied successfully to some
existing WGANs like WGAN-TS (Liu et al., 2018). In what follows, we present an example which
uses Sinkhorn distance to estimate WW, while we believe other ways of estimation are also possible.
3.2	WBGAN with Sinkhorn Distance
In this section, we give an instantiation, Sikhorn distance (Cuturi, 2013), to effectively compute the
bounded term WW. The motivation of using Sinkhorn distance lies in that in theory, the Wasserstein
term of WGAN will eventually converge to the 1-Wasserstein distance between the real distribution
Pr and the generated distribution Pg (Arjovsky et al., 2017; Gulrajani et al., 2017). Therefore, we can
use the 1-Wasserstein distance between the empirical distributions, Pr and Pg , as the upper-bound
WW . Since the computation of Wasserstein distance involves a large linear programming which
3
Under review as a conference paper at ICLR 2020
Algorithm 1 WBGAN with Sinkhorn distance
Require: learning rate a, batch size M, the number of iterations of the critic per generator iteration
Ncritic, weight of gradient penalty λGP, weight of Sinkhorn distance λs, initial parameters θ and
φ0, other hyper-parameters;
1:	while φt has not converged do
2:	for n = 1, . . . , Ncritic do
3:	Sample a batch {x(m)}M=ι 〜Pr from real data;
4:	Sample a batch {z(m')}M=ι 〜 Pz of prior samples;
5:	W 一 M PM=1 Dθ(x(m)) - MZM=1 Dθ(Gφt (z(m)));
6:	Calculate Sinkhorn distance dλ(Pr, Pg) between {x(m)}M=ι and {Gφt (z(m))}M=ι;
7:	Lθ J-W +[W - dλ(Pr, Pg)]+ + λGP ∙ GP;
8:	θ — Adam(Lθ ,θ,α,β1,β2);
9:	end for
10:	Sample a batch {z(m)}mM=1 〜Pz of prior samples;
11:	Calculate Sinkhorn distance dλ(Pr, Pg) between {x(m)}M=ι and {Gφt(z(m))}M=ι;
_	_	-	-A ,ʌ	ʌ .
12:	Lφt <------Ez 〜Pz [Dθ (G @t(Z))] + λs ∙ d (Pr , Pg );
13:	φt+1 J Adam(Lφt, φt, α, β1, β2);
14:	end while
Ensure: trained parameters θ and φT (converged).
suffers heavy computational costs, we replace it by Sinkhorn distance instead - the Sinkhorn distance
between Pr and Pg can be computed using Sinkhorn’s matrix scaling algorithm (Cuturi, 2013), which
is orders of magnitude faster than the linear programming solvers.
Mathematically, consider a generator function Gφ(z) that produces samples by transforming noise
input z drawn from a simple distribution Pz, e.g., Gaussian distribution. Dθ stands for a critic
function parameterized by θ. The objective of the critic is:
Lθ(Pr, Pg)
DmaxI	/出[Dθ(X)] - Ex~Pg [Dθ(x)]
卯r Dθ(X)] - Ex〜Pg [Dθ(X)] - d'(Pr, Pg)]十
(8)
—
where dλ(Pr, Pg) is the Sinkhorn distance defined in Eq. 6. On the other hand, given a fixed critic
function Dθ? , considering that Sinkhorn distance allows gradient back-propagation (Genevay et al.,
2017), we can find the optimal generator Gφ? by solving:
.,	一	一 一	,,	. . τ	-	,ʌ , ʌ	ʌ .
φ = arg min —Ez〜pz [Dθ? (Gφ(z))] + λs ∙ dλ(Pr, Pg),
φz
(9)
where λs is a balancing hyper-parameter, which we set λs = 0.5 in this paper. In Algorithm 1, we
summarize the flowchart of training WBGAN with Sinkhorn distance.
_ . _ ,ʌ , ʌ ʌ ` -----------------------------、
3.2.1 RELATIONSHIP BETWEEN dλ(Pr, Pg) AND Wi(Pr, Pg)
We employ dλ(Pr, Pg) as an approximation of 1-Wasserstein distance W1(Pr, Pg). Let (X, d) be a
separable metric space. P(X) denotes the set of Borel probability measures. Pp(X) denotes the
set of all μ ∈ P(X) such that JX d(x, y)pdμ(x) < +∞ for some y ∈ X. We can suppose real
data distribution Pr , generated data distribution Pg and their empirical distribution Pr and Pg all in
Pp(X).
Proposition 1. Let Pr and Pg be real data distribution and generated data distribution. Suppose that
ʌ 一 q	. .	__ _______ _	_	_	____	_ .	_ __, ʌ ʌ .,
Pr and Pg are empirical measures of Pr and Pg. Then we have 0 6 Wi(Pr, Pg) 6 E[Wι(Pr, Pg)].
Proof. Please refer to Appendix A.
Proposition 1 tells us that as E[Wι(Pr, Pg)] → 0, Wi(Pr, Pg) is forced to 0. Cuturi (2013) has
pointed out that if λ is chosen large enough, dλ(Pr, Pg) coincides with Wi(Pr, Pg). So, it is
reasonable to use dλ(Pr, Pg) to constrain the Wasserstein term.
4
Under review as a conference paper at ICLR 2020
3.3 ANALYSIS OF WBGAN
Most GANs measure the distance between distributions based on probability divergence. We will
prove that the Eq. 8 is indeed a valid divergence. First, we have the following definition.
Definition 1. Given probability measures p and q, D is a functional of p and q. If D satisfies the
following properties:
1.	D(p, q) ≥0;
2.	P = q ^⇒ D(p, q) = 0,
(10)
then we say D is a probability divergence between p and q.
Remark 1. The following W (Pr, Pg) satisfies the Definition 1 and is therefore a probability diver-
gence.
W(Pr, Pg) = max Ex〜pr[D(x)] - Ex〜Pg [D(x)],	(11)
D∈L1	r	g
where L1 is the 1-Lipschitz constraint. Please see the proof and detailed discussion in Su (2018).
This is the objective of critic used by WGAN (Arjovsky et al., 2017).
Remark 2. Equation 8 satisfies the Definition 1 and is a probability divergence.
Proof. The proof is given in Appendix B.
Remark 3. Consider two distributions Pr (x) = δ(x - α), Pg (x) = δ(x - β) that have no
intersection (α 6= β). δ is the Dirac delta function. In such an extreme case, Eq. 8 can still be
optimized by gradient descent.
Proof. The proof is in Appendix C
Remark 2 tells us that Eq. 8 is a valid divergence. Since the real data distribution is supported by low-
dimensional manifolds, the supports of generated distribution and real data distribution are unlikely to
have a non-negligible intersection. Remark 3 shows that compared to the standard GAN (Goodfellow
et al., 2014), WBGAN can continuously measure the difference between two distributions, even if
there is almost no intersection between the distributions.
4	Experiments
4.1	Settings and Baselines
To verify that WBGAN is a generalized approach, we select three variants of WGAN, namely,
WGAN-GP (Gulrajani et al., 2017), WGAN-div (Wu et al., 2018) and WGAN-GPReal (gradient
penalty on real data only) (Mescheder et al., 2018) as our baselines. By adding bound constraints
to these WGAN variants, we obtain the counterparts WBGAN-GP, WBGAN-div, and WBGAN-
GPReal, respectively. Two different network architectures are used, i.e., DCGAN (Radford et al.,
2015) and BigGAN (Brock et al., 2018). For DCGAN, we directly output the activation before the
sigmoid layer. BigGAN is a conditional GAN (Mirza & Osindero, 2014) architecture, in which class
conditioning is passed to generator by supplying it with class-conditional gains and biases in the
batch normalization layer (Ioffe & Szegedy, 2015; de Vries et al., 2017; Dumoulin et al., 2017). In
addition, the discriminator is conditioned (Miyato & Koyama, 2018) by using the cosine similarity
between its features and a set of learned class embedding. We use the spectral norm (Miyato et al.,
2018) in BigGAN, but for the sake of simplicity, we do not use the self-attention module (Wang et al.,
2017; Zhang et al., 2018). Other hyper-parameters and the network architecture of BigGAN simply
follow the original paper.
We choose the Frechet Inception Distance (FID) (HeUsel et al., 2017) for quantitative evaluation,
which has been proven to be more consistent with individual assessment in evaluating the fidelity and
variation of the generated image samples.
4.2	Mid-Resolution Experiments
We first investigate mid-resolution image generation on the CelebA dataset (Liu et al., 2015), a
large-scale face image dataset with more than 200K face images. During training, we crop 108 × 108
face from the original images and then resize them to 64 × 64.
5
Under review as a conference paper at ICLR 2020
100
90
80
70
60
g 50
40
30
20
10
0 20 40 60 80 100 120 140 160 180 200
Epoch
(a)
Q
lL
200
180
160
140
120
100
80
60
40
20
0
20 40 60 80 100 120 140 160 180 200
Epoch
(b)
Q
lL
200
180
160
140
120
100
80
60
40
20
0
20 40 60 80 100 120 140 160 180 200
Epoch
(C)
Figure 2: FID curves on the CelebA dataset, with WGAN-GP, WGAN-div and WGAN-GPReal as
baselines, respectively. Each figure contains 5 individual runs for both each counterpart.
Table 1: FID comparison between WGAN-based methods and WBGAN-based methods. The
BigGAN architecture uses spectral normalization in the generator and discriminator, and the number
of conditional labels is set to be 1 because the training dataset only contains face images.
NetWork architecture	Loss	Dataset	Resolution	Batch	G Param(M)	D Param(M)	FID
	WGAN-GP						6.76 ± 0.17
DCGAN	WBGAN-GP (ours) WGAN-div WBGAN-div (ours) WGAN-GPReal	CelebA	64 × 64	128	5.1	4.3	7.32 ± 0.55 13.94 ± 2.67 6.26 ± 0.30 34.92 ± 6.84
	WBGAN-GPReal (ours)						6.01 ± 0.33
	WGAN-GP						13.39
BigGAN	WBGAN-GP (ours) WGAN-div WBGAN-div (ours) WGAN-GPReal	CelebA	64 × 64	128	8.4	4.9	6.97 45.93 7.23 42.71
	WBGAN-GPReal (ours)						9.61
FID Stability. We first use DCGAN to build our generator and discriminator. Training curves are
shown in Fig. 2, and quantitative results are summarized in Table 1. Each approach is executed for
5 times and the average is reported. All FID curves are obtained from generators directly without
using the moving average strategy (Karras et al., 2017; Mescheder et al., 2018; Brock et al., 2018;
Yaziciet al., 2018) to avoid over-smoothing the FID curves, such that We can diagnose the underlying
oscillating properties of different methods during training. One can see that WBGAN-based counter-
parts improve the stability during training, and achieve superior performance over the WGAN-based
baselines. We emphasize that the converged FID values reported by WBGAN-div and WBGAN-
GPReal are loWer than those reported by WGAN-div and WGAN-GPReal. In particular, WGAN-div
suffers several FID fluctuation unexpectedly, and WGAN-GPReal has not ever achieved FID conver-
gence during the entire training process. Regarding WGAN-GP, although the final FID is slightly bet-
ter than that of WBGAN-GP (6.76 vs. 7.32), We observe a much sloWer convergence rate in Fig. 2(a).
For the generated face images by different approaches, please refer to Fig. 10 in Appendix F for details.
We also investigate a stronger backbone by replacing
the netWork With BigGAN, a conditional GAN archi-
tecture that uses spectral normalization on both gen-
erator and discriminator. We set the number of labels
to be 1 since the CelebA dataset only contains face
images. Training curves are shoWn in Fig. 3 and quanti-
tative results are summarized in Table 1. Among three
WGAN-based methods, only WGAN-GP achieves con-
vergence, but its convergence speed and the FID value
are inferior to those reported by WBGAN-GP. In op-
posite, both WGAN-div and WGAN-GPReal fails to
converge While the counterparts equipped With WB-
GAN perform Well. For the generated face images by
different approaches, please refer to Fig. 11 and Fig. 12
in Appendix F for details.
Figure 3: FID curves of BigGAN-based
approaches on the CelebA dataset.
6
Under review as a conference paper at ICLR 2020
----WGAN-GP
----WBGAN-GP
E」①1ΛΛ
O O
-10 -----------ι---------------ι------------1
0	5000	10000	15000
Iterations
WGAN-GP
-----WGAN-GP+D-bound
-----WGAN-GP+G-Sinkhorn
-----WGAN-GP+D-bound+G-Sinkhorn
∞80604020
—QLL.
Figure 4: Curves of the Wasserstain term, pro-
duced by WGAN-GP and WBGAN-GP
0 0 0 ^0^0
0 5 0 5 5 0
sso-
-150
0	2000	4000	6000
Iterations
(a)
8000	10000
-20
0
8060402°
Ss-°
500	1000	1500	2000
Iterations
(b)
0 20 40 60 80 100 120 140 160 180 200
Epoch
Figure 5: FID curves of DCGAN-based
ablation study on the CelebA dataset.
Ss-°
-10
0	500	1000	1500	2000
Iterations
(C)
Figure 6: Generator loss in the beginning iterations. BigGAN on CelebA. (a) WGAN-GP vs
WBGAN-GP, (b) WGAN-div vs WBGAN-div, (c) WGAN-GPReal vs WBGAN-GPReal.
Wasserstein Loss and Generator Loss Stability. Next, we evaluate the stability of WBGAN in
terms of the Wasserstein term and generator loss. In Fig. 4, we evaluate the impact on WGAN-GP
(DCGAN on CelebA). One can see that, after the bound is applied, the Wasserstein term W is
stablized especially during the start of training. Due to space limit, more results using BigGAN on
CelebA are provided in Appendix E. In addition, we compute a new term named the generator loss,
which is defined as Gloss = -Ez〜pz [Dθ (Gφ(z))]. Fig. 6 shows the curves of this statistics during
the starting iterations. Compared to WGAN-based approaches, WBGAN-based approaches produce
more stable Gloss terms, which verifies that the training process of GAN becomes more stable.
Ablation Study. Before continuing to high-resolution experiments, we conduct an ablation study
to investigate the contribution made by different components of WBGAN. The backbone network
is DCGAN, and the dataset is CelebA. We compare four configurations, i.e., WGAN-GP, with
the original loss term used in WGAN-GP; WGAN-GP+D-bound, which adds a bound (Sinkhorn
distance) to the Wasserstein term of the critic D of WGAN-GP; WGAN-GP+G-Sinkhorn, which
adds Sinkhorn distance to the loss function of the generator G in WGAN-GP; and WGAN-GP+D-
bound+G-Sinkhorn, which is equivalent to the final WBGAN-GP, with Sinkhorn distance added to
both critic D and generator G. Fig. 5 plots the FID curves of all four settings. One can see that,
although the FID curves of WGAN-GP and WGAN-GP+G-Sinkhorn descend quickly in the first
10 epochs, they begin to fluctuate between 20 to 40 epochs. On the other hand, when WGAN-GP is
combined with D-bound, FID is able to descend smoothly (without fluctuation), showing that it is
the bounded constraint that stablizes the training process. Finally, by integrating both D-bound and
G-Sinkhorn into WGAN-GP, the FID curve descends not only smoothly but also fast, which is what
we desire in real-world applications.
4.3 High-Resolution Experiments and Remarks
In this section, we evaluate our approach on higher-resolution (128 × 128) images. We use the
CelebA-HQ dataset (Karras et al., 2017), and use BigGAN (Brock et al., 2018) as the backbone. As
7
Under review as a conference paper at ICLR 2020
Table 2: FID comparison in high-dimensional experiments.
Network architecture Loss	Dataset Resolution Batch G Param(M) D Param(M) FID
BigGAN
WGAN-GP
WBGAN-GP (ours)
WGAN-div
WBGAN-div (ours)
WGAN-GPReal
WBGAN-GPReal (ours)
CelebA-HQ
128 × 128
64
8.4
9.6
17.58
18.32
21.05
17.26
21.33
12.87
(a)	(b)	(C)
Figure 7: Curves of FID using BigGAN network architecture on the CelebA-HQ dataset. (a) WGAN-
GP vs WBGAN-GP, (b) WGAN-div vs WBGAN-div, (c) WGAN-GPReal vs WBGAN-GPReal.
Lower is better.
the target become larger (128 × 128), the number of images we can feed into a single batch becomes
smaller (64). Since we are using an empirical way of estimating Sinkhorn distance, it becomes less
accurate in the scenario of small batch size and large image size. In other words, it is no longer the
best choice to use Sinkhorn distance to estimate the upper-bound W.
Returning to our generalized formulation, Eq. 7, we note that other forms of bound to constrain the
critic. Here we consider a very simple bound, which is also based on empirical study. Note that
the baseline methods, though not converging very well, can finally arrive at a stablized W value.
Heuristically, we use this constant value (there is no need to be accurate) as the bound, which is
10 for WGAN-GP, 5 for WGAN-div and 3 for WGAN-GPReal, respectively. In Appendix D, we
provide the curves of the Wasserstein term for these baselines, which lead to our estimation.
FID curves and quantitative results using these constant bounds are shown in Fig. 7 and Table 2,
respectively. We find that WBGAN-GP produces a similar convergence rate with WGAN-GP,
WBGAN-div is slightly better than WGAN-div, and WBGAN-GPReal outperforms WGAN-GPReal
and produces the best results. For the generated face images by different approaches, please refer to
Fig. 13 and Fig. 14 in Appendix F for details.
Discussions. From the above experiments, we can see that Sinkhorn distance is just one way of
upper-bound estimation. In case that it becomes less accurate, we can freely replace it with other
types of estimation. Besides the constant bound used above, there also exist other examples, such
as the two-step computation of the exact Wasserstein distance (Liu et al., 2018). However, it is
still a challenge to estimate the Wasserstein distance between high-resolution (1024 × 1024) image
distributions efficiently. Nevertheless, the most important deliveries of our work are that a bounded
Wasserstein term can bring benefits on training stability, and that we can use it to a wide range of
frameworks based on WGAN.
5 Conclusions
This paper introduced a general framework called WBGANs, which can be applied to a variety
of WGAN variants to stabilize the training process and improve the performance. We clarify that
WBGANs can stabilize the Wasserstein term at the beginning of the iterations, which is beneficial
for smoother convergence of WGAN-based methods. We present an instantiated bound estimation
method via Sinkhorn distance and give a theoretical analysis on it. It remains an open topic on how
to set a better bound for higher resolution image generation tasks.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein Generative Adversarial Networks.
In ICML,pp. 214-223, 2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv:1809.11096, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.
In NIPS, 2016.
Marco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In NIPS, pp.
2292-2300, 2013.
Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron Courville.
Modulating early visual processing by language. In NIPS, 2017.
Ishan Deshpande, Ziyu Zhang, and Alexander Schwing. Generative Modeling Using the Sliced
Wasserstein Distance. In CVPR, pp. 3483-3491. IEEE, 2018.
Hao-Wen Dong and Yi-Hsuan Yang. Convolutional Generative Adversarial Networks with Binary
Neurons for Polyphonic Music Generation. arXiv:1804.09399 [cs, eess, stat], April 2018.
Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. MuseGAN: Multi-track Se-
quential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment.
arXiv:1709.06298 [cs, eess], September 2017.
Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A Learned Representation For Artistic
Style. In ICLR, 2017.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning Generative Models with Sinkhorn
Divergences. arXiv:1706.00292 [stat], June 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), NIPS, pp. 2672-2680. Curran Associates,
Inc., 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In NIPS, pp. 5767-5777, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In NIPS,
2017.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift. arXiv:1502.03167 [cs], February 2015.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive Growing of GANs for
Improved Quality, Stability, and Variation. arXiv:1710.10196 [cs, stat], October 2017.
Cheolhyeong Kim, Seungtae Park, and Hyung Ju Hwang. Local Stability and Performance of Simple
Gradient Penalty mu-Wasserstein GAN. arXiv:1810.02528 [cs, stat], October 2018.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On Convergence and Stability of
GANs. arXiv:1705.07215 [cs], May 2017.
Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-ting Sun. Adversarial Rank-
ing for Language Generation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), NIPS, pp. 3155-3165. Curran Associates, Inc., 2017.
Huidong Liu, GU Xianfeng, and Dimitris Samaras. A Two-Step Computation of the Exact GAN
Wasserstein Distance. In ICML, pp. 3165-3174, 2018.
9
Under review as a conference paper at ICLR 2020
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
ICCV, pp. 3730-3738, 2015.
Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain Gelly.
High-Fidelity Image Generation With Fewer Labels. arXiv:1903.02271 [cs, stat], March 2019.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually Converge? In ICML, 2018.
Mehdi Mirza and Simon Osindero. Conditional Generative Adversarial Nets. arXiv:1411.1784 [cs,
stat], November 2014.
Takeru Miyato and Masanori Koyama. cGANs with Projection Discriminator. In ICLR, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral Normalization for
Generative Adversarial Networks. arXiv:1802.05957 [cs, stat], February 2018.
Olof Mogren. C-RNN-GAN: Continuous recurrent neural networks with adversarial training.
arXiv:1611.09904 [cs], November 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. F-GAN: Training Generative Neural
Samplers using Variational Divergence Minimization. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), NIPS, pp. 271-279. Curran Associates, Inc., 2016.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs.
arXiv:1709.08894 [cs, stat], September 2017.
Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, and Lior Wolf. Language Generation with
Recurrent Generative Adversarial Networks without Pre-training. arXiv:1706.01399 [cs], June
2017.
Guo-Jun Qi. Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities.
arXiv:1701.06264 [cs], January 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised Representation Learning with Deep
Convolutional Generative Adversarial Networks. arXiv:1511.06434 [cs], November 2015.
Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal, and Aaron Courville. Adversarial
Generation of Natural Language. arXiv:1705.10929 [cs, stat], May 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved Techniques for Training GANs. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), NIPS, pp. 2234-2242. Curran Associates, Inc., 2016.
Jianlin Su. GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint.
arXiv:1811.07296 [cs, stat], November 2018.
Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. Improving Generalization and Stability of
Generative Adversarial Networks. arXiv:1902.03984 [cs, stat], February 2019.
Cedric Villani. Optimal Transport: Old and New, volume 338. Springer Science & Business Media,
2008.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local Neural Networks.
arXiv:1711.07971 [cs], November 2017.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the Improved Training of
Wasserstein GANs: A Consistency Term and Its Dual Effect. arXiv:1803.01541 [cs, stat], March
2018.
Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, and Luc Van Gool. Wasserstein divergence
for gans. In ECCV, pp. 653-668, 2018.
Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. MidiNet: A Convolutional Generative Adversarial
Network for Symbolic-domain Music Generation. arXiv:1703.10847 [cs], March 2017.
10
Under review as a conference paper at ICLR 2020
Yasin Yazici, ChUan-Sheng Foo, Stefan Winkler, Kim-HUi Yap, Georgios Piliouras, and Vijay
Chandrasekhar. The Unusual Effectiveness of Averaging in GAN Training. arXiv:1806.04498 [cs,
stat], JUne 2018.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and AUgUstUs Odena. Self-Attention Generative
Adversarial Networks. arXiv:1805.08318 [cs, stat], May 2018.
JUnbo Zhao, Michael MathieU, and Yann LeCUn. Energy-based Generative Adversarial Network.
arXiv:1609.03126 [cs, stat], September 2016.
11
Under review as a conference paper at ICLR 2020
A Proof of Proposition 1
Proof. Suppose μ, υι, υ2∈ Pp(X), t1,t2 ≥ 0, tι +12 = 1, then there exist γι(x, y) and γ2(x, y)
with marginals (μ, υι) and (μ, υ2) satisfying:
Wι(μ,υι) = / kx - ykιdYι(χ,y),
X ×X
W1(μ,υ2) = /	∣∣x - y∣∣1dγ2(x,y).
X ×X
(12)
(13)
Let U = t1υ1 + t2υ2, Y(x, y) = t1γ1(x, y) + t2γ2(x, y), then Y(x, y) has marginals (μ, υ). We can
derive:
Wι(μ,υ) ≤ /	kx — ykιdγ(χ,y)
X ×X
= t1 kx - yk1dY1(x, y) +t2	kx - yk1dY2(x,y)
=tiWi(μ,Uι) + t2W1(μ,U2).
This conclusion can be extended to a general form:
Wι(μ,υ) ≤ tιW1(μ,υ1) + t2W1(μ,υ2) + .. .inWι(μ,Un),
(14)
(15)
where U1, U2,... Un ∈ Pp(x), tl, t2,...tn ≥ 0, tl +12 + •…+ tn = 1, U = tlUι + t2U2 + •…+ tn,Vn.
Suppose Pgi (1 ≤ i ≤ n) are the independent empirical measures drawn from Pg. From Eq. 15, we
can get
nn
Wl (Pr ,- X Pgi) ≤ — X Wl(Pr, Pg)	(16)
n i=1	n i=1
n
According to the strong law of large numbers, we can derive that with probability 1, n Ei=I Pgi →
Pg as - → ∞ (assuming Pg has finite first moments). Since W1 is continuous in Pp(X), we can
derive that with probability 1, Wι(Pr, n Pi=ι Pgi) → Wι(Pr, Pg) as — → ∞. By the law of large
numbers again, with probability 1, n Pi=ι Wι(Pr, Pgi) → E[W1(Pr, Pg)] as — → ∞. Thus we can
deduce that:
- ʌ .-
Wl (Pr, Pg ) ≤ E[Wl (Pr, Pg )].	(17)
Similarly, suppose Pri (1 ≤ i ≤ —) are the independent empirical measures drawn from Pr. Since the
symmetry of Wasserstein distance, we can deduce that:
ʌ - , ʌ ʌ .,
Wl (Pr, P g ) ≤ E[Wl (Pr, Pg )].	(18)
Therefore, combining Eq. 17 and Eq. 18, we can get Wι(Pr, Pg) ≤ E[W1(Pr, Pg)].
B Proof of Remark 2
Proof. First, let Dθ (x) ≡ 0, then
Lθ (Pr, Pg) = max	Ex 〜pr[Dθ (x)] - Ex 〜Pg [Dθ (x)]
Dθ∈L1	r	g
-[Ex〜Pr[Dθ(x)] - Ex〜Pg [Dθ(x)] - dλ(Pr, Pg )]十
≥ Ex〜Pr [0] - Ex〜Pg [0] - [Ex〜Pr [0] - Ex〜Pg [0] - dλ(Pr, Pg)]十
= 0,
(19)
where dλ(Pr, Pg) ≥ 0 is the Sinkhorn distance defined in Eq. 6.
Next, if Pr = Pg, then we have Lθ (Pr, Pg ) = 0. So we only need to show Lθ (Pr , Pg) > 0 if
Pr 6= Pg .
12
Under review as a conference paper at ICLR 2020
Let Dθ(x) = sign(Pr(x) - Pg(x)), we have
W = Ex〜Pr [Dθ (x)] - Ex〜Pg [Dθ (x)]
J (Pr(x) — Pg(x)) ∙ sign(Pr(x) — Pg(x))dx
(20)
>0
Applying this into Eq. 8 leads to
Lθ(Pr,Pg) = max	Ex〜Pr[Dθ(x)] - Ex〜Pg[Dθ(x)]
Dθ∈L1	r	g
-[Ex〜Pr [Dθ(x)] - Ex〜Pg [Dθ(x)] - dλ(Pr, Pg)i +
≥ w - [w - dλ(Pr, Pg)]十	QI)
(W,	if W ≤ dλ(Pr, Pg)
d dλ(Pr, Pg). otherwise
Since Pr = Pg, we know that dλ(Pr, Pg) > 0. Therefore, we have Lθ(Pr, Pg) > 0 while Pr = Pg.
We finish the proof.
C Proof of Remark 3
Proof. Let Pr(x) = δ(x - α), Pg(x) = δ(x - β) and α 6= β, then we have
Lθ(Pr, Pg) = max	Ex〜Pr[Dθ(x)] - Ex〜Pg [Dθ(x)]
Dθ∈L1	r	g
-[Ex〜Pr Dθ(x)] - Ex〜Pg Dθ(x)] - dλ(Pr, Pg)i 十
=maxι	Dθ(α) - Dθ(β) - [dθ(α) - Dθ(β) - dλ(P,, Pg)[	(22)
=maχ	(Dθ(α) - Dθ(β), if Dθ(α) - Dθ(β) ≤ dλ(Pr, Pg)
Dθ ∈Lι	1dλ(P r, P g).	otherwise
We know that Wasserstein distance W(Pr, Pg) = maxDθ∈L1 Dθ(α) - Dθ(β). Since Pr, Pg are
Dirac distributions, then we have W (Pr , Pg) = dλ (Pr, Pg). Combining this into Eq. 22 leads to
Lθ (Pr, Pg) = dλ(Pr , Pg).
Considering that Sinkhorn distance dλ(Pr, Pg) (Cuturi, 2013) allows gradient back-propagation, we
finish the proof.
D Curves of the Wasserstein term on CelebA-HQ Dataset
Fig. 8	shows the convergence curves of Wasserstein term for three WGAN methods. Convergence
values are different for different WGANs. For example, WGAN-GP converges to 10. WGAN-div is
5. WGAN-GPReal is 3. We use these values as bound.
E Additional S tab ility Experiments on the Wasserstein term
Fig. 9	shows the curves of Wasserstein term in the beginning iterations. Compared to the WGAN-
based method, WBGAN-based method is more stable. The network architecture is BigGAN and the
dataset is CelebA.
F Samples and Interpolations from Face Models
Here we display a few generated samples of face images by different approaches on the CelebA and
CelebA-HQ datasets.
13
Under review as a conference paper at ICLR 2020
100
80
50
WGAN-GP
WBGAN-GP
• W term=10
- WGAN-dv
- WBGAN-div
-W term=5
-50
-100 I-------1-------1--------1-------1-------1
0	2	4	6	8	10
Iterations	×104
(a)
90
WGAN-GPReal
WBGAN-GPReal
'W term=3
40	T	60 I
Λ___________________________________I	；____________________________________I
0	2	4	6	8	10	0	2	4	6	8	10
Iterations ×104	Iterations ×104
(b)	(C)
Figure 8:	Wasserstein term for all training iterations. (a) WGAN-GP vs WBGAN-GP, (b) WGAN-div
vs WBGAN-div, (c) WGAN-GPReal vs WBGAN-GPReal.
20 I-------1--------1-------1-------1-------- 15 -------------------1-------1-------1--------1-------1	10
3 O-5
E」9 »
15
WGAN-GP
----WBGAN-GP
WGAN-dv
----WBGAN-div
WGAN-GPReal
----WBGAN-GPReal
Efcs
O-5
E」s»
-20 1-------1-------1---------------1--------
0	2000	4000	6000	8000	10000
-15 ---------1-------1--------1--------1-------1
0	100	200	300	400	500
-10 1--------1-------1-------1--------1-------1
0	100	200	300	400	500
Iterations
(a)
Iterations
(b)
Iterations
(C)
Figure 9:	Wasserstein term in the beginning iterations. (a) WGAN-GP Vs WBGAN-GP, (b) WGAN-
div Vs WBGAN-div, (c) WGAN-GPReal vs WBGAN-GPReal.
(a) WGAN-GP
(b) WBGAN-GP
14
Under review as a conference paper at ICLR 2020
(c) WGAN-div
(d) WBGAN-div
(e) WGAN-GPReal
(f) WBGAN-GPReal
Figure 10: Samples of DCGAN on CelebA64
15
Under review as a conference paper at ICLR 2020
(a) WGAN-GP
(b) WBGAN-GP
(c) WGAN-div
(d) WBGAN-div
16
Under review as a conference paper at ICLR 2020
(e) WGAN-GPReal
(f) WBGAN-GPReal
Figure 11: Samples of BigGAN on CelebA64
17
Under review as a conference paper at ICLR 2020
(e) WGAN-GPReal
Figure 12: Interpolations of BigGAN between z on CelebA64
18
Under review as a conference paper at ICLR 2020
(a) WGAN-GP
(b) WBGAN-GP
(c) WGAN-div
(d) WBGAN-div
19
Under review as a conference paper at ICLR 2020
(e) WGAN-GPReal
(f) WBGAN-GPReal
Figure 13: Samples of BigGAN on CelebA-HQ128
20
Under review as a conference paper at ICLR 2020
(a) WGAN-GP
(b) WBGAN-GP
21
Under review as a conference paper at ICLR 2020
(c) WGAN-div
(d) WBGAN-div
22
Under review as a conference paper at ICLR 2020
(e) WGAN-GPReal
(f) WBGAN-GPReal
Figure 14: Interpolations of BigGAN between z on CelebA-HQ128
23