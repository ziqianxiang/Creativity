Under review as a conference paper at ICLR 2020
Representing Unordered Data Using
Multiset Automata and Complex Numbers
Anonymous authors
Paper under double-blind review
Ab stract
Unordered, variable-sized inputs arise in many settings across multiple fields. The ability
for set- and multiset- oriented neural networks to handle this type of input has been the
focus of much work in recent years. We propose to represent multisets using complex-
weighted multiset automata and show how the multiset representations of certain existing
neural architectures can be viewed as special cases of ours. Namely, (1) we provide a
new theoretical and intuitive justification for the Transformer model’s representation of
positions using sinusoidal functions, and (2) we extend the DeepSets model to use complex
numbers, enabling itto outperform the existing model on an extension of one of their tasks.
1	Introduction
Neural networks which operate on set-structured input have been gaining interest for their ability to handle
unordered and variable-sized inputs (Vinyals et al., 2015; Wagstaff et al., 2019). They have been applied
to various tasks, such as processing graph nodes (Murphy et al., 2018), hypergraphs (Maron et al., 2019),
3D image reconstruction (Yang et al., 2019), and point cloud classification and image tagging (Zaheer et al.,
2017). Similar network structures have been applied to multiple instance learning (Pevny and SomoL 2016).
In particular, the DeepSets model (Zaheer et al., 2017) computes a representation of each element of the set,
then combines the representations using a commutative function (e.g., addition) to form a representation of
the set that discards ordering information. Zaheer et al. (2017) provide a proof that any function on sets can
be modeled this way, by encoding sets as base-4 fractions and using the universal function approximation
theorem, but their actual proposed model is far simpler than the model constructed by the theorem.
In this paper, we propose to compute representations of multisets using weighted multiset automata, a variant
of weighted finite-state (string) automata in which the order of the input symbols does not affect the output.
In some sense, this is the most general representation of a multiset that can be computed incrementally using
only a finite amount of memory, and it can be directly implemented inside a neural network. We show how to
train these automata efficiently by approximating them with string automata whose weights form complex,
diagonal matrices.
Our representation generalizes DeepSets slightly, and it also turns out to be a generalization of the Trans-
former’s position encodings (Vaswani et al., 2017). In Sections 4 and 5, we discuss the application of our
representation in both cases.
•	The Transformer (Vaswani et al., 2017) models the absolute position of a word within a sentence.
This position can be thought of as a multiset over a single element, and indeed the Transformer
uses a position encoding involving sinusoidal functions that turns out to be a special case of our
representation. So weighted multiset automata provide a new theoretical and intuitive justification
for sinusoidal position encodings. We also experiment with several variations on position encodings
1
Under review as a conference paper at ICLR 2020
inspired by this justification, and although they do not yield any improvement, we do find that
learned position encodings in our representation do better than learning a different vector for each
absolute position.
•	We extend the DeepSets model to use our representation, which amounts to upgrading it from real
to complex numbers. On an extension of one of their tasks (adding a sequence of one-digit numbers
and predicting the units digit), our model is able to reach perfect performance, whereas the original
DeepSets model does no better than chance.
2	Weighted Multiset Automata
We define weighted finite automata below using a matrix formulation. Throughout, let K be either R or C.
Definition 1. A K-weighted finite automaton (WFA) over Σ is a tuple M = (Q, Σ, λ, μ, P), where Q =
{1, . . . , d} is a finite set of states, Σ is a finite alphabet, λ ∈ K1×d is a row vector of initial weights,
μ : Σ → Kd×d assigns a transition matrix to every symbol, and P ∈ Kd×1 is a column vector of final
weights.
(We do not use the final weights P in this paper, but include them for completeness.)
We extend the mapping μ to strings: If W = wι ∙∙∙ Wn ∈ Σ*, then μ(w) = μ(wι) ∙∙∙ μ(wn). Then, the
vector of forward weights of a string W is fwM (W) = λ (Qn=I μ(wp)).
Note that, different from many definitions of weighted automata, this definition does not allow -transitions,
and there may be more than one initial state. (Throughout this paper, we use to stand for a small real
number.)
The analogue of finite automata for multisets is the special case of the above definition where multiplication
of the transition matrices μ(a) does not depend on their order.
Definition 2. A K-weighted multiset finite automaton is one whose transition matrices commute pairwise.
That is,forall a, b ∈ Σ, we have μ(a)μ(b) = μ(b)μ(a).
Our proposal, then, is to represent a multiset w by the vector of forward weights, fwM (w), with respect to
some weighted multiset automaton M. In the context of a neural network, the transition weights μ(a) can be
computed by any function as long as it does not depend on the ordering of symbols, and the forward weights
can be used by the network in any way whatsoever.
3	Training
Definition 2 does not lend itself well to training, because parameter optimization needs to be done subject
to the commutativity constraint. Previous work (DeBenedetto and Chiang, 2018) suggested approximating
training of a multiset automaton by training a string automaton while using a regularizer to encourage the
weight matrices to be close to commuting. However, this strategy cannot make them commute exactly, and
the regularize], which has O(∣Σ∣2) terms, is expensive to compute.
Here, We pursue a different strategy, which is to restrict the transition matrices μ(a) to be diagonal. This
guarantees that they commute. As a bonus, diagonal matrices are computionally less expensive than full
matrices. Furthermore, we show that if we allow complex weights, we can learn multisets with diagonal
matrices almost as well as with full matrices. We show this first for the special case of unary automata (§3.1)
and then general multiset automata (§3.2).
2
Under review as a conference paper at ICLR 2020
3.1	Unary automata
Call an automaton unary if | Σ | = 1. Then, for brevity, We simply write μ instead of μ(a) where a is the only
symbol in Σ.
Let k ∙ k be the FrobeniUs norm; by equivalence of norms (Horn and Johnson, 2012, 352), the results below
should carry over to any other matrix norm, as long as it is monotone, that is: if A ≤ B elementwise, then
kAk ≤ kBk.
As stated above, our strategy for training a unary automaton is to allow μ to be complex but restrict it to be
diagonal. The restriction does not lose much generality, because any matrix can approximated by a complex
diagonal matrix in the following sense (Horn and Johnson, 2012, 116):
Proposition 1. For any complex square matrix A and > 0, there is a complex matrix E such that kEk ≤
and A + E is diagonalizable in C.
Proof. Form the Jordan decomposition A = PJP-1. We can choose a diagonal matrix D such that kDk ≤
K(P) (where K(P) = ∣∣P∣∣kPT∣∣) and the diagonal entries of J + D are all different. Then J + D is
diagonalizable. Let E = PDP-1; then ∣∣E∣ ≤ ∣∣Pk∣D∣∣PTk = K(P)∣D∣ ≤ g and A + E = P(J +
D)P-1 is also diagonalizable.	口
Thus, for a unary automaton M with transition matrix μ, we can choose Qμ0Q-1 close to μ such that
μ is diagonal. So M is close to the automaton with initial weights λ0 = λQ and transition weights μ0 ≈
Q-1μQ. This means that in training, we can directly learn complex initial weights λ0 and a complex diagonal
transition matrix μ0, and the resulting automaton (M0) should be able to represent multisets almost as well
as a general unary automaton (M) can.
It might be thought that even if μ0 approximates μ well, perhaps the forward weights, which involve possibly
large powers of μ, will not be approximated well. As some additional assurance, we have the following error
bound on the powers of μ:
Proposition 2. For any complex square matrix A, > 0, and 0 < r < 1, there is a complex matrix E such
that A + E is diagonalizable in C and, for all n ≥ 0,
∣(A+E)
∣(A+E)
n-An∣ ≤rn
n -An∣
∣An ∣
if A nilpotent,
otherwise.
≤ n
For the proof, please see Appendix A.
3.2	General case
In this section, we allow Σ tobe of any size. Proposition 1 unfortunately does not hold in general for multiple
matrices (O’Meara and Vinsonhaler, 2006). That is, it may not be possible to perturb a set of commuting
matrices so that they are simultaneously diagonalizable.
Definition 3. Matrices A1, . . . , Am are simultaneously diagonalizable if there exists an invertible matrix P
such that PAiP-1 is diagonal for all i ∈ {1, ∙∙∙ , n}.
We say that Ai,…,Am are approximately simultaneously diagonalizable (ASD) if, for any e > 0, there are
matrices E1, . . . , Em such that ∣Ei∣ ≤ and A1 + E1, . . . , Am + Em are simultaneously diagonalizable.
O’Meara and Vinsonhaler (2006) give examples of sets of matrices that are commuting but not ASD. How-
ever, ifwe are willing to add new states to the automaton (that is, to increase the dimensionality of the weight
matrices), we can make them ASD.
3
Under review as a conference paper at ICLR 2020
Proposition 3. Any weighted multiset automaton is close to an automaton that can be converted to a
complex-weighted diagonal automaton, possibly with more states.
Proof. First we start with a fact from O’Meara and Vinsonhaler (2006).
Lemma 4. Suppose A1 , . . . , Ak are commuting n × n matrices over an algebraically closed field F. Then
there exists an invertible matrix C such that C-1A1C, . . . , C-1AkC are block diagonal matrices with
matching block structures and each diagonal block has only a single eigenvalue (ignoring multiplicities).
That is, there is a partition n = nι + •…+ n of n such that
「Bn	-
1	Bi2
C TAiC = Bi =	,	,	(1)
.
.
Bir
where each Bij is an nj × nj matrix having only a single eigenvalue for i = 1, . . . , k and j = 1, . . . , r.
Moreover, ifB1j, . . . , Bkj are ASD for j = 1, . . . , r, then A1, . . . , Ak are ASD.
Furthermore, O’Meara and Vinsonhaler (2006) observe that each block can be written as Bij = λij I + Nij
where Nij is nilpotent, so A1, . . . , Ak are ASD iff N1j, . . . , Nkj are for all j.
So the transition matrices of the automaton can be rewritten in the above form, and the problem of converting
an automaton to one that is ASD is reduced to the problem of converting an automaton with nilpotent
transition matrices (equivalently, an automaton recognizing a finite language) to one that is ASD (possibly
with more states). See Appendix B for one such construction.	□
This means that if we want to learn representations of multisets over a finite alphabet Σ, it suffices to con-
strain the transition matrices to be complex diagonal, possibly with more states. Unfortunately, the best
construction we know of (Appendix B) increases the number of states by a lot. But this does not in any
way prevent the use our representation; we can choose however many states we want, and it’s an empirical
question whether the number of states is enough to learn good representations.
The following two sections look at two practical applications of our representation.
4 Position Encodings
One of the distinguishing features of the Transformer network for machine translation (Vaswani et al., 2017),
compared with older RNN-based models, is its curious-looking position encodings,
j-1 = sin 10000-2(j-1)/d(p - 1)
ep2j = cos 10000-2(j-1)/d(p - 1)
(2)
which map word positions p (ranging from 1 to n, the sentence length) to points in the plane and are the
model’s sole source of information about word order.
In this section, we show how these position encodings can be interpreted as the forward weights of a weighted
unary automaton. We also report on some experiments on some extensions of position encodings inspired
by this interpretation.
4
Under review as a conference paper at ICLR 2020
4.1 As a weighted unary automaton
Consider a diagonal unary automaton M in the following form:
λ=	exp iφ1		exp -iφ1	exp iφ2	exp —iφ2	…
		exp iθ1	0	0	0	∙∙∙
		0	exp -iθ1	0	0	…
μ 二		0	0	exp iθ2	0	...
		0 .	0 .	0 .	exp —iθ2	•… ..
		. .	. .	. .	..
In order for a complex-weighted automaton to be equivalent to some real-weighted automaton, the entries
must come in conjugate pairs like this, so this form is fully general.
By a change of basis, this becomes the following unary automaton M 0 (this is sometimes called the real
Jordan form):
λ0=	cos φ1	sin φ1	Cos φ2 Sin φ2 …]			
	cos θ1	sin θ1	0	0	• • •	
	—Sin θι	cos θ1	0	0	• • •	(3)
μ0 =	0	0	cos θ2	sin θ2	...	
	0 . . .	0 . . .	— sin θ2 . . .	cos θ2 . . .	••• . . .	
Then, for any string prefix u (making use of the angle sum identities):
fwM o (U) =	[cos(φι +	∣u∣θι)	sin(φι	+	∣u∣θι)	cos(φ2 +	∣u∣θ2)	sin(φ2 + ∣u∣θ2)	…]
If we let
φi = π
Vi	2
θj = -10000-2(j-1)/d
this becomes exactly equal to the position encodings defined in (2). Thus, the Transformer’s position encod-
ings can be reinterpreted as follows: it runs automaton M0 over the input string and uses the forward weights
of M0 just before position p to represent p. This encoding, together with the embedding of word wp, is used
as the input to the first self-attention layer.
4.2 Experiments
This reinterpretation suggests some potential extensions to position encodings: 1. Using the diagonal, polar
form of the transition matrix (3), learn the φi and θi instead of keeping them fixed. 2. Learn all the initial
weights and full transition matrix directly.
We carried out some experiments to see if these methods perform better or worse than the original. We used
an open-source implementation of the Transformer, Witwicky.1 The settings used were the default settings,
except that we used 8k joint BPE operations and d = 512 embedding dimensions. We tested the following
variations on position encodings.
1https://github.com/tnq177/witwicky
5
Under review as a conference paper at ICLR 2020
Model	Training	case-insensitive BLEU						
		En-Vi*	Uz-En	Ha-En	Hu-En	Ur-En	Ta-En	Tu-En
diagonal polar	fixed	32.6	25.7	24.4	34.2	11.5	13.4	25.7
	learned angles	32.7	25.8	25.4	34.0	11.1	14.1	25.7
full matrix	random	32.6	25.9^^	25.8	34.1	10.9	12.8	26.1
	learned	32.5	24.5	23.6	33.5	11.4	14.5	23.8
per position	random	32.6	24.9	24.6	34.1	11.0	14.1	24.4
	learned	32.1	22.6	21.2	33.0	11.7	14.4	21.1
*tokenized references								
Table 1: Machine translation experiments with various position encodings. Scores are in case-insensitive
BLEU, a common machine translation metric. The best score in each column is printed in boldface.
• Diagonal polar
-fixed: The original sinusoidal encodings (VasWani et al., 2017).
-learned angles: Initialize the φ% and θ% to the original values, then optimize them (#1 above).
• Full matrix
- random: Randomize initial Weights so that their expected norm is the same as the original, and
transition matrix using orthogonal initialization (Saxe et al., 2013), and do not optimize them.
-learned: Initialize λ and μ as above, and then optimize them (#2 above).
• Per position
-	random: Choose a vector With fixed norm and random angle for each absolute position, and
do not optimize them.
-	learned: Learn an encoding for each absolute position (Gehring et al., 2017).
Table 1 shoWs that no method is clearly the best. The only method that appears to be Worse than the others
is “per position, learned,” Which, although best on Urdu-English, does much Worse than other methods on
several tasks. By contrast, the learned embeddings based on multiset automata (“diagonal polar, learned
angles” and “full matrix, learned”) are usually close to the best, lending some support to our interpretation.
5 Complex DeepSets
In this sectoin, We incorporate a Weighted multiset automaton into the DeepSets (Zaheer et al., 2017) model,
extending it to use complex numbers.
5.1	Models
The DeepSets model computes a vector representation for each input symbol and sums them to discard
ordering information. We may think of the elementWise layers as computing the log-Weights of a diagonal
multiset automaton, and the summation layer as computing the forWard log-Weights of the multiset. (The
logs are needed because DeepSets adds, Whereas multiset automata multiply.) HoWever, DeepSets uses only
real Weights, Whereas our multiset automata use complex Weights. Thus, DeepSets can be vieWed as using a
multiset representation Which is a special case of ours.
6
Under review as a conference paper at ICLR 2020
Figure 1: Neural architecture for (left to right) LSTM, GRU, DeepSets, and our model. Each cell indicates
layer type and output dimension. Color indicates which part of a multiset automaton it corresponds to, though
only our model is fully general for multiset automata.
________________IMUItiSet input
I DiagOnal transition matrices
I FOrWard WeightS
We conduct experiments comparing the DeepSets model (Zaheer et al., 2017), a GRU model, an LSTM
model, and our complex multiset model. The code and layer sizes for the three baselines come from the
DeepSets paper.2 See Figure 1 for layer types and sizes for the three baseline models.
In our system, to avoid underflow when multiplying many complex numbers, we store each complex number
as er(a+bi) where r, a, and b are real and a and b are normalized suchthata2+b2 = 1 prior to multiplication.
Thus, for each complex-valued parameter, we have three scalars (r, a, and b) to learn. To this end, each input
is fed into three separate embedding layers of size 50 (for r, a, and b). (While the DeepSets code uses a
dense layer at this point, in our network, we found that we could feed the embeddings directly into a complex
multiplication layer to discard ordering information. This reduced the number of parameters for our model
and did not affect performance.) The output of this is then a new r, a, and b which are concatenated and fed
into a final dense layer as before to obtain the output. Since our diagonalized automata have complex initial
weights (λ0), we also tried learning a complex initial weight vector λ0, but this had no effect on performance.
The total number of parameters for each model was 4,161 parameters for the DeepSets model, 31,351 pa-
rameters for the LSTM model, 44,621 parameters for the GRU model, and 1,801 parameters for our model.
In order to eliminate number of parameters as a difference from our model to the DeepSets model, we also
tried the DeepSets model without the first dense layer and with embedding sizes of 150 to exactly match the
number of parameters of our model, and the results on the test tasks were not significantly different from the
baseline DeepSets model.
For all experiments, we used mean squared error loss, a learning rate decay of 0.5 after the validation loss
does not decrease for 2 epochs, and early stopping after the validation loss does not decrease for 10 epochs.
2https://github.com/manzilzaheer/DeepSets/blob/master/DigitSum/text_sum.ipynb
7
Under review as a conference paper at ICLR 2020
ycarucc
ycarucc
Digit Sum, Units Digit
1.0-- -
0.8
0.6
0.4
0.2
K八-小不不”不，不.不•,不不不不不二K K不一不不一
0.0
20	40	60	80
Number of digits
DeePSets LSTM GRU Our Method
Figure 2: Results for task 1 (left) and task 2 (right). In task 1, the LSTM and GRU models were unable
to generalize to examPles larger than seen in training, while DeePSets and our model generalize to all test
lengths. For task 2, only our model is able to return the correct units digit for all test lengths. The GRU,
LSTM, and DeePSets models fail to learn any behavior beyond random guessing.
5.2 Experiments
Task 1: Sum of digits In this task, taken from Zaheer et al. (2017), the network receives a set of single
digit integers as inPut and must outPut the sum of those digits. The outPut is rounded to the nearest integer
to measure accuracy. The training set consisted of 100k randomly generated sequences of digits 1-9 with
lengths from 1 to 50. They were fed to each network in the order in which they were generated (which only
affects GRU and LSTM). This was then sPlit into training and dev with aPProximately a 99/1 sPlit. The test
set consisted of randomly generated sequences of lengths that were multiPles of 5 from 5 to 95. Figure 2
shows that both our model and DeePSets obtain Perfect accuracy on the test data, while the LSTM and GRU
fail to generalize to longer sequences.
Task 2: Returning units digit of a sum The second task is similar to the first, but only requires returning
the units digit of the sum. The data and evaluation are otherwise the same as task 1. Here, random guessing
within the outPut range 0-9 achieves aPProximately 10% accuracy. Figure 2 show that DeePSets, LSTM,
and GRU are unable to achieve Performance better than random guessing on the test data. Our method is
able to return the units digit Perfectly for all test lengths, because it effectively learns to use the cyclic nature
of comPlex multiPlication to Produce the units digit.
6 Conclusion
We have Proven that weighted multiset automata can be aPProximated by automata with (comPlex) diagonal
transition matrices. This formulation Permits simPler elementwise multiPlication instead of matrix multi-
Plication, and requires fewer Parameters when using the same number of states. We show that this tyPe of
automaton naturally arises within existing neural architectures, and that this rePresentation generalizes two
existing multiset rePresentations, the Transformer’s Position encodings and DeePSets. Our results Provide
new theoretical and intuitive justification for these models, and, in one case, lead to a change in the model
that drastically imProves its Performance.
8
Under review as a conference paper at ICLR 2020
References
Justin DeBenedetto and David Chiang. Algorithms and training for weighted multiset automata and regular
expressions. In Cezar Campeanu, editor, Implementation and Application of Automata, pages 146-158.
Springer, 2018.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence
to sequence learning, 2017. arXiv:1705.03122.
Roger A. Horn and Charles A. Johnson. Matrix Analysis. Cambridge Univ. Press, 2nd edition, 2012.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph net-
works. In Proc. ICLR, 2019.
Ryan L. Murphy, Balasubramaniam Srinivasan, Vinayak A. Rao, and Bruno Ribeiro. Janossy pooling:
Learning deep permutation-invariant functions for variable-size inputs, 2018. arXiv:1811.01900.
K.C. O’Meara and C. Vinsonhaler. On approximately simultaneously diagonalizable matrices. Linear Alge-
bra and its Applications, 412(1):39-74, 2006.
TomaS Pevny and Petr SomoL Using neural network formalism to solve multiple-instance problems. In
Proc. ISNN, 2016.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks, 2013. arXiv:1312.6120.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. NeurIPS, 2017.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets, 2015.
arXiv:1511.06391.
Edward Wagstaff, Fabian B. Fuchs, Martin Engelcke, Ingmar Posner, and Michael A. Osborne. On the
limitations of representing functions on sets, 2019. arXiv:1901.09006.
Bo Yang, Sen Wang, Andrew Markham, and Niki Trigoni. Robust attentional aggregation of deep feature
sets for multi-view 3D reconstruction. International Journal of Computer Vision, 08 2019. doi: 10.1007/
s11263-019-01217-w.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, BarnabaS Poczos, Ruslan Salakhutdinov, and Alexan-
der J. Smola. Deep sets. In Proc. NeurIPS, 2017.
A Proof of Proposition 2
Lemma 5. IfJ is a Jordan block with nonzero eigenvalue, then the bound of Proposition 2 holds for J.
Proof. Note that for any δ, ≥ 0, we have
(1 - δ)(1 - ) ≥ 1 - δ - ≥ 1 - 2 max{δ, }
(1 - )n ≥ 1 - n
9
Under review as a conference paper at ICLR 2020
The powers of J look like
	「(n) λn (n) λn-1 (n) λn-2	∙∙∙ι
	(n)λn	(n)λn-1	…
Jn =	an… . . .
More concisely, for k ≥ j ,	
	[Jn]jk = k-njλn-k+j.
Let D be a diagonal matrix whose elements are in [-λ, 0) and are all different. The powers of (J + D) are
[(J + D)n]jk = cjk[Jn]jk
where
cjk ≥ (1 - )n-k+j
≥ 1 - (n - k + j)
≥ 1 - n.
Finally, form their difference:
[(J + D)n - Jn]jk = (cjk - 1)[Jn]jk
|[(J + Dr- Jn]jk | ≤ n ∣[Jn]jk |
k(J+D)n-Jnk≤nkJnk
k(J + Dn- Jnk ≤ n
Jl	≤
Lemma 6. If J is a Jordan block with zero eigenvalue, then for any > 0, r > 0, there is a complex matrix
E such that M + E is diagonalizable in C and
k(M+E)n-Mnk ≤ rn.
Proof. In this case, We have to perturb the diagonal elements to nonzero values. For any δ ≤ 2, let D be a
diagonal matrix whose elements are in (0, δ] and are all different. Then the elements of ((J + D)n - Jn)
are:
[(J + D)n - Jn]jk ≤ k-nj δn-k+j	(0≤k-j < min{n, d})
< 2n δ min{0,n-d}+1
so the error is at most 2d-1d(2δ)min{0,n-d}+1. Let δ = min{2, r, (2)d d}.	口
NoW We can prove Proposition 2.
10
Under review as a conference paper at ICLR 2020
Proof. Form the Jordan decomposition M = PJP-1, where
We begin with the non-nilpotent case. Let κ(P) = kP kkP -1 k be the Frobenius condition number of P. For
each Jordan block Jj :
•	If Jj has nonzero eigenvalue, perturb it so that the absolute error of the nth power is at most
K(P)2 kJpk ,by Lemma 5.
•	If Jj has zero eigenvalue, perturb it so that the absolute error is at most K(P)2 PJ) , by Lemma 6.
n	kJnk
Then the total absolute error of all the blocks With nonzero eigenvalue is at most JPy ^k. And since
kJnk
ρ( J)n ≤ k Jnk, the total absolute error of all the blocks with zero eigenvalue is also at most K(P)2 ^k. So
the combined total is
n
k(j+Ey- Jnk ≤ F kJnk.
Finally,
k(M +E)n - Mnk = kP((J+D)n - Jn)P-1k
≤κ(P)k((J+D)n-Jn)k
n
≤ KP kJnk
≤ 忐 kP-1M np k
≤ nkMnk
k(M + E)n - Mnk
Pl ≤n
□
If M is nilpotent, the above argument does not go through, because ρ(J) = 0. Instead, use Lemma 6 to
bound the absolute error of each block by 个, so that the total absolute error is at most rne.
B	Making Automata ASD
In this section, we give a construction for converting a multiset automaton to one that is equivalent, but
possibly has more states.
Let ㊉ stand for the direct product of vector spaces, 0 for the Kronecker product, and define the shuffle
product AluB = A 01 +10 B. (This is known as the Kronecker sum and is sometimes notated ㊉, but we
11
Under review as a conference paper at ICLR 2020
use that for direct product.) These operations extend naturally to weighted multiset automata and correspond
roughly to union and concatenation, respectively:
λA㊉B =	λA ㊉	λB	μA㊉B (a)	= μA (a)㊉ RB (a)	PA㊉B	= PA ㊉ PB
XaluB =	λA 脸》b	MAluB (a)	= μA (a)	LU μB (a)	PALLJB	= PB 0 PB
They are of interest here because they preserve the ASD property:
Proposition 7. If Mi and M2 are multiset automata with ASD transition matrices, then Mi ㊉ M2 hasASD
transition matrices, and Mi LU M2 has ASD transition matrices.
Proof. First consider the ㊉ operation. Let μι (a) (for all a) be the transition matrices of Mi. For any e > 0,
let Ei(a) be the perturbations of the μι(a) such that ∣∣Ei(a)k ≤ e/2 and the μι(a) + Ei(a) (for all a) are
simultaneously diagonalizable. Similarly for M2. Then the matrices (μi(a) + Ei(a))㊉(μ2(a) + E2(a))
(for all a) are simultaneously diagonalizable, and
k(μi(a) + Ei(a))㊉(μ?(a) + E2(a)) — μi(a)㊉ μ2(a)k = ∣∣Ei(a)㊉ E2(a)k
≤ kEi(a)k + kE2(a)k
= .
Next, We consider the LlJ operation. Let di and d2 be the number of states in Mi and M2, respectively.
This time, we choose ∣∣Ei(a)k ≤ e∕(2d2) and ∣∣E2(a)k ≤ e∕(2di). Then the matrices (μi(a) + Ei(a)) LLJ
(μ2(a) + E2(a)) (for all a) are simultaneously diagonalizable, and
(μi(a) + Ei(a)) LU (μ2(a) + E2(a)) = (μi(a) + Ei(a)) 0 I + I 0 (μ2(a) + E2(a))
=μi(a) 0 I + Ei(a) 0 I + I 0 μ2(a) + I 0 E2(a)
=(μi(a) LU μ2(a)) + (Ei(a) LU E2(a))
k(μi(a) + Ei(a)) LU (μ2(a) + E2(a)) 一 μi(a) LU μ2(a)k = ∣∣Ei(a) LU E2(a)k
= ∣Ei(a) 0 I + I 0 E2 (a)∣
≤ ∣Ei(a)0I∣ + ∣I 0 E2(a)∣
≤ ∣Ei(a)∣d2+di∣E2(a)∣
≤ .
□
Proposition 8. IfM is a weighted multiset automaton with d states recognizing a finite language (that is, all
of its transition matrices are nilpotent), there exists an equivalent automaton with O(d2|£|+i) states whose
transition matrices are ASD.
Proof. Because any set of commuting matrices can be simultaneously triangularized by a change of basis,
assume without loss of generality that M’s transition matrices are upper triangular, that is, there are no
transitions from state q to state r where q > r.
The idea is that M0 should simulate arun of M in which the symbols are read in lexicographic order. It does
so by building up partial runs, one for each symbol in Σ, and then stitching them together.
Let Q be the states of M, and let ai, . . . , am be the symbols of Σ. For all a ∈ Σ, q, r ∈ Q, define Mq,a,r to
be the automaton which simulates M starting from state q, reading only a’s, and ending in state r. Then
M0 = M …M λq0Mq0,αi,qi 山…山 Mqm-ι,am,qm
q0 ∈Q	qm ∈Q
(where the multiplication by the scalar λq0 means scaling the initial weight vector by λq0). By Proposition 7,
M0 is ASD, and because each of the Mq,a,r has no more than d states, M0 has at most d2|£|+i states. 口
12