Under review as a conference paper at ICLR 2020
Variance Reduced Local SGD
with Lower Communication Complexity
Anonymous authors
Paper under double-blind review
Ab stract
To accelerate the training of machine learning models, distributed stochastic gra-
dient descent (SGD) and its variants have been widely adopted, which apply mul-
tiple workers in parallel to speed up training. Among them, Local SGD has gained
much attention due to its lower communication cost. Nevertheless, when the data
distribution on workers is non-identical, Local SGD requires O(T 3 N3) CommU-
nications to maintain its linear iteration speedup property, where T is the total
number of iterations and N is the number of workers. In this paper, we propose
Variance Reduced Local SGD (VRL-SGD) to further reduce the communication
complexity. Benefiting from eliminating the dependency on the gradient variance
among workers, we theoretically prove that VRL-SGD achieves a linear iteration
13
Speedup with a lower communication complexity O(T2 N 2) even if workers ac-
cess non-identical datasets. We conduct experiments on three machine learning
tasks, and the experimental results demonstrate that VRL-SGD performs impres-
sively better than Local SGD when the data among workers are quite diverse.
1	Introduction
With the expansion of data and model scale, the training of machine learning models, especially
deep learning models has become increasingly time-consuming. To accelerate the training process,
distributed parallel optimization has attracted widespread interests recently, which encourages mul-
tiple workers to cooperatively optimize the model.
For large-scale machine learning problems, stochastic gradient descent (SGD) is a fundamental tool.
It can be easily parallelized by collecting stochastic gradient from different workers and hence it is
widely adopted. Previous studies (Dekel et al., 2012; Ghadimi & Lan, 2013) justify that synchronous
stochastic gradient descent (S-SGD) has a linear iteration speedup for both general convex and non-
convex objectives, which means that the total number of iterations is reduced by N times with N
workers. However, S-SGD suffers from a major drawback: the communication cost among workers
is expensive when the number of workers is large, which prevents S-SGD from achieving a linear
time speedup. Therefore, it is crucial to overcome the communication bottleneck.
To reduce communication cost, several studies (Wang & Joshi, 2018; Zhou & Cong, 2018; Stich,
2019; Yu et al., 2019b; Shen et al., 2019) have managed to lower the communication frequency.
Among them, Local SGD (Stich, 2019) is a representative distributed algorithm, where workers can
conduct SGD locally and average model with each other every k iterations. Compared with S-SGD,
the algorithms based on Local SGD reduce the communication rounds from O(T) to O(T /k). To
deal with the gradient variance among workers, previous studies require at least one of the follow-
ing extra assumptions: (1) the bounded gradient variance among workers; (2) an upper bound for
gradients; (3) identical data on all workers. When the data distribution on workers is identical,
which is the so-called identical case, the algorithms based on Local SGD can exhibit superior per-
formance. Nevertheless, the identical data assumption is not always valid in real cases. When the
data distribution on workers is non-identical, which is the so-called non-identical case, these algo-
rithms would encounter a significant degradation in the convergence rate due to the gradient variance
among workers. We seek to eliminate the gradient variance among workers, which may make the
algorithm converge much faster than the vanilla Local SGD.
In this paper, we propose Variance Reduced Local SGD (VRL-SGD), a novel distributed optimiza-
tion algorithm to further reduce the communication complexity. Benefiting from an additional vari-
1
Under review as a conference paper at ICLR 2020
ance reduction component, VRL-SGD eliminates the extra assumption about bounded gradient vari-
ance among workers in previous studies based on Local SGD (Yu et al., 2019a;b; Shen et al., 2019).
33	13
Thus the communication complexity can be reduced from O(T4 N4) to O(T2 N2) in VRL-SGD
for the non-identical case, which is crucial for achieving a better time speedup. Therefore, VRL-
SGD is more suitable than Local SGD for the scenarios, such as federated learning (KoneCny et al.,
2016), where the gradient variance across workers might be large.
Contributions of this paper are summarized as follows:
•	We propose VRL-SGD, a novel distributed optimization algorithm with a state-of-the-art
communication complexity. Specifically, the communication complexity is reduced from
33	13
O(T4 N4) to O(T2 N2) for the non-identical case. To the best of our knowledge, this
is the first time that an algorithm based on Local SGD possesses such a communication
complexity for the non-identical case. Meanwhile, VRL-SGD also achieves the optimal
communication complexity for the identical case.
•	We provide a theoretical analysis and prove that VRL-SGD has a linear iteration speedup
with respect to the number of workers. Our method does not require the extra assumptions,
e.g. the gradient variance across workers is bounded.
•	We validate the effectiveness of VRL-SGD on three standard machine learning tasks. And
experimental results show that the proposed algorithm performs significantly better than
Local SGD if data distribution in workers is different, while maintains the same conver-
gence rate as Local SGD if all workers access identical datasets.
2	Related Work
Synchronous stochastic gradient descent (S-SGD) is a parallelized version of mini-batch SGD and
is theoretically proved to achieve a linear iteration speedup with respect to the number of work-
ers (Dekel et al., 2012; Ghadimi & Lan, 2013). Nevertheless, due to the communication bottleneck,
it is difficult to obtain the property of linear time speedup. To eliminate communication bottlenecks,
many distributed SGD-based methods are proposed, such as lossy compression methods (Alistarh
et al., 2017; Aji & Heafield, 2017; Bernstein et al., 2019; Lin et al., 2018b; Karimireddy et al., 2019;
Tang et al., 2019), which use inexact approximations or partial data to represent the gradients, and
methods (Stich, 2019; Yu et al., 2019b) based on the lower communication frequency.
Among them, Local SGD (Stich, 2019), a representative method to lower the communication fre-
quency, has been widely used in the training of large-scale machine learning models, and its superior
performance is verified in several tasks (Povey et al., 2014; Su & Chen, 2015; Lin et al., 2018a). In
Local SGD, each worker conducts SGD updates locally and averages its model with others period-
ically. Previous studies have proven that Local SGD can attain a linear iteration speedup for both
strongly convex (Stich, 2019) and non-convex (Yu et al., 2019b) problems. To fully utilize hardware
resources, a variant of Local SGD, called CoCoD-SGD (Shen et al., 2019), is proposed with the de-
coupling of computation and communication. Furthermore, Yu et al. (2019a) provide a clear linear
speedup analysis for Local SGD with momentum. However, most of the above algorithms assume
that the gradient variance among workers is bounded, and some of them even depend on a stronger
assumption, e.g., the data distribution on workers is identical. Dependence on these assumptions
may lead to a slow convergence rate for the non-identical case, which limits the further reduction
of communication frequency and avoids a better time speedup. Haddadpour et al. (2019) verify that
the use of redundant data can lead to lower communication complexity and hence faster conver-
gence. The redundant data can help reduce the gradient variance among workers, thus it avoids the
slow convergence rate. Nevertheless, this method may be constrained in some cases. For instance, it
could not be widely applied in federated learning (Konecny et al., 2016) as data cannot be exchanged
between workers for privacy-preserving.
Although there are many studies proposed to reduce the variance in SGD, e.g., SVRG (Johnson
& Zhang, 2013), SAGA (Defazio et al., 2014), and SARAH (Nguyen et al., 2017), they could not
directly deal with the gradient variance among workers in distributed optimization. In recent years,
several studies (Shi et al., 2015; Mokhtari & Ribeiro, 2016; Tang et al., 2018) have proposed to
eliminate the gradient variance among workers in the decentralized setting. Among them, Shi et al.
(2015) propose a novel decentralized algorithm, EXTRA, which provides an ergodic convergence
2
Under review as a conference paper at ICLR 2020
Table 1: Comparisons of the communication complexity for different algorithms. The second col-
umn and the third column show communication complexity for identical and non-identical datasets
respectively. Here, we regard the following assumptions as extra assumptions: (1) an upper bound
for gradients; (2) the bounded gradient variance among workers.
Reference	IDENTICAL DATA	non-identical data	Extra Assumptions
Ghadimi & Lan (2013)	T	T	NO
Yu et al. (2019b)	33 O(N 3 T 4)	33 O(N 3 T 4)	(1)
Shen et al. (2019)	O(N 3 T 2)	O(N 3 T 4)	(2)
This Paper	O(N 3 T 2)	O(N 3 T 2)	NO
rate for convex problems and a linear convergence rate for strongly convex problems benefiting from
eliminating the variance among workers. The D2 (Tang et al., 2018) algorithm further applies the
variance reduction on non-convex stochastic decentralized optimization problems and removes the
impact of the gradient variance among workers on the convergence rate.
To eliminate the gradient variance among workers and accelerate the training, we incorporate the
variance reduction technique into Local SGD, and hence reduce the extra assumptions in the theoret-
ical analysis. For a better comparison with related algorithms in terms of communication complexity
and assumptions, we summarize the results in Table 1. It presents that our algorithm achieves better
communication complexity compared with the previous algorithms for the non-identical case and
does not need extra assumptions.
3	Preliminary
3.1	Problem definition
We focus on data-parallel distributed training, where N workers collaboratively train a machine
learning model, and each worker may have its data with different distributions, which is the non-
identical case. We use Di to denote the local data distribution in the i-th worker. Specifically, we
consider the following finite-sum optimization:
min f (x) := 1j X fi(x),	⑴
x∈Rd	N
i=1
where fi(x) := Eξi-Di [fi(x, ξi)] is the local loss function of the i-th worker.
3.2	Notations
First of all, we summarize the key notations of this paper as follows.
•	k ∙ k denotes the '2 norm of a vector.
•	f * is the optimal value of equation (58).
•	E denotes that the expectation is taken with respect to all random indexes sampled to cal-
culate stochastic gradients in all iterations.
•	xit denotes the local model of the i-th worker at the t-th iteration.
•	xt denotes the average of local models over all N workers, and that is xt =焉 PN=I xt
•	Vfi(χt, ξt) is a stochastic gradient of the i-th worker at the t-th iteration.
•	t0 represents the iteration of the last communication, and that is t0 = [ t C k.
•	t00 represents the iteration of the penultimate communication, and that is t00 = ([kC 一 1)k.
3
Under review as a conference paper at ICLR 2020
3.3	Assumptions
Throughout this paper, we make the following assumptions, which are commonly used in the theo-
retical analysis of distributed algorithms (Stich, 2019; Yu et al., 2019a; Shen et al., 2019).
Assumption 1
(1)	Lipschitz gradient: All local functions fi ’s have L-Lipschitz gradients
l∣Vfi(x) - Vfi(y)k ≤ Lkx - yk,∀i,∀x, y ∈ Rd.	(2)
(2)	Bounded variance within each worker: There exists a constant σ such that
Eξ〜DikVfi(x,ξ)-Vfi(x)k2 ≤ σ2, ∀x ∈ Rd, ∀i.	(3)
(3)	Dependence of random variables: ξit ’s are independent random variables, where t ∈
{0,1,…，T — 1} and i ∈ {1, 2,…，N}.
Previous studies based on Local SGD assume that the gradient variance among workers is bounded,
or even depend on a stronger assumption, e.g., an upper bound for gradients or identical data distri-
bution on workers, while we do not require these assumptions.
4	Algorithm
In this section, we first introduce the proposed algorithm and then give an intuitive explanation.
4.1	Variance Reduced Local SGD
We propose VRL-SGD, a variant of Local SGD. VRL-SGD allows locally updating in each worker
to reduce the communication cost. But there are a few more steps in VRL-SGD to eliminate the
gradient variance among workers. And in VRL-SGD, a worker:
1.	Communicates with other workers to get the average of all local models xt = N PN=I xt∙
2.	Calculates ∆it0, which denotes the average deviation of gradient between the local gradients
and the global gradients in the previous period. And it is defined as
N' =∆i00 + -1(xt - xt),	(4)
kγ
where k is the communication period and γ is the learning rate.
3.	Updates local model k times with a stochastic approximation gradient vit in the form of
xit+1 = xti - γvit .	(5)
The essential part of equation (5) is the gradient approximation vit , which is formed by
vit=Vfi(xit,ξit)-∆it0.	(6)
The complete procedure of VRL-SGD is summarized in Algorithm 1. VRL-SGD allows each
worker to maintain its local model xit and gets the average of all local models every k steps. Note
that VRL-SGD with k = 1 is equivalent to S-SGD. While VRL-SGD with k > 1 reduces the num-
ber of communication rounds by k times compared with S-SGD. And VRL-SGD is equivalent to
Local SGD ifwe set ∆i be 0 in line 5 of Algorithm 1 all the time.
To achieve a linear iteration speedup, Local SGD requires that T is more than O(N 3k4). In
13
other words, the communication period k in Local SGD is bounded by O(T4/N4), which re-
33
duces the communication complexity to O(N4T4). Notice that a better communication period
bound O(T 2 /N3) can be attained in the identical case in the previous studies (Shen et al., 2019;
Yu et al., 2019a). Nevertheless, the proposed algorithm can attain the communication period bound
O(T1 /N3) in both the identical case and the non-identical case.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Variance Reduced Local SGD (VRL-SGD)
1:	Input: Initialize x0 = X0 ∈ Rd, ∆0 = 0 ∈ Rd, ∀i and t = 0. Set learning rate γ > 0 and
communication period k > 0.
2:	while t < T do
3:	Worker Wi does:
4:	Communicate with other workers to get the average of all local models: Xt = 芸 PN=I Xt.
5:	δ = ∆t" + kγ(Xt- Xt).
6:	Update local model Xt = Xt.
7:	for τ = t to t + k - 1 do
8:	Calculate a stochastic gradient Rfi(XZ,ξ[).
9:	Vir = Vfi(XT ,ξ) — ∆t0.
10:	Each worker updates its local model:
τ +1	τ τ
Xiτ+1 = Xiτ - γviτ .
11:	end for
12:	t = t + k.
13:	end while
One might wonder why VRL-SGD can improve the convergence rate of Local SGD. VRL-SGD uses
an inexact variance reduction technique to reduce the variance among workers. To better understand
the intuition of VRL-SGD, let us see the update of ∆i in equation (4). By summing up all ∆i from
0 to t0 and using the fact that ∆i0 = 0, we have
(7)
By summing UP the above equality over i = 1, ∙∙∙ ,N, we obtain
N	1 N b k C	1 / b k C	N b k C	∖
X∆i0 = kγXX (Xks -Xks) = kγ (NXXks -XXXks)=0.
i=1	γ i=1 s=0	γ	s=0	i=1 s=0
It shows that the expectation of ∆it0 over i is zero, thus we can obtain the new update form with
respect to Xt.
NN	N
Xt= XtT-YN X * 1 * * * V ViiT= XtT- YN X (vfi(χt,ξt) - Nt) = XtT- YN X Vfi(χt,ξt). (8)
i=1	i=1	i=1
It can be noticed that the update of Xt in equation (8) is in the form of the generalized stochastic
gradient descent. In addition, we can obtain a new representation of ∆it0 as below:
t0-1 N	t0-1
∆i0	= ∆i00 + kγ ∣Xt00 - γ X ɪ X Vj - Xt0 + γ X vτ)
τ =t00	j=1	τ =t00
/ t0-1	t0-1	N	∖
=△，+ 记(Y X (Vfi(XT ,ξj) - δi ) -γ X N X (Vfj (XT ,ξj) - △;))
Y τ =t00	τ =t00	j=1
1 t0-1	1 N
=k X	Vfi(Xj ,ξj) - N X Vfj (XT ,ξj).	⑼
T=t00	j=1
Substituting equation (9) into equation (6), we have
1 t0-1	1 t0-1 N
Vi = Vfi(Xi,ξi) - k X Vfi(Xj,ξj) + Nk XX
Vfj (XjT, ξjT).	(10)
T =t00	T =t00 j=1
The representation of vit in equation (10) can be regarded as the form of the generalized variance
reduction, which is similar to SVRG (Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014).
5
Under review as a conference paper at ICLR 2020
To observe that the variance among workers is reduced, we assume that the gradient variance within
each worker is zero, which means that We calculate Vfi(xt) in line 8 of Algorithm 1. When all
local model Xt, xT and the average model Xt converge to the local minimum x*,it holds that
t0-1	t0-1 N
Vt	= Wi(Xi) - k X Vfi(xT) + Nk XX Vfj (Xj)
τ =t00	τ =t00 j=1
t0-1	t0-1 N
→ Vfi(X*) - k X Vfi(X*) + Nk XX Vfj (X*)
τ =t00	τ =t00 j=1
t0 -1 N
→ NXXVfj(x*) →Vf(x*) → 0.	(11)
τ=t00 j=1
Therefore, vit can converge to zero when the variance within each worker is zero, which helps VRL-
SGD converge faster. On the other hand, the gradient Vfi(xit, ξit) in Local SGD cannot converge to
zero, which prevents the local model XT from converging to the local minimum x*, so it is hard to
converge for Local SGD. In summary, that is why VRL-SGD performs better than Local SGD for
the non-identical case, where the gradient variance among workers is not zero.
5 Theoretical Analysis
In this section, we provide a theoretical analysis of VRL-SGD. We bound the expected squared
gradient norm of the average model, which is the commonly used metric to prove the convergence
rate for non-convex problems (Ghadimi & Lan, 2013; Tang et al., 2018; Yu et al., 2019a).
Theorem 5.1 Under Assumption 1, if the learning rate satisfies Y ≤ 吉 and 72k2γ2L2 ≤ 1, we
have the following convergence result for VRL-SGD in Algorithm 1:
1 X EkVf(Xt)k2 ≤ 3(f (X0； — f*)+ 3γLσ2 +56kγ2σ2L2 + 12苧C,
T	Tγ	2N	T
t=0	γ
where C is defined as
k-1 N
C
t=0 i=1
t-1
X (Vfi(XT) -Vf (Xτ))
T=0
2
(12)
The proof of Theorem 5.1 is given in Appendix C. Note that C will be 0 if k = 1 according to
equation (12). It is consistent with the fact that VRL-SGD when k = 1 is equivalent to S-SGD,
where the convergence of S-SGD is not related to the variance among workers.
By setting a suitable learning rate γ, we have the following corollary.
Corollary 5.2 Under Assumption 1, when the learning rate is set as Y = j√T, the communication
period is set as k =O(T2 /N3) and the total number ofiterations satisfies T ≥ 72N:*?, we have
the following convergence result for Algorithm 1:
T -1
XE∣∣Vf(Xt)∣∣ ≤
t=0
1
T
3σ(f (X0) - f * + 3L)	12NC
√NT	+ σ2T2
where C is defined in Theorem 5.1.
The detailed proof of Corollary 5.2 is given in Appendix D.
Remark 5.3 Warm-up. We can set the first communication period k to 1 in VRL-SGD, which
is VRL-SGD with a warm-up (VRL-SGD-W), then the variable C in Theorem 5.1 and Corollary
5.2 will be 0. Essentially, this is equivalent to conduct one S-SGD update and initialize ∆i =
Vfi(X0, ξ0) — N PjN=I Vfj(X0, ξ0). Therefore, the convergence result is not related to the extent
of non-iid. We conduct additional experiments to verify this conclusion in Appendix E.
6
Under review as a conference paper at ICLR 2020
Remark 5.4 Consistent with D2. In D2 (Tang et al., 2018), the convergence rate is O( √Nt +
T++272), where Z0 represents the extent of non-iid in the first iteration. While the convergence rate
in VRL-SGD is O( √Nt + σCT2), where C is similar to Z2. However, we can reduce the dependence
on C by a warm-up, which leads to a tighter convergence rate.
Remark 5.5 Linear Speedup. For non-convex optimization, if there are N workers training a
model collaboratively, according to Corollary 5.2, VRL-SGD converges at the rate OQNNrT),
which is consistent with S-SGD and Local SGD. To achieve E-optimal solutuioin,O(N12) iterations
are needed. Thus, VRL-SGD has a linear iteration speedup with respect to the number of workers.
Remark 5.6 Communication Complexity. By Corollary 5.2, to achieve the convergence rate
OQzNT), the number ofiterations T needs to satisfy T ≥ O(N3k2), which requires the commu-
13	13
nιcatιon period k ≤ O(T 2 /N 2). Consequently, by setting k =O(T 2 /N 2), VRL-SGD can reduce
communication complexity by a factor k. However, for the non-identical case, previous algorithms
13
based on Local SGD can only reduce communication complexity by afactor O(T 4 /N 4).
Remark 5.7 Mini-batch VRL-SGD. Although we consider only a single stochastic gradient in
each worker so far, VRL-SGD can calculate mini-batch gradients with size b in line 8 of Algorithm
1. It reduces the variance σ2 within each worker by afactor b, thus VRL-SGD can converge at the
rate O(1∕√bNT) by setting the learning rate Y =意N.
6 Experiments
6.1	Experimental Settings
Experimental Environment We implement algorithms with Pytorch 1.1 (Paszke et al., 2017).
And we use a machine with 8 Nvidia Geforce GTX 1080Ti GPUs, 2 Xeon(R) E5-2620 cores and
256 GB RAM Memory. Each GPU is regarded as one worker in experiments.
Baselines We compare the proposed algorithm VRL-SGD with Local SGD (Stich, 2019), EASGD
(Zhang et al., 2015) and S-SGD (Ghadimi & Lan, 2013).
Data Partitioning To validate the effectiveness of VRL-SGD in various scenarios, we consider
two cases: the non-identical case and the identical case. In the non-identical case, each worker can
only access a subset of data. For example, when 5 workers are used to train a model on 10 classes
of data, each worker can only access to two classes of data. In the identical case, we allow each
worker to access all data.
Datasets and Models We consider three typical tasks: (1) LeNet (El-Sawy et al., 2016) on MNIST
(LeCun, 1998); (2) TextCNN (Kim, 2014) on DBPedia (Lehmann et al., 2015); (3) transfer learning
on tiny ImageNet 1, which is a subset of the ImageNet dataset (Deng et al., 2009). When training
TextCNN on DBPedia, we retain the first 50 words and use a GloVe (Pennington et al., 2014) pre-
trained model to extract 50 features for word representation. In transfer learning, we use an Inception
V3 (Szegedy et al., 2016) pre-trained model as the feature extractor to extract 2,048 features for each
image. Then we train a multilayer perceptron with one fully-connected hidden layer of 1,024 nodes,
200 output nodes, and relu activation. All datasets are summarized in Table 2. A lot of deep learning
models use batch normalization (Ioffe & Szegedy, 2015), which assumes that the mini-batches are
sampled from the same distribution. Applying batch normalization directly to the non-identical case
may lead to some other issues, which is beyond the scope of this paper.
Hyper-parameters For the above three different tasks, we set the weight decay to be 10-4. And
we initialize model weights by performing 2 epoch SGD iterations in all experiments. Other detailed
hyper-parameters can be found in Table 2.
1The tiny ImageNet dataset can be downloaded from https://tiny-imagenet.herokuapp.com.
7
Under review as a conference paper at ICLR 2020
Table 2: Parameters used in experiments and a summary of datasets. N denotes the number of
workers, b denotes batch size on each worker, γ is the learning rate, k is the communication period,
n represents the number of data samples and m represents the number of data categories.
Model	N	b	Y	k	Dataset	n	m
LeNet	ɪ		0.005	^20^	MnIst	60,000	10
TextCNN	ɪ	^6^	0.01	~00~	DBPedia 一	560,000	14
Transfer Learning	ɪ	^3F	0.025	^20^	Tiny ImageNet	100,000	200
Metrics In this paper, we mainly focus on the convergence rate of different algorithms. Local SGD
has a more superior training speed performance than S-SGD, which has been empirically observed
in various machine learning tasks (Povey et al., 2014; Su & Chen, 2015). Besides, VRL-SGD has
only a minor change over Local SGD. So VRL-SGD and Local SGD have the same training time
in one epoch and both of them have a faster training speed compared with S-SGD. VRL-SGD and
EASGD would have the same communication complexity under the same period k. Therefore, we
compare only the convergence rate (the training loss with regard to epochs) of different algorithms.
Figure 1: Epoch loss for the non-identical case. VRL-SGD converges as fast as S-SGD, and Local
SGD, EASGD converge slowly or even cannot converge.
Figure 2: Epoch loss for the identical case. All of the algorithms have a similar convergence rate.
6.2	Non-identical case
This paper seeks to address the problem of poor convergence for Local SGD when the variance
among workers is high. Therefore, we focus on comparing the convergence rate of all algorithms in
the non-identical case, where the data variance among workers is maximized.
We choose three classical tasks: image classification, text classification, and transfer learning. Fig-
ure 1 shows the training loss with regard to epochs on the three tasks. The results are indicative
of the strength of VRL-SGD in the non-identical case. Local SGD converges slowly compared
with S-SGD when the communication period k is relatively large, while VRL-SGD enjoys the same
convergence rate as that of S-SGD. This is consistent with theoretical analysis that VRL-SGD has
8
Under review as a conference paper at ICLR 2020
a better communication period bound compared to Local SGD. When the variance among work-
ers is not zero, Local SGD requires that T is greater than O(N 3k4) to achieve a linear iteration
speedup. Thus Local SGD losses this property if k is larger than O(T4 /N3). However, benefiting
from eliminating the dependency on the gradient variance among workers, VRL-SGD can attain a
13
better communication period bound O(T2 /N2) than Local SGD as shown m Corollary 5.2. There-
fore, under the same communication period, VRL-SGD can achieve a linear iteration speedup and
converges much faster than Local SGD. To maintain the same convergence rate, Local SGD needs
to set a smaller communication period, which will result in higher communication cost. EASGD
converges the worst under the same communication period in the non-identical case.
There are more experimental results to analyze the influence of parameter k in Appendix F.
6.3	Identical case
In addition to the above extreme case, we also validate the effectiveness of VRL-SGD in the identical
case. As shown in Figure 2, all algorithms have a similar convergence rate. VRL-SGD, EASGD
and Local SGD converge as fast as S-SGD when workers can observe unbiased stochastic gradients.
7 Conclusion & Future Work
In this paper, we propose a novel distributed algorithm VRL-SGD for accelerating the training of
machine learning models. VRL-SGD incorporates the variance reduction technique into Local SGD
to further reduce the communication complexity. We theoretically prove that VRL-SGD can achieve
a linear iteration speedup for nonconvex functions with the optimal communication complexity
13
O(T 2 N 2) whether each worker accesses identical data or not. Experimental results verify the
effectiveness of VRL-SGD, where VRL-SGD is significantly better than traditional Local SGD for
the non-identical case and enjoys the same convergence rate as that of Local SGD.
In the future, we will consider the deep learning models with batch normalization layers, which may
lead to an unstable convergence in the non-identical case.
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.
440-445, 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural
Information Processing Systems, pp. 1709-1720, 2017.
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD with
majority vote is communication efficient and fault tolerant. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=BJxhijAcY7.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction
using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
Ahmed El-Sawy, EL-Bakry Hazem, and Mohamed Loey. Cnn for handwritten arabic digits recog-
nition based on lenet-5. In International Conference on Advanced Intelligent Systems and Infor-
matics, pp. 566-575. Springer, 2016.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
9
Under review as a conference paper at ICLR 2020
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Trading
redundancy for communication: Speeding up distributed sgd for non-convex optimization. In
ICML,pp. 2545-2554, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes
signsgd and other gradient compression schemes. In ICML, pp. 3252-3261, 2019.
Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746-1751,
2014.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Soren Auer, et al. Dbpedia-a large-
scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6(2):167-195, 2015.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018a.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing
the communication bandwidth for distributed training. In International Conference on Learning
Representations, 2018b. URL https://openreview.net/forum?id=SkhQHMW0W.
Aryan Mokhtari and Alejandro Ribeiro. Dsa: Decentralized double stochastic averaging gradient
algorithm. The Journal of Machine Learning Research, 17(1):2165-2199, 2016.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In ICML, pp. 2613-2621, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543,
2014. URL http://www.aclweb.org/anthology/D14-1162.
Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of dnns with natural gradi-
ent and parameter averaging. arXiv preprint arXiv:1410.7455, 2014.
Shuheng Shen, Linli Xu, Jingchang Liu, Xianfeng Liang, and Yifei Cheng. Faster distributed deep
net training: Computation and communication decoupled stochastic gradient descent. In IJCAI,
2019.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentral-
ized consensus optimization. SIAM Journal on Optimization, 25(2):944-966, 2015.
Sebastian Urban Stich. Local sgd converges fast and communicates little. In ICLR 2019 ICLR 2019
International Conference on Learning Representations, number CONF, 2019.
Hang Su and Haoyu Chen. Experiments on parallel training of deep neural network using model
averaging. arXiv preprint arXiv:1507.01239, 2015.
10
Under review as a conference paper at ICLR 2020
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over
decentralized data. In ICML, pp. 4855-4863, 2018.
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. Doublesqueeze: Parallel stochastic
gradient descent with double-pass error-compensated compression. In ICML, pp. 6155-6165,
2019.
Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient mo-
mentum sgd for distributed non-convex optimization. In ICML, pp. 7184-7193, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019b.
Sixin Zhang, Anna Choromanska, and Yann LeCun. Deep learning with elastic averag-
ing SGD. In Advances in Neural Information Processing Systems 28: Annual Con-
ference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 685-693, 2015. URL http://papers.nips.cc/paper/
5761- deep- learning- with- elastic- averaging- sgd.
Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradient
descent algorithm for nonconvex optimization. In Proceedings of the 27th International Joint
Conference on Artificial Intelligence, pp. 3219-3227. AAAI Press, 2018.
11
Under review as a conference paper at ICLR 2020
A Proof of Partially Accumulated Local Gradients
In this section, we present Lemma 1 and Lemma 2 to bound the partially accumulated local gradi-
ents, which are defined as
t = ʃ	vf(xt,ξt),	t<k	,13
Vi = I	Vf (χt,ξt) +1PT二0(寺 Pj= Vfj (Xj0 ,ξτ0)-Vfi(Xτ0,ξτ0)).	t ≥ k	(3)
Lemma 1 Under Assumption 1, we have the following inequality for t ≥ k
N	t-1	2 N t-1	t-1 t0 -1	t0 -1
N X Ek X vTk2 ≤ 12L X k X EkxT - xτk2 +2 X X EkxT - xτ0k2 +2k X EkXτ0-xT0k2
i=1	τ=t0	i=1	τ=t0	τ=t0 τ0 =t00	τ0 =t00
t0-1
+12k X ∣∣Vf(xτ0)k2 + 18kσ2. (14)
T0=t00
Proof. By the definition of vit in (13), we have
N
t-1
2
-
⅛ X E X VT
i=1
T=t0
+
1N
ɪ X E
N乙
i=1
1N
ɪ X E
N乙
i=1
t-1 /	1 t0-1 1 1 N
X	Vfi(xτ,ξi) + - X IN X Vfj(xτ',ξτ')-Vfi(W)
T=t0	T0 =t00	j=1
t-1 /	1 t0-1 - 1 N
X	Vfi(xiτ ,ξi ) - Vfi(xτ) + - X	N∙ X (Vfj (xτ0 ,ξT0 ) -Vfj (xτ0 ))
T=t0	T0 =t00	j=1
+Vfi(xiT0) -Vfi(xiT0,ξiT0)	+ Vfi(xiT) +-Σ	NX X Vfj (xτ0) - Vfi(xiT0)
T0=t00	j=1
≤
i=1
I
2N
⅛ X E
N
i=1
2
t-1 /	1 t0-1 1 1 N
X I Vfi(xτ,ξi) - Vfi(xτ) + - X N X (Vfj(xτ0,ξT0) -Vfj(xτ0))
T=t0	T0=t00	j=1
+ Vfi(xiT0) - Vfi(xiT0,ξiT0)2
______________________________________ - /
T1
t-1	/	] t0-1 / ] N
X	Vfi(xT) +1 X	NXXVfj(xτ0)-Vfi(xτ0)
T=t0	T0=t00	j=1
2
T2
(15)
where the inequality follows from Cauchy’s inequality. We next bound T1 as
T1
≤
t-1	2
3EX (Vfi(xiT, ξiT) - Vfi(xiT))	+3E
T=t0
1------------------{------------------} J
T3
(t-t) X (Vfi(xτ0) - Vfi(xT0,ξT0))
T0=t00
T4
(t	t0)	t0-1	1 N	0	0	0
+3 E	L-~1	X N X	(Vfj	(xτ	,ξj) - Vfj	(xτ))
T0=t00 j=1
'----------------------------{------------------------
T5
(16)
12
Under review as a conference paper at ICLR 2020
Because ξit ’s are independent at different time and workers, and the variance of stochastic gradient
in each worker is bounded by σ2 , we can bound T3 , T4 and T5 as
T3
T4
t-1
XEkVfi(Xij,ξij)-Vfi(Xij)k2
j=t0
+2 X EhVfi(Xij1,ξij1)-Vfi(Xij1),Vfi(Xij2,ξij2)-Vfi(Xij2)i
t0 ≤j1 <j2 ≤t-1
t-1
XEkVfi(Xij,ξij)-Vfi(Xij)k2
j=t0
(t- t0)σ 2
kσ2,
(t-pɪ ( X E Il Vfi(xj0) -Vfi(XT0,ξT0)∣∣2
j0=t00
(17)
≤
≤
≤
+2 X	EDVfi(Xij10)-Vfi(Xij10,ξij10),Vfi(Xij20)-Vfi(Xij20,ξij
t00 ≤j10 <j20 ≤t0 -1
0 2 t -1	∣	0	0	0 ∣2
(k2 ) X E∣∣Vfi(xj ) - Vfi(XT ,ξj )∣∣
j0=t00
kσ2,
(18)
2
T5
(t- t0)2
N 2k2
t0-1 N
E XXWj (Xj 0,ξT0) -Vfj (Xj j)
j0=t00 j=1
工(XI E
j0=t00
00
,ξjj0)-Vfj(Xjj0)
+2
t00 ≤j10 <j20 ≤t
(t-10)2
N 2k2
t0-1
E XN	Vfj (Xjj10, ξjj10) -
N
N
1)) ,XVfj
j=1
2
j0=t00
(t-10)2	X
N 2k2	乙
j0=t00
X EXVfj(Xjj0,ξjj0)-Vfj(Xjj0))
j=1
EVfj(Xjj0,ξjj0) -Vfj(Xjj0)
00
2
0
0	2
2川
≤
+2 X	DVfj1 (Xjj10, ξjj10)	-	Vfj1	(Xjj10),	Vfj2	(Xjj20,	ξjj20)	- Vfj2 (Xjj20)E
1≤j1<j2≤N
0 2 t0 -1 N	2
⅛2L X XEIlVfj(XTW) -Vfj(χj0)∣∣
j0=t00 j=1
kσ2
ɪ.
Substituting (17), (18) and (19) into (16), we have
T1 ≤ 3(T3 + T4 + T5) ≤ 9kσ2.
(19)
(20)
13
Under review as a conference paper at ICLR 2020
We next bound T2 as
T2
E
E
t-1 /	1 t0-1 1 1 N	∖
X	Wi(χT) + k X N X▽力(吗')-Vfi(χτ,)
T = t ∖	T 0 = t00 ∖	j=1	)
t-1 /	1 t0-1
E	▽力(XT)- ▽力(XT)+ ▽力(XT)- k E ^fi(χτ')
T = tz ∖	T z = tzz
1	t0-1	1	t0-1	N
+k	X	«fi(XTj	-Wi(XT')) + Nk	X	X (▽力(XT0)	- ▽力(Xt'))
T' = t"	T' = t'' j =1
+k tX(V/(^τ') - Vf(^τ))+ Vf(^τ)
T =t
<
(t-1	2
6 E X (Vfi(xT) - Vfi(^τ)) + E
∖	T = t'
X (Vfi (^τ) - k tX Vfi (^τ 0)
T = tz ∖	T ' = t 0 0
+E
1 t-1 t0-1	2
k X X (Vfi(^τ0) -Vfi(XT0))	+ E
T = t' T0 = t00
t-1 t0-1 N
Nk X X X(Vfj (XT0) -Vfj (犷，
T =t0 T0=t00 j = 1
<
+E
1 t-1 t0-1	2
k X X (Vf(^τ0) -Vf(^τ))	+ E
T = t0 T0 = t00
6(t - t') X (EkVfi(XT) - Vfi(^τ)k2 + E
T =t0 \
t-1
X Vf (^τ)
T = t0
1 t0-1	0
Vfi(^τ) - k E Vfi(^τ')
T 0 =t'0
-
+E - X (Vfi(^τ0) - Vfi(xT0))	+ E
T 0 = t00
t0-1	N
Nk X X (Vfj(XT0) -Vfj(#))
T0 = t00 j = 1
+E
-X，(Vf(-τ0) -Vf( Bt))| I
+ E∣∣Vf(^τ )k2
<
t-1	t0-1	2
6(t- t') X EkVfi( XT) - Vfi( ^τ)k2 + k X E∣∣Vfi(^τ) -Vfi(^τ0)||
T =t0 ∖	T 0 =t00
1 t0-1	2	1	t0-1 N	2
+k X E∣∣Vfi(^τ 0) -Vfi (X T 0 )∣∣ + Nk X X EII Vfj (X T 0) -Vfj (^τ0 )∣∣ + EkVf(^τ )k2
T0 = t00	T0 = t00 j=1
<
<
t-1	t0-1	2	t0-1	2
6(t - t')L2 X E kXT - ^τk2 + k X EMT - ^τ0∣∣ + - X EMT0-XT0II
T = t0 ∖	T 0=t00	T 0 =t00
1 t0-1 N	2、	t-1
+Nk X XE∣∣XT0-^τ0∣∣	+6(t-t')XEkVf(^τ)k2
T0 = t00 j = 1	)	T=t0
(t-1	t-1 t0-1	2	t0-1	2
k X E k X T - ^τk2 +2 X X EIkT - ^τ 0l1 + k X EIkT 0 - xT 0∣∣
T = t0	T =t0 T0=t00	T0=t00
k t0-1 N	2、	t-1
+N X X E∣∣xT 0 - ^τ0∣∣	+6k X EkVf(^τ )k2 ,
T0 = t00 j = 1	)	T = t0
(21)
14
Under review as a conference paper at ICLR 2020
where the first three inequalities follow from Cauchy’s inequality, and the fourth inequality follows
from the Lipschitz gradient assumption. According to (21), we have
N	2 N	t-1	t-1 t0-1	2	t0-1
NN X T2	≤ -Nr Xlk	X EkxT-xτk2	+ 2 XX	EaT - xτ0∣∣+2k X	EaT0-χT0∣
i=1	i=1	τ=t0	τ=t0 τ0=t00	τ0=t00
t-1
+12k X E kVf (xτ)k2 ,
T=t0
Substituting (20 ), (22) into (15), we obtain Lemma 1.
(22)
Lemma 2 Under Assumption 1, we have the following inequality for t < k,
N
i=1
t-1
T=t0
2 N t-1	t-1
≤ 4kNL- XX EkxT - xτk2 + 4k X kVf (xτ )k2 + 4kσ2
i=1 T=t0	T=t0
N
+]4 X
i=1
t-1	2
X Efi(XT) - Vf (xτ))
T=t0
(23)
-
N X E X vτ
Proof. By the definition of vit, t < k in (13), we have
1N
—X E
NJ
i=1
1N
—X E
NJ
i=1
1N
—X E
NJ
i=1
t-1
X viT
T=t0
t-1
Vfi(XiT,ξiT)
T=t0
t-1
X ((Vfi (XiT ,ξT) - Vfi(XT)) + (Vfi (XT) - Vfi(XT))
T=t0
+ (Vfi(XT) - Vf (Xt)) + Vf (Xt))k2
2
2
2
2
≤
≤
≤
N
事X (e
t-1
t-1
t-1
E(Vfi(XiT,ξiτ) -Vfi(χτ))	+ E E(Vfi(XT)-Vfi(Xτ))
T=t0
t-1
T=t0
2
+E E(Vfi(XT)-Vf(Xτ))	+ E EVf(XT)
T=t0
事X(
t-1
T=t0
t-1
kσ2 + k X EkVfi(XT) - Vfi(XT)k2 + E
T=t0
+k∑ E kVf (Xt)k2
T=t0
N	t-1
N X(kσ2 + kL2 X EkXT
i=1
t-1
T=t0
+k∑ EkVf(XT)k2	,
T=t0
t-1
t-1
X (Vfi(XT) — Vf (Xt))
T=t0
-Xτk2 + E E(Vfi(XT) - Vf (Xτ))
T=t0
(24)
2
2
2
where the second inequalities can be obtained by using (17) again. Rerrangeing the inequality, we
obtain Lemma 2.
B	Proof of Lemma 3
In this section, we introduce Lemma 3, which bounds the difference between the local model Xit and
the average model Xt.
15
Under review as a conference paper at ICLR 2020
Lemma 3 Under Lemma 1 and Lemma 2 , when the learning rate γ and the communication period
k satisfy that 72γ2 k2 L2 ≤ 1, we have the following inequality
T-1 N	T-1
NnXXEkxi-xtk2 ≤ 1≡1⅛X皿(xt)k2 + 1-S⅛
t=0 i=1	t=0
18kγ2σ2T	4γ2C
11 - 36k2Y2L2 + 1 - 36k2γ2L2，
T-1 t0-1
XX E忖-叫
t=k τ0=t00
(25)
where
t=0 i=1
t-1	2
X (Vfi(xτ) - Vf (xτ))
τ=0
(26)
Proof. According to the updating scheme in Algorithms 1, xit can be represented as
t-1
t	t0	τ
xi = x - Y ʌ Vi ,
τ=t0
(27)
On the other hand, by the definition of xt, We can represent it as
N t-1
xt0-N XX vτ
i=1 τ=t0
(28)
Substituting (27) and (28) into the left hand side of (25) , We have
N
N X Enxt-明2
i=1
N	t-1 N
NnXEJ xt0-⅛XXVT
i=1	τ=t0 j=1
2
t-1
t0	τ
x - 2_^ YVi
τ=t0
1N
ɪ X E
N
i=1
t-1
X Yviτ
τ=t0
t-1 N
-NN xX vτ
τ=t0 j=1
1N
-X E
N乙
i=1
t-1
X Yviτ
τ=t0
t-1 N
-
NXE NXXVT
i=1
1N
ɪ X E
N
i=1
t-1
X Yviτ
τ=t0
τ=t0 j=1
2
N1
-2 X -1E
N
i=1
τ
Yvi
+E
t-1	N
X Y X N vτ
τ=t0	j=1
- 2E
1N
ɪ X E
N乙
i=1
t-1
X Yviτ
τ=t0
-E
t-1 N
⅛ XXVfj(xiτ,ξjτ)
τ=t0 j=1
t-1 N
X Y X N Vj
τ=t0 j=1
2
1N
ɪ X E
N乙
i=1
t-1
X Yviτ
τ=t0
t-1 N
N XX VT)
T=t0 j=1
(29)
≤
2
2
2
2
2
N
2
1
+
2
According to the result in Lemma 1 and Lemma 2, for t ≥ k, We have
N	1o 2τ2 N / t-1	t-1	t0-1
NχEIIxt-xij2	≤	12YNLχ kχEkxT-xτk2+2χ	χ EkxT-xτ0k2
i=1	i=1	T=t0	T=t0 T0=t00
t0-1	ʌ	t0-1
+2k X EkxT - xT ∣∣2 I + 12kY2 X ∣∣Vf(xT )k2+ 18kY2σ2,(30)
T0=t00	T0=t00
and for t < k, We have
i=1
≤
2 2 N t-1	t-1
4kNL XXEkxT- xTk2 +4kY2 X kVf(xT)k2
i=1 T=0	T=0
2N
+4kY σ + ɪ X
i=1
t-1	I2
X EMxT) - Vf (xt))
T=t0	I
(31)
16
Under review as a conference paper at ICLR 2020
Summing up (30) and (31) from t = 0 to T - 1, we obtain
T-1 N
NN XX Enxt-叫2
t=0 i=1
≤
2	2 T-1	N	t-1	t-1	t0-1	t0-1
12γNL- XX	k X EkxT	-	xτk2 +2 X X	EkxT	-	xτ0k2 +2k X	EkxT0-xT0k2
t=k	i=1	τ=t0	τ=t0	τ0=t00	τ0=t00
T-1 t-1	2 2 k-1 N t-1
+12kγ2X X kVf (xτ )k2 + 18kγ2σ2(T - k) + 4kγNL- XXX EkxT - xτk2
t=k T=t0	t=0 i=1 T=t0
k-1 t-1
+4kγ 2 XX
kVf(xτ )k2 +4k2γ2σ2 +
t=0 T=t0
k-1 N
2 k-1 N t-1
*XX X (Vfi(xτ) - Vf (xτ))
t=0 i=1
t-1
T=t0
2
≤
2	2 T-1 N	T-1 t-1 t0-1
12NL- XX 3k2Ekxi - xtk2 + 24γ2L2 XX X EkxT - xτ0k2
t=0 i=1	t=k T=t0 T0=t00
-	2 - n-
+12k2γ2 E ∣∣Vf(xt)k2 + 18kγ2σ2T + 4N-	nn
(Vfi(xT) - Vf (xT))
T-1
t=0
k-1 N
t=0 i=1
t-1
T=t0
2
≤
2 2	2 T-1 N	T-1 t0-1
^lNL XXEkxt- xtk2 +24kγ2L2 X X Ekxt - xT0k2
t=0 i=1	t=k T0=t00
T-1
k-1 N
-	2 -	t-
+12k2γ2 X ∣∣Vf(xt)k2 + 18kγ2σ2T + 4-N- XXnnX
(Vfi(xτ) - Vf(xT))
t=0
t=0 i=1
t-1
T=t0
(32)
2
where the second and the third inequalities can be obtained by using a simple counting argument.
Denote C = N Pk-(I PN=1 ∣∣pT=0 (Vfi(XT)- Vf(XT))∣∣ . Rerrangeing the inequality, We ob-
tain
T-1 N
(1 - 36k2γ2L2) N XX Ekxt- xtk2
t=0 i=1
T-1	T-1 t0-1
≤ 12k2γ2 X kVf (xt)k2 +24kγ2L2 X X Ekxt-xT 0 k2
t=0	t=0 T0=t00
+18kγ2σ2T+4γ2C.	(33)
Dividing 1 - 36k2γ2L2 on both sides completes the proof.
C Proof of Theorem 5.1
In this section, We give the proof of Theorem 5.1.
Theorem 5.1 Under Assumption 1, if the learning rate satisfies Y ≤ 吉 and 72k2γ2L2 ≤ 1, we
have the following convergence result for Algorithm 1:
1 T-1
T EEkVf(xt)k2 ≤
t=0
3(f(x0) - f *) + 3γLσ
+ 2N
Tγ
-+56kγ2σ2L2 + 12γTL2c
(34)
Proof. Since fi(∙), i = 1, 2,∙∙∙,N are L-Smooth, it is easy to verify that f (∙) is L-Smooth. We
have
f(xt+ι) ≤ f(xt) + (Vf(xt),xt+1 - xt) + 2∣∣xt+1
= f(xt) - Y *Vf(Xt), NF X V + + 与
=f(xt) -Y kf(xt),⅛∙X Vfi(xt,ξt),
2
—
2
1N
N X vi
i=1
+亨
2
.	(35)
17
Under review as a conference paper at ICLR 2020
By applying expectation with respect to all the random variables at step t and conditional on the past
(denote by 旧讣),We have
≤
Et∣∙f (Xt+ι)
f(Xt) - Y (
N2
Vf (xt),NN X Vfi(xi) ∖ + LY-
i=1
∙Et∣.
f (Xt) - 2
Wf(Xt)『+ NN X vfi (Xt)
+Lγ2 Et∣∙
i=1
2
1N
Vf (Xt) - IN X Vfi(Xt)
i=1
(36)
N
2
—
2
Note that
Et∣∙
Et∣∙
Et∣∙
1 N	2
N X Vfi(Xt,ξt)
i=1
1N	1N	1N
河 X Vfi(Xt,ξt) - N X Vfi(Xt) + 河 X Vfi(Xt)
i=1	i=1	i=1
1N	1N
IN X Vfi(Xt,ξt) - N X Vfi(Xt)
i=1	i=1
1 N	2
NN X Vfi(Xi)
i=1
2
+
1N
+2Et∣,(河 ∑Vfi(Xt,ξt)-
i=1
N
1N	1N
R X Vfi (心,N X Vfi (Xt)
i=1
2
i=1
Et∣∙
N X Vfi(Xt,ξt)- N X Vfi(Xi)	+
i=1
i=1
N X Vfi(Xi),
i=1
(37)
N
N
+
2
where the last equality holds because 旧讣
(含 ∑i=1 Vfi(xt,ξt) - NN ∑i=1 Vfi(Xt))
= 0, and
N
2
Etl∙ N
i=1
N
1N
N X Vfi(Xi)
i=1
2
+N2	E	Et∣<Vfiι (Xiι ,ξiι) - Vfii (Xiι), Vfi2 (Xt2 ,ξt2) -Vfi2 (Xt2)〉
1≤i1 <i2 ≤N
1N
Et∣∙N2 XnVfi(Xi,ξt) -Vfi(Xi)II2
i=1
(38)
where the second equality holds because the random variables on different workers are independent.
Substituting (37) into (36) and applying expectation with respect to all the random variables, we
obtain
I 1N	I2
Ef(Xt+1) ≤ Ef (Xt) - 2 EkVf(Xt)k2 - Y(1 - LY)E N X Vfi(Xi)
I	i=1	I
I	N	I2	2	2
+2E Vf(Xt) - N X Vfi(Xi) + γ2Nr-
I	i=1	I
(39)
≤ N，
18
Under review as a conference paper at ICLR 2020
We then bound the difference of Nf(Xt) and N PN=I Vfi(xi) as
I	1N
E Vf (Xt) - N EVfi(Xt)
I	i=1
2
E
1N
nn E(Vfi(Xt) -Vfi(Xi))
2
≤
≤
i=1
1N
N EEIl(Vfi(Xt) -Vfi(Xi)川2
i=1
2N
NXeIIx -XiH ,
i=1
(40)
where the two inequalities follow from Cauchy’s inequality and Lipschitz gradient assumption, re-
spectively. Substituting (40) into (39) yields
Ef (Xt+ι) ≤ Ef(Xt)-
2EkVf (xt)k2 - 2(1 - LY)E N X Wi(Xi)
i=1
N
2
i=1
t t∣∣2, γ2Lσ2
-Xik +^T
(41)
Rearranging the inequality and summing up both sides from t = 0 to T - 1, we have
X I YEkVf(Xt)k2 + 2(1 - LY)E N X Vfi(Xt)
T-1
t=0
i=1
N
2
2 N T-1	2	2
≤	f(xo) - f* + YNXXEkXt-Xik2 + -γ2Nσ-
i=1 t=0
(42)
Substituting Lemma 3 into (42) and combing 72k2γ2L2 ≤ 1, we obtain
≤
≤
2
X I YEkVf(Xt)k2 + 2(1 - LY)E N X Vfi(Xt)
T-1
t=0
…、产,Ty2 Lσ
f (X0) - f +^N-
i=1
2_ q	6k2Y3L2	T-ζv/八 A 9kY3σ2L2T
+ 1 - 36k2Y2L2 Σ kVf(X )k + 1 - 36k2L2Y2
2y3L2C	12kY3L4
11 — 36k2Y2L2 + 1 — 36k2Y2L2
卡•「、 ʃ* , TYLσ
f (Xo) - f +	2N
T-1 t0-1
XX E k- XT
t=k τ0=t00
T-1
02
+ 12k2Y3L2 E IIVf(Xt)k2 + 18kY3σ2L2T
t=0
N
T-1 t0-1
+4γ3L2C + 24kγ3L4 X X E∣∣Χt- XT
t=k τ0=t00
'---------{z-------
T6
02
✓
(43)
19
Under review as a conference paper at ICLR 2020
Next, we bound T6 .
T6
T-1 t0-1
XX E Ilxt-
t=k τ0=t00
2
T-1 t0-1
t-1
XX E X 得 X Vs
t=k τ0=t00
T-1 t0-1
τ0	i=1
t-1 N
t-1 N
NXXEXX(Vfi(xs,ξs) -Vfi (xs)) + XX
Vfi(xis)
t=0 τ0 =t00
τ0 i=1
τ0 i=1
2 T-1 t0-1
NXX (e
t=k τ0 =t00
t-1 N
t-1 N
ΣΣ(Vfi(xis,ξis) -Vfi(xis))II +EII
Vfi(xis)
τ0 i=1
τ0 i=1
N
2
2
2
2
t-1 N	t-1 N
+2E XX
(Vfi(xis,ξis)-Vfi(xis)), XX
Vfi(xis)
2
2
T-1 t0-1
t-1 N
T-1 t0-1
t-1 N
NX XE XX(Vfi(xs,ξs)-Vfi(xs))	+ NX XE XXVfi(Xs).
t=k τ0 =t00
|
τ0 i=1
{z
T7
t=k τ0 =t00
τ0 i=1
(44)
2
2
✓
Since ξit ’s are independent, we have
T7
X (E
s=τ0
N
X(Vfi(xis,ξis)-Vfi(xis))
i=1
2
+2
τ
t-1
NN
X E X(Vfi(xis1,ξis1)-Vfi(xis1)),X(Vfi(xis2,ξis2)-Vfi(xis2))
0 ≤s1 <s2 ≤t-1
IN
i=1
+)
EIII	(Vfi(xis,ξis) - Vfi(xis))
2
t-1 N
X XE kVfi(xis,ξis) - Vfi(xis)k2
s=τ0 i=1
+ 2 X	E hVfiι (xsι ,ξSι) -Vfii (xsι), Vfi2 (xs2 芯2 ) -Vfi2 (xs2 )i 1
1≤i1 <i2 ≤N
t-1 N
XXEkVfi(xis,ξis) -Vfi(xis)k2.	(45)
s=τ0 i=1
Substituting (45) into (44), we have
T6
2 T-1 t0-1 t-1 N	2 T-1 t0-1
NX X XXEkVfi(xs,ξs)-Vfi(xs)k2+NX XE
t=k τ0 =t00 s=τ0 i=1
t=k τ0=t00
t-1 N
XX
Vfi(xis)
2k2 Y 2σ2T
N
T-1 t0-1
+XXE
t=k τ0=t00
≤
t-1 N
N XXVfi(xis)
s=τ0 i=1
(46)
20
Under review as a conference paper at ICLR 2020
where the inequality holds since t - τ0 ≤ k ≤ t - t00 ≤ 2k. Substituting (46) into (43), we obtain
T-1
X
t=0
1 N 2
2 EkVf (xt)k2 + 2(1 - LY)E NN X Vfi(Xi)
i=1
≤
f(Xo) - f * + Tw' + 12k2γ3L2 X kVf(Xt)k2 + 18kγ3σ2L2T
2N	t=0
T-1 t0-1	2
+4γ3L2C + 24kγ3L4 X X EMt- Xτ0∣∣
t=k τ0=t00
≤
f(Xo) - f * + Tw' + 12k2γ3L2 X kVf(Xt)k2 + 18kγ3σ2L2T
2N	t=0
3 5 2 4	T-1 t0-1 ∣ t-1 N
+4Y3L2C + 48k γNσ L T + 24kY3L4 XX	NN XX Vfi(Xs)
t=k τ0=t00 ∣	s=τ0 i=1
(47)
Rearranging this inequality and dividing both sides by T2γ, we get
1X (1- 24k2γ2L2) EkVf(Xt)k2
T t=0
≤
2(f(X0)- f *) + YLσ2
+ N
Tγ
+ 36kγ2 σ2 L2 +
T-1
t0-1
t-1 N
8γ2L2 C	96k3γ4σ2L4
-T- +
∣2
T-1
r	-ɪ	-ɪ	"	-ɪ	"	-ɪ	, `	r -ɪ	-ɪ	r	, `
T X 48kY2L4 X N XX Vfi(Xs) - T X (1 - LY)ER X Vfi(Xt)
2
.(48)
t=k
τ0=t00
τ0 i=1
^^"{z
T8
t=0
i=1
N
N
+
✓
Then We prove T8 ≤ 0. If the Iearnign rate Y satisfies Y ≤ 克,then We have (1 - LY) ≥ 11.
T 1
t0 1
t1 N
2
T 1
N
T8	≤
-	--	-
2T(X 96kγ 4L4 X N XX Vfi(Xs) - X E N X Vfi(Xi)
2
t=k
τ0=t00
τ0 i=1
t=0
i=1
≤
(T-1 t0-1 t-1
192k2Y4L4 X X X
t=k τ0=t00 s=τ0
1N
nn X Vfi(Xs)
i=1
2 T-1
-XE
t=0
1N
nn X Vfi(Xt)
i=1
≤
2T ∣384k4Y4 L4 ∑
t=0
1N
nn X Vfi(Xi)
i=1
2 T-1
-XE
t=0
1 N	∣2
nn X Vfi(Xi)
i=1	∣
≤
384k4Y4 L4 - 1
2T
T-1
X
t=0
1N
nn X Vfi (Xt)
i=1
(49)
Since 72k2Y2L2 ≤ 1, then We have 384k4Y4L4
dividing both sides by 1 - 24k2Y2L2 , We get
≤ 1, and thus T8
≤ 0. Rearranging (48) and
1 T-1
T X EkVf(Xt)k2	≤
T t=0
2(f(X0) - f *)	γLσ2	36kγ2σ2L2
Tγ(1 - 24k2γ2L2) + N(1 - 24k2γ2L2) + 1 - 24k2γ2L2
8γ2L2C	96k3γ4σ2L4
+ T(1 - 24k2γ2L2) + N(1 - 24k2γ2L2)
≤
3(f(X0) - f *)
Tγ
+ 3YLσ2
+	2N
+ 54kγ2 σ2L2 + *C + 2k4
≤ 羽埠 -f*)+3γ祟 + 56kγ2σ2L2 + 与LC,
TY	2N	T
where the inequalities hold because k2γ2L2 ≤ 71 and 1-24k2γ2L2 ≤ 3.
21
Under review as a conference paper at ICLR 2020
D Proof of Corollary 5.2
In this section, we give the proof of Corollary 5.2.
Corollary 5.2 Under Assumption 1, when the learning rate is Set as Y = ∕√N and the total number
64N3L2k2
satisfies T ≥	'L k , we have thefollowιng convergence resultfor Algorithm 1:
1
T
T-1
XE∣∣vf(xt)∣∣ ≤
t=0
3σ(f(X0) - f * + 3L)	12NC
√NT	+ σ2T2 .
(51)
Proof. Since Y = -√√τ, T ≥	NLkk ≥ 72N∣2L2, We have 72γ2k2L2 ≤ 1 and Y ≤ 克.Then
we can have the result in (34) and get
1 X EkVf(Xt)k2 ≤ 3(f (嚷-f)+ 3γLσ! +56kγ2σ2L2 十 丝辛丝.
T	Tγ	2N	T
t=0	γ
(52)
Combing Y = √N=, k2γ2L2 ≤ 72 and T ≥ 72NNLkk ≥ 64N3L2k2, We have
σ T	72	σ	σ
56kγ2σ2 L2	≤ 56k F σ2L2 = -	σ2T	56kN L2 1	7σL 	=	≤ —. Tτ Tτ ~ Nnt	(53)
3γLσ2	3σL		(54)
2N	=2√NT,		
3(f(Xo)- f *)	=3σ(f(Xo)- f *)		(55)
TY	一	√NT	,	
12γ2 L2C	12NC		(56)
-T-	=~σ2T2.		
We can get the final result			
T X1 E∣∣Vf(xt)∣∣ ≤ 3σ(f(x√Nf* +3L)+1≡,	(57)
Which completes the proof.
22
Under review as a conference paper at ICLR 2020
E More Experiments
In this section, we evaluate the effectiveness of our algorithm on different variance among workers.
Specifically, we consider the following finite-sum optimization
minf(x) ：= 1(fι(x) + f2(x)) = 3x2 + 6b2,	(58)
x∈R	2
where f1(x) := (x + 2b)2 and f2(x) := 2(x - b)2 respectively denote the local loss function of the
first and the second worker.
We can set a large variance among workers by adjusting b. Therefore, we can compare the con-
vergence rate of algorithms in different variance, where the variance among workers is large with
a large b. VRL-SGD-W denotes VRL-SGD with a warm-up, where the first communication period
is set to 1. Figure 3 shows the gap with regard to iteration on different k and b. We can see that
Local SGD converges slowly compared with VRL-SGD-W and VRL-SGD when the communication
period k is relatively large. And VRL-SGD without warm-up is related to b while VRL-SGD-W is
not sensitive to b. Figure 4 shows that the variance of vit in VRL-SGD and VRL-SGD-W converges
to 0, while the variance of Vfi(x) in Local SGD is a constant related to b. The experimental results
verify our conclusion that VRL-SGD has a better convergence rate compared with Local SGD in
the non-identical case, and VRL-SGD with a warm-up is more robustness to the variance among
workers.
b=iθ, k=l	b=10f k=4	b=10, k=16	b=10, k=64
0 5 0 5 0
-112
- - -
*x.xol
=*X.X=Z6OI
0 5 0 5
112 2
- - - -
*x.xol
♦ Local SGD
-∙- VRL-SGD-W
VRL-SGD
O IOO 200	300	400	500
b=100f k=l
0 5 0 5 0
-112
- - -
*x.xol
=*X.X=Z6OI
O IOO 200	300	400	500
b=100f k=4
=*X.X=Z6OI
O IOO 200	300	400	500
b=100f k=16
O IOO 200	300	400
b=100f k=64
♦ Local SGD
-∙- VRL-SGD-W
VRL-SGD
0	100 200 300 400 500
b=1000f k=l
6	100 200 300 400 500
b=1000f k=4
6	100 200 300 400 500
b=1000f k=16
O 100	200	300	400
b=1000, k=64
0	100	200	300	400	500
b=10000, k=l
5 0 5 0
-112
- - -
d*x.xz60-
0	100	200	300 400	500
d*x.x=zβo-
0	100	200	300	400	500
b= 10000, k=4
0	100	200	300	400	500
b=10000, k=16
0	100	200	300	400
b=10000, k=64
♦ Local SGD
∣→- VRL-SGD-W
→- VRL-SGD
0	100 200 300 400 500	0	100 200 300 400 500	0	100	200	300	400
Figure 3: Logarithm of distance to the global minimum for different b and communication period k.
23
Under review as a conference paper at ICLR 2020
φ3ueμe>)z8-
b=10, k=l
b=10f k=4
b=10, k=16
O IOO 200 300 400 500
O IOO 200 300 400 500
O IOO 200 300 400 500
auu-Je>)z* 一
b=10, k=64
b=1000, k=64
O 100	200	300	400
b=10000, k=64
Figure 4: Logarithm of variance among workers for different b and communication period k.
cal SGD
∣→- VRL-SGD-W
→- VRL-SGD
0	100	200	300	400
24
Under review as a conference paper at ICLR 2020
F THE Analysis of Parameter K
In this section, we evaluate all algorithms with different communication period k.
As shown in Figure 5, VRL-SGD converges as fast as S-SGD, while Local SGD, EASGD converge
slowly even if we set the period k to half of it in Figure 1. The results show that k in Local SGD
1
should be smaller, such as k = 2 or k = 5 in transfer learning, which is in line with t⅛ =
N 4
117,1874 ≈ 3.9. However, we can set k to T2 = 117,187 2 ≈ 15 in VRL-SGD. Figure 6 compares
8 4	N 2	8 2
the convergence of different algorithms with a larger k. We observe that the convergence of VRL-
SGD will be affected with much large k, but VRL-SGD is still faster than Local SGD and EASGD,
which is consistent with our theoretical analysis.
Figure 5: Epoch loss for the non-identical case. We set k = 10 for LeNet, k = 25 for TextCNN and
k = 10 for Transfer Learning.
Figure 6: Epoch loss for the non-identical case. We set k = 40 for LeNet, k = 100 for TextCNN
and k = 40 for Transfer Learning.
25