Under review as a conference paper at ICLR 2020
Variational	Constrained Reinforcement
Learning with Application to Planning at
Roundabout
Anonymous authors
Paper under double-blind review
Ab stract
Planning at roundabout is crucial for autonomous driving in urban and rural environ-
ments. Reinforcement learning is promising not only in dealing with complicated
environment but also taking safety constraints into account as a as a constrained
Markov Decision Process. However, the safety constraints should be explicitly
mathematically formulated while this is challenging for planning at roundabout
due to unpredicted dynamic behaviour of the obstacles. Therefore, to discriminate
the obstacles’ states as either safe or unsafe is desired which is known as situation
awareness modeling. In this paper, we combine variational learning and constrained
reinforcement learning to simultaneously learn a Conditional Representation Model
(CRM) to encode the states into safe and unsafe distributions respectively as well
as to learn the corresponding safe policy. Our approach is evaluated in using
Simulation of Urban Mobility (SUMO) traffic simulator and it can generalize to
various traffic flows. (Anonymous code is available to reproduce the experimental
results and additional videos are also available 1.)
1	Introduction
The recent progress in model-free reinforcement learning (RL) (Sutton et al., 1992) has produced
many interesting results in planning and control problems and proves to be effective in finding
optimal policy for nonlinear stochastic systems when the dynamics are either unknown or affected by
severe uncertainty (BuSoniu et al., 2018), including complicated robotic locomotion and manipulation
(Kumar et al., 2016; Xie et al., 2019; Hwangbo et al., 2019). To further ensure the safety in control,
applying constraints over states and actions is a natural way. Typically, a standard and well-studied
formulation for reinforcement learning with constraints are the constrained Markov Decision Process
(CMDP) framework (Altman, 1999), where the discounted sum of safety cost should be under certain
bounds. As a consequence, discount parameter or bound has to be tuned to ensure safety (Garcia &
Fernandez, 2015).
However, in many complicated scenarios when the traffic flow changes frequently, the mathematical
description of the obstacles are difficult to obtain explicitly. In this scenario, the constraints are
dynamic with uncertainty, which is challenging to define proper constraint. Besides, it is still
challenging to tune a cost function with such complicated constraints. Without certain predefined
constraints, existed methods fail to find a feasible policy.
When humans driving a car, they do not know any specific safety constraint and still perform well. For
example, at the very beginning, one is always not sure about the distance between the headstock and
wall or other vehicles, which may lead to some accidents. However, with more driving experience,
the better awareness of safety and latent constraints will be slowly formed. In this scenario, the
constraint is not formulated as any specific distance between obstacles but the feeling according to
experience. One of the talents of humans is that humans heavily rely on to make safe decisions is that
we could understand the latent information of complicated situation or environment and awareness,
whether it is safe or dangerous (Bubic et al., 2010). This capability is called situation awareness (SA)
(Endsley, 2017). In RL, SA could be regarded as a model that can accurately aware current situation,
1https://www.dropbox.com/sh/oo6zty99c6tclx1/AAA8RXynrE8K9SYpxzqBhv4Va?
dl=0
1
Under review as a conference paper at ICLR 2020
internally represent the complex dynamics and covering enough latent information like constraints.
Furthermore, it should be able be embedded into RL framework to guide the agent how to safely
interact with environment.
This paper proposes a novel approach for encoding the measure of safety in scenarios where the
explicit safety cost is not available, or the states are interfered by severe uncertainties. The contribution
of this paper is in two-folds: 1) present a variational-based method to encode the safe and unsafe
states; 2) measure the level of safety in latent space with Wasserstein distance, taking the uncertainty
in states into consideration. Our framework can be generally combined with various RL algorithms,
improving the performance of RL algorithms in terms of safety. Finally, we evaluate our approach
in roundabout task with different traffic flow and show that our approach significantly improves the
success rate of baseline.
2	Related Work
There are several prior works about the reinforcement learning with safety constraints have been
done. Achiam et al. (2017) proposed a safety constrained policy optimization (CPO) approach
based on the trust region method, which guarantees the constraint satisfaction with a safe initial
policy. Wen & Topcu (2018) came up with a constrained cross-entropy method for finite CMDP
tasks, which an effectively learn feasible policies with respect to constraint satisfaction. Chow
et al. (2019) came up with Lyapunov-based approach and combine with both on-policy and off-
policy reinforcement learning algorithms, which achieves better results in terms of balancing the
performance and constraint satisfaction compare with CPO. However, both of the results above need
full knowledge of the constraints, and the tasks are almost static environment without uncertainty,
rather than a complex dynamic case with many unpredictable obstacles, which is the focus of this
paper.
Situation awareness is the key to successful decision-making (Nullmeyer et al., 2005). Recently,
many works on reinforcement learning with situation awareness have been done. Teng & Tan
(2008) proposed TD-FALCON methods that could integrate Context-aware Decision Support (CaDS)
system and learning for supporting context-aware decision making, where CaDS system exploits
contextual information for focused situation assessment and goal-oriented decision support. D’Aniello
et al. (2014) proposed Context Space Theory (CST) to represent raw data in high-level, domain-
relevant concept, namely context attribute, which could identify situations the user is involved in and
considering user’s situated preferences. Yin et al. (2019) propose two new metrics CTSa, CTSn to
assess the taxi situation, and they found a significant relationship is revealed between the taxi delay
and CTSa at Level-1, and the taxi time and CTSn at Level-2, which provides strong reference to
airport ground movements for control and management purposes. However, all of which are focused
on a simple task or static environment. Most importantly, the proposed situation awareness models
are all deterministic, which might lose much information in the task with the action-conditioned
settings and dynamics uncertainty.
3	Problem Formulation
Reinforcement learning with safety constraints tends to be a promising way to offer intelligence
and safety simultaneously for autonomous driving. In this paper, we will focus on the decision and
control tasks with safety constraints which can be modeled by constrained Markov decision process
(CMDP) (Altman, 1999). A CMDP is a tuple, (X, A, r, P, c, ρ), where X is the set of states and A
is the set of actions. P (x0 |x, a) is the transition probability function and ρ(x) is the starting state
distribution. r(x, a) and c(x, a) are the reward the constraint function, respectively.
In CMDP where the constraints c is not well defined, we aim to simultaneously find a policy π and
safety cost C such that
∞
max ET〜∏ [	γtr(xt, at)]	(1)
π	t=0
∞
s.t. ET〜∏ [X γtC(xt, at)] ≤ d	(2)
t=0
2
Under review as a conference paper at ICLR 2020
where Y ∈ [0,1) is the discount factor, T denotes a trajectory (T = (xo, a0, si,…))，and T 〜∏ is
shorthand for indicating that the distribution over trajectories depends on π: xo 〜ρ, at 〜π(∙∣xt),
sx+i 〜P(∙∣xt,at). d is the learned safety threshold. Though the form of C is unknown,
we assume that the data set composed of states labeled as safe or dangerous is available, i.e.
{(x1, s1), (x2, s2), (x3, s3), . . . } where s ∈ {0, 1} indicates whether the state is safe or danger-
ous. Note that it is possible the same state x has different labels at different data points, since various
actions may be taken. Based on this data set, We aim to learn a model qφ(z∣χ) to map the states to
latent space and then construct a safety cost C based on this latent model.
4	Main Results
Inference the implicit constraints and awareness of a state to be safe or dangerous are the essential
capability for safe decision making. To model the awareness of constraints, the key is to map from
physical state space to a latent space where represents the implicit constraints. We proposed a novel
method could infer the safety condition as well as the latent constraints.
In this section, our approach for inference of the latent constraints will be described in detail. First, in
Section 4.1, a conditional representation model is proposed to map the states into the latent space,
where the latent space of safe and dangerous states are separated from each other. Then in Section 4.2,
we employ Wasserstein distance in measuring level of safety of the given state in latent space, i.e. the
constructed safety cost C.
4.1	Conditional Representation Model
In this part, we propose the conditional representation model (CRM) that is capable of mapping the
state space to a latent variable space Z. For this purpose, we borrow the encoder-decoder structure
from variational autoencoder (VAE) (Kingma & Welling, 2013; Doersch, 2016), which provides an
efficient approach for approximating posterior distribution of the latent variable z given an observation
x. For normal VAE, the variational bound is written as
log pθ (χ) = DKL(qφ(z∣χ)kpθ (z∣χ)) + L(χ; θ,Φ)	(3)
L(X； θ, φ) = -DKL(q°(ZIx)kPθ (Z)) + Eqφ [log Pθ (XIz)]	(4)
where qφ(z∣χ) is the recognition model, an approximation to the intractable true posterior pθ(z∣x).
Pθ(x∣z) is the generative conditional distribution and pθ(z) is an arbitrary prior distribution. DKL
denotes the Kullback-Leibler (KL) divergence.
Figure 1: a). For VAE, the green circle denotes the prior distribution. All the latent distributions
from both safe and dangerous states map into a compact region near each others, which has no
capability to distinguish the safe and dangerous stateb). For CRM, there are two latent space. We
hope all the dangerous state maps to the white space after inference, while normal state maps to the
green space after inference. The figure shows when the trajectories drive into a dangerous situation,
the latent distribution will move close to white latent space from green latent space. Our method is
able to separate the safe and dangerous latent space.
In our framework, the CRM is not only aimed at encoding, but also contains enough information
for measuring how safe or unsafe the current latent is, which is not a concern in classical VAE. In
3
Under review as a conference paper at ICLR 2020
equation 3, the latent of both safe and dangerous states are sampled from the same prior distribution
pθ(z) (as shown in Figure 1(a)), resulting in that distributions of latent variable for safe and dangerous
states are hardly separable, which brings trouble for measuring the safety in the following step.
The data set for CRM is composed of a series of data points containing both the state and its label,
{(x1, s1), (x2, s2), (x3, s3), . . . }. To exploit the binary classification label s in the process of latent
representation learning, we rewrite the marginal likelihood in Eq.(3) as
Ep(S) logPθ(x|s) = Ep(S) (DKL(qφ(z∣x) ∣∣Pθ(z|x, S)) + L(x; θ Φ, S))	(5)
L(x; θ,φ,s) = -Dkl(qφ(z∣x)∣Pθ(z∣s)) + Eqφ [logPθ(x∣z,s)]	(6)
Here, the objective is to use an encoding model qφ (z|x) to approximate the conditional posterior
pθ(z|x, s), which is conditioned on both the state and its label. This is equivalent to maximizing
the lower bound in Eq.(6). Additionally, we would like the z for safe and dangerous states are
separately distributed as shown in Figure 1(b), so that we are able to measure the safety of a given
state Xt by calculating the divergence between qφ(zt∣xt) and the distribution of latent for dangerous
states pθ(z|s = 1). Take the trajectory in Figure 1(b) as example, as the divergence function
D(q(zt∣xt),pθ(z|s = 1)) decreases, it is intuitive that the state Xt is becoming more and more
unsafe. The discussion on how to exploit specific D to construct safety cost C is deferred to the next
subsection.
Inference Stage	Only USed For Training
Cost is a feedback from environment that measures how close to the constraints
23 Encoder and its hyperparameter
Reward is a feedback from environment that measures how good the state is
Z Latent distribution
Figure 2: Overview of the data flow of Conditional Representation Model for both training and
inference stages.1). Training Stage: During interaction with the environment, the agent collects both
dangerous and safe states. Through the encoder qφ , where φ are the hyperparameters of the network
(more details on the neural network architecture can be found in Appendix B),we expected all the
dangerous state maps to the arbitrary prior distribution p(z|S = 1), and all the safe state maps to the
another arbitrary prior distribution p(z|S = 0), contributing to the first term of the objective function
min Dkl (qφ (z∣x)∣pθ (z∣c)). To verify the inferred distribution carrying effective information, we
sample from inferred latent distribution qφ(z|x) then use a decoder to reconstruct the original data
based on the samples, forming the second term of the objective function ∣X0 - X∣2 . 2). Inference
Stage: The agent uses encoder to infer latent information and formulates safety cost by Wasserstein
distance. Finally,an CMDP reinforcement learning algorithm is employed to update the policy with
safety cost and reward.
Before proceeding, there are several issues to be addressed. First, different to the lower bound in VAE,
the objective we aim to maximize is also conditioned on label S. If a task is highly safe or unsafe, i.e.
p(S = 1) or p(S = 0) ≈ 0, then the objective falls back to the form in VAE, since the rare part of data
plays an insignificant role in the approximation loss. Thus it is necessary to strike a balance between
the amount of safe and dangerous data. Secondly, in this paper, we use Gaussian distributions with
different expectations as the prior distributions, i.e. pθ(z|s = i) = N(μs=i, I),i ∈ {0,1}. The
expectations μs=0 and μs=1 are selected to be reasonably apart from each other to ensure that the
safe and dangerous states are separable in the latent space.
In this paper, the encoder qφ (z|x) is parameterized by a Gaussian MLP (a fully connected neural
network) and decoder pθ (x|z, s) parameterized by an MLP. For training of these networks, we come
4
Under review as a conference paper at ICLR 2020
up with the following empirical objective function,
Ep(S)L(X; θ, φ, S) = Ep(S)[ -D KL(qφ(z∖x') kp(z| S)) + Ep θ (x0 ∣z,s),qφ(z∣x) kx - xk ]	⑺
where χ0 is the generated state based on the latent sampled from qφ(z∖χ). Since both the encoder
network and the prior distributions are both Gaussian, the above equation could be further written as
Ep(S)L(x; θ, φ, S)=Ep(S)[(1 + log(σ(x)2) - (μ(x) - μs)2 - σ(x)2)
+ Epθ (x0∣z,s),qφ(z∣x) kx - Xk 2]
(8)
where μs denotes the expectation of prior distributions. μ(χ) and σ(x) represents the expectation and
diagonal elements of covaraiance matrix for the Gaussian MLP. The latent of a given state x could be
generated by
Z = μ(x) + σ(x) Θ e,e 〜N(0, I)
(9)
To sum up, we expected all the dangerous state maps to the arbitrary prior distribution p(z|s = 1),
and all the safe state maps to the another arbitrary prior distribution p(z|s = 0), contributing to the
first term of the objective function min DκL(qφ(z∣x)kpθ(z∣c)). To verify the inferred distribution
carrying effective information, We sample from inferred latent distribution qφ(z∣χ) then use a decoder
to reconstruct the original data based on the samples, forming the second term of the objective
function kx0 - xk2 .
Data flow between encoder and decoder and how each part is involved in the above objective function
during training are shown in Figure 2.
4.2	Construction of Safety Constraints
In this part, the measurement of divergence between the approximated posterior latent distribution
qφ(z∖χ) and the prior pθ(z∖s = 1) is specified and exploited in constructing the safety cost ^ in
equation 2.
First, for the measurement of divergence D(qφ(z∖X), pθ(z∖S = 1)), a number of common metrics
exist, such as KL divergence (Hershey & Olsen, 2007) and Jensen-Shannon (JS) divergence (Lin,
1991). However, both KL and JS divergence are not suitable for measuring the divergence between
distributions that are distinct from each other. Instead, we use Wasserstein distance to measure the
divergence between the aforementioned distributions. Wasserstein distance(Panaretos & Zemel, 2019)
is a metric of divergence inspired by the problem of optimal mass transportation. The p-Wasserstein
distance between two distributions p1 and p2 on Rd is defined as:
Wp(p1,p2) =	inf	(EkX - Y kp)1/p	(10)
ɪ	X 〜pi,Y 〜p2
where the infimum is taken over all pairs of d-dimensional random vectors X and Y marginally
distributed as p1 andp2, respectively. Compared with other metrics, Wasserstein distance is more
informative even when the distributions are far apart from each other, while KL-divergence goes to
infinity and JS-divergence equals to a constant in this case. As qφ(z∖X) andpθ(z∖S = 1) are both
Gaussian distributions, the Wasserstein distance between them could be analytically written as:
W2(qφ(z∖x),p(z∖s = 1)) = kμ(x) - μs=1k2 - tr(I — diag(σ(x)))	(11)
where the tr(∙) denotes the trace of a matrix and diag(∙) denotes forming a diagonal matrix with the
given vector.
For CMDP tasks, the agent is required to keep away from the dangerous states (or constraints) beyond
certain threshold. Thus, we propose a general Wasserstein-based cost as:
C(X) = ma4 W2‰(z∖xL(z∖s = 1)) - 1, 0)
(12)
where d is the threshold to be tuned. The safety cost decreases as Wasserstein distance increases and
becomes zero when Wasserstein distance is larger than d. Only if the the current state achieves the
threshold of safety-level, there will be a cost, which means there will not any influence or restriction
within the threshold of safety-level, as shown in Figure 3.
5
Under review as a conference paper at ICLR 2020
Figure 3: The curve of safety cost.X-axis indicates the value of Wasserstein distance and Y-axis
indicates the value of safety cost. The red dot indicates the threshold of safe Wasserstein distance,
under which the state is regarded as potentially dangerous.
Algorithm 1 CRM-based Reinforcement learning
Randomly initialize neural networks of reinforcement learning algorithm, and encoder q°, decoder
pθ of CRM
Assume dangerous prior distribution μ, and safe prior distribution V
for each iteration do
Sample s0 according to ρ
for each time step do
Sample at from policy π and step forward
Observe st+1 and rt
Evaluate qφ(zt+1 |st+1) with encoder and calculate ct according to (12)
Store (st, at, rt, ct, st+1) for policy update
if Dead or collision, store st-n:t+1 as dangerous state for CRM update
else Store st+1 as safe state for CRM update
end for
for each update step do
Update reinforcement learning policy
Sample equal amount of data from safe states and dangerous states then update the CRM
using gradient decent minimizing (8)
end for
end for
Finally, we demonstrate how the proposed approach is incorporated with the general CMDP RL
algorithms in Algorithm 1. During training, the data for training CRM is collected from the agent’s
interaction with the environment, while CRM provides the safety cost at the same time. Once the
agent dies, the last n steps state will be labeled as dangerous while others labeled as safe. For every
update step, equal amount of data is sampled from the set of safe and dangerous states, then used
for evaluating the gradient of encoder and decoder with respect to Eq.(8). If a datasets of safe and
dangerous states are available before the training starts, it is also desirable to pre-train CRM in
advance, which in general accelerates the convergence of policy learning.
5	Experiments
We tested our algorithm on the task of driving through a double-lane roundabout with four exits.
Figure 4 depicts a bird’s-eye view of the roundabout.
The challenges faced in the roundabout include a large amount of interaction between the traffic
participants, the complexity of other agents’ driving behavior, the vast perceived uncertainty due to
the geometry of the road and the continuous operation in a short period time (turn to the inner track,
turn to outside lane, negotiation). Most importantly, it is difficult to define the constraints and design
a cost function properly under dynamic traffic with uncertainty. Thus, we aimed to model the latent
6
Under review as a conference paper at ICLR 2020
safety constraints with CRM and employ the CMDP RL algorithm together with the learned safety
cost to find a safe policy.
For the experimental environment, the roundabout scenario is simulated by the Simulation of Urban
Mobility (SUMO) traffic simulator (Behrisch et al., 2011) with TraCI (Wegener et al., 2008) for
the interface. The roundabout is connected to approach roads with give-way lines at the junctions.
Besides, there is a suggested speed limit that can be exceeded by the ego vehicle as well as other
vehicles. The simulation parameters for the selected roundabout and the vehicle properties are
presented in Table 5 below.
Lane width	3.5m
Velocity limit	13.89m∕s
Vehicle Length	5m
Max Acceleration/Deceleration	2.6m∕s2
Max Deceleration	4.5m∕s2
Sensor range	42m
Table 1: Simulation scenario and Vehicle parameters
The agent is to drive into the roundabout from the 1st entrance and drive out in the 4th exit as fast as
possible. If there is no collision with other vehicles, it succeeds. The ego vehicle follows the policies
computed the speed using the reinforcement learning algorithm, whereas other participants use the
default car following model (Kraus) integrated in SUMO. Basically, the agent is to learn a speed
profile under the default lateral control strategy of SUMO.
Besides, to show the general applicability of our methods, we designed two scenes with different
density of traffic, namely the congested traffic and regular traffic. The illustration of traffic density is
shown below in Figure 4. For the congested traffic, there are 12 equally distributed vehicles every 10
seconds from different entrances driving into the roundabout, while for the normal traffic, there are
12 equally spaced vehicles driving in every 30 seconds.
Figure 4: The figure depicts the bird’s-eye view of the double-lane four-exit roundabout with an
intersection network in two different traffic situations.. The number denotes the index of different
exits/entrances. The ego vehicle is in blue, while other vehicles are green and yellow, indicating if
vehicles are in the range of the radar. Congested traffic: around 24-30 vehicles are in the roundabout
at the same time. Regular traffic: around 12-15 vehicles are in the roundabout at the same time.
(b) Normal traffic
6 Results
In our experiments, we aim to answer the following questions:
• Does our method successfully learn the latent constraints?
7
Under review as a conference paper at ICLR 2020
•	How does the trained agent perform compared with those guided by manually designed
constraints?
•	Is our method sensitive to hyperparameters?
In this part, we evaluate the convergence of the CRM and combine it with a CMDP RL algorithm
safe SAC (SSAC) (Chow et al., 2019). SSAC is a safety constrained variant of the original algorithm
Haarnoja et al. (2018) through Lagrangian relaxation procedure. Details of the Lagrangian-based safe
baselines are referred to Appendix C. To make fair comparison, we also designed a very conservative
cost with optimized structure and parameter, and also use SSAC to train the agents. It is worth
mentioning that, for image-based tasks, it is generally difficult to design a cost, while our framework
and algorithm is still applicable, which we leave for future work.
6.1	Convergence
As demonstrated in Figure 5, CRM could efficiently model the latent constraint. Besides, throughout
the training, it converges with low variance even though both the model and environment are randomly
initialized.
Figure 5: Training of CRM. X-axis indicates the training steps in thousand and Y-axis indicates the
loss of CRM. The shadowed region shows the 1-SD confidence interval over 5 random seeds.
6.2	Comparison with baseline
In this part, we compare the performance between agents trained by SSAC with and without CRM.
Specifically, we use success rate, i.e. the probability of leaving from the right exit without collision,
as the metric of safety performance. As demonstrated in Figure 6, in both congested traffic and
regular traffic scenarios, SSAC with CRM achieves better success rate and convergence speed than
SSAC baseline.
(a) Congested traffic
Figure 6: Success rate of agents trained by SSAC with and without CRM during training. X-axis
indicates the training steps in thousand and Y-axis indicates the success rate. The shadowed region
shows the 1-SD confidence interval over 5 random seeds.
(b) Regular traffic
8
Under review as a conference paper at ICLR 2020
6.3	Insensitivity to hyperparameters
The threshold d ∈ [0, WD(pθ(z∣s = 0),pθ(z|s = 1))] in 12 is a hyperparameter to be tuned. We
show in this part that the performance of our approach is insensitive to this hyperparameter. We
tested 3 different threshold setting [u5b, 2UUb, 3uUb], where Ub = WD(pθ(z|s = 0),pθ(z|s = 1)). As
shown in Figure 7, CRM performs stably across different settings. The Wasserstein cost is highly
generalized and insensitive to hyperparameters, which is easily for implementation. To the contrary,
the manually designing safety cost for complicated tasks like autonomous driving requires lots of
tuning and could be very sensitive to parameters.
Figure 7: Success rate of agents trained by SSAC with CRM under different d. X-axis indicates the
training steps in thousand and Y-axis indicates the success rate. The shadowed region shows the 1-SD
confidence interval over 5 random seeds.
7 Conclusion
Inspired by the situation awareness of human intelligence, in this paper, we proposed a universal
approach of and latent constraints representation, which could be inherently embedded into CMDP
reinforcement learning framework. We believe better situation awareness is an essential steps towards
creating more intelligent agents. The roundabout experiment shows that our approach can indeed
help agent aware the potential danger and improve the performance. Besides, the CRM is able
to generalize through different traffic flow. Our works represent an initial step in combining RL
and situation modeling. Future work includes: i) optimizing the prior distribution; ii) extending
to image-based task which still a open problem for CMDP; iii) exploring other kinds of situation
awareness for reinforcement learning.
9
Under review as a conference paper at ICLR 2020
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 16),pp. 265-283, 2016.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 22-31.
JMLR. org, 2017.
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Michael Behrisch, Laura Bieker, Jakob Erdmann, and Daniel Krajzewicz. Sumo-simulation of urban
mobility: an overview. In Proceedings of SIMUL 2011, The Third International Conference on
Advances in System Simulation. ThinkMind, 2011.
Andreja Bubic, D Yves Von Cramon, and Ricarda I Schubotz. Prediction, cognition and the brain.
Frontiers in human neuroscience, 4:25, 2010.
Lucian Buyoniu, Tim de Bruin, Domagoj ToliC, Jens Kober, and Ivana Palunko. Reinforcement
learning for control: Performance, stability, and deep approximators. Annual Reviews in Control,
2018.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and Edgar Duenez-
Guzman. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031, 2019.
Giuseppe D’Aniello, Matteo Gaeta, Vincenzo Loia, Francesco Orciuoli, and Stefania Tomasiello.
A dialogue-based approach enhanced with situation awareness and reinforcement learning for
ubiquitous access to linked data. In 2014 International Conference on Intelligent Networking and
Collaborative Systems, pp. 249-256. IEEE, 2014.
Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.
Mica R Endsley. Toward a theory of situation awareness in dynamic systems. In Situational
Awareness, pp. 9-42. Routledge, 2017.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437-1480, 2015.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and
applications. arXiv preprint arXiv:1812.05905, 2018.
John R Hershey and Peder A Olsen. Approximating the kullback leibler divergence between
gaussian mixture models. In 2007 IEEE International Conference on Acoustics, Speech and Signal
Processing-ICASSP’07, volume 4, pp. IV-317. IEEE, 2007.
Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen
Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science
Robotics, 4(26):eaau5872, 2019.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Vikash Kumar, Abhishek Gupta, Emanuel Todorov, and Sergey Levine. Learning dexterous manipu-
lation policies from experience and imitation. arXiv preprint arXiv:1611.05095, 2016.
Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information
theory, 37(1):145-151, 1991.
10
Under review as a conference paper at ICLR 2020
Robert T Nullmeyer, David Stella, Gregg A Montijo, and Stephen W Harden. Human factors in
air force flight mishaps: Implications for change. In Proceedings of the 27th Annual Interser-
vice/Industry Training, Simulation, and Education Conference, number 2260. National Training
Systems Association Arlington, VA, 2005.
Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review of
statistics and its application, 6:405-431, 2019.
Richard S Sutton, Andrew G Barto, and Ronald J Williams. Reinforcement learning is direct adaptive
optimal control. IEEE Control Systems Magazine, 12(2):19-22, 1992.
Teck-Hou Teng and Ah-Hwee Tan. Cognitive agents integrating rules and reinforcement learning
for context-aware decision support. In 2008 IEEE/WIC/ACM International Conference on Web
Intelligence and Intelligent Agent Technology, volume 2, pp. 318-321. IEEE, 2008.
Axel Wegener, MichaI PiOrkowski, Maxim Raya, Horst HellbrUck, Stefan Fischer, and Jean-Pierre
Hubaux. Traci: an interface for coupling road traffic and network simulators. In Proceedings of
the 11th communications and networking simulation symposium, pp. 155-163. ACM, 2008.
Min Wen and Ufuk Topcu. Constrained cross-entropy method for safe reinforcement learning. In
Advances in Neural Information Processing Systems, pp. 7450-7460, 2018.
Zhaoming Xie, Patrick Clary, Jeremy Dao, Pedro Morais, Jonathan Hurst, and Michiel van de Panne.
Iterative reinforcement learning based design of dynamic locomotion skills for cassie. arXiv
preprint arXiv:1903.09537, 2019.
Jianan Yin, Minghua Hu, Yuanyuan Ma, Ke Han, and Dan Chen. Airport taxi situation awareness with
a macroscopic distribution network analysis. Networks and Spatial Economics, 19(3):669-695,
2019.
A	Experiment Setting
A. 1 State and Action
we assume the the ego vehicle could detect and track the a maximum of three other vehicles’ position,
orientation, and velocity. The parameters of the sensor field used are given in the Table 5. Besides,
the ego vehicle is able to estimate its condition. Using the above information, the ego-state and
observation space of the ego vehicle is defined as follows:
Sego =< x, y, p, v >	(13)
where x, y, p, v indicate the 2-D position, orientation, velocity respectively. The coordinate transfor-
mation that aligns the vehicle position and velocity along the vehicle axis makes the vehicle state
invariant to road geometry.
The state or observations of other vehicles in the range of sensor field is recorded as:
Oi =< xi, yi, pi, vi >	(14)
where Oi refers to the radar information of vehicle i. If no vehicle is present in the range of sensor
field, then the x, y are set to sensor range limits, and orientation and v are set to 0.
Using the above and then setting that up to three vehicles could be tracked, the state of the agent at
each time step can be defined as the ego state and observation of the three closest vehicles follows:
xt =< Sego, O1, O2, O3 >	(15)
As mentioned before, our agent aims to learn a speed profile under the default lateral control strategy
of SUMO. Thus, the action can be defined as:
at =< v >	(16)
where v is the desired velocity of ego vehicle within the range of maximum speed.
11
Under review as a conference paper at ICLR 2020
A.2 Reward and cost
The agent updates policy according to the feedback of its interaction with the environment. For our
scenario, the goal is to drive safely and efficiently through a roundabout without any collision with
other vehicles. Thus, the reward is defined as:
rt = v	(17)
where the V is the current speed of ego vehicle, while the safety cost ^(x) is defined in Equation 3.
B Trainning Details
Hyperparameters	Intense Roundabout	Intense Roundabout
Minibatch size	256	256
Actor learning rate	1e-4	1e-4
Critic learning rate	3e-4	3e-4
Target smoothing coefficient(τ)	0.005	0.005
Discount(γ)	0.99	0.99
Safety γ	0.5	0.5
Threshold	0.5	0.5
Table 2: SSAC Hyperparameters
For SSAC, there are two networks: the policy network and the Q network. For the policy network,
we use a fully-connected MLP with two hidden layers of 256 units, outputting the mean and standard
deviations of a Gaussian distribution. For the Q network and the Lyapunov network, we use a
fully-connected MLP with two hidden layers of 256 units, outputting the Q value and the Lyapunov
value. All the hidden layers use Relu activation function and we adopt the same invertible squashing
function technique as Soft Actor-critic (SAC) Haarnoja et al. (2018) to the output layer of the policy
network.
Hyperparameters
Minibatch size
learning rate
Conditional representation model
512
0.001
Table 3: CRM Hyperparameters
For CRM, there are two networks: the decoder and the encoder. For the encoder, we use a fully-
connected MLP with two hidden layers of 256 and 128 units repectively, outputting the mean and
standard deviations of a Gaussian distribution. For the decoder, we use a fully-connected MLP with
two hidden layers of 128 and 256 units respectively, outputting the reconstruction of input. All the
hidden layers use Sigmoid activation function.
The implementation of the algorithms are based on TensorFlow (Abadi et al., 2016).
C Lagrangian-based Safe Algorithm
We include a Lagrangian based method as the baseline in our experiments, namely the safe soft
actor-critic (SSAC). SSAC attempts to solve the following unconstrained optimization problem
m∏inmaxLg λ) = E(s,ag)〜D [β[log(∏θ(fθ(e, s)∣s)) + Ht] - Q(s, fθ(e, S)) + λ(Qc(s, a) - d)]
(18)
where Qc is the safety Q-function. By employing the same policy gradient algorithm as in SAC, a
safe policy is obtained. The value of Lagrange multipliers β, λare adjusted by the gradient ascent
method with the following objectives, respectively,
J (β)= β E(s,a)〜D [log(∏θ (a∣s)) + Ht]	(19)
J(λ) = λE(s,a)〜D∙[Lc(s0,fθ(e, S0)) - Lc(s,a) + α3c(s,a)]	(20)
12