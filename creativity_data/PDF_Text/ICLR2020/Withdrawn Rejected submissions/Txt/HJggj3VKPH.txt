Under review as a conference paper at ICLR 2020

ON  THE  DYNAMICS  AND  CONVERGENCE  OF  WEIGHT

NORMALIZATION FOR TRAINING NEURAL NETWORKS

Anonymous authors

Paper under double-blind review

ABSTRACT

We present a proof of convergence for ReLU networks trained with weight normal-
ization. In the analysis, we consider over-parameterized 2-layer ReLU networks
initialized at random and trained with batch gradient descent and a fixed step size.
The proof builds on recent theoretical works that bound the trajectory of parameters
from their initialization and monitor the network predictions via the evolution
of a “neural tangent kernel” (Jacot et al.  2018).  We discover that training with
weight normalization decomposes such a kernel via the so called “length-direction
decoupling”.  This in turn leads to two convergence regimes and can rigorously
explain    the utility of WeightNorm. From the modified convergence we make a few
curious observations including a natural form of “lazy training” where the direction
of each weight vector remains stationary.

1    INTRODUCTION

Dynamic normalization in neural networks is a re-parametrization procedure between the layers that
improves stability during training and leads to faster convergence. This approach was popularized
with the introduction of Batch Normalization (BatchNorm) in [20] and has led to a plethora of
additional normalization methods, notably including Layer Normalization (LayerNorm) [6] and
Weight Normalization (WeightNorm) [28]. WeightNorm was proposed as a method that emulates
BatchNorm and benefits from similar stability and convergence properties. Moreover, WeightNorm
has the advantage of not requiring a batch setting, therefore considerably reducing the 
computational
overhead that is imposed by BatchNorm [16].  WeightNorm is widely used in training of neural
networks and is the focus of this work.

Today, normalization methods are ubiquitous in the training of neural nets since in practice they
significantly improve the convergence speed and stability in training. Despite the impressive 
empirical
results and massive popularity of dynamic normalization methods, explaining their utility and 
proving
that they converge when training with non-smooth, non-convex loss functions has remained an
unsolved problem.  In this paper we provide sufficient conditions on the data, initialization, and
over-parameterization for dynamically normalized ReLU networks to converge to a global minimum
of the loss function, and rigorously illustrate the utility of normalization methods.

Consider the class of 2-layer ReLU neural networks f  :  Rd  →  R parameterized by (W, c)  ∈

Rm×d × Rm as


m

f (x; W, c) =  √m

k=1

ckσ(wkTx).                                    (1.1)

Here the activation function is the ReLU, σ(s) = m√ax{s, 0} [26], m denotes the width of the second

layer, and f is normalized accordingly by a factor    m. We investigate gradient descent training 
with

WeightNorm for (1.1), which re-parameterizes the network in terms of (V, g, c) ∈ Rm×d ×Rm ×Rm


m

f (x; V, g, c) =         

m

k=1

ckσ

.gk ·

vkTx

ǁvkǁ₂

.                              (1.2)

This gives a similar parameterization to [14] that study convergence of gradient optimization of
convolutional filters on Gaussian data. We consider the regression task, optimizing with respect to 
the

1


Under review as a conference paper at ICLR 2020

L² loss with random parameter initialization and focus on the over-parametrized regime, meaning
that m > n, where n is the number of training samples.

The neural network function class (1.1) has been studied in many papers including [3, 15, 33, 36]
along with other similar over-parameterized architectures [1, 14, 23]. An exuberant series of recent
works prove that feed-forward ReLU networks converge to zero training error when trained with
gradient descent from random initialization.  Nonetheless, to the best of our knowledge, there are
no proofs that ReLU networks trained with dynamic normalization on general data converge to a
global minimum. This is in part because normalization methods completely change the optimization
landscape during training.  Here we show that neural networks of the form given above converge
at linear rate when trained with gradient descent and WeightNorm.  The analysis is based on the
over-parameterization of the networks, which allows for guaranteed descent while the gradient is
non-zero.

For regression training, a group of papers studied the trajectory of the networks’ predictions and
showed that they evolve via a “neural tangent kernel” (NTK) as introduced by Jacot et al. [21].
The latter paper studies neural network convergence in the continuous limit of infinite width over-
parameterization, while the works of [3, 15, 27, 33, 36] analyze the finite width setting. For 
finite-
width over-parameterized networks, the training evolution also exhibits a kernel that takes the 
form of
a Gram matrix. In these works, the convergence rate is dictated by the least eigenvalue of the 
kernel.
We build on this fact, and also on the general ideas of the proof of [15] and the refined work of 
[3].

Compared  with  un-normalized  training,  we  prove  that  normalized  networks  follow  a  modified
kernel evolution that features a “length-direction” decomposition of the NTK. This leads to two
convergence regimes in WeightNorm training and explains the utility of WeightNorm from the
perspective of the NTK. In the settings considered, WeightNorm significantly reduces the amount of
over-parameterization needed for provable convergence, as compared with un-normalized settings.
The   decomposition of the NTK also connects to observations of [12] that discuss “lazy training”
which refers to a training regime where the weights of the network stay close to their 
initialization (see
Section 6). Further, we present a more careful analysis that leads to improved over-parameterization
bounds as compared with [15].

In this work we rigorously analyze the dynamics of weight normalization training and its convergence
from the perspective of the neural tangent kernel. We discover WeightNorm training has two regimes
with distinct behaviors. The main contributions of this work are:

We prove the first general convergence result for dynamically normalized 2-layer ReLU networks
trained with gradient descent. Our formulation does not assume the existence of a teacher network
and has mild assumptions on the training data.

We explain the utility of normalization methods via a decomposition of the neural tangent kernel.
In the analysis we highlight two distinct convergence regimes and give a concrete example of “lazy
training” for finite-step gradient descent.

It is shown that finite-step gradient descent converges for all weight magnitudes at initialization
and we significantly reduce the amount of over-parameterization required for provable convergence
as compared with un-normalized training.

The paper is organized as follows. In Section 2 we provide background on WeightNorm and derive
key evolution dynamics of training in Section 3. We present and discuss our main results, alongside
with the idea of the proof, in Section 4. We discuss related work in Section 5, and offer a 
discussion of
our results and their implications to dynamic normalization training and “lazy training” in Section 
6.
Proofs are presented in the Appendix.

2    WEIGHTNORM

Here we give an overview of the WeightNorm procedure and review some known properties of
normalization methods.

Notation    We use lowercase, lowercase boldface, and uppercase boldface letters to denote scalars,
vectors and matrices resp. We denote the Rademacher distribution as U {1, −1} and write N (µ, Σ)
for  a Gaussian with mean µ and covariance Σ. Training points are denoted by x₁ . . . xn ∈ Rd and
parameters of the first layer by vk ∈  Rd.  We use σ(x) := max{x, 0}, and write ǁ · ǁ₂, ǁ · ǁF for

2


Under review as a conference paper at ICLR 2020

the spectral and Frobenius norms for matrices. λmin(A) is used to denote the minimum eigenvalue
of a matrix A and ⟨·, ·⟩ denotes the Euclidean inner product.  For a vector v denote the l√2  
vector

norm as ǁvǁ₂ and for a positive definite matrix S define the induced vector norm ǁvǁS :=    vTSv.

The projections of x onto u and u⊥ are defined as xᵘ :=  ᵘᵘTx  , xᵘ⊥  := x.I −  uuT Σ. Denote the

indicator function of event A as 1A and for a weight vector at time t, vk(t), and data point xi we
denote 1ik(t) := 1{vk (t)Txi≥ 0}.

WeightNorm procedure    For a single neuron σ(wTx), WeightNorm re-parametrizes the weight

w ∈ Rd in terms of v ∈ Rd, g ∈ R as

w(v, g) = g ·     v    ,    σ.g ·  vTx Σ.                                  (2.1)

ǁvǁ₂                  ǁvǁ₂

This decouples the magnitude and direction of each weight vector (referred as the “length-direction”
decomposition).   In  comparison,  for  BatchNorm  each  output  wTx  is  normalized  according  to
the       average statistics in a batch.  We can draw the following analogy between WeightNorm and
BatchNorm if the inputs xi are centered (Ex = 0) and the covariance matrix is known (ExxT = S).
In       this case, batch training with BatchNorm amounts to

σ.γ ·  .      wTx        ΣΣ = σ.γ ·  √wTx    Σ = σ.γ ·  wTx Σ.              (2.2)

From this prospective, WeightNorm is a special case of (2.2) with S = I [22, 28].

Properties of WeightNorm    We start by giving an overview of known properties of WeightNorm
that will be used to derive the gradient flow dynamics of WeightNorm training.

For re-parameterization (2.1) of a network function f that is initially parameterized with a weight 
w,
the gradient ∇wf relates to the gradients ∇vf, ∇gf by the identities


g

∇vf =  ǁvǁ

(∇wf )

,     ∇gf = (∇wf )  .

This implies that    vf   v = 0 for each input x and parameter v. For gradient flow, this 
orthogonality
results in   v(0)  ₂ =   v(t)  ₂ for all t. For gradient descent (with step size η) the 
discretization in
conjunction with orthogonality leads to increasing parameter magnitudes during training [4, 19, 28],
as illustrated in Figure 1,

2                         2           2                  2                         2

ǁv(s + 1)ǁ₂ = ǁv(s)ǁ₂ + η  ǁ∇vf ǁ₂ ≥ ǁv(s)ǁ₂.                          (2.3)

vk(s)


vk(t)

α

dvk (0)

−∇vk L

vk(0)

vk(0)

Figure 1: WeightNorm updates for gradient flow and gradient descent. For gradient flow, the norm of
the weights are preserved, i.e., ǁvk(0)ǁ₂ = ǁvk(t)ǁ₂ for all t > 0. For gradient descent, the norm 
of
the weights ǁvk(s)ǁ₂ is increasing with s.

Problem Setup    We analyze (1.1) with WeightNorm training (1.2), so that


m

f (x; V, c, g) =         

m

k=1

ckσ

.gk ·

vkTx     .

ǁvkǁ₂

3


Under review as a conference paper at ICLR 2020

We take an initialization in the spirit of [17, 28]:

vk(0) ∼ N (0, α²I),    ck ∼ U {−1, 1},    and    gk(0) = ǁvk(0)ǁ₂/α.             (2.4)

Where α is the variance of vk at initialization.  The initialization of gk(0) is therefore taken to 
be
independent of α. We remark that the initialization (2.4) gives the same initial output distribution
as in methods that study the un-normalized network class (1.1). The parameters of the network are
optimized using the training data {(x₁, y₁), . . . , (xn, yn)} with respect to the square loss

L(f ) =  1 Σ(f (x ) − y )² =  1 ǁf − yǁ2,                               (2.5)

where f  = (f₁, f₂, . . . , fn)T = (f (x₁), f (x₂), . . . , f (xn))T and y = (y₁, y₂, . . . , yn)T.

3    EVOLUTION  DYNAMICS

We present the gradient flow dynamics of training (2.5) to illuminate the modified dynamics of
WeightNorm as compared with vanilla gradient descent. In Appendix C we tackle gradient descent

training  with  WeightNorm  where  the  predictions’  evolution  vector  ᵈᶠ  is  replaced  by  the  
finite

difference f (s + 1)     f (s). For gradient flow, each parameter is updated in the negative 
direction of
the partial derivative of the loss with respect to that parameter. The optimization dynamics give

dvk  = − ∂L ,    dgk  = − ∂L .                                      (3.1)

dt         ∂vk       dt         ∂gk

We  consider  the  case  where  we  fix  the  top  layer  parameters  ck  during  training.   In  
the  over-
parameterized regime, the dynamics of ck and gk turn out to be the same.

To quantify convergence, we monitor the time derivative of the i-th prediction, which is computed
via the chain rule as

∂fi = Σ  ∂fi dvk +  ∂fi dgk .

Substituting (3.1) into the i-th prediction evolution and grouping terms yields

m                            m

∂fi = − Σ  ∂fi  ∂L  − Σ ∂fi  ∂L .                                 (3.2)

	

                                 

i                                                 i

v                                    g

The gradients of fi and L with respect to vk are written explicitly as

n


∂fi

∂v

  1   c    g  (t)

(t) =  √m ǁv  (t)ǁ

vk(t)⊥

xi             ik(t),

∂L            1  

∂v  (t) =  √m

(f (t)     y ) ck · gk (t)

ǁv  (t)ǁ

xvk (t)⊥ 1ik(t).

Thus T ⁱ (t) in (3.2) can be calculated as


Σ                   1  Σ

		

. c  · g  (t) Σ2.         ⊥                    ⊥ Σ

		

Defining the v-orthogonal Gram matrix V(t) as


 1  Σ

. αc  · g  (t) Σ2.         ⊥                    ⊥ Σ

	


we can write T ⁱ as

m

k=1

ǁvk(t)ǁ₂

n

T ⁱ (t) = Σ Vij (t)(f (t) − y ).

Note that V(t) is the induced neural tangent kernel [21] for the parameters v of WeightNorm 
training.
While it resembles the Gram matrix H(t) studied in [3], here we obtain a matrix that is not 
piece-wise

4


Under review as a conference paper at ICLR 2020

constant in v since the data points are projected onto the orthogonal component of v. We compute

T ⁱ in (3.2) analogously. The associated derivatives with respect to gk are


∂fi

∂g

  1         ck     

(t) =  √m ǁv  (t)ǁ

σ(vk(t)T

xi),

∂L                  n

∂g  (t) =  √m

      ck     

(fj(t) − yj) ǁv  (t)ǁ

σ(vk(t)T

xj),


k                            k          2

and we obtain

k                       j=1                                     k          2


T ⁱ(t) =

kΣ=1

n

m

j=1

(fj(t) − yj)

      ck             ²

ǁvk(t)ǁ₂

σ(vk(t)Txj)σ(vk(t)Txi).


Given that c²  = 1, define G(t) as

 1  Σ

σ(v  (t)Tx )σ(v  (t)Tx  )


hence we can write

m

k=1

n

ǁvk(t)ǁ2

T ⁱ(t) = Σ Gij(t)(fj(t) − yj).

j=1

Combining Tv and Tg, the full evolution dynamics are given by

df  = −. V(t) + G(t)Σ(f (t) − y).                                   (3.5)

Denote Λ(t) :=  V⁽ᵗ⁾ + G(t) and write ᵈᶠ  = −Λ(t)(f (t) − y). We note that V(0), G(0), defined

α2                                               dt

in (3.3), (3.4), are independent of α:

Observation 1 (α independence).  For initialization (2.4) and α > 0 the Gram matrices V(0), G(0)

are independent of α.

This fact is proved in Appendix A. When training the neural network in (1.1) without WeightNorm


(see [3, 15, 36]) , the corresponding neural tangent kernel H(t) is defined by ∂ᶠi   = Σm

∂fi  dwk  =


− Σm

∂fi     ∂L

= − Σn

Hij(t)(fj − yj) and takes the form

∂t             k=1  ∂wk   dt


k=1  ∂wk ∂wk

j=1

H

(t) =   1  Σ xTx  1

(t)1

(t).                                   (3.6)

The analysis presented above shows that vanilla and WeightNorm gradient descent are related as
follows.

Proposition 1.  Define V(0), G(0), and H(0) as in (3.3), (3.4), and (3.6) respectively. then for 
all

α > 0,


Thus, for α = 1,

V(0) + G(0) = H(0).

∂f

∂t = −Λ(0)(f (0) − y) = −H(0)(f (0) − y).

That is, WeightNorm decomposes the NTK in each layer into a length and a direction component.
We refer to this as the “length-direction decoupling” of the NTK, in analogy to (2.1).  From the
proposition, normalized and un-normalized training kernels initially coincide if α = 1. The utility 
of
normalization methods can be attributed to the modified NTK Λ(t) that occurs when the WeightNorm
coefficient, α, deviates from 1. For α     1 the kernel Λ(t) is dominated by G(t), and for α     1 
the
kernel Λ(t) is dominated by V(t). We elaborate onthe details of this in the next section. In 
practice,
by (2.3) as training progresses,   vk  ₂ grow monotonically, leading to larger α and a transition 
from
the V-dominated to the G-dominated regime. In our analysis we will study the two regimes α > 1
and α < 1 in turn.

5


Under review as a conference paper at ICLR 2020

4    MAIN  CONVERGENCE  THEORY

In this section we discuss our convergence theory and main results. From the continuous flow (3.5),
we observe that the convergence behavior is described by V(t) and G(t). The matrices V(t) and
G(t) are positive semi-definite since they can be shown to be covariance matrices. This implies that
the least eigenvalue of the evolution matrix Λ(t) =   ¹  V(t) + G(t) is bounded below by the least

eigenvalue of each kernel matrix,

λmin(Λ(t)) ≥ max{λmin(V(t))/α², λmin(G(t))}.

For finite-step gradient descent, a discrete analog of evolution (3.5) holds. However, the discrete 
case
requires additional care in ensuring dominance of the driving gradient terms. For gradient flow, it 
is
relatively easy to see linear convergence is attained by relating the rate of change of the loss to 
the
magnitude of the loss. Suppose that for all t ≥ 0,

λmin  Λ(t)   ≥ ω/2,    with ω > 0.                                    (4.1)

Then the change in the regression loss is written as

d ǁf (t) − yǁ2  = 2(f (t) − y)T df (t)  = −2(f (t) − y)TΛ(t)(f (t) − y)


dt                  ²

dt

(4.1)

2

≤   −ωǁf (t) − yǁ₂.

Integrating this time derivative and using the initial conditions yields

2                                                           2

ǁf (t) − yǁ₂ ≤ exp(−ωt)ǁf (0) − yǁ₂,

which  gives  linear  convergence.   The  focus  of  our  proof  is  therefore  showing  that  
(4.1)  holds
throughout training.

By Observation 1 we have that V and G are independent of the WeightNorm coefficient α (α only
appears in the 1/α² scaling of Λ). This suggests that the kernel Λ(t) =   ¹  V(t) + G(t) can be 
split
into two regimes: When α < 1 the kernel is dominated by the first term  ¹  V, and when α > 1 the
kernel is dominated by the second term G.  We divide our convergence result based on these two
regimes.

In each regime, (4.1) holds if the corresponding dominant kernel, V(t) or G(t), maintains a positive
least eigenvalue. Having a least eigenvalue that is bounded from 0 gives a convex-like property that
allows us to prove convergence. To ensure that condition (4.1) is satisfied, for each regime we show
that the corresponding dominant kernel is “anchored” (remains close) to an auxiliary Gram matrix
which we define in the following for V and G.

Define the auxiliary v-orthogonal and v-aligned Gram matrices V∞, G∞ as

Vi∞j   := Ev∼N₍₀,α2 I)  ⟨xᵛ⊥ , xᵛ⊥ ⟩1ik(0)1jk(0),                            (4.2)

G∞ij  := Ev∼N₍₀,α2 I)  ⟨xᵛ, xᵛ⟩1ik(0)1jk(0).                                (4.3)

For now, assume that V∞ and G∞ are positive definite with a least eigenvalue bounded below by ω
(we give a proof sketch below). In the convergence proof we will utilize over-parameterization to
ensure that V(t), G(t) concentrate to their auxiliary versions so that they are also positive 
definite
with a least eigenvalue that is greater than ω/2. The precise formulations are presented in Lemmas 
B.4
and B.5 that are relegated to Appendix B.

To prove our convergence results we make the assumption that the xis have bounded norm and are
not parallel.

Assumption  1  (Normalized  non-parallel  data).   The  data  points  (x₁, y₁), . . . , (xn, yn)  
satisfy

ǁxiǁ₂ ≤ 1 and for each index pair i /= j, xi     β · xj  for all β ∈ R \ {0}.

In order to simplify the presentation of our results, we assume that the input dimension d is not 
too
small, whereby d    50 suffices. This is not essential for the proof. Specific details are provided 
in
Appendix A.

Assumption 2.  For data points xi ∈ Rd assume that d ≥ 50.

6


Under review as a conference paper at ICLR 2020

Both assumptions can be easily satisfied by pre-processing, e.g., normalizing and shifting the data,
and adding zero coordinates if needed.

Given Assumption 1, V∞, G∞ are shown to be positive definite.

Lemma 4.1.  Fix training data points   (x₁, y₁), . . . , (xn, yn)   satisfying Assumption 1.  Then 
the
v-orthogonal and v-aligned Gram matrices V∞ and G∞, defined as in (4.2) and (4.3), are strictly
positive definite. We denote the least eigenvalues λmin(V∞) =: λ₀, λmin(G∞) =: µ₀.

Proof sketch    Here we sketch the proof of Lemma 4.1.  The main idea, is the same as [15], is to
regard the auxiliary matrices V∞, G∞ as the covariance matrices of linearly independent operators.
For each data point xi, define φi(v)  :=  xᵛ⊥ 1{xTv≥0}.  The Gram matrix V∞ is the covariance

matrix of   φi  i₌₁:n taken over Rd with the measure N (0, α²I). Hence showing that V∞ is strictly
positive definite is equivalent to showing that   φi  i₌₁,...n are linearly independent.  Unlike 
[15],
the functionals under consideration are not piecewise constant so a different construction is used
to prove independence.  Analogously, a new set of operators, θi(v)  :=  σ(xᵛ), is constructed for
G∞.  Interestingly, each φi corresponds to  ᵈθi .  The full proof is presented in Appendix D. As
already observed from evolution (3.5), different magnitudes of α lead to two distinct regimes that 
are
discussed below. We present the main results for each regime.

V-DOMINATED CONVERGENCE


For α < 1 convergence is dominated by V(t) and λ

min

(Λ(t)) ≥    1   λ

min

(V(t)).  We present the

convergence theorem for the V-dominated regime here.

Theorem 4.1 (V-dominated convergence).  Suppose a neural network of the form (1.2) is initialized as
in (2.4) with α ≤ 1 and that Assumptions 1,2 hold. In addition, suppose the neural.network is 
traineΣd


then with probability 1 − δ,

1.  For iterations s = 0, 1, . . . , K, the evolution matrix Λ(s) satisfies λ

min

(Λ(s)) ≥   λ0   .

2.  WeightNorm training with gradient descent of step-size η = O.    α2      Σ converges linearly

as                                                                                                 
ǁV∞ǁ2

ǁf (s) − yǁ2  ≤ .1 −  ηλ0 Σsǁf (0) − yǁ2.

The proof of Theorem 4.1 is presented in Appendix C. We will provide a sketch below. We make the
following observations about our V-dominated convergence result.

The required over-parameterization m is independent of α.  Further, the dependence of m on the
failure probability is log(1/δ). This improves previous results that require polynomial dependence 
of
order δ³. Additionally, we reduce the dependence on the sample size from n⁶ (as appears in [3]) to
n⁴ log(n).

In Theorem 4.1, smaller α leads to faster convergence, since the convergence is dictated by λ₀/α².
Nonetheless, smaller α is also at the cost of smaller allowed step-sizes, since η = O(α²/  V∞  ₂).
The trade-off between step-size and convergence speed is typical.  For example, this is implied in
Chizat et al. [12], where nonetheless the authors point out that for gradient flow training, the 
increased
convergence rate is not balanced by a limitation on the step-size.  The works [4, 19, 32] define an
effective step-size (adaptive step-size) η′ = η/α² to avoid the dependence of η on α.

G-DOMINATED CONVERGENCE

For α > 1 our convergence result for the class (1.2) is based on monitoring the least eigenvalue of

G(t). Unlike V-dominated convergence, α does not affect the convergence speed in this regime.

Theorem  4.2  (G-dominated  convergence).  Suppose  a  network  of  the  form  (1.2)  is  
initialized
as  in  (2.4)  with  α  ≥   1  and  that  Assumptions  1,  2  hold.    In  addition,  suppose  the  
neural
network. is  tra.ined  via  the  regression  loss  (2.5)  ΣwΣith  targets  y  satisfying  ǁyǁ∞  =   
O(1).   If

	

1.  For iterations s = 0, 1, . . . , K, the evolution matrix Λ(s) satisfies λmin(Λ(s)) ≥  µ0 .

7


Under review as a conference paper at ICLR 2020

as                                                                                                  
ǁΛ(t)ǁ

ǁf (s) − yǁ2  ≤ .1 −  ηµ0 Σsǁf (0) − yǁ2.

We make the following observations about our G-dominated convergence result, and provide a proof
sketch further below.

0                                             0

Taking α =     n/µ₀ gives an optimal required over-parameterization of order

m = Ω.n² log(n/δ)/µ²Σ.

This significantly improves on previous results [15] for un-normalized training that have 
dependencies
of order 4 in the least eigenvalue, cubic dependence in 1/δ, and n⁶ dependence in the number of
samples n. In contrast to V-dominated convergence, here the rate of convergence µ₀ is independent
of     α but the over-parameterization m is α-dependent. We elaborate on this curious behavior in 
the
next sections.

Proof sketch of main results    The proof of Theorems 4.1 and 4.2 is inspired by a series of works
including [3, 13, 15, 33, 36]. The proof has the following steps: (I)  We show that at 
initialization
V(0), G(0) can be viewed as empirical estimates of averaged data-dependent kernels V∞, G∞
that are strictly positive definite under Assumption 1.  (II)   For each regime, we prove that the
corresponding kernel remains positive definite if vk(t) and gk(t) remain near initialization for 
each

1      k     m.  (III)  Given a uniformly positive definite evolution matrix Λ(t) and sufficient 
over-
parameterization we show that each neuron, vk(t), gk(t) remains close to its initialization. The 
full
proof is presented in Appendix B for gradient flow and Appendix C for finite-step gradient descent.

While the spirit of the proof is familiar from other works, the different convergence regimes 
explain
the utility of WeightNorm. We elaborate further on this in the discussion.

5    RELATED  WORK

Dynamic normalization theory    A number of recent works attempt to explain the dynamics and
utility of dynamic normalization methods. The original works [20, 28] of BatchNorm and Weight-
Norm resp. suggest that dynamic normalization methods improve training by fixing the intermediate
layers’ output distributions.  The works of Bjorck et al. [8] and Santurkar et al. [29] argue that
BatchNorm may improve optimization by improving smoothness of the Hessian of the loss, therefore
allowing for larger step-sizes with reduced instability.  Hoffer et al. [18] showed that the 
effective
step-size in BatchNorm is divided by the magnitude of the weights, this followed the work of WNgrad

[32] that introduces an adaptive step-size algorithm based on this fact.   Following the intuition
of WNGrad, Arora et al. [4] proved that for smooth loss and network functions, the diminishing
“effective step-size” of normalization methods lead to convergence with optimal convergence rate for
properly initialized step-sizes. The work of [22] explains the accelerated convergence of BatchNorm
from  a “length-direction decoupling” perspective. The authors along with [9] analyze the linear 
least
squares regime, with [22] presenting a bisection method for finding the optimal weights. Robustness
and regularization of Batch Normalization is investigated in [25] and improved generalization is
analyzed empirically. Shortly after the original work of WeightNorm, [35] showed that for a single
precptron WeightNorm may speed-up training and emphasized the importance of the norm of the
initial weights.  Additional stability properties were studied by [34] via mean-field analysis.  The
authors show that gradient instability is inevitable even with BatchNorm as the number of layers
increases; this is in agreement with [7] for networks with residual connections.  The work of [5]
suggests initialization strategies for WeightNorm and derives lower bounds on the width to guarantee
same order gradients across the layers.

Over-parametrized neural networks    There has been a significant recent literature studying the
convergence of un-normalized over-parametrized neural networks. The analysis of the majority of the
works relies on the width of the layers. These include 2-layer networks trained with Gaussian inputs
and outputs from a teacher network [30], [24] and [14] (with WeightNorm). Assumptions on the data
distribution are relaxed in [15] and the works that followed [3, 33, 36]. Our work is inspired by 
the
mechanism presented in this chain of works. Wu et al. [33] extend convergence results to adaptive

8


Under review as a conference paper at ICLR 2020

step-size methods and propose AdaLoss. Recently, the global convergence of over-parameterized
neural networks was also extended to deep architectures [2, 13, 37, 38]. In the over-parameterized
regimes, Arora et al. [3] develop generalization properties for the networks of the form (1.1).  In
addition, in the context of generalization, Allen-Zhu et al. [1] illustrates good generalization 
for deep
neural networks trained with gradient descent. Cao and Gu [10] and [11] derive generalization error
bound of gradient descent and stochastic gradient descent for learning over-parameterization deep
ReLU neural networks.

6    DISCUSSION

In this section we interpret our main results and make a connection of the convergence theory with
“lazy training”.

Chizat et al. [12] analyze recent works of over-parameterized convergence based on the NTK and
note that re-scaling the network’s outputs during gradient flow training leads to fast convergence
while at the same time the weights remain close to their initialization. This is interpreted as a 
“linear
solution”, meaning that the direction of the weights does not change at training. The authors also
refer to this type of convergence as “lazy training”.

In our G-dominated convergence, we observe a new type of “lazy training” that is different from the
one presented in [12] . We refer to our G regime as “lazy training” since the directions of the 
weights
remain fairly stationary. Nonetheless, in G-dominated “lazy training”, the magnitudes of the weights
(gk) change. Moreover, we observe this phenomenon not only in the continuous flow setting but also
in the finite step gradient descent setting. Unlike [12] where it is argued that neural networks do 
not
necessarily follow the “lazy training” regime, we believe that G-dominated convergence actually is
common but that it emerges at later stages in training, after the weights have adopted their 
directions
in the V-dominated regime and improves stability.

Overall training with WeightNorm leads to a gradual transition from V-dominated to the G-dominated
regimes. We recall that since the weights grow (see (2.3)), the WeightNorm coefficient α increases
during training progressively. The direction of the weights changes rapidly at the earlier stages of
training when α is small, and G-dominated convergence ensues as α grows, leading to improved
stability. For α > 1 this allows us to ease the requirements made on the over-parameterization (less
is sufficient) and step size (a bigger step size is possible, of order η = O(1/  G(t)  ₂)). From 
this
perspective, the utility of WeightNorm is attributed to the allowed larger step-sizes and increased
stability at the later stages of training (G-dominated), all while maintaining fast convergence at 
the
beginning of training (V-dominated convergence).

Dynamic normalization is the most common optimization set-up of current deep learning models, yet
understanding the convergence of such optimization methods is still an open problem. In this work we
present a proof giving sufficient conditions for convergence of dynamically normalized 2-layer ReLU
networks trained with gradient descent. To the best of our knowledge this is the first proof 
showcasing
convergence of gradient descent training of neural networks with dynamic normalization and general
data, where the objective function is non-smooth and non-convex. The notion of “length-direction
decoupling” is clarified by the neural tangent kernel Λ(t) that naturally separates in our analysis
into “length”, G(t), and “direction”, V(t)/α², components. For α = 1 the decomposition initially
matches un-normalized training. Yet we discover that in general, normalized training with gradient
descent leads to 2 regimes dominated by different pieces of the neural tangent kernel.  We note
that training typically commences in the V-dominated regime and transitions into the G-dominated
regime as training proceeds and the magnitude of the weights grow. Our improved analysis is able to
reduce the amount of over-parameterization that was needed in previous convergence works in the
un-normalized setting and in the G-dominated regime, we prove convergence with a significantly
lower amount of over-parameterization as compared with un-normalized training.

REFERENCES

[1]  Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparame-
terized neural networks, going beyond two layers. In NeurIPS, 2019.

[2]  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.  A convergence theory for deep learning via
over-parameterization.  In International Conference on Machine Learning, pages 242–252,
2019.

9


Under review as a conference paper at ICLR 2020

[3]  Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pages 322–332, 2019.

[4]  Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu.   Theoretical analysis of auto rate-tuning by
batch normalization.  In International Conference on Learning Representations, 2019.  URL
https://openreview.net/forum?id=rkxQ-nA9FX.

[5]  Devansh Arpit and Yoshua Bengio.  The benefits of over-parameterization at initialization in
deep ReLU networks. In NeurIPS, 2019.

[6]  Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. Deep Learning
Symposium, NIPS-2016, 2016.

[7]  David  Balduzzi,  Marcus  Frean,  Lennox  Leary,  JP  Lewis,  Kurt  Wan-Duo  Ma,  and  Brian
McWilliams.   The shattered gradients problem:  If resnets are the answer, then what is the
question?  In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 342–350. JMLR. org, 2017.

[8]  Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger.  Understanding batch
normalization. In Advances in Neural Information Processing Systems, pages 7694–7705, 2018.

[9]  Yongqiang Cai, Qianxiao Li, and Zuowei Shen. A quantitative analysis of the effect of batch
normalization on gradient descent. In International Conference on Machine Learning, pages
882–890, 2019.

[10]  Yuan Cao and Quanquan Gu.  A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.

[11]  Yuan Cao and Quanquan Gu.  Generalization bounds of stochastic gradient descent for wide
and deep neural networks. In NeurIPS, 2019.

[12]  Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. In NeurIPS, 2019.

[13]  Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.   Gradient descent finds
global minima of deep neural networks.  In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of
Proceedings of Machine Learning Research, pages 1675–1685, Long Beach, California, USA,
09–15  Jun 2019. PMLR.

[14]  Simon S. Du, Jason D. Lee, and Yuandong Tian. When is a convolutional filter easy to learn?  
In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=SkA-IE06W.

[15]  Simon  S.  Du,  Xiyu  Zhai,  Barnabas  Poczos,  and  Aarti  Singh.   Gradient  descent  
provably
optimizes  over-parameterized  neural  networks.   In  International  Conference  on  Learning
Representations, 2019. URL https://openreview.net/forum?id=S1eK3i09YQ.

[16]  Igor Gitman and Boris Ginsburg. Comparison of batch normalization and weight normalization
algorithms for the large-scale image classification. arXiv preprint arXiv:1709.08145, 2017.

[17]  Xavier Glorot and Yoshua Bengio.   Understanding the difficulty of training deep feedfor-
ward neural networks. In Proceedings of the thirteenth international conference on artificial
intelligence and statistics, pages 249–256, 2010.

[18]  Elad Hoffer, Itay Hubara, and Daniel Soudry.   Train longer, generalize better:  closing the
generalization gap in large batch training of neural networks. In Advances in Neural Information
Processing Systems, pages 1731–1741, 2017.

[19]  Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks.   In Advances in Neural Information Processing
Systems, pages 2164–2174, 2018.

[20]  Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning, pages
448–456, 2015.

[21]  Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571–8580, 2018.

10


Under review as a conference paper at ICLR 2020

[22]  Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Ming Zhou, and Klaus
Neymeyr. Exponential convergence rates for batch normalization: The power of length-direction
decoupling in non-convex optimization. In Kamalika Chaudhuri and Masashi Sugiyama, editors,
Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning
Research, pages 806–815. PMLR, 16–18 Apr 2019.

[23]  Yuanzhi Li and Yingyu Liang.  Learning overparameterized neural networks via stochastic
gradient descent on structured data.  In Advances in Neural Information Processing Systems,
pages 8168–8177, 2018.

[24]  Yuanzhi Li and Yang Yuan.   Convergence analysis of two-layer neural networks with relu
activation. In Advances in Neural Information Processing Systems, pages 597–607, 2017.

[25]  Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Understanding regularization in
batch normalization. arXiv preprint arXiv:1809.00846, 2018.

[26]  Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted Boltzmann 
machines.
In Proceedings of the 27th international conference on machine learning (ICML-10), pages
807–814, 2010.

[27]  Samet Oymak and Mahdi Soltanolkotabi.  Towards moderate overparameterization:  global
convergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674,
2019.

[28]  Tim Salimans and Durk P Kingma.  Weight normalization:  A simple reparameterization to
accelerate training of deep neural networks.  In Advances in Neural Information Processing
Systems, pages 901–909, 2016.

[29]  Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.  How does batch
normalization help optimization?  In Advances in Neural Information Processing Systems, pages
2483–2493, 2018.

[30]  Yuandong Tian.  An analytical formula of population gradient for two-layered relu network
and its applications in convergence and critical point analysis.   In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 3404–3413. JMLR. org,
2017.

[31]  Roman Vershynin.  High-dimensional probability: An introduction with applications in data
science, volume 47. Cambridge University Press, 2018.

[32]  Xiaoxia Wu, Rachel Ward, and Léon Bottou.  Wngrad:  Learn the learning rate in gradient
descent. arXiv preprint arXiv:1803.02865, 2018.

[33]  Xiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods
for an over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.

[34]  Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoen-
holz.  A mean field theory of batch normalization.  In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=SyMDXnCcF7.

[35]  Yuki Yoshida, Ryo Karakida, Masato Okada, and Shun-ichi Amari.  Statistical mechanical
analysis of online learning with weight normalization in single layer perceptron. Journal of the
Physical Society of Japan, 86(4):044002, 2017.

[36]  Guodong Zhang, James Martens, and Roger Grosse.  Fast convergence of natural gradient
descent for over-parameterized neural networks. In NeurIPS, 2019.

[37]  Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In NeurIPS, 2019.

[38]  Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. Machine Learning, 2019.

11


Under review as a conference paper at ICLR 2020

APPENDIX

We present the detailed proofs of the main results of the paper below.  The appendix is organized
as follows. We provide proofs to the simple propositions regarding the NTK presented in the paper
in Appendix A, and prove the main results for V-dominated and G-dominated convergence in the
settings of gradient flow and gradient descent in Appendices B,C. The proofs for gradient flow and
gradient descent share the same main idea, yet the proof for gradient descent has a considerate 
number
of additional technicalities.  In Appendices D and E we prove the lemmas used in the analysis of
Appendices B and C respectively.

A    WEIGHTNORM  DYNAMICS  PROOFS

In this section we provide proofs for Proposition 1, which describes the relation between vanilla 
and
WeightNorm NTKs and Observation 1 of the paper.

Proof of Proposition 1:

We would like to show that V(0) + G(0) = H(0). For each entry, consider


(V(0) + G(0))

=   1  Σ .xvk (0)⊥ , x  vk(0)⊥ Σ1

(0)1

(0) +  1  Σ .xᵛk (0), x  ᵛk (0)Σ1

(0)1

(0).


Note that

ij       m

i                 j

k=1

ik         jk

m          i              j
k=1

ik         jk


This gives

i                 i                 j

(V(0) + G(0))

j                             i

=   1  Σ .x , x  Σ1

(0)1

(0) = H

i

(0)


which proves the claim.

ij       m

i

k=1

j      ik         jk                 ij

Proof of Observation 1:

We show that the initialization of the network is independent of α. Take α, β > 0, and for each k,

initialize vα, vβ as

k      k

vα(0) ∼ N (0, α²I),    vβ(0) ∼ N (0, β²I).


Then

k

vα(0)

k

vβ(0)

        k            ∼          k            ∼  Unif(Sd−1)   (in distribution).


ǁvα(0)ǁ₂

ǁv  (0)ǁ

Hence the distribution of each neuron σ     ᵛk (0)         at initialization is independent of α.  
Next for

ǁvk(0)ǁ₂

gk(0), we note that

α                  α    β

ǁvk (0)ǁ₂ ∼  β ǁvk (0)ǁ₂.

Initializing gα(0), gβ(0) as in (2.4),

k           k

gα(0) =  ǁvk(0)ǁ₂ ,    gβ (0) =  ǁvk(0)ǁ₂ ,


gives

k                     α            k

β                        gα(0)vα(0)

β

gβ(0)vβ(0)


gα(0),    g  (0) ∼ χd,    and

∼                         ∼ N (0, I),


ǁvα(0)ǁ₂

ǁv  (0)ǁ

for all α, β.  This shows that the network initialization is independent of α and is equivalent to
the initialization of the un-normalized setting. Similarly, inspecting the terms in the summands of
V(0), G(0) shows that they are also independent of α. For


 1  Σ

. αc  · g  (0) Σ2.         ⊥                     ⊥ Σ

		

12


Under review as a conference paper at ICLR 2020

the terms 1ik(0), xᵛk (0)⊥  are independent of scale, and the fraction in the summand is 
identically 1.


G(0) defined as

G   (0) =   1  Σ 1

(0)1

(0).xvk(0), xvk(0)Σ

is also invariant of scale since the projection onto a vector direction vk(0) is independent of 
scale.

B    CONVERGENCE  PROOF  FOR  GRADIENT  FLOW

In this section we derive the convergence results for gradient flow.

The main results are analogous to Theorems 4.1, 4.2 but by considering gradient flow instead of
gradient descent the proofs are simplified. In Appendix C we prove the main results from Section 4
(Theorem 4.1, 4.2) for finite step gradient descent.

We state our convergence results for gradient flow.

Theorem B.1 (V-dominated convergence).  Suppose a network from the class (1.2) is initialized as in
(2.4) with α < 1 and that assumptions 1,2 hold. In addition, suppose the neural network is trained 
via
the regression loss (2.5) with target y satisfying ǁyǁ∞ = O(1). Then if m = Ω  n⁴ log(n/δ)/λ⁴  ,

WeightNorm training with gradient flow converges at a linear rate, with probability 1 − δ, as

2                                       2                              2

ǁf (t) − yǁ₂ ≤ exp(−λ₀t/α  )ǁf (0) − yǁ₂.

This theorem is analogous to Theorem 4.1 but since here, the settings are of gradient flow there is 
no
mention of the step-size. It is worth noting that smaller α leads to faster convergence and appears 
to
not affect the other hypotheses of the flow theorem. This “un-interuptted” fast convergence behavior
does not extend to finite-step gradient descent where the increased convergence rate is balanced by
decreasing the allowed step-size.

The second main result for gradient flow is for G-dominated convergence.

Theorem B.2 (G-dominated convergence).  Suppose a network from the class (1.2) is initialized
as  in  (2.4)  with  α  >  1  and  that  assumptions  1,  2  hold.   In  addition,  suppose  the  
neural  net-

0                                      0

verges at a linear rate, with probability 1 − δ, as

2                                                              2

ǁf (t) − yǁ₂ ≤ exp(−µ₀t)ǁf (0) − yǁ₂.

B.1    PROOF SKETCH

To prove the results above we follow the steps introduced in the proof sketch of Section 4. The main
idea of the proofs for V and G dominated convergence are analogous and a lot of the proofs are
based of Du et al. [15]. We show that in each regime, we attain linear convergence by proving that
the least eigenvalue of the evolution matrix Λ(t) is strictly positive. For the V-dominated regime 
we
lower bound the least eigenvalue of Λ(t) as λmin(Λ(t)) ≥ λmin(V(t))/α² and in the G-dominated
regime we lower bound the least eigenvalue as λmin(Λ(t)) ≥ λmin(G(t)).

The main part of the proof is showing that λmin(V(t)), λmin(G(t)) stay uniformly positive. We use
several lemmas to show this claim.

In each regime, we first show that at initialization the kernel under consideration, V(0) or G(0), 
has
a positive least eigenvalue. This is shown via concentration to an an auxiliary kernel (Lemmas B.1,
B.2), and showing that the auxiliary kernel is also strictly positive definite (Lemma 4.1).

Lemma B.1.  Let V(0) and V∞ be defined as in (3.3) and (4.2), assume the network width m

satisfies m = Ω. n2 log(n/δ) Σ. Then with probability 1 − δ,


ǁV(0) − V∞ǁ₂

λ₀

≤   4  .

13


Under review as a conference paper at ICLR 2020

Lemma  B.2.   Let  G(0)  and  G∞  be  defined  as  in  (3.4)  and  (4.3),  assume  m satisfies  m  =
Ω. n2 log(n/δ) Σ. Then with probability 1 − δ,                           


ǁG(0) − G∞ǁ₂

µ₀

≤   4  .

After showing that V(0), G(0) have a positive least-eigenvalue we show that V(t), G(t) maintain this
positive least eigenvalue during training. This part of the proof depends on the 
over-parameterization
of the networks.  The main idea is showing that if the individual parameters vk(t), gk(t) do not
change too much during training, then V(t), G(t) remain close enough to V(0), G(0) so that they
are still uniformly strictly positive definite. We prove the results for V(t) and G(t) separately 
since
each regime imposes different restrictions on the trajectory of the parameters.

For now, in Lemmas B.3, B.4, B.5, we make assumptions on the parameters of the network not
changing  “too  much”;  later  we  show  that  this  holds  and  is  the  result  of  
over-parameterization.
Specifically, over-parameterization ensures that the parameters stay at a small maximum distance
from their initialization.

V-dominated convergence    To prove the least eigenvalue condition on V(t), we introduce the
surrogate Gram matrix Vˆ (t) defined entry-wise as

m

Vˆ    (t) =   1  Σ .xᵛk (t)⊥ , xᵛk (t)⊥ Σ1  (t)1   (t).                          (B.1)

This definition aligns with V(t) if we replace the scaling term   αᶜkgk (t)   2  in each term in 
the sum

ǁvk(t)ǁ₂

Vij(t) by 1.

To monitor V(t)     V(0) we consider Vˆ (t)     V(0) and V(t)     Vˆ (t) in Lemmas B.3 and B.4
respectively:

Lemma B.3 (Rectifier sign-changes).  Suppose v₁(0), . . . , vk(0) are sampled i.i.d. as (2.4).   In

addition assume we have m = Ω.(m/δ)1/dn log(n/δ) Σ and ǁv  (t) − v  (0)ǁ   ≤           αλ0          
 =: R  .


Then with probability 1 − δ,

Lemma B.4.  Define

αλ0

ǁVˆ (t) − V(0)ǁ₂

k

λ₀

≤   8  .

k         2         96n(m/δ)1/d                    v


         λ₀         

R  =                        ,    R

        αλ₀        

=                        .                             (B.2)

ᵍ     48n(m/δ)1/d           v       96n(m/δ)1/d

Suppose the conditions of Lemma B.3 hold, and that ǁvk(t)−vk(0)ǁ₂ ≤ Rv, ǁgk(t)−gk(0)ǁ₂ ≤ Rg

for all 1 ≤ k ≤ m. Then with probability 1 − δ,


ǁV(t) − V(0)ǁ₂ ≤

λ₀ .

4

G-dominated convergence    We ensure that G(t) stays uniformly positive definite if the following
hold.

Lemma B.5.  Given v₁(0), . . . , vk(0) generated i.i.d. as in (2.4), suppose that for each k, 
ǁvk(t) −


√2παµ0

8n(m/δ)1/d

=: R˜v , then with probability 1 − δ,

µ₀

ǁG(t) − G(0)ǁ₂ ≤   4  .

After deriving sufficient conditions to maintain a positive least eigenvalue at training, we 
restate the
discussion of linear convergence from Section 4 formally.

14


Under review as a conference paper at ICLR 2020

Lemma B.6.  Consider the linear evolution  ᵈᶠ  = −.G(t) + V⁽ᵗ⁾ Σ(f (t) − y) from (3.5). Suppose

		

that λ     .G(t) + V⁽ᵗ⁾ Σ ≥  ω  for all times 0 ≤ t ≤ T . Then

	

ǁf (t) − yǁ₂ ≤ exp(−ωt)ǁf (0) − yǁ₂

for all times 0 ≤ t ≤ T .

Using the linear convergence result of Lemma B.6, we can now bound the trajectory of the parameters
from their initialization.

Lemma B.7.  Suppose that for all 0 ≤ t ≤ T , λ     .G(t) +  ¹  V(t)Σ ≥  ω  and |g  (t) − g  (0)| ≤

Rg ≤ 1/(m/δ)¹/ᵈ. Then with probability 1 − δ over the initialization

ǁ                                     4√nǁf (0) − yǁ₂     :    ′


vk(t) − vk(0)ǁ₂ ≤

for each k and all times 0 ≤ t ≤ T .

Lemma B.8.  Suppose that for all 0 ≤ t ≤ T , λ

αω√m        =  Rv                                         (B.3)

.G(t) +  ¹  V(t)Σ ≥  ω . Then with probability

1 − δ over the initialization                                 √                         

|                                 4   nǁf (0) − yǁ₂     :    ′

gk(t) − gk(0)| ≤            √mω         =  Rg

for each k and all times 0 ≤ t ≤ T.

The distance of the parameters from initialization depends on the convergence rate (which depends
on λmin(Λ(t))) and the width of the network m. We therefore are able to find sufficiently large m 
for
which the maximum parameter trajectories are not too large so that we have that the least eigenvalue
of Λ(t) is bounded from 0; this proves the main claim.

Before proving the main results in the case of gradient flow, we use two more technical lemmas.

Lemma B.9.  Suppose that the net√work is initialized as (2.4) and that y ∈ Rn has bounded entries

Lemma B.10 (Failure over initialization).  Suppose v₁(0), . . . , vk(0) are initialized i.i.d. as 
in (2.4)

with input dimension d. Then with probability 1 − δ,


1

max

ǁv  (0)ǁ

(m/δ) ¹/ᵈ

≤      α        .

In addition by (2.3), for all t ≥ 0, with probability 1 − δ,


1

max

ǁv  (t)ǁ

(m/δ) ¹/ᵈ

≤      α        .

Remark (Assumption 2).  Predominately, machine learning applications reside in the high dimen-
sional regime with d     50.  Typically d      50.  This therefore leads to an expression (m/δ)¹/ᵈ
that is essentially constant. For example, if d = 50, for max                ¹            10, one 
would need

ǁvk(0)ǁ₂

m/δ     10⁸⁰  (the tail of χ²  also has a factor of (d/2)!   2ᵈ/²  which makes the assumption even
milder). The term (m/δ)¹/ᵈ therefore may be taken as a constant for practicality,


1

max

ǁv  (0)ǁ

C

≤  α .

While we make Assumption 2 when presenting our final bounds, for transparency we do not use
Assumption 2 during our analysis and apply it only when we present the final over-parameterization
results to avoid the overly messy bound.  Without the assumption the theory still holds yet the
over-parameterization bound worsens by a power 1 + 1/(d    1). This is since the existing bounds
can be modified, replacing m with m¹− 1 .

15


Under review as a conference paper at ICLR 2020

Proof of Theorem B.1:

Note that m = Ω  n⁴ log(n/δ)/λ⁴   implies that Lemma B.1 holds. Further since the gradient flow
updates are continous (since the time derivative with respect to each parameter may be bounded),
there exist a small time T for which the parameters are bounded from initialization for 0 ≤ t ≤ T
and for all 1 ≤ k ≤ m,

ǁvk(t) − vk(0)ǁ₂ ≤ Rv, |gk(t) − gk(0)| ≤ Rg.

The bounded trajectory above, along with the over-parameterization, ensures that with probability

1 − δ over the initialization that Lemma B.4 holds and that


λmin

.G(t) +  1  V(t)Σ ≥ λ

min

(V(t))/α²       λ0  

2α2

for times 0 ≤ t ≤ T . The condition on the eigenvalue of the evolution matrix along with the bounded
trajectory of gk implies that Lemmas B.7, B.8 hold for at least times 0 ≤ t < T .  Define T₀ to be
the  first failure poin.t of either LemmΣas B.8, B.7, we have that T₀  ≥  T  > 0.  For 0 ≤  t < T₀, 
by

calculation utilizing Lemma B.7 shows that

B.7  α√nǁf (0) − yǁ₂


ǁvk(t) − vk(0)ǁ₂  ≤

Similarly by Lemma B.8 m ensures that

√mλ₀

≤ Rv.

B.8  α²√nǁf (0) − yǁ₂


|gk(t) − gk(0)|  ≤

√mλ₀

≤ Rg.

The over-parameterization of m implies that the parameter trajectories stay close enough to 
initializa-


tion to satisfy the hypotheses of Lemmas B.3, B.4 and that λ

min

(Λ(t)) ≥ λ

min

(V(t))/α² ≥   λ0    at

time T₀. This implies that Lemmas B.7, B.8 both hold at time T₀ which contradicts the definition
of T₀. Therefore we conclude that Lemmas B.7, B.8 hold for t > 0 which implies that Lemma B.4
holds for t > 0 hence Lemma B.6 guarantees linear convergence.

Here we consider the case where the convergence is dominated by G. This occurs when α > 1.

Proof of Theorem B.2:

Similarly to Theorem B.1, we note that m = Ω  max   n⁴ log(n/δ)/α⁴µ⁴, n² log(n/δ)/µ²      im-

plies that Lemma B.2 holds. Further since the gradient flow updates are continous (since the time
derivative with respect to each parameter may be bounded), there exist small time T for which the
parameters are bounded from initialization for 0 ≤ t ≤ T and for all 1 ≤ k ≤ m

ǁvk(t) − vk(0)ǁ₂ ≤ R˜v .

The bounded trajectory above, along with the over-parameterization, ensures that with probability

1 − δ over the initialization that Lemma B.5 holds and that


λmin

.G(t) +  1  V(t)Σ ≥ λ

min

(G(t))/α²      µ0

2

for times 0  ≤  t  ≤  T .  The condition on the eigenvalue of the evolution matrix along with the
bounded trajectory of gk implies that Lemmas B.7, B.8 hold for at least times 0 ≤ t < T . Define
T₀ to be the first failure poi.nt of e.ither Lemmas B.8, B.7, we have thatΣTΣ0  ≥ T > 0. For 0 ≤ t 
<

	

ǁf (0) − yǁ₂ of Lemma B.9, a direct calculation utilizing Lemma B.7 shows that

B.7  4√nǁf (0) − yǁ₂  B.9  Cn√log(n/δ)       ˜


ǁvk(t) − vk(0)ǁ₂  ≤

Similarly by Lemma B.8 m ensures that

αµ₀√m

≤        αµ √m      ≤ Rv.

B.8  √nǁf (0) − yǁ₂


|gk(t) − gk(0)|  ≤

√mµ₀

≤ Rg.

16


Under review as a conference paper at ICLR 2020

The over-parameterization of m implies that the parameter trajectories stay close enough to initial-
ization to satisfy the hypotheses of Lemmas B.5 and that λmin(Λ(t))     λmin(G(t))/α²      µ₀/2 at
time T₀. This implies that Lemmas B.7, B.8 both hold at time T₀ which contradicts the definition
of T₀. Therefore we conclude that Lemmas B.7, B.8 hold for t > 0 which implies that Lemma B.5
holds for t > 0 hence Lemma B.6 guarantees linear convergence with rate µ₀/2.

Note that if α is large, the required complexity on m is reduced. Taking α = Ω(    n/µ₀) gives the
improved bound

m = Ω              2                 .

0

C    FINITE  STEP-SIZE  TRAINING

The general technique of proof for gradient flow extends to finite-step gradient descent. 
Nonethless,
proving convergence for WeightNorm gradient descent exhibits additional complexities arising from
the discrete updates and joint training with the new parameterization (1.2). We first introduce some
needed notation.

Define Si(R) as the set of indices k     [m] corresponding to neurons that are close to the activity
boundary of ReLU at initialization for a data point xi,

Si(R) := {k ∈ [m] : ∃ v with ǁv − vk(0)ǁ₂ ≤ R and 1ik(0) =/     1{vTxi ≥ 0}}.

We upper bound the cardinality of |Si(R)| with high probability.

Lemma C.1.  With probability 1 − δ, we have that for all i


|Si(R)| ≤

√2mR

√πα   +

16 log(n/δ)

.

3

Next we review some additional lemmas needed for the proof of Theorems 4.1, 4.2. Analogous to
Lemmas B.7, B.8, we bound the finite-step parameter trajectories in Lemmas C.2, C.3.

Lemma  C.2.  Suppose  the  norm  of    f (s)      y  ²  decreases  linearly  for  some  convergence 
 rate

ω during  gradient  descent  training  for  all  iteration  steps  s  =  0, 1, . . . , K  with  
step-size  η as

ǁf (s) − yǁ2  ≤ (1 −  ηω )ˢǁf (0) − yǁ2  . Then for each k we have


2                       2                                  2

|gk(s) − gk(0)| ≤

4√nǁf (0) − yǁ₂

mω

for iterations s = 0, 1, . . . , K + 1.

Lemma C.3.  Under the assumptions of Lemma C.2, suppose in addition that  gk(s)     gk(0)
1/(m/δ)¹/ᵈ for all iterations steps s = 0, 1, . . . K . Then for each k,

8√nǁf (0) − yǁ₂


for s = 0, 1, . . . , K + 1.

ǁvk(s) − vk(0)ǁ₂ ≤

α√mω

To prove linear rate of convergence we analyze the s + 1 iterate error ǁf (s + 1) − yǁ₂ relative to 
that
of the s iterate, ǁf (s) − yǁ₂. Consider the network’s coordinate-wise difference in output between
iterations, fi(s + 1) − fi(s), writing this explicitly based on gradient descent updates yields

m

  1   Σ  ckgk(s + 1)                      T             ckgk(s)               T

				

17


Under review as a conference paper at ICLR 2020

We now decompose the summand in (C.1) looking at the updates in each layer, fi(s + 1)     fi(s) =

ai(s) + bi(s) with

  1   Σ  ckgk(s + 1)               T             ckgk(s)               T

k=1        k                 2                                               k          2

m

  1   Σ  ckgk(s + 1) .                  T                           T      Σ

		

Further, each layer summand is then subdivided into a primary term and a residual. ai(s), 
correspond-

ing to the difference in the first layer     ᶜkgk (s+1)   −   ckgk (s)      , is subdivided into aI 
(s) and aII (s)


as follows:

  1   Σ

ǁvk(s+1)ǁ₂

. c  g  (s + 1)

ǁvk(s)ǁ₂                                     i                      i

 c  g  (s)  Σ


m

k=1

  1   Σ

ǁvk(s)ǁ₂

.  c  g  (s + 1) 

ǁvk(s)ǁ₂

c  g  (s + 1) Σ

bi(s) is sub-divided based on the indices in the set Si that monitor the changes of the rectifiers. 
For
now, Si = Si(R) with R to be set later in the proof. bi(s) is partitioned to summands in the set Si
and the complement set,

I           1    Σ   ckgk(s + 1) .                  T                           T      Σ

 

II           1    Σ   ckgk(s + 1) .                  T                           T      Σ

		

With this sub-division in mind, the terms corresponding to convergence are aI (s), bI (s) whereas
aII (s), bII (s) are residuals that are the result of discretization. We define the primary and 
residual
vectors p(s), r(s) as

p(s) =  aI (s) + bI (s) ,    r(s) =  aII  + bII (s) .                          (C.4)

η                                 η

If the residual r(s) is sufficiently small and p(s) may be written as p(s) =    Λ(s)(f (s)     y) 
for
some iteration dependent evolution matrix Λ(s) that has

λmin(Λ(s)) = ω/2                                                       (C.5)

for ω > 0 then the neural network (1.2) converges linearly when trained with WeightNorm gradient
descent of step size η. We formalize the condition on r(s) below and later derive the conditions on
the over-parameterization (m) ensuring that r(s) is sufficiently small.

Property 1.  Given a network from the class (1.2) initialized as in (2.4) and trained with gradient
descent of step-size η, define the residual r(s) as in (C.4) and take ω as in (C.5).  We specify the
“residual condition” at iteration s as

ǁr(s)ǁ₂ ≤ cωǁf (s) − yǁ₂

for a sufficiently small constant c > 0 independent of the data or initialization.

Here we present Theorem C.1 which is the backbone of Theorems 4.1 and 4.2.

Theorem C.1.  Suppose a network from the class (1.2) is trained via WeightNorm gradient descent
with an evolution matrix Λ(s) as in (C.5) satisfying λmin(Λ(s))  ≥  ω/2 for s =  0, 1, . . . K.  In
addition if the data meets assumptions 1, 2, the step-size η of gradient descent satisfies η ≤      
    1        


and that the residual r(s) defined in (C.4) satisfies Property 1 for

3ǁΛ(s)ǁ2

s = 0, 1, . . . , K then we have that


for s = 0, 1, . . . , K.

ǁf (s) − yǁ₂ ≤

.1 −

ηω   ˢ

2

ǁf (0) − yǁ₂

18


Under review as a conference paper at ICLR 2020

Proof of Theorem C.1:

This proof provides the foundation for the main theorems. In the proof we also derive key bounds to
be used in Theorems 4.1, 4.2. We use the decomposition we described above and consider again the
difference between consecutive terms f (s + 1) − f (s),

m

  1   Σ  ckgk(s + 1)                      T             ckgk(s)               T

				

Following the decompostion introduced in (C.2), aI (s) is re-written in terms of G(s),


m

ai (s) =  √    

      ck     .

− η ∂L(s) Σ

σ(vk(s)Txi)

m k=1  ǁvk(s)ǁ₂             ∂gk

m                         n

= − η  Σ       ck        Σ(f (s) − y )       ck        σ(vT(s)x  )σ(vT(s)x )

	


Σ                    1  Σ

. v  (s)Tx  Σ

. v  (s)Tx  Σ


j=1
n

m

k=1

ǁvk(s)ǁ₂

ǁvk(s)ǁ₂

= −η     (fj(s) − yj)Gij(s),

j=1

where the first equality holds by the gradient update rule gk(s + 1) = gk(s)     η   gk L(s).  In 
this
proof we also derive bounds on the residual terms of the decomposition which we will aid us in the
proofs of Theorems 4.1, 4.2.  aI (s) is the primary term of ai(s), now we bound the residual term

aII (s). Recall aII (s) is written as

i                                 i

  1   Σ .  c  g  (s + 1)       c  g  (s + 1) Σ

		

which corresponds to the difference in the normalization in the second layer.  Since     vk L(s) is
orthogonal to vk(s) we have that

ǁvk(s + 1)ǁ₂      ǁvk(s)ǁ₂


= c  g  (s + 1). √                   1

	

−         1       Σσ(v  (s)Tx )

	

−ckgk(s + 1)η²ǁ∇v  L(s)ǁ2


=                                                     k                    2

σ(vk(s)Txi)

ǁvk(s + 1)ǁ₂ǁvk(s)ǁ₂(ǁvk(s)ǁ₂ + ǁvk(s + 1)ǁ₂)

−ckgk(s + 1)η²ǁ∇v  L(s)ǁ2     . vk(s)Txi Σ

					

≤      2ǁv  (s)ǁ  ǁv  (s + 1)ǁ       σ   ǁv  (s)ǁ      ,

where the first equality above is by completing the square, and the inequality is due to the 
increasing
magnitudes of ǁvk(s)ǁ₂.

Since 0     σ   ᵛk (s)Txi               1, the above can be bounded as

ǁvk(s)ǁ₂


  1   Σ

  g  (s + 1)η²ǁ∇    L(s)ǁ2  

			


|ai   (s)| ≤  √m

   k                         vk            2

  2ǁv  (s)ǁ  ǁv  (s + 1)ǁ

	

  1   Σ η².1 + R  (m/δ)¹/ᵈΣ3nǁf (s) − yǁ2(m/δ)¹/ᵈ

η²n.1 + R  (m/δ)¹/ᵈΣ3ǁf (s) − yǁ2(m/δ)¹/ᵈ

19


Under review as a conference paper at ICLR 2020

The second inequality is the result of applying the bound in equation (E.1) on the gradient norm

ǁ∇vk L(s)ǁ₂ and using Lemma B.10.

Next we analyze bi(s) and sub-divide it based on the sign changes of the rectifiers. Define the set
Si := Si(R) as in Lemma C.1 with R taken to be such that   vk(s + 1)     vk(0)  ₂      R for all k.
Take bII (s) as the sub-sum of bi(s) with indices k from the set Si.

bI (s) corresponds to the sub-sum with the remaining indices. By the definition of Si, for k    Si 
we
have that 1ik(s + 1) = 1ik(s). This enables us to factor 1ik(s) and represent bI (s) as a Gram 
matrix

similar to V(s) with a correction term from the missing indices in Si.

I              1    Σ .  ckgk(s + 1)  Σ.  .                   ΣΣ

	


η  Σ .  c  g  (s + 1)  Σ.  c  g  (s)  Σ Σn

	

.         ⊥           Σ

Note that .xᵛk (s)⊥ , xiΣ = .xᵛk (s)⊥ , xᵛk (s)⊥ Σ therefore,


η  Σ .  c  g  (s + 1)  Σ.  c  g  (s)  Σ Σn

	 			

.         ⊥                     ⊥ Σ

	


Define V˜ (s) as

 1  Σ

. αc  g  (s + 1) Σ. αc  g  (s) Σ

	

.         ⊥                     ⊥ Σ

	

This matrix is identical to V(s) except for a modified scaling term .    c2 gk (s+1)gk (s)      Σ. 
We note


however that

..  ckgk(s)  Σ2   .  ckgk(s + 1)  Σ2Σ

			

ǁvk(s)ǁ₂ǁvk(s+1)ǁ₂

.  ckgk(s)  Σ.  ckgk(s + 1)  Σ

			


min

ǁv  (s)ǁ

,

ǁv  (s + 1)ǁ

≤     ǁv  (s)ǁ

ǁv  (s + 1)ǁ


≤ max

ǁv  (s)ǁ

,

ǁv  (s + 1)ǁ

because gk(s), c² are positive. Hence the matrix V˜ (s) satisfies the hypothesis of Lemma B.4 
entirely.
We write bI (s) as


i

where we have defined

ij

j=1


V˜ ⊥(s) =   1

Σ . αckgk(s) Σ. αckgk(s + 1) Σ1

(s)1

(s).xᵛk (s)⊥ , xᵛk (s)⊥ Σ.       (C.8)

We then bound the magnitude of each entry V˜ i⊥j (s):


V˜ ⊥(s) =   1

Σ . αckgk(s) Σ. αckgk(s + 1) Σ1

(s)1

(s).xvk(s)⊥ , xvk(s)⊥ Σ

(1 + R  (m/δ)¹/ᵈ)² S

.                                                                   (C.9)

m

20


Under review as a conference paper at ICLR 2020

Lastly we bound the size of the residual term bII (s),


II                 1     Σ   ckgk(s + 1) .                  T

		

T      Σ 


|bi   (s)| =   −  √m

ǁv  (s + 1)ǁ

σ(vk(s + 1)

xi) − σ(vk(s)

xi)

gk(s + 1)η|Si| · ǁ∇vk L(s)ǁ₂

mǁvk(s + 1)ǁ₂

η|Si|(1 + (m/δ)¹/ᵈRg)ǁ∇v  L(s)ǁ₂

               k                     .
α   m

Where we used the Lipschitz continuity of σ in the first bound, and took Rg  > 0 that satisfies

|gk(s + 1) − gk(0)| ≤ Rg in the second inequality. Applying the bound (E.1),


|bi   (s)| ≤

η S  √n(1 + R  (m/δ)¹/ᵈ)²  f (s)     y

α2m                         .                      (C.10)

The sum f (s + 1)     f (s) = aI (s) + aII (s) + bI (s) + bII (s) is separated into the primary term
ηp(s)  =  aI (s) + bI (s) and the residual term ηr(s)  =  aII (s) + bII (s) which is a result of the
discretization. With this, the evolution matrix Λ(s) in (C.5) is re-defined as

Λ(s) := G(s) + V˜ (s) − V˜ ⊥(s)

α2


and

f (s + 1) − f (s) = −ηΛ(s)(f (s) − y) + ηr(s).

Now we compare ǁf (s + 1) − yǁ2  with ǁf (s) − yǁ2,

2                                         2

2                                                                              2

ǁf (s + 1) − yǁ₂ =ǁf (s + 1) − f (s) + f (s) − yǁ₂

=  f (s)     y  ² + 2  f (s + 1)     f (s), f (s)     y

+ .f (s + 1) − f (s), f (s + 1) − f (s)Σ.

Substituting

f (s + 1) − f (s) = aI (s) + bI (s) + aII (s) + bII (s) = −ηΛ(s)(f (s) − y) + ηr(s)

we obtain

ǁf (s + 1) − yǁ2  =ǁf (s) − yǁ2  + 2(−ηΛ(s)(f (s) − y) + ηr(s))T(f (s) − y)

2                                 2

+ η²(Λ(s)(f (s) − y) − r(s))T(Λ(s)(f (s) − y) − r(s))

≤ǁf (s) − yǁ2  + (f (s) − y)T(−ηΛ(s) + η²Λ²(s))(f (s) − y)

+ ηr(s)T(I − ηΛ(s))(f (s) − y) + η²ǁr(s)ǁ2.

Now as λ     (Λ(s))     ω/2 and η =        ¹       , we have that

3ǁΛ(s)ǁ2

(f (s) − y)T(−ηΛ(s) + η²Λ²(s))(f (s) − y) = −η(f (s) − y)T(I − ηΛ(s))Λ(s)(f (s) − y) ≤ − 3ηω ǁf (s) 
− yǁ2.

8                     2

Next we analyze the rest of the terms and group them as q(s),

q(s) := ηr(s)T(I − ηΛ(s))(f (s) − y) + η²ǁr(s)ǁ2

2                2


By Property 1 we have

≤ ηǁr(s)ǁ₂ǁf (s) − yǁ₂ + η  ǁr(s)ǁ₂.

q(s) ≤ ηcωǁf (s) − yǁ2(1 + ηcω) ≤ 2cηωǁf (s) − yǁ2,

21


Under review as a conference paper at ICLR 2020


so that

q(s) ≤ c′ηωǁf (s) − yǁ2,

for c′ sufficiently small. Substituting, the difference f (s + 1) − y is bounded as

ǁf (s + 1) − yǁ2  ≤ ǁf (s) − yǁ2  − ηω(1 − ηǁΛ(s)ǁ₂)ǁf (s) − yǁ2  + c′ηωǁf (s) − yǁ2

2                                   2                                                               
                 2                                              2

≤ (1 − ηω(1 − ηǁΛ(s)ǁ₂) + c′ηω)ǁf (s) − yǁ2

≤ (1 − ηω/2)ǁf (s) − yǁ₂,

for well chosen absolute constant c. Hence for each s = 0, 1, . . . , K,

2                                                               2

ǁf (s + 1) − yǁ₂ ≤ (1 − ηω/2)ǁf (s) − yǁ₂

so the prediction error converges linearly.

In what comes next we prove the necessary conditions for Property 1, and define the appropriate ω

for the V and G dominated regimes, in order to show λmin(Λ(s)) ≥ ω/2.

Proof of Theorem 4.1:

To prove convergence we would like to apply Theorem C.1 with ω/2 =   λ0   . To do so we need to

show that m = Ω.n⁴ log(n/δ)/λ⁴Σ guarantees that Property 1 holds and that                           
           .

2α2

λmin(Λ(s)) ≥ λ₀/2α²

For finite-step gradient training, take


         αλ₀         

R   =                          ,    R

         λ₀         

=                        .                          (C.11)

ᵛ     192n(m/δ)1/d           g       96n(m/δ)1/d

Note the residual r(s) and the other terms bI (s), bII (s) depend on the sets Si that we define here
using Rv. We make the assumption that   vk(s)     vk(0)  ₂      Rv and  gk(s)     gk(0)      Rg for 
all k
and that s = 0, 1, . . . K + 1, this guarantees that bI (s) and Λ(s) are well defined. Applying 
Lemmas
B.1, B.4 with Rv, Rg defined above, we have that λmin(V˜ (s)) ≥  5λ0 . Then the least eigenvalue of

the evolution matrix Λ(s) is bounded below


λmin

(Λ(s)) = λ

≥ λ

min

min

G(s) + V˜ (s) − V˜ ⊥(s)

α2

V˜ (s) − V˜ ⊥(s)

α2

=  λmin(V˜ (s) − V˜ ⊥(s))

α2

≥  5λ₀  −  ǁV˜ ⊥(s)ǁ₂ .

The first inequality holds since G(s) > 0 and the last inequality is since λmin(V˜ (s)) ≥  5λ0 .


To show λ

min

(Λ(s)) ≥   λ0    we bound ǁV˜ ⊥(s)ǁ₂

≤  λ0 . By (C.9), we have


˜⊥              (1 + Rg(m/δ)¹/ᵈ)|Si|

1/d  . √2R˜v

16 log(n/δ) Σ

Substituting Rv, Rg and m, a direct calculation shows that

|V˜ ⊥(s)| ≤  λ0 ,

ij               8n

which yields


ǁV˜ ⊥(s)ǁ₂

≤ ǁV˜ ⊥(s)ǁF

λ₀

≤   8  .


Hence λ

min

(Λ(s)) ≥   λ0    for iterations s = 0, 1, . . . K.

22


Under review as a conference paper at ICLR 2020

We proceed by showing the residual r(s) satisfies property 1. Recall r(s) is written as


r(s) =

aII (s)

η

bII (s)

+             .

η

and Property 1 states that ǁr(s)ǁ   ≤          ǁf (s) − yǁ   for sufficiently small absolute 
constant c < 1.

This is equivalent to showing that both aII (s), bII (s) satisfy


ǁa   (s)ǁ₂ ≤
ǁb   (s)ǁ₂ ≤

cηλ

α2    ǁf (s) − yǁ₂,                                    (C.12)

cηλ

α2    ǁf (s) − yǁ₂.                                    (C.13)

We consider each term at turn. By (C.10),

ǁbII (s)ǁ₂ ≤ √n max bII (s)


max

i

ηn(1 + Rg(m/δ)¹/ᵈ)²|Si|ǁf (s) − yǁ₂

α2m

CmRvηnǁf (s) − yǁ₂

α2m

≤  λ₀ηǁf (s) − yǁ₂  · nCR  .

In the above we used the values of Rv, Rg defined in (C.11) and applied Lemma C.1 in the third
inequality. Taking m = Ω  n⁴ log(n/δ)/λ⁴   with large enough constant yields


ǁb   (s)ǁ₂

cλ₀ηǁf (s) − yǁ₂ .
α2

Next we analogously bound ǁaII (s)ǁ via the bound (C.7),

ǁaII (s)ǁ₂ ≤ √n max aII (s)

η²n³/².1 + R  (m/δ)¹/ᵈΣ3ǁf (s) − yǁ2(m/δ)¹/ᵈ

ηλ ǁf (s) − yǁ      η.1 + R  (m/δ)¹/ᵈΣ3n³/²ǁf (s) − yǁ  (m/δ)¹/ᵈ

₂√

ηλ₀ǁf (s) − yǁ₂     η    Cn     log(n/δ)

≤ cηωǁf (s) − yǁ₂.

In  the  above  we  applied  Lemma  B.9  once  again.     The  last  inequality  holds  since  m   
=

Ω(n⁴ log(n/δ)/λ⁴)  and  η  =  O.    α2       Σ,  hence  r(s)  satisfies  Property  1.   Now  since  
Theo-

rem C.1 holds with ω = λ₀/α² we have that the maximum parameter trajectories are bounded as
vk(s)    vk(0)  ₂      Rv and   gk(s)    gk(0)       Rg for all k and every iteration s = 0, 1, . . 
. , K + 1
via Lemmas C.2, C.3.

To finish the proof, we apply the same contradiction argument as in Theorems B.1, B.2, taking the
first iteration s = K₀ where one of Lemmas C.2, C.3 does not hold. We note that K₀ > 0 and by
the definition of K₀, for s = 0, 1, . . . , K₀     1 the Lemmas C.2, C.3 hold which implies that by 
the
argument above we reach linear convergence in iteration s = K₀. This contradicts one of Lemmas
C.2, C.3 which gives the desired contradiction, so we conclude that we have linear convergence with
rate λ₀/2α² for all iterations.

Proof of Theorem 4.2:

For G-dominated convergence, we follow the same steps as in the proof of Theorem 4.1. We redefine
23


Under review as a conference paper at ICLR 2020

the trajectory constants for the finite step case


R˜v

√2παµ₀

:=  64n(m/δ)1/d

,    Rg

:=           µ0              .

48n(m/δ)1/d

To use Theorem C.1 we need to show that m = Ω  n⁴ log(n/δ)/α⁴µ⁴   guarantees Property 1, and
that λmin(Λ(s))     µ₀/2. We again note that the residual r(s) and bI (s), bII (s) depend on the 
sets
Si that we define here using R˜v  above as Si := Si(R˜v ).

We start by showing the property on the least eigenvalue.  We make the assumption that we have
linear convergence with ω/2 = µ₀/2 and step-size η for iterations s = 0, . . . K so that Lemmas 
C.2,

C.3 hold. Via an analogous analysis of the continous case we reach that m = Ω  n⁴ log(n/δ)/µ⁴α⁴


implies

ǁvk(s) − vk(0)ǁ₂ ≤

16α√nǁf (0) − yǁ₂

α   mµ₀

≤ R˜v ,    |gk(s) − gk(0)| ≤

8√nǁf (0) − yǁ₂

mµ₀

≤ Rg.

for s = 0, . . . K + 1 by Lemmas C.2, C.3 and that Λ(s), bI (s) are well defined. Using the bounds
on the parameter trajectories, Lemma B.5 and R˜v  defined above yield λmin(G(s)) ≥  5µ0 . The least

eigenvalue of the evolution matrix Λ(s) is bounded below as


λmin

(Λ(s)) = λ

min

G(s) + V˜ (s) − V˜ ⊥(s)

α2

≥ λmin(G(s)) − ǁV˜ ⊥(s)ǁ₂

since V˜ (s)     0 and α    1. We bound the spectral norm of   V˜ ⊥(s)  ₂, for each entry i, j we 
have by
(C.9) that


|V˜ i⊥j (s)| ≤

(1 + Rg(m/δ)¹/ᵈ)|Si|

m


1/d  . √2R˜v

16 log(n/δ) Σ


≤ (1 + Rg(m/δ)

8R˜v

≤  √2πα

µ₀

≤  8n.

)    √πα  +        3m

where in the above inequalities we used our bounds on R˜v , Rg and m.  Then the spectral norm is
bounded as

ǁV˜ ⊥(s)ǁ₂ ≤ ǁV˜ ⊥(s)ǁF ≤ µ₀/8.

Hence we have that λmin(Λ(s)) ≥ µ₀/2 for s = 0, 1, . . . K.

Next we show the residual r(s) satisfies Property 1. Recall r(s) is written as


r(s) =

aII (s)

η

bII (s)

+             .

η

Property 1 states the condition   r(s)  ₂      cωη  f (s)     y  ₂ for sufficiently small c < 1 
with ω = µ₀.
This is equivalent to showing that both aII (s), bII (s) satisfy that

II

ǁa   (s)ǁ₂ ≤ cηµ₀ǁf (s) − yǁ₂,                                    (C.14)

ǁb   (s)ǁ₂ ≤ cηµ₀ǁf (s) − yǁ₂,                                    (C.15)

for sufficiently small absolute constant c. For bII (s) we have that (C.10) gives

ǁbII (s)ǁ₂ ≤ √n max bII (s)


max

i

η(1 + Rg(m/δ)¹/ᵈ)²|Si|nǁf (s) − yǁ₂

α  m

24


Under review as a conference paper at ICLR 2020

Next applying Lemmas C.1 and B.9 in turn yields

CmR˜v ηnǁf (s) − yǁ₂

α2m

R˜v

≤ ηµ₀ǁf (s) − yǁ₂ nα2

Substituting m = Ω  n⁴ log(n/δ)/µ⁴α⁴   for a large enough constant and Rv we get

ǁb   (s)ǁ₂ ≤ cηµ₀ǁf (s) − yǁ₂.

Analogously we bound ǁaII (s)ǁ₂ using (C.7),

ǁaII (s)ǁ₂ ≤ √n max aII (s)

η²n³/².1 + R  (m/δ)¹/ᵈΣ3ǁf (s) − yǁ2(m/δ)¹/ᵈ

η.1 + R  (m/δ)¹/ᵈΣ3n³/²ǁf (s) − yǁ  (m/δ)¹/ᵈ


η

≤ ηµ₀ǁf (s) − yǁ₂ ·  α2  ·

≤ cηµ₀ǁf (s) − yǁ₂.

Cn²    log(n/δ)

α2µ²√m

Where we have used Lemma B.9 in the third inequality and substituted m = Ω(n⁴ log(n/δ)/α⁴µ⁴)


noting that

η  =  O

     ¹         and that

ǁΛ(s)ǁ2

α ≥  1

in the last inequality.  Therefore we have that

0

r(s)

satisfies Property 1 so that Theorem C.1 holds. By the same contradiction argument as in Theorem

4.1 we have that this holds for all iterations.

D    ADDITIONAL  TECHNICAL  LEMMAS  AND  PROOFS  OF  LEMMAS  FROM

APPENDIX  B

Proof of Lemma 4.1:

We prove Lemma 4.1 for V∞, G∞ separately. V∞ can be viewed as the covariance matrix of the
functionals φi defined as


φ (v) = x .I −  vvT Σ1{vTx

≥ 0}                                             (D.1)

over the Hilbert space     of L²(N (0, α²I)) of functionals. Under this formulation, if φ₁, φ₂, . . 
. , φn
are linearly independent, then V∞ is strictly positive definite.  Thus, to show that V∞ is strictly
positive definite is equivalent to showing that

c₁φ₁ + c₂φ₂ + · · · + cnφn = 0  in V                                               (D.2)

implies ci  =  0 for each i.  The φis are piece-wise continuous functionals, and equality in      is
equivalent to

c₁φ₁ + c₂φ₂ + · · · + cnφn = 0  almost everywhere.

For the sake of contradiction, assume that there exist c₁, . . . , cn that are not identically 0, 
satisfying


contradiction by constructing a non-zero measure region such that the linear combination
non-zero.

Denote the orthogonal subspace to xi as Di := {v ∈ Rd  : vTxi = 0}. By Assumption 1,

Di /⊆        Dj

j/=i

i ciφi is

25


Under review as a conference paper at ICLR 2020


spaces Di ∩ Dj of dimension d − 2 (since xi and xj are not parallel). Thus, take z ∈ Di\

Since    j/=i Dj is closed in R  , there exists an R > 0 such that

B(z, 4R) ∩       Dj = Ø.

j/=i

j/=i Dj .

Next take y ∈ ∂B(z, 3R) ∩ Di (where ∂ denotes the boundary) on the smaller disk of radius 3R so
that it satisfies ǁyǁ₂ = maxy'∈∂B(z,3R)∩Di  ǁy′ǁ₂. Now for any r ≤ R, the ball B(y, r) is such that
for all points v ∈ B(y, r) we have ǁvˣ⊥i  ǁ₂ ≥ 2R and ǁvˣi ǁ₂ ≤ R. Then for any r ≤ R, the points
v ∈ B(y, r) ⊂ B(z, 4R) satisfy that

ǁx    ǁ   ≥ ǁx ǁ   −              ≥ ǁx ǁ  .1 −        Σ ≥             .

xi · v                          R         ǁxiǁ₂


i      2               i   2

ǁvǁ₂

ⁱ  ²           2R             2

Next we decompose the chosen ball    (y, r) = B⁺(r)    B−(r) to the areas where the ReLU function
at the point xi is active and inactive

B⁺(r) = B(y, r) ∩ {xTi  v ≥ 0},    B−(r) = B(y, r) ∩ {xiTv < 0}.

Note that φi has a discontinuity on Di and is continuous within each region B⁺(r) and B−(r).
Moreover, for j = i, φj is continuous on the entire region of    (y, r) since    (y, r)        (z, 
4R)     Dᶜ.
Since we have that φj is continuous in the region, the Lebesgue differentiation theorem implies that
for r → 0, φi satisfies on B⁺(r), B−(r):


lim         1       

r→0  µ(B+(r))

φi

B+(r)

= xʸ⊥

0,    lim         1       

r→0  µ(B−(r))

φi

B−(r)

= 0.

For j = i φj is continuous on the entire ball    (y, r) hence the Lebesgue differentiation theorem 
also
gives


lim         1        ∫

φ  = φ (y),    lim         1        ∫

φ  = φ (y).

 

We integrate c₁φ₁ + . . . cnφn over B−(r) and B⁺(r) separately and subtract the integrals. By the
assumption, c₁φ₁ +       + cnφn  =  0 almost everywhere so each integral evaluates to 0 and the
difference is also 0,


0 =         1       

µ(B+(r))

B+(r)

c₁φ₁

+ · · · + cnφn

       1       

µ(B−(r))

B−(r)

c₁φ₁

+ · · · + cn

φn.       (D.3)

By the continuity of φj for j /= i taking r → 0 we have that


       1         lim ∫

φ  −         1        ∫

φ  = φ (y) − φ (y) = 0.

 

For φi the functionals evaluate differently. For B−(r) we have that


       1         lim

µ(B−(r)) r→0

φi

B−(r)

=         1         lim

µ(B−(r)) r→0

B−(r)

0 = 0,

while the integral over the positive side, B⁺(r) is equal to


       1        ∫

φ (z)dz =         1        ∫

xᶻ⊥ dz = xʸ⊥ .

By construction,   xʸ⊥    2  > R and is non-zero so we conclude that for (D.3) to hold we must have
ci = 0. This gives the desired contradiction and implies that φ₁, . . . φn are independent and V∞ is
positive definite with λmin(V∞) = λ₀.

26


Under review as a conference paper at ICLR 2020

Next we consider G∞ and again frame the problem in the context of the covariance matrix of
functionals. Define

θ (v) := σ. vTxi Σ

ǁvǁ₂

for v /= 0. Now the statement of the theorem is equivalent to showing that the covariance matrix of

{θi} does not have zero-eigenvalues, that is, the functionals θis are linearly independent. For the 
sake
of contradiction assume ∃ c₁, . . . , cn such that

c₁θ₁ + c₂θ₂ + · · · + cnθn = 0  in V   (equivalent to a.e).

Via the same contradiction argument we show that ci = 0 for all i. Unlike φi defined in (D.1), each

θi is continuous and non-negative so equality “a.e” is strengthened to “for all v”,

c₁θ₁ + c₂θ₂ + · · · + cnθn = 0.

Equality everywhere requires that the derivatives of the function are equal to 0 almost everywhere.

Computing derivatives with respect to v yields

c₁xᵛ⊥ 1{vTx₁ ≥ 0} + c₂xᵛ⊥ 1{vTx₂ ≥ 0} + · · · + cnxᵛ⊥ 1{vTxn ≥ 0} = 0.


1

Which coincide with

2                                                                     n

c₁φ₁ + · · · + cnφn

By the first part of the proof, the linear combination c₁φ₁ +       + cnφn is non-zero around a ball
of positive measure unless ci = 0 for all i. This contradicts the assumption that the derivative is 
0
almost everywhere; therefore G∞ is strictly positive definite with λmin(G∞) =: µ₀ > 0.

We briefly derive an inequality for the sum of indicator functions for events that are bounded by
the sum of indicator functions of independent events.  This enables us to develop more refined
concentration than in Du et al. [15] for monitoring the orthogonal and aligned Gram matrices during
training.

Then with probability 1 − δ, S satisfies

S     p  2 + 8 log(1/δ)   .

3mp


Proof of Lemma D.1:

Bound S as

m

S =             1Ak

m

k=1

m

1Bk .

m

k=1


We apply Bernstein’s concentration inequality to reach the bound.  Denote Xk

1Bk

m

and S˜  =


m

k=1

Xk. Then

Var(X  ) ≤ EX² = (1/m)²P(X  ) + 0 ≤    p

,   ES˜ = E Σ X

≤ p.

Applying Bernstein’s inequality yields


P  ˜     E ˜

.         −t²/2          Σ

		


(S −    S ≥ t) ≤ exp

Σm     EX² +   ᵗ     .

Fix δ and take the smallest t such that P(S˜ − ES˜ ≥ t) ≤ δ. Denote t = r · ES˜, either P(S˜ − ES˜ 
≥

ES˜)     δ, or t = rES˜ corresponds to r     1. Note that t = rES˜     rp. In the latter case, the 
bound is
written as


P(S˜ −

ES˜ ≥ rp) ≤ exp

−(pr)²/2

p/m +  ᵖʳ

≤ exp

−(pr)²/2

ᵖ (1 + ʳ )

≤ exp

−(pr)²/2

p ( 4r )

= exp

. −3prm Σ.

3m                        m           3                                   m    3

27


Under review as a conference paper at ICLR 2020


Solving for δ gives

Hence with probability 1 − δ,

rp ≤

8 log(1/δ)

.

3m

S ≤ S˜ ≤ max .p.1 + 8 log(1/δ) Σ, 2pΣ ≤ p.2 + 8 log(1/δ) Σ.

Proof of Lemma B.1:

We prove the claim by applying concentration on each entry of the difference matrix.  Each entry

Vij(0) is written as


Vij(0) =

 1  Σ .xvk(0)⊥ , xvk(0)

⊥ Σ. αck · gk

2

1ik(0)1jk(0).


m          i                 j
k=1

ǁvkǁ₂

At initialization gk(0) = ǁvk(0)ǁ₂/α, c²  = 1 so Vij(0) simplifies to

m

V   (0) =   1  Σ .xᵛk (0)⊥ , xᵛk (0)⊥ Σ1  (0)1   (0).

Since  the  weights  vk(0)  are  initialized  independently  for  each  entry  we  have  EvVij(0)   
=

Vi∞j .     We  measure  the  deviation  V(0)  −  V∞  via  concentration.     Each  term  in  the  
sum


 1   Σm

.xᵛk (0)⊥ , xᵛk (0)⊥ Σ1ik(0)1jk(0) is independent and bounded,


m      j=1        i

j

−1 ≤ .xᵛk (0)⊥ , xᵛk (0)⊥ Σ1ik(0)1jk(0) ≤ 1.

Applying Hoeffding’s inequality to each entry yields that with probability 1 − δ/n², for all i, j,


|Vij(0) − Vi∞j | ≤

2    log(n2/δ)

√m       .

Taking a union bound over all entries, with probability 1 − δ,       


|Vij(0) − Vi∞j | ≤

Bounding the spectral norm, with probability 1 − δ,

4    log(n/δ)

√m      .


ǁV(0) − V∞ǁ2  ≤ ǁV(0) − V∞ǁ2

≤ Σ |Vij(0) − V∞|2


2                                            F

16n² log(n/δ)

ij

i,j

.

m

Taking m = Ω. n2 log(n/δ) Σ therefore guarantees


ǁV(0) − V∞ǁ₂

λ₀

≤   4  .

Proof of Lemma B.2:

This is completely analogous to B.1. Recall G(0) is defined as,

m


G   (0) =   1  Σ .xᵛk (0), xᵛk (0)Σc² 1

(0)1

(0)

28


Under review as a conference paper at ICLR 2020

with c²  = 1 and vk(0)      N (0, α²I) are initialized i.i.d.  Since each term is bounded like B.1 
the
same analysis gives

∞  ₂      16n² log(n/δ)

ǁGij(0) − Gij ǁ₂ ≤             m         .

Taking m = Ω. n2 log(n/δ) Σ therefore guarantees,


ǁG(0) − G∞ǁ₂

µ₀

≤   4  .

Proof of Lemma B.3:

For a given R, define the event of a possible sign change of neuron k at point xi as

Ai,k(R) = {∃v : ǁv − vk(0)ǁ₂ ≤ R, and 1{vk(0)Txi ≥ 0} = 1{vTxi ≥ 0}}

Ai,k(R) occurs exactly when |vk(0)Txi| ≤ R, since ǁxiǁ₂ = 1 and the perturbation may be taken
in the direction of −xi. To bound the probability Ai,k(R) we consider the probability of the event

P(Ai,k(R)) = P(|vk(0)Txi| < R) = P(|z| < R).

Here, z      N (0, α²) since the product vk(0)Txi follows a centered normal distribution. The norm
of   xi  ₂ = 1 which implies that z computes to a standard deviation α. Via estimates on the normal
distribution, the probability on the event is bounded like

2R

P(Ai,k(R)) ≤  α√2π .

We use the estimate for P(Ai,k(R)) to bound the difference between the surrogate Gram matrix and
the Gram matrix at initialization V(0).

Recall the surrogate Vˆ (t) is defined as,

m

Vˆ    (t) =   1  Σ .xᵛk (t)⊥ , xᵛk (t)⊥ Σ1  (t)1   (t).


ij

Thus for entry i, j we have

ˆ                                  1  Σ

m

.  vk(t)

k=1

⊥

i                 k

vk(t)⊥ Σ

ik         jk

vk(0)⊥

vk(0)⊥


|Vij(t) − Vij(0)| =   m

k=1

xi          , xj

1ik(t)1jk(t) − ⟨xi           , xj           ⟩1ik(0)1jk(0) 

This sum is decomposed into the difference between the inner product and the difference in the
rectifier patterns terms respectively:

i               j                       i                j

Define

Y ᵏ = ..xᵛk (t)⊥ , xᵛk (t)⊥ Σ − .xᵛk (0)⊥ , xᵛk (0)⊥ ΣΣ.1ik(t)1jk(t)Σ,


Then

ij               i                 j

   1  Σ

	

   1  Σ              1  Σ

		


|Vˆ ij (t) − Vij(0)| =  

 

Y k + Zk   ≤               k                    k

		


To bound | 1   Σm

Y ᵏ| we bound each |Y ᵏ| as follows.

29


Under review as a conference paper at ICLR 2020

				

i                 j                       i                 j

			


ǁvk(t)ǁ₂

ǁvk(t)ǁ₂

ǁvk(t)ǁ₂

ǁvk(t)ǁ₂


ǁvk(t)ǁ₂

 

ǁvk(t)ǁ₂


 ǁvk(0)ǁ₂    ǁvk(0)ǁ₂

ǁvk(t)ǁ₂      ǁvk(0)ǁ₂       ǁvk(t)ǁ₂


ǁvk(t)ǁ₂

ǁvk(0)ǁ₂       ǁvk(0)ǁ₂


  ¨     ǁvk(t)ǁ₂      ǁvk(0)ǁ₂

ǁvk(t)ǁ₂  

ǁvk(t)ǁ₂

ǁvk(0)ǁ₂

ǁvk(t)ǁ₂  

   vk(t)            vk(0)   

≤ 2                   −                        .


   1  Σ   k          2  Σ

¨    vk(t)   

	

   vk(0)    ¨

	 


  m      Yij   ≤  m

¨ǁv  (t)ǁ

−  ǁv  (0)ǁ   ¨

4Rv(2m/δ)¹/ᵈ

≤              α

8Rv(m/δ)¹/ᵈ

,

α

where the first inequality follows from Lemma B.10.   Note that the inequality holds with high
probability 1 − δ/2 for all i, j.


For the second sum, |  1   Σm     Zᵏ | ≤   1   Σm

1A   (R)  +  1   Σm

1A   ₍R₎ so we apply Lemma

D.1 to get, with probability 1 − δ/2n²


   1  Σ   k           2Rv  .

2√2πα log (2n²/δ) Σ


  m k=1

Zij   ≤  α√2π   2 +

8Rv

≤  α√2π

3mRv


probability 1 −    δ 2 ,

ˆ

αλ0

  8Rv

8Rv(m/δ)¹/ᵈ

ij              ij

12Rv(m/δ)¹/ᵈ

|Vij(t) − Vij(0)| ≤  α√2π +           α          ≤              α          .


Taking a union bound, with probability 1 − δ/2,                       

.Σ

12nRv(m/δ)¹/ᵈ

30


Under review as a conference paper at ICLR 2020

Bounding the spectral norm by the Frobenous norm,


ǁVˆ (t) − V(0)ǁ₂ ≤

Taking Rv =         αλ0           gives the desired bound.

12nRv(m/δ)¹/ᵈ

.

α


ǁVˆ (t) − V(0)ǁ₂

λ₀

≤   8  .

Proof of Lemma B.4:

To bound ǁV(t) − V(0)ǁ₂ we now consider ǁV(t) − Vˆ (t)ǁ₂. The entries of Vij(t) are given as


 1  Σ .         ⊥                    ⊥ Σ

		

. αc  · g    Σ2

The surrogate Vˆ (t) is defined as

m


Vˆ    (t) =   1  Σ .xᵛk (t)⊥ , xᵛk (t)⊥ Σ1

(t)1

(t).

The only difference is in the second layer terms. The difference between each entry is written as


ˆ                 1  Σ

.  vk(t)⊥

vk(t)⊥ Σ

.. αck

· gk Σ2          Σ 


|Vij(t) − Vij(t)| =   m

xi          , xj

1ik(t)1jk(t)

ǁv  (t)ǁ         − 1

k=1                                                                                             k   
       2

α²gk(t)²

≤ 1max      ǁ            2  − 1   .

≤k≤m       vk (t)ǁ2

Write 1 =  α2 g2 (0)  , since ǁv  (t)ǁ   is increasing in t according to (2.3)

ǁvk(0)ǁ2                       k          2


α²gk(t)²

α²gk(t)²

α²gk(0)²

1/d

1/d


ǁv  (t)ǁ2  − 1 =  ǁv  (t)ǁ2  −  ǁv  (0)ǁ2   ≤ 3Rg (m/δ)

+ 3Rv(m/δ)

/α.

The above inequality is shown by considering different cases for the sign of the difference gk(t)
gk(0). Now


 .                                 Σ.

α²g  (t)²      α²g  (0)²               αgk(t)          αgk(0)           αgk(t)  

  αgk(0)   Σ 

₂  −                 ₂                          +                                       −

ǁvk(t)ǁ₂      ǁvk(0)ǁ₂           ǁvk(t)ǁ₂      ǁvk(0)ǁ₂       ǁvk(t)ǁ₂      ǁvk(0)ǁ₂


ǁvk(0)ǁ₂           ǁvk(0)ǁ₂       ǁvk(t)ǁ₂

ǁvk(0)ǁ₂

 

ǁvk(t)ǁ₂      ǁvk(0)ǁ₂


ǁvk(0)ǁ₂           ǁvk(0)ǁ₂     ǁvk(0)ǁ₂ + Rv

ǁvk(0)ǁ₂  

≤ (2 + Rg(m/δ)¹/ᵈ) max .Rg(m/δ)¹/ᵈ, Rg(m/δ)¹/ᵈ + Rv(m/δ)¹/ᵈ/αΣ


≤ 3Rg(m/δ)

1/d

+ 3Rv(m/δ)

1/d

/α,

where the second inequality holds due to Lemma B.10 with probability 1 − δ over the initialization.
Hence:

ǁVˆ (t) − V(t)ǁ₂ ≤ ǁVˆ (t) − V(t)ǁF =            |Vˆ ij (t) − Vij(t)|2  ≤ 3nRg(m/δ)¹/ᵈ + 
3nRv(m/δ)¹/ᵈ/α.

i,j

31


Under review as a conference paper at ICLR 2020


Substituting Rv, Rg gives

ǁVˆ (t) − V(t)ǁ₂

λ₀

≤   8  .

Now we use Lemma B.3 to get that with probability 1 − δ

λ₀

ǁV(t) − V(0)ǁ₂ ≤   8


combining we get with probability 1 − δ

ǁV(t) − V(0)ǁ₂ ≤

λ₀ .

4

We note that the source for all the high probability uncertainty 1     δ all arise from 
initialization and
the application of Lemma B.10.

Proof of Lemma B.5:

To prove the claim we consider each entry i, j of G(t) − G(0). We have,


   1  Σ   . v  (t)Tx  Σ  . v  (t)Tx  Σ       . v  (0)Tx  Σ

	

						

. v  (0)Tx  Σ 

	


|Gij(t) − Gij(0)| =   m

σ     k            i    σ

ǁv  (t)ǁ

k            j

ǁv  (t)ǁ

− σ   ǁv  (0)ǁ      σ

k            j

ǁv  (0)ǁ


1   Σ

. v  (t)Tx  Σ

. v  (t)Tx  Σ

. v  (t)Tx  Σ

. v  (0)Tx  Σ 


≤  m        σ

k            i    σ

ǁv  (t)ǁ

k            j

ǁv  (t)ǁ

− σ   ǁv  (t)ǁ      σ

k            j

ǁv  (0)ǁ


1   Σ

. v  (t)Tx  Σ

. v  (0)Tx  Σ

. v  (0)Tx  Σ

. v  (0)Tx  Σ 


+              σ     ᵏ

i    σ     ᵏ              − σ

i    σ     k            j


m  k=1

vk(t)

ǁvk(t)ǁ₂

   vk(0)   

ǁvk(0)ǁ₂

2R˜v (m/δ)¹/ᵈ

ǁvk(0)ǁ₂

ǁvk(0)ǁ₂

≤ 2¨ǁv  (t)ǁ    −  ǁv  (0)ǁ   ¨   ≤             α         .


   vk(0)   

   vk(t)   

ǁvk(t) − vk(0)ǁ₂

(m/δ)¹/ᵈ


¨ǁv  (0)ǁ

−  ǁv  (t)ǁ   ¨   ≤

ǁv  (0)ǁ

≤         α      ǁvk(t) − vk(0)ǁ₂,

where the first inequality uses that   vk(0)  ₂        vk(t)  ₂ and is intuitive from a geometrical 
stand-

point. Hence,


ǁG(t) − G(0)ǁ₂ ≤ ǁG(t) − G(0)ǁF =

.Σ |Gij(t) − Gij(0)|2  ≤

2nR˜v (m/δ)¹/ᵈ

√            .


i,j

α   2π


˜          √2παµ₀

8n(m/δ)1/d

gives the desired bound. Therefore, with probability 1 − δ,

µ₀

ǁG(t) − G(0)ǁ₂ ≤   4  .

Now that we have established bounds on V(t), G(t) given that the parameters stay near 
initialization,

we show that the evolution converges in that case:

Proof of Lemma B.6:

Consider the squared norm of the predictions ǁf (t) − yǁ2.  Taking the derivative of the loss with

respect to time,

d                2                                         T.            V(t) Σ

ǁf (t) − yǁ   = −2(f (t) − y)      G(t) +             (f (t) − y).

32


Under review as a conference paper at ICLR 2020


Since we assume that λ

.G(t) + V⁽ᵗ⁾ Σ ≥  ω , the derivative of the squared norm is bounded as


min

α2                      2

d                2                                         2

dt ǁf (t) − yǁ₂ ≤ −ωǁf (t) − yǁ₂.

Applying an integrating factor yields

ǁf (t) − yǁ₂ exp(ωt) ≤ C.

Substituting the initial conditions, we get

2                                                           2

ǁf (t) − yǁ₂ ≤ exp(−ωt)ǁf (0) − yǁ₂.

For now, assuming the linear convergence derived in Lemma B.6, we bound the distance of the
parameters from initialization.  Later we combine the bound on the parameters and Lemmas B.4,

B.5 bounding the least eigenvalue of Λ(t), to derive a condition on the over-parameterization m and
ensure convergence from random initialization.

Proof of Lemma B.7:

Denote f (xi) at time t as fi(t). Since ǁxᵛk (t)⊥ ǁ₂ ≤ ǁxiǁ₂ = 1, we have that

¨ dvk(t) ¨      ¨ Σ                    1                     1         v⊥                 ¨

				


¨    dt    ¨

¨      (yi − fi(t)) √mckgk(t) ǁv  (t)ǁ

xi    1ik(t)


  1  

≤  √m

 ckgk(t) 

|yi − fi(t)| ǁv  (t)ǁ   .

 ǁvk(t)ǁ₂  

 ǁvk(t)ǁ₂          k      ǁvk(0)ǁ₂             α             ᵍ      ᵏ           ²

By Lemma B.10, we have that with probability 1 − δ over the initialization,

α/ǁvk(0)ǁ₂ ≤ C(m/δ)¹/ᵈ.

Hence αRg/ǁvk(0)ǁ₂ ≤ 1. This fact bounds   ckgk (t)    with probability 1 − δ for each k,

 ǁv  (t)ǁ     ≤ 2/α.

Substituting the bound,

¨ d        ¨           2     Σ


vk(t)

dt

≤  α√m

2√n

i=1

|fi(t) − yi|

≤  α√m ǁf (t) − yǁ₂

2√n

≤  α√m exp(−ωt/2)ǁf (0) − yǁ₂.

Thus, integrating and applying Jensen’s inequality,

∫  s ¨ dvk(s) ¨          4√nǁf (0) − yǁ₂

	

Note that the condition |gk(t) − gk(0)|  ≤  Rg is stronger than needed and merely assuring that

|gk(t) − gk(0)| ≤ 1/(m/δ)¹/ᵈ suffices.

33


Under review as a conference paper at ICLR 2020

Analogously we derive bounds for the distance of gk from initialization.

Proof of Lemma B.8:

Consider the magnitude of the derivative ᵈᵍk ,

  dg            1   Σ                 c    

 		

k            T

  =  √          (fj − yj)           σ(v   xj) .

     ck     σ(vTx  )  =  σ. vkTxj  Σ  ≤ 1


dgk(t)

2√n

2√n


dt     ≤

√m ǁf (t) − yǁ₂ ≤

√m exp(−ωt/2)ǁf (0) − yǁ₂,


∫  t   dgk

∫  t  2√n

4√nǁy − f (0)ǁ₂


Proof of Lemma B.9:

Consider the ith entry of the network at initialization,

  1   Σ

. g  vTx  Σ

Since the network is initialized randomly and m is taken to be large we apply concentration to 
bound

f (0) for each i.  Define z   =  c  σ   ᵍk (0)vk (0)Txi      .  Note that z   are independent 
sub-Gaussian

ǁvk(0)ǁ₂


random variables with

ǁzkǁψ ≤ ǁN (0, 1)ǁψ = C.

Here       ψ denotes the 2-sub-Gaussian norm, (see [31] for example). Applying Hoeffding’s 
inequality
bounds fi(0) as


P(|

    

mfi(0)| > t) ≤ 2 exp

−  Σm

t²/2

ǁz  ǁ


= 2 exp

k=1

t2

.

2mC

k   ψ2

Which gives with probability 1 − δ/n that                               

|fi(0)| ≤ C˜    log (n/δ).

Now with probability 1 − δ we have that, for each i,

|fi(0) − yi| ≤ |yi| + C˜    log(n/δ) ≤ C₂    log(n/δ).

Since yi = O(1). Hence, with probability 1 − δ,                            

ǁf (0) − yǁ₂ ≤ C√n log(n/δ).

Proof of Lemma B.10:

At initialization vk      N (0, α²I) so the norm behaves like   vk(0)  ²       α²χd.  The cumulative
density of a chi-squared distribution with d degrees of freedom behaves like F (x) = Θ(xᵈ/²) for

34


Under review as a conference paper at ICLR 2020

small x so we have that with probability 1 −  δ , that ǁv  (0)ǁ    ≥  α(m/δ)    where d is the 
input

dimension. Applying a union bound, with probability 1 − δ, for all 1 ≤ k ≤ m,


       1       

ǁv  (0)ǁ    ≤

.m/δΣ 1/d

Now by (2.3) for t ≥ 0, ǁvk(t)ǁ₂ ≥ ǁvk(0)ǁ₂ so


      1      

ǁv  (t)ǁ

       1       

≤  ǁv  (0)ǁ    ≤

.m/δΣ 1/d

E    PROOFS  OF  LEMMAS  FROM  APPENDIX  C

Proof of Lemma C.1:

Fix  R,  without  the  loss  of  generality  we  write  Si for  Si(R).   For  each  k,  vk(0)  is  
initialized
independently via     N (0, α²I), and for a given k, the event 1ik(0) = 1  vTxi     0   corresponds 
to
vk(0)Txi      R. Since   xi  ₂ = 1, vk(0)Txi     N (0, α²). Denoting the event that an index k    
Si
as       Ai,k, we have


Next the cardinality of Si is written as

2R

P(Ai,k) ≤  α√2π .

m

|Si| =        1Ai,k .

k=1


Applying Lemma D.1, with probability 1 − δ/n,

2mR

|Si| ≤  α√2π +

16 log(n/δ)

.

3

Taking a union bound, with probability 1 − δ, for all i we have that


2mR

|Si| ≤  α√2π +

16 log(n/δ)

.

3

Proof of Lemma C.2:

To show this we bound the difference gk(s)     gk(0) as the sum of the iteration updates. Each 
update
is written as

  ∂L(s)           1   Σ                        c      

As  c  σ   ᵛk (s)Txi                 1,

ǁvk(s)ǁ₂

  ∂L(s)         1   Σ                       √n

	

By the assumption in the statement of the lemma,


  ∂L(s)  

√n(1 −  ηω )ˢ/²ǁf (0) − yǁ₂

35


Under review as a conference paper at ICLR 2020

Hence bounding the difference by the sum of the gradient updates:


Σ   ∂L(s)  

4η√nǁf (0) − yǁ₂ Σ

ηω  s/2


|gk(K + 1) − gk(0)| ≤ η

s=0

∂gk    ≤

√m

s=0

(1 −   2  )     .

The last term yields a geometric series that is bounded as

1                  4


Hence

1 − √1 −  ηω

|gk(K + 1) − gk(0)| ≤

≤  ηω ,

4√nǁf (0) − yǁ₂

ω   m

Proof of Lemma C.3:

To show this we write vk(s) as the sum of gradient updates and the initial weight vk(0). Consider
the norm of the gradient of the loss with respect to vk,

¨   1   Σ                   c  g  (s)                      ⊥ ¨

 		

			

1ik

ǁ∇vk L(s)ǁ₂ = ¨√m      (fi(s) − yi) ǁv  (s)ǁ         (s)xi

Since   vk(s)  ₂        vk(0)  ₂      α(δ/m)¹/ᵈ with probability 1     δ over the initialization, 
applying

Cauchy Schwartz’s inequality gives


ǁ∇vk L(s)ǁ₂ ≤

(1 + R  (m/δ)¹/ᵈ)√n  f (s)     y

α√m                    .                        (E.1)

By the assumption on ǁf (s) − yǁ₂ this gives

2√n(1 −  ηω )ˢ/²ǁf (0) − yǁ₂

ǁ∇vk L(s)ǁ₂ ≤                       α   m                .

Hence bounding the parameter trajectory by the sum of the gradient updates:


ǁvk(K + 1) − vk(0)ǁ₂ ≤ η

Σs=0

ǁ∇vk L(s)ǁ₂ ≤

2√nǁf (0) − yǁ₂

α   m

Σs=1

.1 −

ηω   s/2

2

yields a geometric series. Now the series is bounded as

1                  4


which gives

1 − √1 −  ηω

ǁvk(K + 1) − vk(0)ǁ₂ ≤

≤  ηω ,

8√nǁf (0) − yǁ₂

α   mω

36


Under review as a conference paper at ICLR 2020


3

2

1                                              Training loss

Average

0

0.014

3

0.012

0.010

2

0.008

0.006                                                    1

0.004


0       2000    4000    6000    8000   10000

Gradient updates

0

0       2000    4000    6000    8000   10000

Gradient Updates


(a) Average α during training

(b) Total variation in angle, next 100 steps

Figure 2: Training dynamics of networks from class (1.2) on digits 4,9 of MNIST

F    NUMERICAL  ILLUSTRATION

Below we provide preliminary numerical illustrations demonstrating the phase transition between
V-dominated to G-dominated convergence. We follow the settings introduced in Section 2. In the
experiments we run WeightNorm gradient descent for K = 10000 gradient iterations and learning
rate η = 3     10−³. We train the neural network from class (1.2), with m = 10000, n = 1000, d =

784, α = 10−³ and use the first 1000 images of the digits 4 and 9 of the MNIST dataset. We restrict

to the digits of 4 and 9 since we focus on the scalar output network with two classes.

To compute α for the network during training we calculate the ratio,

Σm     ǁvk(s)ǁ₂

During training we note that α grows, as the training loss decreases and the convergence rate slows
down (emergence of the G-dominated regime).  To measure the change in the direction of vk(t)
during training we compute the angle variation of the parameter, this is computed for each neuron 
for
each gradient update as,

∆θ  (s) = arccos .   ⟨vk(s), vk(s + 1)⟩   Σ.

ǁvk(s)ǁ₂ǁvk(s + 1)ǁ₂

To visualize the angle change of the network during training, we compute the mean angle change for

each iteration, ∆ₐvₑ(s) =   ¹  Σm     ∆θk(s). We plot a moving sum of ∆ₐvₑ(s) window of size 100,

37

