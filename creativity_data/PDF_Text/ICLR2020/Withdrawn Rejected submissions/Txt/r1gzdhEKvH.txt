Under review as a conference paper at ICLR 2020
Neural Linear Bandits:
Overcoming	Catastrophic	Forgetting
through Likelihood Matching
Anonymous authors
Paper under double-blind review
Ab stract
We study neural-linear bandits for solving problems where both exploration and
representation learning play an important role. Neural-linear bandits leverage the
representation power of deep neural networks and combine it with efficient explo-
ration mechanisms, designed for linear contextual bandits, on top of the last hidden
layer. Since the representation is being optimized during learning, information
regarding exploration with ”old” features is lost. Here, we propose the first limited
memory neural-linear bandit that is resilient to this catastrophic forgetting phe-
nomenon. We perform simulations on a variety of real-world problems, including
regression, classification, and sentiment analysis, and observe that our algorithm
achieves superior performance and shows resilience to catastrophic forgetting.
1 Introduction
Deep neural networks (DNNs) can learn representations of data with multiple levels of abstraction
and have dramatically improved the state-of-the-art in speech recognition, visual object recognition,
object detection and many other domains such as drug discovery and genomics (LeCun et al., 2015;
Goodfellow et al., 2016). Using DNNs for function approximation in reinforcement learning (RL)
enables the agent to generalize across states without domain-specific knowledge, and learn rich
domain representations from raw, high-dimensional inputs (Mnih et al., 2015; Silver et al., 2016).
Nevertheless, the question of how to perform efficient exploration during the representation learning
phase is still an open problem. The -greedy policy (Langford & Zhang, 2008) is simple to implement
and widely used in practice (Mnih et al., 2015). However, it is statistically suboptimal. Optimism in
the Face of Uncertainty (Abbasi-Yadkori et al., 2011; Auer, 2002, OFU), and Thompson Sampling
(Thompson, 1933; Agrawal & Goyal, 2013, TS) use confidence sets to balance exploitation and
exploration. For DNNs, such confidence sets may not be accurate enough to allow efficient exploration.
For example, using dropout as a posterior approximation for exploration does not concentrate with
observed data (Osband et al., 2018) and was shown empirically to be insufficient (Riquelme et al.,
2018). Alternatively, pseudo-counts, a generalization of the number of visits, were used as an
exploration bonus (Bellemare et al., 2016; Pathak et al., 2017). Inspired by tabular RL, these ideas
ignore the uncertainty in the value function approximation in each context. As a result, they may lead
to inefficient confidence sets (Osband et al., 2018).
Linear models, on the other hand, are considered more stable and provide accurate uncertainty
estimates but require substantial feature engineering to achieve good results. Additionally, they are
known to work in practice only with ”medium-sized” inputs (with around 1, 000 features) due to
numerical issues. A natural attempt at getting the best of both worlds is to learn a linear exploration
policy on top of the last hidden layer of a DNN, which we term the neural-linear approach. In
RL, this approach was shown to refine the performance of DQNs (Levine et al., 2017) and improve
exploration when combined with TS (Azizzadenesheli et al., 2018) and OFU (O’Donoghue et al.,
2018; Zahavy et al., 2018a). For contextual bandits, Riquelme et al. (2018) showed that neural-linear
TS achieves superior performance on multiple data sets.
A practical challenge for neural-linear bandits is that the representation (the activations of the last
hidden layer) change after every optimization step, while the features are assumed to be fixed over
time when used by linear contextual bandits. Riquelme et al. (2018) tackled this problem by storing
the entire data set in a memory buffer and computing new features for all the data after each DNN
learning phase. The authors also experimented with a bounded memory buffer, but observed a
significant decrease in performance due to catastrophic forgetting (Kirkpatrick et al., 2017), i.e., a
loss of information from previous experience.
1
Under review as a conference paper at ICLR 2020
Next
iteration
1. Sample weight vectors from the uncertainty set
2. Act optimally with respect to the sampled weights
3. Store observed data in the replay buffer
Figure 1: Neural-Linear contextual Thompson sampling with limited memory.
In this work, we propose a neural-linear bandit that uses TS on top of the last layer of a DNN (Fig. 1)2 1.
Key to our approach is a novel method to compute priors whenever the DNN features change that
makes our algorithm resilient to catastrophic forgetting. Specifically, we adjust the moments of the
likelihood of the reward estimation conditioned on new features to match the likelihood conditioned
on old features. We achieve this by solving a semi-definite program (Vandenberghe & Boyd, 1996,
SDP) to approximate the covariance and using the weights of the last layer as prior to the mean.
We present simulation results on several real-world and simulated data sets, including classification
and regression, using Multi-Layered Perceptrons (MLPs). Our findings suggest that using our method
to approximate priors improves performance when memory is limited. Finally, we demonstrate that
our neural-linear bandit performs well in a sentiment analysis data set where the input is given in
natural language (of size R8k) and we use a Convolution Neural Network (CNNs). In this regime, it is
not feasible to use a linear method due to computational problems. To the best of our knowledge, this
is the first neural-linear algorithm that is resilient to catastrophic forgetting due to limited memory.
2 Background
The stochastic, contextual (linear) multi-
armed bandit problem. There are N arms
(actions). At time t a context vector b(t) ∈ Rd,
is revealed. The history at time t is defined to be
Ht-1 = {b(τ),a(τ),ra(τ)(τ),τ = 1, ..., t-1},
where a(τ) denotes the arm played at time
τ . The contexts b(t) are assumed to be re-
alizable, i.e., the reward for arm i at time t
is generated from an (unknown) distribution
s.t. E [ri(t)|b(t),Ht-1] = E [ri(t)|b(t)] =
b(t)Tμi, where {μi ∈ Rd}N=ι are fixed but Un-
known parameters. An algorithm for this prob-
lem needs to choose at every time t an arm a(t
Algorithm 1 TS for linear contextual bandits
∀i ∈ [1.., N], set Bi = Id, 1^i = 0d, fi = 0d
for t = 1, 2, . . . , do
∀i ∈ [1.., N], sample μ% from N(μ^i,v2B-1)
Play arm a(t) := argmaxg b(t)T/∑i
Observe reward rt
Update: Ba(t) = Ba(t) + b(t)b(t)T
fa(t) = fa(t) + b⑴rt, μa(t) = B-(It) fa(t)
end for
)
current context b(t). Let a* (t) denote the optimal arm at time t, i.e. a* (t)
to play, with the knowledge of history Ht-1 and
arg maXib(t)Tμi, and
let ∆i (t) the difference between the mean rewards of the optimal arm and of arm i at time t, i.e.,
∆i(t) = b(t)Tμa*(t) - b(t)Tμi. The objective is to minimize the total regret R(T) = PT=I ∆α(t),
where the time horizon T is finite.
TS for linear contextual bandits. Thompson sampling is an algorithm for online decision problems
where actions are taken sequentially in a manner that must balance between exploiting what is known
to maximize immediate performance and investing to accumulate new information that may improve
future performance (Russo et al., 2018; Lattimore & Szepesvari, 2018). For linear contextual bandits,
TS was introduced in (Agrawal & Goyal, 2013, Alg. 1).
1Image credits: bandit (bottom), Microsoft research; confidence ellipsoid (right), OriginLab.
2
Under review as a conference paper at ICLR 2020
Suppose that the likelihood of reward ri(t), given context b(t) and parameter μ%, were given by
the Pdf of Gaussian distribution N(b(t)Tμi, V2), and let Bi(t) = B0 + Pt-I b(τ)b(τ)T‰=a^τ),
μi(t) = B-1(t) PT=I b(τ)ra{τ)(τ) li=a(τ), where 1 is the indicator function. Given a Gaussian
prior for arm i at time t, N(∕^i(t),v2B-1(t)), the posterior distribution at time t + 1 is given by,
Pr(μi∣ri(t))〜Pr(ri(t)∣μi)Pr(μi)〜N(μi(t + 1),v2 *B-1(t +1)).	⑴
At each time step t, the algorithm generates samples {μi(t)}N=1 from the posterior distribution
N(μi(t), v2B-1(t)), plays the arm i that maximizes b(t)Tμi(t) and updates the posterior. TS is
guaranteed to have a total regret at time T that is not larger than O(d3/2√T), which is within a factor
of √d of the information-theoretic lower bound for this problem. It is also known to achieve excellent
empirical results (Lattimore & Szepesvari, 2018).
Although that TS is a Bayesian approach, the description of the algorithm and its analysis are prior-
free, i.e., the regret bounds will hold irrespective of whether or not the actual reward distribution
matches the Gaussian likelihood function used to derive this method (Agrawal & Goyal, 2013).
Bayesian Linear Regression. A different mechanism, based on Bayesian Linear Regres-
sion, was proposed by Riquelme et al. (2018). Here, the noise parameter ν (Alg. 1) is re-
placed with a prior belief that is being updated over time. The prior for arm i at time t
is given by Pr(μi, νf) = Pr(Vf)Pr (国亚),where Pr(V2 is an inverse-gamma distribution
InV-Gamma(ai(t),bi(t)), and the conditional prior density Pr(μ∕V2) is a normal distribution,
Pr(μi∖V) a N (μi(t), VBi(t)-1) . For Gaussian likelihood, the posterior distribution at time
T = t + 1 is, Pr(Vi) = Inv-Gamma (ai(τ), bi(τ)) and Pr(μi∣ν>i) = N (μi(τ), V2Bi(τ)-1), where:
t-1
Bi(t) = Bi + X b(τ)b(τ)Tli=a(τ), fi(t) = XT-=1 b(τ )ri(τ ) li=a(τ),
τ=1
μi(t) = Bi(t)T(B0〃0 + fi(t)), ai(t) = a0 + g,	R2(t) = R2(t- 1)+ r2
bi(t) = b0 + 2(R2(t) + (μ0)τB0μ0 - μi(t)τBi(t)μi(t)).	⑵
The problem with this approach is that the marginal distribution of μi is heavy tailed (multi-variate
t-student distribution, see O’Hagan & Forster (2004), page 246, for derivation), and does not satisfy
the necessary concentration bounds for exploration in (Agrawal & Goyal, 2013; Abeille et al., 2017).
Thus, in order to analyze the regret of this approach, new analysis has to be derived, which we leave
to future work. Empirically, this update scheme was shown to convergence to the true posterior and
demonstrated excellent empirical performance (Riquelme et al., 2018). This can be explained by the
fact that the mean of the noise parameter ν%, given by EVi = ObN-I, is decreasing to zero with time,
which may compensate for the lack of shrinkage due to the heavy tail distribution.
3	Limited memory neural-linear TS
Our algorithm, as depicted in Fig. 1, is composed of four main components: (1) A DNN that takes a
raw context as an input and is trained to predict the reward of each arm; (2) An exploration mechanism
that uses the last layer activations of the DNN as features and performs linear TS on top of them; (3)
A memory buffer that stores previous experience; (4) A likelihood matching mechanism that uses the
memory buffer and the DNN to account for changes in representation. We now explain how each of
these components works; code can be found in (link).
To derive our algorithm we make the assumption that all the representations that are produced by the
DNN are realizable. That is, for each representation there exist a different linear coefficients vector
(e.g. μ for φ, β for ψ,) such that the expected reward is linear in the features. Explicitly, this means
that for representations φ,ψ it holds that E[ri(t)∣φ(t)] = φ(t)T μi = ψ(t)T βi = E[ri(t)∣ψ(t)].
While the realizability assumption is standard in the existing literature on contextual multi-armed
bandits (Chu et al., 2011; Abbasi-Yadkori et al., 2011; Agrawal & Goyal, 2013), it is quite strong and
may not be realistic in practice. We further discuss these assumptions in the discussion paragraph
below and in Section 5.
1. Representation. Our algorithm uses a DNN, denoted by D, that takes the raw context b(t) ∈ Rd
as its input. The network has N outputs that correspond to the estimation of the reward of each arm;
given context b(t) ∈ Rd, D(b(t))i denotes the estimation of the reward of the i-th arm.
3
Under review as a conference paper at ICLR 2020
Using a DNN to predict the reward of each arm allows our algorithm to learn a nonlinear representation
of the context. This representation is later used for exploration by performing linear TS on top of the
last hidden layer activations. We denote the activations of the last hidden layer of D applied to this
context as φ(t) = LastLayerActivations(D(b(t))), where φ(t) ∈ Rg. The context b(t) represents
raw measurements that can be high dimensional (e.g., image or text), where the size of φ(t) is a
design parameter that we choose to be smaller (g < d). This makes contextual bandit algorithms
practical for such data sets. Moreover, φ(t) can potentially be linearly realizable (even if b(t) is not)
since a DNN is a global function approximator (Barron, 1993) and the last layer is linear.
1.1 Training. Every L iterations, we train D for P mini-batches. Training is performed by sampling
experience tuples {b(τ), a(τ), ra(τ)(τ)} from the replay buffer E (details below) and minimizing the
mean squared error (MSE),
llD(b(τ ))a(τ La(T)(T *2,	⑶
where ra(τ) is the reward that was received at time τ after playing arm a(τ) and observing context
b(τ) (similar to Riquelme et al. (2018)). Notice that only the output of arm a(τ) is differentiated.
We emphasize that the DNN, including the last layer, are trained end-to-end to minimize Eq. (3).
Algorithm 2 Limited Memory Neural-linear TS
Set ∀i ∈ [1,.., N]: φ0 = Id,μi = μ0 = 0d, φi = 0dxd, fi = 0d, ai = a0, bi = b0
Initialize Replay Buffer E, and DNN D
Define φ(t) J LastLayerActivations(D(b(t)))
for t = 1, 2, . . . , do
Observe b(t), evaluate φ(t)
Posterior sampling: ∀i ∈ [1, .., N], sample:
z>i(t)〜Inv-Gamma(ai(t),bi(t))
μi(t)〜N (μi,i>i(t)2(Φ0 + Φi)-1)
Play arm a(t) := argmaxg φ(t)T"i(t)
Observe reward rt
Store {b(t), a(t), rt} in E
if E is full then
Remove the first tuple in E with a = a(t) (round robin)
end if
Bayesian linear regression update:
Φa(t) = Φa(t) + φ(t)φ(t)T, fa(t) = fa(t) + φ(t)T rt
μa(t)=(此⑴ +φa(t))-1 (%tM(t) + fa(t))
aα(t) = aα(t) + 2
R2a(t) = R2a(t) + rt2
ba(t) = %t) + 2 (Rl(t) + (μ0(=))' B2(t)μ0(t) - μ1T(t) Ba(t)μa(t))
if (t mod L) = 0 then
for ∀i ∈ [1,..,N] do
Evaluate old features on the replay buffer: Eφi old
end for
Train DNN for P steps
Compute priors for new features:
for ∀i ∈ [1, .., N] do
Evaluate new features on the replay buffer: Eφi
Solve for Φi0 using Eq. (6) with Eφi , Eφi old, Φiold
Set μ0 J LastLayerWeights(D)i
Φi = Pjn=i1φij(φij)T,fi = Pjn=i1(φij)Trj.
end for
end if
end for
2. Exploration. Since our algorithm is performing training in phases (every L steps), exploration is
performed using a fixed representation φ (D has fixed weights between training phases). At each time
step t, the agent observes a raw context b(t) and uses the DNN D to produces a feature vector φ(t).
The features φ(t) are used to perform linear TS, similar to Algorithm 1, but with two key differences.
4
Under review as a conference paper at ICLR 2020
First, we introduce a likelihood matching mechanism that accounts for changes in representation (see
4. below for more details). Second, we follow the Bayesian linear regression equations, as suggested
in (Riquelme et al., 2018), and perform TS while updating the posterior both for μ, the mean of the
estimate, and ν, its variance.
This is done in the following manner. We begin by sampling a weight vector μ% for each arm i ∈ 1..N,
from the posterior by following two steps. First, the variance V is sampled from Inv-Gamma (a%, bi).
Then, the weight vector μi is sampled, from N (μ%, ν2(Φ0 + Φ%)-1) . Once we sampled a weight
vector for each arm, we choose to play arm a(t) = arg max%φ(t)Tμi, and observe reward r@(t)(t).
This is followed by a posterior update step, based on Eq. (2):
Φa(t) =	Φa(t) + φ(t)φ(t)T, fa(t) =	fa(t) + φ(t)Trt, R2a(t) =	R2a(t)	+ ra(t) (t)2	(4)
μa(t) =	(φa(t) + φ2(2 1 ®a(t)〃a(t)	+ fa(t)), aα(t)	= aα(t) +	4,
ba(t) = b0,a(t) + 2 (R2(t) + μT,a(t)φ0,a(t)μ0,a(t) - μa(t)⑴Tφa(t) (t)μa(t)(t)
The exploration mechanism is responsible for choosing actions; it does not change the weights of
the DNN.
3.	Memory buffer. After an action a(t) is played at time t, we store the experience tuple
{b(t), a(t), ra(t) (t)} in a finite memory buffer of size n that we denote by E. Once E is full,
we remove tuples from E in a round robin manner, i.e., we remove the first tuple in E with a = a(t).
4.	Likelihood matching. Before each learning phase, we evaluate the features of D on the replay
buffer. Let Ei be a subset of memory tuples in E at which arm i was played, and let ni be its size.
We denote by Eφi old ∈ Rni×g a matrix whose rows are feature vectors that were played by arm i.
After a learning phase is complete, we evaluate the new activations on the same replay buffer and
denote the equivalent set by Eφi ∈ Rni×g.
Our approach is to summarize the knowledge that the algorithm has gained from exploring with
the features φold into priors on the new features Φ0,μ0. Once these priors are computed, we restart
the linear TS algorithm using the data that is currently available in the replay buffer. For each
arm i, let φij = (Eφi )j be the j-th row in Eφi and let rj be the corresponding reward, we set
Φi =Pjn=i1φij(φij)T,fi =Pjn=i1(φij)Trj.
We now explain how we compute Φ0,μ0. Recall that under the realizability assumption we have
that E[ri(t)∣φ(t)] = φ(t)Tμ% = φod(t∖τμθld = E[r%(t)∣ψ(t)]. Thus, the likelihood of the reward
is invariant to the choice of representation , i.e. N(φ(t)Tμ%,ν2)〜 N(φold(t)Tμθld,ν2). For
all i, define the estimator of the reward as θ%(t) = φ(t)Tμi(t), and its standard deviation st,i =
Pφ(t)TΦi(t)-1φ(t) (see (Agrawal & Goyal, 2013) for derivation). By definition of μi(t), marginal
distribution of each θi(t) is Gaussian with mean φi(t)Tμi(t) and standard deviation νist,i. The goal
is to match the likelihood of the reward estimation θi (t) given the new features to be the same as with
the old features.
4.1	Approximation of the mean μ0: Recall that the realizability assumption implies a linear con-
nection between μθld, μ0, i.e., (μild)τφold = (μ0)Tφ, thus, we can solve a linear set of equations
and get a linear mapping from μ0 to μθld :
(μ0)τ = (μθld)τ Eφ old (Eφ )-1.
(5)
In addition to the realizability assumption, for Eq. (5) to hold the matrix Eφi must be invertible. In
practice, we found that a different solution that is based on using the DNN weights performed better.
Recall that the DNN is trained to minimize the MSE (Eq. (3)). Thus, given the new features φ, the
weights of the last layer of the DNN make a good prior for μ0. This approach was shown empirically
to make a good approximation (Levine et al., 2017), as the DNN was optimized online by observing
all the data (and is therefore not limited to the current replay buffer).
4.2	Approximation of the variance sj,i:. For each arm i, our algorithm receives as input the sets of
new and old features Eφi , Eφi old; denote the elements in these sets by {φjold, φj}jn=i 1. In addition, the
algorithm receives the correlation matrix Φiold. Notice that due the nature of our algorithm, Φiold holds
5
Under review as a conference paper at ICLR 2020
information on contexts that are not available in the replay buffer. The goal is to find a correlation
matrix,Φi0, for the new features that will hold the same information on past context as Φiold. I.e., we
want to find Φi0 such that ∀i ∈ [1..N], j ∈ [1..ni] sj2,i =. (φjold)T (Φiold)-1φjold = φjT (Φi0)-1φj.
Using the cyclic property of the trace, this is equivalent to finding Φi0, s.t. ∀j ∈ [1, .., ni], sj2,i =
Trace (Φi0)-1φjφjT . Next, we define Xi to be a vector of size ni in the vector space of d × d
symetric matrices, with its j-th element Xj,i to be the matrix φjφjT. Notice that (Φi0)-1 is constrained
to be semi positive definite (being a correlation matrix), thus, the solution can be found by solving an
SDP (Eq. (6)). Note that Trace(XjT,i(Φi0)-1) is an inner product over the vector space of symmetric
matrices, known as the Frobenius inner product. Thus, the optimization problem is equivalent to a
linear regression problem in the vector space of PSD matrices. In practice, we use cvxpy (Diamond
& Boyd, 2016) to solve for all actions i ∈ [1..N] :
minimizeXn= 1 ||Trace(XTi(虫O)T)-Sj,i||2 subject to (φO)T 占 0.	⑹
(Φi )	j=
Discussion. The correctness of our algorithm follows from the proof of (Agrawal & Goyal, 2013).
To see this, recall that we match the moments of the reward estimate θi (t) after every time that
the representation changes. Assuming that we solve Eq. (5) and Eq. (6) precisely, then the reward
estimation given the new features have precisely the same moments and distribution as with the old
features. Since the distribution of the estimate did not change, its concentration and anti-concentration
bounds do not change, and the proof in (Agrawal & Goyal, 2013) can be followed.
The problem is, that in general, we cannot guarantee to solve Eq. (5) and Eq. (6) exactly. We will soon
show that under the realizability assumption, in addition to an invertibility assumption, it is possible to
choose an analytical solution for the priors μ0, Φ0 that guarantees an exact solution. However, these
conditions may be too strong and not realistic. We describe this scenario to highlight the existence of
a scenario (and conditions) in which our algorithm is optimal; we hope to relax them in future work.
We emphasize here that if these conditions do not hold, then our algorithm is only an approximation,
without theoretical guarantees. In the next section we justify using our algorithm through thorough
experimentation.
For simplicity, we consider a single arm. Assume that m past observations, which we denote
by Eφmoid, were used to learn estimators (μm)00d,①露“using BLR (Eq. (2)). Due to the limited
memory, some of these measurements are not available in the replay buffer, and all of the information
regarding them is summarized in (∕^m)o0d, Φmld. In addition, we are given a replay buffer of size
n, that is used to produce (before and after the training) new and old feature matrices Eφnold , Eφn.
We also denote by Rn the reward vector (using data from the replay buffer) and by Rm the reward
vector that corresponds to features Eφmold which is not available in the replay buffer. Recall that the
realizeability assumption implies that the features φ and φo0d are linear mappings of the raw context
b, i.e., φ = Aφ b, φo0d = Aφoldb. Under the assumption that all the relevant matrices are invertible,
we use Eq. (5) to find a prior for μο, i.e., we set μτ = (μθ!^')τEnoid(En)-1. In addition, for the
covariance matrix, we set (Φ0)-1 = (Eφn)-1Eφnold (Φom0d)-1(Eφnold)T((Eφn)T)-1, which is a solution
to Eq. (6).
In addition, we get that if the relevant matrices are invertibele, then Φο = Φm, and that Φομο =
Eφm Rm (see the supplementary for derivation). Plugging these estimates as priors in the Bayesian
linear regression equation we get the following solution for μ :
μ=(Φn + Φ0)-1(Φ0μ0 + EnRn) = (Φn + Φm)-1(EmRm + EφRn),
i.e., we got the linear regression solution for μ as if we were able to evaluate the new features φ on
the entire data, while having a finite memory buffer and changing features!
3.1 Computational complexity
Solving the SDP. Recall that the dimension of the last layer is g < d where d is the dimension of
the raw features, and the size of the buffer is n. Following this notation, when solving the SDP,
we optimize over matrices in Rg×g that are subject to n equality constraints. We refer the reader
to Vandenberghe & Boyd (1996) for an excellent survey on the complexity of solving SDPs. Here,
we will refer to interior-point methods. The number of iterations required to solve an SDP to a
given accuracy grows with problem size as O(g0.5). Each iteration involves solving a least-squares
6
Under review as a conference paper at ICLR 2020
problem of dimension g2 . If, for example, this least-squares problem is solved with a projected
gradient descent method, then the time complexity for finding an -optimal solution is g2/, and the
computational complexity of each gradient step is ng2 (matrix-vector multiplications). Vandenberghe
& Boyd experimented with solving SDPs of different sizes and observed that it takes almost the same
amount of iterations to solve them. In addition, they found that SDP algorithms converge much faster
than the worst-case theoretical bounds.
In our case, g, the size of the last layer, was fixed to be 50 in all the experiments. Thus, although
the dimension of the raw features d varies in size across data sets, the complexity of the solving the
SDP is fixed. The dependence of the computational complexity on the buffer size n is at most linear
(CVXPY exploits sparsity structure of the matrix to enhance computations); we didn’t encounter a
significant changes in computation time when changing the buffer size in the range of 200 - 2000. It
took us 10 - 30 seconds on a standard ”MacBook Pro” to solve a single SDP.
Dependence on T . The full memory approach results in computational complexity of O(T 2) and
memory complexity of O(T) where T is the number of contexts seen by the algorithm. This is because
it is estimating the TS posterior using the complete data every time the representation changes. On the
other hand, the limited memory approach uses only the memory buffer to estimate the posterior but
additionally solves an SDP. This gives a memory complexity of O(1) and computational complexity
of O(T).
Dependence on A. The computational complexity is linear in the number of actions (we solve an
SDP for each action). There is a large variety of problems where this is not an issue (as in our
experiments). However, if the problem of interest has many discrete actions, our approach may not
be useful.
To summarize, our method is more efficient than the full memory baseline in problems with big
data (large T ). Nevertheless, our method requires to solve an SDP (every L iterations), which is
computationally prohibitive in general. We deal with this issue by restricting the size of the last layer
to be small (g = 50), for which solving the SDP is reasonable.
4 Experiments
We begin this section by testing the resilience of our method to catastrophic forgetting. We present an
ablative analysis of our approach and show that the prior on the covariance is crucial. Then, we present
results for using MLPs on ten real-world data sets, including a high dimensional natural language data
on a task of sentiment analysis (all of these data sets are publicly available through the UCI Machine
Learning Repository). Additionally, in the supplementary material, we use synthetic data to test and
visualize the ability of our algorithm to learn nonlinear representations during exploration. In all the
experiments we used the same hyperparameters (as in (Riquelme et al., 2018)) for the model, and
the same network architecture (an MLP with a single hidden layer of size 50). The only exception is
with the text CNN (details below). The size of the memory buffer is set to be 100 per action.
4.1 Catastrophic forgetting
Alg1. Linear TS
Alg 2. NeUraI-Linear TS, full memory
Alg 3. Neural-Linear TS, finite memory (ours)
Alg 4. Neural-Linear TS, finite memory, only mean prior (ablative)
Alg 5. Neural-Linear TS, finite memory, no priors (ablative)
Figure 2: Catastrophic forgetting
We use the Shuttle Statlog data set
(Newman et al., 2008), a real world,
nonlinear data set. Each context is
composed of 9 features of a space
shuttle flight, and the goal is to predict
the state of the radiator of the shuttle
(the reward). There are k = 7 pos-
sible actions, and if the agent selects
the right action, then reward 1 is gen-
erated. Otherwise, the agent obtains
no reward (r = 0).
We experimented with the following
algorithms: (1) Linear TS (Agrawal
& Goyal, 2013, Algorithm 1) using
the raw context as a feature, with an
additional uncertainty in the variance
(Riquelme et al., 2018). (2) Neural-
Linear TS (Riquelme et al., 2018). (3) Our neural-linear TS algorithm with limited memory. (4) An
ablative version of (3) that calculates the prior only for the mean, similar to (Levine et al., 2017).
7
Under review as a conference paper at ICLR 2020
(5) An ablative version of (3) that does not use prior calculations. Algorithms 3-5 make an ablative
analysis for the limited memory neural-linear approach. As we will see, adding each one of the priors
improves learning and exploration.
Fig. 2 shows the performance of each of the algorithms in this setup. We let each algorithm run for
4000 steps (contexts) and average each algorithm over 10 runs. The x-axis corresponds to the number
of contexts seen so far, while the y-axis measures the instantaneous regret. All the neural-linear
methods retrained the DNN every L = 400 steps for P = 800 mini-batches.
First, we can see that the neural linear method (2nd row) outperforms the linear one (1st row),
suggesting that this data set in nonlinear. We can also see that our approach to computing the
priors allows the limited memory algorithm (3rd row) to perform almost as good as the neural linear
algorithm without memory constraints (2nd row).
In the last two rows we can see a version of the limited memory neural linear algorithm that does not
calculate the prior for the covariance matrix (4th row), and a version that does not compute priors at
all (5th row). Both of these algorithms suffer from ”catastrophic forgetting” due to limited memory.
Intuitively, the covariance matrix holds information regarding the number of contexts that were seen
by the agent and are used by the algorithm for exploration. When no such prior is available, the
agent explores sub-optimal arms from scratch every time the features are modified (every L = 400
steps, marked by the x-ticks on the graph). Indeed, we observe ”peaks” in the regret curve for these
algorithms (rows 4&5); this is significantly reduced when we compute the prior on the covariance
matrix (3rd row), making the limited memory neural-linear bandit resilient to catastrophic forgetting.
4.2	Real world data
We evaluate our approach on several (10) real-world data sets; for each data set, we present the
cumulative reward achieved by the algorithms, detailed above, averaged over 50 runs. Each run was
performed for 5000 steps.
Linear vs. nonlinear data sets: The results are divided into two groups, linear and nonlinear data
sets. The separation was performed post hoc, based on the results achieved by the full memory
methods, i.e., the first group consists of five data sets on which Linear TS (Algorithm 1) outperformed
Neural-Linear TS (Algorithm 2), and vice versa. We observed that most of the linear datasets
consisted of a small number of features that were mostly categorical (e.g., the mushroom data set has
22 categorical features that become 117 binary features). The DNN based methods performed better
when the features were dense and high dimensional.
			Full memory	Limited memory, Neural-Linear
Name	d	-A	Linear	Neural-Linear	Both Priors	μ Prior	No Prior
Linear Data SetS
Mushroom	117	2	11022 ± 774	10880 ± 853	10923 ± 839	9442 ± 1351	7613 ± 1670
Financial	21	-8-	4588 ± 587	4389 ± 584	4597 ± 597	4311 ± 598	4225 ± 594
Jester	32	-8-	14080 ± 2240-	12819± 2135	9624 ± 2186	10996 ± 2013	11114 ± 2050-
AdUlt		88	~Γ~	4066.1 ± 11.03~	4010.0 ± 22.19~	3943.0 ± 54.29	3839.5 ± 17.63~	3608.2 ± 34.94
Covertype	54	~T~	3054 ± 557	2898 ± 545	2828 ± 593	2347 ± 615	2334 ± 603
Nonlinear Data Sets
Census	377	9	1791.5 ± 39.47	2135.5 ± 51.47	2023.16 ± 37.3	1873 ± 757	1943.83 ± 84.2
StatlOg		"^9~	~T~	4483 ± 353	4781 ± 274	4825 ± 305	4681 ± 285	4623 ± 276
Epileptic	178	~1~	1202.9 ± 34.68~	1706.9 ± 41.26	1716.8 ± 60.44	1572.9 ± 48.66	1411.0 ± 33.43~
Smartphones	561	~6~	3085.8 ± 24.64	3643.5 ± 64.89	2660.4 ± 84.72~	3064.5 ± 55.06	2851.6 ± 58.77~
Scania Trucks	170	~Γ~	4691.8 ± 7.23	4784.7 ± 6.05	4742.0 ± 33.0	4698.0 ± 13.06	4470.4 ± 37.9-
Table 1: Cumulative reward of TS algorithmS on 10 real world data SetS. The context dim d and the
Size of the action Space A are reported for each data Set. The mean reSult and Standard deviation of
each algorithm iS reported for 50 runS.
Linear data sets: Since there iS no apriori reaSon to believe that real world data SetS Should be
linear, we were SurpriSed that the linear method made a competitive baSeline to DNNS. To inveStigate
thiS further, we experimented with the beSt reported MLP architecture for the covertype data Set
(taken from Kaggle). Linear methodS were reported (link) to achieve around 60% teSt accuracy.
ThiS number iS conSiStent with our reported cumulative reward (3000 out 5000). Similarly, DNNS
achieved around 60% accuracy, which indicateS that the Covertype data Set iS indeed relatively linear.
However, when we meaSure the cumulative reward, the deep methodS take initial time to learn, which
can explain the Slightly worSt Score. One particular architecture (MLP with layerS 54-500-800-7) waS
reported to achieve 68%; however, we didn’t find thiS architecture to yield better cumulative reward.
8
Under review as a conference paper at ICLR 2020
Similarly, for the Adult data set, linear and deep classifiers were reported to achieve similar results
(link) (around 84%), which is again equivalent to our cumulative reward of 4000 out of 5000. A
specific DNN was reported to achieve 90% test accuracy but did not yield improvement in cumulative
reward. These observations can be explained by the different loss function that we optimize or by the
partial observably of the bandit problem (bandit feedback). Alternatively, competitions tend to suffer
from overfitting in model selection (see the ”reusable holdout” paper for more details (Dwork et al.,
2015)). Regret, on the other hand, is less prune to model overfitting, because the model is evaluated
at each iteration, and because we shuffle the data at each run.
Limited memory: Looking at Table 1 we can see that on eight out of ten data sets, using the
prior computations (Algorithm 3), improved the performance of the limited memory Neural-Linear
algorithms. On four out of ten data sets (Mushroom, Financial, Statlog, Epileptic), Algorithm 3 even
outperformed the unlimited Neural-Linear algorithm (Algorithm 2).
Limited memory neural linear vs. linear: as linear TS is an online algorithm it can store all the
information on past experience using limited memory. Nevertheless, in four (out of five) of the
nonlinear data sets the limited memory TS (Algorithm 3) outperformed Linear TS (Algorithm 1).
Our findings suggest that when the data is indeed not linear, than neural-linear bandits beat the linear
method, even if they must perform with limited memory. In this case, computing priors improve the
performance and make the algorithm resilient to catastrophic forgetting.
4.3	Sentiment analysis from text using CNNs
We use the ”Amazon Reviews: Unlocked Mobile Phones” data set, which contains reviews of un-
locked mobile phones sold on ”Amazon.com”. The goal is to find out the rating (1 to 5 stars) of
each review using only the text itself. We use our model with a Convolutional Neural Network
(CNN) that is suited to NLP tasks (Kim, 2014; Zahavy et al., 2018b). Specifically, the architec-
ture is a shallow word-level CNN that was demonstrated to provide state-of-the-art results on a
variety of classification tasks by using word embeddings, while not being sensitive to hyperpa-
rameters (Zhang & Wallace, 2015). We use the architecture with its default hyper-parameters
(Github) and standard pre-processing (e.g., we use random embeddings of size 128, and we trim
and pad each sentence to a length of 60). The only modification we made was to add a linear
layer of size 50 to make the size of the last hidden layer consistent with our previous experiments.
Since the input is in R7k (60 × 128),
we did not include a linear baseline in
these experiments as it is impractical
to do linear algebra (e.g., calculate an
inverse) in this dimension. Instead,
we focused on comparing our final
method with the full memory neural
e-greedy	Neural-Linear	Neural-Linear Limited Memory
2963.9 ± 68.5	3155.6 ± 34.9	3143.9 ± 33.5
Figure 3: Cumulative reward on Amazon review’s
linear TS and both prior computations with an -greedy baseline. We experimented with 10 values
of , ∈ [0.1, 0.2, ..., 1] and report the results for the value that performed the best (0.1). Looking at
Fig. 3 we can see that the limited memory version performs almost as good as the full memory, and
better than the -greedy baseline.
5	Discussion
We presented a neural-linear contextual bandit algorithm that is resilient to catastrophic forgetting
and demonstrated its performance on several real-world data sets. Our algorithm showed comparable
results to a previous method that stores all the data in a replay buffer. The method requires to solve
an SDP, which is computationally prohibitive in general. Thus, we restricted the size of the last layer
to be small, such that solving the SDP is feasible.
To design our algorithm, we assumed that all the representations that are produced by the DNN are
realizable. In practice, the features that are learned in the first iterations are nearly realizable and
further iterations improve them. We hope to relax these assumptions in future work.
Our algorithm presented excellent performance on multiple real-world data sets. Moreover, its
performance did not deteriorate due to the changes in the representation and the limited memory. We
believe that our findings make an important step towards solving problems where both exploration
and representation learning play an important role.
9
Under review as a conference paper at ICLR 2020
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
Marc Abeille, Alessandro Lazaric, et al. Linear thompson sampling revisited. Electronic Journal of
Statistics, 11(2):5165-5197, 2017.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In
International Conference on Machine Learning, pp. 127-135, 2013.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397-422, 2002.
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through bayesian deep q-networks. arXiv preprint arXiv:1802.04412, 2018.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930-945, 1993.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics, pp. 208-214, 2011.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth.
The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636-638,
2015.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint, 2014.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting
in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526,
2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/
content/114/13/3521.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in neural information processing systems, pp. 817-824, 2008.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. 2018.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Nir Levine, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. Shallow updates
for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp.
3135-3145, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
David Newman, Padhraic Smyth, Max Welling, and Arthur U Asuncion. Distributed inference for
latent dirichlet allocation. In Advances in neural information processing systems, pp. 1081-1088,
2008.
Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. International Conference on Machine Learning, 2018.
Anthony O’Hagan and Jonathan J Forster. Kendall’s advanced theory of statistics, volume 2B:
Bayesian inference, volume 2. Arnold, 2004.
10
Under review as a conference paper at ICLR 2020
Ian Osband, John Aslanides, and Cassirer Albin. Randomized prior functions for deep reinforcement
learning. Advances in Neural Information Processing Systems, 2018.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning, 2017.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown. In Interna-
tional Conference on Learning Representations, 2018.
Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on
thompson sampling. Foundations and Trends® in Machine Learning, 11(1):1-96, 2018.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with
deep neural networks and tree search. Nature, 529(7587):484-489, jan 2016. ISSN 0028-0836.
doi: 10.1038/nature16961.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Lieven Vandenberghe and Stephen Boyd. Semidefinite programming. SIAM review, 38(1):49-95,
1996.
Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not
to learn: Action elimination with deep reinforcement learning. Advances in Neural Information
Processing Systems, 2018a.
Tom Zahavy, Alessandro Magnani, Abhinandan Krishnan, and Shie Mannor. Is a picture worth a
thousand words? a deep multi-modal fusion architecture for product classification in e-commerce.
The Thirtieth Conference on Innovative Applications of Artificial Intelligence (IAAI), 2018b.
Ye Zhang and Byron Wallace. A sensitivity analysis of (and practitioners’ guide to) convolutional
neural networks for sentence classification. arXiv preprint arXiv:1510.03820, 2015.
11
Under review as a conference paper at ICLR 2020
Appendix A Additional simulations: Non linear representation
LEARNING ON A SYNTHETIC DATA SET
Setup: we adapted a synthetic data set, known as the ”wheel bandit” (Riquelme et al., 2018), to
investigate the exploration properties of bandit algorithms when the reward is a nonlinear function of
the context. Specifically, contexts x ∈ R2 are sampled uniformly at random in the unit circle, and
there are k = 5 possible actions.
One action , a5, always offers reward r5 〜N(μ5, σ), independently of the context. The reward of
the other actions depend on the context and a parameter δ, that defines a δ-circle kxk ≤ δ.
For contexts that are outside the circle, actions a1, .., a4 are equally distributed and sub-optimal, with
ri 〜N(μ, σ) for μ < μ5,i ∈ [1..4].
For contexts that are inside a circle, the reward of each action depends on the respective quadrant.
EaCh action achieves ri 〜N(μi, σ), where μ5 < μi = μ in exactly one quadrant, and μi = μ < μ5
in all the other quadrants. For example, μ1 = μ in the first quadrant {x : ∣∣x∣∣ ≤ δ, x1 ,x2 > 0}
and μ1 = μ elsewhere. We set μ = 0.1, μ5 = 0.2, μ = 0.4, σ = 0.1. Note that the probability
of a context randomly falling in the high-reward region is proportional to δ. For lower values of
δ, observing high rewards for arms a1 , .., a4 becomes more scarce, and the role of the nonlinear
representation is less significant.
We train our model on n = 4000 contexts, where we optimize the network every L = 200 steps for
P = 400 mini batches. The results can be seen in Table 2.
Not surprisingly, the neural-linear approaches, even with limited memory, achieved better reward
than the linear method (Table 2) 2.
Figure 4: Representations learned on the wheel data set with δ = 0.5. Reward samples (top), linear
predictions (middle) and neural-linear predictions (bottom). Columns correspond to arms.
Fig. 4 presents the reward of each arm as a function of the context. In the top row, we can see
empirical samples from the reward distribution. In the middle row, we see the predictions of the linear
bandit. Since it is limited to linear predictions, the predictions become a function of the distance from
the learned hyper-plane. This representation is not able to separate the data well, and also makes
mistakes due to the distance from the hyperplane. For the neural linear method (bottom row), we
can see that the DNN was able to learn good predictions successfully. Each of the first four arms
learns to make high predictions in the relevant quadrant of the inner circle, while arm 5 makes higher
predictions in the outer circle.
2We will provide a detailed comparison of the neural-linear algorithms and priors later in this section.
12
Under review as a conference paper at ICLR 2020
	Linear	NeUraI-Linear Lim- ited Memory
δ=0.5	737.44 ± 3.04	899.72 ± 12.79
δ=0.3	735.37 ± 2.58	781.09 ± 11.34
δ=0.1	735.51 ± 2.59	751.75 ± 3.6
Table 2: Cumulative reward on the wheel bandit
Appendix B	Analysis
B.1 Derivation of auxiliary results for the sanity check
The realizability assumption gives Us a method to compute μo for the new features:
μT = (μmd)T Enold (En)T = ((E 机(E 沁)T)-i(e 沁)T Rm) En(Enold)-1.	⑺
Similarly, using the analytically solution to the SDP, we get
Eφn(Φ0)-1(Eφn)T =Eφnold(Φomld)-1(Eφnold)T,	(8)
Using Eq. (7) and Eq. (8) and rearranging we get that
φ0μo =
n n -1 m
= Eφ (Eφold)	Eφold Rm
= (Aφ bn)(Aφold bn)	Aφold bm Rm
= Aφbmb-m1Aφ-o1ldAφoldbmRm
= Aφbm Rm = Eφm Rm .
Similarly, for Φ0 we get that
Φ0 = Eφn(Eφnold)-1(Φomld)((Eφnold)T)-1(Eφn)T
= (Aφbn)(Aφoldbn)-1(Φomld)(bn-1Aφ-o1ld)T(Aφbn)T
m
= AφAφ-o1ldAψ XbibiTATφold(ATφold)-1ATφ
i=1
m
= Aφ X bibi Aφ = Φm.
i=1
13