Under review as a conference paper at ICLR 2020
Question Generation from Paragraphs: A Tale
of Two Hierarchical Models
Anonymous authors
Paper under double-blind review
Ab stract
Automatic question generation from paragraphs is an important and challenging
problem, particularly due to the long context from paragraphs. In this paper, we
propose and study two hierarchical models for the task of question generation from
paragraphs. Specifically, we propose (a) a novel hierarchical BiLSTM model with
selective attention and (b) a novel hierarchical Transformer architecture, both of
which learn hierarchical representations of paragraphs. We model a paragraph
in terms of its constituent sentences, and a sentence in terms of its constituent
words. While the introduction of the attention mechanism benefits the hierarchi-
cal BiLSTM model, the hierarchical Transformer, with its inherent attention and
positional encoding mechanisms also performs better than flat transformer model.
We conducted empirical evaluation on the widely used SQuAD and MS MARCO
datasets using standard metrics. The results demonstrate the overall effectiveness
of the hierarchical models over their flat counterparts. Qualitatively, our hierar-
chical models are able to generate fluent and relevant questions.
1	Introduction
Question Generation (QG) from text has gained significant popularity in recent years in both
academia and industry, owing to its wide applicability in a range of scenarios including conver-
sational agents, automating reading comprehension assessment, and improving question answering
systems by generating additional training data. Neural network based methods represent the state-
of-the-art for automatic question generation. These models do not require templates or rules, and
are able to generate fluent, high-quality questions.
Most of the work in question generation takes sentences as input (Du & Cardie, 2018; Kumar et al.,
2018; Song et al., 2018; Kumar et al., 2019). QG at the paragraph level is much less explored and
it has remained a challenging problem. The main challenges in paragraph-level QG stem from the
larger context that the model needs to assimilate in order to generate relevant questions of high
quality.
Existing question generation methods are typically based on recurrent neural networks (RNN), such
as bi-directional LSTM. Equipped with different enhancements such as the attention, copy and cov-
erage mechanisms, RNN-based models (Du et al., 2017; Kumar et al., 2018; Song et al., 2018)
achieve good results on sentence-level question generation. However, due to their ineffectiveness
in dealing with long sequences, paragraph-level question generation remains a challenging problem
for these models.
Recently, Zhao et al. (2018) proposed a paragraph-level QG model with maxout pointers and a
gated self-attention encoder. To the best of our knowledge this is the only model that is designed
to support paragraph-level QG and outperforms other models on the SQuAD dataset (Rajpurkar
et al., 2016). One straightforward extension to such a model would be to reflect the structure of a
paragraph in the design of the encoder. Our first attempt is indeed a hierarchical BiLSTM-based
paragraph encoder ( HPE ), wherein, the hierarchy comprises the word-level encoder that feeds its
encoding to the sentence-level encoder. Further, dynamic paragraph-level contextual information in
the BiLSTM-HPE is incorporated via both word- and sentence-level selective attention.
However, LSTM is based on the recurrent architecture of RNNs, making the model somewhat rigid
and less dynamically sensitive to different parts of the given sequence. Also LSTM models are
1
Under review as a conference paper at ICLR 2020
slower to train. In our case, a paragraph is a sequence of sentences and a sentence is a sequence of
words. The Transformer (Vaswani et al., 2017) is a recently proposed neural architecture designed
to address some deficiencies of RNNs. Specifically, the Transformer is based on the (multi-head)
attention mechanism, completely discarding recurrence in RNNs. This design choice allows the
Transformer to effectively attend to different parts of a given sequence. Also Transformer is rela-
tively much faster to train and test than RNNs.
As humans, when reading a paragraph, we look for important sentences first and then important
keywords in those sentences to find a concept around which a question can be generated. Taking
this inspiration, we give the same power to our model by incorporating word-level and sentence-level
selective attention to generate high-quality questions from paragraphs.
In this paper, we present and contrast novel approachs to QG at the level of paragraphs. Our main
contributions are as follows:
•	We present two hierarchical models for encoding the paragraph based on its structure. We
analyse the effectiveness of these models for the task of automatic question generation from
paragraph.
•	Specifically, we propose a novel hierarchical Transformer architecture. At the lower level,
the encoder first encodes words and produces a sentence-level representation. At the higher
level, the encoder aggregates the sentence-level representations and learns a paragraph-
level representation.
•	We also propose a novel hierarchical BiLSTM model with selective attention, which learns
to attend to important sentences and words from the paragraph that are relevant to generate
meaningful and fluent questions about the encoded answer.
•	We also present attention mechanisms for dynamically incorporating contextual informa-
tion in the hierarchical paragraph encoders and experimentally validate their effectiveness.
2	Related Work
Question generation (QG) has recently attracted significant interests in the natural language process-
ing (NLP) (Du et al., 2017; Kumar et al., 2018; Song et al., 2018; Kumar et al., 2019) and computer
vision (CV) (Li et al., 2018; Fan et al., 2018) communities. Given an input (e.g., a passage of text in
NLP or an image in CV), optionally also an answer, the task of QG is to generate a natural-language
question that is answerable from the input.
Existing text-based QG methods can be broadly classified into three categories: (a) rule-based meth-
ods, (b) template-base methods, and (c) neural network-based methods. Rule based methods (Heil-
man & Smith, 2010) perform syntactic and semantic analysis of sentences and apply fixed sets of
rules to generate questions. They mostly rely on syntactic rules written by humans (Heilman, 2011)
and these rules change from domain to domain. On the other hand, template based methods (Ali
et al., 2010) use generic templates/slot fillers to generate questions. More recently, neural network-
based QG methods (Du et al., 2017; Kumar et al., 2018; Song et al., 2018) have been proposed. They
employ an RNN-based encoder-decoder architecture and train in an end-to-end fashion, without the
need of manually created rules or templates.
Du et al. (2017) were the first to propose a sequence-to-sequence (Seq2Seq) architecture for QG.
Kumar et al. (2018) proposed to augment each word with linguistic features and encode the most
relevant pivotal answer in the text while generating questions. Similarly, Song et al. (2018) encode
ground-truth answers (given in the training data), use the copy mechanism and additionally employ
context matching to capture interactions between the answer and its context within the passage.
They encode ground-truth answer for generating questions which might not be available for the test
set.
Zhao et al. (2018) recently proposed a Seq2Seq model for paragraph-level question generation,
where they employ a maxout pointer mechanism with a gated self-attention encoder. Tran et al.
(2018) contrast recurrent and non-recurrent architectures on their effectiveness in capturing the hi-
erarchical structure. In Machine Translation, non-recurrent model such as a Transformer (Vaswani
et al., 2017) that does not use convolution or recurrent connection is often expected to perform bet-
ter. However, Transformer, as a non-recurrent model, can be more effective than the recurrent model
2
Under review as a conference paper at ICLR 2020
because it has full access to the sequence history. Our findings also suggest that LSTM outperforms
the Transformer in capturing the hierarchical structure. In contrast, Goldberg (2019) report settings
in which attention-based models, such as BERT are better capable of learning hierarchical structure
than LSTM-based models.
3	Hierarchical Paragraph Representation
We propose a general hierarchical architecture for better paragraph representation at the level of
words and sentences. This architecture is agnostic to the type of encoder, so we base our hier-
archical architectures on BiLSTM and Transformers. We then present two decoders (LSTM and
Transformer) with hierarchical attention over the paragraph representation, in order to provide the
dynamic context needed by the decoder. The decoder is further conditioned on the provided (candi-
date) answer to generate relevant questions.
Notation: The question generation task consists of pairs (X, y) conditioned on an encoded answer
z, where X is a paragraph, and y is the target question which needs to be generated with respect to
the paragraph.. Let us denote the i-th sentence in the paragraph by xi, where xi,j denotes the j-th
word of the sentence. We assume that the first and last words of the sentence are special beginning-
of-the-sentence < BOS > and end-of-the-sentence < EOS > tokens, respectively.
3.1	Hierarchical Paragraph Encoder
Our hierarchical paragraph encoder (HPE ) consists of two encoders, viz., a sentence-level and a
word-level encoder; (c.f. Figure 1).
Word-Level Encoder: The lower-level encoder WORDENC encodes the words of individual sen-
tences. This encoder produces a sentence-dependent word representation ri,j for each word xi,j in
a sentence xi, i.e., ri = WORDENC (xi). This representation is the output of the last encoder block
in the case of Transformer, and the last hidden state in the case of BiLSTM. Furthermore, we can
produce a fixed-dimensional representation for a sentence as a function of ri, e.g., by summing (or
averaging) its contextual word representations, or concatenating the contextual representations of
its < BOS > and < EOS > tokens. We denote the resulting sentence representation by Si for a
sentence xi .
Sentence-Level Encoder: At the higher level, our HPE consists of another encoder to produce
paragraph-dependent representation for the sentences. The input to this encoder are the sentence
representations produced by the lower level encoder, which are insensitive to the paragraph context.
In the case of the transformer, the sentence representation is combined with its positional embedding
to take the ordering of the paragraph sentences into account. The output of the higher-level encoder
is contextual representation for each set of sentences s = SENTENC (Ss), where si is the paragraph-
dependent representation for the i-th sentence.
In the following two sub-sections, we present our two hierarchical encoding architectures, viz., the
hierarchical BiLSTM in Section 3.2) and hierarchical transformer in Section 3.3).
3.2	Dynamic Context in BiLSTM-HPE
In this first option, c.f., Figure 1, we use both word-level attention and sentence level attention in a
Hierarchical BiLSTM encoder to obtain the hierarchical paragraph representation. We employ the
attention mechanism proposed in (Luong et al., 2015) at both the word and sentence levels. We
employ the BiLSTM (Bidirectional LSTM) as both, the word as well as the sentence level encoders.
We concatenate forward and backward hidden states to obtain sentence/paragraph representations.
Subsequently, we employ a unidirectional LSTM unit as our decoder, that generates the target ques-
tion one word at a time, conditioned on (i) all the words generated in the previous time steps and (ii)
on the encoded answer. The methodology employed in these modules has been described next.
Word-level Attention: We use the LSTM decoder’s previous hidden state and the word encoder’s
hidden state to compute attention over words (Figure 1). We the concatenate forward and backward
3
Under review as a conference paper at ICLR 2020
Figure 1: Our hierarchial LSTM Architecture.
Decoder i
Sentence Level
Encoder
Word Level
Encoder
Word
Embeddings
hidden states of the BiLSTM encoder to obtain the final hidden state representation (ht) at time step
t. Representation (ht) is calculated as: ht = WORDENC (ht-1 , [et, ftw]), where et represents the
GLoVE (Pennington et al., 2014) embedded representation of word (xi,j) at time step t and ftw is
the embedded BIO feature for answer encoding.
The word level attention (atw) is computed as: atw = Softmax([utwi]iM=1), where M is the number of
words, and utwi = vTw tanh(Ww[hi, dt]) and dt is the decoder hidden state at time step t.
We calculate sentence representation (S) using word level encoder's hidden states as: Si =
∣X1^ Pj rij, where r%/is the word encoder hidden state representation of the jth word of the ith
sentence.
Sentence-level Attention: We feed the sentence representations s to our sentence-level BiLSTM
encoder (c.f. Figure 1). Similar to the word-level attention, we again the compute attention weight
over every sentence in the input passage, using (i) the previous decoder hidden state and (ii) the
sentence encoder’s hidden state. As before, we concatenate the forward and backward hidden states
of the sentence level encoder to obtain the final hidden state representation. The hidden state (gt)
of the sentence level encoder is computed as: gt = SENTENC (gt-ι, [5t,f；]), where fS is the
embedded feature vector denoting whether the sentence contains the encoded answer or not.
The selective sentence level attention (ats) is computed as: ats = Sparsemax([utwi]iK=1), where, K is
the number of sentences, utsi = vsT tanh(Ws [gi, dt]).
The final context (ct) based on hierarchical selective attention is computed as: ct =
Pi aSi Pj aW jri,j, where aW § is the word attention score obtained from aW corresponding to jth
word of the ith sentence.
The context vector ct is fed to the decoder at time step t along with embedded representation of the
previous output.
3.3	Dynamic Context in Transformer-HPE
In this second option (c.f. Figure 2), we make use of a Transformer decoder to generate the target
question, one token at a time, from left to right. For generating the next token, the decoder attends to
the previously generated tokens in the question, the encoded answer and the paragraph. We postulate
that attention to the paragraph benefits from our hierarchical representation, described in Section
3.1. That is, our model identifies firstly the relevance of the sentences, and then the relevance of the
words within the sentences. This results in a hierarchical attention module (HATT) and its multi-
4
Under review as a conference paper at ICLR 2020
」9POOU9 LIdegBJed
Aggregate sentence encoder output
Output
probabilities
SoftmaX Layer
Linear Layer
Decoder #n
Add & Normalise
Feed Forward ∣
」9POOU9 90usu9s
Figure 2: Our hierarchical Transformer architecture.
Encoder-Decoder Attention
Add & Normalise
Decoder Input
Position (Question)
Encoding
head extension (MHATT), which replace the attention mechanism to the source in the Transformer
decoder. We first explain the sentence and paragragh encoders (Section 3.3.1) before moving on
to explanation of the decoder (Section 3.3.2) and the hierarchical attention modules (HATT and
MHATT in Section 3.3.3).
3.3.1	Sentence and paragraph encoder
The sentence encoder transformer maps an input sequence of word representations x =
(x0,…，xn) to a sequence of continuous sentence representations r = (ro,…，rn). Paragraph
encoder takes concatenation of word representation of the start word and end word as input and re-
turns paragraph representation. Each encoder layer is composed of two sub-layers namely a multi-
head self attention layer (Section 3.3.3) and a position wise fully connected feed forward neural
network (Section 3.3.4). To be able to effectively describe these modules, we will benefit first from
a description of the decoder (Section 3.3.2).
3.3.2	Decoder
The decoder stack is similar to encoder stack except that it has an additional sub layer (encoder-
decoder attention layer) which learn multi-head self attention over the output of the paragraph en-
coder.
The output of the paragraph encoder is transformed into a set of attention vectors Kencdec and
Vencdec . Encoder-decoder attention layer of decoder takes the key Kencdec and value Vencdec .
Decoder stack will output a float vector, we can feed this float vector to a linear followed softmax
layer to get probability for generating target word.
3.3.3	THE HATT AND MHATT MODULES
Let us assume that the question decoder needs to attend to the source paragraph during the generation
process. To attend to the hierarchical paragraph representation, we replace the multi-head attention
mechanism (to the source) in Transformer by introducing a new multi-head hierarchical attention
module MHATT (qs, Ks, qw, Kw, V w) where qs is the sentence-level query vector, qw is the word
5
Under review as a conference paper at ICLR 2020
level query vector, Ks is the key matrix for the sentences of the paragraph, Kw is the key matrix for
the words of the paragraph, and V w is the value matrix fr the words of the paragraph.
The vectors of sentence-level query qs and word-level query qs are created using non-linear trans-
formations of the state of the decoder ht-1, i.e. the input vector to the softmax function when
generating the previous word wt-1 of the question. The matrices for the sentence-level key Ks and
word-level key Kw are created using the output. We take the input vector to the softmax function
ht-1, when the t-th word in the question is being generated. Firstly, this module attends to para-
graph sentences using their keys and the sentence query vector: a = softmax(qsKs/d). Here, d
is the dimension of the query/key vectors; the dimension of the resulting attention vector would be
the number of sentences in the paragraph. Secondly, it computes an attention vector for the words
of each sentence: bi = sof tmax(qw Kwi /d). Here, Kwi is the key matrix for the words in the i-th
sentences; the dimension of the resulting attention vector bi is the number of tokens in the i-th sen-
tence. Lastly, the context vector is computed using the word values of their attention weights based
on their sentence-level and word-level attentions:
|d|
HATT(qs,Ks, qw,Kw,Vw) = X ai(bi ∙ Vr)
i=1
Attention in MHATT module is calculated as:
QwKw
Attention(Qw, Vw, Kw) = Softmax( —^^)Vw	(1)
dk
Where Attention(Qw, Vw, Kw) is reformulation of scaled dot product attention of (Vaswani et al.,
2017). For multiple heads, the multihead attention z = Multihead(Qw, Kw, Vw) is calculated as:
Multihead(Qw, Kw, Vw) = Concat(h0, h2, .., hn)WO	(2)
where hi = Attention(QwWiQ,KwWiK,VwWiV), WiQ ∈ Rdmodel ×dk, WiK ∈ Rdmodel×dk ,
WiV ∈ Rdmodel ×dv, WO ∈ Rhdv×dmodel, dk = dv = dmodel/h = 64.
z is fed to a position-wise fully connected feed forward neural network to obtain the final input
representation.
3.3.4	Position-wise Fully Connected Feed Forward Neural Network
Output of the HATT module is passed to a fully connected feed forward neural net (FFNN) for
calculating the hierarchical representation of input (r) as: r = FFNN (x) = (max(0, xW1 +
b1))W2 + b2, where r is fed as input to the next encoder layers. The final representation r from last
layer of decoder is fed to the linear followed by softmax layer for calculating output probabilities.
4	Experimental Setup
4.1	Datasets
We performed all our experiments on the publicly available SQuAD (Rajpurkar et al., 2016) and
MS MARCO (Nguyen et al., 2016) datasets. SQuAD contains 536 Wikipedia articles and more
than 100K questions posed about the articles by crowd-workers. We split the SQuAD train set by
the ratio 90%-10% into train and dev set and take SQuAD dev set as our test set for evaluation. We
take an entire paragraph in each train/test instance as input in all our experiments. MS MARCO
contains passages that are retrieved from web documents and the questions are anonimized versions
of BING queries. We take a subset of MS MARCO v1.1 dataset containing questions that are
answerable from atleast one paragraph. We split train set as 90%-10% into train (71k) and dev (8k)
and take dev set as test set (9.5k). Our split is same but our dataset also contains (para, question)
tuples whose answers are not a subspan of the paragraph, thus making our task more difficult.
4.2	Evaluation metrics
For evaluating our question generation model we report the standard metrics, viz., BLEU (Papineni
et al., 2002) and ROUGE-L(Lin, 2004). We performed human evaluation to further analyze quality
6
Under review as a conference paper at ICLR 2020
of questions generated by all the models. We analyzed quality of questions generated on a) syntactic
correctness b) semantic correctness and c) relevance to the given paragraph.
4.3	Models
We compare QG results of our hierarchical LSTM and hierarchical Transformer with their flat coun-
terparts. We describe our models below:
Seq2Seq + att + AE is the attention-based sequence-to-sequence model with a BiLSTM encoder,
answer encoding and an LSTM decoder.
HierSeq2Seq + AE is the hierarchical BiLSTM model with a BiLSTM sentence encoder, a BiL-
STM paragraph encoder and an LSTM decoder conditioned on encoded answer.
TransSeq2Seq + AE is a Transformer-based sequence-to-sequence model with a Transformer en-
coder followed by a Transformer decoder conditioned on encoded answer.
HierTransSeq2Seq + AE is the hierarchical Transformer model with a Transformer sentence en-
coder, a Transformer paragraph encoder followed by a Transformer decoder conditioned on answer
encoded.
Model	BLEU-I	BLEU-2	BLEU-3	BLEU-4	ROUGE-L
Seq2Seq + att + AE	52.86	29.02	17.06	10.26	-3817-
TransSeq2Seq + AE	42.07	22.03	12.33	7.45	36.77
HierSeq2seq + AE	54.36	30.62	18.43	11.50	-38.83-
HierTransSeq2Seq + AE	54.49	29.79	17.45	10.80	41.13
Table 1: Automatic evaluation results on the SQuAD dataset. For each metric, best result is bolded
Model	BLEU-1	BLEU-2	BLEU-3	BLEU-4	ROUGE-L
Seq2Seq + att + AE	35.90	23.52	15.15	10.10	-3105-
TransSeq2Seq + AE	24.90	15.32	9.46	6.13	30.76
HierSeq2Seq + AE	38.08	25.33	16.48	11.13	-32.82-
HierTransSeq2Seq + AE	31.49	20.05	12.60	8.68	31.88
Table 2: Automatic evaluation results on the MS MARCO dataset. For each metric, best result is
bolded
Model	Syntax		Semantics		Relevance	
	Score	Kappa	Score	Kappa	Score	Kappa
Seq2Seq + att + AE	86	0.57	79.33	0.61	70.66	0.56
TransSeq2Seq + AE	86	0.66	84	0.62	50	0.60
HierSeq2Seq + AE	80	0.49	73.33	0.54	81.33	0.64
HierTransSeq2Seq + AE	90	0.62	85.33	0.65	68	0.56
Table 3: Human evaluation results (column “Score”) as well as inter-rater agreement (column
“Kappa”) for each model on the SQuAD test set. The scores are between 0-100, 0 being the worst
and 100 being the best. Best results for each metric (column) are bolded. The three evaluation
criteria are: (1) syntactically correct (Syntax), (2) semantically correct (Semantics), and (3) relevant
to the text (Relevance).
5	Results and discussion
In Table 1 and Table 2 we present automatic evaluation results of all models on SQuAD and MS
MARCO datasets respectively. We present human evaluation results in Table 3 and Table 4 respec-
tively.
A number of interesting observations can be made from automatic evaluation results in Table 1 and
Table 2:
7
Under review as a conference paper at ICLR 2020
Model	Syntax		Semantics		Relevance	
	Score	Kappa	Score	Kappa	Score	Kappa
Seq2Seq + att + AE	83.33	0.68	69.33	0.65	38.66	0.52
TranSSeq2Seq + AE	80.66	0.73	73.33	0.55	35.33	0.47
HierSeq2Seq + AE	85.33	0.73	70.66	0.68	51.33	0.60
HierTranSSeq2Seq + AE	86	0.87	73.33	0.65	32.66	0.60
Table 4: Human evaluation results (column “Score”) as well as inter-rater agreement (column
“Kappa”) for each model on the MS MARCO test set. The scores are between 0-100, 0 being
the worst and 100 being the best. Best results for each metric (column) are bolded. The three eval-
uation criteria are: (1) syntactically correct (Syntax), (2) semantically correct (Semantics), and (3)
relevant to the text (Relevance).
•	Overall, the hierarchical BiLSTM model HierSeq2Seq + AE shows the best performance,
achieving best result on BLEU2-BLEU4 metrics on both SQUAD dataset, whereas the hier-
archical Transformer model TransSeq2Seq + AE performs best on BLEU1 and ROUGE-L
on the SQuAD dataset.
•	Compared to the flat LSTM and Transformer models, their respective hierarchical counter-
parts always perform better on both the SQuAD and MS MARCO datasets.
•	On the MS MARCO dataset, we observe the best consistent performance using the hierar-
chical BiLSTM models on all automatic evaluation metrics.
•	On the MS MARCO dataset, the two LSTM-based models outperform the two
Transformer-based models.
Interestingly, human evaluation results, as tabulated in Table 3 and Table 4, demonstrate that the hi-
erarchical Transformer model TransSeq2Seq + AE outperforms all the other models on both datasets
in both syntactic and semantic correctness. However, the hierarchical BiLSTM model HierSeq2Seq
+ AE achieves best, and significantly better, relevance scores on both datasets.
From the evaluation results, we can see that our proposed hierarchical models demonstrate benefits
over their respective flat counterparts in a significant way. Thus, for paragraph-level question gener-
ation, the hierarchical representation of paragraphs is a worthy pursuit. Moreover, the Transformer
architecture shows great potential over the more traditional RNN models such as BiLSTM as shown
in human evaluation. Thus the continued investigation of hierarchical Transformer is a promising
research avenue. In the Appendix, in Section B, we present several examples that illustrate the ef-
fectiveness of our Hierarchical models. In Section C of the appendix, we present some failure cases
of our model, along with plausible explanations.
6	Conclusion
We proposed two hierarchical models for the challenging task of question generation from para-
graphs, one of which is based on a hierarchical BiLSTM model and the other is a novel hierarchi-
cal Transformer architecture. We perform extensive experimental evaluation on the SQuAD and
MS MARCO datasets using standard metrics. Results demonstrate the hierarchical representations
to be overall much more effective than their flat counterparts. The hierarchical models for both
Transformer and BiLSTM clearly outperforms their flat counterparts on all metrics in almost all
cases. Further, our experimental results validate that hierarchical selective attention benefits the hi-
erarchical BiLSTM model. Qualitatively, our hierarchical models also exhibit better capability of
generating fluent and relevant questions.
References
Husam Ali, Yllias Chali, and Sadid A Hasan. Automation of question generation from sentences.
In Proceedings ofQG2010: The Third Workshop on Question Generation, pp. 58-67, 2010.
Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia. In
ACL (1), pp. 1907-1917, 2018.
8
Under review as a conference paper at ICLR 2020
Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading
comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1, pp. 1342-1352, 2017.
Zhihao Fan, Zhongyu Wei, Piji Li, Yanyan Lan, and Xuanjing Huang. A question type driven
framework to diversify visual question generation. In IJCAI, pp. 4048-4054, 2018.
Yoav Goldberg. Assessing bert’s syntactic abilities. CoRR, abs/1901.05287, 2019.
Michael Heilman. Automatic factual question generation from text. PhD thesis, Carnegie Mellon
University, 2011.
Michael Heilman and Noah A Smith. Good question! statistical ranking for question generation.
In Human Language Technologies: The 2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pp. 609-617. Association for Computational
Linguistics, 2010.
Vishwajeet Kumar, Kireeti Boorla, Yogesh Meena, Ganesh Ramakrishnan, and Yuan-Fang Li. Au-
tomating reading comprehension by generating question and answer pairs. In Pacific-Asia Con-
ference on Knowledge Discovery and Data Mining, pp. 335-348. Springer, 2018.
Vishwajeet Kumar, Ganesh Ramakrishnan, and Yuan-Fang Li. Putting the horse before the cart: A
generator-evaluator framework for question generation from text. In The SIGNLL Conference on
Computational Natural Language Learning (CoNLL 2019), 2019.
Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou.
Visual question generation as dual task of visual question answering. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 6116-6124, 2018.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In ACL 2004, 2004.
Thang Luong, Hieu Quang Pham, and Christopher D. Manning. Effective approaches to attention-
based neural machine translation. In EMNLP, 2015.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and
Li Deng. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint
arXiv:1611.09268, 2016.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 1532-1543, Doha, Qatar, October 2014. Association for Computational
Linguistics. doi: 10.3115/v1/D14-1162. URL https://www.aclweb.org/anthology/
D14-1162.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2383-2392, 2016.
Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. Leveraging context in-
formation for natural question generation. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 2 (Short Papers), volume 2, pp. 569-574, 2018.
Ke Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling
hierarchical structure. arXiv preprint arXiv:1803.03585, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
9
Under review as a conference paper at ICLR 2020
Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation
with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, pp. 3901-3910, 2018.
10
Under review as a conference paper at ICLR 2020
A Training Details
We implement1 all our models in PyTorch2. In HierSeq2Seq we used 2-layer BiLSTM sentence
and paragraph encoders and a single-layer LSTM decoder. We set RNN hidden size to 600. For
TransSeq2Seq and HierTransSeq2Seq we used 4-layer Transformer encoder and decoder with model
dimension set to 256 and inner hidden dimension set to 2048.
For both the above settings we use a vocabulary of size 45k, shared across source and target of train
and dev sets. We prune out all words with frequency lower than 5 in the entire dataset (train and
dev). We use pretrained GloVe embeddings of 300 dimensions to represent the words in vocabulary.
We use beam search decoder with beam size 5 to decode questions.
To train HierSeq2seq model, we use SGD with momentum for optimization. We set learning rate to
0.1 at the beginning, with the value being halved at every even epochs starting from epoch 8. We
train our models for 20 epochs and select the model with lowest perplexity as the final model. we
train HierTransSeq2seq using Adam optimizer, we set initial learning rate to 1e-7 and warmup steps
to 2000 . We use standard inverse square root scheduler for scheduling the learning rate.
B	Example questions generated by our best model (HierSeq2Seq
+ AE ) on MS MARCO test set
Paragraph - lop stands for lack of prosecution . generally dismissal for lack of prosecution occurs
when the court closes a case as nothing has been filed within a specified period of time . the court
presumes that as there is no record of activity the party does not wish to pursue the case .
Answer - lack of prosecution .
Generated Question - what does lop mean ?
Paragraph - latin meaning : the name claire is a latin baby name . in latin the meaning of the name
claire is : from the feminine form of the latin adjective ’ clarus ’ meaning bright or clear . also
distinguished . famous bearer : twelfth century st clare ( or clara ) of assisi founded the poor clares
order of nuns .
Answer - bright or clear .
Generated Question - what does the name claire mean ?
Paragraph - golden pheasants and silver pheasants ( also a native of china ) have been used symbol-
ically in ancient chinese culture and tradition for hundreds of centuries . to the chinese the golden
pheasant is a symbol of beauty , good fortune and refinement . exotic golden pheasant symbolic in
chinese culture and tradition , photo melbourne zoo . in the song dynasty ( 960 - 1279 ) women wore
robes decorated with colorful pheasants for important state occasions .
Answer - beauty , good fortune and refinement .
Generated Question - what does golden pheasant symbolize ?
Paragraph - distribution of the tiger . ever wonder where tigers live ! well ... most tigers live in
asia , specifically throughout southeast asia , china , korea and russia . tigers like to live in swamps
, grasslands , and rain forests . usually where tigers live there are trees , bushes , and clumps of tall
grass
Answer - tigers live in asia , specifically throughout southeast asia , china , korea and russia .
Generated Question - where do tigers live ?
1Code and dataset will be released upon publication
2https://pytorch.org/
11
Under review as a conference paper at ICLR 2020
C Some Examples where our best model (HierSeq2Seq + AE )
FAILS
Paragraph - 1 boil until tender [ usually , 30 - 45 minutes , depending on the size of the beets]. 2
boiling can take up to 60 minutes for larger beets . 3 drain and run cold water over beets . 4 the skins
will slip right off with the root ends , but make sure you wear kitchen gloves so your hands don’t
turn red .
Answer - 30 - 45 minutes
Expected Question - how long does it take to boil beets
Generated Question - how long does it take to boil spaghetti
Explanation - Model got confused because of highly frequent named entity (spaghetti) in the
dataset. This mistake cannot be attributed to our model/architecture.
Paragraph - ok , the above answer is not true . as an rn in florida i can say that rn ’s starting salary
is between $ 21-$24 per hour with an average of 1 dollar pay increases per hour per year , not to
mention this is before taxes are taken out . rn ’s are in demand in florida , but they are not as well
compensated as is rumored . on average a rn makes from $ 38,000 to $ 70,000 per year . new grads
in some areas such as ny can ma ke $ 50,000 per year . after you get a year or two under your belt
in med surge , try traveling nursing . you can make upwards of $ 80 k to $ 90 k per year ... this
assignments usually are 4 weeks to 12 weeks .
Answer - $ 38,000 to $ 70,000 per year.
Expected Question - how much does an rn make in florida
Generated Question - how much does a rn make in california
Explanation - Model got confused because of highly frequent named entity (california) in the train-
ing set. This may be because we use pre-trained GloVe embeddings. This mistake cannot be at-
tributed to our model/architecture.
12