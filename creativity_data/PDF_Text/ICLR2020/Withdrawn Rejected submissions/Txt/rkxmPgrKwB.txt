Under review as a conference paper at ICLR 2020
Weight-space symmetry in neural network
LOSS LANDSCAPES REVISITED
Anonymous authors
Paper under double-blind review
Ab stract
Neural network training depends on the structure of the underlying loss landscape,
i.e. local minima, saddle points, flat plateaus, and loss barriers. In relation to the
structure of the landscape, we study the permutation symmetry of neurons in each
layer of a deep neural network, which gives rise not only to multiple equivalent
global minima of the loss function but also to critical points in between partner
minima. In a network of d- 1 hidden layers with nk neurons in layers k = 1, . . . , d,
we construct continuous paths between equivalent global minima that lead through
a ‘permutation point’ where the input and output weight vectors of two neurons in
the same hidden layer k collide and interchange. We show that such permutation
points are critical points which lie inside high-dimensional subspaces of equal loss,
contributing to the global flatness of the landscape. We also find that a permutation
point for the exchange of neurons i and j transits into a flat high-dimensional
plateau that enables all nk! permutations of neurons in a given layer k at the same
loss value. Moreover, we introduce higher-order permutation points by exploiting
the hierarchical structure in the loss landscapes of neural networks, and find that
the number of K-th order permutation points is much larger than the (already huge)
number of equivalent global minima - at least by a polynomial factor of order
K . In two tasks, we demonstrate numerically with our path finding method that
continuous paths between partner minima exist: first, in a toy network with a single
hidden layer on a function approximation task and, second, in a multilayer network
on the MNIST task. Our geometric approach yields a lower bound on the number
of critical points generated by weight-space symmetries and provides a simple
intuitive link between previous theoretical results and numerical observations.
1	Introduction
The structure of the loss landscape plays an important role in the optimization of neural network
parameters. A large number of numerical (Dauphin et al., 2014; Goodfellow et al., 2014; Li et al.,
2018; Sagun et al., 2014; 2016; Ballard et al., 2017; Garipov et al., 2018; Draxler et al., 2018; Sagun
et al., 2017; Baity-Jesi et al., 2018) and theoretical (Choromanska et al., 2015; Rasmussen, 2003;
Freeman and Bruna, 2016; Soudry and Carmon, 2016; Nguyen and Hein, 2017) studies have explored
the properties of the loss landscape. In particular, in a multilayer network of d - 1 hidden layers
with n neurons each, there are (n!)d-1 equivalent configurations corresponding to the permutation of
neuron indices in each layer of the network (Goodfellow et al., 2016; Bishop, 1995). The permutation
symmetries give rise to a loss landscape where any given global minimum in the weight space must
have (n!)d-1 - 1 completely equivalent partner minima. This property of neural network landscapes
is called weight-space symmetry.
Several (Saad and Solla, 1995; Amari et al., 2006; Wei et al., 2008) works explored the implications of
weight-space symmetry for training dynamics in two-layer networks and found that training dynamics
slow down near the singular regions caused by weight-space symmetry. Dauphin et al. (2014); Orhan
and Pitkow (2017) argue that optimization paths may get close to the singular regions induced by
weight-space symmetry and this, in turn, slows down training for deep neural networks. Exploiting
weight-space symmetries, we give insights into and partial explanations of three observations on
neural network landscapes.
1
Under review as a conference paper at ICLR 2020
Observation 1. Training dynamics are slow near singular regions caused by weight-space symmetry
and stochastic gradient descent might travel near these regions throughout training (Saad and Solla,
1995; Wei et al., 2008; Amari et al., 2006; Dauphin et al., 2014; Orhan and Pitkow, 2017).
Observation 2. The Hessian of the loss function has numerous almost-zero eigenvalues throughout
training, thus the landscape is flat in many directions (Sagun et al., 2017; Papyan, 2018; Ghorbani
et al., 2019).
Related to observation 1 and 2, we prove the existence of numerous connected high-dimensional
plateaus extending across the landscape due to weight-space symmetries.
Observation 3. The number of saddles can grow exponentially in neural network landscapes (Auer
et al., 1996; Dauphin et al., 2014; Choromanska et al., 2015).
Related to observation 3, we prove that there are at least polynomially many more saddles than the
global minima due to weight-space symmetries in neural networks, without any further assumptions.
In addition, we propose a novel low-loss path finding algorithm to find barriers between partner
minima. We start from the known permutation symmetries and consider continuous low-loss paths
that connect two equivalent global minima by merging the weight vectors of two neurons in a specific
way. At a so-called permutation point, where the distance between the input and output weight
vectors of the two neurons vanishes, the indices of the two neurons can be interchanged at no extra
cost. After the change, the system returns on the ‘mirrored’ path back to the original configuration
— except for the permutation of one pair of indices.
Surprisingly, we find that we can permute all neuron indices in the same layer at the same cost
as the loss at a permutation point reached by moving along the path that merges a single pair of
neurons. These constant-loss permutations are possible because each permutation point lies in a
high-dimensional plateau of critical points. Our theory can be extended to higher-order saddles
and provides explicit lower bounds for the number of first- and higher-order permutation points.
Numerically, we confirm the existence of first-order permutation saddles.
In particular, the specific contributions of our work are:
•	A simple low-loss path-finding algorithm linking partner global minima via a permutation
point, implemented by minimization under a single scalar constraint (distance of weight
vectors).
•	The theoretical characterization of permutation points, for example that these are critical
points and several permutation points are connected via paths at equal loss.
•	A lower bound for the number of first- and higher-order permutation points and their
corresponding plateaus.
•	Numerical demonstrations of the path finding method in multilayer neural networks trained
on MNIST.
1.1	Related Work
Structure of the landscape. For linear networks, it was shown that all the critical points - except
for the global minimum - are saddles in the case of two-layer (Baldi and Hornik, 1989) or multilayer
networks (Freeman and Bruna, 2016; Kawaguchi, 2016; Lu and Kawaguchi, 2017). Interestingly,
deep linear networks are reported to exhibit sharp transitions at the edges of extended plateaus (Saxe
et al., 2013), similar to the plateaus observed in deep nonlinear networks (Goodfellow et al., 2014).
For nonlinear multilayer networks, Choromanska et al. (2015) argue that all local minima lie below a
certain loss value by drawing connections to the spherical spin-glass model. Improving upon this
result, Soudry and Carmon (2016); Nguyen and Hein (2017) prove that almost all local minima are
global minima for multilayer networks under mild over-parametrization assumptions.
Bottom of the landscape. Another line of research studies the bottom of the landscape containing
global minima and low-loss barriers between them. Freeman and Bruna (2016) prove the existence of
low-loss paths connecting global minima for wide two-layer networks by upper-bounding the loss
along the path with a parameter that depends on the number of parameters and data smoothness.
Draxler et al. (2018) use Nudged Elastic Band method introduced in J6nsson et al. (1998) to
connect independent minima and numerically find that the barrier vanishes consistently for increasing
2
Under review as a conference paper at ICLR 2020
width and depth in DenseNet, ConvNet and ResNet architectures trained on CIFAR datasets. In a
simultaneous work, Garipov et al. (2018) confirm that there is no significant barrier by connecting
independent minima with polygonal chains.
Training dynamics in the landscape. For general loss functions, Lee et al. (2016) show that gradient
descent with sufficiently small step-size converges to local minima if all the saddles have at least
one negative eigenvalue. For overparametrized neural networks, gradient descent converges to
global minima without moving far from initialization (Jacot et al., 2018; Du et al., 2018a;b), thus
suggesting convex-like behavior around random initialization. For the finite size networks, how
training dynamics converge to a minima and in particular how fast they converge remain an open
question. For soft-committee machines, it turns out that the initial learning dynamics are slowed
down by correlation of hidden neurons (Saad and Solla, 1995; Engel and Van den Broeck, 2001;
Inoue et al., 2003). Amari et al. (2006); Wei et al. (2008) show that training dynamics slow down
near singular regions due to weight-space symmetry. Dauphin et al. (2014) empirically argue that
the large number of saddle points in the landscape makes training slow. Orhan and Pitkow (2017)
numerically find that stochastic gradient descent may slow down near plateaus due to weight-space
symmetry for deep (30 layers) feedforward networks trained on CIFAR100.
In this paper we show that there is an impressively large number of permutation points. Each
permutation point is a critical point (either a local minimum or a saddle) with a large number of flat
directions, potentially linked to the empirically observed plateaus. In contrast to an earlier study by
Fukumizu and Amari (2000) with a scalar output for two-layered networks where a line of critical
points around the permutation point was reported, we study a deep network with d - 1 hidden layers
and find multi-dimensional equal-loss plateaus. Moreover, we give a novel lower bound on the
number of permutation points and construct sample paths between global minima using an algorithm
that is different from previously used methods (Garipov et al., 2018; Draxler et al., 2018), since it
exploits the symmetries at the permutation point.
1.2	Preliminaries
We study multilayer neural networks f(x; θ) with input x ∈ Rn0, d layers of n1, . . . , nd neurons
per layer, parameters θ = {W (k) ∈ Rnk×nk-1 and b(k) ∈ Rnk : k ∈ {1, . . . , d}} ∈ Θ and
nd-dimensional output
f(x; θ) = W⑷g(…g(w⑵g(W⑴X + b⑴)+ b(2))…)+ b(d),	(1)
where g is a nonlinear activation function that operates component-wise on any vector.
Definition 1 & 2. (Parameter vector) We define the parameter vector Em) of neuron m in
layer k as the incoming weights to a neuron m in layer k concatenated with its bias term:
E(mk) = W(mk,)1, . . . , W (mk,)nk-1 , b(mk). (Output weight vector) We define the output weight vec-
tor of neuron m in layer k as its outgoing weights from neuron m in layer k to the next layer:
W (1k,m+1),..., W (nkk++11),m.
Since one can permute the neurons within each layer without changing the network function f(x; θ),
any point θ induces a ‘permutation set’. Definition 3. (Permutation set)
P (θ) = {θ0 ∈ Θ: W 0σ((kk))(i),σ(k-1)(j) = W i(,kj) and b0σ((kk))(i) =bi(k),k∈{1,...,d}},	(2)
of points θ0 with f(x; θ) = f(x; θ0), where σ(k) are permutations of the neuron indices {1, . . . , nk}
in (hidden) layer k where σ(0) and σ(d) are fixed trivial permutations, since we want to permute
neither the indices of the input nor that of the output. We will use the notation θ0 = σl(⇔k)m(θ) to
indicate a point θ0 that differs from θ only by swapping neurons l and m in layer k.
Note that the cardinality of a permutation set is maximal with |P(θ)∣ = Qd-I nj only if all
parameter vectors El(k) 6= E(mk) are distinct for every l 6= m and layer k ∈ {1, . . . , d - 1}. In the
following, we will assume that, at global minima, all parameter vectors are distinct at every layer k.
Definition 4. (Permutation point) Consider a minimum of a multilayer network with (n1, . . . , nk -
1, . . . , nd ) neurons per layer. We can map this minimum to a configuration in the landscape of a
3
Under review as a conference paper at ICLR 2020
multilayer network with (n1 , . . . , nk , . . . , nd) neurons per layer by duplicating one neuron m ∈
{1, . . . , nk - 1} in layer k as follows: (i) substitute the parameter vector of the new neuron with
a copy of the parameter vector Qm) of neuron m, (ii) replace the output weight vector of the new
neuron and the duplicated neuron m with the initial output weight of neuron m rescaled by 2, and
(iii) keep all the other parameters the same. This new configuration where the parameter vectors and
the output weight vectors of the new neuron and the duplicated neuron m are the same will be called
a permutation point, denoted by θl(⇔k)m, i.e. θl(⇔k)m = σl(⇔k)m(θl(⇔k)m).
For training data D = {(xμ,yμ) : μ ∈ {1,...,T}} with targets yμ ∈ Y, we define a loss function
L(θ; D) = Tt PT=I '(y*,f (x*; θ)), where ' : YX Rnd → R is some single-sample loss function.
To simplify notation we will usually omit the explicit mentioning of the data in the loss function, i.e.
L(θ) ≡ L(θ; D).
A	B
Figure 1: A. Configuration of two parameter vectors Ql(k) and Q(mk) (black) at a minimum θ and a
potential path (green) towards a permutation point θl(⇔k)m (?). The path is parametrized by the distance
d. Along the path the distance d (blue dashed lines) decreases continuously starting at dl(,km) (θ). Note
that the path can lead to a permutation point far away from the initial configuration, outside the linear
subspace spanned by the parameter vectors Ql(k) and Q(mk). B. We exclude hypothetical paths where
the distance along the path increases.
2 Main results
In this section, we will first present a novel method to find a low-loss path between partner minima.
Our method ensures that this path passes through a ‘permutation point’. We study the properties of
permutation points. Furthermore, we will introduce higher-order permutation points and provide a
lower-bound on their number.
2.1	A novel method to construct low-loss paths between partner minima
One natural question regarding the geometry of the bottom of the landscape is the following: is it
possible to find a continous low-loss path that connects two minima? In this work, we are interested
in finding the barriers between partner global minima. In particular, we want to find a continuous
low-loss path γ : [0, 1] → Θ connecting two partner minima by first merging two parameter vectors
and output weight vectors (‘permutation point’) and then completing the path using symmetry. We
will first introduce some concepts to introduce this low-loss path between partner minima formally.
Definition 5. (Distance function) dl(,km) : Θ → R+ is a distance function that takes a configuration θ
and returns the squared Euclidean distance between the parameter vectors of neuron l and m at layer
k:
dl(,km) (θ) = kQl(k) - Q(mk)k2	(3)
4
Under review as a conference paper at ICLR 2020
Our idea is to find a low-barrier path γ* = argminγjo, 1 ]→θ maxt∈[o, 1 ] L(Y(t)) under the following
constraints for the initial (t = 0) and quarter-way (t = 1) configurations: γ*(0) = θ, where θ is the
parameter configuration at the minimum, and d(k) (γ*(4)) = 0. Furthermore, the distance between
parameter vectors g∖kk and Hm) in layer k is decreasing, i.e. d(kn(γ*(t)) < d(Ikm(γ*(t0)) for all
t>t0 ∈ [0,1 ]1 (see Fig. 1 and pseudocode in Appendix).
The quarter-way configuration guarantees that the parameter vectors are identical Hl(k) = H(mk), but
puts no constraints on the output weights of neurons m and l in layer k. We can continously move
from the quarter-way configuration (t = 4 )to a configuration at t = 2 where the outputs weights
of the related neurons are equal, without making any changes to the network output or the loss L
as follows: we will increase all output weights W kn+,l 1 of neuron l and decrease the corresponding
output weights W kn+,m1 of neuron m by the same amount continuously so as to keep their sum fixed
until Wkn+,l1 = W kn+,m1 for every neuron n ∈ {1, . . . , nk+1} at layer k + 1 (see Appendix Fig. 4).
Lemma 1. The configuration at t = 11 is one of the permutation points.
Once we have reached t = 2, we interchange the neuron indices of the 'merged, neurons and continue
on the ‘mirror’ path that results from walking the first half of the path backwards with interchanged
neuron indices, until we arrive at the partner minimum at t = 1, i.e. γ( 1 + T) = σ(⇔⇔m(γ( 1 - T))
for τ ∈ [0,1 ].
To find such paths algorithmically, we reparametrize H(mk)(t) = Hl(k)(t) + d(t)e(t) where d(t) is
a positive scalar and e(t) is a unit-length vector. We start with d(0) = dl(,km) (θ) and initialize e in
direction of the difference H(mk) - Hl(k) at the global minimum i.e., the initial parameter configuration.
Next, we decrease d infinitesimally and perform gradient descent for fixed d on the loss L until
convergence. Note that all parameters can change, including Hl(k) and e during gradient descent. This
procedure is repeated until d = 0 at t = 4. Finally we shift the respective output weights to the same
value without changing the network function (see Appendix Fig. 4).
Since the path connects two partner minima, there must be at least one saddle point on the path
γ(t), t ∈ [0, 1], potentially but not necessarily, at the permutation point. Moreover, there is no
guarantee that the highest saddle should be located at the permutation point (see Appendix Fig. 5).
2.2 Characterization of permutation points
In an earlier work, Fukumizu and Amari (2000) studied a specific set of critical points induced by
the hierarchical structure in the neural network landscapes in two-layer neural networks. Let L(H )
be the loss function (‘landscape’) of a two-layer neural network with H neurons in the hidden layer
and a single output. They showed that any critical point in the landscape of L(H-1) induces a line
of critical points in the landscape of L(H). We study permutation points in the general setup for the
neural networks with multiple outputs and multiple layers.
Theorem 1. (Fukumizu and Amari, 2000) By duplicating one parameter vector of any critical point
in L(H-1) and keeping the sum of the two output weights corresponding to the duplicated parameter
vectors fixed at the value of the original output weight, one obtains a line of critical points in L(H). 1 2
Proposition 1. (i) Permutation points θl(⇔k)m are critical points of the original loss function.
(ii)	Any permutation point lies inside a nk+1-dimensional equal-loss subspace of critical points.
(iii)	All other permutations of neuron indices in layer k can be performed by continuous equal-loss
transformations starting from permutation points θl(⇔k)m of neurons l and m, i.e. there is a continuous
1Note that the parametrization t including the exact t values for the mentioned configurations are arbitrary
and only used for conceptualizing the path. The relevant parametrization of the path will be the distance d.
2Theorem 1 is easily verified by setting the derivatives of all parameters to zero in L(H-1) (critical point)
and observing that the derivatives of the corresponding configuration in L(H) will be zero under the mapping
stated in the theorem.
5
Under review as a conference paper at ICLR 2020
5
5
d
d
Figure 2: Paths to the same or to different permutation points. Top row. Configuration of the 5
weight vectors Wi(,1:)/bi(1) of the first layer at the global minimum (blue) and at a permutation point
(red) reached after merging the parameter vectors of two neurons. Note that the global minimum (i.e.,
the starting configuration) is the same for (A), (B) and (C) but the loss at the permutation point can
be the same - (A) and (B) - or different - (A) and (C) - depending on the pair of neurons chosen for
merging. Numbers indicate neurons. Bottom row. Quadratic loss L as a function of the distance d
between the neurons to be merged. The distance was decreased in 200 logarithmically spaced steps
from d = d(1m(θ*) to 1/104 of the initial value. For each d, full batch gradient descent on the loss L
was performed until convergence. Training data was generated by sampling 103 two-dimensional
input points xμ from a standard normal distribution and computing labels yμ = f (xμ; θ*) using a
teacher network (shown in blue, equivalent to the configuration at the global minima). The teacher
had a single layer of five hidden neurons with rectified-linear activation function g and one linear
output layer.
path ρ : [0, 1] → Θ such that ρ(0) = θl(⇔k)m and ρ(1) = θi(⇔k)j and L(ρ(t)) = L(ρ(0)) for all
t ∈ [0, 1] and all i 6= j ∈ {1, . . . , nk}. (Proofs: see appendix).
Therefore, each of the nkn2T) different permutation points of layer k corresponds to a plateau
of nk+1 dimensions. This plateau enables the exchange of all indices in layer k. Note that there
can be multiple plateaus on different loss levels that correspond to different local minima of the
smaller networks where one of the neurons is dropped at a permutation point. For example in
Fig. 2 one can exchange all indices in the hidden layer through the configuration in A and B or
through the configuration in C that has another loss level. Note that, amongst all these permutation
points embedded in different plateaus, we could for example search for the one with the lowest cost
—and this lowest-cost permutation would then also connect all global minima caused by arbitrary
permutations of neurons in layer k.
Definition 6. (Higher-order permutation point) Consider a minimum of a multilayer network with
(n1 , . . . , nk - K, . . . , nd ) neurons per layer. We can map this minimum to a configuration in the
landscape of a multilayer network with (n1 , . . . , nk, . . . , nd) neurons per layer by replicating some
neurons mj ∈ {1, . . . , nk - K} in layer k to fill out the parameters of new neurons as follows:
(i) substitute the parameter vectors of the new neurons with one of the parameter vectors of the
initial minimum, (ii) replace the output weight vectors of the new neurons and the output weight of
the corresponding parameter vector mj with the initial output weight of the replicated neuron mj
normalized so that the mentioned output weight vectors sum up to the original output weight vectors,
and (iii) keep all the other parameters the same. This new configuration where the parameter vectors
and the output weight vectors of the new neurons and the replicated neuron mj for several j in layer
k are the same will be called a K-th order permutation point.
6
Under review as a conference paper at ICLR 2020
This natural generalization on the 1-st order permutation points to higher-orders enables generalizing
Proposition 1(i) and (ii) to the K-th order permutation points.
Proposition 2. (i) A K-th order permutation point is a critical point of the original landscape. (ii)
Any K-th order permutation point at layer k lies in a Knk+1-dimensional subspace of equal loss
parameter configurations. (Proofs: see appendix).
2.3 Counting Higher-Order Permutation Points
A configuration in the landscape of L(H-K) can be mapped to an equivalent configuration in L(H)
with the procedure described in Definition 6. We can then count the number of permutation points that
reduce to the same configuration in L(H-K) combinatorially (see Appendix Fig. 6 for the explanation
of combinatorial counting). For counting, we consider the cardinality of the permutation set of a
permutation point. Since some parameter vectors are replicated, we have to consider permutations of
sometimes identical neurons. This enables finding a lower bound on the number of critical points that
have higher loss values than the global minima in general.
Proposition 3. In a neural network with (n1, . . . , nd) neurons per layer, let T(K, nk) denote the
ratio of the number of Kth-order permutations points at layer k to the number of global minima for
k = 1, . . . , d - 1 and K ≥ 1.
(i)	For K = 1, 2, 3 and nk ≥ 2K, we find T (K, nk) to be:
•	T (K =1,nk )= (nk-1) 2
•	T (K = 2,nk )= "J2)* + (nk2-2) 22
•	T (K = 3,nk )= Ink-3) 4! + (nk2-3) 3! + (nkJ3) 23
(ii)	For general K ≤ n/2, we find the bound T(K,n) ≥ (nkKK)条.(Proofs: see appendix)
Considering all the layers, we note that the number of permutation points of order K is at least
Pd=1 2K (nk-κ) times more than the global minima for 2K ≤ mink n.
Lemma 2. For finite K ∈ Z+ and n → ∞, T(K, nk) > CKnK, since 2K (nkKK) → CKnK∙
When one layer has large number of neurons (i.e. nk → ∞) then the ratio T(K, nk) grows with nkK.
Every permutation point lies inside a high-dimensional subspace of equal loss (Proposition 2(ii),
see Appendix Fig. 7 for illustration). Importantly, every permutation point lies inside a distinct
but connected subspace. Therefore the count for permutation points holds for the corresponding
high-dimensional equal-loss subspaces of critical points.
Lemma 3. In a neural network with (n1, . . . , nd) neurons per layer, there are (at least)
Pkd-=11 T(K, nk) Qkd-=11 nk! many Knk+1-dimensional equal-loss subspaces of critical points at the
loss of a K-th order permutation point for 2K ≤ mink nk.
We could start at an arbitrary configuration in consider the landscape of L(H-K) and the corre-
sponding equal-loss high-dimensional subspaces in L(H), where each configuration in the subspace
computes the same function as the initial configuration. This procedure again would yield the same
number of high-dimensional equal-loss subspaces. Therefore due to weight-space symmetry, neural
network landscapes do not only exhibit numerous high-dimensional plateaus of critical points but
also numerous high-dimensional plateaus (of usually non-critical points) at various loss values.
3	Empirical results
Using a similar procedure as in the toy example (see Fig. 2), we constructed paths between global
minima in a fully connected three-layer network with n1 = n2 = H and n3 = 10 neurons (see
Fig. 3). In order to study global minima we used a student-teacher setting3: the teacher network was
3Paths between local minima could be constructed by starting with a pre-trained network on MNIST and
applying our path finding method with standard gradient descent training on MNIST. We expect to find similar
results.
7
Under review as a conference paper at ICLR 2020
pre-trained on the MNIST data set using negative log-likelihood loss and its parameters θ were kept
fixed thereafter. We initialized the student with the parameters θ* of the teacher and decreased the
distance d between the parameter vectors of two selected neurons m and l in layer k = 2 in 100
logarithmically spaced steps from dm2)l(θ^) to 1/104 of the original value. For every value of d, the
student was trained on a regression task with a mean-squared error loss L between teacher and student
output using full batch gradient descent until convergence. With yμ = f (xμ; θ*) ∈ R10 being the
output of the last layer before the softmax operation, we chose L = T PT=1 kyμ -f (xμ; θ)k2/hyf2i
as the mean squared error loss between teacher and student, where h.i denotes the mean over patterns
and dimensions and μ = 1,...,T enumerates the samples of the data set. Apart from a few cases,
where the trajectory towards the permutation point passed through a saddle on the way, in most cases
the loss increased monotonically until the permutation point. This indicates that the permutation
point is a saddle, and not a minimum. As expected from theoretical results (Freeman and Bruna,
2016) and empirically observed by (Draxler et al., 2018), the barrier height (loss at saddle) decreased
with the number H of hidden neurons per layer.
7
A
——10
15
20
25
4
10
1864
999
...
000
ycarucca gniniart
B
7a
-2
10
0
10
Figure 3: A low-loss permutation path in the loss landscape of a multi-layer network using a
student-teacher setup trained on MNIST. We merged the parameter vectors of two neurons with
high cosine-similarity in the second hidden layer of a three-layer student network. The corresponding
teacher network with H = 10, 15, 20 or 25 was trained on MNIST. For each hidden layer size we
trained 6 teacher networks with different random seeds and display one curve per hidden layer size
and seed. A. In most cases, the mean squared loss L between teacher and student output increases
monotonically along our constructed paths from a global minimum until the permutation point. In
these cases the latter corresponds to the loss barrier along the path. Note that the barrier height (loss
at saddle) decreases with H . B. The MNIST classification accuracy on the training set decreases only
marginally when moving to a permutation point.
4	Discussion
The surprising training performance of neural networks despite their highly non-convex nature
has been drawing attention to the structure of the loss landscape. In this paper, we explored how
weight-space symmetry induces saddles and plateaus in the neural network loss landscape. We
found that special critical points, so-called permutation points, are embedded in high-dimensional
flat plateaus. We proved that all permutation points in a given layer are connected with equal-loss
paths, suggesting new perspectives on loss landscape topology. We provided a novel lower bound
for the number of first- and higher-order permutation points and proposed a low-loss path finding
method to connect equivalent minima. The empirical validation of our path finding algorithm in a
multilayer network trained on MNIST showed that permutation points could indeed be reached in
practice. Additionally, we observed that the loss at the permutation point (barrier) decreased with
network size and thus confirmed Freeman and Bruna (2016)’s findings for loss barriers between
global minima. High-dimensional flat regions around permutation points could be one of the causes
of the empirically observed slow phases in training.
8
Under review as a conference paper at ICLR 2020
References
Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking
the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural
Information Processing Systems, pages 2933-2941, 2014.
I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization
problems. arXiv preprint arXiv:1412.6544, 2014.
H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural nets.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,
Advances in Neural Information Processing Systems 31, pages 6389-6399. Curran Associates, Inc.,
2018.
L. Sagun, V. U. Guney, G. B. Arous, and Y. LeCun. Explorations on high dimensional landscapes.
arXiv preprint arXiv:1412.6615, 2014.
L. Sagun, L. Bottou, and Y. LeCun. Eigenvalues of the Hessian in deep learning: Singularity and
beyond. arXiv preprint arXiv:1611.07476, 2016.
A. J. Ballard, R. Das, S. Martiniani, D. Mehta, L. Sagun, J. D. Stevenson, and D. J. Wales. Energy
landscapes for machine learning. Physical Chemistry Chemical Physics, 19(20):12585-12603,
2017.
T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode
connectivity, and fast ensembling of DNNs. In Advances in Neural Information Processing
Systems, pages 8789-8798, 2018.
F. Draxler, K. Veschgini, M. Salmhofer, and F. A. Hamprecht. Essentially no barriers in neural
network energy landscape. arXiv preprint arXiv:1803.00885, 2018.
L.	Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. Empirical analysis of the Hessian of
over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
M.	Baity-Jesi, L. Sagun, M. Geiger, S. Spigler, G. B. Arous, C. Cammarota, Y. LeCun, M. Wyart,
and G. Biroli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint
arXiv:1803.06969, 2018.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer
networks. In Artificial Intelligence and Statistics, pages 192-204, 2015.
C. E. Rasmussen. Gaussian processes in machine learning. In Summer School on Machine Learning,
pages 63-71. Springer, 2003.
C.	D. Freeman and J. Bruna. Topology and geometry of half-rectified network optimization. arXiv
preprint arXiv:1611.01540, 2016.
D.	Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Q.	Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proceedings of the
34th International Conference on Machine Learning-Volume 70, pages 2603-2612. JMLR. org,
2017.
I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. 2016.
C.	M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, 1995.
D.	Saad and S. A. Solla. On-line learning in soft committee machines. Physical Review E, 52(4):
4225, 1995.
S.-I. Amari, H. Park, and T. Ozeki. Singularities affect dynamics of learning in neuromanifolds.
Neural computation, 18(5):1007-1065, 2006.
9
Under review as a conference paper at ICLR 2020
H. Wei, J. Zhang, F. Cousseau, T. Ozeki, and S.-i. Amari. Dynamics of learning near singularities in
layered networks. Neural computation, 20(3):813-843, 2008.
A.	E. Orhan and X. Pitkow. Skip connections eliminate singularities. arXiv preprint arXiv:1701.09175,
2017.
V. Papyan. The full spectrum of deep net Hessians at scale: Dynamics with sample size. arXiv
preprint arXiv:1811.07062, 2018.
B.	Ghorbani, S. Krishnan, and Y. Xiao. An investigation into neural net optimization via Hessian
eigenvalue density. arXiv preprint arXiv:1901.10159, 2019.
P. Auer, M. Herbster, and M. K. Warmuth. Exponentially many local minima for single neurons. In
Advances in neural information processing systems, pages 316-322, 1996.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural Networks, 2(1):53-58, 1989.
K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pages 586-594, 2016.
H. Lu and K. Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580,
2017.
A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning
in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
H.J6nsson, G. Mills, and K. W. Jacobsen. Nudged elastic band method for finding minimum energy
paths of transitions. Classical and Quantum Dynamics in Condensed Phase Simulations, pages
385-404, 1998.
J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient descent converges to minimizers.
arXiv preprint arXiv:1602.04915, 2016.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in Neural Information Processing Systems, pages 8571-8580, 2018.
S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. arXiv preprint arXiv:1810.02054, 2018a.
S. S. Du, J. D. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. arXiv preprint arXiv:1811.03804, 2018b.
A. Engel and C. Van den Broeck. Statistical mechanics of learning. Cambridge University Press,
2001.
M. Inoue, H. Park, and M. Okada. On-line learning theory of soft committee machines with correlated
hidden units - a steepest gradient descent and natural gradient descent. Journal of the Physical
Society of Japan, 72(4):805-810, 2003.
K. Fukumizu and S. Amari. Local minima and plateaus in hierarchical structures of multilayer
perceptrons. Neural Networks, 13(3):317 - 327, 2000.
10
Under review as a conference paper at ICLR 2020
A Supplementary Figures
A
B
Figure 4: A. The loss landscape (schematic) as a function of two parameters: the difference Wm(k,)i -
Wl(,ki ) between the weights from neuron i to neurons m and l in layer k and the weight Wn(k,m+1)
from neuron m to neuron n in the next layer (see B for network graph). The red curve indicates the
path from one of the global minima (red triangle) to a half-way configuration where the difference
between the input weight vectors of neurons m and l in layer k vanishes (Wm(k,)i = Wl(,ki), red
square). Along the axis Wm(k,)i - Wl(,ki) = 0, we can change the output weight Wn(k,m+1) at constant
loss (dashed horizontal line), as long as the sum Wn(k,m+1) + Wn(k,l+1) = c remains constant. The point
Wn(k,m+1) = Wn(k,l+1)
where the two output weights are identical (and assuming the same matching
condition for the other output weights) defines the permutation point θl(⇔k)m (red ?) where we can
swap the indices of neurons m and l in layer k at equal loss and continuously in parameter space. If
we then shift (dashed green line) all output weights of neuron m in layer k to zero (Wn(k,m+1) = 0 for
all n, green filled circle), we are free to change the weight Wm(k,)i at constant loss (green arrows) so as
to perform further permutations of neurons in layer k.
Layer:
11
Under review as a conference paper at ICLR 2020
Figure 5: Loss L (vertical axis) on the permutation path as a function of the distance d between the
two parameter vectors to be permuted (schematic). A. In the teacher network the two parameter
vectors have a distance d°. Along the path, the distance is reduced to zero. At the permutation point
(*), the loss reaches a maximum which corresponds to a saddle point of the total loss function. B.
On the path towards the permutation point (?) an intermediate saddle point (S) may occur. C. The
permutation point (?) could be a minimum along the path.
12
Under review as a conference paper at ICLR 2020
Figure 6: Visualizing how permutation points arise in between global minima for a (hypothetical)
network function with scalar inputs and one hidden layer with three neurons. Here we do not consider
biases for simplicity. Blue dots: 3! many equivalent global minima. A red ?: One permutation point
(of first order at layer k = 1) represented by its weights in the first layer, i.e. (2.5, 2.5, 0.4). All red ?:
One permutation set where the weight value 2.5 is duplicated. All green ?: The other permutation
set where the weight value 0.4 is duplicated. Note that overall, we have 6 permutation points that
give rise to the same network function as the weight configuration with two hidden neurons with
(2.5,0.4). Indeed, T(K = 1,nι = 3) = (3-1) 11 = 1 confirms Why the number permutation points
corresponding to this particular weight configuration with two hidden neurons is equal to the number
of global minima. Note that the Weight values are assigned randomly for visualization purposes.
13
Under review as a conference paper at ICLR 2020
A
B
Figure 7: A. Zooming in permutations points of one of the permutation sets (red ? in Fig. 6). B.
Visualizing how permutation points (at layer k = 1, see A) lie inside equal-loss lines in the weight
space of the layer k + 1 = 2. Only two out of three lines are shown for simplicity. We observe that
the number of such equal-loss lines (hyperplanes) is equal to the number of permutation points, i.e.
each permutation point lies inside one distinct line.
14
Under review as a conference paper at ICLR 2020
B Path finding algorithm
Algorithm 1 Finding Low-Loss Paths through Permutation Points
Require: parameters θ, indices k, l, m, Schedule of decreasing distances
1:	do J d(,km (θ)
2:	Re-parametrize θ→ θ = ({∂(j)}(i,j) = (m,k), e, d) s.t. "j)= Wj),婢=WJ) + de/Iek
3:	d J do, e J Wmk — W(k)
4:	for all d0 in SCHEDULE(d0) do
5:	d J d0
6:	γ(d0) J θ(d0) as found by gradient descent on L(θ \ {d})
7:	end for
8:	return γ
C Proofs
C.1 Half-way configurations are permutation points
Proof of Lemma 1. Since the parameter vectors and output vectors of the two neurons are identical at
t = 2, we can merge the two neurons into a single one (by appropriately rescaling the corresponding
output weights) so that the number of neurons nk in layer k is reduced by one. Since our path-finding
method minimizes the gradient, the configuration at t = 2 is a local minimum of the smaller network,
hence a permutation point.
Alternatively, we will reformalize the path-finding problem with Lagrange multipliers: minimize
L(θ) + λdl(,km) (θ) for increasing values of λ, starting with λ = 0 and increasing (potentially)
until λ → ∞. The goal here is to decrease the distance down to zero while keeping the loss
minimal. For every λ, we will obtain a minimizer θ . This sequence of θ configurations will
be an approximate discretization of the ideal path γ*. With the Lagrangian formulation, we can
easily show that the half-way configuration is a critical point: it is a minimizer of L(θ) + λdl(,km) (θ)
when d(km (θ) = 0 for a big enough λ. Therefore, VL(θ) + λVd(km(θ) = 0. Note that the
partial derivatives of dl,m with respect to the parameter vectors of neuron l and m in layer k are
ddl,mm = 2(W(k) — W(k)), ddl,mm = 2(W(k) — W(k)) and both are equal to zero at the half way
∂*	l l	m ∂ ds『	m	l
configuration since kWl(k) — W(mk) k2 = 0. All other partial derivatives with respect to other parameter
vectors are 0 since dl(,km) does not depend on them. Therefore Vdl(,km) (θ) = 0. Consequently,
VL(θ) = 0. We note that the Lagrangian formulation is an approximation on the ideal path γ*.
C.2 Properties of (first-order) permutation points
Proof of Proposition 1. (i) Let us assume we merged the neurons l and m in layer k and scaled the
output weight vectors of these neurons with 1. In that case, the output vector in layer k + 1 remains
the same. The derivative with respect to an output in the layer k + 1 will be the same, i.e. ∂Lk for
an
n ∈ [nk+1] will not change under this mapping. The derivatives with respect to the output weights
(say wik
∂an+1
for i ∈ [nk]) Oflayer k is Pn=11 ∂∂L⅛ ^wk
. These derivatives would be the same as the
corresponding derivatives before the mapping is performed. The derivatives with respect to the
parameter vectors of layer k is
∂L dam
remains the same up to a scaling in the output weight value.
dam ∂^m
All the other derivatives remain the same (up to a scalar constant) as the corresponding derivatives
before the mapping is performed. Therefore, a critical point in the landscape of the neural network
with (n1, . . . , nk - 1, . . . , nd) neurons per layer will map to another critical point in the landscape
of the neural network with (n1, . . . , nk, . . . , nd) neurons per layer under the function preserving
mapping described in the definition of a permutation point. This proposition is a straightforward
15
Under review as a conference paper at ICLR 2020
extension to the Theorem 1 in (Fukumizu and Amari, 2000) for multiple output and multiple-layer
neural networks.
(ii)	Once the parameter vectors of neurons m and l in layer k are identical, they implement the same
function. Any change of an output weight W (nk,m+1) preserves the network function, and criticality
(see Proposition 1-(i) above), as long as the output weight W (nk,l+1) is coadapted so as to keep the
sum W (k+1) + W (kl+1) constant. The sum-constraint W (k+1) + W (kl+1) = c for each n in layer
n,m	n,l	n,m	n,l
k + 1 defines an nk+1-dimensional hyperplane of critical points. In particular, at each point in the
hyperplane, we have nk+1 directions of the Hessian with zero Eigenvalues if the activation function
g is twice-differentiable.
(iii)	We make the following sequence of continuous transformations that are all possible at fixed loss.
First, we decrease the output weights of neuron m to zero while increasing those of l by the same
amount, keeping the sum of weights W (jkm+1) +W(jkl+1) constant for each j in layer k+1. Second, we
,,
change smoothly the input parameter vector of neuron m to match those of an arbitrary other neuron
i in the same layer k. Third, we increase the output weights of neuron m while decreasing those of
neuron i until all output weights of neuron i are zero, keeping the sum of weights W (jk,m+1) + W j(,ki+1)
constant for each j in layer k + 1. Fourth, we reduce the input parameter vector of neuron i to zero.
Fifth, we increase the input parameter vector of neuron i to match that of neuron l at the permutation
point. Finally, we equally share output weights between neurons i and l so that i has the same weights
as previously neuron m at the permutation point.
Effectively, this procedure enables us to exchange an arbitrary neuron i with neuron m, but the
procedure can be repeated for further permutations. The permutations constructed in the proof of
property (ii) start at permutation points where the parameter vectors of neurons l and m merge.
Therefore the loss associated with all the permutations constructed in the proof is L(θl(⇔k)m), the one
of this permutation point. However, we could also begin with two other weight vectors i and j and
construct the path leading to another θi(⇔k)j , that has, in general, a different loss.
C.3 Properties of higher-order permutation points
Proof of Proposition 2. (i) We take a minimum in the landscape of neural network with (n1, . . . , nk-
K, . . . , nd ) neurons per layer. Using Proposition 1-(i), we map to a critical point in the landscape of
neural network with (n1, . . . , nk - K + 1, . . . , nd) neurons per layer where one parameter vector in
layer k is duplicated and the corresponding output weights are rescaled with 1. Repeating the same
mapping starting at the latter critical point, we map to another critical point in the landscape of neural
network with (n1, . . . , nk - K + 2, . . . , nd) neurons per layer. Repeating this mapping for K times,
we end up a K-th order permutation point (up to changes in the output weights of the replicated
neurons as long as the network function is preserved) and this is a critical point in the landscape of
neural network with (n1, . . . , nk, . . . , nd) neurons per layer by induction.
(ii)	We will denote the parameter vectors of a neural network with (n1, . . . , l = nk - K, . . . , nd)
neurons per layer with {砂Ik), ^2k),..., E(k)} and that of f (nι,...,nk,..., nd) neurons with
{^1(k), ^2(k),..., Qnk)}. Let,s consider an unordered partition of n = si + s2 + ... + Sl with
sm ≥ 1 for m = 1, . . . , l. Without loss of generality, let’s assume that the first s1 parameter vectors
of layer k are the same, then the next s2 and so on. Equivalently, for all m = 1, . . . , l, we have
Q0j(k) = Q(mk) forj = sm-1 + 1, sm-1 + 2, . . . , sm-1 + sm where s0 = 0.
The only free variables are the outgoing weights of the replicated neurons- except for the constraint
that the summation of these weights is fixed. These constraints correspond to Pjs=ms-1+sm+1 W i0,(jk+1) =
W i(,km+1) for all neurons i at layer k+1. Overall, there are (Plm=1 sm)nk+1 = nknk+1 free variables
constrained by lnk+1 equations. One equation defines a nknk+1 - 1 dimensional hyperplane in the
nknk+1 space. Intersecting lnk+1 of these hyperplanes, we end up having a (nk - l)nk+1 = Knk+1
dimensional equal-loss hyperplane.
16
Under review as a conference paper at ICLR 2020
C.4 Counting higher-order permutation points
We simulate a smaller neural network with the big network of (n1, . . . , nk, . . . , nd) neurons across
the d layers. We assume that a minimum of the small neural network has nk - K distinct parameter
vectors at layer k and nj distinct parameter vectors at other layers j 6= k. A K-th order permutation
point (at layer k) of the big network implements all the (n1, . . . , nk - K, . . . , nd) parameter vectors
of the small network and only these. Since the big networks has nk neurons in layer k and the
small network only nk - K, the big network must reuse some of these parameter vectors of the
smaller network several times. Therefore we count the number of permutations of indices to calculate
T(K, nk). We will start with K = 1.
Proof of Proposition 3.
(1)	The case K = 1:
At a first-order permutation point in layer k, we have l = nk - 1 distinct parameter vectors
{/Ik), ^2k),..., £(k)} for a total of nk neurons. Therefore two of the nk neurons must have the
same parameter vector. More formally, there is only one way to partition nk into nk - 1 positive in-
tegers without respecting order and this unordered partition can be represented as nk = 2+1+ . . .+1.
The duplicated parameter vector could be the first one, {砂Ik)}, or the second one,... or the last one,
{^(k)}. Therefore there are (nk-1) choices. For each of these choices (say, we double the third
parameter vector), we have n^ permutations of indices of neurons in layer k. If we include the
permutations that are possible at all other layers, we have (nk-1) 2 Q；=： nj! first-order permutation
points (see Fig. 6). Since the cardinality of the permutation set induced by a global minimum is
|P(θ)∣ = Q；=1 nj!, we arrive at a ratio T(K = 1,n) = (nk=1)：.
(2)	The case K = 2:
There are two ways to have nk - 2 distinct vectors out of nk, corresponding to two unordered
partitions of nk : (i) nk = 3 + 1 + . . . + 1, and (ii) nk = 2 + 2 + 1 + . . . + 1.
(i)	nk = 3 + 1 + . . . + 1
For this case, We have 叫 permutations given by permuting the neuron indices of layer k instead of
the usual nk! permutations since we should eliminate the equivalent permutations corresponding to
the permutations among the replicated parameter vectors with a division by 3!. Therefore, this 2-nd
order permutation point induces a permutation set with cardinality | P (θ) | = ： Qd= 1 nj!.
Now we should consider other 2-nd order permutation points (at layer k) giving rise to the same
network function and corresponding to the same unordered partition, which we did not count in this
permutation set. If we had chosen another parameter vector to replicate three times, this 2-nd order
permutation point would induce another permutation set. Note that we can choose the parameter
vector to replicate out of n - 2 in (nk=2) ways and there are * Q；=： nj! many points in each
one of the permutation sets. Therefore, we end up having (nk-2) 1 Qd-I nj! many 2-nd order
permutation points (at layer k) corresponding to this unordered partition. (ii)
(ii) nk = 2 + 2 + 1 + . . . + 1
For this case, we have 辨 permutations given by permuting the neuron indices of layer k instead of
the usual nk! permutations since we should eliminate the equivalent permutations corresponding to the
permutations among the two pairs of duplicated parameter vectors with a division by 2!2. Therefore,
this 2-nd order permutation point induces a permutation set with cardinality |P(θ)∣ =奈 Q；=： nj!∙
Again, we should consider other 2-nd order permutation points (at layer k) giving rise to the same
network function and corresponding to the same unordered partition. Note that we can choose two
parameter vectors to duplicate out of n — 2 in (nk2^2) ways and there are + Qd-I nj! many points
in each one of the permutation sets. Therefore, we end up having (nk-2)+ Q；=： nj! many 2-nd
17
Under review as a conference paper at ICLR 2020
order permutation points (at layer k) corresponding to this unordered partition. Overall, we have
(nk-2)3! Qd-1 n" (nk-2)备 Q
d-1
j=1
nj! many 2-nd order permutation points at layer k.
(3)	The case K = 3:
There are three ways to have nk - 3 distinct vectors out of nk, corresponding to three unordered parti-
tions of nk : (i) nk = 4+ 1 + . . . + 1, (ii) nk = 3 + 2 + 1 + . . . + 1, and (iii) nk = 2 + 2 + 2 + 1 + . . . + 1.
(i)	nk = 4 + 1 + . . . + 1
For this case, We have 等 permutations given by permuting the neuron indices of layer k. Therefore,
this 3-rd order permutation point induces a permutation set with cardinality |P(θ)∣ =才 Q；-： %」.
As usual, We should consider other 3-rd order permutation points (at layer k) giving rise to the same
network function and corresponding to the same unordered partition. If we had chosen another
parameter vector to replicate four times, this 3-rd order permutation point would induce another
permutation set. Note that we can choose the parameter vector to replicate out of nk - 3 in nk1-3
ways and there are 才 Q；-： nj! many points in each one of the permutation sets. Therefore, we end
UP having (nkJ3)孑 Q；-： nj! many 3-rd order permutation points (at layer k) corresponding to this
unordered partition.
(ii)	nk = 3 + 2 + 1 + . . . + 1
For this case, we have 羯 permutations given by permuting the neuron indices of layer k. Therefore,
this 3-rd order permutation point induces a permutation set with cardinality |P(θ)∣ =忐 Qd-I n」.
As usual, we should consider other 3-rd order permutation points (at layer k) giving rise to the
same network function and corresponding to the same unordered partition. If we had chosen
two other parameter vector to replicate, say r∂'∖kk and Emk), this 3-rd order permutation point
would induce another permutation set. Note that we can choose two parameter vectors to
replicate out of nk - 3 in 2! nk2-3 ways. We have an extra 2! factor here since we have different
permutation sets if we replicate E0l(k) twice and E0m(k) three times, or vice versa. Yet, there are
3⅛ Qd-I nj! many points in each one of the permutation sets. Therefore, we end up having
(nk-3)31! Qd—1 nj! many 3rd-order permutation points (at layer k)
corresponding to this unordered partition.
(iii)	nk = 2 + 2 + 2 + 1 + . . . + 1
For this case, we have 斜 permutations given by permuting the neuron indices of layer k. Therefore,
this 3-rd order permutation point induces a permutation set with cardinality |P(θ)∣ = + Q；-1 nj!.
As always, we should consider the other 3-rd order permutation points (at layer k) giving rise to the
same network function and corresponding to the same unordered partition. Note that we can choose
three parameter vectors to duplicate out of n - 3 in (nk3^3) ways and there are 击 Qd-I nj! many
points in each one of the permutation sets. Therefore, we end up having (nk3^3)击 Qd-I nj! many
3-rd order permutation points (at layer k) corresponding to this unordered partition. Overall, we
have (nk-3) : Qd-1 n7∙! + (nk-3) * Q；-1 n7∙! + (nk3^3)击 Q；-1 nj! many 3-rd order permutation
points at layer k.
(4)	A note on the general closed form formula for T (K, nk):
For a general integer K there is no closed-form formula for the number of partitions, although it is
easy to have a closed-form formula for the number of permutation points for one given unordered
partition following the counting arguments. Thus, we believe that there is no way to find a closed-form
formula for T(K, nk) as a function of K. However, the lower bound for T(K, nk) is also the dom-
18
Under review as a conference paper at ICLR 2020
inating term for large nk, since every other summand would be a polynomial of at most (K1)-th order.
(5)	A lower bound for general K :
For general K , we have l = nk - K distinct parameter vectors in the small network. There are many
ways to partition nk into l positive integers without respecting order. Since we are interested in a
lower bound, we only consider the following unordered partition: nk = 2 + . . . + 2 + 1 + . . . + 1,
i.e. we have K duplicated parameter vectors and nk - 2K parameter vectors that appear once. For
this unordered partition, we have nkK-K ways to choose the duplicated parameter vectors. For
each one of these choices, We can permute the neuron indices in 强 different ways. Including the
permutations in other layers j = k, we end UP with (nk-κ) 2K Qd-I n/points in the permutation
set. The number is a lower bound of T(K, nk), because other unordered partitions of nk give rise to
other Kth-order permutation points at layer k.
C.5 Proof of Lemma 2
We can approximate the factorial an integer n using Stirling’s formula
n
n! → λ∕2πn
as n → ∞
(4)
We note that this approximation leads to accurate results even for small n.
As n → ∞, both n - K → ∞ and n - 2K → ∞ for finite K. Therefore, we can apply Stirling’s
formula both for n - K and n - 2K :
1 (n - K∖ __ 1	(n - K)!
2!κ V K J = 2!κ (n - 2K)!K!
(5)
1	(n - K)!
2!k (n - 2K)!K!
1	√2π(n - K)
n-K
→ 2!κ------------7	、n-2^- as n →∞
2!	√2π(n - 2K) (I)	K!
(6)
1 In - K (n - K)n-κ	1	.	、
2K V n - 2K (n - 2K)iκ eκK! as n →∞
(7)
1 IK - 1 (K∙ - 1)n-κ
2!k∖∣ Kκ - 2 (κκ - 2)n-2κ
11
Kk -....›	ɪ( — )k
eκK!	2!k (K)
KK
eKK!as n →∞
(8)
2!K eK K!
nK = cK nK as n → ∞
(9)
C.6 Proof of Lemma 3
We already know the number of equilavent K-th order permutation points (the ones that reduce to the
same configuration in the landscape of the neural network with (n1, . . . , nk - K, . . . , nd)) is at least
Pkd-=11 T(K, nk) Qkd-=11 nk! (Proposition 3-(ii)). We can easily that see that every permutation point
gives rise to a distinct high-dimensional subspace of critical points by observing that their parameter
vectors in layer k would be in distinct positions when projected on the parameters of the layer k
since we never repeat the same set of parameters in layer k. If two such subspaces were the same,
their projection on a lower dimensional space (parameters of the layer k) would be same necessarily.
Therefore, the subspaces mentioned are distinct and the number of them is equivalent to the number
of related permutation po
19