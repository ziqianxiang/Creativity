Under review as a conference paper at ICLR 2020
A Theoretical Analysis of Deep Q-Learning
Anonymous authors
Paper under double-blind review
Ab stract
Despite the great empirical success of deep reinforcement learning, its theoretical
foundation is less well understood. In this work, we make the first attempt to
theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015)
from both algorithmic and statistical perspectives. In specific, we focus on the
fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight
simplification of DQN that captures the tricks of experience replay and target
network used in DQN. Under mild assumptions, we establish the algorithmic
and statistical rates of convergence for the action-value functions of the iterative
policy sequence obtained by FQI. In particular, the statistical error characterizes
the bias and variance that arise from approximating the action-value function using
deep neural network, while the algorithmic error converges to zero at a geometric
rate. As a byproduct, our analysis provides justifications for the techniques of
experience replay and target network, which are crucial to the empirical success of
DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN
algorithm for zero-sum Markov game with two players, which is deferred to the
appendix due to space limitations.
1	Introduction
Reinforcement learning (RL) attacks the multi-stage decision-making problems by interacting with
the environment and learning from the experiences. With the breakthrough in deep learning, deep
reinforcement learning (DRL) demonstrates tremendous success in solving highly challenging
problems, such as the game of Go (Silver et al., 2016; 2017), robotics (Kober & Peters, 2012),
and dialogue systems (Chen et al., 2017). In DRL, the value or policy functions are often represented
as deep neural networks and the related deep learning techniques can be readily applied. For example,
deep Q-network (DQN) (Mnih et al., 2015), asynchronous advantage actor-critic (A3C) and (Mnih
et al., 2016) demonstrate superhuman performance in various applications and become standard
algorithms for artificial intelligence.
Despite its great empirical success, there exists a gap between the theory and practice of DRL. In
particular, most existing theoretical work on reinforcement learning focuses on the tabular case
where the state and action spaces are finite, or the case where the value function is linear. Under
these restrictive settings, the algorithmic and statistical perspectives of reinforcement learning are
well-understood via the tools developed for convex optimization and linear regression. However, in
presence of nonlinear function approximators such as deep neural network, the theoretical analysis
of reinforcement learning becomes intractable as it involves solving a highly nonconvex statistical
optimization problem.
To bridge such a gap in DRL, we make the first attempt to theoretically understand DQN, which can
be cast as an extension of the classical Q-learning algorithm (Watkins & Dayan, 1992) that uses deep
neural network to approximate the action-value function. Although the algorithmic and statistical
properties of the classical Q-learning algorithm are well-studied, theoretical analysis of DQN is
highly challenging due to its differences in the following two aspects.
1
Under review as a conference paper at ICLR 2020
First, in online gradient-based temporal-difference reinforcement learning algorithms, approximating
the action-value function often leads to instability. Baird (1995) proves that this is the case even with
linear function approximation. The key technique to achieve stability in DQN is experience replay
(Lin, 1992; Mnih et al., 2015). In specific, a replay memory is used to store the trajectory of the
Markov decision process (MDP). At each iteration of DQN, a mini-batch of states, actions, rewards,
and next states are sampled from the replay memory as observations to train the Q-network, which
approximates the action-value function. The intuition behind experience replay is to achieve stability
by breaking the temporal dependency among the observations used in the training of the deep neural
network.
Second, in addition to the aforementioned Q-network, DQN uses another neural network named
target network to obtain an unbiased estimator of the mean-squared Bellman error used in training
the Q-network. The target network is synchronized with the Q-network after each period of iterations,
which leads to a coupling between the two networks. Moreover, even if we fix the target network
and focus on updating the Q-network, the subproblem of training a neural network still remains less
well-understood in theory.
In this paper, we focus on a slight simplification of DQN, which is amenable to theoretical analysis
while fully capturing the above two aspects. In specific, we simplify the technique of experience
replay with an independence assumption, and focus on deep neural networks with rectified linear
units (ReLU) (Nair & Hinton, 2010) and large batch size. Under this setting, DQN is reduced to the
neural fitted Q-iteration (FQI) algorithm (Riedmiller, 2005) and the technique of target network can
be cast as the value iteration. More importantly, by adapting the approximation results for ReLU
networks to the analysis of Bellman operator, we establish the algorithmic and statistical rates of
convergence for the iterative policy sequence obtained by DQN. As shown in the main results in §3,
the statistical error characterizes the bias and variance that arise from approximating the action-value
function using neural network, while the algorithmic error geometrically decays to zero as the number
of iteration goes to infinity.
Our contribution is two-fold. First, we establish the algorithmic and statistical errors of the neural
FQI algorithm, which can be viewed as a slight simplification of DQN. Under mild assumptions,
our results show that the proposed algorithm obtains a sequence of Q-networks that geometrically
converges to the optimal action-value function up to an intrinsic statistical error induced by the
approximation bias of ReLU network and finite sample size. Second, as a byproduct, our analysis
justifies the techniques of experience replay and target network used in DQN, where the latter can be
viewed as the value iteration. In addition, we also extend our algorithm to zero-sum Markov games.
Due to space limit, we defer these results to the appendix.
Notation. For a measurable space with domain S, we denote by B(S, V ) the set of measurable
functions on S that are bounded by V in absolute value. Let P(S) be the set of all probability
measures over S. For any ν ∈ P(S) and any measurable function f : S → R, we denote by kf kp,ν
the `p-norm of f with respect to measure ν for p ≥ 1. In addition, for simplicity, we write kfkν
for kf k2,ν. In addition, let {f (n), g(n)}n≥1 be two positive series. We write f(n) . g(n) if there
exists a constant C such that f (n) ≤ C ∙ g(n) for all n larger than some no ∈ N. In addition, We
write f(n) g(n) iff(n) . g(n) and g(n) . f (n).
2	Background
In this section, We introduce the background. We first lay out the formulation of the reinforcement
learning problem, and then define the family of ReLU neural netWorks.
2.1	Reinforcement Learning
A discounted Markov decision process is defined by a tuple (S, A, P, R, γ). Here S is the set of
all states, Which can be countable or uncountable, A is the set of all actions, P : S × A → P(S)
2
Under review as a conference paper at ICLR 2020
is the Markov transition kernel, R : S × A → P(R) is the distribution of the immediate reward,
and γ ∈ (0, 1) is the discount factor. In specific, upon taking any action a ∈ A at any state s ∈ S,
P(∙ |s, a) defines the probability distribution of the next state and R(∙ |s, a) is the distribution of the
immediate reward. Moreover, for regularity, we further assume that S is a compact subset of Rd
which can be infinite, A = {a1, a2, . . . , aM} has finite cardinality M, and the rewards are uniformly
bounded by Rmax, i.e., R(∙ |s, a) is supported on [-Rmaχ, Rmax] for any S ∈ S and a ∈ A.
A policy π: S → P(A) for the MDP maps any state S ∈ S to a probability distribution π(∙ | S)
over A. For policy π , the corresponding value function V π : S → R is defined as the cumulative
discounted reward obtained by when the actions are executed according to π , that is,
∞
Vπ (S)= E X Yt∙ Rt So = s,At 〜∏(∙∣ St),St+ι 〜P (∙∣ St ,At) .	(2.1)
t=0
Similarly, the action-value function Qπ : S × A → R is defined as
∞
Qπ(s,a)= E X γt∙ Rt S0 = s,A0 = a,At 〜∏(∙∣St),St+ι 〜P (∙∣ St,At) .	(2.2)
t=0
For any given action-value function Q : S × A → R, define the one-step greedy policy πQ as any
policy that selects the action with the largest Q-value, that is, for any S ∈ S, ∏q(∙ |s) satisfies
πQ(a | S) = 0 if Q(S, a) 6= max Q(S, a0).	(2.3)
a0∈A
Moreover, we define operator Pπ by
(PπQ)(s, a) = E[Q(S0, A0) I S0 〜P(∙∣ S,a),A0 〜∏(∙ | 9)],	(2.4)
and define the Bellman operator Tπ by (TπQ)(s, a) = r(S,a) + Y ∙ (PπQ)(s, a), where r(S,a)=
rR(dr | S, a) is the expected reward obtained at state S when taking action a. Then it can be verified
that Qπ is the unique fixed point of Tπ .
The goal of reinforcement learning is to find the optimal policy, which achieves the largest cumulative
reward. To characterize optimality, We define optimal action-value function Q* as
Q*(S,a) = sup Qπ(S,a),	(2.5)
π
where the supremum is taken over all policies. Based on Q*, we define the optimal policy π* as any
policy that is greedy with respect to Q*. It can be shown that Q* = Qπ*. Finally, we define the
Bellman optimality operator T via
(TQ)(S,a) = r(S,a) + Y ∙ E[max Q(S0, a0)1 S0 〜P(∙ | s, a)].	(2.6)
Then we have Bellman optimality equation TQ* = Q*.
2.2	Deep Neural Network
We study the performance of DQN with rectified linear unit (ReLU) activation function σ(u) =
max(u, 0). For any positive integer L and {dj}jL=+01 ⊆ N, a ReLU network f : Rd0 → RdL+1 with L
hidden layers and width {dj}jL=+01 is of form
f(x) = WL+1σ(WLσ(WL-1 . . . σ(W2σ(W1x + v1) + v2) . . .vL-1) + vL),	(2.7)
where w` ∈ Rd'×d'-1 and v` ∈ Rd' are the weight matrix and the shift vector in the '-th layer,
respectively. Here we apply σ to to each entry of its argument in (2.7). In deep learning, the network
structure is fixed, and the goal is to learn the network parameters (weights) {W', V'}'∈[l+i] with the
convention that vL+1 = 0. For deep neural networks, the number of parameters greatly exceeds the
input dimension d0 . To restrict the model class, we focus on the class of ReLU networks where most
parameters are zero.
3
Under review as a conference paper at ICLR 2020
Definition 2.1 (Sparse ReLU Network). For any L, s ∈ N, {dj}jL=+01 ⊆ N , and V > 0, the family of
sparse ReLU networks bounded by V with L hidden layers, network width d, and weight sparsity s
is defined as
L+1
F (LM }L+*,V)={f: *i] kf'k∞ ≤ 1, X fl ≤ s, Mx. kfjk∞ ≤ V}, (S
where we denote (w`, v`) by %. Moreover, f in (2.8) is expressed as in (2.7), and fj is the j-th
component of f .
Here we focus on functions that are uniformly bounded because the value functions in (2.1) and
(2.2) are always bounded by Vmax = Rmax/(1 - γ). In the sequel, we write F(L, {dj}jL=+01, s, Vmax)
as F(L, {dj}jL=+01, s) to simplify the notation. In addition, we restrict the networks weights to be
sparse, i.e., s is much smaller compared with the total number of parameters. Such an assumption
implies that the network has sparse connections, which are useful for applying deep learning in
memory-constrained situations such as mobile devices (Han et al., 2015; Liu et al., 2015).
Moreover, We introduce the notion of Holder smoothness as follows, which is a generalization of
Lipschitz continuity, and is widely used to characterize the regularity of functions.
Definition 2.2 (Holder Smooth Function). Let D be a compact subset of Rr, where r ∈ N. We
define the set of Holder smooth functions on D as
Cr (D,β,H )= f: D→ R: X	k∂ αfk∞ +	X sup	Idaf(X) -3*» ≤ h},
a	a: ∣a∣<β	a: kakι = bβC x,y∈D,x=y	Ilx - yk∞	J
where β > 0 and H > 0 are parameters and bβc is the largest integer no greater than β . In addition,
here we use the multi-index notation by letting α = (α1, . . . , αr)> ∈ Nr, and ∂a = ∂α1 . . . ∂αr.
Finally, we conclude this section by defining functions that can be written as a composition of multiple
Holder functions, which captures complex mappings in real-world applications such as multi-level
feature extraction.
Definition 2.3 (Composition of Holder Functions). Let q ∈ N and {pj }j∈[q] ⊆ N be integers, and
let {aj, bj}j∈[q] ⊆ R such that aj < bj j ∈ [q]. Moreover, let gj : [aj, bj]pj → [aj+1, bj+1]pj+1 be a
function, ∀j ∈ [q]. Let (gjk)左曰?,十1 ] be the components of gj, and we assume that each gjk is Holder
smooth, and depends on at most tj of its input variables, where tj could be much smaller than pj , i.e.,
gjk ∈ Ctj ([aj, bj]tj , βj, Hj). Finally, we denote by G({pj, tj, βj, Hj}j∈[q]) the family of functions
that can be written as compositions of {gj}j∈[q], with the convention that pq+1 = 1. That is, for any
f ∈ G({pj, tj, βj, Hj}j∈[q]), we can write
f = gq ◦ gq-1 ◦ . . . ◦ g2 ◦ g1,
with gjk ∈ Ctj ([aj, bj]tj,βj, Hj) for each k ∈ [pj+1] andj ∈ [q].
3 Understanding Deep Q-Network
In the DQN algorithm, a deep neural network Qθ : S ×A→ R is used to approximate Q*, where
θ is the parameter. For completeness, we state the DQN as Algorithm 2 in §A. As shown in the
experiments in Mnih et al. (2015), two tricks are pivotal for the success of DQN.
First, DQN use the trick of experience replay (Lin, 1992). Specifically, at each time t, we store the
transition (St, At, Rt, St+1) into the replay memory M, and then sample a minibatch of independent
samples from M to train the neural network via stochastic gradient descent. Since the trajectory of
MDP has strong temporal correlation, the goal of experience replay is to obtain uncorrelated samples,
which yields accurate gradient estimation for the stochastic optimization problem.
Another trick is to use a target network Qθ? with parameter θ? . Specifically, with independent
samples {(si, ai, ri, s0i)}i∈[n] from the replay memory, to update the parameter θ of the Q-network,
4
Under review as a conference paper at ICLR 2020
We compute the target Yi = r + Y ∙ maXa∈A Qθ? (si, a), and update θ by the gradient of
1n
L(θ) = - y^[Yi — Qθ(si, ai)]2.	(3.1)
n i=1
Whereas parameter θ? is updated once every Ttarget steps by letting θ? = θ. That is, the target netWork
is hold fixed for Ttarget steps, and is thus updated in a sloWer pace.
To demystify DQN, it is crucial to understand the role played by these tWo tricks. For experience
replay, in practice, the replay memory size is usually very large. For example, the replay memory size
is 106 in Mnih et al. (2015). Moreover, DQN use the -greedy policy, Which enables exploration over
S × A. Thus, When the replay memory is large, experience replay is close to sampling independent
transitions from an explorative policy. This reduces the variance of the VL(θ), which is used to
update θ.
Thus, experience replay stabilizes the training of DQN, which benefits the algorithm in terms of
computation. To understand the statistical property of DQN, we replace the experience replay by
sampling independent transitions from a fixed distribution σ ∈ P(S × A). That is, instead of sampling
from the replay memory, we sample i.i.d. observations {(Si, Ai)}i∈[n] from σ. Moreover, for any
i ∈ [n], let Ri and Si0 be the immediate reward and the next state when taking action Ai at state
Si. Under this setting, we have E(Yi | Si, Ai) = (TQθ? )(Si, Ai), where Qθ? is the target network,
which, as we show as follows, is motivated from a statistical consideration.
Let us first neglect the target network and set θ? = θ. Using bias-variance decomposition, the the
expected value of L(θ) in (3.1) is
EL(θ) = kQθ-TQθk2σ+E Y1 -(TQθ)(S1,A1)2 .
(3.2)
Here the first term in (3.2) is known as the mean-squared Bellman error (MSBE), and the second
term is the variance of Y1. Whereas L(θ) can be viewed as the empirical version of the MSBE,
which has bias E{[Y1 - (TQθ)(S1, A1)]2} that also depends on θ. Thus, without the target network,
minimizing L(θ) can be drastically different from minimizing the MSBE.
To resolve this problem, we use a target network in (3.1), which has expectation
E[L(θ)] = kQθ- TQθ* kσ + E{[Yι-(TQθ*)(Si,Ai)]2},
where the variance of Y1 does not depend on θ. Thus, minimizing L(θ) is close to solving
minimize ∣∣Qθ — TQθ? ||彳,	(3.3)
θ∈Θ
where Θ is the parameter space. Note that in DQN we hold θ? still and update θ for Ttarget steps.
When Ttarget is sufficiently large and we neglect the fact that the objective in (3.3) is nonconvex, we
would update θ by the minimizer of (3.3) for fixed θ? .
Therefore, in the ideal case, DQN aims to solve the minimization problem (3.3) with θ? fixed, and
then update θ? by the minimizer θ . Interestingly, this view of DQN offers a statistical interpretation
of the target network. In specific, if {Qθ : θ ∈ Θ} is sufficiently large such that it contains TQθ?,
then (3.3) has solution Qθ = TQθ?, which can be viewed as one-step of value iteration (Sutton
& Barto, 2011) for neural networks. In addition, in the sample setting, Qθ? is used to construct
{Yi}i∈[n], which serve as the response in the regression problem defined in (3.1), with (TQθ?) being
the regression function.
Furthermore, turning the discussion above into a realizable algorithm, we obtain the neural fitted
Q-iteration (FQI) algorithm, which generates a sequence of value functions. Specifically, let F be a
class of function defined on S ×A. In the k-th iteration OfFQLlet Qk be current estimate of Q*.
_________________________________________________________- ___________________-
〜
〜
Similar to (3.1) and (3.3), we define 匕 = Ri + Y ∙ maXa∈A Qk(Si, a), and update Qk by
1n
Qk+1 = argmin - 丁[匕 - f (Si, Ai)]2.
f∈F n i=1
(3.4)
5
Under review as a conference paper at ICLR 2020
This gives the fitted-Q iteration algorithm, which is stated in Algorithm 1.
When F is the family of neural networks, Algorithm 1 is known as the neural FQI, which is proposed
in Riedmiller (2005). Thus, we can view neural FQI as an modification of DQN, where we replace
experience replay by sampling from a fixed distribution σ, so as to understand its the statistical
property. As a byproduct, such a modification naturally justifies the trick of target network in DQN.
In addition, note that the optimization problem in (3.4) appears in each iteration of FQI, which is
nonconvex when neural networks are used. However, since we focus solely on the statistical aspect,
we make the assumption that the global optima of (3.4) can be reached, which is also contained F.
Interestingly, a recent line of research on deep learning (Du et al., 2018b;a; Zou et al., 2018; Chizat &
Bach, 2018; Allen-Zhu et al., 2018a;b; Jacot et al., 2018; Cao & Gu, 2019; Arora et al., 2019; Ma
et al., 2019; Mei et al., 2019; Yehudai & Shamir, 2019) has established global convergence of gradient-
based algorithms for empirical risk minimization when the neural networks are overparametrized. We
provide more discussions on the computation aspect in §B. Moreover, we make the i.i.d. assumption
in Algorithm 1 to simplify the analysis. Antos et al. (2008b) study the performance of fitted value
iteration with fixed data used in the regression sub-problems repeatedly, where the data is sampled
from a single trajectory based on a fixed policy such that the induced Markov chain satisfies certain
conditions on the mixing time. Using similar analysis as in Antos et al. (2008b), our algorithm can
also be extended to handled fixed data that is collected beforehand.
Algorithm 1 Fitted Q-Iteration Algorithm
Input: MDP (S, A, P, R, γ), function class F, sampling distribution σ, number of iterations K,
number of samples n, the initial estimator Q0 .
for k = 0, 1, 2, . . . , K - 1 do
Sample i.i.d. observations {(Si, Ai), i ∈ [n]} from σ, obtain Ri 〜 R(∙ ∣Si,Ai) and Si 〜
P (∙∣Si,Ai).
Compute Yi = Ri + Y ∙ maXa∈A Qk(Si, a).
Update the action-value function:
1n	2
Qk+1 - argmin	[匕一f (Si,Ai)].
f∈F n i=1
end for
Define policy πK as the greedy policy with respect to QK .
Output: An estimator QK of Q* and policy ∏k.
4	Theoretical Results
We establish statistical guarantees for DQN with ReLU networks. Specifically, let QπK be the
action-value function corresponding to πK, which is returned by Algorithm 1. In the following, we
obtain an upper bound for ∣∣QπK - Q* k ι,μ, where μ ∈ P (S × A) is allowed to be different from V.
In addition, we assume that the state space S is a compact subset in Rr and the action space A is
finite. Without loss of generality, we let S = [0, 1]r hereafter, where r is a fixed integer. To begin
with, we first specify the function class F in Algorithm 1.
Definition 4.1 (Function Classes). Following Definition 2.1, let F(L, {dj}jL=+01, s) be the family of
sparse ReLU networks defined on S with d0 = r and dL+1 = 1. Then we define F0 by
Fo = {f: S ×A→ R: f(∙,a) ∈ F(L,{dj}L+1,s) for any a ∈ A}.	(4.1)
In addition, let G({pj ,~tj ,βj ,Hj }j∈[q]) be set of composition of Holder smooth functions defined on
S ⊆ Rr. Similar to F0, we define a function class G0 as
Go = {f ： S×A→ R ： f(∙,a) ∈ G({pj ,tj ,βj ,Hj }j∈[q]) for any a ∈ A}.	(4.2)
6
Under review as a conference paper at ICLR 2020
By this definition, for any function f ∈ Fo and any action a ∈ A, f (∙,a) is a ReLU network defined
on S , which is standard for Q-networks. Moreover, G0 contains a broad family of smooth functions
on S × A. In the following, we make a mild assumption on F0 and G0 .
Assumption 4.2. We assume that for any f ∈ F0, we have T f ∈ G0, where T is the Bellman
optimality operator defined in (2.6). That is, for any f ∈ F and any a ∈ A, (T f)(s, a) can be written
as compositions of Holder smooth functions as a function of S ∈ S.
We remark that this assumption holds when the MDP satisfies some smoothness conditions. For any
state-action pair (s, a) ∈ S ×A, let P(∙ | s, a) be the density of the next state. By the definition of
the Bellman optimality operator in (2.6), we have
(Tf)(s,a) =r(s,a)+γ
Sam0∈aAxf(s0
a0)] ∙ P(s0 | s, a)ds0.
(4.3)
For any s0 ∈ S and a ∈ A, we define functions g1, g2 by letting g1 (s) = r(s, a) and g2(s) =
P(s01 s, a). Suppose both gι and g2 are Holder smooth functions on S = [0,1]r with parameters β
and H . Since kf k∞ ≤ Vmax , by changing the order of integration and differentiation with respect
to s in (4.3), we obtain that function S → (Tf )(s, a) belongs to the Holder class Cr (S, β, H0) with
H0 = H(1 + Vmax). Furthermore, in the more general case, suppose for any fixed a ∈ A, we can
write P(s0 | s, a) as h1 [h2(s, a), h3(s0)], where h2 : S → Rr1, and h3 : S → Rr2 can be viewed as
feature mappings, and h1 : Rr1+r2 → R is a bivariate function. We define function h4 : Rr1 → R by
h4(u)
Sam0∈aAxf(s0
a0) h1 [u, h3(s0)]ds0.
Then by (4.3) we have (T f)(s, a) = g1(s) + h4 ◦ h2(s, a). Then Assumption 4.2 holds if h4 is
Holder smooth and both gι and h2 can be represented as compositions of Holder functions. Thus,
Assumption 4.2 holds if both the reward function and the transition density of the MDP are sufficiently
smooth.
Moreover, even when the transition density is not smooth, we could also expect Assumption 4.2 to
hold. Consider the extreme case where the MDP has deterministic transitions, that is, the next state s0
is a function of s and a, which is denoted by s0 = h(s, a). In this case, for any ReLU network f, we
have (Tf )(s, a) = r(s, a) + Y ∙ max。，*/ f [h(s, a), a0]. Since
max f (s1, a0) - maxf(s2, a0) ≤ maxf (s1, a0) - f(s2,a0)
a0∈A	a0∈A	a0∈A
for any s1,s2 ∈ S, and network f (∙,a) is Lipschitz continuous for any fixed a ∈ A, function
m1(s) = maxa0 f(s, a0) is Lipschitz on S. Thus, for any fixed a ∈ A, if both g1(s) = r(s, a) and
m2(s) = h(s, a) are compositions of Holder functions, so is (Tf )(s, a) = gι(s) + mi ◦ m2(s).
Therefore, even if the MDP has deterministic dynamics, if both the reward function r(s, a) and the
transition function h(s, a) are sufficiently nice, Assumption 4.2 still holds true.
In the following, we define the concentration coefficients, which measures the similarity between two
probability distributions under the MDP.
Assumption 4.3 (Concentration Coefficients). Let ν1 , ν2 ∈ P(S × A) be two probability measures
that are absolutely continuous with respect to the Lebesgue measure on S × A. Let {πt }t≥1 be a
sequence of policies. Suppose the initial state-action pair (S0, A0) of the MDP has distribution ν1,
and we take action At according to policy ∏t. For any integer m, we denote by Pπm Pπm-1 ∙∙∙ Pπ1 νι
the distribution of (Sm, Am). Then we define the m-th concentration coefficient as
κ(m; ν1, ν2) = sup	Eν2
π1 ,...,πm
d(Pπm Pπm-1
dν2
…P π1 νι)
(4.4)
where the supremum is taken over all possible policies.
Furthermore, let σ be the sampling distribution in Algorithm 1 and let μ be a fixed distribution on
S ×A. We assume that there exists a constant φμ,σ < ∞ such that
(1 - γ)2 ∙ X Ym-i ∙ m ∙ κ(m; μ, σ) ≤ φμ,σ,	(4.5)
m≥1
where (1 - γ)2 in (4.5) is a normalization term, since Pm≥i Ym-i ∙ m =(I- Y)-2.
7
Under review as a conference paper at ICLR 2020
By definition, concentration coefficients in (4.4) quantifies the similarity between ν2 and the dis-
tribution of the future states of the MDP when starting from ν1. Moreover, (4.5) is a standard
assumption in the literature. See, e.g., Munos & Szepesvari (2008); Lazaric et al. (2016); Scherrer
et al. (2015); Farahmand et al. (2010; 2016). This assumption holds for large class of systems MDPs
and specifically for MDPs whose top-Lyapunov exponent is finite. See Munos & SZePeSVari (2008);
Antos et al. (2007) for more detailed discussions on this assumption.
Now we are ready to present the main theorem.
Theorem 4.4. Under Assumptions 4.2 and 4.3, let F0 be defined in (4.1) based on the family of sparse
ReLU networks F(L*, {dj}L=+1, s*) and let G0 be given in (4.2). Moreover, for any j ∈ [q 一 1], we
define βj = βj ∙ Q'=j+ι min(β', 1); let β* = 1. In addition, let a* = maxj∈[q] t7-/(2/；+17-). For
the parameters of G0, we assume that there exists a constant ξ > 0 such that
max tj ≤ ξ,	log tj . (log n)ξ and max pj . (log n)ξ .
j∈[q]	j∈[q]
(4.6)
j∈[q]
For the hyperparameters L*, {dj}L=+1, and s* of the ReLU network, we set d0 = 0 and dL*+ι = 1.
Moreover, we set L* . (log n)ξ0,
max{pj+ι ∙ tj} ∙ nα . mindj ≤ max^ dj . nξ , and	s* N nα ∙ (logn)ξ	(4.7)
for some constant ξ0 > 0. For any K ∈ N, let QπK be the action-value function corresponding to
policy πK, which is returned by Algorithm 1 based on function class F0. Then there exists constants
ξj and C such that
kQ* - QnK kι,μ ≤ C ∙ (φμ-σ YYY ∙ |A| ∙ (logn)ξ* ∙ n(α*-1)/2 + (4γKY)2 ∙ Rmax.	(4.8)
This theorem implies that the statistical rate of convergence is the sum of a statistical error and an
algorithmic error. The algorithmic error converges to zero in linear rate as the algorithm proceeds,
whereas the statistical error reflects the fundamental difficulty of the problem. Thus, when the number
of iterations satisfy
K ≥ C0 ∙ [log |A| + (1 一 αj) ∙ logn]/ log(1∕γ)
iterations, where C0 is a sufficiently large constant, the algorithmic error is dominated by the statistical
error. In this case, if we view both Y and φμ,σ as constants and ignore the polylogarithmic term,
Algorithm 1 achieves error rate
|A| ∙ n(α*-1"2 = |A| ∙ maxn-βj/(2βj+tj),
j∈[q]
(4.9)
which scales linearly with the capacity of the action space, and decays to zero when the n goes
to infinity. Furthermore, the rates {n-βj/(2βj+tj)}j∈[q] in (4.9) recovers the statistical rate of
nonparametric regression in '2-norm, whereas our statistical rate n(α*-1)/2 in (4.9) is the fastest
among these nonparametric rates, which illustrates the benefit of compositional structure of G0 .
Furthermore, as a concrete example, we assume that both the reward function and the Markov
transition kernel are Holder smooth with smoothness parameter β. As stated below Assumption 4.2,
for any f ∈ F0, we have (Tf)(∙, a) ∈ Cr(S, β, H0). Then Theorem 4.4 implies that Algorithm 1
achieves error rate |A| ∙ n-e/(2e+r) when K is sufficiently large. Since |A| is finite, this rate achieves
the minimax-optimal statistical rate of convergence within the class of Holder smooth functions
defined on [0, 1]d (Stone, 1982) and thus cannot be further improved.
In addition, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum
Markov game with two players. Specifically, in this setting, there are two players with action spaces
A and B. The action-value function Qj(s, a, b) : S × A × B → R can be similarly defined, which
correspond to the value obtained by a pair of policies that constitute the Nash equilibrium. Minimax-
DQN differs from the original DQN mainly in the computation of target, which is obtained by solving
8
Under review as a conference paper at ICLR 2020
a zero-sum matrix game via linear programming. Using similar proof technique, we establish both
the algorithmic and statistical convergence rates of the action-value functions associated with the
sequence of policies returned by the Minimax-DQN algorithm. Due to space limit, we defer the
algorithm and its theory to §E.1 in the appendix.
9
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. Cambridge
University Press, 2009.
Andras Antos, Csaba Szepesvari, and Remi Munos. Value-iteration based fitted policy iteration:
learning with a single trajectory. In IEEE International Symposium on Approximate Dynamic
Programming and Reinforcement Learning, pp. 330-337. IEEE, 2007.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Fitted Q-iteration in continuous action-space
mdps. In Advances in neural information processing systems, pp. 9-16, 2008a.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with Bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71
(1):89-129, 2008b.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief
survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866, 2017.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995, pp. 30-37. Elsevier, 1995.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE transactions on Information Theory,
44(2):525-536, 1998.
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC dimension bounds for piecewise
polynomial networks. In Advances in Neural Information Processing Systems, pp. 190-196, 1999.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017a.
Peter L Bartlett, Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension and
pseudodimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930,
2017b.
Benedikt Bauer, Michael Kohler, et al. On deep learning as a remedy for the curse of dimensionality
in nonparametric regression. The Annals of Statistics, 47(4):2261-2285, 2019.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Michael Bowling. Rational and convergent learning in stochastic games. In International Conference
on Artificial Intelligence, 2001.
Justin A Boyan. Technical update: Least-squares temporal difference learning. Machine Learning,
49(2-3):233-246, 2002.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference
learning. Machine learning, 22(1-3):33-57, 1996.
10
Under review as a conference paper at ICLR 2020
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. A survey on dialogue systems: Recent
advances and new frontiers. arXiv preprint arXiv:1711.01731, 2017.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
Vincent Conitzer and Tuomas Sandholm. AWESOME: A general multiagent learning algorithm that
converges in self-play and learns a best response against stationary opponents. Machine Learning,
67(1-2):23-43, 2007.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6(Apr):503-556, 2005.
Jianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning. arXiv preprint
arXiv:1904.05526, 2019.
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesvari, and Shie Mannor. Regular-
ized fitted Q-iteration for planning in continuous-space Markovian decision problems. In American
Control Conference, pp. 725-730. IEEE, 2009.
Amir-massoud Farahmand, Csaba Szepesvari, and Remi Munos. Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems, pp. 568-576,
2010.
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesvari, and Shie Mannor. Reg-
ularized policy iteration with nonparametric function spaces. The Journal of Machine Learning
Research, 17(1):4809-4874, 2016.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. arXiv preprint arXiv:1901.11275, 2019.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Matthew Hausknecht and Peter Stone. Deep recurrent Q-learning for partially observable mdps. In
2015 AAAI Fall Symposium Series, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv
preprint arXiv:1710.05468, 2017.
11
Under review as a conference paper at ICLR 2020
Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function
combinations including neural networks. arXiv preprint arXiv:1607.01434, 2016.
Jens Kober and Jan Peters. Reinforcement learning in robotics: A survey. In Reinforcement Learning,
pp. 579-610. Springer, 2012.
Michail G Lagoudakis and Ronald Parr. Value function approximation in zero-sum markov games.
In Uncertainty in Artificial Intelligence, pp. 283-292, 2002.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement
learning, pp. 45-73. Springer, 2012.
Alessandro Lazaric, Mohammad Ghavamzadeh, and Remi Munos. Finite-sample analysis of least-
squares policy iteration. Journal of Machine Learning Research, 13(Oct):3041-3074, 2012.
Alessandro Lazaric, Mohammad Ghavamzadeh, and Remi Munos. Analysis of classification-based
policy iteration algorithms. The Journal of Machine Learning Research, 17(1):583-612, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-8166,
2018.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-Rao metric, geometry,
and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine learning, 8(3-4):293-321, 1992.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994, pp. 157-163. Elsevier, 1994.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolu-
tional neural networks. In Conference on Computer Vision and Pattern Recognition, pp. 806-814,
2015.
Ruishan Liu and James Zou. The effects of memory replay in reinforcement learning. arXiv preprint
arXiv:1710.06574, 2017.
Chao Ma, Lei Wu, et al. A comparative analysis of the optimization and generalization property
of two-layer neural network and random feature models under gradient descent dynamics. arXiv
preprint arXiv:1904.04326, 2019.
Wolfgang Maass. Neural nets with superlinear VC-dimension. Neural Computation, 6(5):877-884,
1994.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
Press, 2012.
12
Under review as a conference paper at ICLR 2020
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(May):815-857, 2008.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
International conference on machine learning, pp. 807-814, 2010.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422-2430, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A PAC-
Bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949-5958,
2017b.
Stephen David Patek. Stochastic and shortest path games: theory and algorithms. PhD thesis,
Massachusetts Institute of Technology, 1997.
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum markov games. In International Conference on Machine Learning (ICML
2015), 2015.
Julien Perolat, Bilal Piot, Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. Softened approximate
policy iteration for markov games. In International Conference on Machine Learning, 2016a.
Julien Perolat, Bilal Piot, Bruno Scherrer, and Olivier Pietquin. On the use of non-stationary
strategies for solving two-player zero-sum markov games. In International Conference on Artificial
Intelligence and Statistics, pp. 893-901, 2016b.
Julien Perolat, Bilal Piot, and Olivier Pietquin. Actor-critic fictitious play in simultaneous move
multistage games. In International Conference on Artificial Intelligence and Statistics, 2018.
HL Prasad, Prashanth LA, and Shalabh Bhatnagar. Two-timescale algorithms for learning Nash
equilibria in general-sum stochastic games. In International Conference on Autonomous Agents
and Multiagent Systems, pp. 1371-1379. International Foundation for Autonomous Agents and
Multiagent Systems, 2015.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
Martin Riedmiller. Neural fitted Q iteration-first experiences with a data efficient neural reinforcement
learning method. In European Conference on Machine Learning, pp. 317-328. Springer, 2005.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.
International Conference on Learning Representations, 2016.
Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu Geist.
Approximate modified policy iteration and its application to the game of tetris. Journal of Machine
Learning Research, 16:1629-1676, 2015.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activa-
tion function. arXiv preprint arXiv:1708.06633, 2017.
13
Under review as a conference paper at ICLR 2020
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. Nature, 550(7676):354, 2017.
Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Perolat, Karl Tuyls, Remi Munos, and
Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments.
In Advances in Neural Information Processing Systems, pp. 3426-3439, 2018.
Charles J Stone. Optimal global rates of convergence for nonparametric regression. The Annals of
Statistics, pp. 1040-1053, 1982.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2011.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems, pp. 1057-1063, 2000.
Taiji Suzuki. Adaptivity of deep relu network for learning in Besov and mixed smooth Besov spaces:
optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.
Manel Tagorti and Bruno Scherrer. On the rate of convergence and error bounds for LSTD (λ). In
International Conference on Machine Learning, pp. 1521-1529, 2015.
Samuele Tosatto, Matteo Pirotta, Carlo D’Eramo, and Marcello Restelli. Boosted fitted Q-iteration.
In International Conference on Machine Learning, pp. 3434-3443, 2017.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In AAAI Conference on Artificial Intelligence, 2016.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. Princeton
University Press, 1947.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International Conference on Machine
Learning, pp. 1995-2003, 2016.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In
Advances in Neural Information Processing Systems, pp. 4987-4997, 2017.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. arXiv preprint arXiv:1904.00687, 2019.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Bayar. Finite-sample analyses for
fully decentralized multi-agent reinforcement learning. arXiv preprint arXiv:1812.02783, 2018.
Shangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint
arXiv:1712.01275, 2017.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
14
Under review as a conference paper at ICLR 2020
A Deep Q-Network
We are present the DQN algorithm for MDP in details, which is proposed by Mnih et al. (2015) and
adapted here to discounted MDP. As shown in Algorithm 2 below, DQN features two key tricks that
lead to its empirical success, namely, experience replay and target network.
Algorithm 2 Deep Q-NetWork (DQN)
Input: MDP (S, A, P, R, γ), replay memory M, number of iterations T , minibatch size n,
exploration probability ∈ (0, 1), a family of deep Q-netWorks Qθ : S × A → R, an integer Ttarget
for updating the target netWork, and a sequence of stepsizes {αt }t≥0 .
Initialize the replay memory M to be empty.
Initialize the Q-netWork With random Weights θ.
Initialize the Weights of the target netWork With θ? = θ.
Initialize the initial state S0 .
for t = 0, 1, . . . , T do
With probability , choose At uniformly at random from A, and With probability 1 - , choose
At such that Qθ(St, At) = maxa∈AQθ(St, a).
Execute At and observe reWard Rt and the next state St+1.
Store transition (St, At, Rt, St+1) in M.
Experience replay: Sample random minibatch of transitions {(si, ai, ri, s0i)}i∈[n] from M.
For each i ∈ [n], compute the target Yi = ri + Y ∙ maXa∈A Qθ? (si, a).
Update the Q-netWork: Perform a gradient descent step
θ 一 θ — αt •— X ： [Y — Qθ(Si, ai)] ∙ VθQθ(Si, ai).
Update the target network: Update θ? J θ every Ttarget steps.
end for
Define policy ∏ as the greedy policy with respect to Qθ.
Output: Action-value function Qθ and policy ∏.
B Computational Aspect of DQN
Recall that in Algorithm 1 we assume that the global optima of the nonlinear least-squares problem
in (3.1) is be obtained in each iteration. We make such assumption as our focus is on the statistical
analysis. In terms of optimization, it has been shown recently that, when the neural network is
overparametrized, (stochastic) gradient descent converges to the global minima of the empirical
function. Moreover, the generalization error of the obtained neural network can also be established.
The intuition behind these results is that, when the neural network is overparametrized, it behaves
similar to the random feature model (Rahimi & Recht, 2008). See, e.g., Li & Liang (2018); Du et al.
(2018b;a); Zou et al. (2018); Chizat & Bach (2018); Allen-Zhu et al. (2018a;b); Jacot et al. (2018);
Cao & Gu (2019); Arora et al. (2019); Ma et al. (2019); Mei et al. (2019); Yehudai & Shamir (2019)
and the references therein. Also see Fan et al. (2019) for a detailed survey. In the following, we make
an initial attempt in providing a unified statistical and computational analysis of DQN.
In the following, we consider the reinforcement learning problem with the state space S = [0, 1]r
and a finite action space A. To simplify the notation, we represent action a using one-hot embedding
and thus identify it as an element in {0, 1}|A|. Thus, we can pack the state S and the action a together
and obtain a vector (S, a) in Rd, where we denote r + |A| by d.
15
Under review as a conference paper at ICLR 2020
We represent the Q-network by the family of two-layer neural networks
1m
Q(s,a; b, W) = √= X^ bj ∙ σ[Wj(s,a)],	∀(s,a) ∈ S ×A.	(B.1)
m j=1
Here m is the number of neurons, bj ∈ R and Wj ∈ Rd for all j ∈ [m], and σ(u) = max{u, 0} is
the ReLU activation function. Here b = (b1, . . . , bm)> ∈ Rm and W = (W1, . . . , Wm) ∈ Rd×m
are the weights of the neural network. Then, in the k-th iteration of Algorithm 1, the optimization
problem in (3.1) becomes
minimize 1- X[Yi - Q(Si,Ai; b,W)]2,	(B.2)
b,W	2n
,	i=1
where Y = Ri+γ∙maxa∈/ Qk(S0, a) is the target and Qk is the Q-networkcomputed in the previous
iteration. Notice that this problem is the least-squares regression with two-layer neural networks in
the overparametrized setting. We solve the optimization in (B.2) via gradient descent. Specifically,
we initialize. the parameters via bj i% Unif({-1,1}) and Wj i妙 N(0,Id∕d), where Id is the
identity matrix in Rd. Moreover, for ease of presentation, during training we keeping {b1, . . . , bm}
fixed as the random initialization and only optimize over W. Moreover, let W(0) ∈ Rd×m be the
initialization of W. We restrict the weight W to a Frobenius ball centered at W(0) with radius B > 0,
i.e., we define
BB = {W ∈ Rd×m: kW - W(0)kfro ≤ B},	(B.3)
where B is a sufficiently large constant. Thus, the problem in (B.2) is transformed into
minimizeLn(W) = 1- X[Yi - Q(Si,Ai; b,W)]2,	(B.4)
W∈BB	2n
i=1
where b is fixed to the initial value. We solve this optimization problem via projected gradient descent,
which generates a sequence of weights {W (t)}t≥0 ⊆ BB satisfying
n
W(t + 1)= ∏Bb [w(t) - η X[Yi - Q(Si, Ai； b, W(t))] ∙ VwQ(Si, Ai； b, W(t))],	(B.5)
n i=1
where ΠBB is the projection operator onto BB and η > 0 is the step size.
To understand the convergence of the updates in (B.4), we utilize the fact that overparametrized
two-layer neural networks behave similar to the random feature model. Specifically, notice that we
have
m
Q(s, a； b,W) = √= X bj ∙ 1{W>(s, a) > 0} ∙ Wjr(s, a),	(B.6)
m j=1
VWjQ(s,a; b,W) = √mbj ∙ 1{W>(s,a) > 0},	Vj ∈ [m].	(B.7)
We define function class
1m
FBtm = {Q(s, a) = √m 鼻 % ∙ 1{Wj(t)>(s, a) > 0}∙ W>(s,a): W ∈Bb j.	(B.8)
By (B.6) and (B.7), every Q ∈ FB(t,)m can be written as
m
Q(s, a) = Q(s, a； b, W(t)) +—√=)： bj ∙ 1{Wj(t)>(s, a) > 0} ∙ [Wj - Wj(t)]>(s, a)
m j=1
=Q(s, a； b, W(t)) +〈VwQ(s, a; b, W(t)),W - W(t)〉.
16
Under review as a conference paper at ICLR 2020
Thus, FBtm contains first-order approximations of Q(s, a; b, W(t)). Furthermore, as shown in Du
et al. (2018b;a); Arora et al. (2019), when m is sufficiently large, the overall effect of the scaled
indicators {1/√m ∙ 1 {Wj(t)>(s, a) > 0} are well approximated by the indicators produced by the
initial weights. That is, when m approaches infinity, for each fixed t, FB(t,)m in (B.8) is close to
1m
FBm = jnQ(s, a)	=	√m	二 bj	∙	1{Wj(0)>(s, a) >	0}	∙	Wr(S,a):	W ∈	BB j.	(B.9)
Notice that {φj∙ (s,a) = 1∕√m ∙bj∙ 1{Wj- (0)>(s, a) > 0} are i.i.d. random variables. Thus, function
class FB,m is family of functions that can be written as combinations of random features, i.e.,
FB,m = ∣Q(s,a) = Q(s,a; b,W(0)) + X φj(s,a)>W∙: ∣∣WIIfro ≤ b},	(B.10)
j=1
where we utilize the definition of BB . More importantly, when m goes to infinity, the empirical
distribution of the random features {φj (S, a)}j∈[m] converges to its population distribution. We
denote φ(∙, ∙; β, W) as the random feature, where β ∈ Unif({-1,1}) and W 〜 N(Id∕d). We denote
μ as thejoint distribution of β and w. Then, FB,m in (B.10) converges to a set FB given by
φ(s, a; β, w)>α(β, W) dμ(β, w), : / llα(β,w)k2 dμ(β,w) ≤ B2
(B.11)
FB = {q(s, a) = Qo(s, a) + /
where α is a function over {-1, 1} × Rd and Q0 (S, a) = limm→∞ Q S, a; b, W(0) .
Furthermore, it can be shown that FB is a subset of a reproducing kernel Hilbert space (Rahimi &
Recht, 2008) H generated by kernel
K((s,a), (s0, a0)) = EN - [l{w>(s,a) > 0} ∙ 1{w>(s0,a0) > 0}h(s, a), (s0,a0)i]. (B.12)
Besides, the inner product induced by the RKHS norm between two functions
f1
/ φ(∙, ∙; β,w)>αι(β,w) dμ(β,w)
and
f2
/ φ(∙, ∙; β,w)>αι(β,w) dμ(β,w)
is given by hfι,f2iH = /<αι(β, w),αι(β, w)i dμ(β, w). Thus, FB given in (B.11) can be written
a RKHS-norm ball
FB = {Q = Qo + f: kfkH ≤ B}
with radius B . As a result, when both m and n go to infinity, the population problem correspond
to (B.2) becomes mιnιmιzeQ∈F* ∣∣Q 一 TQk∣∣σ, whose solution is denoted by Qk+ι∙ Therefore,
utilizing the recent result in optimization for two-layer neural networks Arora et al. (2019), the
Q-networks obtained by the projected gradient descent in (B.5) satisfy
∣Q(∙,∙,W(t)) — Qk+ι∣σ . 1∕√n	(B.13)
when both m and t are sufficiently large. That is, when the network is overparametrized, projected
gradient descent produces a solution Qt+ι with O(1∕√n) generalization error. Moreover, using
inequality (a + b)2 ≤ 2a2 + 2b2, we have
∣Qt+ι - TQk∣σ ≤ 2 ∙kQt+ι - Qk+1kσ2 +2∣Qk+ι - TQkk* . ∖∕√ + dist(FB,σ,T),
(B.14)
where dist(FB, σ, T) is defined as
dist(FB,σ,T) = Jnf* SUp if 一 Tg∣σ,
f∈FB* g∈FB*
which measures the approximation error of functions in the RKHS ball FB with respect the Bellman
operator and the sampling distribution σ. Finally, combining (B.14) with the error propagation result
in Theorem D.1, we obtain the error of neural-FQI with overparametrized two-layer neural networks.
17
Under review as a conference paper at ICLR 2020
C Related Work
There is a huge literature in deep reinforcement learning, where algorithms are based on Q-learning
or policy gradient (Sutton et al., 2000). We refer the reader to Arulkumaran et al. (2017) for a survey
of the recent developments of DRL. In addition, the DQN algorithm is first proposed in Mnih et al.
(2015), which applies DQN to Artari 2600 games (Bellemare et al., 2013). The extensions of DQN
include double DQN (van Hasselt et al., 2016), dueling DQN (Wang et al., 2016), deep recurrent
Q-network (Hausknecht & Stone, 2015), and asynchronous DQN (Mnih et al., 2016). All of these
algorithms are corroborated only by numerical experiments, without theoretical guarantees. Moreover,
these algorithms not only inherit the tricks of experience replay and the target network proposed in
the original DQN, but develop even more tricks to enhance the performance. Furthermore, recent
work such as Schaul et al. (2016); Liu & Zou (2017); Zhang & Sutton (2017) study the effect of
experience replay and propose various modifications.
In addition, our work is closely related to the literature on batch reinforcement learning (Lange et al.,
2012), where the goal is to estimate the value function given transition data. These problems are
usually formulated into least-squares regression, for which various algorithms are proposed with
finite-sample analysis. However, most existing work focus on the settings where the value function
are approximated by linear functions. See Bradtke & Barto (1996); Boyan (2002); Lagoudakis &
Parr (2003); Lazaric et al. (2016); Farahmand et al. (2010); Lazaric et al. (2012); Tagorti & Scherrer
(2015) and the references therein for results of the least-squares policy iteration (LSPI) and Bellman
residue minimization (BRM) algorithms. Beyond linear function approximation, a recent work
Farahmand et al. (2016) study the performance of LSPI and BRM when the value function belongs to
a reproducing kernel Hilbert space. However, we study the fitted Q-iteration algorithm, which is a
batch RL counterpart of DQN. The fitted Q-iteration algorithm is proposed in Ernst et al. (2005), and
Riedmiller (2005) propose the neural FQI algorithm. A finite-sample bound for FQI is established in
Munos & Szepesvari (2008) for a large class of regressors. However, their results are not applicable
to ReLU networks due to the huge capacity of deep neural networks. Furthermore, various extensions
of FQI are studied in Antos et al. (2008a); Farahmand et al. (2009); Tosatto et al. (2017); Geist et al.
(2019) to handle continuous actions space, ensemble learning, and entropy regularization.
Furthermore, our work is also related to works that apply reinforcement learning to zero-sum Markov
games. The Minimax-Q learning is proposed by Littman (1994), which is an online algorithm that
is an extension Q-learning. Subsequently, for Markov games, various online algorithms are also
proposed with theoretical guarantees. These work consider either the tabular case or linear function
approximation. See, e.g., Bowling (2001); Conitzer & Sandholm (2007); Prasad et al. (2015); Wei
et al. (2017); Perolat et al. (2018); Srinivasan et al. (2018); Wei et al. (2017) and the references
therein. In addition, batch reinforcement learning is also applied to zero-sum Markov games by
Lagoudakis & Parr (2002); Perolat et al. (2015); Perolat et al. (2016a;b); Zhang et al. (2018), which
are closely related to our work. All of these works consider either linear function approximation or a
general function class with bounded pseudo-dimension (Anthony & Bartlett, 2009). However, there
results cannot directly imply finite-sample bounds for Minimax-DQN due to the huge capacity of
deep neural networks.
Finally, our work is also related a line of research on the model capacity of ReLU deep neural
networks, which leads to understanding the generalization property of deep learning (Mohri et al.,
2012; Kawaguchi et al., 2017). Specifically, Bartlett (1998); Neyshabur et al. (2015b;a); Bartlett
et al. (2017a); Golowich et al. (2017); Liang et al. (2017) propose various norms computed from
the networks parameters and establish capacity bounds based upon these norms. In addition, Maass
(1994); Bartlett et al. (1999); Schmidt-Hieber (2017); Bartlett et al. (2017b); Klusowski & Barron
(2016); Suzuki (2018); Bauer et al. (2019) study the Vapnik-Chervonenkis (VC) dimension of neural
networks and Dziugaite & Roy (2017); Neyshabur et al. (2017a) establish the PAC-Bayes bounds for
neural networks. Among these work, our work is more related to Schmidt-Hieber (2017); Suzuki
(2018), which relate the VC dimension of the ReLU networks to a set of hyperparameters used to
define the networks. Based on the VC dimension, they study the statistical error of nonparametric
18
Under review as a conference paper at ICLR 2020
regression using ReLU networks. In sum, theoretical understanding of deep learning is pertinent
to the study of DRL algorithms. See Kawaguchi et al. (2017); Neyshabur et al. (2017b); Fan et al.
(2019) and the references therein for recent developments on theoretical analysis of the generalization
property of deep learning.
D Proof of the Main Theorem
In this section, we present a detailed proof of Theorem 4.4.
Proof. The proof requires two key ingredients. First in Theorem D.1 we quantify how the error
of action-value function approximation propagates through each iteration of Algorithm 1. Then in
Theorem D.2 we analyze such one-step approximation error for ReLU networks.
Theorem D.1 (Error Propagation). Recall that {Qek}0≤k≤K are the iterates of Algorithm 1. Let
πK be the one-step greedy policy with respect to QK, and let QπK be the action-value function
corresponding to πK. Under Assumption 4.3, we have
kQ*- QnK kl,μ≤
2φμ,σ Y	+ 4Y K+1	R
(1 - γ)2	max +(1 - γ)2	max,
(D.1)
where we define the maximum one-step approximation error εmax = maxk∈[K] kT Qk-1 - Qkkσ.
Here φμ,σ is a constant that only depends on the probability distributions μ and σ.
Proof. See §F.1 for a detailed proof.	□
In the sequel, we establish an upper bound for the one-step approximation error kT Qk-1 - Qk kσ for
each k ∈ [K].
Theorem D.2 (One-step Approximation Error). Let F ⊆ B(S × A, Vmax) be a class of measurable
functions on S × A that are bounded by Vmax = Rmax/(1 -Y), and let σ be a probability distribution
on S × A. Also, let {(Si, Ai)}i∈[n] be n i.i.d. random variables in S × A following σ. For each
i ∈ [n], let Ri and Si0 be the reward and the next state corresponding to (Si, Ai). In addition, for any
fixed Q ∈ F, we define Yi = Ri + Y ∙ maXa∈A Q(Si, a). Based on {(Xi, Ai, Yi)}i∈[n], we define
Q as the solution to the least-squares problem
minimize - X[∕(Si, Ai)-匕]2.	(D.2)
f∈F	n
i=1
Meanwhile, for any δ > 0, let N(δ, F, k ∙ ∣∣∞) be the minimal δ-covering of F with respect to
'∞-norm, and we denote by N its cardinality. Then for any e ∈ (0,1] and any δ > 0, we have
kQ - TQ∣σ ≤ (1 + e)2 ∙ 3(F) + C ∙ Vmax/(n ∙ e) ∙ log N + C0 ∙ Vmax ∙ δ, (D.3)
where C and C0 are two positive absolute constants and ω(F) is defined as
3(F)=sup inf kf -Tgk2σ.	(D.4)
g∈F f∈F
Proof. See §F.2 for a detailed proof.
□
EK , ♦	1	FC 11 m Z-∖	Z-∖ 11	∙ 1 ∙ r-ɪ-ɪl	ɪʌ Y	, √^ι P∖	∙
To obtain an upper bound for kT Qk-1 - Qkkσ as required in Theorem D.1, we set Q = Qk-1 in
Theorem D.2. Then according to Algorithm 1, Q defined in (D.2) becomes Qk. We set the function
class F in Theorem D.2 to be the family of ReLU Q-networks F0 defined in (4.1). By setting e = 1
and δ = 1/n in Theorem D.2, we obtain
kQk+1 - TQk ∣∣σ ≤ 4 ∙ 3(F0) + C ∙ V2ax∕n ∙ log No,	(D.5)
19
Under review as a conference paper at ICLR 2020
where C is a positive absolute constant and
No = ∣N(1∕n,Fo, k ∙ k∞)∣	(D.6)
is the 1/n-covering number of F0. In the subsequent proof, we establish upper bounds for ω(F0)
defined in (D.4) and log N ,respectively. Recall that the family of composite Holder smooth functions
G0 is defined in (4.2). By Assumption 4.2, we have Tg ∈ G0 for any g ∈ F0. Hence, we have
ω(F0) = sup inf kf - f0k2σ ≤ sup inf kf - f0k2∞,	(D.7)
f0 ∈G0 f∈F0	f0 ∈G0 f∈F0
where the right-hand side is the '∞-error of approximating the functions in Go using the family of
ReLU networks F0 .
By the definition of Go in (4.2), for any f ∈ Go and any a ∈ A, f(∙, a) ∈ G({(pj, tj, βj, Hj)}j∈[q])
is a composition of Holder smooth functions, that is, f (∙,a) = gq ◦•••◦ gι. Recall that, as defined in
Definition 2.3, gjk is the k-th entry of the vector-valued function gj. Here gjk ∈ Ctj ([aj, bj]tj , βj, Hj)
for each k ∈ [pj+ι] and j ∈ [q]. In the sequel, we construct a ReLU network to approximate f (∙,a)
and establish an upper bound of the approximation error on the right-hand side of (D.7). We first
show that f (∙,a) can be reformulated as a composition of Holder functions defined on a hypercube.
We define h1 = g1∕(2H1) + 1∕2,
hj (u) = gj(2Hj-1u - Hj-1)∕(2Hj) + 1∕2, for all j ∈ {2, . . . , q - 1},
and hq (u) = gq(2Hq-1u - Hq-1). Then we immediately have
f(∙,a) = gq ◦•••◦ gι = hq。…。hi.	(D.8)
Furthermore, by the definition of Holder smooth functions in Definition 2.2, for any k ∈ [p2], we
have that h1k takes value in [0, 1] and h1k ∈ Ct1 ([0, 1]t1 , β1, 1). Similarly, for anyj ∈ {2, . . . , q - 1}
and k ∈ [pj+1], hjk also takes value in [0, 1] and
hjk ∈Ctj ([0,1]tj,βj, (2Hj-i)βj).
Finally, recall that we use the convention that pq+1 = 1, that is, hq is a scalar-valued function that
satisfies
hq ∈Ctq ([0,1]tq ,βq ,Hq (2Hq-I)Ie ).
Now we employ the following lemma of Schmidt-Hieber (2017) to construct a ReLU network that
approximates each hjk, which combined with (D.8) yields a ReLU network that is close to f (∙, a).
Recall that, as defined in Definition 2.2, we denote by Cr (D, β, H) the family of Holder smooth
functions with parameters β and H on D ⊆ Rr .
Lemma D.3 (Approximation of Holder Smooth Function). For any integers m ≥ 1 and N ≥
max{(β+1)r, (H +1)}, let L = 8+(m+5)∙(1+dlog2 r]), do = r, dj = 12rΝ for each j ∈ [L], and
dL+1 = 1. For any g ∈ Cr([0, 1]r, β, H), there exists a ReLU network f ∈ F(L,{dj}jL=+o1, s, Vmax)
as defined in Definition 2.1 such that
kf - gk∞ ≤ (2H + 1) ∙ 3r+1 ∙ N ∙ 2-m + H ∙ 2β ∙ N-e/r,
where the parameter S satisfies S ≤ 94 ∙ r2 ∙ (β +1)2r ∙ N ∙ (m + 6) ∙ (1 +「log? r]).
Proof. See Theorem 3 of Schmidt-Hieber (2017) for a detailed proof.
□
We apply Lemma D.3to hjk : [0,1]tj → [0,1] for any j ∈ [q] and k ∈ [pj+ι]. WeSet m = njlog? n]
for a sufficiently large constant η > 1, and set N to be a sufficiently large integer depending on n,
which will be specified later. In addition, we set
Lj = 9 + (m + 5) ∙ (1 + dlog2 tje)	(D.9)
and define
W = max(mχ-(2Hj-ι)βj,Hq(2Hq-ι)βq },
(D.10)
20
Under review as a conference paper at ICLR 2020
which is a constant. Without loss of generality, we assume W ≥ 1 hereafter. Then by Lemma D.3,
there exists a ReLU network hjk such that
kbjk - hjkk∞ ≤ (2W + 1) ∙ 3tj ∙ N ∙ n-η + W ∙ 2% ∙ N-βj/tj.
Furthermore, we have hjk ∈ F(Lj, {tj, 12tjN, . . . , 12tjN, 1}, sj) with
Sj ≤ 94 ∙ tj ∙ (βj + 1)2tj ∙ N ∙ (m + 6) ∙ (1 +「log? tj]).
(D.11)
(D.12)
Meanwhile, since hj+1 = (h(j+1)k)k∈[pj+2] takes input from [0, 1]tj+1, we need to further transform
hjk so that it takes value in [0, 1]. In particular, we define σ(u) = 1 -(1 -u)+ = min{max{u, 0}, 1}
for any u ∈ R. Note that σ can be represented by a two-layer ReLU network with four nonzero
weights. Then we define ehjk = σ ◦ bhjk and ehj = (ehjk)k∈[pj+1]. Note that by the definition ofehjk,
we have hjk ∈ F(Lj + 2, {tj, 12tjN, . . . , 12tjN, 1}, sj + 4), which yields
7 - k∕τ Ier∕rc∕7∖r	FC∕7∖r	1 /	. λ ∖	∖
hj ∈ F(Lj + 2, {tj, 12tjN ∙ pj + 1, ..., 12tjN ∙ pj + 1,pj + 1}, (sj + 4) ∙ pj + 1).
Moreover, since both hjk and hjk take value in [0, 1], by (D.11) we have
∣∣T	7. Ii	U _ _ G	_ _ 7. Ii JUG 7. Ii
khjk -	hjk k∞	=	kσ ◦ hjk -	σ ◦ hjk k∞ ≤ khjk - hjk k∞
≤	(2W + 1) ∙	3tj ∙ N ∙ n-η + W ∙ 2% ∙ N-β/j,	(D.13)
where the constant W is	defined m (D.10). Now We define f: S → R by f	= hq	◦ ∙ ∙ ∙ ◦ hi, which
falls in the function class
F(Le, {r, de, . . . , de, 1}, se),	(D.14)
qq
where We define L = Ej=I(Lj + 2), d = maxj∙∈[q] 12tj- ∙ Pj+i ∙ N, and S = Ej=I(Sj + 4) ∙ Pj+i.
Recall that, as defined in (D.9), we have Lj = 9 + (m + 5) ∙ (1 +「log? tj]). Then when n is
sufficiently large, we have
qq
L ≤ £[11 + (log2 n + 5) ∙ (1 + dlogj tj])] ≤ £4logj ti ∙ log2 n . (log n)1+ξ,	(D.15)
i=i	i=i
where ξ > 0 is an absolute constant. Here the last inequality follows from (4.6). Moreover, for d in
(D.14), by (4.6) we have
N ∙ max{pj+ι ∙ tj} . d ≤ 12 ∙ N ∙ (maxPj) ∙ (maxtj) . N ∙ (log n)ξ.	(D.16)
In addition, combining (D.12), (4.6), and the fact that tj ≤ Pj, we obtain
e . N ∙ log n ∙ (maxpj)∙^X logtj) . N ∙ (log n)1+2ξ.	(D.17)
Now we show that the function class in (D.14) can be embedded in F(L*, {d*}L=+1, s*), where L*,
{d*}L=+1, and s* are specified in (4.7). To this end, we set
N = Im蜂 C ∙ ntj/(2e；+tj)],	(D.18)
where the absolute constant C > 0 is sufficiently large. Note that we define α* = maxj∙∈ 团 tj/(2β* +
tj). Then (D.18) implies that N N nα*. When ξ0 in (4.7) satisfies ξ0 ≥ 1 + 2ξ, by (D.15) we have
L ≤ L* . (log n)ξ0.
In addition, (D.16) and (4.7) implies that we can set dj ≥ d for all j ∈ [L*]. Finally, by (D.17) and
(D.18), we have e . nα* ∙ (logn)ξ0, which implies e + (L* 一 L) ∙ r ≤ s*. For an L-layer ReLU
21
Under review as a conference paper at ICLR 2020
network in (D.14), We can make it an L*-layer ReLU network by inserting L* 一 L identity layers,
since the inputs of each layer are nonnegative. Thus, ReLU networks in (D.14) can be embedded in
F[L*, {r,r,...,r,d,... ,d, 1},e + (L* — L)r],
which is a subset of F(L*, {d*"=+，，s*) by (4.7).
To obtain the approximation error ∣∣f — f (∙, a)k∞, we define Gj = hj ◦∙ ∙ •◦ hi and Gj = hj ◦∙ ∙ •◦ hi
for any j ∈ [q]. By triangle inequality, for any j > 1 we have
.... —— , -. .. — — ..
∣Gj — Gej∣∞ ≤ ∣hj ◦ Gej-i — hj ◦ Gj-i∣∞ + ∣ehj ◦ Gej-i — hj ◦ Gej-i ∣∞
≤ W ∙ IlGj-I- Gj-ιk∞∧1 + khj - hjk∞,	(D∙19)
where the second inequality holds since hj is Holder smooth. To simplify the notation, we define
λj = Qq=j+ι (β' ∧ 1) for any j ∈ [q — 1], and set λq = 1. By applying recursion to (D.19), we
obtain
q
kf (∙, a) - f ll∞ = IGq - Gq ll∞ ≤ W X Ihj - hjI∞ ,
j=i
(D.20)
where the constant W is defined in (D.10). Here in (D.20) we use the fact that (a + b)α ≤ aα + bα
for all α ∈ [0, 1] and a, b > 0.
In the sequel, we combine (D.7), (D.13), (D.20), and (D.18) to obtain the final bound on ω(F0).
Since we can set the constant η in (D.13) to be sufficiently large, the second term on the right-hand
side of (D.13) is the leading term asymptotically, that is,
Iejk — hjkk∞ . NTj/j.	(D.21)
Also note that β* = βj ∙ Qq=j+ι(β' ∧ 1) = βj ∙ λj for all j ∈ [q — 1]. Thus we have β* = βj ∙ λj
for all j ∈ [q]. Combining (D.20) and (D.21), we have
q
q
kf(∙,α)- f∣∣∞ . X(NTj∕j)λj = XNTm . maχN-β"j.
j=i	j=i	j∈[q]
Thus, we combine (D.7), (D.18), and (D.22) to obtain
ω(Fo) ≤ (maxN-β"tj)2 X maxn-2βj/(2βj+tj) = nα*-i.
(D.22)
(D.23)
As the final step of the proof, it remains to control the covering number of F0 defined in (4.1). By
definition, for any f ∈ F0, we have f (∙, a) ∈ F(L*, {d*}]；1, s*) for any a ∈ A. For notational
simplicity, we denote by Nδ the δ-covering of F(L*, {dj*}jL=* +i i, s*), that is, we define
Nδ = N[δ,F(L*, {dj}L=;1,s*), k∙∣∞].
By the definition of covering, for any f ∈ F0 and any a ∈ A, there exists ga ∈ Nδ such that
kf (∙, a) — ga∣∞ ≤ δ. Then we define a function g: S × A → R by g(s, a) = g。(S) for any
(s, a) ∈ S × A. By the definition of g, it holds that lf — gl∞ ≤ δ. Therefore, the cardinality of
N(δ, Fo, k ∙ k∞) satisfies
∣N(δ,Fo, k ∙ k∞)∣ ≤ ∣Nδ|1A1.	(D.24)
Now we utilize the following lemma in Anthony & Bartlett (2009) to obtain an upper bound of the
cardinality of Nδ .
Lemma D.4 (Covering Number of ReLU Network). Recall that the family of ReLU networks
F(L, {dj}L+o1, s, VmaX) is given in Definition 2.1. Let D = QL=I1(d' + 1). For any δ > 0, we have
log∣N[δ, F(L, {dj}L=+ol, s, Vmax), k∙k∞] ∣ ≤ (s + 1) ∙ log[2δ-i ∙ (L +1) ∙ D2].
22
Under review as a conference paper at ICLR 2020
Proof. See Theorem 14.5 of Anthony & Bartlett (2009) for a detailed proof.
□
Recall that We denote N(1/n, Fo, k ∙ k∞) by No in (D.6). By combining (D.24) with Lemma D.4
and setting δ = 1/n, we obtain that
log No ≤ |A| ∙ log |N | ≤ |A| ∙ (s* + 1) ∙ log[2n ∙ (L* + 1) ∙ D2],
where D = QL=+1(d； + 1). By the choice of L*, s*, and {dj}L=+1 in (4.7), We conclude that
log No . |A| ∙ s* ∙ L* max log(d*) . nα* ∙ (log n)1+2ξ0.	(D.25)
j∈[L*]	J
Finally, combining (D.1), (D.5), (D.23), and (D.25), we conclude the proof of Theorem 4.4.	□
E Extension to Two-Player Zero-Sum Markov Games
In this section, we propose the Minimax-DQN algorithm, which combines DQN and the Minimax-Q
learning for two-player zero-sum Markov games. We first present the background of zero-sum
Markov games and introduce the the Minimax-DQN algorithm in §E.1. Borrowing the analysis for
DQN in the previous section, we provide theoretical guarantees for the proposed algorithm in §E.2.
E.1 Minimax-DQN Algorithm
As one of the simplistic extension of MDP to the multi-agent setting, two-player zero-sum Markov
game is denoted by (S, A, B, P, R, γ), where S is state space, A and B are the action spaces of the
first and second player, respectively. In addition, P : S × A × B → P(S) is the Markov transition
kernel, and R : S × A × B → P(R) is the distribution of immediate reward received by the first
player. At any time t, the two players simultaneously take actions At ∈ A and Bt ∈ B at state
St ∈ S, then the first player receives reward Rt 〜R(St, At, Bt) and the second player obtains -Rt.
The goal of each agent is to maximize its own cumulative discounted return.
Furthermore, let π : S → P(A) and ν : S → P(B) be policies of the first and second players,
respectively. Then, we similarly define the action-value function Qπ,ν : S × A × B → R as
∞
QnV(s, a,b)	= E X Yt ∙	Rt	(So, Ao,	Bo)	= (s, a, b),At	〜∏(∙	|St),	Bt	〜V(∙	|St)	,	(E.1)
t=o
and define the state-value function V π,ν : S → R as
Vπ,ν(s) = E[Qπ,ν(s, A,B)∣A 〜∏(∙ | s),B 〜ν(∙ | s)].	(E.2)
Note that these two value functions are defined by the rewards of the first player. Thus,
at any state-action tuple (s, a, b), the two players aim to solve maxπ minν Qπ,ν (s, a, b) and
minν maxπ Qπ,ν (s, a, b), respectively. By the von Neumann’s minimax theorem (Von Neumann &
Morgenstern, 1947; Patek, 1997), there exists a minimax function of the game, Q* : S × A × B → R,
such that
Q* (s, a, b) = max min Qπ,ν (s, a, b) = min max Qπ,ν (s, a, b).	(E.3)
Moreover, for joint policy (π, ν) of two players, we define the Bellman operators Tπ,ν and T by
(T2Q)(s, a, b)= r(s, a,b)+ Y ∙ (P…Q)(s, a, b),
(TQ)(S, a, b) = r(S, a,b)+ Y ∙ (P*Q)(S, a, b),
where r(s, a, b) = rR(dr | s, a, b), and we define operators Pπ,ν and P* by
(P，Q)(S, a, b) = Es0〜P(∙ | s,a,b),a0〜∏(∙∣ s0),b0〜ν(∙∣ s0) [Q(S , a , b )],
(P*Q)(s, a, b) = Es0〜P(∙∣ s,a,b) { ,mx) ,mi为)Ea0〜∏0,b0〜V0 [Q(s0, a0, b0)] ∣.
(E.4)
(E.5)
23
Under review as a conference paper at ICLR 2020
Note that P * is defined by solving a zero-sum matrix game based on Q(s0, ∙, ∙) ∈ RlAl×lBl, which
could be achieved via linear programming. It can be shown that both Tπ,ν and T are γ-contractive,
with Qπ,ν defined in (E.1) and Q* defined in (E.3) being the unique fixed points, respectively.
Furthermore, similar to (2.3), in zero-sum Markov games, for any action-value function Q, the
equilibrium joint policy with respect to Q is defined as
[∏q(∙ | s),vq(∙ | s)] = argmax argmin Ea〜∏o,b〜“，[Q(s,a,b)],	∀s ∈ S.	(E.6)
π0∈P(A) ν0∈P(B)
That is, ∏q(∙ | S) and vq(∙ | S) solves the zero-sum matrix game based on Q(s, ∙, ∙) for all S ∈ S. By
this definition, we obtain that the equilibrium joint policy with respect to the minimax function Q*
defined in (E.3) achieves the Nash equilibrium of the Markov game.
Therefore, to learn the Nash equilibrium, it suffices to estimate Q*, which is the unique fixed point of
the Bellman operator T. Similar to the standard Q-learning for MDP, Littman (1994) proposes the
Minimax-Q learning algorithm, which constructs a sequence of action-value functions that converges
to Q* . Specifically, in each iteration, based on a transition (S, a, b, S0), Minimax-Q learning updates
the current estimator of Q*, denoted by Q, via
Q(s, a, b) J (1 — α) ∙ Q(s, a,b) + a ∙
r(s,a,b) + Y ∙ max min E。，〜∏o b,〜”’
π0∈P(A) ν0∈P(B)	,
Q(S0, a0, b0)	,
where α ∈ (0, 1) is the stepsize.
Motivated by this algorithm, we propose the Minimax-DQN algorithm which extend DQN to two-
player zero-sum Markov games. Specifically, we parametrize the action-value function using a deep
neural network Qθ : S × A × B → R and store the transition (St, At, Bt, Rt, St+1) into the replay
memory M at each time-step. Parameter θ of the Q-network is updated as follows. Let Qθ* be the
target network. With n independent samples {(Si, ai, bi, ri, S0i)}i∈[n] from M, for all i ∈ [n], we
compute the target
Yi = r + Y ∙ max min Ea〜n，,b〜v，[Qθ* (si, a, b)],	(E.7)
π0∈P(A) ν0∈P(B)	,	i
which can be attained via linear programming. Then we update θ in the direction of VθL(θ), where
L(θ) = n-1 Pi∈[n][Yi — Qθ(Si, ai, bi)]2. Finally, the target network Qθ* is updated every Ttarget
steps by letting θ* = θ.
To understand the theoretical aspects of this algorithm, we similarly utilize the framework of batch
reinforcement learning for statistical analysis. With the insights gained in §3, we consider a modifica-
tion of Minimax-DQN based on neural fitted Q-iteration, whose details are stated in Algorithm 4.
As in the MDP setting, we replace sampling from the replay memory by sampling i.i.d. state-action
tuples from a fixed distribution σ ∈ P(S × A × B), and estimate Q* in (E.3) by solving a sequence
of least-squares regression problems.
E.2 Theoretical Results for Minimax-FQI
Following the theoretical results established in §4, in this subsection, we provide statistical guarantees
for the Minimax-FQI algorithm with F being a family of deep neural networks with ReLU activation.
Hereafter, without loss of generality, we assume S = [0, 1]r with r being a fixed integer, and the
action spaces A and B are both finite. To evaluate the performance of the algorithm, we first introduce
the best-response policy as follows.
Definition E.1. For any policy π : S → P(A) of player one, the best-response policy against π,
denoted by νπ* , is defined as the optimal policy of second player when the first player follows π . In
other words, for all S ∈ S, we have ν∏(∙ | S) = argmin” Vπ,v(s), where Vπ,v is defined in (E.2).
Note that when the first player adopt a fixed policy π, from the perspective of the second player, the
Markov game becomes a MDP. Thus, νπ* is the optimal policy of the MDP induced by π. Moreover,
it can be shown that, for any policy π, Q*(S, a, b) ≥ Qπ,νπ* (S, a, b) holds for every state-action tuple
24
Under review as a conference paper at ICLR 2020
Algorithm 3 Minimax Deep Q-Network (Minimax-DQN) for the second player
Input: Zero-Sum Markov game (S, A, B, P, R, γ), replay memory M, number of iterations T,
minibatch size n, exploration probability ∈ (0, 1), a family of deep Q-networks Qθ : S ×A×B →
R, an integer Ttarget for updating the target network, and a sequence of stepsizes {αt }t≥0.
Initialize the replay memory M to be empty.
Initialize the Q-network with random weights θ.
Initialize the weights of the target network by letting θ? = θ.
Initialize the initial state S0 .
for t = 0, 1, . . . , T do
With probability , choose Bt uniformly at random from B, and with probability 1 - , sample
Bt according to the equilibrium policy eQθ (∙ | St) defined in(??).
Execute Bt and observe the first player,s action At, reward Rt satisfying -Rt 〜R(St, At, Bt),
and the next state St+ι 〜P(∙ | St, At, Bt).
Store transition(St, At, Bt, Rt, St+1) in M.
Experience replay: Sample random minibatch of transitions {(si, ai, bi, ri, s0i)}i∈[n] from M.
For each i ∈ [n], compute the target
Yi = r + Y ∙ max, zmin^)Ea〜∏o,b〜“0[Qθ* (Si,a, b)].
Update the Q-network: Perform a gradient descent step
θ 一 θ — αt ∙ n〉： [Yi — Qθ(si, ai, bi)] ∙ VθQθ(Si, ai, bi).
Update the target network: Update θ? J θ every Ttarget steps.
end for
Output: Q-network Qθ and equilibrium joint policy with respect to Qθ .
Algorithm 4 Fitted Q-Iteration Algorithm for Zero-Sum Markov Games (Minimax-FQI)
Input: Two-player zero-sum Markov game (S, A, B, P, R, γ), function class F, distribution
σ ∈ P(S × A × B), number of iterations K, number of samples n, the initial estimator Q0 ∈ F.
for k = 0, 1, 2, . . . , K - 1 do
Sample n i.i.d. observations {(Si, Ai, Bi)}i∈[n] from σ, obtain Ri 〜 R(∙ ∣Si,Ai,Bi) and
Si 〜P(∙∣Si,Ai,Bi).	〜
Compute Yi = Ri + Y ∙ max∏o∈p(A) min”，∈p(B)Ea〜∏,b〜“，[Qk (Si a, b)]..
Update the action-value function:
1
Qk+1 J argmin -
f∈F n
n
X[Yi-f(Si,Ai,Bi)]2.
i=1
end for
Let (πK, νK) be the equilibrium joint policy with respect to QK, which is defined in (E.6).
Output: An estimator QK of Q* andjoint policy (∏κ, VK).
(S, a, b). Thus, by considering the adversarial case where the opponent always plays the best-response
policy, the difference between Qπ.νπ* and Q* servers as a characterization of the suboptimality of
π. Hence, to quantify the performance of Algorithm 4, we consider the closeness between Q* and
QπK,νπ*K , which will be denoted by Q*K hereafter for simplicity. Specifically, in the following we
establish an upper bound for ∣∣Q* - QK ∣∣ι,μ for some distribution μ ∈ P(SXA ×B).
We first specify the function class F in Algorithm 4 as follows.
25
Under review as a conference paper at ICLR 2020
Assumption E.2 (Function Classes). Following Definition 4.1, let F(L, {dj}jL=+01, s) and
G({pj, tj, βj,Hj}j∈[q]) be the family of sparse ReLU networks and the set of composition of Holder
smooth functions defined on S, respectively. Similar to (4.1), we define F1 by
Fi = {f: S ×A→ R: f (∙, a, b) ∈ F (L, {dj }L+1, S) for any (a, b) ∈ A× B}.	(E.8)
For the Bellman operator T defined in (E.5), we assume that for any f ∈ F1 and any state-action
tuple (s, a, b), we have (Tf )(∙, a, b) ∈ G({pj, tj, βj ,Hj }j∈[q]).
We remark that this Assumption is in the same flavor as Assumption 4.2. As discussed in §4, this
assumption holds if both the reward function and the transition density of the Markov game are
sufficiently smooth.
In the following, we define the concentration coefficients for Markov games.
Assumption E.3 (Concentration Coefficient for Zero-Sum Markov Games). Let {τt : S → P(A ×
B)} be a sequence of joint policies for the two players in the zero-sum Markov game. Let ν1, ν2 ∈
P(S × A × B) be two absolutely continuous probability measures. Suppose the initial state-
action pair (S0 , A0, B0 ) has distribution ν1, the future states are sampled according to the Markov
transition kernel, and the action (At, Bt) is sampled from policy τt. For any integer m, we denote by
PTmPτm-1 •…Pτ1 νι the distribution of (Sm, Am, Bm). Then, the m-th concentration coefficient is
defined as
κ(m; ν1, ν2) = sup	Eν2
τ1 ,...,τm
dν2
d(pTmPτm-1 …Pτ1 Vi)
1/2
(E.9)
where the supremum is taken over all possible joint policy sequences {τt}t∈[m] .
Furthermore, for some μ ∈ P (S ×A×B),we assume that there exists a finite constant φμ σ SUCh that
(1-Y)2 ∙ Pm≥i Ym-1 ∙ m ∙ κ(m; μ, σ) ≤ φμ,σ, where σ is the sampling distribution in Algorithm 4
and κ(m; μ, σ) is the m-th concentration coefficient defined in (E.9).
We remark that the definition of the m-th concentration coefficient is the same as in (4.4) ifwe replace
the action space A of the MDP by A × B of the Markov game. Thus, Assumptions 4.3 and E.3 are of
the same nature, which are standard in the literature.
Now we are ready to present the main theorem.
Theorem E.4. Under Assumptions E.2 and E.3, consider the Minimax-FQI algorithm with
the function class F being F1 defined in (E.8) based on the family of sparse ReLU net-
works F(L*, {dj}jL=+1,s*). We make the same assumptions on F(L*, {dj}jL=+1,s*) and
G({pj, tj, βj, Hj}j∈[q] ) as in (4.6) and (4.7). Then for any K ∈ N, let (πK, νK) be the policy
returned by the algorithm and let QK be the action-value function corresponding to (∏κ, VnK). Then
there exists constants ξ* and C such that
kQ* - QK k1,μ ≤ C J ∙ 1A1 ∙ |B1 ∙ (lOg nA" ∙ "T"2 + ； ∙ Rmax,	(E.10)
where α* = maxj-∈[q] tj/(2β* + tj) and φμ,σ is specified in Assumption E.3.
Similar to Theorem 4.4, the bound in (E.10) shows that closeness between (πK, νK) returned by
Algorithm 4 and the Nash equilibrium policy (∏q* , vq* ), measured by ∣∣Q* - QK∣∣ι,μ, is bounded
by the sum of statistical error and an algorithmic error. Specifically, the statistical error balances the
bias and variance of estimating the value functions using the family of deep ReLU neural networks,
which exhibits the fundamental difficulty of the problem. Whereas the algorithmic error decay to
zero geometrically as K increases. Thus, when K is sufficiently large, both Y and φμ,σ are constants,
and the polylogarithmic term is ignored, Algorithm 4 achieves error rate
∣A∣∙∣B∣∙ nα*-i = ∣A∣∙∣B∣∙ maxn-βj/(2βj+tj),	(E.11)
j∈[q]
26
Under review as a conference paper at ICLR 2020
which scales linearly with the capacity of joint action space. Besides, if |B| = 1, The Minimax-FQI
algorithm reduces to Algorithm 1. In this case, (E.11) also recovers the error rate of Algorithm 1.
Furthermore, the statistical rate n(α -1)/2 achieves the optimal '2-norm error of regression for
nonparametric regression with a compositional structure, which indicates that the statistical error in
(E.10) can not be further improved.
Proof. See §G for a detailed proof.	□
F	Proofs of Auxiliary Results
In this section, we present the proofs for Theorems D.1 and D.2, which are used in the §D to establish
our main theorem.
F.1 Proof of Theorem D.1
Proof. Before we present the proof, we introduce some notation. For any k ∈ {0, . . . , K - 1}, we
F	, m Z-∖	1	√^ι	FFC
denote T Qk by Qk+1 and define
%k = Qk - Qk .	(F.1)
Also, we denote by πk the greedy policy with respect to Qk . In addition, throughout the proof, for
two functions Q1, Q2 : S × A → R, we use the notation Q1 ≥ Q2 if Q1(s, a) ≥ Q2(s, a) for any
s ∈ S and any a ∈ A, and define Q1 ≤ Q2 similarly. Furthermore, for any policy π, recall that in
(2.4) we define the operator Pπ by
(Pπ Q)(s, a) = E[Q(S 0, A0) I S0 〜P(∙∣ s,a),A0 〜∏(∙ | S0)].	(F.2)
In addition, we define the operator Tπ by
(TπQ)(S, a) = r(s, a) + Y ∙ (PnQ)(S, a).
Finally, we denote Rmax/(1 - γ) by Vmax. Now we are ready to present the proof, which consists of
three key steps.
Step (i): In the first step, We establish a recursion that relates Q* - Qk+1 With Q* - Qk to measure
the sub-optimality of the value function Qk. In the following, we first establish an upper bound for
Q* - Qk+1 as folloWs. For each k ∈ {0, . . . , K - 1}, by the definition of %k+1 in (F.1), We have
Q - Qk+1 = Q - (Qk+1 - %k+1) = Q - Qk+1 + %k+1 = Q - TQk + %k+1
*	*
= Q - T Qek + (T Qek - TQek) + %k+1,	(F.3)
Where π* is the greedy policy With respect to Q*. NoW We leverage the folloWing lemma to shoW
*
Tπ Qe k ≤ T Qek .
Lemma F.1. For any action-value function Q : S × A → R and any policy π, it holds that
TπQQ = TQ ≥ TπQ.
Proof. Note that We have maxa0 Q(S0, a0) ≥ Q(S0, a0) for any S0 ∈ S and a0 ∈ A. Thus, it holds that
(TQ)(S,a) = r(s,a) + Y ∙ E [max Q(S0, a0) ∣ S0 〜P(∙ | s,a)]
≥ r(s, a)+ γ ∙ E[Q(S0, A0) I S0 〜P (∙ | s, a), A0 〜∏(∙ | S0)] = (Tπ Q)(s, a).
Recall that πQ is the greedy policy With respect to Q such that
P [A ∈ argmax Q(s, a) ∣ A 〜∏q(∙∣ s)] = 1,
a
27
Under review as a conference paper at ICLR 2020
which implies
E[Q(s0,A0)∣A0 〜∏q(∙∣ s0)] = max Q(s0, a0).
a
Consequently, we have
(TπQQ)Ga) = r(s,a) + Y ∙ E[Q(S0, AO)I S0 〜P(∙ | s, a),A0 〜πQ(∙ | SO)]
=r(s, a) + γ ∙ E[max Q(S0, α0) ∣ S0 〜P(∙ | s, a)] = (TQ)(s, a),
which concludes the proof of Lemma F.1.	□
*K	ι	*
By Lemma F.1, We have TQk ≥ Tπ Qk. Also note that Q* is the unique fixed point of Tπ . Thus,
by (F.3) we have
**	*	**
Q* -	Qek+1	= (Tπ	Q*	-Tπ	Qek)+(Tπ	Qek -TQek)+%k+1	≤ (Tπ	Q*	-Tπ	Qek)	+ %k+1,
(F.4)
In the following, we establish a lower bound for Q* - Qek+1 based on Qe* - Qek. Note that, by Lemma
F.1, we have TπkQek = TQek and TQ* ≥ TπkQ*. Similar to (F.3), since Q* is the unique fixed point
of T , it holds that
Q	- Qek+1 = Q -	TQek	+ %k+1 = Q - T	kQek	+ %k+1 = Q - T kQ + (T	kQ	- T	k Qek)	+ %k+1
= (TQ* -TπkQ*)+(TπkQ* -TπkQek)+%k+1 ≥ (TπkQ*-TπkQek)+%k+1.
(F.5)
Thus, combining (F.4) and (F.5) we obtain that, for any k ∈ {0, . . . , K - 1},
**
T kQ - T k Qek + %k+1 ≤ Q - Qek+1 ≤ T Q - T Qek + %k+1.	(F.6)
The inequalities in (F.6) show that the error Q* - Qk+1 can be sandwiched by the summation of a
term involving Q* - Qek and the error %k+1, which is defined in (F.1) and induced by approximating
the action-value function. Using Pπ defined in (F.2), we can write (F.6) in a more compact form,
*
γ ∙ P (Q - Qek) + %k+1 ≥ Q - Qek+1 ≥ γ ∙ P k (Q - Qek) + %k+1 .	(F.7)
Meanwhile, note that Pπ defined in (F.2) is a linear operator. In fact, Pπ is the Markov transition
operator for the Markov chain on S × A with transition dynamics
St+1 〜P (∙∣ St,At),	At+1 〜∏(∙∣ St+ι).
By the linearity of the operator Pπ and the one-step error bound in (F.6), we have the following
characterization of the multi-step error.
Lemma F.2 (Error Propagation). For any k, ` ∈ {0, 1, . . . , K - 1} with k < `, we have
`-1
Q* - Q' ≤ X Yj- ∙(Pπ* )'-1-i%i+ι + γ '-k ∙(Pπ* 尸(Q* - Qk),	(F.8)
i=k
`-1
Q* - Q' ≥ X Ye-I ∙(Pπ'-1 P π'-2 ∙∙∙ P πi+1 )%i+ι + Yei ∙(Pπ'-1 P π'-2 ∙∙∙P πk )(Q* - Qk).
i=k
(F.9)
Here %i+1 is defined in (F.1) and we use Pπ Pπ0 and (Pπ )k to denote the composition of operators.
Proof. Note that Pπ is a linear operator for any policy π. We obtain (F.8) and (F.9) by iteratively
applying the inequalities in (F.7).	□
28
Under review as a conference paper at ICLR 2020
Lemma F.2 gives the upper and lower bounds for the propagation of error through multiple iterations
of Algorithm 1, which concludes the first step of our proof.
Step (ii): The results in the first step only concern the propagation of error Q* - Qk. In contrast, the
output of Algorithm 1 is the greedy policy πk with respect to Qek. In the second step, our goal is to
quantify the suboptimality of Qπk, which is the action-value function corresponding to πk. In the
following, we establish an upper bound for Q* - Qπk .
To begin with, We have Q* ≥ Qnk by the definition of Q* in (2.5). Note that We have Q* = Tπ* Q*
and Qπk = TπkQπk. Hence, it holds that
*	*	*	*
Q* -Qπk =Tπ Q* -TπkQπk =Tπ Q* + (-Tπ Qek +Tπ Qek) +(-TπkQek +TπkQek) -TπkQπk
*	**
= (Tπ	Qek	-TπkQek)	+(Tπ	Q* -Tπ	Qek)	+ (TπkQek	-TπkQπk).	(F.10)
NoW We quantify the three terms on the right-hand side of (F.10) respectively. First, by Lemma F.1,
We have
**
T	Qek	- T	k Qek	= T	Qek	- TQek	≤	0.	(F.11)
MeanWhile, by the definition of the operator Pπ in (F.2), We have
**	*
Tπ	Q* - Tπ Qk = Y ∙ Pπ	(Q* -	Qk),	TπkQk	-	TπkQnk	= Y ∙ Pπk(Qk -	Qnk).	(F.12)
Plugging (F.11) and (F.12) into (F.10), We obtain
*
Q* - Qπk ≤ γ ∙ Pπ (Q* - Qk) + γ ∙ Pπk (Qk- Qnk)
*
=Y ∙ (Pπ - Pnk)(Q* - Qk) + Y ∙P7τk(Q* - Qπk),
Which further implies that
*
(I - γ ∙ Pπk)(Q* - Qπk) ≤ γ ∙(Pπ - Pnk)(Q* - Qk).
Here I is the identity operator. Since Tπ is a γ-contractive operator for any policy ∏, I - Y ∙ Pπ is
invertible. Thus, We obtain
0 ≤ Q* - Qπk ≤ Y ∙ (I - Y ∙ Pπk)-1 [Pπ* (Q* - Qk)- Pπk (Q* - Qk)],	(F.13)
Which relates Q* - Qπk With Q* - Qek. In the folloWing, We plug Lemma F.2 into (F.13) to obtain the
multiple-step error bounds for Qπk. First note that, by the definition of Pπ in (F.2), for any functions
f1 , f2 : S × A → R satisfying f1 ≥ f2, We have Pπ f1 ≥ Pπ f2 . Combining this inequality With the
upper bound in (F.8) and the loWer bound in (F.9), We have that, for any k < `,
`-1
Pπ* (Q* - Q') ≤ X Y'-1-i ∙ (Pπ*)'-i%i+ι + Y'-k ∙ (Pπ* )'-k+1(Q* - Qk),
i=k
`-1
Pπ'(Q* - Q') ≥ X Y'-1-i ∙ (Pπ'Pπ'-1 …Pπi+1 )%i+ι
i=k
+ Y'-k ∙ (Pπ'Pπ'-1 ∙∙∙Pπk)(Q* - Qk).
Then We plug (F.14) and (F.15) into (F.13) and obtain
`-1	*
0 ≤ Q* - Qπ' ≤ (I - Y ∙ Pπ')-1{ EYJ ∙ [(Pπ*)'-i - (Pπ'Pπ'-1 …P21 )]%i+ι
i=k
P πk)](Q*
(F.14)
(F.15)
+ ['+I —k ∙ [(P∏ )'-k + 1 - (Pn`Pn`-i
- Qk)
(F.16)
29
Under review as a conference paper at ICLR 2020
for any k < '. To quantify the error of QπK, we set' = K and k = 0 in (F.16) to obtain
KT
0 ≤ Q* - QKK ≤ (I - YPTK)T{ E YKT
i=0
[(P∏* )K-i - (PTKP∏K-1
∙∙∙ P πi+1)] %i+1
+ YK+1
[(Pπ* )k+1 - (PTKPTKT …Pπ0 )](Q
-Q>o)).
(F.17)
*
For notational simplicity, we define
Qi
(1 - Y)YKTT
1 - yK+1
for 0 ≤ i ≤ K - 1,
and QK
(1 - Y)YK
1 - yk+i
(F.18)
One can show that PK=O Qi = 1. Meanwhile, we define K + 1 linear operators {Ok}K=0 by
Oi = (1 - y)/2 ∙ (I - yP TK )-1[(P t* )K-i + (PTK P TKT ∙∙∙ P Ti+1)], for 0 ≤ i ≤ K - 1,
OK = (1 - y)/2 ∙ (I - yP tk )-1[(P t* )k+1 + (P tk P TKT ∙∙∙ PTO)].
Using this notation, for any (s, a) ∈ S ×A,by (F.17) we have
∣Q*(s,α) - Qtk(s,α)∣
≤，:1 - Y)2^^) ∙ [^X Qi ∙ (Oi|%i+1|)(s,a)+ qK ∙ (OK |Q* - QOI)(S,a) ,	(F.19)
where both Oi∣%i+11 and OK IQ* - Qo| are functions defined on S ×A. Here (F.19) gives a uniform
upper bound for Q* - QTK, which concludes the second step.
Step (iii): In this step, we conclude the proof by establishing an upper bound for ∣∣Q* - Qtk ∣∣1,μ
based on (F.19). Here μ ∈ P (S ×A) is a fixed probability distribution. To simplify the notation, for
any measurable function f : S ×A → R,we denote μ(f) to be the expectation of f under μ, that is,
μ(f) = JSXA f (s, α)dμ(s, a). Using this notation, by (F.19) we bound ∣∣Q* - QTgIlι,μ by
∣Q*- Qtkkι,“ = μ(∣Q* - QtkI)
≤ τ(1 ^Y)2^^[ ∙ μ [^X Qi ∙ (O/Oi+lD + qK ∙ (OK |Q* - QOI) .	(F.2O)
By the linearity of expectation, (F.20) implies
||Q* - QTK ∣∣1,μ ≤，(1 - Y)2^^ɪ ∙ ]^X Qi ∙ M(Oi|0i+1|) + qK ∙ μ(OK |Q* - QOI) . (F.21)
Furthermore, since both Q* and Qo are bounded by VmaX = RmaX/(1 - Y) in '∞-norm, we have
μ(OKIQ* - Qo|) ≤ 2 ∙ Rmax∕(1 - Y).	(F.22)
Moreover, for any i ∈ {0,...,K - 1}, by expanding (1 - γPTK )-1 into a infinite series, we have
μ(OiI%i+ιI) = μ{亍∙ (1 - γP tk )-1[(P t* )K-i + (P tk P tk-1 ∙∙∙ PTi+1)] |%i+1|}
=F ∙μ{XYj ∙ [(Ptk)j(Pt*)K-i + (Ptk)j+1(Ptk-1 ∙∙∙PTi+1 )]I%i+ιI}.
(F.23)
To upper bound the right-hand side of (F.23), we consider the following quantity
μ [(P TK )j(P Tm P Tm-I
∙∙ Pτ1 )f ] = I
SxA
[(P tk )j (P Tm P Tm-I ∙∙∙ PT1 )f] (s,α)dμ(s,α). (F.24)
30
Under review as a conference paper at ICLR 2020
Here τ1, . . . , τm are m policies. Recall that Pπ is the transition operator of a Markov process defined
on S × A for any policy π. Then the integral on the right-hand side of (F.24) corresponds to the
expectation of the function f(Xt), where {Xt}t≥0 is a Markov process defined on S × A. Such a
Markov process has initial distribution X。〜μ. The first m transition operators are {Pτj}j∈[m],
followed by j identical transition operators PπK. Hence, (PπK)j(PTmPTmT …Pτ1 )μ is the
marginal distribution of Xj+m, which We denote by μj for notational simplicity. Hence, (F.24) takes
the form
μ[(P πK )j (PFPTmT …PT )f ] = E[f (Xj+m)] = ej (f ) = ( J (s,a)dej (s,a)	(F.25)
for any measurable function f on S × A. By Cauchy-Schwarz inequality, we have
ej (f)
I ∖dσ (s,a)jdσ(s,a)
1/2
JS ∕f(s,a)∣2dσ(s, a)
(F.26)
in which dej-/dσ: S ×A→ R is the Radon-Nikodym derivative. Recall that the (m + j)-th order
concentration coefficient κ(m + j; μ, σ) is defined in (4.4). Combining (F.25) and (F.26), we obtain
ej (f) ≤ κ(m + j; μ,σ) ∙ kf kσ .
Thus, by (F.23) we have
1-Y ∞	*
μG∣%i+ι∣) = Y ∙ XYj ∙ {μ[(PπK)j(Pπ )K-i∣%i+ι∣] + μ[(PπK)j+1(PπK-1 …Pπi+1 )∣%i+ι∣]}
2	j=0
∞
≤ (I- γ) ∙ XYj ∙K(K - i + j;μ,σ) ∙ ll%i+ιkσ.	(F.27)
j=0
Now we combine (F.21), (F.22), and (F.27) to obtain
kQ* - QnK kl,μ ≤，(] - Y)?~~) ∙ ]^X αi ∙ μ(Oi∣%i+1∣) + αK ∙ μ(OK ∣Q* - Q0∣)
≤ 2γ(1- γκ+1)
—(1 - Y)
K-1 ∞
ΣΣαi ∙ γj ∙ κ(K - i + j; μ, σ) ∙ k%i+ιkσ
4γ(1 - YK+1)
+ (i-γ)3
• αK ∙ Rmax.
Recall that in Theorem D.1 and (F.1) we define εmaχ = maxi∈[κ] k%ikσ. We have that ∣∣Q* -
QπKkl,μ is further upper bounded by
kQ*- QnK kl,μ
(F.28)
≤ 2y(1- yk+1)
一(1 - y)
K-1 ∞
•	ai . Yj . κ(K - i + j; μ, σ) .
2y(1- Yk+1)
(I-Y)
i=0 j=0
R X (1-Y)YK-i-1
.匕 ⅛	1 - YK+1
4Y(1 - YK+1)
εmax H---7∖---73--- • αK • Rmax
(1 - Y)3
j	4YK+1
•	Y • K(K - i + j; μ, σ) • εmax + γz-Γ2 • Rmax,
(1 - Y)2
where the last equality follows from the definition of {αi}0≤i≤K in (F.18). We simplify the summa-
tion on the right-hand side of (F.28) and use Assumption 4.3 to obtain
XX X (1 - Y)YK-i-1	j
XX 1 - γκ+ι	. γj . K(K - i+j； μ,σ)
1 - Y
1 - γκ+1
∞ K-1
XX
γK-i+j-1 . κ(K -i+ j;μ,σ)
≤ τ⅛
j=0 i=0
∞
^X Ym-1 . m . κ(m; μ, σ) ≤
m=0
(1 - yk+1)(1 - y),
(F.29)
31
Under review as a conference paper at ICLR 2020
where the last inequality follows from (4.5) in Assumption 4.3. Finally, combining (F.28) and (F.29),
we obtain
kQ*- QnK kl,μ≤
2γ ∙ φμ,σ
(1-γ)2
• εmax +
4γK+1
(1 - γ)2
. Rmax,
which concludes the third step and hence the proof of Theorem D.1.
□
F.2 Proof of Theorem D.2
Proof. Recall that in Algorithm 1 we define Yi = Ri + γ • maxa∈A Q(Si+1, a), where Q is any
function in F. By definition, we have E(Yi | Si = s, Ai = a) = (T Q)(s, a) for any (s, a) ∈ S × A.
Thus, T Q can be viewed as the underlying truth of the regression problem defined in (D.2), where
the covariates and responses are {(Si, Ai)}i∈[n] and {Yi}i∈[n], respectively. Moreover, note that TQ
is not necessarily in function class F. We denote by Q* the best approximation of T Q in F, which is
the solution to
minimizekf-TQk2σ =E f(Si,Ai) -Q(Si,Ai)2 .
(F.30)
For notational simplicity, in the sequel we denote (Si, Ai) by Xi for all i ∈ [n]. For any f ∈ F, we
define kf k2n = 1/n • Pin=1 [f (Xi)]2. Since both Qb andTQ are bounded by Vmax = Rmax/(1 - γ),
we only need to consider the case where log Nδ ≤ n. Here Nδ is the cardinality ofN(δ, F, k • k∞).
Moreover, let f1 , . . . , fNδ be the centers of the minimal δ-covering of F . Then by the definition of
δ-coverιng, there exists k* ∈ [Nδ] such that ∣∣Q 一 fk* k∞ ≤ δ. It Is worth mentioning that k* Is a
random variable since Q is obtained from data.
In the following, we prove (D.3) in two steps, which are bridged by E[kQ - TQk2n].
Step (i): We relate E[kQb - TQk2n] with its empirical counterpart kQb - TQk2n. Recall that we define
Yi = Ri + γ • maxa∈A Q(Si+1, a) for each i ∈ [n]. By the definition of Qb, for any f ∈ F we have
Xn Yi-Qb(Xi)2 ≤Xn Yi-f(Xi)2.	(F.31)
i=1	i=1
For each i ∈ [n], we define ξi = Yi - (T Q)(Xi). Then (F.31) can be written as
2n
∣Q - TQkn ≤ ∣f 一 TQkn+-∑ξi . [Q(Xi) - f (Xi)].	(F.32)
n i=1
Since both f and Q are deterministic, we have E(kf 一 TQk2n) = kf 一 TQk2σ. Moreover, since
E(ξi | Xi) = 0 by definition, we have E[ξi • g(Xi)] = 0 for any bounded and measurable function g.
Thus, it holds that
e{XXξi . [Q(Xi) - f(Xi)]} = e{XXξi . [Q(Xi) - (TQ)(Xi)] }.	(F.33)
In addition, by triangle inequality and (F.33), we have
(F.34)
where fk* satisfies kfk* 一 Qk∞ ≤ δ. In the following, we upper bound the two terms on the
right-hand side of (F.34) respectively. For the first term, by applying Cauchy-Schwarz inequality
32
Under review as a conference paper at ICLR 2020
twice, we have
∣E∣XXξi ∙ [Q(χi) -fk*(Xi)]j∣ ≤√n∙ e](Xξ2) / ∙kQ-fk*kn
≤√n ∙ E(X ξ2) / ∙ [E(kQ-fk*kn)]"2≤ nδ ∙ [E(ξ2)]1/2,	(F.35)
where We use the fact that {ξi}i∈[n] have the same marginal distributions and ∣∣Q - fk* kn ≤ δ. Since
both Yi and T Q are bounded by Vmax, ξi is a bounded random variable by its definition. Thus, there
exists a constant Cξ > 0 depending on ξ such that E(ξ2) ≤ Cξ ∙ Vmmaχ. Then (F.35) implies
∣E∣XX ξi ∙ [Q(Xi) - fk* (Xi)] j| ≤ Cξ ∙ /ax ∙ nδ.	(F.36)
It remains to upper bound the second term on the right-hand side of (F.34). We first define Nδ
self-normalized random variables
1n
Zj = -√n∑ξi	∙	[fj(Xi)-(TQ)(Xi)]	∙kfj	-	(TQ)k-1	(F.37)
i=1
for all j ∈ [Nδ]. Here recall that {fj}j∈[Nδ] are the centers of the minimal δ-covering of F. Then we
have
∣e{XXξi ∙ [fk*(Xi)-(TQ)(Xi)]}∣ = √n∙E[kfk* -TQkn ∙ |Zk*|]
≤ √n ∙ E{[k Q - TQkn + IlQ - fk* kn] ∙∣Zk* I O ≤ √n ∙ E{[k Q - TQkn + δ]，|Zk* ∣},
(F.38)
where the first inequality follows from triangle inequality and the second inequality follows from
the fact that kQ - fk* k∞ ≤ δ. Then applying Cauchy-Schwarz inequality to the last term on the
right-hand side of (F.38), we obtain
E{[kQ - TQkn + δ] ∙∣Zk*∣} ≤ (e{[∣∣Q - TQkn + δ]2})i"∙ [E(Zk*)] 1/2
≤ ({e[∣q - TQkn]01/2+°) ∙ [e(max Zj)i1/2，	(f.39)
where the last inequality follows from
E[kQb-TQkn] ≤ nE[kQb-TQk2n]o1/2.
Moreover, since ξi is centered conditioning on {Xi}i∈[n] and is bounded by 2Vmax, ξi is a sub-
Gaussian random variable. In specific, there exists an absolute constant Hξ > 0 such that kξi kψ2 ≤
Hξ ∙ VmaX for each i ∈ [n]. Here the ψ2-norm of a random variable W ∈ R is defined as
kWkψ2 = supp-1/2 [E(IWIp)]1/p.
p≥1
By the definition of Zj in (F.37), conditioning on {Xi}i∈[n], ξi ∙ [fj(Xi) - (TQ)(Xi)] is a centered
and sub-Gaussian random variable with
临.[fj(Xi) - (TQ)(Xi)] ∣∣ψ2 ≤ Hξ ∙ /ax ∙ ∣fj(Xi) - (TQ)(Xi)I.
Moreover, since Zj is a summation of independent sub-Gaussian random variables, by Lemma 5.9 of
?, the ψ2-norm of Zj satisfies
1n
kZjkψ2 ≤ C ∙ Hξ ∙ /ax ∙ kfj - TQk-1 ∙ n ∑∣[fj-(Xi) - (TQ)(Xi)]I2
i=1
1/2
≤ C ∙ Hξ ∙ Vmax,
33
Under review as a conference paper at ICLR 2020
where C > 0 is an absolute constant. Furthermore, by Lemmas 5.14 and 5.15 of ?, Zj2 is a
sub-exponential random variable, and its the moment-generating function is bounded by
E[exp(t ∙ Zj)] ≤ exp(C ∙ t2 ∙ Hξ ∙ Vmax)	(F∙40)
for any t satisfying C0 ∙∣t∣∙ Hξ ∙ Vmax ≤ 1, where C and C0 are two positive absolute constants.
Moreover, by Jensen’s inequality, we bound the moment-generating function of maxj∈[Nδ] Zj2 by
E kxp(t ∙ max Z])] ≤ ^X E[exp(t ∙ Zj)^].
j∈[Nδ]	j∈[Nδ]
Combining (F.40) and (F.41), we have
E(j∈ax]Zj) ≤ C2 ∙ Hξ ∙ Vmax ∙ log Nδ,
(F.41)
(F.42)
where C > 0 is an absolute constant. Hence, plugging (F.42) into (F.38) and (F.39), we upper bound
the second term of the right-hand side of (F.33) by
ξi∙ fk*(Xi)-(TQ)(Xi)]
≤ ({E[kQ - TQkn] 01/2 + δ) C Hξ ∙ /ax ∙ Pn ∙log Nδ.
Finally, combining (F.32), (F.36) and (F.43), we obtain the following inequality
E[kQ -TQkn ≤ 叫E[kf - TQkn] + Cξ ∙ /ax ∙ δ
f ∈F
+ ({E[kQ -(TQ)kn]}1/2 + δ) ∙ C ∙ Hξ ∙ /ax ∙ PlogNδ/n
(F.43)
(F.44)
_______ 1 ʌ	`ɪ 1/2
≤ C ∙ /axPlogNδ/n ∙ {E[kQ - (TQ)kn] }	+ 黑E[kf - TQkn] + C0 ∙ /axδ,
where C and C0 are two positive absolute constants. Here in the first inequality we take the infimum
over F because (F.31) holds for anyf ∈ F, and the second inequality holds because log Nδ ≤ n.
Now we invoke a simple fact to obtain the final bound for E[kQb-TQk2n] from (F.44). Let a, b, and cbe
positive numbers satisfying a2 ≤ 2ab+c. For any e ∈ (0,1], since 2ab ≤ e∙ a2∕(1 + e) + (1 + e)∙b2∕e,
we have
a2 ≤ (1 + e)2 ∙ b2/e +(1 + e) ∙ c.	(F.45)
Therefore, applying (F.45) to (F.44) with a2 = E[kQ - TQkn], b = C ∙ Vmax ∙ PlogNδ/n, and
C = inff∈fE[∣f - TQkn] + C0 ∙ Vmax ∙ δ, we obtain
E[kQ - TQkn ≤ (1 + e) ∙ inf E[kf - TQM] + C ∙ Vmax ∙ log N/(ne) + C0 ∙ Vmax ∙ δ,
f ∈F
(F.46)
where C and C0 are two positive absolute constants. Now we conclude the first step.
Step (ii). In this step, we relate the population risk kQb - T Qk2σ with E[kQb - T Qk2n], which
is characterized in the first step. To begin with, we generate n i.i.d. random variables {Xi =
(Si, Ai)}i∈[n] following σ, which are independent of {(Si, Ai, Ri, Si)}i∈[n]. Since ∣∣Q - fk* ∣∣∞ ≤ δ,
for any x ∈ S × A, we have
[Qb(x) - (T Q)(x)]2 - [fk*(x) - (T Q)(x)]2
= IQ(X)- fk* (x) HQ(x) + fk* (x) - 2(TQ)(x) I ≤ 4%ax ∙ δ, (F.47)
34
Under review as a conference paper at ICLR 2020
where the last inquality follows from the fact that kT Qk∞ ≤ Vmax and kfk∞ ≤ Vmax for any f ∈ F.
Then by the definition of kQb - T Qk2σ and (F.47), we have
1n
kQ - TQkσ = El - = [Q(Xi)-(TQ)(Xi)]2
-n
≤ E kQ - TQkn + — ∑[fk* (Xi) - (TQ)(Xi)]2
n i=1
—
+ 8Vmax ∙ δ
-n
E(kQ - TQkn) + E n fhk*(Xi,Xi) +8Vmax ∙ δ,	(F.48)
i=1
where we apply (F.47) to obtain the first inequality, and in the last equality we define
hj(x,y)= [fj(y)-(TQ)(y)]2- [fj(x)-(TQ)(x)]2,	(F.49)
for any (χ,y) ∈ S × A and any j ∈ [Nδ]. Note that hk* is a random function since k is ran-
dom. By the definition of hj in (F.49), we have |hj (x, y)| ≤ 4Vm2ax for any (x, y) ∈ S × A and
E[hj(Xi, Xi)] = 0 for any i ∈ [n]. Moreover, the variance of hj(Xi, Xi) is upper bounded by
Var[hj(Xi,Xei)] =2Var [fj(Xi) -(TQ)(Xi)]2
≤ 2E{[fj(Xi)-(TQ)(Xi)]4} ≤ 8Υ2 ∙ Vmax,
where we define Υ by letting
Υ2 = max(4Vmaχ ∙ log N/n,器箫 e{ [fj(Xi)-(TQ)(Xi)]1)
Furthermore, we define
n
T = sup X hj(Xi,Xi)∕Υ .
j ∈[Nδ] i=1
Combining (F.48) and (F.51), we obtain
kQ - TQkσ ≤ E[kQ - TQkn + Υ∕n ∙ E(T) + 8Vmax ∙ δ.
(F.50)
(F.51)
(F.52)
In the sequel, we utilize Bernstein’s inequality to establish an upper bound for E(T), which is stated
as follows for completeness.
Lemma F.3 (Bernstein’s Inequality). Let U1, . . . Un be n independent random variables satisfying
E(Ui) = 0 and |Ui| ≤ M for all i ∈ [n]. Then for any t > 0, we have
n
P	Ui ≥ t
i=1
where σ2 = Pin=1 Var(Ui) is the variance of Pin=1 Ui.
)≤ 2eXP(2M .；»，
We first apply Bernstein’s inequality by setting Ui = hj(Xi, Xi)∕Υ for each i ∈ [n]. Then we take a
union bound for all j ∈ [Nδ] to obtain
- n
P(T ≥ t) = P sup - Vhj(Xi,Xi)∕Υ ≥ t
j∈[Nδ] n i=1
≤ 2Nδ ∙ exP{ 8*χ ∙ [t∕(3Υ)+ 司 I(F.53)
Since T is nonnegative, We have E(T) = R∞ P(T ≥ t)dt. Thus, for any U ∈ (0,3Υ ∙ n), by (F.53)
it holds that
∞
u
E(T) ≤ u +
P(T ≥ t)dt
3γ∙n	/	-t2	、
≤ U + 2Nδ∕	exp(l6VG)
∞	/-3Υ ∙八
dt + 2n"y∕xp(e)
dt
≤ U + 32Nδ ∙ Vmax ∙ n/u ∙ exp (i6V2u ∙n ) + 32Nδ ∙ Vmax/(3Y) ∙ exp ( 16V 2 ∙ n ),
(F.54)
35
Under review as a conference paper at ICLR 2020
where in the second inequality We use the fact that J∞ exp(-t2∕2)dt ≤ 1/s ∙ exp(-s2/2) for all
s > 0. Now we set U = 4Vmaχ√n ∙ log N in (F.54) and plug in the definition of Y in (F.50) to
obtain
E(T) ≤ 4 Vmaxpn ∙ log Nδ + 8Vmaxpn/log Nδ + 6 Vmaxpn/log Nδ ≤ 8 Vmaxpn ∙ log Nδ,
(F.55)
where the last inequality holds when log Nδ ≥ 4. Moreover, the definition of Υ in (F.50) implies that
Y ≤ max[2Vmax VZlog Nδ∕n, kQ - TQkσ + δ]. In the following, we only need to consider the case
where Y ≤ kQ - TQkσ + δ, since we already have (D.3) if kQ - TQkσ + δ ≤ 2Vmax v4ogNδ7n,
which concludes the proof.
Then, when Y ≤ kQb - TQkσ + δ holds, combining (F.52) and (F.55) we obtain
IlQ - TQkσ ≤ E[kQ - TQkn] + 8Vmaxplog(N)∕n ∙ kQ - TQkσ + 8Vmax，log Nδ∕n ∙ δ + 8Vmax ∙ δ
≤ e[∣∣q - TQkn] + 8Vmaxplog Nδ7n ∙ IlQ - TQkσ + 16Vmax ∙ δ∙	(F.56)
We apply the inequality in (F.45) to (F.56) with a = kQ - TQkσ, b = 8VmaxdlogNδ/n, and
C = E[kQ - TQkn + 16VmaX ∙ δ. Hence we finally obtain that
kQ-TQkσ ≤ (1 + E) ∙E[kQ-TQkn
+ (1 + e)2 ∙ 64Vmax ∙ lOg(N)/(n ∙ e) + (1+ €) ∙ 18Vmax ∙ δ,	(F.57)
which concludes the second step of the proof.
Finally, combining these two steps together, namely, (F.46) and (F.57), we conclude that
kQ - TQkσ ≤ (1 + e)2 •叫E[kf - TQkn] + Cl ∙ VmaX ∙ logNδ/(n ∙ e) + C ∙ VmaX ∙ δ,
f∈F
where C1 and C2 are two absolute constants. Moreover, since Q ∈ F, we have
fi∈nFfE[kf-TQk2n] ≤ supnfi∈nFfE[kf-TQk2n]o,
which concludes the proof of Theorem D.2.
□
G Proof of Theorem E.4
In this section, we present the proof of Theorem E.4. The proof is similar to that of Theorem 4.4,
which is presented in §D in details. In the following, we follow the proof in §D and only highlight the
differences for brevity.
Proof. The proof requires two key ingredients, namely the error propagation and the statistical error
incurred by a single step of Minimax-FQI.
Theorem G.1 (Error Propagation). Recall that {Qk}0≤k≤K are the iterates of Algorithm 4 and
(∏κ,νκ) is the equilibrium policy with respect to QK. Let QK be the action-value function
corresponding to (∏κ, VnK), where VnK is the best-response policy of the second player against ∏k.
Then under Assumption E.3, we have
kQ* - QKkι,μ ≤
2φμ,ρ ∙ γ ε + 4YK+1	R
(1 - γ)2	max + (1 - γ)2	max,
(G.1)
where we define the maximum one-step approximation error εmax = maxk∈[K] kT Qk-1 - Qkkσ,
and constant φμ,ν is specified in Assumption E.3.
Proof. We note that the proof of Theorem D.1 cannot be directly applied to prove this theorem. The
main reason is that here we also need to consider the role played by the opponent, namely player two.
Different from the MDP setting, here Q*K is a fixed point of a nonlinear operator due to the fact that
player two adopts the optimal policy against πK . Thus, we need to conduct a more refined analysis.
See §G.1 for a detailed proof.	□
36
Under review as a conference paper at ICLR 2020
By this theorem, we need to derive an upper bound of εmax . We achieve such a goal by studying the
one-step approximation error kT Qek-1 - Qek kσ for each k ∈ [K].
Theorem G.2 (One-step Approximation Error). Let F ⊆ B(S × A × B, Vmax) be a family of
measurable functions on S × A × B that are bounded by Vmax = Rmax/(1 - γ). Also, let
{(Si, Ai, Bi)}i∈[n] be n i.i.d. random variables following distribution σ ∈ P(S × A × B). . For
each i ∈ [n], let Ri and Si0 be the reward obtained by the first player and the next state following
(Si , Ai , Bi ). In addition, for any fixed Q ∈ F , we define the response variable as
Yi = Ri + Y ∙ max min Ea〜∏o,b〜“， Q(Si0,a,b) .	(G.2)
π0∈P(A) ν0∈P(B)	,	i
ɪʌ	F	r/ʌz- Λ	FC Z~?	,1	1	, ∙	,	, 1	1	,	1 1
Based on {(Xi, Ai, Yi)}i∈[n], we define Q as the solution to the least-squares problem
min1 XX[f (Si, Ai) — Yi]2.	(G.3)
f∈ n i=1
Then for any ∈ (0, 1] and any δ > 0, we have
kQ - TQkσ ≤ (1 + e)2 ∙ SUp inf kf - Tgkσ + C ∙ Vmax/(n ∙ e) ∙ log N + C0 ∙ Vmax ∙ δ, (G.4)
g∈F f∈F
where C and C0 are two positive absolute constants, T is the Bellman operator defined in (E.5), Nδ
is the cardinality of the minimal δ-covering of F with respect to '∞-norm.
Proof. By the definition ofYi in (G.2), for any (s, a, b) ∈ S × A × minν0∈P(B), we have
E(Yi | Si =s,Ai =a,Bi =b)
=r(S, a) + Y ∙ EsO〜P(∙ | s a b)	max min EaO〜π0,b0〜ν0
( 1 ,, ) π0∈P(A) ν0∈P(B)	,
[Q(s0, a, b)
(T Q)(s, a, b).
Thus, TQ can be viewed as the ground truth of the nonlinear least-squares regression problem in
(G.3). Therefore, following the same proof of Theorem D.2, We obtain the desired result. □
Now we let F be the family of ReLU Q-networks F1 defined in (E.8) and set Q = Qk-1 in Theorem
G.2. In addition, setting = 1 and δ = 1/n in (G.4), we obtain
kQk+1 - TQkkσ ≤ 4 ∙ SUp inf kf - Tgkσ + C Vmax/n ∙ log N
g∈F1 f∈F1
≤ 4 ∙ sup inf kf - f0k∞ + C ∙ Vmajn ∙ log Ni,	(G.5)
fO∈G1 f∈F1
where C is a positive absolute constant, N1 is the 1/n-covering number of F1, and function class G1
is defined as
Gi = {f ： S × A→ R: f (∙,a,b) ∈ G({Pj ,tj ,βj ,Hj }j∈[q]) for any (a, b) ∈A×B}.	(G.6)
Here the second inequality follows from Assumption E.2.
Thus, it remains to bound the '∞-error of approximating functions in Gi using ReLU Q-networks in
Fi and the 1/n-covering number ofFi. In the sequel, obtain upper bounds for these two terms.
By the definition of Gi in (G.6), for any f ∈ Gi and any (a, b) ∈ A × B, we have f (∙, a, b) ∈
G({(pj, tj, βj, Hj)}j∈[q]). Following the same construction as in §F.2, we can find a function f in
F(L*, {d*}LZ+i,s*) such that
kf(∙,a,b) - fk∞ . maxn-2βj/(2βj+tj) = nα*-i,
j∈[q]
which implies that
SUp inf kf-f0k2∞ .nα*-i.	(G.7)
fO∈G1 f∈F1
37
Under review as a conference paper at ICLR 2020
Moreover, for any f ∈ Fi and any (a, b) ∈ A × B, we have f(∙, a, b) ∈ F(L*, {dj}L=+1, s*).
Let Nδ be the δ-covering of F(L*, {d；}「+1, s*) in the '∞-norm. Then for any f ∈ Fi and any
(a, b) ∈ A×B, there exists gab ∈ N SUch that ∣∣f (∙, a, b) - ga,bk∞ ≤ δ. Thus, the cardinality of
theN(δ, Fi, ∣ ∙ ∣∣∞) satisfies
∣N(δ,Fi, k∙k)∣≤∣Nδ|1AHB1.	(G.8)
Combining (G.8) with Lemma D.4 and setting δ = 1/n, we obtain that
log Ni ≤ |A| ∙ |B| ∙ log |N | ≤ AHBI ∙ (s* + 1) JogRn ∙ (L* + 1) ∙ D2]
≤ AHBI ∙ s* ∙ L* m筋 log(d*) . AHBI ∙ nα* ∙ (log n)i+2ξ0,	(G.9)
where D = QL=+i(d* + 1) and the second inequality follows from (4.7).
Finally, combining (G.1), (G.5), (G.7), and (G.9), we conclude the proof of Theorem E.4.	□
G.1 Proof of Theorem G.1
Proof. The proof is similar to the that of Theorem G.1. Before presenting the proof, we first introduce
the following notation for simplicity. For any k ∈ {0, . . . , K - 1}, we denote TQk by Qk+i and
define %k = Qk - Qk. In addition, throughout the proof, for two action-value functions Qi and Q2,
we write Qi ≤ Q2 if Qi(s, a, b) ≥ Q2(s, a, b) for any (s, a, b) ∈ S × A × B, and define Qi ≥ Q2
similarly. Furthermore, we denote by (πk, νk) and (π*, ν*) the equilibrium policies with respect to
Qk by Q*, respectively. Besides, in addition to the Bellman operators Tπ,ν and T defined in (E.4)
and (E.5), for any policy π of the first player, we define
TπQ(s, a, b)
r(s, a, b) + Y ∙ Es0〜P(∙ | s a,b)	min Ea0〜∏,b0〜ν0 [Q(s , a , b )] r,
ν0∈P(B)
(G.10)
corresponds to the case where the first player follows policy π and player 2 adopts the best policy
in response to π. By this definition, it holds that Q* = Tπ* Q*. Unlike the MDP setting, here Tπ
is a nonlinear operator due to the minimization in (G.10). Furthermore, for any fixed action-value
function Q, we define the best-response policy against π with respect to Q, denote by ν(π, Q), as
ν(π,Q)(∙ I S) = argminEa〜∏,b〜v，[Q(s,a,b)].	(G.11)
ν0∈P(B)
Using this notation, we can write (G.10) equivalently as
TπQ(s,a,b) = r(s,a, b) + Y ∙ (Pπ,v(π,Q))(s,a,b).
Notice that P π,ν(π,Q) is a linear operator and that νQ = ν(πQ, Q) by definition.
Now we are ready to present the proof, which can be decomposed into three key steps.
Step (i): In the first step, we establish recursive upper and lower bounds for {Q* - Qk}0≤k≤K. For
each k ∈ {0, . . . , K - 1}, similar to the decomposition in (F.3), we have
**
Q -	Qek+i	= Q - T	Qek	+ (T	Qek	-	T Qek)	+ %k+i,	(G.12)
where π* is part of the equilibrium policy with respect to Q* and Tπ* is defined in (G.10).
*
Similar to Lemma F.1, we utilize the following lemma to show Tπ Qk ≥ TQk.
Lemma G.3. For any action-value function Q : S × A × B → R, let (πQ, νQ) be the equilibrium
policy with respect to Q. Then for and any policy π of the first player, it holds that
TπQQ = TQ ≥ TπQ.
Furthermore, for any policy π : S → P(A) of player one and any action-value function Q, we have
T π,ν(π,Q)Q = TπQ ≤ Tπ,νQ	(G.13)
for any policy ν: S → P(B), where ν(π, Q) is the best-response policy defined in (G.11).
38
Under review as a conference paper at ICLR 2020
Proof. Note that for any s0 ∈ S, by the definition of equilibrium policy, we have
max min
π0∈P(A) ν0 ∈P(B)
Ea0 〜π0,b0 〜ν0
min Ea，〜∏q,b，〜V' [Q(s0, a', b0)].
ν0∈P(B)	Q, L
Thus, for any state-action tuple (s, a, b), taking conditional expectations of s with respect to
P(∙ | s,α,b) on both ends of this equation, We have
(TInQQ)(s, α, b) = r(s, a, b) + Y ∙ Es，〜P(∙∣ §,0,.){“0理?万)E。，〜∏Q,b'〜ν' [Q(s', a0, b0)] }
=r(s, a, b) + Y ∙ Es0〜P(∙∣ s,a,b)	max min Ea0〜π0,b0〜ν0 Q(s0, a0, b0)	= (T Q)(s, a, b),
π0∈P(A) ν0∈P(B)
Which proves TπQQ = TQ. Moreover, for any policy π of the first player, it holds that
max min
π0∈P(A) ν0∈P(B)
Q(s0, a0, b0) ≥
min E。，〜n,b，〜v，[Q(s0, α0, b0)].
ν0∈P(B)	,
Taking expectations with respect to s0 〜P(∙ | s, a, b) on both ends, we establish TQ ≥ TπQ.
It remains to shoW the second part of Lemma G.3. By the definition of ν(π, Q), We have
Ea0〜∏,bθ"(∏,Q) [Q(s0,a0,b0)] = min E。，〜.&，〜“，[Q(s0,a0,b0)],
ν0∈P(B)
which, combined with the definition of Tπ in (G.10), implies that T π,ν(π,Q)Q = TπQ. Finally, for
any policy ν of player two, we have
,min?) E。，〜∏,b0〜v0 [Q(s , a , b )] ≥ Ea0〜π,b0〜v [Q(s , a , b )],
which yields TπQ ≤ Tπ,vQ. Thus, we conclude the proof of this lemma.	□
Hereafter, for notational simplicity, for each k, let (πk, νk) be the equilibrium joint policy with respect
to Qk, and we denote V(∏*, Qk) and ν(∏k,Q*) by Vk and Vk, respectively. Applying Lemma G.3 to
(G.12) and utilizing the fact that Q* = Tπ* Q*,we have
*	* .	_* -~■,
Q* - Qek+1 ≤ (Q* -Tπ Qek)+%k+1 = (Tπ Q* -Tπ Qek)+%k+1
**	*
≤ (Tπ ,ekQ*- Tπ ,ekQk) + %k+ι = Y ∙ Pπ ,ek (Q*- Qk) + %k+ι,	(G.14)
where the last inequality follows from (G.13). Furthermore, for a lower bound of Q* - Qk+1, similar
to (F.5), we have
~ _ _ . . — _
Q* - Qek+1 = (TQ* -TπkQ*)+(TπkQ* -TπkQek)+%k+1
______ ,	__个、	_Γ-.	,..	一、
≥ (TπkQ*	-	TπkQk)	+	%k+ι	≥ Y ∙	Pπk,"k (Q* - Qk)	+	%k+ι,	(G.15)
where the both inequalities follow from Lemma G.3. Thus, combining (G.14) and (G.15) we have
*
Y ∙ P , k (Q - Qk) + %k+1 ≥ Q - Qk+1 ≥ Y ∙ P , k (Q - Qk) + %k+1.	(G.16)
for any k ∈ {0, . . . , K - 1}. Similar to the proof of Lemma F.2 , by applying recursion to (G.16),
we obtain the following upper and lower bounds for the error propagation of Algorithm 4.
Lemma G.4 (Error Propagation). For any k, ` ∈ {0, 1, . . . , K - 1} with k < `, we have
Q*
`-1
-Qe ≤ X YeT-j ∙ (Pπ*,e'-1 Pπ*,v'-2 …Pπ*,ei+1 )%i+ι
i=k
+ y'-k ∙ (Pπ*,e'-1 Pπ*,v'-2 ∙
〜
*
∙∙Pπ ,ek)(Q* - Qk),
(G.17)
Q*
`-1
-Q' ≥ XYeT-i ∙ (Pπ'-1,v'τPπ'-2,p'-2 …Pπi+1,pi+1 )%i+1
i=k
+ Y'-k ∙ (Pπ'-1M-1 Pπ'-2M-2 -Pπi+1,vk)(Q* - Qk).
(G.18)
39
Under review as a conference paper at ICLR 2020
Proof. The desired results follows from applying the inequalities in (G.16) multiple times and the
linearity of the operator Pπ,ν for any joint policy (π, V).	□
The above lemma establishes recursive upper and lower bounds for the error terms {Q* -
Qk}0≤k≤K-1, which completes the first step of the proof.
Step (ii): In the second step, we characterize the suboptimality of the equilibrium policies constructed
by Algorithm 4. Specifically, for each ∏k, We denote by Qk the action-value function obtained when
agent one follows πk while agent two adopt the best-response policy against πk. In other words,
Qi is the fixed point of Bellman operator Tπk defined in (G.10). In the following, we obtain an
upper bound of Qi - Qk which establishes the a notion of suboptimality of policy (∏k, Vk) from the
perspective of the first player.
To begin with, for any k, we first decompose Qi - Qik by
Qi - Qk = (Tπ*Qi - Tπ*Qk) + (Tπ*Qk - TπkQk) + (TπkQk - TπkQk).	(G.19)
*	*∙ι	* ∙	∙ *	*	* *	*	mn-*
Since πk is the equilibrium policy with respect to Qk, by Lemma G.3, we have Tπ Qk ≤ Tπk Qk .
Recall that (πi, Vi) is the joint equilibrium policy with respect to Qi. The second argument of
Lemma G.3 implies that
**
Tπ Qi ≤ Tπ ,νek Qi,	Tπk Qek ≤ Tπk,νbkQek,	(G.20)
where Vek = V(πi, Qk) and we define Vbk = V(πk, Qik). Thus, combining (G.19) and (G.20) yields
that
0 ≤ Qi - Qk ≤ γ ∙ Pπ* ,ek (Qi - Qk) + γ ∙ Pπk，bk (Qk- Qk)
=γ ∙ (Pπ*Rk - Pπkbk)(Qi - Qk) + γ ∙ Pπk,bk (Qi - Qk).
(G.21)
Furthermore, since I - Y ∙ Pπk,bk is invertible, by (G.21) we have
0 ≤ Qi - Qk ≤ γ ∙ (I - γ ∙ Pπk,bk)-1 ∙ [Pπ*,ek (Qi - Qk) - Pπk,bk (Qi - Qk)].
(G.22)
Now we apply Lemma G.4 to the right-hand side of (G.22). Then for any k ≤ `, we have
`-1
Pπ*,e' (Qi - Q') ≤ X γ'-1-j ∙ (Pπ*,e'Pπ*,e'-1 ∙∙∙ Pπ*,ei+1 )%i+ι
i=k
+ γ'-k ∙ (P π*,e P π*,e'-1 ∙∙∙ P π*,ek )(Qi - Qk),
(G.23)
`-1
Pπ'νb' (Qi - Q') ≥ X γ'-1-i ∙ (Pπ'ν' Pπ'ν'-2 ... P πi+1,νi+1 )%i+1
i=k
+ γ'-k ∙ (Pπ'，"' P π'-1 ,p'-1 ∙∙∙ P πk,νk )(Qi - Qk).
(G.24)
Thus, setting ` = K and k = 0 in (G.23) and (G.24), we have
Qi - QK ≤ (I - Y ∙ PπK，VK)-1∙
(G.25)
K-1
X YK-1
i=0
* ,eκ P ∏* ,eκ-ι ... P ∏* ,Vi+ι ) - (P ∏k ,νκ P ∏k-i ,Vκ-ι ... P ∏i+ι ,Vi+ι )] %,十]
+ γK+1
* *
,VKPπ ,VK-I ... Pπ
(PπK，VKPπK-1,νK-1 ∙ ∙ ∙ pπ0,ν0)i (Qi - Q0)}.
—
To simplify the notation, we define {αi}iK=0 as in (F.18). Note that we have PiK=0 αi = 1 by
definition. Moreover, we define K + 1 linear operators {Ok}kK=0 as follows. For any i ≤ K - 1, let
O，= 1 - Y ∙ (I - γ ∙ P ∏K ,VK ) -1 h(P π*,eκ P Π*,eκ-1 ... P π*,ei+ι ) - (P ∏k ,νκ P ∏κ-ι,Vκ-ι ... P ∏i+ι,Vi+ι )]
40
Under review as a conference paper at ICLR 2020
Moreover, we define OK by
OK = 1 - Y ∙ (I - Y ∙ P ∏K ,νκ )-1 [(p ∏* ,eκ P π*,eκ-ι ... P π*,e0 ) - (P ∏K ,Vκ P ∏K-1,0K-1 ... P ∏0,V0 )]
Therefore, taking absolute values on both sides of (G.25), we obtain that
∣Q*(s,a,b) — QK (SMb)I
2γ(1- γκ+1)
≤	(1-γ)2
〜
αi ∙ (Oi∣%i+ι∣)(s,a,b) + ακ ∙ (Ok|Q* - Qo∣)(s,a,b) , (G.26)
for any (s, a, b) ∈ S × A × B, which concludes the second step of the proof.
Step (iii): We note that (G.26) is nearly the same as (F.19) for the MDP setting. Thus, in the last step,
we follow the same proof strategy as in Step (iii) in §F.1. For notational simplicity, for any function
f: S×A×B→ R and any probability distribution μ ∈ P (SXAXB),we denote the expectation
of f under μ by μ(f). By taking expectation with respect to μ in (G.26), we have
∣∣q* - QK k1,μ ≤，(1 — ；)2^^) ∙ ]χ ɑi ∙ μ(Oi∣%i+1∣) + αK ∙ μ(OK |Q* - QOI) ∙ (G.27)
By the definition of Oi, we can write μ(Οi∣%i+ι∣) as
)iI%i+1I .
μ(Oi∣%i+ι∣) = 1-γ ∙ μ XX Yj ∙ h(P πK ,bκ)j(P π*,eκ P π*,eK-1 ∙∙∙Pπ* ,ei+1)	(G.28)
2	j=0
+ (P ∏K ,VK )j (P ∏K ,VK P ∏K-1,Vκ-1 ... P ∏i+1,Vi+1
To upper bound the right-hand side of (G.28), we consider the following quantity
μ[PTm …Pτ1 )f] = f	[(PTmPTmT
S×A×B
Pτ1 )f] (s,a, b)dμ(s,a, b),
where {τt : S → P(A X B)}t∈[m] are m joint policies of the two-players. By Cauchy-Schwarz
inequality, it holds that
μ[Pτm …Pτ1 )f]
≤ ∖[	d(PTm PTmT …P T1 / (s,a,b)2dσ(s,a,b)
S×A×B ∣	dσ	∣
I
S×A×B
If (s, a, b)I2dσ(s, a, b)
1/2
≤ κ(m; μ,σ) ∙ kfkσ,
where κ(m; μ, σ) is the m-th concentration parameter defined in (E.9). Thus, by (G.28)we have
∞
μ(Oi∖%i+ι∖) ≤ (I- Y) ∙ Xγj ∙K(K — i + j;μ,ν) ∙ ll%i+ιkσ.	(G.29)
j=0
ɪʌ ∙ 1	∙	1 , 1 ∕A+ F P∖	1	FFK r∙)	/ / -1	∖ ∙ Λ	1
Besides, since both Q* and Qo are bounded by RmaX/(1 — Y) in '∞-norm, we have
μ(Oκ ∖Q* - Qo∖) ≤ 2 ∙ Rmax/(1 — Y).	(G.30)
Finally, combining (G.27), (G.29), and (G.30), we obtain that
kQ*- QnK kl,μ
≤ 2y(1- YK+1)
一(1 - Y)
K-1	∞
ΣΣαi ∙ Yj ∙ κ(K - i + j ； μ, V) ∙ k%i+1kσ
4y(1 - YK+1)
+	(1 - Y)3
• αK ∙ RmaX
≤ 2y(1- yk+1)
一(1 - Y)
R X (1-Y)YK-i-1	j ?.，.、
• N A 1 - yk+i • Yj •K(K - i+j；μ,ν)
• ε +JYK+L • R
εmax + (1 γ)2 Rmax，
41
Under review as a conference paper at ICLR 2020
where the last inequality follows from the fact that εmax = maxi∈[K] k%i kσ . Note that in (F.29) we
show that it holds under Assumption E.3 that
X— X∞ (1 - Y)γK-i-1	j K i l ∙.	φμ,ν
Xj 1- —1 - Y K+1 Y ∙ K(K - i + j; μ,ν) ≤ (1 - Y K+1)(1 - Y).
Hence, We obtain (G.1) and thus conclude the proof of Theorem G.1.	□
42