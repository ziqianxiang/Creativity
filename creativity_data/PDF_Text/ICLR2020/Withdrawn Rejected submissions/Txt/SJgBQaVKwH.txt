Under review as a conference paper at ICLR 2020
Effective Use of Variational Embedding
Capacity in Expressive End-to-End Speech
Synthesis
Anonymous authors
Paper under double-blind review
Ab stract
Recent work has explored sequence-to-sequence latent variable models for ex-
pressive speech synthesis (supporting control and transfer of prosody and style),
but has not presented a coherent framework for understanding the trade-offs be-
tween the competing methods. In this paper, we propose embedding capacity
(the amount of information the embedding contains about the data) as a unified
method of analyzing the behavior of latent variable models of speech, comparing
existing heuristic (non-variational) methods to variational methods that are able
to explicitly constrain capacity using an upper bound on representational mutual
information. In our proposed model (Capacitron), we show that by adding condi-
tional dependencies to the variational posterior such that it matches the form of
the true posterior, the same model can be used for high-precision prosody transfer,
text-agnostic style transfer, and generation of natural-sounding prior samples. For
multi-speaker models, Capacitron is able to preserve target speaker identity during
inter-speaker prosody transfer and when drawing samples from the latent prior.
Lastly, we introduce a method for decomposing embedding capacity hierarchically
across two sets of latents, allowing a portion of the latent variability to be specified
and the remaining variability sampled from a learned prior. Audio examples are
available on the web1 .
1	Introduction
The synthesis of realistic human speech is a challenging problem that is important for natural human-
computer interaction. End-to-end neural network-based approaches have seen significant progress
in recent years (Wang et al., 2017; Taigman et al., 2018; Ping et al., 2018; Sotelo et al., 2017),
even matching human performance for short assistant-like utterances (Shen et al., 2018). However,
these neural models are sometimes viewed as less interpretable or controllable than more traditional
models composed of multiple stages of processing that each operate on reified linguistic or phonetic
representations.
Text-to-speech (TTS) is an underdetermined problem, meaning the same text input has an infinite
number of reasonable spoken realizations. In addition to speaker and channel characteristics, im-
portant sources of variability in TTS include intonation, stress, and rhythm (collectively referred to
as prosody). These attributes convey linguistic, semantic, and emotional meaning beyond what is
present in the lexical representation (i.e., the text) (Wagner & Watson, 2010). Recent end-to-end TTS
research has aimed to model and/or directly control the remaining variability in the output.
Skerry-Ryan et al. (2018) augment a Tacotron-like model (Wang et al., 2017) with a deterministic
encoder that projects reference speech into a learned embedding space. The system can be used
for prosody transfer between speakers (“say it like this”), but does not work for transfer between
unrelated sentences, and does not preserve the pitch range of the target speaker. Lee & Kim (2019)
partially address the pitch range problem by centering the learned embeddings using speaker-wise
means.
1https://variational-embedding-capacity.github.io/demos/
1
Under review as a conference paper at ICLR 2020
Other work targets style transfer, a text-agnostic variation on prosody transfer. The Global Style
Token (GST) system (Wang et al., 2018) uses a modified attention-based reference encoder to transfer
global style properties to arbitrary text, and Ma et al. (2019) use an adversarial objective to disentangle
style from text.
Hsu et al. (2019) and Zhang et al. (2019) use a variational approach (Kingma & Welling, 2014) to
tackle the style task. Advantages of this approach include its ability to generate style samples via the
accompanying prior and the potential for better disentangling between latent style factors (Burgess
et al., 2018). Additionally, Hsu et al. (2019) use a Gaussian mixture prior over the latents, which
(when interpreting the mixture component index as a high-level discrete latent) allows a form of
hierarchical control.
This work extends the above approaches by providing the following contributions:
1.	We propose a unified approach for analyzing the characteristics of TTS latent variable
models, independent of architecture, using the capacity of the learned embeddings (i.e., the
representational mutual information between the embedding and the data).
2.	We target specific capacities for our proposed model using a Lagrange multiplier-based opti-
mization scheme, and show that capacity is correlated with perceptual reference similarity.
3.	We show that modifying the variational posterior to match the form of the true posterior
enables style and prosody transfer in the same model, helps preserve target speaker identity
during inter-speaker transfer, and leads to natural-sounding prior samples even at high
embedding capacities.
4.	We introduce a method for controlling what fraction of the variation represented in an
embedding is specified, allowing the remaining variation to be sampled from the model.
2	Measuring reference embedding capacity
2.1	Learning a reference embedding space
Existing heuristic (non-variational) end-to-end approaches to prosody and style transfer (Skerry-Ryan
et al., 2018; Wang et al., 2018; Lee & Kim, 2019; Henter et al., 2018) typically start with the teacher-
forced reconstruction loss, (1), used to train Tacotron-like sequence-to-sequence models and simply
augment the model with a deterministic reference encoder, ge (x), as shown in eq. (2).
L(x, yT, yS) ≡ - logp(x|yT, yS) = kfθ(yT,yS) - xk1 + K	(1)
L0(x,yT,yS) ≡ - logp(x|yT, yS, ge(x)) = kfθ(yT,yS,ge(x)) - xk1 + K (2)
where x is an audio spectrogram, yT is the input text, yS is the target speaker (if training a multi-
speaker model), fθ(∙) is a deterministic function that maps the inputs to spectrogram predictions,
and K is a normalization constant. Teacher-forcing implies that fθ(∙) is dependent on x<t when
predicting spectrogram frame xt. In practice, fθ(∙) serves as the greedy deterministic output of the
model, and transfer is accomplished by pairing the embedding computed by the reference encoder
with different text or speakers during synthesis.
In these heuristic models, the architecture chosen for the reference encoder determines the transfer
characteristics of the model. This decision affects the information capacity of the embedding and
allows the model to target a specific trade-off between transfer precision (how closely the output
resembles the reference) and generality (how well an embedding works when paired with arbitrary
text). Higher capacity embeddings prioritize precision and are better suited for prosody transfer
to similar text, while lower capacity embeddings prioritize generality and are better suited for
text-agnostic style transfer.
The variational extensions from Hsu et al. (2019) and Zhang et al. (2019) augment the reconstruction
loss in eq. (2) with a KL divergence term. This encourages a stochastic reference encoder (variational
posterior), q(z|x), to align well with a prior, p(z) (eq. (3)). The overall loss is then equivalent to the
negative evidence lower bound (ELBO) of the marginal likelihood of the data (Kingma & Welling,
2014).
LELBO(x, yT, yS) ≡ EZ〜q(z∣x) [- logp(x∣z, yT, yS)] + DKL(q(z∣x)kp(z))	(3)
-logp(x|yT,yS) ≤ LELBO(x,yT,yS)	(4)
2
Under review as a conference paper at ICLR 2020
Controlling embedding capacity in variational models can be accomplished more directly by manipu-
lating the KL term in (3). Recent work has shown that the KL term provides an upper bound on the
mutual information between the data, x, and the latent embedding, Z 〜q(z∣x) (Hoffman & Johnson,
2016; Makhzani et al., 2015; Alemi et al., 2018).
RAVG ≡ Ex~pd(x)[DKL(q(z|x)kp(z))],	R ≡ DKL(q(z∣x)kp(z))	⑸
Iq(X;Z) ≡ Ex~pd(x)[DKL(q(z|x)kq(z))],	q(z) ≡ Ex*(x)q(z|x)	⑹
RAVG = Iq(X; Z) + DKL(q(z)kp(z))	(7)
=⇒ Iq (X; Z) ≤ RAVG	(8)
where pD (x) is the data distribution, R is the the KL term in (3), RAVG is the KL term averaged over
the data distribution, Iq(X; Z) is the representational mutual information (the capacity of z), and
q(z) is the aggregated posterior. This brief derivation is expanded in Appendix C.1.
The bound in (8) follows from (7) and the non-negativity of the KL divergence, and (7) shows that
the slack on the bound is DKL(q(z)kp(z)), the aggregate KL. In addition to providing a tighter bound,
having a low aggregate KL is desirable when sampling from the model via the prior, because then the
samples of z that the decoder sees during training will be very similar to samples from the prior.
Various approaches to controlling the KL term have been proposed, including varying a weight on
the KL term, β (Higgins et al., 2017), and penalizing its deviation from a target value (Alemi et al.,
2018; Burgess et al., 2018). Because we would like to smoothly optimize for a specific bound on the
embedding capacity, we adapt the Lagrange multiplier-based optimization approach of Rezende &
Viola (2018) by applying it to the KL term rather than the reconstruction term.
min max{Ez~qθ(z∣x)[-logPθ(x|z,yT,yS)] + β(DKL(qθ(z∣x)kp(z)) - C)}	(9)
θ β≥0
where θ are the model parameters, β serves as an automatically-tuned weight on the KL term, C
is the capacity limit, and updates to θ and β are interleaved during training. We constrain β to be
non-negative by passing an unconstrained parameter through a softplus non-linearity, which makes
the capacity constraint a limit rather than a target. This approach is less tedious than tuning β by
hand and leads to more consistent behavior from run-to-run. It also allows more stable optimization
than directly penalizing the `1 deviation from the target KL.
2.2	Estimating embedding capacity
Estimating heuristic embedding capacity Unfortunately, the heuristic methods do not come
packaged with an easy way to estimate embedding capacity. We can estimate an effective capacity
ordering, however, by measuring the test-time reconstruction loss when using the reference encoder
from each method. In Figure 1, we show how the reconstruction loss varies with embedding
dimensionality for the tanh-based prosody transfer (PT) and softmax-based global style token (GST)
bottlenecks (Skerry-Ryan et al., 2018; Wang et al., 2018) and for variational models (Var.) with
different capacity limits, C. We also compare to a baseline Tacotron model without a reference
encoder. For this preliminary comparison, we use the expressive single-speaker dataset and training
setup described in Section 4.2. Looking at the heuristic methods in Figure 1, we see that the GST
bottleneck is much more restrictive than the PT bottleneck, which hurts transfer precision but allows
sufficient embedding generality for text-agnostic style transfer.
Bounding variational embedding capacity We saw in (8) that the KL term is an upper bound on
embedding capacity, so we can directly target a specific capacity limit by constraining the KL term
using the objective in eq. (9). For the three values of C in Figure 1, we can see that the reconstruction
loss flattens out once the embedding reaches a certain dimensionality. This gives us a consistent way
to control embedding capacity as it only requires using a reference encoder architecture with sufficient
structural capacity (at least C) to achieve the desired representational capacity in the variational
embedding. Because of this, we use 128-dimensional embeddings in all of our experiments, which
should be sufficient for the range of capacities we target.
3
Under review as a conference paper at ICLR 2020
(W sso(jUOIsΠJsuoo0κ
7200
Reconstruction Loss vs. Embedding Dim
8400 I------------------------------------------F------
8	16
32	64
Embedding Dimensionality
128
Figure 1: Reconstruction loss vs. embedding dimensionality for a variety of heuristic and variational
models. For the variational model (Var.), we vary the capacity limit, C. Notice how the reconstruction
loss flattens out at lower values for higher values of C. The heuristic models are denoted by PT and
GST for Prosody Transfer and Global Style Tokens. Figure B.1 in the appendix shows how the KL
term changes when varying C as well as the KL weight, β .
3	Making effective use of embedding capacity
3.1	Matching the form of the true posterior
In previous work (Hsu et al., 2019; Zhang et al., 2019), the variational posterior has the form q(z|x),
which matches the form of the true posterior for a simple generative model p(x|z)p(z). However,
for the conditional generative model used in TTS, p(x|z, yT, yS)p(z), it is missing conditional
dependencies present in the true posterior, p(z|x, yT, yS). Figure 2 shows this visually. In order to
p(x|z, yτ, yS)P(Z)
Figure 2: Adding conditional dependencies to the variational posterior. Shaded nodes indicate
observed variabes. [left] The true generative model. [center] Variational posterior missing conditional
dependencies present in the true posterior. [right] Variational posterior that matches the form of the
true posterior.
match the form of the true posterior, we inject information about the text and the speaker into the
network that predicts the parameters of the variational posterior. Speaker information is represented
as learned speaker-wise embedding vectors, while the text information is summarized into a vector
by passing the output of the Tacotron text encoder through a unidirectional RNN as done by Stanton
et al. (2018). Appendix A.1 gives additional details.
For this work, we use a simple diagonal Gaussian for the approximate posterior, q(z|x, yT, yS) and a
standard normal distribution for the prior, p(z). We use these distributions for simplicity and efficiency,
but using more powerful distributions such as Gaussian mixtures or normalizing flows (Rezende &
Mohamed, 2015) should decrease the aggregate KL, leading to better prior samples.
Because we are learning a conditional generative model, p(x|yT, yS), we could have used a learned
conditional prior, p(z|yT, yS), in order to improve the quality of the output generated when sampling
via the prior. However, in this work We focus on the transfer use case where We infer zref 〜
q(z|xref, yTref, yrSef) from a reference utterance and use it to re-synthesize speech using different text
or speaker inputs, x0 〜p(x∣zref, yT, y《). Using a fixed prior allows Z to share a high probability
region across all text and speakers so that an embedding inferred from one utterance is likely to lead
to non-degenerate output when being used with any other text or speaker.
4
Under review as a conference paper at ICLR 2020
3.2	Decomposing embedding capacity hierarchically
In inter-text style transfer uses cases, we infer zref from a reference utterance and then use it to
generate a new utterance with the same style but different text. One problem with this approach is
that zref completely specifies all variation that the latent embedding is capable of conveying to the
decoder, p(x|zref, yT, yS). So, even though there are many possible realizations of an utterance with
a given style, this approach can produce only one2.
To address this issue, We decompose the 山tents, z, hierarchically (S0nderby et al., 2016) into high-
level latents, zH, and low-level latents, zL, as shown in Figure 3. This differs from the hierarchical
interpretation of the Gaussian mixture prior used by Hsu et al. (2019) in that here the high-level
latents are continuous vectors rather than a single categorical variable. Factorizing continuous latents
in this Way alloWs us to specify hoW the joint capacity, Iq(X; [ZH, ZL]), is divided betWeen zH and
zL. This approach can also be extended to additional levels of latents, each containing a prescribed
proportion of the overall joint capacity.
P(Xlyτ, yS)P(ZLIzH)P(ZH)	q(ZHIzL)q(ZLx yτ, yS)
Figure 3: Hierarchical decomposition of the latents. Shaded nodes indicate observed variables. [left]
The true generative model. [right] Variational posterior that matches the form of the true posterior.
As shoWn in eq. (8), the KL term, RAVG, is an upper bound on Iq (X; Z). We can also derive similar
bounds for Iq(X; ZH) and Iq(X; ZL). Derivations of these bounds are provided in Appendix C.2.
Iq (X; ZL) ≤ RAVG = Ex 〜PD (χ)[DκL(q(zH∣ZL)q(zL |x) kp(zL |ZH)P(ZH))]	(10)
Iq(X; ZH) ≤ RHVG ≡ Ex〜PD(x),ZL〜q(ZL|x)[DKL(q(zH|zL)kp(zH))]	(11)
If We define RLAVG ≡ RAVG - RHAVG , We end up With the folloWing capacity limits for the hierarchical
latents:
=⇒ Iq(X;ZH) ≤RHAVG,	Iq(X;ZL) ≤ RHAVG + RLAVG	(12)
The negative ELBO for this model can be Written as:
LELBO(x, yT, NS = -EZL〜q(zL∣x)[logp(x∣zl, yT, Ys)]+ RH + RL	(13)
Where RH and RL are single data point estimates of RHAVG and RLAVG computed from x. In order to
specify hoW the joint capacity is distributed betWeen the latents, We extend (9) to have tWo Lagrange
multipliers and capacity targets.
min QmaxC {Ezl〜qθ(zL∣x,yT,ys)[- logPθ(x∣zl, yT, Ys)] + β≡(R≡ - CH) + Bl(Rl - Cl)} (14)
θ βH,βL ≥0
CH limits the information capacity of zH, and CL limits hoW much capacity zL has in excess
of ZH (i.e., the total capacity of ZL is capped at CH + Cl). This allows us to infer ZHf 〜
q(zH|zL)q(zL|xref, YTref, Yrsef) from a reference utterance and use it to sample multiple realizations,
x0 〜p(x∣zl, Yt, ys)p(zL∣zHf). Intuitively, the higher CH is, the more the output will resemble the
reference, and the higher CL is, the more variation we would expect from sample to sample when
fixing zrHef and sampling zL from p(zL ∣zrHef).
2If the decoder were truly stochastic (not greedy), we could actually sample multiple realizations given the
same zref, but, at high embedding capacities the variations would likely be very similar perceptually.
5
Under review as a conference paper at ICLR 2020
4 Experiments
4.1	Model architecture and training
Model architecture The baseline model we start with is a Tacotron-based system (Wang et al.,
2017) that incorporates modifications from Skerry-Ryan et al. (2018), including phoneme inputs
instead of characters, GMM attention (Graves, 2013), and a WaveNet neural vocoder (van den Oord
et al., 2016) to convert the output mel spectrograms into audio samples (Shen et al., 2018). The
decoder RNN uses a reduction factor of 2, meaning that it produces two spectrogram frames per
timestep. We use the CBHG text encoder from Wang et al. (2018) and the GMMv2b attention
mechanism from Battenberg et al. (2019).
For the heuristic models compared in Section 2.2, we augment the baseline Tacotron with the reference
encoders described by Skerry-Ryan et al. (2018) and Wang et al. (2018). For the variational models
that we compare in the following experiments, we start with the reference encoder from Skerry-Ryan
et al. (2018) and replace the tanh bottleneck layer with an MLP that predicts the parameters of the
variational posterior. When used, the additional conditional dependencies (text and speaker) are fed
into the MLP as well.
Training To train the models, the primary optimizer is run synchronously across 10 GPU workers
(2 of them backup workers) for 300,000 training steps with an effective batch size of 256. It uses the
Adam algorithm (Kingma & Ba, 2015) with a learning rate that is annealed from 10-3 to 5 × 10-5
over 200,000 training steps. The optimizer for β is run asychronously on the 10 workers and uses
SGD with momentum 0.9 and a fixed learning rate of 10-5. The updates for these two optimizers are
interleaved, allowing β to converge to a steady state value that achieves the target value for the KL
term, as demonstrated in Figure B.2 in the appendix. Additional architectural and training details are
provided in Appendix A.
4.2	Experiment setup
Datasets For single-speaker models, we use an expressive English language audiobook dataset
consisting of 50,086 training utterances (36.5 hours) and 912 test utterances spoken by Catherine
Byers, the speaker from the 2013 Blizzard Challenge. Multi-speaker models are trained using high-
quality English data from 58 voice assistant-like speakers, consisting of 419,966 training utterances
(327 hours). We evaluate on a 9-speaker subset of the multi-speaker test data which contains 1808
utterances (comprising US, UK, Australian, and Indian speakers).
Tasks The tasks that we explore include same-text prosody transfer, inter-text style transfer, and
inter-speaker prosody transfer. We also evaluate the quality of samples produced when sampling via
the prior. For these tasks, we compare performance when using variational models with and without
the additional conditional dependencies in the variational posterior at a number of different capacity
limits. For models with hierarchical latents, we demonstrate the effect of varying CH and CL for
same-text prosody transfer when inferring zH and sampling zL or when inferring zL directly.
Evaluation We use crowd-sourced native speakers to collect two types of subjective evaluations.
First, mean opinion score (MOS) rates naturalness on a scale of 1-5, 5 being the best. Second, we
use the AXY side-by-side comparison proposed by Skerry-Ryan et al. (2018) to measure subjective
similarity to a reference utterance relative to the baseline model on a scale of [-3,3]. For example, a
score of 3 would mean that, compared to the baseline model, the model being tested produces samples
much more perceptually similar to the ground truth reference. We also use an objective similarity
metric that uses dynamic time warping to find the minimum mel cepstral distortion (Kubichek, 1993)
between two sequences (MCD-DTW). Lastly, for inter-speaker transfer, we follow Skerry-Ryan
et al. (2018) and use a simple speaker classifier to measure how well speaker identity is preserved.
Additional details on evaluation methodologies are provided in Appendix A.
4.3	Results
Single speaker For single-speaker models, we compare the performance on same and inter-text
transfer and the quality of samples generated via the prior for models with and without text condi-
tioning in the variational posterior (Var+Txt and Var, respectively) at different capacity limits, C.
6
Under review as a conference paper at ICLR 2020
Reference Similarity
5 0 5 0 5
■ ■ ■ ■ ■
Iloo-
(əuəseg ∙SA Iωpo) OJO。S XXV
MOS (Naturalness)
2.5
2.0
—V— Var Prior —f— Var+Txt Prior
Var STT	Var+Txt STT
Var ITT	Var+Txt ITT
—I- Baseline -∣— Ground Truth
一ɪ . V
10	50	100	300	10	50	100	300
Capacity Limit, C [nats]	Capacity Limit, C [nats]
Figure 4:	Comparing same-text transfer (STT), inter-text transfer (ITT), and prior samples (Prior) for
variational models with and without text dependencies in the variational posterior (Var+Txt and Var,
respectively). Error bars show 95% confidence intervals for the subjective evaluations.
Similarity results for the transfer task are shown on the left side of Figure 4 and demonstrate increas-
ing reference similarity as C is increased, with the exception of the model without text conditioning
on the inter-text transfer task. Looking at the MOS naturalness results on the right side of Figure 4,
we see that both inter-text transfer and prior sampling take a serious hit as capacity is increased for
the Var model, while the Var+Txt model is able to maintain respectable performance even at very
high capacities on all tasks.
Listening to the audio examples, we can hear that the Var model produces degenerate output at high
capacities when attempting to transfer the style from a short utterance to a long utterance. This
indicates that the decoder probably hasn’t seen similar embeddings paired with long utterances during
training, which suggests that z is improperly correlated with text length. Similar behavior is also
observed when generating prior samples using shorter or longer text. This means that an arbitrary z
(sampled from the prior or inferred from a reference) is unlikely to pair well with text of an arbitrary
length.
Multi-speaker For multi-speaker models, we compare inter-speaker same-text transfer perfor-
mance and prior sample quality with and without speaker conditioning in the variational posterior
(Var+Txt+Spk and Var+Txt, respectively) at a fixed capacity limit of 150 nats. In Table 1, we see
that both models are able to preserve characteristics of the reference utterance during transfer (AXY
Ref. Similarity column), while the Var+Txt+Spk model has an edge in MOS for both inter-speaker
transfer and prior samples (almost matching the MOS of the deterministic baseline model even at
high embedding capacity).
Similar to the utterance length argument in the single speaker section above, it is likely that adding
speaker dependencies to the posterior allows the model to use the entire latent space for each speaker
(meaning z is not correlated with speaker identity), thereby forcing the decoder to learn to map all
plausible points in the latent space to natural-sounding utterances that preserve the target speaker’s
pitch range. The speaker classifier results show that the Var+Txt+Spk model preserves target speaker
identity about as well as the baseline model and ground truth data (~5% of the time the classifier
chooses a speaker other than the target speaker), whereas for the Var+Txt model this happens about
22% of the time. Though 22% seems like a large speaker error rate, it is much lower than the
79% figure presented by Skerry-Ryan et al. (2018) for a heuristic prosody transfer model. This
demonstrates that even with a weakly conditioned posterior, the capacity limiting properties of
variational models lead to better transfer generality and robustness.
Hierarchical latents To evaluate hierarchical decomposition of capacity in a single speaker setting,
we use the MCD-DTW distance to quantify reference similarity and same-reference inter-sample
variability. As shown in Table B.1 in the appendix, MCD-DTW strongly (negatively) correlates with
subjective similarity.
The left side of Figure 5 shows results for samples generated using high-level latents, zH, inferred
from the reference. As CH is increased, we see a strong downward trend in the average distance to
7
Under review as a conference paper at ICLR 2020
Table 1: Inter-speaker same-text prosody transfer results for C = 150 with and without speaker
dependencies in the variational posterior (Var+Txt+Spk and Var+Txt, respectively). SpkID denotes
the fraction of the time the target speaker was chosen by the speaker classifier. For reference, we
provide MOS and SpkID numbers for the baseline model and ground truth audio (though neither are
“prior” samples).
Model	Inter-Speaker Transfer			Prior Samples	
	AXY Ref. Similarity	MOS	SpkID	MOS	SpID
Var+Txt	0.364 ± 0.104	3.994 ± 0.066	80.1%	3.674 ± 0.077	78.0%
Var+Txt+Spk	0.439 ± 0.087	4.099 ± 0.061	95.8%	3.906 ± 0.066	94.9%
Baseline	—	—	—	4.086 ± 0.060	95.7%
Ground Truth	-	-	-	4.535 ± 0.044	96.9%
the reference. We can also see that for a fixed CH, increasing CL results in a larger amount of sample-
to-sample variation (average MCD-DTW between samples) when inferring a single zrHef from the
variational posterior and then sampling ZL 〜p(zl |zHf) from the prior to use in the reconstructions.
The right side of Figure 5 shows the same metrics but for samples generated using low-level latents,
zrLef, inferred from the variational posterior. In this case, we see a slight downward trend in the
reference distance as the total capacity limit, C, is increased (the trend is less dramatic because the
capacity is already fairly high). We also see significantly lower inter-sample distance because the
variation modeled by the latents is completely specified by zL. In this case, we sample multiple zrLef’s
from q(zL|xref, yrTef) for the same xref because using the same zL would lead to identical output from
the deterministic decoder.
Using Capacitron with hierarchical latents increases the model’s versatility for transfer tasks. By
inferring just the high-level latents, zH, from a reference, we can sample multiple realizations of
an utterance that are similar to the reference, with the level of similarity controlled by CH, and the
amount of sample-to-sample variation controlled by CL. The same model can also be used for higher
fidelity, lower variability transfer by inferring the low-level latents, zL, from a reference, with the
level of similarity controlled by C = CH + CL. As mentioned before, this idea could also be extended
to use additional levels of latents, thereby increasing transfer and sampling flexibility.
Transfer via ZH
Transfer via ZL
Inter-Sample Distance	Inter-Sample Distance
血Mu山L
Distance From Reference
Distance From Reference
CH = 20	CH = 20	CH = 50	CH = 50	CH = 100	CH	= 100
CL = 50	伉=100	伉=50	CL = 100 CL = 50	伉=100
CH = 20	CH = 50	CH = 20	CH = 50	CH = 100	CH = 100
CL = 50	CL = 50	CL = 100	CL = 100	CL = 50	CL = 100
5.5
5.0
4.5
4.0
3.5
C = 70 C = 120 C = 100 C = 150 C = 150 C = 200 C = 70 C = 100 C = 120 C = 150 C = 150 C = 200
Figure 5:	MCD-DTW reference distance and inter-sample distance for hierarchical latents When
transferring via ZH and rzL
To appreciate the results fully, it is strongly recommended to listen to the audio examples available
on the web3.
3https://Variational-embedding-Capacity.github.io/demos/
8
Under review as a conference paper at ICLR 2020
5 Conclusion
We have proposed embedding capacity (i.e., representational mutual information) as a useful frame-
work for comparing and configuring latent variable models of speech. Our proposed model, Capac-
itron, demonstrates that including text and speaker dependencies in the variational posterior allows
a single model to be used successfully for a variety of transfer and sampling tasks. Motivated by
the multi-faceted variability of natural human speech, we also showed that embedding capacity can
be decomposed hierarchically in order to enable the model to control a trade-off between transfer
fidelity and sample-to-sample variation.
There are many directions for future work, including adapting the fixed-length variational embeddings
to be variable-length and synchronous with either the text or audio, using more powerful distributions
like normalizing flows, and replacing the deterministic decoder with a proper likelihood distribution.
For transfer and control uses cases, the ability to distribute certain speech characteristics across
specific subsets of the hierarchical latents would allow more fine-grained control of different aspects
of the output speech. And for purely generative, non-transfer use cases, using more powerful
conditional priors could improve sample quality.
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a
broken elbo. In International Conference on Machine Learning, pp. 159-168, 2018.
Eric Battenberg, RJ Skerry-Ryan, Soroosh Mariooryad, Daisy Stanton, David Kao, Matt Shannon,
and Tom Bagby. Location-relative attention mechanisms for robust long-form speech synthesis.
arXiv preprint arXiv:1910.10288, 2019.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-vae. arXiv preprint arXiv:1804.03599,
2018.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,
2013.
Gustav Eje Henter, Jaime Lorenzo-Trueba, Xin Wang, and Junichi Yamagishi. Deep encoder-
decoder models for unsupervised learning of controllable speech synthesis. arXiv preprint
arXiv:1807.11470, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017.
Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the variational
evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016.
Wei-Ning Hsu, Yu Zhang, Ron Weiss, Heiga Zen, Yonghui Wu, Yuan Cao, and Yuxuan Wang.
Hierarchical generative modeling for controllable speech synthesis. In International Conference
on Learning Representations, 2019.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference for Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference
on Learning Representations, 2014.
R Kubichek. Mel-cepstral distance measure for objective speech quality assessment. In Communica-
tions, Computers and Signal Processing, 1993., IEEE Pacific Rim Conference on, volume 1, pp.
125-128. IEEE, 1993.
Younggun Lee and Taesu Kim. Robust and fine-grained prosody control of end-to-end speech
synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 5911-5915. IEEE, 2019.
9
Under review as a conference paper at ICLR 2020
Shuang Ma, Daniel Mcduff, and Yale Song. A generative adversarial network for style modeling in a
text-to-speech system. In International Conference on Learning Representations, 2019.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Meinard Muller. Dynamic time warping. Information retrieval for music and motion, pp. 69-84,
2007.
Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan
Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. In International
Conference on Learning Representations, 2018.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
Conference on Machine Learning, pp. 1530-1538, 2015.
Danilo Jimenez Rezende and Fabio Viola. Taming VAEs. arXiv preprint arXiv:1810.00597, 2018.
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng
Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by conditioning
wavenet on mel spectrogram predictions. In 2018 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2018.
RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron Weiss, Rob
Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive speech synthesis
with Tacotron. In Proceedings of the 35th International Conference on Machine Learning, 2018.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in neural information processing systems, pp. 3738-3746,
2016.
Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville, and
Yoshua Bengio. Char2wav: End-to-end speech synthesis. In International Conference on Learning
Representations, Workshop Track, 2017.
Daisy Stanton, Yuxuan Wang, and RJ Skerry-Ryan. Predicting expressive speaking style from text in
end-to-end speech synthesis. In 2018 IEEE Spoken Language Technology Workshop (SLT), pp.
595-602. IEEE, 2018.
Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nachmani. Voiceloop: Voice fitting and synthesis
via a phonological loop. In International Conference on Learning Representations, 2018.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. In 9th ISCA Speech Synthesis Workshop, 2016.
Michael Wagner and Duane G Watson. Experimental and theoretical advances in prosody: A review.
Language and cognitive processes, 25(7-9):905-945, 2010.
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng
Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark,
and Rif A. Saurous. Tacotron: Towards end-to-end speech synthesis. In Proceedings of Interspeech,
August 2017.
Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao,
Ye Jia, Fei Ren, and Rif A Saurous. Style tokens: Unsupervised style modeling, control and
transfer in end-to-end speech synthesis. In International Conference on Machine Learning, pp.
5167-5176, 2018.
Ya-Jie Zhang, Shifeng Pan, Lei He, and Zhen-Hua Ling. Learning latent representations for style
control and transfer in end-to-end speech synthesis. In ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6945-6949. IEEE, 2019.
10
Under review as a conference paper at ICLR 2020
A	Experiment details
A. 1 Architecture details
Baseline Tacotron The baseline Tacotron We start with (which serves as fθ (∙) in eq. (1)) is similar to
the original sequence-to-sequence model described by Wang et al. (2017) but uses some modifications
introduced by Skerry-Ryan et al. (2018). Input to the model consists of sequences of phonemes
produced by a text normalization pipeline rather than character inputs. The CBHG text encoder
from Wang et al. (2017) is used to convert the input phonemes into a sequence of text embeddings.
Before being fed to the CBHG encoder, the phoneme inputs are converted to learned 256-dimensional
embeddings and passed through a pre-net composed of two fully connected ReLU layers (with 256
and 128 units, respectively), with dropout of 0.5 applied to the output of each layer. For multi-speaker
models, a learned embedding for the target speaker is broadcast-concatenated to the output of the text
encoder.
The attention module uses a single LSTM layer with 256 units and zoneout of 0.1 followed by an
MLP with 128 tanh hidden units to compute parameters for the monotonic 5-component GMM
attention window. Instead of using the exponential function to compute the shift and scale parameters
of the GMM components as in (Graves, 2013), we use the softplus function, which we found leads to
faster alignment and more stable optimization.
The autoregressive decoder module consists of 2 LSTM layers each with 256 units, zoneout of 0.1,
and residual connections between the layers. The spectrogram output is produced using a linear layer
on top of the 2 LSTM layers, and we use a reduction factor of 2, meaning we predict two spectrogram
frames for each decoder step. The decoder is fed the last frame of its most recent prediction (or the
previous ground truth frame during training) and the current context as computed by the attention
module. Before being fed to the decoder, the previous prediction is passed through the same pre-net
used before the text encoder above.
Mel spectrograms The mel spectrograms the model predicts are computed from 24kHz audio using
a frame size of 50ms, a hop size of 12.5ms, an FFT size of 2048, and a Hann window. From the FFT
energies, we compute 80 mel bins distributed between 80Hz and 12kHz.
Reference encoder The common reference encoder we use to compute reference embeddings starts
with the mel spectrogram from the reference and passes it through a stack of 6 convolutional layers,
each using ReLU non-linearities, 3x3 filters, 2x2 stride, and batch normalization. The 6 layers have
32, 32, 64, 64, 128, and 128 filters, respectively. The output of this convolution stack is fed into a
unidirectional LSTM with 128 units, and the final output of the LSTM serves as the output of our
basic reference encoder.
To replicate the prosody transfer model from Skerry-Ryan et al. (2018), we pass the reference encoder
output through an additional tanh bottleneck layer to compute the embedding. For the Style Tokens
model in Wang et al. (2018), we pass the output through the Style Tokens bottleneck described in the
paper. For the approximate posterior in our variational models, we pass the output of the reference
encoder (and potentially vectors describing the text and/or speaker) through an MLP with 128 tanh
hidden units to produce the parameters of the diagonal Gaussian posterior which we sample from
to produce a reference embedding. For all models with reference encoders, the resulting reference
embedding is broadcast-concatenated to the output of the text encoder.
Conditional inputs When providing information about the text to the variational posterior, we pass
the sequence of text embeddings produced by the text encoder to a unidirectional RNN with 128 units
and use its final output as a fixed-length text summary that is passed to the posterior MLP. Speaker
information is passed to the posterior MLP via a learned speaker embedding.
A.2 Training Details
For the optimization problems shown in eqs. (9) and (14), we use two separate optimizers. The first
minimizes the objective with respect to the model parameters using the SyncReplicasOptimizer 4
from Tensorflow with 10 workers (2 of them backup workers) and an effective batch size of 256. We
4https://www.tensorflow.org/api_docs/python/tf/train/
SyncReplicasOptimizer
11
Under review as a conference paper at ICLR 2020
also use gradient clipping with a threshold of 5. This optimizer uses the Adam algorithm (Kingma &
Ba, 2015) with β1 = 0.9, β2 = 0.999, = 10-8, and a learning rate that is set to 10-3, 5 × 10-4,
3 × 10-4, 10-4, and 5 × 10-5 at 50k, 100k, 150k, and 200k steps, respectively. Training is run for
300k steps total.
The optimizer that maximizes the objective with respect to the Lagrange multiplier is run asyn-
chronously across the 10 workers (meaning each worker computes an independent update using its
32-example sub-batch) and uses SGD with a momentum of 0.9 and a learning rate of 10-5 . The
Lagrange multiplier is computed by passing an unconstrained parameter through the softplus function
in order to enforce non-negativity. The initial value of the parameter is chosen such that the Lagrange
multiplier equals 1 at the start of training.
A.3 Evaluation Details
Subjective evaluation Details for the subjective reference similarity and MOS naturalness eval-
uations are provided in Figures A.1 and A.2. To evaluate reference similarity, we use the AXY
side-by-side template in Figure A.1, where A is the reference utterance, and X and Y are outputs
from the model being tested and the baseline model.
MCD-DTW We evaluate the models with hierarchical latents using the MCD-DTW distance to
quantify reference similarity and the amount of inter-sample variation. To compute mel cepstral
distortion (MCD) (Kubichek, 1993), we use the same mel spectrogram parameters described in A.1
and take the DCT to compute the first 13 MFCCs (not including the 0th coefficient). The MCD
between two frames is the Euclidean distance between their MFCC vectors. Then we use the dynamic
time warping (DTW) algorithm (Muller, 2007) (with a warp penalty of 1.0) to find an alignment
between two spectrograms that produces the minimum MCD cost (including the total warp penalty).
We report the average per-frame MCD-DTW.
To evaluate reference similarity, we simply compute the MCD-DTW between the synthesized audio
and the reference audio (a lower MCD-DTW indicates higher similarity). The strong (negative)
correlation between MCD-DTW and subjective similarity is demonstrated in Table B.1. To quantify
inter-sample variation, we compute 5 output samples using the same reference and compute the
average MCD-DTW between the first sample and each subsequent sample.
B Additional results
Rate-distortion plots In Figure B.1, we augment the reconstruction loss plots from Figure 1 with
additional rate/distortion plots (Alemi et al., 2018) and vary the KL weight, β, and as well as C.
Optimization Examples Figure B.2 shows examples of how the KL weight, β, and KL term,
R, evolve during training for different capacity limits, C . The curves shown are from the single
speaker Var+Txt models discussed in Section 4.3. The target KL is achieved very quickly and then
maintained throughout training via continual updates to β that are interleaved with updates to the
model parameters.
Single-speaker similarity and naturalness results Tables B.1 and B.2 list the raw numbers used
in the single-speaker reference similarity and MOS naturalness plots shown in Figure 4 in the main
paper. Also shown is MCD-DTW reference distance alongside subjective reference similarity.
Hierarchical latents results The similarity and inter-sample variability results for hierarchical
latents from Figure 5 are shown in table format in Table B.3.
12
Under review as a conference paper at ICLR 2020
You will first be presented with audio of a reference speech sample. First listen to the audio for the reference. Next, listen to two different speech samples and decide
1. you do not have headphones, or
2. there is background noise, or
3. you think you do not have good listening ability, or
4. for any reason, you can't hear the audio samples
IMPORTANT:
This task requires you listen to audio samples using headphones in a quiet environment. Please release this task if:
For this task, you will be given a reference speech and will be asked to decide which of two speech samples more closely matches its prosody (i.e. intonation, stress
and flow).
music playing, air-conditioners, and fans, etc.).
What makes your
preferred side closer?
Which side sounds
closer to the reference?
Are there any problems
with the sample?
(check only if
applicable)
How are you listening
to these speech
samples?
The audio for the
reference
speech samples you
will listen to
Text of the speech
samples
The reference for the
speech sample you are
rating
the right speech.
following:
• Stress put on each word or syllable (e.g. Ioudness or pitch changes).
• Speaking rate, and how it changes throughout the speech sample.
• Pause lengths.
Please give your qualitative opinion. If one sample feels closer but you cannot articulate why, that is OK.
Figure A.1: Evaluation template for AXY prosodic reference similarity side-by-side evaluations.
A human rater is presented with three stimuli: a reference speech sample (A), and two competing
samples (X and Y) to evaluate. The rater is asked to rate whether the prosody of X or Y is closer
to that of the reference on a 7-point scale. The scale ranges from “X is much closer” to “Both are
about the same distance” to “Y is much closer”, and can naturally be mapped on the integers from
-3 to 3. Prior to collecting any ratings, we provide the raters with 4 examples of prosodic attributes
to evaluate (intonation, stress, speaking rate, and pauses), and explicitly instruct the raters to ignore
audio quality or pronunciation differences. For each triplet (A, X, Y) evaluated, we collect 1 rating,
and no rater is used for more than 6 items in a single evaluation. To analyze the data from these
subjective tests, we average the scores and compute 95% confidence intervals.
Table B.1: Detailed subjective reference similarity scores and objective MCD-DTW reference distance
for single speaker models at different capacity limits, C, and with and without text conditioning in
the variational posterior (Var+Txt and Var, respectively). Notice how subjective reference similarity
for same-text transfer is strongly negatively correlated with MCD-DTW.
Model	Ref. Similarity		MCD-DTW
	Same-TT	Inter-TT	Same-TT
Var(C=10)	0.192 ± 0.093	0.182 ± 0.097	5.67
Var(C=50)	0.970 ± 0.102	0.509 ± 0.118	5.13
Var(C=100)	1.203 ± 0.102	-0.275 ± 0.139	5.04
Var(C=300)	1.625 ± 0.092	-0.502 ± 0.143	4.81
Var+Txt(C=10)	0.138 ± 0.097	0.065 ± 0.095	5.68
Var+Txt(C=50)	1.014 ± 0.102	0.942 ± 0.104	5.11
Var+Txt(C=100)	1.346 ± 0.096	1.177 ± 0.103	4.94
Var+Txt(C=300)	1.514 ± 0.095	1.167 ± 0.110	4.83
13
Under review as a conference paper at ICLR 2020
Instruction
IMPORTANT:
In this project, you will listen to audio samples. Please release this task if any of the following is true:
1)	You do not have headphones
2)	You think you do not have good listening ability
3)	There is considerable background noise (street noise, loud fan/air-conditioner, open TV/radio, people talking, etc).
4)	For any reason, you can't hear the audio samples
AUDIO DEVICE (Headphones):
1)	There are many types of headphones. If you have more than one type, this is the preferred order: (a) closed-back headphones, (b) open-back headphones, (c) any other type of headphones.
If you are not sure which type you have, please see this WikiPedia article.
2)	Please set the volume of your audio device to a comfortable level.
In this task, we would like you to listen to a speech sentence and then choose a score for the audio sample you,ve just heard. This score should reflect your opinion of how natural or unnatural
the sentence sounded. You should not judge the grammar or the content of the sentence, just how it sounds.
Please:
1)	Listen to each sample at least twice, with at least a one sec break between them.
2)	Use the given 5-point scale to rate the naturalness of the speech sample. The following table provides a description of each naturalness level of the scale, as well as one or more reference
speech example(s) for each level. Review the table and listen to all of the references. Important note: you do not need to listen to the references if you have listened to them before.
In-Between Ratings: Please note that you are allowed to assign "in-between" ratings (for example, a rating between "Excellent and Good"). Feel free to use them if you think the quality of the
speech sample falls between two levels.
Naturalness Scale:
S∞re	Naturalness	Description	Reference	
5^0	Excellent	Completely natural speech	L	
40	Good	Mostly natural speech	L	
Jo	Fair	Equally natural and unnatural speech	L	
ZO	Poor	Mostly unnatural speech	L	
∙L0	Bad	Completely unnatural speech	L	Sten J
How are you listening to the speech sample?
Headphones, with no noise in the background. I am listening to the speech sample using headphones and there is no noise around me (people talking, music playing, air-conditioners,
and fans, etc.).
Headphones, with some low-level noise in the background. I am listening to the speech sample using headphones and there is some low-level noise around me (people talking, music
playing, air-conditioners, and fans, etc.).
Audio speakers or other.
Speech sample (please listen at least twice)
> 0:00 / 0:02 .	4» ：
Please rate the naturalness of the speech sample:
Score Naturalness Description

Excellent Completely natural speech
Good	Mostly natural speech
Fair	Equally natural and unnatural speech
Poor	Mostly unnatural speech
Bad	Completely unnatural speech
Figure A.2: Evaluation template for mean opinion score (MOS) naturalness ratings. A human rater is
presented with a single speech sample and is asked to rate perceived naturalness on a scale of 1-5,
where 1 is “Bad” and 5 is “Excellent”. For each sample, we collect 1 rating, and no rater is used for
more than 6 items in a single evaluation. To analyze the data from these subjective tests, we average
the scores and compute 95% confidence intervals. Natural human speech is typically rated around
4.5.
Table B.2: MOS naturalness scores for single speaker models at different capacity limits, C, with
and without text conditioning in the variational posterior (Var+Txt and Var, respectively). Scores
are shown for prior samples (Prior), same-text transfer (Same-TT), and inter-text transfer (Inter-TT).
These results are visualized in Figure 4 in the main paper.
Model	MOS score		
	Prior	Same-TT	Inter-TT
Ground Truth Base	4.582 ± 0.041 4.492 ± 0.048		
Var(C=10)	4.438 ± 0.049	4.366 ± 0.053	4.396 ± 0.049
Var(C=50)	4.035 ± 0.066	4.460 ± 0.051	4.029 ± 0.067
Var(C=100)	3.404 ± 0.093	4.388 ± 0.055	3.249 ± 0.095
Var(C=300)	2.343 ± 0.098	4.369 ± 0.054	2.733 ± 0.099
Var+Txt(C=10)	4.358 ± 0.056	4.444 ± 0.052	4.312 ± 0.053
Var+Txt(C=50)	4.360 ± 0.053	4.433 ± 0.052	4.326 ± 0.054
Var+Txt(C=100)	4.309 ± 0.056	4.447 ± 0.048	4.270 ± 0.055
Var+Txt(C=300)	3.805 ± 0.076	4.430 ± 0.050	4.162 ± 0.062
14
Under review as a conference paper at ICLR 2020
(10) ssojUOIsn,ηsuoo9κ
(10) ssojUonɔmjsuoɔəd
Reconstruction LoSS vs. Embedding Dim	Reconstruction LoSS vs. RAVG
Varying β	Varying β
8400
8200
8000
7800
7600
7400
7200
8400
8200
8000
7800
7600
7400
7200
8	16	32	64	128	0	100	200	300	400
Embedding Dimensionality	RAVG, Average KL [nats]
Figure B.1: Figure 1 with additional plots showing reconstruction loss vs. the average KL term. In
these plots we can see how RAVG varies with embedding dimensionality for constant KL weight, β,
while using the KL limit, C, from the optimization problem in eq. 9 achieves constant RAVG .
Table B.3: Transfer using hierarchical latents. “Ref.” is the the average MCD-DTW distance from
the reference, and “X-samp.” is the average inter-sample MCD-DTW. These are the numbers used in
the plots in Figure 5.
(a) Transfer via zH.
(b) Transfer via zL.
Capacity Limits MCD-DTW	Capacity Limits MCD-DTW
CH	CL	C Ref. X-samp.	CH	CL	C Ref. X-samp.
0	0	0	6.054	-	0	0	0	6.054	-
20	50	70	5.517	4.638	20	50	70	4.991	3.899
20	100	120	5.453	4.670	50	50	100	4.916	3.876
50	50	100	5.172	4.245	20	100	120	4.882	3.847
50	100	150	5.166	4.332	100	50	150	4.834	3.830
100	50	150	4.952	3.999	50	100	150	4.797	3.832
100	100	200	5.000	4.147	100	100	200	4.852	3.858
15
Under review as a conference paper at ICLR 2020
Training steps
Training steps
Figure B.2: Evolution of the KL weight, β, and KL term, R, for different capacity limits, C. The
values for the KL term are computed using single training batches. The top plots show the first 10,000
training steps, while the bottom plots show the entire 300,000-step training run. The target KL is
achieved within the first 1,000 steps and then maintained throughout training via continual updates to
β that are interleaved with updates to the model parameters. The value of β is initialized at 1.0 and
ends up at around 9.7, 2.6, 1.0, and 0.14 for C = 10, 50, 100, and 300, respectively.
16
Under review as a conference paper at ICLR 2020
C Derivations
C.1 Bounding representational mutual information
Definitions:
R≡
∕q⑵X)Iog 端 dz
RAVG
pD(x)q(z|x) log
q(ZIX)
p(z)
dXdZ
Iq (X； Z) ≡ JJ
pD(X)q(Z|X) log
叫 dxdz
q(Z)
q(z) ≡
pD(X)q(z|X)dX
(KL term)
(Average KL term)
(Representational mutual information)
(Aggregated posterior)
(15)
(16)
(17)
(18)

KL non-negativity:
/ q(x) log q(x)dx ≥ 0
q (x) log q(x) ≥	q(x) log p(x)dx
(19)
(20)
Mutual information is upper bounded by the average KL (Alemi et al., 2018):
Iq(X； Z) ≡ [PPD(x)q(z∖x) log q(Z|X)dxdz q(z)	(21)
=	pD (x)q (z∖x) log q (z∖x)dxdz -	pD (x)q (z∖x) log q (z)dxdz	(22)
=	pD(x)q(z∖x) logq(z∖x)dxdz -	q(z) logq(z)dz	(23)
≤	pD(x)q(z∖x) logq(z∖x)dxdz -	q(z) log p(z)dz	(24)
=	pD (x)q (z∖x) log q (z∖x)dxdz -	pD (x)q (z∖x) log p(z)dxdz	(25)
=IPPD(x)q(z∖x) log q(Z|X) dxdz p(z)	(26)
≡ RAVG	(27)
=⇒ Iq(X；Z) ≤ RAVG	(28)
where the inequality in (24) follows from (20).
The difference between the average KL and the mutual information is the aggregate KL:
Ravg — Iq(X； Z) = ZZPd(x)q(z∖x) log PZdxdz	(29)
=j q(z)log Pldz	(30)
= DKL(q(z)kP(z))	(Aggregate KL)	(31)
C.2 Hierarchically bounding mutual information
The model with hierarchical latents shown in Figure C.1 gives us the following:
p(z) = p(zH, zL) = p(zL|zH)p(zH)
q(z|X) = q(zH, zL|X) = q(zL|X)q(zH|zL)
(32)
(33)
17
Under review as a conference paper at ICLR 2020
P(X't, yS)P(ZLIzH)P(ZH)
q(ZHIzL)q(ZLx yτ, yS)
Figure C.1: Hierarchical decomposition of the latents. Shaded nodes indicate observed variables.
[left] The true generative model. [right] Variational posterior that matches the form of the true
posterior.
The conditional dependencies on yT and yS are omitted for compactness.
Define marginal aggregated posteriors:
q(ZL) ≡	pD (x)q(ZL |x)dx q(ZH) ≡	q(ZL)q(ZH|ZL)dZL	(34) (35)
We can write the average joint KL term and mutual information as follows:
RAVG
pD(x)[DKL(q(zH|zL)q(zL|x)kp(zL|zH)p(zH))]dx
Iq(X;[ZH,ZL])=
pD(x)[DKL(q(zH|zL)q(zL|x)kq(zH|zL)q(zL))]dx
(36)
(37)
Next we show that Iq(X; [ZH, ZL]) = Iq(X; ZL):
Iq(X; [ZH,ZL])
ZZZpD (x)q(zH, ZL∣X) log	' d^M
q(zH |zL)q(zL)
pD(x)q(zH,zL|x)log
q(zL|x)
dxdZHZL
q(zL)
pD(x)q(ZL|x)log
q(zL |x)
q(ZL)
dxdZL
Iq(X;ZL)
(38)
(39)
(40)
(41)
Bound Iq (X; ZL):
Iq(X;[ZH,ZL]) =Iq(X;ZL)
Iq(X;[ZH,ZL]) ≤ RAVG
=⇒ Iq(X;ZL) ≤ RAVG
(42)
(43)
(44)
where (43) was shown in eq. (28).
18
Under review as a conference paper at ICLR 2020
Again, using the non-negativity of the KL, we can bound Iq (ZH; ZL):
Iq(ZH;ZL)	=Iq q(zH∣zL)q(zL)log q(zHlz;) dzHdzL q (zH )	(45)
	≤ [q q(zH∣zL)q(zL)log q(zHlzL) dzHdzL p(zH)	(46)
	= j P P PD (x)q(zH∣zL)q(zL∣x)log "zH1"：) dzHdzLdx p(zH)	(47)
	=	PD(x)q (zL|x)DKL(q(zH|zL)kP(zH))dzLdx	(48)
	AVG ≡ RH	(49)
Iq(X;ZH)	≤ Iq(ZL; ZH)	(50)
Iq(X;ZH)	≤ RHAVG	(51)
where (50) can be demonstrated by applying the data processing inequality to a reversed version of
the Markov chain, X → ZL → ZH
Define RL:
RL ≡ R - RH
( |)
JJ q(zL∣x)q(zH∣ZL)log P(ZLIZH) dzκdzL
Giving us the following bounds on Iq (X; ZL) and Iq (X; ZH):
=⇒ Iq(X;ZH) ≤RHAVG,	Iq(X;ZL) ≤ RHAVG + RLAVG
(52)
(53)
(54)
19