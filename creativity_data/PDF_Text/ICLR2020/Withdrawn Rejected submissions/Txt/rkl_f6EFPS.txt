Under review as a conference paper at ICLR 2020
The Probabilistic Fault Tolerance of Neural
Networks in The Continuous Limit
Anonymous authors
Paper under double-blind review
Ab stract
The loss of a few neurons in a brain rarely results in any visible loss of function.
However, the insight into what “few” means in this context is unclear. How many
random neuron failures will it take to lead to a visible loss of function? In this paper,
we address the fundamental question of the impact of the crash of a random subset
of neurons on the overall computation of a neural network and the error in the output
it produces. We study fault tolerance of neural networks subject to small random
neuron/weight crash failures in a probabilistic setting. We give provable guarantees
on the robustness of the network to these crashes. Our main contribution is a
bound on the error in the output of a network under small random Bernoulli crashes
proved by using a Taylor expansion in the continuous limit, where close-by neurons
at a layer are similar. The failure mode we adopt in our model is characteristic
of neuromorphic hardware, a promising technology to speed up artificial neural
networks, as well as of biological networks. We show that our theoretical bounds
can be used to compare the fault tolerance of different architectures and to design
a regularizer improving the fault tolerance of a given architecture. We design
an algorithm achieving fault tolerance using a reasonable number of neurons. In
addition to the theoretical proof, we also provide experimental validation of our
results and suggest a connection to the generalization capacity problem.
1	Introduction
Understanding the inner working of artificial neural networks (NNs) is currently one of the most
pressing questions (20) in learning theory. As of now, neural networks are the backbone of the most
successful machine learning solutions (37; 18). They are deployed in safety-critical tasks in which
there is little room for mistakes (10; 40). Nevertheless, such issues are regularly reported since
attention was brought to the NNs vulnerabilities over the past few years (37; 5; 24; 8).
Fault tolerance as a part of theoretical NNs research. Understanding complex systems requires
understanding how they can tolerate failures of their components. This has been a particularly fruitful
method in systems biology, where the mapping of the full network of metabolite molecules is a
computationally quixotic venture. Instead of fully mapping the network, biologists improved their
understanding of biological networks by studying the effect of deleting some of their components,
one or a few perturbations at a time (7; 12). Biological systems in general are found to be fault
tolerant (28), which is thus an important criterion for biological plausibility of mathematical models.
Neuromorphic hardware (NH). Current Machine Learning systems are bottlenecked by the under-
lying computational power (1). One significant improvement over the now prevailing CPU/GPUs
is neuromorphic hardware. In this paradigm of computation, each neuron is a physical entity (9),
and the forward pass is done (theoretically) at the speed of light. However, components of such
hardware are small and unreliable, leading to small random perturbations of the weights of the model
(41). Thus, robustness to weight faults is an overlooked concrete Artificial Intelligence (AI) safety
problem (2). Since we ground the assumptions of our model in the properties of NH and of biological
networks, our fundamental theoretical results can be directly applied in these computing paradigms.
Research on NN fault tolerance. In the 2000s, the fault tolerance of NNs was a major motivation
for studying them (14; 16; 4). In the 1990s, the exploration of microscopic failures was fueled by
the hopes of developing neuromorphic hardware (NH) (22; 6; 34). Taylor expansion was one of
the tools used for the study of fault tolerance (13; 26). Another line of research proposes sufficient
conditions for robustness (33). However, most of these studies are either empirical or are limited to
simple architectures (41). In addition, those studies address the worst case (5), which is known to be
1
Under review as a conference paper at ICLR 2020
more severe than a random perturbation. Recently, fault tolerance was studied experimentally as well.
DeepMind proposes to focus on neuron removal (25) to understand NNs. NVIDIA (21) studies error
propagation caused by micro-failures in hardware (3). In addition, mathematically similar problems
are raised in the study of generalization (29; 30) and robustness (42).
The quest for guarantees. Existing NN approaches do not guarantee fault tolerance: they only
provide heuristics and evaluate them experimentally. Theoretical papers, in turn, focus on the worst
case and not on errors in a probabilistic sense. It is known that there exists a set of small worst-
case perturbations, adversarial examples (5), leading to pessimistic bounds not suitable for the
average case of random failures, which is the most realistic case for hardware faults. Other branch
of theoretical research studies robustness and arrives at error bounds which, unfortunately, scale
exponentially with the depth of the network (29). We define the goal of this paper to guarantee
that the probability of loss exceeding a threshold is lower than a pre-determined small value. This
condition is sensible. For example, self-driving cars are deemed to be safe once their probability of
a crash is several orders of magnitude less than of human drivers (40; 15; 36). In addition, current
fault tolerant architectures use mean as the aggregation of copies of networks to achieve redundancy.
This is known to require exponentially more redundancy compared to the median approach and, thus,
hardware cost. In order to apply this powerful technique and reduce costs, certain conditions need to
be satisfied which we will evaluate for neural networks.
Contributions. Our main contribution is a theoretical bound on the error in the output of an NN in
the case of random neuron crashes obtained in the continuous limit, where close-by neurons compute
similar functions. We show that, while the general problem of fault tolerance is NP-hard, realistic
assumptions with regard to neuromorphic hardware, and a probabilistic approach to the problem,
allow us to apply a Taylor expansion for the vast majority of the cases, as the weight perturbation is
small with high probability. In order for the Taylor expansion to work, we assume that a network
is smooth enough, introducing the continuous limit (39) to prove the properties of NNs: it requires
neighboring neurons at each layer to be similar. This makes the moments of the error linear-time
computable. To our knowledge, the tightness of the bounds we obtain is a novel result. In turn, the
bound allows us to build an algorithm that enhances fault tolerance of neural networks. Our algorithm
uses median aggregation which results in only a logarithmic extra cost - a drastic improvement on the
initial NP-hardness of the problem. Finally, we show how to apply the bounds to specific architectures
and evaluate them experimentally on real-world networks, notably the widely used VGG (38).
Outline. In Sections 2-4, we set the formalism, then state our bounds. In Section 5, we present
applications of our bounds on characterizing the fault tolerance of different architectures. In Sec-
tion 6 we present our algorithm for certifying fault tolerance. In Section 7, we present our ex-
perimental evaluation. Finally, in Section 8, we discuss the consequences of our findings. Full
proofs are available in the supplementary material. Code is provided at the anonymized repo
github.com/iclr-2020-fault-tolerance/code. We abbreviate Assumption 1 → A1,
Proposition 1 → P1, Theorem 1 → T1, Definition 1 → D1.
2	Definitions of Probabilistic Fault Tolerance
In this section, we define a fully-connected network and fault tolerance formally.
Notations. For any two vectors x, y ∈ Rn we use the notation (x, y) = Pin=1 xiyi for the standard
scalar product. Matrix γ-norm for γ = (0, +∞] is defined as kAkγ = supx6=0 kAxkγ /kxkγ . We use
the infinity norm kxk∞ = max |xi | and the corresponding operator matrix norm. We call a vector
0 6= x ∈ Rn q-balanced if min |xi| ≥ qmax |xi|. We denote [n] = {1, 2, ..., n}. We define the
Hessian Hij = ∂2y (x)∕∂xi∂xj as a matrix of second derivatives. We write layer indices down and
element indices up: Wlij . For the input, we write xi ≡ xi . If the layer is fixed, we omit its index. We
use the element-wise Hadamard product (x y)i = xiyi.
Definition 1. (Neural network) A neural network with L layers is a function yL : Rn0 → RnL defined
by a tuple (L, W, B,夕)with a tuple of weight matrices W = (Wι,…，Wl) (or their distributions) of
size Wl : nl × nl-1, biases B = (b1, ..., bL) (or their distributions) of size bl ∈ Rnl by the expression
yι 二 夕(Zl) withpre-aCtivationS Zl = Wlyι-ι + bi, l ∈ [L], y0 = X and yL = ZL. Note that the last
layer is linear. We additionally require 夕 to be I-LiPSChitz 1. We assume that the network was trained
11-Lipschitz g s.t. |夕(x)一夕(y)| 6 |x — y|. If g is K-Lipschitz, we rescale the weights to make K = 1:
Wij → Wiij/K. This is the general case. Indeed, if we rescale g(x) → K夕(x),then, yι-ι → Ky'0-ι, and in
the sum Zl = P Wij/K ∙ Kyι-ι ≡ Zl
2
Under review as a conference paper at ICLR 2020
using input-output pairs x,y* 〜X X Y using ERM2 3 for a loss ω. Loss Iayerfor input X and the true
label y*(x) is defined as yL+ι(x) = Ey*〜Y∣χω(yL(x), y*)) with ω ∈ [一1,1]3
Definition 2. (Weight failure) Network (L, W, B,夕)with weight failures U of distribution U 〜
D|(x, W) is the network (L, W + U, B, φ) for U 〜D|(x, W). We denote a (random) output ofthis
network as yw+U(x) = yL(x) with activations yι andpre-activations Zl, as in DL
Definition 3. (Bernoulli neuron failures) Bernoulli neuron crash distribution is the distribution with
i.i.d. ξ∣ 〜 Be(Pl), Uij = -ξi ∙ Wlij. For each possible crashing neuron i at layer l we define
Uli = Pj |Ulij | and Wli = Pj |Wlij |, the crashed incoming weights and total incoming weights. We
note that we see neuron failure as a sub-type of weight failure.
This definition means that neurons crash independently, and they start to output 0 when they do.
We use this model because it mimics essential properties of NH (41). Components fail relatively
independently, as we model faults as random (41). In terms of (41), we consider stuck-at-0 crashes,
and passive fault tolerance in terms of reliability.
Definition 4. (Output error for a weight distribution) The error in case of weight failure with
distribution D|(x, W) is ∆l (x) = ylw +U (x) 一 ylw (x) for layers l ∈ [L + 1]
We extend the definition of ε-fault tolerance from (23) to the probabilistic case:
Definition 5. (Probabilistic fault tolerance) A network (L, W, B,夕)is said to be (ε, δ)-fault tol-
erant over an input distribution (x,y*) 〜 X × Y and a crash distribution U 〜 D∣(x,W)
if P(χ,y*)〜x×y,u〜d∣(x,w){Δl+i(x) ≥ ε} ≤ δ. For such network, we write (W, B) ∈
FT(L, ψ,P,ε,δ).
Interpretation. To evaluate the fault tolerance of a network, we compute the first moments of ∆L+1 .
Next, we use tail bounds to guarantee (ε, δ)-FT. This definition means that with high probability
1 一 δ additional loss due to faults does not exceed ε. Expectation over the crashes U 〜D|x can be
interpreted in two ways. First, for a large number of neural networks, each having permanent crashes,
E∆ is the expectation over all instances of a network implemented in the hardware multiple times.
For a single network with intermittent crashes, E∆ is the output of this one network over repetitions.
The recent review study (41) identifies three types of faults: permanent, transient, and intermittent.
Our definition 2 thus covers all these cases.
Now that we have a definition of fault tolerance, we show in the next section that the task of certifying
or even computing it is hard.
3	The Hardness of Fault Tolerance
In this section, we show why fault tolerance is a hard problem. Not only it is NP-hard in the most
general setting but, also, even for small perturbations, the error of the output of can be unacceptable.
3.1 NP-Hardness
A precise assessment of an NN’s fault tolerance should ideally diagnose a network by looking at the
outcome of every possible failure, i.e. at the Forward Propagated Error (23) resulting from removing
every possible subset of neurons. This would lead to an exact assessment, but would be impractical in
the face of an exponential explosion of possibilities as by Proposition 1 (proof in the supplementary
material).
Proposition 1. The task of evaluating E∆k for any k = 1, 2, ... with constant additive or multiplica-
tive error for a neural network with 夕 ∈ C ∞, Bernoulli neuron crashes and a constant number of
layers is NP-hard.
We provide a theoretical alternative for the practical case of neuromorphic hardware. We overcome
NP-hardness in Section 4 by providing an approximation dependent on the network, and not且COnStant
factor one: for weights W we give ∆ and ∆ dependent on W such that ∆(W) ≤ E∆ ≤ ∆(W). In
addition, we only consider some subclass of all networks.
3	.2 Pessimistic Spectral Bounds
By Definition 4, the fault tolerance assessment requires to consider a weight perturbation W + U
given current weights W and the loss change yL+ι(W + U) 一 yL+ι(W) caused by it. Mathematically,
2Empirical Risk Minimization - the standard task 1 /k Pm=I ω(yL (Xk), y%) → min
3The loss is bounded for the proof of Algorithm 1’s running time to work
3
Under review as a conference paper at ICLR 2020
Quantity
Discrete
Continuous
Input
Weights
Pre-activations
x : [n0] → R
Wl: [nl] × [nl-1] →R
x : [0, 1] → R
zli = PjWlijyli-1 +bli
7-→	Wl : [0, 1]2 → R
zι(t) = R01 Wι(t,t'0)yι-ι6dt' + b ⑴
Table 1: Correspondence between discrete and continuous quantities. When an regular (discrete)
NN is a function mapping vectors to vectors, a continuous NN is an operator mapping functions to
functions
this means calculating a local Lipschitz coefficient K (43) connecting |yL+1 (W + U) -yL+1(W)| ≤
K|U|. In the literature, there are known spectral bounds on the Lipschitz coefficient for the case of
input perturbations. These bounds use the spectral norm of the matrix ∣∣ ∙ k2 and give a global result,
valid for any input. This estimate is loose due to its exponential growth in the number of layers, as
∣W ∣2 is rarely < 1. See Proposition 2 for the statement:
Proposition 2 (K using spectral properties). kyL(χ2) — yL(χ1)k2 6 ∣∣χ2 — χ1k2 ∙ QL=1 ∣Wι∣2
The proof can be found in (29) or in the supplementary material. It is also known that high
perturbations under small input changes are attainable. Adversarial examples (5) are small changes
to the input resulting in a high change in the output. This bound is equal to the one of (23), which is
tight in case if the network has the fewest neurons. In contrast, in Section 4, we derive our bound in
the limit n → ∞.
We have now shown that even evaluating fault tolerance of a given network can be a hard problem.
In order to make the analysis practical, we use additional assumptions based on the properties of
neuromorphic hardware.
4 Realistic S implifying Assumptions for Neuromorphic Hardware
In this section, we introduce realistic simplifying assumptions grounded in neuromorphic hardware
characteristics. We first show that if faults are not too frequent, the weight perturbation would be
small. Inspired by this, we then apply a Taylor expansion to the study of the most probable case. 4
Assumption 1. The probability of failure p = max{pl l ∈ [L]} is small: p . 10-4..10-3
This assumption is based on the properties of neuromorphic hardware (35). Next, we then use the
internal structure of neural networks.
Assumption 2. The number of neurons at each layer nl is sufficiently big, nl & 102
This assumption comes from the properties of state-of-the-art networks (1).
The best and the worst fault tolerance. Consider a 1-layer NN with n = n0 and nL = n1 = 1 at
input xi = 1: y(x) = P xi/n. We must divide 1/n to preserve y(x) as n grows. This is the most
robust network, as all neurons are interchangeable. Here E∆ = -p and Var∆ = p/n, variance
decays with n. In contrast, the worst case y(x) = x1 has all but one neuron unused. Therefore
E∆ = p and Var∆ = p, variance does not decay with n.
The next proposition shows that under a mild additional regularity assumption on the network,
Assumptions 1 and 2 are sufficient to show that the perturbation of the norm of the weights is small.
Proposition 3. Under A1,2 and if {Wli}in=l 1 are q-balanced, for α > p, the norm of the weight
perturbation Ui at layer l is probabilistically bounded as: δ0 = P{∣Uι ∣ι ≥ α∣Wj∣} ≤
exp(-nι∙ q ∙ dκL(α∣∣pι)) with KL-divergence between numbers a, b ∈ (0,1), dκL(a, b) =
alog a/b+ (1 - a) log (1 - a)/(1 - b) and Wli from D3
Inspired by this result, next, we compute the error ∆ given a small weight perturbation U using a
Taylor expansion. 5
4The inspiration for splitting the loss calculation into favorable and unfavorable cases comes from (27)
5In order to certify fault tolerance, we need a precise bounds on the remainder of the Taylor approximation.
For example, for ReLU functions, Taylor approximation fails. The supplementary material contains another
counter-example to the Taylor expansion of an NN. Instead, we give sufficient conditions for which the Taylor
approximation indeed holds.
4
Under review as a conference paper at ICLR 2020
1.0
0.5
0 m = ιo) 1
O m = 20) ɪ
O m = 30)ɪ
----Cont. z(t)f t∈ [0f 1]
Discrete zit ∕∈ [n]
Figure 1: Discrete (standard) neural network approximating a continuous network
Assumption 3. As the width n increases, networks NNn have a continuous limit (39) NNn → NNc,
where NNc is a continuous neural network (19), and n = min{nl}. That network NNc has globally
bounded operator derivatives Dk for orders k = 1, 2. We define D12 = max{D1 , D2}.6 7
See Figure 1 for a visualization of A3 and Table 1 for the description of A3. The assumption means
that with the increase of n, the network uses the same internal structure which just becomes more
fine-grained. The continuous limit holds in the case of explicit duplication, convolutional networks
and corresponding explicit regularization. The supplementary material contains a more complete
explanation.
The derivative bound for order 2 is in contrast to the worse-case spectral bound which would be
exponential in depth as in Proposition 2. This is consistent with experimental studies (11) and can be
connected to generalization properties via minima sharpness (17).
Proposition 4. Under A3, derivatives are equal the operator derivatives of the continuous limit:
ML 一 一	δ^L+ o(1) n→∞
∂yi …∂y	nk δyι(i1)...δyι(ik)
For example 7, consider y(x) = 1/ni P.1=]夕(Pn0=I xi0/n0) at Xi ≡ 1. Factors 1/no and 1/ni
appear because the network must represent the same y as n0,n1 → ∞. Then, dy/dxi =，⑴/nι
and ∂2y/dxidxj =夕"(l)/n1.
Theorem 1. For crashes at layer l and output error ∆L at layer L UnderA1-3 with q = 1 /nι and
r = p + q, the mean and variance of the error can be approximated as
eδl=PI X IyL∙∣ξ=0 + θ± (I)D2r2, Varδl =pι X (.1=0) +θ± (I)D12r3
By Θ±(1) we denote any function taking values in [-1, 1].8
The full proof of the theorem is in the supplementary material. The remainder terms are small as
both P and q = 1/ni are small quantities under A1-2. In addition, P4 implies IyL/∂ξi 〜1/ni and
thus, when n → ∞, E∆ = O(1) remains constant, and Var∆L = O(1/ni). This is the standard
rate in case if we estimate the mean of a random variable by averaging over nι independent samples,
and our previous example in the beginning of the Section shows that it is the best possible rate. Our
result shows sufficient conditions under which neural networks allow for such a simplification.9 In
the next sections we use the obtained theoretical evaluation to develop a regularizer increasing fault
tolerance, and say which architectures are more fault-tolerant.
5	Probabilistic Guarantees on Fau lt Tolerance Using Tail Bounds
In this section, we apply the results from the previous sections to obtain a probabilistic guarantee on
fault tolerance. We identify which kinds of architectures are more fault-tolerant.
6A necessary condition for Dk to be bounded is to have a reasonable bound on the derivatives of the ground
truth function y* (x). We assume that this function is sufficiently smooth.
7The proposition is illustrated in proof-of-concept experiments with explicit regularization in the supplemen-
tary material. There are networks for which the conclusion of P4 would not hold, for example, a network with
wij = 1. However, such a network does not approximate the same function as n increases since y(x) → ∞,
violating A3
8The derivative ∂yL/∂ξi(ξ) ≡ — ∂yL(yι — ξ Θ yι)∕∂yi ∙ yi is interpreted as if ξi was a real variable.
9However, the dependency Var∆ 〜1/ni is only valid if n < ρ-2 〜108 to guarantee the first-order term
to dominate, p/n > r3 . In case if this is not true, we can still render the network more robust by aggregating
multiple copies with a mean, instead of adding more neurons. Our current guarantees thus work in case if
p2 ≤ n-1 ≤ p. In the supplementary material, we show that a more tight remainder, depending only on p/n,
hence decreasing with n, is possible. However, it complicates the equation as it requires D3 .
5
Under review as a conference paper at ICLR 2020
Under the assumptions of previous sections, the variance of the error decays as Var∆ 〜E Cipi/n
as the error superposition is linear (see supplementary material for a proof), with Cl not dependent
on ni. Given a fixed budget of neurons, the most fault-tolerant NN has its layers balanced: one layer
with too few neurons becomes a single point of failure. Specifically, an optimal architecture with a
fixed sum N = P n has n 〜√pιC
Given the previous results, certifying (ε, δ)-fault tolerance is trivial via a Chebyshev tail bound
(proof in the supplementary material):
Proposition 5. A neural network under assumptions 1-3 is (ε, δ)-fault tolerant for t = ε - E∆L > 0
with δ = t-2Var∆L for E∆ and Var∆ calculated by Theorem 1.
Evaluation of E∆ or Var∆ using Theorem 1 would take the same amount of time as one forward
pass. However, the exact assessment would need O(2n) forward passes by Proposition 1.
In order to make the networks more fault tolerant, we now want to solve the problem of loss minimiza-
tion under fault tolerance rather than ERM (as previously formulated in (41)): inf(W,B)∈FT L(w, B)
where FT = FT(L,夕,p, ε, δ) from Definition 5. Regularizing10 11 with Equation 1 can be seen as
an approximate solution to the problem above. Indeed, Var∆ ≈ Pl Pi (号∙ yl) (from T1) is
connected to the target probability (P5). Moreover, the network is required to be continuous by A3,
which is achieved by making nearby neurons’ weights close using a smoothing regularizing function
smooth (W) ≈ / | W； (t, t0)∣dtdt0. The μ term for q-balancedness comes from P3 as it is a necessary
condition for A3. See the supplementary material for complete details. Here L is the regularized loss,
L the original one, and λ, μ, ν, ψ are the parameters:
nl
L(W ) = L(W) + λ X
i=1
2
+ μ
(maxi, Wi ʌ
I mini Wi )
+ ψ ∙ smooth(Wi) + VkW∣∣∞
(1)
We define the terms corresponding to λ, μ, ψ as Ri ≈ Var∆∕pι, R? = q2, R3 = smooth(Wi). If we
have achieved δ < 1/3 by P5, we can apply the well-known median trick technique (31), drastically
increasing fault tolerance. We only use R repetitions of the network with component-wise median
aggregation to obtain (ε, δ ∙ exp(-R))-fault tolerance guarantee. See supplementary material for the
calculations.
In addition, we show that after training, when EχVwyL+i(x) = 0, then EχEξΔl+i = 0 + O(r2)
(proof in the supplementary material). This result sheds some light on why neural networks are
inherently fault-tolerant in a sense that the mean ∆L+1 is 0. Convolutional networks of architecture
Conv-Activation-Pool can be seen as a sub-type of fully connected ones, as they just have
locally-connected matrices Wi, and therefore our techniques still apply. Using large kernel sizes (see
supplementary material for discussion), smooth pooling and activations lead to a better approximation.
We developed techniques to assess fault tolerance and to improve it. Now we combine all the results
into a single algorithm to certify fault tolerance.
6	An algorithm for Certifying Fault Tolerance
We are now in the position to provide an algorithm (Algorithm 1) allowing to reach the desired
(ε, δ)-fault tolerance via training with our regularizer and then physically duplicating the network a
logarithmic amount of times in hardware, assuming independent faults. We note that our algorithm
works for a single input x but is easily extensible if the expressions in Propositions are replaced with
expectations over inputs (see supplementary material).
In order to estimate the required number of neurons, we use bounds from T1 and P5 which require
n 〜p∕ε2. However, using the median approach allows for a fast exponential decrease in failure
probability. Once the threshold of failing with probability 1/3 is reached by P5, it becomes easy to
reach any required guarantee. The time complexity (compared to the one of training) of the algorithm
is O(D12 + Cιpι∕ε2) and space complexity is equal to that of one training call. See supplementary
material for the proofs of resource requirements and correctness.
10We note that the gradient of Var∆ is linear time-computable since it is a Hessian-vector product.
11More neurons do not solve the problem, as E∆ stays constant with the growth ofn by Theorem 1. Intuitively,
this is due to the fact that if a mean of a random variable is too high, more repetitions do not make the estimate
lower.
6
Under review as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Data: Dataset D, input-output point (x, y*), failure probabilities Pl, depth L, activation function
夕 ∈ C∞, target ε and δ0, the error tolerance parameters from the Definition 5, maximal
complexity guess C ≈ R |yl0(t)|dt ≈ R3guess
Result: An architecture with (ε, δ0)-fault-tolerance on x
Select initial width N = (n1, ..., nL-1);
while true do
Train a network to obtain W, B ;
Compute q from Proposition 3;
If q < 10-2, increase regularization parameter μ from Eq. 1, continue;//go to line 3;
Compute δ0 from Proposition 3 using q;
If δ0 > 1/3, increase n by a constant amount, continue;
Compute R3 from Eq. 1;
If R3 > C, increase regularization parameter ψ from Eq. 1, continue;
Compute E∆ and Var∆ from Theorem 1;
IfE∆ > ε, output infeasible; // cannot do better than the mean11;
Compute δ from Proposition 5;
If δ > 1/3, increase n by a constant amount and increase λ in Eq. 1, continue;
Compute R = O (log ±);
Output number of repetitions R, layer widths N, parameters W, B;
end
Algorithm 1: Achieving fault tolerance after training. The numbers qmax = 10-2 and δmax =
1/3 are chosen for simplicity of proofs. The asymptotic behavior does not change with different
numbers, as long as δmax < 1/2 and the constraints on q mentioned in the supplementary material
are met 7 * * * * * * * is
Figure 2: The effect of the layer width nl, l = 1, horizontal axis on the variance of the fault tolerance
error Var∆, vertical axis
7 Experimental Evaluation
In this section, we test the theory developed in previous sections in proof-of-concept experi-
ments. We first show that we can correctly estimate the first moments of the fault tolerance
using T1 for small (10-50 neurons) and larger networks (VGG). We test the predictions of
our theory such as decay of Var∆, the effect of our regularizer and the guarantee from Al-
gorithm 1. See the supplementary material for the technical details where we validate the as-
sumption of derivative decay (A3) explicitly. Our code is provided at the anonymized repository
github.com/iclr-2020-fault-tolerance/code.
Increasing training dropout. We train sigmoid networks with N 〜 100 on MNIST (see
ComparisonIncreasingDropoutMNIST.ipynb). We use probabilities of failure at infer-
ence and training stages pi = 0.05 at the first layer and 10 values ofpt ∈ [0, 1.2pi]. The experiment
is repeated 10 times. When estimating the error experimentally, we choose 6 repetitions of the
training dataset to ensure that the variance of the estimate is low. The results are in the Table 2. The
experiments show that crashing MAE (Mean Absolute Error for the network with crashes at inference)
is most dramatically affected by dropout. Specifically, training with Pt 〜Pi makes network more
robust at inference, which was well-established before. Moreover, the bound from T1 can correctly
order which network is trained with bigger dropout parameter with only 4% rank loss, which is the
fraction of incorrectly ordered pairs. All other quantities, including norms of the weights, are not
able to order networks correctly. See supplementary material for a complete list of metrics in the
experiment.
7
Under review as a conference paper at ICLR 2020
Quantity	Train rank loss	Test rank loss	Quantity	Rank Loss
Crashing, MAE	5.6%	5.6%	T1 Var∆	3.6%
Crashing, Accuracy	19.8%	17.7%	P2 E∆	24.8%
Correct, MAE	23.3%	22.0%	P2 Var∆	31.7%
Correct, Accuracy	31.7%	39.9%	T1 E∆	40.8%
(a) Experimental metrics	(b) Theoretical bounds
Table 2: Comparison of networks trained with increased dropout. Rank loss between ptrain and
various metrics. Experiment shows crashing and correct networks, MAE or accuracy and test/train
datasets. Theoretical bounds include P212and T1.
Regularization for fault tolerance. Previously, our bound is demonstrated to be able to correctly
predict which network is more resilient. We therefore use it as a regularization technique suggested by
Eq. 1, see Regularization.ipynb. We establish that the resilience of the network regularized
with Dropout is similar to that of a network regularized with the bound
Testing the bound on larger networks. We test the bound on VGG16 and on a smaller convnet,
see ConvNetTest-MNIST.ipynb and ConvNetTest-VGG16.ipynb and verify that they
correctly predict the magnitude of the error
Architecture and fault tolerance. Comparing different architectures on a single image with
p = 0.01 (VGG16, VGG19, MobileNet) shows (and ConvNetTest-ft.ipynb) that the bigger
the mean width of the layer (approximated by the number of parameters), the better is the fault
tolerance, as predicted in Section 5. In addition, training networks on the MNIST dataset (see
FaultTolerance-Continuity-FC-MNIST.ipynb) shows a decrease in variance with nl
as predicted by Theorem 1, see Figure 2: the variance decays as 1/nl. We regularize with ψ =
(10-4, 10-2) for derivatives and smoothing respectively (see supplementary material for explanation
of coefficients) and λ = 0.001.
Testing the algorithm. We test the Algorithm 1 on the MNIST dataset for ε = 9 ∙ 10-3, δ = 10-5
and obtain R = 20, nι = 500, λ = 10-6, μ = 10-10, ψ = (10-4, 10-2). We evaluate the tail
bound experimentally. Our experiment demonstrates the guarantee given by Proposition 5 and can be
seen as an experimental confirmation of the algorithm’s correctness. See TheAlgorithm.ipynb.
We hence conclude that our proof-of-concept experiments show an overall validity of our assumptions
and of our approach.
8 Conclusion
Fault tolerance is an important overlooked concrete AI safety issue (2). This paper describes a
probabilistic fault tolerance framework for NNs that allows to get around the NP-hardness of the
problem. Since the crash probability in neuromorphic hardware is low, we can simplify the problem
to allow for a polynomial computation time. We use the tail bounds to motivate the assumption
that the weight perturbation is small. This allows us to use a Taylor expansion to compute the error.
To bound the remainder, we require sufficient smoothness of the network, for which we use the
continuous limit: nearby neurons compute similar things. After we transform the expansion into a
tail bound to give a bound on the loss of the network. This gives a probabilistic guarantee of fault
tolerance. Using the framework, we are able to guarantee sufficient fault tolerance of a neural network
given parameters of the crash distribution. We then analyze the obtained expressions to compare
fault tolerance between architectures and optimize for fault tolerance of one architecture. We test our
findings experimentally on small networks (MNIST) as well as on larger ones (VGG-16, MobileNet).
Using our framework, one is able to deploy safer networks into neuromorphic hardware.
Mathematically, the problem that we consider is connected to the problem of generalization (29;
27) since the latter also considers the expected loss change under a small random perturbation
EW +U L(W + U) - L(W), except that these papers consider Gaussian noise and we consider
Bernoulli noise. Evidence (32), however, shows that sometimes networks that generalize well are not
necessarily fault-tolerant. Since the tools we develop for the study of fault tolerance could as well be
applied in the context of generalization, they could be used to clarify this matter.
12Variance for P2 is derived in the supplementary material
8
Under review as a conference paper at ICLR 2020
References
[1]	D. Amodei and D. Hernandez. AI and compute. Downloaded from https://blog.openai.com/ai-
and-compute, 2018.
[2]	D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane. Concrete problems
in ai safety. arXiv preprint arXiv:1606.06565, 2016.
[3]	A. P. Arechiga and A. J. Michaels. The robustness of modern deep learning architectures against
single event upset errors. In 2018 IEEE High Performance extreme Computing Conference
(HPEC), pages 1-6. IEEE, 2018.
[4]	Y. Bengio and R. De Mori. Use of neural networks for the recognition of place of articulation.
In Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference
on, pages 103-106. IEEE, 1988.
[5]	B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndic, P. Laskov, G. Giacinto, and F. Roll.
Evasion attacks against machine learning at test time. In Joint European conference on machine
learning and knowledge discovery in databases, pages 387-402. Springer, 2013.
[6]	C.-T. Chiu et al. Robustness of feedforward neural networks. In IEEE International Conference
on Neural Networks, pages 783-788. IEEE, 1993.
[7]	M. Costanzo, B. VanderSluis, E. N. Koch, A. Baryshnikova, C. Pons, G. Tan, W. Wang, M. Usaj,
J. Hanchard, S. D. Lee, et al. A global genetic interaction network maps a wiring diagram of
cellular function. Science, 353(6306):aaf1420, 2016.
[8]	E. El Mhamdi, R. Guerraoui, and S. Rouault. The hidden vulnerability of distributed learning in
Byzantium. In Proceedings of the 35th International Conference on Machine Learning, vol-
ume 80 of Proceedings ofMachine Learning Research, pages 3521-3530, Stockholmsmassan,
Stockholm Sweden, 10-15 Jul 2018. PMLR.
[9]	S. K. Esser, R. Appuswamy, P. Merolla, J. V. Arthur, and D. S. Modha. Backpropagation
for energy-efficient neuromorphic computing. In Advances in Neural Information Processing
Systems, pages 1117-1125, 2015.
[10]	A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S. Thrun.
Dermatologist-level classification of skin cancer with deep neural networks. Nature,
542(7639):115, 2017.
[11]	B. Ghorbani, S. Krishnan, and Y. Xiao. An investigation into neural net optimization via hessian
eigenvalue density. arXiv preprint arXiv:1901.10159, 2019.
[12]	J. I. Glass, N. Assad-Garcia, N. Alperovich, S. Yooseph, M. R. Lewis, M. Maruf, C. A.
Hutchison, H. O. Smith, and J. C. Venter. Essential genes of a minimal bacterium. Proceedings
of the National Academy of Sciences, 103(2):425-430, 2006.
[13]	N. C. Hammadi and H. Ito. A learning algorithm for fault tolerant feedforward neural networks.
IEICE TRANSACTIONS on Information and Systems, 80(1):21-27, 1997.
[14]	S. Haykin. Neural networks and learning machines, volume 3. Pearson Education Upper Saddle
River, 2009.
[15]	N. Kalra and S. Paddock. How many miles of driving would it take to demonstrate autonomous
vehicle reliability. Driving to Safety, 2016.
[16]	P. Kerlirzin and F. Vallet. Robustness in multilayer perceptrons. Neural computation, 5(3):473-
482, 1993.
[17]	N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch train-
ing for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836,
2016.
9
Under review as a conference paper at ICLR 2020
[18]	A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in neural information processing Systems, pages 1097-1105,
2012.
[19]	N. Le Roux and Y. Bengio. Continuous neural networks. In Artificial Intelligence and Statistics,
pages 404-411, 2007.
[20]	Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
[21]	G. Li, S. K. S. Hari, M. Sullivan, T. Tsai, K. Pattabiraman, J. Emer, and S. W. Keckler. Under-
standing error propagation in deep learning neural network (dnn) accelerators and applications.
In Proceedings of the International Conference for High Performance Computing, Networking,
Storage and Analysis, page 8. ACM, 2017.
[22]	K. Mehrotra, C. K. Mohan, S. Ranka, and C.-t. Chiu. Fault tolerance of neural networks.
Technical report, DTIC Document, 1994.
[23]	E. M. E. Mhamdi and R. Guerraoui. When neurons fail. In 2017 IEEE International Parallel
and Distributed Processing Symposium (IPDPS), pages 1028-1037, May 2017.
[24]	S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturba-
tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 1765-1773, 2017.
[25]	A. S. Morcos, D. G. Barrett, N. C. Rabinowitz, and M. Botvinick. On the importance of single
directions for generalization. In International Conference on Learning Representations, 2018.
[26]	A. F. Murray and P. J. Edwards. Enhanced mlp performance and fault tolerance resulting from
synaptic weight noise during training. IEEE Transactions on neural networks, 5(5):792-802,
1994.
[27]	V. Nagarajan. Deterministic PAC-Bayesian generalization bounds for deep networks via
generalizing noise-resilience. coRR, pages 1-36, 2019.
[28]	S. Navlakha and Z. Bar-Joseph. Distributed information processing in biological and computa-
tional systems. Communications of the ACM, 58(1):94-102, 2015.
[29]	B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring Generalization in Deep
Learning. coRR, 2017.
[30]	B. Neyshabur, S. Bhojanapalli, and N. Srebro. A pac-bayesian approach to spectrally-normalized
margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.
[31]	W. Niemiro and P. Pokarowski. Fixed precision mcmc estimation by median of products of
averages. Journal of Applied Probability, 46(2):309-329, 2009.
[32]	D. S. Phatak. Relationship between fault tolerance, generalization and the vapnik-chervonenkis
(vc) dimension of feedforward anns. In Neural Networks, 1999. IJCNN’99. International Joint
Conference on, volume 1, pages 705-709. IEEE, 1999.
[33]	D. S. Phatak and I. Koren. Complete and partial fault tolerance of feedforward neural nets.
IEEE Transactions on Neural Networks, 6(2):446-456, 1995.
[34]	V. Piuri. Analysis of fault tolerance in artificial neural networks. Journal of Parallel and
Distributed Computing, 61(1):18-48, 2001.
[35]	C. D. Schuman, T. E. Potok, R. M. Patton, J. D. Birdwell, M. E. Dean, G. S. Rose, and J. S.
Plank. A survey of neuromorphic computing and neural networks in hardware. arXiv preprint
arXiv:1705.06963, 2017.
[36]	W. Schwarting, J. Alonso-Mora, and D. Rus. Planning and decision-making for autonomous
vehicles. Annual Review of Control, Robotics, and Autonomous Systems, 2018.
10
Under review as a conference paper at ICLR 2020
[37]	D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. nature, 529(7587):484, 2016.
[38]	K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
[39]	S. Sonoda and N. Murata. Double continuum limit of deep neural networks. In ICML Workshop
Principled Approaches to Deep Learning, 2017.
[40]	J. Stilgoe. Machine learning, social learning and the governance of self-driving cars. Social
studies ofscience, 48(1):25-56, 2018.
[41]	C. Torres-Huitzil and B. Girau. Fault and error tolerance in neural networks: A review. IEEE
Access, 5:17322-17341, 2017.
[42]	T.-W. Weng, P.-Y. Chen, L. M. Nguyen, M. S. Squillante, I. Oseledets, and L. Daniel.
Proven: Certifying robustness of neural networks with a probabilistic approach. arXiv preprint
arXiv:1812.08329, 2018.
[43]	D. Zou, R. Balan, and M. Singh. On lipschitz bounds of general convolutional neural networks.
arXiv preprint arXiv:1808.01415, 2018.
11
Probabilistic Fault Tolerance
of Neural Networks
in the Continuous Limit.
Supplementary material
Anonymous authors
September 2019
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
1	Introduction
First we prove all the propositions (labeled Proposition 1, 2, ...) from the main paper. Their names and sections
match the main paper. We also give additional results, they are labelled Additional Proposition 1, 2, etc. If they
are formal statements of the results referred to in the main paper, they are in the same section as the reference.
We abbreviate Assumption 1 → A1, Proposition 1 → P1, Theorem 1 → T1, Definition 1 → D1.
Less precise statements with possible future research directions on fundamental questions required to make the
guarantee even more strong are flushed right.
2	Definition of Probabilistic Fault Tolerance
Notations. For any two vectors x, y ∈ Rn we use the notation (x, y) = in=1 xiyi for the standard scalar product.
Matrix γ-norm for γ = (0, +∞] is defined as kAkγ = supx6=0 kAxkγ /kxkγ. We use the infinity norm kxk∞ = max |xi|
and the corresponding operator matrix norm. We call a vector 0 6= x ∈ Rn q-balanced if min |xi | ≥ q max |xi|. We
denote [n] = {1, 2,..., n}. We define the Hessian Hij = ∂* 2y(x)∕∂xi∂xj as a matrix of second derivatives. We write
layer indices down and element indices up: Wlij . For the input, we write xi ≡xi . If the layer is fixed, we omit its
index. We use the element-wise Hadamard product (x y)i = xiyi .
Definition 1. (Neural network) A neural network with L layers is a function yL : Rn0 → RnL defined by a tuple
(L,W,B,夕)with a tuple of weight matrices W = (Wι,…，Wl) (or their distributions) of size Wi: nι X nι-ι,
biases B = (bi,…/l) (or their distributions) of size bi ∈ Rnl by the expression yi =夕(zi) Withpre-activations
zi = Wiyi-i + bi, l ∈ [L], yo = X and yL = zl. Note that the last layer is linear. We additionally require φ to
be 1-Lipschitz 1. We assume that the network was trained using input-output pairs x,y* 〜X × Y using ERM
for a loss ω. Loss layer for input X and the true label y*(x) is defined as yL+1(x) = Ey*~γ∣χ<ω(yL(x),y*)) with
ω∈[-1,1]3
Definition 2. (Weight failure) Network (L,W,B,夕)with weight failures U of distribution U 〜D∣(x,W) is the
network (L, W + U, B, φ) for U 〜D|(x, W). We denote a (random) output of this network as yw+U(x) = yL(x)
with activations yi and pre-activations ^i, as in D1.
11-LiPsChitz φ s.t. |夕(x)一夕(y)| 6 |x — y|. If φ is K-Lipschitz, We rescale the weights to make K = 1: Wij → Wij/K. This is the
general case. Indeed, if We rescale φ(x) → Kφ(x), then, yι-1 → Ky10-1, and in the sum z0 = E Wij/K ∙ Kyι-1 ≡ zι
2Empirical Risk Minimization — the standard task 1/k Pkm=I ω(yL(xk),y^) → min
3The loss is bounded for the proof of Algorithm 1’s running time to work
1
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
Definition 3. (Bernoulli neuron failures) Bernoulli neuron crash distribution is the distribution with i.i.d. £；〜
Be(pi), Uij = —ξi ∙ Wij. For each possible crashing neuron i at layer l we define Ul = Pj ∣gij∣ and Wj = Pj |W；j |,
the crashed incoming weights and total incoming weights. We note that we see neuron failure as a sub-type of weight
failure.
Definition 4. (Output error for a weight distribution) The error in case of weight failure with distribution D|(x, W)
is ∆i (x) = yiW +U (x) — yiW (x) for layers l ∈ [L + 1]
We extend the definition of ε-fault tolerance from [18] to the probabilistic case:
Definition 5. (Probabilistic fault tolerance) A network (L,W,B,夕)is said to be (ε,δ)-fault tolerant over an input
distribution (x,y*)〜X X Y and a crash distribution U 〜D∣(x,W) if P(χ,y*)〜x×y,u〜d∣(x,w){Δl+i(x) ≥ ε} ≤ δ.
For such network, we write (W, B) ∈ FT(L,夕,p,ε,δ).
3	The Hardness of The Fault Tolerance
3.1	NP-Hardness
Proposition 1. The task of evaluating E∆k for any k = 1, 2, ... with constant additive or multiplicative error for
a neural network with φ ∈ C∞, Bernoulli neuron crashes and a constant number of layers is NP-hard.
Proof. To prove that a problem is NP-hard, it suffices to take another NP-hard problem, and reduce any instance
of that problem to our problem, meaning that solving our problem would solve the original one.
We take the NP-hard Subset Sum problem. It states: given a finite set of integers xi ∈ Z, i ∈ [M], determine if
there exist a non-empty subset S ⊆ [M] such that	xi = 0.
x∈S
We take a subset sum instance xi ∈ Z and feed it as an input to a neural network with two first layer neurons
with φ being a piecewise-linear function from 0 to 1 at points —ε and ε for some fixed ε ∈ (0,1). Note that in
this proof We only compute φ at integer points, and therefore it is possible re-define φ ∈ C∞ such that it has same
values in natural points.
Note that inputs to it are integers, therefore the outputs are 1 if and only if (iff) the sum is greater than zero.
First neuron has coefficients all 1s and second all —1s with no bias. The next neuron has coefficients 1 and 1 for
both inputs, a bias —1.5 and threshold activation function, outputting 1 only if both inputs are 1. Again, since
inputs to this neuron are integers, we can re-define φ to be C∞. The final neuron is a (linear) identity function to
satisfy Definition 1. It takes the previous neuron as its only input. Now, we see that y(x) = 1 if and only if the
sum of inputs to a network is 0.
We now feed the entire set S to the network as its input x. In case if y(x) = 1 (which is easy to check), we have
arrived at a solution, as the whole set has zero sum. In the following we consider the case when y (x) = 0.
Suppose that there exist an algorithm calculating answer to the question E∆k > z for any finite-precision z .
Now, the expectation has terms inside E∆k = P p|s| (1 —p)1-|s|yk(xs). Suppose that one of the terms is non-zero.
s∈S
Then y(x s) 6= 0, which means that the threshold neuron outputs a value > 0 which means that sum of its inputs
is greater than 1.5. This can only happen if they are both 1, which means that sum for a particular subset is both
≥ 0 and ≤ 0. By setting z = 0 we solve the subset sum problem using one call to an algorithm determining if
E∆k > 0. Indeed, in case if the original problem has no solution, the algorithm will output E∆k ≤ 0 as there are
no non-zero terms inside the sum. In case if there is a solution, there will be a non-zero term in the sum and thus
E∆k > 0 (all terms are non-negative).
Now, suppose there exist an algorithm which always outputs an additive approximation to E∆k giving numbers
μ and ε such that E∆k ∈ (μ 一 ε,μ + ε) for some constant ε. Take a network from previous example with additional
layer scaling output by C = PM. Take a subset sum instance xl. Output YES iff 0 ∈ (μ 一 ε,μ + ε). This is correct
because if 0 ∈ (μ 一 ε,μ + ε), then it must be that E∆k > 0 and vice versa. Multiplicative approximation has an
analogous proof where we scale the outputs even more.	□
2
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
We note that computing the distribution of ∆ for one neuron, binary input and weights and a threshold activation
function is known as noise sensitivity in theoretical Computer Science. There exists an exact assessment for
W11i ≡
wi = 14 , however for the case wi 6= 1 the exact distribution is unknown [6].
3.2	Pessimistic Spectral Bounds
Additional Proposition 1 (Norm bound or bound b1). For any norm ∣∣ ∙ ∣∣ the error Δl at the last layer on input
x and failures in input can be upper-bounded as (for ξ x being the crashed input)
k∆L∣ 6 ∣Wl∣∙ ... ∙∣Wιkkξ Θ x-xk
Proof. We assume a faulty input (crashes at layer 0). By Definition 4 the error at the last layer is Δl = yL — yL.
By definition 1,nL = WLyL-I + bL and by Definition 3, yL = Wl^l-i + bL. Thus, ∣∣∆l∣∣ = ∣WLyL-ι + bL —
WLyL-ι — bL∣ = ∣∣Wl(0l-i — yL-ι)k ≤ IlWLIlI∣0l-i - yL-ι∣.
Next, since i^L-ι = 夕(WL-i0l-2 + bL-ι) and yL-ι = 夕(WL-1yL-2 + bL-ι), We USe the I-LiPSchitzneSS of 夕.In
particular, we use that for two vectors X and y and 夕(x),夕(y) applied element-wise, we have ∣夕(x)一夕(y)k ≤ ∣∣x — y∣
because the absolute difference in each component is less on the left-hand side. We thus get ∣∣yL-ι — yL-ι∣ ≤
IlWL-IyL-2 + bL-1 — WL-iyL-2 - bL-ik = ∣∣WL-ι(yL-2 - yL-2)k ≤ ||卬工-11汝工-2 - yL-2∣∙ Plugging it in to the
previous equation, we have ∣Δl∣∣ ≤ IlWLI∣ ∙ IlWL-IIlIlyL-2 - yL-2∣∣
Now, since the inner layer act in a similar manner, we inductively apply the same argument for underlying layers
and get
∣∆L∣≤∣Wl∣∙ ... ∙∣Wι∣∣x-Xk
Moreover we have failing input, thus X = ξ Θ X which completes the proof.	□
Proposition 2 (K using spectral properties [21]). ∣∣yL(x2) - IyL(xι)∣2 6 IM - xι∣ ∙ QL=IkWlk2
Proof. Application of API for ∣∣ ∙ ∣ = k T∣2 and X2 = ξ Θ X
□
We see, there are bounds considering different norms of matrices IAIγ (AP1) or using the triangle inequality
and absolute values (AP2). They still lead to pessimistic estimates. All these bounds are known to be loose [19]
due to IW I = supx6=0 IW XI/IXI being larger than the input-specific IW XI/IXI. We will circumvent this issue by
considering the average case instead of the worst case.
Additional Corollary 1 (Infinity norm, connecting [9] to norm bounds). For an input X with C = IXI∞ for
failures at the input with O(1) dependent only on layer size, but not on the weights,
∣∆L∣∞ 6 PCkWL k∞ ∙... ∙∣Wιk∞ ∙ O(1) + O(p2)
Here IXI∞ = max |Xi|
Proof. First we examine the expression (4) from the other paper [9] and show that it is equivalent to the result we
are proving now:
LL
Erf=EI∆I∞ ≤XClflKL-lwm(L) Y (Nl0 -fl0)wm(l0)
l=1	l0=l+1
here we have Cl the maximal value at layer l, K the Lipschitz constant, wm is the maximal over output neurons
(rows of W ) and mean absolute value over input neurons (columns of W ) weight, fl is the number of crashed
neurons.
Now we set f1 = pN1 and fi = 0 for i > 1 and moreover we assume K = 1 as in the main paper.
4 As we only have one neuron, the index is one-dimensional
3
Therefore the bound is rewritten as:
L
Ek∆k∞ ≤C1N1pwm(L) YNl0wm(l0)
l0=2
Now we notice that the quantity Nlwml = kWl k∞ and therefore
N1
Ek∆k∞ ≤ pCι-1 ∣∣W2∣∣∞∙...∣∣Wl∣∣∞
NL
Now we assume that the network has one more layer so that the bound from [9] works for a faulty input in the
original network:
N0
ElIδ∣∣∞ ≤pCy~r~IlWIIl∞ ∙... ∙ IlWLIι∞
NL
Here C = max{∣x∕} = ∣∣x∣∞. Next We prove that result independently using Additional Proposition 1 for ∣∣ ∙ ∣∣∞
k∆L∣∞ ≤ IlWLll∞ ∙... ∙ ∣Wιk∞kξ ΘX - xk∞
NoW We calculate EIξ x - xI∞ . We Write the definition of the expectation With f (p, n, k) = pk (1 - p)n-k =
pk + O(pk+1 ) being the probability that a binary string of length n has a particular configuration With k ones, if
its entries are i.i.d. Bernoulli Be(p). Here Sl is the set of all possible netWork crash configurations at layer l. Each
configuration sl ∈ Sl describes Which neurons are crashed and Which are Working. We have |Sl | = 2nl .
EIξ x - xI∞ =	f(p, n, |s|) max{s	x - x}
s∈S
NoW since for |s| = 0 the max{s x - x} = max{x - x} = 0, We consider cases |s| = 1 and |s| > 1. For |s| > 1
the quantity f(p, n, |s|) = O(p2) and therefore, in the first order:
EIξ x - xI∞ = pX |xi| + O(p2) ≤ pN0IxI∞ + O(p2)
i
Next We plug that back into the expression for EI∆I∞ :
E∣∣∆∣l∞ ≤ IWlI∞ ∙ ... ∙ kW1k∞pN0kxk∞ + O(p2)
106
107
108
109
110
111
112
113
114
115
NoW We note that this expression and the expression from [9] differ only in a numerical constant in front of the
bound: N0 instead of No, but the bounds behave in the same way with respect to the weights.
□
Additional Proposition 2 (Absolute value bound or bound b2). The error on input x can be upper-bounded as:
E∣Δl∣ 6 p∣Wl∣∙...∙∣Wi∣∣x∣
For |W| being the matrix of absolute values (|W |)ij = |Wij |. |x| means component-wise absolute values of the
vector.
Proof. This expression involves absolute value of the matrices multiplied together as matrices and then multiplied
by an absolute value of the column vector. The absolute value of a column vector is a vector of element-wise
absolute values.
We assume a faulty input. By Definition 4, the error at the last layer is Δl =勺匕—yL. By definition 1,
yL = WLyL-I + bL and by Definition 3, yl = Wl^l-i + bL. Thus for the i,th component of the error,
∣∆L∣ = ∣WLyL-1 + biL — WLyL-ι - bLl = WL(yL-ι — yL-ι)l = I EWij(yL-1 - yL-ι)l
j
4
By the triangle inequality,
116
117
118
119
120
121
122
123
124
125
126
127
128
∆L∣ ≤ EwLjHyL-1 - yjL-1∣ =(严力|班-1 -yL-ι∣)i
j
Next we go one level deeper according to Definition 1:
|yL-1 - yjL-1| = 3(WL-1yL-2 + bL-1)- iXWL-lUL-2 + bL-I)I
And then apply the I-LiPschitzness property of φ:
|yL-1 - yj-1| ≤ IWL-1yL-2 + bL-1 - WL-1yL- - bL-1| = IWL-1(yL-2 - yL-2)|
This brings us to the previous case and thus we analogously have
∣Δl∣ ≤ ∣Wl∣∙ ∣Wl-1∣∙ ∣yL-2 - yL-2∣
Inductively repeating these steps, we obtain:
∣Δl∣≤∣Wl∣∙ ... ∙∣W1∣∣X - χ∣
Now we take the expectation and move it inside the matrix product by linearity of expectation:
E∣Δl∣ ≤ E∣Wl∣∙ ... ∙ ∣W1∣∣X - x| = ∣Wl∣ ∙ ... ∙ ∣W1∣E∣X - x|
The last expression involves E∣x — x∣. This is component-wise expectation of a vector and We examine a single
component. Since ξi is a Bernoulli random variable,
E∣χiξi - xi| = P ∙ |xi| + (1 一P) ∙ 0
Plugging it into the last expression for E∣Δl∣ proves the proposition.	□
4 Realistic Simplifying Assumptions for Neuromorphic Hardware
4.1	When is The Weight Perturbation Small?
In this section we give sufficient conditions for which case the probability of large weight perturbation under a crash
distribution is small. First, we define the properties of neuromorphic hardware.
Assumption 1. The probability of failure P = max{Pl l ∈ [L]} is small: P . 10-4..10-3
Assumption 2. The number of neurons at each layer nl is sufficiently big, nl & 102
Additional Assumption 1. Vectors Wli = L IWliL I are q-balanced for q > 0 for each layer l.5
5A similar assumption on an ”even distribution” of weights was made in [20].
5
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
Toy examples. Naturally, we expect the error to decay with an increase in number of neurons n, because of
redundancy. We show that this might not be always the case. First, consider a 1-layer NN with n = n0 and
nL = n1 = 1 at input xi = 1: y(x) = xi/n. This is most robust network, as all neurons are interchangeable as
they are computing the same function each. Essentially, this is an estimate of the mean of ξi given n samples. Here
E∆ = 一 £EgiXi = -P and Var∆ = 1/n£ Varξi = p(1 — p)/n 〜 p/n, variance decays with n. In contrast, the
worst case y(x) = xι has all but one neuron unused. Therefore E∆ = XiEξip and Var∆ = x2Varξi = p(1 — P)〜p,
variance does not decay with n.
This proposition gives sufficient conditions under which the weight perturbation is small and it is less and less
as n increases and P decreases:
Proposition 3. Under A1,2 and AA1, for α > P, the norm of the weight perturbation Uli at layer l is probabilistically
bounded as: δo = P{kg∙ kι ≥ α∣∣Wjk} ≤ exp (—nι ∙ q ∙ dκL(α∣∣pι)) with KL-divergence between numbers a, b ∈ (0,1),
dKL(a, b) = a log a/b + (1 - a) log (1 - a)/(1 - b)
Proof. See [5] for the proof details as this is a standard technique based on Chernoff bounds for Binomial distribution
with P → 0. These are a quantified version of the Law of Large Numbers (which states that an average of
identical and independent random variables tends to its mean). Specifically, Chernoff bounds show that the tail of
a distribution is exponentially small: P{X ≥ EX + εEX} ≤ exp(-cε2).
Specifically, if we consider a case X = Bin(n, P) with P → 0, for which q = 1, we have [5, 1] for α = k/n > P:
P[X ≥ k] ≤ exp —nDKL
G>))
In case if we rewrite k = αn, this gives us the result.
Specifically, if We ConSiderkUlkI = P	|gi|	≈ k∣Wι|.	Therefore, this probability is	P[X	≥	k]	=	P[∣∣U1∣∣1 ≥
i
αkWl-kι]. This shows that the probability that a fraction of Bernoulli successes is more and more concentrated
around its mean, nP. Therefore, it is less and less probable that the this fraction is ≥ α > P.
Factor q appears because in the analysis of the sum
Pn |Wi|2
n
P |Wi|2
i=1
n2W2
≥ ‘° E mm
_ nW2
max
nq
□
4.2	Taylor Expansion for a Small Weight Perturbation
In this section, we develop a more precise expression for E∆ and Var∆. Previously, we have seen that the perfect
fault-tolerant network has Var∆ = O(P/n). In this section, we give sufficient conditions when complex neural
networks behave as the toy example from the previous section as well.
We would like to obtain a Taylor expansion of the expectation of random variable E∆ = T1 + T2 in terms of P
and q = 1/n with r = P + q where T1 = O(P) and T2 = O(r2). For the variance, we want to have Var∆ = T3 + T4
with T3 = O(P/n) and T4 = O(r3). Our goal here is to make this expression decay as n → ∞ as in the toy example.
We will show in Theorem 1, the first-order terms indeed behave as we expect. However, the expansion also contains
a remainder (terms T2 and T4). In order for the remainder to be small, we need additional assumptions. It is easy
to come up with an example for which the first term T1 is zero, but the error is still non-zero, illustrating that the
remainder is important. Consider one neuron working and the rest having zero weights. Consider a summation of
outputs of the neurons, each with a quadratic activation. Then y = (Xi — α)2, and Vχy = 0 at Xi = α. In addition,
E∆ = Θ(P), however, the first term in Taylor expansion is T1 = 0. The remainder here is T2 = O(P) and not O(r2),
no matter how many neurons we have. The problem with this example is discontinuity: one neuron with non-zero
weights is not at all like its neighbors. We thus show that discontinuity can lead to a lack of fault tolerance. Next,
we generalize this sketch and show that some form of continuity is sufficient for the network to be fault-tolerant.
First, we reiterate on the toy motivating example from the previous section. Consider a 1-layer neural network
n
y(X) =	wiXi
i=1
6
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
We assume that all neurons and inputs are used. Specifically, for the q-factor q(x) = max |xi |/ min |xi|, we have
q(x) ≈ q(w)〜1. We are interested in how the network behaves as n → ∞ (the infinite width limit). For the input,
we want the magnitude of individual entries to stay constant in this limit. Thus, |xi | = O(1). Now we look at
the function y(x) that the network computes. Since the number of terms grows, each of them must decay as 1/n:
Wi 〜 1/n. The simplest example has X = (1,…,1) and W = (1/n,…,1/n), which results in y(x) ≡ 1 for all n. Now
n
we consider fault tolerance of such a network. We take ∆ = E Wi((1 — ξi) — 1)xi for ξi 〜Be(P) being the indicator
i=1
that the i,th input neuron has failed. Therefore, E∆ = -p∕nfx%Wi = —p, and Var∆ = ExI2w 之 p(1 — P)〜p/n.
We see that the expectation does not change when n grows, but variance decays as 1/n. These are the values
that we will try to obtain from real networks. Intuitively, we expect that the fault tolerance increases when width
increases, because there are more neurons. This is the case for the simple example above. However, it is not the
case if all but one neuron are unused. Then, the probability of failure is always P, no matter how many neurons
there are. Thus, the variance does not decrease with n. We thus are interested in utilizing the neurons we have to
their maximal capacity to increase the fault tolerance.
----Cont. z(t), t∈ [0,1]
Discrete zi, ∕∈ [n]
Figure 1: Discrete (standard) neural network approximating a continuous network
Overall, we have the following plan. We give an expansion of Var∆ in terms of P and q = 1/n, and give sufficient
conditions for which the remainder terms are small. We use the first term explicitly when regularizing the network.
In order for the expansion to work, we formalize the difference between ”all neurons doing the same task” and ”all
but one are unused”. Specifically, we define a class of ”good” networks for which fault tolerance is sufficient, via
the continuous limit [23].
Functions, functionals and operators. We call maps from numbers to numbers as functions, maps from
functions to numbers as functionals and maps from functions to functions as operators.
We consider a subset of the space of real-valued functions with domain T : F(T) = BPC∞(T) for T = [0, 1].
This is a space of bounded piecewise-continuous functions f ∈ F, f : T → R such that there is a finite set of
points {xi} ⊂ T such that f ∣(χi,χi+ι) ∈ C∞(T) — infinitely-differentiable. We note that F ⊂ L1 which means that
Rt∈τ ∣f (t)∣dt < ∞. In addition, R |fk(t)| < ∞. Moreover, for two f,g ∈ F, (f ∙ g)(t) = f(t) ∙ g(t) ∈ F.
Additional Definition 1. (Continuous neural network) A continuous neural network [15] of function class H with
L layers is an operator yL : H → H defined by a tuple (L, W, B, φ) with a tuple of weights functions W = (Wι,…，WL)
with Wl ∈ H(T2) (or their distributions), bias functions B = (b1, ..., bL) (or their distributions) with bl ∈ F by the
expression yι(t)=夕(zι(t)),	zι(t)	= J	Wι(t,t0)yi-i(t0)dt0 +	bι(t),	l ∈ 1,L, zo = yo = x and yL(t)	=	ZL(t).
t0∈T
We note that a regular neural networks has Tι = [nι]. Continuous networks were re-introduced in [15]. The
classes of discrete and continuous networks can be generalized as Deep Function Machines [12], as a special case
when L1-norms of inputs and activations of discrete networks approximating continuous ones increase linearly with
n. In [23], depth of a network is also considered to be continuous. Continuous networks are also called the integral
representation [23]. Previous work considers other limits rather than L1-norm growing linearly, but here we only
consider this case.
Next, we connect continuous and regular (discrete) networks. To do that, we define how each quantity transforms
when n grows.
Additional Definition 2. (Continuous-discrete correspondence with kxk1 = O(n0)) Consider a continuous NNc =
(L, Wc, Bc,φ) and a discrete one NNd = (L, Wd, Bd,夕).We consider a distribution of continuous inputs F 3 x 〜Pc
and a distribution of discrete inputs Rn 3 X 〜Pd. These distributions are linked: each element from Pc corresponds
to exactly one from Pd . We define the approximation error A as
A = sup sup sup
(xc,xd)〜PcXPd Lθ≤l≤L Ll≤i≤nι
Wiy) (C
7
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
We define n = min{n0, ..., nL} the minimal width of the discrete network. If a series of discrete networks NNn
has An → 0, n → ∞ for some NNc, we say that NNn → NNc .
The summary of correspondences between discrete and continuous quantities is shown in Table 1
Quantity	Discrete (D1)	Continuous (AD1)	Relation (AD2, AP4)
Input Norm Bias Weights	x : [n0] → R kxdk1 = Pi |xi| bl : [nl] → R Wl: [nl] × [nl-1] →R	x : [0, 1] → R kxck1 = R |x(t)|dt bl : [0, 1] → R Wl : [0, 1]2 → R	Xi ≈ X (⅛⅛) n0kχdkι ≈ kχckι bi ≈ bi (n⅛) Wlij ≈ ni-1 Wi (n⅛, n⅛)
Pre-activations	zli = Pj Wlijyli-1 + bli	zl(t) = R01Wl(t,t0)yl-1(t0)dt0+bl(t)	Zi ≈ Zl (n⅛)
Number of changes	Cd=Pij Wli+1,j — Wlij	Cc = R R dtdt0 |Wt0(t, t0)|	Cd ≈ Cc
Table 1: Correspondence between discrete and continuous quantities
Additional Proposition 3. Continuous networks with H = F are universal approximators
Proof. Based on the proof of [15]. By the property of discrete networks, they are universal approximators [15].
Take a discrete network y with a sufficiently low error, and define a continuous network N Nc by using a piecewise-
constant W and b from y. Then, NNc ≡ NNd . The weights and biases are bounded and piecewise-continuous with
nι discontinuities at each layer. In addition, all functions are bounded since the weights are finite.	□
In the following we will always use H = F, as it is expressive enough (AP3), and it is useful for us. We give a
sufficient condition for which An → 0 as n → ∞.
Additional Proposition 4. If a discrete network NNn is defined from a continuous network NNc in the following
way, then NNn → NNc with error A = O(1/n). See (Figure 1)
Xi=χ(n≡) ,Wιij = n-ι Wi (n⅛ ,n⅛) Jb =仇 GB
τ Ji r 11	∙ j ʌ	i ——1 r	∙ ι ■ -ι	r	ι r ； ι ∙ ι ∙	∙ ι
In the following We write ii = ；/ for an index i = 1..nι, as the range for each of the indices is known.
Proof. For layer 0, the error is 0 by definition of xi . Also, x and its derivatives are bounded. Suppose we
have shown that the error for layers 1..l - 1 is ≤ ε, and that yl0 is bounded with globally bounded deriva-
tives. Consider ∣(yd)i - (yC)(总)∣ = |bi - bi + Pj Wij y- - R W (i,t')gi-i(t')dtl ≤ |1/ni-i Pj W (ij)(yj-ι +
ε) — R W (i, t0)yι-ι(t0)dt01 ≤ ε R |W ∣dtdt0 + | R f(t)dt — 1/n P f (j)| for f(t) = W (i, t)yι-ι(t). The second term
|T|2
is a Riemann sum remainder and can be upper-bounded as sup |f0(t)∣. We note that |[0,1]| = 1 and that
lf0(t)l = IWi(i,t)yι-ι(t) + W(i, t)y0-ι(t)∣ ≤ sup |W0∣ sup |y| + sup |W| sup ∣y01 < ∞ and does not depend on n or the
input, as the bound is global. Therefore, the error at layer l is ≤ (ε + 1/n)C ≤ D/n for the initial choice ε = 1/n.
Thus, the full error is decaying as 1/n: An = O(1∕n).	□
Using that result, we conclude that for any sufficiently smooth operator y, there exist a sequence NNn ap-
proximating y. First, by [15], continuous networks are universal approximators. Taking one and creating discrete
networks from it (as in AP4) gives the desired result as sup kyc — ydk∞ ≤ A = O(1/n) ≤ ε and kyc — yk ≤ ε by the
x
approximation theorem for continuous nets.
Assumption 3. We assume that NNn → NNd. We assume that there are global reasonable bounds on the
derivatives of the function we want to approximate
su	δk yL(tL)	1
XUP δyι(i1)...δyι(ik) (i1, …,ik)!
≤ Dk, Da:b = max{Da,Da+1, ..., Db}
Here for is ∈ [n] and qj = |{is =j}| we define (i0, ..., ik)!
only need k ∈ {1, 2}.
a multinomial coefficient. In the paper, we
8
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
The limit means that close-by neurons inside a discrete network compute similar functions. This helps fault
tolerance because neurons become redundant. There can be many of such sequences NNn , but for the sake of clarity,
one might consider that gradient descent always converges to the ”same way” to represent a function, regardless
of how many neurons the network has. This can be made more realistic if we instead consider the distribution of
continuous networks to which the gradient descent converges to. In this limit, the distribution of activations at
each layer (including inputs and outputs) stays the same as n grows, and only the number of nodes changes. Note
that this limit makes neurons ordered: their order is not random anymore. The limit works when nl are sufficiently
large. After that threshold, the network stops learning new features and only refines the ones that it has created
already.
The derivative bound part of A3 can be enforced if there is a very good fit with the ground truth function
y(x). Indeed, if y(x) ≡ yL(x), then the derivative of the network depends on the function being approximated. For
discrete-continuous quantities we write Xd ≈ Xc meaning that |Xd - Xc | ≤ ε for a sufficiently large n.
We note that our new assumptions extend the previous ones:
Additional Proposition 5. A3 results in AA1 with
q
(RR W(t,t0MdtdtY
R (R W(t,t0)∣dt)2 dt0
+ o(1), nl → ∞
Proof. A simple calculation:
X |Wi| = n ∙ 1 XX Wij∣→ nZ W(t,t0)∣dtdt0, X |Wi|2 → nZ (/ ∣W(t,t0)dt1∣) dt
Dividing these two gives the result.
□
Can we make the input norm bounded? Sometimes, input data vectors are normalized: kxk = 1 [14]. In
our analysis, it is better to do the opposite: kxk1 = O(n0). First, we want to preserve the magnitude of outputs
to fit the same function as n increases. In addition, the magnitude of pre-activations must stay in the same range
to prevent vanishing or exploding gradients. We extend that condition to the input layer as well for convenience.
Another approach would be to, for example, keep the norm x constant as n grows. In that case, to guarantee
the same output magnitude, input weights must be larger. This is just a choice of scaling for the convenience of
notation. We note that inputs can be still normalized component-wise.
Less weight decay leads to no fault tolerance. Consider the simplest case with Gaussian weights: y = wTx,
Wi 〜 N(0,σ2). Then y 〜 N(0,σ2∣∣x∣∣2). In order to avoid vanishing gradients, We set σ2∣∣xk2 = const. If ∣∣xk2 =
O(n), then we must set σ2 〜1/n for the variance of the output to stay constant. This is a well-known initialization
technique preserving the variance of pre-activations and thus eliminating vanishing or exploding gradients. In
contrast to this, by A3 we preserve the mean activation at each neuron instead, like for stochastic neural networks
[24]. In the Gaussian case, the error ∆ = — PξiXiWi. E∆ = 0 and Var∆ = P x2E(ξi)2Ew2 = σ2p∣xk2 = P ∙ const.
This does not decay with n. A more formal statement can be found in AP6.
The NTK limit. In the NTK limit [14], there is a decay of the variance Var∆ since both ∣∣x∣2 = 1 and σ2 〜1/n,
but that happens because variance decreases in any case, not related to fault tolerance, since σ2∣x∣2 〜 1/n. The
”lazy” regime [7] of the NTK limit implies that every hidden neuron is close to its initialization, which is random.
Thus, there is no continuity in the network: close-by neurons do not compute similar functions. Thus, the NTK
limit is incompatible with our continuous limit.
Below we formalize the claim that a network with constant pre-activations variance is not fault-tolerant. This
means that an untrained network with standard initialization is not fault tolerant.
Additional Proposition 6. The fault tolerance error variance for a network y = wTx with Ewi = 0 and
Var[y(x)] = const does not decay with n
Proof. Consider	y(x)	=	wixi	and ∆ = —	wixiξi.	Since Ewi	= 0,	Ey(x)	=	E∆	= 0. In addition, Vary(x)	=
P Ew2E(xi)2 = const. Then, Var∆ = P Ew2E(xi)2E(ξi)2 = PVary(X) = P ∙ const, not decaying with n	□
Next, we want to harness the benefits of having a continuous limit by A3. We want to bound the Taylor
remainder, and for that, we bound higher-order derivatives. We bound them via the operator derivative of the
continuous limit.
9
Operator derivatives. The derivatives for functionals and operators are more cumbersome to handle because
of the many arguments that they have (the function the operator is taken at, the function the resulting derivative
operator acts on, and the arguments for these functions), so we explicitly define all of them below. We consider
operators of the following form, where x ∈ F and Y [x] ∈ F are functions:
Y [x](t)
= σ W(t, t0)x(t0)dt0
This is one layer from AD1. We define the operator derivative of Y [x] point-wise with the RHS being a functional
derivative:
(δY[x] ʌ (t) _ δ(Y[x]⑴)
V δx ) ( )= δx
For functionals F [x] we use the standard functional derivative:
δFx1 5
δx
For our case F[x] = Ft [x] = Y [x](t), we have
δF[x] 5
δx
lim ε-1
ε→0
lim F[x + 8] - F[X]
ε→0	ε
W (t,t0)(x(t0) + ε2(t0)
W(t, t0)x(t0)dt0
σ0(F[x])
J W(t,t0)夕(t0)dt0
Next, we consider the functional derivative at a point. This quantity is similar to just one component of the
gradient of an ordinary function, with the complete gradient being similar to the functional derivative defined above.
We define the derivative at a point δF[x]∕δx(s) via the Euler-Lagrange equation, since We only consider functionals
of the form Ft [x] = Y [x](t) for some fixed t which are then given by an integral expression
Ft [x] = Y [x](t) = σ
/ W(t,t0)x(t0) dt0
∖	L(t0,χ(t0),χ0(t0)),
We define the inner part G[x] =	W(t, t0)x(t0)dt0, thus, F[x] = σ(G[x]). In this case, since the integral only
depends on the function x explicitly, but not on its derivatives, the functional derivative at point s is
坐=σ0(G[x])件 -信
δx(s)	∂x ds ∂x0
σ1(G[x]) IL
σ0(G[x])W (t, s)
The definition of a functional derivative at a point δF[x]∕δx(s) (XcomPonent of the gradient”) can be reconciled
with the definition of the functional derivative δF[x]∕δx (”full gradient”) if we consider the Dirac delta-function:
δFx⅛=δ(t0 - s)]=σ0(G[x]) IW (t,t')δ(t' - s)=σ0(G[x])w (t,s) ≡ δFχ]
We define the operator derivative at a point in a point-wise manner via the functional derivative at a point:
δ δY [xn (t)=δY [x](t)
vδχ(sy J()= δx(s)
275
276
277
278
279
280
281
282
283
Having that definition, we compute for our case (δY[x]∕δx(s))(t) = σ0(Gt[x])W(t, s).
Now we see that the rules for differentiating operators in our case are the same as the well-known rules for
the derivatives of standard vector-functions. Indeed, if we consider yi (x) = σ( j Wij xj ) with the inner part
gi(x) = ∑j Wijxj giving yi(x) = σ(gi(x)), then ∂y%∕∂xk = σ0(g(x))Wik. This looks exactly like the expression
for ∂Y[x](t)∕∂x(s). By induction, this correspondence holds for higher derivatives as well. However, this does not
imply that these quantities are equal. In fact, we will show that they differ by a factor of 1/nlk where k is the order
of the derivative.
i1	ik
We characterize the derivatives of a discrete NN ∂kyL∕∂yll ...∂yj in terms of operator derivatives of a continuous
NN δkyL∕δyι(i1)...δyι(ik). We only assume the continuous limit:
Proposition 4. For a sequence NNn → NNc with φ ∈ C∞, the derivatives decay as:
∂kyL k = 4	, f yL , k、+ o(1), nι →∞
∂yil ...∂yil	nk δyι(i1)...δyι(ik)
10
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
Intuitively, this means that the more neurons we have, the less is each of them important. First, consider a
simple example y = σ(E Wixi). Here the weight function is Wi = 1/n, w(i) = 1, Xi = 1, and A = 0. Then
∂y∕∂xi = σ0(∙)wi 〜 1/n and ∂2y∕∂xi∂xj = σ00(∙)wiWj 〜 1/n2. We note that the expression inside the sigmoid has
a limit and it’s close to the integral by continuity of σ0 and σ00 .
Proof. Now we prove P4. Consider the first and second derivatives. Note that the operator derivatives only depend
on the number of layers and the dataset. It does not depend on any n anymore.
∂yiL
∂xio
WLiLiL-1...W1i1i0σ0(zLiL)...σ0(z1i1)
iL-1 ,...,i1
≈ 力如 W1(t1,M(z1(t1)).../dtJWL-1(tL-1,J) ∙ WL(tL,tL-1) ∙σ0(Zj(J))
1 ∂yWL
no ∂x(i0)
By definition of a neural net
continuous limit
operator 〜ordinary
Crucially, the factor 1/n0 appears because we do not sum over i0 , as it is fixed, but we have a weight vector
Wi 〜1/n。nevertheless. For all other indices, We have a weight matrix Wl 〜 1∕nι-ι as well as a summation over
il-1.
∂2yiL
∂xio ∂xi0
X	WLiLiL-1...W1i1i0 Xσ00(zsis) Y σ0(zsis00) X Wsi0si0s-1...W1i01i00
iL-1 ...i1	s=1	s0 6=s	i0s...i01
From previous
≈ ɪ δ2UL(iL
n2 δx(io)δx(i0)
continuous+operator 〜ordinary
Here, the factor 1/n0 appears because we never sum over i。, but the weight matrix Wi 〜 1/n。appears twice. □
When does the limit hold? Now, we have assumed that a network has a continuous limit in A3. However,
this might not be the case: the NTK limit [14] is an example of that, as weights there stay close to their random
initialization [7], thus, they are extremely dissimilar.
1.	Explicit duplication. We copy each neuron multiple times and reduce the outgoing weights. If we set
W(t, t0) to be piecewise-constant, then the approximation error is zero A = 0, and it does not depend on
the degree of duplication. This is the obvious case where the network is becoming more fault-tolerant using
duplication, and our framework confirms that. The problem with explicit duplication of non-regularized
networks is that their fault-tolerance is suboptimal. Not all neurons are equally important to duplicate.
Thus, it’s more efficient to utilize all the neurons in the best way for fault tolerance by duplicating only the
important ones.
2.	Explicit regularization. We make adjacent neurons compute similar functions, thus, allowing for redun-
dancy. We first consider the local ”number of changes” metric (Table 1). Specifically, for some function
z(t) ∈ [0, 1],	|z0(t)|dtrepresents how many times the function goes fully from 0 to 1 or vice versa. Our idea
is to use that for the weights to quantify their discontinuity:
CI = XIWlij-Wli+1,jl = no XIW (i,j)-W(i +1j)1
ij	ij
Wt0
ij


IWt0 (t, t0 )Idtdt0
11
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343	3.
344
The term above, if small, guarantees that, for each input neuron, neighboring output neurons will use it in
similar ways. The same is applied for WT as well: C2 = n0/n1 Pij |Wij - W i,j+1 | ≈ R |Wt00 (t, t0)|dtdt0 which
guarantees that, for each output neuron, neighboring input neurons are used similarly by it.
In addition to making adjacent neurons computing similar functions, we add another term C3 by Gaussian
smoothing: a Gaussian kernel is convolved with the weights, and the weights are subtracted from the result.
The difference shows how much the current value differs from an aggregated local ”mean” value.
We explicitly enforce the continuous limit by adding a regularization term of smooth(W) := C1 +C2+C3. Here
smooth consists of three parts, see FilterPlayground.ipynb . The first part C1, C2 computes the numerical
derivative with 2, 4, 10, 14 points in the array. The second part convolves the input with a Gaussian kernel and
subtracts the original, resulting in a measure of discontinuity. All metrics are normalized to work with any
size, so that scaling the network up does not change the magnitude of Ci significantly. The implementation
can be found in continuity.py.
We check the derivative decay prediction (P4) which must follow from the continuity assumption A3, exper-
imentally. We run experiments for the MNIST dataset and architecture (784, N, 100,10) - 2 hidden layers
with sigmoid activations, unit Lipshitz coefficient, batch size of 1000, and N from 50 to a few thousand. We
measure kW k∞ = maxi j |Wij | ≈ maxt t0 |W (t, t0)|dt0 which should stay constant (we measure the product
of these over layers l, as only the total norm is expected to stay constant, whereas individual layers can change
the magnitude of W). We also measure the components of the first derivative avg(|Di |) and the components
of the Hessian avgi(|Hii |), avg(ij) (|Hij |) which we expect to decay with n as by P4. We show that without
regularization, the weights kW k∞ increase, see WeightDecay-FC-MNIST.ipynb, and the derivatives and Hes-
sians do decay but with a smaller rate, insufficient for T1 to work, see DerivativeDecay-FC-MNIST.ipynb.
We repeat each experiment 10 times and report mean and standard deviation, see Figure 2. In contrast,
our proposed regularization results in a smooth transition between neurons at each layer. They are grouped
by their similarity. This can be visually seen in WeightDecay-Continuity-FC-MNIST.ipynb or in Figure
2. Contrary to networks without regularization, first layer weight profiles seem to have a meaningful image,
and these images are similar for close-by neurons. There is also an increase in continuity of activations for
a fixed input. For this regularization technique, we show that the product of the weights stabilizes, and the
derivatives decay with proper slopes of -1 and -2. The accuracy, however, drops from 98% to 90%. We note
that this does not demonstrate that our approach necessarily leads to a decrease in accuracy, as continuous
networks are general by AP3.
We test the result on Fashion MNIST as well (Figure 3, notebook names are the same but with Fashion) and
report similar behavior. On the Boston Housing dataset, non-regularized networks already have the desired
characteristics (Figure 4).
Note: the condition above of C1 + C2 + C3 being small is, strictly speaking, only a necessary condition for
A3, but not a sufficient one. Even if networks are smooth enough, they might not have a limit NNn → NNc,
as they could implement the function in drastically different ways: for example, the networks NNn can be
all smooth, but approximate different continuous functions. However, this condition is sufficient for the
derivatives Dk to stay constant (second part of A3), which is the only requirement for T1 to work. a Thus,
our approach can give a formal guarantee of fault tolerance. An attempt to give a truly sufficient condition
for A3 would be to train the bigger network given duplicated weights of a smaller network, and penalizing
a bigger network from having weights different from their initialization.
Different smoothing techniques. Currently, C2 is unused in our implementation, as it was sufficient to
use C1 + C3 to achieve the fault tolerance required in our experiments. Another method to make close-by
neurons similar could be to use some clustering loss in the space of weights accounting for locality, like the
Kohonen self-organizing map. One more idea is to regularize the weights with a likelihood of a Gaussian
process, enforcing smoothness and a required range of dependencies. Another idea is to use the second
derivative, which is connected to the curvature of a curve (x, y(x)) in the 2D space: κ = y00/(1 + (y0)2)3/2.
The interpretation here is that κ = 1/R for R being the radius of curvature, a geometrical property of a
curve showing how much it deviates from being a line.
aA discussion of why Dk stay constant is given in the analysis of the correctness for Algorithm 1.
Convolutional networks. For images, this limit naturally corresponds to scaling the images [12] with
intermediary pixels being just added between the original ones. Images are piecewise-continuous because
12
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
adjacent pixels are likely to belong to the same object having the same color, and there are only finitely many
objects on the image. Convolutional networks, due to their locality, fit our assumption on one condition. We
need to have a large enough kernel size, as otherwise the non-smoothness is high. Specifically, for CNNs, C1
is small, as neighboring neurons have similar receptive fields. In contrast, C2 can be large in case if the kernel
size is small: for example, the kernel (-1, 1) in the 1D case will always result in a high discontinuity: the
coefficients are vastly different and require more values in between them to allow for a continuous limit and
redundancy.
The notebook ConvNetTest-VGG16-ManyImages.ipynb investigates into this. We note that we do not present
this result in the main paper and make it a future research direction instead. In this paper, we give qualitative
description of applicability of our techniques to convolutional networks (big kernel size for a small C2 with
respect to kernel size, smooth activation and pooling to allow for T1 to work), as it would be out of the scope
of a single paper to go into details.
4. Theoretical considerations for the general case.
How does the network behave when the number of neurons increases, and it is trained with gradient descent
from scratch? First, there are permutations of neurons, which we ignore. Secondly, there could be many
ways to represent the same function. One constraint is that the magnitude of outputs in the output
layer is preserved. Intermediate layers need to have a non-vanishing pre-activation values to overcome
vanishing/exploding gradients. In addition, input limit might be enforced such that xi ≈ x(i). Now,
gradient descent results in a discrete network NNd which can be seen as a discretization of some continuous
network NNc. Since NN’s derivatives are globally bounded, GD converges to a critical point. Each critical
point determines the range of initializations which lead to it, partitioning the whole space into regions. Each
fixed point with a sufficiently low loss thus corresponds to a set of continuous networks ”passing through” a
resulting discrete network. Each of the continuous networks can have different implementations in discrete
networks of larger size. We choose a path in networks of different sizes n and denote the probability sn over
initializations to choose that particular continuous network. Therefore, on that path, derivatives decay as we
want since NNn → NNc. The problem might arise if a particular continuous limit NNc has an extremely
small probability (over initializations) of gradient descent giving NNn: if sn → 0, this particular network
NNc is unlikely to appear. We leave the study of this as a future research direction.
Now we use the derivative decay from P4 to show fault tolerance using a Taylor expansion. We write q = 1/nl
and r = P + q. In the following We will use Assumption 3 only by its consequence - Proposition 4. We note that
the conclusion of it can hold in other cases as well. We just give sufficient conditions for which it holds.
Theorem 1. For crashes at layer l and output of layer L under assumption 3 the mean and variance of the error
can be approximated as
nl	nl	2
E∆l = pi X ~∂ξi + Θ±(1)D2r2, Var∆L = Pl X ( ∂ξ-J +Θ±⑴D^r3
i=1	l	i=1	l
By Θ士(1) We denote any function taking values in [—1,1]. The derivative ∂yL/∂ξ∣(ξ) ≡ -∂yL(yl — ξ Θ yl)∕∂yli ∙ yli
is interpreted as if ξli was a real variable.
Proof. We consider crashes at layer l as crashes in the input x to the rest of the layers of the network. Thus,
without loss of generality, we set l = 0.
Consider ∆(ξ) = y((1 — ξ) Θ x) — y(x). Then we explicitly compute
、-----V----}
企⑹
∂ △⑹	∂y(X(ξ))	∂ △⑹	∂2y(X(ξ))
______ _____________ʃv*. _______ ____________ʃv* . ʃv* .
∂ξi	∂xi	2，∂ξi∂ξj	∂xi ∂xj 2 3
Now, consider ∆(ξ) = ∆(0) + (∆o(0), ξ) + 2ξτ∆00(t(ξ))ξ by the Taylor theorem with a Lagrange remainder.
13
9π'N"do一S w~PlH)60一
H dιag
_ ____
Uural-S
60000-
W prod
5	6	7
∣og(∏)
20000
5	6	7	O
∣og(n)
val acc
1 O
9 9
l
Uural-S
107
PO-KrM
Figure 2: Non-regularized MNIST (first two rows), regularized MNIST (second two rows). Odd rows show the
charts of the decay of first derivatives (D) and second derivatives (average over all i, j Hessian’s components |Hij |,
and only the diagonal elements |Hii |). Next, validation accuracy is shown as well is the product of the infinity
norms of the weights ∣∣Wl∣∣∞ •…∙∣∣Wi ∣∣∞. Even rows show first-layer weights as images
W prod
1250
We assume kxk∞ ≤ 1 (otherwise we rescale W1 ). We group the terms into distinct cases i = j and i 6= j :
E∆ = -p∆0(0) +	E(ξi)2∆00(t(ξ)) +	Eξi∆00(t(ξ))ξj
i=j
V------------
E1
i6=j
J X---------------
E2
}
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
The second term is ∣Eι∣ ≤ p ∙ 2D2∕n2 ∙ no = O(pD2∕n0) = (pqD2)
The third term is |E3 | ≤ n20D2p2/n02 = O(p2D2 ).
Therefore, We have an expansion E∆ = -Vξ∆(0) ∙ X + (q + P) ∙ pD?
-p X Xi d∂(x) + O(Dιp + D2r2).
∂xi
i
The expectation just decays with p, but not with n0.
NoW, consider the variance Var∆ = Var(∆0(0)ξ + 0.5ξT ∆00(t(ξ))ξ) = E∆2-(E∆)2 = E(V1+V2)2-(E(V1+V2))2
VarV1 + VarV2 + 2Cov(V 1, V 2)
Consider VarVI = pP(dyXχx) )2x2
{} ×-----{----}
V1	V2
= O(pqD1). This is the leading term, the rest are smaller.
And the second term VarV2 ≤ EV22 =	D22∕n04Eξiξjξkξl . Here we consider various cases for indices i, j, k, l,
ijkl
based on the partition of 4 = 0 + 4 = 2 + 2 = 1 + 3. If all indices are different, we get O(p4) from Eξ. If all are the
same, we get O(p∕n3). If we have 2 groups of 2, we get O(p2∕n2), if 3+ 1 we get O(p2∕n2). Thus, VarV2 = O(D22r4)
Consider the final term Cov(V1, V2)	≤ E|V1V2|	+ E|V1|E|V2|.	E|V1|	≤	O(pD1),	E|V2|	≤	D2p∕n.	EV1V2	=
D1D2∕n30Eξiξjξk. All different indices give p3, all same indices give p∕n20, and 2+1 give p2∕n. Thus, Cov(V1, V2) =
ijk
O(Dι D2 r3)___________________________________________________________________________________________________
Finally, Var∆ = pV2∆(0)x + O(r3D1D2) + O(r4D2) = E
dyx )2 χ2
+ O(r3D2) = O(Dpln + D2r3)	□
A better remainder. It is possible to obtain a remainder of O(p2 ∕n) + O(p∕n2) for the variance instead of a
generic O(r3). This makes the remainder decay With n as Well. A more fine-grained analysis of the remainder could
14
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
4	5	6	7
∣og(n)
Val acc
'ɪC
8 C
IC
Figure 3: Non-regularized Fashion MNIST (first two rows), regularized Fashion MNIST (second two rows). De-
scription is the same as in Fig. 2
H diag
500	1000	1500
n
val_loss
W prod
SSOzle>
Figure 4: Non-regularized Boston Housing dataset
consider the expression for the variance and explicitly compute a correlation between ξi∆0i(t(ξ)) and ξj∆0j (t(ξ)).
However, We were unsuccessful in doing so. Another approach is to take one more term in the expansion: O(r5) - it
will make the previous term with p3 go away, leaving only terms p2/n and p/n2 , as the difference expressions in the
∞
variance will cancel out. Another idea is to use a Taylor series Var∆ = P Trk (existing in principle). Next, we
k=0
explicitly bound the remainder since we have a global bound on Dk ≤ D. Finally, if we expand around the mean
μ = Ex: Ef (x) = f (μ) + 1 Θ±(1)(f0i, Varx) [2], we get a remainder p/n right away, but need to compute f (μ) which
is the network at a modified input (x - px).
5 Probabilistic Guarantees on The Fault Tolerance Using Tail Bounds
Under our assumptions, Var∆ 〜Pl Cpl. The constant Ci comes from Theorem 1 (or its more computationally
tractable form in Add. Coroll. 2). Since we know that the error superposition is linear (Additional Proposition 7),
we sum the individual layer terms, hence l . Proposition 4 motivates the inverse dependency on nl .
Median trick. Suppose that we have a random variable X satisfying P{X ≥ ε} < 1/3. Then we create R
independent copies of X and calculate X = median{Xi}iR=1. Then P{X ≥ ε} < (1/3)R/2 because in order for the
median to be larger than value ε, at least half of its arguments must be larger than ε, and all Xi are independent.
Thus R = O (log 1∕δ) in order to guarantee (1/3)R/2 < δ. This is a standard technique.
Proposition 5. A neural network under assumptions 1-3 is (ε, δ)-fault tolerant for t = ε - E∆L > 0 with δ =
δ0 + t-2Var∆L for E∆ and Var∆ calculated by Theorem 1 and δ0 from Proposition 3.
15
Proof. We apply the Taylor expansion from Theorem 1. We directly apply the Chebyshev’s inequality for X = ∆:
VarX
δ = P{X ≥ t + EX} ≤ -^2-
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
□
5.1	Properties of The Fault Tolerance
Proposition 6. Suppose that a C2 network is at a stationary point after training: EχVwL(X) = 0. Then in the
first order on p, q, Ex Eξ ∆L+1 = 0
Proof. Consider the quantity in question, use Additional Proposition 9 for it and apply Ex on both sides:
Eξ∆L+1 = Ex
-P (Ex 焉,W
Now since We know that ExdlL = 0, the linear term is 0
□
Additional Proposition 7. (Linearity of error superposition for small P limit) Consider a network (L,W,B,夕)
with crashes at each layer with probability pi, l ∈ 0, L. Then in the first order on p, the total mean or variance of
the error at the last layer is equal to a vector sum of errors in case crashes were at a single layer
E∆pL0,...,pL= E∆pL0+.. .+ E ∆pLL
-ar∆pL0,...,pL = -ar ∆pL0 +.. .+-ar∆pLL
Proof. Consider each layer i having a probability of failure Pi ∈ [0,1] for i ∈ 0,L.
In this proof we utilize Assumption 1. We write the definition of the expectation with f (P, n, k) = Pk (1 - P)n-k
being the probability that a binary string of length n has a particular configuration with k ones, if its entries are
i.i.d. Bernoulli Be(P). Here Si is the set of all possible network crash configurations at layer l. Each configuration
si ∈ Si describes which neurons are crashed and which are working. We have |Si | = 2nl .
E△£"."" = £ ... £ f(P0, |S0|,N0).∙.f(PL, |sL|,NL)(yL - VL)
s0 ∈S0	sL ∈SL
where
Vl	=	( WL夕(∙∙∙(夕(Wι(	X Θ ξo)	+bι)	Θ ξι)	∙∙∙)	Θ ξL-ι	+6l)	G 2l
Vl	= WL2(…(夕(Wι	x	+bι)	∙∙∙)	+bL
We utilize the fact that the quantity f(P, n, k) = Pk(1 - P)n-k = Pk + O(Pk+1). Only those sets of crashing neurons
(s0, ∙∙∙∙, sL) matter in the first order, which have one crash in total (in all layers). We denote it as s1k ∈ Sk. Therefore
we write
EδL0 ,...,pL = E f (P0, 1,N0)(yL - Vl) + ∙∙∙+ E f (PL, 1,NL )(yL - VL )
s10 ∈S0	s1L ∈S0
This is equivalent to a sum of individual layer crashes up to the first order on P:
E∆p0,…，PL = E∆L0 + ∙∙∙ + E∆pLL
The proof for the variance is analogous.
□
6 Algorithm for Certifying Fault Tolerance
In case if we consider the RHS quantities in Theorem 1 averaged over all data examples (x, y*), then the Algorithm
1 from the main paper would give a guarantee for every example: it will guarantee that Px,ξ [△ ≥ E∆ +1] is small.
Indeed, if we know that Ex—ar∆ is small, we know that the total variance -arx,ξ△ = Ex-arξ△ + —arxEξ△ (by the
law of total variance) is small as well. Indeed, the second term -arxEξ△ bounded by supx Eξ△ which we assume to
be small. In case if it is not small, it is unsafe to use the network, as even the expectation of the error is too high.
16
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
Given a small total variance Varx,ξ ∆, we apply, as in Proposition 5 a Chebyshev’s inequality to the random variable
∆ over the joint probability distribution x, D|x. This will give Px,ξ [∆ ≥ E∆ + t] ≤ t-2 Varx,ξ ∆. This probability
indicates how likely it is that a network with crashes and random inputs will encounter a too high error.
Median aggregation of R copies works for the input distribution as well. Denote ”yi (x) is bad” as the event
that the loss exceeds ε for the i’th copy. We denote [T rue] = 1 and [F alse] = 0 (Iverson brackets). Now,
δ = Px,ξ (M ed(yi (x)) bad) = Ex Eξ [at least half of yi are bad]. Since the inner probability can be bounded as
≤ t-2Var∆(x) exp(-R), taking an expectation over Ex results in the quantity discussed before, ExVar∆.
We note that it would not be possible to consider yL+1 to be the total loss, as it makes quantities such as
∂L∕∂Wij(x, W) ill-defined, as they depend on X as well.
6.1	Analysis of the Algorithm 1
The algorithm consists of the main loop which is executed until we can guarantee the desired (ε, δ)-fault tolerance.
It trains networks, and upon obtaining a good enough network with δ < 1/3, it repeats the network a logarithmic
number of times. We note that the part on q and δ0 is not strictly required to guarantee fault tolerance. Rather,
satisfying these conditions is a natural necessary conditions to satisfy the more strict ones (on R3, E∆ and δ).
These conditions are necessary because, by AP5, continuous limit implies that the q is reasonable.
Space requirements. After each iteration of the main loop of the algorithm, the previous network can be deleted
as it is no longer used. Therefore, each new iteration does not require any additional space. We only need to store
the network itself and the computational graph for Var∆, which depends on network’s gradients. The total space
complexity is then O PlL=1 nl × nl-1 which is just the space to store the network’s weights.
Time and neurons requirements. Each iteration trains the network and then performs computations which
involve only 1 forward pass, so the bottleneck is still the training stage which we assume can be done in time
O(T). Now let us calculate the number of iterations. New iteration is requested in case if one of the conditions is
not satisfied. We assume the loss to be bounded in [-1, 1] by D1. We analyze each of the possible non-satisfied
conditions:
1.	q < 10-2 . In that case the target value of max |Wi |/ min |Wi | > 100 which is many times greater than the
loss itself (by assumption ω ∈ [一1,1]). In this case the algorithm increases the regularization parameter μ
to make q smaller. We assume that We double μ each time. So by setting μ* = 1 the network will definitely
achieve q > 10-2. Therefore the number of doublings of μ is at most log 1∕μ0. Since μ0 is represented as a
float, log 1∕μ0 = O⑴
2.	δo > 1/3. Since δ0 = exp(-nιqdκL(α∣pι)), it will decay exponentially with the decay of nι. In order to make
δo < 1/3, we need to set nιqdκL(α∣pι) > 2 which leads to n ≥ 壮K：00m/ ∙ Since both α and P are small, we
take only the first-order term in dκL(α∣p) = αlog α∕p + (1 — α) log(1 — α)∕(1 — p). The last term here is
〜P — α ≥ —α. In case if ɑ ≥ e2p6, log α∕p ≥ 2 and dκL(α∣p) & α. Now, a needs to be sufficiently small in
order to guarantee the second-order term in the Taylor expansion α2D12《17. Therefore, α2 〜1∕D12 and
n 〜200Di2. If the algorithm increases n by a constant amount, it will need to take O(D12) operations
3.	R3 > C . In this case, we increase ψ. Since this directly influences R3 via regularization, doubling ψ every
time results in a similar O(1) performance as in the analysis for q and μ. Since R3 ≈ C is the number of
changes that the weights make, which is > 1, and the loss is bounded by 1.
4.	δ = ε-2Var∆ ≈ ε-2 Cpl > 1∕3 (see Section 5) for Cl dependent on the function approximated by the NN and
the continuous limit. Therefore, n 〜O(ClPl∕ε2).
The total number of iterations is therefore O(D12 + ClPl∕ε2) for Cl = nlVar∆∕Pl for some nl (this is now a property
of the function being approximated and the continuous limit). We note that the constants 1∕3 and 10-2 are chosen
for the simplicity of the proof. The asymptotic behavior of the algorithm does not depend on them, as long as they
are constant.
6 For e : log e = 1
,Technically, here We silently implied that e2P《IND12 in order to make α ≥ e2p, which means that We cannot implement
a function with too high second derivatives in neuromorphic hardware with a constant p, not matter how many neurons we take.
Intuitively, this happens because for such a function, even a failure of e2p fraction of neurons, which is a reasonable expectation, is too
large to begin with. We assume that we are fitting a function which is not like that. If we encounter this, we will see that by having a
too high E∆ and the algorithm will output infeasible
17
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
Correctness: guarantee of robustness. NoW We analyze correctness. We note that the algorithm is not
guaranteed to find a good trade-off betWeen accuracy and fault-tolerance. Up to this point, there is no complete
theory explaining the generalization behavior of neural netWorks or their capacity. Therefore, We cannot give a
proof for a sufficient trade-off Without discovering first the complete properties of NNs capacity. We only shoW that
the algorithm can achieve fault tolerance.
First, the condition on E∆ and Var∆ implies that the first-order terms in the expansion from T1 are small
enough. NoW We argue that the remainder is small as Well.
The condition R3 < C implies that discrete function is smooth enough to apply |Wt0(t, t0)dtdt0 | ≈ C1 < R3 < C
as Well as |Wt00 (t, t0)|dtdt0 ≈ C2 < R3 < C. This means that the integral is small, Which alloWs to bound the
Riemann remainder R3 /nl from the proof of AP4. This implies that there exist a continuous netWork NNc such
that the approximation error A from AD2 is small. Here, the right metric is R3/nl ≈ C/nl	1. The number of
changes C must be less than a number of neurons at a layer nl .
NoW, the remainder depends on the operator derivative bound D12 . By the Assumption 3 (part on Dk), they
are small. BeloW We describe Why this part holds.
i2 i
Experimental evidence for Dk being small. First, since We explicitly have a term Ri for Var∆ ≥ (∂y∕∂yi) y；
(AC2) in the regularizer, the first-order derivatives D1 would be small for each layer by design of our algorithm.
ij	i0j0
Next, second-order derivatives ∂2yL∕∂W∣3∂W∣ 3 are found to be small in experimental studies of the Hessian [11].
Since derivatives W.r.t. yl can be expressed via the derivatives W.r.t. Weights by AP8, and the continuous limit
holds, D2 is small as Well.
General considerations for Dk being small. Note, we can always renormalize |x(t)| ≤ 1 and |y(t)| ≤ 1 by
rescaling the input and output weights. For the input layer, D12 can be bounded via the properties of the ground
truth function y*, if y approximates y* well. However, it is known that it can be false: the network can have a
sufficient accuracy on the training dataset, but have much larger output-input derivatives (d[out]/d[in]) than the
ground truth function. Specifically, in the task of image recognition, we (humans) assume the problem to be quite
smooth: a picture of a cat with a missing ear or whiskers is still a picture of a cat. However, it was shown [13]
that modern CNNs use non-robust features. This implies that CNNs are much more sensitive to small changes in
input x, in contrast to the smooth ground truth function y* that We want it to learn, making the bound for the
derivative D1 large.
For the hidden layers, it is possible that the continuous limit has large derivatives. For example, we can first apply
an injective transformation F with high derivatives, and then apply the inverse transform implemented as another
neural network F-1 existing by AP3a. Then, we have an overall smooth (identity) operator yL which, however,
consists of two very non-smooth parts.
We list the following approaches that potentially could resolve this issue theoretically rather than experimentally.
First, we can consider the infinite depth limit [23]. This would allow to have regularities throughout the network’
layers. Another approach is to study mathematically the ways that an operator can be decomposed into a
hierarchical composition of operators. For example, for images, a natural decomposition of the image classification
operator could first detect edges, then simple shapes, then groups them into elements pertinent to specific classes
and then detects the most probable class [26]. At each stage, only robust features are used. Thus, for such
a decomposition, output-hidden derivatives would be reasonable as well as the output-input ones: indeed, the
decision to recognize a cat would not change significantly if some internal hidden layer features (ears or whiskers)
are not present. Interestingly, just enforcing the continuous limit seems to make features more robust in our
experiments, see hidden layer weights on Figures 2 and 3. Without regularization, the weights seem noisy, there
are a lot of unused neurons, and even the used ones contain noisy patterns. In contrast, continuity-regularized
networks seem to have first-layer weights similar to the input images. This could be an interesting research
direction to continue.
We note, however, that such a study is not connected to the fault tolerance anymore, as it is a fundamental
investigation into the properties of the hierarchical functions that we want to approximate, and into the properties
of the neural networks which we can find by gradient descent.
aThe idea to construct y = F ◦ F-1 with F-1 implementable by a neural network is taken from [8]
18
5 O
2 O
6 6
Sso- *uα
O O
O
2
<⅛ cobφ φ>4φ
UUoUN—Iq
EJOUIq
EU「Iq
O O
O
2
<⅛ cobφ φ>4φ
O O
20
% cobφ≥l 第-α
O O
O
2
cobφ> 口α
S3U即用
322 g

2
S
%
T^H⅛ 2
⅛ 3
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
Figure 5: Boston-trained networks. Rank loss (worst value 0.5) of ordering networks using bounds (left 4) and
relative error in predicting the value of ∆ (right 4). Mean of the error E∆ (top) and standard deviation (bottom).
Variable input on a fixed network (1st and 3rd columns) and variable network with fixed input (2nd and 4th
columns).
7 Experimental Evaluation
Experiments were performed on a single machine with 12 cores, 60GB of RAM and 2 NVIDIA GTX 1080 cards run-
ning Ubuntu 16.04 LTS and Python/Tensorflow/Keras. We test the proposed bounds on two datasets as a proof of
concept: the Boston Housing dataset (https://www.cs.toronto.edu/d~elve/data/boston/bostonDetail.html)
(regression) and the MNIST dataset (classification) (http://yann.lecun.com/exdb/mnist/). In addition, we use
the Fashion MNIST dataset (classification, https://www.kaggle.com/zalando-research/fashionmnist). We use
standard pre-trained networks from Keras (VGG, MobileNet, ..., https://keras.io/applications/).
For the Dropout experiment p < 0.03 is used as a threshold after which no visible change is happening. We use
the unscaled version of dropout8 .
In the experiments, we use computationally tractable evaluations of the result given by T1. The ”b1 bound” is
the Spectral bound from P2. The ”b2 bound” corresponds to AP2. The ”b3 bound” corresponds to first-order terms
from T1, or the Additional Corollary 2, and the ”b4 bound” corresponds to an exact evaluation of single-neuron
crashes (taking O(nl) forward passes).
7.1	Boston-trained networks with different initialization: rank loss with experimen-
tal error (additional)
We compare sigmoid networks with N 〜50 trained on Boston Housing dataset (see ErrorComParisonBoston .ipynb).
We use different inputs on a single network and single input on different networks. We compare the bounds using
rank loss which is the average number of incorrectly ordered pairs. The motivation is that even if the bound does
not predict the error exactly, it could be still useful if it is able to tell which network is more resilient to crashes.
The second quantity is the relative error of ∆ prediction, which is harder to obtain. Experimental error computed
on a random subset S0 of all possible crashed configurations S with |S| = 220 〜106 is used as ground truth, with
|S0 | big enough to make the expectation and variance results not change from one launch to another. The results
are shown in Figure 5. Even on this simple dataset we have only b3 and b4 giving meaningful results both for rank
loss and relative error of error. The results show that b4 is always better than b3, as expected. The same is done
for random networks.
8 We note that it is the same as the scaled version up to the first order
19
516
517
518
519
520
521
522
523
524
525
526
527
528
7.2	Comparison on random networks (additional)
The same setup as for the Different Dropout Comparison experiment in the main paper is done for random networks
with N 〜20, ReLU activation function, see ErrorComParisonRandom.ipynb. The results are shown in Figure 6
and they are qualitatively the same as for Boston dataset.
SSO-XUEa
EoU'Iq
Eof-≡ls
EoUIq
Zq
2
N
O O
O
2
% cobφ φ>-⅛-φ^ %
% Uoxlωω≥"-ωα
O O
SSO-XUEa
2 Q
O O
SSO-XUEa
00O
cobφ φ>~⅛-φ^
8 Ω8 τm⅛
OOɪ-
4
O O
O
2
cobφ φ>~⅛-φ^
1 ⅛
N
S
S
Figure 6: Random networks. Rank loss (worst value 0.5) of ordering networks using bounds (left 4) and relative
error in predicting the value of ∆ (right 4). Mean of the error E∆ (top) and standard deviation (bottom). Variable
input on a fixed network (1st and 3rd columns) and variable network with fixed input (2nd and 4th columns).
7.3	Additional charts
Table 2 presents the results of ConvNetTest-ft.ipynb.
Figure 9	presents the results from ComparisonIncreasingDropoutMNIST.ipynb.
Figure 10	shows results from Regularization.ipynb
Figure 8 shows how the algorithm’s (Main paper, Algorithm 1, TheAlgorithm.ipynb) state evolves over itera-
tions. Figure 7 shows the resulting distribution of the error ∆ at the output layer for the final network given by
the algorithm.
4000
AUU ① nba-H
----Target threshold
Experimental error
2000
0
-0.02 -0.01 0.00	0.01
Δ
Figure 7:	The distribution ∆ of the error in the output for the network obtained using Algorithm 1 from the main
paper
20
g -eulπdE山
0.3
0.2
0.1
0.0
-1 01234567
Algo iteration and stage (color)
0.0004
----Experiment
---Theory
tŋ
0.0002 >
,0.0000
8
Figure 8:	Evolution of the experimental error probability and the variance of the error over algorithm’s iterations.
There are two stages. In the first stage (red, iterations ≤ 4) the algorithm increases continuity of the network via
increasing ψ. In the second stage (green, iterations ≥ 5) the algorithm increases the number of neurons n1 and
the regularization parameter λ. First, it can be seen that at first stages the network is not continuous enough, as
the algorithm makes it more continuous. This leads to an increase in the empirical probability δ of the network
outputting a loss > ε and in the increase in the gap between Theoretical Var∆ and Experimental Var∆. This
happens because there are not enough neurons in the network. Later, as the number of neurons increases, the gap
becomes smaller and the empirical probability decreases. Note that the first network (at iteration 0) empirically
satisfies our fault tolerance guarantee. Nevertheless, we do not have a proof for such a network because it is not
continuous enough. Therefore, in order to guarantee robustness, we need to proceed with the iterations.
Model	log Var∆	log[Parameters/Layers]	log[Parameters]	Layers
VGG16	-18.7	15.6	18.7	23
VGG19	-18.4	15.5	18.8	26
MobileNet	-9.1	10.7	15.3	93
Table 2: Comparison of bigger convolutional networks when there are faults at every layer with p = 10-2
529
530
531
532
533
534
535
536
537
538
539
540
541
7.4	Error superposition (testing AP7, additional)
We test a random network with L = 4 on random input (see ErrorAdditivityRandom.ipynb). The error is
computed on subsets of failing layers a, b. Then all pairs of disjoint subsets are considered, and the relative error
of ∆ estimation using linearity is computed as ∣∣∆a∪b — △a 一 ∆bk∕k∆a∪b∣∣. The results (see Figure 11) show that
this relative error is only few percent both for mean and variance, with better results for the mean.
7.5	Testing AP9 (additional)
The result predicts the expected error EχEξ*△ to decay with the decay of the gradient of the loss. We tested
that experimentally on the Boston dataset using sigmoid networks (see ErrorOnTraining.ipynb) and note that
a similar result holds for ReLU9 . The results are shown in Figure 12. The chart shows first that the experiment
corresponds to b4, and b3 is close to b4. b3 is also equal to the result of AP9, both of which decay, as predicted.
7.6	Practical bounds
9 ReLU∈/ C 1 . However only a small percentage of neurons are near 0 thus approximation works for most of them.
21
Figure 9: Comparison of networks trained with different dropout. Shown: accuracy and Var∆ plots for different
train dropout probability. Green curve shows accuracy of correct network, red shows accuracy for the crashing
network. Dashed line show train dataset and solid represent test dataset. Variance of ∆ estimated by b4 shown in
orange, by b3 in blue. Error bars are standard deviations in 10 repetitions of the experiment.
Regularization
Figure 10: Horizontal axis represents increasing regularization parameter. Left vertical axis represents accuracy
and has test crashing network accuracy (red) and correct network accuracy (green). Right axis shows variance of
the error Var∆ estimated by bound b3 used as a regularizer. Error bars are standard deviations in 5 repetitions of
the experiment.
Additional Proposition 8. For neural network we have for any weight matrix Wl and input to the l’th layer yl-1 :
X -d‰Wij = X 工yj
j dWl3	j MLI
1
The equality also holds for one particular ylj-1 :
X dyjW Wij = ^d^yjT
i	∂Wlij	∂ylj-1
542
543 Proof. Fix some layer l. The output of the network y depends on the weight matrix Wl and on the input to the
544 l’th layer yl-1 . However we note that it only depends on their product and not on these quantities separately.
Therefore we write y =yL(Wl,yl-1, ...) = η(Wlyl-1) and denote x = yl-1, W = Wl and z = Wx:
y = η(W x)
Now we take one
∂y = X ∂η ∂zi
∂xj	∂zi ∂xj
Figure 11: Distribution of relative error in the predicted ∆ in percent for 25 subset pairs. Distribution for the mean
is shown in blue, for the variance in orange.
22
0.02
Epochs
Figure 12: Decay with training of the loss (blue line, left vertical axis) and the error ExdyEξ△ (red, right axis)
together with bounds b3 and b4 predictions (red and brown curves) and experimental values (dots). ”Th8” means
AC2
Since Zi = P Wikxk, ;Izi = Wij. Therefore We plug that in:
kj
And multiply with xj :
∂xj Xj=X ∂i Wij Xj
Now we compute IW- using that in vector Z only Zi depends on Wij and also that Z = Wx:
∂y ∂η ∂zi ∂η
∂Wij = ∂Zi ∂Wij = ∂ZiXj
Then we multiply it by Wij and sum over i:
X ∂⅜ Wij = X ∂η XjWij
ii
545 And now we note that the expressions for 孕-Xj and P Jy^r Wij are exactly the same.
∂xj	∂ Wij
i
546 Differentiating the above expression gives a connection between second derivatives w.r.t weights and activations.
547	□
548
Additional Corollary 2. The error expressed in weights in the first order is as in T1:
Eξ δl = -P X 篇 Wlij
∂Wj
i,j l
Var∆L = pXX "y^"yrWj'iWlki
L	∂Wlji ∂Wlki l l
549 Where △lL is the error in case of neurons failing at layer l. Layer 0 means failing input.
In other words, both mean and variance are defined by a tensor Xl = Wl Θ IW■, Zj = — P Xij:
li
E∆'l = PX Zi, Var∆L = PX(Zi)2
ii
550
Proof. According to Theorem 1 the expression for mean and variance are:
∂y
E∆ ≈ —p(Vy(X), x)= -pE∂⅛Xi
ii
Var∆ ≈ ((Vy(X))2,x2) = pX (dyX。
i
23
551
552
553
554
555
556
557
558
Then by Additional Proposition 8 the expression used can be rewritten as (note the index swap i 什 j):
dy	X _dy_w
W-Xi =>J ατ" Wj
∂xi	∂Wji
Then for the mean it is:
s = -p XX ∂⅛ Wji
ij j
and the indices can be swapped again for the sake of notation. For the variance we write the square as the inner
sum repeated twice with different indices k and j :
Var△=P XXX ∂⅛ Wji ∂Wi Wki
i jk
This gives the first statements. Then We notice that both expressions depend on the values of tensor d^ Θ W
but not on W or ∂y alone. We therefore define X = W Θ ∂dW and Zj = — P Xij (sum over the first index) and
i
rewrite
E∆= —pXXXij =pXzj
Var∆ = p X XjiXki=P X(X Xji) = P X(-Zi)2
ijk	i j	i
□
Additional Proposition 9. For a neural network with C1 activation function we have for a particular input x in
the first order as in T1:
Eξ ∆l+i = -p(Vw L,W)
Proof. Take the expression
Eξ∆ = -P P ∂wij Wij
ij
Therefore,
A = Eξ∆L+1 = d∂L,, Eξ∆)
and consider Eξ∆.
By Additional Corollary 2 we have
A=-p (dy, X ∂Wj Wi) =-p X Wij
ij	ij
By the chain rule for a single input x we have
∂L
∂Wij
We plug that back in:
A=-p X ∂Wj Wij
ij
This is the exact statement from the proposition.
□
8 Unused extra results
24
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
Skip-connections. If we consider a model with skip-connections (which do not fit Definition 1) with faults at
every node, we expect that an assumption similar to A3 would lead to a result similar to T1. However, we did not
test our theory on models with skip-connections.
Another idea for fault tolerance. In case if we know p exactly, we can compensate for E∆ by multiplying
each neuron’s output by (1 - p)-1 . In this way, the mean input would be preserved, and the network output will
be unbiased. However, this only works in case if we know exactly p of the hardware.
Additional Proposition 10 (Variance for bound b1, not useful since the bound is not tight. Done in a similar
manner as the mean bound in [9]). The variance of the error Var∆ is upper-bounded as
nn
Var∆ 6 Xai Y bj
i=1 j=i+1
for
aL = CL2 pL (αL + pLβL) + 2CLpL(1 - pL)βLE∆L-1,
bL = (1 - pL)(αL + (1 - pL)βL)
Cl = max{|yli|}
αL = max{(w(L+1), w (L+1))}
βL = max{kw(L+1) k1 kw0(L+1) k1 - (w(L+1) , w0(L+1))}
L
Proof. In the notation of [9], consider ∣Δl∣ =	∣Fneu-Ffaii|	= | P	Wi	(yj	-yj	ξj	)|, EΔl	6	PlIlw(L+1)||iCl +
i=1
KIlw(L+1)∣∣ι(1 - PL)EΔl-i, where yjL) is the corrupted output from layer L and K is the activation function
L	L	LL
Lipschitz constant. Thus, E∆L	6 P Pl Iw(L+1) I1ClKL-l Q (1	- Ps)Iw3 4 (s) 6 7 8 I1 = P Pl Iw(l+1) I1Cl	Q K(1	-
l=1	s=l+1	l=1	s=l+1
Ps)Iw(s+1) I1
1.	Variance: Var∆ = E(∆2) - (E∆)2 6 E(∆2), here we will calculate E∆2:
Nl Nl	°、 z,lλ	z.lλ ∕c∖ z.lλ	z,lλ	一	£2 ’
2.	E∆2 = E P P wj2)yj1)(1-ξj1))wj2)yj1)(1-ξj1)) 6 fC1wj +。2Nf P wjiwj2
j1=1 j2=1	1	j1 6=j2
f2-f1	6	fi_	— n2	P	w. w.	— Ilwk2	— Ilwk2
N 2-N	6	N 2	= P .	乙	wji wj2	= kwk1 kwk2.
1	1	j1 6=j2
Therefore, 6 C2p∣wk2 + C2p2(kwk1 - kw∣2)
3. E∆ι∆1 — E P P wj2)yj1)(i-ξj1))wj22)yj1)(i-ξj1)) 6 fιC2wiw0 + C2N⅛ P wjiwj2
j1=1 j2=1	1	j1 6=j2
6 pC2(w,w0) + p2C2(kwkιkw0∣ι - (w,w0))
NN
4. E∆L 6 P(w(L+1))2E(y(L)- y(L)ξ(L))2 + P w(L+1)wjL+1)E [(y(L) - y(L)ξ(L))(y(L) - yjL)ξ(L))]
6 PLkwk2 + QL(kwk2 -kwk2)
5. EΔlΔL 6 PL w(L+1)w0(L+1)E(yi(L)- y(L)ξi(L))2 + P w(L+1)wj(L+1)E [(y(L) - y(L)ξ(L))(yjL) - yLξjL
6 Pl(w, w0) + QL(kwk1kw0k1 - (w,w0))
6. P1 = PC12 , Q1 = P2C12
7. PL = E(y - yξ)2 = Ey2Ρ{ξ = 0} + K2E∆L-ιP{ξ =1} 6 CLPL + K2E∆L-ι(1 - PL)
8. Ql = E(y - yξ)(y0 - y0ξ0) 6 CL Eηη0 +(1 -PL)PLKCLE(∣Δl-i∣ + ∣∆L-∣) + (1 - Pl)2K2EΔl-iΔL-i.
6Pl
25
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
nn
9.	Consider a recurrence x1 = a1, xn = an + bnxn-1. Then xn =	ai	bj
i=1 j=i+1
10.	Define αL = max{(w(L+1) , w0(L+1))} and βL = max{kw(L+1) k1 kw0(L+1) k1 - (w(L+1) , w0(L+1))}. Then
E∆L∆l, E∆L 6 PLaL + Ql尸工
11.	Thus, E∆2L 6 aL + bLE∆2L-1 , where
aL = CL2 pL (αL + pLβL) + 2KCLpL(1 - pL)βLE∆L-1,	(1)
bL = K2(1 - pL)(αL + (1 - pL)βL).	(2)
Therefore,
LL
E∆L 6 X (C2pι(aι + Plel) + 2KCιpι(1 -pι)βιE∆1-1) Y K2(1 -pυ)(aιo + (1 -p”)的)
l=1	l0=l+1
□
The goal of this proposition was to give an expression for the variance in a similar manner as it is done for the
mean in [9]. However this proposition did not make it to the article because bound b1 was not showing any good
experimental results.
9 Introduction into fault tolerance for neuromorphic hardware
Neuromorphic hardware (NH). Given the amount of processing power required for modern Machine Learning
applications, emerging hardware technologies are nowadays reviving the neuromorphic project and its promise to cut
off energy consumption of machine learning by several orders of magnitude [10]. Neuromorphic implementation of
an NN is a physical device where each neuron corresponds to a piece of hardware, and neurons are physically linked
forming weights. Thus, the computation is done (theoretically) at the speed of light, compared to many CPU/GPU
cycles. The surge in performance could arguably even exceed the one that followed the switch from training on CPUs
to training on GPUs and TPUs [3]. Recent results on neuromorphic computing report on concrete successes such as
milliwatt image recognition [10] or basic vowel recognition using only four coupled nano-oscillators [22]. Since the
components of a neuromorphic network are small [25] and unreliable [17], there are crashes in individual neurons
or weights inside the network [25, 17]. They lead to a performance degradation. A failure within a neuromorphic
architecture involved in a mission-critical application could be disastrous. Hence, fault tolerance in neural networks
is an important concrete Artificial Intelligence (AI) safety problem [4]. In terms of fault tolerance, the unit of failure
in these architectures is fine-grained, i.e., an individual neuron or a single synapse, with failure mode frequently
being a complete crash. This is in contrast with the now classical case of a neural network as a software deployed
on a single machine where the unit of failure is coarse-grained, i.e., the whole machine holding the entire neural
network. For instance, in the popular distributed setting of ML, the so-called parameter-server scheme [16], the
unit of failure is a worker or a server, but never a neuron or a synapse. Whilst very important, fine-grained fault
tolerance in neural networks has been overlooked as a concrete AI safety problem.
References
[1]	Wikipedia. binomial distribution. section tail bounds. https://en.wikipedia.org/wiki/Binomial_
distribution. Accessed: 2019-09-19.
[2]	agronskiy.	Taking the expectation of taylor series (especially the remainder).
https://stats.stackexchange.com/questions/70490/taking-the-expectation-of-taylor-series-especially-the-
remainder, 2013.
[3]	D. Amodei and D. Hernandez. AI and compute. Downloaded from https://blog.openai.com/ai-and-compute,
2018.
26
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
[4]	D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane. Concrete problems in ai safety.
arXiv preprint arXiv:1606.06565, 2016.
[5]	R. Arratia and L. Gordon. Tutorial on large deviations for the binomial distribution. Bulletin of mathematical
biology, 51(1):125—131, 1989.
[6]	I. Benjamini, G. Kalai, and O. Schramm. Noise sensitivity of boolean functions and applications to percolation.
Publications MathCmatiques de l'Institut des Hautes Etudes Scientifiques, 90(1):5-43, 1999.
[7]	L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. 2019.
[8]	A. Doerig, A. Schurger, K. Hess, and M. H. Herzog. The unfolding argument: Why iit and other causal
structure theories cannot explain consciousness. Consciousness and cognition, 72:49-59, 2019.
[9]	E. El Mhamdi, R. Guerraoui, and S. Rouault. On the robustness of a neural network. In Reliable Distributed
Systems (SRDS), 2017 IEEE 36th Symposium on, pages 84-93. IEEE, 2017.
[10]	S. K. Esser, R. Appuswamy, P. Merolla, J. V. Arthur, and D. S. Modha. Backpropagation for energy-efficient
neuromorphic computing. In Advances in Neural Information Processing Systems, pages 1117-1125, 2015.
[11]	B. Ghorbani, S. Krishnan, and Y. Xiao. An investigation into neural net optimization via hessian eigenvalue
density. arXiv preprint arXiv:1901.10159, 2019.
[12]	W. H. Guss. Deep function machines: Generalized neural networks for topological layer expression. arXiv
preprint arXiv:1612.04799, 2016.
[13]	A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry. Adversarial examples are not bugs,
they are features. arXiv preprint arXiv:1905.02175, 2019.
[14]	A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in
Neural Information Processing Systems 31, pages 8580-8589. Curran Associates, Inc., 2018.
[15]	N. Le Roux and Y. Bengio. Continuous neural networks. In Artificial Intelligence and Statistics, pages 404-411,
2007.
[16]	M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y.
Su. Scaling distributed machine learning with the parameter server. In OSDI, volume 1, page 3, 2014.
[17]	M. Liu, L. Xia, Y. Wang, and K. Chakrabarty. Fault tolerance in neuromorphic computing systems. In
Proceedings of the 24th Asia and South Pacific Design Automation Conference, pages 216-223. ACM, 2019.
[18]	E. M. E. Mhamdi and R. Guerraoui. When neurons fail. In 2017 IEEE International Parallel and Distributed
Processing Symposium (IPDPS), pages 1028-1037, May 2017.
[19]	V. Nagarajan. Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-
resilience. coRR, pages 1-36, 2019.
[20]	Naihong Wei, Shiyuan Yang, and Shibai Tong. A modified learning algorithm for improving the fault tolerance
of bp networks. In Proceedings of International Conference on Neural Networks (ICNN’96), volume 1, pages
247-252 vol.1, June 1996.
[21]	B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring Generalization in Deep Learning.
coRR, 2017.
[22]	M. Romera, P. Talatchian, S. Tsunegi, F. A. Araujo, V. Cros, P. Bortolotti, J. Trastoy, K. Yakushiji,
A. Fukushima, H. Kubota, et al. Vowel recognition with four coupled spin-torque nano-oscillators. Nature,
563(7730):230, 2018.
[23]	S. Sonoda and N. Murata. Double continuum limit of deep neural networks. In ICML Workshop Principled
Approaches to Deep Learning, 2017.
[24]	Y. Tang and R. R. Salakhutdinov. Learning stochastic feedforward neural networks. In Advances in Neural
Information Processing Systems, pages 530-538, 2013.
27
662
663
664
665
666
[25]	A. Tran, S. Yanushkevich, S. Lyshevski, and V. Shmerko. Design of neuromorphic logic networks and fault-
tolerant computing. In 2011 11th IEEE International Conference on Nanotechnology, pages 457-462. IEEE,
2011.
[26]	M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European conference on
computer vision, pages 818-833. Springer, 2014.
28