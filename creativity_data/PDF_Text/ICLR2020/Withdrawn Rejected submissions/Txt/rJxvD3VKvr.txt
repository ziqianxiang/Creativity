Under review as a conference paper at ICLR 2020
Wide Neural Networks are Interpolating
Kernel Methods: Impact of Initialization on
Generalization
Anonymous authors
Paper under double-blind review
Ab stract
The recently developed link between strongly overparametrized neural networks
(NNs) and kernel methods has opened a new way to understand puzzling features
of NNs, such as their convergence and generalization behaviors. In this paper,
we make the bias of initialization on strongly overparametrized NNs under gradi-
ent descent explicit. We prove that fully-connected wide ReLU-NNs trained with
squared loss are essentially a sum of two parts: The first is the minimum complex-
ity solution of an interpolating kernel method, while the second contributes to the
test error only and depends heavily on the initialization.
This decomposition has two consequences: (a) the second part becomes negli-
gible in the regime of small initialization variance, which allows us to transfer
generalization bounds from minimum complexity interpolating kernel methods to
NNs; (b) in the opposite regime, the test error of wide NNs increases significantly
with the initialization variance, while still interpolating the training data perfectly.
Our work shows that - contrary to common belief - the initialization scheme has a
strong effect on generalization performance, providing a novel criterion to identify
good initialization strategies.
1	Introduction
Neural networks (NNs) have celebrated many successes over the past decade and achieved state
of the art performance across various domains and tasks. From a theoretical standpoint, however,
many aspects of neural networks are not well understood, as for example illustrated by Zhang et al.
(2016). Neural networks seem to contradict classical learning theory as, in many scenarios, they
are able to fit random labels perfectly while still generalizing well when trained on the true labels.
In addition, overparametrized neural networks frequently exhibit even improved test performance
when the number of parameters is increased further (Belkin et al., 2019). NN models thus often
seem to avoid overfitting.
Very recently there have been advances in understanding the training and evaluation behavior of
neural networks in the infinitely wide limit (Jacot et al., 2018; Hayou et al., 2019) and also in
the strongly overparametrized regime (Du et al., 2018b; Li & Liang, 2018; Allen-Zhu et al., 2018a),
which is close to the infinite limit NN. Both lines of work express the behavior of the neural network
in terms of the so-called neural tangent kernel (NTK). In particular, Du et al. (2018b) showed in this
way that, under mild conditions, strongly over-parametrized neural networks converge to a global
minimum of zero training error.
In another line of research, Belkin et al. (2018b) suggested a new picture of learning in the over-
parametrized regime, introducing an “interpolating kernel method” which successfully learns in this
regime. This method selects the least complex function that interpolates all data points perfectly, as
opposed to traditional methods which balance the function’s complexity against its goodness-of-fit.
It is suggested that this picture of overparametrized learning could help understand neural networks.
If overparametrized NNs are indeed linked to such kernel methods, then zero (or small) training error
alone might not tell us much about the test performance, as the measure of function complexity could
be determined by the NN architecture, initialization, and the optimization method. A bad NN design
may lead to an unfavorable complexity measure that could result in a large generalization gap.
1
Under review as a conference paper at ICLR 2020
The question therefore remains in which way exactly are overparametrized neural networks linked
to interpolating kernel methods? And how can this connection help us better understand neural
networks and their puzzling features?
1.1	Our Contributions
In this paper we answer the first question by making the link to interpolating kernel methods explicit
for strongly overparametrized NNs. After that, we exhibit two implications for NNs which this
connection allows us to draw.
First, for strongly overparametrized ReLU-NNs we make the bias explicit that is imposed by the NN
initialization and by training with discrete gradient descent steps on the squared loss. We achieve
this by decomposing the learned NN function as well as the test error into two terms: The first
term corresponds to a minimum complexity interpolating kernel method, whereas the second term
is proportional to the initialized weights and gives a non-zero contribution only if the test input is
not part of the training set.
Second, we are able to bound the difference between a NN with standard initialization of small
variance and the solution of a minimum complexity interpolating kernel method based on the NTK.
This bound provides a new way to transfer existing test error bounds from interpolating kernel
methods to NNs, and vice versa.
Our third contribution shows that without putting any constraints on the NN initialization, low train-
ing error does not imply low test error. This is because the second term in the above test error
decomposition grows as a power of the initialization variance, as we show, whereas the first term is
independent of this variance. Thus, the test error grows dramatically with increasing initialization
variance, despite (close to) zero training error. Our findings provide an additional theoretical way to
analyze initialization schemes, besides the existing heuristics and intention to prevent vanishing and
exploding gradients (He et al., 2015; Glorot & Bengio, 2010).
Our theoretical bounds and insights are nicely corroborated in several experimental settings.
1.2	Notation
We consider a supervised learning setting with data D = {(xi, yi)}iN=1 ⊂ B1d(0) × R consisting of
N training points, i.e. the inputs come from the bounded d-dimensional ball B1d(0) = {x ∈ Rd :
||x||2 ≤ 1}1. We often view the training inputs X = (x1, . . . , xN)T ∈ RN×d and corresponding
labels Y = (y1 , . . . , yN)T ∈ RN in matrix form. For any function g with domain Rd, we define
g(X) as the row-wise application of g. Similarly, for any set Dtest ⊂ Rd × R of Ntest test points we
define Xtest ∈ RNtest×d and Ytest ∈ RNtest. We call a dataset D consistent if xi = xj implies yi = yj.
We denote the output of a fully-connected NN with L layers and parameters θ by
fNN (x) = -^=w La(hLT(X)) + σbL,
√mL
hl (X) = √σ=Wla(hl-1) + σbl for l = 2,...,L - 1,	⑴
h1 (x) = -^W 1x + σb1 ∈ Rd,
√mι
where a(h) := max{h, 0} is the ReLU-activation function applied componentwise, ml denotes
the number of neurons in layer l, and Wl ∈ Rml×ml-1 and bl ∈ Rml are the weights and biases.
For simplicity we choose the number of units in each hidden layer to be the same2 ml = m for
l = 1, . . . , L - 1 and mL = 1. θl = [Wl, bl] denotes the vector of weights and biases in layer l, and
θ = [θ1, . . . , θL] the vector of all parameters, whose total number is M = PlL=1(ml-1 + 1)ml. We
keep the “initialization variance” σ2 (equivalently, σ > 0) as an explicit scalar parameter, that can
be varied.
1We consider one-dimensional outputs in this paper, although this is easy to generalize.
2Our arguments hold for general fully-connected NNs, too in the “wide limit” minl=1,...,L-1{ml} → ∞.
2
Under review as a conference paper at ICLR 2020
Our parametrization (1) together with a standard normal initialization Wl,j, bi 〜N(0,1) is the
so-called NTK-parametrization (e.g. Du et al. (2018b)). In a forward pass this is equivalent to
the standard parametrization (setting σ/√ml → 1 in Eq. (1)) with the weights and biases initial-
ized at variances σ2∕ml and σ2, respectively (He et al., 2015). During gradient training the two
parametrizations are equivalent up to a parameter-dependent scaling factor of the learning rate (Lee
et al., 2019).
We consider the squared error loss '(y, y) = |y - y∣2∕2 and train by minimizing the empirical risk
LD(θ)= X '(fNN(X),y) = kfNN(x) - Yk2∕2	⑵
(x,y)∈D
using gradient descent (GD). That is, starting from the initialization θ = θ0 the weights are updated
according to the discrete iteration θt+ι = θt - ηVθL(θt) for t = 0,1,..., where η is a learning rate.
We connect NNs to interpolating kernel methods w.r.t. two specific kernel functions that are defined
for any initialization θ = θ0 of a NN (1). The first kernel Ψ : Rd × Rd → R is associated with
training only the last layer θL of a NN (Lee et al., 2019),
Ψ(x, x0) = ψ(x)ψ(x0)T, where ψ(x) = VθL fθNN (x)θ=θ ∈ R1×(m+1)	(3)
is the corresponding feature map. The second is the so-called neural tangent kernel (NTK) Φ :
Rd × Rd → R associated with training all NN parameters (Jacot et al., 2018; Lee et al., 2019),
Φ(x, x0) = φ(x)φ(x0)T,	where φ(x) = VθfθNN(x)θ=θ ∈ R1×M	(4)
again represents the feature map. The “minimum complexity interpolating kernel method” fits a
consistent training set D perfectly, using a function of minimum kernel norm w.r.t. a kernel K:
fKint := arg min kf kHK	subject to Y = f(X),	(5)
f∈HK	K
where HK is the reproducing kernel Hilbert space (RKHS) associated with K (Belkin et al. (2018b);
see also App. B). Additionally, we denote the solution of a fully converged fully-connected ReLU-
NN trained with gradient descent, squared loss and random initialization by fNN. We denote the test
losses for these two predictors by Ltmesetth = kfmeth(Xtest) - Ytestk2∕(Ntest) for meth = NN, int.
2	Related Work
There are two main lines of research related to our work. The first line investigated in which way
strongly overparametrized NNs, where the number mof units per hidden layer scales polynomially
with the number N of training data points, converge to arbitrary small training error during training.
The first papers to investigate this for 1 hidden layer were Daniely (2017), Li & Liang (2018) and
Du et al. (2018b). Later, Allen-Zhu et al. (2018a), Allen-Zhu et al. (2018b), Zou et al. (2018) and
Du et al. (2018a) extended the work to deeper networks, CNNs and RNNs (Goodfellow et al., 2016).
Li & Liang (2018) mostly focus on the cross entropy loss, whereas Du et al. (2018b) focuses on the
squared error as we do here. Additionally, Du et al. (2018b) is the first work to connect the training
behaviour of finite NNs (not only in the limit of infinite width m) to the NTK-kernel introduced
by Jacot et al. (2018). This connection is important for our work. In a similar vein, Arora et al.
(2019b) developed data dependent generalization bounds based on the fact that the weights do not
move far from their initialization, as shown in the above works. Furthermore, Arora et al. (2019a)
experimentally evaluate the generalization behaviour of the infinite width solution of NNs in the
NTK-limit.
Most of these contributions focus on showing that the training loss converges to a global minimum
or that the training dynamics is close to a trajectory that converges to zero training loss. We build
upon these works, but in contrast our focus is the connection between the test error of a trained
NN and the error of an interpolating kernel method. We achieve this by resolving the implicit bias
that the initialization and gradient descent (GD) has on the test loss. The initialization bias on the
training loss is minor compared to the test loss. The most related work to ours is by Zhang et al.
(2019). They investigate a similar issue as we do but use a different method to weaken the effects
3
Under review as a conference paper at ICLR 2020
of non-zero initialization and also do not quantify the impact of initialization on generalization but
give evidence for such an effect. The recent contribution Lee et al. (2019), building upon Lee et al.
(2017), Matthews et al. (2018) and Neal (1996), explicitly solves the training dynamics and shows
that the solution in the trained infinite model seems to be similar to a Gaussian Process, even though
its covariance is not the Bayesian posterior of a simple prior. The solution of their continuous
training time analysis is similar to the linearized model in the proof of our Thm. 2. We perform a
discrete-time analysis, as in actual NN training, instead of the continuous-time (ODE-like) analysis
of other works. Woodworth et al. (2019) uses a continuous time analysis as well to focus on the
effect of noise in the gradients. Borovykh (2019) investigates the effect of the scaling between the
“deep” and the “kernel” regime.
The second line of related work introduces interpolating kernel methods and the idea of an over-
parametrized learning regime which does not suffer from overfitting even though the number of
parameters is be much larger than the number of training samples. This line of work was mainly
introduced by Belkin et al. (2018b) and the follow-up works Ma et al. (2017); Belkin et al. (2018c;a).
The main idea is, instead of regularizing the function while trying to fit the data points well, to
regularize (the complexity of) a function that perfectly interpolates all data points. This newly
identified overparametrized learning regime challenges the conventional thinking about the bias-
variance-tradeoff (Vapnik, 2013) from the underparametrized regime. Belkin et al. (2019) introduces
a bouble-dip picture to describe the transition between the two regimes. Liang & Rakhlin (2018)
have derived generalization bounds for these interpolating kernel methods using novel techniques.
Using our explicit link to NNs, this will provide an alternative to NN generalization bounds like
margin or PAC-Bayes-based bounds (e.g., Bartlett et al. (2017); Neyshabur et al. (2017)). While a
connection between overparametrized NNs and interpolating kernel methods has been suggested in
the papers above, the link has not been made explicit in a rigorous way, to the best of our knowledge.
The implicit bias of (stochastic) gradient descent ((S)GD) and random initialization on the trained
NN has been the topic of many previous works, e.g. Soudry et al. (2018); Glorot & Bengio (2010);
Daniely et al. (2016); Rahaman et al. (2018); Oymak & Soltanolkotabi (2018), which all elucidate
certain aspects of the implicit bias. Our work is unique in that it investigates the effect (implicit bias
of (S)GD and random initialization) on the test error of trained NNs, exhibiting NNs where - despite
always vanishing training error - the test error can be either very good or arbitrarily large. Without
gradient descent training and in a deterministic fashion, such examples have been constructed in
(Mucke & Steinwart, 2019).
3	Linking wide NNs and Interpolating Kernel Methods
We now develop the link between strongly overparametrized neural networks and minimum com-
plexity interpolating kernel methods mathematically. We first illustrate the main idea with the sim-
pler case where only the last NN layer is trained, before extending the result to general NN training.
3.1	Paradigmatic linear case: Training only the last NN layer
As the NN function fθNN(x) (1) is linear in the parameters θL of the last layer, the associated fea-
tures ψ(χ) = Vθl fNN(x)鼠 and corresponding kernel Ψ from (3) do not depend on the last-layer
initialization θL (but depend on the other layers Θql-1), and fθNN(χ) = (θL, ψ(χ)i holds exactly
for all θ and x. Starting NN gradient descent at θ0L = 0, we get the following simple link:
Theorem 1 Let fNN be the fully converged solution of a fully-connected strongly overparametrized
ReLU-NN (1) with L layers, where only the last layer θL = [WL, bL] has been trained, using
gradient descent under squared loss on a consistent dataset D, after it had been initialized at θ0L = 0,
and all other layers have been kept at their random initialization 〜N(0,1). Then it holds that
fNN(X*) = fψnt(χ*)	∀X*∈ Rd,	(6)
where fΨint is the minimum complexity interpolating kernel solution (5) of D w.r.t. kernel Ψ (3).
Proof. Abbreviating the last-layer parameters by H = θL and using the aforementioned linearity
fθ(χ) = 3, ψ(χ)i in H, the gradient descent update rule reads:
Ht+1 =Ht-ηψ(X)T(ψ(X)Ht-Y).
(7)
4
Under review as a conference paper at ICLR 2020
This iteration can be solved explicitly using induction and the binomial theorem (Shah et al., 2018):
办=犷0 + Ψ(X)T (XX(-i)i-1 Qni(ψ(x)ψ(x)T)i-1) (Y - Ψ(X)诙)	⑻
=% + ψ(x)T(ψ(x)ψ(x)T)-1 [ι - (i - nψ(x)ψ(x)T)ti (Y - ψ(x)阮),
where (ψ(X)ψ(X)T)-1 denotes the pseudoinverse. If the learning rate satisfies η <
2∕λmaχ(ψ(x)ψ(x)T) SUch that gradient descent converges, We get(1 - nψ(x)ψ(x)T)t → 0
in the limit t → ∞ as the spectral radius is smaller than one and the term in square brackets can
be simplified to 1. To be precise this holds since D is consistent and We are in the strongly over-
parametrized regime, so that Y (and ψ(x)比)lies in the range of ψ(x)ψ(x)T almost surely (over
the random initialization θ0:3-1, on which ψ depends), see also Du et al.(2018b).
Setting "o = 0 as presupposed, we obtain the following predictor at full convergence (t = ∞):
f NN(X*) = h%, ψ(x*)i = ψ(χ*)ψ(x )T (ψ(x )ψ(x )T )-1Y.	(9)
The same value H∞ can be obtained as the solution to the following least squares interpolation:
H∞ = arg min k训2 subject to Y = ψ(x)H.	(10)
"
This can in turn be written as minimizing the RKHS norm of an interpolation function w.r.t. the
kernel Ψ from Eq. (3) (for details on this see App. A and B):
fΨint = arg min kf kHΨ	subject to Y = f(x).	(11)
f∈HΨ	Ψ
The claim has thus been proven by deriving the explicit solution (9) and showing it to equal fΨint.
3.2 General case: Training the full Neural Network
In the case of training the full NN, we want to follow similar steps as in Sect. 3.1. For this, we
first approximate fθNN (x) affine-linearly by fθlin around its initialization θ = θ0, using recent results
on wide NNs (Du et al., 2018b). We further drop the requirement that any components of θ0 must
vanish; note, e.g., that the suggestive requirement θ0 = 0 (analogous to Sect. 3.1) would, for L ≥ 1,
lead to vanishing features φ(χ) = VθffNN(χ)∣θo and a degenerate kernel Φ = 0. Nonzero θo will
however result in an initialization bias in fθlin as we will see (Eq. (14)).
Theorem 2 Let fNN be the fully converged solution of a strongly overparametrized ReLU-NN (1)
with L layers, each with m neurons, where all layers θ have been trained using gradient descent
under squared loss on a consistent dataset D after random initialization θo 〜N(0,1). Then there
exists a N ∈ N SuchthatforaUNNwith m ≥ N and ∀x* ∈ Bd(0) it holds with probability at least
1 - 2δ over the initialization that
If nn(x*)- fφnt(χ*)∣≤ δσL2 + O (焉)	(12)
where fΦint is the minimum complexity interpolating kernel solution (5) ofD w.r.t. kernel Φ (4).
For the proof of Thm. 2 we describe here especially those steps which are new compared to Thm. 1.
The first property we need is the approximate linearity of the NN in its parameters θ during the whole
gradient training. This is the reason why we restrict to strongly overparametrized fully-connected
NNs. For such networks, a combination of the works (Jacot et al. (2018); Du et al. (2018b); Lee
et al. (2019)) shows that the solution θt of the training dynamics t = 0, 1, . . . stays close to the
initialization θ0, and the deviation from a linear model vanishes with growing width m:
Lemma 1 (Lee et al. (2019); Du et al. (2018b)) Denote the linearization ofan NN (1) around its
initialization θ° by fθin(x) := fθNN(x) + (VθfNN(x)∣θJ(θ — θo). Furtherfort ≥ 0, let θt and θt be
the parameters obtained by gradient descent training starting from θ0 with sufficiently small step size
η of the full NN and its linearisation respectively. Then, there exists some N ∈ N such that for all
m ≥ N it holds with probability at least 1 一 δ over the initialization that: supt∣fNN(X) — fθin(x)∣2 ≤
O(1∕m) for all x ∈ B1d(0).
5
Under review as a conference paper at ICLR 2020
Similar linearisation hold for strongly overparametrized RNNs and CNNs as well, not only for
fully-connected NNs. The theorem proves that for every sufficiently wide NN the linearised solu-
tion is close to the trained full NN. For a more detailed analysis of the dependence of N on the
width m, variance σ and δ we refer to Lee et al. (2019); Du et al. (2018b). Using this result, we
continue our analysis with	flin	which we can rewrite as	fθlin(x)	=	fθNN(x)	+	hθ	-	θ0, φ(x)i	=
(fθNN(X) — hθo, Φ(x)i) + hθ, Φ(x)i were φ(x) = VθfNN(X)∣θ0 is the feature vector defined in Eq.
(4). Similar to the proof of Thm. 1, gradient descent on fθlin with the loss function kfθlin(X) -Y k2/2
and sufficiently small step size η < 2∕λmaχ(ψ(X)ψ(X)T) leads to the final (t → ∞) parameter
θ∞ = θo + φ(X)T (φ(X)φ(X)T)-1 (Y 一 fNN(X)), resulting in the function
flin(X) = fθN0N(X) + φ(X)φ(X)T(φ(X)φ(X)T)-1(Y - fθN0N(X)).	(13)
To connect Eq. (13), and thereby fNN, to the solution fΦint of the interpolating model, two steps
remain to be done. First we show that the two summands in Eq. (13) which are not proportional to
the labels Y can be simplified into a single form for fully-connected ReLU-NNs; this can be done by
showing fθo (x) = L hθo, Φ(x)i (see Lemma 2 for more details). Secondly, We can bound this single
expression from above in terms of the initialization variance (Lemma 3; for proofs see Appendix.
The term proportional to Y is then identified with fint.
Lemma 2 For a fully-connected ReLU-NN fθNN (Eq. 1) with L layers, the trained linearized version
flin from Eq. (13) can be written as follows:
flin(χ) = Φ(χ)Φ(X)T(Φ(X)Φ(X)T)-1 Y +1 φ(χ) [1 一 Φ(X)T(Φ(X)Φ(X)T)-1 Φ(X)] θo. (14)
L
Thus, the trained f nn(x) = f lin(x) + O(1∕√m) decomposes essentially into the two terms of Eq.
(14) (for X ∈ B1d(0)).
As |fnn(x) 一 f lin(x) | ≤ O(1/√m), the expression (14) holds for the trained fnn in the wide limit
m → ∞ as well. This technical result enables all of our results in Thms. 2, 3, and 4.
Eq. (14) can be understood intuitively (in the case of a consistent D): The first term makes the
model interpolate the training data D perfectly (flin(X) = Y), independently of the initialization
θ0, whereas the second term vanishes on the training data X = X and thus only contributes to test
inputs x = x*. The second term furthermore depends on the initialization θo, more precisely on the
component of θ0 that is orthogonal to the feature manifold spanned by the data D, i.e. orthogonal to
the range of φ(X)T. Note that [1 一 φ(X)T (φ(X)φ(X)T)-1 φ(X)] is the projector onto the kernel
of φ(X), about which the dataset D does not give any information.
Lemma 3 For a fully-connected ReLU-NN fθNN, it holds for all x ∈ B1d(0) with probability at least
1 — δ over the initialization θo that L ∣φ(x)(1 — φ(X)T(φ(X)φ(X)t)-1φ(X)) θo∣ ≤ √ .
As in the proof of Thm. 1 (see Eqs. (9)-(11)) the first part of the decomposition in Eq. (14) is
fΦint. By Lemma 3, we see that for small σ2 the distance between flin and fΦint becomes small
with high probability, and flin and fNN are close too because of m N . The triangle inequality
|fNN(X*) — fφnt(χ*)∣ ≤ |fNN(X*) — f/phi(χ*)∣ + |flin(χ*) — fint(χ*)∣ gives thedesiredresult. Thus,
strongly overparametrized neural networks initialized with small variance and trained under gradient
descent correspond to solutions of a minimum complexity interpolating kernel method.
4	Implications for neural networks
Now that we have established the link between NNs and interpolating kernel methods, we show
two implications this has for the test loss of neural networks. First, we derive an upper bound on
the NN test loss, which - in the regime of small NN initialization variance - allows us to transfer
generalization bounds from interpolating kernel methods to neural networks. Second, a lower bound
on the NN test loss gives - in the regime of large NN initialization variance - examples of NNs,
trained under gradient descent after random initialization, that generalize badly despite (close to)
zero training loss.
6
Under review as a conference paper at ICLR 2020
4.1	Transferring Generalization B ounds to Neural Networks
From Thm. 2, it is now almost straightforward to connect the test error of the interpolating kernel
method to the test error (w.r.t. squared loss) of the neural network.
Theorem 3 Let fNN be the fully-converged solution of a strongly overparametrized NN (1) with L
layers and m hidden units per layer and initialization variance σ2, where all layers θ have been
trained using gradient descent under squared loss on a consistent dataset D after random initializa-
tion θo 〜N(0,1). Then it holds with probability at least 1 一 2δ over the random initialization that
p/LNst ≤ p/Ltest+σ^√δ+O(I/√m).
To prove this theorem We use the fact that |flιn(x) 一 fNN(x)∣ ≤ O(1∕√m) holds uniformly for all
X ∈ Bd(0) and that ∣∣flin(Xtest) 一 fint(Xtest)k2/NteSt ≤ σ2L∕δ holds with high probability; the last
fact folloWs in the same Way as in the proof of Lemma 3 by, instead of applying Markov’s inequality
to the squared deviation for a single test point x, applying it to averaged sum of squared deviations
over all test points. Then, we make use of the triangle inequality and evaluate the square to prove
the desired result. This inequality now enables us to transfer bounds for the test error from one
formalism to the other and gives us a new grip on finding bounds on the test error for NNs. Good
generalization bounds on minimum complexity interpolating models have been established in (Liang
& Rakhlin (2018)). On the other hand, Belkin et al. (2018b) showed that conventional bounds in the
underparametrized regime might be of little use for minimum complexity kernel methods.
4.2	Zero NN training error alone does not imply small test error
In the past many people trained neural networks with the goal to achieve a low training error with
the hope that this might also result in a low test error. The initialization was merely thought of as
a necessity to converge faster to a low training error or to give a favourable starting point for the
training dynamics. We now show with our analysis that random initialization alone is not enough,
but instead we need certain constraints on the initialization to be able to give generalization bounds.
Before we prove this let us derive some simple properties. Let Φ(x, σ) be the feature vector of
a NN initialized with NTK-parametrization with initialization variance σ . Then it is easy to see
that Φ(x, σ) = σLΦ(x, 1) because of the homogeneity of the derivative w.r.t constants. Fur-
ther we define J(XteSt,σ) := ∣LΦ(Xtest)(1 - Φ(X)t(Φ(X)Φ(X)t)-1Φ(X)) θ0k2∕√Nst and
J(Xtest) := J(Xtest, 1). With this notation, we can show the following theorem:
Theorem 4 Let fNN be the fully-converged solution of a strongly overparametrized NN (1) with L
layers and m hidden units per layer and initialization variance σ2, where all layers θ have been
trained using gradient descent under squared loss on a consistent dataset D after random initial-
ization θ0 〜N(0,1). Then it holds with probability at least 1 一 δ over random initialization
/LNst ≥ ∣σL j(XteSt)- O(I/√m) - /Lntl
For general datasets D, it is highly likely that J(Xtest, σ) 6= 0. To prove the statement we use the
decomposition of Lemma 2. Now instead of making the variance small to bound the second term
we can use the homogeneity of Φ(x, σ) to show that J (Xtest, σ) = σLJ(Xtest). Now using twice the
reverse triangle inequality we and the lemma 1 get the desired result.
Now looking at the expression of Thm. 4 and using again the homogeneity of Φ we can see that
，LtnSt does not change when changing σ because the different factors cancel out and the term
corresponding to the linear approximation should also not impact the result too much as long as we
choose m large and do not increase σ significantly. Respecting this constraint we can now easily
increase the variance to make the second term as large as we want. Thus, by increasing σ we are able
to increase the test loss arbitrarily while not changing the training error at all because the second term
in our decomposition does not contribute during training time. This shows that without considering
an explicit initialization the training error of a NN might tell us very little about the actual test loss.
Thus, our results underline the importance of finding good initialization strategies.
7
Under review as a conference paper at ICLR 2020
5	Experiments
In this section we give experimental evidence for our theoretical findings: the influence of NN
initialization on the generalization performance of fully-trained wide NNs (Sect. 4) as well as the
link to interpolating kernel methods in the first place (Sect. 3).
We perform experiments (Fig. 1) in three different settings3: (1) A toy experiment, fitting a 2D-
dataset with N = 10 datapoints sampled uniformly from f(x) = exp(-kxk2), x ∈ [- 1, 1]2, with a
NN of 1 hidden layer (L = 2) with m = 10000 neurons; here, the “wide NN” requirement m N
is satisfied to the highest degree. (2) Fitting N = 100 MNIST digits 0 and 1 by a single-hidden-layer
network with m = 4000 hidden units. In line with our framework and with other works on wide
NNs (e.g., (Du et al., 2018b; Arora et al., 2019b)), we fit these classification labels using squared
error loss. (3) Fitting MNIST digits 0 and 1 with 2 hidden layers (L = 3) of m = 1500 neurons
each. The reason for using small training sets is to maintain the overparametrization limit m N .
In each setting we vary the variance σ2 of the NN initialization, performing always 10 repetitions.
We stop training at the same “close to zero” training loss for all σ2 , chosen in each setting so small
that empirically the test loss remains almost constant (while the training loss is decreasing further).
The left panels (Fig. 1) show the test losses of the trained NN vs. the linearization flin of the ini-
tialized NN (Lemma 1), at different NN initialization variances σ2 . Both curves are very close,
confirming the basic premise that the approximation of wide NNs by linear models is quite accurate
for our NNs, even though we are not in the guaranteed overparametrized regime (e.g., m ≥ O(N6)
from Du et al. (2018b)). The approximation is better for the two MNIST experiments, where we are
actually further from the this regime, whereas the larger repetition variances for the 2D bump are
due to sampling the training inputs from a small volume of low dimension.
Most strikingly, the test loss of the trained NN depends heavily on its weight initialization θ0 even
though all NNs have been trained to the same low training error. This is shown by the increase at
larger initialization variance σ2. Via the link to flin, this can be understood from the test behavior of
strongly overparametrized linear models flin with big initialization θ0 (Eq. (14) and Sect. 4.2).
The right panels underline this behavior predicted by our theory quantitatively: The test error of
trained ReLU-NNs as well our our analytical lower bound on it (Thm. 4) both behave at big σ2 like
Θ(σ2L) (see also Eq. (12)), with L the number of layers. This in particular shows how one can
find natural examples of (overparametrized) NNs, i.e. initialized randomly and trained by gradient
descent, that interpolate the training data perfectly, yet with arbitrarily large test error.
In the regime of small σ2 on the other hand, the right panels confirm Thm. 3, namely that the test
error is close to the (low) test error of the interpolating kernel method fΦint (Eq. (5)). Note that its
test error in the figures is almost constant in σ2 , as fint does not depend directly on the weight
initialization θ0 but on the (many and random) NN features φ(x), which via the kernel Φ only
determine function smoothness. This is a more reasonable modeling behavior than that of wide NNs
initialized at large σ2 .
6	Discussion
In this paper we make the bias explicit that is imposed by gradient descent and random initialization
on fully-connected strongly overparametrized NNs. We achieve this by decomposing a gradient
descent-trained network with (close to) zero training error into an interpolating kernel term and a
second term that vanishes on training inputs but not on test inputs.
Because this second term is independent of the training labels but depends strongly on the initializa-
tion, it is favorable for good generalization to reduce its impact. This can be achieved by choosing
the variance of the NN initialization to be sufficiently small, as we have shown. On the other hand,
too small an initialization variance limits the expressivity of the NN and its features.
Our work furthers the practical understanding of modern NNs by utilizing recent theoretical ad-
vances in strongly overparametrized NNs. In particular, we saw that the initialized weights are not
merely a starting point for training, but significantly influence the performance of the NN trained to
3More details on the settings needed to reproduce the experiments can be found in Appendix E.
8
Under review as a conference paper at ICLR 2020
Toy experiment (2D Gausslan bump)
io-1
Variance
¢- Trained NN
-⅛- Linearized Solution
IO1
10 12 3
O O - - -
Ilooo
111
ssσlsaj.
h∣∈- Tfalned NN
T¢- Unearized Solution
MNIST with L=I hidden layer
ssσl-jsaj.
Io-N	IoT	10°
Variance
ssσl-jsaj.
MNlSTWIth L=2 hidden IayerS
h∣t- Trained NN
Tc- Unearized Solution
MNlST with L=2 hidden layers
ssσl-jsaj.
ssσl-jsaj.
10-3-l-.-------.——.~n--------------------.——.~
IoT	IoT	100
Variance
LoT
Variance
Figure 1: Experiments on the 2D bump f(x) = exp(-kxk2) (top row) and on MNIST with L = 2
and L = 3 layers, respectively (last two rows). Left panels: The test error of trained wide NNs
as well as their linearized versions increases significantly with the NN initialization variance σ2,
despite training all NNs to the same very low training error. This is explained by the theory in Sect.
4.2, as the linearized approximation of our NNs is quite accurate (see also Sect. 3). Right panels:
Quantitatively, at big σ2, the NN test loss as well as its lower bound both grow as Θ(σ2L) (Thm. 4).
At small σ2, on the other hand, the NN test error is close to that of the interpolating kernel method
fΦint (Eq. (5)), confirming Thm. 3.
zero empirical error. Similarly, focussing on the first term of our decomposition, learning-theoretical
results on interpolating kernel methods lead to a better understanding of puzzling features of NNs.
Our work provides intuition for the “essentially no barriers” phenomenon first observed in Draxler
et al. (2018); Garipov et al. (2018): When scaling all initialization weights θ0 by a common contin-
uous factor (like σ), all of the finally trained NNs fσNN are connected continuously and their training
error stays at (close to) zero. This is because Φ changes by only a scalar factor σ2L, which conti-
nously changes the second (test) term in the linearization fNN ≈ flin (Eq. 14) but cancels out of the
first (training) term. It would be interesting for further work to investigate varying θ0 more generally
in the hypersurface of zero training loss, which induces non-scalar changes in Φ.
The connection of NNs with minimum complexity interpolating kernel methods opens up more
directions for future research. An important next step would be to go from strongly overparametrized
NNs to merely overparametrized NNs, where the (infinitesimal) kernel dynamics effected by NN
training is to be understood in addition to fixed-kernel interpolation. Further, the connection remains
to be made explicit for CNNs and RNNs, as well as for other (classification) loss functions, whose
strict minimization often requires infinite weights. For SGD at least, instead of full gradient descent,
the basic decomposition into a training part and its orthogonal component (Eq. 14) remains valid.
9
Under review as a conference paper at ICLR 2020
Another direction needed to fully leverage the link to NNs made explicit in Sect. 4.1, is to understand
better the properties - e.g. the generalization behavior (Liang & Rakhlin, 2018) - of interpolating
kernel methods, whose close connection to large modern-day NNs we have highlighted in this paper.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. arXiv preprint arXiv:1810.12065, 2018b.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for
classification and regression rules that interpolate. In Advances in Neural Information Processing
Systems, pp. 2300-2311, 2018a.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. arXiv preprint arXiv:1802.01396, 2018b.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict
statistical optimality? arXiv preprint arXiv:1806.09471, 2018c.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Anastasia Borovykh. The effects of optimization on generalization in infinitely wide neural net-
works. ICML Workshop, 2019.
Amit Daniely. Sgd learns the conjugate kernel class of the network.	In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems 30, pp. 2422-
2430. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6836- sgd- learns- the- conjugate- kernel- class- of- the- network.pdf.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A Hamprecht. Essentially no barri-
ers in neural network energy landscape. arXiv preprint arXiv:1803.00885, 2018.
Simon S Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.
arXiv preprint arXiv:1901.08572, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information
Processing Systems, pp. 8789-8798, 2018.
10
Under review as a conference paper at ICLR 2020
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Training dynamics of deep networks using
stochastic gradient descent via neural tangent kernel. arXiv preprint arXiv:1905.13654, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel” ridgeless” regression can gener-
alize. arXiv preprint arXiv:1808.00387, 2018.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. arXiv preprint arXiv:1712.06559,
2017.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018.
Nicole Mucke and Ingo Steinwart. Global minima of dnns: The plenty pantry. arXiv preprint
arXiv:1905.10686, 2019.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? arXiv preprint arXiv:1812.10004, 2018.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A Hamprecht,
Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. arXiv preprint
arXiv:1806.08734, 2018.
Vatsal Shah, Anastasios Kyrillidis, and Sujay Sanghavi. Minimum norm solutions do not always
generalize well for over-parameterized problems. arXiv preprint arXiv:1811.07055, 2018.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
11
Under review as a conference paper at ICLR 2020
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
2013.
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and
deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced by
initialization in deep neural networks. arXiv preprint arXiv:1905.07777, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
12
Under review as a conference paper at ICLR 2020
A Appendix
A Minimum 2-Norm solution
Let us consider an affine linear model f (x) = Φ(x)θ, where Φ(x) is our feature map and θ our
weights. We want to choose the weights as a solution of the following problem:
θ = min kθk2 subject to Y = Φ(X)θ
First, we use the method of Lagrange multipliers to and define the following Lagrange function
L(θ, λ) = kθkɪ - λτ (Φ(X)θ - Y)
Taking the derivative with respect to θ and setting it to zero gives Us θτ = λτΦ(X). Now plugging
this into the Lagrangian we get
L(λ) = -1 λτ Φ(X )Φ(X )t λ + λτ Y
Minimizing this term with respect to λ We get λ = (Φ(X)Φ(X)T) 1 Y. Inserting this relation We
get
θ = Φ(X)T (Φ(X)Φ(X)T)-1 Y
B Connecting the interpolating minimum 2-Norm solution to the minimum
RKHS-Norm interpolating kernel method
Building upon the previous section we now want to show that a linear predictor where the weights are
the solution of a minimum 2-norm interpolation with feature map Φ(x) is equivalent to a minimum
complexity interpolating kernel method with
/二获产学皿犷卜 subject to Y = f (X),
were the kernel of the RKHS is given by K(x, y) = hΦ(x), Φ(y)i and Φ is the same function used
for the linear ansatz in the minimum 2-norm solution.
We can find the solution of the minimum complexity interpolating kernel method by minimizing the
following Lagrangian:
L(f,λ) = 2 Mhk + λT (Y - f (X))
Because the RKHS-Norm is bounded by definition (for non degenerate data) we can use the repre-
senter theorem (f = PiN=1 αiK(xi, .) with αi ∈ R) and the reproducibility of the kernel K to arrive
at
L(α, λ) = 1 ατK(X, X)α + λτ (Y - K(X, X)α)
Now taking the derivative with respect to α we get α = λ. Inserting this into our Lagrangian we get
L(λ) = -1 λτK(X, X)λ + λτY
This Lagrangian is equivalent to the Lagrangian of the minimal two norm solution in the dual rep-
resentation and due to the convexity of the problem the solutions must also be equivalent.
C Lemma 2
In this section we want to give the proof for lemma 2. Let fNN be a fully-connected NN as defined
in the notation section. For simplicity we switch from the NTK initialization to the He et al. (2015)
initialization. The proof is equivalent in both parametrizations. The only difference is that we can
13
Under review as a conference paper at ICLR 2020
suppress some of the scaling factors in the initialization of the weights. We again assume a ReLU-
activation function. Before showing the actual lemma we will first show the following expression
which we will use to proof the lemma
fWN(x) = 1 hW, VWf(x)i,
L
(15)
were the scalar product is defined as the Hilbert-Schmid-product with hA, Bi = tr ATB and
W = diag(WL, ..., W1) is the diagonal matrix with all the weigh matrices on the diagonal. We
restrict ourselves to networks without biases but the proof with biases is almost identical.
To prove the lemma we can first proof that ∀l ∈ {1, ..., L} we have fWN N (x) = hWl, VWl f (x)i. If
this statement is true it is easy to see that the formula above is also true.
A ReLU Network can be written in the following way
fWN N (x) = WLlhL-1(x)≥0WL-1 ...lh1(x)≥0W1x,	(16)
where lhl (x)≥0 is the diagonal matrix with the step function on the diagonal corresponding to the
components of hl (x) ≥ 0. Now, we can define a(WL:l, x) := WLlhL-1(x)≥0WL-1...lhl(x)≥0 and
b(Wl:1,x) := lhl-1(x)≥0Wl-1... to get
fWN N (x) = a(WL:l,x)Wlb(Wl:1,x)
Using the fact that ∂Wij— = 0 because We define "：(隔=0)" = 0. Defining the derivative of the
stepfunction at the step to be zero is also done in most of the standard frameworks like tensorflow.
Additionally, the region Where the stepfunction jumps is a d - 1 dimensional subspace and thus the
expression holds almost everyWhere anyWay. NoW, since our Weights do not move much and only
a neglectable number change the step function We can just define this derivative to be zero. The
derivative of b(Wl:1, x) With respect to Wl is clearly zero because it does not depend on Wl.
Using this We can see that almost everyWhere
hWl,VWlfNN(x)i =	(Wl)nm
nm
∂fWN (x)
∂ (Wl)nm
a(WL:l, x)Wlb(Wl:1, x) =fNN(x),
Applying this result L-times we have get that almost everywhere fNN (x) = L (W, VWf (x)). Thus,
We get almost everyWhere
fθo (X) = 1 hθ0, φ(X)i
L
and therefore also
fθNN(x)-Φ(x)Φ(X)T(Φ(X)Φ(X)t)-1fNN(X) = 1Φ(x) (l - Φ(X)T(Φ(X)Φ(X)t)-1Φ(X)) θ0
L
D	Lemma 3
In this section we want to show the statement of lemma 3. We want to show that
P Q∣Φ(x) (I - Φ(X)t(Φ(X)Φ(X)t)-1Φ(X)) θo∣ < σ2Lδ
≥ 1 - δ
First, let us focus on the operator (I — Φ(X)T(Φ(X)Φ(X)T)-1Φ(X)). The second term has the
properties of a projection operator (symmetric and equal to its square). Thus, all the eigenvalues
are either zero or one. The operator projects M -dimensional vectors into a N -dimensional subspace
and then back to the M -dimensional space. Thus, it has N eigenvalues which are one and all the
rest are zero. Now, looking at the whole operator we see that it has M - N eigenvalues which are
one and the rest are zero. Therefore, we can neglect the projection operator because this will only
make our result larger due to the neglected N dimensions which would be projected out.
Using this and the formula we have derived in th proof for lemma 2 we get almost everywhere
1 ∣Φ(χ) (I - Φ(X)t(Φ(X)Φ(X)t)-1φ(x)) θo∣ ≤ 11φ(χ)θo∣ = IfNN(X)∣
LL
14
Under review as a conference paper at ICLR 2020
Now, we can use Markov’s inequality to arrive at
P (l-1φ(χ) (I - Φ(X)T(Φ(X)Φ(X)T)-1Φ(X)) θ0
2 < E [IfNN(X)I2] /δ
≥ 1-δ
Thus, the only thing that is left to do is to show that E fθNN(x) ≤ σ2L. To start of we can again
write the Neural Network as a product of matrices
fθN0N(x) =WLlhL-1(x)≥0WL...lh0(x)≥0W1x.
We use a similar approach as Du & Hu (2019) and generalize this approach to non-linear networks.
We make use of the fact that for for a random matrix A ∈ Rd1 ×d2 with i.i.d. N(0, 1) entries and a
arbitrary non-zero vector V ∈ Rd1 /0 the distribution of，郴 is χd2 distributed. Additionally, it is
klAv≥0 Avk2	kWi lhi-1 (x)≥0 Wi-1 ...lh1 (x)≥0 W1 xk
easy to see that	IlAvlJ ≤ 1. Defining Zi=	%i-ι(χ))≥c,Wi ...lhi(χ)≥0)Wιxk2	We thus get
E[IfθN0N(x)I2] ≤E[Z1...ZL]=ΠlL=1E[Zl]=σ2L.
Inserting the result for E [f0(x, θ0)] We get
P (力Φ(x) (I - Φ(X)T(Φ(X)Φ(X)T)-1Φ(X)) θ0∣ < σL∕√δ) ≥ 1 - δ.
E Details on our Experiments
In this section We Want to talk about hoW We conducted the experiments. Since the experimental
settings are not very complicated this should be more than enough to easily reproduce our results.
As described in the experimental section We used three different setting to investigate the dependence
of the Neural NetWork solution on the initialization variance. We used a standard He et al. (2015)-
initialization Which is, as mention in the notation section, equivalent to the NTK-parametrization
and standard normal initialization When We rescale the learning rate. Since We choose the theoretical
value of η = λ (ψ(X)ψ(x)t)for the learning rate the settings are equivalent. For all experiments
We use TensorfloW With ReLU-activation functions, set the bias initially to zero and train With GD.
For all netWorks We use a loW number of training data points to make sure that We are in or close to
the over-parametrized regime. This is necessary to assure the validity of the linearisation.
The first NetWork for the 2D exponential Bump is a single hidden layer NN With 10000 hidden units.
We use 10 training data points and 300 testing points. We train 10 netWorks for each variance value
and until the training loss drops first belloW a loss of 10-5 . The final training loss of the curves
plotted in the experimental section can be seen in figure 2. We choose the stopping point for the
training error in such a Way that even though the training loss is still decreasing further the test loss
is almost constant.
Figure 2: Benchmark experiment With a 2D-Gaussian Bump
For the single layer MNIST netWork We use 4000 hidden units and for the tWo layer netWork tWo
layers of each a 1500 hidden units. We use 100 training samples of0 and 1 labelled examples of the
15
Under review as a conference paper at ICLR 2020
training set and 100 samples from the test set to calculate the test error. For the MNIST Network we
train the networks to reach a training error smaller than 10-4. The plot of the final training errors
that corresponds to the curves in the Experimental section are shown in figure 3.
IO-4 -
9.9998X10-5
9.9996X10-5
3 9.9994 ×10-5
oɪ
c
f 9.9992 × 10-5
E
F
9.999× 10^5
9.9988 ×IO-5
9.9986 ×10-5
io-2	IO-1	IO0
MNIST with L= 2 hidden layers
ωωσl 6u-u-ejl
io-2
Figure 3: Experiments with MNIST for a NN with 1 and 2 hidden layers
Variance
Variance
16