Under review as a conference paper at ICLR 2020
On importance-weighted autoencoders
Anonymous authors
Paper under double-blind review
Abstract
The importance weighted autoencoder (IWAE) (Burda et al., 2016) is a
popular variational-inference method which achieves a tighter evidence
bound (and hence a lower bias) than standard variational autoencoders by
optimising a multi-sample objective, i.e. an objective that is expressible as
an integral over K > 1 Monte Carlo samples. Unfortunately, the IWAE
multi-sample objective leads to inference-network gradients which break
down as K is increases (Rainforth et al., 2018). This breakdown can only
be circumvented by removing high-variance score-function terms, either by
heuristically ignoring them (which yields the ‘sticking-the-landing’ IWAE
(IWAE-STL) gradient from Roeder et al. (2017)) or through an identity from
Tucker et al. (2019) (which yields the ‘doubly-reparametrised’ IWAE (IWAE-
DREG) gradient). In this work, we develop an encompassing framework
which directly optimises the proposal distribution in importance sampling
as in the reweighted wake-sleep (RWS) algorithm from Bornschein & Bengio
(2015). From this unified framework, most of the previously proposed gradi-
ent estimators can be naturally derived. This permits a better understanding
of the assumptions and trade-offs that are at play. Importantly, the derived
gradient estimators are guaranteed to not degenerate as K → ∞.
1	Introduction
Let x be some observation and let z be some latent variable taking values in some space
Z. These are modeled via the generative model pθ (z,x) = pθ(Z)pθ (x|Z) which gives rise to
the marginal likelihood pθ (x) = Z pθ(z, x) dz of the model parameters θ. In this work, we
analyse algorithms for variational inference, i.e. algorithms which aim to
1.	learn the generative model, i.e. find a value θ* which is approximately equal to the
maximum-likelihood estimate (MLE) θml := arg maxθ pθ (x);
2.	construct a tractable VariatiOnal approximation qφ,χ(Z) of pθ(z|x) = pθ(z,x)/pθ(x),
i.e. find the value φ such that q@*,x (Z) is as close as possible to pθ (z|x) in some
suitable sense.
A few comments about this setting are in order. Firstly, as is common in the literature, we
restrict our presentation to a single latent representation-observation pair (z,x) to avoid
notational clutter — the extension to multiple independent observations is straightforward.
Secondly, we assume that no parameters are shared between the generative model pθ (Z, x)
and the variational approximation qφ,x(Z). This is common in neural-network applications
but could be relaxed. Thirdly, our setting is general enough to cover amortised inference.
For this reason, we often refer to φ as the parameters of an inference network.
Two main classes of stochastic gradient-ascent algorithms for optimising ψ := (θ, φ) which
employ K ≥ 1 Monte Carlo samples (‘particles’) to reduce errors have been proposed.
• IWAE. The importance weighted autoencoder (IWAE) (Burda et al., 2016) max-
imizes a joint lower bound LψK ≤ pθ (x) whose bias decreases as K → ∞. The
gradients of this objective can be unbiasedly approximated via the Monte-Carlo
method. Unfortunately, the signal-to-noise ratio of the IWAE φ-gradient vanishes
as K grows (Rainforth et al., 2018). Two modified IWAE φ-gradients avoid this
breakdown by removing high-variance ‘score-function’ terms:
1
Under review as a conference paper at ICLR 2020
-IWAE-STL. The 'sticking-the-Ianding’ IWAE (IWAE-STL) φ-gradient
(Roeder et al., 2017) heuristically drops the problematic score-function terms
from the IWAE φ-gradient. This induces bias for the IWAE objective.
- IWAE-DREG. The ‘doubly-reparametrised’ IWAE (IWAE-DREG) φ-gradient
(Tucker et al., 2019) unbiasedly removes the problematic score-function terms
from the IWAE φ-gradient using a formal identity.
•	RWS. The reweighted wake-sleep (RWS) algorithm (Bornschein & Bengio, 2015)
optimises two separate objectives for θ and φ. Its gradients are approximated by
self-normalised importance sampling with K particles: this induces a bias which
vanishes as K → ∞. RWS can be viewed as an adaptive importance-sampling
approach which iteratively improves its proposal distribution while simultaneously
optimising θ via stochastic approximation. Crucially, the RWS φ-gradients do not
degenerate as K → ∞.
Of these two methods, the IWAE is the most popular and Tucker et al. (2019) demonstrated
empirically that RWS can break down, conjecturing that this is due to the fact that RWS
does not optimise a joint objective (for θ and φ). Meanwhile, the IWAE-STL gradient
performed consistently well despite lacking a firm theoretical footing. Yet, IWAE suffers from
the above-mentioned φ-gradient breakdown and exhibited inferior empirical performance to
RWS (Le et al., 2019). Thus, it is not clear whether the multi-sample objective approach of
IWAE or the adaptive importance-sampling approach of RWS is preferable.
In this work, we show that directly optimising the proposal distribution, e.g. as done by RWS,
is preferable to optimising the IWAE multi-sample objective because (a) the multi-sample
objective typically relies on reparametrisations and, even if these are available, leads to the
φ-gradient breakdown, (b) modifications of the IWAE φ-gradient which avoid this breakdown
(i.e. IWAE-STL and IWAE-DREG) can be justified in a more principled manner by taking
an RWS-type adaptive importance-sampling view. This conclusion was already reached by
Le et al. (2019) based on numerical experiments. They demonstrated that the need for
reparametrisations can make IWAE inferrior to RWS e.g. for discrete latent variables. Our
work complements theirs by formalising this argument. To this end, we slightly generalise the
RWS algorithm to obtain a generic adaptive importance-sampling framework for variational
inference which we term adaptive importance sampling for learning (AISLE) for ease of
reference. We then show that AISLE admits not only RWS but also the IWAE-DREG and
IWAE-STL gradients as special cases.
Contributions. Novel material is presented in Section 3, where we introduce the AISLE-
framework. From this, most of the previously proposed gradient estimators can be naturally
derived in a principled manner. Importantly, the derived gradient estimators are guaranteed
to not degenerate as K → ∞. Specifically, we establish the following connections.
•	We prove that the IWAE-STL gradient can be recovered as a special case of AISLE
via a principled and novel application of the ‘double-reparametrisation’ identity from
Tucker et al. (2019). This indicates that the breakdown of RWS observed in Tucker
et al. (2019) may not be due to its lack of a joint objective as previously conjectured
(since IWAE-STL avoided this breakdown despite having the same idealised objective
as RWS). Our work also provides a theoretical foundation for IWAE-STL which was
hitherto only heuristically justified as a biased IWAE-gradient.
•	We prove that AISLE also admits the IWAE-DREG gradient as a special case. Our
derivation also makes it clear that the learning rate should be scaled as O(K) for
the IWAE φ-gradient (and its modified version IWAE-DREG) unless the gradients
are normalised as implicitly done by popular optimisers such as ADAM (Kingma &
Ba, 2015). In contrast, the learning rate for AISLE need not be scaled up with of K.
•	When applied to the family of α-divergences, AISLE leads to a new family of gradient
estimators that generalises some previously derived in the literature.
•	In the supplementary materials, we provide insights into the impact of the self-
normalisation bias on some of the importance-sampling based gradient approxima-
2
Under review as a conference paper at ICLR 2020
tions (Appendix A) and empirically compare the main algorithms discussed in this
work (Appendix B).
We stress that the focus of our work is not necessarily to derive new algorithms nor to
establish which of the various special cases of AISLE is preferable. Indeed, while we compare
all algorithms discussed in this work empirically on Gaussian models in the supplementary
materials, we refer the reader to Tucker et al. (2019); Le et al. (2019) for an extensive
empirical comparisons of all the algorithms discussed in this work.
Notation. We repeatedly employ the shorthand p(f) := Z f (z)p(z) dz for the integral
of some P-integrable test function f; thus, P(f) = EZ〜P [f (Z)] if P is a probability measure.
Furthermore, q0K(Z 1:K) := "k=1 q(Zk)∙ To keep the notation concise, We hereafter suppress
dependence on the observation x, i.e. we write qφ(z) := qφ,x (z) as well as
∏θ (Z )：= Pθ (z | x)
Pθ (z,x)
Pθ (x)
γθ(Z)
Where γθ (Z) := Pθ (Z, x) and Where Zθ := Pθ (x) = Z γθ (Z) dZ.
2 Background
2.1	Importance sampling
The expectation qφ (f) of a test function f : Z → R can be unbiasedly estimated by the
quantity [f(Z1) + . . . + f(ZK)]/K using a set of K particles, z := (z1...,zK)〜qφK, which
are independent and identically distributed (IID) according to qφ. Similarly, expectations of
the type πθ(f) can be approximated by the self-normalised importance sampling estimate
πθθφ,z>(f):= ⅛Wf((Z)with wψ := ∑⅛ψ⅛)and Ww(Z):= γ≡.
The notation〈φ, Z stresses the dependence of this estimator on φ and z. The quantity
wψ (Zk) are called the kth importance weight and wsψk is its self-normalised version. For
readability, we have dropped the dependence of wsψk on z ∈ ZK from the notation.
T^⅛	ITC♦ r	/	∖	1 Λ	1 ∙ 1 a / I ∖ Γ / 1 ∖	l	/ TC ∖~∖ / τ/ ♦
Remark 1. Since Z§ = qe(wψ), the quantity Z§(φ, Z := [wψ (Z1) + ... + wψ(ZK)]/K is an
unbiased (‘importance-sampling’) estimator of Zθ .
Remark 2. The Self-normalised estimate π§ (φ, z)( f) is typically not unbiased. Under mild
assumptions (e.g. if sup wψ < ∞), its bias vanishes at rate O(K-1), its standard deviation
vanishes at Monte-Carlo rate O(K-112) and π§(φ, z)(f) → n§(f) almost surely as K → ∞.
2.2	Importance weighted autoencoder (IWAE)
Objective. The importance weighted autoencoder (IWAE), introduced by Burda et al.
(2016), seeks to find a value θ* of the generative-model parameters θ which maximises a
lower bound LψK on the log-marginal likelihood (‘evidence’). This bound depends on the
inference-network parameters φ and the number of samples, K ≥ 1:
ψ* := (θR) := arg maxw LK, LK := E [log Z (Φ,Z∖	⑴
where the expectation is w.r.t. Z 〜qφK. For any finite K, optimisation of the inference-
network parameters φ tightens the evidence bound. Burda et al. (2016) prove that for any
φ we have that LwK ↑ log Z§ as K → ∞. If K = 1, the IWAE reduces to the variational
autoencoder (VAE) from Kingma & Welling (2014). However, for K > 1, as pointed out in
Cremer et al. (2017); Domke & Sheldon (2018), the IWAE also constitutes another VAE
on an extended space based on an auxiliary-variable construction developed in Andrieu &
Roberts (2009); Andrieu et al. (2010); Lee (2011) (see, e.g. Finke, 2015, for a review).
3
Under review as a conference paper at ICLR 2020
Standard reparametrisation gradient. The gradient of the IWAE objective from (1):
VψLK = E[vψ log Z(φ, Z + Gψ(z)], with Gψ(z)=log Z@z)工K=I Vψ log qΦ(Zk). The
intractable quantity E Gψ (z) can be approximated unbiasedly via a vanilla Monte Carlo
approach using a single (K-dimensional) sample point Z =(z-,..., zκ)〜qφK. Unfortunately,
this approximation typically has such a large variance that it becomes impracticably noisy
(Paisley et al., 2012). To remove this high-variance term, the well known reparametrisation
trick (Kingma & Welling, 2014) is usually employed. It requires the following assumption.
(R1) There exists a distribution qe on some space E and a family of differentiable mappings
hφ : E → Z such that if E 〜 qe we have that Z = Z (E) = hφ (E)〜 q@.
Under R1, with E1..., EK 灯 qe and Zk := Z (Ek) := hφ (Ek), the gradient can be expressed as
KK
VψLκ = E E W Vψ log wψ(Zk) = E Σ W (Hψ(Zk)θ-VYiog qφ(Zk))；⑵
with Hψ(Z) := Vφ[log◦ wψ/ ◦ hφ]|ψ/=ψ(h-1(Z)). Here, the notation ψ! indicates that one does
not differentiate wψ w.r.t. ψ. The IWAE then uses a vanilla Monte Carlo estimate of (2),
-ʌ ■ VΘvae(φ, z)	K :=E w ψ k=1	Vθ log Yθ (Zk)	^	.	(3)
ʌ V Φvae( θ, Z)		Hψ(Zk) - Vφ log qφ(Zk)	
Before proceeding, we state the following lemma, proved in Tucker et al. (2019, Section 8.1),
which generalises of the well-known identity qφ(Vφ log qφ) = 0.
Lemma 1 (Tucker et al. (2019)). Under R1, for suitably integrable fψ : Z → R, we have
qφ ( fψ Vφ log qφ ) = qe ( Vφ [ fψ1 ◦ hφ] | ψf=ψ ) = qφ (Vφ [ fψ1 ◦ hφ] |ψf=ψ ◦ hφ ) .
We now exclusively focus on the φ-portion of the IWAE gradient, V^ a z).
Remark 3 (drawbacks of the IWAE φ-gradient). The gradient Vφw (θ,z) has three
drawbacks. The last two of these are attributable to the ‘score-function’ terms Vφ log qφ(Z)
in the φ-gradient portion of (3).
•	Reliance on reparametrisations. A reparametrisation a la R1 is necessary to
remove the high-variance term Gψ (z). For, e.g. discrete, models that violate R1,
control-variate approaches (Mnih & Rezende, 2016) or continuous relaxations have
been proposed but these incur additional implementation, tuning and computation
costs whilst not necessarily reducing the variance (Le et al., 2019).
•	Vanishing signal-to-noise ratio. The φ-gradient breaks down in the sense that
its signal-to-noise ratio vanishes as E[VΦWAe(θ, z)]/ var[V^vae(θ, z)]112 = O(KT12)
(Rainforth et al., 2018). This is because VφVAE(θ,z) constitutes a self-normalised
importance-sampling approximation of πθ (Hψ -Vφ log qφ) = 0, an identity which
directly follows from Lemma 1 with fψ = wψ .
•	Inability to achieve zero variance. As pointed out in Roeder et al. (2017),
var[VφVAe(θ, z)] > 0 even in the ideal scenario where q@ = ∏θ despite the fact that in
this case, wψ is constant and hence var [log Zθ (φ, z)] = 0.
Two modifications of VYV (θ, Z) have been proposed which (under R1) avoid the score-
function terms in (3) and hence (a) exhibit a stable signal-to-noise ratio as K → ∞ and
(b) can achieve zero variance if qφ = πθ (because then Hψ ≡ 0 since wψ is constant).
•	IWAE-STL. The ‘sticking-the-landing’ IWAE (IWAE-STL) gradient proposed by
Roeder et al. (2017) heuristically ignores the score function terms,
K
V Φvae-stl( θ, z):=工 wψ< ψ (Zk).	⑷
k=1
As shown in Tucker et al. (2019)), this introduces an additional bias whenever K > 1.
4
Under review as a conference paper at ICLR 2020
•	IWAE-DREG. The ‘doubly-reparametrised’ IWAE (IWAE-DREG) gradient pro-
posed by Tucker et al. (2019) removes the score-function terms through Lemma 1,
K
V 5vae-dreg( θ 公I = £(诚)2 H ψ (Zk).
k=1
(5)
一	.. 4…一…~ ，-,	- d…一,.	一.
The quantities v∖e-drega Z and Vφv <φ, z∣ are equal in expectation.
2.3	Reweighted wake-sleep (RWS)
The reweighted wake-sleep (RWS) algorithm was proposed in Bornschein & Bengio (2015).1
Letting KL(P∣∣q) := JZ log[P(Z)/q(Z)]P(Z) dZ is the Kullback-Leibler (KL)-divergence from P
to q, the RWS algorithm seeks to optimise ψ = (θ, φ) as
θ θ* := arg maxg log Zθ ,
∖φ: = arg minφ KL(∏θ* ∣∣qψ).
The θ- and φ-gradients read
Vθ log Zθ
-Vφ KL(∏θ ∣qφ)
Vθ log γθ
πθ Vφ log qφ .
(6)
These quantities are usually intractable and therefore approximated by replacing πθ by the
self-normalised importance sampling approximation ∏θ (φ, z∣ (this does not require R1):
K
VRws(φ, Z∣ =「k Vθ log Yθ (Zk)
VΦws(θ, z“ = k=1 ψ [▽ φ log qφ(Zk)_
(7)
Since (7) relies on self-normalised importance sampling, Remark 2 shows that its bias relative
to (6) is of order O(1/K). Appendix A discusses the impact of this bias on the φ-gradient in
more detail. The optimisation of both θ and φ is carried out simultaneously, allowing both
gradients to share the same particles and weights. Nonetheless, the lack of a joint objective
(for both θ and φ) is often viewed as the main drawback of RWS.
RWS-DREG. Under R1, Tucker et al. (2019) proposed the following ‘doubly-
reparametrised, RWS (RWS-DREG) gradient which is equal to Vφws(θ, z∣ in expectation and
is derived by applying Lemma 1 to the latter. It reads
K
Vφws-dreg(θ, z∣ =工 F (wψ ) Hψ(Zk),	⑻
k=1
where the function F(w) := w(1 -w ) is used to transform the self-normalised importance
weights wsψk . In high-dimensional settings, it is typically the case that the ordered self-
normalised importance weights wsψ(K) < . . . < wsψ(1) < 1 are such that wsψ(1) ≈ 1 - wsψ(2) and
W(k)《Wψ2 for k ≥ 3. The transformed weights {F(Wψ)}κ=i are then mainly supported on
the two particles with the largest self-normalised weights.
3 AISLE: A unified adaptive importance-sampling framework
3.1	Objective
If θ is fixed, the RWS algorithm reduces to an adaptive importance-sampling scheme which
optimises the proposal distribution by minimising the ‘inclusive’ KL-divergence from the
target distribution ∏θ to the proposal qφ (see, e.g., Douc et al., 2007; Cappe et al., 2008). If
instead φ is fixed, the RWS algorithm reduces to a stochastic-approximation algorithm for
estimating the MLE of the generative-model parameters θ. The advantage of optimising θ
1Following Tucker et al. (2019) (based on empirical results in Le et al. 2019), we only use the
‘wake-phase’ φ-updates for RWS.
5
Under review as a conference paper at ICLR 2020
and φ simultaneously is that (a) Monte Carlo samples used to approximate the θ-gradient
can be re-used to approximate the φ-gradient and (b) optimising φ typically reduces the
error (both in terms of bias and variance) of the θ-gradient approximation.
However, adapting the proposal distribution qφ in importance-sampling schemes need not
necessarily be based on minimising the (inclusive) KL-divergence. Numerous other techniques
exist in the literature (e.g. Geweke, 1989; Evans, 1991; Oh & Berger, 1992; Richard & Zhang,
2007; Cornebise et al., 2008) and may sometimes be preferable. Indeed, another popular
approach with strong theoretical support is based on minimising the χ2-divergence (see,
e.g., Deniz Akyildiz & Miguez, 2019). Based on this insight, We slightly generalise the
RWS-objective as
θ θ* := arg maxθ log Zθ ,
I。* := argmin $ Df( ∏e* ∣∣ qφ) ∙
(9)
Here, Df(P∣∣q) := JZ f(P(Z)/q(Z))q(Z) dZ is some f-divergence from P to q. We reiterate that
alternative approaches for optimising φ (which do not minimise f-divergences) could be used.
HoWever, We state (9) for concreteness as it suffices for the remainder of this Work; We call
the resulting algorithm adaptive importance sampling for learning (AISLE). As will become
clear below, this unified framework permits a straightforward and principled derivation of
robust φ-gradient estimators that do not degenerate as K → ∞.
3.2	θ-gradient
Optimisation is again performed via a stochastic gradient-ascent. The intractable θ -gradient
Vθ log Zθ = ∏θ(Vθ log γθ) is approximated as in RWS, i.e. for Z 〜q$ K:
K
V AISLE( φ,公I= V 詈"Φ,公I = V θvae( Φ,公I =工 wψ V θ log Yθ (Zk).
k=1
The θ-gradient is thus the same for all algorithms discussed in this work although the IWAE-
paradigm views it as an unbiased gradient of a (biased) lower-bound to the evidence, while
AISLE (and RWS) interpret it as a self-normalised importance-sampling (and consequently
biased) approximation of the gradient Vθ log Zθ for the ‘exact’ objective.
3.3	φ-grAdIEnt
3.3.1	GEnErAL dErIvAtIon
In the derivations to follow, integrals of the form πθ([F ◦ wψ]Vφ log qφ) naturally appear.
These can also be expressed as Zθ-1 qφ([H ◦ wψ]Vφ log qφ) with H(y) := F (y)y. By Lemma 1,
πθ([F ◦ wψ]Vφlogqφ)
=Z- E Z ~ qφ [wψH'(wψ(Z))▼ ψ(Z)].
Zθ
Approximating the expectation as well as the normalising constant Zθ on the r.h.s. with the
vanilla Monte Carlo method with K samples Z1...,ZK 〜q$K yields the estimator
K
∏θ([F ◦ Wψ]Vφ log q$) ≈ 工 Wψ H/(wψ(Zk)) ▼ ψ(Zk).	(10)
k=1
Remark 2 shows that this approximation has a bias of order O(K-1) and a standard-
deviation of order O(K-1/2). Now, most of the f-divergences used for variational in-
ference in intractable models are such that there exists a function f: R → R satisfying
Df(∏θ Ilq$) = ZK JZ f[wψ(Z)]q$(Z) dZ + C(θ) for an exponent K ∈ R and constant C(θ) inde-
pendent of φ. In other words, for a given value ofθ, the optimization of the f-divergence
as a function of φ can be carried out without relying on the knowledge of Zθ . Writing
g(y) := f/(y) — f (y)/y, simple algebra then directly shows that
-Vφ
Df( πθ Il q$) = Z κ +1/
g(wψ(Z))[Vφlogqφ(Z)]πθ(Z) dZ.
(11)
6
Under review as a conference paper at ICLR 2020
Since the integral in (11) is an expectation with respect to πθ, it can be approximated with self-
importance sampling, possibly multiplied an additional importance-sampling approximation
Zθ(φ, Z of Zθ raised to some power. This leads to,
K
-▽ φ Df(∏θ Il qφ) ≈Zθ(φ, Z〉K+1 工 Wψg(wψ (Zk))▽ φ log qφ(Zk).	(12)
k=1
Indeed, Equation (10) applies to (11), leading to the reparametrised estimator
K
—▽ φ Df(∏θ Ilqφ) ≈ Zθ (φ, ZK+1 工 wψ hKwψ(Zk))Hψ(Zk),	(13)
k=1
where h(y) = g(y)y and g : R → R given immediately above (11). We now describe several
particular cases.
3.3.2 Special case ‘inclusive’ KL-divergence: RWS and IWAE-STL
We have KL(∏θ∣∣qφ) = ZK JZ f(wψ(Z))qφ(Z)dZ + C(θ) With K = -1 and f(y) = y log(y). In
that case, with the notations of Section 3.3.1, we have g(y) = 1 and h/(y) = 1.
• AISLE-KL-NOREP/RWS. Without relying on any reparametrisation, Equation
(12) yields the following gradient, which clearly equals Vφws(θ, z)：
K
-Vφ Df(∏θ I qφ) ≈ Vφisle-kl-norep(θ, z):=工 wψ Vφ log qφ(Zk).	(14)
k=1
• AISLE-KL. Using reparametrisation, Equation (13) yields the gradient：
K
-Vφ Df(∏θ I qφ) ≈ Vφisle-kl(θ, z):=工 wψpψ (Zk).	(15)
k=1
We thus arrive at the following result which demonstrates that IWAE-STL can be derived in
a principled manner from AISLE, i.e. without the need for a multi-sample objective.
ʌ ___ ___ . ʌ________ _ .
Proposition 1. For any (θ, φ, z), V^ISLE-KL(θ, z) = V£ stl g, z).
Proposition 1 is notable because it shows that IWAE-STL (which avoids the breakdown
highlighted in Rainforth et al. (2018) and which can also achieve zero variance) can be
derived in a principled manner from AISLE, i.e. without relying on a multi-sample objective.
Proposition 1 thus provides a theoretical basis for IWAE-STL which was previously viewed
as an alternative gradient for IWAE for which it is biased and only heuristically justified.
Furthermore, the fact that IWAE-STL exhibited good empirical performance in Tucker et al.
(2019) even in an example in which RWS broke down, suggests that this breakdown may not
be due to RWS’ lack of optimising a joint objective as previously conjectured.
Finally, recall that Tucker et al. (2019) obtained an alternative ‘doubly-reparametrised’
RWS φ-gradient VΦws-dreg(θ, Z) given in (8) by first replacing the exact (but intractable)
φ-gradient by the self-normalised importance-sampling approximation Vφws(θ, z) and then
applying the identity from Lemma 1. Note that this may result in a variance reduction but
does not change the bias of the gradient estimator. In contrast, AISLE-KL is derived by
first applying Lemma 1 to the exact (RWS) φ-gradient and then approximating the resulting
expression. This can potentially reduce both bias and variance.
3.3.3 Special case α-divergence: IWAE-DREG
Up to some irrelevant additive constant, the α-divergence between two distributions p
and q is given by fz(P(Z)/q(Z))αq(Z) dZ for some α > 1. This can also be expressed as
ZK JZ f(wψ(Z))qφ(Z) dZ with κ = — α and f(y) = yα. In this case, with the notation from
Section 3.3.1, we have g(y) = (α — 1)yα-1 and h/(y) = α(α — 1) yα-1. Note that the case a =
2 is equivalent, up to an irrelevant additive constant, to a standard χ2-divergence. Minimising
this divergence is natural in importance sampling since X2(∏θ∣∣q$) = VarZ〜qφ [wψ/Zθ] is the
variance of the importance weights.
7
Under review as a conference paper at ICLR 2020
• AISLE-α-NOREP. Without relying on any reparametrisation, Equation (13)
yields
Vφisle-α-norep(θ, Z := (α - 1)KαT ⅛(wψ)α▽ φ log qφ(Zk),	(16)
k=1
with the following special case which is al2so proportional to the ‘score gradient’ from
Dieng et al. (2017, Appendix G): Vφisle-X -NOREPG Z ：= K EK=乂Wψ)2Vφ log qφ(zk).
• AISLE-α. Using reparametrisation, Equation (12) becomes
K
Vφisle-α (θ,公I ：= α(α - 1)KɑT £(就)αHψ(Zk),	(17)
k=1
again with the special case Vφisle-X (θ, z∣ := 2K EK=ι(Wψ)2Hψ(zk).
This demonstrates that IWAE-DREG can be derived (up to the proportionality factor 2K)
in a principled manner from AISLE, i.e. without the need for a multi-sample objective.
Proposition 2. For any (θ,φ,z), V Φisle- X (θ, z∣ = 2 K V ^e-dreg(。,工〉.	口
Note that if the implementation normalises the gradients, e.g. as effectively done by ADAM
(Kingma & Ba, 2015), the constant factor cancels out and AISLE-χ2 becomes equivalent
to IWAE-DREG. Otherwise (e.g. in plain stochastic gradient-ascent) this shows that the
learning rate needs to be scaled as O(K) for the IWAE or IWAE-DREG φ-gradients.
3.3.4 SpEcial casE ‘ExclusivE’ KL-divERgENcE.
For the 'exclusive' KL-divergence, We have KL(qψ∣∣∏θ) = J f (wψ(Z))qψ(Z) dZ + C(θ) with
f(y) = log(y). In that case, with the notation from Section 3.3.1, we have hz(y) = 1 /y. This
directly leads to the following approximation,
1K
-V φ Df( ∏θ ∣ qφ) ≈ V φisle-rev-kl( θ, z∣ ：= K £h ψ (Zk).
k=1
This can be recognised as a simple average over K independent replicates of the ‘sticking-
the-landing’ estimator for VAEs proposed in Roeder et al. (2017, Equation 8). As we discuss
in Appendix A, optimising this ‘exclusive’ KL-divergence can sometimes lead to faster
convergence of φ than optimising the ‘inclusive, KL-divergence KL(∏θ∣∣qφ). However, care
must be taken because minimising the exclusive divergence does not necessarily lead to well
behaved or even well-defined importance weights and thus can negatively affect learning of θ
(whose gradient is a self-normalised importance-sampling approximation which makes use of
those weights).
4 CONclusiON
We have shown that the adaptive-importance sampling paradigm of the reweighted wake-sleep
(RWS) (Bornschein & Bengio, 2015) is preferable to the multi-sample objective paradigm of
importance weighted autoencoders (IWAEs) (Burda et al., 2016) because the former achieves
all the goals of the latter whilst avoiding its drawbacks. To formalise this argument, we
have introduced a simple, unified adaptive-importance-sampling framework termed adaptive
importance sampling for learning (AISLE) (which slightly generalises the RWS algorithm)
and have proved that AISLE allows us to derive the ‘sticking-the-landing’ IWAE (IWAE-STL)
gradient from Roeder et al. (2017) and the ‘doubly-reparametrised’ IWAE (IWAE-DREG)
gradient from Tucker et al. (2019) as special cases.
We hope that this work highlights the potential for further improving variational techniques
by drawing upon the vast body of research on (adaptive) importance sampling in the
computational statistics literature. Conversely, the methodological connections established
in this work may also serve to emphasise the utility of the reparametrisation trick from
Kingma & Welling (2014); Tucker et al. (2019) to computational statisticians.
In a companion article, we are extending the present work to the variational sequential
Monte Carlo methods from Maddison et al. (2017); Le et al. (2018); Naesseth et al. (2018)
and to the tensor Monte Carlo approach from Aitchison (2018).
8
Under review as a conference paper at ICLR 2020
References
Laurence Aitchison. Tensor Monte Carlo: particle methods for the GPU era. arXiv e-prints,
art. arXiv:1806.08593, Jun 2018.
Christophe Andrieu and Gareth O Roberts. The pseudo-marginal approach for efficient
Monte Carlo computations. The Annals of Statistics, 37(2):697-725, 2009.
Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov chain Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
72(3):269-342, 2010. With discussion.
Robert Bamler, Cheng Zhang, Manfred Opper, and Stephan Mandt. Perturbative black box
variational inference. Advances in Neural Information Processing Systems (NeurIPS), pp.
5079-5088, 2017.
Jorg Bornschein and Yoshua Bengio. ReWeighted wake-sleep. In 3rd International Conference
on Learning Representations (ICLR), 2015.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.
In 4th International Conference on Learning Representations (ICLR), 2016.
Olivier Cappe, Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert.
Adaptive importance sampling in general mixture classes. Statistics and Computing, 18
(4):447-459, 2008.
Julien Cornebise, Eric Moulines, and Jimmy Olsson. Adaptive methods for sequential
importance sampling with application to state space models. Statistics and Computing, 18
(4):461-480, 2008.
Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting importance-weighted
autoencoders. In 5th International Conference on Learning Representations (ICLR), 2017.
Omer Deniz Akyildiz and JOaqUin Miguez. Convergence rates for optimised adaptive
importance samplers. arXiv e-prints, art. arXiv:1903.12044, 2019.
Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, and David Blei. Variational
inference via χ upper bound minimization. Advances in Neural Information Processing
Systems (NeurIPS), pp. 2732-2741, 2017.
Justin Domke and Daniel R Sheldon. Importance weighting and variational inference.
Advances in Neural Information Processing Systems (NeurIPS), pp. 4475-4484, 2018.
Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Convergence
of adaptive mixtures of importance sampling schemes. The Annals of Statistics, 35(1):
420-448, 2007.
Michael Evans. Adaptive importance sampling and chaining. Statistical Numerical Integration,
Contemporary Mathematics, 115:137-143, 1991.
Axel Finke. On extended state-space constructions for Monte Carlo methods. PhD thesis,
Department of Statistics, University of Warwick, UK, 2015.
John Geweke. Bayesian inference in econometric models using Monte Carlo integration.
Econometrica, 57(6):1317-1339, 1989.
Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical
Statistics, 17(2):295-311, 2008.
Diederik P Kingma and Jimmy Lei Ba. ADAM: A method for stochastic optimization. In
3rd International Conference on Learning Representations (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In 2nd International
Conference on Learning Representations (ICLR), 2014.
9
Under review as a conference paper at ICLR 2020
Augustine Kong, Jun S. Liu, and Wing Hung Wong. Sequential imputations and Bayesian
missing data problems. Journal of the American Statistical Association, 89(425):278-288,
1994.
Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood. Auto-encoding
sequential Monte Carlo. In 6th International Conference on Learning Representations
(ICLR), 2018.
Tuan Anh Le, Adam R Kosiorek, N Siddharth, Yee Whye Teh, and Frank Wood. Revisiting
reweighted wake-sleep for models with stochastic control flow. In Proceedings of the 35th
Conference on Uncertainty in Artificial Intel ligence (UAI), 2019.
Anthony Lee. On auxiliary variables and many-core architectures in computational statistics.
PhD thesis, Department of Statistics, University of Oxford, UK, 2011.
Jun S Liu. Metropolized independent sampling with comparisons to rejection sampling and
importance sampling. Statistics and Computing, 6(2):113-119, 1996.
Jun S. Liu. Monte Carlo Strategies in Scientific Computing. Springer Series in Statistics.
Springer, 2001.
Chris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy
Mnih, Arnaud Doucet, and Yee Whye Teh. Filtering variational objectives. Advances in
Neural Information Processing Systems (NeurIPS), pp. 6573-6583, 2017.
A. Mnih and D. J. Rezende. Variational inference for Monte Carlo objectives. In 33rd
International Conference on Machine Learning (ICML), 2016.
Christian A Naesseth, Scott W Linderman, Rajesh Ranganath, and David M Blei. Variational
sequential Monte Carlo. In 21st International Conference on Artificial Intel ligence and
Statistics (AISTATS), 2018.
Man-Suk Oh and James O Berger. Adaptive importance sampling in Monte Carlo integration.
Journal of Statistical Computation and Simulation, 41(3-4):143-168, 1992.
John Paisley, David Blei, and Michael Jordan. Variational Bayesian inference with stochastic
search. In 29th International Conference on Machine Learning (ICML), 2012.
Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank
Wood, and Yee Whye Teh. Tighter variational bounds are not necessarily better. In
Bayesian Deep Learning (NeurIPS 2018 workshop), 2018.
Jean-Franois Richard and Wei Zhang. Efficient high-dimensional importance sampling.
Journal of Econometrics, 141(2):1385-1411, 2007.
Geoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-
variance gradient estimators for variational inference. Advances in Neural Information
Processing Systems (NeurIPS), pp. 6925-6934, 2017.
George Tucker, Dieterich Lawson, Shixiang Gu, and Chris J Maddison. Doubly reparameter-
ized gradient estimators for Monte Carlo objectives. In 7th International Conference on
Learning Representations (ICLR), 2019.
Ming Xu, Matias Quiroz, Robert Kohn, and Scott A Sisson. Variance reduction properties
of the reparameterization trick. In The 22nd International Conference on Artificial
Intel ligence and Statistics (AISTATS), pp. 2711-2720, 2019.
10
Under review as a conference paper at ICLR 2020
A On the r6le of the self-normalisation bias within
RWS/AISLE
A.1 The self-normalisation bias
Within the self-normalised importance-sampling approximation, the number of particles, K ,
interpolates between two extremes:
•	As K ↑ ∞, πθ〈φ, z〉( f) becomes an increasingly accurate approximation of ∏θ (f).
•	For K = 1, however, πθ〈φ, z〉( f) = f (Z 1) reduces to a vanilla Monte Carlo approx-
imation of qφ (f) (because the single self-normalised importance weight is always
equal to 1).
This leads to the following insight about the estimators Vφisle-kl(θ, Z and Vφisle-X <θ, z).
•	As K ↑ ∞, these two estimators become increasingly accurate approxi-
mations of the 'inclusive'-divergence gradients -Vφ KL(∏θ ∣∣q$) = ∏θ(Hφ) and
-Vφ X2(∏θIqφ) = 2∏θ([wψ/Zθ]Hφ), respectively.
•	For K = 1, however, these two estimators reduce to vanilla Monte Carlo ap-
proximations of the 'exclusive'-divergence gradients -Vφ KL(q@∣∣∏θ) = q$(Hφ) and
-2Vφ KL(qφI∏θ) = 2q@(Hφ), respectively.
This is similar to the standard IWAE φ-gradient which also represents a vanilla Monte Carlo
approximation of -Vφ KL(q@∣∣∏θ) if K = 1 as IWAE reduces to a VAE in this case.
Characterising the small-K self-normalisation bias of the reparametrisation-free AISLE φ
gradients, AISLE-KL-NOREP and AISLE-χ1 2-NOREP, is more difficult because if K = 1, they
constitute vanilla Monte Carlo approximations of qφ(Vφ log qφ) = 0. Nonetheless, Le et al.
(2019, Figure 5) lends some support to the hypothesis that the small-K self-normalisation
bias of these gradients also favours a minimisation of the exclusive KL-divergence.
A.2 Inclusive vs exclusive KL-divergence minimisation
Recall that the main motivation for use of IWAEs (instead of VAEs) was the idea that
we could use self-normalised importance-sampling approximations with K > 1 particles to
reduce the bias of the θ-gradient relative to Vθ log Zθ . The error of such (self-normalised)
importance-sampling approximations can be controlled by ensuring that qφ is close to πθ (in
some suitable sense) in any part of the space Z in which πθ has positive probability mass.
For instance, it is well known that the error will be small if the ‘inclusive’ KL-divergence
KL(∏θIqφ) is small as this implies well-behaved importance weights. In contrast, a small
‘exclusive, KL-divergence KL(q@∣∣∏) is not sufficient for well-behaved importance weights
because the latter only ensures that qφ is close to πθ in those parts of the space Z in which
qφ has positive probability mass.
Let Q := {qφ} (which is indexed by φ) be the family of proposal distributions/the variational
family. Then we can distinguish two scenarios.
1. Sufficiently expressive Q. For the moment, assume that the family Q is flexible
(‘expressive') enough in the sense that it contains a distribution q®* which is (at least
approximately) equal to ∏θ and that our optimiser can reach the value φ* of φ. In this
case, minimising the exclusive KL-divergence can still yield well-behaved importance
weights because in this case, φ* := arg min @ KL( ∏θ ∣∣ q$) is (at least approximately)
equal to arg min@ KL(q@∣∣∏θ).
2. Insufficiently expressive Q. In general, the family Q is not flexible enough
in the sense that all of its members are ‘far away’ from πθ , e.g. if the D
components z1 , . . . , zD of z = z1:D are highly correlated under πθ whilst
q@(Z) = ∩dD=1 q@,d(Zd) is fully factorised. In this case, minimising the exclusive
KL-divergence could lead to poorly-behaved importance weights and we should
optimise φ := arg min@ KL(∏θ ∣∣ q@) as discussed above.
11
Under review as a conference paper at ICLR 2020
Remark 4. In Scenario 1 above, i.e. for a sufficiently flexible Q, using a gradient-descent
algorithm which seeks to minimise the exclusive divergence can sometimes be preferable to a
gradient-descent algorithm which seeks to minimise the inclusive divergence. This is because
both find (approximately) the same optimum but the latter may exhibit faster convergence
in some applications. In such scenarios, the discussion in Subsection A.1 indicates that a
smaller number of particles, K, could then be preferable for some of the φ-gradients because
(a) the O(K-1) self-normalisation bias outweighs the O(K-1/2) standard deviation and
(b) the direction of this bias may favour faster convergence.
Unfortunately, simply setting K = 1 for the approximation of the φ-gradients2 is not
necessarily optimal because
•	even in the somewhat idealised scenario 1 above and even if the direction of the
self-normalisation bias encourages faster convergence, increasing K is still desirable
to reduce the variance of the gradient approximations and furthermore, even in
this scenario, seeking to optimise the exclusive KL-divergence could lead to poorly
behaved importance-sampling approximations of the θ-gradient whenever φ is still
far away from optimal;
•	not using the information contained in all K particles and weights (which have
already been sampled/calculated to approximate the θ-gradient) seems wasteful;
•	if K = 1, the reparametrisation-free AISLE φ-gradients, AISLE-KL-NOREP and
AISLE-χ2-NOREP are simply vanilla Monte Carlo estimates of 0 and the RWS-
DREG φ-gradient is then equal to 0.
B Empirical illustration
B.1	Algorithms
In these supplementary materials, we illustrate the different φ-gradient estimators (recall
that all algorithms discussed in this work share the same θ-gradient estimator). Specifically,
we compare the following approximations.
•	AISLE-KL-NOREP. The gradient for AISLE based on the KL-divergence without
any further reparametrisation from (14) i.e. this coincides with the standard RWS-
gradient from (7). This gradient does not require R1 but does not achieve zero
variance even if qφ = πθ .
•	AISLE-KL. The gradient for AISLE based on the KL-divergence after reparametris-
ing and exploiting the identity from Lemma 1; it is given by (15) and coincides with
the IWAE-STL-gradient from (4).
•	AISLE-χ2-NOREP. The gradient for AISLE based on the χ2-divergence without
any reparametrisation given in (16). This gradient again does not require R1 but
does not achieve zero variance even if qφ = πθ .
•	AISLE-χ2 . The gradient for AISLE based on the χ2-divergence after reparametris-
ing and exploiting the identity from Lemma 1; it is given by (17) and is alsow
proportional to IWAE-DREG from Tucker et al. (2019) which was stated in (5).
When normalising the gradients (as, e.g. implicitly done by optimisers such as ADAM
Kingma & Ba, 2015) the proportionality constant cancels out so that both these
gradient approximations lead to computationally the same algorithm.
•	IWAE. The gradient for IWAE employing the reparametrisation trick from Kingma
& Welling (2014). Its sampling approximation is given in (3). Recall that this is
the φ-gradient whose signal-to-noise ratio degenerates with K as pointed out in
Rainforth et al. (2018) (and which also cannot achieve zero variance even if qφ = πθ).
2 Within the IWAE-paradigm, using different numbers of particles for the θ and φ-gradients was
recently proposed in Rainforth et al. (2018); Le et al. (2018) who termed this approach ‘alternating
evidence lower bounds’, albeit their aim was to circumvent the signal-to-noise ratio breakdown of
the IWAE φ-gradient which is distinct from the phenomenon discussed here.
12
Under review as a conference paper at ICLR 2020
•	IWAE-DREG. The ‘doubly-reparametrised’ IWAE gradient from (5) which was
proposed in Tucker et al. (2019). It is proportional to AISLE-χ2 .
•	RWS-DREG. The ‘doubly-reparametrised’ RWS φ-gradient from (8) which was
proposed in Tucker et al. (2019) who derived it by applying the identity from
Lemma 1 to the RWS φ-gradient.
B.2	Model
Generative model. We have N D-dimensional observations x(1) , . . . , x(N) ∈ RD and N
D-dimensional latent variables z(1) , . . . , z(N) ∈ RD. Unless otherwise stated, any vector
y ∈ RD is to be viewed as a D × 1 column vector.
Hereafter, wherever necessary, we add an additional subscript to make the dependence on
the observations explicit. The joint law (the ‘generative model’), parametrised by θ, of the
observations and latent variables then factorises as
NN
∏ Pθ(Z(n))Pθ(2(n)1 Z(n)) = ∏ γθ,x(n) (z(n)).
n=1	n=1
We model each latent variable-observation pair (z, x) as
Pθ(Z) := N(Z; μ,∑),
Pθ(X|Z) := N(X; Z;I),
where θ := μ = μ 1：D ∈ RD, where Σ := (σd,d，)(d,d,)∈{i,…，d} ∈ RD×D is assumed to be known
and where I denotes the D × D-identity matrix. For any θ,
Zθ,x = pθ (x) = N( x; μ, I + ∑),	(18)
∏θ,x(Z) = Pθ(Z|X) = N(Z； Vθ,χ, P),	(19)
with P :=(Σ-1 + I)-1 and vθ χ := P(Σ-1 μ + x).	In particular, (18) implies that
θ ml = Nn Σ n=ι X (n )∙	,
Proposal/variational approximation. We take the proposal distributions as a fully-
factored Gaussian:
qφ,x(Z) := N(Z; Ax +b,C),	(20)
where A = (ad,d,)(d,d,)∈{i,…D产 ∈ RD×D, b = b 1：D ∈ RD and, for C1：D =: c ∈ RD, C :=
diag(e2c1 , . . . , e2cD ). The parameters to optimise are thus
φ := (a1T, . . . ,aTD,bT,cT),
where ad := [ad,1, ad,2, . . . , ad,D]T ∈ RD×1 denotes the column vector formed by the elements
in the dth row of A. Furthermore, for the reparametrisation trick, we take q(E) := N(E; 0, I),
where 0 ∈ RD is a vector whose elements are all 0, so that
hφ,x(E) := Ax + b + C1 //E,
which means that hφ-,1x(Z) = C -1/2 (Z - Ax - b).
Note that the mean of the proposal in (20) coincides with the mean of the posterior in (19)
if A = P and b = PΣ-1 μ.
This model is similar to the one used as a benchmark in Rainforth et al. (2018, Section 4)
and also in Tucker et al. (2019, Section 6.1) who specified both the generative model and
the variational approximation to be isotropic Gaussians. Specifically, their setting can be
recovered by taking Σ := I and fixing Cd = log(2/3)/2 so that C = 31 throughout. Here, in
order to investigate a slightly more realistic scenario, we also allow for the components of
the latent vectors Z to be correlated/dependent under the generative model. However, as
the variational approximation remains restricted to being fully factored, it may fail to fully
capture the uncertainty about the latent variables.
13
Under review as a conference paper at ICLR 2020
Gradient calculations. We end this subsection by stating the expressions needed to
calculate the gradients in the Gaussian example presented above. Throughout, we use
the denominator-layout notation for vector and matrix calculus and sometimes write
E = Ei：D = h—X(Z) to simplify the notation. Thus,
Vθ log γθ,χ (Z) = 2-1(Z - μ) ∈ RD,
VZ log γθ,χ (Z) = 2-1(μ - z) + x - z ∈ RD,	(21)
Vz log qφ,x(Z) = -C-1(Z - Ax - b)
=-C T /2 E ∈ R d.	(22)
Let ad := [ad,1, ad,2, . . . , ad,D]T ∈ RD×1 denote the column vector formed by the elements in
the dth row of A. Then, letting Θ denote elementwise multiplication and using the convention
that addition or subtraction of the scalar 1 is to be done elementwise,
Vad logqφ,x(Z) = exp(-2cd)(Zd - adTx - bd)x
=exp(- Cd) Edx ∈ RD,	d ∈ {1,..., D },
Vblogqφ,x(Z) = C-1(Z - Ax - b)
=C T /2 E ∈ RD,
Vc log qφ,χ(Z) = C-1 /2(Z — Ax — b) Θ C-1 /2(Z — Ax — b) — 1
=E Θ E — 1 ∈ RD,
Furthermore, write hφ,x = [hφ,x,1, . . . , hφ,x,D]T, i.e.
hφ,x,d(E) = Zd = ad x + bd + exp(Cd) Ed,
and let ι(d) = [0, . . . , 0, 1, 0, . . . , 0]T ∈ RD be the vector whose entries are all 0 except for the
dth entry which is 1. Then, for d ∈ {1, . . . , D},
[V ad z hφ,χ,d ](E) = 1{ d = d /} x ∈ RD, d / ∈ {1,..., D },	(23)
[Vbhφ,χ,d](E) = ι() ∈ RD,	(24)
[Vchφ,χ,d](E) = exp(Cd)Ed∣() ∈ RD	(25)
Again writing E = h-X(Z) implies that
Vφ [log ◦ wψ,x ◦ hφ,x] |ψ1 = ψ ( E) = [Vφhφ,x, 1, ..., Vφhφ,χ,D](E) VZ log wψ,x (Z),
so that, letting [Vz log wψ,x(Z)]d denote the dth element of the vector Vz log wψ,x(Z),
V ad [log ◦ Wψ ,χ ◦ hφ,χ ]| ψ,=ψ ( E ) =	[V Z log Wψ,χ ( Z )] dx,
Vb [log ◦ wψ,x ◦ hφ,x] |ψt=ψ (E) =	VZ log wψ,x (Z),
Vc [log ◦ wψ,x ◦ hφ,x] |ψt=ψ (E) =	E θ C / VZ log wψ,x (Z) .
From this, since
Vφ [log ◦ wψ,x ◦ hφ,x]( E) = Vφ [log ◦ wψ,x ◦ hφ,x] |ψt= ψ ( E) - Vφ log qφ,x (Z),
we have that
V ad [log ◦ Wψ,χ ◦ hφ,χ ]( E ) = ([V Z log Wψ,χ ( Z )] d - C T / 2 Ed ) x,
V b [log ◦ Wψ,χ ◦ hφ,χ ]( E ) = V Z log Wψ,χ ( Z ) - C T /2 E,
Vc[log ◦ Wψ,χ ◦ hφ,χ](E) = E Θ C1 /2VZ log wψ,χ(Z) - E Θ E + 1.
Impact of the reparametrisation. We end this subsection by briefly illustrating the
impact of the reparametrisation trick combined with the identity from Tucker et al. (2019)
which was given in Lemma 1. Recall that this approach yields φ-gradients that are expressible
as integrals of path-derivative functions Hψ,χ := Vφ[log ◦ wψ,χ ◦ hφ,χ] |ψ,=ψ ◦ h—X, Thus, if
there exists a value φ such that qφ,χ = ∏θ,χ then wψ,χ Y ∏θ,x∕qφ,x ≡ 1 is constant so that we
obtain zero-variance φ-gradients (see, e.g., Roeder et al., 2017, for a discussion on this).
For simplicity, assume that 2 = I and recall that we then have q®*,x = ∏θ,χ if the values
(A, b, C) implied by φ* are (Ak, b∖ C*) = (111, 2μ, 111).
14
Under review as a conference paper at ICLR 2020
By (21) and (22), and with the usual convention E = h-X(Z), We then have
VZ log wψ,χ (Z) = (x + μ) — 2Z + C-1(Z — Ax — b)
=2[( A^x + b) — (Ax + b) + C T / 2( C * — C) E ].	(26)
Note that the only source of randomness in this expression is the multivariate normal random
variable E. Thus, by (23) and (24), for any values of A and b and any K ≥ 1, the variance of
the A - and b -gradient portion of AISLE-KL/IWAE-STL and AISLE- X2/IWAE-DREG goes
to zero as C → C * = 21. In other words, in this model, these ‘score-function free, φ-gradients
achieve (near) zero variance for the parameters governing the proposal mean as soon as
the variance-parameters fall within a neighbourhood of their optimal values. Furthermore,
(25) combined with (26) shows that for any K ≥ 1, the variance of the C -gradient portion
also goes to zero as (A, b, C) → (A*, b*, C *). A more thorough analysis of the benefits of
reparametrisation-trick gradients in Gaussian settings is carried out in Xu et al. (2019).
B.3 Simulations
Setup. We end this section by empirically comparing the algorithms from Subsection B.1.
We run each of these algorithms for a varying number of particles, K ∈ {1, 10, 100},
and varying model dimensions, D ∈ {2, 5, 10}. Each of these configurations is repeated
independently 100 times. Each time using a new synthetic data set consisting of N = 25
observations sampled from the generative model after generating a new ‘true’ prior mean
vector as μ 〜N(0,1). Since all the algorithms share the same θ-gradient, we focus only on
the optimisation of φ and thus simply fix θ := θml throughout. We show results for the
following model settings.
•	Figure 1. The generative model is specified via Σ = I. In this case, there exists a
value φ* of φ such that qφχ(Z) = ∏θ,χ(Z). Note that this corresponds to Scenario 1
in Subsection A.2.
•	Figure 2. The generative model is specified via Σ = (0.95|d-d l+1)(d,df)∈{i,…D}2.
Note that in this case, the fully-factored variational approximation cannot fully
mimic the dependence structure of the latent variables under the generative model.
That is, in this case, qφ,χ(Z) = πθ,χ(Z) for any values of φ. Note that this corresponds
to Scenario 2 in Subsection A.2.
To initialise the gradient-ascent algorithm, we draw each component of the initial values
φ0 of φ IID according to a standard normal distribution. We use both plain stochastic
gradient-ascent with the gradients normalised to have unit L1-norm (Figures 1a, 2a) and
ADAM (Kingma & Ba, 2015) with default parameter values (Figures 1b, The total number
of iterations is 10, 000; in each case, the learning-rate parameters at the ith step are i-1/2 .
We also ran the algorithms in each of the above-mentioned scenarios with fixed values of cd ,
e.g. as in Rainforth et al. (2018); Tucker et al. (2019). However, we omit the results as this
did not significantly change the relative performance of the different algorithms. For the
same reason, we omit results related to the optimisation of A and C.
15
a. Gradient ascent.
b. ADAM.
Figure 1. Average L1-error of the estimates of the parameters b = b1:D governing the mean of the Gaussian variational family. The average
is taken over the D components of b and the figure displays the median error at each iteration over 100 independent runs of each algorithm,
each using a different data set consisting of 25 observations sampled from the model. Note the logarithmic scaling on the second axis. Here,
the covariance matrix Σ = I is diagonal.
10-3
10-1
10-2
10-3
10-4
1
10-1
10-2
10-4
1
gio-1
中 10-2
昌 10-3
10-4
1
1	2000	4000 6000	8000	1	2000	4000 6000 8000	1	2000	4000	6000 8000
Iteration
a. Gradient ascent.
Iteration
b. ADAM.
Figure 2. The same setting as in Figure 1 except that here, the covariance matrix Σ = (0.95|d-e|+1)(d,e)∈{1 ,...,D}2 is not a diagonal matrix.
Again, note the logarithmic scaling on the second axis.
Under review as a conference paper at ICLR 2020
Summary of results. Below, we outline what we believe to be the main takeaways from
these simulation results for this particular model. However, further theoretical analysis is
required to determine whether these hold in more general scenarios.
1.	The ‘score-function free’ KL-divergence based AISLE algorithms typically performed
somewhat better than their χ2-divergence based counterparts, i.e. AISLE-KL outper-
formed AISLE-χ2 . We conjecture that this is due to the fact that the χ2-divergence
based variants square the (self-normalised) importance weights which increases the
variance of the φ-gradients.
2.	The performance of the φ-gradients AISLE-KL-NOREP and AISLE-χ2-NOREP
(which do not use any reparametrisation) typically benefited strongly from moderate
(relative to the dimension of the latent variables) increases in the number of particles.
In the scenario shown in Figure 2, for larger K , these gradients almost attained
the performance of the ‘score-function free’ φ-gradient AISLE-KL/IWAE-STL and
outperformed AISLE-X2/IWAE-DREG. We conjecture that this is due to the fact
that in the scenario shown in Figure 2, the variational family does not include the
target distribution, i.e. qφ = πθ for any φ, and as a result, the main advantage of
the ‘score-function free, gradients — i.e. the fact that they can potentially achieve
zero variance — cannot be realised.
3.	The standard IWAE φ-gradient performed worse than the other methods in any
of the scenarios considered (except in the trivial case K = 1 in which IWAE
reduces to the VAE). Indeed, as expected, the performance of the standard IWAE
φ-gradient consistently worsened with increasing K. This can be attributed to the
issue highlighted in Rainforth et al. (2018) (see Subsection 2.2), i.e. to the fact that
the signal-to-noise ratio of this gradient vanishes as O(K-1/2) (as this gradient
constitutes a self-normalised importance-sampling approximation of an integral
which is equal to zero).
4.	More surprisingly, the ‘score-function free’ φ-gradients AISLE-KL/IWAE-STL,
AISLE-X2 /IWAE-DREG did not necessarily improve with increasing K. Indeed,
their performance sometimes became worse as can be seen most clearly in Figure 1.
We note that this cannot be explained by the signal-to-noise ratio decay (which
Rainforth et al. (2018) highlighted for the standard IWAE φ-gradient) because
the ‘score-function free’ φ-gradients do not constitute self-normalised importance-
sampling approximations of integrals which are equal to zero. Instead, we conjecture
that as discussed in Remark 4 in the scenario shown in Figure 1, the O(K-1)
self-normalisation bias of these gradients happens to be beneficial and outweighs the
O(K-1/2) standard-deviation decrease obtained from increasing K.
5.	The ‘doubly-reparametrised’ RWS-gradient RWS-DREG from Tucker et al. (2019)
and given in (8) performed well for a moderate to large number of particles. Though
it crucially requires K > 1 (note that for K = 1 this gradient is simply a vector of
zeros).
17