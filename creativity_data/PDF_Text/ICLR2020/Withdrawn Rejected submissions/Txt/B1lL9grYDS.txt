Under review as a conference paper at ICLR 2020
Efficient and Robust Asynchronous Feder-
ated Learning with Stragglers
Anonymous authors
Paper under double-blind review
Ab stract
We address the efficiency issues caused by the straggler effect in the recently
emerged federated learning, which collaboratively trains a model on decentral-
ized non-i.i.d. (non-independent and identically distributed) data across massive
worker devices without exchanging training data in the unreliable and heteroge-
neous networks. We propose a novel two-stage analysis on the error bounds of
general federated learning, which provides practical insights into optimization.
As a result, we propose a novel easy-to-implement federated learning algorithm
that uses asynchronous settings and strategies to control discrepancies between the
global model and delayed models and adjust the number of local epochs with the
estimation of staleness to accelerate convergence and resist performance deterio-
ration caused by stragglers. Experiment results show that our algorithm converges
fast and robust on the existence of massive stragglers.
1	Introduction
Distributed machine learning has received increasing attention in recent years, e.g., distributed
stochastic gradient descent (DSGD) approaches (Gemulla et al., 2011; Lan et al., 2017) and the
well-known parameter server paradigm (Agarwal & Duchi, 2011; Li et al., 2013; 2014). However,
these approaches always suffer from communication overhead and privacy risk (McMahan et al.,
2017). Federated learning (FL) (Konecny et al., 2016) is proposed to alleviate the above issues,
where a subset of devices are randomly selected, and training data in devices are locally kept when
training a global model, thus reducing communication and protecting user privacy. Furthermore,
FL approaches are dedicated to a more complex context with 1) non-i.i.d. (Non-independent and
identically distributed), unbalanced and heterogeneous data in devices, 2) constrained computing
resources with unreliable connections and unstable environments (McMahan et al., 2017; Konecny
et al., 2016).
Typically, FL approaches apply weight averaging methods for model aggregation, e.g., FedAvg
(McMahan et al., 2017) and its variants (Sahu et al., 2018; Wang et al., 2018; Kamp et al., 2018;
Leroy et al., 2019; Nishio & Yonetani, 2019). Such methods are similar to the synchronous dis-
tributed optimization domain. However, synchronous optimization methods are costly in synchro-
nization (Chen et al., 2018), and they are potentially inefficient due to the synchrony even when
collecting model updates from a much smaller subset of devices (Xie et al., 2019b). Besides, wait-
ing time for slow devices (i.e., stragglers or stale workers) is inevitable due to the heterogeneity
and unreliability as mentioned above. The existence of such devices is proved to affect the conver-
gence of FL (Chen et al., 2018). To address this problem, scholars propose asynchronous federated
learning (AFL) methods (Xie et al., 2019a; Mohammad & Sorour, 2019; Samarakoon et al., 2018)
that allow model aggregation without waiting for slow devices. However, asynchrony magnifies the
straggler effect because 1) when the server node receives models uploaded by the slow workers,
it probably has already updated the global model for many times, and 2) real-world data are usu-
ally heavy-tailed in distributed heterogeneous devices, where the rich get richer, i.e., the straggler
effect accumulates when no adjustment operations in stale workers, and eventually it affects the
convergence of the global model. Furthermore, dynamics in AFL brings more challenges in param-
eter tuning and speed-accuracy trade-off, and the guidelines for designing efficient and stale-robust
algorithms in this context are still missing.
1
Under review as a conference paper at ICLR 2020
Contributions Our main contributions are summarized as follows. We first establish a new two-
stage analysis on federated learning, namely training error decomposition and convergence analysis.
To the best of our knowledge, it is the first analysis based on the above two stages that address the
optimization roadmap for the general federated learning entirely. Such analysis provides insight into
designing efficient and stale-robust federated learning algorithms.
By following the guidelines of the above two stages, we propose a novel FL algorithm with asyn-
chronous settings and a set of easy-to-implement training strategies. Specifically, the algorithm
controls model training by estimating the model consistency and dynamically adjusting the number
of local epochs on straggle workers to reduce the impact of staleness on the convergence of the
global model.
We conduct experiments to evaluate the efficiency and robustness of our algorithm on imbalanced
and balanced data partitions with different proportions of straggle worker nodes. Results show that
our approach converges fast and robust on the existence of straggle worker nodes compared to the
state-of-the-art solutions.
Related Work Our work is targeting the AFL and staleness resilience approaches in this context.
Straggler effect (also called staleness) is one of the main problems in the similar asynchronous gradi-
ent descent (Async-SGD) approaches, which has been discussed by various studies and its remedies
have been proposed (Hakimi et al., 2019; Mitliagkas et al., 2016; Hadjis et al., 2016; Lian et al.,
2015; Chen et al., 2016; Cui et al., 2016; Chai et al., 2019; Zheng et al., 2017; Dai et al., 2018;
Zhou et al., 2018; Hakimi et al., 2019). However, these works are mainly targeting the distributed
Async-SGD scenarios, which is different from FL as discussed in the previous section. Existing
FL solutions that address the straggler effect are mainly consensus-based. Consensus mechanisms
are introduced where a threshold metric (i.e., control variable) is computed, and only the workers
who satisfy this threshold are permitted to upload their model (Chen et al., 2018; Smith et al., 2017;
Nishio & Yonetani, 2019). Thus it significantly reduces the number of communications and up-
dates model without waiting for straggle workers. However, current approaches are mainly focusing
on synchronized FL. Xie et al. (2019a) propose an AFL algorithm which uses a mixing hyperpa-
rameter to adaptively control the trade-off between the convergence speed and error reduction on
staleness. However, this work and above mentioned FL solutions only consider the staleness caused
by network delay instead of imbalanced data size in each worker and only evaluate on equal size of
local data, which is inconsistent with the real-world cases. Our approach is similar to (Xie et al.,
2019a), but instead we adaptively control the number of local epochs combined with the approxima-
tion of staleness and model discrepancy, and prove the performance guarantee on imbalanced data
partitions. We illustrate our approach in the rest of this paper.
2	Preliminaries and Definitions
We first summarize the general form of FL. Generally, an FL system consists of M distributed
worker nodes (e.g., mobile phones) and a server node. The goal is training a global model across
these worker nodes without uploading local data. Each worker node employs the same machine
learning model, and an optimizer (e.g., stochastic gradient descent) to iteratively optimize the loss
function of the local model. At t-th communication round, the server node uses an aggregation
operator (e.g., averaging) to aggregate the local models uploaded by worker nodes, and broadcasts
the aggregated global model to workers.
We use X(i) = xi1, xi2, ..., xim to present local data points in worker node i, where mi is the
size of data points in this worker. The whole dataset χ = Si X(i), where i ∈ {1, 2, 3, ..., M}. We
assume that X(i) T X(j) = 0 for i = j, and apparently, the total size of data m = PM=I m》We
denote the model in worker node i by ωi ∈ Rd, and the objective function of worker node i by
mi
1 mi
FiM) = m i X f (xj; ωi),
(1)
j=1
2
Under review as a conference paper at ICLR 2020
where f (∙) : X → R is user-defined loss function. Then the objective function of the global model
is
m
F(M = m X f (Xj 3 =
j=1
EMI Pjm=If (xj; ω)
m
M
X ≡Fi(ω),
=1
(2)
where ω is the aggregated global model. The overall goal is to find a model ω* with:
ω* = arg min F(ω).
(3)
Usually, we can use gradient-descent methods to solve equation 3. At the communication round
t + 1, the global model can be formulated as
ωt+1 = ωt - ηt VF (ωt),	(4)
where ηt is the learning rate for model ωt . From equation 2 and equation 4, we can get
MM	M
ωt+1 = j - ηt X ɪmi VFi (ωt) = X ɪmi (ωt- ηt VFi (ωt)) = X ɪmi ωt+1.	⑸
i=1	i=1	i=1
Note that equation 5 is efficient only for reliable environment. To extend equation 5 to unreliable
and heterogeneous FL, we give a more general form as
M
ωt+1 = ωt - ηtg(ωt, ξt) = ωt + Xh(ωit+1,τi)	(6)
where g(∙) is the user-defined aggregation function,
and ξt is a vector which describes the settings of ac-
tivated workers, such as worker ID, number of lo-
cal epochs, and the learning rate. Here and there-
after, we use g(ωt) to represent g(ωt, ξt) for con-
venience. We denote update term as h(∙), a user-
defined function which represents the model param-
eter differences between the collected models from
activated worker nodes and previous global model,
and -ηtg(ωt) = PiM=1 h(ωit+1,τi). Here τi is the
time when worker node i received the global model
ωτi. When h(ωt+1,τ)=鬻(ωt+1 - ωt), we get
FedAvg (McMahan et al., 2017) as a special case of
equation 6.
Figure 1: Illustration of initialization error,
local training error and local-global error.
3 Methodology
In this section, we aim to design an efficient and ro-
bust FL algorithm. To do so, we first establish a two-
stage analysis, and finally, propose our new FL algo-
rithm by combining the insights provided by the two
stages.
Stage 1: Traning Error Decomposition. We first discuss the main errors of the general FL. We
assume that each worker node has a local optimal model ωi* = arg min Fi (ω). Then at the commu-
nication round t, we define the global error as
MM
kωt 一 ω*ll ≤ X ɪmillωt — ω*k + X ɪmikω* — ω*k,	⑺
J 1 ][^w / i=1	i=1
global error J	_丫_	J J	_ ʌz _	J
initialization and local error local-global error
where ∣∣∙∣ is L2 norm. For worker node i, two terms in the right-hand side of inequality 7 respectively
represent 1) initialization and local error: the error between the local model at communication round
3
Under review as a conference paper at ICLR 2020
t and the optimal local model (the well known empirical risk). Here, the initialization error (i.e., the
error between the initial model and local model at communication round t) partially contributes to
the first term. 2) local-global error: the error between optimal local models and optimal global
solution, which is a constant given a specific learning task. Figure 1 illustrates these errors. Usually,
the error between the initial model and the optimal global model is greater than the local-global error,
and thus at the early stage of training, the first term is greater than the second term in the right-hand
side of inequality 7. Therefore, reducing the initialization error and the local error at the beginning
of model training can reduce the global error ∣∣ωt - ω*∣∣. Afterward, when initialization and local
error is minimized, the local-global error dominates the global error. However, as we mentioned
previously, the local-global error is a constant that can not be solved directly. Therefore, we need a
more sophisticated analysis to reach ω* since 7 is no longer appropriate to guide the optimization
other than the early stage of FL training. Following the above analysis, we analyze the convergence
bounds of the general FL (Eq. 6) on the rest of the training stages other than the early stage.
Stage 2: Convergence Analysis. First, we make the following assumptions on the objective func-
tions:
Assumption 1. Smoothness. For all i in {1, 2, 3, ..., M} and given constant β, the objective func-
tion F(ω) and Fi (ω) are β-smooth, i.e.,
∣VF(ω) -VF(ω0)k≤ β∣ω - ω0∣∣,
∣VFi(ω) -VFi(ω0)k ≤ β∣ω - ω0∣∣.
Assumption 2. The first and second moment conditions. The objective function F(ω) and the
aggregation operation g(ωt) satisfy the following:
(a)	The objective function F (ω) is bounded by a scalar Finf = F (ω*).
(b)	There exist scalars δG	≥ δ >	0 such that VF (ωt)>E(g(ωt))	≥
δ∣VF (ωt)∣2 and ∣E(g(ωt))∣ ≤ δG∣VF(ωt)∣.
(c)	There exist scalars L ≥ 0 such that V(g(ωt)) = E(∣g(ωt)∣2) - ∣E(g(ωt)∣2 ≤ L+ ∣VF (ωt)∣2.
E(∙) is abbreviation of Eξt (∙) which denotes the expected value w.r.t. the distribution ofthe random
variable ξt given t.
Assumption 3. Strong convexity. For all i in {1, 2, 3, ..., M} and given constant c, the objective
function F(ω) and Fi(ω) are c-strong convex, i.e.,
F(ω0) ≥ F(ω) + VF(ω)>(ω0 - ω) + 1 c∣ω0 - ω∣2,
Fi(ω0) ≥ Fi(ω) + VFi(ω)>(ω0 — ω) + $c∣ω0 — ω∣2.
Theorem 1.	Convergence for strongly-convex problems. When c and β in assumption 1 and 3
satisfy C ≤ β, we can set the step size ηt = η, where 0 < η ≤ e^, and LG = 1 + δG. With η, the
upper error bound of global model satisfies:
E(F(ωt)- F (ω*)) ≤ ⅞-f+(ι - ηcδ)t-1 (F(ω1)- F (ω*)- η~F).⑻
2cδ	2cδ
The proof of theorem 1 is provided in appendix A.1. Theorem 1 gives an error bound for the general
form of model aggregation without assuming that g(ωt) should come from VF(ωt). Note that
the scalars δ and δG are equal to 1 when g(ωt) is the unbiased estimation of VF(ωt). However,
current convergence bound in theorem 1 is too loose, and it can be further optimized by introducing
controlled local epoch settings.
We assume that VFi(ωi+1 = YVFi(ω" + R with step size η ≤ V. Here Y is the projection
length of VFi(ωit+1) on VFi(ωit), v = 1 - γ, and Rit is the remainder term which is perpendicular
to VFi(ωt). Then given a local epoch E, We have
δ H E — o(νE),L H E2 + o(νE2).	(9)
Then We can extend theorem 1 with the local epoch E.
4
Under review as a conference paper at ICLR 2020
Theorem 2.	Convergence with selected local epoch. We use δ0 and L0 to represent the scalar δ
and L in assumption 2 when E=L and all worker nodes are assumed to participate in model
training, then under 9, 8 can be rewritten as
E(FS- F(ω*)) ≤ neL0E + (1 - ηcδ0E)t-1 (F(ω1) - F(ω*) - '^L0E
2cδ0	2cδ0
(10)
The theorem 2 gives us the error bound of FL with the selected number of local epochs for strongly-
convex problems. The proof of theorem 2 is provided in appendix A.2. The right-hand side of
theorem 2 implies the dynamics of hyper-parameters in local models for efficiency and robustness
trade-off. In a general FL algorithm, e.g., FedAvg, the model settings in worker nodes are always
predefined and kept fixed, which may lead to a higher variance. We now discuss such dynamics and
practical insights on designing efficient and robust FL algorithms.
Selection of local epochs. We discuss how to reduce the global error and communication round
simultaneously for general FL. From the second term of the right-hand side of 10, we can see that
theorem 2 yields to linear convergence when E(F(ωt) - F(ω*))》写L0E. In this condition,
to quickly reduce the global error, we can reduce the second term of the right-hand side of 10 by
increasing the local epoch E while reducing the communication round t. Therefore, We can dynam-
ically assign each worker with a bigger number of local epoch while reducing the communication
round.
Asynchronous acceleration with stragglers. We discuss why asynchronous strategies are needed
in FL. We rearrange 10 as:
E(F(ωt) - F(ω*)) ≤(1-(1- ηcδ0E)t-v) ⅛L0^ + (1 - ηcδ0E)t-1 (F(ω1) - F(ω*)) . (11)
2cδ0
When t increases, and we fix E and η, the global error only depends on L°. L° can be controlled
by sampling more worker nodes within a communication round. Specifically, we compare n-worker
participation with M-workerparticipation for model aggregation at the server node. When we select
n workers outofM, L0 increases according to assumption 2(c) since the variance increases. Thus, to
get the same precision, we decrease η, while it significantly slows the convergence speed. However,
in practice, waiting for all the workers to be ready is time-consuming. Thus, we can introduce
asynchronous mechanisms in model aggregation without waiting for all workers.
Robust training on straggle workers. We discuss how to reduce the global error for FL on the ex-
istence of stragglers. As we mentioned above, asynchronous strategies can accelerate model training
by reducing the waiting time at each communication round. However, the straggler effect is mag-
nified by asynchrony, as discussed in section 1. Stale workers accumulate their staleness, which
increases the variances and affects the convergence of the global model. A practical strategy to tame
such effect is increasing the number of local epoch under the considerations that when the distri-
butions of local data are far away from the global data, we use more epochs to train from the local
data. However, the divergence of these local epoch numbers between stale and non-stale workers
may affect variance adversely, and we can adjust the number of local epoch with the normalized
epochs from all workers to reduce such variance.
Theorem 3.	Convergence for non-convex problems With the Assumption 1 and 2, we can select a
step size ηt = η with 0 < η ≤ ^L^, LG = 1 + δG. The expected error bound satisfies:
E(LXX kVF(ωi)k2) ≤ F+ 2(F(U-Finf)
t W	δo	tηδ0E
(12)
Theorem 3 is similar to theorem 2 that the first term of the right-hand side of (12) does not decrease
by iterative training. Note that the above remarks are also applicable to theorem 3. We provide proof
of theorem 3 in appendix A.3.
Proposed Algorithm. Under the guidance of the above analysis and the practical insights discussed
above, we propose a fast and stale-robust AFL algorithm. Algorithm 1 and 2 illustrate the processes
in worker nodes and the server node, respectively. H(t) is a predefined function at communication
round t which determines how long should the server node waiting for the updated models from
5
Under review as a conference paper at ICLR 2020
workers. H(t) can be used to control the accuracy-speed trade-off. The training processes on the
server node can be divided into two stages, i.e., the initial stage and the converging stage. We switch
the stages by estimating the consistency of model updates.
Algorithm 1 Process at the server node
Input: Function H(t), model F (ω), initial
epoch Einit , initial waiting time ∆t, threshold
of communication rounds T2 .
Output: Updated model F (ω)
1: Initialize ωt as a constant or a random vec-
tor, t J 0.
2: Broadcast ωt and Einit to each worker.
3: repeat // optimizing the initialization and
local error.
4:	Set t J t + 1.
5:	During ∆t time, receive the triplet
(ωi, τi, Ei) from any worker i.
6:	Update ωt with ωi with τi = t - 1 using
5.
7:	Broadcast (ωt , t) to each worker.
8:	Calculate U using 13.
9: until U ≤ 0.1
10: Broadcast start flag to each worker.
11: repeat // the converging stage.
12:	Set t J t + 1, ∆t J H (t).
13:	During ∆t time, receive the triplet
(ωi, τi, Ei) from any worker i.
14:	Update ωt by 6 and 14.
15:	Update mean(E) by 15.
16:	Broadcast the triplet (ωt , t, mean(E)) to
each worker
17: until t > T2
Algorithm 2 Process at the worker node
Input: Number of local epoch Einit, CommU-
nication round t, global model ωt, batch size Bi,
average global epochs mean(E), local optimizer
opt, communication time ∆ti , start flag sent by
the server node
Output: Triplet (ωi, τi, Ei)	
1	Estimate staleness S using S J [hΔ⅛].
2	Set Ei0 J Einit .
3	if start flag is True then
4	Set Ei J mean(E) * s.
5	end if
6	Set τi J t, Ei J 0.
7	for e in 1, 2, 3, ..., Ei0 do
8	Ei J Ei + 1.
9	Randomly divide X* (i) 11 with batch size
	Bi.
10	Update ωi by using opt for each batch.
11	if e = Ei0 then
12	if communication to the server is
	available then
13	Send triplet (ωi, τi, Ei) to the
	server.
14	else
15	eJe- 1.
16	end if
17	end if
18	end for
Definition 1. Update consistency. The model update consistency of n worker nodes is the similari-
ties between worker models at communication round t, i.e.,
U
1
n(n - 1)
nn
X X cos(ωit+1
i=1 j=1,j 6=i
- ωt
ωjt+1 - ωt).
(13)
U is consistent with the global error in inequality 7, and in algorithm 1 we empirically set 0.1 as the
threshold to switch from the initial stage and the converging stage of the global model training. At
the initial stage of global model training, we use a bigger local epoch to accelerate training time as
discussed above, and repeat this process until U ≤ 0.1. After the initial stage, we define the update
term as
(14)
注 is the above mentioned normalized local epoch E with ψi
mean(E 0)mi
Eid(ωt+1)
and
1 nt	E
mean(E)=哈 …
(15)
夕 is the regularization term where 夕=Pn=ι 夕i. Finally, We define a stale-related penalty function
of Wi as:
d ( t+1∖ _ ∕et-τi，cθs(ωt + 1 - ωt, ωf+e1sh - ωt) ≤ -0.1
i	t - τi + 1, others
(16)
6
Under review as a conference paper at ICLR 2020
Here, ωft+re1sh is the average model of worker nodes with τi = t. The key processes of worker nodes
are 1) estimating its staleness level, and 2) assign the number of local epoch using mean(E) in the
received triplet from the server node and the previously estimated staleness level. In the next section,
we evaluate the performance of our algorithms.
4	Experiments
We evaluate the performance of our approach on both imbalanced and balanced data partitions with
the existence of stale worker nodes.
Experiment Settings. We conduct experiments on Fashion-MNIST (Xiao et al., 2017) and CIFAR-
10 (Krizhevsky et al., 2009) to test the accuracy of our approach on 100 simulated workers, where
60 workers are stale. We use 55,000 on Fashion-MNIST and 50,000 on CIFAR-10 for training and
10,000 for testing. [0, 1] normalization is used in the data preprocessing. We conduct all experi-
ments on CPU devices. We use a light-weight convolutional neural network (CNN) model, which is
suitable for mobile edge devices. It has 4 convolutional layers that use 3 × 3 kernels with the size
of 32, 64, 64, 128. Rectified linear unit (ReLU) is used in each convolutional layer, and every two
convolutional layers are followed by a 2 × 2 max-pooling layer and a dropout of 50%. Finally, we
use a 512-unit dense layer with ReLU and a dropout of 50% and an output layer with softmax. We
use an SGD optimizer with a learning rate of 0.01. We set batch size as 50, and the initial number of
local epochs EE as 50. We randomly split the data size in each worker node ranging from 2 to 2122
with a standard deviation of 480 on CIFAR-10, and 9 to 2157 with a standard deviation of 540 on
Fashion-MNIST. For the balanced cases we randomly assign each worker with 500 samples. The
communication speed of nodes is divided into ten levels ranging from 100 milliseconds to 1 second,
and the 60 stale workers are assigned with bigger levels (6-10). Finally, we set H(t) = 0.4s.
Baselines. We compare the performance of our proposed method with four approaches: 1) FedAvg
(McMahan et al., 2017) (synchronized). We set the sampling rate C = 0.1 FedProx (Sahu et al.,
2018) (synchronized). We set C = 0.1, μ = 1 as the best parameters provided in their paper.
FedAsync (Xie et al., 2019a) (asynchronized). We set γ = 0.1, ρ = 0.005, t - τ ≤ 4. FedAsync +
Hinge (Xie et al., 2019a) (asynchronized). We set a = 4, b = 4, γ = 0.1, ρ = 0.005, t - τ ≤ 10 as
shown in their paper. All the baselines besides FedProx use the same strategy of local epoch decay
with local epoch E decreases by 1 per 50 global communication rounds until it decays to 1. All the
four approaches use the same batch size of 50 and initial local epochs of 10.
Results and Analysis. Figure 2 shows the performance of our proposed algorithm and four base-
lines. Our method converge faster compared to all the baselines, and the convergence is promised
with 60% stale workers. Furthermore, the whole upload times of our method do not increase with
the same level of accuracy. From the experiment results on Fashion-MNIST, we can see that our
method has the same accuracy level on test data compared with synchronized approach such as Fe-
dAvg. We can also see that on imbalanced data partitions (i.e., more realistic FL scenarios), our
method is faster and more stable compared to other baselines. Finally, we can clearly see the stage
transition from the initial training stage to the converging stage (e.g., the transitions in imbalanced
cases in figure 2(b) and (d)), which validates the efficiency of our approach. Figure 3 shows the
performance of our method with different proportion of stale nodes in 1,000 global communication
rounds. Our method outperforms the AFL baseline (i.e., FedAsync) in both accuracy and loss, and
when the proportion of stale workers is less than 80%, our method outperforms the synchronized FL
baseline (i.e., FedAvg).
5	Conclusions and Future Work
In this paper, we propose a new two-stage analysis on federated learning, and inspired by such
analysis, we propose a novel AFL algorithm that accelerates convergence and resists performance
deterioration caused by stragglers simultaneously. Experimental results show that our approach
converges two times faster than baselines, and it can resist the straggler effect without sacrificing
accuracy and communication. As a byproduct, our approach improves the generalization ability
of neural network models. We will theoretically analyze it in future work. Besides, while not the
focus of our work, security and privacy are essential concerns in federated learning, and as the
future work, we can apply various security methods to our approach. Furthermore, besides the stale-
7
Under review as a conference paper at ICLR 2020
*
≡0.6
u
U
(O 0.4
o.o-
6	500 1000 1500 2000 2500 3000
Global wall time
∙8∙64 2
Oooo
ABn Ue3
o.o-
6 500 1000 1500 2000 2500 3000 3500
Upload times
----FedProx
FedAvg
....FedAsync
....FedAsync+Hinge
Proposed method
0。	-	I
0	200	400	600	800	1000
Communication rounds
(a)	Accuracy on Fashion-MNIST (balanced case)
∙8∙64∙2∙0
Ooooo
>u2⊃uura ttQH
0	500 1000 1500 2000 2500 3000
Global wall time
∙8∙64 2 Q
Ooooo
ABnXe3
∙8∙64∙2e
Ooooo
ABn Xe3
0	200	400	600	800	1000
Communication rounds
0 500 1000 1500 2000 2500 3000 3500
Upload times
(b)	Accuracy on Fashion-MNIST (imbalanced case)
7∙65432∙1g
Oooooooo
>ussuuπB
7∙65432∙1Q
Oooooooo
0 500 1000 1500 2000 2500 3000 3500
Upload times
∙87∙6∙543∙2∙1e
Ooooooooo
O
200	400	600	800 IOOO
Communication rounds
0	500 1000 1500 2000 2500 3000
Global wall time
(c)	Accuracy on CIFAR-10 (balanced case)
B765432L
Oooooooo
0.0
0.0
∙87∙65432∙1
Oooooooo
0	500 1000 1500 2000 2500 3000	0	500 1000 1500 2000 2500 3000 3500	6	200	400	600	800	1000
Global wall time	Upload times	Communication rounds
(d) Accuracy on CIFAR-10 (imbalanced case)
Figure 2: Accuracy on test data of CIFAR-10 and Fashion-MNIST with imbalanced and balanced
data partitions.
1.1-
1.0-
0.9-
0.8-
90%
Percentage of stale workers
80%	60%	20%
Percentage of stale workers

Figure 3: Test accuracy and loss with different proportion of stale workers on CIFAR-10 dataset in
1,000 communication rounds. We respectively test the performance with 20%, 60%, 80%, and 90%
of stale workers. The green dotted line is FedAvg which waits all selected workers.
8
Under review as a conference paper at ICLR 2020
resistance ability, the discrepancy estimation in our method also has the potential ability to resist
malicious attacks to the worker nodes such as massive Byzantine attacks, which has been addressed
in (Bagdasaryan et al., 2018; Li et al., 2019; Munoz-Gonzalez et al., 2019). We will analyze and
evaluate such ability in future work.
References
Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in
Neural Information Processing Systems,pp. 873-881, 2011.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. arXiv preprint arXiv:1807.00459, 2018.
Zheng Chai, Hannan Fayyaz, Zeshan Fayyaz, Ali Anwar, Yi Zhou, Nathalie Baracaldo, Heiko Lud-
wig, and Yue Cheng. Towards taming the resource and data heterogeneity in federated learning.
In 2019 {USENIX} Conference on Operational Machine Learning (OpML 19), pp. 19-21, 2019.
Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting dis-
tributed synchronous sgd. arXiv preprint arXiv:1604.00981, 2016.
Tianyi Chen, Georgios Giannakis, Tao Sun, and Wotao Yin. Lag: Lazily aggregated gradient for
communication-efficient distributed learning. In Advances in Neural Information Processing Sys-
tems, pp. 5050-5060, 2018.
Henggang Cui, Hao Zhang, Gregory R Ganger, Phillip B Gibbons, and Eric P Xing. Geeps: Scalable
deep learning on distributed gpus with a gpu-specialized parameter server. In Proceedings of the
Eleventh European Conference on Computer Systems, pp. 4. ACM, 2016.
Wei Dai, Yi Zhou, Nanqing Dong, Hao Zhang, and Eric P Xing. Toward understanding the impact
of staleness in distributed machine learning. arXiv preprint arXiv:1810.03264, 2018.
Rainer Gemulla, Erik Nijkamp, Peter J Haas, and Yannis Sismanis. Large-scale matrix factorization
with distributed stochastic gradient descent. In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining, pp. 69-77. ACM, 2011.
Stefan Hadjis, Ce Zhang, Ioannis Mitliagkas, Dan Iter, and Christopher Re. Omnivore: An optimizer
for multi-device deep learning on cpus and gpus. arXiv preprint arXiv:1606.04487, 2016.
Ido Hakimi, Saar Barkai, Moshe Gabel, and Assaf Schuster. Taming momentum in a distributed
asynchronous environment. arXiv preprint arXiv:1907.11612, 2019.
Michael Kamp, Linara Adilova, Joachim Sicking, Fabian Huger, Peter Schlicht, Tim Wirtz, and
Stefan Wrobel. Efficient decentralized deep learning by dynamic model averaging. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 393-
409. Springer, 2018.
Jakub Konecny, H Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Guanghui Lan, Soomin Lee, and Yi Zhou. Communication-efficient algorithms for decentralized
and stochastic optimization. Mathematical Programming, pp. 1-48, 2017.
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph Dureau. Federated
learning for keyword spotting. In ICASSP 2019-2019 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pp. 6341-6345. IEEE, 2019.
Liping Li, Wei Xu, Tianyi Chen, Georgios B Giannakis, and Qing Ling. Rsa: Byzantine-robust
stochastic aggregation methods for distributed learning from heterogeneous datasets. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 1544-1551, 2019.
9
Under review as a conference paper at ICLR 2020
Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen, and Alexander Smola. Param-
eter server for distributed machine learning. In Big Learning NIPS Workshop, volume 6, pp. 2,
2013.
Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed ma-
chine learning with the parameter server. In Advances in Neural Information Processing Systems,
pp.19-27, 2014.
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for
nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 2737-2745,
2015.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282, 2017.
Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, and Christopher Re. Asynchrony begets momentum,
with an application to deep learning. In 2016 54th Annual Allerton Conference on Communica-
tion, Control, and Computing (Allerton), pp. 997-1004. IEEE, 2016.
Umair Mohammad and Sameh Sorour. Adaptive task allocation for asynchronous federated mobile
edge learning. arXiv preprint arXiv:1905.01656, 2019.
LUiS Mufioz-Gonzaiez, Kenneth T Co, and Emil C Lupu. Byzantine-robust federated machine learn-
ing through adaptive model averaging. arXiv preprint arXiv:1909.05125, 2019.
Takayuki Nishio and Ryo Yonetani. Client selection for federated learning with heterogeneous
resources in mobile edge. In ICC 2019-2019 IEEE International Conference on Communications
(ICC), pp. 1-7. IEEE, 2019.
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith.
On the convergence of federated optimization in heterogeneous networks. arXiv preprint
arXiv:1812.06127, 2018.
Sumudu Samarakoon, Mehdi Bennis, Walid Saady, and Merouane Debbah. Distributed fed-
erated learning for ultra-reliable low-latency vehicular communications. arXiv preprint
arXiv:1807.08127, 2018.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. In Advances in Neural Information Processing Systems, pp. 4424-4434, 2017.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and
Kevin Chan. When edge meets learning: Adaptive control for resource-constrained distributed
machine learning. In IEEE INFOCOM 2018-IEEE Conference on Computer Communications,
pp. 63-71. IEEE, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asynchronous federated optimization. arXiv preprint
arXiv:1903.03934, 2019a.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In International Conference on Machine Learning, pp. 6893-
6901, 2019b.
Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu.
Asynchronous stochastic gradient descent with delay compensation. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 4120-4129. JMLR. org, 2017.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter Glynn, Yinyu Ye, Li-Jia Li,
and Li Fei-Fei. Distributed asynchronous optimization with unbounded delays: How slow can
you go? In International Conference on Machine Learning, pp. 5965-5974, 2018.
10
Under review as a conference paper at ICLR 2020
A Proofs
A.1 Proof of Theorem 1
Lemma 1. Under the assumption 1, we can get:
E(F(ωt+1)) - F(ωt) ≤ -%VF(ωt)>E(g(ωt)) + 2唯βE(kg(ωt)∣∣2).	(17)
Proof: Under the assumption 1, for any ω and ω0, we have:
F(ω) = F(ω0) + 广 dF(ω0 + ：(“ - ωO)) 4t
0	dt
F (ω0) + Z1VF(ω0+t(ω - ω0))>(ω - ω0)dt
0
F(ω0)+VF(ω0)>(ω-ω0)+
≤ F(ω0)+VF(ω0)>(ω-ω0)+
Z1[VF(ω0+t(ω - ω0)) -VF(ω0)]>(ω - ω0)dt
0
Z1βkt(ω-ω0)kkω-ω0kdt
0
F(ω0) + VF(ω0)>(ω - ω0) + 1 β∣∣ω - ω0k2.
(18)
Then using 18, we have:
F(ωt+1) - F(ωt) ≤ VF(ωt)> (ωt+1 - ωt)
+ 2 βkωt+1-ωtk2
=-VF (ωt)>ηtg(ωt) + 2 kηtg(ωt)k2.	(19)
Taking expectations in 19 w.r.t the distribution of ξt, We complete the proof.	□
Lemma 2. Under the assumption 1 and 2, we can get:
E(F(ωt+1)) - F(ωt) ≤ -加(δ - ηtβLG)∣∣VF(ωt)∣∣2 + 2η2βL.	(20)
Proof: Using assumption 2(b) and 2(c), we have:
E(kg(ωt)k2) ≤ kE(g(ωt))k2 + L + kVF (ωt)k2
≤L+ (1+δG2 )kVF(ωt)k2
=L+LGkVF(ωt)k2.	(21)
Then using 21, assumption 2(b) and lemma 1, we have:
E(F(ωt+1)) - F(ωt) ≤ -ηtVF (ωt)>E(g(ωt))
+ 2 η2βE(kg(ωt)k2)
2
≤ -ηtδkVF(ωt)k2 + ηβ(L + LGkVF(ωt)∣∣2).	(22)
We can easily get Lemma 2 by rearranging 22.	□
Then we prove Theorem 1 under the assumption 1, 2, 3. First, we define
F(ω) = F(ωt) + VF(ωt)>(ω - ωt) + C∣∣ω - ωt∣∣2.	(23)
Function F is a quadratic model relevant to ω. Then it has the minimal value when all the partial
derivatives are 0. That is
dF(ω) = VF(ωt) + c(ω - ωt) = ~.
∂ω
11
Under review as a conference paper at ICLR 2020
Then, when We select ω = ω* = ωt 一 *F( ) for 23, We get the minimal of 23, which is
Fmin= F(ωt)-R.
2c
From assumption 3, we have
F(ω*) ≥ F(ω*) ≥ Fmin = FS- k^Fωtk2,
2c
which is equivalent to
2c(F(ωt) - F(ω*)) ≤ ∣∣VFωtk2.
Then from Lemma 2, when a fixed η ≤e is selected, we have:
E(F(ωt+1)) - F(ωt) ≤ 一%∣IIVF(ωt)∣2 + |桔βL.
And using 26, we have
E(F(ωt+1)) - F(ωt) ≤ -%δc(F(ωt) - F(ω*)) + 2律户工
Subtracting F(ω*) from both sides and moving F (ωt) from left to right, we get
E(F(ωt+1)) - F(ω*) ≤ -%δc(F(ωt) 一 F(ω*)) + (F(ωt) - F(ω*)) + 1 律户工
Taking the whole expectations and rearranging 29, we obtain
E(F(ωt+1) - F(ω*)) ≤ (1 一 力δc)E(F(ωt) - F(ω*)) + 2宿户工
Substracting the constant η2CL from both sides of 30, we have
E(F(ωt+1) - F(J))- η-y ≤ (I- ηtδC)(E (F(J)- F(J))- n-v).
2cδ	2cδ
(24)
(25)
(26)
(27)
(28)
(29)
(30)
(31)
The left hand side of 31 is a geometric series with common ratio 1 - ηtδc, then we complete the
proof.
A.2 Proof of Theorem 2
We first prove 9. Assume VFi(ωit+1) = (1 - ν)VFi(ωit) + Resit, we have
∣Resit∣ = ∣VFi(ωit+1)-(1-ν)VFi(ωit)∣
= ∣(1-ν+ν)VFi(ωit+1)-(1-ν)VFi(ωit)∣
≤ ∣(1 - ν) (VFi(ωt+1) - VFi(ωt)) ∣ + IlVVFi(ωt+1)∣
≤(1-ν)β∣ωit+1-ωit∣ +ν∣VFi(ωit+1)∣
=(1-ν)βηtIVFi(ωit)I +νIVFi(ωit+1)I
≤ (1-ν)νIVFi(ωit)I +νIVFi(ωit+1)I.	(32)
LetIVFi(ωit)I =aandIνVFi(ωit+1)I =b,we define
h(ν) = a(1 — v)ν + bν = V(a - — V).	(33)
aa
Since a+b > 1 and V ≤ 1, we have h(ν')min = h(0) = 0, and a ≈ b when n is small. We know
that the smaller V is, the smaller IResit I is.
Then we consider the situation that Ei = E = 1, and define
VF (ωt)>E(gE=1(ωt)) ≥ δ0IVF (ωt)I2 and
V(gE=1(ωt)) = E(IgE=1(ωt)I2) - IE(gE=1(ωt))I2
≤ L0 + IVF(ωt)I2.
12
Under review as a conference paper at ICLR 2020
When VFi(ωt+1) = (1 - V)VFi(ωt) + ReSt is satisfied, we have
E(gE=E (J))
= E(gE=1 (ωt)(1 + (1 - ν)+, ..., +(1 - ν)E-1 + ReS0t))
1 - (1 - V)E	， 一
=—(——二 E(gE=ι(ωt) + Res0t)).	(34)
V
Based on Taylor expansion, we can get
(1 — V )E = 1 — EV + o(Eν)	(35)
Then equation 34 can be written as
E(gE=E (J))= EE(gE=1(J) + ReSOt),	(36)
ReS0t is the sum of all ReSit, and it is perpendicular gE=1(ωt). Then we have
VF (ωt)>E(gE=E (ωt))
≥ EVF(ωt)>E(gE=1(ωt) + ReS0)
≥ Eδo∣∣VF(ωt)k2 = δkVF(ωt)k2.	(37)
Variance is the second moment of expectation. Based on 37, we have 9. Then using 9 and 8, we
complete the proof.	□
A.3 Proof of Theorem 3
When assumption 1, 2 are satisfied, and lemma 2 is hold. If η = η ≤ ^1^ holds and We take
expectation at both sides of 20, we have
E(F(ωt+1)) — E(F(ωt)) ≤ -m2E(∣∣VF(ωt)∣∣2) + ∣*βL.	(38)
Then sum all the form of 38 from 1 to t. We have
E(F(ωt+1)) - E(F (ω1)) =
E(F(ωt+1)) — F(ω1) ≤-n2
t
XE(kVF(ωi)k2)
i=1
1
+ 2 tn2βL.	(39)
Besides, We can easily understand that Finf ≤ E(F(ωt+1)), because Finf is the minimal value of
F. Then We have
Finf - F(ω1) ≤ E(F(ωt+1)) — F(ω1) ≤ -ηt∣ XXE(∣∣VF(ωi)∣∣2) + 2tη2βL.
i=1
By rearrange 40, We have
E(XX kVF(ωi)k2) ≤ tηtβL + 2(FO - Finf).
δ	ηt δ
i=1	t
Dividing t from both sides of 41, We get
E(| XX kVF(ωi)k2) ≤ 吟 + 2(FI；) -Finf).
i=1	ηt
Then using equation 9, We complete the proof.
(40)
(41)
(42)
□
B Additional Experiments on Stale-Robustness
We conduct additional experiments to evaluate stale-robustness of our algorithm on CIFAR-10 based
on the settings in section 4. We visualize the impact of different staleness levels at different com-
munication rounds With cosine angles (i.e., discrepancies) betWeen the update terms (i.e., update
directions of local models) of stale Workers and fresh Workers in figure 4. The results shoW that our
method (in the first roW) effectively adjusts the update direction of the reversed stale nodes While
angles of stale nodes reverse With FedAvg compared to our algorithm, Which shoWs the robustness
of our method.
13
Under review as a conference paper at ICLR 2020
(a) Proposed method
Figure 4: Impact visualization of different levels of staleness using cosine angles between the update
terms defined in section 2 of fresh nodes (40 out of 100) and stale nodes (60 out of 100 worker nodes)
on CIFAR-10 at different communication round. The blue numbers represent the staleness levels by
using the differences of version numbers of models between the stale nodes and the fresh nodes.
E.g., the staleness level is 10 at this communication round means that the fresh nodes has updated
10 more versions compared to the stale nodes.
14