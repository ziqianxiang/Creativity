Under review as a conference paper at ICLR 2020
Measuring causal influence with
back-to-back regression: the linear case
Anonymous authors
Paper under double-blind review
Ab stract
Identifying causes from observations can be particularly challenging when i) poten-
tial factors are difficult to manipulate individually and ii) observations are complex
and multi-dimensional. To address this issue, we introduce “Back-to-Back” regres-
sion (B2B), a method designed to efficiently measure, from a set of co-varying
factors, the causal influences that most plausibly account for multidimensional
observations. After proving the consistency of B2B and its links to other linear
approaches, we show that our method outperforms least-squares regression and
cross-decomposition techniques (e.g. canonical correlation analysis and partial
least squares) on causal identification. Finally, we apply B2B to neuroimaging
recordings of 102 subjects reading word sequences. The results show that the early
and late brain representations, caused by low- and high-level word features respec-
tively, are more reliably detected with B2B than with other standard techniques.
1 Introduction
Natural sciences are tasked to find, from a set of hypothetical factors, the minimal subset that suffices
to reliably predict novel observations. This endeavor is impeded by two major challenges.
First, causal and non-causal factors may be numerous and collinear. This issue becomes increasingly
pronounced as the number of potential factors increases. In neuroscience, for example, identifying
whether the frequency of a word presented on a subject’s retina modulates brain activity can be
surprisingly difficult. Indeed, the frequency of words in natural language covaries with other factors
such as their length (short words are more frequent than long words), their categories (determinants
are more frequent than adverbs) and so forth (Kutas & Federmeier, 2011; Pegado et al., 2014). Instead
of selecting a set of words that control for all of these factors simultaneously, it is thus common to use
forward modeling, i.e. to train a model to predict observations (e.g. brain activity) from a minimal
combination of competing factors (e.g. word length, word frequency, e.g. (Huth et al., 2016)), and
investigate, in the model, the estimated contribution of each factor (Friston et al., 1994).
The second challenge to measuring causal influence is that observations can be large and complex.
The relationship between causes and effects is thus often considered in a backward manner, by
training models to maximally predict causes from multidimensional observations. For example, brain
activity is often recorded with hundreds or thousands of sensors simultaneously. As multiple sensors
may be affected by common noise sources, it is common to use backward regression, by, for example,
fitting a support vector machine across multiple sensors to decode the category of a stimulus (Cichy
et al., 2014; Kriegeskorte et al., 2008; Norman et al., 2006).
Both forward and backward modeling have competing benefits and drawbacks. Specifically, forward
modeling disentangles the independent contribution of collinear factors, but does not combine
multidimensional observations. By contrast, backward modeling combines multiple observations, but
does not disentangle collinear factors (Weichwald et al., 2015; Hebart & Baker, 2018; King et al.,
2018). To combine the benefits of forward and backward modeling, several authors have proposed to
use cross-decomposition techniques such as Partial Least Squares (PLS) and Canonical Correlation
Analysis (CCA) (de Cheveigne et al., 2019). CCA and PLS aim to find, from two sets of data X and
Y , the components H and G were XH and Y G are maximially correlated or maximally covarying
respectively. Because CCA and PLS are based on a generalized eigen decomposition, their resulting
coefficients are mixing the features of X and Y in a way that makes them notoriously difficult to
interpret (Lebart et al., 1995).
1
Under review as a conference paper at ICLR 2020
Observations
Y ∈ Rn×dy
Factors
Cause
selection
×
+
Noise
N ∈ Rn×dx
Cause-effect
mapping
×
X ∈ Rn×dx	E ∈ Ddx×dx
F ∈ Rdx×dy
1) X: regression from Y to X
z----------------A----------------{
， 、
X 〜P(X)	E = diag((X>X2 + Λx)-1X>匕(%>匕 + AY)-1 Y>Xι)
I
N 〜P (N)
{^^^^^^^^^^
2) regression from X to X
}
Figure 1: Back-to-back regression identifies the subset of factors Eii = 1 in X that influence some
observations Y by 1) regressing from Y to X to obtain X, and 2) returning the diagonal of the
regression coefficients from X to X.
Here, we introduce the ‘back-to-back regression’ (B2B), which not only combines the benefits of
forward and backward modeling (Section 2), but also provide robust, interpretable and unidimensional
causal coefficients for each tested factor.
The present paper focuses on the restricted issue of disentangling the causal influence of linearly
correlated predictors (X) onto noisy multivariate observations (Y). The present approach thus differs
from other causal discovery algorithms based on temporal-delays and/or nonlinear interactions in
systems where the directionality of causation (from X to Y or vice versa) is unknown (e.g. (Peters
et al., 2017; Granger, 1969; Janzing et al., 2013; Scholkopf et al., 2016).
After detailing B2B method and proving its convergence (Section 2.2), we show with synthetic
data that it outperforms state-of-the-art forward, backward and cross-decomposition techniques in
identifying causal influence (Section 3.1). Finally, we apply B2B to a large neuroimaging dataset
and reveal that distinct but collinear word features lead to distinguishable brain representations
(Section 3.5).
2	Back-to-Back regression
We consider the measurement of multivariate signal Y ∈ Rn×dy , generated from a set of putative
causes X ∈ Rn×dx, via some unknown linear apparatus F ∈ Rdx×dy. Not all the variables in X
exert a causal influence on Y. By considering a square binary diagonal matrix of causal influences
E ∈ Ddx ×dx, we denote by XE the causal factors of Y. In summary, the problem can be formalized
as:
yi = (xiE + ni)F	(1)
where i is a given sample, and ni is a sample-specific noise drawen from a centered distribution.
While the triplet of variables X and N are independent, we allow each of them to have any form of
covariance. In practice, we observe n samples (X, Y) from the model. This problem space, along
with the sizes of all variables involved, is illustrated in Figure 1. Given the model in Equation (1),
the goal of Back-to-Back Regression (B2B) is to estimate the matrix of causal influences E.
2.1	Algorithm
Back-to-Back Regression (B2B) consists of two steps. First, we estimate the linear regression
coefficients G from Y to X, and construct the predictions X = YG. This backward regression
recovers the correlations between Y and each factor of X . Second, we estimate the linear regression
2
Under review as a conference paper at ICLR 2020
t`r` ♦	. ττ i`	PT- . G mi 1 ∙	IC ,1	♦	t`r` ♦	. ττ ι . ι ι	ι ∙	/ τ'τ∖
coefficients H from X to X. The diagonal of the regression coefficients H, denoted by E = diag(H),
is the desired estimate of the causal influence matrix E, as detailed in the A.1.
If using l2-regularized least-squares (Hoerl, 1959; Rifkin & Lippert, 2007), B2B has a closed form
solution:
G = (Y >Y + ΛY)-1Y > X,
H = (x >x + λx )-1x >YG,
(2)
(3)
where ΛX and ΛY are two diagonal matrices of regularization parameters, useful to invert the
covariance matrices of x and Y if these are ill-conditioned.
Performing two regressions over the same data sample can result in overfitting, as spurious correlations
in the data absorbed by the first regression will be leveraged by the second one. To avoid this issue,
we split our sample (x, Y) into two splits (x1, Y1) and (x2, Y2). Then, the first regression is
performed using (x1, Y1), and the second regression is performed using (x2, Y2). To compensate
for the reduction in sample size caused by the split, B2B is repeated over many random splits, and
the final estimate E of the causal influence matrix is the average over the estimates associated to
each split (Breiman, 1996). To accelerate this ensembling procedure, we implemented an efficient
leave-one-out cross-validation scheme as detailed in (Rifkin & Lippert, 2007) , as follows:
Y = (ΣχGY - diag(ΣχG)Y) / diag(I - ΣχG)	(element-wise division) (4)
where ΣX is the x kernel matrix and where G is computed with an eigen decomposition of x :
ΣX = QVQT	(5)
G=Q(V + λI)-1QT
where Q, V and λ are the eigen vectors, eigen values and regularization, respectively.
We summarize the B2B procedure in Algorithm 1. The rest of this section provides a theoretical
guarantee on the correctness of B2B.
Algorithm 1: Back-to-back regression.
Input: input data x ∈ Rn×dx, output data Y ∈ Rn×dy, number of repetitions m ∈ N.
Output: estimate of causal influences E ∈ Ddχ×dx.
ι E J 0;
2	for i = 1, . . . , m do
3	(x, Y) J ShuffleRows((x, Y));
4	(x1,Y1),(x2,Y2) J SplitRowsInHalf((x, Y));
5	G = LinearRegreSSion(Y1,Xι) ;	. G=(Yι>Y1 +Λγ )-1γ>X1
6	H = LinearRegreSSion(X2,Y2G) ;	. H=(X>X2 +Λχ)-1X>Y2G
7	E J E + diag(H);
8	end
9	E J E/m;
TTT-	T ∙	T-I	♦/ PT- R ，广、
10	W J LinearRegression(X E, Y);
，	含 TTT-
11	return E, W
2.2 Theoretical guarantees
Theorem 1 (B2B consistency - general case). Consider the B2B model from Equation Y = (XE +
N)F, N centred and full rank noise. Let Img(M) refers to the image of the matrix M. If F and
X arefull-rank on the Img(E), then, the solution ofB2B, H, will minimize minH ∣∣X — XH∣∣2 +
∣∣NH∣∣2and satisfy EH = H
Proof. See Appendix A.1.
□
3
Under review as a conference paper at ICLR 2020
Since EH = H , We have
HH = argmin ∣∣X - XEHk2 + ∣∣NEHk2 = (EX>XE + ENINE『EXX>.	(6)
H
Assuming, without loss of generality, that the active features in E are the k ∈ Z : k ∈ [0, dx] first
features, and rewriting X = (X1, X2) and N = (N1, N2) (X1 and N1 containing the k first features),
we have:
X>X =	ΣX1X1	ΣX1X2
ΣX1X2	ΣX2X2
N> N =	ΣN1 N1	ΣN1 N2
ΣN1N2	ΣN2N2
(7)
Where ΣAB is the covariance ofA and B, and:
HH =( 3xιXι +ςNINI)TςXiXi
(ΣX1X1 + ΣN1N1)-1ΣX1X2
0
(8)
diagk (Hr) = diag((∑χχι + ∑NinJ-1∑XiXi ) = diag((I + ∑χ%1 ∑N1N1 )-1)	(9)
In the absence of noise, We have ΣN1N1 = 0, and so diagk(H ) = I, and
diag(HH) = diag(E)
Therefore, We recover E from HH.
In the presence of noise, the causal factors of E correspond to the positive elements of diag(HH). The
methods to recover them are presented in the Appendix A.4.
3	Experiments
We perform tWo sets of experiments to evaluate B2B: one on controlled synthetic data, and a second
one on a real, large-scale magneto-encephalography (MEG) dataset. We use scikit-learn’s PLS and
RidgeCV (Pedregosa et al., 2011) as Well as pyrcca’s regularized canonical component analysis
(RegCCA, (Bilenko & Gallant, 2016)) objects to compare B2B against the standard baselines.
3.1	Synthetic data
We evaluate the performance of B2B throughout a series of experiments on controlled synthetic
data. The purpose of these experiments is to evaluate the ability of B2B in terms of prediction of
independent and identically distributed data, as Well as a method to recover causal factors.
The data generating process for each experiment constructs n = 1000 training examples according
to the model Y = (hXE + N)F, Where h is a scalar that modulates the signal-to-noise ratio.
Here, F ∈ Rdx ×dy contains entries draWn from N(0, σ2) Where σ2 is inversely proportional to
dx, X ∈ Rn×dx contains roWs draWn from N(0, ΣX), N ∈ Rn×dx contains roWs draWn from
N(0, ΣN), E ∈ Rdx×dx is a binary diagonal matrix containing nc ones, ΣX = AA> Where
A ∈ Rdx×dx contains entries draWn from N(0, σ2), ΣN = BB> Where B ∈ Rdx ×dx contains
entries draWn from N(0, σ2), and the factor h ∈ R+.
To simulate a Wide range of experimental conditions, We sample 10 values in log-space for dx , dy ∈
[10, 100], nc ∈ [3, 63], h ∈ [0.001, 10]. We discard the cases Where nc > dx, limit dx, dy to 100 to
keep the running time under 2 hours for each condition, and average over 5 random seeds.
We compare the performance of B2B against four competing methods, all implemented in scikit-learn
(Pedregosa et al., 2011) and pyrcca (Bilenko & Gallant, 2016):
3.2	Baseline models
ForWard regression consists of an l2-regularized ”ridge” regression from the putative causes X to the
observations Y :
Hfwd = (XTX + λI)-1XTY	(10)
4
Under review as a conference paper at ICLR 2020
Figure 2: Synthetic experiments. Average AUC (top) and Feature Importance ∆R (bottom) when
varying experimental conditions individually. Higher is better. B2B compares favorably in all cases.
Backward regression consists of an l2-regularized ”ridge” regression from Y to X :
Gbwd = (YTY +λI)-1YTX	(11)
CCA finds Gcca ∈ Rdz ,dy and Hcca ∈ Rdz,dx s.t. X and Y are maximally correlated in a latent Z
space:
Gcca,Hcca = argmaxcorr(XHT,YGT)	(12)
G,H
PLS finds Gpls ∈ Rdz ,dy and Hpls ∈ Rdz ,dx s.t. X and Y are maximally covarying in a latent Z
space:
Gpls, Hpls = argmax cov(XHT, Y GT)	(13)
G,H
We employ 5-fold cross-validation to select the optimal number of components for CCA and PLS.
Regressions were '2-regularized with a λ regularization parameters fitted with the efficient leave-one-
out procedure implemented in scikit-learn RidgeCV (Pedregosa et al., 2011).
3.3	Evaluating Causal Discovery from models’ coefficients
B2B leads to unbiased (i.e. zeros-centered) scalar coefficients for non-causal features. In contrast, the
Forward, Backward, CCA and PLS models lead to a loading vector Hi per feature i (or one vector
Gi for the backward model). To transform such vector into an estimated causal contribution 反 we
take the sum of square coefficients: Ei = Pj Hi
To estimate whether models accurately identify causal factors, we compute the area-under-the-curve
(AUC) across factors AUC(E, E). The AUC allows evaluating the capacity of models at detecting
the causal importance of factors when ground truth labels are available, as is the case in this setup.
We report AUC results in Figures 2 (top) and 5 (left, in Appendix), and compare favorably to all
baselines.
3.4	Evaluating Causal Discovery with held-out prediction reliability
In most cases, E is not known and AUC can thus not be estimated. To address this issue, we assess
the ability of each model to reliably predict independent and identically distributed data from Y ,
given all of the X features versus all-but-ones feature X-i (i.e. ’knock-out X’). This procedure
5
Under review as a conference paper at ICLR 2020
Figure 3: Ninety subjects each read
approximately 2,700 words while their
brain activity was recorded with MEG.
Top. Average brain response to words
(word onset at t=0 ms), as viewed from
above the head (red= higher gradient of
magnetic flux). Bottom. Each line rep-
resents a magnetometer, color-coded by
its spatial position. Posterior responses,
typical of primary visual cortex activity,
peak around 100 ms after word onset and
© ------------------1----I-----------1----1 are followed by an anterior propagation
-100	0	100	200	300	400	500	600	∙+ ,八二ι “______J，二一〜
Time from word OnSet (ms)	Of activity typical Of SemantiC PrOCeSSing
in the associative cortices.
results in two correlation metrics Rfull and Rknockout , whose difference ∆Ri = Rf ull - Rknockout
indicates how much each Xi improves the prediction of Y . In our figures, ∆R is the average of
∆Ri . A higher score means that for prediction, the model relies on individual features rather than
combinations of features.
We show in Appendix A.3 pseudo-code to assess feature importance for our algorithm as well as
baselines. For the Backward Model, feature importance cannot be assessed as the X colinearity is
never taken into account.
We show in Figures 2 (bottom) and 5 (right, in Appendix) that our method outperforms baselines.
3.5 Magnetoencephalography data
Next, we apply our method to brain imaging data from the anonymized multimodal neuroimaging
“Mother Of all Unification Studies” (MOUS) dataset (Schoffelen et al., 2019). The dataset contains
magneto-encephalography (MEG) recordings of 102 healthy native-Dutch adults who participated in
a reading task. Twelve subjects were excluded from the analysis because of corrupted file headers.
Subjects were exposed to a rapid serial visual presentation of Dutch words. The word lists consisted
of 120 sentences, and scrambled lists of the same words. Each word was presented on the computer
screen for 351ms on average (min: 300ms, max: 1400ms). Successive words were separated by a
blank screen for 300ms, and successive sentences were separated by an empty screen for a few (3-4)
seconds.
3.5.1	MEG preprocessing
The raw MEG data was bandpass-filtered between 0.1 and 40Hz using MNE-Python default parame-
ters (Gramfort et al., 2013; 2014). Specifically, we used a zero-phase finite impulse response filter
(FIR) with a Hamming window and with transition bands of 0.1Hz and 10Hz for the low and high
cut-off frequencies. The raw data was then segmented 100ms before word onset and 1s after word
onset (t = 0ms corresponds to word onset). Finally, each resulting segment was baseline-corrected
between -100ms and 0ms, and decimated by 5 and thus led a sampling frequency of 240Hz. The
average responses across words is displayed in Figure 3. For each subject and each time sample
relative to word onset, we build an observation matrix Y ∈ Rn×dy of n ≈ 2700 words by dy = 301
MEG channels (273 magnetometers and 28 compensation channels). Each of the columns of Y is
normalized to have zero mean and unit variance.
3.5.2	Feature definition
We aim to identify the word features that cause a variation in brain responses. We consider four
distinct but colinear features. First, ’Word Length’ refers to the total number of letters. Word Length
is expected to specifically cause a variation in the early evoked MEG responses (i.e. from 100
ms after stimulus onset) elicited by the retinotopically-tuned visual cortices (e.g. (Pegado et al.,
2014).). Second, ’Word Frequency’ indexes how frequently each word appears in Dutch and was
derived with the the Zipf logarithmic scale of (Van Heuven et al., 2014) provided by the WordFreq
6
Under review as a conference paper at ICLR 2020
package (Speer et al., 2018). Word Frequency is expected to specifically cause a variation in the
late evoked MEG responses (i.e. from 400 ms), because it variably engages semantic processes in
the temporal cortices (Kutas & Federmeier, 2011). Third, ’Word Function’ indicates whether each
word is a content word (i.e. a noun, a verb, an adjective or an adverb) or a function word (i.e. a
preposition, a conjunction, a determinant, a pronoun or a numeral), and was derived from Spacy’s
part of speech tagger (Honnibal & Montani, 2017). To our knowledge, this feature has not been
thouroughly investigated with MEG. Its causal contribution to reading processes in the brain thus
remains unclear. Finally, to verify that B2B and other methods would not inadequately identify
non-causal features, we added a dummy feature, constructed from a noisy combination of Word
Length and Word Frequency: dummy = z (length) + z(f requency) + N , where z normalizes
features and N is a random vector sampling Gaussian distribution (all terms thus have a zero-mean
and a unit-variance). This procedure yields an X ∈ Rn×dx matrix of n ≈ 2700 words by dx = 4
features for each subject. Each of the columns of X is normalized to have a mean and a standard
deviation of 0 and 1 respectively.
3.5.3	Models and statistics
We compare B2B to four standard methods: Forward regression, Backward regression, CCA and PLS,
as implemented in scikit-learn (Pedregosa et al., 2011) and (Bilenko & Gallant, 2016), and optimized
with nested cross-validation over twenty l2 regularization parameters logarithmically spaced between
10-4 and 104 (for regression and CCA methods) or 1 to 4 canonical components (for PLS).
We used the feature importance described in Algorithm 2 to assess the extent to which each feature
Xi specifically improves the prediction of held-out Y data, using a 5-fold cross-validation (with
shuffled trials to homogeneize the distributions between the training and testing splits).
Each model was implemented for each subject and each time sample independently. Pairwise
comparison between models were performed using a two-sided Wilcoxon test across subjects (n=90)
using the average ∆R across time. Corresponding effect sizes are shown in Figure 4, and p-values
are reported below.
3.5.4	Results
We compared the ability of Forward regression, Backward regression, CCA, PLS and B2B to estimate
the causal contribution of four distinct but collinear features on brain evoked responses to words.
As expected, the Backward model reveals a similar decoding time course for Word Length and
Word Frequency, even though these features are known to specifically influence early and late MEG
responses respectively (Kutas & Federmeier, 2011). In addition, the same decoding time course was
observed for the dummy variable. These results illustrate that backward modelling cannot be used to
estimate the causal contribution of collinear features.
We thus focus on the four remaining methods (i.e. Forward Regression, PLS, CCA, and B2B) and
estimate their ∆R (i.e. the improvement of Y prediction induced by the introduction of a given
feature into the model,as described in Algorithm 2). Contrary to the Backward Model, none of the
models predicted the Dummy Variable to improve the Y prediction: all ∆R < 0 (all p > .089).
Figure 4 shows, for each model, the effects obtained across time (left) and subjects (right).
Word Length and Word Frequency improved the prediction performance of all methods: ∆R > 0
for all models (all p < 0.0001). As expected, the time course associated with Word Length and
Word Frequency rose from ≈ 100 ms and from ≈ 400 ms respectively. Furthermore, Word Function
improved the prediction performance of all models (all p < 0.0002) except for PLS (p = 0.7989).
Overall, these results confirm that Word Length, Word Frequency and Word Function causally
influence specific periods of brain responses to words.
To assess which model would be most sensitive to these causal discoveries, we compared B2B
to other models across subjects (Figure 4 right). For Word Length B2B outperforms all models
(all p < 0.00001) but CCA (p = 0.0678). For Word Frequency, B2B outperforms all models (all
p < 0.0006). For ”Word Function”, B2B outperforms all models (all p < 0.0015). Overall, these
results show that B2B reliably outperforms standard methods, especially when the effects are difficult
to detect.
7
Under review as a conference paper at ICLR 2020
4 Related work
Forward and cross-decomposition
models have been used to identify the
causal contribution of collinear fea-
tures onto multi-dimensional observa-
tions (e.g. (Naselaris et al., 2011)).
These approaches typically lead to
multiple coefficients for each features
(i.e. one per dimension of Y or one
per component respectively). Further-
more, these coefficients can be diffi-
cult to summarize into a single causal
estimate. By contrast, B2B quickly
(Fig. 6) leads to a single unbiased
scalar values E tending towards 1 and
0 for causal and non-causal features
respectively.
A variety of other statistical methods
applied to neuroimaging data have
been proposed to clarify what is be-
ing represented in brain responses -
i.e. what feature causes specific brain
activity. One of the popular linear
method is Representational Similarity
Analysis (RSA) (Kriegeskorte et al.,
2008), and consists in analyzing the
similarity of brain responses associ-
ated with specific categorical condi-
tions (e.g. distinct images), by (1)
fitting one-against-all classifiers on
each condition and (2) testing whether
these classifiers can discriminate all
other conditions. The resulting con-
fusion matrix is then analyzed in an
unsupervised manner to reveal which
conditions lead to similar brain activ-
ity patterns. B2B differs from RSA in
that (1) it uses regressions instead of
classifications, and can thus general-
ize to new items and new contexts and
(2) it is fully supervised.
Word Function
∆R -
0∙00j** ∙薄越
Model
Figure 4: Multiple models (color-coded) are compared on
their ability to reliably predict single-trial MEG signals
evoked by words. Left. Average improvement of corre-
lation coefficient ∆R for each of the four features (rows).
Error bars indicate standard error of the mean (SEM) across
subjects. Right. Average ∆R across time for each subject
(dots). Top horizontal lines indicate when B2B significantly
outperforms other methods (red) and vice versa.
Finally, CCA has been used in neuroimaging for a variety of purposes such as denoising and subject
alignment (Hotelling, 1936; de Cheveigne et al., 2019). While CCA relates to B2B, these two
methods diverge in several ways. First, CCA and B2B have different objectives: CCA aims to find the
potentially numerous and poorly interpretable components where X and Y are maximally correlated,
whereas B2B aims to recover the causal factors from X to Y. Second, B2B is not symmetric between
X and Y : it aims to identify specific causal features by first optimizing over the decoders G and then
over H . By contrast, CCA is symmetric between X and Y , and aims to find G and H such that they
project X and Y on maximally correlated dimensions. Third, CCA is based an eigen decomposition
of XH and Y G - the corresponding canonical components are thus mixing the X features in way
that limit interpretability and potentially dilute the impact of each feature onto multiple components.
In contrast B2B assesses each feature Xj on a single Y component specifically selected to maximize
signal-to-noise ratio of that feature j . Fourth, and unlike B2B, CCA does not separately optimize two
distinct regularization parameters for G and H . Finally, CCA does not use different data splits to
estimate G and H . Together, these differences may explain why B2B reliably outperform CCA on
estimating causal influences (Figs. 2 and 5).
8
Under review as a conference paper at ICLR 2020
5 Conclusion
In this work, we proposed Back-to-Back (B2B) regression, a linear method to measure the causal
influence of a potential set of variables generating multidimensional observations. B2B performs
two successive multidimensional regressions: one from the output domain, and another one from the
input domain. We provided a theoretical guarantee about the consistency of B2B, and compared it to
several baselines in controlled synthetic experiments. We also applied B2B to a recent brain imaging
dataset, analyzing the timing of brain responses and their connection to word features. We obtained
results consistent with prior work in neuroscience literature, confirming the efficacy of B2B for real
data analysis.
9
Under review as a conference paper at ICLR 2020
References
Natalia Y Bilenko and Jack L Gallant. Pyrcca: regularized kernel canonical correlation analysis in
python and its applications to neuroimaging. Frontiers in neuroinformatics, 10:49, 2016.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Radoslaw Martin Cichy, Dimitrios Pantazis, and Aude Oliva. Resolving human object recognition in
space and time. Nature neuroscience, 17(3):455, 2014.
Alain de Cheveigne, Giovanni M Di Liberto, Dorothee Arzounian, Daniel DE Wong, Jens Hjortkjaer,
S0ren Fuglsang, and Lucas C Parra. Multiway canonical correlation analysis of brain data.
NeuroImage, 186:728-740, 2019.
Karl J Friston, Andrew P Holmes, Keith J Worsley, J-P Poline, Chris D Frith, and Richard SJ
Frackowiak. Statistical parametric maps in functional imaging: a general linear approach. Human
brain mapping, 2(4):189-210, 1994.
Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A Engemann, Daniel Strohmeier, Christian
Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, et al. Meg and eeg data
analysis with mne-python. Frontiers in neuroscience, 7:267, 2013.
Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A Engemann, Daniel Strohmeier, Christian
Brodbeck, Lauri Parkkonen, and Matti S Hamalainen. Mne software for processing meg and eeg
data. Neuroimage, 86:446-460, 2014.
Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica: Journal of the Econometric Society, pp. 424-438, 1969.
Martin N Hebart and Chris I Baker. Deconstructing multivariate decoding for the study of brain
function. Neuroimage, 180:4-18, 2018.
Arthur E Hoerl. Optimum solution of many variables equations. Chemical Engineering Progress, 55
(11):69-78, 1959.
Matthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embed-
dings, convolutional neural networks and incremental parsing. To appear, 2017.
H. Hotelling. Relations between two sets of variables. Biometrika, (28):129-149, 1936.
Alexander G Huth, Wendy A de Heer, Thomas L Griffiths, Frederic E Theunissen, and Jack L Gallant.
Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600):453,
2016.
Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, Bernhard Scholkopf, et al. Quantifying
causal influences. The Annals of Statistics, 41(5):2324-2358, 2013.
G. V. Kass. Significance testing in automatic interaction detection (a.i.d.). Journal of the Royal
Statistical Society. Series C (Applied Statistics), 24(2):178-189, 1975.
Jean-Remi King, Laura Gwilliams, Chris Holdgraf, Jona Sassenhagen, Alexandre Barachant, Denis
Engemann, Eric Larson, and Alexandre Gramfort. Encoding and decoding neuronal dynamics:
Methodological framework to uncover the algorithms of cognition, 2018.
Nikolaus Kriegeskorte, Marieke Mur, and Peter A Bandettini. Representational similarity analysis-
connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2:4, 2008.
Marta Kutas and Kara D Federmeier. Thirty years and counting: finding meaning in the n400
component of the event-related brain potential (erp). Annual review of psychology, 62:621-647,
2011.
Ludovic Lebart, Alain Morineau, and Marie Piron. Statistique exploratoire multidimensionnelle,
volume 3. Dunod Paris, 1995.
10
Under review as a conference paper at ICLR 2020
J. A. Morgan and J. N. Sonquist. Problems in the analysis of survey data: and a proposal. J. Amer.
Statist. Ass., (58):415-434,1963.
Thomas Naselaris, Kendrick N Kay, Shinji Nishimoto, and Jack L Gallant. Encoding and decoding
in fmri. Neuroimage, 56(2):400-410, 2011.
Kenneth A Norman, Sean M Polyn, Greg J Detre, and James V Haxby. Beyond mind-reading:
multi-voxel pattern analysis of fmri data. Trends in cognitive sciences, 10(9):424-430, 2006.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. Journal of machine learning research, 12(Oct):2825-2830, 2011.
Felipe Pegado, Enio Comerlato, Fabricio Ventura, Antoinette Jobert, Kimihiro Nakamura, Marco
Buiatti, Paulo Ventura, Ghislaine Dehaene-Lambertz, Regine Kolinsky, Jose Morais, et al. Timing
the impact of literacy on visual processing. Proceedings of the National Academy of Sciences, 111
(49):E5233-E5242, 2014.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference: foundations
and learning algorithms. MIT press, 2017.
Ryan M Rifkin and Ross A Lippert. Notes on regularized least squares. Technical report, MIT, 2007.
Jan-Mathijs Schoffelen, Robert Oostenveld, Nietzsche HL Lam, Julia Udden, Annika Hulten, and
Peter Hagoort. A 204-subject multimodal neuroimaging dataset to study language processing.
Scientific data, 6(1):17, 2019.
Bernhard Scholkopf, David W Hogg, Dun Wang, Daniel Foreman-Mackey, Dominik Janzing, Carl-
Johann Simon-Gabriel, and Jonas Peters. Modeling confounding by half-sibling regression.
Proceedings of the National Academy of Sciences, 113(27):7391-7398, 2016.
Robyn Speer, Joshua Chin, Andrew Lin, Sara Jewett, and Lance Nathan. Luminosoinsight/wordfreq:
v2.2, October 2018. URL https://doi.org/10.5281/zenodo.1443582.
Walter JB Van Heuven, Pawel Mandera, Emmanuel Keuleers, and Marc Brysbaert. Subtlex-uk: A new
and improved word frequency database for british english. The Quarterly Journal of Experimental
Psychology, 67(6):1176-1190, 2014.
Sebastian Weichwald, Timm Meyer, Ozan Ozdenizci, Bernhard Scholkopf, Tonio Ball, and Moritz
Grosse-Wentrup. Causal interpretation rules for encoding and decoding models in neuroimaging.
NeuroImage, 110:48-59, 2015.
11
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Proof of consistency theorem
Proof of the theorem in 2.2:
Theorem 2 (B2B consistency - general case). Consider the B2B model from equation 1
Y = (XE + N)F
with N centred and full rank noise.
If F and X are full-rank on Img(E), then, the solution ofB2B, H minimizes
minkX-XH k2+kNH k2
H
and satisfies
ʌ ʌ
EH = H
>Λ	∕' T	A	1 T T Λ .1	1	i' .t i'	Λ	Λ	♦	i' l ʌ --∖ l ʌ
Proof. Let G and H be the solutions of the first and second regressions of B2B.
Since G is the least square estimator of X from Y
(G = arg min E[∣∣YG - X『]
Replacing Y by its model definition Y = (XE + N)F, we have
(G = arg min E[∣∣X - (XE + N)FG『]=arg min E[∣∣X - XEFG + NFG『]
Since N is centered and independent ofX, we have
G^ = argmin ∣∣X - XEFGk2 + ∣∣NFGk2
(14)
Samely, for H, We have
H = argminE[∣XH - YG^∣∣2] = argminE[∣XH - (XE + N)fG∣2]
=argminE[∣X(H - EFG)∣2] + E[∣NF(Gk2]
H
=argminE[∣X(H - EFG)∣2]
H
a positive quantity Which reaches a minimum (zero) for
ʌ ʌ
H = EFG	(15)
T	. 1 . 7-T 7-ɪ A	L A
Let us noW prove that EFG = FG.
Let F * be the pseudo inverse of F, and Z = Ft EFG, we have FZ = FFt EFG
Since F is full rank on Img(E), we have FF*E = E, and FZ = EFG
As E is a binary diagonal matrix, it is an orthogonal projection and therefore a contraction, thus
kNEFG∣2 ≤ ∣∣NFGGk2
and
∣∣X - XEFZk2 + ∣∣NFZ∣∣2 = ∣∣X - XEFGk2 + IlNEFG∣∣2 ≤ ∣∣X - XEFG∣∣2 + ∣∣NF(Gk2
But since G = argminG ∣∣X - XEFG∣2 + ∣∣NFG∣2, we also have
∣∣X - XEFG『+ IlNFG『≤ ∣∣X - XEFZ∣∣2 + ∣∣NFZ∣2
12
Under review as a conference paper at ICLR 2020
Summarizing the above,
∣∣X - XEF(G∣∣2 + ∣∣NFgG∣∣2 ≤ ∣∣X - XEFGk2 + kNEFGk2 ≤ ∣∣X - XEFGk2 + ∣∣NF(Gk2
∣∣X — XEFG∣∣2 + ∣∣NFG∣∣2 = kX — XEFG k2 + ∣∣NEF(G ∣∣2
∣∣NFG∣∣2 = kNEFG^k2
N being full rank, this yields EFG = FG.
Replacing into (14), and setting H = EFG, we have
G = arg min ∣∣X — XEFGk2 + ∣∣NFGk2
= arg min kX -XEFGk2 + kNEFGk2
H = arg min ∣∣X — XH∣2 + ∣∣NH∣2
H
Finally, EH = EEFG = EFG = H, since E, a binary diagonal matrix, is involutive. ThiS
completes the proof.	□
13
Under review as a conference paper at ICLR 2020
A.2 Modeling measurement noise
Equation 1 does not explicitly contain a measurement noise term. Yet, in moast experimental cases,
the problem is best described as:
Y = (XE+N)F+M
(16)
with M ∈ Rn×dy.
This equation is actually equivalent to Equation 1 given our hypotheses. Indeed, we can rewrite
M = MF-1F over Img(F), which leads to:
Y = (XE+N)F +M = (XE+N+MF-1)F = (XE + N0)F
Consequently, assuming that F is full rank on Img(XE), B2B yields the same solutions to equations
1 and 16.
A.3 Feature importance
For B2B, feature importance is assessed as follows:
Algorithm 2: B2B feature importance.
Input： Xtrain ∈ Rn×dx , Xtest ∈ Rn0×dx , Ytrain ∈ Rn×dy , Ytest ∈ Rn0×dy ,
Output: estimate of prediction improvement ∆R ∈ Ddx .
1	H,G = B2B(Xtrain, Ytrain);
2	Rfull = corr(X
test H, YtestG);
3	for i = 1, . . . , dx do
4	K = Id;
5	K [i] - 0;
6	Rk = corr(XtestKH, YtestGi);
7	∆Ri = Rfull - Rk;
8	end
9	return ∆R
For the Forward Model, the feature importance is assessed as follows:
Algorithm 3: Forward feature importance.
Input: Xtrain ∈ Rn×dx, Xtest ∈ Rn0×dx
,^^Ytr ain ∈ R	, , ^^Ytest ∈ R y ,
Output: estimate of prediction improvement ∆R ∈ Ddx,dy .
1	H = LinearRegression(Xtrain, Ytrain) Rfull = corr(Xtest K, Ytest);
2	for i = 1, . . . , dx do
3	K=Id;
4	K [i] - 0;
5	Rk = corr(XtestKH, Ytest);
6	∆Ri = Rfull - Rk;
7	end
8	return ∆R
14
Under review as a conference paper at ICLR 2020
For the CCA and PLS models, the feature importance is assessed as follows:
Algorithm 4: CCA and PLS feature importance.
Input: Xtrain ∈ Rn×dx, Xtest ∈ Rn0×dx, Ytrain ∈ Rn×dy, Ytest
Output: estimate of prediction improvement ∆R ∈ Ddx,dz .
∈ Rn0 ×dy
1
2
3
4
5
6
7
8
9
H,G = CCA(Xtrain, Ytrain);
Rfull = corr(Xt
est H, YtestG);
for i = 1, . . . , dx do
end
K = Id;
K [i] - 0;
Rk = corr(XtestKH, YtestG);
∆Ri = Rfull - Rk;
return ∆R
For the Backward Model, feature importance cannot be assessed because there is no prediction.
A.4 Recovering E
In case of noise, B2B yields non binary E. Three thresholding rules can be used to binarize its values
thus explicitly recover causal features.
First, given known signal-to-noise ratio,the threshold above which a feature should considered to be
causal can be derived analytically. Indeed, Equation 9 implies that the k first diagonal elements of HH
are bounded:
σXk	σX1
0 ≤ -~~—— ≤ diagk (H) ≤ --——
σXk + σN1	σX1 + σNk
where σX1, σXk, σN1 and σNk denote the largest and smallest eigenvalues of ΣX1X1 and ΣN1N1 .
The average value μ of non-zero coefficients of diag(H) is the trace of H divided by k, and can be
computed as
=	Var(X)	(i7)
μ = Var(X) + Var(N)	( )
The decision threshold between causal and non-causal elements is thus a fraction μ, whose proportion
arbitrarily depends on the necessity to favor type I and type II errors. In practice, we cannot use this
procedure for our MEG study, because signal-to-noise ratio is unknown.
Second, diag(H) can be binarized with the Sonquist-Morgan criterion (Morgan & Sonquist, 1963), a
non-parametric clustering procedure separating small and large values in a given set. This procedure
maximizes the ratio of inter-group variance while minimizing the intra-group variance, over all
possible splits of the diagonal into p largest values and dx - p smallest values. Let m0 and m1 be the
average values of the two clusters, p and dx - p their size, and v the total variance of the sample,
Sonquist-Morgan criterion maximizes (Kass, 1975):
p(dχ - P) (mi - mo)2
dx	v
(18)
This procedure assumes that there exists at least one causal and at least one non-causal feature. Third,
second-order statistics across multiple datasets can be used to identify the elements of diag(H) that
are sgnificantly different from 0. This procedure is detailed in the method section of our MEG
experiment.
Overall, these three procedure thus varies in their additional assumptions: i.e. (1) a known signal-to-
noise ratio, (2) the existence of both causal and non-causal factors or (3) independent repetitions of
the experiment.
15
Under review as a conference paper at ICLR 2020
AUC
ΔR
ΔR
Figure 5:	Synthetic experiments. Distribution (over conditions) of AUC (top) and Feature Importance
∆R (bottom) metrics between our method (y-axis) and the baselines (x-axis). Each dot is a distinct
synthetic experiment. Dots below the diagonal indicates that B2B outperform the tested model.
(SPUOKS)①IUQ V-UO-UI--TOS
IO1 ≡
IO0 :
lθ-ɪ :
IO-2 ξ
Forward PLS RegCCA B2B
Figure 6:	Wall-clock run-time for our method B2B and for the baselines. Each dot is a distinct
synthetic experiment. B2B runs much faster than cross-decomposition baselines.
16
Under review as a conference paper at ICLR 2020
Figure 7: Comparison of ∆R when the models are tested on four variables (top) and when the models
are tested on an these four variables as well as another 10 word-embedding features (bottom). These
results illustrate that, unlike Regularized CCA, B2B remains robust even when the number of tested
factors increases.
B	Additional Figures
B.1 Robustness to increasing number of factors
To test whether each of the methods robustly scales to an increasingly large number of potential
causes X , we enhanced the four ad-hoc features (word length, word frequency, word function, dummy
variable) with another ten features. These additional features corresponds to the first dimensions of
word embedding as provided by Spacy (Honnibal & Montani, 2017). The results shown in Figure 7,
show that the feature importance of ad-hoc features as derived by B2B remain unchanged and are
actually improved.
17