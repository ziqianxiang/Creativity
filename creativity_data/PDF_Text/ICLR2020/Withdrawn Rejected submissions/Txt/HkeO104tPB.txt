Under review as a conference paper at ICLR 2020
Reinforcement	Learning without	Ground-
Truth State
Anonymous authors
Paper under double-blind review
Abstract
To perform robot manipulation tasks, a low-dimensional state of the environment
typically needs to be estimated. However, designing a state estimator can some-
times be difficult, especially in environments with deformable objects. An alter-
native is to learn an end-to-end policy that maps directly from high-dimensional
sensor inputs to actions. However, if this policy is trained with reinforcement
learning, then without a state estimator, it is hard to specify a reward function
based on high-dimensional observations. To meet this challenge, we propose a
simple indicator reward function for goal-conditioned reinforcement learning: we
only give a positive reward when the robot’s observation exactly matches a target
goal observation. We show that by relabeling the original goal with the achieved
goal to obtain positive rewards (Andrychowicz et al., 2017), we can learn with the
indicator reward function even in continuous state spaces. We propose two meth-
ods to further speed up convergence with indicator rewards: reward balancing and
reward filtering. We show comparable performance between our method and an
oracle which uses the ground-truth state for computing rewards. We show that
our method can perform complex tasks in continuous state spaces such as rope
manipulation from RGB-D images, without knowledge of the ground-truth state.
1 Introduction
To perform robot manipulation tasks, a low dimensional
state of the environment typically needs to be estimated.
In reinforcement learning, this state is also used to com-
pute the reward function. However, designing a state es-
timator can be difficult, especially in environments with
deformable objects, as shown in Figure 1. An alternative
is to learn an end-to-end policy that maps directly from
high-dimensional sensor input to actions. However, with-
out a state estimator, it is hard to specify a reward function
based on high-dimensional observations.
Figure 1: An illustration of the rope push-
ing task. The Sawyer robot is given an im-
age of the current configuration of the rope
and an image of the goal configuration (illus-
trated as the translucent rope) and the task is
to push the rope to the goal configuration.
Past efforts to use reinforcement learning for robotics
have avoided this issue in a number of ways. One com-
mon approach is to use extra sensors to determine the
state of the environment during training, even if such sen-
sors are not available at test time. Examples of this in-
clude using another robot arm to hold all relevant ob-
jects (Levine et al., 2016), placing an IMU sensor (Gu
et al., 2017; Yahya et al., 2017), motion capture markers on such objects (Kormushev et al., 2010),
or ensuring that all relevant objects are placed on scales (Schenck & Fox, 2016).
However, such instrumentation is not always easy to set up for each task. This is especially true
for deformable object manipulation, such as rope or cloth manipulation, in which every part of the
object must be instrumented in order to measure the full state of the entire object. Attaching such
sensors to food or granular material would present additional difficulties.
We present an alternative approach for goal-conditioned reinforcement learning for specifying re-
wards using raw (e.g. high-dimensional and continuous) observations without requiring explicit
state estimation or access to the ground-truth state of the environment. We achieve this using a
1
Under review as a conference paper at ICLR 2020
simple indicator reward function, which only gives a positive reward when the robot’s observation
exactly matches a target goal observation. Naturally, in continuous state spaces, we do not ex-
pect any two observed states to be identical. Surprisingly, we show that we can learn with such
an indicator reward, even in continuous state spaces, if we use goal relabeling (Kaelbling, 1993;
Andrychowicz et al., 2017), which relabels the original goal with the achieved observation such that
a positive reward is given. As the indicator rewards produce extreme sparse positive rewards, we
further introduce reward balancing to balance the positive and negative rewards, as well as reward
filtering to filter out uncertain rewards.
We show theoretically that the indicator reward results in a policy with bounded suboptimality com-
pared to the ground-truth reward. We also empirically show comparable performance between our
method and an oracle which uses the ground-truth state for computing rewards, even though our
method only operates on raw observations and does not have access to the ground-truth state. We
demonstrate that an indicator reward can be used to teach a robot complex tasks such as rope manip-
ulation from RGB-D images, without knowledge of the ground-truth state during training. Videos
of our method can be found at https://sites.google.com/view/image-rl.
2	Related Work
2.1	Obtaining Ground-truth State for Training
Adding sensors: To obtain ground-truth states for calculating rewards, one approach is to perform
state estimation. However, such an approach can be noisy and challenging to implement, especially
for the deformable objects that we study in this work. Another approach is to add extra sensors
during training to accurately record the state. For example, in past work, one robot arm (covered
with a cloth at training time) is used to rigidly hold and move an object, while another robot arm
learns to manipulate the object (Levine et al., 2016). In such a case, the object position can be
inferred directly from the position of the robot gripper that is holding it. In other work on teaching a
robot to open a door, an IMU sensor is placed on the door handle to determine the rotation angle of
the handle and whether or not the door has been opened (Gu et al., 2017; Yahya et al., 2017). One can
also ensure that all relevant objects for a task are placed on scales (Schenck & Fox, 2016) or affixed
with motion capture markers to obtain a precise estimate of their position (Kormushev et al., 2010).
However, such instrumentation is challenging for deformable objects, granular material, food, or
other settings. Further, such instrumentation is costly and time-consuming to setup; hence most of
these previous approaches assume that such instrumentation is only available at training time and
these methods do not allow further fine-tuning of the policy after deployment.
Training in simulation: Another common approach is to train the policy entirely in simulation
in which ground-truth state can be obtained from the simulator (Fang et al., 2018; Andrychowicz
et al., 2018b; Zhu et al., 2018; Sadeghi & Levine, 2016; Pinto et al., 2018). Many approaches have
been explored to try to transfer such a policy from simulation to the real world, such as domain
randomization (Tobin et al., 2017) or building a more accurate simulator (Tan et al., 2018; Chebotar
et al., 2018). However, obtaining an accurate simulator is often very challenging, especially if the
simulator differs from the real-world in unknown ways. Further, building the simulator itself can be
fairly complex. Because these methods require the ground-truth state to obtain the reward function,
they require training in a simulator and do not allow further fine-tuning after deployment in the real
world; our method, in contrast, does not require the ground-truth state for the reward function.
2.2	Robot Learning without Ground-truth State
Learning a reward function without supervision: One line of work for learning a reward function
is to first learn a latent representation and then derive the reward function based on the distance in
the embedding space, such as cosine similarity. The representation can be learned by maximizing
the mutual information between the achieved goal and the intended goal (Warde-farley et al., 2018),
reconstruction of the observation with VAE (Nair et al., 2018), or learning to match keypoints with
spatial autoencoders (Finn et al., 2016b). Recent work also explicitly learns a representation that
is suitable for gradient based optimizer and then use it for specifying rewards (Yu et al., 2019b).
However, there is no guarantee that these learned representation are suitable for deriving rewards
for control. In addition, if the representation is pre-trained, it may also be incorrect in some parts
of the observation space which can be exploited by the agent. Our approach is much simpler in that
2
Under review as a conference paper at ICLR 2020
the reward function does not have any parameters that need to be learned and we empirically show
better performance to some reward learning approaches.
Changing the optimization: Another approach is to forego maximizing a sum of rewards as is
typically done in reinforcement learning and instead optimize for another objective. For example,
one method is to choose one-step greedy actions based on a learned one-step inverse dynamics
model; after training, the policy is then applied directly to a multi-step goal (Agrawal et al., 2016).
An alternative method is to learn a predictive forward dynamics model directly in a high-dimensional
state space and use visual model-predictive control (Finn et al., 2016a; Finn & Levine, 2017; Ebert
et al., 2017; 2018a;b). Although these methods have shown some promise, predicting future high-
dimensional observations (such as images or depth measurements) is challenging. Another approach
is to obtain expert demonstrations and define an objective as trying to imitate the expert (Sermanet
et al., 2018; 2016; Peng et al., 2018; Finn et al., 2017). Our approach, however, applies even when
demonstrations are not available.
2.3	Manipulating Deformable Objects
Deformable object manipulation presents many challenges for both perception and control. One
approach to the perception problem is to perform non-rigid registration to a deformable model of
the object being manipulated (Huang et al., 2015; Lee et al., 2015; Schulman et al., 2013; Wang et al.,
2011; Javdani et al., 2011; Miller et al., 2011; Cusumano-Towner et al., 2011; Phillips-Grafflin &
Berenson, 2014). However, such an approach is often slow, leading to slow policy learning, and can
produce errors, leading to poor policy performance. Further, such an approach often requires a 3D
deformable model of the object being manipulated, which may be difficult to obtain. Our approach
applies directly to high-dimensional observations of the deformable object and does not require a
prior model of the object being manipulated.
3	Problem Formulation
In reinforcement learning, an agent interacts with the environment over discrete time steps. In
each time step t, the agent observes the current state st and takes an action at . In the next time
step, the agent transitions to a new state st+1 based on the transition dynamics p(st+1 |st, at) and
receives a reward rt+ι = r(st, at, st+ι). The objective for the agent is to learn a policy π(a∕st)
that maximizes the expected future return R = E Pt∞=0 γtrt+1 , where γ is a discount factor.
3.1	Goal-reaching Reinforcement Learning
In order for the agent to learn diverse and general skills, we define a goal reaching MDP (Schaul
et al., 2015; Andrychowicz et al., 2017) as follows: In the beginning of each episode, a goal state
Sg is sampled from a goal distribution G. We learn a goal conditioned policy ∏(at∣st, Sg) that
tries to reach any goal state from the goal distribution. We use a goal conditioned reward function
rt = r(st+ι, Sg) and optimize for Esg〜G [P∞=0 γtrt]. The transition dynamics p(st+ι∣st,at) of
the environment remain independent of the goal.
In many real-world scenarios, it is often difficult to construct a well-shaped reward function. Past
work has shown that sparse rewards, combined with an appropriate learning algorithm, can achieve
better performance than poorly-shaped dense rewards in goal-reaching environments (Andrychow-
icz et al., 2017). We thus define a sparse reward function that only makes the binary decision of
whether the goal is reached or not. Specifically, let S+(Sg) be a subset of the state space such that
any state in this set is determined to be sufficiently close to Sg (in some unknown metric); in other
words, if the environmental state is within S+(Sg), then the task of reaching Sg can be considered
to be achieved.1 Naturally, we can assume that Sg ∈ S+ (Sg). A binary reward function can then be
defined as
r(St+1, Sg) = RR+ St+1∈SS+(Sg)	(1)
R- St+1 ∈/ S+ (Sg ),
where R+ and R- are constants representing the rewards received for achieving the goal and failing
to achieve the goal, respectively.
1 S+ is a function that maps from the state space to a subset of the space.
3
Under review as a conference paper at ICLR 2020
3.2 Rewards from Images
In many cases, the ground-truth state st is unknown and we cannot directly use the true reward
function defined in Equation 1. Instead, the agent observes high-dimensional observations ot from
sensors, from which We must instead define a proxy reward function r(ot,+1, og). The question now
becomes how to choose ^ to be optimal for reinforcement learning?
The most common approach in robotics is to perform state estimation. However, in many cases, the
state estimator might be hard to obtain, such as for deformable object manipulation, e.g. laundry
folding or food preparation. We therefore investigate whether an alternative reward function that
does not depend on state estimation can be used.
function defined in observation space of the form
r(ot+1 ,og) = {R+
Specifically, let us consider a general reward
一 A /	∖
ot+1 ∈ O+(og)
ot+1 ∈/ O+ (og),
(2)
where og is a representation of the goal in observation space and O+ (og) is a subset of the obser-
vation space for which we will give positive rewards. A number of past approaches have proposed
various methods for learning an observation-based reward function r(ot+1, og) (Warde-farley et al.,
2018; Nair et al., 2018; Florensa et al., 2019; Yu et al., 2019b). However, these approaches do not
analyze the properties needed by such a reward function to enable optimal learning. Next we will
investigate trade-offs between different choices of O+ (og) and how they will affect policy training
time when trained with rewards of r(ot+1, og).
4 Reward Misclassifications
We will now investigate how
to design a good proxy reward
function r(ot+1, og), based on
raw sensor observations, that we
can use to train the policy; we
desire for the policy trained with
r(ot+1 ,og) to optimize the orig-
inal reward r(st+1, sg) based on
the ground-truth state (which we
Increasing rate of false
negatives rewards
Increasing rate of false
positive rewards
do not have access to). Our Figure 2: As we increase false negative/positive rewards, the learning
first insight into choosing a good curves with false positive rewards are affected more severely.
proxy reward function r(ot+1, og) is that we should think about reward functions in terms of false
positives and false negatives. Let us define a false positive reward to occur when the agent re-
ceives a positive reward based on our proxy reward function r(ot+1, og) when it would have re-
ceived a negative reward based on the original reward function r(st+1 , sg). In other words, sup-
pose that an unknown function f maps from an observation to its corresponding ground-truth state,
st = f(ot). The function f exits as we assume that the environment is fully observable and no two
states will have the same observation. Then a false positive reward occurs when ot+1 ∈ O+ (og)
while f(ot+1) ∈/ S+(sg). Similarly, a false negative reward can be defined.
Intuitively, both false positive rewards and false negative rewards can negatively impact learning.
However, for any estimated reward function r(ot,+1, og), we will have either false positives or false
negatives (or both) unless we have access to a perfect state estimator. To design a good proxy reward
function, we must ask: which will more negatively affect learning: false positives or false negatives?
The two types of mistakes are not symmetric. As we will see, a false positive reward can signif-
icantly hurt policy learning, while a false negative reward is much more tolerable. Under a false
positive reward, the agent receives a positive reward (under the proxy reward function r(o, og)) for
reaching some observation o, even though the agent should receive a negative reward based on the
corresponding ground-truth state f (o) under the original reward function r(st, sg). This false posi-
tive reward will encourage the agent to continue to try to reach the state f (o), even though reaching
this state does not achieve the original task since f (o) ∈/ S+ (sg).
On the other hand, false negative rewards are much more tolerable. Under a false negative, the agent
observes some observation o such that f (o) ∈ S+ (sg) but the agent receives a negative reward.
4
Under review as a conference paper at ICLR 2020
However, if the agent still receives a positive reward for some other observation o0 such that f (o0) ∈
S+ (sg), then the agent can still learn to reach the goal states S+ (sg), though learning might be
slower and the learned policy may be suboptimal.
We provide a simple example to verify this intuition. Consider a robot arm reaching task, with
the observation space O ∈ R3 being the 3D position of the end-effector (EE). Note that here the
observation space coincides with the state space and the agent directly observes the states. The
action space A ∈ R3 controls the position of EE. The true reward is defined by S+ (sg) = {o |
kst - sg k2 < }. We define two types of noisy reward functions used for training. The reward
function rFP gives the same rewards as the true reward function, except that with a probability of
pFP (False Positive Rate), a negative reward will be flipped to a positive reward. The reward function
rFN can be similarly defined, where a positive reward will be flipped to a negative reward with a
probability of pFN (False Negative Rate). For this experiment, we use a standard reinforcement
learning algorithm DDPG (Lillicrap et al., 2016) combined with goal relabeling (Andrychowicz
et al., 2017). The learning performance of this same algorithm with different noisy rewards can be
shown in Figure 2. We can see that the agent is able to learn the task even with a very large false
negative rate. But when the false positive rate increases, the performance sharply decreases.
5	Approach
5.1	Indicator Rewards
Following this idea, we propose using a proxy reward function that does not have any false positive
rewards. To do so, we will use an extreme reward function of O+(og) = {og}. In other words, we
will use an indicator reward function:
^ind(0t+1,0g )= [R+ 0t+1 = Og	⑶
R- ot+1 6= og ,
It should be clear that this reward function will have no false positives, since the reward is positive
only if ot+1 = og, which implies that f (ot+1) = f(og), or equivalently, st+1 = sg. As sg ∈ S+(sg)
by definition, all positive rewards are true positives. However, this reward function is extremely
sparse and has many false negatives. In fact, without goal relabeling, in continuous state spaces, we
would expect all rewards to be negative under this indicator reward function, as no two observations
in continuous spaces will ever be identical. Next, we will describe how to learn with this reward
function with goal relabeling.
5.2	Goal Relabeling for Off-policy Learning
Fortunately, for off-policy multi-goal learning, we can adopt the goal relabeling technique intro-
duced in (Kaelbling, 1993; Andrychowicz et al., 2017) to learn the goal-conditioned Q-function.
Suppose that some transitions (ot,at,ot+ι) are observed when the agent takes an action at 〜
π(ot, og) with a goal of og. Because Q-learning is an off-policy reinforcement learning algorithm,
we can replace the goal observation og with any other observation og0 in our Bellman update of the
Q-function. This works because the transition dynamics p(st+1 |st, at), and likewise the observation
transition dynamics p(ot+1 |ot, at), are independent of the goal og. Specifically, for some transi-
tions, we will choose to replace og with the observation ot+1. By re-labeling og with ot+1 and using
our indicator reward function, we will have that Irind(ot+ι, og) = rind(ot+ι, ot+ι) = R+∙ Thus,
using goal relabeling, we can get positive rewards, even when using an indicator reward function in
continuous state spaces.
5.3	Reward Balancing and Filtering
As mentioned above, after sampling a batch of data, we train the Q-function with goal relabeling.
We use three different strategies for choosing which goals to use for relabeling: with probability p1,
we relabel og with ot+1 , which will receive a positive reward under our indicator reward function.
With probability p2, we relabel the goal og with ot0 with an observation from some future time t0
step within the episode. The indicator reward function will most likely give a negative reward in this
case, which is possibly a false negative because the new goal is possibly considered achieved based
on the state-based, ground-truth reward funciton. Finally, with probability p3 , we use the original
5
Under review as a conference paper at ICLR 2020
goal (with no relabeling), which will again most likely give a negative reward under the indicator
reward function; as before, this might be a false negative.
Reward balancing: We refer to “reward balancing” as setting p1 = p2 = 0.45 and p3 = 0.1,
leading us to receive positive rewards approximately 0.45 of the time and negative rewards approx-
imately 0.55 of the time. Thus the ratio of positive and negative rewards that we use to train the
Q-function are approximately balanced, even with indicator rewards. From another perspective, p1
and p2 determine the relative frequency between providing positive rewards and propagating rewards
to other timesteps in the episode. Additionally, training with a small fraction of the original goals
(i.e. p3) can be seen as a regularization which ensures that the distribution of the relabeled goals
moves towards the original goal distribution.
Reward filtering: While false negative rewards do not hurt learning as much as false positives, we
still wish to avoid them if possible to improve the convergence time of the learned policy. We achieve
this using “reward filtering,” in which we filter out transitions that we suspect of having a high chance
of being false negatives. We refer to “reward filtering” as discarding a sampled transition if its Q
value is above a threshold q0 . If the assigned reward is negative based on the proxy reward function
but the Q-value is sufficiently high, then there is a chance that this reward a false negative. To reduce
the fraction of false negatives, we filter out such transitions and do not use them for training.
We can estimate how to set the threshold q0 as follows: for a given transition (ot, at, ot+1), if
ot+1 = Og, We know that rind(ot+ι,θg) = R+. In this case, Q*(ot, at, Og) = R+∕(1 - γ), where
Q* is the optimal Q-function, assuming the optimal policy will continue to receive (discounted)
positive rewards in the future. Similarly, if ot+ι = Og, then rind(ot+ι,θg) = R-. Since we
know that the policy starting from ot will thus receive at least one negative reward before receiving
positive rewards, then Q*(ot, at, Og) ≤ R- + γR+∕(1 - γ). Thus, we can set a threshold qo,
where R- + γR+∕(1 — Y) < qo < R+∕(1 - γ); if we find that Q(ot, Og, at) > qo, then the
corresponding reward rind(ot+ι, Og) is likely to be a false negative (assuming that the Q-function
has been trained well); we thus filter out such rewards, to reduce the number of false negatives that
we use for training. We can see that qo is set to a rather conservative fitlering value. Additionally, the
Q-function is initialized to a relatively low value to avoid overestimation of the Q-function which
can lead to incorrect filtering in the beginning of the training.
6	Analysis
In this section, we analyze the performance of learning with indicator rewards. We first interpret the
goal conditioned Q-function as a measure of the time to reach one observation from another.
6.1	Minimum Reaching Time Interpretation
Let us define d = Dπ(Ot, at, O+(Og)) as the number of time steps it takes for the policy π to go from
the current observation Ot, starting with action at, to reach the set O+(Og) of goal observations. For
simplicity, we assume that, once the agent receives a positive reward, it will take actions to continue
to receive positive rewards. The Q-function can be written as
Qπ(Ot, at, Og) = R- + γR- + ... + γd-1R- + γdR+ + γd+1R+ + ...
γd	1
=ɪ (R+ - R-) +1- R-.
1-γ	1-γ
(4)
Now it can be easily seen that, as long as R+ > R-, Qπ is strictly monotonically decreasing w.r.t.
d. As such, maximizing Qπ over π is equivalent to minimizing the time the agent takes to reach the
goal O+(Og). Note that this is true for varying definitions of O+(Og); thus the policy trained under
the true reward function (Equation 1) will minimize Dπ(Ot, at, O+(Og)) whereas the policy trained
under the indicator reward function will minimize Dπ (Ot, at, {Og}) (slightly overloading notation
for Dπ). Below we will show how this interpretation of the policy’s behavior at convergence can
lead to a simple analysis of the suboptimality of the learned policy under the indicator reward.
6.2	Analysis of Sub-optimality
Due to the false negative rewards given by the indicator function Irind, the learned policy may not
be optimal with respect to the original reward function r(st+1, sg) defined in Equation 1. Here we
6
Under review as a conference paper at ICLR 2020
give the worst case bound for the policy learned with the indicator reward. Following the minimum
reaching time interpretation of the previous section, we evaluate the performance of the policy in
terms of the time it takes to reach the set of goal observations O+(og) from the current observation.
Given ot , og , denote t1 as the minimum number of time steps to reach from ot to the set of true goal
observations, i.e. t1 = D(ot, O+(og)). Let t2 = D(ot, og) be the minimum time to reach from ot
to og. Define the diameter of this goal observation set as d = max{D(o1, o2)|o1, o2 ∈ O+(og)}.
From the optimality of t2, we know that
t2 ≤ D(ot, o) + D(o, og), ∀o ∈ O+ (og).
Thus,
t2 ≤ min D(ot, o) + D(o, og) ≤ t1 + d.
o∈O+(og)
(5)
From the analysis in the previous section, the optimal policy which optimizes the indicator reward
will reaches og in t2 time steps; since og ∈ O+(og), we know that this policy will reach O+(og) in
some time t3 ≤ t2 . Also recall that we have defined t1 such that the optimal policy under the true
reward function of Equation 1 will reach O+(og) in t1 steps. Thus t3/t1 ≤ (t1 + d)/t1 is an upper
bound on the suboptimality of the policy trained under the indicator reward, at convergence.
7	Experiments
Our experiments address the following questions:
1.	In the case of visual input, how much are the sample efficiency and the final performance
affected without assuming access to the ground-truth reward?
2.	How much does reward balancing and filtering improve learning efficiency?
3.	Can our method scale to real world robotic tasks?
We denote our method, which uses indicator rewards with reward balancing and filtering, as Indi-
cator+Balance+Filter. We compare our method with the following baselines:
•	Oracle: This method assumes access to the ground-truth reward from state space r(st, sg).
•	Indicator: This is an ablation of our method, without reward balancing and filtering.
•	Auto Encoder (AE): We train an autoencoder with an L2 reconstruction loss of the im-
age observation, jointly with the RL agent. We then use cosine similarity in the learned
embedding space to provide dense rewards, as similarly compared in (Warde-farley et al.,
2018). Specifically, assuming the learned encoding of an observation o is φ(o) after L2
normalization, the reward will be r(o, og) = max(0, φ(o)T φ(og)).
•	Variational Auto Encoder (VAE): Similarly to the AE baseline, a VAE is jointly trained
with the RL agent to provide rewards, as done in (Nair et al., 2018). For a fair comparison,
the goal sampling strategy for this baseline is kept the same a other approaches.
•	Distributional Planning Network (DPN) (Yu et al., 2019b): DPN aims to learn a repre-
sentaiton that is suitable for a gradient based planner to reach a goal observation. Following
(Yu et al., 2019b), we first pre-train DPN using randomly generated samples and then use
the learned representation for giving rewards.
Only Oracle uses the ground-truth, state-based reward function. We use the standard off-policy
learning algorithm DDPG (Lillicrap et al., 2016) with goal relabeling (Andrychowicz et al., 2017).
For methods without reward balancing, we re-label the current goal with an achieved goal sampled
uniformly from one of the future time steps within the same episode with a probability of 0.9;
otherwise, the original goals are used. For all the environments, the ground-truth rewards are based
on the L2 distance in the state space: R+ if kst+1 - sg k ≤ and R- otherwise. More details on
algorithms, architectures, and hyperparameters can be found in the Appendix.
We first evaluate all the methods in a set of simulated environments in MuJoCo (Todorov et al.,
2012), where both current and goal observation are given by RGB-D images:
•	Reacher: Teach a two-link arm to reach a randomly located position in 2D space.
7
Under review as a conference paper at ICLR 2020
•	FetchReach: Move the end effector of the Fetch robot to a random position in 3D.
•	RopePush: Push a rope from a random initial configuration into a target configuration.
The first two environments above are standard environments from Gym (Andrychowicz et al.,
2018a). For the more complex RopePush task, the robot needs to push a 15-link rope to a tar-
geted pose, as shown in Figure 1. To accelearate learning, we fix the orientation of the gripper and
parameterize the action as (x1 , y1 , x2, y2) ∈ R4, denoting the starting and ending position of one
push from the gripper. We generate the initial rope pose by giving the rope a random push from a
fixed location. The goal poses are generated by giving the rope two more pushes based on the initial
push (these pushes are hidden from the policy). The robot can give three pushes to push the rope to
the goal pose. More details on environments can be found in Appendix A.
The results are shown in Figure 3. We see that our method (Indicator+balance+Filter) achieves
nearly the same performance as the Oracle even though our method operates only from RGB-D
images and does not have access to the ground-truth state. For the RopePush environment, only
the Oracle and our method are able to learn to achieve the task to a reasonable accuracy. For AE,
VAE and DPN, the learned representation may not lead to a perfect reward function everywhere and
the agent will exploit states that yield a high proxy rewards, even though the goal is not achieved.
Instead, our method does not require learning a reward function and outperforms these baselines.
---VAE   Oracle ---------- Indicator+Balance+Filter - Indicator   DPN ------- AE
ViSUal ReaCher
OK
0 5 0 5
2 110
0.0.0.0.
-BOo Ol(υ0uπ5q∞α -Pu 一LL.
Figure 3: The final distance to goal of different methods in different environments throughout the training. The
observations are from the RGB-D images rendered in simulation.
In Appendix C, we perform an ablation where we removed either Balance or Filter from our method.
These experiments show that both Balance and Filter are important for optimal performance across
the environments tested. In Appendix D, we show experiments in which the ground-truth states are
used as inputs to the policy but are not used to compute the rewards. In these experiments, we also
see that our method has a similar performance to the Oracle and outperforms the other approaches.
7.1 Indicator Rewards with Real Images
50K IOOK 150K	200K
Timesteps
25K	50K	75K IOOK
Timesteps
Using RGB-D observations and goals, we train
a Sawyer robot for a 3-dimensional reaching
task. Figure 4 shows example observation and
goal images as well as the distance to goal
throughout training. As before, our method (In-
dicator+Balance+Filter) performs similarly to
the Oracle in terms of final goal distance; the
baseline of Indicator Rewards without balanc-
ing or filtering performs significantly worse and
diverges in the end due to many negative re-
wards, many of which are false negative.
8	Conclusion
In this work, we show that we can train a robot
ViSUal SaWyer ReaCher
Figure 4: An example observation image (top left) and
goal image (top right); final distance to goal (bottom).
to perform complex manipulation tasks directly from high-dimensional images, without requiring
access to the ground-truth state in either the policy input or the reward function. We empirically
show that our method enables a robot to learn complex skills for manipulating deformable objects,
for which state estimation is often challenging, including a real-world experiment.
8
Under review as a conference paper at ICLR 2020
We provide a theoretical analysis which shows that the optimal policy under the indicator reward
has a bounded sub-optimality compared to the optimal policy under the ground-truth reward. We
hope that our method will enable robot learning in the real world in cases where it is difficult to add
extra sensors or accurately simulate the environment.
References
Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke
by poking: Experiential learning of intuitive physics. In NIPS, pp. 5θ74-5082, 2016.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-
play. In NIPS, pp. 5048-5058, 2017.
Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa-
chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018a.
Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa-
chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018b.
Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff,
and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world
experience. arXiv preprint arXiv:1810.05687, 2018.
Marco Cusumano-Towner, Arjun Singh, Stephen Miller, James F O’Brien, and Pieter Abbeel. Bring-
ing clothing into desired configurations with limited perception. In ICRA, pp. 3893-3900. IEEE,
2011.
Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with
temporal skip connections. arXiv preprint arXiv:1710.05268, 2017.
Frederik Ebert, Sudeep Dasari, Alex X Lee, Sergey Levine, and Chelsea Finn. Robustness
via retrying: Closed-loop robotic manipulation with self-supervised learning. arXiv preprint
arXiv:1810.03043, 2018a.
Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual fore-
sight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint
arXiv:1812.00568, 2018b.
Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li Fei-Fei, and Silvio
Savarese. Learning task-oriented grasping for tool manipulation from simulated self-supervision.
RSS, 2018.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In Robotics and
Automation (ICRA), 2017 IEEE International Conference on, pp. 2786-2793. IEEE, 2017.
Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction
through video prediction. In Advances in neural information processing systems, pp. 64-72,
2016a.
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep
spatial autoencoders for visuomotor learning. In ICRA, pp. 512-519. IEEE, 2016b.
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imita-
tion learning via meta-learning. arXiv preprint arXiv:1709.04905, 2017.
Carlos Florensa, Jonas Degrave, Nicolas Heess, Jost Tobias Springenberg, and Martin Riedmiller.
Self-supervised learning of image embedding for continuous control. Workshop at NIPS 2018,
2019.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning
for robotic manipulation with asynchronous off-policy updates. In ICRA, pp. 3389-3396. IEEE,
2017.
9
Under review as a conference paper at ICLR 2020
Sandy H Huang, Jia Pan, George Mulcaire, and Pieter Abbeel. Leveraging appearance priors in non-
rigid registration, with application to manipulation of deformable objects. In IROS, pp. 878-885.
IEEE, 2015.
Shervin Javdani, Sameep Tandon, Jie Tang, James F O’Brien, and Pieter Abbeel. Modeling and
perception of deformable one-dimensional objects. In ICRA, pp. 1607-1614. IEEE, 2011.
Leslie Pack Kaelbling. Learning to achieve goals. In Proceedings of the Thirteenth International
Joint Conference on Artificial Intelligence, Chambery, France, 1993. Morgan Kaufmann.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Representations (ICLR), 2014.
Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Robot motor skill coordination with
em-based reinforcement learning. In IROS, pp. 3232-3237. IEEE, 2010.
Alex X Lee, Max A Goldstein, Shane T Barratt, and Pieter Abbeel. A non-rigid point and normal
registration algorithm with applications to learning from demonstrations. In ICRA, pp. 935-942,
2015.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. ICLR,
2016.
Stephen Miller, Mario Fritz, Trevor Darrell, and Pieter Abbeel. Parametrized shape models for
clothing. In ICRA, pp. 4861-4868. IEEE, 2011.
Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual
reinforcement learning with imagined goals. Advances in Neural Information Processing Systems,
pp. 9191-9200, 2018.
Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational dis-
criminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining infor-
mation flow. arXiv preprint arXiv:1810.00821, 2018.
Calder Phillips-Grafflin and Dmitry Berenson. A representation of deformable objects for motion
planning with no physical simulation. In ICRA 2014, pp. 98-105. IEEE, 2014.
Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asym-
metric actor critic for image-based robot learning. 2018.
Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a single real image.
arXiv preprint arXiv:1611.04201, 2016.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International conference on machine learning, pp. 1312-1320, 2015.
Connor Schenck and Dieter Fox. Guided policy search with delayed sensor measurements. arXiv
preprint arXiv:1609.03076, 2016.
John Schulman, Alex Lee, Jonathan Ho, and Pieter Abbeel. Tracking deformable objects with point
clouds. In ICRA, pp. 1130-1137. IEEE, 2013.
Pierre Sermanet, Kelvin Xu, and Sergey Levine. Unsupervised perceptual rewards for imitation
learning. arXiv preprint arXiv:1612.06699, 2016.
Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey
Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In
ICRA, pp. 1134-1141. IEEE, 2018.
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and
Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. RSS, 2018.
10
Under review as a conference paper at ICLR 2020
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
IROS,pp. 23-30. IEEE, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IROS 2012, pp. 5026-5033. IEEE, 2012.
Ping Chuan Wang, Stephen Miller, Mario Fritz, Trevor Darrell, and Pieter Abbeel. Perception for
the manipulation of socks. In IROS 2011, pp. 4877-4884. IEEE, 2011.
David Warde-farley, Tom Van De Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and
Volodymyr Mnih. Unsupervised Control through Non-parameteric Discriminative Rewards.
ICLR, pp. 1-17, 2018.
Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, and Sergey Levine. Collective robot
reinforcement learning with distributed asynchronous guided policy search. In IROS, pp. 79-86.
IEEE, 2017.
Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, and Chelsea Finn. Unsupervised visuomotor control
through distributional planning networks. https://github.com/tianheyu927/dpn,
2019a.
Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, and Chelsea Finn. Unsupervised visuomotor control
through distributional planning networks. Robotics: Science and Systems, 2019b.
Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvunakool,
Janos Kramar, Raia HadselL Nando de Freitas, et al. Reinforcement and imitation learning for
diverse visuomotor skills. arXiv preprint arXiv:1802.09564, 2018.
11
Under review as a conference paper at ICLR 2020
A Environment Details
During evaluation, for all environments, a binary sparse reward is given at each time step. A positive
reward R+ = 1 is given when the goal is reached, i.e. ||st+1 - sg|| ≤ and a negative reward
R- = -1 is given otherwise. Other environment details are summarized in Table 1.
For the RopePush environment, to save the time for computing initial and goal configuration, 10,000 initial configurations of the rope are pre-computed and cached which are later used for training.						
Environment	Observation Dimension	Goal Dimension	Rendered Dimension	Action Dimension	Horizon (T)	(m)
Reacher	10	2	100x100x3	2	50	0.01
FetchReach	10	3	100x100x4	3	50	0.05
FetchPush	25	3	-	4	50	0.05
RopePush	45	30	100x100x3	4	3	0.1
VisualReacher	-	-	100x100x4	3	25	0.1
(Sawyer)						
Table 1: Summarized environment details. The observation and goal dimension are the dimensions of the low
dimension state representation when available. The rendered dimensions are the dimensions of the rendered
RGBD images used in the visual experiments.
Sawyer robot experiment details: The observation is recorded with an Intel RealSense D435 depth
camera. The goal observations are sampled by moving the robot arm to a uniformly sampled location
in a cuboid of diagonal length 1.3m. The episode was considered successful if the end effector
moved to within 0.1 m from goal location at the end of the episode (we used a time horizon of 25
steps). The trained policy performs position control and outputs end effector displacement within a
range of -0.05m to 0.05m in each direction.
B	Hyper-parameters
All the experiments are run for two random seeds. The hyper-parameters of the training algorithm
with indicator rewards are summarized in Table 2. For all experiments with visual observation, the
parameters of the convolution layers are shared among the observation input and goal input. Due to
the complexity of the RopePush environment, a spatial softmax layer (Levine et al., 2016) with an
output Size of 32 is applied before the fully Connected layers.____
Parameter	Value
shared positive reward (R+) negative reward (R- ) reward filtering (q0) optimizer learning rate discount (γ) target network smoothing (τ) nonlinearity	1 -1 2 [A- + γR+∕(i-γ) + R+∕(1-y)] Adam (Kingma & Ba, 2014) 0.001 T-1 ^Γ~ 0.98 tanh
state observation replay buffer size minibatch size network architecture	106 256 3 hidden layers with 256 neurons for each
visual observation replay buffer size minibatch size network architecture	5 ∙ 103 128 4 convolution layers followed by 3 hidden layers with 256 neurons for each
Table 2: Summarized hyper-parameters.
12
Under review as a conference paper at ICLR 2020
C Ablation Analysis
We show different ablations of our methods in Figure 5. We can see that, in the RopePush
environment, filtering is required for the policy to learn. On the other hand, the Reacher and
FetchReach environments show that balancing is required for optimal performance. In all cases,
Indicator+Balance+Filter consistently performs better than all the ablated methods. Thus, these re-
sults show that both balancing and filtering are important for optimal performance across a range of
Visual Reacher
20
-POD OI ①。U£SQ-BUiz
OK	IOOK 200K
Timesteps
-PoD 0一 əuupls 一 Cl-PU 一LL.
ViSllal FetChReaCh
Visual RopePush
Figure 5: The success (top) and the final distance to goal (bottom) of different ablations of our method.
With indicator rewards, we do not have any false positive rewards but may have many false negative
rewards. In Figure 6, we show the accuracy of the given rewards using different approaches with
indicator rewards. The ground-truth rewards are calculated based on the ground-truth states which
we do not have during training and are only used for analysis. We can see that with a longer
time horizon, reward balancing is more important for the Visual Reacher and Visual FetchReach
environment, which significantly lower the false negative rates. On the other hand, both reward
filtering and balancing are important in the Visual RopePush environment. In all the cases, the
accuracy of the rewards are improved a lot and sometimes almost perfect with reward balancing and
filtering and We Can see that this is necessary for the good performance of using indicator rewards.
Visual Reacher	Visual FetchReach	Visual RopePush
0.100
0.400
0.300
0.200
Visual Reacher	_____ Visual FetchReach
AUBJnUV PJBMea
< P.IPMBa
0.400
2
5 0.600
< P.IPMBa
OK	IOOK 200K	OK	IOOK 200K
Timesteps	Timesteps
1.000
⅛ 0.980
0.960
0.940
0.920
Figure 6: The false negative rate and the reward accuracy calculated from a batch sampled from the replay
buffer of different ablations of our method.
①⅛α ①>⅛6 0 N ① s-bll.
13
Under review as a conference paper at ICLR 2020
D Learning with Indicator Rewards with State Input
The performances of different learning methods when learning with low dimensional state represen-
tation are shown in Figure 7. In all the environments, using indicator rewards with reward balancing
and filtering achieves comparable performances to Oracle. Compare this method to the Indicator
baseline which does not have reward balancing and filtering, Indicator+Balance+Filter achieves a
better sample efficiency. Interestingly, in the FetchReach environment, the default distance thresh-
old for receiving an R+ reward is set to 0.05. Thus, the policy that learns with this reward stops
learning when the policy reach to such a distance to the goal, while the policy learned with indicator
rewards keep reaching closer to the goals. This shows another benefit of using the indicator reward
that the user does not need to tune the hyper-parameter e to achieve the best performance.
Figure 7: The success (upper row) and the final distance to goal (lower row) of different methods in different
environments throughout the training. The success is defined as the mean probabily of getting the R+ reward
over all time steps. The final distance to goal is defined as the L2 distance to the goal in the state space, in the
last time step of the episode. The input to the policy is the low dimension state representation.
E	Implementation Details of Baselines
•	Auto Encoder (AE): The network architecture for the encoder is four convolutional layers
each followed by a max-pooling layer. The latent code as a dimension of 32. The decoder is
four up-pooling and up-convolution layers. The auto encoder is trained jointly with the RL
agent by the reconstruction loss of the observation images in the sampled transition. The
learning rate is set to 0.001. We then use cosine similarity in the learned embedding space to
provide dense rewards, as similarly compared in (Warde-farley et al., 2018). Specifically,
assuming the learned encoding of an observation o is φ(o) after L2 normalization, the
reward will be r(o, og) = max(0, φ(o)T φ(og)).
•	Variational Auto Encoder (VAE) The VAE uses the same architecture and training pro-
cedure as the AE baseline. Following (Nair et al., 2018), we use a variant of the VAE,
β-VAE with β = 5. Following (Nair et al., 2018), given the latent representation z, zg of
the current observation o and the goal observation og , the rewards are given by
r(o,og) = -||z-zg||2,
which is the negative of the Euclidean distance.
•	Distributional Planning Network (DPN) (Yu et al., 2019b) We use the released imple-
mentation (Yu et al., 2019a) and use the default hyperparameters. The network architecture
for encoder is three convolutional layers, each with kernel size 5, stride 2. After each
convolutional layer, there is layer normalization and sigmoid non-linearity. The output rep-
resentation from CNN goes into a fully connected layer with latent code dimension of 128.
For each environment, we collected 20,000 transitions from a random policy and performed
100,000 minibatch updates. The input dimension for each of the three environments is the
same as the Rendered Dimension column specified in Table 1. Once the DPN is pre-trained,
the representation is fixed during the training of the RL agent. Following (Yu et al., 2019b),
given the latent representation z, zg of the current observation and the goal observation, the
14
Under review as a conference paper at ICLR 2020
rewards are given by
r(o,θg) = - exp(||dDPN(z - Zg,δ)∣∣ι),
where
dDPN(x, δ)i =
J 2 χ2
[6|xi| - 1 δ2
for |xi | ≤ δ
otherwise
δ is set to 0.85. To avoid overflow of the rewards during the exponential, we normalize z
and zg such that they have a norm of 1 after the representation is trained.
F Sensitivity on Filtering Threshold
In Section 5.3, we give the range of the filtering threshold q0 . In previous experiments, we use the
middle value of this range. Here, we train with different values of q0 within this range. The results
for the rope pushing task is shown in Figure 8. We can see that the learning is very stable with
different values of q0.
Figure 8: Learning with indicator rewards on the rope push environment using different filtering threshold. The
range given in Section 5.3 is [1, 3]. Here we show 6 values of q0 evenly spaced within this range.
15