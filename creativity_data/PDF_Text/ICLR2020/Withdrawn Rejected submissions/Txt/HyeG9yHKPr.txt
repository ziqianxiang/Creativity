Under review as a conference paper at ICLR 2020
Causally Correct Partial Models for
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
In reinforcement learning, we can learn a model of future observations and re-
wards, and use it to plan the agent’s next actions. However, jointly modeling
future observations can be computationally expensive or even intractable if the
observations are high-dimensional (e.g. images). For this reason, previous works
have considered partial models, which model only part of the observation. In this
paper, we show that partial models can be causally incorrect: they are confounded
by the observations they don’t model, and can therefore lead to incorrect planning.
To address this, we introduce a general family of partial models that are provably
causally correct, yet remain fast because they do not need to fully model future
observations.
1 Introduction
The ability to predict future outcomes of hypothetical decisions is a key aspect of intelligence.
One approach to capture this ability is via model-based reinforcement learning (MBRL) (Munro,
1987; Werbos, 1987; Nguyen & Widrow, 1990; Schmidhuber, 1991). In this framework, an agent
builds an internal representation st by sensing an environment through observational data yt (such
as rewards, visual inputs, proprioceptive information) and interacts with the environment by taking
actions at according to a policy ∏(at |st). The sensory data collected is used to build a model that
typically predicts future observations y>t from past actions a≤t and past observations y≤t. The
resulting model may be used in various ways, e.g. for planning (Oh et al., 2015; Silver et al., 2017a),
generation of synthetic training data (Weber et al., 2017), better credit assignment (Heess et al.,
2015), learning useful internal representations and belief states (Gregor et al., 2019; Guo et al.,
2018), or exploration via quantification of uncertainty or information gain (Pathak et al., 2017).
Within MBRL, commonly explored methods include action-conditional, next-step models (Oh et al.,
2015; Ha & Schmidhuber, 2018; Chiappa et al., 2017; Schmidhuber, 2010; Xie et al., 2016; Deisen-
roth & Rasmussen, 2011; Lin & Mitchell, 1992; Li et al., 2015; Diuk et al., 2008; Igl et al., 2018;
Ebert et al., 2018; Kaiser et al., 2019; Janner et al., 2019). However, it is often not tractable to
accurately model all the available information. This is both due to the fact that conditioning on
high-dimensional data such as images would require modeling and generating images in order to
plan over several timesteps (Finn & Levine, 2017), and to the fact that modeling images is chal-
lenging and may unnecessarily focus on visual details which are not relevant for acting. These
challenges have motivated researchers to consider simpler models, henceforth referred to as partial
models, i.e. models which are neither conditioned on, nor generate the full set of observed data (Guo
et al., 2018; Gregor et al., 2019; Amos et al., 2018).
In this paper, we demonstrate that partial models will often fail to make correct predictions under a
new policy, and link this failure to a problem in causal reasoning. Prior to this work, there has been
a growing interest in combining causal inference with RL research in the directions of non-model
based bandit algorithms (Bareinboim et al., 2015; Forney et al., 2017; Zhang & Bareinboim, 2017;
Lee & Bareinboim, 2018; Bradtke & Barto, 1996) and causal discovery with RL (Zhu & Chen,
2019). Contrary to previous works, in this paper we focus on model-based approaches and propose
a novel framework for learning better partial models. A key insight of our methodology is the fact
that any piece of information about the state of the environment that is used by the policy to make
a decision, but is not available to the model, acts as a confounding variable for that model. As a
1
Under review as a conference paper at ICLR 2020
result, the learned model is causally incorrect. Using such a model to reason may lead to the wrong
conclusions about the optimal course of action as we demonstrate in this paper.
We address these issues of partial models by combining general principles of causal reasoning,
probabilistic modeling and deep learning. Our contributions are as follows.
•	We identify and clarify a fundamental problem of partial models from a causal-reasoning per-
spective and illustrate it using simple, intuitive Markov Decision Processes (MDPs) (Section 2).
•	In order to tackle these shortcomings we examine the following question: What is the minimal
information that we have to condition a partial model on such that it will be causally correct with
respect to changes in the policy? (Section 4)
•	We answer this question by proposing a family of viable solutions and empirically investigate
their effects on models learned in illustrative environments (simple MDPs and 3D environments).
Our method is described in Section 4 and the experiments are in Section 5.
2 A simple example: FuzzyB ear
We illustrate the issues with partial models using a simple example. Consider the FuzzyBear MDP
shown in Figure 1(a): an agent at initial state s0 transitions into an encounter with either a teddy
bear or a grizzly bear with 50% random chance, and can then take an action to either hug the bear
or run away. In order to plan, the agent may learn a partial model q®(r2∣s0, ao, aι) that predicts the
reward r2 after performing actions {a0, a1} starting from state s0. This model is partial because it
conditions on a sequence of actions without conditioning on the intermediate state s1. The model
is suitable for deterministic environments, but it will have problems on stochastic environments, as
we shall see. Such a reward model is usually trained on the agent’s experience which consists of
sequences of past actions and associated rewards.
Legend
(ɔ Statenode
O action node
■ terminal state
rM transition
pλ3 probability
r reward
Figure 1: Examples of stochastic MDPs. (a) FuzzyBear: after visiting a forest, the agent meets either a teddy
bear or a grizzly bear with 50% chance and can either hug the bear or run away. (b) AvoidFuzzyBear: here,
the agent has the extra option to stay home.
Now, suppose the agent wishes to evaluate the sequence of actions {a0 = visit forest, a1 = hug}
using the average reward under the model qθ(r2 |s0, a0, a1). From Figure 1(a), we see that the correct
average reward is 0.5 × 1 + 0.5 × (-0.5) = 0.25. However, if the model has been trained on past
experience in which the agent has mostly hugged the teddy bear and ran away from the grizzly bear,
it will learn that the sequence {visit forest, hug} is associated with a reward close to 1, and that the
sequence {visit forest, run} is associated with a reward close to 0. Mathematically, the model will
learn the following conditional probability:
p(r2|s0, a0, a1) =	p(s1|s0, a0, a1)p(r2|s1, a1) =
p(s1|s0, a0)π(a1 |s1)
Ps0p(s1∣S0,ao)π(aι∣s1)双『2囱",
where s1 is the state corresponding to either teddy bear or grizzly bear. In the above expression,
p(sι∣s0, ao) and p(r2∣s1, aι) are the transition and reward dynamics of the MDP, and π(aι∣sι) is
the agent’s behavior policy that generated its past experience. As we can see, the behavior policy
affects what the model learns.
The fact that the reward model q® (r2∣s0, a0,a1) is not robust to changes in the behavior policy has
serious implications for planning. For example, suppose that instead of visiting the forest, the agent
2
Under review as a conference paper at ICLR 2020
could have chosen to stay at home as shown in Figure 1(b). In this situation, the optimal action
is to stay home as it gives a reward of 0.6, whereas visiting the forest gives at most a reward of
0.5×1+0.5 ×0 = 0.5. However, an agent that uses the above reward model to plan will overestimate
the reward of going into the forest as being close to 1 and choose the suboptimal action.1
One way to avoid this bias is to use a behavior policy that doesn’t depend on the state s1,
i.e. π(a1 |s1) = π(a1). Unfortunately, this approach does not scale well to complex environments as
it requires an enormous amount of training data for the behavior policy to explore interesting states.
A better approach is to make the model robust to changes in the behavior policy. Fundamentally,
the problem is due to causally incorrect reasoning: the model learns the observational conditional
p(r2|s0, a0, a1) instead of the interventional conditional given by:
p(r2|s0, do(a0), do(a1)) =	p(s1|s0, a0)p(r2|s1, a1),
s1
where the do-operator do(∙) means that the actions are performed independently of the unspecified
context (i.e. independently of s1). The interventional conditional is robust to changes in the policy
and is a more appropriate quantity for planning. In contrast, the observational conditional quantifies
the statistical association between the actions a0 , a1 and the reward r2 regardless of whether the
actions caused the reward or the reward caused the actions. In Section 3, we review relevant concepts
from causal reasoning, and based on them we propose solutions that address the problem.
Finally, although using p(r2|s0, do(a0), do(a1)) leads to causally correct planning, it is not opti-
mal either: it predicts a reward of 0.25 for the sequence {visit forest, hug} and 0 for the sequence
{visit forest, run}, whereas the optimal policy obtains a reward of 0.5. The optimal policy makes the
decision after observing s1 (teddy bear vs grizzly bear); it is closed-loop as opposed to open-loop.
The solution is to make the intervention at the policy level instead of the action level, as we discuss
in the following sections.
3 Background on causal reasoning
Figure 2: Illustration of various causal graphs. (a) Simple dependence without confounding. This is the
prevailing assumption in many machine-learning applications. (b) Graph with confounding. (c) Intervention on
graph (b) equivalent to setting the value of x and observing y. (d) Graph with a backdoor z blocking all paths
from u to x. (e) Graph with a frontdoor z blocking all paths from x to y. (f) Graph with a variable z blocking
the direct path from u to y.
Many applications of machine learning involve predicting a variable y (target) from a variable x
(covariate). A standard way to make such a prediction is by fitting a model qθ(y|x) to a dataset of
(x, y)-pairs. Then, if we are given a new x and the data-generation process hasn’t changed, we can
expect that a well trained qθ(y|x) will make an accurate prediction of y.
Confounding: In many situations however, we would like to use the data to make different kinds of
predictions. For example, what prediction ofy should we make, if something in the environment has
changed, or ifwe set x ourselves? In the latter case x didn’t come from the original data-generation
process. This may cause problems in our prediction, because there may be unobserved variables
u, known as confounders, that affected both x and y during the data-generation process. That is,
the actual process was of the form p(u)p(x|u)p(y|x, u) where we only observed x and y as shown
in Figure 2(b). Under this assumption, a model qθ(y|x) fitted on (x, y)-pairs will converge to the
target p(y∣x) H Jp(u)p(x∣u)p(y|x, u)du. However, if at prediction time We set X ourselves, the
actual distribution ofy will be p(y|do(x)) = p(u)p(y|x, u)du. This is because setting x ourselves
changes the original graph from Figure 2(b) to the one in Figure 2(c).
1This problem is not restricted to toy examples. In a medical domain, a model could learn that leaving the
hospital increases the probability of being healthy.
3
Under review as a conference paper at ICLR 2020
Interventions: The operation of setting x to a fixed value x0 independently of its parents, known as
the do-operator (Pearl et al., 2016), changes the data-generation process to p(u)δ(x - x0)p(y|x, u),
where δ(x - x0) is the delta-function. As explained above, this results in a different target distri-
bution p(u)p(y|x0, u)du, which we refer to as p(y|do(x = x0)), or simply p(y|do(x)) when x0
is implied. Let parj be the parents of xj . The do-operator is a particular case of the more general
concept of an intervention: given a generative process p(x) = Qj pj (xj |parj), an intervention is
defined as a change that replaces one or more factors by new factors. For example, the intervention
Pk(Xk ∣parQ → ψk (Xk ∣park) changes p(χ) top(χ) ψk(Xk∣pak). The do-operator is a “hard” interven-
tion whereby We replace a node by a delta function; that is, p(x/k, do(χk = V)) = P(X) p：(Xk-V'))，
where X/k denotes the collection of all variables except Xk.
Backdoors and frontdoors: In general, for graphs of the form of Figure 2(b), P(y|X) does not
equal P(y|do(X)). As a consequence, it is not generally possible to recover P(y|do(X)) using ob-
servational data, i.e. (X, y)-pairs sampled from P(X, y), regardless of the amount of data available
or the expressivity of the model. However, recovering P(y|do(X)) from observational data alone
becomes possible if we assume additional structure in the data-generation process. Suppose there
exists another observed variable z that blocks all paths from the confounder u to the covariate X as
shown in Figure 2(d). This variable is a particular case of the concept of a backdoor (Pearl et al.,
2016, Chapter 3.3) and is said to be a backdoor for the pair X - y. In this case, we can express
P(y|do(X)) entirely in terms of distributions that can be obtained from the observational data as:
P(y|do(X)) = Ep(z)[P(y|z,X)].	(1)
This formula holds as long as P(X|z ) > 0 and is referred to as backdoor adjustment. The same
formula applies when z blocks the effect of the confounder u on y as in Figure 2(f). More generally,
we can use P(z) and P(y|z, X) from Equation (1) to compute the marginal distribution P(y) under an
arbitrary intervention of the form p(x∣z) → ψ(x∣z) on the graph in Figure 2(b). We refer to the new
marginal as Pdo(ψ) (y) and obtain it by:
Pdo(ψ)(y) = E3(x|z)p(z)[p(y|z,x)].	(2)
A similar formula can be derived when there is a variable z blocking the effect of X on y, which is
known as a frontdoor, shown in Figure 2(e). Derivations for the backdoor and frontdoor adjustment
formulas are provided in Appendix A.
Causally correct models: Given data generated by an underlying generative process P(X), we say
that a learned model qθ(X) is causally correct with respect to a set of interventions I if the model
remains accurate after any intervention in I. That is, if qθ (X) ≈ P(X) and qθ (X) is causally correct
with respect to I, then qθ,do(ψ) (X) ≈ Pdo(ψ) (X) for all do(ψ) in I.
Backdoor-adjustment and importance sampling: Given a dataset of N tuples (zn, Xn, yn) gen-
erated from the joint distributionP(u)P(z|u)P(X|z)P(y|X, u), we could alternatively approximate the
marginal distribution Pdo(ψ)(y) after an intervention p(x∣z) → ψ(x∣z) by fitting a distribution q& (y)
to maximize the re-weighted likelihood:
L(θ) = Ep(U)P(z|u)p(x|z)p(y|x,u)[w(X,Z)log Qθ (y)] ≈ N X W(Xn,Zn)lθg Qθ 加,	(3)
where w(χ,z) = ψ(x∣z)∕p(x∣z) are the importance weights. While this solution is a mathematically
sound way of obtaining Pdo(ψ) (y), it requires re-fitting of the model for any new ψ(χ∣z). Moreover,
if ψ(x∣z) is very different from p(x∣z) the importance weights w(x, Z) will have high variance. By
fitting the conditional distribution P(y|z, X) and using Equation (2) we can avoid these limitations.
Connection to MBRL: As we will see in much greater detail in the next section, there is a direct
connection between partial models in MBRL and the causal concepts discussed above. In MBRL
we are interested in making predictions about some aspect of the future (observed frames, rewards,
etc.); these would be the dependent variables y. Such predictions are conditioned on actions which
play the role of the covariates X. When using partial models, the models will not have access to the
full state of the policy and so the policy’s state will be a confounding variable u. Any variable in
the computational graph of the policy that mediates the effect of the state in the actions will be a
backdoor with respect to the action-prediction pair.
4
Under review as a conference paper at ICLR 2020
4 Learning causally correct models
Figure 3: Graphical representations of the environment, the agent, and the various models. Circles are stochastic
nodes, rectangles are deterministic nodes. (a) Agent interacting with the environment, generating a trajectory
{yt , at}tT=0 . These trajectories are the training data for the models. (b) Same as (a) but also including the
backdoor zt in the generated trajectory. The red arrows indicate the locations of the interventions. (c) Standard
autoregressive generative model of observations. The model predicts the observation yt which it then feeds into
ht+1 . (d) Example of a Non-Causal Partial Model (NCPM) that predicts the observation yt without feeding it
into ht+1. (e) Proposed Causal Partial Model (CPM), with a backdoor zt for the actions.
We consider environments with a hidden state et and dynamics specified by an unknown transition
probability of the form p(et|et-1, at-1). At each step t, the environment receives an action at-1,
updates its state to et and produces observable data yt 〜p(yt∣et) which includes a reward r and
potentially other forms of data such as images. An agent with internal state st interacts with the
environment via actions at produced by a policy π(at |st) and updates its state using the observations
yt+1 by st+1 = fs(st, at, yt+1), where fs can for instance be implemented with an RNN. The agent
will neither observe nor model the environment state et ; it is a confounder on the data generation
process. Figure 3(a) illustrates the interaction between the agent and the environment.
Consider an agent at an arbitrary point in time and whose current state2 is s0 , and assume we
are interested in generative models that can predict the outcome3 yT of a sequence of actions
{a0, . . . , aT-1} on the environment, for an arbitrary time T. A first approach, shown in Figure 3(c),
would be to use an action-conditional autoregressive model of observations; initializing the model
state h1 to a function of (s0, a0), sample y1 fromp(.|h1), update the state h2 = fs(h1, a1, y1), sam-
ple y2 fromp(.|h2), and so on until yT is sampled. In other words, the prediction of observation yT
is conditioned on all available observations (s0, y<T) and actions a<T. This approach is for instance
found in (Oh et al., 2015).
In contrast, another approach is to predict observation yT given actions but using no observation
data beyond s0. This family of models, sometimes called models with overshoot, can for instance be
found in (Silver et al., 2017b; Oh et al., 2017; Luo et al., 2019; Guo et al., 2018; Hafner et al., 2018;
Gregor et al., 2019; Asadi et al., 2019) and is illustrated in Figure 3(d). The model deterministically
updates its state ht+1 = fh(ht, at), and generates yT fromp(.|hT). An advantage of those models
is that they can generate yT directly without generating intermediate observations.
More generally, we define a partial view vt as any function of past observations y≤t and actions a≤t.
We define a partial model as a generative model whose predictions are only conditioned on s0 , the
partial views v<t and the actions a<t: to generate yT, the agent generates v1 from p(.|h1), updates
the state to h2 = fh(h1, v1, a1), and so on, until it has computed hT and sampled yT from p(.|hT).
Both previous examples can be seen as special cases of a partial model, with Vt = yt and Vt = 0
respectively.
2We reindex time for notational simplicity, but recall that s0 is indeed a function of past observations y≤0.
3For full generality, it may be that the predicted observation is only a subset or simple function of the full
observation yT ; for instance one could predict only future rewards. For notational simplicity we make no
difference between the full observation and the prediction.
5
Under review as a conference paper at ICLR 2020
Table 1: Comparison between noncausal partial model and the proposed architecture. The shaded cells indicate
the key differences in architectures.
	NCPM architecture (overshoot)	CPM architecture
Agent	Action	at 〜 ∏(a⅛∣s⅛) State Update st+ι = RNNs (st, at, yt+1)	Backdoor	Zt	〜 m(zt∖st) Action	at	〜π(at∖zt) State Update st+i = RNNs (st, at, yt+1)
Model	State Init.	hi = g(so, a°) State Update	ht+i= RNN%(ht, at) Prediction	yt	〜p(yt∖ht)	State Init.	hi	=	g(s0,a0) Backdoor	Zt	〜p(zt∖ht) State Update	ht+i	=	RNNh(ht,	zt,at) Prediction	yt	〜p(yt∖ht)
A subtle consequence of conditioning the model only on a partial view vt is that the variables y<T
become confounders for predicting yT, in addition to the state of the environment which is always
a confounder. In Section 3 we showed that the presence of confounders makes it impossible to
correctly predict the target distribution after changes in the covariate distribution. In the context of
partial models, the covariates are the actions a<T executed by the agent and the agent’s initial state
s0, whereas the targets are the predictions yT we want to make at time T. A corollary of this is that
the learned partial model may not be robust against changes in the policy and thus cannot be used to
make predictions under different policies π, and therefore should not be used for planning.
In Section 3 we saw that if there was a variable blocking the influence of the confounders on the
covariates (a backdoor) or a variable blocking the influence of the covariates on the targets (a front-
door), it may be possible to make predictions under a broad range of interventions if we learn the
correct components from data, e.g. using the backdoor-adgustment formula in Equation (2). In gen-
eral it may not be straightforward to apply the backdoor-adjustment formula because we may not
have enough access to the graph details to know which variable is a backdoor. In reinforcement
learning however, we can fully control the agent’s graph. This means that we can choose any node
in the agent’s computational graph that is between its internal state st and the produced action at as
a backdoor variable for the actions. Given the backdoor zt, the action at is conditionally independent
of the agent state st .
To make partial models causally correct, we propose to choose the partial view vt to be equal
to the backdoor zt . This allows us to learn all components we need to make predictions under an
arbitrary new policy. In the rest of this paper we will refer to such models as Causal Partial Models
(CPM), and all other partial models will be henceforth referred to as Non-Causal Partial Models
(NCPM). We assume the backdoor zt is sampled from a distribution m(zt|st) and the policy is a
distribution conditioned on zt, π(at∣zt). This is illustrated in Figure 3(b) and described in more
details in Table 1(right). We can perform a simulation under a new policy ψ(ajht,zt) by directly
applying the backdoor-adjustment formula, Equation (1), to the RL graph as follows:
pdo(ψ(at |ht ,zt)) (yt+1 |ht ) = Eψ(at |ht ,zt)p(zt |ht) [p(yt+1 |ht+1 )],	(4)
where the components p(zt|ht) and p(yt+1|ht+1) with ht+1 = fh(ht, zt, at) can be learned from
observational data produced by the agent.
Modern deep-learning agents (e.g. as in Espeholt et al. (2018); Gregor et al. (2019); Ha & Schmidhu-
ber (2018)) have complex graphs, which means that there are many possible choices for the backdoor
zt . So an important question is: what are the simplest choices of zt? Below we list a few of the
simplest choices we can use and discuss their advantages and trade-offs; more choices for zt are
listed in Appendix C.
Agent state: Identifying zt with the agent’s state st can be very informative about the future, but
this comes at a cost. As part of the generative model, we have to learn the componentp(zt|ht). This
may be difficult in practice when zt = st due to the high-dimensionality ofst, hence and performing
simulations would be computationally expensive.
6
Under review as a conference paper at ICLR 2020
Policy probabilities: The zt can be the vector of probabilities produced by a policy when we have
discrete actions. The vector of probabilities is informative about the underlying state, if different
states produce different probabilities.
Intended action: The zt can be the intended action before using some form of exploration, e.g. ε-
greedy exploration. This is an interesting choice when the actions are discrete, as it is simple to
model and, when doing planning, results in a low branching factor which is independent of the
complexity of the environment (e.g. in 3D, visually rich environments).
The causal correction methods presented in this section can be applied to any partial model. In our
experiments, we will focus on environment models of the form proposed by Gregor et al. (2019).
These models consist ofa deterministic “backbone” RNN that integrates actions and other contextual
information. The states of this RNN are then used to condition a generative model of the observed
data yt, but the observations are not fed back to the model autoregressively, as shown in Table 1(left).
This corresponds to learning a model of the formp(yt|s0, a0, . . . , at-1).
We will compare this against our proposed model, which allows us to simulate the outcome of any
policy using Equation (4). In this setup, a policy network produces zt before an action at . For
example, if the zt is the intended action before ε-exploration, zt will be sampled from a policy
m(zt|st) and the executed action at will then be sampled from an ε-exploration policy ∏(at∣zt)=
(1 - ε)δz5at + εn1, where n is the number of actions and ε is in (0,1). Acting with the sampled
actions is diagrammed in Figure 3(b) and the mathematical description is provided in Table 1.
The model components p(zt|ht) and p(yt|ht) are trained via maximum likelihood on observational
data collected by the agent. The partial model does not need to model all parts of the yt observation.
For example, a model to be used for planning can model just the reward and the expected return.
In any case, it is imperative that we use some form of exploration to ensure that π(at |zt) > 0 for
all at and zt as this is a necessary to allow the model to learn the effects of the actions. The model
usage is summarized in Algorithms 1 and 2 in Appendix D and we discuss the model properties in
Appendix E.
5	Experiments
We analyse the effect of the proposed corrections on a variety of models and environments. When the
enviroment is an MDP, such as the FuzzyBear MDP from Section 2, we can compute exactly both the
non-causal and the causal model directly from the MDP transition matrix and the behavior policy. In
Section 5.1, we compare the optimal policies computed from the non-causal and the causal model via
value iteration. For this analysis, we used the intended-action backdoor, since it’s compatible with
a tabular representation. In Section 5.2, we repeat the analysis using a learned model instead. For
these experiments, we used the policy-probabilities backdoor. The optimal policies corresponding
to a given model were computed using a variant of the Dyna algorithm (Sutton, 1991) or expectimax
(Michie, 1966). Finally in Section 5.3, we provide an analysis of the model rollouts in a visually
rich 3D environment.
5.1	Value-iteration analysis on MDPs
Given an MDP and a behavior policy ∏, the optimal values VMM(∏) of planning based on a NCPM
and CPM are derived in Appendix I. The theoretical analysis of the MDP does not use empirically
trained models from the policy data, but rather assumes that the transition probabilities of the MDP
and the policy from which training data are collected are accurately learned by the model. This
allows us to isolate the quality of planning using the model from how accurate the model is.
Optimal behavior policy: The optimal policy of the FuzzyBear MDP (Figure 1(a)) is to always
hug the teddy bear and run away from the grizzly bear. Using training data from this behavior
policy, we show in Figure 7 (Appendix I) the difference in the optimal planning based on the NCPM
(Figure 3(d)) and CPM with the backdoor zt being the intended action (Figure 3(e)). Learning from
optimal policies with ε-exploration, the converged causal model is independent of the exploration
parameter ε. We see effects of varying ε on learned models in Figure 8 (Appendix I).
7
Under review as a conference paper at ICLR 2020
(b)
(a)
Figure 4: MDP Analysis: (a) In the FuzzyBear environment, we randomly generate 500 policies and scatter
plot them with x-axis showing the quality of the behavior policy Veπnv and y-axis showing corresponding model
optimal evaluations VMM(π). For each policy, We derive the corresponding converged model M(π) equivalent
to training on data generated by the policy. We then compute the optimal evaluation VMM(n)using this model.
We contrast the unrealistic optimism of the non-causal model evaluations VNCPM(n)With the more realistic
causal model evaluations VCPM(π) for good policies π, as well as the over-pessimism of the non-causal model
compared to the causal model for bad policies. (b) Same plot as (a) but for the AvoidFuzzyBear environment.
Sub-optimal behavior policies: We empirically shoW the difference betWeen the causal and non-
causal models When learning from randomly generated policies. For each policy, We derive the
corresponding converged model M(π) using training data generated by the policy. We then compute
the optimal value of VMM(π) USing this model. On FuzzyBear (Figure 4(a)), we see that the causal
model alWays produces a value greater than or equal to the value of the behavior policy. The value
estimated by the causal model can always be achieved in the real environment. If the behavior
policy was already good, the simulation policy used inside the model can reproduce the behavior
policy by respecting the intended action. If the behavior policy is random, the intended action is
uninformative about the underlying state, so the simulation policy has to choose the most rewarding
action, independently of the state. And if the behavior policy is bad, the simulation policy can
choose the opposite of the intended action. This allows to find a very good simulation policy, when
the behavior policy is very bad. To further improve the policy, the search for better policies should
be done also in state s1. And the model can then be retrained on data from the improved policies.
If we look at the non-causal model, we see that it displays the unfortunate property of becoming
more unrealistically optimistic as the behavior policy becomes better. Similarly, the worse the pol-
icy is, i.e. the lower Veπnv is, the non-causal model becomes less able to improve the policy. On
AvoidFuzzyBear (Figure 4(b)), the optimal policy is to stay at home. Learning from data generated
by random policies, the causal model indeed always prefers to stay home with any sampled inten-
tions, resulting in a constant evaluation for all policies. On the other hand, the non-causal model
gives varied, overly-optimistic evaluations, while choosing the wrong action (visit forest).
5.2	Learned Models on MDPs
We previously analyzed the case where the transition probabilities and theoretically optimal policy
are known. We will now describe experiments with learned models trained by gradient descent,
using the same training setup as described in Section 4.
AvoidFuzzyBear with Dyna: In this experiment we demonstrate that we can learn the optimal
policy purely from off-policy experience using a general n-step-return algorithm derived from a
causal model. The algorithm is described in detail in Appendix F. In short, we simulate experiences
from the partial model, and use policy gradient to learn the optimal policy on these experiences as if
they were real experiences (this is possible since the policy gradient only needs action probabilities,
values, predicted rewards and ends of episodes). We compare a non-causal model and a causal
8
Under review as a conference paper at ICLR 2020
model where the backdoor zt is the intended action. For the environment we use AvoidFuzzyBear
(Figure 1(b)). We collect experiences that are sub-optimal: half the time the agent visits the forest
and half the time it stays home, but once in the forest it acts optimally with probability 0.9. This
is meant to simulate situations either where the agent has not yet learned the optimal policy but is
acting reasonably, or where it is acting with a different objective (such as exploration or intrinsic
reward), but would like to derive the optimal policy. We expect the non-causal model to choose
the sub-optimal policy of visiting the forest, since the sequence of actions of visiting the forest and
hugging typically yields high reward.
This is what we indeed find, as shown in Figure 5(a). We see that the non-causal model indeed
achieves a sub-optimal reward (less than 0.6), but believes that it will achieve a high reward (more
than 0.6). On the other hand, the causal model achieves the optimal reward and correctly predicts
that it will achieve the corresponding value.
training iterations
(a)
Figure 5: (a) Dyna on AvoidFuzzyBear. Models were trained on a sub-optimal behavior policy that explores
both parts of the environment, and the evaluation policy was trained purely inside the model. The non-causal
model (dotted lines) achieved sub-optimal reward, while expecting large reward. The causal model (solid lines)
achieved optimal reward and had a correct expectation of the reward. The shaded area indicates 95% confidence
intervals from 50 runs. (b) Models solving the AvoidFuzzyBear MDP with expectimax. The non-causal model
misled the agent when using search depth 3 or higher.
	Search		depth
Model	1	2	3
Non-causal	/	/	X
Intended action	/	/	/
Clustered probs	/	/	/
Clustered obs	/	/	/
(b)
AvoidFuzzyBear with Expectimax: In this experiment, we used the classical expectimax search
(Michie, 1966; Russell & Norvig, 2009). On the simple AvoidFuzzyBear MDP, it is enough to use
a search depth of 3: a decision node, a chance node and a decision node. The behavior policy was
progressively improving as the model was trained.
In Figure 5(b), we see the results for the different models. Only the non-causal model was not able to
solve the task. Planning with the non-causal model consistently preferred the stochastic path with the
fuzzy bear, as predicted by our theoretical analysis with value iteration. The models with clustered
probabilities and clustered observations approximate modeling of the probabilities or observations.
These models are described in Appendix H.
5.3	Visually rich 3D Environment
The setup for these experiments is similar to Gregor et al. (2019), where the agent is trained using the
IMPALA algorithm (Espeholt et al., 2018), and the model is trained alongside the agent via ELBO
optimization on the data collected by the agent. The architecture of the agent and model is based
on Gregor et al. (2019) and follows the description in Table 1(right). For these experiments, the
backdoor zt was chosen to be the policy probabilities, and p(zt|ht) was parametrized as a mixture
of Dirichlet distributions. See Appendix J for more details. We demonstrate the effect of the causal
correction on the 3D T-Maze environment where an agent walks around in a 3D world with the
goal of collecting the reward blocks (food). The layout of this environment is shown in Figure 6(a).
From our previous results, we expect NCPMs to be unrealistically optimistic. This is indeed what
we see in Figure 6(b). Compared to NCPM, CPM with generated z generates food at the end of a
rollout with around 50% chance, as expected given that the environment randomly places the food
on either side. In Figure 6(c) and Figure 6(d, left) we show subsets of rollouts from NCPM and
CPM respectively (See Figures 10-12 in Appendix for full rollouts).
9
Under review as a conference paper at ICLR 2020
Environment
fixed colors
food
random agent (reward)
colors	J /
NCPM, forced action
(a)
CPM, forced action + generated z
Figure 6: Causal partial model (CPM) and Non-causal partial model (NCPM) rollouts in a 3D T-Maze environ-
ment. (a) The agent always begins in a walled-off corridor, and can obtain food reward that spawns randomly
on either the left (red) or the right (black) side. The colors of the corridor and side walls are randomized every
episode. (b) Probability of the model generating food in rollouts for NCPM and CPM. Error bars represent
standard error over 5 runs, each with 30 episodes. (c)-(d) Subset of frames from example rollouts using (c)
NCPM and (d) CPM. In all rollouts depicted, the top row shows the real frames observed by an agent following
a fixed policy (Ground Truth, GT). Bottom 5 rows indicate model rollouts, conditioned on 3 previous frames
without revealing the location of the food. CPM and NCPM differ in their state-update formula and action
generation (See Table 1), but frame generation yt 〜p(yt∣ht) is the same for both, as introduced in Gregor et al.
(2019). For CPM, we compare rollouts with forced actions and generated z to rollouts with forced actions and
forced z from the ground truth. We can observe that rollouts with the generated z (left) respect the randomness
in the food placement (with and without food), while the rollouts with forced z (right) consistently generate
food blocks, if following actions consistent with the backdoor z from the well-trained ground truth policy.
6	Conclusion
We have characterized and explained some of the issues of partial models in terms of causal reason-
ing. We proposed a simple, yet effective, modification to partial models so that they can still make
correct predictions under changes in the behavior policy, which we validated theoretically and exper-
imentally. The proposed modifications address the correctness of the model against policy changes,
but don’t address the correctness/robustness against other types of intervention in the environment.
We will explore these aspects in future work.
Time
(d)
=no=o≈
CPM, forced action + forced z
Time
10
Under review as a conference paper at ICLR 2020
References
Brandon Amos, Laurent Dinh, Serkan Cabi, Thomas RothorL Sergio Gomez Colmenarejo, Alistair
Muldal, Tom Erez, Yuval Tassa, Nando de Freitas, and Misha Denil. Learning awareness models.
arXiv preprint arXiv:1804.06318, 2018.
Kavosh Asadi, Dipendra Misra, Seungchan Kim, and Michel L. Littman. Combating the
compounding-error problem with a multi-step model. arXiv preprint arXiv:1905.13320, 2019.
Elias Bareinboim, Andrew Forney, and Judea Pearl. Bandits with unobserved confounders: A causal
approach. In Advances in Neural Information Processing Systems 28, pp. 1342-1350, 2015.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference
learning. Machine learning, 22(1-3):33-57, 1996.
Silvia Chiappa, Sebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment
simulators. arXiv preprint arXiv:1704.02254, 2017.
Marc P. Deisenroth and Carl E. Rasmussen. PILCO: A model-based and data-efficient approach to
policy search. In International Conference of Machine Learning, 2011.
Carlos Diuk, Andre Cohen, and Michael L. Littman. An object-oriented representation for efficient
reinforcement learning. In Proceedings of the 25th international conference on Machine learning,
pp. 240-247, 2008.
Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual fore-
sight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint
arXiv:1812.00568, 2018.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. IMPALA: Scalable distributed deep-RL with
importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In IEEE Inter-
national Conference on Robotics and Automation, pp. 2786-2793, 2017.
Andrew Forney, Judea Pearl, and Elias Bareinboim. Counterfactual data-fusion for online reinforce-
ment learners. In Proceedings of the 34th International Conference on Machine Learning, pp.
1156-1164, 2017.
Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron
van den Oord. Shaping belief states with generative environment models for RL. arXiv preprint
arXiv:1906.09237, 2019.
Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A. Pires, Toby Pohlen, and
Remi Munos. Neural predictive belief representations. arXiv preprint arXiv:1811.06407, 2018.
David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018.
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learn-
ing continuous control policies by stochastic value gradients. In Advances in Neural Information
Processing Systems, pp. 2944-2952, 2015.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational
reinforcement learning for POMDPs. arXiv preprint arXiv:1806.02426, 2018.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. Advances in Neural Information Processing Systems, 2019.
11
Under review as a conference paper at ICLR 2020
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
Sanghack Lee and Elias Bareinboim. Structural causal bandits: Where to intervene? In Advances
in Neural Information Processing Systems 31 ,pp. 2568-2578. 2018.
Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, and Ji He. Recurrent
reinforcement learning: a hybrid approach. arXiv preprint arXiv:1509.03044, 2015.
Long Lin and Tom Mitchell. Memory approaches to reinforcement learning in non-Markovian
domains. Technical report, Carnegie Mellon University, 1992.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna-
tional Conference on Learning Representations, 2019.
Donald Michie. Game-playing and game-learning automata. In Advances in programming and
non-numerical computation, pp. 183-200. Elsevier, 1966.
Paul Munro. A dual back-propagation scheme for scalar reward learning. In 9th Annual Conference
of the Cognitive Science Society, pp. 165-176, 1987.
Derrick Nguyen and Bernard Widrow. The truck backer-upper: An example of self-learning in
neural networks. In Advanced neural computers, pp. 11-19. Elsevier, 1990.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder Singh. Action-conditional
video prediction using deep networks in atari games. In Advances in neural information process-
ing systems, pp. 2863-2871, 2015.
Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural
Information Processing Systems, pp. 6118-6128, 2017.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pp. 16-17, 2017.
Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. Causal inference in statistics: A primer.
John Wiley & Sons, 2016.
Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall Press,
Upper Saddle River, NJ, USA, 3rd edition, 2009. ISBN 0136042597, 9780136042594.
Jurgen Schmidhuber. Curious model-building control systems. In Proceedings 1991 IEEE Interna-
tional Joint Conference on Neural Networks, pp. 1458-1463, 1991.
JUrgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE
Transactions on Autonomous Mental Development, 2(3):230-247, 2010.
David Silver, Richard S. Sutton, and Martin Muller. Sample-based learning and search with per-
manent and transient memories. In Proceedings of the 25th international conference on Machine
learning, pp. 968-975, 2008.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lawrence R. Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lill-
icrap, Fan Fong Celine Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis
Hassabis. Mastering the game ofgo without human knowledge. Nature, 550:354-359, 2017a.
David Silver, Hado P. van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel
Dulac-Arnold, David P. Reichert, Neil C. Rabinowitz, Andre Barreto, and Thomas Degris. The
predictron: End-to-end learning and planning. In International Conference on Machine Learning,
2017b.
12
Under review as a conference paper at ICLR 2020
Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approxi-
mating dynamic programming. In Machine Learning Proceedings 1990, pp. 216-224. Elsevier,
1990.
Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
Bulletin, 2(4):160-163, 1991.
TheoPhane Weber, Sebastien Racaniere, David P. Reichert, Lars Buesing, Arthur Guez,
Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia
Li, et al. Imagination-augmented agents for deep reinforcement learning. arXiv preprint
arXiv:1707.06203, 2017.
Paul J. Werbos. Learning how the world works: Specifications for predictive networks in robots and
brains. In Proceedings of IEEE International Conference on Systems, Man and Cybernetics, NY,
1987.
Chris Xie, Sachin Patil, Teodor Moldovan, Sergey Levine, and Pieter Abbeel. Model-based rein-
forcement learning with parametrized physical models and optimism-driven exploration. In 2016
IEEE international conference on robotics and automation, pp. 504-511, 2016.
Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandits: A causal approach.
In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 1340-
1346, 2017.
Shengyu Zhu and Zhitang Chen. Causal discovery with reinforcement learning. arXiv preprint
arXi:1906.04477v, 2019.
13
Under review as a conference paper at ICLR 2020
A Backdoor and frontdoor adjustment formulas
Starting from a data-generation process of the form illustrated in Figure 2(b), p(x, y, u) =
p(u)p(x|u)p(y|x, u), we can use the do-operator to compute p(y|do(x)) = p(u)p(y|x, u)du.
Without assuming any extra structure in p(x|u) or in p(y|x, u) it is not possible to compute
p(y|do(x)) from the knowledge of the joint p(x, y) alone.
If there was a variable z blocking all the effects of u on x, as illustrated in Figure 2(d), then
p(y|do(x)) can be derived as follows:
Joint density	p(u)p(z|u)p(x|z)p(y|x, u)	(5)
Intervention	p(x|z) → ψ(x)	(6)
Joint after intervention	p(u)p(z∣u)ψ(x)p(y∣x, u)	(7)
Conditioning the new joint	/ H /、、	J,p(u)P(zlu)ψ(χ)P(ylx,u)dudz p(y|do(X))= 	^x		(8)
	=	p(z)p(y|x, z)dz	(9)
	= Ep(z)[p(y|x, z)],	(10)
where we used the formula		
p(u)p(z|u)p(y|x, u)du = p(z) p(u|z)p(y|x, u)du		(11)
	= p(z)p(y|x, z).	(12)
If instead of just fixing the value of x, We perform a more general intervention p(x|z) → ψ(χ∣z),
then Pdo(ψ(χ∣z)) (y) can be derived as follows:
Joint density
Intervention
Joint after intervention
New marginal
p(u)p(z|u)p(x|z)p(y|x, u)	(13)
p(x∣z) → ψ(x∣z)	(14)
p(u)p(z∣u)Ψ(χ∣z)p(y∣χ,u)	(15)
Pdo(ψ(x∣z))(y) = / p(u)p(z∣u)ψ(x∣z)p(y∣x, u)dudzdx
(16)
Jp(z)ψ(x∣z)p(y∣x, z)dzdx	(17)
Ep(z)ψ(χ∣z)[p(ylx,z)].	(18)
Applying the same reasoning to the graph shown in Figure 2(e), we obtain the formula
p(y|do(x)) = Ep(z|x)[p(y|do(z))] = Ep(z|x)p(x0) [p(y|x0, z)],	(19)
where p(z|x), p(x0) and p(y|x0, z) can be directly measured from the available (x, y, z) data. This
formula holds as long as p(z|x) > 0, ∀x, z and it is a simple instance of frontdoor adjustment (Pearl
et al., 2016).
B Derivation of causal correctness for the models in Figure 3
Here, we will show in more detail that the models (c) and (e) in Figure 3 are causally correct,
whereas model (d) is causally incorrect. Specifically, we will show that given an initial state s0 and
after setting the actions a0 to aT to specific values, models (c) and (e) make the same prediction
about the future observation yT +1 as performing the intervention in the real world, whereas model
(d) does not.
14
Under review as a conference paper at ICLR 2020
Model (c) Using the do-operator, a hard intervention in the model is given by:
T+1
qθ(yτ +1∣S0,do(a0:T)) = qθ(yτ +ι∣so,αo:T) = / ɪɪ qθ(yt∣ht) dyi：T,	(20)
t=1
where ht is a deterministic function of s0, a0:t-1 and y1:t-1. The same hard intervention in the real
world is given by:
p(yT +1 |s0, do(a0:T)) =	p(y1:T+1|s0, do(a0:T)) dy1:T
T+1
=	p(yt|s0, do(a0:T), y1:t-1) dy1:T
t=1
T+1
=	p(yt|s0, a0:t-1, y1:t-1) dy1:T.
t=1
(21)
(22)
(23)
If the model is trained perfectly, the factors qθ (yt∣ht) will become equal to the conditionals
p(yt|s0, a0:t-1, y1:t-1). Hence, an intervention in a perfectly trained model makes the same pre-
diction as in the real world, which means that the model is causally correct.
Model (d) The interventional conditional in the model is simply:
qθ(yτ +1∣S0, do(a0:T)) = qθ(yτ +ι∣so,αo:T) = qθ(yτ +ι∣hτ +1),	(24)
where hT+1 is a deterministic function of s0 and a0:T. In a perfectly trained model, we have that
qθ (yT+1 |hT+1) = p(yT+1|s0, a0:T). However, the observational conditional p(yT +1 |s0, a0:T) is
not generally equal to the inverventional conditional p(yT +1 |s0, do(a0:T)), which means that the
model is causally incorrect.
Model (e) Finally, the interventional conditional in this model is:
qθ(yT +1∣S0, do(a0:T)) = qθ(yT+1∣S0, a。：T) = / qθ(yT+i|hT +1) Y qθ(zt∣ht) dzi：T,	(25)
where ht is a deterministic function of s0, a0:t-1 and z1:t-1. The same intervention in the real world
can be written as follows:
p(yT +1 |s0, do(a0:T)) =	p(yT +1 |s0, do(a0:T), z1:T)p(z1:T|s0, do(a0:T)) dz1:T	(26)
T
=	p(yT+1|s0, do(a0：T), z1：T)	p(zt|s0, do(a0：T), z1：t-1) dz1：T (27)
t=1
T
=	p(yT+1|s0, a0：T, z1：T)	p(zt|s0, a0：t-1, z1：t-1) dz1：T.	(28)
t=1
In a perfectly trained model, We have that q§ (yT +ι∖hr +i) = p(yT +i |s。，a。： T ,zi：T) and q§ (zt∣ht)=
p(zt|s0, a0：t-1, z1：t-1). That means that the intervention in a perfectly trained model makes the
same prediction as the same intervention in the real world, hence the model is causally correct.
C ADDITIONAL CHOICES OF THE BACKDOOR zt
The first alternative backdoor we consider is the empty backdoor:
Empty backdoor Zt = 0: This backdoor is in general not appropriate; it is however appropriate
when the behavior policy does in fact depend on no information, i.e. is not a function of the state
15
Under review as a conference paper at ICLR 2020
st . For example, the policy can be uniformly random (or any non-state dependent distribution over
actions). This severely limits the behavior policy. Because the backdoor contains no information
about the observations, the simulations are open-loop, i.e. we can only consider plans which consist
of a sequence of fixed actions, not policies.
An intermediate layer: In principle, the zt can be any layer from the policy. To model the layer with
a p(zt |ht ) distribution, we would need to know the needed numerical precision of the considered
layer. For example, a quantized layer can be modeled by a discrete distribution. Alternatively, if
the layer is produced by a variational encoder or variational information bottleneck, we can train
p(zt|ht) to minimize the KL(pencoder (zt |st) k p(zt|ht)).
Finally, if a backdoor is appropriate, we can combine it with additional information:
Combinations: It is possible to combine a layer with information from other layers. For example,
the intended action can be combined with extra bits from the input layer. Such zt can be more
informative. For example, the extra bits can hold a downsampled and quantized version of the input
layer.
D	Algorithms for training and simulating from the model
Algorithms 1 and 2 describe how the model is trained and used to simulate trajectories. The algo-
rithm for training assumes a distributed actor-learner setup (Espeholt et al., 2018).
Algorithm 1 Model training
Data collection on an actor:
For each step:
Zt 〜m(zt∣st) ... sample the backdoor (e.g., the partial view with the intended action)
at 〜∏(at∣zt) ... sample the executed action (e.g., add ε-exploration)
Collect:
st . . . agent state
zt . . . partial view
at . . . executed action
yt+1 . . . targets (rewards, returns, . . . )
Model training on a learner:
Require a trajectory: s0, a<T , z<T , y≤T
h1 = g(s0, a0) . . . initialize the model state
For each trajectory step:
Train p(yt |ht) to model yt.
Train p(zt|ht) to model zt.
ht+1 = RNNh(ht, zt, at) . . . update the model state
Algorithm 2 Using the model to generate a simulation under a new policy ψ
Require an agent state: s0
ao = ψ(ao |so) ... choose the first action
h1 = g(s0, a0) . . . initialize the model state
For each trajectory step:
Predict the wanted targets p(yt|ht) (e.g., rewards, returns, . . . ).
Zt 〜p(zt∣ht) ... sample the partial view
at 〜 ψ(at∣ht,zt) ... choose the next action
ht+1 = RNNh(ht, Zt, at) . . . update the model state
E	Discussion of model properties
Table 2 provides an overview of properties of autoregressive models, deterministic non-causal mod-
els and the causal partial models. The causal partial models have to generate only a partial view. The
16
Under review as a conference paper at ICLR 2020
partial view can be small and easy to model. For example, a partial view with the discrete intended
action can be flexibly modeled by a categorical distribution.
The causal partial models are fast and causally correct in stochastic environments. The causal partial
models have a low simulation variance, because they do not need to model and generate unimportant
background distractors. If the environment has deterministic regions, the model can quickly learn to
ignore the small partial view and collect information only from the executed action.
It is interesting that the causal partial models are invariant of the ∏(at∣zt) distribution. For example,
if the partial view zt is the intended action, the optimally learned model would be invariant of the
used ε-exploration: ∏(at∣zt). Analogously, the autoregressive models are invariant of the whole pol-
icy ∏(ajst). This allows the autoregressive models to evaluate any other policy inside of the model.
The causal partial model can run inside the simulation only policies conditioned on the starting state
s0, the actions a<t and the partial views z≤t. Ifwe want to evaluate a policy conditioned on different
features, we can collect trajectories from the policy and retrain the model. The model can always
evaluate the policy used to produce the training data. We can also improve the policy, because the
model allows to estimate the return for an initial (s0, a0) pair, so the model can be used as a critic
for a policy improvement.
Table 2: Models and their properties.
	Autoregressive	Deterministic	Causal Partial Model
Generates	observation	nothing	partial view: Zt
Speed	slow	fast	fast
Causally correct	always	in deterministic environments or with on-policy simulations	always
Simulation variance	high (distracted)	lowest	low
Extra branching	huge	0	controlled by Zt size
Invariant of	∏(at∣st)		-	∏(at∣zt)	
Evaluable policies	any	ψ(at∣S0,a<t)		ψ(at∣S0,a<t,z≤t)
Training	once	iterative with policy	iterative with policy
F	Dyna-style policy-gradient algorithm
In this section we derive an algorithm for learning an optimal policy given a (non-optimal) expe-
rience that utilizes n-step returns from partial models presented in this paper. In general, a model
of the environment can be used in a number of ways for reinforcement learning. In Dyna (Sutton,
1990), we sample experiences from the model, and apply a model-free algorithm (Q-learning in
the original implementation, but more generally we could consider SARSA or policy gradient) as if
these were real experiences. In Dyna-2 (Silver et al., 2008), the same process is applied but in the
context the agent is currently in—starting the simulations from the current state—and adapting the
policy locally (for example through separate fast weights). In MCTS, the model is used to build a
tree of possibilities. Can we apply our model directly in these scenarios? While we don’t have a full
model of the environment, we can produce a causally correct simulation of rewards and values; one
that should generalize to policies different from those the agent was trained on. Policy probabilities,
values, rewards and ends of episodes are the only variables that the above RL algorithms need.
Here we propose a specific implementation of Dyna-style policy-gradient algorithm based on the
models discussed in the paper. This is meant as a proof of principle, and more exploration is left for
future work.
As the agent sees an observation yt+1, it forms an internal agent state st from this observation and
the previous agent state: st+1 = RNNs(st, at, yt+1). The agent state in our implementation is the
state of the recurrent network, typically LSTM (Hochreiter & Schmidhuber, 1997). Next, let us
assume that at some point in time with state s0 the agent would like to learn to do a simulation from
the model. Let ht be the state of the simulation at time t. The agent first sets h1 = g(s0, a0) and
proceeds with n-steps of the simulation recurrent network update ht+1 = RNN(ht, zt, at). The
agent learns themodelp(zt|ht) which it can use to simulate forward. We assume that the model was
trained on some (non-optimal) policy/experience. We would like to derive an optimal policy and
17
Under review as a conference paper at ICLR 2020
value function. Since these need to be used during acting (if the agent were to then act optimally
in the real environment), they are functions of the agent state st： ∏(at∣st), V (st). Now in general,
ht 6= st but we would like to use the simulation to train an optimal policy and value function. Thus
We define a second pair of functions ∏h(at∣ht, zt), Vh(ht, zt). Here the extra zt's are needed, since
the ht has seen z’s only up to point zt-1.
Next we are going to train these functions using policy gradients on simulated experiences. We start
with some state st and produce a simulation ht+1, . . . , hT by sampling zt from the model at each
step and action at 〜∏h(at∣ht, zt). However at the initial point t, we sample from π, not ∏h, and
compute the value V, not Vh. Sequence of actions, values and policy parameters are the quantities
needed to compute a policy gradient update. We use this update to train all these quantities.
There is one last element that the algorithm needs. The values and policy parameters are trained
at the start state and along the simulation by n-step returns, computed from simulated rewards and
the bootstrap value at the end of the simulation. However this last value is not trained in any way
because it depends on the simulated state Vh(hT ) not the agent state sT . We would like this value
to equal to what the agent state would produce： V(sT). Thus, during training of the model, we also
train Vh(hT) to be close to V(sT) by imposing an L2 penalty. In our implementation, we actually
impose a penalty at every point t during simulation but we haven’t experimented with which choice
is better.
G Simple extensions
Variance reduction. To reduce the variance of a simulation, it is possible to sample the zt from a
proposal distribution q(zt|ht). The correct expectation can be still recovered by using an importance
P(ZtIht)
q(ZtIht)
weight： w
Data efficient training. Usually, we know the distribution of the used partial view: Zt 〜m(zt∣st).
When training the p(zt|ht) model, we can then minimize the exact KL(m(Zt|st) k p(Zt|ht)).
H	Models trained by clustering
When using a tree-search, we want to have a small branching factor at the chance nodes. A good
zt variable would be discrete with a small number of categories. This is satisfied, if the zt is the
intended action and the number of the possible actions is small. We do not have such compact
discrete zt, if using as zt the observation, the policy probabilities or some other modeled layer.
Here, we will present a model that approximates such causal partial models. The idea is to cluster
the modeled layers and use just the cluster index as zt . The cluster index is discrete and we can
control the branching factor by choosing the the number of clusters.
Concretely, let’s call the modeled layer xt. We will model the layer with a mixture of components.
The mixture gives us a discrete latent variable zt to represent the component index. To train the
mixture, we use a clustering loss to train only the best component to model the xt, given ht and zt :
Lclustering = min(-βclustering logp(zt∣h) - logP(Xt|ht,Zt))	(29)
Zt
where p(zt|ht) is a model of the categorical component index and βclustering ∈ (0, 1) is a hyper-
parameter to encourage moving the information bits to the latent zt . During training, we use the
index of the best component as the inferred zt . In theory, a better inference can be obtained by
smoothing.
In contrast to training by maximum likelihood, the clustering loss uses just the needed number of
the mixture components. This helps to reduce the branching factor in a search.
In general, the cluster index is not guaranteed to be sufficient as a backdoor, if the reconstruction
loss - logp(xt |ht, zt) is not zero. For example, ifxt is the next observation, the number of mixture
components may need to be unrealistically large, if the observation can contains many distractors.
18
Under review as a conference paper at ICLR 2020
I Value-iteration analysis on MDPs
I.1	Optimal Value Derivations
We derive the following two model-based evaluation metrics for the MDP environments.
•	VNCPM(∏)(so): optimal value computed With the non-causal model, which is trained with
training data from policy π, starting from state s0.
•	VCPM(∏) (so): optimal value computed with the causal model, which is trained with training
data from policy π, starting from state s0 .
The theoretical analysis of the MDP does not use empirically trained models from the policy data but
rather assumes that the transition probabilities p(si+1 | si, ai) of the MDP, and the policy, π(ai | si)
or π(zi | si), from which training data are collected are accurately learned by the model.
Computation of VNCPM⑺： For the non-causal model,
k
VNCPM(π)(so) = max EEsi [ri+ι(si,ai) | so,ao,aι, ...,ai]
a0,...,ak
i=0
k
= max	p(si | s0, a0, . . . , ai)ri+1(si,ai).
a0,...,ak
i=0 si
Notice that the probability of si is affected by ai here, because the network gets ai as an input,
when predicting the ri+1 . This will introduce the non-causal bias. The network implements the
expectation implicitly by learning the mean of the reward seen in the training data. We can compute
the expectation exactly, if we know the MDP. The p(si | s0, a0, . . . , ai) can be computed recursively
in two-steps as:
p(si | s0, a0, . . . , ai)
P(Si | S0,ao,...,ai-i)∏(ai | Si)
s0 p(s0i | s0, a0, . . . , ai-1)π(ai | s0i)
i
(30)
Here, we see the dependency of the learned model on the policy π. The remaining terms can be
expressed as:
p(Si	| S0, a0, .	. . , ai-1)	=	p(Si, Si-1 | S0, a0,	.	. . , ai-1)	(31)
si-1
=	p(Si-1 | S0, a0, . . .	,	ai-1)p(Si | Si-1,	ai-1).	(32)
si-1
Denoting p(Si | S0, a0, . . . , aj) by Si,j, we have the two-step recursion
S _	Si,i-1 ∏(ai | Si)
L P S0i-1 ∏3 I Si),
i,
Si,i-1 =	Si-1,i-1 p(Si | Si-1 , ai-1 )
si-1
(33)
(34)
with S1,0 = p(sι ∣ so, ao). We then compute Vcm(S0) as max。。,…a Pk=o Psi Si,iri+ι(si, ai).
19
Under review as a conference paper at ICLR 2020
Computation of VCPM⑺ ： For the causal model,
VCPM(∏)(so) =maaxɪ2p(zi I so,ao)maaxɪ2P(z2 I So,ao,z1,a1)… 0 z1	1 z2	(35)
max	p(zk I so, ao, z1, a1, . . . , zk-1, ak-1) ak-1 zk	(36)
k max	E[ri+1(si,ai) I so,ao,z1,a1, . . .,ai)], ak k i=o	(37)
where for any i ∈ [1, k],	
p(zi	I so,	ao,	z1,	a1,	. . . ,	zi-1,	ai-1) =	p(si,	zi	I so,	ao, z1,	a1, . . . ,	zi-1, ai-1) si	(38)
=	p(si I so, ao, z1 . . . ,zi-1,ai-1)π(zi I si). s si	(39)
	(40)
Denoting p(si I so, ao, z1 . . . , zi-1, ai-1) by Zi, we have	
Zi =	p(si-1, si I so, ao, z1 . . . , zi-1, ai-1) si-1	(41)
=	p(si I si-1, ai-1)p(si-1 I so, ao, z1 . . . , zi-1, ai-1) si-1	(42)
=	p(si I si-1, ai-1)p(si-1 I so, ao, z1 . . . , zi-1), si-1	(43)
where we used the fact that si-1 is independent of ai-1, given zi-1. Furthermore,
p(si-1, zi-1 | s0, a0, z1 . . . , ai-2)
p(si-ι | so, a0,z1 ..., Zi-ι)= ——;------1------------------ʌ—	(44)
p(zi-1 | s0, a0, z1 . . . , ai-2)
=	∏(zi-i | Si-i)p(si-i | so, ao,zι ..., Zi-2,ai-2)	当)
EsO ∏(zi-i | si-ι)p(si-ι | so, ao,zι..., Zi-2,ai-2)
i-1
π(zi-1 | si—I)Zi— 1
Psi-1∏(Zi-1 I s0-ι)Zi-ι
(46)
Therefore we can compute Zi recursively,
Zi =	p(si | si—1
si-1
ai—1 ) P
π (zi—1 | si—1 )Zi—1
si-In(Zi-1 | Si-I)Zi—1
(47)
20
Under review as a conference paper at ICLR 2020
with Zi = p(sι | so, a。). The last term to compute in the definition of VCPM(∏) (so) is
kk
E[ri+1(si, ai) | so, ao, z1, a1, . . . ,ai)] =	E[ri+1(si,ai) | so, ao, z1, a1, . . . ,zi)]	(48)
i=o	i=o
k
=	p(si | so,ao,z1,a1, . . . ,zi)ri+1(si, ai)
i=o si	(49)
(50)
π(zi | si)Zi
入上 Ps0∏(Zi I si)Z0ri+1(si,ai).
i=o si	si	i i
I.2	Planning with Non-Causal vs Causal Models on FuzzyBear
0.
(a) Non-Causal Model
Figure 7: FuzzyBear decision trees evaluated with NCPM and CPM based on the optimal policy data with
the intended action. Decision paths through red action nodes that give the maximum expected rewards are
highlighted in red. The blue nodes are states and the gray ones are chance nodes.
p(z1
0.5
U = o.
(b) Causal Model
In Figure 7(a), the non-causal agent always chooses hug at step t = 1, since it has learned from
the optimal policy that a reward of +1 always follows after taking a1 = hug. Thus from the non-
causal agent’s point of view, the expected reward is always 1 after hugging. This is wrong since only
hugging a teddy bear gives reward 1. Moreover it exceeds the maximum expected reward 0.5 of the
FuzzyBear MDP. In Figure 7(b), the causal agent first samples the intention z1 from the optimal
policy, giving equal probability of landing in either of the two chance nodes. Then it chooses hug
if z1 = 0, indicating a teddy bear since the optimal policy intends to hug only if it observes a teddy
bear. Likewise, it chooses run if z1 = 1, indicating a grizzly bear. While the non-causal model
expects unrealistically high reward, the causal model never over-estimates the expected reward.
I.3	LEARNING WITH OPTIMAL POLICY AND VARYING ε-EXPLORATION:
We analyze learning from optimal policy with varying amounts of ε-exploration for models on
FuzzyBear (Figure 8(a)) and AvoidFuzzyBear (Figure 8(b)). As the parameter ε-exploration varies
in range (0, 1], the causal model has a constant evaluation since the intended action is not affected
by the randomness in exploration. The non-causal model, on the other hand, evaluates based on
the deterministic optimal policy data (i.e. at ε = 0) at an unrealistically high value of 1.0 when
the maximum expected reward is 0.5. As ε → 1, the training data becomes more random and
its optimal evaluation expectantly goes down to match the causal evaluation based on a uniformly
random policy. The causal evaluation based on the optimal policy VCM (∏*)converges to the ground
truth environmental evaluation Vn as ε → 0.
21
Under review as a conference paper at ICLR 2020
Figure 8: (a) In FuzzyBear, we use optimal policy with ε-exploration to generate training data for the Non-
Causal Partial Model (NCPM) and Causal Partial Model (CPM). We vary the exploration parameter ε ∈ (0, 1]
and observe differences in found optimal values VMM(π) under the model M(∏), where M(π) denotes either
CPM or NCPM trained on behavior policy π. The NCPM evaluation VNCPM(π*) gives an unrealistically high
value 1.0 learned from the deterministic optimal policy (ε = 0). Expectantly, it decreases to the level of
CPM optimal value VCPM(πrand)learned from the uniformly random policy as ε → 1. The CPM optimal values
VCPM(π*)are constant for any value of ε based on the theoretical analysis in Section I.1. (b) shows the same
plots as (a) for the AvoidFuzzyBear environment. Learning from any policy π, the CPM optimal value always
equals the maximum expected reward 0.6, by correctly choosing to stay home.
J	Details for 3D experiments
J.1 Conditional Dirichlet Mixture
When the backdoor variable zt was chosen to be the action probabilities, the distribution p(zt|ht)
was chosen as a mixture-network with Nc Dirichlet components. The concentration parameters
αk(ht) of each component were parametrized as αk(ht) = α softmax(fk(ht)), where fk is the
output of a relu-MLP with layer sizes [256, 64, Nc × Na], α is a total concentration parameter and
Na is the number of actions.
J.2 Hyper Parameters and Training
The hyper-parameter value ranges used in our 3D experiments are similar to Gregor et al. (2019)
and are shown in Table 3.
To speed up training, we interleaved training on the T-maze level with a simple “Food” level, in
which the agent simply had to walk around and eat food blocks (described by Gregor et al. (2019)).
J.3 Analysis of rollouts
For each episode, 5 rollouts are generated after having observed the first 3 frames from the environ-
ment. For the 5 rollouts, we processed the first 25 frames to classify the presence of food blocks by
performing color matching of RGB values, using K-means and assuming 7 clusters. Rollouts were
generated shortly after the policy had achieved ceiling performance (15-20 million frames seen),
but before the entropy of the policy reduces to the point that there is no longer sufficient exploration.
See Figure 9 for these same results for later training.
22
Under review as a conference paper at ICLR 2020
Table 3: Hyper-parameters used. Each reported experiment was repeated at least 5 times with different random
seeds.
Hyper-parameter	Description	Value
	μpolicy		policy learning rate	0.0001
	μmodel		model learning rate	0.0005
c	policy entropy regularization	0.0004
β1	Adam βι	0.9
β2	Adam β2	0.999
Lo	Overshoot Length	8
Lu	Unroll Length	100
Nt	Number of points used to evaluate the generative loss per trajectory	6
Ng	Number of points used to evaluate the generative loss per overshoot	2
Ns	Number of ConvDRAW Steps	8
Nh	Number of units in LSTM	512
α	Total concentration of Dirichlet distributions	100
Nc	Number of components of Dirichlet mixture	10
.0
3
- - - - - C
505050
..................
221100
stigol ycilop fo yportnE
10M	20M	30M	40M	50M
Training frames seen
Figure 9: While earlier in training, CPM generates a diverse range of outcomes (food or no food), as the
policy becomes more deterministic (as seen in the right plot of the policy entropy over training), CPM starts to
generate more food and becomes overoptimistic, similar to NCPM. This can be avoided by training the model
with non-zero ε-exploration.
23
Under review as a conference paper at ICLR 2020
K Model Rollouts
Table 4: Different types of model rollouts considered. Blue cells indicate variables that require interaction with
the real environment, depending on the agent’s state st . Orange cells indicate variables that can be computed
from the model’s state ht .
Rollout I Rollout with forced actions			Rollout with forced actions and backdoor
Backdoor Action State Prediction	Zt 〜p(zt∖ht) at 〜∏(at∖zt)	Zt 〜p(zt∖ht) at 〜∏(at∣Zt); Zt 〜m(^t∣st)	Zt 〜m(zt∖st) at 〜∏(at∖zt)
	ht = RNNh(ht-i,at-i,zt-i)		
		yt 〜p(yt Iht)			
Figure 10: Full rollouts for NCPM, conditioned on forced actions.
24
Under review as a conference paper at ICLR 2020
observed
frames
time
Figure 11: Full rollouts for CPM, conditioned on forced actions and generated z .
25
Under review as a conference paper at ICLR 2020
observed
frames
Figure 12: Full rollouts for CPM, conditioned on forced actions and forced
26