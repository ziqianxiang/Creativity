Under review as a conference paper at ICLR 2020
Samples Are Useful? Not Always:
DENOISING POLICY GRADIENT UPDATES USING
VARIANCE EXPLAINED
Anonymous authors
Paper under double-blind review
Ab stract
Policy gradient algorithms in reinforcement learning optimize the policy directly
and rely on efficiently sampling an environment. However, while most sampling
procedures are based on sampling the agent’s policy directly, self-performance
measures could be used to improve sampling before each policy update. Follow-
ing this line of thoughts, we propose SAUNA, a method where transitions are
rejected from the gradient updates if they do not meet a particular criterion, and
kept otherwise. The criterion, Vex, is the fraction of variance explained by the
value function V : a measure of the discrepancy between V and the returns. In
this work, Vex is used to estimate the impact transitions will have on learning: it
refines sampling by simplifying the underlying state space and improves policy
gradient methods. In this paper: (a) We introduce Vex , the criterion used for de-
noising policy gradient updates. We refer to this procedure as transition dropout
and explore its implications on learning. (b) We conduct experiments across a vari-
ety of benchmark environments, including continuous control problems. SAUNA
clearly improves performance. (c) We investigate how Vex reliably selects sam-
ples with most positive impact on learning. (d) We study how this criterion can
work as a dynamic tool to adjust the ratio between exploration and exploitation.
1	Introduction
Learning to control agents in simulated environments has been a challenge for decades in rein-
forcement learning (RL) (Nguyen & Widrow, 1990; Werbos, 1989; Schmidhuber & Huber, 1991;
Robinson & Fallside, 1989) and has recently led to a lot of research efforts in this direction (Mnih
et al., 2013; Burda et al., 2019; Ha & Schmidhuber, 2018; Silver et al., 2016; Espeholt et al., 2018),
notably in policy gradient methods (Schulman et al., 2016; Silver et al., 2014; Lillicrap et al., 2016;
Mnih et al., 2016). Despite the definite progress made, policy gradient algorithms still heavily suffer
from sample inefficiency (Kakade, 2003; Wu et al., 2017; Schulman et al., 2017; Wang et al., 2017).
In particular, many policy gradient methods are subject to use as much experience as possible in
the most efficient way. However, the quality of the sampling procedure also determines the final
performance of the agent. We make the hypothesis that not all experiences are worth to use in the
gradient update. Indeed, while perhaps trajectory simulations should be as rich as possible, some
transitions may instead add noise to the gradient update, diluting relevant signals and hindering
learning.
The central idea of SAUNA is to reject transitions that are not informative for the particular task
at hand. For that purpose, we use a measure of the discrepancy between the estimated state value
and the observed returns. This discrepancy is formalized with the notion of the fraction of variance
explained Vex (Kvalseth, 1985). Transitions for which Vex is close to zero are those for which the
correlation between the value function V and the observed returns is also close to zero. SAUNA
keeps transitions where there is either strong correlation or lack of fit between V and the returns,
while avoiding the learning signals to be diluted by the dropped out samples. We will examine
the impact of transition dropout (inspired by Srivastava et al. (2014) and Freeman et al. (2019))
and its theoretical implications. We consider on-policy methods for their unbiasedness and stability
1
Under review as a conference paper at ICLR 2020
compared to off-policy methods (Nachum et al., 2017). However, our method can be applied to
off-policy methods as well, and we leave this investigation open for future work.
The contributions of this paper are summarized as follows:
1.	We propose to move from a simple policy-based sampling procedure based to one taking
into account the agent’s ability in an environment measured by Vex. We explore how the
use of Vex can drive the alignment between the samples used to update the policy and the
agent’s progress, while simplifying the underlying state space with transition dropout.
2.	We provide a method that transforms policy gradient algorithms by assuming that not all
samples are useful for learning and that these disturbing samples should, therefore, be
rejected. While our method is a simple extension of policy gradient algorithms, it adds a
variance criterion to the optimization problem and introduces a novel rejection sampling
procedure.
3.	By combining (1) and (2), we obtain a learning algorithm that is empirically effective in
learning neural network policies for challenging control tasks. Our results extend the state-
of-the-art in using reinforcement learning for high-dimensional continuous control. We
also evaluate the theoretical implications of our method.
2	Preliminaries
We consider a Markov Decision Process (MDP) with states s ∈ S, actions a ∈ A, transition
distribution st+ι 〜P (st, at) and reward function r(s, a). Let π = {π(a∣s), S ∈ S, a ∈ A} denote
a stochastic policy and let the objective function be the traditional expected discounted reward:
∞
J(π) , E X γtr (st, at) ,	(1)
T 〜∏	/ /
t=0
where γ ∈ [0, 1) is a discount factor (Puterman, 1994) and τ = (s0, a0, s1, . . . ) is a trajectory
sampled from the environment. Policy gradient methods aim at modelling and optimizing the policy
directly (Williams, 1992). The policy π is generally implemented with a function parameterized by
θ. In the sequel, we will use θ to denote the parameters as well as the policy. In deep reinforcement
learning, the policy is represented in a neural network called the policy network and is assumed to
be continuously differentiable with respect to its parameters θ.
2.1	Fraction of variance explained: Vex
The fraction of variance that the value function explains about the returns corresponds to the pro-
portion of the variance in the dependent variable V that is predictable from the independent variable
st. We define Vτex as the fraction of variance explained for a trajectory τ:
Vτex , 1 -
Pt∈τ (Rt- V(St))
Pt∈τ (Rt-琢，
(2)
where Rt and V(St) are respectively the return and the expected return from state St ∈ T, and R
is the mean of all returns in trajectory τ. In statistics, this quantity is also known as the coeffi-
cient of determination R2 and it should be noted that this criterion may be negative for non-linear
models (Kvaiseth, 1985), indicating a severe lack of fit of the corresponding function:
•	Vτex = 1: V perfectly explains the returns - V and the returns are correlated;
•	Vτex = 0 corresponds to a simple average prediction - V and the returns are not correlated;
•	Vτex < 0: V provides a worse fit to the outcomes than the mean of the returns.
One can have the intuition that Vτex close to 1 is interesting because it gives samples from an ex-
ercised behavior while Vτex < 0 corresponds to a high mean-squared error for the value function,
meaning the agent will learn from the corresponding samples. On the other hand, Vτex close to 0
does not provide any valuable information for a correct fitting of the state value function. We will
demonstrate that V ex is a relevant indicator for assessing self-performance in RL.
2
Under review as a conference paper at ICLR 2020
2.2	Policy gradient methods: PPO and A2C
We use PPO (Schulman et al., 2017) throughout this work. We also use A2C, a synchronous vari-
ant of Mnih et al. (2016), to demonstrate SAUNA’s performance. Below we provide some details
regarding PPO, as we use it generously in this work. In previous work, PPO has been tested on a set
of benchmark tasks and has produced impressive results in many cases despite a relatively simple
implementation. At each iteration, the new policy θnew is obtained from the old policy θold :
θnew — argmax E	[LPPO (st, at, θ0id, θ) .	(3)
θ	st,at~πθold
We use the clipped version of PPO whose objective function is:
LPPO(St,at, θoid,θ)= min ( Tn *St) Aπθoid (St,电),g(e,Aπθoid (st,at))) ,	(4)
"Sold"1%)	J
where
g(, A) =	((11 + ))AA, AA ≥ 00	(5)
(1 - ) ,	< 0.
A is the advantage function, A(S, a) , Q(S, a) - V (S). The expected advantage function Aπθold
is estimated by an old policy and then re-calibrated using the probability ratio between the new and
the old policy. In Eq. 4, this ratio is constrained to stay within a small interval around 1, making the
training updates more stable.
2.3	Related Work
Our method incorporates three key ideas: (a) function approximation with a neural network com-
bining or separating the actor and the critic with an on-policy setting, (b) transition dropout reducing
signal dilution in gradient updates while simplifying the underling MDP and (c) using Vτex as a mea-
sure of correlation between the value function and the returns to allow for better sampling and more
efficient learning. Below, we consider previous work building on some of these approaches.
Actor-critic algorithms essentially use the value function to alternate between policy evaluation and
policy improvement (Sutton & Barto, 1998; Barto et al., 1983). In order to update the actor, many
methods adopt the on-policy formulation (Peters & Schaal, 2008; Mnih et al., 2016; Schulman et al.,
2017). However, despite their important successes, these methods suffer from sample complexity.
In the literature, research has also been conducted in prioritization sampling. While Schaul et al.
(2016) makes the learning from experience replay more efficient by using the TD error as a mea-
sure of these priorities in an off-policy setting, our method directly selects the samples on-policy.
Schmidhuber (1991) is related to our method in that it calculates the expected improvement in pre-
diction error, but with the objective to maximize the intrinsic reward through artificial curiosity.
Instead, our method estimates the expected fraction of variance explained and filters out some of the
samples to improve the learning efficiency.
Finally, motion control in physics-based environments is a long-standing and active research field.
In particular, there are many prior work on continuous action spaces (Schulman et al., 2016; Levine
& Abbeel, 2014; Lillicrap et al., 2016; Heess et al., 2015) that demonstrate how locomotion behavior
and other skilled movements can emerge as the outcome of optimization problems.
3	SAUNA: a dynamic transition dropout method
3.1	Estimating Vex
While sampling the environment, SAUNA rejects samples where V (St) is not correlated with the
return from St. Therefore, Vτex must be estimated at each timestep so we define Vθex (St) as the pre-
diction of Vτex under parameters θ at state St ∈ τ. In addition, for shared parameters configurations,
an error term on the value estimation is added to the PPO objective. The final objective becomes:
L(st,at,θoid,θ) = E LPiO(St,at,θoid,θ)-cι (%(St)-凡)2-。2 (Vex(St) -VTx)2 , (6)
3
Under review as a conference paper at ICLR 2020
where c1 and c2 are respectively the coefficient for the squared-error loss of the value function and
of the fraction of variance explained function. For cases where the network is not shared between
the policy and the value function, Vτex is added to the value function network. Appendix A illustrates
how Vθex(st) is embedded in the original architecture. The rest of the network is unchanged, making
it very easy to use SAUNA without altering the complexity of existing policy gradient methods.
3.2	Using Vex for dynamic temporal abstraction
Mechanistically, SAUNA results in dropping out several transitions from a trajectory τ , before each
gradient update, and as a function of the starting state st of those transitions. The mechanism is
analogous to the method of dropout in deep learning but articulated here by a dropout in the state
space of the MDP. To evaluate the theoretical implications of the transition dropout, our method can
be formulated using the Options framework (Sutton et al., 1999; Precup, 2000) thoroughly applied
to policy gradient methods in Smith et al. (2018). Previous work shows that by reasoning at several
levels of abstraction, reinforcement learning agents are able to infer, learn, and plan more effectively.
We detail below how SAUNA can be theoretically understood using the Options framework.
In this work, we take semi-Markov options that may depend on the entire preceding sequence. The
agent is given access to a set of options, indexed by σ. Interestingly, the framework can be reduced
as follows: all options share the same policy, ∏σ(at∣st) = ∏θ(at∣st), introduced in section 2, the
same initiation set and the same termination condition which we parameterize as the function βθ(st).
βθ(st) represents the probability of terminating an option and is defined as follows:
βθ(st) =	01
if bθ(s0, . . . , st) ≥ r
if bθ(s0, . . . , st) < r
with
bθ(s0, . . . , st)
∣vexι
∣vex-il + e0
(7)
where for simplicity we rewrite Vex(St) as Vex, Vex-I is the median of Vex between timesteps 0 and
t - 1, eo = 10-8 is to avoid division by zero and r is the dropout parameter. One may legitimately
ask why not use directly bθ(St) = |Vex |. The rationale is practical: the ratio becomes a standardized
measure as the agent learns, stabilized by the median, more robust to outliers than the mean.
V, π,%e V, ∏,%e K ∏,%ex V, ∏,%ex
^■llj
t = 0
βθ (St)= 1
于 e =.1
1
0
.8
2
0
.7
0|
ɪɪfl
3
0
.6
4	5	6
0	1	1
.9	-.1	.1
(a)
a
▼
a
a
Figure 1: (a) Red solid arrows illustrate the transitions affecting the gradient update. We illustrate
the termination condition mechanism with an example. For t = (1, 2, 3,4), Vex is large enough
so βθ(St) = 0. The transitions sampled by policy ∏σ affect the gradient update. The others are
dropped. Note that an option σ can be one-step long, i.e. βθ(St) = 1, βθ(St+1) = 0, βθ(St+2) = 1.
(b) Grey area samples: kept for the gradient update. White area samples: dropped (r = 0.3).
Fig. 1a illustrates an example of transition dropout dynamics in SAUNA while Fig. 1b depicts the
behavior of Eq. 7. The termination function βθ(St) dynamically selects the transitions for which Vtex
is either high or low, but not in between. Those transitions should have a strong impact on learning:
a high absolute score means that the samples correspond to a state St for which the value function is
highly correlated with the returns or does not fit them properly. With this modification, the resulting
gradients should be affected accordingly, which we investigate in section 4.2.2.
4
Under review as a conference paper at ICLR 2020
3.3	Putting it all together: SAUNA sampling procedure
In this work, we hypothesize that low correlated transitions dilute the valuable signal information
of the transitions with high absolute correlation. The temporal abstraction provided by the Options
framework is leveled dynamically thanks to a termination function exploiting the self-performance
measure Vtex . Instead of keeping these noisy transitions, the sampling procedure drops them until
the trajectory is T -steps long. Algorithm 1 illustrates how learning is achieved with SAUNA applied
to PPO, in particular, the fitting of the V ex function in Eq. 10 and the transition dropout in the if
statement. We choose to depict a configuration where the parameters between the policy network,
the value function and the Vex function are not shared, since from this configuration the shared
parameter case is direct.
Algorithm 1 SAUNA sampling procedure in PPO.
Initialize policy parameters θ0
Initialize value function parameters φ0 and Vex function parameters ψ0
for k = 0, 1, 2, . . . do
Initialize trajectory τ to capacity T
while size(τ) ≤ T do
at 〜∏θk (st), Vt = Vφk (st), Vex = VψX(st)
execute action at and observe reward rt+1 and next state st+1
if D J：1	≥ r then
|V0:t-1|+e0
collect transition (st, at, rt, vt, st+1, Vtex) in τ
else
continue without collecting the transition
. For each update k
. For each timestep t
PPO sampling
4
} SAUNA dropout
Gradient Update
Θk+1 - argmaxXmin ( "θ(叫"、Aπθk (st,at) ,g (e, Aπθk (st, aj))	(8)
θ t∈τ	πθk (at|st)
Φk+ι — argmin X (vφk (St)- Rt)	(9)
φ t∈τ
Ψk+1 - argmin X (vψx (St)- VTx)2	(10)
ψ t∈τ
4	How does SAUNA learn?
For ease of reproducibility and sharing, we have forked the original baselines repository from Ope-
nAI and modified the code to incorporate our method1. The complete list of hyperparameters and
details of our implementation are given in Appendix E and F respectively. A discussion about ad-
ditional experiments whose results are non-positive, but which we think contribute positively to this
paper, can be found in Appendix B.
4.1	SAUNA in the continuous domain: MuJoCo and Roboschool
To verify the generalizability of our method, we study SAUNA against PPO and A2C. We compare
SAUNA (PPO+V ex in red) with its natural baseline PPO (PPO in blue). We use 6 simulated robotic
tasks from OpenAI Gym (Brockman et al., 2016) using MuJoCo (Todorov et al., 2012). The two
hyperparameters required by our method (r = 0.3 from Eq. 2 and c2 = 0.5 from Eq. 6) and all the
others (identical to those in Schulman et al. (2017)) are exactly the same for all tasks. We made this
1Code is available here: https://github.com/iclr2020-submission/denoising-gradient-updates
5
Under review as a conference paper at ICLR 2020
choice within a clear and objective framework of comparison between the two methods. Thus, we
have not optimized the rest of the hyperparameters for SAUNA, and its reported performance is not
necessarily the best that could be obtained with more intensive tuning.
Figure 2: Comparison of SAUNA with PPO on 6 MuJoCo environments (106 timesteps, 6 different
seeds). Red is our method PPO+Vex . Line: average performance. Shaded area: standard deviation.
From the graphs reported in Fig. 2, we see that our method outperforms all continuous control tasks.
We also present in Table 1 the scores obtained for each task on the experiments with PPO and A2C.
The graphs for A2C are reported in Appendix C.1.
Table 1: Average total reward of the last 100 episodes over 6 runs on the 6 MuJoCo environments
on PPO and A2C. Boldface mean ± std indicate better mean performance.
Task	PPO	PPO+V ex	A2C	A2C+V ex
HalfCheetah	2277 ± 432	2929 ± 169	1389 ± 157	1731±147
Hopper	2106 ± 133	2250 ± 73	1367 ± 110	1627±97
InvertedDoublePendulum	6100 ± 143	6893 ± 350	4151 ± 67	5132±409
InvertedPendulum	532 ± 19	609 ± 24	686 ± 15	684 ± 10
Reacher	-7.5 ± 0.8	-7.2 ± 0.3	-9.2 ± 0.8	-8.5 ± 0.7
Swimmer	99.5 ± 5.4	100.8 ± 10.4	44.1 ± 10.3	59.0 ± 5.5
We then experiment with the more difficult, high-dimensional continuous domain environment of
Roboschool (Klimov & Schulman, 2017) with different neural network capacities for the model.
When resources are limited in terms of number of parameters, it seems natural that dropping out
samples based on their predicted learning impact allows to reduce noise in the gradient update and
accelerate learning. And as a result, our method is faster and more efficient than the baseline. When
resources are not limited, the gap closes towards the end of the training and our method performs as
well as the baseline.
4.2	Understanding the impact of SAUNA on learning
4.2.1	What advantage does dropout give?
We further study the impact of dropping out noisy samples by conducting additional experiments in
predicting V ex while omitting the filtering step before the gradient update: the if statement in Al-
gorithm 1 is removed and all transitions are collected in τ . Indeed, the SAUNA algorithm could
improve the agent’s performance by simply training the shared network to optimize the variance
explained head. Fig. 3 (full results are provided in Appendix D) demonstrates the positive effects of
dropping out the samples. In addition, we studied the number of sample dropouts per task and their
evolution throughout the training. On average, SAUNA rejects 5-10% of samples at the beginning
of training which reduces to 2-6% at the end.
6
Under review as a conference paper at ICLR 2020
Figure 3: Comparison of SAUNA with PPO on 3 MuJoCo environments (106 timesteps, 6 different
sees). Red is our method PPO+V ex, Orange is PPO+V ex without the filtering out of noisy samples.
Line: average performance. Shaded area: standard deviation.
4.2.2	How does SAUNA impact the gradients ?
Prior to the gradient updates, SAUNA remove the transiions for which the state value is poorly
correlated with the returns. By doing so, we hypothesized that information signals from samples
with significant Vex would be less diluted by dropped out samples. One can easily see by the graphs
that the norm of the gradients is affected by such a change in the learning procedure. Fig. 4 shows
that SAUNA dropout generates larger gradients. Hence policy updates have bigger steps, which
eventually results in better performance.
(a)	(b)
Figure 4: Gradients L1-norm from the (a) first layer and (b) last layer of the shared parameters
network for PPO and when SAUNA i applid to PPO. Task: HalfCheetah-v2.
One may wonder quite rightly Why performance is not damaged, since larger gradients could hinder
learning. The performance results suggest that the gradients contain more useful information from
each of the transitions that passed SAUNA sampling. In other terms, the relevant information for
the task at hand is less diluted, the gradients are more qualitative and have been partially denoised.
4.2.3	HalfCheetah： a qualitative study
(a)	(b)
Figure 5: (a) Example of PPO getting trapped in a local minimum (top row) while PPO+Vex reaches
a better optimum (bottom row). (b) V ex score for PPO (orange) and SAUNA (green).
In HaIfCheetah, a well-known behavior (Lapan, 2018) is that for multiple seeds PPO is converging to
a local minimum forcing the agent to move forward on its back. However, we observed that SAUNA
made it possible to leave from, or at least to avoid these local minima. This is illustrated in Fig. 5a
where we can see still frames of two agents trained with PPO and SAUNA for 106 timesteps on
7
Under review as a conference paper at ICLR 2020
identically seeded environments. Their behavior is entirely different. Looking at Vex in Fig. 5b, we
can see that the graphs differ quite interestingly. The orange agent seems to find very quickly a local
minimum on its back while the green agent’s Vex varies much more. This seems to allow the latter
to explore more states than the former and finally find the fastest way forward. Supported by the
previous study, we can infer that SAUNA agents are better able to explore interesting states while
exploiting with confidence the value given to the states observed so far.
5	Discussion
Intuitively, for the policy update, our method will only use qualitative samples that provide the agent
with (a) reliable and exercised behavior (high Vex) and (b) challenging states from the point of
view of correctly predicting their value (low Vex). The SAUNA algorithm keeps samples with high
learning impact, rejecting other noisy samples from the gradient update.
5.1	Denoising policy gradient updates and the policy gradient theorem
Policy gradient algorithms are backed by the policy gradient theorem (Sutton et al., 2000):
VθL(θ)=8 X dπ(s) X Qπ(s,a)Vθ∏(a|s).	(11)
s∈S	a∈A
As long as the asymptotic stationary regime is not reached, it is not reasonable to assume the sampled
states to be independent and identically distributed (i.i.d.). Hence, it seems intuitively better to ignore
some of the samples for a certain period, to allow the most efficient use of information. One can
understand SAUNA as making gradient updates more robust through dropout, especially when the
update is low and the noise can be dominant. Besides, not taking all samples reduces the bias in the
state distribution dπ . Therefore, it now seems more reasonable to consider the sampled states i.i.d.,
which we theoretically need for the policy gradient theorem.
5.2	Impact of Vex on the shared network parameters
The shared network predicts V ex in conjunction with the value function and the policy. Therefore, as
its parameters are updated through gradient descent, they converge to one of the objective function
minima (hopefully, a global minimum). This parameter configuration integrates Vex , predicting
how much the value function has fit the observed samples, or informally speaking how well the
value function is doing for state st . This new head tends to lead the network to adjust predicting a
quantity relevant for the task. Instead of using domain knowledge for the task, the method rather
introduces problem knowledge by constraining the parameters directly.
6	Conclusion
Policy gradient methods optimize the policy directly through gradient ascent with the objective to
maximize the expected return. We have introduced a new, lightweight and agnostic method techni-
cally applicable to any policy gradient method using a neural network as function approximation.
The central idea of this paper is that samples that are uncorrelated with the return are dropped out
and not considered for the policy update. Those low correlated samples are ignored by SAUNA,
the mechanism is reflected in a dynamic temporal abstraction controlled by the estimated variance
explained of each state. The relevant signal information being less diluted, this results in a denoising
effect on the gradients, ultimately leading to improved performance.
We demonstrated the effectiveness of our method when applied to two policy gradient methods on
several standard benchmark environments. We also established that samples can be removed from
the gradient update without hindering learning but can, on the opposite, improve it. We further
studied the impact that such a modification in the sampling procedure has on learning. Several
open topics warrant further study. The influence of SAUNA on the distribution of states could, for
instance, be theoretically investigated. We also find the effort (Cobbe et al., 2019; Zhang et al.,
2018) to go further towards generalization in RL very promising, and we think SAUNA could be
useful in these problems as a way to regularize policy gradient methods.
8
Under review as a conference paper at ICLR 2020
References
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike adaptive elements
that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and
Cybernetics ,13(5):834-846,1983.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-
ronment: An evaluation platform for general agents. Journal of Artificial Intelligence Research,
47:253-279, 2013.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019.
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generaliza-
tion in reinforcement learning. In International Conference on Machine Learning, pp. 1282-1289,
2019.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala:
Scalable distributed deep-rl with importance weighted actor-learner architectures. In International
Conference on Machine Learning, pp. 1406-1415, 2018.
Daniel Freeman, Luke Metz, and David Ha. Learning to predict without looking ahead: World
models without forward prediction. In Advances in Neural Information Processing Systems, 2019.
David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances
in Neural Information Processing Systems, pp. 2450-2462, 2018.
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learn-
ing continuous control policies by stochastic value gradients. In Advances in Neural Information
Processing Systems, pp. 2944-2952, 2015.
Sham Machandranath Kakade. On the sample complexity of reinforcement learning. PhD thesis,
University of London, 2003.
Oleg Klimov and J Schulman. Roboschool, 2017.
Tarald O. Kvalseth. Cautionary Note about R2. The American Statistician, 39(4):279-285, 1985.
Max Lapan. Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-
networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more. Packt Publishing,
2018.
Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under
unknown dynamics. In Advances in Neural Information Processing Systems, pp. 1071-1079,
2014.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 2775-2785, 2017.
9
Under review as a conference paper at ICLR 2020
Derrick Nguyen and Bernard Widrow. The truck backer-upper: An example of self-learning in
neural networks. In Advanced Neural Computers,pp. 11-19, 1990.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks: the official journal of the International Neural Network Society, 21(4):682-697, 2008.
Doina Precup. Temporal abstraction in reinforcement learning. Ph. D. thesis, University of Mas-
sachusetts, 2000.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0471619779.
Anthony J. Robinson and F Fallside. Dynamic reinforcement driven error propagation networks
with application to game playing. In Conference of the Cognitive Science Society, pp. 836-843,
1989.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
International Conference on Learning Representations, 2016.
Jurgen Schmidhuber. Curious model-building control systems. In IEEE International Joint Confer-
ence on Neural Networks, pp. 1458-1463, 1991.
Jurgen Schmidhuber and Rudolf Huber. Learning to generate artificial fovea trajectories for target
detection. International Journal of Neural Systems, 2(1/2):135-141, 1991.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In International Con-
ference on Learning Representations, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International Conference on Machine Learning, pp.
387-395, 2014.
David Silver, Aja Huang, Christopher Maddison, Arthur Guez, Laurent Sifre, George Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game ofGo with
deep neural networks and tree search. Nature, 529:484, 2016.
Matthew Smith, Herke Hoof, and Joelle Pineau. An inference-based policy gradient method for
learning options. In International Conference on Machine Learning, pp. 4710-4719, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Richard S. Sutton and Andrew G. Barto. Introduction to reinforcement learning. Cambridge: MIT
Press, 1998.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-
211, 1999.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems, pp. 1057-1063, 2000.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
10
Under review as a conference paper at ICLR 2020
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. In International
Conference on Learning Representations, 2017.
Paul J. Werbos. Neural networks for control and system identification. In IEEE Conference on
Decision and Control,, pp. 260-265, 1989.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
Neural Information Processing Systems, pp. 5279-5288, 2017.
Amy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overfitting and generalization in
continuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018.
11
Under review as a conference paper at ICLR 2020
A Illustration of the SAUNA architecture
Figure 6: Network-agnostic variance explained head.
In Fig. 6 we rewrite Vθex (st) as Vtex. On the right side of the figure is illustrated the Vtex head that
is added to the shared or the separate network configurations. Note that even if Vτex is defined for a
sampled trajectory τ , the model predicts its value at each state st ∈ τ .
B	Additional experiments with non-positive results
Atari domain. We tested our method on the Atari 2600 domain (Bellemare et al., 2013) without
observing any improvement in learning. By comparing the two algorithms where the method of
filtering the samples is used or not, we could not observe any difference, as some tasks were better
performed by one method and others by the other.
Mean of Vex. Although Vex, the median of Vex , is more expensive to calculate, we observe that it
gives much better results than if We use its mean in Eq. 7. Using the median helps (Kvalseth, 1985)
because the distribution of Vex is not normal and includes outliers that will potentially produce
misleading results.
Non-empirical Vex . We also experimented With using the real values of Vex in Eq. 7 When calcu-
lating V0e:xt-1, instead of the predicted ones. This has yielded less positive results, and it is likely that
this is due to the difference betWeen the predicted and actual values at the beginning of learning,
Which has the effect of distorting the ratio in Eq. 7.
Adjusting state count. In order to stay in line With the policy gradient theorem (Sutton et al., 2000),
We have Worked to adjust the distribution of states dπ to What it really is, since some states that the
agent has visited are not included in the gradient update. We adjusted it using the ratio betWeen the
number of states visited and the actual number of transitions used in the gradient update, but this did
not improve the learning, and instead, We observed a decrease in performance.
Random dropout. We experimented With dropping out at random, and before each gradient update,
a number of samples corresponding to the same approximate number of samples that SAUNA drops.
This resulted in a decrease in performance compared to PPO, as one can expect.
12
Under review as a conference paper at ICLR 2020
C Additional experiments with SAUNA in the continuous domain
C.1 MuJoCo: Comparison of A2C+SAUNA and A2C
Figure 7: Comparison of SAUNA with A2C on 6 MuJoCo environments (106 timesteps, 6 different
seeds). Red is our method A2C+V ex . Line: average performance. Shaded area: standard deviation.
In Fig. 7 we compare A2C+SAUNA (A2C+V ex in red) with A2C (A2C in green) on again 6 simu-
lated robotic tasks from OpenAI Gym using the MuJoCo physics engine.
C.2 Roboschool: Comparison of PPO+Vex and PPO
We experiment here with the more difficult, high-dimensional continuous domain environment of
Roboschool (Klimov & Schulman, 2017): RoboschoolHumanoidFlagrunHarder-v1. The purpose
of this task is to allow the agent to run towards a flag whose position varies randomly over time. It
is continuously bombarded by white cubes that pushes it out of its path, and if it does not hold itself
up it is left to fall.
Figure 8: Comparison of SAUNA with PPO on the more challenging Roboschool environment (108
timesteps, 6 different seeds). Red is our method PPO+V ex . Line: average performance. Shaded
area: standard deviation.
In Fig. 8a, the same fully-connected network as for the MuJoCo experiments (2 hidden layers each
with 64 neurons) is used. In Fig. 8b, the network is composed of a larger 3 hidden layers with 512,
256 and 128 neurons. We trained those agents with 32 parallel actors. In both experiments, SAUNA
performs better and learns faster at the beginning. Then, only when the policy and value functions
benefit from a larger network, the gap closes, and our method does as well as the baseline. When
resources are limited in terms of the number of parameters, it seems natural that filtering out samples
based on their predicted training impact helps to reduce noise in the gradient update and accelerate
learning.
13
Under review as a conference paper at ICLR 2020
Finally, we investigated further and conducted the same experiment with the larger network (3 hid-
den layers with 512, 256 and 128 neurons), but with 128 actors in parallel instead of 32. Results are
reported in Fig. 8c: our method still learns faster and achieves better performance than the baseline.
D Additional experiments on the advantage of transition
DROPOUT
In order to identify the effects of the training of the Vex head and the filtering out of sample, we
verify the hypothesis that filtering out noisy samples does improves the performance. To do so,
in Section 4.2.1, we conduct experiments where the network predicts Vex but where the noisy sam-
ples are not filtered out: the if statement in Algorithm 1 is removed and all transitions are collected
in τ . In Fig. 9, we see that when the noisy samples are not filtered out the performance is worst than
the baseline, confirming the positive denoising impact of filtering out the variance-selected samples.
Figure 9: Comparison of our method with PPO on 6 MuJoCo environments (106 timesteps, 6 dif-
ferent seeds). Red is our method PPO+Vex , Orange is PPO+Vex without the filtering out of noisy
samples. Line: average performance. Shaded area: standard deviation.
E Hyperparameters
We have tuned the hyperparameters of our method by performing a grid search and selecting the
best combinations by considering those with the largest consensus.
Hyperparameter	Value
Horizon (T) Adam stepsize Nb. epochs Minibatch size Discount (Y) GAE parameter (λ) Clipping parameter (e) VF Coef (ci) V ex Coef(C2) Dropout threshold (r)	2048 (MuJoCo), 512 (Roboschool) 3∙10-4 10 (MuJoCo), 15 (Roboschool) 64 (MuJoCo), 4096 (Roboschool) 0.99 0.95 0.2 0.5 0.5 0.3
Table 2: Hyperparameters used both in PPO and SAUNA. The two last hyperparameters are only
relevant for our method.
14
Under review as a conference paper at ICLR 2020
Hyperparameter	Value
Horizon (T)	^^048
Adam stepsize	3∙10-4
Nb. epochs	10
Minibatch size	64
Nb. envs	8
Discount (Y)	0.99
VF coef (c1)	0.5
Vex coef (c2)	0.5
Dropout threshold (r)	0.35
Table 3: Hyperparameters used both in A2C and SAUNA. The two last hyperparameters are only
relevant for our method.
F Implementation details
Unless otherwise stated, the policy network used for MuJoCo and Roboschool tasks is a fully-
connected multi-layer perceptron with 2 hidden layers of 64 units. For Atari, the network is shared
between the policy and the value function and is the same as in Mnih et al. (2016). The architecture
for the Vex function head is the same as for the value function head.
G Clipped surrogate objective details
In Eq. 4, we use the following standard definitions for the advantage function:
Aπ (s, a) = Qπ (s, a) - V π (s),	(12)
where
∞
Qn(St ,3 = st%	X Ylrt + l
at + J∞ Ll = O
∞
and Vπ (St)= aE∞ X Ylrt+l
st + 1：8 Ll = O
(13)
H An analogy with saunas
Saunas originated in Northern Europe and are thought to date back to 7000 BC. Their use helps to re-
lease impurities [filtered out noisy samples] and improves the regeneration of cells [improved policy
gradient updates]. Their temperatures could be fatal if not regulated by humidity [Vex criterion].
15