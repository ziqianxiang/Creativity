Under review as a conference paper at ICLR 2020
Semi-Supervised Named Entity Recognition
WITH CRF-VAES
Anonymous authors
Paper under double-blind review
Ab stract
We investigate methods for semi-supervised learning (SSL) of a neural linear-
chain conditional random field (CRF) for Named Entity Recognition (NER) by
treating the tagger as the amortized variational posterior in a generative model
of text given tags. We first illustrate how to incorporate a CRF in a VAE, en-
abling end-to-end training on semi-supervised data. We then investigate a se-
ries of increasingly complex deep generative models of tokens given tags enabled
by end-to-end optimization, comparing the proposed models against supervised
and strong CRF SSL baselines on the Ontonotes5 NER dataset. We find that our
best proposed model consistently improves performance by ≈ 1% F1 in low- and
moderate-resource regimes and easily addresses degenerate model behavior in a
more difficult, partially supervised setting.
1	Introduction
Named entity recognition (NER) is a critical subtask of many domain-specific natural language
understanding tasks in NLP, such as information extraction, entity linking, semantic parsing, and
question answering. State-of-the-art models treat NER as a tagging problem (Lample et al., 2016;
Ma & Hovy, 2016; Strubell et al., 2017; Akbik et al., 2018), and while they have become quite
accurate on benchmark datasets in recent years (Lample et al., 2016; Ma & Hovy, 2016; Strubell
et al., 2017; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018), utilizing them for new
tasks is still expensive, requiring a large corpus of exhaustively annotated sentences (Snow et al.,
2008). This problem has been largely addressed by extensive pretraining of high-capacity sentence
encoders on massive-scale language modeling tasks (Peters et al., 2018; Devlin et al., 2018; Howard
& Ruder, 2018; Radford et al., 2019; Liu et al., 2019b), but it is natural to ask if we can squeeze
more signal from our unlabeled data.
Latent-variable generative models of sentences are a natural approach to this problem: by treating
the tags for unlabeled data as latent variables, we can appeal to the principle of maximum marginal
likelihood (Berger, 1985; Bishop, 2006) and learn a generative model on both labeled and unlabeled
data. For models of practical interest, however, this presents multiple challenges: learning and
prediction both require an intractable marginalization over the latent variables and the specification
of the generative model can imply a posterior family that may not be as performant as the current
state-of-the-art discriminative models.
We address these challenges using a semi-supervised Variational Autoencoder (VAE) (Kingma et al.,
2014), treating a neural tagging CRF as the approximate posterior. We address the issue of optimiza-
tion through discrete latent tag sequences by utilizing a differentiable relaxation of the Perturb-and-
MAP algorithm (Papandreou & Yuille, 2011; Mensch & Blondel, 2018; Corro & Titov, 2018), al-
lowing for end-to-end optimization via backpropagation (Rumelhart et al., 1988) and SGD (Robbins
& Monro, 1951). Armed with this learning approach, we no longer need to restrict the generative
model family (as in Ammar et al. (2014); Zhang et al. (2017)), and explore the use of rich deep gen-
erative models of text given tag sequences for improving NER performance. We also demonstrate
how to use the VAE framework to learn in a realistic annotation scenario where we only observe a
biased subset of the named entity tags.
Our contributions can be summarized as follows:
1
Under review as a conference paper at ICLR 2020
1.	We address the problem of semi-supervised learning (SSL) for NER by treating a neural
CRF as the amortized approximate posterior in a discrete structured VAE. To the best of
our knowledge, we are the first to utilize VAEs for NER.
2.	We explore several variants of increasingly complex deep generative models of text given
tags with the goal of improving tagging performance. We find that a joint tag-encoding
Transformer (Vaswani et al., 2017) architecture leads to an ≈ 1% improvement in F1 score
over supervised and strong CRF SSL baselines.
3.	We demonstrate that the proposed approach elegantly corrects for degenerate model perfor-
mance in a more difficult partially supervised regime where sentences are not exhaustively
annotated and again find improved performance.
4.	Finally, we show the utility of our method in realistic low- and high-resource scenarios,
varying the amount of unlabeled data. The resulting high-resource model is competitive
with state-of-the-art results and, to the best of our knowledge, achieves the highest reported
F1 score (88.4%) for models that do not use additional labeled data or gazetteers.
2	Methods
We first introduce the tagging problem and tagging model. We then detail our proposed modeling
framework and architectures.
2.1	Problem Statement
NER is the task of assigning coarsely-typed categories to contiguous spans of text. State-of-the-art
approaches (Lample et al., 2016; Ma & Hovy, 2016; Strubell et al., 2017; Akbik et al., 2018; Liu
et al., 2019a) do so by treating span extraction as a tagging problem, which we now formally define.
We are given a tokenized text sequence x1:N ∈ XN and would like to predict the corresponding tag
sequence y1:N ∈ YN which correctly encodes the observed token spans. 1 In this work, we use the
BILOU (Ratinov & Roth, 2009) tag-span encoding, which assigns four tags for each of the C span
categories (e.g., B-PER, I-PER, L-PER, U-PER for the PERSON category.) The tag types
B, I, L, U respectively encode beginning, inside, last, and unary tag positions in the original
span. Additionally we have one O tag for tokens that are not in any named entity span. Thus our tag
space has size |Y| = 4C + 1.
2.2	TAGGING CRF
We call the NER task of predicting tags for tokens inference, and model it with a discriminative dis-
tribution qφ (y1:N |x1:N) having parameters φ. Following state-of-the-art NER approaches (Lample
et al., 2016; Ma & Hovy, 2016; Strubell et al., 2017; Akbik et al., 2018), we use a neural encoding
of the input followed by a linear-chain CRF (Lafferty et al., 2001) decoding layer on top.
We use the same architecture for qφ throughout this work, as follows:
1.	Encode the token sequence, represented as byte-pairs, with a fixed pretrained language
model. 2 That is, we first calculate:
h01:N = Pretrained-LM(x1:N), h01:N ∈ RN×dLM
In our first experiments exploring the use of pretrained autoregressive information for gen-
eration (§3.1), we use the GPT2-SM model (Radford et al., 2019; Hugging Face, 2019).
In the experiments after (§3.2) we use the RoBERTa-LG model (Liu et al., 2019b; Hug-
ging Face, 2019).
2.	Down-project the states: h11:N = h10:N W1 + b1, W1 ∈ RdLM×dyq , b1 ∈ Rdyq
1We will often omit sequence boundaries (X — xi：N) to save space, but will always index individual
elements xi .
2We represent the tags at the byte-pair level to ensure alignment between the number of tokens and tags for
the generative models in §2.3
2
Under review as a conference paper at ICLR 2020
3.	Compute local tag scores: syi = vy>hi1 + b2,y, vy ∈ Rdyq , b2 ∈ R|Y|
4.	Combine local and transition potentials: ψyi,yi+1 = syi + Tyi,yi+1 , Tyi,yi+1 ∈ R
5.	Using special start and end states yo = *,yN+1 = ◊ with binary potentials ψ*,y =
T*,y,ψyQ = Tye and the forward algorithm (Lafferty et al., 2001) to compute the the
partition function Z, we can compute the joint distribution:
N
qφ (y1:N |x1:N) = exp{	ψyi,yi+1 - logZ(ψ)}	(1)
i=0
Our tagging CRF has trainable parameters φ = {W1, b1, V, b2, T} 3 and we learn them on a dataset
of fully annotated sentences DS = {(xi1:Ni , y1i:Ni)} using stochastic gradient descent (SGD) and
maximum likelihood estimation.
LS(φ; DS) =	X logqφ(y∣χ)	⑵
(x,y)∈DS
2.3	Semi-Supervised CRF-VAE
We now present the CRF-VAE, which treats the tagging CRF as the amortized approximate poste-
rior in a Variational Autoencoder. We first describe our loss formulations for semi-supervised and
partially supervised data. We then address optimizing these objectives end-to-end using backprop-
agation and the Relaxed Perturb-and-MAP algorithm. Finally, we propose a series of increasingly
complex generative models to explore the potential of our modeling framework for improving tag-
ging performance.
2.3.1	Semi-Supervised VAE
The purpose of this work is to consider methods for estimation of qφ in semi-supervised data
regimes, as in Kingma et al. (2014); Miao & Blunsom (2016); Yang et al. (2017), where there is
additional unlabeled data DU = {(xi1:N i)}. To learn in this setting, we consider generative models
of tags and tokens pθ (x1:N |y1:N)p(y1:N) and, for unobserved tags, aim to optimize the marginal
likelihood of the observed tokens under the generative model.
log pθ (x1:N) = log	pθ (x1:N |y1:N)p(y1:N)
y1:N
This marginalization is intractable for models that are not factored among yi , so we resort to opti-
mizing the familiar evidence lower bound (ELBO) (Jordan et al., 1999; Blei et al., 2017) with an
approximate variational posterior distribution, which we set to our tagging model qφ . We maximize
the ELBO on unlabeled data in addition to maximum likelihood losses for both the inference and
generative models on labeled data, yielding the following objectives:
LS = E log Pθ (χ∣y)+log qφ(y∣χ)	(3)
(x,y)∈DS
LU = X Eqφ[logpθ(XIy)] - βKL(qφMy))	⑷
x∈DU
L(θ, φ; DS ∪ DU, α, β) = LS + αLu	(5)
where α is scalar hyper-parameter used to balance the supervised loss LS and the unsupervised loss
LU (Kingma et al., 2014). β is a scalar hyper-parameter used to balance the reconstruction and KL
terms for the unsupervised loss (Bowman et al., 2015; Higgins et al., 2017). We note that, unlike a
traditional VAE, this model contains no continuous latent variables.
3We omit the Pretrained-LM parameters since they are not updated during training.
3
Under review as a conference paper at ICLR 2020
2.3.2	Partially Supervised Learning (PSL)
Assuming that supervised sentences are completely labeled is a restrictive setup for semi-supervised
learning of a named entity tagger. It would be useful tobe able to learn the tagger on sentences which
are only partially labeled, where we only observe some named entity spans, but are not guaranteed
all entity spans in the sentence are annotated and no O tags are manually annotated. 4 This presents
a challenge in that we are no longer able to assume the usual implicit presence of O tags, since
unannotated tokens are ambiguous. While it is possible to optimize the marginal likelihood of the
CRF on only the observed tags yO , O ⊂ {1, . . . , N } in the sentence (Tsuboi et al., 2008), doing
so naively will result in a degenerate model that never predicts O, by far the most common tag (Jie
et al., 2019). Interestingly, this scenario is easily addressed by the variational framework via the KL
term. We do this by reformulating the objective in Equation 5 to account for partially observed tag
sequences:
Let DP = {(xi1:N , yOi )} be the partially observed dataset where, for some sentence i, O ⊂
{1, . . . , Ni} is the set of observed positions and U = {1, . . . , Ni} \ O is the set of unobserved
positions. Our partially supervised objective is then
LP =
(x,yO)∈DP
[logqφ(yo|x) + αEqφ[logPθ(x|yo ∪
yU)] - αβ KL(qφ (yU |x, yO)||p(yU))
(6)
which can be optimized as before using the constrained forward-backward and KL algorithms de-
tailed in Appendix B.
We also explore using this approach simply for regularization of the CRF posterior by omitting
the token model pθ (x|y). Since We do not have trainable parameters for the generative model in
this case, the reconstruction likelihood drops out of the objective and we have, for a single datum
(xi , yOi ) ∈ DP , the folloWing loss:
LiP = log qφ(yOi |xi) - αβKL(qφ(yUi |xi,yOi )||p(yUi))
2.3.3	Differentiable Perturb-and-MAP
Optimizing Equations 5 and 6 With respect to θ and φ using backpropagation and SGD is straight-
forward for every term except for the expectation terms Eqφ(y∣x)[logPθ(x|y)]. To optimize these
expectations, We first make an Monte Carlo approximation using a single sample draWn from qφ .
This discrete sample, however, is not differentiable with respect to φ and blocks gradient compu-
tation. While we may appeal to score function estimation (Miller, 1967; Williams, 1992; Paisley
et al., 2012; Ranganath et al., 2014; Miao & Blunsom, 2016; Mohamed et al., 2019) to work around
this, its high-variance gradients make successful optimization difficult.
Following Papandreou & Yuille (2011); Mensch & Blondel (2018); Corro & Titov (2018); Kim
et al. (2019), we can compute approximate samples from qφ that are differentiable with respect to
φ using the Relaxed Perturb-and-MAP algorithm (Corro & Titov, 2018; Kim et al., 2019). Due to
space limitations, we leave the derivation of Relaxed Perturb-and-MAP for linear-chain CRFs to
Appendix A and detail the resulting CRF algorithms in Appendix B.
2.4	Proposed Generative Models
We model the prior distribution of tag sequences y1:N as the per-tag product of a fixed categorical
distribution p(y1:N) = Qi p(yi). The KL between qφ and this distribution can be computed in
polynomial time using a modification of the forward recursion derived in Mann & McCallum (2007),
detailed in Appendix B.
We experiment with several variations of architectures for pθ (x1:N |y1:N), presented in order of
increasing complexity.
Baseline - CRF-Autoencoder (AE): The CRF Autoencoder (Ammar et al., 2014; Zhang et al.,
2017) is the previous state-of-the-art semi-supervised linear-chain CRF, which we consider a strong
4This regime applies to situations such as weak supervision (i.e. a low-recall database or gazatteer used for
distant supervision), incidental supervision (i.e., a random Wikipedia sentence), or online and active learning.
4
Under review as a conference paper at ICLR 2020
baseline. This model uses a tractable, fully factored generative model of tokens given tags and does
not require approximate inference. Due to space limitations, we have detailed our implementation
in Appendix C.
MF: This is our simplest proposed generative model. We first embed the relaxed tag samples,
represented as simplex vectors yi ∈ ∆lYl, into Rdyp as the weighted combination of the input vector
representations for each possible tag:
Ui = Uyi, U ∈ Rdyp ×lYl	(7)
We then compute factored token probabilities with an inner product
Pθ (xi∣yi) = σχ (w>i Ui)
where σX is the softmax function normalized over X . This model is generalization of the CRF
Autoencoder architecture in Appendix C where the tag-token parameters θx,y are computed with a
low-rank factorization W U> .
MT: The restrictive factorization of MF is undesirable, since we expect that information about
nearby tags may be discriminative of individual tokens. To test this, we extend MF to use the full
tag context by encoding the embedded tag sequence jointly using a two-layer transformer (Vaswani
et al., 2017) with four attention heads per layer before predicting the tokens independently. That is,
Pθ(xi∣yι^N) = σχ(w>iVi), vi:N = Transformers(ui：N)
MF-GPT2: Next, we see if we can leverage information from a pretrained language model to
provide additional training signal to pθ . We extend MF by adding the fixed pretrained language
modeling parameters from GPT2 to the token scores:
Pθ (xi∣yi,x<i) = σχ
d dGPT2
z>ih
where zxi and hi0 are the input token embeddings and hidden states from GPT2, respectively. We
additionally normalize the scales of the factors by the square root of the vector dimensionalities to
prevent the GPT2 scores from washing out the tag-encoding scores (dyp = 300 and dGPT2 = 768).
MT-GPT2: We add the same autoregressive extention to MT, using the tag encodings v instead of
embeddings U.
pθ (xi |y1:N, x<i) = σX
d dGPT2
z>ih
MT-GPT2-PoE: We also consider an autoregressive extension of MT, similar to MT-GPT2, that
uses a product of experts (PoE) (Hinton, 2002) factorization instead
Pθ (xi∣y,x<i) = σχ (pθ (xilyi：N )pGPT2(x∕x<i))
pθ (xi |y1:N) = σX (wx>i vi), pGPT2(xi|x<i) = σX(zx>ihi0)
MT-GPT2-Residual: Our last variation directly couples GPT2 with pθ by predicting a residual via
a two-layer MLP based on the tag encoding and GPT2 state:
Pθ (xi\yi:N ,x<i ) = σX(Z>i hO), h 0 = h + fMLP(S0,Vii)
For the MF-GPT2, MT-GPT2, and MT-GPT2-PoE models, we choose these factorizations specifi-
cally to prevent the trainable parameters from conditioning on previous word information, removing
the possibility of the model learning to ignore the noisy latent tags in favor of the strong signal
provided by pretrained encodings of the sentence histories (Bowman et al., 2015; Yang et al., 2017;
Kim et al., 2018). We further freeze the GPT2 parameters for all models, forcing the only path for
improving the generative likelihood to be through the improved estimation and encoding of the tags
y1:N.
5
Under review as a conference paper at ICLR 2020
3	Experiments and Results
We experiment first with the proposed models generative models for SSL and PSL in a moderately
resourced regime (keeping 10% labeled data) to explore their relative merits. We then evaluate
our best generative model from these experiments, (MT), with an improved bidirectional encoder
language model in a low- and high-resource settings, varying the amount of unlabeled data.5
For data, we use the OntoNotes 5 (Hovy et al., 2006) NER corpus, which consists of 18 entity types
annotated in 82,120 train, 12,678 validation, and 8,968 test sentences.
3.1	Exploration of Generative Architectures
We begin by comparing the proposed generative models, M* along with the following baselines:
1.	Supervised (S): The supervised tagger trained only on the 10% labeled data.
2.	Supervised 100% (S*): The supervised tagger trained on the 100% labeled data, used for
quantifying the performance loss from using less data.
3.	AE-Exact: The CRF Autoencoder using exact inference (detailed in Appendix C.)
4.	AE-Approx: The same tag-token pair parameterization used by the CRF Autoencoder,
but trained with the approximate ELBO objective as in Equation 11 instead of the exact
objective in Equation 12. The purpose here is to see if we lose anything by resorting to the
approximate ELBO objective.
To simulate moderate-resource SSL, we keep annotations for only 10% of the sentences, yielding
8, 212 labeled sentences with 13, 025 annotated spans and 73, 908 unlabeled sentences. Results are
shown in Table 1. All models except S* use this 10% labeled data.
We first evaluate the proposed models and baselines without the use of a prior, since the use of
a locally normalized factored prior can encourage overly uncertain joint distributions and degrade
performance (Jiao et al., 2006; Mann & McCallum, 2007; Corro & Titov, 2018). We then explore
the inclusion of the priors for the supervised and MT models with β = 0.01.6
We explore two varieties of prior tag distributions: (1) the “gold” empirical tag distribution (Emp)
from the full training dataset and (2) a simple, but informative, hand-crafted prior (Sim) that places
50% mass on the O tag and distributes the rest of its mass evenly among the remaining tags. We
view (2) as a practical approach, since it does not require knowledge of the gold tag distribution, and
use (1) to quantify any relative disadvantage from not using the gold prior. We find that including
the prior with a small weight, β = 0.01, marginally improved performance and interestingly, the
simple prior outperforms the empirical prior, most likely because it is slightly smoother and does
not emphasize the O tag as heavily.7
Curiously, we found that the approximate training of the CRF Autoencoder AE-Approx outper-
formed the exact approach AE-Exact by nearly 2% F1.
We also note that our attempts to leverage signal from the pretrained autoregressive GPT2 states had
negligible or negative effects on performance, thus we conclude that it is the addition of the joint
encoding transformer architecture MT that provides the most gains (+0.8% F1).
PSL: We also evaluate the supervised and transformer-based generative models, S and MT, on the
more difficult PSL setup, where naively training the supervised model on the marginal likelihood of
observed tags produces a degenerate model, due to the observation bias of never having O tags. In
this setting we drop 90% of the annotations from sentences randomly, resulting in 82,120 incom-
pletely annotated sentences with 12,883 annotations total. We compare the gold and simple priors
for each model. From the bottom of Table 1, we see that again our proposed transformer model MT
outperforms the supervised-only model, this time by +1.3% F1. We also find that in this case, the
MT models need to be trained with higher prior weights β = 0.1, otherwise they diverge towards
using the O tag more uniformly with the other tags to achieve better generative likelihoods.
5Code and experiments are available online at github.com/<anonymizedforsubmission>
6In preliminary SSL experiments we found β > 0.01 to have a negative impact on performance, likely due
to global/local normalization mismatch of the CRF and the prior.
7The empirical prior puts 85% mass on the O tag
6
Under review as a conference paper at ICLR 2020
	Model		α	β	P	R	F1
Supervised 100% (S*)	-	0.0	0.808	0.798	0.803
Supervised (S)	-	0.0	0.761	0.738	0.749
AE-Exact	-	0.0	0.736	0.721	0.728
AE-Approx	0.1	0.0	0.767	0.728	0.747
MF (Factored)	ɪr	0.0	0.761	0.719	0.739
MT (Transformer)	0.1	0.0	0.758	0.756	0.757
MF-GPT2	0.1	0.0	0.754	0.710	0.731
MT-GPT2	0.1	0.0	0.762	0.755	0.759
MT-GPT2 (no-scale)	0.1	0.0	0.766	0.734	0.751
MT-GPT2-PoE	0.1	0.0	0.766	0.742	0.753
MT-GPT2-Residual	0.1	0.0	0.766	0.740	0.753
S (EmP)	^0ΓΓ"	0.01	0.754	0.733	0.743
S (Sim)	0.1	0.01	0.754	0.734	0.743
MT (Emp)	0.1	0.01	0.760	0.756	0.758
MT (Sim)	0.1	0.01	0.762	0.757	0.760
S (EmP) - PSL	ɪr	0.01	0.741	0.725	0.733
S (Sim) - PSL	0.1	0.01	0.730	0.740	0.735
MT (Emp) - PSL	0.1	0.1	0.731	0.761	0.746
MT (Sim) - PSL	0.1	0.1	0.724	0.774	0.748
Table 1: Semi-supervised and partially-supervised models on 10% supervised training data: best in
bold, second best underlined. The proposed MT* improves performance in SSL and PSL by +1.1%
F1 and +1.3% F1, respectively.
3.2	Varying Resources
Next we explore our best proposed architecture MT and the supervised baseline in low- and high-
resource settings (1% and 100% training data, respectively) and study the effects of training with an
additional 100K unlabeled sentences sampled from Wikipedia (detailed in Appendix E).
Since we found no advantage from using pretrained GPT2 information in the previous experiment,
we evaluate the use of the bidirectional pretrained language model, RoBERTa (Liu et al., 2019b),
since we expect bidirectional information to highly benefit performance (Strubell et al. (2017); Ak-
bik et al. (2018), among others). We also experiment with a higher-capacity tagging model, S-LG,
by adding more trainable Transformers (L = 4, A = 8, H = 1024) between the RoBERTa encod-
ings and down-projection layers.
From Table 2 we see that, like in the 10% labeled data setting, the CRF-VAE improves upon
the supervised model by 0.9% F1 in this 1% setting, but we find that including additional data
from Wikipedia has a negative impact. A likely reason for this is the domain mismatch between
Ontonotes5 and Wikipedia (news and encyclopedia, respectively).
In the high-resource setting, we find that using RoBERTa significantly improves upon GPT2 (+5.7%
F1) and the additional capacity of S-LG further improves performance by +2.2% F1. Although we
do not see a significant improvement from semi-supervised training with Wikipedia sentences, our
model is competitive with previous state-of-the-art NER approaches and outperforms all previous
approaches that do not use additional labeled data or gazetteers.
4	Related Work
Utilizing unlabeled data for semi-supervised learning in NER has been studied considerably in the
literature. A common approach is a two-stage process where useful features are learned from un-
supervised data, then incorporated into models which are then trained only on the supervised data
(Fernandes & Brefeld, 2011; Kim et al., 2015). With the rise of neural approaches, large-scale
word vector (Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining methods
(Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018) can be regarded in the same vein.
7
Under review as a conference paper at ICLR 2020
Model	IDs I	IDu I	α	β	P	R	F1
S	13K	0	-	-	0.744	0.712	0.728
MT (Sim)	13K	82K	0.01	0.1	0.752	0.739	0.737
MT (Sim) + Wiki	13K	182K	0.01	0.1	0.746	0.721	0.733
Strubell et al. (2017)	82K	^^0-	-	-	-	-	0.869
Clark et al. (2018)t	82K	>1M	-	-	-	-	0.888
Chenetal. (2019)	82K	0	-	-	0.878	0.876	0.877
Akbik et al. (2018)t	95K	0	-	-	-	-	0.891
LiU etal. (2019a)t	82K	0	-	-	-	-	0.899
S (GPT2)	82K	^^0-	-	-	0.808	0.798	0.803
S	82K	0	-	-	0.864	0.855	0.860
S-LG	82K	0	-	-	0.873	0.892	0.882
MT-LG (Sim) + Wiki	82K	182K	0.1	0.01	0.880	0.890	0.884
Table 2: Low- and high-resource results with RoBERTa, varying available unlabeled data. Best
scores not using additional labeled data in bold. f Uses additional labeled data or gazetteers.
Another approach is to automatically create silver-labeled data using outside resources, whose low
recall induces a partially supervised learning problem. Bellare & McCallum (2007) approach the
problem by distantly supervising (Mintz et al., 2009) spans using a database. Carlson et al. (2009)
similarly use a gazetteer and adapt the structured perceptron (Collins, 2002) to handle partially
labeled sequences, while Yang et al. (2018) optimize the marginal likelihood (Tsuboi et al., 2008) of
the distantly annotated tags. Yang et al. (2018)’s method, however, still requires some fully labeled
data to handle proper prediction of the O tag. The problem setup from Jie et al. (2019) is the same as
our PSL regime, but they use a cross-validated self-training approach. Greenberg et al. (2018) use a
marginal likelihood objective to pool overlapping NER tasks and datasets, but must exploit dataset-
specific constraints, limiting the allowable latent tags to debias the model from never predicting O
tags.
Generative latent-variable approaches also provide an attractive approach to learning on unsuper-
vised data. Ammar et al. (2014) present an approach that uses the CRF for autoencoding and Zhang
et al. (2017) extend it to neural CRFs, but both require the use of a restricted factored generative
model to make learning tractable. Deep generative models of text have shown promise in recent
years, with demonstrated applications to document representation learning (Miao et al., 2016), sen-
tence generation (Bowman et al., 2015; Yang et al., 2017; Kim et al., 2018), compression (Miao
& Blunsom, 2016), translation (Deng et al., 2018), and parsing (Corro & Titov, 2018). However,
to the best of our knowledge, this framework has yet to be utilized for NER and tagging CRFs. A
key challenge for learning VAEs with discrete latent variables is optimization with respect to the
inference model parameters φ. While we may appeal to score function estimation (Williams, 1992;
Paisley et al., 2012; Ranganath et al., 2014; Miao & Blunsom, 2016), its empirical high-variance
gradients make successful optimization difficult. Alternatively, obtaining gradients with respect to
φ can be achieved using the relaxed Gumbel-max trick (Jang et al., 2016; Maddison et al., 2016)
and has been recently extended to latent tree-CRFs by (Corro & Titov, 2018), which we make use
of here for sequence CRFs.
5	Conclusions
We proposed a novel generative model for semi-supervised learning in NER. By treating a neural
CRF as the amortized variational posterior in the generative model and taking relaxed differentiable
samples, we were able to utilize a transformer architecture in the generative model to condition on
more context and provide appreciable performance gains over supervised and strong baselines on
both semi-supervised and partially-supervised datasets. We also found that inclusion of powerful
pretrained autoregressive language modeling states had neglible or negative effects while using a
pretrained bidirectional encoder offers significant performance gains. Future work includes the use
of larger in-domain unlabeled corpora and the inclusion of latent-variable CRFs in more interesting
joint semi-supervised models of annotations, such as relation extraction and entity linking.
8
Under review as a conference paper at ICLR 2020
References
Alan Akbik, Duncan Blythe, and Roland Vollgraf. Contextual string embeddings for sequence
labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pp.
1638-1649, 2018.
Waleed Ammar, Chris Dyer, and Noah A Smith. Conditional random field autoencoders for un-
supervised structured prediction. In Advances in Neural Information Processing Systems, pp.
3311-3319, 2014.
Kedar Bellare and Andrew McCallum. Learning extractors from unlabeled text using relevant
databases. In Sixth international workshop on information integration on the web, 2007.
James O Berger. Statistical decision theory and Bayesian analysis; 2nd ed. Springer Series in
Statistics. Springer, New York, 1985. doi: 10.1007/978-1-4757-4286-2. URL https://cds.
cern.ch/record/1327974.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statis-
tics). Springer-Verlag, Berlin, Heidelberg, 2006. ISBN 0387310738.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American Statistical Association, 112(518):859-877, 2017.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
Andrew Carlson, Scott Gaffney, and Flavian Vasile. Learning a named entity tagger from gazetteers
with the partial perceptron. In AAAI Spring Symposium: Learning by Reading and Learning to
Read, pp. 7-13, 2009.
Hui Chen, Zijia Lin, Guiguang Ding, Jianguang Lou, Yusen Zhang, and Borje Karlsson. Grn:
Gated relation network to enhance convolutional neural network for named entity recognition. In
Proceedings of AAAI, 2019.
Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. Semi-supervised se-
quence modeling with cross-view training. arXiv preprint arXiv:1809.08370, 2018.
Michael Collins. Discriminative training methods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical meth-
ods in natural language processing-Volume 10, pp. 1-8. Association for Computational Linguis-
tics, 2002.
Caio Corro and Ivan Titov. Differentiable perturb-and-parse: Semi-supervised parsing with a struc-
tured variational autoencoder. arXiv preprint arXiv:1807.09875, 2018.
Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. Latent alignment and
variational attention. In Advances in Neural Information Processing Systems, pp. 9712-9724,
2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Eraldo R Fernandes and Ulf Brefeld. Learning from partially annotated sequences. In Joint Eu-
ropean Conference on Machine Learning and Knowledge Discovery in Databases, pp. 407-422.
Springer, 2011.
Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew
Peters, Michael Schmitz, and Luke Zettlemoyer. Allennlp: A deep semantic natural language
processing platform. arXiv preprint arXiv:1803.07640, 2018.
Nathan Greenberg, Trapit Bansal, Patrick Verga, and Andrew McCallum. Marginal likelihood train-
ing of bilstm-crf for biomedical named entity recognition from disjoint label sets. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2824-2829,
2018.
9
Under review as a conference paper at ICLR 2020
Emil Julius Gumbel. Statistical theory of extreme values and some practical applications: a series
of lectures, volume 33. US Government Printing Office, 1954.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
volume 3, 2017.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation,14(8):1771-1800, 2002.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. Ontonotes:
The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL-Short ’06, pp. 57-60, Stroudsburg, PA, USA, 2006.
Association for Computational Linguistics. URL http://dl.acm.org/citation.cfm?
id=1614049.1614064.
Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.
arXiv preprint arXiv:1801.06146, 2018.
Inc. Hugging Face. PyTorch Pretrained BERT: The Big & Extending Repository of
pretrained Transformers, May 2019. URL https://github.com/huggingface/
pytorch-pretrained-BERT.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. Semi-supervised
conditional random fields for improved sequence segmentation and labeling. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pp. 209-216. Association for Computational
Linguistics, 2006.
Zhanming Jie, Pengjun Xie, Wei Lu, Ruixue Ding, and Linlin Li. Better modeling of incomplete
annotations for named entity recognition. In Proceedings of NAACL, 2019.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. Structured attention networks.
arXiv preprint arXiv:1702.00887, 2017.
Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized
variational autoencoders. arXiv preprint arXiv:1802.02550, 2018.
Yoon Kim, Alexander M Rush, Lei Yu, AdhigUna Kuncoro, Chris Dyer, and Gabor Melis. UnsUPer-
vised recurrent neural network grammars. arXiv preprint arXiv:1904.03746, 2019.
Young-Bum Kim, Minwoo Jeong, Karl Stratos, and Ruhi Sarikaya. Weakly suPervised slot tagging
with Partially labeled sequences from web search click logs. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, PP. 84-92, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. arXiv preprint
arXiv:1412.6980, 2014.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-suPervised
learning with deeP generative models. In Advances in neural information processing systems, PP.
3581-3589, 2014.
John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence data. 2001.
10
Under review as a conference paper at ICLR 2020
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360, 2016.
Tianyu Liu, Jin-ge Yao, and Chin-Yew Lin. Towards improving neural named entity recognition
with gazetteers. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics,pp. 5301-5307, 2019a.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019b.
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv
preprint arXiv:1603.01354, 2016.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Gideon S Mann and Andrew McCallum. Efficient computation of entropy gradient for semi-
supervised conditional random fields. In Human Language Technologies 2007: The Conference
of the North American Chapter of the Association for Computational Linguistics; Companion
Volume, Short Papers, pp. 109-112. Association for Computational Linguistics, 2007.
Arthur Mensch and Mathieu Blondel. Differentiable dynamic programming for structured prediction
and attention. arXiv preprint arXiv:1802.03676, 2018.
Yishu Miao and Phil Blunsom. Language as a latent variable: Discrete generative models for sen-
tence compression. arXiv preprint arXiv:1609.07317, 2016.
Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In Interna-
tional conference on machine learning, pp. 1727-1736, 2016.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013.
Laurence B Miller. Monte carlo analysis of reactivity coefficients in fast reactors general theory and
applications. Technical report, Argonne National Lab.(ANL), Argonne, IL (United States), 1967.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:
Volume 2-Volume 2, pp. 1003-1011. Association for Computational Linguistics, 2009.
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient esti-
mation in machine learning. arXiv preprint arXiv:1906.10652, 2019.
John Paisley, David Blei, and Michael Jordan. Variational bayesian inference with stochastic search.
arXiv preprint arXiv:1206.6430, 2012.
George Papandreou and Alan L Yuille. Perturb-and-map random fields: Using discrete optimization
to learn and sample from energy models. In 2011 International Conference on Computer Vision,
pp. 193-200. IEEE, 2011.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,
2018.
11
Under review as a conference paper at ICLR 2020
Martin PoPel and Ondfej Bojar. Training tips for the transformer model. The Prague Bulletin of
Mathematical Linguistics,110(1):43-70, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1:8, 2019.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
Intelligence and Statistics, pp. 814-822, 2014.
Lev Ratinov and Dan Roth. Design challenges and misconceptions in named entity recognition. In
Proceedings of the thirteenth conference on computational natural language learning, pp. 147-
155. Association for Computational Linguistics, 2009.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by
back-propagating errors. Cognitive modeling, 5(3):1, 1988.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y Ng. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on
empirical methods in natural language processing, pp. 254-263. Association for Computational
Linguistics, 2008.
Emma Strubell, Patrick Verga, David Belanger, and Andrew McCallum. Fast and accurate entity
recognition with iterated dilated convolutions. arXiv preprint arXiv:1702.02098, 2017.
Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke Mori, and Yuji Matsumoto. Training con-
ditional random fields using incomplete annotations. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1, pp. 897-904. Association for Computational
Linguistics, 2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and TrendsR in Machine Learning, 1(1-2):1-305, 2008.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Yaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, and Min Zhang. Distantly super-
vised ner with partial annotation learning and reinforcement learning. In Proceedings of the 27th
International Conference on Computational Linguistics, pp. 2159-2169, 2018.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. In Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70, pp. 3881-3890. JMLR. org, 2017.
Xiao Zhang, Yong Jiang, Hao Peng, Kewei Tu, and Dan Goldwasser. Semi-supervised structured
prediction with neural crf autoencoder. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 1701-1711, 2017.
12
Under review as a conference paper at ICLR 2020
A Relaxed Perturb-and-MAP for Linear Chain CRFs
Let qφ(y∣χ; T) be the distribution on y with the potentials ψ for each tag at each position perturbed
by GUmbel noise Y iid G(0,1) (Gumbel, 1954) and T ≥ 0 be the temperature:
eχp {(P ψyi,yi+ι + Yyi"τ}
qΦ(y∖x; T) =-------------i=0N-------------------------
P eχp {( P ψy0 ,y0+ι + Yy0 )/t )}
y10 :N	i=0
We know from Papandreou & Yuille (2011) that the MAP sequence from this perturbed distribution
is a sample from the unperturbed distribution. Coupled with the property that the zero temperature
limit of the Gibbs distribution is the MAP state (Wainwright et al., 2008), it immediately follows
that the zero temperature limit of the perturbed q is a sample from q:
y = arg maχ qφ(y∣x; τ)	(8)
y∈Y
lim qφ(y∣x; T) = one-hot(argmaxqφ(y∣x))	(9)
τ→0	y∈Y
⇒ lim qφ(y∣x; T) = one-hot(y)	(10)
τ→0
where qφ(y∣χ; T) is the tempered but unperturbed q@ and “one-hot” is a function that converts ele-
ments of Y N to a one-hot vector representation.
Thus We can use the temperature T to anneal the perturbed joint distribution Qφ(y∣χ; T) to a sample
from the unperturbed distribution, y 〜 qφ. When t > 0, qφ(y∣χ; T) is differentiable and can
be used for end-to-end optimization by allowing us to approximate the expectation with a relaxed
single-sample Monte Carlo estimate:
Eqφ(y∣χ)[iogpθ(x|y)] ≈ logpθ(χ∖qφ(y∖χ; t))	(ii)
where we have modified logpθ (χ∖y) to accept the simplex representations of yi：N from q@ instead of
discrete elements, which has the effect oflogpθ(x∖y) computing a weighted combination of its input
vector representations for y ∈ Y similarly to an attention mechanism or the annotation function in
Kim et al. (2017) (see Equation 7.)
This can be thought of as a generalization of the Gumbel-softmax trick from Jang et al. (2016);
Maddison et al. (2016) to structured joint distributions.
The statements in (8-10) also imply something of practical interest: we can compute (1) the argmax
(Viterbi decoding) and its differentiable relaxation; (2) a sample and its differentiable relaxation;
(3) the partition function; and (4) the marginal tag distributions, all using the same sum-product
algorithm implementation, controlled by the temperature and the presence of noise. We have detailed
the algorithm in Appendix B.
B CRF Algorithms
In Algorithm 1 we have detailed the stable, log-space implementation of the generalized forward-
backward algorithm for computing (1) the argmax (Viterbi decoding) and its differentiable relax-
ation; (2) a sample and its differentiable relaxation; (3) the partition function; and (4) the marginal
tag distributions below. While this algorithm does provide practical convenience, we note that real
implementations should have separate routines for computing the partition function (running only
the forward algorithm), and the discrete T = 0 Viterbi algorithm, since it is more numerically stable
and efficient.
We also have included the dynamic program for computing the constrained KL divergence between
qφ and a factored p(y ) in Algorithm 2.
C CRF Autoencoder
The idea of using a CRF to reconstruct tokens given tags for SSL has been explored before by
Ammar et al. (2014); Zhang et al. (2017), which we consider to be a strong baseline and restate
13
Under review as a conference paper at ICLR 2020
Algorithm 1 Relaxed, Constrained, Perturbed Forward-Backward
Notation: LSE := log Px exp
xx
Input: Local potentials ΨyiN, transition potentials Ψy,y∕, perturb boolean, temperature T, the spe-
cial start symbol and end symbols *, ◊, and the set of allowable tags for each position Yi ⊆ Y
(allows for partially observed/constrained sequences.)
Procedure:
1:	log α[0, y] J ψ*,y/τ, log β[N +1,y] J ψy,jτ	. Initialize recursions bases
2:	if perturb then
3:	ψyi J ψyi + Yyi, Yyi 吧 G(0,1)	. PertUrb local potentials
4:	end if
5:	for i = 1, . . . , N do	. Compute forward lattice
6:	for y ∈ Yi do
7:	log α[* i, 2 3 4 5 6 7 8 9 10 y] J LSE (ψy0,y + ψy )/T + log α[i - 1,y0]
y0∈Yi-1
8:	end for
9:	end for
10:	for i = N, . . . , 1 do	. Compute backward lattice
11:	for y ∈ Yi do
12:	log β[i,y] J LSE (ψy,y0 + ψy)∕τ + log β[i+1,y0]
y0 ∈Yi+1
13:	end for
14:	end for
15:	μyi J σγi ( LSE log α[i,yi] + (ψyi + ψyi,yi+J∕τ + logβ[i + 1,yi+ι]) . Tag marginals
yi+1∈Yi+1
Output:
If perturb then
Rel	axed sample ⅞φ(y∣χ; T) J μy^N	. Converges to sample at T = 0
Else if τ = 1 then
Partition function Z(ψ) J P exp{ψy0, + log α[N, y0]}
y0∈YN
Tag marginals qφ(y∣χ) J μy>N
Else
Rel	axed argmax qφ(y∣χ; T) J μyiN	. Converges to Viterbi at T = 0
Algorithm 2 Constrained KL
Notation: LSE := log Px exp
xx
Input: Local potentials Ψyι:N, transition potentials Ψy,y0, prior distributionp(yi：N) = Qp(yi), the
special start symbol and end symbols *, ◊, and the set of allowable tags for each position Yi ⊆ Y
(allows for partially observed/constrained sequences.)
Procedure:
1: log α[0,	y]	J ψ*,y,	KLα[0,	y]	J 0	∀y	∈	Yi	. Initialize recursions bases
2: for i = 1, . . . , N do
3:	for yi ∈ Yi do	. Same as forward algorithm
4:	logα[i,yi] J LSE (ψyi-1,yi +ψyi) + log α[i - 1, yi-1]
yi-1 ∈Yi-1
5:	end for
6:	for yi+1 ∈ Yi+1 do	. Compute KL lattice
7:	q(yi|yi+1) J σYi (log α[i, yi] +ψyi,yi+1 +ψyi+1)
8:	KLα[i,yi+1] J P q(yi|yi+1) log q(yi|yi+1) - logp(yi) + KLα[i - 1,yi]
yi∈Yi
9:	end for
10: end for
Output: KL(q||p) J KLα [N, ◊]
14
Under review as a conference paper at ICLR 2020
here for clarity. Termed the CRF Autoencoder, the model treats the the tags as intermediate latent
variables in a conditional model and optimizes the marginal conditional likelihood of reconstructing
the input.
logp(X∣χ) = log £ pθ(X|yi：N)q@(y1.N|x)
y1:N
By judiciously choosing pθ(X|y) to be factored among positions i, We can compute the marginal
reconstruction likelihood exactly:
N
logp(X∣x) = log E qφ(yi:N|x) ɪɪpθ(Xi∣yi)
y1:N	i=1
N
=	log£exp{£ Ψyi,yi+1 + logPθ(Xi∣yi)} - log Z(ψ)
y1:N	i=0
=	log Z(ψ + logpθ) - log Z(ψ)	(12)
Where log Z(ψ + log pθ ) is a slight abuse of notation intended to illustrate that the first term in
Equation 12 is the same computation as the partition function, but With the generative log-likelihoods
added to the CRF potentials.
We note that instead of using the Mixed-EM procedure from Zhang et al. (2017), we model pθ (Xi | yi)
using free logit parameters θx,y for each token-tag pair and normalize using a softmax, Which alloWs
for end-to-end optimization via backpropagation and SGD.
D	Experiment Hyperparameter and Optimization Settings
We train each model to convergence using early-stopping on the F1 score of the validation data, with
a patience of 10 epochs. For all models that do not have trainable transformers, we train using the
Adam optimizer (Kingma & Ba, 2014) with a learning rate of 0.001, and a batch size of 128. For
those with transformers (MT*), we train using Adam, a batch size of 32, and the Noam learning
rate schedule from Vaswani et al. (2017) with a model size of dyp = 300 and 16, 000 warm-up steps
(Popel & Bojar, 2018).
Additionally, we use gradient clipping of 5 for all models and a temperature of τ = .66 for all
relaxed sampling models. We implemented our models in PyTorch (Paszke et al., 2017) using the
AllenNLP framework (Gardner et al., 2018) and the Hugging Face (2019) implementation of the
pretrained GPT2 and RoBERTa.
We have made all code, data, and experiments available online at github.com/
<anonymizedforsubmission> for reproducibility and reuse. All experimental settings can
be reproduced using the configuration files in the repo.
E Gathering Additional Unlabeled Data
For the experiments in §3.2, we gather an additional training corpus of out-of-domain encyclopedic
sentences from Wikipedia. To try to get a sample that better aligns with the Ontonotes5 data, these
sentences were gathered with an informed process, which was performed as follows:
1.	Using the repository <anonymized for submission>, we extract English Wikipedia and
align it with Wikidata.
2.	We then look up the entity classes from the Ontonotes5 specification (Hovy et al., 2006)
in Wikidata and, for each NER class, find all Wikidata classes that are below this class in
ontology (all subclasses).
3.	We then find all items which are instances of these classes and also have Wikipedia pages.
These are the Wikipedia entities which are likely to be instances of the NER classes.
15
Under review as a conference paper at ICLR 2020
4.	Finally, we scan Wikipedia, mapping any available links to these NER classes, and keep
the top 100K sentences according to the number of found annotations/token - the most
”densely” annotated sentences.
16