Under review as a conference paper at ICLR 2020
QXplore: Q-learning Exploration by Maximiz-
ing Temporal Difference Error
Anonymous authors
Paper under double-blind review
Ab stract
A major challenge in reinforcement learning is exploration, especially when reward
landscapes are sparse. Several recent methods provide an intrinsic motivation to
explore by directly encouraging agents to seek novel states. A potential disad-
vantage of pure state novelty-seeking behavior is that unknown states are treated
equally regardless of their potential for future reward. In this paper, we propose an
exploration objective using the temporal difference error experienced on extrinsic
rewards as a secondary reward signal for exploration in deep reinforcement learn-
ing. Our objective yields novelty-seeking in the absence of extrinsic reward, while
accelerating exploration of reward-relevant states in sparse (but nonzero) reward
landscapes. We implement the objective with a two-policy Q-learning method in
which Q and Qx are the action-value functions for extrinsic and secondary rewards,
respectively. Secondary reward is given by the absolute value of the TD-error of
Q. Training is off-policy, based on a replay buffer containing a mix of trajectories
sampled using Q and Qx . We characterize performance on a set of continuous
control benchmark tasks, and demonstrate comparable or faster convergence on all
tasks when compared with other state-of-the-art exploration methods.
1	Introduction
Deep reinforcement learning (RL) has recently achieved impressive results across several challenging
domains, such as playing games (Mnih et al., 2016; Silver et al., 2017; OpenAI, 2018; Baker et al.,
2019) and controlling robots (OpenAI et al., 2018; Kalashnikov et al., 2018). In many of these
tasks, a well-shaped reward function is critical to learning performant policies. On the other hand,
deep RL still remains challenging for tasks where the reward function is sparse. In these settings,
state-of-the-art RL methods often perform poorly and train very slowly, if at all, due to the low
probability of observing improved rewards by following the current optimal policy or with a naive
exploration policy such as -greedy sampling.
The challenge of learning from sparse rewards is typically framed as a problem of exploration,
inspired by the notion that a successful RL agent must efficiently explore the state space of its
environment in order to find improved sources of reward. One common exploration paradigm is to
directly determine the novelty of states and to encourage the agent to visit states with the highest
novelty. In small MDPs this can be achieved through counting how many times each state has been
visited. This approach often performs poorly in high-dimensional or continuous state spaces, but
recent work (Tang et al., 2017; Bellemare et al., 2016; Fu et al., 2017) using count-like statistics have
shown success on benchmark tasks with complex state spaces. Another paradigm for exploration
learns a dynamic model of the environment and computes a novelty measure proportional to the
error of the model in predicting transitions in the environment. This exploration method relies on
the core assumption that well-modeled regions of the state space are similar to previously visited
states and thus are less interesting than other regions of state space. Predictions of the transition
dynamics can be directly computed (Pathak et al., 2017; Stadie et al., 2015; Savinov et al., 2019;
Burda et al., 2019a), or related to an information gain objective on the state space, as described in
VIME (Houthooft et al., 2016) and EMI (Kim et al., 2018).
Several exploration methods have recently been proposed that capitalize on the function approxima-
tion properties of neural networks. Random network distillation (RND) trains a function to predict
the output of a randomly-initialized neural network from an input state, and uses the approximation
1
Under review as a conference paper at ICLR 2020
error as a reward bonus for a separately-trained RL agent (Burda et al., 2019b). Similarly, DORA
(Fox et al., 2018) trains a network to predict zero on observed states and deviations from zero are
used to indicate unexplored states.
An important shortcoming of existing exploration methods is that they only incorporate information
about states and therefore assume all unobserved states are equally motivating, regardless of their
viability for future reward. The viability of this assumption is highly task dependent: While games
like Montezuma’s Revenge or Super Mario Bros, where novelty correlates highly with success, can
be attacked effectively by state novelty methods alone (Burda et al., 2019b; Pathak et al., 2017;
Ecoffet et al., 2019; Kim et al., 2018), other tasks such as hide-and-seek or some Atari games where
novelty and utility are less correlated tend to frustrate state novelty methods (Burda et al., 2019b;
Baker et al., 2019; Burda et al., 2019a). Baker et al. (2019) explored using both RND and a simple
state counting baseline to discover skills such as navigation and block-pushing in a hide-and-seek
environment. However, the authors found that careful construction of the state representation used
for novelty seeking was necessary to discover any such skills, as novelty in the full state space did
not correspond to novelty in the intuitive sense (Baker et al., 2019).
Instead of focusing on the state-space, this work uses the temporal difference error (TD-error) which
provides a signal into novelty in the reward landscape. Past works have also utilized information
from the reward landscape as a learning signal to various extents. Schmidhuber et. al. first describe
using reward misprediction and model prediction error for exploration (Schmidhuber, 1991; Thrun &
Moller, 1991; 1992). However, the work was primarily concerned with model-building and system-
identification in small MDPs, and used reward prediction error rather than TD-error. Later, Gehring
& Precup (2013) used TD-error as a negative signal to constrain exploration to focus on states that
are well understood by the value function to avoid common failure modes. Related to maximizing
TD-error is maximizing the variance or KL-divergence of a posterior distribution over MDPs or
Q-functions, which can be used as a measure of uncertainty (Osband & Van Roy, 2017; O’Donoghue
et al., 2017; Chen et al., 2017; Fox et al., 2018; Osband et al., 2018). Posterior uncertainty over
Q-functions can be used for information gain in the reward or Q-function space, as opposed to
information gain in the state space as described by VIME among others (Houthooft et al., 2016;
Kim et al., 2018), though posterior uncertainty methods have thus-far largely been used for local
exploration as an alternative to dithering methods such as -greedy sampling, though Osband et al.
(2018) do apply posterior uncertainty to Montezuma’s Revenge.
In this paper we propose QXplore, a new exploration formulation that seeks novelty in the predicted
reward landscape instead of novelty in the state space. QXplore exploits the inherent reward-space
signal from the computation of temporal difference error (TD-error) in value-based RL, and explicitly
promotes visiting states where the current understanding of reward dynamics is poor. In the following
sections, we describe QXplore and demonstrate its utility for efficient learning on a variety of complex
benchmark environments with continuous controls and sparse rewards.
2	Preliminaries
We consider RL in the terminology of Sutton & Barto (1998), in which an agent seeks to maximize
reward in a Markov Decision Process (MDP). An MDP consists of states s ∈ S, actions a ∈ A, a
state transition function S : S × A × S → [0, 1] giving the probability of moving to state st+1 after
taking action at from state st for discrete timesteps t ∈ 0, ..., T. Rewards are sampled from reward
function r : S × A → R. An RL agent has a policy π(st, at) = p(at|st) that gives the probability of
taking action at when in state st . The agent aims to learn a policy to maximize the expectation of the
time-deCayed sum of reward R∏ (so) = PT=0 Ytr(st, at) Where at 〜∏(st, at).
A value function Vθ(st) with parameters θ is a function which computes Vθ(st ) ≈ Rπ(st) for some
policy π. Temporal Difference (TD) error δt measures the bootstrapped error between the value
function at the current timestep and the next timestep as
δt = Vθ(St) - (r(st,at 〜∏(st)) + γVθ(st+ι))∙	(1)
A Q-function is a value function of the form Q(st, at), which computes Q(st, at) = r(st, at) + Y ∙
maxa0Q(st+1, a0), the expected future reward assuming the optimal action is taken at each future
timestep. An approximation to this optimal Q-function Qθ with some parameters θ may be trained
using a mean squared TD-error objective Lq@ = ∣∣Qθ (st,at) - (r(st, at) + Y ∙ max。，Q% (st+ι, a0))∣∣2
2
Under review as a conference paper at ICLR 2020
given some target Q-function Q0θ0, commonly a time-delayed version of Qθ (Mnih et al., 2015).
Extracting a policy π given Qθ amounts to approximating argmaxaQθ (st , a). Many methods exist
for approximating the argmaxa operation in both discrete and continuous action spaces (Lillicrap
et al., 2015; Haarnoja et al., 2018). Following the convention of Mnih et al. (2016), we train Qθ using
an off-policy replay buffer of previously visited (s, a, r, s0) tuples, which we sample uniformly.
3	QXplore: TD-Error as Adversarial Reward Signal
3.1	Method Overview
Uo=OB-I əw-
Leam-ng
Figure 1: Method diagram for QXplore. We define two Q-functions which sample trajectories from
their environment and store experiences in separate replay buffers. Q is a standard state-action value-
function, whereas Qx’s reward function is the unsigned temporal difference error of the current Q on
data sampled from both replay buffers. A policy defined by Qx samples experiences that maximize
the TD-error of Q, while a policy defined by Q samples experiences that maximize discounted reward
from the environment.
We first provide an overview of the method - a visual representation is depicted in Figure 1. At a high
level, QXplore is an exploration method that jointly trains two independent agents equipped with
their own Q-functions and reward functions:
1.	Q: A standard Q-function, that learns a value function on reward provided by the external
environment.
2.	Qx : A Q-function that learns a value function directly on the TD-error of Q.
The policy πQx that samples Qx and the Q-function Q form an adversarial pair, wherein πQx seeks
to sample state-action pairs that produce large TD-errors while Q’s training objective LQθ attempts
to minimize the TD-error for previously sampled state-action pairs. Thus, πQx achieves reward when
the agent ventures into states whose reward dynamics are foreign to Q (i.e. Q under/overestimates
reward achieved). Separate replay buffers are maintained for each agent, but each agent receives
samples from both buffers at train time. A similar adversarial sampling scheme was used to train
an inverse dynamics model by Hong et al. (2018), and Colas et al. (2018) use separate goal-driven
exploration and reward maximization phases for efficient learning, but to our knowledge parallel
adversarial sampling policies have not previously been used for exploration.
3.2	TD-error objective
We directly treat TD-error as a reward signal and use a Q-function trained on this signal to induce an
exploration policy, rather than as a supplementary objective or to compute a confidence bound. Cru-
cially, when combined with neural network function approximators, this signal provides meaningful
exploration information everywhere as discussed in Section 3.4. For a value function with parameters
θ, and TD-error δt we define our exploration reward function as
rχ,θ(st,at,st+ι) = ∣δt| = ∣Qθ(st,at) - (rE(st,at) + Ymax。，Q%(st+ι,a0))∣	(2)
for some extrinsic reward function rE and target Q-function Q0θ . Notably, we use the absolute value
of the temporal difference (rather than the squared error) used to compute updates for Qθ to keep the
magnitudes of rE and rx comparable and reduce the influence of outlier temporal differences on the
gradients of Qx, which we describe below.
3
Under review as a conference paper at ICLR 2020
Algorithm 1 QXplore Algorithm
Input: MDP S, Q-function Qθ with target Q0θ0, Qx function Qx,φ with target Q0x,φ0, replay buffers
ZQ and ZQx, batch size B and sampling ratios RQ and RQx, CEM policies πQ and πQx, time
decay parameter γ, soft target update rate τ, and environments EQ, EQx
while not converged do
Reset EQ , EQx
while EQ and EQx are not done do
Sample environments
ZQ — (S,a,r, SO)〜nQ|EQ
ZQx — (S,a,r, SO)〜πQχ |EQx
Sample minibatches for Qθ and Qx,φ
(sq, aQ, rQ, soq) J B * RQ samples from ZQ and B * (1 - RQ) samples from ZQx
(SQx ,aQx ,rQx ,s0Qx) J B *Rqx samples from ZQx and B * (1 -RQx) samples from ZQ
Train
rχ,θ J 1qθ(sQx ,aQx ) — (rQx + YQθo (SOQx KQIsQx )))1
LQ j llQθ(SQ, αQ) - (rQ + YQθo(s'q,πQ(SOQ)))||2
LQx j ||Qx,。(SQx,aQx) - (rχ,θ + YQX"(SOQx,πQx(s0Qx)))ll2
Update θ α LQ
Update φ α LQx
θOJ (1 -τ)θO+τθ
φO J (1 - τ)φO + τφ
end while
end while
Intuitively, a policy maximizing the expected sum of rx will sample trajectories where Qθ does not
have an accurate estimate of the future rewards it will experience. This is useful for exploration
because rx will be large not only for state-action pairs producing unexpected reward, but for all state-
action pairs leading to such states, providing a much denser exploration reward function. Further,
TD-error-based exploration with a dedicated exploration policy removes the exploration-versus-
exploitation tradeoff that state-novelty methods must contend with, where trajectories maximizing
state novelty often do not also maximize reward. Separate exploration and exploitation policies allow
us to sample trajectories maximizing rx that provide information about the task for Qθ to train on
without impacting its ability to maximize reward.
3.3	Qx: LEARNING A Q-FUNCTION TO MAXIMIZE TD-ERROR
Next, we will describe how we use the TD-error signal defined in Section 3.2 to define an exploration
policy. The reward function rx is generic, and can be maximized by any RL algorithm. However,
given its derivation from a bootstrapped Q-function, training a second Q-function to maximize rx
allows the entire algorithm to be trained off-policy with two replay buffers that share data between Qθ
and the Q-function maximizing rx, which we term Qx. This approach is beneficial for exploration, as
it avoids needing to trade off between exploration and exploitation via a weighting hyperparameter,
and sharing data between replay buffers improves data efficiency for training both Q-functions.
We define a Q-function, Qx,φ(S, a) with parameters φ, whose reward objective is rx. We train Qx,φ
using the standard bootstrapped loss function
LQx,φ = IlQx,φ(%, at) - (rx(St, at, St+1) + Ymaxa0QX,φ0 (St+1, a))“2.	⑶
The two Q-functions, Qθ and Qx, are trained off-policy in parallel, sharing replay data so that Qθ can
train on sources of reward discovered by Qx and so that Qx can better predict the TD-errors of Qθ.
Since the two share data, πQx acts as an adversarial teacher for Qθ , sampling trajectories that produce
high TD-error under Qθ and thus provide novel information about the reward landscape. To avoid
off-policy stability issues due to the different reward objectives, we sample a fixed ratio of experiences
collected by each policy for each training batch. We use a nonparametric cross-entropy method policy
inspired by Kalashnikov et al. (2018), previously described as more robust to hyperparameter variance
(Simmons-Edler et al., 2019; Kalashnikov et al., 2018). We also experimented with a variant using
4
Under review as a conference paper at ICLR 2020
DDPG-style parametric policies (Lillicrap et al., 2015) for both Qθ and Qx , but found preventing
sampling collapse by Qθ's policy difficult. Our full method is shown in Figure 1, and pseudocode in
Algorithm 1.
3.4	State Novelty from Neural Network Function Approximation Error
A key question in using TD-error for exploration is: What happens when the reward landscape is flat?
Theoretically, in the case that ∀(s, a), r(s, a) = C for some constant C ∈ R, an optimal Q-function
which generalizes perfectly to unseen states will, in the infinite time horizon case, simply output
∀(s, a), Q?(s, a) = Pt∞=0 Cγt. This results in a TD-error of 0 everywhere and thus no exploration
signal. However, using neural network function approximation, we find that perfect generalization
to unseen states-action pairs does not occur, and in fact observe in Figure 2 that the distance of a
new datapoint from the training data manifold correlates with the magnitude of the network output’s
deviation from Pt∞=1 Cγt and thus with TD-error. As a result, in the case where the reward landscape
is flat TD-error exploration converges to a form of state novelty exploration. This property of neural
network function approximation has been used by several previous exploration methods to good effect,
including RND (Burda et al., 2019b) and DORA (Fox et al., 2018). In particular, the exploration
signal used by RND (extrapolation error from fitting the output of a random network) should be
analogous to rx (extrapolation error from fitting a constant value), meaning we should expect to
perform comparably to RND in the worst case where no extrinsic reward exists.
Figure 2: A neural network trained to predict a constant value does not interpolate or extrapolate
well outside its training range, which can be exploited for exploration. Predictions of 3-layer MLPs
of 256 hidden units per layer trained to imitate f (x) = 0 on R → R with training data sampled
uniformly from the range [-0.75, -0.25] ∪ [0.25, 0.75]. Each line is the final response curve of an
independently trained network once its training error has converged (MSE < 1e-7).
4	Experiments
We performed several experiments to demonstrate the effectiveness of Qx on continuous control
benchmark tasks. We compare QXplore with a related state of the art state novelty-based method,
RND (Burda et al., 2019b), DORA (Fox et al., 2018), and with -greedy sampling as a simple baseline.
Each method is implemented in a shared code base on top of TD3 Fujimoto et al. (2018b) using a
cross entropy method policy as proposed by Qt-Opt Kalashnikov et al. (2018) for hyperparameter
stability. We also compare to results from several previous works on SparseHalfCheetah.
Finally, we present several ablations to QXplore, as well as analysis of its robustness in response to
several hyperparameters. Implementation details and hyperparameters for QXplore, RND, DORA,
and -greedy can be found in Appendix A.
4.1	Experimental Setup
We benchmark on four continuous control tasks using the MuJoCo physics simulator that each require
exploration due to sparse rewards. First, the SparseHalfCheetah task originally proposed by
VIME (Houthooft et al., 2016). Next, we benchmark on three OpenAI gym tasks, FetchPush,
FetchSlide and FetchPickAndPlace, originally developed for goal-directed exploration
methods such as HER (Andrychowicz et al., 2017). We chose these tasks as they are challenging
exploration problems that are relatively simple to control, but still involve large continuous state
spaces and in the case of the Fetch tasks learning to generalize across random object/goal posi-
tions. For consistent reward shaping across tasks we used a reward function in the range [-1-0] for
5
Under review as a conference paper at ICLR 2020
SparseHalfCheetah similar to the Fetch tasks, but results on the original reward function
from Houthooft et al. (2016) can be found in Appendix E, where we perform comparably. We ran 5
random seeds for each experiment. More details on these environments can be found in Appendix B.
4.2	Exploration Benchmark Performance

(a) SparseHalfCheetah-v1
PJeMaJ
(c) FetchPush-v1
Figure 3: Performance of QXplore compared with RND and -greedy sampling. QXplore outperforms
RND and -greedy on continuous control tasks. QXplore performs better due to efficient exploration
sampling by Qx and the separation of the exploration and exploitation objectives. Q indicates the
performance of our exploitation Q-function, while Qx indicates the performance of our exploration
Q-function, whose objective does not directly maximize reward but which may lead to high reward
regardless.
500000
QXpIore - Q (ours)
QXpIore - Qx (ours)
1000000	1500000 -----e-greedy
updates	---- RND
(b) FetchSlide-v1
----QXpIore - Q (ours)
I	!	!_ QXpIore - Qx (ours)
O 500000	1000000	1500000 e-greedy
updates	---- RND
(d) FetchPickAndPlace-v1

Episodes until mean reward of	QXplore	VIME	EX2	EMI	GEP-PG	DORA	SimHash
50	2000	10000*	4740*	2580*	NA	X	x*
100	3000	x*	6180*	4520*	4000	X	X*
200	4000	x*	x*	8440*	X	X	X*
300	7900	x*	x*	x*	X	X	X*
Table 1: Number of episodes required to reach mean reward milestones on SparseHalfCheetah
for several methods. QXplore outperforms previously published methods. Results marked with “*”
are previously published numbers. VIME from Houthooft et al. (2016), EMI and EX2 from Kim et al.
(2018), and SimHash from Tang et al. (2017). Results marked with “x” indicate that the mean reward
was not achieved. For GEP-PG (Colas et al., 2018) we used the author’s implementation, which did
not permit easy evaluation of intermediate performance.
We show the performance of each method on each task in Figure 3. QXplore outperforms RND
modestly on the SparseHalfCheetah task, but performs much better comparatively on the
Fetch tasks- only on FetchPush, the easiest task, did RND find non-random reward. We theorize
that this improved performance on the Fetch tasks is because QXplore’s TD-error exploration
drives the agent to discover the conditional relationship between the changing goal position and the
reward function, whereas RND and other state novelty methods are goal-agnostic since the goal is
static for the entire episode. While QXplore is not a goal-directed RL method, and does not achieve
state-of-the-art performance compared to dedicated goal-directed RL methods, the fact that this
relationship is discovered through TD-error exploration is encouraging as to its broader applicability.
6
Under review as a conference paper at ICLR 2020
We compare to several other exploration methods is in Table 1. The methods from previous work
are built on top of TRPO (Schulman et al., 2015), so a comparison in terms of training iterations as
in Figure 3 would not be informative due to TRPO’s variable update rule. We instead compare the
number of episodes of interaction required to reach a given level of reward, though QXplore was not
intended to be performant with respect to this metric. While some decrease in episode efficiency is
expected due to differing baseline methods (TRPO (Schulman et al., 2015) versus TD3 Fujimoto
et al. (2018b)), compared to published results for EMI (Kim et al., 2018), EX2 (Fu et al., 2017),
VIME (Houthooft et al., 2016), and SimHash (Tang et al., 2017) on the SparseHalfCheetah
task, QXplore reaches every reward milestone faster, and achieves a peak reward (300) not achieved
by any previous method.
We also include here the performance of our implementation of DORA (Fox et al., 2018) on
SparseHalfCheetah. DORA performed poorly, possibly because it was not intended for use
with continuous action spaces, and thus we did not test it on other tasks.
Finally, we compare to GEP-PG (Colas et al., 2018), which used separate exploration and exploitation
phases similar to QXplore. We downloaded the author’s implementation (built on top of DDPG) and
tested it on SparseHalfCheetah using the parameters for the HalfCheetah-v2 task it was
originally tested on. The author’s implementation did not facilitate evaluating performance midway
through training, and thus we report only their final performance number after 4000 episodes, which
was 120.2.
4.3	Robustness
As RL tasks are highly heterogeneous, and good parameterization/performance can be hard to obtain
in practice for many methods (Henderson et al., 2018), we performed sweeps over several hyperpa-
rameters and introduce several ablations of QXplore on SparseHalfCheetah to demonstrate the
method’s robustness and validate aspects of the algorithm.
Parameter Sweeps We swept over the learning rates of Q and Qx , as well as the ratio of self-
collected versus other-collected data used to train each function. The results suggest that while the
performance of Q is somewhat sensitive to learning rate, keeping learning rates for Q and Qx the
same works well. The results also show that while our ratio of 75% self/25% non-self performs best,
Q is fairly robust to the on/off-policy data ratio, including when Q is trained entirely off-policy on
data collected by Qx . Results are shown in Figures 8 and 9 in Appendix D.
Weight Initialization Also, since neural network generalization is key to QXplore, we tested several
different network weight initialization schemes, including some that were deliberately poor priors.
We found that while the performance of Q is sensitive to initialization scheme, Qx robustly finds
reward in all cases. See Figure 12 in Appendix G.
The ‘Noisy TV’ Problem One drawback that naive state novelty exploration methods have is that un-
predictable observations (such as from a TV displaying static) act as maxima in the exploration reward
function. Naive methods are unavoidably drawn to such states instead of exploring. TD-error driven
exploration is not sensitive to unpredictable observations as they do not affect the underlying reward
function. To demonstrate this, we tested QXplore with a variant of the SparseHalfCheetah task
with noisy observations. We observe that QXplore performs as normal in this case. A description of
the task can be found in Appendix F.
4.4	Ablations
There are two features of QXplore that distinguish it from prior work in exploration: the use of a pair
of policies that share replay data, the use of unsigned TD-Error to drive exploration. We performed
several ablations that assess the impact of aspects of each of these features. Detailed results can be
found in Appendix C.
Single-Policy QXplore First, we test a single-policy version of QXplore by replacing Qθ(s, a) with
a value function Vθ(s). We use a value function rather than Q-function in this case to avoid large
estimation errors stemming from fully off-policy training such as reported by Fujimoto et al. (2018a).
We observe in Figure 5 that while the policy is able to find reward quickly and converge faster, the
need to satisfy both objectives results in a lower converged reward than the original QXplore method.
7
Under review as a conference paper at ICLR 2020
1-Step Reward Prediction Second, we run an ablation where we replaced Qθ (s, a) with a function
that simply predicts the current r(st, at). Using reward error instead of a value function in Qx can
still produce the same state novelty fallback behavior in the absence of reward; however, it provides
only limited reward-based exploration utility. We tested this variant and observe in Figure 5 that it
fails to sample reward. Reward prediction error is not sufficient to allow strong exploration behavior.
QXplore with State Novelty Exploration To assess the importance of TD-error specifically in our
two policy algorithm, we replaced the TD-error maximization objective of Qx with the random
network prediction error maximization objective of RND, while still performing separate rollouts
of each policy. The results are shown in Figure 6. We observe that while the modified Qx function
does sample reward, it is too infrequent to guide Q to learn the task, and further that the modified Qx
function does not display directional preference in exploration once reward is discovered.
QXplore with Signed TD-Error Objective While we used unsigned TD-error to train Qx , we also
tested QXplore using signed TD-error. We used the negative signed TD-error -δt from equation 1
so that better-than-expected rewards result in positive rx values. The results of this experiment are
shown in Figure 7. The unsigned TD-error performs better on SparseHalfCheetah.
4.5	Qualitative Behavioral Analysis
Qualitatively, on SparseHalfCheetah we observe interesting behavior from Qx late in training.
After initially converging to obtain high reward, Qx appears to get “bored” and will focus on the
reward threshold, stopping short or jumping back and forth across it, which results in reduced reward
but higher TD-error. This behavior is distinctive of TD-error seeking over state novelty seeking, as
such states are not novel compared to moving past the threshold but do result in higher TD-error.
Such behavior from Qx motivates Q to explore the state space around the reward boundary. Example
sequences of such behaviors are shown in Figure 4.
Figure 4: Example trajectories showing Qx’s behavior late in training that is distinctive of TD-error
maximization. The corresponding Q network reliably achieves reward at this point. In ”fake-out”,
Qx approaches the reward threshold and suddenly stops itself. In ”cross and re-cross”, Qx crosses
the reward threshold going forward and then goes backwards through the threshold.
5	Discussion and Conclusions
Here, we have described a new method for using TD-error to explore in reinforcement learning. We
instantiate a reward function using TD-error, and show that when combined with neural network
approximation, it is sufficient to discover solutions to challenging exploration tasks in fewer training
iterations than recent state novelty-based exploration methods. We hope that our results can spur
further work on diverse exploration signals in RL.
It is also worth noting that there may be additional benefits provided by Qx for Q learning in non-
exploration contexts. Maximizing TD-error can be seen as a form of hard example mining, and for
complex tasks could result in better generalization behavior and faster transfer to new tasks through
efficient trajectory sampling by Qx .
One potential future area of investigation is in our method’s connection to biological models of
dopamine pathways in the brain where levels of dopamine correlate with TD-error in learning trials
(NiV et al., 2005), a phenomenon previously described in animals (Arias-Carrion & Poppel, 2007).
8
Under review as a conference paper at ICLR 2020
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welin-
der, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight
Experience Replay. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vish-
wanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
5048-5058. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7090-hindsight-experience-replay.pdf.
Oscar Arias-Carrion and Ernst PoPPeL Dopamine, learning, and reward-seeking behavior. Acta
neurobiologiae experimentalis, 2007.
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528,
2019.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exPloration and intrinsic motivation. In Advances in Neural Information
Processing Systems, PP. 1471-1479, 2016.
Yuri Burda, Harri Edwards, DeePak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.
Large-Scale Study of Curiosity-Driven Learning. In International Conference on Learning Repre-
sentations, 2019a. URL https://openreview.net/forum?id=rJNwDjAqYX.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. ExPloration by random network
distillation. In International Conference on Learning Representations, 2019b. URL https:
//openreview.net/forum?id=H1lJJnR5Ym.
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exPloration via q-ensembles.
arXiv preprint arXiv:1706.01502, 2017.
Cedric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. Gep-pg: Decoupling exploration and
exPloitation in deeP reinforcement learning algorithms. arXiv preprint arXiv:1802.05054, 2018.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-Explore: a
New Approach for Hard-Exploration Problems, 2019.
Lior Fox, Leshem Choshen, and Yonatan Loewenstein. {DORA} The Explorer: Directed Outreaching
Reinforcement Action-Selection. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=ry1arUgCW.
Justin Fu, John Co-Reyes, and Sergey Levine. EX2: Exploration with Exemplar Models for Deep Re-
inforcement Learning. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan,
and R Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2577-2587.
Curran Associates, Inc., 2017. URL https://arxiv.org/abs/1703.01260.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. arXiv preprint arXiv:1812.02900, 2018a.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error
in Actor-Critic Methods. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 1587-1596, Stockholmsmassan, Stockholm Sweden, 2018b. PMLR. URL http:
//proceedings.mlr.press/v80/fujimoto18a.html.
Clement Gehring and Doina Precup. Smart Exploration in Reinforcement Learning using Absolute
Temporal Difference Errors. In Autonomous Agents and Multiagent Systems (AAMAS), 2013.
ISBN 978-1-4503-1993-5.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
9
Under review as a conference paper at ICLR 2020
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Max-
imum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1861-1870, Stockholmsmassan, Stockholm
Sweden, 2018. PMLR. URL http://proceedings.mlr.press/v80/haarnoja18b.
html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artificial Intelli-
gence, 2018.
Zhang-Wei Hong, Tsu-Jui Fu, Tzu-Yun Shann, Yi-Hsiang Chang, and Chun-Yi Lee. Adversarial
exploration strategy for self-supervised imitation learning. CoRR, abs/1806.10019, 2018. URL
http://arxiv.org/abs/1806.10019.
Rein Houthooft, Xi Chen, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel.
VIME: Variational Information Maximizing Exploration. In D D Lee, M Sugiyama, U V
Luxburg, I Guyon, and R Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 1109-1117. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6591-vime-variational-information-maximizing-exploration.pdf.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. QT-Opt:
Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. In Aude Billard,
Anca Dragan, Jan Peters, and Jun Morimoto (eds.), Proceedings of The 2nd Conference on
Robot Learning, volume 87 of Proceedings of Machine Learning Research, pp. 651-673. PMLR,
2018. ISBN 012492543X. doi: arXiv:1806.10293v2. URL http://arxiv.org/abs/1806.
10293.
Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. EMI:
Exploration with Mutual Information, 2018.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning: Deep
Deterministic Policy Gradients (DDPG). ICLR, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles
Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane
Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,
518(7540):529-33, feb 2015. ISSN 1476-4687. doi: 10.1038/nature14236. URL http://www.
ncbi.nlm.nih.gov/pubmed/25719670.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. IJCAI
International Joint Conference on Artificial Intelligence, 2016. ISSN 10450823. doi: 10.1038/
nature14236.
Yael Niv, Michael O Duff, and Peter Dayan. Dopamine, uncertainty and TD learning. Behavioral
and brain functions : BBF, 1:6, may 2005. ISSN 1744-9081. doi: 10.1186/1744-9081-1-6.
URL https://www.ncbi.nlm.nih.gov/pubmed/15953384https://www.ncbi.
nlm.nih.gov/pmc/articles/PMC1171969/.
Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. arXiv preprint arXiv:1709.05380, 2017.
OpenAI. OpenAI Five Benchmark: Results, 2018. URL https://blog.openai.com/
openai- five- benchmark- results/.
10
Under review as a conference paper at ICLR 2020
OPenAL Marcin Andrychowicz, BoWen Baker, Maciek Chociej, RafaI JOzefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider,
Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning
Dexterous In-Hand ManiPulation. CoRR, 2018. URL http://arxiv.org/abs/1808.
00177.
Ian Osband and Benjamin Van Roy. Why is Posterior samPling better than oPtimism for reinforcement
learning? In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 2701-2710. JMLR. org, 2017.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized Prior functions for deeP reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 8617-8629, 2018.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-Driven Exploration
by Self-Supervised Prediction. In IEEE Computer Society Conference on Computer Vision and
Pattern Recognition Workshops, 2017. ISBN 9781538607336. doi: 10.1109/CVPRW.2017.70.
Nikolay Savinov, Anton Raichuk, Damien Vincent, Raphael Marinier, Marc Pollefeys, Timothy
Lillicrap, and Sylvain Gelly. Episodic Curiosity through Reachability, 2019. URL https:
//openreview.net/forum?id=SkeK3s0qKQ.
Jurgen Schmidhuber. Adaptive confidence and adaptive curiosity. In Institutfur Informatik, Technische
Universitat Munchen, Arcisstr. 21, 800 Munchen 2. Citeseer, 1991.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the
game of Go without human knowledge. Nature, 2017. ISSN 1476-4687. doi: 10.1038/nature24270.
Riley Simmons-Edler, Ben Eisner, Eric Mitchell, Sebastian Seung, and Daniel Lee. Q-learning for
continuous actions with cross-entropy guided policies. arXiv preprint arXiv:1903.10605, 2019.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing Exploration In Reinforcement
Learning With Deep Predictive Models. CoRR, abs/1507.0, 2015.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. {IEEE} Trans.
Neural Networks, 9(5):1054, 1998. doi: 10.1109/TNN.1998.712192. URL https://doi.org/
10.1109/TNN.1998.712192.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John
Schulman, Filip DeTurck, and Pieter Abbeel. #Exploration: A Study of Count-Based Exploration
for Deep Reinforcement Learning. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus,
S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
2753-2762. Curran Associates, Inc., 2017. URL https://arxiv.org/abs/1611.04717.
Sebastian B Thrun and Knut Moller. On planning and exploration in non-discrete environments.
GMD Sankt Augustin, Germany, 1991.
Sebastian B Thrun and Knut Moller. Active exploration in dynamic environments. In Advances in
neural information processing systems, pp. 531-538, 1992.
11
Under review as a conference paper at ICLR 2020
Table 2: Parameters used for benchmark runs.
Default Parameters		
CEM		
ITERATIONS	4	
NUMBER OF SAMPLES	64	
TOP K	6	
All Networks		
NEURONS PER LAYER	256	
NUMBER OF LAYERS	3	
non-linearities	RELU	
Optimizer	Adam	
Adam momentum terms	β1 = 0.9, β2 =	0.99
Training		
Q learning rate	0.001	
BATCH SIZE	128	
TIME DECAY γ	0.99	
TARGET Q-FUNCTION UPDATE τ	0.005	
TARGET UPDATE FREQUENCY	2	
TD3 policy noise	0.2	
TD3 noise clip	0.5	
TRAINING STEPS PER ENV TIMESTEP	1	
QXplore-specific		
Qx LEARNING RATE	0.001	
Q BATCH DATA RATI O	0.75	
Qx BATCH DATA RATIO	0.75	
βQ (Q INITIAL OUTPUT BIAS)	0	
RND-specific		
PREDICTOR NETWORK LEARNING RATE	0.001	
Extrinsic reward weight	2	
Intrinsic reward weight	1	
γE	0.99	
γI	0.99	
DORA-specific		
	0.1	
β	0.05	
γE	0.99	
γQ	0.99	
-GREEDY-SPECIFIC		
	0.1	
A Implementation Details and Hyperparameters
We describe here the details of our implementation and training parameters. We held these factors
constant and used a shared codebase for QXplore, RND, and -greedy to enable a fair comparison. We
used an off-policy Q-learning method based off of TD3 (Fujimoto et al., 2018b) and CGP (Simmons-
Edler et al., 2019) with twin Q-functions and a cross-entropy method policy for better hyperparameter
robustness. Each network (Qθ, Qx,φ, RND’s random and predictor networks) consisted of a 4-layer
MLP of 256 neurons per hidden layer, with ReLU non-linearities. We used a batch size of 128 and
learning rate of 0.001, and for QXplore sampled training batches for Q and Qx of 75% self-collected
data and 25% data collected by the other Q-function’s policy as described in Algorithm 1.
For DORA (Fox et al., 2018), we used the hyperparameters and training procedure specified by
the original paper where possible, though it was necessary to adapt the method somewhat to the
continuous action domain. This is because the original formulation proscribed an “LLL” action
selection scheme that requires taking discrete log-probabilities of the distribution of Q and E values
over actions, which is not tractable in continuous action spaces. Instead, we tried selecting actions
using either a CEM policy that maximizes the sum of the two objectives, or using the E values
as a reward bonus for training Q and selecting actions that maximize Q only. We thus expect the
performance of our implementations to be somewhat worse than a hypothetical distributional-DORA,
12
Under review as a conference paper at ICLR 2020
though the action selection scheme we used does make this version directly comparable to QXplore
and RND. Both formulations behaved similarly on SparseHalfCheetah and did not achieve
reward with any frequency.
For -greedy sampling with continuous actions, we sampled a uniform distribution of the valid action
range (-1 to 1 for all tasks) with probability and act greedily otherwise. We note that the stochastic
cross-entropy method policies we used for all experiments also introduce some amount of local
exploration through noisy action selection.
We present the parameters we used for the benchmark tasks in Table 2.
B Environment Details
We use the SparseHalfCheetah environment proposed by Houthooft et al. (2016) in which a
simulated cheetah receives a reward of 0 if it is at least 5 units forward from the initial position and
otherwise receives a reward of -1. We also use the OpenAI gym tasks, FetchPush, FetchSlide,
and FetchPickAndPlace, which were originally developed for benchmarking HER (Andrychow-
icz et al., 2017). The objective in these environments is to move a block to a target position, with a
reward function returning -1 if the block is not at the target and 0 if it is at the target. For consistency
in reward shaping, we structured the reward function of the SparseHalfCheetah task to match
the Fetch tasks, such that the baseline reward level is -1 while a successful state provides 0 reward,
but report reward values on a 0 to 500 scale for direct comparison with previous work. We trained each
method with 5 random seeds for 5,000 episodes on SparseHalfCheetah and 50,000 episodes
on Fetch tasks. Time to convergence on these tasks for any exploration method is highly variable,
and as such we visualize the mean and standard deviation of the runs in our results.
C Ablations
To study the effects of different components of QXplore, we performed several ablations, as discussed
in Section 4.4. First, we replaced Qθ with simple 1-step reward prediction, and Qx’s objective
function with maximizing cumulative 1-step reward prediction error plus extrinsic reward, which
we label as “QXplore-1-step” in Figure 5. This ablation fails to find reward, as the 1-step reward
prediction error makes long range exploration more difficult to learn.
Figure 5: Plot showing the performance of two ablations, 1-Step Reward Prediction (QXplore-1-step)
and Single-Policy QXplore (QXplore-value), compared to the original QXplore method. In the 1-Step
ablation, Qx is trained to predict a combination of extrinsic reward and reward prediction error, and
fails to make progress. In the Single-Policy ablation, the policy converges faster, but to a worse policy
than vanilla QXplore due to the need to balance TD-error and extrinsic reward maximization.
Next, we tested a variant of QXplore using only a single sample policy and treating TD-error
as a reward bonus, more in line with traditional exploration bonus methods. We trained a value
function Vθ(S) trained via bootstrap and computed rx as Γχ,θ(st, at, st+ι) = ∣Vθ(St)-(『E(st, at) +
γVθ00 (st+1))|. This variant uses only a single sample policy, Qx, which is trained via bootstrapped
off-policy Q-learning using one-step reward targets r1 = (rx(st, at, st+1) + αrE(st, at) to maximize
a combination of intrinsic and extrinsic rewards, controlled by the hyperparameter α. We used
α = 0.1, which we found to work well for SparseHalfCheetah in tuning experiments. We
13
Under review as a conference paper at ICLR 2020
used a value function Vθ(s) rather than a Q-function for this ablation to avoid the wildly optimistic
max action selection fully off-policy Q-functions have been reported to suffer from (Fujimoto et al.,
2018a). We label this experiment as “QXplore-value” in Figure 5. This variant performs comparably
to the Qx function of normal QXplore, but performance does not decrease late in training thanks to
the extrinsic reward signal. However, overall performance is still well below that of normal QXplore’s
exploitation policy, which does not have to satisfy two conflicting training objectives.
Figure 6: Plots showing the performance of QXplore where the objective of Qx is replaced by the
RND exploration objective, as well as the mean position of the cheetah during an episode throughout
training. While Qx does sample reward, it does so too infrequently to guide Q to learn the task.
While the Qx function of QXplore-RND does reach states far from the origin, it does not display
directional preference, whereas original QXplore’s Qx function converges to sample states around
the reward threshold at 5 units.
updates ----------- QXpIore RND - Q
Third, we tested a variant of QXplore where the TD-error maximization objective of Qx was replaced
by the RND random network prediction error maximization objective. We call this ablation “QXplore-
RND” and results are shown in Figure 6 for both Q and Qx policies. We observe that neither function
converges to achieve reward. While we see that Qx does sample reward, Q samples reward only
during two episodes of training, and Qx does not converge to achieve high expected rewards itself.
Looking at the mean position of the cheetah during an episode over training, we observe that for
QXplore-RND Qx samples states relatively far from the origin compared to Q, based on the wider
standard deviation, but does not display a directional preference (besides the inbuilt tendency to
move forward more readily than backward that the cheetah has built in), since states found in both
directions are equally novel. Comparatively, the Qx function of normal QXplore displays a strong
forward preference once reward is found, and converges on sampling states close to the 5-unit reward
threshold (this results in a mean position less than 5 due to time spent traveling from the origin),
while the corresponding Q function prefers to move well past the reward theshold (a mean position
above 5) to reliably achieve reward.
Figure 7: The performance of QXplore’s Q function with Qx maximizing signed versus unsigned
TD-error on two different reward variants of SparseHalfCheetah. While Q is able to learn
the task for all variants, performance is reduced with the signed objective. Performance on the
SparseHalfCheetah variant with -1 to 0 reward function is shown shifted to match the axes of
the 0 to 1 variant for comparison.
Finally, we tested maximization of signed TD-error by Qx rather than unsigned. This objective
tracks closer to dopamine-seeking in animals, where increases in dopamine (corresponding to an
14
Under review as a conference paper at ICLR 2020
unexpectedly positive outcome) are sought out while decreases in dopamine (from unexpectedly
negative outcomes) are avoided. To emulate this, we negate the signed TD error such that negative
TD-error (the predicted Q-value was less than the target value) is maximized, while positive TD-error
is minimized. Qx is otherwise trained as normal. The results are shown for both variants of the
SparseHalfCheetah reward function (-1 to 0 and 0 to 1) in Figure 7, with and without setting the
initial output bias of Q to 10 in the 0 to 1 case. We observe that while QXplore does train with signed
TD-error, performance is reduced. While this result bares further investigation, we hypothesize this is
because prior to finding reward the sign of the TD-error is not correlated with the novelty of a state,
thus the state novelty exploration phase is less efficient.
D Parameter Sweeps
We performed two sets of parameter sweeps for QXplore: varying the learning rates of Q and Qx,
and varying the ratios of data sampled by each Q-function’s policy used in training batches for each
method. For learning rate, we tested combinations (QLR, QxLR) (0.01, 0.01), (0.01, 0.001), (0.001,
0.01), (0.001, 0.001), (0.001, 0.0001), (0.0001, 0.001), (0.0001, 0.0001).
For batch data ratios, we tested combinations (specified as self-fraction for Q, then self-fraction for
Qx) of (0, 1), (0.25, 0.75), (0.5, 0.5), (0.75, 0.25).
Results for these sweeps can be seen in Figures 8 and 9. QXplore is sensitive to learning rate, but
relatively robust to the training data mix, to the point of Q training strictly off-policy with only
modest performance loss.
(a) SparseHalfCheetah
Figure 8: Learning rate sweeps for Q and Qx
(b) FetchPush-v1
D.1 RND Parameter Sweeps
As we have adapted RND to operate with vector observations and continuous actions, we performed
several hyperparameter sweeps to ensure a fair comparison. We report in Figure 10 the results of
varying both predictor network learning rate “lr” and extrinsic reward weight “rw” independently
on the SparseHalfCheetah task. The baseline values for these parameters used elsewhere are
0.001 and 2 respectively. We observe that RND is fairly sensitive to reward weight, but a value of 1
or two performs well, while a learning rate of 0.001 appears to learn faster early in training without
loss of final performance.
E	REWARD SHIFTING VERSUS βQ
For most of our experiments with the SparseHalfCheetah environment, we used a reward
function that is -1 for non-goal states and 0 for goal states to have consistent reward shaping with
the Fetch tasks in the OpenAI Gym. However, as the original SparseHalfCheetah proposed
by Houthooft et al. (2016) used a reward function that is 0 for non-goal states and 1 for goal states,
15
Under review as a conference paper at ICLR 2020
baseline
——qrat_50_qxrat_50
----qrat θ qxrat lθθ
—qrat_7 5_qxrat_2 5
■" qrat_2 5_qxrat_7 5
-300 ---------------
(a) SparseHalfCheetah
Figure 9: Sample ratio sweeps for Q and Qx
baseline
-qrat_50_qxrat_50
----qrat θ qxrat lθθ
—qrat_7 5_qxrat_2 5
—qrat_2 5_qxrat_7 5
-30 T
0	2000	4000	6000	8000 IOOOO
episode
(b) FetchPush-v1
PJeMaJ
-200-
-300-
-100-
RND Parameter Sweeps
o
-rwl
-rwl
-rwθl
-rw05
-IrOl £
-∣rt)001 00000	1000000	1500000	2000000	2500000
-original	updates
RND
---RND
Figure 10: Parameter sweeps for RND. A reward weight of either 1 or 2 works best, with a learning
rate of 0.0001 a close second.
we present results for QXplore and our implementation of RND on the original reward function
as well in Figure 11. Because we initialized the output distribution Q to be close to 0 initially,
QXplore performed worse on this reward function due to the much smaller magnitude of TD-errors
during the initial reward-free exploration phase slowing down exploration. However, adjusting
the hyperparameter βQ , the initial bias of the output neuron of Q, allows us to obtain identical
performance to the -1 to 0 reward function. QXplore’s state novelty search efficiency is sensitive
to this initial TD-error magnitude, which varies depending on the reward function, but in a coarse
parameter sweep of initial biases of -10, 0, 1, 10, and 100 we found a good setting for the parameter
which performed comparably to the -1 to 0 reward function. This dependency is similar in concept
to the reward weighting used by many reward bonus methods to trade off between exploration and
exploitation, the setting of which may also depend on the reward landscape.
F	The ‘Noisy TV’ Problem
The ‘Noisy TV’ problem is a classic issue with some state-novelty exploration methods in which
states with unpredictable observations serve as maxima in the novelty reward space. QXplore’s
TD-error objective is not fundamentally vulnerable to the problem, but to demonstrate that our
function approximation early in training is also no subject to it, we trained QXplore on a variant of the
SparseHalfCheetah task where we add a random normally-distributed value to the observation
vector of the agent. The variance of this noise value increases proportionately to the movement of the
cheetah in the negative direction (away from the reward threshold). An agent vulnerable to the noisy
tv problem will be enticed to explore in the negative direction rather than forward, as this maximizes
the novelty/unpredictability of the observations.
16
Under review as a conference paper at ICLR 2020
Figure 11: QXplore performance on SparseHalfCheetah with the 0 to 1 reward function.
Adjusting βQ recovers full performance compared to the -1 to 0 reward function (shown here shifted
by 500 reward for comparison). We tested several different values for βQ and found that a value of
10 worked best for SparseHalfCheetah.
We show the results of training QXplore on this environment in Figure 13 for both Q and Qx, as well
as the mean position of the cheetah along the movement dimension during Qx ’s training rollouts. As
expected, the performance of neither Q nor Qx is meaningfully altered relative to the baseline, and
Qx is not biased to explore backwards to a greater degree than it typically does early in training.
G Weight Initialization
As we use neural net function approximation error as a state novelty baseline for early exploration,
the behavior of Qx may be sensitive to weight initialization. To test this, in addition to the Pytorch
default initialization method “Kaiming-Uniform,” (He et al., 2015) which we used for all runs outside
this section, we also tested initializing both Q and Qx with “Kaiming-Normal” and “Xavier-Uniform,”
(Glorot & Bengio, 2010) two other initialization methods that result in higher variance between
initial outputs of the networks, which translates into reduced training performance. We further tested
two naive distributions that produced very high variance in outputs, “Normal,” sampling weight
values from N(0, 1) and “Uniform,” sampling values from U(-1, 1). These configurations were not
expected to perform as well as “Kaiming-Uniform”, but do test the ability of Qx to explore given
a poor initialization. In all cases other than “Kaiming-Uniform” we set the bias of each neuron to
0. The results of QXplore with each initialization scheme on SparseHalfCheetah are shown in
Figure 12.
“Kaiming-Normal” and “Xavier-Uniform” both showed moderate decrease in overall performance,
though both Q and Qx were able to converge on reward. “Normal” and “Uniform” however both
more-or-less prevented Q from converging on reward. Their effect on the ability of Qx to find reward
however is much more mild- only “Normal” and to a lesser extent “Uniform” caused significant issues
with discovering and converging on reward. This suggests that Qx is not particularly dependent on
careful weight initialization to explore with function approximation error.
17
Under review as a conference paper at ICLR 2020
kaimiπg uniform
kaimiπg normal
initialization - Q
seMaJ
0	500000	1000000	1500000	2000000	2500000
updates
kaιmιπg uniform
kaiming normal
initialization - Qx
seMaJ
0	500000	1000000	1500000	2000000	2500000
updates
(a) Initialization of Q	(b) Initialization of Qx
Figure 12:	Several alternate initialization schemes for Q and Qx . While Q is adversely impacted, Qx
is relatively robust even to very poor initializations such as “Normal” and “Uniform.”
robustness to the lnoιsy tv' problem - Q
500	・ YI
Q×P∣ore
QXpIore w/ Noisy Obs
PJeMaJ
-400
robustness to the 'noisy tvl problem - Qx
0
-500
-QXPlore 500000 I 1000000	1500000	2000000	2500000
-QXpIore w/ Noisy Obs	updates
1000000	1500000	2000000	2500000
updates
(a) Noisy observation effects on Q	(b) Noisy observation effects on Qx
average position of cheetah
3 2
PJeMaJ
1000000	1500000	2000000	2500000
updates
-----T ,
-----QXpIore
QXpIore w/ Noisy Obs
(c) Noisy observation effects on absolute position
Figure 13:	QXplore trained on a ‘noisy tv’ variant of SparseHalfCheetah where one element of
the observation vector is normally distributed random value whose variance increases if the cheetah
moves in the negative direction. The performance of QXplore is not impacted in any way by this
noise, and it trains as normal.
18