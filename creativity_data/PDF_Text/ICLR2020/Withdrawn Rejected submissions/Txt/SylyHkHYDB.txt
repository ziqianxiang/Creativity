Under review as a conference paper at ICLR 2020
Knossos: Compiling AI with AI
Anonymous authors
Paper under double-blind review
Ab stract
Machine learning workloads are often expensive to train, taking weeks to con-
verge. The current generation of frameworks relies on custom back-ends in order
to achieve efficiency, making it impractical to train models on less common hard-
ware where no such back-ends exist. Knossos builds on recent work that avoids
the need for hand-written libraries, instead compiles machine learning models in
much the same way one would compile other kinds of software. In order to make
the resulting code efficient, the Knossos complier directly optimises the abstract
syntax tree of the program. However in contrast to traditional compilers that em-
ploy hand-written optimisation passes, we take a rewriting approach driven by the
A? search algorithm and a learned value function that evaluates future potential
cost reduction of taking various rewriting actions to the program. We show that
Knossos can automatically learn optimisations that past compliers had to imple-
ment by hand. Furthermore, we demonstrate that Knossos can achieve wall time
reduction compared to a hand-tuned compiler on a suite of machine learning pro-
grams, including basic linear algebra and convolutional networks. The Knossos
compiler has minimal dependencies and can be used on any architecture that sup-
ports a C++ toolchain. Since cost model the proposed algorithm optimises can be
tailored to a particular hardware architecture, the proposed approach can poten-
tially applied to a variety of hardware.
1 Introduction
While the development of any kind of software can benefit from compliers able to produce fast code,
runtime efficiency is particularity important for modern machine learning. In particular, because
modern models they can take weeks to train (OpenAI, 2018), complier optimisations that lead to
execution speed-ups are of huge value. In parallel, machine learning is being deployed on a variety
of diverse devices ranging from wearables to huge clusters clusters of powerful GPUs. Since each
architecture has different performance profile and requires different code optimisations, it is difficult
to provide tooling that works fast on all of them.
Traditionally, the tension between performance and interoperability is resolved by machine learning
frameworks (Paszke et al., 2017; Abadi et al., 2016). In these frameworks, while code execution is
outsourced to hardware-specific back-ends such as XLA (XLA authors, 2016). While this approach
has seen huge initial success, the cost of providing customised back-ends for each target architecture
is prohibitive. Moreover, the frameworks also custom front-ends that require the programmer to
specify the model being trained as a compute graph. Since the compute graph has semantics separate
from the host programming language, this process is often error-prone and time-consuming. In order
to address these obstacles, a new generation of tools has recently appeared that transform machine
learning code using the same techniques that have been used for compiling traditional software.
The need for a separate front-end API for machine learning operations is eliminated by including
automatic differentiation as a first-class feature of the complied language (Innes et al., 2019; Frostig
et al., 2018). Instead of custom back-ends, modern machine learning compliers use an intermediate
representation and perform extensive code optimisations (Innes et al., 2019; Frostig et al., 2018; van
Merrienboer et al., 2018; Wei et al., 2018; Sotoudeh et al., 2019; Rotem et al., 2018). In addition,
program optimisation is being modelled as a machine learning task itself, with the complier learning
how to perform rewrites (Chen et al., 2018b;a).
Knossos expands on this line of work. The Knossos system includes a compiler which combines effi-
cient program optimisation with an intermediate representation (IR) designed with machine learning
1
Under review as a conference paper at ICLR 2020
(rule
(rule
(rule
(rule
(rule
(rule
(+ 0 e) e)
(* 1 e) e)
(/ (* a b) c)
(* (/ a c) b))
(* (exp a) (exp b))
(exp (+ a b)))
(if p a a) a))
(apply (lam x body) arg)
(let x arg body))
(a)	Sample arithmetic expression tree (MDP state).
The tree corresponds to Jj：]
(b)	Example rewrite rules. Each rule defines a class
of MDP transitions. See Appendix C for the full list
of rules.
Figure 1:	The Knossos MDP.
in mind. We formalize program optimisation as a finite-horizon Markov Decision Process (MDP),
with the reward signal determined by the cost of executing a program. By solving this MDP, we
are able to produce fast code tailor-made for any given task and architecture, without relying on
backend-specific hand-written libraries. Knossos works by re-writing programs written in an inter-
mediate representation (IR). Akin to JAX (Frostig et al., 2018) and Zygote (Innes et al., 2019), all
Knossos functions are potentially differentiable, avoiding the syntactic awkwardness that arises from
embedding a differentiable program in a host language. The IR can then be transpiled, allowing it
to run on any platform that supports a C++ toolchain. This allows Knossos code to be seamlessly
deployed on specialized or embedded hardware without the need of manual tuning, both for training
and for deployment of models, enabling a much broader user base than competing approaches.
To our knowledge, Knossos is the first compiler that combines RL-based program optimisation, first-
class support for deep learning primitives and the ability to target any architecture supporting the
C++ toolchain. We defer detailed scope comparisons with prior work to Section 4. We empirically
demonstrate the benefits of our program optimisation in Section 5, showing that Knossos was able
to automatically learn loop fusion, a type of compiler optimisation that previously had to be applied
manually.
2	Code Optimisation as a Reinforcement Learning Problem
We model code optimisation as a finite-horizon Markov Decision Process (MDP). An MDP is de-
fined (Puterman, 2014; Sutton & Barto, 2018) as a tuple (S, A, T, R, H, p0), where S denotes the
state space, A denotes the action space, T denotes the transition dynamics, R denotes the rewards,
H is the maximum time budget allowed to solve the problem (the horizon) and p0 is a fixed proba-
bility distribution over initial states. We provide a detailed description of the states, transitions and
rewards later on this section.
States and transitions An MDP state s = (es , ts) consists of a Knossos program (or expression)
e ∈ E and the remaining time budget ts ∈ [0, 1, . . . , H] (i.e., the number of remaining steps), where
H is the maximum budget. Any state with ts = 0 is terminating. The initial state distribution
p0 models the expressions that the RL agent is likely to be asked to optimize. A sample Knossos
expression is shown in Fig. 1a. The action set A corresponds to different possible ways of rewriting
the same expression (see Fig. 1b). The transition function T : S × A → S returns the next state
after taking an action. For example, the first rule in Fig. 1b says that adding zero to any expression
can be simplified to the expression itself. Once the action is chosen, the transition is deterministic.
Because rewrite rules can be applied to different subexpressions, we specify A using generic rewrite
rules, which are applied by pattern matching. There are over 50 rules like this - We provide the
details in Appendix C. An essential feature of the rewrites is that they do not change the meaning of
the program, i.e. by simplifying from one expression to another we also implicitly generate a proof
that the expressions are equivalent.
2
Under review as a conference paper at ICLR 2020
Policies and Value Functions The RL agent maintains a policy ∏(a∣s), which defines the Prob-
ability of taking an action in state s given there are ts steps remaining till the total time budget is
exhausted. A policy π generates rollouts τπ . A rollout τπ is defined as a sequence of states, ac-
tions and rewards obtained from the MDP τπ = (s1, a1, r1, s2, a2, r2, . . . sH, rH). Since the policy
π can be stochastic, it is modelled as a random variable. The goal of RL agent is to find an op-
timal policy π? = arg maxπ Jπ , which attains the best possible return. The return is defined as
Jπ = Eτπ Ptt==0H-1 R(st, st+1) . Given a policy and the number of timesteps t remaining till
the end of episode, we define a value function V (s) = Eτ Pit=s-01 R(si, si+1) s0 = s , where ts
denotes the remaining time budget at state s. The optimal value function V ? is defined as the value
function of an optimal policy π? .
Rewards and the Cost Model We assume access to a function c(s), which provides the cost
model, i.e. the computational cost of running es, the expression represented by state s = (es, ts), on
representative inputs. While developing a perfect cost models is theoretically impossible due to the
intractability of the halting problem (Turing, 1937), very good cost models exist for the particular
subset of programs that compliers are asked to optimise. The ideal cost model cB would correspond
to the run-time of the program on typical inputs, but evaluating costs by benchmarking is very
computationally intensive. In practice, one can often finda surrogate cost function such that for most
initial programs s0, the state that is reachable from s0 and minimizes the surrogate cost function c
agrees with that for the ideal cost function cB, that is,
arg min c(s) = arg min cB (s),	(1)
S〜S0	S〜S0
which is much easier to acquire. In other words, the cost function c does not have to produce the
same run-time but the same minimum over programs. We show experimentally in Section 5 that it
is indeed possible to reduce the wall clock time of running a program by optimising such a proxy
cost model. Knossos has a modular architecture, making it easy to change the cost function. This
makes it possible to quickly re-tune Knossos programs for any target hardware. We stress that the
formalism allows us to find optimisations even in case getting to the optimized version of the code
requires using intermediate programs of higher cost (see Fig. 8).
Our reward function is based on this cost model. The rewards R(s1, s2) = c(s2) - c(s1) correspond
to the attained reduction in cost when rewriting expression eS1 into eS2 . This formulation ensures
that return Jπ equals the total cost reduction attained along the length of the rollout τ . Similarly, the
value function corresponds to the expected cost reduction under the current policy. Since our MDP
includes a ‘no-op’ rewrite rule that allows us to keep the current expression and hence the cost, the
optimal value function is monotonic in t i.e.
V ?((e, t0)) ≥ V?((e,t)) for any e, t0 ≥ t.	(2)
3	Training the RL Agent
Hard and Easy Aspects of Rewriting There are two main ways in which the task of rewriting
expressions is more challenging than typical RL benchmarks. First, the allowed set of actions not
only changes from state to state, but grows with the size of the expression. This makes exploration
hard. Second, the states of the MDP, which correspond to the expressions being rewritten, are
represented as graphs, whose size and topology varies as optimisation progresses. This is unlike
traditional deep Reinforcement Learning (Mnih et al., 2013), which learns either from pixels or
from data of fixed shape. While the rewriting task has many features that make it difficult, it is
also easier than many traditional RL tasks for three reasons. First, MDP transitions are completely
deterministic. Second, the task has a large degree of locality in the sense that the performance of
a program can often be substantially improved by optimising its parts separately. Third, we can
generate state transitions in any order convenient to us, as opposed to the traditional RL setting,
where we are constrained by the order imposed by the environment. Overall, we have a problem
similar to traditional planning, but which requires us to generalise well in order to obtain competitive
solutions. To do this, Knossos uses a custom RL algorithm, based on A? search supported by value
function learned with a graph neural networks (Algorithm 1). We describe how to obtain the heuristic
in Section 3.2, and the search algorithm in Section 3.1.
3
Under review as a conference paper at ICLR 2020
Algorithm 1 Knossos
function TRAIN(Etra⅛, H, M)
Input: Etrain: Set of expressions to train
H : Maximum depth of the search
M: Number of epochs
Initialize a value function V
for M iterations do
for e ∈ Etrain do
S, Vtarget — A*(e,V,H)	. Optimize e and store the learned values Vtarget
V J Fit(S, Vtarget, V)	. Fit the value function - see Section 3.2
end for
end for
end function
function OPTIMIZE(e, V, H)
Input: e: Expression to optimize
V : Learned value function
H: Maximum depth of the search
S, _ J A?(e, V, H)	. Optimize e
return arg mins∈S c(s)	. Return the best expression found during the process
end function
Algorithm 2 Deep A? search
function A*(s0, V, H)
Input: s0 : Expression to optimize
V : Learned value function
H: Maximum depth of the search
s0 J (e0, H)
O J {s0 }	. O is an open list storing all unexplored states
C J {s0 } . C is a closed list storing all states . t represents the step budget left for the state.
while not term_condition() and O is not empty do
s J arg maxs∈O f (s).	. f is the heuristic function defined in equation 3.
OJO\{s}
for all a ∈ A(s) do
n J T(s, a)	. Obtain the next state n = (en, tn)
if (en, t) ∈/ C(∀t) or (en, t) ∈ C and tn > t then
if tn > 0 then
OJ O∪{n}
end if
if n ∈/ C then
CJ C∪{n}
end if
end if
end for
end while
for all s ∈ C do
Vtarget(s) J maxs0∈C∪DiSTANCE(s,s0)≤ts (c(s) - c(s0))
. Empirical estimate of maximum cost reduction achievable from s
end for
return C, t, V target
end function
4
Under review as a conference paper at ICLR 2020
Edge type	Directionality
first child	directed
second child	directed
third child	directed
tuple child	directed
is-identical	undirected
(b) List of edge types
(a) Message passing on the expression graph.
Figure 2:	Graph neural network
3.1	SEARCHING THE SPACE OF REWRITES WITH A?
We use the A? algorithm (Hart et al., 1968) both to train the compiler and to deploy it. A? maintains
two priority queues. One queue (O) stores the frontier, i.e. states from which transitions have not
been explored yet. The other one (C) stores the states visited so far and is used to avoid exploring
the same path twice. The states are explored in the order induced by the A? heuristic, which in our
case corresponds to the learned value function V, obtained from previous iterations. In particular,
node priority is set as follows:
f (S) = f((es ,ts )) = V(S) + PH- ts-1 R(Si ,si+1).	⑶
S--{Z-}	S---------{---------}
s	c(s0)-c(s)
Here, V(S) is the estimated future cost reduction obtained from state S within t remaining time-
steps. The quantity c(S0) - c(S) corresponds to the cost reduction that has already been achieved by
time t, measured against the cost of the initial expression. Thus, f(S) is an estimate of the maximum
possible cost improvement from a trajectory passing through state S at time t.
After the search, we compute the empirical estimate of the maximum cost reduction achievable
(V target(S)) for each visited state. The estimated value of S with ts timesteps is the maximum cost
reduction found from S within ts steps. DISTANCE(S, S0) in Algorithm 2 is the number of steps
required to reach S0 from S. The algorithm stops after the value function was evaluated a set number
of times. In the code this is represented with the function term-condition.
A? is well-suited for the rewriting task because it exploits its characteristic features. In particular,
it exploits determinism by assuming that a cost reduction achievable once can always be achieved
again. It exploits the availability of reset by considering nodes in the order defined by the heuristic
function. It exploits locality by preferring re-writes that need a small number of rule applications.
Before deciding on A? , we also performed experiments with Monte Carlo Tree Search (MCTS).
MCTS does not make use of reset and had worse empirical performance (see Appendix D for de-
tails).
3.2	Learning Value Functions with Graph Neural Networks
States in the Knossos MDP correspond to computation graphs. In order to apply deep RL to these
graphs, we need to be able to construct differentiable embeddings of them. To do this, we employ
Graph Neural Networks based on Gated Recurrent Units (Li et al., 2016). During the forward pass,
the GNN begins with an initial embedding of the graph nodes. It then iteratively applies a diffusion
process to the graph. At each step, the obtained representation is fed into a gated recurrent unit
(GRU). The process implicitly encodes the edge structure of the graph in the obtained representation.
Graph representation We represent a Knossos expression as a graph. The graph nodes corre-
spond to subexpressions (see Fig. 1a). The graph edges are of two kinds. The first kind of edges
connects the nodes with their parents. In addition, we use another kind of edges, which is used to
explicitly provide the information that two subexpressions are identical. See Table 2b for a list of
all edge types. Edges can be directed or undirected, with the directed edges going in opposite ways
considered different.
5
Under review as a conference paper at ICLR 2020
Graph neural network To compute the value function for an expression e and time budget t, we
start from computing the initial node embedding h0v ∈ Rd for all node v ∈ N (e), where N (e) is
the set of vertices in expression e. The initial node embedding consists of a one-hot encoding of the
node type (constant, variable, etc) followed by zero padding.
This embedding is then fed into the following recurrent computation (see Fig. 2a):
htv+1 =f htv, Lv0∈Ap(v), mp (htv0)	(t=0,...,T -1),	(4)
p∈P
where p ∈ P indexes different edge types, Ap (v) is the set of neighbors of node v with respect to
the pth edge type. We choose the message function mp to be a single dense layer for each edge type
P and the aggregation operator ㊉ as the sum of all incoming messages. We use the GRU cell (Cho
et al., 2014) as the recurrent unit f. The final node embedding hvT is computed by unrolling equation
4 for T time steps.
Finally the value of expression e is computed by taking a weighted sum of the final node embedding
hvT and passing through a dense layer as follows:
V(e) = O (Pv∈N(e) σ (g(hT, h0))∙ r(hT))	(5)
where g : R2d → Rd, r : Rd → Rd, O : Rd → RH are all one-layer dense networks, σ denotes the
sigmoid function, and V(e) = [V ((e, 0)), V ((e, 1)), . . . , V ((e, H - 1))]> ∈ RH.
We train the above GNN to approximate the optimal value function V?. Let V(S) = V(es)[ts] the
value function V computed for expression es and time budget ts . To track an approximate lower
bound of the optimal value function V?, We minimize the loss l(V(s) - Vtarget(s)∕ts). The target
value Vtarget (s) is defined in Algorithm 2 and corresponds to the best cost improvement obtained
With the current policy in ts steps. Normalization by ts is introduced to ease optimisation by ensuring
that target values for all outputs of V are in a similar magnitude. Thus the value function estimate
V(S) canbe obtained from per-step value estimate V(S) as V(S) = t ∙ V(s). For the loss function l
We use the Huber loss. Details about the optimiser used to minimize the loss l are given in Appendix
B. In the pseudocode in Algorithm 1, this optimisation is represented With the function fit.
4	Related Work
Knossos builds on a long tradition of compiler technology. Similarly to traditional compliers (Santos
& Peyton-Jones, 1992; Lattner & Adve, 2004) and the more recent deep learning compliers such as
Myia (van Merrienboer et al., 2018), DLVM (Wei et al., 2018), ISAM (Sotoudeh et al., 2019) and
GLOW (Rotem et al., 2018), Knossos uses an intermediate representation to optimize programs.
HoWever, While these approaches rely on layers of hand-coded optimisation heuristics, Knossos
learns the algorithm used to optimize its programs.
In this respect, Knossos is a spiritual successor of benchmark-driven hardWare-agnostic optimisation
approaches in computational linear algebra (Padua, 2011) and signal processing (Frigo & Johnson,
1998). HoWever, unlike these approaches, Knossos is a fully-fledged complier, and can optimize
arbitrary programs. Moreover, thanks to its Reinforcement Learning-driven optimizer, Knossos has
an advantage over existing approaches that attempt to learn hoW to optimize arbitrary code. For
example, Bunel et al. (2017) learns parameters of a code optimizer With a hard-coded hierarchy.
REGAL (PaliWal et al., 2019) only learns the hyper-parameters for a fixed genetic algorithm that
preforms the actual optimisation. The TVM compiler (Chen et al., 2018a) learns a cost model over
programs, but uses simple simulated annealing to perform the optimisation. Similarly, Chen et al.
(2018b) handles only index summation expressions and again relies on simulated annealing. LIFT
(SteuWer et al., 2017) defines an intermediate language suited for expressing numerical computa-
tion, but focuses on providing the right set of reWrite rules rather than on the program optimisation
process itself. In Section 5, We demonstrate that the RL optimizer used by Knossos outperforms this
approach by a large margin.
Knossos is also related to JAX (Frostig et al., 2018), Which performs just-in-time compilation of
Python code using the XLA backend (XLA authors, 2016). Knossos differs from JAX in tWo Ways.
6
Under review as a conference paper at ICLR 2020
arithmetic
common
subexpression	Cost = 6.1
elimination
arithmetic
subexpression	CoSt = 6.1
elimination	(optimal)
Figure 3: Example of tricky optimisation task. Two expressions are similar but the optimal rewrite
strategies are different.
First, it uses efficient RL code optimisation, which is architecture-agnostic. In fact, since Knossos
generates C++ code, it supports a much broader variety of target architectures. Also, unlike JAX, it
makes use of the benefits of a statically typed languages. In terms of scope, Knossos is also similar
to Zygote for Julia (Innes et al., 2019). However, unlike these compliers, Knossos makes use of an
RL-driven code optimizer.
Since Knossos provides first class support for automatic differentiation, it is also related to estab-
lished deep learning frameworks (Maclaurin et al., 2015; Abadi et al., 2016; Paszke et al., 2017).
However, unlike Knossos, these frameworks do not learn how to optimize code, instead relying on
manually-prepared back-ends. Moreover, using them either requires meta-programming, where the
user has to use a high-level language to specify the desired computation graph using constructions
external to the language (Abadi et al., 2016), or is constrained to a restricted subset of the language
(Paszke et al., 2017). In contrast, the Knossos language can be used directly, without manually
specifying computation graph constructs or restricting oneself to an allowed subset of the language.
In parallel, the idea of automated rewriting to achieve a given objective was explored in the context
of automated theorem provers. This is conceptually related to our approach since finding an equiv-
alence between formulae is the same as finding a proof that they are equal. However, recent work
in this space has substantial differences in scope. In particular, state-of-the-art work that searches
for refutational proofs in first-order logic (Zombori et al., 2019; Kaliszyk et al., 2018) uses hard-
coded features and cannot learn any new ones. Also, the optimal objective is very different. While
a mathematical proof is only correct when completely reduced to a tautology, we are satisfied with
simplifying an expression by a certain margin, not necessarily in the most optimal way possible.
For the Reinforcement Learning part, our algorithm differs from standard techniques in that it has a
much larger action space and a state space that consists of graphs, which makes the application of
traditional RL algorithms like DQN (Mnih et al., 2013), A2C (Mnih et al., 2016) and PPO (Schulman
et al., 2017) ineffective. AlphaGo (Silver et al., 2016; 2017), which also performs a search over a
large state space, but differs from Knossos in that it learns for pixel observations and uses an action
space of bounded size. Reinforcement Learning has also been applied to expression rewriting and
scheduling problems (Chen & Tian, 2019). However, since this approach used actor-critic RL that
does not exploit reset, it less well-suited for compilation tasks as described in Section 3.
Generalisation to Unseen Data
5	Benchmarks
We evaluated Knossos in three settings. First, to understand how close and reliably we can achieve
the best optimisation, we applied Knossos to a manually curated set of arithmetic expressions, where
we know the best available sequence of rewrites. Second, we applied Knossos to a set of linear
algebraic operations, which are representative of typical workloads in numerical computing. Third,
7
Under review as a conference paper at ICLR 2020
(a) Generalisation to Unseen Data
Figure 5: Performance of Knossos on a set of arithmetic expressions. Horizontal axis shows epochs,
vertical axis shows cost. Shaded area spans the 20% and 80% percentile over 10 repetitions.
(b) Bootstrap Mode
in order to reflect common modern use-cases, we evaluated Knossos on a computer vision task that
uses a convolutional neural network. Since Knossos is as an alternative to traditional compliers,
we compare it to a hand-written rule-based transpiler of the
Knossos IL, which we call ksc. Both Knossos and ksc output
C++ , which is compiled to binary using gcc with optimisation
enabled, ensuring a fair comparison. We describe the results
below.
While arithmetic expressions are simple, optimising them is
not always a simple task. Figure 3 shows an example of two
similar arithmetic expressions. Although they look very simi-
lar, they require different optimisation strategy to reach to the
optimal form. The left expression gets to optimal by an arith-
metic simplification (×x to a denominator and a numerator)
but the right expression gets to optimal by a common subex-
pression elimination. It is difficult for a rule-based compiler to
distinguish the two and optimise such similar expressions us-
ing different strategies. To test Knossos on arithmetic expres-
sions, we used a training set of 36 arithmetic expressions and
a test set of 12 different ones. The details of the experimental
setup are given in Appendix B. In this setting, we pick 6 ex-
pressions randomly from a training set to train in each epoch.
We ran training for 30 epochs and running 10 repetitions for
each experiment with different random seeds. Search depth
was limited to 10 and the termination condition in A? was set
to 5000 evaluations of the value function. See Appendix B for
the full details including network parameters. We show the re-
sults in Figure 5a. It can be seen from the figure that Knossos
achieved the oracle cost for all expressions. We also performed
an ablation, comparing Knossos to A? algorithm (shown as
NoGNN) that does not perform the GNN recurrence in equa-
tion 4. As a baseline, we compared to greedy best-first search,
which picks a next state to explore greedily without using the
value function f(s) := c(s0) - c(s). We also show a com-
parison to random search and the initial cost of the expression,
before any optimisation.
Bootstrap Mode Similarly to a traditional complier, where
we are given a concrete program to optimize, the expressions
used to evaluate Knossos in this benchmark were the same
ones that we used used during training. Even in this setup,
Figure 4: General Matrix Mul-
tiply (GEMM) program rewrite
sequence obtained by Knossos.
The initial expression was obtained
from our rule-based ksc compiler
and shown in the middle. The fi-
nal expression was obtained after
10 rewriting steps and shown in the
bottom.
8
Under review as a conference paper at ICLR 2020
(a) Basic Linear Algebra (b) Convolutional Neural Network
Figure 6: Performance of Knossos on basic linear algebra and convolutional network. Shaded area
indicates one standard deviation.
(a) Basic Linear Algebra
(b) Convolutional Neural Network
-wall time comparison with ksc.
Figure 7: Comparison of wall time. Shaded area indicates one standard deviation.
Knossos still generalises, but it does it across sub-expressions of the expressions in the training set.
We tested that on 8 expressions, training for 30 epochs. Other experimental setup is the same as
Arithmetic Expressions. Figure 5a shows the comparison of the minimum cost achieved by each
agent. It can be seen from the figure that Knossos achieved the best possible cost for all expressions.
Linear Algebra Primitives Numerical linear algebra is fundamental to most calculations in sci-
entific computing and machine learning. Primitives such as vector multiplication, plane rotation,
matrix multiplications and similar primitives often represent the most time-consuming part of the
given computation. To evaluate the performance of Knossos on in this setting, we trained on a set
of 11 such linear algebra primitives and evaluated on General Matrix Multiplication (GEMM). We
trained for 5 epochs, each of which included optimisation of cost of 6 primitives. Search depth was
limited to 30 and the termination condition in A? was set to 5000 evaluations of the value function.
Figure 6a shows the cost of GEMM. The plot shows results for 10 independent runs of the Knossos
code optimizer on the same input source file. We used an augmented set of training rules, which
included vector operations (see Table 4 in Appendix). Because of the complexity of the task, we
split the search into two phases of 15 steps each. The training phases differ in the set of allowed
rules. In the first phase, we only allow rules that result in large changes to the cost (Table 4). In the
second phase, we allow all rules. The shaded area represents one standard deviation across the runs
of Knossos. Results show that Knossos produced code of lower cost than the output of the traditional
ksc complier according to our cost model. We also performed a benchmark using wall clock time,
shown in Fig. 7a, again showing an improvement. In addition, we performed a qualitative evaluation
of the output in Fig. 4. In the program obtained by ksc (middle listing), three temporary variables
mat_x, mat_x_6, and mat_y corresponding to the result of A ∙ B, α ∙ mat_x, and β ∙ C, respectively,
are created. In the output of Knossos (bottom listing), all the temporary variables are gone. Hence,
Knossos has discovered a form of loop fusion - the type of optimisation that previously had to be
built into a complier by a laborious manual process.
Convolutional Network In order to evaluate Knossos on workloads characteristic of modern ma-
chine learning pipelines, we also evaluated Knossos on a computer vision task. We optimize a code
for training a convolutional deep network on the MNIST dataset (LeCun, 1998). The source code
9
Under review as a conference paper at ICLR 2020
Figure 8: Reverse mode of convolutional network rewrite sequence obtained by Knossos. The initial
expression was obtained from our rule-based ksc compiler and shown in the bottom left. The final
expression was obtained after 32 rewriting steps and shown in the bottom right. The expression in
the bottom middle corresponds to the highest point in the cost sequence.
represents a typical implementation of a deep learning algorithm and contains primitives such as
dense layers, convolutional layers, pooling layers, and so on. While MNIST is a basic benchmark,
we stress that the goal of Knossos was code optimisation as opposed to the computer vision task
itself. We trained on 5 expressions and evaluated on a reverse mode of a convolutional layer. We
fixed the search depth to 40. The termination condition in A? was set to 30000 evaluations of the
value function. We used an augmented set of training rules and split the search into two phases of
20 steps each, allowing rules that result in large changes to the cost in the first phase and all rules
in the second phase. Results are shown in Figure 6b for the cost model and Figure 7b for the wall
clock time. The shaded area represents the standard deviation across the runs of Knossos and the
resulting binary. As above, the Knossos optimizer produced code that outperformed the baseline.
Summary of Benchmarks We have demonstrated that Knossos is capable of producing code that
is faster than the output of a traditional complier. Moreover, unlike traditional compliers, Knossos
does not rely on hand-crafted optimisation passes that are very laborious to implement. Instead,
traditional optimisation passes are replaced by atomic rewrite rules that can be combined in many
ways. In fact, in our benchmark of linear algebra primitives, Knossos was able to automatically
discover loop fusion, an optimisation strategy long known to complier designers. Knossos code in
our experiments can perform both training and inference and can be run on any hardware supporting
the C++ toolchain, including inexpensive embedded devices.
6 Conclusions
We have introduced Knossos, a new complier targetting machine learning and numerical compu-
tation. Thanks to its automatic code optimisation, Knossos produces binaries that achieve better
run-times than a traditional, rule-based complier. Knossos can deal with complex code generated by
automatic differentiation and automatically discover optimisations that previously required careful
complier design. We believe that Knossos will pave the way towards a new generation of future
compliers, which will crucially rely on automatically inferring the correct optimisations.
10
Under review as a conference paper at ICLR 2020
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry
Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A System for Large-Scale Machine Learning. In
Kimberly Keeton and Timothy Roscoe (eds.), 12th USENIX Symposium on Operating Systems Design and
ImPIementation, OSDI2016, Savannah, GA, USA, November 2-4, 2016,pp. 265-283. USENIX Association,
2016.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2-3):235-256, 2002.
Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfsha-
gen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree
search methods. IEEE Transactions on ComPutational Intelligence and AI in games, 4(1):1-43, 2012.
Rudy Bunel, Alban Desmaison, M. Pawan Kumar, Philip H. S. Torr, and Pushmeet Kohli. Learning to superop-
timize programs. In 5th International Conference on Learning RePresentations, ICLR 2017, Toulon, France,
APril 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q. Yan, Haichen Shen, Meghan Cowan,
Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: An Automated
End-to-End Optimizing Compiler for Deep Learning. In Andrea C. Arpaci-Dusseau and Geoff Voelker
(eds.), 13th USENIX SymPosium on OPerating Systems Design and ImPlementation, OSDI 2018, Carlsbad,
CA, USA, October 8-10, 2018, pp. 578-594. USENIX Association, 2018a.
Tianqi Chen, Lianmin Zheng, Eddie Q. Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, and
Arvind Krishnamurthy. Learning to Optimize Tensor Programs. In Samy Bengio, Hanna M. Wallach,
Hugo Larochelle, Kristen Grauman, NiColo Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, 3-8 December 2018, Montreal, Canada,pp. 3393-3404, 2018b.
Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. NeurIPS,
abs/1810.00337, 2019. URL http://arxiv.org/abs/1810.00337.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv PrePrint arXiv:1406.1078, 2014.
Conal Elliott. The simple essence of automatic differentiation. PACMPL, 2(ICFP):70:1-70:29, 2018. doi:
10.1145/3236765.
Matteo Frigo and Steven G. Johnson. FFTW: An adaptive software architecture for the FFT. In Proceedings of
the 1998 IEEE International Conference on Acoustics, SPeech and Signal Processing, ICASSP ’98, Seattle,
Washington, USA, May 12-15, 1998, pp. 1381-1384. IEEE, 1998. ISBN 978-0-7803-4428-0. doi: 10.1109/
ICASSP.1998.681704.
Roy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via high-level tracing.
2018. URL http://www.sysml.cc/doc/2018/146.pdf.
Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum
cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100-107, 1968.
Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckus, Elliot Saba, Viral B Shah, and Will Tebbutt.
Zygote: A differentiable programming system to bridge machine learning and scientific computing. arXiv
PrePrint arXiv:1907.07587, 2019.
Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and Miroslav Olsak. Reinforcement Learning of Theorem
Proving. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada, pp.
8836-8847, 2018.
Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European conference on machine
learning, pp. 282-293. Springer, 2006.
11
Under review as a conference paper at ICLR 2020
Chris Lattner and Vikram S. Adve. LLVM: A Compilation Framework for Lifelong Program Analysis &
Transformation. In 2nd IEEE / ACM International Symposium on Code Generation and Optimization (CGO
2004), 20-24 March 2004, San Jose, CA, USA, pp. 75-88. IEEE Computer Society, 2004. ISBN 978-0-
7695-2102-2. doi: 10.1109/CGO.2004.1281665.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks.
In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4,
2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.05493.
Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Autograd: Effortless gradients in numpy. In ICML
2015 AutoML Workshop, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Interna-
tional conference on machine learning, pp. 1928-1937, 2016.
OpenAI. AI and Compute. https://openai.com/blog/ai-and-compute/, 2018. Blog post.
David A. Padua. Automatically Tuned Linear Algebra Software (ATLAS). In Encyclopedia of Parallel Com-
Puting,pp. 101.Springer, 2011. ISBN 978-0-387-09765-7. doi:10.1007/978-0-387-09766-4_2061.
Aditya Paliwal, Felix Gimeno, Vinod Nair, Yujia Li, Miles Lubin, Pushmeet Kohli, and Oriol Vinyals. REGAL:
Transfer Learning For Fast Optimization of Computation Graphs. CoRR, abs/1905.02494, 2019.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. 2017.
Martin L Puterman. Markov Decision Processes.: Discrete Stochastic Dynamic Programming. John Wiley &
Sons, 2014.
Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Summer Deng, Roman Dzhabarov, James Hegeman, Roman
Levenstein, Bert Maher, Nadathur Satish, Jakob Olesen, Jongsoo Park, Artem Rakhov, and Misha Smelyan-
skiy. Glow: Graph Lowering Compiler Techniques for Neural Networks. CoRR, abs/1805.00907, 2018.
Andre L. M. Santos and Simon L. Peyton-Jones. On Program Transformation in the Glasgow Haskell Compiler.
In John Launchbury and Patrick M. Sansom (eds.), Functional Programming, Glasgow 1992, Proceedings
of the 1992 Glasgow WorkshoP on Functional Programming, Ayr, Scotland, UK, 6-8 July 1992, Workshops
in Computing, pp. 240-251. Springer, 1992. ISBN 978-3-540-19820-8.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv PrePrint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Do-
minik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray
Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks
and tree search. Nature, 529(7587):484-489, 2016. doi: 10.1038/nature16961.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowl-
edge. Nature, 550(7676):354, 2017.
Matthew Sotoudeh, Anand Venkat, Michael J. Anderson, Evangelos Georganas, Alexander Heinecke, and Ja-
son Knight. ISA mapper: A compute and hardware agnostic deep learning compiler. In Francesca Palumbo,
Michela Becchi, Martin Schulz, and Kento Sato (eds.), Proceedings of the 16th ACM International Confer-
ence on ComPuting Frontiers, CF 2019, Alghero, Italy, APril 30 - May 2, 2019, pp. 164-173. ACM, 2019.
ISBN 978-1-4503-6685-4. doi: 10.1145/3310273.3321559.
Michel Steuwer, Toomas Remmelg, and Christophe Dubach. Lift: a functional data-parallel IR for high-
performance GPU code generation. In Proceedings of the 2017 International SymPosium on Code Gen-
eration and OPtimization, CGO 2017, Austin, TX, USA, February 4-8, 2017, pp. 74-85, 2017. URL
http://dl.acm.org/citation.cfm?id=3049841.
12
Under review as a conference paper at ICLR 2020
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second
edition, 2018.
Alan Mathison Turing. On computable numbers, with an application to the entscheidungsproblem. Proceedings
of the London mathematical society, 2(1):230-265, 1937.
Bart van Merrienboer, Olivier Breuleux, Arnaud Bergeron, and Pascal Lamblin. Automatic differentiation in
ML: Where we are and where we should be going. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, NiColo Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Pro-
cessing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
3-8 December 2018, Montreal, Canada,pp. 8771-8781, 2018.
Richard Wei, Lane Schwartz, and Vikram S. Adve. DLVM: A modern compiler infrastructure for deep learning
systems. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Workshop Track Proceedings. OpenReview.net, 2018.
XLA authors. TensorFlow XLA (Accelerated Linear Algebra). https://www.tensorflow.org/xla/overview, 2016.
Zsolt Zombori, Adrian Csiszarik, Henryk MiChaleWski, Cezary Kaliszyk, and Josef Urban. Towards Finding
Longer Proofs. CoRR, abs/1905.13100, 2019.
13
Under review as a conference paper at ICLR 2020
Table 1: List of Experimental Settings
Arithmetic Expressions Parameter	Value
Number of training expressions	36
Number of test expressions	12
Rewrite rules	Simple rule set
Number of evaluation budget per epoch	5000
Minimum batch size for evaluation	16
Maximum Search Depth Basic Linear Algebra	10
Number of training expressions	11
Number of test expressions	1
Rewrite rules	Extended rule set
Number of evaluation budget per epoch	30000
Minimum batch size for evaluation	16
Maximum Search Depth	30
Convolutional Neural Network	
Number of training expressions	5
Number of test expressions	1
Rewrite rules	Extended rule set
Number of evaluation budget per epoch	30000
Minimum batch size for evaluation	16
Maximum Search Depth	40
Appendices
Appendix A	Knossos Intermediate Representation
For background, we give a brief overview of the intermediate representation (IR) used by the Knos-
sos complier. the Knossos IR provides a convenient symbolic form for the reinforcement learning
optimizer. It also has a LISP-like surface syntax, which we used to implement our programs. In
the future, we plan to provide transpilers, allowing for the compilation of code written in other lan-
guages into Knossos. We provide a sample Knossos program in Figure 4.In order to facilitate Ma-
chine Learning workloads, the Knossos IL has native support for automatic differentiation. We use
a new unified view of automatic differentiation as generalised transposition (Elliott, 2018). Rather
than having an explicit distinction between forward mode and reverse mode AD, Knossos uses uses
a type system together with a set of consistent rewrite rules. Whenever the gradient operator is used
as part of a Knossos algorithm, the complier first generates a syntax tree corresponding to the dif-
ferentiated program and then applies rewrites to optimize the cost of its execution. This means that
the resulting AD algorithm is tailor-made and optimized with that exact use case in mind. This is in
contrast to systems such as PyTorch, which have hard-coded routines for backward-mode AD. From
the perspective of the user, this process is completely transparent in the sense that taking gradients
can be applied to any piece of Knossos code.
While the details of this process are beyond the scope of this paper, from the perspective of this
work, the important feature of AD is that it corresponds to a transformation of the abstract syntax
tree. The resulting AST can then be optimised in the same way as any other code.
Appendix B	Reproducibility and Details of Experimental Setup
We now describe the parameters used to perform the experiments reported on in the paper. The
parameters used by A? in the four tasks described in Sec. 5 are listed in Tab. 1. The hyper-parameters
for the value network training are given in Tab. 2.
14
Under review as a conference paper at ICLR 2020
Table 2: List of hyperparameters
Value Function
Model Parameter
Number of features per node	200
Number of propagation steps	10
Activation function for GNN	tanh
Dropout rate for GNN	0
Number of hidden layers in MLP	2
Number of dimensions of hidden layers in MLP	200
Activation function for MLP	tanh
Dropout rate for MLP	0.2
Training Parameter	
Loss function	Huber loss
Optimizer	Adam optimizer
Learning rate	0.0001
Batch size	16
In the Graph Neural Network, initial node features are one-hot vectors that represent the node types.
The used node types are: constant, variable, let, if, tuple, select, +, -, *, /, exp, log, ==, >, >=,
or, build, apply, lam, sum, sumbuild, constVec, and deltaVec. Edge types are listed in Tab. 2b. The
auxiliary edge type“is-identical” is inserted to identify identical subexpressions. It was added so that
it is easier to learn re-writes that rely on matching expressions. The GNN was implemented using a
sparse adjacency matrix instead of dense matrix in order to conserve GPU memory in settings where
some expressions grow beyond > 10000 nodes during training. We ran the GNN recursion 10 times.
For optimization we used the Adam optimizer with learning rate 0.0001 and set the dropout rate zero
for GNN and 0.2 for the MLP.
Appendix C	Rewrite rules
We list the basic rule set used in the arithmetic expressions benchmark in Tab. 3. The additional
rewrite rules used in basic linear algebra and convolutional neural network are given in Tab. 4.
Appendix D	Additional ablations.
In addition to A? search, we compare the performance of Monte Carlo Tree Search (Browne et al.,
2012) using the UCT formula (Auer et al., 2002; Kocsis & Szepesvari, 2006). In order to disam-
biguate across subtly different versions of the algorithm, we describe it below.
Each iteration of MCTS consists of four steps (Algorithm 3).
1.	Selection: Starting from the root, a tree policy is recursively descends through the tree until
it reaches a leaf node.
2.	Expansion: A child node is added to expand the tree.
3.	Simulation: A rollout policy is applied from the new node until the end of the episode.
4.	Back-up: The simulation result is backed up through the selected nodes to update their
statistic.
The tree policy πt and rollout policy πr are defined as follows.
T xi∖q∖ X ar<rmaY	XX(SO) -k R / ln n(S)
πt(als) — argmaxa∈A (nF)十 βV n(s0 )+1
∏r (a|s) = softmaxa∈ a (R(s, a)十 V (s'), α)
(6)
(7)
Here, n(s) is a visitation count of state s, and β is a constant to control the exploration bonus.
X(s0)/n(s0) is the average cost reduction achieved by a set of trajectories which passed through
15
Under review as a conference paper at ICLR 2020
Table 3: List of rewrite rules used for arithmetic expressions.
LHS	Simple Rule Set RHS	Side conditions
(add 0.0 a) (mul 1.0 a) (sub a 0.0) (div a 1.0) (add a (mul b -1.0)) (mul a b) (add a b) (sub a a) (add (add a b) c) (sub (add a b) c) (add (sub a b) c) (sub (sub a b) c) (sub a (sub b c)) (mul (mul a b) c) (div (mul a b) c) (mul (div a b) c) (div (div a b) c) (div a (div b c)) (div (div a b) c) (mul a (add b c)) (add (mul a b) (mul a c)) (mul a (sub b c)) (add a (mul -1.0 b)) (sub (mul a b) (mul a c)) (add (div y x) (div z x)) (sub (div y x) (div z x)) (div a (mul b c)) (mul 0.0 a) e (op (let (x e) x)) (let (x e1) (let (x e1) e2)) (let (x e1) e2) (let (x e1) e2)	Arithmetic Rules a a a a (sub a b) (mul b a) (add b a) 0.0 (add (add a c) b) (add (sub a c) b) (sub (add a c) b) (sub (sub a c) b) (sub (add a c) b) (mul (mul a c) b) (mul (div a c) b) (div (mul a c) b) (div (div a c) b) (div (mul a c) b) (div a (mul b c)) (add (mul a b) (mul a c)) (mul a (add b c)) (sub (mul a b) (mul a c)) (sub a b) (mul a (sub b c)) (div (add y z) x) (div (sub y z) x) (div (div a b) c) 0.0 Binding Rules (let (x e) x) (let (x e) (op x)) (let (x e1) e2) (let (x e1) e2[x/e1]) e2	x ∈/ op replace all free x in e2 with e1 x ∈/ e2
Table 4: List of rewrite rules used in addition to simple rule set (Table 3) for Basic Linear Algebra
and Convolutional Neural Network benchmarks. In the two-phase strategy described in Sec. 5, only
the rules with checkmarks in the last column are used for the first 15 epochs.
Extended Rule Set			Include in phase 1
LHS	RHS	Side conditions	
(size (build n (lam x b)))	n		
(index arg (build sz (lam x body)))	(let (x arg) body)		X
(select i (tuple e1 e2 e3...))	ei		
(sum (build n (lam x b)))	(sumbuild n (lam x b))		X
(sumbuild n (lam x (deltaVec m i v)))	(deltaVec m i (sumbuild n (lam x v)))	x∈/ m ∩ x ∈/ i.	X
(sumbuild n (lam x (tuple f g)))	(tuple (sumbuild n (lam x f)) (sumbuild n (lam x g)))		X
(sumbuild n (lam x (if c t f)))	(if c (sumbuild n (lam x t)) (sumbuild n (lam x f)))		X
(sumbuild n (lam x c))	(mul n c)	x ∈/ c ∩ c is a constant	X
(sumbuild o (lam oi (add e1 e2)))	(add (sumbuild o (lam oi e1)) (sumbuild o (lam oi e2)))		X
(sumbuild o (lam oi (build n (lam ni e))))	(build n (lam ni (sumbuild o (lam oi e))))		X
(add (deltaVec o oi e1) (deltaVec o oi e2))	(deltaVec o oi (add e1 e2))		X
(add (tuple x y) (tuple a b))	(tuple (add x a) (add y b))		X
(mul (tuple x y) z)	(tuple (mul x z) (mul y z))		X
(if True a b)	a		
(if False a b)	b		
(if p True False)	p		
(if p a a)	a		
(eq a a)	True		
16
Under review as a conference paper at ICLR 2020
Algorithm 3 Monte Carlo Tree Search (MCTS)
function MCTS(S0, V, d, α, c)
πt⑷S)-argmaχa∈A (X (SO)/n(SO)+巩 nnsn(+ι)
∏r (a|s) - Softmaxa∈a(R(s, a) + V(s0), α)
M — 0
∀s ∈ S : n(s) J 0	. Initialize a visitation count
while not term_condition() do
Tt J TREESEARCH(S0, d, πt)
Sl J tail(Tt )
Tr J ROLLOUTSEARCH(Sl, d - length(Tt), πr)
T J concat([S0], Tt, Tr )
x J c(S0) - mins∈Tr c(S)
. x is the max cost reduction found by rollout
for all S ∈ Tt do	. MCTS backup
X(S)JX(S)+x
n(S) J n(S) + 1
end for
MJM∪{T}
end while
return M
end function
function TREESEARCH(S, d, π)
T J []
while length(T ) < d and n(S) > 0 do
a J π(a∣s)	. Tree policy π is deterministic.
s J a ◦ s
T J concat(T, [S])
end while
return T
end function
s0 in M: X(s0) = T2h∈H∙.so∈τ maxsd∈τd(sd)≥d(s0) c(so) — c(sd), and M is a set of trajectories
sampled at current epoch for the current expression so far. The more the state s0 is visited, the
exploration bonus (β J ;%*)]) is reduced. This way, the agent is encouraged to try a diverse set of
actions.
We evaluated the performance of A? search and MCTS for both training and test. The experimental
setup is the same as the Generalisation to Unseen Data experiment in Section 5 except for the used
search algorithm. For MCTS, we used α = 5.0 and β = 0.5 for both training and test.
Figure 9a shows the results of running all possible combinations of search algorithms when used
for training and test in various configurations. Overall, using A? for both training and test achieved
the best performance. In particular, when we fixed the algorithm used during test to A? and varied
the training algorithm between A? and MCTS, A? achieved a significantly lower the total minimum
cost than MCTS. Similarly, when we fixed the algorithm used for training to A? and compared the
performance during testing A? achieved significantly lower cost than MCTS again.
Appendix E	List of Expressions
17
Under review as a conference paper at ICLR 2020
Table 5:	List of expressions for Generalization to Unseen Expressions.
Train expression set
(mul(div (mul 2.0 (add X y))(sub (add X y) 1))(mul X y))
(add (add (mul x 2.0) 1) (div (add (div x (sub y 1)) (mul x 2.0)) 2))
(div (add (mul 2.0 (mul X y)) (mul X y)) (div 1.0 (add X 1)))
(div (add (add (div X (sub y 1)) (log X)) 1) (sub (mul (div X (sub y 1)) 3) 1))
(add (div (div 1.0 (add X 1)) (add (add X y) (div 1.0 (add X 1)))) (div 1.0 (add X 1)))
(mul (add (log X) (mul X 2.0)) (mul (log X) 3))
(mul (add (add X y) (log X)) (mul (add X y) 3))
(div (add (add (mul X 2.0) 1) 1) (sub (add (add X y) (mul X 2.0)) 1))
(sub (mul (mul 2.0 (mul X y)) (mul X y)) (mul X 2.0))
(mul (add (div X (sub y 1)) 1) (add (mul X y) (div X (sub y 1))))
(mul (div (div 1.0 (add X 1)) (add (div X (sub y 1)) (div 1.0 (add X 1)))) (div 1.0 (add X 1)))
(add (add (log X) (div 1.0 (add X 1))) (div (mul (log X) 3) 2))
(add (add (add X y) (div X (sub y 1))) (div (mul (add X y) 3) 2))
(div (mul (mul X 2.0) (mul X 2.0)) (sub (mul X 2.0) (div 1.0 (add X 1))))
(mul (div (mul 2.0 (mul X y)) (sub (mul X y) 1)) (log X))
(add (add (div X (sub y 1)) 1) (div (add (add X y) (div X (sub y 1))) 2))
(div (add (mul 2.0 (div 1.0 (add X 1))) (div 1.0 (add X 1))) (mul X 2.0))
(div (add (add (log X) (mul X y)) 1) (sub (mul (log X) 3) 1))
(div (add (add (add X y) (mul X 2.0)) 1) (sub (mul (add X y) 3) 1))
(add (div (mul X 2.0) (add (mul X y) (mul X 2.0))) (mul X 2.0))
(mul (add (mul X y) (div X (sub y 1))) (mul (mul X y) 3))
(div (add (add (div X (sub y 1)) 1) 1) (sub (add (div 1.0 (add X 1)) (div X (sub y 1))) 1))
(sub (mul (mul 2.0 (div 1.0 (add X 1))) (div 1.0 (add X 1))) (log X))
(mul (add (log X) 1) (add (add X y) (log X)))
(mul (add (add X y) 1) (add (div 1.0 (add X 1)) (add X y)))
(mul (div (mul X 2.0) (add (log X) (mul X 2.0))) (mul X 2.0))
(add (add (mul X y) (add X y)) (div (mul (mul X y) 3) 2))
(div (mul (div X (sub y 1)) (div X (sub y 1))) (sub (div X (sub y 1)) (mul X 2.0)))
(mul (div (mul 2.0 (div 1.0 (add X 1))) (sub (div 1.0 (add X 1)) 1)) (mul X y))
(add (add (log X) 1) (div (add (div X (sub y 1)) (log X)) 2))
(add (add (add X y) 1) (div (add (mul X y) (add X y)) 2))
(div (add (mul 2.0 (mul X 2.0)) (mul X 2.0)) (div X (sub y 1)))
(div (add (add (mul X y) (div 1.0 (add X 1))) 1) (sub (mul (mul X y) 3) 1))
(add (div (div X (sub y 1)) (add (log X) (div X (sub y 1)))) (div X (sub y 1)))
(mul (add (div 1.0 (add X 1)) (add X y)) (mul (div 1.0 (add X 1)) 3))
(div (add (add (log x) 1) 1)(SUb (add (mul X 2.0) (log x)) 1))
Test expression set
(div (add (mul 2.0 (add X y)) (add X y))(mul X 2.0))
(div (add (add (mUl X 2.0) (mUl X y)) 1) (sUb (mUl (mUl X 2.0) 3) 1))
(add (div (mUl X y) (add (div X (sUb y 1)) (mUl X y))) (mUl X y))
(mUl (add (div X (sUb y 1)) (div 1.0 (add X 1))) (mUl (div X (sUb y 1)) 3))
(div (add (add (div 1.0 (add X 1)) 1) 1) (sUb (add (log X) (div 1.0 (add X 1))) 1))
(sUb (mUl (mUl 2.0 (log X)) (log X)) (add X y))
(sUb (mUl (mUl 2.0 (add X y)) (add X y)) (div 1.0 (add X 1)))
(mUl (add (mUl X 2.0) 1) (add (log X) (mUl X 2.0)))
(mUl (div (mUl X y) (add (add X y) (mUl X y))) (mUl X y))
(add (add (div X (sUb y 1)) (mUl X 2.0)) (div (mUl (div X (sUb y 1)) 3) 2))
(div (mUl (div 1.0 (add X 1)) (div 1.0 (add X 1))) (sUb (div 1.0 (add X 1)) (mUl X y)))
(mUl (div (mUl 2.0 (log X)) (sUb (log X) 1)) (div X (sUb y 1)))
18
Under review as a conference paper at ICLR 2020
(a) Generalization (Train)
Figure 9: Comparison of A? search and Monte Carlo Tree Search.
Total minimum cost
is
epoch
(b) Generalization (Test)
(c) Generalization
IastarsstaQ
tmcts,astar)
[rollou⅛asta4
(m<isvf,astar)
⅛star,mrts)
tπcts,(nτts)
(rolk>utmcts)
(mctsvf,mcts)
tastar,rallouQ
[mτts,rallout)
Irolloutrolloue
Table 6:	List of expressions for Generalization scenario.
Train/Test expression set
(div (div 1.0 x) (add 1.0 (div 1.0 x)))
(add (div (div 1.0 x) (add (div 1.0 x) 1.0)) (div 1.0 x))
(add (div (div 1.0 x) (add (div 1.0 x) 2.0)) (div 1.0 x))
(mul (div x y) (div x y))
(div (mul (div x y) x) y)
(add (div (mul x y) (add 1.0 (mul x y))) (mul x y))
(add (div 1.0 (add 1.0 (mul x y))) (mul x y))
(div (mul x y) (add 1.0 (mul x y)))
19
Under review as a conference paper at ICLR 2020
Figure 10: List of expressions in training set for Linear Algebra Primitives.
(def
float_matrix_multiply (Vec n (Vec m Float))
((alpha : Float) (mat_x : Vec n (Vec m Float)))
(build n
(lam (i : Integer) (build m
(lam (j : Integer) (mul alpha (index i
(index j mat_x))))))))
(def
matrix_matrix_multiply (Vec n (Vec l Float))
((mat_x : Vec n (Vec m Float)) (mat_y : Vec m (Vec l Float)))
(build n
(lam (i : Integer) (build l
(lam (k : Integer) (sumbuild m
(lam (j : Integer) (mul (index i
(index j
mat_x))
(index j
(index k
mat_y))))))))))
(def
vector_vector_add (Vec n Float)
((vec_x : Vec n Float) (vec_y : Vec n Float))
(build n (lam (i : Integer) (add (index i vec_x) (index i vec_y)))))
(def
matrix_matrix_add (Vec n (Vec m Float))
((mat_x : Vec n (Vec m Float)) (mat_y
(build n
Vec n (Vec m Float)))
(lam (i : Integer) (build m
(lam (j : Integer)
j mat_x))
(add (index i (index
(index i (index j
mat_y)))))))
)
(def
outer_product (Vec n (Vec m Float))
((vec_x : Vec n Float) (vec_y : Vec m Float))
(build n
(lam (i : Integer) (build m
(lam (j : Integer) (mul (index i vec_x)
(index j vec_y)))))))
(def
transpose (Vec m (Vec n Float))
((mat_x : Vec n (Vec m Float)))
(build m
(lam (i : Integer) (build n
(lam (j : Integer) (index j (index i
mat_x)))))))
(def
scal (Vec n Float)
((a : Float) (vec_x : Vec n Float))
(build n (lam (i : Integer) (mul a (index i vec_x)))))
(def
axpy (Vec n Float)
((alpha : Float) (vec_x : Vec n Float) (vec_y : Vec n Float))
(build n
(lam (i : Integer) (add (index i (scal alpha vec_x))
(index i vec_y)))))
(def
dot Float
((vec_x : Vec n Float) (vec_y : Vec n Float))
(sumbuild n
(lam (i : Integer) (mul (index i vec_x) (index i vec_y)))))
(def
rscl (Vec n Float)
((vec_x : Vec n Float))
(build n (lam (i : Integer) (div (index i vec_x) (sum vec_x)))))
(def
rot (Tuple (Vec n Float) (Vec n Float))
((x : Vec n Float)
(tuple (build n
(lam
(build n
(lam
(y : Vec n Float) (c : Float) (s : Float))
(i : Integer) (add (mul c (index i x)) (mul s (index
y)))))
(i : Integer) (add (mul (mul -1.0 s) (index i x))
(mul c (index i y)))))))
20
Under review as a conference paper at ICLR 2020
Figure 11: Test expression for Linear Algebra Primitives (General Matrix Multiplication).
(def
gemm (Vec n (Vec l Float))
((var0 : (Tuple Float
(Vec n
(Vec m
Float
(Vec n
(Vec
(Vec
(Vec
m
l
l
Float))
Float))
Float)))))
(let
(let
(let
(let
(let
(beta
(mat_b
(mat_a
(alpha
(mat_c
(get$4$5 var0))
(get$3$5
(get$2$5
(get$1$5
(get$5$5
(let
(mat_x (build
var0))
var0))
var0))
var0))
n (lam
(var4 : Integer)
(build l (lam (k : Integer)
(sumbuild m (lam (var5 : Integer)
(mul
(let
(mat_x_6 (build n
(build
(mul
(index var4 (index var5 mat_a))
(index var5 (index k mat_b))))))))))
(lam (var2 : Integer)
m (lam (var3 : Integer)
alpha (index var2 (index var3 mat_x))))))))
(let
(mat_y (build n (lam (var6 : Integer)
(build m (lam (var1 : Integer)
(build n
(build
(add
(mul beta
(lam (i :
m (lam (j
(index i
(index i
(index var6 (index var1 mat_c))))))))
Integer)
: Integer)
(index j mat_x_6))
(index j mat_y)))))))))))))))
21
Under review as a conference paper at ICLR 2020
Figure 12:	List of expressions in train set for Convolutional Network.
(def
rev$matrix_multiply (Tuple (Vec o (Vec k Float)) (Vec k Float))
((w : Vec o (Vec k Float))
(image : Vec k Float)
(d$r : Vec o Float))
(sumbuild o (lam (oi : Integer)
(let ((a_6 (index oi d$r))
(a_5 (build k (lam (sum$i : Integer) a_6))))
(tuple (sumbuild k (lam (ki : Integer)
(deltaVec o oi
(deltaVec k ki
(mul (index ki image) (index ki a_5))))))
(build k (lam (ki : Integer)
(mul (index ki (index oi w)) (index ki a_5)))))))))
(def
rev$matrix_multiply_transpose (Tuple (Vec k (Vec o Float))
(Vec k Float))
((w : Vec k (Vec o Float))
(image : Vec k Float)
(d$r : Vec o Float))
(sumbuild o (lam (oi : Integer)
(let ((a_6 (index oi d$r))
(a_5 (build k (lam (sum$i : Integer) a_6))))
(tuple (build k (lam (ki : Integer)
(deltaVec o oi
(mul (index ki image) (index ki a_5)))))
(build k (lam (ki : Integer)
(mul (index oi (index ki w)) (index ki a_5)))))))))
(def
rev$max_ (Tuple Float Float)
((x1 : Float) (x2 : Float) (d$r : Float))
(if (gt x1 x2) (tuple d$r 0.0) (tuple 0.0 d$r)))
(def
rev$maxpool (Vec n Float)
((image : Vec n Float) (d$r : Vec m Float))
(sumbuild (div n 2) (lam (ni : Integer)
(let ((t1698 (mul 2 ni))
(t1702 (add 1 t1698))
(t1705 (rev$max_ (index t1698 image)
(index t1702 image)
(index ni d$r))))
(add (deltaVec n t1698 (select t1705 0))
(deltaVec n t1702 (select t1705 1)))))))
(def
rev$expensive (Tuple Float Float)
((x1 : Float) (x2 : Float) (d$r : Float))
(tuple 0.0 0.0))
(def
rev$expensivepool (Vec n Float)
((image : Vec n Float) (d$r : Vec m Float))
(sumbuild (div n 2) (lam (ni : Integer)
(let ((t1720 (mul 2 ni))
(t1724 (add 1 t1720))
(t1727 (rev$expensive (index t1720 image)
(index t1724 image)
(index ni d$r))))
(add (deltaVec n t1720 (select t1727 0))
(deltaVec n t1724 (select t1727 1)))))))
22
Under review as a conference paper at ICLR 2020
Figure 13:	Test expression for Convolutional Network.
(def
rev$conv1d (Tuple (Vec k (Vec l (Vec kn Float)))
(Vec l (Vec n Float)))
((var0 : (Tuple (Vec k (Vec l (Vec kn Float)))
(Vec l (Vec n Float))
(Vec k (Vec n Float)))))
(let
((kernels (get$1$3 var0))
(image (get$2$3 var0))
(d$r (get$3$3 var0)))
(sumbuild k (lam (ki : Integer)
(let (a_6 (index ki d$r))
(sumbuild n (lam (ni : Integer)
(let (a_8 (index ni a_6))
(let (a_7 (build kn (lam (var1 : Integer) a_8)))
(sumbuild kn (lam (kni : Integer)
(let (a_10 (index kni a_7))
(let (a_11 (build l (lam (sum$i : Integer) a_10)))
(sumbuild l (lam (li : Integer)
(let (noi (sub (add ni (div kn 2)) kni))
(let (outside_image (or (gt 0 noi) (gte noi n)))
(add (if outside_image
(tuple (constVec k (constVec l (
constVec kn 0.0)))
(constVec l (constVec n 0.0)))
(tuple (constVec k (constVec l (
constVec kn 0.0)))
(deltaVec l li
(deltaVec n noi
(mul (index kni (index li (
index ki kernels)))
(index li a_11))))))
(tuple (deltaVec k ki
(deltaVec l li
(deltaVec kn kni
(mul (if outside_image
0.0
(index noi (index li
image)))
(index li a_11)))))
(constVec l (constVec n 0.0))))))))
)))))))))))
23