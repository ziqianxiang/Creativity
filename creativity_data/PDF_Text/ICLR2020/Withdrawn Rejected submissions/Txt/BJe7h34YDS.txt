Under review as a conference paper at ICLR 2020
Understanding and Stabilizing GANs’ Train-
ing Dynamics with Control Theory
Anonymous authors
Paper under double-blind review
Ab stract
Generative adversarial networks (GANs) have made significant progress on realistic
image generation but often suffer from instability during the training process. Most
previous analyses mainly focus on the equilibrium that GANs achieve, whereas a
gap exists between such theoretical analyses and practical implementations, where
it is the training dynamics that play a vital role in the convergence and stability
of GANs. In this paper, we directly model the dynamics of GANs and adopt the
control theory to understand and stabilize it. Specifically, we interpret the training
process of various GANs as certain types of dynamics in a unified perspective
of control theory, which enables us to model the stability and convergence easily.
Borrowed from control theory, we adopt the widely-used negative feedback control
to stabilize the training dynamics, which can be considered as an L2 regularization
on the output of the discriminator. We empirically verify our method on both
synthetic data and natural image datasets. The results demonstrate that our method
can efficiently stabilize the dynamics and converge to better results.
1	Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) have shown promise in generating
realistic natural images (Brock et al., 2018) and facilitating unsupervised and semi-supervised
learning (Chen et al., 2016; Li et al., 2017; Donahue & Simonyan, 2019). In GANs, an implicit
generator (G) is defined by mapping a noise distribution to the data space. Since no density function
is defined for the implicit generator, a discriminator (D) is introduced to estimate the density ratio
between the data distribution pD and the generating distribution pG by telling the real samples
from fake ones. G aims to recover the data distribution by maximizing this ratio. This framework
is formulated as a minimax optimization problem, which can be solved by optimizing G and D
alternately. In practice, however, GANs can be extremely sensitive to hyperparameters (Lucic et al.,
2018; Radford et al., 2015), and oscillations are often observed (Liang et al., 2018; Chavdarova &
Fleuret, 2018), thereby suffering from the instability of training (Goodfellow, 2016).
There are some theoretical analyses aiming to understand and stabilize the training process of GANs,
such as designing different objective functions using other statistical divergences (Nowozin et al.,
2016; Nguyen et al., 2017; Du et al., 2018; Mao et al., 2017), and introducing auxiliary regularization
terms (Gulrajani et al., 2017; Arjovsky et al., 2017). However, there is a gap between the theoretical
analyses and practical implementations: most of previous work assumes that (1) D achieves optimal
when training G (Goodfellow et al., 2014; Arjovsky et al., 2017; Gulrajani et al., 2017; Nowozin et al.,
2016); and (2) the optimal D is a smooth function of G (Metz et al., 2016). These two assumptions
are violated in most practical cases. First, GANs are optimized by alternating between k steps of
optimizing D and one step of optimizing G, where k generally takes 1 or 5. It results in a sub-optimal
D, especially in the early stage of the training process. Second, D is constant w.r.t. G when G is
optimized, which results in missing gradients and an unstable training process (Metz et al., 2016).
Instead, to ensure practical convergence, it is the training dynamics of both G and D, i.e., how G (or
D) changes given current G and D during the training process, that should be considered, where the
previous two assumptions are no more required. There are some methods that focus on the dynamics
of the parameters. Mescheder et al. (2017) and Nagarajan & Kolter (2017) propose to model the
dynamics of the parameters using the vector field defined by its gradients. In such cases, the local
convergence and stability are fully determined by the eigenvalues of the Jacobian matrix. Mescheder
et al. (2018) further propose a regularization term to stabilize GANs by adjusting the eigenvalues
1
Under review as a conference paper at ICLR 2020
of the Jacobian matrix. Gidel et al. (2018) analyze the effect of momentum and propose to stabilize
GANs using negative momentum. In these methods, the Jacobian matrix is the key to understand and
stabilize the dynamics, whose calculation is, however, computationally expensive. It largely impedes
us to generalize this analysis to complex models such as neural networks (LeCun et al., 2015).
In this paper, we argue that modeling the dynamics in the function space is more convenient for
convergence analysis. Specifically, we directly model G and D as two dynamics whose output is
considered as a signal of time t, i.e., representing the output of G and D as G(t, z) and D(t, x)
respectively. The control theory (Kailath, 1980) provides a powerful tool to understand them and
analyze their stability. Specifically, differential equations can be used to denote the dynamics, e.g.,
the dynamics of G can be denoted as dGtz
f(G, D), which represents how G changes given
current G and D. By applying Laplacian transformation (Widder, 2015) to the differential equations,
the dynamics can be represented as a transfer function, and then the stability and convergence can be
easily modeled (Kailath, 1980) as introduced in Sec. 2.
Under the above perspective of control theory, we unify the dynamics of G and D as certain dynamics
that are well-studied in control theory for various GANs, including Standard GAN (SGAN), WGAN
and LSGAN (Mao et al., 2017), as illustrated in Sec. 3 and Appendix A. Specifically, the training
process of GANs can be considered as a dynamic whose output is determined by its intrinsic
properties (i.e., the objective function of G and D) and its input (i.e., the data distribution). Within
the proposed framework, a variety of existing methods (Mao et al., 2017; Mescheder et al., 2018;
Gidel et al., 2018) can be considered as certain controller which is widely used in control theory as
discussed in Sec. 4.1 and Appendix C. It is worth noting that through control theory, the stability of
GANs can be easily inferred from the transfer function, instead of analyzing the complicated Jacobian
matrix of the dynamics. The proposed framework provides a promising direction that can further
benefit the training dynamics of GANs using advanced control methods. To verify it, we propose to
use the widely adopted controlling method, the negative feedback (NF) control (Astrom & Hagglund,
1995), to stabilize GANs. What’s more, when applying NF to stable models, the performance can
still be improved. The proposed NF acts as a regularization term which penalizes the L2 norm of
the output of D as we described in Sec. 4. NF is verified on the toy data such as Dirac GAN as well
as the natural images such as CIFAR10 (Krizhevsky et al., 2009). The results demonstrate that our
method can successfully stabilize the dynamics of GANs and outperform the baseline significantly.
2	Preliminary
In this section, we provide the background and preliminary about Laplacian transformation (LT) and
control theory. LT is a powerful tool to present an ordinary differential equation (ODE) as a rational
fraction. Since most dynamics can be represented as an ODE, LT can largely simplify the analysis of
stability by merely analyzing the properties of the rational fraction.
2.1	Laplacian Transformation
Laplacian transformation (LT) (Widder, 2015) can be considered as an extension of Fourier transfor-
mation (FT), which transforms a signal, i.e., a function of time, to a function of complex variables.
Formally, the transformation and its inversion are given by:
∞
F(f)(s) =
0
f(t)e-stdt, F-1 (F)(t)
estF (s)ds,
(1)
where f is a signal that is represented as a function over time t, c is a constant that ensures F (s) ≤ ∞.
s ∈ C is a complex number, i.e., s = σ + ωi with real numbers σ and ω and for each s ∈ C, F(s)
denotes the components of certain frequency in the original signal f similar to FT.
With LT, the integration and derivation can be presented as a single operator, which is given by:
F(f…Cf) FʤU) = S FCf)
(2)
With this property, an ODE can be converted to an algebraic equation by substituting derivation as an
operator s. A simple example is illustrated as follows:
d2x
dt2
-x+u→ s2X = -X+U,
(3)
2
Under review as a conference paper at ICLR 2020
…、	1	，、
gτ 力⑸=E Hs)
Figure 1: The diagram representations of the inputs and dynamics after LT. Left: the original diagram of the spring dynamic in Eqn. (3). Right:
the diagram of a classical controlled dynamic with feedback Hb and controller Hc .
where u and x are the input and output signal correspondingly and U and X are their Laplacian
transformation. This ODE can represent a spring dynamic, whose acceleration 骁 depends on the
input u and the current position x according to Hooke’s law. In this case, the ODE can be solved
by simply applying the inverse Laplacian transformation to X = s2+1 U = Hf(S)U(s). Here
Hf(S) = s2+1 is the transfer function of this dynamic which can also be represented as a diagram
illustrated in Fig. 1. Since the input signal can be considered as summation of infinity Dirac signal at
different positions, i.e., f(t) = f(u)δ(t - u)du where δ denotes the Dirac delta function (Dirac,
1981), we only need to analyze the output of the dynamics for Dirac signal whose LT is constant, i.e.,
F(δ) = 1. Therefore, the transfer function represents the intrinsic properties of the dynamics.
2.2	Control Theory and Negative Feedback
Given a dynamic, or equivalently a transfer function, the stability can be analyzed according to the
roots of the denominator of the transfer function, e.g., S2 + 1 = 0 for Eqn. (3). The roots are called
the poles of the dynamics. Specifically, if all poles’ real parts are negative, the dynamic is stable; if
there are poles which are pure imaginary numbers, the dynamic will oscillate; if there is at least one
pole with positive real parts, the dynamic will diverge (Kailath, 1980). In the above example, the
poles of the dynamic are 0 ± i, which indicates that the object will oscillate around the original.
For those unstable dynamics, we need to stabilize it by adjusting the poles of the dynamics. One
of the most commonly adopted methods here is introducing negative feedback with an auxiliary
controller (Kailath, 1980). The diagram is illustrated in Fig. 1 (right). The Hb(S) denotes the transfer
function of feedback and Hc(S) denotes the transfer function of controller.
In this setting, the dynamic takes the error (i.e., E(S) = U(S) - Hb(S)X(S)) rather than U(S) as
input, resulting the fact that the transfer function of the whole controlled dynamic is given by:
E(S) = U (S)- Hb(S)X (s),X (S) = Hc (S)Hf (S)E(S) → X = _ HcwssH /、U (4)
1 + Hc (S)Hf (S)Hb (S)
which provides us an approach to adjust the poles of an unstable dynamic. For most cases, simply
adopting the negative feedback, i.e., let Hb = Hc = 1, is enough to stabilize a dynamic. In the
following, we provide the dynamics of GANs in the perspective of control theory and introduce the
negative feedback control to stabilize the training process of GANs.
3	Unifying the Dynamics of GANs in Control Theory
In this section, we provide a novel perspective on the dynamic of GANs. Unlike previous methods
that mainly focus on the dynamics of the parameters, we directly model the output of two dynamics
G and D in the function space. In this part, we mainly focus on the Dirac GAN as in Mescheder
et al. (2018) and unregularized WGAN. This method can be extended to other GANs easily, as we
illustrated in Appendix A.
To model the dynamics of GANs, we first denote the output of G and D as a function with respect
to time t, which is also known as a signal, i.e., D = D(t, x) and G = G(t, z). Then the training
process can be represented as a dynamic using an ODE, and the equilibrium is equivalent to the final
value of the dynamics (Oppenheim et al., 1998), i.e., the output of the dynamics with t → ∞.
3.1	The Dynamics of Dirac GAN
In Dirac GAN, the data distribution is defined as pD(x) = δc(x) = δ(x - c) which is the density of
an idealized point mass at point c. The generator distribution is defined as pG(x) = δθ(x), where θ is
the learnable parameter of G. In this case, we use G(t) = G(t, z) for simplicity as its output does
3
Under review as a conference paper at ICLR 2020
PD(X)
Figure 2: The diagram block of Dirac GAN and unregularized WGAN. Left: Dirac GAN, Right: unregularized WGAN.
not depend on z. The discriminator is a simple linear function D(t, x) = φ(t) × x. Therefore, the
minimax optimization problem of Dirac GAN is formulated as:
min max
GD
EpD [φx] - EpG [φx] = cφ - θφ.
(5)
For the discriminator, the objective function is given by: maxφ U(D) = cφ - θφ, which indicates
that the dynamics of D can be formulated as:
d∂UD) = c-θ→ 孚=c-θ→ W= X × (c-θ), ∀χ.	⑹
∂φ	dt	∂t
By considering c - θ as the error signal, i.e., e = c - θ, the dynamics of the discriminator can be
modeled as an integral partD(t,x) =x R0t c - G(t)dt. By applying LT to the dynamic, we have:
F(D(t,x)) = xFs(e) = XE(S) ∀x,
(7)
where we consider the dynamics ofDat each x. For the generator, the objective function is given by
maxGU(G) = θφ. Therefore the dynamics of G can be formulated as:
∂U(G)	dθ(t)	∂G(t) ∂D(t, x)
∂θ = φ → ~dΓ = φ → ∂t = -∂X-
(8)
Since D(t,x) = x Rt C 一 G(t)dt, We have ADdlX) = Rtc 一 G(t)dt. We therefore combine the x
part inD’s dynamic and the the partial derivation in G into an identity mapping. Then D can be
considered as an integral dynamic and its transfer function is given by HD(s) = 1. Further, the
generator also integrate the output of D, which is also S.
Hence, the block diagram of the Dirac GAN is illustrated in the left panel of Fig. 2. As We can see,
D takes the difference between the real sample and fake sample, i.e., c 一 θ as input, and output the
integration of the input signal. The generator takes D’s output as input and output the generator
distribution which is the integration of D’s output.
Remark 1. When treating G and D as dynamics, the both the inputs and outputs of the dynamics are
functions over t. For example, the input of D is pD(t, x) 一 pG(t, x) and the output is D(t, x).
3.2	The Dynamics of WGAN
In this subsection, we further model the dynamics of WGAN (Arjovsky et al., 2017) without Lipschitz
continuity constraints in the function space. In this case, we consider the nonparametric setting,
where D is a piece-wise linear function, G is a mapping from the noise space to the data space, and
both of them are of infinite capacity. This analysis can be extended to other variants of GANs easily.
In the general case, the objective function of D is given by:
maxU (D) = EpD [D(x)] 一 EpG [D(x)]
(pD(x) 一 pG(x))D(x)dx.
(9)
According to the calculus of variations (Gelfand et al., 2000), the gradient ofU(D) with respect to
the function D, and the dynamic of D are given by:
∂U(D)	dD(t, x)
=PD ― PG → -=- = PD(x) ― PG(X)	(10)
∂D	dt
Since the input signal is PD 一 Pg, the dynamics of WGAN is similar to that of the Dirac GAN, whose
transfer function can be denoted as HD(s) = S.
4
Under review as a conference paper at ICLR 2020
For the generator, the objective function and its dynamics are given by:
max U(G)= Epz(Z) [D(G(z))]= ZPz(Z)D(G(Z))dz → dU≡ = Pz⑶ *：吁,∀z. (11)
G	∂G	∂G(z)
It is worth noting that compared to Dirac GAN, this integral is taken in the function space, i.e., G(t +
δt,z) = G(t,z)+ δtpz(z) dD(GzZ)). Besides, the generator distributionPG is not a Dirac distribution,
resulting in that another operation, which converts the generated samples to the distribution, should
be added after G to make the model consistent. The diagram blocks of WGAN is illustrated in
Fig. 2 (right). Compared to Dirac-GAN, another two operations are introduced to the framework. We
further provide the dynamics of other GANs, including standard GAN proposed by Goodfellow et al.
(2014) and Least-Square GAN proposed by Mao et al. (2017) in Appendix A.
Remark 2. We only consider the unregularized WGAN to simplify the analysis. Note that the
dynamics of D follows Eqn. (10) at least locally around the equilibrium where updating D according
to Eqn. (10) will not violate the Lipschitz constraints. Formally proof is given in Appendix D.
Remark 3. Here we model the dynamics of G and D in the functional space, which is more
convenient for the following stability analysis and designing controllers. In practice, we update the
models in the parameter space to approximate the dynamics in Eqn. (10) and Eqn. (11).
4	Understanding and Stabilizing GANs with Negative Feedback
In this section, we first provide the stability analysis of GANs from the perspective of control theory.
Besides, many previous methods can be interpreted in this perspective, and we use the momentum
as an example. Other further examples are given in Appendix C. We further propose to stabilize
the dynamics of GANs using the negative feedback controlling methods, and an example of another
controller is given in Appendix B.
4.1	Stability
In the dynamics of GANs, we argue that it is dominated by the two integral dynamics of G and D.
Therefore, we ignore the two non-linear operations in GANs in the following analysis. Since both
G and D can be considered as integral parts, we can formulate the transfer function of G and D as
HG = HD = S ∙ According to Eqn. (4), the transfer function OfWGAN is given by:
H (S) = TTHDHG = T⅛ = T⅛.	(12)
It is worth noting that the poles of the transfer function of the whole dynamic are 0 ± i, which
indicates that the output will oscillate around the equilibrium rather than converge to 0. This result
is also consistent with the analysis in (Mescheder et al., 2018). This perspective can also partially
explain why larger learning rates or adopting multiple update steps for D, where the transfer function
of D can be approximated by S with k > 1, cannot provide a principle solution for stabilizing GANs.
In this case, the poles of the dynamics are adjusted to 0 ± √ki which is still an oscillation dynamics.
4.1.1	The Affect of Momentum
Our method can also provide stability analysis for other practical techniques used in training GANs. In
this part, we use the momentum as an example. Gidel et al. (2018) provide some theoretical analysis
of momentum in training GANs, which is formulated in the parameter space. In the following, we
re-analyze the momentum in the function space under the perspective of control theory.
The momentum method (Qian, 1999) achieves great success in training deep neural networks.
Previous methods such as SGD with momentum can help neural networks to escape from local
optima. Its theoretical formulation is given by:
Θt+1 = βθ + (1- β)Vθt, θt+ι = θt + ηθt+ι.	(13)
The β is the coefficient for the exponential decay of the gradients, which generally takes 0.9. However,
things changed when it comes to training GANs. DCGAN (Radford et al., 2015) recommends a
smaller momentum with β = 0.5. Furthermore, recent state-of-the-art models (Mescheder et al.,
2018; Brock et al., 2018; Gulrajani et al., 2017) just remove the momentum.
5
Under review as a conference paper at ICLR 2020
(a) The diagram of GANs with negative feedback.
Figure 3: The diagram and its corresponding dynamics of GANs using negative feedback.
Time
Time
(b) The dynamics of GANs with negative feedback.
Algorithm 1 Negative Feedback GAN
1:	Input: Buffer size Nb, feedback coefficient λ, batch size N, initialized θ and φ, learning rate η.
2:	Initialize Br and Bf for real samples and fake samples respectively.
3:	repeat
4:	Sample a batch of {xr }〜PD, {xf }〜PG of N samples.
5:	Update Br with {xr }. Update Bf with {xf}.
6:	Sample a batch of Xr 〜Br, Xf 〜Bf of N samples respectively.
7:	Estimate the objective of D:
U(D) = NPχ∈{xr} D(X)- Pχ∈{xf} D(X)] - 2N[Pχ∈{xr} D2(x) + Pχ∈{xf} D2(x)].
8:	Update φ to maximize U(D) with learning rate η.
9:	Estimate the objective of G: U(G) = N Px∈{xf }D(x).
10:	Update θ to maximize U(G) with learning rate η.
11:	until Convergence
In control theory, the momentum is equivalent to adding an exponential decay to the input of the
dynamics (An et al., 2018), and the LT of an exponential decay dynamic is 士.τ > 0 denotes the
decay coefficient which depends on β. Therefore, the transfer function of the D with momentum is
HmD (S) = 3(§+=), and the transfer function of the WGAN with momentum is given by:
H(s)
HmD
1 + HmD HG
1∕(s(s + τ))	= S
1 + 1/(s2(s + T))	s3 + τs2 + 1
(14)
By letting (s + a)(s + b)(s +c) = s3 +τs2 + 1, we have that a+ b+ c = τ > 0, which indicates that
there are at least one pole of this dynamic whose real part is larger than 0, indicating the instability of
the dynamics for GANs with momentum.
4.2	Stabilize GANs
According to the above analysis, for both the unregularized WGAN and the GAN with momentum,
the equilibrium point (i.e., PG = PD) is not a stable point. In the following, we propose to stabilize
GANs using negative feedback control, which is the most widely-used method in control theory.
In this setting, we set HNF = λ which is the transfer function of the negative feedback for D. After
applying the negative feedback to D, The diagram of the controlled dynamics is given in Fig. 3 (a).
According to Eqn. (4), the transfer function of D and the whole dynamic can be represented as:
HD0 (s)
—1/f— = _1_____→ H(s) = ___HD(S)____=______S____
1 + λ∕s s + λ	1 + HD (s)Hg(s)	s2 + λs + 1
(15)
Therefore, for a positive and small λ, the real part of the two poles are - 2 < 0, which indicates that
the dynamic is stable. The dynamics of the Dirac GAN using different λ is illustrated in Fig. 3 (b).
As we can see, when no feedback is applied (i.e., λ = 0), the dynamic will oscillate around the
equilibrium. Applying negative feedback can significant improve the stability of the dynamics.
4.2.1 Practical Implementation
The most direct way to add negative feed is to introduce a regularization term RD = 2 J D2 (x)dx
to the objective function of D. However, directly integrating over the input space is computationally
infeasible. Instead, we give an approximation to RD. We assume that the area where |D(X)| is large
mainly concentrates on its training samples, i.e., previous samples used to train D. Therefore, we
keep two buffers Br and Bf of size Nb to store the old real samples and fake samples, respectively.
Then RD is evaluated on these two buffers to regularize D, and the two buffers are updated with
replacement at each step. The training procedure is illustrated in Alg. 1. In the following, we use
NF-GAN (∙) to denote our proposed method with the hyperparameters λ∕2 denoted in the parentheses.
6
Under review as a conference paper at ICLR 2020
7 6 5 4 3
əjous co-⅛8c-
5	10	15	20
Time/Hours
Oooo
8 6 4 2
2	4	6	8	10
Time/hours
Reg-SGAN
Rcg-WGAN
WGAN-GP
NF-SGAN(IO)
NF-SGAN(15)
NF-WGAN(IO)
NF-WGAN(15)
Figure 4: The learning curve of baselines and our proposed methods. Left: The Inception Score of CIFAR10. Right: The FID score for CelebA.
We plot the curve with respect to time for better representation of the computational cost and convergence speed.
5	Related Work
GANs are known for its unstable training process. Radford et al. (2015) provide several practical
techniques to stabilize the training, such as designing suitable architecture of neural networks and the
hyperparameters selection. The Unrolled GAN (Metz et al., 2016) proposes to make D as a function
of G by unrolling the training process of the discriminator where the gap between the theoretical
analysis and implementation can be reduced. Arjovsky et al. (2017) provide a theoretical analysis of
the gradient vanishing problem and further propose to optimize GANs using Wasserstein distance.
Gulrajani et al. (2017) use the gradient penalty to ensure the Lipschitz continuity in discriminator,
which is required by WGAN. However, GANs still suffer from instability in the training process.
Besides, there is some work that directly focuses on the stability of the training process. Except for
the works mentioned in Sec. 1, Peng et al. (2019) also directly model the dynamics of the parameters
and adjust the update rules to stabilize GANs. Gidel et al. (2016) propose to find the saddle point
of the optimization problem using Frank-Wolfe algorithms. Feizi et al. (2017) also use the control
theory to model the stability of GANs, while the dynamics of GANs are not explicitly captured.
Balduzzi et al. (2018) decompose dynamics into the oscillating and non-oscillating behaviour and
model training dynamics as finding stable fixed points. However, none of them only can generalize
their methods to complex datasets such as natural images.
6	Experiments
In this section, we empirically verify our proposed method on the widely-adopted CI-
FAR10 (Krizhevsky et al., 2009) and CelebA (Liu et al., 2015) datasets. We denote the regularization
proposed in Mescheder et al. (2018) as Reg-GAN, which is the direct baseline of our method. Besides,
we also apply NF-GAN to SN-GAN (Miyato et al., 2018) and provide a significant improvement to
the state-of-the-art method. For fair comparisons, our implementation mainly follows the officially
released code for Reg-GAN and SN-GAN, respectively. More details about the experimental setting
and further results on a synthetic dataset can be found in Appendix B. The code is provided here.
6.1 Comparison to Reg-GAN
We first compare our proposed method with Reg-
GAN. Since for large datasets, the non-linearity
cannot be ignored, which results in the gap between
the simulation of Dirac GAN and our methods. A
small λ∕2, such as 0.01 which is suitable for Dirac
GAN is not enough for stabilize the large mod-
els. We therefore select the coefficient λ∕2 among
{1, 2, 5, 10, 15, 20}. In this part, we only report
representative results. Though our analysis mainly
focuses on the WGAN framework, NF can also eas-
ily generalize to SGAN whose stability can also be
improved and NF-SGANs with different coefficient
λ∕2 are evaluated.
Table 1: The FID Score (top) and Inception Score (bottom) on
CIFAR10. The results reported here are the best results over the
training process. The results of IS are averages over 3 runs.
Regularization	WGAN	SGAN
No regularization	105.21	28.51
Reg-GAN	30.43	28.39
Gradient Penalty	28.20		
NF-GAN(2)(oUrS)	21.90	22.35
NF-GAN(5) (ours)	22.31	21.61
NF-GAN(10)(oUrS)	21.19	23.34
ObjeCtive SN-GAN	NF-SN-GAN 一		
WGAN	3.29	8.28 Hinge 8.22 ± .05	8.45		± .09 ±.11
We use the Inception Score (IS) (Salimans et al., 2016) to evaluate the image quality on CIFAR10 and
FID score (Gulrajani et al., 2017) on both CIFAR10 and CelebA. The quantitative results are given in
Table 1 and the learning curve is reported in Fig. 4. Our method gives an improvement to the FID
7
Under review as a conference paper at ICLR 2020
Figure 5:
: WGAN-GP, Reg-WGAN, NF-WGAN(5), NF-SGAN(5).
Figure 6: The generated results of CelebA dataset. From left to right: WGAN-GP, Reg-WGAN, NF-WGAN(15), NF-SGAN(15).
and IS for both WGAN and SGAN. Specifically, for unregularized methods, the models demonstrate
the unstable training process and finally diverge away from the data distribution. Compared with
Reg-GAN, our method maintains stability and provides better convergence results, as also shown in
Table 1 (top). Besides, our method is more computationally efficient, since regularizing the output
of D needs less computation compared to regularizing || dD(x) || and this gap is significant for large
dataset. Specifically, on Geforce 1080Ti, our method can conduct approximate 12/8 iterations per
second of training for CIFAR10/CelebA, whereas Reg-GAN can only conduct 10/4 iterations per
second. What’s more, our method is robust to the choice of the hyperparameter λ. We present the
generated images in Fig. 5 and Fig. 6.
6.2 Comparison to SN-GAN
We further apply NF-GAN to the SN-GAN’s architecture. With stability guaranteed, our proposed
method can further help SN-GAN to address the potentially unstable issue and converge to better final
results, as illustrated in Table 1 (bottom). We manually select λ∕2 ∈ {0.01,0.1,1} and finally adopt
λ∕2 = 0.1. Though SN-GAN provides an elegant normalization method to ensure the Lipschitz
continuity, SN-GAN with WGAN loss still cannot converge to the data distribution and provide
unreasonable results, which verifies with our stability analysis. Instead, the hinge loss, as introduced
Eqn. (16)&(17) in Miyato et al. (2018), is used to stabilize SN-GAN but no theoretical guarantee is
provided. In contrast, NF-GAN can successfully regularize SN-GAN with WGAN loss and provide
better results to previous state-of-the-art results. What’s more, NF-GAN can also benefit SN-GAN
with Hinge loss and provide a significant improvement the IS of CIFAR-10.
7	Conclusion
In this paper, we propose a novel perspective to analyze the dynamics of GANs. Instead of focusing
on the equilibrium, we directly model the dynamics of the discriminator and the generator during
the training process. Using Laplacian transformation and control theory, the stability can be easily
understood according to the poles of the dynamics. Various methods can be motivated from this
perspective, and we verify it with the widely used negative feedback control. The empirical results
demonstrate that our method can successfully stabilize GANs and provide better convergence results.
Negative feedback shows promise results, but further analysis is required for better results. For
one thing, our analysis mainly focuses on the continuous cases, where the practical implementation
optimizes both G and D in discrete time steps, where the Z-transformation provides better tools for
discrete-time dynamics. For another, two approximations are made: using the update in the parameter
space to approximate the dynamics and ignore two non-linear operations in GANs. Recent analyses
of GANs on the functional spaces (Johnson & Zhang, 2018) provides a promising solution to solve
the first approximation and modern control theory and non-linear control methods (Khalil, 2002) can
be adopted to solve the second one. Indeed, our method does not provide a perfect solution to GANs’
dynamics. It does provide a novel perspective to understand and stabilize GANs.
8
Under review as a conference paper at ICLR 2020
References
Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A pid controller
approach for stochastic optimization of deep networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8522-8531, 2018.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223, 2017.
Karl Johan Astrom and Tore Hagglund. PID controllers: theory, design, and tuning, volume 2.
Instrument society of America Research Triangle Park, NC, 1995.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel.
The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Tatjana Chavdarova and FrangoiS Fleuret. Sgan: An alternative training of generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 9407-9415, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Paul Adrien Maurice Dirac. The principles of quantum mechanics. Number 27. Oxford university
press, 1981.
Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. arXiv preprint
arXiv:1907.02544, 2019.
Chao Du, Kun Xu, Chongxuan Li, Jun Zhu, and Bo Zhang. Learning implicit generative models by
teaching explicit ones. arXiv preprint arXiv:1807.03870, 2018.
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans: the lqg setting. arXiv
preprint arXiv:1710.10793, 2017.
Izrail Moiseevitch Gelfand, Richard A Silverman, et al. Calculus of variations. Courier Corporation,
2000.
Gauthier Gidel, Tony Jebara, and Simon Lacoste-Julien. Frank-wolfe algorithms for saddle point
problems. arXiv preprint arXiv:1610.07797, 2016.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Lepriol, Gabriel Huang,
Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
arXiv preprint arXiv:1807.04740, 2018.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp.
315-323, 2011.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
9
Under review as a conference paper at ICLR 2020
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent. Cited on, 14:8, 2012.
Rie Johnson and Tong Zhang. Composite functional gradient learning of generative adversarial
models. arXiv preprint arXiv:1801.06309, 2018.
Thomas Kailath. Linear systems, volume 156. Prentice-Hall Englewood Cliffs, NJ, 1980.
Hassan K Khalil. Nonlinear systems. Upper Saddle River, 2002.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Chongxuan Li, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In Advances in
neural information processing systems, pp. 4088-4098, 2017.
Kevin J Liang, Chunyuan Li, Guoyin Wang, and Lawrence Carin. Generative adversarial network
training is a continual learning problem. arXiv preprint arXiv:1811.11083, 2018.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created
equal? a large-scale study. In Advances in neural information processing systems, pp. 700-709,
2018.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least
squares generative adversarial networks. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 2794-2802, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? arXiv preprint arXiv:1801.04406, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. ArXiv, abs/1611.02163, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5585-5595, 2017.
Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual discriminator generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2670-2680, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems,
pp. 271-279, 2016.
Alan V Oppenheim, Alan S Willsky, and S Hamid Nawab. Senales y Sistemas. Pearson Educacidn,
1998.
Wei Peng, Yuhong Dai, Hui Zhang, and Lizhi Cheng. Training gans with centripetal acceleration.
arXiv preprint arXiv:1902.08949, 2019.
10
Under review as a conference paper at ICLR 2020
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):
145-151,1999.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems, pp.
2234-2242, 2016.
David Vernon Widder. Laplace transform (PMS-6). Princeton university press, 2015.
11
Under review as a conference paper at ICLR 2020
A GANs and their corresponding dynamics
In this section, we provide an illustration for different kinds of GANs and their corresponding
dynamics.
A. 1 Vanilla GAN and Non-saturation GAN
The GANs proposed by (Goodfellow et al., 2014) are formulated as follows:
min max EpD [log σ(D(x))] + EpG [log(1 - σ(D(x)))].
Therefore, the objective function and the corresponding dynamics of D are given by:
•	The objective function:
max U(D) =EpD [log σ(D(x))] +EpG[log(1 - σ(D(x)))]
1
PD (x) log
1 + exp(-D(x)) +pG(x) log
exp( —D(x)) dx
1+exp(-D(x))
= -	PD (x) log(1 + exp(-D(x))) + PG(x) log(1 + exp(D(x)))dx.
•	Given current G, the gradient of U(D) with respect to D at certain x is given by:
∂U (D)
dD(x)
exp(-D(x))
exp(D(x))
PD 1 + exp(-D(x))	PG 1 + exp(D(x))
pD(1 - σ(D(x))) - pGσ(D(x))
dDt(x) ∂U(D(x))
dt
dD(x)
pD (1 - σ(D(x))) - pGσ(D(x)).
→
The objective function and the corresponding dynamics of G are given by:
•	The objective function:
mGin U(G) =EpG [log(1 - σ(D(x)))] = Epz [log(1 - σ(D(G(z))))]
exp(-D(G(z))),
TPZ (z)lθg1+exp(-D(G(z))) dz
= -1 × Pz (z) log(1 + exp(D(G(z))))dz.
•	Since G is optimized using reparameterization, we rewrite G(z) as xz for simplicity. Given
current D, the gradient of U(G) w.r.t. G at a certain z is given by:
∂D(xz)
dU(G)	]、，/、exP(D(Xz)) dD(xz)
^xr = -1 × p(z)1+exp(D(xz)) ^xr = -1 × p(z)σ(D(xz))
∂xz
dGttz1 := p(z)σ(D(xz ))
∂D(xz)
∂xz
For a well-trained D, which can successfully tell real samples from generated samples with high
confidence, i.e., D(xz) ≈ 0 for all z, the gradient for G will tend to zero almost everywhere, which
will result in gradient vanishing. This analysis is consistent with previous literature (Johnson &
Zhang, 2018; Arjovsky et al., 2017).
For non-saturation GAN, the objective function and dynamics of G are given by:
•	The objective is given by:
max U(G) =EpG [log σ(D(x))] = Epz [log σ(D(G(z)))]
exp(D(G(z)))
Pz (Z)Iog1 + exp(D(G(z))) dz
/
Z
pz(z)(D(G(z)) - log(1 + exp(D(G(z)))))
12
Under review as a conference paper at ICLR 2020
•	We rewrite G(z) as xz and provide the gradient of U(G) w.r.t. xz as follows:
∂U(G) _	, 1( exp(D(xz)) ∂D(xz)
∂xz = Pz(Z)( - 1 + exp(D(xz))) ∂xz
∂D(xz)
=Pz(Z)(I — σ(D(Xz))) ^x—
dGt(z)	∂D(xz)
→	=P(Z)(I-σ(D(Xz)))
A.2 Least-Square GAN
(Mao et al., 2017) propose to use the least square loss to train both the discriminator and the generator.
The objective for the D and G are selected differently according to its theoretical analysis. Specifically,
the objective function of D and G are given by:
mln U (D)= 1 EpD [(D(X)- 1)2] + 2 EpG [(D(X))I
min U(G) = IEpζ[(D(G(ζ)) - 1)2]∙
G2
Therefore, the dynamics of the D and G are given by:
•	The dynamic of D:
∂U (D)
∂D(x)
1∂
2 dD( ( (PD (D(X) - 1)2 + PGD(X)2)
2 ∂ D(X)
= PD(D(X) - 1) + PGD(X).
→ dD；X) = Pd - (Pd + PG)Dt(X)
dt
(16)
•	The dynamic of G:
∂U (G)
∂Xz
∂D(Xz)
Pz(D(Xz) - 1) ∂Xζ
dGt(Z)	∂D(Xz)
→ —=Pz(Z)(I- D(Xz))
It is worth noting that if the G is fixed, the dynamics of D can be considered as an exponential
decay which is the same as the D with negative feedback control. However, since G is changing
during the training process, the dynamics of D is actually coupling with the dynamics of G, which
generally results in the unstable dynamics (Kailath, 1980). The experimental results on CIFAR10
also verify this argument. It also explains why we use two buffers of old samples to approximate the
regularization term instead of directly regularizing the output of D using current samples.
B Further Experimental Results
B.1	Detailed Experimental Settings
In this paper, we mainly focus on the stability of the training dynamics of GANs, and therefore
directly compare our proposed method NF-GAN with the vanilla SGAN/WGAN and the most direct
baseline Reg-GAN. We use the resnet (He et al., 2016) for both the generator and the discriminator
following Mescheder et al. (2018) and adopt the ReLU activation (Glorot et al., 2011). The batch size
is 64, and the buffer size is set to 100 times of the batch size for all settings. We use rmsprop (Hinton
et al., 2012) optimizer with learning rate as 0.0001 and α = 0.99 for all models.
Note that we make the following two modifications to the original implementation of Reg-GAN,
which results in a more challenging setting to the stability. First, in the original Reg-GAN, a
13
Under review as a conference paper at ICLR 2020
WGAN
SGAN
Reg-WGAN
NF-GAN(0.1)
NF-GAN(0.01)
Figure 7: The generated samples for mixture of gaussian distribution. The red points demonstrate the location of data distribution and the blue
points are generated samples. Each distribution is plotted using kernel density estimation with 50,000 samples.
coefficient 0.1 is introduced to the residual connection of the ResNet for both G and D. Specifically,
hL+ι = hL + 0.1 * ∕l(Al) where /l is the transformation defined in the L-th layer and 人七 is the
output of L - 1-th layer as well as the input of the L-th layer. We instead remove the coefficient and
just use the regular ResNet, i.e., hL+1 = hL + fL(hL). Second, for both G and D, we use the ReLU
activation, rather than leaky ReLU. The above two modifications make the training of GANs more
difficult, which explains why the reported results are not consistent with the scores in the original
paper. In this more challenging setting, our method still outperforms the Reg-GAN. Specifically, the
inception score of Reg-GAN is below 6.0 in this difficult setting and about 6.5 in the original setting.
In contrast, both NF-WGAN and NF-SGAN achieve more than 6.7 in this difficult setting.
B.2	Synthetic Data
In this section, we evaluate our proposed method on a mixture of Gaussian on the two dimensions.
The data distribution consists of 8 2D isotropic Gaussian distributions arranged in a ring, where the
radius of the ring is 1, and the deviation of each component Gaussian distribution is 0.05. For the
coefficient λ∕2, We follow the theoretical analysis and set it among λ∕2 ∈ {0.01,0.05,0.1}. We
adopt two-layer MLPs for both the generator and the discriminator whose hidden units are 128 and
512 respectively. The batch size is is 512 and other settings just follow the Reg-GAN.
The generated results are illustrated in Fig. 7 and we further provide the dynamics of the generator
distribution in Fig. 8 in Fig. 8. As we can see, the unregularized WGAN and SGAN suffer from
severe model collapse problem and cannot cover the whole data distribution. Besides, the oscillation
can be observed during the training process of WGAN: the generator distribution oscillates among
the modes of data distribution. Our method can successfully cover all modes compared to the WGAN
and SGAN and is comparable to Reg-GAN.
B.3	Further Control Method: PD Controller
Besides negative feedback, the most commonly used controller is the PID controller, which consists
of three parts: the partial (P), integration (I) and derivation (D). Specifically, the PID controller takes
the error signal as input, and output the summation of the above three parts, whose formal definition
is defined as follows:
c(t) = Kpe(t) + Ki Z	e(u)du + Kd :)→ Hc(S)
u=0	dt
Kps + Ki + Kds2	(17)
s
Kp, Ki, Kd denotes the corresponding coefficients for PID controller. e(t) denotes the error signal
and c is the output of the controller, which is also the actual input of the dynamic. Generally, the
I part can reduce the static error, and the D part can make the dynamic stable and accelerate the
convergence.
The PID controller is introduced to adjust the poles of the dynamic. Since the static error, i.e., the
difference between the stable point and targeted point, is zero, the integral part is not necessary, and
we only employ the PD controller in the following. By letting Hc(s) = p + ds, where p and d is the
corresponding coefficient for the P and D part, the transfer function of the whole dynamic is given by:
H(s)
Hc(s)HD (s)	= s(p + ds)
1 + Hc(s)Hg(s)Hd (s)	s2 + ds + P
(18)
As we can see, given positive p and d, the PD controller can successfully adjust the real parts poles of
the dynamic to negative. In the following analysis and implementation, we fix p = 1 for simplicity.
14
Under review as a conference paper at ICLR 2020
WGAN
SGAN
WGAN-GP
Reg-WGAN
NF-GAN(0.1)
NF-GAN(0.05)
NF-GAN(0.01)
H		•	S • •	•	： •	•： =±= j⅜⅜ j⅛⅛F •	• • • • • • ・ • • • >• • • ~ • • ■	:	•	；1	•	:	pτ∏ ：| . • . .. • • 二:！二 HW
	π ： ,∙ .:.∙*∙. ” . ∙ ∙ ∙ 9 9 ∙	S用Q; 3 0 O;	• ∙ ∙i J L J ； .• - • • ∙ ∙n ".'IKm ・ “ • • • ・• ■ ・ ■ 1• • 1
ε .∙ ɪ	一•: :,一 J :二二:…: F Tl - G ∙ β •	-"∙⅛. - • - • ■■ • “ • •• , • • • • „, • • „ • • 9	⅛	“ •	•	•• •	•	•	•	« •	• • ∙ ^" ∙ ∙ • • " • • • ；， '• • ，"⅜ “ . • . • • „ ∙ ∙ f, • • •	. » • , • > “ • • •	•	∙	∙	^'	∙	∙	"' β	•	•	• •	■•	∙	~I 丁~	- ]- ' •	〜	• •	-	•	∙	'T	•	"	・ ♦	•	. •	9	〜♦	∙ I J	•	• • • ・' • • • • • • • “ ・ • 「一 ,Λ	"` ⅛	•	1	•	• •	“	•	-	•	'»	T	3	•	
1000 its
2000 its
5000 its	10000 its
20000 its
50000 its
100000 its
Figure 8: The training dynamics of various GANs on synthetic data.
(a) The diagram of GANs with PD Controller.
Figure 9: The diagram and its corresponding dynamics of GANs using PD controller.
Time
(b) The dynamics of GANs with with PD Controller.
15
Under review as a conference paper at ICLR 2020
1000 its	2000 its
5000 its
10000 its
20000 its
50000 its
100000 its
Figure 10: The generated results of PD controller with d = 5.
Figure 11: The diagram of previous methods. Left: the diagram of Reg-GAN which takes the || ∂∂X || as regularization. Right: the diagram of
negative momentum together with weight decay.
The dynamics of Dirac GANs using PD controller is given in Fig. 9 (b). With a PD controller, the
dynamic can be successfully stabilized. For general GAN, the objective of D at time t is given by:
U (D, t) = EpD(t,x)[D(x)] - EpG(t,x)[D(x)]+
λ{EpD(t,x) [D(x)] - EpG(t,x)[D(x)] - EpD(t-δt,x)[D(x)] + EpG(t-δt,x)[D(x)]}.	(19)
The regularization is actually minimizing the loss of D in the current step as well as maximizing the
loss of D for previous step.
B.3.1	Experimental Results
The results of the synthetic data are illustrated in Fig. 10. WGAN with PD controller can successfully
cover the whole data distribution and be stable at the equilibrium.
However, we fail to generalize PD controller to the natural image dataset such as CIFAR10. We argue
that the main reason is the overshooting for the dynamics of D, as illustrated in Fig. 9 (b), where the
D gives a large derivation from the equilibrium at the beginning. Because of the highly-nonlinearity
in neural networks, this overshooting will result in unexpected behavior of neural networks. In
contrast, for the negative feedback, the overshooting is much smaller, which enables it to generalize
to the non-linear neural networks.
C Unifying Previous Methods in Control Theory
In this section, we also provide some analysis that interprets previous methods in the perspective of
control theory.
C.1 REG-GAN
Mescheder et al. (2018) propose a regularization which is added on D’s gradient of x, i.e., Rd(x) =
|| dDXX) ||2. Therefore, it can be considered as an other form of negative feedback, which is illustrated
in Fig. 11.
Compare to our analysis, Reg-GAN provides more accurate modeling of the dynamics and directly
regularizes on the input of the dynamic of G. Therefore, Reg-GAN can also successfully stabilize the
dynamic of GANs, which is consistent with our experimental results. Compared to Reg-GAN, our
method is more computationally efficient.
C.2 Weight Decay and Negative Momentum
Gidel et al. (2018) analyze the effect of momentum in the training of GANs, and we already interpret
this analysis in the context of control theory. Besides, the authors also propose to use the negative
16
Under review as a conference paper at ICLR 2020
momentum to stabilize GANs, which is illustrated as follows:
Wt+1 = Wt + ηVW + λ(Wt - Wt-ι).	(20)
Together with weight decay, the controlled dynamics of W is given:
Wt+1 = Wt+ηVW+λ1(Wt -Wt-1) -λ2Wt.	(21)
In this setting, Eqn. (21) is equivalent to the PD controller with negative feedback in the parameter
space whose diagram can be illustrated in Fig. 11. The λ1(Wt - Wt-1) denotes the D part and λ2Wt
denotes the P part.
D Local approximation to the regularized dynamics
In this section, we prove that around the equilibrium, the dynamics of regularized D with Lipschitz
constraint is equivalent to the unregularized D as in Eqn. (10). Since the updating direction of D
is the gradient of D in the functional space, we only need to prove that updating D according to
Eqn. (10) will not violate the Lipschitz constraints, at least locally around the equilibrium. Here we
make the following assumptions:
1.	Both PD(x) and PG(t,x) are C 1-smooth: dpxχ) exists and is continuous for PD and
pG(t, x),∀t.
2.	P(X) → 0 and dpx → 0 when X → 0 for PD and PG (t, x), ∀ t.
3.	There exists an M such that | dpdX) 12 < M for PD and pg (t, x), ∀ t.
The above assumptions are satisfied for most probability density functions.
The distance in the function space is defined as d(P1,P2) = supx∈Rn |P1(x) - P2(x)| which always
exists because of the 2-nd conditions above. We define Ωl = {p(x) |p(x) ∈ C1, | dpχx) ∣2 < L ∀χ.}
and B() = {P(x)|P(x) ∈ C1, supx |P(x)| < }. Then we have the follow theorem:
Theorem 1. There exists δ > 0, SUCh that ∀D(x) ∈ Ω0.5, we have D(x) + J(pd(x) 一PG(X)) ∈ Ωι.
Proof. By denoting D0(x) = D(x) + δ(PD (x) - PG(x)), We have:
d(D(x) + δ(PD(x) - PG(X)))
dx
dD(x) + δ(PD(x) 一 PG(X))
dx	dx dx .
Therefore, we have
|
d(D(X) + δ(pD(x) ―PG(X)))
dX
dD(X)	PD(X)	PG(X)
|2 ≤l F |2 + δ(l F |2 + | F)|2
≤ 0.5 + δ(M + M).
(22)
(23)
By letting δ =焉,we have |dD) ∣2 ≤ 0.75. Therefore we have D0(χ) ∈ Ωι.	□
The above theorem indicates that when D(X) is sufficient close to the equilibrium, then the dynamics
of D still follows Eqn. (10).
E Connection to Regularization on Jacobian Matrix
In this paper, we mainly analyze our proposed method in the functional space, including stability
analysis and controller designing. Instead, our proposed method can also be interpreted as certain
regularization terms on the Jacobian matrix of the training dynamics. Below we provide a formal
demonstration.
First, We denote the equilibrium of G and D in the functional space as (θ*, φ*), wherepg(x; θ*)=
Pd (x) and D(χ; φ*) = 0 for all χ. Therefore, We have that φ* is also a global minimum point of the
regularization term L(D) = R D2(χ)dχ. Then We have d LD)占 0.
17
Under review as a conference paper at ICLR 2020
We denote U (D, G) as the objective function of the minimax optimization problem in WGAN
without NF regularization. Then the Jacobian matrix of the training dynamic can be denoted as:
(∂2U (D,G)	∂2U (D,G)
-∂φ2	-∂φ∂θ~
∂2U (D,G)	∂2U (D,G)
-∂θ∂φ-	-∂θ2
(24)
Because of the linearity of the derivation, the training dynamics of the WGAN with NF regularization
is denoted as:
∂ ∂2L(D) o∖
J = J - JL = J - I ~φ2ξ~ 0 卜	(25)
where we abuse the 0 to denote the zero matrix with certain size to match the size of J. Since
d ∂φ(D)占 0, We have -JL W 0. Therefore, the NF regularization introduces a negative semi-definite
matrix to the original Jacobian matrix, which is helpful to stabilize the training dynamics of GANs.
18