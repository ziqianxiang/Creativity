Under review as a conference paper at ICLR 2020
Localized Meta-Learning: A PAC-Bayes Anal-
ysis for Meta-Learning Beyond Global Prior
Anonymous authors
Paper under double-blind review
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
Ab stract
Meta-learning methods learn the meta-knowledge among various training tasks
and aim to promote the learning of new tasks under the task similarity assumption.
However, such meta-knowledge is often represented as a fixed distribution, which
is too restrictive to capture various specific task information. In this work, we
present a localized meta-learning framework based on the PAC-Bayes theory. In
particular, we propose an LCC-based prior predictor that allows the meta learner to
adaptively generate local meta-knowledge for specific tasks. We further develop
a practical algorithm with deep neural network based on the bound. Empirical
results on real-world datasets demonstrate the efficacy of the proposed method.
1	Introduction
Recent years have seen a resurgence of interest in the field of meta-learning, or learning-to-learn
(Thrun & Pratt, 2012), especially for empowering deep neural networks the capability of fast adapt-
ing to unseen tasks just as humans (Finn et al., 2017; Ravi & Larochelle, 2017). More concretely,
the neural networks are trained from a sequence of datasets, associated with different learning tasks
sampled from a meta-distribution (also called task environment (Baxter, 2000; Maurer, 2005)). The
principal aim of meta learner is to extract transferable meta-knowledge from observed tasks and
facilitate the learning of new tasks sampled from the same meta-distribution. The performance is
measured by the generalization ability from a finite set of observed tasks, which is evaluated by
learning related unseen tasks. For this reason, there has been considerable interest in theoretical
bounds on the generalization in terms of the meta-learning algorithm (Denevi et al., 2018b;a).
One typical line of work (Pentina & Lampert, 2014; Amit & Meir, 2018) use PAC-Bayes bound to
analyze the generalization behavior of the meta learner and quantify the relation between the ex-
pected loss on new tasks and the average loss on the observed tasks. In this setup, we formulate
meta-learning as hierarchical Bayes. Accordingly, meta-knowledge is instantiated as a global dis-
tribution over all possible priors, which we call hyperprior and is chosen before observing training
tasks. Each prior is a distribution over a family of classifiers w.r.t. a particular task. To learn versa-
tile meta-knowledge across tasks, the meta learner observes a sequence of training tasks and adjusts
its hyperprior into a hyperposterior distribution over the set of priors. To solve a new task, the base
learner produces a posterior distribution over a family of classifiers based on the associated sample
set and the prior generated by the hyperposterior.
However, such meta-knowledge is shared across tasks. The global hyperposterior is rather generic,
typically not well-tailored to various specific tasks. Consequently, it leads to sub-optimal perfor-
mance for any individual prediction task. As a motivational example, suppose we have two dif-
ferent tasks: distinguishing motorcycle versus bicycle and distinguishing motorcycle versus car.
Intuitively, each task uses distinct discriminative patterns and thus the desired meta-knowledge is
required to extract these patterns simultaneously. This could be a challenging problem to represent
it with a global hyperposterior since the most significant patterns in the first task could be irrelevant
or even detrimental to the second task.
Hence, we are motivated to pursue a meta-learning framework to effectively define the hyperpos-
terior. The inspiration comes from the PAC-Bayes literature on data distribution dependent priors
(Catoni, 2007; Parrado-Hernandez et al., 2012; Dziugaite & Roy, 2018). The choice of posterior
in each task is constrained by the need to minimize the relative entropy between prior and pos-
terior since this divergence forms part of the bound and is typically large in standard PAC-Bayes
1
Under review as a conference paper at ICLR 2020
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
approaches (Lever et al., 2013). Thus, choosing an appropriate prior for each task which is close to
the related posterior could yield improved generalization bounds.
Inspired by this, we propose a Localized Meta-Learning (LML) framework. Instead of formulating
meta-knowledge as a global hyperposterior, we learn a conditional hyperposterior given task data
distribution that allows a meta learner to adaptively generate an appropriate prior for a new task.
However, the task data distribution is unknown, and our only perception for it is via the associated
sample set. Nevertheless, if the conditional hyperposterior is relatively stable to perturbations of the
sample set, then the generated prior could still reflect the underlying task data distribution, resulting
in a generalization bound that still holds with smaller probability. Following this intuition, the
dependence of a conditional hyperposterior on the task data distribution is parameterized by a prior
predictor using Local Coordinate Coding (LCC)(Yu et al., 2009). In particular, if the classifier in
each task is specialized to a parametric model, including deep neural network, the proposed LCC-
based prior predictor predicts the model parameters using the sample set by exploiting the local
information on the latent manifold. LCC-based prior predictor is invariant under permutations of its
inputs and could be further used for unseen tasks.
The main contributions of this work include: (i) We present a localized meta-learning framework
which provides a means to tighten the original PAC-Bayes meta-learning bound (Pentina & Lam-
pert, 2014; Amit & Meir, 2018) by minimizing the task-complexity term by choosing data-dependent
prior; (ii) We propose an LCC-based prior predictor, an implementation of conditional hyperposte-
rior, to generate local meta-knowledge for specific task; (iii) We derive a practical localized meta-
learning algorithm for deep neural networks by minimizing the bound; (iv) Experimental results
demonstrate improved performance over meta-learning method in this field.
2	Preliminaries
2.1	Local Coordinate Coding
We first review some definitions of Local Coordinate Coding (LCC) (Yu et al., 2009) based on which
we develop the proposed LCC-based prior predictor.
Definition 1. (Lipschitz Smoothness (Yu et al., 2009).) A function f(x) on Rd is a (α, β)-Lipschitz
smooth w.r.t. a norm ∣∣ ∙ ∣∣ if kf (x) 一 f (x0)k ≤ α∣∣x - x0∣∣ and ∣∣f (x0) 一 f (x) — ▽/(x)>(x0 — x)k ≤
βkx - x0k2.
Definition 2. (Coordinate Coding (Yu et al., 2009).) A coordinate coding isa pair (γ, C), where
C ⊂ Rd is a set of anchor points, and γ is a map of x ∈ Rd to [γu(x)]u∈C ∈ R|C| such
that u γu (x) = 1. It induces the following physical approximation of x in Rd : γ(x) =
Pu∈C γu(x)u.
Definition 3. (Latent Manifold (Yu et al., 2009).) A subset M ⊂ Rd is called a smooth manifold
with an intrinsic dimension d := dM if there exists a constant cM such that given any x ∈ M,
there exists d bases u1(x), . . . , ud(x) ∈ Rd so that ∀x0 ∈ M:
d
inf ∣x0 - x - X γjuj (x)∣2 ≤ cM ∣x0 - x∣22 ,
Y ∈R∣c∣	j=1
where γ = [γ1, . . . , γd]> are the local codings w.r.t. the bases.
Definition 4. (Covering Number (Yu et al., 2009).) The covering number N (, M) is the smallest
cardinality ofan -ccover C ⊂ M. That is,
sup inf ∣x - v∣ ≤ .
x∈Mv∈C
Definition 2 and 3 imply that any point in Rd can be expressed as a linear combination of a set of
anchor points. Later, we will use them to develop the prior predictor.
2.2	PAC-Bayes Meta-Learning
In order to present the advances proposed in this paper, we next recall some definitions in PAC-Bayes
theory for single-task learning and meta-learning (Catoni, 2007; Baxter, 2000; Pentina & Lampert,
2
Under review as a conference paper at ICLR 2020
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
2014; Amit & Meir, 2018). In the context of classification, we assume all tasks share the same input
space X, output space Y, space of classifiers (hypotheses) H ⊂ {h : X → Y} and loss function
` : Y × Y → [0, 1]. The meta learner observes n tasks in the form of sample sets S1, . . . , Sn.
The number of samples in task i is denoted by mi . Each observed task i consists of a set of i.i.d.
samples Si = {(xj, y7- )}m=ι, which is drawn from a data distribution Si 〜 Dmi. Following the
meta-learning setup in (Baxter, 2000), we assume that each data distribution Di is generated i.i.d.
from the same meta distribution τ. Let h(x) be the prediction of x, the goal of each task is to find
a classifier h that minimizes the expected loss Ex〜D'(h(x),y). Since the underlying 'true' data
distribution Di is unknown, the base learner receives a finite set of samples Si and produces an
“optimal” classifier h = Ab(Si) with a deterministic learning algorithm Ab(∙) that will be used to
predict the labels of unseen inputs.
PAC-Bayes theory studies the properties of randomized classifier, called Gibbs classifier. Let Q be
a posterior distribution over H, to make a prediction, the Gibbs classifier samples a classifier h ∈ H
according to Q and then predicts a label with the chosen h. The expected error under data distribution
D and empirical error on the sample set S are then given by averaging over distribution Q, namely
er(Q) = Eh〜QE(χ,y)〜D'(h(x),y) and er(Q) = Eh〜Qm1 Pm=I '(h(xj),yj), respectively. Then,
we can get the following PAC-Bayes generalization bound of Catoni (2007) in a simplified form
suggested by Germain et al. (2009).
Theorem 1. (Catoni’s bound) Let P be some prior distribution over H. Then for any δ ∈ (0, 1],
and any real number c > 0, the following inequality holds uniformly for all posteriors distribution
Q with probability at least 1 - δ,
er(Q) ≤ 1 -Ce-C [eT(Q) +
KL(Q∣∣P ) + log 1
mc
(1)
.
The PAC-Bayes bound holds uniformly for all Q, it also holds for the data dependent Q. By choosing
the posterior Q that minimizes the PAC-Bayes bound, we obtain an learning algorithm with general-
ization guarantees. Note that the value c allows to control the trade-off between the empirical error
and the complexity term.
The goal of the meta learner is to extract meta-knowledge contained in the observed tasks that will
be used as prior knowledge for learning new tasks. The prior knowledge P is in the form of a
distribution over classifiers H. In each task, the base learner produces a posterior Q = Ab(S, P)
over H based on a sample set S and a prior P. All tasks are learned through the same learning
procedure. The meta learner treats the prior P itself as a random variable and assumes the meta-
knowledge is in the form of a distribution over all possible priors. Let hyperprior P be an initial
distribution over priors, meta learner uses the observed tasks to adjust its original hyperprior P into
hyperposterior Q from the learning process. The quality of the hyperposterior Q is measured by the
expected task error of learning new tasks using priors generated from it, which is formulated as:
er(Q) = EP ^QE(D,m)^τ,s^Dm er(Q = Ab(S,P)).	(2)
Accordingly, the empirical counterpart of the above quantity is given by:
1n
e-r(Q) = EP 〜Q — Ter(Q = Ab(Si,P ))∙	⑶
n i=1
3	PAC-Bayes Meta-Learning B ound with Gaus sian Randomization
Based on the above definition, Pentina & Lampert (2014) and Amit & Meir (2018) present meta-
learning PAC-Bayes generalization bounds w.r.t. hyperposterior Q by using McAllester’s single-task
bound (McAllester, 1999). Here we present a new meta-learning generalization bound with Gaus-
sian randomization by using Catoni’s bound in Eq. (1). In particular, the classifier h is parameterized
as hw with w ∈ Rdw. The prior and posterior is a distribution over the set of all possible parameters
w. We choose both the prior P and posterior Q to be spherical Gaussians, i.e. P = N (wP, σw2 Idw)
and Q = N (wQ , σw2 Idw ). The mean wP is a random variable distributed first according to the
hyperprior P, which we formulate as N(0, σw2 Idw), and later according to hyperposterior Q, which
we model as N (wQ, σw2 Idw). When encountering a new task i, we first sample the mean of prior
3
Under review as a conference paper at ICLR 2020
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
wiP from the hyperposterior N(wQ, σw2 Idw), and then use it as a basis to learn the mean of poste-
rior wiQ = Ab(Si, P), as shown in Figure 1(left). Then, we could derive the following PAC-Bayes
meta-learning bound.
Theorem 2. Consider the Meta-Learning (ML) framework, given the hyperprior P = N(0, σv2 Idv),
then for any hyperposterior Q, any c1, c2 > 0 and any δ ∈ (0, 1] with probability ≥ 1 - δ we have,
er(Q) ≤c1c2er(Q) + (X 2 c1c2 2 + 9 c1 2 )kwQk2 + X 2 c1c2 2 k E WQ - wQk2
i=1 2c2nmiσw2	2c1nσw2	i=1 2c2nmiσw2 wP i
n	c01 c02	1	2n	c01	2
+ ɪ?----------2(ʒ + log ɪ)+	2 log 卞,	(4)
i=1 c2nmiσw2 2 δ	c1nσw2	δ
where。1 = ι-⅛ and c2 = 1-∣⅛•
Proof. See Appendix B.3 for the proof.
Notice that the expected task generalization error is bounded by the empirical multi-task error plus
two complexity terms. The first term demonstrates the environment-complexity which converges
to zero if an infinite number of tasks are observed from the task environment (n → ∞), while the
second is the task-complexity of the observed tasks which converges to zero when the sufficient
samples in each task is observed (mi → ∞). Besides, the derived bound converges at the rate
of O(ml) instead of O(√=) in (Pentina & Lampert, 2014; Amit & Meir, 2018), due to the use of
Catoni’s bound.
4	PAC-Bayes Localized Meta-learning
4.1	Overall Framework
Our motivation stems from a core challenge in PAC-Bayes meta-learning bound in 41, wherein the
complexity term Pn=ι 物:嚎.02 11 EwQ - wQk2 is typically vital to the bound and so finding the
tightest possible bound generally depends on minimizing this term. It is obvious that the optimal
wQ is pn=ι mmmw^ . However, if the learned posteriors for each task are mutually exclusive, i.e.,
one learned posteriorwhas a negative effect on another task, this term could be inevitably large.
wQ is the mean of hyperposterior Q and this term naturally indicates the divergence between the
mean of prior wiP sampled from the hyperposterior Q and the mean of posterior wiQ in each task.
Therefore, we propose to adaptively choose the mean of prior wiP according to task i. It is obvious
that the complexity term vanishes if we set wiP = wiQ , but the prior Pi in each task has to be
chosen independently of the sample set Si. Fortunately, the PAC-Bayes theorem allows us to choose
prior upon the data distribution Di. Therefore, we propose a prior predictor Φ : Dm → wP which
receives task data distribution Dm and outputs the mean of prior wP . In this way, the generated
priors could focus locally on those regions of model parameters that are of particular interest in
solving specific tasks.
Particularly, the prior predictor is parameterized as Φv with v ∈ Rdv . We abuse notation P and
Q and assume v as a random variable distributed first according to the hyperprior P, which we
reformulate as N(0, σv2 Idv), and later according to hyperposterior Q, which we reformulate as
N (vQ, σv2 Idv). Given a new task i, we first sample v from hyperposterior N(vQ, σv2 Idv) and
estimate the mean of prior wiP by leveraging prior predictor wiP = Φv(Dim ). Then, the base
learner utilizes the sample set Si and the prior Pi = N (wiP , σw2 Idw ) to produce a mean posterior
wiQ = Ab(Si, Pi), as illustrated in Figure 1(right).
4.2	LCC-Based Prior Predictor
To make wP close to wQ in each task, the prior predictor is required to (i) uncover the tight relation-
ship between the sample set and model parameter. Intuitively, features and parameters yield similar
local and global structures in their respective spaces in the classification problem. Features in the
4
Under review as a conference paper at ICLR 2020
Task 1	Task 2	Task n	Future Task∙	TaSk 1	TaSk 2	TaSk n	FUtUre Task∙
”(W。,嘘IdW) ---*,	WP ----->- WQ	①V(Dm)-------► Wp ------► WQ
Figure 1: Comparison between meta-learning (left) and localized meta-learning (right). In regular
meta-learning, the mean of prior wP is sampled from a global hyperposterior distribution Q =
N(wQ, σw2 Idw). In the localized meta-learning, wP is produced by a prior predictor Φv(Dm).
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
same category tend to be spatially clustered together while maintaining the separation between dif-
ferent classes. Take linear classifiers as an example, let wk be the parameters w.r.t. category k, the
separability between classes is implemented as X ∙ Wk, which also explicitly encourages intra-Class
compactness. A reasonable choice of wk is to maximize the inner product distance with the input
features in the same category and minimize the distance with the input features of the non-belonging
categories. Besides, the prior predictor should be (ii) category-agnostic since it will be used contin-
uously as new tasks and hence new categories become available. Lastly, it should be (iii) invariant
under permutations of its inputs.
To satisfy the above conditions, we follow the idea of nearest class mean classifier (Mensink et al.,
2013), which represents class parameter by averaging its feature embeddings. This idea has been
explored in transductive few-shot learning problem (Bertinetto et al., 2016; Yang et al., 2018). Snell
et al. (2017) learns a metric space across tasks such that when represented in this embedding, pro-
totype (centroid) of each class can be used for label prediction in the new task. Qiao et al. (2018)
directly predicts the classifier weights using the activations by exploiting the close relationship be-
tween the parameters and the activations in a neural network associated with the same category. In
summary, the classification problem of each task is transformed as a generic metric learning prob-
lem which is shared across tasks. Once this mapping has been learned on observed tasks, due to the
structure-preserving property, it could be easily generalized to new tasks. Formally, let each task
be a K-class classification problem. Then the parameter of the classifier in task i is represented as
Wi = [Wi[1], . . . , Wi[k], . . . , Wi[K]]. The prior predictor for class k could be defined as:
WP [k]=φv(Dmik ) = , E mik ɪ X Φv(Xj ),	(5)
Sik~Dik mik Xj∈Sik
where φv(∙) : Rd → Rdw is the feature embedding function, mik is the number of samples belong-
ing to category k, Sik and Dik are the sample set and data distribution for category k in task i. We
call this function the expected prior predictor. Since data distribution Dik is considered unknown
and our only insight as to Dik is through the sample set Sik, we approximate the expected prior
predictor by its empirical counterpart, based on mik observed samples in the category k :
WP[k] = ΦV(Sik) = — X φv(xj),	(6)
mik
ik xj∈Sik
which we call the empirical prior predictor. Although we can implement the embedding func-
tion φv(∙) with a multilayer perceptron (MLP), both input X and model parameter W are high-
dimensional, making the empirical prior predictor Φv(∙) difficult to learn. According to Definition
(3), any points on the latent manifold can be approximated by a linear combination ofa set of anchor
points. Inspired by this, if the anchor points are sufficiently localized, the empirical prior predic-
tor Φv(S) can also be approximated by a linear function w.r.t. a set of codings. Accordingly, we
propose an LCC-based prior predictor, which is defined as:
W P [k]=Φ V (Sik) = ~ X X Yu(Xj )Φv(u),	⑺
mik xj∈Sik u∈C
5
Under review as a conference paper at ICLR 2020
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
where Φv (u) ∈ Rdw is the feature embedding of base u ∈ Rd. As such, the pa-
rameters of LCC-based prior predictor w.r.t. category k can be represented as vk =
[Φvk (u1), Φvk (u2), . . . , Φvk (u|C|)]. Lemma 1 illustrates the approximation error.
Figure 2: A geometric view of Local Coordinate Coding. Given a set of anchor points, if data lie
on a manifold, the empirical prior predictor Φv (S) can be locally approximated by a linear function
w.r.t. the coding. Given all bases, Φv (S) can be globally approximated.
Lemma 1. (Empirical Pior Predictor Approximation) Given the definition of WP [k] and W P [k] in
Eq. (6) and Eq. (7), let (γ, C) be an arbitrary coordinate coding on Rd and φ be an (α, β)-Lipschitz
smooth function. We have for all x ∈ Rd
kwP [k] - W P [k]k ≤ — X (αkxj-xjk + β X kxj - uk2) = Oα,β (Y, C),⑻
mik
xj ∈Sik	u∈C
where Xj = Eu∈° Yu(Xj)u∙ Then given any e > 0, there exists a coding (γ, C) such that
|C| ≤ (1 + dM)N(e, M),
Oα,β(Y, C) ≤ [αcM + (1 + 5PdM)β]e2.	⑼
Proof. See appendix B.1 for the proof.
The first inequality of Lemma 1 demonstrates that a good LCC-based prior predictor should make x
close to its physical approximation X and should be localized. The second and third inequality show
that if a set of anchor points C has cardinality O(dMN (e, M)), emprical prior predictor can be
linearly approximated using LCC UP to accuracy O(√dMe2). The complexity of the LCC coding
scheme depends only on the number of anchor points |C | instead of the input dimension. In fact, a
small |C | is usually sufficient to achieve good approximation.
Optimization of LCC. We minimize the first inequality in (8) to obtain a set of anchor points. As
with (Yu et al., 2009), We simplify the localization error term by assuming X = x, and then We
optimize the following objective function:
n
argminXX akχj- Xjk2 + β X kχj- uk2 s.t.	X Yu(X) =1,∀χ,	(IO)
γ, i=1 xj ∈Si	u∈C	u∈C
where X = Pu∈c Yu(X)u. In practice, we update C and Y by alternately optimizing a LASSO
problem and a least-square regression problem, respectively.
4.3	PAC-Bayes Localized Meta-Learning Bound with Gaussian Randomization
In order to derive a PAC-Bayes generalization bound for localized meta-learning, we first bound the
approximation error between expected prior predictor and LCC-based prior predictor.
Lemma 2. Given the definition of wp and WP in Eq. (5) and (7), let X be a compact set with
radius R, i.e., ∀X, X0 ∈ X, kX - X0k ≤ R. For any δ ∈ (0, 1] with probability ≥ 1 - δ, we have
kwP-W P k2 ≤ x( √⅛ (ι+r 2log( 1))+Oαβ (y，c )!.	(II)
6
Under review as a conference paper at ICLR 2020
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
Proof. See appendix B.2 for the proof.
Lemma 2 shows that the approximation error between expected prior predictor and LCC-based
prior predictor depends on (i) the concentration of prior predictor and (ii) the quality of LCC coding
scheme. The first term implies the number of samples for each category should be larger for better
approximation. This is consistent with the results of estimating the center of mass (Cristianini &
Shawe-Taylor, 2004). Based on Lemma 2, we have the following PAC-Bayes LML bound.
Theorem 3. Consider the Localized Meta-Learning (LML) framework, give the hyperprior P =
N(0, σv2 Idv), then for any hyperposterior Q, any c1, c2 > 0 and any δ ∈ (0, 1] with probability
≥ 1 - δ we have,
er(Q) ≤c1c2er(Q)+ E ^mσ + 2ccσ2 )kvQk2+X
c2nmiσw2
kEwQ - ΦVQ (Si)k2
v
n
+X
i=1
n
+X
i=1
c1c2
CnmiσtW
c1c2	I ɪ T
c2nmiσW I σW k=ι
2------- ∖ 2
ι+v2log( ⅞ ))+Oα,β (γ,C)	+dwK( σv )2
4n	2
log T + 2c^σv2 log δ
(12)
where c； =「-口 and c2
notation and obtain that
n
er(Q) ≤c1c2er(Q) + (X
i=1
ι-e-c2. To get a better understanding, we further simplify the
Cc +
2c2nmiσv2
c01
2c1nσv2
)kvQk2+ X F kE wQ - φ vQ (Si)k2
+ const(α, β, R,δ, n, mi).	(13)
Proof. See appendix B.3 for the proof.
Similarly with the PAC-Bayes meta-learning bound in Theorem 2 and the bounds in (Pentina &
Lampert, 2014; Amit & Meir, 2018), the expected task error er(Q) is bounded by the empirical
task error e^r(Q) plus the task-complexity and environment-complexity terms. The main innovation
here is to exploit the potential to choose the mean of prior wP based on task data S. Intuitively, if
the selection of the LCC-based prior predictor is appropriate, it will narrow the divergence between
the mean of prior wiP sampled from the hyperposterior Q and the mean of posterior wiQ in each
task. Therefore, the bound can be tighter than the ones in the meta-learning framework. Our em-
pirical study in Section 5 illustrates that the algorithms derived from this bound can achieve better
performance than the methods derived from standard PAC-Bayes meta-learning bounds.
When one is choosing the LCC-based prior predictor ΦV(∙), the number of anchor points |C|, there
is a balance between accuracy and simplicity. As we increase |C|, it will essentially increase the
expressive power of Φv(∙) and reduce the complexity term ∣∣EwQ - ΦVQ(S)k2. However, at the
V
same time, it will increase the complexity term kvQk2 and make the bound loose. If we set |C| to
1, it is degraded to the regular meta-learning framework.
4.4	Localized Meta-Learning Algorithm
Since the bound in (27) holds uniformly w.r.t. Q, the guarantees of Theorem 3 also hold for the
resulting learned hyperposterior Q = N (vQ, σV2 Idv), so the mean of prior wP sampled from the
learned hyperposterior work well for future tasks. The PAC-Bayes localized meta-learning bound in
(27) can be compactly written as
nn
XEeh(Qi = Ab(Si,P))+ αι∣vQk2 + X 01 ∣EwQ - φVQ(&)『，	(14)
i=1 V	i=1 mi V
where a；, α2 > 0 are hyperparameters. For task i, the learning algorithm Ab(∙) can be formulated
as w? = arg min Eeri(Qi = N(wQ,σWIdw)). Following Amit & Meir (2018), we jointly opti-
i	wiQ V	i	w
7
Under review as a conference paper at ICLR 2020
206
207
208
209
210
211
212
213
214
mize the parameters of LCC-based prior predictor v and the parameters of classifiers in each task
w1 , w2 , . . . , wn, which is formulated as
nn
arg min	X E,^ri(wi) + αι∣∣vQ ∣∣2 + X — ∣∣EwQ — Φ VQ (Si)k2.	(15)
v,w1,...,wn	v	mi v
i=1	i=1
We can optimize v and w via mini-batch SGD. The details of algorithms for meta-training are
given in Algorithms 1. The expectation over Gaussian distribution and its gradient can be efficiently
Algorithm 1 Localized Meta-Learning (LML) algorithm
Input: Data sets of observed tasks: Si,..., Sn.
Output: Learned prior predictor Φ parameterized by v.
Initialize v ∈ Rdv and wi ∈ Rdw for i = 1 . . . , n.
Construct LCC scheme (γ, C) from the whole training data by optimizing Eq. (10).
while not converged do
for each task i ∈ {1, . . . , n} do
Sample a random mini-batch from the data Si0 ⊂ Si .
Approximate Eeri(Wi) using Si.
v
end for
Compute the objective in (15), i.e. J _ Pn=i Ee⅛i(wi) + αι∣vQk2 + Pn=i ^∣∣EwQ —
ΦVQ (Si) k2.	V	' V
Evaluate the gradient of J w.r.t. {v, w1, . . . , wn} using backpropagation.
Take an optimization step.
end while
estimated by using the re-parameterization trick (Kingma & Welling, 2014; Rezende et al., 2014).
For example, to sample W from the posterior Q = N(wQ,σWIdw), We first draw ξ 〜 N(0,Idw)
and then apply the deterministic function wQ + ξ σ, where is an element-wise multiplication.
5	Experiments
Caltech-256	G	CIFAR-100
Caltech-256	G	CIFAR-100
m∞70∞50m
⅝vz Eo >wε3wu4
s,,5osQ5Q5Q
8u776β5544
⅝vz Eo >wε3wu4
3	5	7	9	11	1	3	5	7	9	11
Number of Tasks	Number of Tasks
35	79	11	41	35	79	11
Number of Tasks	Number of Tasks
(a) With pre-trained feature extractor	(b) Without pre-trained feature extractor
Figure 3:	The average test accuracy of learning a new task for different number of training tasks
(|C| = 64).
<	4	8	16 32	64 128 256
Number of Bases
CIFAr-IOO
(a)
Caltech-256	, „	CIFAR-100
3	5	7	9	11
Number of Tasks
VWEV7V>Ξ
3	5	7	9	11
Number of Tasks
(b)
Figure 4:	(a) The impact of the number of bases |C | in LCC. (b) The divergence value (normalized)
between the mean generated prior wP and the mean of learned posterior wQ .
5.1	Datasets and Setup
We use CIFAR-100 and Caltech-256 in our experiments. CIFAR-100 (Krizhevsky, 2009) contains
60,000 images from 100 fine-grained categories and 20 coarse-level categories. As in (Zhou et al.,
8
Under review as a conference paper at ICLR 2020
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
2018), we use 64, 16, and 20 classes for meta-training, meta-validation, and meta-testing, respec-
tively. Caltech-256 has 30,607 color images from 256 classes (Griffin et al., 2007). Similarly, we
split the dataset into 150, 56 and 50 classes for meta-training, meta-validation, and meta-testing. We
consider 5-way classification problem. Each task is generated by randomly sampling 5 categories
and each category contains 50 samples. The base model uses the convolutional architecture in (Finn
et al., 2017), which consists of 4 convolutional layers, each with 32 filters and a fully-connected
layer mapping to the number of classes on top. High dimensional data often lies on some low di-
mensional manifolds. We utilize an auto-encoder to extract the semantic information of image data
and then construct the LCC scheme based on the embeddings. The parameters of prior predictor and
base model are random perturbations in the form of Gaussian distribution. We design two different
meta-learning environment setting to validate the efficacy of the proposed method. The first one uses
the pre-trained base model as an initialization, which utilizes all the meta-training classes (64-class
classification in CIFAR-100 case) to train the feature extractor. The second one uses the random
initialization. We compare the proposed LML method with ML-PL method (Pentina & Lampert,
2014), ML-AM method (Amit & Meir, 2018) and ML-A which is derived from Theorem 2. In these
methods, we use their main theorems about the PAC-Bayes generalization bound to derive the objec-
tive for the algorithm. We also compare with two typical meta-learning few-shot learning methods:
MAML (Finn et al., 2017) and MatchingNet (Vinyals et al., 2016). To ensure a fair comparison, all
approaches adopt the same network architecture and pre-trained feature extractor.
5.2	Results
In Figure 3, we demonstrate the average test error of learning a new task based on the number of
training tasks in different settings (with or without a pre-trained feature extractor). It is obvious
that the performance continually increases as we increase the number of training tasks for all the
methods. This is consistent with the generalization bounds that the complexity term converges to
zero if large numbers of tasks are observed. ML-A consistently outperforms ML-PL and ML-AM
since the bound w.r.t. ML-A in Theorem 2 converges at the rate of O(ml) while the bounds w.r.t.
ML-PL and ML-AM converge at the rate of O (√=). This demonstrates the importance of using tight
generalization bound. Our proposed LML significantly outperforms the baselines, which validates
the effectiveness of the proposed LCC-based prior predictor. It is a more suitable representation for
meta-knowledge than the traditional global hyperposterior in ML-A, ML-AM, and ML-PL.
Moreover, we can find that all PAC-Bayes baselines outperform MAML and MatchingNet. Note
that MAML and MatchingNet adopt the episodic training paradigm to solve the few-shot learn-
ing problem. The meta-training process requires millions of tasks and each task contains limited
samples, which is not the case in our experiment. Scarce tasks in meta-training leads to severely
meta-overfitting. In our method, the learned prior serves both as an initialization of base model and
as a regularizer which restricts the solution space while allowing variation based on specific task
data. It yields a model with smaller error than its unbiased counterpart when applied to a similar
task.
Finally, we observe that if the pre-trained feature extractor is provided, all of these methods do better
than meta-training with random initialization. This is because pre-trained feature extractor can be
regarded as a data-dependent hyperpior. Itis closer to the hyperposteior than the randomly initialized
hyperprior. Therefore, it reduces the environment complexity term and improves the generalization
performance.
In Figure 4(b), we show the divergence between the mean of generated prior wP from meta model
and the mean of learned posterior wQ for LML and ML-A. This further validates the effectiveness
of the LCC-based prior predictor which could narrow the divergence term and thus tight the bound.
In Figure 4(a), we vary the number of bases |C | in LCC scheme from 4 to 256, the optimal value is
around 64 in both datasets. This indicates that LML is sensitive to the number of bases |C |, which
further affects the quality of LCC-based prior predictor and the performance of LML.
6	Related Work
Meta-Learning. Meta-learning literature commonly considers the empirical task error by directly
optimizing a loss of meta learner across tasks in the training data. Recently, this has been success-
9
Under review as a conference paper at ICLR 2020
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
fully applied in a variety of models for few-shot learning (Ravi & Larochelle, 2017; Snell et al.,
2017; Finn et al., 2017; Vinyals et al., 2016). Although Vuorio et al. (2018); Rusu et al. (2019);
Zintgraf et al. (2019); Wang et al. (2019) consider task adaptation when using meta-knowledge for
specific tasks, all of them are not based on generalization error bounds, which is the focus of our
work. Meta-learning in the online setting has regained attention recently (Denevi et al., 2018b;a;
2019; Balcan et al., 2019), in which online-to-batch conversion results could imply generalization
bounds. Galanti et al. (2016) analyzes transfer learning in neural networks with PAC-Bayes tools.
Most related to our work are (Pentina & Lampert, 2014; Amit & Meir, 2018) which provide a
PAC-Bayes generalization bound for meta-learning framework. In contrast, neither work considers
localized meta-knowledge for specific tasks.
Localized PAC-Bayes Learning. There has been a prosperous line of research for learning priors
to improve the PAC-Bayes bounds Catoni (2007); Guedj (2019). (Parrado-Hernandez et al., 2012)
showed that priors can be learned by splitting the available training data into two parts, one for
learning the prior, one for learning the posterior. (Lever et al., 2013) derived an expression for the
overall best prior, i.e. the distribution resulting in the smallest possible bound value and bounded
the KL divergence by a term independent of data distribution. Recently, (Rivasplata et al., 2018)
bounded the KL divergence by investigating the stability of the hypothesis. (Dziugaite & Roy, 2018)
optimized the prior term in a differentially private way. In summary, theses methods construct some
quantities that reflect the underlying data distribution, rather than the sample set, and then choose
the prior P based on these quantities. These works, however, are only applicable for single-task
problem and could not transfer knowledge across tasks in meta-learning setting.
7	Conclusion
This work contributes a novel localized meta-learning framework from a theoretical perspective. We
propose a generalization bound based on PAC-Bayes theory with Gaussian randomization. Instead
of formulating meta-knowledge as a global distribution, we propose an LCC-based prior predictor
to output local meta-knowledge by using task information. We further develop a practical algorithm
with deep neural network based on the bound. An interesting topic for future work would be to
explore other principle to construct the prior predictor and apply the localized meta-learning frame-
work to a more realistic scenario that tasks are sampled non-i.i.d. from an environment. Another
challenging problem is to extend our techniques to derive localized meta-learning algorithms for
regression and reinforcement learning problem.
References
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory.
In International Conference on Machine Learning, pp. 205-214, 2018.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-
based meta-learning. In Proceedings of the 36th International Conference on Machine Learning,
ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 424-433, 2019.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:
149-198, 2000.
LUca Bertinetto, Joao F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learning feed-
forward one-shot learners. In Advances in Neural Information Processing Systems, pp. 523-531,
2016.
O Catoni. PAC-Bayesian supervised classification: The thermodynamics of statistical learning.
institute of mathematical statistics lecture notes—monograph series 56. IMS, Beachwood, OH.
MR2483528, 2007.
Nello Cristianini and John Shawe-Taylor. Kernel methods for pattern analysis, volume 173. Cam-
bridge University Press Cambridge, 2004.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. In Proceedings of the Thirty-Fourth Conference on Uncertainty
10
Under review as a conference paper at ICLR 2020
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
in Artificial Intelligence, UAI2018, Monterey, California, USA, August 6-10, 2018, pp. 457-466,
2018a.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around
a common mean. In Advances in Neural Information Processing Systems, pp. 10169-10179,
2018b.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
pp. 1566-1575, 2019.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent PAC-Bayes priors via differential
privacy. In Advances in Neural Information Processing Systems, pp. 8430-8441, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126-1135. JMLR. org, 2017.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning.
Information and Inference: A Journal of the IMA, 5(2):159-209, 2016.
Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. Pac-bayesian learn-
ing of linear classifiers. In Proceedings of the 26th Annual International Conference on Machine
Learning, pp. 353-360. ACM, 2009.
Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.
Benjamin Guedj. A primer on pac-bayesian learning. arXiv preprint arXiv:1901.05353, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer,
2009.
Guy Lever, Francois Laviolette, and John Shawe-Taylor. Tighter PAC-Bayes bounds through
distribution-dependent priors. Theoretical Computer Science, 473:4-28, 2013.
Andreas Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research,
6(Jun):967-994, 2005.
David A McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164-170. ACM, 1999.
Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image
classification: Generalizing to new classes at near-zero cost. IEEE transactions on pattern anal-
ysis and machine intelligence, 35(11):2624-2637, 2013.
Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-Bayes
bounds with data dependent priors. Journal of Machine Learning Research, 13(Dec):3507-3531,
2012.
Anastasia Pentina and Christoph Lampert. A PAC-Bayesian bound for lifelong learning. In Inter-
national Conference on Machine Learning, pp. 991-999, 2014.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting
parameters from activations. In 2018 IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 7229-7238, 2018.
11
Under review as a conference paper at ICLR 2020
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31th International Con-
ference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 1278-1286,
2014.
Omar Rivasplata, Csaba Szepesvari, John S Shawe-Taylor, Emilio Parrado-Hernandez, and Shiliang
Sun. PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Advances in
Neural Information Processing Systems, pp. 9214-9224, 2018.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-
dero, and Raia Hadsell. Meta-learning with latent embedding optimization. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,
2019.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4077-4087, 2017.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J Lim. Toward multimodal model-agnostic
meta-learning. arXiv preprint arXiv:1812.07172, 2018.
Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and Joseph E Gonzalez. Tafe-net: Task-aware
feature embeddings for low shot learning. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1831-1840, 2019.
Flood Sung Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proc. of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 2018.
Kai Yu, Tong Zhang, and Yihong Gong. Nonlinear learning using local coordinate coding. In
Advances in neural information processing systems, pp. 2223-2231, 2009.
Fengwei Zhou, Bin Wu, and Zhenguo Li. Deep meta-learning: Learning to learn in the concept
space. arXiv preprint arXiv:1802.03596, 2018.
Luisa M. Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast
context adaptation via meta-learning. In Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 7693-7702,
2019.
12
Under review as a conference paper at ICLR 2020
397
398
399
400
401
402
403
404
405
406
407
408
409
This supplementary document contains the technical proofs of theoretical results and details of ex-
periments. It is structured as follows: Appendix A present notations for prior predictor. Appendix B
gives the proofs of the main results. Appendix B.1 and B.2 show the approximation error between
LCC-based prior predictor and empirical prior predictor, expected prior predictor, respectvely. They
are used in the proof of Theorem 3. Next, in Appendix B.3 and B.4 we show the PAC-Bayes gener-
alization bound of localized meta-learning in Theorem 3 and also provides the PAC-Bayes general-
ization bound of regular meta-learning in Theorem 2. Finally, details of experiments are presented
in Appendix C.
A Notations
Let φv(∙) : Rd → Rdw be the feature embedding function. mik denotes the number of samples
belonging to category k. Sik and Dik are the sample set and data distribution for category k in task
i, respectively. Then, the expected prior predictor w.r.t. class k in task i is defined as:
wiP [k] =Φv(Dimkik)
E ɪ X φv(xj).
Sik-Dmkik mik χj∈Sik
The empirical prior predictor w.r.t. class k in task i is defined as:
W P [k] = φv(Sik ) =  X φv(xj ).
i	mik xj∈Sik
The LCC-based prior predictor w.r.t. class k in task i is defined as:
WP[k] = ΦV(Sik) = L X X Yu(Xj)Φv(u).
ik xj∈Sik u∈C
B	Theoretical Results
B.1	Proof of Lemma 1
This lemma bounds the error between the empirical prior predictor W P [k] and the LCC-based prior
predictor W P [k].
Lemma 1 Given the definition of WP [k] and WP [k] in Eq. (6) and Eq. (7), let (γ, C) be an arbitrary
coordinate coding on Rdx and φ be an (α, β)-Lipschitz smooth function and . We have for all
x ∈ Rdx
kW P [k] -	W P [k]k	≤	1—	X	(αkxj-χj k	+ β X	kxj-uk2)	=	Oα,β (Y, C),	(16)
mik
xj ∈Sik	u∈C
where Xj = PueC Yu(Xj)u. Then given any e > 0, there exists a coding (γ, C) such that
|C| ≤ (1 + dM)N(g M),
Oα,β(Y, C) ≤ [αcM + (1 + 5PdM)β]e2.	(17)
13
Under review as a conference paper at ICLR 2020
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
Proof. Let Xj = 52u∈c Yu(Xj)u. Wehave
..ʌ ， ` 一， ..
kΦV(Sik) - ΦV(Sik)k2
=L X kΦv(Xj ) - X Yu(Xj )Φv(u)k2
ik xj∈Sik	u∈C
≤ ɪ X (kΦv(Xj ) - Φv(Xj )k2 + k X Yu(Xj )(Φv(u)- Φv(Xj )k2)
mik
xj ∈Sik	u∈C
= -1- X (kΦv(Xj) - Φv(Xj)k2 + k X Yu(Xj)(Φv(u) - Φv(X Yu(Xj)u)) - VΦv(Xj)(u -Xj)k2)
m
ik xj∈Sik	u∈C	u∈C
≤ ɪ X (kΦv(Xj ) - Φv(Xj )k2 + X ∣Yu(Xj )lk(Φv(u) - Φv( X Yu (Xj )u)) - VΦv (Xj )(u - X j )k2)
m
ik xj∈Sik	u∈C	u∈C
≤ 1— X (akXj - Xjk2 + β X llXj- - uk2) = Oα,β(y,c)
mik
xj ∈Sik	u∈C
In the above derivation, the first inequality holds by the triangle inequality. The second equality
holds since Pu∈C Yu(Xj) =1forall Xj. The last inequality uses the assumption of (α, β)-Lipschitz
smoothness of Φv(∙).
According to the Manifold Coding Theorem in (Yu et al., 2009), if the data points X lie on a compact
smooth manifold M. Then given any > 0, there exists anchor points C ⊂ M and coding Y such
that
|C| ≤ (1 + dM)N (, M),
---X (αkXj - Xj k2 + β X kXj - uk2)≤ [αcM + (I + 5PdM)β]e2.	(18)
mik
xj ∈Sik	u∈C
This implies the desired bound.	口
The first inequality of this lemma demonstrates that the quality of LCC approximation is bounded
by two terms: the first term ∣∣Xj∙ - Xj∙∣∣2 indicates X should be close to its physical approximation
X, the second term ∣∣Xj∙ - Uk implies that the coding should be localized. The second and third
inequality show that the approximation error of local coordinate coding depends on the intrinsic di-
mension of the manifold instead of the dimension of input. Ifa set of anchor points C has cardinality
O(dMN (, M)), emprical prior predictor can be linearly approximated using LCC up to accuracy
θ(√dMe2).
B.2 Proof of Lemma 2
In order to proof Lemma 2, we first introduce a relevant theorem.
Theorem 4. (Vector-valued extension of McDiarmid’s inequality (Rivasplata et al., 2018)) Let
X1,...,Xm ∈ X be independent random variables, and f : Xm → Rdw be a vector-valued
mapping function. If, for all i ∈ {1, . . . , m}, and for all X1, . . . , Xm, X0i ∈ X, the function f
satisfies
sup kf(X1:i-1, Xi, Xi+1:m) - f(X1:i-1,X0i,Xi+1:m)k ≤ ci	(19)
xi ,x0i
Then Ekf(XI：m) — E[f (Xi：m)]| ≤ PPm=Ic2∙ For any δ ∈ (0,1) With probability ≥ 1 — δ we
have
kf(X1:m)-E[f(X1:m)]k ≤
um
tuXci2+
i=1
JP⅛j log(δ).
(20)
The above theorem indicates that bounded differences in norm implies the concentration of f(X1:m)
around its mean in norm, i.e., kf(X1:m) - E[f(X1:m)]k is small with high probability.
14
Under review as a conference paper at ICLR 2020
425 Then, we bound the error between expected prior predictor wiP and the empirical prior predictor
426 WP.
Lemma 3. Given the definition of wP [k] and WP [k] in (5) and (6), let X be a compact set with
radius R, i.e., ∀x, x0 ∈ X, kx - x0k ≤ R. For any δ ∈ (0, 1] with probability ≥ 1 - δ, we have
kwP [k]- W W [k]k ≤ √αRk (1 + y∣log(δ)).	(21)
Proof. According to the definition of ΦV(∙) in (6), for all points xι,..., Xj-1, Xj+ι,..., Xmk, Xj
in the sample set Sik, we have
0
) - φv(XIj-1, Xj, Xj+1:mk )k
SUp kΦv(xιj-i, Xj, Xj+ιmk
xi ,x0i
ɪsup kφv(Xj) - φv(Xj)k
mik xj ,x0j
1
≤ ----
mik
sUp αkXj -
xj ,x0j
0 αR
Xjk ≤ mk,
(22)
427
where R denotes the domain ofX, say R = sUpx kXk. The first inequality follows from the Lipschitz
smoothness condition of Φv(∙) and the second inequality follows by the definition of domain X.
Utilizing Theorem 4, for any δ ∈ (0, 1] with probability ≥ 1 - δ we have
kwP[k] - WP[k]k = kΦV(Sik)- E[ΦV(Sik)]k ≤ 工(1 +J1log(1)).	(23)
mik	2 δ
This implies the bound.	□
428 Lemma 3 shows that the bounded difference of function Φv(∙) implies its concentration, which can
429 be further used to bound the differences between empirical prior predictor WP [k] and expected prior
430 predictor WiP [k]. Now, we bound the error between expected prior predictor WiP and the LCC-based
43i prior predictor W P.
Lemma 2 Given the definition of ww and WP in (5) and (7), let X be a compact set with radius R,
i.e., ∀X, X0 ∈ X, kX - X0k ≤ R. For any δ ∈ (0, 1] with probability ≥ 1 - δ, we have
kWP -W P k2 ≤ x( √αR=(ι+r io© 1))+"β (γ,C)).	(24)
Proof According to the definition of wp, WP and WP, we have
kWP - WPk2
K
=X kwP[k] - W P [k]k2
k=1
K
=X kE[Φ V(Sik)] - Φ V(Sik )+Φ V (Sik) - Φ V(Sik)k2
k=1
K
=X (kE[ΦV(Sik)] - φV(Sik)k2 + kφV(Sik) - φV(Sik)k2 + 2(E[ΦV(Sik)] - φV(Sik))>(ΦV(Sik) - φV(Sik)))
k=1
K
≤ X (kE[ΦV(Sik)] - ΦV(Sik)k2 + kΦV(Sik) - ΦV(Sik)k2 + 2kE[ΦV(Sik)] - ΦV(Sik)kkΦV(Sik) - ΦV(Sik)k).
k=1	(25)
Substitute Lemma 3 and Lemma 1 into the above inequality, we can derive
1 + r 2 log( δ )) + Oα,β(Y,C)) ∖ ≥ 1 - δ∙ (26)
PSik Hm IkWP - w p k2 ≤X
15
Under review as a conference paper at ICLR 2020
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
This gives the assertion.
Lemma 2 shows that the approximation error between expected prior predictor and LCC-based prior
predictor depends on the number of samples in each category and the quality of the LCC coding
scheme.
B.3 Proof of Theorem 3
Theorem 3 Let Q be the posterior of base learner Q = N (wQ , σw2 Idw ) and P be the prior
N(ΦV(S),σWIdw). The mean of prior is produced by the LCC-based prior predictor ΦV(S) in
Eq. (7) and its parameter v is sampled from the hyperposterior of meta learner Q = N (vQ, σv2 Idv).
Give the hyperprior P = N(0, σV2 Idv), then for any hyperposterior Q, any c1, c2 > 0 and any
δ ∈ (0, 1] with probability ≥ 1 - δ we have,
er(Q) ≤c1c2er(Q)+(X 2c2cm⅛+2c⅛ )kvQk2+X
c2nmiσw2
kEwQ - ΦVQ (Si)k2
V
n
+X
i=1
n
+X
i=1
CC (ɪX
c2nmiσW ∖ σw k=ι
2---------- ∖ 2
1 + Y 2 log( ɪ )) + Oα,β(Y, C)) + dwK( σv )2
Cc
C2nmiσw
4n	C01	2
log T + 2cinσ2 log δ
(27)
where c；=二-口 and c2 =
I-;-2 . We can simplify the notation and obtain that
n
er(Q) ≤C1c2er(Q) + (X
i=1
2c2c‰ + 2c⅛)kvQk2 + X c2⅛kEwQ - φvq(Si)k2
+ const(α, β, R,δ, n, mi).
(28)
Proof Our proof contains two steps. First, we bound the error within observed tasks due to ob-
serving a limited number of samples. Then we bound the error on the task environment level due
to observing a finite number of tasks. Both of the two steps utilize Catoni’s classical PAC-Bayes
bound (Catoni, 2007) to measure the error. We give here a general statement of the Catoni’s classical
PAC-Bayes bound.
Theorem 5. (Classical PAC-Bayes bound, general notations) Let X be a sample space and X
be some distribution over X, and let F be a hypotheses space of functions over X. Define a loss
function g(f, X) : F × X → [0, 1], and let X1G , {X1, . . . , XG} be a sequence of K independent
random variables distributed according to X. Let π be some prior distribution over F (which must
not depend on the samples X1, . . . , Xk). For anyδ∈ (0, 1], the following bounds holds uniformly
for all posterior distribution ρ over F (even sample dependent),
PXK 〜x
1 i.i.d
≥ 1 - δ.
1G
尸 E E g(f，Xk +
G g=ι f ~p
KL(P||n) + log 1
K×c
, ∀ρ
(29)
J E E g(f,X) ≤ ―—
[x~Xf 〜*，，—1 - e-c
First step We utilize Theorem 5 to bound the generalization error in each of the observed tasks.
Let i ∈ 1, . . . , n be the index of task. For task i, we substitute the following definition into
the Catoni’s PAC-Bayes Bound. Specifically, Xg , (xij , yij ), K , mi denote the samples and
X , Di denotes the data distribution. We instantiate the hypotheses with a hierarchical model
f , (v, w), where v ∈ Rdv and w ∈ Rdw are the parameters of meta learner (prior predic-
tor) Φv(∙) and base learner h(∙) respectively. The loss function only considers the base learner,
which is defined as g(f, X) ，'(hw(x),y). The prior over model parameter is represented
as π , (P, P) , (N (0, σV2 Idv ), N(wP , σw2 Idw )), a Gaussian distribution (hyperprior of meta
16
Under review as a conference paper at ICLR 2020
450
451
452
453
454
455
456
457
458
459
460
461
learner) centered at 0 and a Gaussian distribution (prior of base learner) centered at WP, respec-
tively. We set the posterior to ρ , (Q, Q) , (N (vQ, σV2IdV),N(WQ, σw2 Idw)), a Gaussian dis-
tribution (hyperposterior of meta learner) centered at vQ and a Gaussian distribution (posterior of
base learner) centered at WQ . According to Theorem 5, the generalization bound holds for any
posterior distribution including the one generated in our localized meta-learning framework. Specif-
ically, we first sample v from hyperposterior N (vQ, σV2 IdV) and estimate WP by leveraging ex-
pected prior predictor WP = ΦV(D). The base learner algorithm Ab(S, P) utilizes the sample set
S and the prior P = N(WP, σw2 Idw ) to produce a posterior Q = Ab(S, P) = N (WQ, σw2 Idw ).
Then we sample base learner parameter W from posterior N (WQ , σw2 Idw ) and compute the in-
CUrred loss '(hw (x),y). On the whole, meta-learning algorithm Am(Sι,..., Sn, P) observes
a series of tasks S1, . . . , Sn and adjusts its hyperprior P = N(vP, σV2 IdV) into hyperposterior
Q=Am(S1,...,Sn,P) =N(vQ,σV2IdV).
The KL divergence term between prior π and posterior ρ is compUted as follows:
KL(Pkn) =fEρlog f
E
E
I	N (vQ,σV IdV)N (wQ,σw Idw)
g N(0,σVIdv )N(wp,σwIdw)
V〜N(vQ,σVIdV )w〜N(wQ,σWIdw )
E	log N "②足 IdV)
V〜N(VQ,σ2 IdV)	N(0, σVIdV )
1— IlvQk2 + E -ɪ k
2σV kv k +v〜N(VE,σ22Idv) 2σW k
V 〜N (vQ,σV IdV )w 〜N (WQQw Idw)
wQ - wP k2.
I NwQifWIdw)
g N(WP,σwIdw)
(30)
In oUr localized meta-learning framework, in order to make KL(Q||P ) small, the center of prior
distribUtion WP is generated by the expected prior predictor WP = ΦV (D). However, the data
distribUtion D is considered Unknown and oUr only insight as to Dik is throUgh the sample set
Sik . In this work, we approximate the expected prior predictor ΦV(D) with the LCC-based prior
predictor WP = ΦV(S). Denote the term E	y⅛∣∣wq - WP∣∣2 by Ey⅛ ∣∣wq - WP∣∣2
V〜N(vQ,σVIdV )2σw"	V 2σw
for convenience, we have
Eττ12-||wQ - WPk2 =E-ι2-||wQ - WP + WP - WPk2
V 2σ2	V 2σ2
ww
=E 2⅛[k
≤E 2⅛[k
WQ - WPk2 + kWP - WPk2 + 2(wq - WP)>(WP - WP)]
WQ — W P k2 + IIW P — WP k2 + 2∣∣wq — W P IIkW P — WP k]
≤-2~EkWQ — Φv(S)∣∣2 + ~eEkWP - WPk2.
σ2 V	σ2 V
ww
(31)
Since WP = ΦV(Si) = [ΦV(SiI),..., ΦV(Sik),..., ΦV(SiK)], we have
K
EkWQ - ΦV(Si)k2 = XEkwQ[k] - ΦV(Sik)k2
VV
-2(EwQ[k])>(ΦVQ(Sik)) + kΦVQ(Sik)k2 + V[kΦV(Sik)k]
=E	kEwQ[k]- ΦVQ(Sik)k2
k=1
= IEwQ — Φ VQ (Si)k2 + dwKσ
V
dV 2
+西σv
V2,
(32)
+
E
E
V
V
17
Under review as a conference paper at ICLR 2020
where V[∣∣ΦV(Sik)k] denotes the variance of ∣∣ΦV(Sik)∣∣. The last equality uses the fact that dv
v
|C|dw. Combining Lemma 2, for any δ0 ∈ (0, 1] with probability ≥ 1 - δ0 we have
EUr kwQ - WPk2
V 2σw2	i
K
≤FkEwi - φVQ(Si)k + dwK(1) +	2 X
σw2 V i	σw	σw2 k=1
2------- ∖ 2
1 + ʌ/2 log( δ )) + Oα,β(Y,C))
(33)
Then, according to Theorem 5, We obtain that for any 与 > 0
(χ,y)〜DiV〜N(VQ ,σ2Idv )w〜N(WQ ,σWIdw )
'(hw (χ),y)
mi
≤	c2	∙ ɪ X
― 1 - e-c2	mi 乙
j=1
V〜N(VQ ,σ2Idv )w〜N(WQ ,σwIdw )
`(hw (xj), yj)
+ (I」) ∙ mi (W kvQk2 + V 〜N (VE,σ2 Idv) 2⅛kwQ -WP k2 +log δi) , ∀Q}≥
1 - 2,
(34)
for all observed tasks i = 1,...,n. Define δ0 =与 and combine inequality (33), we obtain
≤ G
E	E	E	'(hw (x),y)
(χ,ygDi57N(vQ,σvIdv )w~N(WQ ,σwIdw )
1 mi
• 一 X E	E	'(hw(χj ),yj)
mi j=1 v~N(VQ,σ2 Idv )w~N(wQ,σwIdw )
+ (1-e-c2 )mi ∙ (W kv
Qk2 + σb kE wQ- φ vq (Si)k2+log δi +dw K(≡ )2
1K
+」F X
σw k=1
(1 + r 2 log( δ )) + Oα,β (γ, C)) ), ∀Q} ≥ 1 - δi,
(35)
E
E
E
E
E
Using the notations in Section 4, the above bound can be simplified as
PS Dmi	E	er(Ab(Si, Pi))
i i Iv~n(VQ,σvIdv),wP=φv(D),Pi=N(WP,σwIdw)
≤ —c2—	E	6r(Ab(Si, Pi)
1 - e-c2 v~N(VQ,σvIdv ),wP =Φv(D),Pi=N(wP,σwIdw )
+ ∩-----^ʒ—	(kvQ kνQk2	+ ~2^ kEwQ	- φ VQ (Si)k2 +	logy	+ dwK ( - )2
(1	- e-c2 )mi 2σV2 σw2 V i	δi	σw
+⅛^ X( √αRh(I+r ιlog( 2i))+O* (γ, C))!, ∀Q) ≥ 1 - δi.
(36)
Second step Next we bound the error due to observing a limited number of tasks from the envi-
ronment. We reuse Theorem 5 with the following substitutions. The samples are (Di, mi, Si), i =
1,...,n, where (Di, mJ are sampled from the same meta distribution T and Si 〜Dmi. The
hyposthesis is parameterized as ΦV (D) with meta learner parameter v. The loss function is
g(f,X)，	E	E	'(hw(x),y), where wQ = Ab(Si,Pi). Let π，N(0,σVIdv) be
(X,y)〜DW〜N(wQ,σwIdw )
18
Under review as a conference paper at ICLR 2020
the prior over meta learner parameter, the following holds for any δ0 > 0,
EEE	E	E	'(hw(x),y)
(D,mhτs~Dmv~N(vQ,σ2 Idv )w~N(wQ,σWIdw )(x,y)〜Di
≤ 1-ce-c1
1
+ (1 - e-c1 )n
1n
-V E	E	E	'(hw (x),y)
n i=1 v~N(vQ,σ2Idv )w~N(wQ,σwIdw )(x,y)~Di
(37)
Using the term in Section 4, the above bound can be simplified as
P(Dmi )~τ,Si~Dm ,i=ι,…,n{ er(Q)
1n
≤ —c1-------V	E	er(Ab(Si,Pi))
1 - e-c1 n i=1 v~N(vQ,σvIdv ),wp =Φv(D),Pi=N(WP,σwIdw )
+ ∩—-C1、	(5^τIIvQk2 + log T) ,∀q] ≥ 1 - δo,
(1 - e-c1 )n 2σv2	δ0
(38)
Finally, by employing the union bound, we could bound the probability of the intersection of the
events in (36) and (38) For any δ > 0, set δo，2 and δi，ɪ for i = 1,...,n,we have
P(Dmi )~τ,Si~Dm ,i=1,…,n{ er(Q)
1n
≤7--------c1C2-----? ∙ - V	E	er(Ab(Si, Pi))
(1 - e-c1 )(1 - e-c2) n i=1 v~N(VQ,σvIdv),wp=φv(D),Pi=N(WP,σwIdw)
+~C1-;7 ∙ -X ∩-—^∖— (ʌIIvQk2 + -kIIEwQ - φvQ(Si)k2 + logɪ
1 - e-c1	n (1 - e-c2 )mi 2σv2	σW2	v	δ
+ σ1w X (√αR=(1 + r 2log(4n))+°a (Y,C)) + dwK(σv )2
+ (1-∖)n (2⅛ kvQk2+logδ)，∀Q} ≥1 - δ∙
We can further simplify the notation and obtain that
P(Dmi )fSi 〜Dmi,i=1,…,n{ er(Q) ≤ c1c2er(Q)
+(X ； + 2⅛ )kvQk2 + X F IEwQ- φ VQ (Si)I2
+const(α, β,R, δ, n,mi), ∀Q ≥ 1 - δ,
(39)
(40)
462 where c； =「；-” and c2 = 二".This completes the proof.
463 B.4 Proof of Theorem 2
Theorem 2 Let Q be the posterior of base learner Q = N (wQ , σW2 Idw ) and P be the prior
N(wP, σW2 Idw ). The mean of prior is sampled from the hyperposterior of meta learner Q =
N(wQ, σW2 Idw ). Give the hyperprior P = N(0, σW2 Idw ), then for any hyperposterior Q, any
19
Under review as a conference paper at ICLR 2020
c1, c2 > 0 and any δ ∈ (0, 1] with probability ≥ 1 - δ we have,
er(Q) ≤c1c2er(Q) + (X s, c1c2 2 +:
1 2	i=1 2c2nmiσw2
n	c01c02	1	2n
+ 2∑---------2(9 + log ɪ) +
i=1 c2nmiσw2 2	δ
2C1c⅛ )kwQk2 + X 2C2‰ kWPWQ-WQk2
Cl 1 2
-----2 log A,
c1nσw2	δ
(41)
464 where 4 = 1-∣⅛ and C = 1-f⅛
Proof Instead of generating the mean of prior with a prior predictor, the vanilla meta-learning
framework directly produces the mean of prior WP by sampling from hyperposterior Q =
N(WQ,σw2 Idw). Then the base learner algorithm Ab(S, P) utilizes the sample set S and the prior
P = N(WP,σw2 Idw) to produce a posterior Q = Ab(S, P) = N (WQ, σw2 Idw). Similarly with the
two-steps proof in Theorem 3, we first get an intra-task bound by using Theorem 5. For any δi > 0,
we have
Psi~Dma∣	EE	E	'(hw(x),y)
i i [(X,yhDawP~N(WQ,σWIdw )w~N(WQ,σWIdw )
c 1 mi
≤ ——— ∙——E	E	'(hw (Xj ),yj)
1 - e-c2 mi ~1 wp~N(wQ,σwIdw )w~N(wQ,σwIdw )
+ ∩-----J------(5^τ IIwQk2 + P , E 2	5~τ kwQ	- wPk2 + log δ^ ) , ∀Q∖	≥ 1 - δi,
(I	- e 2) ∙ mi	∖2σw	wp~N(wQ,σwIdw) 2σw	δi ) J
(42)
The term E	7⅛ k WQ — WP k2 can be simplified as
wP ~N (wQ,σw Idw )2σw" i	i
E -ɪ ∣∣wq - wpk2
wp ~N(WQ,σw Idw ) 2σw
A E E kwQk2 - 2( E wQ)>wq + kwQk2 + V [kwPk]
2σw wP	wP	wiP
- wQk2 +σw2
(43)
where V [kwiP k] denotes the variance of kwiP k. Then we get an inter-task bound. For any δ0 > 0,
wiP
we have
P Dmi	S Dmi i 1	E E	E	E	E	`(hw (X), y)
(Di ) TS D ,,…,n [(D,m)^τS^Dmwp-N(wQ,σwIdw )w-N(wQ,σwIdw )(x,y)~Dz
1n
≤ —^1-------V E	E	E	'(hw (x),y)
1 - e c1	n i=1 wP~N(wQ,σwIdw )w~N(wQ,σwIdw ')(x,y)~Di
1
+ L
,—'
市(2⅛kwQk2 + log δ0)，∀q}≥ 1 - δ0.
(44)
20
Under review as a conference paper at ICLR 2020
For any δ > 0, set δo，2 and δi，ɪ for i = 1,...,n. Using the union bound, We finally get
er(Q)
≤_________c1c2_________
_(1 - e-cι)(1 - e-c2)
1n
∙1 X
n
i=1
E
v~N(VQ,σV Idv ),wP =φv (D),Pi=N (WPQiw Idw )
er(Ab(Si,Pi))
+ T⅛1
1
+ (1-e-s
1n
∙1 X-
n t1
1
(1-e-c2) ∙ mi G⅛kwQk2 + 2⅛kWPwQ
wQk2+2+log2n
)n	2σw2
WQk2 + log 2) ,∀q} ≥ 1 - δ.
(45)
—
Similarly, We can further simplify the notation and obtain that
P(Dmi )~τ,Si~Dmi,i=1,…,n{er(Q) ≤ c1c2er(Q)
十(X c1c2— + c1	)kwQk2 + X c1c2— k E WQ - WQk2
+± 2c2nmiσW + 2cιnσW )kw k + = 2C2nmiσW kWPWi W k
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
+const(δ, n, mi), ∀Q ≥ 1 - δ,
where c；= 二-口 and c2 = 二-2.This completes the proof.
(46)
C Details of Experiments
C.1 Data Preparation
We used the 5-way 50-shot classification setups, where each task instance involves classifying im-
ages from 5 different categories sampled randomly from one of the meta-sets. We did not employ
any data augmentation or feature averaging during meta-training, or any other data apart from the
corresponding training and validation meta-sets.
C.2 Network Architechture
Auto-Encoder for LCC For CIFAR100, the encoder is 7 layers with 16-32-64-64-128-128-256
channels. Each convolutional layer is followed by a LeakyReLU activation and a batch normaliza-
tion layer. The 1st, 3rd and 5th layer have stride 1 and kernel size (3, 3). The 2nd, 4th and 6th layer
have stride 2 and kernel size (4, 4). The 7th layer has stride 1 and kernel size (4, 4). The decoder is
the same as encoder except that the layers are in reverse order. The input is resized to 32 × 32. For
Caltech-256, the encoder is 5 layers with 32-64-128-256-256 channels. Each convolutional layer is
followed by a LeakyReLU activation and a batch normalization layer. The first 4 layers have stride
2 and kernel size (4, 4). The last layer has stride 1 and kernel size (6, 6). The decoder is the same
as encoder except that the layers are in reverse order. The input is resized to 96 × 96.
Base Model The network architecture used for the classification task is a small CNN with 4 con-
volutional layers, each with 32 filters, and a linear output layer, similar to (Finn et al., 2017). Each
convolutional layer is followed by a Batch Normalization layer, a Leaky ReLU layer, and a max-
pooling layer. For CIFAR100, the input is resized to 32 × 32. For Caltech-256, the input is resized
to 96 × 96.
C.3 Optimization
Auto-Encoder for LCC As optimizer we used Adam(Kingma & Ba, 2015) with β1 = 0.9 and
β2 = 0.999. The initial learning rate is 1 × 10-4. The number of epochs is 100. The batch size is
512.
21
Under review as a conference paper at ICLR 2020
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
LCC Training We alternatively train the coefficients and bases of LCC with Adam with β1 = 0.9
and β2 = 0.999. In specifics, for both datasets, we alternatively update the coefficients for 60 times
and then update the bases for 60 times. The number of training epochs is 3.The number of bases is
64. The batch size is 256.
Pre-Training of Feature Extractor We use a 64-way classification in CIFAR-100 and 150-way
classification in Caltech-256 to pre-train the feature embedding only on the meta-training dataset.
For both CIFAR100 and Caltech-256, an L2 regularization term of 5e-4 was used. We used the
Adam optimizer. The initial learning rate is 1 × 10-3, β1 is 0.9 and β2 is 0.999. The number of
epochs is 50. The batch size is 512.
Meta-Training We use the cross-entropy loss as in (Amit & Meir, 2018). Although this is inconsis-
tent with the bounded loss setting in our theoretical framework, we can still have a guarantee on a
variation of the loss which is clipped to [0, 1]. In practice, the loss is almost always smaller than one.
For CIFAR100 and Caltech-256, the number of epochs of meta-training phase is 12; the number of
epochs of meta-testing phase is 40. The batch size is 32 for both datasets. As optimizer we used
Adam with β1 = 0.9 and β2 = 0.999. In the setting with a pre-trained base model, the learning rate
is 1 × 10-5 for convolutional layers and 5 × 10-4 for the linear output layer. In the setting without
a pre-trained base model, the learning rate is 1 × 10-3 for convolutional layers and 5 × 10-3 for the
linear output layer. The confidence parameter is chosen to be δ = 0.1. The variance hyper-parameter
for prior predictor and base model are σw = σv = 0.01. The hyperparameter α1, α2 in LML and
ML-A are set to 0.01. For MAML (Finn et al., 2017) and MatchingNet (Vinyals et al., 2016). Both
two methods use the Adam optimizer with initial learning rate 0.0001. In the meta-training phase,
we randomly split the samples of each class into support set (5 samples) and query set (45 samples).
The number of epochs is 100. For MAML, the learning rate of inner update is 0.01.
22