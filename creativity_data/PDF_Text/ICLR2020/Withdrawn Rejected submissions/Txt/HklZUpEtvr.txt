Under review as a conference paper at ICLR 2020
Optimal Transport, CycleGAN, and Penalized
LS for Unsupervised Learning in Inverse Prob-
LEMS
Anonymous authors
Paper under double-blind review
Ab stract
The penalized least squares (PLS) is a classic method to inverse problems, where
a regularization term is added to stabilize the solution. Optimal transport (OT) is
another mathematical framework for computer vision tasks that provides means
to transport one measure to another at minimal cost. The cycle-consistent genera-
tive adversarial network (cycleGAN) is a recent extension of GAN to learn target
distributions with less mode collapsing behavior. Although similar in that no su-
pervised training is required, the algorithms look different, so the mathematical
relationship between these approaches is not clear. In this article, we provide an
important advance to unveil the missing link. Specifically, we reveal that a cy-
cleGAN architecture can be derived as a dual formulation of the OT problem, if
the consistency constraint of PLS is enforced as a regularization term of the OT
problem. This suggests that cycleGAN can be considered stochastic generaliza-
tion of classical PLS approaches. Our derivation is so general that various types
of cycleGAN architectures can be easily derived by merely changing the transport
cost. As proofs of concept, this paper provides a novel cycleGAN architecture for
unsupervised learning in accelerated magnetic resonance imaging (MRI) and de-
convolution microscopy problems, which confirm the efficacy and the flexibility
of the theory.
1 Introduction
Inverse problems are ubiquitous in computer vision, biomedical imaging, scientific discovery, etc.
In inverse problems, a noisy measurement y ∈ Y from an unobserved image x ∈ X is modeled by
y =	Hx +w ,
(1)
where w is the measurement noise, and H : X 7→ Y is the measurement operator. In inverse
problems originating from physics, the measurement operator is usually represented by an integral
equation:
Hx(r)
h(r, r0)x(r0)dr0,
r ∈ D ⊂ Rd,	d = 2, 3,
(2)
where h(r, r0; x) is an integral kernel. Then, the inverse problem is formulated as an estimation
problem of the unknown x from the measurement y. It is well known that inverse problems are
ill-posed. A classical strategy to mitigate the ill-posedness is the penalized least squares (PLS)
approach:
X = arg min c(x; y) := ∣∣y — Hxkq + R(X)
x
(3)
for q ≥ 1, where R(x) is a regularization (or penalty) function (l1, total variation (TV), etc.) (Chaud-
huri et al., 2014; Sarder & Nehorai, 2006; McNally et al., 1999). In some inverse problems, the mea-
surement operator H is not well defined, so both the unknown operator H and the image x should
be estimated.
Recently, deep learning approaches with supervised training have become the mainstream ap-
proaches for inverse problems because of their excellent and ultra-fast reconstruction performance.
1
Under review as a conference paper at ICLR 2020
For example, in low-dose x-ray computed tomography (CT) denoising problems, a convolutional
neural network (CNN) is trained to learn the relationship between the noisy image y and the matched
noiseless (or high-dose) label images x (Kang et al., 2017). In the context of (3), the supervised
neural network can be understood as directly learning the operation X = arg minx c(χ; y). Unfortu-
nately, in many applications, matched label data are not available. Therefore, unsupervised training
without matched reference data has become an important research topic.
Recently, the generative adversarial network (GAN) has attracted significant attention in the machine
learning community by providing away to generate target data distribution from random distribution
(Goodfellow et al., 2014). In particular, Arjovsky et al. (2017) proposed the so-called Wasserstein
GAN (W-GAN), which is closely related to the mathematical theory of optimal transport (OT) (Vil-
lani, 2008; Peyre et al., 2019). In OT, for two given probability measures supported on the X and
Y spaces, one pays a cost for transporting one measure to another. Then, the minimization of the
average transportation cost provides an unsupervised way of learning the transport map between
the two measures. Unfortunately, these GAN approaches often generate artificial features due to
mode collapsing, so cycle-consistent GAN (cycleGAN) (Zhu et al., 2017), which imposes one-to-
one correspondence, has been extensively investigated (Kang et al., 2019; Lu et al., 2017). Recently,
cycleGAN was generalized to bridge two different domains by generating a continuous sequence of
intermediate domains flowing from one domain to the other (Gong et al., 2019).
Although classical PLS, OT, and cycleGAN share the commonality of unsupervised learning which
does not require matched training data, there is no mathematical theory to systematically link these
seemingly different approaches. Therefore, one of the main contributions of this paper is to unveil
the missing link between these methods. In particular, we reveal that a cycleGAN architecture can
be derived as a dual formulation of the OT problem when the consistency term in PLS is enforced as
a regularization term for the OT problem. Moreover, this framework is so general that various cycle-
GAN architectures can be easily obtained by changing the PLS cost. Accordingly, our framework
provides a principled way of deriving unsupervised neural networks for various inverse problems.
As proofs of concept, we provide novel cycleGAN architectures for unsupervised learning in two
physical inverse problems: accelerated MRI and deconvolution microscopy. In these applications,
we show that only one CNN generator is required, which significantly reduces the training com-
plexity. The experimental results confirm that the proposed unsupervised learning approaches can
successfully provide accurate inversion results without any matched reference. All the proofs of the
main theoretical results can be found in the Appendix.
2	Related Works
2.1	Optimal Transport (OT)
OT compares two measures in a Lagrangian framework (Villani, 2008; Peyre et al., 2019). Formally,
we say that T : X → Y transports μ ∈ P(X) to V ∈ P (Y), if
V(B) = μ (TT(B)), for all V-measurable sets B,	(4)
which is often simply represented by V = 7#仙，where T# is often called the push-forward operator.
Monge,s original OT problem (Villani, 2008; Peyre et al., 2019) is then to find a transport map T
that transports μ to V at the minimum total transportation cost:
rnn M(T):= JX c(x,T(x))dμ(x), subject to V = T#@
However, this is usually computationally expensive due to the nature of combinatorial assignment.
Kantorovich relaxed the assumption to consider probabilistic transport that allows mass splitting
from a source toward several targets (Villani, 2008; Peyre et al., 2019). Specifically, Kantorovich
introduced a joint measure π ∈ P(X × Y) and the associated cost c(x, y), x ∈ X, y ∈ Y such that
the original problem can be relaxed as
mπin	K(π) := RX×Y c(x, y)dπ(x, y)	(5)
subject to π(A XY)= μ(A), π(X X B) = V(B)
for all measurable sets A ∈ X and B ∈ Y. Here, the last two constraints come from the observation
that the total amount of mass removed from any measurable set has to equal the marginals (Villani,
2
Under review as a conference paper at ICLR 2020
2008; Peyre et al., 2019). Another important advantage of Kantorovich formulation is the dual
formulation as stated in the following theorem:
Theorem 1 (Kantorovich duality theorem). (Vllani, 2008, Theorem 5.10, p.57-p.59) Let (X, μ) and
(Y , ν ) be two Polish probability spaces (separable complete metric space) and let c : X × Y → R
be a continuous cost function, such that ∣c(x,y)∣ ≤ CX (x) + CY (y) for some CX ∈ L1(μ) and
CY ∈ L1(ν), where L1(μ) denotes the set of 1-Lipschitz functions with the measure μ. Then, there
is duality:
min ［	c(x,y)dπ(x,y) = max ｛［夕(x)dμ(x)+ ［夕c(y)dν
ππ(μ,ν) JxYy	w∈L1W)I JX	JY
and the above maximum is taken over the so-called Kantorovich potential φ whose c-transform
φc(y) = SuPx(C(x,y) 一 夕(x)) is properly defined.
2.2	PLS with Deep Learning Prior
Recently, PLS frameworks using a deep learning prior have been extensively studied (Zhang et al.,
2017; Aggarwal et al., 2018) thanks to their similarities to the classical regularization theory. The
main idea of these approaches is to utilize a pre-trained neural network to stabilize the inverse
solution. For example, in model based deep learning architecture (MoDL) (Aggarwal et al., 2018),
the problem can be formulated as
min C(x; y, Θ, H) = ky 一 Hxk2 + λkx 一 QΘ(x)k2	(6)
x
for some regularization parameter λ > 0, where QΘ (x) is a pre-trained CNN with the network
parameter Θ and the input x. This problem is usually solved in an alternating minimization frame-
work:
xn+1 = arg min ky 一 Hxk2 +λkx 一 znk2 , zn+1 = QΘ(xn+1)	(7)
x
Another type of inversion approach using a deep learning prior is the so-called deep image prior
(DIP) (Ulyanov et al., 2018). Rather than using an explicit prior, the deep neural network architec-
ture itself is used as a regularization by restricting the solution space:
min C(Θ; y, H) = ky 一 HQΘ(z)k2	(8)
Θ
where Z is a random vector. Then, the final solution becomes X = Qθ*(z) with Θ* being the
estimated network parameters. Similarly, generative approaches (Van Veen et al., 2018; Bora et al.,
2017; Wu et al., 2019) either estimate the random variable z from (8) by fixing Θ or attempt to
estimate both the random z and the network weight Θ.
3	Main Contributions
One of the basic assumptions in the aforementioned PLS formulation with a deep learning prior
is that the measurement y is fixed and one is interested in finding the unknown x. In this paper,
we relax the assumption to consider situations where both of them are random samples from joint
probability measure π(x, y). Specifically, we propose a new PLS cost function with a novel deep
learning prior as follows:
C(x, y; Θ, H) = ky 一 Hxkq + λkGΘ(y) 一 xkp	(9)
with p, q ≥ 1, where the first two terms before the semi-colon are the random vectors. Here, the
regularization parameter λ takes care of the dimensional difference of x and y, with λ = 1 for the
same dimension, but for simplicity, in the rest of the paper, we assume λ = 1.
The new PLS cost in (9) has very unique properties. Suppose that the global minimum with
C(x, y; Θ, H) = 0 exists. The corresponding global minimizer should then satisfy
y = Hx, x = GΘ (y)	(10)
If this holds for all y, then GΘ is an inverse operator ofH. In practice, due to the limited capacity of
the neural network, the global minimum with C(x, y; Θ, H) = 0 may never be achieved. Even in this
3
Under review as a conference paper at ICLR 2020
case, thanks to the symmetric form of PLS, the cost in (9) has an important implication: the neural
network GΘ is now estimated by enforcing the consistency y = Hx as a regularization term. This
implies that the roles of the consistency and regularization terms in the PLS can be interchanged
in our formulation. Another important advantage is that the unknown image x can be obtained as
an output of the feedforward neural network GΘ (y) for the given measurement y. This makes the
inversion procedure much simpler. One may say that the PLS cost function for the DIP in (8) can
produce a different variation of unsupervised learning. However, the resulting formulation is not a
feed-forward neural network. In fact, to obtain the unknown image x, an additional optimization
step is required even after the neural network is trained. See Appendix C for more details.
Note that Θ, H are the network and measurement system parameters that should be estimated. These
parameters can be found by minimizing the average transport cost for all combinations of x ∈ X
and y ∈ Y with respect to the joint measure π(x, y):
T(Θ, H) := min	c(x, y; Θ, H)dπ(x, y)
(11)
where the minimum is taken over all joint distributions whose marginal distributions with respect to
X and Y are μ and V, respectively. Below, We show that the average transportation cost in (11) has
an interesting decomposition, which leads to the cycleGAN architecture.
Lemma 1. If the mapping GΘ : Y 7→ X is single-valued, then the average transportation cost
T(Θ, H) in (11) can be decomposed as
where
T(Θ, H)	= 'cycle(Θ, H)+ 'ot 0 (Θ, H)
'cycle(Θ, H) = mπin ʌ	()
c(x, y; Θ, H)dπ(x, y)
'ot0 (Θ, H) = min /	c(x,y; Θ, H)dπ(x,y)
π √X×γ∖Γ(Gθ)UΓ(H)
(12)
(13)
(14)
and Γ(GΘ), Γ(H) denote the graphs of GΘ, H, respectively:
Γ(Gθ) ：= {(χ,y) ∈X×Y∣ X = Gθ(y), y ∈Y},	Γ(H) := {(χ,y) ∈X×Y∣ y = Hχ,χ ∈X}
Note the nature of the sets Γ(Gθ) and Γ(H) and 'cycle. Since both Γ(Gθ) and Γ(H) are graphs of
functions, they are low-dimensional manifolds embedded in the ambient space X × Y. In this sense,
their relative measures become zero and 'cycle seems to be a negligible part of the whole T. This
term, however, can contribute a significant portion of the total loss T depending on the existence of
the singularity in the distribution. We defer the discussion to after the following proposition, which
explains why we name the first term 'cycle .
Proposition 1.
'cycle(Θ, H)
mπin Γ(GΘ )UΓ(H)
c(x, y; Θ, H)dπ(x, y)
/
Γ(GΘ)UΓ(H)
c(x, y; Θ, H)π* (dx, dy)
x — Gθ(Hx)kpρ(x)μ(dx) +
ky
Y
— HGΘ (y)kqσ(y)ν(dy)
(15)
k
X
where π* denotes the optimum measure and P and σ are measurablefUnctions such that 0 ≤ ρ(x) ≤
1 μ-a.e., and 0 ≤ σ(y) ≤ 1 V-a.e.
The remaining term in (12), which corresponds to the GAN term, can be obtained as follows:
Proposition 2. Forp = q = 1, the cost 'OT 0 (Θ, H) in (14) can be computed as
'ot0(Θ,H) = max [2(x)[1 — ρ(x)]dμ(x) — / 夕(Gθ(y))[1 — σ(y)]dν(y)
W JX	JY
+ max ψ ψ(y)[1 — σ(y)]dν(y) — ψ ψ(Hx)[1 — ρ(x)]dμ(x)
ψY	X
(16)
(17)
where φ and ψ are I-LiPschitz Kantorovich potential functions.
4
Under review as a conference paper at ICLR 2020
Figure 1: (a) Supervised learning with matched references, (b) OT with absolute continuous mea-
sures, and (c) OT having measures with singularities. CycleGAN corresponds to scenario (c).
The proof of Propositions 1 and 2 can be found in Appendix B. The proof idea comes from the
computation of c(x, y; Θ, H) on subsets Γ(GΘ), Γ(H), and Γ(GΘ) ∩ Γ(H) by considering the sin-
gularity of the optimum measure π* on these sets. In this regard, it is important to know how
singularity of π* is distributed on these manifolds. If the targets are perfectly matched as shown in
Fig. 1(a), σ(y) := π*({x = Gθ(y)}∣y) = 1; V-a.e so that ∏*(∙∣y) is distributed like a point mass
of magnitude σ(y) on {x = Gθ(y)}. This also leads to P(X) := π*({y = Hx}∣x) = 1; μ-a.e.
In this case, 'OT， = 0 and Gθ (y) is a perfect inverse to compute x. On the other hand, if the
joint distribution π(x, y) is absolutely continuous as shown in Fig. 1(b), then P(X) = 0; μ-a.e. and
σ(y) = 0; v-a.e. In this case, 'cycle = 0 and the contribution of the cycle consistency term can be
neglected. The most interesting case arises when the joint measure π(X, y) has significant singular-
ities on the manifold Γ(Gθ) ∪ Γ(H) so that 0 < σ(y) = π*({x = Gθ(y)}∣y) < 1 (see Fig. 1(c)).
In fact, unsupervised learning applies to such situations where once the mappings are found, a sig-
nificant portion of the data in X and Y can be paired with the mappings, even though unpaired data
remain. This is when the cycle consistency terms plays an important role in unsupervised learning.
Then, GΘ (y) is a desirable approximation ofX with high probability.
Now, the final step is implementing the Kantorovich potential using CNNs with parameters Φ and
Ξ, i.e. φ :=(φ and ψ := Ψξ. By collecting all terms in the transportation cost T(Θ, H) , the dual
formulation results in a cycleGAN cost function:
min T(Θ, H) =minmax '(Θ, Hf, Ξ)	(18)
where
'(Θ, H; Φ, Ξ) = 'cycle(Θ, H) + 'gan(Θ, H; Φ, Ξ)	(19)
where 'cycle(Θ, H) denotes the cycle-consistency loss in (15) and 'GAN(Θ, h; Φ, Ξ) is the GAN
loss given by:
'gan(Θ, H∙,Φ, Ξ) = / 夕φ(x)[1 - ρ(x)]dμ(x) - / 夕φ(Gθ(y))[1 — σ(y)]dv(y)	(20)
XY
+ JyψΞ(y)[1 - σ(y)]dv(y)-[旭(HX)I- P(X)]d"(X)
Here, the Kantorovich 1-Lipschitz potentials φ :=(φ and ψ := Ψξ correspond to the W-GAN dis-
criminators. Specifically, φφ tries to find the difference between the true image X and the generated
image GΘ (y), whereas ψ := ψΞ attempts to find the fake measurement data that are generated by
the synthetic measurement procedure HX. Furthermore, if P and σ are constant almost everywhere,
they become simple scaling factors for the cycle-consistency terms:
'cycle(Θ, H)
P/ ∣∣X - Gθ(Hx)kpdμ(X) + σ /
ky - HGΘ(y)kqdν(y)
(21)
Note that our formulation using (19) with (20) and (21) has many unique features compared to the
standard cycleGAN (Zhu et al., 2017).
•	The standard cycleGAN (Zhu et al., 2017) usually requires two generators in the form of
deep neural networks. On the other hand, our formulation often requires a single generator
using a deep neural network, if the other generator is given by the forward model HX.
This significantly reduces the number of weights, which makes the training much more
stable. This can be viewed as the consistency term from the forward model working as a
regularization term for OT.
5
Under review as a conference paper at ICLR 2020
•	When the solution of the dual problem achieves the global minimum with diminishing cost,
then the dual solution x = GΘ (y) is equivalent to the primal solution (10), validating the
dual approach. This is not the case in the conventional cycleGAN with two deep generators.
•	While We only consider P = q = 1 due to the simple c-transform Sc(X) = -φ(x) for
I-LiPSChitZ φ, the use of the general PLS cost would be interesting, and it may lead to an
interesting variation of the cycleGAN architecture. This could be done using a regularized
version of OT (Peyre et al., 2019).
•	The parameters ρ(x) and σ(y) in (1) and (20) originate from the singularity of the optimal
joint measure. However, the estimation of ρ(x) and σ(y) is not feasible in practice, so
the constant hyperparameters in (21) are usually used, and the hyper parameters should be
selected by trial and error, which is the same as in the standard cycleGAN. Nevertheless, the
probability interpretation may lead to a more simplified search for the hyper-parameters.
4 Applications to Inverse Problems
4.1	Accelerated MRI
In accelerated magnetic resonance imaging (MRI), the goal is to recover high-quality MR images
from sparsely sampled k-space data to reduce the acquisition time. This problem has been exten-
sively studied using compressed sensing (Lustig et al., 2007), but recently, deep learning approaches
have been the main research interest due to their excellent performance and significantly reduced
run-time complexity (Hammernik et al., 2018; Han & Ye, 2019). A standard method for neural net-
work training for accelerated MRI is based on supervised learning, where the MR images from fully
sampled k-space data are used as references and subsampled k-space data are used as the input for
the neural network. Unfortunately, in accelerated MRI, high-resolution fully sampled k-space data
are very difficult to acquire due to the long scan time. Therefore, the need for unsupervised learning
without matched reference data is increasing.
In accelerated MRI, the forward measurement model can be described as
X = Pω F x + W	(22)
where F is the 2-D Fourier transform and Pω is the projection to Ω that denotes k-space sampling in-
dices. To implement every step of the algorithm as image domain processing, (22) can be converted
to the image domain forward model by applying the inverse Fourier transform:
y = F T Pω F x + F T W	(23)
This results in the following cost function for the PLS formulation:
c(x, y; Θ) = ky - FTPωFxk + ∣∣Gθ(y) - xk	(24)
where we set λ = 1 in (9) since the dimensions ofx and y are the same. Usually, the sampling mask
Ω is known so that the forward mapping for the inverse problem is deterministic.
The schematic diagram of the associated cycleGAN architecture is illustrated in Fig. 2, whose gen-
erator and discriminator architectures are shown in Fig. 6 in Appendix D. Note that we just need a
Figure 2: Proposed cycleGAN architecture for accelerated MRI with 1-D downsampling patterns.
6
Under review as a conference paper at ICLR 2020
Table 1: Quantitative comparison for various algorithms on 500 test sets of fastMRI data.
Metric	Input (×4)	Supervised Learning	Conventional CycleGAN	Proposed Method
PSNR (dB)	26.8056	27.9609	25.0092	28.1704
SSIM	06419	0.6419	0.4689	0.6550
single generator and discriminator, as the mapping from the clean to aliased images is deterministic
for a given sampling pattern. As for the loss function, we use (19) with the σ = ρ = 0.5 for the cycle
consistency term in (21). For GAN implementation, we use the W-GAN in (20) with the Lipschitz
penalty loss (Gulrajani et al., 2017) to ensure that the Kantorovich potential becomes 1-Lipschitz.
The detailed description of the training procedure is provided in Appendix D. The reconstruction
results in Fig. 3 and quantitative comparison results for all test sets in Table 1 clearly show that the
proposed unsupervised learning method using dedicated cycleGAN successfully recovered fine de-
tails without matched references. Moreover, the performance is even better than that of the standard
cycleGAN and is comparable with that of supervised learning.
4.2 Deconvolution Microscopy
Deconvolution microscopy is extensively used to improve the resolution of widefield fluorescent
microscopy. However, classical deconvolution approaches require the measurement or estimation
of the point spread function (PSF) and are usually computationally expensive. Recently, CNN ap-
proaches have been extensively studied as fast and high-performance alternatives. Unfortunately, the
CNN approaches usually require matched high-resolution images for supervised training. Therefore,
we are interested in developing unsupervised deep neural networks for deconvolution microscopy.
Mathematically, a blurred measurement can be described as
y = h * X + w ,
(25)
where h is the PSF. Here, we consider the blind deconvolution problem where both the unknown
PSF h and the image x should be estimated. This results in the following cost function for the PLS
Figure 3:	Unsupervised learning results for accelerated MRI using pro-
posed cycleGAN. The values in the corners are PSNR/SSIM values for
each image. Here, PSNR =	20 log 10 (n∣∣x*k∞∕kx — x*∣∣2) and SSIM =
(2μxμχ* + cι)(2σχχ* + c2)∕(μX + μX* + cι)(σ2 + σX* + c2), where X and x* denote the
reconstructed images and ground truth, respectively, n is the number of pixels, μχ is an average
of x, σX is a variance of X and σχχ* is a covariance of X and x*, and c1,c2 are two variables to
stabilize the division.
7
Under review as a conference paper at ICLR 2020
formulation:
c(x,y[Θ,h) = ky - h * Xk + ∣∣Gθ(U) - x∣∣.	(26)
The corresponding cycleGAN architecture is illustrated in Fig. 4. In contrast to the conventional
cycleGAN approaches that require two generators, the proposed cycleGAN approach needs only a
single generator, and the blur image can be generated using a linear convolution layer corresponding
to the PSF, which significantly improves the robustness of network training. However, we still
need two discriminators, since both linear and deep generators should be estimated. The detailed
descriptions of the network architecture and training are given in Appendix D.
Figure 4: Proposed cycleGAN architecture with a blur kernel for deconvolution microscopy. Here,
GΘ denotes the CNN-based low-resolution to high-resolution generator. The blur generator is com-
posed of a linear blur kernel h.
Fig. 5 shows lateral views of the deconvolution results of microtubule samples by various meth-
ods. Here, input images are degraded by blur and noise. The supervised learning and the standard
cycleGAN with two generators showed better contrast and removed blur; however, the structural
continuity was not preserved. On the other hand, in our cycleGAN approach, blur and noise were
successfully removed, and the continuity of the structure was preserved. The quantitative compari-
son with known ground-truth data is provided in Appendix D, which clearly confirms the superiority
of the proposed method,
{a} Measurement (b) Classical deconvolution
(c) Supervised learning
(d) Conventional cycleGAN (e) Proposed cycleGAN
Figure 5: Comparison of reconstruction results by various methods: (a) blurred image measure-
ments, (b) classical deconvolution method, (c) supervised learning, (d) conventional cycleGAN, and
(e) proposed cycleGAN. The regions of interest (marked yellow) show enlargements of the areas
marked in yellow.
5 Conclusions
In this paper, we presented a general design principle for a cycleGAN architecture for various in-
verse problems. Specifically, the proposed architecture was derived as a dual formulation of an OT
problem that enforces the data consistency of PLS as a regularization term for the OT problem. As
proofs of concept, we designed novel cycleGAN architectures for accelerated MRI and deconvolu-
tion microscopy examples, providing accurate reconstruction results without any matched reference
data. Given the generality of our design principle, we believe that our method can be an important
platform for unsupervised learning for inverse problems.
8
Under review as a conference paper at ICLR 2020
References
Hemant K Aggarwal, Merry P Mani, and Mathews Jacob. MoDL: Model-based deep learning
architecture for inverse problems. IEEE transactions on medical imaging, 38(2):394-405, 2018.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 537-546. JMLR. org, 2017.
Subhasis Chaudhuri, Rajbabu Velmurugan, and Renu Rameshan. Blind deconvolution methods: A
review. In Blind Image Deconvolution, pp. 37-60. Springer, 2014.
Ozgun Cicek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D U-
Net: learning dense volumetric segmentation from sparse annotation. In International conference
on medical image computing and computer-assisted intervention, pp. 424-432. Springer, 2016.
Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. DLOW: domain flow for adaptation and gener-
alization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2477-2486, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht, Daniel K Sodickson, Thomas
Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI
data. Magnetic resonance in medicine, 79(6):3055-3071, 2018.
Yoseob Han and Jong Chul Ye. k-Space Deep Learning for Accelerated MRI. IEEE Transactions
on Medical Imaging (in press), also available as arXiv preprint arXiv:1805.03779, 2019.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Eunhee Kang, Junhong Min, and Jong Chul Ye. A deep convolutional neural network using direc-
tional wavelets for low-dose X-ray CT reconstruction. Medical physics, 44(10), 2017.
Eunhee Kang, Hyun Jung Koo, Dong Hyun Yang, Joon Bum Seo, and Jong Chul Ye. Cycle-
consistent adversarial denoising network for multiphase coronary CT angiography. Medical
physics, 46(2):550-562, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Hagai Kirshner, Daniel Sage, and Michael Unser. 3D PSF models for fluorescence microscopy in
ImageJ. In Proceedings of the Twelfth International Conference on Methods and Applications of
Fluorescence Spectroscopy, Imaging and Probes (MAF’11), volume 154, 2011.
Yongyi Lu, Yu-Wing Tai, and Chi-Keung Tang. Conditional cyclegan for attribute guided face image
generation. arXiv preprint arXiv:1705.09966, 2017.
Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The application of compressed
sensing for rapid MR imaging. Magn. Reson. Med., 58(6):1182-1195, 2007.
James G McNally, Tatiana Karpova, John Cooper, and Jose Angel Conchello. Three-dimensional
imaging by deconvolution microscopy. Methods, 19(3):373-385, 1999.
9
Under review as a conference paper at ICLR 2020
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends in
Machine Learning,11(5-6):355-607, 2019.
Daniel Sage, LaUrene Donati, Ferreol Soulez, Denis Fortun, Guillaume Schmit, Arne Seitz, Romain
Guiet, Cedric Vonesch, and Michael Unser. DeconvolutionLab2: An open-source software for
deconvolution microscopy. Methods, 115:28-41, 2017.
Pinaki Sarder and Arye Nehorai. Deconvolution methods for 3-D fluorescence microscopy images.
IEEE Signal Processing Magazine, 23(3):32-45, 2006.
David Simmons. Conditional measures and conditional expectation; rohlin’s disintegration theorem.
Discrete & Continuous Dynamical Systems-A, 32(7):2565-2582, 2012.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 9446-9454, 2018.
Dave Van Veen, Ajil Jalal, Mahdi Soltanolkotabi, Eric Price, Sriram Vishwanath, and Alexandros G
Dimakis. Compressed sensing with deep image prior and learned regularization. arXiv preprint
arXiv:1806.06438, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Yan Wu, Mihaela Rosca, and Timothy Lillicrap. Deep compressed sensing. arXiv preprint
arXiv:1905.06723, 2019.
Jure Zbontar, Florian Knoll, Anuroop Sriram, Matthew J Muckley, Mary Bruno, Aaron Defazio,
Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, et al. fastMRI: An open
dataset and benchmarks for accelerated MRI. arXiv preprint arXiv:1811.08839, 2018.
Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep CNN denoiser prior
for image restoration. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3929-3938, 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223-2232, 2017.
A Additional Theorems and Lemma
Theorem 2 (Optimality is inherited by restriction). (Villani, 2008, Theorem 4.6, p.46) Under the
same assumption of Kantorovich duality theorem, let π* be an optimal transport plan, and π be a
nonnegative measure on XXY, such that π ≤ π* and π[X XY] > 0. Let π0 := ∏χ×γ], μ0, ν0 be
the maginals of π0. Then optimality is inherited to its restriction, i.e., π0 is an optimal transference
plan between μ0 and ν0.
Theorem 3 (Restriction for the Kantorovich duality). (Villani, 2008, Theorem 5.19, p.75-p.76) Un-
der the same assumption OfKantorovich duality theorem, let π* be an optimal transport plan and
夕* be an optimal potential that attain the maximum, and π be a measure on XXY, such that
0 ≤ π ≤ π* and Z := π[X × Y] > 0. Let π0 = π∕Z, and μ0,ν0 be the marginals of π0. Then
there exists 夕0 such that 夕0 = 夕" onprojX(supp(π*)), and 夕0 solves the dual KantOrOviCh problem
between (X, μ0) and (Y, ν0).
We are now ready to provide our duality result for the restricted measure.
Lemma 2. Let
'ot 0
'ot
min	c(x, y)dπ (x, y)
min	c(x, y)dπ(x, y)
10
Under review as a conference paper at ICLR 2020
Then, we can replace 'ot0 with 'ot in the sense that both of them share the same optimal Kan-
torovich potential. More specifically, if 夕* is the optimal Kantorovich potential, which satisfies
'ot =[g"(χ)dμ(χ)+ /(2*)c(y)dν(y),
XY
then
'ot 0 =
( ^*(x)[1 - ρ(χ)]dμ(χ)+ ( (ψ,-')c(y)∖1 - σ(y)]dν(y).
XY
Proof. This is a corollary of Theorem 2 and 3 by taking π = ∏*∣x×y∖(γ(Gθ)∪γ(H)). Then, ∏ is
nonnegative measure on XXY. Use the same notation for Z, μ0, V0 in Theorem 2 and 3. When
Z = 0 , 'ot0 = 0, and μ0 = 0,ν0 = 0, so the statement trivially holds. Suppose Z > 0. Then
by Theorem 3, there exists 夕0 such that 夕0 = 夕" on projχ(supp(∏*)), and 夕0 solves the dual
Kantorovich problem between (X, μ0) and (Y, ν0).
`ot0/z = /	c(x,y；a H) dπ(x'y)
X×Y	Z
[夕*(x)dμ0(x)+ [
XY
Wmy
The remaining part of the proof is to show the relationship that μ0 = 1Zρ μ and V0 = 1Zσ V. From
the proof of Proposition 1, the marginals of ∏* ∣γ(Gθ)∪γ(H) are ρμ and σν. Therefore, the marginals
of π = π* — ∏*∣γ(Gθ)∪γ(H) are μ — ρμ and V — σν so that Zμ0 = (1 — ρ)μ and ZV0 = (1 — σ)ν.
：. 'OT0
/ 2*(χ)[1 - ρ(χ)]dμ(χ)+ /(2*)c(y)[1- o(y)]dV(y)
XY
□
B Proof of Main Results
B.1 Proof of Lemma 1
Proof. From Theorem 2, we extract information about two separated integrals. For some optimal
transportation plan ∏*, the restrictions ∏* ∣γ(Gθ)∪γ(H) and ∏*∣x×y∖(γ(Gθ)∪γ(H)) acquire optimality.
Therefore,
T(Θ, H) = (	c(x, y; Θ, H)dπ*(x, y)
X×Y
=/	c(x, y; Θ, H)dπ*(x, y) + /	c(x,y[Θ, H)dπ* (x,y)
A ABB	J X×Y∖(A∪B)
='cycle(Θ, H)+ 'OT 0 (Θ, H)
This concludes the proof.
□
B.2 Proof of Proposition 1
Proof. By virtue of the definitions of the cost function and the sets Γ(GΘ) and Γ(H), we have
c(x, y; Θ, H) = ky - HGΘ(y)kq on Γ(GΘ),
c(x, y; Θ, H) = kx - GΘ(Hx)kp on Γ(H),
c(x, y; Θ, H) = 0	on Γ(GΘ) ∩ Γ(H).
Hence,
/ I	c(χ,y; Θ, H)d∏*(x,y) = /	∣∣χ-Gθ(Hχ)kpd∏*(χ,y)+/	ky-HGθ (y)kq d∏*(χ,y)
11
Under review as a conference paper at ICLR 2020
Furthermore, We use disintegration theorem (Simmons, 2012) to split the joint measure ∏ as fol-
lows:
dπ*(x,y) = π* (dy∣x)μ(dx) = π*(dx∣y)ν (dy),
where ∏*(Y∣x) = 1 μ-a.e. and ∏*(X∣y) = 1 ν-a.e. Then, we have
J ∣∣x — Gθ(Hx)kpd∏*(x, y) = J ∣∣x — Gθ(Hx)kpπ*(dy∣x)μ(dx)
=[[	∣∣χ — Gθ(Hχ)∣p∏*(dy∣χ)μ(dχ)
X y=Hx
=/ ∣∣x — Gθ(Hx)kp/ 1y=Hχ∏*(dy∣x)μ(dx)
=/ ∣∣x — Gθ(Hx)∣pρ(x)μ(dx)
X
where 1s denotes the indicator function for the set S and ρ(χ) = J 1y=Hx∏*(dy∣χ). In a similar
fashion, with σ(y) = / 1χ=Gθ(y)∏^ (dx|y), we have
C	c(x,y;Θ, H)dπ*(x,y) = ∣ ∣y — HGθ(y)∣qσ(y)ν(dy).
Γ(GΘ)	Y
This concludes the proof.
□
B.3 Proof of Proposition 2
Proof. This is a just simple corollary of the original Kantorovich’s duality formulation proof and
the classical results of optimal transport on the restricted measure. We start with the first term of
lOT (Θ, H) in (14). Specifically, we have
∣	∣∣Gθ(y) — x∣∣p∏*(dx,dy) =max / 2(x)μ0(dx)+ [夕c(Gθ(y))ν0(dy)
√X×Y∖(Γ(Gθ)∪Γ(H))	W JX	JY
where μ0 and V are marginals of the restriction of optimal transportation plan on the restricted set
X × Y \ (Γ(GΘ) ∪ Γ(H)). Now, using Lemma 2 in Appendix, we have
/	、(( C (	∣Gθ(y) — χ∣∣∏*(dχ,dy)
=max / 2(x)[1— ρ(x)]μ(dx) + I φc(Gθ(y))[1 — σ(y)]ν(dy)
WX	Y
=Wmax) (0(X)[1 — ρ(x)]μ(dx)— ∕hgθ(J))[1 — σ(y)]ν (M)
for the non-restricted marginals μ and V, where for the last equality we use 夕C = 一夕 for P = 1
when 夕 is 1-Lipschitz (Villani, 2008). Using the same technique, the second term in (14) can be
computed as
/
∕x×Y∖(Γ(Gθ)∪Γ(H))
ky - Hxkdπ(x, y)
= max ψ ψ(y)[1 — σ(y)]dν(y) — ψ ψ(Hx)[1 — ρ(x)]dμ(x).
ψ∈L1 (ν) Y	X
By collecting terms, We conclude the proof.
□
C	Other Variants of Unsupervised Learning
Here, we derive the dual optimal transport formulation when the PLS cost with deep learning prior
as in (8) is used as the transportation cost for the OT problem. Specifically, the transportation cost
of the OT problem with respect to the DIP cost in (8) becomes
min c(y, z; Θ, H) = ∣y — HQΘ (z)∣	(27)
Θ
12
Under review as a conference paper at ICLR 2020
where we consider y and z as random variable and we use non-squared norm for simplicity. Then,
the corresponding OT problem can be dualized as follows:
min min	ky - HGΘ(z)kdπ(x, z) = min max	ψ(y)dν(y) -	ψ(HGΘ(z))dη(z).
Θ π X×Z	Θ ψ∈L1(ν) Y	X
Although this is a nice way of pretraining deep image prior model using unmatched training data
set, the final image estimate still comes from the following optimization problem:
X = Gθ (z *) where Z * = arg min ∣∣y -HGθ*(z )k
z
where Θ* is the estimated network parameters from previous training step. This is equivalent to
deep generator model (Bora et al., 2017). Therefore, this is not useful in designing a feed-forward
neural network in an unsupervised manner.
D Experimental Details
D.1 Accelerated MRI
We use dataset for fastMRI challenge (Zbontar et al., 2018) for our experiments. This dataset is com-
posed of MR images of knees. We extracted 3500 MR images from fastMRI single coil validation
set. Then, 3000 slices are used for training/evaluation, and 500 slices are used for test. These MR
images are fully sampled images, so we make undersampled images by a randomly subsampling
k-space lines. The acceleration factor is four, and autocalibration signal (ACS) region contains 4%
of k-space lines. Each slice is normalized by standard deviation of the magnitude of each slice. To
handle complex values of data, we concatenate real and imaginary values along the channel dimen-
sion. Each slice has different size, so we use only single batch. The images are center-cropped to
the size of 320 × 320, and then the peak signal-to-noise ratio (PSNR) and structural similarity index
(SSIM) values are calculated.
(a)
(b)
Figure 6: Proposed network architecture for (a) generator and (b) discriminator in accelerated MRI.
We use U-Net generator to reconstruct fully sampled MR images from undersampled MR images as
shown in Fig. 6(a). Our generator consists of 3 × 3 convolution, Instance normalization, and leaky
ReLU operation. Also, there are skip-connection and pooling layers. At the last convolution layer,
we do not use any operation. Our discriminator is same as the discriminator of original CycleGAN.
We use PatchGAN (Isola et al., 2017), so the discriminator classifies inputs at patch scales. The
discriminator also consists of convolution layer, instance normalization, and leaky ReLU operation
as shown in Fig. 6(b). We use Adam optimizer to train our network, with momentum parameters
β1 = 0.5, β2 = 0.9, and learning rate of 0.0001. The discriminator is updated 5 times for every
generator updates. We use batch size of 1, and trained our network during 100 epochs. Our code
was implemented by TensorFlow.
13
Under review as a conference paper at ICLR 2020
Figure 7: A modified 3D U-net architecture for our high-resolution image generator in the deconvo-
lution microscopy problem.
Multiple Discriminator
，Crop 0.5x
f Crop 0.5x
Li-id-
Conv (+ Instance Norm) + ReLU
Conv
Figure 8: Multi-PatchGANs discriminator architecture in the deconvolution microscopy problem.
D.2 Deconvolution microscopy
The network architecture of the high resolution image generator GΘ from the low-resolution image
is a modified 3D-Unet (Gicek et al., 2016) as shown in Fig 7. Our U-net structure consists of
contracting and expanding paths. The contracting path consists of the repetition of the following
blocks: 3D conv- Instance Normalization (Ulyanov et al., 2016)- ReLU. Here, the generator has
symmetric configuration so that both encoder and decoder have the same number of layers, i.e.
κ = 7. Throughout the network, the convolutional kernel dimension is 3 × 3 × 3. There exists a
pooling layer and skipped connection for every other convolution operations. To enhance the image
contrast, we add additional sigmoid layer at the end of U-Net. On the other hand, the low-resolution
image generator from high resolution input is based on a single 3D convolution layer that models
a 3D blurring kernel. The size of the 3D PSF modeling layer is chosen depending on the problem
set by considering their approximate PSF sizes. In this paper, the size of the 3D PSF layer is set to
20 × 20 × 20.
As for the discriminators, we follow the original CycleGAN that uses multi-PatchGANs (mPGANs)
(Isola et al., 2017), where each discriminator has input patches with different sizes used. As shown
in Fig 8, it consist of three independent discriminators. Each discriminator takes patches at different
sizes: original, and half, and quarter size patches.
D.2. 1 Real Experiment
A total 18 epi-fluorescent (EPF) microscopy images of tubulin with a size of 512 × 512 × 30 were
used for training, and one for validation. As for unmatched sharp image volume, we used deblurred
image generated by utilizing a commercial software AutoQuant X3 (Media Cybernetics, Rockville).
The EPF volume depth was increased to 64 using the reflection boundary condition. Due to GPU
memory limitations, the EPF volume was split into 64 × 64 × 64 size patches. For data augmentation,
rotation, flip, translation, and scale were imposed on the input patches. We normalized the patches
and set them to [0,1]. Adam optimizer (Kingma & Ba, 2014) was also used for training. The learning
rate was initially set to 0.0001, which is decreased linearly after 40 epoch, and the total number of
epoch was 200 epoch. For the optimizer, we used only a single batch.
14
Under review as a conference paper at ICLR 2020
		PSNR (dB)	SSIM
	Input	18.1087	0.5651
(a)	Supervised learning	25.6607	0.9209
	Conventional CycleGAN	25.7616	0.9444
	Proposed CycleGAN	26.4960	0.9513
	Input	17.8018	0.5201
(b)	Supervised learning	25.4048	0.9162
	Conventional CycleGAN	25.5837	0.9410
	Proposed CycleGAN	26.2891	0.9480
	Input	17.5045	0.4929
(c)	Supervised learning	25.1191	0.9113
	Conventional CycleGAN	25.3379	0.9369
	Proposed CycleGAN	26.0044	0.9437
Table 2: Performance comparison of various methods of Fig. 9 in terms of PSNR and SSIM.
D.2.2 Simulation Study
For simulation studies with the ground-truth data, we used synthetic microtubule network data set
(Sage et al., 2017) to train and validate our model. Specifically, from the ground-truth high resolution
synthetic microtubule images, we generate blurred images by convolving them with a model PSF. In
particular, the numerical PSF was computed using the Born and Wolf model (Kirshner et al., 2011),
which is given by
h(rx
ry ,Tz ) = ∣c∕ J0*1NnAPqrrx + r 卜-11IjkP2r ( NnA ) ρdρ∣
(28)
where C is a normalizing constant, k = 2∏∕λ is the wave number of emitted light, λ is the Wave-
length, N A is the numerical aperture, and ni is the refractive index of immersion layer. For all
simulation, we use NA = 1.4, and ni = 1.5. Then, the simulation is performed with different λ for
the PSF model (28). The convolution was performed using DeconvolutionLab2 (Sage et al., 2017).
The convolved data was then added with the mixture of zero mean Gaussian with the standard
deviation σ = 3 and Poisson noises with the parameter m = 7. The size of the synthetic data was
256 × 512 × 128, and 18 samples of the synthetic data were used for training and the other one is
used for validation. Due to memory limitations, the volume was split into 64 × 64 × 64 size patches,
which are used as inputs to the neural network.
Fig. 9 shows qualitative comparisons of different reconstruction methods over datasets generated
by the theoretical PSFs at different wavelengths. Here, the training was performed using the blurred
data with λ = 500nm PSF kernnel, but the inference was performed using data with wavelength 400
nm, 500 nm, and 600 nm, respectively. The goal of this study is to investigate the generalization
power of the proposed method with respect to different wavelengths.
The supervised method removed most blur and noise; however, it lost many fine details of the
tubule structure and showed false continuity over the tubule structure. The conventional cycleGAN
method showed better qualitative results than the supervised one, presenting finer details and the
correct continuity of the tubule structure, but still some of the structures were not fully recovered.
On the other hand, the proposed method showed the best qualitative results, recovering most of the
microtubule structures. Moreover, as shown in Table 2, the proposed method showed the best f and
SSIM scores. This confirmed that the proposed method generalizes well, even when the PSF model
for the training data is not matched well with that of the inference phase.
E Comparis on of the Classical and the Proposed Penalty for
Inverse Problems
Recall that the inverse problem y = Hx have many solutions due to the ill-poseness of the mapping
H. The main motivation of the classical PLS presented in Section 2.2 is to introduce the penalty for
choosing the solution x, which is most likely based on the prior distribution of data (see Fig. 10(a)).
On the other hand, unsupervised learning with training data provides an another way to resolve the
ambiguity of the feasible solutions. Specifically, if we define a single-valued function GΘ (y) and
15
Under review as a conference paper at ICLR 2020
Figure 9: Generalization performance comparison of various methods. The following parameters
were used for the training and inference: NA = 1.4, ni = 1.5, and σ = 3, m = 7. For the training
data, the wavelength was λ = 500nm. (a) Reconstruction result at the inference phase using the data
with λ = 400nm. (b) Reconstruction result at the inference phase using the data with λ = 500nm.
(c) Reconstruction result at the inference phase using the data with λ = 600nm. Two different views
(YZ, XZ) were displayed along the corresponding lines. The ROIs (marked yellow) show the area
for the enlarged parts.
impose the constraint X = Gθ* (y) with the learned parameter Θ*, many of the feasible solutions
for y = Hx can be cut out as shown in Fig. 10(b). Now, the remaining question is how to determine
the optimal parameter Θ*. This is where the optimal transport theory comes into play. Specifically,
our OT formulation tries to find the parameter Θ* by minimizing the average transportation cost
c(x, y; H, Θ* ) so that it eventually approaches zero, the primal and the dual solution may become
equivalent, and GΘ* can be an inverse of the forward operator H. This is the main motivation for
using the new penalty function in our formulation.
Figure 10: Two strategies for resolving ambiguities in the feasible solutions in an ill-posed inverse
problem. (a) Classical PLS approach using a close form prior distribution, and (b) our PLS approach
using an inverse mapping to define a prior.
16