Under review as a conference paper at ICLR 2020
αα-RANK: SCALABLE MULTI-AGENT EVALUATION
through Evolution
Anonymous authors
Paper under double-blind review
Ab stract
Although challenging, strategy profile evaluation in large connected learner net-
works is crucial for enabling the next wave of machine learning applications. Re-
cently, α-Rank, an evolutionary algorithm, has been proposed as a solution for
ranking joint policy profiles in multi-agent systems. α-Rank claimed scalability
through a polynomial time implementation with respect to the total number of
pure strategy profiles. In this paper, we formally prove that such a claim is not
grounded. In fact, we show that α-Rank exhibits an exponential complexity in
number of agents, hindering its application beyond a small finite number of joint
profiles. Realizing such a limitation, we contribute by proposing a scalable evalua-
tion protocol that we title αα-Rank. Our method combines evolutionary dynamics
with stochastic optimization and double oracles for truly scalable ranking with lin-
ear (in number of agents) time and memory complexities. Our contributions allow
us, for the first time, to conduct large-scale evaluation experiments of multi-agent
systems, where we show successful results on large joint strategy profiles with
sizes in the order of O(225) (i.e., ≈ 33 million strategies) - a setting not evaluable
using current techniques.
1	Introduction
Scalable policy evaluation and learning have been long-standing challenges in multi-agent rein-
forcement learning (MARL) with two difficulties obstructing progress. First, joint-strategy spaces
exponentially explode when a large number of strategic decision-makers is considered, and second,
the underlying game dynamics may exhibit cyclic behavior (e.g. the game of Rock-Paper-Scissor)
rendering an appropriate evaluation criteria non-trivial.
Focusing on the second challenge, much work in multi-agent systems followed a game-theoretic
treatment proposing fixed-points, e.g., Nash (Nash et al., 1950) equilibrium, as potentially valid
evaluation metrics. Though appealing, such measures are normative only when prescribing behav-
iors of perfectly rational agents - an assumption rarely met in reality Grau-Moya et al. (2018); Wen
et al. (2019). In fact, many game dynamics have been proven not converge to any fixed-point equi-
libria (Hart & Mas-Colell, 2003; Viossat, 2007), but rather to limit cycles (Palaiopanos et al., 2017;
Bowling & Veloso, 2001). Apart from these aforementioned inconsistencies, solving for a Nash
equilibrium even for “simple” settings, e.g. two-player games is known to be PPAD-complete (Chen
& Deng, 2005) - a demanding complexity class when it comes to computational requirements.
To address some of the above limitations, Omidshafiei et al. (2019) recently proposed α-Rank as
a graph-based game-theoretic solution to multi-agent evaluation. α-Rank adopts Markov Conley
Chains to highlight the presence of cycles in game dynamics, and attempts to compute stationary
distributions as a mean for strategy profile ranking. Though successful in small-scale applications,
α-Rank severely suffers in scalability contrary to polynomial time claims made in Omidshafiei et al.
(2019). In fact, we show that α-Rank exhibits exponential time and memory complexities shedding
light on the small-scale empirical study conducted in Omidshafiei et al. (2019), whereby the largest
reported game included only four agents with four available strategies each.
In this work, we put forward αα-Rank as a scalable alternative for multi-agent evaluation with linear
time and memory demands. Our method combines numerical optimization with evolutionary game
theory for a scalable solver capable of handling large joint spaces with millions of strategy profiles.
To handle even larger profiles, e.g., tens to hundreds of millions, we further introduce an oracle
1
Under review as a conference paper at ICLR 2020
★: {	,	,	}	▲:{,,}	■:{ ∙ , ∙ , ∙ }
(b)
Figure 1: Example of population based evaluation on N = 3 learners each with 3 strategies and 5
copies. a) Each population obtains a fitness value Pi depending on the strategies chosen, b) mutation
strategy (red star), and c) population either selecting original strategy, or adopting the novel strategy.
(McMahan et al., 2003) mechanism transforming joint evaluation into a sequence of incremental
sub-games with varying sizes. Given our algorithmic advancements, we justify our claims in a large-
scale empirical study involving systems with O(225) possible strategy profiles. We first demonstrate
the computation advantages of αα-Rank on varying size stochastic matrices against other imple-
mentations in Numpy, PyTorch, and OpenSpiel (Lanctot et al., 2019). With these successes, we then
consider experiments unsolvable by current techniques. Precisely, we evaluate multi-agent systems
in self-driving and Ising model scenarios each exhibiting a prohibitively-large strategy space (i.e.,
order of thousands for the former, and tens of millions for the latter). Here, we again show that
αα-Rank is capable of recovering correct strategy ranking in such complex domains.
2	α-RANK & ITS LIMITATIONS
In α-Rank, strategy profiles of N agents are evaluated through an evolutionary process of mutation
and selection. Initially, agent populations are constructed by creating multiple copies of each learner
i ∈ {1, . . . , N} assuming that all agents (in one population) execute the same unified policy. With
this, α-Rank then simulates a multi-agent game played by randomly sampled learners from each
population. Upon game termination, each participating agent receives a payoff to be used in policy
mutation and selection after its return to the population. Here, the agent is faced with a probabilistic
choice between switching to the mutation policy, continuing to follow its current policy, or randomly
selecting a novel policy (other than the previous two) from the pool. This process repeats with the
goal of determining an evolutionary strong profile that spreads across the population of agents. Each
of the above three phases is demonstrated in Fig. 1 on a simple example of three agents - depicted
by different symbols - each equipped with three strategies - depicted by the colors.
Mathematical Formalisation, Notation, and Definitions: We next formalize the process posed by
α-Rank, which will lead to its limitations, and also pave the way for our own proposed solution.
We consider N agents with each agent i having access to a set of strategies of size si . At round
k of the evaluation process, We denote the strategy profile for agent i by S? = {n；，,..., ∏ikS].},
with πi[k,j] : X × Ai → [0, 1] representing the jth allowed policy of the learner. X represents the set
of states and Ai is the set of actions for agent i. With this, we define a joint strategy profile for all
participating agents as policies belonging to thejoint strategy pool, SjOint = s1k] X s2k] …×∙∙∙× sjN]:
πj[oki]nt = {π1[k,j]1,. ..,πN[k,]jN} with πi,ji ∈ Si[k] and ji ∈ {1,.. .,si}.
To evaluate performance, we assume each agent is additionally equipped with a payoff (reward)
function Pi[k] : Sjoint → R+ . Crucially, the domain of Pi[k] is the pool of joint strategies so to
accommodate the effect of other learners on the ith player performance further complicating the
[k]
evaluation process. Finally, given a joint profile πjoint, we define the corresponding joint payoff to be
the collection of all individual payoff functions, i.e., Pjoknt = {P；k] ("[；),.•., P[k] (∏jknt)}∙
After attaining rewards from the environment, each agent returns to its population and faces a choice
between switching to a mutation policy, exploring a novel policy, or sticking to the current one Such
2
Under review as a conference paper at ICLR 2020
a choice is probabilistic and defined proportional to rewards. Precisely, agent i adopts
P(πi[k,a] → πi[k,b],π-[ki])
eαPi[k](πi[k,a],π-[ki])+eαPi[k](πi[k,b],π-[ki])
-2 for (∏[ka ,∏ikb)∈sik]×
p(∏[ka → ∏[kc, ∏-i)=ɪ,	∀∏ikc ∈ Si[k] \ {∏ika,∏ikb},
Ki - 2	,
with μ ∈ R+ denoting an exploration parameter1, π[ki = π[k] \ ∏ik] representing policies followed
by other agents at round k, and α ∈ R+ an intensity ranking parameter. As noted in Omidshafiei
et al. (2019), one can relate the above switching process to a random walk on a Markov chain with
states defined as elements in Sj[oki]nt and transition probabilities through payoff functions. In particular,
each entry in the transition probability matrix T ∈ RSj[oin]t ×Sj[oin]t refers to the probability of one
agent switching from one policy in a relation to attained payoffs. Precisely, consider any two joint
strategy profiles 仆着 and 缶馨 that differ in only one individual strategy for the ith agent, i.e., there
eχists a UniqUe agentSUCh thatπ舄={凛,π-" andπ舄={π ikb, π[ki} with πika=πikb, we
set	[T[k]]	[k]分网=1∕pN=ι(sι -	i)ρ	[k]即](∏-i),	with ρ 网	^[k↑	(π-i) defining the probability
πjoint,πjoint	πi,a,πi,b	-	πi,a,πi,b	-
that one copy of agent i with strategy πi[k,a] invades the popUlation with all other agents (in that
population) playing ∏ik]. Following Pinsky & Karlin (2010) for Pik (∏[3 ∏-i) = P[k] (∏ik], ∏-i),
sUch a probability is formalized as:
1-e-α(Pik](∏ka ,∏-i)-P 产㈤kb,∏-i))
ρ∏[ka㈤kb(π[ki) = UmWakiwnW，and 1/m otherwise,	(1)
with m being the size of the popUlation. So far, we presented relevant derivations for the
(∏j%, ∏j%t) entry of the state transition matrix when exactly the ith agent differs in exactly
one strategy. Having one policy change, however, only represents a sUbset of allowed variations,
where two more cases need to be considered. Now we restrict oUt attention to variations in joint
policies involving more than two individual strategies, i.e., Injoint \ ∏joint ≥ 2∣. Here, We set2
[k] = 0. ConseqUently, the remaining event of self-transitions can be thUs written as
,πjoint
T[k] [k] [k] = 1 - P [k] T[k]	[k] [k] . Summarising the above three cases, we can then write
πjoint ,πjoint	πjoint	π ,π
the (njoiV，∏joint),s entry of the Markov chain,s transition matrix as:
T[k] π[k]
joint
hT∖,咽J
1/pN=i(si - 1)P∏[kl ,∏[kl (π-i)	if Ekint \ π⅛l = 1,
1 - Pnj[oklt [T同i∏[k],∏[k]	if jnt = πjki]nt SeIf-loops,
0 j°mt	if ∣n⅛ \ ∏⅛l≥2,
(2)
The goal in α-Rank is to establish an ordering in policy profiles dependent on evolutionary stability
of each joint strategy. In other words, higher ranked strategies are these that are prevalent in popula-
tions with higher average times. Formally, such a notion can be easily derived as the limiting vector
v[k] = limt→∞ [T[k]]T t
v0 of our Markov chain when evolving from an initial distribution v0 .
Knowing that the limiting vector is a stationary distribution, one can calculate strategy rankings as
the solution to the following eigenvector problem:
hT[k] iT v[k] = v[k]
(3)
Limitations of α-Rank: Though the work in Omidshafiei et al. (2019) seeks to determine a solution
to the above problem, it is worth mentioning that α-Rank suffers from one major drawback-the
1 Please note that in the original paper μ is heuristically set to a small positive constant to ensure at maximum
two varying policies per-each population. Theoretical justification can be found in Fudenberg & Imhof (2006)
2This assumption significantly reduces the analysis complexity as detailed in Fudenberg & Imhof (2006).
3
Under review as a conference paper at ICLR 2020
Scalability-that We remedy in this paper. We note that the solution methodology in α-Rank is in
fact unscalable to settings involving more than a hand-full of agents. Particularly, authors claim
polynomial complexities of their solution to the problem in Eqn. 3. Though polynomial, such a
complexity, hoWever, is polynomial in an exponential search space, i.e., the space of joint strategy
profiles. As such, the polynomial complexity claim is not grounded, and need to be investigated. In
short, α-Rank exhibits an exponential (in terms of the number of agents) complexity for determining
a ranking, thus rendering it inapplicable to settings involving more than a small amount of agents.
In What comes next, We first discuss traditional approaches that could help solve the Eqn. 3; soon
We realize an off-the-shelve solution is unavailable. Hence, We commence to propose an efficient
evaluation algorithm, i.e., αα-Rank, based on stochastic optimization With suitable complexities
and rigorous theoretical guarantees. At the end, We propose a search heuristic to further scale up our
method by introducing oracles and We name it by αα-Oracle.
3 S calable Evaluation for Multi-Agent Systems
The problem of computing stationary distributions is a long-standing classical problem from lin-
ear algebra. Various techniques including power method, PageRank, eigenvalue decomposition, and
mirror descent can be utilized for solving the problem in Eqn. 3. As We demonstrate next, any such
implementation scales exponentially in the number of learners, as We summarize in Table 1.
3.1	Traditional Approaches.
Power Method. One of the most common approaches to computing the solution in Eqn. 3 is
the poWer method. PoWer method computes the stationary vector vk by constructing a sequence
{vj}j≥0 from a non-zero initial vector v0 by applying vj+1 = 1/T [k],Tvj T [k],Tvj. Though vi-
able, We first note that the poWer method exhibits an exponential memory complexity in terms
of the number of agents. To formally derive the bound, define n to represent the total number of
joint strategy profiles (i.e., n = |Sj[oki]nt| = QiN=1 si), and msize the total number of transitions be-
tWeen the states of the Markov chain in Section 2. By the construction, one can easily see that
msize = n(PN=ι Si — N + 1)as each row and column in T[k] contains PN=I Si — N + 1
non-zero elements. Hence, memory complexity of such an implementation are in the order of
O(msize) = 0(n[Pi=1 Si- N + ”) ≈ O(QN=I Si PN=ι Si) = O(nPN=ι s,∙
Analyzing its time complexity, on the other hand, requires a careful consideration that links
convergence rates with the resulting graph topology of the Markov chain. Precisely, the conver-
gence rate of the power method is dictated by the second-smallest eigenvalue of the normalized
Laplacian, LG, of the graph, G, associated to the Markov chain in Section 2, i.e., v [k] — vj 2 ≤
C (μ2 (Lg))", with μ2 (LG) being the second-smallest eigenvalue of Lg, and C > 0. Hence, as
long as the second-smallest eigenvalue of the normalized Laplacian is well-behaved, one would
expect suitable time complexity guarantees. To this end, we prove the following lemma
Lemma: [Second-Smallest Eigenvalue] Consider the Markov chain defined in Section 2 with
states in Sj[oki]nt and transition probability matrix T[k]. The second-smallest eigenvalue of the
normalized Laplacian of the graph associated with the Markov chain is g^ven by: μ2 (LG) =
Pmms S--N+1, with Si denoting the number of strategies Ofagent i.
Due to space constraints, the full proof of the above lemma is refrained to Appendix A.1. The
importance of Lemma A.1 is that the resultant time complexity of the power method is also ex-
ponential of the form O(n X log [(PN=ι si - N +i)/(mini si - 1)])=O(nlogn)=O(nN). The
above results is true as (miniS - 1)∕PN=ι si - N + 1)≤ (miniS - I)/logn = Ω(1∕iogn). Hence,
(PiN=1 si - N + 1)/(mini si - 1) = O (log n).
PageRank. Inspired by ranking web-pages on the internet, one can consider PageRank (Page et al.,
1999) for computing the solution to the eigenvalue problem in Eqn. 3. Applied to our setting, we first
realize that the memory is analogous to the power method that is 0(msize) = θ(n PN=I Si),and the
time complexity are in the order of 0( msize + n) = θ(n + n[PN=ι Si-N +1]) ≈ O(n PN=I Si).
4
Under review as a conference paper at ICLR 2020
Table 1: Time and space complexity comparison (: precision parameter).
Method	Time	Memory
Power Method	O (QN=I Si P=I log s,	O (QN=I Si PN=I Sy
PageRank	O (QN=I Si PN=IlOg Sj	O (QN=I Si PN=1 可
Eig. Decomp.	O (QN=I Sw )	O (QN=I Si PN=1 可
OSMD	O (QN=ISi PN=I bg Si)	O (QN=I Si PN=1 S∣^
Our Method	O(⅛4 P=ι Sif^^	O G PN=1 Si)
Eigenvalue Decomposition. Apart from the above, we can also consider the problem as a standard
eigenvalue decomposition task (also what the original α-Rank is implemented according to Lanctot
et al. (2019)) and adopt the method in Coppersmith & Winograd (1990) to compute the stationary
distribution. Unfortunately, state-of-the-art techniques for eigenvalue decomposition also require
exponential memory and exhibit a time complexity of the form O(nω) = O(QiN=1 siw) with ω ∈
[2, 2.376]. Clearly, these bounds restrict α-Rank to small number of agents N.
Mirror Descent. The ordered subsets mirror descent (Ben-Tal et al., 2001) requires at each iter-
ation a projection on standard n-dimensional simplex: Ξn = {x ∈ Rn : xT1 = 1 & x 0}.
As stated in the paper, the computing of this projection requires O(n log n) time. In our setting,
n = QiN=1 si is the total number of joint strategy profiles. Hence, the projection step is exponential
in the number of agents N . This makes mirror descent inapplicable for α-Rank when N is large.
3.2	Our Proposal: An Optimization-Based Solution.
Rather than seeking an exact solution to the problem in Eqn. 3, one can consider approximate solvers
by defining a constraint optimization objective:
min1 ∣∣T[k],Tx — x||： s.t. xT1 = 1, andX 占 0.	(4)
The constrained objective in Eqn. 4 simply seeks a vector x minimizing the distance between x,
itself, and T[k],Tx (i.e., attempting to solve T [k],Tx = x) while ensuring that x lies on an n-
dimensional simplex (i.e., xT1 = 1, and x 0). Due to time and memory complexities required
for computing exact solutions, We focus on determining an approximate vector X defined to be the
solution to the following relaxed problem of Eqn. 4:
min — ∣ ∣T[k],Tx 一 χ∣ ∣	s.t. ∣xt1 _ 1∣ ≤ δ for 0 < δ < 1, and X 占 0.	(5)
The optimization problem in Eqn. 5 can be solved using a barrier-like technique that We detail beloW.
Before that, it is instructive to clarify the connection betWeen the original and the relaxed problems
Proposition: [Connections to Markov Chain] Let X bea solution to the relaxed optimization problem
in Eqn. 5. Then, x∕∣∣x∣∣ι = v[k] is the stationary distribution to the Markov chain in Section 2.
Importantly, the above proposition alloWs us to focus on solving the problem in Eqn. 5 Which only
exhibits inequality constraints. Problems of this nature can be solved by considering a barrier func-
tion leading to an unconstrained finite sum minimization problem. To do so, denoting b[ik] to be
the ith row of T[k],T 一 I, we can write £ ∣ ∣T[k],Tx 一 x∣∣j = 1 pn=1 (XTbik]) . Introducing
logarithmic barrier-functions, With λ > 0 being a penalty parameter, We arrive at
n2	n
XmRn n X(XTbik]) - λ lθg (S2 - H 1 - U) - n X lθg(Xi)，
(6)
i=1
i=1
Eqn. 6 is a standard finite minimization problem that can be solved using any off-the-shelve stochas-
tic optimization algorithm, e.g., stochastic gradients, ADAM (Kingma & Ba, 2014) among others.
A stochastic gradient execution involves sampling a strategy profile it 〜[1,... ,n] at iteration t,
5
Under review as a conference paper at ICLR 2020
and then executing a descent step: xt+1 = xt - ηtVxfit(xt), with Vxfit (xt) being a sub-sampled
gradient ofEqn. 6, and λ being a scheduled penalty parameter with λt+ι = Ah for some γ > 1,
Vxfit (xt)=2 M],T1)bik] + FM * - λt∕n	T
- xt 1 - 1
(7)
See Phase I in Algorithm 1 for the pseudo-code. We can further derive a convergence theorem of:
Theorem: [Convergence ofBarrier Method] Let Xλ be the output ofa gradient algorithm descending
in the objective in Eqn. 6, after T iterations, then
E
IIhT [k],Ti χλ-引口
≤O λ+
where expectation is taken w.r.t. all randomness ofa stochastic gradient implementation, and γ > 1
is a decay-rate for λ, i.e., λ[t+1] = λ[t] ∕γ.
Algorithm 1 αα -Rank: Scalable Multi-Agent Evaluation
1:	Inputs: Evaluation Parameters: Initial a vector xo = 1∕n1, total number of iterations T =
O(1∕2), decaying learning rate {ηt}tT=1, penalty parameter λ ≈ O(), λ decay rate γ > 1, total
number of joint strategy profiles n, and a constraint relaxation term δ.Oracle Parameters:
initialize a subset of strategy pools for all agents {Si[0]} by randomly sampling from {Si}
2:	Set outer iteration count k = 0
3:	while stopping criteria do:
4:	Phase I: Scalable Policy Evaluation (Section 3.2):
5:	for t = 0 → T 一 1 do:
6:	Uniformly sample one strategy profile itk] ~ {1,..., n}
7:	Construct b[ik] as the i[tk] row of T [k],T - I with T[k] entries defined Eqn. 2
8:	Update solution x[tk+]1 = x[tk] - ηtVxfi[k] (x[tk] ) as in Eqn. 7 & set λt+1 = λt∕γ
9:	Extract njkintoP from approximate stationary distribution v[k] = xTk∕∣xT∣∣ι
10:	Phase II (if turned on): Scalable Policy Evaluation with Oracle (Section 3.3):
11:	—for each agent i do:
12:	Compute the best-response strategy ∏i by solving Eqn. 8.
13:	Update strategy pools for each agent i as Si[k] = Si[k] ∪ πi?
14:	Setk = k+ 1
15:	Return: Best performing strategy profile πjoint,? across all agents.
The proof of the above theorem (see the full proof in Appendix A.2) is interesting by itself, a more
important aspect is the memory and time complexity implications posed by our algorithm. Theo-
rem A.2 implies that after T = O(1∕2) iterations with being a precision parameter, our algorithm
outputs a vector Xλ 占 0 such that
∣XT1 一 1∣ ≤ δ and E h∣∣T [k],Tχχ 一 Xι∣∣J ≈ O (e).
Moreover, one can easily see3 that after T steps, the overall time and memory complexities of our
update rules are given by O(T2 PiN=1 si) and O(T PiN=1 si), respectively. Using T = O(1∕2)
eventually leads to a memory complexity of O(1∕2 PiN=1 si) and O(1∕4 PiN=1 si) for time (see the
comparison in Table. 1). Hence, our algorithm is able to achieve an exponential reduction, in terms
of number of agents, in both memory and time complexities.
3.3	Heuristic Search by Introducing Oracles
So far, we have presented scalable multi-agent evaluations through stochastic optimization. We can
further boost scalability (to tens of millions of joint profiles) of our method by introducing an ora-
cle mechanism. The heuristic of oracles was first introduced in solving large-scale zero-sum matrix
3More details on these derivations can be found in the Appendix A.3
6
Under review as a conference paper at ICLR 2020
(a)	(b)	(O
Figure 2: Ranking intensity sweep on (a) Battle of Sexes (b) Biased RPS (C) Prisoner,s Dilemma.
2 10 12 3
ɪ01010LL0-
111
spuooaς U- υ∈-H
102	IO3	IO4
Size of Matrix
Ou ΛJ0E3w
IO2	IO3	104
Size of Matrix
Figure 3: Comparisons of time and memory complexities on varying sizes of random matrices.
games (McMahan et al., 2003). The idea is to first create a restricted sub-game in which all play-
ers are only allowed to play a restricted number of strategies, which are then expanded by adding
incorporating each of the players’ best-responses to opponents; the sub-game will be replayed with
agents’ augmented strategy pools before a new round of best responses is found. The worse-case
scenario of introducing oracles would be to solve the original evaluation problem in full size. The
best response is assumed to be given by an oracle that can be simply implemented by a grid search.
Precisely, given the top-rank profile π-[ki],top at iteration k, the goal for agent i is to select4 the optimal
∏i from the pre-defined strategy pool Si to maximize the reward
πi? = argπm∈aSxEπi,π[k],top X0 γi[k],hPi[k](x[hk], u[ik,h], u[-ki],h),	(8)
With xhk denoting the state, Uikh 〜πi(Uikhιxikh), u-i,h 〜π-i,high(Ulki,h|x[kIh) denoting the ac-
tions from agent i and the opponents, respectively. The heuristic of solving the full game from
restricted sub-games is crucial especially when it is prohibitively expensive to list all joint-strategy
profiles, e.g., in scenarios involving tens-of-millions of joint profiles.
For a complete exposition, we summarize the pseudo-code in Algorithm 1. In the first phase, vanilla
αα-Rank is executed (lines 4-9), while in the second (lines 11 - 13), αα-Rank with Oracle (if turned
on) is computed. To avoid any confusion, we refer to the latter as αα-Oracle. Note that even though
in the two-player zero-sum games, the oracle algorithm (McMahan et al., 2003) is guaranteed to
converge to the minimax equilibrium. Providing valid convergence guarantees for αα -Oracle is an
interesting direction for future work. In this paper, we rather demonstrate the effectiveness of such
an approach in a large-scale empirical study as shown in Section 4.
4	Experiments
In this section, we evaluate the scalability properties of αα -Rank5. Precisely, we demonstrate that
our method is capable of successfully recovering optimal policies in self-driving car simulations
and in the Ising model where strategy spaces are in the order of up to tens-of-millions of possible
strategies. We note that these sizes are well beyond the capabilities of state-of-the-art methods, e.g.,
4 Note that the oracle can also be used to find an improved policy that is not in the original strategy pool,
similar to PSRO (Lanctot et al., 2017), which we leave to the future work.
5All of the experiments are run by a single machine with 64 GB memory, and 10-core Intel I9-9900X CPU.
7
Under review as a conference paper at ICLR 2020
Oa-Rank
04-0racle
α-Rank o(io7)
Power Method o(io7)
0%	30% Iterations 70%	100%
(C)
--Our Method
--MCMC
0=JOJd &-£ "≡a"UJgo"U"
Number of iterations
(a)
"」s?e」ed」3p」0
High-Way Evaluation
PhaSe
Change
10-3	10-2	10-1	100	101	102
Inverse Temperature
Ising Model Evaluation	(b)
Figure 4: Large-scale multi-agent evaluations. (a) Convergence of the optimal joint-strategy profile
of self-driving simulation on the highway. (b) Status of the Ising-model equilibrium measured by
ξ = lN↑-N，| .(c) Convergence of the top-rank profile from aα-Oracle under T = 1,α = 1.
α-Rank (Omidshafiei et al., 2019) that considers at maximum four agents with four strategies, or
AlphaStar which handles about 600 strategies as detailed in Vinyals et al. (2019).
Sparsity Data Structures. During the implementation phase, we realised that the transition proba-
bility, T [k], of the Markov chain induces a sparsity pattern (each row and column in T[k] contains
PiN=1 si - N + 1 non-zero elements, check Section 3.2) that if exploited can lead to significant
speed-ups. To fully leverage such sparsity, we tailored a novel data structure for sparse storage and
computations needed by Algorithm 1. More details can be found in Appendix B.1.
Correctness of Ranking Results. Before conducting large-scale sophisticated experiments, it is
instructive to validate the correctness of our results on the simple cases especially those reported by
Omidshafiei et al. (2019). We therefore test on three normal-form games. Due to space constraints,
we refrain the full description of these tasks to Appendix B.2. Fig. 2 shows that, in fact, results
generated by aα-Rank, the Phase I of Algorithm 1, are consistent with a-Rank's results.
Complexity Results on Random Matrices. We measured the time and memory needed by our
method for computing the stationary distribution with varying sizes of simulated random matri-
ces. Baselines includes eigenvalue decomposition from Numpy, optimization tools in PyTorch, and
α-Rank from OpenSpiel (Lanctot et al., 2019). For our algorithm we terminated execution with gra-
dient norms being below a predefined threshold of 0.01. According to Fig. 3, αα-Rank can achieve
three orders of magnitude reduction compared to eigenvalue decomposition in terms of time. Most
importantly, the performance gap keeps developing with the increasing matrix size.
Autonomous Driving on Highway: High-way (Leurent, 2018) provides an environment for simu-
lating self-driving scenarios with social vehicles designed to mimic real-world traffic flow as strategy
pools. We conducted a ranking experiment involving 5 agents each with 5 strategies, i.e. a strategy
space in the order of O(55) (3125 possible strategy profiles). Agent strategies varied between “ra-
tional” and “dangerous” drivers, which we encoded using different reward functions during training
(complete details of defining reward functions can be found in Appendix C.2). Under this setting,
we know, upfront, that optimal profile corresponds to all agents is five rational drivers. Cars trained
using value-iteration and rewards averaged from 200 test trails were reported. Due to the size of the
strategy space, we considered both αα-Rank and αα-Oracle. We set αα-Oracle to run 200 iterations
of gradient updates in solving the top-rank strategy profile (Phase I in Algorithm 1). Results depicted
in Fig. 4(a) clearly demonstrate that both our implementations are capable of recovering the correct
highest ranking strategy profile. We also note that though such sizes are feasible using α-Rank and
the power-method, our results achieve 4 orders of magnitude reduction in total number of iterations.
Ising Model Experiment: The Ising model (Ising, 1925) is the model for describing ferromagnetism
in statistical mechanics. It assumes a system of magnetic spins, where each spin aj is either an up-
spin, ↑, or down-spin, 1. The system energy is defined by E(a, h) = - Pj (hjaj + 2 Pk=j αjak)
with hj and λ being constant coefficients. The probability of one set of spin configuration is
P (a) = Pexex-EE(柒//)where T is the environmental temperature. Finding the equilibrium of
8
Under review as a conference paper at ICLR 2020
the system is notoriously hard because it is needed to enumerate all possible configurations in com-
puting P (a).Traditional approaches include Markov Chain Monte Carlo (MCMC). An interesting
phenomenon is the phase change, i.e., the spins will reach an equilibrium in the low temperatures,
with the increasing τ , such equilibrium will suddenly break and the system becomes chaotic.
Here we try to observe the phase change through multi-agent evaluation methods. We assume each
spins as an agent, and the reward to be rj = hj aj + 2 Pk=j aj ak, and set α = 1 to build the link
between Eqn. 1 and P (a). We consider the top-rank strategy profile from αα-Oracle as the system
equilibrium and compare it against the ground truth from MCMC. We consider a five-by-five 2D
model which induces a prohibitively-large strategy space of size 225 (tens of millions) to which the
existing baselines, including αα-Rank on the single machine, are inapplicable. Fig. 4(b) illustrates
that our method identifies the same phase change as what MCMC suggests. We show an example of
how αα-Oracle's top-ranked profile finds the system equilibrium in Fig. 4(c) at T = 1.
5	Conclusion
In this paper, we demonstrated that the approach in Omidshafiei et al. (2019) exhibits exponential
time and memory complexities. We then proposed αα-Rank as a scalable solution for multi-agent
evaluation with linear time and memory demands. In a set of experiments, we demonstrated that our
method is truly scalable capable of handling large strategy spaces.
There are a lot of interesting avenues for future research. First, we plan to theoretically analyze con-
vergence properties of the resulting oracle algorithm, and further introduce policy learning through
oracles. Second, we plan take our method to the real-world by conducting multi-robot experiments.
References
Sasmita Barik, Ravindra Bapat, and Sukanta Pati. On the laplacian spectra of product graphs. Ap-
plicable Analysis and Discrete Mathematics, 9, 04 2015. doi: 10.2298/AADM150218006B.
Aharon Ben-Tal, Tamar Margalit, and Arkadi Nemirovski. The ordered subsets mirror descent opti-
mization method with applications to tomography. SIAMJournal on Optimization, 12(1):79-108,
2001.
Michael Bowling and Manuela Veloso. Convergence of gradient dynamics with a variable learning
rate. In ICML, pp. 27-34, 2001.
X Chen and X Deng. Settling the complexity of 2-player nash equilibrium. eccc. Technical report,
Report, 2005.
Don Coppersmith and Shmuel Winograd. Matrix multiplication via arithmetic progressions. Journal
of symbolic computation, 9(3):251-280, 1990.
Drew Fudenberg and Lorens A Imhof. Imitation processes with small mutations. Journal of Eco-
nomic Theory, 131(1):251-262, 2006.
Jordi Grau-Moya, Felix Leibfried, and Haitham Bou-Ammar. Balancing two-player stochastic
games with soft q-learning. arXiv preprint arXiv:1802.03216, 2018.
Sergiu Hart and Andreu Mas-Colell. Uncoupled dynamics do not lead to nash equilibrium. American
Economic Review, 93(5):1830-1836, 2003.
Ernst Ising. Beitrag zur theorie des ferromagnetismus. Zeitschriftfur Physik A Hadrons and Nuclei,
31(1):253-258, 1925.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat,
David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 4190-4203, 2017.
9
Under review as a conference paper at ICLR 2020
Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay,
Julien Perolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al.
Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453,
2019.
Edouard Leurent. An environment for autonomous driving decision-making. https://github.
com/eleurent/highway-env, 2018.
H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost func-
tions controlled by an adversary. In Proceedings of the 20th International Conference on Machine
Learning (ICML-03),pp. 536-543, 2003.
John F Nash et al. Equilibrium points in n-person games. Proceedings of the national academy of
sciences, 36(1):48-49, 1950.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA,
second edition, 2006.
Shayegan Omidshafiei, Christos Papadimitriou, Georgios Piliouras, Karl Tuyls, Mark Rowland,
Jean-Baptiste Lespiau, Wojciech M Czarnecki, Marc Lanctot, Julien Perolat, and Remi Munos.
α-rank: Multi-agent evaluation by evolution. Scientific Reports, Nature, 2019.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford InfoLab, 1999.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence, limit cycles and chaos. In Advances
in Neural Information Processing Systems, pp. 5872-5882, 2017.
Mark Pinsky and Samuel Karlin. An introduction to stochastic modeling. Academic press, 2010.
Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M
Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al. Alphastar: Mas-
tering the real-time strategy game starcraft ii. DeepMind Blog, 2019.
Yannick Viossat. The replicator dynamics does not lead to correlated equilibria. Games and Eco-
nomic Behavior, 59(2):397-407, 2007.
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. arXiv preprint arXiv:1901.09207, 2019.
10
Under review as a conference paper at ICLR 2020
Appendix
A Comprehensive Proofs
A. 1 Lemma of Second-Smallest Eigenvalue
Lemma: [Second-Smallest Eigenvalue] Consider the Markov chain defined in Section 2 with
states in Sj[oki]nt and transition probability matrix T [k]. The second-smallest eigenvalue of the
normalized Laplacian of the graph associated with the Markov chain is given by: μ2 (LG) =
Pmms S-N1+ι, with Si denoting the number of strategies Ofagent i.
Proof: For simplicity we drop round index k in the below derivation. Notice, the underlying graph
for the constructed Markov Chain can be represented as a Cartesian product of N complete graphs
Ks 6:
si
G = Ksi ×Ks2 ×∙∙∙× KsN	(9)
Indeed, two vertices π[k], ∏[k] ∈ G are connected by the edge if and if only these joint strategy
profiles differ in at most one individual strategy, i.e ∃!i ∈ {1,...,N} : π[k] = {n；：], ∏-i}, ∏ [k] =
{∏ik], ∏-i}.Hence, the spectral properties of G can be described in terms of spectral properties of
Ksi as follows (Barik et al., 2015):
Spectr(G) =
{μi1 (L(Iun)) + ... + μiN (L(Iun))产一：* ,	1
i1	Ks1	iN KsN i1 =1,i2 =1,...,iN =1
Eigenvectors of G =
{"ii,1 ㊈ g,2 ㊈…㊈ &N ,N }Si=XX.,iN = 1.
where μi(L(un)) is the ith eigenvalue of the unnormalized Laplacian of the complete graph Ksj and
&,j is the corresponding eigenvector7. The spectrum of unnormalized Laplacian of the complete
graph Ksi is given by Spectr(Ksi ) = {0, si - 1} and the only eigenvector corresponding to zero
eigenvalue is 1 ∈ Rsi. Therefore, the minimum non-zero eigenvalue of unnormalized Laplacian of
G is given by mini si - 1. Finally, due to the fact that G is a regular graph (with degree of each node
is equal to PiN=1 si - N + 1), the smallest non-zero eigenvalue of the normalized Laplacian ofG is
givenby Pminsis--N+ι.
Giving this result, the overall time complexity of Power Method is bounded by
O (n X log hPi=I SLN+1 i) = O(nlogn). Indeed, notice that	minisi-1「≤ minisi-1 =
mini si -1	,	iN=1 si -N +1	log n
Ω (IoIn), hence, Pminsis-N+1 = O (log n). AS for the memory complexity, Power Method
requires has the same requirements as PageRank algorithm. 8 These results imply that Power
Method scales exponentially with number of agents N, and therefore, inapplicable when N is large.
A.2 Theorem of Convergence of Barrier Method
Theorem: [Convergence ofBarrier Method] Let Xλ be the output ofa gradient algorithm descending
in the objective in Eqn. 6, after T iterations, then
EJhT [k],Ti xλ-xλ∣∣2] ≤O (λ+√t+γ⅛),
where expectation is taken w.r.t. all randomness ofa stochastic gradient implementation, andγ > 1
is a decay-ratefor λ, i.e., λ[t+1] = λ[t]∕γ. See theAlgorithm2.
6Here, Kp denotes a complete graph with p nodes.
7In other words, LKsJ a,j = μi(Lκsj )&,j for all i = 1,..., Sj and j = 1,...,N.
8Due to necessity to store matrix T [k]
11
Under review as a conference paper at ICLR 2020
Algorithm 2 Log-Barrier Stochastic Gradient Descent
1:	Input: X0 = 11,T, λ,δ, e, {ηt}T=ι,γ > 1.
2:	Output: V
3:	Set δo = δ, λo = λ.
4:	for t = 0 to T — 1 do:
5:	Sample it 〜[1,... ,n] and compute:
-1 -I
[xt]1
Xt ”…十 δ≡⅛⅛
1 — X
n
1
-[xt] n -
6:	Update xt+1 = Xt - ηt^xfit (xt).
7:	Update λt+ι = λγt.
8:	end for
9:	Set V = xτ
Proof: Let X and X^ be the solutions ofEqn.(5) and Eqn. (6) respectively. Convergence guarantees
for logarithmic barrier method (Nocedal & Wright, 2006) with penalty parameter λ and barrier
parameter Y gives:
T
2
IT网]xλ — xλ —
(10)
and using [T网]T X = X in (10) gives:
(11)
Applying the convergence guarantees of stochastic gradient descent method to convex function F(x)
gives:
E [F(Xλ) — F(Xλ)] ≤θ(√T)
Using the definition of function F(x):
nE [F(Xλ) — F(Xλ)] ≥ E
「 一ιT	2	「	-1 T	2
[t如 Xλ — Xλ	—	[tM Xλ — Xλ — O(λ)
Combining the last two results:
E
2
2
T	2
[t网]Xλ — Xλ
[t 同]T Xλ — Xλ
≤O(λ) + O
Finally, using (11) gives:
E
hτ[fc]] Xa — X』2〕≤O(λ + √T + Ynλ
By choosing λ = j and T = max{ Ir, 2；：g : }, we have λ + + + 壬 ≤ f + f + j = e. Hence,
E
卜肉］t
∙-v	∙-v
Xλ — Xλ
≤og
2
12
Under review as a conference paper at ICLR 2020
A.3 TIME AND MEMORY COMPLEXITY OF αα -RANK
The above result implies given precision parameter e > 0, after T = O (表)iterations, Algorithm 2
outputs vector Xλ A 0 SUch that:
I xT 1 - 1∣ ≤ δ, and E
hT[k]iT Xλ - Xλ∣∣ ] ≤O(e)
Hence, by tuning parameters δ and one can approximate a stationary distribution vector ν [k]
Algorithm 2 starts with uniform distribution vector xo = § 1 and at step t it updates the
previous iterative xt by a rule given in line 6. Let Nt = {j ∈ [n] : ∃r ∈ [t] with [b[ik]]j 6= 0} 9.
Then, forj ∈/ Nt all entries [xt+1]j are equal to each other and updated as:
[xt+1]j = [xt]j +
2ηtλt(xtT1 - 1)
ηtλt
___________________________
δ2 - (xT1 - 1)2__n[χt]j
Given value xtT1, the above computation takes O(1) time and space. For entries j ∈ Nt, all entries
[xt+1]j might be different ( worst case ) and, therefore, update
[xt+ι]j = [xt]j-21Tbik*j + δΤ⅛⅛
ηtλt
n[xt]j
	
need to be performed at most O(|Nt|) times. Notice, only O(PiN=1 si - N + 1) of these entries
require the same computation of 1Tb[ik] and all other entries can be updated in constant time and
space. Hence, the computation of all these entries can be done in O(|Nt|) time and space. Finally,
term xtT+1 1 can be computed in O(|Nt|) time and space. Hence, storing and updating of all en-
tries [xt+1]j require O t PiN=1 si time and space. Therefore, after O(T) steps, the overall time
and memory complexity of Algorithm 2 are given by O T2 PiN=1 si and O T PiN=1 si respec-
tively. Using T = O (£) eventually gives O G PN=I Si) for time and O G PN=I Si) for space
complexity. Our result is summarized in Table 1.
B IMPLEMENTATION OF αα-RANK
B.1	The data structure for Sparsity
The transitional probability matrix T in αα-Rank is sparse; each row and column inT[k] contains
PiN=1 Si - N + 1 non-zero elements (see Section 3.2). To fully leverage such sparsity, we design
a new data structure (see Fig. 5) for the storage and computation. Compared to standard techniques
(e.g., COO, CSR, and CRS10) that store (row, column, value) of a sparse vector, our data structure
adopts a more efficient protocol that stores (defaults, positions, biases) leading to improvements
in computational efficiency, which gives us additional advantages in computational efficiency. We
reload the operations for such data structure including addition, scalar multiplication, dot product,
element-wise square root, L1 norm. We show the example of addition in Fig. 5.
9Here [n] = {1, . . . , n} and [t] = {0, . . . , t}.
10https://docs.scipy.org/doc/scipy/reference/sparse.html
13
Under review as a conference paper at ICLR 2020
Sparse Vector Representation
Default Value: 0.1
Size: 6
Entries:
0.1
0.1
+
Default Value: 0.1
Entries:
Default Value: 0.2
Entries:
+
Default Value: 0.3
Entries:
0.6	0.3
0.2
0.8	0.3
Index:
Index:
Index:
Figure 5: Sparse vector representation in αα-Rank.
B.2 Validity Check on Normal-form Games.
Our algorithm provides the expected ranking in all three normal-form games shown in Fig. 6, which
is consistent with the results in α-Rank (Omidshafiei et al., 2019).
Battle of sexes. Battle of sexes is an asymmetric game ROM=[ 03,,20 20,,30 ].αα
-Rank suggests that
populations would spend an equal amount of time on the profile (O,O) and (M,M) during the evolu-
tion. The distribution mass of (M,O) drops to 0 faster than that of (O,M), this is because deviating
from (M,O) for either player has a larger gain (from 0 to 3) than deviating from (O,M) (from 0 to 2).
0	-0.5	1
Biased Rock-Paper-Scissor. We consider the biased RPS game RRPS =	0.5 0	-0.1 . As it
is a single-population game, we adopt the transitional probability matrix of Eqn. 11 in Omidshafiei
et al. (2019). Such game has the inherent structure that Rock/Paper/Scissor is equally likely to be
invaded by a mutant, e.g., the scissor population will always be fixated by the rock population, there-
fore, our method suggests the long-term survival rate for all three strategies are the same (1,1,1).
Note this is different from the Nash equilibrium solution that is (焉,5,得).
Prison’s Dilemma. In prison’s dilemma RCD = [ 0-,1-, 3-1 --23,, 0-2 ], cooperation is an evolutionary
transient strategy since the cooperation player can always be exploited by the defection player. Our
method thus yields (D, D ) as the only strategy profile that could survive in the long-term evolution.
C Additional Details for Experiments
C.1 Hyper-parameters Settings
For all of our experiments, the gradient updates include two phases: warm-up phase and Adam
(Kingma & Ba, 2014) phase. In the warm-up phase, we used standard stochastic gradient descent;
after that, we replace SGD with Adam till the convergence. In practice, we find this yields faster
convergence than normal stochastic gradient descent. As our algorithm does column sampling for
the stochastic matrix (i.e. batch size equals to one), adding momentum term intuitively help stabilize
the learning. The warm-up step is 100 for all experiments
14
Under review as a conference paper at ICLR 2020
Ranking Intensity a	Ranking Intensity a	Ranking Intensity a
(a)	(b)	(c)
Figure 6: Ranking intensity sweep on (a) Battle of Sexes (b) Biased RPS (c) Prisoner’s Dilemma
We also implement infinite α Lanctot et al. (2019), when calculating transition matrix (or its col-
umn), where our noise term is set to be 0.01.
For most of our experiments that involve αα -rank, we set the terminating condition to be, when the
gradient norm is less than 10-9 . However, for Random Matrix experiment, we set the terminating
gradient norm to be 10-2
•	Learning rate to be in between 15 - 17
•	Alpha (ranking intensity) to be in between 1 - 2.5
•	Number of Population to be between 25 - 55 (in integer)
For all of the Adam experiments, after the warmup-step we chooses to decay δ and λ by 0.999 for
each time steps, where we have δ to always be 0.1. Similarly, λ starts at the value 0.5. However, in
speed and memory experiment, we chooses the decay to be 0.9
Experiments	m	ɑ	η_	Max. Iteration	βι_	λ	δ
NFG (WithOUt Self-Play)	-30^	10-4 〜102	1.0	^I000	0.9	ɪr	
NFG (Self-Play)	一	~30~	10-4 〜102	^003^	^I000	0.9	ɪr	
Random Matrix	n/a	n/a	^≡T	^TOO0	n/a	ɪr	^00Γ
Car Experiment (SGD)	^40^	T.0	T50^	-2000	0.999		
Car Experiment (Oracle)	^40^	T.0	ɪŋ-	^200	0.999		
ISing Model	~40~	90.0	一	~QT	4000	—	0.999	~0F	~0T~
List of symbols and names
•	Population size: m
•	Ranking Intensity: α
•	Learning rate: η
C.2 Self-driving Car Experiment
The environmental reward given to each agent is calculated by
Current Velocity
Collision Reward + Defa 止丫&]0协 X Proportional Speed Reward
Collision Reward is calculated when agent collided with either social car or other agents.
All of our value iteration agents are based on Leurent (2018) environment discretization, which
represents the environment in terms of time to collision MDP, taking into account that the other
agents are moving in constant speed. For all experiments, we run value-iteration for 200 steps with
the discounting factor of 0.99.
15
Under review as a conference paper at ICLR 2020
For each controllable cars, the default speed is randomized to be between 10 to 25, while the social
cars, the speed are randomized to be between 23 to 25.
We define five types of driving behaviors (one rational + four dangerous) by letting each controlled
car have a different ego reward function during training (though the reward we report is the envi-
ronmental reward which cannot be changed). By setting this, we can make sure, at upfront, the best
joint-strategy strategy should be all cars to drive rationally.
Table 2: Reward settings.
	Collision Reward	Proportional Speed Reward
Environment reward	^Σ0	^04
Rational driver	^Σ0	^04
Dangerous driver 1	T00	10.0
Dangerous driver 2	-20.0	T0.0
Dangerous driver 3	ʒθ	T0.0
Dangerous driver 4	40.0	10.0	—
16