Under review as a conference paper at ICLR 2020
Deep Learning-Based Average Consensus
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the problem of accelerating the linear average consensus
algorithm over complex networks. We specifically present a data-driven method-
ology for tuning the weights of temporal (i.e., time-varying) networks by using
deep learning techniques. We first unfold the linear average consensus protocol
to obtain a feedforward signal flow graph, which we regard as a neural network.
We then train the neural network by using standard deep learning technique to
minimize the consensus error over a given finite time-horizon. As a result of the
training, we obtain a set of optimized time-varying weights for faster consensus
in the complex network. Numerical simulations are presented to show that our
methodology can achieve a significantly smaller consensus error than the static
optimal strategy.
1	Introduction
The distributed agreement problem on networks, often referred to as a consensus problem (Olfati-
Saber et al., 2007), is an important problem in the network science and engineering, with applications
in load balancing (Cybenko, 1989), data fusion (Xiao et al., 2005), multi-agent coordination (Ren &
Beard, 2005), distributed computing (Xiao & Boyd, 2004), distributed sensor networks (Cortes &
Bullo, 2005), wireless communication systems (Senel & Akar, 2017), and power systems (Dorfler
& Bullo, 2012). Recently, it also appears in online machine learning procedures to process big
data (Chen & Sayed, 2012; Tsianos et al., 2012, and references therein).
In the average consensus problem, nodes in the network seek to converge their state variables to the
average of their initial states in a distributed manner. The standard solution to the average consensus
problem is to use the linear average consensus algorithm (Olfati-Saber & Murray, 2004), in which
each node updates its state by taking a weighted linear average of its own state and the state of its
neighbors. This algorithm results in a linear dynamical system whose state transition matrix involves
the Laplacian matrix of the underlying communication network.
Designing consensus algorithms with fast convergence speed is of significant practical interest be-
cause such algorithms allow the multi-agent systems to reach an agreement with fewer iterations and,
therefore, by consuming less communication resource. In the context of the linear average consen-
sus algorithm, the problem of finding the optimal weights of edges for maximizing the asymptotic
consensus speed can be reduced to a convex optimization problem (Xiao & Boyd, 2004), under
the assumption that the communication network is static and undirected. It was recently shown by
Kempton et al. (2018) that the optimal weights can be computed in a distributed manner by an iter-
ative computation. Zelazo et al. (2013) clarified the role of cycles in the linear average consensus
algorithm and presented a methodology for accelerating the consensus by adding edges to a network.
On the other hand, for the case of directed networks, Hao & Barooah (2012) presented a method to
accelerate the convergence rate of a linear (but not necessarily an average) consensus algorithm by
tuning the weights of edges in the network.
A natural consequence of seeking for further acceleration of consensus algorithms is the emergence
of finite-time consensus algorithms (Sundaram & Hadjicostis, 2007), in which edge-weights are
typically assumed to be time-varying and the designer exploits the additional flexibility to realize
consensus in a finite-time. The finite-time consensus algorithm proposed by Hendrickx et al. (2015)
achieves consensus by stochastic (but possibly asymmetric) matrices inN(N- 1)/2 iterations, where
N denotes the number of nodes in the network. Safavi & Khan (2015); Shang (2016) used tools from
graph signal processing (see, e.g., Shuman et al. (2013)) to show that, by allowing non-stochasticity
1
Under review as a conference paper at ICLR 2020
for the state-update matrices, one can realize a finite-time consensus in at most N steps. The theo-
retical aspects of these works have been further investigated by Apers & Sarlette (2017). Recently,
Falsone et al. (2018) showed that the number of steps required for consensus can be further improved
to N/2 in the specific case of ring networks having an even number of nodes.
Despite the aforementioned advances for consensus acceleration, there is still a lack of an effective
methodology for answering the following basic question: Given a finite time-window as well as an
underlying network structure, how should we dynamically tune the edge weights in the network for
achieving as accurate consensus as possible at the end of the time-window? If the length of the time-
window is not long enough to run the aforementioned finite-time consensus algorithms, currently
available options are effectively limited to using the static optimal strategies (e.g., Xiao & Boyd
(2004)), which does not allow us to dynamically tune the weights of the network. To fill in this gap,
in this paper we present a data-driven approach for tuning the weights of an undirected temporal (i.e.,
time-varying) networks by using deep learning techniques. We first unfold the consensus algorithm
and obtain a feedforward signal flow graph (Ito et al., 2019), which we regard as a neural network.
We then use the standard stochastic gradient descent algorithm to train the parameters in each layer
of the neural network (i.e., the weights of each snapshot of the temporal network) to minimize the
consensus error over a finite time-horizon, which results in an optimized temporal network for faster
consensus. We numerically confirm that our approach can drastically accelerate the convergence
speed in the linear average consensus algorithm.
This paper is organized as follows. In Section 2, we state the problem of dynamically tuning the edge
weights to accelerate the linear average consensus algorithm, and then we propose our methodology
for solving the problem using standard techniques in the field of deep learning. In Section 3, we
evaluate the performance of the proposed method with various numerical simulations. We finally
conclude the paper in Section 4.
2	Weight Optimization by Deep Learning Techniques
In this section, we describe our methodology for tuning the edge weights of the networks for acceler-
ating the linear average consensus algorithm within a given finite-time horizon. We first give a brief
review of the linear average consensus algorithm and state its basic properties. We then describe our
data-driven methodology for tuning the weights of the network, in which we apply the techniques in
the deep learning to the signal flow graph obtained by unfolding the consensus algorithm.
2.1	Linear average consensus algorithm
Let G be an undirected and unweighted network having the node set V = {1, . . . ,N} and the edge
set E consisting of unordered pairs of nodes in V. Each node in G represents an agent, which is
supposed to communicate with its neighbors at each time. In this paper, we focus on the discrete-
time case. Let xi(k) ∈ R denote the state of the ith node at time k ≥ 0, and Ni denote the set of
neighbors of node i. In the standard linear average consensus protocol (Olfati-Saber et al., 2007),
each node i updates its own state according to the following difference equation:
xi(k+ 1) = xi(k) + ∑ wij(k)(xj(k) -xi(k)), xi(0) =x0,i,	(1)
j∈Ni
where wij (k) = wji (k) ≥ 0 represents the weight of the (undirected) edge {i, j} at time k and x0,i is
the initial state of node i. For each time k ≥ 0, we define the (i, j) element of the adjacency matrix
of the network W(k) ∈ RN×N by
wi j (k), if j ∈ Ni,
0, otherwise,
and the degree matrix of the network at time k by
D(k)=diag(d1(k),...,dN(k)), di(k)= ∑ wij(k).
j∈Ni
Then, using the Laplacian matrix of the network
L(k) = D(k) -W(k),
Wij (k) =
2
Under review as a conference paper at ICLR 2020
x(0)	+ χ(1)	+ x(2) χ(T — 1)	+ X(T)	\^____/iw i2(0) W 21 (θ)ʌj^,ʃ`w 12(1) W 21 (1
X 1( T)
X 2(0)
X 2(2)
X 2( T )
(a)	Signal flow graph obtained by un-
folding the linear average consensus
algorithm (1) for A: = 0,... ,7 — 1, ⅛ =
diag[Ψ(⅛)l] — W(左)with z,y-element
0f W(fe)is Wij(k) if (i,j) ∈ E and oth-
erwise 0.
XNl T)
(b)	Corresponding neural network
Figure 1: Unfolded signal flow graph and corresponding neural network
the evolution of the state vector
M k ) = [X 1(k)	…XN(k )]>
in the linear average consensus protocol (1) is written as
X(k+1) = (I - L(k))X(k), X(0) =X0,
where
X0 = [X 1,0	…XN ,θ]>
denotes the initial state vector.
The objective of this paper is to present a framework for tuning the weights {Wij(k)}k≥ο,{i,j}∈E for
the faster average consensus in a given finite time window. Let us denote the average of the initial
states of the nodes by
1N
C = N ∑ XQ
Define the consensus error vector
e(k) =X(k)-c1,
where 1 denotes the all-one N-dimensional column vector. We are now ready to state the problem
studied in this paper.
Problem 2.1 (Consensus acceleration problem). Let G be an undirected and unweighted network
having N nodes. Let T be a positive integer. Assume that the set of initial states follow a probability
distribution X0, i.e.,
{ x0,1,…，X0,N }~X0.
Find the set of nonnegative weights
{ wij (k)} k ∈{0,1,…,T-1},{ i,j}e E
that minimizes the average consensus error defined by
γT = E[ke(T)k],
where k∙k denotes the Euclidean norm in RR,anEH denotes the eXpectedvalue.
Because Problem 2.1 is a non-convex problem, it is difficult to compute a set of {wij(k)}k≥0,{i,j}∈E
thatminimizesγT. This difficulty motivates us to tackle this problem using a data-driven approach
to find a suboptimal solution. In the next subsection, we describe our data-driven approach for tun-
ing the edge weights by using deep leaning techniques. We remark that, although we assume our
knowledge of the initial probability distribution X0 in the process of optimization, the optimized
edge-weights can drastically accelerate the consensus protocol even if the initial states do not fol-
low the given distribution. We numerically illustrate this universality property of our approach in
Subsection 3.4.
3
Under review as a conference paper at ICLR 2020
2.2 Data-driven weight optimization
To adjust the weights by using deep learning techniques, we first unfold the recursive state-update
formula (1) and obtain a signal-flow graph shown in Fig. 1a. Unlike a standard deep neural network,
the resulting neural network has a structure and contains no activation function. The structure of the
neural network corresponds to the structure of the graph G, and the same between all layers. The
neurons of kth layer corresponds to the nodes at time k as shown in Fig. 1b.
We then apply a standard technique in the field of deep learning to adjust the weights. We use the
mean squared error, γ2T, as the loss function, which is then regularized by appending a regularization
term of the Frobenius norm. As in Ito et al. (2019), we use the technique of the incremental training
for adjusting the weights during the training process. In the incremental training, we first consider
only the first layer (i.e., we setk = 1 in Fig. 1a) and attempt to minimize the regularized loss function
of the average consensus error γ12 + λ kW (0)kF using a number of randomly generated initial state
x0 as the training data, which we call the 1st generation. After training the first set of weights wi j (0),
we proceed to training the first two sets of edge weights by appending the second layer to the neural
network and replacing the loss function by γ22 + λkW(1)kF. In this training, we use the result from
the 1st generation as the initial value of the first layer and train the entire neural network. We repeat
this process to finally optimize the weights wij(T - 1) between T - 1st and Tth layers by minimizing
γT2 +λ kW(T - 1)kF. We train the network with a stochastic gradient descent algorithm.
3	Performance evaluation
In this section, we illustrate the effectiveness of the proposed method by various numerical simula-
tions. The weighting factor for the regularization is set to λ = 1, the number of data-set per learning
is set to 10 000, and the size of minibatch is one. For evaluations, 100 samples are used. With
the above setup, the simulations were performed in PyTorch (Paszke et al., 2017) using Adam with
learning rate 0.01 for training.
3.1	Baseline strategy
Throughout this section, we compare the performance of the proposed method with that of the static
optimal strategy presented in Xiao & Boyd (2004). Assume that the initial state x0 is a deterministic
vector. Let us further assume that the edge weights wij (k) do not depend on time k. Under these
assumptions, Xiao & Boyd (2004) have shown that the problem of finding the static edge weights
minimizing the (worst-case) asymptotic convergence factor
rasym = suplimsup	)1/k
x0 6=c1 k→∞	ke(0)k
(2)
reduces to solving a linear matrix inequality, which can be globally and efficiently solved (Boyd
et al., 1994). Then, as the baseline strategy, we use the following time-invariant consensus protocol
xi(k + 1)=xi(k)+ ∑ wistjat(x j(k) - xi(k)), 0 ≤ k ≤ T - 1,
j∈Ni
(3)
where wistjat are the static optimal weights obtained by solving the linear matrix inequality.
3.2	Deterministic networks
In this subsection, we use the following two empirical and synthetic deterministic networks; Karate
network (Zachary, 1977) (N = 34 nodes) and the square lattice network (N = 62 = 36 nodes). We
assume that the initial state of each node independently follows a uniform distribution on the inter-
val [-1, 1].
For Karate network, we set T = 10 and numerically optimized the edge weights of the network at
the times k = 0, . . . , 9. In Fig. 2, we present the optimized weights of edges in the network. We
then empirically evaluated the average consensus error E [ke(k)k] for k = 0, . . . , 9. The results are
shown in Fig. 3. The accuracy of the consensus achieved by the proposed method is about 50 times
better than the static optimal policy. We observe that the optimized weights of the network are
4
Under review as a conference paper at ICLR 2020
(b) k = 1
(c)k=2
(d) k = 3
(a)k=0
(f)k=5
(g)k=6
(h)k=7
(j)k=9
(k) Static optimal
Figure 2: Optimized weight of edges in the Karate network. (a)-(j): Proposed method. (k): Static
optimal strategy. The width of the lines indicate the values of the weights (the thicker a line is, the
larger its weight is). The edges having weight less than 10-2 are indicated by dashed lines.
(a) Proposed method
(b) Static optimal
Figure 3:	Empirical mean of the con-
sensus errors (Karate network)
Figure 4:	State trajectories (Karate network)
dynamically changing in a non-trivial manner. In Fig. 4, we show the sample trajectories of the
average consensus protocol with the static optimal and proposed edge weights. We confirm that the
proposed edge weights achieves a more precise average consensus at the final time k = 10 compared
with the static optimal strategy.
We then consider the average consensus on the square lattice network. We set T = 10 and numeri-
cally optimized the edge weights of the network. The results are shown in Figs. 5-7. The accuracy
of the consensus by the proposed method at the final time k = 10 is about 14 times better than the
static optimal methodology. The optimized weights show a trend similar to the one for the Karate
network. However, it is worth noting that the weights at time k = 4 are relatively large at various
edges in the network. This sudden increase in edge weights in fact drives the nodes away from
the consensus state but only temporarily. After all, despite this phenomena, the proposed approach
allows the nodes to achieve a better average consensus at the final time.
3.3	Random synthetic networks
We consider the following three random and synthetic network models: the ErdoS-Renyi (ER) net-
work (N = 100 nodes and M = 252 edges, where the probability for edge creation is 0.05) the
BarabaSi-Albert (BA) model (Barabasi & Albert, 1999)(N = 100 and M = 291, where the number
of edges to attach from a new node to existing nodes is 3), and the Watts-Strogatz (WS) model (Watts
& Strogatz, 1998) (N = 100 andM = 200, where each node is joined with its 4 nearest neighbors in
a ring topology, and the probability of rewiring each edge is 0.15). We set T = 10. As in the case
of the deterministic networks, we assume that the initial states of the nodes independently follow
a uniform distribution on the interval [-1, 1]. For each of the networks, we used the deep learning
technique to find the weights of the edges at times k = 0, . . . , 9. We then evaluated the empirical
5
Under review as a conference paper at ICLR 2020
(a)k=0
(b) k = 1
(c)k=2
(d) k = 3
(e)k=4
(f)k=5
(g)k=6
(h)k=7
(i) k = 8
(j)k=9
(k) Static optimal
Figure 5: Optimized weight of edges in the lattice network. (a)-(j): Proposed method. (k): Static
optimal strategy. The width of the lines indicate the values of the weights (the thicker a line is, the
larger its weight is). The edges having weight less than 10-2 are indicated by dashed lines.
(a) Proposed method
(b) Static optimal
Figure 6: Empirical mean of the con-
sensus errors (lattice network)
(a) ErdoS-Renyi network
(b) Barabasi-Albert model
Figure 7: State trajectories (lattice network)
(c) Watts-Strogatz model
Figure 8: Mean consensus errors for random network models.
average of the consensus error E[ke(k)k] for k = 0, . . . , 10. We show the results in Fig. 8. As in
the case of the deterministic networks in Subsection 3.2, the proposed method achieves significantly
less consensus errors at the final time.
We notice that, only in the case of the WS network, the consensus error temporarily and significantly
increases at time k = 5. In order to examine if this phenomena is specific to the WS model, the
following experiment was performed: For each of the three random graph models, we created 10
realizations of networks, for which we ran the proposed algorithm to obtain the optimized edge
weights. We then empirically computed the mean consensus errors E[ke(k)k] for k = 0, . . . , 10 for
each of the 3 × 10 cases. We show the results in Fig. 9. From the figure, we confirm that only the
case of WS network model presents a temporal increase in the mean consensus errors, while the
errors from the other two cases decrease almost monotonically.
6
Under review as a conference paper at ICLR 2020
Figure 9: Distribution of the mean consen-
sus errors for the ER, BA, and WS network
models. Solid lines and shaded areas repre-
sent the averages and the standard deviations,
respectively.
Table 1: Asymptotic convergence factors
	Proposed	Baseline
Karate	6.69 X 10-1	9.25 X 10-1
Lattice	7.23 × 10-1	9.21 X 10-1
ER	6.53 X 10-1	8.70 X 10-1
BA	5.85 X 10-1	7.79 X 10-1
WS	6.99 X 10-1	9.35 X 10-1
3.4 Periodic continuation
In the previous subsections, we have confirmed that the proposed method can drastically accelerate
the average consensus algorithm under the assumption that we are given a prespecified finite time-
window and that we know the distribution of the initial states. In this subsection, we further show that
a periodic continuation of our algorithm discussed in Subsection 2.2 yields an consensus algorithm
that effectively accelerates the consensus for any initial state vector and over an infinite time-window.
For given G and T, let L?(0), . . . , L?(T - 1) denote the optimized weighted Laplacian matrices of
the network. Then, the consensus algorithm proposed in Subsection 2.2 is written as
x(k+1) = (I - L?(k))x(k), 0 ≤k≤ T-1.
By periodically extending the state transition matrices I - L?(0), . . . , I - L?(T - 1), we obtain the
following average consensus protocol over an infinite time horizon:
ɪ (ST + T + 1)= (∏ (I - L ?(t -1))) ɪ (ST), 0 ≤ T ≤ T - 1, S ≥ 0.	(4)
The next lemma gives an explicit representation of the asymptotic convergence factor (2) of the
consensus algorithm (4).
Lemma 3.1. Let L?(0), . . . , L?(T - 1) denote the optimized weighted Laplacian matriceS of the
networkS by our deep learning algorithm. The aSymptotic convergence factor of the conSenSuS
algorithm (4) equalS
raTsym = sup lim sup
x0 6=c1 k→∞
∏ (I - L?(T - 1 -1))
t=0
(5)
where k ∙ k is the spectral norm.
Proof. First note that
k
e(k + 1) = ∏(I - L?(k -1))e(0).
t=0
By expressing k using S and T as
k = sT + τ, T ∈{0,…,T - 1},
7
Under review as a conference paper at ICLR 2020
we have
lim sup = lim sup	max
k一∞	s→∞ Te{0,…，T-1}
because T := {0,…,T - 1} is a finite countable set. Next, notice that
raTsym = sup lim sup max
x06=c1 S→∞ T∈T
1
X x (ST + τ + 1) - C lkʌ st+t+t
ke(0)k
sup lim sup mT ∈aTx
x0 6=c1 S→∞
ST+T
k∏ S=+T (I - L?(ST + TT) e (0)k
1
sT+τ+1
lim sup max ∏ (I -L?(ST + T -t))
S→∞
T∈T
t=0
ke(0)k
ll 1
(sT +τ+1)
≤ limSUPmax f Il ∏ (I - L?(t - 1 -1))
T-1
∏ (I - L?(T TT))
t=0
1
ST+τ+1
S
T-1
1
T
T-1
∏ (I - L?(T - 1 -1))
t=0
≤ lim sup max ∏ (I-L?(T - 1 -t))
S→∞ T∈T t=0
Here, we used
limsup y1/s = 1
S→∞
for a constant γ. On the other hand, we have
T	k	kXx(sT) -c 1∣∣∖ st
rasym ≥ suP limsuP 一容而可一
x06=c1 S→∞	ke(0)k
limsup Mnf(I-L?(T - 1 -1))
S→∞	t=0
1
S
1
T
S
T-1
∏ (I - L?(T - 1 -1))
t=0
This completes the proof.
□
Lemma 3.1 states that raTsym is the geometric mean of the spectral norm of the product of T -step
transition matrices of I - L?(T - 1 -1), t = 0,…，T - 1. Note that rTsym with T = 1 does not
necessarily correspond to the static-optimal in (2).
Using Lemma 3.1, we computed the asymptotic convergence factor of the consensus algorithm (4)
with T = 10 for each of the five networks (i.e., Karate, lattice, ER, BA, and WS networks). We
also computed the asymptotic convergence factor of the baseline strategy (3) for each of the five
networks. The obtained asymptotic convergence factors are given in Table 1. We observe that the
proposed method achieved less convergence factors, which shows the effectiveness of the proposed
approach even in the case of infinite time-horizon problems.
4 Conclusion
In this paper, we have presented a data-driven approach for accelerating the linear average consensus
algorithm over undirected temporal networks. We have first unfolded the consensus algorithm to
obtain an equivalent feedforward signal flow graph, which we have regarded as a neural network.
We have then showed that we can apply standard deep learning techniques to train the obtained
neural network and obtain a temporal network having optimized edge-weights. We have numerically
confirmed that our methodology can outperform the average consensus algorithm with the static
optimal edge-weights.
8
Under review as a conference paper at ICLR 2020
References
Simon Apers and Alain Sarlette. Accelerating consensus by spectral clustering and polynomial
filters. IEEE Transactions on Control ofNetwork Systems, 4(3):544-554, 2017.
Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. Science, 286
(5439):509-512, 1999.
S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities in System and
Control Theory. Society for Industrial Mathematics, 1994.
Jianshu Chen and Ali H. Sayed. Diffusion adaptation strategies for distributed optimization and
learning over networks. IEEE Transactions on Signal Processing, 60(8):4289-4305, 2012.
Jorge Cortes and Francesco Bullo. Coordination and geometric optimization via distributed dynam-
ical systems. SIAM Journal on Control and Optimization, 44(5):1543-1574, 2005.
George Cybenko. Dynamic load balancing for distributed memory multiprocessors. Journal of
Parallel and Distributed Computing, 7(2):279 - 301, 1989.
Florian Dorfler and Francesco Bullo. Synchronization and transient stability in power networks and
nonuniform Kuramoto oscillators. SIAM Journal on Control and Optimization, 50(3):1616-1642,
2012.
Alessandro Falsone, Kostas Margellos, Simone Garatti, and Maria Prandini. Finite-time distributed
averaging over gossip-constrained ring networks. IEEE Transactions on Control of Network Sys-
tems, 5(3):879-887, 2018.
He Hao and Prabir Barooah. Improving convergence rate of distributed consensus through asym-
metric weights. In American Control Conference, pp. 787-792, 2012.
Julien M. Hendrickx, Guodong Shi, and Karl H. Johansson. Finite-time consensus using stochastic
matrices with positive diagonals. IEEE Transactions on Automatic Control, 60(4):1070-1073,
2015.
Daisuke Ito, Satoshi Takabe, and Tadashi Wadayama. Trainable ISTA for sparse signal recovery.
IEEE Transactions on Signal Processing, 67(12):3113-3125, 2019.
Louis Kempton, Guido Herrmann, and Mario Di Bernardo. Self-organization of weighted networks
for optimal synchronizability. IEEE Transactions on Control of Network Systems, 5(4):1541-
1550, 2018.
Reza Olfati-Saber and Richard M. Murray. Consensus problems in networks of agents with switch-
ing topology and time-delays. IEEE Transactions on Automatic Control, 49(9):1520-1533, 2004.
Reza Olfati-Saber, J. Alex Fax, and Richard M. Murray. Consensus and cooperation in networked
multi-agent systems. Proceedings of the IEEE, 95(1):215-233, 2007.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In 31st Conference on Neural Information Processing Systems, 2017.
Wei Ren and Randal W. Beard. Consensus seeking in multiagent systems under dynamically chang-
ing interaction topologies. IEEE Transactions on Automatic Control, 50(5):655-661, 2005.
Sam Safavi and Usman A. Khan. Revisiting finite-time distributed algorithms via successive nulling
of eigenvalues. IEEE Signal Processing Letters, 22(1):54-57, 2015.
Kamil Senel and Mehmet Akar. A distributed coverage adjustment algorithm for femtocell networks.
IEEE Transactions on Vehicular Technology, 66(2):1739-1747, 2017.
Yilun Shang. Finite-time weighted average consensus and generalized consensus over a subset.
IEEE Access, 4(8):2615-2620, 2016.
9
Under review as a conference paper at ICLR 2020
David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The
emerging field of signal processing on graphs: Extending high-dimensional data analysis to net-
works and other irregular domains. IEEE Signal Processing Magazine, 30(3):83-98, 2013.
Shreyas Sundaram and Christoforos N Hadjicostis. Finite-time distributed consensus in graphs with
time-invariant topologies. In American Control Conference, pp. 711-716, 2007.
Konstantinos I. Tsianos, Sean Lawlor, and Michael G. Rabbat. Consensus-based distributed opti-
mization: Practical issues and applications in large-scale machine learning. In Annual Allerton
Conference on Communication, Control, and Computing, pp. 1543-1550, 2012.
Duncan J. Watts and Steven H. Strogatz. Collective dynamics of ’small-world’ networks. Nature,
393(6684):440-442, 1998.
Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control
Letters, 53(1):65-78, 2004.
Lin Xiao, Stephen Boyd, and Sanjay Lall. A scheme for robust distributed sensor fusion based
on average consensus. In Fourth International Symposium on Information Processing in Sensor
Networks, pp. 63-70, 2005.
Wayne W Zachary. An information flow model for conflict and fission in small groups. Journal of
Anthropological Research, 33(4):452-473, 1977.
Daniel Zelazo, Simone Schuler, and Frank AllgoWer. Performance and design of cycles in consensus
networks. Systems and Control Letters, 62(1):85-96, 2013.
10