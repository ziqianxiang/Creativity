Under review as a conference paper at ICLR 2020
Regularly varying representation for sen-
TENCE EMBEDDING
Anonymous authors
Paper under double-blind review
Ab stract
The dominant approaches to sentence representation in natural language rely
on learning embeddings on massive corpuses. The obtained embeddings have
desirable properties such as compositionality and distance preservation (sentences
with similar meanings have similar representations). In this paper, we develop a
novel method for learning an embedding enjoying a dilation invariance property.
We propose two algorithms: Orthrus, a classification algorithm, constrains the
distribution of the embedded variable to be regularly varying, i.e. multivariate
heavy-tail. and uses Extreme Value Theory (EVT) to tackle the classification task
on two separate regions: the tail and the bulk. Hydra, a text generation algorithm
for dataset augmentation, leverages the invariance property of the embedding learnt
by Orthrus to generate coherent sentences with controllable attribute, e.g. positive
or negative sentiment. Numerical experiments on synthetic and real text data
demonstrate the relevance of the proposed framework.
1	Introduction
Representing the meaning of natural language in a mathematically grounded way is a scientific
challenge that has received increasing attention with the explosion of digital content and text data in
the last decade. Relying on the richness of contents, several sentence embeddings have been proposed
(Peters et al. (2018); Radford et al. (2018); Devlin et al. (2018)) with demonstrated efficiency for
the considered tasks. These embeddings, learnt on massive datasets, are commonly used to perform
downstream tasks such as classification or data generation.
In a classification context, e.g. when the goal is to distinguish between positive and negative
sentiments in user reviews, most classifiers are based on Empirical Risk Minimization (ERM)
strategies and variants of it. However, nothing guarantees that such classifiers perform satisfactorily
on the tails of the explanatory variables, i.e. in regions attached to rare events or even outside the
domain of observed data. The present paper builds upon the methodological framework proposed by
(Jalalzai et al. (2018)) performing classification in extreme regions with guarantees concerning the
excess risk of the ERM classifier. Their approach relies on multivariate extreme value theory (EVT)
and is valid under regularity assumptions concerning the multivariate distributional tail which appear
to be an homogeneity property above large thresholds: the tail behaviour at infinity is similar to the
behaviour of polynomial function near infinity (see equation 1 below). To wit, it is necessary for the
multivariate survival function of the considered random vector to be approximately homogeneous
of degree -1, up to appropriate marginal standardization. Although there is experimental evidence
that their framework improves the performance of baseline classifiers in extreme regions on low
dimensional data, there is no reason to assume that the previously mentioned text embeddings satisfy
the required regularity assumptions. The aim of the present work is to apply Jalalzai et al. (2018)’s
methodology to text data represented by state of the art embeddings. To achieve this, we train the
algorithm to perform a transformation that allows it to map any text embedding X onto a random
vector Z which satisfies the aforementioned regularity assumptions. The transformation is learnt
by an adversarial strategy (Goodfellow et al. (2016)). As a by-product, we obtain a novel data
augmentation mechanism which takes advantage of the scale invariance properties of Z to generate
synthetic sentences that keep invariant the attribute of the original sentence.
The work detailed in this paper can be connected to other articles (Socher et al. (2013); Guu et al.
(2018); Siffer et al. (2017)), though our framework is different. Our contribution is twofold. We
1
Under review as a conference paper at ICLR 2020
introduce (i) a novel classification algorithm (Orthrus), taking as input an embedded sentence to map
it onto a heavy-tailed vector in the latent space satisfying Equation 1. (ii) a semi-supervised data
augmentation method (Hydra), which leverages the homothetic invariance property of the embedding
learnt by Orthrus to produce new sentences with prescribed label.
Label preserving data augmentation, which increases the number of training samples in machine
learning systems, is an effective solution to data insufficiency and is an efficient pre-processing step
in computer vision (Wang & Perez (2017)). Due to the discrete nature of text, data augmentation
is a much harder problem in natural language processing (NLP) than in computer vision. Current
work in natural language (NL) dataset augmentation with label preservation mainly relies either on
back-translation (Shleifer (2019)), on synonym replacements (Kobayashi (2018)), on slot filling (Hou
et al. (2018)) or on the use of handcrafted heuristics (Wei & Zou (2019)) (e.g swap, deletion, synonym
replacements). Unlike other state of the art solutions, Hydra can generate multiple sentences with a
considered sentiment using a unique input sentence. Hydra does not require any additional resources
(e.g. synonym dictionary), nor does it rely on hand-crafted heuristics, or use style transfer methods
(Hu et al. (2017); Colombo et al. (2019)).
In this paper, we work with output vectors issued by BERT sentence embedding as inputs (Devlin
et al. (2018)). BERT is currently one of the state-of-the art algorithms for sentence embedding. We
demonstrate on both synthetic and real datasets that the embedding learnt by Orthrus on top of BERT
indeed follows a heavy-tail distribution, and that Orthrus is able to outperform a state-of-the art
classifier built on BERT. On the dataset augmentation task, quantitative and qualitative experiments
demonstrate the ability of Hydra to generate new sentences while preserving labels.
The rest of this paper is organized as follows. Section 2 introduces the necessary background in
multivariate extremes and adversarial learning. The methodology we propose is detailed at length in
section 3. Illustrative numerical experiments on both synthetic and real data are gathered in section 4.
Additional experimental results are available in the supplementary material.
2	Background
2.1	Heavy tails and regular variation
By definition, heavy-tail phenomena are those which are ruled by very large values, occurring with
a far from negligible probability and with significant impact on the system under study. When the
phenomenon of interest is described by the distribution of a univariate random variable, the theory
of regularly varying functions provides the appropriate mathematical framework for the study of
heavy-tailed distributions. For the sake of clarity and in order to introduce notations we recall some
related theoretical background. One may refer to (Resnick (2013)) for an excellent account of the
theory of regularly varying functions and its application to the study of heavy-tailed distributions.
A random variable X with cumulative distribution function (c.d.f.) F is heavy-tailed of index α > 0
if and only if for any fixed x > 0:
nP {X/F-1(1 - 1/n) > x} -→ x-α
n→∞
where F-1(u) = inf {t : F(t) ≥ u} denotes F’s generalized inverse. Based on this characterization,
the heavy-tail model is classically extended to the multivariate setup as follows. Consider now a
d-dimensional random vector X = (X(1), . . . , X(d)) taking its values in Rd+. Then X is said to be
heavy-tailed with tail index α > 0 if there exists a positive Radon measure μ on the punctured set
[0, ∞]d∖{0} and a function b(t) → ∞ such that:
tP {X∕b(t) ∈ A}——→ μ(A)
t→∞
(1)
For any Borelian set A ⊂ [0, ∞]d which is bounded away from 0 and such that the measure μ of the
boundary ∂A is zero. In such a case, μ fulfills the homogeneity property μ(tA) = t-αμ(A) for all
t > 0 and any set A in [0, ∞]d∖{0}. Using the homogeneity property, one may show that μ can be
decomposed into a radial component and an angular component Φ, which are independent from each
other. Indeed, for all x = (x1 , . . . , xd) ∈ Rd, set
R(x)
kxk
Θ(x)
∈ S,
(2)
2
Under review as a conference paper at ICLR 2020
Figure 1: Colored cones correspond to a given label from the classifier on the simplex.
where S is the positive orthant of the unit sphere in Rd for the chosen norm k .∣∣. The choice of the
norm is unimportant as all norms are equivalent in Rd. Then, for every B ⊂ S, we have:
μ[{x : R(x)∕b(t) >t, Θ(x) ∈ B}]= t-αΦ(B),
where Φ is referred to as the spectral or angular measure.
2.2 Classification in Extreme Region
In the standard setup of binary classification, (X, Y ) ∈ Rd+ × {-1, 1} is a random pair defined on
a certain probability space with unknown joint probability distribution P. Now, equipped with the
concepts introduced in the previous section we shall assume regular variation for both classes with
the same tail index. Stated otherwise (as formulated in (Jalalzai et al. (2018))) we shall make the
assumption that:
tP {X ∈ tA | Y = σ1} → μσ (A)
where σ ∈ {-1, +1}, A ⊂ Rd+, 0 6∈ ∂A. If one considers (X∞, Y∞) a random pair defined
by P{X∞ ∈A,Y∞ =+1} = limt→∞, P{X ∈tA,Y=+1 | ||X|| >t}, P{Y∞ =+1} =
limt→∞ P {Y = +1 | ||X || > t}, then one can prove (see Theorem 1 in Jalalzai et al. (2018)) that
the Bayes regression function η∞(χ) = P {Y∞ = 1∣X∞ = χ}for the limiting pair is constant along
rays i.e. is a function of Θ(x) ∈ S only. The same is true of the Bayes classifier for the asymptotic
pair. As a consequence, the optimal classifiers on extreme regions are based on indicator functions
of truncated cones on the kind {||x|| > r, Θ(x)B}, where B ⊂ S. As a consequence, the opti-
mal classifiers on extreme regions are based on indicator functions of truncated cones on the kind
{kxk > r, Θ(x) ∈ B}, where B ⊂ S (see Figure 1). This observation justifies an ERM strategy
using the extreme points of the dataset to train an angular classifier.
Let {(Xi, Yi)}in=1 be n i.i.d copies of (X, Y). By sorting the training observations by decreasing
order of magnitude, we introduce X(i) (with corresponding sorted label Y(i)) the i-th order statistics,
i.e. ||X(1) || ≥ . . . ≥ ||X(n) ||. Let GS be a class of classifiers defined on the sphere S with finite VC
dimension VGS < ∞. Thus for any g ∈ GS and any θ ∈ S, g(θ) ∈ {-1, 1}. By extension, for any
X ∈ r+ we define g(χ) = g(θ(x)) ∈ { —1,1}.
Let tτ be the quantile at level (1 - τ) of the r.v ||X|| where τ > 0 corresponds to a small fraction of
extreme observations: P {||X|| > tτ} = τ. Set k = bnτc and consider the empirical risk dedicated
to the extreme samples:
1k
Lk(O)= k ΣS 1{Y(i) = g(O(X(i)))},
i=1
which corresponds to the empirical version of Ltτ, the loss at level tτ. Now consider solutions to the
minimization problem:
min Lbk (g)	(3)
g∈GS
Thence the authors of Jalalzai et al. (2018) provide guarantees concerning the classification risk in
out-of-sample regions.
3
Under review as a conference paper at ICLR 2020
Theorem 1 (Jalalzai et al. (2018)) Let gbk be any solution of equation 3. Recall k = bnτ c. Then,
for δ ∈ (0, 1), ∀n ≥ 1, we have with probability larger than 1 - δ:
Ltτ (bk) - L；T ≤√1k (P2(1-τ )log(2∕δ) + C PVGS "/O)
+ ； (5 + 2log(l0 + Plog(I©(CPVGS + √2)) + |gif Ltτ (g)- L；T},
where C is a constant independent from n, τ and δ.
2.3 Adversarial learning
Adversarial networks, introduced in (Goodfellow et al. (2014)), form in a system where two neural
networks are competing. A first model called the generator generates samples as close as possible to
the input dataset. A second model called the discriminator aims at distinguishing samples produced
by the generator from the input dataset. The goal of the generator is to maximize the probability of
the discriminator making a mistake. Hence, if Pinput is the distribution of the input dataset then the
adversarial network intends to minimize the distance (as measured by the Jensen-Shannon divergence)
between the distribution of the generated data PG and Pinput.
Auto-encoders and derivations (Goodfellow et al. (2016); Laforgue et al. (2018); Fard et al. (2018))
shape a subclass of neural networks whose purpose is to learn a suitable representation by learning
encoding and decoding functions which capture the core properties of the input data. An adversarial
auto-encoders (see Makhzani et al. (2015)) is a specific kind of auto-encoder where the code plays
the role of the generator of an adversarial network. Thus the latent code is forced to follow a given
distribution while containing information relevant to reconstructing the input.
3	Dilation Invariant Representation
3.1 Learning a dilation invariant representation
We now introduce Orthrus, a novel algorithm for classification of text data represented by high
dimensional vectors as issued by pre-trained embeddings such as BERT which is assumed to not
satisfy the regular variation assumption. The idea behind Orthrus is to modify the output X of BERT
in order to increase the information carried by the resulting representation Z =夕(X) regarding
the label Y , including in the upper tail regions of Z corresponding to low probability sentences,
in order to improve the performance of a downstream classifier. This is achieved by training an
encoding function 夕 in such a way that (i) the marginal distribution q(z) of the code Z be close
to a user-specified heavy tailed distribution p satisfying the regularity property (1) ; and (ii) the
classification loss of a multilayer perceptron trained on the code Z be small. A major difference
distinguishing Orthrus from existing auto-encoding schemes is that the target distribution on the
latent space is not chosen as a Gaussian distribution but as a heavy-tailed one.
As the Bayes classifier in the extreme region has a potentially different structure from the Bayes
classifier on the bulk (recall from Section 2 that the regression function at infinity depends on
the angle Θ(x) only), Orthrus trains two different classifiers, Cext on the extreme region of the
latent space on the one hand, and Cbulk on its complementary set on the other hand. Given a high
threshold t, the extreme region of the latent space is defined as the set {z : kz k > t}. In practice,
the threshold t is chosen as an empirical quantile of order (1 - κ) (for some small, fixed κ) of the
norm of encoded data ∣∣Zik = k“Xi) ∣∣. In the end, the classifier issued by Orthrus is of the kind
C(z) = Cext(z)1{kzk > t} + Cbulk(z)1{kzk ≤ t} . Our goal is to minimize the weighted risk
R(φ, Cext, Cbulk) = λιP {Y = Cext(Z), ∣Zk ≤ t} + λ2P {Y = Cbulk(Z), ∣Zk > t}
+ λ3D(q(z), p(z)),
where Z =夕(X), d is the Jensen-Shannon distance between the heavy tailed target distribution
p and the code distribution q, and λ1 , λ2 , λ3 are positive weights. Following common practice in
the adversarial literature, the Jensen-Shannon distance is approached (up to a constant term) by the
4
Under review as a conference paper at ICLR 2020
♦	q，	∖	q， t-λ∖ ∖ Ji
empirical proxy L(q, p) = supD∈Γ L(q, p, D)), with
m
1
L(q,P,D)=嬴 ElogD(Zi) + log(1 - D(Zi))
m i=1
where Γ is a wide class of discriminator functions, and where independent samples Zi, Zi are
respectively sampled from the target distribution and the code distribution q. In the end, Orthrus
solves the following min-max problem infceχt,cb∏ik,φ SuPD Rb(φ, Cext, Cbulk, D) where
k	n-k
Rb(ψ, Cext, Cbulk) = IIX i{Y(i) = Cext(Z(i))} + n-k X i{Y(i) = Cbulk(Z(i))} + …
…λ3 L(q,p)),
where {Z(i)=夕(X(i)), i = 1,...,n} are the encoded observations with associated labels Y(i)sorted
by decreasing magnitude of ∣∣Z∣∣ (i.e. ∣∣Z(i)k ≥ ∙∙∙ ≥ ∣∣Z(n) k), and k = [κn∣ is the number of
extreme samples among the n encoded observations.
This minimization problem results in Algorithm 1 detailed below, where m is the batch size, ρ is the
proportion of the points given to train the extreme classifier and the cost function ` is the negative
log-likelihood.
3.2 A dilation invariant representation for dataset augmentation
We now introduce Hydra, a data augmentation algorithm, which relies on the dilation invariance of
labels on the extreme embeddings learnt by Orthrus. Let U = (u1, . . . , uT) be a sequence of inputs
of length T . The dataset augmentation problem consists in generating multiple sentences {Ui0}i∈N
such that each sequence Ui0 is label coherent with U. Hydra follows the seq2seq approach (Sutskever
et al. (2014)) and learns two decoders with attention (Bahdanau et al. (2014)), one for each region
of the representation previously learnt by Orthrus. On the whole, a decoder takes an input Z (the
latent code) and generates an output sequence Ui0 = (u0i , . . . , u0i 0 ), where each word u0i is in the
vocabulary. To generate an output word u0k, the decoder iteratively takes as input the previously
generated word u0k-1 (u00 being a start symbol), updates its internal state, and returns the next word
with highest probability. This process is repeated until the decoder generates either a stop symbol or
the length of the generated sequence reaches the maximum sequence length.
Hydra is a semi-supervised algorithm in that it requires a first labeled dataset Dn to learn embeddings
through Orthrus and a second dataset Dgn (not necessarily labeled) to train a decoder Gext (resp
Gbulk) for the extremes (resp non extremes) on Dgn. The learning is carried out by optimising the
classical negative log likelihood of individual tokens `gen .A detailed description of Hydra is provided
in Algorithm 2 below.
After Hydra has been trained, the data augmentation proceeds as follows: For each input X with
extreme embedding, multiple sentences with constant label are obtained by applying a λ dilation
(see Figure 2b). In other words, if Gext is a decoder learnt by Hydra and applicable to the extreme
region of the representation, synthetic sentences of the form Gext(λz) can be obtained, with the same
attribute as z .
X以X )
X -----
Y ∈{-ι,i}
/Cext
÷ Z
X^Cbulk
Y ∈{-ι,i}
中(X)
X 丫' ' >
Z
λ1 Z
GeXt
λN Z
Gbulk
———>
s1
.
.
.
sN
s
(
(a) Orthrus
(b) Hydra
Figure 2: Pipeline of the introduced algorithms.
5
Under review as a conference paper at ICLR 2020
Algorithm 1 Orthrus
INPUT: Coef. λ1,λ2,λ3 > 0, Training dataset
Dn = {(U1, Y1), . . . , (Un, Yn)}, proportion
κ ≤ 1 of extreme observations, batch size m ≤ n.
Initialization: parameters of the encoder ψτ, Classi-
fiers, Cθext, Cθb0ulk, decoders and discriminator Dγ
while (τ, θ, θ0) not converged do
Sample {(U1, Y1) . . . , (Um, Ym)} from Dn.
Sample {Z1, . . . , Zm} from the prior PZ.
Sample Zi from 中T(Z|Xi) for i ∈ {1,..., m}.
Update Dγ by ascending:
m
m X log DY (Zi) + * Iog(I- DY (Zi)).
m i=1
Sort {Zi}i∈{1,...,m} by decreasing order of magni-
tude ∣∣Z(i)∣∣ ≥ ... ≥ ||Z(m)||.
Update Cθext by descending:
bκmc
Lext =f bκλmc x '(Y(i),cext(¾)).
bκmc i=1
Update Cθbu0lk by descending:
m
LbUlk 理 m - Kml X	'(Y(i), Cbulk(Z(i))).
i=bκmc+1
Update 中T by descending:
1m
ɪ X -λ3 ∙ log DY(Zi) + Lext + Lbulk.
m
i=1
end while
COmPUte {Zi}i∈{1,...,n} = W(Xi)i∈{ι,...,n}
Sort {Zi}i∈{1,...,n} by decreasing order of magni-
tude ||Z⑴ || ≥ ... ∣∣Z(［κnC)∣∣ ≥ …≥ ||Z(n)||.
OUTPUT: encoder W, classifiers Cθext applicable on
the region {x : ∣∣w(x)∣∣ ≥ |邑［5)||} and
Cθbulk applicable on the region {x : ||W(x) || <
..~ ....
||Z(bKn」)||}.
Algorithm 2 Hydra
INPUT: Coef. λ1, λ2, λ3 > 0, Training datasets
Dn = {(U1, Y1), ..., (Un,Yn)}, Dgn =
{Ug1 , . . . , Ugn } batch size m ≤ n,
Initialization: parameters of the encoder W, classi-
fiers, Cθext, Cθb0ulk , decoders, Geψxt, Gbψu0lk and dis-
criminator DY
Optimization:
W,Cext,Cbulk =Orthrus(λ3,Dn,)
while (ψ, ψ0) not converged do
Sample {(Xg1 ) . . . , (Xgm)} from the training set
Dgn.
Sample Zi from W(Z|XgJ for i ∈ {1,...,m}.
Sort {Zi}i∈{1,...,m} by decreasing order of magni-
tude
..ɪ . . ɪ
11z(1)11 ≥ ... ≥ llZ(m)ll∙
Update Geψxt by descending:
bρmc
Lgxt == bρmC X 'gen.(¾,Gψxt(-Z(i))).
Update Gbψu0lk by descending:
m
bulk def	λ2	bulk
L = m - 1 0m∣ 工	'gen. V(i),GΨ0 (Z(i))J.
ρ	i=bρmc+1
end while
Compute {Zi}i∈{1,...,n} = W(Xi)i∈{1,...,n}
Sort {Zi}i∈{1,...,n} by decreasing order of magni-
tude ||Z⑴|| ≥ ... ||Z(k)|| ≥ ... ≥ ||Z(n)||.
OUTPUT: encoder W, decoder Gext applicable on the
region {x : ∣∣w(x)∣∣ ≥ |必［环)||} and Gbulk ap-
plicable on the region {x : ∣∣w(x)∣∣ < ∣∣Z"κnC)∣∣}.
Algorithm: Orthrus and Hydra. For both algorithm X is the embedding provided by BERT to an
input sentence U.
6
Under review as a conference paper at ICLR 2020
4	Numerical Experiments
Heavy-tail distribution: In the remaining of this paper, the regularly varying target distribution
is a logistic distribution with parameter δ = 0.9 (see Appendix B.2). The logistic distribution is
widely used in the context of extreme values analysis. This distribution is simulated according
to Stephenson (2003) which is defined in Rd with parameter δ ∈ (0, 1] by its parametric c.d.f.
F(x) = exp { - (Pd=I x(j)1 )δ}. Figure 4 in Appendix illustrates this distribution with various
values of δ.
Experimental Settings: Classifiers Cbulk , Cext are Multi Layer Perceptrons (MLP). For each dataset,
one-fourth of data is retained as a test set while the remaining data points are used as the train set.
We denote by T the extreme test set region as selected with Algorithm 1. The considered norm is the
'∞ norm. For the generation task, the dilation coefficient varies between 1 and 1.5 (λ ∈ [1,1.5]).
4.1	Application to Sentiment Analysis
We assess Othrus ability to perform sentiment classification tasks using two balanced binary labeled
datasets: Yelp, Amazon introduced by (Kotzias et al. (2015)). Further results on the datasets are
reported in the Appendix. Each dataset is balanced and is composed of 1000 sentences and each
sentence expresses a strong positive or negative sentiment.
Baseline: Orthrus is compared with a state of the art classifier: a MLP trained on the initial BERT
embedding (NN) taking as input the same training data.
Classification performance: Table 1 illustrates the performance of considered models in terms of
Hamming loss on the test set. Performance of Orthrus is better than NN on both dataset on the
whole test set. The difference in terms of performance is even greater when considering the extreme
samples.
Model	Hloss All	Hloss Extreme	Model	Hloss All	Hloss Extreme
NN	-0.30	0.30	NN	-0.33	0.32
Orthrus	0.26	0.24	Orthrus	0.29	0.22
(a)	(b)
Table 1: Hamming loss (Hloss) on the Yelp (Table 1a) and Amazon (Table 1b) datasets. The first
column transcribes the loss on the whole test set while the second column transcribes the loss on the
extreme samples.
4.2	Application to dataset augmentation for labelled sentences
Comparison to related work: We compare Hydra with two state of the art methods for dataset
augmentation: EDA (Wei & Zou (2019)) and round-trip translation (Shleifer (2019)). Compared
to EDA which uses heuristics and a synonym dictionary, Hydra generates grammatically correct
sentences and conserve the labels. Compared to round-trip translation Hydra can generate multiple
sentences for a single input and conserves sentence labels. We also compare Hydra to a Vanilla
Sequence to Sequence to demonstrate the validity of our approach. In this section, we validate
Orthrus using the Yelp dataset. To train Orthrus the dataset Dn is the Yelp dataset introduced in
(Kotzias et al. (2015)). Dgn is a subset of the full Yelp Corpus composed of 5 ∙ 105 sentences.
Evaluation: Automatic evaluation of generative models for text is still an open research problem.
We evaluate our models through three criteria (C1, C2, C3). C1 measures Cohesion (Crossley
& McNamara (2010)) (Are the generated sentences grammatically and semantically consistent?).
C2 (named Sent. in Table 1a.) evaluates label conservation (Does the expressed sentiment in the
generated sentence match the sentiment of the input sentence?). C3 measures the diversity of the
sentences (Is the dataset augmented with diverse sentences?)
We carry out two types of evaluations: a qualitative one based on a user study - that provides
evaluation through C1 and C2 - and a quantitative one - that provides evaluation through C3
measures. We measure diversity (C3) with two automatic methods: (1) Distinct n, the diversity
measure introduced by (Li et al. (2015)) which relies on a n-grams count; (2) a F1 score that reports
the improvement induced by the data augmentation while training a fastText (Joulin et al. (2016))
classifier. This improvement reflects the information added by newly generated sentences.
7
Under review as a conference paper at ICLR 2020
Model	Sent	Cohesion	Model	F1	Distinct 1	Distinct 2
Raw	0.90	0.85	Raw	0.87	0.385^^	0.817
Back	0.78	0.84	Back	0.87	0.405	0.816
Eda	0.87	0.68	Eda	0.95	0.387	0.813
Seq2seq	0.60	0.87	Seq2seq	0.807	0.375	0.813
Hydra	0.88	0.90	Hydra	0.95	0.457	0.844
(a)	(b)
Table 2: Evaluation of Hydra with user evaluation (Table 1a) and with automatic metrics (Table 1b).
input	penne vodka excellent!	input	i love their fries and their beans.
λ=1	penne vodka splendid!	λ = 1	i love their fries and their bananas.
λ = 1.2	the penne vodka is excellent!	λ = 1.1	i like their pies and their bananas.
λ = 1.3	the penne vodka is excellent!	λ = 1.3	i enjoy her fries and her beans.
λ = 1.5	the penne vodka is perfect!	λ = 1.5	i enjoy enjoy enjoy and beans.
input	awful service.	input	awesome selection of beer.
λ=1	bad maintenance.	λ = 1	great selection of beer.
λ = 1.1	a terrible service.	λ = 1.1	a superb selection of beer.
λ = 1.3	it,s a terrible service.	λ = 1.3	an amazing selection of beer.
λ = 1.5	horrible service.	λ = 1.5	great choice of beer.
Table 3: Sentences generated by Hydra for extreme embeddings implying label (sentiment) invariance
for generated sentence. λ is the dilation factor.
Qualitative Evaluation: We compare the different methods on the extreme set of sentences according
to Hydra. As previously, the extreme test set T consists of 92 sentences. For each sentence, we
randomly sample λ ∈ [1, 1.5] and generate a new sentence with Hydra. To compare our method,
we use Eda, round-trip translation and a Vanilla Seq2seq to generate new sentences. Results of the
qualitative evaluation are reported in Table 2a. As expected Hydra exhibits comparable performance
with all other models except Eda in terms of cohesion. Such a result is expected since Eda’s use
of random swaps or permutations may result in ungrammatical sentences. In terms of expressed
sentiments, Hydra outperforms Seq2seq and round-trip translation, demonstrating the relevance of
the proposed approach.
Quantitative Evaluation: We report in (Table 2b) the results of evaluation for C3. We observe
that Hydra outperforms all other methods in term of distinct 1 and distinct 2. Table 1b shows that
improvement in F1 score induced by dataset augmentation by Hydra beats all other methods and is
only equalled by EDA.
Example of generated sentences: We report in Table 3 sentences generated by Hydra. While we
make no claim that these generated sentences are better than sentences generated by existing methods,
we believe that these samples are at least competitive with the generative models in the literature and
highlight the potential of the introduced framework.
5 Discussion & Perspectives
The approach promoted in this paper relies on learning a regularly varying representation by minimiz-
ing an empirical proxy of an objective function. The latter writes as a weighted sum of a classification
risk and a regularization term penalizing the distance between the representation distribution and a
heavy-tailed target. Our experiments show that the obtained representation allows to diminish the
classification error compared to a MLP with comparable complexity.The obtained representation
used for a text data augmentation task, is competitive with existing data augmentation methods. The
attribute invariance under code dilation is the key to generate meaningful sentences with prescribed
attribute. Future work will investigate the possibility to train an auto-encoder satisfying this dilation
invariance property without relying on a pretrained classifier such as Orthrus.
8
Under review as a conference paper at ICLR 2020
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Pierre Colombo, Wojciech Witon, Ashutosh Modi, James Kennedy, and Mubbasir Kapadia. Affect-
driven dialog generation. arXiv preprint arXiv:1904.02793, 2019.
Scott Crossley and Danielle McNamara. Cohesion, coherence, and expert evaluations of writing
proficiency. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 32,
2010.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Maziar Moradi Fard, Thibaut Thonet, and Eric Gaussier. Deep k-means: Jointly clustering with
k-means and learning representations. arXiv preprint arXiv:1806.10069, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing Systems,pp. 2672-2680, 2014.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by
editing prototypes. Transactions of the Association for Computational Linguistics, 6:437-450,
2018.
Yutai Hou, Yijia Liu, Wanxiang Che, and Ting Liu. Sequence-to-sequence data augmentation for
dialogue language understanding. arXiv preprint arXiv:1807.01554, 2018.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled
generation of text. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1587-1596. JMLR. org, 2017.
Hamid Jalalzai, Stephan Clemencon, and Anne Sabourin. On binary classification in extreme regions.
In Advances in Neural Information Processing Systems, pp. 3092-3100, 2018.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient
text classification. arXiv preprint arXiv:1607.01759, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Sosuke Kobayashi. Contextual augmentation: Data augmentation by words with paradigmatic
relations. arXiv preprint arXiv:1805.06201, 2018.
Dimitrios Kotzias, Misha Denil, Nando De Freitas, and Padhraic Smyth. From group to individual
labels using deep features. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 597-606. ACM, 2015.
Klaus Krippendorff. Content analysis: An introduction to its methodology. Sage publications, 2018.
Pierre Laforgue, Stephan Clemencon, and Florence d'Alche Buc. Autoencoding any data through
kernel autoencoders. arXiv preprint arXiv:1805.11028, 2018.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Saif M Mohammad. Word affect intensities. arXiv preprint arXiv:1704.08798, 2017.
9
Under review as a conference paper at ICLR 2020
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.
Sidney I Resnick. Extreme values, regular variation and point processes. Springer, 2013.
Sam Shleifer. Low resource text classification with ulmfit and backtranslation. arXiv preprint
arXiv:1903.09244, 2019.
Alban Siffer, Pierre-Alain Fouque, Alexandre Termier, and Christine Largouet. Anomaly detection
in streams with extreme value theory. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 1067-1075. ACM, 2017.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empirical methods in natural language processing, pp.
1631-1642, 2013.
A. Stephenson. Simulating multivariate extreme value distributions of logistic type. Extremes, 6(1):
49-59, 2003.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Jason Wang and Luis Perez. The effectiveness of data augmentation in image classification using
deep learning. Convolutional Neural Networks Vis. Recognit, 2017.
Jason W Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on
text classification tasks. arXiv preprint arXiv:1901.11196, 2019.
10
Under review as a conference paper at ICLR 2020
Appendix
A Remarks
Remark 1 (Selection of k) To the best of our knowledge, selection of k in Algorithm 1 and Algo-
rithm 2 is still a vivid problem in EVT which is not solved yet. As k gets large the number of extreme
points increases including samples which are not large enough and deviates from the asymptotic
distribution of extremes. Smaller values of k increase the variance of the classifier/generator. This
bias-variance trade-off is beyond the scope of this paper.
Remark 2 In Figure 3a selecting the extreme samples on the input space is not a straightforward
step since the tails of the input clusters are not heavy-tail. It was decided to standardise the input data
(Xi)i∈{1,… ,n} by applying the rank-transformation:
Tb(x) = 1/1-Fbj(x)
for all X = (χ1,…Xd) ∈ Rd where Fj =f n+ɪ PZi 1{Xj ≤ x} is the jth empirical marginal dis-
tribution. It comes ∀i ∈ {1, ∙∙∙ ,n},Vi = T(|Xi|). Vi margins follow a standard Pareto distribution.
Extreme samples are thus {Vi, ||Vi|| ≥ n/k} with k = √n by convention.
One may notice that extremes in Figure 3c are more label balanced compared to Figure 3a.
B	Experiments
Further synthetic experiments were conducted to show the improvement of the classifier.
B.1	Synthetic data
Dataset description: Synthetic data was generated using Scikit-Learn (Pedregosa et al. (2011)).
Focus is made on the classification dataset where clusters of points are normally distributed for each
class (see Figure 3a).
Convergence to a logistic: In Figure 3b we visualise the latent space of the output of the encoder
夕(see Figure 2). In Orthrus, the adversarial approach to minimize the discrepancy between the
distribution of the output of the encoder and a logistic distribution. Notice the similarity between
Figure 3b and Figure 4a.
Extremes selection: Extreme samples from Figure 3a are assessed directly on the input data while
extremes from Figure 3c are obtained from the latent space. Extremes from the latent space (output
of 夕 after the training phase) are located on the border of the distribution which comforts the correct
behaviour of the training. We also note similarities while comparing in the input space: extreme
samples selected in the input space and extreme ones selected in the learnt latent space. In Figure 3a
no extreme observation appear on the negative class whereas labels of extremes of Figure 3c are
balanced.
B.2	Logistic distribution
Figure 4 illustrates the logistic distribution with varying parameter δ.
B.3	Additional Experiment Settings For Real Data
We use BERT pretrained models and code from the library pytorch-transformers. All models were
implemented using Pytorch. The output of BERT is a R768 vector, Z is a R50 vector. The dimension
of Z (50) was obtained by cross validation. D, Cbulk and Cext are MLP composed of 3 layers and
uses a ReLu as activation function. Decoders Dbulk and Dext are GRU composed of 6 layers. For
training the batch Size is set to 64 and we set λι = (1 - b(||Z|| ≥ ∣∣Zbκnc ∣∣)-1,λ2 = b(||Z|| ≥
∣∣ZbκnC ∣∣)-1,λ3 = 10-3. All neural networks have been optimized with Adam (Kingma & Ba
(2014)). For generation the maximum dilation factor λ is set to 1.5 since we observe that with greater
values a stuttering phenomenon appears.
11
Under review as a conference paper at ICLR 2020
χ(i)	χ(D	χ(D
(a)	(b)	(c)
Figure 3: Figure 3a: Bivariate synthetic samples normally distributed and designed for binary
classification. Extreme observations are based on a standardisation of the observations in the input
space (refer to the Supplementary for details about the standardisation). Figure 3b: Bivariate outputs
of the encoder φ on the dataset visualised in Figure 3a. Figure 3c Input space where extreme
observations are based on ||Z|| where Z is the output of 夕.
(a) independence
(b) low dependence
(c) high dependence
Figure 4: Illutration of the direction Θ(V) obtained with bivariate samples V generated from a
logistic model with different coefficients of dependence from no dependence Figure 4a (δ = 0.9) to
high dependence Figure 4c (δ = 0.1) through low dependence Figure 4b (δ = 0.5). Non extreme
samples are in gray, extreme samples are in black and directions Θ(V) (extreme samples projected
on the sup norm sphere) in red. Note that not all extremes are shown since the plot was truncated for
a better visualization. However all projections on the sphere are shown.
B.4	Additional Experiments on Yelp Data
Iiiiiiiiiiiiiiiiiiiiii Iiiiiiiiiiiiiiiii
100	150
samples
(a)
UO=-- P
ZU
4U	b∪
samples
(b)
80
200
Figure 5:	Evolution of the prediction of Orthrus on the whole test set (a) and on the extreme test set
(b). Colors (black and white) represent predicted labels for each sample (λ XteSt) with λ being the
dilation factor (Y-axis).
Dilation invariance of Cext: Figure 5a shows the evolution of the prediction for each sample of the
test set applying the dilation. Predictions of Cbulk change for some samples resulting in bichromatic
columns. Figure 5b focuses on the samples from T and is exclusively composed with monochromatic
columns demonstrating that the predicted labels from Cext do not change, Cext is homothetic invariant.
With no assumption on Cext, this classifier enjoys favorable theoretical properties since it belongs to
the class of angular classifiers at the end of the training phase.
12
Under review as a conference paper at ICLR 2020
input	(it wasn,t busy either), the building was cold.	input	seriously killer hot chai latte.
λ=1	(it was not occupied either), the building was cold.	λ=1	"it,s a real killer.
λ = 1.1	(i wasn,t busy either), the building was frozen.	λ= 1.2	he is a real killer.
λ=1.3	also, the building was freezing.	λ= 1.3	he likes to kill.
λ=1.5	plus, the building was colder than ice.	λ= 1.5	i loves murders.
input	food quality has been horrible .	input	all of the tapas dishes were delicious !
λ=1	food quality has been terrible.	λ=1	all the tapas was delicious.
λ = 1.1	the quality of the food was horrible.	λ = 1.1	all tapas dishes were delicious!
λ=1.3	the quality of the food has been horrible.	λ= 1.3	all the tapas dishes were delicious!
λ=1.5	the quality of food was terrible.	λ= 1.5	the tapas were great!
input	overall, i like there food and the service .	input	there was hardly any meat.
λ=1	i love food and the service.	λ=1	there was almost no meat.
λ=1.1	on the whole, i like food and service.	λ = 1.1	there was practically no meat.
λ=1.3	in general, i like the food and the service.	λ= 1.3	there was almost no meat.
λ=1.5	in general, i like food and service.	λ= 1.5	there was no meat.
input	the desserts were a bit strange .	input	waiter was ajerk.
λ=1	the desserts were a little weird.	λ=1	the waiter was a jerk.
λ=1.1	the desserts were very strange.	λ = 1.1	awaiter was a poor guy.
λ=1.3	the desserts were terrible.	λ= 1.3	waiter was an idiot.
λ=1.5	the desserts were terrible.	λ= 1.5	waiter was such an idiot.
input	we definately enjoyed ourselves .	input	i ,m not eating here !
λ=1	we enjoyed ourselves.	λ = 1	i don,t eat here.
λ = 1.1	we had a lot of fun.	λ = 1.1	i don,t eat here!
λ = 1.3	we,ve really enjoyed each other.	λ = 1.3	i,m not going to eat here!
λ = 1.5	we certainly had fun.	λ = 1.5	i will never going to eat here!
Table 4: Supplementary sentences generated by Hydra for extreme embeddings implying label
(sentiment) invariance for generated sentence. λ is the dilation factor.
B.5 Additional Experiments on Amazon DATA
Experiments on the Amazon dataset were conducted similarly to to the ones from the Yelp dataset
from subsection 4.1, similar conclusions can be drawn from Figure 6.
O 5-
鳄
P
IMII HIWIIll IlIiIllIl MHI III
0	50	100	150	200
samples
⑶
(b)
Figure 6:	Evolution of the prediction of Orthrus on the whole test set (a) and on the extreme test
set (b). Colors represent predicted labels for each sample (λ XteSt) with λ being the dilatation factor
(Y-axis).
B.6	Qualitative Evaluation
For the qualitative evaluation, we compute the KriPPendOrfrS alpha coefficient Krippendorff (2018) to
evaluate the inter-annotator agreement. We obtain α = 0.41 for the grammar analysis and α = 0.53
for the sentiment analysis. If a = 1 it indicates perfect reliability; if a = 0 it indicates the absence of
reliability. Such values of ɑ can be considered as relatively high given our huge number of annotators
(126 for the grammar and 134 for the sentiment analysis).
B.7	Additional generated sentences
We present in Table 4 additional extreme sentences.
C A linguistic point of view of extremes
in this section, we provide a comparative linguistic analysis of the extreme samples in case of
sentiment classification through the affect scores provided by the lexicon for English words from
13
Under review as a conference paper at ICLR 2020
Mohammad (2017).
To better understand the content of extreme and non extreme text samples in experiments conducted
in section 4, the maximum and the minimum of the affect score for each sentence are studied (see
Figure 7).
For both Yelp and Amazon datasets, one can notice that the violin plot of the maximum of the affect
score is wider among the extremes than in the bulk. This illustrates that extreme tend to have both a
more positive and a more negative affect score than the non extreme observations. Same goes for the
minimum affect score of extremes whose violin plot tends to be larger among the extremes compared
to the violin plot of the minimum affect score on the bulk.
max extremes max bulk min extremes min bulk
Amazon
(a)
max extreme max bulk min extreme min bulk
Yelp
(b)
Figure 7:	Illutration of the affect score on the Amazon dataset (Figure 7a) and on the Yelp dataset
(Figure 7b dataset). The blue (respectively orange) violin plot represents the maximum affect score of
sentences in the extreme (respectively non extreme) samples. The green (respectively red) violin plot
represents the minimum affect score of sentences in the extreme (respectively non extreme) samples.
14