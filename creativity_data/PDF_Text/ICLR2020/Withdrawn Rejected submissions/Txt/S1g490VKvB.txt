Under review as a conference paper at ICLR 2020
The Dynamics of Signal Propagation in Gated
Recurrent Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Training recurrent neural networks (RNNs) on long sequence tasks is plagued with
difficulties arising from the exponential explosion or vanishing of signals as they
propagate forward or backward through the network. Many techniques have been
proposed to ameliorate these issues, including various algorithmic and architectural
modifications. Two of the most successful RNN architectures, the LSTM and the
GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer
from instabilities when trained on very long sequences. In this work, we develop a
mean field theory of signal propagation in LSTMs and GRUs that enables us to
calculate the time scales for signal propagation as well as the spectral properties of
the state-to-state Jacobians. By optimizing these quantities in terms of the initial-
ization hyperparameters, we derive a novel initialization scheme that eliminates
or reduces training instabilities. We demonstrate the efficacy of our initialization
scheme on multiple sequence tasks, on which it enables successful training while
a standard initialization either fails completely or is orders of magnitude slower.
We also observe a beneficial effect on generalization performance using this new
initialization.
1	Introduction
A common paradigm for research and development in deep learning involves the introduction of novel
network architectures followed by experimental validation on a selection of tasks. While this method-
ology has undoubtedly generated significant advances in the field, it is hampered by the fact that the
full capabilities of a candidate model may be obscured by difficulties in the training procedure. It is
often possible to overcome such difficulties by carefully selecting the optimizer, batch size, learning
rate schedule, initialization scheme, or other hyperparameters. However, the standard strategies for
searching for good values of these hyperparameters are not guaranteed to succeed, especially if the
trainable configurations are constrained to a low-dimensional subspace of hyperparameter space,
which can render random search, grid search, and even Bayesian hyperparameter selection methods
unsuccessful.
In this work, we argue that for long sequence tasks, the trainable configurations of initialization
hyperparameters for LSTMs and GRUs lie in just such a subspace, which we characterize theoretically.
In particular, we identify precise conditions on the hyperparameters governing the initial weight
and bias distributions that are necessary to ensure trainability. These conditions derive from the
observation that in order for a network to be trainable, (a) signals from the relevant parts of the input
sequence must be able to propagate all the way to the loss function and (b) the gradients must be stable
(i.e., they must not explode or vanish exponentially). As shown in Figure 1, training of recurrent
networks with standard initialization on certain tasks begins to fail as the sequence length increases
and signal propagation becomes harder to achieve. However, as we shall show, a suitably-chosen
initialization scheme can dramatically improve trainability on such tasks.
We study the effect of the initialization hyperparameters on signal propagation for a very broad class
of recurrent architectures, which includes as special cases many state-of-the-art RNN cells, including
the GRU (Cho et al. (2014)), the LSTM (Hochreiter & Schmidhuber (1997)), and the peephole
LSTM (Gers et al. (2002)). The analysis is based on the mean field theory of signal propagation
developed in a line of prior work (Schoenholz et al. (2016); Xiao et al. (2018); Chen et al. (2018);
Yang et al. (2019)), as well as the concept of dynamical isometry (Saxe et al. (2013); Pennington
1
Under review as a conference paper at ICLR 2020
Sequence length
Figure 1: Critical initialization improves trainability of
recurrent networks. Test accuracy for peephole LSTM
trained to classify sequences OfMNIST digits after 8000
iterations. As the sequence length increases, the network
is no longer trainable with standard initialization, but still
trainable using critical initialization.
et al. (2017; 2018)) that is necessary for stable gradient backpropagation and which was shown to
be crucial for training simpler RNN architectures (Chen et al. (2018)). We perform a number of
experiments to corroborate the results of the calculations and use them to motivate initialization
schemes that outperform standard initialization approaches on a number of long sequence tasks.
2	Background and related work
2.1	Mean field analysis of neural networks
Signal propagation at initialization can be controlled by varying the hyperparameters of fully-
connected (Schoenholz et al. (2016); Yang & Schoenholz (2017)) and convolutional (Xiao et al.
(2018)) feed-forward networks, as well as for simple gated recurrent architectures (Chen et al. (2018)).
In all these cases, such control was used to obtain initialization schemes that outperformed standard
initializations on benchmark tasks. In the feed-forward case, this enabled the training of very deep
architectures without the use of batch normalization or skip connections.
By forward signal propagation, we specifically refer to the propagation of correlations between
inputs at the start of an input sequence through the hidden states of the recurrent network at later
times, as will be made precise in Section 4.2. This appears to be a necessary but not sufficient
condition for trainability on certain tasks. Another is the stability of the gradients, which depend on
the state-to-state Jacobian matrix, as discussed in Bengio et al. (1994). In our context, the goal of the
backward propagation analysis is to improve the conditioning of the Jacobian by controlling the first
two moments of its squared singular values. Forward signal propagation and the spectral properties
of the Jacobian at initialization can be studied using mean field theory and random matrix theory
(Poole et al. (2016); Schoenholz et al. (2016); Yang & Schoenholz (2017; 2018); Xiao et al. (2018);
Pennington et al. (2017; 2018); Chen et al. (2018); Yang et al. (2019)).
As neural network training is a nonconvex problem, using a modified initialization scheme could lead
to convergence to different points in parameter space in a way that adversely affects the generalization
error. We provide some empirical evidence that this does not occur, and in fact, the use of initialization
schemes satisfying these conditions has a beneficial effect on the generalization error.
2.2	The exploding/vanishing gradient problem and signal propagation in
RECURRENT NETWORKS
The exploding/vanishing gradient problem is a well-known phenomenon that hampers training on long
time sequence tasks (Bengio et al. (1994); Pascanu et al. (2013)). Apart from the gating mechanism,
there have been numerous proposals to alleviate the vanishing gradient problem by constraining the
weight matrices to be exactly or approximately orthogonal (Pascanu et al. (2013); Wisdom et al.
(2016); Vorontsov et al. (2017); Jose et al. (2017)), or more recently by modifying some terms in the
gradient (Arpit et al. (2018)), while exploding gradients can be handled by clipping (Pascanu et al.
(2013)). Another recently proposed approach to ensuring signal propagation in long sequence tasks
introduces auxiliary loss functions (Trinh et al. (2018)). This modification of the loss can be seen
as a form of regularization. Chang et al. (2019) study the connections between recurrent networks
and certain ordinary differential equations and propose the AntisymmetricRNN that can capture long
term dependencies in the inputs. While many of these approaches have been quite successful, they
2
Under review as a conference paper at ICLR 2020
typically require modifying the training algorithm, the loss function, or the architecture, and as such
exist as complementary methods to the one we investigate here. We postpone the investigation of a
combination of techniques to future work.
2.3	Dynamics beyond initialization
While the analysis below applies to the network at initialization, it was recently shown that as the
network width becomes large, parameters move very little from their initial values (Jacot et al. (2018);
Lee et al. (2019)). A non-trivial and surprising consequence is that covariance matrices like those
studied here remain constant during training. While Jacot et al. (2018); Lee et al. (2019) do not
specifically study RNNs, the analysis of Yang (2019) suggests the conclusions are likely to carry over.
Moreover, even for finite-width networks, simply satisfying the conditions derived in this work at
initialization can lead to a dramatic improvement in trainability as we show below.
3	Notation
We denote matrices by bold upper case Latin characters and vectors by bold lower case Latin
characters. Dx denotes a standard Gaussian measure. The normalized trace of a random N × N
matrix A, NEtr(A), is denoted by T(A). ◦ is the Hadamard product, σ(∙) is a sigmoid function and
both σ(∙), tanh(∙) act element-wise. We denote by Da a diagonal matrix with a on the diagonal.
4	Mean field analysis of signal propagation and dynamical
ISOMETRY
4.1	Model description and important assumptions
	Vanilla RNN	GRU Cho et al. (2014)	LSTM HOChreiter & SChmidhUber (1997)
K gk	{f} .	f} 	σ(ur)		{i,f, r, o} .
f	σ(Uf)	σ(utf) ◦ st-1 +(1 - σ (utf)) ◦ tanh(utr2 )	σ(uto) ◦ tanh(c0+ tt P Π σ(uf) ◦ σ(ui) ◦ tanh(ur))
Table 1: A number of recurrent architectures written in the form 1. K is the set of pre-activation
subscripts, f is the state update function in eqn. (1a) and gk is the function in eqn. (1c). The LSTM
cell state is unrolled in order to emphasize that it can be written as a function of variables that are
Gaussian at the large N limit. See Table 3 for additional architectures.
We begin with a general description of recurrent architectures that can be specialized to the GRU,
LSTM and peephole LSTM among others. We denote the state of a recurrent network at time t by
St ∈ RN with s0 〜Do, and a sequence of inputs to the network by {z1,…，zτ}, zt ∈ RN.
The state evolution of the network is given by
st = f(st-1, {u1k}, ..., {utk}, zt)	(1a)
where f is an element-wise, affine function of st-1 and {utk} is a set of pre-activations utk ∈ RN, k ∈
K defined for a set of subscripts K . They are given by
utk = Wkst-1 + Ukzt + bk	(1b)
where Wk, Uk ∈ RN×N, bk ∈ RN. We define additional pre-activations
utk2 = Wk2Dgk(utk)st-1+Uk2zt +bk2	(1c)
where gk : RN → RN is an element-wise function and utk is defined as in eqn. (1b) 1. In cases where
there is no need to distinguish between variables of the form (1b) and (1c) we will refer to both as
1Variables of the form (1c) will be present only in the GRU (see Table 1).
3
Under review as a conference paper at ICLR 2020
utk . This state update equation describes all the architectures studied in this paper, as well as many
others, as detailed in Tables 1,3. The pre-activations utk, which are typically used to construct gates
in recurrent network with the application of appropriate nonlinearities, will be of interest because
they will tend in distribution to Gaussians as the network width is taken to infinity.
We assume Wkj 〜N(0,σ2∕N),Uk,j 〜N(0, V2/N), bki 〜N(μk,ρk) i.i.d. and denote Θ =
U{σ2, νk,ρk,μk}. As in Chen et al. (2018), We make the untied weights assumption that Wk is
k
independent of st . Although the assumption of untied Weights may seem counter-intuitive in the
context of recurrent netWorks, recall that We are only interested in characterizing various statistics
of the netWork, not in computing the entire input-output map, for Which using tied Weights Would
certainly be crucial. For merely characterizing certain statistics, the untied Weights assumption
actually has long history of yielding correct predictions, and therefore it is not at all unnatural to adopt:
1) When computing the covariance of gradients in an MLP, the Weights during backpropagation can be
treated as independently sampled from the Weights used during the forWard propagation (Schoenholz
et al., 2016; Yang & Schoenholz, 2017); 2) When computing the Jacobian singular value distribution
of an MLP, the pre-activations h = Wx can be treated as the result of multiplying an iid copy
W0 of W (i.e. h = W0x) (Pennington et al., 2017; 2018; Hayase, 2019); 3) a Wide, randomly
initialized MLP corresponds to the same Gaussian process Whether or not its Weights across layers
are tied (Poole et al., 2016), and similarly, a simple RNN evolves the same Whether or not its Weights
are tied across time, as long as its Width is large (Chen et al., 2018). Indeed, We provide empirical
evidence that calculations performed under this assumption still have considerable predictive poWer,
even in cases Where it is violated.
4.2	Forward signal propagation
We noW study hoW correlations betWeen inputs propagate into the hidden states of a recurrent netWork.
This section folloWs closely the development in Chen et al. (2018). We noW consider tWo sequences
of normalized inputs {zta}, {ztb} With zero mean and covariance R = R Σ1 Σ1z fed into tWo
copies of a netWork With identical Weights, and resulting in sequences of states {sta}, {stb}. We
consider the time evolution of the moments and correlations
μS = E[sta]	(2a)
Qts ≡ Qts,aa = E[stia sita]	(2b)
∑S2 * *Ct + (μS)2= E[stastb]	(2c)
where we define ∑S2 = Qs - (μS)2.
Returning to the pre-activations defined in eqn. (1). We make the mean field approximation2 that the
pre-activations are jointly Gaussian at the infinite width N → ∞ limit:
(ukia)~N( Ik)，(Qk-〃k)( CkCt))
where the second moment Qtk is given by
Qk = E[ukia ukiα ] = σk E[stasia]+ ν2E[ztazta]+ ρk + μk = σk Qtt + νkR + ρk + μk
(3)
(4a)
and defining ∑k2 = Qk - μk
Ct _	E[ukiaukib]- μk	_	σ2	(ςS2CS	+	μ2	+ v2rςz	+ ρk	z4b.
Ck=	Qk-μk	=	QI-μ	.	(4b)
The variables utk ia, utk ib given by eqn. (1c) are also asymptotically Gaussian, with their covariance
detailed in Appendix A. We will subsequently drop the vector subscript i since all elements are
2For feed-forward networks, it has been shown using the Central Limit Theorem for exchangeable random
variables that the pre-activations at all layers indeed converge in distribution to a multivariate Gaussian (Matthews
et al. (2018)).
4
Under review as a conference paper at ICLR 2020
identically distributed and f , gk act element-wise, and the input sequence index in expressions that
involve only one sequence.
For any l ≤ t, the ulk are independent of sl at the large N limit. Combining this with the fact that
their distribution is determined completely by μS-1, QS-1,CS-1 and that f is affine, one can rewrite
eqn. (2a-c) using eqn. (1a-c) as the following deterministic dynamical system
(μS, QsC) = M(μS-1,QS-1, CS-1,…,μ1,Ql,。1)	⑸
where the dependence on Θ and the data distribution has been suppressed. In the peephole LSTM
and GRU, the form will be greatly simplified to M(μs-1,Qs-1, Cs-1). In Appendix E We compare
the predicted dynamics to simulations, showing good agreement.
One can now study the fixed points of eqn. (5) and the rates of convergence by linearizing the dynamics.
The fixed point of eqn. (5) is pathological, in the sense that any information that distinguishes two
input sequences is lost. Therefore, delaying the convergence to the fixed point should allow for signals
to propagate across longer time horizons. Quantitatively, the rate of convergence of eqn. (5) to
a fixed point gives an effective time scale for signal propagation from the inputs to the hidden
state at later times.
While the dynamical system is two-dimensional and analysis of convergence rates should be per-
formed by linearizing the full system and studying the smallest eigenvalue of the resulting matrix, in
practice as in Chen et al. (2018) this eigenvalue appears to always corresponds to the Cst direction3 .
Hence, if we assume convergence of Qs, μs we need only study
Cs = Mc U,Q；,CsT)	(6)
where MC also depends on expectations of functions of {u1k}, ...{utk-2} that do not depend on Cst-1.
While this dependence is in principle on an infinite number of Gaussian variables as the dynamics
approach the fixed point, Mc can still be reasonably approximated in the case of the LSTM as
detailed in Appendix D. We show that in the case of the peephole LSTM this map is convex in
Appendix C. This can be shown for the GRU by a similar argument. It follows directly that it has a
single stable fixed point in these cases.
The rate of approach to the fixed point, XCr is given by linearizing around it. Setting CS = C* + εt,
we have
C!+1 = C: + εt+1 = Mc(μ;, QS,C* + εt) = MC(μ;,Q*, C*) + XCsεt + O((εt)2).	(7)
The time scale of convergence to the fixed point is thus given by
ξC
1
log XCs
(8)
s
s
which diverges as XCss approaches 1 from below. Due to the detrimental effect of convergence to the
fixed point described above, it stands to reason that a choice of Θ such that XCs = 1 - δ for some
small δ > 0 would enable signals to propagate from the initial inputs to the final hidden state when
training on long sequences.
4.3	Backwards signal propagation - the state-to-state Jacobian
We now turn to controlling the gradients of the network. A useful object to consider in this case is the
asymptotic state-to-state transition Jacobian
J
lim
t→∞
∂ st+1
∂ St
J and powers of it will appear in the gradient as the covariance dynamics described in Section 4.2
approach their fixed point (specifically, the gradient of a network trained on a sequence of length T
will depend on a matrix polynomial of order T in J), hence we desire to control the squared singular
3This observation is explained in the case of fully-connected feed-forward networks in Schoenholz et al.
(2016).
5
Under review as a conference paper at ICLR 2020
value distribution of J. This distribution is described by moments of the form mJJT,n = τ ((JJT)n),
with mean and variance of the distribution given by
mJJT,1 = E	ak	(9)
k∈K∪{0}
σJ2JT = mJJT,2 - m2JJT,1 = E	2akal - a20 - E	ak	(10)
k,l∈K∪{0}	k∈K∪{0}
where the scalars {ak}k∈K∪{0} are architecture and hyperparameter dependent and are given in
Appendix A.2.
Forward and backward signal propagation are in fact intimately related, as the following lemma
shows:
Lemma 1. For a recurrent neural networks defined by (1), the mean squared singular value of the
state-to-state Jacobian defined in (16) and χCs that determines the time scale of forward signal
propagation (given by (7)) are related by
mJJT ,1 = χ3 = 1,∑z = 1	(II)
Proof. See Appendix C.	□
If one can find a setting of the hyperparameters such that mJJT ,1 is close to 1 and σJ2JT is small, then
the spectrum of powers of J should remain well-conditioned ensuring that gradients will not explode
or vanish.
4.4	Dynamical Isometry
Combining the results of Sections 4.2 and 4.3, we conclude that an effective choice of initialization
hyperparamters should satisfy
XCs = 1	(12a)
mJJT,1 = 1	(12b)
σJJT = 0.	(12c)
We refer to these as dynamical isometry conditions. eqn. (12)a ensures stable signal propagation from
the inputs to the loss, and eqn. (12)b-c are motivated by the additional requirement of preventing
the gradients from exploding/vanishing. Both of these conditions appear to be necessary in order
for a network to be trainable. Combining eqns. (9, 10, 11) we find that if Σz = 1, the dynamical
isometry conditions are satisfied if a0 = 1, ak6=0 = 0, which can be achieved by setting ∀k : σk2 = 0
and taking μf → ∞. This motivates the general form of the initializations used in the experiments.
We demand that these equations are only satisfied approximately since for a given architecture,
there may not be a value of Θ that satisfies them all. Additionally, even if such a value exists,
the optimal value of χCs for a given task may not be 1. There is some empirical evidence that
if the characteristic time scale defined by χCs is much larger than that required for a certain task,
performance is degraded.
The typical values of Σz will depend on the data set, yet satisfying the dynamical isometry conditions
is simplified if Σz = 1 due to Lemma 1. It is thus a natural choice, yet we acknowledge that a more
comprehensive treatment should consider the case of general Σz . We also find empirically that the
results obtained under the Σz = 1 assumption prove to be predictive and enable training on a number
of tasks without requiring a detailed analysis of Σz .
4.5	The Stationary Distribution of the LSTM Cell State
In architectures that were considered in previous works, as well as the peephole LSTM and GRU, all
the quantities required to calculate the dynamical isometry conditions through eqns. (9, 10) can be
written in terms of integrals over a finite number of Gaussian variables at the large width and time
6
Under review as a conference paper at ICLR 2020
peephole LSTM
Figure 2: Training accuracy on the padded MNIST classification task described in 5.1 at different
sequence lengths T and hyperparameter values Θ0 + αΘ1 for networks with untied weights, with
different values of Θ0, Θ1 chosen for each architecture. The dark and light green curves are respec-
tively 3ξ, 6ξ where ξ is the theoretical signal propagation time scale in eqn. (8). As can be seen, this
time scale predicts the transition between the regions of high and low accuracy across the different
architectures and directions in hyperparameter space.
limit and are thus easy to evaluate. This is not the case in the standard LSTM due to the presence of
the cell state. By unrolling the cell state update equation
ct = σ(utf) ◦ ct-1 + σ(uit) ◦ tanh(utr)	(13)
in time, we find that ct depends on the entire sequence of utk from the first time step 4.
From eqn. 13 we find that the asymptotic cell state distribution is that of a perpetuity, which is
a random variable X that obeys X =d XY + Z where Y, Z are random variables and =d denotes
equality in distribution. The stationary distribution of a perpetuity generally does not have a closed
form, and neither does its likelihood (Goldie (1991)). In practice, one can overcome this difficulty by
sampling from the stationary cell state distribution, as described in Appendix D.
5	Experiments
5.1	Padded MNIST Classification
The calculations presented above predict a characteristic time scale ξ (defined in (8)) for forward
signal propagation in a recurrent network. It follows that on a task where success depends on
propagation of information from the first time step to the final T-th time step, the network will not be
trainable for T ξ. In order to test this prediction, we consider a classification task where the inputs
are sequences consisting of a single MNIST digit followed by T - 1 steps of i.i.d Gaussian noise and
the targets are the digit labels. By scanning across certain directions in hyperparameter space, the
predicted value of ξ changes. We plot training accuracy of a network trained with untied weights
after 1000 iterations for the GRU and 2000 for the LSTM, as a function of T and the hyperparameter
values, and overlay this with multiples of ξ. As seen in Figure 2, we observe good agreement between
the predicted time scale of signal propagation and the success of training. As expected, there are
some deviations when training without enforcing untied weights, and we present the corresponding
plots in the supplementary materials.
，C TTZDCTT 口CNzrNTTQTAZnrTl=? ARJ 八				MNIST	CIFAR-10
.-	standard LSTM	98.6	588
The calculation results motivate critical initial-	critical LSTM	98.9	-618
izations that we test on standard long sequence
benchmarks. The details of the initializations
are presented in Appendix E. We unroll an
MNIST digit into a sequence of length 784 and
Table 2: Test accuracy on unrolled MNIST and
CIFAR-10.
train a critically initialized peephole LSTM with 600 hidden units. We also train a critically initialized
4In contrast, the hidden state distribution of the GRU at time t for instance can be expressed as an integral
over {utk-1} alone (which are Gaussian at the large t fixed point) through eqn. 2.
7
Under review as a conference paper at ICLR 2020
Iteration	Iteration	Iteration
Figure 3: Training accuracy for unrolled, concatenated MNIST digits (top) and unrolled MNIST
digits with replicated pixels (bottom) for different sequence lengths. Left: For shorter sequences the
standard and critical initialization perform equivalently. Middle: As the sequence length is increased,
training with a critical initialization is faster by orders of magnitude. Right: For very long sequence
lengths, training with a standard initialization fails completely (and is unstable from initialization in
the lower right panel).
LSTM with hard sigmoid nonlinearities on unrolled CIFAR-10 images feeding in 3 pixels at every
time step, resulting in sequences of length 1024. We also apply standard data augmentation for this
task. We present accuracy on the test set in Table 2. Interestingly, in the case of CIFAR-10 the best
performance is achieved by an initialization with a forward propagation time scale ξ that is much
smaller than the sequence length, suggesting that information sufficient for successful classification
may be obtained from a subset of the sequence.
5.3	Repeated pixel MNIST and multiple digit MNIST
In order to generate longer sequence tasks, we modify the unrolled MNIST task by repeating every
pixel a certain number of times and set the input dimension to 7. To create a more challenging task,
we also combine this pixel repetition with concatenation of multiple MNIST digits (either 0 or 1),
and label such sequences by a product of the original labels. In this case, we set the input dimension
to 112 and repeat each pixel 10 times. We train a peephole LSTM with both a critical initialization
and a standard initialization on both of these tasks using SGD with momentum. In this former task,
the dimension of the label space is constant (and not exponential in the number of digits like in the
latter). In both tasks, we observe three distinct phases. If the sequence length is relatively short the
critical and standard initialization perform equivalently. As the sequence length is increased, training
with a critical initialization is faster by orders of magnitude compared to the standard initialization.
As the sequence length is increased further, training with a standard initialization fails, while training
with a critical initialization still succeeds. The results are shown in Figure 3.
6	Discussion
We have derived initialization schemes for recurrent networks motivated by ensuring stable signal
propagation from the inputs to the loss and of gradient information from the loss to the weights.
These schemes dramatically improve performance on long sequence tasks.
The subspace of initialization hyperparameters Θ that satisfy the dynamical isometry conditions is
multidimensional, and there is no clear principled way to choose a preferred initialization within it. It
would be of interest to study this subspace and perhaps identify preferred initializations based on
additional constraints. One could also use the satisfaction of the dynamical isometry conditions as
8
Under review as a conference paper at ICLR 2020
a guiding principle in simplifying these architectures. A direct consequence of the analysis is that
the forget gate, for instance, is critical, while some of the other gates or weights matrices can be
removed while still satisfying the conditions. A related question is the optimal choice of the forward
propagation time scale ξ for a given task. As mentioned in Section 5.2, this scale can be much shorter
than the sequence length. It would also be valuable understand better the extent to which the untied
weights assumption is violated, since it appears that the violation is non-uniform in Θ, and to relax
the constant Σz assumption by introducing a time dependence.
Another compelling issue is the persistence of the dynamical isometry conditions during training and
their effect on the solution. It has been recently shown that in sufficiently wide networks the gradient
flow dynamics in function space are effectively linear (Jacot et al. (2018); Lee et al. (2019)) and under
these same conditions the dynamical isometry conditions persist during training. Understanding the
finite width and learning rate corrections to such calculations could help extend the analysis of signal
propagation at initialization to trained networks in this regime.
References
Devansh Arpit, Bhargav Kanuparthi, Giancarlo Kerg, Nan Rosemary Ke, Ioannis Mitliagkas, and
Yoshua Bengio. h-detach: Modifying the lstm gradient towards better optimization. arXiv preprint
arXiv:1810.03023, 2018.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.
Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. AntisymmetricRNN: A dynamical system
view on recurrent neural networks. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=ryxepo0cFX.
Minmin Chen, Jeffrey Pennington, and Samuel S Schoenholz. Dynamical isometry and a mean field
theory of rnns: Gating enables signal propagation in recurrent neural networks. arXiv preprint
arXiv:1806.05394, 2018.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 1724-1734, 2014.
Felix A Gers, NiCol N Schraudolph, and Jurgen SChmidhuber. Learning precise timing with lstm
recurrent networks. Journal of machine learning research, 3(Aug):115-143, 2002.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Charles M Goldie. Implicit renewal theory and tails of solutions of random equations. The Annals of
Applied Probability, pp. 126-166, 1991.
Charles M Goldie and Rudolf Grubel. Perpetuities with thin tails. Advances in Applied Probability,
28(2):463-480, 1996.
Tomohiro Hayase. Almost surely asymptotic freeness for jacobian spectrum of deep network. arXiv
preprint arXiv:1908.03901, 2019.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Cijo Jose, Moustpaha Cisse, and Francois Fleuret. Kronecker recurrent units. arXiv preprint
arXiv:1705.10142, 2017.
9
Under review as a conference paper at ICLR 2020
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
arXiv preprint arXiv:1902.06720, 2019.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning, pp. 1310-1318, 2013.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural information
processing systems, pp. 4785-4795, 2017.
Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. The emergence of spectral universality
in deep networks. arXiv preprint arXiv:1802.09979, 2018.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential
expressivity in deep neural networks through transient chaos. In Advances in neural information
processing systems, pp. 3360-3368, 2016.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. arXiv preprint arXiv:1611.01232, 2016.
Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. Learning longer-term dependencies in
rnns with auxiliary losses. arXiv preprint arXiv:1803.00144, 2018.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. In International Conference on Machine Learning,
pp. 3570-3578, 2017.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity
unitary recurrent neural networks. In Advances in Neural Information Processing Systems, pp.
4880-4888, 2016.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances
in neural information processing systems, pp. 7103-7114, 2017.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Greg Yang and Sam S. Schoenholz. Deep Mean Field Theory: Layerwise Variance and Width
Variation as Methods to Control Gradient Explosion. ICLR Workshop, February 2018. URL
https://openreview.net/forum?id=rJGY8GbR-. 00000.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A
Mean Field Theory of Batch Normalization. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=SyMDXnCcF7.
10
Under review as a conference paper at ICLR 2020
Appendices
Appendix A Details of Results
A.1 COVARIANCES OF utk
The variables utk2ia , utk2ib given by 1c are asymptotically Gaussian at the N → ∞,
with
Qk2 = σ22 / gk (Ua)DkQS + ν22 R + pk2 + μk2
(σk2 R gk(ua)gk(Ua)Dk 3S2Ct + μ2)、
F ='	+vK R2 + P2	)
―	Qk2 - μ∖
(14a)
(14b)
where Dk is a Gaussian measure on (Ua, Ub) corresponding to the distribution in
eqn. (3).
A.2 Moments of the state-to-state Jacobian
The moments of the squared singular value distribution are given by the normalized
traces
mJJT,n = τ((JJT)n).
Since Ut0, t0 < t is independent of st, if we index by k, k0 the variables defined by
eqn. (1b) and (1c) respectively we obtain
∂f	∂f	∂f
J = ∂S* + ∑∂U*Wk + ∑∂U^Wk2 (Dgk(Uk0)+ WkODgk0(uk0)Ds*).	(15)
k k	k0	k20
Under the untied assumption Wk , Wk2 are independent of st , utk at the large N limit,
and are also independent of each other and their elements have mean zero. Using
this and the fact that f acts element-wise, we have
mJJT,1 = τ(JJT) = E	ak
k∈K∪{0}
(16)
where
[D0
σ2D2
2 D2 (	gk (Uk)
I k2 k I +σks*2gk2(uk)
k=0
uk is given by (1b)
uk is given by (1c)
(17)
and Do =皓,Dk =笈.The values of Do, Dk for the architectures considered in
the paper are detailed in Appendix A.3.
Controlling the first moment of JJT is not sufficient to ensure that the gradients do
not explode or vanish, since the variance of the singular values may still be large.
This variance is given by
σJ2JT = mJJT,2 - m2JJT,1 .
11
Under review as a conference paper at ICLR 2020
The second moment mJJT,2 can be calculated from (15), and is given by
mJJT,2 = E	2akal - a02	(18)
k,l∈K∪{0}
where the ak are defined in eqn. (17). We arrange the scalars a0 , {ak} into a vector
a. In Appendix B we show that the results of the calculations in this section match
the empirical spectrum of the Jacobian.
For all the architectures considered in this paper, We find that Do = σ(uf) while
Dk are finite as ∀k : σk2 → 0. Combining this with (11), (16), (18), we find that if
Σz = 1 the dynamical isometry conditions are satisfied if a0 = 1, ak6=0 = 0, which
can be achieved by setting ∀k : σk = 0 and taking μf → ∞. This motivates the
general form of the initializations used in the experiments 5 although there are many
other possible choices of Θ such that the ak6=0 vanish.
Given the above general form of the dynamical isometry conditions for recurrent
networks, we now provide the detailed forms that apply to the LSTM, peephole
LSTM and GRU.
A.3 Dynamical isometry conditions for selected architectures
We specify the form of χc*,∑ and a for the architectures considered in this paper:
A.3.1 GRU
Xc^,∑ = E
σ(ufa)σ(ufb)+ σf	tanh(ur^)^(ur,b) y0(ufa)σ0(ufb)
+σ22(I- σ(Uf O))(I- σ(Ufb ))tanh0 (Ur2a)tanh0(Ur2b) (+σ2 h(Uσa)σ(ujσ)(u* b)
/	σ2(uf)	∖
_	σf (tanh2(ur2) + Qh) σ02(uf)
a	σf1σf2 h*2(1 — σ(uf ))2 tanh02(ur2 )σ02 (u；1)
σr22(1 - σ(U*f))2tanh02(Ur*2)σ2(Ur*1)
A.3.2 peephole LSTM
= E	σ(U*fa)σ(Uf*b) + σi2σ0(Ui*a)σ0(Ui*b) tanh(Ur*a) tanh(Ur*b)
χCs,Σ	σf2c*acb*σ0(Uf*a)σ0(Uf*b) + σr2σ(Ui*a)σ(Ui*b)tanh0(Ur*a)tanh0(Ur*b)
(
a =
∖
σ2 (U*f)
σ2σ02 (u*) tanh2 (Ur)
σ2 c*2σ02(uf)
σr2σ2 (Ui*) tanh02(Ur*)
5In the case of the LSTM, we also want to prevent the output gate from taking very small values, as explained
in Section A.3.
12
Under review as a conference paper at ICLR 2020
A.3.3 LSTM
Figure 4: Squared singular values of the State-to-state
Jacobian in eqn. (15) for two choices of hyperparam-
eter settings Θ. The red lines denote the empirical
mean and standard deviations, while the dotted lines
denote the theoretical prediction based on the calcu-
lation described in Section 4.3. Note the dramatic
difference in the spectrum caused by choosing an ini-
tialization that approximately satisfies the dynamical
isometry conditions.
XC 总 Σz = E
σ(Ufa)σ(ufb) + σ2σ0(OO(Uob) tanhGNanhO
/	σfcacbσ'(Uff a)σ0(ufb)
+σ(Ui□σ(Uob) tanh0(Ca) tanh0(Cf)	+σ2σ0(Ufa)σ0(ufa) tanh(Ura) tanh(Urb)
+σr2σ(Uifa)σ(Uifa) tanh0(Urfa) tanh0(Urfb)
a
/	σ2(Uf)	∖
σ2σ2(Uf) tanh02(cf)σ02(Uf) tanh2(Ur)
σfσ2(Uf) tanh02(cf)cf2σ02(Uf)
σ2σ2(Uf) tanh02(c*)σ2(Uf)tanh02(Ur)
σo2 σ02 (Uof) tanh2 (Cf)
When evaluating 7 in this case, We write the cell state as Ct = tanh-1 (σs⅛), and
assume b'0-1)≈ 1 for large t. The stability of the first equation and the accuracy of
the second approximation are improved if ot is not concentrated around 0.
A.4 RNN Table supplement
	Minimal RNN Chen etal. (2018)	peephole LSTM GerS et al.(2002)
K	f}	{i,f, r, o}
Pt	St	σ(uθ) ◦ tanh(St)
f	σ(uf) ◦ StT +(1 - σ(uf)) ◦ Xt	σ(uf) ◦ st-1 +σ(ui) ◦ tanh(U)
Table 3: Additional recurrent architectures written in the form 1. pt is the output of the network at
every time step. See Table 1 for more details.
Appendix B	S quared Jacobian spectrum histograms
To verify the results of the calculation of the moments of the squared singular value
distribution of the state-to-state Jacobian presented in Section 4.3 we run an untied
peephole LSTM for 100 iterations with i.i.d. Gaussian inputs. We then compute
the state-to-state Jacobian and calculate its spectrum. This can be used to compare
the first two moments of the spectrum to the result of the calculation, as well as to
observe the difference between a standard initialization and one close to satisfying
the dynamical isometry conditions. The results are shown in Figure 4. The validity
of this experiment rests on making an ergodicity assumption, since the calculated
spectral properties require taking averages over realizations of random matrices,
while in the experiment we instead calculate the moments by averaging over the
13
Under review as a conference paper at ICLR 2020
eigenvalues of a single realization. The good agreement between the prediction and
the empirical average suggests that the assumption is valid.
Appendix C	Auxiliary Lemmas and Proofs
Proof of Lemma 1. Despite the fact that each ukai as defined in 1 depends in prin-
ciple upon the entire state vector sa , at the large N limit due to the isotropy of the
input distribution we find that these random variables are i.i.d. and independent of
the state. Combining this with the fact that f is an element-wise function, it suffices
to analyse a single entry of st = f (st-1, {u1k}, ..., {utk}), which at the large t limit
gives
Mc (μ: ,QS,Cs)
Ef(Sa,U□f(sb, UQ-(〃；)2
qS - (μS)2
where Ua = {uka∣k ∈ K, Uka 〜N(μk, Qk — μk} and Ub is defined similarly (i.e.
we assume the first two moments have converged but the correlations between the
sequences have not, and in cases where f depends on a sequence of {u1k}, ..., {utk}
we assume the constituent variables have all converged in this way). We represent
uSka , uSkb via a Cholesky decomposition as
uka = ∑k Zka + μk	(19a)
Ukb = Σk (Ckzka + JI-(Ck)2Zkb) + μk	(19b)
where	zka, zkb	〜	N(0,1)	i.i.d. We thus have	∂kb
Combining this with the fact that
Dzg(z)z =	Dzg0(z) for any g(z), integration by parts gives for any
g1, g2
急 R DzkaDzkbgI(Uka)g2 (Ukb)
=R DzkaDzkbgl (Uka) ¾kb) d⅛	(20)
=∑k R DzkaDzky ⅛gb)
Denoting Σ2 = QS — (μS)2 and defining ∑k, ∑k? similarly, we have
dCka = σ2∑
dCs = ∑ka
dCkb —脸夕2
f
÷
*
K = ^∑kb
gk (USka)gk (UkSb)
σ22 ,2小+(海)2) ∂gk(uka)gk(ukb)
dCk
Dk
∂Ck	dCs
(21)
14
Under review as a conference paper at ICLR 2020
σ22∑2.
-∑2^ *
k2
gk(Uka) gk(Ukb)
+}*s ) gk(Uika)gk(Ukb)
Dk
where in the last equality we used 20. Using 20 again gives
∂ Mc (μ*,Q*,Cs)	V2	C, ∂f (Uka) ∂f (Ukb)
—∂C— = ςv DzkaDzkb ” .
We now note, using 4, that if Cs = 1, Σz = 1 we obtain Ck = 1 and thus
dCk2
dCs Cs=1,Σz=1
2s
Σ2 2fc2
g	∕g2(uk )Dk	ʌ
I +σkQSR(gk(uk))2Dk )
∂Mc (μ1,Q1,Cs)
∂Ck	Cs=1,Σz=1
ςJ Dzk(fLy
∂MC(μ*,Q*,Cs)	= E r(∂f(s))2
dCS	Cs=1,∑z =1	. ds
combining the above equations with 21 and comparing the result to 16 completes
the proof.
□
Lemma 2. For any oddfunction g(x), (X )〜N( (，)，∑( C C) )，θ ≤
C ≤ 1 we have Eg(x)g	(y ) ≥0.
Proof of Lemma 2. For C = 1 the proof is trivial. We now assume 0 ≤ C < 1.
We split up R2 into four orthants and consider a point (a, b) with a, b ≥0. We
have	(a)g	(b)	=g	(-a)g	(-b) =	-g	(a)g	(-b)	= -g	(-a)g	(b) ≥0. We will show
that p(a, b) + p(-a, -b) > p(a, -b) + p(-a, b) where p is the probability density
function of (x, y) and hence the points where the integrand is positive will contribute
more to the integral than the ones where it is negative. Plugging these points into
p(x, y ) gives
p(a, b) + p(-a, -b)
p(a, -b) + p(-a, b)
e—α [(a-μ)2 — 2C(b-μ)(a-μ)+(b-μ)2] + e-α [(a+μ)2-2C(b+μ)(a+μ)+(b+μ)2]
e-α[(a-μ)2+2C(b+μ)(a-μ)+(b+μ)2] + g—α[(a+μ)2+2C(b-μ)(a+μ)+(b-μ)2]
where α is some positive constant that depends on the determinant of the covariance
(since C < 1 the matrix is invertible and the determinant is positive).
exp (2αCab) CoSh (2μα(1 — C)(a + b)) ≥ 1
exp (—2αCab) CoSh (2μα(1 — C)(a — b))一
15
Under review as a conference paper at ICLR 2020
where the last inequality holds for 0 ≤ C < 1. It follows that the positive contribu-
tion to the integral is larger than the negative one, and repeating this argument for
every (a, b) in the positive orthant gives the desired claim (if a = 0 or b = 0 the four
points in the analysis are not distinct but the inequality still holds and the integrand
vanishes in any case).	□
Lemma 3. The map 5 is convex in the case of the peephole LSTM.
Proof of Lemma 3. We have
M (C t) _ Jfa f ((Qc - (μC)2)CC + μc2) + / 幽 Jrt rb + 2jftj JL 也上
M(C)=	Qc -μc2
.
From the definition of utkb and Ckt we have
and using Dxg(x)x = Dxg0 (x) we then obtain for any g(x)
∂Ct R DzkaDzkbg(uka)g(Ukb) = R DzkaDzkbg(Uka)g(ukb)喇
=σ2(QC - μc2) R DzkaDzkbgO(Uka)g0(Ukb)
(22)
(Qc - μC2)-^dC J = ∂C2 Z fafb ((Qc - (μC)2)Cc + μC2) + Z iaib Z rarb
=d Rfafb ((Qc- (〃c)2)Cc + μc2) + 2∂CC Rfafb(Qc - (μc)2)
C+ d2 / iαi Rr + + d / iaib d / rarb + R i 工序 / rαrb
+--∂C2	'a'b + - -∂cc ∂cc	+ iib ∂C2
From 22 and non-negativity of some of the integrands
≥ d∂ff ((Qc - (μc)2)Cc+μc2) + d2∂cr / 3b + / iaibd2∂Cτ
.
From Lemma 2 We have R r%rb ≥ 0 and d 必兆=α R g00gb0 ≥ 0 for g = f,i,r. We
thus have
∂ 2M(Cc)
∂C2
≥0
for 0 ≤ Cc ≤ 1.
Convexity of this map has a number of consequences. One immediate one is that the
map has at most one stable fixed point.	□
16
Under review as a conference paper at ICLR 2020
Appendix D	The LSTM cell state distribution
Algorithm 1 LSTM hidden state moment fixed point iteration using cell state sampling
function FIXEDPOINTITERATION(μS-1, QS-1, Θ, n§,niters)
QkT — CALCULATEQK(QS-1, Θ)	. Using 4
Initialize c ∈ Rns
for i J 1 to niters do
Ui, Uf, Ur J SAMPLEUS(Qk-1, Θ)	. Using 3
c J UPDATEC(c, ui, uf, ur)	. Using 13
end for
(μS, Qs) J CALCULATEM(μS-1, QS-1, Θ, C)	. Using 5
return (μs,Qs)
end function
As mentioned in the main text, the cell state differs substantially from other random
variables that appear in this analysis since it cannot be expressed as a function of
a finite number of variables that are Gaussian at the large N and t limit (see Table
1). Since at this limit the uit are independent, by examining the cell state update eqn.
(13) we find that the asymptotic cell state distribution is that of a perpetuity, which
obeys
X =d XY + Z
for some random variables Y, Z . Its stationary distributions will have heavy tails
Goldie (1991). Due to the bounds on Y, Z in the case of the LSTM, one expects
based on results in (Goldie & GrUbeI (1996)) that the tails of the stationary cell state
distribution will be exponential in the case of hard sigmoid nonlinearities or decay
like x-x in the case of soft sigmoid nonlinearities, and this tail behavior is indeed
observed in simulations. Aside from the tails, the bulk of the distribution can take a
variety of different forms and can be highly multimodal, depending on the choice of
Θ which in turn determines the distributions of Y, Z .
We overcome this difficulty by sampling from the stationary cell state distribution.
For a given value of Qh, the variables utk appearing in (13) can be sampled since
their distribution is given by (3) at the large N limit. The update equation (13)
can then be iterated and the resulting samples approximate well the stationary cell
state distribution for a range of different choices of Θ, which result in a variety of
stationary distribution profiles (see Appendix E3). The fixed points of (5) can then
be calculated numerically as in the deterministic cases, yet care must be taken since
the sampling introduces stochasticity into the process.
An example of the fixed point iteration eqn. (5) implemented using sampling is
presented in Algorithm 1. The correlations between the hidden states can be calcu-
lated in a similar fashion. In practice, once the number of samples ns and sampling
iterations niters is of order 100 reasonably accurate values for the moment evolution
and the convergence rates to the fixed point are obtained (see for instance the right
panel of Figure 2). The computational cost of the sampling is linear in both ns , niters
(as opposed to say simulating a neural network directly in which case the cost is
quadratic in ns).
17
Under review as a conference paper at ICLR 2020
Appendix E Additional Experiments and Details of Experiments
E.1 Dynamical system
We simulate the dynamics of Qts in eqn. (5) for a GRU using inputs with Σtz = 0
for t < 10 and Σtz = 1 for t ≥ 10. Note that this evolution is independent of the
evolution of Cst . The results show good agreement in the untied case between the
calculation at the large N limit and the simulation, as shown in Figure 5.
Figure 5: Top: Dynamics of the correlations (6) for the GRU with 3 different values of μf as a
function of time. The dashed line is the prediction from the mean field calculation, while the red
curves are from a simulation of the network with i.i.d. Gaussian inputs. Left: Network with untied
weights. Right: Network with tied weights. Bottom: The predicted fixed point of (6) as a function of
different μf. Left: Network with untied weights. Right: Network with tied weights.
E.2 Heatmaps
In Figure 6 we present results of training on the same task shown in Figure 2 with tied
weights, showing the deviations resulting from the violation of the untied weights
assumption.
E.3 Sampling the LSTM cell state distribution
As described in Appendix D, calculating the signal propagation time scale and
the moments of the state-to-state Jacobian for the LSTM requires integrating with
respect to the stationary cell state distribution. The method for doing this is described
in Algorithm 1. As is shown in Figure 7, this distribution can take different forms
based on the choice of initialization hyperparameters Θ, but in all cases we have
studied the proposed algorithm appears to provide a reasonable approximation to
this distribution efficiently. The simulations are obtained by feeding a network of
width N = 200 with i.i.d. Gaussian inputs.
18
Under review as a conference paper at ICLR 2020
Figure 6: Training accuracy on the padded MNIST classification task described in 5.1 at different
sequence lengths T and hyperparameter values Θ for networks with tied weights. The green curves
are multiples of the forward propagation time scale ξ calculated under the untied assumption. We
generally observe improved performance when the predicted value of ξ is high, yet the behavior of
the network with tied weights is not part of the scope of the current analysis and deviations from the
prediction are indeed observed.
Figure 7: Sampling from the LSTM cell state distribution using Algorithm 1, showing good agreement
with the cell state distribution obtained by simulating a network with untied weights. The two panels
correspond to two different choices of Θ
19
Under review as a conference paper at ICLR 2020
E.4 Critical initializations
Peephole LSTM:
〃i,〃r,μo,ρ2,ρf ,ρ2,ρ2,ν2,νf, ν2, ν2 = 0
μf = 5
σi2 , σf2 , σr2 , σo2 = 10-5
LSTM (Unrolled CIFAR-10 task):
μi,μr ,μo,ρ2,ρf ,ρ2,ρ2,νf, ν2
222
ν2,ν2,σ0,μf = 1
σi2, σf2, σr2 = 10-5
0
The value of μf was found by a grid search, since for this task information necessary
to solve it did not require signal propagation across the entire sequence. In other
words, classification of an image can be achieved with access only to the last few
rows of pixels. The utility of the analytical results in this case, as mentioned
in the text, is to greatly constrain the hyperparameter space of potentially useful
initializations from theoretical considerations.
E.5 Standard initialization
LSTM and peephole LSTM:
Kernel matrices (corresponding to the choice of νk2) : Glorot uniform initialization
Glorot & Bengio (2010)
Recurrent matrices (corresponding to the choice of σk2): Orthogonal initialization
(i.i.d. Gaussian initialization with variance 1/N also used giving analogous perfor-
mance)
μi ,μr ,μo,ρ2,ρf ,ρ2,ρ2 = 0
μf = 1
E.5.1 Long sequence tasks
Learning rate scan: 8 equally spaced points between 10-2 and 10-5. Validation set:
10000 images for MNIST and CIFAR-10. In Table 2, the results for the standard
LSTM on MNIST were reproduced from Arpit et al. (2018) and the results on
CIFAR-10 were reproduced from Trinh et al. (2018).
20