Under review as a conference paper at ICLR 2020
Asynchronous Multi-Agent Generative Ad-
versarial Imitation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Imitation learning aims to inversely learn a policy from expert demonstrations,
which has been extensively studied in the literature for both single-agent settings
with Markov decision processes (MDPs), and multi-agent settings with Markov
games (MGs). However, existing approaches for MGs only work for synchronous
scenarios with all agents simultaneously making decisions at each turn, and do
not work for general MGs, allowing agents to asynchronously make decisions in
different turns. We propose a novel framework, asynchronous multi-agent gen-
erative adversarial imitation learning (AMAGAIL), for general Markov games.
The learned expert policies are proven to guarantee subgame perfect equilibrium
(SPE), a stronger equilibrium than Nash equilibrium (NE). The experiment results
demonstrate that compared to state-of-the-art baselines, our AMAGAIL model
can better infer the policy of each expert agent using their demonstration data
collected from asynchronous decision-making scenarios.
1	Introduction
Imitation learning (IL) also known as learning from demonstrations allows agents to imitate expert
demonstrations to make optimal decisions without direct interactions with the environment. Espe-
cially, inverse reinforcement learning (IRL) (Ng et al. (2000)) recovers a reward function ofan expert
from collected demonstrations, where it assumes that the demonstrator follows an (near-)optimal
policy that maximizes the underlying reward. However, IRL is an ill-posed problem, because a
number of reward functions match the demonstrated data (Ziebart et al. (2008; 2010); Ho & Ermon
(2016); Boularias et al. (2011)), where various principles, including maximum entropy, maximum
causal entropy, and relative entropy principles, are employed to solve this ambiguity (Ziebart et al.
(2008; 2010); Boularias et al. (2011); Ho & Ermon (2016); Zhang et al. (2019)).
Going beyond imitation learning with single agents discussed above, recent works including Song
et al. (2018),Yu et al. (2019), have investigated a more general and challenging scenario with demon-
stration data from multiple interacting agents. Such interactions are modeled by extending Markov
decision processes on individual agents to multi-agent Markov games (MGs) (Littman & Szepesvari
(1996)). However, these works only work for synchronous MGs, with all agents making simultane-
ous decisions in each turn, and do not work for general MGs, allowing agents to make asynchronous
decisions in different turns, which is common in many real world scenarios. For example, in multi-
player games (Knutsson et al. (2004)), such as Go game, and many card games, players take turns to
play, thus influence each other’s decision. The order in which agents make decisions has a significant
impact on the game equilibrium.
In this paper, we propose a novel framework, asynchronous multi-agent generative adversarial imita-
tion learning (AMAGAIL): A group of experts provide demonstration data when playing a Markov
game (MG) with an asynchronous decision-making process, and AMAGAIL inversely learns each
expert’s decision-making policy. We introduce a player function governed by the environment to
capture the participation order and dependency of agents when making decisions. The participa-
tion order could be deterministic (i.e., agents take turns to act) or stochastic (i.e., agents need to
take actions by chance). A player function of an agent is a probability function: given the perfectly
known agent participation history, i.e., at each previous round in the history, we know which agent(s)
participated, it provides the probability of the agent participating in the next round. With the gen-
eral MG model, our framework generalizes MAGAIL (Song et al. (2018)) from the synchronous
1
Under review as a conference paper at ICLR 2020
Markov games to (asynchronous) Markov games, and the learned expert policies are proven to guar-
antee subgame perfect equilibrium (SPE) (Fudenberg & Levine (1983)), a stronger equilibrium than
the Nash equilibrium (NE) (guaranteed in MAGAIL Song et al. (2018)). The experiment results
demonstrate that compared to GAIL (Ho & Ermon (2016)) and MAGAIL (Song et al. (2018)), our
AMAGAIL model can better infer the policy of each expert agent using their demonstration data
collected from asynchronous decision-making scenarios.
2	Preliminaries
2.1	Markov Games
Markov games (MGs) (Littman (1994)) are the cases of N interacting agents, with each agent mak-
ing a sequence of decisions with strategies only depending on the current state. A Markov game1 is
denoted as a tuple (N, S, A, Y, ζ, P, η, r, γ) with a set of states S and N sets of actions {Ai}iN=1. At
each time step t with a state st ∈ S, if the indicator variable Ii,t = 1, an agent i is allowed to take an
action; otherwise, Ii,t = 0, the agent i does not take an action. As a result, the participation vector
It = [Iι,t,…,lN,t] indicates active Vs inactive agents at step t. The set of all possible participation
vectors is denoted as I, namely, It ∈ I. Moreover, h1 = [Io,…，It-ι] represent the PartiCiPa-
tion history from step 0 to t - 1. The player function Y (governed by the environment) describes the
probability of an agent i being allowed to make an action at a step t, given the participation history
ht-1, namely, Y (i|ht-1). ζ defines the participation probability of an agent at the initial time step
ζ : [N] 7→ [0, 1]. Note that, the player function can be naturally extended to a higher-order form
when the condition includes both previous participation history and previous state-action history;
thus, it can be adapted to non-Markov processes. The initial states are determined by a distribution
η : S 7→ [0, 1]. Let φ denotes no participation, determined by player function Y , the transition
process to the next state follows a transition function: P : S×Aι ∪ {φ} ×∙∙∙× AN ∪ {φ} → P(S).
Agent i obtains a (bounded) reward given by a function ri : S ×Ai 7→ R2. Agent i aims to maximize
its own total expected return Ri = Pt∞=0 γtri,t, where γ ∈ [0, 1] is the discount factor. Actions are
chosen through a stationary and stochastic policy πi : S × Ai 7→ [0, 1]. In this paper, bold variables
without subscript i denote the concatenation of variables for all the agents, e.g., all actions as a, the
joint policy defined as π(a∣s) = QN=I ∏i(a∕s), r as all rewards. Subscript -i denotes all agents
except for i, then (a2, a-i) represents the action of all N agents (aι,…,&n). We use expectation
with respect to a policy π to denote an expectation with respect to the trajectories it generates. For
example, E∏,γ[ri(s,ai)]，Est,a〜∏,it〜γ[P∞=o Y%(st,ai)], denotes the following sample pro-
cess as so 〜η, Io 〜Z, It 〜 Y, a 〜π(∙∣st), st+ι 〜P(st+ι∣st, a), for ∀i ∈ [N]. Clearly,
when the player function Y(i|ht-1) = 1 for all agents i’s at any time step t, a general Markov
game boils down to a synchronous Markov game (Littman (1994); Song et al. (2018)), where all
agents take actions at all steps. To distinguish our work from MAGAIL and be consistent with the
literature Chatterjee et al. (2004) and Hansen et al. (2013), we refer the game setting discussed in
MAGAIL as synchronous Markov games (SMGs), and that of our work as Markov games (MGs).
2.2	Subgame Perfect Equilibrium for Markov Games
In synchronous Markov games (SMGs), all agents make simultaneous decisions at any time step
t, with the same goal of maximizing its own total expected return. Thus, agents’ optimal policies
are interrelated and mutually influenced. Nash equilibrium (NE) has been employed as a solution
concept to resolve the dependency across agents, where no agents can achieve a higher expected
reward by unilaterally changing its own policy (Song et al. (2018)). However, in Markov games
(MGs) allowing asynchronous decisions, there exist situations where agents encounter states (sub-
games) resulted from other agents’ “trembling-hand” actions. Since the NE does not consider the
“trembling-hand” resulted states and subgames, when trapped in these situations, agents are not able
to make optimal decisions based on their polices under NE. To address this problem, Selten firstly
proposed subgame perfect equilibrium (SPE) (Selten (1965)). SPE ensures NE for every possible
1 Note that Markov games defined in MAGAIL (Song et al. (2018)) are in fact synchronous Markov games,
with all agents simultaneously making decisions in each turn. We follow the rich literature (Chatterjee et al.
(2004); Hansen et al. (2013)) to define Markov games, which allow both synchronous and asynchronous
decision-making processes.
2Because of the asynchronous setting, the rewards only depend on agents’ own actions.
2
Under review as a conference paper at ICLR 2020
subgame of the original game. It has been shown that in a finite or infinite extensive-form game
with either discrete or continuous time, best-response strategies all converge to SPE, rather than NE
(Selten (1965); Abramsky & Winschel (2017); Xu (2016)).
2.3	Multi-Agent Imitation Learning in Synchronous Markov Games
In synchronous Markov games, MAGAIL (Song et al. (2018)) was proposed to learn experts’ poli-
cies constrained by Nash equilibrium. Since there may exist multiple Nash equilibrium solutions,
a maximum causal entropy regularizer is employed to resolve the ambiguity. Thus, the optimal
policies can be found by solving the following multi-agent reinforcement learning problem.
N
MARL(r) = argmaxX(βHi(πi) + Eπi,πE-i [ri]),	(1)
π i=1
where Hi (∏i) is the γ-discounted causal entropy of policy ∏ ∈ Π, Hi(∏i)，E∏J- log ∏i(a∕s)]=
Est ,ai 〜∏i [- P∞=o Yt log ∏i(ai∣st)], and β is a weight to the entropy regularization term. In practice,
the reward function is unknown. MAGAIL applies multi-agent IRL (MAIRL) below to recover
experts’ reward functions, with ψ as a convex regularizer,
NN
MAIRLψ (∏e) = arg max -ψ(r) + X(EnE [ri]) - (max X(βHi(πi)) + E∏i∏E--i [ri]). (2)
r	i=1	π i=1
Moreover, MAGAIL solves MARL ◦ MAIRLψ (πE) to inversely learn each expert’s policy via
applying generative adversarial imitation learning (Ho & Ermon (2016)) to each expert i ∈ [N]:
N
min max Eπθ	log Dwi (s,
θw θ
i=1
N
ai) +EπE X log(1
i=1
- Dwi (s, ai)) .
(3)
Dwi is a discriminator for agent i that classifies the experts’ vs policy trajectories. πθ represent the
learned experts’ parameterized policies, which generate trajectories with maximized the scores from
Dwi for i ∈ [N].
3	Asynchronous Multi-Agent Imitation Learning
Extending multi-agent imitation learning to general Markov games is challenging, because of the
asynchronous decision making and dynamic state (subgame) participating. In this section, we will
tackle this problem using subgame perfect equilibrium (SPE) solution concept.
3.1	Asynchronous Multi-Agent Reinforcement Learning
In a Markov game (MG), the Nash equilibrium needs to be guaranteed at each state s ∈ S 3, namely,
we apply subgame perfect equilibrium (SPE) solution concept instead. Formally, a set of agent
policies {πi}iN=1 is an SPE if at each state s ∈ S (also considered as a root node of a subgame),
no agent can achieve a higher reward by unilaterally changing its policy on the root node or any
other descendant nodes of the root node, i.e., ∀i ∈ [N], ∀^i = πi, E∏i,∏-i,γ[ri] ≥ E∏i,∏-i,γ[ri].
Therefore, our constrained optimization problem is (Filar & Vrieze (2012), Theorem 3.7.2)
N
ɪminfr(∏, V) = X( X Vi(s∣h) - Eai〜∏i(∙∣s)[qi(s,ai∣h)])	(4)
,	i=1 s∈S,h∈H
s.t. vi(s|h)	≥	qi(s, ai |h)	∀i	∈	[N],	s ∈ S,	ai	∈ Ai, h	∈ H,	(5)
V , [vι; ∙∙∙ ； VN].	(6)
3 Note that in a synchronous Markov game, where each agent makes simultaneous decisions at each time
step t, subgame perfect equilibrium (SPE) is equivalent to Nash equilibrium, since the Nash equilibrium at each
state s (i.e., a subgame) is the same.
3
Under review as a conference paper at ICLR 2020
For an agent i with a probability of taking action a at state st given a history ht-1, its Q-function is
qi(st,ai|ht-1) = Eπ-i,Y [Y (i|ht-1)ri(st, ai)+γ	P r(It|ht-1)	P(st+1|st,ast)vi(st+1|ht)],
It ∈I	st+1 ∈S
(7)
where P r(It|ht-1) = Qi:Ii,t=1 Y (i|ht-1) Qj:Ij,t=0(1 - Y(j|ht-1)) is the probability of partici-
pation vector It given history ht-1. The constraints in eq. (5) guarantee an SPE, i.e., (vi(s|h) -
qi(s, ai|h)) is non-negative for any i ∈ [N]. Consistent with MAGAIL (Song et al. (2018)) the
objective has a global minimum of zero under SPE, and π forms SPE if and only if fr(π, v) reaches
zero while being a feasible solution.
We use AMA-RL(r) to denote the set of policies that form a sSPE under reward function r, and can
maximize γ-discounted causal entropy of policies:
AMA-RL(r) = arg min fr (π, v) - H(π),	(8)
π∈Π,v
s.t. vi(s|h) ≥ qi(s,ai|h) ∀i ∈ [N],s ∈ S,ai ∈ Ai,∀h ∈ H,	(9)
where qi is defined in eq. (7). Our objective is to define a suitable inverse operator AMAIRL in
analogy to MAIRL in eq. (2). The key idea of MAIRL is to choose a reward that creates a margin
between a set of experts and every other set of policies. However, the constraints in SPE optimiza-
tion eq. (8) can make this challenging. To that end, we derive an equivalent Lagrangian formulation
of eq. (8) to defined a margin between the expected rewards of two sets of policies to capture the
“difference”.
3.2 Asynchronous Multi-Agent Inverse Reinforcement Learning
The SPE constraints in eq. (9) state that no agent i can obtain a higher expected reward via 1-step
temporal (TD) difference learning. We replace 1-step constraints with (t+1)-step constraints with
the solution remaining the same as AMARL. The general idea is consistent with MAGAIL (Song
et al. (2018)). The detailed derivation is in Appx A.1. The updated (t+1)-step constraints are
Vi(S⑼;π, r,Z) ≥ Q(t)({s(j), a(j)}j=o; ∏, r,ht-i),	(10)
∀t ∈ N+ , i ∈ [N], s(j) ∈ S, ai(j) ∈ Ai , ht-1 ∈ H.
By implementing the (t+1)-step formulation eq. (10), we aim to construct the Lagrangian dual of
the primal in eq. (8). Since for any policy π, f (π, V) = 0 given ^ defined as in Theorem 1 in
Appx A.1 (proved in Lemma 1 in Appx A.2), we just focus on the constraints in eq. (10) to get the
dual problem
N
mλ≥a0x mπn r	(π, ) ,X X X λg ht-1.)(Q(t) (τi; ∏, r, ht-i) — Vi(s(0); ∏, r,Z)),
≥	i=1 ht-1∈Hτi∈Tit
(11)
where Tit is the set of all length-t trajectories of the form {S(j), ai(j)}tj=0 , with S(0) as initial state,
λ is a vector of N ∙ |Ti(t) | ∙ |H| Lagrange multipliers, and Vi is defined as in Theorem 1 in APPx A.1.
Theorem 2 illustrates that a specific λ is able to recover the difference of the sum of expected rewards
between not all optimal and all optimal policies.
Theorem 2 For any two policies ∏ and π, let
t-1
λ∏(τi; ht-i)= η(s⑼)pr(ht-i) Y(X ∏-i(a-i∣s(j))P(s(j+1) |s(j), a(j))) Y	∏i(a(j)∣s(j))
j=0 aj-i	s(j):Ii,j=1
be the probability of generating the Sequence Ti using policy ∏ and π-% and ht-ι, where
P r(ht-1) = P r(I0) Qtk-=11 P r(Ik|hk-1) is the probability of history ht-1. Then
N
t→∞ Lrt+1)(∏* ,λ∏) = X EniEn-i,γ [ri(s(j),a(j))] - E∏*,γ[ri(s"a”]
i=1
4
Under review as a conference paper at ICLR 2020
where the dualfunction is Lrt+1)(π*, λ∏) and each multiplier can be considered as the probability
of generating a trajectory of agent i ∈ N, τi ∈ Tit, and ht-1 ∈ H.
Theorem 2 (proved in Appx A.3) provides a horizon to establish AMAIRL objective function with
regularizer ψ.
NN
AMA-IRLψ(πE) = arg mrax -ψ(r) + X(EπE,Y [ri]) - (mπaxX(βHi(πi) +Eπi,πE-i,Y [ri])),
i=1	i=1	(12)
where Hi(∏i) = E∏i,∏E J- log ∏i(a∣s)] is the discounted causal entropy for policy ∏ when other
agents follow πE-i, and β is a hyper-parameter controlling the strength of the entropy regularization
term as in GAIL (Ho & Ermon (2016)).
Corollary 2.1. IfI = 1 for all i ∈ [N] then AMA-IRLψ (πE) = MAIRLψ (πE); furthermore, if
N = 1, β = 1 then AMA-IRLψ (πE) = IRLψ(πE).
3.3 Asynchronous Multi-Agent Occupancy Measure Matching
We first define the asynchronous occupancy measure in Markov games:
Definition 1 For an agent i ∈ [N] with a policy πi ∈ Π, define its asynchronous occupancy measure
ρpπi : S × Ai ∪ {φ} 7→ R as
ρpπi (s, a) =
∞
πi⑷S)S(S)Z ⑶ + XXYtPr(St = s∣∏i, ∏E-i )Y (i∣ht-i)),	if a ∈ Ai,
t=1 ht-1
∞
η(S)(I- Z⑶)+ XX YtPT(St = s|ni, πE-i)(1 - γ(ilht-ι)), if a ∈ {φ}.
t=1 ht-1
The occupancy measure can be interpreted as the distribution of state-action pairs that an agent
i encounters under the participating and nonparticipating situations. Notably, when Z(i) = 1,
Y(i|ht-1) = 1 for all t ∈ {1, ..., ∞}, ht-1 ∈ H, asynchronous occupancy measure in MG turns to
the occupancy measure defined in MAGAIL and GAIL, i.e., ρpπ = ρπi . With the additively sepa-
rable regularization ψ, for each agent i, πEi is the unique optimal response to other experts πE-i .
Therefore we obtain the following theorem (see proof of Theorem 3 in Appendix A.4):
Theorem 3 Assume ψ(r) = PiN=1ψi(ri), ψi is convex for each i ∈ [N], and that AMA-RL(r) has
a unique solution4 for all r ∈ AMA-IRLψ (πE), then
N
AMA-RL ◦ AMA-IRLψ (∏e ) = arg min X X -βHi(∏i) + 砂；(夕京用五_
ρpπE)	(13)
—
i=1 h∈H
where πi, E-i denotes πi for agent i, and πE-i for other agents.
In practice, we are only able to calculate ρpπE and ρpπ. As following MAGAIL (Song et al. (2018)),
we match the occupancy measure between ρpπE and ρpπ rather than ρpπE and ρpπi,πE .
4	Practical Asynchronous Multi-Agent Imitation Learning
In this section, we propose practical algorithms for asynchronous multi-agent imitation learning,
and introduce three representative scenarios with different player function structures.
4.1	Asynchronous Multi-Agent Generative Adversarial Imitation Learning
The selected ψi in Proposition 1 (in Appx A.5) contributes to the corresponding generative adver-
sarial model where each agent i has a generator πθi and a discriminator, Dwi. When the generator is
allowed to behave, the produced behavior will receive a score from discriminator. The generator at-
tempts to train the agent to maximize its score and fool the discriminator. We optimize the following
4The set of subgame perfect equilibrium is not always convex, so we have to assume AMA-RL(r) returns
a unique solution.
5
Under review as a conference paper at ICLR 2020
(c) Stochastic participation
(b) Deterministic participation
(a) Synchronous participation
Figure 1: AMAGAIL with three player function structures. (a) Synchronous participation: The
player function is equal to 1, all agents take actions at all time steps. (b) Deterministic partici-
pation: In this example, three agents take turns to make actions with a fixed order. (c) Stochastic
participation: Three agents all have stochastic player functions (i.e., yellow boxes), thus, each
agent has a certain probability to make an action w.r.t the player function given the participation
history ht-1; in this example, only agents #2 and #3 happen to make actions, and agent #1 does not.
objective:
min maxEπθ,Y
NN
XlogDwi(s,ai) +EπE,Y X log(1
i=1	i=1
- Dwi (s, ai))
(14)
In practice, the input of AMAGAIL is Z , the demonstration data from N expert agents in the same
environment, where the demonstration data Z = {(st, a)}T=0 are collected by sampling s0 〜η,
Io〜Z, I〜Y, a〜 π* (佃 ),st+ι 〜 P(st+1∣st, a). The assumptions include knowledge of
N, γ, S , A. Transition P, initial state distribution η, agent distribution ζ, player function Y are all
considered as black boxes, and no additional expert interactions with environment during training
process are allowed. In the RL process of finding each agent’s policy πθi , we follow MAGAIL
(Song et al. (2018)) to apply Multi-agent Actor-Critic with Kronecker-factors (MACK) and use
the advantage function with the baseline Vν for variance reduction. The summarized algorithm is
presented in Algorithm 1 in Appx B.
4.2	Player Function Structures
In MGs, the order in which agents make decisions is determined by the player function Y . Below,
we discuss three representative structures of player function Y, including synchronous participation,
deterministic participation, and stochastic participation.
Synchronous participation. When Y (i|ht-1) = 1 holds for all agents i ∈ [N] at every step t (as
shown in Figure 1a), agents make simultaneous actions, and a general Markov game boils down to
a simple synchronous Markov game.
Deterministic participation. When the player function Y(i|ht-1) is deterministic for all agents
i ∈ [N], it can only output 1 or 0 at each step t. Many board games, e.g., Go, and Chess, have
deterministic player functions, where agents take turns to play. Figure 1b shows an example of
deterministic participation structure.
Stochastic participation. When the player function is stochastic, namely, Y(i|ht-1) ∈ [0, 1] for
some agent i ∈ [N] at certain time step t, the agent i will make an action by chance. As illustrated
in Figure 1c, three agents all have stochastic player functions at step t, and agent #1 does not take
an action at step t, while agent #2 and #3 happen to take actions.
5	Experiments
We evaluate AMAGAIL with both stochastic and deterministic player function structures under
cooperative and competitive games, respectively. We compared our AMAGAIL with two baselines,
including Behavior Cloning (BC) by OpenAI (Dhariwal et al. (2017)) and decentralized Multi-agent
generative adversarial imitation learning (MAGAIL) (Song et al. (2018)). The results are collected
by averaging over 5 random seeds (refer to Appx C for implementation details).
We use the particle environment (Lowe et al. (2017)) as a basic setting, and customize it into four
games to allow different asynchronous player function structures. Deterministic Cooperative Nav-
igation: Three agents (agent #1, #2 and #3) need to cooperate to get close to three randomly placed
landmarks through physical actions. They get high rewards if they are close to the landmarks and
6
Under review as a conference paper at ICLR 2020
(a) Deterministic Cooperative Navigation
-*- Expert - BC - Random + AMAGAIL - MAGAIL
Figure 2: Average true reward from cooperative tasks. Performance of experts and random policies
are normalized to one and zero respectively. We use inverse log scale for better comparison.
are penalized for any collision with each other. Ideally, each agent should cover a single distinct
landmark. In this process, the agents must follow a deterministic participation order to take actions,
i.e., in the first round all three agents act, in the second round only agent #1 and #2 act, in the
third round only agent #1 acts, and repeat these rounds until the game is completed. Stochastic
Cooperative Navigation: This game is the same with deterministic cooperative navigation except
that all three agents have a stochastic player function. Each agent has 50% chance to act at each
round t. Deterministic Cooperative Reaching: This game has three agents with their goals as
cooperatively reaching a single landmark with minimum collision. In this game, agents follow a
deterministic player function, same as that in deterministic cooperative navigation game, to make
actions. Stochastic Predator-Prey: Three slower cooperating agents (referred to as adversaries)
chase a faster agent in an environment of two landmarks; the faster agent acts first, then each adver-
sary with a stochastic player function of 50% chance to act with the same goal of catching the faster
agent. The adversaries and the agent need to avoid two randomly placed landmarks. The adversaries
collect rewards when touching the agent, where the agent is penalized. Note that, an agent that does
not participate in a round of a game does not get a reward.
In these four game environments, agents are first trained with Multi-agent ACKTR (Wu et al. (2017);
Song et al. (2018)), thus the true reward functions are available, which enable us to evaluate the qual-
ity of recovered policies. When generating demonstrations from well-trained expert agents, a “null”
(no-participation) as a placeholder action is recorded for each no-participation round in the trajec-
tory. The quality of a recovered policy is evaluated by calculating agents’ average true reward of
a set of generated trajectories. We compare our AMAGAIL with two baselines - behavior cloning
(BC) (Pomerleau (1991)) and decentralized Multi-agent generative adversarial imitation learning
(MAGAIL) (Song et al. (2018)). Behavior cloning (BC) utilizes the maximum likelihood estimation
for each agent independently to approach their policies. Decentralized multi-agent generative ad-
versarial imitation learning (MAGAIL) treats each agent with a unique discriminator working as the
agent’s reward signal and a unique generator as the agent’s policy. It follows the maximum entropy
principle to match agents’ occupancy measures from recovered policies to demonstration data.
5.1	Performances with Deterministic and Stochastic Play Functions
We compare AMAGAIL with baselines under three particle environment games, namely, deter-
ministic cooperative navigation, stochastic cooperative navigation, and deterministic cooperative
reaching games. Figure 2 show the normalized rewards, when learning policies with BC, MAGAIL
and AMAGAIL, respectively.
When there is only a small amount of expert demonstrations, the normalized rewards of BC and
AMAGAIL increase, especially, when less demonstration data are used, i.e., less than 400 demon-
strations. After a sufficient amount of demonstrations are used, i.e., more than 400, AMAGAIL
has higher rewards than BC and MAGAIL. This makes sense since at certain time steps there exist
non-participating agents (based on the player functions), but BC and MAGAIL models consider the
no-participation as an action the agent can choose, where in reality it is governed by the environ-
ment. On the other hand, with the introduced player function Y , AMAGAIL characterizes such no
participation events correctly, thus more accurately learns the expert policies.
The normalized awards of BC are roughly unchanged in Figure 2(a)&(c), and in Figure 2(b) after
400 demonstrations, which seems contradictory to that of Ross & Bagnell (2010); Song et al. (2018),
and can be explained as follows. In Figure 2(b) (stochastic cooperative navigation), the performance
of BC is low when using less demonstrations, but increases rapidly as more demonstrations are used,
and finally converges to the “best” performance around 0.65 with 300 demonstrations. In Figure 2(a)
(resp. Figure 2(c)), deterministic cooperative navigation (resp. reaching) is easier to learn compared
7
Under review as a conference paper at ICLR 2020
Table 1: Average agent rewards in stochastic predator-prey. We compare behavior cloning (BC)
and multi-agent GAIL (MAGAIL) methods. Best results are marked in bold. Note that high vs low
rewards are preferred, when running BC for agent vs adversaries, respectively).
Task	Stochastic PredatOr-Prey	
Agent Adversaries	Behavior Cloning BC	MAGAIL	AMAGAIL	MAGAIL AMAGAIL Behavior Cloning
Rewards	-5.0 ± 10.8 -9.0 ± 13.1 -14.0 ± 19.4	-3.6 ± 8.5	-2.1 ± 6.9~
with the stochastic cooperative navigation game shown in Figure 2(b), since there is no randomness
in the player function. The performance with only 200 demonstrations is already stabilized at 0.7
(resp. 0.94). In the stochastic cooperative navigation game (Figure 2(b)), AMAGAIL performs
consistently better than MAGAIL and BC. However, in the deterministic cooperative navigation
game (Figure 2(b)), with 200 demonstration, AMAGAIL does not perform as well as MAGAIL. This
is due to the game setting, namely, two players actively searching for landmarks are sufficient to gain
a high reward in this game. The last agent, player #3, learned to be “lazy”, without any motivation
to promote the total shared reward among all agents. In this case, it is hard for AMAGAIL to learn
a good policy of player #3 with small amount of demonstration data, because player #3'S has 3
absence rate, given the pre-defined deterministic participation function. Hence, AMAGAIL does
not have enough state-action pairs to learn player #3. This gets improved when there are sufficient
data, say, more than 400 demonstrations. When we adjust the game setting from 3 landmarks to
1 landmark, i.e., all agents need to act actively to reach the landmark. This is captured in the
deterministic cooperative reaching game. In this scenario, an inactive player will lower down the
overall reward. As shown in Figure 2(c), AMAGAIL outperforms BC and MAGAIL consistently,
even with a small amount of demonstration data.
5.2	Performance with mixed game mode
Now, we further evaluate the performance of AMAGAIL under a mixed game mode with both coop-
erative and adversarial players, i.e., stochastic predator-prey game. Since there are two competing
sides in this game, we cannot directly compare each methods’ performance via expected reward.
Therefore, we use the Song et al. (2018)’s evaluation paradigm and compare with baselines by let-
ting (agents trained by) BC play against (adversaries trained by) other methods, and vice versa.
From Table 1, AMAGAIL consistently performs better than MAGAIL and BC.
6	Related Work, Discussion, and Conclusion
Imitation learning (IL) aims to learn a policy from expert demonstrations, which has been exten-
sively studied in the literature for single agent scenarios (Finn et al. (2016); Ho & Ermon (2016)).
Behavioral cloning (BC) uses the observed demonstrations to directly learn a policy (Pomerleau
(1991); Torabi et al. (2018)). Apprenticeship learning and inverse reinforcement learning (IRL)
((Ng et al. (2000); Syed & Schapire (2008); Ziebart et al. (2008; 2010); Boularias et al. (2011)))
seek for recovering the underlying reward based on expert trajectories in order to further learn a
good policy via reinforcement learning. The assumption is that expert trajectories generated by the
optimal policy maximize the unknown reward. Generative adversarial imitation learning (GAIL) and
conditional GAIL (cGAIL) incorporate maximum casual entropy IRL (Ziebart et al. (2010)) and the
generative adversarial networks (Goodfellow et al. (2014)) to simultaneously learn non-linear policy
and reward functions (Ho & Ermon (2016); Zhang et al. (2019); Baram et al. (2017)). A few recent
studies on multi-agent imitation learning, such as MAGAIL (Song et al. (2018) and MAAIRL (Yu
et al. (2019)), model the interactions among agents as synchronous Markov games, where all agents
make simultaneous actions at each step t. These works fail to characterize a more general and prac-
tical interaction scenario, i.e., Markov games including turn-based games (Chatterjee et al. (2004)),
where agents make asynchronous decisions over steps. In this paper, we make the first attempt to
propose an asynchronous multi-agent generative adversarial imitation learning (AMAGAIL) frame-
work, which models the asynchronous decision-making process as a Markov game and develops a
player function to capture the participation dynamics of agents. Experimental results demonstrate
that our proposed AMAGAIL can accurately learn the experts’ policies from their asynchronous
trajectory data, comparing to state-of-the-art baselines. Beyond capturing the dynamics of partici-
pation vs no-participation (as only two participation choices), our proposed player function Y (and
AMAGAIL framework) can also capture a more general case5, where Y determines how the agent
participates in a particular round, i.e., which action set Aim to follow, with m ∈ [M] and M ≥ 1.
5Thanks for ICLR reviewers for bringing up this interesting idea in the anonymous review process.
8
Under review as a conference paper at ICLR 2020
References
Samson Abramsky and Viktor Winschel. Coalgebraic analysis of subgame-perfect equilibria in
infinite games without discounting. Mathematical Structures in Computer Science, 27(5):751-
761, 2017.
Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end differentiable adversarial imita-
tion learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 390-399. JMLR. org, 2017.
Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,
pp. 182-189, 2011.
Krishnendu Chatterjee, RUPak Majumdar, and Marcin Jurdzinski. On nash equilibria in stochastic
games. In International Workshop on Computer Science Logic, pp. 26-40. Springer, 2004.
Prafulla Dhariwal, ChristoPher Hesse, Oleg Klimov, Alex Nichol, Matthias PlaPPert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OPenai baselines. GitHub, https:
//github.com/openai/baselines, 2017, 2017.
Jerzy Filar and Koos Vrieze. Competitive Markov decision processes. SPringer Science & Business
Media, 2012.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: DeeP inverse oPtimal control
via Policy oPtimization. In International Conference on Machine Learning, PP. 49-58, 2016.
Drew Fudenberg and David Levine. Subgame-Perfect equilibria of finite-and infinite-horizon games.
Journal of Economic Theory, 31(2):251-268, 1983.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, PP. 2672-2680, 2014.
Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly Poly-
nomial for 2-Player turn-based stochastic games with a constant discount factor. Journal of the
ACM (JACM), 60(1):1, 2013.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, PP. 4565-4573, 2016.
Bjorn Knutsson, Honghui Lu, Wei Xu, and Bryan HoPkins. Peer-to-Peer suPPort for massively
multiPlayer games. In IEEE INFOCOM 2004, volume 1. IEEE, 2004.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, PP. 157-163. Elsevier, 1994.
Michael L Littman and Csaba Szepesvari. A generalized reinforcement-learning model: Conver-
gence and aPPlications. In ICML, volume 96, PP. 310-318, 1996.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, pp. 2, 2000.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neu-
ral Computation, 3(1):88-97, 1991.
Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings ofthe
thirteenth international conference on artificial intelligence and statistics, pp. 661-668, 2010.
9
Under review as a conference paper at ICLR 2020
Reinhard Selten. SPieltheoretische behandlung eines oligopolmodells mit nachfragetragheit: Teil
i: Bestimmung des dynamischen PreisgleichgeWichts. Zeitschrift fur die gesamte Staatswis-
Senschaf/Journal OfInstitutional and Theoretical Economics, (H. 2):301-324,1965.
Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. Multi-agent generative adversarial
imitation learning. In Advances in Neural Information Processing Systems, PP. 7461-7472, 2018.
Umar Syed and Robert E SchaPire. A game-theoretic aPProach to aPPrenticeshiP learning. In
Advances in neural information processing systems, PP. 1449-1456, 2008.
Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint
arXiv:1805.01954, 2018.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deeP reinforcement learning using kronecker-factored aPProximation. In Advances in
neural information processing systems, PP. 5279-5288, 2017.
Zibo Xu. Convergence of best-resPonse dynamics in extensive-form games. Journal of Economic
Theory, 162:21-54, 2016.
Lantao Yu, Jiaming Song, and Stefano Ermon. Multi-agent adversarial inverse reinforcement learn-
ing. arXiv preprint arXiv:1907.13220, 2019.
Xin Zhang, Yanhua Li, Xun Zhou, and Jun Luo. Unveiling taxi drivers strategies via cgail-
conditional generative adversarial imitation learning. In 2019 IEEE International Conference
on Data Mining (ICDM). IEEE, 2019.
Brian D Ziebart, AndreW L Maas, J AndreW Bagnell, and Anind K Dey. Maximum entroPy inverse
reinforcement learning. In AAAI, volume 8, PP. 1433-1438. Chicago, IL, USA, 2008.
Brian D Ziebart, J AndreW Bagnell, and Anind K Dey. Modeling interaction via the PrinciPle of
maximum causal entroPy. International Conference on Machine Learning (ICML), 2010.
10
Under review as a conference paper at ICLR 2020
A Appendix A. Proofs
A. 1 Time Difference Learning
Theorem 1. For a certain policy π and reward r, let Vi(s(t); π, r, ht-ι) be the unique solution to
the Bellman equation:
Vi(s㈤;π, r, ht-i) =En [Y(i∣h"ri(s⑴,a(t)) + Y X Pr(It∣ht-i) X P(S(MIS⑴,a⑴加用+1))],
It∈I	s(t+1)∈S
t ∈ N+,∀S(t) ∈ S,ht-1 ∈ H.
Denote q(t)({s(j), a(j)}j-=10, S(t), ai(t); π, r, ht-1) as the discounted expected return for the i-th
agent conditioned on visiting the trajectory {S(j), a(j)}tj-=10, S(t) in the first t - 1 steps and choosing
action ai(t) at the t-th step, when other agents using policy π-i:
q(t)({s(j), a(j)}j=0, s(t),a(t)； ∏, r,ht-i)
t-1
=Xγjri(S(j),ai(j))Ii,j
j=0
+ γtE∏-i[Y(i∣ht-i)ri(s(t),a(t)) + Y X Pr(It∖ht-i) X P(s(t+1)|s(t), a(t))vi(s(t+1)； ∏, r,h)].
It∈I	s(t+1)∈S
Then π is subgame perfect equilibrium if and only if:
Vi(s(0); ∏, r,ζ) ≥E∏-i [q(t)({s(j), a(j)}j=0, S⑶,a(t)； ∏, r, ht-i)]
(15)
,Qi ({S(j), aij }tj=0; π, r, ht-1)
∀t ∈ N+ , i ∈ [N], S(j) ∈ S, ai(j) ∈ Ai , ht-1 ∈ H.
Theorem 1 illustrates that if we replace the 1-step constraints with (t + 1)-step constraints, we still
get the same solution as AMA-RL(r) in terms of a subgame perfect equilibrium solution.
A.2 Existence and Equivalence of V and Subgame Perfect Equilibrium
Lemma 1 By definition of Vi(s(t); π, r, ht-ι) in Theorem 1 and (ji(s(t`),ai； ∏, r, ht-ι) in eq. 7. Then
for any π, f (π, V) = 0. Furthermore, π is subgame perfect equilibrium under r if and only if
Vi(s; π, r,ht-i) ≥ ^i(s, ai； ∏, r, ht-i) for all i ∈ [N], S ∈ S, a% ∈ Ai and ht-i ∈ H.
Proof We have
Vi(s(t); ∏, r, ht-i)
=EπhY(i∖ht-1)ri(S(t),ai(t))+Y X Pr(It∖ht-1) X P(S(t+1)∖S(t),a(t))vi(S(t+1))i
It∈I	s(t+1) ∈S
=EπiEπ-ihY(i∖ht-i)ri(S(t),ai(t))+Y XPr(It∖ht=i) X P(S(t+i) ∖S(t), a(t))vi(S(t+i))i
It∈I	s(t+1)∈S
=E∏i [qi(s(t),a(t); π, r, ht-i)].
which utilizes the fact that ai and a-i are independent at s. Therefore, We can easily get f (π, V)=
0.
If π is a subgame perfect equilibrium, and existing one or more of the constrains does not hold,
so agent i can receive a strictly higher expected reWard for rest of the states, Which is against the
subgame perfect equilibrium assumption.
If the constraints hold, i.e., for all i and (s, ai), ^i(s; π, r, ht-i) ≥ ^i(s, ai； ∏, r, ht-i) then
Vi(s； ∏, r, ht-i) ≥ E∏i [qi(s, ai； ∏, r, ht-i)] = Vi(s； ∏, r, ht-i).
11
Under review as a conference paper at ICLR 2020
Value iteration, thus, over Vi(s; π, r, ht-ι) converges. If one can find another policy π0 so that
Vi(s; π, r, ht-ι) < E∏i [^i(s, a%; π, r, ht-ι)], then at least one violation exists in the constraints
since πi0 is a convex combination over action ai . Therefore, for any policy πi0 and action ai for
any agent i, E∏J^i(s, ai； π, r, ht-ι)] ≥ E∏o [qi(s, ai； ∏, r, ht-ι)] always hold, so ∏ is the optimal
reply to π-i, and π constitutes a subgame perfect equilibrium once it repeats this argument for all
agents. Notably, by assuming f (π, v) = 0 for some v; if V satisfies the assumptions, then V = v.
A.3 Proof to Theorem 2
Proof We use Q*,q*,V* to denote the Q, q and V quantities defined for policy π*. For the two
terms in L%+1) (π*, λ∏) We have:
N
Lt+1)(π*,λ∏) , X X X λ*G; ht-i)(Q↑(τi; π*, r,ht-i) - Vi (S ⑼；∏*, r,Z))
i=1 ht-1∈H τi∈Tit
For agent i, τi and ht-1 we have,
λ∏(Ti； ht-i) ∙ QKTi πi, r, ht-i) = Pr(Ti； ht-i) ∙ Qi(方；∏i, r, ht-i).
For any agent i, we note that
X X λ∏(τi; ht-1) ∙ Qi (τi; πi, r,ht-i)
ht-1 ∈H τi ∈Ti
t-1
=EniEn-i[X Yj ri(s⑶,a(j))Ii,j+ Y tE∏-i [Y(i∣ht-i)ri(s⑴,a(t))+
j=0
Y XPr(It∖ht-i) X P(s(t+1)∣s(t), a(t))vi(s(t+1); πi, r, ht)]]
It	s(t+1) ∈S
t-1
=EniEn-i,Y [X Yj ri(Sj ʤj + YtqM- πi, r,ht-1)]
j=0
which is using πi for agent i for the first t steps and using πii for the remaining steps, whereas
other agents follow π-钎 As t → ∞, this converges to Eni E∏ι i,γ [ri(sj), a(j))] as Yt → 0 and
qi(s(t), a(t); πi, r, ht-ι) is bounded. Moreover, for Vi(S⑼;πi, r,Z) we have
X X λ*(τi; ht-i)Vi(s(0);n*, r,ζ)= Es(0)〜η[Vi(s⑼;πi, r,Z)] = E∏*,γ[ri(s⑶,a"].
ht-1 ∈H τi∈Tit
Combining the two we have,
N
lim Lrt+1)(πi,λ∏) = XE∏iE∏*i,γ[ri(s(j),a(j))] - E∏*,γ[ri(s⑶,a"]
t-→∞	-i
i=1
which describes the differences in expected rewards.
A.4 Proof to Theorem 3
Proof For a single agent i where other agents have policy πE-i, we give the following analysis and
definition.
For a policy πi ∈ Π, its occupancy measure defined in Def. 3.3 allows us to write Eni,Y [ri (S, a)] =
Ps,a ρpni (S, a)ri(S, a) for any reward function ri. A basic result is that the set of valid occupancy
measures Di , {ρpni : πi ∈ Π} can be written as a feasible set of affine constraints:
Di
={ρpni : ρpni ≥ 0 and
X	ρpni (S, a) = η(S) +YXP(S∖S0,a)πE-i(a-i∖S0)ρpni(S0,ai) ∀S ∈ S}.
a∈Ai∪{φ}	s0,a
12
Under review as a conference paper at ICLR 2020
Therefore, the proof of AMA-RL◦ AMA-IRL can be derived in a similar fashion with GAIL (Ho &
Ermon (2016)) and MAGAIL (Song et al. (2018)).
A.5 Proposition 1
Proposition 1:
otherwise, and
then
If β = 0 and ψ(r) = PiN=1 ψi(ri) where ψi(ri) = EπE,Y [g(ri)] if ri > 0; +∞
( )	-x - log(1 - ex) if ri > 0
g	+∞	o.w.
NN
arg m∏n X ψi (Pni,∏E-i - p∏e ) = arg min X ψi (ρ∏i,∏-i - p∏e ) = πe.
i=1	i=1
Theorem 3 and Proposition 1 discuss the differences from the single agent scenario similar in Song
et al. (2018). On the one hand, in Theorem 3 we make the assumption that AMA-RL(r) has a
unique solution, which is always true in the single agent case due to convexity of the space of the
optimal policies. On the other hand, in Proposition 1 we remove the entropy regularizer because here
the causal entropy for πi may depend on the policies of the other agents, so the entropy regularizer
on two sides are not the same quantity. Specifically, the entropy for the left hand side conditions
on πE-i and the entropy for the right hand side conditions on π-i (which would disappear in the
single-agent case).
B Appendix B. Algorithm
Algorithm 1 Asynchronous Multi-Agent GAIL (AMAGAIL)
Input: Initial parameters of policies, discriminators and value (baseline) estimators, θ, w , ν ; state-
action pair demonstrations Z = {(sj , a)}jT=0; batch size B; Markov game as a block box
(N,S,A,P,η,ζ,Y,r,γ).
Output: Learned policies ∏θ:s and reward functions Dwi,s, for i ∈ [N].
1:	for each epoch u = 0, 1, 2, ... do
2:	Generate state-action pairs of batch size B from πu through the process: so 〜η, Io 〜
Z, It 〜 Y, a 〜 πu(∙∣st),st+ι 〜 P(st+ι∣st, a); φ is recorded as a placeholder action when
an agent does not participate in a round; denote the generated state-action pair set as X .
3:	Sample state-action pairs from Z with batch size B; denote the demonstrated state-action
pair set as XE.
4:	for each agent i = 1,…，N do
5:	Filter out state-action pairs (s, φ) from X and XE.
6:	Update wi to increase the objective: EX,Y [log Dwi (s, ai)] +EXE,Y [log(1 - Dwi (s, ai))].
7:	end for
8:	for each agent i = 1,…，N do
9:	Compute value estimate Vi* and advantage estimate Ai for (s, aQ ∈ X.
10:	Filter out state-action pairs (s, φ) from X and XE.
11:	Update νi to decrease the objective: EX,Y [(Vνi (s) - V *(s))2].
12:	Update θi by policy gradient with the setting step sizes: EX,Y [Oθi πθi (ai|si)Ai(s, a)].
13:	end for
14:	end for
15:	Return learned policies ∏θi's and reward functions DwJs, for i ∈ [N].
C Appendix C. Experiment Details
C.1 Hyperparameters
For the particle environment, we follow the setting of MAGAIL (Song et al. (2018)) to use two
layer multiple layer perceptrons with 128 cells in each layer for the policy generator network, value
13
Under review as a conference paper at ICLR 2020
Table 2: Performance in stochastic cooperative navigation.
#Expert Episodes	200	400	600	800	1000
Expert Random	-12.5 ± 6.0 -61.6 ± 20.0
Behavior Cloning MAGAIL AMAGAIL	-45.8 ± 12.0^^-30.7 ± 9.9^^-30.8 ± 10.4^^^-30.9 ± 10.5^^-30.1 ± 9.8 -34.4 ± 13.5	-25.4 ± 8.9	-24.5 ± 8.3	-25.8 ± 8.4	-26.6 ± 8.4 -26.1 ± 8.8	-19.0 ± 8.5	-17.5 ± 8.2	-16.6 ± 7.9	-16.0 ± 7.3
Table 3: Performance in deterministic cooperative navigation.
#Expert Episodes	200	400	600	800	1000
Expert Random	-13.8 ± 6.8 -61.6 ± 16.5
Behavior Cloning MAGAIL AMAGAIL	-29.3 ± 11.0^^-29.0 ± 10.8^^-28.8 ± 10.8^^-28.7 ± 10.6^^-29.0 ± 10.8 -19.0 ± 7.6	-18.3 ±	7.5	-18.3 ± 7.3	-18.8 ± 7.3	-20.0 ± 8.0 -26.6 ± 7.8	-17.5 ±	7.0	-17.2 ± 6.9	-17.1 ± 6.9	-17.0 ± 7.0
network and the discriminator. We use a batch size of 1,000. The policy is trained using Kronecker-
factored Approximate Curvature (K-FAC) optimizer (Martens & Grosse (2015)) with parameters
the same in Song et al. (2018).
C.2 Detailed Results
Below we list the exact performance (average over agents and before normalization) in tables 2, 3
and 4. The means and standard deviations are computed over 1,000 epidodes. The policies in the
cooperative tasks are trained with varying number of expert demonstrations. The policies in the
competitive task are trained on a dataset with 1,000 expert trajectories.
The environment for each episode is drastically different (e.g. location of landmarks are randomly
sampled), which leads to the seemingly high standard deviation across episodes.
For each game with different numbers of expert demonstrations, we consider the average reward of a
random policy as the minimum reward, and the average reward of an expert policy as the maximum
reward. Then, we obtain normalized reward by running min-max normalization to normalize the
rewards from different methods. The normalized rewards are used in Figure 2 in the paper.
Table 4: Performance in deterministic cooperative reaching.
#Expert Episodes	200	400	600	800	1000
Expert Random	-78.1 ± 16.4 -140.7 ± 30.3
Behavior Cloning MAGAIL AMAGAIL	-81.8 ± 17.2^^-82.0 ± 17.3^^-81.9 ± 17.0^^-82.0 ± 17.5^^-81.6 ± 17.1 -79.9 ± 17.2	-79.5 ± 16.9	-79.5 ± 16.8	-79.6 ± 16.8	-79.5 ± 17.1 -79.6 ± 16.7	-79.3 ± 17.2	-79.1 ± 16.9	-79.0 ± 16.8	-79.0 ± 16.90
14