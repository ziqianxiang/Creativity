Under review as a conference paper at ICLR 2020
Gradient Perturbation is Underrated for
Differentially Private Convex Optimization
Anonymous authors
Paper under double-blind review
Abstract
Gradient perturbation, widely used for differentially private optimization,
injects noise at every iterative update to guarantee differential privacy.
Previous work first determines the noise level that can satisfy the privacy
requirement and then analyzes the utility of noisy gradient updates as in
non-private case. In this paper, we explore how the privacy noise affects the
optimization property. We show that for differentially private convex opti-
mization, the utility guarantee of both DP-GD and DP-SGD is determined
by an expected curvature rather than the minimum curvature. The expected
curvature represents the average curvature over the optimization path, which
is usually much larger than the minimum curvature and hence can help us
achieve a significantly improved utility guarantee. By using the expected
curvature, our theory justifies the advantage of gradient perturbation over
other perturbation methods and closes the gap between theory and practice.
Extensive experiments on real world datasets corroborate our theoretical
findings.
1 Introduction
Machine learning has become a powerful tool for many practical applications. The training
process often needs access to some private dataset, e.g., applications in financial and medical
fields. Recent work has shown that the model learned from training data may leak unintended
information of individual records (Fredrikson et al., 2015; Wu et al., 2016; Shokri et al., 2017;
Hita j et al., 2017). It is known that Differential privacy (DP) (Dwork et al., 2006a;b) is a
golden standard for privacy preserving data analysis. It provides provable privacy guarantee
by ensuring the influence of any individual record is negligible. It has been deployed into real
world applications by large-scale corporations and U.S. Census Bureau (Erlingsson et al.,
2014; McMillan, 2016; Abowd, 2016; Ding et al., 2017).
We study the fundamental problem when differential privacy meets machine learning: the
differentially private empirical risk minimization (DP-ERM) problem (Chaudhuri & Mon-
teleoni, 2009; Chaudhuri et al., 2011; Kifer et al., 2012; Bassily et al., 2014; Talwar et al., 2015;
Wu et al., 2017; Zhang et al., 2017; Wang et al., 2017; Smith et al., 2017; Jayaraman et al.,
2018; Feldman et al., 2018; Iyengar et al., 2019; Wang & Gu, 2019). DP-ERM minimizes
the empirical risk while guaranteeing that the output of learning algorithm is differentially
private with respect to the training data. Such privacy guarantee provides strong protection
against potential adversaries (Hitaj et al., 2017; Rahman et al., 2018). In order to guarantee
privacy, it is necessary to introduce randomness to the algorithm. There are usually three
ways to introduce randomness according to the time of adding noise: output perturbation,
objective perturbation and gradient perturbation.
Output perturbation (Wu et al., 2017; Zhang et al., 2017) first runs the learning algorithm
the same as in the non-private case then adds noise to the output parameter. Objective
perturbation (Chaudhuri et al., 2011; Kifer et al., 2012; Iyengar et al., 2019) perturbs the
objective (i.e., the empirical loss) then release the minimizer of the perturbed objective.
Gradient perturbation (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016; Wang et al.,
2017; Lee & Kifer, 2018; Jayaraman et al., 2018) perturbs each intermediate update. If each
1
Under review as a conference paper at ICLR 2020
update is differentially private, the composition theorem of differential privacy ensures the
whole learning procedure is differentially private.
Gradient perturbation comes with several advantages over output/objective perturbations.
Firstly, gradient perturbation does not require strong assumption on the ob jective because it
only needs to bound the sensitivity of gradient update rather than the whole learning process.
Secondly, gradient perturbation can release the noisy gradient at each iteration without
damaging the privacy guarantee as differential privacy is immune to post processing (Dwork
et al., 2014). Thus, it is a more favorable choice for certain applications such as distributed
optimization (Rajkumar & Agarwal, 2012; Agarwal et al., 2018; Jayaraman et al., 2018).
At last, gradient perturbation often achieves better empirical utility than output/ob jective
perturbations for DP-ERM.
However, the existing theoretical utility guarantee for gradient perturbation is the same as or
strictly inferior to that of other perturbation methods as shown in Table 1. This motivates
us to ask
“What is wrong with the theory for gradient perturbation? Can we justify the empirical
advantage of gradient perturbation theoretically?”
We revisit the analysis for gradient perturbation approach. Previous work (Bassily et al.,
2014; Wang et al., 2017; Jayaraman et al., 2018) derive the utility guarantee of gradient
perturbation via two steps. They first determine the noise variance at each step that meets
the privacy requirement and then derive the utility guarantee by using the convergence
analysis the same as in non-private case. However, the noise to guarantee privacy naturally
affects the optimization procedure, but previous approach does not exploit the interaction
between privacy noise and optimization of gradient perturbation.
In this paper, we utilize the fact the privacy noise affects the optimization procedure and
establish new and much tighter utility guarantees for gradient perturbation approaches. Our
contribution can be summarized as follows.
•	We introduce an expected curvature that can characterize the optimization property
accurately when there is perturbation noise at each gradient update.
•	We establish the utility guarantees for DP-GD for both convex and strongly convex
ob jectives based on the expected curvature rather than the usual minimum curvature.
•	We also establish the the utility guarantees for DP-SGD for both convex and strongly
convex objectives based on the expected curvature. To the best of our knowledge,
this is the first work to remove the dependency on minimum curvature for DP-ERM
algorithms.
In DP-ERM literature, there is a gap between the utility guarantee of non-strongly convex
ob jectives and that of strongly convex objectives. However, by using the expected curvature,
we show that some of the non-strongly convex ob jectives can achieve the same order of
utility guarantee as the strongly convex ob jectives, matching the empirical observation. This
is because the expected curvature could be relatively large even for non-strongly convex
objectives.
As we mentioned earlier, prior to our work, there is a mismatch between theoretical guar-
antee and empirical observation of gradient perturbation approach compared with other
two perturbation approaches. Our result theoretically justifies the advantage of gradient
perturbation and close the mismatch.
1.1	Paper Organization
The rest of this paper is organized as follows. Section 2 introduces notations and the
DP-ERM task. In Sections 3, we first introduce the expected curvature and establish the
utility guarantee of both DP-GD and DP-SGD based on such expected curvature. Then we
give some discussion on three perturbation approaches. We conduct extensive experiments
in Section 4. Finally, we conclude in Section 5.
2
Under review as a conference paper at ICLR 2020
Table 1: Expected excess empirical risk bounds under (, δ)-DP, where n and p are the
number of samples and the number of parameters, respectively, and β, μ and V are the
smooth coefficient, the strongly convex coefficient and the expected curvature, respectively,
and v ≥ μ (see Section 3.1). We note that μ = 0 denotes the convex but not strongly convex
objective. The Lipschitz constant L is assumed to be 1. We omit log (1∕δ) for simplicity.
Authors	Perturbation	Algorithm	Utility (μ = 0)	Utility (μ > 0)
Chaudhuri et al. (2011)	Objective	N/A	o( '√)一	O(H ) ∖ μn2e2 I
Zhang et al. (2017)	Output	GD	O ((√p K/∖	-OT≡βO- ∖ μ2n2e2 )
Bassily et al. (2014)	Gradient	SGD	O (√plog3/2(n))	o (p log2(n)) ∖ μn2e2 )
Jayaraman et al. (2018)	Gradient	GD	N/A	^-O7⅛≡)F ∖ μ2n2e2 )
Ours	Gradient	GD	-o (√p ∧ βp⅛(i))- n	ν2n22	o (βp2lo2(n) Γ~ ν2n2e2
Ours	Gradient	SGD	O (√ Iogs) ∧ p log(n)) k ne	νn2e2 J	O(P⅛)厂 νn2e2
2	Preliminary
We introduce notations and definitions in this section. Given dataset D = {d1, . . . , dn}, the
objective function F(x; D) is defined as F(x; D) = 1 Pn=1 f (x; di), where f(x; di) : Rp → R
is the loss of model x ∈ Rp for the record di .
For simplicity, we use F(x) to denote F(x; D). We use kvk to denote the l2 norm of a
vector v. We use Xf = argminχ∈Rp f (x) to denote the set of optimal solutions of f (x).
Throughout this paper, we assume Xf non-empty.
Definition 1 (Objective properties). For any x, y ∈ Rp , a function f : Rp → R
•	is L-Lipschitz if |f (x) - f(y)| ≤ L kx - yk.
•	is β-smooth if f(y) ≤ f(x) + hVf (x), y — Xi + 2 ∣∣y — x∣∣2.
•	is convex ifhVf (x) — Vf (y), X — y)≥ 0.
•	is μ-strongly convex (or μ-SC) if {Vf (x) — Vf (y), X — y) ≥ μ ∣∣x — y∣2.
The strong convexity coefficient μ is the lower bound of the minimum curvature of function
f over the domain.
We say that two datasets D, D0 are neighboring datasets (denoted as D 〜D0) ifD can be
obtained by arbitrarily modifying one record in D0 (or vice versa). In this paper we consider
(, δ)-differential privacy as follows.
Definition 2 ( (, δ)-DP (Dwork et al., 2006a;b)). A randomized mechanism M : D → R
guarantees (, δ)-differential privacy if for any two neighboring input datasets D, D0 and for
any subset of outputs S ⊆ R it holds that Pr[M(D) ∈ S] ≤ e Pr[M(D0 ) ∈ S] + δ.
We note that δ can be viewed as the probability that original -DP fails and a meaningful
setting requires δ《n. By its definition, differential privacy controls the maximum influence
that any individual record can produce. Smaller , δ implies less information leak but usually
leads to worse utility. One can adjust , δ to trade off between privacy and utility.
DP-ERM requires the output Xout ∈ Rp is differentially private with respect to the input
dataset D. Let x* ∈ XF be one of the optimal solutions of F(x), the utility of DP-ERM
algorithm is measured by expected excess empirical risk: E[F(Xout) — F(x*)], where the
expectation is taken over the algorithm randomness.
3
Under review as a conference paper at ICLR 2020
3	Main Results
In this section, we first define the expected curvature ν and explain why it depends only on
the average curvature. We then use such expected curvature to improve the analysis of both
DP-SGD and DP-GD.
3.1	Expected Curvature
In non-private setting, the analysis of convex optimization relies on the strongly convex
coefficient μ, which is the minimum curvature over the domain and can be extremely small
for some common objectives. Previous work on DP-ERM uses the same analysis as in
non-private case and therefore the resulting utility bounds rely on the minimum curvature.
In our analysis, however, we avoid the dependency on the minimum curvature by exploiting
how the privacy noise affects the optimization. With the perturbation noise, the expected
curvature that the optimization path encounters is related to the average curvature instead of
the minimum curvature. Definition 3 uses ν to capture such average curvature with Gaussian
noise. We use x* = argmi□χ∈χ* ∣∣x — xj∣ to denote the closest solution to the initial point.
Definition 3 (Expected curvature). A convex function F : Rp → R, has expected curvature
V with respect to noise N(0, σ2Ip) if for any X ∈ Rp and X = X — Z where Z 〜N(0, σ2Ip),
it holds that
EKVF(X),X — x*〉] ≥ νE[∣∣X — x*k2],	(1)
where the expectation is taken with respect to Z .
Claim 1. If F is μ-strongly convex, we have V ≥ μ.
Proof. It can be verified that V = μ always holds because of the strongly convex definition. □
In fact, v represents the average curvature and is much larger than μ. We use X0
to denote the transpose of X. Let Hx = V2F(X) be the Hessian matrix evaluated at X. We
use Taylor expansion to approximate the left hand side of Eq (1) as follows
E[hVF(X), X — X*i] ≈ E[hVF(x) — Hxz, X — Z — x*〉]
=	hVF (X), X — X*i + E[Z0HxZ]
=	hVF (X), X — X*〉 + σ2 tr(Hx).
For convex objective, the Hessian matrix is positive semi-definite and tr(Hx) is the sum of
the eigenvalues of Hx. We can further express out the right hand side of Eq (1) as follows
E[∣∣X — X*k2] = E[kx — z — X*k2] = v (∣∣x — X*k2 + pσ2).
Based on the above approximation, we can estimate the value of V in Definition 3: V .
tr(Hx2j+μkx-χ*k . For relatively large σ2, this implies V ≈ tr(Hx) that is the average
pσ2+kx-x* k	p
curvature at X. Large variance is a reasonable setting because meaningful differential privacy
guarantee requires non-trivial amount of noise.
The above analysis suggests that V can be independent of and much larger than μ. This is
indeed true for many convex objectives. Let us take the l2 regularized logistic regression
as an example. The ob jective is strongly convex only due to the l2 regularizer. Thus, the
minimum curvature (strongly convex coefficient) is the regularization coefficient λ. Sharmir
et al. [1] shows the optimal choice of λ is Θ(n-1/2) (Section 4.3 in [1]). In practice, typical
choice of λ is even smaller and could be on the order of n-1. Figure 1 compares the minimum
and average curvatures of regularized logistic regression during the training process. The
average curvature is basically unaffected by the regularization term λ. In contrast, the
minimum curvature reaches λ in first few steps. Therefore removing the dependence on
minimum curvature is a significant improvement. We also plot the curvatures for another
dataset KDDCup99 in the Appendix C. The resulting curvatures are similar to Figure 1.
Perturbation noise is necessary to attain v > μ. We note that V = μ when the training
process does not involve perturbation noise (corresponding to σ = 0 in Definition 3). For
4
Under review as a conference paper at ICLR 2020
Figure 1: Curvatures of regularized lo-
gistic regression on Adult dataset over
training. Dot/cross symbol represents av-
erage/minimum curvature respectively.
Figure 2: Illustration of a generic loss
function in the high dimensional setting
(p>n, Figure 3 in Negahban et al. (2012)).
example, objective/output perturbation cannot utilize this expected curvature condition as
no noise is injected in their training process. Therefore, among three existing perturbation
methods, gradient perturbation is the only method can leverage such effect of noise.
We note that μ = 0 does not necessarily lead to V = 0. A concrete example is given
in Figure 2 (from Negahban et al. (2012)). It provides an illustration of the loss function in
the high-dimensional (p > n) setting, i.e., the resticted strongly convex scenario: the loss is
curved in certain directions but completely flat in others. The average curvature of such
objective is always positive but the worst curvature is 0. Though some recent work shows the
utility guarantee of high dimensional DP-ERM task may not depend on the worst curvature
(Wang & Gu, 2019), Figure 2 still provides a good illustration for the case of ν > μ = 0.
Moreover, as shown in Figure 1, the average curvature of logistic regression on Adult dataset
is above 0 during the training procedure even the regularization term is 0. As we will show
later, a positive ν over the optimization path is sufficient for our optimization analysis.
3.2 Utility Guarantee of DP-GD Based on Expected Curvature
In this section we show that the expected curvature can be used to improve the utility bound
of DP-GD (Algorithm 1).
Algorithm 1: Differentially Private Gradient Descent (DP-GD)
Input: Privacy parameters , δ; running steps T ; learning rate η. Loss function F (x)
with Lipschitz constant L.
for t = 1 to T do
Compute gt = VF (Xt).
Update parameter xt+1 = Xt — ηt (gt + Zt), where Zt 〜N(0, σ2Ip).
end for
Algorithm 1 is (e, δ)-DP if We set σt = Θ (LvZTnog(I/δ)) (Jayaraman et al., 2018). Let
X1, . . . , XT be the training path and ν = min{ν1, . . . , νT} be the minimum expected curvature
over the path. Now we present the utility guarantee of DP-GD for the case of ν > 0 .
Theorem 1 (Utility guarantee, ν > 0.). Suppose F is L-Lipschitz and β-smooth with ν
expected curvature. Set η ≤ β, T = 2崂、and σt = Θ (LPT log(1∕δ)/ne), We have
E[F (XT +ι) - F (x* )] = O ( β ⅛⅞2 鬻g")).
Proof. All proofs in this paper are relegated to Appendix A.	□
Remark 1. Theorem 3 only depends on the expected curvature over the training path ν.
5
Under review as a conference paper at ICLR 2020
The expectation is taken over the algorithm randomness if without specification. Theorem 1
significantly improves the original analysis of DP-GD because of our arguments in Section 3.1.
If ν = 0, then the curvatures are flatten in all directions. One example is the linear function,
which is used by Bassily et al. (2014) to derive their utility lower bound. Such simple function
may not be commonly used as loss function in practice. Nonetheless, we give the utility
guarantee for the case of ν = 0 in Theorem 2.
Theorem 2 (Utility guarantee, V = 0.). Suppose F is L-Lipschitz and β-smooth. Set η = 1,
T = n√p^ and σt = Θ (LPTlog(1∕δ)∕nc) . Let X = T PT=I Xi+1, We have
E[F (X) - F (x*)] = O ( √L ::(1/6 ).
We use parameter averaging to reduce the influence of perturbation noise because gradient
update does not have strong contraction effect when ν = 0.
3.3 Utiltiy Guarantee of DP-SGD Based on Expected Curvature
Stochastic gradient descent has become one of the most popular optimization methods
because of the cheap one-iteration cost. In this section we show that expected curvature can
also improve the utility analysis for DP-SGD (Algorithm 2). We note that Vf (x) represents
an element from the subgradient set evaluated at X when the objective is not smooth. Before
stating our theorem, we introduce the moments accountant technique (Lemma 1) that is
essential to establish privacy guarantee.
Lemma 1 (Abadi et al. (2016)). There exist constants c1 and c2 so that given running steps
T, for any < c1T/n2, Algorithm 2 is (, δ)-differentially private for any δ > 0 if we choose
σ ≥ C2 √T1;g≡.
n
Algorithm 2: Differentially Private Stochastic Gradient Descent (DP-SGD)
Input : Dataset D = {d1, . . . , dn }. Individual loss function: fi (X) = f (X; di ) with
Lipschitz constant L. Number of iterations: T. Learning rate: ηt .
1	for t = 1 to T do
2	Sample it from [n] uniformly.
3	Compute gt = Vfit (Xt).
4	Update parameter Xt+ι = Xt — η (gt + zt), where Zt 〜 N 0, L2σ2Ip .
5	end
For the case of ν > 0, Theorem 3 presents the utility guarantee of DP-SGD.
Theorem 3	(Utility guarantee, ν > 0.). Suppose F is L-Lipschitz with ν expected curvature.
Choose σ based on Lemma 1 to guarantee (e, δ)-DP. Set η = Vt and T = n2e2, We have
E[F (XT) - F(*)] = O (PmognnT (1M)).
Remark 2. Theorem 3 does not require smooth assumption.
Theorem 3 shows the utility guarantee of DP-SGD also depends on V rather than μ. We set
T = Θ(n2) following Bassily et al. (2014). We note that T = Θ(n2) is necessary even for
non-private SGD to reach 1/n2 precision. We next show for a relatively coarse precision, the
running time can be reduced significantly.
Theorem 4.	Suppose F is L-Lipschitz With V expected curvature. Choose σ based on
Lemma 1 to guarantee (e, δ)-DP. Set η = Vt and T = √p. Suppose p < n2, We have
E[F (XT) - F (x*)] = O (√pL2 log(n)).
nV
6
Under review as a conference paper at ICLR 2020
We note that the analysis of Bassily et al. (2014) yields E[F (XT) - F (x*)] = O ( √pLn* (n))
if setting T = √p, which still depends on the minimum curvature. Theorem 5 shows the
utility for the case of ν = 0.
Theorem 5 (Utility guarantee, ν = 0.). Suppose F is L-Lipschitz. Assume kxtk ≤ D
for t ∈ [T]. Choose σ based on Lemma 1 to guarantee (, δ)-DP. Let G = L 1 + pσ2, set
ηt = GD√t and T = n2 心,We have
E[F (XT) - F (x*)] = O ( 'plog(10Llog(n)
n
This utility guarantee can be derived from Theorem 2 in (Shamir & Zhang, 2013).
3.4 Discussion on three perturbation approaches.
In this section, we briefly discuss two other perturbation approaches and compare them to
the gradient perturbation approach.
Output perturbation (Wu et al., 2017; Zhang et al., 2017) perturbs the learning algorithm
after training. It adds noise to the resulting model of non-private learning process. The
magnitude of perturbation noise is propositional to the maximum influence one record can
cause on the learned model. Take the gradient descent algorithm as an example. At each
step, the gradient of different records would diverge the two sets of parameters generated by
neighboring datasets, the maximum distance expansion is related to the Lipschitz coefficient.
At the same time, the gradient of the same records in two datasets would shrink the parameter
distance because of the contraction effect of the gradient update. The contraction effect
depends on the smooth and strongly convex coefficient. Smaller strongly convex coefficient
leads to weaker contraction. The sensitivity of output perturbation algorithm is the upper
bound on the largest possible final distance between two sets of parameters.
Objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012; Iyengar et al., 2019) perturbs
the objective function before training. It requires the objective function to be strongly convex
to guarantee the uniqueness of the solution. It first adds L2 regularization to obtain strong
convexity if the original objective is not strongly convex. Then it perturbs the objective with
a random linear term. The sensitivity of objective perturbation is the maximum change of
the minimizer that one record can produce. Chaudhuri et al. (2011) and Kifer et al. (2012)
use the largest and the smallest eigenvalue (i.e. the smooth and strongly convex coefficient)
of the objective’s Hessian matrix to upper bound such change.
In comparison, gradient perturbation is more flexible than output/objective perturbation.
For example, to bound the sensitivity, gradient perturbation only requires Lipschitz coefficient
which can be easily obtained by using the gradient clipping technique. However, both output
and objective perturbation further need to compute the smooth coefficient, which is hard for
some common objectives such as softmax regression.
More critically, output/ob jective perturbation cannot utilize the expected curvature condition
because their training process does not contain perturbation noise. Moreover, they have
to consider the worst performance of learning algorithm. That is because DP makes the
worst case assumption on query function and output/objective perturbation treat the whole
learning algorithm as a single query to private dataset. This explains why their utility
guarantee depends on the worst curvature of the ob jective.
4 Experiment
In this section, we evaluate the performance of DP-GD and DP-SGD on multiple real world
datasets. We use the benchmark datasets provided by Iyengar et al. (2019). Objective
functions are logistic regression and softmax regression for binary and multi-class datasets,
respectively.
Datasets. The benchmark datasets includes two multi-class datasets (MNIST, Covertype)
and five binary datasets, and three of them are high dimensional (Gisette, Real-sim, RCV1).
7
Under review as a conference paper at ICLR 2020
Table 2: Algorithm validation accuracy (in %) on various kinds of real world datasets.
Privacy parameter is 0.1 for binary dataset and 1 for multi-classes datasets.
	KDDCup99	Adult	MNIST	Covertype	Gisette	Real-sim	RCV1
Non private	99.1	84.8	-^919	71.2	96.6	93.3	93.5
AMP1	97.5	79.3	71.9	64.3	62.8	73.1	64.5
Out-SGD	98.1	77.4	69.4	62.4	62.3	73.2	66.7
DP-SGD	98.7	80.4	87.5	67.7	63.0	73.8	70.4
DP-GD	98.7	80.9	88.6	66.2	67.3	76.1	74.9
Adult
β4β2β07β
C*)e-n8e
KDDCup99
999β9796
C*)e-n8e
β0604020
C*)e-n8e
MNlST
Realsim
0 5 0 5 0 5
9 8 8 7 7 6
Ce-n8e
10^-1.5	1O^-1	10^-0.5 1O^O 10^-1.5	1O^-1	10^-0.5 1O^O 10^-1.5	1O^-1	10^-0.5 1O^O 10^-1.5	1O^-1	10^-0.5 1O^O
Figure 3: Algorithm validation accuracy (in %) with varying . NP represents non-private
baseline. Detailed description about evaluated datasets can be found in Table 3.
Following Iyengar et al. (2019), we use 80% data for training and the rest for testing. Detailed
description of datasets can be found in Appendix B
Implementation details. We track Renyi differentialy privacy (RDP) (Mironov, 2017)
and convert it to (, δ)-DP. Running step T is chosen from {50, 200, 800} for both DP-GD
and DP-SGD. For DP-SGD, we use moments accountant to track the privacy loss and the
sampling ratio is set as 0.1. The standard deviation of the added noise σ is set to be the
smallest value such that the privacy budget is allowable to run desired steps. We ensure
each loss function is Lipschitz by clipping individual gradient. The method in Goodfellow
(2015) allows us to clip individual gradient efficiently. Clipping threshold is set as 1 (0.5
for high dimensional datasets because of the sparse gradient). For DP-GD, learning rate
is chosen from {0.1, 1.0, 5.0} ({0.2, 2.0, 10.0} for high dimensional datasets). The learning
rate of DP-SGD is twice as large as DP-GD and it is divided by 2 at the middle of training.
Privacy parameter δ is set as n12. The l2 regularization coefficient is set as 1 X 10-4. All
reported numbers are averaged over 20 runs.
Baseline algorithms. The baseline algorithms include state-of-the-art objective and output
perturbation algorithms. For objective perturbation, we use Approximate Minima Perturba-
tion (AMP) (Iyengar et al., 2019). For output perturbation, we use the algorithm in Wu et al.
(2017) (Output perturbation SGD). We adopt the implementation and hyperparameters
in Iyengar et al. (2019) for both algorithms. For multi-class classification tasks, Wu et al.
(2017) and Iyengar et al. (2019) divide the privacy budget evenly and train multiple binary
classifiers because their algorithms need to compute smooth coefficient before training and
therefore are not directly applicable to softmax regression.
Experiment results. The validation accuracy results for all evaluated algorithms with
= 0.1 (1.0 for multi-class datasets) are presented in Table 2. We also plot the accuracy
results with varying in Figure 3. These results confirm our theory in Section 3: gradient
perturbation achieves better performance than other perturbation methods as it leverages
the average curvature.
5 Conclusion
In this paper, we show the privacy noise actually helps optimization analysis, which can be
used to improve the utility guarantee of both DP-GD and DP-SGD. Our result theoretically
1 For multi-class datas sets MNIST and Covertype, we use the numbers reported in Iyengar et al.
(2019) directly because of the long running time of AMP especially on multi-class datasets.
8
Under review as a conference paper at ICLR 2020
justifies the empirical superiority of gradient perturbation over other methods and advance
the state of the art utility guarantee of DP-ERM algorithms. Experiments on real world
datasets corroborate our theoretical findings nicely. In the future, it is interesting to consider
how to utilize the expected curvature condition to improve the utility guarantee of other
gradient perturbation based algorithms.
9
Under review as a conference paper at ICLR 2020
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In ACM SIGSAC Conference on Computer
and Communications Security, 2016.
John M Abowd. The challenge of scientific reproducibility and privacy protection for statistical
agencies. Census Scientific Advisory Committee, 2016.
Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and Brendan
McMahan. cpsgd: Communication-efficient and differentially-private distributed sgd. In Advances
in Neural Information Processing Systems, 2018.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Differentially private empirical risk mini-
mization: Efficient algorithms and tight error bounds. Annual Symposium on Foundations of
Computer Science, 2014.
Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances in
Neural Information Processing Systems, 2009.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk
minimization. Journal of Machine Learning Research, 2011.
Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In
Advances in Neural Information Processing Systems, 2017.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, 2006b.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and TrendsR in Theoretical Computer Science, 2014.
Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable Privacy-
preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC conference on computer
and communications security, 2014.
Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification by
iteration. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),
2018.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit
confidence information and basic countermeasures. In ACM SIGSAC Conference on Computer
and Communications Security, 2015.
Ian Goodfellow. Efficient per-example gradient computations. arXiv preprint arXiv:1510.01799,
2015.
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under the gan: information
leakage from collaborative deep learning. In Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security, 2017.
Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang.
Towards practical differentially private convex optimization. In IEEE Symposium on Security
and Privacy, 2019.
Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Distributed learning without
distress: Privacy-preserving empirical risk minimization. In Advances in Neural Information
Processing Systems, 2018.
Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization
and high-dimensional regression. In Conference on Learning Theory, 2012.
Jaewoo Lee and Daniel Kifer. Concentrated differentially private gradient descent with adaptive
per-iteration privacy budget. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, 2018.
10
Under review as a conference paper at ICLR 2020
Robert McMillan. Apple tries to peek at user habits without violating privacy. The Wall Street
Journal, 2016.
Ilya Mironov. Renyi differential privacy. In IEEE 30th Computer Security Foundations Symposium
(CSF), 2017.
Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, Bin Yu, et al. A unified framework
for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science,
27(4):538-557, 2012.
Md Atiqur Rahman, Tanzila Rahman, Robert Laganiere, Noman Mohammed, and Yang Wang.
Membership inference attack against differentially private deep learning model. Transactions on
Data Privacy, 2018.
Arun Rajkumar and Shivani Agarwal. A differentially private stochastic gradient descent algorithm
for multiparty classification. In Artificial Intelligence and Statistics, 2012.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence
results and optimal averaging schemes. In International Conference on Machine Learning, 2013.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks
against machine learning models. In IEEE Symposium on Security and Privacy (SP), 2017.
Adam Smith, Abhradeep Thakurta, and Jalaj Upadhyay. Is interaction necessary for distributed
private learning? In IEEE Symposium on Security and Privacy (SP). IEEE, 2017.
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with
differentially private updates. In Global Conference on Signal and Information Processing
(GlobalSIP), 2013 IEEE, 2013.
Kunal Talwar, Abhradeep Guha Thakurta, and Li Zhang. Nearly optimal private lasso. In Advances
in Neural Information Processing Systems, 2015.
Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited:
Faster and more general. In Advances in Neural Information Processing Systems, 2017.
Lingxiao Wang and Quanquan Gu. Differentially private iterative gradient hard thresholding for
sparse learning. In 28th International Joint Conference on Artificial Intelligence, 2019.
Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. A methodology for formalizing
model-inversion attacks. In 2016 IEEE 29th Computer Security Foundations Symposium (CSF),
2016.
Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on
differential privacy for scalable stochastic gradient descent-based analytics. In ACM International
Conference on Management of Data, 2017.
Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efficient private erm for smooth ob jectives.
In International Joint Conference on Artificial Intelligence, 2017.
11
Under review as a conference paper at ICLR 2020
Appendix A Proofs Related to DP-GD and DP-SGD
Proof of Theorem 1. Let x1 , . . . , xt be the path generated by optimization procedure. Since xt
contains Gaussian perturbation noise zt-1 , Definition 3 gives us
Ezt-1 [hxt — X*, VF (xt)i] ≥ VtEzt-ι[kxt - x*k2].
Since F is β-smooth, we have
1
hxt — x*, NF(Xtyi ≥ - IIVF(Xt)II .
β
Take linear combination of above inequalities,
Ezt-IKXt - X*, VF (xt)i] ≥ θνtEzt-ι [kxt - X* k2] + (1--θyEzt-1 [kVF (xt)k2]
β	(2)
≥ θνEzt-ι[kxt - x*k2] + (1-θEzt-ι[kVF(xt)k2].
β
Let rt = IXt - X* I be the solution error at step t. We have the following inequalities between rt+1
and rt.
rt2+1 = IXt - ηVF(Xt) - ηzt - X*I2 ,	(3)
= IXt - X*I2 - 2ηhVF(Xt) + zt,Xt - X*i + η2 IVF (Xt) + ztI2 .
Take expectation with respect to zt , we have
Ezt[rt2+1] ≤ IXt - X*I2 - 2ηhVF (Xt) , Xt - X*i + η2 IVF (Xt)I2 + pη2σt2.	(4)
Further take expectation with respect to zt-1 and use Eq 2, we have
Ezt,zt-1[rt2+1] ≤ Ezt-1 [IXt - X*I2] - 2ηEzt-1 [hVF (Xt) ,Xt - X*i] + η2Ezt-1[IVF(Xt)I2] + pη2σt2,
≤ (1 - 2(1 - θ) ην) Ezt-ι [r2] + (η2 - 2βθ) Ezt-JkVF (xt)k2] + pη2σl
(5)
Set θ = 1 and η ≤ 1,
Ezt,zt-1 [rt2+1] ≤ (1 - ην) Ezt-1 [rt2] + pη2σt2.	(6)
Applying Eq (6) and taking expectation with respect to zt, zt-ι, •…,zɪ iteratively yields
t
E[rt2+1] ≤ (1 -ην)tr12 +pη2X(1 - ην)t-i σi2.	(7)
Uniform privacy budget allocation scheme sets
σ2 =Θ ("").
n2 2
Therefore	E[rT+ι]≤ (1 - ην)T r2 + Θ ()∙	⑻
Set T ≥ 2^og(n, we have
ην
(1 - ην )T -12 = exp
-ην)log(n2)) r2
≤
ην
η1ν Iog(I+ τ-ηνην)	r2
r2 < -2.
n2
exp (log(1∕n2) nV log(1 + 1-νην)) -2,
(9)
Last inequality holds because nV log(ι + ι-ην) >1 for ην ≥ V ≥ 1.
12
Under review as a conference paper at ICLR 2020
Therefore, for T ≥ 2 log(n), We have the excepted solution error E[rT+ι] satisfies
E[rT+ι]= o (PnrLVnoO⑶).	(10)
Since F (x) is β-smooth, we have
F(x) — F(x*) ≤ 2 kx — x*k2∙	(11)
Using Eq (10) and Eq (11), we have the excepted excess risk satisfies
E[F(XT +ι) — F(x*)] = O (βpnTL⅛(1∕δ))
for T ≥ 2⅛(ni. The Utility bound is minimized when T = 2⅛(ni.	□
Proof of Theorem 2. The smooth condition gives us,
F(xt+ι) ≤ F(Xt) + BF(xt), Xt+1 一 Xti + 2 I∣xt+1 一 Xtk2
=F(Xt) — nhVF(Xt), VF(Xt) + Zti + βη2 kVF(Xt) + zt∣∣2 .
Take expectation with respect to Zt and substitute η = β,
Ezt[F(xt+ι)] = F(Xt) — ɪ kVF(xt)k2 + ɪpσ2.
Subtract F(x*) on both sides and use convexity,
Ezt[F(Xt+ι) 一 F(Xt)] = F(Xt) 一 F(x*) - ɪ ∣VF(Xt)k2 + 2βpσ2
≤ hVF(Xt), Xt - X*i - 2β kVF(Xt)k2 + 2βpσ2.
(12)
(13)
(14)
Substitute VF(Xt) = β(Xt 一 Xt+1) 一 Zt ,
Ezt [F (Xt+1) 一 F(Xt)] ≤ βhXt 一 Xt+1, Xt 一 Xti 一
= βhXt 一 Xt+1, Xt 一 Xti 一
= βhXt 一 Xt+1, Xt 一 Xti 一
= βhXt 一 Xt+1, Xt 一 Xti 一
2β Ezt [ke(Xt - Xt+1) 一 zt∣∣2 ] + 2βpσ2
2 llXt - Xt+ik2 - Ezt hXt+i, Zt)
2 ∣∣Xt 一 Xt+ιk2 一 Ezt〈Xt — ηVF(Xt) — ηzt, Zti
2 ||Xt - Xt+i『+ βPσ2
2(kXt 一 X*『一 llXt+i 一 x*∣∣2) + 1 Pσ2.
(15)
Summing over t = 1, . . . , T and take expectation with respect to Z1 , . . . , ZT,
T	β	T1
∑E[F(xt+ι) 一 F(x*)] ≤ 2 kxι — Xtk2 +∑ βPσ2.	(16)
Use convexity,
E[F(X) 一 F(Xt)] ≤ 2T kxι - x*k2 + βPσ2
(17)
≤ 2T kxι - Xtk2 + Θ (L¾≡)	()
Choose T = n√⅛, we have
E[F(X) 一 F(xt)] = O (√pL log"")! .	(18)
□
13
Under review as a conference paper at ICLR 2020
Proof of Theorem 3 and 4. We start by giving a useful lemma.
Lemma 2. Choose η = Vt, the expected solution error of Xt in Algorithm 2 for any t > 1 satisfies
E[∣∣xt - x* k2] ≤
2L2 (1+ pσ2)
tν2
Proof of Lemma 2. We have
kxt+1 - x*k2 = kxt - ηtgt - ηtzt - x*k2
= kxt - x*k2 - 2ηthxt - x*,gt + zti + ηt2 kgtk2 - 2ηt2hgt,zti + ηt2 kztk2 .
Take expectation with respect to perturbation noise zt and uniform sampling, we have
Ezt,it[kxt+1 - x*k2] = Ezt,it[kxt - ηtgt - ηtzt - x*k2]
≤ Ilxt - χ*k2 - 2ηthxt - x*, VF (xt)i + η2L + pη2L2σ2.
Further take expectation to zt-1 and apply Definition 3,
Ezt,zt-ι,it[kxt+1 - x*k2] ≤ (1 - 2νtηt) Ezt-1 [∣Xt - x*k2]+ η2L (1+ pσ2)
≤ (1 - 2νηt) Ezt-1 [∣xt - x*k2] + η2L2(1 + pσ2).
(19)
(20)
(21)
Now We use induction to conduct the proof. Substitute η = tV into Eq 21, we have Lemma 2 hold
for t = 2.
Assume E[∣xt — x*k2] ≤ 2L(1+叫)holds for t > 2, then
E[kxt+ι- x*k2] ≤ (1 - 2) E[kxt- χ*k2] + L (V+2pσ)
1	2 ∖ 2L2 (1+ pσ2)	L2(1+ pσ2)
t t2)	ν2	V	ν2t2
2	3 ∖ L2 (1 + pσ2) ≤ 2L2(1 + pσ2)
t	t2) v2	-	(t +1) v2
(22)
□
It’s easy to check that Eq 20 holds for arbitrary x rather than x* . Rearrange Eq 20 and take
expectation, we have
EKxt- x, VF(Xt)i] ≤ E[kxt- xk2] - E[kxt+1 - xk2] + ηtL2 (1 + pσ2).	(23)
2ηt	2
Let k be arbitrarily chosen from {1, . . . , bT /2c}. Summing over the last k + 1 iterations and use
convexity to lower bound hxt - x, VF (xt)i by F (xt) - F (x),
T
E[F (xt) - F (x)] ≤
t=T-k
E[kxT-k-"|2] +2 XX E[kxt - xk2]
2ηT k	2 t=T k+1
E[kxT +1 - xk2]
2ηT
L2 (1+ pσ2)
+	2
T
X	ηt.
t=T k
(24)
—
Substitute η = Vt and follow the idea in Shamir & Zhang (2013) by choosing x = xτ-k, we arrive
at
T	T	L (I	2) T
X E[F (xt) - F (xτ-k)] ≤ 2 X E[kxt - xτ-k k2] + L <12ζpσ)X t.	(25)
t=T k	t=T k+1	t=T k
14
Under review as a conference paper at ICLR 2020
Now we bound E[kxt - xT-k k2] for t ≥ T - k,
E[∣∣xt - XT-k∣∣2] ≤ 2E[∣∣xt - x*∣∣2] + 2E[∣∣xτ-k - x*『]
4L2(1 + pσ2) (1	1 ∖	8L2(1 + pσ2) ( 1 )
V2	∖t + T - k) ≤	V2	T--k)
16L2(1 + pσ2)
-V2	.
(26)
Substitute Eq 26 into Eq 25,
8kL2(1 + pσ2)	L2(1 + pσ2)	1
X E[F (Xt) - F (XT-k)] ≤ ——-V	)+	' 2ν	)X t.	(27)
Let Sk = k++ι PT=τ-k E[F (xt)] be the averaged expected values of the last k + 1 iterations. We
are interested in So — F (x*) = E[F (XT)] — F (x*). Now We derive an inequality between Sk and
Sk-1. By definition,
kSk-1 = (k + 1) Sk - E[XT -k].
(28)
Rearrange Eq 27 to upper bound -E[XT -k],
Sk-1
k + 1	E[XT -k]
-JTSk.....-
≤ k + 1 S _ Sk + 8L2 (1 + Pσ2) + L2 (1+ P/2) X 1
— k k k (k +1) TV 2k (k +1) ν	t
t=T -k
(29)
S	L2(1+ pσ2) ( 16	1 X 1
≤ Sk +	2V —∖kτ + k (k + 1) ʌ i
t=T -k
Summing over k = 1, . . . , k = b- /2c,
S0 ≤ SbT /2c +
L2 (1+ pσ2)
2v
T/2c	bT/2c T
X ⅛+ XX
k=1	k=1 t=T -k
1
k (k + 1) t
(30)
Now we bound S∖τ∕2c — F(x*). Choose X = x* and η = tV in Eq 24 ,
X E[F (Xt) - F (X*)] = VdT/23丁"-明巾 + 2 X	E[kXt-X*k2]
t=dT /2e	t=dT /2e+1
L2 (1+ pσ2) X
+	2	上 ηt
t=dT/2e
≤ 乎中σ2⅛+	X ɪ + X 2t)
t=dT /2e+1	t=dT /2e
(31)
L2 (1+ pσ2)
V
≤
3T1
(1 + 2 E t)
t=dT/2e
≤
4L2(1+ pσ2)
V
The second inequality uses Lemma 2. The last inequality holds because the fact that PT=「丁/2] t ≤
log(2). Dividing Eq 31 by dT /2e,
S∖τ∕2C - F(x*) ≤ 8L2(T+pσ2).	(32)
15
Under review as a conference paper at ICLR 2020
We have PbT/2c 16 ≤ 16(1+lOg(T)) because it is harmonic sequence. Lastly,
k=1 kT	T
bT/2c X k=1	T1 X -1- ≤ t=T-k k (k + 1) L	bT/2c X k=1	log(2) k(k +1)	(33)
	≤	bT/2c X k=1	竿 ≤ 2log(2). k	
Plugging these bounds into Eq 30, we have
So - F(x*) = O ((1+ "TV2lθg(T)) .	(34)
Choose σ2 = Θ (TInf；/δ ) to guarantee (gδ)-DP. Set T = n2e2, We have
So - F(x*) = O (PL **o O/" ) .	(35)
Set T = -np and assume p < n2, we have
So- F (x*)= O √ √pL2 log(n)! .	(36)
nν
□
Appendix B	Detailed description on benchmark datasets
Table 3: Detailed description of seven real world datasets.
dataset	Adult	KDDCup99	MNIST	Covertype	Gisette	Real-sim	RCV1
# records	45220	70000	65000	581012	6000	72309	50000
# features	104	114	784	54	5000	20958	47236
# classes	2	2	10	7	2	2	2
Appendix C	Comparison between Average and Minimum
Curvatures on Different dataset
In this section we plot the average and minimum curvatures in Figure 4 for another dataset
KDDCup99. The ob jective function is still regularized logistic regression.
As shown in Figure 4, the average curvature is still larger than the minimum curvature (especially
when the regularization term is small). Despite this, the average curvature of KDDCup99 is smaller
than Adult, this may be the reason why the improvement in Section 4 is larger for the Adult dataset.
16
Under review as a conference paper at ICLR 2020
2 3 4
- - -
Ooo
111
OJnaeAJnU
10^6
O	20	40	60
running steps
80
Figure 4: Curvatures of regularized logistic regression on KDDCup99 dataset over training.
Dot symbol represents average curvature and cross symbol represents minimum curvature.
17