Under review as a conference paper at ICLR 2020
On the expected running time of nonconvex
OPTIMIZATION WITH EARLY STOPPING
Anonymous authors
Paper under double-blind review
Ab stract
This work examines the convergence of stochastic gradient algorithms that use
early stopping based on a validation function, wherein optimization ends when
the magnitude of a validation function gradient drops below a threshold. We
derive conditions that guarantee this stopping rule is well-defined and analyze the
expected number of iterations and gradient evaluations needed to meet this criteria.
The guarantee accounts for the distance between the training and validation sets,
measured with the Wasserstein distance. We develop the approach for stochastic
gradient descent (SGD), allowing for biased update directions subject to a Lyapunov
condition. We apply the approach to obtain new bounds on the expected running
time of several algorithms, including Decentralized SGD (DSGD), a variant of
decentralized SGD, known as Stacked SGD, and the stochastic variance reduced
gradient (SVRG) algorithm. Finally, we consider the generalization properties of
the iterate returned by early stopping.
1	Introduction
This work considers the minimization of a differentiable and possible nonconvex objective function:
min f (x).
x∈Rd
(1)
A generally accepted success criteria for algorithms that use only first-order information is that an
approximate stationary point is generated. These are points x ∈ Rd at which the function f has a
small gradient. In a typical machine learning scenario, f is the average loss over a dataset of training
examples, and the method of choice involves using some form of stochastic gradient method, for
instance, stochastic gradient descent, or SGD (see Algorithm 1). The success of SGD in machine
learning problems has led to many extensions of the algorithm, including variance-reduced and
distributed variants (reviewed in Section 1.1).
A common approach to stopping optimization in practice is to use early stopping based on a validation
function. In this scenario, a stopping criterion is periodically evaluated on the validation function,
and the algorithm stops once this criterion is met. The validation and training sets often are disjoint
(although this is not required in the present work). Although this approach is used frequently, there
is little theoretical work on the runtime of nonconvex optimization using early stopping based on
a validation function. In general, the runtime and performance will depend on several factors,
including the relation between the validation and training functions, and the desired level of solution
accuracy. In this work, the stopping criterion is that the algorithm has generated a point, which is
an approximate stationary point for the validation function. Our analysis focuses on bounding the
number of iterations and gradient evaluations used until the algorithm meets the stopping criteria.
Formally, we consider the stopping time defined as the first time an iterate has the property of being
an approximate stationary point for the validation function, and we derive upper bounds on the
expected value of this stopping time. Using bounds on the Wasserstein distance between the training
and validation sets, we also may derive a bound on the stationary gap of the training function at the
resulting iterate. As an extension, we also describe how Wasserstein concentration bounds can be
used to bound the stationarity gap with respect to the testing distribution to which both the training
and validation datasets are drawn.
We apply our analysis to several settings, including stochastic gradient descent (SGD), stochastic
variance reduced gradient (SVRG), decentralized SGD (DSGD) and stacked SGD (SSGD). The
1
Under review as a conference paper at ICLR 2020
result is new bounds on the expected number of Incremental First-order Oracle (IFO) calls needed to
generate approximate stationary points for some known algorithms (SGD; DSGD; SVRG), as well as
a new algorithm (SSGD).
Main contributions Our main contributions include:
-	We present a non-aSymPtotic analysis of SGD with early stopping that leads to a bound on the
expected number of gradient evaluations needed to find approximate stationary points of the
training function (Corollary 3.5). The analysis allows for biases in the update direction, subject
to a Lyapunov-type inequality on the error terms.
-	We rigorously analyze the expected running time of two distributed SGD algorithms: Decen-
tralized SGD, and Stacked SGD (Algorithm 2), a new decentralized form of SGD designed to
exploit connectivity patterns consisting of a network of parameter-server type clusters (Corollary
4.4.)
-	We apply the analysis to nonconvex SVRG to obtain a bound on the expected number of IFO
calls for this algorithm (Corollary 6.2).
-	We demonstrate how Wasserstein concentration bounds can be leveraged to bound the general-
ization performance of parameters returned by SGD with early stopping (Corollary 7.2).
1.1 Related work
The study of stochastic optimization goes back (at least) to the pioneering efforts of Robbins
and Monro Robbins & Monro (1951). Subsequent developments include the ordinary differential
equation (ODE) method Ljung (1977) and stochastic approximation Kushner & Clark (1978), which
emphasizes the asymptotic behavior of the algorithms.
The subject of non-asymptotic performance guarantees has attracted interest as well, including lower
bounds on algorithm performance Nemirovski & Yudin (1983). For a review of non-asymptotic
guarantees for convex SGD, the reader may consult Nemirovski et al. (2009); Rakhlin et al. (2012);
Bach & Moulines (2013). Extensions such as distributed Zinkevich et al. (2010) and asynchronous
Zhang et al. (2015); Agarwal & Duchi (2011) variants of the algorithms also have been investigated.
Many machine learning optimization problems involve an objective represented as a finite sum of
functions, and, for this case, variance reduction techniques lead to improved rates of convergence
over SGD Johnson & Zhang (2013); L. Roux et al. (2012); Defazio et al. (2014).
The randomized stochastic gradient (RSG) method Ghadimi & Lan (2013) uses randomization to
obtain a non-asymptotic performance guarantee for SGD. The randomization technique has became a
standard tool for analyzing optimization algorithms in the nonconvex setting Ghadimi & Lan (2016);
Lian et al. (2017); Allen-Zhu (2018a;b); Reddi et al. (2016b;a); Zhang et al. (2016); Reddi et al.
(2016b); Lei et al. (2017); Lian et al. (2015). Follow-up works have included analysis of nonconvex
optimization in more sophisticated algorithmic settings, such as asynchronous Lian et al. (2015) and
decentralized Lian et al. (2017) optimization. Analysis of variance reduced optimization has extended
beyond convex functions beginning with an application to principal components analysis Shamir
(2015) and later to general nonconvex functions Allen-Zhu & Hazan (2016); Reddi et al. (2016b;a);
Lei et al. (2017). A highlight in this area is that the IFO complexity of SVRG for nonconvex functions
is superior to that of RSG Allen-Zhu & Hazan (2016); Reddi et al. (2016a). Algorithms with better
convergence rates than SVRG also have been developed Allen-Zhu (2018a;b).
We are particularly interested in results for biased SGD, e.g., Bertsekas & Tsitsiklis (2000) which
considers the asymptotic convergence of biased SGD to stationary points. In this work, we are
interested in a similar scenario but instead focus on the non-asymptotic behavior of the algorithm.
Several recent works also have explored the average amount of resources needed to reach a desired
performance level in optimization. The expected running time of a stochastic trust region algorithm
(STORM) is given in Blanchet et al. (2016). A similar methodology also has been used to analyze
stochastic line search methods Paquette & Scheinberg (2018). Our convergence analysis is similar
in spirit to these works as we also are interested in the expected amount of time or other resources
required to meet the performance guarantee. However, the algorithms and assumptions in this work
differ.
2
Under review as a conference paper at ICLR 2020
Other recent work has analyzed the theoretical aspects of early stopping. For instance, Duvenaud
et al. (2016) developed an interpretation of early stopping in terms of variational Bayesian inference.
Early stopping for a least squares problem in a reproducing kernel Hilbert space has been treated in
Lin & Rosasco (2016). The implications of early stopping generalization were studied in Hardt et al.
(2016). However, to our knowledge, this work is the first to analyze the algorithms’ runtimes when
using a validation function for early stopping in nonconvex optimization.
2	Preliminaries
Let f : Rq × Rd → R be a loss function whose value we denote by f (y, x). Intuitively, the variable y
represents an input/output pair, and x represents the model parameters. Throughout, we shall assume
the gradient of the objective function with respect to x is Lipschitz continuous (defined as follows).
Assumption 2.1. The function f : Rq X Rd → R is bounded from below by f * ∈ R ,and the
derivative of f with respect to x is L-Lipschitz continuous:
∀y ∈ Rq,χ1,χ2 ∈ Rd,	∣∣Vχf(y,χι) - Vxf(y,χ2)k ≤ L∣∣χι 一 χ2∣∣∙
At times, we will make a distinction between the training function fT , which is used to calculate
gradients, and a validation function fV used to decide when to stop training.
Assumption 2.2. The training function fT is defined using a set YT ⊆ Rq of nT elements as
fτ (x) = n^ Ey∈γτ f (y, x), and the validation function fv is defined using a set YV ⊆ Rq of nv
elements as fv (x) = nV Py∈γv f (y, x).
To guarantee that the early stopping rule leads to a well-defined algorithm, we will assume a relation
between the training and validation functions. Intuitively, the functions fT and fv will be similar
when the datasets YT and Yv are similar. Formally, the datasets YT and Yv determine probability
measures μτ and μγ, defined as μτ = n1τ Py∈YT δy and μv = nV Py∈YV δy, respectively, where
δy is the delta measure δy(A) = 1y∈A for all sets A. We can compare these measures using the
Wasserstein distance as in:
For q ≥ 1, p ≥ 1, we denote by Pp(Rq) the probability measures on Rq with finite moments of order
p. Recall that a coupling of probability measures μι and μ2 is a probability measure Y on Rq × Rq
such that for all measurable sets A, Y(A × Rq) = μι(A) and Y(Rq × A) = μ2(A). Intuitively, a
coupling transforms data distributed like μι into a dataset that is distributed according to μ2. The
p-Wasserstein distance on Pp(Rq), denoted by dp, is defined as:
dp(μ1,μ2) =	inf ( E [∣∣xι — x2∣∣p ])	,	(2)
γ∈r(μ1 ,μ2) ∖(x1,X2)〜Y	)
where Γ(μ1,μ2) is the set of all couplings of μι and μ2. For more details the reader is referred to
Villani (2008).
Assumption 2.3. There is a constant G ≥ 0 such that
∀x ∈ Rd,	∣Vfv(X)-VfT(x)k ≤ Gdι(μv,μτ).
There are several cases in which this assumption will be satisfied. It is trivially satisfied if the training
and validation sets are the same as μτ = μv. As a consequence of the Kantorovich duality formula
(Villani (2008), Remark 6.5), it is also satisfied if the function y 7→ Vxf(y, x) is a G-Lipschitz
function, uniformly for all x. Consider the following example:
Example 2.4. Suppose that g : Rq × Rd → R is a smooth function. Let h : Rd → R be
the function that applies the hyperbolic tangent function to each of its components: h(x) =
(tanh(x1), . . . , tanh(xd)), and define f(y, x) = g(y, h(x)). Further suppose that the training data
are bounded: ∣y∣ ≤ J for all y ∈ YT. Then Assumption 2.3 is satisfied because y 7→ Vxf(x, y) is a
∂2g
LiPSChitZ function With G = SUpkyk三川⑶匕3 k ∂X∂y g x)k∙
In our analyses the notion of success is that an algorithm generates an approximate stationary point:
Definition 2.5. A point x ∈ Rd is an -approximate stationary point of f if ∣Vf (x)∣2 ≤ .
3
Under review as a conference paper at ICLR 2020
We measure the complexity of algorithms according to how many function value and gradient queries
they make. Formally, an IFO) is defined as follows (Agarwal & Bottou, 2015):
Definition 2.6. An IFO takes a parameter X and an input y and returns the pair (f (y, x), Vχf (y, x)).
In Appendix A.1, we briefly recall the notion of filtration, stopping times, and other concepts from
stochastic processes that will be used in our analyses. We also refer readers to Williams (1991) for
more details.
3 Biased SGD
This section details our analysis of SGD with early stop-
ping shown on the right in Algorithm 1. Starting from an
initial point x1 , at each iteration, the parameter is updated
with an approximate gradient hn using a step-size η. The
gradient norm of the validation function is evaluated ev-
ery m iterations, and the algorithm ends when the norm
decreases below a threshold .
We assume that the update direction ht is a sum of two
components, vt and ∆t , that represent an unbiased gradi-
ent estimate and an error term, respectively:
ht = vt + ∆t .	(3)
Algorithm 1 SGD with early stopping
1
2
3
4
5
6
7
8
9
input: Initial point x1 ∈ Rd
t=1
while kVfV (xt)k2 > do
for n = t to t + m - 1 do
xn+1 = xn - ηhn
end
t=t+m
end
return xt
Let {Ft}t≥0 be a filtration such that x1 is F0-measurable, and for all t > 1, the variables vt, ∆t are
Ft-measurable. Our assumptions on the vt are as follows.
Assumption 3.1. For any t ≥ 1, it holds that
E[vt-VfT(xt) | Ft-1] =0,	(4)
E hkvt - VfT (xt)k2 | Ft-1i ≤ σv2.	(5)
Assumption 3.1 states that the update directions vt are valid approximations to the gradient VfT(xt),
and it also bounds the error in the approximation. For the variables ∆t we assume the following:
Assumption 3.2. There is a sequence of random variables V1, V2, . . ., and U1, U2, . . . such that for
all t ≥ 1 the variables Vt and Ut are Ft-measurable, k∆t k2 ≤ Vt, and the Vt satisfy the following
Lyapunov-type inequality: For constants α ∈ [0, 1) and β ≥ 0,
V1 ≤ β,	(6)
∀t ≥ 2,	Vt ≤ αVt-1 + Ut-1,	(7)
∀t ≥ 1, E [Ut | Ft-1] ≤ β.	(8)
Assumption 3.2 models a scenario where the gradient error dynamics is a combination of contracting
and expanding behaviors. Contraction shrinks the error and is represented by a factor α. External
noise, represented by the Ut terms, prevents the error from vanishing completely. Note that the
assumption would be satisfied in the unbiased case by simply setting Vt = 0.
We can now state our result on the expected number of iterations for SGD with early stopping:
Proposition 3.3. Let {xt}t≥1 be as in Algorithm 1. Let Assumptions 2.1, 2.2, 2.3, 3.1, and 3.2 hold.
For > 0, letτ() be the stopping time τ() = inf {n ≥ 1 | n ≡ 1 (mod m) and kVfV (xn)k2 ≤ }.
Suppose that η ≤ L and E — 2Lmησ2 一 2mβ∕(1 — a) 一 G2d1(μv, μτ)2 > 0. Then
E[()] ≤ ηG2dι(μv,μτ)2 + 2(fT(XI) — f*) + 3∕m + 2⅞β∕(I — a)
η e 一 η6∕m — 2Lη2σ2 — 2ηβ∕(1 — a) — ηG2dι(μv, μτ)2∕m .
Furthermore, it holds with probability 1 that ∣∣Vfτ (XT (e))∣∣2 ≤ 2e + 2G2d1(μv, μτ )2.
(9)
This result can be strengthened by assuming a coupling between the step-size and the expansion
bound β, then the result can be strengthened as demonstrated in the next corollary.
4
Under review as a conference paper at ICLR 2020
Corollary 3.4. Let Assumptions 2.1, 2.2, 2.3, 3.1, and 3.2 hold. In the context of Proposition 3.3, let
the constant β be of theform β = ηR for some R ≥ 0, and suppose that e > G2dι(μv, μτ)2. Let
c ∈ (0, 1) and let the step-size be
1 e - G2dι(μv, μτ)2	∖
η = C ∙ mint L,m(2Lσv + 2R∕(1 - α)) ʃ .
(10)
Then
E[τ(e)]
m (1 + R∕(1 — a))
(1 — c) c (e — G2dι(μv, μτ)2)2
(11)
See the appendix for the complete formula, including lower order terms.
From this corollary we see that when β is proportional to the step-size η, the complexity bound is of
the order O(1∕e2). As we shall see below, this coupling assumption is satisfied for Stacked SGD.
For the case of using SGD to minimize a finite sum using unbiased, we obtain the following:
Corollary 3.5. Let Assumptions 2.1, 2.2, and 2.3 hold. Suppose each gradient estimate is obtained
by selecting a data point yt ∈ YT uniformly at random and setting Vt = Vχf (yt, Xt) and that there
is a σV ≥ 0 such that∀y0 ∈ YT, x ∈ Rd, ∣∣Vχf (y0,x) — nT Py∈γτ Vχf (y,x)∣∣ ≤ σ2. Ifthe
step-sizes are defined according to Equation equation 20 using c = 1∕2 then the expected number of
IFO calls used by SGD before reaching an e-approximate stationary point is
E [IFO (e)]
2
mnV + m2
(e — G2dι (μv ,μτ )2)2
+ nV
See the appendix for the complete formula, including lower-order terms.
Note that when d(μv, μτ) is on the order of √e, this result states that the expected IFO complexity is
O(1∕(e2)). This can be compared with the RSG algorithm, where O(1∕(e2)) iterations are sufficient
for the expected (squared) norm of the gradient at a random iterate to be at most e (Cor. 2.2 in
Ghadimi & Lan (2013)).
4 Stacked SGD
In this section, we introduce SSGD for decentralized optimiza-
tion, which involves a distributed system with two types of
nodes: workers and communicators. The communication pat-
tern is shown on the right for a hypothetical system consisting
of 16 workers and 4 communicators. Workers (circles) are
grouped into clusters, each containing a communicator (trian-
gles).
Algorithm 2 depicts the steps of SSGD. For every m epochs,
the gradient of the validation function is computed at the mean
of the communication node parameters. The algorithm stops
when the norm of this gradient drops below a threshold. This step is naturally carried out by one of
the communication nodes because they can store the average computed during Line 9 of the algorithm.
For workers, each iteration begins on Line 6 with one step of SGD. Then, on Line 7, an averaging
step is performed to partially synchronize the model parameters within the local cluster. The steps for
a communication node begin in Line 9 with an averaging step, among the other communication nodes
from each cluster. On Line 10, there is a partial synchronization with the worker nodes in the cluster.
The key to the algorithm’s efficiency is that the averaging among communication nodes can happen
in parallel with the gradient descent step as it occurs within the cluster. In the naive approach to
parallelizing SGD, all nodes must block after each iteration for synchronization of their parameters.
We assume there are M ≥ 1 clusters, each containing K ≥ 1 computation nodes and 1 communi-
cation node. Thus, there are M(K + 1) total nodes. Given a worker node 1 ≤ i ≤ KM, we let
5
Under review as a conference paper at ICLR 2020
Algorithm 2 SSGD with early stopping
1: input: Node id i, initial parameters
2
3
4
5
6
xi1 (for workers), or xbi1 (for communicators)
t=1
while Il Vfv
do
for n = t to t + m - 1 do
if node i is a computation node then
10
11
12
13
14
15
else (node i is a communication node)
X 1 =L PM _ Xj
xn+ 1	M 2=1 = 1 Xn
bn+1 = K+1 (bn+1 +Pj∈c-1(i)
end if
end
t=t+m
end
return bt+ 1
SSGD results for a climate modeling task.
Top: throughput vs number of nodes.. Bot-
tom: comparison of the loss of the algo-
rithms.
7
8
9
x
x
c(i) ∈ {1, . . . , M} denote the index of the communication node of the group containing worker i.
For analysis, we define the filtration {Ft}t≥0 as follows:
Fo = σ({x1∣1 ≤ i ≤ KM} ∪ {x1∖l ≤ i ≤ M}),
∀t ≥ 1, Ft = σ({xi,vn∣1 ≤ n ≤ t, 1 ≤ i ≤ KM} ∪ 隹 ∣ 1 ≤ i ≤ M}).
We assume that the gradient estimates used in SSGD are unbiased and have bounded variance:
Assumption 4.1. For any t ≥ 1 and 1 ≤ i ≤ KM,
E vti - VfT (xit) | Ft-1	=0,	(12)
E hIIvti - VfT (xit)II2 | Ft-1 i ≤ σv2.	(13)
The first step in our analysis is a bound on the dispersion of the parameters across the system.
Proposition 4.2. Let Assumption 2.1, 2.2, and 4.1 hold, and let the variables xbit be as defined in Line 6
of Algorithm 2. Suppose the step-size satisfies η < 1/(2LK). Define the variables V1 , U1 , V2, U2, . . .
and the constants α, β as follows:
Ut
(KηK5∕8) M X XX K KX Vk-VfT(Xt))-( KX Vk-VfT(Xj))∣
i=1 j=1	k∈c-1(i)	k∈c-1(j)
(K + 3/4)2
(K + 1)2
β = η ∙
4LσV
(K + 5/8)
(14a)
(14b)
(14c)
(14d)
Then, for all t ≥ 1, it holds that Vt+1	≤ αVt + Ut and E[Ut | Ft-1 ] ≤ β.
6
Under review as a conference paper at ICLR 2020
In the preceding definitions, Vt represents the dispersion of the parameter values across different
nodes. An averaging step tends to reduce the dispersion by a factor of α, while the independent
gradient updates at each node may increase parameter dispersion by an amount β . This result allows
us to model SSGD as a form of biased SGD, leading to the following:
Proposition 4.3. Let Assumptions 2.1, 2.2, 2.3, and 4.1 hold. Assume that the initial parameters at
every node are equal: xi1 = xj1, for all 1 ≤ i, j ≤ KM and xbi1 = xbj1 for 1 ≤ i,j ≤ M. For some
C < 2K，suppose that the step-size η is
1	E — G2dι(μv, μτ)2
L, m(2LσV∕K + 32LσV(K + 1)/(K + 5/8))
Let Xt be the average of the communicator states at time t: bt = 吉 PM=I bt∙ For e > 0, define T (E)
to be the stopping time T(E) = inf{n ≥ 1 | n ≡ 1 (mod m) and ∣∣Vfv(bn)k2 ≤ e}. Then,
2
m2
(1 一 c)c(e — G2dι(μv, μτ)2)2
Refer to the Appendix for the complete formula, including lower-order terms.
(15)
η = c ∙ min
E[T(E)] = O
This leads to a bound on the expected IFO complexity to minimize a finite sum using SSGD:
Corollary 4.4. Let Assumptions 2.1, 2.2, and 2.3 hold. Suppose each gradient estimate is obtained
by selecting a data point ytj ∈ YT uniformly at random and setting vtj = Vxf(ytj, xtj) and that there
is a σ2 ≥ 0, such that∀y0 ∈ YT, x ∈ Rd,	∣∣Vχf (y',x) - nT Py∈YT Vχf (y,x)∣∣ ≤ σ^. Ifthe
step-sizes are defined as in Equation equation 15 with c = 1/(4K), then the expected number of IFO
calls used by SSGD before reaching an E-approximate stationary point is
E [IFO(E)] = O
mK (nV + mK )
(E- G2dι(μv,μτ)2)2
+ nV
Experimental Result SSGD has been implemented to train a neural network model as part of
research into spatio-temporal data analysis for climate research. The model is LSTNet, a neural
network architecture that includes a convolutional neural network to extract short-term local depen-
dency patterns from spatial variables and a recurrent neural network component to discover long-term
patterns from time series trends Lai et al. (2018). LSTNet is trained for the prediction of solar
radiation from past sensor measurements. Fig. 4 shows results from the experiment. The upper
plot compares the throughput of SGD and SSGD as the number of nodes increases. We see that the
performance of SGD degrades after 64 nodes, while SSGD maintains near-linear scalability. The
lower plot shows that although SSGD involves biased gradients, it does not sacrifice accuracy, and
yields similar prediction errors compared to SGD. See Appendix G.1 for more details about the
experimental methodology.
5 Decentralized SGD
In this section we show how our methodology can be applied to Decentralized SGD (DSGD) which
is another variant of distributed SGD. Recently, DSGD was analyzed using randomization Lian et al.
(2017). In this section we complement that analysis by studying the expected running time of the
algorithm.
The steps of DSGD are shown in Algorithm 3. The procedure involves M > 0 worker nodes that
participate in the optimization. A communication matrix a describes the connectivity among the
workers; ai,j > 0 means that workers i and j will communicate after each gradient descent step. At
each step of optimization, every node computes a weighted average of the parameters in its local
neighborhood, as determined by the connectivity matrix. This is combined with a local gradient
approximation to obtain the new parameter at the worker. The data that is returned by the algorithm
(assuming the termination criteria is met) is the average of the parameters throughout the system,
denoted xt:
1 M
xt = ME"	(16)
i=1
7
Under review as a conference paper at ICLR 2020
Every m epochs, the norm of gradient of the validation function is evaluated at the average parameter
and the algorithm terminates when this norm falls below a threshold.
The intuitive justification for DSGD is that it may be more
efficient compared to naive approaches to parallelizing
SGD, since whenever ai,j = 0 then the nodes i andj need
not communicate. In Lian et al. (2017) those authors offer
theoretical support for the superiority of DSGD. In the
present work, our goal is to analyze the expected running
time of DSGD as an example of our the abstract theory
developed above may be applied in practice. We leave
comparisons of the algorithms for future work.
For the analysis, we define the filtration {Ft}t≥0 as fol-
lows:
Fo = σ({x1 I 1 ≤ i ≤ M}),
∀t ≥ 1, Ft = σ( {x1,vn ∣ 1 ≤ n ≤ t, 1 ≤ i ≤ M})
Algorithm 3 DSGD with early stopping
1: input: Initial point x1 ∈ Rd , initial
parameters xi1 .
2:	t = 1
3:	while ∣∣Vfv(Xt)II2 > e do
4:	for n = t to t + m - 1 do
M
5:	xin+1 =	ai,jxjn - ηvni
j=1
6:	end
7:	t = t + m
8:	end
9:	return Xt
The connectivity matrix a is subject to the same conditions as in Lian et al. (2017):
Assumption 5.1. The M × M connectivity matrix a is symmetric and stochastic. The spectral gap,
denoted by P and defined as P = (max{∣λ2(a)∣, |1m(a)∣})2 is assumed to satisfy ρ < 1.
To make the proofs clear and concise, we make the assumption the parameters at each node are single
real-numbers. That is, throughout this section we assume d = 1 in Assumption ??.
We also assume that the gradient estimates used at each worker at unbiased and have bounded
variance.
Assumption 5.2. For any t ≥ 1 and 1 ≤ i ≤ M,
E vti - VfT (Xit) | Ft-1 =0,	(17)
E |vti - VfT (Xit)|2 | Ft-1 ≤ σv2.	(18)
For the analysis, We show that the sequence of averages Xt for t = 1,2,... can be modeled as being
generated by a biased version of SGD, using the tools from Section 3. This involves showing that the
distance between local parameter values Xit and the system average can be controlled, as shown in the
following.
Proposition 5.3. Let Assumptions 2.1, 2.2, 5.1, and 5.2 hold. Suppose the step-size satisfies η ≤
(1 一 √ρ)∕(4√2L). Define the variables Vι, U1,V2,U2,... and the constants α, β asfollows:
L2 M
M Xlxt-Xtl2,
Vt
i=1
Ut = 8 η2 M1+p X Ivn-Vf(Xt)I2,
α =心(√P+1 Y
2ρ <	2	)
L	L√2 2
β = n；---σv
1-Pv
Then for all t ≥ 1 it holds that Vt+1 ≤ αVt + Ut and E[Ut I Ft-1] ≤ β.
(19a)
(19b)
(19c)
(19d)
Using this result on the dispersion of the parameters, we can move to the main result on decentralized
SGD. The result gives conditions that guarantee the expected time E[τ ()] is finite, and also bounds
this time in terms of the problem data. Notably, it shows a dependence on P, which is the mixing rate
of the connectivity matrix.
Proposition 5.4. LetAssumptions 2.1, 2.2, 5.1, 2.3 and5.2 hold. Assume that the initial parameters at
every node are equal: x∖ = x《for all 1 ≤ i,j ≤ M. Let C ≤ 1-√p and define R = L√2σV∕(1 — P)
8
Under review as a conference paper at ICLR 2020
Algorithm 4 SVRG with early stopping
1
2
3
4
5
6
input: Initial point x1m ∈ Rd
for s = 1, 2, . . . do
xs0+ = xsm
gs+1 = nT Py∈Yτ Vfτ(y,x0+1)
if kgs+1 k2 ≤ then return xs0+1
for t = 0 to m - 1 do
×105
7
8
9
10:
Sample yts uniformly at random from YT
vts =Vf(yts,xts+1)-Vf(yts,xs0+1)+gs+1
xts++11 = xts+1 - ηvts
end
IFO calls to approximate stationarity for
MNIST (top) and CIFAR-10 (bot.).
11:	end
and let α be as in Equation equation 19c. Let the step-size be
1 e — G2dι(μv, μτ)2	]
η = C ∙ mint Bm2Lσv + 2R/(1 —α)) ʃ .
(20)
For e > 0 define τ(e) to be the first time the norm of the gradient of the validation function falls
below e; That is, τ(e) = inf{η ≥ 1 | n ≡ 1 (mod m) and IlVfV(Xn)k2 ≤ e}. Then
E[τ(e)]
m (1 + R/(1 — a))
(1 — C) c (e — G2dι(μv, μτ)2)2
(21)
Note that in the above result, the order of the convergence is the same as for regular SGD and stacked
SGD. An interesting avenue for future work would be to explore whether it is possible to obtain
bounds where the step-size condition does not depend on the epoch length m.
6	SVRG
We demonstrate how the method can applied to a variant of the SVRG Johnson & Zhang (2013) with
early stopping, shown in Algorithm 4. Each epoch begins with a full gradient computation (Line 4),
and then an inner loop runs for m steps. The first step of the inner loop is to choose a random data
point (Line 7). Then, the update direction is computed (Line 8) and used obtain the next parameter
(Line 9).
Combining some existing bounds for SVRG with our stopping time approach yields the following
bound on the expected number of iterations until SVRG with early stopping terminates:
Proposition 6.1. Let Assumptions 2.1 and 2.2 hold and consider the variables xts+1 defined by
Algorithm 4. Let ξ = 1/4 and suppose that the step-size is set to η = ξ∕(Ln13) and the
epoch length is m = [nτ/(3ξ)C. For e > 0, define T(e) to be the stopping time T(e) =
inf {s ≥ 1 IkVfT(x0+1)k2 ≤ e} . Then, E[τ(e)] ≤ 1 + (40Ln^(fτ(x∖) — f *))/e.
Note that Proposition 6.1 counts the number of epochs until an approximate stationary point is
generated. A bound on the number of IFO calls can be obtained by multiplying T by the number of
IFO calls per epoch, which is nT + 2m. This immediately leads to the following result:
Corollary 6.2. Let Assumptions 2.1 and 2.2 hold and suppose the step-size η and epoch length m
are defined as in Proposition 6.1. Then, the expected number of IFO calls until SVRG returns an
approximate stationary point is E [IFO (e)] = O((n5T/3/e) + nT).
This result may be compared with Cor. 4 of Reddi et al. (2016a), which concerns an upper bound on
the IFO calls for the expected (squared) norm of the gradient at a randomly selected iterate to be less
than e. Our result concerns the expected number of IFO calls before the algorithm terminates with an
iterate that is guaranteed to be an approximate stationary point with probability 1, a stronger property.
9
Under review as a conference paper at ICLR 2020
Fig. 6 illustrates the expected IFO complexity of SVRG and SGD. The top plot shows the results
of an experiment using a simple logistic classifier on the MNIST dataset, and the bottom shows the
result of training a one-layer neural net on the CIFAR-10 dataset. The error bands represent the
standard deviation of the measurements over five independent runs. Appendix G.2 includes more
details about the experimental methodology. In each case, SVRG is better at obtaining accurate
solutions, while, for SGD, the expected IFO calls seem to become unbounded for sufficiently small .
7	Generalization Properties
Typically, the training and validation sets are made from independent samples of a test distribution μ,
and it is of interest to estimate the model performance on samples from this test distribution. Define
fμ : Rd → R as f (x) = Eμ [f (y, x)]. Inthis section, We derive a bound on E[kVfμ (x「9)∣∣2], where
the expectation is not only over the variates generated by optimization, but also over the random
choice of the datasets YV and YT . We will show how Wasserstein concentration bounds, which
concern the average distance between μ and its empirical versions, can be used for this task. Consider
the following from Dereich et al. (2013).
Theorem 7.1 (Dereich et al. (2013), Special case of Theorem 1). Let d ≥ 3 and let μ be a measure
on Rd, such that J = Eμ [∣∣y∣∣3] 1/3 < ∞. Then, there is a constant Kd, such that E[d2(μ, μv)2] ≤
KdJn-3/.
This leads to a bound on the generalization performance of the iterates returned by SGD with early
stopping, whose proof is deferred to the Appendix.
Corollary 7.2. Let the conditions ofProposition 3.3 hold. Further assume that J = Eμ [∣∣y∣∣3] 1/3 <
∞, the validation set YV is an empirical version of μ, and y → Vxf (y, x) is uniformly G-Lipschitz.
If Xτ(e) is the output ofAlgorithm 1, then E[∣∣Vfμ(xτ(e))∣∣2] ≤ 2 + 2G2κ/Jn-33d.
Note that for strongly convex optimization, it is possible to obtain rates of convergence of the test
error that are independent of the dimension Hsu & Sabato (2016). Corollary 7.2 is interesting as it
accounts for data distribution properties (via the 3rd moment J) and does not depend on the number
of iterations used in SGD. This result could be compared with Hardt et al. (2016), where the authors
proved a bound on the generalization gap for function values in terms of the number of iterations T
and samples in the training set nT. There, the bound is independent of d but increasing with T, while
our bound is independent of the number of iterations in SGD. It will be interesting to determine if
these two analyses can be combined.
8	Discussion
This work presents an analysis of several stochastic gradient methods that use early stopping based on
a validation function. We demonstrated that by blending existing analysis techniques with some basic
tools related to stopping times, it is possible to bound the expected number of iterations and gradient
evaluations to generate approximate stationary points. We also considered decentralized optimization
and introduced a new algorithm, SSGD, that proved amenable to analysis in our framework. For
SSGD, we obtained a convergence rate using our results for biased SGD, and experiments showed
that the algorithm has favorable scaling properties compared to basic parallel SGD. Our application
to SVRG demonstrated that the theoretical approach can be applied in various settings. We also
considered the generalization properties of the output of early stopping. We hope these efforts inspire
other works that investigate the theoretical and practical aspects of early stopping.
References
Agarwal, A. and Bottou, L. A lower bound for the optimization of finite sums. In Proceedings of the
32nd International Conference on International Conference on Machine Learning-Volume 37, pp.
78-86. JMLR. org, 2015.
Agarwal, A. and Duchi, J. C. Distributed delayed stochastic optimization. In Shawe-Taylor, J., Zemel,
R. S., Bartlett, P. L., Pereira, F., and Weinberger, K. Q. (eds.), Advances in Neural Information
Processing Systems 24, pp. 873-881. Curran Associates, Inc., 2011.
10
Under review as a conference paper at ICLR 2020
Allen-Zhu, Z. Natasha 2: Faster Non-Convex Optimization Than SGD. In Proceedings of the 32nd
Conference on Neural Information Processing Systems, NIPS ’18, 2018a.
Allen-Zhu, Z. How To Make the Gradients Small Stochastically. In Proceedings of the 32nd
Conference on Neural Information Processing Systems, NeurIPS ’18, 2018b.
Allen-Zhu, Z. and Hazan, E. Variance reduction for faster non-convex optimization. In Proceedings
of the 33rd International Conference on International Conference on Machine Learning - Volume
48,ICML'16,pp. 699-707.JMLR.org, 2016.
Bach, F. and Moulines, E. Non-strongly-convex smooth stochastic approximation with convergence
rate o (1/n). In Advances in neural information processing systems, pp. 773-781, 2013.
Bertsekas, D. P. and Tsitsiklis, J. N. Gradient convergence in gradient methods with errors. SIAM
Journal on Optimization, 10(3):627-642, 2000.
Blanchet, J., Cartis, C., Menickelly, M., and Scheinberg, K. Convergence Rate Analysis of a
Stochastic Trust Region Method via Submartingales. ArXiv e-prints, September 2016.
Defazio, A., Bach, F., and Lacoste-Julien, S. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Ghahramani, Z., Welling, M., Cortes,
C., Lawrence, N. D., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing
Systems 27, pp. 1646-1654. Curran Associates, Inc., 2014.
Dereich, S., Scheutzow, M., and Schottstedt, R. Constructive quantization: Approximation by
empirical measures. Ann. Inst. H. Poincar Probab. Statist., 49(4):1183-1203, 11 2013. doi:
10.1214/12-AIHP489.
Duvenaud, D., Maclaurin, D., and Adams, R. Early stopping as nonparametric variational inference.
In Gretton, A. and Robert, C. C. (eds.), Proceedings of the 19th International Conference on
Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pp.
1070-1077, 2016.
Ghadimi, S. and Lan, G. Stochastic first- and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013. doi: 10.1137/120880811.
Ghadimi, S. and Lan, G. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Mathematical Programming, 156(1-2):59-99, 2016.
Hardt, M., Recht, B., and Singer, Y. Train faster, generalize better: Stability of stochastic gradient
descent. In Proceedings of the 33rd International Conference on International Conference on
Machine Learning - Volume 48, ICML’16, pp. 1225-1234. JMLR.org, 2016.
Hsu, D. and Sabato, S. Loss minimization and parameter estimation with heavy tails. Journal of
Machine Learning Research, 17(18):1-40, 2016. URL http://jmlr.org/papers/v17/
14-273.html.
Johnson, R. and Zhang, T. Accelerating stochastic gradient descent using predictive variance
reduction. In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q.
(eds.), Advances in Neural Information Processing Systems 26, pp. 315-323. Curran Associates,
Inc., 2013.
Kushner, H. and Clark, D. Stochastic Approximation Methods for Constrained and Unconstrained
Systems. Number v. 26 in Applied Mathematical Sciences. Springer-Verlag, 1978.
L. Roux, N., Schmidt, M., and Bach, F. R. A stochastic gradient method with an exponential
convergence rate for finite training sets. In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger,
K. Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 2663-2671. Curran
Associates, Inc., 2012.
Lai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling long-and short-term temporal patterns
with deep neural networks. In The 41st International ACM SIGIR Conference on Research &
Development in Information Retrieval, pp. 95-104. ACM, 2018.
11
Under review as a conference paper at ICLR 2020
Lei, L., Ju, C., Chen, J., and Jordan, M. I. Non-convex finite-sum optimization via scsg methods. In
Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R.
(eds.), Advances in Neural Information Processing Systems 30, pp. 2348-2358. Curran Associates,
Inc., 2017.
Lian, X., Huang, Y., Li, Y., and Liu, J. Asynchronous parallel stochastic gradient for nonconvex opti-
mization. In Proceedings of the 28th International Conference on Neural Information Processing
Systems - Volume 2, NIPS’15, pp. 2737-2745, Cambridge, MA, USA, 2015. MIT Press.
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. Can decentralized algorithms
outperform centralized algorithms? a case study for decentralized parallel stochastic gradient
descent. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and
Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 5330-5340. Curran
Associates, Inc., 2017.
Lin, J. and Rosasco, L. Optimal learning for multi-pass stochastic gradient methods. In Advances in
Neural Information Processing Systems, pp. 4556-4564, 2016.
Ljung, L. Analysis of recursive stochastic algorithms. IEEE transactions on automatic control, 22
(4):551-575, 1977.
Nemirovski, A. and Yudin, D. Problem Complexity and Method Efficiency in Optimization. Wiley,
1983.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation approach to
stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009. doi: 10.1137/
070704277.
Paquette, C. and Scheinberg, K. A Stochastic Line Search Method with Convergence Rate Analysis.
arXiv e-prints, art. arXiv:1807.07994, July 2018.
Rakhlin, A., Shamir, O., and Sridharan, K. Making gradient descent optimal for strongly convex
stochastic optimization. In Proceedings of the 29th International Coference on International
Conference on Machine Learning, ICML’12, pp. 1571-1578, USA, 2012. Omnipress. ISBN
978-1-4503-1285-1.
Reddi, S., Hefny, A., Sra, S., Poczos, B., and Smola, A. Stochastic variance reduction for nonconvex
optimization. In International conference on machine learning, pp. 314-323, 2016a.
Reddi, S. J., Sra, S., Pczos, B., and Smola, A. Fast incremental method for smooth nonconvex
optimization. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 1971-1977,
Dec 2016b. doi: 10.1109/CDC.2016.7798553.
Robbins, H. and Monro, S. A stochastic approximation method. Ann. Math. Statist., 22(3):400-407,
09 1951. doi: 10.1214/aoms/1177729586.
Shamir, O. A stochastic pca and svd algorithm with an exponential convergence rate. In Proceedings
of the 32Nd International Conference on International Conference on Machine Learning - Volume
37, ICML’15, pp. 144-152. JMLR.org, 2015.
Villani, C. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.
Williams, D. Probability with Martingales. Cambridge mathematical textbooks. Cambridge Univer-
sity Press, 1991. ISBN 9780521406055.
Zhang, H., J. Reddi, S., and Sra, S. Riemannian svrg: Fast stochastic optimization on riemannian
manifolds. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.),
Advances in Neural Information Processing Systems 29, pp. 4592-4600. Curran Associates, Inc.,
2016.
Zhang, S., Choromanska, A., and LeCun, Y. Deep learning with elastic averaging sgd. In Proceedings
of the 28th International Conference on Neural Information Processing Systems - Volume 1,
NIPS’15, pp. 685-693, Cambridge, MA, USA, 2015. MIT Press.
Zinkevich, M., Weimer, M., Li, L., and Smola, A. J. Parallelized stochastic gradient descent. In
Advances in neural information processing systems, pp. 2595-2603, 2010.
12
Under review as a conference paper at ICLR 2020
Appendix: On the expected running time of nonconvex optimization
WITH EARLY STOPPING
A Preliminaries
Our analyses make use of a quadratic bound for the training function which follows from Assumption
2.1:
∀x,v ∈ Rn, fτ(x + V) ≤ fτ(x) + NfT(X)TV + 刍Wk2.	(22)
A.1 Stochastic processes
The formal setting of a stochastic optimization algorithm involves a probability space (Ω, F, P),
consisting of a sample space Ω, a σ-algebra F of subsets of Ω and a probability measure P on the
subsets of Ω that are in F. The algorithm takes an initial point xi and defines a sequence of random
variables {χt(ω)}t>ι. Intuitively Ω represents the random data used by the algorithm, such as indices
used to define mini-batches. For ease of notation we will omit the dependence of random variates in
the algorithms on ω ∈ Ω. A filtration {Ft}t=0,1,…is an increasing sequence of σ-algebras, with the
interpretation that Ft represents the information available to an algorithm up to and including time
t. A random variable X : Ω → Rd is said to be Ft measurable if it can be expressed in terms of the
state of the algorithm up and including time t. A rule for stopping an algorithm is represented as a
stopping time, which is a random variable T : Ω → {0,1,..., ∞} with the property that the decision
of whether to stop or continue at time n is made based on the information up to and including time n.
The following proposition will be used through out our analysis of the different algorithms.
Proposition A.1. Let τ be a stopping time with respect to a filtration {Ft}t=0,1,. Suppose there is
a number c < ∞ such that τ ≤ c with probability one. Let X1, X2, . . . be any sequence of random
variables such that each Xt is Ft-measurable and E[kXtk] < ∞. Then
τ
E X Xt
t=1
τ
E X E [Xt | Ft-1]
t=1
(23)
Proof. This is a consequence of the optional stopping theorem (Theorem 10.10 in Williams (1991)).
t
Define S0 = 0 and for t ≥ 1, let St = P (Xi - E[Xi | Fi-1]). Then S0, S1, . . . is a martingale with
i=1
respect to the filtration {Ft}t=0,1,..., and the optional stopping theorem implies E[Sτ] = E[S0]. But
E[So] = 0, and therefore E[S" = 0, which is equivalent to Equation equation 23.	□
B Analysis of Biased SGD
Proof of Proposition 3.3
Proof. For convenience, define the random variables δt for t = 1, 2, . . . as δt = Vt - NfT(Xt). From
equation 22, it holds that
fτ(xt+ι) ≤ fτ(χt) - nNfτ(Xt)T (VfT(Xt) + δt + ∆t) + Lη2kVfτ(Xt) + δt + ∆tk2.
13
Under review as a conference paper at ICLR 2020
Summing this over t = 1, . . . , m yields
m
fτ(Xm+1) ≤ fτ(xi) - X η7fτ(xt) (VfT(Xt) + δt + ∆t)
t=1
m
+ X v η2kVfτ (Xt) + δt + ∆tk2
t=1
mL	m
=fτ(XI)- Xη (1 - 2η) kvfτ(Xt)k2 - Xη(I- LnyVfT(Xt)T瓦	(24)
mm	m
+ XLn2kδtk2 - Xn(1-Ln)Vfτ(Xt)T∆t + XILn2k∆tk2
m
+ X Lη2 δtT ∆t .
t=1
Note that, in general, for any numbers a, b it is the case that |ab| ≤ ɪ a2 + ɪ b2. Then
∣δT∆t∣ ≤ kδtkk∆tk ≤ 1 kδtk2 + Jk∆tk2	(25)
and
IVfT(Xt)T∆t∣ ≤ kVfT(Xt)kk∆tk ≤ 1 kVfT(Xt)k2 + 1 k∆tk2.	(26)
Combining Equations equation 24, equation 25, and equation 26, we obtain
fT(Xm+1) ≤
m
f(X1) - Xη
t=1
kVfT(Xt)k2
m	NL 1
-∑n(1 -Ln)VfT(Xt)Tδt + £ ILn2 + 2Ln2) |如『
+ X (Ln2 + 2n(1 - Ln)+ Ln2 11) Idk2
mm	m
f(xi) - Xn2kVfT(Xt)k2 - Xn(1 - LnyVfT(Xt)T瓦 + XLn2kδtk2
m
+ X 5 n(1 + Ln)k∆tk2.
t=1
Rearranging terms and noting that 于1(Xm+ι) ≥ f *, this yields
m	mm
Xn2kVfT(Xt)k2 ≤ fT(xi) - f* - Xn(1 - Ln)VfT(Xt)Tδt + XLn2kδtk2
m1
+ X ”(1 + Ln)k∆t k2
t=1
mm
≤f(X1)-f* - X n(1 - Ln)VfT (Xt)T δt + X Ln2kδtk2
t=1	t=1
m
+ X nVt.
t=1
(27)
where in the second inequality we used the assumptions that n ≤ 1/L and k∆tk2 ≤ Vt. Based on
our assumption that kVfv(x) - VfT(X)Il ≤ Gdι(μv, μT), it follows that
kVfv(X)k2 ≤ 2G2dι(μv,μT)2 + 2kVfT(X)k2.	(28)
14
Under review as a conference paper at ICLR 2020
Also note that
n
X 1t≡1 (mod m) = ] 一 ] ≤-----+1∙	(29)
mm
t=1
Combining equation 28 and equation 29 results in
τ ()∧n	τ ()∧n
):1t≡1	(mod	m)kvfv (Xt)k	≤〉:	1t≡1	(mod	m)G	d1(μV,	μT)
t=1	t=1
τ ()∧n
+ X 1t≡1 (mod m)kVfT(Xt)k2	(30)
t=1
≤ G2dι(μv, μτ)2 ((τ(')八 n)+1) + X ∣∣vfτ(χt)k2.
m
t=1
For each n ≥ 1 define τ() ∧n to be the stopping time which is the minimum ofτ() and the constant
n. Applying Proposition A.1 and Assumption 4, it holds that
τ ()∧n
E X VfT(Xt)Tδt = 0	(31)
t=1
and using Proposition A.1 with Assumption 5 gives
τ ()∧n
E X ∣δt∣2 ≤ σv2E[τ() ∧n].	(32)
t=1
Next, according to conditions equation 6, and equation 7, it holds for any m ≥ 1 and with probability
one that
m	mm
X Vt ≤ α X Vt + X Ut + β	(33)
and by equation 8 together with Proposition A.1,
τ ()∧n
E X Ut	≤ E[τ () ∧ n]β.	(34)
t=1
Combining equation 33 and equation 34, then
τ ()∧n
X Vt
t=1
E
τ ()∧n
≤ αE X Vt + (E[τ( ∧ n)] + 1)β
t=1
which, upon rearranging, results in
τ ()∧n	β
E X Vt ≤ (E[τ(e) ∧ n] + 1)
t=1	1 - α
Furthermore, by definition of τ ,
τ ()∧n
E X 1t≡1 (mod m)∣VfV (Xt)∣2
t=1
(τ ()∧n)-1
≥E X	1t≡1 (mod m)∣VfV (Xt)∣2
t=1
≥ E[(τ(E) ∧ n) - 1] ʌ
Here we used that
n-1
1t≡1 (mod m)
t=1
n-1
m
n-1
≥ ---
m
(35)
(36)
15
Under review as a conference paper at ICLR 2020
Combining equation 27, equation 30, equation 31, equation 32, equation 35 and equation 36 results in
户(E[τ(E) ∧ n] - 1) ≤ηiG2dι(μv,μτ)2 (驷亚型 + 1) + fτ(xι) - f *
2m	2	m
β
+ Lη2σVE [τ (E) ∧ n] + η--(E[τ(E) ∧ n] + 1).
v	1-α
This can be rearranged into
—----2Lη2σV -	-------G2dι(μv, μτ)2) E[τ(E) ∧ n] ≤ ηG2dι(μv, μτ)2
m	v 1-α m
+ 2CMxI)-f*) + 2η占 + 条
which in turn is equivalent to
eγ /、∧ ] V ηG2dι(μv,μτ)2 + 2(fτ(xι) - f *) + η"m + 2ιβ∣Q - a)	m、
[T(E) n] ≤ η6∕m - 2Lη2σ2 - 2ηβ∕(1 - α) - nG2dι(μv, μτ)2/m .	( )
Note that the sequence of random variables {(τ (E) ∧ n)}n=1,2,... is monotone increasing, and
converges pointwise to τ (E). Then the claimed relation equation 9 follows from equation 37 by the
monotone convergence theorem.
Finally, by applying equation 28 with the roles of fv and fT reversed, and using the definition of
τ (E), it follows that
kVfτ(xτ(e))k≤ 2kVfv(xτ(e))k2 + 2G2dι(μv, μτ)2
≤ 2e + 2G2di (μv, μτ)2.
□
Proof of Corollary 3.4
Proof. According to the assumption on the step-size η (Equation equation 20),
η (E - G d1 (μv , μT ) )∕m - η(2Lσv + 2R∕(1 - α)) ≥ η(1 - c)(E - G d1 (μv , μT) )∕m
(38)
and
(39)
1 L m(2LσV + 2R∕(1 — a))
η 一 c + C (E - G2dι(μv, μτ)2).
Combining these inequalities with the conclusion of Proposition 3.3 (relation equation 9) yields
≤ ηG2dι(μv,μτ)2 + 2(fτ(xi) 一 f *) + ηc∕m + 2ηβ∕(1 — a)
一 η(E 一 G2dι(μv, μτ)2)∕m - 2Lη2σv2 一 2ηβ∕(1 一 α)	.
" ηG2dι(μv,μτ)2 + 2(fτ(xi) 一 f *) + ηc∕m + 2η2R∕(1 — a)
η(E - G2dι(μv, μτ)2)∕m - 2Lη2σV - 2η2R∕(1 - a)
≤ ηG2dι(μv,μτ)2 + 2(fτ(xi) - f *) + ηc∕m + 2η2R∕(1 - a)
一	η(1 - C)(E - G2dι(μv, μτ)2)∕m
(40)
Step A was established by Proposition 3.3. Step B uses the assumption that β = ηR. Step C is an
application of equation 38. Next, we will upper-bound the final inequality in three steps. First, using
Inequality equation 39, we see that
2(fτ(xi)- f *)	≤	2(fτ(xi)- f *)
η(1 - C)(E - G2dι(μv, μτ)2)∕m — (1 - C)(E - G2dι(μv, μτ)2)∕m)
(L	m(2LσV + 2R∕(1 — a)))
X(C + C (e - G2di(μv, μτ )2))
_	2m(fτ(xi)- f *)	L
(1 - C) C (e - G2di(μv, μτ)2)
+	2m2(fτ(xi)- f*)	OL°2 +2R∕(1 _ Q
+ (1 - C) C (e - G2di(μv,μτ)2)2 2 V +	/(	)).
(41)
16
Under review as a conference paper at ICLR 2020
Next,
2η2R∕(1 — α)	2ηR∕(1 — a)
η(1 一 C)(E — G2dι(μv,μτ)2)∕m	(1 一 C)(E — G2dι(μv, μτ)2)∕m
2cR∕(1 — α)	E — G，d∖(μv, μτ平
一(1 — C)(E — G2dι(μv, μτ)2)∕m X m(2LσV + 2R∕(1 — a))
_ c	2R∕(1 — a)
=(1 — C) × (2LσV +2R∕(1 — α))
C
≤ (1 -c).
(42)
Finally,
ηG2dι(μv, μτ)2 + ηe∕m	G2dι(μv, μτ)2 + e∕m
η(1 — C)(E — G2dι(μv ,μτ )2)∕m	(1 — C)(E — G2dι(μv ,μτ )2)∕m
mcG2dι(μv, μτ)2 + CE
(1 — C)C(E — G2dι(μv, μτ)2).
(43)
Combining equation 40 with equation 41, equation 42 and equation 43, we find that
E(M < 4m2(fT(XI)- f *) (LσV + r∕(1 — a))
E[T(E)] — 一(1 - C) C (E - G2dι(μv,μτ)2)2一
2Lm(fτ(xι) — f *) + mCG2dι(μv, μτ)2 + ce	C
+	(1 — c) c (e — G2dι(μv, μτ)2)	+ 1 — C
(44)
Proof of Corollary 3.5
Proof. If the algorithm runs until iteration τ (E), then the number of times that the full gradient of fV
is calculated is dτ (E)∕me ≤ τ (E)∕m + 1, and the number of IFO calls for the training function is
τ(E) — 1. Therefore
IFO(E) ≤ (	+ 1 ) nv + (T(e) — 1) ≤ T(e) (nV + 力 + nv∙
mm
(45)
Note that under our assumption on the gradient estimates vt , we are in the unbiased setting where
R = 0. Combining equation 44 with R = 0 and equation 45, we obtain
E[IFO(E)] ≤
4m2(fτ(xι) — f* )LσV
(1 — c) c (e — G2dι(μv, μτ)2)2
2Lm(f (xι) — f *) + mcG2dι(μv, μτ )2 + CE
+	(1 — c) c(e — G2dι(μv,μτ)2)	+
Using C = 1∕2 and neglecting lower-order terms, then,
(46)
ɪ ) X (nV + 1)+ nv.
1—C m
E[IFO(E)] = O
2
mnV + m
(e — G2dι(μv ,μτ )2)2
+ nV
□
□
C	Analysis of Stacked SGD
Proof of Proposition 4.2
Proof. Note that the variables inside the stacked algorithm satisfy the following identities: For all
1 ≤ i ≤ MK,
xbtc(i) = xit ,	(47)
17
Under review as a conference paper at ICLR 2020
and for all pairs communication nodes nodes 1 ≤ j ≤ M, and 1 ≤ k ≤ M, and t ≥ 1,
1 .
2
1
2
(48)
Let 1 ≤ i ≤ M and 1 ≤ j ≤ M be arbitrary indices of communication nodes. By the definitions in
Algorithm 2, we can see that for any t ≥ 1,
kxbit+1 - xbtj+1 k
A
B
k⅛ ( X	χk+ 2
k∈c-1(i)
k⅛ IXJ 2
	
K+1 ( X	(Xk -'冠)
k∈c-1(i)
K+1 ( X (Xk- Irv)
k∈c-1(j)
	
K+1 ( X	(Xt- nVk)
k∈c-1(i)
K^ IXJXt- nVk)
(49)
KKT (Xj
占X商+小X商
k∈c-1(i)	k∈c-1(j)
K+1
xbti - xbtj + η
k⅛	X	Vk -占 X	Vk .
k∈c-1(i)	k∈c-1(j)
Step A follows from the definition of xbit+1, xbtj+1 in Algorithm 2 and Equation equation 48. Step B
follows from the definition of xk. 1 in Algo. 2. Step C follows from Equation equation 47. Step D
t+ 2
follows from rearranging terms in the previous step, and noting that there are K workers in a cluster.
Finally, Step E is simply the triangle inequality.
For the second term on the right of the final inequality above,
k⅛	X	Vk - k⅛	X Vk
k∈c-1(i)	k∈c-1(j)
Vf (bt) - Vf (bj)
+ (KK X Vk -VfT(Xt)
k∈c-1(i)
KL
≤ KTIkXj- Xj H
KK X	vk -VfT(Xj)
k∈c-1(j)
(50)
K
+ K +1
K X	Vk-VfT(XZ)
k∈c-1(i)
K X Vk -VfT(Xj)
k∈c-1(j)
where the last inequality uses the Lipschitz gradient property (Assumption 2.1) and the triangle
inequality. Combining equation 49 and equation 50 yields
ι - Xj+』≤ κκ+1(i+Ln) (Xi - Xj(
+KnKI11⅛ X Vk-VfT(Xt)
k∈c-1(i)
Note that for any k1 > 0 and all a, b we have
K X Vk-VfT(Xj)
k∈c-1(j)
|a + b|2 ≤ (1 + kι)a2 + (1 + ^k-) b2.
(51)
(52)
C
D
E
≤
K
K
K + 1
	
x
	
	
	
18
Under review as a conference paper at ICLR 2020
Combining equation 51 and equation 52 while using the assumption that η ≤ 1/(2LK) results in
kxbti+1 - xbtj+1k2 ≤
(1 + kι)K2
(K +1)2
1+
(1 + kι)K 1
+ ηk1(κ + 1)2 2L
KK X	vk -vfτ(bi)) -1KK X	Vk -vfτ(bj)
k∈c-1 (i)	k∈c-1 (j)
(53)
2
Let k1 = (K+3∕4)2 - 1. Then by the definition of α,
(1 + kι)K2 < ɪY= (1 + kι)(K + 1/2)2 = (K + 3/4)2
(K +1)2 I + 2KJ =	(K +1)2	= (K +1/2)2
Furthermore,
1 + kι ≤ (K + 1)2
kι ~ K/2 + 5/16
so
(1 + kι)K 1 /	(K +1)2 K 1
ηkι(K + 1)2 2L ≤ η(K/2 + 5/16) (K +1)2 2L
_ K 1
=η (K/2 + 5/16) 2L
_ K
=ηL(K + 5/8).
(54)
(55)
Multiplying each side of equation 53 by L2/M2, summing the resulting inequality over i = 1, . . . , M
and j = 1, . . . , M, and using the relations equation 54, equation 55 we see that for all t ≥ 1,
Vt+1 ≤ αVt + Ut .
It remains to confirm that E[Ut | Ft-1] for all t ≥ 1.
Taking expectations in equation 14b, while noting that xbtc(k) = xtk and applying the variance bound
equation 13 along with the inequality ∣∣a + bk2 ≤ 2 (ka∣∣2 + ∣∣b∣∣2) it holds for all t ≥ 1 that
E [Ut | Ft-1]
KL
≤ η ∙ (K + 5/8)
× 击 X XE
i=1 j=1
K X	Vk -VfT(bt)
k∈c-1(i)
-(K X Vk -VfT (bj)
k∈c-1 (j)
2
Ft-1
KL 1
η (K + 5/8) M
β.
MM	2	2
XX2 (⅞ + ⅞)
i=1 j=1
□
Proof of Proposition 4.3
Proof. Note that according to Line 9 of SSGD,
(56)
and Lines 7 and 10 mean that for all j with c(j ) = i,
(57)
19
Under review as a conference paper at ICLR 2020
Using these equations, together with the definitions in the SSGD algorithm, we obtain that
J = MmXκ⅛ι 卜+1 + X χj+2)
i=1	j∈c-1(i)
(1	IMl	/	、
k+1 bt+ M X k+1 X (xj-ηvj)
i=1	j∈c-1(i)
=(k+1
=k+1
1M 1
Xt+ MXK+1 X (Xt-ηvt))
i=1	j∈c-1(i)
κ ʌ ι Λ ι V j
Xt- m∑k+ij Σ(「%
xbt +
K+1
E ʌ 1 ι M ι
=xt -η ( MTK+1
i=1
Σ vj I.
j∈c-1(i)
For Step A, note that xbt is the average of the xbit, and then use definition of the xbit from Line 9 of the
SSGD algorithm. Step B follows from equation 56 and the definition of the xi, 1 from Line 5 of
t+ 2
SSGD. Step C follows from equation 57. Step D follows by rearranging terms in the previous step,
and again noting the definition of xbt is the average of the xbit . Step E follows by grouping terms.
Continuing, we can express this recursion as an approximate form of gradient descent:
bt+1 = bt-η (MXk+1 X Vj)
i=1	j∈c-1(i)
=χt-ηKKι(MXK X Vj
i=1	j∈c-1(i)
bt- Kη+1(Vt + δ0，
where vt and ∆t are
vt
VMbt)+M X (K ( ∑	Vj) -VfT(Xi)
i=1	j∈c-1(i)
(58)
and
E (X VfT(Xt)) -VfT(bt).
(59)
Based on the definition of vt in Equation equation 58 and on Assumption 4.1, for all t ≥ 1 it holds
that
E[vt-Vfτ(bt) ∣Ft-ι]=O,
2
E [kvt -Vfτ(Xt)k2 |Ft-i] ≤ KV.
Thus Assumption 3.1 is confirmed. Next we consider the Lyapunov condition of Assumption 3.2.
Let the variables Ut, Vt for t ≥ 1 and the constants α, β be defined as in equation 14a-equation 14d.
20
Under review as a conference paper at ICLR 2020
Then by Assumption 2.1,
M
1 M
k∆tk2 = M ^fτ(bt) -Vf(Xt)
2
=1
2
≤ L2
	
L2
1M	1M
Mm X(Xt-Mm X Xj
=1
j=1
2
2
L2
	
MM
≤ L2 击 XXki
i=1 j=1
xbtj2
	
Vt.
The second line uses the Lipschitz gradient property, and the second to last line follows from Jensen’s
inequality (Section 6.6, Williams (1991)).
By our assumption that each node has the same initial state, then V1 = 0, hence Inequality equation 6
holds. The Inequalities equation 7 and equation 8 are established by Prop 4.1. According to the
definition of β (Equation equation 14d, We may write β = ηR where R = 4LσV∕(K + 5/8).
Note that 1/(1 - α) = 2(K + 1)2/(K + 5/8). Therefore,
R
-——=8Lσ2(K + 1)2/(K + 5/8)2 ≤ 32LσV
-α v	v
(60)
According to Corollary 3.4, then, a step-size of
1	E — G2dι(μv, μτ)2	]
L, m(2LσV∕K + 16LσV(K + 1)2/(K + 5/8)2) J
leads to
E()1	4m2(fτ(XI)- f *) (LσV∕K + 32LσV)
[τ(£)] ≤ 一(1 - C) C(E - G2dι(μv,μτ)2)2一
2Lm(fτ(xι) — f *) + mcG2dι(μv, μτ)2 + CE C
+	(1 — c) c (E — G2dι(μv, μτ)2)	+ 1 — C
Dropping the lower order terms, we see that
2
m2
(1 一 c)c(e - G2dι(μv, μτ)2)2
E[τ(E)] = O
Proof of Corollary 4.4
(61)
(62)
□
Proof. If SSGD runs until iterationτ(E), then number of times that the full gradient of fV is calculated
is dτ(e)/m] ≤ T3/m + 1, and the number of IFO calls for the training function is (τ(E) — 1)K.
Therefore
IFO(E) ≤
+ 1 ) nv + (τ(e) 一 1)K ≤ τ(e) (nV + K) + nv.
mm
(63)
Next, note that (1 一 c)c = (1 一 心)4K =；第1, which implies
1	_ 16K 2 / 16K
(1 - C)C = 4K — 1 ≤ 3 .
(64)
21
Under review as a conference paper at ICLR 2020
Combining equation 62, equation 63, and equation 64 we see that
E [IFO()]
mK (nV + mK)
O 1(E- G2d1(μv,μτ)2)2
+ nV
□
D Analysis of Decentralized SGD
The following result is a restatement of Lemma 5 of Lian et al. (2017).
LemmaD.1. UnderAssumPtion 5.1,thematrix limk→∞ ak is well-defined and has entries a∞j = 吉
for 1 ≤ i, j ≤ M. Furthermore, for all k ≥ 1, bound on the spectral gap implies ka∞ - ak k2 ≤ ρk.
Proof of Proposition 5.3
Proof. For ease of notation, for each t ≥ 1 let yt and zt be the M -dimensional vectors with
components defined as
yi = Xt - xt,	(65)
M
(66)
Let a∞ be the M X M matrix with entries a∞j =吉(see Lemma D.1). Then according to Line 5 of
Algorithm 3, the yt satisfy the recursion
yt+1 = (a - a∞)yt + ηzt.
Note that zt can be expressed as
Zi = Vf (xt) -Vf (Xt)
1M	1M
+ Vi- Vf(Xt)- M X(Vj - Vf(Xj)) + M X(Vf(Xj) - Vf(Xt))
M j=1	M j=1
Using the Lipschitz property (Assumption 2.1 ) then,
MM
|zn| ≤ LIxn- Xn| + |vn -Vf(Xn)I + M X |vn- Vf (Xn)I + M X wn-私|
M j=1	M j=1
(67)
(68)
(69)
Squaring and summing Equation equation 69,
M	M /	M	TM	∖ 2
X ∣znI2	= X	L∣Xn -XnI +	麓-Vf⑶)∣	+ M X	隰-Vf图)| + MX /-x„i
i=1	i=1	M j=1	M j=1
MM
=l24 X 禺-XnI2+4 X Ivn - Vf (Xn)I2
i=1	i=1
MM	2MM
+ M XXIvn-Vf(Xjn)I2+告 XXIxn FI2
i=1 j=1	i=1 j=1
M
= L28kynk2 + 8 X Ivni -Vf(Xin)I2
i=1
Taking square roots on each sides of this equation yields
uM
kznk ≤ tuL28kynk2 + 8 X Ivni -Vf(Xin)I2
≤ √L28kynk +

M
8 X Ivni - Vf(Xin)I2
i=1
(70)
22
Under review as a conference paper at ICLR 2020
Combining equation 67 and equation 70, then,
M
X |vni - vf(xin)|2
i=1
l∣yn+ιk ≤ (ka - a∞k + ηL√8) kynk + ηt 8
M
X ∣vn - vf (χn)∣2
i=1
≤ √ρp + ηLM8) kynk + ηt 8
In the second step we have applied Assumption equation 5.1 and Lemma D.1. Squaring this equation,
for any k1 > 0 it holds that
kyn+1k2 ≤ (I + kI) (√p + ηL√8) kynk2 + η2 (1 + k) 8 X Ivn-Vf(Xn)K	(71)
1	i=1
Let kι = l2ρP (in which case 1 + k1 =芒P). Multiplying each side of 71 by L2/M and noting that
Vt = M2 kyt k2, itfollows that
Vt+ι ≤( 2p" (√p + ηL√8) Vt+ Ut
It follows from the variance bound in Assumption 5.2 that
E [Ut | Ft-1] ≤ 8 η2 L21+pσV
1-p v
(72)
(73)
Using the assumption that η ≤ 4-√p, then equation 72 and equation 73 become, respectively,
Vt+1 ≤ ɪ
2
Vt + Ut
and
E [Ut | Ft-1] ≤√2η(1 - √P) L1+ρσV
1 - ρ
L√2 2
≤ η-.—σV
1-ρv
□
Proof of Proposition 5.4
Proof. To begin, note that the system average Xt satisfies the recursion
M
xt+ι=Xt+mm χ Vit	%
i=1
Define the variables vt and ∆t, for t ≥ 1, as
1M
Vt = Vf (Xt) + 而 E(Vt -Vf (Xt))
i=1
1M
∆n = M E(Vf (Xt)-Vf(Xt))
M t=1
Then we can express the recursion equation 74 as
Xt+ι = η (vt + ∆t)
23
Under review as a conference paper at ICLR 2020
We will show that this can be interpreted as a form of biased SGD and therefore we may apply
Corollary 3.4. For the unbiased component vt, observe that
1M
E[vt-Vfτ(Xt) | Ft-ι] = E MN(VtTf (Xi)) ∣Ft-ι =。
and
1M
E [kvt-VfT(Xt)k2 | Ft-ι] ≤ E 而 ∑>t-Vf(xt)∣2 = /
i=1
For the bias term, note that
(75)
(76)
L2 M
k∆tk2 ≤ MM X Ixt-Xtl2= Vt
M i=1
Assumption equation 3.1 follows from equation 75 and equation 76, while Assumption equation 3.2
follows from Proposition 5.3. The result then follows from Corollary 3.4.	□
E Analysis of SVRG
For the analysis of SVRG, define the filtration {Ft}t=0,1,... as follows. F0 = σ(X1m) and for all
s ≥ 1,
Fs = σ ({x5ln} ∪ {ij J 0 ≤ t ≤ m - 1, 1 ≤ j ≤ s}) ∙
We will leverage some prior results concerning the behavior of SVRG. The following is adapted from
Reddi et al. (2016a).
Proposition E.1. Let Assumptions 2.1 and 2.2 hold. Let β > 0 and define the constants
cm, cm-1, . . . , c0 as follows: cm = 0, and for 0 ≤ t ≤ m- 1, let ct = ct+1(1 +ηβ +2η2M2) +η2M3.
Define Γt for 0 ≤ t ≤ m 一 1 as Γt = η 一 ctβ1η 一 η2L 一 2ct+ιη2. Suppose that the step-size n and
the analysis constant β are chosen so that Γt > 0 for 0 ≤ t ≤ m - 1, and set γ = inf 0≤t<m Γt.
Then for all s ≥ 1,
mX E[kVfτ(xs+1)k2 I Fs-ι] ≤ fT(xm)- E[fτ(xm+1) IFs-1].	(77)
t=0	γ
Furthermore, if η is oftheform η = ξ∕(Ln2/3) for some ξ ∈ (0,1) and ifthe epoch length is set to
m = [n∕(3ξ)C, then there is a valuefor β such that Y ≥ L(ξ⅛ where V(ξ) is a constant dependent
only on ξ. In particular, if ξ = 1/4 then
Y ≥ 40Ln2∕3.	(78)
Proof. The proof of equation 77 follows from nearly the same reasoning used to establish Equation
10 in (Section B, Reddi et al. (2016a)), the only difference being that conditional expectations replace
expectations in all of the relevant formulas. The details are left to the reader.
Formula equation 78 is a numerical inequality whose proof can be derived from the proof of Theorem
3 given in in (Appendix B, Reddi et al. (2016a)).	□
Proof of Proposition 6.1
Proof. First, note that τ() is a well-defined stopping time with respect to the filtration {Fs}s=0,1,.
For s = 1, 2, . . . define the random variables δs as
kVfτ(χs+1)k2 一 fT(Xm) 一 fT(XmH)
t=0	Y
m-1
δs = X
24
Under review as a conference paper at ICLR 2020
It holds trivially that for all s ≥ 1,
X ∣∣Vfτ(χs+1)k2 = fT(xm) - fτ(xm+1) + δs
t=0	γ
and by Proposition E.1, for all s ≥ 1,
E[δs | Fs-ι] = XE h∣∣Vfτ(xS+1)∣2 I Fs-Ii- fT(xm) - E[fτ(xm+1) IFsTI
t=0	γ
≤ 0.
Summing Equation equation 79 over s = 1, . . . , q yields
XXmXI kVfτ(χs+1)k2 = fT(xm)-fT(Xm+1) + XXδs,
s=1 i=0	γ	s=1
Rearranging terms and noting that fτ (χm+1) ≥ f * results in
q m-1	q
γ X X ∣VfT(xis+1)∣2 ≤ fT (x1m )-f*+γ X δs.
(79)
(80)
(81)
(82)
It follows that
qq
γ	∣VfT(xs0+1)∣2 ≤fT(x1m) -f*+γ	δs.	(83)
For r ≥ 1, let τ() ∧ r be the stopping time which is the minimum of τ() and the constant value r.
Applying Proposition A.1 together with Equation 80, it holds that
τ ()∧r
E X δs ≤0
s=1
(84)
Furthermore, by definition of τ ,
τ ()∧r
E X ∣VfT(xs0+1)∣2
s=1
(τ ()∧r)-1
≥ E X	∣VfT(xs0+1)∣2
s=1
≥E
(τ ()∧r)-1
X e
s=1
=	E[(τ () ∧ r) - 1].
(85)
Combining equation 83, equation 84, and equation 85 yields
γE[(τ()∧n)-1] ≤fT(x1m)-f*
Rearranging terms in the above yields
E[τ(E) ∧ n] ≤ fT(Xm) - f * + 1.
γ
Applying the monotone convergence theorem, then,
E[τ(e)] ≤ fT(xm) - f * + 1.
γE
Next, specialize η and m to η = ξ∕(Ln2/3) and m = [n∕(3ξ)C with ξ = 1/4. Then by Proposition
E.1, γ ≥ 1/(40Ln2/3). Therefore,
E[τ(E)] ≤
40Ln2∕3(fτ(xm)- f *) +1
E
□
25
Under review as a conference paper at ICLR 2020
F	Generalization Analysis
Proof of Corollary 7.2
Proof. Under our assumption on the LiPschitz property of y → Nxf (y, x), it holds that
kVfμ(xτ(e))k ≤ kVfv(xτ"k+ Gdι(μ,μv).
Squaring and taking expectations, while noting that d1 ≤ d2 (see Villani (2008), Remark 6.6),
E[kVfμ(xτ(e))k2] ≤ 2E[kVfv(xτ9)k2] + 2G2E[d2(μ,μv)2].
Using the Wasserstein concentration bound from Theorem 7.1 and the definition ofτ(), we obtain
E[kVfμ(xτ(e))k2] ≤ 2 + 2G2KdJnV3"
□
G	Experimental methodology
G.1 SSGD experiments
The neural network model used for these experiments is LSTNet Lai et al. (2018)with CUDA-aware
MPI and extended to use the Stacked SGD training method. The objective function is the squared
error between the prediction and the true sensor measurement, averaged over the dataset of training
instances.
Our experiments compared SSGD (Algorithm 2) and SGD (1). For SSGD, each cluster has 4 worker
nodes and 1 communication node. Hence, K = 4 and M is varied from 1 to 64. Since one physical
node has four GPU devices, each physical node in the HPC environment is modeled a single local
cluster in SSGD. The scalability of SSGD is compared with the basic parallel implementation of
SGD, where all node synchronize with all-reduce collective communication call after each parameter
update.
The experiment is conducted in a high performance computing environment that is equipped with
108 nodes. Each node has a dual Intel Xeon E5-2695v4 (Broadwell) CPU, four NVidia K40 GPUs,
SAS-based local storage, and 256 GB of memory. The nodes are inter-connected with a non-blocking
Infiniband EDR fabric.
G.2 SVRG experiments
For the first experiment in this section, we trained a neural network with no hidden layer (i.e., a
logistic classifier) for the MNIST classification task. For the second experiment, we trained a neural
network with one hidden layer of n = 100 nodes on the CIFAR-10 classification task. The activation
function in the hidden layer was the logistic function σ(x) = (1 + e-x)-1. In both cases, the
objective function is the average cross entropy loss.
Our experiments compared SVRG (Algorithm 4) and SGD (Algorithm 1). The only modification to
the algorithms was that mini-batches were used to make the gradient estimates. For each algorithm
we determined the values of the learning rate and mini-batch size using a validation set. For the
learning rate, we searched over the values η ∈ {0.001, 0.01, 0.1, 1.0}. For the mini-batch size, we
considered values in {32, 64, 128}. The values used for full training were determined by running the
training procedure for several epochs and evaluating the resulting model on a held-out portion of the
training dataset. The parameters that gave best performance on this held-out dataset were used for
full training. Using the found settings, we ran five independent runs of training, and report the mean
and confidence bands (representing one standard deviation) for the expected IFO calls.
26