Under review as a conference paper at ICLR 2020
Curvature-corrected learning dynamics in
DEEP LINEAR NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks exhibit complex learning dynamics due to highly non-
convex loss landscape. Second order approaches, such as natural gradient descent,
mitigate such problems by neutralizing the effect of potentially ill-conditioned
curvature, yet it is largely unknown how the current theory of deep learning gen-
eralizes beyond gradient descent to these higher order learning rules. To answer
these questions, we derive exact solutions to learning dynamics of deep linear
networks under a spectrum of curvature-corrected learning rules. Our analysis
reveals that curvature corrected learning preserves a core feature of gradient de-
scent, a conservation law, such that the learning trajectory follows precisely the
same path in the underlying manifold as gradient descent, only accelerating the
temporal dynamics along the path. We also show that layer-restricted approxi-
mations of natural gradient, which are widely used in most second order methods
(e.g. K-FAC), can significantly distort the learning trajectory into highly diverging
dynamics that significantly differs from true natural gradient, which may lead to
undesirable network properties. We also introduce fractional natural gradient that
applies partial curvature correction, and show that it provides most of the benefit
of full curvature correction in terms of convergence speed, with additional ben-
efit of superior numerical stability and neutralizing vanishing/exploding gradient
problems, which holds true also in layer-restricted approximations.
1	Introduction
Difficulty in training deep neural networks arises from the fact that the network’s input-output map
fθ (∙) is nonlinearly related to its parameters θ. This causes non-convex loss landscape with Prolifera-
tion of saddle-points and poorly-conditioned curvature where gradient-based first order optimization
methods perform poorly (Martens, 2010; Dauphin et al., 2014). Second order methods, such as nat-
ural gradient descent (Amari, 1998), compensate for the effect of curvature by using the distance
metric intrinsic to the space of input-output maps to define the update steps (Pascanu & Bengio,
2013; Martens, 2014; Bernacchia et al., 2018), rather than the parameter space. Recent advance-
ments led to approximate implementations of these methods that prove efficient for practical scale
applications (Ba et al., 2016; Grosse & Martens, 2016; Martens et al., 2018; Osawa et al., 2019).
Despite their practical effectiveness, however, the exact nature of such curvature-corrected learning
process remains largely unknown. Do curvature-corrected learning methods simply accelerate con-
vergences towards the same minimum solutions as gradient descent, or do they impose implicit bias
toward qualitatively different solutions?
As a first step toward establishing theoretical understanding of these questions, we analyze the exact
learning dynamics of deep linear networks under a spectrum of curvature-corrected update rules.
Deep linear networks provide an excellent mathematical framework for developing insightful theo-
retical understanding of the complex inner workings of deep nonlinear networks (Goodfellow et al.,
2016). Despite their simplicity, deep linear networks capture the essential nonlinear relationship be-
tween network’s input-output maps and their parameters, and exhibit comparable learning behavior
to their nonlinear counterparts that can be exactly solved for rigorous analysis. Indeed, many recent
works analyzed the learning trajectories of deep linear networks under gradient descent to compute
the convergence rate under various initial conditions (Arora et al., 2018a;b; Bartlett et al., 2019; Du
& Hu, 2019), revealed decoupled modes of convergence dynamics to explain the origin of multiple
1
Under review as a conference paper at ICLR 2020
stage-like loss profiles (Saxe et al., 2013), and showed the implicit bias for regularization (Du et al.,
2018; Arora et al., 2019) and resistance to overfitting (Advani & Saxe, 2017; Lampinen & Ganguli,
2018; Poggio et al., 2018). Yet, it is uncertain whether these convergence properties generally apply
for update rules beyond gradient descent.
Our contribution The main results are summarized as follows.
1.	We derive a generalized conservation law that describes the optimization paths of network
parameters under gradient descent as well as curvature-corrected update rules. Conse-
quently, curvature correction only affects the speed of convergence without affecting other
qualitative properties of parameter update process.
2.	There is a trade-off between map dynamics and parameter dynamics. The full curvature
correction effect of natural gradient descent (NGD) completely linearizes the map learn-
ing dynamics of deep networks, equivalent to that of shallow networks. Such complete
linearization, however, sacrifices stability of parameter update dynamics to explode when
gradient vanishes and vice versa.
3.	We introduce a regularized version of NGD that partially corrects for the effect of curva-
ture, called √NGD, which facilitates the parameter update dynamics by eliminating the
vanishing/exploding update problems. This makes the map dynamics slightly nonlinear,
but no more so than that of single hidden layer networks under gradient descent.
4.	NGD makes the learning process prone to overfitting by simultaneously learning both the
signal and the noise dimensions of data, whereas √NGD partially retains gradient descent's
resistance to overfitting by separating the time-scales between the signal and the noise
dimensions.
5.	The widely-used block-diagonal approximation of NGD breaches the aforementioned con-
servation law, resulting in highly divergent parameter update dynamics, which breaks the
weight balance across layers. In contrast, block-diagonalization of VzNGD preserves sta-
bility of parameter update dynamics, yielding efficient and stable learning algorithms. 2
2 Setup and notations
Consider a depth d network that consists of an input layer, d - 1 hidden layers, an output layer, and
weight matrices w ≡ {wi }id=1 that connect the adjacent layers. The network’s input-output map is
W ≡ Qd=ι Wi = Wd …wι, such that fw(x) = Wx. The network learns the input-output statistics
of a dataset D = {χμ, yμ}p=ι by minimizing the squared-error loss:
1
L(W) = 2ED[kWx - y『]=Tr g(w - W*)Σ,(W - W*)
|
+ const,
2
where ED is the expectation over the dataset D, Σx ≡ ED [xx|] is the input correlations, and
W* ≡ ED [yx|] Σ-1. Neglecting the constant term, the loss function is expressed as
L(W)= Tr ^∆∑χ∆l .	(∆ ≡ W — W*)	(1)
where ∆ denotes the displacement between W and W*.
Shallow networks (d = 1, W = wi) exhibit linear learning dynamics under gradient descent,
whose convergence rates scale with eigenvalues of Σx . In this case, curvature correction has the
well-understood effect of normalizing the convergence rates, which is also achievable by simple
pre-whitening of input correlation. Instead, we are interested in the less-understood effect of how
curvature correction facilitates the complex nonlinear dynamics of deep networks (d ≥ 2). There-
fore, we consider pre-whitened input distribution Σx = I to isolate the nonlinear effect of curvature
correction, but this condition is not critical for the analysis.
2
Under review as a conference paper at ICLR 2020
Gradient and Hessian+ We use bold symbols to collectively represent network parameters and
derivatives in array form. For example, W ≡ W 1 and g ≡
continuous-time weight update and the gradient of a depth d = 2 network. Hessian is fully charac-
terized by its operation on weight update, which, by definition, produces gradient update:
w2| ∆
∆ w1|
represent the
Hw = g
^ T ɪ	. T A -
w| △ + W | ∆
ɪ	T	A . T
△w| + ∆w∣
/ Λ	ʌ	∙ ɪ	、	Z/-»\
(△ = W = W2W1 + W 2 W1)	(2)
However, true Hessian-based methods (e.g. Newton-Raphson method) can converge to any extrema
types. To guarantee convergence to (local) minimum solutions, natural gradient methods use positive
semi-definite (PSD) approximations of Hessian (e.g. Fisher matrix (Amari, 1998; Heskes, 2000;
Martens & Grosse, 2015; Bernacchia et al., 2018), Generalized-Gauss-Newton matrix (Martens,
2014; Botev et al., 2017; Roux et al., 2008)1), which correspond to
H+W
- τ :.
w2 △
△ w1.
(3)
慈
This operation is indeed PSD, since W ∙ H+w = Tr[w]w。+ W、△ w|] = Tr[^^1] ≥ 0, where
the dot-product denotes a ∙ b ≡ Pd=ι Tr[aib∣]. We refer to this operation as Hessian+.
Null-space and Conservation laws Deep linear networks exhibit inherent symmetries that their
input-output map W is invariant under transformations that multiply arbitrary matrix m to one layer
and its inverse to the next layer, i.e.	W1	→	mW1	,	∀m.	Wn皿	≡	mW1	are the equivalent
W2	W2 m-1	nu	-W2m
continuous-time transformations that yield the invariance △ = W = W2mW1 — W2mW1 = 0, ∀m.
These transformations form the null-space of H+, since W口皿∙ H+w口皿=Tr^Z] = 0, which is
orthogonal to gradient, since Wnull ∙ g = Tr[4Z] = 0. Also orthogonal to the null-space is natural
gradient, since W∩un ∙ Hjg = g ∙ H+W询=0, where H+ denotes Moore-Penrose pseudo-inverse.
These continuous symmetries imply the following, self-explanatory theorem (Noether’s theorem):
Theorem 1 All update rules W that are orthogonal to the null-space, i.e.
d
W ∙ Wnull = ETr[(WiW| - Wl+ιWi+ι)mi] = 0, Ymi
i=1
exhibit the following conservation law
d/dt (WiWi| - Wi|+1Wi+1) = 0, ∀i	(4)
This result was previously only known for gradient descent dynamics (Arora et al., 2018b; Du et al.,
2018), which is generalized here.
3	Learning dynamics
In this section, we analyze the learning dynamics of the network parameters W (Section 3.1) and the
update dynamics of the input-output map W (Section 3.2) under a spectrum of curvature-corrected
update rules. We then analyze how block-diagonal approximation modifies the curvature-corrected
dynamics (Section 3.3).
3.1	parameter dynamics
We follow the singular value decomposition (SVD)-based analysis of Saxe et al. (2013); Advani &
Saxe (2017); Lampinen & Ganguli (2018), by considering network weights that are initialized to
1Fisher matrix and Generalized-Gauss-Newton matrix are equivalent in many cases, including the least
squares problem considered here (Pascanu & Bengio, 2013; Martens, 2014).
3
Under review as a conference paper at ICLR 2020
have their map's singular vectors aligned with those of w* .2Under such initialization, the update
dynamics of weight matrices simplifies to their singular value dynamics, with their singular vec-
tors remain unchanging. This simplified case admits exact analytic solutions, which provide good
approximation to general learning dynamics. Moreover, this aligned singular vector condition is au-
tomatically satisfied for networks initialized with small random weights Saxe et al. (2013); Advani
& Saxe (2017).
Steepest gradient descent (SGD) Under SGD update, deep networks’ weight parameters exhibit
coupled nonlinear dynamics: (d = 2 example, η: learning rate)
w + ηg
W ι + ηw∣ ∆
W 2 + η ∆ w∣
(5)
The SVD analysis decomposes eq (5) to individual singular mode dynamics. The dynamics of one
singular mode is described by2 3(See S.I.)
d
σi + ησ∆ ji = 0	(σ∆ = σ - σ*, σ = 口⑪)	(6)
i=1
where σi, σ*, σ, σ∆ are the singular values of w%, W*, W, ∆, and ji ≡ ∂σ/∂σ% = 6/σi denotes the
coupling between the input-output map and parameters, i.e. Jacobian. Note that this singular mode
dynamics follows the hyperbolic paths
σi2 - σk2 = constant, ∀i, k	(7)
which is the direct consequence of the conservation law (4). The update speed ∣∣<σk is proportional
to the displacement ∣σ∆∣ and the coupling strength ∣∣j∣
kσk « lσ∆lkjk,	(kσk2 ≡ Pd=I σ2, kjk2 ≡ Pd=Ij2)⑻
which vanishes for networks with small coupling strength and explodes for large coupling strength.
Natural gradient descent (NGD) NGD finds the minimum-norm update solution (min W ∙ W)
subject to the constraint (i.e. Moore-Penrose pseudo-inverse solution)
H+W + ηg
■ T / ：	. X -1
w2 (∆ + η∆)
/ ɪ	ɪ ∖ T
(∆ + η∆)w1
0,
(9)
which can be solved using Lagrange multipliers to yield (See S.I.)
(10)
where Λ satisfies
w∣S(Λ) = S(Λ)w∣ = 0.	(S(A) ≡ (w2w∣ )Λ + A(w∣wJ — ∆)	(11)
Remarkably, the only change from SGD update (5) is replacing ∆ with Λ as the main drive of
dynamics eq (10), which preserves orthogonality to null-space and hence the conservation law (4) 4.
The singular mode dynamics of NGD update eq (10) is5
ji
σi + 〃= j2 = 0，
(12)
where σ∆ of SGD eq (6) is replaced by qλ = σ∆∕∣j∣2, the singular values of A (See S.I.). NGD
dynamics eq (12) follows the same hyperbolic paths of SGD eq (7), but with modified update speed
kσ k
∣σ∆∣
(13)
which inversely scales with ∣j ∣. Therefore, NGD’s update speed explodes for small coupling
strength, reciprocal to SGD’s vanishing speed phenomenon.
2Given SVD of weight matrices Wi = LiDiR and W* = L*D*R1, where D are the diagonal singu-
lar value matrices and L/R are the left/right singular vector matrices, the aligned singular vector condition
assumes R1 = R* , Ld = L* and Ri+1 = Li for all layers 1 ≤ i ≤ d - 1.
3 The dynamics eq (6),(12) apply to all active singular modes. Inactive modes that have σ = 0 stay frozen.
The number of active modes is determined by the bottleneck size, i.e. the narrowest width of network.
4The Moore-Penrose pseudo-inverse solution is guaranteed to be orthogonal to the null-space, since non-
zero null-space components only increase the solution’s norm without affecting the constraint eq (9).
4
Under review as a conference paper at ICLR 2020
Figure 1: Learning dynamics of a singular mode of a single hidden layer network (d = 2). The
contour lines visualize the manifolds of constant displacement levels qδ ≡ σ1σ2 - σ*. The op-
timal solution σ∆ = 0 is shown in black. Tangent space of the manifolds defines the null-space
of Hessian+. The vector field visualizes the displacement-normalized update [σι,σ2]∕∣σ∆∣, whose
amplitude is the normalized update speed: k<τk∕∣σ∆∣ H IIjk1-2/q. (A,B,C) SGD, NGD, √NGD
share the same update directions defined by the hyperbolic paths that conserve σ12 - σ22 (red lines),
orthogonal to the null-space. But they exhibit different update speed: SGD exhibits vanishing speed
problem for small weights, while NGD has the opposite problem. In contrast, √NGD exhibits con-
stant normalized speed. (D) NGD-d exhibits radially diverging vector field that conserves σ1∕σ2 .
The learning trajectories of NGD and NGD-d traverse the contour lines with synchronized timing
(red dots), (E) √NGD-d exhibits vector field of constant direction and amplitude that conserves
M|-g|.
Fractional Natural Gradient Descent (qNGD) Above results can be generalized to a spectrum
of update rules that apply partial curvature corrections, described by 塞Hw + ηg = 0, where
√H+ is a fractional power of Hessian+ (q ≥ 1). The singular mode dynamics of qNGD is
σi + W j⅛ =0，	(14)
which interpolates between NGD (q = 1) and SGD (q → ∞). Eq (14) follows the same hyperbolic
paths of SGD eq (7), but with modified update speed
kσ kfx∣σ∆∣jk1jq.	(15)
Note that for q = 2, termed √NGD, the update speed becomes independent of the coupling strength
kσ Il = η ⑸，	(I6)
thus eliminating the vanishing/exploding update speed problems of SGD/NGD (See Fig 1C).
Relation to Regularized NGD Alternative interpolation solves (H+ w + ηg) + eI(w + ηg) = 0
( ≥ 0), which yields the regularized (or damped) inverse
W = -η(e + 1)(eI + H+)-1g,6 *	(17)
similar to Levenberg-Marquardt damping (less the (e + 1) term), whose singular mode dynamics is
σi + nσ 俞(a!l+ll∣+∣1 )=0，	(a ≡e/kjk)	(18)
6 This expression reduces to SGD in the limit → ∞, which differs from the usual regularized inverse
W = — η(eI + H+)-1g, which reduces to 0.
5
Under review as a conference paper at ICLR 2020
Figure 2: Learning curves of input-output map and loss profile for various stiffness P levels: P =
0 corresponds to NGD update, 1/2 ≤ P < 1 corresponds to VzNGD update, and 1 ≤ p < 2
corresponds to SGD update for network depth ranging between 2 ≤ d < ∞. Top: Learning curves
of map singular modes(j(t)from eq (21). Dashed lines show the mode-strength of dataset σ*. Note
that large P increases the stiffness of dynamics, i.e. extreme changes of time-scale: between extreme
slow and extreme fast. Half-max points (black circles) are shown to visualize the overall time-scale
of learning dynamics, which decreases with mode strength as σ-p. Bottom: Corresponding loss
profiles. Initial conditions:1(0)= 0 for P < 1, and 行⑼ = σ"100 for P ≥ 1. η= 1.
where the ratio a ≡ /kj k describes the effective degree of interpolation between NGD (a → 0)
and SGD (a → ∞). Note that a should be large enough to provide sufficient damping, but not too
large to nullify the effect of curvature correction, which is difficult to simultaneously satisfy across
all singular modes with fkxed e. √NGD can be considered as providing ideally and adaptively tuned
regularization (a = 1) for all singular modes, where the regularization is most effective.
3.2	Map dynamics
The parameter update ultimately drives the learning dynamics of the input-output map, via Jacobian
d
σ = X σ iji,	(19)
i=1
which yields the following map learning dynamics under qJNGD update (14)
σ = -η(σ - σ*)jk2(IT/q).	(20)
In general, eq (20) does not admit closed-form solutions due to the coupling strength term, with the
exception of NGD (q = 1). As shown by the vector field in Figure 1, however, the coupling strength
changes in a streotypical manner along the learning trajectories. Therefore, the general charac-
teristics of map dynamics can be appreciated from the representative case of balanced weights:
σi = σ1∕d ∀i, or in terms of the conserved quantities, wiwi| - wi|+1wi+1 = 0. Note that this bal-
anced weight condition is automatically approximately satisfied if the networks are initialized with
small random weights.
Under the balanced weight condition, eq (20) simplifies to
σ = -η(σ - σ*) σp	(P ≡ 2(d一1)(q一1))	(21)
dq
where η ≡ nd1-1/q is the depth-calibrated learning rate, and P represents the combined effect
of depth and curvature correction that determines the stiffness, or degree of nonlinearity, of map
6
Under review as a conference paper at ICLR 2020
, as well as p = 2 case:
(p=0)
(p=0.5)
(p=1)
dynamics. Figure 2 shows the following notable closed-form solutions
σ(t) = σ*(1 - e-ηt)
σ(t) = σ* tanh2 (η√σ7t∕2)
—	σ*
σ(t) = i + (σ"σ(0) - i)e-ησ*t
where zero initial condition 行(0)= 0 is assumed for p < 1 cases.
NGD update (q = 1, p = 0) Under NGD update, the map dynamics exhibits fully linearized
convergence dynamics with a constant time-scale η-1 for all depth d and data mode-strength σ*. Its
learning curves exhibit finite growth rate near zero 行⑶ ≈ η σ*t, which entails exploding parameter
update speed as the coupling strength approaches zero. Therefore, the full curvature correction of
NGD sacrifices stability of parameter dynamics in order to perfectly cancel out all nonlinearities of
map dynamics.
√NGD update (q = 2, P = 1 — 1/d) For √NGD update, the stiffness ranges from P = 0.5 for
single hidden layer networks top → 1 in infinite depth limit. Its learning curves exhibit polynomial
growth near zero, σ(t) a t1/(1-p), which takes finite time to escape from zero initial condition.
even though the initial growth rate vanishes with the coupling strength. The overall time-scale of
learning decreases with mode strength as σ-p, such that stronger singular modes (large σ*) learn
faster than weaker modes.
SGD update (q → ∞, P = 2 - 2/d) Under SGD update, the stiffness ranges from P = 1 for
single hidden layer networks to P → 2 in infinite depth limit. Its learning curves exhibit sigmoidal
shape that take infinite time to escape from the saddle point at zero initial condition: the escape time
diverges as O(- log行(0)) for P = 1 and O(σj-p) for P > 1. Also, the increasedP causes greater
separation of time-scales (ησp)-1 across singular modes, which results in stage-like transitions over
the course of training, with each singular mode making sudden transition from slow learning to rapid
convergence (Saxe et al., 2013).
Effective Depth Network depth d and curvature correction q interact in a symmetric manner,
which can be intuitively understood by representing stiffness in terms of the corresponding network
depth under SGD update, called the effective depth:
dq
deff = ~j~	7,
d+q- 1
(22)
which approaches the actual depth deff → d in the SGD limit (q → ∞), and similarly, approaches
deff → q in the limit of infinite depth (d → ∞). Therefore, qNGD reduces the network,s effective
depth to be strictly less than q. For √NGD, this upper-limit is 2, i.e. single hidden layer network.
To summarize, curvature correction lowers the nonlinearity/stiffness of map dynamics of deep net-
works by reducing their effective depth. The full curvature correction effect of NGD perfectly
cancels out all nonlinearities of map dynamics to exhibit linear convergence, equivalent to shallow
network learning, but it sacrifices stability of parameter dynamics to explode at the saddle point.
In contrast, partial curvature correction of VzNGD directly facilitates the parameter update dynam-
ics, which eliminates the vanishing/exploding update problem, and it makes the map dynamics only
slightly nonlinear, but no more so than that of single hidden layer networks under gradient descent.
3.3	effect of layer-restricted approximation
Block-diagonal NGD (NGD-d) In most practical deep learning applications, numerically esti-
mating and inverting Hessian+ becomes prohibitively expensive. Instead, most second-order meth-
ods approximate NGD by applying layer-restricted curvature corrections, ignoring the off-block-
diagonal Hessian+ terms across different layers (Martens & Grosse, 2015; Ba et al., 2016; Grosse &
Martens, 2016; Martens et al., 2018; Bernacchia et al., 2018): (d = 2 example)
HIw ι + ηιgι
H2W 2 + η2g2
w| w?w 1 + ηιw2∆
w 2W1W∣ + η2∆w∣
0,
(23)
7
Under review as a conference paper at ICLR 2020
which nevertheless satisfies the NGD constraint (9) if Pid=1 ηi
η. Hi denotes the block-diagonal
Hessian+ term of layer i. Singular mode dynamics of eq (23) is (with η = η∕d)
ησ∆
σi + J ∙	= 0,
d ji
(24)
where the layer-restricted factor ji2 substitutes the full curvature correction factor kj k2 of NGD (12).
This block-diagonalization significantly modifies the parameter update dynamics by adding non-
zero null space components. Instead of the hyperbolic paths (7), eq (24) follows radially diverging
paths that conserve σi /σk as constants of motion. Consequently, NGD-d update exhibits larger
parameter update speed than NGD7, and converges to less efficient, large norm solutions that are
highly sensitive to initial conditions and perturbations (Fig 1D, red line). Despite the vastly dif-
ferent parameter dynamics, however, NGD-d exhibits identical map learning dynamics as NGD
σ = -η (σ — σ*) (Fig 1BD, red dots), because the input-output map is invariant under null-space
transformations.
Block-diagonal √NGD (√NGD-d) More generally, block-diagonalized fractional NGD
H? Qwi + η gi/d1/q = 0 yields
η σ∆ 1-2/q
σ i+而 ji =0,
(25)
which conserves a2(1-1/q) - a2(1-1/q) as constants of motion. For q = 2, called √NGD-d, the
singular mode dynamics
σi + η√∆ sign(ji) = 0，
(26)
follows non-diverging, parallel paths that conserve ∣σi∣-∣σk |, with identical parameter update speed
as √NGD's ∣∣<σk = η ∣σ∆∣ (Fig 1E). Therefore, √NGD-d yields neutrally-stable update dynamics
that neutralizes the vanishing/exploding update speed problems.
4	Implicit bias for regularization
Recent works have shown that learning dynamics of deep networks under SGD update exhibits
implicit bias towards keeping the network well regularized. Here, we consider two such properties,
and analyze how they generalize under curvature-corrected update rules.
Weight balance Deep neural networks often exhibit redundant parameterizations, such that con-
figurations of parameters implement the same input-output map. One such redundancy, or symmetry,
that concerns both deep linear networks and deep ReLU networks is homogeneity, such that multi-
plying a layer by a positive scalar c and divide another layer by c, does not change the input-output
map. The problem is that c can be arbitrarily large or small, yielding potentially unbounded, yet
valid solutions. Such unboundedness poses major theoretical difficulty for convergence analysis of
gradient-based local optimization methods (Lee et al., 2016; Shamir, 2018).
Fortunately, SGD update exhibits implicit bias toward automatically balancing the norm of different
layer’s weight matrices. The proof directly follows from the following conserved quantity of scalar
multiplication symmetry ∣wi∣F2rob - ∣wi+1 ∣2Frob, which is a relaxed version of the aforementioned
conservation law eq (4). Thus, if the weights are initially small, this differences between squared
norm will remain small through out learning process, thus establishing balancedness across layers
Du et al. (2018).
As shown in section 2, curvature-corrected updates (e.g. NGD and √NGD) retain orthogonality to
the null-space of symmetry, and thus comply with the same conservation laws as SGD. We show
numerical confirmation of this prediction in S.I. The conservation of squared difference of norms
for homogeneous ReLU networks still requires similar numerically confirmation.
In contrast, block-diagonalized methods do not follow the same conservation law. NGD-d conserves
the ratio between singular values across layers σi /σk , which does not guarantee balancedness even
7
∣∣(σ kNGD-d
≥ MIINgd can be shown using Jensen,s inequality: d Pd=ι
8
Under review as a conference paper at ICLR 2020
with small initialization. √NGD-d, however, conserves the absolute difference of singular values
across layers ∣σ∕ - E |, which guarantees balancedness, at least under the condition of aligned
singular vectors: That is, the ratio between the singular values would approach close to 1 if they
grow from small initial values, while maintaining the small absolute difference. Although this does
not constitute a formal proof for general case, √NGD-d confirms to maintain balancedness across
layers in numerical simulations (See S.I.).
Low rank approximation / Generalization dynamics The learning dynamics of the input-output
map under SGD update separates the time-scales of learning across singular modes (ησp)-1, such
that the singular modes with stronger data correlation preferentially are learned faster (Saxe et al.,
2013). This property yields an implicit regularization property for deep networks to efficiently
extract low-rank structure of the dataset, such as for finding matrix factorizations with minimum
nuclear norm (Gunasekar et al., 2017; Arora et al., 2019). It also allows deep networks to avoid
overfitting via early stopping by first learning the signal dimensions of noisy dataset, before the
overfitting of the noise dimensions occurs, as long as signal-to-noise ratio is sufficiently large Ad-
vani & Saxe (2017); Lampinen & Ganguli (2018) which yields good generalization performance to
unseen data. However, this approach requires the network to be trained from small random weight
initialization, where SGD suffers from vanishing gradient problem.
In curvature-corrected cases, the learning speed of map dynamics eq (21) scales as σ-p. Under
NGD, the map dynamics is perfectly linearized (p = 0), which also removes its ability to separate
out the time-scales. This makes NGD update prone to large generalization error due learning the
noise dimension simultaneously with the signal. In contrast, √NGD partially retains time-scale
separation in the learning dynamics, while also accelerating the parameter update dynamics near
zero weights.
We the generalization property of curvature corrected learning rules with student-teacher task from
Lampinen & Ganguli (2018), in which the training and test dataset are generated by a teacher net-
work yμ = W*xμ + zμ, where xμ ∈ RN is the input data, yμ ∈ RN is the output, W*xμ is the signal
and zμ ∈ RN is the noise. Teacher,s input-output map w* ∈ RN×N is assumed to have a low-rank
structure (rank 3), and the student is a depth d = 4 network of constant width N = 16, whose
weight matrices are initialized to have maximum singular value of 0.05. The number of training
dataset {χμ, yμ}p=ι is set to be equal to the effective number of parameters P = N, which makes
the learning process most susceptible to overfitting.
For numerical calculation of NGD and √NGD, the Hessian+ block between layer i and k are com-
puted as described in Bernacchia et al. (2018) (eq 42), which are then concatenated to the full
Hessian+ matrix and numerically inverted (or sqrt-inverted) via eigen-decomposition. Levenberg-
Marquardt damping of e = 10-5 and update clipping are used for numerical stability of NGD.
√NGD does not require such clipping or damping terms.
Figure 3 shows the result of training. SGD exhibits stage-like transitions, which first learns the three
signal modes, well separated from the onset of overfitting of the noise modes begins, which allows
effective early stopping scheme. However, it suffers from the long plateaus due to vanishing gradient
problem.
NGD (and NGD-d) updates learn all singular modes simultaneously including the noise modes (See
Fig 3 D), which leads to high generalization error. Note that NGD,s loss profile deviates from
exponential decay due to the clipping. In contrast, √NGD (and √NGD-d) allows fast learning
while separating the signal dimensions from the noise dimensions, achieving comparable test loss
as SGD update, but also with fast early-stopping time comparable to NGD update. Note that all
three update rules achieve the same test loss after overfitting is complete. This is due to the shared
learning path for each singular mode across the methods.
5	Conclusion
To summarize our contribution, we derived a generalized conservation law that describes the opti-
mization paths of network parameters under gradient descent as well as curvature-corrected update
rules. Consequently, curvature correction only affects the speed of convergence without affecting
other qualitative properties of parameter update process.
9
Under review as a conference paper at ICLR 2020
Iteration #
Figure 3: Curvature correction effect on generalization dynamics: (A) Singular mode strength of
input-output correlation of training dataset. Dataset is generated from a rank-3 teacher network
with added noise (SNR = 10). (B, C) Training and testing loss profiles of a 3-hidden-layer student
network (See text). Note that all methods eventually converge to the identically overfitted solution
(horizontal dashed-line). (D) Time-separated learning dynamics of across singular modes. To obtain
the individual mode components, we computed the network’s input-output correlation matrix, and
projected it to the singular vector basis of the data correlation matrix.
We revealed a trade-off between map dynamics and parameter dynamics: The full curvature cor-
rection effect of natural gradient descent (NGD) completely linearizes the map learning dynamics
of deep networks, equivalent to that of shallow networks. Such complete linearization, however,
sacrifices stability of parameter update dynamics to explode when gradient vanishes and vice versa.
Moreover, We introduced √NGD that partially corrects for the effect of curvature, which facili-
tates the parameter update dynamics by eliminating the vanishing/exploding update problems. This
makes the map dynamics slightly nonlinear, but no more so than that of single hidden layer net-
works under gradient descent. Moreover, NGD makes the learning process prone to overfitting by
simultaneously learning both the signal and the noise dimensions of data, whereas √NGD partially
retains gradient descent’s resistance to overfitting by separating the time-scales between the signal
and the noise dimensions. We also showed that the widely-used block-diagonal approximation of
NGD breaches the aforementioned conservation law, resulting in highly divergent parameter up-
date dynamics, which breaks the weight balance across layers. In contrast, block-diagonalization
of √NGD preserves stability of parameter update dynamics, yielding efficient and stable learning
algorithms.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251—
276, 1998.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019.
10
Under review as a conference paper at ICLR 2020
Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using
kronecker-factored approximations. 2016.
Peter L Bartlett, David P Helmbold, and Philip M Long. Gradient descent with identity initializa-
tion efficiently learns positive-definite linear transformations by deep residual networks. Neural
computation, 31(3):477-502, 2019.
Alberto Bernacchia, Mate Lengyel, and Guillaume Hennequin. Exact natural gradient in deep linear
networks and its application to the nonlinear case. In Advances in Neural Information Processing
Systems, pp. 5941-5950, 2018.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 557-565. JMLR. org, 2017.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Simon S Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.
arXiv preprint arXiv:1901.08572, 2019.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Sys-
tems, pp. 384-395, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. .
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. In International Conference on Machine Learning, pp. 573-582, 2016.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Tom Heskes. On “natural” learning and pruning in multilayered perceptrons. Neural Computation,
12(4):881-901, 2000.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on learning theory, pp. 1246-1257, 2016.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for
recurrent neural networks. 2018.
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.
Large-scale distributed second-order optimization using kronecker-factored approximate curva-
ture for deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 12359-12367, 2019.
11
Under review as a conference paper at ICLR 2020
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
Tomaso Poggio, Qianli Liao, Brando Miranda, Andrzej Banburski, Xavier Boix, and Jack Hidary.
Theory iiib: Generalization in deep networks. arXiv preprint arXiv:1806.11379, 2018.
Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gra-
dient algorithm. In Advances in neural information processing Systems, pp. 849-856, 2008.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Ohad Shamir. Are resnets provably better than linear predictors? In Advances in neural information
processing systems, pp. 507-516, 2018.
Supplemental Materials
S.I.1 Moore-Penrose inverse solution: eq (15,16,17) in Section 3.2
In section 4, We find the Moore-Penrose inverse solution that minimizes the update norm W ∙ W
Pi Tr[WiW1] while satisfying the natural gradient constraint: (d = 2 example)
HW + ηg
■ T , ：	,、-1
w2 Q + nA = 0
(A + n∆)w∣
(S.I.1)
This constrained optimization problem is described by the following Lagrangian:
L(Wι,w2, Λ2, Λ2) = (W 1 ∙ W1 + W2 ∙ W2)/2 + Λι ∙ w2(A + nA) + Λ2 ∙ (A + n∆)w1,
where A = w2w 1 + W2Wι, and dot notation denotes inner-product: a ∙ b ≡ Tr[alb]. Optimality
condition on Wi yields
∂L∕∂W 1 = W1 + w∣w2 Λι + w∣Λ2W1 = 0	(S.I.2)
∂L∕∂W 2 = W 2 + w2Λ1w∣ + Λ2W1 w∣ = 0	(S.I.3)
which, via change of variables Λ ≡ (w2Λ1 + Λ2w1)∕n, reduces to
W1 + nw∣Λ = 0	(S.I.4)
W 2 + nΛw∣ = 0	(S.I.5)
which can be plugged into the optimality condition on Λi
∂L∕∂ Λ1 = w∣(A + nA) = 0	(S.I.6)
∂L∕∂ Λ2 = (A + nA)w∣ = 0	(S.I.7)
to produce a linear equation for Λi :
w2∣S(Λ) = S(Λ)w1∣ = 0	(S.I.8)
where S(Λ) = (w2w2∣)Λ + Λ(w1∣w1) - A.	(S.I.9)
Note that if w2, w1 are invertible, it is easy to see that eq (S.I.6)(S.I.7) reduce to exponentially
converging dynamics A + RA = w + n(w - w*) = 0, with the solution of S(Λ) = 0 driving the
parameter update eq (S.I.4)(S.I.5). This result also holds true for the over-complete cases, where the
hidden layer width is larger than the minimum of input layer or output layer size. For the under-
complete cases, i.e. with bottleneck hidden layers, the exponential convergence applies only to the
subspace dimensions permitted by the bottleneck, with the other dimensions remain frozen.
1
Under review as a conference paper at ICLR 2020
Figure S.I.1: Scaling relationships for normalized loss update L/L and weight/parameter update
speed ∣∣ιw∣∣∕√L shown across different depth d and curvature correction factor q. a is the over-
all scale factor for the weights. Dotted-lines show predictions for the case of balanced weights,
eq (S.I.16)(S.I.17). The numerical results show the result from networks of random weights sam-
pled from Gaussian distribution, (Xavier normalization (Glorot & Bengio, 2010)), They exhibit the
same power-law exponent as the dotted-lines (i.e. same slope), but shifted toward right. Block-
diagonal approximation does not affect the power-law exponents, but it affects the coefficient such
that NGD-d exhibits much larger parameter update size than NGD, while exhibiting identical zero
power-law for loss update. In contrast, sNGD-d exhibits identical match with sNGD for both pa-
rameter update and loss update. See S.I. for detailed setup. Error bars show standard deviation from
10 simulations with random weights.
S.I.2	Singular mode analysis eq (18) in Section 3.2
We follow the SVD-base analysis under the aligned singular vector condition (Saxe et al., 2013).
We introduce Qi,G,σδ,σλ,σs which represent the singular values of Wi,w,∆,Λ,S(A) of one
particular singular mode, and j ≡ ∂∂h = Q= @k In this representation, eq (S.I.4)(S.I.5) reduce to
σ i = -σΛji	(S.I.10)
whereas, eq (S.I.8)(S.I.9) reduce to
σiσS = 0
d
σS =	ji2σΛ - ησ∆
i=1
(S.I.11)
(S.I.12)
An active singular mode should have at least one non-zero σi, which according to eq (S.I.11),
implies σS = 0. Therefore, from eq (S.I.12),
σΛ
σ∆
η Pd -
i=1
which plugs into (S.I.10) to produce the result in the main text
σ∆
ρ3 j
(S.I.13)
(S.I.14)
S.I.3	Scaling laws of update dynamics
The parameter update (14) and map learning dynamics (20) of a singular mode exhibit the following
scaling relationships with respect to the coupling strength ∣j ∣:
kσ∣∣∕∣σ∆∣(x kjk1-2/q,	σ/σ∆ K -kjk2-2/q,	(S.I.15)
2
Under review as a conference paper at ICLR 2020
defined for individual singular modes under the well-aligned singular vector condition.
For the balanced weight case where the coupling strength term is constant across all singular modes,
eq (S.I.15) reduces to more generally applicable scaling relationships:
kwk∕√L (X a(dT)(I-2/q)	(S.I.16)
|L/L| X a(d-1)(2-2/q),	(S.I.17)
where α is the overall scale factor for the weight matrices. Instead of the detailed description of
individual singular mode dynamics, eq (S.I.16)(S.I.17) encapsulate the overall scaling law between
weights and update speed that can be readily measured without requiring the aligned singular vector
condition.
Under SGD (q → ∞), weight update scales as αd-1, and double the power for the loss update,
exhibiting the vanishing update problem for small α. Under sNGD (q = 1/2), weight update is
constant (zero power-law) with respect to α and loss update scales as αd-1. Under NGD (q = 1),
loss update is constant (zero power-law) with respect to α, but the weight update inversely scales as
α-(d-1), exhibiting the exploding update problem for small α.
Numerical experiments indeed confirm these scaling laws (Figure S1): For the case of balanced
weights the predictions hold exactly, and approximately for random weight matrices. Note that the
zero-power law predictions for weight update under sNGD and for loss update under NGD are exact,
because they indeed satisfy the balanced condition: the coupling strength term with zero exponent
kj k0 is indeed constant.
4	Numerical confirmation of conservation law
See Figure S2. Figure S2 plots the learning trajectory of a 3 layer network, and shows the elements
of the weight matrices evolving over time (w1,w2,w3). It also shows the conserved quantities
wιw∣ 一 w|w2, w2w∣ 一 w∣w3, which indeed remain constant for SGD, NGD and √NGD, while
it blows UP for NGD-d. √NGD also violates the conservation law, but the weights remain balanced
over time.
3
Under review as a conference paper at ICLR 2020
wɪiuɪ — wjw2
Figure 2: Training of a linear 3-layer network (D = 2).
Iteration #
4