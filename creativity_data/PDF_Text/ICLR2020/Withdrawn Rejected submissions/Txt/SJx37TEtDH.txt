Under review as a conference paper at ICLR 2020
Why Adam Beats SGD for Attention Models
Anonymous authors
Paper under double-blind review
Ab stract
While stochastic gradient descent (SGD) is still the de facto algorithm in deep
learning, adaptive methods like Adam have been observed to outperform SGD
across important tasks, such as attention models. The settings under which SGD
performs poorly in comparison to Adam are not well understood yet. In this pa-
per, we provide empirical and theoretical evidence that a heavy-tailed distribution
of the noise in stochastic gradients is a root cause of SGD’s poor performance.
Based on this observation, we study clipped variants of SGD that circumvent this
issue; we then analyze their convergence under heavy-tailed noise. Furthermore,
we develop a new adaptive coordinate-wise clipping algorithm (ACClip) tailored
to such settings. Subsequently, we show how adaptive methods like Adam can be
viewed through the lens of clipping, which helps us explain Adam’s strong perfor-
mance under heavy-tail noise settings. Finally, we show that the proposed ACClip
outperforms Adam for both BERT pretraining and finetuning tasks.
1	Introduction
Stochastic gradient descent (SGD) is the canonical algorithm for training neural networks (Rob-
bins & Monro, 1951). SGD iteratively updates model parameters in the negative gradient direction
and thus seamlessly scales to large-scale settings. Though a well-tuned SGD outperforms adaptive
methods (Wilson et al., 2017) in many traditional tasks including ImageNet classification (see Fig-
ure 1b), certain tasks necessitate the use of adaptive variants of SGD (e.g., Adagrad (Duchi et al.,
2011), Adam (Kingma & Ba, 2014), AMSGrad (Reddi et al., 2019)), which employ adaptive per-
parameter learning rates. For instance, consider the task of training an attention model (Vaswani
et al., 2017) using BERT (Devlin et al., 2018). Figure 1a shows loss curves of BERT pretraining
obtained from SGD with momentum as well as Adam. It can be seen that in spite of extensive
hyperparameter tuning, SGD converges much slower than Adam during BERT training.
The first significant hint to the performance of Adam on BERT comes from the distribution of the
stochastic gradients. For Imagenet, the norms of the mini-batch gradients are typically quite small
and well concentrated around their mean. On the other hand, the mini-batch gradient norms for
BERT take a wide range of values and are sometimes much larger than their mean value. More
formally, while the distribution of the stochastic gradients in Imagenet is well approximated by a
Gaussian, the distribution for BERT seems to be heavy-tailed.
In this work, we perform a rigorous theoretical and empirical study of the convergence of optimiza-
tion methods under such heavy-tailed noise. In this setting, some of the stochastic gradients are
much larger than the mean and can excessively influence the updates of SGD. This makes SGD
unstable and leads to its poor performance. A natural strategy to stabilize the updates is to clip the
magnitude of the stochastic gradients. We prove that indeed this is sufficient to ensure convergence
even under heavy-tailed noise. Further, we show that adaptive methods in fact implicitly have such
a clipping behavior, thereby providing a explanation for their superiority to SGD on BERT.
More specifically, we make the following main contributions:
•	We empirically show that in tasks on which Adam outperforms SGD (BERT pretraining), the
noise in stochastic gradients is heavy-tailed. On the other hand, on tasks where traditionally
SGD outperforms Adam (ImageNet training), we show that the noise is well concentrated.
•	We study the convergence of gradient methods under heavy-tailed noise condition where SGD’s
performance degrades and previous convergence proofs fail. We then establish the convergence
1
Under review as a conference paper at ICLR 2020
BERT pretraining
O 5
■ ■
3 2
O
■
2
SSo-Uo=ep=e>
0.5	1.0	1.5	2.0
Iterations	le2
ImageNet with ResNet50
5 0 5 0 5
■
3 3 2 2 1
SSO-UOAeP=BA
5
■
(a)	(b)
Figure 1: (a) Validation loss for BERTbase pretraining. Although hyperparameters for SGD momen-
tum are finetuned, a large performance gap is still observed between SGD and Adam. (b) Validation
loss for ResNet50 trained on ImageNet. SGD momentum outperforms Adam
of clipped gradient methods under the same condition and prove that they obtain theoretically
optimal rates.
•	We show that Adam implicitly performs coordinate-wise gradient clipping and can hence, unlike
SGD, tackle heavy-tailed noise. We prove that using such coordinate-wise clipping thresholds
can be significantly faster than using a single global one. This can explain the superior perfor-
mance of Adam on BERT pretraining.
•	Inspired by our theoretical analysis, we propose a novel adaptive-threshold coordinate-wise
clipping algorithm and experimentally show that it outperforms Adam on BERT training tasks.
1.1 Related work
Adaptive step sizes. Adaptive step size during optimization has long been studied (Armijo, 1966;
Polyak, 1987). More recently, Duchi et al. (2011) developed the Adagrad algorithm that benefits
from the sparsity in stochastic gradients. Inspired by Adagrad, several adaptive methods have been
proposed in the deep learning community (Tieleman & Hinton, 2012; Kingma & Ba, 2014). Re-
cently, there has been a surge in interest to study the theoretical properties of these adaptive gradient
methods due to (Reddi et al., 2019), which pointed out the non-convergence of Adam and proposed
an alternative algorithm, AMSGrad. Since then, many works studied different interesting aspects of
adaptive methods, see (Ward et al., 2018; Li & Orabona, 2018; Zhou et al., 2018a; Staib et al., 2019;
Chen et al., 2018; Zou & Shen, 2018; Zhou et al., 2018b; Agarwal et al., 2018; Zou et al., 2019).
Another direction of related work is normalized gradient descent, which has been studied for quasi-
convex and non-convex settings (Levy, 2016; Hazan et al., 2015; Zhang et al., 2019). In contrast
to our work, these prior works assume standard noise distributions that might not be applicable to
key modern applications such as attention models, which exhibit heavy-tailed noise. Furthermore,
mostly, the convergence rates of adaptive methods are worse than SGD.
Noise in neural network. There has been little study of the actual stochastic gradient noise distribu-
tions in neural network training. To our knowledge, Simsekli et al. (2019) is the first work to focus
on this topic and observe heavy tailed noise in network training. Our work differs in two important
ways: first, we treat the noise as a high dimensional vector, while (Simsekli et al., 2019) treat devia-
tions in each coordinate as scaler noises to estimate tail index. Second, we focus on convergence of
optimization algorithm, the original paper focus on Langevin dynamics and consistence of the index
estimator. More experiment comparison is in Appendix H.
2 Heavy-tailed noise in stochastic gradients
To gain intuition about the difference between SGD and Adam, we start our discussion with the study
of noise distributions of stochastic gradient that arise during neural network training. In particular,
we focus on noise distributions while training two popular deep learning models — BERT and
ResNet. Note that BERT and ResNet are typically trained with Adam and SGD (with momentum)
respectively and can thus, provide insights about difference between these optimizers.
2
Under review as a conference paper at ICLR 2020

X4'su9α
3 2 1
Ooo
■ ■ ■
Ooo
X4uα
0.112
1.2
1.4	1.6
Gaussian
ωucπ≡π> pωleEcs山
(a) ImageNet training
(b) Synthetic Gaussian
.^Su9α
而
0.110 ∖h___________________
° i . 2 1一
Sample size le7
(c) ImageNet emp. variance
8cπEπ> PBleEqSW
(d) Bert pretraining (e) Synthetic Levy-stable (f) Bert empirical variance
Figure 2: (a) Histogram of sampled gradient noise for ResNet50 using Imagenet dataset. (b) His-
togram of samples from a sum of squared Gaussians. (c) Estimated variance of the stochastic gra-
dient for Resnet50. (d) Histogram of sampled gradient nosie for BERT using Wikipedia+Books
dataset. (e) Histogram of samples from a sum of squared α-stable random variables. (f) Estimated
variance of the stochastic gradient for BERT model.
We first investigate the distribution of the gradient noise norm ∣∣g - Vf (x)k in the aforementioned
neural network models, where g is the stochastic gradient computed from a minibatch sample. Fig-
ure 7 (a) and (d) show these distributions for ResNet50 on ImageNet and BERT on the Wikipedia
and books dataset at model initialization respectively. For comparison, we plot distributions of a
normalized sum of squared Gaussians, a well-concentrated distribution, and a Levy-α-stable distri-
bution, a heavy-tailed distribution, in Figure 7 (b) and (e) respectively. We observe that the noise
distribution for BERT appears heavy-tailed, while that of ResNet50 is well-concentrated.
To support this observation, in Figure 7 (c) and (f) we further show the empirical variance of stochas-
tic gradients with respect to the sample size used in the estimation. The results highlight that while
the corresponding estimator converges for Imagenet, the empirical variance does not converge in
BERT training even as the sample size approaches 107.
Figure 7 clearly shows that while the noise pattern is well-concentrated in some cases, it is also
heavy tailed in other settings. We hypothesize that this is one major aspect that determines the
performance of SGD and adaptive methods. More specifically, we argue and provide evidence that
adaptive methods can be faster than SGD in scenarios where heavy-tailed noise distributions arise.
More empirical details can be found in Section 5.
2.	1 Optimization problem setup
We now formalize the optimization setup with heavy-tailed noise. Neural network training can be
seen as minimizing a differentiable stochastic function f(x) = Eξ [f (x, ξ)], where f : Rd → R can
be potentially nonconvex. At each iteration, we assume access to an unbiased stochastic gradient
g(x) = Vf (x, ξ) corresponding to the parameters x, and also assume finite α moment.
Assumption 1 (Existence of α-moment). There exists positive real numbers α ∈ (1, 2] andG > 0
such that for all x, E[∣g(x)∣α] ≤ Gα.
Note that it is possible that the variance of g(x) is unbounded while simultaneously satisfying the
above assumption for α < 2, e.g. the Pareto distribution. Hence Assumption 1 is much weaker
than the standard bounded second moment assumption. The possibility that the variance could be
unbounded has a profound impact on the optimization process. In particular, previous convergence
proofs for SGD fail and the algorithm may diverge (see Appendix A). As we will illustrate over the
next few sections, using a biased clipped stochastic gradient estimator allows us to circumvent the
problem of unbounded variance, while ensuring convergence even under heavy-tailed noise.
3
Under review as a conference paper at ICLR 2020
3 Clipped gradient descent and ACClip algorithm
Our key idea is that by clipping the stochastic gradient, we can effectively control its variance. While
this does introduce some additional bias, the clipping threshold can be (possibly adaptively) tuned
to trade-off variance with the introduced bias.
Empirically, this is a prevalent practice e.g. Merity et al. (2018) use clipped SGD to train AWD-
LSTM whereas Devlin et al. (2018) use clipped Adam to pretrain BERT. The general form of clipped
gradient descent is outlined in Algorithm 1.
Algorithm 1 A general framework for clipped gradient descent
1:	x, mk J xo, 0
2:	for k = 1, ∙, T do
3:	mk J β1mk-1 + (1 - β1)gk
4：	gk J CliP(Tk ,mk)
5：	χk J χk-ι - ηkgk
return XK, where random variable K is supported on {1,…，T}.
PerhaPs the two most natural candidates for cliPPing mechanisms are Global CLIPPing where a
single global threshold is used and Coordinate-wise CLIPping using d coordinate-wise thresholds
GCliP(Tk,mk) = min{k¾, l}mk , for Tk ∈ R≥o or,	(GCliP)
CClip(τk,mk) = min{PmkJ, l}mk , for Tk ∈ R≥o .	(CClip)
where all oPerations for CCliP are Performed element-wise. Smaller threshold values of Tk imPly
smaller variance, but also higher bias. If the noise distribution is very heavy-tailed (or has a large
variance), one could compensate by picking a smaller threshold. See Theorems 1 and 2 for a theo-
retical treatment of this for GClip. While GClip preserves the update direction and only scales its
magnitude, CClip scales each coordinate individually and may not preserve the direction. However,
if the noise distribution varies significantly across coordinates, CClip can take advantage of this and
only clip those which are more heavy-tailed and consequently converge faster (see Theorem 3).
However, to perform such a coordinate-wise variance-bias trade off optimally would require tuning
all d thresholds, which could be extremely large in deep learning. Even if tuning were feasible, the
noise distribution may be non-stationary and significantly vary as training progresses (see Fig. 5).
To address this challenge, we propose Adaptive Coordinate-wise CLIPping, which adaptively sets
the thresholds for each coordinate
ACCliP(Tk,mk) = min{扁+e,l}mk, Ta = β2Ta-I + (1- β2)∣gk∣α,	(ACClip)
where operations are element-wise, is a small number for numerical stability, and we set α = 1
by default. The use of a coordinate-wise α-moment (as opposed to the traditional second moment)
is motivated by our theory and estimates B = E[|gka|]1/a in Assumption 2. Since E[|gka|]1/a is
increasing in α ≥ 1, using α = 1 is a conservative bound on B when the true α is unknown.
3.1	Adam as an adaptive clipping method
We study Adam without momentum, i.e. RMSProp, and show that it resembles coordinate-wise
clipping. When β1 = 0, we can rewrite the update for Adam and ACClip as SGD with effective
step-sizes hAdam and hclip respectively
xk+1 = xk - e+√β2vk+(i-β2^∖gk∖2gk =： Xk - hAdamgk , and
xk+1 = xk - ηk min{ ∖gk∖, 1 }gk =: xk - hclipgk .
Given any set of parameters for RMSProp, if we set the parameters for ACClip as
ηk = -2^ and Tk = e+√β2Vk,
k ∈+ β'βwk	k	l-β2 ,
then 2 雇厢 ≤ hAdam ≤ 2hclip. Thus, the two differ by at most af factor of 2 and Adam can be seen as
ACClip where Tk is set using √vk, which estimates E[∣gk∣2]1/2, and a correspondingly decreasing
4
Under review as a conference paper at ICLR 2020
Table 1: Error bounds after k iterations: Define α-moment E[kg(x)kα] ≤ Gα (Assump 1) and
coordinate-wise moments E[∣g(x)∣α] ≤ Ba (AssUmP 2), which satisfy G2 ≤ d∣∣B∣∣α. In the Stan-
dard setting (α = 2), GClip recovers the optimal rates of SGD. For heavy-tailed noise (α ∈ (1, 2)),
GCliP converges both for convex (Thm 1) and non-convex fUnctions (Thm 2), whereas the Proof for
SGD fails, denoted as N/A. Under a more fine-grained noise model, CCliP has better convergence
rates (Thm 3). We also show matching lower-boUnds for all α ∈ (1, 2] Proving the oPtimality of
cliPPing methods (Thm 4).
	Strongly Convex FUnction		Non-Convex FUnction	
	Heavy-tailed noise (α ∈ (1, 2))	Standard noise (α ≥ 2)	Heavy-tailed noise (α∈ (1, 2))	Standard noise (α ≥ 2)
SGD	N/A	θ(G2k-1)	N/A	O(GkT)
GClip	θ(G2k ⅛1))	θ(G2k-1)	2	2 a	—2( a — 1)、 O ( G 30-2 k 3a-2	)	O(Gk-2)
CClip	θ(kBk2k -⅛a )	θ(kBk2k-1)	O(IIBIl 户 k -⅛u)	θ(kB∣2k-2)
Lower-boUnd	Ω(k -⅛-i)	Ω(k-1)	?	?
steP-size. This cliPPing effect can exPlain the faster convergence of Adam for training attention
models in the Presence of heavy-tailed noise (Fig. 1a). In contrast to Adam, ACCliP (with α = 1)
Uses a simPler UPdate scheme where τk is set Using an estimate of E[|gk|] and a constant steP-size.
This allows ACCliP to take larger stePs than Adam and hence converge slightly faster (Section 5).
4	Theoretical analysis
In this section, we address the following three key Points. First, we show how gradient cliPPing
ensUres convergence for both convex and non-convex fUnctions Under heavy-tailed noise. Then, we
stUdy when and why coordinate-wise cliPPing (CCliP) oUtPerforms global cliPPing (GCliP). Finally,
we show that (UPto nUmerical constants) cliPPing is an oPtimal strategy Under the strongly convex
setting. Together, these Provide a comPrehensive analysis of the effect of cliPPing and a strong
theoretical motivation for Using it whenever one sUsPects the Presence of noise with UnboUnded
variance. OUr resUlts are sUmmarized in Table 1, and all Proofs are relegated to the APPendices.
4.1	Convergence of Globally Clipped SGD
We state the rates for strongly-convex fUnctions and for smooth non-convex fUnctions. We focUs on
GCliP with momentUm β = 0, Under the assUmPtion 1. In the strongly convex case, we can Prove
the following rates of convergence to the oPtimUm.
Theorem 1 (Strongly-Convex convergence). Suppose that f is a μ-strongly convex function and
the noise satisfies assumption 1. Then let {xk } be the iterates of Algorithm 1 using GClip, clip-
Ping parameter Tk = Gka-I and steps-size ηk = *(二).Define the output to be a j-weighted
combination of the iterates: Xk = Pk=ι jxj-ι/(Pk=I j). Then the output Xk satisfies:
16G2
Ef(Xk)] 一 f(X ) ≤ μ(k +1)2(a-1)∕a .
Observe that when α = 2 and with boUnded second moment, we recover exactly the oPtimal SGD
rate of O(G2∕μk) of LacoSte-JUlien et al. (2012). For any other α ∈ (1,2), the performance
of GCliP gracefUlly degrades. As we will later show, this rate is in fact oPtimal for every α ∈ (1, 2].
In contrast, SGD can be arbitrarily bad when α < 2 (see Appendix A). We can also prove rates of
convergence to a stationary point for smooth non-convex fUnctions.
Theorem 2 (Non-convex convergence). Suppose that f is a possible non-convex L-smooth function
and the noise satisfies assumption 1. Then let {Xk} be the iterates of Algorithm 1 using GClip
f (x∩ )— f ? QCao T 2α-2
with momentum β = 0, constant step-size ηk = [(22(%+[))	2 /L3α-2J and constant clipping
parameter Tk = G∕(ηL) 1. Then, the SeqUenCe {xk} satisfies
5
Under review as a conference paper at ICLR 2020
1K
IK 斗[kVf(Xk-1)『]≤
4G 3≡ (W -f(x?)
2a-2
3a-2
The rates of convergence for the non-convex case in Theorem 2 exactly match those of the usual
SGD rates of O(1/√k) when α = 2 and gracefully degrades for α ∈ (1, 2]. Thus, GClip converges
for both convex and non-convex functions under Assumption 1. However, Assumption 1 is quite
crude and summarizes noise into a single number G. The noise present in the different gradient
coordinates can be quite varied, and as we next show, CClip can take advantage of this and clip each
coordinate independently. This allows CClip (when tuned properly) to have a significantly better
convergence rate when compared to GClip.
4.2	B enefits of Coordinate-wise Clipping
We had previously assumed a global bound on the α-moment of the norm of the stochastic gradient
is bounded by G. However, G might be hiding some dimension dependence d. We next study a
more fine-grained model of the noise in order to tease out this dependence.
Assumption 2 (Coordinate-wise α moment). Denote {gi(x)} to be the coordinate-wise gradients
for i ∈ [d]. We assume there exist constants {Bi} ≥ 0 and α ∈ (1, 2] such that E[∣gi(x)∣α] ≤ Ba .
For the sake of convenience, We denote B = [Bi； B2;…；Bd] ∈ Rd, |田||。= (P Ba)1/a. Under
this more refined assumption, we can show the following corollary:
Corollary 1 (GClip under coordinate-wise noise). Suppose we run GClip under the Assumption
of 2 to obtain the Sequence {xk}. Then, if f is μ-strongly convex, with appropriate step-sizes and
averaging, the output Xk satisfies
E[f (Xk)] - f (x?) ≤ ' 16叫BHa)/ .
k	μ(k + 1)2(a-1"a
Thus, the convergence of GClip can have a pretty strong dependence on d, which for large-scale
problems might be problematic. We show next that using coordinate-wise clipping avoids this issue.
Theorem 3 (CClip under coordinate-wise noise). Suppose we run CClip under the Assumption
of2 with Tk = Bka-I to obtain the SeqUence {xk}. Then, if f is μ-strongly convex, with appropriate
step-sizes and averaging, the output Xk satisfies
Ef(Xk)]-f (XD ≤ μ(k+*∕a .
Note that kB k2 ≤ kB ka . Hence CClip has a worst-case convergence independent of d under
the coordinate-wise noise model. Similar comparison between GClip and CClip can be done for
non-convex conditions too (see Thms 5 and 3 in Appendix F). This improvement has two sources:
coordinate-wise clipping can adapt to different Bi in each coordinate; the global L2 clipping does
not align with the geometry of coordinate-wise noise. Although we only compare upper-bounds
here, when the noise across coordinates is independent the upper bounds here may be tight (see
Lemma 4). Further, our experiments where d n (over-parameterized models) also empirically
confirms superior performance of coordinate-wise clipping.
Given that CClip may be better than GClip, we next address the natural question of whether there
exists some other strategy (which may perhaps not involve clipping at all) which is even better.
4.3	An information theoretic lower bound
We show a strong lower-bound for a simple class of one-dimensional quadratic functions with
stochastic gradients satisfying E[|g(X)|a] ≤ 1. This matches the upper bounds of Theorems 1
and 3 for strongly-convex functions, showing that the simple clipping mechanism of Algorithm 1 is
(up to constants) information theoretically optimal, providing a strong justification for its use.
Theorem 4. For any α ∈ (1, 2] and any (possibly randomized) algorithm A, there exists a problem
f which is 1-strongly COnVex and 1-smooth (μ = 1 and L = 1), and stochastic gradients which
satisfy Assumptions 1 and 2 with G, kB k ≤ 1 such that the output Xk of the algorithm A after
processing k stochastic gradients has an error
E[f (Xk)] - f (X?) ≥ ω( k2(a-1)∕a ).
6
Under review as a conference paper at ICLR 2020
Figure 3: Validation loss for BERTbase pre-
training with the sequence length of 128.
While there remains a large gap between
non-adaptive methods and adaptive meth-
ods, clipped SGD momentum achieves faster
convergence compared to standard SGD
momentum. The proposed algorithm for
adaptive coordinate-wise clipping (ACClip)
achieves a lower loss than Adam.
Table 2: BERT pretraining: Adam vs ACClip. Compared to Adam, the proposed ACClip algo-
rithm achieves better evaluation loss and Masked LM accuracy for all model sizes.
	BERT Base 6 layers		BERT Base 12 layers		BERT Large 24 layers	
	Val. loss	Accuracy	Val. loss	Accuracy	Val. loss	Accuracy
Adam	1.907	63.45	1.718	66.44	1.432	70.56
ACClip	1.877	63.85	1.615	67.16	1.413	70.97
5	Experiments
We perform extensive evaluations of ACClip on BERT pre-training and fine-tuning tasks and demon-
strate its advantage over Adam in Section 5.1. Since ACClip was explicitly designed to tackle heavy-
tailed noise, this provides further evidence that Adam owes its efficacy to its gradient clipping effect.
For completeness, an experiment on ImageNet is included in Appendix I. Then, in Section 5.2 we
study the cause of the heavy-tailedness and show that both the architecture and the data play a role.
5.1	Performance of ACClip for BERT pre-training and fine-tuning
We evaluate the empirical performance of our proposed ACClip algorithm on BERT pre-training as
well fine-tuning using the SQUAD v1.1 dataset. As a baseline, we use Adam optimizer and the same
training setup as in the BERT paper (Devlin et al., 2018). For ACClip, we set lr = 1 × 10-4, β1 =
0.9, β2 = 0.99, = 1 × 10-5 and wd = 1 × 10-5. We compare both setups on BERT models of
three different sizes, BERTbase with 6 and 12 layers as well as BERTlarge with 24 layers.
Figure 3 shows the validation loss for pretraining BERTbase using SGD with momentum, GClip,
Adam and ACClip. The learning rates and hyperparameters for each method have been extensively
tuned to provide best performance on validation set. However, even after extensive tuning, there
remains a large gap between (clipped) SGD momentum and adaptive methods. Furthermore, clipped
SGD achieves faster convergence as well as lower final loss compared to standard SGD. Lastly, the
proposed optimizer ACClip achieves a lower loss than the Adam. Table 2 further shows that ACClip
achieves lower loss and higher masked-LM accuracy for all model sizes.
Next, we evaluate ACClip on the SQUAD v1.1 fine-tuning task. We again follow the procedure
outlined in (Devlin et al., 2018) and present the results on the Dev set in Table 3. Both for F1 as well
as for exact match, the proposed algorithm outperforms Adam on all model sizes.
The experimental results on BERT pretraining and fine-tuning indicate the effectiveness of the pro-
posed algorithm. The increasing clipping threshold during the training is motivated by our theoreti-
cal analysis an indeed helps with empirical performance by reducing the bias in gradient estimators.
5.2	Noise Patterns in Neural Network Training
In the following experiment, we aim to understand the effect of model architecture and training data
on the shape of gradient noise, and to understand how this shape evolves during training.
To address the first question, we measure the noise distribution in an Attention and a ResNet-like
model on both Wikipedia and synthetic Gaussian data. We used BERTbase model as the Attention
model, and the ResNet is constructed by removing the self-attention modules in the transformer
blocks. Gaussian synthetic data is generated by replacing the token embedding layer with nor-
malized Gaussian input. The resulting noise histograms are plotted in Figure 4. From the figure,
7
Under review as a conference paper at ICLR 2020
Table 3: SQUAD v1.1 dev set: Adam vs ACClip. The mean and standard deviation of F1 and
exact match score for 5 runs. The first row contains results reported from the original BERT paper,
which are obtained by picking the best ones out of 10 repeated experiments.
	BERT Base 6 layers		BERT Base 12 layers		BERT Large 24 layers	
	EM	F1	EM	F1	EM	F1
Adam (Devlin et al., 2018)			80.8	88.5	84.1	90.9
Adam	76.85 ± 0.34	84.79 ± 0.33	81.42 ± 0.16	88.61 ± 0.11	83.94 ± 0.19	90.87 ± 0.12
ACClip	78.07 ± 0.24	85.87 ± 0.13	81.62 ± 0.18	88.82 ± 0.10	84.93 ± 0.29	91.40 ± 0.15
0.00
(a) Attention + Wikipedia
_ ___
X4"u8α
25	30
Noise
35
(b) Attention + Gaussian
0.00
X4≡ulυα
3 2 10
Oooo
■ ■ ■ ■
Oooo
X4uα
(d) Resnet + Gaussian.
0.27 0.28 0.29 0.30 0.31
Noise norm
Figure 4: Noise distribution in Attention and ResNet on two data sources: Wikipedia and synthetic
Gaussian. The noise pattern is from the interaction of both architecture and data distribution.
heavy-tailed noises are observed in the Attention model independently of the input data. For the
ResNet model, we see Gaussian input leads to Gaussian noise while Wikipedia data makes the noise
to be heavy-tailed. Therefore, the noise pattern results from both the model architecture as well as
the data distribution.
Figure 5 shows how the distribution of gradient noise is evolving during training. The results high-
light that the noise distribution is non-stationary during BERT training as the noise is becoming
increasingly more concentrated. In contrast, for the ResNet model on ImageNet, the shape of the
noise distribution remains almost unchanged. This result supports the use of exponential moving
average as an estimator for the underlying non-stationary distribution to accelerate optimization.
0.050
0.025
0.000
Iteration Ok
Noise norm
4k
Noise norm
Noise norm
(a) Development of noise distribution during BERT training.
NoiSe norm	Noise norm	Noise norm	NoiSe norm
(b) Development of noise distribution during ResNet50 training on ImageNet.

Figure 5: The distribution of gradient noise is non-stationary during BERT training, while it remains
almost unchanged for ResNet training on ImageNet.
6	Conclusion
Our work theoretically and empirically ties the advantage of adaptive methods over SGD to the
heavy-tailed nature of gradient noise. A careful analysis of the noise and its impact yielded several
insights: that adaptive methods implicitly perform gradient clipping, that clipping is an excellent
strategy to deal with heavy-tailed noise, and that the adaptive coordinate-wise clipping yields state
of the art performance for training attention models. Our results add to a growing body of work
which demonstrate the importance of the structure of the noise in understanding neural network
training. We believe additional such investigations into the source of the heavy tailed-ness, as well
as a characterization of the noise in other scenarios where Adam is used (e.g. GAN training or
reinforcement learning) can lead to further insights with significant impact on practice.
8
Under review as a conference paper at ICLR 2020
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
The case for full-matrix adaptive regularization. arXiv preprint arXiv:1806.02958, 2018.
Larry Armijo. Minimization of functions having Lipschitz continuous first partial derivatives. Pacific
Journal of mathematics ,16(1):1-3, 1966.
Sebastien Bubeck, Nicolo Cesa-Bianchi, and Gabor Lugosi. Bandits with heavy tail. IEEE Trans-
actions on Information Theory, 59(11):7711-7717, 2013.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex opti-
mization. In Advances in Neural Information Processing Systems, pp. 1594-1602, 2015.
Diederik P Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining
an o (1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint
arXiv:1212.2002, 2012.
Kfir Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint
arXiv:1611.04831, 2016.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM
language models. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=SyyGPP0TZ.
Abhishek Panigrahi, Raghav Somani, Navin Goyal, and Praneeth Netrapalli. Non-gaussianity of
stochastic gradient noise. arXiv preprint arXiv:1910.09626, 2019.
Boris T Polyak. Introduction to optimization. optimization software. Inc., Publications Division,
New York, 1, 1987.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of ADAM and beyond. arXiv
preprint arXiv:1904.09237, 2019.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,
22:400-407, 1951.
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient
noise in deep neural networks. arXiv preprint arXiv:1901.06053, 2019.
Matthew Staib, Sashank J Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra. Escaping saddle points
with adaptive gradient methods. arXiv preprint arXiv:1901.09149, 2019.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
LukaSz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
9
Under review as a conference paper at ICLR 2020
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Analysis of gradient clipping and
adaptive scaling with a relaxed smoothness condition. arXiv preprint arXiv:1905.11881, 2019.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of
adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018a.
Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu.
Adashift: Decorrelation and convergence of adaptive learning rate methods. arXiv preprint
arXiv:1810.00143, 2018b.
Fangyu Zou and Li Shen. On the convergence of weighted adagrad with momentum for training
deep neural networks. arXiv preprint arXiv:1808.03408, 2018.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for conver-
gences of adam and rmsprop. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 11127-11135, 2019.
10
Under review as a conference paper at ICLR 2020
A	Necessity of adaptivity and non-convergence of SGD
Consider a simple toy function f (x) = 2∣∣x - a∣∣2 with an optimum value of 0 achieved at
x? = a. Suppose that we receive stochastic gradients g(x) with potentially unbounded variance
i.e. E[∣g(x) -Vf(x)k2] = ∞. Then, starting from x0 = 0, let us run SGD (possibly with momen-
tum) using with arbitrary but fixed step-sizes i.e. the step-sizes are predetermined and do not depend
on the stochastic gradients. The output xk is then simply a linear combination of {g(xj )}j≤k with
some (fixed) weights {ηj ∈ R}j≤k
k
xk =	ηjg(xj) .
j=1
The the expected function suboptimality at xk can be lower bounded as follows
E[f(xk)] -	f(a) =	E[∣xk -	a∣2] ≥	E[∣xk -	E[xk]∣2] = EX ηj (g(xj)	- E[g(xj))]	.
The last term in the above expression is the variance of sum of k independent random variables and
ηj are fixed constants. We can then simplify as
k 2 k
E Xηj(g(xj) - E[g(xj))]	= X ηj2E(g(xj) - E[g(xj))]
j=1	j=1
≥ (Pik=0 ηi2) maxj∈[k] E[∣g(xj) - Vf(xj)]∣2] .
This implies that if there is any i, j ≤ k such that ηi2 > 0 and E[∣g(xj) - Vf (xj)]∣2 = ∞, we
have E[f (xk)] = ∞! Thus, the solution provided by SGD can be arbitrarily bad when the domain
is unbounded.
It was important in our proof above that the step-sizes are fixed before-hand and are independent of
the stochastic gradients. Notably, both gradient clipping and adaptive methods do not satisfy this
property, and can indeed tackle unbounded variance. In some sense, the above example establishes
adaptivity as being necessary when dealing with heavy-tailed noise.
B Additional definitions and notation
Here we describe some of the formal definitions which were previously skipped.
Definition 3 (μ-strong-convexity). f is μ-strongly convex, if there exist positive constants μ such
that ∀x, y,
f(y) ≥ f(x) + hVf(x),y - Xi + μky - x∣2
The strong convexity assumption and the bounded gradient assumption implicitly implies that the
domain is bounded. However, the domain diameter is not used explicitly in the proof. The projected
clipped gradient descent follows exactly the same argument and the fact that orthogonal projections
contract distances.
Definition 4 (L-smoothness). f is L-smooth, if there exist positive constants L such that ∀x, y,
f (y) ≤ f (χ) + Nf(X), y - Xi + Lky - χ∣2.
Note that the above definitions together imply that μ ≤ L.
C	Effect of global clipping on variance and bias
We focus on (GClip) under stochastic gradients which satisfy Assumption 1.
Lemma 2. For any g(x) suppose that assumption 1 holds with α ∈ (1,2]. Then the estimator g
from (GClip) with global clipping parameter τ ≥ 0 satisfies:
E[kg(x)k2] ≤ Gατ2-ɑ and ∣E[g(x)] - Vf (x)∣2 ≤ G2ατ-2(α-1).
11
Under review as a conference paper at ICLR 2020
Proof. First, we bound the variance.
E[kg(x)k2] = E[kg(x)kαk^(x)k2-α]
By the fact that g(χ) ≤ T, We get
E[kg(x)k2] = E[kg(x)kɑτ2-α] ≤ Gατ2-α.
Next, We bound the bias,
kE[g(x)] -Vf (x)k = kE[g(x) - g(x)]k
≤ - - g(X)k] = E[kg(X) - g(X)k1{∣g(χ)∣≥τ}]
≤ E[kg(x)k1{∣g(x)∣≥τ}] ≤ E[kg(x)kα]τTaT).
The last inequality follows by Markov inequality.	□
As We increase the clipping parameter τ, note that the variance (the first term in Lemma 2) increases
while the bias (which is the second term) decreases. This way, we can carefully trade-off the variance
of our estimator against its bias, thereby ensuring convergence of the algorithm.
D Strongly-Convex Rates (Proof of Theorem 1)
For simplicity, we denote gk = ηkg(xk) and the bias bk = E[gk] 一 Vf (xk).
IlXk ― x*k2 = IlXk-I ― ηkgk-1 ― x*k2
=IlXk-I ― X*k2 ― 2ηk(Xk-1 ― X*, Vf(Xk-1)
―2ηk(Xk-1 ― X*,bk-1 i + η2kgk-ik2
≤ (I 一 μηk )IlXk-I- x*∣∣2 一 2ηk (f (Xk-I)- f *))
+ 2ηk ( μ kXk-1 - X*『+	∣∣bk k2 )+ η2kgk-1k2.
4	μ
Rearrange and we get
f (Xk-I) - f * ≤	— ~~9	-	IlXk-I- x*ii2	-	ηk- IlXk-	x*ii2	+ - -Iibk k2 + η Ilgk-1k2.
2	2	μ	2
After taking expectation and apply the inequality from Lemma 2, we get
E[f(Xk-ι)] - f* ≤ E ηk1 -μ/2IXk-1 -X*I2- η-1 l∣Xk-X*I2
+ 4G2ατ2-2αμT + ηk Gaτ2-a∕2.
Then take ηk = .1.,Tk = Gk1 and multiply both side by k, we get
kE[f(Xk-1)] - f* ≤ AE[k(k - 1)11^-1 - X*『-k(k + 1)∣IXk - X*『]
8
+ 8G2k 2-α μ-1.
Notice that PK=I kT ≤ R0K+1 k20α dk ≤ (K + 1)2/a. SUm over k and we get
PK=1 kE[f(Xk-i)] - f* ≤ μE[-T(T +1)Ixt - X*I2]
8
+ 8G2(K + 1)2 μ-1.
Devide both side by K(K+1) and we get
^7Γ2	PNI kE[f(Xk-i)] - f* ≤ 8G2KT(K +1)宁μ-1.
K (K + 1)
Notice that for K ≥ 1, K-1 ≤ 2(K + 1)-1. We have
2	PK=I kE[f(xk-ι)] - f* ≤ 16G2(K +1)2-2αμ-1.
K (K + 1)
The theorem then follows by Jensen’s inequality.
□
12
Under review as a conference paper at ICLR 2020
E	Non-convex Rates (Proof of Theorem 2)
For simplicity, We denote gk = g(xk-ι) and the bias bk = E[^k] - Vf (Xk). By Assumption 4, We
have
η2L
f (xk) ≤ f (Xk-I) + hVf (Xk-I ), -ηkgki +-2~ Ilgk k
≤ f(xk-l) - ηk IlVf(Xk-1)k2 - ηkhVf(Xk-1),bk i + k^k kgk k2
≤ f(xk-l) - ηk IlVf(Xk-1)∣∣2 - nkhV f (Xk-I),bk i + k^k kgk k2
≤ f(Xk-1)-nk IlVf(Xk-1)∣∣2 + 今 IlVf(Xk-1)∣∣2 + ηk kbk ∣∣2 + η∣- kJk ∣∣2 .
Here the last step used the AM-GM inequality. Then, taking expectation in both sides and using
Lemma 2 gives
f∏" 、i i η ∖	ηk ip" ^∣∣2 , η2LGατ2-α , ηkG2α
E[f (Xk )|Xk-1] ≤ f (Xk-I) - -2 IlVf (Xk-I)Il +	2-------+ 2τ 2a-2 .
Rearrange and sum the terms above for some fixed step-size {ηk = η} and threshold {τk = τ} to
get
1 K	2	G2α
K ∑E[IVf(Xk-1)I2] ≤ ηκ(f(X0) - E[f(xk)]) + ηLGατ2-α + 720-2
2
η ηK
(f (X0) - f?) + ηLGατ2-α
{z
T1
*{z
T2
G2a
+ T 2α-2 .
} |
Since we use a threshold T = ―Gɪ, we can simplify T2 as
(nL)α
2α	2α
ηLGατ2-α + J = % = 2G2
T 2α-2	T 2α-2
2a-2
(ηL) ɪ .
Denote f0 = f(X0) - f? to ease notation. Then, adding T2 back to T1 and using a step-size
η = [(Gfκ) 3α-2/L3α-22] we get
Ti + T = ηK0 + 2G2(ηL) 2α-2
This proves the statement of the theorem.
4fo
ηκ
2α-2
『(Lfo ∖ 3α-2
4G 3α-2
K
□
F Effect of coordinate-wise moment b ound
We now examine how the rates would change if we replace Assumption 1 with Assumption 2.
F.1 Convergence of GClip (proof of Corollary 1)
We now look at(GClip) under assumption 2. We can in fact also prove the following corollary for
the non-convex case:
Corollary 3 (Non-convex GClip under coordinate-wise noise). Suppose we run GClip under the
Assumption of 1 to obtain the sequence {Xk}. Then, if f is L-smooth and possibly non-convex, with
appropriate step-sizes and averaging,
2α-2
ɪ X E[IVf (Xk-1)I2] ≤ 4(√d∣∣B∣∣α)3α-2 (f(X0)-f(x*))3α-2
KK
k=1
13
Under review as a conference paper at ICLR 2020
The proof of both the convex and non-convex rates following directly from the following Lemma.
Lemma 4. For any g(x) suppose that assumption 2 with α ∈ (1, 2]. Then suppose we have a
constant upper-bound
E[kg(x)kα] ≤ D.
Then D satisfies
d2TkBka ≤ D ≤ dαr2kBkα.
Proof. Note that the function (∙)α/2 is concave for α ∈ (1, 2]. Using Jensen's inequality We can
rewrite as:
[/ d	∖ a/2"	d d	-
(d X ∣g(x)(i)∣2)	≥ da/2TE X ∣g(x)(i)∣a .
Since the right hand-side can be as large as d2-1 ∣∣B∣∣α, We have our first inequality. On the other
hand, We also have an upper bound beloW:
E[kg(x)kα] = E [(X |g(x)(i)l2!	≤ E (d(mlXg(X)⑴)2)a/2
dd
≤ E[da/2(mdixg(x)(i))a] ≤ E da/2 X(g(x)⑴)a ≤ da/2 X Ba
i=1	i=1	i=1
Where kB kaa = Pid=1 Bia . Thus, We have shoWn that
d2TkBka ≤ E[kg(x)ka] ≤ da/2kBka .
We knoW that Jensen’s inequality is tight When all the co-ordinates have equal values. This means
that if the noise across the coordinates is linearly correlated the loWer bound is tighter, Whereas the
upper bound is tighter if the coordinates depend upon each other in a more complicated manner or
are independent of each other.	□
Substituting this bound on G in Theorems 1 and 2 gives us our corollaries.
F.2 Convergence of CClip (Proof of Theorem 3)
The proof relies on the key lemma Which captures the bias-variance trade off under the neW noise-
assumption and coordinate-Wise clipping.
Lemma 5. For any g(x) suppose that assumption 2 with α ∈ (1, 2] holds. Denote gi to be ith
component of g(x), Vf (x)i to be ith component of Vf (x). Then the estimator g(x) = [^ι;… ;gd]
from (CClip) with clipping parameter T = [τι; τ2; •一;Td] satisfies:
E[k^ik2] ≤ BaTi2-a and kE[gi] - Vf (x)ik2 ≤ ByTS ∙
Proof. Apply Lemma 2 to the one dimensional case in each coordinate.	□
ProofofTheorem 3. Theorem 1 For simplicity, we denote gk = ηk^(xk) and the bias bk = E[^k] 一
Vf(xk).
Ilxk ― x*k2 = Ilxk-I ― ηkgk-1 ― x*k2
=Ilxk-I ― x*k2 ― 2ηkhxk-1 ― x*, Vf (xk-i)i
―2ηkhxk-1 ― x*,bk-ii + η2kgk-ik2
≤ (1 一 μηk )∣xk-1- x*∣∣2 一 2ηk (f (Xk-I)- f *))
+ 2ηk (ɪ IIxk-I- x*∣∣2 +	Ilbk k2) + η2∣∣gk-ι∣∣2.
4	μ
14
Under review as a conference paper at ICLR 2020
Rearrange and we get
f (Xk-I) - f * ≤ ηk~~9 μ~ llxk-1 - x*k2 - ηk- Ilxk - x*k2 + ■—Ilbk Il2 + ηk llgk-1k2.
2	2	μ	2
After taking expectation and apply the inequality from Lemma 2, we get
E[f(xk-ι)] - f* ≤ e[η-1 - μ/2 Ixk-ι -x*k2 -工Ixk -x*k2
+ Pd=I 4B2ɑTi2-2αμT + ηkGατ2-α∕2.
Then take ηk = .法]),τ = Bik 1 and multiply both side by k, We get
kE[f (xk-1)] - f * ≤ 7^E[k(k - I)IIxk-I- x*『-k(k + I)Ilxk - x*∣∣2]
8
+ 8 PLI Bik2-α μ-1.
Notice that PK=I k2αα ≤ R0K+1 k2-α dk ≤ (K + 1)2/a. SUm over k and we get
PK=1 kE[f(xk-ι)] -f*≤ μE[-T(T + 1)Ixτ — x*I2]
8
+ 8 Pd=I Bik2-α μ-1.
Devide both side by K(K+1) and we get
需	PK=I kE[f (xk-ι)] - f* ≤ 8Pd=I Bik2-αμ-1.
K(K + 1)
Notice that for K ≥ 1, K-1 ≤ 2(K + 1)-1. We have
需	PK=1 kE[f (xk-ι)] - f * ≤ 16 PL B2k2-αμ7.
K(K + 1)
The theorem then follows by Jensen,s inequality.	□	□
We can in fact also show a non-convex convergence rate:
Theorem 5 (Non-convex CClip under coordinate-wise noise). Suppose we run CClip under the
Assumption of 1 to obtain the sequence {xk}. Then, if f is L-smooth and possibly non-convex, with
appropriate step-sizes and averaging,
K	。	2α-2
KK X E[IVf(xk-ι )I2] ≤ 4∣∣B∣∣3α⅛ ( f(x0) K f3) )3α-2
K k=1	K
Proof. This is very similar to Theorem 2, except that we use the same trick as in the proof above for
Theorem 3.	□
G Lower B ound (Proof of Theorem 4)
We consider the following simple one-dimensional function class parameterized by b:
min {fb(x) = 2(x — b)2} , for b ∈ [0,1/2].	⑴
χ∈[0,1∕2]
Also suppose that for α ∈ (1, 2] and b ∈ [0, 1∕2] the stochastic gradients are of the form:
g(x)〜Vfb(x) + Xb, E[g(x)] = Vfb(x), and E[∣g(x)∣α] ≤ 1.	(2)
Note that the function class (1) has μ = 1 and optimum value fb(b) = 0, and the α-moment of the
noise in (2) satisfies G= B ≤ 1. Thus, we want to prove the following:
15
Under review as a conference paper at ICLR 2020
Theorem 6. For any α ∈ (1, 2] there exists a distribution χb such that the stochastic gradients
satisfy (2). Further, for any (possibly randomized) algorithm A, define Ak(fb + χb) to be the output
of the algorithm A after k queries to the stochastic gradient g(x). Then:
max
b∈[0,1∕2]
E[fb(Ak(fb + Xb))] ≥ ω(k2(α-i)∕a).
Our lower bound construction is inspired by Theorem 2 of Bubeck et al. (2013). Let Ak (fb + χb)
denote the output of any possibly randomized algorithm A after processing k stochastic gradients
of the function fb (with noise drawn i.i.d. from distribution χb). Similarly, let Dk (fb + χb) denote
the output of a deterministic algorithm after processing the k stochastic gradients. Then from Yao’s
minimax principle we know that for any fixed distribution B over [0, 1/2],
m[n maχ EA[Eχbfb(Ak(f + Xb))] ≥ mɪn Eb〜B[Eχbfb(Dk(fb + Xb))].
A b∈ [0,1/ 2]	D
Here we denote EA to be expectation over the randomness of the algorithm A and Eχb to be over
the stochasticity of the the noise distribution Xb. Hence, we only have to analyze deterministic
algorithms to establish the lower-bound. Further, since Dk is deterministic, for any bijective trans-
formation h which transforms the stochastic gradients, there exists a deterministic algorithm D such
that Dk (h(fb + Xb)) = Dk(fb + χb). This implies that for any bijective transformation h(∙) of the
gradients:
min Eb〜B[Eχb fb(Dk(fb + Xb))] = min Eb〜B[Eχb fb(Dk(h(fb + Xb)))] ∙
In this rest of the proof, we will try obtain a lower bound for the right hand side above.
We now describe our construction of the three quantities to be defined: the problem distribution B,
the noise distribution Xb, and the bijective mapping h(∙). All of our definitions are parameterized
by α ∈ (1, 2] (which is given as input) and by ∈ (0, 1/8] (which represents the desired target
accuracy). We will pick to be a fixed constant which depends on the problem parameters (e.g. k)
and should be thought of as being small.
•	Problem distribution: B picks b0 = 2 or b1 = at random i.e. ν ∈ {0, 1} is chosen by an
unbiased coin toss and then we pick
bν = (2 - ν) .
(3)
•	Noise distribution: Define a constant γ = (4e)1/(a-1) and pν = (γa - 2Vγe). Simple compu-
tations verify that γ ∈ (0, 1/2] and that
ɑ	1	1
PV = (4e)O-T - 2ν(4ea)α-1 = (4 - 2ν)(4ea)α-1 ∈ (0,1).
Then, for a given V ∈ {0, 1} the stochastic gradient g(X) is defined as
/、	∫X - 21 with prob. PV ,
g (X) =
X	with prob. 1 - PV .
(4)
To see that we have the correct gradient in expectation verify that
α-1
E[g(x)] = x — 2γ = x------2——+ Ve = X - (2 - V)e = X - b“ = VfbV(X).
Next to bound the α moment of g(X) we see that
1 α1 1
E[∣g(X)lα] ≤ Ya(X - 2γ) + Xa ≤ 2 + 2 = 1.
The above inequality used the bounds that α ≥1, X ∈ [0, 1/2], and γ ∈ (0, 1/2]. Thus g(X)
defined in (4) satisfies condition (2).
• Bijective mapping: Note that here the only unknown variable is V which only affects pν. Thus
the mapping is bijective as long as the frequencies of the events are preserved. Hence given a
stochastic gradient g(Xi) the mapping we use is:
h- 、、	11	if g(Xi) = Xi - 1 ,
h(g(Xi)) = j。otherwise. Y
(5)
16
Under review as a conference paper at ICLR 2020
Given the definitions above, the output of algorithm Dk is thus simply a function of k i.i.d. samples
drawn from the Bernoulli distribution with parameter pν (which is denoted by Bern(pν)). We now
show how achieving a small optimization error implies being able to guess the value of ν.
Lemma 6. Suppose we are given problem and noise distributions defined as in (3) and (4), and an
bijective mapping h(∙) as in (5). Further suppose that there is a deterministic algorithm Dk whose
output after processing k stochastic gradients satisfies
Eb〜B[Eχb fb(Dk(h(fb + Xb)))] < e2∕64 .
Then, there exists a deterministic function Dk which given k independent SamPleS of Bern(PV)
outputs ν0 = Dk (Bern(pν)) ∈ {0, 1} such that
PrhDk(Bern(PV)) = Vi ≥ 3 .
Proof. Suppose that we are given access to k samples ofBern(PV). Use these k samples as the input
h(fb + χb)) to the procedure Dk (this is valid as previously discussed), and let the output ofDk be
x(kV) . The assumption in the lemma states that
22
EV [Eχb |XkV) - bν∣2] < 32, which implies that Eχb |xkV) - bν∣2 < " almost surely.
Then, using Markov’s inequality (and then taking square-roots on both sides) gives
PrhIxkV)- bV| ≥ ei ≤ 1.
Consider a simple procedure Dk which outputs V0 = 0 if XkV) ≥ 学,and V = 1 otherwise. Recall
that |b0 - b1 | = e with b0 = 2e and b1 = e. With probability 4, |xkV) - bV | < I and hence the output
ν0 is correct.	□
Lemma 6 shows that if the optimization error of Dk is small, there exists a procedure Dk which
distinguishes between the Bernoulli distributions with parameters p0 and p1 using k samples. To
argue that the optimization error is large, one simply has to argue that a large number of samples are
required to distinguish between Bern(p0) and Bern(p1).
Lemma 7. For any deterministic procedure Dk (Bern(pν)) which processes k samples of Bern(pν)
and outputs ν0
Pr[ν0 = ν] ≤ + + kk(4e) α-1.
Proof. Here it would be convenient to make the dependence on the samples explicit. Denote S(kν) =
s(1V), . . . , s(kV)	∈ {0, 1}k to be the k samples drawn from Bern(PV) and denote the output as
ν0 = D(SkV)). With some slight abuse of notation where We use the same symbols to denote the
realization and their distributions, we have:
PrhD(SkV)) = ν] = 2 PrhD(SkI)) = l] + ； PrhD(S"=。]=；+ 1 EhD(Sk1)) - D(s"].
Next using Pinsker’s inequality we can upper bound the right hand side as:
EhD(SkI))-D(sk0))i ≤ ∣D(sk1)) -D(sk0))∣τV ≤ JIKL(DS), D(SkO)))
where HTV denotes the total-variation distance and KL(∙, ∙) denotes the KL-divergence. Recall two
properties of KL-divergence: i) for a product measures defined over the same measurable space
(p1, . . . ,pk) and (q1, . . . , qk),
k
KL((P1, . . . ,Pk), (q1, . . . , qk)) =	KL(Pi, qi) ,
i=1
17
Under review as a conference paper at ICLR 2020
and ii) for any deterministic function D,
. . .~ . ~ .
KL(p,q) ≥ KL(D(p), D(q)).
Thus, we can simplify as
PrhD(SkV)) = Vi ≤ 2 + 48 KL(Bern(pi), Bern(P0))
≤ 1 + Sk (P0 -Pi)2
-2	8 8Po(1 - Po)
≤ 1 + 一
_ 2 V 4γα
=1 + Jk(4(2-∙α)f)α-1 .
Recalling that α ∈ (1, 2] gives Us the statement of the lemma.	□
If we pick to be
< _	1
'=16k(α-1”α ,
we have that
1	α 3
2	+ √k(4e)α-1 < 4.
Given Lemmas 6 and 7, this implies that for the above choice of ,
Eb〜B[Eχbfb(Dk(h(fb + Xb)))] ≥ e2∕64 = 四^:)/。∙
This finishes the proof of the theorem. Note that the readability of the proof was prioritized over
optimality and it is possible to obtain significantly better constants.	□
H A Comparison with (Simsekli et al., 2019)
We are not the first to stUdy the heavy-tailed noise behavior in neUral network training. The novel
work by Simsekli et al. (2019) stUdies the noise behavior of AlexNet on Cifar 10 and observed that
the noise does not seem to come from GaUssian distribUtion. However, in oUr AlexNet training
with ImageNet data, we observe that the noise histogram looks GaUssian as in FigUre 6(a, b). We
believe the difference resUlts from that in (Simsekli et al., 2019), the aUthors treat the noise in each
coordinate as an independent scaler noise, as described in the original work on applying tail index
estimator. We on the other hand, consider each the noise as a high dimensional random vector
compUted from a minibatch. We are also able to observe heavy tailed noise if we fix a single
minibatch and plot the noise in each dimension, as shown in FigUre 6(c). The fact that noise is well
concentrated on Cifar is also observed by Panigrahi et al. (2019).
X4=U3P
2 1
O O
αα
X4U3P
1.20	1.25	1.30	1.35
Norm of noise
X4-SUaP
0.00 L,■…L ，二二二二」
0.0000 0.00050.0010 0.0015 0.0020 0.00250.0030
Norm of noise
(a)	(b)
(c)
FigUre 6: (a) Noise histogram of AlexNet on ImageNet data at initialization. (b)Noise histogram of
AlexNet on ImageNet data at 5k iterations. (c) The per dimension noise distribUtion within a single
minibatch at initialization.
18
Under review as a conference paper at ICLR 2020
3 2 10
Oooo
■ ■ ■ .
Oooo
X4uωɑ
0.3:	0.34
Noise norm
(a) ImageNet training, α = 1.99
-"suəɑ
(b) Bert pretraining, α = 1.08
Figure 7: Tail index estimation of gradient noise in ImageNet training and BERT training.
Furthermore, we used the tail index estimator presented in Simsekli et al. (2019) to estimate the
tail index of noise norm distribution. Though some assumptions of the estimator are not satisfied
(in our case, the symmetry assumption; in Simsekli et al. (2019), the symmetry assumption and
independence assumption), we think it can be an indicator for measuring the “heaviness” of the tail
distribution.
I ACClip in ImageNet Training
For completeness, we test ACClip on ImageNet training with ResNet50. After hyperparameter
tuning for all algorithms, ACClip is able to achieve better performance compared to ADAM, but
worse performance compared to SGD. This is as expected because the noise distribution in ImageNet
+ ResNet50 training is well concentrated. The validation accuracy for SGD, ADAM, ACClip are
0.754, 0.716, 0.730 respectively.
3.5
ImageNet with ResNet50
0 5 0
■ ■ ■
3 2 2
Sso-Uoqbp--Ba
---SGD momentum
ADAM
ACCIip
0.00	0.25	0.50	0.75	1.00
Iterations	le5
(a)
Figure 8: Validation loss for ResNet50 trained on ImageNet. SGD outperforms Adam and ACClip.
19