Under review as a conference paper at ICLR 2020
RTC-VAE: Harnessing the peculiarity of Total
Correlation in Learning Disentangled Repre-
SENTATIONS
Anonymous authors
Paper under double-blind review
Ab stract
In the problem of unsupervised learning of disentangled representations, one of
the promising methods is to penalize the total correlation of sampled latent vari-
ables. Unfortunately, this well-motivated strategy often fails to achieve disentan-
glement due to a problematic difference between the sampled latent representation
and its corresponding mean representation. We provide a theoretical explanation
that low total correlation of sample distribution cannot guarantee low total corre-
lation of the mean representation. We prove that for the mean representation of
arbitrarily high total correlation, there exist distributions of latent variables of a
bounded total correlation. However, we still believe that total correlation could
be a key to the disentanglement of unsupervised representative learning, and we
propose a remedy, RTC-VAE, which rectifies the total correlation penalty. Ex-
periments show that our model has a more reasonable distribution of the mean
representation compared with baseline models, e.g., β-TCVAE and FactorVAE.
1	Introduction
VAEs (Variational AutoEncoders) Kingma & Welling (2013); Bengio et al. (2007) follow the com-
mon assumption that the high-dimensional real world observations x can be re-generated by a lower-
dimension latent variable z which is semantically meaningful. Recent works Kim & Mnih (2018);
Chen et al. (2018); Kumar et al. (2017) suggest that decomposing the ELBO (Evidence Lower
Bound) could lead to distinguishing the factor of disentanglement. In particular, recent works Kim
& Mnih (2018); Chen et al. (2018) focused on a term called total correlation (TC). The popular be-
lief Chen et al. (2018) is that by adding weights to this term in objective function, a VAE model can
learn a disentangled representation. This approach appears to be promising since the total correlation
of a sampled representation should describe the level of factorising since total correlation is defined
to be the KL-divergence between the joint distribution Z 〜 q(z) and the product of marginal dis-
tributions Qj q(zj). In this case, a low value suggests a less entangled joint distribution. However,
Locatello et al. (2018) pointed out that the total correlation of sampled distribution T Csample being
low does not necessarily give rise to a low total correlation of the corresponding mean representa-
tion T Cmean . Conventionally, the mean representation is used as the encoded latent variables, an
unnoticed high T Cmean is usually the culprit behind the undesirable entanglement. Moreover, Lo-
catello et al. (2018) found that as regularization strength increases, the total correlation of sampled
representation T Csample and mean representation T Cmean are actually negatively correlated.
Locatello et al. (2018) put doubts on most methods of disentanglement including penalizing the total
correlation term Kim & Mnih (2018); Chen et al. (2018), and they concluded that ”the unsupervised
learning of disentangled representations is fundamentally impossible without inductive biases”.
Acknowledging the difficulty in learning disentangled representation, we provide a detailed ex-
planation of the seemingly contradictory behaviors of the total correlations of sampled and mean
representation in previous works on TC penalizing strategy. Moreover, we find that this problem de-
scribed above can be remedied simply with an additional penalty term on the variance of a sampled
representation.
Our contributions:
1
Under review as a conference paper at ICLR 2020
•	In Theorem 1, we prove that for all mean representations, there exists a large class of
sample distributions with bounded total correlation. Particularly, a mean representation
with arbitrarily large total correlation can have a corresponding sample distribution with
low total correlation. This implies that a low total correlation of sample distribution cannot
guarantee a low total correlation of the mean representation. (Section. 2)
•	Acknowledging the issue above, we further delve into total correlation, and provide a sim-
ple remedy by adding an additional penalty term on the variance of sample distribution.
The penalty term forces a sampled representation to behave similar to the corresponding
mean representation. Such penalty term is necessary for the strategy of penalizing T Cmean
in the view of Theorem 1. (Section. 4)
•	We study several different methods of estimating total correlation. They are compared and
benchmarked against the ground truth value on the multivariate Gaussian distribution Lo-
catello et al. (2018). We point out that the method of (minibatch) estimators suffers from
the curse of dimensionality and other drawbacks, making their estimation accuracy decay
significantly with the increase of the dimension of the latent space, and some strong corre-
lated distributions can be falsely estimated to have low total correlation. (Section. 5)
2	The peculiarity of Total Correlation
In information theory, total correlation is one of the generalizations of mutual information, which
measures the difference between the joint distribution of multiple random variables and the product
of their marginal distributions. A high value means the joint distribution is far from an independent
distribution, and hence it suggests high entanglement among these random variables.
Definition 1. Total correlation of random variable x,
TC(X) =KL ∣p(x)ll ∏p(Xj- )1 = Ep(X)
log
p(x)
QjP(Xj)
Naturally, people seek the solution of disentanglement in the form of low total correlation of the
latent variables, e.g. Kim & Mnih (2018); Chen et al. (2018). However, there can be large differ-
ence between the total correlations of sample representation and mean representation. Forcing the
former to be small does not guarantee the latter being small. In fact, given a mean representation of
arbitrarily large total correlation, we can construct a family of distribution of sample representation
that have a bounded total correlation, where the bound does not rely on the total correlation of the
mean.
Theorem 1. Let μ 〜 N(0, Σ) and σj∙ be the Standard deviation of μj∙, j = 1, ∙∙∙ ,D, and
maxj∙ σj = co. For a fixed μ, let Z 〜 N(μ, Σ0(μ)), where Σ0(μ) is diagonal and satisfies that
for Some R > 0,
C2 > σj (μ) > ci > 0,
c3 > σj (μ) > μι, for some l ≥ 1,
if ∣μ∣ <R,
if ∣μ∣ >R.
(1)
for some constants c1,c2, c3, c4. Then TC(Z) ≤ C for some C = C (R, c0,…,c4,l) > 0.
The details of the proof are presented in Appendix A.3. Here’s another way to interpret Theorem 1:
with C and parameters R,co, ∙∙∙ ,c4, l fixed, one can make TC(μ) arbitrarily large, since TC(μ)
depends only on the correlation matrix of μ (see Proposition 1).
Theorem 1 provides an explanation to the contradiction observed by Locatello et al. (2018) that
TC(Z) is low does not mean TC(μ) is low (actually much higher than TC(Z)). Since there exist
such a large class of distributions of Z that all have bounded TC(Z). When the objective function
only penalizes TC(μ), neural networks are so flexible to easily find a distribution with low TC(z),
and total correlation estimators like MSS can encourage shutting down latent dimensions (see Sec-
tion 5.3), which together cause the disparity of TC(μ) and TC(z). This fact was not noticed until
Locatello et al. (2018), and our investigation gives an explanation of the peculiar property of total
correlation. Hence, Theorem 1 leads to the necessity of a regularizer of the difference between the
2
Under review as a conference paper at ICLR 2020
distributions of μ and Z When penalizing TCsampie. In Section. 4, We propose a simple regularizer
that serves this goal.
It is an interesting question whether there exists a distribution of Z N(μ, Σ0(μ)) with arbitrarily
small TC(Z) given μ. If not, what,s the lower bound of TC(z)? These questions remain open to US
for now, and we leave them to future work.
3	Related Works
In the study of disentanglement, Higgins et al. (2017) proposed a modification of the VAE framework
and introduced an adjustable hyperparameter β that balances latent channel capacity and indepen-
dence constraints with reconstruction accuracy. One drawback ofβ-VAE is the trade-off between the
reconstruction quality and disentanglement. Motivated to alleviate this trade-off of β-VAE, Kim &
Mnih (2018) proposed FactorVAE which decomposes the evidence lower bound and penalize a term
measuring the total correlation between latent variables. Around the same time, Chen et al. (2018)
proposed a similar ELBO decomposition method called β-TCVAE. The major difference between
FactorVAE and β-TCVAE lies in their different strategies of estimating total correlation. Chen et al.
(2018) used formulated estimators while Kim & Mnih (2018) utilized the density-ratio trick which
requires an auxiliary discriminator network and an inner optimization loop. We will discuss these
two strategies more in details in Section. 5.
The works above belong to representative learning without inductive biases. There are also works
about representative learning with inductive biases, see Rolinek et al. (2019) and references therein.
As for the disentanglement metric, this will be discussed in Section. 6.
4	Rectified-TCVAE
To simplify notation, let p(n) = p(xn), q(z|xn) = q(z|n). Recall the average evidence lower bound
(ELBO),
ELBO := Ep(n) Eq(z|n)[log p(n|z)] - KL(q(z|n)kp(z)) .	(2)
Chen et al. (2018) and independently by Kim & Mnih (2018) introduced objective function that
penalizes total correlation, which can be formulated as
Lβ-TC := ELBO - βTC(z).	(3)
This approach unfortunately has a drawback. It turns out that instead of being able to obtain disen-
tangled representation, we often find a sample representation appears to be disentangled while the
mean representation is still entangled. In fact, when we are maximizing Lβ-TC, we could totally
end up learning a distribution ofZ that makes TC(Z) goes low, while the total correlation of its mean
μ is still high. To resolve this, we define RTC-VAE,
LRTC := Le-TC - η ∙ tr(Ep(n)Covq(z∣n) [z]),	(4)
where
D
tr(Ep(n)Covq(z|n) [z]) =	Ep(n) [σk2(n)].
k
Our penalty originates from the first term of the law of total covariance Covq(z) [z] =
Ep(n)Covq(z|n) [z] + Covp(n)(Eq(z|n) [z]). Factorized representation1 indicates a diagonal covari-
ance matrix Covq(z) [z]. Motivated by this, Kumar et al. (2017) penalizes the off-diagonal terms in
the second term, while ignores Ep(n)Covq(z|n) [z] since it is diagonal. Their penalty term leads to a
vanishing μ, which is the mean representation. The remedy to this is to add another penalty term on
the distance between μ's and 1. DIP-VAE Kumar et al. (2017) employs this remedy, however, DIP-
VAE does not outperform other VAE’s when measured by various disentanglement metrics, e.g.,
FactorVAE score, see Fig. 3 & 14 in Locatello et al. (2018). This is actually not surprising since the
1Factorized representation means each latent dimension is independent.
3
Under review as a conference paper at ICLR 2020
two penalty terms in DIP-VAE contribute in opposite directions, with one leading to vanishing μ's
and another fighting against it. This formulation can easily get the model stuck in saddle points.
Our objective, on the other hand, does not penalize directly on μ. Instead, it penalizes on σ, the
standard deviation of the distribution q(z|n). This may seem little counter-intuitive at first sight,
since penalizing a diagonal component of covariance Cov[z] = Covq(z) [z] seems not helpful to
factorising. However, in the view of Theorem 1, our objective will force the distribution of z to be
similar to the distribution of μ. Hence, it pushes us away from the situation of large TC(μ) and low
TC(z). Consequently, by minimizing TC(Z) we get a model that has low TC(μ), a disentangled
mean representation.
5 Estimation of total correlation
The naive Monte Carlo method comes with an intrinsic issue of underestimating total correlation.
To avoid or resolve this, Kim & Mnih (2018) proposed a discriminator network with the help of
density-ratio trick (see equation (3) and Appendix D. of Kim & Mnih (2018)). In Chen et al. (2018),
two kinds of estimator of total correlation are proposed, Minibatch Weighted Sampling (MWS) and
Minibatch Stratified Sampling (MSS) (see Appendix C.1 and C.2 in Chen et al. (2018)).
5.1	Method of Minibatch Estimators
For instance, MSS can be described as followed. For a minibatch of sample, BM+1 =
{n1, . . . , nM+1},
1	M+1
Eq(z,n) [log q(z)] ≈ M + 1 E log f Qi, %, BM +1 \ {"i}),	⑸
i=1
where2
1	1 M-1	N - M
f (Z, n , BM +1 \ {n }) = Nq(ZIn ) + ME q(z|nm) + NM q(zlnm).	⑹
m=1
For the convenience of readers, MWS is listed in Appendix A.1.
5.2	Method of density-ratio trick and dis criminator
Density-ratio trick Nguyen et al. (2010); Sugiyama et al. (2012) can be used to estimate KL-
divergence,
TC(z) = KL(q(Z)k	q(Zj)) = Eq(z)
j
q(Z)
log Qjq(Zj)
≈ Eq(z)
D(Z)-
1 - D(Z) 一
(7)
(8)
where D is discriminator that classifies z being sampled from q(Z) or jq(Zj). For detail imple-
mentation, please refer to section 3 in Kim & Mnih (2018).
5.3	Comparison of the two methods
For multivariate normal distribution, the total correlation can be explicitly calculated which can be
used as a ground truth for our comparison. To be specific,
Proposition 1. Let X 〜N (0, Σ) ,then
TC(X) = 1(log∣diag(Σ)∣- log∣∑∣).	⑼
2There is a small part of the implementation of MSS in Chen et al.’s code that is not quite clear to us,
specifically, the computation of log importance weight matrix in equation 6. In our experiment, we implement
MSS with our understanding and denote it as MSS1 , and we denote Chen et al.’s implementation MSS0. See
Appendix A.2
4
Under review as a conference paper at ICLR 2020
It’s difficult to track the exact reference of Proposition 1 since it is a fundamental property in infor-
mation theory. Locatello et al. (2018) used this proposition to approximate the total correlation of
the mean representation in latent space. In appendix, we provide a simple proof for the convenience
of the readers.
We compared the performance of each method, MWS, MSS0 and MSS1 on the estimation of total
correlation. For μ 〜N(0, I), and z∣μ 〜N(μ, Σ) where Σ = diag(σ2) and σ = 0.1. We choose σ
small so that the distribution of z can be approximated by normal distribution. Results are presented
in Figure 1.
Figure 1: Different estimators of total correlation vs. ground truth on latent space of dimension low
to high.
From Figure. 1, we can summarize the following observations: 1. MWS tends to underestimate total
correlation in general; 2. For latent space of dimension ≤ 4, MSS0 and MSS1 are quite accurate;
3. For latent space of high dimension, both MSS0 and MSS1 tend to overestimate total correlation
when the actual value of total correlation is small; 4. Overall MSS1 estimates closer to ground truth
than MMS0 does.
In the following analysis of the above observations, we use a less formal way of analyzing, which
can be formalized to be rigorous, in order to convey our idea directly.
To interpret the third observation, let μ 〜N(0, Id) and z∣μ 〜N(0, Σ), where Id is identity matrix
and Σ = diag(σ2). Then TC(μ) = 0 and TC(Z) small if σ small. Consider q(zki) Inj)) where
(i, j, k) are indices of a cube (minibatch, minibatch, dimension) with size M × M × D and n(j)
is a sample drawn in a minibatch and z(i) = z(n(i)). We claim this: when the ground truth total
correlation of z is low (the off-diagnal values of correlation matrix is small), only the elements on
the diagonal surface of the cube, namely those elements of index (i, i, k), take some bounded values
O(1), and all the other elements are very small o(1) (since σ = 0.1).
To see the claim, let's first consider 1-D case, where μ 〜 N(0,1), z∣μ 〜N(0, σ2). When σ is
small, z can be approximately treated as N(0,1). z(i) and μ(j) are independent for i = j, hence
z(i) - μ(j) 〜N(0, 2), and
P (∣z(i) - μj) I < t) = t + O(t2)
(10)
See a proof in Appendix A.5. Then for D-dimension, the probability P(∣z(i) 一 μ(j) ∣ < t) would be
O(tD). Now, only if t takes value as small as σ, q(zk(i) In(j)) is not small. For example, σ = 0.1
and D = 10 and the chance of such case to happen is O(10-10). Compared to batchsize, usually
O(103), the amount of such cases can be ignored.
5
Under review as a conference paper at ICLR 2020
Hence,
TC(Z) = Eq(∕og Qksy
= Eq(z,n)[log q(z)] -
Eq(z,n) [log	q(zk )]
≈ Mm X flog M XYq(Zki) 1n(j))- log Y Mm X q(Zki) 1n(j))
i	jk	k	j
≈ Mm Xog Mm X o(i)-log Y Mm X。⑴)
i	j=i	k	j=i
≈ Mm X(log O( M) - log O( M))
i
≈ O((D - )logM ).
Assigning weights to elements q(Zk(i) |n(j)) such as MSS does not make essential change to the
analysis above.
In addition, β-TCVAE (trained with MSS in our experiments and with MWS in Locatello et al.
(2018)) seems to have an increasing total correlation of mean representation as regularization
strength increases (higher β's), as observed by Locatello et al. (2018). Here, We provide an ex-
planation to the cause of this problem:
First, MSS and MWS prefer to shut doWn latent dimensions, meaning that distributions With feWer
active dimensions can score lower estimated total correlation. Consider that μo 〜 N(0,0.01),
μo-〜N(0, Id) and also z∣μ 〜N(0, Σ), where Σ = diag(σ2). Then all elements of index
(i, j, 0), i.e., q(Z0(i) |n(j)) are not small, say O(1), for the same reason as previous in D- 1 dimension
latent space. Thus, MM Pjq(Zs) Inj)) ≈ M PjO(I) ≈ O(1), and
TC(z) = Eq(z)
log Qk≡ 一
i
i

焉 Pj Qk q(Zki) Inej))
Qk mM Pjq(Zki) Tej))

M Pj (q(ZOi)Inj)) ∙Qk>0 q(Zki)Inj)))
∏k>o MM Pjq(Zki)Inj))

1 Xlog 焉∙。(I)
m V g ∏k>o 焉∙ o(i)
≈ logO(MD-2)
≈ O((D -2)logM ),
0.01	-0.1
-0.1	1
compared to O((D 一 1) log M) when μ 〜N(0, Id) in previous analysis.
Now, consider any strongly correlated z's (e.g., (zι, z2)	〜 N(0, Σ), where Σ =
, see Figure 2). Then the Gaussian (ground truth) total correlation is arbitrar-
ily large (TC(z1 , z2) = ∞). This kind of distribution can score a relatively low TC value (for
instance lower than z) with estimators such as MSS and MWS by the analysis above. Hence, as β
increases, VAE trained with these estimators will be encouraged to obtain some dimensions of very
low variance, and these dimensions are easily trapped in a strong correlation with other dimensions
(like z1 and z2).
6
Under review as a conference paper at ICLR 2020
Figure 2: One dimension of mean has low variance (shutting down), and the distribution is strongly
correlated (It appears to be almost flat due to small scale of the shutdown dimension). A sampled
distribution (e.g. z∣μ 〜N(μ, 1)) has a very low TC.
Shutting down dimension is not preferable because latent dimensions should not be fewer than
ground truth. Moreover, considering datasets such as dSprites, though shape is labelled as a single
dimension, models can learn to represent complex geometry with multiple dimensions, hence more
active dimensions are learned than the number of ground-truth dimensions.
Based on the reasons above, we opt for the method of discriminators (density-ratio trick) in our
implementation. Experiment shows that density-ratio trick provides a more stable estimation of
total correlation when training VAE.
6	Experiments
The datasets we use include dSprites Matthey et al. (2017), Shapes3D Burgess & Kim (2018) and
3D faces Paysan et al. (2009). At the time of writing, the scale of our experiments is limited, but
there are already some evidence to deliver our arguments. We have scheduled further experiments
and tests on larger scale in future works.
For every model, we trained with 10 different initialization. Hyperparameter β , also the regulariza-
tion strength, takes 2,4,…，10. For hyperparameter η, we fix η = 10 for a simple reason. Since the
variance term in equation 4 becomes small (close to 0) shortly after training begins, this term will
not contribute much compared with Lbeta-T C term. So, η = 10 is enough to strengthen the penalty
at the beginning of training. In our experiments, higher value ofη cannot bring further improvement
since Z is close to μ already. And lower values may not guarantee Z being close to μ.
From experiments, we observe that RTC-VAE has much lower T Cmean with different regulariza-
tion strength than FactorVAE does (Figure 4). And on different datasets, this is also the case (see
Appendix). The T Cmean behaves almost identically as T Csample in RTC-VAE (see Figure 3 (b)).
The problem of contradictory behaviors of T Cmean and T Csample is evidently remedied by RTC-
VAE. In addition, the ELBO of RTC-VAE seems to converge faster than FactorVAE as a byproduct
(see Figure 5 and Figure 6). Examining the distributions of latent dimensions (mean representation),
FactorVAE tends to have some strongly correlated latent dimensions (see Figure 8), and RTC-VAE
shows well factorized latent distributions (see Figure 7).
6.1	Metrics of disentanglement
We wish there were a widely accepted metric of disentanglement to compare our model RTCVAE
with other models. Unfortunately, it is still an open question, how we can measure disentanglement.
Various attempts have been made so far, but Locatello et al. (2018) challenged most of them, in-
dicating that the score under any metric varies due to different initializations and datasets. Here,
7
Under review as a conference paper at ICLR 2020
Figure 3: The shaded region indicates 90% confidence interval. (a) The total correlation of sample
and mean representation of FactorVAE on dSprite with β = 2, 6, 10. There is a large difference in
scales of T Csample and T Cmean. (b) The total correlation of sample and mean representation of
RTCVAE on dSprite with β = 2, 6, 10 and η = 10. There is almost no difference between T Csample
and T Cmean due to the variance penalty term in equation 4.
Figure 4: Direct comparison between the T Cmean of FactorVAE and RTCVAE.
we analyse several important metrics and attempt to point out some blind spots that have not being
considered by these metrics.
Higgins et al. (2017) proposed using a classifier to measure each dimension of latent space and each
ground truth factor, e.g. (x, y) coordinates, scale, rotation, etc. Kim & Mnih (2018) revised this
Figure 5: Training ELBO of RTCVAE and FactorVAE.
8
Under review as a conference paper at ICLR 2020
Figure 6: Training ELBO of RTCVAE and FactorVAE on Shapes3D.
Figure 7: The pairplot of 10 latent dimensions of RTCVAE on Shapes3D. No dimensions show
strong correlation.
approach yet kept the idea. Chen et al. (2018) introduced mutual information gap (MIG), which
estimates the mutual information between each latent dimension and each ground truth factor. Note
that this is a classifier-free metric. Eastwood & Williams (2018) proposed a framework of disentan-
9
Under review as a conference paper at ICLR 2020
Figure 8: The pairplot of 10 latent dimensions of FactorVAE on Shapes3D. Some dimensions show
strong correlation, e.g. dim 1&3, dim 1&4, dim 3&4, dim 3&6.
glement metric that considers modularity, compactness and explicitness. Then Ridgeway & Mozer
(2018) made analysis on compactness, and compactness mean that each ground truth factor asso-
ciates with only one or a few latent dimensions. They pointed out that in some situation a perfectly
disentangled representation may not be compact (see Section. 3 in Ridgeway & Mozer (2018)).
Here we argue that modularity also should be reconsidered. A modular representation means that
each dimension of latent space conveys information of at most one ground truth factor. This is
exactly the goal attempted by Higgins et al. (2017); Kim & Mnih (2018); Chen et al. (2018), etc.
However, multiple latent dimensions can work together to represent multiple ground truth factors
meanwhile these latent dimensions are disentangled. For instance, x and y coordinates can be
represented by r and θ in polar coordinate system (or any coordinate system under rotation, i.e.,
(x0 , y0 )T = A(x, y)T where A is any orthogonal matrix). These coordinate systems are perfectly
disentangled but r (or x0) conveys information of both x and y.
7	Conclusion
In this work, we demonstrated that our RTC-VAE, which rectifies the total correlation penalty can
remedy its peculiar properties (disparity between total correlation of the samples and the mean rep-
resentations). Our experiments show that our model has a more reasonable distribution of the mean
representation compared with baseline models including β-TCVAE and FactorVAE. We also pro-
vide several theoretical proofs which could help diagnose several specific symptoms of entangle-
10
Under review as a conference paper at ICLR 2020
ment. Hopefully, our contributions could add to the explainability of the unsupervised learning of
disentangled representations.
11
Under review as a conference paper at ICLR 2020
References
Yoshua Bengio, Yann LeCun, et al. Scaling learning algorithms towards ai. Large-scale kernel
machines, 34(5):1-41,2007.
Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018.
Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentangle-
ment in vaes. arXiv preprint arXiv:1802.04942, 2018.
Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disen-
tangled representations. 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. ICLR, 2(5):6, 2017.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983,
2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentan-
gled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard SchOlkopf, and Olivier
Bachem. Challenging common assumptions in the unsupervised learning of disentangled repre-
sentations. arXiv preprint arXiv:1811.12359, 2018.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face
model for pose and illumination invariant face recognition. In 2009 Sixth IEEE International
Conference on Advanced Video and Signal Based Surveillance, pp. 296-301. Ieee, 2009.
Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic
loss. In Advances in Neural Information Processing Systems, pp. 185-194, 2018.
Michal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca direc-
tions (by accident). In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 12406-12415, 2019.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density-ratio matching under the breg-
man divergence: a unified framework of density-ratio estimation. Annals of the Institute of Sta-
tistical Mathematics, 64(5):1009-1044, 2012.
A Appendix
A. 1 Minibatch Weighted Sampling (MWS)
See Chen et al. (2018),
M
M
Eq(z) [log q(z)] ≈
MM
Mf log∑S q(z(ni)lnj)-
i=1
j=1
log(N M)
(11)
12
Under review as a conference paper at ICLR 2020
A.2 MMS0 AND MMS1
There is no mathematical difference between MSS0 and MSS1 (same formulation as equation 5),
only a difference in implementation. We replace this chunk of code (https://github.com/
rtqichen/beta-tcvae/blob/master/vae_quant.py#L199-L201) to
for i in range (batch_size ):
W[ i , i ] = 1 /N
W[i,(1+i)%batch_size] = strat_weight
A.3 Proof of Theorem 1
In the following proof, we use the convention of mathematical analysis that the meaning of C can
change through lines to eliminate some redundant work of tracking.
Theorem (Theorem 1 restated). Let μ 〜N(0, Σ) and σ, be the Standard deviation of μj, j =
1,…，D, and max, σj = co. For a fixed μ, let Z 〜 N(μ, Σ0 (μ)), where Σ0(μ) is diagonal and
SatiSfieS that for Some R > 0,
C2 > σj (μ) > ci > 0,
C3 > σj (μ) > ,,for some l ≥ 1,
if ∣μ∣ <R,
if ∣μ∣ >R.
(12)
for some constants c1,c2, c3, c4. Then TC(Z) ≤ C for some C = C (R, c0,…,c4,l) > 0.
Proof. Let
S+ = {z ∈ RD∣p(z) ≥ Y p(zj)},	S- = {z ∈ RD∣p(z) < Yp(zj)},
jj
then
TC(Z) =	+
S+	S-
TC(Z)++TC(Z)-.
Since KL-divergence is non-negative, if TC(Z)+ is bounded, then TC(Z) must be bounded. In the
following, we work on S+, i.e., we assume p(z) ≥ Qj p(zj).
For ∣z ∣ < R,
P(Z) = Ep(μ)[p(Z∣μ)]
/ P(M)
1
1
√(2∏)D ∣∑0∣
e-2(z-μ)(∑0) I(Z-μ)Tdμ
≤	,	二
一 √(2π)Dc2D
C
≤干
/p(μ)dμ
and
Y P(Zj) ≥ Mbr(0) PM) √⅛I
C
≥营
2R2

ɪ
1
c
13
Under review as a conference paper at ICLR 2020
For |z | > 2R,
p(z)
j
BR(0)
j
JBM (0)∖Br(0)
C
CFe
Ilzl-Rl2
τc∣
c4
lzl2
+B
czι⑼
p(μ)
e-Hμ-z)(£O)T W-Z)T
√(2∏)d∣∑0∣
dμ
C
CFe
Ilzl-Rl2
τc∣
Izl2
TB 鼻(0)
p(μ)
∣μ∣Dl
√(2π)D CD
dμ
Ilzl-Rl2
CD e
C1
C1 ,
KzI
r1
τc∣
C
+ CF|z1
Izl2
Izl2
dle	8。2
C
+ CD|z1
lzl2
Dl + D-2e-阳
Dl+D-2e-甚,
+
L (0)
≤
≤
≤
≤


C

+C ¥
e-软
+C ¥
4

e-软



where r0 = max(C0, C2) and r1 = min(C1, C4), and since l ≥ 1 and D ≥ 1, Dl + D - 2 ≥ 0. For
IZI ∈ (R1, 2R1), it is easy to see that P(Z) < C. And for IZI > R,
Y PpZz) ≥ Y UBCO(0)PM)
(zj-μ)2
C-
≥ De e
2∣z∣2
—，	二 e
√(2π)D c2
Dμj
1


c
ɪ
1
2
Hence,
]
TC(z) = Ep(z)
log
p(z)
j p(zj )
≤
BR(0)
CD
P(Z)IogC 部
2R2
e c1 dz +
B2cR(0)
p(z) log(C
2D
r1
2∣z∣2 Izl2
ZIDl+D-2e 可 - 呵)dz + C


≤
≤
D log CT +
2R2
-	-且
/ e 8r0 [C +(Dl + D - 2)log∣z∣ +
B2cR(0)
+C
2R2
D log c2 + 彳 + C.
A.4 Proof of Proposition 1
Proposition (Proposition 1 restated).
Let X 〜N (0, Σ) ,then
TC(x)
2 (log∣diag(∑)∣ - log∣∑∣).
(13)
Proof. First, recall that the KL-divergence between two distributions P and Q is defined as
P
KL(PIIQ)= Ep[log Q]
Also, the density function for a multivariate Gaussian distribution N(μ, Σ) is
p(x)
1
(2∏W2det(Σ)”
exp( — — (X — μ)TΣ I(X — μ)).
14
Under review as a conference paper at ICLR 2020
Now, for two multivariate Gaussian P1 and P2, we have
KL(P1||P2) = EP1 [logP1 -logP2]
1 detΣ2 2logdet∑ι	+ 2Epι(x)[-(X - μl)TςII(X -	■ μι) + (x - M2)Tς2 I(X - μ2)]
1	detΣ2 -log 2 sdet∑ι	+ 2Epι(x) [-tr(ςi 1 (X - μI)(X	- μi)T) + tr(£2 I(X - M2)(x - M2)T)]
2logdet∑2 - 2tr('-1'I) + 2Epι(χ)[tr(£-I((XxT - 2χμT + μ2μT))]
Xlog 1 ,v2 - + + 5Epi(x)[tr(£-I((X - μ1 + μ1)(x - μ1 + μ1)τ - 2xμT + μ2 μT))]
2 detΣ1	2	2
3logTTv2 -	+	+	5Epi(x)[tr(£-I((X	- μI)(X -	μI)T	+ 2(X	- μι)μι +μιμT	-	2xμT	+	μ2μT))]
2 det∑ι	2	2	、	-〜一	/
Epι(χ)(x)=μι
，g,：12 - ∣n + %Kς-Yςi + (μ2 - μι)(μ2 - μI)T))
2 detΣ1	2	2
X (logλ ,v2 - n + tr(£-1£I) + (μ2 - μI)Tς-1 (μ2 - μI))
2	detΣ1
Let P be a multivariate Gaussian N(μ, ∑ι), and then the product of the marginal distribution
QiPi(X) is also Gaussian N(μ, ∑2), where ∑2 = diag(∑ι). Thus, the total correlation of multi-
variate Gaussian distribution is
TC(x) = DKL(p(X)|| Ypi(X))
i
=j(logdnɪ - n + tr(∑-1∑1) + (μ - μ)τ∑-1(μ - μ))
2 detΣ1
ι∩	det¾	、
=2(logdet∑; -n+n)
=2 (log∣diag(∑ι)∣ - log∣∑ι∣)
A.5 Proof of equation 10
Proof.
P(Izu) - μ(j)∣ < t) = P(|x| < t) where X 〜N(0, 2)
=t + O(t2)
A.6 Experiments
15
Under review as a conference paper at ICLR 2020
Figure 9: The shaded region indicates 90% confidence interval. (a) The total correlation of sample
and mean representation of FactorVAE on Shapes3D with β = 2, 6, 10. There is a large difference
in scales of T Csample and T Cmean . (b) The total correlation of sample and mean representation
of RTCVAE on Shapes3D with β = 2, 6, 10 and η = 10. There is almost no difference between
T Csample and T Cmean due to the variance penalty term in equation 4.
Figure 10: Direct comparison between the T Cmean of FactorVAE and RTCVAE.
16