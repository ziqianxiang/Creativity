Under review as a conference paper at ICLR 2020
Last-iterate convergence rates for min-max
OPTIMIZATION
Anonymous authors
Paper under double-blind review
Ab stract
While classic work in convex-concave min-max optimization relies on average-
iterate convergence results, the emergence of nonconvex applications such as
training Generative Adversarial Networks has led to renewed interest in last-iterate
convergence guarantees. Proving last-iterate convergence is challenging because
many natural algorithms, such as Simultaneous Gradient Descent/Ascent, provably
diverge or cycle even in simple convex-concave min-max settings, and previous
work on global last-iterate convergence rates has been limited to the bilinear and
convex-strongly concave settings. In this work, we show that the Hamiltonian
Gradient Descent (HGD) algorithm achieves linear convergence in a variety
of more general settings, including convex-concave problems that satisfy a novel
“sufficiently bilinear” condition. We also prove convergence rates for stochastic
HGD and for some parameter settings of the Consensus Optimization algorithm of
Mescheder et al. (2017).
1	Introduction
In this paper we consider methods to solve smooth unconstrained min-max optimization problems.
In the most classical setting, a min-max objective has the form
minx1 maxx2 g(x1 , x2)
where g : Rd × Rd → R is a smooth objective function with two inputs. The usual goal in such
problems is to find a saddle point, also known as a min-max solution, which is a pair (x；,xg) ∈
Rd × Rd that satisfies
g(xl,x2) ≤ g(xl,x2) ≤ g(x1,x2)
(1)
for every x1 ∈ Rd and x2 ∈ Rd . Min-max problems have a long history, going back at least as far as
Neumann (1928), which formed the basis of much of modern game theory, and including a great deal
of work in the 1950s when algorithms such as fictitious play were explored (Brown, 1951; Robinson,
1951).
The convex-concave setting, where we assume g is convex in x1 and concave in x2, is a classic
min-max problem that has a number of different applications, such as solving constrained convex
optimization problems. While a variety of tools have been developed for this setting, a very popular
approach within the machine learning community has been the use of so-called no-regret algorithms
(Cesa-Bianchi & Lugosi, 2006; Hazan, 2016). This trick, which was originally developed by Hannan
(1957) and later emerged in the development of boosting (Freund & Schapire, 1999), provides a
simple computational method via repeated play: each of the inputs x1 and x2 are updated iteratively
according to no-regret learning protocols, and one can prove that the average-iterates (X1,X2)
converge to a min-max solution.
Recently, interest in min-max optimization has surged due to the enormous popularity of Generative
Adversarial Networks (GANs), whose training involves solving a nonconvex min-max problem where
x1 and x2 correspond to the parameters of two different neural nets (Goodfellow et al., 2014). The
fundamentally nonconvex nature of this problem changes two things. First, it is infeasible to find a
“global” solution of the min-max objective. Instead, a typical goal in GAN training is to find a local
min-max, namely a pair (x12, x22) that satisfies (1) for all (x1, x2) in some neighborhood of (x12, x22).
1
Under review as a conference paper at ICLR 2020
Second, iterate averaging lacks the theoretical guarantees present in the convex-concave setting. This
has motivated research on last-iterate convergence guarantees, which are appealing because they
more easily carry over from convex to nonconvex settings.
Last-iterate convergence guarantees for min-max problems have been challenging to prove since
standard analysis of no-regret algorithms says essentially nothing about last-iterate convergence.
Widely used no-regret algorithms, such as Simultaneous Gradient Descent/Ascent (SGDA), fail
to converge even in the simple bilinear setting where g(x1 , x2) = x1>Cx2 for some arbitrary
matrix C. SGDA provably cycles in continuous time and diverges in discrete time (see for example
Daskalakis et al. (2018); Mescheder et al. (2018)). In fact, the full range of Follow-The-Regularized-
Leader (FTRL) algorithms provably do not converge in zero-sum games with interior equilibria
(Mertikopoulos et al., 2018). This occurs because the iterates of the FTRL algorithms exhibit cyclic
behavior, a phenomenon commonly observed when training GANs in practice as well.
Much of the recent research on last-iterate convergence in min-max problems has focused on
asymptotic or local convergence (Mertikopoulos et al., 2019; Mescheder et al., 2017; Daskalakis
& Panageas, 2018; Balduzzi et al., 2018; Letcher et al., 2019; Mazumdar et al., 2019). While
these results are certainly useful, one would ideally like to prove global non-asymptotic last-iterate
convergence rates. Provable global convergence rates allow for quantitative comparison of different
algorithms and can aid in choosing learning rates and architectures to ensure fast convergence in
practice. Yet despite the extensive amount of literature on convergence rates for convex optimization,
very few global last-iterate convergence rates have been proved for min-max problems. Existing work
on global last-iterate convergence rates has been limited to the bilinear or convex-strongly concave
settings (Tseng, 1995; Liang & Stokes, 2019; Du & Hu, 2019; Mokhtari et al., 2019). In particular,
the following basic question is still open:
“What global last-iterate convergence rates are achievable for convex-concave min-max problems?”
Our Contribution Understanding global last-iterate rates in the convex-concave setting is an
important stepping stone towards provable last-iterate rates in the nonconvex-nonconcave setting.
Motivated by this, we prove new linear last-iterate convergence rates in the convex-concave setting
for an algorithm called Hamiltonian Gradient Descent (HGD) under weaker assumptions
compared to previous results. HGD is gradient descent on the squared norm of the gradient, and it has
been mentioned in Mescheder et al. (2017); Balduzzi et al. (2018). Our results are the first to show
non-asymptotic convergence of an efficient algorithm in settings that not linear or strongly convex in
either input. In particular, we introduce a novel “sufficiently bilinear” condition on the second-order
derivatives of the objective g and show that this condition is sufficient for HGD to achieve linear
convergence in convex-concave settings. The “sufficiently bilinear” condition appears to be a new
sufficient condition for linear convergence rates that is distinct from previously known conditions
SUCh as the Polyak-Lojasiewicz (PL) condition or PUre bilinearity. OUr analysis relies on showing
that the squared norm of the gradient satisfies the PL condition in various settings. As a corollary
of this resUlt, we can leverage Karimi et al. (2016) to show that a stochastic version of HGD will
have a last-iterate convergence rate of O(1∕vzk) in the “sufficiently bilinear” setting. On the practical
side, while vanilla HGD has issUes training GANs in Practice, Mescheder et al. (2017) show that a
related algorithm known as ConsensUs Optimization (CO) can effectively train GANs in a variety of
settings, inclUding on CIFAR-10 and celebA. We show that CO can be viewed as a pertUrbation of
HGD, which implies that for some parameter settings, CO converges at the same rate as HGD.
FigUre 1: HGD converges qUickly, while SGDA
spirals. This nonconvex-nonconcave objective is
in defined in Appendix K.
We begin in Section 2 with backgroUnd material
and notation, inclUding some of oUr key assUmp-
tions. In Section 3, we discUss Hamiltonian
Gradient Descent (HGD), and we present oUr
linear convergence rates for HGD in varioUs set-
tings. In Section 4, we present some of the key
technical components Used to prove oUr resUlts
from Section 3. Finally, in Section 5, we present
oUr resUlts for Stochastic HGD and ConsensUs
Optimization. The details of oUr proofs are in
Appendix H.
2
Under review as a conference paper at ICLR 2020
2 Background
2.1	Preliminaries
In this section, We discuss some key definitions and notation. We will use ∣∣∙∣∣ to denote the Euclidean
norm for vectors or the operator norm for matrices or tensors. For a symmetric matrix A, we will use
λmin(A) and λmax(A) to denote the smallest and largest eigenvalues of A. For a general real matrix
A, σmin(A) and σmax(A) denote the smallest and largest singular values of A.
Definition 2.1. A critical point of f : Rd → R is a point X ∈ Rd such that Vf (x) = 0.
Definition 2.2 (Convexity / Strong convexity). Let μ ≥ 0. A function f : Rd → R is μ-strongly
convex if for any u, v ∈ Rd, f (U) ≥ f (v) + Nf(V) U — Vi + μμ ||u 一 v||. When f is twice-
differentiable, f is μ-StrOngly-convex iff for all X ∈ Rd, V2 f (x)占 μI. If μ = 0 in either of the
above definitions, f is called convex.
Definition 2.3 (Monotone / Strongly monotone). Let μ ≥ 0. A vectorfield V : Rd → Rd is μ-strongly
monotone if for any x,y ∈ Rd,(x — y, V(X) — v(y)i ≥ μ ||x — y||2. If μ = 0, V is called monotone.
Definition 2.4 (Smoothness). A function f : Rd → R is L-smooth if f is differentiable everywhere
and for all u, V ∈ Rd satisfies ||Vf (u) — Vf (V)|| ≤ L ||u — V||.
Notation Since g is a function of X1 ∈ Rd and X2 ∈ Rd , we will often consider X1 and X2 to
be components of one vector X = (X1 , X2 ). We will use superscripts to denote iterate indices.
Following Balduzzi et al. (2018), we use ξ = (Vx1 g, —Vx2 g) to denote the signed vector of partial
derivatives. Under this notation, the Simultaneous Gradient Descent/Ascent (SGDA) update can be
written as X(k+1) = X(k) — ηξ(X(k) ).
We will use J to denote the Jacobian ofξ, i.e. J ≡ Vξ =	Vx21x1g
—Vx2x1g
. Note that unlike
the Hessian in standard optimization, J is not symmetric, due to the negative sign in ξ. When clear
from the context, we often omit dependence on X when writing ξ, J, g, H, and other functions. Note
that ξ, J, and H are defined for a given objective g - we omit this dependence as well for notational
clarity. We will always assume g is sufficiently differentiable whenever we take derivatives. In
particular, we assume second-order differentiability in Section 3.
We will also use the following non-standard definition for notational convenience:
Definition 2.5 (Higher-order Lipschitz). A function g : Rd → R is (L1, L2, L3)-Lipschitz if for
all x ∈ Rd, ∣∣ξ(x)∣∣ ≤ Li and ∣∣Vξ(x)∣∣ ≤ L2, and for all x,y e Rd, ∣∣Vξ(x) — Vξ(y)∣∣ ≤
L3 ||X — y||.
Notions of convergence in min-max problems The convergence rates in this paper will apply to
min-max problems where g satisfies the following assumption:
Assumption 2.6. All critical points of the objective g are global min-maxes (i.e. they satisfy (1)).
In other words, we prove convergence rates to min-maxes in settings where convergence to criti-
cal points is necessary and sufficient for convergence to min-maxes. This assumption is true for
convex-concave settings, but also holds for some nonconvex-nonconcave settings, as we discuss in Ap-
pendix E. This assumption allows us to measure the convergence of our algorithms to -approximate
critical points, defined as follows:
Definition 2.7. Let E ≥ 0. A point X ∈ Rd X Rd is an E-approximate criticalpointif ∣∣ξ(x)∣∣ ≤ e.
Convergence to approximate critical points is a necessary condition for convergence to local or
global minima, and it is a natural measure of convergence since the value of g at a given point gives
no information about how close we are to a min-max. Our main convergence rate results focus
on this first-order notion of convergence, which is sufficient given Assumption 2.6. We discuss
notions of second-order convergence and ways to adapt our results to the general nonconvex setting
in Appendix A.
3
Under review as a conference paper at ICLR 2020
2.2	Related work
Asymptotic and local convergence Several recent papers have given asymptotic or local con-
vergence results for min-max problems. Mertikopoulos et al. (2019) show that the extragradient
(EG) algorithm converges asymptotically in a broad class of problems known as coherent saddle
point problems, which include quasiconvex-quasiconcave problems. However, they do not prove
convergence rates. For more general smooth nonconvex min-max problems, a number of different
papers have given local stability or local asymptotic convergence results for various algorithms, which
we discuss in Appendix A.
Non-asymptotic convergence rates Work on global non-asymptotic last-iterate convergence rates
has been limited to very restrictive settings. A classic result by Rockafellar (1976) shows a linear
convergence rate for the proximal point method in the bilinear and strongly convex-strongly concave
cases. Another classic result, by Tseng (1995), shows a linear convergence rate for the extragradient
algorithm in the bilinear case. Liang & Stokes (2019) show that a number of algorithms achieve
a linear convergence rate in the bilinear case, including Optimistic Mirror Descent (OMD) and
Consensus Optimization (CO). They also show that SGDA obtains a linear convergence rate in the
strongly convex-strongly concave case. Mokhtari et al. (2019) show that OMD and EG obtain a
linear rate for the strongly convex-strongly concave case, in addition to proving similar results for
generalized versions of both algorithms. Finally, Du & Hu (2019) show that SGDA achieves a linear
convergence rate for a convex-strongly concave setting with a full column rank linear interaction
term.1
Non-uniform average-iterate convergence A number of recent works have studied the conver-
gence of non-uniform averages of iterates, which can be viewed as an interpolation between the
standard uniform average-iterate and last-iterate. We discuss these works further in Appendix B.
3	Hamiltonian Gradient Descent
Our main algorithm for finding saddle points of g(x1 , x2) is called HAMILTONIAN GRADIENT
DESCENT (HGD). HGD consists of performing gradient descent on a particular objective function H
that we refer to as the Hamiltonian, following the terminology of Balduzzi et al. (2018).2 If we let
ξ := (∂∂g, - ∂∂dg) be the vector of (appropriately-signed) partial derivatives, then the Hamiltonian is:
H(X) := 2kξ(x)k2 = 2 (k∂∂g(x)k2 + k∂g(x)k2).
Since a critical point occurs when ξ(x) = 0, we can find a (approximate) critical point by finding a
(approximate) minimizer of H. Moreover, under Assumption 2.6, finding a critical point is equivalent
to finding a saddle point. This motivates the HGD update procedure on x(k) = (x(1k) , x(2k)) with
step-size η > 0:
x(k+1) = x(k) - ηVH(x(k)),	(2)
HGD has been mentioned in Mescheder et al. (2017); Balduzzi et al. (2018), and it strongly resembles
the Consensus Optimization (CO) approach of Mescheder et al. (2017). The HGD update requires a
Hessian-vector product because VH = ξ>J, making HGD a second-order iterative scheme. However,
Hessian-vector products are cheap to compute when the objective is defined by a neural net, taking
only two gradient oracle calls (Pearlmutter, 1994). This makes the Hessian-vector product oracle a
theoretically appealing primitive, and it has been used widely in the nonconvex optimization literature.
Since Hessian-vector product oracles are feasible to compute for GANs, many recent algorithms for
local min-max nonconvex optimization have also utilized Hessian-vector products (Mescheder et al.,
2017; Balduzzi et al., 2018; Adolphs et al., 2019; Letcher et al., 2019; Mazumdar et al., 2019).
1Specifically, they assume g(x1, x2) = f(x1) + x2T Ax1 - h(x2), where f is smooth and convex, h is
smooth and strongly convex, and A has full column rank. We make a brief comparison of our work to that of Du
& Hu (2019) for the convex-strongly concave setting in Appendix D.
2We note that the function H is not the Hamiltonian as in the sense of classical physics, as we do not use the
symplectic structure in our analysis, but rather we only perform gradient descent on H.
4
Under review as a conference paper at ICLR 2020
To the best of our knowledge, previous work on last-iterate convergence rates has only focused on
how algorithms perform in three particular cases: (a) when the objective g is bilinear, (b) when g is
strongly convex-strongly concave, and (c) when g is convex-strongly concave (Tseng, 1995; Liang
& Stokes, 2019; Du & Hu, 2019; Mokhtari et al., 2019). The existence of methods with provable
finite-time guarantees for settings beyond the aforementioned has remained an open problem. This
work is the first to show that an efficient algorithm, namely HGD, can achieve non-asymptotic
convergence in settings that are not strongly convex or linear in either player.
3.1	Convergence Rates for HGD
We now state our main theorems for this paper, which show convergence to critical points. When
Assumption 2.6 holds, we get convergence to min-maxes. All of our main results will use the
following multi-part assumption:
Assumption 3.1. Let g : Rd × Rd → R.
1.	Assume a critical point for g exists.
2.	Assume g is (L1, L2, L3)-Lipschitz and let LH = L1L3 + L22.
Our first theorem shows that HGD converges for the strongly convex-strongly concave case. Although
simple, this result will help us demonstrate our analysis techniques.
Theorem 3.2. Let Assumption 3.1 hold and let g(x1, x2) be α-strongly convex in x1 and α-strongly
concave in x2. Then HGD with step-size η = 1/LH starting from some x(0) ∈ Rd × Rd will have
thefollowing convergence rate: ∣∣ξ(x(k))∣∣ ≤(1 一 卉)/ ∣∣ξ(x(0))∣∣.
Next, we show that HGD converges when g is linear in one of its arguments and the cross-derivative
is full rank. This setting allows a slightly tighter analysis compared to Theorem 3.4.
Theorem 3.3. Let Assumption 3.1 hold and let g(x1, x2) be L-smooth in x1 and linear in x2, and
assume the cross derivative V2x ,x g is full rank with all singular values at least γ > 0 for all
x ∈ Rd × Rd. Then HGD with step-size η = 1/LH starting from some x(0) ∈ Rd × Rd will have the
following convergence rate: ξ(x(k)) ≤ 1 -
Y4
(2γ2 + L2 )Lh
k/2
k/2 ξ(x(0))
Finally, we show our main result, which requires smoothness in both players and a large, well-
conditioned cross-derivative.
Theorem 3.4. LetAssumption 3.1 hold and let g be L-smooth in xι and L-smooth in x2. Let μ2 =
minχ1,x2 λmin((VX2χ2g(x1,x2))2) and P = minχιχ λmin((VXiχιg(x1,x2))2), and assume the
cross derivative VX® g is full rank with all singular values lower bounded by γ > 0 and upper
bounded by Γ for all x ∈ Rd × Rd. Moreover, let the following “sufficiently bilinear” condition hold:
(γ2 + ρ2)(μ2 + γ2) - 4L2Γ2 > 0.
Then HGD with step-size η = 1/LH starting from some x(0) ∈ Rd × Rd will satisfy
∣∣ξ(χ(k))∣∣≤ (ι- (γ2+PY2+p2⅛)-Γ2),2∣∣ξ(χ⑼)∣∣.
(3)
(4)
As discussed above, Theorem 3.4 provides the first last-iterate convergence rate for min-max problems
that does not require strong convexity or linearity in either input. For example, the objective
g(x1, x2) = f(x1) + 3Lx1>x2 - h(x2), where f and h are L-smooth convex functions, satisfies the
assumptions of Theorem 3.4 and is not strongly convex or linear in either input. We discuss a simple
example that is not convex-concave in Appendix E. We also show how our results can be applied to
specific settings, such as the Dirac-GAN, in Appendix G.
The “sufficiently bilinear” condition (3) is in some sense necessary for our linear convergence rate
since linear convergence is impossible in general for convex-concave settings, due to lower bounds
on convex optimization (Agarwal & Hazan, 2018; Arjevani et al., 2017). We give some explanations
for this condition in the following section. In simple experiments for HGD on convex-concave and
nonconvex-nonconcave objectives, the convergence rate speeds up when there is a larger bilinear
component, as expected from our theoretical results. We show these experiments in Appendix K.
5
Under review as a conference paper at ICLR 2020
3.2	Explanation of “sufficiently bilinear” condition
In this section, we explain the “sufficiently bilinear” condition (3). Suppose our objective is
g(x1,x2) = g(x1,x2) + cχ>χ2 for a smooth function g. Then for sufficiently large values of
c (i.e. g has a large enough bilinear term), we see that g satisfies (3). To see this, note that if we have
γ4 > 4L2Γ2, then condition (3) holds. Let γ0 and Γ0 be lower and upper bounds on the singular values
of VXiχ2g. Then it suffices to have (γ0 + c)4 > 4L2(Γ0 + c)2, which is true for C = 3max{L, Γ0}
(i.e. c = O(L) suffices).
This condition is analogous to the case when we use SGDA on the objective g(χι, χ2) = g(χι, χ2) +
c ∣∣χι∣∣2 一 c∣∣χ2∣∣2 for L-smooth convex-concave g. According to Liang & Stokes (2019), SGDA
「一匚2
will converge at arate of roughly L log(1∕e) for L-smooth and c-strongly convex-strongly concave
objectives.3 For c = 0, SGDA will diverge in the worst case. For c = o(L), we get linear convergence,
but it will be slow because Lcc is large (this can be thought of as a large condition number). Finally,
for c = Ω(L), we get fast linear convergence, since L+c = O(1). Thus, to get fast linear convergence
it suffices to make the problem “sufficiently strongly convex-strongly concave” (or “sufficiently
strongly monotone”).
Theorem 3.4 and condition (3) show that there exists another class of settings where we can achieve
linear rates in the min-max setting. In our case, if we have an objective g(χ1,χ2) = g(χ1,χ2)+cχ>χ2
for a smooth function g, we will get linear convergence if IlVxlx2gk ≤ δL and C ≥ 3(1 + δ)L, which
ensures that the problem is “sufficiently bilinear.” Intuitively, it makes sense that the “sufficiently
bilinear” setting allows a linear rate because the pure bilinear setting allows a linear rate.
Another way to understand condition (3) is that it is a sufficient condition for the existence ofa unique
critical point in a general class of settings, as we show in the following lemma, which we prove in
Appendix F.
Lemma 3.5. Let g(x1, x2) = f(x1) + cx1>x2 一 h(x2) where f and h are L-smooth. Moreover,
assume that V2f(x1) and V2h(x2) each have a 0 eigenvalue for some x1 and x2. If (3) holds, then
g has a unique critical point.
4	Proof sketches for HGD convergence rate results
In this section, we go over the key components of the proofs for our convergence rates from Section 3.1.
Recall that the intuition behind HGD was that critical points (where ξ(x) = 0) are global minima
of H = 2 ∣∣ξ∣∣2. On the other hand, there is no guarantee that H is a convex potential function,
and a priori, one would not assume gradient descent on this potential would find a critical point.
Nonetheless, we are able to show that in a variety of settings, H satisfies the PL condition, which
allows HGD to have linear convergence. Proving this requires proving properties about the singular
values of J ≡ Vξ .
4.1	The Polyak-Lojasiewicz condition for THE Hamiltonian
We begin by recalling the definition of the PL condition.
Definition 4.1 (Polyak-Lojasiewicz (PL) condition Polyak (1963); Lojasiewicz (1963)). A function
f: Rd → R satisfies the PL condition with parameter α > 0 if for all X ∈ Rd, 1 ||Vf (x)||2 ≥
a(f (X) — minx*∈Rd f (x*))∙
The PL condition is well-known to be the weakest condition necessary to obtain linear convergence
rate for gradient methods; see for example Karimi et al. (2016). We will show that H satisfies the PL
condition, which allows us to use the following classic theorem.
Theorem 4.2 (Linear rate under PL Polyak (1963); Lojasiewicz (1963)). Let f : Rd → R be
L-smooth and let x* ∈ arg minx∈Rd f (x). Suppose f satisfies the PL condition with parameter
α. Then if we run gradient descent from x(0) ∈ Rd with step-size 1, we have: f (x(k)) — f (x*) ≤
(1 - L )k (f (x(0)) — f (x*)).
3The actual rate is C log(1 /e), for some parameter β that is at least (L + c)2.
6
Under review as a conference paper at ICLR 2020
For completeness, we provide the proof of Theorem 4.2 in Appendix C.
All of our results use Assumption 3.1, so we are guaranteed that g has a critical point. This implies
that the global minimum of H is 0, which allows us to prove the following key lemma:
Lemma 4.3. Assume we have a twice differentiable g(x1, x2) with associated ξ, H, J. Let c > 0. If
J J >	αI for every x, then H satisfies the PL condition with parameter α.
Proof. Consider the squared norm of the gradient of the Hamiltonian:
2kVHk2 = 1 k J>ξk2 = 1 hξ, (JJ>)ξi ≥ 2 I∣ξ∣∣2 = αH.
The proof is finished by noting that H(X) = 0 when X is a critical point.	口
To use Theorem 4.2, we will also need to show that H is smooth, which holds when g is (L1, L2, L3)-
Lipschitz. The proof of Lemma 4.4 is in Appendix H.
Lemma 4.4. Consider any g(X1, X2) which is (L1, L2, L3)-Lipschitz for constants L1, L2, L3 > 0.
Then the Hamiltonian H(X) is (L1L3 + L22)-smooth.
To use Lemma 4.3, we will need control over the eigenvalues of JJ>, which we achieve with the
following linear algebraic lemmas. We provide their proofs in Appendix H.
Lemma 4.5. Let H = (-M1> -M2) and let E ≥ 0∙ If Mi * CI and M2 Y -EI, then for all
eigenvalues λ of HH>, we have λ > 2.
Lemma 4.6. Let H = -CA> C0 , where C is square and full rank. Then if λ is an eigenvalue of
σ41in,
(C)
HH>, then we must have λ ≥ 2仃2 (C)+∣∣a∣∣2 ∙
4.2	Proof sketches for Theorems 3.2, 3.3, and 3.4
We now proceed to sketch the proofs of our main theorems using the techniques we have described.
The following lemma shows it suffices to prove the PL condition for H for the various settings of our
theorems:
Lemma 4.7. Given g : Rd × Rd → R, suppose H satisfies the PL condition with parameter α and
is LH -smooth. Then ifwe use HGD starting from some X(0) ∈ Rd × Rd with step-size η = 1/LH,
then we have the following:
∣∣ξ(χ(k))∣∣ ≤ (1- LH)k/2 归X(O))I∣.
Proof. Since H satisfies the PL condition with parameter α and H is LH-smooth, we know by
Theorem 4.2 that gradient descent on H with step-size 1/LH converges at a rate of H(X(k) ) ≤
(1 -茂)kH(X(O)). Substituting in for H gives the lemma.	口
It remains to show that H satisfies the PL condition in the settings of Theorems 3.2 to 3.4. First, we
show the result for the strongly convex-strongly concave setting of Theorem 3.2.
Lemma 4.8 (PL for the strongly convex-strongly concave setting). Let g be c-strongly convex in X1
and c-strongly concave in X2. Then H satisfies the PL condition with parameter = c2.
Proof. We apply Lemma 4.5 with H = J. Since g is c-strongly-convex in X1 and c-strongly
concave in X2 we have Mi = NQxIxI g * CI and M2 = -^χ2χ2g * CI. Then the magnitude of the
eigenvalues of J is at least c. Thus, JJ> c2I, so by Lemma 4.3, H satisfies the PL condition with
parameter c2.	口
Next, we show that H satisfies the PL condition for the nonconvex-linear setting of Theorem 3.3. We
prove this lemma in Appendix H.4 by using Lemma 4.6.
7
Under review as a conference paper at ICLR 2020
Lemma 4.9 (PL for the smooth nonconvex-linear setting). Let g be L-smooth in x1 and lin-
ear in x2. Moreover, for all X ∈ Rd X Rd, let VXιχ2g(x1,x2) be full rank and square with
σmin(VXιχ2 g(xι, x2)) ≥ γ. Then H satisfies the PL condition with parameter α = ?’+ 匕2.
Finally, we prove that H satisfies the PL condition in the nonconvex-nonconvex setting of Theorem 3.4.
The proof for Lemma 4.10 is in Appendix H.5, and it uses Lemma H.2, which is similar to Lemma 4.6.
Lemma 4.10 (PL for the smooth nonconvex-nonconvex setting). Let g be L-smooth in x1 and
L-smooth in x2. Also, let V2x1 x2 g be full rank and let all of its singular values be lower bounded by
γ and upper bounded by Γfor all x ∈ Rd × Rd. Let ρ2 = minx1,x2λmin ((V2x x g(x1, x2))2) and
μ = minχ1,χ2 λmin((VX2χ2 g(xι, x2))2). Assume the following condition holds:
(γ2 + ρ2)(γ2 + μ2) - 4L2Γ2 > 0.
Then H satisfies the PL condition with parameter α
(γ2+ρ2)(γ2+μ2)-4L2Γ2
2γ2+ρ2+μ2
Combining Lemmas 4.8 to 4.10 with Lemma 4.7 yields Theorems 3.2 to 3.4.
5 Extensions of HGD results
Stochastic HGD Our results above also imply rates for stochastic HGD, where the gradient VH
in (2), is replaced by a stochastic estimator v of VH such that E[v] = VH. Since we show that H
satisfies the PL condition with parameter α in different settings, We can use Theorem 4 in Karimi et al.
(2016) to show that stochastic HGD converges at a O(1∕√k) rate in the settings of Theorems 3.2
to 3.4, including the “sufficiently bilinear” setting. We prove Theorem 5.1 in Appendix I.
Theorem 5.1. Let Assumption 3.1 hold and suppose H satisfies the PL condition with parameter α.
Suppose we use the update x(k+1) = x(k) - ηkv(x(k)), where v is a stochastic estimate of VH such
that E[v] = VH and E[∣∣v(x(k))k2] ≤ C2 for all x(k). Then if we use ηk = 2J(k+l)2, we have the
following convergence rate: E[kξ(x(k))k] ≤ ∖ LHC .
Consensus Optimization The Consensus Optimization (CO) algorithm of Mescheder et al. (2017)
is as follows:
x(k+1) = x(k) - η(ξ(x(k)) + γVH(x(k)))	(5)
where γ > 0. This is essentially a weighted combination of SGDA and HGD. Mescheder et al. (2017)
remark that while HGD has poor performance on nonconvex problems in practice, CO can effectively
train GANs in a variety of settings, including on CIFAR-10 and celebA. While they frame CO as
SGDA with a small modification, they actually set γ = 10 for several of their experiments, which
suggests that one can also view CO as a modified form of HGD.
Using this perspective, we prove Theorem 5.2, which implies that we get linear convergence of
CO in the same settings as Theorems 3.2 to 3.4 provided that γ is sufficiently large (i.e. the HGD
update is large compared to the SGDA update). The key technical component is showing that HGD
still performs well even with a certain kind of small arbitrary perturbation. Previously, Liang &
Stokes (2019) proved that CO achieves linear convergence in the bilinear setting, so our result greatly
expands the settings where CO has provable non-asymptotic convergence. We prove Theorem 5.2 in
Appendix J.
Theorem 5.2. Let Assumption 3.1 hold. Letg be Lg smooth and suppose H satisfies the PL condition
with parameter α. Then if we update some x(0) ∈ Rd × Rd using the CO update (5) with step-size
η = 4工；工 and Y = 4Lg, we get the following convergence:
k
1 - 4LH)归X(O))II .	(6)
We also show that CO converges in practice on some simple examples in Appendix K.
8
Under review as a conference paper at ICLR 2020
References
Jacob Abernethy, Kevin A Lai, Kfir Y Levy, and Jun-Kun Wang. Faster rates for convex-concave
games. Conference on Learning Theory (COLT), 2018.
Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. In Artificial Intelligence and Statistics (AISTATS),
2019.
Naman Agarwal and Elad Hazan. Lower bounds for higher-order convex optimization. In Conference
on Learning Theory (COLT), 2018.
Yossi Arjevani, Ohad Shamir, and Ron Shiff. Oracle complexity of second-order methods for smooth
convex optimization. Mathematical Programming, pp. 1-34, 2017.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel.
The mechanics of n-player differentiable games. In International Conference on Machine Learning
(ICML), 2018.
George W Brown. Iterative solution of games by fictitious play. Activity analysis of production and
allocation, 13(1):374-376, 1951.
Nicolo Cesa-Bianchi and Gdbor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent
in min-max optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp.
9255-9265, 2018.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. In International Conference on Learning Representations (ICLR), 2018.
Simon S Du and Wei Hu. Linear convergence of the primal-dual gradient method for convex-concave
saddle point problems without strong convexity. In Artificial Intelligence and Statistics (AISTATS),
2019.
Yoav Freund and Robert E. Schapire. Adaptive Game Playing Using Multiplicative Weights. Games
and Economic Behavior, 29(1-2):79-103, October 1999.
Gauthier Gidel, Hugo Berard, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality
perspective on generative adversarial nets. International Conference on Learning Representations
(ICLR), 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 2672-2680, 2014.
James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games,
3:97-139, 1957.
Elad Hazan. Introduction to online convex optimization. Foundations and TrendsR in Optimization,
2(3-4):157-325, 2016.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the POlyak-IOjasieWiCZ condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive groWing of gans for
improved quality, stability, and variation. International Conference on Learning Representations
(ICLR), 2018.
Christian Kroer. First-order methods With increasing iterate averaging for solving saddle-point
problems. arXiv preprint arXiv:1903.10646, 2019.
9
Under review as a conference paper at ICLR 2020
Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rocktaschel, and Shimon Whiteson. Stable op-
ponent shaping in differentiable games. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=SyGjjsC5tQ.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. Artificial Intelligence and Statistics (AISTATS), 2019.
Lojasiewicz. A topological property of real analytic subsets (in french). Coll. du CNRS, Les equations
aux deriveespartielles, pp. 87-89,1963.
Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On finding local nash equilibria (and
only local nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.
Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial
regularized learning. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA), pp. 2703-2717. SIAM, 2018.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra(-
gradient) mile. In International Conference on Learning Representations (ICLR), 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in
Neural Information Processing Systems (NeurIPS), pp. 1825-1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In International Conference on Machine Learning (ICML), pp. 3478-3487,
2018.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. arXiv preprint
arXiv:1901.08511, 2019.
J v Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320, 1928.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
B. T. Polyak. Gradient methods for minimizing functionals (in russian). Zh. Vychisl. Mat. Mat. Fiz.,
pp. 643-653, 1963.
Julia Robinson. An iterative method of solving a game. Annals of mathematics, pp. 296-301, 1951.
R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on
control and optimization, 14(5):877-898, 1976.
Paul Tseng. On linear convergence of iterative methods for the variational inequality problem.
Journal of Computational and Applied Mathematics, 60(1-2):237-252, 1995.
Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay
Chandrasekhar. The unusual effectiveness of averaging in gan training. International Conference
on Learning Representations (ICLR), 2019.
10
Under review as a conference paper at ICLR 2020
A	General nonconvex min-max optimization
In standard nonconvex optimization, a common goal is to find second-order local minima, which are
approximate critical points where V2 f is approximately positive definite. Likewise, a common goal
in nonconvex min-max optimization is to find approximate critical points where an analogous second-
order condition holds, namely that V2x1x1g(x) is approximately positive definite and V2x2x2 g(x) is
approximately negative definite. Critical points where this second-order condition holds are called
local min-maxes. When Assumption 2.6 holds, all critical points are global min-maxes, but in
more general settings, we may encounter critical points that do not satisfy these conditions. Critical
points may be local min-mins or max-mins or indefinite points. A number of recent papers have
proposed dynamics for nonconvex min-max optimization, showing local stability or local asymptotic
convergence results (Mescheder et al., 2017; Daskalakis & Panageas, 2018; Balduzzi et al., 2018;
Letcher et al., 2019; Mazumdar et al., 2019). The key guarantee that these papers generally give is
that their algorithms will be stable at local min-maxes and unstable at some set of undesirable critical
points (such as local max-mins). This essentially amounts to a guarantee that in the convex-concave
setting, their algorithms will converge asymptotically and in the strictly concave-strictly convex
setting (i.e. where there is only an undesirable max-min), their algorithms will diverge asymptotically.
This type of local stability is essentially the best one can ask for in the general nonconvex setting, and
we show how to give similar guarantees for our algorithm in Section A.1.
A.1 Nonconvex extensions for HGD
While the naive version of HGD will try to converge to all critical points, we can modify HGD
slightly to achieve second-order stability guarantees as in various related work such as Balduzzi et al.
(2018); Letcher et al. (2019). In particular, we consider modifying HGD so that there is some scalar
α in front of the VH term as follows:
x(k+1) = x(k) - ηαVH(x(k))
(7)
We now present two ways to choose α. Our first method is inspired by the Simplectic Gradient
Adjustment algorithm of Balduzzi et al. (2018), which is as follows:
x(k+1) = x(k) - η(ξ(x(k)) - λA>ξ(xk))
(8)
where A is the antisymmetric part of J and λ = sgn hξ, Ji A>ξ, J . Balduzzi et al. (2018)
show that λ is positive when in a strictly convex-strictly concave region and negative in a strictly
concave-strictly convex region. Thus, if we choose α = λ = sgn hξ, Ji A>ξ, J , we can ensure
that the modified HGD will exhibit local stability around strict min-maxes and local instability around
strict max-mins. This follows simply because we will do gradient descent on H in the first case and
gradient ascent on H in the second case.
Another way to choose α involves using an approximate eigenvalue computation on V2x x g and
V2x2x2g to detect whether V2x1x1g is positive semidefinite and V2x2x2g is negative semidefinite (which
would mean we are in a convex-concave region). We set α = 1 if we are in a convex-concave region
and -1 otherwise, which will guarantee local stability around min-maxes and local instability around
other critical points. This approximate eigenvector computation can be done using a logarithmic
number of Hessian-vector products.
B	Background on non-uniform average iterates
A number of recent works have focused on the performance of a non-uniform average ofan algorithm’s
iterates. Iterate averaging can lend stability to an algorithm or improve performance if the algorithm
cycles around the solution. On the other hand, uniform averages can suffer from worse performance
in nonconvex settings if early iterates are far from optimal. Non-uniform averaging is a way to
achieve the stability benefits of iterate averaging while potentially speeding up convergence compared
to uniform averaging. In this way, one can view non-uniform averaging as an interpolation between
average-iterate and last-iterate algorithms.
11
Under review as a conference paper at ICLR 2020
One popular non-uniform averaging scheme is the exponential moving average (EMA). For an
algorithm with iterates z(0) , ..., z(T), the EMA at iterate t is defined recursively as
zEMA = βzEMA + (1 - β)zEMA
where ZEMA = Z⑼ and β < 1. A typical value for β is 0.999. Yazici et al. (2019) and Gidel et al.
(2019) show that uniform and EMA schemes can improve GAN performance on a variety of datasets.
Mescheder et al. (2018) and Karras et al. (2018) use EMA to evaluate the GAN models they train,
showing the effectiveness of EMA in practice.
In terms of theoretical results, Kroer (2019) studies saddle point problems of the form
minxι maxχ2 f (xι) + g(xι) + hKx1,x2i - h*(x2), where f is a smooth convex function, g and h
are convex functions with easily computable prox-mappings, and K is some linear operator. They
show that for certain algorithms, linear averaging and quadratic averaging schemes are provably at
least as good as the uniform average scheme in terms of iterate complexity. Abernethy et al. (2018)
show how linear and exponential averaging schemes can be used to achieve faster convergence rates
in some specific convex-concave games.
Overall, while non-uniform averaging is appealing for a variety of reasons, there is currently no
theoretical explanation for why it outperforms uniform averages or why it would converge at all in
many settings. In fact, one natural way to show convergence for an EMA scheme would be to show
last-iterate convergence.
C Proof of linear convergence rate under PL condition
Here we present a classic proof of Theorem 4.2.
Proof of Theorem 4.2.
〃x(k+1))- 〃x*)≤ f(x⑻)-〃x*)- 2L ∣∣vf(χ(k))∣∣2	(9)
≤ f(x(k))- f (χ*) - α f(x(k))- f (χ*))	(10)
L
=(1- L) (f(x(k)) - f(x*))	(11)
where the first line comes from smoothness and the update rule for gradient descent, the second
inequality comes from the PL condition. Applying the last line recursively gives the result. □
D Comparison of Theorem 3.4 to Du & Hu (2019)
In this section, we compare our results in Theorem 3.4 to those of Du & Hu (2019). Du & Hu (2019)
prove a rate for SGDA when g is L-smooth and convex in xι and L-smooth and μ-strongly concave in
x2 and v2x1x2g is some fixed matrix A. The specific setting they consider is to find the unconstrained
min-max for a function g : Rd1 × Rd2 → R defined as g(x1, x2) = f(x1) + x2>Ax1 - h(x2) where
f is convex and smooth, h is strongly-convex and smooth, and A ∈ Rd2×d1 has rank d1 (i.e. A has
full column rank).
Their rate uses the potential function Pt = λat + bt , where we have:
λ =2LRL + Γ2)
μγ2
ak = ∣∣x1k) - x"∣
bk = ∣∣χ2k)- χ2∣∣
(12)
(13)
(14)
where (x21, x22) is the min-max for the objective. Their rate (Theorem 3.1 in Du & Hu (2019)) is
Pk+1 ≤
(15)
12
Under review as a conference paper at ICLR 2020
for some constant c > 0. To translate this rate into bounds on ∣∣ξ∣∣, We can use the smooth-
ness of g in both of its arguments to note that 11 黑(χ1,χ2)∣∣ = 11 悬(xi, χ2)- 黑(x；,x3 )∣∣ ≤
L ∣ ∣χ1k) - χl∣∣ and likewise for χ2. So the rate on Pk translates into a rate on ∣∣ξ∣∣ with some
additional factor in front.
Their rate and our rate are incomparable - neither is strictly better. For instance when Y = Γ
is much larger than all other quantities, their rates simplify to(1 - O (号)),while ours go to
(1 - O (Lγ2)) / . While our convergence rate requires the sufficiently bilinear condition (3) to
hold, we do not require convexity in xι or concavity in x2. Moreover, we allow VX1x2g to change as
long as the bounds on the singular values hold whereas Du & HU (2019) require VX® g to be a fixed
matrix.
E Nonconvex-nonconcave setting where Assumption 2.6 and the
conditions for Theorem 3.4 hold
In this section we give a concrete example of a nonconvex-nonconcave setting where Assumption 2.6
and the conditions for Theorem 3.4 hold. We choose this example for simplicity, but one can easily
come up with other more complicated examples.
For our example, we define the following function:
j-3(x + ∏2)	forX ≤ -2
F(x) = < -3 cos x	for — ∏ < x ≤ π∏
I — cos x + 2x — π for x > π
The first and second derivatives of F are as follows:
- -3 for X ≤ - 2
F0(x) = < 3 sin x for — π < x ≤ π
Isin X + 2 for x > π
0
3 cos x
cos x
for X ≤ - 2
for - 2 < x ≤ 2
for x > 2
(16)
(17)
(18)
From Figure 2, we can see that this function is neither convex nor concave.
Our objective will be g(x1, g2) = F(x1) + 4x1>x2 - F(x2). Note that L = 3 because F00(x) ≤ 3
for all x. Also, γ = Γ = 4 since V2x x g = 4I .
First, we show that g satisfies Assumption 3.1. We see that g has a critical point at (0, 0). Moreover,
g is (L1, L2, L3)-Lipschitz for any finite-sized region of R2. Thus, if we assume our algorithm stays
within a ball of some radius R, the (L1, L2, L3)-Lipschitz assumption will be satisfied. Since our
algorithm does not diverge and indeed converges at a linear rate to the min-max, this assumption is
fairly mild.
Next, we show that g satisfies condition (3). Condition (3) requires γ4 > 4L2Γ2 for g. We see that
this holds because γ4 = 44 = 256 and 4Lr2 = 4 * 3 * 42 = 192.
Therefore, the assumptions of Theorem 3.4 are satisfied.
We can also show that this objective satisfies Assumption 2.6, so we get convergence to the min-max
ofg. We will show that g has only one critical point (at (0, 0)) and that this critical point is a min-max.
We first give a “proof by picture” below, showing a plot of g in Figure 3, along with plots of g(∙, 0)
and g(0, ∙) showing that (0,0) is indeed a min-max.
We can also formally show that (0, 0) is the unique critical point of g and that it is a min-max. We
prove this for completeness, although the calculations more or less amount to a simple case analysis.
13
Under review as a conference paper at ICLR 2020
Figure 2: Plot of nonconvex function F(x) defined in (16), as well as its first and second derivatives
Let us look at the derivatives of g with respect to x1 and x2 :
1, x2)
-3 + 4x2
3 sin x1 + 4x2
sin x1 + 2 + 4x2
for xι ≤ -2
for - 2 <xι ≤ 2
for xι > 2
(19)
1, x2)
3 + 4x1
-3 sin x2 + 4x1
- sin x2 + 2 + 4x1
for x2 ≤ - 2
for - 2 <x2 ≤ 2
for x2 > 2
(20)
Observe that if xι ∈ [-2, 2] then critical points of g must satisfy 3sin xι + 4x2 = 0, which implies
that x2 ∈ [-4, 3]. Likewise, if x2 ∈ [-∏, ∏], then critical points of g must have xι ∈ [-1, 3]. We
show that this implies that g only has critical points where xi and x2 are both in the range[-2, 2].
Suppose g had a critical point such that xi ≤ -∏. Then this critical point must satisfy χ2 = 3.
But from our observation above, if a critical point has χ2 = 4, then xi must lie in [-3, 3], which
contradicts xi ≤ - -∏.
Next, suppose g had a critical point such that xi > π∏. Then this critical point must satisfy x2 =
-i(Sinxi + 2), which implies that x2 ∈ [-44, 3]∙ But then by the observation above, xi must lie in
[-3, 4], which contradicts xi > ∏.
From this we see that any critical point of g must have xi ∈ [-π∏, ∏]. We can make analogous
arguments to show that any critical point of g must have x2 ∈ [-∏, ∏].
From this, we can conclude that all critical points of g must satisfy the following:
3 sin xi + 4x2 = 0
-3 sin x2 + 4xi = 0
(21)
(22)
14
Under review as a conference paper at ICLR 2020
Figure 3: Plot of nonconvex-nonconcave g(x1, x2) = F (x1) + 4x1>x2 - F (x2)
These equations imply the following:
x1
3sin X2
x2
-3sin xι
⇒ x1
⇒ x2
-Sin - - Sin x2
4	42
--sin ( - sin x2
4	42
(23)
(24)
(25)
(26)
That is, for all critical points of g, xι must be a fixed point of hi (x) = 3 sin (- 3 sin x) and x2 must
be a fixed point of h2(x) = -4 sin (∣ sin x). Since |h；(x)| < 1 and ∣h2(x)∣ < 1 always, hi and h2
are contractive maps, so they have only one fixed point each. Thus, g will only have one critical point,
namely the point (xi, x2) such that xi is the unique fixed point of hi and x2 is the unique fixed point
of h2.
Finally, we can observe that (0, 0) is a critical point of g, so it must be the unique critical point of g.
One can also see that this is a min-max by looking at the second derivatives of F in (18).
F Proof of Lemma 3.5
To prove Lemma 3.5, we will use the following lemma:
15
Under review as a conference paper at ICLR 2020
Figure 4: Plot of g(∙, 0). We can see that there is only one min and it occurs at xi = 0.
Figure 5: Plot ofg(0, x2). We can see that there is only one max and it occurs at x2 = 0.
16
Under review as a conference paper at ICLR 2020
Lemma F.1. Let g(x1, x2) = f(x1) + cx1>x2 - h(x2) where f and h are L-smooth. Then if c > L,
g has a unique critical point.
Proof of Lemma 3.5. Condition (3) is as follows:
(γ2 + ρ2)(μ2 + γ2) - 4L2Γ2 > 0.	(27)
Note that in our setting, Y = Γ = c. Next, observe that if V2f (xi) and V2h(χ2) each have a 0
eigenvalue for some x1 and x2, condition (3) reduces to:
c > 2L.	(28)
Then by Lemma F.1,we see that g must have a unique critical point.	□
Next, we prove Lemma F.1.
Proof of Lemma F.1. Suppose our objective is g(x1, x2) = f(x1) + cx1>x2 - h(x2) where f and h
are both L-smooth convex functions. Critical points of g must satisfy the following:
Vf(x1)+	cx2 = 0	(29)
-Vh(x2) +	cx1 = 0	(30)
⇒ x1 =	1 Vh(x2) c	(31)
⇒ x2 =	-CVf (CVh(X2))	(32)
In other words, x2 must be a fixed point of F(Z) = 一C Vf (C Vh(z)). The function F will have a
unique fixed point if it is a contractive map. We now show that for c > L, this is the case.
l|F(U)- F(v)ll =	C Vf (CVh(U)) 一 C vf (CVh(V))			(33)
≤ L ∙ C		LVh(U) - LVh(V)		(34)
L2 ≤ RI		u 一 v∣∣ < ∣∣u 一 v∣∣		(35)
where the inequalities follow from smoothness of f and h. An analogous property can be shown by
solving for x1 instead. Thus, if c > L, then g will have a unique fixed point.
Condition (3) is thus a sufficient condition for the existence of a unique critical point for the class of
objectives above.	□
G	Applications
In this section, we discuss how our results can be applied to various settings. One simple setting
is the Dirac-GAN from Mescheder et al. (2018), where g(x1, x2) = minx1 maxx2 f(x1>x2) - f(0)
for some function f whose derivative is always non-zero. When f(t) = t, the Dirac-GAN is just
a bilinear game, so HGD will converge globally to the Nash Equilibrium (NE) of this Dirac-GAN,
as shown in Balduzzi et al. (2018). Our results prove global convergence rates for HGD on the
Dirac-GAN even when a small smooth convex regularizer is added for the discriminator or subtracted
for the generator. Moreover, Lemma 2.2 of Mescheder et al. (2018) shows that the diagonal blocks of
the Jacobian are 0 at the NE for arbitrary f with non-zero derivative. As such, HGD will achieve the
convergence rates in this paper in a region around the NE for the Dirac-GAN for arbitrary f with
non-zero derivative even when a small smooth convex regularizer is added for either player.
Du & Hu (2019) list several applications where the min-max formulation is relevant, such as in
ERM problems with a linear classifier. Given a data matrix A, the ERM problem involves solving
minx '(Ax) + f (x) for some smooth, convex loss ' and smooth, convex regularizer f. This problem
has the saddle point formulation minχ maxy y>Ax 一 ' (y) + f (x). According to Du & HU (2019),
this formulation can be advantageous when it allows a finite-sum structure, reduces communication
complexity in a distributed setting, or allows some sparsity structure to be exploited. Our results show
that linear rates are possible for this problem if A is square, well-conditioned, and sufficiently large
compared to ` and f .
17
Under review as a conference paper at ICLR 2020
H Proofs for Section 4
In this section, we prove our main results about the convergence of HGD, starting with some key
technical lemmas.
H.1 Proof of Lemma 4.4
Proof. We have VH = ξ> J. Let u,v ∈ Rd X Rd. Then We have:
||VH(u) -VH(v)∣∣ = ∣∣ξ>(u)J(u) -ξ>(v)J(v)∣∣
= ξ>(u)J(u) -ξ>(u)J(v)+ξ>(u)J(v)-ξ>(v)J(v)
≤ ∣∣∣∣ξ>(u)J(u)-ξ>(u)J(v)∣∣∣∣+∣∣∣∣ξ>(u)J(v)-ξ>(v)J(v)∣∣∣∣
≤∣∣ξ(u)∣∣∙∣∣J(U)- J(v)|| + I∣ξ(u) - ξ(v)∣∣∙∣∣J(v)∣∣
≤ (LIL3 + L2) ||u - v||	口
H.2 Proof of Lemma 4.5
M2 + BBT
Proof. Note that HH> =	-(MM11B++BBBM2)T
-M1B -BM2	M1	-B 2
M22 + BTB = -BT	M2	.
NoW let Z = -MB1T -MB . It suffices to show that for any eigenvalue δ of Z, ∣δ∣ ≤ e. For the
sake of contradiction, let V be an eigenvalue of Z with eigenvalue δ such that ∣δ∣ ≤ e. Let V = (：1).
Since Zv = δv for ∣δ∣ ≤ E and Mi * EI and M2 Y -EI, we must have vι=0 and V = 0. Then
we have:
M1V1 - BV2	δ V1
M2V2 - B>V1	V2
This implies
(M1 - δI)V1 = BV2
(M2 - δI)V2 = B > V1
(36)
(37)
(38)
Let Mi = Mi 一 δI and let M2 = M2 一 δI. Note that M1 * 0 and M2 Y 0. Then we can write
vi = M-IBV2. Further, we can substitute into (38) to get
M2V2 = B~>M-^1Bv2	(39)
^⇒ -M-1B>M-1Bv2 = —V2	(40)
ii	i
In other words, V2 is an eigenvector of -M2-iB>Mi-iB with eigenvalue -1. Let A = -M2-i and
T = B>M-1B. Note that A is positive definite and T is PSD. Then we have:
AT = Ai/2(Ai/2TAi/2)A-i/2	(41)
Since Ai/2TAi/2 is PSD, and AT is similar to Ai/2TAi/2, we must have that all of the eigenvalues
of AT are nonnegative. This contradicts that V2 is an eigenvector of AT with eigenvalue -1.
Thus, all eigenvalues of Z must have magnitude greater than E.
□
H.3 Proof of Lemma 4.6
Proof. Suppose λ is an eigenvalue of HH> with eigenvector V = VVi . WLOG, suppose λ <
σm2 in(C). Since V is an eigenvector, we have:
A2 + CC>	-AC	Vi	Vi
-C>A	C>C	V2	= λ V2
(42)
18
Under review as a conference paper at ICLR 2020
Thus, we have:
(A2 +CC> - λI)v1 - ACv2 = 0	(43)
-C>Av1 + (C>C - λI)v2 = 0	(44)
Since λ < σm2 in(C), we have that C>C - λI is invertible, so we can write v2 = (C>C -
λI)-1C>Av1 from the (44). Plugging this into (43) gives:
(A2 +CC> - λI - AC(C>C - λI)-1C>A)v1 = 0	(45)
(A(I - C(C>C - λI)-1C>)A + CC> - λI)v1 = 0	(46)
Write the SVD of C as C = UΣV>. Then we have:
C(C>C - λI)-1C> = UΣV > (V ΣU>UΣV > - λI)-1V ΣU>	(47)
= UΣV>(V(Σ2 - λI)V >)-1V ΣU>	(48)
= UΣV >V -T (Σ2 - λI)-1V -1V ΣU>	(49)
= UΣ2(Σ2 - λI)-1U>	(50)
= UDU>	(51)
where the second line follows because V V > = I when C is full rank and where D is a diagonal
matrix such that Dii = σσiCC-λ.
Let M = I 一 D, so M is diagonal with Mii = σ2(Cj-λ. Then (46) becomes：
(AMA + CC> - λI)v1 = 0	(52)
This means T = AMA + CC> 一 λI has a 0 eigenvalue. A simple lower bound for the eigenvalues
of T is
λmin(T) ≥ - ∣∣A∣∣2 —-------ʌ + *n(C) - λ	(53)
σmin 一 λ
We will show that if λ < δ, where δ =。*也(。) + 11A^ — /(。・由 + lAl^∙)2 一。丸山,then
λmin (T) > 0, which is a contradiction. It suffices to show the following inequality:
一 ||A||2 -----∖ + σ2ιin(C) - λ > 0	(54)
σmin 一 λ
0 σ*m (C)- λ> ||A||2 2 λ	(55)
σmin 一 λ
g(σ2in(C) — λ)2 > ||A||2 λ	(56)
0 λ2-(2σmm(C) + ||A||2)λ + σ4ιm(C) > 0	(57)
(57) has zeros at the following values:
σ2un(C) + 呼 ±t 卜 min + 呼! 一 σ4ιm(C)	(58)
Since (57) is a convex parabola, if λ is less than both zeros, we will have proved (57). This is clearly
true if λ < δ.
As a last step, we can give a slightly nicer form of δ, using Lemma H.1. Letting X =。宗由(。)+ |A-
and C = σ41in(C), we have δ > 2~ σmC)(CIA∣∣2. So to reiterate, if λ < 2σ2 σmC)(CIA∣∣2 < δ, then
(57) holds, so T > 0, which contradicts (52).	口
Lemma H.1. For x ∈ (0, 1) and c ∈ (0, x2), we have:
x — px2 — c > —
2x
19
Under review as a conference paper at ICLR 2020
Proof.
x-
c
2x
□
H.4 Proof of Lemma 4.9
Proof. Let C(χι, χ2) = VX®g(χι, χ2). For all X ∈ Rd X Rd, C(χι, χ2) is square and full rank
by assumption, so we can apply Lemma 4.6 with H = J at each point x ∈ Rd × Rd, which gives
λ(JJT) ≥ 2σ∙ 2 (C(Xσmin)+(XX)) ”x x2 川2 . We have IIVx1x1 g(xι, χ2)∣∣ ≤ L SinCe g is smooth
2σmin(C (X1 ,x2)) 十 || Vχ]X] g(χ1,χ2)∣∣
in xi. Also,。宗山。(χ1,χ2)) ≥ γ. Then We have that JJ> 占 272+ 工21, so by Lemma 4.3, H
satisfies the PL condition with parameter 2Y工工2.	□
H.5 Proof of Lemma 4.10
To prove Lemma 4.10, we use the following lemma:
Lemma H.2. Let H =	-C> -B , where C is square and full rank. Moreover, let c =
(σm2 in(C) + λmin(A2))(λmin(B2) + σm2 in(C)) - σm2 ax(C)(||A|| + ||B||)2 and assume c > 0. Then
> A2 + CC>	-AC - CB
ifλ is an eigenvalue of HH> = -C>A - BC> B2 + C>C , we must have
λ≥
(σmin(C) + λmm(A2))(λmin(B2) + σ*m(C)) -。2^(。)(||A|| + ||B||)2
(2σmm(C)+ λmin(A2)+ λmin(B2))2
Proof of Lemma H.2. This proof resembles that of Lemma 4.6. Let v = vv1 be an eigenvector of
HH> with eigenvalue λ. Expanding HH>v = λv, we have:
(A2 +CC> - λI)v1 - (AC + CB)v2 = 0	(59)
-(C>A + BC>)v1 + (B2 +C>C-λI)v2 = 0	(60)
X------{-----}
M
⇒ v2 = M-1(C>A + BC>)v1	(61)
⇒ (-(AC + CB)M-1(C>A + BC>) +A2 +CC> - λI)v1 = 0	(62)
where M is invertible because C>C is positive definite and WLOG, we may assume that λ <
λmin(C>C) = σm2 in(C). We will show that if the assumptions in the statement of the lemma hold,
then we get a contradiction if λ is below some positive threshold. In particular, we show that the
following inequality holds for small enough λ (this inequality contradicts (62)):
σmm(C)-λ + λmin(A2) >σmaχ(C)(∣∣A∣∣ + ∣∣B∣∣)2∣∣M-1∣∣
U σmm(C) - λ + λmin(A2) >	⑵2；4(C)(C	X (Mil + l∣B∣l)2
min	λmin (B2 ) + σm2 in (C) - λ
U⇒ λ2 - (2σm2 in (C) + λmin(A2) + λmin(B2))λ+
(σmm(C) + λmin(A2))(λmin(B2) + σ2ιm(C))-。2_(。)(∣∣A∣∣ + ∣∣B∣∣)2 > 0
Letting b = 2σm2 in(C) + λmin(A2) + λmin(B2), we can solve for the zeros of the above equation:
b ± √b2 — 4c
λ =------------
(63)
2
Note that we have c > 0 by assumption, so this equation has only positive roots. Note also that
b2 > 4c, so the roots will not be imaginary. Then we see that if λ < δ = b-√22-4c, we get
20
Under review as a conference paper at ICLR 2020
a contradiction. Using Lemma H.1, We see that δ > C. So we've proven that λ < b gives a
COntradiCtion, so we must have λ ≥ C, i.e.
λ ≥ (σmm(C) + λmin(A2))(λmm(B2) + σ2ιm(C)) -。2^(。)(||A|| + ||B||)2
≥	2σ2lm(C) + λmin(A2) + λmin(B2)	^
□
Proof of Lemma 4.10. The proof is very similar to that of Lemma 4.9. Let C(x1, x2) =
▽X1X2g(x1,x2). For all X ∈ Rd X Rd, C(χ1,χ2) is square and full rank with bounds on its
singular values by assumption. Moreover, (3) holds, so we can apply Lemma H.2 with H = J at
each point x ∈ Rd × Rd. Using the fact that g is smooth in x1 and x2, this gives
λ(JJ>) ≥
(σ21in(C (x1,x2D + λmin(A2Mσmin(C (x1, χ2^ + 〃2 ) — 4乙2^^乂(。(χ1,χ2^
2σ2lm (C(X1,X2)) + λmin(A2)+ μ2
Using the bounds on the singular values of C(x1, x2), we have that JJ>
(γ2 + λmin(A2))(γ2+μ2)-4L2Γ2
2γ2 + λmin(A2 ) + μ2
(γ2 + λmin(A2))(γ2+μ2)-4L2Γ2
2γ2 + λmin(A2 ) + μ2
I, so by Lemma 4.3, H satisfies the PL condition with parameter
.	□
I Proof of Theorem 5.1
In this section, we prove Theorem 5.1. The proof leverages the following theorem from Karimi et al.
(2016).4
Theorem I.1 (Karimi et al. (2016)). Assume that f is L-smooth, has a non-empty solution set X *,
and satisfies the PL condition with parameter a. Let V be a stochastic estimate of Vf such that
E[v] = Vf. Assume E[∣∣v(x(k))k2] ≤ C2 for all x(k) and some C. Ifwe use the SGD update
x(k+I) = x(k) 一 ηkv(χ(k)) with ηk = 2∕k+l)2, then, we get a convergence rate of
LC2
E[f (Xk—*]≤ E	(64)
Ifinstead we use a COnStant ηk = η < *,then we obtain a linear convergence rate up to a solution
level that is proportional to η,
E[f(x(k)) — f*] ≤ (1 — 2αη)k[f(x(0)) — f*]+ ILCn	(65)
Now we can prove Theorem 5.1.
Proof of Theorem 5.1. If H satisfies the PL condition with parameter α, then we can apply Theo-
rem I.1 to the stochastic variant of HGD. since H* = 0, we get
E [ 1 kξ(x(k))k2l ≤ LHC2	(66)
2	2kα2
The theorem follows from Jensen’s inequality, which implies that E kξ(X(k))k ≤ E kξ(X(k))k2.
□
J	Proof of Theorem 5.2
In this section, we prove our main result about Consensus Optimization, namely Theorem 5.2.
The key technical component is showing that HGD still performs well even with small arbitrary
perturbations, as we show in the following theorem:
4The actual theorem in Karimi et al. (2016) is stated in a slightly different way, but it is equivalent to our
presentation.
21
Under review as a conference paper at ICLR 2020
Theorem J.1. Let x(k+1) = x(k) 一 NH(Xlk) 十 5 V(k) Where V(k) is some arbitrary vector such
that v(k) = ξ(x(k)). Let g be Lg -smooth and suppose H satisfies the PL condition with
parameter α. Let η =* and let n =4二二二.Then we get Ihefollowing convergence:
k
1 - 4⅛) ∣∣ξ(x(0))∣∣.	(67)
From Theorem J.1, it is simple to prove Theorem 5.2
ProofofTheorem 5.2. Note that the CO update (5) with Y = 4Lg is exactly the update in Theorem J.1
with v(k) = -ξ(χ(k)), so we get the desired convergence rate.	□
Our result treats SGDA as an adversarial perturbation even though this is not the case, which suggests
that this analysis may be improved. It would be nice if one could directly apply the PL-based analysis
that we used for HGD, but this does not seem to work for CO since CO is not gradient descent on
some objective.
Now we prove Theorem J.1.
Proof of Theorem J.1. Let χ(k+1/2) = χ(k) — ηVH(x(k)), so x(k+1) = x(k+1/2) + ηv v(k). From
(11) in the proof of Theorem 4.2 with η =*,we get
∣∣ξ(χ(k+1%∣∣ ≤ (ι 一 LH)1/2 ∣∣ξ(χ(k))∣∣ ≤ (1 - M)∣∣ξ(χ(k))∣∣.	(68)
Next, note that the triangle inequality and smoothness ofg imply:
∣∣ξ(χ(k+1))∣∣ ≤ ξ(χ(k+1∕2)) + ∣∣ξ(χ(k+1))-ξ(χ(k+1∕2))∣∣	(69)
≤ ξ(x(k+1/2)) + Lg∣∣χ(k+1)- χ(k+T∣	(70)
=∣∣ξ(χ(k+1∕2))∣∣+Lg l∣ηvv||	(71)
Using the above result and ∣∣V(k) ∣∣ = ∣∣ξ(x(k))∣∣, we get:
∣∣ξ(X(k+1))∣∣≤ (1-	+Lg ηv)∣∣ξ(X(k))∣∣	(72)
Setting ηv =4二二二 gives the result.	□
Note that for this result, we assume g is Lg smooth in X1 and X2 jointly, whereas in other parts of the
aper we assume g is smooth in X1 or X2 separately. If g is L-smooth in X1 and L-smooth in X2 and
∣V2x1x2g(X1, X2)∣∣ ≤ Lc for all X1, X2, then g will be L + Lc smooth.
K	Experiments
In this section, we present some experimental results showing how SGDA, HGD, and CO perform
on a convex-concave objective and a nonconvex-nonconcave objective. For our CO plots, γ refers
to the γ parameter in the CO algorithm. All of our experiments are initialized at (5, 5). The step-
size η for HGD and SGDA is always 0.01, while the step-size η for CO with γ = {0.1, 1, 10}
is {0.1, 0.01, 0.001} respectively to account for the fact that increasing γ increases the effective
step-size, so the η parameter needs to be decreased accordingly. The experiments were all run on a
standard 2017 Macbook Pro.
The main takeaways from the experiments are that CO with low γ will not converge if there is a large
bilinear term, while CO with high γ and HGD all converge for small and large bilinear terms. When
the bilinear term is large, CO with high γ and HGD both will converge in fewer iterations (for the
same step-size). We did not optimize for step-size, so it is possible this effect may change if the
optimal step-size is chosen for each setting.
22
Under review as a conference paper at ICLR 2020
K. 1 Convex-concave objective
The convex-concave objective we use is g(x1, x2) = f(x1) + cx1x2 - f(x2) where f(x)
log(1 + ex). We show a plot of f in Figure 6.
Figure 6: Plot of f(x) = log(1 + ex) with its first and second derivatives. This is a convex, smooth
function
When c = 3, SGDA converges, and when c = 10, SGDA diverges. We note that HGD and CO (for
large enough γ) tend to converge faster when c is larger.
K.1.1 SGDA CONVERGES (c = 3)
These plots show g when c = 3, so SGDA converges, as does CO with γ = 0.1.
23
Under review as a conference paper at ICLR 2020
-10	-5
-10
SGDA
=HGD
♦ Starting point
★ Min-max
(a)
——"SGDA
HGD
-15
-10
-5 f
0
Xi
♦ Starting point
★ Min-max
-10
(b)
Figure 7: SGDA vs. HGD for 300 iterations for g(x1, x2) = f(x1) + cx1x2 - f(x2) where
f(x) = log(1 + ex) and c = 3. SGDA slowly circles towards the min-max, and HGD goes directly
to the min-max.
24
Under review as a conference paper at ICLR 2020
(a)
1O∙3°∙°
■200
■300
CO with gamma
CO with gamma
CO with gamma
Starting point
Min-max
(b)
Figure 8: CO for 100 iterations with different values of γ for g(x1, x2) = f(x1) + cx1x2 - f(x2)
where f(x) = log(1 + ex) and c = 3. The γ = 0.1 curve slowly circles towards the min-max, while
the other curves go directly to the min-max.

25
Under review as a conference paper at ICLR 2020
(a)
HGD
— CO with gamma = 1
■■■ CO with gamma
♦ Starting point
⅛- Minmax
(b)
Figure 9:	HGD vs. CO for 100 iterations for g(x1, x2) = f(x1) + cx1x2 - f(x2)
log(1 + ex) and c = 3 with different values of γ.
where f (x)
26
Under review as a conference paper at ICLR 2020
K.1.2 SGDA DIVERGES (c= 10)
These plots show g when c = 10, so SGDA diverges, as does CO with γ = 0.1. Note that in this case,
CO with γ ≥ 1 and HGD both require very few iterations (typically about 2) to reach the min-max.
(a)
Starting point
Min-max
(b)
Figure 10:	SGDA vs. HGD for 150 iterations for g(x1, x2) = f(x1) + cx1x2 - f(x2) where
f (x) = log(1 + ex) and c = 10. SGDA slowly circles away from the min-max, while HGD goes
directly to the min-max.
27
Under review as a conference paper at ICLR 2020
Xi
■	" CO with gamma — 0.1
- CO with gamma = 1
-	-CO with gamma = 10
♦	Starting point
★	Min-max
(a)
1O∙3°∙°
CO with gamma
CO with gamma
CO with gamma
Starting point
Min-max
(b)
Figure 11: CO for 15 iterations with different values of γ for g(x1, x2) = f(x1) + cx1x2 - f(x2)
where f(x) = log(1 + ex) and c = 10. The γ = 0.1 curve makes a cyclic pattern around the
min-max, while the other curves go directly to the min-max.

28
Under review as a conference paper at ICLR 2020
10.0
2.5
0.0
Consensus Optimization vs. Hamiltonian Gradient Descent
L.<l-∖ -
■400
HGD
---CO with gamma = 1
...CO with gamma = 10
♦ Starting point
★ Min-max
[400
200
o g
-200
(b)
Figure 12: HGD vs. CO for 15 iterations with different values ofγ for g(x1, x2) = f(x1) + cx1x2 -
f (x2) where f(x) = log(1 + ex) and c = 10.
29
Under review as a conference paper at ICLR 2020
K.2 Nonconvex-nonconcave objective
The nonconvex-nonconcave objective we use is g(x1, x2) = F(x1) + cx1x2 - F(x2) where F is
defined as in (16) in Appendix E.
(—3(x + π2)	for X ≤ — π2
F(x) = —3 cos x	for — ∏2 < x ≤ ∏2	(73)
I — cos x + 2x — π for x > ∏2
We show a plot of F in Figure 13.
Figure 13: Plot of nonconvex function F(x) defined in (16), as well as its first and second derivatives
As in the convex-concave case, when c = 3, SGDA converges, and when c = 10, SGDA diverges.
Again, HGD and CO (for large enough γ) tend to converge faster when c is larger.
K.2. 1 SGDA CONVERGES (c = 3)
These plots show g when c = 3, so SGDA converges, as does CO with γ = 0.1.
30
Under review as a conference paper at ICLR 2020
(a)
10.0
lθ,-ŋɪθ-θ
τo∙)7≥一
-5.0 N
-2.5
---SGDA
-HGD
♦ Starting point
★ Min-max
■200
(b)
Figure 14: SGDA vs. HGD for 300 iterations for g(x1, x2) = F(x1) + cx1x2 - F(x2) where F(x)
is defined in (73) and c = 3. SGDA slowly circles towards the min-max, and HGD goes more directly
to the min-max.
31
Under review as a conference paper at ICLR 2020
Xi
(a)
÷200
-5.0
-2.5 nn
0.0
Xi
⅛300
ιo.∙υi°∙o
V-5.0
-7.5
CO with gamma
CO with gamma
CO with gamma
Starting point
Min-max
(b)
Figure 15: CO for 100 iterations with different values of γ for g(x1 , x2) = F (x1) + cx1x2 - F (x2)
where F (x) is defined in (73) and c = 3. The γ = 0.1 curve slowly circles towards the min-max,
while the other curves go more directly to the min-max.
32
Under review as a conference paper at ICLR 2020
(a)
■200
HGD
---CO with gamma = 1
...CO with gamma = 10
♦ Starting point
★ Minmax
(b)
Figure 16: HGD vs. CO for 100 iterations for g(x1 , x2)
defined in (73) and c = 3 with different values of γ.
F (x1) + cx1x2 - F (x2) where F (x) is
33
Under review as a conference paper at ICLR 2020
K.2.2 SGDA DIVERGES (c= 10)
These plots show g when c = 10, so SGDA diverges, as does CO with γ = 0.1. Note that in this case,
CO with γ ≥ 1 and HGD both require very few iterations (typically about 2) to reach the min-max.
(a)
.000
10.0
IOPlo.o
-5.0 j
-2.5
---SGDA
=HGD
♦ Starting point
★ Minmax
(b)
Figure 17: SGDA vs. HGD for 150 iterations for g(x1, x2) = F(x1) + cx1x2 - F(x2) where F(x)
is defined in (73) and c = 10. SGDA slowly circles away from the min-max, while HGD goes directly
to the min-max.
34
Under review as a conference paper at ICLR 2020
(a)
-5.0
-2.5 nn
0.0
Xi
W≠-5∙0
5.θf^s^7.5
7'5 ιo.υi°∙o
CO with gamma
CO with gamma
CO with gamma
Starting point
Min-max
(b)
Figure 18: CO for 15 iterations with different values of γ for g(x1, x2) = F(x1) + cx1x2 - F(x2)
where F (x) is defined in (73) and c = 10. The γ = 0.1 curve makes an erratic cycle around the
min-max, slowly diverging, while the other curves go directly to the min-max.
35
Under review as a conference paper at ICLR 2020
MI.../ / /
HGD
—∙ CO with gamma ≡ 1
... CO with gamma = 10
♦ Starting point
Min-max
COnSgnEUS OPtimization vs. Hamiltonian Gradient Descent
ιo.o
(a)
-400
HGD
CO with gamma = 1
CO with gamma
Starting point
Min-max
Moo
-200
(b)
Figure 19: HGD vs. CO for 15 iterations with different values ofγ for g(x1, x2) = F(x1) + cx1x2 -
F(x2) where F(x) is defined in (73) and c = 10.
36
Under review as a conference paper at ICLR 2020
K.3 Convergence of HGD for nonconvex-nonconvex objective with
different-sized bilinear terms
In this section, we look at the convergence of HGD for the same objective as discussed in the previous
section, namely g(x1, x2) = F(x1) + cx1x2 - F(x2) where F is defined as in (16) in Appendix E.
(-3(x + 2)
F(x) =	-3cosx
I — cos X + 2x — π
for x ≤ — 2
for — 2 < x ≤ 2
for x > 2
(74)
In this case, we will vary c to show that HGD converges faster for higher c and will not converge for
sufficiently low c.
O	5	IO	15	20
Number of iterations
Figure 20:	Distance to minmax for HGD iterates for different values of c in the objective g(x1 , x2)
F(x1) + cx1x2 - F(x2) where F(x) is defined in (73).
37
Under review as a conference paper at ICLR 2020
O	5	IO	15	20
Number of iterations
Figure 21:	Gradient norm for HGD iterates for different values of c in the objective g(x1, x2) =
F(x1) + cx1x2 - F(x2) where F(x) is defined in (73). Since all runs are initialized at (5, 5), when
c is increased, the initial gradient norm also increases. Nonetheless, HGD still converges faster for
the cases with higher c.
38