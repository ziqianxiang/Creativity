Under review as a conference paper at ICLR 2020
Deep Hierarchical-Hyperspherical
Learning (DH2L)
Anonymous authors
Paper under double-blind review
Ab stract
Regularization is known to be an inexpensive and reasonable solution to alleviate
the overfitting problem arising in inference models, including deep neural net-
works. In this paper, we propose hierarchical regularization which preserves the
semantic structure of a sample distribution. At the same time, this regularization
promotes diversity by imposing a distance between parameter vectors enlarged
within semantic structures. To generate evenly distributed parameters which is
considered less redundant, we constrain them to lie on hierarchical hyperspheres.
To define a hierarchical parameter space, we propose to reformulate the topology
space with multiple hyperspheres. On each hypersphere, a projection is parame-
terized by two individual parameters. Since maximizing a groupwise and pairwise
distance between points on hypersphere is nontrivial (generalized Thomson prob-
lem), we propose a new discrete metric integrated with a continuous metric. The
proposed method shows improved generalization performance on extensive exper-
iments using publicly available datasets (CIFAR-10, CIFAR-100, CUB200-2011,
and Stanford Cars), especially when the number of super-classes is large.
1 Introduction
Diversity promoting learning has been widely adopted via enlarging pairwise distances (Xie et al.,
2018; 2017a; Liu et al., 2018), increasing orthogonality (Xie et al., 2018), reducing covariance be-
tween parameters (Xie et al., 2017b), or reducing correlation on feature (Cogswell et al., 2016) to
improve generalization performance. Among them, diversity promoting regularization (Xie et al.,
2017a;b; Liu et al., 2018) by enforcing large diversity between projection parameters achieves a
reasonable performance without modifying the model structure. While optimizing the objective
function with a covariance matrix in (Xie et al., 2017b; 2018) is nontrivial, the diversity promoting
regularization via minimizing energy of parameters of deep neural networks has been proposed (Liu
et al., 2018) in a simpler way. By minimizing a pairwise distance between parameters on a hyper-
sphere with the known metric, they achieved the improved generalization performance.
Following an efficient regularization on the hypersphere, we explore further this direction with three
main concepts (hierarchical and hyperspherical learning with discrete metrics).
1)	Why hierarchical learning? Hierarchical inference explains human intelligence. In (Kurzweil,
2013), it states that “the neocortex contains about 300 million very general pattern recognizers,
arranged in a hierarchy”. Applying the hierarchy of multiple classes based on semantic taxonomy
is a natural choice to devise machine intelligence. Effectiveness of the hierarchical learning can be
found in (Verma et al., 2012).
2)	Why hyperspherical learning? Hypersphere is the set of points at the equidistance (radius) from a
given point (centroid) in a certain dimensional space. Due to the denominator in the unit-length nor-
malization (k^, W ∈ Rd+1), the angular distance defined on the hypersphere converges when the
magnitude of w goes infinity while Euclidean distance goes infinity. Due to this bounded property,
a hierarchical structure with multiple separated hyperspheres can be defined.
3)	Why discrete metric learning? If vector points form discontinuous series with discrete represen-
tation (e.g. multi-dimensional binary or ternary), they are isolated from each other with a certain
margin. This property may fit with a disconnected/groupwise manifold space problem. We note that
making points to be equidistributed where a pairwise distance is maximized is a nontrivial task. As
1
Under review as a conference paper at ICLR 2020
Figure 1: Multiple (hyper)spheres as quotient spaces of a topology space on Euclidean space might be found
by gluing process with identifying points. Those separated hyperspheres are assumed to be under the quotient
space conditions (Tu, 2010). Within an individual (hyper)sphere, projection parameters in deep neural networks
preserve a hierarchical structure. The space can be formed in a series: (a), (b), (c), and (d)
points in a discrete metric space are finite and isolated, search efforts to satisfy those constraints can
be reduced.
In this paper, we propose to apply a hierarchical structure to parameter regularization on the multiple
groupwise hyperspheres. In order to find an appropriate metric in this space, we explore a discrete
angular metric. We examine the proposed method on extensive experimental setups in terms of
datasets and deep network models.
2	Multiple separated hyperspheres
Samples observed from the real world may be on disconnected manifolds. In other words, the
disjoint union of those manifolds can generate the global manifold (Lee, 2000). In this section, we
decompose the one space into multiple spaces (manifolds) and re-define the spaces in a hierarchical
point of view.
2.1	Disconnected manifold via equivalent relations
Since it is not suitable to measure a pairwise distance between high dimensional vectors which have
the hierarchical structure in the same space, we construct another identification space where sub-
spaces (sub-manifolds) are isolated (via equivalence relation (Tu, 2010)). Denote d-sphere Sd to be
the set of points that satisfies Sd = {w ∈ Rd+1 : kwk = 1}. We construct multiple separated
hyperspheres using multiple identifying relations. In Figure 1, we use a center parameter and a
surface parameter to define a projection parameter vector w which constitutes a hypershpere space.
2.2	Prior distribution and regularization
To make parameter vectors uniformly distributed on the unit hypersphere, parameters are sampled
from the Gaussian normal distribution (Muller, 1959; Harman & Lacko, 2010). This is because the
normal distribution is spherically symmetric (Muller, 1959). In a Bayesian point of view, neural net-
works with Gaussian priors are known to induce l2-norm regularization (Vladimirova et al., 2019).
From two evidences, we know that enforcing parameters to have the Gaussian prior is important
for hyperspherical learning in neural networks. Note that a parameter which is calculated from the
difference arithmetic operation with two parameters on the normal Gaussian distribution is on the
normal difference distribution.
3	Method
In deep neural networks, the objective function J with regularization R in addition to a loss L,
JR(W) = L(x, W) + λR(W), is optimized to find the optimal W having a near minimum loss L,
arg minW JR(x,W), where x ∈ Rd0 denotes an input vector, W = {Wi ∈ Rdi-1×ci : Wi = {wj ∈
Rdi-1},j = 1,...,ci,i = 1,..., L} denotes a set of parameter matrices (i.e. neurons/kernels),
L denotes the number of layers, and λ > 0 is to control the degree of the regularization. For
a classification task, the cross entropy loss is used for the loss function L. We propose a new
regularization formulation R preserving a hierarchical parameter structure in Section 3.1.
2
Under review as a conference paper at ICLR 2020
Figure 2: A series of levelwise (l) spheres With a radius (ri) is shown: (a) A radius of an oveall area converges
to ιr0δ (= P∞ r0δl: the sum of radius series) as l goes to infinity where l denotes a level, r° is their initial
radius, and δ is the ratio between radiuses (rι,rι-ι), rrl < L (b) The radius of the overall area is bounded
to the initial radius r0 of spheres. This bears a resemblance to the process of repeat of Hypersphere packing
which arranges non-overlapping spheres within a containing space. (c) 2-sphere is defined following (b) which
is appropriate to model a hierarchical structure. This can be generalized with a hypersphere (Sd, d ≥ 3) in a
higher dimensional space.
3 .1 Regularization for Hierarchical and Hyperspherical Hypotheses
Denote w a parameter vector (called a projection parameter, an element of W at a certain layer) to
transform a given input into an embedding space defined in a Euclidean metric space: x ∈ Rd+1 7→
WTX ∈ R. By the definition of unit-length projection kW^, a new parameter vector W can be defined
on d-sphere: Sd = {W ∈ Rd+1 : ∣∣W∣∣ = 1} where k ∙ k denotes l2-norm and a center is zero. In
other word, the parameter W can be defined by a center parameter vector Wc ∈ Rd+1 and a surface
parameter vector Ws ∈ Rd+1 using an arithmetic operation: W := Ws — wc. We define the d-sphere
with the center and surface parameters: Sdwc = {Ws - Wc ∈ Rd+1 : ∣Ws - Wc ∣ = 1}. For a
notation simplicity, we use W instead of W hereafter. While we consider a radius equals to 1 for
simplicity, the parameter vector can have a radius r > 0.
3.1.1	Hierarchical parameterization via levelwise and groupwise parameters
We assume that the hierarchical structure consists of a levelwise structure with a notation (l) and a
groupwise structure with a notation g below. Through defining these structures serially, we explain
hierarchical parameters.
Levelwise structure The projection parameter W ∈ Rd+1 on Sdw can be defined with a levelwise
notation (l) as follows,
W(l) := Ws(l) — Wc(l)	(1)
where Sd (l) denotes the l-th level d-sphere centered at Wc(l) . In this paper, we define the parameter
wc
in a higher dimensional space than that shown in Figure 2.
In a levelwise setting, Ws(l) and Wc(l) at the current level (l) are additively represented based on a cen-
ter parameter of the previous level (l — 1): Wc(l-1) + ∆W(l) 7→ Wc(l), where Wc(l-1) = Pli-1 ∆W(i)
is the accumulated and ∆W(I) denotes a connecting parameter from W(IT) to W(l). By denoting
∆W(l) as W(l,l-1), the center vector at the l-th level is defined as, Wc(l) := Wc(l,l-1) +Wc(l-1), and the
surface vector as Ws(l) := Ws(l,l-1) + Wc(l-1). Both the center parameter and the surface parameter at
the current level are based on the center parameter at the previous level1. Hence, Eq. 1 is equivalent
to
W(l) = Ws(l,l-1) — Wc(l,l-1).	(2)
Note that we use a superscript (l, l — 1) to denote parameters at the l-th level connected from the
center parameter at the (l — 1)-th level.
Groupwise structure With a group notation gk, the center parameter in Eq. 2 can be rephrased as
Wc(l,,glk-1), on Sd (l,l-1) which denotes d-sphere of gk group at the l-th level. Each group constitutes
wc,gk
a group set at the l-th level g(l) := {gk }|kg=1| where g(l) ⊆ G(l), G(l) denotes a batch group set at
the l-th level, and | ∙ | denotes the cardinality. The group set g(l) at the l-th level is conditioned on
1As not every sample has a child sample, it might be more reasonable to branch from representative param-
eter or center parameter rather than from individual projection parameters.
3
Under review as a conference paper at ICLR 2020
a group set at the (l - 1)-th level, g(l-1) := {gk0}|kg0(=-11)| Where g(l-1) ⊆ G(l-1). By their group-
wise relation over levels, an adjacency indication2 3 * P(l,l-1)({G(l-1),G(l)}) ∈ {0,1}lG(l 1)l×lG(l)1
is given. Hence, the i-th parameter vector on Sd (ll-1) is defined as folloWs,
wc,,gk
W(l) := W(l,l-1) - W(l,l-1)	(3)
Wgk,i := Ws,gk,i - Wc,gk ,	(3)
where {Ws(l,,gl-,i1), Wc(,l,glk-1)} ∀i is calculated based on a center vector Wc(l,g-k10) at the (l - 1)-th level3,
i = 1, . . . , |gk|, and |gk| denotes the number of surface parameter vectors in the group gk.
3.1.2	Hierarchical regularization
In this section, we define a regularization term using the hierarchical parameters defined above. A
set of parameters {Ws(,lg,lk-1),wc(l,g,lk-1),wc(l,g-01)} ∈ W ∀gk, ∀gk0 where Ws(,lg,lk-1) := {ws(l,,glk-,i1)}|ig=k1|,
is an optimizing target of hierarchical regularization as follows:
R(W) := X λιRιW,gU, WC P(l，lT))+ X Ci (Wclgl-I), W?-1); P(I,lT))⑷
ll
where Rl works on individual spheres Sd (ll-1), λl > 0, and Cl aims to apply geometry-aware con-
wc,,gk
straints across spheres. Rl consists of two terms for regularization: 1) Rl,p for projection parameters
in the same group gk on Sd (ll-1) and 2) Rl c for center parameters across groups in the same level
wc,,gk
on Sd (l-1),
w0
c,gk0
where
Rl(Ws(,lg,lk-1),Wc(l,g,lk-1)) := Rl,p(Ws(,lg,lk-1), Wc(l,,glk-1)) + Rl,c(Wc(l,,glk-1)),
(5)
Rl,p(Ws(,lg,lk-1),wc(l,,glk-1)) :
Rl,c(wc(l,,glk-1) ) :
ɪ X	2	X d(W(I,l-1) w(l,lT))
G 乙 Gk(Gk - ι)	d(wgk,i , wgk,j ),
{gk ∈g(l) }	{i6=j∈gk}
_____2____ SX	d(w(l,l-1) w(l,l-1))
C(c - i) 2-^	d(wc,gi	, wc,gj ),
{gi 6=gj ∈g(l) }
(6)
(7)
where w(l,li-1) := ws(l,l-i1)
gk ,	s,gk ,
—
Wc(l,,glk-1), (Wg(lk,,lj-1) is similarly defined), G = |{gk ∈ g(l)}|,
Gk = |{i = j ∈ gk}|, C = |{gi = gj ∈ g(l)}|, and d(∙, ∙) denotes a distance metric between
parameters. The distance metric is defined in Section 3.2. We explain regularization terms for-
mulated using these parameters and pairwise distances further in Figure 4 of Appendix. When
a minibatch of inputs (mx : a set of inputs {xi} ) is given, the regularization term becomes:
E(R(W))= ∣mχ∣ Pmx R(W； mx).
The constraint term help construct geometry-aware relational parameters between different spheres
on the same level and on the across levels. Multiple constraints are defined as Cl := Pk λkCl,k,
where Cl,k is the k-th constraint between parameters at the l-th and the (l - 1)-th level, and λk > 0
is a Lagrange multiplier. We propose three constraints (defined in Appendix) in a geometric point
of view.
3.2	Discrete and continuous angular distance metric
Discrete metric is a good fit for the above groupwise definition. We expect that points projected
by parameters in a discrete metric space are isolated each other. In Figure 3, we show that discrete
distances of parameter pairs have different values while their continuous distances are same. Hence,
maximization of discrete distances of parameter pairs could help parameters distributed isolatedly
and diversely.
2This can be replaced with a probability model.
3The center vector wc(l,)gk indicates a representative vector of the group gk at the l-th level and itis equivalent
to a mean vector of W(I) - ∀i: μ(w(l) -) = τ‰ Plgk| W(I) ；. The center vector for the group gk can be
s,gk ,i	μ s,gk ,i	|gk ∣ Ai	s,gk ,i	g P gk
determined by a parameter vector at the (l - 1) - th level using an adjust factor (, || < 1): Wc(,l,glk-1) =
e ∙wglk-?, Where Wgk;1) ∈ Sw(I).
c,gk0
4
Under review as a conference paper at ICLR 2020
Using parameter vectors wi and wj in Rd+1 , we define a discrete dis-
tance metric using a sign function as follows:
1d
Dh ：= dfsign(wi (k)) ∙ sign(wj (k)),	(8)
k
where sign(x) ：= -11,, otihfexrw≥is0e , -1 ≤ Dh ≤ 1, and w =
{w(k) | ∀k = 1, . . . , d + 1} ∈ Rd+1}. This is a normalized version
of Hamming distance. For a ternary discrete, sign(x) ∈ {-1, 0, 1}
is used. In order to consider the discrete distance as an angular dis-
tance within [0,1], normalized one is defined as Dh01 := -Dh+1, where
0 ≤ Dh01 ≤ 1. The angular distance based on the above product can be
rephrased as θDh = Dh014 where 0 ≤ θDh ≤ 1.
As the discrete distance could underestimate to approximate the model
distribution, we merge the above discrete distance metric with contin-
UoUS angular distance metric (θ = ∏ arccos (口”,Ww』),0 ≤ θ ≤ 1)
into a single metric. We simply use the definition of Pythagorean means
which consist of the arithmetic mean (AM), the geometric mean (GM),
FigUre 3: While the pair-
wise angUlar distances Da
between a pair of vectors
{w1,w2} and {w2, w3}
are the same, the pairwise
discrete prodUct distances
Dh between vectors are dif-
ferent. To diversify a pa-
rameter space, the space
with sign coUld be effec-
tive to recognize their dif-
ference.
and the harmonic mean (HM). Pythagorean means Using a pair of angUlar distances are defined as
follows:
DAM ：= -^h , DGM ：= θDh θ, DHM ：= θj+θ .
(9)
In the angUlar distance5 Using a pair of angles {θDh , θ}, a reversed form 1 - D{θD ,θ} is adopted
to maximize an angle in the optimization formulation by minimization instead of (∙)-s where S =
1, 2, . . . which is Used in Thomson problem that Utilizes s-energy (BraUchart & Grabner, 2015). The
cosine similarity using the pair of angles can be defined as follows:
Dcos(AM) ：= COS (θDh2+θ∏), Dcos(GM) ：= Cos (Θd*∏), DCoS(HM) ：= Cos (^^^∏),	(10)
then the cosine similarity functions are normalized with cos(2)+1 to have a value within a range [0,1].
Finally, Pythagorean means of cosine similarities can be calculated as follows:
cos θDh π+cos θπ+2	(cos θDhπ +1)(cos θπ+1)	(cos θDh π+1)(cos θπ +1)
DAMcOs ：=	4	, DGMcOs ：=	4	, DHMcOs ：=	cos θDh +cos θ + 2	.
Metric functions defined in (9), (10), and (11) satisfy metric conditions: non-negativity, symme(1tr1y),
and triangle inequality. The distance using the above metric function between any two parameter
points is bounded, because the hypersphere is a compact manifold.
3.3	Gradients and Backpropagation
As the sign function is not differentiable at the value 0, we adopt alternative backpropagation func-
tion. We adopt straight-through estimator (STE) (Bengio et al., 2013) in the backward path of the
neural networks for the sign function in the discrete metric. The derivative of the sign function
is substituted with 1∣w∣≤1 in the backward pass, known as the saturated STE. As the derivative of
arccos(x) (√--χ2) is undefined at the value X = ±1, we apply clamping to the cosine function to
have x ∈ [-0.99, 0.99]6 where x = cos(θπ), and 0 ≤ θ ≤ 1.
4	Experiments
4.1	Experimental setup
Datasets We conduct experiments using four publicly available datasets including small size im-
ages (CIFAR-10 and CIFAR-100) and large size images (CUB200-2011 (Wah et al., 2011) and
4On the other hand, the angle can be considered as a cosine similarity directly, Dh := cos θDhπ. So as
to get the angular distance, it needs an arccosine function θ0h = ∏1 arccos Dh. In summary, for the angle
distance θ0h, either “Dh01 ” or "D'h01 = ∏1 arccos Dh” where 0 ≤ Dh01 ≤ 1, can be adopted.
5In 0 ≤ θ ≤ 1, the angle and its cosine value show an inverse relationship:0 ≤ θ ≤ 1 → 1 ≥ cos θπ ≥ -1.
6X = {0.99 ∙ lχ>0.99, x, -0.99 ∙ lχ<-0.99}
5
Under review as a conference paper at ICLR 2020
Stanford-Cars (Krause et al., 2013b), shortly CUB200 and Cars hereafter respectively). CIFAR-10
dataset is used to validate effectiveness of the proposed metric without hierarchical information.
Except CIFAR-10, we use two-level hierarchy pairs {parent, child} for the proposed hierarchical
regularization on three datasets. In Table 7 at Appendix, statistics of datasets in detail is provided.
Deep neural network models and training setting We used the deep residual neural network
(resnet) (He et al., 2016) with different sizes of parameters (weights) dependent on an image size of
datasets. For a small size input (32 × 32 pixels) of CIFAR-10 and CIFAR-100, we used resnet with
smaller number of parameters (light models, resnet-20 (0.29M) and resnet-110 (1.73 M)) so as not
to have redundant parameters leading to over-fitting. For a fine-grained input (224 × 224 pixels) of
CUB200 and Cars, the original resnet with larger number of parameters (heavy models7 8, Resnet-18
(11.28M) and Resnet-50 (23.91M)) is used.
In training of the deep neural network with the hierarchical regularization, we assume that the global
hierarchical structure is not given. Instead, stochastic or partial hierarchical structure is shown within
each mini-batch by hierarchy pairs. Mini-batches, 512 for light models and 256 for heavy models,
are used in the SGD optimizer. We applied the hierarchical regularization in the FC layer of the
resnet. Even though SGD is known as an unbiased estimation, a stochastic hierarchical structure
could affect the overall approximation performance upon the class distribution within the mini-
batch. Settings in more detail are provided in Appendix.
Baseline setting In terms of the regularization, we used three reference settings, denoted ‘base-
line’, ‘baseline+l2’, and ‘E’ for a reference. First, in ‘baseline’, we examined neural networks
without regularization. Then, in ‘baseline+l2’, we added l2 -norm minimization based regulariza-
tion (which is equivalent to a weight decay in SGD setting (Zhang et al., 2019)) via λf Pk kwkk,
where wk ∈ W and λf > 0 to ‘baseline’. Lastly, in ‘E’, energy minimization (Liu et al., 2018),
diversity promoting regularization without hierarchical information, via λe Pi6=j d(wi, wj), where
d(∙, ∙) is a pairwise distance and λe > 0, is added to baseline+l2. When the proposed hierar-
chical regularization (Eq. 5) is applied to the FC layer, we denoted it as ‘H’. Hence, the reg-
ularization is incrementally adopted to the baseline: l2+‘E’+‘H’. In terms of a distance metric,
we compared the proposed metric with other continuous metrics such as Euclidean distance with
the unit-length projection ('N-euclidean2', Pi=j k kWWij - ^^^∣∣-2)8, angular distance ('angular2',
Pi=j arccos( 八瑞蓝.||) 2) where '2’ is from RieSz s-energy shown higher accuracy, and cosine
Similarity CCoSine', 'P= 篇湍 k).
4.2	Results
Low resolution object classification In Table 1, we showed test accuracy (%) using different met-
rics with energy minimization regularization without the hierarchical regularization9 along a weight
decay (l2-norm) using CIFAR-10 and resnet-20. The ‘baseline’ method is examined without (× in
Table 1) or with (◦ in Table 1) the weight decay (l2). The discrete angular metric based regulariza-
tion (Dhter (ternary code), Dhbin (binary code), Dcos (HM), and DH0 M ) on the baseline can improve
the generalization performance in term of test accuracy compared to the other continuous metrics
such as N-euclidean2, angular2, and cosine. Due to the unit-length projection in N-euclidean2, their
performance is comparable to that other angular metrics. D0 denotes an arccosine adopted discrete
distance used as Dh0 (see footnote 4 in Section 3.2). Regularization methods are applied over all
layers except Batch Normalization (BN) layers. As explained in (van Laarhoven, 2017; Zhang et al.,
2019), l2-norm minimization based regularization (weight decay) showed much improvement due
to their effective learning rate in SGD.
As shown in Table 2, the regularization with proposed metrics shows better accuracy performance
than that of the baseline on both resnet-20 and resnet-110 using CIFAR-100 dataset. Comparing to
the baseline+l2, with energy minimization ‘E’ regularization, metrics such as Dhter, Dcos(AM), and
Dc0 os(HM) showed better performance than other metrics. If the proposed hierarchical ‘H’ regular-
7which is available at the pytorch library
8 - P (2-2 wi∙wj )-1
2-i=j (2	2kwikkwj k )
9This is because that hierarchy information is not included in CIFAR-10.
6
Under review as a conference paper at ICLR 2020
Table 1: Test accuracy (%) using different met-
rics on energy minimization without hierarchy
information along l2 -norm minimization (weight
decay) on CIFAR-10 and resnet-20.
metric \ l2	×		◦
baseline	90.34	92.21
N-euclidean2 90.93		92.35
angular2	90.47	92.38
cosine	90.53	92.40
ter Dh	90.67	92.48
bin Dh	90.67	92.48
Dcos(HM)	l	90.84	92.93
DHMcos	90.94	92.69
Table 3: Test accuracy (%) using different met- rics on energy minimization (‘E’) and hierarchical		
regularization (‘H’) on CUB200:		‘E’ / ‘E+H’ re-
spectively. metric \ db	ReSnet-18	Resnet-50
baseline	72:17	74.21
baseline+l2	72.29	74.05
N-euclidean2	72.61 /75.99	73.49 / 76.14
angular2	72.43 / 76.11	73.55 / 76.66
cosine	72.12/75.64	73.26 / 76.85
Dhter	72.58/75.99	73.57 / 76.37
bin Dh	72.55/76.21	73.57 / 76.99
DAM	73.04/76.02	73.88 / 75.95
DAMcos	72.31 / 76.14	73.59 / 77.32
DAMcos	72.28 / 75.37	72.42 / 74.12
DGM	72.90 / 76.35	74.16 / 75.30
DH0 Mcos	72.55 / 76.11	74.64 / 76.94
Dcos(HM)	72.55 / 76.32	72.86 / 76.56
Table 2: Test accuracy (%) using different met-
rics on energy minimization (‘E’) and hierarchical
regularization (‘H’) on CIFAR-100: ‘E’ / ‘E+H’
respectively. metric \ db	resnet-20	resnet-110
baseline	63:86	62.02
baseline+l2	68.03	72.90
N-euclidean2	67.59 / 68.65	73.95/73.96
angular2	67.83 / 67.76	74.40 / 73.89
cosine	68.11 / 68.45	73.37/73.37
ter Dh	68.44 / 68.68	73.73/73.97
bin Dh	68.52 / 68.69	73.97/74.26
DAM	68.58 / 68.86	73.43/73.50
Dcos(AM)	68.58 / 68.60	73.14 / 73.65
Dcos(AM)	67.57 / 68.36	73.14 / 73.72
Dcos(HM)	68.62 / 68.65	73.07 / 73.65
Table 4: Test accuracy (%) using different met-
rics with energy minimization (‘E’) and hierarchi-
cal regularization (‘H’) on Cars: ‘E’ / ‘E+H’ re-
spectively.
metric \ db	Resnet-18	Resnet-50
baseline	85:10	87.99
baseline+l2	85.58	87.92
N-euclidean2	85.48 / 85.56	87.96/87.97
angular2	85.11 / 85.13	88.34 / 87.85
cosine	85.57 / 85.73	88.01 /87.86
ter Dh	85.35 / 85.99	85.35/88.07
bin Dh	86.22 / 85.99	88.32/88.14
DAM	85.66 / 85.66	88.39/88.11
DAMcos	85.52 / 86.05	87.92 / 88.57
DAMcos	85.76 / 86.43	88.07 / 87.96
D0 Dcos(AM)	85.54 / 85.52	88.22 / 88.13
ization is applied, the generalization performance is further improved in most metric cases on both
resnet-20 and resnet-110. As the binary metric shows a better performance than that ternary, we
adopt binary discretization for proposed angular metrics (D∙, Dgs(∙), and D∙cos) in experiments.
Fine-grained visual categorization In this experiment, we used two large size image datasets
with a fine-grained category. One is with birds (CUB200) and another is with cars (Cars) that focus
on single species of objects. A hierarchy of CUB200 is defined by the academic expert on birds
whereas that of Cars is categorized manually based on a name of cars by a non-expert. Moreover,
the rate between the number of superclass (parent) per subclass of CUB200 (0.35) is much larger
than that of Cars (0.0459) (as shown in Table 7 at Appendix). That rate of Cars is even smaller than
that of CIFAR-100.
As shown in Table 3, the proposed hierarchical regularization significantly improved the test accu-
racy over all metrics on both Resnet-18 and Resnet-50. Compared to CUB200, as shown in Table 4,
the improvement of the proposed method is not that significant for Cars dataset. This is because
CUB200 dataset has not only better hierarchical categorization by the expert but also more diversi-
tified pairs between the superclass and the subclass.
Ablation study In Table 5, we showed how metrics affect the generalization performance using
resnet-20 and CIFAR-100 dataset. The proposed method with different proposed metrics showed
significantly improved performance compared to the baseline+l2 . Individual means (AM, GM, and
HM) show different improvement patterns.
In Table 6, we showed how heterogeneous metrics on convolutional (conv.) layers (with the en-
ergy minimization, ‘E’) and fully connected (FC) layers (with the hierarchical ‘H’ regularization,
‘H’) affect the generalization performance using resnet-20 and CIFAR-100 dataset. The cases ap-
plying hierarchical regularization showed better performance than that the baseline applying only
energy minimization regularization. In this experiment, a combination GM and HM shows a better
improvement than that other combinations.
7
Under review as a conference paper at ICLR 2020
Table 5: Test accuracy (%) using different met-
rics on energy minimization (‘E’) and hierarchical
regularization (‘H’) on CIFAR-100 and resnet-20.
metric
baseline+l2	68.03
DA0 M	68.64
DGM	68.70
DH0 M	68.80
DAMcos	69.24
DGMcos	68.55
DHMcos	68.77
DA0 Mcos	68.96
DG0 Mcos	69.00
DGMcos	68.83
Table 6: Test accuracy (%) using heterogeneous
metrics at Conv. layers and FC layer respec-
tively, on energy minimization (‘E’) and hier-
archical regularization (‘H’) on CIFAR-100 and
resnet-20.
metrics (in conv., in FC)
baseline+l2	68.03
(DGM, null)	68.22
(DGM, DAM)	68.58
(DGM, DGM)	68.62
(DGM, DHM)	69.04
(DGM, DAM)	68.62
(DGM, DGM)	68.65
(DGM, DHM)	68.70
5	Related works
Promoting of diversity in an embedding space or model parameters is an widely adopted strategy
in machine learning related area to improve the generalization performance (Cogswell et al., 2016;
Yang et al., 2019; Li et al., 2012; Ratzlaff & Fuxin, 2019; Xie et al., 2017b; 2018; 2017a; Liu et al.,
2018). This diversity based strategy can be applied in a variety of levels such as in a feature level
(Cogswell et al., 2016; Xie et al., 2018), in a projection parameter level (Xie et al., 2017a; Liu et al.,
2018), in a model ensemble level (Zhou et al., 2018; Ratzlaff & Fuxin, 2019), in a latent space model
level (Ratzlaff & Fuxin, 2019; Liu et al., 2018), and in a generative model level (Yang et al., 2019;
Ratzlaff & Fuxin, 2019). Among these approaches, some regularization methods require additional
optimization effort. Enlarging pairwise distance between features (Xie et al., 2018) requires com-
putational efforts due to their covariance matrix. In (Xie et al., 2017b), unit-eigenvalue is utilized
via singular value decomposition. To optimize a direction and magnitude of the parameter vector
alternatively, an alternating direction method of multipliers (ADMM) (Xie et al., 2017a) is used.
In terms of learning on a hypersphere, (Liu et al., 2017) proposed that hyperspherical convolution
(SphereCov) replaces the traditional inner-product based convolution in order to conduct learning
angular representation. On a hypersphere, regularization via promoting diversity of parameters has
been proposed (Liu et al., 2018) based on Minimum Hyperspherical Energy.
In terms of hierarchical learning, (Nickel & Kiela, 2017; Ganea et al., 2018) proposed to apply hy-
perbolic space to embed the data in deep neural networks. They showed that learning representations
in the hyperbolic space is effective to the data preserving a latent hierarchy compared to in the Eu-
clidean space. Different from these works which are aimed for representation learning, our proposed
method is focused on regularization learning using explicit (predefined) hierarchy information.
6	Conclusion
We proposed the regularization method, which maximizes a pairwise distance between parameters
preserving a hierarchical structure. To define a hierarchical parameter space, we reformulated the
topology space with multiple hyperspheres. In each hypersphere, a projection is parameterized
by the surface parameter with the center parameter. By imposing a maximum distance between
hierarchical parameters, diversity of parameters preserving semantic structure was promoted. For
the distance metric in multiple separated spaces, we proposed a discrete metric integrated with a
continuous metric. Extensive experiments using publicly available datasets (CIFAR-10, CIFAR-
100, CUB200-2011, and Stanford Cars), the deep neural network with our proposed regularization
method showed superior classification performance, especially when the number of super-classes
is large. For further exploration in future, our proposed method can be combined with hierarchical
representation learning such as hyperbolic (or Poincare) embeddings (Nickel & Kiela, 2017; Ganea
et al., 2018).
References
Y. Bengio, Nicholas Lonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv, Aug. 2013.
8
Under review as a conference paper at ICLR 2020
Johann S. Brauchart and Peter J. Grabner. Distributing many points on spheres. J. Complex., 31(3):
293-326, June 2015.
Michael Cogswell, Faruk Ahmed, Ross B. Girshick, Larry Zitnick, and Dhruv Batra. Reducing
Overfitting in Deep Networks by Decorrelating Representations. In International Conference on
Learning Representations, 2016.
Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic neural networks. In Advances
in Neural Information Processing Systems 31, pp. 5345-5355. 2018.
Radoslav Harman and Vladimr Lacko. On decompositional algorithms for uniform sampling from
n-spheres and n-balls. Journal of Multivariate Analysis, 101(10):2297 - 2304, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las
Vegas, NV, USA, June 27-30, 2016, pp. 770-778, 2016.
Jonathan Krause, Jun Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of fine-
grained cars. In Second Workshop on Fine-Grained Visual Categorization (FGVC2), 2013a.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013b. Ddataset available at http://imagenet.stanford.
edu/internal/car196/.
Ray Kurzweil. How to Create a Mind: The Secret of Human Thought Revealed. Penguin Books,
New York, NY, USA, 2013.
J.M. Lee. Introduction to Topological Manifolds. Graduate texts in mathematics. Springer, 2000.
Nan Li, Yang Yu, and Zhi-Hua Zhou. Diversity regularized ensemble pruning. In Proceedings of
the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases
- Volume Part I, ECMLPKDD’12, pp. 330-345, 2012.
Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep
hyperspherical learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
3950-3960, 2017.
Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning
towards minimum hyperspherical energy. In Proceedings of the 32Nd International Conference on
Neural Information Processing Systems, NIPS’18, pp. 6225-6236, USA, 2018. Curran Associates
Inc.
Mervin E. Muller. A note on a method for generating points uniformly on n-dimensional spheres.
Commun. ACM, 2(4):19-20, April 1959.
Maximillian Nickel and DoUWe Kiela. Poincare embeddings for learning hierarchical representa-
tions. In Advances in Neural Information Processing Systems 30, pp. 6338-6347. 2017.
Neale Ratzlaff and Li Fuxin. HyperGAN: A generative model for diverse, performant neural net-
Works. In Proceedings of the 36th International Conference on Machine Learning, volume 97,
pp. 5361-5369, 09-15 Jun 2019.
L.W. Tu. An Introduction to Manifolds. Universitext. Springer NeW York, 2010.
TWan van Laarhoven. L2 regularization versus batch and Weight normalization. arXiv preprint,
arXiv:1706.05350, 2017.
N. Verma, D. Mahajan, S. Sellamanickam, and V. Nair. Learning hierarchical similarity metrics. In
2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2280-2287, June 2012.
9
Under review as a conference paper at ICLR 2020
Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, and Julyan Arbel. Understanding priors in
Bayesian neural networks at the unit level. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of
Proceedings of Machine Learning Research, pp. 6458-6467, Long Beach, California, USA, 09-
15 Jun 2019. PMLR.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-
200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology,
2011. Dataset available at http://www.vision.caltech.edu/visipedia-data/
CUB-200-2011/.
Pengtao Xie, Yuntian Deng, Yi Zhou, Abhimanu Kumar, Yaoliang Yu, James Zou, and Eric P. Xing.
Learning latent space models with angular constraints. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp.
3799-3810, 2017a.
Pengtao Xie, Aarti Singh, and Eric P. Xing. Uncorrelation and evenness: A new diversity-promoting
regularizer. In Proceedings of the 34th International Conference on Machine Learning - Volume
70, ICML’17, pp. 3811-3820. JMLR.org, 2017b.
Pengtao Xie, Wei Wu, Yichen Zhu, and Eric P. Xing. Orthogonality-promoting distance metric
learning: Convex relaxation and theoretical analysis. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-
15, 2018, pp. 5399-5408, 2018.
Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tiangchen Zhao, and Honglak Lee. Diversity-
sensitive conditional generative adversarial networks. In International Conference on Learning
Representations, 2019.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization. In International Conference on Learning Representations, 2019.
Tianyi Zhou, Shengjie Wang, and Jeff A Bilmes. Diverse ensemble evolution: Curriculum data-
model marriage. In Advances in Neural Information Processing Systems 31, pp. 5905-5916.
Curran Associates, Inc., 2018.
Appendix
Dataset acquisition details CIFAR-100 dataset provides a pair of superclass-subclass labels. In
CUB200 dataset, the pairs can be extracted from their filename. In Cars dataset, we parsed each
subclass label to one of nine superclass vehicle types, such as “Sedan”, “SUV”, “Van” and etc.,
following (Krause et al., 2013a).
Table 7: Statistics of benchmark datasets
Dataset	#classes {pa, ch}	#train	#test	input size	#samples /class	#super /subclass
CIFAR-10	{1, 10}	50,000	10,000	32 × 32	5000.00	0.1000
CIFAR-100	{20, 100}	50,000	10,000	32 × 32	500.00	0.2000
CUB200	{70, 200}	5,994	5,794	224 × 224	29.97	0.3500
Cars	{9, 196}	8,144	8,041	224 × 224	41.55	0.0459
Deep neural network models and training details First, resnet-20 (0.29M) and resnet-110 (1.73
M), which include a combination of Basic blocks with output channels [16, 30, 64] are adopted for
light models. An input dimensionality of the fully connected (FC) layer (a classier) is 64 for both
resnet-20 and resnet-110. Second, heavy models are adopted such as Resnet-18 (11.28M10) and
10Dependent on the number of classes and the corresponding center parameters, the size could variate (e.g.
11.42M for CUB200, 11.31M for Cars).
10
Under review as a conference paper at ICLR 2020
Resnet-50 (23.91M) which consists of the basic blocks (Resnet-18) or the bottleneck blocks with
output channels [64, 128, 256, 512] in Conv. layers. An input dimensionality of the FC layer is 512
for Resnet-18 and 2048 for Resnet-50.
Networks are optimized with SGD for both light and heavy models: we fixed 1) the weight initializa-
tion with Random-Seed number ‘0’ in pytorch, 2) learning rate schedule [0.1, 0.01, 0.001], 3) with
momentum 0.9, 4) regularization: l2 -norm minimization with λf = 0.0005, 5) energy minimization
λe = {0.1, 1}, and hierarchical minimization λl = 0.1 × {1, 5}. All the regularization is not applied
to the parameters of Batch Normalization (BN) layers, since BN is also a regularization function. A
bias term in the FC layer is not used. The images in training and test, images are resized to 256 size.
The images is cropped with 224 × 224 size at random location in training and at center location in
test. Horizontal flipping is applied in training. The light models are trained from scratch without
the pretrained weights for 300 epochs. The heavy model is trained using pretrained model provided
by pytorch library11 with 100 epochs. The experiments are conduced using GPU “NVIDIA TESLA
P40”.
d(w/,w%)
d(w/,w黑)
d(w)[n, w^gi,m)
Figure 4:	Rl,p(Ws(,lg,li-1), wc(l,,gli-1))	=	d(wg(li,,ln-1),wg(li,,lm-1))	+	d(wg(li,,lm-1),	wg(li,,lk-1))	+
d(wg(li,,lk-1) , wg(li,,ln-1)) + . . . corresponds to Eq (6) and Rl,c(wc(l,,glk-1)) = d(wc(l,,gli-1) , wc(l,,glj-1)) +
d(wc(l,,glj-1), wc(l,,glk-1)) . . . corresponds to Eq (7)
Constraints in Eq. 4 Cl := k λk Cl,k can be given as follows:
1.	Constraint 1 (C1 ): This constraint describes that a radius of an inner sphere must be smaller
than that of its outer sphere.
r(l-1) - r(l) ≥ 0 ⇒ kw(l-1) - w(l) k = kws(l-1) - wc(l-1) k - kws(l) - wc(l)k ≥ 0.
2.	Constraint 2 (C2): This constraint describes that a center of an inner sphere must be located
within its outer sphere.
r(l-1) - (kwc(l,l-1) k + r(l)) ≥ 0 ⇒ r(l-1) - (kwc(l-1) - wc(l)k + r(l)) = kws(l-1) -
wc(l-1)k - (kwc(l-1) - wc(l)k + kws(l) - wc(l)k) ≥ 0.
3.	Constraint 3 (C3): This constraint describes that a margin between spheres must be larger
than zero.
kw(I,lT)k(2 - 2cosθ)0.5 - 2((I)	≥	0 ⇒	||w(l)||(2 - 2Pi=j Wl；Wc'))，)	-
k wc	k(2	2cosθ)	2r	≥	0 ⇒	Ilwc ∣∣(2	2	∣∣w(i)k2	)
11from https://download.pytorch.org/models/resnet18-5c106cde.pth, https://download.pytorch.org/models/resnet50-
19c8e357.pth respectively
11
Under review as a conference paper at ICLR 2020
2kws(l)	-	wc(l) k,	where	kwc(l,l-1) k(2 - 2 cos θ)0.5
kwc(l,l-1) k(r(l-1) sin θ2 - (r(l-1) - r(l-1) cos θ)2) . .
12