Under review as a conference paper at ICLR 2020
Scheduling the Learning Rate Via Hypergra-
dients: New Insights and a New Algorithm
Anonymous authors
Paper under double-blind review
Ab stract
We study the problem of fitting task-specific learning rate schedules from the
perspective of hyperparameter optimization. This allows us to explicitly search
for schedules that achieve good generalization. We describe the structure of
the gradient of a validation error w.r.t. the learning rate, the hypergradient, and
based on this we introduce a novel online algorithm. Our method adaptively
interpolates between the recently proposed techniques of Franceschi et al. (2017)
and Baydin et al. (2018), featuring increased stability and faster convergence. We
show empirically that the proposed method compares favorably with baselines and
related methods in terms of final test accuracy.
1	Introduction
Learning rate (LR) adaptation for first-order optimization methods is one of the most widely studied
aspects in optimization for learning methods — in particular neural networks — with early work dating
back to the origins of connectionism (Jacobs, 1988; Vogl et al., 1988). More recent work focused on
developing complex schedules that depend on a small number of hyperparameters (Loshchilov &
Hutter, 2017; Orabona & Pdl, 2016). Other papers in this area have focused on the optimization of
the (regularized) training loss (Schaul et al., 2013; Baydin et al., 2018; Wu et al., 2018) . While quick
optimization is desirable, the true goal of supervised learning is to minimize the generalization error,
which is commonly estimated by holding out part of the available data for validation. Hyperparameter
optimization (HPO), a related but distinct branch of the literature, specifically focuses on this
aspect, with less emphasis on the goal of rapid convergence on a single task. Research in this
direction is vast (see Hutter et al. (2019) for an overview) and includes model-based (Snoek et al.,
2012; Hutter et al., 2015), model-free (Bergstra & Bengio, 2012; Hansen, 2016), and gradient-
based (Domke, 2012; Maclaurin et al., 2015) approaches. Additionally, works in the area of learning
to optimize (Andrychowicz et al., 2016; Wichrowska et al., 2017) have focused on the problem of
tuning parameterized optimizers on whole classes of learning problems but require prior expensive
optimization and are not designed to speed up training on a single specific task.
The goal of this paper is to automatically compute in an online fashion a learning rate schedule for
stochastic optimization methods (such as SGD) only on the basis of the given learning task, aiming
at producing models with associated small validation error. We study the problem of finding a LR
schedule under the framework of gradient-based hyperparameter optimization (Franceschi et al.,
2017): we consider as an optimal schedule η* = (η0,..., ηT-J ∈ RT a solution to the following
constrained optimization problem
min{fτ(η) = E(wτ(η)) ： η ∈ RT} s.t. wo = w, wt+ι(η) = Φt(wt(η),ηt)	(1)
for t = {0, . . . , T - 1} = [T], where E : Rd → R+ is an objective function, Φt : Rd × R+ → Rd
is a (possibly stochastic) weight update dynamics, W ∈ Rd represents the initial model weights
(parameters) and finally wt are the weights after t iterations. We can think of E as either the training
or the validation loss of the model, while the dynamics Φ describe the update rule (such as SGD,
SGD-Momentum, Adam etc.). For example in the case of SGD, Φt(wt, ηt) = Wt - ηtVLt(wt),
with Lt(wt) the (possibly regularized) training loss on the t-th minibatch. The horizon T should be
large enough so that the training error can be effectively minimized, in order to avoid underfitting.
Note that a too large value of T does not necessarily harm since ηk = 0 for k > T is still a feasible
solution, implementing early stopping in this setting.
1
Under review as a conference paper at ICLR 2020
Problem (1) can be in principle solved by any HPO technique. However, most HPO techniques,
including those based on hypergradients Maclaurin et al. (2015) or on a bilevel programming
formulation (Franceschi et al., 2018; MacKay et al., 2019) would not be suitable for the present
purpose since they require multiple evaluations of f (which, in turn, require executions of the weight
optimization routine), thus defeating one of the main goals of determining LR schedules, i.e. speed.
In fact, several other researchers (Almeida et al., 1999; Schraudolph, 1999; Schaul et al., 2013;
Franceschi et al., 2017; Baydin et al., 2018; Wu et al., 2018) have investigated related solutions
for deriving greedy update rules for the learning rate. A common characteristic of methods in this
family is that the LR update rule does not take into account information from the future. At a high
level, we argue that any method should attempt to produce updates that approximate the true and
computationally unaffordable hypergradient of the final objective with respect to the current learning
rate (in relation to this, Wu et al. (2018) discusses the bias deriving from greedy or short-horizon
optimization). In practice, different methods resort to different approximations or explicitly consider
greedily minimizing the performance after a single parameter update (Almeida et al., 1999; Schaul
et al., 2013; Baydin et al., 2018). The type of approximation and the type of objective (i.e. the training
or the validation loss) are in principle separate issues although comparative experiments with both
objectives and the same approximation are never reported in the literature and validation loss is only
used in the experiments reported in Franceschi et al. (2017).
One additional aspect needs to be taken into account: even when the (true or approximate) hypergra-
dient is available, one still needs to introduce additional hyper-hyperparameters in the design of the
online learning rate adaptation algorithm. For example in Baydin et al. (2018), hyper-hyperparameters
include initial value of the learning rate η0 and the hypergradient learning rate β . We find in our
experiments that results can be quite sensitive to the choice of these constants.
In this work, we make a step forward in understanding the behavior of online gradient-based
hyperparameter optimization techniques by (i) analyzing in Section 2 the structure of the true
hypergradient that could be used to solve Problem (1) if wall-clock time was not a concern, and (ii)
by studying in Section 3 some failure modes of previously proposed methods along with a detailed
discussion of the type of approximations that these methods exploit. In Section 4, based on these
considerations, we develop a new hypergradient-based algorithm which reliably produces competitive
learning rate schedules aimed at lowering the final validation error. The algorithm, which we call
MARTHE (Moving Average Real-Time Hyperparameter Estimation), has a moderate computational
cost and can be interpreted as a generalization of the algorithms described in Baydin et al. (2018)
and Franceschi et al. (2017). Unlike previous proposals, MARTHE is almost parameter-free in that it
incorporates heuristics to automatically tune its configuration parameters (i.e. hyper-hyperparameters).
In Section 5, we empirically compare the quality of different hypergradient approximations in a
small scale task where true hypergradient can be exactly computed. In Section 6, we present a set
of real world experiments showing the validity of our approach. We finally discuss potential future
applications and research directions in Section 7.
2	Structure of the Hypergradient
We study the optimization problem (1) under the perspective of gradient-based hyperparameter
optimization, where the learning rate schedule η = (η0, . . . , ηT-1) is treated as a vector of hyperpa-
rameters and T is a fixed horizon. Please refer to Appendix A for a summary of the main notation
used throughout the paper. Since the learning rates are positive real-valued quantities, assuming both
E and Φ are smooth functions, we can compute the gradient of f ∈ RT , which is given by
VfT(η) = WTVE(WT), where WT = dwT ∈ Rd×T,	(2)
where "|" means transpose. The total derivative WT can be computed iteratively with forward-mode
algorithmic differentiation (Griewank & Walther, 2008; Franceschi et al., 2017) as
∂Φt (Wt, ηt)	∂Φt(Wt, ηt)
W o = 0,	W t+ι = AtW t + Bt,	With	At =-------------,	Bt =------石-----.	(3)
∂Wt	∂η
The Jacobian matrices At and Bt depend on Wt and ηt, but we will leave these dependencies implicit
to ease our notation. In the case of SGD1, At = I 一 ηt Ht (Wt) and [Btj = -δj VLt (Wt)τ, where
1Throughout we use SGD to simplify the discussion, however, similar arguments hold for any smooth
optimization dynamics such as those including momentum terms.
2
Under review as a conference paper at ICLR 2020
」",=SYVsBt
subscripts denote columns (starting from 0), δtj = 1 if t = j and 0 otherwise and Ht is the Hessian
of the training error on the t-th mini-batch 2. We also note that, given the high dimensionality of η,
reverse-mode differentiation would result in a more efficient (running-time) implementation. We use
here forward-mode both because it is easier to interpret and visualize and also because it is closely
related to the computational scheme behind MARTHE, as we will show in Section 4. Finally, we
note that stochastic approximations of Eq. (2) may be obtained with randomized telescoping sums
(Beatson & Adams, 2019) or hyper-networks based stochastic approximations (MacKay et al., 2019).
Eq. 3 describes the so-called tangent system (Griewank & Walther, 2008) which is a discrete affine
time-variant dynamical system that measures how the parameters of the model would change for
infinitesimal variations of the learning rate schedule, after t iterations of the optimization dynamics.
Notice that the “translation matrices” Bt are very sparse, having, at any iteration, only one non-zero
column. This means that [W tj remains 0 for all j ≥ t: η affects only the future parameters trajectory.
Finally, for a learning rate ηt, the derivative (a scalar) is
|
VE(WT) = -VLt(Wt)IPT-IVE(WT),	(4)
t
where the last equality holds true for SGD. Eq. (4) can be read as the scalar product between the
gradients of the training error at the t-th step and the objective E at the final iterate, transformed by
the accumulated (transposed) Jacobians of the optimization dynamics, shorthanded by PtT+-11. As it is
apparent from Eq. (4), given Wt, the hypergradient of ηt is affected only by the future trajectory and
does not depend explicitly on ηt .
In its original form, where each learning rate is left free to take any permitted value, Eq. (1) represents
a highly nonlinear setup. Although, in principle, it could be solved by projected gradient descent,
in practice it is unfeasible even for small problems: evaluating the gradient with forward-mode is
inefficient in time, since it requires maintaining a (large) matrix tangent system. Evaluating it with
reverse-mode is inefficient in memory, since the entire weight trajectory (Wi)iT=0 should be stored.
Furthermore, it can be expected that several updates of η are necessary to reach convergence where
each update requires computation of fT and the entire parameter trajectory in the weight space. Since
this approach is computationally very expensive, we turn out attention to online updates where ηt is
required to be updated online based only on trajectory information up to time t.
3	Online Gradient-Based Adaptive Schedules
Before developing and motivating our proposed technique, we discuss two previous methods to
compute the learning rate schedule online. The real-time hyperparameter optimization (RTHO)
algorithm suggested in (Franceschi et al., 2017), reminiscent of stochastic meta-descent (Schraudolph,
1999), is based on forward-mode differentiation and uses information from the entire weight trajectory
by accumulating partial hypergradients. Hypergradient descent (HD), proposed in (Baydin et al.,
2018) and closely related to the earlier work of Almeida et al. (1999), aims at minimizing the loss
w.r.t. the learning rate after one step of optimization dynamics. It uses information only from the past
and current iterate.
Both methods implement update rules of the type
ηt = max [ηt-1 - β∆ηt,0] ,	(5)
where ∆ηt is an online estimate of the hypergradient, β > 0 is a step-size or hyper-learning rate and
the max ensures positivity3. To ease the discussion, we omit the stochastic (mini-batch) evaluation of
the training error L and possibly of the objective E .
2Note that techniques based on implicit differentiation (Pedregosa, 2016; Agarwal et al., 2017) or fixed-point
equations (Griewank & Faure, 2002) (also known as recurrent backpropagation (Pineda, 1988)) cannot be
readily applied to compute VfT since the training loss L does not depend explicitly on η.
3Updates could be also considered in the logarithmic space as done e.g. by Schraudolph (1999); we find it
useful, however, to let η reach 0 whenever needed, offering a natural way to implement early stopping in this
context.
3
Under review as a conference paper at ICLR 2020
Figure 1: Loss surface and trajectories for 500 steps of gradient descent with HD and RTHO for
Beale function (left) and (smoothed and simplified) Bukin N. 6 (right). Center: best objective value
reached within 500 iterations for various values of β that do not lead to divergence.
The update rules4 are given by
t-1	|
△RTHOnt = X pt-iBi	VE(wt);	△HDnt = Bt-1 VE(Wt)	⑹
i=0
for RTHO and HD respectively, where Ptt-1 := I. As it can be seen △RTHO = △HD +r((wi, ni)it=-02):
the correction term r can be interpreted as an “on-trajectory approximations” of longer horizon
objectives as we will discuss in Section 4.
Although successful in some learning scenarios, we argue that both these update rules suffer from
(different) pathological behaviors, as HD may be “shortsighted”, being prone to underestimate the
learning rate (as noted by Wu et al. (2018)), while RTHO may be too slow to adapt to sudden changes
of the loss surface or, worse, may be unstable, with updates growing exponentially in magnitude. We
exemplify these behaviors in Fig. 1, using two bidimensional test functions5 from the optimization
literature, where we set E = L and we perform 500 steps of gradient descent, from a fixed initial
point. The Beale function, on the left, presents sharp peaks and large plateaus. RTHO consistently
outperforms HD for all probed values of β that do not lead to divergence (Fig. 3 upper center). This
can be easily explained by the fact that in flat regions gradients are small in magnitude, leading
to △HDnt to be small as well. RTHO, on the other hand, by accumulating all available partial
hypergradients and exploiting second order information, is capable of making faster progress. We use
a simplified and smoothed version of the Bukin function N. 6 to show the opposite scenario (Fig. 3
lower center and right). Once the optimization trajectory closes the valley of minimizers y = 0.01x,
RTHO fails to discount outdated information, bringing the learning rate first to grow exponentially,
and then to suddenly vanish to 0, as the gradient changes direction. HD, on the other hand, correctly
damps n and is able to maintain the trajectory close to the valley.
These considerations suggest that neither △RTHO nor △HD provide globally useful update directions,
as large plateaus and sudden changes on the loss surface are common features of the optimization
landscape of neural networks (Bengio et al., 1994; Glorot & Bengio, 2010). Our proposed algorithm
smoothly and adaptively interpolates between these two methods, as we will present next.
4	Our proposal: MARTHE
In this section, we develop and motivate MARTHE, an algorithm for computing LR schedule online
during a single training run. This method maintains an adaptive moving-average over approximations
of Eq. (4) of increasingly longer horizon, using the past trajectory and gradients to retain a low
computational overhead. Further, we show that RTHO (Franceschi et al., 2017) and HD (Baydin
et al., 2018) outlined above, can be interpreted as special cases of MARTHE, shedding further light
on their behaviour and shortcomings.
4 In (Franceschi et al., 2017) the authors the hyperparameter is updated every K iterations. Here we focus on
the case K = 1 which better allows for a unifying treatment. HD is developed using as objective the training
loss L rather than the validation loss E. We consider here without loss of generality the case of optimizing E.
5We use the Beale function defined as L(x,y) = (1.5 — X + Xyy2 + (2.25 — X + xy2)2 + (2.625 — X + xy3)2
and a simplified smoothed version of Buking N. 6: L(x, y) = (/((y - 0.01x)2 + ε)1/2 + ε, with ε > 0.
4
Under review as a conference paper at ICLR 2020
Shorter horizon auxiliary objectives. For K > 0, define gK (u, ξ), with ξ ∈ R+K as
gK (u, ξ) = E(uK (ξ)) s.t. u0 = u, ui+1 = Φ(ui, ξi) for i = [K].	(7)
The gKs define a class of shorter horizon objective functions, indexed by K, which correspond to the
evaluation of E after K steps of optimization, starting from u ∈ Rd and using ξ as the LR schedule6.
Now, the derivative of gK w.r.t. ξ0, denoted gK0 , is given by
gK (u, ξ) = dgK∂ξu,ξ) = [B0]∣ PKTVE (UK) = -VL(U)IPKTVE (UK),	⑻
where the last equality holds for SGD dynamics. Once computed on subsets of the original optimiza-
tion dynamics (wi)iT=0, the derivative reduces for K = 1 to g10 (wt, ηt) = -VE(wt+1)VL(wt)| (for
SGD dynamics), and for K = T - t to gT0 -t(wt, (ηi)iT=-t1) = [Vf (η)]t. Intermediate values of K
yield cheaper, shorter horizon approximations of (4).
Approximating the future trajectory with the past. Explicitly using any of the approximations
given by gK0 (wt, η) as ∆ηt is, however, still largely impractical, especially for K	1. Indeed,
it would be necessary to iterate the map Φ for K steps in the future, with the resulting (wt+i)iK=1
iterations discarded after a single update of the learning rate. For K ∈ [t], we may then consider
evaluating gK0 exactly K steps in the past, that is evaluating gK0 (wt-K, (ηi)it=-t1-K). Selecting K = 1
is indeed equivalent to ∆HD, which is computationally inexpensive. However, when past iterates are
close to future ones (such as in the case of large plateaus), using larger K’s would allow in principle
to capture longer horizon dependencies present in the hypergradient structure of Eq. 4. Unfortunately
the computational efficiency of K = 1 does not generalize to K > 1, since setting ∆ηt = gK0 would
require maintaining K different tangent systems.
Discounted accumulation of gk0 s. The definition of the gKs, however, allows one to highlight the
recursive nature of the accumulation of gK0 . Indeed, by maintaining the vector tangent system
Zo = [B0(u0, ξ0)]0 Zi+1 = μAi(u,ξi)Zi + [Bi(ui,ξi)]i for i ≥ 0, Zi ∈ Rd, (9)
computing SK,μ(u,ξ) = PK-I μK-1-igK-i(ui, (ξj)K-1) = ZKVE(UK) from SK-i requires
only updating (9) and recomputing the gradient of E for a total cost of O(c(Φ)) per step both in
time and memory using fast Jacobians vector products (Pearlmutter, 1994) where c(Φ) is the cost
of computing the optimization dynamics (typically c(Φ) = O(d)). The parameter μ ∈ [0,1] allows
to control how quickly past history is forgotten. One can notice that ∆RTHOηt = St,1(w0, (ηj)it=-01),
while μ = 0 recovers ∆HDnt. Values of μ < 1 help discounting outdated information, while as μ
increases so does the horizon of the hypergradient approximations. The computational scheme of Eq.
9 is quite similar to that of forward-mode algorithmic differentiation for computing W (see Section
2 and Eq. 3); we note, however, that the “tangent system” in Eq. 9, exploiting the sparsity of the
matrices Bt, only keeps track of the variations w.r.t the first component ξ0, drastically reducing the
running time.
Adapting μ and β online. We may set ∆ηt = St,μ(w0, (n)t=0). ThiS still would require choosing
a fixed value of μ, which should be validated on a separate set of held-out data. This may add an
undesirable overhead on the optimization procedure. Furthermore, as discussed in Section 3, different
regions of the loss surface may benefit from different effective approximation horizons. To address
these issues, we propose to compute μ online. Ideally, we would like to verify that ∆ηt[Vf (η)]t > 0,
i.e. whether the proposed update is a descent direction w.r.t. the true hypergradient. While this is
unfeasible (since Vf(η) is unavailable), we can cheaply compute, after the update Wt+1 = Φ(Wt, ηt),
the quantity
q(μt) = ∆ηt+i ∙ gi(wt,ηt) = (μtAtZt-ι + [Bt]t)lVE(wt+ι) ∙ [Bt]|E(wt+i)	(10)
which, ex post, relates μt to the one-step descent condition for gi. We set μt+ι = hμ(q(μt)) where
hμ is a monotone scalar function with range in [0,1] (note that if μt = 0 then Eq. 10 is non-negative).
For space limitations, we defer the discussion of the choice of hμ and the effect of adapting online
the approximation horizons to the Appendix. We can finally define the update rule for MARTHE as
t-1 t-2
△nt = E ∏μj g0-i (Wi,(η)tj=1)
i=0 j=i
(11)
6 Note that, formally, ξ and u are different from η and w from the previous sections; later, however, we will
evaluate the gK ’s on subsequences of the optimization trajectory.
5
Under review as a conference paper at ICLR 2020
We further propose to adapt β online, implementing with this work a suggestion from Baydin et al.
(2018). We regard the LR schedule as a function of β and apply the same reasoning done for η,
keeping μ = 0, to avoid maintaining an additional tangent system which would involve third order
derivatives of the training loss L. We then set βt+ι = βt - β∆ηt+ι ∙ ∆ηt, where, with a little notation
override, β becomes a fixed step-size for adapting the hyper-learning rate. This may seem a useless
trade-off at first; yet, as we observed experimentally, one major advantage of lifting the adaptive
dynamics by one more level is that it injects additional stability in the learning system, in the sense
that good values of this last parameter of MARTHE lays in a much broader range range than those of
good hyper-learning rates. In fact, we observe that when β is too high, the dynamics diverges within
the first few optimization steps; whereas, when it does not, the final performances are rather stable.
Algorithm 1 presents the pseudocode of MARTHE. The runtime and memory requirements of the
algorithm are dominated by the computation of the variables Z . Being these structurally identical to
the tangent propagation of forward mode algorithmic differentiation, we conclude that the runtime
complexity is up to four times that of the underlying optimization dynamics Φ and the memory
requirement up to two times (see Griewank & Walther, 2008, Sec. 4). We suggest the default values
of β0 = 0 and η0 = 0 when no prior knowledge of the task at hand is available. Finally, we suggest
to wrap Algorithm 1 in a selection procedure for β, where one may start with a high enough β and
aggressively diminish it (e.g. decimating it) until the learning system does not diverge.
Algorithm 1 MARTHE; requires β, ηο, βo = 0	
Initialization of w,η and Zo — 0, μo — 0 for t = 0 to T do ηt — max [ηt-ι - βt∆ηt, 0] Zt+ι J μtAt(Wt, ηt)Zt + [Bt(wt, ηt)]t wt+1 J Φt(wt,ηt) μt+1 J hμ(q(μt)) βt+ι J βt - β∆ηt+ι ∙ ∆ηt end for	{Update LR if t > 0} {Tangent system update} {Parameter update} {Compute μt, see Eq. 10} {Update hyper-LR}
5	Optimized and Online Schedules
In this section, we empirically compare the optimized LR schedules found by approximately solving
Problem 1 by gradient descent (denoted LRS-OPT), where the hypergradient is given by Eq. 4, against
HD, RTHO & MARTHE schedules where for MARTHE we consider both the adaptation schemes
for μ and β presented in the previous section as well as fixed hyper-learning rate and discount factor
μ. We are interested in understanding and visualizing the qualitative similarities among the schedules,
as well as the effect of μ and β and the adaptation strategies on the final performance measure. To
this end, we trained feedforward neural networks, with three layers of 500 hidden units each, on a
subset of 7000 MNIST (LeCun et al., 1998) images. We used the cross-entropy loss and SGD as the
optimization dynamics Φ, with a mini-batch size of 100. We further sampled 700 images to form
the validation set and defined E to be the validation loss after T = 512 optimization steps (about 7
epochs). For LRS-OPT, we randomly generated different mini-batches at each iteration to prevent the
schedule from unnaturally adapting to a specific sample progression7. We initialized η = 0.01 ∙ I512
for LRS-OPT and set η0 = 0.01 for all adaptive methods, and repeated the experiments for 20 random
seeds (except LRS-OPT, repeated only for 4 seeds). Results are visualized in Figure 2.
Figure 2 (left) shows the LRS-OPT schedules found after 5000 iterations of gradient descent: the
plot reveals a strong initialization (random seed) specific behavior of η* for approximately the first
100 steps. The LR schedule then stabilizes or slowly decreases up until around 50 iterations before
the final time, at which point it quickly decreases (recall that with LRS-OPT all ηi, including η0, are
optimized “independently” and may take any permitted value). Figure 2 (center) present a qualitative
comparison between the offline LRS-OPT schedule and the online ones. HD generates schedules
that quickly decay to very small values, while RTHO schedule linger or fail to decrease, possibly
causing instability and divergence in certain cases. Fixing μ = 0.99 seems to produce schedules that
7We retained, however, the random initialization of the network weights, to account for the impact that this
may have on the initial part of the trajectory (see Figure 2 (left)). This is done to offer a more fair comparison
between LRS-OPT and online methods, which compute the trajectory only once.
6
Under review as a conference paper at ICLR 2020
Figure 2: Left: schedules found by LRS-OPT (after 5000 iterations of SGD) on 4 different random seeds.
Center: qualitative comparison between offline and online schedules for one random seed. For MARTHE with
fixed μ, We report the best performing one. For each method, We report the schedule generated with the value of
β that achieves the best average final validation accuracy. Plots for the remaining random seeds can be found in
the appendix. Right: Average validation accuracy of over 20 random seeds, for various values of β. When no
point is reported it means that the achieved average accuracy for that configuration falls beloW 88% (or diverged).
For reference, the average validation accuracy of the network trained with η = 0.01 ∙ I512 is 87.5%, while
LRS-OPT attains 96.2%.
remarkably mimic the the optimized one; yet, unfortunately, this happens only for a small range of
values of μ which we expect to be task dependent. Using both the adaptation schemes for μ and β
(curve named MARTHE in the plots), allows to reliably find highly non-trivial schedules that capture
the general behavior of the optimized one (additional plots in the Appendix).
Figure 2 (right) shows the average validation accuracy over 20 runs (rather than loss, for easier
interpretation) of the online methods, varying β and discarding values below 88% of validation
accuracy. In particular, fixing μ > 0 seems to have a beneficial impact for all tried values of the
hyper-learning rate β. Using only the heuristic for adapting μ online (blue line, named hμ in the plot)
further helps, but is somewhat sensitive to the choice of β . Using both the adaptive mechanisms,
beside improving the final validation accuracy, seems to drastically lower the sensitivity on the choice
of this parameter, provided that the learning system does not diverge. Finally, we note that even with
this very simple setup, a single run of LRS-OPT (which comprises 5000 optimization steps) takes
more than 2 hours on an M-40 NVIDIA GPU. In contrast, all adaptive methods requires less than a
minute to conclude (HD being even faster).
6	Experiments
We run experiments with an extensive set of learning rate scheduling techniques. Specifically, we
compare MARTHE against the following fixed LR scheduling strategies: (i) exponential decay (ED) -
where the LR schedule is defined by ηt = η1 γt (ii) staircase decay and (iii) stochastic gradient descent
with restarts (SGDR) by Loshchilov & Hutter (2017). Moreover, we compare against online strategies
such as HD and RTHO. For all the experiments, we used a single Volta V100 GPU (AWS P3.2XL).
We fix the batch-size at 128 samples for all the methods, and terminate the training procedure after a
fixed number of epochs (200). We set L as the cross entropy loss with weight-decay regularization
(with factor of 5 ∙ 10-4) and set E as the unregularized cross entropy loss on validation data. All the
experiments with SGDM have an initial learning rate (no) of 0.1 and for Adam, We set it to 3 ∙ 10-4.
For staircase, we decay the learning rate by 90% after every 60 epochs. For exponential decay, we
fix a decay factor of 0.99 per epoch, and for SGDR we use T0 = 10 and Tmult = 2. For HD and
RTHO, we set the β as 10-6 and 10-8 respectively. Momentum for SGDM is kept constant to 0.9.
For Adam we used the standard values for the remaining configuration parameters. We run all the
experiments with 5 different seeds reporting average and standard deviation, recording accuracy, loss
value and generated learning rate schedules.
We trained image classification models on two different datasets commonly used to benchmark
optimization algorithms for deep neural networks: CIFAR-10 (Krizhevsky et al., 2014) where we
trained a VGG-11 (Simonyan & Zisserman, 2014) network with BatchNorm (Ioffe & Szegedy, 2015)
using SGDM as the inner optimizer, and CIFAR-100 (Krizhevsky et al., 2014) where we trained a
ResNet-18 (He et al., 2016) using Adam (Kingma & Ba, 2014). The source code in PyTorch and
TensorFlow to reproduce the experiments will be made publicly available.
In Figures 3 and 4, we report the results of our experiments for CIFAR-10 with VGG-11 and CIFAR-
100 with ResNet-18 respectively. For both figures, we report from left to right: accuracy in percentage,
validation loss value, and an example of a generated learning rate schedule.
7
Under review as a conference paper at ICLR 2020
Figure 3: Results of VGG-11 on CIFAR-10, and SGDM as the inner optimizer concerning: (Left) accuracy,
(Center) loss of the objective function on the validation Set and (Right) generated learning rate schedule for each
method.
Validation Loss - CIFAR100 ResNet
ιe-4 Learning Rate - CIFAR100 ResNet
Figure 4: Results of ResNet-18 on CIFAR-100, and Adam as the inner optimizer concerning: (Left) accuracy,
(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for each
method.
MARTHE produces LR schedules that lead to trained models with very competitive final validation
accuracy in both the experimental settings, virtually requiring no tuning. For setting the hyper-learning
rate step-size of MARTHE we followed the simple procedure outlined at the end of Section 4, while
for the other methods we performed a grid search to select the best value of the respective algorithmic
parameters. On CIFAR-10, MARTHE obtains a best average accuracy of 92.79% statistically on par
with SGDR (92.54%), while clearly outperforming the other two adaptive algorithms. On CIFAR-100,
MARTHE leads to faster convergence during then whole training compared to all the other methods,
reaching an accuracy of 76.68%, comparable to staircase schedule with 76.40%. We were not able
to achieve competitive results with SGDR in this setting, despite trying several values of the two
main configuration parameters within the suggested range. Further, MARTHE produces aggressive
schedules (see Figure 3 and 4 right, for an example) that increase the LR at the beginning of the
training, sharply decreasing it after a few epochs. We observe empirically that this leads to improved
convergence speed and competitive final accuracy.
Additional experimental validation is reported in Appendix D and includes results for MARTHE with
an initial learning rate η0 = 0.
7	Conclusion
Finding a good learning rate schedule is an old but crucially important issue in machine learning. This
paper makes a step forward, proposing an automatic method to obtain performing LR schedules that
uses an adaptive moving average over increasingly long hypergradient approximations. MARTHE
interpolates between HD and RTHO taking the best of the two worlds. The implementation of our
algorithm is fairly simple within modern automatic differentiation and deep learning environments,
adding only a moderate computational overhead over the underlying optimizer complexity.
In this work, we studied the case of optimizing the learning rate schedules for image classification
tasks; we note, however, that MARTHE is a general technique for finding online hyperparameter
schedules (albeit it scales linearly with the number of hyperparameters), possibly implementing
a competitive alternative in other application scenarios, such as tuning regularization parameters
(Luketina et al., 2016). We plan to further validate the method both in other learning domains for
adapting the LR and also to automatically tune other crucial hyperparameters. We believe that another
interesting future research direction could be to learn the adaptive rules for μ and β in a meta learning
fashion.
8
Under review as a conference paper at ICLR 2020
References
Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine
learning in linear time. The Journal ofMachine Learning Research, 18(1):4148-4187, 2017.
LUis B Almeida, ThibaUlt Langlois, Jose D Amaral, and Alexander Plakhov. Parameter adaptation
in stochastic optimization. In On-line learning in neural networks, pp. 111-134. Cambridge
University Press, 1999.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David PfaU, Tom SchaUl,
and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in
Neural Information Processing Systems, pp. 3981-3989, 2016.
Atilim Gunes Baydin, Robert Cornish, David Mardnez-Rubio, Mark Schmidt, and Frank D. Wood.
Online learning rate adaptation with hypergradient descent. In Proc. of the 6th Int. Conf. on
Learning Representations (ICLR), 2018. URL http://arxiv.org/abs/1703.04782.
Alex Beatson and Ryan P. Adams. Efficient optimization of loops and limits with randomized
telescoping sUms. ArXiv, 2019.
YoshUa Bengio, Patrice Simard, Paolo Frasconi, et al. Learning long-term dependencies with gradient
descent is difficUlt. IEEE transactions on neural networks, 5(2):157-166, 1994.
James Bergstra and YoshUa Bengio. Random search for hyper-parameter optimization. Journal
of Machine Learning Research, 13(Feb):281-305, 2012. URL http://www.jmlr.org/
papers/v13/bergstra12a.html.
JUstin Domke. Generic Methods for Optimization-Based Modeling. In AISTATS, volUme 22, pp. 318-
326, 2012. URL http://www.jmlr.org/proceedings/papers/v22/domke12/
domke12.pdf.
LUca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 1165-1173, 2017. URL https://arxiv.org/abs/
1703.01785.
LUca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil. Bilevel programming
for hyperparameter optimization and meta-learning. In Proceedings of the 35th International
Conference on Machine Learning-Volume 70, pp. 1568-1577, 2018. URL https://arxiv.
org/abs/1806.04910.
Xavier Glorot and YoshUa Bengio. Understanding the difficUlty of training deep feedforward neUral
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Andreas Griewank and Christele FaUre. RedUced fUnctions, gradients and hessians from fixed-point
iterations for state eqUations. Numerical Algorithms, 30(2):113-139, 2002.
Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of
algorithmic differentiation, volUme 105. Siam, 2008.
Samantha Hansen. Using deep q-learning to control optimization hyperparameters. arXiv preprint
arXiv:1602.04062, 2016.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), JUne
2016.
Gao HUang, ZhUang LiU, LaUrens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolUtional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
9
Under review as a conference paper at ICLR 2020
Frank Hutter, Jorg Lucke, and Lars Schmidt-Thieme. Beyond Manual Tuning of Hyperparam-
eters. KI - Kiinstliche InteUigenz, 29(4):329-337, November 2015. ISSN 0933-1875, 1610-
1987. doi: 10.1007/s13218-015-0381-0. URL http://link.springer.com/10.1007/
s13218-015-0381-0.
Frank Hutter, Lars Kotthoff, and J. Vanschoren. Automatic machine learning: methods, systems,
challenges. Springer, 2019. URL https://www.automl.org/wp-content/uploads/
2018/12/automl_book.pdf.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Robert A Jacobs. Increased rates of convergence through learning rate adaptation. Neural networks,
1(4):295-307, 1988.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www. cs.
toronto. edu/kriz/cifar. html, pp. 4, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. ICLR, 2017.
Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning
of continuous regularization hyperparameters. In International conference on machine learning,
pp. 2952-2960, 2016.
Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, and Roger Grosse. Self-tuning
networks: Bilevel optimization of hyperparameters using structured best-response functions. arXiv
preprint arXiv:1903.03088, 2019.
Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti-
mization through reversible learning. In Proceedings of the 32nd International Conference on
Machine Learning, 2015. URL http://www.jmlr.org/proceedings/papers/v37/
maclaurin15.pdf.
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-Dickstein.
Understanding and correcting pathologies in the training of learned optimizers. In International
Conference on Machine Learning, pp. 4556-4565, 2019.
Francesco Orabona and DgVid Pdl. Coin betting and parameter-free online learning. In Advances in
Neural Information Processing Systems, pp. 577-585, 2016.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Proceedings of the
33rd International Conference on Machine Learning, 2016.
Fernando J Pineda. Generalization of back propagation to recurrent and higher order neural networks.
In Neural information processing systems, pp. 602-611, 1988.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International
Conference on Machine Learning, pp. 343-351, 2013.
NN Schraudolph. Local gain adaptation in stochastic gradient descent. In 1999 Ninth International
Conference on Artificial Neural Networks ICANN 99.(Conf. Publ. No. 470), volume 2, pp. 569-574.
IET, 1999.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
10
Under review as a conference paper at ICLR 2020
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Thomas P Vogl, JK Mangis, AK Rigler, WT Zink, and DL Alkon. Accelerating the convergence of
the back-propagation method. Biological cybernetics, 59(4-5):257-263, 1988.
Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio G6mez Colmenarejo, Misha
Denil, Nando Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and generalize. In
International Conference on Machine Learning, pp. 3751-3760, 2017.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in
stochastic meta-optimization. arXiv preprint arXiv:1803.02021, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
11
Under review as a conference paper at ICLR 2020
A Table of Notation
Please refer to Table 1 for a summary and description of the notation used throughout the paper.
Table 1: Notation, with description and examples when appropriated, in order of appearance.
Notation	Description	Examples and notes
T∈N	Total number of iterations, horizon	
w ∈ Rd	Model parameters, inner optimization vari-	Weights of a neural net-
	ables	work
η = (η0, . . . ,ηT-1)	Learning rate schedule (LRS), hyperparam- eter	
fT(η) ∈ R+	Response function after T iterations (w.r.t. the LRS)	
E(w) ∈ R+	Response function (w.r.t. the parameters)	Validation error
Lt(w) ∈ R+	Training loss	
Φt(wt,ηt) ∈Rd	Optimization (weight update) dynamics	SGD: Φt (wt, ηt) = wt - ηtVLt(wt)
W ∈ Rd×T	Total derivative of w w.r.t. η, variable of the tangent system	
A = dφt (Wtmt) t	∂wt	Jacobian of the dynamics w.r.t. the weights	SGD: At = I - ηtHt
B = dφt(Wtmt) t =	∂η	Jacobian of the dynamics w.r.t. the LRS	SGD: [Bt]j = -δtj VLt(Wt)T
Prs = Qis=r Ai	Product of Jacobians from iteration s to r	
∆ηt	Update for the learning rate at iteration t	
β ∈ R+	Hyper-learning rate	
∆HD	HD update	See (Baydin et al., 2018)
∆RTHO	RTHO update	See (Franceschi et al., 2017)
gk(u, ξ) ∈ R+	Shorter horizon objectives, with horizon K, u starting point and ξ ≥ 0 ∈ RK LRS	
g0 (u, ξ)	Derivative of gK w.r.t. ξ1	
μ	Dampring factor	
Z ∈ Rd	Variables of the vector tangent system	Cf. tangent matrix W
Sk,μ(u,η) ∈ R	Online accumulation of shorter horizon derivatives gK0	
hμ	Heuristics for dampening factor μ	See Appendix B
B	Choice of Heuristic FOR Adapting μ
We introduced in Section 4 a method to compute online the dampening factor μt based on the quantity
q(μt) = ∆ηt+ι ∙ g1 (wt,ηt) = (μtAtZt-i + [Bt]t)lVE(wt+ι) ∙ [Bt↑lE(wt+ι).
We recall that if q(μt) is positive then the update ∆ηt+ι is a descent direction for the one step
approximation of the objective fT . We describe here the precise heuristic rule that we use in our
experiments. Call q(μt) = max(min(q(μt) g1(wt, ηt)-2,1), 0) ∈ [0,1] the normalized, thresholded
q(μt). We propose to set
〜	1	.
μt+1 = hμ(μt) = q(μt) ct + 1	With c0 = 0,	ct+1 = μt(I + Ct),
where ct acts as a multiplicative counter, measuring the effective approximation horizon. The resulting
heuristics is independent on the initialization of μ since Z° = 0. We note that whenever μt is set to 0,
the previous hypergradient history is forgotten. Applying this heuristic to the optimization of the tWo
test functions of Sec. 3 reveals to be successful: for the Beale function, hμ selects μt = 1 for all t,
while for the smoothed Bukin, it selects μt = 0 for around 40% of the iterations, bringing down the
minimum optimality gap at 10-6 for β = 0.0005.
12
Under review as a conference paper at ICLR 2020
We conducted exploratory experiments with variants of hμ which include thresholding between -1
and 1 and “penalizing” updates larger than g10 without observing statistically significant differences.
We also verified that randomly setting μt to 0 does not implement a successful heuristics, while
introducing another undesirable configuration parameter. We believe, however that there is further
space of improvement for hμ (and possibly to adapt the hyper-learning rate), since g1 does not
necessarily capture the long term dependencies of Problem 1. Meta-learning these update rules could
be an interesting direction that we leave to future investigation.
C Optimized and Online Schedules: Additional Details
We show in Figure 5 the LR schedules for the experiments described in Section 5 for the remaining
random seeds. The random seed controls the different initial points w0, which is the same for all
online methods and for LRS-OPT, and determines the mini-batch progression for the online methods
(while for LRS-OPT the mini-batch progression is randomized at each outer iteration).
Figure 5:	Comparison between optimized and online schedules for the remaining three seeds. For
each method, we report the schedule generated with the hyper-learning rate (or step-size for adapting
it) that achieves the best final validation accuracy.
D	Additional Experimental Results
We report in this section additional experimental results to complement the analysis of Section 6.
Figure 6 shows accuracy, validation loss and learning rates schedules for CIFAR-100 dataset with
a ResNet-18 model and SGDM as (inner) optimization methods. In accordance with previous
experimental validation (Zagoruyko & Komodakis, 2016; Huang et al., 2017) we set the initial
learning rate η0 to 0.1 for all methods. We include also relevant statistics for MARTHE0, that is
MARTHE obtained by letting the schedule start at 0 (cyan lines in the plots). Setting η0 = 0 is
clearly not an optimal choice, nevertheless MARTHE is able to obtain competitive results also in this
disadvantaged scenario, producing schedules that quickly reach high values and then sharply decrease
within the first 40 epochs. Figure 6 (left) reports a sample of generated schedule. It is important
to highlight how the heuristic methods, such as exponential decay, are not able to handle η0 = 0.
In fact, for non-adaptive methods, η0 is indeed another configuration parameter that must be tuned.
MARTHE0, on the other hand, constitute a virtually parameterless method (β can be quickly found
with the strategy outlined at the end of Section 4) that can be employed in situations where we have
no prior knowledge of the task at hand. Conversely as noted by Maclaurin et al. (2015) and Metz
et al. (2019), too high (initial) learning rates are not well suited for gradient-based adaptive strategies:
instability of the inner optimization dynamics indeed propagates to the hypergradient computation,
possibly leading to “exploding” hypergradients.
Finally, we tried another configuration of parameters for SGDR, in order to make the last restart
completing the training exactly after 200 epochs. We selected T0 = 10 and Tmul = 2.264 (i.e.
restarts at 10, 33, 84, 200). Figure 7 and Figure 8 report the same experiments of Section 6 with this
slightly different baseline. We note that the performance of SGDR remains similar.
13
Under review as a conference paper at ICLR 2020
3
0.12
0.10
0Λ)8
D0Q6∙
0.04
0Λ)2
0.00
Learning Rate - C∣FAR1OO ResNet
O 25 50 75 IOO 125 150 175 200
Epoch
Figure 6:	Results of ResNet-18 on CIFAR-100, and SGDM as the inner optimizer concerning: (Left) accuracy,
(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for each
method. In cyan We report the results for MARTHE with no = 0.
Figure 7:	Results of VGG-11 on CIFAR-10, and SGDM as the inner optimizer concerning: (Left) accuracy,
(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for each
method.
E SENSITIVITY ANALYSIS OF INADAPTIVE MARTHE WITH RESPECT TO η0 ,
μ AND β
In this section, We study the impact of no, μ and β for MARTHE, when our proposed online adaptive
methodologies for μ and β are not applied. We think that the sensitivity of the methods is very
important for the HPO algorithms to work well in practice, especially when they depend on the choice
of some (new) hyperparameters such as μ and β.
We show the sensitivity of inadaptive MARTHE with respect to no and μ, fixing β. We used VGG-11
on CIFAR-10 with SGDM as optimizer, but similar results can be obtained in the other cases. Figure
9 shows the obtained test accuracy fixing β to 10-7 (Left) and 10-8 (Right). The plots show a
certain degree of sensitivity, especially with respect to the choice of a (fixed) μ. This suggest that
implementing adaptive strategies to compute online the dampening factor μ and the hyper-learning
rate β constitute an essential factor to achieve the competitive results reported in Section 6 and
Appendix D.
Validation Loss - CIFAR100 ResNet

Figure 8:	Results of ResNet-18 on CIFAR-100, and Adam as the inner optimizer concerning: (Left) accuracy,
(Center) loss of the objective function on the validation set and (Right) generated learning rate schedule for each
method.
14
Under review as a conference paper at ICLR 2020
Figure 9: Sensitivity analysis of inadaptive MARTHE with respect to η0 and μ fixing the value of β
to 10-7 (Left) and 10-8 (Right). We used VGG-11 on CIFAR-10 with SGDM as optimizer. Darker
colors mean lower final accuracy.
15