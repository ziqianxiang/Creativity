Under review as a conference paper at ICLR 2020
SGD Learns One-Layer Networks in WGANs
Anonymous authors
Paper under double-blind review
Ab stract
Generative adversarial networks (GANs) are a widely used framework for learning
generative models. Wasserstein GANs (WGANs), one of the most successful vari-
ants of GANs, require solving a minmax optimization problem to global optimality,
but are in practice successfully trained using stochastic gradient descent-ascent.
In this paper, we show that, when the generator is a one-layer network, stochastic
gradient descent-ascent converges to a global solution with polynomial time and
sample complexity.
1 introduction
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are a prominent framework for
learning generative models of complex, real-world distributions given samples from these distributions.
GANs and their variants have been successfully applied to numerous datasets and tasks, including
image-to-image translation (Isola et al., 2017), image super-resolution (Ledig et al., 2017), domain
adaptation (Tzeng et al., 2017), probabilistic inference (Dumoulin et al., 2016), compressed sensing
(Bora et al., 2017) and many more. These advances owe in part to the success of Wasserstein GANs
(WGANs) (Arjovsky et al., 2017; Gulrajani et al., 2017), leveraging the neural net induced integral
probability metric to better measure the difference between a target and a generated distribution.
Along with the afore-described empirical successes, there have been theoretical studies of the
statistical properties of GANs—see e.g. (Zhang et al., 2018; Arora et al., 2017; 2018; Bai et al.,
2018; Dumoulin et al., 2016) and their references. These works have shown that, with an appropriate
design of the generator and discriminator, the global optimum of the WGAN objective identifies the
target distribution with low sample complexity.
On the algorithmic front, prior work has focused on the stability and convergence properties of gradi-
ent descent-ascent (GDA) and its variants in GAN training and more general min-max optimization
problems; see e.g. (Nagarajan & Kolter, 2017; Heusel et al., 2017; Mescheder et al., 2017; 2018;
Daskalakis et al., 2017; Daskalakis & Panageas, 2018a;b; Gidel et al., 2019; Liang & Stokes, 2019;
Mokhtari et al., 2019; Jin et al., 2019; Lin et al., 2019) and their references. It is known that, even
in min-max optimization problems with convex-concave objectives, GDA may fail to compute the
min-max solution and may even exhibit divergent behavior. Hence, these works have studied condi-
tions under which GDA converges to a globally optimal solution under a convex-concave objective,
or different types of locally optimal solutions under nonconvex-concave or nonconvex-nonconcave
objectives. They have also identified variants of GDA with better stability properties in both theory
and practice, most notably those using negative momentum.
In the context of GAN training, Feizi et al. (2017) show that for WGANs with a linear generator and
quadratic discriminator GDA succeeds in learning a Gaussian using polynomially many samples in
the dimension. In the same vein, we are the first to our knowledge to study the global convergence
properties of stochastic GDA in the GAN setting, and establishing such guarantees for non-linear
generators. In particular, we study the WGAN formulation for learning a single-layer generative
model with some reasonable choices of activations including tanh, sigmoid and leaky ReLU.
Our contributions. For WGAN with a one-layer generator network using an activation from a large
family of functions and a quadratic discriminator, we show that stochastic gradient descent-ascent
learns a target distribution using polynomial time and samples, under the assumption that the target
distribution is realizable in the architecture of the generator. This is achieved by a) analysis of the
dynamics of stochastic gradient-descent to show it attains a global optimum of the minmax problem,
and b) appropriate design of the discriminator to ensure a parametric O( √1n) statistical rate (Zhang
et al., 2018; Bai et al., 2018).
1
Under review as a conference paper at ICLR 2020
Related Work. We briefly review relevant results in GAN training and learning generative models:
-	Optimization viewpoint. For standard GANs and WGANs with appropriate regularization, Nagarajan
& Kolter (2017), Mescheder et al. (2017) and Heusel et al. (2017) establish sufficient conditions to
achieve local convergence and stability properties for GAN training. At the equilibrium point, if the
Jacobian of the associated gradient vector field has only eigenvalues with negative real-part at the
equilibrium point, GAN training is verified to converge locally for small enough learning rates. A
follow-up paper by (Mescheder et al., 2018) shows the necessity of these conditions by identifying
a prototypical counterexample that is not always locally convergent with gradient descent based
GAN optimization. However, the lack of global convergence prevents the analysis to provide any
guarantees of learning the real distribution.
The work of (Feizi et al., 2017) described above has similar goals as our paper, namely understanding
the convergence properties of basic dynamics in simple WGAN formulations. However, they only
consider linear generators, which restrict the WGAN model to learning a Gaussian. Our work goes
a step further, considering WGANs whose generators are one-layer neural networks with a broad
selection of activations. We show that with a proper gradient-based algorithm, we can still recover
the ground truth parameters of the underlying distribution.
More broadly, WGANs typically result in nonconvex-nonconcave min-max optimization problems.
In these problems, a global min-max solution may not exist, and there are various notions of local
min-max solutions, namely local min-local max solutions Daskalakis & Panageas (2018b), and local
min solutions of the max objective Jin et al. (2019), the latter being guaranteed to exist under mild
conditions. In fact, Lin et al. (2019) show that GDA is able to find stationary points of the max
objective in nonconvex-concave objectives. Given that GDA may not even converge for convex-
concave objectives, another line of work has studied variants of GDA that exhibit global convergence
to the min-max solution Daskalakis et al. (2017); Daskalakis & Panageas (2018a); Gidel et al. (2019);
Liang & Stokes (2019); Mokhtari et al. (2019), which is established for GDA variants that add
negative momentum to the dynamics. While the convergence of GDA with negative momentum is
shown in convex-concave settings, there is experimental evidence supporting that it improves GAN
training (Daskalakis et al., 2017; Gidel et al., 2019).
-	Statistical viewpoint. Several works have studied the issue of mode collapse. One might doubt the
ability of GANs to actually learn the distribution vs just memorize the training data (Arora et al., 2017;
2018; Dumoulin et al., 2016). Some corresponding cures have been proposed. For instance,Zhang
et al. (2018); Bai et al. (2018) show for specific generators combined with appropriate parametric
discriminator design, WGANs can attain parametric statistical rates, avoiding the exponential in
dimension sample complexity (Liang, 2018; Bai et al., 2018; Feizi et al., 2017).
Recent work of Wu et al. (2019) provides an algorithm to learn the distribution of a single-layer
ReLU generator network. While our conclusion appears similar, our focus is very different. Our
paper targets understanding when a WGAN formulation trained with stochastic GDA can learn in
polynomial time and sample complexity. Their work instead relies on a specifically tailored algorithm
for learning truncated normal distributions Daskalakis et al. (2018).
2	Preliminaries
We consider GAN formulations for learning a generator GA : Rk → Rd of the form z 7→ x = φ(Az),
where A is a d × k parameter matrix and φ some activation function. We consider discriminators
Dv : Rd → R or DV : Rd → R that are linear or quadratic forms respectively for the different
purposes of learning the marginals or the joint distribution. We assume latent variables z are sampled
from the normal N(0, Ik×k), where Ik×k denotes the identity matrix of size k. The real/target
distribution outputs samples X 〜D = Ga* (N(0, Ik0×k0)), for some ground truth parameters A*,
where A* is d X ko, and We take k ≥ ko for enough expressivity, taking k = d when ko is unknown.
The Wasserstain GAN under our choice of generator and discriminator is naturally formulated as:
min, maχ {f(A, V) ≡ Ex 〜D Dv (x) — Ez 〜N (0,ik×k)Dv (GA(Z))} .1
A∈Rd×k v∈Rd
1We will replace v by V ∈ Rd×d when necessary.
2
Under review as a conference paper at ICLR 2020
We use ai to denote the i-th row vector of A. We sometimes omit the 2 subscript, using kxk to denote
the 2-norm of vector x, and kX k to denote the spectral norm of matrix X. Sn ⊂ Rn×n represents all
the symmetric matrices of dimension n × n. We use Df (X0)[B] to denote the directional derivative
of function f at point X。with direction B: Df (X0)[B] = limt→o f(x0+tB)-f (XO).
3	Warm-up: Learning the Marginal Distributions
As a warm-up, we ask whether a simple linear discriminator is sufficient for the purposes of learning
the marginal distributions of all coordinates of D. Notice that in our setting, the i-th output of the
generator is φ(x) where X 〜N(0, kaik2), and is thus solely determined by Ilaik2. With a linear
discriminator Dv(x) = v>x, our minimax game becomes:
Aminfc max {fι(A, V) ≡ Ex〜D [v>x] - Ez〜N(0,ι%×k) [v>φ(Az)] } .	(1)
Notice that when the activation φ is an odd function, such as the tanh activation, the symmetric
property of the Gaussian distribution ensures that Ex〜D [v>x] = 0, hence the linear discriminator
in fι reveals no information about A*. Therefore specifically for odd activations (or odd plus a
constant activations), we instead use an adjusted rectified linear discriminator Dv (x) ≡ v>R(x - C)
to enforce some bias, where C = 2(φ(χ) + φ(-χ)) for all x, and R denotes the ReLU activation.
Formally, we slightly modify our loss function as:
fι(A, V) ≡ Ex〜D [v>R(x — C)] — Ez〜N(0,ik×k) [v>R(φ(Az) - C)].	⑵
We will show that we can learn each marginal of D if the activation function φ satisfies the following.
Assumption 1. The activation function φ satisfies either one of the following:
1.	φ is an odd function plus constant, and φ is monotone increasing;
2.	The even component of φ ,i.e. 2 (φ(x)+φ(-x)), is positive andmonotone increasing on X ∈ [0, ∞).
Remark 1. All common activation functions like (Leaky) ReLU, tanh or sigmoid function satisfy
Assumption 1.
Lemma 1. Suppose A* = 0. Consider fι with activation that satisfies Assumption 1.2 and fι with
activation that satisfies Assumption 1.1. The stationary points ofsuch fι and fι yield parameters A
satisfying kaik = kai*k,∀i ∈ [d].
To bound the capacity of the discriminator, similar to the Lipschitz constraint in WGAN, we regularize
the discriminator. For the regularized formulation we have:
Theorem 1. In the same setting as Lemma 1, alternating gradient descent-ascent with proper
learning rates on
minmax{f1 (A, V) — kVk2/2} or respectively	minmax{fι(A, V) — ∣∣v∣2∕2}
recovers A such that kaik = kai* k, ∀i ∈ [d].
All the proofs of the paper can be found in the appendix. We show that all local min-max points in the
sense of (Jin et al., 2019) of the original problem are global min-max points and recover the correct
norm of a*, ∀i. Notice for the source data distribution X = (xι, x2, ∙…Xd)〜D with activation φ,
the marginal distribution of each Xi follows φ(N (0, kai* k2)) and is determined by kai* k. Therefore
we have learned the marginal distribution for each entry i. It remains to learn the joint distribution.
4 Learning the Joint Distribution
In the previous section, we utilize a (rectified) linear discriminator, such that each coordinate vi
interacts with the i-th random variable. With the (rectified) linear discriminator, WGAN learns the
correct kai k, for all i. However, since there’s no interaction between different coordinates of the
random vector, we do not expect to learn the joint distribution with a linear discriminator.
To proceed, a natural idea is to use a quadratic discriminator DV (x) := x>Vx = hxx>, Vi to
enforce component interactions. Similar to the previous section, we study the regularized version:
min max {f2(A,V) — 3∣VkF},
A∈Rd×k V∈Rd×dU2' ,	'	2 11	11Fʃ,
(3)
3
Under review as a conference paper at ICLR 2020
where	f2(A,V) = Ex〜DDV(X)- Ez〜N(o,ik×k)Dv(Φ(Az))
=〈Ex〜D [χχ>] - Ez〜N(0,ik×k) [Φ(Az)Φ(Az)>] , V〉.
By adding a regularizer on V and explicitly maximizing over V :
g(A)	≡ max |f2(A,V) - 2IIVIlF j
12
=2 IIEx〜D [χx>] - Ez〜N(o,ik×k) [φ(Az)φ(Az)>] ∣∣f .
In the next subsection, we first focus on analyzing the second-order stationary points of g, then we
establish that gradient descent ascent converges to second-order stationary points of g .
4.1 Global Convergence for Optimizing the Generating Parameters
We first assume that both A and A* have unit row vectors, and then extend to general case since
we already know how to learn the row norms from Section 3. To explicitly compute g(A), we rely
on the property of Hermite polynomials. Since normalized Hermite polynomials {hi}i∞=0 forms an
orthonomal basis in the functional space, we rewrite the activation function as φ(x) = Pi∞=0 σihi,
where σi is the i-th Hermite coefficient. We use the following claim:
Claim 1 ((Ge et al., 2017) Claim 4.2). Let φ be a function from R to R such that φ ∈ L2 (R, e-x2/2),
and let its Hermite expansion be φ = i∞=1 σi hi. Then, for any unit vectors u, v ∈ Rd, we have that
∞
Ex〜N(0,Id×d) [φ(u>χ)φ(v>x)] = Xσ2(u>v)i.
i=0
Therefore we could compute the value of f2 explicitly using the Hermite polynomial expansion:
f2(A,V) = (Xσ ((A*(A*)>)°i — (AA>)°i) ,V) .
Here X ◦i is the Hadamard power operation where (X "j = (Xjk )i. Therefore we have:
∞
g(A)
∞
2 Xσ ((A*(A*)>)°i-(AA>)°i)
2
i=0
F
We reparametrize with Z = AA> and define g(Z) = g(A) with individual component functions
Gjk (z) ≡ 1 (P∞=0 σ2((z*k)i — zi))2. Accordingly Zjk =(a*, ak〉is the (j, k)-th component of the
ground truth covariance matrix A* (A* )> .
Assumption 2. The activation function φ is an odd function plus constant. In other words, its
Hermite expansion φ = i∞=0 σi hi satisfies σi = 0 for even i ≥ 2. Additionally we assume σ1 6= 0.
Remark 2. Common activations like tanh and sigmoid satisfy Assumption 2.
Lemma 2. For activations including leaky ReLU and functions satisfying Assumption 2, gG(Z) has a
unique stationary point where Z = A* (A*)>.
Notice gG(Z) = jk gGjk(zjk) is separable across zjk, where each gGjk is a polynomial scalar function.
Lemma 2 comes from the fact that the only zero point for gGj0 k is zjk = zj*k, for odd activation φ and
leaky ReLU. Then we migrate this good property to the original problem we want to solve:
Problem 1. We optimize over function g when Iai* I = 1, ∀i:
1
2
minA
g(A)
∞
X σ ((A*(A*)>)°i-(AA>)°i)
i=0
s.t.	ai> ai = 1, ∀i.
4
Under review as a conference paper at ICLR 2020
Existing work JoUrnee et al. (2008) connects g(Z) to the optimization over factorized version for
g(A) (g(A) ≡ g(AA>)). Specifically, when k = d, all second-order stationary points for g(A) are
first-order stationary points for g(Z). Though g is not convex, We are able to show that its first-order
stationary points are global optima when the generator is sUfficiently expressive, i.e., k ≥ k0 . In
reality we won’t know the latent dimension k0, therefore we just choose k = d for simplicity. We
make the following conclusion:
Theorem 2. For activations including leaky ReLU and functions satisfying Assumption 2, when
k = d, all second-order KKT points for problem 1 are its global minimum. Therefore alternating
projected gradient descent-ascent on Eqn. (3) converges to A : AA> = A* (A*)>.
The extension for non-unit vectors is straightforward, and we defer the analysis to the Appendix.
5	Finite Sample Analysis
Algorithm 1 Online stochastic gradient descent ascent on WGAN
1: Input: n training samples: xι, x2,•…Xn, where each Xi 〜φ(A*z), Z 〜N(0, Ik×k), learning
rate for generating parameters η, number of iterations T .
2: Random initialize generating matrix A1 * * (0) * * .
3: for t = 1,2, ∙∙∙ ,T do
4:	Generate m latent variables z(t), z2t),…,Zm〜N(0, Ik×k) for the generator. The empirical
function becomes
ft)n(A,V )
m
Im Xφ(AZ(t))φ(AZ(t) )>
i=1
-nX MV)- 1 kV k2
i=1
5:	Gradient ascent on V with optimal step-size ηV = 1:
V㈤一V⑴一ηvVVfm)n(A(tτ), V(t-1)).
6:	Sample noise e uniformly from unit sphere
7:	Projected Gradient Descent on A, with constraints C = {A|(AA>)ii = (A*A*>)ii} :
A㈤ 一 Projc(A(I)- n(VA魏n(*T, V(t)) + e)).
8:	end for
9:	Output: A(T)(A(T))>
In this section, we consider analyzing Algorithm 1, i.e., gradient descent ascent on the following:
mn
魏n (A,V )=( m X Φ(Az(t))Φ(Az(t))> — n X XiX>,V)
m i=1	n i=1
-2 kV k2.
(4)
Notice in each iteration, gradient ascent with step-size 1 finds the optimal solution for V . By
Danskin’s theorem (Danskin, 2012), our min-max optimization is essentially gradient descent over
g(m,n(A) ≡ maxv fm)n(A,V) = 2 k m1 Pm=I φ(Az(t))φ(Az(t))> - n Pn=I XiX>kF with a batch
of samples {z(t)}, i.e., stochastic gradient descent for fn(A) ≡ Ezi〜N(o,ik×Q,∀i∈[m] [gm,n(A)].
Therefore to bound the difference between fn (A) and the population risk g(A), we analyze the
sample complexity required on the observation side (Xi 〜D,i ∈ [n]) and the mini-batch size
required on the learning part (φ(Azj), Zj 〜N(0, Ik×k),j ∈ [m]). We will show that with large
enough n, m, the algorithm specified in Algorithm 1 that optimizes over the empirical risk will yield
the ground truth covariance matrix with high probability.
Our proof sketch is roughly as follows:
1.	With high probability, projected stochastic gradient descent finds a second order stationary point A
of fn(∙) as shown in Theorem 31 of (Ge et al., 2015).
5
Under review as a conference paper at ICLR 2020
2.	For sufficiently large m, our empirical objective, though a biased estimator of the population
risk g(∙), achieves good ^-approximation to the population risk on both the gradient and Hessian
(Lemmas 4&5). Therefore A is also an O(e)-approximate second order stationary point (SOSP) for
the population risk g(A).
3.	We show that any -SOSP A for g(A) yields an O()-first order stationary point (FOSP) Z ≡ AA>
for the semi-definite programming on g(Z) (Lemma 6).
4.	We show that any O(E)-FOSP of function g(Z) induces at most O(E) absolute error compared to
the ground truth covariance matrix Z* = A* (A*)> (Lemma 7).
5.1	Observation Sample Complexity
For simplicity, we assume the activation and its gradient satisfy Lipschitz continuous, and let the
Lipschitz constants be 1 w.l.o.g.:
Assumption 3. Assume the activation is 1-Lipschitz and 1-smooth.
To estimate observation sample complexity, we will bound the gradient and Hessian for the population
risk and empirical risk on the observation samples:
12
g(A)	≡	2 IlEx〜D [χχ>] - Ez〜Nar”)[φ(Az)φ(Az)> ∣∣f , and
1
2
gn (A)	≡
1n
x'x2-iχi - Ez 〜N (0,Ik×k) [φ(Az)φ(Az)>]
n i=1
2
F
Claim 2.
Vg(A) - Vgn(A) = 2Ez [diag(φ0(Az))(X - Xn)φ(Az)z>],
where X = Ex〜D [xx>], and Xn = 1 En=ι XiX>∙ The direCtional derivative with arbitrary
direction B is:
DVg(A)[B] - DVgn(A)[B] =2Ez [diag(φ0(Az))(Xn - X)φ0(Az) ◦ (Bz)z>]
+2Ez [diag(φ00(Az) ◦ (Bz))(Xn - X)φ(Az)z>]
Lemma 3. Suppose the activation satisfies Assumption 3. Pr[kX - Xn k ≤ EkX k] ≥ 1 - δ, for
n ≥ Θ(d∕E log2(1∕δ))2.
Lemma 4. Suppose the activation satisfies Assumption 2&3. With samples n ≥ Θ(d∕E log2(1∕δ)),
kVg(A) - Vgn(A)k2 ≤ O(EdkAk2) with probability 1 - δ. Meanwhile, kDVg(A)[B] -
D Vgn (A)[B]k2 ≤ O(Ed3/2 kAk2 kBk2) with probability 1 - δ.
5.2	B ounding Mini-batch Size
Normally for empirical risk for supervised learning, the mini-batch size can be arbitrarily small
since the estimator of the gradient is unbiased. However in the WGAN setting, notice for each
iteration, we randomly sample a batch of random variables {zi}i∈[m] , and obtain a gradient of
gm,n(A) ≡ 2 II1 Pn=I XiX> — m Pm=I Φ(Azj)Φ(Azj)>∣∣f, in Algorithm 1. However, the finite
sum is inside the Frobenius norm and the gradient on each mini-batch may no longer be an unbiased
estimator for our target gn(A) = 1 ∣∣1 Pn=I XiX> — Ez [φ(Az)φ(Az)>] ||；.
In other words, we conduct stochastic gradient descent over the function f (A) ≡ Ezgmn(A)
Therefore wejust need to analyze the gradient error between this f (A) and gn(A) (i.e. gmn is almost
an unbiased estimator of gn). Finally with the concentration bound derived in last section, we get the
error bound between f(A) and g(A).
Lemma 5. The empirical risk gm,n is almost an unbiased estimator of gn. Specifically, the expected
function f (A) = Ezi〜N(o,ik×Q,i∈[m] [gm,n] satisfies:
kVf(A) -Vgn(A)k ≤O(1 kAk3d2).
m
2Θ hides log factors of d for simplicity.
6
Under review as a conference paper at ICLR 2020
For arbitrary direction matrix B,
∣∣DVf(A)[B] - DVgn(A)[B]k ≤ θ(ɪ|田|||因间5/2).
m
In summary, we conduct concentration bound over the observation samples and mini-batch sizes,
and show the gradient of f(A) that Algorithm 1 is optimizing over has close gradient and Hessian
with the population risk g(A). Therefore a second-order stationary point (SOSP) for f(A) (that
our algorithm is guaranteed to achieve) is also an approximated SOSP for g(A). Next we show
such a point also yield an approximated first-order stationary point of the reparametrized function
g(Z) ≡ g(A),∀Z = AA>.
5.3	Relation on Approximate Optimality
In this section, We establish the relationship between g and g. We present the general form of our
target Problem 1:
minA∈Rd×k g(A) ≡ g(AAv)	(5)
s	.t.	Tr(A>XiA) = yi, Xi ∈ S,y% ∈ R, i = 1,…，n.
Similar to the previous section, the stationary property might not be obvious on the original problem.
Instead, we could look at the re-parametrized version as:
minz∈s g(Z)	(6)
s	.t.	Tr(XiZ) = yi,Xi ∈ S,y ∈ R,i = 1,…，n,
Z 0,
Definition 1. A matrix A∈ Rd×k is called an -approximate second-order stationary point (-SOSP)
of Eqn. (5) if there exists a vector λ such that:
Tr(A>XiA) = yi, i ∈ [n]
k(Vz g(AA>) - En=I λiXi )a j k ≤ Ekaj ∣∣, {a j }j SPan the column space of A
Tr(B>DVAL(A, λ)[B]) ≥ -kBk2,	∀B s.t. Tr(B>XiA) = 0
Here L(A, λ) is the Lagrangianform g(AA>) — Pn=ι λi(Tr(A>XiA) — yi).
Specifically, when E = 0 the above definition is exactly the second-order KKT condition for optimiz-
ing (5). Next we present the approximate first-order KKT condition for (6):
Definition 2. A symmetric matrix Z ∈ Sn is an E-approximate first order stationary point of function
(6) (E-FOSP) if and only if there exist a vector σ ∈ Rm and a symmetric matrix S ∈ S such that the
following holds:
(Tr(XiZ) = y,i ∈ [n]
I Z 占 O,
S-EI,
I ∣∣Sajk ≤ d∣ajk,	{aj}j span the column space of Z
[ S = VZg(Z) - Pn=ι σiXi.
Lemma 6. Let latent dimension k = d. For an E-SOSP of function (5) with A and λ, it infers an
E-FOSP of function (6) with Z, σ and S that satisfies: Z = AA>, σ = λ and S = VZ g(AA>)—
PiλiXi.
Now it remains to show an E-FOSP of g(Z) indeed yields a good approximation for the ground truth
parameter matrix.
Lemma 7. If Z is an E-FOSP offunction (6), then ∣∣Z — Z* ∣f ≤ O(E). Here Z* = A*(A*)> is the
optimal solution for function (6).
Together with the previous arguments, we finally achieve our main theorem on connecting the
recovery guarantees with the sample complexity and batch size3:
Theorem 3. For arbitrary δ < 1, e, given small enough learning rate η < 1/poly(d, 1/e, log(1∕δ)),
let sample size n ≥ Θ(d5∕E2 log2(1∕δ)), batch size m ≥ Ο(d5/e), for large enough
T =poly( 1∕η, 1/e, d, log(1∕δ)), the output of Algorithm 1 satisfies ∣∣A(T )(A(T ))> — Z *∣f ≤ O(E)
with probability 1 - δ, under Assumptions 2 & 3 and k = d.
3The exact error bound comes from the fact that when diagonal terms of AA> are fixed, k A∣∣2 = 0(ʌ/d).
7
Under review as a conference paper at ICLR 2020
Figure 1: Recovery error (∣∣AA> - Z*∣∣f) with different observed sample sizes n and output
dimension d.
6	Simulations
In this section, we provide simple experimental results to validate the performance of stochastic
gradient descent ascent and provide experimental support for our theory.
We focus on Algorithm 1 that targets to recover the parameter matrix. We conduct a thorough
empirical studies on three joint factors that might affect the performance: the number of observed
samples m (we set n = m as in general GAN training algorithms), the different choices of activation
function φ, and the output dimension d. In Figure 1 we plot the relative error for parameter estimation
decrease over the increasing sample complexity. We fix the hidden dimension k = 2, and vary
the output dimension over {3, 5, 7} and sample complexity over {500, 1000, 2000, 5000, 10000}.
Reported values are averaged from 20 runs and we show the standard deviation with the corresponding
colored shadow. Clearly the recovery error decreases with higher sample complexity and smaller
output dimension.
(a) leaky ReLU activation (α = 0.2)
(b) tanh activation
Figure 2: Comparisons of different performance with leakyReLU and tanh activations. Same color
starts from the same starting point. For both cases, parameters always converge to true covariance
matrix. Each arrow indicates the progress of 500 iteration steps.
To visually demonstrate the learning process, we also include a simple comparison for different φ: i.e.
leaky ReLU and tanh activations, when k = 1 and d = 2. We set the ground truth covariance matrix
to be [1, 1; 1, 1], and therefore a valid result should be [1, 1] or [-1, -1]. From Figure 2 we could see
that for both leaky ReLU and tanh, the stochastic gradient descent ascent performs similarly with
exact recovery of the ground truth parameters.
7	Conclusion
We analyze the convergence of stochastic gradient descent ascent for Wasserstein GAN on learning a
single layer generator network. We show that stochastic gradient descent ascent algorithm attains
the global min-max point, and provably recovers the parameters of the network with absolute error
measured in Frobenius norm, from Θ(d5/e2) i.i.d samples.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 224-232. JMLR. org, 2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and
empirics. 2018.
Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in
GANs. arXiv preprint arXiv:1806.10586, 2018.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative
models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
537-546. JMLR. org, 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
John M Danskin. The theory of max-min and its application to weapons allocation problems,
volume 5. Springer Science & Business Media, 2012.
Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and
constrained min-max optimization. arXiv preprint arXiv:1807.04252, 2018a.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, pp. 9236-9246,
2018b.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. arXiv preprint arXiv:1711.00141, 2017.
Constantinos Daskalakis, Themis Gouleakis, Christos Tzamos, and Manolis Zampetakis. Efficient
statistics, in high dimensions, from truncated samples. In the 59th IEEE Annual Symposium on
Foundations of Computer Science (FOCS), 2018.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky,
and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding GANs: the LQG setting.
arXiv preprint arXiv:1710.10793, 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Le Priol, Gabriel Huang,
Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
9
Under review as a conference paper at ICLR 2020
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pp. 6626-6637, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Minmax optimization: Stable limit points of
gradient descent ascent are locally optimal. arXiv preprint arXiv:1902.00618, 2019.
Michel JoUmee, Francis Bach, P-A AbsiL and RodolPhe Sepulchre. Low-rank optimization for
semidefinite convex problems. arXiv preprint arXiv:0807.4423, 2008.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta,
Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image
super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4681-4690, 2017.
Tengyuan Liang. On how well generative adversarial networks learn densities: Nonparametric and
parametric results. arXiv preprint arXiv:1811.03179, 2018.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. In the 22nd International Conference on Artificial Intelligence
and Statistics ( AISTATS), 2019.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. arXiv preprint arXiv:1906.00331, 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually converge? arXiv preprint arXiv:1801.04406, 2018.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. arXiv preprint
arXiv:1901.08511, 2019.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5585-5595, 2017.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7167-7176, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by
one-layer relu networks. arXiv preprint arXiv:1909.01812, 2019.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. 2018.
10
Under review as a conference paper at ICLR 2020
A Omitted Proof for Learning the Distribution
A. 1 Stationary Point for Matching First Moment
φ(x)
Proof of Lemma 1. To start with, we consider odd-plus-constant monotone increasing activations.
Notice that by proposing a rectified linear discriminator, we have essentially modified the activation
function as φ := R(φ - C), where C = 1 (φ(χ) + φ(-χ)) is the constant bias term of φ. Observe
that we can rewrite the objective fι for this case as follows:
fι(A, V)= Ez〜N(0,ik0×ko)V>Φ(A*z)- Ez〜N(0,ik×k)vτφ>(Az).
Moreover, notice that φ is positive and increasing on its support which is [0, +∞).
Now let us consider the other case in our statement where φ has a positive and monotone increasing
even component in [0, +∞). In this case, let us take:
φ(x) + φ(-x), x ≥ 0
0,	o.w.
Because of the symmetry of the Gaussian distribution, we can rewrite the objective function for this
case as follows:
f1(A, V)= Ez〜N(θ,lk0×k0)v>φ(A*z) - Ez〜N(0,ik×k)vτφ>(Az).
Moreover, notice that φ is positive and increasing on its support which is [0, +∞).
To conclude, in both cases, the optimization objective can be written as follows, where φ satisfies
Assumption 1.2 and is only non-zero on [0, +∞).
f1(A, V)= Ez〜N(θ,lk0×k0)v>φ(A*z) - Ez〜N(0,ik×k)vτφ>(Az).
The stationary points of the above objective satisfy:
(
▽vfι(A, V)= Ez〜N(O,ik0×k0)φ(A Z)- Ez〜N(0,ik×k)(φ(Az) = 0,
▽ajf1(A, V) = -Ez〜N(0,Ik×k)vjφ(a>z)z =0.
We focus on the gradient over V. To achieve ▽vf1(A, V) = 0, the stationary point satisfies:
∀j, Ez〜N(0,lk0×k0)3((a；)>Z)= Ez〜N(0,lk×k)φ(aj z), i∙e∙
0
∀j, Ex〜N(0,ka*k2)φ(x) = ExO〜N(0,kajk2)φ(X ).	(T)
To recap, for activations φ that follow Assumption 1, in both cases we have written the necessary
condition on stationary point to be Eqn. (7), where φ is defined differently for odd or non-odd
activations, but in both cases it is positive and monotone increasing on its support [0, ∞). We then
argue the only solution for Eqn. (7) satisfies ∣∣ajk = ∣∣a*k,∀j. This follows directly from the
following claim:
Claim 3. Thefunction h(α) := Ex 〜N (0,α2)f (x), a > 0 is a monotone increasing function if f is
positive and monotone increasing on its support [0, ∞).
We could see from Claim 3 that the LHS and RHS of Eqn. (7) is simply h(∣aj∣∣) and h(∣a*∣) for
each j. Now that h is an monotone increasing function, the unique solution for h(∣aj∣∣) = h(∣a*∣)
is to match the norm: ∣aj∣ = ∣∣a*∣, ∀j.
Proof of Claim 3.
h(α)
Ex 〜N (0,α2)f (x)
y: = x/a
f
0
Z
0
∞	/、-
f (x)e 2ɑ2 dx
αf(αy)e-* dy
Ey 〜N (0,i)αf(αy).
11
Under review as a conference paper at ICLR 2020
Notice h0(α) = Ex〜N(0,1)[αxf0(ax) + f (ax)]. Since f, f, and α > 0, and we only care about the
support of f where X is also positive, therefore h0 is always positive and h is monotone increasing. □
To sum up, at stationary point where Vf 1 (A, V) = 0, we have
∀i,∣∣a"∣ = Ilail∣.
□
A.2 Proof of Theorem 1
Proof of Theorem 1. We will take optimal gradient ascent steps with learning rate 1 on the discrimi-
nator side v, hence the function we will actually be optimizing over becomes (using the notation for
φ from section A.1):
h(A)= mVχfι(A, V) = 2 快〜N(o,ik0×k0)φ(A*z) - Ez〜N(0,ik×k)φ(Az)∣∣ .
We just want to verify that there’s no spurious local minimum for h(A). Notice there’s
no interaction between each row vector of A. Therefore we instead look at each
hi := 2 (Ez〜N(0,ik0×k0)φ(S)>z)— Ez〜N(o,ik×k)φm>z)) foreach i. NoW vhi(ai) =
-(Ez〜N(θ,lk0×k0)φ>((ai)τz) - Ez〜N(0,Ik×k)φm>zD (Ez〜N(0,lk×k)zφ(a>zT). Due to the
symmetry of the Gaussian, we take ai = ae1 , where a = Iai I. It is easy to see that checking
whether Ez〜N(o,ik×k)ζφ0(aT z) = 0 is equivalent to checking whether Ezi〜N(o,i)Zιφ0(azι) = 0.
Recall that φ is supported on [0, +∞) and it is monotonically increasing on its support. Hence,
ττn	7// ∖ / l-∖ 1	Γ∖ T T	Il Il ! I-X ∖ ! - rɪ-il X-7 7 / Λ ∖ Γ∖ ∙ /'/'
Ezi〜N(0,i)ZιΦo(azι) = 0 unless a = 0. Hence, suppose ∣∣aik = 0,∀i. Then VAh(A) = 0 iff
h(A) = 0, i.e. Ez〜N(o,iko ×ko)φ(A Z) = Ez〜N(0,ik×k)φ(Az).
Therefore all stationary points of h(A) are global minima where Ez〜N(0,ik ×k )Φ(A*ζ) =
Ez 〜N (o,ik×k) Φ(Aζ) and according to Lemma 1, this only happens when ∣∣aik = ∣∣a*∣, ∀i ∈ [d], □
A.3 Stationary Points for WGAN with Quadratic Discriminator
ProofofLemma 2. To study the stationary point for g(Z) = Ejk gjk (Zjk), we look at individual
gjk(Z) ≡1 (P∞=o σi ((Zjky-Z ))2.
Notice for odd-plus-constant activations, σi is zero for even i > 0. Recall our assumption in Lemma 2
also requires that σ1 6= 0. Since the analysis is invariance to the position of the matrix Z, we simplify
the notation here and essentially want to study the stationary point for f (a) = 2(Pi odd σ2 (ai 一 bi))2
for some constant b and σi , where σ1 6= 04.
f0(a)
Xσi2(ai - bi)	Xiσi2ai-1
i odd	i odd
(a - b)(σ2 + X σi N ∣ ∣σ2 + X '…
i≥3 odd	a	i≥3 odd
(a - b)(I)(II).
Notice now f0(a) = 0 ⇔ a = b. This is because the polynomial f0(a) is factorized to a - b and two
factors I and II that are always positive. Notice here we use caα-b to denote Pj=° ajbi-j, which is
always nonnegative. This is simply because ai - bi always shares the same sign as a - b when i is
odd. Therefore I=σ2 + pi≥3 odd σ2 a-b > 0, ∀a.
4The zero component has been cancelled out.
12
Under review as a conference paper at ICLR 2020
Meanwhile, since ai-1 is always nonnegative for each odd i, we have II= σ12 + Pi≥3 odd iσi2ai-1 is
also always positive for any a.
Next, for activation like ReLU, loss Ojk(Z) = 2(h(z) — h(zjk))2, where h(x) = ∏(√1 - x2 + (π 一
CosT(X))X) (DanieIy et al., 2016). Therefore h0(-1) = 0 for any Zjk. This fact prevents US from
getting the same conclusion for ReLU.
However, for leaky ReLU with coefficient of leakage α ∈ (0, 1), φ(X) = max{X, αX} = (1 -
α)ReLU(X) + αX.
We have
Ez〜N(0,ik×k [φ(a>z)φ(a>z)]
=(1 - α)2EzReLU(ai>z)ReLU(aj>z) + (1 - α)αEzReLU(ai>z)aj>z
+ (1 - α)αEzai>zReLU(aj>z) + α2Ezai>zaj>z
=(1 - α)2 h(ai>aj ) + αai>aj
Therefore for leaky ReLU gjk(z) = 11 ((1 — ɑ)2(h(z) — h(zjk*)) + α(z — Zjk))2, and gjk(z)=
((1 — α)2(h(z) — h(zjk*)) + α(z — Zjk))((I — a)2h0(z) + α). NoW with α > 0, (1 — a)2h0(z) + α ≥ α
for all Z and gOjk(Z) = 0 ⇔ Z = Zjjk.
To sUm Up, for odd activations and leaky ReLU, since each gOjk (Z) only has stationary point of
Z = Zjjk, the stationary point Z of gO(Z) = Pjk gOjk also satisfy Z = Zj = Aj (Aj)>.
□
Proof of Theorem 2. Instead of directly looking at the second-order stationary point of Problem 1,
we look at the following problem on its reparametrized version:
Problem 2.
minZ
s.t.
gO(Z) =
Zii = 1, ∀i.
Z 0.
∞
X σ ((Z *产-Z。)
i=0
Here Zj = Aj(Aj)> and satisfies Ziji = 1, ∀i.
Compared to fUnction g in the original problem 1, it satisfies that gO(AA>) ≡ g(A).
A matrix Z satisfies the first-order stationary point for Problem 2 if there exists a vector σ sUch that:
Zii = 1,
Z 0,
S0,
SZ = 0,
S = NZ g(Z) — diag(σ).
13
Under review as a conference paper at ICLR 2020
Therefore for a stationary point Z, since Z* = A*(A*)> 占 0, and S 占 0, we have〈S, Z* — Z)
hS, Z* i ≥ 0. Meanwhile,
hZ* — Z,Si
=(Z* — Z, Vzf (Z)- diag(σ)i
=(Z* — Z, VZf (Z)〉	(diag(Z* - Z)=0)
=	(zi*j — zij)gi0j (zij)
i,j
= X(zij — zi*j)P (zij)(zi*j — zij)
i,j
(Refer to proof of Lemma 2 for the value of g0)
= — X(zij — zi*j)2P(zij)
≤0	(P is always positive)
Therefore hS, Z* — Zi = 0, and this only happens when Z = Z*.
Finally, from JoUrnee et al. (2008) we know that any first-order stationary point for Problem 2
is a second-order stationary point for our original problem 1 5. Therefore we conclude that all
second-order stationary point for Problem 1 are global minimum A: AA> = A*(A*)>.	□
A.4 Landscape Analysis for Non-unit Generating Vectors
In the previous argument, we simply assume that the norm of each generating vectors ai to be 1. This
practice simplifies the computation but is not practical. Since we are able to estimate kai k for all i
first, we could analyze the landscape of our loss function for general matrix A.
The main tool is to use the multiplication theorem of Hermite functions:
b n2C
hnα (x) := hn (αx) = X αn-2i (α2
i=0
-1)i⑦等
2-ihn-2i(x).
For the ease of notation, we denote the coefficient as ηn,i := αn-2i(α2 — 1)i g) (2!)!2-i. We extend
the calculations for Hermite inner product for non-standard distributions.
Lemma 8. Let (x, y) be normal variables that follow joint distribution N(0, [[α2, αβρ]; [αβρ, β2]]).
Then,
E[hm(x)hn(y)] = ʃ Pb= 0 ηlɑiηlβ PIfifm ≡ n (mod 2)
0	o.w.
(8)
Here l = min{m, n}.
5Throughout the analysis for low rank optimization in Journee et al. (2008), they require function g(Z) to be
convex. However, by carefully scrutinizing the proof, one could see that this condition is not required in building
the connection of first-order and second-order stationary points of g(A) and gg(Z). For more cautious readers,
we also show a relaxed version in the next section, where the equivalence of SOSP of g and FOSP of gg is a
special case of it.
14
Under review as a conference paper at ICLR 2020
Proof. Denote the normalized variables X = x∕ɑ, y = y∕β. Let l = min{m, n}.
E[hm(x)hn(y)]
=E[hm(X)hβ(y)]
b m C b 2 C
=XXηm,iηn,jE[hm-2i(X)hn-2j (y)]
i=0 j=0
b m c b 2 c
=XXηαm,iηβn,jδ(m-2i),(n-2j)ρn-2j	(Lemma ??)
i=0 j=0
=ʃ Pb= 0 ηlαiηlβiρl-2i if m ≡ n (mod 2)
0	o.w.
□
Now the population risk becomes
12
g(A) =2 IlEx〜D [χχ>] - Ez〜N©I”)[φ(Az)φ(Az)>]∣∣
=2 X (Ez~N(o,ik0×k0)o((a">Z)φ((W)Tz) - Ez~N(o,ik×k)φm>z欣a>Z))
i,j∈[d]
≡ 2 Egij (Zij ).
i,j
To simplify the notation, for a specific i, j pair, We write X = a>ζ∕α, α = ∣∣aik and y = a>z∕β,
where β = ∣∣aj∣∣. Namely we have (X, y)〜N(0, [[1, ρ]; [ρ, 1]]), where P = Coshai, aj). Again,
recall φ(αx) = Pk odd σihi(ɑx) = Pk odd σihα(X).
Ez 〜N (0,ik×k)[φ(ax)φ(βy)]
=E X σmhm(X) X σnhβ(y
m odd	n odd
=X σmσnES [hm(x)hβ(y)]
m,n odd
b 2C
= X σm X σn Xηαn,kηβn,kρn-2k
m odd	n≤m odd	k=0
Therefore we could write out explicitly the coefficient for each term ρk, k odd, as: ck =
n — k n - k
Pn≥k odd σnηα, F 9β F (Pm≥n σm). WehaVe gij (Zij ) = (Pk 戏 Ck Zkj- Pk odd Ck(Z 卷) )乙
Now suppose σi to have the same sign, and ∣αi∣ ≥ 1, ∀ or ∣αi∣ ≤ 1, ∀i, each coefficient ci ≥ 0.
Therefore still the only stationary point for g(Z) is Z*.
B Omitted Proofs for Sample Complexity
B.1 Omitted Proofs for Relation on Approximate Stationary Points
Proof of Lemma 6. We first review what we want to prove. For a matrix A that satisfies -approximate
SOSP for Eqn. (5), we define SA = NZg(AA>) 一 PZi λiXi. The conditions ensure that A, λ, SA
satisfy:
[Tr(A>XiA)= yi,
k IlSAa i∣∣2 ≤ Eka i∣2,	{a j} j span the column space of A (9)
[Tt(b>DaVaL(A, λ)[B]) ≥ -e∣B∣∣F, ∀B s.t. Tr(B>XiA) = 0.
15
Under review as a conference paper at ICLR 2020
We just want to show Z := AA>, σ := λ, and S := SA satisfies the conditions for -FOSP of Eqn.
(6). Therefore, by going over the conditions, its easy to tell that all other conditions automatically
apply and it remains to show SA -I.
By noting that VaL(A, λ) = 2SaA, one has:
∣Tr(B>DAVAL(A,λ)[B])
n
=Tr(B>SaB) + Tr(B>DaVzg(AA>)[B]A) - XDa%[B] Tr(B>X，A)
i=1
(from Lemma 5 of JOurn6e et al. (2008))
=Tr(B>SaB) + Tr(AB>DaVz g(AA>)[B])	(10)
(From Eqn. (9) we have Tr(B>XiA) = 0)
Notice that A ∈ Rd×k and we have chosen k = d for simplicity. We first argue when A is rank-
deficient, i.e. rank(A) < k. There exists some vector v ∈ Rk such that Av = 0. Now for any vector
b ∈ Rd, let B = bv>. Therefore AB> = Avb> = 0. From (10) we further have:
∣Tr(B>DaVaL(A, λ)[B])
= Tr(B>SaB) + Tγ(AB>Da Vz g(AA>)[B])
= Tr(vb>SAbv>) = kvk2b>SAb
≥ - /2kBk2F	(from (9))
= - /2kvk2kb>k2
Therefore from the last three rows we have b>SAb ≥ -/2kbk2 for any b, i.e. SA -/2Id×d.
On the other hand, when A is full rank, the column space of A is the entire Rd vector space, and
therefore SA -Id×d directly follows from the second line of the -SOSP definition.
□
B.2 Detailed Calculations
Recall the population risk
12
g(A) ≡ 2 IlEx〜D [χx>] - Ez〜Nar”)[φ(Az)φ(Az)> ∣∣F.
Write the empirical risk on observations as:
1
2
gn(A)	≡
1n
n Exixl—Ez 〜N (o,ik×k)[。(Az)φ(Az)>
i=1
2
F
Claim 4.
Vg(A) -Vgn(A) =2Ez diag(φ0(Az))(X - Xn)φ(Az)z> ,
where X = Ex〜D [xx>], and Xn = 1 En=I xix> .
Proof.
Vg (A) - Vgn(A) = V(g (A) - gn(A))
=1V〈X - Xn, X + Xn- 2Ez〜N(0,ik×k) [φ(Az)φ(Az)>D
=V〈Xn - X, Ez〜N(0,ik×k) [φ(Az)φ(Az)>D
Now write S(A) = φ(Az)φ(Az)> .
[S(A + ∆A) - S(A)]ij
=φ(ai>z + ∆ai>z)φ(aj>z + ∆aj>z) - φ(ai>z)φ(aj>z)
=φ0(ai>z)∆ai>zφ(aj>z) + φ0(aj>z)∆aj>zφ(ai>z) + O(k∆Ak2)
16
Under review as a conference paper at ICLR 2020
Therefore
[S(A + ∆A) - S(A)]i:
=φ0(ai>z)∆ai>zφ(Az)> + (φ0(Az) ◦ ∆Az)>φ(ai>z) + O(k∆Ak2)
Therefore
S(A + ∆A) - S(A) = diag(φ0(Az))∆Azφ(Az)> + φ(Az)z>∆A>diag(φ0(Az)).	(11)
And
g(A + ∆A) - gn(A + ∆A) - (g(A) - gn(A))
=hXn-X,Ez[S(A+∆A) -S(A)]i
=EzhXn - X, diag(φ0(Az))∆Azφ(Az)> + φ(Az)z>∆A>diag(φ0(Az))i
=2Ezhdiag(φ0(Az))(Xn - X)φ(Az)z>, ∆Ai.
Finally we have Vg(A) - Vgn(A) = 2Ez [diag(φ0(Az))(Xn 一 X)φ(Az)z>].	□
Claim 5. For arbitrary matrix B, the directional derivative of Vg(A) — Vgn(A) with direction B
is:
DAVg(A)[B] -DAVgn(A)[B]
= 2Ez [diag(φ0(Az))(Xn - X)φ0(Az) ◦ (Bz)z>
+2Ez [diag(φ00(Az) ◦ (Bz))(Xn - X)φ(Az)z>
Proof.
g(A + tB)
=2Ez [diag(φ0(Az + tBz))(Xn - X)φ(Az + tBz)z>
=2Ez [diag(φ0(Az) + t(Bz) ◦ φ00(Az))(Xn — X)(φ(Az) + tφ0(Az) ◦ (Bz))z>] + O(t2)
Therefore
lim g(A+tB)-g(A)
t—→0	t
=2Ez [diag(φ0(Az))(Xn - X)φ0(Az) ◦ (B>z)z>]
+2Ez [diag(φ00(Az) ◦ (Bz))(Xn - X)φ(Az)z>]
□
B.3 Omitted Proofs for Ob servation Sample Complexity
ProofofLemma 3. For each	Xi	=	φ(Azi),	Zi	〜	N(0,Ik×*).	Each coordinate	∣xi,j |	=
∣φ(a>Zi )| ≤ ∣a>z∕ since φ is I-LiPsChitz. 6. Without loss of generality we assumed ∣∣aj k = 1, ∀j,
therefore a>z 〜N(0,Ik×*). For all i ∈ [n],j ∈ [d] ∣xi,j | ≤ log(nd∕δ) with probability 1 — δ.
Then by matrix concentration inequality ((Vershynin, 2010) Corollary 5.52), we have with Probability
1—δ: (1-e)X W Xn W (1+e)X if n ≥ Ω(d∕e2 log2(nd∕δ)), Thereforeset n = <Θ(d∕e2 log2(1∕δ))
will suffice.	□
Proof of Lemma 4.
Xij	= Ez 〜N (0,ik×k)φ9[z)φS>z)
0	i 6= j
一ɪ E[φ2 (a>z)] ≤ ∏2 i = j
6For simplicity, we analyze as if φ(0) = 0 w.o.l.g. throughout this section, since the bias term is canceled
out in the observation side with φ(A*z) and the learning side with φ(Az).
17
Under review as a conference paper at ICLR 2020
Therefore ∣∣X∣∣2 ≤ ∏2. Together with Lemma 3, ∣∣X - Xnk ≤ E∏ w.p 1 - δ. Recall
Vg(A)- Vgn(A) = 2Ez [diag(φ0(Az))(X - Xn)φ(Az)z>] := 2EzG(z),
where G(z) is defined as diag(φ0(Az))(X -Xn)φ(Az)z>. We have ∣G(z)∣ ≤ ∣A∣∣z∣2∣X -Xn∣.
∣Vg(A) - Vgn(A)∣2 = 2∣Ez[G(z)]∣
≤ 2Ez ∣G(z)∣
≤ 2Ez∣A∣∣z∣2∣X-Xn∣
2
≤ 2∣A∣e - Ez kzk2
π
2
=2∣ A∣ed-
π
For the directional derivative, we make the concentration bound in a similar way. Denote
D(z) = diag(φ0(Az))(Xn - X)φ0(Az) ◦ (Bz)z> + diag(φ00(Az) ◦ (Bz))(Xn - X)φ(Az)z>.
∣D(z)∣ ≤ ∣Xn - X∣2∣B∣∣z∣2(1 + ∣z∣∣A∣).
Therefore kDaVg(A)[B] - DaVgn(A)[B↑^ ≤ O(Ed3/2|4|田||) with probability 1 - δ.	□
B.4 Omitted Proofs on B ounding Mini-Batch Size
Recall
gm,n(A) ≡ 2
1n
-X χix>
n i=1
m
-m Xφ(Azj )φ(Azj )>
2
F
Write Sj (A) ≡ φ(Azj)φ(Azj)>. Then we have
1	1m
gm,n (A)=2 X Xn---Sj (A), Xn -
n j=1
1m
m X Sj (A))
1	1m	1
= 2mm2 WSi(A),Sj (A)i- n ΣhSj- (A), Xni + 2 ∣XnkF
On the other hand, our target function is:
n
11 n
gn(A) ≡2 n X
χiχ> - Ez〜N(0,ik×k) [φ(Az)φ(Az)>]
i=1
2
F
=1 ∣Es [S ]kF -hEs [S ],Xn i + 1 ∣XnkF
Therefore ESgm,n(A)- gn(A) = 2⅛(ES∣S(A)kF - ∣EsS(A)kF).
Claim 6.
2
VESgm,n(A)-Vgn(A) = ^Ez [diag(φ0(Az))S(A)φ(Az)z> - diag(φ0(Az))Es[S(A)]φ(Az)z>].
Proof.
hVEsgm,n -Vgn, ∆A
=Esgm,n(A + ∆A) + gn(A + ∆A) - (Esgm,n(A) + gn(A)) + O(Il∆A∣∣2)
=22m (ES ∣S(A + ∆A)∣F - ES ∣S(A)∣F -∣Es S (A + ∆A)∣F + ∣Es S(A)∣F) + Ο(∣∆A∣2)
=L (EshS(A), S(A + ∆A) - S(A))-(Es[S(A)], ES[S(A + ∆A) - S(A)]) + Ο(∣∆A∣2)
m
=L (hEzhS(A), diag(φ0(Az))∆Azφ(Az)>i- (Es[S(A)],Ezdiag(φ0(Az))∆Azφ(Az)>i)
m
+ O(∣∆A∣2)	(from Eqn. (11) and symmetry of S)
2
=hmmEz [diag(φ0(Az))S(A)φ(Az)z> - diag(φ0(Az))Es[S(A)]φ(Az)z>] , ∆A> + O(Il∆A∣∣2)
18
Under review as a conference paper at ICLR 2020
□
Similarly to the derivation in the previous subsection, we again derive the bias in the directional
derivative:
Claim 7. For arbitrary matrix direction B,
DAVEsgm,n(A)[B] - DAVgn(A)[B]
2
=—Ez diagg(φ00(Az) ◦ (Bz))(S(A) - ESS(A))φ(Az)zτ
m
+ diag(φ0(Az)) ((φ0(Az) ◦ (Bz))φ(Az)τ - Ez[(φ0(Az) ◦ (Bz))φ(Az)τ]) φ(Az)zτ
+ diag(φ0(Az)) (φ(Az)(φ0(Az) ◦ (Bz))τ - Ez[φ(Az)(φ0(Az) ◦ (Bz))τ]) φ(Az)zτ
+ diag(φ0(Az))(S(A) - ESS(A))(φ0(Az) ◦ (Bz))zτ
B.5 Omitted proof of the main theorem
ProofofLemma 7. On one hand, suppose Z is an E-FOSP property of g in (6) along with the matrix
S and vector σ, we have:
hVg(Z),Z - Z*i
= hS,Z - Z*i
(since Z — Z* has 0 diagonal entries)
≤kPτ(S)k2 kPτ◦ (Z - Z*)∣∣F
(T is the tangent cone of PSD matrices at Z)
≤kPτ (S)k2 kZ - Z *kF
=max{a >s aj }kZ- Z*kF
(a j is the basis of the column space of Z )
≤e∣∣Z - Z*k	(12)
(from the definition of -FOSP)
On the other hand, from the definition of g, we have:
hZ - Z *, Vg(Z )i
=X(Zij- Zij)gij (Zij)
ij
=X(Zij-Ziij)2Xσk2Pk(Zij)Xσk2kZikj-1
ij	k odd	k odd
≥kZ-Zik2Fσ14	(13)
Here polynomial Pk(Zij) ≡ (Zikj - (Ziij)k)/(Z - Zi) is always positive for Z 6= Zi and k to be odd.
Therefore by comparing (12) and (13) we have EkZ - Zi kF ≥ kZ - Zi k2F σ14 , i.e. kZ - Zi kF ≤
O(e).	□
Proof of Theorem 3. From Theorem 31 from Ge et al. (2015), we know for small enough learning
rate η, and arbitrary small E, there exists large enough T, such that Algorithm 1 generates an output
A(T) that is sufficiently close to the second order stationary point for f. Or formally we have,
Tr((A(T))τXiA(T)) =yi,
k(VAf (A(T)) - Pi=ι %XiA(T))：,jk2 ≤ Emin |出,：|幅	∀j ∈ [k]
Tr(BτDAVALf(A(T),λ)[B]) ≥ -EkBk22,	∀B, s.t. Tr(BτXiA) =0
Lf (A, λ) = f (A) - pd=ι λi(Tr(AτXiA) - yi). Let {ai = A(T)ri}k to form the basis of the
column vector space of A(T). Then the second line is a sufficient condition for the following:
Ila>(Va∕(A(T)) - Pi=ι %XiA(TDrj∣∣2 ≤ e, ∀j ∈ [k].
19
Under review as a conference paper at ICLR 2020
Now with the concentration bound from Lemma 5, suppose our batch size m ≥ O(d5/), we have
kVAgn(A(T)) - VAf(A(T))k2 ≤ 3 and kDA^Agn(Λ(T))[B] - DA%Af (AT))[B]k2 ≤ EkBk2
for arbitrary B . Therefore again we get:
Tr((Λ(T))>XiΛ(T)) = yi
ka>(VAgn(Λ(T)) - Pi=I λiXiΛ(T))rjk2 ≤ 2e,	∀j ∈ [k]
Tr(B>DAVALgm(Λ(T),λ)[B]) ≥ -23kBk22,	∀B, s.t. Tr(B>XiΛ) =0
Next we turn to the concentration bound from Lemma 4. Suppose we have when the sample size
n ≥ OIddla log2(1∕δ)), ∣∣DaVAg(A)[B] - DAVAgn(Λ)[B]k2 ≤ O(d∣B∣∣2), and ∣∣Vg(Λ)-
Vgn(Λ)k2 ≤ O(3) with probability 1 - δ. Therefore similarly we get Λ(T) is an O(3)-SOSP for
g(Λ) = 2 IIPi=Oσ2 ((Λ*(Λ*)>)-i - (ΛΛ>)-i) IlF.
Now with Lemma 6 that connects the approximate stationary points, we have Z := Λ(T) (Λ(T))> is
also an "OSP of g(Z) = 2 ∣∣P∞=o σ2 ((Z*产-Z◦i) ∣∣F.
Finally with Lemma 7, WegetkZ - Z*∣∣f ≤ O(e).
□
20