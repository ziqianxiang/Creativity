Under review as a conference paper at ICLR 2020
A Stochastic Trust Region Method for Non-
convex Minimization
Anonymous authors
Paper under double-blind review
Ab stract
We target the problem of finding a local minimum in non-convex finite-sum min-
imization. Towards this goal, we first prove that the trust region method with
inexact gradient and Hessian estimation can achieve a convergence rate of order
O(1/k2/3) as long as those differential estimations are sufficiently accurate. Com-
bining such result with a novel Hessian estimator, We propose a SamPle-efficient
stochastic trust region (STR) algorithm which finds an (e, √e)-approximate local
minimum within O(√n∕e1∙5) stochastic Hessian oracle queries. This improves the
state-of-the-art result by a factor of O(n1/6). Finally, we also develop Hessian-free
STR algorithms which achieve the lowest runtime complexity. Experiments verify
theoretical conclusions and the efficiency of the proposed algorithms.
1	Introduction
We consider the following finite-sum non-convex minimization problem
1n
mind F(X) = n Xi=I fi(X)，
(1)
where each (non-convex) component function fi : Rd → R is assumed to have L1-Lipschitz
continuous gradient and L2-Lipschitz continuous Hessian. Since first-order stationary points could
be saddle points with inferior generalization performance (Dauphin et al., 2014), in this work we are
particularly interested in computing (e, √e)-approximate second-order stationary points, e-SOSP:
kVF(Xe)k ≤ e and V2F(xe) < -√L^I.
(2)
To find a local minimum of problem (1), the cubic regularization approach (Nesterov & Polyak,
2006) and the trust region algorithm (Conn et al., 2000; Curtis et al., 2017) are two classical methods.
Specifically, cubic regularization forms a cubic surrogate function for the objective F(X) by adding a
third-order regularization term to the second-order Taylor expansion, and minimizes it iteratively.
Such a method is proved to achieve an O(1/k2/3) global convergence rate and thus needs O(n/e1.5)
stochastic first- and second-order oracle queries, namely the evaluation number of stochastic gradient
and Hessian, to achieve a point that satisfies (2). On the other hand, trust region algorithms estimate
the objective with its second-order Taylor expansion but minimize it only within a local region.
Recently, Curtis et al. (2017) proposes a trust region variant to achieve the same convergence rate as
the cubic regularization approach. But both methods require computing full gradients and Hessians
of F(X) and thus suffer from high computational cost in large-scale problems.
To avoid costly exact differential evaluations, many works explore the finite-sum structure of prob-
lem (1) and develop stochastic cubic regularization approaches. Both Kohler & Lucchi (2017b)
and Xu et al. (2017) propose to directly subsample the gradient and Hessian in the cubic surrogate
function, and achieve O(1∕e3∙5) and O(1∕e2∙5) stochastic first- and second-order oracle complexities
respectively. By plugging a stochastic variance reduced estimator (Johnson & Zhang, 2013) and the
Hessian tracking technique (Gower et al., 2018) into the gradient and Hessian estimation, the approach
in (Zhou et al., 2018a) improves both the stochastic first- and second-order oracle complexities to
O)(n0.8∕e1.5). Recently, Zhang et al. (2018) and Zhou et al. (2018b) develop more efficient stochastic
cubic regularization variants, which further reduce the stochastic second-order oracle complexity to
O(n2∕3∕e1∙5) at the cost of increasing the stochastic first-order oracle complexity to O(n2/3/e2.5).
1
Under review as a conference paper at ICLR 2020
Table 1: Stochastic first- and second-order oracle complexities, SFO and SSO for short respectively,
of the proposed STR approaches and other state-of-the-art methods. When SSO is prioritized, our
STR1 has strictly better complexity than both SCR and Lite-SVRC. When SFO and SSO are treated
equally, STR2 improves the existing result in SVRC.
Algorithm	SFO	SSO
TR (Conn et al., 200(; CUrtiS et al., 2017)	Ow	O^
CR (Nesterov & Polyak, 2006)	o(黄)	0(舞)
SCR (Kohler & Lucchi, 2017a)	O(冬)	0(亳)
SVRC (Zhou et al., 2018c)	O(展)	0( nx)
Lite-SVRC (Zhouetal, 2018b)		O(嘿)	0(展)
STRi (this paper)	O(min{券,誓n})	0(min{⅛∙,禹})
STR2 (this paper)	0（联）	0（展）
Contributions: In this paper we propose and exploit a formulation in which we make explicit control
of the step size in the trust region method. This idea is leveraged to develop two efficient stochastic
trust region (STR) approaches. We tailor our methods to achieve state-of-the-art oracle complexities
under the following two measurements: (i) the stochastic second-order oracle complexity is prioritized;
(ii) the stochastic first- and second-order oracle complexities are treated equally. Specifically, in
Setting (i), our method STR1 employs a newly proposed estimator to approximate the Hessian
and adopts the estimator in (Fang et al., 2018) for gradient approximation. Our novel Hessian
estimator maintains an accurate second-order differential approximation with lower amortized oracle
complexity. In this way, STRi achieves O(min{1∕e2, √n∕e1∙5}) stochastic second-order oracle
complexity. This is lower than existing results for solving problem (1). In Setting (ii), our method
STR2 substitutes the gradient estimator in STR1 with one that integrates stochastic gradient and
Hessian together to maintain an accurate gradient approximation. As a result, STR2 achieves
convergence in O(n3/4/e1.5) overall stochastic first- and second-order oracle queries. Finally, based
on STR, we further develop Hessian-free STR algorithms, namely STRfree and STRfree+, which
outperform existing Hessian-free algorithms theoretically.
1.1	Related Work
Computing local minimum to a non-convex optimization problem is gaining considerable amount of
attentions in recent years. Both cubic regularization (CR) approaches (Nesterov & Polyak, 2006)
and trust region (TR) algorithms (Conn et al., 2000; Curtis et al., 2017) can escape saddle points
and find a local minimum by iterating the variable along the direction related to the eigenvector of
the Hessian with the most negative eigenvalue. As the CR heavily depends on the regularization
parameter for the cubic term, Cartis et al. (2011) propose an adaptive cubic regularization (ARC)
approach to boost the efficiency by adaptively tuning the regularization parameter according to the
current objective decrease. Noting the high cost of full gradient and Hessian computation in ARC,
sub-sampled cubic regularization (SCR) (Kohler & Lucchi, 2017a) is developed for sampling partial
data points to estimate the full gradient and Hessian. Recently, by exploring the finite-sum structure
of the target problem, many works incorporate variance reduction technique (Johnson & Zhang, 2013)
into CR and propose stochastic variance-reduced methods. For example, Zhou et al. (2018c) propose
stochastic variance-reduced cubic (SVRC) in which they integrate the stochastic variance-reduced
gradient estimator (Johnson & Zhang, 2013) and the Hessian tracking technique (Gower et al., 2018)
with CR. Such a method is proved to be at least O(n1/5) faster than CR and TR. Then Zhou et al.
(2018b) use adaptive gradient batch size and constant Hessian batch size, and develop Lite-SVRC to
further reduce the stochastic second-order oracle O(n4/5/e1.5) of SVRC to O(n2/3/e1.5) at the cost
of higher gradient computation cost. Similarly, except turning the gradient batch size, Zhang et al.
(2018) further adaptively sample a certain number of data points to estimate the Hessian and prove
the proposed method to have the same stochastic second-order oracle complexity as Lite-SVRC.
2	Preliminary
Notation. We use kvk to denote the Euclidean norm of vector v and use kAk to denote the spectral
norm of matrix A. Let S be the set of component indices. We define the minibatch average of
component functions by f (x; S) d=f 吉 Pii∈s fi(x). Then we specify the assumptions that are
necessary to the analysis of our methods.
2
Under review as a conference paper at ICLR 2020
MetaAlgorithm 1 Inexact Trust Region Method
Input: initial point x0 , step size r, number of iterations K, construction of differential estimators gk
and Hk
1:	for k = 0 to K - 1 do
2:	Compute hk and λk by solving (8);
3:	xk+1 := Xk + hk;
4:	if λk ≤ 3,〃-2 then
5:	Output X = Xk+1;
6:	end if
7:	end for
Assumption 2.1. F is bounded from below and its global optimal is achieved at x*. We denote
∆ = F(x0) - F(x*).
Assumption 2.2. Each fi : Rd → R has L1 -Lipschitz continuous gradient: for any x, y ∈ Rd
l∣Vfi(x) - Vfi(y)k ≤ Likx - y∣∣.	(3)
Assumption 2.3. Each fi : Rd → R has L2-Lipschitz continuous Hessian: for any x, y ∈ Rd
kV2fi(x) - V2fi(y)k ≤ L2kx - yk.	(4)
2.1 Trust Region Method
Here we briefly introduce the trust region method (Conn et al., 2000). In each step, it first solves the
Quadratic Constraint Quadratic Program (QCQP) defined as
hk := argmin hVF(xk), h〉+ 1 (V2F(xk)h, h〉，	(5)
h∈Rd,khk≤r	2
where r is the trust-region radius. Then it updates the new variable as
xk+1 :=xk +hk.	(6)
Since V2F(xk) is indefinite, the trust-region subproblem (5) is non-convex. But its global optimizer
can be characterized by the following lemma (Corollary 7.2.2 in (Conn et al., 2000)).
Lemma 2.1. Any global minimizer of problem (5) satisfies the equation
(V2F(xk) + λl) hk = -VF(xk),	(7)
where the dual variable λ ≥ 0 should satisfy V2F(xk) + λI < 0 and λ(khkk - r) = 0.
In particular, the standard QCQP solver returns both the minimizer hk as well as the corresponding
dual variable λ of subproblem (5). In the following section, we first prove that the deterministic
trust-region update (5) and (6) converges at the rate of O(1/k2/3), much sharper than existing
provable convergence rate O(1/√k) (Conn et al., 2000), and then develop a more efficient stochastic
trust-region approach.
3	Methodology
Here we first introduce a general inexact trust region method which is summarized in MetaAlgorithm 1.
It accepts inexact gradient estimation gk and Hessian estimation Hk as input to the QCQP subproblem
hk := argmin (gk, h)+ ∙^(Hkh, hi.	(8)
h∈Rd,khk≤r	2
Similar to (5), Lemma 2.1 characterizes the global optimum to problem (8) which can be efficiently
solved by Lanczos method (Gould et al., 1999). Assume the dual variable of the minimizer hk is λk.
We prove that such inexact trust region method achieves the optimal O(1/k2/3 ) convergence rate
when the estimation gk and Hk at each iteration are sufficiently close to their full (exact) counterparts
VF(xk) and V2F(xk) respectively:
∣gk - VF(xk)k ≤ f, ∣Hk - V2F(xk)k ≤ ".	(9)
63
3
Under review as a conference paper at ICLR 2020
Algorithm 2 STR1
Input: initial point x0 , step size r, number of iterations K
1:	for k = 1 to K do
2:	Construct gradient estimator gk by Estimator 4;
3:	Construct Hessian estimator Hk by Estimator 3;
4:	Compute hk and λk by solving (8);
5:	xk+1 := Xk + hk;
6:	if λk ≤ 3,〃-2 then
7:	Output X = Xk+1;
8:	end if
9:	end for
Such result allows us to derive stochastic trust-region variants with novel differential estimators that
are tailored to ensure the optimal convergence rate. We state our formal results in Theorem 3.1,
whose proof is deferred to Appendix B.1 due to the space limit.
Theorem 3.1 (Main Result). Consider problem (1) under Assumption 2.1-2.3. If the differential
estimators gk and Hk satisfy Eqn. (9) for all k, MetaAlgorithm 1 finds an O()-SOSP in less than
K = O(√L2∆∕e1∙5) iterations by setting the trust-region radius as r =，6/L2.
Remark 3.1. We emphasize that MetaAlgorithm 1 degenerates to the exact trust region method by
taking gk = VF(Xk) and Hk = V2 * 4F(Xk). Such result is of its own interest because this is the
first proof to show that the vanilla trust region method has the optimal O(1/k2/3) convergence rate.
Similar rate is achieved by Curtis et al. (2017) but with a much more complicated trust region variant.
Theorem 3.1 shows the explicit step size control of the trust region method: Since the dual variable
satisfies λk > 3e0-5∕√L2 > 0 for all but the last iteration, we always find the solution to the trust-
region subproblem (8) in the boundary, i.e. khk k = r, according to the complementary condition
(15) in Appendix B.1. Such exact step size control property is missing in the cubic-regularization
method where the step size is implicitly decided by the cubic regularization parameter.
More importantly, we emphasize that such explicit step size control is crucial to the sample efficiency
of our variance reduced differential estimators. The essence of variance reduction is to exploit the
correlations between the differentials in consecutive iterations. Intuitively, when two neighboring
iterates are close, so are their differentials due to the Lipschitz continuity, and hence a smaller number
of samples suffice to maintain the accuracy of the estimators. On the other hand, smaller step size
reduces the per-iteration objective decrease which harms the convergence rate of the algorithm (see
proof of Theorem 3.1). Therefore, the explicit step size control in trust region method allows us to
well trade-off the per-iteration sample complexity and convergence rate, from which we can derive
stochastic trust region approaches with the state-of-the-art sample efficiency. In contrast, existing trust
region methods change the step size at every iteration according to progress made, which requires
loss evaluations that can be as expensive as gradient computations (e.g. the non-convex linear model
in Section 7) and is thus prohibitive for large-scale problems.
4 Stochastic Trust Region Method: Type I
Having the inexact trust region method as prototype, we now present our first sample-efficient
stochastic trust region method, namely STR1, in Algorithm 2 which emphasizes cheaper stochastic
second-order oracle complexity. As Theorem 3.1 already guarantees the optimal convergence
rate of MetaAlgorithm 1 when the gradient estimator gk and the Hessian estimator Hk meet the
requirement (9), here we focus on constructing such novel differential estimators. Specifically, we
first present our Hessian estimator in Estimator 3 and our first gradient estimator in Estimator 4, both
of which exploit the trust region radius r = /e/L? to reduce their variances.
4.1 Hessian Estimator
Our epoch-wise Hessian estimator Hk is given in Estimator 3, where p2 controls the epoch length
and s2 (and optionally s02) controls the minibatch size. At the beginning of each epoch, Estimator 3
has two options, designed for different target accuracy: Option I is preferable for the high accuracy
4
Under review as a conference paper at ICLR 2020
Estimator 3 Hessian Estimator
Input: Epoch length p2, sample size s2, s02 (optional)
1:	if mod(k, p2)= 0 then
2:	Option I:	high accuracy case (small )
3:	Hk := V2F(Xk);
4:	Option II:	low accuracy case (moderate )
5:	Draw s02 samples indexed by H0 and let Hk := V2f(Xk; H0);
6:	else
7:	Draw s2 samples indexed by H and let Hk := V2f(Xk; H) - V2f(Xk-1 ; H) + Hk-1;
8:	end if
Estimator 4 Gradient Estimator: Case (1)
1:	if mod(k,pι)=0 then
2:	gk := VF(Xk);
3:	else
4:	Draw s1 samples indexed by G and gk = Vf(Xk; G) - Vf (Xk-1; G) + gk-1;
5:	end if
case ( < O(1/n)) where we compute the full Hessian to avoid approximation error, and Option
II is designed for the moderate accuracy case ( > O(1/n)) where we only need an approximate
Hessian estimator. Then, p2 iterations follow with Hk defined in a recurrent manner. These recurrent
estimators exist for the first-order case (Nguyen et al., 2017; Fang et al., 2018), but their bound only
holds under the vector `2 norm. Here we generalize them into Hessian estimation with matrix spectral
norm bound.
The following lemma analyzes the amortized stochastic second-order oracle (Hessian) complexity
for Algorithm 3 to meet the requirement in Theorem 3.1. As we need an error bound under the
spectral norm, we will appeal to the matrix Azuma’s inequality (Tropp, 2012). The proof is deferred
to Appendix B.2.
Lemma 4.1. Assume Algorithm 2 takes the trust region radius r = yJe∕L as in Theorem 3.1.
For any k ≥ 0, Estimator 3 produces estimator Hk for the second order differential V2F(Xk)
such that ∣∣Hk - V2F(xk )k ≤ √eL2∕3 With probability at least 1 一 δ∕Ko if we set (1) p2 = √n
and S2 = 32√n lοg(dKο∕δ) in option I, or (2) p2 = Lι∕(2√eL2), s2 = 16L2/(eL2)lοg(dKο∕δ),
and S2 = 32Lι∕(√eL2) lοg(dKο∕δ) in option IL Here Ko is a constant to be determined later.
Consequently the amortized per-iteration stochastic second-order oracle complexity to construct Hk
is no more than 2s2 = min{64√nlog dδ0, √L1 log d^δ0}.
4.2 Gradient Estimator: Case (1)
When the stochastic second-order oracle complexity is prioritized, we directly employ the SPIDER
gradient estimator to construct gk (Fang et al., 2018). Similar to the construction for Hk, the estimator
gk is also constructed in an epoch-wise manner as presented in Estimator 4, where p1 controls the
epoch length and s1 controls the minibatch size.
We now analyze the stochastic first-order oracle complexity to meet the requirement in Theorem 3.1.
Lemma 4.2. Assume Algorithm 2 takes the trust region radius r = V^TL2. Estimator 4 pro-
duces estimator gk of the first order differential VF(Xk) such that ∣gk - VF(Xk)∣ ≤ ∕6 with
probability at least 1 - δ∕K0 for any k ≥
0, ifwe set p1 = max{1, n
Lz/(CL log Kδ0)} and
Si = min{n, ,cnL2 log(K0∕δ)∕(EL2)}, where the constant C = 1152 and Ko is a constant to be
determined later. Consequently, the amortized per-iteration stochastic first-order oracle complexity
to COnStruCt gk is min{n, ,4cnL2 log (K0∕δ)∕,L2)}.
The proof of Lemma 4.2 is similar to the one of Lemma 4.1 and is deferred to Appendix B.3. These
two lemmas only guarantee that the differential estimators satisfy the requirement (9) in a single
iteration and can be extended to hold for all k by using the union bound with Ko = 2K, where K
denotes the number of iterations. Combining such lifted result with Theorem 3.1, we can establish
the computational complexity bound as follows.
5
Under review as a conference paper at ICLR 2020
Algorithm 5 STR2
Input: initial point x0 , step size r, number of iterations K
1:	for k = 1 to K do
2:	Construct gradient estimator gk by Estimator 6;
3:	Construct Hessian estimator Hk by Estimator 3;
4:	Compute hk and λk by solving (8);
5:	xk+1 := Xk + hk;
6:	if λk ≤ 3,〃-2 then
7:	Output X = Xk+1;
8:	end if
9:	end for
Estimator 6 Gradient Estimator: Case (2)
1:	if mod(k,pι)=0 then
2:	Let X := xk, gk := VF(X)
3:	else
4:	Draw s1 samples indexed by G ;
5:	gk = Vf(Xk G) - Vf(Xk-1; G) +	gk-1	+ [V* 2F(X)	- V2f (X; G)](xk	- xk-1);
6:	end if
Corollary 4.1 (Computational Complexity of STR1 ). Assume Algorithm 2 will use Estimator 4 to
construct the first-order differential estimator gk and use Estimator 3 to construct the second-order
differential estimator Hk. Tofind a 12e-SOSP with probability at least 1 一 δ ,the overall stochastic
first-order oracle complexity is O(min{ In^L5δ , VnLI log( L2⅜)}) and the overall stochastic second-
order oracle complexity is O(min{ √l5δ , L⅛δ} log( dɪ^ʌ)).
From Corollary 4.1 We see that O(min{√n∕e1∙5,1/e2}) stochastic second-order oracle queries are
sufficient for STR1 to find an e-SOSP which is significantly better than both the subsampled cubic
regularization method O(1∕e2∙5) (Kohler & Lucchi, 2017a) and the variance reduction based ones
O(n2∕3∕e1∙5) (Zhou et al., 2018b; Zhang et al., 2018). Recently, Zhou & Gu (2019) developed a
stochastic recursive variance-reduced cubic regularization (SRVRC) method which finds an (e, √e)-
approximate local minimum with O(n∕e1∙5,1∕e3) SFO and O(√n∕e1∙5,1∕e2) SSO. But the result
of SRVRC needs to assume stochastic gradient to be bounded, i.e., kVfi(X) - VF (X)k ≤ σ. With
this extra assumption, STR1 enjoys O(n∕e1∙5, n∕e2,1∕e3) SFO and O(√n∕e1∙5,1∕e2) SSO. Thus,
if 1∕e ≤ n ≤ 1∕e2, STR1 outperforms SRVRC; otherwise they have the same complexities.
5 Stochastic Trust Region Method: Type II
In the above section, we focus on the setting where the stochastic second-order oracle complexity is
prioritized over the stochastic first-order oracle complexity. In this setting, STR1 achieves the state-
of-the-art efficiency. In this section, we consider a different complexity measure where first-order
and second-order oracle complexities are treated equally and our goal is to minimize the maximum
of them. We note that, currently the best result is O(n4∕5∕e1∙5) of the SVRC method (Zhou et al.,
2018c).
Since the Hessian estimator Hk of STR 1 already delivers the superior O(√n∕e1∙5) stochastic Hessian
complexity, in STR2 (see Algorithm 5), we retain Estimator 3 for second-order differential estimation
and use Estimator 6 to further reduce the stochastic gradient complexity.
5.1 Gradient Estimator: Case (2)
When stochastic gradient and Hessian complexities are equally important, we use Hessian to improve
the gradient estimation. Denote X(a) = aXt + (1 — a)X. From Assumption 2.3, we have
kVfi(Xt)-Vfi(X)-V2fi(X)(Xt- X)k = k
Z 1[V2fi (X(a))
0
-V2fi(X)](Xt - X)dak≤ LkXt- Xk2.
6
Under review as a conference paper at ICLR 2020
Such property can be used to improve Lemma 4.2 of Estimator 4. Specifically, define the correction
Ck = W2F(X) - v2f (x; G)](xk - xk-1),
where X is some reference point updated in an epoch-wise manner. Estimator 6 adds ck to the
estimator in Estimator 4. Note that in Estimator 6, the first- and second-order oracle complexities are
the same. We now analyze the first-order (and second-order) oracle complexity to meet requirement
(9).
Lemma 5.1. Assume Algorithm 5 takes the trust region radius r = yJe∕L as in Theorem 3.1. For
any k ≥ 0, Estimator 6 produces estimator gk for the first order differential vF(Xk) such that kgk -
VF(Xk )k ≤ e/6 with probability at least 1 一 δ∕K, ifwe set pi = n0.25 and si = n0∙75c log(K0∕δ),
where c = 1152 and K0 is a constant to be determined. Consequently, the amortized per-iteration
StochaSticfirSt-OrderoracIe complexity to ConStruct gk is 2si = 2n0.75clog(K0∕δ).
The proof of Lemma 5.1 is similar to the one of Lemma 4.1 and is deferred to Appendix B.4.
Similar to the previous section, Lemma 5.1 only guarantees that the gradient estimator satisfies the
requirement (9) in a single iteration. Such result can be extended to hold for all k by using the union
bound with K0 = 2K, which together with Theorem 3.1 gives the following corollary.
Corollary 5.1 (Computational Complexity of STR2). Algorithm 5 finds a 12e-SOSP with probability
at least 1 一 δ, within O(n-^L2δ log(噂衿))overall stochastic first-order oracle queries and
O( no7：Y5”4 log( IiWL吟))overall stochastic Second-Order oracle queries.
Corollary 5.1 shows that to find an e-SOSP, both SFO and SSO of STR2 are O(n3/4/e1.5) which
surpasses the best existing one O(n4/5/e1.5) in (Zhou et al., 2018c).
6 Practical Stochastic Trust Region Variants
6.1	Handling Inexact QCQP Solutions
One drawback of MetaAlgorithm 1 is that it requires the exact solution to the QCQP subproblem (8)
and uses the dual variable as stopping criterion. We address this problem by developing a practical
variant, MetaAlgorithm 7, which admits inexact QCQP solutions without access to the dual variable.
This algorithm repeatedly invokes a procedure called InexactTRWEAK, which, as we shall see,
outputs an O(e)-SOSP with a constant probability of 2/3 in O(1/ei.5) iterations. By repeatedly
invoking INEXACTTRWEAK for Θ(log(1∕δ)) times, MetaAlgorithm 7 boosts the probability to (1 一 δ)
for any desired δ. This repeating technique has been studied by, e.g., (Allen-Zhu & Li, 2018; Allen-
Zhu, 2018b). To test whether the t-th run outputs an O(e)-SOSP, we need to compute kvF(Xt)k
and the smallest eigenvalue of v2F(Xt). The latter one can be approximated by solving the QCQP
vt := argmin ψt(v) = hHtv,vi,	(10)
kvk≤i
where Ht is the full Hessian v2F(Xt) or its estimation. One can show that MetaAlgorithm 7 finds an
O(e)-SOSP w.p. at least (1 一 δ) in O(1∕e1∙5) iterations. We defer the detailed analysis to Appendix C.
6.2	Hessian-Free Implementation
Based on MetaAlgorithm 7, we propose a Hessian-free method named STRfree, which is summarized
in Algorithm 8. STRfree leverages the full/stochastic Hessian and Estimator 4 to construct Hk and gk,
respectively. Besides, it uses Lanczos method (Gould et al., 1999; Carmon & Duchi, 2018) as the
QCQP solver, which can be implemented in a Hessian-free manner (i.e., using only Hessian-vector
products without explicit Hessian matrix evaluations). Thus, Hk is only accessed through Hessian-
vector products and is never explicitly constructed. Since Hessian-vector products can be computed
in linear time (in terms of the dimension d) for many machine learning problems (Allen-Zhu, 2018b;
Agarwal et al., 2017), Hessian-free methods are usually more practical than Hessian based ones
for high dimensional problems. The following theorem, whose proof can be found in Appendix D,
establishes the runtime complexity (i.e., the total complexity of stochastic gradient and Hessian-vector
product evaluations (Zhou & Gu, 2019)) of STRfree.
7
Under review as a conference paper at ICLR 2020
MetaAlgorithm 7 Inexact Trust Region Method II
Input: initial point x0, step size r, number of inner iterations K, constants δ, ζ ∈ (0, 1), c1, c2,
number of outer iterations T = Θ(log(1∕δ)), sample size S (optional)
1:	for t = 1 to T do
2:	Xt — INEXACTTRWEAK(x0, r, K, Z)；
3:	Option I:	high accuracy case (small )
4:	Ht := V2F(xt);
5:	Option II:	low accuracy case (moderate )
6:	Draw s samples indexed by H and let Ht := V2f(Xt; H);
7:	Compute Vt by solving (10) UP to accuracy √eL2 with probability 1 一 δ∕4;
8:	if kVF(xt)k ≤ cιe and ψt(Vt) ≥ -√c2eL2 then
9:	return X := Xt;
10:	end if
11:	end for
12:	procedure INEXACTTRWEAK (X0, r, K, ζ)
13:	for k = 0 to K 一 1 do
14:	Compute gk and Hk such that (9) holds with probability 1 一 余;
15:	Compute hk by solving (8) up to accuracy e1.5∕√L2 with probability 1 一 亲;
16:	xk+1 := xk + hk;
17:	end for
18:	Randomly select k from {0,..., K - 1};
19:	return Xk+1;
20:	end procedure
Algorithm 8 STRfree
1:	In the same setting as MetaAlgorithm 7,
2:	construct gradient estimator gk by Estimator 4;
3:	construct Hessian estimator Hk by
4:	Option I: Hk := V2F(Xk);
5:	Option II: Draw s samples indexed by H and let Hk := V2f(Xk; H);
6:	use Lanczos method to solve QCQP subproblems.
Theorem 6.1. Consider Algorithm 8 for solving problem (1). Let Z = 1/3, r = ∖e/L2, K =
4√L2∆∕e1-5, T = 3log(2∕δ), ci = 600, c2 = 500, and S = 3L1 log(4d∕δ). The hyper-parameters
in Estimator 4 are set to the same values as those in Lemma 4.2. The number of iterations of Lanczos
method is set to O(1∕(L2e)0∙25). Tofind an O(e)-SOSP w.p. at least 1 — δ, the runtime complexity is
O(d min{n∕e1∙75,1∕e2^75 + √n∕e2}log(1∕δ)).
To solve the QCQP more efficiently, here we develop a faster solver which is based on the AppxPCA
method (Allen-Zhu & Li, 2016) and KatyushaXW (Allen-Zhu, 2018a). See details in Appendix E. By
replacing Lanczos method with this solver in STRfree, we further improve the runtime complexity
to O(dmin{n∕e1^5 + n0-75∕e1-75,1∕e2^5 + √n∕e2}). We call this new algorithm STRfree+ whose
details can be found in Appendix E. Table 2 shows that for the runtime complexities, both STRfree
and STRfree+ outperform existing methods. See more comparison and discussion in Appendix E.4.
7 Experiments
Here we compare the proposed STR with several state-of-the-art (stochastic) cubic regularized algo-
rithms and trust region approaches, including trust region (TR) algorithm (Conn et al., 2000), adaptive
cubic regularization (ARC) (Cartis et al., 2011), sub-sampled cubic regularization (SCR) (Kohler &
Lucchi, 2017a), stochastic variance-reduced cubic (SVRC) (Zhou et al., 2018c), Lite-SVRC (Zhou
et al., 2018b), and SRVRC (Zhou & Gu, 2019). For STR, we estimate the gradient as the way in case
(1). This is because such a method enjoys lower Hessian computational complexity over the way in
case (2) and for most problems, computing their Hessian matrices is much more time-consuming
than computing their gradients. For the subproblems in these compared methods, we use Lanczos
8
Under review as a conference paper at ICLR 2020
Table 2: Runtime complexities of STRfree, STRfree+, and other state-of-the-art methods.
Algorithm	Runtime
Hessian-free Cubic (Carmon & Duchi, 2016) Fast-Cubic (Agarwal et al., 2017) Stochastic Cubic (Tripuraneni et al., 2018) SRVRCfree (Zhou & Gu, 2019)	O( dn) 石/ dn I dn0.75 ʌ O( ^T_5^ + e1.75 ) O(袅)* O(min｛鲁,鲁｝) *
STRfree (this paper) STRfree+ (this paper)	0.5 O(min｛畀,-2d75 + '｝) O(min｛第 + di售,袅 + 衅｝)
* These entries rely on an additional assumption: kVfi(x) - VF(x)k ≤ σ a.s.	
-2 -2
0	5	10	15	20
# Epoch
20
15
# Epoch
-+-TR
-▼-ARC
-*-SCR
Y SVRC
+ Lite-SVRC
"∙' SRVRC
♦ STR
(a) a9a
(b) ijcnn
-4
-8∙
- -10
星-12
O -14
+ TR
-▼-ARC
-A-SCR
Y SVRC
+ Lite-SVRC
SRVRC
+ STR
0	0.5	1	1.5	2
Time (second)
# Epoch
+ TR
-▼-ARC
-*-SCR
Y SVRC
f Lite-SVRC
-∙ SRVRC
-♦-STR
0	5	10	15	20
0	2	4	6	8	10
# Epoch
-+-TR
-▼-ARC
-A-SCR
Y-SVRC
-► Lite-SVRC
-∙ SRVRC
-♦-STR
(c) codrna
(d) phishing
Figure 1:	Comparison on the logistic regression with non-convex regularizer.
method (Gould et al., 1999; Kohler & Lucchi, 2017a) to solve the subproblem approximately in a
Hessian-related Krylov subspace. We run simulations on seven datasets from LibSVM (a9a, ijcnn,
codrna, phishing, w8a, epsilon and mnist). We run our algorithm for 40 epochs and use the output
as the optimal value f * for sub-optimality estimation. Note the output has very small gradient already
verified by Figure 2 and 4 in appendix. For all the considered algorithms, we set their initializations
as zeros and tune their hyper-parameters optimally. For more experimental settings, e.g. details of
testing datasets and algorithm parameter settings, please refer to Appendix F.
Two evaluation non-convex problems. Following (Kohler & Lucchi, 2017a; Zhou et al., 2018c),
we evaluate all considered algorithms on two learning tasks: the logistic regression with non-
convex regularizer and the nonlinear least square. Given n data points (xi , yi) where xi ∈ Rd
is the sample vector and yi ∈ ｛-1, 1｝ is the label, logistic regression with non-convex reg-
ularizer aims at distinguishing these two kinds of samples by solving the following problem
minw n Pn=Ilog(1 + exp(-yiwτXi)) + λR(w; α), where the non-convex regularize] R(w; α) is
defined as R(w; α) = Pid=1 αw2∕(1 + αw2). The nonlinear least square problem fits the nonlinear
data by minimizing minw 2n P2ι [yi - φ(wτ Xi)]2 + λR(w, α). For these two kinds of problems,
we set the parameters λ = 10-3 and α = 10 for all testing datasets.
Comparison of Hessian based algorithms. Figure 1 summarizes testing results on the non-convex
logistic regression problem. For each dataset, we report the function value gap v.s. the overall
algorithm running time which can reflect the overall computational complexity of an algorithm, and
also show the function value gap v.s. Hessian sample complexity which reveals the complexity of
Hessian computation. From Figure 1, one can observe that our proposed STR algorithm runs faster
than the compared algorithms in terms of the algorithm running time, showing the overall superiority
of STR. Furthermore, STR also reveals much sharper convergence curves in terms of the Hessian
9
Under review as a conference paper at ICLR 2020
0	0.5	1	1.5	2
Time (second)
+ TR
-▼-ARC
-A-SCR
Y SVRC
f Lite-SVRC
-∙ SRVRC
-♦-STR
)*f -f(gol ecnatsiD evitcejbO
2	4	6	8	10
Time (second)
(a) a9a
0	5	10	15	20
# Epoch
+ TR
-▼-ARC
-A-SCR
Y-SVRC
f-Lite-SVRC
-∙-SRVRC
-♦-STR
)*f -f(gol ecnatsiD evitcejbO
+ TR
-▼-ARC
— SCR
Y-SVRC
+ Lite-SVRC
-∙-SRVRC
→-STR
(b) w8a
0	2	4	6	8	10
# Epoch
2	4
# Epoch
(c) codrna
Figure 2:	Comparison on the nonlinear least square problem.
sample complexity which is consistent with our theory. This is because to achieve an -accuracy local
minimum, the Hessian sample complexity of the proposed STR is O(n0.5/e1.5) and is superior over
the complexity of the compared methods (see the comparison in Sec. 4.2). Indeed, this also explains
why our algorithm is also faster in terms of algorithm running time, since for most optimization
problems, Hessian matrix is much more computationally expensive than the gradient and thus more
efficient Hessian sample complexity means faster overall convergence speed. Note, as all compared
methods need to compute the Hessian and gradient, their memory complexity are all O(d2 + d).
Figure 2 displays results of the compared algorithms on the nonlinear least square problem. STR
shows very similar behaviors as those in Figure 1. Specifically, STR achieves fastest convergence rate
in terms of both algorithm running time and Hessian sample complexity. On the codrna dataset we
further plot the gradient norm versus running time and Hessian sample complexity. One can obverse
that the gradient in STR vanishes significantly faster than other algorithms which means that STR can
find the stationary point with high efficiency. See Figure 4 in Appendix F.2 for more experimental
results on gradient norm comparison. All these results confirm the superiority of the proposed STR.
Comparison of Hessian-free algorithms.
Here we compare our proposed Hessian-free
STR, namely STRfree, with other state-of-
the-art Hessian-free algorithms on the two
high-dimensional datasets, including epsilon
and mnist (see details in Appendix F). Here
we do not compare STRfree+, as it is based
on AppxPCA method (Allen-Zhu & Li, 2016)
and KatyushaXW (Allen-Zhu, 2018a) which
require tuning a lot of hyper-parameters. From
the results in Figure 3, one can observe that
compared with other algorithms, our STRfree
-1
-*-Hessian-free Cubic
stθ Stochastic CUbic
free
f SRVRC
free
・ ♦■STR,
▼ free
-+-STR
-*-Hessian-free Cubic
-4 Stochastic CUbic
free
f SRVRC
free
・ ♦■STR,
▼ free
-+-STR
50	100
Time (second)
(a) epsilon
100	200	300	400
Time (second)
(b) mnist
Figure 3: Comparison of Hessian-free algorithms
on non-convex logistic and soft-max regressions.
achieves the best convergence speed which demonstrates its high efficiency in realistic applications.
Besides, one also can find that STRfree is much faster than Hessian based STR since computing full
Hessian is actually much computationally expensive than the computation of the Hessian vector.
*
8 Conclusion
We proposed two stochastic trust region variants. Under two settings (whether stochastic first- and
second-order oracle complexities are treated equally), the proposed methods achieve state-of-the-
art oracle complexities. We also propose Hessian-free variants with lowest runtime complexity.
Experimental results testify our theoretical implications and the efficiency of the proposed algorithms.
10
Under review as a conference paper at ICLR 2020
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate
local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory ofComputing, pp.1195-1199. ACM, 2017. 7, 9, 19
Zeyuan Allen-Zhu. Katyusha x: Practical momentum method for stochastic sum-of-nonconvex
optimization. arXiv preprint arXiv:1802.03866, 2018a. 8, 10, 19, 20
Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. In Advances in Neural
Information Processing Systems, pp. 2675-2686, 2018b. 7
Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: even faster svd decomposition yet without agonizing
pain. In Advances in Neural Information Processing Systems, pp. 974-982, 2016. 8, 10, 19
Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. In Advances
in Neural Information Processing Systems, pp. 3716-3726, 2018. 7, 19
Yair Carmon and John C Duchi. Gradient descent efficiently finds the cubic-regularized non-convex
newton step. arXiv preprint arXiv:1612.00547, 2016. 9
Yair Carmon and John C Duchi. Analysis of krylov subspace solutions of regularized non-convex
quadratic problems. In Advances in Neural Information Processing Systems, pp. 10705-10715,
2018. 7, 18
Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. Adaptive cubic regularisation methods for
unconstrained optimization. part i: motivation, convergence and numerical results. Mathematical
Programming, 127(2):245-295, 2011. 2, 8
Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1. Siam, 2000.
1,2,3,8, 13
Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with
a worst-case iteration complexity of (epsilon 3/2) for nonconvex optimization. Mathematical
Programming: Series A and B, 162(1-2):1-32, 2017. 1, 2, 4
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014. 1
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex opti-
mization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 686-696, 2018. 2, 5
Olaf E Flippo and Benjamin Jansen. Duality and sensitivity in nonconvex quadratic optimization
over an ellipsoid. European journal of operational research, 94(1):167-178, 1996. 19
Nicholas IM Gould, Stefano Lucidi, Massimo Roma, and Philippe L Toint. Solving the trust-region
subproblem using the lanczos method. SIAM Journal on Optimization, 9(2):504-525, 1999. 3, 7, 9
Robert Gower, Nicolas Le Roux, and Francis Bach. Tracking the gradients using the hessian: A new
look at variance reducing stochastic methods. In Proceedings of the Twenty-First International
Conference on Artificial Intelligence and Statistics, 2018. 1, 2
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013. 1, 2
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex opti-
mization. arXiv preprint arXiv:1705.05933, 2017a. 2, 6, 8, 9
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex op-
timization. In Proceedings of the 34th International Conference on Machine Learning, 2017b.
1
11
Under review as a conference paper at ICLR 2020
Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.
Mathematical Programming, 108(1):177-205, 2006. 1,2
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning, pp. 2613-2621, 2017. 5
Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic
regularization for fast nonconvex optimization. In Advances in Neural Information Processing
Systems, pp. 2899-2908, 2018. 9,21
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012. 5, 14, 18
Jiulin Wang and Yong Xia. A linear-time algorithm for the trust region subproblem based on hidden
convexity. Optimization Letters, 11(8):1639-1646, 2017. 19
Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Newton-type methods for non-convex
optimization under inexact hessian information. arXiv preprint arXiv:1708.07164, 2017. 1
Junyu Zhang, Lin Xiao, and Shuzhong Zhang. Adaptive stochastic variance reduction for subsampled
newton method with cubic regularization. arXiv preprint arXiv:1811.11637, 2018. 1, 2, 6
Dongruo Zhou and Quanquan Gu. Stochastic recursive variance-reduced cubic regularization methods.
arXiv preprint arXiv:1901.11518, 2019. 6, 7, 8, 9,21
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic variance-reduced cubic regularized Newton
methods. In Proceedings of the 35th International Conference on Machine Learning, 2018a. 1
Dongruo Zhou, Pan Xu, and Quanquan Gu. Sample efficient stochastic variance-reduced cubic
regularization method. arXiv preprint arXiv:1811.11989, 2018b. 1, 2, 6, 8, 22
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic variance-reduced cubic regularized newton
method. In ICML, 2018c. 2, 6, 7, 8, 9, 22
12
Under review as a conference paper at ICLR 2020
A Appendix
In this appendix, Sec. B first provides the proofs for the results in the manuscript. Then, we analyze
MetaAlgorithm 7 and STRfree in Sec. C and Sec. D, respectively. Next, in Sec. E, we develop a fast
QCQP solver to further improve the computational complexity of STRfree. Finally, more experimental
details and results are presented in Sec. F.
B	Deferred Proofs
B.1 Proof of Theorem 3.1
Proof. For simplicity of notation, we denote
Vk d=f VF(Xk) - gk and Vk d=f V2F(Xk) - Hk.
From Assumption 2.3 we have
F(xk+1) ≤F(xk) + hVF(xk), hki + 1 hV2F(xk)hk, hk〉+ L∣∣hk∣∣3
=F (xk ) + hVk+gk, hki + 1 ([V] + Hk ]hk, hk i + L ∣hk k3.
26
Use the CauchySchwarz inequality to obtain
F(xk+1) ≤ F(xk) + hgk,hki + 1 (Hkhk,hki + L∣hkk3 + ∣Vkk∣hk∣ + 1 ∣Vk∣∣hk∣2.	(11)
26	2
The requirement (9) together with the trust region radius ∣∣h∣ ≤ r = V^TL2 allows Us to bound
∣Vkk∣hkk + 1∣Vkk∣hkk2 ≤ 1 ∙√=.	(12)
2	3	L2
The optimality of (5) indicates that there exists a dual variable λk ≥ 0 so that (Corollary 7.2.2 in
(Conn et al., 2000))
First Order : gk + Hkhk + λkL2hk = 0,	(13)
Second Order : Hk + λ L2 ∙ I < 0,	(14)
Complementary : λk ∙ (∣hk∣∣ - r) = 0.	(15)
Multiplying (13) by hk, we have
(gk + Hkhk + λ-L2hk, hki =0.	(16)
Additionally, using (14) we have
λkL
h(Hk + YI)hk, hki ≥ 0,
which together with (16) gives
(gk, hki ≤0.	(17)
Moreover, the complementary property (15) indicates ∣∣hk ∣∣ =7e/L? as we have λk > 37e/L2 > 0
before MetaAlgorithm 1 terminates. Plug (12), (16), and (17) into (11) and use ∣∣hk ∣∣ = /e/L2:
F (xk+1) ≤ F (xk) — Lλ- ∙ ʌ + 1 ∙√X.	(18)
4	L2	2	L2
Therefore, if we have λk > 3e0∙5 /√L2, then
F(xk+1) ≤ F(xk)——1= ∙ e1∙5.	(19)
4 L2
13
Under review as a conference paper at ICLR 2020
Using Assumption 2.1, We find λk ≤ 3e0∙5∕√L2 in no more than 4√L2 ∙ (F(x0) - F(x*))∕e1∙5
iterations.
We now show that once λk ≤ 3e0∙5/√L2, then xk+1 is already an O(e)-SOSP: From (13), we have
kgk + Hkhkk = Lλ ∙khkk ≤ 2e.	(20)
The assumptions ∣∣Vk ∣∣ ≤ e/6 and ∣∣Vk ∣∣ ≤ √eL2∕3 together with the trust region radius ∣∣hk ≤
√e∕L2 imply
∣VF(Xk) + V2F(xk)hk∣∣ ≤ Ilgk + Hkhk∣ + IlVk∣ + ∣Vk ∙ hk∣∣ ≤ 2.5e.	(21)
On the other hand use Assumption 2.3 to bound
∣∣VF(xk+1) -VF(xk) -V2F(xk)hk∣∣ ≤ L2∣∣hk∣∣2 ≤ e.
Combining these two results gives ∣VF (Xk+1)∣ ≤ 3e.
Besides, using Assumption 2.3, ∣Vk ∣∣ ≤ √L2∕3, and (14), we derive the Hessian lower bound
V2F(xk+1) < V2F(xk) - L2 ∙ ∣hk∣I < Hk-PeL2/3I-L2∣hk∣I <-P12L2l.
Hence xk+1 is a 12e-stationary point. Additionally, we have ∣hk ∣ = r according to the complemen-
tary condition (15) for all but the last iteration.	□
B.2 Proof of Lemma 4.1
Proof. Without loss of generality, we analyze the case 0 ≤ k < q2 for ease of notation. We first
focus on Option II. The proof for Option I follows the similar argument.
Option II: Define for k = 0 and i ∈ [s02]
Bi0 d=ef V2fi(x0) - V2F(x0),
and define for k ≥ 1 and i ∈ [s2]
Bik d=ef V2fi(xk) - V2fi(xk-1) - (V2F(xk) - V2F (xk-1)).
{Bik } is a martingale difference sequence. We have for all k and i,
E[Bik|xk] = 0.
Besides, we use Assumption 2.2 for k = 0 to bound
∣Bi0∣ ≤ ∣V2fi(x0)∣ +∣V2F(x0)∣ = 2L1,	(22)
and use Assumption 2.3 for k ≥ 1 to bound
∣Bk∣ ≤∣V2fi(xk) - V2fi(xkT)∣∣ + ∣V2F(xk) - V2F(XkT)Il ≤ 2PL2
From the construction of Hk, we have
s02	0 k s2	j
Hk -V2F(xk) = X Bi + XX B.
Thus using the matrix Azuma’s Inequality in Theorem 7.1 of (Tropp, 2012) and k ≤ p2, we have
Pr{∣Hk - V2F(xk)∣ ≥ t} ≤d ∙ exp{------o------------粤-----------------}
——	PS= 1 4L2∕s22 + Pj= Ps= 1 4eL2∕s2
≤d ∙ exp{-
t2/8
4L1∕s2 + 4p2eL2∕s2
}.
Consequently, we have
Pr{∣Hk - V2F(xk)∣ ≤ peL2} ≥ 1 - δ∕Ko.
14
Under review as a conference paper at ICLR 2020
by taking t = √L, S7 = 16L1 /(eL2) log(dKo∕δ), s2 = 32Lι∕(√eL2) log(dKo∕δ), and p2 =
Li/(2√eL2).
Option I: The proof is similar to the one of Option II except that we replace Bi0 with zero matrix. In
such case, the matrix Azuma’s Inequality implies
Pr{∣∣Hk - V2F(Xk)k ≥ t} ≤ d ∙ exp{----K---t_J^_-------} ≤ d ∙ eχp{---t /— }.
Pk=I Ps= 14心/§2	I 4p2"2/"
Thus by taking t = √eL2, s? = 32√n log(dK0∕δ), and p = √n, we have the result.
Amortized Complexity: In option I, the choice of parameters ensures that: s02 ≤ p2 × s2 and in
option II: n ≤ p2 × s2 . Consequently, the amortized stochastic second-order oracle complexity is
bounded from above by 2s?.	□
B.3 Proof of Lemma 4.2
Without loss of generality, we analyze the case 0 ≤ k < q1 for ease of notation. Define for k ≥ 1
and i ∈ [s1]
aik d=ef Vfi(Xk) - Vfi(Xk-1) - (VF(Xk) - VF (Xk-1)).
{aik } is a martingale difference sequence: for all k and i
E[aik|Xk]=0.
Besides, aik has bounded norm:
kaikk ≤kVfi(Xk)-Vfi(Xk-1)k+kVF (Xk)-VF (Xk-1)k
≤Lιkxk - XkTk+Likxk- XkTk
≤2Lιp∕L2.	(23)
From the construction ofgk, we have
k s1 j
gk - VF(Xk) = XX a-.
j=1 i=1	1
Recall the Azuma’s Inequality. Using k ≤ p1, we have
Pr{kgk - VF(Xk)k ≥t}
t2/8	t2/8
≤ p{-Pk=I Ps=IgL2} ≤	p{-4eLM∕(s1L2)}.
Take t = /6 and denote c = 1152. To ensure that
Pr{kgk -VF(xk)k ≥e∕6} ≤ δ∕K°,
cL2	K	s
we need L1 log K ≤ 胃.The best amortized stochastic first-order oracle complexity can be
obtained by solving the following two-dimensional programming:
min p1≥i,s1≥i	(n+ si(pi - 1))∕pi
s.t.	cL2 log K ≤ 巴, L2	δ	pi
which has the solution s1
min{n,
血∙ cL1 Lg K0}, and pi=maχ{1, qne∙缶
}. Note
that when we take si = n, we directly compute gk = VF(Xk) without sampling.
The amortized stochastic first-order oracle complexity is obtained by plugging in the choice of si and
pi , which completes the proof.
15
Under review as a conference paper at ICLR 2020
B.4 Proof of Lemma 5.1
Without loss of generality, we analyze the case 0 ≤ k < q1 for ease of notation. Define for k ≥ 1
and i ∈ [s1]
bk =fVfi(xk) - Vfi(xk-1) - V2fi(X)(xk - xk-1)
-[VF(xk) - VF(xk-1) - V2F(X)(xk - xk-1)].
{bik } is a martingale differenCe seqUenCe: for all k and i
E[bik |xk] = 0.
Besides, bik has bounded norm:
kbkk≤kVfi(xk)-Vfi(XkT)-V2fi(X)(xk - xk-1)k
+ ∣∣VF(xk)-VF(xk-1) -V2F(X)(xk - XkT)k.
WeCanboUnd ∣∣Vfi(xk) -Vfi(xk-1) - V2fi(x)(xk — XkT)Il as follows.
∣Vfi(xk) - Vfi(xk-1) - V2fi(x)(xk - xk-1)∣
=k J： [V2fi(xkτ + t(xk - xk-1)) - V2fi(x)i (xk - xk-1)dt∣
1
≤ / L2∣∣txk + (1 - t)xk-1 - x∣∣dt ∙ ∣xk - xk-1∣
≤ /1 (t∣xk - Xk + (1- t)kxk-1 - x∣)dt ∙ L2r
0
≤ L2kr2,
where the first ineqUality follows from AssUmption 2.3 and the last ineqUality holds beCaUse kxk -
Xk ≤ kr and ∣∣xk-1 - Xk ≤ kr, where r is the trust region radius. Similarly, We have ∣∣VF(xk)-
VF(xk-1) — V2F(X)(xk — xk-1)k ≤ L2kr2. Thus, we bound
kbikk ≤ 2L2kr2 ≤ 2p1
From the ConstruCtion ofgk, we have
k s1 bj
gk -VF (xk )= XX }.
j=1 i=1	1
We use k ≤ p1 and the Azuma’s inequality to bound
Pr{kgk - VF(xk)k ≥t}
t2/8	t2/8
≤ exp{- j Ps=14∣2 }≤ exp{-4^2pTs1}.
Thus, by taking t = e/6 and C = 1152, we need sp3 ≥ Clog K. Further we want s1p1 ` O(n)
and hence we take p1 = n0.25 and s1 = n0.75clog K. The amortized stochastic first-order oracle
Complexity is bounded by 2s1.
C Analysis of MetaAlgorithm 7
We first show that INEXACTTRWEAK finds an O(e)-SOSP in O(1/e1.5 ) iterations with probability at
least 2/3 as stated in the following lemma.
Lemma C.1. Consider problem (1) under Assumptions 2.1-2.3. Suppose that the differential estima-
tors gk and Hk satisfy Eqn. (9) with probability at least (1 一 e).Besides, suppose that hk is an
approximate solution to (8) such that w.p. (1 一 条),
1	1	e1.5
hgk, hki + -hHkhk, hki ≤ hgk, hki + -hHkhk, hki + -7=,	(24)
2	2	L2
where hk is a global solution to (8). By setting Z = 1/3, r = 'e∕L2, and K = 4√L2∆∕e1∙5,
INEXACTTRWEAK outputs a 500e-SOSP w.p. at least 2/3.
16
Under review as a conference paper at ICLR 2020
Proof. Combining (11) and (24), We have w.p. (1 -奈)，
1	1.5
F(xk+1) ≤ F(Xk) + hgk, hki + -hHkhk, hki + -7=
2	L2	(25)
+ L62 khk k3 + kVk kkhk k + -M kkhk k2,
where hk is a global solution to the QCQP (8) and hk is an approximate solution satisfying (24). We
let λk denote the dual variable corresponding to the global solution hk as defined in Lemma 2.1. We
note that hk and λk are used only in our analysis. The INEXACTWEAK algorithm only requires the
approximate solution Xk without knowledge of hk or λk.
By the assumption that (9) holds with probability (1 -条)and the fact that ∣∣hk ∣∣ ≤ r = √L2e, we
have w.p. (1 -e),
L	1	1.5
Vkhkk3 + ∣vkkkhkk + -Mkkhkk2 ≤ —=.	(26)
6	2	2 L2
Plugging (16), (17), and (26) into (25) and applying the union bound, we have w.p. at least (1 -余),
F(xk+1) ≤ F(Xk) - L2λk]hkk2 +岁=F(xk) - L^ + 岁,	(27)
4	2 L2	4	2 L2
where the second inequality follows from (15):
0 = λk(khkk - r) = λk(khkk - r)(khkk + r) = λk(khkk2 - r2).	(28)
Summing inequality (27) from k = 0 to K - 1 and applying the union bound, we have w.p. at least
(1 - Z/2),
1 K-1
⅛ X λk ≤
k=0
4(F(x0) - F(xK+1))	6e1∙5	4∆	6√1
L2r2K	+ L2.5r2 ≤ eK + √L2
(29)
where the second inequality follows from Assumption 2.1 and our choice of the trust region radius.
By sampling k uniformly from {0,..., K - 1}, we obtain
K-1
E[λfc] = K X λk,
k=0
(30)
where the expectation is taken over the randomness of k Combining (29) and (30) and taking
K = 4∆√L2∕e1∙5, we have w.p. at least (1 - Z/2)
E[λk] ≤ ⅛e.
√L2
(31)
Since λk is always no-negative, by Markov’s inequality and the union bound, with probability at least
1 - ζ, we have
λk ≤ ≡
(32)
By taking Z = 1/3, we have w.p. at least 2/3, λk ≤ 42，!/L2. The rest of the proof is similar to
Theorem 3.1 and we have the result.	□
The following theorem shows that MetaAlgorithm 7 finds an O(e)-SOSP w.p. (1 - δ) after running
INEXACTTRWEAK for θ(log(1/b)) times.
Theorem C.1 (Iteration Complexity of MetaAlgorithm 7). In the same setting as Lemma C.1, let
T = 3log(2/6), ci = 600, C2 = 500, and S = 32L1 log(4d/6). Then MetaAlgorithm 7 finds a
600e-SOSP with probability at least (1 - δ).
17
Under review as a conference paper at ICLR 2020
Proof. By Lemma C.1 and our choice of T, with probability (1 - δ∕2), at least one of Xt is
a 500e-SOSP. On the other hand, since ψt(vt) ≤ ψt(vt) + √L with probability 1 - δ∕4, if
Ψt(Vt) ≥ -√c2eL2, then, w.p. 1 - δ∕4,
ψt(Vt) ≥ ψt(vt) - PL ≥ -Pc2eL2 - PL ≥ -P550eL2,	(33)
where the last inequality follows from our choice of c2 .
Option I: Since Ht = V2F(xt) is the full Hessian, ψt(vt) is the smallest eigenvalue of V2F(Xt).
Applying the union bound, we conclude that MetaAlgorithm 7 outputs a 600e-SOSP w.p. (1 - δ).
Option II: Let Bi := V2fi(Xt) - V2F(Xt) for i ∈ H, then
1s
Ht - V2F(xt) = — £ Bi∙	(34)
i=1
By Assumption 2.2, we have
kBik ≤ kV2fi(Xt)k + kV2F(Xt)k ≤2L1∙	(35)
Applying the matrix Azuma’s Inequality in Theorem 7.1 of Tropp (2012) leads to
Pr{kHt- v2F(Xt)Il ≥ PeL2} ≤ d ∙ exp( 32Lj )∙	(36)
32L2
By taking S = -^LIlog(4d∕δ) and applying the union bound, we have with probability 1 - δ,
V2F(xt) < Ht - PeLI < (ψt(vt) - PL)I < -P600eL2I,	(37)
where the last inequality follows from (33). This completes the proof.	□
D	Proof of Theorem 6.1
Proof. We first analyze the computational cost of Lanczos method. By Corollary 2 in (Carmon &
Duchi, 2018), for any desired accuracy e, Lanczos method achieves this accuracy in O(√logr∣Pd)
Lanczos iterations w.p. at least (1 - p). Without loss of generality, we assume that the number of
Lanczos iterations is strictly smaller than the dimension d, otherwise the QCQP subproblem can
be solved exactly. We note that each Lanczos iteration involves computation of one matrix-vector
product. Therefore, to satisfy the condition (24) in Lemma C.1, one needs to evaluate O(1∕(L2e)0.25)
Hessian-vector products of the form Hk v. Similarly, to solve (10) up to accuracy √eL2 w.h.p., one
needs to evaluate O(1∕(L2e)0.25) Hessian-vector products of the form Htv.
In MetaAlgorithm 7, to verify whether the candidate solution vt is indeed an O(e)-SOSP, one needs at
most O(n) stochastic gradient evaluations and O(min{n, log(4d∕δ)L12∕(L2e)}∕(L2e)0.25)) stochas-
tic Hessian-vector product evaluations, where the latter one follows from the proof of Theorem 7.
We proceed to analyze the computational complexity of the InexactTRWEAK procedure. Recall that
the iteration complexity of MetaAlgorithm 7 is O(log(1∕δ)∕e1.5). Following Lemma 4.2 and Corol-
lary 4.1, the stochastic first-order oracle complexity is O(min{n∕e1∙5, √n∕e2}log(1∕δ)), Following
the proof of Lemma 4.1 and Corollary 4.1, when p2 = 1, the overall stochastic Hessian sample com-
plexity is O(min{n∕e1∙5,1∕e2∙5} log(1∕δ)), Since it takes O(1∕e0∙25) Lanczos iterations to meet the
condition (24) as stated above, the overall stochastic Hessian-vector product oracle complexity is
O(min{n∕e1∙75,1∕e2∙75}log(1∕δ)). Combining the stochastic first-order and Hessian-vector product
complexities, the overall runtime is O(dmin{n∕e1∙75,1∕e2∙75 + √n∕e2}log(1∕δ)).	□
E A Faster Hessian-Vector Based QCQP S olver
We recall from the previous section that, to approximately solve a quadratic subproblem in STRfree,
Lanczos method requires O(min{n∕e0∙25,1∕e1∙25}) stochastic Hessian-vector product evaluations.
In this section, we propose a faster QCQP solver with an O(min{n + n0.75∕e0.25,1∕e}) complexity.
Replacing Lanczos method with this QCQP solver in STRfree results in a faster Hessian-free method,
which we refer to as STRfree+.
18
Under review as a conference paper at ICLR 2020
E.1 Convex Reformulation of QCQP
To begin with, we present a known result that is key to achieve faster algorithm than Lanczos method.
We summarize this result in Lemma E.1 which shows that the trust region subproblem is equivalent
to a convex QCQP.
Lemma E.1. (Convex Reformulation of QCQP (Flippo & Jansen, 1996; Wang & Xia, 2017)) Denote
λmin as the smallest eigenvalue of Hk. Let umin be a corresponding eigenvector. W.l.o.g., we assume
that hgk, Umini ≤ 0 ∙ Let μ = min{λmin, 0}. Then the QCQP (8) is equivalent to the convex problem
min qk(h) = hgk, hi + 1 h(Hk - μI)h, hi + 1μr2	(38)
h∈Rd ,khk≤r	2	2
in the sense that (8) and (38) have the same minimum function value. Moreover, when λmin < 0, for
any optimal solution of (38), denoted by hck,
hck +
√hhk, Umini2 -kUmink2(khk k2 — r2) -hh, Umini
kumink2
umin
(39)
is a global minimizer of the original QCQP (8).
To perform the above reformulation, one needs to compute the exact eigenpair (λmin, Umin). Nev-
ertheless, as We shall see, it is sufficient to compute an approximate eigenpair (λ, U) such that
λmin ≤ λ = UTHkU ≤ λmin + ?,	(40)
where e is a target accuracy to be determined later. We note that e ≤ 2L2 W.l.o.g. since kHk k ≤ L2.
With this approximate eigenpair, it remains to solve the following convex problem
min	qk(h) = hgk, hi + 1 h(Hk - μI)h, hi + 1 μr2	(41)
h∈Rd ,khk≤r	2	2
where μ = mm{0, λ - e}. One can check that the problem (41) well approximates (38).
Corollary E.1. Let qk and qk be the minimum function value of (38) and (41), respectively. Assume
λmin ≤ λq = UqTHkUq ≤ λmin + eq. Then
|qk- qk∣≤ qr2.	(42)
We note that the above convex reformulation approach divides an indefinite QCQP into two subprob-
lems: (i) computation of an approximate eigenpair (λ, Uq); (ii) solving the convex problem (41). As
we shall see, by exploiting the finite-sum structure of the Hessian Hk, these two subproblems can be
efficiently solved. We treat these two subproblems in the following two subsections, respectively.
E.2 Finding the Smallest Eigenvector
To find a unit vector that satisfies requirement (40), we resort to the AppxPCA method (Allen-Zhu
& Li, 2016), which first finds an approximate eigenvalue λ = λmin - eq via binary search and then
applies Power method to the positive definite matrix (Hk - λI)-1 for a logarithmic number of
iterations. Computing (Hk - λI)-1v for any vector v is equivalent to solving the eq-strongly convex
problem (Allen-Zhu & Li, 2018)
min φk (u) := 1 ut(Hk - λI)u - hv, Ui	(43)
u2
We note that Hk = ∣sS∣ P∈ V2fi(xk). Specifically, in STRfree, either |S| = n (i.e., HkiS the
full Hessian) or |S| = O(Ll∕(L2e)) by Lemma 4.1. Therefore, φk(∙) can be expressed as sum of
non-convex functions
X Φk (U)=2 X (1UT(V2fi(xk) - λI)U
i∈S	i∈S
By observing that each φik is non-convex and has (4L2 )-Lipschitz gradient, we can use
KatyushaXS (Allen-Zhu, 2018a) to solve problem (43) in O(|S| + |S|3/4pL2/e) stochastic Hessian-
vector product (i.e., V2fi (xk)U) evaluations. The following result is taken from (Agarwal et al.,
2017, Section G.3), which gives the overall computation complexity of AppxPCA.
φk(U) = |S|
19
Under review as a conference paper at ICLR 2020
Algorithm 9 Fast QCQP Solver
Input: Hk, gk, r,己 6ι
1:	Use APPXPCA to find (λ, U) satisfying (40), in which the matrix inverse is solved by KatyushaXs;
2:	Use KatyushaXW to solve (41) up to accuracy ∈ι, i.e., find a vector h such that qk(h) - qk ≤ ∈ι
with high probability;
3:	Return h+ (VZ〈h, U〉2 -∣∣Uk2(∣∣h∣∣2 - r2) -(h, Ui)U/∣∣U∣∣2.
Algorithm 10 STRfree+
1:	In the same setting as MetaAlgorithm 7,
2:	construct gradient estimator gk by Estimator 4;
3:	construct Hessian estimator Hk by
4:	Option I: Hk := V2F(Xk);
5:	Option II: Draw S samples indexed by H and let Hk := V2f (xk; H);
6:	use Algorithm 9 to solve QCQP subproblems.
LemmaE.2. Let Hk =吉 P%∈s V2 fi(xk) ∈ Rd×d, where ∣∣V2 fi(xk )∣ ≤ L?. With probability at
least 1 — P, AppxPCA produces a unit vector U satisfying UTHkU ≤ λmin + e. The total stochastic
Hessian-vector product oracle complexity is O(|S| + |S|3/4 y∕L2∕e).
E.3 S olving the Convex QCQP
In what follows, we show that the convex problem (41) can be solved efficiently. We first observe that
problem (41) has a finite-sum structure and can be rewritten as an unconstrained problem of the form
mRd S Xqi(h)+ψ(h) = |S| χ (hgk, hi+2h(v2fi(χk)-μI)h, hi+1 μr2) +ψ(h), 附
where Ψ(h) = 0 if ∣h∣ ≤ r, otherwise Ψ(h) = +∞. We note that each qk(h) in (45) has
(4L2)-Lipschitz continuous gradient since ∣∣V2fi(Xk)Il ≤ L2 and e ≤ 2L2. Therefore, we can use
KatyUShaXW (Allen-Zhu, 2018a) to solve (45). By (Allen-Zhu, 2018a, Theorem 4.6), KatyushaXW
finds a point h such that E[qk (h) - qk] ≤ Ii using O(|S| + ∣S∣3∕4√L2 ∙ r∕√1) stochastic Hessian-
vector products, where & is the target accuracy to be determined later.
E.4 Putting it All Together
The complete procedure of our fast QCQP solver is summarized in Algorithm 9. Combining all the
above results and setting r = //L2, & = √eL2∕2, and &=er2, one can find an approximate
solution to QCQP (8) satisfying requirement (24) in O&(|S| + |S|3/4L20.25/0.25) stochastic Hessian-
vector product evaluations. By replacing Lanczos method with this solver in STRfree, we derive a new
Hessian-free method called STRfree+, which is summarized in Algorithm 10. The following theorem
establishes the overall runtime complexity of STRfree+ for finding an &-SOSP.
Theorem E.1. Consider Algorithm 10 for solving problem (1). Let Z = 1/3, r = &JLL^, K =
4√L2∆∕e1-5, T = 3log(2∕δ), ci = 600, c? = 500, and S = 3L1 log(4d∕δ). The hyper-parameters
in Estimator 4 are set to the same values as those in Lemma 4.2. Besides, let e = √eL2∕2 and
&&i = &&r2 in Algorithm 9. To find an O(&)-SOSP w.p. at least 1 - δ, the runtime complexity is
O(d min{n∕e1∙5 + n0-75∕e1-75,1∕e2^5 + √n∕e2}log(1∕δ)).
Proof. The proof directly follows from that in Sec. D.
□
We compare the runtime complexity of STRfree and STRfree+ with existing Hessian free methods
in Table 2. One can see that STRfree strictly outperforms Hessian-free Cubic. Besides, STRfree
outperforms Fast-Cubic if n ≥ Ω(1∕e4/3), which is a mild condition for large-scale problems in the
moderate accuracy case. STRfree+ strictly outperforms both Hessian-free Cubic and Fast-Cubic. We
20
Under review as a conference paper at ICLR 2020
ijcnn
a9a
-1
-20
-20
0O
-21
0.5
1
1.5
2
20
Time (second)
(a) non-convex logistic regression problem
1.5
Time (second)
5	10	15
# Epoch
5	10
# Epoch
9 -10
二
›
力-15
-
o
* TR
-▼-ARC
-A-SCR
Y SVRC
Li Lite-SVRC
-β SRVRC
→-STR
%
S≡j-15
+ TR
-▼-ARC
-A-SCR
Y SVRC
Li Lite-SVRC
-R SRVRC
→-STR
9-10
2-15
* TR
-▼-ARC
-^-SCR
Y SVRC
Li Lite-SVRC
Y SRVRC
→-STR
-+-TR
-▼-ARC
-A-SCR
Y・SVRC
+ Lite-SVRC
-∙-SRVRC
→-STR
w08
a9a
-20
-20
-250
0.5
1
1.5
2
Time (second)
5	10	15
# Epoch
5-10
1-15
ħθ
* TR
-▼-ARC
-A-SCR
Y SVRC
Li Lite-SVRC
-β SRVRC
-♦-STR
-1
-15
+ TR
-▼-ARC
-A-SCR
Y SVRC
÷ Lite-SVRC
-∙ SRVRC
♦ STR
20
-20 ∙
力-15
ħθ
O
+ TR
-▼-ARC
*SCR
Y SVRC
+ Lite-SVRC
-∙ SRVRC
+ STR
-25 _
0	2	4	6	8	10
Time (second)
-1
-14
-+-TR
-▼-ARC
-A-SCR
Y・SVRC
+ Lite-SVRC
-∙-SRVRC
+ STR
0	2	4	6	8	10
# Epoch
^⅞-1t-



(b) nonlinear least square problem
Figure 4: Comparison of gradient norm on both the non-convex logistic regression and nonlinear
least square problems.
note that the runtime analyses in (Tripuraneni et al., 2018; Zhou & Gu, 2019) rely on an additional
assumption which states that for all x, with probability 1,
kVfi(x) -VF(x)k≤ σ.	(46)
Under this additional assumption, one can use the same argument as in Section B.2 and B.3 to
prove that STRfree achieves a runtime complexity of O(dmin{n∕e1∙75, n0∙5∕e2 + 1∕e2∙75,1/e3})1.
Similarly, the runtime complexity of STRfree+ would be O(dmin{n/e1.5 + n0-75∕e1-75, 1∕e2^5 +
√n∕e2,1/e3}), In this sense, both STRfree and STRfree+ outperform Stochastic Cubic and SRVRCf3.
Table 3: Descriptions of the five testing datasets.
	#sample	#feature ∣		#sample	#feature
a9a	32,561	123	w8a	49,749	300
ijcnn	49,990	22	phishing	7,604	68
codrna	28,305	8	mnist	60,000	784
epsilon	40,000	2,000			
F Additional Experimental Results
F.1 More Experimental Details
Descriptions of Testing Datasets. We briefly introduce the seven testing datasets in the manuscript.
Among them, six datasets are provided by the LibSVM website2, including (a9a, ijcnn, codrna,
phishing, w8a and epsilon). The detailed information is summarized in Table 3. We can observe
that these datasets are different from each other in feature dimension, training samples, etc.
1To obtain this complexity, one needs to replace the full gradient in line 2 of Estimator 4 (and line 6 of
MetaAlgorithm 7) with a mini-batch stochastic gradient when n ≥ Ω(1∕e2).
2https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
21
Under review as a conference paper at ICLR 2020
Experimental Settings. In the manuscript, following SVRC (Zhou et al., 2018c) and Lite-
SVRC (Zhou et al., 2018b), we select hyper parameters from a set, namely s1 from {0.2n, 0.6n, n},
s2 from {0.01n, 0.1n, 0.2n}, p1 and p2 from {0.01n0.5, 0.05n0.5, 0.1n0.5}. For the Hessian estima-
tion at the beginning of each p2 iterations, we use full Hessian. Similarly, for the gradient estimation
at the beginning of each p1 iterations, we adopt the full gradient as the gradient estimation.
Memory Analysis. SVRC, Lite-SVRC, and our method need to store the previous and current
gradient and Hessian and thus their memory complexity is 2(d2 + d). TR, CR (ARC) and SCR need
to compute current and Hessian and thus has complexity d2 + d. So these memory is of the same
order but our method is much faster than TR, CR and SCR both validated by theory and experiments.
F.2 More Experiments
Here we give more experimental results on the gradient norm v.s. the algorithm running time and
the Hessian sample complexity. Due to the space limit, in the manuscript we only provide the
gradient-norm related results on the codrna dataset. Here we provide results of a9a and ijcnn
datasets in Figure 4. One can observe that on both the logistic regression with non-convex regularizer
and the nonlinear least square problems, the proposed algorithm always shows sharper convergence
behavior in terms of both the running time and the Hessian sample complexity. These observations
are consistent with the results in Figure 2 in the manuscript. All these results demonstrate the high
efficiency of our proposed algorithm and also confirm our theoretical implication.
22