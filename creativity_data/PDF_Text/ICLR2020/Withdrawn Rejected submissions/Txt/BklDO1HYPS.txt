Under review as a conference paper at ICLR 2020
Accelerated Variance Reduced Stochastic
Extragradient Method for Sparse Machine
Learning Problems
Anonymous authors
Paper under double-blind review
Ab stract
Recently, many stochastic gradient descent algorithms with variance reduction
have been proposed. Moreover, their proximal variants such as Prox-SVRG can
effectively solve non-smooth problems, which makes that they are widely applied
in many machine learning problems. However, the introduction of proximal oper-
ator will result in the error of the optimal value. In order to address this issue, we
introduce the idea of extragradient and propose a novel accelerated variance re-
duced stochastic extragradient descent (AVR-SExtraGD) algorithm, which inher-
its the advantages of Prox-SVRG and momentum acceleration techniques. More-
over, our theoretical analysis shows that AVR-SExtraGD enjoys the best-known
convergence rates and oracle complexities of stochastic first-order algorithms such
as Katyusha for both strongly convex and non-strongly convex problems. Finally,
our experimental results show that for ERM problems and robust face recognition
via sparse representation, our AVR-SExtraGD can yield better performance than
state-of-the-art algorithms such as Prox-SVRG and Katyusha. The asynchronous
variant of AVR-SExtraGD outperforms KroMagnon and ASAGA, which are the
asynchronous variants of SVRG and SAGA, respectively.
1 Introduction
In this paper, we mainly consider the following composite convex optimization problem:
min IP(X) d=f F(x) + R(X) = ~χf fi(x) + R(X) I
x∈Rd	n
(1)
where F (x) : Rd → R is the average of smooth convex component functions fi (x), and R(x) is a
relatively simple convex function (but may not be differentiable). In this paper, We use k ∙ k to denote
the standard Euclidean norm, and ∣∣∙kι to denote the 'ι-norm. Moreover, we use P* to denote the
real optimal value of P(∙), and P* to denote the optimal value obtained by algorithms.
This form of optimization problems often appears in machine learning, signal processing, data sci-
ence, statistics and operations research, and has a wide range of applications such as regularized
empirical risk minimization (ERM), sparse coding for image and video recovery, and represen-
tation learning for object recognition. Specifically, for a collection of given training examples
{(a1, b1), ..., (an, bn)}, where ai ∈ Rd, bi ∈ R (i = 1, 2, ..., n) and ai is a feature vector, while
bi is the desired response. When fi(x) = 11 (aTx-bi)2, we can obtain the ridge regression problem
by setting R(X) = 1 ∣∣x∣∣2. We also get the Lasso or Elastic-Net problems by setting R(X) = λ∣x∣ι
or R(X)=削 ∣x∣∣2 + λι∣X∣ι, respectively. Moreover, if we set fi(X) = log(1 + exp(-biXT ai)), we
will get the regularized logistic regression problem.
1.1	Recent Research Progress
The proximal gradient descent (PGD) method is a standard and effective method for Problem (1),
and can achieve linear convergence for strongly convex problems. Its accelerated algorithms, e.g.,
accelerated proximal gradient (APG) (Tseng (2008); Beck & Teboulle (2009)), attain the conver-
gence rate of O(1/T 2) for non-strongly convex problems, where T denotes the number of iterations.
1
Under review as a conference paper at ICLR 2020
In recent years, stochastic gradient descent (SGD) has been successfully applied to many large-scale
learning problems, such as training for deep networks and linear prediction (Tong (2004)), because
of its significantly lower per-iteration complexity than deterministic methods, i.e., O(d) vs. O(nd).
Besides, many tricks for SGD have also been proposed, such as Loshchilov & Hutter (2016). How-
ever, the variance of the stochastic gradient may be large due to random sampling (Johnson & Tong
(2013)), which leads that the algorithm requires a gradually reduced step size, thus it will converge
slow. Even under the strongly convex condition, SGD only achieves a sub-linear convergence rate
O(1/T). Recently, many SGD methods with variance reduction have been proposed. For the case
of R(x) = 0, Roux et al. (2012) developed a stochastic average gradient descent (SAG) method,
which is a randomized variant of the incremental aggregated gradient method proposed by Blatt
et al. (2007). Then stochastic variance reduced gradient (SVRG) (Johnson & Tong (2013)) was
proposed, and has been widely introduced into various subsequent optimization algorithms, due to
its lower storage space (i.e., O(d)) than that of SAG (i.e., O(nd)). SVRG reduced the variance
effectively by changing the estimation of stochastic gradients. The introduction of a snapshot point
X mainly has the effect of correcting the direction of gradient descent, and reduces the variance.
Later, KoneCny & Richtarik (2013) proposed the semi-stochastic gradient descent methods as well
as their mini-batch version (KoneCny et al. (2014)). And their asynchronous distributed variant
(Ruiliang et al. (2016)) is also been proposed later. More recently, Lin & Tong (2014) proposed the
Prox-SVRG method, which introduced the proximal operator, and then applied the idea of SVRG to
solve the non-smooth optimization problems. However, Prox-SVRG can only be used to solve the
strongly convex optimization problems. In order to solve the non-strongly convex problems, Zeyuan
& Yuan (2016) proposed the SVRG++ algorithm. Besides, to accelerate the algorithm and reducing
the complexity, by combining the main ideas of APG and Prox-SVRG, Nitanda (2014) proposed
an accelerated variance reduction proximal stochastic gradient descent (Acc-Prox-SVRG) method,
which can effectively reduce the complexity of the algorithm compared to the two basic algorithms.
Very recently, Zeyuan (2017) developed a novel Katyusha algorithm which introduced the Katyusha
momentum to accelerate the algorithm. With the development of parallel and distributed computing
which can effectively reduce computing time and improve performance, Ryu & Wotao (2017) came
up with an algorithm called Proximal Proximal Gradient, which combined the proximal gradient
method and ADMM (Gabay & Mercier (1976)). Furthermore, it is easy to implement in parallel and
distributed environments because of its innovative algorithm structure.
1.2	Our Main Contributions
We find that due to the introduction of proximal operator, there is a gap between P* and P*, and
its theoretical derivation can be seen in Appendix A. To address this issue, Nguyen et al. (2017)
proposed the idea of extragradient which can be seen as a guide during the process, and introduced
it into the optimization problems. Intuitively, this additional iteration allows us to examine the
geometry of the problem and consider its curvature information, which is one of the most important
bottlenecks for first order methods. By using the idea of extragradient, we can get a better result in
each inner-iteration. Therefore, the idea of extragradient is our main motivation. In this paper, we
propose a novel algorithm for solving non-smooth optimization problems. The main contributions
of this paper are summarized as follows.
•	In order to improve the result of the gap between Pb* and P*, and achieve fast convergence, a
novel algorithm, which combines the idea of extragradient, Prox-SVRG and the trick of momen-
tum acceleration, is proposed, called accelerated variance reduced stochastic extragradient descent
(AVR-SExtraGD).
•	We provide the convergence analysis of our algorithm, which shows that AVR-SExtraGD achieves
linear convergence for strongly convex problems, and the convergence condition in the non-strongly
convex case is also given. According to the convergence rate, we can know that AVR-SExtraGD has
the same excellent result as the best-known algorithms, such as Katyusha.
•	Finally, we show by experiments that the performance of AVR-SExtraGD (as well as VR-
SExtraGD, which is the basic algorithm of AVR-SExtraGD) is obviously better than the popular
algorithm, Prox-SVRG, which confirms the advantage of extragradient. For the widely used accel-
erated algorithm, Katyusha, the performance of our algorithm is still improved.
2
Under review as a conference paper at ICLR 2020
2 Related Work
2.1	Basic Assumptions
We first make the following assumptions to solve the problem (1):
Assumption 1 (Smoothness). The convex function F (∙) is L-smooth, i.e., there exists a constant
L> 0 such that for any x,y ∈ Rd, ∣∣VF (X)-VF (y)k ≤ L∣∣x-y∣∣.
Assumption 2 (Lower Semi-continuity). The regularization function R(∙) is a lower Semi-
continuous function, i.e., ∀x0 ∈Rd,
lim inf R(x) ≥ R(x0).
x→x0
But it is not necessarily differentiable or continuous.
Assumption 3 (Strong Convexity). In Problem (1), thefunction R(∙) is μ-strongly convex, i.e., there
exists a constant μ>0 such thatfor allx,y ∈Rd, it holds that
R(X) ≥ R(y) + hG, χ - yi + μ∣∣χ - yk2,	⑵
where G ∈ ∂R(y) which is the set OfSUb-gradient of R(∙) at y.
2.2	Prox-SVRG and Extragradient Descent Methods
An effective method for solving Problem (1) is Prox-SVRG which improved Prox-FG (Lions &
Mercier (1979)) and Prox-SG (Langford et al. (2009)) by introducing the stochastic gradient and
combining the idea of SVRG, respectively. For strongly convex problems, Prox-SVRG can reach
linear convergence with a constant step size, and its main update rules are
Vfik(Xk-I) = Vfik(Xk-1)-Vfik(X)+ VF(X); Xk = ProXR(Xk-I- ηVf⅛(xk-ι)), ⑶
where X is the snapshot point used in SVRG, Vfik(Xk-I) is the variance reduced stochastic gradient
estimator, and ProxR(∙) is the proximal operator. Although Prox-SVRG can converge fast, because
of proximal operator, the final solution has the deviation, which makes the solution inaccurate, thus
Prox-SVRG still needs to be further improved, which is our important motivation.
The extragradient method was first proposed by Korpelevic (1976). It is a classical method for
solving variational inequality problems, and it generates an estimation sequence by using two pro-
jection gradients in each iteration. By combining this idea with some first-order descent methods,
Nguyen et al. (2017) proposed an extended extragradient method (EEG) which can effectively solve
the problem (1), and can also solve relatively more general problems as follows:
min
x∈Rd
P(X)d=efF(X) + R(X)
where F(X) is not necessarily composed by multiple functions fi(X). Unlike the classical extragra-
dient method, EEG uses proximal gradient instead of orthogonal projection in each iteration. The
main update rules of EEG are
yk = ProxsRk (Xk - skVF(Xk)); Xk+1 = ProxαRk (Xk - αkVF(yk)),
where sk and αk are two step sizes. From the update rules of EEG, we can see that in each iteration,
EEG needs to calculate two gradients, which will definitely slow down the algorithm. Therefore,
the algorithm needs to be further accelerated by an efficient technique.
2.3	Momentum Acceleration and MiG
Firstly, we introduce the momentum acceleration technique whose main update rules are
vdwt = βvdwt-1 + (1 - β)dwt; wt = wt-1 - αvdwt ,
where dw is the gradient of the objective function at w, β is a parameter, and α is a step size.
The update rules take not only the gradient of the current position, but also the gradient of the
past position into account, which makes the final descent direction of wt after using momentum
3
Under review as a conference paper at ICLR 2020
Algorithm 1 AVR-SExtraGD
Input: Initial vector x°, the number of epochs S, the number of iterations m per epoch, the step
sizes η1, η2, momentum parameter β, and the set K.
Initialize: X0 = x1 = x0, ρ = 1+ημ.
1:	for s = 1, 2, . . . , S do
2:	Compute VF(Xs-1);
3:	βs = β (SC) or βs = s+4 (non-SC);
4:	for k = 1, 2, . . . , m do
5:	Pick ik uniformly at random from {1, ..., n};
6:	if k ∈ K then
7:	Xk-1/2 = PrOXRI xxk-ι - ηι⅜k(βsχk-ι + (1-eS)XsT));
8:	Xk = PrOXR2 卜k-1/2 -η2Vfik(IesXk-1/2 + (1-eS)XsT));
9:	else
10:	Xk = PrOXRI (Xk-I- ηι⅜k(βs Xk-I +(I- es)Xs-1));
11:	end if
12:	end for
13:	Xs = βs(Pk=1 PkT)T Pm=1 PkT xk-1∕2+xk +(1-βs)Xs-1 (SC)
or Xs = βs Pm=I xk-122+xk +(1-βs)Xs-1 (non-SC);
s+1	s
:	X0	= Xm;
15: end for
Output: Xs .
reduce the oscillation of descent, thus this method can effectively accelerate the convergence of the
algorithm.
According to the Nesterov’s momentum, lots of accelerated algorithms were proposed, such as APG
and Acc-ProX-SVRG. Later, Zeyuan (2017) proposed Katyusha to further accelerate the algorithm,
and MiG (Kaiwen et al. (2018)) was proposed to simplify the structure of Katyusha, and the mo-
mentum acceleration of MiG is embodied in each iteration as follows:
yk-ι = βsXk-ι + (1- βs)Xs-ι.
Moreover, it is easy to get that the oracle compleXity of MiG is less than that of ProX-SVRG and
APG, which means that MiG can effectively accelerate the original ProX-SVRG algorithm. There-
fore, we can also use this acceleration technique to accelerate our algorithm and address the issue of
slow convergence due to the calculations of two different gradients.
3 Our AVR-SExtraGD Method
We note that EEG requires computing two full gradients in each iteration, which will take a lot
of time for large-scale machine learning problems. Therefore, we first consider and propose the
stochastic variant of the algorithm, namely stochastic extragradient descent (SEXtraGD), to re-
duce the per-iteration computational compleXity, and further propose an efficient variance reduced
stochastic extragradient descent (VR-SEXtraGD) algorithm. Their main update rules and the de-
tailed algorithm of VR-SEXtraGD can be found in AppendiX C.
On the basis of VR-SEXtraGD, we refer to the momentum acceleration technique proposed in MiG,
and propose an innovative accelerated variance reduced stochastic eXtragradient descent algorithm,
called AVR-SExtraGD. It is used to solve non-smooth (both SC and non-SC) optimization prob-
lems. To further accelerate the algorithm and address the issue of slow convergence speed caused by
two gradients in each inner-iteration, only part of the iterations are updated by eXtragradient descent.
Our AVR-SEXtraGD algorithm is outlined in Algorithm 1.
Firstly, we give some eXplanation for Algorithm 1. For our AVR-SEXtraGD, we need to compute
a full gradient of F (X). And Step 3 in Algorithm 1 is the selection of momentum parameter. Step
6 to Step 8 are the update rule of AVR-SEXtraGD, and Step 9 is the update rule of MiG. Here we
only use AVR-SEXtraGD in the set K. Step 13 is the formulation of the snapshot point. Finally, we
4
Under review as a conference paper at ICLR 2020
give the set of the start point of next inner-iteration in Step 14. Moreover, our output is the snapshot
point of the last outer iteration.
And for our AVR-SExtraGD algorithm, we have the following remarks.
•	Following the requirement of step sizes in EEG, the step sizes in our algorithms also need to satisfy
similar conditions. After combining all the conditions, We get the conditions: ηι ≤ 芸, η ≤ L -ηι.
•	In AVR-SExtraGD, we use one more trick to speed up the algorithm, that is, only part of the
iterations (i.e., When k ∈ K) are updated by extragradient descent, and the rest of the iterations are
still updated by the update rules of MiG. For different problems and different data sets, We manually
adjust the choice of K, and the details can be seen in Section 5.3.
•	For the momentum parameter β, when P(∙) is a strongly convex function, we can set β as a
constant which is generally set as 0.9. And β is also set as 1-V^nl in Acc-Prox-SVRG, while we
1+ V μn2
set β = 0.9 in our AVR-SExtraGD. However, when P(∙) is non-strongly convex, the value of β in
each iteration is no longer fixed. We set βs as a decreasing sequence, which satisfies β21τ ≥ 量.
Particularly, in AVR-SExtraGD, we set βs = ^++4, which satisfies the inequality defined above.
As we all know, for the general GD method, the iterate xk in each iteration eventually converges to
the real optimal point of the function, so there is no error in the final optimal value. Therefore, the
proximal operator will introduce a bad result in convergence.
To adress this issue, we introduce the idea of extragradient, which takes one more proximal operator
than Prox-SVRG. And according to the idea of extragradient, we know that the update structure of
EEG can make use of the curvature information of the objective function. Although we change the
original EEG into a stochastic version, the advantage of the extragradient structure is still retained
to some degree, and thus our algorithm can get a better result than the algorithm without extragra-
dient.That is, although the method of extragradient can not directly reduce the gap of the optimal
value, it can improve the bad result brought by the gap, and obtain a better result after every inner
loop, Thus, AVR-SExtraGD can improve the accuracy of the algorithm.
In summary, our AVR-SExtraGD method combines the advantage of Prox-SG for solving non-
smooth optimization problems, the advantage of EEG, and the trick of SVRG to reduce the variance
of stochastic gradient. And it is further accelerated by introducing the momentum acceleration used
in MiG. Therefore, our algorithm has more advantages than the basic algorithms mentioned above.
4	Convergence Analysis
In this section, we analyze the convergence properties of AVR-SExtraGD under strongly convex and
non-strongly convex conditions. For convenience analysis, we use VkF(∙) to denote Wik (∙), that
defined in (3) in the analysis of AVR-SExtraGD. We give some key lemmas, which are important
to prove the convergence of AVR-SExtraGD in Appendix B, and all the proofs of our lemmas and
theorems in this section are also given in Appendix B.
4.1	For SC Problems
For strongly convex problems, the linear convergence of AVR-SExtraGD can be guaranteed by the
following theorem.
Theorem 1 (Strongly Convex). Suppose that Assumptions 1, 2 and 3 hold, and let x* =
argminx P(x). In addition, assume ηι = η2 = η > 0 and Le + Le ≤ ɪ. Then, by appropri-
ately choosing η, β and m = Θ(n), Algorithm 1 achieves an -additive error with following oracle
complexities in expectation:
(o(√κnlogP(XO)-Pd)), if m ≤ 3,
[O(nlog P(XO)-P(x*)),	if £ > 3,
which also means that for SC problems, the oracle complexity of Algorithm 1 is O((n +
√κn) log P (XO)-P (X*)).
5
Under review as a conference paper at ICLR 2020
This result means that for strongly convex problems, AVR-SExtraGD achieves linear convergence
and enjoys the best-known oracle complexity of stochastic first-order algorithms, such as Katyusha.
4.2	For Non-SC Problems
The convergence of AVR-SExtraGD for solving non-SC problems can be guaranteed by the follow-
ing theorem.
Theorem 2 (Non-Strongly Convex). Suppose that Assumptions 1 and 2 hold, and let x* =
argminx P(x). In addition, assume ηι = η = η = £ > 0 and 1 一 βs 一 α-⅛ ≥ 0, where a is a
constant. Then by setting βs = ^++4, we have
E[P(χS)-P(x*)] ≤ (4(+-ββ)2 (P(xo)-P(x*))+(S+Lɑ)2m kxo-x*k2,
which also means that when we choose m= Θ(n), Algorithm 1 achieves the following oracle com-
plexity in expectation:
P (χo)-P (χ*) +
The result shows that AVR-SExtraGD enjoys the same oracle complexity as Katyusha and MiG,
which is close to the best-known complexity in this case (i.e., O(nlog ɪ + ʌ/nL)). In addition, We
also analyze the convergence of VR-SExtraGD and give and prove the related lemmas and theorems
to guarantee its convergence, which can be found in Appendix D.
O(n
V-
nL∣∣xο-x*k2)

5	Experiments
In this section, we evaluate the performance of AVR-SExtraGD and compare it with its counterparts
including Prox-SVRG and Katyusha on real-world data sets, whose information is shown as Table
2 in Appendix E.
Besides, for these real-world data sets, we consider the two common problem models: Lasso and
Elastic-Net. We also apply our algorithm to face recognition tasks and compare it with the compared
algorithms. Next, we give the setup of the related parameters as follows:
•	Regularization Parameters: The regularization parameters for real-world datasets are shown in
Table 2.
•	The Number of Inner-Iteration: The number of inner-iterations of Katyusha and Prox-SVRG is
usually set as m=2n. Our algorithm adds one more gradient calculation in each inner-iteration than
Prox-SVRG, and for an equal complexity of each epoch, we set m = n in AVR-SExtraGD, so that
in each epoch, all the three algorithms require calculating 3n stochastic gradients. What’s more, the
reasonableness of such a setting can be found in Sebbouh et al. (2019).
•	Step Sizes: We set our step sizes as: ηι =言,η =言.We note that the selected step sizes do not
satisfy the conditions requested in the remark of Section 3. Nevertheless, we can see from the ex-
perimental results that our algorithm still converges well, which means that in practice experiments,
we can choose larger step sizes to improve the convergence speed.
For fair comparison, we implemented all the methods in C++ with a Matlab interface, and performed
all the experiments on a PC with an Intel i7-7700 CPU and 32GB RAM.
5.1 Results of Lasso, Elastic-Net and Logistic Regression
In this part, We consider three common problems, including Lasso, Elastic-Net and the 'ι -norm
regularized logistic regression, whose models can be found in Appendix E.
Figure 1 shows the performance of all the algorithms for Lasso and Elastic-Net on all the data
sets. For running time, our AVR-SExtraGD obviously outperforms Prox-SVRG and Kayusha, which
shows the faster convergence speed of AVR-SExtraGD than Katyusha, and justifies that the extra-
gradient and the momentum acceleration are able to improve Prox-SVRG efficiently. Moreover, for
6
Under review as a conference paper at ICLR 2020
5 0 5
iɔ-'-'
1
(X) Q，( SX) Q
"<3 Prox-SVRG
-A-AVR-SExtraGD
—K— Katyusha
10	15
Running time (sec)
10-15
5 _u
1ff
(X) Q ' (SX) Q
0	5	10	15	20
Running time (sec)
(a) a9a
Prox-SVRG
AVR-SExtraGD
Katyusha
10-3
I
10-2
10-3
10-4
(X1 , ( SX) Q
Prox-SVRG
—Φ-1
AVR-SExtraGD
Katyusha
prθ Prox-SVRG
T-AVR-SExtraGD
―K-Katyusha
(') Q ' (SX) Q
10-3
0	500	1000	1500	2000
Running time (sec)
(c) rcv1
0	50	100	150
Running time (sec)
(b) Covtype


Figure 1:	Comparison of experimental results of different algorithms for Lasso (top) and Elastic-Net
(bottom) problems on different data sets. The y-axis represents the gap between the objective value
and the minimum, and the x-axis corresponds to running time.
the two problems, we propose the asynchronous sparse variant of AVR-SExtraGD by bringing our
algorithm into a sparse asynchronous framework and compare its performance with KroMagnon
(Mania et al. (2015)) and ASAGA (Leblond et al. (2016)) on rcv1 and real-sim, as shown in Table
2. The results are shown in Figure 2, which verify that the asynchronous variant of AVR-SExtraGD
significantly outperforms the variants of SVRG (i.e., KroMagnon) and SAGA (Defazio et al. (2014))
(i.e., ASAGA) in terms of iterations and running time. Then, for a more comprehensive compari-
son, We compare the performance of more algorithms for Lasso and the 'ι -norm regularized logistic
regression on a9a and Covtype, and the results are shown as Figure 4 and Figure 5 in Appendix E.
10-1
W 10-2
10
10
10
10
0	500	1 000	1500	2000	2500
Number of effective passes
-“ AVR-SExtraGD
--I--KroMagnon
一—-ASAGA
-“ AVR-SExtraGD
--I---KroMagnon
一-e.-ASAGA________
,0∙3
Q.
10-4
—“ AVR-SExtraGD
--I—KroMagnon
一—-ASAGA
/23*
OOOO
(*x) Q，&) Q
L *x) Q，A) Q
—“ AVR-SExtraGD
--I—KroMagnon
一∙e∙-ASA3⅜_______
/23*
OOOO
(*x) Q，&) Q
0	1 0	20	30	0	20	40	60	80	0	20	40	60	80	1 00	1 20	0	50	100	1 50
Running time (sec)	Running time (sec)	Running time (sec)	Running time (sec)
(a.1) Lasso, rcv1	(a.2) Lasso, real-sim	(b.1) Elastic-Net, rcv1	(b.2) Elastic-Net, real-sim
Figure 2:	Comparison of experimental results of different algorithms for Lasso (the first tWo
columns) and Elastic-Net (the latter tWo columns) problems on different data sets. The y-axis repre-
sents the gap of objective value, and the x-axis corresponds to the number of effective passes (top)
or running time (bottom).
5.2	Results on Face Recognition
We also apply our AVR-SExtraGD as Well as Prox-SVRG and Katyusha to robust face recogni-
tion via sparse representation (John et al. (2009)) on the AR and Yale. We set the loss function
7
Under review as a conference paper at ICLR 2020
in the training process as the same function as the Lasso and Elastic-Net problems. For approxi-
mately equal time, the number of outer loops is 200 for Prox-SVRG and AVR-SExtraGD, and 50
for Katyusha. In order to compare the results reasonably, we implement all the algorithms for 20
times and get the average and standard deviation of recognition rates, as shown in Table 1.
Table 1: Comparison of Recognition Rates on the AR and Yale Datasets.
Problems	Algorithms	AR	Yale
Lasso	Prox-SVRG Katyusha AVR-SExtraGD	-0.6000 ± 0.0648- 0.5880 ± 0.0832 0.6200 ± 0.0756	-0.6200 ± 0.0447- 0.7480 ± 0.0415 0.8100 ± 0.0265
Elastic-Net	Prox-SVRG Katyusha AVR-SExtraGD	-0.5630 ± 0.0580- 0.5560 ± 0.0738 05800 ± 00634	-0.6400 ± 0.0394- 0.6870 ± 0.0433 0.7140 ± 0.0297
The results in Table 1 show that the recognition rate of AVR-SExtraGD is significantly higher than
other algorithms on both the AR and Yale data sets. This means that our AVR-SExtraGD can learn
a more efficient representation for face recognition.
5.3	THE SELECTION OF K IN AVR-SEXTRAGD
For the selection of K , we do some relevant experiments as examples. We choose different K to
solve Lasso problem by our algorithm, and get the results as shown in Figure 3.
(a.1) a9a
(a.2) a9a
(b.1) Covtype
(b.2) Covtype
Figure 3:	Comparison of experimental results about different choices of K in AVR-SExtraGD for
Lasso on different data sets. The y-axis represents the gap of objective value, and the x-axis corre-
sponds to the number of effective passes ((a.1) and (b.1)) or running time ((a.2) and (b.2)).
where, K = n means that the extragradient is calculated every integer multiple of n (n ∈
{1, 8, 25, 75, 250}). For a9a, when the extragradient is used every time, the function value decreases
faster with respect to the number of iterations, but the result is not good for running time. Thus, for
a9a, we choose K=25. As for Covtype, obviously, K= 1 is the best choice.
6 Conclusions and Future Work
In this paper, we mainly considered the non-smooth optimization problem in large-scale and high-
dimensional settings. By introducing the idea of extragradient and momentum acceleration, we
improved the classical Prox-SVRG and then proposed a novel algorithm, called AVR-SExtraGD.
From our theoretical analysis, we can know that AVR-SExtraGD attains linear convergence for SC
problems, and achieves the same oracle complexity as Katyusha, which is the best-known one of
stochastic first-order algorithms in both SC and non-SC cases. Finally, the experimental results
showed that AVR-SExtraGD improved the result of the gap of the optimal value introduced by
proximal operator, and thus improved the accuracy of solutions and convergence speed, which con-
firmed the efficiency of extragradient and momentum acceleration. For future work, we can extend
the ideas introduced in this paper to many existing proximal algorithms, including Prox-AFG (Beck
& Teboulle (2009)), Prox-SAG (Schmidt et al. (2017)) and Prox-SDCA (Shalev-Shwartz & Tong
(2012); Shalev-shwartz & Tong (2014)) which is a proximal variant of SDCA (Shalev-Shwartz &
Tong (2013)), and it will certainly improve the performance of these algorithms. Moreover, we can
also rewrite our algorithm into the form of mini-batch, whose computation of gradient evaluations
can be parallelized (Agarwal & Duchi (2011); Dekel et al. (2012)).
8
Under review as a conference paper at ICLR 2020
References
A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Decision and Control,
2011.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. Siam J Imaging Sciences, 2(1):183-202, 2009.
Doron Blatt, Alfred O. Hero, and Hillel Gauchman. A convergent incremental gradient method with
a constant step size. Siam Journal on Optimization, 18(1):29-51, 2007.
Aaron Defazio, Francis Bach, and Simon Lacostejulien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In International Conference on Neu-
ral Information Processing Systems, 2014.
Ofer Dekel, Gilad Bachrach Ran, Ohad Shamir, and Xiao Lin. Optimal distributed online prediction
using mini-batches. Journal of Machine Learning Research, 13(1):165-202, 2012.
Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via finite element approximation. Computers and Mathematics with Applications, 2(1):
17-40, 1976.
Wright John, Allen Y Yang, Ganesh Arvind, Sastry S Shankar, and Ma Yi. Robust face recognition
via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31
(2):210-227, 2009.
R. Johnson and Zhang Tong. Accelerating stochastic gradient descent using predictive variance
reduction. In International Conference on Neural Information Processing Systems, 2013.
Zhou Kaiwen, Shang Fanhua, and Cheng James. A simple stochastic variance reduced algorithm
with fast convergence rates. ICML, 2018.
Jakub Konecny and Peter Richtarik. Semi-stochastic gradient descent methods. Mathematics, 3:9-,
2013.
Jakub Konecny, LiU Jie, Peter Richtarik, and Martin Taka. ms2gd: Mini-batch semi-stochastic
gradient descent in the proximal setting. IEEE Journal of Selected Topics in Signal Processing,
10(2):242-255, 2014.
G. M Korpelevic. An extragradient method for finding saddle points and for other problems. Mate-
con, 12:747-75, 1976.
John Langford, Li Lihong, and Zhang Tong. Sparse online learning via truncated gradient. Journal
of Machine Learning Research, 10(2):777-801, 2009.
Remi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. Asaga: Asynchronous parallel saga,
2016.
Xiao Lin and Zhang Tong. A proximal stochastic gradient method with progressive variance reduc-
tion. Siam Journal on Optimization, 24(4), 2014.
P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. Siam
Journal on Numerical Analysis, 16(6):964-979, 1979.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR
2017 (5th International Conference on Learning Representations), 2016.
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and
Michael I. Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. Mathe-
matics, 18(1):51-52, 2015.
Trong Phong Nguyen, Edouard Pauwels, Emile Richard, and Bruce W. Suter. Extragradient method
in optimization: Convergence and complexity. Journal of Optimization Theory and Applications,
176(1):1-26, 2017.
9
Under review as a conference paper at ICLR 2020
A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. In International
Conference on Neural Information Processing Systems, 2014.
R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, 1970.
Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponen-
tial convergence rate for finite training sets. In International Conference on Neural Information
Processing Systems, 2012.
Zhang Ruiliang, Zheng Shuai, and Kwok James T. Asynchronous distributed semi-stochastic gradi-
ent optimization. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
EmestK.RyuandYinWotao. Proximal-proximal-gradient method. UCLA CAMReport,pp. 17-51,
2017.
Mark Schmidt, Nicholas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(5):1-30, 2017.
Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis Bach, and Robert M. Gower. To-
wards closing the gap between the theory and practice of svrg, 2019.
Shai Shalev-Shwartz and Zhang Tong. Proximal stochastic dual coordinate ascent. Mathematics,
2012.
Shai Shalev-Shwartz and Zhang Tong. Accelerated mini-batch stochastic dual coordinate ascent.
Advances in Neural Information Processing Systems, pp. 378-385, 2013.
Shai Shalev-shwartz and Zhang Tong. Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. In International Conference on International Conference on Ma-
chine Learning, 2014.
Zhang Tong. Solving large scale linear prediction problems using stochastic gradient descent algo-
rithms. In International Conference on Machine Learning Omnipress, 2004.
P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Siam Journal
on Optimization, 2008.
Allen-Zhu Zeyuan. Katyusha: the first direct acceleration of stochastic gradient methods. STOC,
pp. 1200-1205, 2017.
Allen-Zhu Zeyuan and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex
objectives. ICML, pp. 1080-1089, 2016.
10
Under review as a conference paper at ICLR 2020
Appendix
A The Error of Optimal Value
In this part, we prove that Prox-FG will cause the deviation of the optimal value. Based on its update
rules, we can explain why Problem (1) can be solved by proximal operators and find out the reason
for the introduction of the error. According to Prox-FG, we have
Xk = Proxnk(Xk-1 - ηkNF(xk-ι))
=arg min {R(u) + 贵 ∣∣u - (xk-i - ηk VF (xk-ι))∣∣2}
=argmin {R(u) + n^∣∣VF(xk-ι)∣∣2 + VF(xk-i)T(u - Xk-i) + *Ilu - Xk-ι∣∣2}
arg min {R(u) + F (Xk-I) + VF (Xk-I)T (u — Xk-i) + / Ilu — Xk-Ik2}
≈ arg min {F (u) + R(u)}.
u∈Rd
The final approximation is obtained by the second-order Taylor expansion of F(u) at Xk-i. From
the above analysis, we can see that when using proximal operators to solve the problems with the
`i -norm regularization, the iterate Xk in each iteration is an estimation of the optimal point, not the
real optimal point. Therefore, in the last iteration, the final output point is also an estimation of P*,
which results in the deviation of the optimal value.
B Proofs of AVR-SExtraGD
B.1 Key Lemmas
Lemma 1. If two vectors Xk, Xk-i ∈Rd satisfy the following equality,
Xk = argmin{ 2⅛ kX-Xk-ik2+% F (Xk-I)，x〉+r(x)}
with a constant vector Vik F (Xk-I) and a convex function R(∙), thenfor ∀u ∈ Rd, we have
hGk F(Xk-I) , Xk-ui ≤ - 2n IlXk-1-Xk k2+2n kXk-1 -uk2- 2⅛ kXk-uk2+r(U)-R(Xk).
Moreover, if R(∙) is μ-strongly convex, the above inequality becomes
hvik F (Xk-i),Xk - ui≤-21n E-1-a k2+卷 kXk-1 -uk2- 1+nμ |Xk-uk2 ”⑻-R由).
Lemma 2 (Variance Bound). Suppose each function fi(∙) is Li-smooth, let VikF(Xk-I)
VikF(xk-i) - VikF(Xs-1) + VF(Xs-i), which is the gradient estimation operator used in Algo-
rithm 1. Then the following inequality holds
EkVF(Xk-i)-VikF(Xk-ι)k2 ≤ 2L(F(XsT)-F(Xk-I)-"F(Xk-1), XsT-Xk-ii).
The detailed proof of Lemma 1 can be found in Kaiwen et al. (2018), and the proof of Lemma 2 can
be seen in Zeyuan (2017), and thus we omit the proofs here. Next, we give the proofs of Theorems
1 and 2.
11
Under review as a conference paper at ICLR 2020
B.2 Proof OF Theorem 1
Proof. In this part, we consider one particular epoch and omit the number of outer iteration S (except
XST and Xs). We assume the parameters η and β satisfy the following inequality,
Lβ + P ≤ 1.
1—β η
(4)
Let Xk-ι = βχk-ι+(1-β)Xs-1, Xk-1/2 = βχk-1∕2+(l—β)Xs-1. Thus, we can obtain Xk-1/2―^k-ι
β(Xk—1/2 —Xk-1). Thus, according to the L-smoothness of F(∙), we can obtain
P(^k-1∕2) ≤βR(xk-1∕2) + (l—β)R(逆-1)+F(^k-ι) + h%F(^k-ι),β(Xk-1/2 —Xk-1 ))
+-2^ IIXk-1/2 -Xk-1 Il2 + WF (企k-1)—&k F (企k-1), β(xk-1/2 —xk-1)i.
Then by using (4), we have
1 ―	〜	、1 —β______ I、 1 一.	、，二一.	、	、
万P(Xk-1/2) ≤ R(Xk-1/2)+ ^-R(X )+ WF(Xk-I)+hvikF(Xk-1), Xk-1/2 -Xk-I)
β β β
+2η IlXk-1/2-Xk-1 Ii2 - 2(1 一 β) IlXk-1/2-Xk-11∣2
，《->->—ɪ / A	∖	T-I / A	\	\
+〈VF (Xk-I)一 %k F (Xk-I), Xk-1/2 — Xk-1).
According to Lemma 1 with Xk-1, Xk = Xk-1/2, U = x* and using the Young,s inequality to expand
(VF(xk-1) — VikF(χk-1), Xk-1/2 — Xk-1) with the parameter θ > 0 and taking expectation with
respect to the sample ik, we have
1____ 、〜、1 — β____________ 1、	1	.	二
万EP(Xk-1/2)≤ R(X*)+-^-R(X	) + 万F(Xk-1)+ EhVik F(Xk-1), χ* — χk-1i
β	/	β	β
1 1 Ii	∣∣2	1+ημm∣	Il 2 Lβ m∣	∣∣2
+ 2η ∣∣x* —Xk-1 Il-2η- E∣∣X* — Xk-1/2|| - 2(1一 β) EkXk-1/2 —Xk-1∣∣
+ 2EIIXk-1/2 -Xk-1∣∣2 + ^2θEllVF(Xk-I)-Vik F(Xk-1)∣∣2.
12
Under review as a conference paper at ICLR 2020
We set θ = Le > 0 and apply Lemma 2, then
万 EP (Xk-1/2)≤ R(χ*)+-7Γ-R(χs-1)÷ 万 F(Xk-I)
β	/	P	p
÷而 kx* - Xk-1∣∣2----2^~ Ek x* -Xk-1/2 ∣∣2 ÷	p~ [F (金S 1) -F (企k-1)]
÷ Ehqk F (企k-1), x* +-7T-^s 1 -万金k-1 +-(Γ~-1k-l —^S D)
PPP
≤ R(X*)+-^R(XS-1)÷-F(Xk-I)
PP
÷7Γ~ IIx* - Xk-1∣∣2-Ellx* -Xk-1/2 ∣∣2 +-J[F (逆-1) -F (企k-1)]
2η	2η	p
1 一二	,一…	„
÷ 后EhMk F(Nk-1), pX* ÷(1 -P)Nk-I -xk-1i
p
≤ R(X*)÷—^^R(*s-1)÷-F (企k-1)
pp
÷2ηk X* Xk-1k2-ɪ Ekx* -Xk-1/2 ∣∣2 ÷	p— [F(逆T) -F(企k-1)]
÷ 万F(pX* ÷(1 - P)企k-1)-万F(企k-1)
pp
≤ R(x*)÷1PpR3s-1)÷ 1 F(^k-1)÷1pβ [F(逆 T)-F (^k-1)]÷ F (x*)
÷-7T^f (企k-1)一万 F (企k-1)÷ʒ- ∣∣x* -Xk-1∣∣2-LWEkx* - Xk-1/2k2
PP	2η	2η
=1—pP3s-1)÷P(X* )÷; ∣X*-Xk-1k2- 1÷μ E∣X* - Xk-1/2k2.
P	2η	2η
The third inequality holds due to E[V^F(^k-1)] = VF(^k-1) and the convexity of F(∙), then we
get
1 E[P(ik-1/2)-P(x*)] ≤ F[P(≡s-1)-P(x*)"；∣X*-Xk-1k2-⅛μEkX*-Xk-1/2k2.
P	P	2η	2η
Moreover, because Xk-1 = PXk-1 ÷(1-P)Xs-1, we can obtain Xk = PXk÷(1 -P)XS-1. Thus, it is
not hard to know
1 E[P (Xk )-P(x*)] ≤ 彳[P (Xs-1)-P (X*)]÷2η kx* -Xk-1/2k2 -号 Ekx* - Xk k2.
We set yk = P xk-122+k ÷(1-P)Xs-1 and it is obvious that 击-1+nμ ≤ 0, then we have
1 E[P(yk)-P(x*)] ≤ 宁[P(Xs-1)-P(x*)]÷4ηkx*-Xk-1k2- 1÷ημE∣x*-Xk∣2.	(5)
13
Under review as a conference paper at ICLR 2020
By setting P =1+ημ and summing (5) over k =1,…，m with increasing weight ρk-1,we have
1 m	ρm	1β	m	1
方 EPkTE[P(yk)-P(x*)]+P-∣∣xm-x*k2≤ T £ρk-1 [P(Xs-1)-P(x*)]+4-∣∣x0-x*∣∣2.
β k=1	4η	β k=1	4η
Because, for SC problems, we set Xs = (Pm=I Pk-I)-I Pm=I Pk-1yk in Algorithm 1, we have
1 m	Pm	1 β m	1
β∙χ ρk-1E[P(Xs)-P(X*)]+ρη kxm-χ*k2≤ T X ρk-1[p(χs-ι)-p(χ*)]+4η kχ0-χ*k2
Then, according to the convergence analysis for SC problems in (Kaiwen et al. (2018)), we can get
a similar result. That is, for the case with 节 ≤ 3, we set η = J3Lnβ, β = Pm≤ 1, and m = Θ(n),
then we can obtain
E[P(Xs)-P(x*)] ≤(0(1 + r3nK)!	∙O(P(X0)-P(x*))∙
We note that X0 = xo, So we get
E[P(Xs)-P(x*)] ≤(0(1 + r3nK)!	∙O(P(Xo)-P(x*)),
which implies that the oracle complexity in this case to achieve an -additive error is
Ο(√κnlog P(XO-P(χ"). However, for the case with £ > 3, we set η = 3L,β = 11 and m = Θ(n),
then we can obtain
E[P(Xs)-P(x*)] ≤ (2)S ∙O(P(X0)-P(x*)).
We know that X0 = x0, so we have
E[P(Xs)-P(x*)] ≤ (2) ∙O(P(xο)-P(x*)),
which implies that the oracle complexity of AVR-SExtraGD in this case is O (nlog P(XO)-P(x*)).
□
B.3 Proof of Theorem 2
Proof. In this part, we also omit the number of outer iteration S (except Xs-1 and Xs). Due to
Xk-1/2 = βXk-1∕2 + (1 -β)Xs-1, we can get
P (Xk-1/2)= P (βXk-1∕2 + (1 -β)Xs-1) = R(βXk-1∕1 + (1 -β)Xs-1) + F (Xk-1/2).
14
Under review as a conference paper at ICLR 2020
From the convexity of R(∙) and L-Smoothness of F(∙), We obtain
P(Xk-1/2) ≤βR(xk-1∕2) + (l-β)R(Xs-1)+F(Xk-1) + h%F(Xk-1),β(xk-1∕2-Xk-1 )i
+-2^- IIXk-1/2 -Xk-1k 2 + BF(Xk-I)-Gk F(Xk-I), β(xk-1∕2 -Xk-I)i
≤ eR(Xk-1/2) + (1 -e)R(XS-I)+ F(Xk-I)+〈GkF(Xk-I), β(Xk-1/2 -Xk-1 )i
+ 字 kXk-1/2 - Xk-1k2 + 9f∕ n kVF (Xk-I)-Gk F (Xk-I)k2,
2	2L(α - 1)
Where the second inequality holds by using the Young’s inequality With the parameter L(α - 1),
Where α is a small constant. After applying Lemma 1 and taking expectation With respect to the
15
Under review as a conference paper at ICLR 2020
sample ik, we have
EP(xk-1/2)≤ βR(x*)÷(l — β)R(xs-1)+F(Xk-1)+ EhVifc F(Xk-1), β(x* -xk-1))
H--2-(kx*-xk-1k2 -E∣∣x* -xk-1∕2∣∣2)
1	_____. 、二一
⅛7~~π EkVF (Xk-I)-X⅛ F (Xk-I)k2
2L(α-1)
≤	βR(χ*)÷(1-β)R(Xs-1)÷F (Xk-ι)÷ Εhq F (Xk-I),β(χ*-Xk-ι)i
÷	2^-(IIx* -Xk-Ik2 -Ekx* -Xk-1/2k2)
÷	工[F (XST)-F (Xk-I)÷EhV⅛ F (Xk-I),Xk-ι-Xs-1i]
α-1
≤	eR(X*) + (1 -e)R(XS-1)÷F(Xk-1)÷--[F(XS-I)-F(Xk-1)]
α-1
÷EhVik F(Xk-1), βx* ÷(1 -e)Xs-1 -Xk-1 ÷-r(Xk-1 -XS-1)i
Q — 1
÷	2^-(∣∣χ*-χk-ι ∣∣2 -Ekχ*-χk-ι∕2∣ι2)
≤	βR(χ*)÷(1 -e)R(XS-I) ÷F(Xk-I)H--r[F(XS-I) -F(Xk-1)]
Q-1
÷f(ex* ÷(1 -e-----^)XS-I ÷---^Xk-I)-F(Xk-I)
Q-1	Q — 1
÷—2^^(∣∣χ*-χk-ιk2 -El∣χ*-χk-ι∕2k2)
≤ βR(χ*)÷(1 -e)R(XS 1)÷F(Xk-I)H----r[F(XS I)-F(Xk-1)]
Q-1
÷βF(χ*)÷(I -β------)F(XS-I)H------F(Xk-I) -F(Xk-I)
Q-1	Q-1
H	2^-(llχ* -χk-11∣2 -Ekχ* -χk-1∕2∣∣2)
= (1-β)P (XST)-βP(χ* )÷ LOe2 (kχ*-χk-ik2-E∣∣χ*-Xk-i∕2k2)∙
The second inequality holds due to Lemma 2. The reasons why the fourth inequality holds are
E[MkF(χk-ι)] = VF(χk-ι) and the convexity of F(∙). Besides, we need to assume 1-β-告 ≥ 0
in this step. Then, we get
E[P(χk-1∕2) -P(X*)] ≤ (1 -β)[P(XS-I)-P(X*)]H-2-(kχ* -χk-1k2 - Ekχ*-χk-1∕2∣∣2)∙
16
Under review as a conference paper at ICLR 2020
Moreover, because Xk-ι = βχk-ι + (1-β)Xs-1, We can obtain Xk = βχk + (1 -β)Xs-1. Thus, it is
not hard to know
E[P(Xk)-P(x*)] ≤ (1-β)[P(Xs-1)-P(x*)] + Lα2β2(∣∣x*-xk-1∕2k2-Ekx*-Xkk2).
Let yk = βxk-122+xk +(1-β)Xs-1, we have
E[P(yk)-P(x*)] ≤ (1-β)[P(Xs-1)-P(x*)] + *(∣∣x*-xιk2-E∣∣x*-Xkk2).
That is,
-12 E[P(yk)- P(χ*)] ≤ ¾β)[P (Xs-1)-P(χ*)] + La (kx*-Xk-1k2-Ekx*-Xk k2).
β2	β2	4
Since we have Xs = mβ Pm=I xk-122+xk +(1 -β)Xs-1 =m Pm=I yk in Algorithm 1, and then by
summing the previous inequality over k= 1, ..., m and according to Xs0+1 =Xsm , we obtain
1b E[P(Xs)-P(X*)] ≤ ⅛βs⅛(XsT)-P(X*)]+Lα (kX* - X0k2-kX* - X0+1k2).
βs	βs	4m
We set βs = s+4 and can easily obtain ^^1- ≥ 1-βs. Then by summing the previous inequality over
s= 1, ..., S, we can get
12E E[P(XS )-P(x*)] ≤ ⅛β^[P (Xs-1)-P (X*)]+La (kX*-X0k2-kX*-Xi□2).
βS	β1	4m
Then we have
E[P (XS )-P(X*)] ≤ (4(+-ββ)2 (P (xo)-p (x*))+(s+α)2m kXo - X*k2,
which holds because ∣∣x* -Xmk2 ≥0. We note that X0 = x⅛ = xo, so we get
E[P(Xs)-P(x*)] ≤ (S+-ββ2 (P(χ0)-P(χ*)) + (SL⅛kχ0-χ*k2.
In other words, by choosing m = Θ(n), the total oracle complexity is
O
P (XO)-P (x*)
+

nLkX0 - X* k2

□
Finally, we finished the convergence analysis of AVR-SExtraGD. In our Algorithm 1, only part of
the iterations are updated by extragradient descent, and the other iterations are updated with the
update rules of MiG. And we know both MiG and AVR-SExtraGD can make the objective function
converge to P* . Therefore, the method in Algorithm 1 will not affect the results and can accelerate
the algorithm.
17
Under review as a conference paper at ICLR 2020
Algorithm 2 VR-SExtraGD
Input: Initial vector x°, the number of epochs S, the number of iterations m per epoch, and the
step sizes η1, η2.
Initialize: X0 = x0.
1:	for s = 1, 2, . . . , S do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
μs-1=VF (xs-1);
X0 = XsT (SC) or X0 = Xsm-1 (non-SC);
for k = 1 , 2, . . . , m do
Pick ik uniformly at random from {1, ..., n};
Vfik (Xk-I)=Vfik (xk-1)-Vfik (XsT)+μs -1;
xk-1∕2=ProXRI (Xk-I- ηιvfik (Xk-I));
Vfik (Xk-1∕2)=Vfik (Xk-1/2)-Vfik (XsT)+μs-1;
Xk=PrOXR2 (Xk-1∕2-Bfik (Xk-1/2));
end for
ss — ɪ Pm Ts∙
X = m 乙k = 1 Xk;
12:
end for
Output: Xs .
C SExtraGD and VR-SExtraGD
Based on the update rules of EEG, we first consider and propose the stochastic variant of the algo-
rithm, namely the stochastic eXtragradient descent (SEXtraGD) algorithm, and its variance reduced
variant called variance reduced stochastic extragradient descent (VR-SEXtraGD), whose update
rules can be formulated as follows:
•	The update rules of SEXtraGD:
Xk-1/2 = ProxR1 (Xk-1 - ηιVfik (Xk-1)); Xk = ProxR2 (Xk-1/2 — η2Vfik (Xk-1∕2)).
•	The update rules of VR-SEXtraGD:
Xk-1/2 = PrOXRI (Xk-I- η1Vfik(Xk-I)) ； Xk = ProxR2 (Xk-1/2 — η2Vf⅛(Xk-1/2))
where Vfik (∙) is the gradient estimation defined in (3).
We note that one difference between these two algorithms and EEG is that we change Xk-1 in
Step two to Xk-1∕2, which will be beneficial to our theoretical analysis, but will not cause any
major change to the results of the algorithms. Thus, we propose the stochastic eXtragradient descent
algorithms, called SEXtraGD and VR-SEXtraGD, for solving non-smooth (both SC and non-SC)
problems. In addition, the detailed process of VR-SEXtraGD is shown as outlined in Algorithm 2.
D Convergence Analysis of VR-SExtraGD
Firstly, we give some key lemmas which are helpful for the convergence analysis of VR-SExtraGD.
Lemmas 3, 4 and 5 are used to prove Lemma 6 which is an important lemma to prove the conver-
gence of VR-SExtraGD.
Lemma 3. Let R(∙) be a ConvexfUnctionfrom Rd to R, and ηk > 0. Then, for all x, y ∈ Rd,
kProxηRk(X) -ProxηRk(y)k ≤ kX -yk.
18
Under review as a conference paper at ICLR 2020
Lemma 4. Let P(X) = F(x)+R(x), and VF(x) is Lipschitz continuous with parameter L. Forany
x ∈ Rd and arbitrary v ∈ Rd, we define
x = ProxR(X — ηv), g = L(X — x0), 4 = V — VF (x),
where η is a step size that satisfies 0 < η ≤ 1. Thus, we can know thatfor any y ∈ Rd,
P(y) ≥ P(χ0) + gT(y — χ) + 2kgk2 + 4T(χ0 - y).
Lemma 5. Considering P(X) as defined in Problem (1) and Vfik (v) as defined in (3), where v is
an arbitrary stochastic sample, and let x* = argmιnχ P(x). We have E[Vfik (v)] = VF(V) and
EkVfik(V)- VF(v)k2 ≤ 4L[P(v) - P(x*) + P(X)- P(x*)].
Because Lemma 3 is well known and often used (e.g., see Section 3 in Rockafellar (1970)), we omit
the proof of this lemma here. For Lemma 4, it is very similar to Lemma 3.7 of Lin & Tong (2014),
and thus can be easily proved, so we also omit the proof here. Similarly, according to Lemma 3.4 in
Lin & Tong (2014), Lemma 5 can be also easily proved, and thus we will not give the detail about
it. Then we can prove the following lemma by these three lemmas.
Lemma 6. For all randomly selected sample V and h, if
v = ProXR (h - nV fik (h)) ,	(6)
we have
EkV - x* k2 ≤ ∣∣h - x*k2 - 2n[EP(v) - P(x*)] +8Ln2[P(h) - P(x*) + P(X) - P(x*)],
where X* = arg minx P (X).
Proof. First, we define a stochastic gradient mapping as follows:
g = 1(h - V) = 1(h - ProxR (h - nV £ (h)).
According to this definition, (6) can be expressed more succinctly as follows:
V = h - ng.
Then we consider the distance between V and X*.
kV - X*k2 = kh - ng - X*k2 = kh - X*k2 - 2ngT(h - X*) + n2kgk2.
19
Under review as a conference paper at ICLR 2020
Applying Lemma4 With X = h, V = Vfik (h), x+ = V and y = x*,we have
-gτ(h — x*) + 2l∣gk2 ≤ P(Xj- P(V)- 4T(V — χ*)
where 4k = Vf⅛(h) — VF(h). Thus We have
∣∣v一 χ*∣∣2 ≤ ∣∣h一 χ*∣∣2 - 2η[P(v)- P(χ*)] - 2η4τ(V - X*).
Then We can give an upper bound of -2η4T(V — x*). First of all, we can define the update of
Prox-FG as shown below (although it is not used in our algorithm):
h = ProxR(h - ηVF(h)).
So, we can obtain
-2η4τ(v - x*) = -2η4T(v - h) - 2η4T(h - x*)
≤ 2η∣4kk∣V - h∣- 2η4T(h - X*)
≤ 2η∣∣4k∣∣∣∣(h -ηv加S))- (h - ηvFS))Il- 2η4T(h - XJ
=2η2∣4k∣∣2 - 2η4T(h - x*).
The first inequality follows from the Cauchy-Schwarz inequality, and the second inequality holds
due to Lemma 3. So, we have
Ilh — χ*∣∣2 ≤ IlV — χ*∣∣2 — 2η[P(h) - P(x*)] + 2η2∣4k∣2 — 2η4T(h — x*).
Then, we take expectation on both sides of the above inequality with respect to ik to obtain
Ekh — χ*∣ ≤ ∣v — χ*∣2 — 2η[EP(h) — P(x*)] + 2η2E∣4k∣∣2 — 2ηE[4T(h — x*)].	(7)
It can be noted that both h and x* are independent of the random variable ik, and we can easily
know that E4k = 0. So
E4T(h - x*)] = (E4k)τ(h - x*)=0.	(8)
Substituting Lemma 5 and (8) into (7), we obtain
Ekh - x*k2	≤	IlV	- x*∣∣2 -	2η[EP(h)	- P(x*)] +	8Lη2[P(v)	- P(x*)+ P(x)	- P(x*)].	(9)
□
20
Under review as a conference paper at ICLR 2020
D.1 For SC Problems
Firstly, we can prove the convergence of VR-SExtraGD for strongly convex (SC) problems, which
is showed by the following theorem.
Theorem 3 (Strongly Convex). Suppose that Assumptions 1, 2 and 3 hold, and let x* =
arg minx P (x). In addition, assume η1 > 0, η2 > 0, η1 ≥ 4Lη22 and η2 ≥ 4Lη12 and m is suf-
ficiently large so that
θ =	1	+ 4L[(m +I)η2 + mη2] ≤ 1
μm(η — 4Lη2)	m(η2 — 4Lη2)	一
Then VR-SExtraGD outlined in Algorithm 2 achieves linear convergence in the expected form, which
can be formulated as follows:
E[P(Xs) — P(x*)] ≤ θs[P(xo) — P(x*)].	(10)
Proof. According to Lemma 6 and the update rules of VR-SExtraGD, we can easily get:
Ekxk-1/2 — x* k2 ≤ kxk-1 — x* k2 — 2η1[EP (xk-1/2) — P(x*)]
(11)
+ 8Lη2[P(xk-1) — P(x*) + P(X)- P(x*)]
and
Ekxk — x* k2 ≤ kxk-1/2 — x* k2 — 2η2 [EP(xk) — P(x*)]
(12)
+ 8Lη2[P (Xk-1/2) — P(x*) + P (X) — P(x*)].
Then, we substitute (11) into (12) to obtain
EkXk — X* k2 ≤ kXk-1 — X* k2 — 2η1 [EP (Xk-1/2 ) — P(X* )]
+ 8Lη2[P(Xk-I)- P(x*) + P(X) — P(x*)] — 2η2[EP(xk) — P(x*)]
+ 8Lη2 [P(xk-1/2) — P(X*) + P(X)- P(X*)].
Suppose ηι ≥ 4Lη2, i.e., —2m ≤ — 8Lη2, We can get
EllXk — x* k2 ≤ ||Xk-1 — x* k2 — 8Lη2 [EP(Xk- 1 ) — P(X*)]
+ 8Lη2[P(Xk-I)- P(x*) + P(X) — P(x*)] — 2η2[EP(Xk) — P(x*)]
+ 8加2[。(Xk-1) - P(%)]+8幽2[。(X)- P(x*)]
=∣Xk-ι — χ*k2 + 8Lη2[P (χk-ι) — P (χ*) + P (X) — P (χ*)]
—2η2[EP (χk) — P (χ*)]+8Lη2[P (X) — P (χ*)].
21
Under review as a conference paper at ICLR 2020
By summing the previous inequality over k = 1, ..., m, we obtain
m-1
IIxm - x*k2 + 2η2[EP(xm) - P(x*)] + 2(η2 - 4Lη1) X [EP(Xk)- P(x*)]
k=1
≤ kxo - x*k2 + 8Lη1 [P(xo) - P(x*)] + m8L(η1 + η2)[P(x) - P(x*)].
Since η - 4Lη2 < η2, and from the algorithm of VR-SExtraGD, We know xo = X. Therefore,
m
2(η2 - 4Lη2) X[EP(Xk) - P(x*)] ≤ ∣∣X - x*∣∣2 + 8L[(m + 1)η2 + mη∣][P(X)- P(x*)].
k=1
Because in a fixed epoch, such as the s-th epoch, there are Xs = mm Pm=I Xk and Xs-1 = xo, and
according to the convexity of P(∙), P(Xs) ≤ mm Pm=I P(Xk) can be obtained. Therefore,
2(η2-4Lη2)m[EP(Xs)-P(x*)] ≤ 8L[(m+1)η2 +mη2][P(Xs-1)-P(X*)] + kXs-1-x*∣∣2.(13)
Because of the convexity of F(∙) and the strong convexity of R(∙), we know P(∙) is also strongly
convex, then we have ∣∣Xs-1 - x*∣2 ≤ 2 [P(Xs-1) - P(x*)]. Thus,
2
2(η2 - 4Lη2)m[EP(x ) - P(x*)] ≤ (- + 8L((m + 1)η2 + mη2))[P(x	) - P(x*)],
μ
which is equivalent to
E[P(Xs) - P(χ*)] ≤ θ[P(Xs-1) - P(χ*)],
where
1	1	4L[(m + 1)η2 + mη2]
μm(η2 — 4Lη2)	m(η2 — 4Lη2)
At last, we have
E[P(Xs) - P(χ*)] ≤ θE[P(XST) - P(χ*)]
≤ θ2E[P(Xs-2) - P(χ*)]
≤ ... ≤ θs[P(Xo) - P(x*)].
We note that X0 = χo, so we can obtain
E[P(Xs) - P(χ*)] ≤ θs[P(χo) - P(χ*)]
□
22
Under review as a conference paper at ICLR 2020
D.2 For Non-SC Problems
We can also use a theorem to give the convergence of VR-SExtraGD for solving non-SC problems,
as shown below.
Theorem 4 (Non-Strongly Convex). Suppose that Assumptions 1 and 2 hold, and let x* =
argminx P(x). In addition, assume ηι > 0,η2 > 0, and ηι = η2 = η = La∙ Then, the conver-
gence property of VR-SExtraGD, as outlined in Algorithm 2, is given as follows:
E[P(χout) -P(x*)] ≤ m4m+2)s[P(χo) - P(x*)] + /，二；Skx0 - χ*k2.	(14)
where Xout = S PS=I XS.
Proof. Because We know F(∙) is L-Smooth, then We have
T->f	∖	.-	7^J∕	∖	.	7^^f∕	∖	.	∕√⅛ Cl	∖	∖
P(xk-1/2) ≤ R(Xk- 1/2)+ F(Xk-I) + Wfik (Xk-I), xk-1/2 - xk-1i
L
+ £ ||Xk-1/2 - Xk-IIl + Ef (Xk-I) - Vfik (Xk-I),Xk-l/2 - Xk-1i
L
≤ R(Xk-1/2)+ F(Xk-I) + hVfik (Xk-1), Xk-i/2 - Xk-1i + £ IlXk-1/2 - Xk-Ill
+ kJ U kVF (Xk-ι) - V fik (Xk-1)k2 + Lm I- I) kXk-1/2 -Xk-1k2.
2L(a — 1)	2
The second inequality holds due to Young’s inequality with parameter L(α- 1), where αis a small
constant. Then after taking expectation with respect to the sample ik and using Lemma 5, we obtain
τττ>Γ rɔ/	M .- Ti/	∖ . τ^f∕	∖ . πτ> ∕τr7 r /	∖	∖
E[P (Xk-1/2)] ≤ R(Xk-1/2) + F(Xk-1) + EhVfik (Xk-1), Xk-1/2 - Xk-1i
+--EkXk-1/2 - Xk-1k2 +----7[P(Xk-I) - P(x*) + P(X) - P(x*)]∙
£	α - 1
Next, we apply Lemma 1 with Xk-1, Xk = Xk-1/2, u = X*, and have
E[P(Xk-1/2)] ≤ R(X*) + F (Xk-1) + EhVfik (Xk-1), X* - Xk-1i
+--2^(kx* - xk-1 k2 - Ekχ* - Xk-1/2 k2)
£
+------- [P(χk-ι) — P(χ*) + P (χ) — P (χ*)]
α- 1
≤ R(X*) + F(X*) +---2^(kx* - xk-1 k2 - Ekχ* - Xk-1/2 k2)
£
+------ [P (χk-ι) — P(χ*) + P (χ) — P (χ*)].
α- 1
23
Under review as a conference paper at ICLR 2020
The second inequality holds because E[V∕ifc(xk—1)] = VF(χk-ι) and F(∙) is convex. Then we
know
____	. 一— 2	—	.	_ ..———
E[P(Xk-1/2) - P(x*)] ≤ --[P(xk-1) - P(x*) + P(X)- P(x*)]
a — 1
+ ^2-(kx* - Xk-1Il2 - Ellχ* - χk-ι∕2k2)∙
And for P(xk), we can deduce by the same way, and obtain the similar result:
2
E[P(xk) - P(χ*)] ≤-------[P(χk-1∕2) - P(χ*) + P(x) - P(χ*)]
a — 1
+ ^2^(kx* - xk-1∕2k2 - Ekx* - xkk2)∙
(15)
(16)
We assume that α is sufficiently large to make α-1 ≤ 1, and sum (15) and (16) together, then we
obtain
一	一 2 一	一 4 一	一
E[P(Xk) - P(x*)] ≤	r[P(xk-1) - P(x*)] +	r[P(x) - P(x*)]
α — 1	α — 1
+----------2^(kx* - Xk-Ik2 - E∣∣x* - xk ∣∣2)∙
which is equivalent to
22
(1 - a 1 )E[P(Xk ) - P (x*)] ≤ α	1 {[P (Xk-I)- P (x*)] - E[P (Xk)- P(x*)]}
+ L[P(X) - P(x*)] + "(kx* - Xk-1k2 - Ekx* - XkI2).
Q — 1	2
By summing the previous inequality over k = 1,...,m,we obtain
(1 -  -1 )〉： E[P(Xk) - P(x*)] ≤  -1{[P(x0)- P(x*)] - [P(Xm)- P(x*)]}
-	k=1	-
+ T[P(X) - P(x*)] + ”(kx* - X0k2 -kx* - Xmk2).
Q — 1	2
Since we set Xs = mm Pm=I Xk and x0+1 = Xm, and we know F(∙) is a convex function. Thus, we
have
22
(1 - QzI)E[P(Xs) - P(X*)] ≤ m(Q- I) {[P(X0) - P(X*)] - [P(x0+1) - P(x*)]}
+ j[P(Xs-1) - P(x*)] + LQ(kx* - X0k2 -kx* - x0+1k2).
Q — 1	2m
By summing the previous inequality over S = 1,..., S,we obtain
2 二	4 二	Lq
(1 - QZl) EE[P(Xs)	- P(x*)]	≤ Q-I ∑[P(Xs)	- P(x*)] +	而(kx*	- x1k2	-kx*	- xmk2)
s=1	s=0
2
+ m(α - 1) {[P(X0) - P(X*)] - [P(Xm) - P(X*)]}.
24
Under review as a conference paper at ICLR 2020
That is,
(1 -
4S
-I) XE[P(Xs) - P(x*)]
s=1
4
≤----
α-1
[P(x0)- P(x*)] +
m(α - 1)
([P(x0)- P(x*)] -[P(xm)- P(x*)])
+ 而(B-X1k2-kx*-Xmk2)
- m(α — 1)
[P (XO)- P(X*)] + o-ɪ[P (XO)- P(X*)] + M kx*- x1k2.
The first inequality holds due to 1 一 α⅛ι ≥ 1 — α-ι — a-1 and the second inequality holds because
P(Xm) - P(x*) ≥ 0 and ∣∣x* - Xm∣∣2 ≥ 0. Because X1 = X0, we have
6S
(1 - α-ɪ) ∑E[p (Xs)- p (x*)] ≤ (
s=1
m(α — 1) + α — 1
)[P(X0)- P(X*)]+Lα kX0-X*k2
Due to the convexity of F(∙), we have
SS
EP(XXs) - P(x*) ≤ - X E[P(Xs) - P(x*)]
s=1
s=1
V 4m + 2
m(α — 7)S
We note that X0 = xo, so We have
[P(XO)- p(χ*)] + 2∣α≡⅛ kx0-x*k2
S
EP(XXs) — P(x*) ≤
s=1
4m + 2
m(α - 7)S
[P(x0) - P(X*)] + 2m(α - 7)S kX0 一 '*『
2
—
α 一 1 α
2
2
2
4
□
E
More Experimental Results
E.1
The Information of Data Sets
Table 2: Summary of Data Sets and Regularization Coefficient
Data sets	Sizes n	Dimensions d	Sparsity	λ1	λ2
a9a	32,562	123	Sparse	10-6	10-4
Covtype	581,012	54	Dense	10-5	10-8
rcv1	20,242	47,236	Sparse	10-8	10-1O
real-sim	72,309	20,598	Sparse	10-6	10-8
E.2
The Problem Models
In this part, we introduce two common problem models. The first one is
1n	λ
min9-Σ(aT X-bi) + X1kXk1 + V llXk2.
x∈Rd 2n	2
i=1
25
Under review as a conference paper at ICLR 2020
When λ1 ≥ 0, λ2 ≡ 0, we can obtain the Lasso problem, and when λ1, λ2 ≥ 0, we can obtain the
Elastic-Net problem, which are all non-smooth optimization problems. The second problem model
is
1n
min—Tlog(1+exp(-bixτ a，i)) + λ∣∣xkι,
x∈Rd n
i=1
which is called the `1 norm regularized logistic regression problem.
E.3 More Experimental Results
For more comprehensive comparison, we provide the performance comparison of more algorithms,
including SVRG++, MiG and our VR-SExtraGD.
Running time (sec)
(a) a9a	(b) Covtype
Figure 4:	Comparison of experimental results of different algorithms for Lasso on different data
sets. The y-axis represents the gap between the objective value and the minimum, and the x-axis
corresponds to the number of effective passes (top) or running time (bottom).
Figure 4 shows the experimental result of different algorithms on different data set to solve the Lasso
problem. We can see that AVR-SExtraGD is superior to other algorithms in terms of the number of
effective passes and running time. Besides, we note that VR-SExtraGD achieves almost the same
result as AVR-SExtraGD in terms of effective passes, which may be due to the advantage of the
extragradient structure. But, since VR-SExtraGD needs to calculate the stochastic gradient twice in
each inner-iteration, the result in terms of running time is not as good as AVR-SExtraGD.
Moreover, in order to avoid the particularity of the problems solved by our algorithm, we also
provide the performance comparison of different algorithms on different data set to solve the `1-
norm regularized logistic regression problem, as shown in Figure 5.
From Figure 5, we know that our AVR-SExtraGD outperforms other compared algorithms in terms
of both effective passes and running time. Although Katyusha is better than our algorithm in terms of
effective passes, its result in terms of running time in not as good as ours because of the complicated
structure of the algorithm.
26
Under review as a conference paper at ICLR 2020
1000
1500
—β- ProX-SVRG
-A- SVRG++
-⅛- Katyusha
—M—MiG
—V—VR-SExtraGD
AVR-SExtraGD
2000
(a) a9a	(b) Covtype
Figure 5:	Comparison of experimental results of different algorithms for the 'ι-norm regularized
logistic regression problem. on different data sets. The y-axis represents the gap between the objec-
tive value and the minimum, and the x-axis corresponds to the number of effective passes (top) or
running time (bottom).
27