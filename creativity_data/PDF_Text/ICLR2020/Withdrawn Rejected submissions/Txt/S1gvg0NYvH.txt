Under review as a conference paper at ICLR 2020
Mean Field Models for Neural Networks in
Teacher-student Setting
Anonymous authors
Paper under double-blind review
Ab stract
Mean field models have provided a convenient framework for understanding the
training dynamics for certain neural networks in the infinite width limit. The
resulting mean field equation characterizes the evolution of the time-dependent
empirical distribution of the network parameters. Following this line of work,
this paper first focuses on the teacher-student setting. For the two-layer networks,
we derive the necessary condition of the stationary distributions of the mean field
equation and explain an empirical phenomenon concerning training speed differ-
ences using the Wasserstein flow description. Second, we apply this approach to
two extended ResNet models and characterize the necessary condition of station-
ary distributions in the teacher-student setting.
1 Introduction
In the past several years, neural networks have achieved tremendous successes in many areas of
machine learning and artificial intelligence (Devlin et al., 2018; He et al., 2016; Silver et al., 2016;
Vaswani et al., 2017), based on remarkable developments in algorithms (Robbins & Monro, 1951;
Duchi et al., 2011; Rumelhart et al., 1988), software (Paszke et al., 2017; Jia et al., 2014; Abadi et al.,
2016), and hardware (Lindholm et al., 2008; Jouppi et al., 2018). In spite of the empirical successes,
the theoretical understanding of neural networks is rather incomplete. To bridge this gap, tools from
different fields, such as statistical mechanics, partial differential equations, dynamics systems, have
been brought into use. Recently, a series of remarkable papers (Chizat & Bach, 2018; Mei et al.,
2018; Rotskoff & Vanden-Eijnden, 2018; Sirignano & Spiliopoulos, 2018) analyze the two-layer
neural networks using mean field models and nonlinear partial differential equations (PDEs). The
main result is that, under the large width limit and sufficient small learning rates, stochastic gradient
descent (SGD) dynamics of a two-layer fully connected neural network with a convex loss can be
modeled as a gradient flow of a convex functional of probability distributions under the Wasserstein
metric (Peyre et al., 2019; Santambrogio, 2015; Villani, 2008). Though only for two-layer networks,
this result significantly simplifies our understanding of the training dynamics. In two recent papers
(Nguyen, 2019; Sirignano & Spiliopoulos, 2019), this approach has been extended to multi-layer
fully-connected networks by taking the widths to infinity successively from higher to lower layers.
Contributions. This paper follows this line of research, focusing on the teacher-student setting. The
teacher network is typically a narrow network that computes the function to be learned, while the
student network is a vastly over-parameterized wide network. The goal is to optimize the parame-
ters of the student in order to produce the function represented by the teacher. Though somewhat
restricted, the teacher-student setting allows for assessing performance by comparing the parameters
of the networks. The main contributions of this paper are listed as follows.
First, for the two-layer networks in the teacher-student setting, we characterize for the first time the
necessary condition fora stationary distribution of the mean field PDE and show how it relates to the
parameters of the teacher network. We also discuss how different activation functions influence the
results. Using the Wasserstein flow description, we also provide a simple explanation for why the
convergence to teacher parameters with larger weight is much faster than to the ones with smaller
weight.
Second, we extend the mean field analysis to two extended ResNet models: the ensemble-ResNet
(an average of a large number of independent ResNets) and the wide-ResNet (a ResNet with wide
residual blocks). We derive mean field PDEs for the training dynamics of the parameters in both
1
Under review as a conference paper at ICLR 2020
cases and also characterize the necessary condition for a stationary distribution for the ensemble-
ResNet model as well as the first and second order approximations of the wide-ResNet model.
Though our derivation remains formal, this is the first time that the mean field model of the ResNet
has been considered.
Related work. Besides the aforementioned papers on the mean field analysis for neural networks,
tools from stochastic analysis (Mandt et al., 2015; Li et al., 2017; 2019; Yaida, 2018; An et al., 2018)
and Wasserstein geometry (Arjovsky et al., 2017; Cuturi, 2013; Frogner et al., 2019; Peyre et al.,
2019; Solomon et al., 2014) have been playing an increasingly prominent role in machine learning
and artificial intelligence.
Another direction that considers the infinite width neural networks is the work (Jacot et al., 2018)
of the neural tangent kernel (NTK), which describes under an appropriate scaling the evolution of
the function represented by the neural network. At the infinite width limit, the NTK converges to a
deterministic limit that depends on the nonlinear activation function and remains unchanged during
the training process. Though it is not yet clear how this limit relates to the actual training dynamics
of neural networks used in practice (Chizat et al., 2019), mathematically it is an interesting regime
that has inspired a lot of recent work (Arora et al., 2019a;b; Cao & Gu, 2019).
In recent years, particle-based methods have also been widely applied to many machine learning
problems, such as in Bayesian inference. One such recent example is the Stein variational gradient
descent (SVGD) method (Liu & Wang, 2016; Liu, 2017) that approximates a target distribution with
a group of interacting particles using iterative gradient updates. It has been shown that, in the large
particle number limit, the empirical distribution of the parameters can be modeled by a nonlinear
PDE, which can be viewed as a gradient flow of a convex loss function under a Wasserstein-type but
non-local metric (Liu, 2017; Lu et al., 2019).
Organization. The rest of the paper is organized as follows. Section 2 concerns the mean field
description of two-layer networks and provides necessary conditions for stationary distributions.
Section 3 extends the analysis to two extended ResNet models. Finally, Section 4 discusses some
future directions.
2 Two-layer network in teacher-student setting
2.1 Two-layer networks
Setup. Given a function f : Rd → R of the independent variable x ∈ Rd, consider its approximation
by a two-layer network model N PjN=↑, Wjσ(χ ∙ θj), where N is the number of hidden units, {θj}
are the parameters in a domain Θ ⊂ Rd, {wj } are the weights, and σ : R → R is an activation
function such as rectified linear unit (ReLU), softplus, etc. The following discussion focuses on the
special case where the weights wj are fixed to be one, i.e.,
1N
N∑Sσ(x ∙ θj).	⑴
The more general case of trainable wj values are discussed in Appendix 5.1. Assuming that the input
X is sampled from a measure μ(χ) independently, the learning problem of this two-layer network
is to minimize the loss E(θι,..., θN) = fχ ' (N PN=I σ(x ∙ θj) - y(x)) μ(x)dx, where '(∙) is a
loss function. The rest of this paper focuses for example on the quadratic loss case, i.e., '(z) = 1 |z|2:
N
2
E(θ1,...,θN) ≡
1 !xN X σ(x
• θj) — y(x) μ(x)dx ≡ ɪ
1N
N∑Sσ(x • θj)- y(X)
j=1
μ
When optimized with a standard algorithm such as gradient descent (GD), each parameter θj for
1 ≤ j ≤ N is updated with
∂E	∂E	1 N	1
θi[k+1] =	θi[k]-α∂θ?[k],	砺[k]	= J I N∑Sσ(x • θj[k])-y(X)J	Nσ (χ∙θi[k])χμ(χ)dχ,
2
Under review as a conference paper at ICLR 2020
where α is the learning rate of the iterative algorithm. For the SGD, the right hand side is supple-
mented with a mean-zero noise term. When the learning rate α is sufficiently small, the iterates
{θi [k]} can be modeled by a continuous-time ordinary differential equation (ODE) system {θi (t)}
by identifying t = kα∕N
dθi(t)
dt
-Z (N X σ(x ∙ θj(t)) - y(x)j σ0(x ∙ θi(t))xμ(x)dx.
(2)
Mean field equation. Viewing θi(t) as interacting particles and considering the empirical distribu-
tion ρ(t,θ) ≡ N PN=I δθj(t)(θ), We can rewrite N Pj- σ(x ∙ θj) - y(x) as R σ(x ∙ θ)ρ(θ)dθ and
E(θ1, . . . , θN) as
2
μ
E(ρ) ≡
J σ(x ∙ θ)ρ(θ)dθ — y(x)
The mean field evolution equation for ρ(t, θ) associated with (2) is
ρt(θ, t) = divθ
ρ(θ, t)
Zx	Zθ0
σ(x ∙ θ0)ρ(θ0, t)dθ0 —
σ0(x ∙ θ)xμ(x)dx
Noticing that the functional derivative ofE(ρ) with respect to ρ is given by
δE(ρ)
δP
Zx	Zθ0
σ(x ∙ θ0)ρ(θ0)dθ0 -
σ(x ∙ θ)μ(x)dx,
one can write the mean field PDE for ρ compactly as
ρt = divθ
ρVθ
δE
δP 一
(3)
which can be interpreted as the gradient flow of the functional E(ρ) under the the Wasserstein metric
(VTρVθ)+ (Jordan et al.,1998; Otto, 2001; Otto & Villani, 2000).
Though the above derivation is mostly formal, one can show that the dynamics of θi(t) converges to
the evolution of ρ(θ, t) for any fixed period of time [0, T] when the number of hidden units N goes
to infinity. This is formalized for example in the following theorem.
Theorem 1 (Reformulated from (Chizat & Bach, 2018)). If θi(0) are sampled independently from
a density ρo(x) at time t = 0, then for any fixed T > 0 the measure N PN=I δθj (t)(θ) converges
weakly to the solution ρ(t, θ) of (3) for any t ∈ [0, T] as N goes to infinity.
For the case of bounded parameter domain Θ, such as a ball in Rd with a fixed radius, one needs to
specify the appropriate boundary conditions both for the ODE system (2) and the mean field PDE
(3). First, the dynamics ofθi(t) should follow normal reflection when θi(t) hits the boundary ∂Θ of
Θ. Second, for the mean field equation of ρ(t, θ),the Neumann boundary condition dp⅛θ) (t θ) = 0
should be used at θ ∈ ∂Θ. The bounded case is particularly relevant to the discussion in Section 3.2.
Stationary distribution in teacher-student setting. Given the PDE formulation of the parameter
evolution, one natural question is to characterize the stationary distribution of (3). In what follows,
we address this question in the teacher-student setting, i.e., there exists a parameter distribution
ρ* (θ) such that the target function y(x) satisfies
y(x) =
θ
σ(x ∙ θ)ρ*(θ)dθ.
With this teacher representation, the loss E(ρ) can be written equivalently as
E[ρ]
2 / UJ(X ∙ θ)(ρ - ρ*
2
μ(x)dx.
The following theorem characterizes the stationary distribution in this teacher-student setting.
3
Under review as a conference paper at ICLR 2020
Theorem 2. Suppose that μ(x) > 0 for X ∈ Rd. Then any stationary distribution ρ(θ) that satisfies
ρ(θ) > 0 for any θ ∈ Θ has zero loss and satisfies the condition
σ σ(x ∙ θ)(ρ(θ) - ρ*(θ))dθ = 0.
(4)
Proof By introducing the operator Σ with kernel given by σ(x ∙ θ), one can write the loss as E(P)
2 ∣∣Σ(ρ - ρ*)∣∣μ and the functional derivative as
δE = ∑τμ∑(ρ-ρ*),
where μ is considered to the operator of multiplying with μ(x) at each x. At a stationary distribution
[ρvθ⅞p] =0 atany θ
ρ, since ρt ≡ 0, divθ
∈ Θ. Taking inner product with 端 gives
τ
divθ ρvθ
δE
δP
0.
Using the fact that div& is the adjoint of Vθ, this states y∕ρ(θ)Vθδδρ = 0, which further implies
Vθδδp = 0 as ρ(θ) > 0. This means that δδP = ∑τμ∑(ρ - ρ*) as a function of θ is a constant.
Taking inner-product of δδρ with (P - ρ*)(θ) and using the fact that J(P - ρ*)(θ)dθ = 0 gives
(P -夕平了夕‘小夕(P - ρ*) = 0.
This implies zero loss and ,μ(x)Σ(P - p*) = 0, which further implies Σ(p - p*) = 0 since
μ(x) > 0. Writing Σ(p - p*) = 0 in the integral form gives (4).	□
Below we consider the implication of Theorem 2 for a few commonly-used activation functions.
Example 1.	Consider the ReLU activation σ(z) = max(z, 0). It is convenient to use the polar
CoordmateS X → (x, |x|) and θ → (θ, ∣θ∣), where | ∙ | and ∙ denote the radial coordinate and the
angular variables, respectively. Then (4) reads
I I	..1 ʌ. , ʌ .	ʌ ... ʌ
/ / ∣x∣∣θ∣dσ(X ∙ θ)(P(θ, ∣θ∣) - P*(θ,∣θ∣))dθd∣θ∣ =0,
θθ J∖θ∖
where the factor ∣θ∣d comes from the homogeneity OfReLU as well as the Jacobian from the polar
coordinate transformation. Ifthe operator with kernel (σ(x ∙ θ))χ θ∈sd-ι has a trivial null space,
.i	1 ι .ι ..1	. . ■	ι∙ . ∙ι . ∙	/ A I Zi I ∖ Jrr	久一 Crl一1
then one can conclude that the StatIOnary distribution p(Θ, ∣θ∣) satisfies for any θ ∈ Sd 1
∖θ∖
.j. ʌ	, ʌ.......
∣θ∣d(P(θ,∣θ∣)-P*(θ,∣θ∣))d∣θ∣ = 0.
Example 2.	Consider for simplicity the 1D case d = 1. When σ(z) is an even function such as the
Gaussian or quadratic activations, (4) states that P(θ) is a stationary distribution if and only if the
difference (P — P*)(θ) is an odd function.
When σ(z) is an odd function such as tanh or arctan, (4) states that P is a stationary distribution if
and only if the difference (P — p* )(θ) is an even function. In addition, when σ(z) is the sum of an
oddfunction and a constant (for example, like the Sigmoidfunction), (4) is equivalent to (P — P* )(θ)
being even, since the integral of (P — P*)(θ) is zero.
Finally, the softplus function σ(z) = ln(1 + ez), which is a smooth approximation of ReLU. (4)
states that
ln(1 + eθx)(P - P*)(θ)dθ
0.
Let us assume for simplicity that both P(θ) and P* (θ) decay faster than θ-2 at large θ and define
f(θ) as the anti-derivative of(P - P*)(θ). Integrating by parts for (4) results in
/1+⅛ f (θ)dθ=0.
4
Under review as a conference paper at ICLR 2020
Noticing that the Sigmoidfunction [+)9 is a sum of an oddfunction and a constant, we conclude
that (4) is equivalent to f (θ) being ^ven and mean-zero. In terms Ofthe difference (P 一 ρ*)(θ), these
two conditions means that (P — ρ*)(θ) is odd and has a vanishing first moment
/ θ ∙(ρ — ρ*)(θ)dθ = 0.
2.2 Convergence speed issue
One interesting empirical observation from training neural networks in the teacher-student setting is
that the convergence to a teacher parameter with a large weight is much faster than the speed to a
teacher parameter with a small weight. More specifically, consider a teacher network given by
ρ*(θ) = α1δc1 (θ) + α2δc2 (θ),
where ci are the parameter values in Rd and the weights αi satisfy α1 α2 > 0 and α1 + α2 = 1.
This can either be realized as a general two-layer network of two hidden units with weights α1 and
α2 or an equal-weight network (1) of α1N units with parameter c1 and α2N units with parameter
c2 . After an initial short period, the evolution of the student parameters decouple into two nearly
independent problems: an α1 fraction of the N student parameters converges to c1, while an α2
fraction of the student parameters converges to c2. The observed behavior is that the convergence to
c1 is much faster than the convergence to c2 .
The mean field PDE allows us to understand this phenomenon using scale analysis. After the initial
stage, the PDE Pt = divθ [ρVθ (∑τμ∑(ρ — ρ*))] is effectively decoupled into two equations
Pt =divθ	[ρ1Vθ	(∑TμΣ(ρ1	—	pɪ))]	, where	ρ1	= ag](θ),
P2 = divθ	[ρ2Vθ	(∑TμΣ(ρ2	—	ρ2))]	, where	p2	=、小(θ).
The key observation is that the quadratic dependence on P on the right hand side is the reason of
different convergence speeds. To see this, let us divide these two equations by α1 and α2, respec-
tively,
α1 divθ
α2 divθ
P1
where — = 6^ (θ),
α1	1
where 区=δc2 (θ).
α2	2
These two are the same equations for normalized densities αp1 and OPl, except the coefficients at
and α2 at the beginning of the right hand sides. The difference in these two coefficients clearly
demonstrates that the first equation converges at/a2 times faster than the second one.
3 ResNet models in teacher-student setting
Consider a simple ResNet model with input x ∈ Rd (see Figure 1 left)
X0 = x,	X'+1 = X' + 1 σ(θ'X'),	0 ≤ '<L,
L
where X' ∈ Rd, the matrix parameter θ' ∈ Θ ⊂ Rd×d, and the activation σ is applied entry-wise to
each component. By introducing θ = (θ0, . . . , θL-t ), we denote the final result at level L by XθL
in order to emphasize the dependence on θ. Given a function y : Rd → Rd, the training problem
searches for the parameters θ = (θ0, . . . , θL-t) ∈ ΘL in order to minimize, say, the quadratic loss
E⑹=2 / IXL(X)- y(X)12μ(X)dx = 1 IIXL(X)— y(X)IIj.
When optimized with GD, the parameter vector θ is updated at each step with
θ[k +1] = θ[k] — αdE (θ[k])
∂θ
(or with an extra noise term for SGD). Instead of directly working with this vanilla ResNet, we
introduce two extended ResNet models in the rest of this section and derive a mean field equation
for each of them.
5
Under review as a conference paper at ICLR 2020
Figure 1: ResNet models. Left: vanilla ResNet. Middle: ensemble-ResNet. Right: wide-ResNet.
Mean field models for the ensemble-ResNet and wide-ResNet models are discussed in Section 3.
3.1	Ensemble-ResNet
Setup. The first model, called ensemble-ResNet, averages over a group of N ResNets with inde-
pendent parameters (see Figure 1 middle). Its j -th ResNet has the same structure as the vanilla
ResNet described above and contains parameters θj = (θj0, . . . , θjL-1) ∈ ΘL. The output of the
ensemble-ResNet is N PN=I XL (x) and the loss function is
2
E (θι,..., θN) = 2
1
N∑XLj (X)- y(x)
j=1
μ
The GD/SGD algorithm with learning rate α takes the following step for each i
θi[k + 1] = θi[k] - a Z NNeiXLi[k](X)T ( N XXLj[k](x) - y(X) ) μ(X)dx.
When a/N is sufficiently small, the dynamics of θ,[k] can be approximated by an ODE system
θi(t) by identifying t = ka/N
dθi = - Z NeiXLi(X)T (N X χθj(X)- y(X)) μ(χ)dχ.
Viewing θi(t) ≡ (θi0(t), . . . , θiL-1(t)) as particles in ΘL and introducing the distribution ρ(t, θ) =
N PN=I δθj(t)(θ), We rewrite E (θɪ,..., Θn ) as E (ρ) ≡ 2 ∣∣Rθ XL (X)ρ(θ)dθ — y(X)∣∣j .The
mean field equation of ρ(t, θ) takes the same form as (3)
ρt(θ) = dive
ρNe
δE~
δρ
which is a gradient flow of quadratic loss E(ρ) under the Wasserstein metric on the densities ρ(θ)
on the product space ΘL.
Stationary distribution in teacher-student setting. In the teacher-student setting, the function
y(x) from Rd to Rd is assumed to take the form y (x) = fθ XL (x)ρ* (θ)dθ, for a certain distribution
ρ* on the space ΘL. This include the special case that y(X) can be represented by a vanilla ResNet
described above. More precisely, assuming that
X 0 = X,	X '+1 = X' + 1 σ(θ' X'), y(X) = X l(x),
then y(X) corresponds to ρ*(θ) = δe* (θ) with θ* ≡ (θ0,..., OLT) ∈ ΘL.
6
Under review as a conference paper at ICLR 2020
Theorem 3. Suppose that μ(x) > 0 for X ∈ Rd. Then any stationary distribution ρ(θ) that satisfies
ρ(t, θ ) > 0 for any t > 0 and θ ∈ ΘL has zero loss and satisfies the condition, for each x,
∕xL(x)(ρ(θ)- ρ*(θ))dθ = 0.
(5)
The proof of the theorem follows the same argument as Theorem 2. One potential criticism of the
ensemble-ResNet model is that the space ΘL ⊂ (Rd×d)L can be very high-dimensional even for
moderate values of L. Therefore, the number of branches N required in order for the mean field
model to be accurate can be quite large.
3.2	Wide-ResNet
In order to overcome the exponential grow of the space ΘL in the ensemble-ResNet model, we
consider a different ResNet model in this subsection. Instead of allowing for multiple independent
branches, the wide-ResNet model sticks to a single path but increases the width of each residual
block (see Figure 1 right). For an input x ∈ Rd, the wide-ResNet evaluates
1N
X0 = X, x'+1 = x' + LN X σ(θjx'), 0 ≤ '<L,
(6)
j=1
where the activation function σ(z) is assumed to be smooth and vanishes at z = 0. By introduc-
ing the parameter group θ' = (θ`,...,θN) for each level ', We write the final result of (6) as
xθL0 ... θL-1 (X) and the quadratic loss function as
E(θ0,…,θLT) = E(θ0,…,θN,…,θL-1,…,θN-1) = 2 ∣∣xL0,...,θL-1 (x) - y(χ)∣∣2
The GD/SGD algorithm with learning rate α takes the form
θ'[k + 1] = θ`[k] - α/ vθ'xL0[k],…,θL-1[k](X)T (XL0[k],…,θL-1[k](X) - y(X)) μ(X)dx,
where Vθ'XL)肉 8工-1向3 contains a factor LN. When the stepsize α is sufficiently small, the
dynamics of θi`[k] can approximated with an ODE system
^dti = - Z vθ'xL0,…,θL-1 (X)T (XLO,…,θL-1 (X)- y (X)) μ(X)dX.
By viewing θ`(t),..., θN(t) as interacting particles at each level ' and introducing ρ'(t, θ`) =
N Pj δθ'(t) (θ`) for each ', one can rewrite the wide-ResNet computation as
X0 = X, X'+1 = X' + J / σ(θ'X')ρ'(θ')dθ', 0 ≤ '<L.
(7)
Bytakingthelimit N to infinity, the loss becomes E(ρ0,...,ρLT) = 1 ∣∣xL),..,ρL-ι (x) — y(x)∣∣2
(with the subscripts denoting the dependence on ρ0, . . . , ρL-1) and the mean field equation for
ρ'(t,θ') for each level' reads
ρ'(Θ')=divθ' ρ'(θ')Vθ'
δE(ρ0,...,ρL-1)
(8)
As σ(z) is assumed to be smooth, expanding (7) recursively and applying the Taylor expansion (see
Appendix 5.2 for a derivation) gives the following approximation
XL ≈ X 0+1 X ∕σ(θx0)ρa(θ)dθ+L XZZVσ(θbX0)θbσ(θaX0)ρb(θb)ρa(θa)dθadθb+...
(9)
7
Under review as a conference paper at ICLR 2020
Furthermore, since σ(z) vanishes at zero by assumption, this expansion has L terms of order O(θ)
each with weight 1/L, L2 terms of order O(θ2) each with weight 1/L2, etc. When the domain Θ of
the matrix parameter θ is bounded by a sufficiently small radius, the contribution from the k-linear
terms is of order O(1/k!) and hence decays rapidly with increasing values of k. Therefore, in the
same parameter regime, it is sufficient to consider approximations of the first few orders.
Linear approximation. The linear approximation for XL in (9) leads to the following first-order
approximation to the loss
2
E(ρ0, ...,PL-1) ≈ E(1)(P0, ...,PL-1) ≡ 2 X + L X ∕σ(θx)ρa(θ)dθ - y(x).
a	μ
Now take the limit of L going to infinity and define ρ(θ, ε) = Ea Pa (θ) ∙ 6&/匕(ε) where ε ∈ [0,1] is
a rescaled continuous variable associated with the depth of the ResNet. In this formal limit, the loss
function becomes E(i)(p) ≡ 2 ∣∣x + Jf σ(θx)ρ(θ, ε)dθdε - y(x) ||* . The mean field equation for
ρ(t, θ, ε) takes the form
Pt(θ, ε) = divθ P(θ, ε)Vθ-zɪɪɪ^j.	(IO)
Notice that this is a degenerate gradient flow with mobility only in the θ variable. In the teacher-
student setting, i.e. y(x) = X + /J σ(θx)ρ* (θ,ε)dθdε for some teacher distribution ρ*(θ,ε),wecan
write compactly E(i)(p) = 2 k∑ι(ρ - P*)kj, where ∑ι is the operator with kernel σ(θx). Notice
that ε does not appear in the kernel explicitly.
Theorem 4.	Suppose that μ(x) > 0 for X ∈ Rd. Then any stationary distribution ρ(θ, ε) that
satisfies ρ(θ, ε) > 0for any θ ∈ Θ and ε ∈ [0, 1] has zero loss and satisfies, at each X ∈ Rd,
“σ(θx) (/ρdε - /ρ*dε) dθ = 0.
(11)
The proof follows the argument of Theorem 2. It is not surprising that the constraint (11) is formu-
lated in terms of the marginal of ρ, since the parameters at different levels become almost commu-
tative in the small θ regime.
Quadratic approximation. The quadratic approximation for XL in (9) leads to the following
second-order approximation to the loss
1	∣	1 L-1
E ⑵(ρ0, ...,PL-1) ≡ 2 X + Lfl σ(θx)ρa(θ)dθ - y(x)
2	L a=0
+ -12 XZZ Vσ(θbX0)θbσ(θaX0)ρb(θb)ρa(θa)dθadθb - y(x).
b>a〃	μ
Taking the large L limit and introducing ρ(θ, ε) ≡ Pa Pa (θ) ∙ δ0∕L (ε) gives rise to the loss function
E(2) (P) ≡ 2
x+
σ(θx)P(θ, ε)dθdε +
Vσ(θ0x)θ0σ(θx)P(θ0, ε0)P(θ, ε)dθ0dθdε0dε - y(x)
2
μ
in terms of P(θ, ε) and the mean field equation takes the same form as (10) except with E(1) replaced
with E(2). In the teacher-student setting, there exists some ρ* (θ, ε) such that
y(x) = x + / σ(θx)ρ*(θ,ε)dθdε + JJ	Vσ(θ0x)θ0σ(θx)ρ*(θ0,ε0)ρ*(θ,ε)dθ0dθdε0dε.
By introducing a new operator ∑2(ρ 0 P) = ff>>£ Vσ(θ0x)θ0σ(θx)ρ*(θ0, ε0)ρ*(θ, ε)dθ0dθdε0dε,
we can write it more compactly as
E(2) (P) = 2 ∣∣ςI(P -夕*) + 夕2(夕 0 P - P 0 P*)k：.
Unfortunately, since E(2)(P) is quartic in P, a stationary distribution might be a local minimum even
with the extra assumption P(θ, ε) > 0. On the other hand, the explicit and simple form of E(2)(P)
might allow multiple runs with different initial condition p(0, θ) to identify the p* more efficiently.
8
Under review as a conference paper at ICLR 2020
4 Discussions
This paper discusses necessary conditions for the stationary distributions of mean field models for
the two-layer neural networks as well as two extended ResNet models. A lot of interesting questions
remain to be addressed concerning the ResNet. The first, and probably most important, question
is that whether it is possible to obtain a mean field model for the vanilla ResNet by treating the
depth L as the large parameter going to infinity. This in fact has been the original motivation of
this paper and it still remains unresolved. Second, in the wide-ResNet model, the resulting PDE is
degenerate as there is no diffusion in the ε variable. Would it be possible to include the diffusion
in ε by allowing parameters to jump between adjacent layers? If this is possible, the next step is to
formulate a mean field model with densities over some “architecture” space for neural architecture
search (Zoph & Le, 2016; Zoph et al., 2018; Elsken et al., 2018; Wistuba et al., 2019).
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Jing An, Jianfeng Lu, and Lexing Ying. Stochastic modified equations for the asynchronous stochas-
tic gradient descent. arXiv preprint arXiv:1805.08244, 2018.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223, 2017.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. arXiv preprint arXiv:1905.13210, 2019.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3036-3046, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. arXiv
preprint arXiv:1808.05377, 2018.
Charlie Frogner, Farzaneh Mirzazadeh, and Justin Solomon. Learning embeddings into entropic
wasserstein spaces. arXiv preprint arXiv:1905.03329, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
9
Under review as a conference paper at ICLR 2020
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing Systems, pp. 8571-
8580, 2018.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser-
gio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. In Proceedings of the 22nd ACM international conference on Multimedia, pp. 675-678.
ACM, 2014.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-
planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.
Norman Jouppi, Cliff Young, Nishant Patil, and David Patterson. Motivation for and evaluation of
the first tensor processing unit. IEEE Micro, 38(3):10-19, 2018.
Qianxiao Li, Cheng Tai, et al. Stochastic modified equations and adaptive stochastic gradient algo-
rithms. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 2101-2110. JMLR. org, 2017.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic
gradient algorithms i: Mathematical foundations. Journal of Machine Learning Research, 20(40):
1-40, 2019.
Erik Lindholm, John Nickolls, Stuart Oberman, and John Montrym. Nvidia tesla: A unified graphics
and computing architecture. IEEE micro, 28(2):39-55, 2008.
Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in neural information
processing systems, pp. 3115-3123, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances in neural information processing systems, pp. 2378-2386, 2016.
Jianfeng Lu, Yulong Lu, and James Nolen. Scaling limit of the stein variational gradient descent:
The mean field regime. SIAM Journal on Mathematical Analysis, 51(2):648-671, 2019.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Continuous-time limit of stochastic gra-
dient descent revisited. NIPS-2015, 2015.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Phan-Minh Nguyen. Mean field limit of the learning dynamics of multilayer neural networks. arXiv
preprint arXiv:1902.02880, 2019.
Felix Otto. The geometry of dissipative evolution equations: the porous medium equation. Comm.
Partial Differential Equations, 26(1-2):101-174, 2001.
Felix Otto and Cedric Villani. Generalization of an inequality by talagrand and links with the loga-
rithmic sobolev inequality. Journal of Functional Analysis, 173(2):361-400, 2000.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning, 11(5-6):355-607, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence
and asymptotic error scaling of neural networks. In Advances in neural information processing
systems, pp. 7146-7155, 2018.
10
Under review as a conference paper at ICLR 2020
David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by
back-propagating errors. Cognitive modeling, 5(3):1, 1988.
FiliPPo Santambrogio. Optimal transport for applied mathematicians. Birkauser, NY, 55:58-63,
2015.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv
preprint arXiv:1805.01053, 2018.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks. arXiv
preprint arXiv:1903.04440, 2019.
Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Wasserstein propagation
for semi-supervised learning. In International Conference on Machine Learning, pp. 306-314,
2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Martin Wistuba, Ambrish Rawat, and Tejaswini Pedapati. A survey on neural architecture search.
arXiv preprint arXiv:1905.01392, 2019.
Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
arXiv:1810.00004, 2018.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 8697-8710, 2018.
11
Under review as a conference paper at ICLR 2020
5 Appendix
5	.1 Two-layer network with varying weights
In this subsection, we discuss the mean field equation for the general two-layer network model with
trainable weights
1N
N∑Swj σ(x ∙ θj),
where wj ∈ W ⊂ R for each j. The quadratic loss E(θ1, w1, . . . , θN, wN) is given by
E (θ1 , w1 , . . . , θN , wN )
1
2
Wjσ(x ∙ θj) — y(x)
μ
GD or SGD with sufficiently small step-size can be modeled by the ODE system
"°d(t = - Z (N X Wjσ(x ∙ θj(t)) — y(x)j Wi(t)σ0(x ∙ θi(t))xμ(x)dx,
dwi((t) = - Z (N X wjσ(X ∙ θj(U)- y(X) σ(X ∙ θi(t))μ(χ)dx.
Viewing (θi (t), Wi (t)) as interacting particles in Θ × W and considering the distribution
1N
ρ(t,θ,w) ≡ N	δθj(t),wj(t)(θ,w)，
/ wσ(x ∙ θ)ρ(θ, w)dθdw — y(x)
one can write N Ej wj∙σ(x ∙ θj) — y(x) as J wσ(x ∙ θ)ρ(θ, w)dθdw and E(θ1,w1,..., Θn, WN) as
E(P) ≡ 2
The mean field equation for ρ(t, θ, W) takes the form
ρt = divθw
δE
3 θw δρ
(12)
which is the gradient flow of E(ρ) under the Wasserstein metric on the densities ρ(θ, W). In the
teacher-student setting, i.e., y(x) = fθw wσ(x ∙ θ)ρ* (θ, w)dθdw for some teacher parameter distri-
bution ρ*(θ,w),
E[ρ]=2 / (/ wσ(x ∙ θ)(ρ(θ, W) — ρ*(θ,
2
dX.
The following theorem characterizes the stationary distribution of (12).
Theorem 5.	Suppose that μ(x) > 0 for X ∈ Rd. Then any stationary distribution ρ(θ,w) that
satisfies ρ(θ, W) > 0for any θ ∈ Θ and W ∈ W has zero loss and satisfies the condition
J wσ(x ∙ θ)(ρ(θ, w) — ρ*(θ,w))dθdw = 0.	(13)
Proof. By introducing the operator Σ with kernel given by wσ(x ∙ θ), one can write the loss E(P)
2 hς(P—ρ*)kμ and
δρ = ς,mς(P — ρ*),
12
Under review as a conference paper at ICLR 2020
where μ is considered to the operator of multiplying with μ(x) at each x. At a stationary distribution
P, since Pt ≡ O, divθw [pV&w^] = O at any θ ∈ Θ and W ∈ W. Taking inner product with δp
gives
T
divθw ρ(θ,w)Vl
θw
0.
Using the fact that divθ is the adjoint of Vθw, this means Jρ(θ,w~)VθwδE = O, which further
implies Vθw δp =O as ρ(θ, W) > O. This means that δp = ∑τμ∑(ρ — ρ*) as a function of θ and W
is constant. Taking inner-product of 端 With (P — ρ*)(θ, w), one arrives at
(P —P平了夕丁小夕(P — PJ = 0,
using the fact that J(P — ρ*)(θ, w)dθdw = O. This implies zero loss and
∙√μ(X)ς(P ― P*) = 0,
which further implies Σ(p — p*) = O since μ(x) > O. Writing the last statement explicitly in the
□
integral form gives (13).
Example 3. Let us consider the ReLU activation σ(z) = max(z, 0). Using the polar coordinates
/ ʌ I I ∖	1 Zi	/次 ∣C∣∖ ∕YC∖	1
X → (x, |x|) and θ → (θ, ∣θ∣), (13) reads
// / ∣χ∣∣θ∣d
Jθ 7∣θ∣ Jw
ʌ, ,	, 0
ʌ
O
wσ(x ∙ θ)(ρ(θ, ∣θ∣,w) — ρ*(θ, ∣θ∣, w))dθd∣θ∣dw = 0.
τr . 1	.	∙.ι .ι 7	I /	/ ʌ A∖ ∖	1	. ■ ■ I II	. 1	ι ι.ι
Ifthe operator with the kernel (σ(x ∙ θ))^ θ∈sd-ι has a trivial null space, then one can conclude that
that for each θ
θ∣dw
O
ʌ
ρ(θ, ∣θ∣,w) — ρ*(θ, ∣θ∣,w)) d∣θ∣dw = 0
that at the stationary distribution ρ.
5.2 Derivation of (9)
The computation follows the definition in (7) and uses Taylor expansion. At the first level,
X1 = X0 + 1 Z σ(θ0X0)ρ0(θ0)dθ0.
L θ0
At the second level,
X2 = X1 + 1 σ σ(θ1X 1)ρ1(θ1)dθ1
L θ1
=X0 + 1 Z σ(θ0X0)ρ0(θ0)dθ0 + 1
L θ0	L
X0 + "/ σ(θ0X0)ρ0(θ0)dθ0)) ρ1(θ1 )dθ1.
Recalling that σ is assumed to be smooth near the origin, a Taylor expansion for σ in the last term
X2 ≈X0 + 1 Z σ(θ0X0)ρ0(θ0)dθ0 + 1 Z σ(θ1 X0)ρ1(θ1)dθ1
L θ0	L θ1
+ jj2 jθ Vσ(θ1X0)θ1 (L σ(θ0X0)ρ0(θ0)dθ0) ρ1(θ1)dθ1 + h.o.t.
Iterating this until the L-th level gives rise to
X L ≈ X 0+1 E ∕σ"0)ρa(θ)dθ+L XZZVσ(θbX0)θbσ(θaX0)ρb(θb)ρa(θa)dθadθb+h.o.t.
13