Under review as a conference paper at ICLR 2020

STEIN  BRIDGING:   ENABLING  MUTUAL  REINFORCE-
MENT   BETWEEN   EXPLICIT   AND   IMPLICIT   GENERA-
TIVE  MODELS

Anonymous authors

Paper under double-blind review

ABSTRACT

Deep generative models are generally categorized into explicit models and im-
plicit models.  The former defines an explicit density form,  whose normalizing
constant is often unknown; while the latter, including generative adversarial net-
works (GANs), generates samples without explicitly defining a density function.
In spite of substantial recent advances demonstrating the power of the two classes
of generative models in many applications, both of them, when used alone, suf-
fer from respective limitations and drawbacks.  To mitigate these issues, we pro-
pose Stein Bridging,  a novel joint training framework that connects an explicit
(un-normalized)  density  estimator  and  an  implicit  sample  generator  with  Stein
discrepancy.  We show that the Stein Bridge induces new regularization schemes
for both explicit and implicit models. Convergence analysis and extensive exper-
iments demonstrate that the Stein Bridging i) improves the stability and sample
quality of the GAN training, and ii) facilitates the density estimator to seek more
modes in data and alleviate the mode-collapse issue. Additionally, we discuss sev-
eral applications of Stein Bridging and useful tricks in practical implementation
used in our experiments.

1    INTRODUCTION

Deep generative model, as a powerful unsupervised framework for learning the distribution of high-
dimensional multi-modal data,  has been extensively studied in recent literature.   Typically,  
there
are two types of generative models (Goodfellow et al., 2014).  Explicit models define an explicit
(unnormalized) density function, while implicit models learn to sample from the distribution without
explicitly defining a density function.

Explicit  models  have  wide  applications  in  undirected  graphical  models  (LeCun  et  al.,  
2006;
Salakhutdinov  &  Hinton,  2009;  Hinton  et  al.,  2006;  Ngiam  et  al.,  2011),  random  graph  
theory
(Robins et al., 2007), energy-based reinforcement learning (Haarnoja et al., 2017), etc.  However,
the unknown normalizing constant makes the model hard to train and sample from,  and the ex-
plicit models might not be able to capture the complex structure of true samples while maintaining
tractability.  In contrast, implicit models are more flexible in training and easy to sample from, 
and
in pariticular,  generative adverarial networks (GANs) have shown great power in learning repre-
sentations of images, natural languages, graphs, etc. (Goodfellow et al., 2014; Radford et al., 
2016;
Arjovsky et al., 2017; Brock et al., 2019). Nevertheless, due to the minimax game between generator
and discriminator/critic in GANs, the training process often suffers from instability, and produces
undesirable samples often associated with missing modes in data or generating extra modes out of
data. Therefore, it motivates us to consider jointly learning of two models that can presumably com-
pensate and reinforce each other in the training process. (More discussions about related works are
in Appendix A.)

Furthermore, there are many of situations where we do need both an explicit (unnormalized) density
and a flexible implicit sampler. For sample evaluation, it is not enough to merely distinguish 
samples
between real and faked one, and one may also expect to provide fine-grained evaluation on generated
samples, where the energy values given by the explicit models can be a good metric (Dai et al., 
2017).
Another  situation  is  outlier  detection.   Implicit  models  often  leverage  all  true  samples 
 (possibly
mixed with corrupted samples) as true examples for training.  To make up for the issue,  explicit

1


Under review as a conference paper at ICLR 2020

models could help to detect out-of-distribution samples via the estimated densities (Zhai et al., 
2016).
Also,  when given insufficient observed samples,  explicit models may fail to capture an accurate
distribution, in which case implicit model may help with data augmentation and facilitate training
for  density  estimation.   These  situations  inspire  us  to  combine  both  of  the  worlds  so  
as  to  take
advantage of two models in an effective way.

In this work, we aim at jointly learning explicit and implicit generative models.  In our framework,
an explicit energy model is used to estimate the unnormalized densities of true samples via mini-
mizing a Stein discrepancy; in the meantime, an implicit generator model is exploited to minimize
the Wasserstein metric (or Jensen-Shannon divergence) between distributions of true and generated
samples.  On top of these, another Stein discrepancy, acting as a bridge between implicit generated
samples and explicit estimated densities, is introduced and pushes the two models to achieve a con-
sensus.  We show that the Stein bridge allows the two generative models to reinforce each other
by imposing new regularizations on both models, which help the generator to output high-quality
samples and facilitate the energy model to avoid mode-collapse.  Moreover, we show that the joint
training helps to stabilize GAN training via a convergence analysis. Extensive experiments on vari-
ous tasks verify our theoretical findings as well as demonstrate the superiority of proposed methods
compared with existing deep energy models and GAN-based models.

2    BACKGROUND

In this section, we briefly provide some technical background used in our model.

Energy Model.  The energy model assigns each data x      Rd  with a scalar energy value Eφ(x),
where Eφ( ) is called the energy function and is parameterized by φ.   The model is expected to
assign low energy to true samples according to a Gibbs distribution pφ(x)  =  exp     Eφ(x)  /Zφ,
where Zφ is a normalizing constant dependent on φ.   The normalizing term Zφ is often hard to
compute, making the training intractable, and various methods are proposed to detour such term
(see Appendix A).

Stein Discrepancy.  Stein discrepancy (Gorham & Mackey, 2015; Liu et al., 2016; Chwialkowski
et al., 2016; Oates et al., 2017) is a measure of closeness between two probability distributions 
that
does not require the knowledge for the normalizing constant of one of the compared distributions.
Let P  and Q  be two probability distributions on X   ⊂  Rd,  and assume Q  has a (unnormalized)
density q. The Stein discrepancy S(P, Q) is defined as

S(P, Q) := sup  Eₓ∼P[AQf (x)] := sup  {φ(Eₓ∼P[∇ₓ log q(x)f (x)T + ∇ₓf (x)])},       (1)

f ∈F                                                     f ∈F

where     is often chosen to be a Stein class (see, e.g., Definition 2.1 in Liu et al. (2016)), f  
: Rd
Rd'   is a vector-valued function called Stein critic and φ is an operation that transforms a d     
d′
matrix into a scalar value. One common choice¹ of φ is trace operation when d′  = d. If     is a 
unit
ball in some reproducing kernel Hilbert space (RKHS) with a positive definite kernel k, it induces
Kernel Stein Discrepancy (KSD). We provide more details in Appendix B.

Wasserstein Metric.   Wasserstein metric is suitable for measuring distances between two distri-
butions with non-overlapping supports (Arjovsky et al., 2017).  The Wasserstein-1 metric between
distributions P and Q is defined as     (P, Q) := minγ  E₍ₓ,y₎   γ[  x     y  ], where the 
minimization
is       over all joint distributions with marginals P and Q.  By Kantorovich-Rubinstein duality, 
it has a
dual representation

W(P, Q) := max  {Eₓ∼P[D(x)] − Ey∼Q[D(y)]} ,                                  (2)

where the maximization is over all 1-Lipschitz continuous functions.

Sobolev  space  and  Sobolev  dual  norm.   Use  L²  to  denote  the  canonical  Hilbert  space  on 
 Rd

equipped with an inner product ⟨u, v⟩L2    :=    Rd  uvdx.  The Sobolev space H¹  is defined as the

closure of C₀∞, the set of smooth functions on Rd  with compact support, with respect to the norm

ǁuǁ       :=  . ∫   (u² + ǁ∇uǁ2)dxΣ1/2.  For v  ∈  L², its Sobolev dual norm ǁvǁ         is defined 
by

ǁvǁH−1  :=  sup  .⟨v, u⟩L2  :  ∫    ǁ∇uǁ   dx ≤ 1, ∫    u(x)dx = 0Σ .

2

                                                            u∈H¹                           Rd       
                                         Rd

1One can also use other forms for φ, like matrix norm when d′ /= d (Liu et al. (2016)).

2


Under review as a conference paper at ICLR 2020

The constraint   Rd  u(x)dx = 0 is necessary in ensuring the finiteness of the supremum.      H−1  
can
be viewed as a measure of smoothness, which measures the similarity (in terms of largest L²-norm)
between v and a subset of smooth functions in H¹.

3    PROPOSED  MODEL

In this section, we formulate our model, Stein Bridging, and highlight its regularization effects.

3.1    MODEL FORMULATION

We  denote  by  Prₑₐl  the  underlying  real  distribution  from  which  the  data    x   are  
sampled.   We
simultaneously learn two generative models – one explicit and one implicit – that represent esti-
mates of Prₑₐl. The explicit generative model has an explicit probability density PE proportional to
exp(   E(x)), where E  is referred to as an energy function.  The implicit generative model trans-
forms an easy-to-sample random noise z with distribution P₀ via a generator G to a generated sample
with distribution PG.  We use the Stein discrepancy as a measure of closeness between

x  =  G(z)

the explicit unnormalized density PE and the real distribution Prₑₐl, and use the Wasserstein metric
as a measure of closeness between the implicit distribution PG and Prₑₐl.

To jointly learn the two generative models PG and PE, arguably the most straightforward approach
is to minimize the sum of the Stein discrepancy and the Wasserstein metric:

min      (Prₑₐl, PG) + λ   (Prₑₐl, PE),

E,G

where λ       0 is a weight coefficient.  However, this approach appears no different than learning
the two generative models separately.  To better train the model, we incorporate the objective an-
other term    (PG, PE) – called the Stein bridge – that measures the closeness between the explicit
unnormalized density PE and the implicit distribution PG:

min      (Prₑₐl, PG) + λ₁   (Prₑₐl, PE) + λ₂   (PG, PE),                               (3)

E,G

where λ₁, λ₂      0 are weight coefficients. Although the Stein bridge might seem redundant mathe-
matically, we show that it helps regularize the models in Section 3.2.

The Wasserstein term in (3) is implemented using its equivalent dual representation (2).  The two
Stein terms in (3) can be implemented using (1) with either a Stein critic parameterized by a neural
network, or the Kernel Stein Discrepancy.  To reduce the computational cost, the two Stein critics
share their parameters, namely, kernels or neural networks. A scheme of our framework is presented
in Fig. 1.  We also discuss some related works that attempt to combine both of the worlds (such as
energy-based GAN, contrastive learning and cooperative learning) in Appendix A.3, and highlight
the difference between our method and theirs in terms of the objective in Table 1.

Remark.  In general, we can also choose other statistical distances in (3) to measure closeness be-
tween probability distributions. For example, the Wasserstein metric     (Prₑₐl, PG) can be replaced
by other common choices for implicit generative models, such as Jensen-Shannon divergence used
in the original GAN paper (Goodfellow et al., 2014). If the normalizing constant of PE is known or
easy to calculate, one can replace the Stein discrepancy by the Kullback-Leibler divergence, which
is equivalent to the maximum likelihood estimation.  We present details for model specifications in
various forms and training algorithm in Appendix E.2.

3.2    REGULARIZATION EFFECTS BY VIRTUE OF THE STEIN BRIDGE

The  intuitive  motivation  of  the  Stein  bridge  term  in  (3)  is  to  push  the  two  models  
to  achieve  a
consensus. In this subsection, we theoretically show that the Stein bridge allows the two models to
reinforce each other by imposing regularizations on the critics.

3.2.1    KERNEL SOBOLEV DUAL NORM REGULARIZATION ON THE WASSERSTEIN CRITIC

We show the regularization effect of the Stein bridge on the Wasserstein critic.  Define the kernel
Sobolev dual norm as


D  H−1 (P;k)  :=   sup

u∈C0∞

.⟨D, u⟩L2 (P)  :  Eₓ,ₓ'∼P[∇u(x)Tk(x, x′)∇u(x′)] ≤ 1,  EP[h] = 0Σ .

3


Under review as a conference paper at ICLR 2020


Figure  1:   Model  framework  for  Stein  Bridging  which
jointly train an implicit sample generator and an explicit

Model                           Objective

GAN                                    1

Energy Model                             2

Energy-based GAN

(Zhao et al. (2017))                      D1

Contrastive Learning

(Kim & Bengio (2017))                  D2

Cooperative Learning

(Xie et al. (2018))                 D2 + D3

     Stein Bridging (ours)         D1 + D2 + D3    

Table  1:   Comparison  of  objectives
between   different   generative   mod-
els,   where  D₁    :=    D₁(Prₑₐl, PG),

D₂   := PD₂(Prₑₐl, PE)  and  D₃   :=


(unnormalized) density estimator via a Stein bridge.

D₃(PG,   E) denote general statistical

which can be viewed as a kernel generalization of the Sobolev dual norm defined in Section 2, which
reduces to the Sobolev dual norm when k(x, x′) = I(x = x′) and P being the Lebesgue measure.
We have the following result.

Theorem 1.  Assume that   PG  G exhausts all continuous probability distributions and     is chosen
as kernel Stein discrepancy. Then problem (3) is equivalent to


min max .E

[D(y)] − E

  1  

[D(x)] −          ǁDǁ                      .

According to Section 2, the regularization term would penalize the non-smoothness of the Wasser-
stein critic D, which is in the same spirit of gradient-based penalty (e.g., Gulrajani et al. 
(2017);
Roth et al. (2017)), but with a new way to encouraging smoothness.

Another way to interpret the Sobolev dual norm penalty is by observing that if k(x, x′) = I(x = x′)

and and EPE [D] = 0 (Villani, 2008), then


ǁDǁ

= lim W₂((1 + ϵD)PE, PE) ,


H−1 (PE ;k)

s→0                           ϵ

where     ₂ denotes the 2-Wasserstein metric. Therefore, the regularization ensures that D would not
change suddenly on the high-density region of PE, and the explicit model reinforces the learning of
the Wasserstein critic.

3.2.2    LIPSCHITZ REGULARIZATION ON THE STEIN CRITIC

We next investigate how the Stein bridge helps to regularize the Stein critic.  Recall that the two
Stein terms in (3) share the same Stein critic. We have the following result.

Theorem 2.  Assume   PG  G exhausts all continuous probability distributions, and the Stein class
defining the Stein discrepancy is compact (in some linear topological space).  Then problem (3) is
equivalent to

min max{λ₁S(Prₑₐl, PE) + λ₂Eₓ∼Preal [Mλ2 APE f (x)]},

where Mλ2 APE f (·) denotes the (generalized) Moreau-Yosida regularization of the function APE f

 1 

with parameter λ₂, i.e., Mλ2 APE f (x) = miny∈X {APE f (y) +     ||x − y||}.

Theorem 2 shows that the Stein bridge, together with the Wasserstein metric     (Prₑₐl, PG), plays 
as
a smoothness regularization on the Stein critic f via Moreau-Yosida regularization, which smoothens
the Stein operator    PE f and further encourages the energy model to seek more modes in data 
instead
of focusing on some dominated modes, thus helps alleviate the mode-collapse issue. To the best of
our knowledge, this suggests a novel regularization scheme for Stein-based GAN.

4


Under review as a conference paper at ICLR 2020

4    CONVERGENCE  ANALYSIS

In Section 3.2, we justify Stein Bridging by showing the regularization effects.  In this section, 
we
further show that it could help to stabilize GAN training with local convergence guarantee.  To this
end, we first compare the behaviors of WGAN, likelihood- and entropy-regularized WGAN, and
our Stein Bridging under SGD via an easy to comprehend toy example.  Then we give a formal
result that interprets why the introduction of (unnormalized) density estimator could stablize GAN
training and help for convergence.

4.1    ANALYSIS OF A LINEAR SYSTEM

The training for minimax game in GAN is difficult.  When using traditional gradient methods, the
training would suffer from some oscillatory behaviors (Goodfellow (2017); Liang & Stokes (2019)).
In order to better understand the optimization behaviors, we first study a one-dimension linear sys-
tem that provides some insights on this problem. Note that such toy example (or a similar one) is 
also
utilized by Gidel et al. (2019); Nagarajan & Kolter (2017) to shed lights on the instability of WGAN
training².  Consider a linear critic Dψ(x) = ψx and generator Gθ(z) = θx.  Then the Wasserstein
GAN objective can be written as a constrained bilinear problem: minθ max ψ   ₁ ψE[x]     ψθE[z],
which could be further simplified as an unconstrained version (the behaviors could be generalized
to multi-dimensional cases (Gidel et al. (2019))):

min max ψ − ψ · θ.                                                           (4)

Unfortunately, such simple objective cannot guarantee convergence by traditional gradient methods
like SGD with alternate updating³: θk₊₁ = θk + ηψk,, ψk₊₁ = ψk + η(1     θk₊₁). Such optimiza-
tion would suffer from an oscillatory behavior, i.e., the updated parameters go around the optimum
point ([ψ∗, θ∗]  =  [0, 1]) forming a circle without converging to the centrality, which is shown in
Fig. 2(a). A recent study in Liang & Stokes (2019) theoretically show that such oscillation is due 
to
the interaction term in (4).

One solution to the instability of GAN training is to add (likelihood) regularization, which has 
been
widely studied by recent literatures (Warde-Farley & Bengio (2017); Li & Turner (2018)).  With
regularization term, the objective changes into minθ max|ψ|≤₁ ψE[x]     ψθE[z]     λE[log µ(θz)],
where µ( ) denotes the likelihood function and λ is a hyperparameter.  A recent study (Tao et al.
(2019)) proves that when λ < 0 (likelihood-regularization), the extra term is equivalent to maximiz-
ing sample evidence, helping to stabilize GAN training; when λ > 0 (entropy-regularization), the
extra term maximizes sample entropy, which encourages diversity of generator. Here we consider a
Gaussian likelihood function for generated sample x′, µ(x′) = exp(    ¹ (x′      b)²) which is up 
to a
constant, and then the objective becomes (see Appendix D.1 for details):

min max ψ − ψ · θ − λ(θ² − θ).                                                (5)


θ

The  above  system  would  converge
with   λ     <     0   and   diverge   with

ψ

3.0

2.5

Stein Bridging
WGAN+LR
WGAN+LR+VA

2.00

1.75

1  =  1,   2  =  1

1  =  1,   2  =  5

1  =  1,   2  =  0.2


λ  >  0  in  gradient-based  optimiza-

2.0

WGAN+ER

1.50

1  =  5,   2  =  1


tion,  shown  in  Fig.  2(a).    Another
issue  of  likelihood-regularization  is
that  the  extra  term  changes  the  op-
timum  point  and  makes  the  model
converge to a biased distribution,  as
proved by Tao et al. (2019).   In this

1.5

1.0

0.5

0.0

0.5

1.0

WGAN

Optimum Point

3.0         2.5         2.0         1.5         1.0         0.5       0.0         0.5         1.0   
      1.5

(a)

1.25

1.00

0.75

0.50

0.25

1  =  0.2,   2  =  1

1.0                          0.5                         0.0                          0.5           
               1.0

(b)


case, one can verify that the optimum
point becomes [ψ∗, θ∗] = [   λ, 1], re-
sulting a bias. To avoid this issue, Tao
et al. (2019) proposes to temporally
decrease |λ|  through training.  How-

Figure   2:     (a)   Numerical   iterations   for   SGD   training

of  WGAN,  likelihood-regularized  WGAN  (WGAN+LR),
variational  annealing  for  WGAN+LR  (WGAN+LR+VA),
entropy-regularized  WGAN  (WGAN+ER)  and  our  Stein
Bridging.        (b) Stein Bridging with different λ₁ and λ₂.

ever,  such method would also be stuck in oscillation when   λ   gets close to zero as is shown in
Fig. 2(a).

²Our theoretical discussions focus on WGAN, and we also compare with original GAN in the 
experiments.

³Here, we adopt the most widely used alternate updating strategy. The simultaneous updating, i.e., 
θk₊₁ =

θk + ηψk and ψk₊₁ = ψk + η(1 − θk), would diverge in this case.

5


Under review as a conference paper at ICLR 2020

Finally, let us consider our proposed model. We also simplify the density estimator as a basic 
energy
model pφ(x)  =  exp(    ¹ x²      φx) whose score function is     ₓ log pφ(x)  =     x     φ.  Then 
if we
specify the two Stein discrepancies in (3) as KSD, we have the objective (see Appendix D.1 for
details),

min max min ψ − ψ · θ + λ1 (1 + φ)² + λ2 (θ + φ)².                               (6)

Interestingly, one can verify that for ∀λ₁, λ₂, the optimum point remains the same [ψ∗, θ∗, φ∗]  =
[0, 1, −1]. Then we show that the optimization can guarantee convergence to [ψ∗, θ∗, φ∗].

Proposition 1.  Using alternate SGD for (6) geometrically decreases the square norm Nt = |ψᵗ|2 +

|θ − 1|2  + |φ + 1|2, for any 0 < η < 1 with λ₁ = λ₂ = 1,

Nt₊₁ = (1 − η²(1 − η)²)Nt.                                                   (7)

In Fig. 2(a),  we can see that Stein Bridging achieves a good convergence to the right optimum.
Compared with (4), the objective (6) adds a new bilinear term φ   θ, which acts like a connection
between the two generator and estimator, and two other quadratic terms,  which help to push the
values to decrease through training.   The added terms and the original terms in (6) cooperate to
guarantee convergence to a unique optimum. (More discussions in Appendix D.1).

We further generalize the analysis to multi-dimensional bilinear system F (ψ, θ) = θTAψ    bTθ
cTψ  which is extensively used by researches for analysis of GAN stability (Goodfellow (2017);
Gemp & Mahadevan (2018); Liang & Stokes (2019); Gidel et al. (2019)).  For any bilinear system,

with the added term H(φ, θ) =  ¹ (θ + φ)TB(θ + φ) where B = (AAT) 1   to the objective, we

2

can prove that i) the optimum point remains the same as the original system (Proposition 2) and ii)

using alternate SGD algorithm for the new objective can guarantee convergence (Theorem 4).  The
results are given in Appendix D.3.

4.2    LOCAL CONVERGENCE FOR A GENERAL MODEL

To study the convergence for Stein Bridging, we proceed to consider a general optimization 
objective

min max min L(θ, ψ, φ),

θ       ψ       φ

where L(θ, ψ, φ) = F (θ, ψ)+H(θ, φ), and ωf = [θ, ψ] and ωh = [θ, φ] (θ is a shared parameter
set).  Use ω∗  = [θ∗, ψ∗, φ∗] to denote the optimum point of L and ωf∗   = [θ∗, ψ∗], ωh∗   = [θ∗, 
φ∗]
represent the optimum points of F and H respectively. Define Ωf = Ωθ    Ωψ and Ωh = Ωθ    Ωφ,
where Ωθ, Ωψ, Ωφ denote constraint sets for θ, ψ, φ respectively. Function H is µ-strongly convex,
and F  is µ-strongly convex for θ and µ-strongly concave for ψ (see Appendix E.4 for definition of
strongly convex condition). Here we define h(ωh) =     θH +    φH, f (ωf ) =     θF         ψF , and
then we have the following theorem.

Theorem 3.  If F  is µ-strongly convex-concave and H  is µ-strongly convex, we can leverage the
alternate SGD algorithm, i.e.


ωt+1  = PΩ

(ωᵗ⁺¹/² − ηh(ωᵗ⁺¹/²)),                                            (8)

ωᵗ⁺¹ = PΩ  (ωᵗ⁺¹/² − ηf (ωᵗ⁺¹/²)),                                            (9)

where ωt+1/2   = [θt, ψt], ωt+1   = [θt+1/2, ψt+1], ωt+1/2   = [θt+1/2, φt], ωt+1   = [θt+1, φt+1],

h                                    h                                              f               
                              f

and PΩ(ω) = arg min ǁω − ω′ǁ2  denotes the projection mapping to Ω.  Then we can achieve the

ω'∈Ω

convergence by using  ¹   < η <  ¹ .

2µ                µ

Theorem 3 shows that Stein Bridging could converge to at least a local optimum.  Due to the un-
known and intricate landscape of deep neural networks, the global optimization and convergence
analysis for GAN has remained as an unexplored problem.  Despite the fact that strong convexity
assumption cannot be guaranteed with deep neural networks,  the optimization could converge to
a stable point once there exists a local region that satisfies the strongly convex conditions.  In 
the
experiments, we will empirically compare the training stability of each method on various datasets
to validate our theoretical discussions.

6


Under review as a conference paper at ICLR 2020


(a) True        (b) GAN      (c) WGAN      (d) DGM     (e) Joint-JS    (f) Joint-W

(a) True        (b) DEM       (c) EGAN      (d) DGM     (e) Joint-JS    (f) Joint-W

5    EXPERIMENTS

Figure 3:  (a) True sam-
ples and (b)   (f) gener-
ated  samples  produced
by   the   generators   of
different   methods   on
Two-Circle (upper line)
and Two-Spiral (bottom
line) datasets.

Figure 4:  (a) True den-
sities   and   (b)   (f)   es-
timated  densities  given
by   the   estimators   of
different   methods   on
Two-Circle (upper line)
and Two-Spiral (bottom
line) datasets.

In this section, we conduct experiments to verify the effectiveness of proposed method from multi-
faceted views.  First, we select three tasks with different evaluation metrics in Section 5.1, 5.2 
and

5.3.  Then we further discuss some applications of joint training as well as some useful tricks in
Section 5.4, 5.5 and 5.6. The codes will be released later.

We consider two synthetic datasets with mixtures of Gaussian distributions:  Two-Circle and Two-
Spiral.  The first one is composed of 24 Gaussian mixtures that lie in two circles.  Such dataset is
extended from the 8-Gaussian-mixture scenario which is widely used in previous GAN papers and
is more difficult, so that we can use it to test the quality of generated samples and mode coverage
of learned energy.  The second synthetic dataset consists of 100 Gaussian mixtures whose centers
are densely arranged on two centrally symmetrical spiral-shaped curves.  This dataset can be used
to  examine  the  power  of  generative  model  on  complicated  data  distributions.   The  
ground-truth
distributions and samples are shown in Fig. 3 (a) and Fig. 4 (a).  Furthermore, we also apply the
methods to MNIST and CIFAR datasets which require the model to deal with high-dimensional
data. In each dataset, we use observed samples as input of the model and leverage them to train the
generators and the estimators. The details for each dataset are reported in Appendix E.1.

In our experiments, we also replace the Wasserstein metric in (3) by JS divergence.  To well distin-
guish different specifications, we term the model Joint-W if using Wasserstein metric and Joint-JS
if using JS divergence in this section.  We consider several competitors.  First, for implicit 
genera-
tive models, we consider valina GAN, WGAN-GP (Gulrajani et al. (2017)), likelihood-regularized
GAN/WGAN-GP (short as GAN+LR/WGAN+LR), entropy-regularized GAN/WGAN-GP (short
as GAN+ER/WGAN+ER) and a recently proposed variational annealing regularization (Tao et al.
(2019)) for GAN (short as GAN+VA/WGAN+VA) to compare the quality of generated samples. We
employ the denoising auto-encoder to estimate the gradient for regularization penalty, which is pro-
posed by Alain & Bengio (2014) and utilized by Tao et al. (2019). Second, for explicit models, we
consider Deep Energy Model (DEM) which is optimized based on Stein discrepancy, and energy-
based GAN (EGAN) (Dai et al. (2017)). Besides, we also compare with Deep Directed Generative
(DGM) Model (Kim & Bengio (2017)) which adopts contrastive divergence to unite sample gener-
ator and energy estimator.  See Appendix A for brief introduction of these methods and Appendix

E.3 for implementation details for each method.

5.1    SAMPLE QUALITY OF IMPLICIT MODEL

Calibrating explicit (unnormalized) density model with implicit generator is expected to improve
the quality of generated samples. In Fig. 3 and Fig. 4 we show the results of different generators 
in
Two-Circle and Two-Spiral datasets. As we can see, in Two-Circle, there are a large number of gen-

7


Under review as a conference paper at ICLR 2020


MNIST (Conditional)
Method          Score     CEPC

DCGAN          8.43       0.168

DCGAN+LR       8.40       0.171

DCGAN+ER       8.33       0.179

DCGAN+VA       8.40       0.172

DGM             8.15       0.201

Joint-JS(ours)      8.53       0.156

MNIST (Unconditional)
Method          Score     CEPC

WGAN-GP        7.71       0.256

WGAN+LR        7.82       0.243

WGAN+ER        7.75       0.252

WGAN+VA        7.74       0.254

DGM             6.87       0.372

Joint-W(ours)      7.90       0.231

CIFAR-10 (Unconditional)
Method          Score     CEPC

WGAN-GP        6.80       0.153

WGAN+LR        6.89       0.154

WGAN+ER        6.99       0.156

WGAN+VA       6.95        0.154

DGM             4.79       0.146

Joint-W(ours)      7.11       0.151

Table 2:  Inception scores (higher is better) and conditional entropies (short as CEPC and lower is
better) on MNIST and CIFAR-10. † We directly use the best result reported in their paper.


0.200

0.175

0.150

0.125

0.100

0.075

0.035

0.030

0.025

0.020

0.015

0.010

Joint-JS
GAN

GAN + LR
GAN + ER

0.200

0.175

0.150

0.125

0.100

0.075

0.030

0.025

0.020

0.015

0.010

0.005

Joint-W
WGAN
WGAN                  + LR
WGAN + ER

450

400

350

300

205

200

195

190

185

Joint-JS
DCGAN
DCGAN                     + LR
DCGAN + ER

550

500

450

400

350

300

205

200

195

190

185

180

Joint-W
WGAN
WGAN                   + LR
WGAN + ER


0.050

0.025

0.000

60000         70000         80000         90000        100000

Iteration

0                 20000             40000             60000             80000            100000

Iteration

0.050

0.025

0.000

60000        70000        80000        90000        100000

Iteration

0               20000           40000           60000           80000           100000

Iteration

250

200

0            50

200      225      250      275      300      325      350      375

Iteration

100         150         200         250         300         350

Iteration

400

250

200

0           50

200     225     250     275     300     325     350     375

Iteration

100        150        200        250        300        350

Iteration

400


(a) Two-Circle

(b) Two-Spiral

(c) MNIST (Condition)

(d) MNIST (Uncondition)

Figure 5:  Learning curves of Joint-W (resp.  Joint-JS) compared with WGAN (resp.  GAN or DC-
GAN) and its regularization-based variants.

erated samples given by GAN, WGAN-GP and DGM (the worst one in this case) locating between
two Gaussian components, and the boundary for each component is not distinguishable.  Since the
ground-truth densities of regions between two components are very low, such generated samples
possess low-quality, which depicts that these models capture the combinations of two dominated
features (i.e., modes) in the data but such combination does not make sense in practice.  By con-
trast, Joint-JS and Joint-W could alleviate such issue, reduce the low-quality samples and produce
more distinguishable boundaries for components.  In Two-Spiral, similarly, the generated samples
given by GAN and WGAN-GP form a circle instead of two spirals while the samples of DGM ‘link’
two spirals.  Joint-JS manages to focus more on true high densities compared to GAN and Joint-W
provides the best results. To quantitatively measure the sample quality, we adopt two metrics: Maxi-
mum Mean Discrepancy (MMD) and High-quality Sample Rate (HSR). The detailed definitions are
given in Appendix E.4 and we report the results in Table 5.

We visualize the generated digits/images on MNIST/CIFAR-10 datasets in Fig. 9 and Fig. 10 and
use Inception Score and conditional entropy of predicted classes (CEPC) to measure the sample
quality (See Appendix E.4 for details).  As shown in Table 2, Joint-W (resp. Joint-JS) is superior
than WGAN-GP (resp. DCGAN), regularized WGAN (resp. DCGAN) and DGM. The CEPC char-
acterizes how well the picture can be distinguished by a pre-trained classifier, i.e., the quality 
of
picture, so the results depict that proposed method could give higher-quality generated pictures.

5.2    DENSITY ESTIMATION OF EXPLICIT MODEL

Another advantage of joint learning is that the generator could help the density estimator to 
capture
more accurate distribution.  As shown in Fig 3, both Joint-JS and Joint-W manage to capture all
Gaussian components while other methods miss some of modes.  In Fig 4, Joint-JS and Joint-W
exactly fit the ground-truth distribution. By contrast, DEM misses one spiral while EGAN degrades
to    a uniform-like distribution. DGM manages to fit two spirals but allocate high densities to 
regions
that have low densities in the groung-truth distribution. To quantitatively measure the performance,
we introduce three evaluation metrics: KL & JS divergence between the ground-truth and estimated
densities and Area Under the Curve (AUC) for false-positive rate v.s.  true-positive rate where we
select points with true high (resp. low) densities as positive (resp. negative) examples. The 
detailed
information and results are given in Appendix E.4 and Table 5 respectively.  The values show that
Joint-W  and  Joint-JS  could  provide  more  accurate  (unnormalized)  density  estimation  than  
other
competitors.

We also rank the generated digits (and true digits) on MNIST w.r.t the densities given by the energy
model in Fig. 11, Fig. 12 and Fig. 13.  As depicted in the figures, the digits with high densities 
(or

8


Under review as a conference paper at ICLR 2020


0.85

0.80

Joint(W)

WGAN

0.75

0.70

0.75

0.65


0.70

0.65

0.60

0.55

Joint-W

DEM


0             10            20            30            40            50

Noise Ratio (%)

(a) Noised Data

500             1000            1500            2000

True Sample Size

(b) Insufficient Data

7.90

7.85

7.80

7.75

7.70

7.65


Figure 6:  Generated digits (resp.  images) given by the
same  noise  z  in  adjacent  training  epoches  on  MNIST
(reps. CIFAR) dataset.

2000                   4000                   6000                   8000                  10000

# of Iterations Before Joint Training

(c) Warm up on CIFAR-10

Figure 7:  Joint-W with (a) noised data,

(b) insufficient data and (c) ‘warm up’
iterations before joint training.

low densities) given by Joint-JS possess enough diversity (the thickness, the inclination angles as
well as the shapes of digits diverses). By constrast, all the digits with high densities given by 
DGM
tend to be thin and digits with low densities are very thick.  Also, as for EGAN, digits with high
(or low) densities appear to have the same inclination angle (for high densities, ‘1’ keeps straight
and ‘9’ ’leans’ to the left while for low densities, just the opposite).  Such phenomenon indicates
that DGM and EGAN tend to allocate high (or low) densities to data with certain modes and would
miss some modes that possibly possess high densities in ground-truth distributions. Fortunately, our
method overcomes the issue and manages to capture complicated distributions.

5.3    ENHANCING THE STABILITY OF GAN

Our discussions and analysis show that joint training helps to stabilize GAN training.   In Fig. 5
we present the learning curves of Joint-W (resp.  Joint-JS) compared with WGAN (resp.  GAN or
DCGAN) and its regularization-based variants on different datasets.  One can clearly see from the
curves that joint training could reduce the variance of metric values especially during the second 
half
of training. Furthermore, we visualize the generated pictures given by the same noise z in adjacent
epoches in Fig. 6. The results show that Joint-W outputs more stable generation in adjacent epoches
while the generated samples given by WGAN-GP and WGAN+VA exhibit an obvious variation.
Especially, some digits generated by WGAN-GP and WGAN+VA change from one class to another.
Such phenomenon is quite similar to the oscillatory behavior with non-convergence in optimization
that we discuss in Section 4.1.

Another issue discussed in Section 4.1 is the bias of model distribution for regularized GAN meth-
ods.   To  quantify  this  evaluation,  we  calculate  l₁  and  l₂  distances  between  the  means  
of  50000
generated digits (resp.  images) and 50000 true digits (resp.  images) in MNIST (reps.  CIFAR-10).
The results are shown in Table 3. The smaller distances given by Joint-W indicate that it converges
to a better local optimum with smaller bias from the original data distribution. Also, in Table 6 
(resp.
Table 7), we report the distances for digits (resp. images) in each class on MNIST (resp. CIFAR).

5.4    DETECTING OUT-OF-DISTRIBUTION SAMPLES

The explicit model estimates densities for each sample and one of its applications is to detect 
outliers
in the input data.  Here, we adopt CIFAR-10 to measure the ability of our estimator to distinguish
the in-distribution samples and (true/false) out-of-distribution samples. We consider four 
situations
and in each case, we consider the test images of CIFAR-10 as positive set (expected to allocate high
densities) and construct a negative set (expected to allocate low densities). We let the model 
output
densities for images in two sets, rank them according to the densities and plot the ROC curve for
false-positive rate v.s. true-positive rate in Fig. 8. In the first case, we flip each image in the 
positive
set             as negative set. Note that such flipped images are not out-of-distribution samples, 
so the model is
expected to allocate high densities to them, i.e., the ROC curve should be close to a straight line 
from
(0,     0) to (1, 1).  The results show that Joint-W, EGAN and DEM give the exact results while DGM
assigns all flipped images with lower densities, which means that it fails to capture the semantics

9


Under review as a conference paper at ICLR 2020


MNIST                   CIFAR

Method         l1 Dis     l2 Dis       l1 Dis      l2 Dis
WGAN-GP      13.80       0.93         80.98        1.72

1.00

0.75

0.50

0.25

0.00

Joint-W                  DEM                DGM              EGAN

1.00

0.75

0.50

0.25

0.00


WGAN+LR
WGAN+ER
WGAN+VA
DGM

Joint-W

12.91       0.86

12.26       0.77

12.38       0.78

12.12       0.79

11.82       0.73

82.96        1.81

72.28        1.59

69.01        1.53

179.30       3.95

64.23        1.41

1.00

0.75

0.50

0.25

0.00

0.0      0.2      0.4      0.6      0.8      1.0

(a) CIFAR10 Flip

0.0      0.2      0.4      0.6      0.8      1.0

1.00

0.75

0.50

0.25

0.00

0.0      0.2      0.4      0.6      0.8      1.0

(b) Random Noise

0.0      0.2      0.4      0.6      0.8      1.0


Table 3:  Distances between means of generated digits

(c) CIFAR10 Overlay

(d) Lsun


(resp.  images) and ground-truth digits (resp.  images)
on MNIST (resp. CIFAR-10).

Figure 8: ROC curves for evaluation of out-
lier detection on CIFAR-10.

in images.  In the following three cases, we i) generate random noise, ii) average two images with
different CIFAR classes, and iii) adopt Lsun Bedroom dataset as the negative set, respectively.  In
these situations, the model is expected to distinguish the images in two sets.  The results in Fig. 
8
show that DGM provides the best results while the performance of Joint-W is quite close to DGM
and much better than DEM and EGAN.

5.5    ADDRESSING DATA INSUFFICIENCY AND NOISY DATA

We proceed to test the model performance in some extreme situations where the observed samples
are mixed with noises or the observed samples are quite insufficient.  The results are presented in
Fig. 7(a) where we add different ratios of random noise to the true samples in Two-Circle dataset
and Fig. 7(b) where we only sample insufficient data for training in Two-Spiral dataset. The details
are in Appendix E.1.  The noise in data impacts the performance of WGAN and Joint-W, but com-
paratively, the performance decline for Joint-W is less insignificant than WGAN, which indicates
better robustness of joint training w.r.t noised data. In Fig. 7(b), when the sample size decreases 
from
2000 to 100, the AUC value of DEM declines dramatically, showing its dependency on sufficient
training samples.  By contrast, the AUC of Joint-W exhibits a small decline when the sample size
is more than 500 and suffers from an obvious decline when it is less than 300.  Such phenomenon
demonstrates lower sensitivity of joint training to observed sample size.

5.6    WHEN TO START JOINT LEARNING

In our experiment, we also observe an interesting phenomenon:  the performance achieved at con-
vergence would be better if we start joint training after some iterations with independent training 
for
the generator and the estimator. In other words, at the beginning, we could set λ₂ = 0 (or some very
small values) in (3) and after some iterations set it as a normal level. We report the inception 
scores
on MNIST with different numbers of iterations for independent training in Fig. 7(c) where we can
see that the score firstly goes up and then goes down when we increase iterations for independent
training.  Such phenomenon is quite similar to the ‘warm up’ trick used for training deep networks
where one can use small learning rates at iterations in the begining and amplify its value for 
further
training.  One intuitive reason behind this phenomenon is that at the beginning, both the genera-
tor and estimator are weak and if we minimize the discrepancy between them at this point,  they
would possibly constrain each other and get limited in some bad local optima.  When they become
strong enough after some training iterations, uniting them through joint training would help them
compensate and reinforce each other as our discussions.

6    CONCLUSIONS

In this paper, we aim at uniting the training for implicit generative model (represented by GAN) and
explicit generative model (represented by a deep energy-based model).  Besides two loss terms for
GAN and energy-based model, we introduce the third loss characterized via Stein discrepancy be-
tween the generator in GAN and the energy-based model. Theoretically, we show that joint training
could i) help to stablize GAN training and facilitate its convergence, and ii) enforcing dual regu-
larization effects on both models and help to escape from local optima in optimization.  We also
conduct extensive experiments with different tasks and application senarios to verify our 
theoretical
findings as well as demonstrate the superiority of our method compared with various GAN models
and deep energy-based models.

10


Under review as a conference paper at ICLR 2020

REFERENCES

Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating
distribution. J. Mach. Learn. Res., 15(1):3563–3593, 2014.

Mart´ın Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks.
In ICML, pp. 214–223, 2017.

Andrew Brock,  Jeff Donahue,  and Karen Simonyan.   Large scale GAN training for high fidelity
natural image synthesis. In ICLR, 2019.

Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton.  A kernel test of goodness of fit.  In

ICML, pp. 2606–2615, 2016.

Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard H. Hovy, and Aaron C. Courville. Calibrat-
ing energy-based generative adversarial networks. In ICLR, 2017.

Chao Du, Kun Xu, Chongxuan Li, Jun Zhu, and Bo Zhang. Learning implicit generative models by
teaching explicit ones. CoRR, abs/1807.03870, 2018.

Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. CoRR,
abs/1903.08689, 2019.

L.C. Evans and American Mathematical Society.  Partial Differential Equations.  Graduate studies
in mathematics. American Mathematical Society, 2010.  ISBN 9781470411442.  URL https:

//books.google.com/books?id=UL9WtAEACAAJ.

Rui Gao and Anton J Kleywegt.   Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.

Rui Gao, Xi Chen, and Anton J Kleywegt.  Wasserstein distributional robustness and regularization
in statistical learning. arXiv preprint arXiv:1712.06050, 2017.

Stuart  Geman  and  Donald  Geman.   Stochastic  relaxation,  gibbs  distributions,  and  the  
bayesian
restoration of images. IEEE Trans. Pattern Anal. Mach. Intell., 6(6):721–741, 1984.

Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of gans using variational
inequalities. CoRR, abs/1808.01531, 2018.

Gauthier Gidel, Hugo Berard, Gae¨tan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A varia-
tional inequality perspective on generative adversarial networks. In ICLR, 2019.

Ian J. Goodfellow.  NIPS 2016 tutorial:  Generative adversarial networks.  CoRR, abs/1701.00160,
2017.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio.  Generative adversarial nets.  In NIPS, pp. 2672–2680,
2014.

Jackson Gorham and Lester Mackey.  Measuring sample quality with stein’s method.  In Advances
in Neural Information Processing Systems, pp. 226–234, 2015.

Ishaan Gulrajani, Faruk Ahmed, Mart´ın Arjovsky, Vincent Dumoulin, and Aaron C. Courville.  Im-
proved training of wasserstein gans. In NIPS, pp. 5767–5777, 2017.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.  Reinforcement learning with
deep energy-based policies. In ICML, pp. 1352–1361, 2017.

Geoffrey E. Hinton. Product of experts. In ICANN99 Artificial Neural Networks, 1999.

Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh.  A fast learning algorithm for deep belief
nets. Neural Computation, 18(7):1527–1554, 2006.

Tianyang Hu, Zixiang Chen, Hanxi Sun, Jincheng Bai, Mao Ye, and Guang Cheng.  Stein neural
sampler. CoRR, abs/1810.03545, 2018.

11


Under review as a conference paper at ICLR 2020

Aapo Hyva¨rinen.   Estimation of non-normalized statistical models by score matching.   J. Mach.
Learn. Res., 6:695–709, 2005.

Taesup Kim and Yoshua Bengio.  Deep directed generative models with energy-based probability
estimation. In ICLR, 2017.

Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.

Yann LeCun, Sumit Chopra, Raia Hadsell, MarcAurelio Ranzato, and Fu Jie Huang.  A tutorial on
energy-based learning. Predicting Structured Data, MIT Press, 2006.

Yingzhen Li and Richard E. Turner.  Gradient estimators for implicit models.  In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings, 2018.

Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. In AISTATS, pp. 907–915, 2019.

Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In NIPS, pp. 2370–2378, 2016.

Qiang Liu and Dilin Wang.   Learning deep energy models:  Contrastive divergence vs. amortized
MLE. CoRR, abs/1707.00797, 2017.

Qiang Liu, Jason D. Lee, and Michael I. Jordan.  A kernelized stein discrepancy for goodness-of-fit
tests. In ICML, pp. 276–284, 2016.

Vaishnavh Nagarajan and J. Zico Kolter.  Gradient descent GAN optimization is locally stable.  In

NIPS, pp. 5585–5595, 2017.

Radford M. Neal. Stochastic relaxation, gibbs distributions, and the bayesian restoration of 
images.

Handbook of Markov Chain Monte Carlo, 2, 2011.

Jiquan Ngiam, Zhenghao Chen, Pang Wei Koh, and Andrew Y. Ng.  Learning deep energy models.
In ICML, pp. 1105–1112, 2011.

Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski.   Plug & play
generative networks:  Conditional iterative generation of images in latent space.   In CVPR, pp.
3510–3520, 2017.

Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Yingnian Wu.   On learning non-convergent non-
persistent short-run mcmc toward energy-based model. CoRR, abs/1904.09770, 2019.

Chris J Oates, Mark Girolami, and Nicolas Chopin. Control functionals for monte carlo integration.

Journal of the Royal Statistical Society, Series B, 2017.

Alec Radford, Luke Metz, and Soumith Chintala.  Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2016.

Garry Robins, Pip Pattison, Yuval Kalish, and Dean Lusher. An introduction to exponential random
graph (p*) models for social networks. Social Networks, 29(2):173–191, 2007.

Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann.  Stabilizing training of
generative adversarial networks through regularization.  In Advances in neural information pro-
cessing systems, pp. 2018–2028, 2017.

Ruslan Salakhutdinov and Geoffrey E. Hinton.  Deep boltzmann machines.  In AISTATS, pp. 448–
455, 2009.

Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczyn´ski. Lectures on stochastic program-
ming: modeling and theory. SIAM, 2009.

Chenyang Tao, Shuyang Dai, Liqun Chen, Ke Bai, Junya Chen, Chang Liu, Ruiyi Zhang, Georgiy V.
Bobashev, and Lawrence Carin. Variational annealing of gans: A langevin perspective. In ICML,
pp. 6176–6185, 2019.

12


Under review as a conference paper at ICLR 2020

Ce´dric Villani.  Optimal transport: old and new, volume 338.  Springer Science & Business Media,
2008.

David Warde-Farley and Yoshua Bengio. Improving generative adversarial networks with denoising
feature  matching.   In  5th  International  Conference  on  Learning  Representations,  ICLR  2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.

Nik Weaver. Lipschitz algebras. World Scientific, 1999.

Ying Nian Wu, Song Chun Zhu, and Xiuwen Liu.  Equivalence of julesz ensembles and FRAME
models. International Journal of Computer Vision, 38(3):247–265, 2000.

Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu.  Cooperative training of descriptor and
generator networks. CoRR, abs/1609.09408, 2016a.

Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu.  A theory of generative convnet.  In

ICML, pp. 2635–2644, 2016b.

Jianwen Xie, Yang Lu, Ruiqi Gao, and Ying Nian Wu. Cooperative learning of energy-based model
and latent variable model via MCMC teaching. In AAAI, pp. 4292–4301, 2018.

Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei Zhang. Deep structured energy based models
for anomaly detection. In ICML, pp. 1100–1109, 2016.

Junbo Jake Zhao,  Michae¨l Mathieu,  and Yann LeCun.   Energy-based generative adversarial net-
works. In ICLR, 2017.

Song Chun Zhu, Ying Nian Wu, and David Mumford. Minimax entropy principle and its application
to texture modeling. Neural Computation, 9(8):1627–1660, 1997.

13


Under review as a conference paper at ICLR 2020

A    LITERATURE  REVIEWS

We discuss some of related literatures and shed lights on the relationship between our work with
others.

A.1    EXPLICIT GENERATIVE MODELS

Explicit generative models are interested in fitting each instance with a scaler (unnormalized) den-
sity expected to explicitly capture the distribution behind data.   Such densities are often up to a
constant and called as energy functions which are common in undirected graphical models (LeCun
et    al. (2006)). Hence, explicit generative models are also termed as energy-based models. An 
early
version of energy-based models is the FRAME (Filters,  Random field,  And Maximum Entropy)
model (Zhu et al. (1997); Wu et al. (2000)). Later on, some works leverage deep neural networks to
model the energy function (Ngiam et al. (2011); Xie et al. (2016b)) and pave the way for researches
on deep energy model (DEM) (e.g., Liu & Wang (2017); Kim & Bengio (2017); Zhai et al. (2016);
Haarnoja et al. (2017); Du & Mordatch (2019); Nijkamp et al. (2019)).  Apart from DEM, there
are also some other forms of deep explicit models based on restricted Boltzmann machines like
deep belief networks (Hinton et al. (2006)) and deep Boltzmann machines (Salakhutdinov & Hinton
(2009)).

The normalized constant under the energy function requires an intractable integral over all possible
instances, which makes the model hard to learn via Maximum Likelihood Estimation (MLE). To
solve this issue, some works propose to approximate the constant by MCMC methods (Geman &
Geman (1984); Neal (2011)).  However, MCMC requires an inner-loop samples in each training,
which induces high computational costs. Another solution is to optimize an alternate surrogate loss
function.  For example, contrastive divergence (CD) (Liu & Wang (2017)) is proposed to measure
how much KL divergence can be improved by running a small numbers of Markov chain steps to-
wards the intractable likelihood, while score matching (SM) (Hyva¨rinen (2005)) detours the constant
by minimizing the distance for gradients of log-likelihoods.  Moreover, the intractable normalized
constant makes it hard to sample from. To obtain an accurate samples from unnormalized densities,
many studies propose to approximate the generation by diffusion-based processes, like generative
flow (Nguyen et al. (2017)) and variational gradient descent (Liu & Wang (2016)).  Also, a recent
work (Hu et al. (2018)) leverages Stein discrepancy to design a neural sampler from unnormalized
densities.  The fundamental disadvantage of explicit model is that the energy-based learning is dif-
ficult  to accurately capture the distribution of true samples due to the low manifold of real-world
instances (Liu & Wang (2017)).

A.2    IMPLICIT GENERATIVE MODELS

Implicit generative models focus on a generation mapping from random noises to generated sam-
ples.  Such mapping function is often called as generator and possesses better flexibility compared
with explicit models.  Two typical implicit models are Variational Auto-Encoder (VAE) (Kingma
& Welling (2014)) and Generative Adversarial Networks (GAN) (Goodfellow et al. (2014)).  VAE
introduces a latent variable and attempts to maximize the variational lower bound for likelihood of
joint distribution of latent variable and observable variable, while GAN targets an adversarial game
between the generator and a discriminator (or critic in WGAN) that aims at discriminating the gen-
erated and true samples.  In this paper, we focus on GAN and its variants (e.g., WGAN (Arjovsky
et al. (2017)), WGAN-GP (Gulrajani et al. (2017)), DCGAN (Radford et al. (2016)), etc.)  as the
implicit generative model and we leave the discussions on VAE as future work.

Two important issues concerning GAN and its variants are instability of training and local optima.
The typical local optima for GAN can be divided into two categories:  mode-collapse (the model
fails to capture all the modes in data) and mode-redundance (the model generates modes that do
not exist in data). Recently there are many attempts to solve these issues from various 
perspectives.
One perspective is from regularization. Two typical regularization methods are likelihood-based and
entropy-based regularization with the prominent examples Warde-Farley & Bengio (2017) and Li &
Turner (2018) that respectively leverage denoising feature matching and implicit gradient approxi-
mation to enforce the regularization constraints.  The likelihood and entropy regularizations could
respectively help the generator to focus on data distribution and encourage more diverse samples,
and       a recent work (Tao et al. (2019)) uses Langevin dynamics to indicate that i) the entropy 
and
likelihood regularizations are equivalent and share an opposite relationship in mathematics, and 
ii)

14


Under review as a conference paper at ICLR 2020

both regularizations would make the model converge to a surrogate point with a bias from original
data distribution.  Then Tao et al. (2019) proposes a variational annealing strategy to empirically
unite two regularizations and tackle the biased distributions.

To deal with the instability issue, there are also some recent literatures from optimization 
perspec-
tives and proposes different algorithms to address the non-convergence of minimax game optimiza-
tion (for instance, Gemp & Mahadevan (2018); Liang & Stokes (2019); Gidel et al. (2019)). More-
over,  the  disadvantage  of  implicit  models  is  the  lack  of  explicit  densities  over  
instances,  which
disables the black-box generator to characterize the distributions behind data.

A.3    ATTEMPTS TO COMBINE BOTH OF THE WORLDS

Recently, there are several studies that attempt to combine explicit and implicit generative models
from different ways.  For instance, Zhao et al. (2017) proposes energy-based GAN that leverages
energy model as discriminator to distinguish the generated and true samples.  The similar idea is
also used by Kim & Bengio (2017) and Dai et al. (2017) which let the discriminator estimate a
scaler energy value for each sample.  Such discriminator is optimized to give high energy to gener-
ated samples and low energy to true samples while the generator aims at generating samples with
low energy.  The fundamental difference is that Zhao et al. (2017) and Dai et al. (2017) both aim
at minimizing the discrepancy between distributions of generated and true samples while the moti-
vation of Kim & Bengio (2017) is to minimize the KL divergence between estimated densities and
true samples.  Kim & Bengio (2017) adopts contrastive divergence (CD) to link MLE for energy
model over true data with the adversarial training of energy-based GAN. However, both CD-based
method and energy-based GAN have limited power for both generator and discriminator. Firstly, if
the generated samples resemble true samples, then the gradients for discriminator given by true and
generated samples are just the opposite and will counteract each other, and the training will stop
before the discriminitor captures accurate data distribution.  Second, since the objective boils 
down
to minimizing the KL divergence (for Kim & Bengio (2017)) or Wasserstein distance (for Dai et al.
(2017)) between model and true distributions, the issues concerning GAN (or WGAN) like training
instability and mode-collapse would also bother these methods.

Another way for combination is by cooperative training. Xie et al. (2016a) (and its improved version
Xie et al. (2018)) leverages the samples of generator as the MCMC initialization for energy-based
model.  The synthesized samples produced from finite-step MCMC are closer to the energy model
and the generator is optimized to make the finite-step MCMC revise its initial samples. Also, a 
recent
work Du et al. (2018) proposes to regard the explicit model as a teacher net who guides the 
training of
implicit generator as a student net to produce samples that could overcome the mode-collapse issue.
The main drawback of cooperative training is that they indirectly optimize the discrepancy between
the generator and data distribution via the energy model as a ‘mediator’, which leads to a fact that
once the energy model gets stuck in a local optimum (e.g., mode-collapse or mode-redundance) the
training for the generator would be affected.   In other words,  the training for two models would
constrain rather than exactly compensate each other.   In Table 1,  we do a high-level comparison
among the above-mentioned generative models w.r.t the objectives.  Different from other methods,
our model considers three discrepancies simultaneously as a triangle to jointly train the generator
and the estimator, enabling them to compensate and reinforce each other.

B    BACKGROUND  FOR  STEIN  DISCREPANCY

Assume q(x) to be a continuously differentiable density supported on          Rd and f  : Rd        
Rd'  a
smooth vector function. Define    q[f (x)] =    ₓ log q(x)f (x)T +    ₓf (x) as a Stein operator. 
If f is
a Stein class (satisfying some mild boundary conditions) then we have the following Stein identity
property:

Eₓ∼q[Aq[f (x)]] = Eₓ∼q[∇ₓ log q(x)f (x)T + ∇ₓf (x)] = 0.

Such property induces the Stein discrepancy between distributions P : p(x) and Q : q(x), x ∈ X:

S(Q, P) = sup{Eₓ∼q[Ap[f (x)]] = sup{φ(Eₓ∼q[∇ₓ log p(x)f (x)T + ∇ₓf (x)])},          (10)

f ∈F                                                      f ∈F

where f is what we call Stein critic that exploits over function space     and if     is large 
enough then

(Q, P) = 0 if and only if Q = P. Note that in (1), we do not need the normalized constant for p(x)

which enables Stein discrepancy to deal with unnormalized density.

15


Under review as a conference paper at ICLR 2020

If     is a unit ball in a Reproducing Kernel Hilbert Space (RKHS) with a positive definite kernel 
func-
tion k( ,  ), then the supremum in (1) would have a close form (see Liu et al. (2016); Chwialkowski
et al. (2016); Oates et al. (2017) for more details):

SK(Q, P) = Eₓ,ₓ'∼q [up(x, x′)],                                              (11)
where    up(x, x′)       =       ∇ₓ log p(x)Tk(x, x′)∇ₓ log p(x′)   +   ∇ₓ log p(x)T∇ₓk(x, x′)   +

ₓk(x, x′)T    ₓ log p(x′)  +  tr(   ₓ,ₓ' k(x, x′)).    The  (11)  gives  the  Kernel  Stein  
Discrepancy
(KSD).

C    PROOFS  OF  RESULTS  IN  SECTION  3.2

C.1    PROOF OF THEOREM 1

Proof.  Using Kantorovich’s duality, we rewrite the problem as

min max{EP[D] − EPreal [D] + λ₁S(Prₑₐl, PE) + λ₂S(P, PE)},

E,P      D

where the minimization with respect to E is over all energy functions, the minimization with respect
to P is over all probability distributions with continuous density, and the maximization is over all
1-Lipschitz continuous functions. From the definition of kernel Stein discrepancy

S(P, PE) = Eₓ,ₓ'∈P[(∇ₓ log P(x) − ∇ₓ log PE(x))Tk(x, x′)(∇ₓ log P(x′) − ∇ₓ log PE(x′))],
(P, PE) is infinite if P is not absolutely continuous with respect to PE.  Hence, to minimize the

objective, it suffices to consider those P’s that are absolutely continuous with respect to PE.  We
set h(x) := dP/dPE(x)     1, where dP/dPE is the Radon-Nikodym derivative.  It follows that the
problem becomes

min max .EPE [D] + EPE [hD] − EPreal [D] + λ₁S(Prₑₐl, PE)


E,h    D

+ λ₂ · Eₓ,ₓ'∼P[∇ₓ log(1 + h(x))Tk(x, x′)∇ₓ log(1 + h(x′))]Σ,

where the minimization with respect to h is over all L¹(PE) functions with PE-expectation zero.

Fixing E, we claim that we can swap minh and maxD.  Indeed, without loss of generality, we can
restrict D to be such that D(x₀) = 0 for some element x₀, as a constant shift does not change the
value of EPE [(1 + h)D]     EPreal [D].  The set of Lipschitz functions that vanish at x₀ is a 
Banach
space, and the subset of 1-Lipschtiz functions is compact Weaver (1999). Moreover, L¹(PE) is also
a Banach space. The above verifies the condition of Sion’s minimax theorem, and thus the claim is
proved.

Swapping minh and maxD, we consider


min max     min

{EPE [hD] + λ₂ · Eₓ,ₓ'∼P[∇ₓ log(1 + h(x))Tk(x, x′)∇ₓ log(1 + h(x′))]}

E      D    h:EPE [h]=0


= min max     min

.EP

[hD] + λ   · E

'   P Σ∇ₓh(x)T k(x, x′) ∇ₓh(x′) ΣΣ

= min max     min     .EPE [hD] + λ₂ · Eₓ,ₓ'∼PE  Σ∇ₓh(x)Tk(x, x′)∇ₓh(x′)ΣΣ ,

where the second equality follows from the chain rule of the derivative. For the inner minimum, we
have that


min

h:EPE [h]=0

.EPE [hD] + λ₂ · Eₓ,ₓ'∼PE  Σ∇ₓh(x)Tk(x, x′)∇ₓh(x′)ΣΣ


= min

min

.EP

[hD] + λ₂r   : Eₓ,ₓ'∼P

Σ∇ₓh(x)Tk(x, x′)∇ₓh(x′)Σ ≤ r²Σ


= min     min     .rEP   [hD] + λ₂r   : Eₓ,ₓ'∼P

Σ∇ₓh(x)Tk(x, x′)∇ₓh(x′)Σ ≤ 1Σ

= min  ,λ₂r² − r ǁDǁ   −1               ,


r≥0

1

H     (PE ;k)


= −  4λ

ǁDǁH−1 (PE ;k) ,

16


Under review as a conference paper at ICLR 2020

where  the  first  equality  follows  the  introduction  of  the  auxiliary  variable  r,  the  
second  equality
follows a change of variable from h to rh; and the second equality the third equality follows from
the definition of the kernel Sobolev dual norm.  Plugging in the previous equation yields the ideal
result.

C.2    PROOF FOR THEOREM 2

Proof.  Essentially, the result is a consequence of distributionally robust optimization with 
Wasser-
stein metric (Gao & Kleywegt, 2016; Gao et al., 2017).  Here we provide a simplified version for
completeness.

Using the definition of Stein discrepancy, we rewrite the problem as

min max{λ₁S(Prₑₐl, PE) + λ₂Ey∼P[APE f (y)] + W(Prₑₐl, P)}

E,P       f

Using the compactness of the Stein class and similar argument to the proof of Theorem 1, we can
swap the minimization over P and the maximization over f . Fixing f , consider

min {λ₂Ey∼P[APE f (y)] + W(Prₑₐl, P)}.

Using the definition of Wasserstein metric

W(Prₑₐl, P) = min E₍ₓ,y₎∼γ[ǁx − yǁ],

where the minimization is over all joint distributions of (x, y) with x-marginal Prₑₐl and 
y-marginal

P, we write the problem above as

min{E₍ₓ,y₎∼γ [λ₂ APE f (y) + ||x − y||]},

where γ has marginals Prₑₐl and P. Since P is unconstrained, the above problem is further equivalent
to

min{E₍ₓ,y₎∼γ [λ₂APE f (y)] + ||x − y||]},

where the minimization is over all joint distributions of (x, y) with x-marginal being Prₑₐl.  Using
the law of total expectation, the problem above is equivalent to


min

{γx}x

Eₓ∼Preal  [Ey∼γx  [λ₂APE f (y) + ||x − y|| | x]]

= Eₓ∼Preal  Σmin{Ey∼γx  [λ₂APE f (y) + ||x − y|| | x]}Σ

= Eₓ∼Preal  Σmin{λ₂APE f (y) + ||x − y||}Σ

where the minimization in the first equation is over γₓ, all conditional distributions of y given x,
and the exchanging of min and E follows from the interchangebability principle (Shapiro et al.,
2009); the second equality holds because the infimum can be restricted to the set of point masses.

We finally have the original problem is equivalent to

min max .λ₁S(Prₑₐl, PE) + Eₓ∼Preal  Σmin{λ₂APE f (y) + ||x − y||}ΣΣ .

Hence the proof is completed using the definition of Moreau-Yosida regularization.

D    PROOFS  AND  MORE  DISCUSSIONS  IN  SECTION  4

D.1    DETAILS FOR ONE-DIMENSIONAL CASE

For the analysis of 1-dim regularized WGAN in section 3.1.1,  we assume a Gaussian likelihood
function for sample x, µ(x)  =  exp(− 1 (x − b)²) which is up to a constant.  Its parameter can be
estimated by b = E[x]. Then for generated sample x′ = θz, we have E(log µ(θz)) = − 1 E[z²]θ² +

17


Under review as a conference paper at ICLR 2020

E[z]E[x]θ −  1 E[x]².  Like the case in WGAN, we consider E[x] = E[z] = 1.  Assume Var[z] = 1
and we have E[z²]  =  1 + E[z].  Hence, for the analysis on likelihood- (and entropy-) regularized
WGAN, we can study the following system:

min max ψ − ψ · θ − λ(θ² − θ).

When λ  =  1,  the above objective degrades to (4);  when λ  <  0 (likelihood-regularization),  the
the gradient of regularization term pushes θ to shrink, which helps for convergence; when λ  >  0
(entropy-regularization), the added term forms an amplifiying strength on θ and leads to 
divergence.

We   proceed   to   consider   1-dim   case   of   Stein   Bridging   with   energy   model   pφ(x) 
    =
exp(    ¹ x²      φx).   If  using  KSD  with  kernel  k(x₁, x₂)  =  I(x₁   =  x₂),  then    
(Prₑₐl, PE)  =
Eₓ1 ,x2 [(   ₓ1  log pφ(x₁)             ₓ1  log µ(x₁))k(x₁, x₂)(   ₓ2  log pφ(x₂)             ₓ2  
log µ(x₂))]       =
Eₓ[(   ₓ log pφ(x)         ₓ log µ(x))²]  =  (φ + E[x])².   Similarly,  one  can  obtain    (PG, 
PE)  =
(φ + θE[z])². Therefore we arrive at the objective in (6)

min max min ψ − ψ · θ + λ1 (1 + φ)² + λ2 (θ + φ)².                             (12)

Interestingly, the added terms  λ1 (1 + φ)² +  λ2 (θ + φ)²  in (6) and the original terms ψ − ψ · θ

in WGAN play both necessary roles to guarantee the convergence to the unique optimum points
[ψ∗, θ∗, φ∗]  =  [0, 1,    1].   If we remove the critic and optimize θ  and φ with the remaining 
loss
terms, we would find that the training would converge but not necessarily to [ψ∗, θ∗] = [0, 1] 
(since
the optimum points are not unique in this case). On the other hand, if we remove the estimator, the
system degrades to (4) and would not converge to the unique optimum point [ψ∗, θ∗]  =  [0, 1].  If
we consider both of the world and optimize three terms together, the training would converge to a
unique global optimum [ψ∗, θ∗, φ∗] = [0, 1, −1].

D.2    PROOF FOR PROPOSITION 1

Proof.  Instead of directly studying the optimization for (6), we first prove the following problem
will converge to the unique optimum,


min max min θψ + θφ +

1 θ2

+ φ².                                           (13)

θ       ψ       φ                                2

Applying alternate SGD we have the following iterations:

ψt₊₁ = ψt + η ∗ θt,

φt₊₁ = φt − η ∗ (θt + 2φt) = (1 − 2η)φt − ηθt,

θt₊₁ = θt − η(ψt₊₁ + φt₊₁ + θt) = −η(1 − 2η)φt + (1 − η)θt − ηψt.

Then we obtain the relationship between adjacent iterations:


ψt+1
φt+1
θt+1

1              0                η           ψt

=     0          1 − 2η          −η     ·    φt

−η    −η(1 − 2η)    1 − η        θt

ψt

= M      φt
θt

We further calculate the eigenvalues for matrix M  and have the following equations (assume the
eigenvalue as λ):

(λ − 1)³ + 3η(λ − 1)² + 2η²(1 + η)(λ − 1) + 2η³ = 0.

One can verify that the solutions to the above equation satisfy |λ| <     (1 − η + η2)(1 + η − η2).

Then we have the following relationship


¨Σψt₊₁Σ¨2        ¨

ΣψtΣ¨2

¨ΣψtΣ¨2


φt+1

¨  θt+1

¨  = ¨[ψt   φt   θt] · M TM ·

θt   ¨2

≤ λm ·      φt

t      2


ψ²     + φ²

+ θ²

≤ (1 − η + η  )(1 + η − η  )[ψ   + φ   + θ  ].

We proceed to replace ψ, φ and θ in (13) by ψ′, φ′  and θ′  respectively and conduct a change of
variable: let θ′ = 1 − θ and φ′ = −1 − φ. Then we get the conclusion in the proposition.

18


Under review as a conference paper at ICLR 2020

D.3    GENERALIZATION TO BILINEAR SYSTEMS

Our analysis in the one-dimension case inspires us that we can add affiliated variable to modify the
objective and stabilize the training for general bilinear system. The bilinear system is of wide 
interest
for researchers focusing on stability of GAN training (Goodfellow (2017); Liang & Stokes (2019);
Gidel et al. (2019); Gemp & Mahadevan (2018)). The general bilinear function can be written as

F (ψ, θ) = θTAψ − bTθ − cTψ,                                            (14)
where  ψ, θ  are  both  r-dimensional  vectors  and  the  objective  is  min max F (ψ, θ)  which  
can  be

θ       ψ

seen as a basic form of various GAN objectives.  Unfortunately,  if we directly use simultaneous
(resp.  alternate) SGD to optimize such objectives, one can obtain divergence (resp.  fluctuation).
To solve the issue, some recent papers propose several optimization algorithms, like extrapolation
from the past (Gidel et al. (2019)), crossing the curl (Gemp & Mahadevan (2018)) and consensus
optimization (Liang & Stokes (2019)). Also, Liang & Stokes (2019) shows that it is the interaction
term which generates non-zero values for    θψF and    ψθF that leads to such instability of 
training.
Different  from  previous  works  that  focused  on  algorithmic  perspective,  we  propose  to  
add  new
affiliated variables which modify the objective function and allow the SGD algorithm to achieve
convergence without changing the optimum points.

Based on the minimax objective of (14) we add affiliated r-dimensional variable φ (corresponding
to the estimator in our model) the original system and tackle the following problem:

min max min F (ψ, θ) + αH(φ, θ),                                           (15)

θ       ψ       φ

where  H(φ, θ)   =   ¹ (θ  + φ)TB(θ  + φ),  B   =   (AAT) 1    and  α  is  a  non-negative  
constant.

2

Theoretically, the new problem keeps the optimum points of (14) unchanged.  Let L(ψ, φ, θ)  =

F (ψ, θ) + αG(φ, θ)

Proposition  2.  Assume  the  optimum  point  of  min max F (ψ, θ)  are  [ψ∗, θ∗],  then  the  
optimum

θ       ψ

points of (15) would be [ψ∗, θ∗, φ∗] where φ∗ = −θ∗.

Proof.  The condition tells us that     θF (ψ∗, θ)  =  0 and     ψF (ψ, θ∗)  =  0.  Then we derive 
the
gradients for L(ψ, φ, θ),

∇ψL(ψ∗, φ, θ) = ∇θF (ψ∗, θ) = 0,                                           (16)


∇θL(ψ, φ, θ∗) = ∇θ

F (ψ, θ∗) + ∇θ

H(φ, θ∗) =  1 (B + BT)(θ∗ + φ),             (17)

2


∇φL(ψ, φ, θ) = ∇φ

H(φ, θ) =  1 (B + BT)(φ + θ),                             (18)

2

Combining (17) and (18) we get φ∗ = −θ∗. Hence, the optimum point of (15) is [ψ∗, θ∗, φ∗] where

φ∗ = −θ∗.

The advantage of the new problem is that it can be solved by SGD algorithm and guarantees conver-
gence theoretically. We formulate the results in the following theorem.

Theorem 4.  For problem min max min L(ψ, φ, θ) using alternate SGD algorithm, i.e.,

θ       ψ       φ


ψt₊₁ = ψt + η∇ψL(θt, ψt, φt),
φt₊₁ = φt − η∇φL(θt, ψt₊₁, φt),
θt₊₁ = θt − η∇θL(θt, ψt₊₁, φt₊₁),

(19)

we can achieve convergence to [ψ∗, θ∗, φ∗] where φ∗  = −θ∗ with at least linear rate of (1 − η₁ +

η²)(1 + η₂ − η²) where η₁  = ησmin, η₂  = ησmₐₓ and σmin (resp.  σmₐₓ) denotes the maximum

(resp. minimum) singular value of matrix A.

To prove Theorem 3, we can prove a more general argument.

19


Under review as a conference paper at ICLR 2020

Lemma 1.  If we consider any first-order optimization method on (15), i.e.,

ψt₊₁ ∈ ψ₀ + span(L(ψ₀, φ, θ), · · ·  , F (ψt, φ, θ)), ∀t ∈ N,

φt₊₁ ∈ ψ₀ + span(L(ψ, φ₀, θ), · · ·  , L(ψ, φt, θ)), ∀t ∈ N,

θt₊₁ ∈ ψ₀ + span(L(ψ, φ, θ₀), · · ·  , L(ψ, φ, θt)), ∀t ∈ N,

Then we have

ψ˜t  = VT(ψt − ψ∗),     φ˜t  = UT(φt − φ∗),     θ˜t  = UT(θt − θ∗),

where U and V are the singular vectors decomposed by matrix A using SVD decomposition, i.e.,
A = UDVT  and the triple ([ψt]i, [φt]i, [θt]i)₁   i  r follows the update rule with step size σiη as
the same optimization method on a unidimensional problem

1          1

min max min θψ + θφ +    θ² +    φ²,                                          (20)

θ       ψ       φ                           2          2

with step size η, where σi denotes the i-th singular value on the diagonal of D.

Proof.  The proof is extended from the proof of Lemma 3 in Gidel et al. (2019).  The general class
of first-order optimization methods derive the following updations:

t+1                                                            t+1

ψt₊₁ = ψ₀ + Σ ρst(ATθs − c) = ψ₀ + Σ ρstAT(θs − θ∗),


s=0

φt+1  = φ0

t+1

t+1

+           δst

2

s=0

s=0

(B + BT)(θs

+ φs),

θ      = θ   + Σ µ   [A(ψ   −     ∗                                     T

where ρst, δst, µst ∈ R depend on specific optimization method (for example, in SGD, ρtt = δtt =

µtt remain as a non-zero constant for ∀t and other coefficients are zero).

Using SVD A = UDVT and the fact θ∗ = −φ∗, B = (UDDTUT) = D, we have

t+1

VT(ψt₊₁ − ψ∗) = VT(ψ₀ − ψ∗) +       ρstDTUT(θs − θ∗)

s=0

t+1

UT(φt₊₁ − φ∗) = UT(φ₀ − φ∗) +       δstUTD(θs − θ∗) + UTD(φs − φ∗),

s=0

t+1

UT(θt₊₁ − θ∗) = UT(θ₀ − θ∗) +       ρst[DVT(ψs − ψ∗) + UTD(θs − θ∗) + UTD(φs − φ∗)],

s=0


and equivalently,

t+1

ψt+1  = ψ0  +       ρstDTθt,

s=0

t+1

t+1

φt = φ₀ +       δstD(θt + φt),

s=0

θt₊₁ = θ₀ +       ρstD(ψt + θt + φt).

s=0

Note that D is a rectangular matrix with non-zero elements on a diagonal block of size r.  Hence,
the above r-dimensional problem can be reduced to r unidimensional problems:

t+1                                                                        t+1

[ψ˜t+1]i = [ψ˜0]i + Σ ρstσi[θ˜t]i,     [φ˜t]i = [φ˜0]i + Σ δstσi([θ˜t]i + [φ˜t]i),

	

[θt₊₁]i = [θ₀]i +       ρstσi([ψt]i + [θt]i + [φt]i).

s=0

The above iterations can be conducted independently in each dimension where the optimization in

i-th dimension follows the same updating rule with step size σiη as problem in (20).

20


Under review as a conference paper at ICLR 2020

Furthermore, since problem (20) can achieve convergence with a linear rate of (1   η+η²)(1+η   η²)
using alternate SGD (the proof is similar to that of ((13))), the multi-dimensional problem in (15)
can achieve convergence by SGD with at least a rate of (1−η₁+η²)(1+η₂ −η²) where η₁ = ησmₐₓ,

η₂ = ησmin and σmₐₓ (resp. σmin) denotes the maximum (resp. minimum) singular value of matrix

A. We conclude the proof for Theorem 4.

Theorem 4 suggests that the added term H(φ, θ) with affiliated variables φ could help the SGD
algorithm  achieve  convergence  to  the  the  same  optimum  points  as  directly  optimizing  F 
(ψ, θ).
Our method is related to consensus optimization algorithm (Liang & Stokes (2019)) which adds
a regularization term       θF (ψ, θ)   +       ψF (ψ, θ)   to (14) resulting extra quadratic terms 
for θ
and ψ.  The disadvantage of such method is the requirement of Hessian matrix of F (ψ, θ) which
is computational expensive for high-dimensional data.  By contrast, our solution only requires the
first-order derivatives.

D.4    STRONGLY CONVEXITY

In section 3.1.2, we assume H(θ, φ) as a µ-strongly convex function which indicates that it 
satisfies
the conditions:

(∇θH(θ, ·) − ∇θH(θ′, ·))T(θ − θ′) ≥ µǁθ − θ′ǁ2, ∀θ, θ′ ∈ Ωθ,
(∇φH(·, φ) − ∇φH(·, φ′))T(φ − φ′) ≥ µǁφ − φ′ǁ2, ∀φ, φ′ ∈ Ωφ.

Bedises, F (θ, ψ) is µ-strongly convex for θ and µ-strongly concave for ψ so it satisfies:

(∇θF (θ, ·) − ∇θF (θ′, ·))T(θ − θ′) ≥ µǁθ − θ′ǁ2, ∀θ, θ′ ∈ Ωθ,
(∇ψF (·, ψ′) − ∇ψF (·, ψ))T(ψ − ψ′) ≥ µǁψ − ψ′ǁ2, ∀ψ, ψ′ ∈ Ωψ.

In section 3.1.2, we also define h(ωh) =     θH +    φH and f (ωf ) =     θF         ψF , so the 
above
condition can be written in a more compact form,


(h(ωh) − h(ωh′  ))T(ωh − ωh′  ) ≥ µǁωh − ω′ ǁ2, ∀ωh, ωh′

∈ Ωh,


(f (ωf ) − f (ωf′  ))T(ωf − ωf′  ) ≥ µǁωf − ω′ ǁ2, ∀ωf , ωf′

∈ Ωf .

f   2

D.5    PROOF FOR THEOREM 2
The proof relies on two lemmas,

Lemma 2.  For any ω ∈ Ω and ω⁺ = PΩ(ω + u), then we have

ǁω⁺ − ωǁ2  ≤ uT(ω⁺ − ω).

Proof.  Since ω⁺ is a projection of ω + u on a convex set Ω, we have

(ω⁺ − (ω + u))T(ω⁺ − ω) ≤ 0.                                             (21)

Rearranging the above inequality one can easily get the lemma.

Lemma 3.  If function Φ(ω) is µ-strongly convex, we have

µǁω − ω∗ǁ2  ≤ ∇Φ(ω)T(ω − ω∗).

Similarly, if Φ(ω) is µ-strongly concave, we have µ(ω − ω∗) ≤ ∇ − Φ(ω)T(ω − ω∗).
Proof.  By optimality of ω∗, we have

∇Φ(ω∗)T(ω − ω∗) ≥ 0.

Since Φ is µ-convex, we can further derive

µǁω − ω∗ǁ2  ≤ ∇Φ(ω∗)T(ω − ω∗) + µǁω − ω∗ǁ2  ≤ ∇Φ(ω)T(ω − ω∗).

21


Under review as a conference paper at ICLR 2020

Proof.  (Proof    for    Theorem    3)    We    apply    Lemma    2    to    (9)    with    (ω, u, 
ω⁺)       =
(ωᵗ⁺¹/², −ηf (ωᵗ⁺¹/²), ωᵗ⁺¹) and we have

f                         f                f

ǁωt+1  − ωt+1/2ǁ ≤ −ηf (ωt+1/2)T(ωt+1  − ωt+1/2)                             (22)


f              f

Then we have

f                   f              f

ǁωt+1  − ω∗ǁ2  = ǁωt+1/2  − ω∗  + ωt+1  − ωt+1/2ǁ2

≤ 2ǁωᵗ⁺¹/² − ω∗ǁ2  + 2ǁωᵗ⁺¹ − ωᵗ⁺¹/²ǁ2      (by    ǁa + bǁ2  ≤ 2ǁaǁ2  + 2ǁbǁ2) (23)

≤ 2ǁωt+1/2  − ω∗ǁ2  − 2ηf (ωt+1/2)T(ωt+1  − ωt+1/2).


f                  f   2

According to Lemma 3, we have

f                   f              f

2ηf (ωᵗ⁺¹/²)T(ωᵗ⁺¹/² − ω∗) ≥ 2ηµǁωᵗ⁺¹/² − ω∗ǁ2.                            (24)


f                   f                  f

Plug (24) into (23) and we get

f                  f   2

ǁωᵗ⁺¹ − ω∗ǁ2  ≤ (2 − 2ηµ)ǁωᵗ⁺¹/² − ω∗ǁ2.                                         (25)

The above inequality is equivalent to

ǁθᵗ⁺¹ − θ∗ǁ2  + ǁψᵗ⁺¹ − ψ∗ǁ2  ≤ (2 − 2ηµ)(ǁθᵗ⁺¹/² − θ∗ǁ2  + ǁψᵗ − ψ∗ǁ2).        (26)

2                                         2                                                         
              2                                   2

Similarly, one can obtain

ǁωᵗ⁺¹ − ω∗ǁ2  ≤ (2 − 2ηµ)ǁωᵗ⁺¹/² − ω∗ǁ2,                                   (27)


i.e.,

h              h   2

h                 h   2

ǁθᵗ⁺¹/² − θ∗ǁ2  + ǁφᵗ⁺¹ − φ∗ǁ2  ≤ (2 − 2ηµ)(ǁθᵗ − θ∗ǁ2  + ǁφᵗ − φ∗ǁ2).           (28)

2                                        2                                                          
  2                                  2

Combining (26) and (28) we have

ǁωᵗ⁺¹ − ω∗ǁ2  = ǁθᵗ⁺¹ − θ∗ǁ2  + ǁψᵗ⁺¹ − ψ∗ǁ2  + ǁφᵗ⁺¹ − φ∗ǁ2

≤ (1 − 2ηµ)ǁθᵗ − θ∗ǁ2  + (2 − 2ηµ)ǁψᵗ⁺¹ − ψ∗ǁ2  + (2 − 2ηµ)ǁφᵗ⁺¹ − φ∗ǁ2  (29)

2                                                                 2                                 
                               2

≤ (2 − 2ηµ)ǁωᵗ − ω∗ǁ2.

Hence, if  ¹   < η <  ¹  we have 0 < 2 − 2ηµ < 1 and ǁωᵗ − ω∗ǁ2  ≤ (2 − 2ηµ)ᵗǁω⁰ − ω∗ǁ2

E    DETAILS  FOR  EXPERIMENT  SETUP

E.1    SYNTHETIC DATASETS

We  provide  the  details  for  two  synthetic  datasets.   The  Two-Circle  dataset  consists  of  
24  Gaus-
sian  mixtures  where  8  of  them  are  located  in  an  inner  circle  with  radius  r₁   =   4  
and  16  of
them  lie  in  an  outer  circle  with  radius  r₂   =   8.    For  each  Gaussian  component,  the 
 covari-
ance  matrix  is     0.2      0       =  σ₁I  and  the  mean  value  is  [r₁ cos t, r₁ sin t],  
where  t  =   ²π·ᵏ ,

k  =  1,       , 8, for the inner circle, and [r₂ cos t, r₂ sin t], where t  =  ²π·ᵏ , k  =  1,     
  , 16 for the
outer circle.  We sample N₁  =  2000 points as true observed samples for model training.  In sec-
tion 5.5, we consider noised data scenario.  In this case, we randomly add n noise points sampled
from Gaussian distribution     (0, σ₀I)  where σ₀  =  2 to the original  true samples.   Here we set
n = [40, 100, 160, 300, 400, 600, 800, 1000].

The Two-Spiral dataset contains 100 Gaussian mixtures whose centers locate on two spiral-shaped
curves.  For each Gaussian component, the covariance matrix is    0.5      0      = σ₂I and the 
mean

value  is  [−c₁ cos c₁, c₁ sin c₁],  where  c₁   =   ²π  + linspace(0, 0.5, 50) ·  2π,  for  one  
spiral,  and
[c₂ cos c₂,    c₂ sin c₂], where c₂  =  ²π + linspace(0, 0.5, 50)   2π for another spiral.  We 
sample
N₂  = 5000 points as true observed samples.  In section 5.5, we consider insufficient data 
scenario.

In this case, the sample size N₂ is reduced to [100, 200, 300, 500, 700, 1000, 2000].

22


Under review as a conference paper at ICLR 2020

D₁      D₂     D₃                                                            Objective

minθ minφ maxψ maxπ Eₓ  Pdata [dψ(x)] − Ez  p0 [dψ(Gθ(z))]

+λ₁Eₓ∼Pdata [Apφ [fπ(x)]] + λ₂Ez∼ı0 [Apφ [fπ(Gθ(z))]]


W       S        S

minθ minφ maxψ Eₓ∼Pdata [dψ(x)] − Ez∼p0 [dψ(Gθ(z))]

 	

minθ minφ maxψ maxπ Eₓ  Pr [log(dψ(x))] + Ez  p0 [log(1 − dψ(Gθ(z)))]

+λ₁Eₓ∼Pdata [Apφ [fπ(x)]] + λ₂Ez∼ı0 [Apφ [fπ(Gθ(z))]]


J S      S        S

minθ minφ maxψ Eₓ∼Pr [log(dψ(x))] + Ez∼p0 [log(1 − dψ(Gθ(z)))]

 	

Table 4:  Objectives for different specifications of    ₁(Prₑₐl, PG),    ₂(Prₑₐl, PE) and    ₃(PG, 
PE).
We specify    ₁ as Wasserstein distance or JS divergence in our paper and for    ₂ and    ₃ we 
consider
the general Stein discrepancy or kernel Stein discrepancy.  Here we use     ,         to denote 
Wasser-
stein distance and JS divergence respectively, and    ,    k to represent general Stein discrepancy 
and
kernel Stein discrepancy respectively.  We omit the gradient penalty term for Wasserstein distance
here but use it in experiments.

E.2    MODEL SPECIFICATIONS AND TRAINING ALGORITHM

In different tasks, we consider different model specifications in order to meet the demand of 
capacify
as well as test the effectiveness under various settings. Our proposed framework (3) adopts Wasser-
stein distance for the first term and two Stein discrepancies for the second and the third terms.  
We
can write (3) as a more general form

min    ₁(Prₑₐl, PG) + λ₁   ₂(Prₑₐl, PE) + λ₂   ₃(PG, PE),                           (30)

θ,φ

where    ₁,     ₂,     ₃  denote three general discrepancy measures for distributions.  As stated 
in our
remark,    ₁ can be specified as arbitrary discrepancy measures for implicit generative models. Here
we also use JS divergence, the objective for valina GAN. To well distinguish them, we call the model
using Wasserstein distance (resp. JS divergence) as Joint-W (resp. Joint-JS) in our experiments. On
the other hand, the two Stein discrepancies in (3) can be specified by KSD (as defined by    k in
(11)) or general Stein discrepancy with an extra critic (as defined by      in (1)).   Hence,  the 
two
specifications for    ₁ and the two for    ₂ (   ₃) compose four different combinations in total, 
and we
organize the objectives in each case in Table 4.

In our experiments, we use KSD with RBF kernels for    ₂ and    ₃ in Joint-W and Joint-JS on two
synthetic datasets.  For MNIST with conditional training (given the digit class as model input), we
also use KSD with RBF kernels.  For MNIST and CIFAR with unconditional training (the class is
not given as known information), we find that KSD cannot provide desirable results so we adopt
general Stein discrepancy for higher model capacity.

The objectives in Table 4 appear to be comutationally expensive.  In the worst case (using general
Stein discrepancy), there are two minimax operations where one is from GAN or WGAN and one
is from Stein discrepancy estimation.  To guarantee training efficiency, we alternatively update the
generator, estimator, Wasserstein critic and Stein critic over the parameters θ, φ, ψ and π respec-
tively. Specifically, in one iteration, we optimize the generator over θ and the estimator over φ 
with
one step respectively, and then optimize the Wasserstein critic over ψ with nd steps and the Stein
critic over π with nc steps.  Such training approach guarantees the same time complexity order of
proposed method as that of GAN or WGAN, and the training time for our model can be bounded
within constant times the time for training GAN model. In our experiment, we set nd = nc = 5 and
empirically find that our model Stein Bridging would be two times slower than WGAN on average.
We present the training algorithm for Stein Bridging in Algorithm 1.

E.3    IMPLEMENTATION DETAILS

We give the information of network architectures and hyper-parameter settings for our model as well
as each competitor in our experiments.

23


Under review as a conference paper at ICLR 2020

Algorithm 1: Training Algorithm for Stein Bridging

1   REQUIRE: observed training samples   x       Prₑₐl.

2   REQUIRE: θ₀, φ₀, ψ₀, π₀, initial parameters for generator, estimator, Wasserstein critic and 
Stein

critic models respectively. αE = 0.0002, βE = 0.9, βE = 0.999, Adam hyper-parameters for

1                        2

explicit models. αI = 0.0002, βI = 0.5, βI = 0.999, Adam hyper-parameters for implicit

1                      2

models. λ₁ = 1, λ₂, weights for    ₂ and    ₃ (we suggest increasing λ₂ from 0 to 1 through
training). nd = 5, nc = 5 number of iterations for Wasserstein critic and Stein critic, 
respectively,
before one iteration for generator and estimator. B = 100, batch size.

3   while not converged do

4              for n = 1, · · ·  , nd do


5                        Sample B true samples {xi}B

Sample B random noise {zi}B

from {x};

∼ P₀ and obtain generated samples xi = Gθ(zi) ;


6

₁  ΣB

i=1

2  // the last ˜term is for gradient

7                        Ldis =  B       i=1 dψ(xi) − dψ(x˜i) − λ(ǁ∇ₓˆi dψ(xˆi)ǁ − 1)

penalty in WGAN-GP where xˆi = ϵixi + (1 − ϵi)xi, ϵi ∼ U (0, 1);

8                        ψk₊₁ ← Adam(−Ldis, ψk, α  , β  , β  )

1       2

9              for n = 1, · · ·  , nc do


10                        Sample B true samples {xi}B

Sample B random noise {zi}B

from {x};

∼ P₀ and obtain generated samples xi = Gθ(zi) ;


11

₁  ΣB

i=1

;

	


12                        Lcritic  =  B

i₌₁ λ₁Apφ [fπ(x)] + λ₂Apφ [fπ(x˜i)]

Sample B random noise {zi}B     ∼ P₀ and obtain generated samples xi = Gθ(zi) ;


14

₁  ΣB

i=1

;

15              Lₑst =  B       i=1 λ₁Apφ [fπ(x)] + λ₂Apφ [fπ(x˜i)]


17              Lgen  =  1  ΣB

1

−dψ(x˜i) + λ₂Ap

2

[fπ(x˜i)]

19   OUTPUT: trained sample generator Gθ(z) and density estimator pφ(x).

The energy function is often parametrized as a sum of multiple experts (Hinton (1999)) and each
expert can have various function forms depending on the distributions. If using sigmoid 
distribution,
the energy function becomes (see section 2.1 in Kim & Bengio (2017) for details)

Eφ(x) =        log(1 + e−⁽Win(x)+bi)),                                          (31)

i

where n(x) maps input x to a feature vector and could be specified as a deep neural network, which
corresponds to deep energy model (Ngiam et al. (2011))

For synthetic datasets, we set the noise dimension as 4.  All the generators are specified as a 
three-
layer fully-connected (FC) neural network with neuron size 4   128   128   2, and all the 
Wasserstein
critics (or the discriminators in JS-divergence-based GAN) are also a three-layer FC network with
neuron size 2     128     128     1.  For the estimators, we set the expert number as 4 and the 
feature
function n(x) is a FC network with neuron size 2     128     128     4. Then in the last layer we 
sum
the outputs from each expert as the energy value E(x).  The activation units are searched within
[LeakyReLU, tanh, sigmoid, softplus].  The learning rate [1e     6, 1e     5, 1e     4, 1e     3, 
1e

2] and the batch size [50, 100, 150, 200].  The gradient penalty weight for WGAN is searched in

[0, 0.1, 1, 10, 100].

For  MNIST  dataset,  we  set  the  noise  dimension  as  100.   All  the  critics/discriminators  
are  im-
plemented  as  a  four-layer  network  where  the  first  two  layers  adopt  convolution  
operations  with
filter  size  5  and  stride  [2, 2]  and  the  last  two  layers  are  FC  layers.   The  size  
for  each  layer  is

1      64      128      256      1.   All  the  generators  are  implemented  as  a  four-layer  
networks  where
the first two layers are FC and the last two adopt deconvolution operations with filter size 5 and
stride [2, 2].  The size for each layer is 100     256     128     64     1.  For the estimators, 
we con-
sider the expert number as 128 and the feature function is the same as the Wasserstein critic ex-
cept that the size of last layer is 128.   Then we sum the outputs from each expert as the energy

24


Under review as a conference paper at ICLR 2020


Method

GAN
GAN+LR
GAN+ER
GAN+VA
WGAN-GP
WGAN+LR
WGAN+ER
WGAN+VA
DEM
EGAN
DGM

Joint-JS
Joint-W

Two-Cirlce

MMD      HSR      KLD       JSD       AUC
0.0033     0.772         -             -             -

0.0106     0.391         -             -             -

0.0103     0.428         -             -             -

0.0118     0.295         -             -             -

0.0010     0.841         -             -             -

0.0013     0.840         -             -             -

0.0008     0.830         -             -             -

0.0016     0.835         -             -             -

-              -         2.036     0.431     0.683

-              -         3.350     0.474     0.616

0.0040     0.774     2.272     0.445     0.600

0.0037     0.883     1.104     0.297     0.962

0.0007     0.844     1.030     0.281     0.961

Two-Spiral

MMD       HSR       KLD       JSD       AUC
0.0082      0.583          -             -             -

0.0068      0.821          -             -             -

0.0071      0.780          -             -             -

0.0085      0.761          -             -             -

0.0090      0.697          -             -             -

0.0095      0.607          -             -             -

0.0182      0.730          -             -             -

0.0159      0.618          -             -             -

-              -          1.206     0.315     0.640

-              -          1.916     0.445     0.499

0.0019      0.833      1.725     0.414     0.589

0.0031      0.717      0.655     0.193     0.808

0.0003      0.909     0.364     0.110     0.810

Table 5: Quantitative results including MMD (lower is better), HSR (higher is better) as the metrics
for quality of generated samples and KLD (lower is better), JSD (lower is better), AUC (higher is
better) as the metrics for accuracy of estimated densities on Two-Circle and Two-Spiral datasets.

value.   The  activation  units  are  searched  within  [ReLU, LeakyReLU, tanh].   The  learning  
rate

[2e    5, 2e    4, 2e    3, 2e    2] and the batch size [32, 64, 100, 128]. The gradient penalty 
weight for

WGAN is searched in [1, 10, 100, 1000].

For CIFAR dataset, we adopt the same architecture as DCGAN for critics and generators. As for the
estimator, the architecture of feature function is the same as the critics except the last year 
where we
set the expert number as 128 and sum each output as the output energy value. The architectures for
Stein critic are the same as Wasserstein critic for both MNIST and CIFAR datasets. In other words,
we consider d′  = 1 in (1) and further simply φ as an average of each dimension of Eₓ  P[   Qf (x)].
Empirically we found this setting can provide efficient computation and decent performance.

E.4    EVALUATION METRICS

We adopt some quantitative metrics to evaluate the performance of each method on different tasks.
In section 4.1, we use two metrics to test the sample quality: Maximum Mean Discrepancy (MMD)
and High-quality Sample Rate (HSR). MMD measures the discrepancy between two distributions X

and Y , MMD(X, Y ) = ǁ 1  Σn     Φ(xi) −   1   Σm    Φ(yi)ǁ where xi and yj denote samples from

X and Y  respectively and Φ maps each sample to a RKHS. Here we use RBF kernel and calculate

MMD between generated samples and true samples. HSR statistics the rate of high-quality samples
over all generated samples.  For Two-Cirlce dataset, we define the generated points whose distance
from the nearest Gaussian component is less than σ₁  as high-quality samples.  We generate 2000
points in total and statistic HSR. For Two-Spiral dataset, we set the distance threshold as 5σ₂ and
generate 5000 points to calculate HSR.

As for Inception Score and CEPC. For MNIST, we pre-train a classifier for 10 digits which can pro-
vide the test accuracy up to 99% for calculation of scores.  The conditional entropy of predicted

classes  (CEPC)  for  given  samples  is  defined  as  H(y|x)  ≈   1  Σn     Σ10     p(yk|xi) log 
p(yk|xi)

where x is a generated instance and y denotes the predicted class given x from a pre-trained classi-
fier.  CEPC measures how well a given sample can be classfied into a right class, i.e.  the quality 
of
such sample. For CIFAR, we use the Inception V3 Network in Tensorflow as pre-trained classifier.

In section 4.2,  we use three metrics to characterize the performance for density estimation:  KL
divergence, JS divergence and AUC. We divide the map into a 300 meshgrid, calculate the unnor-
malized density values of each point given by the estimators and compute the KL and JS divergences
between estimated density and ground-truth density. Besides, we select the centers of each Gaussian
components as positive examples (expected to have high densities) and randomly sample 10 points
within a circle around each center as negative examples (expected to have relatively low densities)
and rank them according to the densities given by the model.  Then we obtain the area under the
curve (AUC) for false-positive rate v.s. true-positive rate.

25


Under review as a conference paper at ICLR 2020


Class

WGAN-GP      l1

l2

WGAN+LR     l1

l2

WGAN+ER     l1

l2

WGAN+VA     l1

l2

DGM           l1

l2

Joint-W         l1
l2

‘0’        ‘1’        ‘2’        ‘3’        ‘4’        ‘5’        ‘6’        ‘7’        ‘8’        
‘9’

20.3     11.4     14.3     14.8     13.5     13.3     13.8     11.0     13.0     12.3

1.74     1.07     0.82     0.98     0.83     0.68     0.95     0.62     0.82     0.75

13.8      5.9      13.6     19.1     11.8     18.3     10.7     11.5     14.0      9.9

0.80     0.34     0.84     1.81     0.65     1.37     0.62     0.70     0.90     0.57

16.1      8.9      11.7     14.2     12.3     10.8     13.9     11.4     12.1     10.9

1.20     0.74     0.54     0.86     0.73     0.54     0.97     0.69     0.72     0.63

16.3      7.1      13.7     13.7     11.9     13.2     13.6     11.2     12.1     10.6

1.12     0.35     0.81     0.85     0.69     0.76     1.04     0.71     0.74     0.71

22.2     10.9     12.7     10.2     10.8      9.0       9.5      10.9     12.7     11.7

1.41     0.83     0.81     0.65     0.67     0.56     0.66     0.67     0.88     0.76

14.1      7.5      14.3     12.9     11.1     11.0     13.7      9.7      12.0     11.5

0.89     0.47     0.93     0.73     0.55     0.51     1.06     0.53     0.70     0.97

Table 6:  l1 and l2 distances between means of true digits and generated digits in each class on
MNIST.


Class

WGAN-GP      l1

l2

WGAN+LR     l1

l2

WGAN+ER     l1

l2

WGAN+VA     l1

l2

DGM           l1

l2

Joint-W         l1
l2

‘0’          ‘1’          ‘2’          ‘3’          ‘4’          ‘5’          ‘6’          ‘7’      
    ‘8’          ‘9’

80.8       82.7       40.2       69.3       44.7       59.2       77.6      107.7     50.81      
89.3

1.75       1.84       0.92       1.57       1.04       1.40       1.78       2.32       1.78       
1.92

78.4       79.2       73.8       86.0       75.8       77.2      106.7     103.0      56.5       
92.3

1.63       1.76       1.59       1.88       1.68       1.74       2.36       2.23       1.24       
2.00

75.5       64.0      100.0      65.0       58.5       69.1       74.5       81.8       62.5       
71.3

1.56       1.45       2.04       1.43       1.35       1.57       1.67       1.82       1.40       
1.58

60.9       70.0       79.4       62.7       63.0       73.9       76.2       77.2       59.8       
66.4

1.32       1.55       1.68       1.39       1.42       1.63       1.70       1.77       1.33       
1.48

167.8     185.0     149.4     250.1     105.3     134.0     223.8     197.3     148.3     231.7

3.67       4.14       3.15       5.41       2.39       3.04       4.68       4.51       3.24       
5.25

59.3       58.1       77.3       54.8       58.1       65.1       63.9       82.8       59.1       
63.2

1.26       1.30       1.60       1.23       1.28       1.44       1.44       1.80       1.27       
1.43

Table 7:  l1 and l2 distances between means of true images and generated images in each class on
CIFAR. (Class ‘0’, ‘1’, ‘2’, ‘3’, ‘4’, ‘5’, ‘6’, ‘7’, ‘8’ and ‘9’ stand for ‘airplane’, 
‘automobile’, ‘bird’,
‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’ and ‘truck’ respectively.)

26


Under review as a conference paper at ICLR 2020

(a) Randomly sampled over all digits               (b)  Randomly sampled over digits with top 50% 
densi-

ties

Figure 9: Generated digits given by Joint-W on MNIST.

(a) Randomly sampled over all images              (b) Randomly sampled over images with top 50% 
densi-

ties

Figure 10: Generated images given by Joint-W on CIFAR.

(a) Generated digits with highest densities                       (b) Generated digits with lowest 
densities

(c) Real digits with highest densities                                (d) Real digits with lowest 
densities

Figure 11: The generated digits (and real digits) with the highest densities and the lowest 
densities
given by Joint-W.

27


Under review as a conference paper at ICLR 2020

(a) Generated digits with highest densities                       (b) Generated digits with lowest 
densities

(c) Real digits with highest densities                                (d) Real digits with lowest 
densities

Figure 12: The generated digits (and real digits) with the highest densities and the lowest 
densities
given by DGM.

(a) Generated digits with highest densities                       (b) Generated digits with lowest 
densities

(c) Real digits with highest densities                                (d) Real digits with lowest 
densities

Figure 13: The generated digits (and real digits) with the highest densities and the lowest 
densities
given by EGAN.

28

