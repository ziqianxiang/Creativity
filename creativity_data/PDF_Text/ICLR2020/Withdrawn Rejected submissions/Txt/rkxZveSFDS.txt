Under review as a conference paper at ICLR 2020
Quantum Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We introduce Quantum Graph Neural Networks (qgnn), a new class of quantum
neural network ansatze which are tailored to represent quantum processes which
have a graph structure, and are particularly suitable to be executed on distributed
quantum systems over a quantum network. Along with this general class of ansatze,
we introduce further specialized architectures, namely, Quantum Graph Recurrent
Neural Networks (qgrnn) and Quantum Graph Convolutional Neural Networks
(qgcnn). We provide four example applications of qgnn’s: learning Hamiltonian
dynamics of quantum systems, learning how to create multipartite entanglement in
a quantum network, unsupervised learning for spectral clustering, and supervised
learning for graph isomorphism classification.
1	Introduction
Variational Quantum Algorithms are a promising class of algorithms are rapidly emerging as a
central subfield of Quantum Computing (McClean et al., 2016; Farhi et al., 2014; Farhi & Neven,
2018). Similar to parameterized transformations encountered in deep learning, these parameterized
quantum circuits are often referred to as Quantum Neural Networks (QNNs). Recently, it was shown
that QNNs that have no prior on their structure suffer from a quantum version of the no-free lunch
theorem (McClean et al., 2018) and are exponentially difficult to train via gradient descent. Thus,
there is a need for better QNN ansatze. One popular class of QNNs has been Trotter-based ansatze
(Farhi et al., 2014; Hadfield et al., 2019). The optimization of these ansatze has been extensively
studied in recent works, and efficient optimization methods have been found (Verdon et al., 2019b; Li
et al., 2019). On the classical side, graph-based neural networks leveraging data geometry have seen
some recent successes in deep learning, finding applications in biophysics and chemistry (Kearnes
et al., 2016). Inspired from this success, we propose a new class of Quantum Neural Network
ansatz which allows for both quantum inference and classical probabilistic inference for data with a
graph-geometric structure. In the sections below, we introduce the general framework of the qgnn
ansatz as well as several more specialized variants and showcase four potential applications via
numerical implementation.
2	Background
2.1	Classical Graph Neural Networks
Graph Neural Networks (gnns) date back to Sperduti & Starita (1997) who applied neural networks
to acyclic graphs. Gori et al. (2005) and Scarselli et al. (2008) developed methods that learned node
representations by propagating the information of neighbouring nodes. Recently, gnns have seen
great breakthroughs by adapting the convolution operator from cnns to graphs (Bruna et al., 2013;
Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Niepert et al., 2016; Hamilton
et al., 2017; Monti et al., 2017). Many of these methods can be expressed under the message-passing
framework (Gilmer et al., 2017).
Let graph G = (A, X) where A ∈ Rn×n is the adjacency matrix, and X ∈ Rn×d is the node feature
matrix where each node has d features.
H(k) =P(A,H(k-1),W(k))
(1)
1
Under review as a conference paper at ICLR 2020
where H(k) ∈ Rn×d are the node representations computed at layer k, P is the message propagation
function and is dependent on the adjacency matrix, the previous node encodings and some learnable
parameters W(k). The initial embedding, H(0) is naturally X. One popular implementation of this
framework is the gcn (Kipf & Welling, 2016) which implements it as follows:
H (k) = P (A, H (k-1), W (k)) = ReLU(D - 1 AD - 1 H(kT)W(kT))	(2)
where A = A + I is the adjacency matrix with inserted self-loops, D = j Aij is the re-
normalization factor (degree matrix).
2.2	Networked Quantum Systems
Consider a graph G = {V, E}, where V is the set of vertices (or nodes) and E the set of edges. We
can assign a quantum subsystem with Hilbert space Hv for each vertex in the graph, forming a global
Hilbert space HV ≡ Nv∈V Hv . Each of the vertex subsystems could be one or several qubits, a
qudit, a qumode (Weedbrook et al., 2012), or even an entire quantum computer. One may also define
a Hilbert space for each edge and form HE ≡ Ne∈E He. The total Hilbert space for the graph would
then be HE XHV. For the sake of simplicity and feasibility of numerical implementation, We consider
this to be beyond the scope of the present work. The edges of the graph dictate the communication
betWeen the vertex subspaces: couplings betWeen degrees of freedom on tWo different vertices are
alloWed if there is an edge connecting them. This setup is called a quantum netWork (Kimble, 2008;
Qian et al., 2019) With topology given by the graph G.
3	Quantum Graph Neural Networks
3.1	General Quantum Graph Neural Network Ansatz
The most general Quantum Graph Neural NetWork ansatz is a parameterized quantum circuit on a
netWork Which consists of a sequence of Q different Hamiltonian evolutions, With the Whole sequence
repeated P times:
PQ
UQGNN (η, θ) = Y Y e-iηpq Hq (θ) ,	⑶
p=1 q=1
Where the product is time-ordered (Poulin et al., 2011), the η and θ are variational (trainable)
parameters, and the Hamiltonians Hq(θ) can generally be any parameterized Hamiltonians Whose
topology of interactions is that of the problem graph:
Hq(θ) ≡ X X WqrjkOjqr)㊈ Pkqr) + XX Bqrv Rjqv) ∙	(4)
{j,k}∈E r∈Ijk	v∈V r∈Jv
Here the Wqrjk and Bqrv are real-valued coefficients Which can generally be independent train-
able parameters, forming a collection θ ≡ ∪q,j,k,r{Wqrjk} ∪q,v,r {Bqrjk}. The operators
Rjqv),Ojqr),P(qr) are Hermitian operators which act on the Hilbert space of the jth node of the
graph. The sets Ijk and Jv are index sets for the terms corresponding to the edges and nodes,
respectively. To make compilation easier, we enforce that the terms of a given Hamiltonian Hq
commute with one another, but different Hq,s need not commute.
In order to make the ansatz more amenable to training and avoid the barren plateaus (quantum
parametric circuit no free lunch) problem (McClean et al., 2018), we need to add some constraints
and specificity. To that end, we now propose more specialized architectures where parameters are tied
spatially (convolutional) or tied over the sequential iterations of the exponential mapping (recurrent).
3.2	Quantum Graph Recurrent Neural Networks (qgrnn)
We define quantum graph recurrent neural networks as ansatze of the form of equation 3 where
the temporal parameters are tied between iterations, ηpq 7→ ηq . In other words, we have tied the
parameters between iterations of the outer sequence index (over p = 1, . . . , P). This is akin to
2
Under review as a conference paper at ICLR 2020
classical recurrent neural networks where parameters are shared over sequential applications of the
recurrent neural network map. As ηq acts as a time parameter for Hamiltonian evolution under
Hq, we can view the QGRNN ansatz as a Trotter-based (Lloyd, 1996; Poulin et al., 2011) quantum
simulation of an evolution c-zδHeff under the Hamiltionian HHeff = ∆-1 Pq ηqHq for a time step of
size ∆ = ∣∣ηkι = Pq ∣ηq|. This ansatz is thus specialized to learn effective quantum Hamiltonian
dynamics for systems living on a graph. In Section 3 we demonstrate this by learning the effective
real-time dynamics of an Ising model on a graph using a qgrnn ansatz.
3.3	Quantum Graph Convolutional Neural Networks (qgcnn)
Classical Graph Convolutional neural networks rely on a key feature: that of permutation invariance.
In other words, the ansatz should be invariant under permutation of the nodes. This is analogous
to translational invariance for ordinary convolutional transformations. In our case, permutation
invariance manifests itself as a constraint on the Hamiltonian, which now should be devoid of local
trainable parameters, and should only have global trainable parameters. The θ parameters thus
become tied over indices of the graph: Wqrjk 7→ Wqr and Bqrv 7→ Bqr . A broad class of graph
convolutional neural networks we will focus on is the set of so-called Quantum Alternating Operator
Ansatze (Hadfield et al., 2019), the generalized form of the Quantum Approximate Optimization
Algorithm ansatz (Farhi et al., 2014).
3.4	Quantum Spectral Graph Convolutional Neural Networks (qsgcnn)
We can take inspiration from the continuous-variable quantum approximate optimization ansatz
introduced in Verdon et al. (2019a) to create a variant of the qgcnn: the Quantum Spectral Graph
Convolutional Neural Network (qsgcnn). We show here how it recovers the mapping of Laplacian-
based graph convolutional networks (Kipf & Welling, 2016) in the Heisenberg picture, consisting of
alternating layers of message passing, node update, and nonlinearities.
Consider an ansatz of the form from equation 3 with four different Hamiltonians (Q = 4) for a given
graph. First, for a weighted graph G with edge weights Λjk , we define the coupling Hamiltonian as
HHC ≡ 2 P{j,k}∈E Ajk(Xj- Xk)2.
The Λjk here are the weights of the graph G , and are not trainable parameters. The operators
denoted here by Xj are quantum continuous-variable position operators, which can be implemented
via continuous-variable (analog) quantum computers (Weedbrook et al., 2012) or emulated using
multiple qubits on digital quantum computers(Somma, 2015; Verdon et al., 2018). After evolving
by HC , which we consider to be the message passing step, one applies an exponential of the kinetic
Hamiltonian, HHK ≡ ɪ Pj∙∈y pj. Here Pj denotes the continuous-variable momentum (Fourier
conjugate) of the position, obeying the canonical commutation relation [Xj ,pj] = iδj4. We consider
this step as a node update step. In the Heisenberg picture, the evolution generated by these two steps
maps the position operators of each node according to
■ T^t	-TT	__
e- γ Ke-	C ： Xj → Xj + YPj - αγ Pk∈y LjkXk,
where
Ljk = δjk	v∈V Λjv - Λjk
is the Graph Laplacian matrix for the weighted graph G . We can recognize this step as analogous to
classical spectral-based graph convolutions. One difference to note here is that momentum is free to
accumulate between layers.
Next, we must add some non-linearity in order to give the ansatz more capacity.1 The next evolution
is thus generated by an anharmonic Hamiltonian HA = Ej∈v f (Xj), where f ιsa nonlinear function
of degree greater than 2, e.g., a quartic potential of the form f (Xj) = ((Xj — μ)2 一 ω2)2 for some
1From a quantum complexity standpoint, adding a nonlinear operation (generated by a potential of degree
superior to quadratic) creates states that are non-Gaussian and hence are non efficiently simulatable on clas-
sical computers (Bartlett et al., 2002), in general composing layers of Gaussian and non-Gaussian quantum
transformations yields quantum computationally universal ansatz.(Lloyd & Braunstein, 1999).
3
Under review as a conference paper at ICLR 2020
A”① pj=U 一
scimanyd-tsop egarev
200	300
Iterations
Hamiltonian
Parameters
Initialization
Learned Values
Figure 1: Left: Batch average infidelity with respect to ground truth state sampled at 15 randomly
chosen times of quantum Hamiltonian evolution. We see the initial guess has a densely connected
topology and the QGRNN learns the ring structure of the true Hamiltonian. Right: Ising Hamiltonian
parameters (weights & biases) on a color scale.
μ, ω hyperparameters. Finally, we apply another evolution according to the kinetic Hamiltonian.
These last two steps yield an update
e-iβHK e-iδHA : Xj → Xj + βPj - δβf(Xj),
which acts as a nonlinear mapping. By repeating the four evolution steps described above in a
sequence of P layers, i.e.,
P
Uqsgcnn(α, β, Y, δ) = Y e-βHK e-iδjHA e-iγjHK e-iaHC
j=1
with variational parameters θ = {α, β, γ, δ}, we then recover a quantum-coherent analogue of the
node update prescription of Kipf & Welling (2016) in the original graph convolutional networks
2
paper.2
4	Applications & Experiments
4.1	Learning Quantum Hamiltonian Dynamics with Quantum Graph Recurrent
Neural Networks
Learning the dynamics of a closed quantum system is a task of interest for many applications (Wiebe
et al., 2014), including device characterization and validation. In this example, we demonstrate that
a Quantum Graph Recurrent Neural Network can learn effective dynamics of an Ising spin system
when given access to the output of quantum dynamics at various times.
Our target is an Ising Hamiltonian with transverse field on a particular graph,
U	^.. ^^	^	^
Htarget =	{j,k}∈E Jjk Zj Zk +	v∈V QvZv +	v∈V Xj.
We are given copies of a fixed low-energy state ∣ψoi as well as copies of the state ∣ψτ)≡
U(T) ∣ψoi = e-iτHtarget for some known but randomly chosen times T ∈ [0,Tmax]. Our goal
is to learn the target Hamiltonian parameters {Jjk, Qv}j,k,v∈v by comparing the state ∣ψτ)with
the state obtained by evolving ∣ψoi according to the QGRNN ansatz for a number of iterations
P ≈ T/∆ (where ∆ is a hyperparameter determining the Trotter step size). We achieve this by
training the parameters via Adam (Kingma & Ba, 2014) gradient descent on the average infidelity
L(θ) = 1 - Bb PB=I | hΨτ7∙ ∣UQgrnn(∆, θ) ∣ψoii∣2 averaged over batch sizes of 15 different times
T. Gradients were estimated via finite difference differentiation with step size = 10-4. The
fidelities (quantum state overlap) between the output of our ansatz and the time-evolved data state
were estimated via the quantum swap test (Cincio et al., 2018). The ansatz uses a Trotterization of a
random densely-connected Ising Hamiltonian with transverse field as its initial guess, and successfully
learns the Hamiltonian parameters within a high degree of accuracy as shown in Fig. 1a.
2For further physical intuition about the behaviour of this ansatz, note that the sum of the coupling and
kinetic Hamiltonians HK + HC is equivalent to the Hamiltonian of a network of quantum harmonic oscillators
coupled according to the graph weights and network topology. By adding a quartic HHa, we are thus emulating
parameterized dynamics on a harmonically coupled network of anharmonic oscillators.
4
Under review as a conference paper at ICLR 2020
Figure 2: Left: Stabilizer Hamiltonian expectation and fidelity over training iterations. A picture of
the quantum network topology is inset. Right: Quantum phase kickback test on the learned GHZ
state. We observe a 7x boost in Rabi oscillation frequency for a 7-node network, thus demonstrating
we have reached the Heisenberg limit of sensitivity for the quantum sensor network.
4.2	Quantum Graph Convolutional Neural Networks for Quantum Sensor
Networks
Quantum Sensor Networks are a promising area of application for the technologies of Quantum
Sensing and Quantum Networking/Communication (Kimble, 2008; Qian et al., 2019). A common
task considered where a quantum advantage can be demonstrated is the estimation of a parameter
hidden in weak qubit phase rotation signals, such as those encountered when artificial atoms interact
with a constant electric field of small amplitude (Qian et al., 2019). A well-known method to
achieve this advantange is via the use of a quantum state exhibiting multipartite entanglement of
the Greenberger-Horne-Zeilinger kind, also known as a GHZ state (Greenberger et al., 1989).
Here we demonstrate that, without global knowledge of the quantum network structure, a qgcnn
ansatz can learn to prepare a GHZ state. We use a QGCNN ansatz with H1 =	{j,k}∈E ZjZk
and H2 = j∈V Xj . The loss function is the negative expectation of the sum of stabilizer group
generators which stabilize the GHZ state (T6th & Guhne, 2005), i.e.,
L(η) = -hj X + Pn-122+ιiη
for a network of n qubits. Results are presented in Fig. 1b. Note that the advantage of using a QGNN
ansatz on the network is that the number of quantum communication rounds is simply proportional to
P , and that the local dynamics of each node are independent of the global network structure.
In order to further validate that we have obtained an accurate GHZ state on the network after training,
we perform the quantum phase kickback test on the network’s prepared approximate GHZ state (Wei
et al., 2019).3 We observe the desired frequency boost effect for our trained network preparing an
approximate GHZ state at test time, as displayed in Figure 2.
4.3	Unsupervised Graph Clustering with Quantum Graph Convolutional
Networks
As a third set of applications, we consider applying the qsgcnn from Section 2 to the task of spectral
clustering (Ng et al., 2002). Spectral clustering involves finding low-frequency eigenvalues of the
graph Laplacian and clustering the node values in order to identify graph clusters. In Fig. 3 we present
the results for a qsgcnn for varying multi-qubit precision for the representation of the continuous
values, where the loss function that was minimized was the expected value of the anharmonic potential
L(η) = hHC + HAiη. of particular interest to near-term quantum computing with low numbers if
qubits is the single-qubit precision case, where we modify the Qsgcnn construction as P → Xj,
3For this test, one applies a phase rotation Nj∈v e-i^Zj on all the qubits in paralel, then one applies a
sequence of CNoT’s (quantum adder gates) such as to concentrate the phase shifts onto a single collector node,
m ∈ V. Given that one had a GHZ state initially, one should then observe a phase shift e-in^Zm where n = |V|.
This boost in frequency of oscillation of the signal is what gives quantum multipartite entanglement its power to
increase sensitivity to signals to super-classical levels (Degen et al., 2017).
5
Under review as a conference paper at ICLR 2020
Energy: 1, Popularity: 4
Energy: -22.67, Popularity: 13
A--SUQα Λ4--qeqo,ld
PoSt-QAOA Energy Probability Density
IQ-2
lQ-3∙
ID-4-
10^s
IO-6
IQ-J
10-∙
10-»
Energy
Post-QAOA Energy Probability Density
Energy: 1, Popularity: 0 Energy: 1, Popularity: 1
Energy: 2, Popularity: 11
ytisneD ytlibaborP
Figure 3: QSGCNN spectral clustering results for 5-qubit precision (top) with quartic double-well
potential and 1-qubit precision (bottom) for different graphs. Weight values are represented as opacity
of edges, output sampled node values as grayscale. Lower precision allows for more nodes in the
simulation of the quantum neural network. The graphs displayed are the most probable (populated)
configurations, and to their right is the output probability distribution over potential energies. We see
lower energies are most probable and that these configurations have node values clustered.
HHa → I and Xj → |1)〈1j which transforms the coupling Hamiltonian as
HC → 1 P{j,k}∈E Ajk(∣iihi∣j -∣iihi∣k)2= Pjk Ljk ∣iihi∣j 乳田〈1|®,	(5)
where 11)〈11 k = (I - Zk )/2. We see that using a low-qubit precision yields sensible results, thus
implying that spectral clustering could be a promising new application for near-term quantum devices.
4.4	Graph Isomorphism Classification via Quantum Graph Convolutional
networks
Recently, a benchmark of the representation power of classical graph neural networks has been
proposed (XU et al., 2018) where one uses classical gcn's to identify whether two graphs are
isomorphic. In this spirit, using the QSGCNN ansatz from the previous subsection, we benchmarked
the performance of this Quantum Graph Convolutional Network for identifying isomorphic graphs.
We used the single-qubit precision encoding in order to order to simulate the execution of the quantum
algorithms on larger graphs.
Our approach was the following, given two graphs Gi and G2, one applies the single-qubit precision
QSGCNN ansatz Qp=ι eiηjHKeiγjHc with HHK = Pj∈V Xj and HHc from equation 5 in parallel
according to each graph,s structure. One then samples eigenvalues of the coupling Hamiltonian HC
on both graphs via standard basis measurement of the qubits and computation of the eigenvalue at
each sample of the wavefunction. One then obtains a set of samples of “energies” of this Hamiltonian.
By comparing the energetic measurement statistics output by the qsgcnn ansatz applied with
identical parameters θ = {η, Y} for two different graphs, one can then infer whether the graphs are
isomorphic.
We used the Kolmogorov-Smirnoff test (Lilliefors, 1967) on the distribution of energies sampled
at the output of the qsgcnn to determine whether two given graphs were isomorphic. In order to
determine the binary classification label deterministically, we considered all KS statistic values above
0.4 to indicate that the graphs were non-isomorphic. For training and testing purposes, we set the
6
Under review as a conference paper at ICLR 2020
----Train {50 Samples)
----Train (20 Samples)
Train (10 Samples)
Validation (50 Samples)
Validation (20 Samples)
Validation (10 Samples)
----Train (5000 Samples) ----------- Validation (5000 Samples)
----Train (1000 Samples) ----------- Validation (1000 Samples)
Train (400 Samples)	Validation (400 Samples)
Figure 4: Graph isomorphism loss curves for training and validation for various numbers of samples.
Left is for 6 node graphs and right is for 15 node graphs. The loss is based on the Kolmogorov-
Smirnov statistic comparing the sampled distribution of energies of the qgcnn output on two
graphs.
Table 1: Classification Accuracy for 15 Node Table 2: Classification Accuracy for 6 Node
Graphs	Graphs
Samples	Test	Validation	Samples	Test	Validation
5000	100.0	100.0	50	100.0	100.0
1000	100.0	100.0	20	90.0	100.0
400	100.0	100.0	10	100.0	80.0
loss function to be L(y, KS) = (1 - y)(1 - KS) + yKS, where y = 1 if graphs are isomorphic, and
y = 0 otherwise.
For the dataset, graphs were sampled uniformly at random; to prepare a balanced dataset, we selected
isomorphic and non-isomorphic pairs. In all of our experiments, we had 100 pairs of graphs for
training, 50 for validation, 50 for testing, and in all cases there are balanced isomorphic and non-
isomorphic pairs. The networks were trained via Adam gradient-based optimizer with batches of size
50.
Presented in Figure 4 is the training and testing losses for various graph sizes and numbers of energetic
samples. In Tables 1 and 2, we present the graph isomorphism classification accuracy for the training
and testing sets using the trained qgcnn with the previously described thresholded KS statistic as
the label. We see we get highly accurate performance even at low sample sizes. This seems to imply
that the qgcnn is fully capable of identifying graph isomorphism, as desired for graph convolutional
network benchmarks. We leave a comparison to similar scale classical graph convolutional networks
to future work.
5	Conclusion & Outlook
Results featured in this paper should be viewed as a promising set of first explorations of the potential
applications of qgnns. Through our numerical experiments, we have shown the use of these
qgnn ansatze in the context of quantum dynamics learning, quantum sensor network optimization,
unsupervised graph clustering, and supervised graph isomorphism classification. Given that there is a
vast set of literature on the use of Graph Neural Networks and their variants to quantum chemistry,
future works should explore hybrid methods where one can learn a graph-based hidden quantum
representation (via a qgnn) of a quantum chemical process. As the true underlying process is
quantum in nature and has a natural molecular graph geometry, the qgnn could serve as a more
accurate model for the hidden processes which lead to perceived emergent chemical properties. We
seek to explore this in future work. Other future work could include generalizing the qgnn to include
7
Under review as a conference paper at ICLR 2020
quantum degrees of freedom on the edges, include quantum-optimization-based training of the graph
parameters via quantum phase backpropagation (Verdon et al., 2018), and extending the qsgcnn to
multiple features per node.
Acknowledgments
Numerics in this paper were executed using a custom interface between Google’s Cirq and TensorFlow
(Abadi et al., 2016). The authors would like to thank Edward Farhi, Hartmut Neven, Jae Yoo, and Li
Li for useful discussions. GV, EL, and VS would like to thank the team at X for the hospitality and
support during their respective Quantum@X and AI@X residencies where this work was completed.
GV acknowledges funding from NSERC.
References
Mardn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Stephen D Bartlett, Barry C Sanders, Samuel L Braunstein, and Kae Nemoto. Efficient classical
simulation of continuous variable quantum information processes. Physical Review Letters, 88(9):
097904, 2002.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
LUkasz Cincio, Yigit Subagi, Andrew T Sornborger, and Patrick J Coles. Learning the quantum
algorithm for state overlap. New Journal of Physics, 20(11):113022, 2018.
Michael Defferrard, Xavier Bresson, and Pierre Vndergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Christian L Degen, F Reinhard, and P Cappellaro. Quantum sensing. Reviews of modern physics, 89
(3):035002, 2017.
Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term
processors. arXiv preprint arXiv:1802.06002, 2018.
Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A quantum approximate optimization algorithm.
arXiv preprint arXiv:1411.4028, 2014.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Daniel M Greenberger, Michael A Horne, and Anton Zeilinger. Going beyond bell’s theorem. In
Bell’s theorem, quantum theory and conceptions of the universe, pp. 69-72. Springer, 1989.
Stuart Hadfield, Zhihui Wang, Bryan O’Gorman, Eleanor G Rieffel, Davide Venturelli, and Rupak
Biswas. From the quantum approximate optimization algorithm to a quantum alternating operator
ansatz. Algorithms, 12(2):34, 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data.
arXiv preprint arXiv:1506.05163, 2015.
8
Under review as a conference paper at ICLR 2020
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):
595-608, 2016.
H Jeff Kimble. The quantum internet. Nature, 453(7198):1023, 2008.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Li Li, Minjie Fan, Marc Coram, Patrick Riley, and Stefan Leichenauer. Quantum optimization with a
novel gibbs objective function and ansatz architecture search. arXiv preprint arXiv:1909.07621,
2019.
Hubert W Lilliefors. On the kolmogorov-smirnov test for normality with mean and variance unknown.
Journal of the American statistical Association, 62(318):399-402, 1967.
Seth Lloyd. Universal quantum simulators. Science, pp. 1073-1078, 1996.
Seth Lloyd and Samuel L Braunstein. Quantum computation over continuous variables. In Quantum
Information with Continuous Variables, pp. 9-17. Springer, 1999.
Jarrod R McClean, Jonathan Romero, Ryan Babbush, and Aldn Aspuru-Guzik. The theory of
variational hybrid quantum-classical algorithms. New Journal of Physics, 18(2):023023, 2016.
Jarrod R McClean, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren
plateaus in quantum neural network training landscapes. Nature Communications, 9(1):4812,
2018.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-5124,
2017.
Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.
In Advances in neural information processing systems, pp. 849-856, 2002.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks
for graphs. In International conference on machine learning, pp. 2014-2023, 2016.
David Poulin, Angie Qarry, Rolando Somma, and Frank Verstraete. Quantum simulation of time-
dependent hamiltonians and the convenient illusion of hilbert space. Physical review letters, 106
(17):170501, 2011.
Kevin Qian, Zachary Eldredge, Wenchao Ge, Guido Pagano, Christopher Monroe, James V Porto,
and Alexey V Gorshkov. Heisenberg-scaling measurement protocol for analytic functions with
quantum sensor networks. arXiv preprint arXiv:1901.09042, 2019.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Rolando D Somma. Quantum simulations of one dimensional quantum systems. arXiv preprint
arXiv:1503.06319, 2015.
Alessandro Sperduti and Antonina Starita. Supervised neural networks for the classification of
structures. IEEE Transactions on Neural Networks, 8(3):714-735, 1997.
GeZa T6th and Otfried Guhne. Entanglement detection in the stabilizer formalism. Physical Review
A, 72(2):022340, 2005.
Guillaume Verdon, Jason Pye, and Michael Broughton. A universal training algorithm for quantum
deep learning. arXiv preprint arXiv:1806.09729, 2018.
9
Under review as a conference paper at ICLR 2020
GUillaUme Verdon, JUan MigUel Arrazola, Kamil Brddler, and Nathan Killoran. A quantum ap-
proximate optimization algorithm for continuous problems. arXiv preprint arXiv:1902.00409,
2019a.
GUillaUme Verdon, Michael BroUghton, Jarrod R McClean, Kevin J SUng, Ryan BabbUsh, Zhang
Jiang, HartmUt Neven, and MasoUd Mohseni. Learning to learn with qUantUm neUral networks via
classical neUral networks. arXiv preprint arXiv:1907.05415, 2019b.
Christian Weedbrook, Stefano Pirandola, Raul Garcia-Patr6n, Nicolas J Cerf, Timothy C Ralph,
Jeffrey H Shapiro, and Seth Lloyd. GaUssian qUantUm information. Reviews of Modern Physics,
84(2):621, 2012.
Ken X Wei, Isaac LaUer, Srikanth Srinivasan, Neereja SUndaresan, DoUglas T McClUre, David Toyli,
David C McKay, Jay M Gambetta, and Sarah Sheldon. Verifying mUltipartite entangled ghz states
via mUltiple qUantUm coherences. arXiv preprint arXiv:1905.05720, 2019.
Nathan Wiebe, Christopher Granade, Christopher Ferrie, and David G Cory. Hamiltonian learning
and certification Using qUantUm resoUrces. Physical review letters, 112(19):190501, 2014.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? arXiv preprint arXiv:1810.00826, 2018.
10