Under review as a conference paper at ICLR 2020
On the Evaluation of Conditional GANs
Anonymous authors
Paper under double-blind review
Ab stract
Conditional Generative Adversarial Networks (cGANs) are finding increasingly
widespread use in many application domains. Despite outstanding progress, quan-
titative evaluation of such models often involves multiple distinct metrics to assess
different desirable properties, such as image quality, conditional consistency, and
intra-conditioning diversity. In this setting, model benchmarking becomes a chal-
lenge, as each metric may indicate a different “best” model. In this paper, we
propose the Fr6chet Joint Distance (FJD), which is defined as the Fr6chet distance
between joint distributions of images and conditioning, allowing it to implicitly cap-
ture the aforementioned properties in a single metric. We conduct proof-of-concept
experiments on a controllable synthetic dataset, which consistently highlight the
benefits of FJD when compared to currently established metrics. Moreover, we
use the newly introduced metric to compare existing cGAN-based models for a
variety of conditioning modalities (e.g. class labels, object masks, bounding boxes,
images, and text captions). We show that FJD can be used as a promising single
metric for cGAN benchmarking and model selection.
1	Introduction
The use of generative models is growing across many domains (van den Oord et al., 2016c; Vondrick
et al., 2016; Serban et al., 2017; Karras et al., 2018; Brock et al., 2019). Among the most promising
approaches, Variational Auto-Encoders (VAEs) (Kingma & Welling, 2014), auto-regressive models
(van den Oord et al., 2016a;b), and Generative Adversarial Networks (GANs) (Goodfellow et al.,
2014) have been driving significant progress, with the latter at the forefront of a wide-range of
applications (Mirza & Osindero, 2014; Reed et al., 2016; Zhang et al., 2018a; Vondrick et al., 2016;
Almahairi et al., 2018; Subramanian et al., 2018; Salvador et al., 2019). In particular, significant
research has emerged from practical applications, which require generation to be based on existing
context. For example, tasks such as image inpainting, super-resolution, or text-to-image synthesis
have been successfully addressed within the framework of conditional generation, with conditional
GANs (cGANs) among the most competitive approaches. Despite these outstanding advances,
quantitative evaluation of GANs remains a challenge (Theis et al., 2016; Borji, 2018).
In the last few years, a significant number of evaluation metrics for GANs have been introduced in
the literature (Salimans et al., 2016; HeUSel et al., 2017; BinkOWSki et al., 2018; Shmelkov et al.,
2018; Zhou et al., 2019; Kynkaanniemi et al., 2019; Ravuri & Vinyals, 2019). Although there is no
clear consensus on which quantitative metric is most appropriate to benchmark GAN-based models,
Inception Score (IS) (Salimans et al., 2016) and FreChet Inception Distance (FID) (Heusel et al.,
2017) have been extensively used. However, both IS and FID were introduced in the context of
unconditional image generation and, hence, focus on capturing certain desirable properties such as
visual quality and sample diversity, which do not fully encapsulate all the different phenomena that
arise during conditional image generation.
In conditional generation, we care about visual quality, conditional consistency 一 i.e., verifying that
the generation respects its conditioning, and intra-Conditioning diversity - i.e., sample diversity per
conditioning. Although visual quality is captured by both metrics, IS is agnostic to intra-conditioning
diversity and FID only captures it indirectly.1 Moreover, neither of them can capture conditional con-
1FID compares image distributions and, as such, should be able to roughly capture the intra-conditioning
diversity. Since it cares about the image marginal distribution exclusively, it fails to capture intra-conditioning
diversity when changes only affect the image-conditioning joint distribution. See Appendix A.
1
Under review as a conference paper at ICLR 2020
sistency. In order to overcome these shortcomings, researchers have resorted to reporting conditional
consistency and diversity metrics in conjunction with FID (Zhao et al., 2019; Park et al., 2019).
Consistency metrics often use some form of concept detector to ensure that the requested conditioning
appears in the generated image as expected. Although intuitive to use, these metrics require pre-
trained models that cover the same target concepts in the same format as the conditioning (i.e.,
classifiers for image-level class conditioning, semantic segmentation for mask conditioning, etc.),
which may or may not be available off-the-shelf. Moreover, using different metrics to evaluate
different desirable properties may hinder the process of model selection, as there may not be a single
model that surpasses the rest in all measures. In fact, it has recently been demonstrated that there is a
natural trade-off between image quality and sample diversity (Yang et al., 2019), which calls into
question how we might select the correct balance of these properties.
In this paper We introduce a new metric called Frechet Joint Distance (FJD), which is able to implicitly
assess image quality, conditional consistency, and intra-conditioning diversity. FJD computes the
Frechet distance on an embedding of the joint image-conditioning distribution, and introduces only
small computational overhead over FID compared to alternative methods. We evaluate the properties
of FJD on a variant of the synthetic dSprite dataset (Matthey et al., 2017) and verify that it successfully
captures the desired properties. We provide an analysis on the behavior of both FID and FJD under
different types of conditioning such as class labels, bounding boxes, and object masks, and evaluate
a variety of existing cGAN models for real-world datasets with the newly introduced metric. Our
experiments show that (1) FJD captures the three highlighted properties of conditional generation;
(2) it can be applied to any kind of conditioning (e.g., class, bounding box, mask, image, text, etc.);
and (3) when applied to existing cGAN-based models, FJD demonstrates its potential to be used as a
promising unified metric for hyper-parameter selection and cGAN benchmarking. To our knowledge,
there are no existing metrics for conditional generation that capture all of these key properties.
2	Related Work
Conditional GANs have witnessed outstanding progress in recent years. Training stability has been
improved through the introduction of techniques such as progressive growing, Karras et al. (2018),
spectral normalization (Miyato et al., 2018) and the two time-scale update rule (Heusel et al., 2017).
Architecturally, conditional generation has been improved through the use of auxiliary classifiers
(Odena et al., 2017) and the introduction of projection-based conditioning for the discriminator
(Miyato & Koyama, 2018). Image quality has also benefited from the incorporation of self-attention
(Zhang et al., 2018a), as well as increases in model capacity and batch size (Brock et al., 2019).
All of this progress has led to impressive results, paving the road towards the challenging task of
generating more complex scenes. To this end, a flurry of works have tackled different forms of
conditional image generation, including class-based (Mirza & Osindero, 2014; Heusel et al., 2017;
Miyato et al., 2018; Odena et al., 2017; Miyato & Koyama, 2018; Brock et al., 2019), image-based
(Isola et al., 2017; Zhu et al., 2017a; Wang et al., 2018; Zhu et al., 2017b; Almahairi et al., 2018;
Huang et al., 2018; Mao et al., 2019), mask- and bounding box-based (Hong et al., 2018; Hinz et al.,
2019; Park et al., 2019; Zhao et al., 2019), as well as text- (Reed et al., 2016; Zhang et al., 2017;
2018a; Xu et al., 2018; Hong et al., 2018) and dialogue-based conditionings (Sharma et al., 2018;
El-Nouby et al., 2019). This intensified research has lead to the development of a variety of metrics
to assess the three factors of conditional image generation process quality, namely: visual quality,
conditional consistency, and intra-conditioning diversity.
Visual quality. A number of GAN evaluation metrics have emerged in the literature to assess visual
quality of generated images in the case of unconditional image generation. Most of these metrics
either focus on the separability between generated images and real images (Lehmann & Romano,
2005; Radford et al., 2016; Yang et al., 2017; Isola et al., 2017; Zhou et al., 2019), compute the
distance between distributions (Gretton et al., 2012; Heusel et al., 2017; Arjovsky et al., 2017),
assess sample quality and diversity from conditional or marginal distributions (Salimans et al., 2016;
Gurumurthy et al., 2017; Zhou et al., 2018), measure the similarity between generated and real images
(Wang et al., 2004; Xiang & Li, 2017; Snell et al., 2017; Juefei-Xu et al., 2017) or are log-likelihood
based (Theis et al., 2016)2. Among these, the most accepted automated visual quality metrics are
2We refer the reader to (Borji, 2018) for a detailed overview and insightful discussion of existing metrics.
2
Under review as a conference paper at ICLR 2020
Inception Score (IS) (Salimans et al., 2016) and Frechet Inception Distance (FID) (HeUsel et al.,
2017).
Conditional consistency. To assess the consistency of the generated images with respect to model
conditioning, researchers have reverted to available, pre-trained feed-forward models. The strUctUre of
these models depends on the modality of the conditioning (e.g. segmentation models are Used for mask
conditioning or image captioning models are applied to evalUate text conditioning). Moreover, the
metric Used to evalUate the forward model on the generated distribUtion depends on the conditioning
modality and inclUdes: accUracy in the case of class-conditioned generation, Intersection over
Union when Using boUnding box- and mask-conditionings, BLEU (Papineni et al., 2002), METEOR
(Banerjee & Lavie, 2005) or CIDEr (Vedantam et al., 2015) in the case of text-based conditionings,
and StrUctUral Similarity (SSIM) or peak signal-to-noise ratio (PSNR) for image-conditioning.
Intra-conditioning diversity. The most common metric for evalUating sample diversity is Learned
PerceptUal Image Patch Similarity (LPIPS) (Zhang et al., 2018b), which measUres the distance
between samples in a learned featUre space. Alternatively, (Miyato & Koyama, 2018) proposed
Intra-FID, which calcUlates a FID score separately for each conditioning and reports the average
score over all conditionings. This method shoUld in principle captUre the desirable properties of
image qUality, conditional consistency, and intra-class diversity. However, it scales poorly with the
nUmber of UniqUe conditions, as the compUtationally intensive FID calcUlation mUst be repeated for
each case, and because FID behaves poorly when the sample size is small (BinkoWski et al., 2018).
FUrthermore, in cases where the conditioning cannot be broken down into a set of discrete classes
(e.g., pixel-based conditioning), Intra-FID is intractable. As a result, it has not been applied beyond
class-conditioning.
3	Review of Frechet Inception Distance (FID)
FID aims to compare the statistics of generated samples to samples from a real dataset. Given two
multivariate Gaussian distributions N(μ, Σ) and N(μ, Σ), Fr6chet Distance (FD) is defined as:
d2 ((μ, Σ),(μ, Σ)) = ∣∣μ - μ∣∣2 + Tr (∑ + Σ - 2(ΣΣ)1/2) .	(1)
When evaluating a generative model, N(μ, Σ) represents the data (reference) distribution, obtained
by fitting a Gaussian to samples from a reference dataset, and N(μ, Σ) represents the learned
(generated) distribution, a result of fitting to samples from a generative model.
In FID, both the real images and model samples are embedded in a learned feature space using a
pre-trained Inception v3 model (Szegedy et al., 2016). Thus, the Gaussian distributions are defined in
the embedded space. More precisely, given a dataset of images {x(i)}iN=0, a set of model samples
{χ⑴}Mo, and an Inception embedding function f, We estimate the Gaussian parameters μ, Σ, μ
and Σ as:
NN	T
μ = N Xf(X(i)), ς = N-I Xff(x(i)) - μ) ff(Xci))- μ),
i=0	i=0
MM	T
μ = mm Xf(χ(i)), ς = M-I X(f(χ(i))- μ)(f(χ(i)) - μ).
i=0	i=0
(2)
(3)
4	Fréchet Joint Distance (FJD)
In conditional image generation, a dataset is composed of image-condition pairs {(χ(i), y(i))}iN=0,
Where the conditioning can take variable forms, such as image-level classes, segmentation masks, or
text. The goal of conditional image generation is to produce realistic looking, diverse images X that
are consistent with the conditioning y. Thus, a set of model samples with corresponding conditioning
can be defined as: {(X(i), y(i))}M=0.
As discussed in Section 3, the Frechet distance (FD) compares any two Gaussians defined over
arbitrary spaces. In FJD, we propose to compute the FD between two Gaussians defined over the
joint image-conditioning embedding space.
3
Under review as a conference paper at ICLR 2020
More precisely, given an image embedding function f , a conditioning embedding function h, a condi-
tioning embedding scaling factor α, and a merging function g that combines the image embedding
with the conditioning embedding into a joint one, we can estimate the respective Gaussian parameters
μ, Σ, μ and Σ as:
1N	1M
μ = N Xg f(χCi)),αh(yCi))), μ = M Xg f(χCi)),αh(yCi))),	⑷
i=0	i=0
1N	T
∑ = N-I X (g f (x(i)), αh(y(i))) - μ) (g f (x(i)), αh(y(i))) - μ) ,	(5)
i=0
MT
∑ = M-IX W(Xii) αh(y(i))) - μ) (g f (χ(i)), αh(y(i))) - μ) .	(6)
M-1 i=0
Note that by computing the FD over the joint image-conditioning distribution, we are able to
simultaneously assess image quality, conditional consistency, and intra-conditioning diversity, all of
which are important factors in evaluating the quality of conditional image generation models.
To ensure reproducibility, when reporting FJD scores it is important to include details such as which
conditioning embedding function was used, which dataset is used for the reference distribution, and
the α value. We report these values for all of our experiments in Appendix B.
4.1	CONDITIONING EMBEDDING FUNCTION: h
The purpose of the embedding function
h is to reduce the dimensionality and
extract a useful feature representation of
the conditioning. As such, the choice
of h will vary depending on the modal-
ity of conditioning. In most cases, an
off-the-shelf, pretrained embedding can
be used for the purposes of extracting
a useful representation. In the absence
of preexisting, pretrained conditioning
Table 1: Suggested embedding functions for different con-
ditioning modalities.
Conditioning Modality	Embedding Function
Class / Attribute labels	One-hot encoding
Bounding boxes / Masks	Regularized AE (Ghosh et al., 2019)
Images	Inceptionv3 (Szegedy et al., 2016)
Captions / Dialogue	Sentence-BERT (Reimers & Gurevych, 2019)
embedding functions, a new one should be learned. For example, for bounding box and mask condi-
tionings the embedding function could be learned with an autoencoder. 3 For suggested assignments
of conditioning modalities to embedding functions please refer to Table 1.
4.2	CONDITIONING EMBEDDING SCALING FACTOR: α
In order to control the relative contribution of the image component and the conditioning component
to the final FJD value, we scale the conditioning embedding by a constant α. In essence, α indicates
how much we care about the conditioning component compared to the image component. When
α = 0, the conditioning component is ignored and FJD is equivalent to FID. As the value of α
increases, the perceived importance of the conditioning component is also increased and reflected
accordingly in the resulting measure. To equally weight the image component and the conditioning
component, we recommend setting α to be equal to the ratio between the average L2 norm of the
image embedding and the conditioning embedding. This weighting ensures that FJD retains consistent
behaviour across conditioning embeddings, even with varying dimensionality or magnitude. We note
that α should be calculated on data from the reference distribution (real data distribution), and then
applied to all conditioning embeddings thereafter. See Appendix F for an example of the effect of the
α hyperparameter.
3In the initial stages of this project, we also explored methods to bypass this additional training step by
projecting a visual representation of bounding box or mask conditioning into an Inceptionv3 embedding space.
However, the Inceptionv3 embedding may not properly capture object positions as it is trained to classify, discard-
ing precise spatial information. Therefore, we consider autoencoders (AE) to be better suited to our setup since
they are trained to recover both object appearance and spatial information from the embedded representation.
4
Under review as a conference paper at ICLR 2020
4.3	MERGING FUNCTION: g
The purpose of the merging function g is to combine the image embedding and conditioning embed-
ding into a single joint embedding. We compared several candidate merging functions and found
concatenation of the image embedding and conditioning embedding vectors to be most effective, both
in terms of simplicity and performance. As such, concatenation is used as the merging function in all
following experiments.
5	Evaluation of THE Properties of Frechet Joint Distance
In this section, we demonstrate that FJD captures the three desiderata of conditional image generation,
namely image quality, conditional consistency and intra-conditioning diversity.
5.1	Dataset
dSprite-textures. The dsprite dataset (Matthey et al., 2017) is a synthetic dataset where each image
depicts a simple 2D shape on a black background. Each image can be fully described by a set
of factors, including shape, scale, rotation, x position, and y position. We augment the dsprite
dataset to create dSprite-textures by adding three texture patterns for each sample. Additionally, we
include class labels indicating shape, as well as bounding boxes and mask labels for each sample (see
Figure 1). in total, the dataset contains 2,211,840 unique images. This synthetic dataset allows us to
exactly control our sample distribution and, thereby, simulate a generator with image-conditioning
inconsistencies or reduced sample diversity. To embed the conditioning for calculating FJD in the
following experiments, we use one-hot encoding for the class labels, and autoencoder representations
for the bounding box and mask labels.4 We are releasing the code to generate dsprite-textures.
Figure 1: Left: dsprite-textures images. center: Bounding box labels. Right: Mask labels.
5.2	Image Quality
in this subsection, we aim to test the sensitivity of FJD
to image quality perturbations. To do so, we draw 10k
random samples from the dsprite-textures dataset to form
a reference dataset. The generated dataset is simulated by
duplicating the reference dataset and adding Gaussian noise
drawn from N(0, σ) to the images, where σ ∈ [0, 0.25]
and pixel values are normalized (and clipped after noise
addition) to the range [0, 1]. The addition of noise mimics
a generative model that produces low quality images. We
repeat this experiment for all three conditioning types in
dsprite-textures: class, bounding box, and mask.
Results are shown in Figure 2, where we plot both FiD
and FJD as a function of the added Gaussian noise (σ is
indicated on the x-axis as Noise Magnitude). We find that,
in all cases, FJD has a very similar trend to FiD, indicating
that it successfully captures image quality. Additional
image quality experiments on the large scale coco-stuff
dataset can be found in Appendix c.
200
Noise Magnitude
Figure 2: Image quality: FiD and FJD
exhibit similar trends for class, bound-
ing box, and mask conditioning under
varying noise levels added to images.
4Architectural details for autoencoders used in this paper can be found in Appendix G.
5
Under review as a conference paper at ICLR 2020

Scale	Orientation
0 20 40 60 BO 100 120 140 160 IBO
Offset (degrees)
0	12	3
OffS改(scale level)
Figure 3: Conditional consistency: Change in FJD with respect to offset on Dsprite-textures dataset
for class, bounding box and mask conditionings.
X Position

Y Position
0	2	4	6 B 10 12 14 16
Offset (pixels)
0	2	4	6 B 10 12 14 16
Offset (pixels)
5.3	Conditional Consistency
In this subsection, we aim to highlight the sensitivity of FJD to conditional consistency. In particular,
we target specific types of inconsistencies, such as incorrect scale, orientation, or position. We draw a
set of 10k samples from the dSprite-textures dataset and duplicate it to represent the reference dataset
and the generated dataset, each with identical image and conditioning marginal distributions. For
30% of the generated dataset samples we swap conditionings of pairs of samples that are identical
in all but one of the attributes (scale, orientation, x position or y position). For example, if one
generated sample has attribute x position 4 and a second generated sample has attribute x position 7,
swapping their conditionings leads to generated samples that are offset by 3 pixels w.r.t. their ground
truth x position. Swapping conditionings in this manner allows us to control for specific attributes’
conditional consistency, while keeping the image and conditioning marginal distributions unchanged.
As a result, all changes in FJD can be attributed solely to conditional inconsistencies.
Figure 3 depicts the results of this experiment for four different types of alterations: scale, orientation,
and x and y positions. We observe that the FID between image distributions (solid blue line) remains
constant even as the degree of conditional inconsistency increases. For class conditioning (dotted
orange line), FJD also remains constant, as changes to scale, orientation, and position are independent
of the object class. Bounding box and mask conditionings, as they contain spatial information,
produce variations in FJD that are proportional to the offset. Interestingly, for the orientation offsets,
FJD with mask conditioning fluctuates rather than increasing monotonically. This behaviour is due to
the orientation masks partially re-aligning with the ground truth around 90° and 180°. Each of these
cases emphasize the effective sensitivity of FJD with respect to conditional consistency. Additional
conditional consistency experiments with text conditioning can be found in Appendix D.
5.4	Intra-conditioning Diversity
In this subsection, we aim to test the sensitivity of FJD to intra-conditioning diversity5, by alter-
nating the per-conditioning image texture variability. More precisely, we vary the texture based on
four different image attributes: shape that is captured in all tested conditionings, as well as scale,
orientation and position that are captured by bounding box and mask conditionings only. To create
attribute-texture assignments, we stratify attributes based on their values. For example, one possible
shape-based stratification of a dataset with three shapes might be: [squares, ellipses, hearts]. To
quantify the dataset intra-conditioning diversity, we introduce a diversity score. A diversity score of 1
means that the per-attribute texture distribution is uniform across stratas, while a diversity score of
0 means that each strata is assigned to a single texture. Middling diversity scores indicate that the
textural distribution is skewed towards one texture type in each strata. We create our reference dataset
by randomly drawing 10k samples. The generated distribution is created by duplicating the reference
distribution and adjusting the per-attribute texture variability to achieve the desired diversity score.
The results of these experiments are shown in Figure 4, which plots the increase in FID and FJD,
for different types of conditioning, as the diversity of textures within each subset decreases. For all
tested scenarios, we observe that FJD is sensitive to intra-conditioning diversity changes. Moreover,
not surprisingly, since a change in the joint distribution of attributes and textures also implies a
change to the image marginal distribution, we observe that FID increases with reduced diversity. This
5Note that for real datasets, intra-conditioning diversity is most often reduced as the strength of conditioning
increases (e.g., mask conditionings usually present a single image instantiation, presenting no diversity).
6
Under review as a conference paper at ICLR 2020
X Position
Shape
Scale
Orientation
Figure 4: Intra-conditioning diversity: FJD and FID as intra-conditioning diversity decreases.
----FID
FJD, class
--FJD, bboχ
FJD, mask
Intra-Conditioning Diversity

experiment suggests that FID is able to capture intra-conditioning diversity changes when the image
conditional distribution is also affected. However, if the image marginal distribution were to stay
constant, FID would be blind to intra-conditioning diversity changes (as is shown in Section 5.3).
6	Evaluation of existing conditional generation models
In this section, we seek to demonstrate the application of FJD to evaluate models with several
different conditioning modalities, in contrast to FID and standard conditional consistency and
diversity metrics. We focus on testing class-conditioned, image-conditioned, and text-conditioned
image generation tasks, which have been the focus of numerous works6. Multi-label, bounding box,
and mask conditioning are also explored in Appendix I. We note that FJD and FID yield similar
rankings of models in this setting, which is to be expected since most models use similar conditioning
mechanisms. Rankings are therefore dominated by image quality, rather than conditional consistency.
We refer the reader to Appendix F and H for examples of cases where FJD ranks models differently
than FID.
Class-conditioned cGANs. Table
2 compares three state-of-the-art
class-conditioned generative models
trained on ImageNet at 128 × 128
resolution. Specifically, we evalu-
ate SN-GAN (Miyato et al., 2018)
trained with and without a projection
discriminator (Miyato & Koyama,
2018), and BigGAN (Brock et al.,
2019). Accuracy is used to evaluate
conditional consistency, and is com-
Table 2: Comparison of class-conditioned models trained on
ImageNet (resolution 128 × 128).
	FJD J	FID J	Acc. ↑	Diversity ↑
SN-GAN (concat)	63.7	39.8	18.2	0.622
SN-GAN (proj)	41.7	27.4	35.7	0.612
BigGAN	17.0	9.55	67.4	0.550
puted as the Inception v3 accuracy of each model’s generated samples, using their conditioning as
classification ground truth. Class labels from the validation set are used as conditioning to generate
50k samples for each model, and the training set is used as the reference distribution. One-hot
encoding is used to embed the class conditioning for the purposes of calculating FJD.
We find that FJD follows the same trend as FID for class-conditioned models, preserving their
ranking and highlighting the FJD’s ability to capture image quality. Additionally, we note that the
difference between FJD and FID correlates with each model’s classification accuracy, with smaller
gaps appearing to indicate better conditional consistency. Diversity scores, however, rank models in
the opposite order compared to all other metrics.
This behaviour evokes the trade-off between realism and diversity highlighted by Yang et al. (2019).
Ideally, we would like a model that produces diverse outputs, but this property is not as attractive if it
also results in a decrease in image quality. At what point should diversity be prioritized over image
quality, and vice versa? FJD is a suitable metric for answering this question if the goal is to find a
model that best matches the target conditional data generating distribution.
Image-conditioned cGANs. Table 3 compares four state-of-the-art image translation models:
Pix2pix (Isola et al., 2017), BicycleGAN (Zhu et al., 2017b), MSGAN (Mao et al., 2019), and
MUNIT (Huang et al., 2018). We evaluate on four different image-to-image datasets: Facades
(Tylecek & Sdra, 2013), Maps (Isola et al., 2017), Edges2Shoes and Edges2Handbag (ZhU et al.,
6A list of pre-trained models used in these evaluations can be found in Appendix E.
7
Under review as a conference paper at ICLR 2020
Table 3: Comparison of image-conditioned models. Results averaged over 5 runs.
Dataset	Facades				Maps			
	FJD j	FID j	Consistency j	Diversity ↑	FJD j	FID j	Consistency j	Diversity ↑
Pix2pix	161.3	104.0	0.413	0.056	233.4	106.8	0.444	0.049
BicycleGAN	145.9	85.0	0.436	0.289	220.4	93.2	0.449	0.247
MSGAN	152.4	93.1	0.478	0.376	249.3	123.3	0.478	0.452
			Edges2Shoes			Edges2Handbags		
Pix2pix	115.4	74.2	0.215	0.040	162.3	95.6	0.314	0.042
BicycleGAN	88.2	47.3	0.239	0.191	142.1	76.0	0.324	0.252
MUNIT	98.1	56.2	0.270	0.229	147.9	79.1	0.382	0.339
2016). To assess conditional consistency we utilize LPIPS to measure the average distance between
generated images and their corresponding ground truth images. Conditioning from the validation
sets are used to generate images, while the training sets are used as reference distributions. An
Inceptionv3 model is used to embed the image conditioning for the FJD calculation. Due to the small
size of the validation sets, we report scores averaged over 5 evaluations of each model.
In this setting we encounter some ambiguity with regards to model selection, as for all datasets, each
metric ranks the models differently. BicycleGAN appears to have the best image quality, Pix2pix
produces images that are most visually similar to the ground truth, and MSGAN and MUNIT achieve
the best sample diversity scores. This scenario demonstrates the benefits of using a single unified
metric for model selection, for which there is only a single best model.
Text-conditioned cGANs. Table 4 shows FJD and FID scores for three state-of-the-art text-conditioned mod- els trained on the Caltech-UCSD Birds 200 dataset (CUB-200) (Welin- der et al., 2010) at 256 × 256 resolu- tion: HDGan (Zhang et al., 2018c), StackGAN++ (Zhang et al., 2018a), and AttnGAN (Xu et al., 2018). Con- ditional consistency is evaluated us-	Table 4: Comparison of text-conditioned models trained on CUB-200 (resolution 256 × 256). FJD ； FID ； VS sim. ↑ Diversity ↑ HDGan	26.1	23.3	0.340	0.687 StackGAN++	21.8	18.4	0.341	0.652 AttnGAN	16.7	13.6	0.477	0.625
ing visual-semantic similarity, as proposed by Zhang et al. (2018c). Conditioning from the test set
captions is used to generate 30k images, and the same test set is also used as the reference distribution.
We use pre-computed Char-CNN-RNN sentence embeddings as the conditioning embedding for FJD,
since they are commonly used with CUB-200 and are readily available.
In this case we find that AttnGAN dominates in terms of conditional consistency compared to HDGan
and StackGAN++, while all models are comparable in terms of diversity. AttnGAN is ranked best
overall by FJD. In cases where the biggest differentiator between the models is image quality, FID
and FJD will provide a consistent ranking as we see here. In cases where the trade-off is more subtle
we believe practitioners will opt for a metric that measurably captures intra-conditioning diversity.
7 Conclusions
In this paper We introduce Frechet Joint Distance (FJD), which is able to assess image quality,
conditional consistency, and intra-conditioning diversity within a single metric. We compare FJD to
FID on the synthetic dSprite-textures dataset, validating its ability to capture the three properties of
interest across different types of conditioning, and highlighting its potential to be adopted as a unified
cGAN benchmarking metric. We also demonstrate how FJD can be used to address the potentially
ambiguous trade-off between image quality and sample diversity when performing model selection.
Looking forward, FJD could serve as valuable metric to ground future research, as it has the potential
to help elucidate the most promising contributions within the scope of conditional generation.
8
Under review as a conference paper at ICLR 2020
References
Amjad Almahairi, Sai Rajeshwar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. Aug-
mented CycleGAN: Learning many-to-many mappings from unpaired data. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings ofMachine Learning Research, pp. 195-204, Stockholmsmassan, Stock-
holm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.ρress∕v80∕
almahairi18a.html.
Martin Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein generative adversarial networks.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214-223,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL http:
//proceedings.mlr.press/v70/arjovsky17a.html.
Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.
Mikolaj BinkOWski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying
MMD GANs. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=r1lUOzWCW.
Ali Borji. Pros and cons of GAN evaluation measures. CoRR, abs/1802.03446, 2018.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in
context. In CVPR, pp. 1209-1218. IEEE Computer Society, 2018.
Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, R. Devon Hjelm, Layla El Asri,
Samira Ebrahimi Kahou, Yoshua Bengio, and Graham W. Taylor. Tell, draw, and repeat: Gen-
erating and modifying images based on continual linguistic instruction. CoRR, abs/1811.09845,
2019.
Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael J. Black, and Bernhard Scholkopf.
From variational to deterministic autoencoders. CoRR, abs/1903.12436, 2019. URL http:
//arxiv.org/abs/1903.12436.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural
Information Processing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014. URL
http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. J. Mach. Learn. Res., 13(1):723-773, March 2012. ISSN 1532-4435.
URL http://dl.acm.org/citation.cfm?id=2503308.2188410.
Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R. Venkatesh Babu. Deligan: Generative
adversarial networks for diverse and limited data. In Computer Vision and Pattern Recognition,
pp. 4941-4949. IEEE Computer Society, 2017. ISBN 978-1-5386-0457-1. URL http://
dblp.uni-trier.de/db/conf/cvpr/cvpr2017.html#GurumurthySB17.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, pp. 6626-6637. Curran Associates, Inc.,
2017. URL http://papers.nips.cc/paper/7240-gans-trained-by-a-two-
time-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf.
9
Under review as a conference paper at ICLR 2020
Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Generating multiple objects at spatially distinct
locations. In International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=H1edIiA9KQ.
Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for
hierarchical text-to-image synthesis. In Computer Vision and Pattern Recognition, pp. 7986-7994.
IEEE Computer Society, 2018.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image
translation. In ECCV, 2018.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. Computer Vision and Patter Recognition (CVPR), 2017.
Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In CVPR, pp.
1219-1228. IEEE Computer Society, 2018.
Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Gang of gans: Generative
adversarial networks with maximum margin ranking. CoRR, abs/1704.04865, 2017. URL
http://arxiv.org/abs/1704.04865.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for
improved quality, stability, and variation. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=Hk99zCeAb.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. arXiv preprint arXiv:1904.06991,
2019.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
E. L. Lehmann and Joseph P. Romano. Testing statistical hypotheses. Springer Texts in Statistics.
Springer, third edition, 2005. ISBN 0-387-98864-5.
Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking generative
adversarial networks for diverse image synthesis. In IEEE Conference on Computer Vision and
Pattern Recognition, 2019.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784,
2014.
Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In International Con-
ference on Learning Representations, 2018. URL https://openreview.net/forum?id=
ByS1VpgRZ.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=B1QRgziT-.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with aux-
iliary classifier GANs. In Proceedings of the 34th International Conference on Machine Learn-
ing - Volume 70, ICML’17, pp. 2642-2651. JMLR.org, 2017. URL http://dl.acm.org/
citation.cfm?id=3305890.3305954.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
10
Under review as a conference paper at ICLR 2020
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with
spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Represen-
tations, 2016.
Suman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models.
arXiv preprint arXiv:1905.10887, 2019.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In Proceedings of the 33rd International Confer-
ence on International Conference on Machine Learning - Volume 48, ICML'16, pp. 1060-1069.
JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390.3045503.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
arXiv preprint arXiv:1908.10084, 2019.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
2234-2242. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6125-
improved-techniques-for-training-gans.pdf.
Amaia Salvador, Michal DrozdzaL Xavier Gir6 i Nieto, and Adriana Romero. Inverse cooking:
Recipe generation from food images. In Computer Vision and Patter Recognition (CVPR). IEEE
Computer Society, 2019.
Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating
dialogues. In AAAI, pp. 3295-3301. AAAI Press, 2017.
Shikhar Sharma, Dendi Suhubdy, Vincent Michalski, Samira Ebrahimi Kahou, and Yoshua Bengio.
Chatpainter: Improving text to image generation using dialogue. CoRR, abs/1802.08216, 2018.
Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. How good is my gan? CoRR,
abs/1807.09499, 2018. URL http://arxiv.org/abs/1807.09499.
Jake Snell, Karl Ridgeway, Renjie Liao, Brett D. Roads, Michael C. Mozer, and Richard S.
Zemel. Learning to generate images with perceptual similarity metrics. In 2017 IEEE Inter-
national Conference on Image Processing, ICIP 2017, Beijing, China, September 17-20, 2017,
pp. 4277-4281, 2017. doi: 10.1109/ICIP.2017.8297089. URL https://doi.org/10.1109/
ICIP.2017.8297089.
Sandeep Subramanian, Sai Rajeswar Mudumba, Alessandro Sordoni, Adam Trischler, Aaron C
Courville, and Chris Pal. Towards text generation with adversarially learned neural outlines.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 7551-7563. Curran Associates, Inc.,
2018. URL http://papers.nips.cc/paper/7983-towards-text-generation-
with-adversarially-learned-neural-outlines.pdf.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. In CVPR, pp. 2818-2826. IEEE Computer
Society, 2016.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. In International Conference on Learning Representations, 2016.
Radim Tylecek and Radim Sdra. Spatial pattern templates for recognition of objects with regular
structure. In Proc. GCPR, Saarbrucken, Germany, 2013.
11
Under review as a conference paper at ICLR 2020
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex
Graves. Conditional image generation with pixelcnn decoders. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems
29, pp. 4790-4798. Curran Associates, Inc., 2016a. URL http://papers.nips.cc/paper/
6527-conditional-image-generation-with-pixelcnn-decoders.pdf.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
In Proceedings of the 33rd International Conference on International Conference on Machine
Learning - Volume 48, ICML’16, pp. 1747-1756. JMLR.org, 2016b. URL http://dl.acm.org/
citation.cfm?id=3045390.3045575.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alexander
Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model
for raw audio. In Arxiv, 2016c. URL https://arxiv.org/abs/1609.03499.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4566-4575, 2015.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynam-
ics. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 29, pp. 613-621. Curran Associates, Inc., 2016.
URL http://papers.nips.cc/paper/6194-generating-videos-with-scene-
dynamics.pdf.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2018.
Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment:
From error visibility to structural similarity. IEEE TRANSACTIONS ON IMAGE PROCESSING,
13(4):600-612, 2004.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
Sitao Xiang and Hao Li. On the effects of batch and weight normalization in generative adversarial
networks. arXiv preprint arXiv:1704.03971, 2017.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1316-1324,
2018.
Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen Zhao, and Honglak Lee. Diversity-
sensitive conditional generative adversarial networks. arXiv preprint arXiv:1901.09024, 2019.
Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi Parikh. LR-GAN: layered recursive gen-
erative adversarial networks for image generation. In International Conference on Learning
Representations. OpenReview.net, 2017.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. In ICCV, 2017.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N.
Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks.
IEEE Transactions on Pattern Analysis and Machine Intelligence, July 2018a.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018b.
12
Under review as a conference paper at ICLR 2020
Zizhao Zhang, Yuanpu Xie, and Lin Yang. Photographic text-to-image synthesis with a hierarchically-
nested adversarial network. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6199-6208, 2018c.
Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In Computer
Vision and Pattern Recognition. IEEE Computer Society, 2019.
Sharon Zhou, Mitchell Gordon, Ranjay Krishna, Austin Narcomey, Durim Morina, and Michael S
Bernstein. Hype: Human eye perceptual evaluation of generative models. arXiv preprint
arXiv:1904.01121, 2019.
Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong
Yu. Activation maximization generative adversarial nets. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=HyyP33gAZ.
Jun-Yan Zhu, PhiliPP Krahenbuhl, Eli SheChtman, and Alexei A. Efros. Generative visual manipula-
tion on the natural image manifold. In Computer Vision - ECCV 2016 - 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V, pp. 597-613, 2016.
doi: 10.1007/978-3-319-46454-1\_36. URL https://doi.org/10.1007/978-3-319-
46454-1_36.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using CyCle-Consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International
Conference on, 2017a.
Jun-Yan Zhu, RiChard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli
SheChtman. Toward multimodal image-to-image translation. In Advances in Neural Information
Processing Systems, 2017b.
13
Under review as a conference paper at ICLR 2020
A Illustration of FID and FJD on two dimensional Gaussian data
In this section, we illustrate the claim made in Section 1 that FID cannot capture intra-conditioning
diversity when the joint distribution of two variables changes but the marginal distribution of one of
them is not altered.
Consider two multivariate GaUssian distributions, (X1, Y1)〜N(0, Σ1) and (X2, Y2)〜N(0, ∑2),
where
Σ1
42
22
Σ2
2.1 2
22
Figure 5 (left) shows 10, 000 samples drawn from each of these distributions, labeled as Dist1 and
Dist2, respectively. While the joint distributions of fX1,Y1 (X1, Y1) and fX2,Y2 (X2, Y2) are different
from each other, the marginal distributions fγ1 (Y1) and fY2 (Y2) are the same (Y1 〜N(0, 2) and
Y2 〜N(0, 2)). Figure 5 (center) shows the histograms of the two marginal distributions computed
from 10, 000 samples.
If we let Xi take the role of the embedding of the conditioning variables (e.g., position) and Yi
take the role of the embedding of the generated variables (i.e., images), then computing FID in this
example would correspond to computing the FD between fY1 and fY2, which is zero. On the other
hand, computing FJD would correspond to the FD between fX1,Y1 and fX2,Y2, which equals 0.678.
But note that Dist1 and Dist2 have different degrees of intra-conditioning diversity, as illustrated by
Figure 5 (right), where two histograms of fγi IXi∈(0.9,1.1) are displayed, showing marked differences
to each other (similar plots can be constructed for other values of Xi). Therefore, this example
illustrates a situation in which FID is unable to capture changes in intra-conditioning diversity, while
FJD is able to do so.
Figure 5: Left: samples from two multivariate Gaussian distributions. Center: Histograms of marginal
distributions for the Y variable. Right: Histogram of conditional distributions for Y conditioned on
X ∈ (0.9, 1.1).
B Experimental Settings for Calculating FJD
Important details pertaining to the computation of the FID and FJD metrics for different experiments
included in this paper are reported in Table 5. For each dataset we report which conditioning modality
was used, as well as the conditioning embedding function. Information about which split and image
resolution are used for the reference and generated distributions is also included, as well as how many
samples were generated per conditioning. Values for α reported here are calculated according to the
balancing mechanism recommended in Section 4.2. Datasets splits marked by “-” indicate that the
distribution is a randomly sampled subset of the full dataset.
C Image Quality Evaluation on COCO-Stuff Dataset
We repeat the experiment initially conducted in Section 5.2 on a real world dataset to see how well
FJD tracks image quality. Specifically, we use the COCO-Stuff dataset (Caesar et al., 2018), which
provides class labels, bounding box annotations, and segmentation masks. We follow the same
experimental procedure as outlined in Section 5.2: Gaussian noise is drawn from N(0, σ) and add to
the images, where σ ∈ [0, 0.25] and pixel values are normalized (and clipped after noise addition) to
14
UnderreVieW as a ConferenCe PaPersICLR 2020
Table 5: Settings used to calculate FJD in experiments.
Dataset	Modality	Reference distribution		Generated distribution			Embedding	a
		Split	Img. res.	Split	Samp, per cond.	Img. res.		
dSprite	Class	-	64	-	1		One-hot	17.465029
dSprite	BBox	-	64	-	1		AutoEncoder	0.54181314
dSprite	Mask	-	64	-	1		AutoEncoder	0.38108996
ImageNet	Class	Train	128	Valid.	1		One-hot	17.810543
Facades	Image	Train	256	Valid. + Test	1		InceptionV3	0.9376451
Maps	Image	Train	512	Test	1		InceptionV3	1.1100407
Edges2Shoes	Image	Train	256	Test	1		InceptionV3	0.73255646
EdgeslHandbags	Image	Train	256	Test	1		InceptionV3	0.7743437
CUB-200	Text	Valid.	256	Valid.	1		Char-CNN-RNN	4.2674055
COCO-Stuff	Class	Valid.	64	Valid.	1		One-hot	8.539345
COCO-Stuff	BBox	Valid.	64	Valid.	1		AutoEncoder	0.00351441
COCO-Stuff	Mask	Valid.	64	Valid.	1		AutoEncoder	0.001909862
COCO-Stuff	Class	Valid.	128	Valid.	1		One-hot	0.001909862
COCO-Stuff	BBox	Valid.	128	Valid.	1		AutoEncoder	0.000039950188
COCO-Stuff	Mask	Valid.	128	Valid.	1		AutoEncoder	0.0150718
LA
Under review as a conference paper at ICLR 2020
the range [0, 1]. The original dataset of clean images is used as the reference distribution, while noisy
images are used to simulate a generated distribution with poor image quality. For the purposes of
calculating FJD, we use N-hot encoding to embed the labels of the classes present in each image,
and autoencoder representations for the bounding box and mask labels. As shown in Figure 6, FID
and FJD both track image quality well, increasing as more noise is added to the generated image
distribution.
Figure 6: Comparison between FID and FJD for class, bounding box, and mask conditioning under
varying noise levels for COCO-Stuff dataset. Evaluated at 128x128 resolution.
D Conditional Consistency Evaluation with Text Conditioning
In order to test the effectiveness of FJD at detecting conditional inconsistencies in the text domain,
we use the Caltech-UCSD Birds 200 dataset (Welinder et al., 2010). This dataset is a common
benchmark for text conditioned image generation models, containing 200 fine-grained bird categories,
11,788 images, and 10 descriptive captions per images. Also included in the dataset are vectors
of detailed binary annotations describing the attributes of the bird in each image. Each annotation
indicates the presence or absence of specific features, such as has\_bill\_shape::curved or
has\_wing\_color::blue.
Our goal in this experiment is to swap captions between images, and in this fashion introduce
inconsistencies between images and their paired captions, while preserving the marginal distributions
of images and labels. We compare attribute vectors belonging to each image using the Hamming
distance to get an indication for how well the captions belonging to one image might describe another.
Small Hamming distances indicate a good match between image and caption, while at larger values the
captions appear to describe a very different bird than what is pictured (as demonstrated in Figure 7).
Caption
Hamming
Distance
This bird is fully covered in red except for some parts of Wing 0
and it has brown feet.
A red bird with a short bill with a black cheek patch.	13
This small bird is bright red with black wings, black eyeing, and 2$
short black beak.
The small brown bird has a yellow beak and black round eyes. 39
The body of the bird is ivory and the crown is bright red while $1
the wing is black and ivory speckled.
Figure 7: A summer tanager, as described by a variety of captions (ground truth caption highlighted
in blue). The Hamming distance between attribute vectors associated with each caption and the
ground truth caption provides an indication of how well each caption describes the image.
To test FJD we create two datsets: one which contains the original image-captions pairs from CUB-
200 to act as the reference distribution, and another in which captions have been swapped to act
as a generated distribution that has poor conditional consistency. Char-CNN-RNN embeddings are
16
Under review as a conference paper at ICLR 2020
used to encode the captions for the purposes of calculating FJD. In Figure 8 we observe that as the
average Hamming distance across captions increases (i.e., the captions become worse at describing
their associated images), FJD also increases. FID, which is unable to detect these inconsistencies,
remains constant throughout.
Figure 8: Change in FJD and FID with respect to the average Hamming distance between original
captions and swapped captions. FJD increases as captions become worse at describing their associated
image, while FID is insensitive.
E List of Sources of Pre-trained Model
Table 6 includes the hyperlinks to all of the pretrained conditional generation models used in our
experiments in Section 6.
Table 6: Source of pre-trained models evaluated in Section 6.
Model	Source
SN-GAN	https://github.com/pfnet-research/sngan_projection
BigGAN	https://github.com/ajbrock/BigGAN-PyTorch
Pix2pix	https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
BicyleGAN	https://github.com/junyanz/BicycleGAN
MSGAN	https://github.com/HelenMao/MSGAN/
MUNIT	https://github.com/nvlabs/MUNIT
HDGan	https://github.com/ypxie/HDGan
StackGAN++	https://github.com/hanzhanggit/StackGAN-v2
AttnGAN	https://github.com/taoxugit/AttnGAN
F	EFFECT OF α PARAMETER
The α parameter in the FJD equation acts as a weighting factor indicating the importance of the image
component versus the conditional component. When α = 0, then FJD is equal to FID, since we
only care about the image component. As the value of α increases, the magnitude of the conditional
component’s contribution to the value of FJD increases as well. In our experiments, we attempt to
find a neutral value for α that will balance the contribution from the conditional component and
the image component. This balancing is done by finding the value of α that would result in equal
magnitude between the image and conditioning embeddings (as measured by the average L2 norm of
the embedding vectors).
Instead of reporting FJD at a single α, an alternative approach is to calculate and plot FJD for a range
of α values, as shown in Figure 9. Plotting α versus FJD allows us to observe any change in rank
of models as the importance weighting on the conditional component is increased. Here we use the
truncation trick to evaluate BigGAN (Brock et al., 2019) at several different truncation values σ. The
17
Under review as a conference paper at ICLR 2020
Figure 9: Alpha sweep for BigGAN at various truncation values σ . FID is equivalent to FJD at
α = 0. The black dashed line indicates the α value that is selected by calculating the ratio between
the average L2 norm of image and conditioning embeddings.
truncation trick is a technique wherein the noise vector used to condition a GAN is scaled by σ in
order to trade sample diversity for image quality and conditional consistency, without needing to
retrain the model (as shown in Table 7).
Table 7: Comparison of BigGAN model evaluated with different truncation values σ. FJD is
calculated at α = 17.8. As σ increases, classification accuracy decreases and diversity increases.
Note that FID and FJD are consistent in their choice of the preferred model at σ = 1.0, however, the
relative ranking of σ = 0.25 and σ = 2.0 is different between the two metrics.
σ	FJD1	FID1	Acc. ↑	Diversity ↑
0.25	50.5	22.6	81.7	0.247
0.5	33.8	16.5	80.8	0.380
1.0	17.1	9.6	67.5	0.550
1.5	26.2	16.3	45.9	0.644
1.75	37.3	23.9	35.8	0.674
2.0	49.1	31.9	27.0	0.696
We find that in several cases, the ranking of models changes when comparing them at α = 0
(equivalent to FID), versus comparing them using FJD at higher α values. Models with low truncation
values σ initially achieve good performance when α is also low. However, as α increases, these
models rapidly drop in rank due to lack of sample diversity, and instead models with higher σ values
are favoured. This is most obvious when comparing σ = 0.25 and σ = 1.75 (blue and yellow lines
in Figure 9) respectively.
G	Autoencoder Architecture
To create embeddings for the bounding box and mask conditionings evaluated in this paper we
utilize a variant of the Regularized AutoEncoder with Spectral Normalization (RAE-SN) introduced
by Ghosh et al. (2019) and enhance it with residual connections (Tables 8 and 9). For better
reconstruction quality, we substitute the strided convolution and transposed convolution for average
pooling and nearest neighbour upsampling, respectively. Spectral normalization (Miyato et al., 2018)
is applied to all linear and convolution layers in the decoder, and an L2 penalty is applied to the latent
representation z during training. Hyperparameters such as the weighting factor on the L2 penalty and
the number of dimensions in the latent space are selected based on which combination produces the
best reconstructions on a held-out validation set.
In Tables 10 and 11 we depict the architecture for an autoencoder with 64 × 64 input resolution, but
this can be scaled up or down by adding or removing residual blocks as required. ch represents a
channel multiplier which is used to control the capacity of the model. M represents the number of
18
Under review as a conference paper at ICLR 2020
latent dimensions in the latent representation. C indicates the number of classes in the bounding box
or mask representation.
Table 8: ResBlock down	Table 9: ResBlock up
Input x	Input x
x → Conv3 × 3 → BN → ReLU → out	x → Conv3 × 3 → BN → ReLU → out
out → Conv3 × 3 → BN → ReLU → out	out → Conv3 × 3 → BN → out
out → AvgPool2 × 2 → out	out → Upsample2 × 2 → out
x → Conv1 × 1 → res	x → Conv1 × 1 → res
res → AvgPool2 × 2 → res	res → Upsample2 × 2 → res
out + res → ReLU → out	out + res → ReLU → out
Table 11: Decoder
Table 10: Encoder
Input X ∈ r64×64×c
ResBlock down C → ch
ResBlock down ch → 2ch
ResBlock down 2ch → 4ch
ResBlock down 4ch → 8ch
Linear 8ch × 4 × 4 → M
Z ∈ RM
Linear M → ch × 8 × 8
BN → ReLU
ResBlock up 8ch → 4ch
ResBlock up 4ch → 2ch
ResBlock up 2ch → ch
Conv ch → C
Tanh
H FJD for Model Selection and Hyperparameter Tuning
In order to demonstrate the utility of FJD for the purposes of model selection and hyperparameter
tuning, we consider the loss function of the generator from an auxiliary classifier GAN (ACGAN)
(Odena et al., 2017), as shown in Equation 7 to 9. Here S indicates the data source, and C indicates
the class label.
LS =E[logP(S=real|Xreal]+E[logP(S= fake|Xfake)]	(7)
LC =E[logP(C=c|Xreal)] +E[logP(C=c|Xfake)]	(8)
LG = λLC - LS	(9)
The generator loss LG is maximized during training, and consists of two components: an adversarial
component LS, which encourages generated samples to look like real samples, and a classification
component LC, which encourages samples to look more like their target class. In this experiment
we add a weighting parameter λ, which weights the importance of the conditional component of the
generator loss. The original formulation of ACGAN is equivalent to always setting λ = 1, but it is
unknown whether this is the most suitable setting as it is never formally tested. To this end, we train
models on the MNIST dataset and perform a sweep over the λ parameter in the range [0, 5], training
a single model for each λ value tested. Each model is evaluated using FID, FJD, and classification
accuracy to indicate conditional consistency. For FID and FJD we use the training set as the reference
distribution, and generate 50, 000 samples for the generated distribution. Classification accuracy is
measured using a pretrained LeNet classifier (LeCun et al., 1998), where the conditioning label is
used as the groundtruth.
19
Under review as a conference paper at ICLR 2020
Scores from best performing models as indicated by FID, FJD, and classification accuracy are shown
in Table 12. Sample sheets are provided in Figure 10, where each column is conditioned on a different
digit from 0 to 9. We find that FID is optimized when λ = 0.25 (Figure 10a). This produces a model
with good image quality, but almost no conditional consistency. Accuracy is optimized when λ = 5.0
(Figure 10c), yielding a model with good conditional consistency, but limited image quality. Finally,
FJD is optimized when λ = 1.0 (Figure 10b), producing a model that demonstrates a balance between
image quality and conditional consistency. These results demonstrate the importance of considering
both image quality and conditional consistency simultaneously when performing hyperparameter
tuning.
Table 12: Scores of ACGAN models trained with different values for conditioning weighting λ.
λ	FID J	FJD J	Accuracy ↑
0.25	56.87	79.65	12.22
1.0	62.39	65.31	74.90
5.0	115.23	119.10	98.01
7ηr~7 7 777*7η
646g666g4c
≤∙3 S 5 3 6 一∖ 4 5 5
3/夕 V ʧl ¥ H
3^33 3373333
，二2222H722
f- I. J/r // ■■. // Il Hf H √∕/ ■ I.
Oo〃O夕OOoo少
q夕〜〜夕9∖Γ"gq3
JggSgooyg M g
77777 ^7^/77
CJGb6 0G∖QaG
5,55<g⅛5≤≤s
/√</' ^vz
3 35J333333
2732aAaz22
H // H /- 1/ / 1« / 11 //
OoooOOCOOO
G g q∕÷54∕/ B
vzH> 夕。SS 9 ɪ
& -ɔ 737√⅛∕vλ y∙s
IL/σrg"白
93,3^J57go
f∙×g 夕 F4¼Γ∖∕0
3 Uz 7 0q ^~ΓO 3/
7<7q3H6 5KQ 7
5 ⅛-q∕3 牛 5 / 7
夕5 ? 6g∕q775
(a)λ = 0.25	(b)λ = 1.0	(c)λ = 5.0
Figure 10: Sample sheets for ACGAN model trained with different conditioning weighting λ. Each
column is conditioned on a different digit, from 0 to 9. Low values of λ produce models with very
little conditional consistency, while overly large values of λ yield models with reduced image quality
and diversity.
I	Training and Evaluating with Multi-label, B ounding Box, and
Mask Conditioning on COCO-Stuff
To demonstrate FJD applied to multi-label, bounding box, and mask conditioning on a real world
dataset, we train a GAN on the COCO-Stuff dataset (Caesar et al., 2018). To this end, we train three
generative models, one for each conditioning type. Following (Johnson et al., 2018), we select only
images containing between 3 and 8 objects, and also ignore any objects that occupy less than 2%
of the total image area. Two image resolutions are considered: 64 × 64 and 128 × 128. We adopt a
BigGAN-style model (Brock et al., 2019), but modify the design such that a single fixed architecture
can be trained with any of the three conditioning types. See Section I.1 for architectural details. We
train each model 5 times, with different random seeds, and report mean and standard deviation of
both FID and FJD in Table 13. N-hot encoding is used as the embedding function for the multi-label
conditioning, while autoencoder representations are used to calculate FJD for bounding box and mask
conditioning.
In most cases we find that FID values are very close between conditioning types. A similar trend is
observed in FJD at the 128 × 128 resolution. For models trained at 64 × 64 resolution however, we
notice a more drastic change in FJD between conditioning types. Mask conditioning achieves the
lowest FJD score, followed by multi-label conditioning and bounding box conditioning. This could
indicate that the mask conditioning models are more conditionally consistent (or diverse) compared
to other conditioning types.
20
Under review as a conference paper at ICLR 2020
Table 13: FJD / FID results averaged over 5 runs on COCO-stuff validation set with multi-label,
bounding box (bbox) and mask conditionings for image resolutions 64 × 64 and 128 × 128.
	class conditioning		bbox conditioning		mask conditioning	
	FJD ；	FID ；	FJD ；	FID ；	FJD ；	FID ；
64	57.35 ± 1.60	40.75 ± 1.38	67.97 ± 1.70	41.81 ± 1.50	49.44 ± 2.46	41.27 ± 2.36
128	68.49 ± 2.72	50.74 ± 2.31	71.58 ± 1.77	51.78 ± 1.55	68.12 ± 1.33	46.02 ± 1.22
I.1	COCO-Stuff GAN Architecture
In order to modify BigGAN Brock et al. (2019) to work with multiple types of conditioning we
make two major changes. The first change occurs in the generator, where we replace the conditional
batch normalization layers with SPADE (Park et al., 2019). This substitution allows the generator
to receive spatial conditioning such as bounding boxes or masks. In the case of class conditioning
with a spatially tiled class vector, SPADE behaves similarly to conditional batch normalization. The
second change we make is to the discriminator. The original BigGAN implementation utilizes a
single projection layer (Miyato & Koyama, 2018) in order to provide class-conditional information
to the discriminator. To extend this functionality to bounding box and mask conditioning, we add
additional projection layers after each ResBlock in the discriminator. The input to each projection
layer is a downsampled version of the conditioning that has been resized using nearest neighbour
interpolation to match the spatial resolution of each layer. In this way we provide conditioning
information at a range of resolutions, allowing the discriminator to use whichever is most useful for
the type of conditioning it has received. Aside from these specified changes, and using smaller batch
sizes, models are trained with the same hyperparameters and training scheme as specified in (Brock
et al., 2019).
I.2	Samples of Generated Images
In this section, we present some random 128 × 128 samples of conditional generation for the models
covered in Section I. In particular, Figures 11-13 show class, bounding box, and mask conditioning
samples, respectively. Each row displays a depiction of conditioning, followed by 4 different samples,
and finally the real image corresponding to the conditioning. As shown in Figure 11, conditioning
on classes leads to variable samples w.r.t. object positions, scales and textures. As we increase the
conditioning strength, we reduce the freedom of the generation and hence, in Figure 12, we observe
how the variability starts appearing in more subtle regions. Similarly, in Figure 13, taking different
samples per conditioning only changes the textures. Although the degrees of variability decrease as
the conditioning strength increases, we obtain sharper, better looking images.
21
Under review as a conference paper at ICLR 2020
22
Under review as a conference paper at ICLR 2020
23