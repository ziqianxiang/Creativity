Under review as a conference paper at ICLR 2019
Antifragile	and	Robust	Heteroscedastic
Bayesian Optimisation
Anonymous authors
Paper under double-blind review
Ab stract
Bayesian Optimisation is an important decision-making tool for high-stakes ap-
plications in drug discovery and materials design. An oft-overlooked modelling
consideration however is the representation of input-dependent or heteroscedas-
tic aleatoric uncertainty. The cost of misrepresenting this uncertainty as being
homoscedastic could be high in drug discovery applications where neglecting
heteroscedasticity in high throughput virtual screening could lead to a failed drug
discovery program. In this paper, we propose a heteroscedastic Bayesian Optimisa-
tion scheme which both represents and optimises aleatoric noise in the suggestions.
We consider cases such as drug discovery where we would like to minimise or be
robust to aleatoric uncertainty but also applications such as materials discovery
where it may be beneficial to maximise or be antifragile to aleatoric uncertainty.
Our scheme features a heteroscedastic Gaussian Process (GP) as the surrogate
model in conjunction with two acquisition heuristics. First, we extend the aug-
mented expected improvement (AEI) heuristic to the heteroscedastic setting and
second, we introduce a new acquisition function, aleatoric-penalised expected im-
provement (ANPEI) based on a simple scalarisation of the performance and noise
objective. Both methods are capable of penalising or promoting aleatoric noise in
the suggestions and yield improved performance relative to a naive implementation
of homoscedastic Bayesian Optimisation on toy problems as well as a real-world
optimisation problem.
1	Introduction
Bayesian Optimisation is already being utilised to make decisions in high-stakes applications such
as drug discovery [1, 2, 3, 4, 5], materials discovery [6, 7, 8], robotics [9], sensor placement [10]
and tissue engineering [11]. In these problems heteroscedastic or input-dependent noise is rarely
accounted for and the assumption of homoscedastic noise is often inappropriate. As a case study,
heteroscedastic noise is the rule rather than the exception in the majority of scientific datasets. This
is the case, not only in experimental datasets, but also in datasets where properties are predicted
computationally. We illustrate this for molecular hydration free energies in Figure 1 using the
dataset of [12] where there is a distribution of noise values and in general the noise function might
be expected to grow in proportion to chemical complexity [13]. The consequences of neglecting
heteroscedastic noise are illustrated using a second example in Figure 2. The homoscedastic model
will underestimate noise in certain regions which could induce a Bayesian Optimisation scheme to
suggest values possessing large aleatoric noise. In an application such as high-throughput virtual
screening the cost of misrepresenting the noise during the screening process could amount to a year
wasted in the physical synthesis of a drug [14].
In materials discovery, we may derive benefit from or be antifragile [15] towards high aleatoric
uncertainty. In an application such as the search for high-performing perovskite solar cells, we are
faced with an extremely large compositional space, with millions of potential candidates possessing
high aleatoric noise for identical reproductions[16]. In this instance we may want to guide search
towards a candidate possessing a high photoluminescence quantum efficiency with high aleatoric
noise. If the cost of repeating material syntheses is small relative to the cost of the search, the large
aleatoric noise will present opportunities to synthesise materials possessing efficiencies far above
their mean values.
1
Under review as a conference paper at ICLR 2019
In this paper we present a heteroscedastic Bayesian Optimisation scheme capable of both representing
and optimising aleatoric noise in the suggestions. Our contributions are:
1.	The introduction of a novel combination of surrogate model and acquisition function
designed to optimise heteroscedastic aleatoric uncertainty to be used in situations where one
wants to minimise (be robust to) aleatoric uncertainty as well as situations where one wants
to maximise (be antifragile towards) aleatoric uncertainty.
2.	A demonstration of our scheme’s ability to outperform naive schemes based on homoscedas-
tic Bayesian Optimisation on toy problems in addition to a real-world optimisation problem.
3.	The provision of an open-source implementation of the model.

AUUənbə-
O ____________________ . _ ________ ___________ ,
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10
Error magnitude (kcal/mol)
(a)	Histogram of calculated uncertainties
o ...................... 一	.	. 一一
0.00 0.25 0.50 0.75 1.00	1.25	1.50 1.75 2.00
Error magnitude (kcal/mol)
(b)	Histogram of experimental uncertainties
Figure 1: Fig. (a) shows the frequency histogram of the calculated uncertainties against the error
magnitude (kcal/mol) for the FreeSolv hydration energy dataset ([12]). Fig. (b) shows a similar plot
for the experimental uncertainties.
Figure 2: Comparison of Homoscedastic and Heteroscedastic GP Fits to the Soil Phosphorus Fraction
Dataset.
2	Related Work
The most similar work to our own is that of [17] where experiments are reported on a heteroscedastic
Branin-Hoo function using the variational heteroscedastic GP approach of [18] although to the best
of our knowledge this work does not consider sequential evaluations. A modification to EI, expected
risk improvement is introduced in [19] and is applied to problems in robotics where robustness to
aleatoric noise is desirable. [20, 21] implement heteroscedastic Bayesian Optimisation but don’t
introduce an acquisition function that penalises aleatoric uncertainty. [22, 23] consider the related
problem of safe Bayesian Optimisation through implementing constraints in parameter space. In this
instance, the goal of the algorithm is to enforce a performance threshold for each evaluation of the
black-box function and so is unrelated to our problem definition. In terms of acquisition functions,
2
Under review as a conference paper at ICLR 2019
[24, 25] propose principled approaches to handling aleatoric noise in the homoscedastic setting that
could be extended to the heteroscedastic setting. Our primary focus in this work however, is to
highlight that heteroscedasticity in the surrogate model is beneficial and so an examination of a subset
of acquisition functions is sufficient for this purpose.
3	Background
Bayesian Optimisation features a surrogate model for the black-box objective. The surrogate model
should maintain calibrated uncertainty estimates in order to guide the acquisition of new data points.
GPs are often a popular choice of surrogate model in BO applications because of their ability to
represent uncertainty.
3.1	Gaussian Processes
In the terminology of stochastic processes we may formally define a GP as follows:
Definition 1. A Gaussian Process is a collection of random variables, any finite number of which
have a joint Gaussian distribution.
The random variables consist of function values f(x) at different locations x within the design space.
The GP is characterised by a mean function
m(x) = E[f (x)]	(1)
and a covariance function
k(x, x0) = E[(f (x - m(x))(f (x0) - m(x0))].	(2)
The process is written as follows
f(x) ~GP(m(x),k(x, x0)).	⑶
In our experiments, the prior mean function will be set to the data mean after normalisation of the
inputs and standardisation of the outputs. As such, m(x) = 0 will be assumed henceforth. The
covariance function computes the pairwise covariance between two random variables (function
values). The covariance between a pair of output values f(x) and f(x0) is a function of an input
pair x and x0. As such, the kernel encodes smoothness assumptions about the latent function being
modelled; similarity in input space yields outputs that are close [26]. The most widely-utilised kernel
is the squared exponential (SE) kernel
kSQE(x, XO)= σf ∙ eχp ( -kx2-2x k )	⑷
where σf2 is the signal amplitude hyperparameter (vertical lengthscale) and ` is the (horizontal)
lengthscale hyperparameter. We use the squared eχponential kernel in all eχperiments. For further
information on Gaussian Processes the reader is referred to [27].
3.2	Bayesian Optimisation
Problem Statement The global optimisation problem is defined as
x* = arg min f (x)	(5)
x∈X
where x* is the global optimiser of a black-boχ function f : X → Y . X is the design space and
is typically a compact subset of Rd. What makes this optimisation problem practically relevant in
applications are the following properties:
3
Under review as a conference paper at ICLR 2019
1.	Black-box Objective: We do not have the analytic form of f . We can however evaluate f
pointwise anywhere in the design space X.
2.	Expensive Evaluations: Choosing an input location x and evaluating f (x) takes a very long
time.
3.	Noise: The evaluation of a given x is a noisy process. In addition, this noise may vary across
X , making the underlying process heteroscedastic.
We have a dataset of past observations D = {(xi, ti)}in=1 consisting of observations of the black-box
function f and fit a surrogate model to these datapoints. We then leverage the predictive mean as
well as the uncertainty estimates of the surrogate model to guide the acquisition of the next data point
xn+1 according to a heuristic known as an acquisition function.
4	Heteroscedastic Bayesian Optimisation
We wish to perform Bayesian Optimisation whilst optimising input-dependent aleatoric noise. In
order to represent input-dependent aleatoric noise, a heteroscedastic surrogate model is required. We
take the most likely heteroscedastic GP approach of [28], adopting the notation presented there for
consistency. We have a dataset D = {(xi, ti)}in=1 in which the target values ti have been generated
according to t = f (Xi) + e〃 We assume independent Gaussian noise terms ei 〜N(0, σi) with
variances given by σi = r(xi). In the heteroscedastic setting r is typically a non-constant function
over the input domain X. In order to perform Bayesian Optimisation, we wish to model the predictive
distribution P(t* | x；,..., x*) at the query points x；,..., x*. Placing a GP prior on f and taking
r(x) as the assumed noise rate function, the predictive distribution is multivariate Gaussian N(μ;, Σ*)
with mean
μ* = E[t*] = K *(K + R)Tt	(6)
and covariance matrix
Σ* = var[t*] = K** + R* - K*(K + R)TK*t,	(7)
where K ∈ Rn×n,	Kij = k(xi, xj),	K； ∈ Rq×n, Ki；j	=	k(xi；, xj), K；； ∈ Rq×q,
Ki；j； = k(xi；,xj；), t	= (t1, t2, . . . , tn)T,	R	= diag(r) with r	=	(r(x1), r(x2), . . . , r(xn))T,
and R； = diag(r；) with r； = (r(x；1), r(x；2), . . . , r(xq；))T.
The most likely heteroscedastic GP algorithm [28] executes the following steps:
1.	Estimate a homoscedastic GP, G1 on the dataset D = {(xi, ti)}in=1
2.	Given G1 , we estimate the empirical noise levels for the training data using
var[ti, G1(xi, D)] = 0.5 (ti - E[x])2 forming a new dataset D0 = {(xi, zi)}in=1. A note on
the form of this variance estimator is give in Appendix B.
3.	Estimate a second GP, G2 on D0 .
4.	Estimate a combined GP, G3 on D using G2 to predict the logarithmic noise levels ri .
5.	If not converged, set G3 to G1 and repeat.
The Bayesian Optimisation problem may be framed as
x； = arg min f(x),	(8)
x∈χ
where the black-box objective f, to be minimised has the form
f(x) = g(x) + s(x).	(9)
4
Under review as a conference paper at ICLR 2019
s(x) is, in this instance, the true noise rate function. We investigate extensions of the expected
improvement [29] acquisition criterion, the principal form of which may be written in terms of the
targets t and the incumbent best objective function value, η, found so far as
∞
EI(x) = E(η - t)+ =	(η - t)+ p(t | x) dt
-∞
(10)
where p(t | x) is the posterior predictive marginal density of the objective function evaluated at x.
(η - t)+ ≡ max (0, η - t) is the improvement over the incumbent best objective function value η .
Evaluations of the objective are noisy in all of the problems we consider and so we use expected
improvement with plug-in [30], the plug-in value being the GP predictive mean [31].
We propose two extensions to the expected improvement criterion. The first is an extension of the
augmented expected improvement criterion
AEI(X) = E[(η — t)+] I 1------n-----
∖	Pvar[t*Γ+σ2
(11)
of [32] where σn is the fixed aleatoric noise level. We extend AEI to the heteroscedastic setting by
exchanging the fixed aleaotric noise level with the input-dependent one:
het-AEI(x) = E[(η -1)+] 1----------r^>----------
pvar[t*] + r(x)
(12)
where r(_) is the predicted aleatoric uncertainty at input _ under the most likely heteroscedastic
GP and var[t* ] is the predictive variance of the heteroScedastic GP incorporating both aleatoric
and epistemic components of the uncertainty. We also propose a simple modification to the ex-
pected improvement acquisition function that explicitly penalises regions of the input space with
large aleatoric uncertainty. We call this acquisition function aleatoric noise-penalised expected
improvement (ANPEI) and denote it
ANPEI = αEI(x) - (1 - α) √T(X)
(13)
where α is a scalarisation constant which we set to 0.5 for the experiments in this paper. In
the multiobjective optimisation setting a particular value of α will correspond to a point on the
Pareto frontier. We use both the modification to AEI (het-AEI) and ANPEI acquisition function in
conjunction with the most likely heteroscedastic GP surrogate model in the experiments that follow.
For the experiments on antifragility to aleatoric uncertainty we propose the antifragile augmented
expected improvement (AAEI) acquisition function
AAEI(_) =E (η-t)+	1 -
ʌ/var[t*] + r(x)
(14)
which has the effect of promoting solutions with high aleatoric noise.
5	Experiments on Robustness to Aleatoric Uncertainty
5.1	Implementation
Experiments were run using a custom implementation of Gaussian Process regression and most likely
heteroscedastic Gaussian Process regression. The code is available at https://anonymous.
5
Under review as a conference paper at ICLR 2019
(a) 1D Toy Objective
(b) Noise Rate Function
(c) Black-box Objective
Figure 3: Toy 1D Problem. The toy objective in a) is corrupted with heteroscedastic noise according
to the function in b). The combined objective, which when optimised maximises the sin wave subject
to the minimisation of aleatoric noise, is given in c) and is obtained by subtracting the noise function
from the 1D sinusoid.
4open.science/r/3361287c-6879-4b38-9153-5c9491271200/. The squared expo-
nential kernel was chosen as the covariance function for both the homoscedastic GP as well as
G1 and G2 of the most likely heteroscedastic GP. The lengthscales, `, of the homoscedastic GP
were initialised to 1.0 for each input dimension across all toy problems after standardisation of the
output values following the recommendation of [33]. The signal amplitude σf2 was initialised to a
value of 2.5. The lengthscale, `, of G2 of the most likely heteroscedastic GP [28] was initialised to
1.0, the initial noise level of G2 was set to 1.0. The EM-like procedure required to train the most
likely heteroscedastic GP was run for 10 iterations and the sample size required to construct the
variance estimator producing the auxiliary dataset was 100. Hyperparameter values were obtained by
optimising the marginal likelihood using the scipy implementation of the L-BFGS optimiser. The
objective function in all cases is the principal objective g(x) minus one standard deviation of the
ground truth noise function s(x).
5.2	1D Toy Objective with Linear Noise Rate Function
Referring to Equation 9 from section 4, in the first experiment we take a one-dimensional sin wave
g(x) = sin(x) + 0.2(x)	(15)
with noise rate function s(x) = 0.25x. These functions as well as the black-box objective f(x) are
shown in Figure 3. The Bayesian Optimisation problem is designed such that the first maximum in
3(a) is to be preferred as samples from this region of the input space will have a smaller noise rate. The
black-box objective in 3(c) illustrates this trade-off. In 6(a) we compare the performance of a Bayesian
Optimisation scheme involving a vanilla GP in conjunction with the EI acquisition function with the
most likely heteroscedastic GP in conjunction with the ANPEI and het-AEI acquisition functions.
The experiment is designed to contrast the performance of a standard Bayesian Optimisation scheme
against our approach in a situation where minimising aleatoric noise is desirable.
5.3	Branin-Hoo with Non-linear Noise Rate Function
In the second experiment we consider the Branin-Hoo function as g(x) with a non-linear noise
rate function given by s(x) = 1.4x12 + 0.3x2. Given that this example is a minimisation problem,
the black-box objective consists of the sum of the Branin-Hoo function and the noise rate function.
Contour plots of the functions are shown in Figure 4. A comparison, in terms of the best objective
function values found, between the vanilla GP and EI acquisition function with the most likely
heteroscedastic GP and ANPEI and AEI acquisition functions is given in 6(b).
5.4	Optimising the Phosphorus Fraction of Soil
In this real-world problem we apply heteroscedastic Bayesian optimisation to the problem of optimis-
ing the phosphorus fraction of soil. Soil phosphorus is an essential nutrient for plant growth and is
6
Under review as a conference paper at ICLR 2019
(a) 2D Branin-Hoo Function
(b) Non-linear Noise Function
(c) Black-box Objective
Figure 4: Toy 2D Problem. The Branin-Hoo objective function in a) is corrupted by the heteroscedastic
noise function in b) s(x1, x2) = 1.4x12 + 0.3x2. The black-box objective function c) is obtained by
summing the functions in a) and b). The sum is required to penalise regions of large aleatoric noise
because the objective is being minimised.
widely used as a fertilizer in agriculture. While the amount of arable land worldwide is declining,
global population is expanding and so is food demand. As such, understanding the availability
of plant nutrients that increase crop yield is essential. To this end, [34] have curated a dataset on
soil phosphorus, relating phosphorus content to variables including soil particle size, total nitrogen,
organic carbon and bulk density. In this experiment, we study the relationship between bulk soil
density and the phosphorus fraction, the goal being to minimise the phosphorus content of soil subject
to heteroscedastic noise. We provide evidence that there is heteroscedasticity in the problem by
comparing the fits of a homoscedastic GP and the most likely heteroscedastic GP in Figure 2 and
provide a predictive performance comparison based on negative log predictive density values in the
appendix. In this problem, we do not have access to a continuous-valued black-box function or a
ground truth noise function. As such, the surrogate models were initialised with a subset of the data
and the query locations selected by Bayesian Optimisation were mapped to the closest data points in
the heldout data. The following kernel smoothing procedure was used to generate pseudo ground
truth noise values:
1.	Fit a homoscedastic GP to the full dataset.
2.	At each point Xi, compute the corresponding square error s2 = (y, - μ(xi))2.
3.	Estimate variances by computing a moving average of the squared errors, where the relative
weight of each si2 was assigned with a Gaussian kernel.
The performances of homoscedastic Bayesian Optimisation using EI and AEI and heteroscedastic
Bayesian Optimisation using ANPEI and AEI are compared in 5(c).
5.5	Discussion
In all robustness experiments, the most likely heteroscedastic GP and ANPEI combina-
tion/heteroscedastic GP and het-AEI combination outperform the homoscedastic GP and EI. The
fact that the homoscedastic GP has no knowledge of the heteroscedasticity of the noise rate function
puts it at a serious disadvantage. In the first sin wave problem, designed to highlight this point, the
heteroscedastic Bayesian Optimisation scheme consistently and preferentially finds the first maximum
as that which minimises aleatoric noise. In contrast the homoscedastic GP, finds it impossible to
differentiate between the two maxima. The experiments provide strong evidence that modelling
heteroscedasticity in Bayesian Optimisation is a more flexible approach to assuming homoscedastic
noise.
6	Experiments on Antifragility to Aleatoric Uncertainty
6.1	Implementation
The implementational details remain the same as for the robustness experiments save for the fact
that the acquisition functions have changed such as to promote antifragility to aleatoric uncertainty.
7
Under review as a conference paper at ICLR 2019
Figure 5: Results of heteroscedastic and homoscedastic Bayesian Optimisation on the 3 robustness
problems considered. Error bars are computed using 10 random initialisations. The first problem is a
maximisation problem whereas the second and third are minimisation problems.
(a) 1D Toy Objective
Figure 6: Results of heteroscedastic and homoscedastic Bayesian Optimisation on the sin and Branin
toy problems where antifragility to aleatoric uncertainty is desirable. Error bars are computed using
10 random initialisations. The first problem is a maximisation problem whereas the second is a
minimisation problem. Heteroscedastic AEI in this instance is the AAEI acquisition function.
(b) 2D Branin-Hoo
This was achieved by changing the sign in the expression for ANPEI and using the AAEI acquisition
function in place of het-AEI. The baseline homoscedastic GP and EI combination remains the same.
The homoscedastic GP is incapable of representing input-dependent aleatoric uncertainty and so
is unable to incorporate information about this quantity in any acquisition function that is used in
conjunction with it.
6.2	Discussion
The results of Figure 6 demonstrate that heteroscedastic Bayesian Optimisation is capable of seeking
out solutions possessing high aleatoric noise, a desirable property in materials discovery applications.
The scalarisation heuristic used in the modified ANPEI acquisition function outperforms the AAEI
acquisition function on the toy sin function example suggesting that this approach may be both valid
and beneficial in its simplicity in certain scenarios.
7	Conclusion and Future Work
We have presented an approach for performing Bayesian Optimisation with the explicit goal of
optimising aleatoric uncertainty in the suggestions. We posit that such an approach can prove useful
for the natural sciences in the search for molecules and materials that are robust and antifragile
to experimental noise and in future work we plan to apply our approach to molecular property
optimisation [4]. We demonstrate concrete improvements on one and two-dimensional toy problems
as well as a real-world optimisation problem and contribute an open-source implementation of the
most likely heteroscedastic GP as a surrogate model for Bayesian Optimisation.
8
Under review as a conference paper at ICLR 2019
References
[1]	Rafael Gdmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose MigUel Herndndez-Lobato,
Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Aldn Aspuru-Guzik. Automatic chemical design using a data-driven
continuous representation of molecules. ACS Central Science, 4(2):268-276, 2018.
[2]	Ryan-Rhys Griffiths and Jose Miguel Hernandez-Lobato. Constrained Bayesian optimization
for automatic chemical design. arXiv preprint arXiv:1709.05501, 2017.
[3]	Omar Mahmood and Jose Miguel Hernandez-Lobato. A cold approach to generating optimal
samples. arXiv preprint arXiv:1905.09885, 2019.
[4]	Ksenia Korovina, Sailun Xu, Kirthevasan Kandasamy, Willie Neiswanger, Barnabas Poczos,
Jeff Schneider, and Eric P Xing. Chembo: Bayesian optimization of small organic molecules
with synthesizable recommendations. arXiv preprint arXiv:1908.01425, 2019.
[5]	Tristan Aumentado-Armstrong. Latent molecular optimization for targeted therapeutic design.
arXiv preprint arXiv:1809.02032, 2018.
[6]	Jialin Song, Yury S. Tokpanov, Yuxin Chen, Dagny Fleischman, Kate T. Fountaine, Harry A.
Atwater, and Yisong Yue. Optimizing Photonic Nanostructures via Multi-fidelity Gaussian
Processes. arXiv:1811.07707 [cs, stat], November 2018.
[7]	Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-fidelity bayesian
optimization with gaussian processes. In The 22nd International Conference on Artificial
Intelligence and Statistics, pages 3158-3167, 2019.
[8]	Henry C Herbol, Weici Hu, Peter Frazier, Paulette Clancy, and Matthias Poloczek. Efficient
search of compositional space for hybrid organic-inorganic perovskites via bayesian optimiza-
tion. npj Computational Materials, 4(1):51, 2018.
[9]	Roberto Calandra, Andre Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimiza-
tion for learning gaits under uncertainty. Annals of Mathematics and Artificial Intelligence, 76
(1-2):5-23, 2016.
[10]	James Grant, Alexis Boukouvalas, Ryan-Rhys Griffiths, David Leslie, Sattar Vakili, and En-
rique Munoz De Cote. Adaptive sensor placement for continuous spaces. In Kamalika Chaudhuri
and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Ma-
chine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2385-2393,
Long Beach, California, USA, 09-15 Jun 2019. PMLR.
[11]	Simon Olofsson, Mohammad Mehrian, Roberto Calandra, Liesbet Geris, Marc Peter Deisenroth,
and Ruth Misener. Bayesian multiobjective optimisation with mixed analytical and black-box
functions: Application to tissue engineering. IEEE Transactions on Biomedical Engineering,
66(3):727-739, 2018.
[12]	Guilherme Duarte Ramos Matos, Daisy Y Kyu, Hannes H Loeffler, John D Chodera, Michael R
Shirts, and David L Mobley. Approaches for calculating solvation free energies and enthalpies
demonstrated with an update of the freesolv database. Journal of Chemical & Engineering
Data, 62(5):1559-1569, 2017.
[13]	Ryan-Rhys Griffiths, Philippe Schwaller, and Alpha A. Lee. Dataset bias in the natural sciences:
A case study in chemical reaction prediction and synthesis design. ChemRxiv, 2018.
[14]	Jose Miguel Hernandez-Lobato, James Requeima, Edward O Pyzer-Knapp, and Alan Aspuru-
Guzik. Parallel and distributed thompson sampling for large-scale accelerated exploration of
chemical space. In International Conference on Machine Learning, pages 1470-1479, 2017.
[15]	Nassim Nicholas Taleb. Antifragile: Things That Gain from Disorder. Random House, New
York, 1st ed edition, 2012. ISBN 978-1-4000-6782-4.
[16]	Yuanyuan Zhou and Yixin Zhao. Chemical stability and instability of inorganic halide per-
ovskites. Energy & Environmental Science, 12(5):1495-1511, May 2019. ISSN 1754-5706.
doi: 10.1039/C8EE03559H.
9
Under review as a conference paper at ICLR 2019
[17]	Roberto Calandra. Bayesian Modeling for Optimization and Control in Robotics. PhD thesis,
Technische Universit泣,2017.
[18]	MigUel Ldzaro-Gredilla and Michalis K Titsias. Variational heteroScedaStic Gaussian process
regression. In Proceedings of the 28th International Conference on International Conference
on Machine Learning, pages 841-848. OmniPress, 2011.
[19]	Scott R Kuindersma, Roderic A Grupen, and Andrew G Barto. Variable risk control via
stochastic optimization. The International Journal of Robotics Research, 32(7):806-825, 2013.
[20]	John-Alexander M Assael, Ziyu Wang, Bobak Shahriari, and Nando de Freitas. Heteroscedastic
treed Bayesian optimisation. arXiv preprint arXiv:1410.7172, 2014.
[21]	Ryo Ariizumi, Matthew Tesch, Howie Choset, and Fumitoshi Matsuno. Expensive multiobjec-
tive optimization for robotics with consideration of heteroscedastic noise. In 2014 IEEE/RSJ
International Conference on Intelligent Robots and Systems, pages 2230-2235. IEEE, 2014.
[22]	Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization
with gaussian processes. In International Conference on Machine Learning, pages 997-1005,
2015.
[23]	Felix Berkenkamp, Andreas Krause, and Angela P. Schoellig. Bayesian optimization with safety
constraints: Safe and automatic parameter tuning in robotics. ArXiv, abs/1602.04450, 2016.
[24]	Peter Frazier, Warren Powell, and Savas Dayanik. The knowledge-gradient policy for correlated
normal beliefs. INFORMS journal on Computing, 21(4):599-613, 2009.
[25]	Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained bayesian
optimization with noisy experiments. Bayesian Analysis, 14(2):495-519, 2019.
[26]	Pedro M Domingos. A few useful things to know about machine learning. Commun. acm, 55
(10):78-87, 2012.
[27]	C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press,
2006.
[28]	Kristian Kersting, Christian Plagemann, Patrick Pfaff, and Wolfram Burgard. Most likely het-
eroscedastic Gaussian process regression. In Proceedings of the 24th International Conference
on Machine Learning - ICML ’07, pages 393-400, Corvalis, Oregon, 2007. ACM Press. ISBN
978-1-59593-793-3. doi: 10.1145/1273496.1273546.
[29]	Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of
expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.
[30]	Victor Picheny, Tobias Wagner, and David Ginsbourger. A benchmark of kriging-based infill
criteria for noisy optimization. Structural and Multidisciplinary Optimization, 48(3):607-626,
2013.
[31]	Emmanuel Vazquez, Julien Villemonteix, Maryan Sidorkiewicz, and Eric Walter. Global
optimization based on noisy evaluations: an empirical study of two statistical approaches. In
Journal of Physics: Conference Series, volume 135, page 012100. IOP Publishing, 2008.
[32]	Deng Huang, Theodore T Allen, William I Notz, and Ning Zeng. Global optimization of
stochastic black-box systems via sequential kriging meta-models. Journal of global optimization,
34(3):441-466, 2006.
[33]	Iain Murray. Introduction to gaussian processes.
[34]	Enqing Hou, Xiang Tan, Marijke Heenan, and Dazhi Wen. A global dataset of plant available
and unavailable phosphorus in natural soils derived by Hedley method. Scientific Data, 5(1):
180166, December 2018. ISSN 2052-4463. doi: 10.1038/sdata.2018.166.
10
Under review as a conference paper at ICLR 2019
A Heteroscedasticity of the Soil Phosphorus Fraction Dataset
Table 1 is used to demonstrate the efficacy of modelling the soil phosphorus fraction dataset using a
heteroscedastic GP. The heteroscedastic GP outperforms the homoscedastic GP on prediction based
on the metric of negative log predictive density (NLPD)
1n
NLPD = n£- logp(ti |xi)
n i=1
(16)
which penalises both over and under-confident predictions.
Table 1: Comparison of NLPD values on the soil phosphorus fraction dataset. Standard errors are
reported for 10 independent train/test splits.
Soil Phosphorus Fraction Dataset GP Het GP
NLPD	1.35 ± 1.33	1.00 ± 0.95
11