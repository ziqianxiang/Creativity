Under review as a conference paper at ICLR 2020
Deep Exploration by Novelty-pursuit with
Maximum State Entropy
Anonymous authors
Paper under double-blind review
Ab stract
Efficient exploration is essential to reinforcement learning in huge state space. Re-
cent approaches to address this issue include the intrinsically motivated goal ex-
ploration process (IMGEP) and the maximum state entropy exploration (MSEE).
In this paper, we disclose that goal-conditioned exploration behaviors in IMGEP
can also maximize the state entropy, which bridges the IMGEP and the MSEE.
From this connection, we propose a maximum entropy criterion for goal selec-
tion in goal-conditioned exploration, which results in the new exploration method
novelty-pursuit. Novelty-pursuit performs the exploration in two stages: first, it
selects a goal for the goal-conditioned exploration policy to reach the boundary
of the explored region; then, it takes random actions to explore the non-explored
region. We demonstrate the effectiveness of the proposed method in environments
from simple maze environments, Mujoco tasks, to the long-horizon video game of
SuperMarioBros. Experiment results show that the proposed method outperforms
the state-of-the-art approaches that use curiosity-driven exploration.
1	Introduction
Efficient exploration is important to learn a (near-) optimal policy for reinforcement learning (RL) in
huge state space (Sutton & Barto, 1998). Dithering strategies like epsilon-greedy, Gaussian action
noise, and Boltzmann exploration are inefficient and require exponential interactions to explore the
whole state space. In contrast, deep exploration (Osband et al., 2016) overcomes this dilemma via
temporally extended behaviors with a long-term vision. Recently, proposed methods include the in-
trinsically motivated goal exploration process (IMGEP) (Forestier et al., 2017), and maximum state
entropy exploration (MSEE) (Hazan et al., 2019). In particular, IMGEP selects interesting states
from the experience buffer as goals for a goal-conditioned exploration policy. In this way, explo-
ration behaviors are naturally temporally-extended via accomplishing self-generated goals. On the
other hand, MSEE aims to search for a policy such that it maximizes the entropy of state distribution.
In this way, the agent can escape from the local optimum caused by insufficient state exploration.
In this paper, we show that the target of maximizing the support of state distribution (discovering
new states) and maximizing the entropy of state distribution (unifying visited state distribution) can
be both achieved by the goal-conditioned policy. From this connection, we propose an exploration
method called novelty-pursuit. Abstractly, our method performs in two stages: first, it selects a
visited state with the least visitation counts as the goal to reach the boundary of the explored region;
then, it takes random actions to explore the non-explored region. An illustration can be seen in
Figure 1. Intuitively, this process is efficient since the agent avoids exploring within the explored
region. Besides, the exploration boundary will be expanded further as more and more new states are
discovered. Finally, the agent will probably explore the whole state space to find the optimal policy.
A naive implementation of the above strategies can lead to inefficient exploration and exploitation
on complex environments. First, to tackle the problem of the curse of dimension and exhaustive stor-
age when selecting the least visited states, we approximate the visitation counts via prediction errors
given by Random Network Distillation (Burda et al., 2019b). Besides, we observe that previous
methods used in IMGEP (Forestier et al., 2017) are inefficient to train the goal-conditioned explo-
ration policy. We employ training techniques based on reward shaping (Ng et al., 1999) and HER
(Andrychowicz et al., 2017) to accelerate training the goal-conditioned policy. Finally, we addition-
ally train an unconditioned exploitation policy to utilize samples collected by the goal-conditioned
1
Under review as a conference paper at ICLR 2020
Figure 1: Illustration for the proposed method. A goal-conditioned policy firstly reaches the explo-
ration boundary, then perform random actions to discover new states.
exploration policy with environment-specific rewards. Thus, the exploration and exploitation is de-
coupled in our method.
Our contributions are summarized as follows: (1) We disclose that goal-conditioned behaviors can
also maximize the state entropy, which bridges the intrinsically motivated goal exploration process
and the maximum state entropy explore. (2) We propose a method called novelty-pursuit from this
connection and give practical implementations. (3) We demonstrate the exploration efficiency of the
proposed method and achieve better performance on environments from the maze, Mujoco tasks, to
long-horizon video games of SuperMarioBros.
2	Background
Reinforcement Learning. In the standard reinforcement learning framework (Sutton & Barto,
1998) a learning agent interacts with a Markov Decision Process (MDP). The sequential decision
process is characterized as follows: at each time t, the agent receives a state st from the environment
and selects an action at from its policy π(s, a) = Pr{a = at|s = st}; that decision is sent back to
the environment, and the environment gives a reward signal r(st , at) and transits to the next state
st+1 based on the state transition probability psas0 = Pr{s0 = st+1 |s = st, a = at}. This process
repeats until the agent encounters a terminal state after which the process restarts. The main target of
reinforcement learning is to maximize the expected discounted return Eπ [Pt∞=0 γtrt] in an unknown
environment, where γ ∈ (0, 1] is a factor that balances the importance of future reward. Without
information about environment dynamics and task-specific rewards in advance, the agent needs ex-
ploration to discover potential valuable states. Apparently, the learned policy may be sub-optimal if
the exploration strategy cannot lead to explore the whole state space.
Intrinsically Motivated Goal Exploration Process. Intrinsically motivated goal exploration pro-
cess (IMGEP) (Baranes & Oudeyer, 2009; Forestier et al., 2017) relies on a goal-conditioned (or
goal-parameterized) policy πg for unsupervised exploration. It involves the following steps: 1) se-
lecting an intrinsic or interesting state from the experience buffer as the desired goal; 2) exploring
with a goal-conditioned policy πg(s, a, g) = Pr{at = a|st = s, gt = g}; 3) reusing experience for
an exploitaion policy πe(s, a) = Pr{at = a|st = s} to maximize the external reward. Note that the
performance of exploitation policy πe relies on samples collected by the goal-exploration policy πg .
Thus, the criterion of goal selection is crucial for IMGEP.
Maximum State Entropy Exploration. Maximum state entropy exploration (Hazan et al., 2019)
aims to search an exploration policy π* SUCh that it maximizes the entropy of induced state distri-
bution (or minimizes the KL-divergence between the uniform distribution and induced state distri-
bution) among the class of stationary policies (i.e., π* ∈ arg maxπ H[dπ], where dπ is the state
distribution induced by π). Without any information about tasks given by the environment, we think
maximum state entropy exploration is safe for exploitation.
3	IMGEP with maximum state entropy exploration
In this section, we bridge the intrinsically motivated goal exploration process and maximum state
entropy exploration. We begin with practical considerations when maximizing state entropy, then
analyze the exploration characteristics of the proposed goal-selection method for IMGEP.
2
Under review as a conference paper at ICLR 2020
In practice, an exact density estimator for high-dimension state space is intractable, and the state
space is unknown, which leads to an empirical state distribution over visited states. The differences
are important. For example, directly optimizing the entropy of empirical state distribution over
visited states is not what we want, because it ignores the non-visited states outside of the empirical
state distribution (see the top row in Fig 2). Instead, we need to first maximize the support of
induced state distribution (i.e., discovering new states), then we maximize the entropy of induced
state distribution with full support (see the bottom row in Fig 2). In the following, we demonstrate
that selecting the states with the least visitation counts among visited states as goals can achieve the
above functions under some assumptions.
Figure 2: Histograms for normalized state visitation counts, where the x-axis represents the index
of state. Top row: directly maximizing the entropy of empirical state distribution over visited states;
Bottom row: firstly maximizing the counting measure of induced state distribution support, then
maximizing the entropy of state distribution with full support.
Let the set {1, 2, ∙ ∙ ∙ , |S ∣} denotes the state space S, π Lt denotes the set of policies {π 1 ,π 2,… ,πt}
over previous iterations, πt+1 denotes the policy of next iteration, xit denotes the cumulative visi-
tation counts of state i induced by history policies πLt, and Nt = Pl,=∣1 Xt denotes the sum of all
state visitation counts. Hence, the entropy of empirical state distribution induced by policies π 1:t
is defined as H[dπ 1:t (S)] = Pl,S=l1 N Iog N (Ht for short), and the counting measure of empirical
state distribution support induced by policies π 1:t is defined as μ[dπ 1:t (S)] = Pl,=∣11(Xt ≥ 1) (μt
for short), where I is the indicator function.
The theoretical analysis starts with the situation that each iteration the goal-conditioned exploration
policy can only select a state to visit without consideration of trajectories towards the goal. Our
question is which state to visit gives the most benefits in terms of maximum state entropy. This
question is closely related to the goal generation in IMGEP. To facilitate the analysis, let the unit
vector e = [0, ∙∙∙ , 1,... ] ∈ RS | denotes a choice (i.e., e (i) = 1 indicates that the policy selects
i-th state to visit). Note that Xt+1 = Xt + et with this assumption.
Proposition (Max Counting Measure of Support) For any state i ∈ {1, ∙ ∙ ∙ , |S|} with Xit ≥ 0,
unless the unvisited state sets K = {i|Xit = 0} is an empty set, for any choice et such that et(i) = 1
with Xt = 0, we have μt+1 = μt + 1.
This Proposition states visiting the non-visited states is to maximize the counting measure of induced
state distribution support. The agent improves its policy by discovering new valuable states. In
practical applications, we don’t have access to non-visited states in advance. In other words, we
can’t select these non-visited states as goals since they are not contained in the experience buffer.
To deal with this problem, we assume that the chance of discovering non-visited states is high when
the agent perform random actions to explore around the exploration boundary. The exploration
boundary can be understood as the set of visited states with the least visitation counts (See Figure 1
for the illustration). Our assumption is based on the fact that the total visitations counts of the visited
region are large and the total visitation counts of the non-visited region are small. In conclusion,
the goal-conditioned exploration policy is asked to reach the exploration boundary, then it performs
random actions to discover new states to maximize the counting measure.
3
Under review as a conference paper at ICLR 2020
Theorem 1 (Max Entropy) For any state i ∈ {1, ∙∙∙ , ∣S∣} with Xt ≥ 1; for any choice et such
that e↑ (i) = 1 with i ∈ arg minj xj, we have e↑ ∈ arg max£七 Ht +ι.
We provide the proof in the appendix A.1. Theorem 1 characterizes the behavior of visiting the
states with the least visitations when the whole state space has been explored (i.e., the stage after
maximizing the counting measure of induced state distribution support). Since Theorem 1 still
suggests selecting states with the least visitation counts as goals, the above method can also be
applied to maximize the entropy of induced state distribution. Actually, it is easy to unify the two
stages via a smoothed entropy Hσ(dπ) = -Edπ [log(dπ) +σ] (Hazan et al., 2019). For our problem,
the definition of entropy is proper by assigning non-visited states with a “dummy” visitation counts
between 0 and 1. In that case, Theorem 1 still holds and suggests firstly selecting these non-visited
states and subsequently selecting the states with least visitation counts to maximize the smoothed
state entropy.
The proposed exploration method is called novelty-pursuit. We notice that the above analysis
neglects the influences of trajectories towards the exploration boundary. However, the fluctuation
of state distribution entropy by the trajectories towards the exploration boundary is less significant
from practical considerations. In fact, the goal-conditioned policy should be trained to reach the
exploration boundary quickly and pays more efforts to discover new states around the exploration
boundary, as our experiment results in Section 5.1 indicate.
4	Method
In this section, we present practical implementations for the proposed method. How to approximate
visitation counts in high-dimension space and how to estimate the exploration boundary is given
in Section 4.1. We describe the training technique of goal-conditioned policy in Section 4.2. Fi-
nally, we introduce an exploitation policy to learn the experience collected by the goal-conditioned
exploration policy in Section 4.3. We outline the proposed exploration method in Algorithm 1.
4.1	Approximating exploration boundary in high-dimension space
Generally, computing the visitation counts in high-dimension space is intractable. However, it is
possible to build some variables related to the visitation counts. For example, Burda et al. (2019b)
show that prediction errors given by two randomly initialized network have a strong relationship to
the number of training samples on the MNIST dataset. Thus, we can use the prediction errors to sort
visited states. Other approaches like pseudo-counts (Bellemare et al., 2016; Ostrovski et al., 2017)
can be also applied, but we find that RND is easy to scale up.
RND is consist of two randomly initialized neural networks: a fixed network called target network
ʌ
f(x; ωt), and a trainable network called predictor networkf (x; ωp). Both two networks take a state
s as input and output a vector with the same dimension. Each time a batch of data feeds into the
predictor network to minimize the difference between the predictor network and the target network
concerning the predictor network’s parameters, shown in Equation 1.
1K
min κ ^∖∣f (Si ； ωt) - f( Si; 3p) ||2	(1)
p i=1
In practice, we employ an online learning setting to train RND and maintain a priority queue to
store states with the highest prediction errors. In particular, after a goal-conditioned policy collects
a mini-batch of transitions, this data feed to train the predictor network. Also, a state with high
prediction error will be stored into the priority queue and the state with the least prediction error
will be removed out of the priority queue if full. This process repeats and no historical data will be
reused to train the predictor network. Besides, each iteration a state will be selected from the priority
queue as a goal for the goal-conditioned policy. After achieving the goal, the exploration policy will
perform random actions to discover new states. Consider the bias due to approximation, we sample
goals from a distribution based on their prediction errors (e.g., softmax distribution).
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Exploration by novelty-pursuit
Input: predictor network update interval K; goal-conditioned policy update interval M; mini-
batch size of samples for goal-conditioned policy N ;
Initialize parameter θ for goal-conditioned exploration policy πg(s, g, a; θ).
ʌ
Initialize parameter ωt for target network f(x; ωt), and ωp for predictor network f(x; ωp).
Initialize a buffer Dg for πg , and a priority queue Q to store states with least visitation counts.
for each iteration do
Reset the environment and get the observation o0 ;
Choose a goal g from priority queue Q, and set goal.success = False;
for each timestep t do
if goal_success == True then
Choose an random action at ; # Explore around the exploration boundary
else
Choose an action at from πg(st, g, at; θ); # Go to the exploration boundary
end if
Send at to the environment and get rte, st+1;
Update goalsuccess(St+1, g);
#	Store new states and update the predictor network
if t % K == 0 then
Store transitions {sk, g, ak, rke}tk=t-K into replay buffer Dg;
Calculate prediction errors for {sk}tk=t-K and store them into priority queue Q;
Update predictor network f(x; ωp) using {sk}tk=t-K;
end if
#	Update πg with reward shaping
if t % M == 0 then
Update πg with {sk, gk, ak,rki }kK=1 sampled from Dg;
end if
end for
end for
4.2	Training goal-conditioned policy efficiently
Before we describe the training techniques for the goal-conditioned policy, we emphasize that train-
ing this policy doesn’t require the external reward signal from the environment. But we additionally
use the external reward for the goal-conditioned policy to reduce the mismatch behaviors between
the goal-conditioned policy πg and the exploitation policy πe .
Following multi-goal reinforcement learning (Andrychowicz et al., 2017; Plappert et al., 2018a), we
manually extract goal information from state space. Specifically, each state s is associated with an
achieved goal of ag, and the desired goal is denoted as g . To avoid ambiguity, a goal-conditioned
policy πg(s, a, g; θ)1 is asked to accomplish a desired goal g. For our settings, the achieved goal is
coordinate information.
r(agt, gt) =	1 if d(agt, gt) <	(2)
t , t 0 otherwise
A proper reward function for the goal-conditioned policy is an indicator function with some tol-
erance, shown in Equation 2. With a little abuse of notations, between the achieved goal ag and
the desired goal g we use d(ag, g) to denote some “distance” (e.g., L1 or L2 norm) between the
achieved goal ag and the desired goal g. If the distance is less than some threshold , the goal-
conditioned policy receives a positive reward otherwise zero. Note that this function is also be used
to judge whether agents reach the exploration boundary. However, the training of goal-conditioned
policy is slow with this sparse reward function. Next, we introduce some techniques to deal with
this problem.
r(agt, gt) = d(agt-1, gt) - d(agt, gt)	(3)
Reward shaping introduces additional training rewards to guide the agent. Reward shaping is in-
variant to the optimal policy if shaping reward function is a potential function (Ng et al., 1999). 1
1With the respect of input to a goal-conditioned policy, s contains ag to keep notations simple.
5
Under review as a conference paper at ICLR 2020
Specifically, we define the difference of two consecutive distances (between the achieved goal and
the desired goal) as shaping reward function, shown in Equation 3. Since shaping reward function
is dense, it can lead to substantial reductions in learning time. Verification of the optimal goal-
conditioned policy is invariant between this function and the indicator reward function is given in
Appendix A.2. Alternatively, one can use also Hindsight Experience Replay (HER) (Andrychowicz
et al., 2017) to train the goal-conditioned policy via replacing each episode with an achieved goal
rather than one that the agent was trying to achieve. But one should be careful since HER changes
the goal distribution for learning. Besides, one can also utilize past trajectories to accelerate training,
which we discuss in Appendix A.3.
4.3	Exploiting experience from exploration policy
Parallel to the goal-conditioned exploration, we additionally train an unconditioned exploitation
policy πe, which only takes the state as input. This policy learns from experience collected by the
exploration policy πg in an off-policy learning fashion. At the same time, the exploitation policy
also interacts with the environment to mitigate the side effect of exploration error (Fujimoto et al.,
2019), a phenomenon that off-policy learning degenerates when data from the exploration policy is
not correlated to the experience generated by the exploitation policy. Note that exploitation policy
is trained with an RL objective to maximize expected discounted external return. Therefore, the
exploration and exploitation are naturally decoupled, which turns out to help escape the local opti-
mum on SuperMarioBros environments. From this perspective, our method is distinguished from
Go-Explore Ecoffet et al. (2019), which employs exploration followed by exploitation.
5 Experiment
In this section, we aim to answer the following research questions: 1) Does novelty-pursuit effec-
tively maximize the state entropy? 2) Do the proposed goal-selection criterion and training tech-
niques improve performance for IMGEP? 3) How does the performance of novelty-pursuit compare
with the state-of-the-art approaches in complex environments? We conduct experiments from the
simple maze environments, Mujoco tasks, to long-horizon video games of SuperMarioBros to eval-
uate the proposed method. Detailed policy network architecture and hyperparameters are given in
Appendix A.6 and A.7, respectively.
Here we briefly describe the environment settings (see Figure 3 for illustrations). Detailed settings
are given in the Appendix A.5.
Empty Room & Four Rooms. An agent navigates in the maze of 17 × 17 to find the exit (Chevalier-
Boisvert et al., 2018). The agent receives a time penalty until it finds the exit and receives a positive
reward. The maximum return for both two environments is +1, and the minimum total reward is
-1. Note that the observation is a partial image of shape (7, 7, 3).
FetchReach. A 7-DOF Fetch Robotics arm (simulated in the Mujoco (Todorov et al., 2012)) is
asked to grip spheres above a table. There are 4 spheres on the table, and the robot receives a
positive reward of+1 when its gripper catches a sphere (the sphere will disappear after being caught)
otherwise it receives a time penalty. The maximum total reward is +4, and the minimum total reward
is -1.
SuperMarioBros. A Mario agent with raw image observation explores to discover the flag. The
reward is based on the score given by the NES simulator (Kauten, 2018) and is clipped into -1 and
+1 except +50 when getting a flag. There are 24 stages in the game, but we only focus on the 1-1,
1-2, and 1-3.
5.1	Comparison of exploration efficiency
In this section, we study the exploration efficiency in terms of the state distribution entropy. We
focus on the Empty Room environment because it is tractable to calculate the state distribution
entropy. Note that we don’t use any external reward the observation for RND is a local-view image.
We consider the following baselines: 1) random: uniformly selecting actions; 2) bonus: a policy
receiving exploration bonus based on the prediction errors of RND (Burda et al., 2019b); 3) novelty-
6
Under review as a conference paper at ICLR 2020
Figure 3: Illustration of four environments considered in this paper.
(c) FetchReach
(d) SuperMarioBros
pursuit: the proposed method. We also consider three variants of our method: 4) novelty-pursuit-
planning oracle: the proposed method with a perfect goal-conditioned policy; 5) novelty-pursuit-
counts-oracle: the proposed method with selecting goals based on true visitation counts; 6) novelty-
pursuit-oracles: the proposed method with both two oracles. The results are summarized in Table 1.
Note that the maximum state distribution entropy for this environment is 5.666.
Table 1: Average entropy of visited state distribution at timesteps 200k over 5 seeds on Empty
Room.
	Entropy
random	5.129 ± 0.021
bonus	5.138 ± 0.085
novelty-pursuit	5.285 ± 0.073
novelty-pursuit-planning-oracle	5.513 ± 0.077
novelty-pursuit-counts-oracle	5.409 ± 0.059
novelty-pursuit-oracles	5.627 ± 0.001
maximum	5.666
First, we can see that novelty-pursuit achieves a higher entropy than the random and bonus method.
Though exploration bonus via prediction errors of RND may help makes an exploration-exploitation
trade-off (Burda et al., 2019b), but is inefficient to a maximum state entropy exploration. We at-
tribute this to delayed and indirect feedbacks of the exploration bonus. Second, when the planning
oracle and visitation counts oracle are available, the entropy of our method roughly improves by
0.228 and 0.124, respectively. We notice that the planning-oracle avoids exploration towards the ex-
ploration boundary and spends more meaningful steps to explore around the exploration boundary,
thus greatly improves the entropy. Based on this observation, we think accelerating goal-conditioned
policy training is more important for our method. Actually, we find the proposed method can satisfy
our need to approximate the exploration boundary via prediction errors of RND (See Appendix A.4
for more results). Third, the combination of two oracles gives a near-perfect performance (the gap
between the maximum state entropy is only 0.039). This result demonstrates that goal-condition
exploration behaviors presented by novelty-pursuit can maximize the state entropy and validates the
analysis in Section 3.
5.2	Ablation study of goal-selection and training techniques
In this section, we study the factors that contribute to our method by ablation experiments. Firstly, we
focus on the criterion of goal-section in IMGEP. We compare novelty-pursuit with two other goal-
selection methods: 1) random-selection: selecting states randomly from the experience buffer; 2)
learning-progress: selecting a feasible state (goal success rate is between 0.3 and 0.7) with probabil-
ity of 0.8 and an arbitrary visited state with the probability of 0.2, which is adopted from (Forestier
et al., 2017). Results on the Empty Room are shown in Figure 4. Secondly, we study how goal-
conditioned policy learning affects performance. We compare HER and the reward-shaping with
distance reward (i.e., reward based on L1 norm in our problem) used in (Forestier et al., 2017).
Results on the Empty Room are shown in Figure 5.
From Figure 4, we see that IMGEP doesn’t work when randomly selecting goals, but novelty-pursuit
gives a greater boost compared to the learning-progress. We think the reason is that this heuristic
method is brittle to the estimation of goal success rate and lacks an explicit exploration objective.
7
Under review as a conference paper at ICLR 2020
From Figure 5, we find that the IMGEP with HER or reward shaping outperforms than the IMGEP
with distance reward. As discussed in Ng et al. (1999), reward based on distance may change the
optimal behavior of goal-condition exploration policy, thus hurts the performance for IMGEP.
Figure 4: Comparison of goal-selection. Figure 5: Comparison of training techniques.
e3bj 3posa3
0 5 0 5
L
-
ebj 3posa3
k
200
5.3	Evaluation on complex environments
In this section, we compare different methods in terms of external reward. We will see that without
sufficient and efficient exploration, the policy may be stuck into the local optimum. Two baseline
methods using reinforcement learning are considered: 1) vanilla: DDPG (Lillicrap et al., 2016) with
Gaussian action noise on Fetch Reach and ACER (Wang et al., 2017) with policy entropy regular-
ization on others; 2) bonus: an off-policy version of (Burda et al., 2019b) that combines the external
reward and intrinsic reward based on the vanilla policy. Note reported results of novelty-pursuit are
the performances of the exploitation policy πe rather than the goal-conditioned exploration policy
πg . We keep the number of samples and training iterations same for all methods.
First, we consider the previously used Empty Room and the Four Room environments. The results
are shown in Figure 6. We see that the vanilla policy hardly finds the exit. Novelty-pursuit is
comparative to bonus and outperforms bonus on the Four Rooms environment, where we observe
that bonus is somewhat misled by the intrinsic reward though we have tried many weights to balance
the external reward and intrinsic reward.
Secondly, we consider the FetchReach environment and results are shown in Figure 6. We see that
novelty-pursuit can consistently grip 4 spheres while other methods sometimes fail to efficiently
explore the whole state space to grip 4 spheres.
Finally, we consider the SuperMarioBros environments, in which it is very hard to discover the flag
due to the huge space state and the long horizon. Learning curves are plotted in Figure 7 and the final
performance is listed in Table 2. We find the vanilla method gets stuck into the local optimum on
SuperMarioBros-1-1 while the bonus method and ours can find a near-optimal policy. All methods
perform well on SuperMarioBros-1-2 thanks to dense rewards. On SuperMarioBros-1-3, reward is
sparse and the task is very challenging. We plot trajectories of SuperMarioBros-1-3 in Figure 8, and
Empty Room
5 0 5
S-
EnlaJ eposae
----novelty-pursuit --------- bonus --------- vanilla
Four Rooms
Fetch Reach
3 2 10
0	250k	500k	750k IM
steps
0	50k IOOk 150k	200k	" 0	125k	250k	375k	500k
steps	steps
Figure 6: Average episode returns over 5 seeds on the Empty Room, Four Rooms and FetchReach
environments. Shadows indicate the standard deviation.
8
Under review as a conference paper at ICLR 2020
----novelty-pursuit --------- bonus --------- vanilla
Figure 7: Average episode returns over 3 seeds on SuperMarioBros. Shadows indicate the standard
deviation.
Figure 8: Trajectory visualization on SuperMarioBros-1-3. Trajectories are plotted in green cycles
with the same number samples (18M). The agent starts from the most left part and needs to fetch the
flag on the most right part. Top row: vanilla; middle row: bonus; bottom row: novelty-pursuit.
more results can be found in Appendix A.4. It turns out only our method can get positive rewards
via a deep exploration presented by the goal-conditioned policy on SuperMarioBros-1-3.
Table 2: Final Performance over 3 seeds on SuperMarioBros.
SuperMarioBros-1-1
SuperMarioBros-1-2
SuperMarioBros-1-3
novelty-pursuit
36.02 ± 8.19
33.30 ± 6.13
8.14 ± 0.55
bonus
17.74 ± 7.84
33.19 ± 1.53
0.20 ± 0.14
vanilla
8.43 ± 0.14
29.64 ± 2.02
-0.07 ± 0.01
6	Related work
Exploration. Traditionally, the exploration strategy is based on the exploitation policy that receives
an external reward from the environment. Traditional exploration methods include injecting noise
on action space (Mnih et al., 2015; Lillicrap et al., 2016) or parameter space (Plappert et al., 2018b;
Fortunato et al., 2018), and adding the policy’s entropy regularization (Schulman et al., 2017; Mnih
et al., 2016).
For tabular Markov Decision Process, there are lots of work utilizing confidence based reward to
balance exploration and exploitation (Kearns & Singh, 2002; Strehl & Littman, 2008; Kolter & Ng,
2009; Lattimore & Hutter, 2014). Several exploration strategies for deep RL based approximation
visitation counts have been proposed in high-dimension space (Bellemare et al., 2016; Ostrovski
et al., 2017). Another type of exploration is curiosity-driven exploration. These methods track the
uncertainty of dynamic (Stadie et al., 2015; Pathak et al., 2017; Burda et al., 2019a;b) to explore
intrinsic states. Deep (temporally extended) exploration via tracking the uncertainty of value func-
tion is studied in (Osband et al., 2016). Besides, maximum (policy) entropy reinforcement learning
9
Under review as a conference paper at ICLR 2020
encourages exploration by maximizing the cumulative sum of external reward and policy entropy
(Ziebart et al., 2008; Haarnoja et al., 2017; O’Donoghue et al., 2016; Haarnoja et al., 2018).
Recently, Hazan et al. (2019) introduce a new exploration objective: maximum state entropy. They
provide an efficient algorithm when restricted to a known tabular MDP (a density estimator oracle
is required for an unknown tabular MDP) and gives the theoretical analysis. We derive the criterion
of goal generation based on the principle of maximum state entropy.
Our method is based on the framework of intrinsically motivated goal exploration processes
(IMGEP)(Baranes & Oudeyer, 2009; Forestier et al., 2017; Pere et al., 2018). Go-ExPIore (Ecoffet
et al., 2019) is reminiscent of IMGEP and achieves dramatic improvement on the hard exploration
Problem of Montezumas Revenge. But with the assumPtion that the environments are resettable or
deterministic and many hand-engineering designs, Go-ExPlore is restricted to sPecific environments.
Our method shares a similar exPloration strategy like Go-ExPlore, but our method is imPlemented
Practically and can be aPPlied to stochastic environments. ImPortantly, we aim to answer the core
question: why such defined goal-conditioned exPloration is efficient?
Goal-conditioned Policy. By taking environment observation and desired goal as inPuts, the goal-
conditioned Policy is exPected to accomPlish a series of tasks. Schaul et al. (2015) ProPose the uni-
versal value function aPProximator (UVFA) and train it by bootstraPPing from the Bellman equation.
However, training goal-condtioned Policy is also still a challenging Problem due to goal-condition
reward is sParse (e.g. 1 for success, 0 for failure). Andrychowicz et al. (2017) ProPose hindsight
exPerience rePlay (HER) by rePlacing each ePisode with an achieved goal rather than one that the
agent was trying to achieve. This oPeration introduces more reward signals and serves as an imPlicit
curriculum. Florensa et al. (2018) use a generator network to adaPtively Produce artificial feasible
goals. We also use a goal-conditioned Policy, but goals are selected from the exPerience buffer rather
than being sPecified in advance. What’s more, we utilize the technique of reward shaPing (Ng et al.,
1999) to accelerate training.
Learning from experience. Off-Policy reinforcement learning algorithms such as DQN(Mnih et al.,
2015), DDPG (LillicraP et al., 2016), and ACER (Wang et al., 2017), reuse exPerience to imProve
data efficiency. Besides, how to additionally utilize (good) exPerience to overcome exPloration
dilemma is studied in (Oh et al., 2018; Goyal et al., 2019). These works are PerPendicular to ours
since we focus on how to discover these valuable states.
7	Conclusion
This PaPer bridges the intrinsically motivated goal exPloration Process (IMGEP) and the maximum
state entroPy exPloration (MSEE). We ProPose a method called novelty-Pursuit from the connection.
We demonstrate the ProPosed method is efficient towards exPloring the whole state sPace. Therefore,
the ProPosed method can escaPe from the local oPtimum and heads the (near-) oPtimal Policy. We
notice that current training techniques of the exPloitation Policy are based on an RL objective, which
may not be efficient to utilize exPerience collected by the exPloration Policy. Theoretically, the
influence of trajectories towards the exPloration bound should be considered. We leave these for
future works.
References
Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,
Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight exPerience rePlay.
In Proceedings of the 30th Advances in Neural Information Processing Systems, pp. 5048-5058,
2017.
Adrien Baranes and Pierre-Yves Oudeyer. R-IAC: robust intrinsically motivated exploration and
active learning. IEEE Transactions on Autonomous Mental Development, 1(3):155-169, 2009.
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom SchauL David Saxton, and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In Proceedings of the 29th
Advances in Neural Information Processing Systems 29, pp. 1471-1479, 2016.
10
Under review as a conference paper at ICLR 2020
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.
Yuri Burda, Harrison Edwards, Deepak Pathak, Amos J. Storkey, Trevor Darrell, and Alexei A.
Efros. Large-scale study of curiosity-driven learning. In Proceedings of the 7th International
Conference on Learning Representations, 2019a.
Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network
distillation. In Proceedings of 7th International Conference on Learning Representations, 2019b.
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym- minigrid, 2018.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore: a
new approach for hard-exploration problems. CoRR, abs/1901.10995, 2019.
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for
reinforcement learning agents. In Proceedings of the 35th International Conference on Machine
Learning ,pp.1514-1523,2018.
Sebastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal explo-
ration processes with automatic curriculum learning. CoRR, abs/1708.02190, 2017.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Os-
band, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles
Blundell, and Shane Legg. Noisy networks for exploration. In Proceedings of the 6th Interna-
tional Conference on Learning Representations, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Proceedings of the 36th International Conference on Machine Learning, pp. 2052-
2062, 2019.
Anirudh Goyal, Philemon Brakel, William Fedus, Soumye Singhal, Timothy P. Lillicrap, Sergey
Levine, Hugo Larochelle, and Yoshua Bengio. Recall traces: Backtracking models for efficient
reinforcement learning. In Proceedings of the 7th International Conference on Learning Repre-
sentations, 2019.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Proceedings of the 34th International Conference on Machine
Learning, pp. 1352-1361, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning, pp. 1856-1865, 2018.
Elad Hazan, Sham M. Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum
entropy exploration. In Proceedings of the 36th International Conference on Machine Learning,
pp. 2681-2691, 2019.
Christian Kauten. Super Mario Bros for OpenAI Gym. GitHub, 2018. URL https://github.
com/Kautenja/gym-super-mario-bros.
Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time.
Machine Learning, 49(2-3):209-232, 2002.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
of the 3rd International Conference on Learning Representations, 2015.
J. Zico Kolter and Andrew Y. Ng. Near-bayesian exploration in polynomial time. In Proceedings of
the 26th Annual International Conference on Machine Learning, pp. 513-520, 2009.
11
Under review as a conference paper at ICLR 2020
Tor Lattimore and Marcus Hutter. Near-optimal PAC bounds for discounted mdps. Theoretical
Computer Science, 558:125-143, 2014.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Pro-
ceedings of the 4th International Conference on Learning Representations, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings of the 33nd International Conference on Machine Learning, pp. 1928-
1937, 2016.
Andrew Y. Ng, Daishi Harada, and StUart J. RUssell. Policy invariance Under reward transformations:
Theory and application to reward shaping. In Proceedings of the 16th International Conference
on Machine Learning, 1999.
Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. PGQ: combining
policy gradient and q-learning. CoRR, abs/1611.01626, 2016.
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In Proceedings
of the 35th International Conference on Machine Learning, pp. 3875-3884, 2018.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via boot-
strapped DQN. In Proceedings of the 29th Advances in Neural Information Processing Systems,
pp. 4026-4034, 2016.
Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. Count-based ex-
ploration with neural density models. In Proceedings of the 34th International Conference on
Machine Learning, pp. 2721-2730, 2017.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the 34th International Conference on Machine
Learning, pp. 2778-2787, 2017.
Alexandre Pere, SebaStien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised learn-
ing of goal spaces for intrinsically motivated goal exploration. In Proceedings of the 6th Interna-
tional Conference on Learning Representations, 2018.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech
Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request
for research. CoRR, abs/1802.09464, 2018a.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
In Proceedings of the 6th International Conference on Learning Representations, 2018b.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1312-1320,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
Bradly C. Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. CoRR, abs/1507.00814, 2015.
12
Under review as a conference paper at ICLR 2020
Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for
markov decision processes. Journal ofComputerand System Sciences, 74(8):1309-1331, 2008.
Richard S. Sutton and Andrew G. Barto. Introduction to reinforcement learning. MIT Press, 1998.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude, 2012.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033,
2012.
ZiyU Wang, Victor Bapst, NicolaS Heess, Volodymyr Mnih, Remi Munos, Koray KavUkcUoglu, and
Nando de Freitas. Sample efficient actor-critic with experience replay. In Proceedings of the 5th
International Conference on Learning Representations, 2017.
Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. MaximUm entropy inverse
reinforcement learning. In Proceedings of the 33nd AAAI Conference on Artificial Intelligence,
pp. 1433-1438, 2008.
A Appendix
A.1 Proof of theorem 1
SUppose we have two choices among state i and state j, we want to compare the difference g(i, j)
between the entropy Hi[d∏上计」by visiting state i and the entropy Hj [d∏上计/by visiting state j.
Let Nt = Pi xit denotes visitation coUnts over all states. Note that the entropy difference of two
choices can be attribUted to changes in xit+1 and xtj+1:
g(i,j) = Hi [d∏ 1：t+ι ] - Hj [d∏ 1：t +ι ]
_ x Xi +1 lcCr Xt +1	Xj	lcb Xt ∖	χ Xj +1 lcCr Xt +1	Xi lcb	Xi ∖	小
=(- Nt+1 Iog N+	- N+	Iog N+ )	-	(- N+ Iog N+	-	N+ Iog	N+ )	(4)
=(Xt+1 log Xt +1-------XL-	log Xt )	—	( Xi + 1 log Xt + 1--x^- log	Xt )
( Nt + 1 ɪog Nt+1	Nt+1	ɪog Nt + 1 )	( Nt + 1 ɪog Nt+1	Nt+1 ɪog	Nt + 1 )
X Qf X x Zp、  X +1 IC x X +1   X IC(T X ɪiɪrlɔiplɔ QlrlC
Let f (X) = Nt + 1 Iog Nt + 1	Nt + 1 Iog Nt + 1, Which yields
g(i, j) = f(xtj) - f(xit)	(5)
By looking at the derivative of f (x), we know that f(x) is a monotonically increasing fUnction.
ThUs, for any xit < xtj, we have that g(i, j) > 0.
f (x) = N⅛ι°g(I +1) > 0	⑹
In conclUsion, Unless state i has the least visitation coUnts, we can always another state j with
xtj < xit to increase the entropy. Hence, visiting the states with the smallest visitation coUnts is
optimal.
A.2 Reward shaping for multi-goal reinforcement policy
Reward shaping is invariant to the optimal policy Under some conditions (Ng et al., 1999). Here we
verify that reward shaping introdUced by oUr method doesn’t change the optimal policy for goal-
conditioned policy. Adding Up shaping rewards gives:
T
- d(agt,g) + d(agt+1, g)
t=1	(7)
=-d(ag 1,g) + d(ag2,g) - d(ag2,g) + d(ag3,g) +-----d(agτ,g) + d(agτ+1 ,g)
= -d(ag1, g) + d(agT+1,g)
13
Under review as a conference paper at ICLR 2020
For the optimal policy πg, d(agT+1, g) = 0, while d(ag1, g) is a constant. Thus, for the fixed g, the
optimal policy πg induced by the reward shaping is invariant to the one induced by sparse-reward in
Equation 2.
A.3 Training goal-conditioned policy with past trajectories
In fact, training the goal-conditioned policy for our problem is different from the settings of multi-
goal reinforcement learning (Andrychowicz et al., 2017; Plappert et al., 2018a). The goal is selected
from visited states rather than the non-visited states. Thus, we can utilize past trajectories to accel-
erate training with supervised learning. The optimization problem is defined in Equation 8. Note
that we cannot rely on this information on stochastic environments like SuperMarioBros.
min 工	-log πg (at lot ,g;θ)
(at ,ot)〜T (g)
(8)
where τ(g) is the trajectory that covers the goal g in the previous exploration process.
Exploration Boundary
Estimated By Ours
Steps Visitation Counts
120k
200k
Figure 9: Visualization for the exploration boundary given by visitation counts and the estimated via
prediction errors of RND.
A.4 Additional results
Empty Room. We depict the exploration boundary by visitation counts and the estimated one by
our method in Figure 9. The agent starts from the left top corner and performs a random policy.
The exploration boundary shown in black is top 10% states with least visitation counts or the largest
prediction errors among all visited states.
SuperMarioBros. In Figure 10, we make additional trajectories visualization on SuperMarioBros-
1-1 and SuperMarioBros-1-2. Trajectories are plotted with same number samples (18M). Vanilla
method gets into the local optimum even with policy entropy regularization on SuperMarioBros-1-
1. In addition, only our method can get the flag on SuperMarioBros-1-2.
14
Under review as a conference paper at ICLR 2020
(a) SuperMarioBros-1-1. Agent starts from the most left part and needs to find the flag on the most right part.
(b) SUPerMarioBroS-1-2. The agent walks in the underworld shown in black and needs to get the flag through
the water pipe on the right part (see arrows).
Figure 10: Trajectory visualization. For each figure, toP row: vanilla (ACER); middle row: bonus;
bottom role: novelty-Pursuit (ours). The vanilla method gets stuck into the local oPtimum even
with Policy entroPy regularization on SuPerMarioBros-1-1. Only our method can get the flag on
SuPerMarioBros-1-2.
A.5 Environment prepossessing
Maze. Different from (Chevalier-Boisvert et al., 2018), we only use the image and coordination
information as inPuts. We only consider four actions: turn left, turn right, move forward and move
backward. The maximum ePisode length is 190 for EmPty Room, and 500 for Four Rooms. Each
time the agent receives a time penalty of 1 /max_episode_length and receives +1 when finding the
exit.
FetchReach. We implement this environment based on FetchReach-v0 in Gym (Brockman et al.,
2016). The maximum episode length is 50. The locations of four spheres are (1.20, 0.90, 0.65),
(1.10, 0.72, 0.45), (1.20, 0.50, 0.60), and (1.45, 0.50, 0.55). When sampling goals, we remove
spheres outside of the table i.e., the valid x range: (1.0, 1.5), the valid y range is (0.45, 1.05), and
valid z range is (0.45, 0.65).
SuperMarioBros. We implement this environment based on (Kauten, 2018) with Gym wrappers.
Prepossessing includes grey-scaling, observation downsampling, external reward clipping (except
that 50 for getting flag), stacked frames of 4, and sticky actions with a probability of 0.25. The
maximum episode length is 800. The environment restarts when the agent dies.
A.6 Network architecture
We use the convolutional neural network (CNN) for Empty Room, Four Rooms, and video games of
SuperMarioBros, and multi-layer perceptron (MLP) for FetchReach environment. Network archi-
tecture design and parameters are based on baselines (Dhariwal et al., 2017). For each environment,
15
Under review as a conference paper at ICLR 2020
RND uses a similar network architecture. The predictor network has additional MLP layers than the
predictor network.
A.7 Hyperparameters
Table 3 gives hyperparameters for ACER (Wang et al., 2017) on the maze and SuperMarioBros (the
learning algorithm is RMSProp (Tieleman & Hinton, 2012)). DDPG (Lillicrap et al., 2016) used
in Fetch Reach environments is based on the HER algorithm implemented in baselines (Dhariwal
et al., 2017) expect that the actor learning rate is 0.0005. We run 4 parallel environments for DDPG
and the size of the priority queue is also 100. As for the predictor network, the learning rate of
the predictor network is 0.0005 and the optimization algorithm is Adam (Kingma & Ba, 2015) for
all experiments, and the batch size of training data is equal to the product of rollout length and the
number of parallel environments.
The goal-conditioned policy is trained with shaping rewards defined in Equation 3 and external
rewards, which helps reduce mismatch behaviors between its and the exploitation policy’s . The
weight is 1 for all environments except 2 for SuperMarioBros. For bonus method used in Section
5, the weight β to balance the exploration bonus and the external reward (i.e., r0 = rext + βrint)
is 0.1 for Empty Room and Four Rooms, 0.01 for FetchReach, 1.0 for SuperMarioBros-1-1 and
SuperMarioBros-1-3, and 0.1 for SuperMarioBros-1-2. We also do a normalization for the intrinsic
reward by dividing the intrinsic rewards via a running estimate of the standard deviation of the sum
of discounted intrinsic rewards.
Table 3: Hyperparameters of our method based on ACER on the maze and SUPerMarioBros.
Hyperparameters	Empty Room	Four Rooms	SuperMarioBros
Rollout length	20	20	20
Number of parallel environments	4	4	8
Learning rate	0.0007	0.0007	0.00025
Learning rate schedule	linear	linear	constant
γ	0.95	0.95	0.95
Entropy coefficient	0.10	0.10	0.10
Size of priority queue	100	100	20
Total training steps	200K	500K	20M
16