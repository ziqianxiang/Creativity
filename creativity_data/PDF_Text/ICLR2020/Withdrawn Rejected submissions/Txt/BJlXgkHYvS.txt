Under review as a conference paper at ICLR 2020
Information-Theoretic Local Minima Charac-
terization and Regularization
Anonymous authors
Paper under double-blind review
Ab stract
Recent advances in deep learning theory have evoked the study of generalizabil-
ity across different local minima of deep neural networks (DNNs). While current
work focused on either discovering properties of good local minima or developing
regularization techniques to induce good local minima, no approach exists that
can tackle both problems. We achieve these two goals successfully in a unified
manner. Specifically, based on the Fisher information we propose a metric both
strongly indicative of generalizability of local minima and effectively applied as
a practical regularizer. We provide theoretical analysis including a generalization
bound and empirically demonstrate the success of our approach in both captur-
ing and improving the generalizability of DNNs. Experiments are performed on
CIFAR-10 and CIFAR-100 for various network architectures.
1	Introduction
Recently, there has been a surge in the interest of acquiring a theoretical understanding over deep
neural network’s behavior. Breakthroughs have been made in characterizing the optimization pro-
cess, showing that learning algorithms such as stochastic gradient descent (SGD) tend to end up in
one of the many local minima which have close-to-zero training loss (Choromanska et al., 2015;
Dauphin et al., 2014; Kawaguchi, 2016; Nguyen & Hein, 2018; Du et al., 2018). However, these
numerically similar local minima typically exhibit very different behaviors in terms of generaliz-
ability. It is, therefore, natural to ask two closely related questions: (a) What kind of local minima
can generalize better? (b) How to find those better local minima?
To our knowledge, existing work focused only on one of the two questions. For the “what” question,
various definitions of “flatness/sharpness” have been introduced and analyzed (Keskar et al., 2017;
Neyshabur et al., 2018; 2017; Wu et al., 2017; Liang et al., 2017). However, they suffer from one or
more of the problems: (1) being mostly theoretical with no or poor empirical evaluations on modern
neural networks, (2) lack of theoretical analysis and understanding, (3) in practice not applicable
to finding better local minima. Regarding the “how” question, existing approaches (Hochreiter &
Schmidhuber, 1997; Sokolic et al., 2017; Chaudhari et al., 2017; Hoffer et al., 2017; Neyshabur
et al., 2015a; Izmailov et al., 2018) share some of the common drawbacks: (1) derived only from
intuitions but no specific metrics provided to characterize local minima, (2) no or weak analysis of
such metrics, (3) not applicable or no consistent generalization improvement for modern DNNs.
In this paper, we tackle both the “what” and the “how” questions in a unified manner. Our an-
swer provides both the theory and applications for the generalization problems across different local
minima. Based on the determinant of Fisher information estimated from the training set, we pro-
pose a metric that solves all the aforementioned issues. The metric can well capture properties that
characterize local minima of different generalization ability. We provide its theoretical analysis,
primarily a generalization bound based on PAC-Bayes (McAllester, 1999b;a). For modern DNNs in
practice, it is necessary to provide a tractable approximation of our metric. We propose an intuitive
and efficient approximation to compare it across different local minima. Our empirical evaluations
fully illustrate the effectiveness of the metric as a strong indicator of local minima’s generalizability.
Moreover, from the metric we further derive and design a practical regularization technique that
guides the optimization process in finding better generalizable local minima. The experiments on
image classification datasets demonstrate that our approach gives consistent generalization boost for
a range of DNN architectures.
1
Under review as a conference paper at ICLR 2020
2	Related Work
It has been empirically shown that larger batch sizes lead to worse generalization (Keskar et al.,
2017). Hoffer et al. (2017) analyzed how the training dynamics is affected by different batch sizes
and presented a perturbed batch normalization technique for better generalization. While it effec-
tively improves generalization for large-batch training, a specific metric that indicates the general-
izability is missing. Similarly, Elsayed et al. (2018) employed a structured margin loss to improve
performance of DNNs w.r.t. noise and adversarial attack yet no metric was proposed. Furthermore,
this approach essentially provided no generalization gain in the normal training setup.
The local entropy of the loss landscape was proposed to measure “flatness” in Chaudhari et al.
(2017), which also designed an entropy-guided SGD that achieves faster convergence in training
DNNs. However, the method does not consistently improve generalization, e.g., a decrease of per-
formance on CIFAR-10 (Krizhevsky & Hinton, 2009). Another method that focused on modifying
the optimization process is the Path-SGD proposed by Neyshabur et al. (2015a). Specifically, the
authors derived an approximate steepest descent algorithm that utilizes the path-wise norm regular-
ization to achieve better generalization. The authors only evaluated it on a two-layer neural network,
very likely since the path norm is computationally expensive to optimize during training.
A flat minimum search algorithm was proposed by Hochreiter & Schmidhuber (1997) based on the
“flatness” of local minima defined as the volume of local boxes. Yet since the boxes have their axes
aligned to the axes of the model parameters, their volumes could be significant underestimations of
“flatness” for over-parametrized networks, due to the specific spectral density of Hessian of DNNs
studied in Pennington & Worah (2018); Sagun et al. (2018). The authors of Wu et al. (2017) also
characterized the “flatness” by volumes. They considered the inverse volume of the basin of attrac-
tion and proposed to use the Frobenius norm of Hessian at the local minimum as a metric. In our
experiments, we show that their metric does not accurately capture the generalization ability of local
minima under different scenarios. Moreover, they have not derived a regularizer from their metric.
Based on a “robustness” metric, Sokolic et al. (2017) derived a regularization technique that SUc-
cessfully improves generalization on multiple image classification datasets. Nevertheless, we show
that their metric fails to capture the generalizability across different local minima.
By using the Bayes factor, MacKay (1992) studied the generalization ability of different local min-
ima obtained by varying the coefficient of L2 regularization. It derived a formula involving the
determinant of Hessian, similar to the one in ours. Whereas, this approach has restricted settings
and, without proposing an efficient approximation, its metric is not applicable to modern DNNs, let
alone serving as a regularizer. A generalization bound is missing in MacKay (1992) as well.
In a broader context of the “what” question, properties that capture the generalization of neural net-
works have been extensively studied. Various complexity measures for DNNs have been proposed
based on norm, margin, Lipschitz constant, compression and robustness (Bartlett & Mendelson,
2002; NeyShabur et al., 2015b; Sokolic et al., 2017; XU & Mannor, 2012; Bartlett et al., 2017; ZhoU
et al., 2019; Dziugaite & Roy, 2017; Arora et al., 2018; Jiang et al., 2019). While some of them
aimed to provide tight generalization bounds and some of them to provide better empirical results,
none of the above approaches explored the “how” question at the same time.
Very recently, Karakida et al. (2019) and Sun & Nielsen (2019) studied the Fisher information of
the neural network through the lens of its spectral density. In specific, Karakida et al. (2019) applied
mean field theory to study the statistics of the spectrum and the appropriate size of the learning
rate. Also an information-theoretic approach, Sun & Nielsen (2019) derived a novel formulation
of the minimum description length in the context of deep learning by utilizing tools from singular
semi-Riemannian geometry.
3	Outline and Notations
In a typical K-way classification setting, each sample x ∈ X belongs to a single class denoted
cx ∈ {0, 1, ..., K} according to the probability vector y ∈ Y, where Y is the k-dimensional
probability simplex so that p(cx = i) = yi and Pi yi = 1. Denote a feed-forward DNN
parametrized by w ∈ RW as fw : X → Y , which uses nonlinear activation functions and a
softmax layer at the end. Denote the cross entropy loss as '(fw(χ), y) = - Pi yi ln fw(x)i. De-
2
Under review as a conference paper at ICLR 2020
note the training set as S, defined over X × Y with |S |= N . The training objective is given as
L(S, W) = -N P(X y)〜S '(fw (x), y). Assume S is sampled from some true data distribution de-
noted D, we can define expected loss L(D, W) = E(χ,y)〜D['(fw (x),y)]. Throughout this paper, we
refer a local minimum of L(S, w) corresponding to a local minimizer w0 as just the local minimum
W0. Given such W0, our paper’s outline as well as our main achievements are:
•	In Section 4 we relates Fisher information to neural network training as a prerequisite.
•	In Section 5.1 we propose ametricγ(W0) that well captures local minima’s generalizability.
•	In Section 5.2 we provide a generalization bound related to γ(W0).
•	In Section 5.3 we propose an approximation γb(W0) for γ(W0), which is shown to be very
effective in Section 7.1 via extensive empirical evaluations.
•	In Section 6 we devise a practical regularizer from γ(W0) that consistently improves gen-
eralizability across different DNNs, as evaluated in Section 7.2.
3.1 Other Notations
Denote Vw as gradient, JW [∙] as Jacobian matrix, V^ as Hessian, Dkl (∙∣∣∙) as KL divergence, k∙∣∣2
as spectrum or Euclidean norm, ∣∣∙kf as Frobenius norm, ∣∙∣ as determinant, tr(∙) as trace norm, ρ(∙)
as spectral radius, ''s (W) as log-likelihood on S, and Hi for selecting the ith entry.
We define 'x(w) ∈ RK whose ith entry is 一 ln fw(x)i so that '(fw(χ),y) = 'x(w)τy. We define y
as argmax(y) and y ∈ RK the one-hot vector whose y-th entry is 1 and otherwise 0. Then We define
L(S, W) ∈ RN as the “simplified” loss vector of S whose entries are '(fw(x), y) for (x, y) ∈ S,
i.e., we approximate the cross entropy loss '(fw(x), y) by '(fw(x), y).
4 Local Minimum and Fisher Information
First of all, if y is strictly one-hot, no local minimum will even exist with 100% training accuracy,
since the cross entropy loss will always be positive. To admit good local minima in the first place,
we assume the widely used label smoothing (Szegedy et al., 2016) is applied to train all models in
our analysis. Label smoothing enables us to assume a local minimum W0 (in this case, also a global
minimum) of the training loss with P(x,y)∈S DKL(fw0 (x)ky) = 0.
Each sample (x, y) ∈ S has its label CX sampled by p(cχ = i|x) = y%, denoted as (x, Cx)〜 S.
The joint probability p(x, cx ) modeled by the DNN is p(x, cx = i; W) = p(cx = i|x; W) p(x) =
[fw (x)]i p(x) with p(x) = Nn. We can relate the training loss L(S,w) to the negative log-likelihood
-''s(W) = 一 P(x,y)∈s Ecx〜y lnP(X, cx；W) by:
L(S, W)
N X 'χ(W)τy
(X,y)∈S
一N x C%inp(cx|x；W)=一N''S(W)+lnN
(X,y)∈S x y
And so W0 also corresponds to a local maximum of the likelihood function. The observed Fisher
information evaluated at wo is defined as I (wo)= 一 ! VW ''s (wo). We can further derive:
I(W0) = V1 2 * * 5wL(S, W0) = E	[Vw lnp(Cx|x; W0)Vw lnp(Cx|x; W0)τ]	(1)
(x,Cx)〜S
The first equality is straightforward; the second has its proof in Appendix A. Since p(Cx = i|x) = yi
and lnp(cχ = i|x; wo) = ['χ(wo)]i, we can further simplify the Equation 1 to:
(2)
1K	τ
I(WO) = N X X (VW ['x(wθ)]i) (Vw ['x(wθ)]i)	I
(x,y)∈S i=1
Remark: When we assume global optimality, we have Vw'(fwo(x), y) = 0 as DKL(fw0 (x)ky)
0; yet it does not indicate I(Wo) ∈ RW×W 6= 0 in Equation 2.
5 Local Minima Characterization
In this section, we derive and propose our metric, provide a PAC-Bayes generalization bound, and
lastly, propose and give intuitions of an effective approximation of our metric for modern DNNs.
3
Under review as a conference paper at ICLR 2020
5.1	Fisher Determinant as Generalization Metric
We would like a metric to compare different local minima. Similar to the various definitions of
“flatness/sharpness”, we take a small neighborhood of the target local minimum w0 into account.
Formally for a sufficiently small V , we define the model class M(w0) as the largest connected
subset of {w ∈ RW : L(S, w) ≤ h} that contains w0, where the height h is defined as a real
number such that the volume (namely the Lebesgue measure) of M(w0) is V . By the Intermediate
Value Theorem, for any sufficiently small volume V there exists a corresponding height h.
We propose our metric γ(∙), where lower γ(wo) indicates a better local minimum w°:
γ(w0) = ln|I(w0)|	(3)
As a metric, γ(w0) requires |I(w0)| 6= 0. Therefore, we state the following Assumption 1.
Assumption 1. The local minima w0 we care about in the comparison are well isolated and unique
in their corresponding neighborhood M(w0).
The Assumption 1 is quite reasonable. For state-of-the-art network architectures used in practice,
this is often the fact. To be precise, the Assumption 1 is violated when the Hessian matrix at a
local minimum is singular. Specifically, Orhan & Pitkow (2018) summarizes three sources of the
singularity: (i) due to a dead neuron, (ii) due to identical neurons, and (iii) linear dependence of
the neurons. As well demonstrated in Orhan & Pitkow (2018), network with skip connection, e.g.
ResNet (He et al., 2016a), WRN (Zagoruyko & Komodakis, 2016), and DenseNet (Huang et al.,
2017) used in our experiments, can effectively eliminate all the aforementioned singularity.
In Dinh et al. (2017), the authors pointed out another source of the singularity specifically for net-
works with scale-invariant activation functions, e.g. ReLU, referred as the rescaling issue. Namely,
one can rescales the model parameters layer-wise so that the underlying function represented by
the network remains unchanged in the region. In practice, this issue is not critical. Firstly, most
modern deep ReLU networks, e.g. ResNet, WRN, and DenseNet, have normalization layers, e.g.
BatchNorm (Ioffe & Szegedy, 2015), applied before the activations. BatchNorm shifts all the inputs
to the ReLU function, equivalently shifting the ReLU horizontally which makes it no longer scale-
invariant. Secondly, due to the ubiquitous use of Gaussian weights initialization scheme and weight
decay, most local minima obtained by gradient learning have weights of a relatively small norm.
Consequently, in practice, we will not compare two local minima essentially the same but have one
as the rescaled version of the other with a much larger norm of the weights.
Note that normally we have a limited size of the dataset, and so an approximation of γ(w0) is a
must. We present our approximation scheme and its intuition in Section 5.3.
5.1.1	Connection to Fisher Information Approximation (FIA) Criterion
Our metric γ(w0) is closely related to the FIA criterion. From Information Theory, the MDL prin-
ciple suggests that among different statistical models the best is the one that best compresses both
the sampled data and the model (Rissanen, 1978). Accordingly, Rissanen (1996) derived the FIA
criterion to compare statistical models, each of which is a class of model in the neighborhood of a
global minimum w0. The model class’s FIA criterion is written as (lower FIA is better):
W1 N	j
FIA = - T E lnp(x,cx； wo) 十3 1口2-+ln /	√∣Iω | dw
(x,y)∈S Cx〜y	2	2π	M(WO)
On the right hand side, the first two terms are both constants. To see the connection to our metric,
we replace the expected Fisher information Iw with the tractable observed one I(w0). Assuming
the training loss is locally quadratic in M(w0), an assumption later formalized and validated as
Assumption 2, since I(w0) = RIwL(S, w0) from Equation 1, We can modify the last term to be
ln V + ln p|I(wo)|.
Remark: Although in a similar format, the FIA criterion and our metric are essentially different due
to the appearance of observed Fisher information in place of the expected one, making our metric
both tractable and much more applicable (no longer requires global optimality).
5.1.2	Connection to Existing Flatness/Sharpness Metrics
4
Under review as a conference paper at ICLR 2020
As mentioned in Section 2, the “flatness” of a local minimum was firstly related to the generaliza-
tion ability of the neural network in Hochreiter & Schmidhuber (1997), where the concept and the
method are both preliminary. The idea is recently popularized in the context of deep learning by a
series of paper such as Keskar et al. (2017); Chaudhari et al. (2017); Wu et al. (2017). Our approach
roughly shares the same intuition with these existing works, namely, a “flat” local minimum admits
less complexity and so generalizes better than a “sharp” one. To our best knowledge, our paper
is the first among these work that provides both the theoretical analysis including a generalization
bound and the empirical verification of both an efficient metric and a practical regularizer for modern
network architectures.
5.2	Generalization B ound
Assumption 2. Given the training loss L(S, w), its local minimum w0 satisfying Assumption 1 and
the associated neighborhood M(w0) whose volume V is sufficiently small, as described in Section
3, 4 and 5.1, respectively, when confined to M(w0), we assume that L(S, w) is quadratic.
The Assumption 2 is quite reasonable as well. GriinWald & GrUnWald (2007) suggests that, a log-
likelihood function, under regularity conditions (1) existence of its 1st, 2nd & 3rd derivatives and
(2) uniqueness of its maximum in the region, behaves locally like a quadratic function around its
maximum. In our case, L(S, W) corresponds to the log-likelihood function ''s(W) and so w° corre-
sponds to a local maximum of ''s(w). Since L(S, W) is analytic and wo is the only local minimum
of L(S, W) in M(W0), the training loss indeed can be considered locally quadratic.
Similar to Langford & Caruana (2002), Harvey et al. (2017) and Neyshabur et al. (2017), We apply
the PAC-Bayes Theorem (McAllester, 2003) to derive a generalization bound for our metric. Specif-
ically, We pick a uniform prior P over W ∈ M(W0) according to the maximum entropy principle
and after observing the training data S pick the posterior Q of density q(w) 8 e-|L(S,wO)-L(S,W)I
Then Theorem 1 bounds the expected L(D, W) using γ(W0). See its proof in Appendix B.
Theorem 1. Given |S|= N, D, L(S, W) and L(D, W) described in Section 3, a local minimum W0
with L0 =∆ L(S, W0), the volume V of M(W0) sufficiently small, the Assumption 1 & 2 satisfied,
and P, Q defined above, for any δ ∈ (0, 1], we have with probability at least 1 - δ that:
2Lo + 2A +ln 2N	A	WV等π&e，(WO)/W
E [L(D, w)] ≤ Lo + A + 2∖	0 N I-J, A =-----------------------------
W〜Q	N N — 1	4πe
In short, Theorem 1 shoWs that a loWer γ(Wo) indicates a more generalizable local minimum Wo.
5.3	Approximation
As stated in Section 4, in practice an approximation of γ(Wo) as γb(Wo) is necessary, as calculating
γ(Wo) involves computing the determinant of a W × W matrix. Let us first assume We have an
imagined training set S0 of size W, a local minimum Wo of L(S0, W) and so correspondingly a full-
rank observed Fisher information matrix I0(Wo) so that ln|I0(Wo)| is Well defined. In reality, We
only have a training set S ⊂ S0 With a singular I(Wo). Notice that Wo is also a local minimum
of L(S, W) since P(x,y)∈S0 DKL(fW0 (x)ky) = 0 as assumed in Section 4. We then approximate
eigenvalues ofI0(Wo) by those of its sub-matrices and so to approximate ln|I0(Wo)|.
First of all, we replace y by its one-hot version y defined in Section 3.1 since they are very close.
This drastically reduces the cost of gradient calculation. With L(S, w) ∈ RN and y defined in
Section 3.1, according to Equation 2, the observed Fisher information I0(Wo) ∈ RW×W is:
10(wo) ≈ WW X (VW ['χ(wo)]y)(Vw ['x(wo)]y)T
(x,y)∈S0
=WWJw[L(S0,w)]TJw[L(S0,w)] = WWJw[L(S0,w)] Jw[L(S0,w)]T	(4)
Let {λm}mW=1 denote the eigenvalues ofI0(Wo); then γ(Wo) = ln QmW=1 λm = PmW=1lnλm.
Without calculating all W eigenvalues, We can perform a Monte-Carlo estimation of γ(Wo) by
5
Under review as a conference paper at ICLR 2020
randomly sampling N0 < N < W eigenvalues from {λm}mW=1. We denote the samples as {λn}nN=0 1
and We have W PN=Iln λn ≈ PW=Iln λm . Suppose the estimation is run T times, we have
limT→∞ T PT=I NW0 PN=Iln λn = Y(WO).
In practice {λn}nN=01 is inaccessible since we don’t haveI0(WO) in the first place. Instead, we sample
St ⊂ S with ISt I= N0 for T times and define
ξt(WO) = Jw[L(St,WO)]Jw[L(St,WO)]T ∈ RN0×N0
Notice that ξt(WO) is a principal sub-matrix of WI0(WO) by removing rows & columns for data in
S \ St. According to Theorem 3, one can roughly estimate the size of eigenvalues of a matrix by
those of its sub-matrices. Therefore we propose to estimate γ(WO) by γb(WO) with:
T
b(WO) = T Xln lξt(WO) l,	Y(WO) ≈ Nb(WO) + Wln W as T → ∞
t=1
(5)
We leave Theorem 3 as well as the derivation of Equation 5 to Appendix C. In proposing γb(w0), we
ignore the constants and irrelevant scaling factors because What matters is the relative size of γ(∙)
when comparing different local minima. Empirically we find that given relatively large number of
sample trials T, our metric b(∙) can effectively capture the generalizability of a local minimum even
for a small N0 (details in Section 7.1 and in Appendix D).
6 Local Minima Regularization
Besides pragmatism, devising a practical regularizer based on γ(w0) also “verifies” our theoretical
understanding of DNN training, helping for future improvement of the learning algorithms. How-
ever, converting γ(w0) to a practical regularizer is non-trivial due to the computation burden of:
1.	optimizing terms related to the gradient, Which involves calculating the Hessian
2.	computing the eigenvalues in each training step, Which is even more expensive
We first solve the second issue and then the first one. To solve the second issue, we propose to
optimize a surrogate term for γ(WO) which avoids eigenvalue computations, namely the trace norm
of the observed Fisher information tr(I(WO)). These two terms have the relation:
WWγ(wo) = WlnIZ(Wo)| ≤ lntr(I(wo)) - ln W
Another major benefit of using the trace norm is that, unlike γ(WO), tr(I(WO)) still remains well
defined even with a small training set IS I= N. From Equation 2 we have:
1K
tr(I (wo)) = N E EkVw ['x(wo )]i∣2
(x,y)∈S i=1
The cost of computing tr(I(WO)) is linear in the number of its terms (in the double summation). We
therefore simplify the calculation by replacing y with y similar to Equation 4 so that
1	1N
tr(I(WO))≈N E kVw'(fw0(χ),y)k2 = NEkVw[L(s,Wo)]i∣∣2
(x,y)∈S	i=1
where y and L(∙, ∙) are defined in Section 3.1. As in gradient-based training we never exactly reach
the local minimum w0, We choose to optimize tr(I(w)) during the entire training process. We have
Vw由 Pi kVw[L3(B, wo)]ik2 =由 Pi Vw∣∣Vw[L(B,wo)]i∣∣2 for each mini-batch B. Then we
can further reduce the computation cost by batching. In specific, we randomly split B into M sub-
batches of equal size, namely {Bi}M1. We define gi = BJ P(x,y)∈Bj(fwo (x),y) and compute
∣∣gik2 for gi ∈ {gi}M=ι instead of computing ∣ [L(B, wo)]i∣∣2 for each data point in B.
6
Under review as a conference paper at ICLR 2020
We deal with the first computation burden by adopting first order approximation. For any w, with a
sufficiently small α > 0 we have L(Bi, w - αgi) ≈ L(Bi, w) - Jw [L(Bi, w)] αgi. Then
1	|Bi|	1	|Bi|
而 X[L( B∙i,W)-L( Bi,w - αgi)]j	≈ 而 X	Jw [L( B,W)]	αgi[	= αkgik2
Therefore, we propose to optimize the following regularized training objective for each update step:
L(B, W) + βRα(W),
Rα (W) =∆
1 M 1	|Bi|
M X 两 X 仅(Bi, W)- LL( Bi,W - agi)]
i=1 |Bi| j=1
(6)
j
We omit any second order term When computing VwRa(W), simply by no back-prop through gi.
On the other hand, we find that gradient clipping, especially at the beginning of the training, is
necessary to make the generalization boost consistent. We have 4 hyper-parameters: α, β, the
number of sub-batches M and the gradient clip threshold τ . Our approach is formalized as:
Algorithm 1 Regularized Mini-batch Learning (Single Update Step)
1	： procedure UPDATE(W, B; α, β, M, τ )	.Last 4 are hyper-parameters
2	{Bi}i=ι 一 B	. Split the mini-batch B into M sub-batches
3	for i — 1 to M do	
4	gi 一 1⅛ P(x,y)∈ Bif (x),y)	. Compute the gradient of the sub-batch
5	：	gi = copy(gi)	. A copy prevents gradient flow
6	： end for	
7	：	Compute Rα(W) by Equation 6	. Use the copied version of gi
8	VwLreg J Vw [L(B,W) + βRɑ(w)]	
9	：	Clip Vw Lreg with threshold τ	. clip_by_global_norm in TensorFlow
10	：	Gradient update with clipped Vw Lreg	. Update with any gradient-based optimizer
11	： end procedure	
7	Experiments
We perform tWo sets of experiments to illustrate the effectiveness of our metric γ(W0). We demon-
strate that: (1) the approximation γb(W0) captures the generalizability Well across local minima; (2)
our regularization technique based on γ(W0) provides consistent generalization gain for DNNs.
Throughout our theoretical analysis, We assume that label smoothing (LS) is applied during model
training in order to obtain Well-defined local minima (first mentioned in Section 4). In all our
empirical evaluations, We perform both the version With LS applied and Without. Results are very
similar and so We stick to the version Without LS since this is the same as the original training setup
in papers of the various netWork architectures that We used.
7.1	Experiments on Local Minima Characterization
We perform comprehensive evaluations to compare our metric γ(∙) with several others on ResNet-
20 (He et al., 2016a) for the CIFAR-10 dataset (architecture details in Appendix E). Our metric
consistently outperforms others in indicating local minima,s generalizability. Specifically, Sokolic
et al. (2017) proposed a robustness-based metric used as a regularizer; Wu et al. (2017) proposed
to use Frobenius norm of the Hessian as a metric; Keskar et al. (2017) proposed a metric closely
related to the spectral radius of Hessian. In summary, we compare 4 metrics, all evaluated at a local
minimum W given S . All four metrics go for “smaller values indicate better generalization”.
•	Robustness： N P(x,y)∈s IIJx[fw(X)]k2
•	Frobenius norm: kV2wL(S, W)k2F
•	Spectral radius： ρ(V2wL(S, W))
•	Ours： b(W) = T PT=1 ln∣ξ(St, w°)|, St ⊂ S
7
Under review as a conference paper at ICLR 2020
Both the Frobenius norm and the spectral radius based metric are related to ours, as from Equation 1
we have ∣∣VWL(S,w)kF = ∣∣I(W)IIF and ρ(VWL(S,w)) = P(I(w)). These two metric, however,
are too expensive to compute for the entire training set S ; we instead calculate them by averaging
the results for T sampled St ⊂ S, similar to when we compute γb(w). We leave details of how we
exactly compute these metrics in our experiments to Appendix D.
We perform evaluations in three scenarios, similar to Neyshabur et al. (2017); Keskar et al. (2017).
We examine different local minima due to (1) a confusion set of varying size in training, (2) different
data augmentation schemes, and (3) different batch size. In specific,
•	In Scenario I, we randomly select a subset of 10000 images as the training set and train the
DNN with a confusion set consisting of CIFAR-10 samples with random labels. We vary
the size of the confusion set so that the resulting local minima generalize differently to the
test set while all remain close-to-zero training losses. We consider confusion size of 0, 1k,
2k, 3k, 4k and 5k. We calculate all metrics based on the sampled 10000 training images.
•	In Scenario II, we vary the level of data augmentation. We apply horizontal flipping, de-
noted flip-only, random cropping from images with 1 pixel padded each side plus
flipping, denoted 1-crop-f, random cropping with 4 pixels padded each side plus flip-
ping, denoted 4-crop-f and no data augmentation at all, denoted no-aug. Under all
schemes, the network achieves perfect training accuracy. All the metrics are computed on
the un-augmented training set.
•	In Scenario III, we vary the batch size. Hoffer et al. (2017) suggests that large batch size
leads to poor generalization. We consider the batch size to be 128, 256, 512 and 1024.
The default values for the 3 variables are confusion size 0, 4-crop-f and batch size 128. For each
configuration in each scenario, we train 5 models and report results (average & standard deviations)
of all metrics as well as the test errors (in percentage). For the confusion set experiments, we sample
a new training set and a new confusion set every time. In all scenarios, we train the model for 200
epochs with an initial learning rate 0.1, divided by 10 whenever the training loss plateaus. Within
each scenario, we find the final training loss very small and very similar across different models and
the training accuracy essentially equal to 1, indicating the convergence to local minima.
The results are in Figure 1, 2 and 3 for Scenario I, II and III, respectively. Our metric significantly
outperforms others and is very effective in capturing the generalization properties, i.e., a lower value
of our metric consistently indicates a better generalizable local minimum.
7.2	Experiments on Local Minima Regularization
We evaluate our regularizer on CIFAR-10 & CIFAR-100 for four different network architectures
including a plain CNN, ResNet-20, Wide ResNet (Zagoruyko & Komodakis, 2016) and DenseNet
(Huang et al., 2017). We use WRN-28-2-B(3,3) from the Wide ResNet paper and the DenseNet-
BC-k=12 from the DensetNet paper. See Appendix E for further architecture details. We denote the
four networks as CNN, ResNet, WRN and DenseNet, respectively.
We manually set α = 0.0001 in all experiments and select the other three hyper-parameters in
Algorithm 1 by validation via a 45k/5k training data split for each of the network architecture on
each dataset. In specific, we consider β ∈ {10, 20, 30, 40, 50, 75, 100, 125}, M ∈ {4, 8, 16} and
τ ∈ {1, 5, 10, 15}. We keep all the other training hyper-parameters, schemes as well as the setup
identical to those in their original paper (details in Appendix E). The training details of the plain
CNN are also in Appendix E. We train 5 separate models for each network-dataset combination and
report the test errors in percentage (mean ± std.) in Table 1, where “+reg” indicates training with
our regularizer applied. The results demonstrate that our method provides consistent generalization
improvement to a wide range of DNNs.
7.2.1	The Choice of The Optimizer
As described in Algorithm 1, our proposed regularizer is not tied to a specific optimizer. We perform
experiments with SGD+Momentum because it is chosen to be used in ResNet, WRN, and DenseNet,
helping all of them achieve current or previous state-of-the-art results. Our regularizer aims to find
better “flatter” minima to improve generalization whereas adaptive optimization methods such as
8
Under review as a conference paper at ICLR 2020
Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al., 2011) try to boost up convergence, yet
usually at the cost of generalizability. Recent works (Wilson et al., 2017; Keskar & Socher, 2017)
show that adaptive methods generalize worse than SGD+Momentum. In specific, very similar to our
setup, Keskar & Socher (2017) demonstrates that SGD+Momentum consistently outperforms the
others on ResNet and DenseNet for CIFAR-10 and CIFAR-100. Other approaches that also utilize
local curvature to improve SGD, such as the Entropy-SGD (Chaudhari et al., 2017) mentioned in
Section 2, have empirical results rather preliminary compared to ours.
7.2.2	Generalization Boost As a Result of B etter Local Minima
Our regularizer essentially optimizes an upper bound of the proposed metric during training. We
perform a sanity check to illustrate that the regularizer indeed induces better local minima charac-
terized by our metric. For ResNet, Wide-ResNet and DenseNet trained on CIFAR-10, we compute
the metric on local minima of similar training loss obtained with or without applying the regularizer.
Table 2 shows that the resulting generalization boost aligns with what captured by our metric.
Figure 1: Scenario I: varied size of the confusion set. 5 models are trained for each size of the
confusion set (x-axis). Solid lines are the average result; shaded areas represent the ± 1 standard
deviation (same for Figure 2 and 3). A larger confusion set leads to a higher test error, a trend well
captured by our metric and the other two; the robustness based metric fails.
Figure 2: Scenario II: varied data augmentation schemes. Four different schemes are used. Our
metric works well as an indicator of the test error while all the other metrics completely fail.
Figure 3: Scenario III: varied batch size. Large batch size leads to poor generalization, captured by
all the metrics except for the robustness based one.
Table 1: Test error (%) for CIFAR-10 (1st row) and CIFAR-100 (2nd row). In general, a model with
more parameters admits more space for our regularizer. The representation power of ResNet-20 is
too limited for CIFAR-100 (resulting in poor convergence); so we ignore it in our experiments.
CNN	CNN+reg	WRN	WRN+reg	I DenSeNet	DenSeNet+reg ∣	ReSNet	ReSNet+reg
8.52 ± 0.08	7.46 ± 0.13	5.44 ± 0.04	4.80 ± 0.10	4.61 ± 0.08	4.31 ± 0.08	8.50 ± 0.31	7.91 ± 0.25
31.12 ± 0.35	29.19 ± 0.21	25.52 ± 0.15	23.65 ± 0.14	22.54 ± 0.32	22.19 ± 0.28	-	-
9
Under review as a conference paper at ICLR 2020
Table 2: The proposed metric computed on local minima obained with or without applying the
proposed regularizer. Each entry represents mean ± std. among 5 runs. Smaller values are bolded.
	ResNet	WRN	DenseNet
	 w/o reg.	-979.3 ± 22.3	-737.6 ± 20.3	-850.3 ± 23.5
with reg.	-1138.1 ± 11.0	-804.8 ± 18.7	-886.2 ± 20.5
8	Conclusion and Future Work
In this paper, we show a bridge between the field of deep learning theory and regularization meth-
ods with respect to the generalizability of local minima. We propose a metric that captures the
generalization properties of different local minima and provide its theoretical analysis including a
generalization bound. We further derive an efficient approximation of the metric and devise a prac-
tical and effective regularizer from it. Empirical results demonstrate our success in both capturing
and improving the generalizability of DNNs. Our exploration promises a direction for future work
on the regularization and optimization of DNNs.
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pp.
254-263, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradi-
ent descent into wide valleys. In International Conference on Learning Representations, 2017.
URL https://openreview.net/forum?id=B1YfAfcgl.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1019-1028. JMLR. org, 2017.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. In Advances in Neural Information Processing Systems,
pp. 842-852, 2018.
Peter D Grunwald and Abhijit Grunwald. The minimum description length principle. MIT press,
2007.
10
Under review as a conference paper at ICLR 2020
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piece-
wise linear neural networks. In Conference on Learning Theory, pp. 1064-1068, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-
son. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap
in deep networks with margin distributions. In International Conference on Learning Represen-
tations, 2019. URL https://openreview.net/forum?id=HJlQfnCqKX.
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of fisher information in
deep neural networks: Mean field approach. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 1032-1041, 2019.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing systems, pp. 586-594, 2016.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
International Conference on Learning Representations, 2017. URL https://openreview.
net/forum?id=H1oyRlYgg.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
John Langford and Rich Caruana. (not) bounding the true error. In Advances in Neural Information
Processing Systems, pp. 809-816, 2002.
Chen-Yu Lee, Patrick W Gallagher, and Zhuowen Tu. Generalizing pooling functions in convo-
lutional neural networks: Mixed, gated, and tree. In Artificial Intelligence and Statistics, pp.
464-472, 2016.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geome-
try, and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-
tation, 4(3):448-472, 1992.
11
Under review as a conference paper at ICLR 2020
David McAllester. Simplified pac-bayesian margin bounds. In Learning theory and Kernel ma-
chines, pp. 203-215. Springer, 2003.
David A McAllester. Pac-bayesian model averaging. In COLT, volume 99, pp. 164-170. Citeseer,
1999a.
David A McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355-363, 1999b.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422-2430, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=Skz_WfbCZ.
Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In
International Conference on Machine Learning, pp. 3727-3736, 2018.
Emin Orhan and Xaq Pitkow. Skip connections eliminate singularities. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
HkwBEMWCZ.
Jeffrey Pennington and Pratik Worah. The spectrum of the fisher information matrix of a single-
hidden-layer neural network. In Advances in Neural Information Processing Systems, pp. 5410-
5419, 2018.
Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465-471, 1978.
Jorma J Rissanen. Fisher information and stochastic complexity. IEEE transactions on information
theory, 42(1):40-47, 1996.
Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of
the hessian of over-parametrized neural networks, 2018. URL https://openreview.net/
forum?id=rJrTwxbCb.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep
neural networks. IEEE Transactions on Signal Processing, 65(16):4265-4280, 2017.
Ke Sun and Frank Nielsen. Lightlike neuromanifolds, occam’s razor and deep learning. arXiv
preprint arXiv:1905.11027, 2019.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective
of loss landscapes. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70. JMLR. org, 2017.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,
2012.
12
Under review as a conference paper at ICLR 2020
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard
C. Wilson and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
(BMVC),pp. 87.1-87.12. BMVA Press, SePtember 2016. ISBN 1-901725-59-6. doi: 10.5244/C.
30.87. URL https://dx.doi.org/10.5244/C.30.87.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous
generalization bounds at the imagenet scale: a PAC-bayesian compression approach. In Interna-
tional Conference on Learning Representations, 2019. URL https://openreview.net/
forum?id=BJgqqsAct7.
A Proof of Equation 1
To prove the second equality in Equation 1, it suffices to prove the following equality:
K
-Vw'`s (W)= E Eyi[Vw lnp(cχ = i|x; w)Vw lnp(cχ = i|x; W)T]
(x,y)∈S i=1
For convenience, we change the notation of the local minimum from W0 to W and further denote
p(cχ = i|x； W) as Pw (i). Since-Vw''s(W) = — P(χ,y)∈s PK=I y Vw lnPw (i), for each (x,y) ∈
S and i ∈ {1, 2, ..., K}, we have:
∂2
[Vw lnPw (i)]j,k =5―ln—lnPw ⑶
∂Wj∂Wk
=_d_ (~∂wkPw ⑶'
dwΛ Pw ⑶,
Pw⑺∂wfcPw⑺ ∂jPw⑺ ∂wkPw(i)
-----1  --- - -:~:---1~: 
Pw⑴2	Pw⑴	Pw⑴
∂wj ∂wk Pw(i)
Pw⑶
∂∂
-西lnPw(i) ∙ ∂WklnPw(i)
Since W0 is a local minimum (also a global minimum) described in Section 4 as yi = Pxw (i) for
i = 1, 2, ..., K, when taking the double summation, the first term above becomes:
K	∂2	∂2	K	∂2
X X ∂wj∂wkPw ⑶=∂wj∂wk X XPW⑶=∂wj∂wk N = 0
x,y)∈S i=1	(x,y)∈S i=1
Then it follows that:
K
[Vw''s(W)j,k = - X X yi[VwlnPw ⑶ VwlnPw ⑶Tj,k
(x,y )∈S i=1
B Proof of the Generalization B ound in Section 5.2
First let us review the PAC-Bayes Theorem in McAllester (2003):
Theorem 2. For any data distribution D and a loss function L(∙, ∙) ∈ [0,1], let L(D,w) and
L(S, W) be the expected loss and training loss respectively for the model paramterized by W, with
the training set |S |= N. For any prior distribution P with a model class C as its support, any
posterior distribution Q over C (not necessarily Bayesian posterior), and for any δ ∈ (0, 1], we
have with probability at least 1 - δ that:
E [L(D, W)] ≤
w〜Q
E [L(S, W)] + 2
w〜Q	V
2Dkl(Q∣∣P)+ln 2N
N-1
13
Under review as a conference paper at ICLR 2020
As eγ(w0) = |I(w0)|, we can rewrite the generalization bound we want to prove in Section 5.2 as:
E [L(D, w)] ≤L0+
W〜Q
+2
W ∙ V2/Wπ^WII(WO/W
4πe
W ∙ V2/Wn1/W∣I(wo)|1/W + 4πeLo + 2πeln 2N
2πe(N - 1)
(7)
As defined in Section 5.2, given the model class M(w0), whose volume is V, for the neural network
fw, the uniform prior P attains the probability density function P(W) = v1 for any W ∈ M(w0)
and the posterior Q has density q(w) 8 e-|L(S,W)-L0|. Based on Assumption 2 and the observed
Fisher information I(W0) derived in Section 4, especially the Equation 2, we have:
L(S, W) = Lo + ɪ(w — wo)TI(wo)(w — wo) ∀w ∈ M(w0)
Denote Σ = [I(wo)]-1 = [VWL(S, wo)]-1. Then Q is a truncated multivariate Gaussian distribu-
tion whose density function q is:
q(W; Wo, Σ)
,(2π)-n∣∑∣τ exp{- 1 (W 一 WO)TΣ-1(w 一 wo)}
Rm(w0) p(2π)-n园T exp{-2(W - WO)TςT(W - wo)} dW
exp{-1 (w 一 WO)TΣ-1(w 一 wo)}
Rm(w0 ) exP{- 2 (W 一 wo )T ςT(W - wo)} dw
(8)
Denote the denominator of Equation 8 as Z and define:
g(w; Wo, ∑) = ―2(w ― WO)T∑-1(w - Wo)} ≤ 0
Then q can also be written as:
q(w; wo, ∑) = exp{g* Wo,.
Z
In order to derive a generalization bound in the form of the PAC-Bayes Theorem, it suffices to prove
an upper bound of the KL divergence term:
DKL(Q||P)
E In萼
W〜Q	P(w)
一E ln VV + E ln q(w)
ln V + E g(w; wo, Σ)+ln1
W〜Q	Z
≤
ln V + E 0 一 ln
W〜Q
M(W0)
exp{g(W; Wo, Σ)} dW
≤
ln V 一 In (/	exp{— max L(S, w)} dw
ln V — ln (V ∙ exp{— max L(S, w)}
W∈M(W0)
ln V 一 ln V + h = h
where h is the height ofM(wo) defined in Section 5.1. For convenience, we shift down L(S, w) by
Lo and denote the shifted training loss Lo(w) =∆ L(S, w) 一 Lo so that Lo(wo) = 0. Then
Lo(w) = ；(W — WO)TΣ-1(w 一 Wo) ∀w ∈ M(wo)
Furthermore, the following two sets are equivalent
{w ∈ RW : L(S, w) = h} = {w ∈ RW : Lo(w) = h 一£。}
14
Under review as a conference paper at ICLR 2020
both of which are the W -dimensional hyperellipsoid given by the equation LO (W) = h - LO, which
can be converted to the standard form for hyperellipsoids as:
Σ-1
(W -WO)T 2(h-L0)(W - W0 ) = 1
The volume enclosed by this hyperellipsoid is exactly the volume of M(WO), i.e., V; so we have
nW/2
Γ( WW + 1)
,2W(h -LO)W∣Σ∣ = V
n、n
Solve for h, with the Stirling S approximation for factorial Γ(n + 1) ≈ √2πn (一)
h = LO +
(V ∙Γ(W + 1))2/W
2π∣∑∣1∕w
≈ L0 +
V2/W n1/W W(W+1)/W|I(W0)|
4πe
, we have
1/W
where Γ(∙) denotes the Gamma function. Notice that for modern DNNs We have W》1, and so
W + 1
W^W^ ≈ W. Then the generalization bound in the form of the PAC-BayeS Theorem is given as:
E [L(D, W)] ≤ E [L(S,W)]+2
w~Q	w~Q
W ∙ V2/Wn1/w∣I(w0)∣1∕w + 4πeL0 + 2πeln 2δN
2πe(N - 1)
We can further bound the first term on the right hand side as:
E [L(S, w)] ≤ E [ max L(S, w)] = h
w~Q	w~Q w∈M(wo)
Putting it all together, we can finally obtain Equation 7.
C Derivation of Equation 5
First, let us present the well-known theorem in linear algebra that relates the eigenvalues ofa matrix
to those of its sub-matrices.
Theorem 3. Given an n × n real symmetric matrix A with eigenvalues λ1 ≤ ... ≤ λn, for any k < n
denote its principal sub-matrix as B obtained from removing n - k rows and columns from A. Let
ν1 ≤ ... ≤ νk be the eigenvalues of B. Then for any 1 ≤ r ≤ k, we have λr ≤ νr ≤ λr+n-k.
Let {νn }N= ι be the eigenvalues of WF ξt(wo), which is a N 0 X N 0 sub-matrix of 10(wo); then
1 T	1 T	1	1 T N0
Y(WO) = τ X ln ∣ξ (WO)I = τ X ln ∣W ∙ Wξ (WO)I = Nln W + τ XX ln Vn
t=1
t=1
t=1 n=1
Theorem 2 gives the relation between νn and λn, defined above and in Section 5.3 as the nth smallest
eigenvalues of J ξt(wo) and that of I 0(wo), respectively. For sufficiently large N0, we can use Vn to
approximate λn. Ignoring the eigenvalues ofI0(WO) larger than λN0 is reasonable when estimating
Y(WO), since in general the majority of the eigenvalues of the Hessian for DNNs are close to zero
with only afew large “outliers” (Pennington & Worah, 2018; Sagun et al., 2018), and so the smallest
eigenvalues are the dominant terms in Y(WO). A specific bound of the eigenvalues remains an open
question, though. In short, we have PnN=0 1 Vn ≈ PnN=0 1 λ0n and consequently:
W	1W
Nb(Wo) + W ln W = NY(WO) — W ln W
=W- b(Wo)-N 0 lnW)
T	N 0
=T X NO X ln Vn
t=1	n=1
T	N 0
≈ T X W X ln λn
t=1	n=1
Finally we we have
1 T W N 0
TimO τ X N X ln λn = Y(WO)
t=1 n=1
15
Under review as a conference paper at ICLR 2020
D Details of Calculating the Metrics in Section 7.1
For the following metrics, we apply estimation by sampling a subset St from the full training set S
for T times and averaging the results:
•	FrobeniUsnorm: k"L(S,w)kF
•	Spectral radius: ρ(VWL(S,w))
•	Ours: b(w) = T PT=ι ln∣ξ(St, wo)∣
For the Frobenius norm based metric, from Equation 2 we have:
1	K	2
kVWL(S,w)kF = kI(w)kF = NN X XII(Vw['χ(wo)]i)(Vw['χ(wo)]i)t∣∣f
(x,y)∈S i=1
Similar to Equation 4, We approximate y by y and so
kVwL(S,w)kF ≈ N X ∣∣(Vw['χ(wο)]y)(Vw['χ(wο)]y)T∣∣2
(x,y)∈S
Summing over the entire Hessian matrix is too expensive as there are W × W × N entries in total.
We therefore estimate the quantity by first sampling a subset St ⊂ S and then sampling 100,000
entries of (Vw['χ(wο)]y)(Vw['x(wο)]y)T. We perform the estimation T times and average the
results, similar to the approach When computing γb(w).
Also by Equation 2 and the approximation in Equation 4, the spectral radius of Hessian is equivalent
to the squared spectral norm of 1∕√WJw [L(S, w)]. We also perform estimation (with irrelevant
scaling constants dropped) by sampling St for T times, i.e., viaT PtkJw [L(S t,w)]k2.
Furthermore, in all our experiments that involves samplings St, we set |St |= N 0 = T = 100.
E Architecture And Training Details in Section 7
Architecture details are as below
•	The plain CNN is a 6-layer convolutional neural network similar to the baseline in Lee et al.
(2016) yet without the “mlpconv” layers (resulting in a much fewer number of parameters).
Specifically, the 6 layers has numbers of filters as {64, 64, 128, 128, 192, 192}. We use
3 × 3 kernel size and ReLU as the activation function. After the second and the fourth
convolutional layer we insert a 2 × 2 max pooling operation. After the last convolutional
layer, we apply a global average pooling before the final softmax classifier.
•	For ResNet-20 and WRN-28-2-B(3,3), we use the same architecture as in their original
papers, with the only difference that we use pre-activation as in He et al. (2016b). This
results in slightly stronger baselines than the models in their original papers.
•	For DenseNet-BC-k=12 we use the the architecture identical to the one used in the original
paper.
The training details are
•	For the plain CNN, we initialize the weights according to the scheme in He et al. (2016a)
and apply l2 regularization of a coefficient 0.0001. We perform standard data augmenta-
tion, the one denoted 4-crop-f in Section 7.1. We use stochastic gradient descent with
Nesterov momentum set to 0.9 and a batch size of 128. We train 200 epochs in total with
the learning rate initially set to 0.01 and then divided by 10 at epoch 100 and 150.
•	For ResNet-20, WRN-28-2-B(3,3) and DenseNet-BC-k=12, we use the same hyper-
parameters, training schemes, data augmentation schemes, optimization methods, etc., as
those in their original papers, respectively.
16