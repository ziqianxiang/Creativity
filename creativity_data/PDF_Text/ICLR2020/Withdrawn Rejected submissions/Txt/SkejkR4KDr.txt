Under review as a conference paper at ICLR 2020
Layer Flexible Adaptive Computation Time
for Recurrent Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Deep recurrent neural networks perform well on sequence data and are the model
of choice. However, it is a daunting task to decide the structure of the networks,
i.e. the number of layers, especially considering different computational needs
of a sequence. We propose a layer flexible recurrent neural network with adap-
tive computation time, and expand it to a sequence to sequence model. Different
from the adaptive computation time model, our model has a dynamic number of
transmission states which vary by step and sequence. We evaluate the model on
a financial data set and Wikipedia language modeling. Experimental results show
the performance improvement of 7% to 12% and indicate the model’s ability to
dynamically change the number of layers along with the computational steps.
1	Introduction
Recurrent neural networks (RNN) are widely used in supervised machine learning tasks for their
superior performance in sequence data, such as machine translation Auli et al. (2013); Liu et al.
(2014), speech recognition Graves et al. (2013); Hannun et al. (2014), image description generation
Karpathy & Fei-Fei (2015); Mao et al. (2015), and music generation Boulanger-Lewandowski et al.
(2012). The design of the underlying network is always a daunting task requiring substantial com-
putational resources and experimentation. Many recent breakthroughs hinge on multilayer neural
networks ability to increase model accuracy, Hinton et al. (2012); Mohamed et al. (2012); Srivas-
tava et al. (2015), leading to the important decision in RNNs of the number of computational steps.
First, the right choice requires running several very expensive training processes to try many dif-
ferent computational steps. Even if a reinforcement learning algorithm is used to determine a good
computational steps, Baker et al. (2017); Zoph & Le (2017), it still requires a substantial training
effort. The second issue with the fixed structure RNNs is the fact that the same computational steps
is applied to each input in a sequence. It is conceivable that some inputs are harder to classify than
others and thus such harder inputs should employ more computational steps. A similar argument
holds for steps, e.g., certain steps in a sample can bear less predictive power and thus should use
fewer computational steps in order to decrease the computational burden. The goal of our work is to
introduce a network that automatically determines the computational steps - and together with this
the number of hidden vectors to use - in training and inference which is dynamic with respect to
samples and step number.
To resolve the inherent problems of fixed structure neural networks, Graves Graves (2016) addresses
this by providing an Adaptive Computation Time (ACT) model for RNN. In Graves’ model, a sig-
moidal halting unit is utilized to calculate a halting probability for each intermediate round within a
step, and a computation stops when the accumulated halting probability reaches or exceeds a thresh-
old. ACT can utilize multiple computation rounds within each individual step and it can dynamically
adapt to different samples and steps. The model is appealing due to its modeling flexibility and its
advantage in increasing model accuracy Dehghani et al. (2018). With the ACT mechanism, when
a step of computation is halted, all intermediate states and outputs are used to calculate one mean-
field state and output. The mean-field state and output have drawbacks. The outputs of the deepest
computational step are the most informative, and should be the final outputs. The output from early
computational steps may cause errors in the mean-field result which calls for using the last output
only. However in such a case all computational steps should benefit from transmissions from the
previous time step. This is not offered by ACT since its design is based on mean-field states and is
a key feature of the proposed model. To distinguish the roles among different computational steps,
1
Under review as a conference paper at ICLR 2020
each one should obtain its computation ability and receive its state individually from the previous
time step. Thus a more natural design should be a multilayer RNN with a flexible number of lay-
ers which is exactly what our proposed model offers. Our experimental results show that ACT has
marginal benefits over basic RNN or sequence to sequence (seq2seq) models, indicating that ACT,
with a single hidden vector, cannot always work well. This also motivates us to develop the layer
flexible RNN model with adaptive computation time.
The novelty of our work is that the number of layers in our model is flexible, so that it can both
achieve adaptive computation time and maintain the individual roles among different layers. Similar
to Graves’ work, we also utilize a unit to determine the action of each computational step within a
time step by calculating their halting probabilities. To obtain the optimal computation ability, each
layer should learn from the previous time step individually, and there should be concepts to decide
how much to learn from each layer in the previous time step. We face the challenge that the number
of layers is different between two consecutive time steps, so that we cannot set specific constant
rules of how to transmit the states. In our model, each time step produces multiple hidden states
(one state per computational time within the step). These multiple hidden states are then combined
into a different number of hidden states for the next step using attention ideas Bahdanau et al. (2014);
Luong et al. (2015) (the number of new hidden states equals to the number of computational steps
in the next step). The network can thus have a flexible number of layers with dynamic number of
transmission states.
In this paper, we proposed a layer flexible adaptive computation time (LFACT) model for RNNs.
Each layer indicates a computational step, produces a hidden state and receives its own transmis-
sion state from the previous time step. We also extend the model to the seq2seq framework. Our
experimental results show that LFACT offers significant improvements over ACT and RNN on dif-
ferent data sets and frameworks. With LFACT, there is no need to decide the specific structure of
an RNN model through extensive experimentation, since LFACT can automatically make decisions
of computational steps based on its inputs. LFACT is designed with a different logic in mind from
ACT, and at the same time overcomes the problems of ACT, e.g. poor performance on certain data
sets. Our model increases the accuracy of 7% to 8% on a financial data set and 12% on Wikipedia
language modeling, which attests to its robustness.
The rest of the manuscript is structured as follows. In Section 2 we review the literature. In Section 3,
the flexible layer adaptive computation time RNN model is presented, including all of the alternative
options. In Section 4 we introduce the data sets and discuss all the experimental results.
2	Literature Review
A deep learning model and algorithm have many hyperparameters. In an RNN, one of the problems
is deciding the computation amount of a certain input sequence. A simple solution is comparing dif-
ferent depths of networks and manually selecting the best option, but a series of expensive training
processes is required to make the right decision. Hyperparameter optimization Bergstra et al. (2011;
2013) and Bayesian optimization Snoek et al. (2012); Mendoza et al. (2016); Saxena & Verbeek
(2016) have been proposed to select an efficient architecture of a network. Based on these concepts,
Zoph Zoph & Le (2017) and Baker Baker et al. (2017) propose mechanisms for network config-
uration using reinforcement learning. However, massive training efforts are still present. Another
problem of such approaches is the assumption of a fixed structure of the network, irrespective of
the underlying sample and step. The difficulty of classification varies in each data set and sample,
and it is comprehensible that harder samples would require more computation. Therefore, applying
networks with the same computational steps is inflexible and it cannot achieve the goal of flexible
computation time among different samples. Conditional computation provides general ideas for al-
leviating the weaknesses of a fixed-structure deep network by establishing a learning policy Dahl
et al. (2012); Bengio et al. (2015). A halt neuron is designed and used as an activation threshold
in self-delimiting neural networks Schmidhuber (2012); Srivastava et al. (2013) to stop an ongoing
computation whenever it reaches or exceeds the halting threshold. Work Ying & Fragkiadaki (2017)
shows that conditional computation helps the networks obtain adaptive depth and thus yield higher
accuracy than fixed depth structures. Graves Graves (2016) introduces an Adaptive Computation
Time (ACT) mechanism for RNN to dynamically calculate each input step computation time and
determine their halting condition. These series of work focus on formulating the policies of halting
2
Under review as a conference paper at ICLR 2020
conditions and use a single hidden vector in each cell; none of them contribute to designing flexible
multilayer networks or study learning the rules of state transmission.
The ACT mechanism Graves (2016) is proved to improve performances and is applied in a few dif-
ferent problems. Universal Transformers Dehghani et al. (2018) apply ACT on a self-attentive RNN
to automatically halt computation. A dynamic time model for visual attention Li et al. (2017) is pro-
posed to accelerate the processing time by adding a binary action at each step to determine whether
to continue or stop. Figurnov et al. Figurnov et al. (2017) prove that applying ACT on Residual Net-
works can dynamically choose the number of evaluated computational steps and propose spatially
adaptive computation time for Residual Networks for image processing to adapt the computation
amount between spatial positions. Similarly, Neumann et al. Neumann et al. (2016) extend ACT to
a recognizing textual entailment task. In addition, ACT is also applied to reduce computation cost
and calculate computation time in speech recognition Li & Liu (2018), image classification Ler-
oux et al. (2018), natural language processing Yu et al. (2018), and highway networks Park & Yoo
(2017). These models simply apply the ACT mechanism on other models to achieve the abilities of
adaptive halting computations. They focus on solving their specific problems but do not make any
change to the structure of ACT cells. However, our work concentrates in the inner design of a layer
flexible ACT cell for its ability of automatically and dynamically adapting the number of layers.
3	Model
We start with an explanation of RNN and ACT. A standard RNN contains three layers: the input
layer, the hidden layer, and the output layer. The input layer receives input sequences x and transmits
them to the hidden layer to compute the hidden states u. The output layer calculates the output y
based on the updated state of each step. The equations are as follows:
ut = f (xt, ut-1),	yt = σ(Wout + bo).
In step t, input xt from the input sequence x is delivered to the network. A cell in the hidden layer
uses the input xt and the state ut-1 from the previous step to update the hidden state ut in the current
step. Long Short-Term Memory (LSTM) Hochreiter & Schmidhuber (1997) and Gated Recurrent
Unit (GRU) Cho et al. (2014) are frequently applied in the hidden layer cell f, which contain the
dynamic computation information and the activation of the hidden cells. The output yt is computed
utilizing an output weight Wo, an output bias bo, and an activation function σ.
ACT extends the standard RNN. The hidden layer contains several rounds of computation and each
round produces an intermediate state and output. The representation of intermediate states utn and
intermediate outputs otn are as follows:
utn=	ff((xxt0n,,uutn--11)),	nn>=00	,	xtn=(δn,xt),	otn	=	σ(Woutn + bo).
The first hidden cell, in step t, receives the state ut-1 from the previous step t - 1 and computes
the first intermediate state. All the following rounds of computation use the previous intermediate
output utn-1 and produce an updated state utn. To distinguish different rounds of computation, a flag
δ0 is augmented to the input xt for the first round and another flag δn is added for all others. Each
intermediate output otn is computed based on the intermediate state utn in the same round.
To determine the halting condition of a series of rounds of computation, units htn are introduced in
each computation round n as htn = σ(Whutn + bh). Here Wh is the halting weight and bh is the
halting bias.
The total computation time Nt in a step is decided by the halting units and the maximum threshold
L. Whenever the accumulated halting units’ value in a step t is over 1 or the computation time
reaches L, the computation halts. The definition of total computation time Nt is as follows:
n
Nt = min{min{n| X hit ≥ 1 - }, L},	(1)
i=1
where is a hyperparameter.
ACT uses all the intermediate states and outputs to calculate one mean-field state ut and output yt
(as represented in (2) and (3) below) for each step. A probability ptn produced by halting unit htn is
3
Under review as a conference paper at ICLR 2020
Figure 1: LFACT model - an example of three consecutive steps. Step t - 1 has three layers, step t
has two layers, and step t + 1 includes four layers.
-(
,$&(
-(
—,$((
#+)&
,($%&)'&
,($%&)&&
,($%&)(&
,($%&)'(
,($%&)&(
■ ,($%&)((
introduced into ACT for calculating the mean-field state and output according to the contribution of
each intermediate computation round in a step. The updated mean-field state ut is transmitted to the
next input step and the output ot is delivered to the output layer as the current step’s output.
n < Nt
n = Nt
Nt	Nt
ut = X pituit	(2)	yt = X pit oit	(3)
i=1	i=1
Given an input sequence x, the ACT model tends to compute as much as possible in each step to
avoid making erroneous predictions and incurring errors. This can cause an extra computational ex-
pense and impede achieving the goal to adapt the computation time. Therefore, training the model
to decrease the amount of computation becomes necessary. ACT introduces ponder cost P(x) as
P(x) = Nt + ptNt to represent the total computation time during the input sequence. The loss func-
tion L(x, gt) with gt being the ground truth is modified to encourage the network to also minimize
P(x):
L(x, gt) = L(y(x), gt) + T P (x)	(4)
where τ is a hyperparameter time penalty that balances the ponder cost and prediction errors.
3.1	Layer Flexible Adaptive Computation Time Recurrent Neural Network
In this section, our Layer Flexible Adaptive Computation Time (LFACT) model is introduced. The
main idea of LFACT is dynamically adjusting the number of layers according to the imminent char-
acteristic of different inputs and efficiently transmitting each layer’s information to the same layer
in the next step. Differing from ACT where only the mean-field state ut in (2) is transmitted to the
next step, which can be viewed as a single layer network, LFACT is designed for transmitting each
layer’s state individually between every consecutive step. In LFACT we compute Nt and Nt+1 as
in ACT. Each cell n (layer n) in step t takes Xt and uj-i as input and creates Un for n = 1,...,Nt.
Vector Un is computed from the output UnT of the previous cell and the hidden state Un-1 from the
previous step and same layer n. The problem is that at step t we produce utn for n = 1, ..., Nt but
for step t + 1 We need Un for n = 1,..., Nt+i. The key of our model is to use the attention principle
to create U1 ,U2,…,UNt+1 from U1,U2,..., UNt. FigUre 1 depicts the model.
The representation of the LFACT model is as folloWs:
UnT = [	g(UnT,UNΛ7,	n = 0 ,	Un	=	f(xt,Un-ι) n ≥0,	on	= σ(WoUn	+ bo).
g (Ut-i, Ut	) n > 0
The LFACT model contains tWo types of states. One state Utn is the primary output of each hidden
cell, which is the same as the states in standard RNN. The other state is the transmission state Un
that is used for transmitting layer information to the next step. The primary state from previous layer
4
Under review as a conference paper at ICLR 2020
u：-1 and the transmission state u"ι from the same layer in the previous time step are combined
together through function g . The combined state is delivered to the current cell. Possible options for
g are a multi-layer fully connected neural network, or an affine transformation of (x, y) followed by
an activation function. In our experiments, we use g(x, y) = σ(W1x + W2y + b).
In step t, the hidden layer cell f uses the input and the combined state from function g to compute
and update the primary state u：. The primary states are used to compute the transmission state Un
for the next step. To avoid possible errors caused by the previous layer, input xt is directly delivered
to each layer as an input. For n ≤ Nt+1, the equations governing the relationship between two
transmission states read
ctn	eβtin
Un = £ αtinun, αtin = ^g---------β-,	Btin = Vn ∙ σ(WQut+1 + VQ ut + bQ) i ≤ cn∙⑸
i=1	j=1 e tjn
To compute the transmission states U：, an attention unit α is introduced to represent the relationship
between the primary states utn in a certain layer n and the primary states in other layers. We propose
two choices to select ctn :
n	min(Nt,n), (a)
ct = Nt .	(b)
Option (a) only considers the relationship between the state Utn of the current layer and the states
Uit from the lower layers (i.e. i ≤ n), called limited (LTD). Alternative (b) utilizes all computed
transmission states (i.e. i ≤ Nt), called ALL. When strategy LTD is applied and Nt+1 ≤ Nt, all
primary states Uit in deeper layers (i.e. i > Nt) cannot be used. Strategy ALL aims to include the
computed information of all the layers. To distinguish different layers, extra weights Vn are utilized
to compute α. Weights Vn , WQ and VQ in (5) to compute α are vectors.
We use the same method as ACT to compute Nt (as represented in (1)), the computation time of
each step. But unlike ACT, the halting unit is computed based on the output and transmission state
of each layer as h： = σ(Whu: + Vhun-I + bh). In addition, instead of computing a mean-field
output, we directly take the output of the deepest layer as one step output as yt = otNt.
When applying loss function (4) to LFACT, the shallow layers have limited involvement in calculat-
ing gradients. Therefore, to get the prediction of each layer as accurate as possible, we introduce all
of the intermediate outputs in the loss function, as
Nt
~/	.、 N, .、	厂，，，、	.、	一、
L(x,gt) = L(x,gt) + μ 工 L(θt(x),gt).	(6)
i=0
In the experiments We use L = L.
3.2	Sequence to Sequence Model with LFACT
In order to deal With sequence tasks, We propose a combination model using a seq2seq (encoder-
decoder) model and our LFACT model, as F igUre 5 in Appendix A.1 shoWs. In the seq2seq model,
a cell in each step is replaced With our LFACT model to form a deep and flexible netWork. The
seq2seq encoder part accepts a sequence input, and in the decoder part, We use the last ground truth
as input.
4 Computational Experiments
All the models are trained starting With random Weights, i.e. no pretraining. Training the LFACT
model takes 20% to 30% more time than a typical ACT model. Most experiments are based on a
single seed, but in Section 4.2 We conclude that the variance is loW if the seed is varied.
5
Under review as a conference paper at ICLR 2020
(a) Model performance improvement
(b) Hyperparameter comparison
2.00
(c) F1 distribution
Figure 2: Results of RNN based models. (a) F1 score improvements over RNN on financial data
set three instances (INS1,INS2 and INS3). (b) Average F1 improvements over RNN for different μ
values on all three instances. (c) Average F1 score at each step on INS1. (d) Average computation
time (Nt) for LFACT on INS1.
Step Index
(d) Nt distribution
4.1 Financial Data Set
We test our LFACT models on a financial data set from Harmon & Klabjan (2018). The data set
consists of the tick prices of twenty-two ETFs at five minute intervals. The data is labeled into five
classes to represent the significance of the price changes, e.g., one class corresponds to the price
being within one standard deviation. We have 22 softmax classification layers in each step. We have
three test instances, and in each one we train our model on 50 weeks of returns (45,950 samples),
use the next week (905 samples) as validation data to save the best performing weights, and test the
model based on the saved weights using the following week (905 samples). Sequences have lenght
20. The financial data set is tested on both RNN and seq2seq frameworks.
RNN Based Models: RNN based models predict the next step price changes in each time step.
The LFACT model utilizes option affine transformation for g (g(x, y) = σ(W1x + W2y + b)) and
strategy ALL for computing transmission state U (Cn = Nt). We test plain ACT and RNN, which
have been tuned with respect to all hyperparameters as our baseline models, and compare them with
the RNN based LFACT model. We apply 0.001 as our ponder time penalty (τ = 0.001) for LFACT
and ACT (the value is obtained by the general optimal τ value of the experiments from GravesGraves
(2016)), and use the Adam optimizer with 0.0005 learning rate to train the models. The maximum
number of layers L is 5 and GRU cells with hidden vectors of size 128 are utilized in all the models.
F igure 2a shows the F1 score improvements of LFACT and ACT over RNN. We test all models
on three different instances INS1, INS2, and INS3. Each bar indicates the average F1 score for all
prediction steps in an instance. The results of LFACT are based on applying 0.1 to μ in loss (6).
The F1 score of RNN is 0.475, 0.461, 0.447 for INS1, INS2, INS3, respectively. From F igure 2a,
LFACT improves 14.1% over RNN on average, and ACT improves 6.3%. We introduce the new
loss function (6) in order to directly update the weights of each layer from the intermediate outputs.
Figure 2b provides the performance comparison for different μ. The results are the average F1
score improvement over RNN for all three instances. The best range for μ in (6) is 0.01 to 0.1, and
is better than the original one in (4) by 1.2%. The application of different μ values shows that our
new loss function yields improvements.
F igure 2c provides the F1 score distribution of steps 1 to 20 on INS1. LFACT consistently performs
better than ACT, indicating that multiple layers of hidden vectors bring better effectiveness than
a single one. The difficulty of a sequential prediction task is higher in early steps than in late
ones, because the early steps have limited information from the input. LFACT and ACT both are
stable in all prediction steps, but RNN acts poorly in early predictions. This benefit of LFACT and
6
Under review as a conference paper at ICLR 2020
0 8 6 4 2 0 2
Ioooooo
0
b3s3s -əʌo3EaA0」dE-
(a) Model performance improve-
ment
0.30 ----------------
123456789	10
Decoder SteP Index
3	■ Maximum 90th PerCentile
o H H Ii mil..........11111111111111111
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Step Index
(c)	Training set Nt distribution
3	■ Maximum 90th PerCentile
o∣∣∣l∣ll∣ll∣ll∣lll∣∣lllllllllll
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Step Index
(d)	Validation set Nt distribution
3	■ Maximum 90th PerCentile
0 ill I III I I ill I ∣∣∣∣ I I........................................................
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Step Index
(b) F1 distribution	(e) Test set Nt distribution
Figure 3:	Results of seq2seq based models. (a) F1 score improvements over seq2seq on financial
data set three instances (INS1, INS2 and INS3). (b) Average F1 score at each step on INS1. (c) (d)
(e) are computation time (Nt) distributions based on optimized LFACT weights on INS1: X-axis is
the step index; 1 to 20 indicate encoder; 21 to 30 are from the decoder part.
ACT implies that adaptive computation can contribute to hard tasks. Figure 2d gives the average
computation time (Nt) of each step on the test set of INS1. Higher average Nt of early steps proves
LFACT’s ability of deeply computing on hard tasks, and further explains why LFACT is so effective
on early predictions.
Seq2seq Based Models (10 Prediction Steps): In addition to the RNN framework, we also use
the seq2seq version of models to predict the following ten steps. The raw sequence data with input
length of 20 is delivered into seq2seq models as the inputs of the encoder part. All hyperparameters
are the same as in the RNN based experiments, and the same strategies for g and c as in the RNN
based LFACT are applied to the seq2seq framework. Considering that the encoder part does not
have outputs, we apply loss function (4) in this task.
In Figure 3a, we present the F1 scores relative changes over seq2seq alone for each instance. The
F1 scores of seq2seq are 0.439, 0.481, 0.447. The ACT model is worse than seq2seq on INS3, so
the improvement here is negative. From the results, the seq2seq based LFACT improves F1 7.4%
over seq2seq, and ACT acts similar to seq2seq. In Figure 3b, we provide the F1 scores for the
ten prediction steps in the decoder individually on INS1. All three models decrease over time, but
LFACT and ACT are more stable than seq2seq. In seq2seq based models, the decoder part has con-
stant input of last ground truth, and can cause information deterioration as time passes. Thus, the
benefits of LFACT on late predictions over seq2seq alone imply better abilities of LFACT on infor-
mation transmission and memorization. Surprisingly, the first prediction of seq2seq is better than
LFACT, which conflicts the results from RNN. This may be caused by LFACT requiring delay when
transforming from input to predictions since it has more trainable weights than seq2seq. However,
the whole point of the seq2seq framework is multiple steps of predictions, and LFACT catches up
very fast at the second prediction, so the disadvantage of LFACT should not be concerning.
F igure 3 also presents the computation time (Nt) results for INS1: F igures 3c and 3d are the
results of the training and validation process based on the optimized weights, and F igure 3e is
for test. The result shows the change of Nt among the different steps, indicating that the LFACT
model has the ability of adapting computation time dynamically according to its input. Because
of the same input in the decoder, Nt values are the same from step 21 to 30 within each set. In
addition, the low Nt values in test set imply that LFACT has low computation request in the decoder
part. Thus, the multiple computation ability of LFACT is not the reason for the good performance
in the seq2seq setting, as it is in the early predictions in the RNN setting. Comparing to seq2seq
alone which contains only one computation time as well in the decoder, the significant benefits in
7
Under review as a conference paper at ICLR 2020
0.14
1.25
1.20
1.15
1.10
1.05
1.00
(a) Models performance
H 0.13
i
0 0.12
=
I 0.11
∈
0.10
0.135
0.00	0.01	0.05	0.10	0.30	0.50
μ
(b) Hyperparameter comparison
0 5 1015 20 25 30 35 40 45 50556065 70 75 80859095
Training Epoch Index
(c) Nt change
Figure 4:	Results of Wikipedia language modeling task. (a) Models performances. Numbers above
bars are BPC values and the percentage inside of bars are the relative changes over RNN. (b) Aver-
age F1 improvements over RNN for different μ values. (C) Computation time (Nt) change during
LFACT model training process.
late predictions for LFACT further confirm the conclusion that LFACT has the excellent abilities for
information transmission and memorization.
We also conduct similar experiments by making 5 predictions. These are shown in Appendix A.2.
The observations are very similar.
4.2 Wikipedia Language Modeling
This task focuses on predicting characters from the Hutter Prize Wikipedia data set, which is also
used in Graves’ ACT paper Graves (2016). The original unicode text is used without any prepro-
cessing. Every character is represented as one-hot, and presents one time step. In our experiment,
10,240 sequences including 512,000 characters in total are randomly selected as the training set,
and 1,280 sequences with 64,000 characters in total are chosen as validation and test sets without
repetition. Each sequence includes 50 consecutive characters, and the next character is predicted
at each time step in this task (RNN setting). GRU cells with 128 hidden size are used to struc-
ture all models. The maximum number of layers L is set to 3, and a softmax layer with size 256
is added to each step in the decoder. We apply the optimized ponder time penalty (τ) 0.06 from
Graves’ experiments Graves (2016) for this task. The models are evaluated using bit per character
BPC = E Pt - log2 P r(xt+1|yt) . Lower BPC values reflect better performances. All results
are based on option affine transformation for g (g(x, y) = σ(W1x + W2y + b)) and strategy ALL
(ctn = Nt).
In Figure 4a, we present the experimental results of LFACT and the two baseline models ACT and
RNN on the language modeling task. The reported BPC values for LFACT are from different settings
of hyperparameter μ in loss (6). Three different random seeds are applied for ACT and RNN to test
the stability of the models. Maximum, minimum, and average BPC values are provided. The bars
in F igure 4a represent average BPC values, and error bars indicate maximum and minimum BPC.
From the experiment, ACT does not have a significant benefit over RNN, but LFACT improves
11.9% over ACT and 12.6% over standard RNN. From the error bars, LFACT has the smallest
variance and ACT varies the most. Strong stability for LFACT reflects its better ability to deal with
complex situations. To test the influence of the hyperparameter μ in loss function (6), We compare
the different settings of μ in Figure 4b. When μ = 0, the loss function is equal to the original
one in (4). From Figure 4b, the best range for μ is from 0.01 to 0.1. However, when μ is set to
be a larger value (μ > 0.3), the new loss function does not bring any performance improvement
over the original loss function. To assess scalability of LFACT, we also test it on different sizes of
training data, as F igure 9 shows in Appendix A.4. The results indicate that LFACT performs well
on different sizes of training data ranging from 100,000 characters to 10 million.
In addition, we test the fully connected network option for g(x, y) and strategy LTD (ctn =
min(Nt, n)). The fully connected network forg provides 1.074 BPC, and LTD gives 1.678. Neither
of them are better than our experimental settings. Therefore, the affine transformation forg and ALL
are better strategies for LFACT.
In F igure 4c, we provide the average maximum and average of each step computation time (Nt)
during training of the Wikipedia language modeling task. We observe a clear decrease during the
early training epochs, which eventually stabilizes. Note that during epochs 5 to 10, the maximum
Nt increases but the average Nt still decreases. We postulate that the LFACT model has already
8
Under review as a conference paper at ICLR 2020
obtained the ability to predict most samples during this period, and is putting more effort on the
difficult samples. Figure 8 in Appendix A.3 shows the Maximum Nt distributions of training,
validation, and test based on the optimized weights. We only present the last 25 steps; the first 25
steps are all 1. The distributions show that the LFACT model is able to keep the computation time as
low as possible, but also has the ability of deep computation for certain samples. With the optimized
weights, only 0.03% of the sequences in the training set have more than one computation time, and
validation and test sets have 0.24% and 0.16% of the sequences with multiple computation. This
difference happens because the model is trained based on the training set, and the model should have
learned the most efficient way to predict characters in the training set.
5 Conclusion
Deciding the structure of recurrent neural networks has been a problem in deep learning applica-
tions, in particular the number of computational steps. A halting unit is applied in a previous work
to adapt the computation time to inputs, but a single hidden vector structure leads to information
transmission weaknesses. We propose LFACT which utilizes an attention strategy in designing an
information transmission policy which leads to a flexible multilayer recurrent neural network with
adaptive computation time. LFACT can automatically adjust computation time according to the
computing complexity of inputs and has outstanding dynamic information transmission abilities be-
tween consecutive time steps. We apply LFACT in an RNN and a seq2seq setting and evaluate the
model on a financial data set and Wikipedia language modeling. The experimental results show a
significant improvement of LFACT over RNN and seq2seq and ACT on both data sets. The dif-
ferent number of layers in practice indicates LFACT’s ability of adapting computation time and
information transmission.
References
Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. Joint language and translation
modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, 2013.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network archi-
tectures using reinforcement learning. International Conference on Learning Representations,
2017.
Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in
neural networks for faster models. International Conference on Learning Representations, 2015.
J. Bergstra, D. Yamins, and D. D. Cox. Making a science of model search: Hyperparameter opti-
mization in hundreds of dimensions for vision architectures. In Proceedings of the 30th Interna-
tional Conference on International Conference on Machine Learning, 2013.
James S Bergstra, Remi Bardenet, YoshUa Bengio, and Balazs KegL Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems, 2011.
Nicolas BoUlanger-Lewandowski, YoshUa Bengio, and Pascal Vincent. Modeling temporal depen-
dencies in high-dimensional seqUences: Application to polyphonic mUsic generation and tran-
scription. Proceedings of the 29th International Conference on Machine Learning, 2012.
KyUnghyUn Cho, Bart van Merrienboer, Dzmitry BahdanaU, and YoshUa Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.
George E Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and
Language Processing, 20(1):30-42, 2012.
9
Under review as a conference paper at ICLR 2020
Mostafa Dehghani, StePhan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Eukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018.
Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry P Vetrov, and
Ruslan Salakhutdinov. SPatially adaPtive comPutation time for residual networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
Alex Graves. AdaPtive comPutation time for recurrent neural networks. arXiv preprint
arXiv:1603.08983, 2016.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. SPeech recognition with deeP recur-
rent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal
Processing, 2013.
Awni Hannun, Carl Case, Jared CasPer, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho SenguPta, Adam Coates, et al. DeeP sPeech: Scaling uP end-to-end
sPeech recognition. arXiv preprint arXiv:1412.5567, 2014.
Mark Harmon and Diego Klabjan. Dynamic Prediction length for time series with sequence to
sequence networks. arXiv preprint arXiv:1807.00425, 2018.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, NavdeeP Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. DeeP neural networks
for acoustic modeling in sPeech recognition: The shared views of four research grouPs. IEEE
Signal Processing Magazine, 29(6):82-97, 2012.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2015.
Sam Leroux, Pavlo Molchanov, Pieter Simoens, Bart Dhoedt, Thomas Breuel, and Jan Kautz.
Iamnn: Iterative and adaptive mobile neural network for efficient image classification. Workshop
on International Conference on Learning Representations, 2018.
Mohan Li and Min Liu. End-to-end speech recognition with adaptive computation steps. arXiv
preprint arXiv:1808.10088, 2018.
Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, and Wei Xu. Dynamic computational time
for visual attention. IEEE International Conference on Computer Vision Workshop, 2017.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. A recursive recurrent neural network for statistical
machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Compu-
tational Linguistics, 2014.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, 2015.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. Deep captioning
with multimodal recurrent neural networks (m-RNN). International Conference on Learning
Representations, 2015.
Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. To-
wards automatically-tuned neural networks. In Workshop on Automatic Machine Learning, 2016.
Abdel-rahman Mohamed, George E Dahl, Geoffrey Hinton, et al. Acoustic modeling using deep
belief networks. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):14-22,
2012.
Mark Neumann, Pontus Stenetorp, and Sebastian Riedel. Learning to reason with adaptive compu-
tation. NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems, 2016.
10
Under review as a conference paper at ICLR 2020
Hyunsin Park and Chang D Yoo. Early improving recurrent elastic highway network. arXiv preprint
arXiv:1708.04116, 2017.
Shreyas Saxena and Jakob Verbeek. Convolutional neural fabrics. In Advances in Neural Informa-
tion Processing Systems, 2016.
Jurgen Schmidhuber. Self-delimiting neural networks. arXivpreprint arXiv:1210.0118, 2012.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Information Processing Systems, 2012.
RuPesh K Srivastava, KlaUs Greff, and Jurgen Schmidhuber. Training very deep networks. In
Advances in Neural Information Processing Systems, 2015.
Rupesh Kumar Srivastava, Bas R Steunebrink, and Jurgen Schmidhuber. First experiments with
powerplay. The 2nd Joint IEEE International Conference on Development and Learning and on
Epigenetic Robotics, 2013.
Chris Ying and Katerina Fragkiadaki. Depth-adaptive computational policies for efficient visual
tracking. In International Workshop on Energy Minimization Methods in Computer Vision and
PatternRecognition, pp. 109-122. Springer, 2017.
Adams Wei Yu, Hongrae Lee, and Quoc Le. Learning to skim text. The 55th Annual Meeting of the
Association for Computational Linguistics, 2018.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. International
Conference on Learning Representations, 2017.
A Appendix
A.1 Seq2seq Model with LFACT
23456
23456
234 5 6
,=&&
,=(&
jg(∙
,=2&
23456 89：;<
23456
#'
i→—»
&&-^l-=*(#=)i)
23456
23456
,$&2
，$(2
23456
■，r&&
,$(&
，$&（
,$（（,
i(

Figure 5: Seq2seq with LFACT model: the first four steps represent the encoder and the last four
steps indicate the decoder. xι to XT，are inputs in the encoder, zι to zt『are inputs in the decoder,
and y1 to yt0 are t0 steps of predictions.
A.2 Seq2seq Based Models (5 Prediction Steps)
To examine the stability of the LFACT model, we further test the seq2seq based models with 5 pre-
diction steps. The setting is the same as in the 10-prediction case except we have only 5 predictions.
F igure 6 shows the relative F1 scores for LFACT and ACT based on seq2seq alone. The F1 scores
for seq2seq on the three instances are 0.492, 0.534, and 0.498. The seq2seq based LFACT performs
better than both ACT and seq2seq in the 5-prediction task, and the benefit is significant over ACT.
11
Under review as a conference paper at ICLR 2020
However, the improvement of LFACT over seq2seq is not as pronounced as in the 10-prediction
task, and ACT is even worse than seq2seq. F igure 7 is the F1 score distributions for the three
models on INS1. The results match the 10-prediction task, and show that the advantage of LFACT
is more likely to affect late predictions in the seq2seq framework.
ba-bas」a>0
0.028
LFACT
■ INS1 ■INS2 ■ INS3
0.04
0.03
0.02
0.01
0.00
-0 -0.01
I -0.02
I -0.03
≡ -0.04
-0.05
-0.026
-0.042	-0.041
0.54
0.52
0.50
0.48
0.46
0.44
0.42
0.40 -----------------------------------------------------
1	2	3	4	5
Decoder Step Index
Figure 6:	Performances of seq2seq based
models (5 prediction steps): F1 relative
changes over seq2seq on the financial data
three instances (INS1, INS2, INS3).
Figure 7:	F1 distributions (5 prediction
steps): average F1 score at each prediction
step for seq2seq based models on INS1.
A.3 Computational Time for Wikipedia Language Modeling based on Optimal
Weights
■ training -validation ■ test
6
2
2 10
IN EnE-Xe工
Iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
27 28 29 30 31 32 33 34 35 36 37 38 39
Step Index
Illlll
40 41 42
HMllIlml
43 44 45 46 47 48 49 50
Figure 8
A.4 Performance on Different Training Sizes for Wikipedia Language
Modeling.
We test LFACT on different training sizes ranging from 100,000 characters in total to 10 million,
as Figure 9 shows. As the training set size increases, our model achieves better performance and
eventually gets around 0.99, which indicates scalability. We conclude that LFACT consistently has
over 7% improvement on all of the training sizes over ACT and RNN. Due to the computational
resource limitations, all the results in Section 4.2, including hyperparameter comparison, are based
on 512,000 characters and 10,240 sequences training size, and 64,000 characters, 1,280 sequences
test size.
OooOO OQO
5432 Io 98
LLLLLL
sal UO ɔdm
Figure 9: Performance on different training size
12