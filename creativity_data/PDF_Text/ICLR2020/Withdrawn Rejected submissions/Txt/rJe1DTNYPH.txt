Under review as a conference paper at ICLR 2020
Towards Disentangling Non-Robust and Ro-
bust Components in Performance Metric
Anonymous authors
Paper under double-blind review
Ab stract
The vulnerability to slight input perturbations is a worrying yet intriguing property
of deep neural networks (DNNs). Though some efforts have been devoted to
investigating the reason behind such adversarial behavior, the relation between
standard accuracy and adversarial behavior of DNNs is still little understood. In
this work, we reveal such relation by first introducing a metric characterizing the
standard performance of DNNs. Then we theoretically show this metric can be
disentangled into an information-theoretic non-robust component that is related
to adversarial behavior, and a robust component. Then, we show by experiments
that DNNs under standard training rely heavily on optimizing the non-robust
component in achieving decent performance. We also demonstrate current state-of-
the-art adversarial training algorithms indeed try to robustify DNNs by preventing
them from using the non-robust component to distinguish samples from different
categories. Based on our findings, we take a step forward and point out the
possible direction of simultaneously achieving decent standard generalization and
adversarial robustness. Itis hoped that our theory can further inspire the community
to make more interesting discoveries about the relation between standard accuracy
and adversarial robustness of DNNs.
1	Introduction
Deep neural networks (DNNs) have achieved wide success over the last decade. In literature, the
majority of deep learning models pursue boosted performance from different aspects (Krizhevsky
et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; He et al., 2016; 2015; Ioffe &
Szegedy, 2015). However, it is found these powerful models are susceptible to perturbations, even
those imperceptible to humans (Szegedy et al., 2013). Therefore beyond the main research stream,
there are also works devoted to investigating effective attacks (Goodfellow et al., 2014; Moosavi-
Dezfooli et al., 2016; Kurakin et al., 2016; Zhao et al., 2018) and designing adversarial robust models
(Goodfellow et al., 2014; Miyato et al., 2015; Madry et al., 2017; Zhang et al., 2019).
It is observed that the adversarial robustness is usually achieved at the cost of a non-trivial degradation
of standard performance. Previous efforts (Schmidt et al., 2018; Tsipras et al., 2018; Nakkiran, 2019)
trying to understand this phenomenon are usually based on simple toy models or heavy assumptions,
and do not provide a general theoretic framework that explicitly shows how adversarial robustness is
related to standard generalization. The work (Ilyas et al., 2019) empirically shows the adversarial
samples might be human imperceptible features that could help generalization, but it does not provide
any general theoretic framework to properly explain this phenomenon. In another work (Zhang et al.,
2019), though a bound on the gap between standard accuracy and adversarial accuracy is derived
so that the gap between these two quantities could be explicitly controlled, the underlying reason of
such trade-off is still not fully characterized.
In this work, we start from a new perspective and consider the performance of deep classification
models with a new metric Cross Category Kullback-Leibler divergence (CCKL), namely the Kull-
back-Leibler (KL) divergence between the model's output distributions over input data from different
categories, instead of the traditional metric of accuracy. Interestingly, by applying Taylor expansion
on CCKL, we show that it can be disentangled into a lower order non-robust component, which is
related to causing adversarial behavior, and a higher order robust component. By applying such
1
Under review as a conference paper at ICLR 2020
disentanglement, we are able to reveal the relation between standard generalization and adversarial
robustness of the deep learning model in a relatively general setting.
Furthermore, we demonstrate by experiments that current deep learning models rely heavily on
optimizing the lower order non-robust component to generalize, which is a major underlying reason
for the adversarial behavior. We also show the state-of-the-art adversarial training algorithms are all
in fact trying to constrain the model from using the lower order non-robust component to discriminate
data of different categories. Based on these findings, we claim that enabling the model to rely more
on the higher order robust component instead of the adversary-prone lower order component might
be the key to achieving decent standard accuracy and adversarial robustness simultaneously.
Our contributions are summarized as follows:
•	We propose a new metric, Cross Category Kullback-Leibler divergence (CCKL), to char-
acterize the standard performance. This metric can be more naturally connected with the
adversarial behavior of DNNs than the traditional metric of accuracy.
•	By applying a simple Taylor expansion on CCKL, we theoretically reveal the relation
between standard generalization and adversarial behavior of DNNs in a general way without
relying on toy models.
•	Based on the above novelties, we take a further step and point out the possible direction for
simultaneously achieving decent standard accuracy and adversarial robustness.
2	Related Work
Adversarial Attack and Defense To study the adversarial behavior of DNNs, gradient-based
algorithms (Goodfellow et al., 2014; Papernot et al., 2016a; Moosavi-Dezfooli et al., 2016; Carlini
& Wagner, 2017; Kurakin et al., 2016; Zhao et al., 2018) are developed to find the most effective
adversarial perturbations, and on the other hand, model-based defense algorithms (Papernot et al.,
2016b; Madry et al., 2017; Miyato et al., 2015; Zhang et al., 2019) are also explored for enhancing
model robustness against the adversarial perturbations. For example, in (Miyato et al., 2015; Zhang
et al., 2019), the model is regularized by the KL divergence between outputs of clean and adversarial
samples, which is also adopted in our theoretic framework.
Fisher Information in Deep Learning Fisher information is used as a tool to study machine
learning topics. Many previous works (Chaudhry et al., 2018; Pascanu & Bengio, 2013; Desjardins
et al., 2015; Andrychowicz et al., 2016; Achille et al., 2019) view the parameters of the model as those
of the Fisher information, while some other (Miyato et al., 2015; Zhao et al., 2018) take the model’s
input as the parameters of Fisher information, making it applicable in the context of adversarial
robustness. We adopt the same definition for Fisher information as (Miyato et al., 2015; Zhao et al.,
2018) in our later analysis.
Explaining Adversarial Behavior of DNNs Previous works trying to explain the robust model’s
degradation in standard performance (Tsipras et al., 2018; Schmidt et al., 2018; Nakkiran, 2019) are
mostly based on some simple toy models that cannot generalize to real world settings. Recently,
(Ilyas et al., 2019) empirically shows that adversarial noise is actually features that can help the
generalization of DNNs. However, this work does not provide a general theoretic framework to
explain this phenomenon. Zhang et al. (2019) provides a bound that characterizes the gap between
standard accuracy and adversarial robustness, but they do not clearly explain the underlying reason of
such trade-off. In our work, we analyze the underlying reason for the adversarial behavior of DNNs
by directly disentangling the proposed CCKL. In this way, we not only theoretically demonstrate the
reasoning behind the trade-off between adversarial robustness and standard generalization, but also
point out the possible direction for achieving both objectives simultaneously.
3	New Performance Metric and Its Disentanglement
In this section, we first propose to consider and measure the performance of deep classification models
from the perspective of CCKL instead of accuracy. Then we explain our finding that the performance
2
Under review as a conference paper at ICLR 2020
of a DNN model is determined by two components that are disentangled from the obtained objective
based on KL divergence, one of which related to the model’s adversarial behavior and the other not.
After that, to help understand our idea, we provide some complementary understandings from the
viewpoint of information geometry.
3.1	Proposed Performance Metric
Normally, DNNs are trained by minimizing the following cross-entropy loss:
L = E(x,y)〜X×Y [KL(ykf (X))],	(I)
where X × Y is the distribution where the data-label pair (x, y) is sampled from and f(x) is the
prediction of the model. Note that y and f(x) are both discrete distribution over categories here.
Correspondingly, the prediction accuracy is adopted to measure the performance of a DNN model.
However, such a metric hides the connection between the model performance and its adversarial
behavior. Since adversarial attack is concerned with changing the prediction of the model on a data
point to another, we propose to shift the performance metric from investigating one single data point
to comparing the data point with its counterparts from other categories, which can help reveal the
above connection. Therefore, we propose the following Cross Category KL Divergence (CCKL)
metric:
CCKL = E((Xi,yi),(Xj,yj ))∈S [KL(f (Xi)IIf (Xj ))],	⑵
where S is defined as
S = {((Xi, yi), (Xj , yj )) I ∀i, j ∈ N , yi 6= yj , Xi, Xj ∈ X }.	(3)
With the definition above, we can derive the following theorem:
Theorem 3.1 Denoting JS(pkq) as the Jensen-Shannon divergence between two distributions p and
q, we have
CCKL ≥E((xi,yi),(xj,yj))∈S[JS(yikyj)] -L-coL,	(4)
where L is the cross-entropy loss defined in Equation (1) and coL = E(x,y)〜(X,Y)[KL(f(x)ky)] is
the co-object for L.
We defer the proof and more comments of Theorem 3.1 to Appendix A due to space limit.
Taking a closer look at Theorem 3.1, we knowthatE((xi,yi),(xj,yj))∈S[JS(yikyj)] remains unchanged
during training, and L and coL are supposed to decrease when the model performance increases.
Therefore, the lower bound on CCKL is supposed to increase with the rise of model performance,
which will asymptotically result in the increase of CCKL. Thus, the CCKL, which characterizes how
well the model discriminates input data from different categories, can serve as a performance metric.
To further expound on our point, we provide a visualization of how CCKL evolves with test accuracy
and test cross entropy loss during training in Figure 2. According to our results, the variation of
CCKL correlates well with the test accuracy and test cross entropy loss during training.
3.2	Connections Between Adversarial Behavior and CCKL
We now explain how the above CCKL objective (2) correlates with the model’s adversarial behavior.
Consider a single data-label pair (X, y). The following cross entropy loss of the model fθ on
adversarial samples (with perturbation η):
Lθ(X, η) = KL(ykfθ(X + η)) s.t. kηk ≤	(5)
is adopted to study the adversarial behavior in most previous literature (Goodfellow et al., 2014;
Kurakin et al., 2016; Madry et al., 2017). However, it is not easy to build connection between this
objective and the standard generalization of the model. Interestingly, in some recent works (Miyato
et al., 2015; Zhao et al., 2018; Zhang et al., 2019), another objective is adopted to characterize the
model’s adversarial behavior:
Lθ(X, η) = KL(fθ(X)kfθ(X + η)) s.t. kηk ≤ .	(6)
Based on the above objective (6), (Zhao et al., 2018) reports state-of-the-art results in the task of
adversarial attack and (Zhang et al., 2019) reports state-of-the-art results in adversarial robustness.
3
Under review as a conference paper at ICLR 2020
Figure 1: Illustration on how output distributions f(x1) and f(x2) evolve during training. Column 2:
ground-truth label. Column 3: network outputs at initialization. Column 4: network outputs when
trained to converge. The value of KL(finit(x1)kfinit(x2)) in the graph is 0.053, and the value of
KL(ftrained(x1)kftrained(x2)) is 3.197.
Figure 2: Visualization of how accuracy (left), cross entropy loss (middle) and CCKL (right) on test
set evolve when training VGG13 on CIFAR-10. It is clear that CCKL well indicates test accuracy and
test cross entropy loss. For more experiment results on relation between CCKL and cross entropy
loss, please refer to Appendix B.
More importantly, as will be shown later in the paper, the adversarial objective (6) can be easily
linked to the standard generalization of the model. Therefore, we adopt the objective (6) in the rest of
the paper. The corresponding adversarial training objective is thus formulated as follows:
min max Lθ (x, η) s.t. kηk ≤ .	(7)
θη
Given the definition of Lθ (x, η), applying Taylor expansion yields the following:
max Lθ(x, η) = max ∖η>Fχη + X Txk) (η) s.t. kηk ≤ 3
η	η2
k=3
(8)
where Fx is the Fisher information of f (x) w.r.t. x and Tx(k) is the following k-th order Taylor
expansion term. Let f(j)(x) be the j-th entry of f(x) and n be the dimension of f (x). Then Fx can
be calculated by
n
Fx = X f ⑶(x)(Vx log f ⑶(x))(Vx log f ⑶(x))>	(9)
j=0
When is sufficiently small, the higher order terms in the above would vanish and Equation (8) could
be simplified into
maxLθ(x,η) = maxη>FXη s.t. kηk ≤ 已	(10)
η	2η
We could therefore construct the lagrangian of the above problem:
L(x,η) = 2η>Fxη — 2λ(η>η - €2).	(11)
By setting VηL(x, η) = 0, we obtain Fxη = λmaxη, where λmax is the maximum eigenvalue of Fx.
Therefore, the most effective adversarial perturbation corresponds to the leading eigenvector of Fx .
4
Under review as a conference paper at ICLR 2020
Consequently, we have
max Lθ(x, η) = EλmaχE .	(12)
η2
Note that λmax here is also the spectral norm of the Fisher information matrix Fx . The above theory
shows that the local adversarial behavior of the model f around the input x is determined by the
spectral norm of the Fisher information matrix: the adversarial behavior around x would be more
severe if the spectral norm of Fx is larger.
Now we analyze CCKL by considering one tuple ((xi, yi), (xj, yj)) randomly sampled from S. We
rewrite KL(f(xi)kf(xj)) as
KL(f(xi)kf(xj)) = K L(f (xi)kf (xi + (xj - xi))) = Lθ(xi,xj - xi).	(13)
Then, we apply the same Taylor expansion as above and obtain
1∞
KL(f(xi)kf(xj)) = Lθ(Xi,xj -	Xi)	= 2(xj	-	xi)>Fxi(Xj- Xi)	+ XTxk)(Xj-	xi)∙	(14)
Comparing Equation (14)1 and Equation (10), we notice they share the same Fisher information
Fxi at data point Xi . Here, we show the adversarial behavior at each data point can be connected
to the performance objective CCKL via Fisher information, which lays the foundation for the
disentanglement in the next subsection.
3.3	Disentanglement of CCKL
As mentioned above, we build the connection between adversarial behavior and CCKL. We here
explain how the non-robust and robust component in CCKL can be disentangled. We first denote
1 (Xj - Xi)>Fxi (Xj - Xi) in Equation (14) as G1 and the following terms P∞=3 Txk)(Xj - Xi) as
G2. Thus KL(f(Xi)kf(Xj)) can be formulated as
KL(f(Xi)kf(Xj))=G1+G2.	(15)
Taking a closer look at Equation (15), we notice the increase of G1 and G2 can both contribute to
the rise of K L(f (Xi)kf (Xj)), which contribute to CCKL. In addition, since λmax is the maximum
eigen value of Fxi , we have the following bound:
1	λ link2	≥ 1	∣" —kηk— (x∙	- X.)[	F	∣" —kηk— (x∙	- X .)]	— GI —kηk2—	(16)
2	λmaxkηk	≥ 2	[∣∣Xi-Xj Il(Xi Xj)]	Fxi[∣∣Xi-Xj II(Xi Xj)]	= G1 IlXi-Xj |户 .	(6)
With the above bound, we know that the rise of G1 would asymptotically result in the rise of the
norm of Fxi , which corresponds to the adversarial behavior.
Therefore, if the model relies heavily on the increase of G1 to boost performance, the norm of Fxi
would also increase drastically. According to Equation (12) and our analysis above, the rise in the
norm of Fxi means more severe adversarial behavior around Xi . The trade-off between standard
performance and adversarial behavior is thus characterized as follows: the model can rely on G1
to boost performance, but it comes with the side effect of more severe adversarial behavior; on the
other hand, G2 contributes to the CCKL but it is not involved in the adversarial objective, thus it
would not cause adversarial behavior to rely on terms in G2 to distinguish Xi from data belonging to
other categories. In this way, we can disentangle the non-robust and robust component in the overall
performance objective CCKL.
To further understand the role of G1 in classification, we visualize how F-norm of Fisher information
evolves during training. We visualize F-norm instead of spectral norm because all norms are
equivalent and spectral norm is not computationally feasible in our case. We first empirically show
how the average F-norm of Fisher information and standard accuracy of VGG13 model on CIFAR-10
test set vary during normal training in Figure 4. We observe that the norm of Fisher information
increases drastically with the rise of accuracy, which indicates DNNs rely heavily on the non-robust
component G1 to boost performance.
1Note that (xj - xi ) is the difference between two input data instead of a small perturbation, so the higher
order terms would not vanish here.
5
Under review as a conference paper at ICLR 2020
^US3UΛ
0.2
0.1
O IO 20	30	40	50	60	70 BO
traininq epoch
UXJOU--oqs∙zΦCT2Φ>< 60-j
Figure 3: Visualization of how standard test accuracy (left) and average F-norm (right) of Fisher
Information Matrix on test set vary during normal training and adversarial training with VGG13 on
CIFAR-10 (for the same experiments on ResNet, see Appendix D). We take the nature logarithm to
better visualize the average F-norm of Fisher information.
Then we compare normal training with the two
state-of-the-art adversarial training algorithms
(Madry et al., 2017; Zhang et al., 2019) using
the same visualization method, and show results
in Figure 3. It is clear that during adversarial
training, although the Fisher information’s aver-
age F-norm also rises with standard accuracy, its
value is significantly smaller than its counterpart
during normal training. That is, the adversar-
ial training process can effectively constrain the
model from relying on Fisher information to
boost performance.
0	10	20	30	40	50	60	70 BO
training epoch
Figure 4: Visualization of how standard accuracy
and average F-norm of Fisher information matrix
on test set vary during normal training. The ex-
periment is conducted with a VGG13 model and
CIFAR-10 data set (for the same experiment on
ResNet, see Appendix D). We take the nature log-
arithm to better visualize the average F-norm of
Fisher information.
e-iou—ll.」e LlSizΦCT2Φ>< 6O-J

Our experiments reveal the widely known but
seldom understood fact that the standard accu-
racy of models under the two adversarial train-
ing algorithms are significantly lower than that
under normal training. According to our theory,
it can be explained as these models trained over
adversarial samples are unable to effectively rely
on adversary-prone non-robust components such
as Fisher information to distinguish the input
data from those belonging to other categories.
Till now, we theoretically and empirically demonstrate the relation between standard performance
and adversarial robustness using the proposed disentanglement theory.
3.4 Explanation from Geometry Point of View
We provide some explanations on the above findings from the information geometry viewpoint. We
note the model is doing maximum likelihood estimation by learning to fit the label distribution over
training data. This can be viewed as a process of the log-likelihood landscape of the model on input
data gradually transiting into a state where the model can well distinguish data of different categories.
Since the training data are sparsely sampled from the whole distribution, the smoothness prior does
not hold during the formation of the model’s log-likelihood landscape. Consequently, the model tends
to use lower order local geometric descriptors such as Fisher information —the local curvature of the
log-likelihood landscape —to form an overly simplified adversary-prone log-likelihood landscape.
When applying adversarial training, a strong smoothness constraint is enforced and the model would
have to rely on higher-order global geometric descriptors that vanish locally to form the whole
landscape. Thus the landscape would be more robust to adversarial perturbations.
6
Under review as a conference paper at ICLR 2020
Table 1: Standard accuracy and adversarial robustness of models of different capacity trained by
(Zhang et al., 2019). We set the maximum allowed L∞ norm of attack noise = 8/255. The learning
rate is 0.01 for all VGG13 models and 0.1 for all resnet models. All models are trained with SGD for
160 epochs with a decay of 10× in learning rate at 80th and 120th epochs. The SGD’s momentum
is 0.9. For adversarial training settings, the coefficient for the regularization term in (Zhang et al.,
2019) is 1 = 5.0, the step size for projected gradient descent is 2/255 and the number of step is 10.
The weight decay during training is 1e-4. For evaluation against adversarial attack, the step size for
PGD attack is 2/255, the number of steps is 20. The CW attack objective’s coefficient is c = 5e2 and
is also solved by PGD with the same optimization parameters. All experiments are conducted on
NVIDIA Tesla V100 GPUs.
	VGG13 pure linear	VGG13 half linear	VGG13 normal	resnet20x1	resnet32x10
accuracy (standard)	35.14	70.48	72.36	72.88	79.45
accuracy (PGD attack, = 8/255)	17.06	38.52	42.29	44.94	49.52
accuracy (CW attack, = 8/255)	13.94	34.12	38.96	41.49	47.67
3.5 ANALYSIS FOR NOT S UFFICIENTLY SMALL
Researchers have found that the local linearity does not hold when the norm of perturbation is
allowed to be relatively larger (Kurakin et al., 2016; Madry et al., 2017). In this case, some higher
order terms may also involve in the adversarial objective. However, since the norm of perturbation is
still considered to be small, the Taylor expansion terms of K L(f (x)kf (x + η)) would still vanish up
to a certain order K . Therefore, the adversarial objective can be rewritten as
K
max Lθ (x, η) = X Tx(k) (η) s.t. kηk ≤ .	(17)
η	k=2
Comparing (14) and (17), we know that the functional of the first K - 1 terms could still jointly
connect adversarial objective and CCKL. Also, we conduct numeric experiments in Appendix C
to show that the Fisher induced term is the dominant component in K L(f (x)kf (x + η)) within a
practical range of perturbation. We leave more detailed theoretic analysis on higher order terms
involved in the adversarial objective to future work.
4	Towards Simultaneous Good Performance and Robustness
With the disentanglement theory introduced above, we now consider how to achieve simultaneous
decent standard accuracy and adversarial robustness. According to our analysis, if relying on the
robust component alone can effectively distinguish data of different categories, then obtaining an
adversarial robust model with high standard accuracy is possible.
Since the expansion terms in the robust component are all higher order terms, we claim that the key to
achieving the two desired qualities simultaneously is to increase the expressive power of the model so
that it would be able to rely more on higher order terms for prediction. In this way, the model would
not have to rely heavily on Fisher information and still have decent standard performance with higher
order terms (the robust component). Our disentanglement provides theoretical justification for the
importance of model capacity in achieving adversarial robustness and decent standard performance.
We experiment on CIFAR-10 to validate our strategy of achieving the two objectives simultaneously.
We train the following models with TRADES algorithm (Zhang et al., 2019): a pure linear VGG13
model without ReLU and with average pooling, a half linear VGG13 model without ReLU but using
max pooling, a normal VGG 13 model, a normal resnet20 model and a resnet32 model with 10×
more channels. We evaluate these models on standard samples and adversarial samples produced
7
Under review as a conference paper at ICLR 2020
Figure 5: We sampled 25896 pairs of images (xi, xj) in CIFAR-10 test set (xi and xj belong to
different categories) and visualized the distribution of KL(f(xi)kf(xj)) (left), distribution of G2
(middle), and distribution of G2/KL(f(xi)kf(xj)) (right) of resnet20x1 and resnet32x10.
by L∞ PGD attack (Madry et al., 2017) and CW attack (Carlini & Wagner, 2017). The results are
provided in Table 1.
We first compare VGG models. Since adversarial training impedes the model from using the
adversary-prone lower order terms to discriminate input data, and the pure linear model is not
capable of utilizing higher order information, the standard generalization and adversarial robustness
of the pure linear model are both very poor. However, as the capacity of the model increases (half
linear and normal VGG13 models), the performance on standard and adversarial samples improves
simultaneously. For resnet models, when the model is shallower and narrower (resnet20x1), the
performance in both standard and adversarial settings is relatively low. However, with a deeper and
wider resnet model (resnet32x10), more higher order information is explored for prediction, so the
performance on standard samples and adversarial samples increases significantly.
In addition, we sampled 25896 pairs of images (xi, xj) in CIFAR-10 test set (xi and xj
belong to different categories) and visualized the distribution of K L(f (xi)kf (xj)), G2, and
G2/KL(f(xi)kf(xj)) with the above mentioned adversarially trained resnet20x1 and resnet32x10
model in Figure 5.
According to our results in Figure 5, we have the following observations. 1) The value of CCKL
of the resnet32x10 model on CIFAR-10 test set is much higher than that of resnet20x1, which
means the resnet32x10 can better distinguish input data from different categories. 2) The value
of G2 of resnet32x10 on the CIFAR-10 test set is significantly higher than that of resnet20x1,
which shows G2 plays a much more important role in resnet32x10 than resnet20x1. 3) The ratio of
G2/KL(f(xi)kf(xj)) of resnet32x10 on CIFAR-10 test set is much higher than that of resnet20x1,
meaning the relative importance of G2 of resnet32x10 is much greater than that of resnet20x1.
These observations validate our claim that a model with better capacity can more effectively leverage
higher order information to achieve higher standard accuracy and also adversarial robustness under
adversarial training.
5	Conclusion
In this work, we provide a novel viewpoint on the relation of standard accuracy and adversarial
robustness of deep learning models. We propose a new CCKL metric to measure the model perfor-
mance instead of accuracy. With CCKL, the overall performance objective can be disentangled into
an adversary-prone non-robust component, and a robust component. Based on such disentanglement,
we then claim that for achieving both adversarial robustness and decent standard accuracy, a DNN
model should rely more on the robust component to generalize. Our findings are well validated. In
the future, we will dig deeper along the geometric properties of the log-likelihood landscape formed
by DNNs on input data, and try to better characterize the relation between standard accuracy and
adversarial robustness.
8
Under review as a conference paper at ICLR 2020
References
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. arXiv
preprint arXiv:1902.03545, 2019.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in Neural Information Processing Systems,pp. 3981-3989, 2016.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian
walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 532-547, 2018.
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et al. Natural neural networks. In Advances
in Neural Information Processing Systems, pp. 2071-2079, 2015.
Dominik Maria Endres and Johannes E Schindelin. A new metric for probability distributions. IEEE
Transactions on Information theory, 2003.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. arXiv:1905.02175, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information
theory, 37(1):145-151, 1991.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2574-2582, 2016.
Preetum Nakkiran. Adversarial robustness may be at odds with simplicity. arXiv preprint
arXiv:1901.00532, 2019.
9
Under review as a conference paper at ICLR 2020
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium
on Security and Privacy (EuroS&P),pp. 372-387. IEEE, 2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016b.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances in Neural Information Processing
Systems, pp. 5014-5026, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition,, 2016. URL http://arxiv.org/abs/1512.00567.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. stat, 1050:11, 2018.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I
Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint
arXiv:1901.08573, 2019.
Chenxiao Zhao, P Thomas Fletcher, Mixue Yu, Yaxin Peng, Guixu Zhang, and Chaomin Shen. The ad-
versarial attack and detection under the fisher information metric. arXiv preprint arXiv:1810.03806,
2018.
10
Under review as a conference paper at ICLR 2020
A	Proof and comments of Theorem 3.1
In practical scenario, one cannot be too confident about a label y due to human annotation errors,
and often replaces it during training with smoothed version yLS using the label smoothing technique
Szegedy et al. (2016). Namely,
yLS = αy +(I - α)于，
K
where K is the number of classes, and α ∈ [0, 1] is the confidence level. The label smoothing version
of Theorem 3.1 still holds.
Proof of Theorem 3.1:
∀ data-label pair (xi,yi), (xj ,yj), where yi 6= yj , since JS1/2 satisfies triangular inequality (Endres
& Schindelin, 2003), we have:
JS(f(xi)kf(xj))1/2 + JS(yikf(xi))1/2 + JS(yjkf(xj))1/2
≥ JS(yikf(xj))1/2 + JS(yjkf(xj))1/2	(18)
≥ JS(yikyj)1/2
Also, we have the inequality:
(a + b + c)2 ≤ 2(a2 + b2 + c2).	(19)
With (18) and (19), we have:
2(JS(f(xi)kf(xj)) + JS(yikf(xi)) + JS(yjkf(xj)))
≥ (JS(f(xi)kf(xj))1/2 + JS(yikf(xi))1/2 + JS(yjkf(xj))1/2)2 (applying (19))	(20)
≥ JS(yikyj). (applying (18))
In addition, we have Lin’s inequality (Lin, 1991):
JS(Pkq) ≤ 1(KL(Pkq) + KL(qkp)).	(21)
Apply (21) to (20), we have:
JS(yikyj)
≤ 2(JS(f(xi)kf(xj)) + JS(yikf(xi)) + JS(yjkf(xj)))
≤ 2(KL(f(Xi)kf(xj)) + KL(f(xj)kf(xi))	(22)
+ KL(yikf(xi)) +KL(yjkf(xj))
+ KL(f(xi)kyi) +KL(f(xj)kyj))
Taking the expectation over the data set in (22), we have:
CCKL≥E∀yi6=yj[JS(yikyj)] - E[KL(ykf(x))] - E[KL(f(x)ky)],	(23)
which proves the theorem 3.1.
B Additional Results on CCKL as Performance Metric
We further empirically validate the relation between CCKL and standard cross entropy loss on test
set in Figure. 6. We trained a ResNet-20 on CIFAR-10 and CIFAR-100. The Pearson Correlation
Coefficients of CCKL and cross entropy loss on test set across the whole training period are -0.6744
and -0.8018 in these two settings, which further justify our use of CCKL as performance metric.
C Approximate KL Divergence Locally with Fisher
Experiment in Tab. 2 shows the relative numerical error of approximating KL locally with Fisher
induced term with a VGG13 model adversarially trained by (Zhang et al., 2019) on CIFAR-10. We
can observe that when is within practical range (≤ 8/255), the relative numeric error is small.
11
Under review as a conference paper at ICLR 2020
O IO 20	30	40	50	60	70 BO
Training Epoch
0	10	20	30	40	50	60	70 BO
Training Epoch
Figure 6: relation between CCKL and test loss of resnet20 on CIFAR-10 and CIFAR-100
Table 2: e and Gi (We denote Gi = ɪητBn here.)
e	1/255	2/255	3/255	4/255	5/255	6/255	7/255	8/255
|KL - Gi |/KL	0.058	0.098	0135	0.170	0.202	0.234	0.263	0.290
D	More Experiments on the Role of Fisher Information
We conduct more visualization experiments about the role of Fisher information in standard perfor-
mance of DNN. The results are shown in Figure 7 and Figure 8. The experiments are conducted on a
resnet20 model. The same conclusion could be drawn according to our statistics.
training epoch
Figure 7: Visualization of how standard accuracy and average F-norm of Fisher information matrix
on test set vary during training. The experiment is conducted on a resnet20 model and CIFAR-10
data set. We take the nature logarithm to better visualize the average F-norm of Fisher information
Also, to better understand the geometry of an adversarially trained model around robust samples and
non-robust samples, we visualized the F-norm of Fisher information at correct and robust samples
and correct but not robust samples of a VGG13 model. According to our results in Figure 9, we
could see that although the overall Fisher information is small, the Fisher information around robust
samples are still significantly smaller than that of non-robust samples.
12
Under review as a conference paper at ICLR 2020
^US3UΛ
0.2
0.1
O IO 20	30	40	50	60	70 BO
traininq epoch
UXJOU--oqs∙zΦCT2Φ>< 60-j
Figure 8: Visualization of how (left) standard test accuracy and (right) average F-norm of Fisher
Information Matrix on test set vary during normal training and adversarial training, with resnet20 on
CIFAR-10. We take the nature logarithm to better visualize the average F-norm of Fisher information
Figure 9: Distribution of F-norm of Fisher information at robust and non-robust samples on CIFAR-10
test set.
E	Interpretation from CRAMLR-RAO bound point of VIEW
According to the main text, adversarial training constrains the input-output Fisher information of a
DNN model. This constrain is a criteria of a good DNN model due to the following reasons. Recall
the well-known Cramer-Rao bound
Var(X)Fx ≥ 1
Says that if we try to use the output probability f (x) to a statistics X to reconstruct the input x, the
uncertainty in terms of variance Var(X) is bounded below by the inverse of Fisher information Fx.
For a DNN model that represents the reality, when it classifies an image with a correct label, say a
dog, the label does not have any information about the environments - what color the dog is, where is
the dog, adversarial perturbation, etc. Therefore, one cannot use the information contained in the
label to reconstruct the original image. This means that the variance Var(X) of any statistics X derived
from output distribution f(x) is relatively large for a good DNN model. in view of Cramer-Rao
bound, this implies that the Fisher information of a DNN is a relatively small value.
13