Under review as a conference paper at ICLR 2020
Towards Understanding Generalization
in Gradient-Based Meta-Learning
Anonymous authors
Paper under double-blind review
Ab stract
In this work we study generalization of neural networks in gradient-based meta-
learning by analyzing various properties of the objective landscapes. We exper-
imentally demonstrate that as meta-training progresses, the meta-test solutions
obtained by adapting the meta-train solution of the model to new tasks via few steps
of gradient-based fine-tuning, become flatter, lower in loss, and further away from
the meta-train solution. We also show that those meta-test solutions become flatter
even as generalization starts to degrade, thus providing an experimental evidence
against the correlation between generalization and flat minima in the paradigm of
gradient-based meta-leaning. Furthermore, we provide empirical evidence that gen-
eralization to new tasks is correlated with the coherence between their adaptation
trajectories in parameter space, measured by the average cosine similarity between
task-specific trajectory directions, starting from a same meta-train solution. We
also show that coherence of meta-test gradients, measured by the average inner
product between the task-specific gradient vectors evaluated at meta-train solution,
is also correlated with generalization.
1	Introduction
To address the problem of the few-shot learning, many meta-learning approaches have been proposed
recently (Finn et al., 2017), (Ravi and Larochelle, 2017), (Rothfuss et al., 2018), (Oreshkin et al.,
2018) and (Snell et al., 2017) among others. In this work, we take steps towards understanding the
characteristics of the landscapes of the loss functions, and their relation to generalization, in the
context of gradient-based few-shot meta-learning. While we are interested in understanding the
properties of optimization landscapes that are linked to generalization in gradient-based meta-learning
in general, we focus our experimental work here within a setup that follows the recently proposed
Model Agnostic Meta-Learning (MAML) algorithm (Finn et al., 2017). The MAML algorithm is
a good candidate for studying gradient-based meta-learning because of its independence from the
underlying network architecture.
Our main insights and contributions can be summarized as follows:
1.	As gradient-based meta-training progresses:
•	the adapted meta-test solutions become flatter on average, while the opposite occurs
when using a finetuning baseline.
•	the adapted final solutions reach lower average support loss values, which never
increases, while the opposite occurs when using a finetuning baseline.
2.	When generalization starts to degrade due to overtraining, meta-test solutions keep getting
flatter, implying that, in the context of gradient-based meta-learning, flatness of minima is
not correlated with generalization to new tasks.
3.	We empirically show that generalization to new tasks is correlated with the coherence
between their adaptation trajectories, measured by the average cosine similarity between
trajectory directions. Also correlated with generalization is the coherence between meta-
test gradients, measured by the average inner product between meta-test gradient vectors
evaluated at meta-train solution. We also show that this metric is correlated to generalization
for few-shot regression tasks where the model must learn to fit sine function curves.
1
Under review as a conference paper at ICLR 2020
Furthermore, based on these observations, we take initial steps to propose a regularizer for MAML
based training and provide experimental evidence for its effectiveness.
2	Related work
There has been extensive research efforts on studying the optimization landscapes of neural networks
in the standard supervised learning setup. Such work has focused on the presence of saddle points
versus local minima in high dimensional landscapes (Pascanu et al., 2014),(Dauphin et al., 2014),
the role of overparametrization in generalization (Freeman and Bruna, 2016), loss barriers between
minima and their connectivity along low loss paths, (Garipov et al., 2018); (Draxler et al., 2018), to
name a few examples.
One hypothesis that has gained popularity is that the flatness of minima of the loss function found
by stochastic gradient-based methods results in good generalization, (Hochreiter and Schmidhuber,
1997); (Keskar et al., 2016). (Xing et al., 2018) and (Li et al., 2017) measure the flatness by the
spectral norm of the hessian of the loss, with respect to the parameters, at a given point in the
parameter space. Both (Smith and Le, 2017) and (Jastrzebski et al., 2017) consider the determinant
of the hessian of the loss, with respect to the parameters, for the measure of flatness. For all of the
work on flatness of minima cited above, authors have found that flatter minima correlate with better
generalization.
In contrast to previous work on understanding the objective landscapes of neural networks in the
classical supervised learning paradigm, in our work, we explore the properties of objective landscapes
in the setting of gradient-based meta-learning.
3	Gradient-based meta-learning
We consider the meta-learning scenario where we have a distribution over tasks p(T), and a model f
parametrized by θ, that must learn to adapt to tasks Ti sampled from p(T). The model is trained on a
set of training tasks {Ti}train and evaluated on a set of testing tasks {Ti}test, all drawn from p(T).
In this work we only consider classification tasks, with {Ti }train and {Ti}test using disjoint sets of
classes to constitute their tasks. Here we consider the setting of k-shot learning, that is, when f adapts
to a task Titest, it only has access to a set of few support samples Di = {(xi(1), yi(1)), ..., (xi(k), yi(k))}
drawn from Titest . We then evaluate the model’s performance on Titest using a new set of target
samples Di0 . By gradient-based meta-learning, we imply that f is trained using information about the
gradient of a certain loss function L(f(Di; θ)) on the tasks. Throughout this work the loss function
is the cross-entropy between the predicted and true class.
3.1	Model-Agnostic Meta-Learning (MAML)
MAML learns an initial set of parameters θ such that on average, given a new task Titest , only a few
samples are required for f to learn and generalize well to that task. During a meta-training iteration
s, where the current parametrization of f is θs, a batch of n training tasks is sampled from p(T). For
each task Ti , a set of support samples Di is drawn and f adapts to Ti by performing T steps of full
batch gradient descent on L(f (Di ; θ)) w.r.t. θ, obtaining the adapted solution θi :
T-1
Gi = θs — α X VθL(f(Di; θ(t)))	(1)
t=0
where θi(t) = θi(t-1) - αVθL(f(Di; θi(t-1))) and all adaptations are independent and start from
θs, i.e. θi(0) = θs , ∀i. Then from each Ti, a set of target samples Di0 is drawn, and the adapted
meta-training solution θs+1 is obtained by averaging the target gradients, such that:
1n
θs+1 = θs - βn E VθL(f (Di; θi))	(2)
i=1
As one can see in Eq.1 and Eq.2, deriving the meta-gradients implies computing second-order
derivatives, which can come at a significant computational expense. The authors introduced a first-
2
Under review as a conference paper at ICLR 2020
order approximation of MAML, where these second-order derivatives are ommited, and we refer to
that other algorithm as First-Order MAML.
3.2	Finetuning baseline
For the finetuning baseline, the model is trained in a standard supervised learning setup: the model is
trained to classify all the classes from the training split using a stochastic gradient-based optimization
algorithm, its output layer size being equal to the number of meta-train classes. During evaluation on
meta-test tasks, the model’s final layer (fully-connected) is replaced by a layer with the appropriate
size for the given meta-test task (e.g. if 5-way classification, the output layer has five logits), with its
parameter values initialized to random values or with another initialization algorithm, then all the
model parameters are optimized to the meta-test task, just like for the other meta-learning algorithms.
4	Analyzing the objective landscapes
Figure 1: Visualizations of metrics measuring properties of objective loss landscapes. The black arrows
represent the descent on the support loss and the dotted lines represent the corresponding displacement in the
parameter space. (1): Curvature of the loss for an adapted meta-test solution θi (for a task Ti), is measured
as the spectral norm of the hessian matrix of the loss. (2): Coherence of adaptation trajectories to different
meta-test tasks is measured as the average cosine similarity for pairs of trajectory directions. A direction vector
is obtained by dividing a trajectory displacement vector (from meta-train solution θs to meta-test solution
ss
θi) by its Euclidean norm, i.e. θi = (θi - θs)/kθi - θs k2. (3): Characterizing a meta-train solution by the
coherence of the meta-test gradients, measured by the average inner product for pairs of meta-test gradient
vectors gi = -VθL(f(Di; θs)).
In the context of gradient-based meta-learning, we define generalization as the model’s ability to
reach a high accuracy on a testing task Titest , evaluated with a set of target samples Di0 , for several
testing tasks. This accuracy is computed after f, starting from a given meta-training parametriza-
tion θs, has optimized its parameters to the task Titest using only a small set of support samples
Di , resulting in the adapted solution θitest (minima). We thus care about the average accuracy
ETtest^p(τ)[Acc(f (Di; θtest)]. With these definitions in mind, for many meta-test tasks Titest, We
consider the optimization landscapes L(f (Di; θ)), and 1) the properties of these loss landscapes
evaluated at the solutions θitest ; 2) the adaptation trajectories When f, starting from θs , adapts to
those solutions; as Well as 3) the properties of those landscapes evaluated at the meta-train solutions
θs. See Figure 1 for a visualization of our different metrics. We folloW the evolution of the metrics
as meta-training progresses: after each epoch, Which results in a different parametrization θs , We
adapt f to several meta-test tasks, compute the metrics averaged over those tasks, and compare With
E[Acc(f (Di;京test)]. We do not deal with the objective landscapes involved during meta-training, as
this is beyond the scope of this Work. From here on, We drop the superscript test from our notation, as
we exclusively deal with objective landscapes involving meta-test tasks Ti, unless specified otherwise.
3
Under review as a conference paper at ICLR 2020
4.1	Flatnes s of minima
We start our analysis of the objective loss landscapes by measuring properties of the landscapes at
the adapted meta-test solutions θ%. More concretely, We measure the curvature of the loss at those
minima, and whether flatter minima are indicative of better generalization for the meta-test tasks.
After s meta-training iterations, We have a model f parametrized by θs . During the meta-test, f
must adapt to several meta-test tasks Ti independently. For a given Ti , f adapts by performing a
feW steps of full-batch gradient descent on the objective landscape L(f(Di; θ)), using the set of
support samples Di , and reaches an adapted solution θi . Here We are interested in the curvature
of L(f(Di; θi)), that is, the objective landscape When evaluated at such solution, and Whether on
average, flatter solutions favour better generalization. Considering the hessian matrix of this loss W.r.t
.
the model parameters, defined as H (Di； θi) = W2L(f (Di; %)), we measure the curvature of the
loss surface around θi using the spectral norm ∣∣ ∙ ∣∣σ of this hessian matrix:
∣∣Hθ (Di； ^i)∣lσ = Jλmaχ (Hθ (Di； θi)HHθ (Di； θi)) = λmaχ(Hθ (Di； θi))	(3)
as illustrated in Figure 1 (1). (WegetkHθ (Di； θi)kσ = λmaχ(Hθ (Di ； θi)) since H (Di； θi) is real
and symmetric.)
We define the average loss curvature for meta-test solutions θi, obtained from a meta-train solution
θs, as:
—	…___,	_	≈	.	. .	r
ETi〜P(T) [∣Hθ(Di； θi)kσ]	(4)
Note that we do not measure curvature of the loss at θs, since θs is not a point of convergence of f
for the meta-test tasks. In fact, at θs , since the model has not been adapted to the unseen meta-test
classes, the target accuracy for the meta-test tasks is random chance on average. Thus, measuring
the curvature of the meta-test support loss at θs does not relate to the notion of flatness of minima.
Instead, in this work we characterize the meta-train solution θs by measuring the average inner
product between the meta-test gradients, as explained later in Section 4.3.
4.2	Coherence of adaptation trajectories
Other than analyzing the objective landscapes at the different minima reached when f adapts to new
tasks, we also analyze the adaptation trajectories to those new tasks, and whether some similarity
between them can be indicative of good generalization. Let’s consider a model f adapting to a task Ti
by starting from θs, moving in parameter space by performing T steps of full-batch gradient descent
with VθL(f (Di； θ)) until reaching θi. We define the adaptation trajectory to a task Ti starting from
θs as the sequence of iterates (θs, θi(1), θi(2), ..., θθi). To simplify the analyses and alleviate some of the
challenges in dealing with trajectories of multiple steps in a parameter space of very high dimension,
we define the trajectory displacement vector (θi - θs). We define a trajectory direction vector θi as
the unit vector: ~i = (θi 一 θs)∕∣θi - θs∣2.
We define a metric for the coherence of adaptation trajectories to meta-test tasks Ti, starting from a
meta-train solution θs, as the average inner product between their direction vectors:
Eτi,TCp(T )[~iT ~j ]	(5)
The inner product between two meta-test trajectory direction vectors is illustrated in Figure 1 (2).
4.3	Characterizing meta-train solutions by the average inner product between
meta-test gradients
In addition to characterizing the adaptation trajectories at meta-test time, we characterize the objective
landscapes at the meta-train solutions θs. More concretely, we measure the coherence of the meta-test
gradients VθL(f (Di； θs)) evaluated at θs.
The coherence between the meta-test gradients can be viewed in relation to the metric for coherence
of adaptation trajectories of Eq. 5 from Section 4.2. Even after simplifying an adaptation trajectory by
4
Under review as a conference paper at ICLR 2020
its displacement vector, measuring distances between trajectories of multiple steps in the parameter
space can be problematic: because of the symmetries within the architectures of neural networks,
where neurons can be permuted, different parameterizations θ can represent identically the same
function f that maps inputs to outputs. This problem is even more prevalent for networks with
higher number of parameters. Since here we ultimately care about the functional differences that
f undergoes in the adaptation trajectories, measuring distances between functions in the parameter
space, either using Euclidean norm or cosine similarity between direction vectors, can be problematic
(Benjamin et al., 2018).
Thus to further simplify the analyses on adaptation trajectories, we can measure coherence between
trajectories of only one step (T = 1). Since we are interested in the relation between such trajectories
and the generalization performance of the models, we measure the target accuracy at those meta-test
solutions obtained after only one step of gradient descent. We define those solutions as: θs + α ∙ gi,
with meta-test gradient gi = -▽6L(f (Di； θs)). To make meta-training consistent with meta-testing,
for the meta-learning algorithms we also use T = 1 for the inner loop updates of Eq. 1.
We thus measure coherence between the meta-test gradient vectors gi that lead to those solutions.
Note that the learning rate α is constant and is the same for all experiments on a same dataset. In
contrast to Section 4.2, here we observed in practice that the average inner product between meta-test
gradient vectors, and not just their direction vectors, is more correlated to the average target accuracy.
The resulting metric is thus the average inner product between meta-test gradients evaluated at θs .
We define the average inner product between meta-test gradient vectors gi, evaluated at a meta-train
solution θs, as:
ETi,Tj〜P(T)[ giTgj ]	⑹
The inner product between two meta-test gradients, evaluated at θs, is illustrated in Figure 1 (3).
We show in the experimental results in Section 5.2 and 5.3 that the coherence of the adaptation
trajectories, as well as of the meta-test gradients, correlate with generalization on the meta-test tasks.
5	Experiments
We apply our analyses to the two most widely used benchmark datasets for few-shot classification
problems: Omniglot and MiniImagenet datasets. We use the standardized CNN architecture used
by (Vinyals et al., 2016) and (Finn et al., 2017). We perform our experiments using three different
gradient-based meta-learning algorithms: MAML, First-Order MAML and a Finetuning baseline.
For more details on the meta-learning datasets, architecture and meta-learning hyperparameters, see
Appendix A
We closely follow the experimental setup of (Finn et al., 2017). Except for the Finetune baseline, the
meta-learning algorithms use during meta-training the same number of ways and shots as during meta-
testing. For our experiments, we follow the setting of (Vinyals et al., 2016): for MiniImagenet, training
and testing our models on 5-way classification 1-shot learning, as well as 5-way 5-shot, and for
Omniglot, 5-way 1-shot; 5-way 5-shot; 20-way 1-shot; 20-way 5-shot. Each experiment was repeated
for five independent runs. For the meta-learning algorithms, the choice of hyperparameters closely
follows (Finn et al., 2017). For our finetuning baseline, most of the original MAML hyperparameters
were left unchanged, as we want to compare the effect of the pre-training procedure, thus are kept
fixed the architecture and meta-test procedures. We kept the same optimizer as for the meta-update
of MAML (ADAM), and performed hyperparameter search on the mini-batch size to use, for each
setting that we present. (For our reproduction results on the meta-train and meta-test accuracy, see
Figure 10a and 10b in B.1.)
5.1	Flatnes s of meta-test solutions
After each training epoch, we compute E[kHθ(Di; θi)kσ] using a fixed set of 60 randomly sampled
meta-test tasks Ti . Across all settings, we observe that MAML first finds sharper solutions θi until
reaching a peak, then as the number of epoch grows, those solutions become flatter, as seen in Figure
2. To verify the correlation between E[kHθ(Di; θi)kσ] and E[Acc(f (Di0; θi))], we train models for
an extra number of epochs until clearly observing a decrease in the generalization performance
E[Acc(f (Di0; θi))], using First-Order MAML with 5-way 1-shot learning on MiniImagenet, and we
5
Under review as a conference paper at ICLR 2020
(a) Omniglot 5-way
(b) Omniglot 20-way
(c) MiniImagenet
5-way, 1-shot
(d) MiniImagenet
5-way, 5-shot
Figure 2: Flatness of meta-test solutions for MAML and First-Order MAML, on Omniglot and
MiniImagenet
verify if it is reflected by an increase in E[kHθ(Di; θi)kσ]. On the contrary, and remarkably, even as
f starts to show poorer generalization (see Figure 3a), the solutions keep getting flatter, as shown in
Figure 3c. Thus for the case of gradient-based meta-learning, flatter minima don’t appear to favour
better generalization. We perform the same analysis for our finetuning baseline (Figures 4a, 4c), with
results suggesting that flatness of solutions might be more linked with E[L(f(Di; θi))], the average
level of support loss attained by the solutions θi (see Figures 4b and 3b), which is not an indicator for
generalization. We also noted that across all settings involving MAML and First-Order MAML, this
average meta-test support loss E[L(f (Di ; θi))] decreases monotonically as meta-training progresses.
(a) Target Accuracy	(b) Support loss
(c) Curvature of solutions
0.290
为0.285
S 0.280
y 0.275
A 0.270
⅛ 0-265
0.260
0.255
1	20	40	60	80	100
Epoch
(a) Target accuracy
Figure 3: MAML: Characterization of meta-test solutions
Epoch
(b) Support loss
70
65
■产 TeT
——I
1	20	40	60	80	100
Epoch
(c) Curvature of solutions
Figure 4: Finetune baseline : Characterization of meta-test solutions
5.2 Coherence of adaptation trajectories
In this section, we use the same experimental setup as in Section 5.1, except here we measure
E[θ~i Tθ~j]. To reduce the variance on our results, we sample 500 tasks after each meta-training epoch.
Also for experiments on Omniglot, we drop the analyses with First-Order MAML, since it yields
performance very similar to that of the Second-Order MAML. We start our analyses with the setting
of "MiniImagenet, First-Order MAML, 5-way 1-shot", as it allowed us to test and invalidate the
correlation between flatness of solutions and generalization, earlier in Section 5.1.
6
Under review as a conference paper at ICLR 2020
(a) MiniImagenet, 5-way, 1-shot, First-Order
Figure 5: Comparison between average inner product between meta-test trajectory direction vectors
(orange), and average target accuracy on meta-test tasks (blue), MAML First-Order and Second-Order,
MiniImagenet 5-way 1-shot. See Figure 11 in Appendix B.2 for full set of experiments.
(b) MiniImagenet, 5-way, 1-shot, Second-Order
We clearly observe a correlation between the coherence of adaptation trajectories and generalization
to new tasks, with higher average inner product between trajectory directions, thus smaller angles,
being linked to higher average target accuracy on those new tasks, as shown in Figure 5a. We then
performed the analysis on the other settings, with the same observations (see Figure 5b and Figure 11
in Appendix B.2 for full set of experiments). We also perform the analysis on the Finetuning baselines,
which reach much lower target accuracies, and where we see that E[θ~i Tθ~j] remains much closer to
zero, meaning that trajectory directions are roughly orthogonal to each other, akin to random vectors
in high dimension (see Figure 6a). As an added observation, here we include our experimental results
on the average meta-test trajectory norm E[k/ 一θs∣∣2], in Figure 6c and 6d, where E[k/ 一θs∣3
grows as meta-training progresses when f is meta-trained with MAML, as opposed to the Finetune
baseline, and note that this norm does not reflect generalization.
0.07
0.06
净.05
* 0.04
0.02
Epoch
5-wβy, 1-shot
5-way, 5-shot
0.0025
百 0.0020
.0015
0.0010
0.0005
^20^
^40^
^βδ^
5-way, ι-shot
5-way, 5-shot
80	100
Epoch
I
6
L∑=t
0∙451 - MAMl	- "AML
FllSt-Ol⅛β∙MAML	0.7 FIlW-OrtWMAMl
0∙25, 泮BS产产 ≡- WKJ
*20	40	60	∞^
Epoch
100
^S 40	∞	80^
Epoch
100


(a) Trajectories coherence
(b) Gradients coherence (c) l2 norm of trajectories(d) l2 norm of trajectories
(1-shot)
(5-shot)
Figure 6:	(a): Average inner product between meta-test adaptation direction vectors, for Finetuning
baseline on MiniImagenet. (b): Average inner product between meta-test gradients, for Finetuning
baseline on MiniImagenet. Average l2 norm of meta-test adaptation trajectories, all algorithms on
MiniImagenet, (c): 1-shot learning, (d): 5-shot learning.
5.3 Characterizing meta-train solutions by the average inner product between
meta-test gradients
Despite the clear correlation between E[θ~i T θ~j] and generalization for the settings that we show
in Figure 5 and 11, we observed that for some other settings, this relationship appears less linear.
We conjecture that such behavior might arise from the difficulties of measuring distances between
networks in the parameter space, as explained in Section 4.3. Here we present our results on the
characterization of the objective landscapes at the meta-train solutions θs, by measuring the average
inner product between meta-test gradient vectors gi .
We observe that coherence between meta-test gradients is correlated to generalization, which is
consistent with the observations on the coherence of adaptation trajectories from Section 5.2. In
Figure 7, we compare E[ giT gj ] to the target accuracy (here we show results for individual model
runs rather than the averages over the runs). See Figure 12 in Appendix B.3 for the full set of
7
Under review as a conference paper at ICLR 2020
x3eJn33v-a6JM_L
0.55
0.50
0.45
0.40
(a) MiniImagenet, 5-way, 5-shot, First-Order (b) MiniImagenet, 5-way, 5-shot, Second-Order
Figure 7:	Comparison between average inner product between meta-test gradient vectors, evaluated at
meta-train solution, and average target accuracy on meta-test tasks, with higher average inner product
being linked to better generalization. See Figure 12 in Appendix B.3 for full set of experiments.
experiments. This metric consistently correlates with generalization across the different settings.
Similarly as in Section 5.2, for our finetuning baselines we observe very low coherence between
meta-test gradients (see Figure 6b). Based on the observations we make in Section 5.2 and 5.3, we
propose to regularize gradient-based meta-learning as described in Section 6.
5.3.1 Few-shot regression: Average inner product between meta-test gradients
Here we extend our analysis by presenting experimental results on E[ giT gj ] for few-shot regression.
Specifically we use a leaning problem which is composed of training task and test tasks, where each
of these tasks are sine functions parameterized as y = a sin(bx + c) . We train a two-layer MLP
which learns to fit meta-training sine functions using only few support samples, and generalization
implies reaching a low Mean Squared Error (MSE) averaged over the target set of many meta-test
sine functions. Results are presented in Figure 8. Similar to our analysis of Few-shot classification
setting, we observe in the case of Few-shot regression, generalization (negative average target MSE
on Meta-test Task) strongly correlates with E[ giT gj ]. See Appendix A.4 for the experimental
details.
-7
— 5-shot, 1 task per batch
Q∙β8
阴
金
3 0.06
0.04
—— 5τhot, 1 task per batch
⅛ so i≡ ɪso an so
Epoch
(b)
⅛ SO MO So ® SO
Epoch
(C)
-2	-1
et MSE
Figure 8: Analysis for Few-shot regression. Comparison between E[ giT gj ] and average negative
target Mean Squared Error on meta-test tasks(generalization performance). (a) and (b) show general-
ization performance correlates with E[ giT gj ] through-out the meta-training (c) and (d) show the
correlation across many values of k (number of shots), while (e) shows the correlation coefficient R
between E[ giT gj ] and final generalization performance, for models with k varying between 2 and
15.
O SO MO ISO as 2»
Epoch
⑹

6 First steps toward s regularizing MAML
Although, MAML has become a popular method for meta-training, there exist a significant gener-
alization gap between its performance on target set of the meta-train tasks and the target set of the
meta-test task, and regularizing MAML has not received much research attention yet. Based on our
observations on the coherence of adaptation trajectories, we take first steps in this direction by adding
a regularization term based on E[θ~i Tθ~j] . Within a meta-training iteration, we first let f adapt to the
n training tasks Ti following Eq 1. We then compute the average direction vector θμ = n P3 ~i.
T
For each task, we want to reduce the angle defined by θi Tθμ, and thus introduce the penalty on
8
Under review as a conference paper at ICLR 2020
Ω(θ) = -~i Tθμ, obtaining the regularized solutions θi. The outer loop gradients are then computed,
just like in MAML following Eq 2, but using these regularized solutions θi instead of θi . We obtain
the variant of MAML with regularized inner loop updates, as detailed in Algorithm 1. We used this
regularizer with MAML (Second-Order), for "Omniglot 20-way 1-shot", thereby tackling the most
challenging few-shot classification setting for Omniglot. As shown in Figure 9, we observed an
increase in meta-test target accuracy: the performance increases from 94.05% to 95.38% (average
over five trials, 600 test tasks each), providing 〜23% relative reduction in meta-test target error.
Algorithm 1 Regularized MAML: Added penalty
on angles between inner loop updates
1:	Sample a batch of n tasks Ti 〜p(T)
2:	for all Ti do
3:	Perform inner loop adaptation as in Eq. 1:
θi = θs - α PT=。1 VθL(f(A; θ(t)))
4:	end for
5:	Compute the average direction vector:
~μ = nl Pi= ~i
6:	Compute the corrected inner loop updates:
7:	for all Ti do
8:	θi = θi-γVθΩ(θ) whereΩ(θ) = -θiτθμ
9:	end for
10:	Perform the meta-update as in Eq. 2, but using
the corrected solutions:
θs+1 = θs — β 1 Pi=ι VθL(f(Di; θi))
Figure 9: Average target accuracy on meta-test
tasks using our proposed regularizer on MAML,
for Omniglot 20-way 1-shot learning, with regular-
ization coefficient Y = 0.5
7 Conclusion
We experimentally demonstrate that when using gradient-based meta-learning algorithms such as
MAML, meta-test solutions, obtained after adapting neural networks to new tasks via few-shot
learning, become flatter, lower in loss, and further away from the meta-train solution, as meta-
training progresses. We also show that those meta-test solutions keep getting flatter even when
generalization starts to degrade, thus providing an experimental argument against the correlation
between generalization and flat minima. More importantly, we empirically show that generalization
to new tasks is correlated with the coherence between their adaptation trajectories, measured by
the average cosine similarity between the adaptation trajectory directions, but also correlated with
the coherence between the meta-test gradients, measured by the average inner product between
meta-test gradient vectors evaluated at meta-train solution. We also show this correlation for few-shot
regression tasks. Based on these observations, we take first steps towards regularizing MAML based
meta-training. As a future work, we plan to test the effectiveness of this regularizer on various datasets
and meta-learning problem settings, architectures and gradient-based meta-learning algorithms.
9
Under review as a conference paper at ICLR 2020
References
Benjamin, A. S., Rolnick, D., and Kording, K. P. (2018). Measuring and regularizing networks in
function space. CoRR, abs/1805.08289.
Dauphin, Y., Pascanu, R., GUlgehre, ¢., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying
and attacking the saddle point problem in high-dimensional non-convex optimization. CoRR,
abs/1406.2572.
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F. A. (2018). Essentially No Barriers in
Neural Network Energy Landscape. ArXiv e-prints.
Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep
networks. CoRR, abs/1703.03400.
Freeman, C. D. and Bruna, J. (2016). Topology and Geometry of Half-Rectified Network Optimiza-
tion. ArXiv e-prints.
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D., and Wilson, A. G. (2018). Loss Surfaces,
Mode Connectivity, and Fast Ensembling of DNNs. ArXiv e-prints.
Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural
networks. In Teh, Y. W. and Titterington, M., editors, Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning
Research, pages 249-256, Chia Laguna Resort, Sardinia, Italy. PMLR.
Hochreiter, S. and Schmidhuber, J. (1997). Flat minima. Neural ComPut, 9(1):1T2.
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A. J. (2017).
Three factors influencing minima in SGD. CoRR, abs/1711.04623.
Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2016). On large-batch
training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836.
Li, H., Xu, Z., Taylor, G., and Goldstein, T. (2017). Visualizing the loss landscape of neural nets.
CoRR, abs/1712.09913.
Oreshkin, B. N., L6pez, P. R., and Lacoste, A. (2018). TADAM: task dependent adaptive metric for
improved few-shot learning. CoRR, abs/1805.10123.
Pascanu, R., Dauphin, Y. N., Ganguli, S., and Bengio, Y. (2014). On the saddle point problem for
non-convex optimization. CoRR, abs/1405.4604.
Ravi, S. and Larochelle, H. (2017). Optimization as a model for few-shot learning. In 5th Interna-
tional Conference on Learning RePresentations, ICLR 2017, Toulon, France, APril 24-26, 2017,
Conference Track Proceedings.
Rothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P. (2018). Promp: Proximal meta-policy
search. CoRR, abs/1810.06784.
Smith, S. L. and Le, Q. V. (2017). A bayesian perspective on generalization and stochastic gradient
descent. CoRR, abs/1710.06451.
Snell, J., Swersky, K., and Zemel, R. S. (2017). Prototypical networks for few-shot learning. CoRR,
abs/1703.05175.
Vinyals, O., Blundell, C., Lillicrap, T. P., Kavukcuoglu, K., and Wierstra, D. (2016). Matching
networks for one shot learning. CoRR, abs/1606.04080.
Xing, C., Arpit, D., Tsirigotis, C., and Bengio, Y. (2018). A Walk with SGD. ArXiv e-Prints.
10
Under review as a conference paper at ICLR 2020
A Additional Experimental Details
A.1 Model Architectures
We use the architecture proposed by (Vinyals et al., 2016) which is used by (Finn et al., 2017),
consisting of 4 modules stacked on each other, each being composed of 64 filters of of 3 × 3
convolution, followed by a batch normalization layer, a ReLU activation layer, and a 2 × 2 max-
pooling layer. With Omniglot, strided convolution is used instead of max-pooling, and images are
downsampled to 28 × 28. With MiniImagenet, we used fewer filters to reduce overfitting, but used 48
while MAML used 32. As a loss function to minimize, we use cross-entropy between the predicted
classes and the target classes.
A.2 Meta-Learning datasets
The Omniglot dataset consists of a total of 1623 classes, each comprising 20 instances. The classes
correspond to distinct characters, taken from 50 different datasets, but the taxonomy among characters
isn’t used. The MiniImagenet dataset comprises 64 training classes, 12 validation classes and 24 test
classes. Each of those classes was randomly sampled from the original Imagenet dataset, and each
contains 600 instances with a reduced size of 84 × 84.
A.3 Hyperparameters used in meta-training and meta-testing for few-shot
CLASSIFICATION
We follow the same experimental setup as (Finn et al., 2017) for training and testing the models using
MAML and First-Order MAML. During meta-training, the inner loop updates are performed via
five steps of full batch gradient descent (except for Section 5.3 where T = 1), with a fixed learning
rate α of 0.1 for Omniglot and 0.01 for MiniImagenet, while ADAM is used as the optimizer for
the meta-update, without any learning rate scheduling, using a meta-learning rate β of 0.001. At
meta-test time, adaptation to meta-test task is always performed by performing the same number
of steps as for the meta-training inner loop updates. We use a mini-batch of 16 and 8 tasks for the
1-shot and 5-shot settings respectively, while for the MiniImagenet experiments, we use batches of 4
and 2 tasks for the 1-shot and 5-shots settings respectively. Let’s also precise that, in k-shot learning
for an m-way classification task Ti , the set of support samples Di comprises k × m samples. Each
meta-training epoch comprises 500 meta-training iterations.
For the finetuning baseline, we kept the same hyperparameters for the ADAM optimizer during
meta-training, and for the adaptation during meta-test. We searched the training hyperparameter
values for the mini-batch size and the number of iterations per epoch. Experiments are run for a 100
epochs each. In order to limit meta-overfitting and maximize the highest average meta-test target
accuracy, the finetuning models see roughly 100 times less training data per epoch compared to a
MAML training epoch. In order to evaluate the baseline on the 1-shot and 5-shot meta-test tasks,
during training we used mini-batches of 64 images with 25 iterations per epoch for 1-shot learning,
and mini-batches of 128 images with 12 iterations per epoch, for 5-shot learning. At meta-test time,
we use Xavier initialization (Glorot and Bengio, 2010) to initialize the weights of the final layer.
A.4 Experimental details for few- shot regression
For the few-shot regression problems (which is also present in the work of (Finn et al., 2017)), we
use a fully-connected architecture of two hidden layers, 40 neurons wide. We use the Mean Square
Error as the loss function. Tasks consists of fitting one dimensional sine functions evaluated on the
domain [-5, 5], Here sine functions vary in amplitude and phase, and meta-train and meta-test sine
functions are generated with disjoint ranges of amplitude and phase.
11
Under review as a conference paper at ICLR 2020
B	Additional Experimental Results
B.1	Performance of models trained with MAML and First-Order MAML, on the
few- shot learning settings
The performance of the models trained with MAML and First-Order MAML, for the few-shot
learning settings of Omniglot and MiniImagenet, are presented in Figure 10. They include the target
accuracies on meta-train tasks and on meta-test tasks (generalization), as meta-training progresses.
■ ■ ■
Ooo
Av33V u,J,L∙⅞
---mlnHmagenet_5_way_5_shot_
-mlnMmagenet_5_way_5_shot_flrst-erder_
---mlnMmagenet_5_way_l_shot_
---mlnMmagenet_5_way_l_shot_flrst-erder_
1.00
0.95
δ∙0.90
v 0.85
δ0∙80
0-60
0-55
0-45
⅛4 0.75	---- omnlgtot_5_way_5_shot_
0.40
----omnlgtot_5_way_5_shot_flrst-flr<ier_
0.70	--- omnlgtot_20_way_l_shot_
0-50
----OmnIgIOt_5_Way-I_Shot_
----omnlgtot_5_way_l_shot_flrst-flr<ler_
----omnlgtot_20_way_l_shot_flrst-flr<ier_
omnlgtot_20_way_5_shot_
----omnlgtot_20_way_5_shot_flrst-flr<ier_
0.35
1.00
K 0.90
W 0.85
iHmagenet_5_way_5_shot_
il-lmagenet_5_way_5_shot_flrst-orcter_
il-lniagenet_5_way_l_shot_
∣l-l magenet_5_way_l_shot_flrst-order_
0.65
0.30
0.70
0.95
⅛0-80
0.75
10 20 30 40 50 ¢0 70 80 90
Epoch
----omnlglot_5_way_l_shot_
omnlglot_5_way_l_shot_flrst-orcler_
----oninlglot_5_way_5_shrt_
----oπι nlglot_5_way_5_shot_flrst-order_
----omnlglot_20_w8y_l_shot_
----omnlglot_20_way_l_shot_flrst-onier_
omnlglot_20_w8y_5_shot_
----oninlglot 20 way 5 shot flrst-orcler
Epoch
1 10 20 30 40 50 e
Epoch

(a) Meta-Train Accuracy	(b) Meta-Test Accuracy
Figure 10:	MAML: Accuracies on training and testing tasks
B.2	Coherence of adaptation trajectories
The relation between target accuracy on meta-test tasks, and angles between trajectory directions is
presented in Figure 11.
B.3	Average inner product between meta-test gradients
The relation between target accuracy on meta-test tasks, and average inner product between meta-test
gradients evaluated at meta-train solution, is presented in Figure 12.
12
Under review as a conference paper at ICLR 2020
S t
0.4B
0.461
0.44
(a) MiniImagenet, 5-way, 1-shot, First-Order (b) MiniImagenet, 5-way, 1-shot, Second-Order
0.12
h-
叵
⅛
0.00
0.06
1 io
90 IOO
J 50 60
Epoch
0.995
0.990
∣0.9B5
^0.980
¢0.975
0.970
0.965
1 10 2 0 30 40 50 60 70 80 90 IOO
Epoch
0.9B
0.96
1 10 20 30 40 50 60 70 80 90 IOO	1 10 20 30 40 50 60 70 80 90 IOO
Epoch	Epoch
(d)	Omniglot, 20-way, 5-shot, Second-Order
早"E ▼，［
A3eJn33v-ja6JeJ.
(c)	Omniglot, 5-way, 5-shot, Second-Order
Figure 11:	Comparison between average inner product between trajectory directions and average
target accuracy on meta-test tasks. Full set of experiments.
13
Under review as a conference paper at ICLR 2020
(a) MiniImagenet, 5-way, 5-shot, First-Order
(b) MiniImagenet, 5-way, 5-shot, Second-Order
(c) MiniImagenet, 5-way, 1-shot, First-Order
°-°-
A3ejn8vseJ.
seed o
seed 1
seed2
seed3
seed4
10 20 30 40 50 60 70 80 90 1∞
Epoch
0.1β
0.16
0.14
忑
⅛012
⅛
0.10-
0.0θ
0.06
(d) MiniImagenet, 5-way, 1-shot, Second-Order
1 10 20 30 40 50 e
Epoch
10 20 30 40 50 60 70 80 90 1∞
Epoch
(f) Omniglot, 20-way, 5-shot, Second-Order
Figure 12: Comparison between average inner product between trajectory displacement vectors, and
average target accuracy on meta-test tasks. Full set of experiments.
(e) Omniglot, 20-way, 1-shot, Second-Order
14