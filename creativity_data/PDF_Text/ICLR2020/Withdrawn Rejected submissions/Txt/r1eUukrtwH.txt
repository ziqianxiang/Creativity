Under review as a conference paper at ICLR 2020
The Variational InfoMax AutoEncoder
Anonymous authors
Paper under double-blind review
Ab stract
The Variational AutoEncoder (VAE) can learn simultaneously a representation
and a generative model, but often, in order to generate sharper samples, a useless
representation is learnt. Starting from the observation that a meaningful represen-
tation is an informative one, in this work we analyse the VAE from an information
theoretic perspective. We define the concept of capacity for the variational net-
work, and we associate the two tasks of VAE: generation and representation qual-
ity, to two information properties of the network: information of the generative
model and capacity of the network. Then we suggest that an optimal generative
model is the one optimising the Capacity-Constrained InfoMax (CCIM), a theo-
retical objective learning the maximal informative generative model while main-
taining bounded the network capacity. The theoretical assumptions are confirmed
by the computational experiments, where between the different families of VAE,
the one optimising a variational lower bound of the CCIM generates sharper sam-
ples and learns a clustered and robust representation. We call this the Variational
InfoMax AutoEncoder (VIMAE).
1	Introduction
A common assumption in machine learning is that any visible data x ∈ X is completely described
by some generative factor o, living in a smaller hidden space O, i.e. x = g(o) with g a (possibly
stochastic) generative function. The aim of unsupervised representation learning research is to find
a representation z of the generative factor o living in a known space Z describing, as well as o, the
visible data x. This is particularly relevant because the learnt small representation z is task agnostic
and, in principle, can be used as input for networks performing different tasks, leading to faster and
more robust learning (generalisation property), (Rifai et al., 2011).
Many models fφ : X → Z trying to learn such representations have been proposed (Dinh et al.,
2016; Hinton et al., 2006; Maddison et al., 2017; Radford et al., 2015), but recently in order to solve
this problem it was proposed to consider a dual problem: define a priori z and find a generator
map gθ, such that for any z, gθ (z) is an element of X . In particular, two families of probabilistic
generative models have become dominant: Variational AutoEncoder (VAE) (Kingma & Welling,
2013; Rezende et al., 2014) and Generative Adversarial Network (GAN) (Goodfellow et al., 2014).
The common idea of the two approaches is that a good generator pθ (x|z) is the one able to generate
the data that is as close as possible to the visible one, i.e. that with respect a certain metric D, the
distance between the marginalpθ(x) = Ep(z)[pθ (x|z)] and the visible distributionPD (x) is minimal.
In this manuscript we restrict our attention to the VAE model, since by its architecture, it is the
only one where the learnt representation, possibly from different datasets, can be used as input for
networks performing different tasks, (Achille et al., 2018; Ramapuram et al., 2017). Although VAE,
by its training robustness and general good generative performance is the most popular model for
representation learning, in particular cases it suffers from the uninformative representation issue: the
representation is entangled and the generative model tends to be independent of z, i.e. pθ(x|z) ≈
pθ (x). As highlighted in the next section such behaviour is intrinsic in the variational loss, the
Evidence Lower BOund (ELBO), encouraging a less informative representation.
Following the direction suggested in (Alemi et al., 2017; Zhao et al., 2017) we describe the VAE
from an information theoretic perspective. Such description lead us to two observations: the WAE
(Tolstikhin et al., 2017) and InfoVAE (Zhao et al., 2017) models are actually maximising a lower
bound of the mutual information associated to a generator pθ(x|z) belonging in a certain family
1
Under review as a conference paper at ICLR 2020
P , the Variational InfoMax (VIM); and given that the capacity of the network is a function of
the entropy of the prior p(z), the VIM is the variational expression of the Capacity-Constrained
InfoMax (CCIM). Following the analogies between the CCIM and the Information Bottleneck (IB),
the theoretical principle associated with the ELBO, we deduce that the representation quality is
associated to the capacity term.
The theoretical arguments are confirmed by the performed experiments where we observe that, dif-
ferently from what was argued in previous works (Higgins et al., 2017; Burgess et al., 2018), it is
possible to train a model that is able to learn good (able to generalise) representations while main-
taining optimal generative performance. The main contributions of the paper are summarised in the
following points:
•	derivation of a variational lower bound for the maximal mutual information of a generaive
model belonging in a certain family, see equation 7;
•	definition and bounds estimation for the network capacity for a variational autoencoder, see
equation 9;
•	association of the two main properties of VAE, generation quality and good representa-
tion, to two different information concepts, respectively Mutual Information and network
capacity;
•	proposal of a new learning principle for unsupervised models: the Capacity-Constrained
InfoMax, see equation 10, that allows both to learn a good representation while maintaining
optimal generative performance.
The work is divided as follows: in the second section we describe briefly the VAE and its variants;
in the third and fourth sections we describe the variational infomax method and related work. We
conclude the paper with the experimental results and the final observations.
2	Background
The aim of this section is to describe VAE, understand principal issues of the ELBO objective and
describe the three most relevant approaches to overcome such issues.
2.1	Notation and preliminary definitions
We use calligraphic letters (i.e. X) for sets, capital letters (i.e. X) for random variables, and lower
case letters (i.e. x) for their samples. With abuse of notation we denote both the probability and the
corresponding density with the lower case letters (i.e. p(x)).
KL divergence
gence
Given two random distributions p(x) and q(x), the Kullback-Leibler (KL) diver-
DKL(p(x)||q(x)) =	log
p(y)dy
(1)
is an (intuitive) measure of the distance between the distributions p and q.
Mutual Information and Capacity Given a channel Z → X with X and Z random variables,
jointly distributed according to p(x, z) and with marginals p(x) and p(z). The mutual information
I(X, Z) = DKL(p(x, z)||p(x)p(z)),
is a measure of the reduction of uncertainty in X due to the knowledge of Z, and the capacity
C(X, Z) = sup I(X, Z)
p(z)∈P
is the maximal information that can be shared for a fixed generator p(x|z).
2
Under review as a conference paper at ICLR 2020
2.2	Variational autoencoder
From now on let us assume that the unknown distribution of the data p(x) coincides with the em-
pirical one pD (x), and that the distribution of the latent representation p(z) is known. In this con-
text the VAE is a model solving the following optimisation problem: find the generative model
pθ (x, z) ∈ Pθ, specified by the parameters θ of the associated neural network, maximising the
ELBO objective
ELBOθ,φ = Ep(χ)[-DκL(qφ(z∣x)∣∣p(z))+ Eq(z∣χ)[log p(x∣z)]],	⑵
a lower bound of the unfeasible-to-compute marginal likelihood Ep(x) [logpθ(x)]. The ELBO ob-
jective is optimised by a regularised autoencoder, with encoder and decoder parametetrising, re-
SPectively, the inference and generative distributions, qφ(z∣χ) and pθ(x|z), with φ ∈ Φ, θ ∈ Θ
and regularise] defined by the rate term DκL(qφ(z∣χ)∣∣p(z)), measuring the excess number of bits
required to encode samPles from the encoder using the oPtimal code designed for p(z).
2.3	Uninformative representation issue
As underlined in the introduction, the main issue of VAE is that the rePresentations are not really
informative of the inPut data and in the worst case, it is learned a Z-indePendent generative model
Pθ(x|z) = pθ(x). Such issues are intrinsic in the ELBO objective, equation 2, that can reach
the optimum when DκL(qφ(z∖χ)∖∖p(z)) = 0 (Zhao et al., 2θl7). The latter case means that the
rePresentation is comPletely uninformative, indeed the rate term, which can be rewritten as
DKL(qφ(z∖x)∖∖p(z)) = Iq(X, Z) + DKL(qφ(z)∖∖p(z)),
is a penalty on the encoding information, and is zero when Iq(X, Z) = 0, with qφ(z∖x) = qφ(z) =
p(z), i.e. when qφ does not encode any information about the input x.
We now describe the three most relevant models that try to overcome the uninformative representa-
tion issue.
InfoVAE In (Zhao et al., 2017) the InfoVAE family of models was proposed, a generalisation of
the VAE model optimising the objective
-αIq(X, Z) - λDKL(qφ(z)∖∖p(z)) + Ep(x)[Eq(z|x)[log p(x∖z)]],
with α and λ two real positive hyper-parameters.
The main advantage of this definition is that it is possible to consider separately the two components
of the rate term. In particular, in (Zhao et al., 2017) it was observed that by eliminating the infor-
mation penalty (α = 0), the generative performance of the model improves and the representation
results are more informative.
β-VAE In (Higgins et al., 2017), starting from the observation that the optimal case is rare, but
most of the learned features by VAE are not disentangled, it is proposed an opposite approach: put a
high penalty to the rate term, in order to constrain the model to learn the most informative property
of the data, and then have a disentangled representation of the data. The β-VAE family is a particular
case of InfoVAE where α = λ 1. This idea, that at first sight looks counter-intuitive, is based on
the observation that by the additive property of the KL-divergence
dim(Z)
DKL(qφ(z∖x)∖∖p(z)) =	DKL(qφ(zi∖x)∖∖p(zi))	(3)
i=1
pushing the penalty associated with the rate is equivalent to penalising the informativeness of most
features, leaving few features containing the relevant information. Starting from a bits-back coding
argument, a similar conclusion was derived in (Chen et al., 2016).
Minimal rate bound In order to avoid the null rate issue, and maintain semantic informative
representation, in (Alemi et al., 2017) was suggested to optimise the following variant of ELBO:
Ep(x)[Eq(z|x)[log p(x∖z)]] - β∖R0 - DKL(qφ(z∖x)∖∖p(z))∖,	(4)
3
Under review as a conference paper at ICLR 2020
where R0 > 0, defines both the lower bound of the rate and the upper bound of the encoding
information Iq(X, Z). If, as we will see below, the objective in equation 4 is a generalisation of
both InfoVAE (R0 = maxθ Iθ (X, Z)) and β-VAE (R0 = 0), the correct R0 is unknown and its
optimisitation is not an easy task.
3	The Model
3.1	The Variational InfoMax
Assuming the distribution associated to the two random variables p(x) and p(z) is known, the
InfoMax objective is defined as: find the joint distribution pθ(x, z) ∈ Pθ := {pθ(x, z) :
Ep(z)[Pθ(x∣z)] = p(x), Ep(χ)[Pθ(z|x)] = p(z)} maximising the mutual information I§(X,Z)=
DKL(pθ(x, z)∣∣p(x)p(z)), i.e. find θ* ∈ Θ s.t. Iθ* ≥ I for any θ ∈ Θ.
Since the definition via KL divergence is computationally intractable, it is necessary to re-write the
mutual information as
Iθ (X,Z) = hθ (X) — hθ (X |Z),	(5)
where hθ(X) = -Ep@(χ)[logpθ(x)] is the entropy of X, and hθ(X|Z) = -Ep@(χ,z)[logPθ(x|z)] is
the conditional entropy hθ(X|Z). Sincepθ(x, Z) ∈ p the entropy hθ(X) = h(X) is constant, and
in order to maximise the mutual information it is sufficient to minimise the conditional entropy.
Excluding some special cases (Bell & Sejnowski, 1997), minimising the conditional entropy is un-
feasible, so it is necessary to consider an associated variational problem: for any qφ(z∣χ) such that
qφ(z) = P(Z' and φ ∈ Φ = Θ, learn the generative model pθ(x|z) minimising the reconstruction
accuracy term Ep(X) [Eqφ(z∣χ) [log(pθ(x|z))]]. Indeed, the following variational objective:
Iθ,φ(X,Z) = h(X) + Ep(X)Eq(z∣χ)[logPθ(x|z)]	s.t. qφ(z) = P(Z)	(6)
is a lower bound of Iθ*(X, Z) and is maximal when q(z∣x) = pθ(z|x) = pθ* (z|x), (see the AP-
pendix).
Unfortunately, the formulation in equation 6 is still unfeasible to compute, because it requires that
qφ(Z) = P(Z), but by the butterfly architecture of the autoencoder, qφ(Z) tends to be uniformly
distributed on the space Z. For this reason, the model is trained maximising the following relaxed
form:
VIMθ,Φ = Ep(X)Eq(z∣χ)[logpθ(x|z)] - λD(qφ(z)∣∣p(z)),	⑺
where it is introduced a term D(qφ(z)∣∣p(z)) encouraging the empirical distribution qφ(z) to be
close, according to the metric D, to P(Z). In the following we assume D = DKL, and in order
to avoid any confusion the variational autoencoder trained maximising equation 7 will be dubbed
VIMAE.
Encoding channel In VAE we observed that an uninformative representation was caused by the
non-informativeness of the encoding map qφ(z∣x). Since from equation 7 it is not clear how qφ(z∣χ)
behaves, we consider an equivalent representation, (see the Appendix):
VIMθ,Φ = -DKL(P(x)∣∣Pθ(x)) — (λ - 1)DκL(qφ(z)∣∣p(z)) + Iθ,φ(X,Z).	(8)
From equation 8 we see that the infomax objective, equation 7, can be read as a composition of three
sub-objectives: find a generative model pθ(x|z), with marginal resembling the visible distribution
P(x) (first term); maximise the (unbounded) variational mutual information (third term); and learn an
inferred distribution qφ(Z, x) close to the generative model Pθ (x, Z). Then the optimum is obtained
by qφ(x, Z) = Pθ(x, Z) such that Iθ(X, Z) is maximal, confirming the validity of the approximation
made above.
3.2	Channel capacity
In a channel with variational mutual information Iθ,φ as defined in equation 6 the (variational)
capacity Cθ,φ (X, Z), is defined as
Cθ,φ(X, Z) =	sup	Iθ,φ(X, Z).	(9)
θ,φ,p(z)∈P
4
Under review as a conference paper at ICLR 2020
If P is the space of all distributions on Z , the capacity of the network coincides with the variational
mutual information of the model trained minimising only the reconstruction loss. In the latter case,
as observed above, it is not guaranteed to learn the generator with pθ (z) = p(z). This is because,
given two equally informative generative models pθ(x|z) and pθo(x|z), with encoder respectively
qφ(z|x) and q©，(z|x), with h(qφ(z)) < h®，(z)), then Iθ,φ < 3⑷.From such observation we
deduce that bounding the entropy of the representation Z is the way to bound the capacity without
penalising the encoding information, and that the penalty introduced in equation 7 is actually a
bound to the capacity itself. So, the VIM objective can be defined as a variational approximation of
the Capacity-Constrained InfoMax:
maxI(X,Z) - λC(X, Z),
(10)
which given a set of equally informative generators, learns the one having the minimal capacity. The
idea of the CCIM is similar to the idea of the IB, from which was derived the β-VAE: constrain the
capacity of the network in order to learn only the relevant features of the input data. The difference
with the IB,
maxI(X,Z) - λIq(X,Z)
(11)
lies in the second term: the network capacity instead of the encoding information. This choice
allows the network to learn a good representation (small capacity) while maintaining good generative
performance (high mutual information). Indeed, as shown in equation 8, the generative performance
is associated to the informativeness of q©(z|x) andp(x|z).
In order to test the assumption that it is sufficient to bound the entropy of Z, instead of the encoding
mutual information, to learn a good representation, in the experiments (see below) we consider
the cases Z is Normal (VIMAE-n) or Logistic (VIMAE-l) distributed. We choose to compare the
popular Normal distribution with the Logistic one for two reasons: the Logistic has less entropy than
a Gaussian distribution and because it is a common assumption in natural science to suppose that
the hidden factors of the visible data are logistically distributed (Hyvarinen et al., 2009).
Entropy vs Rate According to what written above the choice of the distribution p(z) is a way to
bound the maximal information Iθ of the generative model as well as R0 in equation 4, indeed, if
we choose R0 = C(X, Z) the theoretical solutions of equation 4 and equation 7 coincide. Although
the two approaches are similar, both defines a target Iθ for the generator pθ(x|z), they differ in one
fundamental aspect: the ratio between the information Iθ and network capacity C . For any given
choice of the prior p(z) the generative model maximising VIM is the one with Iθ = C, instead
for any R0 < C the information Iθ associated to the generative model learned, by equation 4,
coincides with R0, i.e. Iθ = R0 < C. That implies an high sensitivity to the hyper-parameter
Ro for the model in equation 4. In fact, as suggested in (Mathieu et al., 2018), a ratio Iθ/C ≈ 1,
guarantees an appropriate level of overlap in the latent space between latent encoding, and then a
better representation quality.
4	Related work
Autoencoder literature Autoencoder models are one of the most used family of neural net-
works to extract features in an unsupervised way (Bengio et al., 2013), and their relationship
with Information Theory is well-established from the first unregularised autoencoders (Baldi &
Hornik, 1989). The classical unregularised autoencoders, minimising the reconstruction loss
Ep(x)[Eqφ(z∣x)[- logPθ(x|z)]], are maximising an unbounded information, i.e. they are looking
for a solution in the space Pθ = {pθ : pθ (x) = p(x)}. In general, a solution in this wide space is
good only for reconstruction performance because Z contains all the possible information that can
be stored in the space Z, and is not robust to input noise (Vincent et al., 2008); but, as observed in
(Grover & Ermon, 2018), if q©(z|x)〜 N(μ(x),σ(x)) the model pθ(x|z) is robust to noise and is
a Gaussian generator. In this context the uninformative issue is avoided, but the price to pay is the
impossibility to sample directly from a priorp(z) that is not defined; indeed, the model described in
(Grover & Ermon, 2018) requires running relatively expensive Markov Chain to obtain samples.
Many regularised models have been proposed, but the most well known is VAE, that minimises
the expected code length of communicating x. As we observed in the previous sections, it is not
5
Under review as a conference paper at ICLR 2020
guaranteed that the method finds a useful representation. Such issue can be solved both controlling
the information of the model, or considering a more flexible prior p(z),(Rezende & Mohamed, 2015;
Kingma et al., 2016; Dinh et al., 2016). In this manuscript we do not consider the latter approach,
because by the model structure, it requires quite expensive computation (autoregressive models are
necessary) and because the learned representation is not suitable for task different than the generation
such as lifelong context, for which the standard VAE is used with success (Achille et al., 2018).
The objective in equation 7 was firstly derived in (Tolstikhin et al., 2017) and (Zhao et al., 2017).
Particularly relevant is the derivation in (Tolstikhin et al., 2017) because it allows us to describe
an informative model pθ (x, z) as the one minimising the transport cost between the original and
generated data.
Finally, we underline that in case we wish to consider a Jensen-Shannon divergence in equation 7
it is necessary to consider an adversarial network model, discriminating the true samples Z 〜P(Z)
from the fake sampled by qφ(z) (Goodfellow et al., 2014). In the latter case the obtained model is
equivalent to the Adversarial AutoEncoder (Makhzani et al., 2015). We conclude by remarking that
in all the cases cited above the Infomax objective was never maximised using a prior p(Z) different
from a Gaussian.
Information theoretic literature Information theory is strongly related with neural networks, and
not only with autoencoders. Originally the InfoMax objective was applied to a self-organised system
with a single hidden layer, (Bell & Sejnowski, 1997; Linsker, 1989) where the bound in the capacity
was given by the numbers of hidden neurons. More recently, the (naive) InfoMax has given way to
a new information-theoretic principle: the Information-Bottleneck (Tishby et al., 2000). The idea
of this principle is that a feed-forward neural network trained for task T tends to learn a minimal
sufficient representation of the data, maximising the following objective:
maxI(Z,T) - βI(X, Z).
(12)
Although it was shown that in the general case this principle does not hold true (Saxe et al., 2018),
the principle was used as a regularisation technique with success both in unsupervised (Alemi et al.,
2017; Higgins et al., 2017) and supervised (Alemi et al., 2016) settings. We observe that the VIM,
equation 7, and IB, equation 12, coincide in the case of a deterministic encoder, where the encoding
information is the entropy of Z.
5	Experiments
Here we empirically evaluate the VIMAE. The section is divided into three parts: in the first part
We compare the ability of the described models to infer the representation, Z 〜N(0, I). Such an
experiment is to evaluate the entropy of Z 〜q(Z) and then, as observed in section 3.2, an indirect
way to estimate capacity of the network (C(X,Z) 8 h(Z)). In the second part we evaluate the
reconstruction and generative performance of the models. Indeed, the combination of the two tasks
is estimation of the mutual information of the generative model Iθ(X, Z), see equation 8.
In the third part we evaluate the robustness to noise and generalisation property of the learnt repre-
sentation, observing that an informative model with small capacity is the best one for these tasks.
In all the described experiments, the divergence DKL(q(Z)||p(Z)) in equation 7 is approximated via
the Maximum Mean Discrepancy (Zhao et al., 2017) defined as:
MMD(q(Z),p(Z)) =	sup	Ep(z)[f(Z)] - Eq(z) [f (Z)]
f ：kfkHk ≤1
(13)
where Hk is the Reproducing Kernel Hilbert Space associated to a positive definite kernel k(∙, ∙):
Z × Z → R+ . Moreover, by difficulties to compute the objective equation 4, as suggested in (Alemi
et al., 2017) we decided to optimise a β-VAE, denoted βA-VAE, with β < 1; in order to avoid any
confusion, the original version proposed in (Higgins et al., 2017) with β 1 will be renamed
βH-VAE.
6
Under review as a conference paper at ICLR 2020
p(z) = N (0, 1)	VAE, 0.70	βH -VAE, 0.53	βA-VAE, 0.76 VIMAE, 0.08
Figure 1: 2-d learned representations. Under each plot the model name is followed by the
respective MMD value.
5.1	THE ENTROPY OF Z
Experiments in this part are performed with an autoencoder trained with the MNIST data-set, a col-
lection of 70k monocromatic handwritten digits, where both the inference and generative distribution
are modelled by 3-layer deep neural nets with 256 hidden units in each layer and Z = R2 .
From the 2d hidden distribution learnt by the different methods in figure 1, we observe that the
ELBO-based models are not able to learn a distribution q(z) that is close to the prior p(z); and, as
we will see in the following experiments, βH -VAE, the only ELBO-model having comparing results
to the VIMAE, has to penalise drastically the decoding information.
Thanks to this experiment we see that, in general, the ELBO-based models are not seeking for a
small capacity network, since h(q(z)) h(p(z)), and that the divergence penalty introduced in
equation 7 is a bound for the entropy h(q(z)), and then of the channel capacity C.
5.2	The model information
The experiments in these final sections were performed with the same settings and autoencoder
models used in (Tolstikhin et al., 2017), an architecture similar to the DCGAN (Radford et al.,
2015) with batch normalization (Ioffe & Szegedy, 2015) (more details given in the Appendix). We
consider four data-sets: MNIST and CIFAR10, two standard data-sets with ground-truth labels;
Omniglot, a data-set of 1623 characters from 50 alphabets, 30 training and 20 evaluation, where
each character appears 80 times, to evaluate the informativeness of the model and the quality of
the learned representation; in the Appendix, we also consider CelebA (Liu et al., 2015), consisting
of roughly of 203k faces of 64 × 64 resolution, in order to compare the generative quality of the
pictures. After considering many parameters for βH, βA and λ, we choose, in accordance with what
was suggested in (Tolstikhin et al., 2017), βH = λ = 10, and βA = 0.2 for MNIST and Omniglot
and β = λ = 100 and βA = 0.4 for CelebA and CIFAR10 experiments.
The goal of this section is to evaluate the informativeness of the learnt generative model pθ(x|z).
From what was described above, the reconstruction loss or the generation quality alone are not reli-
able metrics, because the reconstruction loss is an estimation of the variational mutual information
Iθ,φ(X, Z), and the generation quality is an estimation of DKL(pθ(χ)∣∣p(χ)) that, as observed in
section 2.3, it is possible to minimise with an uninformative generator. But, according to equation 8
the combination of the two task performances is a good empirical estimation of Iθ (X, Z); indeed,
by generative experiments, we require that q(z) = p(z), so the KL divergence term in equation 8
comes for free.
Reconstruction and generative performances According to what was asserted above, we ob-
serve from figure 2 (top) and figure 3 that an high penalty on the rate term (small encoding infor-
mation) in the ELBO based model coincides with poor reconstruction. Indeed, the βH -VAE, is the
one with worst reconstruction, and instead βA-VAE is the model that behaves similarly to VIMAE,
that is optimising an encoding information-free objective. Such behaviour is consistent with the rate
term associated to each model, see caption of each figure, where we observe that VIMAE, have the
highest rate and in particular this is twice the one associated to βA-VAE, confirming that VIMAE
learns a maximal informative decoder, and that βA-VAE is only theoretically analogous to VIMAE.
The models that We are considering are defined as generative models, so giving a sample Z 〜P(Z)
they should be able to generate a new data x similar to the original one. As we observe from the
7
Under review as a conference paper at ICLR 2020
目 EGG
YYaCJ
uɔ MCU
BGɔ & b
mG
YXaCj
υu 妙 H
自口QQj
YYaO
υu 4 科
Qb 64J-6
自回QQ
YXaQ
U U M- U
Qb6 b
VAE,	βH -VAE,	βA-VAE,	VIMAE-n,	VIMAE-l,
(0.75, 48.7)	(0.9, 18.8)	(0.74, 67)	(0.75, 125)	(0.76, 120)
Figure 2: Test reconstruction (top) and random generative samples (bottom) of the different
methods with Omniglot (right). In test reconstructions, the odd rows are the original data. Each
model is denoted by its name and the reconstruction k ∙ ∣∣2 and rate term.
VAE,	βH -VAE,	βA-VAE,	VIMAE-n,	VIMAE-l,
(8.29, 251)	(9.8, 28)	(5.7, 423)	(4.74, 445)	(4.85, 443)
Figure 3: Test reconstruction, CIFAR 10. Odd rows are the original data. Each model is denoted by
its name and the reconstruction k ∙ ∣∣2 and rate term.
generated samples of Omniglot in figure 2 (bottom) and Negative LogLikelihood (NLL) listed in
table 1, both VAE and βH -VAE do not generate good samples, confirming that sharper samples are
associated to informative generators. Such qualities on Omniglot are confirmed by the experiments
with the CIFAR10 and CelebA data-set (Liu et al., 2015), see the Appendix, where itis observed that
if the generative difference between the two VIMAEs is small, the difference with the VAE counter-
parts is high. Such behaviour is in agreement with what was observed until now: the ELBO based
model does not learn a good generative network, and the good reconstruction is simply associated
to a large entropy of Z, (encoding information) instead of an informative generative model pθ(x|z).
Similar experiments were conducted with the MNIST dataset and are dis-
cussed in the appendix.
5.3 Generalisation property
Table 1: NLL for
generated samples
on Omniglot (smaler
is better)
We defined a good representation as the one containing the relevant prop-	Method	NLL
erties of the visible data and able to generalise from the task for which was	VAE	1224
trained. In order to evaluate such quality, following the approach proposed	βH-VAE	1254
in (Rifai et al., 2011), we evaluate the accuracy of an SVM directly trained	βA-VAE	1228
on the learned features of the data. Proceeding as in (Zhao et al., 2017), we	VIMAE-n	1190
train the M1+TSVM (Kingma et al., 2014) and use the semi-supervised per-	VIMAE-l	1223
formance over 1000 (100 for Omniglot) samples as an approximate metric to
verify the relevance and the quality of the learned representation. In order to
evaluate the robustness of the learned features, we performed the same algo-
rithm on the representation associated to corrupted data, i.e. Z 〜q(z∣x + V),
considering two types of noise: Gaussian and mask. In the Gaussian case,
we add to each pixel a ν value sampled from N(0, σ2) with σ ∈ {0.2, 0.3, 0.4}, and in the masking
8
Under review as a conference paper at ICLR 2020
Table 2: Semi-supervised classification CIFAR10.
Method	accuracy (%)		
	v=0	N(0, 0.32)	B(0.2)
VAE	30	25	16
βH -VAE	29	26	19
βA-VAE	31	31	18
VIMAE-n	29	28	23
VIMAE-l	32	34	23
Table 3: Semi-supervised classification, MNIST.
Table 4: Semi-supervised classification,
Omniglot (random sampling: 20%).
accuracy (%)
Method V = 0 V = N (0,σ2) V = B(p)
4
.
0
5
.
0
accuracy (%)
v = 0 v = N(0, σ2) v = B(p)
2
.
0
4
.
0
2
.
0
5
.
0
VAE
βH-VAE
βA-VAE
VIMAE-n
VIMAE-l
02333
89999
0.2 7786669292
02368
78188
.2 21522
0 79899
24567
58688
21224
22222
21233
22222
7214
1222
29123
21222
67422
11222
case a fraction ν of the elements is forced to be 0: each pixel is masked according to a Bernoulli
distribution B(p), p ∈ {0.2, 0.5}. Higher classification performance suggests that the learned repre-
sentation contains the relevant information and, in case of corrupted input data, that it is robust. In
the Omniglot case by the challenge of the task (the test alphabet was never seen in the training) we
consider a 5-character data-set, split into 300 (60 × 5) for training and 100 for evaluation.
From the classification scores listed in tables 2- 4, we see that the ELBO-based model learnt good
representations for clean data, but not when corrupted data is given as input. This is particularly
clear in the Bernoulli case, that is a noise different from the one seen in the training. Particularly
relevant is the behaviour of the two VIMAEs: they are comparable in the cases of clean data and
small noise, but the one with big capacity, VIMAE-n, suffers in large noise, while the one with small
capacity, VIMAE-l, is the most robust and in some challenging cases, see table 2, the noise helps to
improve the model accuracy. Such a result is consistent with the idea that a small capacity network
is learning the relevant factors of the input data, that are the only ones robust to the input noise.
6 Conclusion
We observe, via an information theoretic description of VAE, that it is possible to learn a good gener-
ative model while maintaining a meaningful hidden representation, and that goal can be reached by
optimising the CCIM, an objective that separates out the two properties of a network: the generative
information and its capacity. We compare both theoretically and computationally the CCIM objec-
tive with the IB and its variation (Alemi et al., 2017), observing that the CCIM optimal solution, in
particular cases, coincides with the IB one, but that in the practical case the difference is substantial.
Future work includes the extension of the VIMAE model to a lifelong task, a scenario where it is
necessary to learn both an informative representation while maintaining bounded the capacity of the
network, in order to avoid the catastrophic forgetting issue (Achille et al., 2018).
References
Alessandro Achille, Tom Eccles, Loic Matthey, Chris Burgess, Nicholas Watters, Alexander Lerch-
ner, and Irina Higgins. Life-long disentangled representation learning with cross-domain latent
homologies. In Advances in Neural Information Processing Systems, pp. 9873-9883, 2018.
9
Under review as a conference paper at ICLR 2020
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy.
Fixing a broken elbo. arXiv preprint arXiv:1711.00464, 2017.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Anthony J Bell and Terrence J Sejnowski. The independent components of natural scenes are edge
filters. Vision research, 37(23):3327-3338, 1997.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner. Understanding disentangling in β-vae. arXiv preprint
arXiv:1804.03599, 2018.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Aditya Grover and Stefano Ermon. Uncertainty autoencoders: Learning compressed representations
via variational information maximization. arXiv preprint arXiv:1812.10539, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
volume 3, 2017.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006.
AaPo Hyvarinen, Jarmo Hurri, and Patrick O Hoyer. Natural image statistics: A probabilistic ap-
proach to early computational vision., volume 39. Springer Science & Business Media, 2009.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in neural information processing systems, pp.
3581-3589, 2014.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In Advances in neural information
processing systems, pp. 4743-4751, 2016.
Ralph Linsker. An application of the principle of maximum information preservation to linear sys-
tems. In Advances in neural information processing systems, pp. 186-194, 1989.
10
Under review as a conference paper at ICLR 2020
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings ofthe IEEE international conference on computer vision, pp. 3730-3738, 2015.
Chris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih,
Arnaud Doucet, and Yee Teh. Filtering variational objectives. In Advances in Neural Information
Processing Systems, pp. 6573-6583, 2017.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Emile Mathieu, Tom Rainforth, Siddharth Narayanaswamy, and Yee Whye Teh. Disentangling
disentanglement. arXiv preprint arXiv:1812.02833, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Jason Ramapuram, Magda Gregorova, and Alexandros Kalousis. Lifelong generative modeling.
arXiv preprint arXiv:1705.09847, 2017.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Salah Rifai, Gregoire MesniL Pascal Vincent, Xavier Muller, Yoshua Bengio, Yann Dauphin, and
Xavier Glorot. Higher order contractive auto-encoder. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 645-660. Springer, 2011.
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren-
dan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning.
2018.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. arXiv preprint arXiv:1711.01558, 2017.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra.
Matching networks for one shot learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
3630-3638. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6385-matching-networks-for-one-shot-learning.pdf.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational
autoencoders. arXiv preprint arXiv:1706.02262, 2017.
11
Under review as a conference paper at ICLR 2020
Relationship between encoding, decoding and variational
Information
Defined q(z,x) := qφ(z∣x)p(x) the encoding distribution andpθ(x,z) := pθ(x∣z)p(z) the decoding
one. The encoding Iq (X, Z), decoding Iθ(X, Z) and the variational Iθ,φ (X, Z) information are
defined respectively:
Iq(X,Z) =DKL(q(z,x)||q(z)p(x)) = h(X) - hq(X|Z)
Iθ (X, Z) = Dkl(pθ (z,x)∣∣p(z)p(x)) = h(X)-hθ (X |Z)
Iθ,φ(X, Z) = h(X) - Eq(z,x)[- logPθ(x|z)]
Assuming θ* ∈ Θ is the parameter associated to the maximal decoding information, Iθ (X, Z) ≤
Iθ*(X,Z) for any θ ∈ Θ, it follows that for any qφ(x,z) ∈ Pθ, i.e. for any qφ(z) =
Ep(x)[qφ(x∣z)] = p(z) and φ ∈ Φ ⊂ Θ,
3(X,Z) ≥ Iq(X,Z).
Then a lower bound of Iq is a lower bound of I§*. By property of KL-divergence We have that for
any pθ(x|z) the following relationship holds:
Eq(z,x)[- log Pθ (X|z)] = hq (X|Z)+ Eq(z)[DKL Sφ(XIz)||P6 (XIz))]	(14)
From equation 14 and the definition of the variational information Iθ,φ we deduce that:
Iθ*(X,Z) ≥ Iq(X,Z) ≥ Iθ,φ(X, Z)
We conclude observing that if Θ = Φ, at optimum the three information terms above are equal, and
then qφ(X, z) = pθ (X, z).
Observation: If Φ ⊂ Θ, the variational mutual information can be at most equal to Iq*, the maximal
Iq, but it is not guaranteed that Iq* = Iθ*.
Derivation of equation 8
In (Zhao et al., 2017), it is observed that equation 7 can be written as follows:
VIMθ,φ = - DKL(p(X)IIpθ(X)) - Ep(x)[DKL(qφ(zIX)IIpθ(zIX))]-
- (λ - 1)DKL(qφ(z)IIp(z)) + Iq (X, Z).
From equation above, to verify that equation 8 is correct, it is sufficient to show that
Iθ,φ(X, Z) = Iq(X, Z) - Ep(x)[DKL(qφ(zIX)IIpθ(zIX))].
The equation 15 follows by the property of the autoencoder and equation 15.
More precisely, by equation 14 we have that
(15)
Iθ,φ(X, Z) = Iq(X,Z) - Eq(z)DKL(qφ(XIz)IIpθ(XIz))
and, by AutoEncoder architecture, as observed in section 3, pθ(z) = Ep(x) [pθ(zIX)] = q(z). Then
the following equation holds
Eq(z)DKL(qφ(XIz)IIpθ(XIz)) = Ep(x)DKL(qφ(zIX)IIpθ(zIX)).
Indeed,
q(XIz)
Eq(Z)DKL(qφ(x∣z)∣∣Pθ(XIz)) = J q(z) J q(x∣z)logP坨(χ∣Z)dxdz
= [[q(x)q(z∣x) log q(xz)q!z, dχdz = PP(X) [ q(z∣χ) log "：叫 dxdz
pθ(XIz)(z)	pθ(zIX)
= Ep(x)DKL(qφ(zIX)IIpθ(zIX)).
12
Under review as a conference paper at ICLR 2020
Further Details on Experiments
In all the experiments in section 5.2 we considered the latent space Z = Rd, for all the models we
choose the priorp(z) to be a Gaussian with zero mean and identity covariance, only in VIMAE-l we
choose the prior p(z) to be a logistic with mean zero and identity variance. We choosepθ(x|z) to be
similar to DCGAN with batch normalization and qφ(z∣χ) to be a convolutional deep neural network.
The entire models are trained end to end by Adam (Kingma & Ba, 2014) with α = 10-3,β1 =
0.5, β2 = 0.999. We considered a deterministic decoder and we approximate the reconstruction loss
with the L2 loss, i.e. Ep(x)[Eq(z|x)[- logpθ(x|z)]] = ||x - Xg∣∣2, with Xg indicating the generated
datum. In VIMAE case, while training we were adding a pixel-wise Gaussian noise truncated at
0.01 to all the images before feeding them to encoder, in order to make the encoder random. In VAE
and β-VAE case, instead we used the standard reparameterization trick (Kingma & Welling, 2013).
In the following we describe the data-sets considered and the associated neural networks, we follow
the same description given in (Tolstikhin et al., 2017) since we used the same neural nets.
MNIST and Omniglot
MNIST is a data-set containing 70k grey-scale handwritten digits and associated labels of resolution
28 × 28, subdivided in three subsets: train (50k), validation (10k) and test (10k).
Omniglot is a data-set containing 1623 different handwritten characters of resolution 28 × 28 from
50 different alphabets. Each of the 1623 characters was drawn online via Amazon’s Mechanical
Turk by 20 different people. In the same fashion as done in (Vinyals et al., 2016) we considered
an augmented version where each character is rotated respectively by 90, 180, 270 degrees, in this
way each character appears 80 times. The Omniglot data-set although has the same resolution of
the MNIST, for this reason we use the same network, it is more challenging because, it is more
entropic then MNIST, in fact the classes move from 10 to 1623 and the test classes are never seen
in the training.
We choose Z = R8, and β = λ = 10, we used mini-batches of size 100 and trained the model for 80
epochs. Both encoder and decoder used fully convolutional architectures with 4 × 4 convolutional
filters.
Encoder:
X ∈ R28×28 → Conv128 → BN → ReLu
→ Conv256 → BN → ReLu
→ Conv512 → BN → ReLu
→ Conv1024 → BN → ReLu
Decoder:
z ∈ R8 → FC7×7×1024
→ FSConv512 → BN → ReLu
→ FSConv256 → BN → ReLu → FSConv1
Where Convk stands for a convolution with k filters, FSConvk for the fractional strided convolution
with k filters, BN for batch normalization, ReLU for the rectified linear units, and FCk for the fully
connected layer mapping to Rk . All the convolutions in the encoder used vertical and horizontal
strides 2 and SAME padding.
CelebA and CIFAR10
CelebA is a data-set with 202 599 faces images. We preprocessed the images by first taking a
140 × 140 center crops and then resizing to the 64 × 64 resolution and we consider the last 20k
images as test subset.
CIFAR10 is a dataset consisting of of 60k 32 × 32 colour images in 10 classes, with 6k images per
class. There are 50k training images and 10k test images.
13
Under review as a conference paper at ICLR 2020
For these data-sets we choose the same network with the same hyper-parameters, λ = 100 and
Z = R64. We used mini-batches of size 100 and trained the model for 60 epochs. Both encoder and
decoder used a fully convolutional architectures with 5 × 5 convolutioanal filters.
Encoder:	x ∈ R64×64×3 → Conv128 → BN → ReLu → Conv256 → BN → ReLu → Conv512 → BN → ReLu → Conv1024 → BN → ReLu
Decoder:	z ∈ R64 → FC8×8×1024 → FSConv512 → BN → ReLu → FSConv256 → BN → ReLu → FSConv128 → BN → ReLu → FSConv1
CelebA and Omniglot Experiments
In section 5.2 we evaluate the generative performance on relatively simple grey-scale data-set. In or-
der to consider a more challenging data-set and quantitatively compare the generative performances
of the trained models we evaluate the Frechet Inception Distance (FID) on CelebA and CIFAR10
based on 104 samples. From table 5, we observe, in agreement on what observed in section 5.2
and figures 4 and 5 that the difference between the two VIMAE models is minimal, instead it is big
the difference with the ELBO based models (βH -VAE is not listed in table 5, because it does not
converge).
Table 5: FID scores for generated samples on CelebA (smaller is better)
Method	FID	
	CelebA	CIFAR10
VAE	82	168
βH-VAE	-	262
βA-VAE	89	174
VIMAE-l	56	103
VIMAE-n	55	104
MNIST Experiments
The MNIST dataset is a classical benchmark but quite simple, indeed as we see from figure 6 all
the models considered are able to reconstruct without big differences, although as we see from the
L2 reconstruction loss in the VIMAE models is smaller than the ELBO-based counterpart. That
results combined with the high value of the rate term, suggest that the VIMAE are actually learning
a maximal informative decoder. We conclude observing that, according to what seen until now,
the high rate value for βA-VAE is associated to an high entropy h(q(z)), this is well visible by the
generated samples from p(z) in figure 7, where the samples associated to βA-VAE seem to belong
to the same cluster.
14
Under review as a conference paper at ICLR 2020
VIMAE-n
VIMAE-l
Figure 4: Test reconstruction (top) and random samples (bottom) of the two VIMAE models,
λ = 10.
βH -VAE
VAE
VIMAE-n
VIMAE-l
Figure 5: random samples generated by the VAEs trained on CIFAR10
βA-VAE
15
Under review as a conference paper at ICLR 2020
VAE
(0.51, 54.88)
βA-VAE,
(0.5 ,70,56)
βH-VAE,
(0.62, 25.92)
VIMAE-n,
(0.47, 101.02)
VIMAE-l,
(0.48, 109.76)
Figure 6: Test reconstruction by the VAEs trained on MNIST
VAE
βH -VAE
βA-VAE
■■
VIMAE-n
VIMAE-l
Figure 7: random samples generated by the VAEs trained on MNIST
16