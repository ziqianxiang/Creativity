Under review as a conference paper at ICLR 2020
Unifying Graph Convolutional Neural
Networks and Label Propagation
Anonymous authors
Paper under double-blind review
Ab stract
Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are
both message passing algorithms on graphs. Both solve the task of node classifi-
cation but LPA propagates node label information across the edges of the graph,
while GCN propagates and transforms node feature information. However, while
conceptually similar, theoretical relation between LPA and GCN has not yet been
investigated. Here we study the relationship between LPA and GCN in terms of
two aspects: (1) feature/label smoothing where we analyze how the feature/label
of one node is spread over its neighbors; And, (2) feature/label influence of how
much the initial feature/label of one node influences the final feature/label of an-
other node. Based on our theoretical analysis, we propose an end-to-end model
that unifies GCN and LPA for node classification. In our unified model, edge
weights are learnable, and the LPA serves as regularization to assist the GCN in
learning proper edge weights that lead to improved classification performance.
Our model can also be seen as learning attention weights based on node labels,
which is more task-oriented than existing feature-based attention models. In a
number of experiments on real-world graphs, our model shows superiority over
state-of-the-art GCN-based methods in terms of node classification accuracy.
1	Introduction
Consider the problem of node classification in a graph, where the goal is to learn a mapping M :
V → L from nodes V to labels L. Solution to this problem is widely applicable to various scenarios,
e.g., inferring income of users in a social network or classifying scientific articles in a citation
network. Different from a generic machine learning problem where samples are independent from
each other, nodes are connected by edges in the graph, which provide additional information and
require more delicate modeling. To capture the graph information, researchers have mainly designed
models on the assumption that labels and features vary smoothly over the edges of the graph. In
particular, on the label side L, node labels are propagated and aggregated along edges in the graph,
which is known as Label Propagation Algorithm (LPA) (Zhu et al., 2005; Zhou et al., 2004; Zhang
& Lee, 2007; Wang & Zhang, 2008; Karasuyama & Mamitsuka, 2013; Gong et al., 2017; Liu et al.,
2019a); On the node side V , node features are propagated along edges and transformed through
neural network layers, which is known as Graph Convolutional Neural Networks (GCN) (Kipf &
Welling, 2017; Hamilton et al., 2017; Li et al., 2018; Xu et al., 2018; Liao et al., 2019; Xu et al.,
2019; Qu et al., 2019).
GCN and LPA are related in that they propagate features and labels on the two sides of the mapping
M, respectively. However, the relationship between GCN and LPA has not yet been investigated.
Specifically, what is the theoretical relationship between GCN and LPA, and how can they be com-
bined to develop a more accurate model for node classification in graphs?
Here we study the theoretical relationship between GCN and LPA from two viewpoints: (1)
Feature/label smoothing, where we show that the intuition behind GCN/LPA is smoothing fea-
tures/labels of nodes across the edges of the graph, i.e., one node’s feature/label equals the weighted
average of features/labels of its neighbors. We prove that if edge weights smooth the node features,
they also smooth the node labels with guaranteed upper bound on the approximation error. And, (2)
feature/label influence, where we quantify how much the initial feature/label of node vb influences
1
Under review as a conference paper at ICLR 2020
the output feature/label of node va in GCN/LPA by studying the Jacobian/gradient of node vb with
respect to node va, and then we also prove their quantitative relationship.
Based on the above theoretical analysis, we propose a unified model GCN-LPA for node classifi-
cation. We show that the key to improving the performance of GCN is to enable nodes within the
same class/label to connect more strongly with each other by making edge weights/strengths train-
able. Then we prove that increasing the strength of edges between the nodes of the same class is
equivalent to increasing the accuracy of LPA’s predictions. Therefore, we can first learn the opti-
mal edge weights by minimizing the loss of predictions in LPA, then plug the optimal edge weights
into a GCN to learn node representations and do final classification. In GCN-LPA, we further com-
bine the two steps together and train the whole model in an end-to-end fashion, where the LPA
part serves as regularization to assist the GCN part in learning proper edge weights that benefit the
separation of different classes. It is worth noticing that GCN-LPA can also be seen as learning at-
tention weights for edges based on node label information, which requires less handcrafting and is
more task-oriented than existing work that learns attention weights based on node feature similarity
(VelickoVic et al., 2018; ThekUmParamPil et al., 2018; Zhang et al., 2018; LiU et al., 2019b). Ex-
periments on five datasets indicate that our model outperforms state-of-the-art methods in terms of
classification accUracy.
2	Unifying GCN and LPA
In this section, we first formUlate the node classification Problem and briefly introdUce LPA and
GCN. We then ProVe their relationshiP from the ViewPoints of smoothing and inflUence. Based on
oUr theoretical analysis, we then ProPose a Unified model GCN-LPA, and we also analyze the reason
why oUr model Performs better than GCN in node classification.
2.1	Problem Formulation and Preliminaries
We begin by describing the Problem of node classification on graPhs and introdUcing notation. Con-
Sider a graph G = (V, A, X, Y), where V = {vι, ∙∙∙ , vn} is the set of nodes, A ∈ Rn×n is the
adjacency matrix (self-looPs are inclUded), X is the featUre matrix of nodes and Y is labels of
nodes. aij (the ij-th entry of A) is the weight of the edge connecting vi and vj . N(v) denotes the
set of immediate neighbors of node v in graPh G . Each node vi has a featUre Vector xi which is the
i-th row of X, while only the first m nodes have labels yι,…，ym from a label set L = {1,…，c}.
The goal is to learn a maPPing M : V → L and Predict labels of Unlabeled nodes.
Label Propagation Algorithm. LPA assUmes that two connected nodes are likely to have the same
label, and thus it propagates labels iteratively along the edges. Let Y (k) = [y(k), •••,"』)]> ∈ Rn×c
be the soft label matrix in iteration k > 0, in which the i-th row yi(k)> denotes the Predicted label
distribution for node Vi in iteration k. When k = 0, the initial label matrix Y⑼=[y(0),…，yn')]>
consists of one-hot label indicator vectors y(0) for i = 1,…，m (i.e., labeled nodes) or zero vectors
otherwise (i.e., unlabeled nodes). Let D be the diagonal degree matrix for A with entries dii =
Pj aij . Then LPA (Zhu et al., 2005) in iteration k is formulated as the following two steps:
Y (k+1) = D-1A Y(k),
(k+1)	(0)
yi	= yi , ∀ i ≤ m.
(1)
(2)
In Eq. (1), all nodes propagate their labels to their neighbors according to weights of edges. Then
in Eq. (2), labels of all labeled nodes are reset to their initial values, because LPA wants to persist
labels of nodes which are labeled so that unlabeled nodes do not overpower the labeled ones as the
initial labels would otherwise fade away.
Graph Convolutional Neural Network. GCN is a multi-layer feedforward neural network that
propagates and transforms node features across the graph. The layer-wise propagation rule of GCN
is X(k+1) = σ(D-1AD-2X(k)W(k)), where W(k) is trainable weight matrix in the k-th layer,
σ(∙) is an activation function such as ReLU, and X(k) = [xIk),…，Xnk)]> are the k-th layer node
representations with X(0) = X. To align with the above LPA, we use D-1A as the normalized
adjacency matrix instead of the symmetric one D- 1 AD- 1 proposed by Kipf & Welling (2017).
2
Under review as a conference paper at ICLR 2020
Therefore, the feature propagation scheme of GCN in layer k is:
X(k+1) = σ(D-1AX(k)W (k)).	(3)
Notice similarity between Eqs. (1) and (3). Next we shall study and uncover the relationship between
the two equations.
2.2	Feature Smoothing and Label Smoothing
The intuition behind both LPA and GCN is smoothing (Zhu et al., 2003; Li et al., 2018): In LPA, the
final label of a node is the weighted average of labels of its neighbors: y(∞) =看 Pj∈N(i) aijyj∞);
in GCN, the final node representation is also the weighted average of representations of its neighbors
if We assume σ is identity function and WG) are identity matrices: x(∞) = 六 Pj∈N⑴ ajxj∞L
Next we show the relationship between feature smoothing and label smoothing:
Theorem 1 (Relationship between feature smoothing and label smoothing) Suppose that the
latent ground-truth mapping M : x → y from node features to node labels is differentiable
and satisfies L-Lipschitz constraint, i.e., |M(x1) - M(x2)| ≤ Lkx1 - x2k2 for any x1 and
x2 (L is a constant). If the edge weights {aij } approximately smooth xi over its immediate
neighbors with error	Ci,	i.e.,	Xi	= 六	Ej∈n⑶	ajXj	+	Ci,	then the edge weights {a4} also
approximately smooth yi over its immediate neighbors with the following approximation error:
Iyi - dii Pj∈N(i) aijyj I ≤ Lkeik2 + o( maXj∈N(i)(kXj - Xik2)), where o(α) denotes a higher
order infinitesimal than α.
Proof of Theorem 1 is in Appendix A. Theorem 1 indicates that label smoothing is theoretically
guaranteed by feature smoothing. Note that if We treat edge Weights {aij} learnable, then feature
smoothing (i.e., Ci → 0) can be directly achieved by keeping node features Xi fixed While setting
{aij } appropriately, Without resorting to feature propagation in a multi-layer GCN. Therefore, a
simple approach to exploit this theorem Would be to learn {aij } by reconstructing node feature
Xi from its neighbors, then use the learned {aij } to reconstruct node labels yi (Karasuyama &
Mamitsuka, 2013). This is equivalent to first minimizing the difference betWeen X (0) and X (1) by
learning D-1A in a one-layer GCN Where σ and W(0) are identity, then plug the learned D-1A in
LPA and run it for one iteration.
As shoWn in Theorem 1, the approximation error of labels is dominated by LkCi k2 . HoWever,
this error could be fairly large in practice because: (1) The number of immediate neighbors for a
given node may be too small to reconstruct its features perfectly, especially in the case Where node
features are high-dimensional and sparse. For example, in a citation netWork Where node features
are one-hot bag-of-Words vectors, the feature of one article can never be precisely reconstructed if
none of its neighboring articles contains the specific Word that appears in this article. As a result,
kCi k2 Will be non-neglibible. This explains Why it is beneficial to apply LPA and GCN for multiple
iterations/layers in order to include information from farther aWay neighbors. (2) The ground-truth
mapping M may not be sufficiently smooth due to the complex structure of latent manifold and
possible noise, Which fails to satisfy L-Lipschitz constraint. In other Words, the constant L Will be
extremely large.
2.3	Feature Influence and Label Influence
To address the above concerns and extend our analysis, We next consider GCN and LPA With multi-
ple layers/iterations, and do not impose any constraint on the ground-truth mapping M.
Consider tWo nodes va and vb in a graph. Inspired by Koh & Liang (2017) and Xu et al. (2018), We
study the relationship betWeen GCN and LPA in terms of influence, i.e., hoW the output feature/label
of va Will change if the initial feature/label of vb is varied slightly. Technically, the feature/label
influence is measured by the Jacobian/gradient of the output feature/label of va With respect to the
initial feature/label of vb . Denote X(ak) as the k-th layer representation vector of va in GCN, and Xb
as the initial feature vector of vb . We quantify the feature influence of vb on va as folloWs:
3
Under review as a conference paper at ICLR 2020
Definition 1 (Feature influence) The feature influence of node vb on node va after k layers of GCN
is the L1-norm of the expected Jacobian matrix ∂x(ak) /∂xb: If(va, vb; k) = E∂x(ak)/∂xb1. The
normalized feature influence is then defined as If(va, vb; k) = If(va, vb; k)/ v ∈V If(va, vi; k).
We also consider the label influence of node vb on node va in LPA (this implies that va is unlabeled
and Vb is labeled). Since different label dimensions of y(∙ do not interact with each other in LPA,
We assume that all yi and y(∙ are scalars within [0,1] (i.e., a binary classification) for simplicity.
Label influence is defined as follows:
Definition 2 (Label influence) The label influence of labeled node vb on unlabeled node va after k
iterations of LPA is the gradient of ya(k) with respect to yb: Il (va, vb; k) = ∂ya(k) /∂yb.
The following theorem shows the relationship between feature influence and label influence:
Theorem 2 (Relationship between feature influence and label influence) Assume the activation
function used in GCN is ReLU. Denote va as an unlabeled node, vb as a labeled node, and β as the
fraction of unlabeled nodes. Then the label influence of vb on va after k iterations of LPA equals, in
expectation, to the cumulative normalized feature influence of vb on va after k layers of GCN:
E[Il(va, vb； k)] = Xk=1 βjIf(Va,vb； j).	⑷
Proof of Theorem 2 is in Appendix B. Intuitively, Theorem 2 shows that if vb has high label influence
on va, then the initial feature vector of vb will also affect the output feature vector of va to a large
extent. Theorem 2 provides the theoretical guideline for designing our unified model in the next
subsection.
2.4	The Unified Model
Before introducing the proposed model, we first rethink the GCN method and see what an ideal
node representation should be like. Since we aim to classify nodes, the perfect node representation
would be such that nodes with the same label are embedded close together, which would give a large
separation between different classes. Intuitively, the key to achieve this goal is to enable nodes within
the same class to connect more strongly with each other, so that they are pushed together by the
GCN. We can therefore make edge strengths/weights trainable, then learn to increase the intra-class
feature influence for each class i:	v v : =i =i If(va, vb) by adjusting edge weights. However,
a , b ya ,yb
this requires operating on Jacobian matrices with the size of d(0) × d(K) (d(0) and d(K ) are the
dimensions of initial and output features, respectively), which is impractical if initial node features
are high-dimensional. Fortunately, we can turn to optimizing the intra-class label influence instead,
i.e., Pv ,v :y =i,y =i Il(va, vb), according to Theorem 2. We further show that, by the following
theorem, the intra-class label influence for a given node va is proportional to the probability that va
is classified correctly by LPA:
Theorem 3 (Relationship between label influence and LPA’s prediction) Consider a given node
va and its label ya. If we treat node va as unlabeled, then the total label influence of nodes with
label ya on node va is proportional to the probability that node va is classified as ya by LPA:
X Il(va,vb; k) <x Pr (yap。= ya),	⑸
vb :yb=ya
where ylfa is the predicted label of v。using a k-iteration LPA.
Proof of Theorem 3 is in Appendix C. Theorem 3 indicates that, if edge weights {aij } maximize
the probability that va is correctly classified by LPA, then they also maximize the intra-class label
influence for node v。. We can therefore first learn the optimal edge weights A* by minimizing the
4
Under review as a conference paper at ICLR 2020
loss of predicted labels by LPA:1
A*
arg min Llpa(A)
A
argAmin mm X J(yapa,ya),
Va :agm
(6)
where J is the cross-entropy loss, y?a and y& are the predicted label distribution of Va using LPA and
the true one-hot label of va, respectively.2 a ≤ m means va is labeled. The optimal A* maximize
the probability that each node is correctly labeled by LPA, thus also increasing the intra-class label
influence (by Theorem 3) and intra-class feature influence (by Theorem 2). Then we can apply A*
and the corresponding D* to a GCN to predict labels and learn optimal transformation matrices:
W*
X(k+1) = σ(D*-1A*X(k)W(k)), k = 0,1,…，K - 1,
arg min Lgcn(W, A*)
W
1
arg min —
Wm
E J(ygcn,ya),
Va ; a≤m
(7)
(8)
where ygcn (which is the a-th row of X(K)) is the predicted label distribution of Va using the GCN
specified in Eq. (7).
In practice, it is generally better to combine the above two steps together and train the whole model
in an end-to-end fashion:
W*,A* = arg min Lgcn(W, A) + λLlpa(A),	(9)
W,A
where λ is the balancing hyper-parameter. In this way, Llpa (A) serves as a regularization term
that assists the learning of edge weights A, since it is hard for the GCN to learn both W and A
simultaneously due to overfitting. The proposed GCN-LPA approach can also be seen as learning the
importance of edges that can be used to reconstruct node labels accurately by LPA, then transferring
this knowledge from label space to feature space for the GCN. From this perspective, GCN-LPA
also connects to Theorem 1 except that the knowledge transfer is in the other direction.
It is also worth noticing how the optimal A* is configured. The principle here is that we do not mod-
ify the basic structure of the original graph (i.e., not adding or removing edges) but only adjusting
weights of existing edges. This is equivalent to learning a positive mask matrix M for the adjacency
matrix A and taking the Hadamard product M ◦A = A*. Each element Mij can be set as either a free
variable or a function of the nodes at edge endpoints, for example, Mij = log exp(xi>Hxj) + 1
where H is a learnable kernel matrix for measuring feature similarity.
2.5	Analysis of GCN-LPA Model Behavior
In this subsection, we show benefits of our unified model compared with GCN by analyzing proper-
ties of embeddings produced by the two models. We first analyze the update rule of GCN for node Vi:
x(k+1) = σ(Pvj∙ ∈n (Vi) aj Xjk)W (k)),where aj = aj/du is the normalized weight of edge (j,i).
This formula can be decomposed into the following two steps: (1) In aggregation step, we calculate
the aggregated representation h(k) of all neighborhoods N(Vi): hik) = Pv.∈n(Vi))aijxjk); (2) In
transformation step, the aggregated representation hi(k) is mapped to a new space by a transforma-
tion matrix and nonlinear function: x(k+1) = σ(h(k)W(k)). We show by the following theorem
that the aggregation step reduces the overall distance in the embedding space between the nodes that
are connected in the graph:
Theorem 4 (Shrinking property in GCN) Let D(X) = 11 PVav Gij ∣∣Xi - Xjk2 beadistanCemet-
ric between the embeddings X of nodes. Then we have D(h(k)) ≤ D(X(k)).
1Here the optimal edge weights A* share the same topology as the original graph G, meaning that we do
not add or remove edges from G but only learning the weights of existing edges. See the end of this subsection
for more discussion.
2Here we somewhat abuse the notations for simplicity, since in Theorem 3 the two notations represent label
category rather than label distribution. But the subtle difference can be easily distinguished based on context.
5
Under review as a conference paper at ICLR 2020
(a) Karate club network (b) GCN on the (c) GCN-LPA on the
with noisy edges original network original network
(d) GCN on the
noisy network
(e) GCN-LPA on the
noisy network
Figure 1: Node embeddings of Zachary’s karate club network trained on a node classification task
(red vs. blue). Node coordinates in (b)-(e) are the embedding coordinates. Notice that GCN does
not produce linearly separable embeddings ((b) vs. (c)), while GCN-LPA performs much better even
in the presence of noisy edges ((d) vs. (e)). Additional visualizations are included in Appendix E.
Proof of Theorem 4 is in Appendix D. Theorem 4 indicates that the overall distance among con-
nected nodes is reduced after taking one aggregation step, which implies that connected components
in the graph “shrink” and nodes within each connected component get closer to each other in the
embedding space. In an ideal case where edges only connect nodes with the same label, the aggre-
gation step will push nodes within the same class together, which greatly benefits the transformation
step that acts like a hyperplane W (k) for classification. However, two connected nodes may have
different labels. These “noisy” edges will impede the formation of clusters and make the inter-class
boundary less clear.
Fortunately, in GCN-LPA, edge weights are learned by minimizing the difference between ground-
truth labels and labels reconstructed from multi-hop neighbors. This will force the model to increase
weight/bandwidth of possible paths that connect nodes with the same label, so that labels can “flow”
easily along these paths for the purpose of label reconstruction. In this way, GCN-LPA is able to
identify potential intra-class edges and increase their weights to assist learning clustering structures.
To illustrate this, we apply a two-layer untrained GCN with randomly initialized transformation
matrices to the well-known Zachary’s karate club network (Zachary, 1977) as shown in Figure 1a,
which contains 34 nodes of 2 classes and 78 unweighted edges (grey solid lines). We then increase
the weights of intra-class edges by ten times to simulate GCN-LPA. We find that GCN works well
on this network (Figure 1b), but GCN-LPA performs even better than GCN because the node em-
beddings are completely linearly separable as shown in Figure 1c. To further justify our claim, we
randomly add 20 “noisy” inter-class edges (grey dotted lines) to the original network, from which we
observe that GCN is misled by noise and mixes nodes of two classes together (Figure 1d), but GCN-
LPA still distinguishes the two clusters (Figure 1e) because it is better at “denoising” undesirable
edges based on the supervised signal of labels.
3	Connection to Existing Work
Edge weights play a key role in graph-based node classification as well as representation learning.
In this section, we discuss three lines of related work that learn edge weights adaptively.
Locally linear embedding (LLE) (Roweis & Saul, 2000) and its variants (Zhang & Wang, 2007;
Kong et al., 2012) learn edge weights by constructing a linear dependency between a node and
its neighbors, then use the learned edge weights to embed high-dimensional nodes into a low-
dimensional space. Our work is similar to LLE in the aspect of transferring the knowledge of edge
importance from one space to another, but the difference is that LLE is an unsupervised dimension
reduction method that learns the graph structure based on local proximity only, while our work is
semi-supervised and explores high-order relationship among nodes.
Classical LPA (Zhu et al., 2005; Zhou et al., 2004) can only make use of node labels rather than node
features. In contrast, adaptive LPA considers node features by making edge weights learnable.
Typical techniques of learning edge weights include adopting kernel functions (Zhu et al., 2003;
LiU et al., 2019a) (e.g., aj = exp(- Pd(Xid - xjd)2∕σ2) where d is dimensionality of features),
minimizing neighborhood reconstruction error (Wang & Zhang, 2008; Karasuyama & Mamitsuka,
2013), using leave-one-out loss (Zhang & Lee, 2007), or imposing sparseness on edge weights
6
Under review as a conference paper at ICLR 2020
(Hong et al., 2009). However, in these LPA variants, node features are only used to assist learning
the graph structure rather than explicitly mapped to node labels, which limits their capability in node
classification. Another notable difference is that adaptive LPA learns edge weights by introducing
the regularizations above, while our work takes LPA itself as regularization to learn edge weights.
Our method is also conceptually connected to attention mechanisms on graphs (VelickoVic et al.,
2018; Thekumparampil et al., 2018; Zhang et al., 2018; Liu et al., 2019b), in which an attention
weight αij is learned between node vi and vj. For example, αij = LeakyReLU(a>[Wxi||Wxj])
in GAT (Velickovic et al., 2018), αj = a ∙ cos(WXi,WXj) in AGNN (ThekUmParamPil et al.,
2018), αij = (W1xi)>W2xj in GaAN (Zhang et al., 2018), and αij = a> tanh(W1xi + W2xj)
in GeniePath (Liu et al., 2019b), where a and W are trainable variables. A significant difference
between these attention mechanisms and our work is that attention weights are learned based merely
on feature similarity, while we ProPose that edge weights should be consistent with the distribution
of labels on the graPh, which requires less handcrafting of the attention function and is more task-
oriented. Nevertheless, all the above formulas for calculating attentions can also be used in our
model as the imPlementation of edge weights.
4	Experiments
We evaluate our model and Present its Performance on five datasets including citation networks and
coauthor networks. We also study the hyPer-Parameter sensitivity and Provide training time analysis.
4.1	Datasets
We use the following five datasets in our exPeriments:
Citation networks: We consider three citation network datasets (Sen et al., 2008): Cora, Citeseer,
and Pubmed. In these datasets, nodes corresPond to documents, edges corresPond to citation links,
and each node has a sParse bag-of-words feature vector as well as a class label. Coauthor networks:
We also use two co-authorshiP networks (Shchur et al., 2018), Coauthor-CS and Coauthor-Phy,
based on Microsoft Academic GraPh from the KDD CuP 2016 challenge. Here nodes are authors
and an edge indicates that two authors co-authored a PaPer. Node features rePresent PaPer keywords
for each author’s PaPers, and class labels indicate most active fields of study for each author.
Statistics of the five datasets are shown in Table 1. We also calculate the intra-class edge rate
(the fraction of edges that connect two nodes within the same class), which is significantly higher
than inter-class edge rate in all networks. The finding suPPorts our claim in Section 2.5 that node
classification benefits from intra-class edges in a graPh.
4.2	Baselines
We comPare against the following baselines in our exPeriments: Multi-layer Perceptron (MLP)
and Logistic Regression (LR) are feature-based methods that do not consider the graPh structure.
Label Propagation (LPA) (Zhu et al., 2005), on the other hand, only consider the graPh struc-
ture and ignore node features. The rest of baselines are GCN-based methods: Graph Convolu-
tional Network (GCN) (KiPf & Welling, 2017) ProPoses a first-order aPProximation to sPectral
graph convolutions. Graph Attention Network (GAT) (Velickovic et al., 2018) propose an at-
tention mechanism to treat neighbors differently in the aggregation steP. Jumping Knowledge
Networks (JK-Net) (Xu et al., 2018) leverages different neighborhood ranges for each node to en-
	Cora	Citeseer	Pubmed	Coauthor-CS	Coauthor-Phy
# nodes	2,708^^	3,327	-^19,717^^	18,333	34,493
# edges	5,278	4,552	44,324	81,894	247,962
# features	1,433	3,703	500	6,805	8,415
# classes	7	6	3	15	5
Intra-class edge rate	81.0%	73.6%	80.2%	80.8%	93.1%
Table 1: Dataset statistics after removing self-loops and duplicate edges.
7
Under review as a conference paper at ICLR 2020
Method	Cora	Citeseer	Pubmed	Coauthor-CS	Coauthor-Phy
MLP	64.6 ± 1.7	62.0 ± 1.8	85.9 ± 0.3	91.7 ± 1.4	94.1 ± 1.2
LR	77.3 ± 1.8	71.2 ± 1.8	86.0 ± 0.6	91.1 ±0.6	93.8 ± 1.1
LPA	85.3 ± 0.9	70.0 ± 1.7	82.6 ± 0.6	91.3 ± 0.2	94.9 ± 0.4
GCN	88.2 ± 08	77.3 ± 1.5	87.2 ± 0.4	93.6 ± 1.5	96.2 ± 0.2
GAT	87.7 ± 0.3	76.2 ± 0.9	86.9 ± 0.5	93.8 ± 0.4	96.3 ± 0.7
JK-Net	89.1 ± 1.2	78.3 ± 0.9	85.8 ± 1.1	92.4 ± 0.4	94.8 ± 0.4
GraphSAGE	86.8 ± 1.9	75.2 ± 1.1	84.7 ± 1.6	92.6 ± 1.6	94.5 ± 1.1
GCN-LPA	88.5 ± 1.5 一	78.7 ± 0.6	87.8 ± 0.6 一	94.8 ± 0.4	96.9 ± 0.2
Table 2: Mean and the 95% confidence intervals of test set accuracy for all methods and datasets.

-e- # GCN layers = 1
T- # GCN layers = 2
-B- # GCN layers = 3
23456789
# LPA iterations
2	5	10	15	20
ww1
(LPOd©」©d ©E= Bu-u-e」
Figure 4: Training time Per
epoch on random graphs.
IK IOK IOOK IM
# nodes
Figure 2:	Sensitivity to # LPA
iterations on Citeseer dataset.
Figure 3:	Sensitivity to λ on
Citeseer dataset.
>U2□UU<
O
able structure-aware representation. We use Concat as the aggregator for JK-Net. Graph Sampling
and Aggregation (GraphSAGE) (Hamilton et al., 2017) is a mini-batch implementation of GCN
that uses neighborhood sampling strategy and different aggregation schemes. We use mean as the
aggregator for GraPhS AGE.
4.3	Experimental Setup
Our experiments focus on the transductive setting where We only know labels of part of nodes but
have access to the entire graph as well as features of all nodes.3 The ratio of training, validation,
and test set are set as 6 : 2 : 2. The weight of each edge is treated as a free variable during
training. We train our model for 200 epochs using Adam (Kingma & Ba, 2015) and report the test
set accuracy when validation set accuracy is maximized. Each experiment is repeated three times
and we report the mean and the 95% confidence interval. We initialize weights according to Glorot
& Bengio (2010) and row-normalize input features. During training, we apply L2 regularization to
the transformation matrices and use the dropout technique (Srivastava et al., 2014). The settings of
all other hyper-parameters can be found in Appendix F.
4.4	Results
The results of node classification are summarized in Table 2. Table 2 indicates that only using node
features (MLP, LR) or graph structure (LPA) will lead to information loss and cannot fully exploit
datasets in general. The results demonstrate that our proposed GCN-LPA model surpasses state-
of-the-art GCN/GNN baselines. We note that JK-Net is a strong baseline on Cora, but it does not
perform consistently well on other datasets.
We investigate the influence of the number of LPA iterations and the training weight of LPA loss
term λ on the performance of classification. The results on Citeseer dataset are plotted in Figures
2 and 3, respectively, where each line corresponds to a given number of GCN layers in GCN-LPA.
From Figure 2 we observe that the performance is boosted at first when the number ofLPA iterations
increases, then the accuracy stops increasing and decreases since a large number of LPA iterations
will include more noisy nodes. Figure 3 shows that training without the LPA loss term (i.e., λ = 0)
is more difficult than the case where λ = 1 〜5, which justifies our aforementioned claim that it is
3The experimental setting here is the same as GCN (Kipf & Welling, 2017). But note that our method can
be easily generalized to inductive case if implemented in a way similar to GraphSAGE (Hamilton et al., 2017).
8
Under review as a conference paper at ICLR 2020
Ratio Oflabeled nodes 0%	20%	40%	60%	80%	100%
ACCuraCy	75.8 ± 1.0 76.3 ± 1.1 76.7 ± 0.8 77.3 ± 0.7 78.1 ± 0.6 78.7 ± 0.6
Table 3: Result of GCN-LPA on Citeseer dataset with differet ratio of labeled nodes in LPA.
hard for the GCN part to learn both transformation matriCes W and edge weights A simultaneously
without the assistanCe of LPA regularization.
To further show how muCh the LPA impaCts the performanCe, we vary the ratio of labeled nodes
in LPA from 100% to 0% during training, and report the result of aCuraCy on Citeseer dataset in
Table 3. From Table 3 we observe that the performanCe of GCN-LPA gets worse when the ratio of
labeled nodes in LPA deCreases. In addition, using more labeled nodes in LPA also helps improve
the model stability. Note that a ratio of 0% does not mean that GCN-LPA is equivalent to GCN
(Kipf & Welling, 2017) beCause the edge weights in GCN-LPA is still trainable, whiCh inCreases the
risk of overfitting the training data.
We study the training time of GCN-LPA on random graphs. We use the one-hot identity veCtor
as feature and 0 as label for eaCh node. The size of training set and validation set is 100 and 200,
respeCtively, while the rest is test set. The average number of neighbors for eaCh node is set as 5, and
the number of nodes is varied from one thousand to one million. We run GCN-LPA and GCN for
100 epoChs on a MiCrosoft Azure virtual maChine with 1 NVIDIA Tesla M60 GPU, 12 Intel Xeon
CPUs (E5-2690 v3 @2.60GHz), and 128GB of RAM, using the same hyper-parameter setting as in
Cora. The training time per epoCh of GCN-LPA and GCN is presented in Figure 4. Our result shows
that GCN-LPA requires only 9.2% extra training time on average Compared to GCN.
5 Conclusion and Future Work
In this paper, we studied the theoretiCal relationship between two types of well-known graph-based
models for node ClassifiCation, label propagation algorithm and graph Convolutional neural networks,
from the perspeCtives of feature/label smoothing and feature/label influenCe. We then propose a
unified model GCN-LPA, whiCh learns transformation matriCes and edge weights simultaneously
in GCN with the assistanCe of LPA regularizer. We also analyze why our unified model performs
better than traditional GCN in node ClassifiCation. Experiments on five datasets demonstrate that our
model outperforms state-of-the-art baselines, and it is also highly time-effiCient with respeCt to the
size of a graph.
We point out two avenues of possible direCtions for future work. First, our proposed model foCuses
on transduCtive setting where all node features and the entire graph struCture are given. An interest-
ing problem is whether it Can be applied to induCtive setting where we have no aCCess to test nodes
during training. SeCond, the question of how to generalize the idea of our model to GNNs with
different aggregation funCtions (e.g., ConCatenation or max-pooling) is also a promising direCtion.
References
Xavier Glorot and Yoshua Bengio. Understanding the diffiCulty of training deep feedforward neural
networks. In Proceedings of the 13th International Conference on Artificial Intelligence and
Statistics, 2010.
Chen Gong, DaCheng Tao, Wei Liu, Liu Liu, and Jie Yang. Label propagation via teaChing-to-
learn and learning-to-teaCh. IEEE Transactions on Neural Networks and Learning Lystems, 28
(6), 2017.
Will Hamilton, Zhitao Ying, and Jure LeskoveC. InduCtive representation learning on large graphs.
In Advances in Neural Information Processing Systems, 2017.
Cheng Hong, ZiCheng Liu, and Jie Yang. Sparsity induCed similarity measure for label propagation.
In Proceedings of the 12th IEEE International Conference on Computer Vision. IEEE, 2009.
Masayuki Karasuyama and Hiroshi Mamitsuka. Manifold-based similarity adaptation for label prop-
agation. In Advances in Neural Information Processing Systems, 2013.
9
Under review as a conference paper at ICLR 2020
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the 3rd International Conference on Learning Representations, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In Proceedings of the 5th International Conference on Learning Representations, 2017.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
Proceedings of the 34th International Conference on Machine Learning, 2017.
Deguang Kong, Chris Ding, Heng Huang, and Feiping Nie. An iterative locally linear embedding
algorithm. In Proceedings of the 29th International Coference on International Conference on
Machine Learning. Omnipress, 2012.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In The 32nd AAAI Conference on Artificial Intelligence, 2018.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In Proceedings of the 7th International Conference on Learning
Representations, 2019.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang.
Learning to propagate labels: Transductive propagation network for few-shot learning. In Pro-
ceedings of the 7th International Conference on Learning Representations, 2019a.
Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath:
Graph neural networks with adaptive receptive paths. In The 33rd AAAI Conference on Artificial
Intelligence, 2019b.
Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. In Proceedings
of the 36th International Conference on Machine Learning, 2019.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. science, 290(5500), 2000.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3), 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. In Neural Information Processing Systems Workshop on
Relational Representation Learning, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
DroPout: a simPle way to Prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1), 2014.
Kiran K ThekumParamPil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graPh neural
network for semi-suPervised learning. arXiv preprint arXiv:1803.03735, 2018.
Petar VeliCkovic, Guillem CucurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. GraPh attention networks. In Proceedings of the 6th International Conference on Learn-
ing Representations, 2018.
Fei Wang and Changshui Zhang. Label ProPagation through linear neighborhoods. IEEE Transac-
tions on Knowledge and Data Engineering, 20(1), 2008.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. RePresentation learning on graPhs with jumPing knowledge networks. In Proceedings
of the 35th International Conference on Machine Learning, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are graPh neural
networks? In Proceedings of the 7th International Conference on Learning Representations,
2019.
Wayne W Zachary. An information flow model for conflict and fission in small grouPs. Journal of
anthropological research, 33(4), 1977.
10
Under review as a conference paper at ICLR 2020
Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan:
Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint
arXiv:1803.07294, 2018.
Xinhua Zhang and Wee S Lee. Hyperparameter learning for graph based semi-supervised learning
algorithms. In Advances in Neural Information Processing Systems, 2007.
Zhenyue Zhang and Jing Wang. Mlle: Modified locally linear embedding using multiple weights.
In Advances in Neural Information Processing Systems, 2007.
Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Scholkopf. Learn-
ing with local and global consistency. In Advances in Neural Information Processing Systems,
2004.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International Conference on Machine
Learning, 2003.
Xiaojin Zhu, John Lafferty, and Ronald Rosenfeld. Semi-supervised learning with graphs. PhD
thesis, Carnegie Mellon University, language technologies institute, school of , 2005.
11
Under review as a conference paper at ICLR 2020
Appendix
A Proof of Theorem 1
Proof. Denote dj = aj / du as the normalized weight of edge (j, i). Itis clear that Ej∈n ,)Gij =
1. Given that M is differentiable, we perform a first-order Taylor expansion with Peano’s form of
remainder at xi for Pj∈N (i) aGij yj:
Gaijyj =	aGijM(xj )
j∈N(i)	j∈N(i)
= X aij (M(Xi) + 'MxXi) (Xj - χi) + o(kxj - χi∣∣2))
j∈N(i)	∂x
=M(χi)+ dM(xi) X aij(Xj - χi)+ X aijo(kxj - xik2)
∂X	j∈N(i)	j∈N(i)
∂M(Xi)
=yi	^ξ~r- Ci +)/ aij O(IlXj - xik2).
∂X	j∈N(i)
According to Cauchy-Schwarz inequality and L-Lipschitz property, we have
dM(Xi)	<	dM(Xi)	H H ‹ r∣∣ H
^X^ei ≤ -^X^ Jeik2 ≤ Lkeik2.
Therefore, the approximation of yi is bounded by
yi -	aaij yj
j∈N(i)
∂M(Xi)
∂γ T	ei 一工 aij o(kXj - Xik2)
∂X	j∈N(i)
(10)
(11)
(12)
≤
∂M(Xi)
∂γT ei + 工 aijo(kXj - Xik2)
∂X	j∈N(i)
≤Lkeik2+o jm∈Na(xi)(kXj - Xik2) .

B Proof of Theorem 2
Before proving Theorem 2, we first give two lemmas that demonstrate the exact form of feature
influence and label influence defined in this paper. The relationship between feature influence and
label influence can then be deduced from their exact forms.
Lemma 1 Assume that the nonlinear activation function in GCN is ReLU. Let Pka→b be a path
[v(k), v(k-1), ∙∙∙ , v(0)] of length k from node Va to node Vb, where v(k) = Va, v(0) = Vb, and
VUT) ∈ N(v(i)) for i = k, ∙∙∙ , 1. Then we have
1
Iaf (Va, Vb; k) = ∑πaav(i-1),v(i),	(13)
Pa→b i=k
where aav(i-1),v(i) is the normalized weight of edge (V(i), V(i-1)).
Proof. See Xu et al. (2018) for the detailed proof.
The product term in Eq. (13) is the probability ofa given path Pka→b. Therefore, the right hand side
in Eq. (13) is the sum over probabilities of all possible paths of length k from Va to Vb, which is the
probability that a random walk starting at Va ends at Vb after taking k steps.
12
Under review as a conference paper at ICLR 2020
(a) Iteration 1
(b) Iteration 2
(c) Iteration 3
(d) Paths from va to vb
Figure 5: An illustrating example of label propagation in LPA. Suppose labels are propagated for
three iterations, and no self-loop exists. Blue nodes are labeled while white nodes are unlabeled. (a)
va’s label propagates to v1 (yellow arrows). Note that the propagation of va’s label to v3 is cut off
since v3 is labeled thus absorbing va ’s label. (b) va ’s label that propagated to v1 further propagates
to v2 and vb (yellow arrows). Meanwhile, va ’s label is reset to its initial value then propagates
from va again (green arrows). (c) The propagating process in iteration 3. Purple arrows denote the
propagation of va’s label starting from va for the third time. (d) All possible paths of length no more
than three from va to vb containing unlabeled nodes only. Note that there is no path of length one
from va to vb .
Lemma 2 Let Uja→b be a path [v(j),v(j-1),…，v(0)] of length j from node Va to node Vb, where
v(j) = Va, V(O) = Vb, v(i-1) ∈ N(V⑻)for i = j, ∙∙∙ , 1, and all nodes along the path are unlabeled
except V(0). Then we have
k1
Il(Va,Vb； k) =E E U&v(i-i),v(i),
j =1 Uja→b i=j
(14)
where "(一)… is the normalized weight of edge (V ⑴,V(iT)).
To intuitively understand this lemma, note that there are two differences between Lemma 1 and
Lemma 2: (1) In Lemma 1, If (Va, Vb; k) sums over all paths from Va to Vb of length k, but in
Lemma 2, Il(Va, Vb; k) sums over all paths from Va to Vb of length no more than k. The is because in
LPA, Vb’s label is reset to its initial value after each iteration, which means that the label of Vb serves
as a constant signal that begins propagating in the graph again and again after each iteration. (2) In
Lemma 1 we consider all possible paths from Va to Vb, but in Lemma 2, the paths are restricted to
contain unlabeled nodes only. The reason here is the same as above: Since the labels of labeled nodes
are reset to their initial values after each iteration in LPA, the influence of Vb ’s label will be absorbed
in labeled nodes, and the propagation of Vb’s label will be cut off at these nodes. Therefore, Vb’s
label can only flow to Va along the paths with unlabeled nodes only. See Figure 5 for an illustrating
example showing the label propagation in LPA.
Proof. As mentioned above, a significant difference between LPA and GCN is that all labeled
nodes are reset to its original labels after each iteration in LPA. This implies that the initial label
yb of node Vb appears not only as y(0), but also as every y(j) for j = 1,…，k - 1. Therefore, the
influence of yb on yθak') is the cumulative influence of yY) on yθak') for j = 0,1,…，k — 1:
∂y(k)	k-1 ∂y(k)
Il(Va” k) = ɪr = X τιjy ∙	(15)
∂yb	j =0 ∂yb
According to the updating rule of LPA, we have
∂yak) _ d Pvz∈N(Va) Gazyzk 1) _ X 〜∂y(k-1)
Tn万=	777)	=	aaz A (j) ∙	(16)
∂yb	∂yb	vz∈N(va )	∂yb
In the above equation, the derivative ^yk is decomposed into the weighted average of dy(⑴),
∂yb(j)	∂yb(j)
where Vz traverses all neighbors of Va. For those Vz’s that are initially labeled, yz(k-1) is reset to
13
Under review as a conference paper at ICLR 2020
their initial labels in each iteration. Therefore, they are always constant and independent of yb(j),
meaning that their derivatives w.r.t. yb(j) are zero. So we only need to consider the terms where vz is
an unlabeled node:
∂y' _ X ∂ ∂yZkT)
jΓ= =	aaz ( (j),
∂yb	vz∈N(va),z>m	∂yb
(17)
where z > m means vz is unlabeled. To intuitively understand Eq. (17), one can imagine that we
perform a random walk starting from node va for one step, where the “transition probability” is the
edge weights a, and all nodes in this random walk are restricted to unlabeled nodes only. Note that
we can further decompose every yz(k-1) in Eq. (17) in the way similar to what we do for ya(k) in
Eq. (16). So the expansion in Eq. (17) can be performed iteratively until the index k decreases to j.
This is equivalent to performing all possible random walks for k - j steps starting from va, where
all nodes but the last in the random walk are restricted to be unlabeled nodes:
Q
∂y(j)
EEl ∏ "),v(i)
vz ∈V Uka-→jz	i=k-j
∂y Zj)
∂y(j)，
(18)
where vz in the first summation term is the end node of a random walk, Uka-→jz in the second summa-
tion term is an unlabeled-nodes-only path from va to vz of length k - j , and the product term is the
∂ (j)	∂ (j)
probability of a given path Ua→z. Consider the last term in Eq. (18). We know that ≤¾∙ = 0
k-j	∂yb(j)	∂yb(j)
for all z = b and dyz^- = 1 for Z = b, which means that only those random-walk paths that end
∂yb
exactly at vb (i.e., the end node vz is exactly vb) count for the computation in Eq. (18). Therefore,
we have
dyk _ XYY 〜
j-= =	H av(i-1) ,v(i)，
∂yb	Uka-→jb i=k-j
(19)
where Uka-→jb is a path from va to vb of length k - j containing only unlabeled nodes except vb .
Substituting the right hand term of Eq. (15) with Eq. (19), we obtain that
k-1	1	k	1
Il(va,vb;k)=	av(i-1) ,v(i) =∑∑πav(i-1) ,v(i).	(20)
j=0 Uka-→jb i=k-j	j=1Uja→b i=j
Now Theorem 2 can be proved by combining Lemma 1 and Lemma 2:
Proof. Suppose that whether a node is labeled or not is independent of each other for the given
graph. Then we have
k1
EIl (va, vb; k) =EIXXnav(i-1) ,v(i)
j=1Uja→b i=j
k1
EE E ∏av(i-i),v(i)
j=1	Uja→b i=j
k1
E E Pr (Pja→b is an unlabeled-nodes-only path) ɪɪ av(i—i),v(i)
j=1Pa→b	i=j
k1
βj	aav(i-1) ,v(i)
j=1Pa→b	i=j
k
X βj Iaf (va, vb; j).
j=1
(21)

14
Under review as a conference paper at ICLR 2020
C Proof of Theorem 3
Proof. Denote the set of labels as L. Since different label dimensions in y!" do not interact with
each other when running LPA, the value of the ya-th dimension in y。(denoted by y「[y。]) comes
only from the nodes with initial label ya . It is clear that
k1
ya(k)[ya]=	X XXYav(i-1) ,v(i) ,	(22)
vb:yb=ya j=1 Uja→b i=j
which equals Pv :y =y Il(va, vb; k) according to Lemma 2. Therefore, we have
PMya = ya) =	ya [yk)	H yak) [ya] = X II(VaNb； k)	(23)
i∈L ya [i]	vb:yb=ya
D Proof of Theorem 4
In this proof we assume that the dimension of node representations is one, but note that the conclu-
sion can be easily generalized to the case of multi-dimensional representations since the function
D(x) can be decomposed into the sum of one-dimensional cases. In the following of this proof, we
still use bold notations xi(k) and hi(k) to denote node representations, but keep in mind that they are
scalars rather than vectors.
We give two lemmas before proving Theorem 4. The first one is about the gradient of D(x):
Lemma 3 h(k) = Xik) -普翁).
Proof.	Xk-	dDxk1	= Xk-	PVj∈N(Vi)	&ij (X(k)	- Xjk))	= PVj∈N(Vi) &ijXjk)	=	h(k).	□
i
It is interesting to see from Lemma 3 that the aggregation step in GCN is equivalent to running
gradient descent for one step with a step size of one. However, this is not able to guarantee that
D(h(k)) ≤ D(X(k)) because the step size may be too large to reduce the value of D.
The second lemma is about the Hessian of D(X):
Lemma 4 V2D(x) W 2I, or equivalently, 2I — V2D(x) is a positive Semidefinite matrix.
Proof.	We first calculate the Hessian of D(x) =	4 ∑vi,v3 aij kxi	— xjk22:
	1 — an	—a12	…	—a1n -	
	—α21	1 — 1¾2	…	—a2n	
	V2D(x) =	.	. .. ..	..	. ..	= I — D-1A.	(24)
	—-an1	- an2	• ∙ ∙	1 - ann	
Therefore, 2I - V2D(X) = I + D-1A. Since D-1A is Markov matrix (i.e., each entry is non-
negative and the sum of each row is one), its eigenvalues are within the range [-1, 1], so the eigen-
values of I + D-1A are within the range [0, 2]. Therefore, I + D-1A is a positive semidefinite
matrix, and we have V2D(X) W 2I.
We can now prove Theorem 4:
Proof. Since D is a quadratic function, we perform a second-order Taylor expansion ofD around
X(k) and obtain the following inequality:
D(h(k)) =D(x(k)) + VD(x(k) )>(h(k) — x(k)) + 1(h(k) — x(k))>V2D(x)(h(k) — x(k))
=D(x(k)) — VD(x(k) )>VD(x(k)) + 1 VD(x(k))>V2D(x)VD(x(k))	(25)
≤D(x(k)) — VD(x(k))>VD(x(k)) + VD(x(k))>VD(x(k)) = D(x(k)).

15
Under review as a conference paper at ICLR 2020
(a) GCN on the original network
(b) GCN-LPA on the original network
(c) GCN on the noisy network
(d) GCN-LPA on the noisy network
Figure 6: Visualization of GCN and GCN-LPA with 1 〜4 layers on karate ClUb network.
E More Visualization Results on Karate Club Network
FigUre 6 illUstrates more visUalization of GCN and GCN-LPA on karate clUb network. In each
sUbfigUre, we vary the nUmber of layers from 1 to 4 to examine how the learned representations
evolve. The initial node featUres are one-hot identity vectors, and the dimension of hidden layers
and oUtpUt layer is 2. The transformation matrices are Uniformly initialized within range [-1, 1]. We
Use sigmoid fUnction as the nonlinear activation fUnction. Comparing the foUr figUres in each row,
we conclUde that the aggregation step and transformation step in GCN and GCN-LPA do benefit the
separation of different classes. Comparing FigUre 6a and 6c (or FigUre 6b and 6d), we conclUde that
more inter-class edges will make the separation harder for GCN (or GCN-LPA). Comparing FigUre
6a and 6b (or FigUre 6c and 6d), we conclUde that GCN-LPA is more noise-resistant than GCN,
therefore, GCN-LPA can better differentiate classes and identify clUstering sUbstrUctUres.
F Hyper-parameter Settings
The detailed hyper-parameter settings for all datasets are listed in Table 4. In GCN-LPA, we Use
the same dimension for all hidden layers. Note that the nUmber of GCN layers and the nUmber of
LPA iterations can actUally be different since GCN and LPA are implemented as two independent
modUles. We Use grid search to determine hyper-parameters on Cora, and perform fine-tUning on
16
Under review as a conference paper at ICLR 2020
	Cora	Citeseer	Pubmed	Coauthor-CS	Coauthor-Phy
Dimension of hidden layers	32	16	32	32	32
# GCN layers	5	2	2	2	2
# LPA iterations	5	5	1	2	3
L2 weight	1 × 10-4	5 × 10-4	2 × 10-4	1 × 10-4	1 × 10-4
LPA weight (λ)	10	1	1	2	1
Dropout rate	0.2	0	0	0.2	0.2
Learning rate	0.05	0.2	0.1	0.1	0.05
Table 4: Hyper-parameter settings for all datasets.
other datasets, i.e., varying one hyper-parameter per time to see if the performance can be further
improved. The search spaces for hyper-parameters are as follows:
•	Dimension of hidden layers: {8, 16, 32};
•	# GCN layers: {1,2,3,4,5,6};
•	# LPA iterations: {1,2,3,4,5,6,7,8,9};
•	L2 weight: {10-7, 2 × 10-7, 5 × 10-7, 10-6, 2 × 10-6,5 × 10-6,10-5,2 × 10-5,5 ×
10-5,10-4,2 × 10-4,5 × 10-4,10-3};
•	LPA weight (λ): {0, 1, 2, 5, 10, 15, 20};
•	Dropout rate: {0, 0.1, 0.2, 0.3, 0.4, 0.5};
•	Learning rate: {0.01, 0.02, 0.05, 0.1, 0.2, 0.5};
17