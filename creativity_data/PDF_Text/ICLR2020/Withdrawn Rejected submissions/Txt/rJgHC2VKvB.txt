Under review as a conference paper at ICLR 2020
Recurrent Neural Networks are
Universal Filters
Anonymous authors
Paper under double-blind review
Ab stract
Recurrent neural networks (RNN) are powerful time series modeling tools in ma-
chine learning. It has been successfully applied in a variety of fields such as natural
language processing (Mikolov et al. (2010), Graves et al. (2013), Du et al. (2015)),
control (Fei & Lu (2017)) and traffic forecasting (Ma et al. (2015)), etc. In those
application scenarios, RNN can be viewed as implicitly modelling a stochastic dy-
namic system. Another type of popular neural network, deep (feed-forward) neural
network has also been successfully applied in different engineering disciplines,
whose approximation capability has been well characterized by universal approxi-
mation theorem (Hornik et al. (1989), Park & Sandberg (1991), Lu et al. (2017)).
However, the underlying approximation capability of RNN has not been fully
understood in a quantitative way. In our paper, we consider a stochastic dynamic
system with noisy observations and analyze the approximation capability of RNN
in synthesizing the optimal state estimator, namely optimal filter. We unify the
recurrent neural network into Bayesian filtering framework and show that recurrent
neural network is a universal approximator of optimal finite dimensional filters
under some mild conditions. That is to say, for any stochastic dynamic systems
with noisy sequential observations that satisfy some mild conditions, we show that
(informal)
∀e > 0, ∃ RNN-based filter, s.t. limsup ∣∣Xk∣k - E[xk∣Yk] ∣∣ < e,
k→∞
where Xk∣k is RNN-based filter,s estimate of state Xk at step k conditioned on
the observation history and E[xk|Yk] is the conditional mean of xk, known as the
optimal estimate of the state in minimum mean square error sense. As an interesting
special case, the widely used Kalman filter (KF) can be synthesized by RNN.
1	Introduction
Recurrent neural network (RNN) is a certain type of neural networks characterized by hidden
variables that memorize the history of input sequences, and it has been successfully applied and
brought amazing results in many different disciplines including computer vision, natural language
processing and optimal control, etc. (Mikolov et al. (2010), Graves et al. (2013), Du et al. (2015),
Fei & Lu (2017), Ma et al. (2015)). Its huge empirical success in different engineering disciplines is
grounded on the expressive power of RNN. However, how to understand the expressive power of
RNN in a quantitative way is not fully understood. Even what RNN expresses is not totally clear.
Another type of neural network, deep feed forward neural network has been well characterized as a
universal function approximator (Hornik et al. (1989), Park & Sandberg (1991)). However, a similar
way to characterize the expressive power of RNN is not obvious.
DNN is a mapping from a finite dimensional Euclidean space to another finite dimensional Euclidean
space, that is to say it can be regarded as a vector-valued multivariate function. However, RNN is a
mapping from a sequence space to another sequence space and the current output depends on both
current input and the whole observation history. The input sequence, in principle, can be infinite.
The function of RNN is capturing the relationship between input process and output process. For
example, in machine translation, the input process (or the observation process) is sentence in one
language and the output process (or the state process) is sentence in another language. And in many
other RNN’s application scenarios such as traffic forecast and optimal control, the input is a noisy
1
Under review as a conference paper at ICLR 2020
observation or measurement sequence and the output is an estimate sequence of a certain quantity,
e.g., the traffic speed.. We observe that the function of RNN is quite similar to a filter.
In our paper, we propose to characterize the expressive power of RNN in a quantitative way from the
perspective of filtering. We consider a discrete filtering system as 1.
xk = f (xk-1) + g(xk-1)wk-1,
yk = h(xk) + vk,
(1)
where the state xk at time instant k is an n-dimensional vector, f is an n-dimensional vector-
valued function, g is an n X r matrix-valued function, {wk, k = 0,1,…} is an r-dimensional
white Gaussian process and Wk 〜N(0, Q), where Q is the covariance matrix of wk. yk is the
m-dimensional observation (measurement) process, h is an m-dimensional vector-valued function,
{vk, k = 1,…} is an m-dimensional white Gaussian process and Vk 〜N(0, R), where R is the
covariance matrix of Vk. And We assume that {wk, k = 0,1, ∙∙∙}, {vk, k = 1, ∙∙∙} and the initial
state x0 are jointly independent. We use Yk to denote the sequence of observations up to time instant
k, i.e.,
Yk := {yι,…，yk}∙
(2)
Given the realization of the sequence of observations Yk, the aim of filtering problem is to compute
the optimal estimate of xk conditioned on Yk.
Not surprisingly, recurrent neural network has been proposed to do filtering. James Ting Ho Lo
showed that Recurrent Multi-layer Perceptron can be used to synthesize optimal filter (Lo (1994)).
However, Lo’s approach is based on simply copying and storing the whole observation history in
the hidden variables and thus require the time horizon to be finite, which is fundamentally limited.
Besides Lo’s work, many efforts have been made to connect RNN and dynamical system. Wilson &
Finkel (2009) implemented a neural network based Kalman Filter (KF) but did not provide theoretical
analysis on the approximation error. Parlos et al. (2001) proposed an algorithmic approach to do
nonlinear filtering using recurrent neural network architecture but did not provide theoretical gurantee.
Schafer & Zimmermann (2006) shows that recurrent neural network is a universal approximator
of dynamical system. However, they only consider the deterministic system dynamics and do not
analyze the filtering relationship between two stochastic processes. Around the same time our work
was under review, a similar paper was also uploaded in arXiv by Lim et al. (2019), The results in the
arXiv paper are similar to the June, 2018 graduation thesis of the first author of the current paper
in the use of sufficient statistics, and emphasize on the design and implementation of the algorithm.
Our current paper has been dramatically improved by introducing the error analysis and asymptotic
convergence of the algorithm. And our work is inspired by the finite dimensional filter such as Benes
filter Benes (1981).
Compared to the existing work, we make the following specific contributions:
•	Motivated by the similarity between RNN and filter, we propose to use the ability to
approximate optimal filter to characterize the expressive power of RNN. Unlike existing work
on expressive power of RNN (Schafer & Zimmermann (2006)) where only deterministic
system is considered, we consider a stochastic dynamic system with noisy observations and
analyze the capability of RNN to estimate unknown state.
•	We unify RNN-based filter into Bayesian filtering framework. In this framework, the hidden
variables of RNN are interpreted as statistics of the observation history. And the evolution
of hidden variables are interpreted as the evolution of statistics.
•	Based on the Bayesian filtering framework, we derive our main result: Recurrent Neural
Networks (RNN) are universal approximators of a large class of optimal finite dimensional
filters. That is to say, RNN estimator’s asymptotic estimation error can be as close to mini-
mum mean square error as desired. As an interesting special case, the widely used Kalman
Filter can be synthesized by RNN. The consideration of asymptotic error differentiates us
from existing work on expressive power of RNN (Schafer & Zimmermann (2006)).
2	Preliminary: The Bayesian framework of filtering
We first introduce the definition of minimum mean square error estimate.
2
Under review as a conference paper at ICLR 2020
Definition 1 (Minimum Mean Square Error (MMSE) Estimate (Jazwinski (1970))). Let X be an
estimate of random variable x and LMSE ：= (X — X)T (X — X) ∙ The estimate that minimizes E[Lmse]
is called the minimum mean square error estimate.
Theorem 1 (Theorem 5.3 in Jazwinski (1970)). Conditioned on the observation history Yk, the
minimum mean square error estimate of state Xk is the conditional mean E[Xk |Yk].
Proof. Proof can be found in our appendix A.1.
□
The Bayesian filtering consists of recursive prediction and update procedures (Jazwinski (1970)):
Prediction Step p(Xk-1 |Yk-1) → p(Xk|Yk-1): Given the posterior distribution p(Xk-1 |Yk-1) at
instant k — 1, the prior distribution p(Xk|Yk-1) of Xk satisfies the Chapman-Kolmogorov equation:
p(Xk|Yk-1)
p(Xk|Xk-1)p(Xk-1|Yk-1) dXk-1;
(3)
Updating Step p(Xk|Yk-1) → p(Xk|Yk): Given prior distributionp(Xk|Yk-1), when the observation
yk at instant k arrives, the posterior distribution p(Xk|Yk) at instant k is given by equation 4,
(∣γ ) =	p(yk|Xk)P(XkIK-I)
k Xk k	Rp(yk ∣Xk)p(Xk ∣Yk-ι) dXk
(4)
Then we can get the MMSE estimate by simply doing integration:
E[XkIYk]
p(XkIYk)Xk dXk.
(5)
To facilitate subsequent discussion, we make the following definition.
Definition 2 (Sufficient Statistic (Benes (1981))). If the conditional distribution P(Xk ∣Yk) (or
p(Xk IYk-1)) can be fully determined by a vector-valued function sk|k (or sk|k-1) of the obser-
vation sequence Yk (or Yk-1), then we say sk|k (or sk|k-1) is a sufficient statistic forP(XkIYk) (or
P(Xk IYk-1)).1
Because sufficient statistic sk|k fully determines the posterior distribution P(XkIYk) and MMSE
estimate E(Xk IYk) is a functional of P(Xk IYk), there exists a function γ that maps sk|k to E[Xk IYk],2
i.e.,
E[Xk|Yk] = Y(Sk|k).	(6)
3	RNN based filter’s architecture
Motivated by the Bayesian framework of filtering, we propose the RNN based filter’s architecture as
shown in Fig. 1.
Our RNN based filter’s architecture, Bayesian Filter Net (BFN), consists of three parts: prediction
network, update network and estimation network. To mimic the prediction step in Bayesian filtering,
we use the prediction network to map the posterior distribution representation vector to a prior
distribution representation vector. To mimic the update step in Bayesian filtering, we then use an
update network to update the prior state distribution representation vector and the observation to get
the posterior state distribution representation vector. Finally, we use an estimation network to map
the current posterior state distribution representation vector to current estimation Xk. We will see
in the subsequent discussions, the so-called representation vector or hidden variables indeed can be
interpreted as statistics of the underlying conditional distribution.
1 The sufficient statistic sk|k (or sk|k-1) can be any vector-valued function of Yk (or Yk-1) as long as it can
fully determine p(xk |Yk) (or p(xk |Yk-1)). Taking linear system as an example, sk|k can be the vector composed
of the conditional mean and covariance because these two quantities sufficiently determine the conditional
density function of the state in linear filtering system. (See more details in our Appendix A.3.)
2For instance, as for the Kalman Filter in linear system, the sufficient statistic sk|k can be chosen to be the
conditional mean and conditional covariance, thus γ function can be the function that copy the conditional mean
part of sk|k.
3
Under review as a conference paper at ICLR 2020
Figure 1: RNN based filter's architecture: Bayesian Filter Net (BFN).
4	RNN based filter is universal
We now show that our proposed RNN based filter is universal in that it can approximate a large class
of optimal finite dimensional filters to any asymptotic accuracy we desire. We summarize our insight
into the diagram 2.
Vfc-ι)
yk updates the distribution.
p(a⅛ lfc)1
Predict the next step distribution.
Ma⅛+ι∣K)
Finite
DimenSiona
Prior
Statistics
Finite
Dimensional
Posterior
Statistics
Statistics Update Mapping φ.
Finite
Dimensional
Prior
Statistics
Statistics Prediction Mapping φ.
⅛-ι
XUPdate network。.
Q PrediCtion network。.
¾+ι∣*
Combine.
Statistics to First Moment Mapping 7.
≈ Estimation network 7.
Combine.
2∕fc+ι
Figure 2: Our approach,s illustrative diagram
As shown in the diagram 2, we model the conditional prior probability p(xk|Yk-1) posterior
probability p(xk |Yk) resp. of the state at step k by finite dimensional prior statistics sk|k-1 (posterior
statistics Sk∣k resp.). In finite dimensional filter case (Benes (1981; 1985); Daum (1987)), there exist
finite dimensional statistics that are sufficient, that is to say, the evolution of conditional probability
can be fully captured by the evolution of a finite dimensional vector. We denote the evolution
function in updating step by 夕 and the evolution function in prediction step by φ. And further, after
modelling the the probability distribution as finite dimensional statistics, we use two neural networks
to approximate the evolution of them. And to get the final estimate of the state, we use another neural
network to approximate the map from the statistics to the optimal estimation.
For the RNN based filter, one can naturally ask:
1.	Will the neural network approximation error accumulate and blow up when time goes to
infinity and make RNN asymptotically not work at all?
2.	How general is this approach? What filter can RNN approximate?
We give the answers in this section.
4
Under review as a conference paper at ICLR 2020
4.1 Kalman Filter (KF) can be synthesized by RNN
When system equation 1 is linear and satisfies Gaussian noise assumption as shown in equation 7,
it is well known that the filtering problem can be optimally solved by Kalman Filter (KF) (Kalman
(1960)). (See more details in our Appendix A.3.)
xk = F xk-1 + Gwk-1
yk = Hxk + vk ,
(7)
where F, G, H are constant matrices with proper dimensions, the initial state x0 is Gaussian, and
{wk, k = 0,1, ∙∙∙} and {vk, k = 1, .一} are two independent white Gaussian sequences that are
also independent of the initial state x0.
It can be known that the prior and posterior distributions are Gaussian and fully characterized by the
sufficient statistics s composed of mean and covariance matrix. (See more details in our Appendix
A.3.)
sk|k-1 := [mk|k-1, vecT(Pk|k-1)]T,	(8)
sk|k := [mk|k, vecT (Pk|k)]T,
where mk|k-1 and Pk|k-1 are the conditional mean and covariance in step k conditioned on Yk-1
and mk∣k and Pk∣k are the mean and covariance in step k conditioned on Yk, and vec(◦m ×n) is the
n1 n2 × 1 column vector obtained by stacking the columns of the matrix ◦ on top of one another.
sk|k (sk|k-1 resp.) is the theoretical statistic that determines the conditional probability distribution
of the state xk conditioned on the observation history Yk (Yk-1 resp.) and evolves according to some
function 夕 and φ. (See more details in our Appendix A.3.) We also know that there exists some
function γ that maps sk|k to MMSE estimate E[xk|Yk]. Thus we have,
Sk∣k =2(Sk∣k-I,yk), Sk+ι∣k = φ(sk∣k), E[xkIYk] = γ(sk∣k).	(9)
We use function 0 generated by a deep (feedforward) neural network (i.e. update network) to
approximate 0, and use φ generated by another DNN (i.e. prediction network) to approximate φ.
And the numerical statistics computed by RNN are denoted as Sk∣k and Sk+ι∣k, i.e.,
Sk∣k = 0(Sk∣k-ι,yk), Sk+ι∣k = Φ(Sk∖k).	(10)
And we use function 7 generated by the third deep feedforward neural network (i.e. estimation
network) to approximate γ in equation 6, i.e.,
xk∣ k = Y(SkIk ).
(11)
We consider the probability space (Ω, F, P) with inner producthx, y)= E[xTy] and norm ∣∣xk :=
E1/2[xTx], which is a Hilbert space and denoted as L2(Ω, F, P). We first state the universal
approximation theorem of feedforward neural network before we proceed to show our results.
Theorem 2 (Universal Approximation Theorem (Hornik et al. (1989))). For any given compact
subset K ⊂ Rn, any given continuous function f defined on K and any given accuracy degree > 0,
there exists a function g represented by a single-hidden-layer neural network with non-constant and
bounded activation function such that maxx∈K |f (x) - g(x)| < .
Proof. It is a natural corollary of the Thm. 2.1 in Hornik et al. (1989).	口
Define ek∣k := ∣∣Sk∣k 一 Sk∖k ∣∣, which represents the cumulative error caused by the approximation
error of φ and 0. Similarly, we define ek∣k-ι := ∣∣Sk∣k-ι — Sk∣k-ι∣. In the following theorem, we
shall give the condition which ensures the cumulative error will not blow up as time k approaches ∞.
Before we proceed to show our main result, we first establish a key lemma, Lem. 2. We make two
assumptions on the system we’ll consider..
Assumption 1. We assume that the linear dynamic system of the state in equation 7 is stable in mean
square Sense (Samuels (1959)), i.e.,	_
lim Ilxk k ≤ M,
k→∞
(12)
where M is a finite constant.
5
Under review as a conference paper at ICLR 2020
Assumption 2. The dynamical system equation 7 is uniformly completely observable and uniformly
completely controllable.
The definitions of uniformly completely observable and uniformly completely controllable can be
found in section 7.5 of Jazwinski (1970). We then state a lemma on the boundedness of conditional
covariance.
Lemma 1 (Lemma 7.1 in Jazwinski (1970)). If Assumption 2 is satisfied and P0|0 < 03 *, then Pk|k is
uniformly bounded from above for all k ≥ N,
Pk|k 4 (1+αβ) I，k ≥ N,	(13)
where N is a positive integer, I is the n × n identity matrix and α, β are positive constants.
Based on Lemma 1, Assumption 1 and Assumption 2, we give the key Lemma 2.
Lemma 2. In the discrete linear system equation 7, suppose the Assumption 1 and Assumption 2 are
satisfied, then for any given > 0 there exists a compact subset K ⊂ Rdim(sk|k) such that the statistics
computed by KF Sk∣k-ι, Sk∣k and the statistics Sk∣k-ι, Sk∣k computed by RNN based filter with non-
constant and bounded activation function satisfy ∣∣Sk∣k-ιIskIk-ι∈κ ∣∣ < G ∣∣Sk∣klsk∣k∈κ ∣∣ < G
∣∣Sk∣k-ι IgkIk-ι∈κ ∣∣ < C and ||3k区1舐块？长 ∣∣ < G where IA is an indicator function.
Proof. Proof can be found in our appendix A.4.	□
We then derive our main result.
Theorem 3. Assume sk|k, k ≥ 0 are the theoretical statistics evolving according to equation 9
and Sk∣k, k ≥ 0 are the real statistics computed by our RNN-based filter evolving according to
equation 10. Suppose the Assumption 1 and Assumption 2 are satisfied. Furthermore, we need to
assume functions 夕,φ, Y are Lipschitz, i.e.,for any si, s2,
13(SLy)-夕(S2,y州 ≤ CWksI- s2k,
kφ(s1) - φ(s2)k ≤ Cφ ks1 - s2k,	(14)
kγ(S1) - γ(S2)k ≤ Cγ kS1 - S2k,
where CW and Cφ are Lipschitz constants. If CW and Cφ satisfy ∣CφCφ∣ < 1, then for any c > 0,
there exists an RNN based filter (with non-constant and bounded activation function) such that
limsup ek∣k = limsup ∣∣Sk∣k - Sk∣k ∣∣ < c.	(15)
k→∞	k→∞
Furthermore, we have
limsup ∣∣Xk∣k - E[xk∣Yk]∣∣ < c.	(16)
k→∞
Proof. For any δ > 0, we have the following. By Lem. 3, there exists a compact ball K = B(0, r) ⊂
Rdim(sk|k), such that ∣∣Sk∣k-ιIskIk-i∈k∣∣ < δ, ∣∣Sk∣klsk∣k∈κ∣∣ < δ, ∣∣Sk∣k-ιIgkIk-]∈k∣∣ < δ and
∣∣Sk∣klgk∣k∈κ∣∣ < δ. By Theorem 2, given any small 6中,δφ ∈ R+ and δγ ∈ R+, there exist two
functions 夕,φ which are represented by the DNN, such that
陷-0k∞ ≤ δw, kφ - φk∞ ≤ δφ, kγ - Yk∞ ≤ δγ.	(17)
K
where ∣∣h∣∣∞ :二 maxχ∈κ |h(x)|. And without loss of generality, we set φ(0) = φ(0),夕(0)=夕(0)
and γ(0) = γS(0). In the prediction step, based on the evolution equations equation 9 and equation 10,
we have
ek∣k-1 = k(sk∣k-1 - SkIk-l)k = kφ(sk-1∣k-l) - φ(sk-1∣k-l)k
≤ kφ(sk-1∣k-l) - φ(sk-1∣k-l)k + ∣∣ φ(sk-1∣ k-1) - 3(Sk-1∣ k-1) k
≤ kφ(sk-1∣k-l) - φ(sk-1∣k-l)k + k(φ(sk-1∣k-l) - φ(sk-1∣k-I)) Igk-1∣k-1∈K k	(18)
+ k(φ(sk-1∣k-l) - φ(sk-1∣k-I)) Igk-1∣k-1∈K k
≤ Cφek-1∣k-1 + δφ + (Cφ + Cφ)δ,
3Here, X < Y (X 4 Y resp.) if and only if X - Y (Y - X resp.) is positive semi-definite, where X and
Y are symmetric matrices.
6
Under review as a conference paper at ICLR 2020
where the last inequality follows from equation 14 and equation 17 and C而 is the LiPschitz constant
0
of φ. We let δφ := δφ + (Cφ + Cφ)δ.
Similarly, in the updating step, we have ek∣k ≤ Cφek∖k-ι + δφ ,where δφ := δφ + (CW + CW)δ.
Combining this and equation 18, we obtain
ek|k ≤ CWek|k-1 + δW ≤ (CWCφ) ek-1|k-1 + CWδφ + δW
Using equation 19 repeatedly, it follows that 4
ek∣k ≤ (CWCφ)k e0∣0 + (CWδφ + δw) (CW Cφ - 1
Thus limsupe^ ≤ (CW (δφ + (Cφ + Cφ)δ) + δφ + (CW + C°)δ
k→+∞
(19)
(20)
k→∞
1
1 — Cw Cφ
once the condition |。山。@| < 1 holds. We choose small enough δφ, δw and δ such that
(CW (δφ + (Cφ + Cφ)δ) + δw + (CW + C°)δ) ɪC C < 邑 Then We get 15. Now we prove
16.	W φ
∣∣xk∣k - E[xkIYk]∣∣ = Ib(GkIk) - Y(SkIk)∣∣ ≤ IIY(SkIk) - Y(MkIk)∣∣ + IIY(MkIk) - Y(MkIk)∣∣
≤ IIY(SkIk) - Y(MkIk)∣∣ + ∣∣(τ(3kIk) - Y(MkIk)) IEk-1∣k-1∈K∣∣ + II(Y(MkIk) - Y(MkIk)) Igk-1∣k-1∈K∣∣
≤CγekIk + δγ	+ (CY + Cg)δ	≤	CY	(CWCφ)k e0I0	+ CY	(Cwδφ	+ δw)	W	Γ^--i---+ δγ	+ (CY + CY)δ
CW Cφ - 1
(21)
where the inequality ?1 follows equation 14 and equation 17, the inequality ?2 follows equa-
tion 59, and Cg is the Lipschitz constant of Y. Thus limsup ∣∣XkIk - E[xk|Yk]∣∣ ≤ CY(Cqδφ +
k→+∞
δW0 )(-CWCφ + 1)-1 + δY + (CY + CYg)δ. Again we can choose small enough δφ, δW, δY and δ such
that CY(Cwδφ + δw)(-CwCφ + 1)-1 + δ) + (CY + CY)δ < e. Then we obtain the desired 16.	□
An example satisfying all the assumptions of Thm. 3 can be found in our Appendix A.5, and a general
class of systems satisfying assumption ∣CwCφ∣ < 1 can be found in Appendix A.6. We also remark
that in our proof, we implicitly require that the Lipschitz constants of φ,夕，Y are uniformly upper
bounded. (See more details in our Appendix A.8) Thm. 3 highlights that the optimal filter in linear
system with Gaussian noise, Kalman Filter, can be synthesized by RNN. And RNN based filter’s
asymptotic error can be as small as wanted under some Lipschitz conditions. That is to say, RNN is
an approximator of Kalman filter.
4.2 RNN based filter is a universal approximator of optimal finite dimensional
FILTER
Thm. 3 shows that Kalman Filter (KF) can be synthesized by RNN. In this section, we’ll try to answer
the question "How general is the RNN based filter?" and extend the result into a more general case.
We’ll show that any optimal finite dimensional filter can be universally approximated by RNN under
some mild conditions. For a general system with noisy observation as shown in equation 1, once the
conditional distribution p(xk |Yk) is obtained, the filtering problem is solved. However, we usually
need to solve an infinite number of ordinary differential equations (ODE) in order to solve p(xk |Yk).
If the distribution p(xk |Yk) admits a finite dimensional sufficient statistics, then we only need to
solve a finite number of ODE (Chen (2003)) and we call such filter finite dimensional filter. Finite
dimensional filter has been an active research area after the seminal work (Benes (1981; 1985)) of
Benes. It's is a large class of filters. Some nontrivial finite dimensional nonlinear filter examples can
be found in Daum (1986); Ferrante & Runggaldier (1990); GUnther (1981); Levine & Marino (1986).
Similarly, we use vector SkIk to denote the finite dimensional sufficient statistics of the posterior
distribution p(xk|Yk) and SkIk-1 to denote the finite dimensional sufficient statistics of the prior
4See more details in our Appendix A.7.
7
Under review as a conference paper at ICLR 2020
distribution p(xk|Yk-1). The evolution functions of the statistics are denoted as Φ and Ψ, and the
map from Sk|k to conditional mean E(xk|Yk) is denoted as Γ, i.e.,
Sk∣k-1 = φ(Sk-IIk-I), SkIk = ψ(sk∣k-1,yk), E(Xk IYk) = γ(SkIk) .	(22)
Similarly, in our proposed neural networks, we use DNN generated function Φ (prediction network)
to approximate Φ, use another DNN generated Ψ (update network) to approximate Ψ, and use the
third DNN generated function Γ to approximate Γ. And the numerical statistics computed by RNN
are denoted as Sk∣k and Sk∣k-ι, i.e.,
SkIk-I = φ(Sk-IIk-I), Sk|k — ψ(Sk ∣k-1, yk ), xk|k — γ(SkIk) .	(23)
We also need the following assumption.
Assumption 3. We assume that for any given > 0 there exists a compact subset K ⊂ Rdim(Sk|k)
such that the statistics computed by the optiaml finite dimensional filter SkIk-1 and SkIk satisfy
IISkIk-1 ISk∣k-ι∈K Il < G and IISkIklSk∣k∈K || < 匕
We can see equation 7 is the special case of system equation 1, and it satisfies the Assumption 3
according to Lemma 2. We further have Lem. 3 and Thm. 4.
Lemma 3. In the discrete system equation 1, for any given > 0 there exists a compact subset
K ⊂ Rdim(SkIk) such that the statistics Sk∖k-ι, Sk∖k computed by RNN based filter with non-constant
and bounded activation function satisfy ∣∣*SkIk-ι l‹jfclfc ι∈κ∣∣ < e and ∣∣SkIk!g 划 ^∈κ∣∣ < J where
IA is indicatorfunction.
Proof. The proof is similar to the step 2 of the proof of Lem. 2.	□
Theorem 4. Consider a discrete filtering system equation 1 with optimal finite dimensional filter and
suppose SkIk, k ≥ 0 are the theoretical statistics evolving according to equation 22 and SkIk, k ≥ 0
are the statistics generated by our RNN based filter and evolving according to equation 23. Suppose
the Assumption 3 is satisfied. Furthermore, we need to assume that functions Φ and Ψ are Lipschitz,
i.e., for any S1, S2,
kΨ(S1, y) - Ψ(S2, y)k ≤ CΨkS1 - S2k,
kΦ(S1) -Φ(S2)k ≤ CΦkS1 -S2k,	(24)
kΓ(S1) -Γ(S2)k ≤ CΓkS1 -S2k
where Cψ and Cφ are Lipschitz constants. If Cψ and Cφ satisfy ∣CψCφ ∣ < 1, thenfor any e > 0,
there exists a RNN based filter with non-constant and bounded activation function such that
limsup ∣∣SkIk - SkIk∣∣ < e.	(25)
k→∞
Furthermore, we have
limsup ∣∣XkIk - E[xkIYk]∣∣ < e.	(26)
k→∞
Proof. The procedure is similar to the proof of Theorem 3.	□
Thm. 4 highlights that RNN based filter can not only approximate Kalman filter, but any optimal
finite dimensional filter under some Lipschitz conditions. Therefore, RNN’s expressive power is
characterized as the universal filtering property.
5 Conclusion
In our paper, we try to characterize the expressive power of RNN from the filtering perspective.
We unify the recurrent neural network into Bayesian filtering framework and show that recurrent
neural network is a universal approximator of optimal finite dimensional filters under some Lipschitz
conditions. As an interesting special case, the widely used Kalman filter can be synthesized by
RNN. Understanding the expressive power of RNN based filter in more general nonlinear filtering
cases (with no finite dimensional sufficient statistics) can be a very interesting future direction.
8
Under review as a conference paper at ICLR 2020
References
Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. arXiv
preprint arXiv:1811.05381, 2018.
V E. Benes. Exact finite-dimensional filters for certain diffusions with nonlinear drift. Stochastics-an
International Journal OfProbability & Stochastic Processes, 5(1-2):65-92, 1981.
V. E. Benes. New exact nonlinear filters with large lie algebras. Systems & Control Letters, 5(4):
217-221, 1985.
Zhe Chen. Bayesian filtering: from Kalman filters to particle filters, and beyond. Statistics: A Journal
of Theoretical and Applied Statistics, 182(1):1-69, 2003.
F. Daum. Solution of the zakai equation by separation of variables. IEEE Transactions on Automatic
Control, 32(10):941-943, 1987.
Frederick Daum. Exact finite-dimensional nonlinear filters. IEEE Transactions on Automatic Control,
31(7):616-622, 1986.
Yong Du, Wei Wang, and Liang Wang. Hierarchical recurrent neural network for skeleton based action
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 1110-1118, 2015.
Juntao Fei and Cheng Lu. Adaptive sliding mode control of dynamic systems using double loop
recurrent neural network structure. IEEE Transactions on Neural Networks and Learning Systems,
29(4):1275-1286, 2017.
Marco Ferrante and Wolfgang J Runggaldier. On necessary conditions for the existence of finite-
dimensional filters in discrete time. Systems & control letters, 14(1):63-69, 1990.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing,
pp. 6645-6649. IEEE, 2013.
Sawitzki GUnther. Finite dimensional filter systems in discrete time. Stochastics: An International
Journal of Probability and Stochastic Processes, 5(1-2):107-114, 1981.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Andrew H Jazwinski. Stochastic processes and filtering theory. Academic Press, New York and
London, 1970.
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. Journal of basic
Engineering, 82(1):35-45, 1960.
J Levine and R Marino. Nonlinear system immersion, observers and finite-dimensional filters.
Systems & Control Letters, 7(2):133-142, 1986.
Bryan Lim, Stefan Zohren, and Stephen Roberts. Recurrent neural filters: Learning independent
bayesian filtering steps for time series prediction. arXiv preprint arXiv:1901.08096, 2019.
J Ting-Ho Lo. Synthetic approach to optimal filtering. IEEE Transactions on Neural Networks, 5(5):
803-811, 1994.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in Neural Information Processing Systems,
pp. 6231-6239, 2017.
Xue Luo and Stephen S-T Yau. Hermite spectral method to 1-d forward kolmogorov equation and
its application to nonlinear filtering problems. IEEE Transactions on Automatic Control, 58(10):
2495-2507, 2013.
9
Under review as a conference paper at ICLR 2020
Xiaolei Ma, Zhimin Tao, Yinhai Wang, Haiyang Yu, and Yunpeng Wang. Long short-term memory
neural network for traffic speed prediction using remote microwave sensor data. Transportation
Research Part C: Emerging Technologies, 54:187-197, 2015.
Tomds Mikolov, Martin Karafidt, Lukds BUrgeL Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Eleventh annual conference of the international speech
communication association, 2010.
Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks.
Neural computation, 3(2):246-257, 1991.
Alexander G Parlos, Sunil K Menon, and A Atiya. An algorithmic approach to adaptive state filtering
using recurrent neural networks. IEEE Transactions on Neural Networks, 12(6):1411-1432, 2001.
J Samuels. On the mean square stability of random linear systems. IRE Transactions on Circuit
Theory, 6(5):248-259, 1959.
Anton Maximilian Schafer and Hans Georg Zimmermann. Recurrent neural networks are universal
approximators. In International Conference on Artificial Neural Networks, pp. 632-640. Springer,
2006.
Robert Wilson and Leif Finkel. A neural implementation of the kalman filter. In Advances in neural
information processing systems, pp. 2062-2070, 2009.
A Appendix
A.1 Proof of Theorem 1
Proof. It is known that
E[Lmse] =E[(xk - Xk)T(Xk - Xk)]
=E[E[(xk - Xk )t (xk - Xk )∖Yk ]],
and let
μk = E[Xk M ].
Since
E [(Xk - μk ) (μk - Xk ) ∖ Yk]
= (Xk - μk) (μk - μk) = 0,
and
E [(Xk - μk )T (Xk - μk)]
is independent of Xk, equation 27 becomes
E[LMSE ] =E [E [ (Xk - Xk )T (Xk - Xk )∖ Yk ]]
=E [E [ (Xk - μk + μk - Xk )T (Xk - μk + μk - Xk ) ∖ Yk]]
=E [E [ (Xk - μk )T (Xk - μk ) + (μk - Xk )T (μk - Xk )
+2(Xk - μk) (μk - Xk) ∖ Yk]]
=E [(Xk - μk )T (Xk - μk )] + E [(μk - Xk)T (μk - Xk)]
=const + E [(μk - Xk)T(μk - Xk)]
(27)
(28)
(29)
(30)
Therefore Ε[Lmsε] is clearly minimized by setting Xk = μk = E[Xk |Yk].
□
10
Under review as a conference paper at ICLR 2020
A.2 The evolution of distribution in updating step of Bayesian filter
Letp(xk|Yk) denote the conditional distribution of state xk conditioned on the observation history
Yk, then using Bayes’ rule, we have:
p(xk|Yk)
Bayes'rule P(Yk ∣Xk )p(xk)
P(Yk )
p(yk,Yk-i∣Xk )p(xk)
p(yk ,Yk-ι)
=p(yk ∣Yk-ι,χk )p(Yk-ι∣χk )p(χk)
p(yk∣Yk-i)p(Yk-i)
Bayes'rule p(yk∣Yk-1,Xk )p(xk ∣Yk-l)p(Yk-l )p(xk )
p(yk ∣Yk-l)p(Yk-l)p(xk)
_ p(yk|xk)p(χk∣Yk-ι)
p(yk ∣Yk-ι)
=	p(yk ∣χk )p(χk IYk-ι)
P p(yk ∣χk )p(χk ∣Yk-ι)dχk.
(31)
It can be seen that the posterior distribution P(xk|Yk) is determined by the prior distribution
P(xk|Yk-1) and likelihood function P(yk|xk), which depends on the observation model and the
known distribution of vk.
A.3 The evolution function of statistics for Kalman Filter
The widely used Kalman Filter assumes that the posterior distribution at every instant is Gaussian,
and then it can be parameterized by the mean and covariance. We consider the following system:
xk = F xk-1 + Gwk-1
yk = Hxk + vk,
(32)
where F, G, H are constant matrices with proper dimensions, the initial state x0 is Gaussian, and
{wk, k = 0,1,…} and {vk, k = 1,…} are two independent white Gaussian sequences that are
also independent of the initial state x0 jointly.
Then KF can be viewed as the following recursive relationship:
P(xk-1|Yk-1) = N (xk-1; mk-1|k-1, Pk-1|k-1)
P(xk|Yk-1) = N (xk; mk|k-1, Pk|k-1)	(33)
P(xk|Yk) = N(xk; mk|k, Pk|k),
where in the prediction step,
mk|k-1 = F mk-1|k-1
Pk|k-1 = GQGT + F Pk-1|k-1F T,
(34)
in the updating step,
mk|k = mk|k-1 + Pk|k-1HT (HPk|k-1HT + R)-1
• (yk - HmkIk-I)
Pk|k = Pk|k-1 -Pk|k-1HT(HPk|k-1HT + R)-1
• HPk|k-1,
and N(x; m, P) is a Gaussian distribution with mean m, and covariance P. The evolution functions
of mean and covariance in the prediction step and updating step can be expressed in the following
compact form:
m mk∣k-1 = φl(mk-1∣k-1,Pk-1∣k-l)
I Pk∣k-1 = φ2(mk-1∣k-1,pk-1∣k-1),
(36)
11
Under review as a conference paper at ICLR 2020
(mk|k = φι(mk∣k-i,Pk∣k-i,yk)
I PkIk = ψ2 (mk∣k-1, Pk∣k-1, yk ),
where φ1,φ2,夕 1 and 夕2 are defined as
(φ1(m, P) := Fm
φ2 (m, P) :=GQGT+FPFT,
and
ʃ中 1(m, P, y) := m + PHT(HPHT + R)-1(y - Hm)
∖中2(m,P,y) := P - PHT(HPHT + R)-1 HP,
Here,
mk|k-1 = E[xk|Yk-1], mk|k = E[xk|Yk]
are the conditional means of state conditioned on observation history.
Pk|k-1 = E (xk - mk|k-1)(xk - mk|k-1)T |Yk-1 ,
Pk|k = E (xk - mk|k)(xk - mk|k)T |Yk
(37)
(39)
(40)
(41)
are the corresponding conditional covariance. Then we know that the posterior distribution p(xk |Yk)
is parameterized by the conditional mean mk|k and covariance Pk|k which are functions of Yk, i.e.,
p(xk|Yk-1) is parameterized by {mk|k, Pk|k}. And the similar conclusion can be obtained for prior
distribution p(xk|Yk-1).
Now we vectorize {mk|k, Pk|k} as the statistics
sk|k := [mk|k , vec (Pk|k )] .
Vectorizing the second equation in equation 36 and equation 37, we have
φι(m,P,y)	= = Γ	中ι(m, VeCT(VeC(P)),y)
_ vec(φ2(m,P,y)) \ 一 [ vec(^2(m, vec-1(vec(P)),y))
,以m, vec(P), y) = φ(s, y),
and
VeCφ(1φ(2m(m, P, P) ))	, φ(m, VeC(P)) = φ(s),
(42)
(43)
(44)
where we omit subsCript CharaCters without Causing Confusion, and VeCn-1×n is the inVerse operator
of vec such that vec-1×n2 (vecg% ×n? )) = % ×n2. Then we obtain the evolution functions of the
statistiCs:
sk∣k =4(SkIk-ι,yk), sk+ι∣k = φ(SkIk),	(45)
where 夕 and φ are defined in equation 43 and equation 44.
A.4 Proof to Lem. 2
Proof. Step 1: It can be known from the KF that the evolution function for conditional covariance is
as follows (see appendix):
(Pk∣k-1 = GQGT + FPk-1∣k-1FT
<	Pk∣k = Pk∣k-ι — Pk∣k-ι H T (HPkIk-IH T + R)T	(46)
I	∙ HPk∣k-ι,
and the initial value is P0I0 . It follows that the conditional covariance P evolves in a deterministic
manner and without any randomness according to the evolution equation equation 46, therefore P is
independent of the observations. Since
PkIk = E [ (xk - mk|k) (xk - mk|k)T∣ 居],
12
Under review as a conference paper at ICLR 2020
and Pk|k is independent of Yk, we have
Pk|k = E [由一mk∣k) (xk - mk∣k)T] ∙	(47)
According to Lemma 1, we have
Pk∣k 4 (1+αβ) I，k ≥ N,	(48)
where N is a positive integer andα,β are positive constants. It can be easily checked that, there
exists a positive constant 0 , such that
Pk|k 4α0I, ∀k	≥ 0.
(49)
According to equation 47, we have
kxk - mk|kk2
=E [(xk - mk∣k)T (Xk - mk|k	(50)
=tr (PkIk),
where tr(?) denotes the trace of matrix ?. Combining equation 49 and equation 50, we know that
∣∣Xk - mk∣kk ≤ √nα0, ∀ k ≥ 0.	(51)
Then according to equation 12 and equation 51, we have
kmk|kk ≤ kxk - mk|kk+	kxkk
≤ √nαo + M
≤ C0, ∀ k ≥ 0,
(52)
where Co is a positive constant and can be chosen to be √n00 + M.
Now we have the conclusion that mk|k and Pk|k are bounded for all k ≥ 0. Similarly, we know
that mk|k-1 and Pk|k-1 are all bounded following the similar procedure as above. According to
equation 8, we know that all sufficient statistics sk|k-1, sk|k are bounded.
Step2: We then prove that mk∣k-ι 1舐依_12长 ∣∣ < E and 心碎1鼠后长 ∣∣ < e. Recall that
sk∣k = 0(MkIk-ι,yk), sk+ι∣k = φ(MkIk),	(53)
where 0M and φ are approximating functions generated by neural networks with non-constant and
bounded activation functions. Let σ denote the activation functions used by the neural network and
assume that ∣σ(x)∣ < Cσ, ∀χ ∈ R. And without loss of generality, we assume 0(Sk∣k-ι,yk)=
V1σ(W1p1(sMkIk-1, yk)+ b1)+ c1, where p1 is the vector valued function that takes input to the last
hidden layer’s output, V1 and W1 are matrices with suitable dimension, b1 is a vector with suitable
dimension. Then we’ll have,
∣∣sMkIk ∣∣
= ∣∣0M(sMkIk-1, yk)∣∣
≤ ∣∣V1 σ (W1 p1 (sMkIk-1, yk)+	b1)+ c1 ∣∣	(54)
≤kV1k ∣∣σ(WIpI(MkIk-1, yk) + b1)∣∣ + ∣∣c1k
≤ ∣V1k pdim(bι)Cσ + ∣∣cιk ,
where dim(x) is the dimension of the vector ofx. We choose the compact set K to cover the compact
ball {χ ∈ Rdim(SkIk) : kXkE ≤ ∣∣V1k 'dim(bι)Cσ + ∣∣cι∣∣} where ||?||石 denotes Euclidean norm
of ?. ThenitCanbeSeenthat ∣∣ Mk∣k-ιIEkIk-ι∈κ ∣∣ < J And ∣∣Mk∣klgk∣k∈κ ∣∣ < E can be shown in a
similar way.	□
13
Under review as a conference paper at ICLR 2020
A.5 An example that satisfies all the assumptions of Thm. 3
Here we verify that the system shown in equation 55 with state’s dimension 1 satisfies all assumptions
in Theorem 3.
[xk = (1 - α)xk-i + √αwk-i
Iyk = αxk + √αvk,
(55)
where 0 < α < 1 is a small positive parameter.
•	As for Assumption 1, from the state equation of equation 55 we have
Xk =(1 - α)xk-i + √awk-i
=(1 - α)2xk-2 + (1 - α)√ɑwk-2 + √awk-i
(56)
=(1 — α)kxo + ^X(1 — a)k-1--,√awi,
i=0
then we have
E[∣Xk |2] =(1 - α)2kE[∣X012] + X(1 - α)2k-2-2iαQ
i=0
=(1 - α)2kE[∣xo|2] +
(1 二 α)2k-2 二(1 二 α)2
(1 — α)2 — 1
αQ.
(57)
It can be easily checked that Assumption 1 is satisfied.
•	As for Assumption 2, it can be easily checked that system shown in equation 55 satifies
Assumption 2 using the definitions of uniformly completely observable and uniformly
completely controllable in section 7.5 of Jazwinski (1970).
A.6 A CLASS OF SYSTEM THAT SATISFIES ∣CφCφ| < 1 IN THEOREM 3
Actually this condition ∣CφCφ∣ < 1 can be satisfied by a general class of systems. Let Us consider
the 1-dimensional linear systems with discretization in continuous observation equation, i.e.,
xtk = axtk-1 +gwtk-1, ytk = b∆txtk + vt∆k,	(58)
where a, g, b are constants, ∆t = tk - tk-ι << 1 is the step size of sampling, {wk, k = 1, ∙一} is
an 1-dimensional white Gaussian process and Wk 〜N(0, Q), and {v∆ } is 1-dimensional white
Gaussian process with distribution N(0, R/∆t). According to Jazwinski (1970), we know that the
discrete observation system in equation 58 is the sampled system of continuous observations.
Following equation (34)-(35) in Appendix A.2, we have
φ1(m,P) = am,φ2(m,P) = gQg+ a2P,
and
夕ι(m, P) = m + K(y — b∆tm), φ2(m, P) = P — Kb∆tP,
where K = P b(∆t)2 ((b∆t)2 P ∆t + R)-1. It can be easily checked that
Vφ
0
a2
1 - Kb∆t
0
(y - δ1M d∂P
1 - KbM-Wp ∂∂Pj ,
a
0
,▽夕
where ∂∂P = (b∆t)2R((b∆t)2P∆t + R)-2. Then we have
Cφ = max{a, a2},C. = EIlVqk2 ≤ EkVHlF = 1 + O(∆t),
where ∣∙ ∣∣f is the Frobenius norm.
If a < 1, which is a very natural stable condition, then
[im ∣CφCφ∣ ≤ ]im |(a + O(∆t)) = a < 1.
Apparently, we have ∣CφCφ∣ < 1 with small step size ∆t.
14
Under review as a conference paper at ICLR 2020
A.7 Calculation details of inequality 20
≤ (CφCφ) (CWG
=(C中Cφ) ek-2∣l
+ δW0
CWδφ0 +δW0
k-2 + [CWCφ + 1] CWδφ
)) + CW δφ + δW
+ δW0 )
(59)
k
≤ (CφCφ)k eo∣0 + (CWδφ + δj X (CφCφ)k
i=0
=(CWCφ) eo∣0 + (°wδφ + WW) (CG)-;
A.8 Approximate φ and 夕 using LIPSCHITZ neural network
ɪ ,1	Γ∙ Γ∙ EI	C	∙	1 ∙ ∙ , 1	∙ ,1 ɪ ∙	1 ∙ ,	,	Γ∙ ~l	F~	∙ l'	1
In the proof of Thm. 3, We implicitly require the LiPschitz constants of φ and 夕 are uniformly
bounded by another constant C. Note that this assumption won’t affect the approximation
capability of Deep Neural NetWork in approximating Lipschitz continuous function φ and
夕(Thm. 3 in Anil et al. (2018)). With this requirement, the internal state variables of our
proposed RNN based filter is alWays inside a compact ball independent of the choice of neural
network. And we will have	(CW	(δφ + (Cφ +	Cφ)δ) +	δφ	+	(CW	+	Cw)δ)	] - C C <
(CW (δφ + (Cφ + C)δ) + 6w + (CW + C)δ)-——1	, thus we can choose small enough
1-CW Cφ
δ, δφ	and Sy to make (CW (δφ + (Cφ + Cφ)δ)	+	通 + & +。0)6)I - C CO	<
(CW (δφ	+ (Cφ + C)δ) + Sw + (CW + C)δ) --1— <	e.
1-CW Cφ
A.9 Experiments
A.9.1 Experiment set up
We use pyTorch as the deep learning framework to implement our algorithm and run our simulation
on Cpu clusters with 5 E5-2630/v3/2.40GHz Cpus, each is equipped with 30GB memory. We use
the Mean Squared Error(MSE) as the metric to measure the performance of the filter. For subsequent
discussion, we make the following definition.
Definition 3 (pMSE). For a set of sampled paths {ωι, ω2,…,3n}, we define the Path Mean
Squared Error at time slot k PMSEk to be N PN=IIlxk M) — Xk (ω.)k2.
Further we define the Temporal path Mean Squared Error TPMSE.
Definition 4 (TPMSE). For a Set ofsampledpaths {ωι, ω2,…,ωN} and a horizon ofsimulation
1 : M, we define the Temporal Path Mean Squared Error (TPMSE) to be 吉 PM=I PMSEk.
A.9.2 Training
Alg.1 shows how we train the RNN based filter.
A.9.3 Kalman filter can be synthesized by RNN based filter efficiently
We consider a stable linear filtering system as in equation 60.
Jxk = (αAn + In)xk-1 + √awk-i
∖y= = αxk + √αvk,
(60)
15
Under review as a conference paper at ICLR 2020
Algorithm 1 Bayesian Filter Net (BFN) training algorithm
1:	Setting the sampling path number N , the number of simulated steps M, the optimization horizon
length l and the learning rate λ
2:	Sampling x0(ωi) i.i.d from the initial distribution σ0(x) and set y0(ωi) = 0
3:	for 0 ≤ m ≤ M 一 1 do
l
4:	Initialize loss(θ) to be 0
5:	for ml ≤ k < (m + 1)l do
6:	for 1 ≤ i ≤ N do
7：	xk+ι(ωi) J f(xk(ωi)) + g(xk(ωi))wk-i(ωi)
8：	yk+ι(ωi) J h(xk+ι(ωi)) + Vk(ωj
9:	end for
10:	for 1 ≤ i ≤ N do
11:	Feed the sampled yk+1(ωi) to the Bayesian-Filter-Net(BFN) and get the corresponding
output xk+ι(ωi)
12： Add the estimation squared error to the loss function loss(θ) J loss(θ) +
kxk+ι(ωi) ― xk+ι(ωi)k2
13： end for
14： end for
15: Doing back propagation and update the network parameters θ J θ 一 λVθloss(θ)
16： end for
where α < 1 is a small positive parameter controlling the changing rate of the system state, In
is a n × n identity matrix and An is a matrix with elements aij satisfying aij = 1, if i = j + 1,
一 i-n1 , if j = n and 0, otherwise.
15000
10000
5000
20000
WsWdL
0	20	40	60	80	100
Training Epochs
200Q
0
10	20	30	40	50	60	70	80	90
Dimensions of hidden VariabIeS in RNN
(a)	(b)
Figure 3: (a) The decrease of TPMSE of RNN based filter with different hidden variable dimensions
in training. (b) Convergent TPMSE of RNN based filter with different dimensions of hidden variables.
From Fig. 3a, we can see that as the number of training epochs increase, RNN-based filter’s MSE
decrease and finally converge. And from Fig. 3b as the dimension of the hidden variable increases,
the converged MSE decreases. The results in Fig. 3 highlight that just with the number of 90 hidden
variables, RNN based filter can achieve the performance of Kalman filter. Considering that there are
65 independent statistics in this case, this is a very efficient approximation.
A.9.4 RNN based filter can still work for infinite dimensional filter
For finite dimensional filter, our analysis has shown that RNN based filter is a universal approximator
of optimal filter. In this subsection, we show that in infinite dimensional filter case (where finite
dimensional statistics are not admitted to fully determine the distribution), our RNN based method
achieves even better performance to state-of-the-art spectral method Luo & Yau (2013). We consider
a special and well-known infinite dimensional case.
xk
yk
χk-ι + √αwk-ι
Qxk + √avk
(61)
16
Under review as a conference paper at ICLR 2020
where α is a positive parameter controlling the change rate of the system. We set the α to be 0.01,
the total sampling steps to be 5000, the number of sampled paths to be 1000, the number of training
epochs to be 100. We only use two fully connected layers for the prediction network, update network
and estimation network. Fig. 4a shows the convergence of PMSE of our Deep-Filter-Net with respect
to the time. And fig. 4b shows that our proposed DeeP-Filter-Net output Xk tracks Xk well. We also
(a)
Figure 4: (a) The evolution of PMSEk. (b) Xk tracks Xk well.
time
(b)
compare our algorithm’s performance with existing state-of-the-art spectral method. Our proposed
DFN’s TPMSE is 0.18, which is much smaller than spectral method’s TPMSE 0.40. One potential
reason for its RNN’s applicability in infinite dimensional filter case is that RNN implicitly learns the
most relevant finite dimensional statistics.
17