Under review as a conference paper at ICLR 2020
Smart Ternary Quantization
Anonymous authors
Paper under double-blind review
Ab stract
Neural network models are resource hungry. Low bit quantization such as binary
and ternary quantization is a common approach to alleviate this resource require-
ments. Ternary quantization provides a more flexible model and often beats binary
quantization in terms of accuracy, but doubles memory and increases computation
cost. Mixed quantization depth models, on another hand, allows a trade-off be-
tween accuracy and memory footprint. In such models, quantization depth is often
chosen manually (which is a tiring task), or is tuned using a separate optimization
routine (which requires training a quantized network multiple times). Here, we
propose Smart Ternary Quantization (STQ) in which we modify the quantiza-
tion depth directly through an adaptive regularization function, so that we train
a model only once. This method jumps between binary and ternary quantization
while training. We show its application on image classification.
1	Introduction
Deep Neural Networks (DNN) models have achieved tremendous attraction because of their success
on a wide variety of tasks including computer vision, automatic speech recognition, natural language
processing, and reinforcement learning (Goodfellow et al., 2016). More specifically, in computer
vision DNN have led to a series of breakthrough for image classification (Krizhevsky et al., 2017),
(Simonyan & Zisserman, 2014), (Szegedy et al., 2015), and object detection (Redmon et al., 2015),
(Liu et al., 2015), (Ren et al., 2015). DNN models are computationally intensive and require large
memory to store the model parameters. Computation and storage resource requirement becomes
an impediment to deploy such models in many edge devices due to lack of memory, computation
power, battery, etc. This motivated the researchers to develop compression techniques to reduce the
cost for such models.
Recently, several techniques have been introduced in the literature to solve the storage and compu-
tational limitations of edge devices. Among them, quantization methods focus on representing the
weights of a neural network in lower precision than the usual 32-bits float representation, saving on
the memory footprint of the model. Binary quantization (Courbariaux et al., 2015), (Hubara et al.,
2016), (Rastegari et al., 2016), (Zhou et al., 2016), (Lin et al., 2017) represent weights with 1 bit
precision and ternary quantization (Lin et al., 2015), (Li & Liu, 2016), (Zhu et al., 2016) with 2 bits
precision. While the latter frameworks lead to significant memory reduction compared to their full
precision counterpart, they are constrained to quantize the model with 1 bit or 2 bits, on demand.
We relax this constraint, and present Smart Ternary Quantization (STQ) that allows mixing 1 bit
and 2 bits layers while training the network. Consequently, this approach automatically quantizes
weights into binary or ternary depending upon a trainable control parameter. We show that this ap-
proach leads to mixed bit precision models that beats ternary networks both in terms of accuracy and
memory consumption. Here we only focus on quantizing layers because it is easier to implement
layer-wise quantization at inference time after training. However, this method can be adapted for
mixed precision training of sub-network, block, filter, or weight easily. To the best of our knowledge
this is the first attempt to design a single training algorithm for low-bit mixed precision training.
2	Related Work
There are two main components in DNN’s, namely, weight and activation. These two components
are usually computed in full precision, i.e. floating point 32-bits. This work focuses on quantizing
1
Under review as a conference paper at ICLR 2020
the weights of the network, i.e. generalizing BinaryConnect (BC) of Courbariaux et al. (2015) and
Ternary Weight Network (TWN) of Li & Liu (2016) towards automatic 1 or 2 bits mixed-precision
using a single training algorithm.
In BC the real value weights w are binarized to wb ∈ {-1, +1} during the forward pass. To map a
full precision weight to a binary weight, the deterministic sign function is used,
wb = sign(w)
+-11
w ≥ 0,
w < 0.
(1)
The derivative of the sign function is zero on R \ {0}. During back propagation, this cancels out
the gradient of the loss with respect to the weights after the sign function. Therefore, those weights
cannot get updated. To bypass this problem, Courbariaux et al. (2015) use a clipped straight-through
estimator
∂ L _ ∂ L
∂W = ∂Wb 1lwl≤1 (w)
(2)
where L is the loss function and 1A(.) is the indicator function on the set A. In other words (2)
approximates the sign function by the linear function f(x) = x within [-1, +1] and by a constant
elsewhere. During back propagation, the weights are updated only within [-1, +1]. The binarized
weights are updated with their corresponding full precision gradients. Rastegari et al. (2016) add a
scaling factor to reduce the gap between binary and full-precision model’s accuracy, defining Binary
Weight Network (BWN). The real value weights W in each layer are quantized as μ X {-1, +1}
where μ = E[|W|] ∈ R. Zhou et al. (2016), generalize the latter work and approximates the full
precision weights with more than one bit while Lin et al. (2017) approximate weights with a linear
combination of multiple binary weight bases.
2.1	Ternary weight networks
Ternary Weight Network (TWN) (Li & Liu, 2016) is a neural network with weights constrained to
{-1, 0, +1}. The weight resolution is reduced from 32 bits to 2 bits, replacing full precision weights
with ternary weights. TWN aims to fill the gap between full precision and binary precision weight.
Compared to binary weight networks, ternary weight networks have stronger expressive capability.
As pointed out in (Li & Liu, 2016), for a 3×3 weight filter in a convolutional neural network, there
is 23×3 = 512 possible variation with binary precision and 33×3 = 19683 with ternary precision.
Li & Liu (2016) find the closest ternary weights matrix Wt to its corresponding real value weight
matrix W using
(μ, Wt = arg min ∣∣W 一 μWt∣∣2,
α,Wt	(3)
S s.t. μ ≥ 0, Wtj ∈ {-1, 0, 1}, i,j = 1, 2,…，n.
The ternary weight Wt is achieved by applying a symmetric threshold ∆
+1	wij > ∆,
Wt =	0	|wij| ≤ ∆,
l 一1	Wij < 一∆.
(4)
Li & LiU (2016) define a weight dependant threshold ∆ and a scaling factor μ that approximately
solves (3). TWN is trained using stochastic gradient descent. Similar to BC and BWN schemes;
ternary-value weights are only used for the forward pass and back propagation, but not for the
parameter updates. At inference, the scaling factor can be folded with the input X
X Θ W ≈ X Θ (μWt) = (μX) Θ Wt,	(5)
where denotes the convolution.
Zhu et al. (2016) proposed a more general ternary method which reduces the precision of weights
in neural network to ternary values. However, they quantize the weights to asymmetric values
{一μι, 0, +μ2} using two full-precision scaling coefficients μι and μ2 for each layer of neural net-
work. While the method achieve better accuracy as opposed to TWN, its hardware implementation
becomes a challenge, because there are two unequal full precision scaling factors to deal with.
Our method provides a compromise between BC and TWN and trains weights with a single trainable
scaling factor μ. Weights jumps between ternary {-μ, 0, +μ} and binary {-μ, +μ}. This provides
a single algorithm for 1 or 2 bits mixed precision.
2
Under review as a conference paper at ICLR 2020
2.2	Regularization
Regularization term is the key to prevent over-fitting problem and to obtain robust generalization for
unseen data. Standard regularization functions, such as L2 or L1 encourage weights to be concen-
trated about the origin. However, in case of binary network with binary valued weights, it is more
appropriate to have a regularization function to encourage the weights about μ × {-1, +1}, with a
scaling factor μ > 0 such as
Rι(w,μ) = ||w| - μ∣,
proposed in Belbahri et al. (2019).
A straightforward generalization for ternary quantization is
R2 (w,μ) = ||w|-2ι — 2
(6)
(7)
Regularizer (6) encourages weights about {—μ, +μ}, and (7) about {-μ, 0, +μ}. The two functions
are depicted in Figure 1. These regularization functions are only useful when the quantization depth
is set before training start. We propose a more flexible version to smoothly move between these two
functions using a shape parameter.
(a) RI(W,μ = 1)
Figure 1: Binary and ternary regularizers; R1 encourages binary weights, with minimums at
{—μ, +μ}, and R2 encourages ternary weights, with minimums at {—μ, 0, +μ}.
(b) R2(w, μ = 1)
3
Under review as a conference paper at ICLR 2020
3	Smart Ternary Quantization
Here we propose an adaptive regularization function that switches between binary regularization of
(6) and ternary regularization of (7)
R(w,μ,β)=min(∣∣w∣- μ∣, tan(β)|w|),	(8)
in which μ is a trainable scaling factor, and β ∈ (4, 2) controls the transition between (6) and (7).
AS a special case β → π2 converges to the binary regularizer (6) and β → 4 coincide with the
ternary regularizer (7), depicted in Figure 2. A large value of tan(β) repels estimated weights away
from zero thus yielding binary quantization, and small tan(β) values encourage zero weights. The
shape parameter β controls the quantization depth. Quantization is done per layer, therefore we let
β very per layer. We recommend to regularize β about π2 i.e. preferring binary quantization apriori
R(w, μ, β) = min (∣|w| 一 μ∣, tan(β)|w|) + Y| cot(β)|,	(9)
in which γ controls the proportion of binary to ternary layers.
For a single filter W the regularization function is a sum over its elements
R(W,μ,β)	= XXmin (∣∣Wj∣-μ∣, tan(β)∣Wj∣) + Y| cot(β)|.	(10)
i=1 j=1
Large values of Y encourage binary layers. In each layer, weights are pushed to binary or ternary
values, depending on the trained value of the corresponding β . A generalization of (9) towards
Lp norms of Belbahri et al. (2019) is also possible. However, here we only focus on regularizer
constructed using the L1 norm as the accuracy did not change significantly by using Lp norm with
different values of p.
The introduced regularization function is added to the empirical loss function L(.). The objective
function defined on weights W, scaling factors μ, and quantization depths β is optimized using
back propagation
L	Kl
L(W, μ, β) = L(W) + X λι X R(Wkl.,βι),	(11)
l=1	k=1
where k indexes the channel, and l indexes the layer. One may use a different regularization constant
λl for each layer to keep the impact of the regularization term balanced across layers, indeed different
layers may involve different number of parameters. We set λl = #W^ where λ is a constant, and
#Wl is the number of weights in layer l.
Figure 2: Adaptive regularization function. When β → 2 the regularization function switches from
ternary to binary.
We propose to use the same threshold-based function of Li & Liu (2016) (3), but with a fixed
threshold ∆l per layer l. Note that Li & Liu (2016) propose a weight-dependant threshold. We let
the possibility for the weights to only accumulate about {-μ, +μ} and not about 0, depending on β.
4
Under review as a conference paper at ICLR 2020
One may set ∆ι to have the same balanced weights in {—μ, 0, +μ} at initialization for all layers and
let the weights evolve during training. Formally, ifσl is the standard deviation of the initial Gaussian
weights in layer l, we propose ∆l = 0.2 × σl. The probability that a single weight lies in the range
[—∆ι, ∆ι] is ≈ 0.16. All the weights falling in this range will be quantized as zeros after applying
the threshold function.
Weights are naturally pushed to binary or ternary values depending on βl during training. Eventually,
a threshold δ close to 2 ≈ 1.57 defines the final quantization depth for each layer.
Final quantization depth of layer l :
Binary
Ternary
βl ≥ δ,
βl < δ
4	Experiment
We run experiments on two simple image classification tasks MNIST (LeCun et al., 1998) and
CIFAR10 (Krizhevsky & Hinton, 2009) datasets. We compare our method (STQ) with BinaryCon-
nect (BC) of Courbariaux et al. (2015), Binary Weight Networks (BWN) of Rastegari et al. (2016),
Ternary Weights Network (TWN) of Li & Liu (2016) and also with a Full Precision network (FP).
The quality of the compression is measured only in terms of memory, it is difficult to compare mixed
precision models, with binary and ternary, in terms of consumed energy. Assume that nl is the quan-
tization depth for the layer l and #Wl is the number of weights in layer l, therefore the compression
ratio is p' #Wl：：2. The compression ratio for a binary network is 32, for a ternary network 16,
and our app=roach falls in between.
STQ network generalizes binary and ternary regularization in a single regularization function. First
we show how to control the proportion of binary and ternary layers using γ in (8). Figure 3 clarifies
the effect of Y on the weight distribution. When Y is large, β is encouraged towards 2 which corre-
sponds to binary quantization. Consequently, weights are pushed about {—μ, +μ} and 0 is removed
from the trained values, see Figure 3a. On the contrary, when Y is small, β tends to ∏ and the weights
started including 0 in their values, see Figure 3b.
(a) γ = 1 × 10-1, β = 1.57
(b) γ = 1 × 10-5, β = 0.79
Figure 3: Effect of Y on the weights distribution ofa layer while training.
4.1	MNIST
MNIST is an image classification benchmark dataset with 28 × 28 gray-scales images representing
digits ranging from 0 to 9. The dataset is split into 60k training images and 10k testing images. We
used the LeNet-5 (LeCun et al., 1998) architecture consisting of 5 layers, 2 convolution followed
by maxpooling, stacked with two fully connected layers and a softmax layer at the end. We train
the network for 60 epochs using Adam optimizer. We used the initial learning rate of 0.01, but
divided by 10 in epoch 15 and 30 to stabilize training. The batch size is set to 64 with L2 weight
decay constant 10-4 only for BC, BWC and TWN. The full precision LeNet-5 is trained with no
regularization as it provided a superior accuracy. STQ is trained with λ = 0.1 and Y = 1 × 10-2 and
the effective regularization constant is divided by the number of weights in each layer to compensate
for the layer size. The best validation accuracy for each method is reported in Table 1, as well as
5
Under review as a conference paper at ICLR 2020
the quantization depth, and the overall compression ratio. We observe that STQ network quantized
the first two convolutional layers in 1 bit, and the last fully-connected layers in 2 bits. The accuracy
improvement and the compression ratio is marginal for simple task and simple architectures. The
effect of smart training becomes more visible for more complex tasks with more layers.
Table 1: Smart ternary quantization (STQ) compared with Binary Connect (BC), Binary Weight
Network (BWN), Terneary Weight Network (TWN), and Full Precision (FP) on MNIST dataset.
	Quantization depth per layer (-bits)	Compression ratio	Accuracy (top-1)
BC	1-1-1-1-1	32	99.35
BWN	1-1-1-1-1	32	99.32
TWN	2-2-2-2-2	16	99.38
STQ	1-1-2-2-2	16.3	99.37
FP		1	99.44
4.2	CIFAR 1 0
CIFAR10 is an image classification benchmark that contains 32 × 32 RGB images from ten classes.
The dataset is split into 50k training images and 10k testing images. All images are normalized using
mean = (0.4914, 0.4822, 0.4465) and std = (0.247, 0.243, 0.261). For the training session, we pad
the sides of the images with 4 pixels, then a 32 × 32 crop is sampled, and flipped horizontally at
random.
We use two VGG-like architectures, i) VGG-7 architecture defined in Li & Liu (2016) in which
we apply batch normalization after each layer and use ReLU activations, ii) a standard VGG-16
architecture. Deep architectures are sensitive to early layer quantization. As commonly practiced,
we did not quantize the first and the last layers in VGG-16 for all competing methods.
We train the network for 150 epochs, using Adam optimizer with the initial learning rate 0.001
divided by 10 at epochs 40 and 80. The batch size is set to 64 with L2 weight decay constant 10-4,
moreover λ = 0.1, γ = 10-3 for STQ. The best validation accuracy for each method is reported
in Table 2. STQ beats pure 2 bits network TWN, even in terms of accuracy. It recommends three
1 bit layers for VGG-7 and seven 1 bit layers for VGG-16. The compression ratio is significantly
higher than a ternary network. The weight distribution of each layers are depicted in Figure 4 for
the VGG-7 architecture. Weights are pushed to {-μ, +μ} or {-μ, 0, +μ} depending on the shape
parameter β .
Table 2: Smart ternary quantization (STQ) compared with Binary Connect (BC), Binary Weight
Network (BWN), Terneary Weight Network (TWN), and Full Precision (FP) on CIFAR10 dataset.
Architecture	Method	Quantization depth per layer (-bits)	Compression ratio	Accuracy (top-1)
	BC	1-1-1-1-1-1-1	32	92.49
	BWN	1-1-1-1-1-1-1	32	92.42
VGG-7	TWN	2-2-2-2-2-2-2	16	92.74
	STQ	2-1-1-1-2-2-2	18.3	92.94
	FP		1	93.72
	BC	32-1-1-1-1-1-1-1-1-1-1-1-1-32	31.5	91.92
	BWN	32-1-1-1-1-1-1-1-1-1-1-1-1-32	31.5	91.85
VGG-16	TWN	32-2-2-2-2-2-2-2-2-2-2-2-2-32	15.9	92.14
	STQ	32-2-1-1-2-2-2-1-1-1-1-1-2-32	25.1	92.38
	FP		1	92.53
6
Under review as a conference paper at ICLR 2020
(a) Layer	1, β1	= (b) Layer	2, β2	= (c) Layer	3, β3	= (d) Layer	4, β4	=
1.56	1.57	1.57	1.57
(e) Layer 5, β5 = 1.00	(f) Layer 6, β6 = 0.86	(g) Layer 7, β7 = 0.98
Figure 4: Layer wise weights distribution in VGG-7 for STQ. The weights are pushed to binary
when the shape parameter β is close to ∏ ≈ 1.57.
5	Conclusion
Smart ternary quantization (STQ) is a training method to build a 1 and 2 bits mixed quantized
layers. Depth optimization requires training network multiple times which is costly, specially if
the network is complex. This approach successfully combines quantization with different depths,
while training the network only once. We tried layer-wise quantization, because it is more suitable
for mixed-precision inference implementation. However, subnetwork, block, filter, or weight mixed
quantization is feasible using a similar algorithm.
STQ makes manual tuning of quantization depth unnecessary. It allows to improve the memory
consumption, by automatically quantizing some layers with smaller precision. This method some-
times even outperforms pure ternary networks in terms of accuracy thank to a formal regularization
function that shapes trained weights towards mixed-precision.
It is well-known that some layers are more resilient to aggressive quantization. Our introduced
methodology trains network similar to pure ternary but give an insight about which layers can be
simplified further by going to binary quantization depth.
References
Mouloud Belbahri, Eyyub Sari, Sajad Darabi, and Vahid PartoVi Nia. Foothill: A quasiconvex regu-
larization for edge computing of deep neural networks. In Image Analysis and Recognition - 16th
International Conference (ICIAR), pp. 3-14, 2019.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems (NIPS), pp. 3123-3131, 2015.
Ian J. Goodfellow, Yoshua Bengio, and Aaron C. Courville. Deep Learning. Adaptive computation
and machine learning. MIT Press, 2016.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks. In Advances in neural information processing systems (NIPS), pp. 4107-4115,
2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. Commun. ACM, 60(6):84-90, 2017.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998.
7
Under review as a conference paper at ICLR 2020
Fengfu Li and Bin Liu. Ternary weight networks. CoRR, abs/1605.04711, 2016.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In
Advances in neural information processing Systems (NIPS), pp. 344-352, 2017.
Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural networks with
few multiplications. CoRR, abs/1510.03009, 2015.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu,
and Alexander C. Berg. SSD: single shot multibox detector. CoRR, abs/1512.02325, 2015.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. CoRR, abs/1603.05279, 2016.
Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once:
Unified, real-time object detection. CoRR, abs/1506.02640, 2015.
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time
object detection with region proposal networks. CoRR, abs/1506.01497, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, 2015.
Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training
low bitwidth convolutional neural networks with low bitwidth gradients. CoRR, abs/1606.06160,
2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. CoRR,
abs/1612.01064, 2016.
8