Under review as a conference paper at ICLR 2020
Domain-Invariant Representations: A Look on
Compression and Weights
Anonymous authors
Paper under double-blind review
Ab stract
Learning Invariant Representations to adapt deep classifiers of a source domain
to a new target domain has recently attracted much attention. In this paper, we
show that the search for invariance favors the compression of representations. We
point out this may have a bad impact on adaptability of representations expressed
as a minimal combined domain error. By considering the risk of compression, we
show that weighting representations can align representation distributions without
impacting their adaptability. This supports the claim that representation invariance
is too strict a constraint. First, we introduce a new bound on the target risk that re-
veals a trade-off between compression and invariance of learned representations.
More precisely, our results show that the adaptability of a representation can be
better controlled when the compression risk is taken into account. In contrast,
preserving adaptability may overestimate the risk of compression that makes the
bound impracticable. We support these statements with a theoretical analysis illus-
trated on a standard domain adaptation benchmark. Second, we show that learning
weighted representations plays a key role in relaxing the constraint of invariance
and then preserving the risk of compression. Taking advantage of this trade-off
may open up promising directions for the design of new adaptation methods.
1	Introduction
In a standard Supervised Learning setup, we consider that training data is representative of test-
ing data. This ideal framework is often violated in practical applications since the data generative
process can be altered during the application phase e.g. different pre-processing, sample rejection,
imbalanced classes, condition of data collection, time evolving data... This so-called Distributional
Shift situation (Quionero-Candela et al. (2009); Kull & Flach (2014)) is recognized as a major chal-
lenge in Machine Learning (Amodei et al. (2016)). Domain Adaptation (DA) (Pan & Yang (2009)) is
a widely adopted strategy to mitigate its effects. It consists in leveraging unlabeled testing data (the
target domain) in order to transfer knowledge learned on labeled training data (the source domain).
A first line of study, named Importance Sampling (IS), assumes some stationarity in distributions
in order to approximate the risk in the target domain by weighting sample contribution (Shimodaira
(2000); Huang et al. (2007)). To be applied, IS needs statistical support sharing across domains
limiting its range of application when addressing high dimensional data. In that challenging con-
text, Ganin & Lempitsky (2015) have shown that deep classifiers can be adapted to a new target
domain by learning domain Invariant Representations (IR) and find theoretical support in the work
of Ben-David et al. (2007; 2010). It assumes that a classifier trained on representations of labeled
data sampled from both domains will achieve a negligible combined error (adaptability). However,
enforcing distribution invariance in the representation space can badly impact its adaptability, for
instance in the context of Target Shift (Zhao et al. (2019)) or when the support of the distributions
do not overlap enough (Johansson et al. (2019)). In the most general case of DA, it is not clear how
IR helps aligning domains and removing spurious correlations as pointed by Arjovsky et al. (2019).
In the present work, we show that the search of invariance favors learning representations with
a significant loss of information from the original features. We refer to this phenomenon as the
risk of Compression. In our understanding, this risk is not addressed in the original work from
Ben-David et al. (2007; 2010). We demonstrate that taking into account this risk has two major
advantages. First, it provides a better control on (non-trainable) terms involving the labels in the
1
Under review as a conference paper at ICLR 2020
target domain. Second, it introduces a new trainable term which embodies a risk of Compression
and then may open-up the path to new learning methods. We provide a theoretical analysis illustrated
with experiments on a standard Domain Adaptation benchmark. We underline that a better control
on non-trainable terms may lead to an over-pessimistic risk of compression. This brings to light
an inherent trade-off when learning representations for domain adaptation: How much compression
risk is acceptable in order to obtain invariance? As a first try to answer to this question, we point
that Learning Weighted Representations plays a key role in aligning representation distributions in
order to obtain invariance. More precisely, we show that under a constraint of strong source risk
conservation, weighting representations does not impact its adaptability.
2	Background
2.1	A decade of Domain Adaptation
The classical setup for modelling distributional shift is to introduce two domains: the source do-
main i.e. dataset where the model is trained with supervision and the target domain i.e. where the
model is tested or applied. Formally, for two random variables (X, Y ) of a given space X × Y,
we introduce two distributions on X × Y: the source distribution Ps (X, Y ) and the target distri-
bution Pt (X, Y ). We use the exponent notation s and t to differentiate source and target terms.
We define the hypothesis space H as a subset of functions from X to Y. The distributional shift
situation is then characterized by the shift: Ps (X, Y ) 6= Pt(X, Y ). Learning under Distribu-
tional Shift (Quionero-Candela et al. (2009)) consists in minimizing the risk on the target domain
εt(h) = Et['(h(X),Y)] for a given loss ' : Y × Y → R+ and an hypothesis function h ∈ H.
Assuming that w(X, Y) = Pt (X, Y)/Ps (X, Y) is tractable, it is straightforward to show that:
εt(h) = Es [w(X, Y )'(h(X), Y)]	(1)
Roughly speaking, this equation shows that it is possible to minimize the target risk using data
(χs,yi')ι≤i≤ns sampled from the source domain (i.e. (Xi,yi)〜Ps(X,Y)) Whilere-Weightingtheir
contribution in the source expectation with a factor w(xi, yi) (Importance Sampling). The difficulty
of Learning under distributional shift arises When w(xi, yi) is not tractable. The first reason is that
data sampled from Pt(X, Y) is often not easily available. In order to get a reasonable estimation of
w, practitioners are used to reformulate the problem as Unsupervised Domain Adaptation.
Definition 1 (Unsupervised Domain Adaptation). Given a loss `, an hypothesis class H, a source
domain (X × Y, Ps (X, Y)) anda target domain (X × Y, Pt (X, Y)), Unsupervised Domain Adapta-
tion consists in minimizing the target risk (i.e. finding h? = argminh∈H Et ['(h(X), Y)]) with finite
labeled sampling of (xs,yS)〜 Ps(X,Y) of the source domain and a finite unlabeled sampling
(Xj)〜Pt(X) of the target domain.
If unlabeled samples in the target domain are available, a first idea is then to approximate w(X, Y)
by w(X) = Pt(X)/Ps (X). This is equivalent to assuming that conditional distributions are con-
served across domains (i.e. Ps(Y|X) = Pt (Y|X)), a situation knoWn as Covariate Shift, that has
already been the subject of an extensive literature (Sugiyama et al. (2008); Huang et al. (2007)).
This formulation becomes ill-posed When feature domains (X, Ps(X)) and (X, Pt(X)) suffer from
non-overlapping support: w(X) is then undefined. To address this issue, Invariant Representations
learning (Long et al. (2015)) performs adaptation by looking for a representation Z = 夕(X) whose
distribution is conserved across domains Ps(Z) = Pt(Z). A common strategy consists in extracting
features Z to learn labels Y in the source domain While controlling representation invariance. That
is formulated as folloWs:
Learning Objective 1 (Learning Invariant Representations for Domain Adaptation).
g?, φ? = arg min (1 — α) ∙ εs(g ◦夕)+ α ∙ D(s,t)(^)	(2)
(g,w)∈G×Φ
where G is the set of classifiers and Φ is the set of representations. The hypothesis space is then
defined as H = {g ◦夕：g ∈ G,夕 ∈ Φ}. D(s,t)(夕)is a distance between distributions Ps(Z) and
Pt(Z) and α is a trade-off parameter.
The choice of D(s,t)(夕)has been the object of an extensive literature including f- divergence
measures such as domain adversarial learning (Ganin & Lempitsky (2015); Tzeng et al. (2014); Long
2
Under review as a conference paper at ICLR 2020
et al. (2018)), Integral Probability Measures such as Maximum Mean Discrepancy (Long et al. (2015;
2016)) or Optimal Transport (Shen et al. (2018)). See Kouw & Loog (2019) for an extensive review.
Interestingly, learning invariant representations to perform adaptation finds theoretical support from
the work of Ben-David et al. (2007; 2010) that we recall in the next section.
2.2	Theoretical guarantees of Domain Adaptation
We consider the set of representations Φ and classifiers G. The hypothesis space is denoted H =
{g ◦夕：g ∈G,夕 ∈ Φ}. We introduce the loss ' : Y ×Y → R+ which is assumed symmetric and
verifies the triangular inequality. We note the risk ε(h) = E['(Y, h(X))] for h ∈ H and ε(h, h0)=
E['(h(X), h0(X))] for (h, h0) ∈ H2. To emphasize the role of a representation Z = 夕(X), We
∙-v	∙-v
introduce f(Z) = E[Y |Z ] and H = {g ◦ φ : g ∈G}⊂H. We underline that： quantities depend on
夕 where, for the ease of reading, this dependence is omitted in notations when it is not ambiguous.
Ben-David et al. (2007; 2010) have derived a theoretical bound of the additional risk when using a
representation Z =夕(X) rather than raw features X:
Inequality 1 (Ben-David et al. (2007; 2010)). For a given φ ∈ Φ and g ∈ G, there is:
εt(g ◦夕)≤ εs(g ◦P)+ sup ∣εs(h,h0)-εt(h,h0)∣
'---{z---}	'-----{z--}	(h,h0 )∈HH
Target risk	Source risk	IVJ
=2∙d(H∆H): Disagreement
+ inf εt(h) + εs(h)
h∈H
|--------{---------}
=λ(H): Adaptability
(3)
The inequality 1 ensures that the target risk is bounded by the sum of the source risk, the disagree-
∙-v	∙-v	∙-v	∙-v
ment risk between two classifiers from representations (d(H∆H) named H∆H distance), and a
∙-v
third term (λ(H)) which quantifies the ability to perform well in both domains from representations.
∙-v
In the rest of the paper, we refer to the latter as the adaptability. Since Et[Y|X] is unknown, λ(H) is
∙-v
intractable in practical applications. By conveniently assuming that λ(H) is negligible, it is possible
to derive a Learning Objective 1 which minimizes εs(g◦2)+ 2d(H∆H). Indeed, minimizing D(s,t)
on 夕 leads to learn representations Z =夕(X) such that Ps(Z) = Pt(Z), where such situation is
∙-v	∙-v
sufficient to ensure that d(H∆H) = 0.
2.3	The curse of invariance in Domain Adaptation
Compressing representations is an easy way to achieve invariance. Assume Xs 〜N(0,1),Xt 〜
U(-1, 1) andY = 1X >0 then a trivial way to obtain representations invariance while separating
source data is to consider 夕：x → 1χ>0. The main drawback is a potential loss of information from
the original feature space. To explain this phenomenon, we first introduce formally the notion of
Hypothesis space compression:
Definition 2 (Hypothesis space compression). Let (夕 1,^2) ∈ Φ2,夕 1 compresses more H if
H(夕 1) ⊂ H(夕2). By language abuse, we will say that Zi = 夕 1(X) is more compressed than
Z2 =2 2(X).
Second, we build two possible solutions (gi,夕1) and (g2,夕2) of Learning Objective 1 by consider-
ʌ
ing the following compositions X →ψ Z1 →ψ Z2 →g Y such that 夕 and ψ ◦夕 are both in Φ and g
and g ◦ ψ are both in G. We set g1 = g ◦ ψ,夕 1 =夕 and g2 = g,夕2 = ψ ◦夕.These solutions are
chosen to ensure that g1 ◦夕 1 = g2 ◦夕2 and Z2 = 夕2(X) is more compressed than Z1 = 夕 1(X).
In the case when D(s,t) is the Jensen divergence, the data processing inequality (see Lemma 4.6 in
Zhao et al. (2019)) implies that (g2,夕2) is a better optimum than (g1,夕 1) according to Learning
Objective 1 since εs(g1 ◦夕 1) = εs(g2 ◦夕2) and D(s,t)(中2) ≤ ◎(*)(»). To conclude, learning
under objective 1 will favor compressed representations to enforce invariance. However, as noted in
Johansson et al. (2019); Zhao et al. (2019); Liu et al. (2019), more compressed representations will
degrade adaptability. Indeed,
λ(H(32))=	inf εt(h) + εs(h) ≤ inf εt(h) + εs(h) = λ(H(0))	(4)
h∈H(w2)	h∈H(wι)
since H(夕1) ⊂ H(夕2). A question which remains not addressed in the literature (except in Johans-
son et al. (2019) under the Covariate Shift assumption) is:
How compression may increase the risk of bad adaptability?
3
Under review as a conference paper at ICLR 2020
Source Target
• Positive
Ne Negative
g ◦夕
g。g0
Figure 1: We fix 夕 ∈ Φ and we note Dd = SuPP(Pd (X)) and Dd = SuPP(Pd (夕(X))) with d = s,t.
Labels in domains are not necessary for the analysis but, for a better understanding, We draw 夕 which
separates well labels in the source domain since our analysis will consider representations trained
by unsupervised domain adaptation. (Left) Illustration of the source Conservation where 夕 and 夕0
are two source equivalent representations and h = g ◦夕 and h0 = g ◦夕0 for a given g ∈ G. Since
xi ∈ Ds ∩ Dt we have 夕(xi)=夕0(χ1). Note that 夕(χ3) may differ with 夕0(χ3) since χ3 ∈ Ds.
This is also the case for x2 even if 夕(χ2) ∈ DS since χ2 ∈ Ds. (Right) Illustration of the projected
hypothesis for a given h ∈ Hs with h = g ◦夕0 and g ∈ G. φ embeds χ1,χ2 and χ3 closely in the
representation space. The source risk conservation enforces to have h = h on Ds . Since x1 and x2
are in Ds ∩ Dt,夕 and 夕0 coincide on these points.夕0 modifies χ3 and changes its output for g. Then
h can not fit this switch of output for x3 without changing outputs of both x1 and x2 illustrating the
impact of compression of 夕.
3	Theoretical analysis of Compression
3.1	Working plan
Our strategy aims to compare a representation 夕 with representations 夕0 which act similarly on the
source domain but differently on the target domain (source equivalence). First, by allowing to com-
pute the minimal combined error on source equivalent representations 夕0, we limit the risk of bad
adaptability of a fixed representation. Through an inequality called Target Compression, we show
this new control of adaptability comes at the cost of a new risk which embodies the Compression.
This is done by studying the projection of an hypothesis defined as its best approximation from
representations in the target domain among those that conserve the source domain error (source
conservation). Second, we rely on the H∆H inequality (Ben-David et al. (2007; 2010)) for stating
a new bound of the target risk (Invariant and Compressed Representations inequality). Finally, we
expose the particular interest of this bound: it allows a better control on adaptability by considering
compression. This is formulated by the Invariance - Compression trade-off.
3.2	Notations and Definitions
In the following, we fix a given 夕 ∈ Φ. We restrict our analysis to the case of ' : (y, y0) ∈ Y2 →
(y - y0)2 as a convenient choice for manipulating properly f(Z) = E[Y |Z]. Note that in the binary
classification case (Y = {0, 1}), ` is the accuracy. We define the set of representations which act
similarly on the source domain but differently in the target domain:
Definition 3 (Source equivalence). We introduce the set of source equivalent representations ΦS =
{夕0 ∈ Φ,夕0 =夕 on Ds} (where Ds = SuPP(PS(X))) and the source equivalent hypothesis space
Hs = {g ◦夕0 : g ∈ G, φ0 ∈ Φs}. Two elements of Φs are said source equivalent.
The source equivalent hypothesis space Hs is an extension of H adding hypothesis built using
representations which are source equivalent to 夕.For a given h ∈ Hs, we exhibit an interesting
subset of H considering hypothesis with the same source error than h.
Definition 4 (Source conservation). For a g^ven h ∈ HS, we note Hh = {h0 ∈ H : εs(h, h0) = 0}.
4
Under review as a conference paper at ICLR 2020
∙-v
∙-v
It is worth noting that for all h ∈ Hs, Hh is not empty (see Appendix A for details). We are now
ready to introduce the projection of an hypothesis h ∈ Hs and to this purpose, we note 夕0 ∈ ΦS and
g0 ∈ G such that h = g0 ◦夕0. Intuitively, the projection of h is defined as its best approximation
from 夕 in the target domain among those which verify the source conservation i.e. from hypothesis
g ◦夕 with g ∈ G such that εs (g ◦ ph) = 0.
∙-v	∙-v
Definition 5 (Projection). π : Hs → H is defined as foUows: π(h) = argminh^H九 εt(h, h0) for
∙-v
h ∈ Hs. π(h) is called the projected hypothesis. If there is no uniqueness of the minimum, any
solution works.
Illustrations of introduced definitions are provided in Figure 1. If h = g ◦夕 ∈ H, the projection
aims to map h to g0 ◦夕 with g0 = g both in source and target L2 norms. If, h = g0 ◦夕0 with 夕0 ∈ Φ,
the projection aims to map h to g ◦夕 such that g = g0 in source L2 norm while g ◦夕 achieves a
minimal distance with g0 ◦夕0 in target L2 norm. We can observe that ∀h ∈ Hs,π ◦ π(h) = π(h) in
both source and target L2 norms hence the (abusive) analogy with projection.
For ease of reading, we remind the notations and definitions in appendix B.
3.3	Our contribution
∙-v
∙-v
In the following, we fix a given H。 such that H ⊂ H。 ⊂ Hs . We are now ready to state
bound of the target risk:
∙-v
Inequality 2 (Target Compression). Given g ∈ G and noting h = π(h), we have:
a new
εt(g ◦中)≤ εt(g ◦ φ,fs ◦ φ) + SUp εt
h∈Ho
'----------------------------------------
∙-v
∙-v	∙-v
(h, h)	+ inf {εt(h) + εt(h, f ◦ φ)}
h∈Ho
}|
Y(Ho); Compression
{^^^^^^^^"∖ιf^^^^^^^^^~
λt (Ho); Adaptability
(5)
}
Proof. See Appendix A.
□
∙-v
This inequality introduces two interesting terms ^(H。)and λt(H°) with opposite roles; the former
∙-v
involves a supremum while the latter involves an infremum on H。. The term λt(HQ) is very similar
∙-v
to the adaptability λ(H) from Inequality 1 and will be further investigated. Before, we focus on
the term Y(H。) which reflects the possibility of finding hypotheses in H。that can not be well
approximated from target representations. The more information is lost from raw features, the higher
the risk, hence its name Compression. This can be formally stated as follows:
Theorem 1 (Y embodies the risk of compression). Let (夕 1,夕2) ∈ Φ2 two source equivalent source
representations. If 夕 1 is more ComPreSSed than 夕2, then Y(夕 1, H。) ≥ Y(夕2, H。).
∙-v	∙-v	∙-v	∙-v	∙-v
Proof. If 夕 ι is more compressed than ^2, there is H(夕 1) ⊂H(夕 2) ⊂ Hs then ∀h ∈ Hs, HhW) ⊂
∙∙w	∙∙w	∙∙w	∙∙w
Hh(22).	The projection involves an infremum then εt(h,	h(22))	≤ εt(h, h(2 1))	where h(φ)	=
argminho∈HKw) εt(h, h0). The supremum on h ∈ Ho conserves the inequality.	□
∙-v	∙-v
We now focus our attention on λt(H°). It differs with λ(H) in two ways. First, the source labelling
∙-v
function is involved only with the intermediate fs(Z) = Es [Y |Z]. Second, the risk associated with
∙-v	∙-v
it (εt(h, fS ◦夕))involves a target expectation. In order to obtain a comparable bound with Inequality
1, we state a new bound of the target risk which is our main contribution:
∙-v
Inequality 3 (Invariant and Compressed Representations). Given g ∈ G and noting h = π(h),
∙-v
β = 2inf九三左 εs(h) + εt(h,fs), we have:
εt(g ◦夕)≤ εs(g ◦夕)+ d(H∆H) + β + SUp εt(h, h)	+
h∈Ho
'-----{----}
γ(Ho )： COmPreSSion
inf εt(h) + εs(h)
h∈Ho
'---------{----------}
λ(Ho): Adaptability
(6)
5
Under review as a conference paper at ICLR 2020
∙-v	∙-v	∙-v	∙-v	∙-v
Proof. See AppendixA. The main idea is to bound εt(h, fS ◦夕)and εt(h, fS) using H∆H inequal-
ity. This introduces respectively εs(h, fS ◦夕)and εs(h, fS ◦夕)with residual terms 1 d(H∆H) + ɪβ
∙-v	∙-v	∙-v
(two times). We use the source risk conservation to bound εs(h, fS ◦夕)≤ εs(h, fS ◦夕).Then We
∙-v
bound £S(h, fS ◦夕)≤ εs(h) leveraging the Pythagorean theorem since ' is the L2 norm.	□
We are now ready to show the better control of the adaptability of the representation 夕：
∙-v	∙-v
Theorem 2	(Better control of adaptability). λ(H0) ≤ λ(H).
∙-v	∙-v
The proof is given by the infremum on Ho ⊃ H. Compression Y(Ho) and adaptability λ(H°) have
an opposite behavior. This reflects an interesting trade-off：
Theorem 3	(Invariance and Compression trade-off). With H and H2 such that H ⊂ H1 ⊂ Η2 ⊂
HS, there is λ(H1 ≥ λ(H) and YH ≤ γ(H2).
The proof is obtained by observing that H1o ⊂ Ho2 . Even if invariance may have a bad impact on
adaptability, our bound shows it is possible to have a better control of the adaptability while con-
serving the same degree of invariance. This control comes at the cost of an additional term involving
compression of representations. The trade-off between invariance and compression emphasizes than
the more control there is over adaptability, the higher the risk associated with compression. Further-
more, this new bound has a particular interest in providing theoretical support for the construction
of new learning objectives. The difficulty in DA comes from in the fact that the adaptability is not
trainable since it uses the labelling function in the target domain. Our study shows that better control
of adaptability (i.e. making the adaptation more reliable) is possible. The two additional terms β
and γY(Ho) can indeed be trained in an Unsupervised Domain Adaptation setting1.
4 Experiments
We are now conducting an experimental analysis on a widely used DA benchmark of digits recog-
nition (MNIST, USPS). We study two tasks M→U and U→M under the Learning Objective 1. The
representation 夕 is a convolutional neural network with a LeNet architecture (LeCun et al. (1998)).
We set the dimension of representations to 50 and G is the set of linear classifiers from the represen-
tation space to classes (with a softmax layer on top) and the Domain Adversarial Neural Network
(DANN Ganin & Lempitsky (2015)) for D(S,t). We use the implementation from Long et al. (2018)
for good reproductibility. Models are trained during 30 epochs and we note。the learned represen-
tation. In the following, we note CE the cross-entropy.
We suggest to study the trade-off between compression and invariance with the family：
Hn = {g。” ∈HS ： Et||d (X) - φ(X )|| ≤ ηEt∣3(X )||}, η> 0	⑺
We interpret this choice as a method for testing robustness of the learned representation to adversarial
attacks in the target domain.
Enforcing h ∈ Hoη . Enforcing h ∈ Hoη is challenging in practrice. We suggest to rely on a
penalization Lo to promote 夕 ≈ @ (in the sense of PS) and ||夕一以| . η ∙∣∣9∣∣ (in the sense of Pt).
We define the penalization Lo as follows：
Lo(φ)
ES|k(X)- 乳X)||
esII⅛^(x )11
+ ReLU
(EtWX)- 我 X )||
I Etιι⅛^(χ )∣ι
(8)
■"W
Source conservation. For a given h, the projected hypothesis is defined as h = g。@ where Y is
obtained by minimizing on g：
Π(g, h) = λ∏ ∙ CEs(g ◦虱X), h(X)) + (1 - λ∏) ∙ CEt(g。nX), h(X))	(9)
with λπ = 0.9 for enforcing the source conservation.
1The trainability of β is shown in Appendix A
6
Under review as a conference paper at ICLR 2020
∙-v
Estimation of λ(H°). We suggest to minimize on (g,夕)∈ H the following loss:
Λ(g, 0 = CEt(g。2(X),Y) + CEs(g。2(X), Y) + λ° ∙ L。(夕)	(10)
with λ0 = 102. CEt(g。0(X),Y) and CEs (g ◦ φ(X), Y) are respectively two proxies of εt(h) and
εs(h). Lo(0) promotes h ∈ Hn.
Estimation of Y(H°). We follow an adversarial training procedure alternating between
1.	maximizing an objective Γ(g, φ) on h = g ◦ φ ∈H (SUP step)
2.	computing the projected hypothesis (inf step)
During experimentation, We have observed that simply setting Γ(g, φ) = -Π(^◦⅛^, g^^)+λ^L◦ (φ)
(where g ◦ 0 is an estimation of the projected hypothesis) leads to learn noisy labels. We suggest
to prevent such pathological solutions by enforcing a low entropy adding to Γ a regularizer λent ∙
CE(h(X), Y) with (Y = y) = 1(h(X) = y) with λent = -1. Then, we set:
Γ(g, 0) = -∏(^。0,g。0) + λ°L°(ψ) + λent ∙ CE(h(X),Y)	(11)
Furthermore, for a fair comparison, we correct γ by comparing it with a baseline. This baseline is
allowed to train representations to fit the adversary.
Analysis We build new target domains modifying the label distribution which is known to badly
∙-v
impact λ(H) (Johansson et al. (2019); Zhao et al. (2019)). For a given target dataset D with classes
balanced, we define D1 ⊂ ... ⊂ D6 as follows: for 1 ≤ i ≤ 6, Di is obtained by removing half of
digit < i from Di-1 and setting D0 = D. Main results are reported in Figure 2. Although estimator
are trained using cross-entropy, the results are reported computing the accuracy on a test set in order
to be consistent with our theoretical analysis.
We can observe that stronger the adversary is (α → 1) (and the more the target shift (i → 6 for Di)),
ʃ—-	'—"
the more significant the difference between λ(H) and λ(H°). This demonstrates a better control
over adaptability especially when IR fails to perform adaptation. By playing with the value of η, we
see that the better control on adaptability is paid to the cost of a higher γ . In our example, the risk of
compression is estimated to a potential drop of performances from 〜12.5% to 〜27.5% of accuracy
in the target domain while the gain on adaptability is marginal by reducing it of 〜4%. To conclude,
a better control on adaptability is obtained with a pessimistic estimation of the risk of compression.
5	Weights for invariance relaxation
The fact that compression and invariance are conflicting objectives supports the point from Wu et al.
(2019) which claims that invariance is too strict a constraint. In this section, we show that invariance
can be relaxed leveraging Weighted Representations. More specifically, we show that taking into
account the risk of compression leads to introduce an adaptability term which does not depend on
learned weights. Then, weighting representations can reduce the discrepancy between the source
and the target distributions without impacting the adaptability. We introduce the set of weights W,
a subset of functions W : X → R+ such that Es[w(X)] = 1 and we note for W ∈ W, Pw∙s(X):=
w(X)Ps(X). The condition Es[w(X)] = 1 ensures that Pw∙s(X) is a distribution. We state a
similar result than Inequality 1.
Inequality 4 (Weighted version of Inequality1). For a given φ ∈ Φ and g ∈ H,
εt(go 0) ≤ εw.s(go0) + 2dw(H∆H)+ λw(H)	(12)
∙-v	∙-v
with dw(H∆H) = sup(h ho)∈HH ∣εw∙s(h, h0) — εt(h, h0)∣ and λw = infh品 εt(h) + εws(h).
Interestingly, W and 0 play similar role in inequality 4 since they are actionable parameters to min-
imize εw s(g。0)+ 2dw(H∆H) while we can not quantify their impact on λw(H). We show that
taking advantage of compression leads to an adaptability which does not depend on weights. This
needs to introduce some definitions:
7
Under review as a conference paper at ICLR 2020
Theorem 2. More the adversary is strong in LO 1 (α → 1), more ∆(λ) increases (similar analysis
for D1 to D5). Down: Illustration of the trade-off from Theorem 3 in the particular case of D4
with α = 0.75 on U → M. Both γ and its correction are reported. When η increases, the risk of
compression increases while the adaptability assumption is better controlled. Better in color.
Definition 6 (Strong source conservation). For a given h ∈ Hs, we note HHh = {h0 ∈ HH : h =
h0 on Ds } where Ds = supp(Pt (X)).
Definition 7 (Strong projection). Π : HL s → H is defined asfollows π(h) = argminho∈H εt(h, h0)
for h ∈ Hs . If there is no uniqueness of the minimum, any solution works.
We are now ready to state the role of weights as a relaxed invariance for a lower compression:
Inequality 5 (Weighting Invariant and Compressed Representations). Given g ∈ G and noting
h = Π(h), βw = 2inf九三4 εws(h) + εt(h,fs), we have:
εt(g。夕)≤	inf	(εw∙s(g。夕)+	1dw(H∆H)	+1 βw	1+ sup εt(h,h)+ inf εt(h)+εt(h, fs。P)
w∈W	2	2	h∈H°	h∈H
Proof. This is a straightforward extension of inequality 2 applied to Pw∙s. Furthermore,
infh∈H° εt(h) + εt(h, fs ◦夕)does not depend on W due to strong risk conservation (risk con-
servation would have been dependent on W through the constraint εws(h,h) = 0).	□
Adding weights exposes to the risk of increasing the variance of estimators when considering finite
sampling analysis (see Cortes et al. (2010) for an extensive theoretical analysis of Importance Sam-
pling). In inequality 5, this can be controlled with a good choice of W e.g. by considering weights
W ≤ M for a given M > 0.
6	Related work
Generalization bounds for DA have been the object of an extensive literature for both Importance
Sampling Cortes et al. (2010) and Invariant Representations Ben-David et al. (2007; 2010); Mansour
et al. (2009). It has attracted a lot of attention in order to understand failure cases observed when
using IR (Zhao et al. (2019); Johansson et al. (2019)). The theoretical analysis in Johansson et al.
(2019) is the work closest to ours. They introduce a loss information metric which quantifies the
risk of using non-invertible representations. The present work differs in three ways. First, our work
follows the formalism introduced in Ben-David et al. (2007) which is widely adopted in the commu-
nity. Second, contrary to the information loss, our theory provides a tractable risk of compressing
representations. Finally, we do not rely on the Covariate Shift assumption which gives our work a
broader range of applications.
8
Under review as a conference paper at ICLR 2020
7	Conclusion
By addressing domain generalization under the perspective of compression, we have bounded the
〜 〜	'—"
target risk by a sum of trainable terms (εs(g ◦夕)+ d(H∆H) + β + Y(H◦) and a term λ(H0)
∙-v
which embeds a new adaptability criterion of representations. We have shown that λ(H) can be
better controlled by enriching the hypothesis class H◦. This control comes at a cost of a higher risk
of compression underlying a trade-off between invariance and compression when learning IR for
DA. Through an empirical study on a standard benchmark, we have exhibited the better control of
adaptability. Furthermore, we have shown how our framework can be applied for studying adapta-
tion robustness to adversarial attacks. Finally, as an attempt to obtain representation distributions
invariance with a lower compression, we have emphasized the role of weighted representations for
relaxing the constraint of invariance.
Achieving representation invariance can reduce the generalization gap between two domains. How-
ever, all invariances will not have the same guarantees for better generalization. Our work shows
that we will look for representations which preserve the best the information in the original target
features spaces. We leave both the design on new Domain Adaptation methods which incorporate
such consideration and relevant design of Ho as future works.
Promising directions (1) Our analysis is restricted to the comparison among source equivalent
representations of a given representation 夕 ∈ Φ. Then, for two given representations 夕 1 and
夕2 of Φ which does not verify the source equivalence, our analysis does not allow to state if 夕 1
presents, or not, better guarantees on the success of the adaptation. Understanding how the source
equivalence can be relaxed can provide to the present work a broader range of applications. (2)
Obtaining a robust estimation of Y(Ho) by following an adversarial training procedure appeared
to be challenging in practice. Finding fast and robust estimations of Y(Ho) (or a proxy of it) may
help to derive new learning objectives or policy for model selection. For instance, Information
Theory gives better suited devices for modelling the phenomenon of compression. (3) A strategy
may to enforce invariance directly in the target domain by transporting source samples. Formally,
assuming Ds ∩ Dt = 0, we set E such that 夕(X) = X on Dt and 夕(Ds) ⊂ Dt. If 夕 is fixed
in the source domain (i.e. e∣ds is fixed) the optimal representation from the compression per-
spective is E|Dt (X) = X. This consideration is connected with two recent trends which perform
adaptation by generating cross domain samples (Sankaranarayanan et al. (2018); Hoffman et al.
(2018); Bousmalis et al. (2016)) or by finding Optimal Transport between the source and the tar-
get domains (Courty et al. (2016; 2017)). (4) The Covariate Shift assumption can be stated as
ʌ ʌ
Ho = {h ∈ H, εs (h, h) = 0 with h = arg minh∈H εs (h)}. Such Ho may provide a new frame-
work for addressing the case of non-overlapping supports in Covariate Shift adaptation.
References
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Con-
crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Advances in neural information processing systems, pp. 343-351,
2016.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In Advances in neural information processing systems, pp. 442-450, 2010.
9
Under review as a conference paper at ICLR 2020
Nicolas Courty, Remi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for do-
main adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-
1865, 2016.
Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In Advances in Neural Information Processing
Systems, pp. 3730-3739, 2017.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International Conference on Machine Learning, pp. 1180-1189, 2015.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International
Conference on Machine Learning, pp. 1994-2003, 2018.
Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Scholkopf, and Alex J Smola. Cor-
recting sample selection bias by unlabeled data. In Advances in neural information processing
systems, pp. 601-608, 2007.
Fredrik Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-
invariant representations. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 527-536, 2019.
Wouter M Kouw and Marco Loog. A review of single-source unsupervised domain adaptation.
arXiv preprint arXiv:1901.05335, 2019.
Meelis Kull and Peter Flach. Patterns of dataset shift. In First International Workshop on Learning
over Multiple Contexts (LMCE) at ECML-PKDD, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable adversarial train-
ing: A general approach to adapting deep classifiers. In International Conference on Machine
Learning, pp. 4013-4022, 2019.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features
with deep adaptation networks. In Proceedings of the 32nd International Conference on Interna-
tional Conference on Machine Learning-Volume 37, pp. 97-105. JMLR. org, 2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136-
144, 2016.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. In 22nd Conference on Learning Theory, COLT 2009, 2009.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset
shift in machine learning. The MIT Press, 2009.
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to
adapt: Aligning domains using generative adversarial networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 8503-8512, 2018.
Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation
learning for domain adaptation. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
10
Under review as a conference paper at ICLR 2020
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V Buenau, and Motoaki Kawanabe.
Direct importance estimation with model selection and its application to covariate shift adaptation.
In Advances in neural information processing systems, pp. 1433-1440, 2008.
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. In International Conference on Machine Learn-
ing, pp. 6872-6881, 2019.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532, 2019.
11
Under review as a conference paper at ICLR 2020
A Main Proofs
Proposition 1 (The source conservation hypothesis is not empty). For a given 夕 ∈ Φ and h ∈ 务S,
∙-v
Hh is not empty.
Proof. If h ∈ Hs, it exists g ∈ G and φ, ∈ Φ such that h = g Q φ,. We set h0 = g Q 夕 ∈ H, there is
εs(h, h0) = εs(g o ",g o 夕)=εs(g o φ, g o φ) = 0 since φ =夕，on DS then h0 ∈ Hh.	□
∙-v
Inequality 6 (Target Compression). Given φ ∈ Φ, g ∈ G and noting h = π(h), there is for any Ho
such that H C Ho UHS:
εt(g o φ) ≤	εt(g	o φ,fs	o φ)	+ sup	εt(h,	h)	+ inf {εt(h) + εt(h,	fs	o φ)}	(13)
h∈Ho	h∈Ho
Proof. Let φ ∈ Φ, g ∈ G and h ∈ H0:
εt(g o φ)	≤	εt(fs	o φ) + εt(fs o φ,g o φ)	(14)
≤	εt(h)	+ εt(h, fs o φ) + εt(fs o ψ,g o φ)	(15)
≤	εt(h)	+ εt(h, h) + εt(h, fs o φ) + εt(fs	o	Ag	o φ)	(16)
≤	εt(fs	o φ,g o φ) + εt(h, h) + εt(h) + εt(h, fs	o φ)	(17)
∙-v	∙-v
This is a succession of triangular inequalities which involve respectively fS o 夕，h and h. The
∙-v
last inequality is a convenient arrangement of terms. We bound εt (h, h) by its supremal value for
∙-v	∙-v	∙-v	∙-v
h ∈ Ho, then εt(g o 夕)≤ εt(fS o 夕,g o 夕)+ suph∈Ho{εt(h, h)} + εt(h) + εt(h,fS o 夕).Since
this holds for all h ∈ Ho, we bound εt(g o 夕)by the infremum for h ∈ Ho.	□
∙-v
Inequality 7 (Invariant and Compressed Representations). Given 夕 ∈ Φ, g ∈ G and noting h
ʃ—"	ʃ—-	ʃ—-
π(h), β = 2inf h∈H £S(h) + εt (h, f s) , there is for any Ho such that H C Ho C Hs :
εt(g o 2)≤ εs(g o 夕)+ d(H∆H) + β +
sup εt(h, h)	+
h∈Ho
、------V-----'
彳(Ho)： Compression
inf εt(h) + εs(h)
h∈Ho
'---------V----------'
X(Ho)： Adaptability
(18)
∙-v	∙-v
Proof. First of all, we recall the H ∆H inequality from Ben-David et al. (2007; 2010), for any fS
and ft, there is:
∀h ∈H,εt(h,ft) ≤ εs(h,fs) + 1 d(H∆H) + inf εt(h,ft)+ εs(h,fs)	(19)
2	h∈H
∙-v
A particular case of that inequality applied to fS = ft = fS o 夕 leads to bound:
∙-v	∙-v	I	∙-v ∙-v	∙-v	∙-v
εt(g o A fs o 0 ≤ εs(g o A fs o 0 + qd(H∆H) + hinf_ εs(h, f s o ⑼ + εt(h, f s o。)
≤ εs(g o 0 + 1 d(H∆H) + inf εs(h) + εt(h,fs o ⑼	(20)
2	h∈H
where bounding εs(g o 夕,fs o ¢) ≤ εs(g o ¢) is a direct property of '(y, y0) = (y - y0)2 and
∙-v	∙-v	∙-v
f S(Z) = Es[Y|Z]: εs(g o ¢) = εs(g o ¢, f S o ¢) + εs(f S o ¢). Note that, it is not possible
to do so for εt(g o ¢, f S o ¢) since the expectation is computed in the target domain. We define
ʃ—"	ʃ—-	ʃ—-
β = 2infh∈H εs(h) + εt(h, fS o ¢). We apply a second time H∆H inequality, there is for any
h ∈ Ho：
ʃ—" ʃ—"	ʃ—" ʃ—"	I	ʃ—-	ʃ—-	'—"	ʃ—"
εt(h, f s o ¢) ≤ εs(h,f s o ¢) + &d(H∆H) + h^f ɛt(h,f s o ¢) + εs(h,fs o ¢)
∙-v	I	∙-v	∙-v	∙-v
≤ εs(h) + 2d(H∆H) + inff εt(h,f s)+ εs(h)	(21)
12
Under review as a conference paper at ICLR 2020
∙-v	∙-v	∙-v
which is licit since h ∈ H. Relying on the projected hypothesis h allows us to preserve the invari-
∙-v	∙-v
ance: even if h ∈ H°, d(H∆H) is involved, not d(H°∆H°). Then, We use the property of source
∙-v	∙-v	∙-v
risk conservation h ∈ H Which enforces εs (h, h) = 0 for noting that εs (h) ≤ εs (h) + εs (h, h) =
εs (h) then:
t ∙-v	∙-v	I	∙-v	∙-v	t	∙-v
εt(h,fs ◦夕)≤ εs(h) + 4d(H∆H) + inf εt(h, fs) + εs(h)
Finally, taking the infremum:
h∈ff εt(h) + εt(h, fs ◦ φ) ≤ 2β + 2d(H∆H) + h∈ff εt(h) + εt(h)	(22)
Combining all inequalities leads to the stated one.	□
∙-v	∙-v
Theorem 4 (Better control on the adaptability). λ(H0) ≤ λ(H).
Proof. We simply use the fact that H ⊂H then using the property of the infremum:
X(H。)= inf εt(h) + εs(h) ≤ inf εt(h) + εs(h) = λ(H)	(23)
h∈f ◦	h∈f
□
Theorem 5 (γ embodies the risk of compression). Let (夕 1,夕2) ∈ Φ2 two source equivalent repre-
sentations. If 夕 1 is more compressed than 夕2, then γ(夕1, Ho) ≥ Y(夕2, Ho) for H ⊂ Ho ⊂ HS.
Proof. If 夕 1 and 夕2 are two equivalent representations, there is H(夕1) ⊂ H(夕2) ⊂ HS then
∙-v	∙-v	∙-v
∀h ∈ Hs, (H(n) ∩{h0 ∈ H : εs(h, h0) = 0}) ⊂ (Hw2) ∩ {h0 ∈ H : εs(h, h0) = 0})
∙-v	∙-v
which guarantees Hh(夕 1) ⊂ Hh(夕2). Finally
εt(h,h(P2)) ≤ εt(h,h(2 1))
∙-v
invoking the property of the infremum where h(夕)=argminho∈f 九(陋)εt(h, h0). The supremum on
h ∈ Ho conserves the inequality.	□
Proposition 2 (β is trainable from the data). Let (gn, ^n)n∈N a Sequence such that a ∙ Ms (gn ◦夕n) +
(1 — a) ∙ D(夕n) —→ 0 then βn —→ 0.
Proof. First of all, since DQn) → 0 then Pt(夕n(X)) → Ps(夕n(X)). We note that for any n ∈ N,
∙-v	∙-v
β ≤ 2∙(εs(gn O^n)+ εt(gn ◦中 n, f ◦^n)) ≤ 2∙(2∙εs (gn O^n) + (εt(gn ◦中 n, f O^n)-εs(gn。中 n )))
∙-v
Since εs(gn ◦ φnt) → and (εt(gn ◦ ‹‰fs) - εs(gn ◦ 4)) → 0 (because Pt Wn(X)) → PSWn(X)))
there is βn → 0.	□
B Notations
•	Φ is the set of representations.
•	G is the set of classifiers.
•	H is the hypothesis space H = G ◦ Φ
•	For a given 夕 ∈ Φ, we note H(夕)={g ◦夕：g ∈ G} = G ◦夕.When there is no ambiguity,
we note H(夕)=H.
•	For a given h ∈ H, we note ε(h) = E[(Y - h(X))2]. For d = s, t, we note a domain risk
εd(h) = Ed[(Y - h(X))2].
•	For d = s,t and a given φ ∈ Φ, we note the conditional expectation fd(Z) = Ed[Y|Z]
with Z =夕(X).
13
Under review as a conference paper at ICLR 2020
∙-v
•	Equivalence. For a given φ ∈ Φ, the set of source equivalent representations Φs(夕)=
{夕0 ∈ Φ,夕0 =夕 on Ds} (where DS = SuPP(Ps(X))) and the source equivalent hypoth-
∙-v	∙-v	∙-v
esis space Hs(夕)={g ◦夕0 : g ∈ Gs ∈ Φs}. TWo elements of Φs are said source
equivalent. When there is no ambiguity, we note ΦS and Hs.
•	Conservation.
-For a given h ∈ Hs, we note Hh = {h0 ∈ H : εs(h, h0) = 0}.
-For a given h ∈ Hs, we note HHh = {h0 ∈ H : h = h0 on Ds} where Ds =
suPP(Pt(X)).
• Projection.
∙-v	∙-v	∙-v
-π : Hs → H is defined as follows: π(h) = argminh^H^九 εt(h,h0) for h ∈ Hs.
π(h) is called the projected hypothesis. If there is no uniqueness of the minimum, any
solution works.
∙-v	∙-v	∙-v
-Π : Hs → H is defined as follows π(h) = argminhy∈h九 εt(h, h0) for h ∈ Hs. If
there is no uniqueness of the minimum, any solution works.
14