Under review as a conference paper at ICLR 2020
Empirical observations pertaining to learned
PRIORS FOR DEEP LATENT VARIABLE MODELS
Anonymous authors
Paper under double-blind review
Ab stract
There exist many forms of deep latent variable models, such as the variational au-
toencoder and adversarial autoencoder. Regardless of the specific class of model,
there exists an implicit consensus that the latent distribution should be regularized
towards the prior, even in the case where the prior distribution is learned. Upon
investigating the effect of latent regularization on image generation our results
indicate that in the case when a sufficiently expressive prior is learned, latent reg-
ularization is not necessary and may in fact be harmful insofar as image quality is
concerned. We additionally investigate the benefit of learned priors on two com-
mon problems in computer vision: latent variable disentanglement, and diversity
in image-to-image translation.
1	Introduction
In the machine learning subfield of deep latent variable models such as variational autoencoders
(VAEs) Kingma & Welling (2014) and adversarial autoencoders (AAEs) Makhzani et al. (2016)
have attracted a significant amount of research interest Bowman et al. (2016); Kingma et al. (2016);
Huang et al. (2017); Rezende & Viola (2018); Dai & Wipf (2019); Tolstikhin et al. (2018). Despite
this, in their standard form they are still largely outperformed in terms of synthesized image quality
by other deep generative models such as generative adversarial networks (GANs) Goodfellow et al.
(2014); Karras et al. (2018a;b), autoregressive models Van den Oord et al. (2016) and flow-based
models Dinh et al. (2015; 2017); Kingma & Dhariwal (2018). Even so, latent variable models
maintain a number of properties that make them an attractive alternative, such as stable and efficient
training as well as efficient synthesis.
In virtually all research done using such models, some form of regularization is imposed on the
encoder distribution in order to push it towards the prior distribution. For VAEs, this regularization
exists in the form of a KL divergence between approximate posterior and prior, while AAEs force
the marginal posterior to match the prior using an adversarial loss. This is of course necessary
if the prior is fixed as is often the case, however we argue that when the prior is learnable it is
possible to achieve a tight fit between marginal posterior and prior without regularization, and that
regularization in this case may actually have a negative impact on sample quality. Experimental
results demonstrating this are given in section 4. We also discuss how better disentanglement of the
latent variable is achieved as a consequence of removing latent regularization, with experimental
results provided in section 5. Finally, we show that removing latent regularization can improve
sample diversity in image-to-image translation tasks; results are provided in section 6.
2	Background and Preliminaries
2.1	Deep latent variable models
Let x be a random vector residing in space X with
X 〜P(X). Additionally, let X = {x(1),..., x(n)} be a set of i.i.d. observations drawn from the data
distribution p(x). When training likelihood-based generative models, we are interested in maximiz-
ing the log-likelihood
n
log p(X) = X logp(X(i))	(1)
i=1
1
Under review as a conference paper at ICLR 2020
Latent variable models introduce a latent vector z residing in latent space Z which together with x
forms the joint distribution p(x, z). The likelihood of an individual datapoint is then given by
p(x(i)) =	p(x(i) |z)p(z)dz
(2)
In the case when the parameterization of p(x(i) |z) is complex, such as when it is parameterized by
a neural network, Eq. 2 becomes intractable.
Variational Autoencoders
Variational autoencoders circumvent the issue of intractability by noting that the log-likelihood of
an individual datapoint can be expressed as:
log p(x(i)) - DKL[q(z|x(i))||p(z|x(i))]
Eq(z|x(i))[log p(x(i)|z)] - DKL[q(z|x(i))||p(z)]
(3)
where the right hand side of Eq. 3 is commonly referred to as the Variational Lower Bound, which
provides a tractable lower bound for the data likelihood that can be optimized directly.
However, variational autoencoders suffer from a number of problems that degrade the quality of
samples. For example, when the posteriors of a set of data points overlap at some point in Z,
then the optimal decoding at that point will be a weighted mean of the corresponding data points
Rezende & Viola (2018). This contributes to the well known problem of blurry samples. The
standard method of encoding the posterior distribution as a diagonal Gaussian is commonly blamed
for exarcerbating this issue; the true posterior p(z|x(i)) will almost certainly never be a diagonal
Gaussian, therefore the model is in a sense forced to fit square pegs into round holes, resulting in
either posterior overlap or “holes” that have high probability under the prior but are never seen by
the decoder Zhao et al. (2017). Works such as IAF Kingma et al. (2016) propose to increase the
flexibility of the approximate posterior by turning it into a normalizing flow (discussed in a later
section), allowing a tighter fit between true and approximate posteriors. In Chen et al. (2017), the
authors propose modelling the prior of a variational autoencoder using an autoregressive flow, and
demonstrate that it is equivalent to modelling the posterior using IAF along the encoder path, while
being deeper along the decoder path.
It is also a well known issue that VAEs tend to not make full use of the latent code, as the objective
becomes trapped in a local minima in which the posterior is close to the prior. Such a state occurs
early on when the signal from the latent code is weak, resulting in a weak reconstruction term
log p(x(i) |z) that is easily outweighed by the KL divergence term in the objective. This exacerbates
the problems caused by overlapping posteriors, resulting in even blurrier samples, and additionally
results in lower sample diversity. This issue is particularly pernicious in the conditional setting if
care is not taken, as it is easily possible for the model to entirely ignore the latent code when it is
conditioned on a relevant context, resulting in a deterministic mapping.
Many methods for encouraging use of the latent code have been proposed. For instance, annealing
the KL divergence term from 0 to full strength Bowman et al. (2016); Huang et al. (2018b) allows
the model to largely ignore the KL divergence term at the beginning of training, such that the au-
toencoder is able to learn a good reconstruction without being constrained by the prior. By the time
the KL divergence term is annealed to full strength, the autoencoder is expected to have made good
use of the latents, preventing the model from regressing to a state in which the posterior is close the
prior. “Free bits”, introduced in Kingma et al. (2016), places a limit on the information in nats per
latent subset that can contribute to the KL divergence term, ensuring that each subset can contribute
at least λ nats of information without penalty. In the context of conditional variational autoencoders,
Zhu et al. (2017) proposed to add a latent reconstruction term to the objective to encourage the model
to make full use of the latent code.
At their core, all of these techniques are intended as a means of alleviating over-regularization im-
posed by the KL divergence term in the objective.
This term is of course necessary if the prior takes on a fixed form (e.g. a standard normal
distribution). Indeed, all other issues aside, forcing the aggregate posterior distribution towards a
pre-determined prior may in and of itself over-regularize the model. In the case that the prior is
2
Under review as a conference paper at ICLR 2020
learned, however, it is possible to eliminate regularization entirely by removing the KL divergence
term from the objective, hence obviating the need for any aforementioned techniques.
Generative Moment Matching Networks Li et al. (2015)
Generative Moment Matching Networks (GMMNs) Li et al. (2015) are a very relevant work in
which the authors propose using an unregularized autoencoder as a generative model. This is the
only work to our knowledge that proposes a latent variable model without any latent regularization.
In their proposal they use maximum mean discrepancy (MMD) in order to match the statistics of the
latent distribution of the autoencoder and a random variable zT = h(z0), where h is parameterized
by a neural network and zo 〜U(-1,1). In their implementation of MMD they use the Gaussian
kernel k(x,x0) = exp(-圭||x - x0∣H) with bandwidth parameter σ2 which, when the kernel
trick is applied, allows matching between all possible moments of the two distributions. The most
obvious drawback with this method is that it requires careful tuning of the bandwidth parameter σ2,
as it plays a crucial role in determining the accuracy of the model. Additionally, training with large
batch sizes can be problematic as the complexity of the objective calculation scales quadratically.
At the same time, accuracy of training with MMD has been shown to degrade rapidly as the batch
size becomes smaller Li et al. (2017).
Adversarial Autoencoders Makhzani et al. (2016)
Another model, closely related to GMMN+AE, is the Adversarial Autoencoder (AAE) Makhzani
et al. (2016); Tolstikhin et al. (2018). Rather than matching distributions using MMD, the authors
propose utilizing an adversarial objective to regularize the distribution of the autoencoder latent
space towards N(0, I). Regularization of the autoencoder in this way may introduce excess noise,
the reasoning being that GAN discriminators are known to constantly shift their probability mass
around during training in response to the generator, and so the decoder of an AAE is forced to
deal with noisy latent codes. Additionally, if the data distribution lies on a manifold that is of
lower dimensionality than the latent space and if deterministic encoders are used the model will
necessarily produce a degenerate latent distribution, in which case the “holes” problem discussed
earlier manifests.
Therefore we propose an alternative: train an unregularized autoencoder and separately learn a
generator which attempts to generate samples from the same distribution as the autoencoder’s latent
space, i.e. a GAN is used to learn the prior. We refer to this modification of AAE as Prior-AAE.
Both the AAE and Prior-AAE models inherit the drawbacks of adversarial training in general, e.g.
mode collapse, unstable training, and lack of convergence Salimans et al. (2016), although many
techniques have emerged in recent years to mitigate these issues Metz et al. (2017); Arjovsky et al.
(2017); Mescheder (2018).
2.2	Normalizing flows
Normalizing flows are a class of model that allows tractable and exact latent variable inference and
log-likelihood evaluation by taking advantage of the change of variables formula for an invertible
function f : X → Z
Px(X)= Pz(f(x)) det(dfXχ))	(4)
Choosing a simple distribution for pz (z) makes evaluation of pz (f (x)) tractable, and also makes
sampling straightforward. In general, computing det( df(χ)) takes O(n3) time, where n is the di-
mensionality ofx. We can improve on this however by carefully choosing the form of the bijection
f (x). Autoregressive flows Kingma et al. (2016); Papamakarios et al. (2017); Huang et al. (2018a)
constrain f (x) to be autoregressive, such that the Jacobian df(x) is triangular. Since the deter-
minant of a triangular matrix is the product of its diagonal entries, computation of the Jacobian
determinant becomes O(n). For autoregressive flows with affine transformations, z is given by
zi = xis(x1:i-1) + t(x1:i-1) where s and t are neural networks. Greater model capacity can be
achieved by composing multiple autoregressive bijections and permuting the order of latent dimen-
sions in-between each, giving f = f1 ◦ w1 ◦ f2 ◦ w2 ◦ ... ◦ fT -1 ◦ wT -1 ◦ fT where wt(x) = Wtx
and Wt is some permutation matrix. Following from this we define the set of random variables
3
Under review as a conference paper at ICLR 2020
{zt ∈ Zt}tT=0 where zt-1 = ft(zt), zT = x and z0 = f (x). It is worth noting that, as autore-
gressive flows are universal approximators Huang et al. (2017), they can be used to model arbitrarily
complex distributions.
By transforming the posterior samples of a VAE based on an autoregressive flow, the capacity of the
posterior can be increased in a way that admits tractable posterior density evaluation, and in doing
so it is easy to see that the issue of posterior overlap can be mitigated. However, due to the highly
non-convex nature of the objective, it is still easily possible for the model to get trapped in local
minima in which the posterior is close to the prior, preventing the model from making full use of the
latent code.
3	Regularization experiment approach
In the description of our approach we use the following notation: The encoder is denoted by Eφ(x),
the decoder is denoted by Dψ(z), and the latent code is denoted by
Z 〜qφ(Z).
In order to empirically investigate the effect of latent regularization on image quality, we consider
two kinds of models. For the first model, we consider VAEs with Gaussian decoder and prior learned
via normalizing flow as proposed in Chen et al. (2017), with the KL divergence term weighted by a
hyperparameter β. Rather than the autoregressive flow used in Chen et al. (2017), we use a flow that
splits the vector in half at each step as in Dinh et al. (2017), as this allows efficient computation of
the bijection in both directions, facilitating both efficient inference and efficient sampling. We note
again that this gives rise to a set of latents {zt ∈ ZtyT=0, where zo 〜N(0, I) and ZT should have
distribution as close as possible to qφ(Z). The bijection is given by fθ : ZT → Z0 with Z0 = fθ(ZT).
To evaluate the effect of β we train the model in the following way. Firstly we assume that the
encoder Eφ encodes the parameters of the posterior qφ(z∣x), and that the decoder Dψ encodes the
parameters of pψ (x|z). We then split the objective into two separate functions:
Lφ,ψ (φ,ψ; X(i)) = EZ 〜qφ(z∣χ(i))[lθg pψ (x(i)|z)
+ logPθ(z) - log qφ(z∣x(i))]
Lθ (θ; Xei)) = EZ 〜qφ(z∣χ(i)) [log Pθ (z)]
(5)
(6)
The encoder and decoder terms can be dropped from Lθ since they are independent of θ . Using
the gradients Vφ,ψLφ,ψ (φ, ψ; x(i)) and VθLθ(θ; x(i)) concurrently in a gradient ascent step is then
equivalent to performing gradient ascent on the original objective. Introducing β into the model, we
can rewrite Lφ,ψ as
EZ〜qφ(z∣χ(i))[logpψ(x(i)|z) + β(logPθ(z) - logqφ(z∣x(i)))]
(7)
For VAEs with a fixed prior (e.g. a standard normal distribution), as β becomes larger we achieve a
more structured latent space at the expense of reconstruction quality Matthey et al. (2017), and vice
versa as β becomes smaller. When the parameters ofthepriorpθ(z) are learnable, however, decreas-
ing β does not necessarily sacrifice latent structure, as any additional incurred divergence between
the aggregate posterior qφ(z) and the prior pθ(z) can be mitigated by adjusting the parameters of
pθ(z). When β = 0 we are left with
Lφ,ψ (φ,ψ; x(i)) = EZ 〜qφ(z∣χ(i))[lθg Pψ (X(i)|z)]
(8)
which is the objective of an unregularized autoencoder. It is worth pointing out that at this point,
if L2 loss is used for the decoder, we are in fact minimizing the 2-Wasserstein distance between
the model and data distributions. This comes from the proof in Tolstikhin et al. (2018) showing
the equivalence between the optimal transport objective and Ex〜p(x)[Ez〜qφ(z∣x) [c(x, z)]] under the
constraint that qφ(z) = pθ(z), where c is a cost function. In their case the constraint is relaxed and
replaced by a regularizer that pushes qφ(z) towards pθ (z). In our case, rather than regularizing
qφ(z), we are instead learning pθ (z).
4
Under review as a conference paper at ICLR 2020
(a)	(b)	(c)	(d)
Figure 1: Distribution of the latents of a flow prior with β = 0 after training on MNIST with
dim(Z) = 2. (a) Distribution of fθ (Eφ (x)) for each x in the test set. Datapoints are colored
according to class. (b) Distribution of Eφ (x) for each x in the test set. (c) Same as (b) but without
class coloring. (d) Distribution of f—1 (z0) where z0 〜N(0, I). It can be seen that while the
autoencoder learns a complex latent distribution with classes well separated, the normalizing flow is
able to learn a close match.
In Dai & Wipf (2019) the authors propose learning the variance of the decoder distribution, i.e.
the decoder loss becomes N(X⑴ ∣Dψ(z), γI) where Y is a learnable parameter. The authors prove
that it is always possible to achieve a better VAE cost by lowering the value of γ. We therefore
also consider such a decoder in our experiments. Although we lose the property of 2-Wasserstein
distance minimization in the case β = 0, in practice performance should not differ greatly. In the
case where β > 0, however, the decoder loss will grow to quickly dominate the KL divergence term,
and so it is expected that behaviour will be more akin to the case where β = 0.
The second model that we consider is an AAE with learnable prior. This is equivalent to an AAE
with fixed prior with the addition that the adversarial loss that regularizes qφ(z) is additionally
used to learn the prior parameters. We can apply a hyperparameter β to control the strength of the
adversarial loss applied to qφ (z) in the same way as we did for the first model. As above, if L2
loss is used for the decoder this model minimizes the 2-Wasserstein distance. A disadvantage of this
model is that it has no way to infer the latent variable z0 from an arbitrary data sample; its generative
components can freely generate samples of zT but the process is not invertible. The ability to infer
z0 can be beneficial for certain tasks. For example, if the dataset is known to have been generated by
a set of independent factors of variation, then it is possible that these factors will be recovered along
the axes of Z0 in an unsupervised manner, an outcome generally referred to as disentanglement.
Additionally, interpolation in Z0 is likely to yield more probable images than interpolation in ZT as
the distribution of zT is likely to be highly chaotic and multi-modal.
4	EXPERIMENTS ON VALUES OF β
We first train our proposed model on MNIST with a 2-dimensional latent code in order to demon-
strate visually the learned latent distributions. This is shown in Figure 1; while the autoencoder has
learned a latent distribution that is complex and multi-modal, the samples from the learned prior are
a close match.
Our hypothesis is that lowering β towards zero will enable more accurate reconstruction, while
learning the parameters of the prior will be sufficient to mitigate the divergence between qφ (z) and
pθ (z). If the hypothesis is correct, decreasing β can only ever be beneficial for sample quality,
therefore we experiment with various values of β between 0 and 1 to confirm that this is the case.
We carried out experiments on the MNIST, Fashion-MNIST and CIFAR-10 datasets. We chose
to adopt the FreChet Inception Distance (FID) Heusel et al. (2017) to measure image quality, a
common measure used in GAN evaluation, for our quantitative comparisons. FID scores are given
by the FreChet distance between layer activations of the Inception v3 network Szegedy et al. (2015),
with lower scores indicating greater similarity between two image sets.
Consistent with the hypothesis, FID scores typically decrease as β is decreased for all models as
can be seen in Figure 2. We report the average across 5 runs, and random seeds were kept fixed
between runs such that the only changing hyperparameter is β . Interpolations in latent space are
5
Under review as a conference paper at ICLR 2020
Figure 2: Experimental results using different values of β for different models. X-axis represents β,
not to scale. Y-axis represents FID score.
shown in Figure 3 in order to demonstrate that the model has learned a smooth manifold and is not
memorizing training images. We use spherical linear interpolation as suggested by White (2016).
We note that we intentionally used priors that were of sufficient expressiveness to learn the latent
distribution. For normalizing flows especially, this can result in a significant number of parameters
being added to the model. We acknowledge that in cases where a lightweight model is desired or
required such that a high capacity prior is not feasible, our results are not applicable, as latent regu-
larization may still be required in order to achieve a good fit between encoder and prior distributions.
For more details regarding the model architecture and training details, please see the appendix.
5	Experimental results on CelebA
5.1	Disentanglement
The term “disentanglement” can cover a broad range of definitions, but a generalized high-level
notion is that the model should capture individual factors of variation within linear subspaces of Z .
We hypothesize that when the underlying factors of variation of the dataset are not independent, our
model will learn a more disentangled representation in the distribution of zT than in the distribution
of z0. As posited by Karras et al. (2018b), the decoder is likely to pressure zT to take on a disen-
tangled form, since intuitively this should make accurate reconstruction easier as opposed to trying
to unwarp a highly entangled representation. In contrast, z0 is forced to take on a fixed distribution
that is highly unlikely to align linearly with the factors of variation. This line of reasoning is equally
applicable to standard VAEs that impose a fixed form prior.
The CelebA dataset contains 40 binary attributes that we can consider as such factors of variation,
and thus we can use these attributes to calculate an approximate measure of disentanglement. If
6
Under review as a conference paper at ICLR 2020
卜*4：EffFR打
▼▼中甘节中ft f工品33
Figure 3: Interpolation between samples from the CIFAR-10 test set for a flow prior with fixed γ
and β = 0. Leftmost and rightmost columns contain real images from the test set before encoding,
middle columns contain interpolations between them.
the model is successful at disentanglement, then for each attribute it should be easy to find a linear
hyperplane that partitions the latent encodings into two sets, with each side of the hyperplane corre-
sponding to one of the two possible values of the given attribute. We therefore consider the linear
separability score proposed in Karras et al. (2018b). In their work they first train a deep network
classifier on the training images that predicts image attributes, and then train a linear SVM classifier
that predicts the classifier network’s output given the latent variable. After this they calculate the
conditional entropy H(Y|X) where Y represents the labels predicted by the deep network classifier,
and X represents the labels predicted by the SVM. It can be seen that lower conditional entropy will
correspond to better linear separation, since the SVM will have higher prediction accuracy and thus
observing Y will give less information. By following their procedure exactly, we can quantitatively
measure disentanglement purely as a function of the generative process of the model.
We evaluated disentanglement of z0 and zT on three separate models: one with a learned flow prior,
a standard VAE, and β-VAE with β set to 25. Results are reported in Table 1. As expected, zT
has a more disentangled representation than z0 and, interestingly, also achieves a more disentangled
representation than either a standard VAE or β-VAE. While β-VAE achieves a slightly better linear
disentanglement than a standard VAE, it takes a heavy hit in FID score. In contrast, the model with
a flow prior achieves both improved disentanglement and improved reconstruction quality.
5.2	Interpolation
For the qualitative results in this section, we use a model trained using VGG19 perceptual features.
As pointed out in Hou et al. (2017), we can use the VGG19 network Simonyan & Zisserman (2015)
Figure 4: PCA of Eφ(x) for each x in the CelebA training set. The black line shows the path
taken when interpolating between two points inZT , while the red line shows the path taken when
interpolating between the same two points in Z0 . The black line takes the most direct route, while
the red line follows a path of higher average density.
7
Under review as a conference paper at ICLR 2020
Figure 5: Adding glasses to a face. Top row of each set: interpolation in Z0 . Bottom row of each
set: interpolation in ZT. Leftmost column: the original image before encoding. The top row shows
a more abrupt change towards the end, while the bottom row is closer to a constant rate of change.
Figure 6: Interpolation between two random samples from the training set. Top row of each set:
interpolation in Z0. Bottom row of each set: interpolation in ZT. Leftmost and rightmost columns:
the original images before encoding. The top row shows more realistic images as a result of the
interpolation path having higher average density.
to improve the appearance of reconstructions, making them sharper and more realistic. We therefore
apply VGG19 perceptual loss Johnson et al. (2016) using the relu_1_1, relu_2_1, relu_3_1
and relu_4_1 layers of the VGG19 network.
When interpolating between points in latent space the motivation is often to achieve some semantic
mixture between two images, orto change some semantic feature ofan image such as putting glasses
on a person’s face. Here we discuss potential differences between interpolating in Z0 or in ZT . As
a point of clarification, when we say we are interpolating between two images x(a) and x(b) in ZT
we are calculating Dψ (lerp(Eφ(x(a)), Eφ(x(b)); t)), and when we say we are interpolating in Z0
we are calculating Dψ (fθ-1 (slerp(fθ (Eφ(x(a) )), fθ (Eφ(x(b))); t))), where t varies between 0 and
1.
As demonstrated in the previous section, the warping of zT via fθ causes the representation to
become highly entangled. Therefore interpolation in Z0 is not appropriate when attempting to
change semantic features, as moving along the path of interpolation is likely to change many
different semantics with varying rates of change at each point on the path. Even if interpolation
manages to coincide with the direction of change of a single semantic feature, the rate of change
along the path will vary considerably if there exists class imbalance of the feature; this is due to the
fixed distribution of z0 . As an example, consider the case of a uniform distribution: the relative
amount of space occupied by points with a particular semantic feature would be proportional to
the class probability of the feature. What this means in practice is that if we were to generate a
sequence of images by interpolating in Z0 along the direction ofa semantic feature with heavy class
imbalance, there would be very little change for most of the sequence followed by an abrupt change
at the end. We posit that interpolating in ZT is more likely to achieve the ideal scenario of having a
constant rate of change, as it is not constrained by a fixed distribution. To demonstrate this, we first
calculate the mean of Eφ(x) for all data points labelled as having glasses, and the mean for all data
points without, and substract the latter from the former. We can then add this vector to the encoding
of an image without glasses, and interpolate between the original and new encodings. Results are
shown in Figure 5.
Interpolating between two different images, on the other hand, is best done in Z0 , as the interpolation
is more likely to occur along a path of higher average density, resulting in more probable images.
A visualization of the difference in paths taken can be seen in Figure 4. If the two images are very
different semantically, then interpolation in ZT can generate highly improbable images, see Figure
6.
6	Diversity in image-to-image translation
Achieving high sample diversity in multi-modal image-to-image translation tasks is often an explicit
goal Zhu et al. (2017); Huang et al. (2018c). When using conditional VAEs for image-to-image
translation tasks, the decoder is often able to learn a fairly accurate reconstruction based on the
8
Under review as a conference paper at ICLR 2020
	Separability	-FID
^VAE	~~1Λl	41.4
β-VAE	^^09	82.69
Flow prior (z0) Flow prior (ZT)	^^82 1.67	33.1
Table 1: Linear separability and FID scores after training on CelebA.
Figure 7: Conditional samples using Variational U-net. Top row: original. Bottom row: proposed.
Leftmost column contains the original image and the pose being conditioned on.
conditioned image alone, and so may ignore the latent code entirely if the KL divergence weight is
too strong. Even when methods such as annealing are used, the model may still only make limited
use of the latent code, severely affecting the diversity of generated samples. In order to quantitatively
test whether our proposed method is able improve diversity, we experiment with the Variational U-
net model proposed in Esser et al. (2018). In their work, they attempt to learn the distribution over
images of people conditioned on their pose. We modified their implementation such that the prior
distribution is learned via normalizing flow, and dropped the KL divergence term in the objective.
Their model conditions the prior distribution on the given pose such that their objective becomes
logp(x(i)|y(i),z) - DKL[q(z|x(i), y(i))||p(z|y(i))]	(9)
where y is the pose and x is the real image. For compatibility we therefore learn the mean of z0
conditioned on the pose and additionally condition each flow transformation ft on the pose. To mea-
sure diversity, we compute the LPIPS distance Zhang et al. (2018) between randomly sampled pairs
which were generated by conditioning on the images in the test set. We additionally calculated the
Inception score Salimans et al. (2016) of the samples to ensure image quality was not affected. Re-
sults comparing our modification with the original implementation after training on the DeepFashion
dataset are reported in Table 2. We also show samples in Figure 7.
7 Conclusion
We have proposed removing latent regularization from the objective in a latent variable models and
demonstrated that this is justified in the case where the prior is sufficiently expressive. We have also
demonstrated empirically that a learned prior results in better sample quality as measured by FID
9
Under review as a conference paper at ICLR 2020
	IS	LPIPS
VUNET+VAE	2.63	0.184
VUNET+Proposed	2.70	0.236
Table 2: Inception and LPIPS scores (higher is better) on the DeepFashion dataset after training
Variational U-net.
scores, better disentanglement as measured by linear separability, and greater conditional sample
diversity as measured by LPIPS. Our results indicate that fixed-form priors should be eschewed in
favour of learned priors with little to no latent regularization.
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein GAN. In International Confer-
ence on Machine Learning (ICML), 2017.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Ben-
gio. Generating sentences from a continuous space. In The SIGNLL Conference on Computational
Natural Language Learning (CoNLL), 2016.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. In International Conference on
Learning Representations (ICLR), 2017.
Bin Dai and David Wipf. Diagnosing and enhancing VAE models. In International Conference on
Learning Representations (ICLR), 2019.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components esti-
mation. In International Conference on Learning Representations (ICLR), 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In
International Conference on Learning Representations (ICLR), 2017.
Patrick Esser, Ekaterina Sutter, and Bjorn Ommer. A variational U-net for conditional appearance
and shape generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances
in Neural Information Processing Systems (NIPS), 2017.
Xianxu Hou, LinLin Shen, Ke Sun, and Guoping Qiu. Deep feature consistent variational autoen-
coder. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2017.
Chin-Wei Huang, Ahmed Touati, Laurent Dinh, Michal Drozdzal, Mohammad Havaei, Laurent
Charlin, and Aaron C. Courville. Learnable explicit density for continuous latent space and vari-
ational inference. In International Conference on Machine Learning (ICML) Workshops, 2017.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron C. Courville. Neural autoregressive
flows. In International Conference on Machine Learning (ICML), 2018a.
Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, and Aaron C. Courville. Improving explorability
in variational inference with annealed variational objectives. In Advances in Neural Information
Processing Systems (NIPS), 2018b.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-
image translation. In European Conference on Computer Vision (ECCV), 2018c.
10
Under review as a conference paper at ICLR 2020
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European Conference on Computer Vision (ECCV), 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. In International Conference on Learning Representations
(ICLR), 2018a.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. ArXiv:1812.04948, 2018b.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations (ICLR), 2014.
Diederik P. Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
autoregressive flow. In Advances in Neural Information Processing Systems (NIPS), 2016.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems (NIPS), 2018.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMD GAN:
towards deeper understanding of moment matching network. In Advances in Neural Information
Processing Systems (NIPS), 2017.
Yujia Li, Kevin Swersky, and Richard S. Zemel. Generative moment matching networks. In Inter-
national Conference on Machine Learning (ICML), 2015.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian J. Goodfellow. Adversarial autoen-
coders. In International Conference on Learning Representations (ICLR), 2016.
Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed,
and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational
framework. In International Conference on Learning Representations (ICLR), 2017.
Lars M. Mescheder. On the convergence properties of GAN training. ArXiv:1801.04406, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. In International Conference on Learning Representations (ICLR), 2017.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density
Estimation. In Advances in Neural Information Processing Systems (NIPS), 2017.
Danilo Jimenez Rezende and Fabio Viola. Taming VAEs. ArXiv:1810.00597, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in Neural Information Processing Systems
(NIPS), 2016.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. In International Conference on Learning Representations (ICLR), 2015.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2015.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein Auto-
Encoders. In International Conference on Learning Representations (ICLR), 2018.
Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Condi-
tional image generation with pixelcnn decoders. In Advances in Neural Information Processing
Systems (NIPS), 2016.
Tom White. Sampling generative networks: Notes on a few effective techniques. ArXiv:1609.04468,
2016.
11
Under review as a conference paper at ICLR 2020
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational
autoencoding models. ArXiv:1702.08658, 2017.
Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, and Eli
Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information
Processing Systems (NIPS), 2017.
A Appendix
12