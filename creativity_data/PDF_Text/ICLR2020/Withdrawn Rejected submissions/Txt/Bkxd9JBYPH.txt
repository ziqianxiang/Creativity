Under review as a conference paper at ICLR 2020
Representing Model Uncertainty of Neural Networks
in Sparse Information Form
Anonymous authors
Paper under double-blind review
Abstract
This paper presents a sparse representation of model uncertainty for deep neural
networks (DNNs) that relies on an inverse formulation of Multivariate Normal
Distribution (MND): an information form. We show that the model uncertainty can
be estimated in this form using a scalable Laplace Approximation scheme, which
involves a diagonal correction of the Kronecker-factored eigenbasis. As this makes
the inversion of the information matrix intractable - an operation that is required
for a full Bayesian analysis, we further devise a novel low-rank approximation of
this eigenbasis that exploits spectral sparsity of DNNs. Methods to realize this
sparsification are provided that develops into a memory-wise tractable sampling
computations. Both of our theoretical analysis and empirical evaluations over
various benchmarks show the superiority of our approach over existing methods.
1	Introduction
Whenever machine learning methods are used for safety-critical applications such as medical image
analysis or autonomous driving, it is crucial to provide a precise estimation of the failure probability
of the learned predictor. Therefore, most of the current learning approaches return distributions rather
than single, most-likely predictions. For example, DNNs trained for classification usually use the
softmax function to provide a distribution over predicted class labels. Unfortunately, this method
tends to severely underestimate the true failure probability, leading to overconfident predictions (Guo
et al., 2017). The main reason for this is that neural networks are typically trained with a principle of
maximum likelihood, neglecting their epistemic or model uncertainty with the point estimates.
A widely known work by Gal (2016) shows that this can be mitigated by using dropout at test time.
This so-called Monte-Carlo dropout (MC-dropout) has the advantage that it is relatively easy to use
and therefore very popular in practice. However, MC-dropout also has significant drawbacks. First, it
requires a specific stochastic regularization during training. This limits its use on already well trained
architectures, because current networks are often trained with other regularization techniques such
as batch normalization. Moreover, it uses a Bernoulli distribution to represent the complex model
uncertainty, which in return, leads to an underestimation of the predictive uncertainty.
Several strong alternatives exist without these drawbacks. Variational inference (Khan et al., 2018;
Kingma et al., 2015; Graves, 2011) and expectation propagation (Herandez-Lobato & Adams,
2015) are such examples. Yet, these methods use a diagonal covariance matrix which limits their
applicability as the model parameters are often highly correlated. Building upon these, Sun et al.
(2017); Louizos & Welling (2016); Zhang et al. (2018); Ritter et al. (2018a) show that the correlations
between the parameters can also be computed efficiently by decomposing the covariance matrix of
MND into Kronecker products of smaller matrices. However, not all matrices can be Kronecker
decomposed and thus, these simplifications usually induce crude approximations (Bae et al., 2018).
As the dimensionality of statistical manifolds are prohibitively too large in DNNs, more expressive,
efficient but still easy to use ways of representing such high dimensional distributions are required.
To tackle this challenge, we propose to represent the model uncertainty in sparse information form
of MND. As a first step, we devise a new Laplace Approximation (LA) for DNNs, in which we
improve the state-of-the-art Kronecker factored approximations of the Hessian (George et al., 2018) by
correcting the diagonal variance in parameter space. We show that these can be computed efficiently,
and that the information matrix of the resulting parameter posterior is more accurate in terms of the
Frobenius norm. In this way the model uncertainty is approximated in information form of the MND.
1
Under review as a conference paper at ICLR 2020
flatten
3136
O
0
(a) Problem Illustration FC
a
7x7x64
O(N幻
samples 因
■
∑3= ■
p(83 I ①，妨 ~ "(夕3, ∑3) N3 = 3211264
Figure 1: Main idea. (a) Covariance matrix Σ for DNNs is intractable to infer, store and sample (an
example taken from our MNIST experiments). (b) Our main insight is that the spectrum (eigenvalues)
of information matrix (inverse of covariance) tend to be sparse. (c) Exploiting this insight a Laplace
Approximation scheme is devised which applies a spectral sparsification (LRA) while keeping
the diagonals exact. With this formulation, the complexity becomes tractable for sampling while
producing more accurate estimates. Here, the diagonal elements (nodes in graphical interpretation)
corresponds to information content in a parameter whereas the corrections (links) are the off-diagonals.
As this results in intractable inverse operation for sampling, we further propose a novel low-rank
representation of the resulting Kronecker factorization, which paves the way to applications on large
network structures trained on realistically sized data sets. To realize such sparsification, we propose a
novel algorithm that enables a low-rank approximation of the Kronecker factored eigenvalue decom-
position, and we demonstrate an associated sampling computations. Our experiments demonstrate
that our approach is effective in providing more accurate uncertainty estimates and calibration on
considered benchmark data sets. A detailed theoretical analysis is also provided for further insights.
We summarize our main contributions below.
•	A novel Laplace Approximation scheme with a diagonal correction to the eigenvalue re-
scaled approximations of the Hessian, as a practical inference tool (section 2.2).
•	A novel low-rank representation of Kronecker factored eigendecomposition that preserves
Kronecker structure (section 2.3). This results in a sparse information form of MND.
•	A novel algorithm to enable a low rank approximation (LRA) for the given representation of
MND (algorithm 1) and derivation of a memory-wise tractable sampler (section B.2).
•	Both theoretical (section C) and experimental results (section 4) showing the applicability
of our approach. In our experiments, we showcase the state-of-the-art performance within
the class of Bayesian Neural Networks that are scalable and training-free.
To our knowledge we explore a sparse information form to represent the model uncertainty of DNNs
for the first time. Figure 1 depicts our main idea which we provide more rigorous formulation next.
2	Methodology
2.1	Background and Notation
We model a neural network as a parameterized function fθ : RN1 → RNl where θ ∈ RNθ are the
weights and Nθ = N1 + …+ Nl. This function fθ is in fact a concatenation of l layers, where each
layer i ∈ {1, ..., l} computes hi = Wiai-1 and ai = φ(hi-1). Here, φ is a nonlinear function, ai are
activations, hi linear pre-activations, and Wi are weight matrices. The bias terms are absorbed into
Wi by appending 1 to each ai. Thus, θ = vec(W1 )T vec(W2)T ... vec(Wl)T T where vec is the
operator that stacks the columns of a matrix to a vector. Let gi = δhi, the gradient of hi w.r.t θ.
Using LA the posterior is approximated with a Gaussian. The mean is then given by the MAP estimate
θMAP and the covariance by the Hessian of the log-likelihood (H + τI)-1 assuming a Gaussian prior
with precision τ. Using loss functions such as MSE or cross entropy and piece-wise linear activation ai
2
Under review as a conference paper at ICLR 2020
(e.g RELU), a good approximation of the Hessian is the Fisher information matrix (IM) I = E δθδθT
for the backpropagated gradients δθ 1 and is typically scaled by the number of data points N (Martens
& Grosse, 2015). IM is of size Nθ × Nθ resulting in too large matrix for moderately sized DNNs.
To make the computation tractable, a number of approximations are applied. First, it is assumed
that the weights across layers are uncorrelated, which corresponds to a block-diagonal form of I
with blocks I1 , I2 , . . . , Il . Then, each realisation of block Ii is represented as a Kronecker product
δθiδθT = ai-ι a3 的 gigT. This has an advantage that the inverse can be computed efficiently using
(A 凶 G)-1 = AT 的 G-1. Then, matrices Aɪ and Gi are assumed to be statistically independent:
Ii,kfac = E 卜i-1aL 国 gig；] ≈ E 卜i-1aL]国 E Igigf ] = Ai-1 ® Gi.	(I)
We refer to Martens & Grosse (2015) for details on KFAC. Here, Ai-1 ∈ Rni×ni and Gi ∈ Rmi×mi , where
the number of weights is Ni = nimi. Applying equation 1 to LA (Ritter et al., 2018a), the parameter
posterior per layer can be represented with a matrix normal distribution MN (defined in section A).
p(θi U, y) ~ NveC(Wi,MAP), Ii-fac) = MN(Wi,MAP, A-11, G-1).	(2)
Typically IM is scaled by the number of data points N and incorporates the Gaussian prior τ. The
herein presented parameter posterior omits the addition of prior precision and scaling term for
simplicity. Further note that, in practice, N and τ are treated as hyperparameters (Ritter et al., 2018a).
NIi + TI ≈ (√NAi-1 + √TI) ® (√NGi + √TI).	(3)
KFAC scales to big data sets such as ImageNet (Krizhevsky et al., 2012) with large DNNs (Ba et al.,
2017) and does not require changes in the training procedure when applied to LA.
2.2	Laplace Approximation with a diagonal correction
Following George et al. (2018), we first employ an eigenvalue correction in the Kronecker factored
eigenbasis for LA. For simplicity, we drop layer indices i and explanation herein applies layer-wise.
Let I = VtrueΛtrue VtTrue be the true eigendecomposition of IM per layer. From this it follows Λtrue =
EhVtTrueδθδθTVtrueiandΛtrue,ii = E∣(匕Tueδθ)2i for elements of layer wise matrices i ∈ {1,2, ∙∙∙ , N},
Defining the eigendecomposition of A and G in equation 1 as A = UAS AUAT and G = UGS GUGT, it
further follows IkfaC ≈ A Θ G = (UA Θ UG)(Sa Θ SG)(UA 0 UG)t from the properties of the Kronecker
product. Now, this approximation can be improved by replacing (Sa 0 Sg) with the eigenvalues Λtrue,
where Vtrue is set to (UA 0 UG) resulting in Λii = E [(UA 0 UG)T δθ]i2 . We denote it as EFB:
Iefb = (UA 0	UG)Λ(UA 0 UG)T	and	Ie-f1b	=(UA0UG)Λ-1(UA0UG)T.	(4)
This technique has many desirable properties. Notably, it holds kI - IefbkF ≤ kI - Ikfac kF wrt. the
Frobenius norm as the computation is more accurate by correcting the diagonal in the eigenbasis.
However, there is an approximation in EFB since (UA 0 UG) is still an approximation of the true
eigenbasis Vtrue. Intuitively, EFB only performs a correction of the diagonal elements in the eigenbasis,
but when mapping back to the parameter space this correction is again harmed by the inexact estimate
of the eigenvectors. Of course, an exact estimation of the eigenvectors is infeasible, but it is important
to note that the diagonals of the exact IM Iii = E δθi2 can be computed efficiently using back-
propagation. This and the fact that the off-diagonal elements of the IM are weaker with larger data
sets or when using weight normalization (Neyshabur et al. (2016); Desjardins et al. (2015); Salimans
& Kingma (2016)), motivates the idea to correct the approximation further as follows:
nm	__
Idef =	(Ua	0 UG)Λ(Ua	0	UG)T	+ D where	Dii	= E [δθ2]	- ɪʒ(vij m)2.	(5)
j=1
1The expectation herein is defined wrt. the paramerterized density Pθ(y|X) assuming i.i.d. samples x.
3
Under review as a conference paper at ICLR 2020
Λ'JJa Θ UgWUa ® Ug)t ≈ (UA1;a ¾lfl)Λhi(‰iα Θ UG1JT
Figure 2: Sparse Information Matrix. We perform a low rank approximation on Kronecker factored
eigendecomposition that preserves Kronecker structure in eigenvectors for two reasons: (a) reducing
directly (UA Θ UG)i：L is memory-wise infeasible, and (b) sampling scheme then only involves matrix
multiplications of smaller matrices UA1:a and UG1:g . Notations on indicing rules are also depicted.
In equation 5, we have represented (Ua® UG)Λ(UA® UG)T as P当也；j ʌ/Λj)2 where V = (UAΘ UG) ∈
Rmn×mn is a Kronecker product with row elements vi,j (see definition 1 below). It follows from the
properties of the Kronecker product that i = m(α - 1) + γ. The derivation is shown in section B. Note
that in this given form, the Kronecker products are never directly evaluated but the diagonal matrix D
can be computed recursively, making it computationally feasible.
Definition 1: For UA ∈ Rn×n and UG ∈ Rm×m, the Kronecker product of V = UA Θ UG ∈ Rmn×mn is
given by vi,j = Uaα,β Ubγ,ζ, with the indices i = m(α - 1) + γ and j = m(β - 1) + ζ. Here, the indices of
the matrices UA and UG are α ∈ {1, ∙∙∙ , n}, β ∈ {1, ∙∙∙ , n}, Y ∈ {1, ∙∙∙ , m} and Z ∈ {1, ∙∙∙ , m}.
Now, the parameter posterior distribution is represented in an information form or inverse formulation
N-1 of MND as shown in equation 6 which is parameterized by an information vector WMIVAP =
Id-e1fvec(WMAP) and matrix Idef from equation 5. Section A provides the formulation.
P(θ I X,y) ~ N(vec(Wmap), I-f) = N-1(W‰, (UA ⑥ UG)Λ(Ua ⑥ UG)t + D)	(6)
Unfortunately, in the current form, it involves a matrix inversion with size N by N when sampling.
For some layers in modern architectures, this is not be feasible. This problem is tackled next.
2.3	Representing model uncertainty in sparse information form
Sampling from the posterior is crucial. For example, an important use-case of the parameter posterior
is estimating the predict uncertainty for test data (X*,y*) by a full Bayesian analysis with Kmc samples
(equation 7). The herein approximation step is so-called Monte-carlo integration (Gal, 2016).
1	Kmc
p(y*Iχ*,χ,y) = J P(y*IX*,θ)p(θ∣X,y)dθ ≈ —ɪʒʃ*(χ*,θt) for θS 〜N-(w‰,Idef)	(7)
However, directly sampling from equation 6 is non-trivial as explained in an example below.
Example 1: Consider the architecture from figure 1 where the covariance matriX Σ3 ∈ RN3 ×N3
for N3 = 3211264. With equation 6, the sampling requires O(N33) compleXity (the cost of inver-
sion and finding a symmetrical factor) and obviously, this operation is computationally infeasible.
Consequently, we neXt describe a sparse formulation of equation 6 that ensures tractability.
To tackle this challenge, We propose the low rank form in equation 82 as a first step. Here,八上L ∈ RL×L,
UA1:a ∈ Rm×a and UG1:g ∈ Rn×g denote low rank form of corresponding eigenvalues and vectors
(depicted in figure 2). Naturally, it follows that L = ag, N = mn and furthermore, the persevered rank
L corresponds to preserving top K and additional J eigenvalues (resulting in L ≥ K, L = ag = K + J).
Idef ≈ Idef = (UAi：a ⑥ UGi：gn1:L(UAi：a ⑥ UGi：g) + D	⑻
2Note that the term D is added after LRA where D is computed similar to equation 5.
4
Under review as a conference paper at ICLR 2020
Figure 3: Illustration of algorithm 1. A low rank approximation on Kronecker factored eigende-
composition that preserves Kronecker structure in eigenvectors constitutes steps 1 to 5.
Note the difference to preserving top L eigenvalues and corresponding eigenvectors (Bishop, 2006) for
LRA. In our case, this results in intractable (UA Θ UG)i：L which defies the purpose. Therefore, as seen
in equation 8, the Kronecker structure in eigenvectors as (UA「a 凶 UG「^) is preserved. Consequently,
due to the Kronecker product operation, preserving top K eigenvalues results in L = K + J eigenvalues.
Example 2: Let matrix E decomposed as E =	U上6人上6 UT^^	∈	R6×6 with	Ui：6	=	hu 1	U2	…	U6∣	∈
R6×6 and Ai：6 = diag(Λi , Λ?,…，人6) ∈ R6×6 in a descending order. In this toy example, the LRA
with top 3 eigenvalues result in Ei：3 = U上3人上3 U；飞 ∈ R6×6 (see notation to above). Instead, consider
now the matrix Ekron = ( Uai：3 ⑥ UG「2)Ai：6(UA口 Θ UG0)T ∈ R6×6. Again, say we want to preserve top 3
the eigenvalues 八上3 and corresponding eigenvectors (Uai：3 Θ UG「2)i：3, However, as (UA「a Θ UG「g)i：3 =
UAA1 ⑥ UG1 uai Θ UG2 ua2 Θ UG J, PreServing the eigenvectors with the Kronecker structure results
in having to store UA1：2 = uA1 uA2 and UG1：2 = uG1 uG2 . Consequently, additional eigenvalue
Λ4 has to be saved in order to fulfill the definition of a Kronecker product Ekron1：3
(Uai：2 ⑥
Ugi：2)Ai：4(Uai：2 ⑥ UgQT ∈ R6×6. In summary, preserving top K eigenvalues results in other J
eigenvalUes, which ensUres the memory-wise tractability when performing LRA on large matrices.
Then, how do we compute a low rank approximation that preserves Kronecker structures in eigenvec-
tors? For this computation we propose algorithm 1 as an algorithmic contribution (also illustrated in
figure 3). Let us start with a definition on indexing rules of Kronecker factored diagonal matrices.
Definition 2: For diagonal matrices SA ∈ Rn×n and S G ∈ Rm×m, the Kronecker prodUct of Λ =
Sa Θ Sg ∈ Rmn×mn is given by Λi = SOes〉z, where the indices i = m(β - 1) + ζ withβ ∈ {1,…，m}
and ζ ∈ {1, ∙∙∙ , n}. Then, given i and m, β = int(mm) + 1 and given β, m, and i, ζ = i - m(β - 1). Here,
int(∙) is an operator that maps its inpUt to lower nUmber integer.
Notations in algorithm 1 are also depicted in figure 2. Now we explain with a toy example below.
Example 3: For explaining algorithm 1, the toy example can be revisited. Firstly, as we preserve
top 3 eigenvalUes, i ∈ {1,2,3} which are indices OfeigenVaIUeS 八上3 (line 1). Then, Using line 2,
β ∈ {1, 2} and ζ ∈ {1, 2} can be compUted Using definition 2. This relation holds as Λ is compUted
from Sa Θ Sg, and thUs, UA and UG are their corresponding eigenvectors respectively. In line 3, we
keep UA1：2 and UG1：2 Using β and ζ. Again, in order to fUlfill the Kronecker prodUct operation, we Use
line 4 to find the eigenVaIUeS j ∈ {1,2,3,4}, and then preserve A1』.As explained, this has resUlted in
saving top 3 and additional 1 eigenvalUes. Algorithm 1 provides the generalization of this and even if
eigendecomposition does not come with a descending order, the same logic trivially applies.
The incorporation of prior or regularization terms also follows without any additional approximation.
NIdef + TI = (Ua 1：a 区 Ug,g)(NAI：L)(04.区 UGg)t + (ND + TI)	(9)
Sampling: A key benefit of the proposed LRA is that now, sampling from the given covariance
(equation 6 with the low rank form in equation 8; equation 9 with an incorporation of priors) only
involves the inversion of a L × L matrix (in offline settings) and matrix multiplications of smaller
Kronecker factored matrices or diagonal matrices during a full Bayesian analysis. To this end, we
derive the analytical form of the sampler in section B.2 which makes the sampling computations
feasible. This enables us to bound the intractable complexity of O(N3) to O(L3) for L << N.
5
Under review as a conference paper at ICLR 2020
Algorithm 1: Sparsification
Input: Matrices UA, UG, Λ and Rank K.
Output： Matrices： UAι,a, Ugι,g, Λl.
Algorithm:
1.	Find indices of top K eigenvalues on Λ. This results
in indices i ∈ {1,…，K}.
2.	For each elements of i, find corresponding indices of
each Kronecker factors of original matrix Sa θ Sg by
using definition 2: β = int(m) + 1 and Z = i - m(β - 1).
This results in indices of β and Z for UA and UG
corresponding to Sa 0 SG or 八LK 二 Λi.
3.	Using obtained indices β and Z, compute
eigenvectors UA「a = UAβ and Ugi：g = UGZ. These are
the preserved eigenvectors in equation 8.
4.	Find indices of top K and additional J eigenvalues
using j = m(β - 1) + Z for all β andz.
5.	Preserve eigenvalues ΛiL = Λj where j ⊆ j. Ai：L
represents the preserved top K and additional J
eigenvalues in equation 8.
Algorithm 2: Inference of IM
Input： Pre-trained Neural Network and train data.
Output: Matrices: UA, UGi：M, Ai：L and D.
Algorithm：
for the given data points do KFAC
for i ：= 1 to l do
I Compute Ai and Gi (equation 1).
end
end
Compute UA, UG with eigenvalue decomposition.
for the given data points do EFB
for i ：= 1 to l do
Compute E 愀2 ].
Compute Aii with UA and UG .
end
end
for all the layers do DEF (without involving data)
I Compute UA, Uq：M, Λ∖∙L (algorithm 1).
I Compute D (equation 5).
end
Overview: An overview is depicted in figure 1 where we first show that IM of DNNs tend to be
sparse in its spectrum (similar to the findings of Sagun et al. (2018)). With this insight we propose to
represent the parameter posterior in a sparse information form which is visualized with its graphical
interpretations. From IM of EFB, we apply our LRA that weakens the strengths of weak nodes
(diagonals of IM) and links (off-diagonals) in a preserving fashion. Then, a diagonal correction can
be added to keep the information of each nodes exact. A key benefit is that the sampling computations
can be achieved in a memory-wise feasible way. Algorithm 2 shows the overall procedures. Further
note that, as IM is estimated after training, our method can be applied to existing architectures. EFB
is also computed in a different way to George et al. (2018) so that our EFB does not require batch
assumption for taking expectations, and the scheme is cheaper since eigenvalue decomposition of
Ai-1 and Gi are computed only once. Computing diagonal correction term also does not involve data.
P(θ U,y) ~ N-1(畲IVap, (UA“a 0 UG 1g)A1：L(UA“a 0 UGIg)T + D)	(10)
As a result our approach yields a sparse information form of MND where the IM has a low rank
eigendecomposition plus diagonal structure that preserves Kronecker structure in eigenvectors (shown
above; prior and scaling terms are omitted to keep the notation uncluttered; ^W1IVλp is an information
vector associated to the proposed IM). Since this formulation of model uncertainty has not bee studied
before, we provide theoretical results in section C for further insights and justifications.
3	Related Works
Sparse Information Filters: A similar idea of sparsifying the information matrix while keeping the
diagonals accurate can be found in sparse information filters. Here, Bayesian tracking is realized
in information form of MND instead of canonical counterparts (Kalman Filters). As this leads to
inefficiency in marginalization, sparsity is introduced while keeping the diagonals accurate (Thrun
et al., 2004). A main difference, however, is that DNNs typically have higher dimensions and a sparse
structure in the spectrum (eigenvalues) in contrast to spaces of parameters in SLAM problems. Thus,
we propose to explore Kronecker factorization and induce spectral sparsity or LRA respectively.
Approximation of the Hessian: The Hessian of DNNs is prohibitively too large as its size is
quadratic to the parameter space. For this problem an efficient approximation is a layer-wise
Kronecker factorization (Martens & Grosse, 2015; Botev et al., 2017) which have demonstrated a
notable scalability (Ba et al., 2017). In a recent extension of (George et al., 2018) the eigenvalues of
the Kronecker factored matrices are re-scaled so that the diagonal variance in its eigenbasis is exact.
The work demonstrates a provable method of achieving higher accuracy. Yet, as this is harmed by
inaccurate estimates of eigenvectors, we further correct the diagonals in the parameter space.
6
Under review as a conference paper at ICLR 2020
Diag Laplace	KFAC Laplace	FB Laplace	Ours (DEF)
BBB	EFB Laplace	DEF with L=1	HMC (ground truth)
Figure 4: Uncertainty on toy regression. The black dots and the black lines are data points (x, y).
The red and blue lines show predictions of the deterministic Neural Network and the mean output
respectively. Upto three standard deviations are shown with blue shades.
Laplace Approximation: Instead of methods rooted in variational inference (Hinton & van Camp,
1993) and sampling (Neal, 1996), we build upon LA (MacKay, 1992) as a practical inference
framework. Recently, diagonal (Becker & Lecun, 1989) and Kronecker-factored approximations
(Botev et al., 2017) to the Hessian have been applied to LA by Ritter et al. (2018a). The authors have
further proposed to use LA in continual learning (Ritter et al., 2018b), and demonstrate a competitive
results by significantly outperforming its benchmarks (Kirkpatrick et al., 2017; Zenke et al., 2017).
Building upon Ritter et al. (2018a) for approximate inference, we propose to use more expressive
posterior distribution than matrix normal distribution. In the context of variational inference, SLANG
(Mishkin et al., 2018) share similar spirit to ours in using a low-rank plus diagonal form of covariance
where the authors show the benefits of low-rank approximation in detail. Yet, SLANG is different to
ours as they do not explore Kronecker structures and requires changes in the training procedure.
Dimensionality Reduction: A vast literature is available for dimensionality reduction beyond
principal component analysis (Wold et al., 1987) and singular value decomposition (Golub & Reinsch,
1971; Van Der Maaten et al., 2009). To our knowledge though, dimensionality reduction in Kronecker
factored eigendecomposition that maintains Kronecker structure of eigenvectors has not been studied
before. Thus, we propose algorithm 1 and further provide its theoretical properties in section C.
4	Experimental Results
An empirical study is presented with a toy regression and classification tasks across MNIST (Lecun
et al., 1998), notMNIST (Bulatov, 2011), CIFAR10 (Krizhevsky, 2009) and SHVN (Netzer et al.,
2011) data-sets. The experiments are designed to demonstrate the quality of predictive uncertainty,
effects of varying LRA, the quality of approximate Hessian, and gains in reduction of computational
complexity due to LRA. All experiments are implemented using Tensorflow (Abadi et al., 2016).
4.1	Toy Regression and Effects of Low Rank Approximation
Predictive Uncertainty: Firstly, an evaluation on toy regression data-set is presented. This experi-
ment has an advantage that we can not only evaluate the quality of predictive uncertainty, but also
directly compare various approximations to the Hessian. For this a single-layered fully connected
7
Under review as a conference paper at ICLR 2020
(a) Diagonal error
dimension [%]
(b) Off-diagonal error
dimension [%] eigenvalues [-]
(c) Overall error
(d) Eigenvalues
Figure 5: Effects of Low Rank Approximation in Frobenius norm of error. This measure is
normalized from 0 to 1. Lower the better. Laplace based methods such as EFB, Diag, KFAC and
DEF are compared in terms of diagonal, off-diagonal and overall resulting approximation error to
exact block diagonal hessian. Eigenvalue histogram is also plotted.
network with seven units in the first layer is considered. We have used 100 uniformly distributed
points X 〜 U(-4,4) and samples y 〜N(X3,32). Visualization of predictive uncertainty is shown in
figure 4. HMC (Neal, 1996), BBB (Blundell et al., 2015), diagonal and KFAC Laplace Ritter et al.
(2018a) have been included as a comparison whereas EFB Laplace, exact block diagonal Hessian
(FB) and DEF with a full rank and one rank are presented for an ablation study. Both Diag and KFAC
Laplace are tuned similar to Ritter et al. (2018a) by regularizing them. DEF variants and FB Laplace
for this experiment did not require hyper-parameter tuning. Implementation details are in section E.
All the methods show higher uncertainty in the regimes far away from training data where BBB
showing the most difference to HMC. Furthermore, Diag, KFAC and EFB Laplace predicts rather
high uncertainty even within the regions that are covered by the training data. DEF variants slightly
underestimate the uncertainty but produces the most comparable fit to the FB Laplace and HMC
(our ground truths). We believe this is the direct effect of modelling the Hessian more accurately 3.
Moreover, since the only difference between EFB and DEF Laplace is a diagonal correction term, this
empirical results suggest that keeping diagonals of IM exact results in accurate predictive uncertainty.
Effects of Low Rank Approximation: Next, we quantitatively study the effects of LRA by directly
evaluating on the approximations of IM. This is because uncertainty estimation, despite being a
crucial entity, are confounded from the problem itself and may not reveal the algorithmic insights to
its full potential. For this, we revisit the toy regression problem and provide a direct evaluation of IM
with measure on normalized Frobenius norm of error errNF in the first layer of the network.
The results are shown in figure 5. Here, the reduced dimension is not proportional to the ranks (e.g.
many zero or close to eigenvalues). Figure 5 (a) depicts that DEF results in accurate estimates on Iii
regardless of the chosen dimensions L while EFB has the more approximation error, which we believe
is due to inaccurate estimates of eigenvectors. KFAC on the other hand, produces the most errors
on diagonal elements, which indicate that its assumption of Kronecker factorization induces crude
approximation in this experiment. Regarding the off-diagonal errors EFB also outperforms KFAC and
Diag estimates. Furthermore, error profile of off-diagonal error Iij also explains the principles of the
LRA that as we decrease the ranks, the error increases but in a preserving manner. These results can
also be explained by Lemma 1 and 4 of section C which reflects the design principles of the method.
4.2	Classification and Reduction in Complexity
Predictive Uncertainty: Next, we evaluate predictive uncertainty on classification tasks in which
the proposed low-rank representation is strictly necessary. Furthermore, our goal is not to achieve
the highest accuracy but evaluate predictive uncertainty. To this end, we choose classification tasks
with known and unknown classes, e.g. a network is not only trained and evaluated on MNIST but
also tested using notMNIST. Note that under such tests, any probabilistic methods should report their
evaluations on both known and unknown classes with the same hyperparameter settings. This is
because a Bayesian Neural Network to be always highly uncertain, which may seem to work well on
3we comment on this statement, and the effects of data-set size to number of parameters in section E.
8
Under review as a conference paper at ICLR 2020
out-of-distribution samples but are always overestimating uncertainty, even for the correctly classified
samples within the distribution similar to the train data. For evaluating predictive uncertainty on
known classes, Expectation Calibration Error (ECE) has been used. As we found it more intuitive,
normalized entropy is reported for evaluating predictive uncertainty on unknown classes.
Table 1: Results of classification experiments. Accuracy and ECE are evaluated on in-domain
distribution (MNIST and CIFAR10) whereas entropy is evaluated on out-of-distribution (notMNIST
and SHVN). Lower the better for ECE. Higher the better for entropy and accuracy.
MNIST	NN	Diag	KFAC	MC-dropout	Ensemble	EFB	DEF
Accuracy	0.993	0.9935	0.9929	0.9929	0.9937	0.9929	0.9927
ECE	0.395	0.0075	0.0078	0.0105	0.0635	0.012	0.0069
Entropy	0.055±0.133	0.555 ± 0.196	0.599 ± 0.199	0.562 ± 0.19	0.596 ± 0.133	0.618 ± 0.185	0.635 ± 0.19
CIFAR	NN	Diag	KFAC	MC-dropout	Ensemble	EFB	DEF
Accuracy	0.8606	08659	0.8572	N/A	0.8651	0.8638	0.8646
ECE	0.0819	0.0358	0.0351	N/A	0.0809	0.0343	0.0084
Entropy	0.245 ± 0.215	0.4129 ± 0.197	0.408 ± 0.197	N/A	0.370 ± 0.192	0.417 ± 0.196	0.4338 ± 0.18
Figure 6: Normalized Entropy histogram (left:
DEF Laplace and right: deterministic) on MNIST
vs notMNIST experiments. Our method clearly
separates the out-of-distribution and wrongly clas-
sified samples (out-dist) to the correctly classified
samples from in-domain distribution (in-dist).
MNIST	DimN [-]	DimL [-]	Percent [%]
CNN-1	800	450	5625
CNN-2	51200	5185	10.12
FC-1	3211264	5625	0.18
FC-2	10240	4775	46.63
CIFAR	DimN [-]	DimL [-]	Percent [%]
CNN-1	-4800	-4800-	100
CNN-2	102400	2112	2.06
FC-1	884736	3980	0.45
FC-2	73728	5499	7.45
FC-3	1920	1920	100
Table 2: Reduction in complexity. Reduced di-
mensions from N to the chosen rank L per layer
are reported for both MNIST and CIFAR experi-
ments. CNN stand for convolution while FC is for
fully connected layers. The complexity of sam-
pling computations O(N3) are reduced to O(L3).
On MNIST-notMNIST experiments, we compare to MC-dropout (Gal, 2016), ensemble (Lakshmi-
narayanan et al., 2017) of size 15, Diag and KFAC Laplace (Ritter et al., 2018a). These methods are
state-of-the-art baselines that have a merit of requiring no changes in the training procedure. The later
is crucial for a fair comparison as we can use the same experiment settings (Mukhoti et al., 2018).
Regarding the architectures, LeNet with RELU and a L2 coefficient of 1e-8 has been the choice. In
particular, this typically makes a neural network overconfident, and we can see the effects of model
uncertainty. This architecture validates our claim as it has the parameters of size θ3 ∈ R3137×1024 in the
3rd layer. Obviously, its covariance is intractable as it is quadratic in size (see figure 1). The results
can be found in table 1. Firstly all the methods improved significantly over the deterministic one
(denoted NN). Furthermore, DEF Laplace achieved here the lowest ECE, at the same time, predicted
with the highest mean entropy on out-of-distribution samples. Figure 6 shows this result where our
method separates between wrong and correct predictions which stems from the domain change.
Further tests were performed on CIFAR10 (known) and SVHN (unknown) to see the generalization
under batch normalization and data augmentation. For this, we trained a 5 layer architecture with 2
CNN and 3 FC layers. The results are also reported in table 1. Similar to MNIST experiments, our
method resulted in a better calibration performance and out-of-distribution detection overall. Note
that for Diag, KFAC and EFB Laplace, grid searches on hyperparameters were rather non-trivial here.
Increasing τI had the tendency to reduce ECE on CIFAR10, but in return resulted in underestimating
the uncertainty on SVHN and vice versa. DEF Laplace instead, required smallest regularization
hyperparameters to strike a good balance between these two objectives. We omitted dropout as using
it as a stochastic regularization instead of batch normalization would result in a different network and
thus, comparison would be not meaningful. More implementation details are provided in section E.
Reduction in Complexity: The proposed LRA has been imposed as a means to tackle the challenges
of computational intractability of MND. To empirically access the reduction in complexity, we depict
the parameter and low rank dimensions N and L respectively in table 2. As demonstrated, our LRA
9
Under review as a conference paper at ICLR 2020
based sampling computations reduce the computational complexity significantly. Furthermore, this
explains the necessity of LRA - certain layers (e.g. FC-1 of both MNIST and CIFAR experiments)
are computationally intractable to store, infer and sample. As a result, we demonstrate an alternative
representation for DNNS without resorting to fully factorized and matrix normal distribution.
Discussion and Limitations: Importantly, we demonstrate that when projected to different success
criteria, no inference methods largely win uniformly. Yet these experiments also show empirical
evidence that our method works in principle and compares well to the state-of-the-art. Representing
layer-wise MND in a sparse information form, and demonstrating a low rank inverse/sampling com-
putations, we show an alternative approach of designing scalable and practical inference framework.
Finally, these results also indicate that keeping the diagonals of IM accurate while sparsifying the
off-diagonals can lead to outstanding performance in terms of predictive uncertainty and generalizes
well to various data, models and even measures. For future works, we share the view that comparing
different approximations to the true posterior is quite challenging for DNNs. Consequently, better
metrics and benchmarks that show the benefits of model uncertainty can be an important direction.
On the other hand, we also address a key limitation of our work which stems from two hypothesis:
(a) when represented in information form, the spectrum of IM should be sparse, and (b) keeping
the diagonals exact while sparsifying the off-diagonals should result in a better estimates of model
uncertainty (equivalently keeping the information content of a node exact while sparsifying the weak
links between the nodes from a graphical interpretation of information matrix). While empirical
evidence from prior works (Sagun et al., 2018; Thrun et al., 2004; Bailey & Durrant-Whyte, 2006)
along with our experiments validate these to some extent, there exists no theoretic guarantees to our
knowledge. Consequently, theoretical studies that connect information geometry (Amari, 2016) of
DNNs and Bayesian Neural Networks can be an exciting venue of future research. Nevertheless,
similar to sparse Gaussian Processes (Snelson & Ghahramani, 2006), we believe our work can be a
stepping stone for sparse Bayesian Neural Networks that goes beyond approximate inference alone.
5	Conclusion
We address an effective approach of representing model uncertainty in deep neural networks using
Multivariate Normal Distribution, which has been thought computationally intractable so far. This
is achieved by designing its novel sparse information form. With one of the most expressive
representation of model uncertainty in current Bayesian deep learning literature, we show that
uncertainty can be estimated more accurately than existing methods. For future works, we plan to
demonstrate a real world application of this approach, pushing beyond the validity of concepts.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete
Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale
machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation
(OSD116),pp. 265-283, 2016.
Shun-ichi Amari. Information Geometry and Its Applications. Springer Publishing Company,
Incorporated, 1st edition, 2016. ISBN 4431559779, 9784431559771.
Sivaram Ambikasaran and Michael O’Neil. Fast symmetric factorization of hierarchical matrices
with applications. CoRR, abs/1405.0223, 2014.
Jimmy Ba, Roger B. Grosse, and James Martens. Distributed second-order optimization using
kronecker-factored approximations. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.
Juhan Bae, Guodong Zhang, and Roger Grosse. Eigenvalue corrected noisy natural gradient. CoRR,
abs/1811.12565, 2018.
Tim Bailey and Hugh Durrant-Whyte. Simultaneous localization and mapping (slam): Part ii. IEEE
robotics & automation magazine, 13(3):108-117, 2006.
10
Under review as a conference paper at ICLR 2020
S. Becker and Yann Lecun. Improving the convergence of back-propagation learning with second-
order methods. In D. Touretzky, G. Hinton, and T. Sejnowski (eds.), Proceedings of the 1988
Connectionist Models Summer School, San Mateo, pp. 29-37. Morgan Kaufmann, 1989.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and
Statistics). Springer-Verlag, Berlin, Heidelberg, 2006. ISBN 0387310738.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. In Proceedings of the 32Nd International Conference on Machine Learning -
Volume 37, ICML'15, pp. 1613-1622. JMLR.org, 2015.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for
deep learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
557-565, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available:
http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2, 2011.
Sheng-Wei Chen, Chun-Nan Chou, and Edward Y. Chang. Bda-pch: Block-diagonal approximation
of positive-curvature hessian for training neural networks. CoRR, abs/1802.06502, 2018.
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and koray kavukcuoglu. Natural Neural
Networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 28, pp. 2071-2079. Curran Associates, Inc., 2015.
Yarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.
Thomas George, CeSar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a kronecker factored eigenbasis. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeUrIPS 2018, 3-8 December 2018, Montreal, Canada.,pp. 9573-9583, 2018.
Gene H Golub and Christian Reinsch. Singular value decomposition and least squares solutions. In
LinearAlgebra, pp. 134-151. Springer, 1971.
Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. S. Zemel,
P. L. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances in Neural Information Processing
Systems 24, pp. 2348-2356. Curran Associates, Inc., 2011.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1321-1330, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
A.K. Gupta and D.K. Nagar. Matrix Variate Distributions. Monographs and Surveys in Pure and
Applied Mathematics. Taylor & Francis, 1999. ISBN 9781584880462.
Jose Miguel Herandez-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable
learning of bayesian neural networks. In Proceedings of the 32Nd International Conference on
International Conference on Machine Learning - Volume 37, ICML,15, pp. 1861-1869. JMLR.org,
2015.
Nicholas J. Higham. Computing a nearest symmetric positive semidefinite matrix. Linear Algebra
and its Applications, 103:103-118, 1988. ISSN 0024-3795. doi: 10.1016/0024-3795(88)90223-6.
Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the Sixth Annual Conference on Computational
Learning Theory, COLT ’93, pp. 5-13, New York, NY, USA, 1993. ACM. ISBN 0-89791-611-5.
Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava.
Fast and scalable bayesian deep learning by weight-perturbation in adam. In Proceedings of the
35th International Conference on Machine Learning, ICML2018, Stockholmsmdssan, Stockholm,
Sweden, July 10-15, 2018, pp. 2616-2625, 2018.
11
Under review as a conference paper at ICLR 2020
Durk P Kingma, Tim Salimans, and Max Welling. Variational Dropout and the Local Reparame-
terization Trick. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 28, pp. 2575-2583. Curran Associates, Inc.,
2015.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences of the United
States of America, 114 13:3521-3526, 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Canadian
Institute for Advanced Research, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems 25: 26th
Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting
held December 3-6, 2012, Lake Tahoe, Nevada, United States., pp. 1106-1114, 2012.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pp. 6402-6413, 2017.
Yann Lecun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.
Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix
gaussian posteriors. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 1708-1716, New York, New York, USA, 20-22 Jun 2016. PMLR.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
2218-2227, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
David J. C. MacKay. A practical bayesian framework for backpropagation networks. Neural
Computation, 4(3):448-472, 1992.
James Martens and Roger B. Grosse. Optimizing neural networks with kronecker-factored approxi-
mate curvature. In Proceedings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, pp. 2408-2417, 2015.
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark W. Schmidt, and Mohammad Emtiyaz
Khan. SLANG: fast structured covariance approximations for bayesian deep learning with natural
gradient. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada., pp.
6248-6258, 2018.
Jishnu Mukhoti, Pontus Stenetorp, and Yarin Gal. On the importance of strong baselines in bayesian
deep learning. CoRR, abs/1811.09385, 2018.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg,
1996. ISBN 0387947248.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011.
Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, and Nati Srebro. Path-normalized opti-
mization of recurrent neural networks with relu activations. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016,
December 5-10, 2016, Barcelona, Spain, pp. 3477-3485, 2016.
12
Under review as a conference paper at ICLR 2020
Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for flexible and accelerated
probabilistic programming in numpyro. In NeurIPS Workshop on Program Transformations 2019
(to appear), 2019.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural
networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018a.
Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for
overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December
2018, Montreal, Canada. ,pp. 3742-3752, 2018b.
Levent Sagun, UtkU Evci, V. UgUr Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of
the hessian of over-parametrized neural networks. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track
Proceedings, 2018.
Tim Salimans and Durk P Kingma. Weight Normalization: A Simple Reparameterization to Acceler-
ate Training of Deep Neural Networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 901-909. Curran
Associates, Inc., 2016.
Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In Y. Weiss,
B. Scholkopf, and J. C. Platt (eds.), Advances in Neural Information Processing Systems 18, pp.
1257-1264. MIT Press, 2006.
Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning Structured Weight Uncertainty
in Bayesian Neural Networks. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th
International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of
Machine Learning Research, pp. 1283-1292, Fort Lauderdale, FL, USA, 20-22 Apr 2017. PMLR.
Sebastian Thrun, Yufeng Liu, Daphne Koller, Andrew Y Ng, Zoubin Ghahramani, and Hugh Durrant-
Whyte. Simultaneous localization and mapping with sparse extended information filters. The
international journal of robotics research, 23(7-8):693-716, 2004.
Laurens Van Der Maaten, Eric Postma, and Jaap Van den Herik. Dimensionality reduction: a
comparative. Journal of Machine Learning Research, 10(66-71):13, 2009.
Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and
intelligent laboratory systems, 2(1-3):37-52, 1987.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3987-3995,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Guodong Zhang, Shengyang Sun, David K. Duvenaud, and Roger B. Grosse. Noisy natural gradient
as variational inference. In Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018,pp. 5847-5856, 2018.
13
Under review as a conference paper at ICLR 2020
A Notations on Distributions
A.1 Matrix Normal Distribution
The matrix normal distribution is a probability density function for the random variable X ∈ Rn×m in
matrix form. It can be parameterized with mean WMAP ∈ Rn×m , scales UA ∈ Rn×n and UG ∈ Rm×m . It
is essentially a multivariate Gaussian distribution with mean vec(WMAP) and covariance U Θ V.
p(X|WMAP, UA, UG) =
exp (-2tr [u-1(X - WMAP)rU-1(X - Wmap)])
(11)
(2π) nm∖UG∖n∖UA∖m
In section B, we denote this distribution with MN parameterized by WMAP, UA and UG so that
p(X∖WMAP, UA, UG) = MN(WMAP, UA, UG). Here, tr stands for trace operation and we omitted layer
indicing i for better clarity. Refer to Gupta & Nagar (1999) for more details.
A.2 Information Form of Multivariate Normal Distribution
Information form of Multivariate Normal Distribution (MND) is a dual representation for the well
known canonical form. Lets denote 元=vec(X) ∈ Rnm, μ = vec(WMAP)∈ Rnm and Σ = I-1 ∈ Rmn×nm
as a random variable, mean and covariance respectively for N = mn.
P(X∖μ, Σ) χ exp
(-2 ,x T Σ-1 X + μ T Σ-1μ)
(12)
Then, equation 12 defines the canonical form. Now we denote its Information form in equation 13.
p(X∖b, F) 8 exp
(-权 T Ix+
WMAPχ}
(13)
Here, X ∈ Rmn represent the random variable as well. WIMAP = ∑-1μ ∈ Rmn and I = Σ-1 ∈ Rmn×mn are
information vector (denoted IV in the main text with superscript) and matrix respectively. We denote
the information form as N -1 which is completely described by an information vector and matrix.
Information matrix is also widely known as precision matrix. Thrun et al. (2004) in Simultaneous
Localization and Mapping (SLAM) literature provides a good overview and explanations.
B	Derivations
B.1	Derivation 1: Diagonal correction without evaluating the Kronecker products
Directly evaluating UA 的 UG may not be computationally feasible for modern DNNs. Therefore, we
derive the analytical form of the diagonal elements for (UA 的 UG)Λ( UA 凶 UG)T without having to
fully evaluate it. Let UA ∈ Rn×n and UG ∈ Rm×m be the square matrices. Λ ∈ Rmn×mn is a diagonal
matrix by construction. V = UA 的 UG ∈ Rmn×mn is a Kronecker product with elements Vi,j with
i = m(α - 1) + γ and j = m(β - 1) + ζ (from definition of Kronecker product). Then, the diagonal
entries of (UA 的 UG)Λ(UA 凶 UG)T can be computed as follows:
nm	___
h(Ua ® UgZA ® UG)T]ii = X(vi∙,j 飙)2	(14)
j=1
Derivation: As a first step of the derivation, we express (A Θ B)Λ(A Θ B)T in the following form:
(Ua 凶 Ug)Λ(Ua 0 UG)T = (UA 凶 UG)Λ2Λ2(Ua 凶 UG)T
=h(Ua 脸 Ug)Λ2] [(Ua 脸 UG)Λ2]T	(15)
= UUT
14
Under review as a conference paper at ICLR 2020
Then, diag(UUT)i = ∣UUTi，, = Pnj= ?、*■ by definition. Now, We let (UA Θ UG)Λ 1 = VΛ2 with Λ2
being again a diagonal matrix. Therefore, Uij = Vi,j pΛj due to the multiplication with a diagonal
matrix from a right hand side. Substituting back these results in [(UA Θ UG)Λ(UA Θ UG)T]..=
PnjmκVi,j ʌ/Λj)2 which completes the derivation. Formulating equation 14 for the non-square matrices
(which results after a low rank approximation) such as UA1:a ∈ Rn×a and UG1:g ∈ Rm×g and paralleling
this operation are rather trivial and hence, they are omitted.
B.2	Derivation 2: Analytical Derivation of a Sampler
For a full Bayesian analysis which is approximated by a Monte Carlo integration, sampling is a crucial
operation (see equation 7) for computing predictive uncertainty. We start by stating the problem.
Problem statement: Consider drawing samples vec(Ws) ∈ Rnm from our sparse information form:
VeC(WS)〜N-1(WMAp，(UAIa ⑭ Ug 1g)Λ1:L(UAIa ⑭ UGIg)T + D)	(16)
Typically, drawing such samples vec(W s) from a canonical form of MND requires finding a symmet-
rical factor of the covariance matrix (e.g. Chloesky decomposition) which is cubic in cost O(N3).
Even worse, when represented in an information form as in equation 16, it requires first an inversion
of information matrix and then the computation of a symmetrical factor which overall constitutes two
operations of cost O(N3). Clearly, ifN lies in a high dimension such as 1 million, even storing is
obvious not feasible, let alone the sampling computations. Therefore, we need a sampling computa-
tion that (a) keeps the Kronecker structure while sampling so that first, the storage is memory-wise
feasible, and then (b) the operations that require cubic cost such as inversion, must be performed in
the dimensions of low rank L instead of full parameter dimensions N. We provide the solution below.
Analytical solution: Let us define Xl ∈ Rmn and XS ∈ Rm×n as the samples from a standard
Multivariate Normal Distribution in equation 17 where we denote the followings: 0nm ∈ Rnm,
Imn ∈ Rmn×mn , 0n×m ∈ Rn×m, In ∈ Rn×n and Im ∈ Rm×m. Note that these sampling operations are cheap.
X 〜N(0nm, Inm) ɑr XS 〜MN(0n×m, In, Im).	(17)
Furthermore, we denote Wl = VeC(WS) ∈ Rmn , θMAp = VeC(WMAp) ∈ Rmn as a sample from equation
16 and its mean as a vector respectively. We also note that Ai：L ∈ RL×L and D ∈ Rmn×mn are the low
ranked form of the re-scaled eigen-values and the diagonal correction term as previously defined.
UA1:a ∈ Rm×a and UG1:g ∈ Rn×g are the eigenvectors of low ranked eigen-basis so that m ≥ a, n ≥ g
and L = ag. Then, the samples of 16 can be computed analytically as4:
Wl = θMAp + FCXl where,
1/	1	IlTlI	71、	(18)
FC = D-2 (Inm - D-2(Ua 1：a ® Ug^A；：L(C-1 + V^Vs)-1A；：L(UAIa ® UGkg)TD-2}
Firstly, the symmetrical factor FC ∈ Rmn×mn in equation 18 is a function of matrices that are feasible
to store as they involve diagonal matrices or small matrices in a Kronecker structure. Furthermore,
1	1
VS = D- 2( Ua 1a % UgQA；：L
C = AC-T(BC - IL)AC-1 with AC and BC
being the Cholesky decomposed matrices of VST VS ∈ RL×L and VT V + IL ∈ RL×L such that：
ACACT = VST VS and
BCBCT = VSTVS+IL.
(19)
(20)
4We show how the Kronecker structure of FC can be exploited to compute FCXl in the derivation only.
15
Under review as a conference paper at ICLR 2020
Consequently, the matrices in equation 18 are defined as C ∈ RL×L, (C-1 + VT V) ∈ RL×L and
IL ∈ RL×L. In this way, the two operations namely Cholesky decomposition and inversion that are
cubic in cost O(N3) are reduced to the low rank dimension L with complexity O(L3).
Derivation: Firstly, note that sampling from a standard multivariate Gaussian for Xl or Xs is
computationally cheap (see equation 17). Given a symmetrical factor for the covariance Σ = FcFcT
(e.g. by Cholesky decomposition), samples can be drawn via θMAP + FcXl as depicted in equation 18.
Our derivation involves finding such symmetrical factor for the given form of covariance matrix while
exploring the Kronecker structure for the sampling computations to bound the complexity as O(L3).
Let us first reformulate the covariance (inverse of information matrix) as follows.
Σ = ((UAIa % UG 1g)ΛlZ(UAIa % UGIg)T +。)
=[D2 (D- 1(Ua 1a % Ug")八2丛1：L(UAIa % UGIg)TD-2 + %) D2J
=D-2 ∣((D-2(Ua 1a % UGig)Λ2:L)(D-2(Uaka % UGQΛ2L)T + %)『D-2
=D-2 hWT + Inmi-1 D-2.
(21)
Here, We define:匕=D-2(UA「a % UG「g)Λf.l. Now, a symmetrical factor for Σ = FCFCr can be
found by exploiting the above structure. We let Wcbe a symmetrical factor for VVT + Inm so that
1 τc-1
Fc = D- 2 Wc is the symmetrical factor of Σ, Following the work of Ambikasaran & O Neil (2014)
the symmetrical factor WC can be found using equations below.
WC = Inm + VsCVsT
C=AC-T(BC-IL)AC-1.
(22)
Note that A and B are Cholesky decomposed matrices of VT V ∈ RL×L and VT V + IL ∈ RL×L
respectively. As a first result, this operation is bounded by complexity O(L3) instead of the full
parameter dimension N . Now the symmetrical factor for Σ can be expressed as follows.
FC = D-2 W-1 = DT(Inm + VsCVT)-1
=D- 2 (Inm - Vs(C-1 + VT Vs)-1 VT ).
Woodbury’s Identity is used here. Now, it follows simply by substitution:
Wl = θMAP + FCXl where,
FC= D- 2 (Inm - Vs(C-1 + Vr Vs)-1 Vr )
=DT (Inm - D-2(Ua 1a % UgQΛ2L(C-1 + VrVS)TΛ2L(UAIa % UGIg)TD-2).
(23)
(24)
This completes the derivation of equation 18. As a result, the inversion operation is bounded by
complexity O(L3). Furthermore, the derivation constitutes smaller matrices UA1:a and UG1:g or diagonal
matrices D and Imn which can be stored as vectors. In short the complexity has significantly reduced.
Now we further derive computations that exploits rules of Kronecker products. Consider:
FcXl = D-2 (Inm - D-2(Ua 1a % Ug号肃：L(C-1 + Vr Vs)-1Λ2:L(UAIa % UGIg)TD-2).	(25)
Then, it follows by defining inverted matrix LC = (C-1 + VsT Vs)-1 ∈ RL×L with a cost O(L3):
16
Under review as a conference paper at ICLR 2020
FCX = D-2Ilnm - D-2(Uai：a ® UGQΛ2LLΛ2L(UAI“ ® UGIg)TD-211.	(26)
_1	7_17	1	1
We further reduce this by evaluating D-2 and defining XD = D-2Xl ∈ Rmn and Pc = ΛjLLAj^ ∈
RL×L . We note that this multiplication operation is memory-wise feasible.
FCXl= X1D -(DT(UAIa % UG 1:g)Pc(UAIa % UGIg)TXD).	(27)
Now, we map XID to matrix normal distribution by an unvec(∙) operation so that XD = unvec(XlD)
∈ Rn×n or equivalently XDl = vec(XDs ). Using a widely known relation for Kronecker product that is -
(UA1:a % UG1:g)T vec(XDs) = vec(UGT 1:g XDs UA1:a), it follows:
FCXl = XDl - (D-1(UA1:a % UG1:g)PCvec(UGT1:g XDs UA1:a)).	(28)
Note that matrix multiplication is performed with small matrices. Repeating a similar procedure as
above we obtain the equation below for XPs = PC vec((UA1:a % UG1:g)T XDl ),
FCXl = XDl - D-1vec[(UA1:a % UG1:g)XPs]
(29)
= XDl - D-1vec(UG1:gXPs UAT1:a) .
This completes the derivation. Lastly, we provide a remark below to summarize the main points.
Remark: We herein presented derivation is to sample from equation 16, a low-rank and information
formulation of MND. This analytical solution ensures (a) O(N3) >> O(L3) for Cholesky decomposi-
tion, (b) O(N3) >> O(L3) for a matrix inversion, (c) storage of small matrices UG1:g, UA1:a, a diagonal
matrix D and identity matrices and finally (d) matrix multiplications that only involve these matrices.
This is a direct benefit of our proposed LRA that preserves Kronecker structure in eigenvectors.
C Theoretical Analysis
Some of the interesting theoretical properties are as follows with proofs provided in section D.
C.1 Diagonal correction leads to more accurate estimation of Information matrix
A theoretical result of adding a diagonal correction term is captured below. This relates to the work
presented in section 2.2 where a diagonal correction term is added to EFB estimates of IM.
Lemma 1: Let I ∈ RN×N be the real Fisher information matrix, and let Idef ∈ RN×N and Iefb ∈ RN×N
be the DEF and EFB estimates of it respeCtively. It is guaranteed to have I -Iefb F ≥ I - IdefF.
Corollary 1: Let IkfaC ∈ RN×N and Idef ∈ RN×N be KFAC and our estimates of real Fisher information
matrix I ∈ RN×N respeCtively. Then, it is guaranteed to have I - IkfaC F ≥ I - IdefF.
Remark: For interested readers, find the proof I - Ik f aCF ≥ kI - IefbkF in George et al. (2018).
Note that I - Ik f aCF ≥ kI - IefbkF may not mean that I -1 - Ik-f1aC ≥ I -1 - Ie-f1bF or vice versa.
Yet, our proposed approximation yield better estimates than KFAC in the information form of MND.
C.2 Theoretical properties of Low-Rank Information matrix
To our knowledge, the proposed sparse IM have not been studied before. Therefore, we theoretically
motivate its design and validity for better insights. The analysis can be found below.
Firstly, we study the effects of preserving Kronecker structure in eigenvectors. We define:
17
Under review as a conference paper at ICLR 2020
ItoK = (Ua 0 UG)1:KΛi:K(Ua 0 UG)；：K	(30)
as a low rank EFB estimates of true Fisher that preserves top K eigenvalues. Similarly, ItoL can be
defined which preserves top L eigenvalues. In contract, our proposal to preserve Kronecker structure
in eigenvectors Ii：L is denoted as shown below. Now, we provide our analysis with Lemma 2.
I1:L = (Ua 1：a ® Ugi：g)A：L(UALa ® Ugi：g)’.	(31)
Lemma 2: Let I ∈ RN×N be the real Fisher information matrix, and let I；K ∈ RN×N, i^L ∈ RN×N
and I1：L ∈ RN×N be the low rank estimates of I ofEFB obtained by preserving top K, L and top K
plus additional J resulting in L eigenvalues. Here, we define K < L. Then, the approximation error of
I1：L is bounded asfollows: ∣∣I - ItoLllF ≥ ∣∣I - I1：LllF ≥ ∣∣I - I1oKllf-
Remark: This bound provides an insight that if preserving top L eigenvalues result in prohibitively
too large covariance matrix, our LRA provides an alternative to preserving top K eigenvalues
given that K < L. In practise, note that I1：L is a memory-wise feasible option as we formulate
I1：l = (Uai：a 0 UG1：g)Ai：l(Uai：a 0 UG1：g)T which preserves the Kronecker structure in eigenvectors.
This can be a case where evaluating (Uai：a 0 UG1：g) or (Uai：a 0 UG1：g)i：k is not feasible to store.
Lemma 3: The low rank matrix Σ = ((Ua 1：a 0 UG1：8)八上L(Ua 1：a 0 UG1：g)T + D) ɪ ∈ RN×N is a non-
degenerate covariance matrix ifthe diagonal correction matrix D and LRA (UA「a 0 Ug 1：g )Λ1:L(UA「a 0
UG1： g) t are both symmetric and positive definite. This condition is satisfied if (UA「a 0 UG1： g)Λ1: L ( Ua 1： a 0
Ug 1：g)T < E ∣δθ2i for all i ∈ {1,2,…，d} and with 八上L * 0.
Remark: This Lemma comments on validity of resulting parameter posterior and proves that
sparsifying the matrix can lead to a valid non-degenerate covariance if two conditions are met. As
non-degenerate covariance can have a uniquely defined inverse, it is important to check these two
conditions. We note that searching the rank can be automated with off-line computations that does
not involve any data. Thus, it does not introduce significant overhead. In case D does not turn out to
be, there are still several techniques that can deal with it. We recommend eigen-value clipping (Chen
et al., 2018) or finding nearest positive semi-definite matrices (Higham, 1988). Lastly, D-1 does not
get numerically unstable when we add a prior precision term and a scaling factor (ND + τI)-1.
Lemma 4: Let I ∈ RN×N be the real Fisher information matrix, and let Idef ∈ RN×N,
Iefb ∈ RN×N and Ikfac ∈ RN×N be the low rank DEF, EFB and KFAC estimates of it respec-
tively. Then, it is guaranteed to have ∣∣ diag(I) 一 diag(Iefb)∣∣F ≥ ∣∣ diag(I) 一 diag(Idef)∣∣F = 0
and ∣∣ diag (I) 一 diag (Ikfac )∣∣ F ≥ ∣∣ diag (I) 一 diag(Idef)∣∣ F = 0. Furthermore, ifthe eigenvalues of
Idef contains all non-zero eigenvalues of Idef, itfollows: ∣∣I 一 IefbllF ≥ llI - Ief∣∣f∙
Remark: Lemma 4 shows the optimally in capturing the diagonal variance while indicating that our
approach also becomes effective in estimating off-diagonal entries if IM contains many close to zero
eigenvalues. Validity of this assumption has been studied by Sagun et al. (2018) where it is shown
that the Hessian of overparameterized DNNs tend to have many close-to-zero eigenvalues. Intuitively,
from a graphical interpretation of IM, diagonal entries indicate information present in each nodes
and off-diagonal entries are links of these nodes (depicted in figure 1). Our sparsification scheme
reduces the strength of the weak links (their numerical values) while keeping the diagonal variance
exact. This is a result of the diagonal correction after LRA which exploits spectrum sparsity of IM.
D	Proofs
D.1 Diagonal correction leads to more accurate estimation of Information matrix
Proposition 1: Let I ∈ RN×N be the real Fisher information matrix, and let Idef ∈ RN×N and
Idef ∈ RN×N be our estimates ofit with rank d and k such that k < d. Their diagonal entries are equal
that is Iii = Idefii = Idefii forall i = 1,2, ..., N.
18
Under review as a conference paper at ICLR 2020
proof: The proof trivially follows from the definitions of I ∈ RN×N, Idef e RN×N and Idef e RN×N. As
the exact Fisher is an expectation on outer products of back-propagated gradients, its diagonal entries
equal Iii = E hδθi2i for all i = 1, 2, . . . , N.
In the case of full ranked I加于,substituting Dii = E 卜用-PMvα,α √Λ)2 with Pkm](vα,α √Λ)2 =
(UA Θ UG )Λ( UA Θ UG) T results in equation 32 for all i = 1,2,..., N.
Idefii = ( Ua 凶 Ug)Λ( Ua 凶 UG)T + Dii
ii	(32)
=(UA ⑭ UG)Λ(Ua ⑭ UG)T + E [δθ2] - (UA ⑭ UGZA ⑭ UG)T = E [δθ2]
Similarly, we substitute Dii = E [δθ2] - PNf(vα,α √ΛL)2 with PNM(vɑɑ √Λ1l)2 = (UAIN ®
UGi：M)Λ1:L(Uai：N Θ UGi：M)T which results in equation 33 for all i = 1,2,..., N.
T
Idefii = (UAi：N @ UGi：M)Λ1：L(UAi：N @ UGi：M)ii + Dii
=(Uai：N ® Ugi：MMi：L(Uai：N ® UGi：M)T + E [δθ2] — (Uai：N ® UGi：M)Ai：L(Uai：N ® UGi：M)T (33)
= E hδθi2i
Therefore, we have Iii = Idefii = Idefii for all i = i,2,..., N.
Lemma 1: Let I ∈ RN×N be the real Fisher information matrix, and let Idef ∈ RN×N and Iefb ∈ RN×N
be the DEF and EFB estimates of it respectively. It is guaranteed to have I -Iefb F ≥ I- IdefF.
proof: Let e2 = kA - Bk2F define a squared Frobenius norm of error between the two matrices
A ∈ RN×N and B ∈ RN×N. Now, e2 can be formulated as,
e2b = kA - Bk2F
=X(A-B)i2i+XX(A-B)i2j	(34)
i	i j,i
The first term of equation 34 belongs to errors of diagonal entries in B wrt A whilst the second term
is due to the off-diagonal entries.
Now, it follows that,
kI - IefbkF ≥ kI - IdefkF
eefb ≥ edef
Pi(I - Iefb)i2i + Pi P j,i(I - Iefb)i2j ≥ Pi(I - Idef)i2i + Pi P j,i(I - Idef)i2j
Pi(I-Iefb)i2i+PiPj,i(I-Iefb)i2j≥PiPj,i(I-Idef)i2j
Pi(I - Iefb)i2i + Pi P j,i(I - Iefb)i2j ≥ Pi P j,i(I - Iefb)i2j
Note that Pi(I - Idef)i2i = 0 using proposition i. Furthermore, PiPj,i(I-Idef)i2j = Pi Pj,i(I-Iefb)i2j
since by definition, Iefb and Idef have the same off-diagonal terms.
Corollary 1: Let Ikf ac ∈ RN×N and Idef ∈ RN×N be KFAC and our estimates of real Fisher Information
matrix I ∈ RN×N respectively. Then, it is guaranteed to have I - Ik f acF ≥ I - IdefF.
Find the proof I - Ik f acF ≥ kI - IefbkF in George et al. (20i8).
D.2 Theoretical properties of Low-Rank Information matrix
Lemma 2: Let I ∈ RN×N be the real Fisher information matrix, and let I；K ∈ RN×N, ILtfγL ∈ RN×N
and Ii： L ∈ RN × N be the low rank estimates of I ofEFB obtained by preserving top K, L and top K
plus additional J resulting in L eigenvalues. Here, we define K < L. Then, the approximation error of
Ii：L is bounded asfollows: ∣∣I - 1^LllF ≥ ∣∣I - Ii：LllF ≥ ∣∣I - IIoKllf∙
i9
Under review as a conference paper at ICLR 2020
proof: From the definition, (UA ® UG)Λ(UA ® UG)t = VΛVt is PSD as Λ万=E [(Vtδθ)2] ≥
0 for all elements i and VVT = I with I as an identity matrix (orthogonality). Naturally, low
rank approximations (UA Θ UG)i：ltop八上Ltop(UA 0 UG)T.ltop, (UA 艺 UG)i：KtOP八上Ktop(UA 0 UG)T：KtOP and
(UA 1：a 0 Ug 1：g)Λi:L(UA 1：a 0 Ug 1:^)t = (UA 0 UG)1：LΛkL(UA 0 UG)T:l are again PSD by the fact that
low rank approximation does not introduce negative eigenvalues.
Now, a well known fact from dimensional reduction literature is that low rank approximation
preserving the top eigenvalues result in best approximation errors in terms of Frobenius norm for the
given rank. Informally stating Wely’s ideas on eigenvalue perturbation：
Let B ∈ Rm×n with rank smaller or equal to p (one can also use complex space C instead of R) and let
E = A - B with A ∈ Rm×n. Then, it follows that,
kA — BkF = σι(A - B)2 + …+。〃(A - B)2 ≥ σP +ι(A - B)2 + …+ σ*(A - B)2 = ∣∣A - B1：PIlF , (35)
where σ1, ∙∙∙ σμ are the singular values of A with μ = min(n, m). The convention here is that σi(A) is
the ith largest singular value and σi(A) = 0 for i > rank(A). Using this insight, and the fact that in the
given settings, squared singular values are variances in new space lead to：
∣∣ι- I1oKIlF ≥ ∣∣ι -11：l∣∣F ≥ ∣∣ι- I1oLllF
Lemma 3: The low rank matrix Σ = ((Ua 1：a 0 UG1：g)Λ].L(Ua 1：a 0 UG1：g)t + D) ɪ ∈ RN×N is a non-
degenerate covariance matrix ifthe diagonal correction matrix D and LRA (UA1：a 0 Ug 1：8)八上L(UA1：a 0
UG1： g) t are both symmetric and positive definite. This condition is satisfied if (UA1： a 0 UG「g)A1： L ( Ua 1： a 0
Ug 1：g)T < E ∣δθ2i for alli ∈ {1,2, •…，N} and with ΛkL * 0.
proof: Let us first rewrite Idef = (UA1：a 0 UG1：8)八上L(UA1：a 0 UG1：g)T + D in the following form.
T	1	1	T
(Ua 1：a 0 UgQArL(Ua* 0 UGIg)T + D = (Ua* 0 UGQA；：lA；：L(Ua* 0 UGIg)T + D
=|(Ua 1：a 0 Ugkg)λJLH(UAIa 0 UGkg)λJL] + D (36)
= UUT + D
Now, if D and (Ua 1：a 0 UG1：8)八上L(UA1：a 0 UG1：g)t is both symmetric and positive definite, it follows
that for an arbitrary vector x ∈ Rd, xTUUTx > 0 as eigen-values Ri > 0 by construction. Furthermore,
xTDx > 0 also holds by the definition of positive definiteness. Therefore, we have xT(UUT + D)x =
xTUUTx + xT Dx > 0 which leads to the proof that Idef is positive definite if D and (UA1：a 0
Ug 1：g)ΛrL(UA1：a 0 UG1：g)t is both symmetric and positive definite. As this results in non-degenerate
IM, the canonical covariance Σ is non-degenerate as well.
Trivially following the definition of Dii = E hδθi2i - (UA 0 UG)Λ(UA 0 UG)iTi, Dii > 0 for all i when
(Ua,a 0 Ugkg)Λ^L(UA0 UG")T < E [δθ2]. Again, by the definition of Λi∙i∙ = E [(Vtδθ)；] ≥ 0, ΛkL
containing no zero eigenvalues result in the positive definite matrix (UA1：a 0 UG1：8)八1：L(UA1：a 0 Ug 1：g)t.
Lemma 4: Let I ∈ RN×N be the real Fisher information matrix, and let Idef ∈ RN×N,
Iefb ∈ RN×N and Ikfac ∈ RN×N be the low rank DEF, EFB and KFAC estimates of it respec-
tively. Then, it is guaranteed to have ∣∣diag(I) 一 diag(Iefb)∣∣F ≥ ∣∣diag(I) 一 diag(Idef)∣∣F = 0 and
∣∣ diag(I) 一 diag(Ikfac)∣∣F ≥ ∣∣ diag(I) 一 diag(Idef)∣∣F = 0. Furthermore, if the eigenvalues of Idef
contains all non-zero eigenvalues of Idef, it follows: ∣∣I -Iefb∣∣ F ≥ HI - idef U F-
proof: The first part follows from proposition 1 which states that for all the elements i, Iii =
Idef, ∣∣ diag(I) - diag(Iefb)∣∣F ≥ ∣∣ diag(I) - diag(Idef)∣∣F = 0 and ∣∣ diag(I) - diag(IkfaC)∣∣F ≥
∣∣ diag(I) - diag(Idef)∣∣F = 0. This results by the design of the method, in which, we correct the
diagonal entries in parameter space after the LRA.
20
Under review as a conference paper at ICLR 2020
For the second part of the proof, lets recap that Lemma 2 (Wely’s idea on eigenvalue perturbation)
that removing zero eigenvalues does not affect the approximation error in terms of Frobenius norm.
This then implies that off-diagonal elements of Idef and Iefb are equivalent. Then,:
kI -IefbkF ≥ ∣∣I - IdefIlF
ee2fb ≥ e2de f
Pi(I-Iefb))i + Pi P j,i(I-Iefb)2- ≥ Pi(I - Idef)2 + Pi P j,i(I - Idef)2-
Pi(I-Iefb)2- + Pi P j,i(I-Iefb)2j ≥ Pi P j,i(I - Idef)2j
Pi(I-Iefb)i2i+PiPj,i(I-Iefb)i2j≥PiPj,i(I-Iefb)i2j
Again, i(I - Idef)i2i = 0 according to proposition 1 for all the elements i.
E	Implementation Details and Further Results
KFAC library from Tensorflow 5 was used to implement the Fisher estimator (Martens & Grosse,
2015) for our methods and the works of Ritter et al. (2018a). Note that empirical Fisher usually is
not a good estimates as it is typically biased (Martens & Grosse, 2015) and therefore, we did not
use it. KFAC library offers several estimation modes for both fully connected and convolutional
layers. We have used the gradients mode for KFAC Fisher estimation (which is also crucial for
our pipelines) whereas the exact mode was used for diagonal approximations. We did not use the
exponential averaging for all our experiments as well as the inversion scheme in the library. However,
when using it in practice, it might be useful especially if there are too many layers that one cannot
access convergence of the Fisher estimation. We have used NVIDIA Tesla for grid searching the
parameters of Diag and KFAC Laplace, and 1080Ti for all other experiments.
E.1 Toy Regression Dataset
Apart from the architecture choices discussed in section 4, the training details are as follows. A
gradient descent optimizer from tensorflow has been used with a learning rate of 0.001 with zero prior
precision or L2 regularization coefficient (τ = 0.2 for KFAC, τ = 0.45 for Diag, N = 1 and τ = 0 for
both FB and DEF have been used). Mean squared error (MSE) has been used as its loss function.
Interestingly, the exact block-wise Hessian and their approximations for the given experimental setup
contained zero values on its diagonals. This can be interpreted as zero variance in information matrix,
meaning no information, resulting in information matrix being degenerate for the likelihood term. In
such cases, the covariance may not be uniquely defined (Thrun et al., 2004). Therefore, we treated
these variances deterministic, making the information matrix non-degenerate (motivated from Lemma
3). Similar findings are interestingly reported by MacKay (1992).
More importantly, we present a detailed analysis to avoid misunderstanding about our toy dataset
experiments. As a starting remark, a main advantage of this toy regression problem is that it simplifies
the understandings of on-going process, in lieu of sophisticated networks with a large number of
parameters. Typically, as of Herandez-Lobato & Adams (2015), Ritter et al. (2018a), Gal (2016),
or even originating back to Gaussian processes literature, this example has been used to check the
predictive uncertainty by qualitatively evaluating on whether the method predicts high uncertainty
in the regimes of no training data. However, a drawback exists: no quantitative analysis has been
reported to our knowledge other than qualitatively comparing it to community wide accepted ground
truth such as Hamiltonian Monte Carlo Sampling (Neal, 1996), and LA using KFAC and Diag seem
to be sensitive to hyperparameters in this dataset which makes the comparison difficult.
This is illustrated in figure 7 where we additional introduce Random which is just a user-set τI for
covariance estimation in order to demonstrate this. Qualitatively analyzing from the first look, all the
methods look very similar in delivering high uncertainty estimates in the regimes of no training data.
Here, we note that the same hyperparameter settings have been used for Diag, KFAC and FB Laplace
whereas the user-set τ = 7 has been found for Random. This agrees to the discussions of Ritter et al.
(2018a) as KFAC resulted in less τ when compared to Diag Laplace.
5https://github.com/tensorflow/kfac
21
Under review as a conference paper at ICLR 2020
(a) KFAC Laplace	(b) Diag Laplace	(c) Random	(d) FB Laplace
Figure 7: Toy regression uncertainty. User-set Laplace means user-set τI for covariance estimation.
This shows that if one tries to tune the "regularizing" parameters, all these approximations to the true
Hessian behaves similarly within this experiment. Now trained with 20 data points.
(a) OKF Laplace
-50
-ιooL
200p
150
IOO
50
64
(b) OKF Laplace (tuned)
(c) OKF inverse
(d) KFAC inverse
Figure 8: Toy regression uncertainty and covariance visualization (only the first layer is shown
here). OKF Laplace means using the left hand side of equation 3 without further approximation (only
possible with this small model and data-set).
However, we also observed that without the approximation step of equation 3 (denoted OKF), using
the same hyper parameter as above resulted in visible over-prediction of uncertainty and inaccurate
estimates on the prediction. This is shown in figure 8. Again, tuning the parameter to a higher
precision τ, similar behavior to figure 7 can be reproduced. This can be analyzed by visualizing the
covariance of KFAC and OKF. As it can be seen, in this experiment settings, figure 8 shows that
equation 3 damps the magnitude of estimated covariance matrix.
A possible explanation is that if the approximate Hessian is degenerate, then small τI places a big
mass on areas of low posterior probabilities for some network parameters with no information (zero
variance and correlations in the approximate Hessian). This can be seen in figure 8 part (a) where
the approximate Hessian contains 3 parameters with exactly zero diagonal elements and zeros in
its off-diagonal elements. If one tries to add a small τ = 0.001 here, then the covariance of these
parameters get close to its inverse τ-1 = 1000 as shown in figure 8 part (c). This would in return
result in over prediction of uncertainty and inaccurate predictions which explains figure 8 part (a).
Another interesting experiments are studying the effects of dataset size to number of parameters.
For this, we have increased the dataset size to 100 in oppose to 20. Again, we now compare the
approximate Hessian by visualizing them. Notably, at using 100 data points resulted in more number
of zero diagonal entries and corresponding rows and columns. This is due to over parameterization of
the model which results in under determined Hessian.
These insights hint for the followings. Accurately estimating the Hessian while forcing its estimates
non-degeneracy via not considering zero eigenvalues for this data and model can lead to less sensitivity
to its hyperparameters or τ in particular. Secondly, further increasing or decreasing the ratio of data
points to number of parameters change the approximate Hessian (similarly found for estimates of
Fisher) changes its structure, and can lead to under-determined approximation (therefore, changing
its loss landscape). Finally, if the Hessian is under-determined, hyperparameters τ affects the
resulting predictive uncertainty (or covariance) if its magnitude significantly differs (and in case of
22
Under review as a conference paper at ICLR 2020
KFAC). However, as more detailed experimental analysis is outside the scope of the paper, can be an
interesting future work to further analyze the relation between the hyperparameters, their probabilistic
interpretation and resulting loss landscape of neural network.
E.2 Benchmark Implementations
We have used Numpyro (Phan et al., 2019) for the implementations of HMC. We have used 50000 MC
samples to generate the results in order to ensure the convergence. For the implementation of Bayes By
Backprop we have used an open-source implementation https://github.com/ThirstyScholar/
bayes-by-backprop for which a similar experiment settings are implemented where the Gaussian
noise is sampled in a batch initially, and a symmetric sampling technique is deployed. We note that
the number of data samples and network architectures are different. Furthermore, we have used 10000
iterations to ensure convergence of the network.
E.3 Classification Tasks
Most of the implementations for MNIST and CIFAR10 experiments were taken from Tensorflow
tutorials 6 including the network architectures and training pipelines if otherwise stated in the main
text. This is in line of argument that our method can be directly applied to existing, and well trained
neural networks. For MNIST experiments, the architecture choices are the followings. Firstly, no
down-scaling has been performed to its inputs. The architecture constitutes 2 convolutional layers
followed by 2 fully connected layer (each convolutional layer is followed by a pooling layer of
size 2 by 2, and a stride 2). For flattening from the second convolutional layer to the first fully
connected layer, a pooling operation of 49 by 64 has been naturally used. RELU activation have
been used for all the layers except the last layer which computes the softmax output. Dropout has
been applied to the fully connected layer with dropout rate of 0.6 after a grid search (explained in
section E.3.1). Regarding the loss functions, cross entropy loss has been used with ADAM as its
optimizer and learning rate of 0.001. An important information is the size of each layers. The first
layer constitutes 32 filters with 5 by 5 kernel, followed by the second layer with 64 filters and 5 by 5
kernel. The first fully connected layer then constitutes 1024 units and the last one ends with 10 units.
We note that, this validates our method on memory efficiency as the third layer has a large number
of parameters, and its covariance, being quadratic in its size, cannot be stored in our utilized GPUs.
Regarding the architecture selection of CI-
FAR10 experiments, no down-scaling of the in-
puts has be done. The chosen architecture is
composed of 2 convolutional layers followed
by 3 fully connected layers. Pooling layers of
size 3 by 3 with strides 2 have been applied to
outputs of the convolutional layers. Obviously,
the third convolutional layer is pooled to match
the input size of the following fully connected
layers. Batch normalization has been applied
to each outputs of convolutional layer before
pooling, with bias 1, α of 0.001/9.0 and β of
0.75 (notations are different to the main text,
and this follows that of tensorflow library). A
weight decay factor of 0.004 has been used, and
trained again with cross entropy loss, now with
(a) the Hessian [100].
(b) the Hessian [20]
Figure 9: Visualization of the approximate Hes-
sian with different data points.
a stochastic gradient descent. Learning rate of 0.001 has been used. Again, the most relevant settings
are: the first layer constitutes 5 by 5 kernel with 64 filters. This is then again followed by the same (but
as input to CIFAR10 is RGB, the second layer naturally has more number of parameters). Units of
384, 192, and 10 have been used for the fully connected layers in an ascending order. Lastly, random
cropping, flipping, brightness changes and contract have been applied as the data augmentation
scheme. Similar to MNIST experiments, the necessity of LRA is capture in CIFAR10 as well.
6https://www.tensorflow.org/tutorials
23
Under review as a conference paper at ICLR 2020
Unlike Ritter et al. (2018a) we did not artificially augment the data for MNIST experiments because
the usual training pipeline did not require it. For our low rank approximation, we always have used
the maximum rank we could fit, after removing all the zero eigenvalues and checking the conditions
from Lemma 3. Lastly we have used 1000 Monte-Carlo samples for MNIST, and 100 samples for
CIFAR10 and toy regression experiments.
E.3.1 Benchmark implementations
Implementation of deep ensemble (Lakshminarayanan et al., 2017) was kept rather simple by not using
the adversarial training, but we combined 15 networks that were trained with different initialization.
The same architecture and training procedure were used for all. Note that CIFAR10 experiments with
similar convolutional architectures were not present in the works of (Lakshminarayanan et al., 2017)
to the best of our knowledge. On MNIST, Louizos & Welling (2017) found similar results to ours that
deep ensemble performed similar to the MC-dropout (Gal, 2016). For dropout, we have tried a grid
search of dropout probabilities of 0.5 and 0.8, and have reported the best results. For the methods
based on Laplace approximation, we have performed grid search on hyperparameters N of (1, 50000,
100000) and 100 values of τ were tried using known class validation set. Note that for every method,
and different data-sets, each method required different values of τI to give a reasonable accuracy. The
starting point of the grid-search were determined based on if the mean values of their predictions
were obtained similar accuracy to the deterministic counter parts. The figure below are the examples
on MNIST where minimum ece points were selected and reported.
Figure 10: Grid search results. For Diag (left two figures) and KFAC Laplace (right two) an
extensive grid search has been conducted to ensure fair comparison. Here, we report the results with
pseudo observation term of 50000 on MNIST. This ensures that a main difference to DEF Laplace is
the expression for model uncertainty as the inference and network architectures are kept the same.
24