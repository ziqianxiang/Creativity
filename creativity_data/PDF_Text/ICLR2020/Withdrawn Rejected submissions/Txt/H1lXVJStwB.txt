Under review as a conference paper at ICLR 2020

DYNAMIC  INSTANCE  HARDNESS

Anonymous authors

Paper under double-blind review

ABSTRACT

We introduce dynamic instance hardness (DIH) to facilitate the training of machine
learning models.  DIH is a property of each training sample and is computed as
the running mean of the sample’s instantaneous hardness as measured over the
training history. We use DIH to evaluate how well a model retains knowledge about
each training sample over time.  We find that for deep neural nets (DNNs), the
DIH of a sample in relatively early training stages reflects its DIH in later stages
and as a result, DIH can be effectively used to reduce the set of training samples
in future epochs.  Specifically, during each epoch, only samples with high DIH
are trained (since they are historically hard) while samples with low DIH can be
safely ignored.  DIH is updated each epoch only for the selected samples, so it
does not require additional computation. Hence, using DIH during training leads
to an appreciable speedup.  Also, since the model is focused on the historically
more challenging samples, resultant models are more accurate. The above, when
formulated as an algorithm,  can be seen as a form of curriculum learning,  so
we    call our framework DIH curriculum learning (or DIHCL). The advantages
of DIHCL, compared to other curriculum learning approaches, are:  (1) DIHCL
does not require additional inference steps over the data not selected by DIHCL
in each epoch, (2) the dynamic instance hardness, compared to static instance
hardness (e.g., instantaneous loss), is more stable as it integrates information over
the entire training history up to the present time.  Making certain mathematical
assumptions, we formulate the problem of DIHCL as finding a curriculum that
maximizes an unknown multi-set function f ( ) from its partial observations, and
derive an approximation bound for a DIH-produced curriculum relative to the
optimal curriculum. Empirically, DIHCL-trained DNNs significantly outperform
random mini-batch SGD and other recently developed curriculum learning methods
in terms of efficiency, early-stage convergence, and final performance, and this is
shown in training several state-of-the-art DNNs on 11 modern datasets.

1    INTRODUCTION

We study the dynamics of training a machine learning model, and in particular, the difficulty a 
model
has over time (i.e., training epochs) in learning each sample from a training set.  To this end, we
introduce the concept of “dynamic instance hardness” (DIH) and propose several metrics to measure
DIH, all of which share the same form as a running mean over different instantaneous sample hardness
measures.  Let at(i) be a measure of instantaneous (i.e., at time t) hardness of a sample, where i
is         a sample index and t is a time iteration index (typically a count of mini-batches that 
have been
processed).  In previous work, at(i) has been called the “instance hardness” (Smith et al., 2014)
corresponding to 1    pw(yi xi), i.e., the complement of the posterior probability of label yi 
given input
xi          for the iᵗʰ sample under model w. We introduce three different notions of instantaneous 
instance

hardness in this work: (A) the loss l(yi, F (xi; wt)), where l(·, ·) is the loss function and F (·; 
w) is
the model with parameters w, (B) the loss change  l(yi, F (xi; wt))     l(yi, F (xi; wt−₁))  
between

two consecutive time steps, and (C) the prediction flip 1[yˆᵗ  =  yˆᵗ−¹], where yˆᵗ is the 
prediction

i          i                              i

of sample i in step t, e.g., argmaxj F (xi; wt)[j] for classification. Our (A) corresponds closely 
to
the “instance hardness” of Smith et al. (2014). However, our (B) and (C) require information from
previous time steps.  Nevertheless, we consider (A), (B), and (C) all variations of instantaneous
instance hardness since they use information from only a very local time window around training

iteration t. Dynamics is achieved when we compute a running average over instantaneous instance
1


Under review as a conference paper at ICLR 2020

hardness, computed recursively as follows:


rt+1

γ × at(i) + (1 − γ) × rt(i)     if i ∈ St
rt(i)                                            else ,

(1)

where γ     [0, 1] is a discount factor, St     V , and V  = [n] is the set of all n training 
sample indices.
St is the set of sample selected for training at time t by some method (e.g., a DIH-based curriculum
learning (DIHCL) algorithm we introduce and study below) or simply a random batch. In general,
St should be large early during training, but as rt(i) decreases to small values for many samples,
choosing significantly smaller St is possible to result in faster training and more accurate 
models.

We find that rt(i) can vary dramatically between different samples since very early stage (with 
small
t). One can think of this as some samples being more memorable and are retained more easily, while
other samples are harder to learn and retain. In addition, the predictions of the hard samples are 
less
stable under changes in optimization parameters (such as the learning rate).

More importantly, once a sample’s rt(i) is established (i.e., once t is sufficiently but not 
unreasonably
large) each sample tends to maintain its DIH properties. That is, a sample’s DIH value converges
relatively quickly to its final relative position amongst all of the samples DIH values. For 
example, if
a sample’s DIH becomes small (i.e., meaning the sample is easily learned), it stays small relative 
to
the other samples, or if it becomes large DIH (i.e., the sample is difficult to learn), it stays 
there. I.e.,
once rt(i) for a sample has converged, its DIH status is retained throughout the remainder training.
We can therefore accurately identify categories of sample hardness relatively early in the course
of training.  This suggests a natural curriculum learning strategy where St corresponds mostly to
those samples that are hard according to rt  ₁(i).  In other words, the model concentrates on that
which it finds difficult. This is similar to strategies that improve human learning, such as the 
Leitner
system for spaced repetition (Leitner, 1970). This is also analogous to boosting (Schapire, 1990) —
in boosting, however, we average the instantaneous sample performance of multiple weak learners at
the current time, while in DIHCL we average the instantaneous sample performance of one strong
learner over the training history.

As mentioned above, instance hardness has been studied before (Smith et al., 2014; Prudencio et al.,
2015; Smith & Martinez, 2016) where it corresponds to the complement posterior probability. More
recently, instance hardness has also been studied as an average over training steps in Toneva et al.
(2019) where the mean of prediction flips over the entire training history is computed. We note that
Toneva et al. (2019) is a special case of DIH in Eq. (1) with γ = 1/t+1  and t = T , where T  is the
total number of training steps. Our study generalizes Toneva et al. (2019) to the running dynamics
computed during training. This therefore leads to a novel curriculum learning strategy and also 
steps
towards a better theoretical understanding of curriculum learning.  Also, in Toneva et al. (2019),
a small neural net is trained beforehand to determine the hard samples, and this is then used to
train large neural nets. In our approach, we take the average over time of at(i), which requires no
additional model or inference steps and hence is computationally trivial.

Another observation we find is that rt(i), for any sample, tends to monotonically decrease with t 
for
any i.  This means, not surprisingly, that during training samples become easier in terms of small
DIH (i.e., they are better learned). This also means that easy samples stay easy throughout 
training,
and hard samples also become easier the more we train on them.  If we also make (admittedly) a
mathematical leap, and assume that rt(i) is generated by the marginal gain of an unknown diminishing
returns function f ( ) that measures the quality of any curriculum, we can formulate DIHCL as an
online learning problem that maximizes the unknown f ( ) by observing its partial observation rt(i)
over time for each i. Here, f is defined over an integer lattice and has a diminishing returns 
property,
although the function is accessible only via the gains of every element. This formulation provides a
setting where the quality of the learnt curriculum is provably approximately good.

As will be shown below, DIHCL performs optimization in a greedy manner.  At each time step t,
DIHCL selects a subset St of samples using rt(i) where the hard samples have higher probabilities
of being selected relative to the easy samples. The model is then updated based only on the selected
subset St rather than V , which requires performing inference (e.g., a forward pass of a DNN) only
on St.  This therefore leads to a speedup to the extent that  St        V  .  The inference produces
new instantaneous instance hardness at(i) that is then used to update rt₊₁(i) as in Equation 1. To
encourage exploration, improve stability, and get an initial estimate of rt(i) for all i     V , 
during
the first few epochs, DIHCL sweeps through the entire training set. We provide several options for
DIH-weighted subset sampling, which introduces different types of randomness in the selection since

2


Under review as a conference paper at ICLR 2020

randomness is essential in optimizing non-convex problems. Under certain additional mathematical
assumptions, we also give theoretical bounds on the curriculum achieved by DIHCL compared
to           the optimal curriculum. We empirically evaluate several variants of DIHCL and compare 
them
against random mini-batch SGD as well as against recent curriculum learning algorithms, and test
on  11 datasets including CIFAR10, CIFAR100, STL10, SVHN, Fashion-MNIST, Kuzushiji-MNIST,
Food-101, Birdsnap, FGVC Aircraft, Stanford Cars and ImageNet.  DIHCL shows an advantage
over other baselines in terms both of time/sample efficiency and test set accuracy.

1.1    RELATED WORK

Early curriculum learning (CL) (Khan et al., 2011; Basu & Christensen, 2013; Spitkovsky et al.,
2009) work shows that feeding an optimized sequence of training sets (i.e., a curriculum), that can 
be
designed by a human expert (Bengio et al., 2009), into the training algorithms can improve the 
models’
performance.  Self-paced learning (SPL) (Kumar et al., 2010; Tang et al., 2012a; Supancic III &
Ramanan, 2013; Tang et al., 2012b) chooses the curriculum adaptive to some instance hardness (e.g.,
per-sample loss) during training. SPL selects samples with smaller losses, and gradually increases 
the
subset size over time to cover all the training data. Self-paced curriculum learning (Jiang et al., 
2015)
combines the human expert in CL and loss-adaptation in SPL. SPL with diversity (SPLD) (Jiang et al.,
2014) adds a negative group sparse regularization term to SPL and increases its weight to increase
selection diversity. Machine teaching (Khan et al., 2011; Zhu, 2015; Patil et al., 2014) aims to 
find
the optimal and smallest training subset leading to similar performance as training on all the data.
Minimax curriculum learning (MCL) (Zhou & Bilmes, 2018) argues that the diversity of samples is
more critical in early learning since it encourages exploration, while difficulty becomes more 
useful
later. It also uses a form of instantaneous instance hardness (the loss) but is not dynamic like 
DIH,
and formulates optimization as a minimax problem. Compared to the above methods, DIHCL has the
following advantages: (1) DIHCL improves the efficiency of CL since extra inference on the entire
training set per step is not required; and (2) DIHCL uses DIH as the metric for hardness which is a
more stable measure than instantaneous hardness.

Our paper is also related to Zhang et al. (2017), which refers to overfitting in noisy data. Our 
observa-
tions suggest that the learning of simple patterns (Arpit et al., 2017) happen mainly amongst the 
easy
memorable early during in training (additional discussion is given in the Appendix and Figure 5). 
Our
paper      is also distinct from catastrophic forgetting (Kirkpatrick et al., 2017), which 
considers sequential
learning of multiple tasks, where later learned tasks make the model forget what has been learned 
from
earlier tasks. In our work, we consider single task learning and show that easy samples remain 
easy.

If we make certain additional mathematical assumptions (as we do in our theoretical discussion
below), our work is related to online submodular function optimization. Specific forms have been
studied including maximization (Streeter & Golovin, 2009; Chen & Krause, 2013), maximization
in the bandit setting with noisy feedback (Chen et al., 2017), and continuous submodular function
maximization (Chen et al., 2018b;a).

The work most related to ours, perhaps, is the work on instance hardness (Smith et al., 2014; 
Prudencio
et al., 2015; Smith & Martinez, 2016), where the hardness of a sample corresponds to the complement
posterior probability, as discussed above. Also, a special case of DIH was studied in Toneva et al.
(2019): they compute DIH after training completes, and show that removing the easy samples (those
having the smallest DIH over training set) leads to less degradation on generalization performance 
than
removing random samples. By contrast, our study of DIH focuses on its dynamics during training.

2    DYNAMIC  INSTANCE  HARDNESS

We start out by conducting an empirical study of DIH in DNN training. We train a WideResNet with
depth of 28 and width factor 10 on CIFAR10 dataset, and apply a modified cosine annealing learning
rate schedule (Loshchilov & Hutter, 2017) for multiple episodes of increasing length (300 epochs
in total) and target learning rate decaying.  We contend that a cyclic learning rate suits our study
because: (1) it includes the most commonly used monotone decreasing schedule since the learning
rate in each cycle is decreasing; (2) compared to monotone decreasing schedule, it can uncover the
properties of DIH in more scenarios such as increasing learning rate and different decaying speeds 
of
the learning rate. In the study, we test two type of instantaneous instance hardness, where at(i) is
either prediction flips or loss (i.e., cases (A) and (C) in the previous section). We visualize 
rt(i) for

3


Under review as a conference paper at ICLR 2020


Epoch 10

Epoch 40

Epoch 10

Epoch 40

Epoch 210

Epoch 210

Figure 1: LEFT: Averaged prediction-flip and RIGHT: losses (mean and std.) of the three groups of 
samples
partitioned by a DIH metric (i.e.,running mean of prediction-flip) computed at epoch 10,40 and 210 
during
training WideResNet-28-10 on CIFAR10.   DIH in early stage (Epoch 40) can predict the forgettable 
and
memorable samples for later stages.  The failed partition based on Epoch 10 DIH implies the 
importance of
sufficient exploration to accurately measure hardness over time.

all i     [50000] training samples, but divide [50000] them into three groups according to rt(i) 
(with
at(i) as prediction flips), and we do this at epoch either 10, 40, or 210. For example, at epoch 
40, the
10,000 samples with the largest r₄₀(i) comprise the first group, the 10,000 samples ones with the
smallest r₄₀(i) comprise the next group, and the remaining 30,000 samples comprise the final group.
In Figure 1, we plot the dynamics for the average prediction flips over each group (left plot) and 
the
mean/standard deviation of loss in each group (right plot).


We observe that samples with small rt(i) are
learned quickly in the early epochs and their
losses remain small and predictions almost un-
changed thereafter, indicating they are easy over
time. By contrast, the samples with large rt(i)
have large variance, i.e., their losses oscillate
rapidly between small and large values, and their
predictions frequently change, indicating diffi-
culty.  The quickly identified easy samples are
never unlearnt, and do not suffer from any large
loss later in the training. The hard samples are
also quickly identified, and remain difficult to
learn even after training for many epochs.  On
average, the dynamics on the hard samples is
more consistent with the learning rate schedule,
which implies that doing well on these samples
can only be achieved at a shared sharp local min-

10 samples with large DIH at Epoch 40
10 samples with small DIH at Epoch 40

10 samples with large DIH at Epoch 40
10 samples with small DIH at Epoch 40

10 samples with large DIH at Epoch 40
10 samples with small DIH at Epoch 40

Figure 2: The three strategies (A), (B), and (C) for DIH
on ten hard and ten easy samples, each that have been
randomly sampled from the top 10k samples with the
largest/smallest DIH at Epoch 40.

ima. This suggests that the model generalizes better in the regions containing the easy samples than
those containing the hard ones. Similar to human learning, a natural resolution is to learn the hard
samples more frequently and reduce the learning on the easy, already learnt, ones. That is, based on
rt(i) (even during the early stages when t is not large), it is prudent to apply additional 
training effort
on hard samples and begin to ignore already learnt easy samples. Further empirical verification can
be found in experiments and Figure 4 in the Appendix.

2.1    PROBLEM FORMULATION

Figure 2 shows that all three types (base on (A), (B), and (C)) of DIH metrics decrease during 
training
for both easy and hard samples.  This indicates that as learning continues, samples become less
informative as more training takes place. If we make additional mathematical assumptions, we can
model  the curriculum learning procedure as a scheme that maximizes an unknown diminishing return
function f (·) via only observing its marginal gains rt(i).

A curriculum is a sequence of selected subsets, where each subset is a mini-batch of data points,

i.e.  (S₁, S₂, . . . , ST ).  We define a function f  :  ZV     → R≥₀  on the non-negative integer 
lattice

V   . Each point z ∈ ZV    is an non-negative integer valued vector of length |V | where z(i) 
counts

4


Under review as a conference paper at ICLR 2020

how many times sample i has been selected in the T  training data subsets. Any subset S ⊆ V  has

a characteristic vector eS where eS(i)  =  1 if i  ∈ S and otherwise sS(i)  =  0 if i  ∈/  S.  We 
also


define S₁:t , Σt

eSt

as a multi-set input to function f and S₁:t ∈ ZV

. Ideally, our goal becomes

finding the best curriculum (S₁, S₂, . . . , ST ), i.e., one that maximizes f (S₁:T ), as in:


max

S₁:T :∀t∈[T ],St⊆V,|St|≤kt

f (S₁:T ),                                                 (2)

where kt is a limit on the size of the set of samples selected at time t.  However, f ( ) can be an
arbitrary unknown function in practice. It is also intractable to estimate since it measures the 
utility
of all possible training sequences (in exponential number), so it is inaccessible for optimization. 
As a
surrogate, information about f ( ) might be available at each step in the form of the DIH values 
rt(i).
That is, if we make the mathematical assumption that there is some function f  such that rt(i) =
f (i S₁:t  ₁) , f (ei + S₁:t  ₁)     f (S₁:t  ₁), then rt(i) may be used in f ’s stead and we can 
optimize

the unknown f ( ) only based on its partial observation rt(i). In such case, DIHCL can be seen as

a form of online optimization problem whose goal is to find a curriculum that maximizes f : at every
step t, we select a subset of samples (e.g., a mini-batch) St ⊆ V  of size |St| = kt to train the 
model,

observing only marginal gains rt(i) of f (·) for each i. We can therefore define the following 
objective:


S     :∀t∈

max V,|S  |≤k  g(S1:T ) ,  Σ

Σ f (i|S₁:t−₁)                               (3)

For  simplicity,  we  slightly  overload  the  notation  of  S₁:T  for  function  g( )  so  that  
we  retain
information about the subset selected at every time step (i.e., we can extract St for 1      t     
T  from
S₁:T ). In practice, we update rt(i) only for samples in St since it is a byproduct of training 
(i.e., the
information needed to update rt(i) requires no more computation than what needed to train the model
on St). Hence, for i  / St, rt(i) = rt  ₁(i). Although our solution is an approximate solution to 
the
ideal but intractable optimization in Eq. (2), we give its approximation bound in Corollary 1 to the
global optimal solution of Eq. (2), and the bound is tight (the best we can get) up to a constant 
factor.

3    DIH CURRICULUM  LEARNING

We arrive at a curriculum learning strategy that increases the probability learning on hard 
samples, and
reduces learning on easy ones. However, directly solving Eq. (3) requires costly inference over all 
the
n training samples before selecting every subset St, as most previous curriculum learning methods 
do.

3.1    A “FREE” CURRICULUM


Most   optimization   algo-
rithms require inference on

Algorithm 1 DIH Curriculum Learning (DIHCL-Greedy)                   


the   training   samples   be-

1:  input: {(xi, yi)}n    , π(·; η), η₁:T , l(·, ·), F (·; w);


fore updating the model pa-

2:

T, T₀; γ, γk ∈ [0, 1]


rameters,  which  generates
the  predictions  and  losses

initialize: w, η₁, k₁ = n, r₀(i) = 1 ∀i ∈ [n]

4:  for t ∈ {1, · · · , T } do


for  the  samples  used  for
training.    In  step  t,  after      5:

the  model  gets  trained  on      6:

if t     T₀ then

St     [n];

else


St,  the  feedback  at(i)  for
i     St is already available.
However, for i  /  St, extra
inference is inevitable if the

7:           Let St = argmaxS: S =kt         i   S rt−₁(i);

8:       end if

9:       Apply optimization π.(·; η) and record F (xi; wt−₁) for i Σ∈ St;


curriculum design requires
instantaneous instance hard-

wt ← wt−₁ + π

∇wt−1             l(yi, F (xi; wt−₁)); ηt

i∈St


ness on the remaining sam-
ples  to  select  next  subset
St₊₁.  By contrast, DIHCL
relies  on  rt(i)  which  is  a
running mean of at(i), and

10:       Compute normalized at(i) for i     St using Eq. (4);

11:       Update dynamic instance hardness rt₊₁(i) using Eq. (1);

12:       kt+1         γk      kt;

13:  end for                                                                                        
    

it only updates rt(i) for i ∈ St and keeps rt(i) for i ∈/ St unchanged, thereby saves extra 
computation.

At step t of DIHCL, we select subset St     [n] with large rt  ₁(i) and then update the model by
training on St. We then update rt(i) via Equation (1). Since the learning rate can change over 
different
steps, and large learning rates means greater model change, we normalize at(i) by the learning rate

5


Under review as a conference paper at ICLR 2020

ηt−₁¹. Specifically, we use one of the following depending on if we’re in case (A), (B), or (C):

at(i) ← l(yi, F (xi; wt−₁))/ηt

a (i) ← |l(y , F (x ; w     )) − l(y , F (x ; w           ))|/ Σt             η '

			 

a (i) ← 1[yˆᵗ−¹ = yˆτt(i)−1]/ Σt             η '

method such as SGD, η₁:T are the learning rates of steps 1 to T  and γk is the reduction factor for

subset sizes kt. DIHCL trains on more samples early on to produce an initial more accurate estimate
of rt(i).  This is indicated by T₀, the number of warm start epochs over the whole training set at
the start.  After this, we start by selecting larger subsets each step and gradually reduce kt down
to the most difficult samples as training proceeds.

A simple method to further reduce training time in the earlier stages is to extract a small subset 
of St
by encouraging the diversity of the selected samples. We gradually reduce the diversity preference 
as
training switching to the exploitation stage (reduce λt by 0      γλ     1 for every step t). 
Inspired by
MCL (Zhou & Bilmes, 2018), after line 7, we reduce St to a subset of size kt′  = γk' kt (0 < γk'    
   1)

by (approximately) solving the following submodular maximization.


max

S⊆

k' Σ rt(i) + λtG(S)                                                   (5)

St,|S|≤  t i∈S

The function G : 2St           R₊ is may be any submodular function (Fujishige, 2005), and hence we 
can
exploit fast greedy algorithms (Nemhauser et al., 1978; Minoux, 1978; Mirzasoleiman et al., 2015) to
solve Eq. (5) with an approximation guarantee.

3.2    APPROXIMATION BOUND UNDER FURTHER MATHEMATICAL ASSUMPTIONS

If in addition to the assumption that there exists some function f  : ZV    → R≥₀ such that rt(i) =

f (i S₁:t  ₁) , f (ei + S₁:t  ₁)     f (S₁:t  ₁), we also assume that f  has certain properties, 
then an
approximation bound is achievable. The diminishing return (DR) property for f can be defined if,

∀0 ≤ x ≤ y:

f (x + ei)     f (x)      f (y + ei)     f (y).                                           (6)
Recall that ei is a one-hot vector with all zeros except for a single one at the ith position. We 
also

assume f is normalized and monotone, i.e., f (0) = 0 and f (x)      f (y),    0      x     y. 
W.l.o.g. we
assume the max singleton gain is bounded by 1 (maxi f (ei)      1). With such an assumption, we see
that rt(i) is monotonically decreasing with increasing t. That is, rt(i) monotonically decreasing 
is a
necessary, but not sufficient, condition for the DR property on f to hold. Empirically we observe, 
in
Figure 2 that rt(i) is indeed decreasing, meaning this evidence does not rule out there being a DR
function governing rt(i)’s behavior.  On the other hand, this of course does not guarantee the DR
property.  Nevertheless, if it is the case that rt(i) is produced as above from some DR function, it
enables the following analysis to proceed.

Under the above assumptions, we may derive bounds of DIHCL-Greedy(Alg. 1) when kt  =  k
t     [T ]. For simplicity, assume n   mod k = 0 and let m , n . We first show the bound on function
g of observed gains, and then connect it to the unknown function f .

Theorem 1.  For         V              on ground set     with DR property, compared to any solution 
   ∗  ,

S₁:T , the solution of D≥IHCL-Greedy, achieves

. 1 − e−¹    k  Σ      ∗


Where Cf,m , m minA

1:m

g(A₁:m) such that Sm    Ai = V, and |Ai| = k.

Corollary 1.  f (S     ) +  ¹ C       ≥ 1  max , 1−e−1 ,  ᵏ , f (S∗  )

The proofs are given the Appendix. The Cf,m term in the bound reflects our loss during the warm
start phase, where we cannot estimate the gain of each sample unless we select each sample at least
once, which is independent of T  and vanishes in the long run.  The 1     e−¹ comes from the DR
property and our greedy procedure.  For the 1/k factor and the k/n factor of the bound on g, we
give hard cases in the Appendix so our bound is tight to constant factors. These factors result 
from

¹We use ηt−1 instead of ηt  because at(i) is computed based on wt−1 before the weight update in 
step t.

6


Under review as a conference paper at ICLR 2020

our assumption about the function f , which may have arbitrary interactions among data points. In
practice, similar data points tend to have similar DIH, and we can incorporate such information by
adding an additional term of submodular function G to the DIH value to model data point 
interactions.

3.3    PRACTICAL METHODS FOR DIH-WEIGHTED SAMPLING IN DIHCL

In  line  7  of  Alg.  1,  we  select  St with  the  highest  rt  ₁(i)  values.   In  practice,  we 
 find  adding
randomness  to  the  selection  procedure  gives  better  performance  as  (1)  exploration  on  
samples
with  small  rt(i)  is  necessary  for  accurate  estimate  to  rt(i),  and  (2)  randomness  of  
training
samples is essential to achieve a good quality solution w for non-convex models such as DNNs.
Instead of choosing greedily the top kt samples, we perform random sampling with probability
pt,i     h(rt  ₁(i)), where h( ) is a monotone non-decreasing function, and we still prefer data 
points
with high DIH. An ideal choice of h( ) should balance between the exploration of data with poorly
estimated DIH and exploitation of data with well estimated DIH. We propose the following three
sampling methods to replace line 7 of Alg. 1, and give extensive evaluations in the experiment 
section.

DIHCL-Rand: Let h(rt(i)) = rt(i). We sample data points weighted by their DIH values.

DIHCL-Exp: We trade-off exploration and exploitation similarly to Exp3 (Auer et al., 2003), which
samples based on the softmax value and reweigh the observation by the selection probability to
encourage exploration:

h(rt(i)) = exp       2 log n/n × rt(i)   ,      at(i) ← at(i)/pt,i   ∀i ∈ St.                     
(8)

DIHCL-Beta:   We  utilize  the  idea  of  Thompson  sampling  (Thompson,  1933)  and  use  a  Beta
distribution prior to balance exploration and exploitation, i.e., h(rt(i))  ∼ Beta(rt(i), c − 
rt(i)),
where c is a sufficiently large constant that c ≥ rt(i), e.g., c = 1 when at(i) is prediction flip. 
The
Beta distribution encourages exploration when the difference between rt(i) and c − rt(i) is small.

Table 1:  The final test accuracy (%) achieved by different methods training DNNs on 11 datasets 
(without
pre-training). We use “Loss, dLoss, Flip” to respectively denote the 3 choices of DIH metrics based 
on (A), (B),
and        (C) respectively. In all DIHCL variants, we apply lazier-than-lazy-greedy (Mirzasoleiman 
et al., 2015) for
Eq. (5) on all datasets except Food-101, Birdsnap, Aircraft (FGVC Aircraft), Cars (Stanford Cars), 
and ImageNet.

Curriculum                  CIFAR10   CIFAR100   Food-101   ImageNet   STL10    SVHN   KMNIST   
FMNIST   Birdsnap   Aircraft     Cars

Rand mini-batch            96.18         79.64         83.56        75.04      86.06    96.48      
98.67        95.22       64.23      74.71     78.73

SPL                                93.55         80.25         81.36        73.23      81.33    
96.15      97.24        92.09       63.26      68.95     77.61

MCL                              96.60         80.99         84.18        75.09      88.57    96.93 
     99.09        95.07       65.76      75.28     76.98

DIHCL-Rand, Loss       96.76         80.77         83.82        75.41      87.25    96.81      
99.10       95.69       65.62      79.00     80.91

DIHCL-Rand, dLoss     96.73         80.65         83.82        75.34      86.93    96.83      99.14 
       95.64       65.25     79.93    78.70

DIHCL-Exp, Loss         97.03         82.23         84.65        75.10      88.36    96.91     
99.20        95.45       66.13      77.68     79.85

DIHCL-Exp, dLoss       96.40         81.42         84.75        75.62      89.41    96.80      
99.18        95.50       66.59      79.72    81.48

DIHCL-Beta, Flip          96.51         81.06         84.94        76.33      86.88    97.18      
99.05        95.66       65.48      78.49     80.13

4    EMPIRICAL  EXPERIMENTAL  EVALUATION

We train different DNNs by using variants of DIHCL, and compare them with three baselines, vanilla
random mini-batch SGD, self-paced learning (SPL) (Kumar et al., 2010), and minimax curriculum
learning (MCL) (Zhou & Bilmes, 2018) on 11 image classification datasets (without pre-training), 
i.e.,

(1) WideResNet-28-10 (Zagoruyko & Komodakis, 2016) on CIFAR10 and CIFAR100 (Krizhevsky
& Hinton, 2009); (2) ResNeXt50-32x4d (Xie et al., 2017) on Food-101 (Bossard et al., 2014), FGVC
Aircraft (Aircraft) (Maji et al., 2013), Stanford Cars (Krause et al., 2013), and Birdsnap (Berg et 
al.,
2014); (3) ResNet50 He et al. (2016) on ImageNet (Deng et al., 2009); (4) WideResNet-16-8 on
Fashion-MNIST (FMNIST) (Xiao et al., 2017) and Kuzushiji-MNIST (KMNIST) (Clanuwat et al.,
2018);          (5) PreActResNet34 He et al. (2016) on STL10 (Coates et al., 2011) and SVHN (Netzer 
et al.,
2011).  We use mini-batch SGD with momentum of 0.9 and cyclic cosine annealing learning rate
schedule (Loshchilov & Hutter, 2017) (multiple episodes with starting/target learning rate decayed
by      a multiplicative factor 0.85). We use T₀ = 5, γ = 0.95, γk = 0.85 for all DIHCL variants, 
and
gradually reduce k from n to 0.2n. On each dataset, we apply each method to train the same model for
the same number of epochs, but each method may select different amount of samples per epoch. More
details about the datasets and settings can be found in the Appendix. For DIHCL variants that 
further
reduce St by solving Eq. (5), we use λ₁ = 1.0, γλ = 0.8, γk'  = 0.4 and employ the “facility 
location”

7


Under review as a conference paper at ICLR 2020

submodular function (Cornuéjols et al., 1977) G(S) =      j   St  maxi∈S ωi,j where ωi,j represents
the similarity between sample xi and xj.  We utilize a Gaussian kernel for similarity using neural

net features (e.g., the inputs to the last fully connected layer in our experiments) z(x) for each 
x, i.e.,


ωi,j

= exp .−ǁz(xi)−z(xj )ǁ2/2σ²Σ, where σ is the mean value of all the k(k−1)/2 pairwise distances.

Figure 3: Training DNNs by using DIHCL (and its variants), SPL (Kumar et al., 2010), MCL (Zhou & 
Bilmes,
2018), and random mini-batch SGD on CIFAR10, Food-101, FGVC Aircraft and Birdsnap. We use “Diverse” 
to
denote DIHCL that further reduces St  by applying submodular maximization for Eq. (5). We report 
how the test
accuracy changes with the number of training batches for each method.

In Figure 3, we show how the test set accuracy changes when increasing the number of training
batches in each curriculum learning method on 3 datasets. The results for other 8 datasets can be 
found
in  the Appendix, together with the wall-clock time for (1) the entire training and (2) the 
submodular
maximization part in DIHCL with diversity and MCL. The final test accuracy achieved by each
method        is reported in Table 1. DIHCL and its variants show significantly faster and smoother 
gains on
test accuracy than baselines during training especially at earlier stages. They also achieve higher 
final
accuracy and show improvements in sample efficiency (meaning they reach their best performance
sooner, after less computation has taken place). MCL can reach similar performance as DIHCL on
some datasets but it shows less stability and requires more computation for submodular maximization.
We also observe a similar instability of SPL. The reason is that, compared to the methods that use
DIH, both MCL and SPL deploy instantaneous instance hardness (i.e., current loss) as the score to
select samples, a measure that is more sensitive to randomness and perturbation that occurs during
training. Compared to MCL and DIHCL, SPL and the random mini-batch curriculum method requires
more epochs to reach their best accuracy, since they spend training effort on the easier and 
memorable
samples but lack sufficient repeated-learning of the forgettable ones.  Although every variant of
DIHCL achieves the best accuracy among all the evaluated methods on some datasets, DIHCL-Exp
using loss and DIHCL-Beta using prediction flip, as the instantaneous hardness, exhibit advantages
over the other DIHCL variants. One possible explanation is that the running mean computed on the

8


Under review as a conference paper at ICLR 2020

loss and prediction flips are more stable along the training trajectory as shown in Figure 2, or 
perhaps
they are more in line with our assumption in Section 3.2 about the diminishing return property of f 
(·).

9


Under review as a conference paper at ICLR 2020

REFERENCES

Devansh Arpit, Stanisław Jastrze˛bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S.
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien.
A closer look at memorization in deep networks. In ICML, volume 70, pp. 233–242, 2017.

Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed
bandit problem. SIAM J. Comput., 32(1):48–77, 2003.

Sumit Basu and Janara Christensen.  Teaching classification boundaries to humans.  In AAAI, pp.
109–115, 2013.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston.  Curriculum learning.  In

ICML, pp. 41–48, 2009.

Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N.
Belhumeur.  Birdsnap:  Large-scale fine-grained visual categorization of birds.  In Proc. Conf.
Computer Vision and Pattern Recognition (CVPR), 2014.

Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative compo-
nents with random forests. In ECCV, 2014.

Lin Chen, Andreas Krause, and Amin Karbasi. Interactive submodular bandit. In Advances in Neural
Information Processing Systems, pp. 141–152, 2017.

Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online optimiza-
tion with stochastic gradient: From convexity to submodularity. arXiv preprint arXiv:1802.08183,
2018a.

Lin Chen, Hamed Hassani, and Amin Karbasi. Online continuous submodular maximization. arXiv
preprint arXiv:1802.06052, 2018b.

Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular
optimization. In International Conference on Machine Learning, pp. 160–168, 2013.

Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature, 2018.

Adam Coates, Honglak Lee, and Andrew Y. Ng. An analysis of single-layer networks in unsupervised
feature learning. In AISTATS, pp. 215–223, 2011.

G. Cornuéjols, M. Fisher, and G.L. Nemhauser. On the uncapacitated location problem. Annals of
Discrete Mathematics, 1:163–177, 1977.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  ImageNet: A large-scale
hierarchical image database. In CVPR, 2009.

Satoru Fujishige. Submodular functions and optimization. Annals of discrete mathematics. Elsevier,
2005.

Kaiming He,  Xiangyu Zhang,  Shaoqing Ren,  and Jian Sun.   Deep residual learning for image
recognition.  2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770–778, 2016.

Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander G. Hauptmann.
Self-paced learning with diversity. In NeurIPS, pp. 2078–2086, 2014.

Lu Jiang,  Deyu Meng,  Qian Zhao,  Shiguang Shan,  and Alexander G. Hauptmann.   Self-paced
curriculum learning. In AAAI, pp. 2694–2700, 2015.

Faisal Khan, Xiaojin (Jerry) Zhu, and Bilge Mutlu. How do humans teach: On curriculum learning
and teaching dimension. In NeurIPS, pp. 1449–1457, 2011.

10


Under review as a conference paper at ICLR 2020

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences (PNAS), 114(13):3521–3526,
2017.

Andreas Krause and Cheng S Ong. Contextual gaussian process bandit optimization. In Advances in
Neural Information Processing Systems, pp. 2447–2455, 2011.

Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization.   In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013.

Alex Krizhevsky and Geoffrey Hinton.   Learning multiple layers of features from tiny images.
Technical report, University of Toronto, 2009.

M. Pawan Kumar, Benjamin Packer, and Daphne Koller.  Self-paced learning for latent variable
models. In NeurIPS, pp. 1189–1197, 2010.

Sebastian  Leitner.     Leitner  system,  1970.     URL  https://en.wikipedia.org/wiki/
Leitner_system.

I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017.

S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.  Fine-grained visual classification of
aircraft. Technical report, 2013.

Michel Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Opti-
mization Techniques, volume 7 of Lecture Notes in Control and Information Sciences, chapter 27,
pp. 234–243. Springer Berlin Heidelberg, 1978.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas
Krause. Lazier than lazy greedy. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial
Intelligence, pp. 1812–1818, 2015.

G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing
submodular set functions. Mathematical Programming, 14(1):265–294, 1978.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning.   In NeurIPS Workshop on Deep
Learning and Unsupervised Feature Learning, 2011.

Kaustubh R Patil, Xiaojin Zhu, Ł ukasz Kopec´, and Bradley C Love. Optimal teaching for limited-
capacity human learners. In NeurIPS, pp. 2465–2473, 2014.

Ricardo B. C. Prudencio, Jose Hernandez-Orallo, and Adolfo Martinez-Uso. Analysis of instance
hardness in machine learning using item response theory.   In 2nd International Workshop on
Learning over Multiple Contexts (LMCE 2015), 2015.

Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990.

Michael R. Smith and Tony Martinez. A comparative evaluation of curriculum learning with filtering
and boosting in supervised classification problems. Comput. Intell., 32(2):167–195, 2016.

Michael R. Smith, Tony Martinez, and Christophe Giraud-Carrier. An instance level analysis of data
complexity. Machine Learning, 95(2):225–256, 2014.

Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. Baby Steps: How “Less is More” in unsu-
pervised dependency parsing. In NeurIPS 2009 Workshop on Grammar Induction, Representation
of Language and Language Learning, 2009.

Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular functions. In

Advances in Neural Information Processing Systems, pp. 1577–1584, 2009.

James Steven Supancic III and Deva Ramanan. Self-paced learning for long-term tracking. In CVPR,
pp. 2379–2386, 2013.

11


Under review as a conference paper at ICLR 2020

Kevin Tang, Vignesh Ramanathan, Li Fei-fei, and Daphne Koller. Shifting weights: Adapting object
detectors from image to video. In NeurIPS, pp. 638–646, 2012a.

Ye Tang, Yu-Bin Yang, and Yang Gao.  Self-paced dictionary learning for image classification.  In

MM, pp. 833–836, 2012b.

William R Thompson. On the Likelihood that One Unknown Probability Exceeds Another in View
of the Evidence of Two Samples. Biometrika, 25(3-4):285–294, 1933.

Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio,
and Geoffrey J. Gordon.  An empirical study of example forgetting during deep neural network
learning. In International Conference on Learning Representations (ICLR), 2019.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.

Saining Xie,  Ross Girshick,  Piotr Dollár,  Zhuowen Tu,  and Kaiming He.   Aggregated residual
transformations for deep neural networks. In CVPR, 2017.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.  Understanding
deep learning requires rethinking generalization. In ICLR, 2017.

Tianyi Zhou and Jeff Bilmes.   Minimax curriculum learning:  Machine teaching with desirable
difficulties and scheduled diversity. In ICLR, 2018.

Xiaojin Zhu. Machine teaching: An inverse problem to machine learning and an approach toward
optimal education. In AAAI, pp. 4083–4087, 2015.

12


Under review as a conference paper at ICLR 2020

A    PROOFS

f  : ZV     → R≥₀ on ground set V  is defined over an integer lattice.  The diminishing return (DR)

property of f is the following inequality 0 ≤ ∀x ≤ y:

f (x + ei)     f (x)      f (y + ei)     f (y),                                           (9)
Where ei is a one-hot vector with all zeros except for a single one at the ith position. We assume 
f is

normalized and monotone, i.e., f (0) = 0 and f (x)      f (y),   0      x     y. W.l.o.g. we also 
assume
the max singleton gain is bounded by 1, i.e., maxi f (ei)      1. We can think that f takes input 
as a
multi-set, and the gain of an item diminishes as its counter increases in the multi-set.

In the setting of selecting mini-batches for training machine learning models, suppose the 
mini-batch
size is k, the training set is V , and at every time step t, we select St     V  with  St  = k, and 
only
observe the gains on the selected subset (e.g., for neural networks, we update the running mean of
training losses during the forward pass of the chosen mini-batch, or DIH type (A)). At every time 
step
of selecting a mini-batch, we observe f (i S₁:t  ₁)   i     St. Let n =  V  , m =  ⁿ , and for 
simplicity
assume n   mod k = 0. We define function g to reflect the observed gains from f as we select data
samples at each training step:

g(S₁:t) =   Σ  Σ f (i|S₁:t'−1)                                               (10)

t'=1:t i∈St'

For simplicity, we slightly overload the notation of S₁:T for function g( ) so that we retain 
information
about the subset selected at every time step (i.e., we can extract St for 1      t     T  from S₁:T 
). Note
that g is permutation-variant for k > 1, i.e., for different ordering in S₁:t, g gives different 
values.

Theorem 1.  For         V              on ground set     with DR property, compared to any solution 
   ∗  ,

S₁:T , the solution of D≥IHCL-Greedy, achieves

. 1 − e−¹    k  Σ      ∗


Where Cf,m , m minA

1:m

g(A₁:m) such that Sm    Ai = V, and |Ai| = k.

To bridge S₁:T with S1∗:T , we first connect S₁:T to the greedy solution with singleton gain 
oracle, but
uses the history of sequence of (S₁, S₂, . . . , ST −₁), which we denote by (Sˆ1, Sˆ2, . . . , SˆT 
):

Sˆt  =   argmax  Σ f (i|S₁:t−₁).                                               (11)

S⊆V,|S|=k i∈S

Note we denote any set with subscript 0 (at time step 0) as an empty set, i.e. S₀ =   , Sˆ0  =   , 
and etc..
We define the observed gain values on the singleton gain oracle with history of (S₁, S₂, . . . , ST 
 ₁)

as:


g(Sˆ1:T |S1:T −1) =   Σ

Σ f (i|S₁:t−₁)                                         (12)

t=1:T i∈Sˆt

Firstly, we derive a lower bound of g(S₁:T ) in terms of g(Sˆ1:T |S₁:T −₁).

Lemma 1.  g(S₁:T ) + Cf,m ≥ g(Sˆ1:T |S₁:T −₁).

Proof.  Define  ζ(i, A₁:t)  to  return  the  subsequence  of  A₁:t that  starts  from  A₁  and  
ends  at  At'

where  At'  is  the  last  set  in  the  whole  sequence  that  contains  the  element  i,  i.e.,  
ζ(i, A₁:t)  =


argmaxA

1:t

'  t′1i∈At' . When i is not present in the whole sequence A₁:t, ζ(i, A₁:t) returns Ø.

By definitions of Cf,m and g(Sˆ1:m  S₁:m  ₁), we have Cf,m     mf (V )      g(Sˆ1:m  S₁:m  ₁) due to
the diminishing return (DR) property.

For T  ≤ m, Lemma 1 is true because of the above inequality.

For T      m + 1, we compare the previous gains of elements in St to the current gains of elements 
in

Sˆt:

13


Under review as a conference paper at ICLR 2020

g(S₁:T ) + Cf,m ≥ g(S₁:T ) + g(Sˆ1:m|S₁:m−₁)                                                       
(13)

≥    Σ    Σ f (i|ζ(i, S₁:t−₁)) + g(Sˆ1:m|S₁:m−₁)                  (14)

≥    Σ    Σ f (i|ζ(i, S₁:t−₁)) + g(Sˆ1:m|S₁:m−₁)                  (15)

t=m+1:T i∈Sˆt

≥    Σ    Σ f (i|S₁:t−₁) + g(Sˆ1:m|S₁:m−₁)                          (16)

t=m+1:T i∈Sˆt

= g(Sˆ1:T |S₁:T −₁)                                                                          (17)

Eq. 13 and Eq. 16 hold due to the diminishing return property and Eq. 15 is a result of the greedy
step (i.e., St is optimal when conditioning on ζ(i, S₁:t  ₁)). Note that we are guaranteed to find 
an
element in the sequence history ( ζ(i, S₁:t  ₁)  > 0 in Eq. 14 and Eq. 15) since we sweep the ground
set    V  in the first m steps of solution S₁:m.

Remarks. In the proof, we ignore the gain at the T  step, i.e.,     i∈ST  f (i|S₁:T −₁) as such 
gain can
potentially be zero.  In other words, g(S₁:T  ₁) + Cf,m     g(Sˆ1:T  S₁:T  ₁).  For the case that f 
 is
modular, i.e., f (x + ei) = f (x) + f (ei), and for only k elements in V , the function evaluations 
are
non-zero, the bound meets in equality: g(S₁:T  ₁) + Cf,m = g(Sˆ1:T  S₁:T  ₁). The idea is that we
have to sweep all elements in the ground set before we identify the non-zero-valued elements.

Next, we find a lower bound of g(Sˆ1:T |S₁:T −₁) in terms of g(S1∗:T ).

Lemma 2.  g(Sˆ     |S         ) ≥ 1−e−1 g(S∗  ).

Proof.  For T ′ < T , we compare g(S1∗:T ) with g(Sˆ1:T |S₁:T −₁):

1 g(S∗  ) =  1   Σ  Σ f (i|S∗     )                                                                 
          (18)


≤ 1   Σ

k × max f (i|S∗

)                                                                   (19)


≤ 1 kf (S∗

)                                                                                                
(20)

≤ f (S₁∗:T '  + S₁:T )                                                                              
          (21)


≤ f (S₁:T ' ) +

i∈S1∗:T

f (i|S₁:T ' )                                                                    (22)


≤ f (S₁:T ' ) + T (

i∈Sˆ   '

f (i|S₁:T ' ))                                                           (23)

= f (S₁:T ' ) + T (g(Sˆ1:T '+1|S₁:T ' ) − g(Sˆ1:T ' |S₁:T '−1))                           (24)

≤ g(Sˆ1:T ' |S₁:T '−1) + T (g(Sˆ1:T '+1|S₁:T ' ) − g(Sˆ1:T ' |S₁:T '−1))              (25)

From Eq. 19 to Eq. 20, we use     t=1:T  maxi∈St∗ f (i|S1∗:t−1) ≤    t₌₁:T f (St∗|S1∗:t−1) = f 
(S1∗:T ).
Eq. 22 is due to DR property and Eq. 23 is a result of greedy selection. Also note that for Eq. 21,

S∗    + S₁:T = ΣT '   eS∗  + ΣT     eS .

By  rearranging  Eq.  25,  we  have   ¹ ( ¹ g(S1∗:T )  − g(Sˆ1:T ' |S₁:T '−1)))   ≤  g(g(Sˆ1:T 
'+1|S₁:T ' )  −

g(Sˆ1:T ' |S₁:T '−1),  i.e.,  every time step,  we reduce the gap to 1/k  of the of optimal 
solution by

at least  ¹ . Therefore g(Sˆ     |S         ) ≥ 1−e−1 g(S∗  ).

14


Under review as a conference paper at ICLR 2020

Remarks. We will show that there is a hard case with 1/k factor. Suppose f is a set cover function
(f (i|A)  =  0  if  i  ∈  A)  and  |V |  =  k².   The  ground  set  V  is  partitioned  into  k  
groups  V   =
V₁ ∪ V₂ ∪ . . . ∪ Vk with k elements in each group, such that f (a) = 1 ∀a ∈ V , f (a|b) = 0 ∀a, b 
∈ Vi,

and f (  a, b  ) = f (a) + f (b)   a     Vi, b     Vj, i = j. For the first time step, g(S₁   ) 
gets a gain of
k which is equal to g(S₁∗). However, S₁ may select one element from each of the group since we
are doing the ground set sweeping exploration, and all the rest gains will be zero conditioned on 
S₁.
The optimal solution, on the other hand, can select all k elements from one group at a time, and 
get a
value of k² in the end.

Combine Lemma 1 and Lemma 2, we get the first factor ¹−ᵉ−1   for the bound in Theorem 1.

Lemma 3.  g(Sˆ1:T |S₁:T −₁) ≥  k  g(S1∗:T ).

Proof.  We will first connect g(Sˆ1:T  S₁:T  ₁) with the solution that selects the entire ground 
set V  at
every step, i.e., g(V₁:T ) = g((V, V, . . . , V )).


g(Sˆ1:T

|S1:T −1

k

) ≥ ng(V

k

1:T

|S1:T −1

)                                                  (26)

≥ ng(V₁:T |V₁:T −₁)                                                  (27)


=  k           Σ f (i|V

1:t−1

)                                        (28)

t=1:T i∈V

k

≥ nf (V₁:T )                                                              (29)

For Eq. 26, we use the fact that Sˆ1:T  achieve the top k gains selected by the greedy process in 
each
step. Next, we will bound any solution g(S1∗:T ) by g(V₁:T V₁:T  ₁). Firstly, we will need to 
partition
S₁∗:T  into two parts: (1) for the first part, we collect all the new elements introduced at every 
time step

t that do not exist in S1∗:t−1, i.e., S˜∗     = (S1∗ \ Ø, S2∗ \ ∪(S1∗:1), S3∗ \ ∪(S1∗:2), . . . , 
ST∗  \ ∪(S1∗:T −1)),

where S˜∗ , S∗ \ ∪(S∗     ) and ∪(S₁:t) , S       Si, which is the set union on all elements in the

set minus operation. Therefore, S˜∗     contains every element in S1∗:T  exactly once, i.e., every 
element

in S1∗:T  only appears once in S˜∗    , and at many time steps, S˜∗ might be empty; (2) the other 
part

contains all the rest elements, i.e., S1∗:T  − S˜∗     = (S1∗ \ S˜∗, S2∗ \ S˜∗, . . . , ST∗  \ S˜∗ 
). We bound the


two parts as follows:

1:T                      1                   2                              T

g(S1∗:T ) =   Σ  Σ f (i|S1∗:t−1)                                                                    
     (30)

=   Σ  Σ f (i|S1∗:t−1) +  Σ     Σ    f (i|S1∗:t−1)                       (31)

		

t=1:T i∈S˜∗                             t=1:T i∈(S∗ \S˜∗ )

≤ Σ f (i) +  Σ     Σ    f (i|S1∗:t−1)                                             (32)

≤      f (i) + f (S1∗:T )                                                                           
  (33)

i∈V


≤ g(V₁:T |V₁:T −₁Σ) + f (V₁Σ:T )

(34)

	

Σ                    ˜∗                                                                    t        
         ∗                                           t

i   V  f (i) since ST  contains one instance of every element in S1:T  and removing the 
conditioning

part  would  make  the  gains  larger  (guaranteed  by  diminishing  return  property).   To  get  
Eq.  33,
we reduce the conditioning part of f (i|S1∗:t−1) in Eq. 32 by using the following inequality:  for
A₁  ⊆ A₂  ⊆ V , denote A₃  =  A₂ \ A₁  and let A₁  =  {i₁, i₂, · · · , i|A1 |}, by diminishing 
return
property of f (·), we have:

f (i|A₂) ≤ f (i₁|A₃) + f (i₂|{i₁} ∪ A₃) + f (i₃|{i₁, i₂} ∪ A₃)+

i∈A1

. . . + f (i|A1 ||A₂ \ {i|A1 |}) = f (A₂|A₃).                                        (35)

15


Under review as a conference paper at ICLR 2020

According to the pre-defined partition, we pick out the first occurrence of every element into S˜∗  
  ,

every remaining element i     (St∗    S˜∗) is guaranteed to find itself in its conditioning history 
S1∗:t   1
and therefore, we may use the inequality described in Eq. 35 to bound the second term in Eq. 32
by f (S1∗:T ) (letting A₁  = St∗     S˜∗  and A₂  = S1∗:t   1  and applying the inequality from t = 
1 to T

sequentially). To make it more concrete, for example, at step t = 2, by using Eq. 35, we have:


i   (SΣ2∗  S˜∗ )

f (i|S1∗) ≤ f (i₁) + f (i₂|i₁) + f (i₃|i₁, i₂)

+ . . . + f (i|  ∗   ˜∗  |S2∗ \ S˜∗ \ {i   ∗   ˜∗  })                          (36)


At time step t = 3, we have:

= f (S2∗ \ S˜∗);                                                                      (37)

Σ    f (i|S1∗:2) ≤ f (i₁|S2∗ \ S˜∗) + f (i₂|{i₁} ∪ (S2∗ \ S˜∗)) + f (i₃|{i₁, i₂} ∪ (S2∗ \ S˜∗))

+ . . . + f (i|  ∗   ˜∗  |(S3∗ \ S˜∗ \ {i   ∗   ˜∗  }) ∪ (S2∗ \ S˜∗))                           
(38)

= f (S3∗ \ S˜∗|S2∗ \ S˜∗).                                                                          
   (39)

Hence, we have the inequality between Eq. 32 and Eq. 33.

To get Eq. 34 from Eq. 33, we use the fact                                                   
because

contains Σi∈V  f (i) at step t  =  1, and the second term in Eq. 34 is due to the fact that f (·) 
is

Finally, we combine Eq. 29 and Eq. 34, we get 2g(Sˆ1:T |S₁:T −₁) ≥ 2k f (V₁:T ) ≥ k g(S1∗:T ).

By combining Lemma 1 and Lemma 3, we get the second factor  ᵏ  for the bound in Theorem 1.

Remarks.  The first factor  ¹−ᵉ−1   dominates when k is relatively small compared to n.  Recall the
hard case example above on the 1/k factor. We can generalize it to any k < n by (almost) equally
distribute the n elements into the k groups described in the hard case. Then, for n < k², the 
optimal
solution gets n in the end while the greedy solution gets k, so the ratio is  ᵏ . For n ≥ k², the 
optimal
solution still gets k² while the greedy solution gets k, so the ratio is  ¹ . In both scenarios, 
our bounds

match the hard example up to constant factors.

Corollary 1.  f (S     ) +  ¹ C       ≥ 1  max , 1−e−1 ,  ᵏ , f (S∗  )

Proof.

1                1

f (S₁:T ) + k Cf,m ≥ k (g(S₁:T ) + Cf,m)                                                 (40)

1            1 − e−¹    k          ∗

≥ k max{     k      , 2n }g(S1:T )                                (41)

1            1 − e−¹    k          ∗

≥ k max{     k      , 2n }f (S1:T )                                (42)

Remarks.   We will show there is a hard case with the 1/k²  factor.   The same as the hard case
mentioned above for the set cover function, f (Sˆ1) gets a gain of 1 since the selected items can be
totally redundant, and the future gains are all zeros since S₁ select one element from each group.
However,  the optimal solution can still achieve an evaluation of k²  in the end.  Also,  note that
Theorem 1 is true for any solution S₁∗:T  and the optimal solution for g and the optimal solution 
for f
can be different.

We mentioned a few weighted sampling method to replace the greedy step. Here, we apply a random
sampling procedure similar to the lazier-than-lazy approach Mirzasoleiman et al. (2015): we sample
a subset Rj ⊆ V  \ St,j−₁ of size ⁿ log ¹ , and then choose the top-gain element from Rj and add it

to St,j−₁ to from St,j. We denote such sampling based greedy as DIHCL-Greedy-random.

16


Under review as a conference paper at ICLR 2020

Theorem 2.  For         V              on ground set     with DR property, compared to any solution 
   ∗  ,

S₁:T , the solution of D≥IHCL-Greedy-random, achieves

E                                                     1 − ϵ  k   1 − e−¹       ∗

[g(S₁:T )] + Cf,m ≥ (1 − (1 −    k    )  )      k      g(S1:T )                          (43)

(1 − e−¹ − ϵ)(1 − e−¹)       ∗

≥                k                   g(S1:T ).                              (44)

Proof.  We can think the selection of every St is a greedy process of k steps, with St as the 
optimal
solution. Suppose up to step j, we select the set St,j. We first bound the probability that the 
sampled
set  has some intersection with the optimal set St.


Pr[Rj

∩ (St

\ St,j−1)

]      1     (1      |St \ St,j−1| )|R|                     (45)

|V  \ St,j−1|

≥ 1 − (1 − |St \ St,j−1| )|R|                     (46)

1 − e− |R| |St\St,j−1 |                                          (47)


≥           n

R  k      St    St,j   1

n

k

(48)

(49)

In step j, we denote the selected item by vj. We can then get the expected gain given the 
probability
that there is some intersection:

       1         Σ

E[f (v  |ζ(v  , S        ))] ≥ Pr[R   ∩ (S  \ S    ) /= Ø]                          f (i|ζ(i, S     
   ))          (50)


=  1 − ϵ        f (i ζ(i, S
k

i∈St

1:t−1

))                                                           (51)

Again, we get the argument that we are reducing the gap to the optimal solution by (1     ϵ)/k for
every selected item vj on expectation.


Σ  E[f (v  |ζ(v  , S

))] ≥ (1 − (1 − 1 − ϵ )ᵏ) Σ E[f (i|ζ(i, S

))]             (52)

We can then apply Eq. 52 in the Eq. 14 of Lemma 1, and get


E[g(S

)] + C

≥ (1 − (1 − 1 − ϵ )ᵏ)E[g(Sˆ     |S

)]                         (53)

Combine with Lemma 2 we get the bound in Theorem 2.

Remarks. When n is large and n   k, we can approximate the sample without replacement using
sample with replacement, and we can independently sample k subsets each of size |R| at every time

step to generate S  . In such a case, the bound becomes E[g(S     )] + C       ≥ 1−e−1 −s g(S∗  ).


k

Similarly, we can also get the expectation bound on f :

1:T

f,m              k

1:T


Corollary 2.  E[f (S

)] +  ¹ C

≥ (1 − (1 − 1−s )ᵏ) ¹−ᵉ−1 f (S∗  )


Proof.

E[f (S

1:T

1

)] +    C

k

f,m

1

≥ k (E[g(S

1:T

)] + C

f,m

)                                              (54)

1 − ϵ  k   1 − e−¹       ∗

≥ (1 − (1 −    k    )  )     k2        g(S1:T )                         (55)

1 − ϵ  k   1 − e−¹        ∗

≥ (1 − (1 −    k    )  )     k2        f (S1:T )                        (56)

17


Under review as a conference paper at ICLR 2020

We can extend the setting so that we get noisy feedback from the gains of function f : f (i S₁:t  
₁)+αt,
and the problem becomes a multi-armed bandit problem. Specifically if we assume the noise at form
a martingale difference sequence, i.e. E[αt α₁, α₂, . . . , αt  ₁] = 0 and all αt are bounded αt    
 σ
and if we make further assumption about the smoothness of the f and g function (assume the gains

of f and g have RKHS-norm bounded by value B with some kernel k),√we can utilize the contextual

bandit UCB algorithm proposed in Krause & Ong (2011) to get a     T  dependent regret.  Also,

under the noise setting, the contextual information becomes crucial, as the function has 
DR-property,
and without an estimate of how much the gain decreases, we cannot have a better estimate of the
upper bound on the noise term. However, we note that utilizing the contextual information involves
calculating large kernel matrices, which is not feasible for our purpose of efficient curriculum 
learning.
We include the following result for completeness.

Theorem 3.  For f  : ZV    → R≥₀ on ground set V  with DR property, suppose the gain of function

0

g has RKHS-norm bounded by value B with some kernel k), and the noise αt’s from a martingale

difference sequence: E[αt α₁, α₂, . . . , αt  ₁] = 0 and all αt are bounded  αt      σ.  We define 
the
maximum information gain if we have the perfect information about f, ρT = maxA1:T  H(yA1:T )
H(yA1:T   f ), where H is the Shannon entropy, and yA1:T   =   f (i A₁:t  ₁) + at i     At, t = 1 : 
T
denotes the collection of gain values we get from the sequence of A₁:T . We get the following regret
bound:

1 − e−¹       ∗                               √

Where, C = 8/ log(1 + σ−²), βT = 2B² + 300ρT ln³(T/δ).

Proof.  The proof directly utilizes the third case of Theorem 1 in Krause & Ong (2011), using the
history sequence (S₁, S₂, . . . , St) as the context:

1 − e−¹       ∗                ˆ                         √

1 − e−¹       ∗                               √

B    DYNAMIC  INSTANCE  HARDNESS  (CONT.)

Figure 4: LEFT: Entry Ai,j  (i < j) is the percentage of shared samples between the top-10k samples 
with
the largest DIH computed in epoch 15i and epoch 15j RIGHT: Entry Ai,j  (i < j) is the percentage of 
shared
samples between the top-10k samples with the smallest DIH computed in epoch 15i and epoch 15j. It 
shows that
both the forgettable and memorable samples in the future are predictable by using the DIH values in 
early epochs.

Firstly, we present a quantitative verification of the second observation in Section 2, i.e., dy-
namic instance hardness in early stages can predict later dynamics. It tries to predict the samples

18


Under review as a conference paper at ICLR 2020

with large/small DIH values in the future by only using the DIH computed on early training history.
In Figure 4, we show two upper triangle matrices quantitatively verifying the above statement. They
are computed based on the results of the CIFAR10 training experiment in Section 2. Take the matrix
A in the left plot for example, given Ui, the top-10k samples with the largest DIH values computed
in epoch 15i, and Uj for any j > i, the entry Ai,j = |Ui∩Uj |/10000. Similarly, the matrix in the 
right
plot measures the same overlapping percentage for the top-10k samples with the smallest DIH values
between epoch 15i and epoch 15j.  They show that after a few first epochs, DIH can accurately
predict the forgettable and memorable samples in the future. This verifies the second statement we
made in Section 2. In addition, they also show that |Ui∩Uj |/10000 between consecutive epochs 15i 
and
15j is close to 100%, which indicates that DIH is a stable and smoothly changed metric with high
consistency across training trajectory.


Epoch 10

10k samples with largest DIH
10k samples with smallest DIH
the remaining 30k samples

Epoch 10


Epoch 40

10k samples with largest DIH
10k samples with smallest DIH
the remaining 30k samples

Epoch 40


Epoch 60

10k samples with largest DIH
10k samples with smallest DIH

the remaining 30k samples

Epoch 60

Figure 5: LEFT: Averaged prediction-flip and RIGHT: losses (mean and std.) of the three groups of 
samples
partitioned by a DIH metric (i.e.,running mean of prediction-flip) computed at epoch 10,40 and 60 
during training
WideResNet-28-10 on CIFAR10 with random labels. In this setting, the random (but wrong) labels will 
be re-
membered       very well after some training, and DIH in early stages loses the capability to 
predict the future DIH, i.e.,
they   can only reflect the history but not the future. This characteristic of DIH might be helpful 
to detect noisy data.


Epoch 10

Epoch 40

10k samples with largest DIH
10k samples with smallest DIH
the remaining 30k samples

learning rate (rescaled)

10k samples with largest DIH
10k samples with smallest DIH
the remaining 30k samples

learning rate (rescaled)

10k samples with largest DIH
10k samples with smallest DIH
the remaining 30k samples

Epoch 140learning rate (rescaled)

10k samples with largest DIH
10k samples with smallest DIH
the remaining 30k samples

learning rate (rescaled)

Epoch 210

Figure 6: LEFT: Averaged prediction-flip and RIGHT: losses (mean and std.) of the three groups of 
samples
partitioned by a DIH metric (i.e.,running mean of prediction-flip) computed at epoch 10, 40, 140 
and 210 during
training a smaller CNN on CIFAR10.  It shows that the difference of memorable and forgettable 
samples is
not sufficiently obvious until very late training epochs, e.g., after epoch-140.

Secondly, we conduct an empirical study of dynamic instance hardness during training a neu-
ral net on very noisy data, as studied in (Zhang et al., 2017) and (Arpit et al., 2017). In 
particular,
we replace the ground truth labels of the training samples by random labels, and apply the same
training setting used in Section 2. Then, we compute the running mean of prediction-flip for each
sample at some epoch (i.e., 10, 40, 60), and partition the training samples into three groups, as we
did  to generate Figure 1. The result is shown in Figure 5. It shows 1) the group with the smallest

19


Under review as a conference paper at ICLR 2020

prediction flip over history (left plot) is possible to have large but unchanging loss as shown in
the right plot; and 2) the DIH in this case can only reflect the history but cannot predict the 
future.
However, it also indicates that the capability of DIH to predict the future is potential to be an 
effective
metric to distinguish noisy data or adversarial attack from real data. We will discuss it in our 
future
work.

Thirdly, we change the WideResNet to a much smaller CNN architecture with three convolu-
tional layers2. We apply the same training setting used in Section 2. Then, we compute the running
mean of prediction-flip for each sample at some epoch (i.e., 10, 40, 140, 210), and partition the
training samples into three groups, as we did to generate Figure 1. The result is shown in Figure 6.
Compared to DIH of training deeper and wider neural nets shown in Figure 1, the memorable and
forgettable samples are indistinguishable until very late stages, e.g., Epoch-140. This indicates 
that
using DIH in earlier stage to select forgettable samples into curriculum might not be reliable when
training small neural nets. We will leave explanation of this phenomenon to our future works.

Figure 7: Top: DIH (running mean of loss) vs. Bottom: instantaneous loss of 50 randomly selected 
samples
from CIFAR10 when training WideResNet-28-10.  It shows that for each individual sample, DIH smoothly
decreases while the corresponding instantaneous loss is much noisier.

Moreover, we provide a comparison of the smoothness between DIH and instantaneous loss on
individual samples in Figure 7.  It shows that the DIH is a smooth and consistent measure of the
learning/memorization progress on individual samples. In contrast, the frequently used instantaneous
loss is much noisier, so selecting training samples according to it will lead to unstable behaviors
during training. In Figure 8, we also provide a comparison of DIH and instantaneous loss on the two
groups of samples in Figure 2, which shows a similar phenomenon.

C    EXPERIMENTS  (CONT.)

We use cosine annealing learning rate schedule for multiple episodes. The switching epoch between
each two consecutive episode for different datasets are listed below.

•  CIFAR10, CIFAR100: (5, 10, 15, 20, 30, 40, 60, 90, 140, 210, 300);

•  Food-101, Birdsnap, FGVCaircraft, StanfordCars: (10, 20, 30, 40, 60, 90, 150, 240, 400);

•  ImageNet: (5, 10, 15, 20, 30, 45, 75, 120, 200);

•  STL10: (20, 40, 60, 80, 120, 160, 240, 360, 560, 840, 1200);

•  SVHN: (5, 10, 15, 20, 30, 40, 60, 90, 140, 210, 300);

•  KMNIST, FMNIST: (5, 10, 15, 20, 30, 40, 60, 90, 140, 210, 300);

We report how the test accuracy changes with the number of training batches for each method, and
the wall-clock time for all the 11 datasets in Figure 9-12.

²The “v3” network from https://github.com/jseppanen/cifar_lasagne.

20


Under review as a conference paper at ICLR 2020

Figure 8: Top: DIH (running mean of loss) vs. Bottom: instantaneous loss of 10 samples randomly 
selected
from the top 10k samples with the largest(red) and the smallest(blue) DIH at epoch 40 of training 
of WideResNet-
28-10 on CIFAR10 (the same as Figure 2. It shows that for each individual sample from the two 
groups, DIH
smoothly decreases while the corresponding instantaneous loss is much noisier.

Table 2: Details regarding the datasets and training settings (#Feature denotes the number of 
features
after cropping if applied), “lr_start” and “lr_target” denote the starting and target learning rate 
for the
first episode of cosine annealing schedule, they are gradually decayed over the rest episodes.

Dataset         CIFAR10     CIFAR100       Food-101         ImageNet         STL10          SVHN

#Training        50000           50000              75750             1281167            5000       
     73257

#Test               10000           10000              25250               50000              8000  
          26032

#Feature      (3, 32, 32)     (3, 32, 32)     (3, 224, 224)    (3, 224, 224)    (3, 96, 96)    (3, 
32, 32)

#Class                10                 100                  101                  1000             
    10                 10

#Epoch T          300                300                  400                   200                
1200              300

BatchSize         128                128                   80                    256                
 128               128

lr_start          2 × 10−¹      2 × 10−¹        2 × 10−¹          2 × 10−¹        2 × 10−¹      2 × 
10−²

lr_target       5 × 10−⁴      5 × 10−⁴        1 × 10−⁴          1 × 10−⁴        5 × 10−⁴      1 × 
10−³

Table 3: Details regarding the datasets and training settings (cont.)

Dataset           Birdsnap       FGVCaircraft    StanfordCARs     KMNIST      FMNIST

#Training          47386                 6667                    8144                50000          
 50000

#Test                  2443                  3333                    8041                10000      
     10000

#Feature      (3, 224, 224)     (3, 224, 224)      (3, 224, 224)      (1, 28, 28)    (1, 28, 28)

#Class                 500                    100                      196                    10    
             10

#Epoch T            400                    400                      400                   300       
        300

BatchSize           258                    256                      256                   128       
        128

lr_start            4 × 10−¹           4 × 10−¹            4 × 10−¹         4 × 10−²      4 × 10−²

lr_target         1 × 10−⁴           1 × 10−⁴            1 × 10−⁴         1 × 10−³      1 × 10−³

21


Under review as a conference paper at ICLR 2020

Figure 9: Training DNNs by using DIHCL (and its variants), SPL (Kumar et al., 2010), MCL (Zhou & 
Bilmes,
2018), and random mini-batch SGD on 3 datasets, i.e., CIFAR10, CIFAR100 and STL-10. We use 
“Diverse” to
denote DIHCL that further reduces St  by applying submodular maximization for Eq. (5). We report 
how the test
accuracy changes with the number of training batches for each method, and the (log-scale) 
wall-clock time for

1) the entire training and 2) the submodular maximization part in DIHCL with diversity and MCL.

22


Under review as a conference paper at ICLR 2020

Figure 10: Training DNNs by using DIHCL (and its variants), SPL (Kumar et al., 2010), MCL (Zhou & 
Bilmes,
2018), and random mini-batch SGD on 3 datasets, i.e., SVHN, Fashion MNIST and Kuzushiji MNIST. We
use “Diverse” to denote DIHCL that further reduces St  by applying submodular maximization for Eq. 
(5). We
report how the test accuracy changes with the number of training batches for each method, and the 
(log-scale)
wall-clock time for 1) the entire training and 2) the submodular maximization part in DIHCL with 
diversity and
MCL.

23


Under review as a conference paper at ICLR 2020

Figure 11: Training DNNs by using DIHCL (and its variants), SPL (Kumar et al., 2010), MCL (Zhou & 
Bilmes,
2018), and random mini-batch SGD on 3 datasets, i.e., ImageNet, Food-101 and Birdsnap. We report 
how the
test accuracy changes with the number of training batches for each method, and the wall-clock time 
for 1) the
entire training and 2) the submodular maximization part in MCL.

24


Under review as a conference paper at ICLR 2020

Figure 12: Training DNNs by using DIHCL (and its variants), SPL (Kumar et al., 2010), MCL (Zhou & 
Bilmes,
2018), and random mini-batch SGD on 2 datasets, i.e., FGVC Aircraft and Stanford Cars. We report 
how the
test accuracy changes with the number of training batches for each method, and the wall-clock time 
for 1) the
entire training and 2) the submodular maximization part in MCL.

25

