Under review as a conference paper at ICLR 2020
Certifiably Robust Interpretation in Deep
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Deep learning interpretation is essential to explain the reasoning behind model
predictions. Understanding the robustness of interpretation methods is important
especially in sensitive domains such as medical applications since interpretation
results are often used in downstream tasks. Although gradient-based saliency
maps are popular methods for deep learning interpretation, recent works show
that they can be vulnerable to adversarial attacks. In this paper, we address this
problem and provide a certifiable defense method for deep learning interpretation.
We show that a sparsified version of the popular SmoothGrad method, which com-
putes the average saliency maps over random perturbations of the input, is certi-
fiably robust against adversarial perturbations. We obtain this result by extending
recent bounds for certifiably robust smooth classifiers to the interpretation setting.
Experiments on ImageNet samples validate our theory.
1	Introduction
1.1	Motivation
The growing use of deep learning in a wide range of highly-sensitive applications, such as au-
tonomous driving, medicine, finance, and even the legal system (Teichmann et al., 2018; Quellec
et al., 2017; Fischer & Krauss, 2018; Nissan, 2017), raises concerns about human trust in machine
learning systems. Interpretations, which explain why certain predictions are made, are critical for
establishing this trust between users and the machine learning system. Moreover, the interpreta-
tion results themselves are often used for downstream tasks. For example, gradient-based saliency
maps have been used in medical applications: Quellec et al. (2017) uses gradient-based interpreta-
tion techniques to highlight signs of retinal disease in retinal fundus photographs (see Figure 1-b.)
Similarly, BenTaieb et al. (2016) uses a gradient-based saliency map as part of a pipeline to auto-
matically analyse histopathology slides of colon tumours. Techniques like these are also used in
medical research to annotate datasets (Lahiani et al., 2019). Outside of the medical field, gradient-
based interpretations can be used in general-purpose image segmentation tasks (Hong et al., 2015).
However, Ghorbani et al. (2019) has shown that several gradient-based interpretation methods are
sensitive to adversarial examples, obtained by adding a small perturbation to the input image. These
adversarial examples maintain the original class label while greatly distorting the saliency map (Fig-
ure 1-a: see ‘Gradient’ interpretation method). Although adversarial attacks and defenses on image
classification have been studied extensively in recent years (Szegedy et al., 2014; Uesato et al., 2018;
Goodfellow et al., 2015; Athalye et al., 2018a;b; Buckman et al., 2018; Kurakin et al., 2017; Paper-
not & McDaniel, 2016; Carlini & Wagner, 2017; Madry et al., 2018), less attention has been paid
to effectively defending deep learning interpretation against adversarial examples (Lee et al., 2019;
Etmann et al., 2019). This is partially due to the difficulty of protecting high-dimensional saliency
maps compared to defending a class label, as well as to the lack of a ground truth for interpretation.
Interestingly, we have observed that the standard adversarial training used for the classification ro-
bustness does not lead to robust interpretations (see Section 4). This calls for better understanding
the robustness of interpretation methods and developing defenses with performance guarantees.
1
Under review as a conference paper at ICLR 2020
Interpretation
Method
(a)
Image
Gradient:
Interpretation of
Unperturbed Image
Adversarial Attack
on Interpretation
Interpretation after
Adversarial Attack
(b)
Proposed
Method:
Medical Image
(Retinal Fundus )
(c)
Gradient-Based interpretation
Showing Evidence of Disease
0.1
0.0
0.6
5 4 3 2
Ci 6 6 6
① leu≡t①ωSS ① UlSnqo=
Sparsification Parameter
Figure 1: (a) An illustration of the sensitivity of gradient-based SalienCy maps to an adversarial
perturbation of an image from CIFAR-10. Our proposed method, Sparsified SmoothGrad (relaxed
variant shown here; see Section 2.3), produces saliency maps which considerably are more robust to
adversarial attack than plain gradient saliency maps. (b) A medical use of gradient-based saliency
maps. Figures are borrowed from Quellec et al. (2017). Signs of lesions indicative of diabetic
retinopathy are automatically highlighted. (c) A comparison of robustness certificate values Rcert/K
of Sparsified SmoothGrad vs. scaled SmoothGrad, on ImageNet samples.
1.2	Proposed Approach and Main Contributions
In the last couple of years, several approaches have been proposed for interpreting neural network
outputs (Simonyan et al., 2014; Sundararajan et al., 2017; Alvarez-Melis & Jaakkola, 2018; Adebayo
et al., 2018; Huang et al., 2017). Specifically, Simonyan et al. (2014) computes the elementwise
absolute value of the gradient of the largest class score with respect to the input. To define some
notation, let g(x) be this most basic form of the gradient-based saliency map, for an input image
x ∈ Rn. For simplicity, we also assume that elements of g(x) have been linearly normalized to be
between 0 and 1. g(x) represents, to a first order linear approximation, the importance of each pixel
in determining the class label. Numerous variations of this method have been introduced, which we
review in the appendix.
A popular saliency map method which extends the basic gradient method is SmoothGrad (Smilkov
et al., 2017), which takes the average gradient over random perturbations of the input. Formally, we
define the smoothing function as:
g(x) ：= E [g(x + e)],	(1.1)
where e has a normal distribution (i.e. e 〜N(0, σ21)). We will discuss other smoothing functions in
Section 2.3 while the empirical smoothing function which computes the average over finitely many
2
Under review as a conference paper at ICLR 2020
perturbations of the input will be discussed in Section 2.2. We refer to the basic method described
in the above equation as the scaled SmoothGrad 1.
Since a ground truth label for interpretation is not available, we use a similarity metric between
the original and perturbed saliency maps as an estimate of the interpretation robustness. Ghorbani
et al. (2019) introduced a top-K overlap metric for this purpose: define R(x, x, K) as the number
of overlapping elements between top K largest elements of saliency maps of x and the perturbed
image x. Ghorbani et al. (2019) developed an L∞ attack on R(x, x, K), which we adapt to the L?
case to test empirical robustness: see the appendix for details. This top-K overlap metric is naturally
motivated: absolute magnitude has little meaning in saliency maps, which are nearly always pre-
sented as normalized. Moreover, it is common (Sundararajan et al., 2017) to clip the highest values
when displaying saliency maps, so that more than a few pixels are clearly visible. This suggests that
relative ranks of salience values (and not their absolute values) are important for interpretation.
Note that for an input x, R(x, x, K) depends on the specific perturbation x. We define R* (x, K)
as the robustness measure with respect to the worst perturbation of x. That is,
R*(x, K) ：= min R(x, x, K)
(1.2)
X
IlX- x∣∣2 ≤ ρ.
For deep learning models, this optimization is non-convex in general. Thus, characterizing the true
robustness of interpretation methods is likely intractable for most modern classifier models.
In this paper, we show that a lower bound on the true robustness value of an interpretation method
(i.e. a robustness certificate) can be computed efficiently. In other words, for a given input x, we
compute a robustness certificate Rcert such that Rcert(x, K) ≤ R* (x, K). To establish the robustness
certificate for saliency map methods, we prove the following result for a general function h(.) whose
range is between 0 and 1:
Theorem 2. Let h(x) be the output ofan interpretation method whose range is between 0 and1 and
let h be its smoothed version defined as in equation 1.1. Let hi(x) and h团(x) be the i-th element
and the i-th largest elements of h(x), respectively. Let Φ be the cdfofthe normal distribution. If
φ (φ-1(h[i](X))- 2ρ)≥ h[2K-i](x),
(1.3)
then for the smoothed interpretation method, we have Rcert(x, K) ≥ i.
Intuitively, this means that, if there is a sufficiently large gap between the i-th largest element of
the smoothed saliency map and its (2K - i)-th largest element, then we can certify that at least i
elements in the top K largest elements of the original smoothed saliency map will also be in the
top K elements of adversarially perturbed saliency map. We present a more general version of
this result with empirical expectations for smoothing in Section 2, as well as another rank-based
robustness certificate in Section 3. The proof of this bound relies on an extension of Cohen et al.
(2019) which addresses certified robustness in the classification case. Proofs for all theorems are
given in the appendix.
Evaluating the robustness certificate for the scaled SmoothGrad method on ImageNet samples pro-
duces vacuous bounds (Figure 1-c). This motivated us to develop variations of SmoothGrad with
larger robustness certificates. One such variation is Sparsified SmoothGrad which is defined by
smoothing a sparsification function that maps the largest elements of g(x) to one and the rest to
zero. Sparsified SmoothGrad obtains a considerably larger value of the robustness certificate (Figure
1-c) while producing high-quality saliency maps (Figure 3). We study other variations of Sparsified
SmoothGrad in Section 2.
A considerably different certification result for robust interpretation is provided by Lee et al. (2019).
That work provides a certified L2 radius in which a first-order gradient saliency map is exactly iden-
tical to the saliency map for the unperturbed image. The method depends on the network structure
being locally linear (ReLU-based): the inner radius of the locally linear region is directly calculated.
1 The original definition of SmoothGrad does not normalize and take the absolute values of gradient elements
before averaging. We start with the definition of equation 1.1 because it can more easily be compared to our
certifiably robust interpretation scheme discussed in Section 2.
3
Under review as a conference paper at ICLR 2020
By contrast, our method makes no demands on the structure of the classifier, and could be applied to
saliency maps generated by methods other than computing simple gradients: any bounded saliency
function can be used as h(x) in our Theorem 1. Additionally, our certification method is computa-
tionally inexpensive enough to allow certificates to be computed on ImageNet-scale samples.
2 Smoothing for Certifiably Robust Interpretation
2.1	Notation
We introduce the following notations to indicate Gaussian smoothing: for a function h, we define
population and empirical smoothed functions, respectively, as:
h(x) =E.N (0,σ2i) [h(x + e)],
1q
h(x) =-∑ h(x + ei), ei 〜N(0,σ2I).
q i=1
(2.1)
In other words, h(x) represents the expected value of h(x) when smoothed under normal pertur-
bations of e with some standard deviation σ while h(x) represents an empirical estimate of h(x)
using q samples. We call σ2 the smoothing variance and q the number of smoothing perturbations.
We use vi to denote the ith element of the vector v. Similarly hi(x) denotes the ith element of the
output h(x). We also define, for any h(x), rank(h(x), i) as the ordinal rank of hi(x) in h(x) (in
the descending order): rank(h(x), i) = j denotes that hi(x) is the jth largest element in h(x). We
use x[i] to denote the ith largest element in x. Ifi is not an integer, the ceiling ofi is used.
2.2	Robustness Certificate
In order to derive a robustness certificate for saliency maps, we present an extension of the classifi-
cation robustness result of Cohen et al. (2019) to real-valued functions, rather than discrete classifi-
cation functions. In our case, we will apply this to the saliency map vector g. First, we define a floor
function to simplify notation.
Definition 2.1. The Floor function is a function L : [0,1] → [0,1], such that
L(Z) = Φ (φ-1 (z) - 2ρ)
where ρ denotes the L2 norm of the adversarial distortion and σ2 denotes the smoothing variance.
Φ is the cdf function for the standard normal distribution and Φ-1 is its inverse.
Below (Theorem 1) is a general result which can be used to derive robustness certificates for inter-
pretation methods. In particular, it is used to derive our robustness certificate, Theorem 2, later in
the section:
Theorem 1. Let h : Rn → [0, 1]n be a real-valued function. Let L(.) be the floor function defined
as in equation 2.1 with parameters σ2 and ρ. Using σ2 ∈ R as the smoothing variance for h,
∀ i,j ∈ [n], x, X ∈ Rn where IIX - X∣∣ ≤ P:
L (hi(x)) ≥ hj(x) ⇒ hi(X) ≥ hj(X).
Note that this theorem is valid for any general function. However, we will use it for our case where
h(χ) is a smoothed saliency map. Theorem 1 states that, for a given saliency map vector h(x), if
L(h i(x)) ≥ hj (x), then if X is perturbed inside an L? norm ball of radius at most ρ, h i(X) ≥ hj (x).
This result extends Theorem 1 in Cohen et al. (2019) in two ways: first, it provides a guarantee
about the difference in the values of two quantities, which in general might not be related, while the
original result compared probabilities of two mutually exclusive events. Second, we are considering
a real-valued function h, rather than a classification output which can only take discrete values. This
bound can be compared directly to Lecuyer et al. (2019)’s result which similarly concerns unrelated
elements in a vector. Just as in the classification case (as noted by Cohen et al. (2019)), Theorem 1
gives a significantly tighter bound than that of Lecuyer et al. (2019) (see details in the appendix).
4
Under review as a conference paper at ICLR 2020
0.02	0.05	OAQ	0.20	0.50
Sparsification Parameter
0.02	0.05	0.10	0.20	0.50	1.00
Sparsification Parameter
(a) Sparsified SmoothGrad	(b) Relaxed Sparsified SmoothGrad
Figure 2: Certified robustness bounds on ImageNet for different values of the sparsification param-
eter τ . The lines shown are for the 60th percentile guarantee, meaning that 60 percent of images
had guarantees at least as tight as those shown. For both examples, K = 0.2n, and ρ = 0.03 (in units
where pixel intensity varies from 0 to 1.)
We can extend Theorem 1 to use empirical estimates of smoothed functions. Following Lecuyer
et al. (2019), We derive upper and lower bounds of the expected value function h(x) in terms of
h(x), by applying Hoeffding’s Lemma. To present our result for the empirical case, we first define
an empirical floor function to derive a similar lower bound when the population mean is estimated
using a finite number of samples:
Definition 2.2. The Empirical Floor function is a function L : [0,1] → [0,1], such that for given
values of ρ, σ, p, q, n, where ρ denotes the maximum L2 distortion, σ2 denotes the smoothing
variance, p denotes the probability bound, q denotes the number of perturbations, and n is the size
of input of the function:
L(Z) = Φ (φ-1 (z - C)- 2p)) - c, where C ：= √ln(2n(； - P)-II.
Corollary 1. Let h ： Rn → [0,1]n be afunction such thatfor given values of q, σ, ∀ i,j ∈ [n], x, X ∈
Rn, IIX - XIl 2 ≤ ρ, with probability at least P,
L(hi(x)) ≥ hj(x) ⇒ hi(X) ≥ hj(X).	(2.2)
Note that unlike the population case, this certificate bound is probabilistic. Also note that this bound
compares the empirical expectation of the saliency map at an observed point to the true population
expectation at all nearby points. This convention is in line with recent works providing probabilistic
certificates for smoothed classifiers (Cohen et al., 2019; Salman et al., 2019).
Theorem 1	allows us to derive certificates for the top-K overlap (denoted by R). In particular:
Theorem 2.	∀ X, X ∈ Rn, ∣∣x - X∣∣2 ≤ ρ, σ ∈ R, q ∈ N, define Rcert(x, K) as the largest i ≤ K such
that L(h[i] (X)) ≥ h[2K-i] (X). Then, with probability at least P,
Rcert(x, K) ≤ R(x, X, K).	(2.3)
where R(x, x, K) denotes the top-K overlap between h(x) and h(x).
Intuitively, if there is a sufficiently large gap between the ith and (2K - i)th largest elements of an
empirical smoothed saliency map, then we can certify that the overlap between top K elements of
the observed saliency map and any possible perturbed population smoothed saliency map is at least
i with probability at least P. If the observed saliency map is possibly adversarially corrupted up to a
radius of ρ, it is thus still guaranteed to be similar to the “ground truth” population saliency map.
5
Under review as a conference paper at ICLR 2020
Note that it requires some modifications of SmoothGrad (Smilkov et al., 2017) for our bounds to
be directly applicable. Smilkov et al. (2017) in particular defines two methods, which we will call
SmoothGrad and Quadratic SmoothGrad. SmoothGrad takes the mean over samples of the signed
gradient values, with absolute value typically taken after smoothing for visualization. Quadratic
SmoothGrad takes the mean of the elementwise squares of gradient values. Both methods, therefore,
require modification for our bounds to be applied: We define scaled SmoothGrad g(x), such that
g(x) is the elementwise absolute value of the gradient, linearly scaled so that the largest element is
one. We can similarly define a scaled Quadratic SmoothGrad. We can then apply Theorem 2 directly
to scaled SmoothGrad (or scaled Quadratic SmoothGrad), simply by scaling the components of g(x)
(or g(x) Θg(x)) to lie in the interval [0,1]. However, we observe that this gives vacuous bounds for
both of them When using the suggested hyperparameters from Smilkov et al. (2017). One issue is that
the suggested value for q (number of perturbations) is 50 which is too small to give useful bounds in
Corollary 1. For a standard size image from the ImageNet dataset (n = 224 × 224 × 3 = 150, 528),
with p = 0.95, this gives c = 0.395 (using Definition 2.2). Note that even for a small ρ:
L(z) = Φ (φ-1 (z - c) - P)) - C ≈ Φ (Φ-1 (z - C)) - C = Z - 2c
Thus the gap between z and L(z) is at least 0.79. We can see from Corollary 1 that agap of 0.79 (on
a scale of 1) is far too large to be of any practical use. We instead take q = 213, which gives a more
manageable estimation error of C = 0.031. However, we found that even with this adjustment, the
bounds computed using Theorem 2 are not satisfactory for either scaled SmoothGrad and or scaled
Quadratic SmoothGrad (see details in the appendix). This prompted the development of Sparsified
SmoothGrad described in Section 2.3.
2.3 Sparsified SmoothGrad and its Relaxation
Scaled SmoothGrad and Quadratic SmoothGrad give vacuous robustness certificates using our pro-
posed method, as demonstrated in Figure 1. We therefore develop a new method, Sparsified Smooth-
Grad, which has (1) non-vacuous robustness certificates at ImageNet scale (Figure 2a), (2) similar
high-quality visual output to SmoothGrad (Figure 3), and (3) theoretical guarantees that aid in set-
ting its hyper-parameters (Section 3).
The Sparsified SmoothGrad is defined as g[τ], where g[τ] is defined as follows:
g[τ](x)={0, ifgi(x)<g[τn](x)	(24)
i	1, if gi(x) ≥ g[τ n] (x)	.
In other words, τ controls the degree of sparsification: a fraction τ of elements (the largest τn
elements of g(x)) are assigned to 1, and the rest are set to 0.
For some applications, it may be desirable to have at least some differentiable elements in the com-
puted saliency map. For this purpose, we also propose Relaxed Sparsified SmoothGrad:
/0,	if gi(x) < g[τn](x)
g[γ,τ ](x) = /1,	if gi(x)≥ g[γn](x)	(2.5)
I gi(X)、, otherwise
I g[γn] (x) ,
Here, τ controls the degree of sparsification and γ controls the degree of clipping: a fraction γ of
elements are clipped to 1. Elements neither clipped nor sparsified are linearly scaled between 0 and
1. Note that Relaxed Sparsified SmoothGrad is a generalization of Sparsified SmoothGrad. With
no clipping (γ = 0), we again achieve nearly-vacuous results. However, with only a small degree of
clipping (γ = 0.01), we achieve results very similar (although slightly worse) than sparsifed Smooth-
Grad; see Figure 2b. We use Relaxed Sparsified SmoothGrad in this paper to test the performance
of first-order adversarial attacks against Sparsified SmoothGrad-like techniques.
3 Rank Certificate for the Proposed S pars ified SmoothGrad
In this section, we show that if the median rank of a saliency map element over smoothing pertur-
bations is sufficiently small (i.e. near the top rank), then for an adversarially perturbed input, that
6
Under review as a conference paper at ICLR 2020
Base
Image
SmoothGrad
Quadratic
SmoothGrad
Sparsified
SmoothGrad
Figure 3: Qualitative comparison of Sparsified SmoothGrad (with the sparsification parameter τ =
0.1) with the SmoothGrad methods defined by Smilkov et al. (2017). All methods lead to high-
quality saliency maps while our proposed Sparsified SmoothGrad is certifiably robust to adversarial
examples as well. Additional examples have been presented in the appendix.
element will certifiably remain near the top rank of the proposed Sparsified SmoothGrad method
with high probability. This provides a theoretical justification for the certifiable robustness of the
proposed Sparsified SmoothGrad method.
To present this result, we first define the certified rank of an element in the saliency map as follows:
Definition 3.1 (Certified Rank). For a given input x and a given saliency map method (denoted by
h : Rn → Rn), let the maximum adversarial distortion be ρ, i.e. ∣∣X - x∣∣2 ≤ ρ. Then, for a probability
p, the certified rank for an element at index i (denoted by rankcert(x, i)) is defined as the minimum
k such that the following condition holds:
L(hi(x)) ≥ h[k] (x).
If the i-th element of the saliency map has a certified rank of k , using Corollary 1, we will have:
hi (X) ≥ h[k](x)	with probability at least p.
That is, the ith element of the population smoothed saliency map is guaranteed to be as large as the
smallest n - k + 1 elements of the smoothed saliency map of any adversarially perturbed input.
Note that certified rank depends on the particular perturbations used to generate the smoothed
saliency map h(X). In the following result, we show that if the median rank of a gradient ele-
ment at index i, over a set of randomly generated perturbations, is less than a specified threshold
value, then the certified rank of that element in the Sparsified SmoothGrad saliency map generated
using those perturbations can be upper bounded.
Theorem 3. Let U be the set of q random perturbations for a given input X using the smoothing
variance σ2. Using the Sparsified SmoothGrad method, with probability p, we have
[Tnl
Median [rank(g(x + e),i)] ≤ [τn]	⇒ rank (x,i) ≤	,
e∈U	L( 2 )
where τ is the sparsification parameter of the Sparsified SmoothGrad method.
(3.1)
7
Under review as a conference paper at ICLR 2020
For instance, if ρ ≪ σ and for sufficiently large number of smoothing perturbations (i.e. q → ∞),
we have L(1/2) → 1/2. If we set τ = K/(2n), then for indices whose median ranks are less
than or equal to K/2, their certified ranks will be less than or equal to K . That is, even after
adversarially perturbing the input, they will certifiably remain among the top K elements of the
Sparsified SmoothGrad saliency map. We present a more general form of this result in the appendix.
-4- Gradient
—|— SmoothGrad (64 samples)
―|— SmoothGrad (8192 samples)
4— Quadratic SmoothGrad (8192 SamPIeS)
—Relaxed SParSified SmoothGrad (τ = 0.05, 8192 samples)
―|— Relaxed Sparsified SmoothGrad (τ = 0.1, 8192 samples)
Relaxed Sparsified SmoothGrad (τ = 0.2, 8192 samples)
Original Image Baseline Model	Robust Classification Model
X g(χ) X g(X) g(χ) X g(X)
Figure 5: Adversarial training for robust classi-
fication is not effective at producing robust in-
terpretations. We attack the interpretation (us-
ing an L2 attack magnitude ρ = 10) of a base-
Figure 4: Empirical robustness of variants of
SmoothGrad to adversarial attack, tested on
CIFAR-10 with ResNet-18. The attack mag-
nitude is in units of standard deviations of
pixel intensity. Robustness is measured as
R(x, x, K)/K with K = n/4.
line CNN and a CNN trained for robust clas-
sification. The classification-robust model has
an average robustness of 0.5853, compared to
a baseline of 0.5727, over the MNIST test set.
Robustness here is measured as R(x, x, K)/K
with K = n/4.
4	Empirical Results
To test the empirical robustness of Sparsified SmoothGrad, we used an L2 attack on R(x, K)
adapted from the L∞ attack defined by Ghorbani et al. (2019); see the appendix for details of our
proposed attack. We test Relaxed Sparsified SmoothGrad (γ = .01; τ = .05, .1, .2), rather than Spar-
sified SmoothGrad because our attack is gradient-based and Sparsified SmoothGrad has no defined
gradients. We tested on ResNet-18 with CIFAR-10 with the attacker using a separately-trained, fully
differentiable version of ResNet-18, with SoftPlus activations in place of ReLU. We present our re-
sults in Figure 4. We observe that our method is significantly more robust than the SmoothGrad
method while its robustness is on par with the Quadratic SmoothGrad method with the same num-
ber of smoothing perturbations. We note that our robustness certificate appears to be loose for the
large perturbation magnitudes used in these experiments; see the appendix for a direct comparison.
The attack introduced by Ghorbani et al. (2019) (and its L2 extension that we introduced) does
not change the the original classification of the image, so protecting against adversarial attacks
on classification is an insufficient defence. To demonstrate this, we compare a model trained
adversarially for classification with a baseline model. Specifically, we adversarially train a simple
CNN classification model on MNIST (see appendix for model architecture) using a state-of-the-art
L2 adversarial training procedure provided by Rony et al. (2019). We used the code available
for this adversarial training procedure directly. While, as predicted by Tsipras et al. (2018), the
resulting model produced high-quality saliency maps, the model was not significantly more robust
to adversarial perturbation tailored for the interpretation than a naively trained classifier (Figure
5, top-K overlap of 58.5% with adversarial training for classification vs. 57.3% on the baseline).
This highlights that adversarial training for classification does not suffice to provide adversarial
robustness for interpretation. In appendix D, we propose an adversarial training approach which
successfully defends interpretations against adversarial attack (top-K overlap greater than 80%
under the same attack) while also producing high-quality saliency maps. However, unlike the
proposed Sparsified SmoothGrad, this approach is not certifiably robust.
8
Under review as a conference paper at ICLR 2020
5	Conclusion
In this work, we studied the robustness of deep learning interpretation against adversarial attacks.
We introduced a sparsified variant of the popular SmoothGrad method which computes the average
saliency maps over random perturbations of the input. By establishing an easy-to-compute robust-
ness certificate for the interpretation problem, we showed that the proposed Sparsified SmoothGrad
is certifiably robust to adversarial attacks while producing high-quality saliency maps. We provided
experiments on ImageNet samples validating our theory.
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. In Proceedings of the 32Nd International Conference on Neural
Information Processing Systems, NIPS'18,pp. 9525-9536, USA, 2018. Curran Associates Inc.
D. Alvarez-Melis and T. S. Jaakkola. Towards Robust Interpretability with Self-Explaining Neural
Networks. Neural Information Processing Systems, 2018.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML, 2018a.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In ICML, 2018b.
A. BenTaieb, J. Kawahara, and G. Hamarneh. Multi-loss convolutional networks for gland analysis
in microscopy. In 2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI), pp.
642-645, April 2016. doi: 10.1109/ISBI.2016.7493349.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In ICLR. OpenReview.net, 2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320, 2019.
Christian Etmann, Sebastian Lunz, Peter Maass, and Carola Schoenlieb. On the connection between
adversarial robustness and saliency map interpretability. In International Conference on Machine
Learning, pp. 1823-1832, 2019.
Thomas Fischer and Christopher Krauss. Deep learning with long short-term memory networks for
financial market predictions. European Journal of Operational Research, 270(2):654-669, 2018.
Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3681-3688, 2019.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. CoRR, abs/1412.6572, 2015.
Seunghoon Hong, Hyeonwoo Noh, and Bohyung Han. Decoupled deep neural network for semi-
supervised semantic segmentation. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 1495-1503. Curran
Associates, Inc., 2015.
Kan Huang, Chunbiao Zhu, and Ge Li. Robust saliency detection via fusing foreground and back-
ground priors. CoRR, abs/1711.00322, 2017.
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schutt, Sven
Dahne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. In Explainable
AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, 2019.
9
Under review as a conference paper at ICLR 2020
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
CoRR, abs/1607.02533, 2017.
Amal Lahiani, Jacob Gildenblat, Irina Klaman, Nassir Navab, and Eldad Klaiman. Generalis-
ing multistain immunohistochemistry tissue segmentation using end-to-end colour deconvolution
deep neural networks. IETImage Processing, 13(7):1066-1073, 2019.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Guang-He Lee, David Alvarez-Melis, and Tommi S. Jaakkola. Towards robust, locally linear deep
networks. In International Conference on Learning Representations, 2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack and
certifiable robustness. arXiv preprint arXiv:1809.03113, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2018.
Ephraim Nissan. Digital technologies and artificial intelligences present and foreseeable impact on
lawyering, judging, policing and law enforcement. Ai & Society, 32(3):441-464, 2017.
Nicolas Papernot and Patrick D. McDaniel. On the effectiveness of defensive distillation. CoRR,
abs/1607.05113, 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
G Quellec, K Charriere, Y Boudi, B Cochener, and M Lamard. Deep image mining for diabetic
retinopathy screening. Medical image analysis, 39:178, 2017.
Jerome Rony, Luiz G Hafemann, Luiz S Oliveira, Ismail Ben Ayed, Robert Sabourin, and Eric
Granger. Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and
defenses. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4322-4330, 2019.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv
preprint arXiv:1906.04584, 2019.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In Proceedings of the 34th International Conference on Ma-
chine Learning-Volume 70, pp. 3145-3153. JMLR. org, 2017.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. In ICLR, 2014.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
ICML, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2014.
Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, and Raquel Urtasun. Multi-
net: Real-time joint semantic reasoning for autonomous driving. In 2018 IEEE Intelligent Vehicles
Symposium (IV), pp. 1013-1020. IEEE, 2018.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint
arXiv:1805.12152, 2018.
10
Under review as a conference paper at ICLR 2020
Jonathan Uesato, Brendan O'Donoghue, PUshmeet Kohli, and Aaron van den Oord. Adversarial risk
and the dangers of evaluating against weak attacks. In ICML, 2018.
11
Under review as a conference paper at ICLR 2020
A Proofs
Theorem 1.	Let h : Rn → [0,1]n be a bounded, real-valued function, σ2 ∈ R be the smoothing
variance for h, then ∀ i,j ∈ [n], x, X ∈ Rn where X - X = δ such that Mg ≤ P:
φ (φT (hi(X))- 2ρ) ≥ hj(X) ⇒ hi(X) ≥ hj (X)
where Φ denotes the cdf function for the standard normal distribution and Φ-1 is its inverse.
We will prove this by first proving a more general lemma2:
Lemma 1. For any boundedfUnction h ： R → [0,1] and smoothing variance σ2 ∈ R, Φ-1(h(X)) is
Lipschitz-continuous with respect to X, with Lipschitz constant σ-1.
Proof. By the definition of Lipschitz continuity, we must show that ∀δ ∈ Rn,
Φ-1(h(X))-业 ≤ Φ-1(h (x + δ)) ≤ Φ-1(h (x)) +1⅛	(A.1)
σσ
We first define a new, randomized function H : R → {0, 1},
H(x)〜Bern (h(x))
Then ∀ X ∈ Rn :
E [H(x + e)] = Ee [Eh [H(x + e)]] = Ee [h(x + e)] = h(x)	(A.2)
Now, we apply the following Lemma (Lemma 4 from Cohen et al. (2019)):
Lemma (Lemma 4 from (Cohen et al., 2019)). Let X Z N(χ, σ2I) and Y Z N(χ + δ, σ2I). Let
f : Rn → {0, 1} be any deterministic or random function, Then:
1.	IfS = {z ∈ Rn : δTz ≤ β} for some β and Pr(f (X) = 1) ≥ Pr(X ∈ S), then Pr(f(Y) =
1) ≥Pr(Y∈S)
2.	IfS = {z∈ Rn : δTz≥β}forsome β and Pr(f(X) = 1) ≤ Pr(X ∈ S), then Pr(f(Y) =
1) ≤Pr(Y∈S)
Using the same technique as used in the proof of Theorem 1 in Cohen et al. (2019), we fix x, δ and
define,
β = σ∣∣δ∣∣2 Φ-1(E[H (χ + e)])
Also define the half-spaces:
S-	= {z : δTz ≤ β + δTx}	= {z	: δT(z	- x)	≤	β}
S+ = {z : δTz ≥ -β + δTx}	= {z	: δT (z	- x)	≥	-β}
Applying algebra from the proof of Theorem 1 in Cohen et al. (2019), we have,
Pr(X ∈ S-) = φ(	) =E[H (χ+e)]
Pr(X ∈S+) =1 - M σ⅛) =1 - (I-M σ⅛ )) =E[H (X+e)]
Pr(Y ∈ S-) = φ( Y -―)=φ(φ-1(E [H(x + e)])
σ同2 σ
Pr(Y ∈ S+) = φ(--β) + ≡2) = φ(φ-1(E [H(x + e)])
σ冏2 σ
(A.3)
(A.4)
(A.5)
(A.6)
2We note that Salman et al. (2019) has recently and independently published a different proof of this lemma.
12
Under review as a conference paper at ICLR 2020
Using equation A.3
Pr(H(X) = 1) = E [H(X)] = E [H(x + e)] ≥ Pr(X ∈ S-)
Applying Statement 1 of Cohen,s lemma, using f = H and S = S-:
E [H(x + δ + e)] = Pr(H(x + δ + e) = 1) = Pr(H(Y) = 1) ≥ Pr(Y ∈ S-)
Using equation A.4,
Pr(H(X) = 1) = E [H(X)] = E [H(x + e)] ≤ Pr(X ∈ S+)
Applying Statement 2 of Cohen,s lemma, using f = H and S = S+:
E [H(x + δ + e)] = Pr(H(x + δ + e) = 1) = Pr(H(Y) = 1) ≤ Pr(Y ∈ S+)
Using equation A.7 and equation A.8:
Pr(Y ∈ S-) ≤ E [H(x + δ + e)] ≤ Pr(Y ∈ S+)
Then by equation A.5 and equation A.6:
(A.7)
(A.8)
φ(φ-1(E [H (x + e)])
≤ E [H(x + δ + e)] ≤ φ(φ-1(E [H(x + e)]) + ⅛∙
Noting that Φ-1 is a monotonically increasing function, we have:
Φ-1(E [H(x + e)])-处 ≤ Φ-1(E [H(x + δ + e)]) ≤ Φ-1(E [H(x + e)])
σ
Using equation A.2 yields equation A.1, which completes the proof.
”112
+ ---
σ
□
We now proceed with the proof of Theorem 1.
Proof. Applying Lemma 1 to hi and h7- gives (recalling that X = δ + X):
Φ-1(hi(x)) - P ≤ Φ-1(h i(x)) - ≡2 ≤ Φ-1(h i(x))	(A.9)
σ	σ
Φ-1(h j (x)) ≤ Φ-1(h j (x)) + ≡2 ≤ Φ-1(hj (x)) + P	(A.10)
σ	σ
Then we have:
=⇒
=⇒
=⇒
=⇒
φ (φ-1(hi (X))- 2ρ) ≥h j(x)
φ-1 (h i(X))- P ≥φ-1 (h j(X)) + P
Φ-1(h i(x))≥ Φ-1(h j (x)) + P
φ-1(h i(x)) ≥ φ-1(hj (x))
h i(X) ≥h j(X)
(by monotonicity of Φ-1)
(by equation A.9 )
(by equation A.10 )
(by monotonicity of Φ)
which proves the implication.	□
Corollary 1. Let h : Rn → [0,1]n be afunction such that fOrgiVen values of q, σ:
1
h(x) = q ∑ h(x + e)	ei Z N(0,σ2I)	(A.11)
∀ i, j ∈ [n], x, X ∈ Rn, ∣∣x - Xg ≤ P, with probability at least P,
L(hi(x)) ≥ hj(x) ⇒ hi(x)) ≥ hj(x)
13
Under review as a conference paper at ICLR 2020
Proof. By Hoeffding's Inequality, for any C > 0, ∀ i:
Pr [∣hi(x) - hi(x)∣ ≥ c] ≤ 2e-2qc2
(A.12)
Then:
Pr ψ (∣hi(x) - hi(x)∣ ≥
c)] ≤ 2ne-2qc2
(A.13)
〜

Since we are free to choose c, we define c such that 1 - p = 2ne-2qc2, then:
〜
c=
ln(2n(1 - p)-1 )
2q
(A.14)
Pr y (∣hi(x)- hi(x)∣ ≥
c)] ≤ 2ne-2qc2 = 1 - p

〜
=⇒
1 - Pr U (∣hi(x) - hi(x)∣ ≥
c)] ≥ p

=⇒ Pr
∩ (Ihi(X)- hi(X)∣ <c)] ≥ P
Then with probability at least p:
hi(x) -c< hi(x)
hj(x)+c>hj(x)
(A.15)
So:
Φ (φ-1 (hi(x)-
≥ hj (x) + c =⇒ Φ (φ-1
≥ h j(x)
(A.16)
The result directly follows from Theorem 1.
□
Theorem 2.	∀ x, X ∈ Rn, ∣∣x - Xg ≤ ρ, σ ∈ R, q ∈ N, define RCerr(x, K) as the largest i ≤ K such
~
^ Z ~
that L(h[i] (x)) ≥ h[2K-i] (x). Then, with probability at least p,
Rcert(x, K) ≤ R(x, X, K).
where R(x, x, K) denotes the top-K overlap between h(x) and h(x).
(A.17)
Proof. Note that the proof of Corollary 1 guarantees that with probability at least P, all estimates
h(x) are within the approximation bound c of h(x). So we can assume that Corollary 1 will apply
simultaneously to all pairs of indices i, j, with probability P.
We proceed to prove by contradiction.
Leti = Rcert(x, K)
=⇒ L(h[i](x)) ≥ h[2K-i](x),
Suppose there exists X such that:
R(x, X, K) < i,
Since L is a monotonically increasing function,
L(h[i](x)) ≥h[2K-i](x)
^ , ~ ,, ~ , , , _______________________
=⇒ L(h[i'](x)) ≥ h[j'](x),	∀ i ≤ i, j ≥ 2K - i,
and therefore by Corollary 1:
..	一，r，、	、	一，r，、	、 .一一	L ，..、、	「，..、	...
∀ m,n rank(h(x), m) ≤ i, rank(h(x), n) ≥ 2K - i ^⇒ hm(x)) ≥ hn(x)	(A.18)
Let X be the set of indices in the top K elements in h(x), and X be the set of indices in the top K
elements in h(X).
By assumption, X and X share fewer than i elements, so there will be at least K - i + 1 elements in
14
Under review as a conference paper at ICLR 2020
X which are not in X .
All of these elements have rank at least K + 1 in h(x).
Thus by pigeonhole principle, there is some index l ∈ X - X, such that rank(h(x), l) ≥ K + K -
i+1=2K-i+1≥2K-i.
Thus by Equation equation A.18,
_	Z ~ Z	、	「，-.、	「，-.、	....
∀m, where rank(h(x),m) ≤ i,	hm(x) ≥ hι(x)	(A.19)
Hence, there are i such elements where rank(h(x), m) ≤ i: these elements are clearly in X.
Because l ∈ X, Equation equation A.19 implies that these elements are all also in X. Thus X and
X share at least i elements,which contradicts the premise.
(In this proof we have implicitly assumed that the top K elements of a vector can contain more than
K elements, if ties occur, but that rank is assigned arbitrarily in cases of ties. In practice, ties in
smoothed scores will be very unlikely.)	口
A. 1 General Form and Proof of Theorem 3
We note that Theorem 3 can be used to derive a more general bound for any saliency map method
that for an input x, first maps g(x) to an elementwise function that only depends on the rank of the
current element in g(x) and not on the individual value of the element. We denote the composition
of the gradient function and this elementwise function as g[rank] . The only properties that the
function must satisfy is that it must be monotonically decreasing and non-negative. Thus, we have
the following statement:
Theorem 3.	LetT be the threshold value and let U be the set of q random perturbations fora given
input x using the smoothing variance σ2 and let p be the probability bound. If i is an element index
such that:
Median [rank(g(x + ), i)] ≤ T
∈U
(A.20)
Then:
rankcert(x, i) ≤
∑jn=1g[[jra]nk](x)
L( g[T⅛](χ))
(A.21)
Furthermore:
n	g[rank] (x)
∑ g[jank] (x), L( [T]  -----) are both independent of x. Thus RHS is a constant. (A.22)
Proof. Let the elementwise function be f : N → R+, i.e f takes the rank of the element as the
input and outputs a real number. Furthermore, we assume that f is a non-negative monotonically
decreasing function. Thus gi[rank] (x) = f (rank(g(x), i)).
We use f(i) to denote the constant value that f maps elements of rank i to.
Note that g[[ir]ank] (x) is the ith largest element of g[rank] (x).
Since f is a monotonically decreasing function:
g[[ir]ank] (x) = f(i) ∀ i ∈ [n]
Thus g[rank] (x) is independent of x, We simply use g[lank] (∙) to denote f (i), i.e:
g[rank](∙)=f(i)	∀ i ∈ [n]
Because Median∈U [rank(g(x + ), i)] ≤ T, for at least half of sampling instances in U,
rank(g(x + ), i) ≤ T.
So in these instances gi[rank] (x + ) ≥ f(T),
The remaining half or fewer elements are mapped to other nonnegative values.
Thus the sample mean:
g[rank](x) = 1 ∑ g[rank](x + e) ≥ g[T]nk](∙)∕2
q ∈U
15
Under review as a conference paper at ICLR 2020
Using Corollary 1, g[rank](x) is Certifiably as large as all elements With indices j SUCh that:
L(g[T]nk](∙)∕2) ≥ gjrank](x)
Now we will find an upper bound on the number of elements with indices j such that:
gjrank](x) > L(g[T]nk(∙)∕2)
Because all the ranks from 1 to n will occur in every sample in U, we have:
n
n
∀ e ∈ U, ∑ gkrank](x + e) = ∑ g[kank](∙)
k=1	k=1
n	n1	n
=⇒ ∑ gkrank](x) = ∑ 1 ∑ g[rank](x + e) = ∑ g[kank](∙)
k=1	k=1 q ∈U	k=1
Thus strictly fewer than ∑kn=1 g[kank](∙)∕L(g[Tnk](∙)∕2) elements Will have mean greater than
L(g[")∕2).
Hence, gi(x) is Certifiably at least as large as n — ( ∑n=ι g[kank](∙)∕L(g[Tnk](∙)∕2)) + 1 elements,
which by the definition of rankcert(x, i) yields the result.
Theorem 3 in the main text follows trivially, because in the Sparsified SmoothGrad case,
∑kn=1 g[[τk]] (∙) = T, and g[[Tτ]] (∙) = 1. Note that this represents the tightest possible realization of
this general theorem.	□
B Related Works
Sundararajan et al. (2017) defines a baseline, which represents an input absent of information and
determines feature importance by accumulating gradient information along the path from the base-
line to the original input. Alvarez-Melis & Jaakkola (2018) builds interpretable neural networks by
learning basis concepts that satisfy an interpretability criteria. Adebayo et al. (2018) proposes meth-
ods to assess the quality of saliency maps. Although these methods can produce visually pleasing
results, they can be sensitive to noise and adversarial perturbations.
Szegedy et al. (2014) introduced adversarial attacks for classification in deep learning. That work
dealt with L2 attacks, and uses L-BFGS optimization to minimize the norm of the perturbation.
Carlini & Wagner (2017) provide an L2 attack for classification which is often considered state of
the art.
One strategy to make classifiers more robust to adversarial attacks is randomized smoothing.
Lecuyer et al. (2019) use randomized smoothing to develop certifiably robust classifiers in both
the L1 and L2 norms. They show that if Gaussian smoothing is applied to class scores, a gap be-
tween the highest smoothed class score and the next highest smoothed score implies that the highest
smoothed class score will still be highest under all perturbations of some magnitude. This guarantees
that the smoothed classifier will be robust under adversarial perturbation.
Li et al. (2018) and Cohen et al. (2019) consider a related formulation. Cohen gives a bound that is
tight in the case of linear classifiers and gives significantly larger certified radii. In their formulation,
the unsmoothed classifier c is treated as a black box outputting just a discrete class label. The
smoothed classifier outputs the class observed with greatest frequency over noisy samples. Salman
et al. (2019) considers adversarial attacks against smoothed classifiers.
In the last couple of years, several approaches have been proposed to for interpreting neural network
outputs. Simonyan et al. (2014) computes the gradient of the class score with respect to the input.
Smilkov et al. (2017) computes the average gradient-based importance values generated from sev-
eral noisy versions of the input. Sundararajan et al. (2017) defines a baseline, which represents an
16
Under review as a conference paper at ICLR 2020
input absent of information and determines feature importance by accumulating gradient informa-
tion along the path from the baseline to the original input. Alvarez-Melis & Jaakkola (2018) builds
interpretable neural networks by learning basis concepts that satisfy an interpretability criteria. Ade-
bayo et al. (2018) proposes methods to assess the quality of saliency maps. Although these methods
can produce visually pleasing results, they can be sensitive to noise and adversarial perturbations
(Ghorbani et al., 2019; Kindermans et al., 2019).
As mentioned in Section 1, several approaches have been introduced for interpreting image classi-
fication by neural networks (Simonyan et al., 2014; Smilkov et al., 2017; Sundararajan et al., 2017;
Shrikumar et al., 2017).
C L2 ATTACK ON S ALIENCY MAPS
We developed an L2 norm attack on Rcert, based on Ghorbani et al. (2019)’s L∞ attack. Our algo-
rithm is presented as Algorithm 1. We deviate from Ghorbani et al. (2019)’s attack in the following
ways.:
•	We use gradient descent, rather than gradient sign descent: this is a direct adaptation to the
L2 norm.
•	We initialize learning rate as WDPxO)M, and then decrease learning rate with increasing
iteration count, proportionately (for the most part) to the reciprocal of the iteration count.
These are both standard practices for gradient descent.
•	We use random initialization and random restarts, also standard optimization practices.
•	If a gradient descent step would cross a decision boundary, we use backtracking line search
to reduce the learning rate until the step stays on the correct-class side. This allows the
optimization to get arbitrarily close to decision boundaries without crossing them.
We measured the effectiveness of our attack (Q = 100, P = 20, T = 5) against a slight modification
of Ghorbani et al. (2019)’s attack, in which the image was projected (if necessary) onto the L2 ball
at every iteration, and also clipped to fit within image box constraints (this was not mentioned in
Ghorbani et al. (2019)’s original algorithm). For this attack, we set the (L∞) learning rate parameter
at ρ∕500, and ran for UP to 1θ0 iterations. We also tested against random perturbations. For random
perturbations, up to 100 points were tested until a point in the correct class was identified. We tested
these attacks on both “vanilla gradient” and SmoothGrad saliency maps. See Figure 6. Experimental
conditions are as described in Section F for experiments on CIFAR-10. In this figure, for each attack
magnitude, we discard any image on which any optimization method failed.
(a) Attacks on vanilla gradient method.
(b) Attacks on SmoothGrad method (q=64).
Figure 6: Comparison of attack methods on images in CIFAR-10. See text of section C.
17
Under review as a conference paper at ICLR 2020
Algorithm 1 L2 attack on top-k overlap	
Input: k, image X, saliency map function h, iteration number P, random sampling iteration number Q, L2 perturbation norm constraint ρ, classifier c, restarts number T	
Output: Adversarial example X	
1:	Define D(z) = -∑i,rank(h(x),i)≤khi(z)
2:	for t = 1, ..., T do
3:	loop
4:	δ — Uniformly random vector on L? = P sphere.
5:	x0 — X + δ
6:	Clip X0 such that it falls within image box constraints.
7:	if c(X0) = c(X) then break inner loop
8:	if Q total iterations have passed over all t cycles of random sampling then
9:	break outer loop
10:	end if
11:	end loop
12:	P α ~ WD(XO)M
13:	for p = 1, ..., P do
14:	loop
15:	Xp ― Xp-1 + αVD(Xp-I)
16:	If necessary, project Xp such that IXp - XI2 ≤ P
17:	Clip Xp such that it falls within image box constraints.
18:	if c(Xp) = c(X) then
19:	break inner loop
20:	else
21:	α - α2
22:	end if
23:	end loop
24:	pα α 一 p+1
25:	end for
26:	Xt = argmaXz∈{χi,…,χP} D(Z)
27:	end for
28:	if random sampling failed at every iteration then fail
29:	X = argmaxz∈{χι,…,χτ} D(Z)
D Adversarial Training for Robust S aliency Maps
Adversarial training has been used extensively for making neural networks robust against adversarial
attacks on classification (Madry et al., 2018). The key idea is to generate adversarial examples for a
classification model, and then re-train the model on these adversarial examples.
In this section, we present an adversarial training approach for fortifying deep learning interpreta-
tions so that the saliency maps generated by the model (during test time) are robust against adversar-
ial examples. We focus on “vanilla gradient” saliency maps, although the technique presented here
can potentially be applied to any saliency map method which is differentiable w.r.t. the input. We
solve the following optimization problem for the network weights (denoted by θ):
min E(χ,y)~D	'cls(x,y) + λ IIg(X) - g(x)∣∣2 ,	(D.1)
θ	L '~V~,	^---------Y---------J
Classification loss Robustness loss
where X is an adversarial perturbation for the saliency map generated from x. To generate X, We
developed an L2 attack on saliency maps by extending the L∞ attack of Ghorbani et al. (2019) (see
the details in Section C). 'cls(x, y) is the standard cross entropy loss, and λ is the regularization pa-
rameter to encourage consistency between saliency maps of the original and adversarially perturbed
images.
We observe that the proposed adversarial training significantly improves the robustness of saliency
maps. Aggregate empirical results are presented in Figure 7, and examples of saliency maps are
presented in Figure 8. It is notable that the quality of the saliency maps is greatly improved for
18
Under review as a conference paper at ICLR 2020
0∙5 1 I	I	I	I	I	I 1 O.5
O	200	400	600	800 IOOO
λ
Figure 7: Effectiveness of adversarial training on MNIST. Increasing the regularization parame-
ter λ in the proposed adversarial training optimization (Equation D.1) significantly increases the
robustness of gradient-based saliency maps while it has little effect on the classification accuracy.
Robustness here is measured as R(x, X, K)/K, where K = n/4.
unperturbed inputs, by adversarial training. We note that this observation is related to the observation
made in Tsipras et al. (2018) showing that adversarial training for classification improves the quality
of the gradient-based saliency maps. We observe that even for very large value of λ, only a slight
reduction in classification accuracy occurs due to the added regularization term.
Figure 8: An illustration of the proposed adversarial training to robustify deep learning interpretation
on MNIST. We observe that the proposed adversarial training not only enhances the robustness but
it also improves the quality of the gradient-based saliency maps.
E	Adversarial Training Architecture Details
We use the Adam optimizer and generate new adversarial examples after each batch of training
according to the updated model. We use a simple convolutional neural network, with SoftPlus acti-
vations to ensure differentiability of the saliency map, on the MNIST data set (Figure 9). Adversarial
perturbations of norm up to ρ = 10 standard deviations of pixel intensity were used. The adversarial
attack used was the L2 attack described in Algorithm 1, with P = 15, Q = 100, T = 3. Training
was performed for 30 epochs using 48,000 images from the MNIST training set, testing was on
the entire MNIST test set. Instances where Algorithm 1 failed were not counted in the averages of
19
Under review as a conference paper at ICLR 2020
saliency map robustness, and were rare. (Highest frequency was for λ = 0, at 0.11%). We used
the implementation of Adam Optimizer provided with PyTorch (Paszke et al., 2017), with default
training parameters. These are (Table 2):
Table 1	Hyper-parameters used in model training for MNIST experiments.
Learning Rate
L2 regularization parameter
β
e
Batch Size
0.001
0
(.9,.999)
10-8
512
Figure 9: Network architecture used for the MNIST classification. SoftPlus activations are applied
after both convolutional layers, and after the first fully connected layer.
F Description of Experiments in Main Text
For ImageNet experiments (Figures 1-b and 2-a,b), we use ResNet-50, using the model pre-trained
on ImageNet that is provided by torchvision.models, and images were pre-processed according to
the recommended procedure for that model. In all of these figures, data are from the ILSV RC 2012
validation set, sample size is 64, and the main data lines represent the 60th percentile in the sample
of the calculated robustness certificate. Error bars represent the 48th and 72th percentile values,
corresponding to a 95% confidence interval for the population quantile.
For CIFAR-10 experiments, we train a ResNet-18 model on the CIFAR-10 training set (with pixel
intensities normalized to σ = 1, μ = 0 in each channel) using Stochastic Gradient Descent with
Momentum as implemented by PyTorch (Paszke et al., 2017). The following training parameters
were used:
Table 2	Hyper-parameters used in model training for CIFAR experiments.
Momentum	0.9
L2 regularization parameter	.0005
Epochs (max)	375
Learning Rate Schedule	,1 (epoch < 150), .01 (epoch ≥ 150)
Batch Size	128
Twenty percent of the training data was used for validation, and early stopping was used to maximize
accuracy relative to this validation set. For adversarial attacks on CIFAR samples, we train a version
of ResNet-18 with SoftMax activations instead of ReLU. The adversarial attack used was the L2
attack described in Algorithm 1, with P = 20, Q = 100, T = 5. When adversarially attacking images
smoothed with q = 8192 perturbations with this model, fewer perturbations are are used (512). In
these experiments, the sample size is 144. Error bars represent the 95% confidence interval of the
population mean. In Figure 4, instances where the adversarial attack failed were not counted at each
point. For MNIST experiments in Section 4, the model architecture, attack method, and baseline
20
Under review as a conference paper at ICLR 2020
training method are the same as used for the adversarial training experiments, detailed in appendix
E above. The adversarially training for classification uses the implementation provided in Rony
et al. (2019), with default arguments.
G Additional Example Images
See Figures 10 and 11.
Figure 10: An illustration of different saliency maps on some images from ImageNet. The input
image is shown in the first column (far left), with interpretations using SmoothGrad (second col-
umn from left), Quadratic SmoothGrad (third column), and Sparsified SmoothGrad (τ = .1, fourth
column). σ = .2, in units where pixel intensity ranges from 0 to 1.
Figure 11: An illustration of different saliency maps on some images from CIFAR-10. The input
image is shown in the first column (far left), with interpretations using SmoothGrad (second column
from left), Quadratic SmoothGrad (third column), and Relaxed Sparsified SmoothGrad (τ = .1, γ =
.01, fourth column). σ = .2, and the noise is scaled to the range of pixel intensities of the image.
21
Under review as a conference paper at ICLR 2020
H Bounds for S caled SmoothGrad, Quadratic SmoothGrad, and
RELAXED SPARSIFIED SMOOTHGRAD WITH γ = 0
In the text, we mention that we achieve vacuous bounds for Scaled SmoothGrad, Quadratic Smooth-
Grad, and Relaxed Sparsified SmoothGrad with γ = 0. Here are these bounds (Figure 12):
0.02	0.05	0.10	0.20	0,50	1,00
Sparsification Parameter
Figure 12: Bounds for Scaled SmoothGrad and and Relaxed Sparsified SmoothGrad with γ = 0.
Note that Scaled SmoothGrad is equivalent to Relaxed Sparsified SmoothGrad with τ = 1. Directly
comparable to Figure 2. For the Quadratic case, the certificates were negligibly small: the largest
robustness certificate achieved was 0.00023, at σ = 0.2.
I Additional Images from Adversarial Training Experiment
(Appendix D)
See Figure 13.
Figure 13: Additional figures from adversarial training on MNIST, for various λ. Note that Figure 7
shows for λ = 200.
22
Under review as a conference paper at ICLR 2020
J C omparis on of Bounds to Empirical Performance for Relaxed
S pars ified SmoothGrad.
We present a detailed view of Figure 4, for small magnitude perturbations, with the robustness
certificate shown. (Figure 14)
Relaxed Sparsified SmoothGrad (τ = 0.1, 8192 samples)
Relaxed Sparsified SmoothGrad (τ = 0.2, 8192 samples)
Certified Robustness of Relaxed Sparsified SmoothGrad (τ = 0.05)
Certified Robustness of Relaxed Sparsified SmoothGrad (τ = 0.1)
Certified Robustness of Relaxed Sparsified SmoothGrad (τ = 0.2)
—I- Gradient
—I- SmoothGrad (64 samples)
—|— SmoothGrad (8192 samples)
H Quadratic SmoothGrad (8192 samples)
—|— Relaxed Sparsified SmoothGrad (τ = 0.05, 8192 samples)
Figure 14: Empirical robustness of variants of SmoothGrad to adversarial attack, tested on CIFAR-
10 with ResNet-18. Attack magnitude is in units of standard deviations of pixel intensity. Robustness
is measured as R(x, x, K)/K, where K = n/4
K Comparison to Bounds in Lecuyer et al. (2019)
Lecuyer et al. (2019) approaches the classification case for certified robustness by smoothing, by
using bounds directly comparably to Theorem 1, but applying them to the class score elements,
rather than the saliency map elements: bounds are certified by demonstrating that the top class score
is certifiably larger than all other class scores. However, as noted by Cohen et al. (2019), these
bounds are rather loose, and Cohen et al. (2019) gives significantly tighter bounds specifically for
classification case, which we extend to apply to interpretation. In Figure 15, we compare our bounds
for interpretation to a straightforward application of Lecuyer et al. (2019)’s results for class scores
to saliency scores. Note that Lecuyer et al. (2019)’s results have a free parameter, for which we
numerically maximize the bound.
23
Under review as a conference paper at ICLR 2020
Figure 15: Comparison of Theorem 1 to results from Lecuyer et al. (2019)
24