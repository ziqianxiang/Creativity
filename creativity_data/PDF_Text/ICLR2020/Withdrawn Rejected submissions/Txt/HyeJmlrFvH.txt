Under review as a conference paper at ICLR 2020
NUQSGD: Provably Communication-efficient
Data-parallel SGD via Nonuniform Quantiza-
TION
Ab stract
As the size and complexity of models and datasets grow, so does the need for
communication-efficient variants of stochastic gradient descent that can be de-
ployed on clusters to perform parallel model training. Alistarh et al. (2017) describe
two variants of data-parallel SGD that quantize and encode gradients to lessen
communication costs. For the first variant, QSGD, they provide strong theoretical
guarantees. For the second variant, which we call QSGDinf, they demonstrate im-
pressive empirical gains for distributed training of large neural networks. Building
on their work, we propose an alternative scheme for quantizing gradients and show
that it yields stronger theoretical guarantees than exist for QSGD while matching
the empirical performance of QSGDinf.
1 Introduction
Deep learning is booming thanks to enormous datasets and very large models, leading to the fact that
the largest datasets and models can no longer be trained on a single machine. One common solution
to this problem is to use distributed systems for training. The most common algorithms underlying
deep learning are stochastic gradient descent (SGD) and its variants, which led to a significant amount
of research on building and understanding distributed versions of SGD.
Implementations of SGD on distributed systems and data-parallel versions of SGD are scalable and
take advantage of multi-GPU systems. Data-parallel SGD, in particular, has received significant
attention due to its excellent scalability properties (Zinkevich et al., 2010; Bekkerman et al., 2011;
Recht et al., 2011; Dean et al., 2012; Coates et al., 2013; Chilimbi et al., 2014; Li et al., 2014; Duchi
et al., 2015; Xing et al., 2015; Zhang et al., 2015; Alistarh et al., 2017). In data-parallel SGD, a large
dataset is partitioned among K processors. These processors work together to minimize an objective
function. Each processor has access to the current parameter vector of the model. At each SGD
iteration, each processor computes an updated stochastic gradient using its own local data. It then
shares the gradient update with its peers. The processors collect and aggregate stochastic gradients to
compute the updated parameter vector.
Increasing the number of processing machines reduces the computational costs significantly. However,
the communication costs to share and synchronize huge gradient vectors and parameters increases
dramatically as the size of the distributed systems grows. Communication costs may thwart the
anticipated benefits of reducing computational costs. Indeed, in practical scenarios, the communica-
tion time required to share stochastic gradients and parameters is the main performance bottleneck
(Recht et al., 2011; Li et al., 2014; Seide et al., 2014; Strom, 2015; Alistarh et al., 2017). Reducing
communication costs in data-parallel SGD is an important problem.
One promising solution to the problem of reducing communication costs of data-parallel SGD is
gradient compression, e.g., through gradient quantization (Dean et al., 2012; Seide et al., 2014; Sa
et al., 2015; Gupta et al., 2015; Abadi et al., 2016; Zhou et al., 2016; Alistarh et al., 2017; Wen et al.,
2017; Bernstein et al., 2018). (This should not be confused with weight quantization/sparsification,
as studied by Wen et al. (2016); Hubara et al. (2016); Park et al. (2017); Wen et al. (2017), which we
do not discuss here.) Unlike full-precision data-parallel SGD, where each processor is required to
broadcast its local gradient in full-precision, i.e., transmit and receive huge full-precision vectors at
each iteration, quantization requires each processor to transmit only a few communication bits per
iteration for each component of the stochastic gradient.
One popular such proposal for communication-compression is quantized SGD (QSGD), due to
Alistarh et al. (2017). In QSGD, stochastic gradient vectors are normalized to have unit L2 norm,
1
Under review as a conference paper at ICLR 2020
and then compressed by quantizing each element to a uniform grid of quantization levels using a
randomized method. While most lossy compression schemes do not provide convergence guarantees,
QSGD’s quantization scheme, is designed to be unbiased, which implies that the quantized stochastic
gradient is itself a stochastic gradient, only with higher variance determined by the dimension and
number of quantization levels. As a result, Alistarh et al. (2017) are able to establish a number
of theoretical guarantees for QSGD, including that it converges under standard assumptions. By
changing the number of quantization levels, QSGD allows the user to trade-off communication
bandwidth and convergence time.
Despite their theoretical guarantees based on quantizing after L2 normalization, Alistarh et al. opt
to present empirical results using L∞ normalization. We call this variation QSGDinf. While the
empirical performance of QSGDinf is strong, their theoretical guarantees on the number of bits
transmitted no longer apply. Indeed, in our own empirical evaluation of QSGD, we find the variance
induced by quantization is substantial, and the performance is far from that of SGD and QSGDinf.
Given the popularity of this scheme, it is natural to ask one can obtain guarantees as strong as
those of QSGD while matching the practical performance of the QSGDinf heuristic. In this work,
we answer this question in the affirmative by providing a new quantization scheme which fits into
QSGD in a way that allows us to establish stronger theoretical guarantees on the variance, bandwidth,
and cost to achieve a prescribed gap. Instead of QSGD’s uniform quantization scheme, we use
an unbiased nonuniform logarithmic scheme, similar to those introduced in telephony systems for
audio compression (Cattermole, 1969). We call the resulting algorithm nonuniformly quantized
stochastic gradient descent (NUQSGD). Like QSGD, NUQSGD is a quantized data-parallel SGD
algorithm with strong theoretical guarantees that allows the user to trade off communication costs with
convergence speed. Unlike QSGD, NUQSGD has strong empirical performance on deep models and
large datasets, matching that of QSGDinf. In particular, we provide a new efficient implementation
for these schemes using a modern computational framework (Pytorch), and benchmark it on classic
large-scale image classification tasks.
The intuition behind the nonuniform quantization scheme underlying NUQSGD is that, after L2 nor-
malization, many elements of the normalized stochastic gradient will be near-zero. By concentrating
quantization levels near zero, we are able to establish stronger bounds on the excess variance. In the
overparametrized regime of interest, these bounds decrease rapidly as the number of quantization
levels increases. Combined with a bound on the expected code-length, we obtain a bound on the total
communication costs of achieving an expected suboptimality gap. The resulting bound is slightly
stronger than the one provided by QSGD.
To study how quantization affects convergence on state-of-the-art deep models, we compare
NUQSGD, QSGD, and QSGDinf, focusing on training loss, variance, and test accuracy on standard
deep models and large datasets. Using the same number of bits per iteration, experimental results
show that NUQSGD has smaller variance than QSGD, as expected by our theoretical results. This
smaller variance also translates to improved optimization performance, in terms of both training loss
and test accuracy. We also observe that NUQSGD matches the performance of QSGDinf in terms of
variance and loss/accuracy. Further, our distributed implementation shows that the resulting algo-
rithm considerably reduces communication cost of distributed training, without adversely impacting
accuracy. Our empirical results show that NUQSGD can provide faster end-to-end parallel training
relative to data-parallel SGD, QSGD, and Error-Feedback SignSGD (Karimireddy et al., 2019) on
the ImageNet dataset.
Summary of Contributions.
•	We establish stronger theoretical guarantees for the excess variance and communication costs of
our gradient quantization method than those available for QSGD’s uniform quantization method.
•	We then establish stronger convergence guarantees for the resulting algorithm, NUQSGD, under
standard assumptions.
•	We demonstrate that NUQSGD has strong empirical performance on deep models and large datasets,
both in terms of accuracy and scalability. Thus, NUQSGD closes the gap between the theoretical
guarantees of QSGD and the empirical performance of QSGDinf.
2
Under review as a conference paper at ICLR 2020
1.1	Related Work
Seide et al. (2014) proposed signSGD, an efficient heuristic scheme to reduce communication costs
drastically by quantizing each gradient component to two values. Bernstein et al. (2018) later
provided convergence guarantees for signSGD. Note that the quantization employed by signSGD is
not unbiased, and so a new analysis was required. As the number of levels is fixed, SignSGD does
not provide any trade-off between communication costs and convergence speed.
Sa et al. (2015) introduced Buckwild!, a lossy compressed SGD with convergence guarantees. The
authors provided bounds on the error probability of SGD, assuming convexity and gradient sparsity.
Wen et al. (2017) proposed TernGrad, a stochastic quantization scheme with three levels. TernGrad
also significantly reduces communication costs and obtains reasonable accuracy with a small degra-
dation to performance compared to full-precision SGD. Convergence guarantees for TernGrad rely
on a nonstandard gradient norm assumption. As discussed, Alistarh et al. (2017) proposed QSGD, a
more general stochastic quantization scheme, for which they provide both theoretical guarantees and
experimental validation (although for different variants of the same algorithm). We note that their
implementation was only provided in Microsoft CNTK; by contrast, here we provide a more generic
implementation in Horovod (Sergeev and Del Balso, 2018), a communication back-end which can
support a range of modern frameworks such as Tensorflow, Keras, Pytorch, and MXNet.
NUQSGD uses a logarithmic quantization scheme. Such schemes have long been used in telephony
systems for audio compression (Cattermole, 1969). Logarithmic quantization schemes have appeared
in other contexts recently: Hou and Kwok (2018) studied weight distributions of long short-term
memory networks and proposed to use logarithm quantization for network compression. Zhang et al.
(2017) proposed a gradient compression scheme and introduced an optimal quantization scheme, but
for the setting where the points to be quantized are known in advance. As a result, their scheme is not
applicable to the communication setting of quantized data-parallel SGD.
2 Preliminaries: Data-parallel SGD and Convergence
We consider a high-dimensional machine learning model, parametrized by a vector w ∈ Rd . Let
Ω ⊆ Rd denote a closed and convex set. Our objective is to minimize f: Ω → R, which is an unknown,
differentiable, convex, and β -smooth function. The following summary is based on (Alistarh et al.,
2017).
Recall that a function f is β-smooth if, for all u,V ∈ Ω, we have ∣∣Vf (U) - Vf (v)k ≤ β ∣∣u - v∣∣,
where ∣∣ ∙ ∣∣ denotes the Euclidean norm. Let (S,Σ, μ) be a probability space (and let E denote
expectation). Assume we have access to stochastic gradients of f, i.e., we have access to a function
g : Ω X S → Rd such that, if S 〜μ, then E[g(w, S)] = Vf (W) for all W ∈ Ω. In the rest of the paper,
we let g(w) denote the stochastic gradient for notational simplicity. The update rule for conventional
full-precision projected SGD is Wt+1 = Pq(wt — αg(wt)), where Wt is the current parameter input,
α is the learning rate, and Pω is the Euclidean projection onto Ω.
We say the stochastic gradient has a second-moment upper bound B when E[∣g(W)∣2] ≤ B for
all w ∈ Ω. Similarly, the stochastic gradient has a variance upper bound σ2 when E[∣g(w)—
V f (W)∣2] ≤ σ2 for all W ∈ Ω. Note that a second-moment upper bound implies a variance upper
bound, because the stochastic gradient is unbiased.
We have classical convergence guarantees for conventional full-precision SGD given access to
stochastic gradients at each iteration:
Theorem 1 (Bubeck 2015, Theorem 6.3). Let f: Ω → R denote a convex and β -smooth function
and let R2，suPw∈ω ∣∣w — w0k2. Suppose that the projected SGD update is executed for T iterations
with α = 1∕(β + 1∕γ) where Y = r,2/T/σ. Given repeated and independent access to stochastic
gradients with a variance upper bound σ2, projected SGD satisfies
Ehf(I ∑0 Wt)] - W∈Ωf (W) ≤ R^T + 吟
(1)
Minibatched (with larger batch sizes) and data-parallel SGD are two common SGD variants used in
practice to reduce variance and improve computational efficiency of conventional SGD.
3
Under review as a conference paper at ICLR 2020
Input: local data, local copy of the parameter vector wt , learning rate α, and K
1 for t = 1 to T do
2
3
4
5
6
7
8
9
10
for i = 1 to K do // each transmitter processor (in parallel)
Compute gi(wt) ; // stochastic gradient
Encode ci,t _ ENCODE(gi(Wt))；
Broadcast ci,t to all processors;
for l = 1 to K do // each receiver processor (in parallel)
for i = 1 to K do // each transmitter processor
Receive ci,t from processor i for each i；
Decode g i(w t) — DECODE (c“)；
Aggregate Wt+1 — Pω(wt - α ∑K=I gi(Wt))；
Algorithm 1: Data-parallel (synchronized) SGD.
Following (Alistarh et al., 2017), we consider data-parallel SGD, a synchronous distributed framework
consisting of K processors that partition a large dataset among themselves. This framework models
real-world systems with multiple GPU resources. Each processor keeps a local copy of the parameter
vector and has access to independent and private stochastic gradients of f .
At each iteration, each processor computes its own stochastic gradient based on its local data and
then broadcasts it to all peers. Each processor receives and aggregates the stochastic gradients from
all peers to obtain the updated parameter vector. In detail, the update rule for full-precision data-
parallel SGD is Wt +1 = Pω(wt - K ∑K=I gι((Wt)) Where gι(wt) is the stochastic gradient computed
and broadcasted by processor l. Provided that gi (wt) is a stochastic gradient with a variance upper
bound σ2 for all l, then K ∑ = gl (wt) is a stochastic gradient with a variance upper bound K Thus,
aggregation improves convergence of SGD by reducing the first term of the upper bound in (1).
Assume each processor computes a minibatch gradient of size B. Then, this update rule is essentially
a minibatched update with size BK.
Data-parallel SGD is described in Algorithm 1. Full-precision data-parallel SGD is a special case
of Algorithm 1 with identity encoding and decoding mappings. Otherwise, the decoded stochastic
gradient gi(wt) is likely to be different from the original local stochastic gradient gi(wt).
By Theorem 1, we have the following convergence guarantees for full-precision data-parallel SGD:
Corollary 1 (Alistarh et al. 2017, Corollary 2.2). Let f, R, and γ be as defined in Theorem 1
and let ε > 0. Suppose that the projected SGD update is executed for T iterations with α =
1∕(β + KK /γ) on K processors, each with access to independent StoChastiC gradients off with a
second-moment bound B. The smallest T for the full-precision data-parallel SGD that guarantees
E[.f (T ∑t=0 wt)] -minw∈Ω f (w) ≤ ε isTε = O(R2max(KBr,β)).
3	Nonuniformly Quantized Stochastic Gradient Descent
Data-parallel SGD reduces computational costs significantly. However, the communication costs
of broadcasting stochastic gradients is the main performance bottleneck in large-scale distributed
systems. In order to reduce communication costs and accelerate training, Alistarh et al. (2017)
introduced a compression scheme that produces a compressed and unbiased stochastic gradient,
suitable for use in SGD.
At each iteration of QSGD, each processor broadcasts an encoding of its own compressed stochastic
gradient, decodes the stochastic gradients received from other processors, and sums all the quantized
vectors to produce a stochastic gradient. In order to compress the gradients, every coordinate (with
respect to the standard basis) of the stochastic gradient is normalized by the Euclidean norm of the
gradient and then stochastically quantized to one of a small number quantization levels distributed
uniformly in the unit interval. The stochasticity of the quantization is necessary to not introduce bias.
Alistarh et al. (2017) give a simple argument that provides a lower bound on the number of coordinates
that are quantized to zero in expectation. Encoding these zeros efficiently provides communication
savings at each iteration. However, the cost of their scheme is greatly increased variance in the
gradient, and thus slower overall convergence. In order to optimize overall performance, we must
balance communication savings with variance.
4
Under review as a conference paper at ICLR 2020
By simple counting arguments, the distribution of the (normalized) coordinates cannot be uniform.
Indeed, this is the basis of the lower bound on the number of zeros. These arguments make no
assumptions on the data distribution, and rely entirely on the fact that the quantities being quantized
are the coordinates of a unit-norm vector. Uniform quantization does not capture the properties of
such vectors, leading to substantial gradient variance.
3.1	Nonuniform Quantization
In this paper, we propose and study a new scheme to quantize normalized gradient vectors. Instead
of uniformly distributed quantization levels, as proposed by Alistarh et al. (2017), we consider
quantization levels that are nonuniformly distributed in the unit interval, as depicted in Figure 1.
In order to obtain a quantized gradient that is suitable for SGD, we need the quantized gradient to
remain unbiased. Alistarh et al. (2017) achieve this via a randomized quantization scheme, which can
be easily generalized to the case of nonuniform quantization levels.
Using a carefully parametrized generalization of the unbiased quantization scheme introduced by
Alistarh et al., we can control both the cost of communication and the variance of the gradient.
Compared to a uniform quantization scheme, our scheme reduces quantization error and variance
by better matching the properties of normalized vectors. In particular, by increasing the number of
quantization levels near zero, we obtain a stronger variance bound. Empirically, our scheme also
better matches the distribution of normalized coordinates observed on real datasets and networks.
We now describe the nonuniform quantization scheme: Let S ∈{1,2,…} be the number of internal
quantization levels, and let L = (10,11,…，ls +1) denote the sequence of quantization levels, where
10 = 0 < 11 < …< ls +1 = 1. For r ∈ [0,1], let s(r) and p(r) satisfy ls(r)≤ r ≤ ls(r)+i and r =
(1 - p(r))ls(r) + p(r)ls(r)+1, respectively. Define τ(r) = k(r)+1 - Sr). Note that S(r) ∈ {0,1,…,S}.
Definition 1. The nonuniform quantization of a vector v ∈ Rd is
Qs(v) , [Qs(V1),…,Qs(Vd)]T where QS(Vi) = ∣∣vk ∙ sign(Vi) ∙ h(v,S)	(2)
where, letting r = ∣Vi∣∕IIvk, the h(v,S) ,s are independent random variables such that hi(v,S) = ls(r)
with probability 1 — P(r) and hi (v, S) = ls(r)+1 otherwise.
We note that the distribution of hi(v, s) satisfies E[hi(v,s)] = ri and achieves the minimum variance
over all distributions that satisfy E[hi(v, s)] = ri with support L. In the following, we focus on a
special case of nonuniform quantization with L = (0,1/2S,…,2S-1∕2S, 1) as the quantization levels.
The intuition behind this quantization scheme is that it is very unlikely to observe large values of ri in
the stochastic gradient vectors of machine learning models. Stochastic gradients are observed to be
dense vectors (Bernstein et al., 2018). Hence, it is natural to use fine intervals for small ri values to
reduce quantization error and control the variance.
After quantizing the stochastic gradient with a small number of discrete levels, each processor
must encode its local gradient into a binary string for broadcasting. We describe this encoding in
Appendix A.
5
Under review as a conference paper at ICLR 2020
4	Theoretical Guarantees
In this section, we provide theoretical guarantees for NUQSGD, giving variance and code-length
bounds, and using these in turn to compare NUQSGD and QSGD. Please note that the proofs of
Theorems 2, 3, 4, and 5 are provided in Appendices B, C, D, and E respectively.
Theorem 2 (Variance bound). Let v ∈ Rd. The nonuniform quantization of v satisfies E[Qs(v)] = v.
Furthermore, provided that s ≤ log(d)/2, we have
E[k®(V)-Vk2] ≤ εQkvk2	⑶
where εQ = min{min{2-2S(d - 22S), 2-s√d - 22S} + O(S),d∕3(2-2s+1 + 1)}.
The result in Theorem 2 implies that if g(w) is a stochastic gradient with a second-moment bound
η, then QS(g(w)) is a stochastic gradient with a variance upper bound εQη. In the range of interest
where d is sufficiently large, i.e., S = o(log(d)), the variance upper bound decreases with the number
of quantization levels. To obtain this data-independent bound, we establish upper bounds on the
number of coordinates of V falling into intervals defined by L .
Theorem 3 (Code-length bound). Let V ∈ Rd. Provided d is large enough to ensure 22S + √d2S ≤ d/e,
the expectation E[|ENCODE(V)|] of the number of communication bitS needed to tranSmit QS(V) iS
bounded above by
NQ = C + 3nS,d + (I + o(I))nS,dlog (n-) + (I + o(I))nS,d loglog ((n +^^))	(4)
where C = b — (1 + o(1)) and n,d = 22S + 2S √d.
Theorem 3 provides a bound on the expected number of communication bits to encode the quantized
stochastic gradient. Note that 22S + √d2S ≤ d/e is a mild assumption in practice. As one would
expect, the bound, (4), increases monotonically in d and S. In the sparse case, if We choose S = o(log d)
levels, then the upper bound on the expected code-length is O(2s√d log (卓)).
Combining the upper bounds above on the variance and code-length, Corollary 1 implies the following
guarantees for NUQSGD:
Theorem 4 (NUQSGD for smooth convex optimization). Let f andR be defined aS in Theorem 1, let
εQ be defined as in Theorem 2, let ε > 0,B =(1 + εQ)B, and let Y > 0 be given by γ2 = 2R2/(BT).
With ENCODE and DECODE defined as in Appendix A, suppose that Algorithm 1 is executedfor T
iterations with a learning rate α = 1∕(β + Kκ /γ) on K processors, each with access to independent
stochastic gradients off with a second-moment bound B. Then Tε = O (max (KB, β) R 2) iterations
suffice to guarantee E [f (1 ∑T=O Wt) — — minw∈Ω f (W) ≤ ε. In addition, NUQSGD requires at most
NQ communication bits per iteration in expectation.
On nonconvex problems, (weaker) convergence guarantees can be established along the lines of, e.g.,
(Ghadimi and Lan, 2013, Theorem 2.1).
NUQSGD Vs QSGD. How do QSGD and NUQSGD compare in terms of bounds on the expected
number of communication bits required to achieve a given suboptimality gap ε ? The quantity that
controls our guarantee on the convergence speed in both algorithms is the variance upper bound,
which in turn is controlled by the quantization schemes. Note that the number of quantization
levels, s, is usually a small number in practice. On the other hand, the dimension, d, can be very
large, especially in overparameterized networks. In Figure 2, we show that the quantization scheme
underlying NUQSGD results in substantially smaller variance upper bounds for plausible ranges of s
and d. Note that these bounds do not make any assumptions on the dataset or the structure of the
network.
For any (nonrandom) number of iterations T, an upper bound, NA, holding uniformly over iterations
k ≤ T on the expected number of bits used by an algorithm A to communicate the gradient on
iteration k, yields an upper bound TNA, on the expected number of bits communicated over T
iterations by algorithm A. Taking T = TA,ε to be the (minimum) number of iterations needed to
guarantee anexpected suboptimality gap of ε based on the properties of A, we obtain an upper bound,
Za,ε = Ta,εNA, on the expected number of bits of communicated on a run expected to achieve a
suboptimality gap of at most ε .
6
Under review as a conference paper at ICLR 2020
Figure 3: Training loss on CIFAR10 (left) and ImageNet (middle) for ResNet models. QSGD,
QSGDinf, and NUQSGD are trained by simulating the quantization and dequantizing of the gradients
from 8-GPUs. On CIFAR10, SGD refers to the single-GPU training versus on Imagenet it refers to
2-GPU setup in the original ResNet paper. SGD is shown to highlight the significance of the gap
between QSGD and QSGDinf. SuperSGD refers to simulating full-precision distributed training
without quantization. SuperSGD is impractical in scenarios with limited bandwidth. (Right) Estimated
normalized variance on CIFAR10 on the trajectory of single-GPU SGD. Variance is measured for
fixed model snapshots during training. Notice that the variance for NUQSGD and QSGDinf is lower
than SGD for almost all the training and it decreases after the learning rate drops.
Theorem 5 (Expected number of communication bits). Provided that S = o(log(d)) and KB > β,
ZNUQSGD,ε = O ( ε12 Pd(d - 22S)log ( √√d )) αnd ζQSGD,ε = O(* d log √d)∙
Focusing on the dominant terms in the expressions of overall number of communication bits required
to guarantee a suboptimality gap ofε, we observe that NUQSGD provides slightly stronger guarantees.
Note that our stronger guarantees come without any assumption about the data.
5	Experimental Evaluation
In this section, we examine the practical performance of NUQSGD in terms of both convergence
(accuracy) and speedup. The goal is to empirically show that NUQSGD can provide the same
performance and accuracy compared to the QSGDInf heuristic, which has no theoretical compression
guarantees. For this, we implement and test these three methods (NUQSGD, QSGD, and QSGDInf),
together with the distributed full-precision SGD baseline, which we call SuperSGD. We split our
study across two axes: first, we examine the convergence of the methods and their induced variance.
Second, we provide an efficient implementation of all four methods in Pytorch using the Horovod
communication back-end (Sergeev and Del Balso, 2018), adapted to efficiently support quantization,
and examine speedup relative to the full-precision baseline. We investigate the impact of quantization
on training performance by measuring loss, variance, accuracy, and speedup for ResNet models (He
et al., 2016) applied to ImageNet (Deng et al., 2009) and CIFAR10 (Krizhevsky).
We evaluate these methods on two image classification datasets: ImageNet and CIFAR10. We train
ResNet110 on CIFAR10 and ResNet18 on ImageNet with mini-batch size 128 and base learning
rate 0.1. In all experiments, momentum and weight decay are set to 0.9 and 10-4, respectively. The
bucket size and the number of quantization bits are set to 8192 and 4, respectively. We observe similar
results in experiments with various bucket sizes and number of bits. We simulate a scenario with k
GPUs for all three quantization methods by estimating the gradient from k independent mini-batches
and aggregating them after quantization and dequantization.
In Figure 3 (left and middle), we show the training loss with 8 GPUs. We observe that NUQSGD
and QSGDinf improve training loss compared to QSGD on ImageNet. We observe significant gap
in training loss on CIFAR10 where the gap grows as training proceeds. We also observe similar
performance gaps in test accuracy (provided in Appendix F). In particular, unlike NUQSGD, QSGD
does not achieve test accuracy of full-precision SGD. Figure 3 (right) shows the mean normalized
variance of the gradient (defined in Appendix F) versus training iteration on the trajectory of single-
GPU SGD on CIFAR10. These observations validate our theoretical results that NUQSGD has
smaller variance for large models with small number of quantization bits.
Efficient Implementation and Speedup. To examine speedup behavior, we implemented all quanti-
zation methods in Horovod (Sergeev and Del Balso, 2018), a communication back-end supporting
Pytorch, Tensorflow and MXNet. Doing so efficiently requires non-trivial refactoring of this frame-
work, since it does not support communication compression—our framework will be open-sourced
7
Under review as a conference paper at ICLR 2020
Figure 4: Scalability behavior for NUQSGD versus the full-precision baseline when training ResNet34
and ResNet50 on ImageNet. The ResNet34 graph examines strong scaling (left), splitting a global
batch of size 256 onto the available GPUs, whereas the ResNet50 graph examines strong scaling
(middle) keeping a fixed per-GPU batch size of 16. Each time bar is split into computation (bottom),
encoding cost (middle), and transmission cost (top). Notice the significant negative scalability of the
SGD baseline in both scenarios. By contrast, the 4-bit communication-compressed implementation
achieves positive scaling, while the 8-bit variant stops scaling between 4 and 8 nodes due to the
higher communication and encoding costs. End-to-end training time for ResNet50/ImageNet for
NUQSGD and EF-SignSGD versus the SuperSGD baseline (right).
upon publication. Our implementation diverges slightly from the theoretical analysis. First, Horovod
applies “tensor fusion” to multiple layers, by merging the resulting gradient tensors for more efficient
transmission. This causes the gradients for different layers to be quantized together, which can lead to
loss of accuracy (due to e.g. different normalization factors across the layers). We addressed this by
tuning the way in which tensor fusion is applied to the layers such that it minimizes the accuracy loss.
Second, we noticed that quantizing the gradients corresponding to the biases has a significant adverse
effect on accuracy; since the communication impact of biases is negligible, we transmit them at full
precision. We apply this for all methods considered. Finally, for efficiency reasons, we directly pack
the quantized values into 32-bit numbers, without additional encoding. We implemented compression
and de-compression via efficient CUDA kernels.
Our baselines are full-precision SGD (SuperSGD), Error-Feedback SignSGD (Karimireddy et al.,
2019), and the QSGDinf heuristic, which we compare against the 4-bit and 8-bit NUQSGD variants
executing the same pattern. The implementation of the QSGDinf heuristic provides almost identical
convergence numbers, and is sometimes omitted for visibility. (QSGD yields inferior convergence
on this dataset and is therefore omitted.) All variants are implemented using a standard all-to-all
reduction pattern. Figures 4 (left), (middle) show the execution time per epoch for ResNet34 and
ResNet50 models on ImageNet, on a cluster machine with 8 NVIDIA 2080 Ti GPUs, for the hyper-
parameter values quoted above. The results confirm the efficiency and scalability of the compressed
variant, mainly due to the reduced communication volume. We note that the overhead of compression
and decompression is less than 1% of the batch computation time for NUQSGD.
Figure 4 (right) presents end-to-end speedup numbers (time versus accuracy) for ResNet50/ImageNet,
executed on 4 GPUs, under the same hyperparameter settings as the full-precision baseline, with
bucket size 512. First, notice that NUQSGD variants match the target accuracy of the 32-bit model,
with non-trivial speedup over the standard data-parallel variant, directly proportional to the per-
epoch speedup. The QSGDinf heuristic yields similar accuracy and performance, and is therefore
omitted. Second, we found that unfortunately EF-SignSGD does not converge under these standard
hyperparameter settings. To address this issue, we performed a non-trivial amount of hyperparameter
tuning for this algorithm: in particular, we found that the scaling factors and the bucket size must
be carefully adjusted for convergence on ImageNet. We were able to recover full accuracy with
EF-SignSGD on ResNet50, but that the cost of quantizing into buckets of size 64. Unfortunately, in
this setting the algorithm transmits a non-trivial amount of scaling data, and the GPU implementation
becomes less efficient due to error computation and reduced parallelism. The end-to-end speedup of
this tuned variant is inferior to NUQSGD-4bit, and only slightly superior to that of NUQSGD-8bit.
Please see Figure 9 in the Appendix and the accompanying text for details.
6	Conclusions
We study data-parallel and communication-efficient version of stochastic gradient descent. Build-
ing on QSGD (Alistarh et al., 2017), we study a nonuniform quantization scheme. We establish
8
Under review as a conference paper at ICLR 2020
upper bounds on the variance of nonuniform quantization and the expected code-length. In the
overparametrized regime of interest, the former decreases as the number of quantization levels
increases, while the latter increases with the number of quantization levels. Thus, this scheme
provides a trade-off between the communication efficiency and the convergence speed. We compare
NUQSGD and QSGD in terms of their variance bounds and the expected number of communication
bits required to meet a certain convergence error, and show that NUQSGD provides stronger guar-
antees. Experimental results are consistent with our theoretical results and confirm that NUQSGD
matches the performance of QSGDinf when applied to practical deep models and datasets including
ImageNet. Thus, NUQSGD closes the gap between the theoretical guarantees of QSGD and empirical
performance of QSGDinf. One limitation of our study which we aim to address in future work is
that we focus on all-to-all reduction patterns, which interact easily with communication compression.
In particular, we aim to examine the interaction between more complex reduction patterns, such as
ring-based reductions (Hannun et al., 2014), which may yield superior performance in bandwidth-
bottlenecked settings, but which interact with communication-compression in non-trivial ways, since
they may lead a gradient to be quantized at each reduction step.
References
D. Alistarh, D. Grubic, J. Z. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efficient
SGD via gradient quantization and encoding. In Proc. Advances in Neural Information Processing
Systems (NIPS), 2017.
M. Zinkevich, M. Weimer, L. Li, and A. J. Smola. Parallelized stochastic gradient descent. In Proc.
Advances in Neural Information Processing Systems (NIPS), 2010.
R. Bekkerman, M. Bilenko, and J. Langford. Scaling up machine learning: Parallel and distributed
approaches. Cambridge University Press, 2011.
B. Recht, C. Ra S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic
gradient descent. In Proc. Advances in Neural Information Processing Systems (NIPS), 2011.
J. Dean, G. Corrado, R. Monga K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker,
K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In Proc. Advances in
Neural Information Processing Systems (NIPS), 2012.
A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and A. Ng. Deep learning with cots hpc systems.
In Proc. International Conference on Machine Learning (ICML), 2013.
T. Chilimbi, Y. Suzue J. Apacible, and K. Kalyanaraman. Project adam: Building an efficient and
scalable deep learning training system. In Proc. USENIX Symposium on Operating Systems Design
and Implementation (OSDI), 2014.
M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita,
and B.-Y. Su. Scaling distributed machine learning with the parameter server. In Proc. USENIX
Symposium on Operating Systems Design and Implementation (OSDI), 2014.
J. C. Duchi, S. Chaturapruek, and C. R6. Asynchronous stochastic convex optimization. In Proc.
Advances in Neural Information Processing Systems (NIPS), 2015.
E.	P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee, X. Zheng, P. Xie, A. Kumar, and Y. Y. Petuum.
Petuum: A new platform for distributed machine learning on big data. IEEE transactions on Big
Data,1(2):49-67, 2015.
S. Zhang, A. E. Choromanska, and Y. LeCun. Deep learning with elastic averaging SGD. In Proc.
Advances in Neural Information Processing Systems (NIPS), 2015.
F.	Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application to
data-parallel distributed training of speech DNNs. In Proc. INTERSPEECH, 2014.
N. Strom. Scalable distributed DNN training using commodity GPU cloud computing. In Proc.
INTERSPEECH, 2015.
9
Under review as a conference paper at ICLR 2020
C. M. D. Sa, Ce. Zhang, K. Olukotun, and C. Re. Taming the wild: A unified analysis ofhogwild-Style
algorithms. In Proc. Advances in Neural Information Processing Systems (NIPS), 2015.
S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep learning with limited numerical
precision. In Proc. International Conference on Machine Learning (ICML), 2015.
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
and M. Devin. Tensorflow: Large-scale machine learning on heterogeneous distributed systems.
arXiv:1603.04467, 2016.
S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional
neural networks with low bitwidth gradients. arXiv:1606.06160, 2016.
W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. TernGrad: Ternary gradients to reduce
communication in distributed deep learning. In Proc. Advances in Neural Information Processing
Systems (NIPS), 2017.
J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signSGD: Compressed optimisa-
tion for non-convex problems. In Proc. International Conference on Machine Learning (ICML),
2018.
W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks.
In Proc. Advances in Neural Information Processing Systems (NIPS), 2016.
I.	Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. In
Proc. Advances in Neural Information Processing Systems (NIPS), 2016.
J.	Park, S. Li, W. Wen, P. Tang, H. Li, Y. Chen, and P. Dubey. Faster CNNs with direct sparse
convolutions and guided pruning. In Proc. International Conference on Learning Representations
(ICLR), 2017.
K.	W. Cattermole. Principles of pulse code modulation. Iliffe, 1969.
Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in
tensorflow. arXiv preprint arXiv:1802.05799, 2018.
L.	Hou and J. T. Kwok. Loss-aware weight quantization of deep networks. In Proc. International
Conference on Learning Representations (ICLR), 2018.
H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang. ZipML: Training linear models with
end-to-end low precision, and a little bit of deep learning. In Proc. International Conference on
Machine Learning (ICML), 2017.
S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine
Learning, 8(3-4):231-358, 2015.
S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical
image database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2009.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of
Toronto, 2009.
S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback fixes SignSGD and other
gradient compression schemes. In Proc. International Conference on Machine Learning (ICML),
2019.
10
Under review as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
13
Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end
speech recognition. arXiv preprint arXiv:1412.5567, 2014.
P. Elias. Universal codeword sets and representations of the integers. IEEE Transactions on
Information Theory, 21(2):194-203,1975.
S.	Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2004.
A	Encoding
Encoding:
Place a 0 at the end of the string;
if N == 1 then
I Stop;
else
Prepend binary(N) to the beginning;
Let N 0 denote # bits prepended minus 1;
Encode N0 recursively;
Decoding:
Start with N = 1;
if the next bit == 0 then
I Stop and return N;
else
Read that bit plus N following bits;
Update N;
Algorithm 2: Elias recursive coding produces a bit string encoding of positive integers.
By inspection, the quantized gradient Qs(v) is determined by the tuple (kvk, ρ,h), where kvk is the
norm of the gradient, P，[sign(vi),…，sign(Vd)]t is the vector of signs of the coordinates Vi,s, and
h，[h 1(v, S),…，hd(v, S)]t are the quantizations of the normalized coordinates. We can describe
the ENCODE function (for Algorithm 1) in terms of the tuple (kvk, ρ, h) and an encoding/decoding
SCheme ERC: {1,2,…}→{0,1}* and ERC-1 : {0,1}*→{1,2,…} for encoding/decoding positive
integers.
The encoding, ENCODE(v), of a stochastic gradient is as follows: We first encode the norm kvk
using b bits where, in practice, we use standard 32-bit floating point encoding. We then proceed
in rounds, r = 0,1, ∙∙∙. On round r, having transmitted all nonzero coordinates up to and including
tr, we transmit ERC(ir) where tr+1 = tr + ir is either (i) the index of the first nonzero coordinate of
h after tr (with t0 = 0) or (ii) the index of the last nonzero coordinate. In the former case, we then
transmit one bit encoding the sign ρtr+1, transmit ERC(log(2S+1htr+1 )), and proceed to the next round.
In the latter case, the encoding is complete after transmitting ρtr+1 and ERC(log(2S+1htr+1 )).
The DECODE function (for Algorithm 1) simply reads b bits to reconstruct kvk. Using ERC-1, it
decodes the index of the first nonzero coordinate, reads the bit indicating the sign, and then uses
ERC-1 again to determines the quantization level of this first nonzero coordinate. The process
proceeds in rounds, mimicking the encoding process, finishing when all coordinates have been
decoded.
Like Alistarh et al. (2017), we use Elias recursive coding (Elias, 1975, ERC) to encode positive
integers. ERC is simple and has several desirable properties, including the property that the coding
scheme assigns shorter codes to smaller values, which makes sense in our scheme as they are more
likely to occur. Elias coding is a universal lossless integer coding scheme with a recursive encoding
and decoding structure.
The Elias recursive coding scheme is summarized in Algorithm 2. For any positive integer N, the
following results are known for ERC (Alistarh et al., 2017):
1.	|ERC(N )| ≤ (1 + o (1)) log N + 1;
2.	ERC(N) can be encoded and decoded in time O(|ERC(N)|);
11
Under review as a conference paper at ICLR 2020
3.	Decoding can be done without knowledge of an upper bound on N.
B Proof of Theorem 2 (Variance Bound)
We first find a simple expression of the variance of Qs(v) for every arbitrary quantization scheme in
the following lemma:
Lemma 1. Let V ∈ Rd, L = (10,11,…，ls +1), and fix s ≥ 1. The variance of Qs (V) for general
sequence of quantization levels is given by
d
E[k Qs(V)-Vk2] = kvk2 ∑ T 2( ri) p (ri )(1-p( ri))	⑸
i=1
where r = vi∣∕kv∣∣ andP(r),s(r), T(r) are defined in Section 3.1.
Proof. Noting the random quantization is i.i.d over elements of a stochastic gradient, we can decom-
poseE[kQs(V)-Vk2] as:
d
E[kQs(V)-Vk2]=∑kVk2σ2(ri)	(6)
i=1
where σ2(ri) = E[(hi (V, s) - ri)2]. Computing the variance of hi(V,s), we can show that σ2(ri) =
T 2( ri) P (ri )(1 - P (ri)).	□
In the following, We consider NUQSGD algorithm with L = (0,1/2s,…，2s-1∕2s, 1) as the quanti-
zation levels. Then, hi(V,s)’s are defined in two cases based on which quantization interval ri falls
into:
1)Ifri ∈ [0,2-s], then
(
hi(V,s) =
0
2-s
with probability 1 - P1 (ri, s);
otherwise
(7)
where P1 r, s = 2sr.
2) If ri ∈ [2j-s, 2j+1-s] for j = 0,…,s - 1, then
(2j-s	with probability 1 -P2(ri,s);
2 j+1-s otherwise
(8)
where P2 (r, S) = 2s-jr - 1. Note that Qs (0) = 0.
Let Sj denote the coordinates of vector V whose elements fall into the (j+ 1)-th bin, i.e., S0 , {i :
ri ∈ [0,2-s]} and Sj1+1，{i: ri∙ ∈ [2j-s,2j+1-s]} for j = 0,…,s — 1. Let dj，|Sjj |. Applying the
result of Lemma 1, we have
E[kQs(V) - Vk2] = kVk2T02 ∑ P1(ri,s)(1-P1(ri,s))
i∈S0
s-1	(9)
+ kvk2 ∑ T2+1 ∑ P2(ri,S)(1-P2(ri,S))
j=0	i∈Sj+1
where Tj，lj+1 - lj for j ∈{0,…,S}.
Substituting To = 2-s and Tj = 2j-1-s for j ∈ {1,…,S} into (9), we have
12
Under review as a conference paper at ICLR 2020
E[kQs(V)-Vk2] = kVk22-2s ∑ p1(ri,s)(1-p1(ri,s))
i∈S0
s-1
+ kvk2∑22(j-s) ∑ P2(r,s)(1 —P2(r,s))
j=0	i∈Sj+1
≤ kVk22—2s ∑ P1(ri,s)
i∈S0
s—1
+ kvk2 ∑ 22(j-s) ∑ P2(ri, s)
(10)
j=0	i∈Sj+1
We first note that ∑i∈S0 p1 (ri,s) ≤ d and ∑i∈S j+1 p2(ri,s) ≤ d for all j, i.e., an upper bound on the
variance of Qs(V) is given by E[kQs(V) - Vk2] ≤ kVk2d/3(2-2s+1 + 1). Furthermore, we have
∑ p 1(r, s) ≤ min{d0,2s√d0}
i∈S0
since ∑i，?"| ≤ √d0. Similarly, We have
∑ P2", s) ≤ min{dj+1,2(s-j) Pd~+Λ }.
(11)
(12)
i∈Sj+1
Substituting the upper bounds in (11) and (12) into (10), an upper bound on the variance of Qs(v) is
given by
E[kQs(V)-Vk2] ≤ min{2-2sd0,2-s√d0Mvk2
S —1	____
+ ∑ min{22(j-s)dj+1,2j - s Pd+1 }∣∣v∣∣2.
j=0
(13)
The upper bound in (13) cannot be used directly as it depends on {d0,…,ds}. Note that dj's depend
on quantization intervals. In the following, we obtain an upper bound on E[kQs (v) - vk2], which
depends only on d and s. To do so, we need to use this lemma inspired by (Alistarh et al., 2017,
Lemma A.5): Let k∙ ∣∣o count the number of nonzero components.
Lemma 2. Let v ∈ Rd. The expected number of nonzeros in Qs(v) is bounded above by
E[kQs(v)k0] ≤ 22S + PO2S.
Proof. Note that d — d0 ≤ 22s since
(d — d0)2—2s ≤ ∑ ri2≤ 1.	(14)
i6∈S0
For each i ∈ S0, Qs(vi) becomes zero With probability 1 — 2sri, Which results in
E[kQs(V)k0] ≤ d — d0 + ∑ ri2S
i∈S0
≤ 22S + PO2S.	(15)
□
Using a similar argument as in the proof of Lemma 2, We have
d — d0 — d 1-----dj ≤ 22(s-j)	(16)
for j = 0,1,…,s — 1. Define bj，d — 22(s—j) for j = 0,…,S — 1. Then
b0 ≤ d0
b1 ≤ d1 + d0
bs - 1 ≤ d0 +---+ ds - 1.	(17)
13
Under review as a conference paper at ICLR 2020
Note that ds = d - d0---ds-ι.
We define
d0，b 0 = d — 222
di，b 1 - b0 = 3∙ 22(ST)
..
..
..
ds-1 , bs-1 - bs-2 = 12
ds , d — d0 — d 1 — ••• 一 ds-1 = 4.	(18)
Note that d0 ≤ d0, d 1 + d0 ≤ d 1 + d0, ∙∙∙, ds-1 + •一+ d0 ≤ ds-1 + •一+ d0, and ds + •一+ d0 =
ds + …+ d0.
Noting that the coefficients of the additive terms in the upper bound in (13) are monotonically
increasing with j, We can find an upper bound on E[k QS(V) - v『]by replacing (d0,…，ds) with
(d0,…,ds) in (13), which gives (3) and completes the proof.
C Proof of Theorem 3 (Code-length Bound)
Let | ∙ | denote the length of a binary string. In this section, we find an upper bound on
E[|ENCODE(v)], i.e., the expected number of communication bits per iteration. Recall from
Appendix A that the quantized gradient Qs(v) is determined by the tuple (kvk, ρ,h). Write
i 1 < i2 < … < i∣∣hko for the indices of the kh∣∣0 nonzero entries of h. Let i0 = 0.
The encoding produced by ENCODE(v) can be partitioned into two parts, R and E, such that, for
j= 1,.. ., khk0,
•	R contains the codewords ERC(i j - ij-1) encoding the runs of zeros; and
•	E contains the sign bits and codewords ERC(log{2s+1 hi j}) encoding the normalized quan-
tized coordinates.
Note that ∣∣[i 1,i2 -i 1,…,i他卜-ikh∣∣0-1]k1 ≤ d. Thus, by (Alistarh et al., 2017, Lemma A.3), the
properties of Elias encoding imply that
R∣≤khk0 + (1 + O(1))kh∣0log (3).	(19)
∣h∣0
We now turn to bounding |E|. The following result in inspired by (Alistarh et al., 2017, Lemma A.3).
Lemma 3. Fix a vectOr q such that ∣q∣ pp ≤ P, let i1 < i2 < . . . i∣q∣0 be the indices Of its ∣q∣0 nOnzerO
entries, and assume each nOnzerO entry is Of fOrm Of 2k, fOr sOme pOsitive integer k. Then
∣q∣0	1
∑ ∣ERC(log(qij))∣≤(1 + O(1))log 匕)+ Ilqll。
j=1	p
+(1+O(I))kqk0loglog (⅛).
14
Under review as a conference paper at ICLR 2020
Proof. Applying property (1) for ERC (end of Appendix A), we have
kqk0	kqk0
∑ |ERC(log(qij))| ≤ (1 + o(1)) ∑ loglogqij + kqk0
j=1	j=1
≤(I+。(I))IOg (P) + l∣qko
kqk0
+(1+o(1)) ∑ lOg lOg qipj
j=1
≤(I+o(I))IOg (P)+ι∣qko
P
+(I+o (I)) kqk0logiog()
lql0
where the last bound is obtained by Jensen’s inequality.
Taking q = 2s+1h, we note that lql2 = 22s+2lhl2 and
khk2 ≤ ∑ (M + 21s )2
≤ 2∑ (∣⅛ + 22s) = 2(1 + 2ds).
By Lemma 3 applied to q and the upper bound (20),
|E| ≤ -(1 + o(1)) + 2lhl0
+ (1 + o(I))khlologlog (2 khkhk ).
Combining (19) and (21), we obtain an upper bound on the expected code-length:
E[|ENCODE(v)|] ≤ N(lhl0)
where
N (kh∣o)= b + 3kh∣o+(1 + o ⑴)E[∣∣hkolog (-d-)]
lhl0
— (1 + o (1)) + (1 + o (1))E[kh∣ologlog (8(22s + d))].
lhl0
□
(20)
(21)
(22)
(23)
It is not difficult to show that, for all k > 0, gι (ɪ)，Xlog《)is concave. Note that gι is an increasing
function up to x = k/e.
Defining g2(x)，Xloglog (§) and taking the second derivative, we have
g00 (X) = -(X ln(2) ln(C/X ))—1(1 + (ln(C / X ))—1).	(24)
Hence g2 is also concave on X < C. Furthermore, g2 is increasing up to some C/5 < X* < C/4.
We note that E[∣h∣0] ≤ 22S + √d2S following Lemma 2. By assumption 22S + √d2S ≤ d/e, and so,
Jensen’s inequality and (22) lead us to (4).
D Proof of Theorem 4 (NUQS GD for Smooth Convex Optimization)
Let g(w) and g(w) denote the full-precision and decoded stochastic gradients, respectively. Then
E[kg(w) — Vf(w)k2] ≤ E[kg(W)- Vf(w)k2]+ E[kg(W) — g(W)k2].	(25)
By Theorem 2, E[∣g(w) — g(w)∣2] ≤ εqE[∣g(w)∣2]. By assumption, E[∣g(w)∣2] ≤ B. Noting g(w)
is unbiased, E[∣g(w) — Vf (w)∣2] ≤ (1 + εQ)B. The result follows by Corollary 1.
15
Under review as a conference paper at ICLR 2020
Figure 5: Estimated variance (left) and normalized variance (right) on CIFAR10 on the trajectory of
single-GPU SGD. Variance is measured for fixed model snapshots during training. Notice that the
variance for NUQSGD and QSGDinf is lower than SGD for almost all the training and it decreases
after the learning rate drops. All methods except SGD simulate training using 8 GPUs. SuperSGD
applies no quantization to the gradients and represents the lowest variance we could hope to achieve.
E Proof of Theorem 5 (Expected Number of Communication Bits)
Assuming KB > β, then Tε = O(KBR2). Ignoring all but terms depending on d and S, We have
Tε = O(B∕ε2). Following Theorems 2 and 3 for NUQSGD, ZNUQSGD,ε = O(NqεqB/ε2), For QSGD
following the results of Alistarh et al. (2017), Zqsgd,s = O(NQεqB/ε2) where NQ = 3(S2 + S√d) +
(⅛⅛ ) + b and εQ = min( d, √).
In overparameterized networks, where d ≥ 22S+1, we have εQ = 2-sVd — 22s + O(S) and εQ =
√d/S. Furthermore, for sufficiently large d, NQ and NQ are given by O(2s√dlog (条))and
O (SVd log( √d)), respectively.
F Additional Experiments
Figure 6: Accuracy on the hold-out set on CIFAR10 (left) and on ImageNet (right) for training
ResNet models from random initialization until convergence. For CIFAR10, the hold-out set is the
test set and for ImageNet, the hold-out set is the validation set.
In this section, we present further experimental results in a similar setting to Section 5.
In Figure 6, we show the test accuracy for training ResNet110 on CIFAR10 and validation accuracy for
training ResNet34 on ImageNet from random initialization until convergence (discussed in Section 5).
Similar to the training loss performance, we observe that NUQSGD and QSGDinf outperform QSGD
in terms of test accuracy in both experiments. In both experiments, unlike NUQSGD, QSGD does not
recover the test accuracy of SGD. The gap between NUQSGD and QSGD on ImageNet is significant.
We argue that this is achieved because NUQSGD and QSGDinf have lower variance relative to QSGD.
It turns out both training loss and generalization error can benefit from the reduced variance.
16
Under review as a conference paper at ICLR 2020
Training Iteration
1e4
Figure 7: Estimated normalized variance on CIFAR10 (left) and ImageNet (right). For different
methods, the variance is measured on their own trajectories. Note that the normalized variance of
NUQSGD and QSGDinf is lower than SGD for almost the entire training. It decreases on CIFAR10
after the learning rate drops and does not grow as much as SGD on ImageNet. Since the variance
depends on the optimization trajectory, these curves are not directly comparable. Rather the general
trend should be studied.
We also measure the variance and normalized variance at fixed snapshots during training by evaluating
multiple gradient estimates using each quantization method. All methods are evaluated on the same
trajectory traversed by the single-GPU SGD. These plots answer this specific question: What would
the variance of the first gradient estimate be if one were to train using SGD for any number of
iterations then continue the optimization using another method? The entire future trajectory may
change by taking a single good or bad step. We can study the variance along any trajectory. However,
the trajectory of SGD is particularly interesting because it covers a subset of points in the parameter
space that is likely to be traversed by any first-order optimizer. For multi-dimensional parameter
space, we average the variance of each dimension.
Figure 5 (left), shows the variance of the gradient estimates on the trajectory of single-GPU SGD on
CIFAR10. We observe that QSGD has particularly high variance, while QSGDinf and NUQSGD
have lower variance than single-GPU SGD.
We also propose another measure of stochasticity, normalized variance, that is the variance normalized
by the norm of the gradient. The mean normalized variance can be expressed as
E[% [∂ l (w; z)∕∂ 辿
E i [EA [(∂ l (w; z)∕∂ Wi )2]]
where l (w; z) denotes the loss of the model parametrized by w on sample z and subscript A refers to
randomness in the algorithm, e.g., randomness in sampling and quantization. Normalized variance
can be interpreted as the inverse of Signal to Noise Ratio (SNR) for each dimension. We argue
that the noise in optimization is more troubling when it is significantly larger than the gradient. For
sources of noise such as quantization that stay constant during training, their negative impact might
only be observed when the norm of the gradient becomes small.
Figure 5 (right) shows the mean normalized variance of the gradient versus training iteration. Observe
that the normalized variance for QSGD stays relatively constant while the unnormalized variance of
QSGD drops after the learning rate drops. It shows that the quantization noise of QSGD can cause
slower convergence at the end of the training than at the beginning.
In Figure 7, we show the mean normalized variance of the gradient versus training iteration on
CIFAR10 and ImageNet. For different methods, the variance is measured on their own trajectories.
Since the variance depends on the optimization trajectory, these curves are not directly comparable.
Rather the general trend should be studied.
ResNet152 Weak Scaling. In Figure 8, we present the weak scaling results for ResNet152/ImageNet.
Each of the GPUs receives a batch of size 8, and we therefore scale up the global batch size by the
number of nodes. The results exhibit the same superior scaling behavior for NUQSGD relative to the
uncompressed baseline.
EF-SignSGD Convergence. In Figure 9, we present a performance comparison for NUQSGD
variants (bucket size 512) and a convergent variant of EF-SignSGD with significant levels of parameter
17
Under review as a conference paper at ICLR 2020
SPUoU ① S-Ip Od ①.！① d ① UIH
25000 -
20000 -
15000 -
10000 -
5000 -
2 GPUs	4 GPUs	8 GPUs
ResNetl52, weak scaling
Figure 8: Scalability behavior for NUQSGD versus the full-precision baseline when training
ResNet152 on ImageNet.
32bit
EF-SIGNSGD (d=64)
NUQSGD 4bit
NUQSGD 8bit
O1-------'--------'-------1-------'-------
0	50000 100000 150000 200000 250000
Time (sec)
Figure 9: End-to-end training time for ResNet50/ImageNet for NUQSGD and EF-SignSGD versus
the SGD baseline.
tuning for convergence. We believe this to be the first experiment to show convergence of the latter
method at ImageNet scale, as the original paper only considers the CIFAR dataset. For convergence,
we have tuned the choice of scaling factor and the granularity at which quantization is applied (bucket
size). We have also considered learning rate tuning, but that did not appear to prevent divergence
in the early stages of training for this model. We did not attempt warm start, since that would
significantly decrease the practicality of the algorithm. We have found that bucket size 64 is the
highest at which the algorithm will still converge on this model and dataset, and found 1-bit SGD
scaling (Seide et al., 2014), which consists of taking sums over positives and over negatives for each
bucket, to yield good results. The experiments are executed on a machine with 8 NVIDIA Titan X
18
Under review as a conference paper at ICLR 2020
GPUs, and batch size 256, and can be found in Figure 9. Under these hyperparameter values the
EF-SignSGD algorithm sends 128 bits per each bucket of 64 values (32 for each scaling factor, and
64 for the signs), doubling its baseline communication cost. Moreover, the GPU implementation
is not as efficient, as error feedback must be computed and updated at every step, and there is less
parallelism to leverage inside each bucket. This explains the fact that the end-to-end performance is
in fact close to that of the 8-bit NUQSGD variant, and inferior to 4-bit NUQSGD.
G	Variance Lower B ound
In the following theorem, We show that for any given set of levels, there exists a distribution of points
with dimension d such that the variance is in Ω(√d), and so our bound is tight in d.
Theorem 6 (Lower bound). Let d ∈ Z>0 and let (0,11,…，l, 1) denote an arbitrary Sequence of
quantization levels. Provided d ≥ (2/l1 )2, there exists a vector v ∈ Rd such that the variance of
unbiased quantization of V is lower bounded by ∣∣vk211 √d/2, i.e., the variance is in Ω(√d).
Proof. The variance of QS(v) for general sequence of quantization levels is given by
d
E[∣Qs(V)-V∣2] = ∣V∣2∑σ2(ri).
i=1
If r ∈ [ls(r), ls(r)+1], the variance σ2(r) can be expressed as
σ 2( r) = τ(r )2 p(r )(1 - P ( r)) = ( ls( r )+1 - r )(r-ls( r)).	(26)
We consider vo = [ r, r, ∙∙∙, r ] T for r = 0. The normalized coordinates is Vo = [1/√d, ∙∙∙, 1/√d] T.
Using (26) and noting 1 / √d < 11, we have
σ2(r0) = 1/Vd(11 — 1/Vd) ≥ lJ(2√d).	(27)
Summing variance of all coordinates and applying (27), the variance of Qs(v0) is lower bounded by
E[kQs(VO)-Vok2] = ∣vok2dσ2(r) ≥ ∣vo∣21∖√d/2.	(28)
H	Worst-case Variance Analysis
In this section, we first derive the optimal worst-case variance upper bound by optimizing over the
distribution of normalized coordinates for an arbitrary sequence of levels, expressed as a solution to
an integer program with quadratic constraints. We then relax the program to obtain a quadratically
constrained quadratic program (QCQP). A coarser analysis yields an upper bound expressed as a
solution to a linear program (LP), which is more amenable to analysis. We solve this LP analytically
for the special case ofs = 1 and show the optimal level is at 1/2.
Then, for an exponentially spaced collection of levels of the form (0, PS,…，P2, P, 1) for P ∈ [0,1]
and an integer number of levels, s, we write the expression of QCQP and solve it efficiently using
standard solvers. We have a numerical method for finding the optimal value of P that minimizes the
worst-case variance, for any given s and d. Through the worst-case analysis, we gain insight into
the behaviour of the variance upper bound. We show that our current scheme is nearly optimal (in
the worst-case sense) in some cases. Using these techniques we can obtain slightly tighter bounds
numerically.
H. 1 Generally Spaced Levels
Let L = (lo, 11, •一，ls, ls +1) denote an arbitrary sequence of quantization levels where 10 = 0 < 11 <
…< IS +1 = 1. Recall that, for r ∈ [0,1], we define s (r) and P (r) such that they satisfy ls(r) ≤ r ≤
ls(r)+1 and r =(1 - P(r))2乳r) + P(r)Sr中,respectively. Define T(r) = Sr5-Sr). Note that
19
Under review as a conference paper at ICLR 2020
6(r) ∈{0,1, •一，S}. Then, hi(v, S)’s are defined in two cases based on which quantization interval r
falls into:
1)Ifri ∈ [0,l1],then
h (v S) =	0 with probability 1-p1(ri,L);	(29)
i , l1 otherwise
where p1 r, L = r/l1 .
2) If r ∈ [lj -1, lj ] for j = 1,…,s + 1, then
h	lj-1 with probability 1-p2(ri,L);	30
hi(v,S) = lj	otherwise	(30)
where P 2( r, L) = (r — lj-1)∕τj -1.
Let Sj denote the coordinates of vector v whose elements fall into the (j+ 1)-th bin, i.e.,Sj,{i:
ri ∈ [lj, lj+1]} for j = 0,…，s. Let dj，|巧|.
Following Lemma 1 and steps in Theorem 2, we can show that
S
E[k QS (V)-vk2] ≤ l∣vk2 ∑ min{τ2dj, Tj pdj }.	(31)
j=0
Theorem 7	(QCQP bound). Let v ∈ Rd. An upper bound on the nonuniform quantization of v iS
given by εQP lvl2 where εQP iS the optimal value of the following QCQP:
S
Q1 :	max	∑ zj
(d0,…,ds, z0,…,ZS) j=0
subject to d — d0------dj ≤ (1/lj +1)2, j = 0,…,S — 1,
S
∑dj ≤ d,
j=0
Zj ≤ T2dj, z2 ≤ T2dj, j = 0,…,S,
dj ≥ 0, j = 0,…，S.
Proof. Following Lemma 2, we have
d — d0 — d 1-----------------------------------dj ≤ (1/lj +1)2	(32)
for j = 0,…，S — 1.
The problem of optimizing (d0,…,d$) to maximize the variance upper bound (31) subject to (32) is
given by
S
Ri :	max	∑ min{ Tjdj, Tj y/dj}
(d0 ,…,dS ) j = 0
subject to (32), j = 0,…,S 一 1,
S
∑dj≤d,	(33)
j=0
dj ∈ Z≥0, j = 0,…，s.	(34)
Let Zj，min{Tjdj, Tjy∕dj} denote an auxiliary variable for j = 0,…,S. Problem Ri can be rewritten
as
S
R2 :	max ∑ zj
(d0,…,ds,z0,…,ZS ) j=0
subject to Zj ≤ T2dj, Z ≤ Tjdj, j = 0, •…,S,	(35)
(32), (33), and (34).
20
Under review as a conference paper at ICLR 2020
The variance optimization problem R2 is an integer nonconvex problem. We can obtain an upper
bound on the optimal objective of problem R2 by relaxing the integer constraint as follows. The
resulting QSQP is shown as follows:
s
Q1 :	max	∑ zj
(d0,∙∙∙,ds,z0,∙∙∙,Zs) j=0
subject to dj ≥ 0, j = 0,…，s,	(36)
(32), (33), and (35).
Note that problem Q1 can be solved efficiently using standard standard interior point-based solvers,
e.g., CVX (Boyd and Vandenberghe, 2004).	□
In the following, we develop a coarser analysis that yields an upper bound expressed as the optimal
value to an LP.
Theorem 8	(LP bound). Let v ∈ Rd. An upper bound on the nonuniform quantization of v is given
by εLPkvk2 where εLP is the optimal value of the following LP:
s
P1 :	max ∑ τ2j dj
(d0,…,ds) j=0 j
subject to d — d0------dj ≤ (1/lj+1)2, j = 0,…，s — 1,
s
∑ dj ≤ d,
j=0
dj≥ 0, j = 0,…，s.
Proof. The proof follows the steps in the proof of Theorem 7 for the problem of optimizing
(d0,…，ds) to maximize the following upper bound
s
E[kQs(v)—vk2] ≤ kvk2∑τ2jdj.	(37)
j=0
□
Corollary 2 (Optimal level). For the special case with s = 1, the optimal level to minimize the
worst-case bound obtained from problem Pi is given by lɪ = 1 /2.
Proof. For s = 1, problem P1 is given by
P0 : max τ02 d0 + τ12d1
(d0 ,d1 )
subject to d — d0 ≤ (1/l1)2,
d0+d1 ≤d,
d0 ≥ 0, d1 ≥ 0.
Note that the objective of P0 is monotonically increasing in (d0,d1). It is not difficult to verify that
the optimal (d0, dɪ) isa corner point on the boundary line of the feasibility region of P°. Geometrical
representation shows that that candidates for an optimal solution are (d — (1/l1)2, (1/l1)2) and (d,0).
Substituting into the objective of P0, the optimal value of P0 is given by
εLP = max{τ2d, τθd + τ2∕τ2 — 1}.
Finally, note that τ0 = τ1 = 1/2 minimizes the optimal value of P0 (38).
(38)
□
21
Under review as a conference paper at ICLR 2020
100
当-APJ日≡do dσoσ
0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9
P
4 3 2 1
Oooo
3nA7e日do do。。
100.2	0.3
0.4	0.5	0.6	0.7	0.8	0.9
P
Figure 10: Optimal value of problem Q1 versus p ∈ [0, 1] for exponentially spaced collection of
levels of the form (0,ps,…，p2,p, 1).
0.1
H.2 Exponentially Spaced Levels
In this section, we focus on the special case of exponentially spaced collection of levels of the form
Lp = (0,ps,…，P2,P, 1) for P ∈ [0,1] and an integer number of levels, S. In this case, We have
To = PS and Tj = (1 - P) PS- j for j = 1,…,S.
For any given s and d, We can solve the corresponding quadratic and linear programs efficiently to
find the Worst-case variance bound. As a bonus, We can find the optimal value ofP that minimizes the
Worst-case variance bound. In Figure 10, We shoW the numerical results obtained by solving QCQP
Q1 With LP versus P using CVX (Boyd and Vandenberghe, 2004). In Figure 10 (left), We fix d and
vary S, While in Figure 10 (right), We fix S and vary d. As expected, We note that the variance upper
bound increases as d increases and the variance upper bound decreases as S increases. We observe
that our current scheme is nearly optimal (in the Worst-case sense) in some cases. Further, the optimal
value of P shifts to the right as d increases and shifts to the left as S increases.
I Nonconvex optimization
We can obtain convergence guarantees to various learning problems Where We have convergence
guarantees for SGD under standard assumptions. On nonconvex problems, (Weaker) convergence
guarantees can be established along the lines of, e.g., (Ghadimi and Lan, 2013, Theorem 2.1). In
particular, NUQSGD is guaranteed to converge to a local minima for smooth general loss functions.
Theorem 9 (NUQSGD for smooth nonconvex optimization). Let f : Ω → R denote a Possibly
nonconvex and β -smooth function. Let wo ∈ Ω denote an initial Point, εQ be defined as in Theorem 2,
T ∈ Z>0, and f * = infw∈Ω f (w). Suppose that Algorithm 1 is executed for T iterations with a
learning rate α = O(1∕β) on K processors, each with access to independent stochastic gradients of
f with a SeCOnd-moment bound B. Then there exists a random stopping time R ∈ {0, •一，T} such
thatNUQSGD guarantees E[∣∣Vf (wr)『]≤ ε where ε = O(0(f (wo) — f*)/T + (1 + εQ)B).
□
22