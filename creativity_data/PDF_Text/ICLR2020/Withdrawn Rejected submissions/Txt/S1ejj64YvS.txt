Under review as a conference paper at ICLR 2020
Good Semi-supervised VAE Requires Tighter
Evidence Lower B ound
Anonymous authors
Paper under double-blind review
Ab stract
Semi-supervised learning approaches based on generative models have now en-
countered 3 challenges: (1) The two-stage training strategy is not robust. (2) Good
semi-supervised learning results and good generative performance can not be ob-
tained at the same time. (3) Even at the expense of sacrificing generative perfor-
mance, the semi-supervised classification results are still not satisfactory. To ad-
dress these problems, we propose One-stage Semi-suPervised Optimal Transport
VAE (OSPOT-VAE), a one-stage deep generative model that theoretically uni-
fies the generation and classification loss in one ELBO framework and achieves
a tighter ELBO by applying the optimal transport scheme to the distribution of
latent variables. We show that with tighter ELBO, our OSPOT-VAE surpasses
the best semi-supervised generative models by a large margin across many bench-
mark datasets. For example, we reduce the error rate from 14.41% to 6.11% on
Cifar-10 with 4k labels and achieve state-of-the-art performance with 25.30% on
Cifar-100 with 10k labels. We also demonstrate that good generative models and
semi-supervised results can be achieved simultaneously by OSPOT-VAE.
1 Introduction
The rise of deep neural networks has led to breakthroughs in computer vision, natural language
processing, and many other domains. Most of these models are trained on large labeled datasets
via supervised learning. However, in many scenarios, although it is easy to acquire a large amount
of the original data, obtaining corresponding labels is often very costly or even infeasible. Semi-
supervised learning (Thomas, 2009) is proposed to address this problem by training classifiers with
sufficient unlabeled data and a small fraction of labeled data.
Recent works on semi-supervised learning can be grouped into three categories: (1) disagreement
based learning via data perturbation (Miyato et al., 2019) and consistency enforcing (Verma et al.,
2019), (2) metric learning (Wu et al., 2018), (3) generative approaches via generative adversar-
ial network (GAN) (Springenberg, 2016) and variational autoencoder (VAE) (Kingma et al., 2014).
Compared with the first two categories, generative approaches have great advantages in interpretabil-
ity. Based on the latent variable assumption (Doersch, 2016), the generative model has an explicit
variational inference form, so it can learn the marginal probability distribution of the raw data as
well as the conditional distribution of the latent variables given the input data, which makes pre-
dictions more reasonable. Besides, generative approaches not only learn the required classification
representations, but also capture the semantics-disentangled factors that generate the data, making it
easier to generalize to different tasks (Narayanaswamy et al., 2017).
However, in practice, semi-supervised generative approaches often encounter three major chal-
lenges: (1) The two-stage training process is not robust. Semi-supervised VAE (Kingma et al.,
2014) needs to be trained carefully with a two-stage hierarchical strategy, while the training pro-
cess of GAN is a two-stage adversarial game (Chrysos et al., 2019). (2) Good semi-supervised
learning results and good generative performance can not be obtained at the same time. In GAN,
good semi-supervised learning performance will lead to a mismatch between the generated results
and the real data distribution (Dai et al., 2017). While in VAE, the evidence lower bound (ELBO)
objective is irrelevant to the classification loss, making it difficult to learn from the labels directly
(Narayanaswamy et al., 2017). (3) Even at the expense of sacrificing generative performance, the
semi-supervised classification results are still not satisfactory. In practice, disagreement-based meth-
1
Under review as a conference paper at ICLR 2020
X qψ(z∣x) = JV'(zjg,σ)
Pe(XlZ, c) ? 7
↑ x1+x2=xλ
⅜(z∣^l)
inference
一施
sample
q0(c∣X) = Mult(c; 7r)
________：1
sampled in Dl
的＞QIX2)
⅜(c∣Afi)
ilr
q0(clx2)
optimal p(z∖Xλ) 加 ClXQ
transport
inference
%(z∣xQ
∣Λ"
q(√c∣xQ
«，/margin estimation
LMZ=DKL (%(z∣X;ι)∣∕( ZlXQ)
J LMC = DKL(Y(C∣XQ∣∣∕(c∣XQ)
φ
y
(a) One-stage SSL VAE
θ
兀2

(b) Optimal Transport Estimation
Figure 1: The schematic of OSPOT-VAE
ods (Xie et al., 2019; Berthelot et al., 2019) have dramatically improved the state-of-the-art results
on several standard datasets, surpassing generative approaches by a large margin. These challenges
naturally raise a question: What limits the performance of generative approaches in semi-supervised
learning?
In this work, we propose One-stage Semi-suPervised Optimal Transport VAE (OSPOT-VAE) to
address these challenges, which consists of two improvements: (1) a one-stage semi-supervised
VAE model that unifies the generation and classification loss in one ELBO framework. (2) an
estimation of the margin between true log-likelihood and the ELBO that exports a tighter evidence
lower bound by applying optimal transport (Ambrosio & Gigli, 2013) scheme to the distribution of
latent variables.
Our model has the following contributions:
•	We show that OSPOT-VAE can be well trained with a direct one-stage strategy.
•	We show that OSPOT-VAE can achieve both good generative performance and semi-
supervised learning results simultaneously on a series of benchmark datasets.
•	We point out that it is the large margin between the ELBO and the log-likelihood of the
input data that limits the performance of semi-supervised VAE. Besides, we evaluate this
assumption across many standard datasets and show that with the proposed tighter ELBO,
OSPOT-VAE surpasses the best semi-supervised generative models by a large margin and
achieves state-of-the-art performance on Cifar-100 with 10k labels.
2	Semi-Supervised Learning Methods
In supervised learning (SL), we are facing with training data that appears as input-target pairs
(X, y) ∈ DL sampled from an unknown distribution p(X, y). Our goal is to learn a function
f (X; φ) parameterized by φ that makes the correct inference y for unseen samples from p(X).
While in semi-supervised learning (SSL), we can obtain an extra collection of unlabeled data
X ∈ DU sampled from the same distribution p(X). We hope to leverage the data from both DL
and DU to achieve a more accurate model than what would have been obtained by only using DL .
In this section, we review some existing methods for SSL. We mainly focus on those who have
reached state-of-the-art results, as well as generative approaches which are strongly connected with
our model; the more comprehensive overview is beyond the scope of this paper, we refer readers to
(Oliver et al., 2018).
2.1	Disagreement Based Learning
Disagreement-based learning refers to the general approaches of imposing disagreement among
multiple learners on the same task or multiple predictions from a single learner. By eliminating the
disagreement, we can enforce the generalization of the model on unseen data. A common technique
for creating disagreement is data augmentation, which applies transformations or perturbations on
the input data and leaves class semantics unchanged. For X ∈ DU, loss term can be derived as
kf (Augment(X); φ) - f(X; φ)k22	(1)
where the Augment(X) is a stochastic function which can be obtained by image transformation (Xie
et al., 2019), virtual adversarial training (Miyato et al., 2019), or mixup method (Verma et al. 2018;
2
Under review as a conference paper at ICLR 2020
Berthelot et al. 2019). Another disagreement construction technique is to train multiple learners on
the same dataset and utilize the loss
kf (x； Φι) - f(χ; Φ2)k2	⑵
to enforce the predictive consistency of different models, for example, “Mean Teacher” (Tarvainen
& Valpola, 2017) and “Teacher Graph” (Luo et al., 2018). The generalization of the models gets
enhanced.
2.2	Generative Approaches
In generative approaches, input x is supposed to have corresponding continuous and discrete latent
variables, which we denote by z and c respectively.
Feature matching (FM) GANs (Salimans et al., 2016; Dai et al., 2017) apply GANs to semi-
supervised learning on K-classification tasks by specifying a (K+1)-class objective for the discrim-
inator. Instead of binary classification, true samples are classified into the first K classes respec-
tively and fake samples are classified into the (K+1)-th class. This target function achieves strong
empirical results by matching the generator distribution with true data distribution and improves
semi-supervised classification performance.
Semi-supervised VAEs (Kingma et al. 2014; Narayanaswamy et al. 2017) construct a probabilistic
model parameterized by θ and φ that respectively describe the generation and inference process
between x and latent variables z, c. The generation process of x by z and c is :
P(Z)= N(z; 0, I);	P(C)= MUlt(c; K, ∏);	Pθ(X|z, c) = f(X;z,c, θ)	(3)
where Mult(K, π) is the multinomial distribution with class K and parameter π. f(x； z, c, θ) is a
sUitable likelihood fUnction, e.g. a BernoUlli or GaUssian distribUtion, parameterized by a non-linear
transformation of the latent variables z and c. The class label y is treated as c if given. For the
inference process, with the following hypothesis
qφ(z, C∣X) = qφ(z∣X)qφ(c|X);	p(z, c|X) = p(z∣X)p(c∣X);	p(z, c) = p(z)p(c) (4)
evidence lower boUnd (ELBO) is Used as objective to predict the posterior distribUtion of latent
variables as follows (see Appendix A.1 for proof):
logp(X) ≥ Eqφ(z,c∣χ)[logPθ(X∣z,c)] - Dkl(qφ(z∣X)kp(z)) - DKL(qφ(c∣X)kp(c)) = ELBO
(5)
For the likelihood log P(x) is infeasible, VAE maximizes its evidence lower boUnd instead, which
derives the negative ELBO loss fUnction L(x; φ, θ) = -ELBO. The predictions for the classifica-
tion label y can be obtained from the inferred posterior distribution qφ(c∣X). When class label y is
not given, missing label sampling technique is used to sample from qφ(c∣X) as
K
Eqφ(c∣χ)f (X; c, θ) = X qφ(yk |X)f (X; y, θ)	⑹
k=1
Here yk represents a one-hot vector with 1 in k-th dimension. Utilizing this sampling method,
the algorithmic complexity of VAE is proportional to K so it is computationally inefficient. Note
that in objective (4), the label predictive distribution qφ(c∣X) only contributes to the generative
performance. To remedy this, existing models simply add a cross-entropy loss to the negative ELBO
loss such that the distribution qφ(c∣X) can also learn classification rules from the labeled data. The
extended objective loss is
minEX〜DUL(X; φ, θ) + E(x,y)〜心工[L(X, C = y; φ, θ) - log qφ(y∣X)]	⑺
φ,θ
Two-stage Training Strategy: In practice, (Kingma et al., 2014) finds that directly training the
one-stage objective (7) will lead to a bad semi-supervised learning result, so a two-stage training
strategy is proposed to improve the model. The two-stage training strategy consists of two parts,
M1 and M2. M1 means to learn a new continuous latent representation z1 first, and M2 means
to train a semi-supervised model (7) with the embedding z1 from M1 instead of the raw data X.
This M1+M2 strategy builds a deep VAE with two layers of random variables: Pθ(X, z1, z2, c) =
Pθ(X∣zi)pθ(z1∣z2, C)P(Z2)p(c), which can dramatically improve the performance of the inference
qφ(c∣X) but is not robust in training. Moreover, GAN's training process can also be considered as
two-stage with generator and discriminator competing with each other, and the two-stage adversarial
game adds the difficulty in training.
3
Under review as a conference paper at ICLR 2020
3 One- S tage Semi-Supervised Optimal Transport VAE
In this section, we introduce our semi-supervised VAE framework, OSPOT-VAE. Firstly, we derive
a one-stage loss function that unifies the generation and classification loss under one ELBO without
introducing any additional auxiliary loss items like (7). Then, we analyze a phenomenon that good
ELBO values do not guarantee good semi-supervised performance and propose the optimal transport
estimation to deal with it. At last, combining the two parts, we give the detailed algorithm of OSPOT-
VAE and discuss some problems in model optimization.
3.1	One-stage Semi-Supervised VAE
Following the notations and assumptions (3, 4) in Section 2.2, we derive our one-stage semi-
supervised VAE. With the empirical distribution Pemp (X; D) = —— E 1x=x，, We utilize the
| | X0∈D
decomposition in (Zhao et al., 2017) and rewrite the second part of (5) into (proof in Appendix A.2)
Epemp(X)DKL(qφ(ZIX)kp(Z)) = Iqφ(X； Z) + DκL(qφ(Z)kp(Z)) ≥ Iqφ(X； Z)
(8)
where qφ(z) = ɪ X qφ(z∣x) and Iqφ(X; z) is the mutual information between X and z. The
| | X∈D
left part of (8) equals to 0 when X and z are independent. This is undesirable, so Iqφ (X; z) can be
regarded as the lower bound of controlled mutual information. The continuous variables in (8) can
be easily extend to discrete variables c. We can use Iz and Ic to denote the controlled information
capacity and derive the objective for the unlabeled dataset DU
LDU (X； θ, φ) = Eqφ(z,c∣X)[- logPθ (X∣z,c)] + βz∣DκL(qφ(z∣X)kp(z) - Iz |
+ βc∣DκL(qφ(c∣X)kp(c)) - Ic|
(9)
where β, Iz , Ic are all hyper-parameters forcing the KL divergence term to match the mutual infor-
mation capacities of z and c.
For the labeled subset DL, instead of directly employing class label y as sampled c, we view it as the
parameter of the true posterior distribution, i.e. p(c|X) = Mult(c; K, y) and derive the following
one-stage ELBO form:
logP(X) = log Eqφ(z∣X),p(c∣X)
p(X, z, c)
qφ(z∣x)p(c∣x)
≥ Eqφ(z∣X),p(c∣X) log
p(X, z, c)
qφ(z∣x)p(c∣x)
γ∖ I 7 ] γ^ιl c)
Eqφ(z∣X),p(c∣X) [log P(XIz, c) + log q@ (ZIX)P(CIX)] = Eqφ(z∣X),p(c∣X) log P(X忆 c)	(IO)
-DKL(qφ(ZIX)kp(Z))- DKL(P(CIX)kqφ(clX)) + Ep(c∣x) log 〃p(cX、
qφ(CIX)
Notice that DKL(P(C∣X)kqφ(c∣X)) is equal to the common cross-entropy loss for y is a one-hot
vector. In this respect, the margin between P(cIX) and qφ(cIX) can be significantly small when
the suitable optimization method is chosen. This allows us to utilize the approximation P(cIX) ≈
P(c)
qφ(c∣X) to modify Ep(c∣χ) log - in (10), resulting in a consist ELBO with LDU (X； θ, φ):
qφ(cIX)
Ep(c∣χ) log	≈(whenP(CIX)≈qφ(CIX)) Eqφ(c∣χ) log	= DKL(qφ(c∣X)kρ(c)) (11)
qφ(cIX)	qφ(cIX)
Combining the ELBO form (10) of DL with the mutual information decomposition (8) and the
approximation (11), the new objective for semi-supervised VAE is:
LDL (X, y； θ, φ) = Eqφ(z∣X),p(c∣X)[- logPθ (X∣Z,c)] + βzDKL(qφ(z∣X)kP(z)) - IzI
+ βCIDKL(qφ(cIX)kP(c)) - ICI + DKL(P(cIX)kqφ(cIX))
With (9) and (12), the objective for the entire dataset is now
min EX 〜Pemp(X;DU )LDu (X； θ, φ) + E(X,y)〜Pemp ((X,y )；DL)LDL (X, y; θ, φ)
φ,θ
(12)
(13)
This one-stage objective with a simple approximate transformation (9) unifies the generation loss as
well as the target of SSL and results in improved performance of semi-supervised learning, which
we demonstrate in Section 4.1.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Optimal transport estimation ingests a batch of observation X as well as the represen-
tation qφ(z∣X), qφ(c∣X) inferred from the original VAE and returns the estimation of the margin
DκL(qφ(z∣X)kp(z∣X)) andDκL(qφ(c∣X)kp(c∣X)).
Input:
Batch of observation X sampled from pemp(X);
Inferred parameter (μ, diag(σ2)) of qφ(z∣X) = N(z; μ, diag(σ2));
Inferred parameter π of qφ(c∣X) = Mult(c; K, π);
Hyperparameter α for mixup vicinal distribution pmixup(X)
Output:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
X sampled from pmixup(X);
Estimation LMz of the margin DκL(qφ(z∣X)kp(z∣X));
Estimation LMc of the margin DκL(qφ(c∣X)kp(c∣X))
X0, π0, μ0, σ02 = RandomPermutation(X, π, μ, σ2)
~ ,
X = λ * X + (1 - λ) * X0,λ ∈ β(α, α)
∏ = OptimalTransportC(π, π0, λ)
(μ, σ2)= OPtimalTranSPortZ((μ, σ2), (μ0, σ02), λ)
qφ(z∣X),qφ(c∣X) = VAE(X)
p(z∣X) = N(z; μ, diag(σ2))
, .二_.	.   、
p(c∣X) = Mult(c; K, π)
.~.....~ ..
LMz = DκL(qφ(z∣X)kP(z∣X))
LMc = DκL(qφ(c∣X)kP(c∣X))
return X, LMz , LMc
3.2	Optimal Transport Estimation
To summarize the above, VAE aims to learn the useful representation qφ(z∣X) and qφ(c∣X) by
reducing the KL divergence between the empirical distribution pemp(X) and the model marginal
p(X) = z c p(X)p(z|X)p(c|X)dzdc. Instead of minimizing DκL(pemp(X)kp(X)) directly,
VAE models use the expected ELBO mentioned in (5) as target via the following inequality
DκL(pemp(X)kp(X)) ≤ H(pemp(X)) - Epemp(X)ELBO	(14)
However, one phenomenon is that good ELBO values do not imply accurate inference. A typical
example has been discussed in (Zhao et al., 2017). Here we mainly focus on the cause of this
phenomenon and propose optimal transport estimation to alleviate this problem in semi-supervised
learning. Following the work in (Rezende et al., 2014), we write down the closed form of the
expected margin between true log-likelihood and ELBO as (proof in Appendix A.3):
Epemp(X)[logP(X) -ELBO] = Epemp(X)[DκL(qφ(z∣X)kp(z∣X)) + DκL(qφ(c∣X)kp(c∣X))]
(15)
Combined with the decomposition (8), training the expected ELBO target can only reduce the dif-
ference between marginal distributions qφ(c),qφ(z) and p(c),p(z). It means that even with a good
ELBO, the margin Epemp(X)DKL(q0(z|X)kp(z|X)) and Epemp(X)DKL(qφ(c∣X)kp(c∣X)) in (15)
can still be large. In this scenario, the consistent optimization of ELBO will contribute no more to
the semi-supervised classification performance. However, optimizing the margin in (15) directly
is impossible, for p(c|X) and p(z|X) are unknown. To remedy this, we extend the empirically
effective approximation in (Zhang et al., 2018) to our VAE framework with the form
Epmixup(X)DκlSφ(ZIX)kp(ZIX)) ≈ Epemp(X)DκlSφ(ZIX)kp(ZIX))(O → O)
Epmixup(X)DKL(qφ(c∣X)kp(c∣X)) ≈ Epemp(X)DKL(qφ(c∣X)kp(c∣X))(α → 0)
(16)
where pmixup(X) is the mixup vicinal distribution (Zhang et al., 2018) and α is the re-
lated parameter. Then we propose optimal transport estimation to construct the estimations of
Epmixup(X)DκL(qφ(zIX)kp(zIX)) as well as Epmixup(X)DκL(qφ(zIX)kp(zIX)) by applying op-
timal transport scheme to latent variables z and c. The computation steps of optimal transport
estimation are provided in Algorithm 1, and we present the details of the optimal transport scheme
in the rest of this section.
5
Under review as a conference paper at ICLR 2020
Algorithm 2 OSPOT-VAE training process with epoch t.
Input:
Batch of labeled pairs (XL, yL) ∈ DL,Batch of unlabeled examples XU ∈ DU;
ELBO hyperparameters: βz, βc, Iz, Ic = ELBOScheduler(t);
Optimal transport estimation weights: wMz , wMc = WeightScheduler(t);
Model parameters: θ(t-1), φ(t-1);
Model optimizer: SGD
Output:
Updated parameters: θ(t) , φ(t)
1:	LL = LDL (XL, yL; θ(t-1), φ(t-1); βz, βc, Iz, Ic)
2:	LU = LDU(XU； θ(t-1), φ(t-1);βz,βc, Iz,Ic)
3:	LMz, Lmc =OPtimalTransPortEstimation(XU,qφ(z∣Xu),qφ(c∣Xu))
4:	L = LL + LU + wMz LMz + wMc LMc
5： θ⑴,Φ㈤=SGD(θ(I), φ(T), ∂θ, ∂L)
6: return θ(t), φ(t)
Optimal Transport Scheme: The mixuP vicinal distribution can be understood as aPPlying linear
transPort between the Points X, X0 ∈ D, extending the original dataset with new Points falling on
one straight line X = λ * X + (1 - λ) * X0,λ ∈ [0,1]. For X, it is a natural thought that this linear
transformation could associate with the shortest-Path transPort in the latent sPace. Based on this,
We calculate the distributions p(z∣X),p(c∣X) of z, C and consider them as the estimation of the true
Posterior distributions. Following the work of (Ambrosio & Gigli, 2013), the norm-2 based oPtimal
transPort scheme γ(x, y) betWeen tWo distributions p(x) and p(y) satisfy:
s.t.
inf kx - yk22γ(x, y)dxdy
γ(x,y) x y
γ(x, y)dy = p(x);	γ(x,y)dx = p(y)
yx
(17)
For the continuous variable Z 〜 N(μ, diag(σ2)) and discrete variable C 〜 Mult(K, π), the fol-
loWing 2 ProPositions are ProPosed to calculate the shortest-Path based on oPtimal transPort scheme
(see APPendix A.4 for Proof).
Proposition 3.1. The Shortest-Path derived from optimal transport Scheme (17) between zι 〜
N(μι, diag(σ2)) and z 〜N(μ2, diag(σ2)) with λ ∈ [0,1] is
μ = λμι +(I - λ)μ2
σ = λσι + (1 — λ)σ2
(18)
Proposition 3.2. The shortest-path derived from KL divergence based optimal transport scheme
between ci 〜Mult(K, ∏ι) and C2 〜Mult(K, ∏2) with λ ∈ [0,1] is
∏ = λ∏i + (1 - λ)∏2	(19)
Algorithm 1 yields the oPtimal transPort estimation of the margin in (15), Which leads to a tighter
ELBO. In Section 4.2, We demonstrate that With this tighter ELBO, the inference Performance of
semi-suPervised VAE is significantly imProved on many benchmark datasets.
3.3 Optimization of OSPOT-VAE
Combining one-stage semi-suPervised VAE and oPtimal transPort estimation, We can get the com-
Plete OSPOT-VAE model. The full OSPOT-VAE algorithm is Provided in Algorithm 2, and a
schematic is shoWn in Figure 1. Note that the conditions for the aPProximations used in Algo-
rithm 1,2 satisfy (1) qφ(c|X) ≈ p(c∣X) and (2) the VAE model has already achieved a good ELBO.
Therefore, the Warm-uP schedule (Higgins et al., 2017) is used to set Parameters Iz, Ic, βz, βc and
wMz , wMc . We list the details of “ELBOScheduler(t)” and “WeightScheduler(t)” in APPendix A.5.
In Algorithm 2, We aPPly stochastic gradient descent (SGD) as oPtimizer, Which needs to cal-
culate the gradient Vθ,φL. The target loss L consists of KL divergence and the expected log-
likelihood . The derivation of KL divergence Part has a closed form, While calculating the gradient
6
Under review as a conference paper at ICLR 2020
BackBone	Method	MNIST(100 labels)	SVHN(1k labels)
Disentangled VAE (Narayanaswamy et al., 2017)	9.71(±0.91)	38.91(±1.06)
Same with M1+M2	M1(Kingma et al., 2014)	11.97(±1.71)	54.33(±0.11)
M1+M2(Kingma et al., 2014)	3.33(±0.14)	36.02(±0.10)
One-stage VAE	3.14(±0.19)	27.38(±0.78)
Table 1: One-stage VAE error rate in MNIST and SVHN.
BackBone	Model category	Model	Cifar10(4k labels)
		TemPOraI EnSembIing(TE) (Laine & Aila, 2017)	16.37
	Disagreement	Mean Teacher(Tarvainen & Valpola, 2017)	15.87
WRN-28-2		VAT+EntMin(Miyato et al., 2019)	13.13
		MixMatch(Berthelot et al., 2019)	6.37
	Generative	GS-BadGANt*(Li et al., 2019)	17.11
		OSPOT-VAE	8.51(±0.32)
		AUtOAUgment(CUbUk et al., 2019)	14.1
	Disagreement	Temporal Ensembing(Laine & Aila, 2017)	12.16
WRN-28-10		MiXMatCh*(Berthelot et al., 2019)	4.95
		GS-BadGANt* (Li et al., 2019)	14.41
	Generative	GAN combine TEt*(Wei et al., 2018)	9.98
		OSPOT-VAE	6.11(±0.34)
Table 2: Error rate in Cifar10. f denotes the best semi-supervised generative approach result. ∣
denotes the model ensemble two categories. * denotes the corresponding backbone is not exactly
WideResNet (Zagoruyko & Komodakis, 2016), but belongs to one kind of its variations with a
comparable amount of parameters.
of Eqφ(z∣x),qφ(c∣x) logPθ(X∣z, C) is difficult. To this end, we follow the work of (Rezende et al.,
2014) and (Jang et al., 2017), using the reparameterization trick as
Vθ,φEqφ(z∣χ) logPθ(X|z) = ENyo,ι)Vθ,φ logPθ(X∣μ + σ ∙ e)
log π +	(20)
EGUmbeιyo,i))Vθ,ΦlogPθ(X∣Softmax(——T——))→ vθ,φEqφ(c∣x) logPθ(X∣c)(τ → 0)
Note that with (20), the algorithmic complexity of one-stage semi-supervised VAE is independent
with the class number K, making it easier to extend to large-scale classification tasks.
4 Experiments
In this section, we demonstrate the 3 contributions of our OSPOT-VAE model with sufficient
experiments on 4 standard SSL benchmark datasets, that is, MNIST, SVHN, Cifar10, and Ci-
far100. In Section 4.1, we show the validity of our one-stage semi-supervised VAE objective
(13) by comparing with other one-stage and two-stage VAE models. Then, we evaluate the per-
formance of OSPOT-VAE under “WideResNet”(Zagoruyko & Komodakis, 2016) backbone and
compare with other state-of-the-art SSL models mentioned in Section 2. Besides, We provide
an ablation study to verify the contribution of the optimal transport estimation. As an addi-
tional application, we show that good generative models and semi-supervised results can be ob-
tained at the same time by OSPOT-VAE (Section 4.3). The source code is available at https:
//github.com/PaperCodeSubmission/OSPOT-VAE; more details are available in Ap-
pendix A.6.
4.1	One- S tage Semi-supervised VAE
We evaluate the effectiveness of the one-stage semi-supervised VAE objective on 2 standard bench-
marks, MNIST and SVHN. As for baseline models, we consider two VAE-based SSL models,
which are one-stage disentangled VAE (Narayanaswamy et al., 2017) and two-stage VAE(M1+M2)
(Kingma et al., 2014). For fairness, except the target loss functions, all models use the same struc-
ture as is used in M1+M2 (Kingma et al., 2014). The results are presented in Table 1, and our model
achieves the best performance.
7
Under review as a conference paper at ICLR 2020
Table 3: Error in Cifar100. f and * have the same meaning as described in Table 2.
BackBone	Model	Cifar100(4k labels)	Cifar100(10k labels)
	Π — Model(Laine & Aila, 2017)	\	39.19
WRN-28-2	GS-BadGANt* (Li et al., 2019)	45.11	37.16
	LP* (Iscenetal., 2019)	43.73	35.92
	OSPOT-VAE	40.58(±0.48)	31.41(±0.21)
WRN-28-10	MixMatch* (Berthelot et al., 2019)	\	25.88
	OSPOT-VAE	33.76(±0.53)	25.30(±0.31)
Table 4: Ablation study with SVHN, Cifar10, and Cifar100.
Methods	SVHN(Ik labels)	Cifar10(4k labels)	Cifar100(10k labels)
One-stage VAE	-10.53(±0.17)-	18.26(±0.51)	38.62(±0.67)
Optimal transport estimation (with encoder only)	6.54(±0.62)	10.71(±0.44)	36.21(±0.29)
OSPOT-VAE	5.79(±0.15)	8.51(±0.32)	31.41(±0.21)
4.2 OSPOT-VAE
We compare the results of OSPOT-VAE with two categories of state-of-the-art models mentioned
in Section2. In all experiments, we use the “WideResNet-28” model or other deep models with
a comparable amount of parameters as the backbone. The results in Table 2,3 demonstrate that
our model outperforms most of the existing methods and surpasses state-of-the-art semi-supervised
generative models (Dai et al., 2017) by a large margin. Notice that recently, data-augmentation
based method, MixMatch, (Berthelot et al., 2019) achieves the absolute state-of-the-art results in all
benchmarks. It uses pre-designed sophisticated data augmentation strategies for different datasets
and outperforms our model. We list its results fairly as a comparison, while OSPOT-VAE surpasses
it in Cifar100 dataset.
Ablation Study: The OSPOT-VAE model consists of two parts: (1) a one-stage VAE objective and
(2) an optimal transport estimation. In ablation study, we analyze the effect of each component in
our model with the backbone “WideResNet-28-2”. To study the independent effects of transport es-
timation, we combine it with the encoder part of OSPOT-VAE to build a classifier with loss function
LMc . The improved classification error rates in Table 4 show that, with optimal transport estima-
tion, the posterior inference qφ(c∣X) gets closer to the true distribution p(c∣X). It indicates that our
optimal transport estimation does reduce the gap between ELBO and the log-likelihood of the input
data and yield a tighter ELBO, which leads to a better semi-supervised performance.
Table 5: Generative performance measured by ELBO with ELBO ≤ log p(X)
Model	Cifar10	Cifar100
-Pure VAE	-226.25(±14.25)~^^-1292.91(±1.10)
OSPOT-VAE	-237.62(±6.27)	-1271.82(±24.15)
4.3 generative performance
Epemp(X)ELBO measures the margin between the true data distribution and the distribution learned
by generation models (Doersch, 2016). By comparing the Epemp(X) value of pure unsupervised VAE
and our semi-supervised VAE model under the same “WideResNet-28-2” backbone, we demonstrate
that good generative models and semi-supervised results can be obtained at the same time in OSPOT-
VAE. The results in Table 5 show that the data generative distribution learned by our OSPOT-VAE
model is as good as the pure VAE model. Further generated results are available in Appendix A.7.
5 Conclusion
In this work, we pointed out that it was the large margin between ELBO and the true log-likelihood
of the raw data that limits the performance of semi-supervised VAE. To this end, we introduced
OSPOT-VAE, a one-stage generative model that unified the classification and generation objec-
tive and achieved a tighter ELBO by optimal transport estimation. We demonstrated our asser-
tion through extensive experiments, and our semi-supervised results significantly outperform former
state-of-the-art generative SSL methods by a large margin on Cifar10 and Cifar100.
8
Under review as a conference paper at ICLR 2020
References
Luigi Ambrosio and Nicola Gigli. A users guide to optimal transport. 2013.
David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin
Raffel. Mixmatch: A holistic approach to semi-supervised learning. CoRR, abs/1905.02249,
2019. URL http://arxiv.org/abs/1905.02249.
Grigorios G. Chrysos, Jean Kossaifi, and Stefanos Zafeiriou. Robust conditional generative adver-
sarial networks. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019, 2019. URL https://openreview.net/forum?id=
Byg0DsCqYQ.
Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment:
Learning augmentation strategies from data. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Zihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, and Ruslan Salakhutdi-
nov. Good semi-supervised learning that requires a bad GAN. In Advances
in Neural Information Processing Systems 30:	Annual Conference on Neu-
ral Information Processing Systems 2017,	4-9 December 2017, Long Beach,
CA, USA, pp. 6510-6520,	2017. URL http://papers.nips.cc/paper/
7229-good-semi-supervised-learning-that-requires-a-bad-gan.
Carl Doersch. Tutorial on variational autoencoders. CoRR, abs/1606.05908, 2016. URL http:
//arxiv.org/abs/1606.05908.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representa-
tions, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. URL
https://openreview.net/forum?id=Sy2fzU9gl.
Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-
supervised learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5070-5079, 2019.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-
26, 2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?
id=rkE3y85ee.
Diederik P. Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-
supervised learning with deep generative models. In Advances in Neural Information Processing
Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13
2014, Montreal, Quebec, Canada, pp. 3581-3589, 2014. URL http://papers.nips.cc/
paper/5352- semi-supervised-learning-with-deep-generative-models.
Max Kuang and Esteban G. Tabak. Preconditioning of optimal transport. SIAM J. Scientific
Computing, 39(4), 2017. doi: 10.1137/16M1074953. URL https://doi.org/10.1137/
16M1074953.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In 5th Inter-
national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?
id=BJ6oOfqge.
Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier, and Corey W. Arnold. Semi-
supervised learning based on generative adversarial network: a comparison between good GAN
and bad GAN approach. CoRR, abs/1905.06484, 2019. URL http://arxiv.org/abs/
1905.06484.
9
Under review as a conference paper at ICLR 2020
Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for
semi-supervised learning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2018, Salt Lake City, UT USA, June 18-22, 2018, pp. 8896-8905, 2018. doi: 10.1109/
CVPR.2018.00927. URL http://openaccess.thecvf.com/content_cvpr_2018/
html/Luo_Smooth_Neighbors_on_CVPR_2018_paper.html.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A
regularization method for supervised and semi-supervised learning. IEEE Trans. Pattern Anal.
Mach. Intell., 41(8):1979-1993, 2019. doi: 10.1109/TPAMI.2018.2858821. URL https://
doi.org/10.1109/TPAMI.2018.2858821.
Siddharth Narayanaswamy, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah D.
Goodman, Pushmeet Kohli, Frank D. Wood, and Philip H. S. Torr. Learning disentangled rep-
resentations with semi-supervised deep generative models. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9
December 2017, Long Beach, CA, USA, pp. 5925-5935, 2017.
Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. Realistic
evaluation of semi-supervised learning algorithms. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track
Proceedings, 2018. URL https://openreview.net/forum?id=ByCZsFyPf.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31th International Con-
ference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 1278-1286,
2014. URL http://proceedings.mlr.press/v32/rezende14.html.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
2234-2242. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6125-improved-techniques-for-training-gans.pdf.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. In 4th International Conference on Learning Representations, ICLR 2016,
San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http:
//arxiv.org/abs/1511.06390.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con-
sistency targets improve semi-supervised deep learning results. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track
Proceedings, 2017. URL https://openreview.net/forum?id=ry8u21rtl.
PhiliPPe Thomas. Semi-supervised learning by olivier ChaPelle, bernhard scholkopf, and alexander
zien (review). IEEE Trans. Neural Networks, 20(3):542, 2009. doi: 10.1109/TNN.2009.2015974.
URL https://doi.org/10.1109/TNN.2009.2015974.
Vikas Verma, Alex Lamb, Christopher Beckham, Aaron C. Courville, Ioannis Mitliagkas, and
Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regu-
larizer. CoRR, abs/1806.05236, 2018. URL http://arxiv.org/abs/1806.05236.
Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation
consistency training for semi-supervised learning. In Proceedings of the Twenty-Eighth Inter-
national Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16,
2019, pp. 3635-3641, 2019. doi: 10.24963/ijcai.2019/504. URL https://doi.org/10.
24963/ijcai.2019/504.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved train-
ing of wasserstein gans: A consistency term and its dual effect. In 6th International Con-
ference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?
id=SJx9GQb0-.
10
Under review as a conference paper at ICLR 2020
Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised fea-
ture learning via non-parametric instance discrimination. In 2018 IEEE Confer-
ence on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City,
UT USA, June 18-22, 2018, pp. 3733-3742, 2018. doi: 10.1109/CVPR.2018.
00393. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_
Unsupervised_Feature_Learning_CVPR_2018_paper.html.
Qizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised
data augmentation. CoRR, abs/1904.12848, 2019. URL http://arxiv.org/abs/1904.
12848.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.
URL http://arxiv.org/abs/1605.07146.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri-
cal risk minimization. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL
https://openreview.net/forum?id=r1Ddp1-Rb.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational au-
toencoders. CoRR, abs/1706.02262, 2017. URL http://arxiv.org/abs/1706.02262.
A Appendix
A. 1 Basic inequality of ELBO
Proposition A.1. The Basic inequality of ELBO is
logP(X) ≥ Eqφ(z,c∣χ)[logPθ(X∣z,c)] - DκL(qφ(z∣X)kp(z)) - DκL(qφ(c∣X)kp(c))
proof
log P(X)=log Ljf q(≡⅛=log %(z,c∣χ) q(≡⅛
≥ Eqφ(ZaX) log qφX, z∣χ))
=/ qφ(z, c∣X) log P(Z, ：；、+ [ qφ(z, c∣X)logPθ(X∣z, c)
z,c	qφ(z, c∣X)	z,c
=-DKL(qφ(z, c∣X)kp(z, c)) + Eqφ(z,c∣X)[logPθ(X∣z, c)]
=WithaSSUmPtion(3,4) Eqφ(z,c∣X)[logPθ(X∣z, c)] - DκL(qφ(z∣X)kp(z)) - DκL(qφ(c∣X)kp(c))
□
A.2 Decomposition of ELBO in Info-VAE
Proposition A.2. The expected ELBO with empirical distribution satisfies
Epemp(X)DκL (qφ(z∣X)kP(z)) = Iqφ (X; z) +DκL(qφ(z)kP(z))
proof
Epemp(X)DKlSφ(ZIX)kp(Z)) = (Pemp(X)∕qφ(ZX) "^X) dzdX
=P Pemp(X) q qφ(z∣X) qφ(ZX) dzdX + P Pemp(X) / qφ(z∣X)^ZdzdX L
JX	Jz	qφ(Z)	JX	Jz	P(Z)
=Z Z qφ(z, X)log	qφ(z, X)Y、dzdX + DκL(qφ(z)kP(z))
X z	qφ(z)Pemp(X)
= Iqφ (X; z) + DκL(qφ(z)kP(z))
11
Under review as a conference paper at ICLR 2020
A.3 The equation form of ELBO
Proposition A.3. The expected margin between the true log-likelihood Epemp(X) log p(X) and
Epemp(X)ELBOis
Epemp(X) [log P(X)- ELBO ] = Epemp(X) [Dkl (qφ(z∣X)kp(z∣X)) + DκL(qφ(c∣X)kp(c∣X))]
proof
We just need to prove the following equation
logP(X)- ELBO=DκL(qφ(z∣X)kp(z∣X)) + DκL(qφ(c∣X)kp(c∣X))
and the proof under assumption (3, 4) is
log P(X) =
z
qφ(z∣X)qφ(c∣X)
log P(X)dzdc =
z,c
qφ(z∣X)qφ(c∣X) log P(X,z, ：，dzdc
P(z|X)P(c|X)
Eqφ(z∣X)qφ(c∣X) log	(z(X): CcIX) + Z qφ(zX)qφ(cX)lOg ^^^^)
qφ(ZIX)qφ(CIX)	Jzc	P(ZIX)P(CIX)
□
ELBO + DκL(qφ(z∣X)kP(z∣X)) + DκL(qφ(c∣X)∣∣P(c∣X))
A.4 Optimal Transport Scheme
Proposition A.4.
1.	The Shortest-Path derived from optimal transport scheme (17) between zι 〜 N (μι, diag(σ2))
and z2 〜N(μ2, diag(σ2)) with λ ∈ [0,1] is
μ = λμ1 + (1 - λ)μ2
σ = λσι + (1 — λ)σ2
(A.1)
proof
Utilizing the conclusions in Kuang & Tabak (2017), the closed form of optimal transport from one
multi-normal distribution N(zi； μι, ∑ι) to another normal distribution N(z2; μ2, ∑2) is
_ 1	1	1	1	_1
Z →T(Z) = μ2 + T(Z — μl); T = ςi 2 (ς; ς2ς 2 ) 2 ςi 2
Utilize the diag matrix assumption, the optimal transport scheme with λ is
zλ = (1 - λ)z1 + λT (z1 )
T = diag(σ1 /σ2)
(A.2)
Utilize (A.2), we can get (A.1) as
Zλ 〜N(μ,diag(σ2));	μ = λμι + (1 — λ)μ2;	σ = λσι + (1 — λ)σ2 □
2.	The shortest-path derived from KL divergence based optimal transport scheme between ci 〜
MUlt(K, ∏ι) and c2 〜MUlt(K, ∏2) with λ ∈ [0,1] is
∏ = λ∏ι + (1 — λ)∏2	(A.3)
proof
As the definition (17) has no closed-form solution for multinomial distribution, we use KL diver-
gence instead. The KL divergence based optimal transport target cλ 〜Mult(K, ∏) between two
multinomial distribution ci 〜Mult(K, ∏ι) and c2 〜Mult(K, ∏2) with λ ∈ [0,1] satisfy
K
min λDκL(∏ιk∏) + (1 - X)Dkl(∏2∣∣∏)	s.t. X ∏i = 1	(A.4)
π	i=i
The Lagrange multiplier form of (A.4) is
K
L(∏,t) = λDκL(∏ιk∏) + (1 — λ)DκL(∏2k∏) +1 * (X∏i — 1)
i=i
12
Under review as a conference paper at ICLR 2020
Table 6: Schedule Parameters
MNIST			SVHN(one-stage)		SVHN		Cifar10		Cifar100	
	hmax	tmax	hmax	tmax	hmax	tmax	hmax	tmax	hmax	tmax
βz	30	50	1	175	1e-3	150	1e-3	150	1e-1	150
βc	30	50	1	175	1	150	1e-3	150	1e-3	150
Iz	17.5	50	50	175	1280	150	200	150	1280	150
Ic	17	50	50	175	2.3	150	2.3	150	4.6	150
wMz	\	\	\	\	1e-3	150	1e-3	150	1e-1	150
wMc	\	\	\	\	1	400	1	280	1	280
and the related KKT conditions are
∂L(Π,t)	λ∏ι + (1 - λ)∏2
-TTT--- = t----------z------- = 0
∂π	π
K	(A.5)
t * (^X ∏i — 1)=0
i=1
Solve the equation (A.5), We can get the closed form of ∏ as
∏ = λ∏ι + (1 — λ)∏2 □
A.5 Schedule Analysis
The Warm-up scheduler aims to sloWly increase the parameters until they reach their maximum. For
a certain hyperparameter h, there are 2 parameters control its Warm-up process, the target value
hmax and the total epoch tmax to reach the target value. We use the exponential function to get the
middle value ht as
ht = hmax × exp [-5 * (1 - min(1,t/tmax))2]	(A.6)
The curve of (A.6) is shoWn in Figure 2, and We list the scheduler parameters of 4 benchmark
datasets, i.e. MNIST, SVHN, Cifar10, Cifar100, in Table 6.
13
Under review as a conference paper at ICLR 2020
Table 7: Details of Training Process
	MNIST	SVHN(one-stage)	SVHN	Cifar10	Cifar100
Latent Dim(Z/c)	10/10	32/10	128/10	128/10	128/100
Mutual Info(Z/c)	17.5/17.0	50/50	1280/2.3	200/2.3	1280/4.6
loss of - logpθ(X|c, Z)	BCE	BCE	MSE	MSE	BCE
α of pmixup (X)	\	\	2	2	2
optimizer		Adam		SGD	
learning rate	5e-4	1e-3		0.1	
lr scheduler(decay ratio)	\	\	every 50 epoch after 200-th(0.5)	[500,600,650](0.2)	
weight decay	0	0		5e-4	
A.6 Details of Training Process
Here we list some important items need to set in the training process for different process. We
classify these items into 2 categories: (1) items related to the loss function and (2) items related to
the optimization strategy. The details are as follows and we list the exact value in Table 7:
1.	Items related to the loss function
•	Latent dim
The latent dim of discrete variable c is the same as the number of classifications, that
is, K . For continuous variable z, the latent dim is determined by experiments.
•	Mutual information
We find the value of continuous mutual information will affect the generative per-
formance, but have little impact on semi-supervised learning results, so we choose a
suitable value to get the best generative performance. For discrete information, we
choose the value in the ideal scene, that is, Ic = log(K).
•	Calculation of Eqφ(z∣x),qφ(c∣x) - logPθ(X|c, Z)
Pθ(X|c, Z) has two forms: (1) normal distribution with N(fθ(c, z), I) and (2) multi-
nomial distribution with Mult(dim(X), fθ (c, z)). For the two forms, the loss func-
tion of - logpθ(X|c, Z) is mean square error(MSE) and binary cross entropy(BCE)
respectively. We use reparameterization trick in (20) to approximate expectation, and
the sampling frequency is 1.
•	α of pmixup (X)
The β(α, α) for mixup vicinal distribution will strongly affect SSL performance. We
set it to 2 in all experiments.
2.	Items related to the optimization strategy
•	Optimizer
In one-stage VAE, we use Adam. In OSPOT-VAE, we use SGD with momentum 0.9.
•	Learning rate
We set the initial learning rate to 0.1 in OSPOT-VAE, which obtains best SSL perfor-
mance.
•	The scheduler of adjusting learning rate
We decay the learning ratio in some milestones with the specified decay rate.
•	Weight decay
Weight decay controls the strength of L2 regularization.
We also list some standard training curves on benchmark datasets in Figure 3 and 4.
14
Under review as a conference paper at ICLR 2020
Figure 3: The training curves of Cifar10.
Left: classification performance. Right: Eq@(z|x),q@(c|x)- logpθ(X|c, Z)
Figure 4: The training curves of Cifar100.
Left: classification performance. Right: Eq@(z|x),q@(c|x)— logpθ(X|c, Z)
A.7 Generative Performance
Following figures5-7 show the generation performance of our OSPOT-VAE.
。/23Y6 67g 0
O /乙 3q5e>7<p夕

O 0
J 2
3 ?
q U
5 5
G S
-78 乡
7 8 7
0 O
]I
N 2
3 3
Y ti
Γ 5
S G
7 ɔ
? 2
9，
O ləs q 5
7
8
O O
∖ /
么Λ
3 3
34
S 6
6 Q
7孑
3 2
夕9
(a) MNIST
(b) SVHN
Figure 5: The generative performance of OSPOT-VAE in MNIST and SVHN
15
Under review as a conference paper at ICLR 2020
(b) OSPOT-VAE
Figure 6: Compare the generative performance of pure VAE and OSPOT-VAE in Cifar10
-EfcffrF5
EnI‰1□BVH 旨
(a) Pure VAE
(b) OSPOT-VAE
Figure 7: Compare the generative performance of pure VAE and OSPOT-VAE in Cifar100
(a) Pure VAE
16