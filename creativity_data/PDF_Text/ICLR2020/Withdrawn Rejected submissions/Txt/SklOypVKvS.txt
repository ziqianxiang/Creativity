Under review as a conference paper at ICLR 2020
Data-Efficient Mutual Information Neural
Estimator for Statistical Dependency Testing
Anonymous authors
Paper under double-blind review
Ab stract
Measuring Mutual Information (MI) between high-dimensional, continuous, ran-
dom variables from observed samples has wide theoretical and practical applica-
tions. Recent works have developed accurate MI estimators through provably low-
bias approximations and tight variational lower bounds assuming abundant supply
of samples, but require an unrealistic number of samples to guarantee statistical
significance of the estimation. In this work, we focus on improving data efficiency
and propose a Data-Efficient MINE Estimator (DEMINE) that can provide a tight
lower confident interval of MI under limited data, through adding cross-validation
to the MINE lower bound (Belghazi et al., 2018). Hyperparameter search is em-
ployed and a novel meta-learning approach with task augmentation is developed
to increase robustness to hyperparameters, reduce overfitting and improve accu-
racy. With improved data-efficiency, our DEMINE estimator enables statistical
testing of dependency at practical dataset sizes. We demonstrate the effectiveness
of DEMINE on synthetic benchmarks and real world fMRI data, with application
of inter-subject correlation analysis.
1	Introduction
Mutual Information (MI) is an important, theoretically grounded measure of similarity between ran-
dom variables. MI captures general, non-linear, statistical dependencies between random variables.
MI estimators that estimate MI from samples are important tools widely used in not only subjects
such as physics and neuroscience, but also machine learning ranging from feature selection and
representation learning to explaining decisions and analyzing generalization of neural networks.
Existing studies on MI estimation between general random variables focus on deriving asymptotic
lower bounds and approximations to MI under infinite data, and techniques for reducing estima-
tor bias such as bias correction, improved signal modeling with neural networks and tighter lower
bounds. Widely used approaches include the k-NN-based KSG estimator (Kraskov et al., 2004) and
the variational lower-bound-based Mutual Information Neural Estimator (MINE) family (Belghazi
et al., 2018; Poole et al., 2018).
Despite the empirical and asymptotic bias improvements, MI estimation has not seen wide adop-
tion. The challenges are two-fold. First, the analysis of dependencies among variables - let alone
any MI analyses for scientific studies - requires not only an MI estimate, but also confidence inter-
vals (Holmes & Nemenman, 2019) around the estimate to quantify uncertainty and statistical sig-
nificance. Existing MI estimators, however, do not provide confidence intervals. As low probability
events may still carry a significant amount of information, the MI estimates could vary greatly given
additional observations (Poole et al., 2018). Towards providing upper and lower bounds of true MI
under limited number of observations, existing MI lower bound techniques assume infinite data and
would need further relaxations when a limited number of observations are provided. Closest to our
work, Belghazi et al. (2018) studied the lower bound of the MINE estimator under limited data, but
it involves bounds on generalization error of the signal model and would not yield useful confidence
intervals for realistic datasets. Second, practical MI estimators should be insensitive to the choice
of hyperparameters. An estimator should return a single MI estimate with its confidence interval
irrespective of the type of the data and the number of observations. For learning-based approaches,
this means that the model design and optimization hyperparameters need to not only be determined
automatically but also taken into account when computing the confidence interval.
1
Under review as a conference paper at ICLR 2020
Towards addressing these challenges, our estimator, DEMINE, introduces a predictive MI lower
bound for limited samples that enables statistical dependency testing under practical dataset sizes.
Our estimator builds on top of the MINE estimator family, but performs cross-validation to remove
the need to bound generalization error. This yields a much tighter lower bound agnostic to hyper-
parameter search. We automatically selected hyperparameters through hyperparameter search, and
a new cross-validation meta-learning approach is developed, based upon few-shot meta-learning,
to automatically decide initialization of model parameters. Meta-overfitting is strongly controlled
through task augmentation, a new task generation approach for meta-learning. With these improve-
ments, we show that DEMINE enables practical statistical testing of dependency for not only syn-
thetic datasets but also for real world functional Magnetic Resonance Imaging (fMRI) data analysis
capturing nonlinear and higher-order brain-to-brain coupling.
Our contributions are summarized as follows: 1) A data-efficient Mutual Information Neural Esti-
mator (DEMINE) for statistical dependency testing; 2) A new formulation of meta-learning using
Task Augmentation (Meta-DEMINE); 3) Application to real life, data-scarce applications (fMRI).
2	Related Work
2.1	MI Estimation
A widely used approach for estimating MI from samples is using k-NN estimates, notably the KSG
estimator (Kraskov et al., 2004). Gao et al. (2017) provided a comprehensive review and studied
the consistency and of asymptotic confidence bound of the KSG estimator (Gao et al., 2018). MI
estimation can also be achieved by estimating individual entropy terms through kernel density es-
timation (Ahmad & Lin, 1976) or cross-entropy (McAllester & Statos, 2018). Despite their good
performance on random variables with few dimensions, MI estimation on high-dimensional random
variables remains challenging for commonly used Gaussian kernels. Fundamentally, estimating
MI requires accurately modeling the random variables, where high-capacity neural networks have
shown excellent performance on complex high-dimensional signals such as text, image and audio.
Recent works on MI estimation have focused on developing tight asymptotic variational MI lower
bounds where neural networks are used for signal modeling. The IM algorithm (Agakov, 2004)
introduces a variational MI lower bound, where a neural network q(z|x) is learned as a variational
approximation to the conditional distribution P(Z|X). The IM algorithm requires the entropy,
H(Z), and EXZ log q(z|x) to be tractable, which applies to latent codes of Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs) as well as categorical variables. Belghazi
et al. (2018) introduces MI lower bounds MINE and MINE-f which allow the modeling of general
random variables and shows improved accuracy for high-dimensional random variables, with appli-
cation to improving generative models. Poole et al. (2018) introduces a spectrum of energy-based
MI estimators based on MINE and MINE-f lower bounds and a new TCPC estimator inspired by
Contrastive Predictive Coding (Oord et al., 2018) for the case when multiple samples from P (Z|X)
can be drawn.
Our work introduces cross-validation to the MINE-f estimator. We derive the lower bound of MINE-f
under limited number of samples, and introduce meta-learning and hyperparameter search to enable
practical statistical dependency testing.
2.2	General Statistical Dependency Testing
Existing works in general statistical dependency testing (Bach & Jordan, 2002; Gretton et al., 2005a;
Berrett & Samworth, 2019) have developed non-parametric independent criterions based on correla-
tion and mutual information estimators equivalent to testing I(X; Z) = 0, followed by detailed bias
and variance analyses. Our approach for independent testing suggest a different direction by har-
nessing the generalization power of neural networks and may improve test performance on complex
signals. The p-values provided by our test do not involve approximated distributions and hold for
small number of examples and arbitrary number of signal dimensions. As different statistical depen-
dency testing approaches have explicit or implicit assumptions and biases that make them suitable in
different situations, a fair comparison across different approaches is a challenging task. Instead, we
focus on a self-contained presentation of our dependency test, and provide preliminary comparisons
with a widely studied Hilbert-Schmidt independence criterion (HSIC) (Gretton et al., 2005a) in the
appendix.
2
Under review as a conference paper at ICLR 2020
2.3	Meta Learning
Meta-learning, or “learning to learn”, seeks to improve the generalization capability of neural net-
works by searching for better hyperparameters (Maclaurin et al., 2015), network architectures (Pham
et al., 2018), initialization (Finn et al., 2017a; 2018; Kim et al., 2018) and distance metrics (Vinyals
et al., 2016; Snell et al., 2017). Meta-learning approaches have shown significant performance im-
provements in applications such as automatic neural architecture search (Pham et al., 2018), few-shot
image recognition (Finn et al., 2017a) and imitation learning (Finn et al., 2017b).
In particular, our estimator benefits from the Model-Agnostic Meta-Learning (MAML) (Finn et al.,
2017a) framework which is designed to improve few-shot learning performance. A network initial-
ization is learned to maximize its performance when fine-tuned on few-shot learning tasks. Appli-
cations include few-shot image classification and navigation.
We leverage the model-agnostic nature of MAML for MI estimation between generic random vari-
able and adopt MAML for maximizing MI lower bounds. To construct a collection of diverse tasks
for MAML learning from limited samples, inspired by MI’s invariance to invertible transformations,
we propose a task-augmentation protocol to automatically construct tasks by sampling random trans-
formations to transform the samples. Results show reduced overfitting and improved generalization.
3	Background
In this section, we will provide the background necessary to understand our approach1. We define
X and Z to be two random variables, P (X, Z) is the joint distribution, and P (X) and P (Z) are
the marginal distributions over X and Z respectively. Our goal is to estimate MI, I(X; Z) given
independent and identically distributed (i.i.d.) sample pairs (xi, zi), i = 1, 2 . . . n from P(X, Z).
Let F = {Tθ(x, z)}θ∈Θ be a class of scalar functions, where θ is the set of model parameters. Let
q(x|z) = p(χ)E 理晨以；Te(χ,z). Results from previous works (Belghazi et al., 2018; Poole et al.,
2018) show that th,e folXloZwing energy-based family of lower bounds ofMI hold for any θ:
I(X; Z) ≥ E(x,z)〜PXZ log qPχXz) = E(x,z)〜PXZ Tθ (X, Z)- Ex〜PX log Ez〜PZ eTθ (x,z) , IEB1
≥ E(x,z)〜PXZ Tθ(x, Z) — log Ex〜PX ,z〜PZ eTθ (x,z) , IMINE	(I)
≥ E(x,z)〜PXZTθ(x, Z) — Ex〜PX,z〜PZeTθ(x,z) + 1，IMINE-f,Ieb
where, E is the expectation over the given distribution. Based on IMINE, the MINE estimator
V
I(X, Z)n is defined as in Eq.2. Estimators for IEB1, IMINE-f and IEB can be defined similarly.
n	nn
I(XTZyn = SUp n XTθ(xi,Zi) 一 log ~2 XXeTθ(xi,zj)
θ∈Θ n	n
i=1	i=1 j=1
(2)
V
With infinite samples to approximate expectation, Eq.2 converges to the lower bound I(X, Z)∞ =
supθ∈Θ IMINE. Note that the number of samples n needs to be substantially more than the number
of model parameters d = ∣θ∣ to guarantee that Tθ(X, Y) does not overfit to the samples (xi, z/,
i = 1, 2 . . . n and overestimate MI. Formally, the sample complexity of MINE is defined as the
minimum number of samples n in order to achieve Eq.3,
VV
Pr(II(X,Z)n - I(X,Z)∞∣≤ e) ≥ 1 — δ.
(3)
Specifically, MINE proves that under the following assumptions: 1) Tθ(X, Z) is L-Lipschitz; 2)
Tθ (X, Z) ∈ [-M, M], 3) {θi ∈ [-K, K],	∀i ∈ 1, . . . , d}, the sample complexity of MINE is
given by Eq.4.
2M 2(d log(16KL√d∕e) + 2dM + log(2∕δ))
n≥
(4)
For example, a neural network with dimension d = 10, 000, M = 1, K = 0.1 and L = 1, achieving
a confidence interval of = 0.1 with 95% confidence (δ = 0.05) would require n ≥ 18, 756, 256
samples. This is achievable for synthetic example generated by GANs like that studied in Belghazi
et al. (2018). For real data, however, the cost of data acquisition for reaching statistically significant
1We follow the same notations in Belghazi et al. (2018). We encourage the review of Belghazi et al. (2018);
Poole et al. (2018) for a detailed understanding of IMINE , IEB1 , and IEB.
3
Under review as a conference paper at ICLR 2020
estimation can be prohibitively expensive. Our approach instead uses the MI lower bounds specified
in Eq.1 from a prediction perspective, inspired by cross-validation. Our estimator, DEMINE, im-
proves sample complexity by disentangling data for lower bound estimation from data for learning
a generalizable Tθ(X, Z). DEMINE enables high-confidence MI estimation on small datasets.
4 Approach
Section 4.1 specifies DEMINE for predictive MI estimation and derives the confidence interval;
Section 4.2 formulates Meta-DEMINE, explains task augmentation, and defines the optimization
algorithms.
4.1 Predictive Mutual Information Estimation
In DEMINE, we interpret the estimation of MINE-f lower bound2 Eq.1 as a learning problem. The
goal is given a limited number of samples, infer the optimal network ‰ (X, Z) with parameters θ*
defined as follows:
θ* = arg max EPXZTθ (X, Z) — EPXEPZ eTθ(X,Z) + 1.
θ∈Θ
Specifically, samples from P (X, Z) are subdivided into a training set {(xi, zi)train, i = 1, . . . , m}
and a validation set {(xi, zi)val, i = 1, . . . , n}. The training set is used for learning a network θ as
an approximation to θ* whereas the validation set is used for computing the DEMINE estimation
V
I(X, Z)n ° defined as in Eq.5.
n	nn
VVZ)n,θ = - XTθ(Xi,zi )∙val - n^XX eTMzj )val +-	(5)
i=1	i=1 j=1
We propose an approach to learn θ, DEMINE. DEMINE learns θ by maximizing the MI lower
bound on the training set as follows:
θ = arg min L({(x, z)}train, θ), where,
θ∈Θ
-	|B|	-	|B| |B|
L({(x,z)}B,θ) = -|B| XTe(xi,Zi)B + |B|2 XXeτθ(xi,zj)B - 1.	(6)
|B| i=1	|B| i=1 j=1
The DEMINE algorithm is shown in Algorithm 2 in appendix.
Sample complexity analysis. Because θ is learned independently of validation samples
{(xi, zi)val, i = 1, . . . , n}, the sample complexity of the DEMINE estimator does not involve the
model class F and the sample complexity is greatly reduced compared to MINE-f. DEMINE esti-
V
mates I(X, Z)∞,θ° when infinite number of samples are provided, defined as:
VVZ)∞,3	= EPXZTe(X,Z) - EPXEPZeT*,Z) +1
≤ supe∈Θ EPXZ Te(X, Z) - EPX EPZ eTθ(X,Z) + 1 ≤I (X; Z)
(7)
We now derive the sample complexity of DEMINE defined as the number of samples n required for
VV
I(X, Z)n,θ° to be a good approximation to I(X, Z)∞,θ° in Theorem 1.
Theorem 1. For Tθ°(X, Z) bounded by [L, U], given any accuracy and confidence δ, we have:
-- -_	__ --
Pr(II(X,Z)n,θ — 1 (x,z)∞,°∣ ≤ e) ≥ 1 — δ
when the number of validation samples n satisfies:
2ξ2n*	- (e-ξ)2n*
n ≥ n*, s.t. f(n*) ≡ m^n 2e (U-L)2 + 4e 2(eU -eL)2 = δ
(8)
2MINE lower bound can also be interpreted in the predictive way, but will result in a higher sample com-
plexity than MINE-f lower bound. We choose MINE-f in favor of a lower sample complexity over bound
tightness.
4
Under review as a conference paper at ICLR 2020
Proof. Since T⅛(X, Z) is bounded by [L, U], applying the Hoeffding inequality to the first half of
Eq.5 yields:
1 匚	-2ξ2n
Pr(∣— ETG(XMzi)- EPXZTg(X,Z)| ≥ ξ) ≤ 2e (U-L2
n i=1
As eTθ (X,Z) is bounded by [eL, eU], applying the Hoeffding inequality twice to the second half of
Eq.5:
Pr(IEPXEPZeTθ(X,Z) - 1 Pn=ι EPZeT^(xi,z)∣ ≥ Z)
Pr(∣EPz 1 Pi=ι eTθ(xi,z) — 1 Pn=I 1 Pi=ι 4卜"叼)I ≥ Z)
Combining the above bounds results in:
VV	2ξ2n
Pr(∣I(X, Z)m - I(X, Z)∞,g∣ ≤ ξ + 2Z) ≥ 1 - 2e (U-L)2
-	2Z2n
≤	2e- (eU-eL)2
-	2Z2n
≤	2e- (eU-eL)2
2Z2n
-4e	(eU -eL)2
By solving ξ to minimize n according to Eq.8 we have:
VV
Pr(∣I(X,Z)n,G — I(X，Z)∞,G∣ ≤ E) ≥ 1 — δ∙ ■
Theorem 1 also implies the following MI lower confidence interval under limited number of samples
V
Pr(I(X; Z) ≥ I(X, Z)n,θG - E) ≥ 1-δ
Compared to MINE, as per the example shown in Section 3, for M = 1 (i.e. L = -1 and U =
1), δ = 0∙05, E = 0∙1, our estimator requires n = 10, 742 compared to MINE requiring n =
18, 756, 256 i.i.d validation samples to estimate a lower bound, which makes MI-based dependency
analysis feasible for domains where data collection is prohibitively expensive, e.g. fMRI scans. In
practice, sample complexity can be further optimized by optimizing hyperparameters U and L.
V
Note that unlike Eq.3, Theorem 1 bounds the closeness of the DEMINE estimate, I(X, Z)n θG, not to-
V,
wards the MI lower bound supθ∈Θ IMINE-f, but towards the MI lower bound I(X, Z)∞,θG. Therefore,
the sample complexity of DEMINE as in Eq.8 makes fair comparison with the sample complexity of
MINE as in Eq.4. MINE’s higher sample complexity stems from the need to bound the generaliza-
tion error of Tθ (X, Z) on unseen {(x, z)}. Existing generalization bounds are known to be overly
loose, as over-parameterized neural networks have been shown to generalize well in classification
and regression tasks (Zhang et al., 2016). By using a learning-based formulation, DEMINE not only
avoids the need to bound generalization error, but also allows further generalization improvements
by learning θ through meta-learning.
τ . < /'ll	. ∙	.	. 1	∙	1'	1 . ∙	-Λ r . iʌ ι ■ » « τ τ ι - .1 . 1	P⅛ r∙
In the following section, we present a meta-learning formulation, Meta-DEMINE, that learns θ for
generalization given the same model class and training samples.
4.2 Meta-Learning
Given training data {(xi, zi)train, i = 1, ∙ ∙ ∙ m}, Meta-DEMINE first generates MI estimation tasks
each consisting of a meta-training split A and a meta-val split B through a novel task augmenta-
tion process. And then a parameter initialization θinit is then learned to maximize MI estimation
performance on the generated tasks using initialization θinit as shown in Eq.9.
θinit = arg minE(A,B)∈tL((x, z)b, θ(t)), with ,θ(t) ≡ MetaTrain((x, z)a, θ(0)).	(9)
θ(0) ∈Θ
Here θ(t) = MetaTrain (x, z)A, θ(0) is the meta-training process of starting from an initialization
θ(0) and applying Stochastic Gradient Descent (SGD) 3 over t steps to learn θ where in every meta
training iteration we have:
θ(t) J θ(t-1) - YVL((X, z)a, θ(t-1))∙
3In practice, the Adam optimizer (Kingma & Ba, 2014) is used for faster optimization. The Adam optimizer
uses first and second order momentums of the gradient to speed up optimization. Illustrating SGD for simplicity.
5
Under review as a conference paper at ICLR 2020
Finally, θ is learned using the entire training set {(xi, zi)train, i = 1, . . . , m} with θinit as initializa-
tion:
G = MetaTrain((x, z)train, θinit) ∙
Task Augmentation: Meta-DEMINE adapts MAML (Finn et al., 2017a) for MI lower bound max-
imization. MAML has been shown to improve generalization performance in N -class K-shot im-
age classification. MI estimation, however, does not come with predefined classes and tasks. A
naive approach to produce tasks would be through cross-validation - partitioning training data into
meta-training and meta-validation splits. However, merely using cross-validation tasks is prone to
overfitting -a θinit, which memorizes all training samples would as a result have memorized all meta-
validation splits. Instead, Meta-DEMINE generates tasks by augmenting the cross-validation tasks
through task augmentation. Training samples are first split into meta-training and meta-validation
splits, and then transformed using the same random invertible transformation to increase task diver-
sity. Meta-DEMINE generates invertible transformation by sequentially composing the following
functions:
Mirror :	m(x)	(2n - 1)x,	n〜 Bernoulli( 2),
Permute :	P(x) =	n nPd,	Permute dimensions.
Offset :	O(x) =	x + ,	e 〜U(-0.1, 0.1),
Gamma :	G(x) =	二 sign(x) ∣x∣γ ,	Y 〜U(0∙5, 2),
Since the MI between two random variables is invariant to invertible transformations on each vari-
able, MetaTrain(∙, ∙) is expected to arrive at the same MI lower bound estimation regardless of the
transformation applied. At the same time, memorization is greatly suppressed, as the same pair
(x, Z) can have different log Ppxxpzz) under different transformations. More sophisticated invertible
transformations (affine, piece-wise linear) can also be added. Task augmentation is an orthogonal
approach to data augmentation. Using image classification as an example, data augmentation gener-
ates variations of the image, translated, or rotated images assuming that they are valid examples of
the class. Task augmentation on the other hand, does not make such an assumption. Task augmenta-
tion requires the initial parameters θinit to be capable of recognizing the same class in a world where
all images are translated and/or rotated, with the assumption that the optimal initialization should
easily adapt to both the upright world and the translated and/or rotated world.
Optimization: Solving θinit using the meta-learning formulation Eq.9 poses a challenging optimiza-
tion problem. The commonly used approach is back propagation through time (BPTT) which com-
putes second order gradients and directly back propagates gradients from MetaTrain((x, z)A, θ(0))
to θinit. BPTT is very effective for a small number of optimization steps, but is vulnerable to explod-
ing gradients and is memory intensive. In addition to BPTT, we find that stochastic finite difference
algorithms such as Evolution Strategies (ES) (Salimans et al., 2017) and Parameter-Exploring Pol-
icy Gradients (PEPG) (Sehnke et al., 2010) can sometimes improve optimization robustness. In
practice, we switch betwen BPTT and PEPG depending on the number of meta-training iterations.
Meta-DEMINE algorithm is specified in Algorithm 1.
5 Evaluation on Synthetic Datasets
Dataset. We evaluate our approaches DEMINE and Meta-DEMINE against baselines and state-of-
the-art approaches on 3 synthetic datasets: 1D Gaussian, 20D Gaussian and sine wave. For 1D and
20D Gaussian datasets, following Belghazi et al. (2018), we define two k-dimensional multivariate
Gaussian random variables X and Z which have component-wise correlation corr(Xi, Zj ) = δij ρ,
where ρ ∈ (-1, 1) and δij is Kronecker’s delta. Mutual information I(X; Z) has a closed form
solution I(X; Z) = -k ln(1 - ρ2). For the sine wave dataset, we define two random variables X
and Z, where X 〜 U(-1,1), Z = sin(aX + ∏) + 0.05g and E 〜 N(0,1). Estimating mutual
information accurately given few pairs of (X, Z) requires the ability to extrapolate the sine wave
given few examples. Ground truth MI for sine wave dataset is approximated by running the the
KSG Estimator (Kraskov et al., 2004) on 1, 000, 000 samples.
Implementation. We compare our estimators, DEMINE and Meta-DEMINE, against the KSG
estimator (Kraskov et al., 2004) MI-KSG and MINE-f (Belghazi et al., 2018). For both DEMINE
and Meta-DEMINE, we study variance reduction mode, referred to as -vr, where hyperparameters
are selected by optimizing 95% confident estimation mean (μ - 2σ*) and statistical significance
mode, referred to as -sig, where hyperparameters are selected by optimizing 95% confident MI
6
Under review as a conference paper at ICLR 2020
Algorithm 1 Meta-DEMINE
Input Data: {(x, z)train, (x, z)val}
Parameters: batch B, Meta Learning Iterations NM , Task Augmentation Iterations NT , Opti-
mization Iterations NO, Ratio r, Learning rate η, Meta Learning Rate ηmeta
Output: MI, Tθinit (X, Z), Tθ (X, Z)
1:	for i = 1 : NM do
2:	for j = 1 : NT do
3:	A = r × train, B = train - A
4:	Split (x, z)train into (x, z)A and (x, z)B
5:	Transformation Rx for x, Rχ(∙) = m(P(O(G(∙))))
6:	Transformation Rzfor z, Rz(∙) = m(P(O(G(∙))))
7:	θmla J θinit
8:	for k = 1 : NO do
9:	Sample a batch of (x, Z)B 〜(x, Z)A
10:	Compute L((Rx(x),Rz(z))b,θmketa)
11:	Compute P0加 L - gradient for θmeta
θmeta
12:	Update θmeta using Adam Kingma & Ba (2014) with η
13:	end for
14:	Compute Lmeta((Rx(x), Rz(Z))B, θmNNO))
15:	Compute Vθo Lmeta - gradient to 仇口让 using BPTT
16:	end for
17:	Update θinit using Adam Kingma & Ba (2014) with ηmeta
18:	end for
19:	θ(0) J θinit
20:	for i = 1 : NO do
21:	Sample a batch of (x, Z)B 〜(x, ζ)trai∏
22:	Compute L((x, z)b, θ⑴)
23:	Compute gradient VθL
24:	Update θ using Adam with η
25:	end for
26:	Compute MI = L((x, z)vaι, θ(NO))
27:	return MI, θinit, θ(NO)
lower bound (μ — e). Samples (x, z) are split 50%-50% into (x, z)train and (x, z)vaɪ. We use a
separable network architecture Tθ(x, Z) = M tanh(w cos f(x), g(Z) + b) - t . f andg are MLP
encoders that embed signals x and Z into vector embeddings. Hyperparameters t ∈ [-1, 1] and
M control upper and lower bounds Tθ(x, Z) ∈ [-M (1 + t), M(1 - t)]. Parameters w and b are
learnable parameters. MLP design and optimization hyperparameters are selected using Bayesian
hyperparameter optimization (Bergstra et al., 2013) described below.
Hyperparameter search on DEMINE-vr and DEMINE-sig was conducted using the hyperopt pack-
age 4. Seven hyperparameters were involved in hyperparameter search: 1) number of encoder layers
[1, 5], 2) encoder hidden size [8, 256], 3) learning rate η [10-4, 3 × 10-1] in log scale, 4) number of
optimization iterations NO [5, 200] (sine wave [5, 5000]) in log scale, 5) batch size B [256, 1024],
6) M, [10-3, 5] in log scale, 7) t, [-1,1]. Mean μ and sample standard deviation σ of MI estiamte
computed over 3-fold cross-validation on (x, z)train. DEMINE-vr maximizes two sigma low μ 一 2σ*
where σ* =%σ due to 3-fold cross-validation. DEMINE-sig maximizes statistical significance
μ - E where E is two-sided 95% confidence interval of ML Meta-DEMINE-vr and Meta-DEMINE-
sig subsequently reuse these hyperparameters as DEMINE-vr and DEMINE-sig.
Meta-learning hyperparameters are chosen as outer loop NM = 3, 000 iterations, task augmentation
NT = 1 iterations, r = 0.8, ηmeta = 3, with task augmentation mode m(P(O(∙))). NO was capped
at 30 iterations for 1D and 20D Gaussian datasets due to memory limit. For the sine wave datasets
with large NO, we used PEPG (Sehnke et al., 2010) rather than BPTT.
4Hyperopt package: https://github.com/hyperopt/hyperopt.
7
Under review as a conference paper at ICLR 2020
For MI-KSG, we use off-the-shelf implementation by Gao et al. (2017) with default number of
nearest neighbors k = 3. MI-KSG does not provide any confidence interval. For MINE-f, we use the
same network architecture same as DEMINE-vr. we implement both the original formulation which
optimizes Tθ on (x, z) till convergence (10k iters), as well as our own implementation MINE-f-ES
with early stopping, where optimization is stopped after the same number of iterations as DEMINE-
vr to control overfitting.
Results. Figure 1(a) shows MI estimation performance on 20D Gaussian datasets with varying
ρ ∈ {0, 0.1, 0.2, 0.3, 0.4, 0.5} using N = 300 samples. Results are averaged over 5 runs to compare
estimator bias, variance and confidence. Note that Meta-DEMINE-sig detects the highest p < 0.05
confidence MI, outperforming DEMINE-sig which is a close second. Both detect p < 0.05 statisti-
cally significant dependency starting ρ = 0.3, whereas estimations of all other approaches are low
confidence. It shows that in contrary to common belief, estimating the variational lower bounds with
high confidence can be challenging under limited data. MINE-f estimates MI > 3.0 and MINE-f-ES
estimates positive MI when ρ = 0, both due to overfitting, despite MINE-f-ES having the lowest
empirical bias. DEMINE variants have relatively high empirical bias but low variance due to tight
upper and lower bound control, which provides a different angle to understand bias-variance trade
off in MI estimation (Poole et al., 2018).
Figure 1(b,c,d) shows MI estimation performance on 1D, 20D Gaussian and sine wave datasets with
fixed ρ = 0.8, 0.3 and a = 8π respectively, with varying N ∈ {30, 100, 300, 1000, 3000} number of
samples. More samples asymptotically improves empirical bias across all estimators. As opposed to
1D Gaussian datasets which are well solved by N = 300 samples, higher-dimensional 20D Gaussian
and higher-complexity sine wave datasets are much more challenging and are not solved using N =
3000 samples with a signal-agnostic MLP architecture. DEMINE-sig and Meta-DEMINE-sig detect
p < 0.05 statistically significant dependency on not only 1D and 20D Gaussian datasets where x
and z have non-zero correlation, but also on the sine wave datasets where correlation between x and
z is 0. This means that DEMINE-sig and Meta-DEMINE-sig can be used for nonlinear dependency
testing to complement linear correlation testing.
We study the effect of cross-validation meta-learning and task augmentation on 20D Gaussian with
ρ = 0.3 and N = 300. Figure 2 plots performance of Meta-DEMINE-vr over NM = 3000
meta iterations under combinations of task augmentations modes and number of adaptation iter-
ations NO ∈ {0,20}. Overall, task augmentation modes which involve axis flipping m(∙) and
permutation P(∙) are the most successful. With NO=20 steps of adaptation, task augmentation
modes P(∙), m(P(∙)) and m(P(O(∙))) prevent ovefitting and improves performance. The perfor-
mance improvements of task augmentation is not simply from change in batch size, learning rate
or number of optimization iterations, because meta-learning without task augmentation for both
NO = 0 and 20 could not outperform baseline. Meta-learning without task augmentation and with
task augmentation but using only O(∙) or G(∙) result in overfitting. Task augmentation with m(∙) or
m(P(O(G(∙)))) prevent overfitting, but do not provide performance benefits, possibly because their
complexity is insufficient or excessive for 20 adaptation steps. Further more, task augmentation with
no adaptation (NO = 0) falls back to data augmentation, where samples from transformed distribu-
tions are directly used to learn Tθ(x, z). Data augmentation with O(∙) outperforms no augmentation,
but is unable to outperform baseline and suffers from overfitting. It shows that task augmentation
provides improvements orthogonal to data augmentation.
6	Application: fMRI Inter-subject correlation (ISC) analysis
Humans use language to effectively transmit brain representations among conspecifics. For exam-
ple, after witnessing an event in the world, a speaker may use verbal communication to evoke neural
representations reflecting that event in a listener’s brain (Hasson et al., 2012). The efficacy of this
transmission, in terms of listener comprehension, is predicted by speaker-listener neural synchrony
and synchrony among listeners (Stephens et al., 2010). To date, most work has measured brain-
to-brain synchrony by locating statistically significant inter-subject correlation (ISC); quantified as
the Pearson product-moment correlation coefficient between response time series for corresponding
voxels or regions of interest (ROIs) across individuals (Hasson et al., 2004; Schippers et al., 2010;
Silbert et al., 2014; Nastase et al., 2019). Using DEMINE and Meta-DEMINE for statistical de-
pendency testing, we can extend ISC analysis to capture nonlinear and higher-order interactions in
continuous fMRI responses. Specifically, given synchronized fMRI response frames in two brain
8
Under review as a conference paper at ICLR 2020
(b) 1D Gaussian dataset, ρ = 0.8
Sine wave a=0.8, N={30.100,300.1000,3000}
(a) 20D Gaussian dataset, N = 300 samples
2OD Gaussian p=0.3, N = {30,100,300,1000,3000}
N=30	N=100 N=300 N=IOOO N=3000
(c) 20D Gaussian dataset, ρ = 0.3
(d) Sine wave dataset, a = 8π
O 500 IOOO 1500 2000 2500 3000
Outer loop iterations NT
Figure 1: Comparing MI Estimation performance of DEMINE and Meta-DEMINE with the KSG
estimator Kraskov et al. (2004) and MINE-f Belghazi et al. (2018) on different datasets using varying
number of samples. The bars show estimator mean and standard deviation averaged over 5 runs
with different seeds. The error bars show 95% confidence interval (not available for MI-KSG). The
statistical significance focused variants DEMINE-sig and Meta-DEMINE-sig achieves the highest
95% confident MI estimation. Meta-DEMINE improves over DEMINE most of the time.
Effect of Data Augmentation	Effect of Task Augmentation	Effect of Task Augmentation
(Inner loop iterations NO=O)	(Inner loop iterations JVO=Io)	(Inner loop iterations No=20)
0	500 1000 1500 2000 2500 3000
Outer loop iterations NT
0	500 1000 1500 2000 2500 3000
Outer loop iterations NT
(a)	Meta-DEMINE-vr NO = 0.
(b)	Meta-DEMINE-vr NO = 10.
(c)	Meta-DEMINE-vr NO = 20.
Figure 2: To study the effect of task augmentation and number of adaptation steps, we run Meta-
DEMINE-vr with different task augmentation modes and vary number of adaptation iterations
NO ∈ {0, 10, 20} on Gaussian 20D, ρ = 0.3 dataset. Combinations of permutation and mirror-
ing operations are effective in reducing overfitting and improving performance.
Table 1: Number of HCP-MMP1
regions with significant correla-
tion (r) and MI (DEMINE, Meta-
DEMINE) during listening.
Table 2: Segment classification accuracy for NeuralMI versus
Pearson’s correlation in 1-vs-rest*.
Classification	ISC Mask	dDMN Mask
Accuracy (%)	P F Br BkIMIlP F Br BklMI
No. shared	r	DEMINE -sig	Meta -DEMINE -sig	Chance Pearson’s r 1vR DEMINE-vr 1vR	3.7 35.0 42.8	1.8 20.4 28.0	2.6 25.8 32.8	1.9 31.5 35.9	N/A N/A 0.637	3.7 14.8 16.5	1.8 6.4 7.9	2.6 11.8 11.6	1.9 9.9 12.0	N/A N/A 0.035
r	37	24	23	Meta-DEMINE-vr 1vR	47.2	32.5	39.9	41.0	0.752	13.7	7.9	8.2	8.9	0.031
DEMINE-Sig 24	28
Meta-DEMINE-Sig 23	26
26 Abbreviations: P: Pieman; F: Forgot; Br: Bronx; Bk: Black, MI: Mutual Information.
29	*Note that all the results are averaging over the subjects.
9
Under review as a conference paper at ICLR 2020
regions X and Z across K subjects Xi, Zi, i = 1, . . . , K as random variables. We model the condi-
tional mutual information I(Xi; Zj|i 6= j) as the MI form of pair-wise ISC analysis. By definition,
I(Xi; Zj |i 6= j) first computes MI between activations Xi and Zj from subjects i andj respectively,
and then average across pairs of subjects i 6= j . It can be lower bounded using Eq. 7 by learning a
Tθ (x, z) shared across all subject pairs.
Dataset. We study MI-based and correlation-based ISC on a fMRI story comprehension dataset
by Nastase et al. (2019) with 40 participants listening to four spoken stories. Average story duration
is 11 minutes. An fMRI frame with full brain coverage is captured at repetition time 1 TR =1.5 sec-
onds with 2.5mm isotropic spatial resolution. We restricted our analysis to subsets of voxels defined
using independent data from previous studies: functionally-defined masks of high ISC voxels (ISC;
3,800 voxels) and dorsal Default-Mode Network voxels (dDMN; 3,940 voxels) from Simony et al.
(2016) as well as 180 HCP-MMP1 multimodal cortex parcels from Glasser et al. (2016). All masks
were defined in MNI space.
Implementation. We compare MI-based ISC using DEMINE and Meta-DEMINE with correlation-
based ISC using Pearson’s correlation. DEMINE and Meta-DEMINE setup follows Section 5. The
fMRI data were partitioned by subject into a train set of 20 subjects and a validation set of 20
different subjects. Residual 1D CNN is used instead of MLP as the encoder for studying temporal
dependency. For Pearson’s correlation, high-dimensional signals are reshaped to 1D for correlation
analysis. Effective sample size for confidence interval calculation is the number of unique non-
overlapping fMRI samples.
Results. We first examine, for the fine grained HCM-MMP1 brain regions, which have p < 0.05
statistically significant MI and Pearson’s correlation. Table 1 shows the result. Overall, more re-
gions have statistically significant correlation than dependency. This is expected because correlation
requires less data to detect. But Meta-DEMINE is able to find 6 brain regions that have statistically
significant dependency but lacks significant correlation. This shows that MI analysis can be used to
complement correlation-based ISC analysis.
By considering temporal ISC over time, fMRI signals can be modeled with improved accuracy. In
Table 2 we apply DEMINE and Meta-DEMINE with L = 10TRs (15s) sliding windows as random
variables to study amount of information that can be extracted from ISC and dDMN masks. We use
between-subject time-segment classification (BSC) for evaluation (Haxby et al., 2011; Guntupalli
et al., 2016). Each fMRI scan is divided into K non-overlapping L = 10 TRs time segments. The
BSC task is one versus rest retrieval: retrieve the corresponding time segment z of an individual
given a group of time segments x excluding that individual, measured by top-1 accuracy. For re-
trieval score, Tθ (X, Z) is used for DEMINE and Meta-DEMINE and ρ(X, Z) is used for Pearson’s
correlation as a simple baseline. With CNN as encoder, DEMINE and Meta-DEMINE model the
signal better and achieve higher accuracy. Also. Meta-DEMINE is able to extract 0.75 nats of MI
from the ISC mask over 10 TRs or 15s, which could potentially be improved by more samples.
7	Conclusion
We illustrated that a predictive view of the MI lower bounds coupled with meta-learning results in
data-efficient variational MI estimators, DEMINE and Meta-DEMINE, that are capable of perform-
ing statistical test of dependency. We also showed that our proposed task augmentation reduces over-
fitting and improves generalization in meta-learning. We successfully applied MI estimation to real
world, data-scarce, fMRI datasets. Our results suggest a greater avenue of using neural networks and
meta-learning to improve MI analysis and applying neural network-based information theory tools
to enhance the analysis of information processing in the brain. Model-agnostic, high-confidence, MI
lower bound estimation approaches - including MINE, DEMINE and Meta-DEMINE- are limited
to estimating small MI lower bounds up to O(log N) as pointed out in (McAllester & Statos, 2018),
where N is the number of samples. In real fMRI datasets, however, strong dependency is rare and
existing MI estimation tools are limited more by their ability to accurately characterize the depen-
dency. Nevertheless, when quantitatively measuring strong dependency, cross-entropy (McAllester
& Statos, 2018) or model-based quantities, alternatives to MI, such as correlation or CCA, may be
measured with high confidence.
10
Under review as a conference paper at ICLR 2020
References
David Barber Felix Agakov. The IM algorithm: a variational approach to information maximization.
Advances in Neural Information Processing Systems, 16:201, 2004.
Ibrahim Ahmad and Pi-Erh Lin. A nonparametric estimation of the entropy for absolutely continuous
distributions (Corresp.). IEEE Transactions on Information Theory, 22(3):372-375, 1976.
Brian B Avants, Charles L Epstein, Murray Grossman, and James C Gee. Symmetric diffeomorphic
image registration with cross-correlation: evaluating automated labeling of elderly and neurode-
generative brain. Medical Image Analysis, 12(1):26-41, 2008.
Francis R Bach and Michael I Jordan. Kernel independent component analysis. Journal of machine
learning research, 3(Jul):1-48, 2002.
Yashar Behzadi, Khaled Restom, Joy Liau, and Thomas T Liu. A component based noise correction
method (CompCor) for BOLD and perfusion based fMRI. NeuroImage, 37(1):90-101, 2007.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Devon
Hjelm, and Aaron Courville. Mutual information neural estimation. In International Conference
on Machine Learning, pp. 530-539, 2018.
James Bergstra, Daniel Yamins, and David Daniel Cox. Making a science of model search: Hyper-
parameter optimization in hundreds of dimensions for vision architectures. 2013.
Thomas B Berrett and Richard J Samworth. Nonparametric independence testing via mutual infor-
mation. Biometrika, 106(3):547-566, 2019.
Robert W Cox. AFNI: software for analysis and visualization of functional magnetic resonance
neuroimages. Computers and Biomedical research, 29(3):162-173, 1996.
Carol Daniel. I knew you were black. https://themoth.org/stories/
i-knew-you-were-black, 2018. Accessed: 2018-10-12.
Oscar Esteban, Christopher Markiewicz, Ross W Blair, Craig Moodie, Ayse Ilkay Isik, Asier Er-
ramuzpe Aliaga, James Kent, Mathias Goncalves, Elizabeth DuPre, Madeleine Snyder, Hiroyuki
Oya, Satrajit Ghosh, Jessey Wright, Joke Durnez, Russell Poldrack, and Krzysztof Jacek Gor-
golewski. FMRIPrep: a robust preprocessing pipeline for functional MRI. bioRxiv, 2018. doi:
10.1101/306951.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1126-1135, 2017a.
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imita-
tion learning via meta-learning. In Conference on Robot Learning, pp. 357-368, 2017b.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Ad-
vances in Neural Information Processing Systems, pp. 9537-9548, 2018.
Vladimir S Fonov, Alan C Evans, Robert C McKinstry, CR Almli, and DL Collins. Unbiased
nonlinear average age-appropriate brain templates from birth to adulthood. NeuroImage, (47):
S102, 2009.
Neil Gaiman. The man who forgot ray bradbury. https://soundcloud.com/neilgaiman/
the-man-who-forgot-ray-bradbury, 2018. Accessed: 2018-10-12.
Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath. Estimating mutual informa-
tion for discrete-continuous mixtures. In Advances in Neural Information Processing Systems,
pp. 5986-5997, 2017.
Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying fixed k-nearest neighbor infor-
mation estimators. IEEE Transactions on Information Theory, 64(8):5629-5661, 2018.
11
Under review as a conference paper at ICLR 2020
Matthew F Glasser, Timothy S Coalson, Emma C Robinson, Carl D Hacker, John Harwell, Essa
Yacoub, Kamil Ugurbil, Jesper Andersson, Christian F Beckmann, Mark Jenkinson, et al. A
multi-modal parcellation of human cerebral cortex. Nature, 536(7615):171, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neu-
ral networks. In In Proceedings of the International Conference on Artificial Intelligence and
Statistics (AISTATS10). Society for Artificial Intelligence and Statistics, 2010.
Krzysztof Gorgolewski, Christopher Burns, Cindee Madison, Dav Clark, Yaroslav Halchenko,
Michael Waskom, and Satrajit Ghosh. Nipype: a flexible, lightweight and extensible neuroimag-
ing data processing framework in python. Frontiers in Neuroinformatics, 5:13, 2011. ISSN
1662-5196. doi: 10.3389/fninf.2011.00013. URL https://www.frontiersin.org/
article/10.3389/fninf.2011.00013.
Krzysztof J Gorgolewski, Tibor Auer, Vince D Calhoun, R Cameron Craddock, Samir Das, Eugene P
Duff, Guillaume Flandin, Satrajit S Ghosh, Tristan Glatard, Yaroslav O Halchenko, et al. The
brain imaging data structure, a format for organizing and describing outputs of neuroimaging
experiments. Scientific Data, 3:160044, 2016.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical de-
pendence with hilbert-schmidt norms. In International conference on algorithmic learning theory,
pp. 63-77. Springer, 2005a.
Arthur Gretton, Alexander J Smola, Olivier Bousquet, Ralf Herbrich, Andrei Belitski, Mark Augath,
Yusuke Murayama, Jon Pauls, Bernhard Scholkopf, and Nikos K Logothetis. Kernel constrained
covariance for dependence measurement. In AISTATS, volume 10, pp. 112-119, 2005b.
Douglas N Greve and Bruce Fischl. Accurate and robust brain image alignment using boundary-
based registration. NeuroImage, 48(1):63-72, 2009.
J Swaroop Guntupalli, Michael Hanke, Yaroslav O Halchenko, Andrew C Connolly, Peter J Ra-
madge, and James V Haxby. A model of representational spaces in human cortex. Cerebral
Cortex, 26(6):2919-2934, 2016.
Uri Hasson, Yuval Nir, Ifat Levy, Galit Fuhrmann, and Rafael Malach. Intersubject synchronization
of cortical activity during natural vision. Science, 303(5664):1634-1640, 2004.
Uri Hasson, Asif A Ghazanfar, Bruno Galantucci, Simon Garrod, and Christian Keysers. Brain-to-
brain coupling: a mechanism for creating and sharing a social world. Trends in cognitive sciences,
16(2):114-121, 2012.
James V Haxby, J Swaroop Guntupalli, Andrew C Connolly, Yaroslav O Halchenko, Bryan R Con-
roy, M Ida Gobbini, Michael Hanke, and Peter J Ramadge. A common, high-dimensional model
of the representational space in human ventral temporal cortex. Neuron, 72(2):404-416, 2011.
Caroline M Holmes and Ilya Nemenman. Estimation of mutual information for real-valued data
with error bars and controlled bias. arXiv preprint arXiv:1903.09280, 2019.
Mark Jenkinson, Peter Bannister, Michael Brady, and Stephen Smith. Improved optimization for
the robust and accurate linear registration and motion correction of brain images. NeuroImage,
17(2):825-841, 2002.
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. arXiv preprint arXiv:1806.03836, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
A. Kraskov, H. Stogbauer, and P. Grassberger. Estimating mutual information. Physical review E,
2004.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning, pp. 2113-
2122, 2015.
12
Under review as a conference paper at ICLR 2020
David McAllester and Karl Statos. Formal limitations on the measurement of mutual information.
arXiv preprint arXiv:1811.04251, 2018.
Samuel A Nastase, Valeria Gazzola, Uri Hasson, and Christian Keysers. Measuring shared responses
across subjects using intersubject correlation. Social Cognitive and Affective Neuroscience, 14(6):
667-685,05 2019. ISSN 1749-5016. doi: 10.1093∕scan∕nsz037. URL https://doi.org/
10.1093/scan/nsz037.
Jim O’Grady. Running from the Bronx. https://soundcloud.com/
the-story-collider/jim-ogrady-running-from-the,	2018a. Accessed:
2018-10-12.
Jim O’Grady. Pie Man. https://themoth.org/stories/pie-man, 2018b. Accessed:
2018-10-12.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search
via parameter sharing. In International Conference on Machine Learning, pp. 4092-4101, 2018.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A. Alemi, and George Tucker. On vari-
ational lower bounds of mutual information. In Bayesian Deep Learning Workshop, NeurIPSW,
2018.
Jonathan D Power, Anish Mitra, Timothy O Laumann, Abraham Z Snyder, Bradley L Schlaggar,
and Steven E Petersen. Methods to detect, characterize, and remove motion artifact in resting
state fMRI. NeuroImage, 84:320-341, 2014.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Marleen B Schippers, Alard Roebroeck, Remco Renken, Luca Nanetti, and Christian Keysers. Map-
ping the information flow from one brain to another during gestural communication. Proceedings
of the National Academy of Sciences, pp. 201001791, 2010.
Frank Sehnke, Christian Osendorfer, Thomas Ruckstieβ, Alex Graves, Jan Peters, and Jurgen
Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551-559, 2010.
Lauren J Silbert, Christopher J Honey, Erez Simony, David Poeppel, and Uri Hasson. Coupled neural
systems underlie the production and comprehension of naturalistic narrative speech. Proceedings
of the National Academy of Sciences, 111(43):E4687-E4696, 2014.
Erez Simony, Christopher J Honey, Janice Chen, Olga Lositsky, Yaara Yeshurun, Ami Wiesel, and
Uri Hasson. Dynamic reconfiguration of the default mode network during narrative comprehen-
sion. Nature Communications, 7:12141, 2016.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4077-4087, 2017.
Greg J Stephens, Lauren J Silbert, and Uri Hasson. Speaker-listener neural coupling underlies
successful communication. Proceedings of the National Academy of Sciences, 107(32):14425-
14430, 2010.
Jeffrey Mark Treiber, Nathan S White, Tyler Christian Steed, Hauke Bartsch, Dominic Holland,
Nikdokht Farid, Carrie R McDonald, Bob S Carter, Anders Martin Dale, and Clark C Chen.
Characterization and correction of geometric distortions in 814 diffusion weighted images. PLOS
ONE, 11(3):e0152472, 2016.
N. J. Tustison, B. B. Avants, P. A. Cook, Y. Zheng, A. Egan, P. A. Yushkevich, and J. C. Gee. N4itk:
improved n3 bias correction. IEEE Transactions on Medical Imaging, 29(6):1310-1320, June
2010. ISSN 0278-0062. doi: 10.1109/TMI.2010.2046908.
13
Under review as a conference paper at ICLR 2020
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing Systems, pp. 3630-3638, 2016.
Sijia Wang, Daniel J Peterson, J Christopher Gatenby, Wenbin Li, Thomas J Grabowski, and Tara M
Madhyastha. Evaluation of field map and nonlinear registration methods for correction of suscep-
tibility artifacts in diffusion mri. Frontiers in Neuroinformatics, 11:17, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Qinyi Zhang, Sarah Filippi, Arthur Gretton, and Dino Sejdinovic. Large-scale kernel methods for
independence testing. Statistics and Computing, 28(1):113-130, 2018.
Yongyue Zhang, Michael Brady, and Stephen Smith. Segmentation of brain MR images through a
hidden markov random field model and the expectation-maximization algorithm. IEEE Transac-
tions on Medical Imaging, 20(1):45-57, 2001.
14
Under review as a conference paper at ICLR 2020
A Appendix
A.1 The DEMINE Algorithm
Algorithm 2 DEMINE
Input Data: {(x, z)train, (x, z)val}
Parameters: Batch B, Iterations NO, Learning rate η
Output: MI, Tθ (X, Z)
1:	θ(0) J Xavier Initialization (GIorot & Bengio, 2010)
2:	for i = 1 : NO do
3:	Sample a batch of (xi, Zi)B 〜(x, z)train
4:	Compute L (xi, zi)B, θ(i-1)
5:	Compute NRL - gradient for θ
6:	Update θ(i) using Adam (Kingma & Ba, 2014) with η
7:	end foVr
8:	MI = I(X, Z)n,θ(NO)
9:	return MI, θ(NO)
A.2 Additional Details of the fMRI Dataset
The dataset we used contains 40 participants (mean age = 23.3 years, standard deviation = 8.9,
range: 1853; 27 female) recruited to listen to four spoken stories56. The stories were renditions of
“Pie Man” and “Running from the Bronx” by Jim OGrady (O’Grady, 2018b;a), “The Man Who
Forgot Ray Bradbury” by Neil Gaiman (Gaiman, 2018), and “I Knew You Were Black” by Carol
Daniel (Daniel, 2018); story durations were 7, 9, 14, and 13 minutes, respectively. After scanning,
participants completed a questionnaire comprising 25-30 questions per story intended to measure
narrative comprehension. The questionnaires included multiple choice, True/False, and fill-in-the-
blank questions, as well as four additional subjective ratings per story. Functional and structural
images were acquired using a 3T Siemens Prisma with a 64-channel head coil. Briefly, functional
images were acquired in an interleaved fashion using gradient-echo echo-planar imaging with a
multiband acceleration factor of 3 (TR/TE = 1500/31 ms where TE stands for “echo time”, resolution
= 2.5 mm isotropic voxels, full brain coverage).
All fMRI data were formatted according to the Brain Imaging Data Structure (BIDS) standard (Gor-
golewski et al., 2016) and preprocessed using the fMRIPrep library (Esteban et al., 2018). Functional
data were corrected for slice timing, head motion, and susceptibility distortion, and normalized to
MNI space using nonlinear registration. Nuisance variables comprising head motion parameters,
framewise displacement, linear and quadratic trends, sine/cosine bases for high-pass filtering (0.007
Hz), and six principal component time series from cerebrospinal fluid (CSF) and white matter (WM)
were regressed out of the signal using the Analysis of Functional NeuroImages (AFNI) software
suite (Cox, 1996).
The fMRI data comprise X ∈ RVi×T for each subject, where Vi represents the flattened and masked
voxel space and T represents the number of samples (in TRs) during auditory stimulus presentation.
Additional Details on Dataset Collection Functional and structural images were acquired using
a 3T Siemens Magnetom Prisma with a 64-channel head coil. Functional, blood-oxygenation-level-
dependent (BOLD) images were acquired in an interleaved fashion using gradient-echo echo-planar
imaging with pre-scan normalization, fat suppression, a multiband acceleration factor of 3, and
no in-plane acceleration: TR/TE = 1500/31 ms, flip angle = 67°, bandwidth = 2480 hz per pixel,
resolution = 2.5 mm3 isotropic voxels, matrix size = 96 x 96, Field of view (FoV) = 240 x 240
mm, 48 axial slices with roughly full brain coverage and no gap, anteriorposterior phase encoding.
At the beginning of each scanning session, a T1-weighted structural scan (where T1 stands for
5Two of the stories were told by a professional storyteller undergoing an fMRI scan; however, fMRI data
for the speaker were not analyzed for the present work due to the head motion induced by speech production.
6The study was conducted in compliance with the Institutional Review Board of the University
15
Under review as a conference paper at ICLR 2020
“longitudinal relaxation time”), was acquired using a high-resolution single-shot Magnetization-
Prepared 180 degrees radio-frequency pulses and RApid Gradient-Echo (MPRAGE) sequence with
an in-plane acceleration factor of 2 using GeneRalized Autocalibrating Partial Parallel Acquisition
(GRAPPA): TR/TE/TI = 2530/3.3/1100 ms where TI stands for inversion time, flip angle = 7。，
resolution = 1.0 x 1.0 x 1.0 mm voxels, matrix size = 256 x 256, FoV = 256 x 256 x 176 mm, 176
sagittal slices, ascending acquisition, anteriorposterior phase encoding, no fat suppression, 5 min
53 s total acquisition time. At the end of each scanning session a T2-weighted (where T2 stands
for “transverse relaxation time”) structural scan was acquired using the same acquisition parameters
and geometry as the T1-weighted structural image: TR/TE = 3200/428 ms, 4 minutes 40 seconds
total acquisition time. A field map was acquired at the beginning of each scanning session, but was
not used in subsequent analyses.
Additional Details on Dataset Preprocessing Preprocessing was performed using the fMRIPrep
library7 Esteban et al. (2018), a Nipype library8 (Gorgolewski et al., 2011) based tool. T1-weighted
images were corrected for intensity non-uniformity using the N4 bias field correction algorithm
(Tustison et al., 2010) and skull-stripped using Advanced Normalization Tools (ANTs) (Avants
et al., 2008). Nonlinear spatial normalization to the International Consortium for Brain Mapping
(ICBM) 152 Nonlinear Asymmetrical template version 2009c (Fonov et al., 2009) was performed
using ANTs. Brain tissue segmentation cerebrospinal fluid, white matter, and gray matter was
was performed using FSL library’s9 FAST tool Zhang et al. (2001). Functional images were slice
timing corrected using AFNI software’s 3dTshift (Cox, 1996) and corrected for head motion us-
ing FSL library’s MCFLIRT tool (Jenkinson et al., 2002). “Fieldmap-less” distortion correction
was performed by co-registering each subject’s functional image to that subject’s intensity-inverted
T1-weighted image (Wang et al., 2017) constrained with an average field map template (Treiber
et al., 2016). This was followed by co-registration to the corresponding T1-weighted image using
FreeSurfer software’s10 boundary-based registration (Greve & Fischl, 2009) with 9 degrees of free-
dom. Motion correcting transformations, field distortion correcting warp, BOLD-to-T1 transforma-
tion and T1-to-template (MNI) warp were concatenated and applied in a single step with Lanczos
interpolation using ANTs. Physiological noise regressors were extracted applying “a Component
Based Noise Correction Method” aCompCor (Behzadi et al., 2007). Six principal component time
series were calculated within the intersection of the subcortical mask and the union of CSF and
WM masks calculated in T1w (T1 weighted) space, after their projection to the native space of each
functional run. Framewise displacement (Power et al., 2014) was calculated for each functional
run. Functional images were downsampled to 3 mm resolution. Nuisance variables comprising six
head motion parameters (and their derivatives), framewise displacement, linear and quadratic trends,
sine/cosine bases for high-pass filtering (0.007 Hz cutoff), and six principal component time series
from an anatomically-defined mask of cerebrospinal fluid and white matter were regressed out of
the signal using AFNI’s 3dTproject (Cox, 1996). Functional response time series were z-scored for
each voxel.
A.3 Comparison between HSIC and DEMINE
We first review the Hilbert-Schmidt independence criterion (HSIC), a widely-studied correlation-
based independence criterion and discuss its connections with the MINE family of mutual informa-
tion lower bound methods, and then study DEMINE and a spectral HSIC implementation on the
synthetic datasets.
The HSIC approach (Gretton et al., 2005b;a) is based on a necessary and sufficient condition of
independence: two random variables X and Z are independent if and only if for all bounded or
positive functions f and g Rd 7→ R, EXZf(X)g(Z) - EXf(X)EZg(Z) = 0, or equivalently
EXZ (f (X) -EXf(X))(g(Z) -EZg(Z)) = 0. A proof can be constructed by showing equivalence
to the definition of independence, P (X, Z) = P (X)P (Z).
To construct an independence test, existing approaches (Gretton et al., 2005b;a) use Reproducing
Kernel Hilbert Spaces (RKHS) for f and g, a function space that not only covers all functions
between [0, 1], but also allows computationally efficient estimation or bounding of
7https://github.com/poldracklab/fmriprep
8https://github.com/nipy/nipype
9https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL
10https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferWiki
16
Under review as a conference paper at ICLR 2020
COCO(X, Z) = supf,g EXZf(X)g(Z) - EXf(X)EZg(Z)
given samples, and test COCO(X, Z) = 0. Confidence intervals are derived through McDiarmid’s
inequality, or using closed-form distributions to approximate the test statistics to a certain order of
moments, and compute the confidence interval from the closed-form distribution.
The COCO(X, Z) used by HSIC estimators bears great resemblance to the MINE family of mutual
information estimators. In fact, it can be shown that
COCO(X,Z) = sup% E(χ,z)〜PXZf(X)g(z) - Ex〜PXf(X)EZ〜PZg(z)
=supf,g E(x,z)〜PXZf(X)g(z) - Ex〜PX,z〜PZ log ef(x)g(z)
≥	suPf,g E(x,z)〜PXZf (x)g(z) - Ex〜PX log Ez〜PZef (XXg(Z) ≈ Iebi	(10)
≥	SUPf,g E(x,z)〜PXZ f(x)g(z) - log Ex 〜PX ,z〜PZ ef(x)g(z) ≈ IMINE
≥	SUPf,g E(x,z)〜PXZf(X)g(z) — Ex 〜PX ,Z〜PZ ef (x)g(z) + 1 ≈ IMINE-f,IEB
It means that within a family of decomposable functions where Tθ(X, Z) = f (X)g(Z),
COCO(X,Z) is an upperbound to the MINE estimates. In addition, the equivalence of COCO(X,Z) =
0 and I(X, Z) = 0 seems to suggest a form of mutual information bound. On the other hand, MINE
allows the use of non-decomposable Tθ(X, Z). Existing results on MINE (Poole et al., 2018) seem
to suggest that a non-decomposable Tθ (X, Z) gives superior empirical mutual information estima-
tion performance over a decomposable Tθ (X, Z). The necessity of non-decomposable Tθ(X, Z)
designs and mutual information lower bounds under decomposable designs of Tθ (X, Z) may be
subjects of further research.
Similar to the MINE estimators, HSIC-based estimators tend to have loose confidence intervals due
to the need to bound generalization error of kernels f and g on unseen data points. We expect a
cross-validation-based approach like DEMINE to also improve the performance of the HSIC-based
estimators.
Comparison between DEMINE and HSIC on synthetic benchmarks. We compare Canonical
Correlation Analysis (CCA), DEMINE, DEMINE-meta and HSIC for independent testing on our 4
synthetic Gaussian and sine wave benchmarks presented in Section 5. Results for a single random
seed is reported for a compact presentation, but we have ran experiments using multiple random
seeds and find the result of a single random seed representative enough.
For CCA, we compute p-value using the χ2 test. For HSIC, we report p-value using a publicly
available implementation for a spectral HSIC test (Zhang et al., 2018)11 12. The default kernel is used.
Hyperparameters are set to recommended setting when available. For DEMINE and DEMINE-meta,
the setup is identical to Section 5. A 2-sided 95% confidence interval is reported, but showing only
the lower side.
Experiment results are compiled in Table 3. Statistically significant dependence detections with p <
0.05 are bolded. Results show that spectral HSIC requires less data to test dependency for the simple
Gaussians dataset. But on the more challenging sine wave dataset, DEMINE-sig and DEMINE-
meta-sig perform better. Overall, we find DEMINE more complementary to linear correlations for
dependency testing on complex signals. Note that Gaussian kernels are used for spectral HSIC.
More complex kernels have potential to improve results.
B Sanity check on statistical dependency testing
We performed sanity check of our approach, as well as several statistical dependency testing imple-
mentations that we compare against. We run different statistical dependency testing implementations
on our 1D Gaussian ρ = 0.0, N = 30 samples dataset where X and Z are independent. A large
number of runs with different random seeds are performed. False positive rate ofp < 0.05 statistical
11https://github.com/oxmlcs/kerpy. We also experimented with classic HSIC with gamma ap-
proximation Gretton et al. (2005a) https://github.com/amber0309/HSIC and a block HSIC imple-
mentation (Zhang et al., 2018) from https://github.com/oxmlcs/kerpy, but find that they both re-
port significantly more than 5% false positives for independent 1D and 20D gaussians ρ = 0 at N = {30, 100}
across 100,000-1,000,000 random seeds, indicating errors in confidence interval calculations.
12This is a false positive case for CCA, because for this sine wave data ground truth correlation is 0.
17
Under review as a conference paper at ICLR 2020
Table 3: Statistical dependency testing comparison between CCA, spectral HSIC and DEMINE
algorithms on synthetic datasets. Statistically significant dependency with p < 0.05 detections are
bolded. See text for detailed experimental setups.
Problem	Samples	CCA	Spectral HSIC	DEMINE-sig	DEMINE-meta-sig
1D Gaussian ρ = 0.8	30	p=0.000	p=0.001	I ≥ 0.006-0.017	I ≥ 0.006-0.017
1D Gaussian ρ = 0.8	100	p=0.000	p=0.001	I ≥ 0.006-0.009	I ≥ 0.006-0.009
1D Gaussian ρ = 0.8	300	p=0.000	p=0.001	I ≥ 0.143-0.132	I ≥ 0.146-0.132
1D Gaussian ρ = 0.8	1000	p=0.000	p=0.001	I ≥ 0.278-0.168	I ≥ 0.292-0.168
1D Gaussian ρ = 0.8	3000	p=0.000	p=0.001	I ≥ 0.365-0.146	I ≥ 0.344-0.146
20D Gaussian ρ = 0.3	30	p=0.015	p=0.019	I ≥ -0.003-0.017	I ≥ 0.001-0.017
20D Gaussian ρ = 0.3	100	p=0.000	p=0.001	I ≥ 0.000-0.009	I ≥ 0.001-0.009
20D Gaussian ρ = 0.3	300	p=0.000	p=0.001	I ≥ 0.005-0.005	I ≥ 0.007-0.005
20D Gaussian ρ = 0.3	1000	p=0.000	p=0.001	I ≥ 0.322-0.170	I ≥ 0.376-0.170
20D Gaussian ρ = 0.3	3000	p=0.000	p=0.001	I ≥ 0.632-0.253	I ≥ 0.689-0.253
20D Gaussian ρ = 0.0	300	p=0.624	p=0.498	I ≥ 0.000-0.005	I ≥ 0.000-0.005
20D Gaussian ρ = 0.1	300	p=0.000	p=0.014	I ≥ 0.000-0.005	I ≥ 0.000-0.005
20D Gaussian ρ = 0.2	300	p=0.000	p=0.001	I ≥ 0.002-0.005	I ≥ 0.003-0.005
20D Gaussian ρ = 0.3	300	p=0.000	p=0.001	I ≥ 0.005-0.005	I ≥ 0.007-0.005
20D Gaussian ρ = 0.4	300	p=0.000	p=0.001	I ≥ 0.191-0.140	I ≥ 0.260-0.140
20D Gaussian ρ = 0.5	300	p=0.000	p=0.001	I ≥ 0.621-0.388	I ≥ 0.815-0.388
Sine wave a = 8π	30	p=0.826	p=0.856	I ≥ 0.000-0.005	I ≥ -0.002-0.017
Sine wave a = 8π	100	p=0.962	p=0.913	I ≥ 0.006-0.009	I ≥ 0.006-0.009
Sine wave a = 8π	300	p=0.093	p=0.498	I ≥ 0.448-0.306	I ≥ 0.511-0.306
Sine wave a = 8π	1000	p=0.04712	p=0.111	I ≥ 1.351-0.450	I ≥ 1.316-0.450
Sine wave a = 8π	3000	p=0.166	p=0.094	I ≥ 1.711-0.442	I ≥ 1.756-0.442
Table 4: Sanity check of several statistical dependency testing implementations. See text for details.
Approach	False Positive Rate (95% test confidence)	Number of Runs	Note
DEMINE-sig	0.000	200	
Spectral HSIC	0.040	612500	https://github.com/oxmlcs/kerpy
Block HSIC	0.418	612500	https://github.com/oxmlcs/kerpy
HSIC with Gamma Approximation	0.055	1000000	https://github.com/amber0309/HSIC
significance was recorded to validate if different implementations actually follow such false positive
rates. Correct implementations should have false positive rate lower or equal to 0.05. Results are
summarized in Table 4. Statistically significant deviations (under Hoeffding inequality) are marked
in bold font. The number of runs for DEMINE is relatively low, but no false positives were found.
Low false positive rate of DEMINE might be due to partly the conservative estimation provided by
Hoeffding inequality, and partly the generalization gap between train and test splits.
18