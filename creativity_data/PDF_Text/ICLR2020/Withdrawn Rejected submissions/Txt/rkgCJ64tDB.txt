Under review as a conference paper at ICLR 2020
Scaling-Translation-Equivariant Networks
with Decomposed Convolutional Filters
Anonymous authors
Paper under double-blind review
Ab stract
Encoding the scale information explicitly into the representation learned by a
convolutional neural network (CNN) is beneficial for many vision tasks especially
when dealing with multiscale inputs. We study, in this paper, a scaling-translation-
equivariant (ST -equivariant) CNN with joint convolutions across the space and the
scaling group, which is shown to be both sufficient and necessary to achieve ST-
equivariant representations. To reduce the model complexity and computational
burden, we decompose the convolutional filters under two pre-fixed separable
bases and truncate the expansion to low-frequency components. A further benefit
of the truncated filter expansion is the improved deformation robustness of the
equivariant representation. Numerical experiments demonstrate that the proposed
scaling-translation-equivariant networks with decomposed convolutional filters
(ScDCFNet) achieves significantly improved performance in multiscale image
classification and better interpretability than regular CNNs at a reduced model size.
1	Introduction
Convolutional neural networks (CNNs) have achieved great success in machine learning problems
such as image classification (Krizhevsky et al., 2012), object detection (Ren et al., 2015), and semantic
segmentation (Long et al., 2015; Ronneberger et al., 2015). Compared to fully-connected networks,
CNNs through spatial weight sharing have the benefit of being translation-equivariant, i.e., translating
the input leads to a translated version of the output. This property is crucial for many vision tasks such
as image recognition and segmentation. However, regular CNNs are not equivariant to other important
group transformations such as rescaling and rotation, and it is beneficial in some applications to also
encode such group information explicitly into the network representation.
Several network architectures have been designed to achieve (2D) roto-translation-equivariance
(SE(2)-equivariance) (Cheng et al., 2019; Marcos et al., 2017; Weiler et al., 2018b; Worrall et al.,
2017; Zhou et al., 2017), i.e., roughly speaking, if the input is spatially rotated and translated,
the output is transformed accordingly. The feature maps of such networks typically include an
extra index for the rotation group SO(2). Building on the idea of group convolutions proposed
by Cohen & Welling (2016) for discrete symmetry groups, Cheng et al. (2019) and Weiler et al.
(2018b) constructed SE(2)-equivariant CNNs by conducting group convolutions jointly across the
space and SO(2) using steerable filters (Freeman & Adelson, 1991). Scaling-translation-equivariant
(ST -equivariant) CNNs, on the other hand, have typically been studied in a less general setting in the
existing literature (Kanazawa et al., 2014; Marcos et al., 2018; Xu et al., 2014; Ghosh & Gupta, 2019).
In particular, to the best of our knowledge, a joint convolution across the space and the scaling group
S has yet been proposed to achieve equivariance in the most general form. This is possibly because
of two difficulties one encounters when dealing with the scaling group: First, unlike SO(2), it is
an acyclic and unbounded group; second, an extra index in S incurs a significant increase in model
parameters and computational burden. Moreover, since the scaling transformation is rarely perfect in
practice (due to changing view angle or numerical discretization), one needs to quantify and promote
the deformation robustness of the equivariant representation (i.e., is the model still “approximately”
equivariant if the scaling transformation is “contaminated” by a nuisance input deformation), which,
to the best of our knowledge, has yet been studied in prior works.
The purpose of this paper is to address the aforementioned theoretical and practical issues in the
construction of ST -equivariant CNN models. Specifically, our contribution is three-fold:
1.	We propose a general ST -equivariant CNN architecture with a joint convolution over R2and S,
which is proved in Section 4 to be both sufficient and necessary to achieve ST -equivariance.
2.	A truncated decomposition of the convolutional filters under a pre-fixed separable basis on the
two geometric domains (R2 and S) is used to reduce the model size and computational cost.
1
Under review as a conference paper at ICLR 2020
3.	We prove the representation stability of the proposed architecture up to equivariant scaling action
of the input signal.
Our contribution to the family of group-equivariant CNNs is non-trivial; in particular, the scaling
group unlike SO(2) is acyclic and non-compact. This poses challenges both in theory and in practice,
so that many previous works on group-equivariant CNNs cannot be directly extended. We introduce
new algorithm design and mathematical techniques to obtain the first general ST -equivariant CNN
in literature with both computational efficiency and proved representation stability.
2	Related Work
Mixed-scale and ST -equivariant CNNs. Incorporating multiscale information into a CNN repre-
sentation has been studied in many existing works. The Inception net (Szegedy et al., 2015) and
its generalizations (Szegedy et al., 2017; 2016; Li et al., 2019) stack filters of different sizes in a
single layer to address the multiscale salient features. Dilated convolutions (Pelt & Sethian, 2018;
Wang et al., 2018; Yu & Koltun, 2016; Yu et al., 2017), pyramid architectures (Ke et al., 2017; Lin
et al., 2017), and multiscale dense networks (Huang et al., 2017) have also been proposed to take
into account the multiscale feature information. Although the effectiveness of such models have
been empirically demonstrated in various vision tasks, there is still a lack of interpretability of their
ability to encode the input scale information. Group-equivariant CNNs, on the other hand, explicitly
encode the group information into the network representation. Cohen & Welling (2016) proposed
CNNs with group convolutions that are equivariant to several finite discrete symmetry groups. This
idea is later generalized in Cohen et al. (2018) and applied mainly to the rotation groups SO(2) and
SO(3) (Cheng et al., 2019; Weiler et al., 2018a;b). Although ST -equivariant CNNs have also been
proposed in the literature (Kanazawa et al., 2014; Marcos et al., 2018; Xu et al., 2014; Ghosh &
Gupta, 2019), they are typically studied in a less general setting. In particular, none of these previous
works proposed to conduct joint convolutions over R2 × S as a necessary and sufficient condition
to impose equivariance, for which reason they are thus variants of a special case of our proposed
architecture where the convolutional filters in S are Dirac delta functions (c.f. Remark 1.) The
scale-space semi-group correlation proposed in the concurrent work (Worrall & Welling, 2019) bears
the most resemblance to our proposed model, however, their approach is only limited to discrete
semigroups, whereas our model does not have such restriction.
Representation stability to input deformations. Input deformations typically induce noticeable
variabilities within object classes, some of which are uninformative for the vision tasks. Models that
are stable to input deformations are thus favorable in many applications. The scattering transform
(Bruna & Mallat, 2013; Mallat, 2010; 2012) computes translation-invariant representations that
are Lipschitz continuous to deformations by cascading predefined wavelet transforms and modulus
poolings. A joint convolution over R2 × SO(2) is later adopted in Sifre & Mallat (2013) to build
roto-translation scattering with stable rotation/translation-invariant representations. These models,
however, use pre-fixed wavelet transforms in the networks, and are thus nonadaptive to the data.
DCFNet (Qiu et al., 2018) combines a pre-fixed filter basis and learnable expansion coefficients in
a CNN architecture, achieving both data adaptivity and representation stability inherited from the
filter regularity. This idea is later extended by Cheng et al. (2019) to produce SE(2)-equivariant
representations that are Lipschitz continuous in L2 norm to input deformations modulo a global
rotation, i.e., the model stays approximately equivariant even if the input rotation is imperfect. To the
best of our knowledge, a theoretical analysis of the deformation robustness of a ST -equivariant CNN
has yet been studied, and a direct generalization of the result in Cheng et al. (2019) is futile because
the feature maps of a ST -equivariant CNN is typically not in L2 (c.f. Remark 2.)
3	ST -EQUIVARIANT CNN AND FILTER DECOMPOSITION
Group-equivariance is the property of a mapping f : X → Y to commute with the group actions on
the domain X and codomain Y . More specifically, let G be a group, and Dg, Tg, respectively, be
group actions on X and Y . A function f : X → Y is said to be G-equivariant if
f(Dgx) =Tg(f(x)),	∀g∈G, x∈X.	(1)
G-invariance is thus a special case of G-equivariance where Tg = IdY . For learning tasks where
the feature y ∈ Y is known a priori to change equivariantly to a group action g ∈ G on the input
x ∈ X, e.g. image segmentation should be equivariant to translation, it would be beneficial to reduce
the hypothesis space to include only G-equivaraint models. In this paper, we consider mainly the
scaling-translation group ST = S × R2 = R X R2. Given g = (β, V) ∈ ST and an input image
2
Under review as a conference paper at ICLR 2020
①⑼(u, λ)
*
½^¾(2-0ι√)2-2*	∖⅛αι,λ)
*伺
W^ς∖(2-α≡√)2-2* N，(“血人)
*
称LN鼠)他必，》)
* 7
Wgk2-st√)2-焉 M)(U,0≈1, A)
*
审限(2—/)2-竟 H()(%%入)
* I
W息(2-8吟2-竟 X("23,入)
W ⑼(«, λ)
*
⅛r^ι∖(2-αι√)2-3*
W^¾(2-q≈√)2-2*
*秘
W,^∖(2-αs√)2-2*
√1∖u,a⅛,λ)
aj(1)(«,a2,A)
①⑴(/a⅛,A)
C ⑵(u,aι,λ)
立⑵(u,a2,λ)
/2)(u,a3,λ)
(a)	A special case of ST -equivariant CNN with
(multiscale) spatial convolutions.
(b)	The general case of ST -equivariant CNN with
joint convolutions (Theorem 1).
a
a
Figure 1: (a) A special case of ST -equivariant CNN with only (multiscale) spatial convolutions. The previous
works on ST -equivariant CNNs (Kanazawa et al., 2014; Marcos et al., 2018; Xu et al., 2014; Ghosh & Gupta,
2019) are all variants of this architecture. (b) The general case of ST -equivariant CNN with joint convolutions
(Theorem 1) where information transfers among different scales. See Remark 1 for more explanation.
x(0)(u, λ) (u ∈ R2 is the spatial position, and λ is the unstructured channel index, e.g. RGB channels
of a color image), the scaling-translation group action Dg = Dβ,v on x(0) is defined as
Dβ,vX⑼(u,λ) := X⑼(2-β(U - v),λ) .	(2)
Constructing ST -equivariant CNNs thus amounts to finding an architecture A such that each trained
network f ∈ A commutes with the group action Dβ,v on the input and a similarly defined group
action Tβ,v (to be explained in Section 3.1) on the output.
3.1 ST -EQUIVARIANT CNNS
Inspired by Cheng et al. (2019) and Weiler et al. (2018b), we consider ST -equivariant CNNs with
an extra index α ∈ S for the the scaling group S = R: for each l ≥ 1, the l-th layer output
is denoted as X(l) (u, α, λ), where u ∈ R2 is the spatial position, α ∈ S is the scale index, and
λ ∈ [Ml] := {1, . . . , Ml} corresponds to the unstructured channels. We use the continuous model
for formal derivation, i.e., the images and feature maps have continuous spatial and scale indices.
In practice, the images are discretized on a Cartesian grid, and the scales are computed only on a
discretized finite interval. Similar to Cheng et al. (2019), the group action Tβ,v on the l-th layer
output is defined as a scaling-translation in space as well as a shift in the scale channel:
TβvX(I)(u, α, λ) := X(I) (2-β(U — v),α — β,λ) ,	∀ l ≥ 1.	(3)
A feedforward neural network is said to be scaling-translation-equivariant, i.e., equivariant to ST, if
X(l) [Dβ,vX(0)] = Tβ,vX(l)[X(0)], ∀l ≥ 1,	(4)
where we slightly abuse the notation X(l) [X(0)] to denote the l-th layer output given the input X(0).
The following Theorem shows that ST -equivariance is achieved if and only if joint convolutions are
conducted over S × R2 as in (5) and (6).
Theorem 1. A feedforward neural network with an extra index α ∈ S for layerwise output is
ST -equivariant if and only if the layerwise operations are defined as (5) and (6):

dU0 + b(1)(λ) ,	(5)
Wλ0,λ (2-αu0,α0) dαdu + b(I) (λ)) , ∀l > 1,	(6)
where σ : R → R is a pointwise nonlinear function.
We defer the proof of Theorem 1, as well as those of other theorems, to the appendix. We note that
the joint-convolution in Theorem 1 is a generalization of the group convolution proposed by Cohen &
Welling (2016) to a non-compact group ST in the continuous setting.
3
Under review as a conference paper at ICLR 2020
Remark 1. When the (joint) convolutional filter Wλ(l0),λ(u, α) takes the special form Wλ(l0),λ (u, α) =
Vλ(0l,)λ(u)δ(α), the joint convolution (6) over R2 × S reduces to only a (multiscale) spatial convolution
x(l)[x(l-1)](u,α,λ) = σ (XZ x(l-1)(u + U,α,λ∖^vλlλ (2-αu0) 2-2ɑdu0 + b(l)(λ)),⑺
i.e., the feature maps at different scales do not transfer information among each other (see Figure 1a).
The previous works (Kanazawa et al., 2014; Marcos et al., 2018; Xu et al., 2014; Ghosh & Gupta,
2019) on ST -equivariant CNNs are all based on this special case of Theorem 1.
Although the joint convolutions (6) on R2 × S provide the most general way of imposing ST-
equivariance, they unfortunately also incur a significant increase in the model size and computational
burden. Following the idea of Cheng et al. (2019) and Qiu et al. (2018), we address this issue by
taking a truncated decomposition of the convolutional filters under a pre-fixed separable basis, which
will be discussed in detail in the next section.
3.2 Separable Basis Decomposition
We consider decomposing the convolutional filters Wλ(l0),λ (u, α) under the product of two func-
tion bases, {ψk (u)}k and {夕m(α)}m, which are the eigenfunctions of the Dirichlet Laplacian on,
respectively, the unit disk D ⊂ R2 and [-1, 1], i.e.,
∆ψk = -μkψk in D, and J 夕* = -Vmgm in [—1,1]
ψk = 0 on dD,	[夕m (-1) =2 m(1)=0.
(8)
In particular, the spatial basis {ψk }k satisfying (8) is the Fourier-Bessel (FB) basis (Abramowitz
& Stegun, 1965). In the continuous formulation, the spatial “pooling” operation is equivalent to
rescaling the convolutional filters in space. We thus assume, without loss of generality, that the
convolutional filters are compactly supported as follows
Wλ(10,)λ ∈ Cc(2j1D), and Wλ(l0),λ ∈ Cc(2jlD × [-1, 1]), ∀l > 1.
Let ψj,k(U) := 2-2jψk(2-ju), then W：0} Can be decomposed under {ψjι,k}k and {ψm}m as
(9)
Wλ(10,)λ (u) = Xa(λ10),λ (k)ψj1,k(u), Wλ(l0),λ (u, α) = X Xa(λl0),λ(k,m)ψjι,k(U)夕m(α), l> 1 (10)
k	mk
where a(λ10),λ (k) and a(λl0),λ (k, m) are the expansion coefficients of the filters. During training, the
basis functions are fixed, and only the expansion coefficients are updated. In practice, we truncate
the expansion to only low-frequency components (i.e., a(λl0),λ (k, m) are non-zero only for k ∈ [K],
m ∈ [Kα]), which are kept as the trainable parameters. Similar idea has also been considered in
the prior works (Qiu et al., 2018; Cheng et al., 2019; Jacobsen et al., 2016). This directly leads to a
reduction of network parameters and computational burden. More specifically, let us compare the l-th
convolutional layer (6) of a ST -equivariant CNN with and without truncated basis decomposition:
Number of trainable parameters: Suppose the filters Wλ(l0),λ (U, α) are discretized on a Cartesian
grid of size L × L × Lα. The number of trainable parameters at the l-th layer of a ST -equivariant CNN
without basis decomposition is L2LαMl-1Ml. On the other hand, in an ScDCFNet with truncated
basis expansion up to K leading coefficients forU and Kα coefficients for α, the number of parameters
is instead KKaMl_1M1. Hence a reduction to a factor of KKa∕L2Lɑ in trainable parameters is
achieved for ScDCFNet via truncated basis decomposition. In particular, if L= 5, Lα = 5, K = 8,
and Kα = 3, then the number of parameters is reduced to (8 × 3)/(52 × 5) = 19.2%.
Computational cost: Suppose the size of the input x(l-1) (U, α, λ) and output x(l)(U, α, λ) at the
l-th layer are, respectively, W × W × Nα × Ml-1 and W × W × Nα × Ml, where W × W is
the spatial dimension, Nα is the number of scale channels, and Ml-1(Ml) is the number of the
unstructured input (output) channels. Let the filters Wλ(l0),λ (U, α) be discretized on a Cartesian grid of
size L× L× Lα. The following theorem shows that, compared to a regular ST -equivariant CNN,
the computational cost in a forward pass of ScDCFNet is reduced again to a factor of KKa∕L2La.
Theorem 2. Assume Ml L2, Lα, i.e., the number of the output channels is much larger than the
size of the convolutional filters in U and α, then the computational cost of an ScDCFNet is reduced to
afactor of KKa∕L1La when compared to a ST-equivariant CNN without basis decomposition.
4
Under review as a conference paper at ICLR 2020
4 Representation Stability of ScDCFNet to Input Deformation
Apart from reducing the model size and computational burden, we demonstrate in this section that
truncating the filter decomposition has the further benefit of improving the deformation robustness of
the equivariant representation, i.e., the equivaraince relation (4) still approximately holds true even
if the spatial scaling of the input Dβ,vx(0) is contaminated by a local deformation. The analysis is
motivated by the fact that scaling transformations are rarely perfect in practice — they are typically
subject to local distortions such as changing view angle or numerical discretization. To quantify the
distance between different feature maps at each layer, we define the norm of x(l) as
M0	2
kx( )k = M0X/卜()(UH du，kx()k =SuPMI
Ml
λ=1
(u, α, λ) du, l ≥ 1. (11)
Remark 2. The definition of kx(l) k is different from that of RotDCFNet (Cheng et al., 2019), where
an L2 norm is taken for the α index as well. The reason why we adopt the L∞ norm for α in (11) is
that x(l) is typically not L2 in α, since the scaling group S, unlike S O(2), has infinite Haar measure.
We next quantify the representation stability of ScDCFNet under three mild assumptions on the
convolutional layers and input deformations. First,
(A1) The pointwise nonlinear activation σ : R → R is non-expansive.
Next, we need a bound on the convolutional filters under certain norms. For each l ≥ 1, define Al as
M0
A1 := πmax Sup	ka(λ10)λkFB
λ λ0=1	,
(	M1-1
，MO Sup X kaλ1!λkFB∣，
Al := π max sup X X ∣∣aλl0),λ(∙,m)kFB,
I λ λ0=1 m
2M-1 X SUp X ∣∣aλl0),λ(∙,m)kFB
Ml	m λ0 λ=1
(12)
where the Fourier-Bessel (FB) norm kakFB of a sequence {a(k)}k≥0 is a weighted l2 norm defined
as IlakFB ：= Pk μka(k)2, where μk is the k-th eigenvalue of the Dirichlet LaPlacian on the unit disk
defined in (8). We next assume that each Al is bounded:
(A2) For all l ≥ 1, Al ≤ 1.
The boundedness of Al is facilitated by truncating the basis decomPosition to only low-frequency
components (small μk), which is one of the key idea of ScDCFNet explained in Section 3.2. After a
ProPer initialization of the trainable coefficients, (A2) can generally be satisfied. The assumPtion
(A2) implies several bounds on the convolutional filters at each layer (c.f. Lemma 2 in the appendix),
which, combined with (A1), guarantees that an ScDCFNet is layerwise non-expansive:
Proposition 1. Under the assumption (A1) and (A2), an ScDCFNet satisfies the following.
(a)	For any l ≥ 1, the mapping ofthe l -th layer, x(l)[∙] defined in (5) and (6), is non-expansive, i.e.,
∣x(l)[x1]	-	x(l)[x2]∣	≤ ∣x1	-	x2∣,	∀x1,	x2.	(13)
(b)	Let x(0l) be the l-th layer output given a zero bottom-layer input, then x(0l)(λ) depends only on λ.
(c)	Let x(cl) be the centered version of x(l) after removing x(0l), i.e., x(c0) (u, λ) := x(0) (u, λ) -
x(00) (λ)	=	x(0) (u, λ),	and	x(cl) (u, α, λ) :=	x(l) (u, α, λ) -	x(0l) (λ), ∀l ≥	1,	then ∣x(cl) ∣	≤
∣x(cl-1) ∣, ∀l ≥ 1. As a result, ∣x(cl) ∣ ≤ ∣x(c0) ∣ = ∣x(0) ∣.
Finally, we make an assumption on the input deformation modulo a global scale change. Given a C2
function τ : R2 → R2, the spatial deformation Dτ on the feature maps x(l) is defined as
Dτ x(0) (u, λ) = x(0) (ρ(u), λ), and Dτ x(l) (u, α, λ) = x(l) (ρ(u), α, λ), l ≥ 1,	(14)
where ρ(u) = u - τ (u). We assume a small local deformation on the input:
(A3) ∣Vτ∣∞ := SUPu ∣∣Vτ(u)k < 1/5, where ∣∣ ∙ ∣∣ is the operator norm.
The following theorem demonstrates the representation stability ofan ScDCFNet to input deformation
modulo a global scale change.
5
Under review as a conference paper at ICLR 2020
First-layer features (CNN) First-layer features (ScDCFNet) Second-layer features (CNN) Second-layer features (ScDCFNet)
Figure 2: Verification of ST -equivariance in Section 5.1. Given the original input x(0) and its rescaled version
Dβ,vx(0), the four figures in each dashed rectangle are: x(l) [x(0)] (l-th layer feature of the original input),
x(l) [Dβ,vx(0)] (l-th layer feature of the rescaled input), Tβ,v x(l) [x(0)] (rescaled l-th layer feature of the original
input), and the difference (x(l) [Dβ,vx(0)] - Tβ,v x(l) [x(0)]) displayed in a (signal intensity) scale relative to
the maximum value of x(l) [Dβ,vx(0)]. It is clear that even after numerical discretization, ST -equivariance still
approximately holds for ScDCFNet, i.e., x(l) [Dβ,vx(0)] - Tβ,v x(l) [x(0)] ≈ 0, but not for a regular CNN.
Theorem 3. Let Dτ be a small spatial deformation defined in (14), and let Dβ,v , Tβ,v be the group
actions corresponding to an arbitrary scaling 2-β ∈ R+ centered at v ∈ R2 defined in (2) and (3).
In an ScDCFNet satisfying (A1), (A2), and (A3), we have, for any L,
卜(L) [Dβ,v ◦ DTX⑼]-Tβ,vX(L) [x⑼]I) ≤ 2β+1 (4L∣Vτ∣∞ + 2-jL ∣τ∣∞) ∣∣x⑼ k.	(15)
Theorem 3 gauges how approximately equivariant is ScDCFNet if the input undergoes not only a
scale change Dβ,v but also a nonlinear spatial deformation Dτ, which is important both in theory
and in practice because the scaling of an object is rarely perfect in reality.
5	Numerical Experiments
In this section, we conduct several numerical experiments for the following three purposes.
1.	To verify that ScDCFNet indeed achieves ST -equivariance (4).
2.	To illustrate that ScDCFNet significantly outperforms regular CNNs at a much reduced model
size in multiscale image classification.
3.	To show that a trained ScDCFNet auto-encoder is able to reconstruct rescaled versions of the
input by simply applying group actions on the image codes, demonstrating that ScDCFNet indeed
explicitly encodes the input scale information into the representation.
The experiments are tested on the Scaled MNIST (SMNIST) and Scaled Fashion-MNIST (SFashion)
datasets, which are built by rescaling the original MNIST and Fashion-MNIST (Xiao et al., 2017)
images by a factor randomly sampled from a uniform distribution on [0.3, 1]. A zero-padding to a
size of 28 × 28 is conducted after the rescaling. If mentioned explicitly, for some experiments, the
images are resized to 64 × 64 for better visualization. The implementation details of ScDCFNet are
explained in Appendix B.1. In particular, we discuss how to discretize the integral (6) and truncate
the scale channel to a finite interval for practical implementation. We also explain how to mitigate the
boundary “leakage” effect incurred by the truncation. Moreover, modifications to the spatial pooling
and batch-normalization modules of ScDCFNet to maintain ST -equivariance are also explained.
5.1	VERIFICATION OF ST -EQUIVARIANCE
We first verify that ScDCFNet indeed achieves ST -equivariance (4). Specifically, we compare
the feature maps of a two-layer ScDCFNet with randomly generated truncated filter expansion
coefficients and those of a regular CNN. The exact architectures are detailed in Appendix B.2.
Figure 2 displays the first- and second-layer feature maps of an original image X(0) and its rescaled
version Dβ,vX(0) using the two comparing architectures. Feature maps at different layers are rescaled
to the same spatial dimension for visualization. The four images enclosed in each of the dashed
rectangle correspond to: X(l) [X(0)] (l-th layer feature of the original input), X(l) [Dβ,vX(0)] (l-th layer
feature of the rescaled input), Tβ,vX(l) [X(0)] (rescaled l-th layer feature of the original input, where
Tβ,v is understood as Dβ,v for a regular CNN due to the lack of a scale index α), and the difference
6
Under review as a conference paper at ICLR 2020
(a) Zero-padding for the scale channel.
Figure 3: The numerical error in equivariance (i.e., the boundary “leakage” effect incurred by scale channel
truncation) as a function of network depth. Either (a) zero-padding or (b) replicate-padding is used for the
convolution in scale. The error is unavoidable as depth becomes larger, but it can be mitigated by (1) using joint
convolutional filters with a smaller support in scale (i.e., a smaller number of “taps” after discretization), and (2)
using a replicate-padding instead of zero-padding. See Appendix B.1 for detailed explanation.
(b) Replicate-padding for the scale channel.
x(l) [Dβ,vx(0)] - Tβ,vx(l) [x(0)]. It is clear that even with numerical discretization, which can be
modeled as a form of input deformation, ScDCFNet is still approximately ST -equivariant, i.e.,
x(l) [Dβ,v x(0)] ≈ Tβ,v x(l) [x(0)],
whereas a regular CNN does not have such a property.
We also examin how the numerical error in equivariance (incurred by the the boundary “leakage”
effect after the scale channel truncation) evolves as the network gets deeper. The error in equivariance
is measured in a relative L2 sense at a particular scale α, i.e.,
Error = kx(l)[Dβ,vX⑼](∙,α) - Ts,vX(I)[x⑼](.,°)山/|禽,。X(I)[x⑼](・,。)除.(16)
It is clear from Figure 3 that the boundary “leakage” effect is unavoidable as the network becomes
deeper. However, the error can be alleviated by either choosing joint convolutional filters with a
smaller support in scale (i.e., the filter size, or the number of “taps”, after discretization in scale is
much smaller compared to the number of scale channels in the feature map), or using a replicate-
padding in the scale channel for the joint convolution. See Appendix B.1 for detailed explanation.
5.2	Multiscale Image Classification
We next demonstrate the improved performance of ScDCFNet in multiscale image classification. The
experiments are conducted on SMNIST and SFashion, and a regular CNN is used as a performance
benchmark. Both networks are comprised of three convolutional layers with the exact architectures
(Table 2) detailed in Appendix B.3. Since the scaling group S = R is unbounded, We compute only
the feature maps X(l) (u, α, λ) with the α index restricted to the truncated scale interval [-1.6, 0]
(2-1.6 ≈ 0.3), Which is discretized uniformly into Nα = 9 channels (again, see Appendix B.1
for implementation details.) The performance of the comparing architectures With and Without
batch-normalization is shoWn in Table 1. It is clear that, by limiting the hypothesis space to ST-
equivaraint models and taking truncated basis decomposition to reduce the model size, ScDCFNet
achieves a significant improvement in classification accuracy With a reduced number of trainable
parameters. The advantage of ScDCFNet is more pronounced When the number of training samples
is small (Ntr = 2000), suggesting that, by hardWiring the input scale information directly into its
representation, ScDCFNet is less susceptible to overfitting the limited multiscale training data.
We also observe that even When a regular CNN is trained With data augmentation (random cropping
and rescaling), its performance is still inferior to that of an ScDCFNet Without manipulation of the
training data. In particular, although the accuracies of the regular CNNs trained on 2000 SMNIST
and SFashion images after data augmentation are improved to, respectively, 93.85% and 79.41%,
they still underperform the ScDCFNets Without data augmentation (93.91% and 79.94%) using only
a fraction of trainable parameters. Moreover, if ScDCFNet is trained With data augmentation, the
accuracies can be further improved to 94.30% and 80.62% respectively. This suggests that ScDCFNet
can be combined With data augmentation for optimal performance in multiscale image classification.
7
Under review as a conference paper at ICLR 2020
Without batch-normalization	SMNIST test accuracy (%)	SFashion test accuracy (%)
Architectures	Ratio	Ntr = 2000	Ntr = 5000	Ntr = 10000	Ntr = 2000	Ntr = 5000
CNN, M = 32	1.00	92.60 ± 0.17	94.86 ± 0.25	96.43 ± 0.18	77.74 ± 0.28	82.57 ± 0.38
ScDCFNet, M = 16 K = 10, Kα = 3	0.84	93.75 ± 0.02	95.70 ± 0.09	96.89 ± 0.10	78.95 ± 0.31	83.51 ± 0.71
K = 8, Kα = 3	0.67	93.91 ± 0.30	95.71 ± 0.10	96.81 ± 0.12	79.22 ± 0.50	83.06 ± 0.32
K = 5, Kα = 3	0.42	93.52 ± 0.29	95.19 ± 0.13	96.77 ± 0.12	79.74 ± 0.44	83.46 ± 0.69
K = 10, Kα = 2	0.56	93.68 ± 0.23	95.54 ± 0.21	96.87 ± 0.13	79.01 ± 0.61	83.43 ± 0.60
K = 8, Kα = 2	0.45	93.67 ± 0.19	95.51 ± 0.20	96.85 ± 0.06	79.15 ± 0.59	83.44 ± 0.37
K = 5, Kα = 2	0.28	93.51 ± 0.30	95.35 ± 0.21	96.49 ± 0.17	78.57 ± 0.53	82.95 ± 0.46
ScDCFNet, M = 8 K = 10, Kα = 2	0.14	93.68 ± 0.17	95.21 ± 0.12	96.51 ± 0.24	79.11 ± 0.76	82.92 ± 0.68
K = 8, Kα = 2	0.11	93.39 ± 0.25	95.25 ± 0.47	96.73 ± 0.16	78.43 ± 0.76	82.53 ± 0.58
K = 5, Kα = 2	0.09	93.21 ± 0.20	94.99 ± 0.12	96.35 ± 0.12	77.97 ± 0.37	82.21 ± 0.67
K = 10, Kα = 1	0.09	93.26 ± 0.52	95.22 ± 0.31	96.52 ± 0.21	78.71 ± 0.46	83.05 ± 0.51
K = 8, Kα = 1	0.06	93.61 ± 0.11	95.03 ± 0.31	96.61 ± 0.18	78.69 ± 0.72	82.96 ± 0.27
K = 5, Kα = 1	0.04	93.23 ± 0.13	94.84 ± 0.38	96.31 ± 0.17	78.38 ± 0.81	82.14 ± 0.25
With batch-normalization		SMNIST test accuracy (%)			SFashion test accuracy (%)	
Architectures	Ratio	Ntr = 2000	Ntr = 5000	Ntr = 10000	Ntr = 2000	Ntr = 5000
CNN, M = 32	1.00	94.78 ± 0.17	96.58 ± 0.17	97.41 ± 0.21	79.79 ± 0.40	84.38 ± 0.17
ScDCFNet, M = 16 K = 10, Kα = 3	0.84	95.69 ± 0.15	96.99 ± 0.23	97.71 ± 0.11	81.12 ± 0.42	84.97 ± 0.16
K = 8, Kα = 3	0.67	95.72 ± 0.29	97.15 ± 0.24	97.72 ± 0.07	81.41 ± 0.35	85.11 ± 0.26
K = 5, Kα = 3	0.42	95.31 ± 0.21	96.75 ± 0.13	97.38 ± 0.07	81.73 ± 0.14	84.70 ± 0.19
K = 10, Kα = 2	0.56	95.58 ± 0.16	96.92 ± 0.07	97.61 ± 0.09	81.21 ± 0.20	84.77 ± 0.15
K = 8, Kα = 2	0.45	95.76 ± 0.17	96.74 ± 0.22	97.68 ± 0.09	80.84 ± 0.28	84.70 ± 0.18
K = 5, Kα = 2	0.28	95.43 ± 0.09	96.54 ± 0.20	97.43 ± 0.13	81.23 ± 0.23	84.65 ± 0.36
SI-CNN	1.02	—	—	97.25 ± 0.09	—	—
VF-SECNN	1.02	—	—	97.56 ± 0.07	—	—
Table 1: Classification accuracy on the SMNIST and SFashion dataset with and without batch-normalization.
The architectures are detailed in Table 2. In particular, M stands for the number of the first-layer (unstructured)
output channels, which is doubled after each layer, and K/Ka is the number of basis function in u∕α used for
filter decomposition. The networks are tested with different training data size, Ntr = 2000, 5000, and 10000,
and the means and standard deviations after three independent trials are reported. The column “ratio” stands
for the ratio between the number of trainable parameters of the current architecture and that of the baseline
CNN. The results of the Locally Scale-Invariant CNN (SI-CNN) (Kanazawa et al., 2014) and Vector Field
Scale-Equivariant CNN (VF-SECNN) (Marcos et al., 2018) reported in (Marcos et al., 2018) are displayed at the
bottom of the table for comparison.
Input
CNN auto-encoder
Decoder(C)
ScDCFNet auto-encoder
DeCOder(O)
De∞der(7⅛ιt,C,)
a
S
a
S
S
Figure 4: Reconstructing rescaled versions of the original test image by manipulating its image code C
according to the group action (3). The first two images on the left are the original inputs; Decoder(C) denotes
the reconstruction using the (unchanged) image code C; Decoder(Dβ,v C) and Decoder(Tβ,vC) denote the
reconstructions using the “rescaled” image codes Dβ,vC and Tβ,v C respectively according to (2) and (3).
Unlike the regular CNN auto-encoder, the ScDCFNet auto-encoder manages to generate rescaled versions of the
original input, suggesting that it successfully encodes the scale information directly into the representation.
5.3	Image Reconstruction
In the last experiment, we illustrate the ability of ScDCFNet to explicitly encode the input scale
information into the representation. To achieve this, we train an ScDCFNet auto-encoder on the
SMNIST dataset with images resized to 64 × 64 for better visualization. The encoder stacks two ST-
equivaraint convolutional blocks with 2 × 2 average-pooling, and the decoder contains a succession
of two transposed convolutional blocks with 2 × 2 upsampling. A regular CNN auto-encoder is also
trained for comparison (see Table 3 in Appendix B.4 for the detailed architecture.)
Our goal is to demonstrate that the image code produced by the ScDCFNet auto-encoder contains
the scale information of the input, i.e., by applying the group action Tβ,v (3) to the code C of a test
8
Under review as a conference paper at ICLR 2020
image before feeding it to the decoder, we can reconstruct rescaled versions of original input. This
property can be visually verified in Figure 4. In contrast, a regular CNN auto-encoder fails to do so.
6 Conclusion
We propose, in this paper, a ST -equivaraint CNN with joint convolutions across the space R2 and
the scaling group S, which we show to be both sufficient and necessary to impose ST -equivariant
network representation. To reduce the computational cost and model complexity incurred by the joint
convolutions, the convolutional filters supported on R2 × S are decomposed under a separable basis
across the two domains and truncated to only low-frequency components. Moreover, the truncated
filter expansion leads also to improved deformation robustness of the equivaraint representation,
i.e., the model is still approximately equivariant even if the scaling transformation is imperfect.
Experimental results suggest that ScDCFNet achieves improved performance in multiscale image
classification with greater interpretability and reduced model size compared to regular CNN models.
For future work, we will study the application of ScDCFNet in other more complicated vision tasks,
such as object detection/localization and pose estimation, where it is beneficial to directly encode
the input scale information into the deep representation. Moreover, the memory usage of our current
implementation of ScDCFNet scales linearly to the number of the truncated basis functions in order
to realize the reduced computational burden explained in Theorem 2. We will explore other efficient
implementation of the model, e.g., using filter-bank type of techniques to compute convolutions with
multiscale spatial filters, to significantly reduce both the computational cost and memory usage.
References
Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas, graphs,
and mathematical tables, volume 55. Courier Corporation, 1965.
Joan BrUna and StePhane Mallat. Invariant scattering convolution networks. IEEE transactions on
pattern analysis and machine intelligence, 35(8):1872-1886, 2013.
Xiuyuan Cheng, Qiang Qiu, Robert Calderbank, and Guillermo SaPiro. RotDCF: DecomPosition
of convolutional filters for rotation-equivariant deeP networks. In International Conference on
Learning Representations, 2019.
Taco Cohen and Max Welling. GrouP equivariant convolutional networks. In International conference
on machine learning, PP. 2990-2999, 2016.
Taco Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homogeneous
sPaces. arXiv preprint arXiv:1811.02017, 2018.
William T. Freeman and Edward H Adelson. The design and use of steerable filters. IEEE Transactions
on Pattern Analysis & Machine Intelligence, (9):891-906, 1991.
Rohan Ghosh and AnuPam K GuPta. Scale steerable filters for locally scale-invariant convolutional
neural networks. arXiv preprint arXiv:1906.03861, 2019.
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q Wein-
berger. Multi-scale dense networks for resource efficient image classification. arXiv preprint
arXiv:1703.09844, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, PP. 448-456, Lille, France, 07-09 Jul 2015. PMLR.
Jorn-Henrik Jacobsen, Jan van Gemert, Zhongyu Lou, and Arnold WM Smeulders. Structured
recePtive fields in cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, PP. 2610-2619, 2016.
Angjoo Kanazawa, Abhishek Sharma, and David Jacobs. Locally scale-invariant convolutional neural
networks. arXiv preprint arXiv:1412.5104, 2014.
Tsung-Wei Ke, Michael Maire, and Stella X Yu. Multigrid neural architectures. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, PP. 6665-6673, 2017.
9
Under review as a conference paper at ICLR 2020
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
Honal neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012.
Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. 2019.
TsUng-Yi Lin, Piotr Dolldr, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2117-2125, 2017.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2015.
StePhane Mallat. Recursive interferometric representation. In Proc. OfEUSICO conference, Dane-
mark, 2010.
Stephane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65
(10):1331-1398, 2012.
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector
field networks. In Proceedings of the IEEE International Conference on Computer Vision, pp.
5048-5057, 2017.
Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, and Devis Tuia. Scale equivariance in cnns
with vector fields. arXiv preprint arXiv:1807.11783, 2018.
Daniel M Pelt and James A Sethian. A mixed-scale dense convolutional neural network for image
analysis. Proceedings of the National Academy of Sciences, 115(2):254-259, 2018.
Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, and Guillermo Sapiro. DCFNet: Deep neural network
with decomposed convolutional filters. In Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4198-4207,
StockholrnsmAdssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 91-99. Curran
Associates, Inc., 2015.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F.
Frangi (eds.), Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015>, pp.
234-241, Cham, 2015. Springer International Publishing. ISBN 978-3-319-24574-4.
Laurent Sifre and Stephane Mallat. Rotation, scaling and deformation invariant scattering for texture
discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 1233-1240, 2013.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In Thirty-First AAAI Conference on
Artificial Intelligence, 2017.
Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi Hou, and Garrison Cottrell.
Understanding convolution for semantic segmentation. In 2018 IEEE Winter Conference on
Applications of Computer Vision (WACV), pp. 1451-1460. IEEE, 2018.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. In Advances in Neural Information
Processing Systems, pp. 10381-10392, 2018a.
10
Under review as a conference paper at ICLR 2020
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation
equivariant cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 849-858, 2018b.
Daniel E Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. arXiv preprint
arXiv:1905.11697, 2019.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028-5037, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, and Zheng Zhang. Scale-invariant
convolutional neural networks. arXiv preprint arXiv:1411.6369, 2014.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Interna-
tional Conference on Learning Representations, 2016.
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated residual networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Richard Zhang. Making convolutional networks shift-invariant again. arXiv preprint
arXiv:1904.11486, 2019.
Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 519-528, 2017.
A Proofs
A.1 Proof of Theorem 1
Proof of Theorem 1. We note first that (4) holds true if and only if the following being valid for all
l≥1,
Tβ,vx(l) [x(l-1)] = x(l)[Tβ,vx(l-1)],
(17)
where Tβ,vx(0) is understood as Dβ,vx(0). We also note that the layer-wise operations of a general
feedforward neural network with an extra index α ∈ S can be written as
x(1) [x(0)](u, α, λ) = σ X x(0) (u + u0, λ0)W (1) (u0, λ0, u, α, λ)du0 + b(1) (λ) ,	(18)
and, for l > 1,
x(l) [x(l-1)](u, α, λ)
x(l-1)(u + u0, α + α0, λ0)
W(l) (u0, α0, λ0, u, α, λ)dα0du0 + b(l)(λ) .
(19)
To prove the sufficient part: when l = 1, (2), (3), and (5) lead to
Tβ,vx(1) [x(0)](u, α, λ) = x(1) [x(0)] (2-β(U — v),α — β,λ)
(2-β(U — v) + U, λ) Wλ1,)λ(2-(a-e)u0) 2-2(α-β)du0 + b⑴(λ)
(2-β(U — V + U),λ0) Wλ1,)λ (2-αU) 2-2αdU + b(1)(λ)),
(20)
and
x(1) [Dβ,v x(0)](U, α, λ)
11
Under review as a conference paper at ICLR 2020
Dβ,vX(O) (U + u0, λ0) Wλ1,)λ (2-αu0) 2-2αdu0 + b(I) (λ)
X(O)(2-β (u + u0 - v), λ0) Wλ1,)λ (2-αu0) 2-2αdu0 + b⑴(λ) .	(21)
Hence Tβ,v x(1) [x(0)] = x(1) [Dβ,vx(0)].
-2αdUda0 + b(l)(λ)
(22)
x(l) [Tβ,vx(l-1)](u, α, λ)
Tβ,vX(IT)(U + u0,α + α, λ0)Wλ0,λ (2-αu0, a0) 2-2αdα0du0 + b(I) (λ)
)(2-β(u + u0 - v),α + α0 - β, λ0)wλ0,λ (2-αu0, α0) 2-2αdα0du0 + b(l)(λ)
(23)
Therefore Tβ,v X(l) [X(l-1)] = X(l) [Tβ,vX(l-1)], ∀l > 1.
To prove the necessary part: when l = 1, we have
X⑴[x(0)] (2-β(u — v),α — β,λ)
V) + u0, λ0) W⑴(u0, λ, 2-β(u - v),α - β, λ) du0 + b(1)(λ)
W⑴(2-βu0, λ, 2-β(u - v),α - β, λ) 2-2βdu0 + b(1)
(24)
and
X(1)[Dβ,v X(O)](u, α, λ)
Dβ,vX(O) (u + u0, λ0) W(1) (u0,λ0,u,α,λ)du0+b(1)(λ)
X(O) (2-β(u + u0 - v), λ0) W⑴(u0, λ0, u, α, λ) du0 + b⑴(λ)
(25)
Hence for (17) to hold when l = 1, we need
W⑴(u0, λ, u, α, λ) = W⑴(2-βu0, λ, 2-β(u - v),α - β, λ) 2-2β, ∀ u, α, λ, u0, λ, v, β. (26)
Keeping u, α, λ, u0, λ0, β fixed while changing v in (26), we obtain that W(1) (u0, λ0, u, α, λ) does
not depend on the third variable u. Thus W(1) (u0, λ0, u, α, λ) = W(1) (u0, λ0, 0, α, λ) , ∀u. Define
Wλ(1,λ) (u0) as
Wλ(10,)λ(u0) := W(1) (u0, λ0, 0, 0, λ) .
(27)
12
Under review as a conference paper at ICLR 2020
Then, for any given u0 , λ0 , u, α, λ, setting β = α in (26) leads to
W⑴(u0, λ, u, α, λ) = W⑴(2-αu0, λ, 2-α(u - V) 0, λ) 2-2α
=W⑴(2-αu0, λ, 0, 0, λ) 2-2α = Wλ1,)λ (2-αu0) 2-2α.	(28)
Hence (18) can be written as (5).
For l > 1, a similar argument leads to
W(I) (u0, α', λ, u, α, λ) = W(I) (2-βu0, α', λ, 2-β(u — v),α — β, λ) 2-2β, ∀u, α, λ, u0, α0, λ, v, β.
(29)
Again, keeping u, α, λ, u0 , α0 , λ0, β fixed while changing v in (29) leads us to the conclusion that
W(l) (u0,
α0, λ0, u, α, λ) does not depend on the fourth variable u. Define
Wλ(l0),λ(u0, α0) := W(l) (u0, α0, λ0, 0, 0, λ) .	(30)
After setting β = α in (29), for any given u0 , α0 , λ0, u, α, λ, we have
W(I) (u0, α, λ, u, α, λ) = W(I) (2-αu0, α, λ, 2-α(u - v), 0, λ) 2-2α
=W(I) (2-α u, α, λ, 0, 0, λ) 2-2α = Wλ0,λ (2-αu0) 2-2α.	(31)
This concludes the proof of the Theorem.	□
A.2 Proof of Theorem 2
Proof of Theorem 2. In a regular ST -equivariant CNN, the l-th convolutional layer (6) is computed
as follows:
y(u, α,
X(IT)(U + u0,α + α0, λ)W[0,λ (2-αu0, ɑ0) 2-2αdu0,
(32)
z(u, α, λ, λ0) =	y(u, α, α0, λ, λ0)dα0,	(33)
R
(Ml-I	∖
X z(u, α, λ, λ0) + b(l)(λ)	.	(34)
The spatial convolutions in (32) take 2W2L2NαLαMlMl-1 flops (there are NαLαMlMl-1 convo-
lutions in u, each taking 2W2L2 flops.) The summation over α0 in (33) requires LαNαW2MlMl-1
flops. The summation over λ0, adding the bias, and applying the nonlinear activation in (34) requires
an additional W2NαMl(2 + Ml-1) flops. Thus the total number of floating point computations in a
forward pass through the l-th layer of a regular ST -equivariant CNN is
W 2NαMl(2L2LαMl-1 + LαMl-1 + Ml-1 + 2).	(35)
On the other hand, in an ScDCFNet with separable basis truncation up to KKα leading coefficients,
(6) can be computed via the following steps:
y(u,α,λ0,m)= / X(I-I)(u,α + α0,λ0)夕m(a0)da0	(36)
R
z(u, α, λ0, k, m) =	y(u + u0, α, λ0, m)ψjl ,k (2-αu0)2-2αdu0	(37)
R2
(Ka K Ml-1	∖
XXX
z(u, α, λ0, k, m)a(λl0),λ(k, m) + b(l) (λ)	.	(38)
m=1 k=1 λ0=1
The convolutions in α (36) require 2LαNαW2KαMl-1 flops (there are W2KαMl-1 convolutions
in α, each taking 2LαNα flops.) The spatial convolutions in (37) take 2W2L2NαMl-1KαK flops
(NαMl-1KαK convolutions in u, each taking 2W2L2 flops.) The last step (38) requires an additional
2W2NαMl(1 + KKαMl-1) flops. Hence the total number of floating point computation for an
ScDCFNet is
2W 2Nα(KKαMl-1Ml + Ml + L2Ml-1KαK + LαKαMl-1).	(39)
In particular, when Ml	L2 , Lα, the dominating terms in (35) and (39) are, respectively,
2W2NαMlMl-1L2Lα and 2W2NαMl-1MlKKα. Thus the computational cost in an ScDCFNet
has been reduced to a factor of KKa .	□
13
Under review as a conference paper at ICLR 2020
A.3 Proof of Proposition 1
Before proving Proposition 1, we need the following two lemmas.
Lemma 1. Suppose that {ψk}k are the FB bases, and F (u)
Ek a(k)2-2j ψk (2-j U) is a smooth function on 2j B(0,1) ,then
k a(k)ψj,k (u)
Z |F(u)| du, Z |u| |VF(u)| du, 2j Z |VF(u)| du ≤ π∣∣a∣∣FB = π (^Xμk ∙ a(k)2)	. (40)
This is Lemma 3.5 and Proposition 3.6 in Qiu et al. (2018) after rescaling u. Lemma 1 easily leads to
the following lemma.
Lemma 2. Leta(λl0),λ(k, m) be the coefficients of the filter Wλ(l0),λ(u, α) under the joint bases {ψk}k
and {夕m}m defined in (10), and define W；0)sm(u) as
Wλ0,λ,m(u) := aλ0,λ(k, m)ψjl,k (u).
k
We have
Bλ1)λ, Cλ1)λ, 2j1 Dλ1)λ ≤ ∏ko⅛kFB, Bλl),λ,m, Clλ,m, 2jlDS),λ,m ≤ ∏kaλl),λ(∙,馆)1向,∀l
(41)
> 1,
(42)
where
Bλ(10 ,)λ :
(u) du,
VuWλ1)λ(u)∣du,
λ0,λ,m :
Dλ(l0),λ,m :=	∣∣∣
,λ,m :
(u) du, l > 1,
VuW⅛,m(u)∣du, l> 1,
▽uW、0；m(u)| du, l> 1.
(43)
We thus have
Bl,Cl,2jlDl ≤Al,
(44)
where
and, for l > 1,
B1 :
D1 :
M0
max SUp	Bλ(10 ,)λ ,
λ λ0=1
M0
max SUp	Cλ(10,)λ ,
λ λ0=1
M0
max SUp	Dλ(10),λ ,
λ λ0=1
M0
——SUp
MI λ0
M0
——SUp
MI λ0
M0
,M su0p
M1
X
λ=1
M1
X
λ=1
M1
XDλ(10,)λ	,
λ=1
(45)
{M1-1
SUp X	X Bλ(l0),λ,m,
λ λ0=1 m
2Ml-1	l	(l)
-Bl- B Blm (，	Bl,m := SUp〉,Bλ0,λ,m,
Ml	m 	λ0 λ=1	,,
{M1-1
SUp X X Cλ(l0),λ,m ,
λ λ0=1 m
2MMMr1 X Cl,m ∖ ,	Cl,m := Sup XXX CPλ,m,
Ml	m 	λ0 λ=1
(46)
{M1-1
SUp X X Dλ(l0),λ,m,
λ λ0=1 m
2Ml-1	(l)
-M- D^ Dl,m ,，Dl,m := SUp ʌ, Dλ0,λ,m∙
Ml	m		λ0 λ=1
In particular, (A2) implies that Bl, Cl, 2jl Dl ≤ 1, ∀l.
14
Under review as a conference paper at ICLR 2020
Proofof Proposition 1. To simplify the notation, we omit (l) in Ws)» Ws} m, and b(l), and let
M = Mi, M0 = Mi-1. The proof of (a) for the case l = 1 is similar to Proposition 3.1(a) of QiU
et al. (2018) after noticing the fact that
W ∣W(2-aU)∣2-2αdU = ∖ ∖W(u)∖ dU,
7r2	7r2
and we include it here for completeness. From the definition of Bi in (45), we have
supX Bλ1)λ ≤ Bι, and supX B以 ≤ BIMM∙
λ λ0	λ0 λ
Thus, given two arbitrary functions x1 and x2, we have
X⑴[X2]) (u, a, λ) ∣
(47)
(48)
≤Σ∕
∖0 J
(U + U, T)Wλ0,λ (2-auz) 2-2αdu0 + b(λ)
x2(u + U, λ0)Wλ,,λ (2-ɑu0) 2-2αdu0 + b(λ) ) ∣
λ0
x1(u + U, λ0)Wλ,,λ (2-ɑu0) 2-20du0 — X X x2(u + U, λ))Wλo,λ (2-ɑ'
λ0 J
U) 2-2adU
2
X [(xι — x2)(u + U, T)Wλz,λ (2-0u,) 2-2αdu0
λ0 J
2
∣ Wk,λ(2-α U)∣2-2αdu0
∖(x1 — x2)(u + U,λ0)∖2 ∣ Wλo,λ(2-ɑu')∣ 2-2αdu0
∖(X1 — X2)(U + U,λ0)∖2∣ Wχ,λ(2-αU)I 2-20dU0) (XBλ?J
≤B1 X ∖ ∖(X1 — X2)(U,λ0)∖2∣ Wk,λ(2-α(U — U))∖2-2αdU
λ0 J
(49)
Therefore, for any α,
X
λ
[xi] — X⑴[x2]) (u, α, λ) J du
≤ X B B1 X I ∖(X1 — X2)(U, λ/)∣2∣ Wλz,λ(2-α (U — u))∖2-2αdUdu
λ J λ0 J
du
=B1 X ∖ ∖(xι — X2)(U,λ0)∖2
λ0 J
=B1 X ∖ ∖(xι — X2)(U, λ0)∖2
λ0 J
≤b1 M X / I(XI- x2)(U, λ°)∖2 dU
=B2M∣∣xι — x2∣∣2
≤M∣E — X2∣∣2,
∣Wλ,,λ(2-α (U — u))∣ 2-20du dU
(50)
where the last inequality makes use of the fact that Bi ≤ Ai ≤ 1 under (A2) (Lemma 2.) Therefore
∣∣x ⑴[xi] — X ⑴[x2]∣∣2 = sup f X / ∣(x ⑴[xi] — X ⑴[x2]) (U,α,λ) ∣ dU ≤ ∣∣xι — x2∣∣2∙
(51)
15
Under review as a conference paper at ICLR 2020
This concludes the proof of (a) for the case l = 1. To prove the case for any l > 1, we first recall
from (46) that
sup XX 或 Iλ,m ≤ B,
λ λ0 m
and X Bl,m ≤ Bl 2M ,	where Bl,m = SUP X BS),λ,m
(52)
Thus, for two arbitrary functions χ1 and χ2, we have
2
(xι — x2)(U + u0,α + αf, λz)Wλ∕,λ (2-auz, ɑz) 2-2αda0du0
≤x L L
0 o J R J R
λ0
X / /(X1 — x2)(u + u,α + α, λ0)2-20 X Wv,λ,m (2-auz) φm(α^dɑldu!
2
2
=XXL2Gm(u + u1, α, λ0)2-2αWχ,λ,m(2-αu')du0
dX" ∖Gm(u + UI α, λ0)∖2 ∣ Wχ,λ,m(2-αu')∣ 2-2αdu0
(XXZ」Wλ" (2-au,)∣2-20du)
∖Gm(u, α, λ0)∖2 ∣ Wχ,λ,m(2-α(u — u))∣2-20du
≤Bl XX
∖Gm(u, α, λ0)∣2 IWχ,ι,m(2-α(u — u))∣2-2αdu,
X m Jr2
where
Gm(u, a, λ0) := / (xι — X2)(u, α + α , λ0)夕m(α')da’.
We claim (to be proved later in Lemma 3) that
MIIGmk2 = supX ∖ ∖Gm(u,α, λ)∖2 du ≤ 2M'∣∣x1 — x2∣∣2,	∀m.
α T JR2
(53)
(54)
(55)
Thus, for any α,
X [ |(x(l)[xi] — x")[x2]) (u, α, λ) ∣ du
λ JR2
≤ X Bl XX
∖Gm(u, α, λ0)∖2 ∣ Wv,λ,m(2-α(u — u))∣2-20dudu
λ 7r2	T m 7r2
=Bi XX / ∖Gm(u,α,
λ m 7r2
=Bl XX
∖Gm(u,α,
T m r22
(2-α(u — u))∣ 2-20du du
16
Under review as a conference paper at ICLR 2020
≤Bι XX
∣Gm(U,α, λ7)|2 BImdu
λ0 m R2
=Bl
m
≤Bι ∙ 2M7
∣Gm(U,α,λ7)∣2 du Bl,m
kx1 - x2k2	Bl,m
m
≤B2 ∙ 2M 0kx1 - x2k2 2M7 ≤ MkxI- x2k2.
(56)
Therefore
kx(l)[xl] - x(l)[x2]k2 =SuP MM X / ∣(x(l)[xι ] - x(l)[x2]) (u, α, λ)∣ du ≤ kxι - x2 k2.
α	λ	R2
(57)
To prove (b), we use the method of induction. When l = 0, x(00)(u, λ) = 0 by definition. When l = 1,
x(01)(u, α, λ) = σ(b(1)(λ)). Suppose x(0l-1)(u, α, λ) = x(0l-1)(λ) for some l > 1, then
x0l)(u, α,λ) = σ ( ^X ] X x0l-1)(u + u , α + α , X0)W[0〈(2-αu7, ɑ7) 2-2αdα7du7 + b(l)(λ)
0 R2 R
=σ (X x0l-1)(λ7) J Z Wλ0,λ (2-αu7, α7) 2-2αdα7du7 + 从l)(λ))
σ Xx(0l-1)(λ7)	Wλ(l0)λ (u7, α7) dα7du7 + b(l) (λ)
λ0	R2 R ,
x(0l)(λ).
(58)
Part (c) is an easy corollary of part (a). More specifically, for any l > 1,
kx(cl)k = kx(l) - x(0l)k = kx(l)[x(l-1)] - x(0l)[x(0l-1)]k ≤ kx(l-1) - x(0l-1)k = kx(cl-1)k.	(59)
□
Lemma 3. Suppose 夕 ∈ L2(R) with supp(夕 m) ⊂ [-1,1] and ||夕||二2 = 1, and x is a function of
three variables
x : R2 × R × [M] → R
(u, α, λ) 7→ x(u, α, λ)
with kxk2 := SuPa MM Eλ Jr2 lx(u, α, λ)∣2du. Define G(u, α, λ) as
G(u, α, λ)
x(u, α
R
+ α7, λ)夕(a7)dα.
Then we have
MkGk2
SuP
α
Xλ R2
∣G(u,α,λ)∣2 du ≤ 2Mkxk2.
(60)
(61)
(62)
(63)
Proof of Lemma 3. Notice that, for any α, we have
Xλ	R2
|G(u, α, λ)∣2 du = XZ ∣∣Z x(u, α + α7 ,
∣2
λ)φ(ar)daf du
≤ X Z (Z ∣x(u,α + α0,λ)∣2 dɑ7) k夕kL2du
17
Under review as a conference paper at ICLR 2020
|x(u, α + α0, λ)∣2 du I da0
≤ Z 1 Mkxk2dα0 = 2Mkxk2.
(64)
Thus
sup
α
Xλ R2
|G(u, α,λ)∣2 du ≤ 2M∣∣xk2.
(65)
□
A.4 Proof of Theorem 3
To prove Theorem 3, we need the following two Propositions.
Proposition 2. In an ScDCFNet satisfying (A1) and (A3), we have
1.	For any l ≥ 1,
∣∣x(l)[Dτx(l-1)] - DTX(I)[x(IT)]Il ≤ 4(B1 + Cι)∣Vτ∣∞∣χClT)∣.	(66)
2.	For any l ≥ 1, we have
∣Tβ,vx(l)∣ =2β∣x(l)∣,	(67)
and
∣∣x(l)[Tβ,v ◦ DTX(IT)]- Tβ,vDTX(I)[x(IT)]∣∣ ≤ 2β+2(Bι + Cl)∣Vτ∣∞∣χCj)∣,	(68)
where the first Tβ,v in (68) is replaced by Dβ,v when l = 1.
3.	If (A2) also holds true, then
∣∣X(I)[Dβ,v ◦ DTx(0)] - Tβ,vDTχ(l)[χ(0)]∣∣ ≤ 2β+3l∣Vτ∣∞∣x⑼∣, ∀l ≥ 1.	(69)
Proposition 3. In an ScDCFNet satisfying (A1) and (A3), we have, for any l ≥ 1,
∣∣Tβ,vDtX(I)- Tβ,vX(I)∣∣ ≤ 2β+1∣τ∣∞DlkχCl-1)k ≤ 2β+1∣τ∣∞Dlkx(0)∣.	(70)
If (A2) also holds true, then
∣∣Tβ,vDtX(I)- Tβ,vX(I)∣∣ ≤ 2β+1-jl ∣τ∣∞∣x⑼k.	(71)
Proof of Theorem 3. Putting together (69) and (71), we have
X(L) [Dβ,v ◦ DTX(O)] - Tβ,vX(L) [x(0)]∣∣
≤ X(L) [Dβ,v ◦ DtX(O)] - Tβ,vDτX(L) [x(O)] ∣∣ + ∣∣Tβ,vDτX(L) [x(0)] - Tβ,vX(L) [X(0)]∣∣
≤2β+3L∣Vτ ∣∞∣x(0)k +2β+1-jL ∣τ ∣∞ ∣x(0)k
=2β+1 (4L∣Vτ∣∞ + 2-jL ∣τ∣∞) ∣x(O)∣	(72)
This concludes the proof of Theorem 3.	□
Finally, we need to prove Proposition 2 and Proposition 3, where the following lemma from Qiu et al.
(2018) is useful.
Lemma 4 (Lemma A.1 of QiU et al. (2018)). Suppose that ∣Vτ11∞) < 1/5, P(U) — U T(U), then at
every point u ∈ R2,
∣∣Jρ∣-1∣≤∣Vτ ∣∞(2+ ∣Vτ ∣∞),	(73)
where JP is the Jacobian of P, and | Jρ∣ is the Jacobian determinant. As a result,
I∣jρ∣- 1| ,∣∣jρ-1∣- 1∣ ≤ 4∣Vτ∣∞,	(74)
and,
|JP| , ∣∣JP-1 ∣∣ ≤ 2.	(75)
18
Under review as a conference paper at ICLR 2020
Proof of Proposition 2. Just like Proposition 1(a), the proof of Proposition 2(a) for the case l = 1 is
similar to Lemma 3.2 of Qiu et al. (2018) after the change of variable (47). We thus focus only on
the proof for the case l > 1. To simplify the notation, we denote x(l) [x(l-1)] as y[x], and replace
x(cl-1), W(l), b(l), Ml-1, and Ml, respectively, by xc, W, b, M0, and M. By the definition of the
deformation Dτ (14), we have
x(ρ(u) + u0,α + α, λ0) Wλ0,λ (2-αu0, ɑ0) 2-2αdα0du0 + b(λ)j
(76)
x(ρ(u + u0),α + α0, λ0)Wλ,,λ (2-αu0, ɑ0) 2-2αdα0du0 + b(λ))
(77)
Thus
∣(Dτy[x] - y[Dτχ])(u, α, λ)∣2
≤ ∣∣∣∣X R2 R
∣0RR
x(ρ(u) + u0,α + al, λ0)Wλo,λ (2-αu0, ɑ0) 2-2αdα0du0 + b(λ)
X(P(U + u ), a + a , λ0)Wλo,λ (2-αu0, a0) 2-2αda0du0 + b(λ) j I
2
(x(ρ(u) + u0, a + a, λ0) — x(ρ(u) + u0, a + a0, λ0)) Wχo,λ (2-αu0, a0) 2-2αda0du0
λ0
=^X / I (xc(ρ(u) + u0, a + a0,λ0) — Xc(ρ(u) + u0,a + a0,λ0)) Wλ0,λ (2-αu0,a0) 2-2αda0du0
I λ0 R2 R
=IIIIXXR2 R
(xc(ρ(u) + u0, a + a , λ0) — Xc(ρ(u) + u , a + a , λ0)) ∙
Wλ0,λ,m (2-αu0) ψm (a0) 2-2αda0du0∣2 ,	(78)
where the second equality results from the fact that x(u, a, λ) — xc(u, a, λ) = x0 (λ) depends only
on λ (Proposition 1(b).) Just like the proof of Proposition 1(a), we take the integral of a0 first, and
define
2
Gm (u, a, λ )
:=
R
Xc(u, a + a , λ0)^m(a0)da0.
(79)
Thus
∣(Dτ y[x] — y[Dτ x])(u,a,λ)∣2
≤∣∣∣∣X0 X R2
λ0
∣2
(Gm(P(U) + U0, a, λ0) — Gm (ρ(u + U0), a, λ0)) Wλ,λ,m (2-αu0) 2-2αdu0
m
=∣∣∣XX
Gm(V a, λo)Wλ0,λ,m (2-α(v — P(U))) 2-2αdv
∣ λ0 m R2
—XX	Gm(V a, λ0)Wλo,λ,m (2-α(ρ-1(v) — u)) 2-2α∣Jρ-1(v)∣dv
λ0 m R2
=∣Ei(u, a, λ) + E2(U, a, λ)∣2 ,
where
2
EI(U, a,λ)= X X ∕2 [Wλ,,λ,m (2-α(v — ρ(u))) — Wλ0,λ,m (2-α(ρ-1 (V)-U))]
(80)
(81)
19
Under review as a conference paper at ICLR 2020
2-2αGm(v,α,λ)dv,
E2(U,α,λ) = XX / W∖›,λ,m (2-α(ρT(V)-U)) [1 - I JPT(V)∣] ∙
λ m JR2
2-2α Gm(v,α,λ0)dv.
Therefore
M ∣∣Dτy[x] - y[Dτx]∣2 =SUPX / ∖(Dτy[x] - y[Dτx])(u, α, λ)∖2 du
ɑ λ JR2
≤ sup 'S~^' I ∖E1(u, α, λ) + E2 (u, α, λ)∖2 du
α λ √R2
=MkEI + E2∣∣2
Hence
∣∣Dτy[χ] - y[Dτχ]∣∣ ≤ ∣∣E1 + E2∣∣.
We thus seek to estimate ∣∣E1∣ and ∣∣E2k individually.
To bound ∣E2 ∣, we let
kλ2)λ,m(v, u, α) := Wλ,λ,m (2-α(ρ-1(v) - u)) [1 - ∖Jρ-1(v)∖] 2-2α
Then
E2(u, “N X L Gm(v, α, T)kλ2)λ,m(v, u, α)dv,
and, for any given V and α
/2 ∖ kλ2,λ,m (v,u,α) ∖ du =/2 ∖ Wλ',λ,m (2-α(PT(V)-U)) ∖ ∖ 1 - ∖jρ-1 (V)I ∖ 2-2αdu
=∖1 -∖JρT(V)∖∣∕jWχ,ι,m(u)∖ du
≤4∖Vτ ∖∞B^x,m,
where the last inequality comes from (74). Moreover, for any given u and α,
/ ∖ kλ‰(v,u,α) ∣dv=/ ∖
RR
R2
Wχ,λ,m (2-α(ρ-1(V) - u)) ∖ ∖ 1 - ∖Jρ-1 (v)∖ ∖ 2-2αdV
Wλ,λ,m(V - 2-αu)∖ ∙ ∖∖Jρ(20V)∖ - 1∖ dV
≤4∖Vτ ∖∞Bf,km,
where the last inequality is again because of (74). Thus, for any given α,
(82)
(83)
(84)
(85)
(86)
(87)
(88)
(89)
(90)
2
)dv du
R2
R2
X
m
X / ∖E2(u,α,λ)∖2 du
λ JR2
X L X X L
∞BλZl,m du
20
Under review as a conference paper at ICLR 2020
"|i X X /2
IGm(V,a/2 (X 4|Vt l∞Bλlo),λj dv
∣Gm(v, a, λ)∣2 dv Bι,m
≤16∣Vτ ∣∞Bι]T
m
≤16∣Vτ ∣∞B1 EM 0kGmk2Bl,m
m
(91)
Since ∣∣Gm∣∣2 ≤ 2∣∣xck2 (by Lemma 3), and Pm Blm ≤ 2MBl by definition (46), We thus have
X I ∣E2(u,α,λ)∣2 du ≤ 16∣Vτ∣∞Bl-M7Bl ∙ 2M0∣Xc∣2 = M(4∣Vτ∣∞Bl∣Xck)2, ∀a.
λ R2	2M0
Taking supα on both sides gives us
kE2k≤ 4∣Vτ∣∞BlIlxci∣.
Similarly, to bound ∣E1 ∣, We introduce
kλ0)λ,m(v, u, a) := [Wλ0,λ,m (2-α(v - ρ(u))) - Wλ0,λ,m (2-α(ρ-1 (v) - u))[ 2-2α
Then
E1(u,a,λ)=Xλ0XmR2Gm(v, a, λ7)kλ(10,)λ,m(v,u, a)dv,
and, for any given v and a, We have
ɪ Ikλ1)λ,m(v,u,a)∣du, ɪ ∣kλ0)λ,m(v,u,a)∣dv ≤ 4|Vtl∞Cλl0),λ,m∙
(92)
(93)
(94)
(95)
(96)
The proof of (96) is exactly the same as that of Lemma 3.2 in Qiu et al. (2018) after a change of
variable, and We thus omit the detail. Similar to the procedure We take to bound ∣E2 ∣, (96) leads to
IlEIIl ≤ 4∣Vτ∣∞Clkxck.	(97)
Putting together (86), (93), and (97), We thus have
IIDTy[x] - y[Dτx]k ≤ ∣E1 + E2k ≤ ∣Eιk + kE2k ≤ 4(Bl + Cl)∣Vτ∣∞∣∣xc∣∣.	(98)
This concludes the proof of (a).
To prove (b), given any β ∈ R, and v ∈ R2, We have
kTβ,vx(l)k2 = sup ʒ1- X I' ∣Tβ,vx(l)(u,a,λ)∣ du
α Ml λ R2
=sup IT X ∣ ∣x(l)(2-β(U -v),a - β,λ)∣ du
α Ml λ R2
= SuP	[ ∣x(l)(U, a - β,λ)∣ 22βdu
α Ml λ R2
= 22βIx(l)I2	(99)
Thus (67) holds true. As for (68), We have
∣∣x(l)[Tβ,v ◦ Dτx(lτ)] - Tβ,vDτx(l)[x(l-1)]∣∣
=Tβ,vx(l)[Dτ x(l-1)] -Tβ,vDτx(l)[x(l-1)]
=2β ∣∣∣x(l)[Dτ x(l-1)] -Dτx(l)[x(l-1)]∣∣∣
≤2β+2(Bl + Cl )∣Vτ ∣∞ kxcl-1)k,	(100)
21
Under review as a conference paper at ICLR 2020
where the first equality holds valid because of Theorem 1, and the second equality comes from (67).
To prove (c), for any 0 ≤ j ≤ l, define yj as
yj = x(l) ◦ x(l-1) ◦•••◦ Tβ,v ◦ DTx(j) ◦…。x(0).	(101)
We thus have
l
∣∣χ(l)[Dβ,v ◦ DTX⑼]-Tβ,vDTχ(l)[χ(0)]∣∣ = kyι -yok ≤ X	-yj-ιk
j=1
l
=^X ∣∣x(l) ◦…。Tβ,v ◦ DTX(j) ◦•••◦ X(0) — X(l) ◦…。X(j) ◦ Tβ,v ◦ DTx(j-1) ◦…。x(0) ∣∣
j=1
l
≤ X ∣∣Tβ,v ◦ DTx(j)[x(j-1)] — x(j)[Tβ,v ◦ DTx(jT)] ∣∣
j=1
l
≤ X 2β+2(Bj+ Cj)∣VτUxcj) k
j=1
l
≤ X 2β+2 ∙ 2∣Vτ∣∞kx叫=2β+3l∣VτI∞kx⑼k,	(102)
k=1
where the second inequality is because of Proposition 1(a), the third inequality is due to (68), and the
last inequality holds true because B1,C1 ≤ Al ≤ 1 under (A2) (Lemma 2.) This concludes the proof
of Proposition 2.	□
Proof of Proposition 3. The second inequality in (70) is due to Proposition 1(c). Because of (67),
the first inequality in (70) is equivalent to
DTX(I)- X(I)∣∣ ≤ 2∣T∣∞DlkxclT)k
(103)
Just like Proposition 2(a), the proof of (103) for the case l = 1 is similar to Proposition 3.4 of Qiu
et al. (2018) after the change of variable (47). A similar strategy as that of Proposition 2(a) can
be used to extend the proof to the case l > 1. More specifically, denote x(l-1), x(cl-1), W (l), b(l),
respectively, as x, xc , W, and b to simplify the notation. We have
λ0
≤ X R2 R
0RR
x (ρ(u) + u0
α + α0, λ0) Wλo,λ (2-αu0, α0) 2-2αdu0da0
—Xj Jx (u + u0,α + α0, λ0) Wλo,λ (2-αu0, α0) 2-2αdu0da0
X R2 R xc (ρ(u) + u0, α + α0, λ0)X Wλ0,λ,m (2-αu0) Pm (α0)2-2αdu0dα0
—X j Jxc (u + u0,α + α, λ0) X Wλ0,λ,m (2-αu0)2m (ɑ0)2-2αdu0da0
X X	Gm(v, α, λ0)kλ0,λ,m (v, u, α)du0 ,
(104)
22
Under review as a conference paper at ICLR 2020
where
Gm(u, α, λ0) := X Xc(u, α + α0, λ0)夕m(α0)dα0,
R
kλ0,λ,m(v, u, α) := 2-2α [Wλ0,λ,m (2-α(v - P(U))) - Wλ0,λ,m (2-α(v - u))].
Similar to (96), we have the following bound
/ ∣kλ0,λ,m(v,u,α)∣ du, / ∣kλ0,λ,m(v,u,α)∣ dv ≤ 2∣Vτ∣∞Dλ),λ,m.
(105)
(106)
(107)
Again, the proof of (107) is the same as that of Proposition 3.4 in QiU et al. (2018) after a change of
variable. The rest of the proof follows from a similar argument as in (92) and (93).	□
B Experimental Details in Section 5
B.1	Implementation Details
Discretization and scale channel truncation. In practice, the scale channel S = R needs to be
truncated to a finite interval I ⊂ S such that the layerwise feature map X(l)(u, α, λ) is only computed
for α ∈ I. This scale channel truncation unavoidably destroys the global scaling symmetry (similar
to the fact that truncating an image to a finite spatial support destroys translation symmetry), which
leads to the boundary “leakage” effect after a (joint) convolution in scale. This boundary “leakage”
effect can be alleviated through the following two ways: (1) we can choose a joint convolutional filter
that is compactly supported on a much smaller scale interval Iα 3 0 compared to the truncated scale
interval I, i.e., ∣Ia∣《|I| (a similar idea has been explored in (Worrall & Welling, 2019)); (2) instead
of using a zero-padding (which is typically chosen by default as a spatial padding method before
spatial convolution), a “replicate”-padding in the scale channel (i.e., extending the truncated scale
channel according to Neumann boundary condition) should be implemented before the convolution
in scale. After a scale channel truncation, the layerwise feature maps X(l)(u, α, λ) and the separable
basis functions {ψk(u)}k, {夕m(α)}m, are discretized both in space and scale on a uniform grid. To
avoid the spatial aliasing effect, the number K of the spatial basis functions {ψk(u)}k∈[K] is not
allowed to be too large (typically we set K ≤ 10.)
ST -equivariant average pooling. Given a feature X(l) (u, α, λ), the traditional average-pooling
in u with the same spatial kernel size across α destroys ST -equivariance (4). To remedy this
issue, similar to (Zhang, 2019), we first convolve X(l) with a scale-specific low-pass filter before
downsampling the convolved signal on a coarser spatial grid. Specifically, We have X(l)(u, α, λ)=
RR2X(l)(Nu +u0,ɑ, λ)η(2-ɑu0)2-2αdu0, where X(l) is the feature after pooling, η is a low-pass
filter, e.g., a Gaussian kernel, and N ∈ Z+ is the pooling factor. We refer to this as ST -equivariant
average-pooling.
ST -equivariant batch-normalization. Batch-normalization Ioffe & Szegedy (2015) accelerates
network training by reducing layerwise covariate shift, and it has become an integral part in various
CNN architectures. With an extra scale index α in the feature map X(l) (u, α, λ) of an SDCFNet, we
need to include α in the normalization in order not to destroy ST -equivariance, i.e., a batch of features
{X(nl)(u, α, λ)}nN=1 should be normalized as if it were a collection of 3D data (two dimensions for u,
and one dimension for α.)
B.2	VERIFICATION OF ST -EQUIVARIANCE
The ScDCFNet used in this experiment has two convolutional layers, each of which is composed
of a ST -equivariant convolution (5) or (6), a batch-normalization, and a 2 × 2 ST -equivariant
average-pooling. The expansion coefficients a(λ10),λ(k) and a(λ20),λ(k, m) are sampled from a Gaussian
distribution and truncated to K = 8 and Kα = 3 leading coefficients for u and α respectively.
Similarly, a regular CNN with two convolutional layers and randomly generated 5 × 5 convolutional
kernels is used as a baseline for comparison.
B.3	Multiscale Image Classification
In the experiments on multiscale image classification on the SMNIST and SFashion dataaset, the
network architectures for the ScDCFNet and the regular CNN are shown in Table 2. Stochastic
23
Under review as a conference paper at ICLR 2020
gradient descent (SGD) with decreasing learning rate from 10-2(10-1) to 10-4(10-3) is used to
train all networks without (with) batch-normalization for 160 epochs.
Layer	(Regular) CNN	ScDCFNet
1	c3x3x1xM ReLU ap2x2	sc(9)9x9x1xM ReLU sap2x2
2	c3x3xMx2M ReLU ap2x2	sc(9)9x9xLαxMx2M ReLU sap2x2
3	c3x3x2Mx4M ReLU ap2x2	sc(9)9x9xLαx2Mx4M ReLU sap2x2
4	fc64 ReLU fc10 SOftmax-loss	fc64 ReLU fc10 softmax-loss
Table 2: Network architectures used for the experiments in Section 5.2. cLxLxM’xM: a regular convolutional
layer With M' input channels, M output channels, and LxL spatial kernels. Sc(Na)LXLXM'xM: the first-layer
convolution operation (5) in ScDCFNet, where Nα is the number of the uniform grid points to discretize the
scale interval [-1.6, 0], and LxL is the spatial kernel size on the largest scale α = 0. sc(Nα)LXLXLα XM’XM:
the l-th layer (l > 1) convolution operation (6) in ScDCFNet, Where the extra symbol Lα stands for the filter
size in α. apLXL(sapLXL): the regular (ST -equivariant) LxL average-pooling. fcM: a fully connected layer
With M output channels. Batch-normalization layers are added to each convolutional layer if adopted during
training.
B.4	Image Reconstruction
The netWork architectures for the SDCFNet and regular CNN auto-encoders are shoWn in Table 3.
The filter expansion in the SDCFNet auto-encoder is truncated to K = 8 and Kα = 3. SGD With
decreasing learning rate from 10-2 to 10-4 is used to train both netWorks for 20 epochs.
Layer	Regular auto-encoder	ScDCF auto-encoder
1	c7x7x1x8 ReLU ap2x2	sc(15)13x13x1x4 ReLU sap2x2
2	c7x7x8x16 ReLU ap2x2	sc(15)13x13x3x4x8 ReLU sap2x2
3	fc128 ReLU fc4096 ReLU	fc128 ReLU fc4096 ReLU
4	ct7x7x16x8 ReLU us2x2	ct7x7x16x8 ReLU us2x2
5	ct7x7x8x1 ReLU us2x2	ct7x7x8x1 ReLU us2x2
Table 3: Architectures of the auto-encoders used for the experiment in Section 5.3. The encoded representation
is the output of the second layer. ctLXLXM’XM: transposed-convolutional layers With M’ input channels, M
output channels, and LxL spatial kernels. us2X2: 2x2 spatial upsampling. See the caption of Table 2 for the
definitions of other symbols. Batch-normalization (not shoWn in the table) is used after each convolutional layer.
24