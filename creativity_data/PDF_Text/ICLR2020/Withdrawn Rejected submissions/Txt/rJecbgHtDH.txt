Under review as a conference paper at ICLR 2020

A  BOOLEAN  TASK  ALGEBRA  FOR  REINFORCEMENT

LEARNING

Anonymous authors

Paper under double-blind review

ABSTRACT

We propose a framework for defining a Boolean algebra over the space of tasks.
This allows us to formulate new tasks in terms of the negation, disjunction and
conjunction of a set of base tasks.  We then show that by learning goal-oriented
value functions and restricting the transition dynamics of the tasks, an agent can
solve these new tasks with no further learning. We prove that by composing these
value  functions  in  specific  ways,  we  immediately  recover  the  optimal  policies
for all tasks expressible under the Boolean algebra.  We verify our approach in
two domains—including a high-dimensional video game environment requiring
function approximation—where an agent first learns a set of base skills, and then
composes them to solve a super-exponential number of new tasks.

1    INTRODUCTION

Reinforcement learning (RL) has achieved recent success in a number of difficult, high-dimensional
environments  (Mnih  et  al.,  2015;  Levine  et  al.,  2016;  Lillicrap  et  al.,  2016;  Silver  
et  al.,  2017).
However, these methods generally require millions of samples from the environment to learn op-
timal behaviours,  limiting their real-world applicability.   A major challenge is thus in designing
sample-efficient agents that can transfer their existing knowledge to solve new tasks quickly. This 
is
particularly important for agents in a multitask or lifelong setting, since learning to solve 
complex
tasks from scratch is typically infeasible.

One approach to transfer is composition (Todorov, 2009), which allows an agent to leverage existing
skills to build complex, novel behaviours.  These newly-formed skills can then be used to solve or
speed up learning in a new task.  In this work, we focus on concurrent composition, where existing
base skills are combined to produce new skills (Todorov, 2009; Saxe et al., 2017; Haarnoja et al.,
2018; Van Niekerk et al., 2019; Hunt et al., 2019; Peng et al., 2019).  This differs from other 
forms
of composition, such as options (Sutton et al., 1999) and hierarchical RL (Bacon et al., 2017), 
where
actions and skills are chained in a temporal sequence.

In this work, we define a Boolean algebra over the space of tasks and optimal value functions. This
extends previous composition results to encompass all Boolean operators: conjunction, disjunction,
and negation. We then prove that there exists a homomorphism between the task and value function
algebras.  Given a set of base tasks that have been previously solved by the agent, any new task
written as a Boolean expression can immediately be solved without further learning, resulting in a
zero-shot super-exponential explosion in the agent’s abilities.

We illustrate our approach in a simple domain,  where an agent first learns to reach a number of
rooms, after which it can then optimally solve any task expressible in the Boolean algebra. We then
demonstrate composition in high-dimensional video game environments, where an agent first learns
to collect different objects, and then compose these abilities to solve complex tasks immediately. 
Our
results show that, even when function approximation is required, an agent can leverage its existing
skills to solve new tasks without further learning.

2    PRELIMINARIES

We consider tasks modelled by Markov Decision Processes (MDPs). An MDP is defined by the tuple

(S, A, ρ, r), where (i) S is the state space, (ii) A is the action space, (iii) ρ is a Markov 
transition

1


Under review as a conference paper at ICLR 2020

kernel (s, a)       ρ₍s,ₐ₎ from             to    , and (iv) r is the real-valued reward function 
bounded by
[rMIN, rMAX].  In this work, we focus on stochastic shortest path problems (Bertsekas & Tsitsiklis,
1991), which model tasks in which an agent must reach some goal. We therefore consider the class
of undiscounted MDPs with an absorbing set G ⊆ S.

the expected return obtained under π  starting from state s.¹   The optimal policy π∗ is the policy
that obtains the greatest expected return at each state:  V π∗ (s)  =  V ∗(s)  =  maxπ V π(s) for 
all
s in    .   A related quantity is the Q-value function,  Qπ(s, a),  which defines the expected 
return

obtained by executing a from s, and thereafter following π. Similarly, the optimal Q-value function
is given by Q∗(s, a) = maxπ Qπ(s, a) for all s in     and a in    . Finally, we denote a proper 
policy
to be a policy that is guaranteed to eventually reach the absorbing set     (James & Collins, 2006;
Van Niekerk et al., 2019).  We assume the value functions for improper policies—those that never
reach absorbing states—are unbounded below.

3    BOOLEAN  ALGEBRAS  FOR  TASKS  AND  VALUE  FUNCTIONS

In this section,  we develop the notion of a Boolean task algebra,  allowing us to perform logical
operations—conjunction (   ), disjunction (   ) and negation (   )—over the space of tasks.  We then
show that, having solved a series of base tasks, an agent can use its knowledge to solve tasks ex-
pressible as a Boolean expression over those tasks, without any further learning.

We consider a family of related MDPs M restricted by the following assumptions:

Assumption 1.  For all tasks in a set of tasks      , (i) the tasks share the same state space, 
action
space and transition dynamics, (ii) the transition dynamics are deterministic, (iii) reward 
functions
between tasks differ only on the absorbing set   , and (iv) the set of possible terminal rewards 
consists
of only two values.  That is, for all (g, a) in             , we have that r(g, a)        rØ, r     
      R  with

rØ     r  .  For all non-terminal states, we denote the reward rs,ₐ to emphasise that it is 
constant

across tasks.

Assumption 2.  For all tasks in a set of tasks       which adhere to Assumption 1, the set of 
possible
terminal rewards consists of only two values. That is, for all (g, a) in           , we have that 
r(g, a)
rØ,          r          R with rØ     r  .  For all non-terminal states, we denote the reward rs,ₐ 
to emphasise
that it is constant across tasks.

Assumption 1 is similar to that of Todorov (2007) and identical to Van Niekerk et al. (2019), and
imply that each task can be uniquely specified by its reward function.  Furthermore, we note that
Assumption 2 is only necessary to formally define the Boolean algebra.  Although we have placed
restrictions on the reward functions, the above formulation still allows for a large number of 
tasks to
be represented. Importantly, sparse rewards can be formulated under these restrictions.

3.1    A BOOLEAN ALGEBRA FOR TASKS

An abstract Boolean algebra is a set      equipped with operators    ,   ,     that satisfy the 
Boolean
axioms of (i) idempotence, (ii) commutativity, (iii) associativity, (iv) absorption, (v) 
distributivity,

(vi) identity, and (vii) complements.²  Given the above definitions and the restrictions placed on 
the
set of tasks we consider, we can now define a Boolean algebra over a set of tasks.

Theorem 1.  Let       be a set of tasks. Define        ,     Ø         to be tasks with the 
respective reward
functions


rMU  :  S × A → R

(s, a)         rU ,      if s ∈ G

rs,ₐ,    otherwise.

rMØ  :  S × A → R

(s, a)         rØ,      if s ∈ G

rs,ₐ,    otherwise.

¹Since we consider undiscounted MDPs, we can ensure the value function is bounded by augmenting the
state space with a virtual state ω such that ρ₍s,ₐ₎(ω) = 1 for all (s, a) in            , and r = 0 
after reaching ω.

²We provide a description of these axioms in the Appendix.

2


Under review as a conference paper at ICLR 2020

Then       forms a Boolean algebra with universal bounds      Ø and          when equipped with the
following operators:

¬ :  M → M

M  ›→ (S, A, ρ, r¬M ),  where r¬M :  S × A → R

(s, a) ›→  rMU (s, a) + rMØ (s, a)   − rM (s, a)

∨ :  M × M → M

(M₁, M₂) ›→ (S, A, ρ, rM1 ∨M2 ),  where rM1 ∨M2  :  S × A → R

(s, a) ›→ max{rM1 (s, a), rM2 (s, a)}

∧ :  M × M → M

(M₁, M₂) ›→ (S, A, ρ, rM1 ∧M2 ),  where rM1 ∧M2  :  S × A → R

(s, a) ›→ min{rM1 (s, a), rM2 (s, a)}

Proof.  See Appendix.

Theorem 1 allows us to compose existing tasks together to create new tasks in a principled way.
Figure 1 illustrates the semantics for each of the Boolean operators in a simple environment.

3.2    EXTENDED VALUE FUNCTIONS

The reward and value functions described in Section 2 are insufficient to solve tasks specified by
the Boolean algebra above. We therefore extend these to define goal-oriented versions of the reward
and value function, given by the following two definitions:

Definition 1.  The extended reward function r¯ : S × G × A → R is given by the mapping


(s, g, a)         N            if g /= s ∈ G

r(s, a)    otherwise,

(1)

where N  ≤ min{rMIN, (rMIN − rMAX)D}, and D is the diameter of the MDP (Jaksch et al., 2010).³

To understand why standard value functions are insufficient, consider two tasks that have multiple
different goals, but at least one common goal.  Clearly, there is a meaningful conjunction between
them—namely, achieving the common goal. Now consider an agent that learns standard value func-
tions         for both tasks, and which is then required to solve their conjunction without further 
learning.
Note that this is impossible in general, since the regular value function for each task only 
represents
the value of each state with respect to the nearest goal.   That is,  for all states where the 
nearest
goal for each task is not the common goal, the agent has no information about that common goal.
Conversely, by learning extended value functions, the agent is able to learn the value of achieving
all   goals, and not simply the nearest one.

Because we require that tasks share the same transition dynamics, we also require that the absorbing
set of states is shared. Thus the extended reward function adds the extra constraint that, if the 
agent
enters a terminal state for a different task, it should receive the largest penalty possible. In 
practice,
we can simply set N to be the lowest finite value representable by the data type used for rewards.

Definition 2.  The extended Q-value function Q¯ : S × G × A → R is given by the mapping


(s, g, a) ›→ r¯(s, g, a) +

S

V¯ π¯ (s′, g)ρ₍s,ₐ₎(ds′),                                    (2)

where V¯ π¯ (s, g)  =  Eπ¯ [    ∞t=0 r¯(st, g, at)].  The extended Q-value function is similar to 
universal
value function approximators (UVFAs) (Schaul et al., 2015), but differs in that it uses the extended
reward function definition. It is also similar to DG functions (Kaelbling, 1993), except here we 
use

task-dependent reward functions, as opposed to measuring distance between states.

The standard reward functions and value functions can be recovered from their extended versions
through the following lemma.

3The diameter is defined as D  =  maxs₌s'      minπ E [T (s′ π, s)],  where T  is the number of 
timesteps
required to first reach s′ from s under π.

3


Under review as a conference paper at ICLR 2020

0.8

0.4

0.0

−0.4


(a) rMLEFT

(b) rMDOWN

(c) rM¬LEFT

(d) Disjunction

(e) Conjunction

(f) Average

−0.8

Figure 1:  Consider two tasks, MLEFT and MDOWN, in which an agent must navigate to the left and
bottom regions of an xy-plane respectively.  From left to right we plot the reward for entering a
region of the state space for the individual tasks, the negation of MLEFT, and the union 
(disjunction)
and intersection (conjunction) of tasks.  For reference,  we also plot the average reward function,
which has been used in previous work to approximate the conjunction operator (Haarnoja et al.,
2018; Hunt et al., 2019; Van Niekerk et al., 2019).  Note that by averaging reward, terminal states
that are not in the intersection are erroneously given rewards.

Lemma 1.  Let rM , r¯M , Q∗M , Q¯∗M  be the reward function,  extended reward function,  optimal Q-
value function, and optimal extended Q-value function for a task M  in M.  Then for all (s, a) in
S × A, we have (i) rM (s, a) = max r¯M (s, g, a), and (ii) QM (s, a) = max Q¯M∗  (s, g, a).


Proof.

(i):

max r¯

g∈G

(s, g, a) = .max{N, rM (s, a)},    if s ∈ G

g∈G


M

g∈G

max rM (s, a),            otherwise.

g∈G

= rM (s, a)                                              (N  ≤ rMIN ≤ rM (s, a) by definition).

(ii):  Each g in G can be thought of as defining an MDP Mg := (S, A, ρ, rMg ) with reward function

rMg (s, a) := r¯M (s, g, a) and optimal Q-value function Q∗M  (s, a) = Q¯∗M (s, g, a). Then using

(i) we have rM (s, a)  =  max rMg (s, a) and from Van Niekerk et al. (2019, Corollary 1), we

g∈G

have that Q∗M (s, a) = max Q∗M  (s, a) = max Q¯∗M (s, g, a).

g∈G                            g∈G

In the same way, we can also recover the optimal policy from these extended value functions by first
applying Lemma 1, and acting greedily with respect to the resulting value function.

Lemma 2.  Denote S− = S \ G as the non-terminal states of M. Let M₁, M₂ ∈ M, and let each g

in G define MDPs M₁,g and M₂,g with reward functions

rM1,g   := r¯M1 (s, g, a) and rM2,g   := r¯M2 (s, g, a) for all (s, a) in S × A.

Then for all g in G and s in S−,

πg∗(s) ∈ arg max Q∗M1,g (s, a) iff πg∗(s) ∈ arg max Q∗M2,g (s, a).

a∈A                                                    a∈A

Proof.  See Appendix.

Combining  Lemmas  1  and  2,  we  can  extract  the  greedy  action  from  the  extended  value  
func-

tion  by  first  maximising  over  goals,   and  then  selecting  the  maximising  action:    π∗(s)
arg maxa       maxg    Q¯∗(s, g, a). If we consider the extended value function to be a set of 
standard
value functions (one for each goal),  then this is equivalent to first performing generalised 
policy

improvement (Barreto et al., 2017), and then selecting the greedy action.

Finally, much like the regular definition of value functions, the extended Q-value function can be
written as the sum of rewards received by the agent until first encountering a terminal state.

4


Under review as a conference paper at ICLR 2020

Corollary 1.  Denote G∗s:g,a  as the sum of rewards starting from s and taking action a up until,
but not including, g.  Then let M  ∈ M and Q¯∗M  be the extended Q-value function.  Then for all
s ∈ S, g ∈ G, a ∈ A, there exists a G∗s:g,a  ∈ R such that

Q¯∗M (s, g, a) = G∗s:g,a + r¯M (s′, g, a′),  where s′       and a′ = arg max r¯M (s′, g, b).

b∈A

Proof.  This follows directly from Lemma 2. Since all tasks M  ∈∗ M share the same optimal policy


π∗ up to (but not including) the goal state g ∈ G, their return Gπg

= ΣT −1 r

(s , π∗(s )) is the

3.3    A BOOLEAN ALGEBRA FOR VALUE FUNCTIONS

In the same manner we constructed a Boolean algebra over a set of tasks, we can also do so for a set
of optimal extended Q-value functions for the corresponding tasks.

Theorem  2.  Let  Q¯∗  be  the  set  of  optimal  extended  Q¯-value  functions  for  tasks  in  M. 
  Define

Q¯∗Ø, Q¯∗   ∈  Q¯∗  to be the optimal Q¯-functions for the tasks MØ, MU  ∈  M.   Then Q¯∗  forms a

BooleaUn algebra when equipped with the following operators:

¬ :  Q¯∗ → Q¯∗

Q¯∗ ›→ ¬Q¯∗,  where ¬Q¯∗ :  S × G × A → R

(s, g, a) ›→ .Q¯∗ (s, g, a) + Q¯∗ (s, g, a)Σ − Q¯∗(s, g, a)

U                        Ø

∨ :  Q¯∗ × Q¯∗ → Q¯∗

(Q¯∗1, Q¯∗2) ›→ Q¯∗1 ∨ Q¯∗2,  where Q¯∗1 ∨ Q¯∗2  :  S × G × A → R

(s, g, a) ›→ max{Q¯∗1(s, g, a), Q¯∗2(s, g, a)}

∧ :  Q¯∗ × Q¯∗ → Q¯∗

(Q¯∗1, Q¯∗2) ›→ Q¯∗1 ∧ Q¯∗2,  where Q¯∗1 ∧ Q¯∗2  :  S × G × A → R

(s, g, a) ›→ min{Q¯∗1(s, g, a), Q¯∗2(s, g, a)}

Proof.  See Appendix.

3.4    BETWEEN TASK AND VALUE FUNCTION ALGEBRAS

Having established a Boolean algebra over tasks and extended value function, we finally show that
there exists an equivalence between the two.  As a result, if we can write down a task under the
Boolean algebra, we can immediately write down the optimal value function for the task.

Theorem 3.  Let F  : M → Q¯∗  be any map from M to Q¯∗  such that F(M ) = Q¯∗M  for all M  in

M. Then F is a homomorphism.

Proof.  See Appendix.

4    ZERO-SHOT  TRANSFER  THROUGH  COMPOSITION

We  can  use  the  theory  developed  in  the  previous  sections  to  perform  zero-shot  transfer 
 by  first
learning extended value functions for a set of base tasks, and then composing them to solve new
tasks expressible under the Boolean algebra. To demonstrate this, we conduct a series of experiments
in a Four Rooms domain (Sutton et al., 1999), where an agent must navigate in a grid world to a
particular location.  The agent can move in any of the four cardinal directions at each timestep, 
but
colliding with a wall leaves the agent in the same location. The transition dynamics are 
deterministic,

and rewards are −0.1 for all non-terminal states, and 1 at the goal.

5


Under review as a conference paper at ICLR 2020

4.1    LEARNING BASE TASKS

We use a modified version of Q-learning (Watkins, 1989) to learn extended Q-value functions de-
scribed previously.  Our algorithm differs in a number of ways from standard Q-learning:  we keep
track of the set of terminating states seen so far, and at each timestep we update the extended 
Q-value
function with respect to both the current state and action, as well as all goals encountered so 
far. We
also use the definition of the extended reward function, and so if the agent encounters a terminal
state of a different task, it receives reward N . The full pseudocode is listed in the Appendix.

If we know the set of goals (and hence potential base tasks) upfront, then it is easy to select a 
minimal
set of base tasks that can be composed to produce the largest number of composite tasks.  We first
assign a Boolean label to each goal in a table, and then use the columns of the table as base tasks.
The goals for each base task are then those goals with value 1 according to the table. In this 
domain,
the two base tasks we select are MT, which requires that the agent visit either of the top two 
rooms,
and ML, which requires visiting the two left rooms.  We illustrate this selection procedure in the
Appendix.

4.2    BOOLEAN COMPOSITION

Having learned the optimal extended value functions for our base tasks, we can now leverage The-
orems 1–3 to solve new tasks with no further learning. Figure 2 illustrates this composition, where
an agent is able to immediately solve complex tasks such as exclusive-or.  We illustrate a few com-
posite tasks here, but note that in general, if we have K base tasks, then a Boolean algebra allows
for 2²K   new tasks to be constructed.  Thus having trained on only two tasks, our agent has enough
information to solve a total of 16 composite tasks.

(a) ML                          (b) MT                  (c) ML  ∨ MT           (d) ML  ∧ MT         
  (e) ML  V MT            (f) ML  −∨ MT

Figure 2: An example of zero-shot Boolean algebraic composition using the learned extended value
functions.   Arrows represent the optimal action in a given state.   (a–b) The learned optimal goal
oriented value functions for the base tasks.  (c) Zero-shot disjunctive composition.  (d) Zero-shot
conjunctive composition.  (e) Combining operators to model exclusive-or composition.  (f) Compo-
sition that produces logical nor. Note that the resulting optimal value function can attain a goal 
not
explicitly represented by the base tasks.

By learning extended value functions, an agent can subsequently solve a massive number of tasks;
however, the upfront cost of learning is likely to be higher.  We investigate the trade-off between
the two approaches by investigating how the sample complexity scales with the number of tasks.
We compare to Van Niekerk et al. (2019), who used regular value functions to demonstrate optimal
disjunctive composition. We note that while the upfront learning cost is therefore lower, the number
of tasks expressible using only disjunction is 2K   1, which is significantly less than the full 
Boolean
algebra. We also test using an extended version of the Four Rooms domain, where additional goals
are placed along the sides of all walls, resulting in a total of 40 goals. Empirical results are 
illustrated
by Figure 3.

Our results show that while additional samples are needed to learn an extended value function, the
agent is able to expand the tasks it can solve super-exponentially. Furthermore, the number of base
tasks we need to solve is only logarithmic in the number of goal states. For an environment with K
goals, we need to learn only   log₂ K   + 1 base tasks, as opposed to the disjunctive approach which
requires K  base tasks.  Thus by sacrificing sample efficiency initially, we achieve an exponential
increase in abilities compared to previous work (Van Niekerk et al., 2019).

6


Under review as a conference paper at ICLR 2020


×10⁵

5                 Q¯

Q

4

3

2

1018

1015

1012

10⁹

10⁶

Boolean task algebra

           Disjunction only
No transfer

1.25

1.00

0.75

0.50

×10⁶

Boolean  task  algebra
Disjunction  only


1                                                                                                   
             10³

0.25


0

0        2        4        6

8       10      12

14      16

10⁰

2                4                6                8               10

0.00

0              10             20             30             40             50


Number of tasks

(a)  Cumulative  number  of  sam-
ples required to learn optimal ex-
tended  and  regular  value  func-
tions.  Error bars represent stan-
dard deviations over 100 runs.

Number of learned tasks

(b) Number of tasks that can be
solved as a function of the num-
ber of existing tasks solved.  Re-
sults are plotted on a log-scale.

Number of tasks

(c)  Cumulative  number  of  sam-
ples required to solve tasks in a
40-goal Four Rooms domain. Er-
ror bars represent standard devia-
tions over 100 runs.

Figure 3: Results in comparison to the disjunctive composition of Van Niekerk et al. (2019). (a) The
number of samples required to learn the extended value function is greater than learning a standard
value function.  However, both scale linearly and differ only by a constant factor.  (b) The 
extended
value functions allow us to solve exponentially more tasks than the disjunctive approach without
further learning.   (c) In the modified task with 40 goals,  we need to learn only 7 base tasks,  as
opposed to 40 for the disjunctive case.

5    COMPOSITION  WITH  FUNCTION  APPROXIMATION

Finally, we demonstrate that our compositional approach can also be used to tackle high-dimensional
domains where function approximation is required.  We use the same video game environment as
Van Niekerk et al. (2019), where an agent must navigate a 2D world and collect objects of different
shapes and colours. The state space is an 84    84 RGB image, and the agent is able to move in any 
of
the four cardinal directions. The agent also possesses a pick-up action, which allows it to collect
an object when standing on top of it.  There are two shapes (squares and circles) and three colours
(blue, beige and purple) for a total of six unique objects. The position of the agent is randomised 
at
the start of each episode.

We  modify  deep  Q-learning  (Mnih  et  al.,  2015)  to  learn  extended  action-value  
functions.⁴   Our
approach differs in that the network takes a goal state as additional input (again specified as an 
RGB
image). Additionally, when a terminal state is encountered, it is added to the collection of goals 
seen
so far, and when learning updates occur, these goals are sampled randomly from a replay buffer. We
first learn to solve two base tasks: collecting blue objects, and collecting squares, which can 
then be
composed to solve new tasks immediately.

We demonstrate composition characterised by (i) disjunction, (ii) conjunction and (iii) exclusive-
or.  This corresponds to tasks where the target items are:  (i) blue or square, (ii) blue squares, 
and

(iii) blue or squares, but not blue squares.  Figure 4 illustrates sample trajectories, as well as 
the
subsequent composed value functions, for the respective tasks.

6    RELATED  WORK

The  ability  to  compose  value  functions  was  first  demonstrated  using  the  
linearly-solvable  MDP
framework (Todorov, 2007), where value functions could be composed to solve tasks similar to the
disjunctive case (Todorov, 2009). Van Niekerk et al. (2019) show that the same kind of composition
can be achieved using entropy-regularised RL (Fox et al., 2016), and extend the results to the stan-
dard RL setting, where agents can optimally solve the disjunctive case.  Using entropy-regularised
RL, Haarnoja et al. (2018) approximates the conjunction of tasks by averaging their reward func-
tions, and demonstrates that by averaging the optimal value functions of the respective tasks, the
agent can achieve performance close to optimal. Hunt et al. (2019) extends this result by composing
value functions to solve the average reward task exactly, which approximates the true conjunctive
case. More recently, Peng et al. (2019) introduce a few-shot learning approach to compose policies

⁴The hyperparameters and network architecture are listed in the Appendix

7


Under review as a conference paper at ICLR 2020


(a)   Trajectories   for   disjunctive
composition.

(b)  Trajectories  for  conjunctive
composition.

(c)  Trajectories  for  exclusive-or
composition.


(d) Value function for disjunctive
composition.

(e)  Value  function  for  conjunc-
tive composition.

(f) Value function for exclusive-
or composition.

Figure 4: By composing extended value functions from the base tasks (collecting blue objects, and
collecting squares), we can act optimally in new tasks with no further learning. To generate the 
value
functions, we place the agent at every location and compute the maximum output of the network over
all goals and actions. We then interpolate between the points to smooth the graph. Any error in the
visualisation is due to the use of non-linear function approximation.

multiplicatively.  Although lacking theoretical foundations, results show that an agent can learn a
weighted composition of existing base skills to solve a new complex task. By contrast, we show that
zero-shot optimal composition can be achieved for all Boolean operators.

7    CONCLUSION

We have shown how to compose tasks using the standard Boolean algebra operators.  These com-
posite  tasks  can  be  immediately  solved  by  first  learning  goal-oriented  value  functions,  
and  then
composing them in a similar manner.  Finally, we note that there is much room for improvement
in learning the extended value functions for the base tasks.  In our experiments, we learned each
extended value function from scratch, but it is likely that having learned one for the first task, 
we
could use it to initialise the extended value function for the second task to improve convergence
times.  One area for improvement lies in efficiently learning the extended value functions, as well
as developing better algorithms for solving tasks with sparse rewards.  For example, it is likely 
that
approaches such as hindsight experience replay (Andrychowicz et al., 2017) could reduce the num-
ber of samples required to learn extended value functions, while Mirowski et al. (2017) provides a
method for learning complex tasks with sparse rewards using auxiliary tasks. We leave incorporating
these approaches to future work. Our proposed approach is a step towards both interpretable RL—
since both the tasks and optimal value functions can be specified using Boolean operators—and the
ultimate goal of lifelong learning agents, which are able to solve combinatorially many tasks in a
sample-efficient manner.

8


Under review as a conference paper at ICLR 2020

REFERENCES

M. Andrychowicz,  F. Wolski,  A. Ray,  J. Schneider,  R. Fong,  P. Welinder,  B. McGrew,  J. Tobin,

P. Abbeel, and W. Zaremba.   Hindsight experience replay.   In Advances in Neural Information
Processing Systems, pp. 5048–5058, 2017.

P. Bacon, J. Harb, and D. Precup.  The option-critic architecture.  In Thirty-First AAAI Conference
on Artificial Intelligence, 2017.

A. Barreto, W. Dabney, R. Munos, J. Hunt, T. Schaul, H. van Hasselt, and D. Silver.   Successor
features for transfer in reinforcement learning.   In Advances in neural information processing
systems, pp. 4055–4065, 2017.

D.P. Bertsekas and J.N. Tsitsiklis. An analysis of stochastic shortest path problems. Mathematics of
Operations Research, 16(3):580–595, 1991.

R. Fox, A. Pakman, and N. Tishby.  Taming the noise in reinforcement learning via soft updates.  In

32nd Conference on Uncertainty in Artificial Intelligence, 2016.

T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. Composable deep reinforcement
learning for robotic manipulation. arXiv preprint arXiv:1803.06773, 2018.

J.  Hunt,  A.  Barreto,  T.  Lillicrap,  and  N.  Heess.   Composing  entropic  policies  using  
divergence
correction. In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 2911–2920. PMLR, 09–15 Jun 2019.

T. Jaksch, R. Ortner, and P. Auer.  Near-optimal regret bounds for reinforcement learning.  Journal
of Machine Learning Research, 11(Apr):1563–1600, 2010.

H.W. James and E.J. Collins. An analysis of transient Markov decision processes. Journal of applied
probability, 43(3):603–621, 2006.

Leslie Pack Kaelbling.  Learning to achieve goals.  In International Joint Conferences on Artificial
Intelligence, pp. 1094–1099, 1993.

S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The
Journal of Machine Learning Research, 17(1):1334–1373, 2016.

T.. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. 
Continuous
control with deep reinforcement learning.  In International Conference on Learning Representa-
tions, 2016.

P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M. Denil, R. Goroshin, L. 
Sifre,

K. Kavukcuoglu, et al.  Learning to navigate in complex environments.  In International Confer-
ence on Learning Representations, 2017.

V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller,

A.  Fidjeland,  G.  Ostrovski,  et  al.   Human-level  control  through  deep  reinforcement  
learning.

Nature, 518(7540):529, 2015.

X. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine.  MCP: Learning composable hierarchical
control with multiplicative compositional policies. arXiv preprint arXiv:1905.09808, 2019.

A.M. Saxe, A.C. Earle, and B.S. Rosman.  Hierarchy through composition with multitask LMDPs.

Proceedings of the 34th International Conference on Machine Learning, 70:3017–3026, 2017.

T. Schaul, D. Horgan, K. Gregor, and D. Silver.  Universal value function approximators.  In Pro-
ceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings
of Machine Learning Research, pp. 1312–1320, Lille, France, 07–09 Jul 2015. PMLR.

D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,

M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):
354, 2017.

9


Under review as a conference paper at ICLR 2020

R. Sutton, D. Precup, and S. Singh.  Between MDPs and semi-MDPs:  A framework for temporal
abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211, 1999.

E. Todorov.  Linearly-solvable Markov decision problems.  In Advances in Neural Information Pro-
cessing Systems, pp. 1369–1376, 2007.

E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information Process-
ing Systems, pp. 1856–1864, 2009.

B. Van Niekerk, S. James, A. Earle, and B. Rosman.  Composing value functions in reinforcement
learning.  In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 6401–6409. PMLR, 09–15 Jun 2019.

C. Watkins. Learning from delayed rewards. PhD thesis, King’s College, Cambridge, 1989.

10


Under review as a conference paper at ICLR 2020

A    APPENDIX

A.1    BOOLEAN ALGEBRA DEFINITION

Definition 3.  A Boolean algebra is a set B equipped with the binary operators ∨ (disjunction) and

∧ (conjunction), and the unary operator ¬ (negation), which satisfies the following Boolean algebra
axioms for a, b, c in B:

(i)  Idempotence: a ∧ a = a ∨ a = a.

(ii)  Commutativity: a ∧ b = b ∧ a and a ∨ b = b ∨ a.

(iii)  Associativity: a ∧ (b ∧ c) = (a ∧ b) ∧ c and a ∧ (b ∨ c) = (a ∨ b) ∨ c.

(iv)  Absorption: a ∧ (a ∨ b) = a ∨ (a ∧ b) = a.

(v)  Distributivity: a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c) and a ∨ (b ∧ c) = (a ∨ b) ∧ (a ∨ c).

(vi)  Identity: there exists 0, 1 in B such that

0 ∧ a = 0
0 ∨ a = a
1 ∧ a = a
1 ∨ a = 1

(vii)  Complements:  for every a in B, there exists an element a′ in B such that a ∧ a′ = 0 and

a ∨ a′ = 1.

A.2    PROOF FOR THEOREM 1

Theorem 1.  Let       be a set of tasks. Define        ,     Ø         to be tasks with the 
respective reward
functions


rMU  :  S × A → R

(s, a)         rU ,      if s ∈ G

rs,ₐ,    otherwise.

rMØ  :  S × A → R

(s, a)         rØ,      if s ∈ G

rs,ₐ,    otherwise.

Then       forms a Boolean algebra with universal bounds      Ø and          when equipped with the
following operators:

¬ :  M → M

M  ›→ (S, A, ρ, r¬M ),  where r¬M :  S × A → R

(s, a) ›→  rMU (s, a) + rMØ (s, a)   − rM (s, a)

∨ :  M × M → M

(M₁, M₂) ›→ (S, A, ρ, rM1 ∨M2 ),  where rM1 ∨M2  :  S × A → R

(s, a) ›→ max{rM1 (s, a), rM2 (s, a)}

∧ :  M × M → M

(M₁, M₂) ›→ (S, A, ρ, rM1 ∧M2 ),  where rM1 ∧M2  :  S × A → R

(s, a) ›→ min{rM1 (s, a), rM2 (s, a)}

Proof.  Let M₁, M₂ ∈ M. We show that ¬, ∨, ∧ satisfy the Boolean properties (i) – (vii).

(i)–(v):  These easily follow from the fact that the min and max functions satisfy the idempotent,
commutative, associative, absorption and distributive laws.

11


Under review as a conference paper at ICLR 2020

(vi):  Let rMU ∧M1   and rM1   be the reward functions for MU ∧ M₁ and M₁ respectively.  Then
for all (s, a) in S × A,


rMU

∧M1

(s, a) =    min{rU , rM1 (s, a)},    if s ∈ G

min{rs,ₐ, rs,ₐ},           otherwise.

=    rM1 (s, a),    if s ∈ G

rs,ₐ,             otherwise.

= rM1 (s, a).

(rM1

(s, a) ∈ {rØ

, rU

} for s ∈ G)

Thus MU ∧ M₁ = M₁. Similarly MU ∨ M₁ = MU , MØ ∧ M₁ = MØ, and MØ ∨ M₁ =

M₁ . Hence MØ and MU are the universal bounds of M.

(vii):  Let rM1 ∧¬M1  be the reward function for M₁ ∧ ¬M₁. Then for all (s, a) in S × A,


rM1 ∧¬M1

(s, a) =    min{rM1 (s, a), (rU + rØ) − rM1 (s, a)},    if s ∈ G

min{rs,ₐ, (rs,ₐ + rs,ₐ) − rs,ₐ},                   otherwise.

rØ,      if s ∈ G and rM1 (s, a) = rU

=     rØ,      if s ∈ G and rM1 (s, a) = rØ

rs,ₐ,    otherwise.

= rMØ (s, a).

Thus M₁ ∧ ¬M₁ = MØ, and similarly M₁ ∨ ¬M₁ = MU .

A.3    PROOF FOR LEMMA 2

Lemma 2.  Denote S− = S \ G as the non-terminal states of M. Let M₁, M₂ ∈ M, and let each g

in G define MDPs M₁,g and M₂,g with reward functions

rM1,g   := r¯M1 (s, g, a) and rM2,g   := r¯M2 (s, g, a) for all (s, a) in S × A.

Then for all g in G and s in S−,

πg∗(s) ∈ arg max Q∗M1,g (s, a) iff πg∗(s) ∈ arg max Q∗M2,g (s, a).

a∈A                                                    a∈A

Proof.  Let g ∈ G, s ∈ S− and let πg∗ be defined by

πg∗(s′)     arg max Q∗M1 ,g (s, a) for all s′      .

a∈A

If g is unreachable from s, then we are done since for all (s′, a) in S × A we have


g /= s′  =⇒  r

M1,g

(s′, a) =    N,         if s′ ∈ G

rs',a,     otherwise

= rM2,g

(s′, a)

=⇒  M1,g  = M2,g .

If g is reachable from s, then we show that following πg∗  must reach g.  Since πg∗  is proper, it 
must
reach a terminal state g′ ∈ G. Assume g′ /= g. Let πg be a policy that produces the shortest 
trajectory

12


Under review as a conference paper at ICLR 2020

to g. Let Gπg∗  and Gπg  be the returns for the respective policies. Then,

Gπg∗  ≥ Gπg


πg∗

T −1

+ rM1,g

(g′, πg∗(g′)) ≥ Gπg ,


where Gπg∗

T −1

=        r

(s , π∗(s )) and T  is the time at which g′ is reached.


πg∗
T −1

+ N  ≥ Gπg ,  since g /= g′ ∈ G


=⇒  N  ≥ Gπg  − Gπg∗

=⇒  (rMIN − rMAX)D ≥ Gπg  − Gπg∗

,  by definition of N


πg∗
T −1

πg∗

T −1

πg∗

T −1

− rMAXD ≥ Gπg  − rMIND,  since Gπg   ≥ rMIND

− rMAXD ≥ 0

≥ rMAXD.

But this is a contradiction since the result obtained by following an optimal trajectory up to a 
terminal
state without the reward for entering the terminal state must be strictly less that receiving rMAX 
for
every step of the longest possible optimal trajectory.  Hence we must have g′ =  g.  Similarly, all

optimal policies of M₂,g must reach g. Hence πg∗(s) ∈ arg max Q∗M2,g (s, a). Since M₁ and M₂ are


arbitrary elements of M

a

, the reverse implication holds too.

A.4    PROOF FOR THEOREM 2

Theorem  2.  Let  Q¯∗  be  the  set  of  optimal  extended  Q¯-value  functions  for  tasks  in  M. 
  Define

Q¯∗Ø, Q¯∗   ∈  Q¯∗  to be the optimal Q¯-functions for the tasks MØ, MU  ∈  M.   Then Q¯∗  forms a

BooleaUn algebra when equipped with the following operators:

¬ :  Q¯∗ → Q¯∗

Q¯∗ ›→ ¬Q¯∗,  where ¬Q¯∗ :  S × G × A → R

(s, g, a) ›→ .Q¯∗ (s, g, a) + Q¯∗ (s, g, a)Σ − Q¯∗(s, g, a)

U                        Ø

∨ :  Q¯∗ × Q¯∗ → Q¯∗

(Q¯∗1, Q¯∗2) ›→ Q¯∗1 ∨ Q¯∗2,  where Q¯∗1 ∨ Q¯∗2  :  S × G × A → R

(s, g, a) ›→ max{Q¯∗1(s, g, a), Q¯∗2(s, a)}

∧ :  Q¯∗ × Q¯∗ → Q¯∗

(Q¯∗1, Q¯∗2) ›→ Q¯∗1 ∧ Q¯∗2,  where Q¯∗1 ∧ Q¯∗2  :  S × G × A → R

(s, g, a) ›→ min{Q¯∗1(s, g, a), Q¯∗2(s, a)}

Proof.  Let Q¯∗M  , Q¯∗M   ∈ Q¯∗ be the optimal Q¯-value functions for tasks M₁, M₂ ∈ M with reward

functions rM1  and rM2 . We show that ¬, ∨, ∧ satisfy the Boolean properties (i) – (vii).

(i)–(v):  These follow directly from the properties of the min and max functions.

13


Under review as a conference paper at ICLR 2020

(vi):  For all (s, g, a) in S × G × A,

(Q¯∗  ∧ Q¯∗  1 )(s, g, a) = min{(Q¯∗ (s, g, a), Q¯∗  1 (s, g, a)}

= min{G∗s:g,a + r¯MU (s′, g, a′), G∗s:g,a + r¯M1 (s′, g, a′)}   (Corollary 1)

= G∗s:g,a + min{r¯MU (s′, g, a′), r¯M1 (s′, g, a′)}

= G∗s:g,a + r¯M1 (s′, g, a′)            (since r¯M1 (s′, g, a′) ∈ {rØ, rU , N })

= Q¯∗M  (s, g, a).


Similarly, Q¯∗  ∨ Q¯∗

= Q¯∗ , Q¯∗Ø ∧ Q¯∗M

= Q¯∗Ø, and Q¯∗Ø ∨ Q¯∗M

= Q¯∗M  .

(vii):  For all (s, g, a) in S × G × A,

(Q¯∗M1  ∧ ¬Q¯∗M1 )(s, g, a) = min{Q¯∗M1 (s, g, a), (Q¯∗ (s, g, a) − Q¯∗Ø(s, g, a)) − Q¯M∗  1 (s, g, 
a)}

= G∗s:g,a + min{r¯M1 (s′, g, a′), (r¯MU (s′, g, a′) + r¯MØ (s′, g, a′))

− r¯M1 (s′, g, a′)}

= G∗s:g,a + r¯MØ (s′, g, a′)

= Q¯∗Ø(s, g, a).


Similarly, Q¯∗M

∨ ¬Q¯∗M

= Q¯∗ .

U

A.5    PROOF FOR THEOREM 3

Theorem 3.  Let F  : M → Q¯∗  be any map from M to Q¯∗  such that F(M ) = Q¯∗M  for all M  in

M. Then F is a homomorphism.

Proof.  Let M₁, M₂ ∈ M. Then for all (s, g, a) in S × G × A,


Q¯∗

¬M

(s, g, a) = G∗s:g,a + r¯¬M1 (s′, g, a′)    (from Corollary 1)

= G∗s:g,a + (r¯MU (s′, g, a′) + r¯MØ (s′, g, a′)) − r¯M1 (s′, g, a′)

U                        Ø                         M1

= ¬Q¯∗M1 (s, g, a)

=⇒  F(¬M₁) = ¬F(M₁)

Q¯∗M1 ∨M2 (s, g, a) = G∗s:g,a + r¯M1 ∨M2 (s′, g, a′)

= G∗s:g,a + max{r¯M1 (s′, g, a′), r¯M2 (s′, g, a′′)}

= max{G∗s:g,a + r¯M1 (s′, g, a′), G∗s:g,a + r¯M2 (s′, g, a′′)}

= max{Q¯∗M1 (s, g, a), Q¯∗M2 (s, g, a)}

= (Q¯∗M1  ∨ Q¯∗M2 )(s, g, a)

=⇒  F(M₁ ∨ M₂) = F(M₁) ∨ F(M₂).

Similarly F(M₁ ∧ M₂) = F(M₁) ∧ F(M₂).

14


Under review as a conference paper at ICLR 2020

A.6    GOAL-ORIENTED Q-LEARNING

Below we list the pseudocode for the modified Q-learning algorithm used in the four-rooms domain.

Algorithm 1: Goal-oriented Q-learning

Input: Learning rate α, discount factor γ, exploration constant ε, lower-bound return N

Initialise Q : S × S × A → R arbitrarily

while Q is not converged do

Initialise state s

while s is not terminal do
if     = Ø then

Select random action a

else

arg max .max Q(s, t, b)Σ    with probability 1 − ε


end

a random action                     with probability ε

Choose a from s according to policy derived from Q

Take action a, observe r and s′

foreach g         do

if s′ is terminal then
if s′ = g then

δ      N

else

δ      r     Q(s, g, a)

end
else

δ      r + γ maxb Q(s′, g, b)     Q(s, g, a)

end

Q(s, g, a)      Q(s, g, a) + αδ

end

s      s′

end

endG ← G ∪ {s}

return Q

Figure 5: A Q-learning algorithm for learning extended value functions. Note that the greedy action
selection step is equivalent to generalised policy improvement (Barreto et al., 2017) over the set 
of
extended value functions.

15


Under review as a conference paper at ICLR 2020

A.7    INVESTIGATING PRACTICAL CONSIDERATIONS

The  theoretical  results  presented  in  this  work  rely  on  Assumptions  1  and   2,  which  
restrict  the
tasks’ transition dynamics and reward functions in potentially problematic ways.  Although this is
necessary to prove that Boolean algebraic composition results in optimal value functions,  in this
section we investigate whether these can be practically ignored.  In particular, we investigate two
restrictions: the requirement that tasks share the same terminal states, and the impact of using 
dense
rewards.

A.7.1    FOUR ROOMS EXPERIMENTS

We use the same setup as the experiment outlined in Section 4, but modify it in two ways. We first
investigate the difference between using sparse and dense rewards.  Our sparse reward function is
defined as

r       (s, a, s′) =    20     if s′ ∈ G

−1    otherwise,

and we use a dense reward function similar to Peng et al. (2019):


rdₑnsₑ(s, a, s′) =

  1 

|G| g∈G

exp(

|s′ − g|2

4

) + rspₐrsₑ(s, a, s′)

Using this dense reward function, we again learn to solve the two base task MT (reaching the centre
of the top two rooms) and ML (reaching the centre of the left two rooms).  We then compose them
to solve a variety of tasks, with the resulting value functions illustrated by Figure 6.

(a) ML                          (b) MT                  (c) ML  ∨ MT           (d) ML  ∧ MT         
  (e) ML  V MT            (f) ML  −∨ MT

Figure 6: An example of Boolean algebraic composition using the learned extended value functions
with dense rewards. Arrows represent the optimal action in a given state. (a–b) The learned optimal
goal oriented value functions for the base tasks with dense rewards. (c) Disjunctive composition. 
(d)
Conjunctive composition. (e) Combining operators to model exclusive-or composition. (f) Compo-
sition  that produces logical nor. We note that the resulting value functions are very similar to 
those
produced in the sparse reward setting.

We also modify the domain so that tasks need not share the same terminating states (that is, if the
agent enters a terminating state for a different task, the episode does not terminate and the agent 
can
continue as if it were a normal state). This results in four versions of the experiment:

(i)  sparse reward, same absorbing set

(ii)  sparse reward, different absorbing set

(iii)  dense reward, same absorbing set

(iv)  dense reward, different absorbing set

We learn extended value functions for each of the above setups, and then compose them to solve each
of the 2⁴ tasks representable in the Boolean algebra.  We measure each composed value functions
by evaluating its policy in the sparse reward setting, averaging results over 100000 episodes.  The
results are given by Figure 7.

Our  results  indicate  that  extended  value  functions  learned  in  the  theoretically  optimal  
manner
(sparse reward, same absorbing set) are indeed optimal.  However, for the majority
of      the tasks, relaxing the restrictions on terminal states and reward functions results in 
policies that
are either identical or very close to optimal.

16


Under review as a conference paper at ICLR 2020

2.0

1.5

1.0

0.5                                                                                                 
                                                                                                    
                                                                                                    
                                                                                    

Domain

₀.₀                                                                                                 
 Sparse rewards, same absorbing set                                                                 
                            

Dense rewards, same absorbing set
Sparse rewards, different absorbing set
Dense rewards, different absorbing set


0.5

Q*         Q*

Q*         ¬Q*

Q*         ¬Q*

Q*       Q*                            Q*

¬Q*

Q*                                 ¬Q*

Q*         Q*

Q*         ¬Q*

Q*         ¬Q*

Q*       Q*

¬(Q*       Q*   )          Q*       Q*


MT                      ML

MT                                ML

ML                                MT

MT                  ML                                                                    MT

MT                                                                              ML

ML

Tasks

MT                      ML

MT                                ML

ML                                MT

MT                  ML

MT                  ML

MT                  ML

Figure 7: Box plots indicating returns for each of the 16 compositional tasks, and for each of the 
four
variations of the domain. Results are collected over 100000 episodes with random start positions.

A.7.2    FUNCTION APPROXIMATION EXPERIMENTS

In  this  section  we  investigate  whether  we  can  again  loosen  some  of  the  restrictive  
assumptions
when tackling high-dimensional environments. In particular, we run the same experiments as those
presented in Section 5, but modify the domain so that (i) tasks need not share the same absorbing
set,   (ii) the pickup-up action is removed (the agent immediately collects an object when reaching
it), and (iii) the position of every object is randomised at the start of each episode.

We first learn to solve three base tasks:  collecting blue objects, collecting purple objects, and 
col-
lecting squares , which can then be composed to solve new tasks immediately. We then demonstrate
composition characterised by disjunction, conjunction and exclusive-or, with the resulting trajecto-
ries and value functions illustrated by Figure 8.


(a)   Trajectories   for   disjunctive
composition (collect blue or pur-
ple objects).

(b)    Trajectories    for    conjunc-
tive   composition   (collect   blue
squares).

(c)   Trajectories   for   exclusive-
or  composition  (collect  blue  or
square   objects,    but   not   blue
squares).


(d) Value function for disjunctive
composition.

(e)  Value  function  for  conjunc-
tive composition.

(f) Value function for exclusive-
or composition.

Figure 8:  Results for the video game environment with relaxed assumptions.  We generate value
functions to solve the disjunction of blue and purple tasks, and the conjunction and exclusive-or of
blue and square tasks.

17


Under review as a conference paper at ICLR 2020

In summary, we have shown that our compositional approach offers strong empirical performance,
even when the theoretical assumptions are violated. Finally, we expect that, in general, the errors 
due
to these violations will be far outweighed by the errors due to non-linear function approximation.

A.8    SELECTING BASE TASKS

The Four Rooms domain requires the agent to navigate to one of the centres of the rooms in the
environment. Figure 9 illustrates the layout of the environment and the goals the agent must reach.

Figure 9:  The layout of the Four Rooms domain.  The circles indicate goals the agent must reach.
We will refer to the goals as top-left, top-right, bottom-left, and bottom-right.

Since we know the goals upfront, we can select a minimal set of base tasks by assigning each goal a
Boolean number, and then using the columns of the table to select the tasks. To illustrate, we 
assign
Boolean numbers to the goals as follows:

  x₁    x₂    Goals                      
rØ    rØ    bottom-right
rØ    rU    bottom-left
rU    rØ    top-right

rU    rU    top-left     

Table 1: Assigning labels to the individual goals.  The two Boolean variables, x₁ and x₂, represent
the goals for the base tasks the agent will train on.

As there are four goals,  we can represent each uniquely with just two Boolean variables.   Each
column in Table 1 represents a base task, where the set of goals for each task are those goals 
assigned
a value rU .  We thus have two base tasks corresponding to x₁ = {top-right, top-left} and

x₂ = {bottom-left, top-left}.

A.9    DQN ARCHITECTURE AND HYPERPARAMETERS

In our experiments, we used a DQN with the following architecture:

1.  Three convolutional layers:

(a)  Layer 1 has 6 input channels, 32 output channels, a kernel size of 8 and a stride of 4.

(b)  Layer 2 has 32 input channels, 64 output channels, a kernel size of 4 and a stride of 2.

(c)  Layer 3 has 64 input channels, 64 output channels, a kernel size of 3 and a stride of 1.

2.  Two fully-connected linear layers:

(a)  Layer 1 has input size 3136 and output size 512 and uses a ReLU activation function.

(b)  Layer 2 has input size 512 and output size 4 with no activation function.

We used the ADAM optimiser with batch size 32 and a learning rate of 10−⁴.  We trained every 4
timesteps and update the target Q-network every 1000 steps. Finally, we used ϵ-greedy exploration,
annealing ϵ to 0.01 over 100000 timesteps.

18

