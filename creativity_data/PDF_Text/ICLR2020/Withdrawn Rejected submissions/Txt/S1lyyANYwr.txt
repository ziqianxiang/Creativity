Under review as a conference paper at ICLR 2020
Constrained Markov Decision Processes via
Backward Value Functions
Anonymous authors
Paper under double-blind review
Ab stract
Although Reinforcement Learning (RL) algorithms have found tremendous success
in simulated domains, they often cannot directly be applied to physical systems,
especially in cases where there are hard constraints to satisfy (e.g. on safety or
resources). In standard RL, the agent is incentivized to explore any behavior as
long as it maximizes rewards, but in the real world undesired behavior can damage
either the system or the agent in a way that breaks the learning process itself. In
this work, we model the problem of learning with constraints as a Constrained
Markov Decision Process, and provide a new on-policy formulation for solving
it. A key contribution of our approach is to translate cumulative cost constraints
into state-based constraints. Through this, we define a safe policy improvement
method which maximizes returns while ensuring that the constraints are satisfied
at every step. We provide theoretical guarantees under which the agent converges
while ensuring safety over the course of training. We also highlight computational
advantages of this approach. The effectiveness of our approach is demonstrated on
safe navigation tasks and in safety-constrained versions of MuJoCo environments,
with deep neural networks.
1	Introduction
Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior
of learning agents in an interactive setting (Sutton & Barto, 2018). Recently, the field ofRL has found
success in many high-dimensional domains, like video games, Go, robot locomotion and navigation.
However, most of the success of RL algorithms has been limited to simulators, where the learning
algorithm has the ability to reset the simulator. In the physical world, an agent will need to avoid
harmful behavior (e.g. damaging the environment or the agent’s hardware) while learning to explore
behaviors that maximize the reward.
A few popular approaches for avoiding undesired behaviors for high-dimensional systems include
reward-shaping (Moldovan & Abbeel, 2012), reachability-preserving algorithms (Mitchell, 2003;
Eysenbach et al., 2017), state-level surrogate constraint satisfaction algorithms (Dalal et al., 2018),
risk-sensitive algorithms (Tamar et al., 2013; Chow et al., 2015) and apprenticeship learning (Abbeel
& Ng, 2004). There also exists model-based Bayesian approaches that are focused on imposing the
constraints via the dynamics (such as classifying parts of state space as unsafe) and then using model
predictive control to incorporate the constraints in the policy optimization and planning (Turchetta
et al., 2016; Berkenkamp et al., 2017; Wachi et al., 2018; Koller et al., 2018). A natural way to
model safety is via constraint satisfaction. A standard formulation for adding constraints to RL
problems is the Constrained Markov Decision Process (CMDP) framework (Altman, 1999), wherein
the environment is extended to also provide feedback on constraint costs. The agent must then attempt
to maximize its expected return while also satisfying cumulative constraints.
A few algorithms have been proposed to solve CMDPs for high-dimensional domains with continuous
action spaces - however they come with their own caveats. Reward Constrained Policy Optimization
(Tessler et al., 2018) and Primal Dual Policy Optimization (Chow et al., 2015) do not guarantee
constraint satisfaction during the learning procedure, only on the final policy. Constrained Policy
Optimization (Achiam et al., 2017) provides monotonic policy improvement but is computationally
expensive due to requiring a backtracking line-search procedure and conjugate gradient algorithm
for approximating the Fisher Information Matrix. Lyapunov-based Safe Policy Optimization (Chow
1
Under review as a conference paper at ICLR 2020
et al., 2019) requires solving a Linear Program (LP) at every step of policy evaluation, although they
show that there exists heuristics which can be substituted for the LP at the expense of theoretical
guarantees.
In this work, we propose an alternate formulation for solving CMDPs that transforms trajectory-level
constraints into localized state-dependent constraints, through which a safe policy improvement
step can be defined. In our approach, we define a notion of Backward Value Functions, which act
as an estimator of the expected cost collected by the agent so far and can be learned via standard
RL bootstrap techniques. We provide conditions under which this new formulation is able to solve
CMDPs without violating the constraints during the learning process. Our formulation allows us to
define state-level constraints without explicitly solving a LP or the Dual problem at every iteration.
Our method is implemented as a reduction to any model-free on-policy bootstrap based RL algorithm,
both for deterministic and stochastic policies, and discrete and continuous action spaces. We provide
the empirical evidence of our approach with Deep RL methods on various safety benchmarks,
including 2D navigation grid worlds (Leike et al., 2017; Chow et al., 2018), and MuJoCo tasks
(Achiam et al., 2017; Chow et al., 2019).
2	Constrained Markov Decision Processes
We write P(Y ) for the set of probability distributions on a space Y . A Markov Decision Process
(MDP) (Puterman, 2014) is a tuple (X, A, P, r, x0), where X is a set of states, A is a set of actions,
r : X × A → [0, RMAX] is a reward function, P : X × A → P(X) is a transition probability
function, and x0 is a fixed starting state. For simplicity we assume a deterministic reward function
and starting state, but our results generalize.
A Constrained Markov Decision Process (CMDP) (Altman, 1999) is a MDP with additional con-
straints that restrict the set of permissible policies for the MDP. Formally, a CMDP is a tuple
(X, A, P, r, x0, d, d0), where d : X → [0, DMAX] is the cost function1 and d0 ∈ R≥0 is the maxi-
mum allowed cumulative cost. The set of feasible policies that satisfy the CMDP is the subset of
stationary policies ΠD := {π : X → P(A) E[PtT=0 d(xt) | x0 , π] ≤ d0}. We consider a finite
time horizon T after which the episode terminates. The expected sum of rewards following a policy π
from an initial state x is given by the value function V π (x) = E[PtT=0 r(xt , at) | π, x]. Analogously,
the expected sum of costs is given by the cost value function VDπ (x) = E[PtT=0 d(xt) | π, x]. The
RL problem in the CMDP is to find the feasible policy which maximizes expected returns from the
initial state x0, i.e.
π* = arg max Vπ (x0)
π∈ΠD
An important point to note about CMDPs is that, in the original formulation, the cost function depends
on immediate states but the constraint is cumulative and thus depends on the entire trajectory.
In the case of MDPs, where a model of the environment is not known or is not easily obtained, it
is still possible for the agent to find the optimal policy using Temporal Difference (TD) methods
(Sutton, 1988). Broadly, these methods update the estimates of the value functions via bootstraps of
previous estimates on sampled transitions (we refer the reader to Sutton & Barto (2018) for more
information). In the on-policy setting, we alternate between estimating the state-action value function
Qπ for a given π and updating the policy to be greedy with respect to the value function.
3	Safe Policy Iteration via Backward Value Functions
Our approach proposes to convert the trajectory-level constraints of the CMDP into single-step
state-wise constraints in such a way that satisfying the state-wise formulation will entail satisfying
the original trajectory-level problem. The advantages of this approach are twofold: i) working
with single-step state-wise constraints allows us to obtain analytical solutions to the optimization
problem, and ii) the state-wise constraints can be defined via value-function-like quantities and can
thus be estimated with well-studied value-based methods. The state-wise constraints are defined
via Backward Value Functions, in Section 3.2, and in Section 3.3 we provide a safe policy iteration
procedure which satisfies said constraints (and thus the original problem).
1Here the cost only depends on states and not state-action pairs.
2
Under review as a conference paper at ICLR 2020
3.1	Backward Markov Chain
Unlike in traditional RL, in the CMDP setting the agent needs to take into account the constraints
which it has accumulated so far in order to plan accordingly for the future. Intuitively, the accumulated
cost so far can be estimated via the cost value function VD running “backward in time”. Before
giving the details of our approach and formally introducing the Backward Value Functions, we
review the main ideas, which are built upon the work of Morimura et al. (2010), who also considered
time-reversed Markov chains but from the standpoint of estimating the gradient of the log stationary
distribution; we extend these ideas to TD methods.
Assumption 3.1 (Stationarity). The MDP is ergodic for any policy π, i.e., the Markov chain charac-
terized by the transition probability Pπ(xt+ι∣xt) = £。=e/P(xt+ι∣xt,at)∏(at∣xt) is irreducible
and aperiodic.
Let M(π) denote the Markov chain characterized by transition probability P π (xt+1 |xt). The
above assumption implies that there exists a unique stationary distribution ηπ associated with π ,
such that it satisfies: ηπ(χt+ι) = Pχt∈χ Pπ(χt+ι∣χt)ηπ(xt). We abuse the notation and denote
Pπ(xt+ι, at∣xt) = P(xt+ι∣xt, at)∏(at∣xt).
According to Bayes’ rule, the probability q(xt-1, at-1 |xt) of a previous state-action pair (xt-1, at-1)
leading to the current state xt is given by:
P (xt|xt-1, at-1)P r(xt-1, at-1)
q(xt-1, at-1 |xt) =	vɔ	C i	、D~7	ʌ .
xt-1∈X	at-1∈A P(xt|xt-1, at-1)P r(xt-1, at-1)
From Assumption 3.1, we have that P r(xt-1, at-1) = ηπ(xt-1)π(at-1 |xt-1), and
Pxt-1∈X Pat-1∈A P(xt|xt-1, at-1)P r(xt-1, at-1) = ηπ(xt). We denote the posterior
q(xt-1, at-1|xt) as backward (or time-reversed) probability Pπ(xt-1, at-1 |xt), and we have:
Pπ (xt-i,at-i∣xt)
P (Xt ∣xt-i,at-i)ηπ (xt-i)∏(at-i∣xt-i)
ηπ(xt)
Pπ(xt, at-i∣xt-i)ηπ(xt-i)
ηπ (Xt)
(1)
The forward Markov chain, characterized by the transition matrix P π (Xt+1 |Xt), runs forward in time,
i.e., it gives the probability of the next state in which the agent will end up. Analogously, a backward
Markov chain is denoted by the transition matrix Pπ(xt-ι∣xt) = Pat_ 1∈a Pπ(xt-ι,at-ι∣xt), and
describes the state and action the agent took to reach the current state.
Definition 3.1 (Backward Markov Chain). A backward Markov chain associated with M(π) is
denoted by B(π) and is characterized by the transition probability P π (Xt-1 |Xt).
3.2	Backward Value Function
We define the Backward Value Function (BVF) to be a value function running on the backward
Markov chain B (π). A BVF is the expected sum of returns or costs collected by the agent so far. We
are mainly interested in maintaining estimates of the cumulative cost incurred at a state in order to
express the total constraint state-wise.
We note that, since every Markov chain M(π) is ergodic by Assumption 3.1, the corresponding
backward Markov chain B(π) is also ergodic (Morimura et al., 2010, Prop. B.1). In particular, every
policy π can reach the initial state via some path in the transition graph of the backward Markov
chain. Thus, the backwards Markov chain are also finite-horizon for some TB, with X0 corresponding
to the terminal state. We define a finite-horizon Backward Value Function for cost as:
TB
V πD (Xt) = EB(π)	d(Xt-k)|Xt
(π) k=0
(2)
Proposition 3.1 (Sampling). Samples from the forward Markov chain M(π) can be used directly to
estimate the statistics of the backward Markov chain B(π) (or the Backward Value Function). We
3
Under review as a conference paper at ICLR 2020
have:
EB(π)
K
d(xt-k)|xt
k=0
K
EM(π)	d(xt-k)|xt, ηπ (xt-K)
k=0
K
Em(∏)	£ d(xt+k)|xt+K, ηπ(xt)
k=0
(3)
where EM(π) and EB
are expectations over the forward and backward chains respectively. The
(π)
Equation (3) holds true even in the limit K → ∞.
The proof is given in Appendix B.1. Using the above proposition, we get an interchangeability
property that removes the need to sample from the backward chain. We can use the traditional RL
setting and draw samples from the forward chain and still estimate the BVFs. Equation (2) can be
written recursively as:
ππ
VD(xt) = EB(π) d(xt) + VD(xt-1) .
In operator form, the above equation can also be written as:
(T πV πD)(xt) = E π hd(xt) + VπD(xt-1)i .	(4)
χt-ι 〜P
Proposition 3.2 (Fixed point). For a policy π, the associated Backward Value Function vector, V π ,
πk	π	π
satisfies limk→∞ T V = V for every vector V , and V is the unique solution of the equation
Vπ = TπVπ.
The proof is given in Appendix B.2. The above proposition allows us to soundly extend the RL
methods based on Bellman operators for the estimation of BVFs.
3.3 Safe Policy Improvement via BVF-based constraints
With the Backward Value Function framework, the trajectory-level optimization problem associated
with a CMDP can be rewritten in state-wise form. Recall that a feasible policy must satisfy the
constraint:
T
EM(π)	d(xk) | x0 ≤ d0.
k=0
Alternatively, for each timestep t ∈ [0, T] of a trajectory:
t
E	d(xk) | x0, π
k=0
T
+ E	d(xk) | x0, π
k=t
- E d(xt) | x0 ≤ d0.
Via the identities EPT=t d(xk) | xo,∏] ≤ Ext』。(Pn)t [VD∏ (xt)] and E[Pk=o d(xk) | xo,∏] ≤
π
Exk〜δx°(P∏)t [VD(xt)](derived in Appendix C)2, We remark that the quantity on the LHS is less
than the expectation over k-step trajectories of V πD (xt) + VDπ(xt) - d(xt). In other words, for each
t ∈ [0,T] :
T
EM(π)	d(xk) | x0
k=0
≤ Ext〜δxo(Pπ)t VDD(Xt)+ Vn(Xt)- d(Xt)] ≤ do.
These are the state-Wise constraints that should hold at each step in a given trajectory - We refer
to them as the value-based constraints. Satisfying the value-based constraints Will automatically
satisfy the given CMDP constraints.
2δx0 is a Dirac distribution at x0, and δx0 (Pπ)t is the distribution of states at time t.
4
Under review as a conference paper at ICLR 2020
This formulation allows us to introduce a policy improvement step, which maintains a safe feasible
policy at every iteration by using the previous estimates of the forward and backward value functions3 4.
The policy improvement step is defined by a linear program, which performs a greedy update with
respect to the current state-action value function subject to the value-based constraints:
∏k+ι(∙∣x) = arg max <∏(∙∣x), Qnk (x, •)〉，	(SPI)
π∈Π
s.t. (π(∙∣x), QD (x, •))+ VD (x) — d(x) ≤ do,	∀x ∈ X.
Our first result is that the policies obtained by the policy improvement step will satisfy the safety
constraints. We write TV(∙, ∙) for the total variation metric between distributions.
Theorem 3.1	(Consistent Feasibility). Assume that successive policies are updated sufficiently
slowly, i.e. TV(∏k+ι(∙∣x), ∏k(∙∣x)) ≤ .-VDxT0) .4 Then the policy iteration step given by (SPI) is
consistently feasible, i.e. ifπk is feasible at x0 then so is πk+1.
It is also possible to consider larger neighbourhoods for updates of successive policies, but at the cost
of everywhere-feasibility. For want of space, we present that result in Appendix D.
Next we show that the policy iteration step given by (SPI) leads to monotonic improvement.
Theorem 3.2	(Policy Improvement). Let πn and πn+1 be successive policies generated by the policy
iteration step of (SPI). Then Vπn+1 (x) ≥ Vπn (x) ∀x ∈ X. In particular, the sequence of value
functions {Vπn}n≥0 given by (SPI) monotonically converges.
Proofs for Theorems 3.1 and 3.2 are given in Appendix D. Finding the sub-optimality gap (if any)
remains an interesting question left for future work.
4	Practical Implementation Considerations
4.1	Discrete Action Space
In discrete action spaces, the problem in (SPI) can be solved exactly as a Linear Programming
problem. It is possible to approximate its analytical solution by casting it into the corresponding
entropy-regularized counterpart (Neu et al., 2017; Chow et al., 2018). The details of the closed form
solution can be found in Appendix E.
Furthermore, if we restrict the set of policies to be deterministic, then it is possible to have an
in-graph solution as well. The procedure then closely resembles the Action Elimination Procedure
(Puterman, 2014, Chapter 6), where non-optimal actions are identified as being those which violate
the constraints.
4.2	Extension to continuous control
For MDPs with only state-dependent costs, Dalal et al. (2018) proposed the use of safety layers, a
constraint projection approach, that enables action correction at each step. At any given state, an
unconstrained action is selected and is passed to the safety layer, which projects the action to the
nearest action (in Euclidean norm) satisfying the necessary constraints. We extend this approach to
stochastic policies to handle the corrections for the actions generated by stochastic policies. When
the policy is parameterized with a Gaussian distribution, then the safety-layer can still be used by
projecting both the mean and standard-deviation vector to the constraint-satisfying hyper-plane5.
In most cases, the standard-deviation vector is kept fixed or independent of the state (Kostrikov,
2018; Dhariwal et al., 2017), which allows us to formulate the problem as solving the following
L2-projection of the mean of the Gaussian in Euclidean space. For μ∏ (.; θ), at any given state X ∈ X,
3In general, it is not possible to obtain the expectation Ext 〜δxcι (P∏)t [∙] directly as it may be intractable to
compute or we may not have access to the true transition distributions of the model. Thus, we sample a batch of
transitions from the current policy and use them for the updates.
4This can be enforced, for example, by constraining iterates to a neighborhood D(π, πk) ≤ δ.
5More information about this claim can be found in Appendix F.
5
Under review as a conference paper at ICLR 2020
the safety layer solves the following projection problem:
12
arg min 2 k(μ - μ∏(x)k	,
μ
s.t.
QD(x, μ) + VD(X)- d(x) ≤ do.
As shown in Dalal et al. (2018); Chow et al. (2019), if the constraints have linear nature then an
analytical solution exists. In order to get a linearized version of the constraints (and simplify the
projection), We can approximate the constraint with its first-order Taylor series at μ = μ∏ (x):
arg min
μ
(5)
s	.t. VD(X)- d(X) + QD(χ,μ∏(X)) + (μ - μ∏(X))T(VQD(χ,μ^μ=μ∏(X)) ≤ d0.
'---------------------------------------------V-----------------------}
First order Taylor expansion
The above objective function is positive-definite and quadratic, and the constraints are linear. Though
this problem can be solved by an in-graph QP solver, there exists an analytical solution (see Ap-
pendix G):
Proposition 4.1. At a given state X ∈ X, the solution to the Eq. (5), μ* is:
where,
μ* = μ∏ (x) - λ*(X) ∙ gμ,D (x),
gμ,D (X) = RQD (X, μ)lμ=μ∏(x),
λ*(X)
-(do + d(X) — VD(x) — QD(x, μ∏(X)))
gμ,D (X)Tgμ,D (x)
+
5 Related Work
Lagrangian-based methods: Initially introduced in Altman (1999), more scalable versions of
the Lagrangian based methods have been proposed over the years (Moldovan & Abbeel, 2012;
Tessler et al., 2018; Chow et al., 2015). The general form of the Lagrangian methods is to
convert the problem to an unconstrained problem via Langrange multipliers. If the policy pa-
rameters are denoted by θ, then Lagrangian formulation becomes: minλ≥o maxθ(L(θ, λ) =
minλ≥o maxθ [Vπθ (Xo) - λ(VDπθ (Xo) - do))] , where L is the Lagrangian and λ is the Lagrange
multiplier (penalty coefficient). The main problems of the Lagrangian methods are that the Lagrangian
multiplier is either a hyper-parameter (without much intuition), or is solved on a lower time-scale.
That makes the unconstrained RL problem a three time-scale 6 problem, which makes it very difficult
to optimize in practice. Another problem is that during the optimization, this procedure can violate
the constraints. Ideally, we want a method that can respect the constraint throughout the training and
not just at the final optimal policy.
Lyapunov-based methods: In control theory, the stability of the system under a fixed policy is
computed using Lyapunov functions (Khalil, 1996). A Lyapunov function is a type of scalar potential
function that keeps track of the energy that a system continually dissipates. Recently, Chow et al.
(2018; 2019) provide a method of constructing the Lyapunov functions to guarantee global safety
of a behavior policy using a set of local linear constraints. Their method requires the knowledge
of TV(∏, ∏*) to guarantee the theoretical claims. They substitute the ideally required Lyapunov
function with an approximate solution that requires solving a LP problem at every iteration. For the
practical scalable versions, they use a heuristic, a constant Lyapunov function for all states that only
depends on the initial state and the horizon. While our methods also constructs state-wise constraints,
there are two notable differences: a) our assumption only rely on the current policy candidate and the
baseline policy, instead of the baseline and the optimal policy, b) our method does not require solving
an LP at every update step to construct the constraint and as such the only approximation error that is
introduced comes from the function approximation.
6Classic Actor Critic is two time-scale (Konda & Tsitsiklis, 2000), and adding a learning schedule over the
Lagrangian makes it three time scale.
6
Under review as a conference paper at ICLR 2020
Conservative Policy Improvement: Constrained Policy Optimization (CPO) (Achiam et al., 2017)
extends the trust-region policy optimization (Schulman et al., 2015) algorithm to satisfy constraints
during training as well as after convergence. CPO is computationally expensive as it uses an
approximation to the Fisher Information Matrix which requires many steps of conjugate gradient
descent (ncg steps) followed by a backtracking line-search procedure (nls steps) for each iteration, so
it is more expensive by O(ncg + nls ) per update. Furthermore, accurately estimating the curvature
requires a large number of samples in each batch (Wu et al., 2017).
6	Experiments
We empirically validate our approach on RL benchmarks to measure the performance of the agent
with respect to the accumulated return and cost during training in the presence of neural-networks
based function approximators. We compare our approach with the respective Unconstrained versions,
and the Lyapunov-based approach (Chow et al., 2018; 2019) in each setting. Even though our
formulation is based on the undiscounted case, we use discounting with γ = 0.99 for estimating the
value functions in order to be consistent with the baselines. 7 *
6.1	Stochastic Grid World
Motivated by the safety in navigation tasks, we first consider a stochastic 2D grid world (Leike et al.,
2017; Chow et al., 2018). The agent (green cell in Fig. 1a) starts in the bottom-right corner, the
safe region, and the objective is to move to the goal on the other side of the grid (blue cell). The
agent can only move in the adjoining cells in the cardinal directions. It gets a reward of +1000
on reaching the goal, and a penalty of -1 at every timestep. Thus, the task is to reach the goal in
the shortest amount of time. There are a number of pits in the terrain (red cells) that represent the
safety constraint and the agent gets a cost of 10 on passing through any pit cell. Occasionally, with
probability p = 0.05, a random action will be executed instead of the one selected by the agent. Thus,
the task is to reach to the goal in the shortest amount of time, while passing through the red grids
at most d0/10 times. The size of the grid is 12 × 12 cells, and the pits are randomly generated for
each grid with probability ρ = 0.3. The agent starts at (12, 12) and the goal is selected uniformly on
(α, 0), where a 〜U(0,12). The threshold do = 20 implies the agent can pass at most two pits. The
maximum horizon is 200 steps, after which the episode terminates.
We use the action elimination procedure described in Sec 4.1 in combination with n-step SARSA
(Rummery & Niranjan, 1994; Peng & Williams, 1994) using neural networks and multiple syn-
chronous agents as in Mnih et al. (2016). We use -greedy exploration. The results are shown in
Fig. 1 (more experimental details can be found in Appendix H). We observe that the agent is able to
respect the safety constraints more adequately than the Lyapunov-based method, albeit at the expense
of some decrease in return, which is the expected trade-off for satisfying the constraints.
6.2	MuJoCo Benchmarks
Based on the safety experiments in Achiam et al. (2017); Chow et al. (2019), we design three simulated
robot locomotion continuous control tasks using the MuJoCo simulator (Todorov et al., 2012) and
OpenAI Gym (Brockman et al., 2016): (1) Point-Gather: A point-mass agent (S ⊆ R9, A ⊆ R2) is
rewarded for collecting the green apples and constrained to avoid the red bombs; (2) Safe-Cheetah:
A bi-pedal agent (S ⊆ R18, A ⊆ R6) is rewarded for running at high speed, but at the same time
constrained by a speed limit; (3) Point-Circle: The point-mass agent (S ⊆ R9 , A ⊆ R2) is rewarded
for running along the circumference of a circle in counter-clockwise direction, but is constrained to
stay within a safe region smaller than the radius of the circle.
We integrate our method on top of the A2C algorithms (Mnih et al., 2016) and PPO (Schulman
et al., 2017), using the procedure described in Section 4.2. More details about the tasks and network
architecture can be found in the Appendix I. Algorithmic details can be found in Appendix J. The
results with A2C are shown in Fig. 2 and the results with PPO are shown in Fig. 3. We observe that
7In practice, the starting states in the episode are unlikely to be distributed according to the stationary
distribution ηπ . We still use the initial trajectories to update the estimates nonetheless in our experiments, but we
use n-step updates.
7
Under review as a conference paper at ICLR 2020
our Safe method is able to respect the safety constraint throughout most of the learning, and with
much greater degree of compliance than the Lyapunov-based method, especially when combined
with A2C. The one case where the Safe method fails to respect the constraint is in Point-Circle with
PPO (Fig. 3(c)). Upon further examination, we note that the training in this scenario has one of two
outcomes: some runs end with the learner in an infeasible set of states from which it cannot recover;
other runs end in a good policy that respects the constraint. We discuss solutions to overcome this in
the final section.
(a) 2D GridWorld
(b) Returns
Figure 1: (a) Example of a gridworld environment. (b,c) Performance over the training for Unconstrained (red),
Lyapunov-based (green), and our method (blue) all trained with n-step SARSA on 2D GridWorld task over
20 random seeds. The x-axis is the number of episodes in thousands. The dotted black line in (c) denotes the
constraint threshold, d0 . The bold line represents mean, and the shaded region denotes 80% confidence-intervals.
(c) Constraints
7	Discussion
We present a method for solving constrained MDPs that respects trajectory-level constraints by
converting them into state dependent value-based constraints, and show how the method can be
used to handle safety limitations in both discrete and continuous spaces. The main advantage of our
approach is that the optimization problem is more easily solved with value-based constraints, while
providing similar guarantees and requiring less approximations. The empirical results presented
show that our approach is able to solve the tasks with good performance while maintaining safety
throughout training. It is important to note that there is a fundamental trade-off between exploration
and safety. It is impossible to be 100% safe without some knowledge; in cases where that knowledge
is not provided a priori, it must be acquired through exploration. We see this in some of our results
(Gridworld, Point-Circle) where our safe policy goes above the constraint in the very early phases of
training (all our experiments started from a random policy). We note that the other methods also suffer
from this shortcoming. An open question is how to provide initial conditions or a priori knowledge,
to avoid this burn-in phase. Another complementary strategy to explore is for cases where an agent is
stuck in an unsafe or infeasible policy space, where a recovery method (trained by purely minimizing
the constraints) could be useful to help the agent recover (Achiam et al., 2017; Chow et al., 2019).
8
Under review as a conference paper at ICLR 2020
Returns
Constraints
(a) Point-Gather
(b) Safe-Cheetah
(c) Point-Circle
Figure 2:	A2C Performance over the training for Unconstrained (red), Lyapunov-based (green), and our method
(blue) all trained with A2C on MuJoCo tasks over 10 random seeds. The x-axis is the number of episodes in
thousands. The dotted black line denotes d0 . The bold line represents the mean, and the shaded region denotes
the 80% confidence-intervals.
(a) Point-Gather
Returns
(b) Safe-Cheetah
(c) Point-Circle
Figure 3:	PPO Performance over the training for Unconstrained (red), Lyapunov-based (green), and our method
(blue) all trained with PPO on MuJoCo tasks over 10 random seeds. The x-axis is the number of episodes in
thousands, and y-axis denotes the undiscounted accumulated returns. The dotted black line denotes d0 . The bold
line represents the mean, and the shaded region denotes the 80% confidence-intervals.
9
Under review as a conference paper at ICLR 2020
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv
preprint arXiv:1705.10528, 2017.
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in Neural Information Processing
Systems,pp. 908-919, 2017.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic
programming and optimal control, volume 1. Athena scientific Belmont, MA, 1995.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. arXiv preprint arXiv:1512.01629, 2015.
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-
based approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708, 2018.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and Edgar Duenez-
Guzman. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031, 2019.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
pp. 1329-1338, 2016.
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine. Leave no trace: Learning to
reset for safe and autonomous reinforcement learning. arXiv preprint arXiv:1711.06782, 2017.
Hassan K Khalil. Nonlinear systems. 1996.
Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model pre-
dictive control for safe exploration and reinforcement learning. arXiv preprint arXiv:1803.08287,
2018.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://github.
com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.
Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A Ortega, Tom Everitt, Andrew Lefrancq, Laurent
Orseau, and Shane Legg. Ai safety gridworlds. arXiv preprint arXiv:1711.09883, 2017.
Ian Michael Mitchell. Application of level set methods to control and reachability problems in
continuous and hybrid systems. 2003.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937, 2016.
10
Under review as a conference paper at ICLR 2020
Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. arXiv
preprint arXiv:1205.4810, 2012.
Tetsuro Morimura, Eiji Uchibe, Junichiro Yoshimoto, Jan Peters, and Kenji Doya. Derivatives of
logarithmic stationary distributions for policy gradient reinforcement learning. Neural Computation,
22(2):342-376,2010. doi: 10.1162∕neco.2009.12-08-922. URL https://doi.org/10.
1162/neco.2009.12-08-922. PMID: 19842990.
Gergely Neu, Anders Jonsson, and Vicen9 G6mez. A unified view of entropy-regularized markov
decision processes. arXiv preprint arXiv:1705.07798, 2017.
Jing Peng and Ronald J Williams. Incremental multi-step q-learning. In Machine Learning Proceed-
ings 1994, pp. 226-232. Elsevier, 1994.
Joelle Pineau. The machine learning reproducibility checklist. https://www.cs.mcgill.ca/
~jpineau/ReproducibilityChecklist.pdf, 2018.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, vol-
ume 37. University of Cambridge, Department of Engineering Cambridge, England, 1994.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Aviv Tamar, Dotan Di Castro, and Shie Mannor. Policy evaluation with variance related risk criteria
in markov decision processes. arXiv preprint arXiv:1301.0104, 2013.
Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. arXiv
preprint arXiv:1805.11074, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in finite markov decision
processes with gaussian processes. In Advances in Neural Information Processing Systems, pp.
4312-4320, 2016.
Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. Safe exploration and optimization of
constrained mdps using gaussian processes. In AAAI Conference on Artificial Intelligence (AAAI),
2018.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing systems, pp. 5279-5288, 2017.
11
Under review as a conference paper at ICLR 2020
A Reproducibility Checklist
We follow the reproducibility checklist (Pineau, 2018) and point to relevant sections explaining them
here. For all algorithms presented, check if you include:
•	A clear description of the algorithm. The algorithms are explained in Sec. J. Any addi-
tional details for Discrete methods are provided in Sec. 4.1, and for continuous Sec. 4.2.
•	An analysis of the complexity (time, space, sample size) of the algorithm. The analytical
solution in Eq. (5) consists of a few vector arithmetic and relu operator and as such has
the same complexity as the baselines. For the discrete case, with deterministic policies
the solution again can be implemented as part of the computation graph, consisting of
basic vector arithmetic operations, and has very little additional overhead. For discrete
actions with stochastic policies, one needs to sovle the LP problem in (SPI). In that case
the complexity is same as the baseline safe-methods (Lyapunov), and is higher than the
unconstrained versions. In terms of computation time (for Deep-RL experiments) the newly
proposed algorithms are almost identical to the baselines due to its parallelizable nature. We
do not make any claims about the sample complexity.
•	A link to a downloadable source code, including all dependencies. The code will be
made available after the acceptance of the paper.
For any theoretical claim, check if you include:
•	A statement of the result. See the main paper for all the claims we make. Additional
details are provided in the Appendix.
•	A clear explanation of any assumptions. See the main paper for all the assumptions.
•	A complete proof of the claim. See the main paper. The cross references to the proofs in
the Appendix have been included in the main paper.
For all figures and tables that present empirical results, check if you include:
•	A complete description of the data collection process, including sample size. For the
base agent we standard benchmarks provided in OpenAI Gym (Brockman et al., 2016),
and rllab (Duan et al., 2016). We use the code from Achiam et al. (2017) for building the
Point-Circle and Point-Gather environments.
•	A link to downloadable version of the dataset or simulation environment. See:
github.com/openai/gym for OpenAI Gym benchmarks, github.com/jachiam/cpo for rllab
based Circle and Gather environments.
•	An explanation of how samples were allocated for training / validation / testing. We
do not use a split as we run multiple runs over random seeds to examine the optimization
performance.
•	An explanation of any data that were excluded. NA
•	The range of hyper-parameters considered, method to select the best hyper-parameter
configuration, and specification of all hyper-parameters used to generate results. The
default hyper-parameters for the MuJoCo baselines are taken from Kostrikov (2018). The
ranges and parameters for Grid experiments are described in Sec. H, and for MuJoCo are
described in Sec. I.
•	The exact number of evaluation runs. The number of evaluation runs is mentioned in the
caption corresponding to each result.
•	A description of how experiments were run. See Experiments Sec. 6 in the main paper
and in the Appendix Sec. H and Sec. I.
•	A clear definition of the specific measure or statistics used to report results. Un-
discounted return and cost using the current policy over the horizon are plotted after every
1000 episodes are plotted. We use a linear-filter with 0.7 weight for smoothing. We use the
smooting algorithm provided by TensorBoard (https://github.com/tensorflow/
tensorboard).
12
Under review as a conference paper at ICLR 2020
•	Clearly defined error bars. Standard error used in all cases.
•	A description of results with central tendency (e.g. mean) and variation (e.g. stddev).
The bold lines in the figure represent the mean, and the shaded region denotes the 80%
confidence interval.
•	A description of the computing infrastructure used. We distribute all runs across 10 CPU
nodes (Intel(R) Xeon(R) CPU E5-2650 v4) and 1 GPU (GP 100) per run for experiments.
B Backward Value Functions
We have the following result from Proposition 1 from Morimura et al. (2010). We give the proof too
for the sake of completeness.
Proposition B.1. Let the forward Markov chain M(π) be irreducible and ergodic, i.e., has a
stationary distribution. Then the associated backward Markov chain B (π) is also ergodic and has the
same unique stationary distribution as M(π):
ηπ(x) = η π (x),	(∀x ∈ X)
where ηπ(x) and η π(x) are the stationary distributions of M(π) and B (π).
Proof. Multiply both sides of Eq. (1) by ηπ(xt) and sum over all actions at-1 ∈ A we obtain detailed
balance like equations (with respect to time):
Pπ(xt-ι∣xt)ηπ(xt)= Pπ(xt,at-1∣xt-1)ηπ(xt-1).	(∀xt-ι ∈ X,x ∈ X)
Sum over all possible xt we have:
X Pπ(χt-ι∣χt)ηπ(χt) = ηπ(χt-ι).
xt∈X
The above equation indicates that B(π) has same stationary distribution as M(π). In the matrix form
ππ
the above equation can be written as ηP = η, that implies that η is stationary distribution with P
transition matrix.	□
B.1	Relation between forward and backward markov chains and backward
Value Functions
Proof. We use the technique of Proposition 2 of Morimura et al. (2010) to prove this. Using the
Markov property and then substituting Eq. (1) for each term we have:
Pπ (xt-1, at-1,. . . ,xt-K,at-K|xt) = Pπ (xt-1, at-1 |xt) . . .Pπ (xt-K,at-K|xt-K+1),
_ Pπ(xt, at-i∣xt-i)…Pπ(xt-κ+ι, at-KE-K)ηπ(xt-κ)
ηπ (Xt)
H Pπ(xt,at-i∣xt-i) ... Pπ(xt-κ+ι, at-κ∣xt-κ)ηπ(xt-κ).
This proves the proposition for finite K. Using the Prop. B.1, K → ∞ case is proven too:
K
lim
K→∞
EB	d(xt-k)|xt
(π) k=0
K
lim EM(π)	d(xt-k)|xt, ηπ (xt-K)
K→∞
k=0
ΣΣπ(a∣x)ηπ (x)d(x).
x∈X a∈A
□
13
Under review as a conference paper at ICLR 2020
B.2	TD FOR BVF
Proof. We use the same technique from Stochastic Shortest Path dynamic programming (Bertsekas
et al., 1995, Vol 2, Proposition 1.1) to prove the above proposition. The general outline of the proof
is given below, for more details we refer the reader to the textbook.
We have,
TπV =d+PπV.
(Eq. (4) in matrix notation)
Using induction argument, we have for all V ∈ Rn and k ≥ 1, we have:
k-1
TπkV=PπkV+XPπmd,
m=0
πk
Taking the limit, and using the result, limk→∞ P V
Bertsekas et al. (1995, Vol 2, Equation 1.2), we have:
0, regarding proper policies from
k-1
lim TπkV = lim X Pπmd=Vπ,
k→∞	k→∞
m=0
Also we have by definition:
Tπk+1V =d+PπTπk
and by taking the limit k → ∞, we have:
π	ππ
V =d+P V ,
which is equivalent to,
To show uniqueness, note that if V = TπV , then V = T π V for all k and letting k → ∞ we get
V = Vπ.
□
C Value-based constraint lemma
Lemma C.1. E [P" d(xk) | xo,π∣ ≤ Ext〜δx0(p∏) [⅞∏(xt)] and E IPk=O d(xk) | xo,∏ ≤
Exk 〜δχo (Pn) VDD (Xk R
Proof. Follows since adding more steps to the trajectory (from T — t steps to T) can only
increase the expected total cost. E PkT=t d(xk) | x0, π = δx0 (Pπ)t PkT=t(Pπ)k d ≤
δχo (Pπ)t (PT=+t(Pπ)k) d = Ext〜δxcι(Pn)t [咛(xt)]. The backward case is analogous.
□
D Properties of the policy iteration (SPI)
Theorem D.1. Let σ(x) ：= TV(∏k+ι(∙∣x),∏k(∙∣x)) = (1/2) Ea ∣∏k+ι(a∣x) — ∏k(a|x)| denote the
total variation between policies ∏k (∙∣x) and ∏k+ι(∙∣x). If the policies are updated sufficiently slowly
and πk is feasible, then so is πk+1. More specifically:
14
Under review as a conference paper at ICLR 2020
(I)	If ∏k is feasible at xo	and σ(x) ≤ 0T2DM(X0)	∀x then ∏k+ι	is feasible at	xo.
(II)	If πk is feasible	everywhere (i.e.	VDπk (x)	≤ d0 ∀x)	and	σ(x)	≤
--------d0-VDnkx)--------∀χ then ∏k+ι is feasible everywhere.
2T maxx0 {d0-V D (x0 )-d(x0 )}
We note that the second case allows the policies to be updated in a larger neighborhood but requires
πk to be feasible everywhere. By contrast the first item updates policies in a smaller neighbourhood
but only requires feasibility at the starting state.
Proof. Similar to the analysis in Chow et al. (2018). We aim to show that VDπk+1 (x0) ≤ d0. For
simplicity we consider k = 0, and by induction the other cases will follow. We write P0 =
Pπ0,Pι = Pπ1, ∆(a∣x) = ∏ι(a∣x) - ∏o(a∣x), and Pδ = [Pa∈A ∆(a∣x)P(x0∣x,a)]{χo,x}. Note
that (I - Po) = (I - Pi + P∆), and therefore (I - Pi + P∆)(I - Po)-1 = I∣χ∣×∣χ∣. Thus, we find
(I - Po)-1 = (I - PI)T(I∣x∣×∣x∣ + P∆(I - Po)-1).
Multiplying both sides by the cost vector d one has
T
VDπ0(x) =E Xd(xt) + ε(xt) | π1,x ,
t=o
for each x, where ε(x) = Pa∈A ∆(a∣x) P,o∈χ P(x0∣x, a)VD∏0 (x0). Splitting the expectation, we
have
T
VDπ1 (x) = VDπ0 (x) - E Xε(xt) | π1, x
t=o
For case (I) we note that VDπ0 (x0) ≤ DMAXT and so -2σ(xt)DMAXT ≤ ε(xt) ∀xt. Using σ(xt) ≤
(do- V∏k)/2DMAXT2 gives V∏1 (xo) ≤ 呼0(xo) -2DmaxT2(do -呼0(xo))∕(2DMAXT2) = do,
i.e. πo is feasible at xo .
For case (II) we note that VDπ0 (x) ≤ maxx0 {do - VπD0 (x0) - d(x0)} =: Θ since πo is feasible
at every x. As before, we have -2σ(xt)Θ ≤ ε(xt) ∀xt and so VDπ1 (x) ≤ VDπ0 (x) - 2ΘT (do -
V∏0 (x))∕(2ΘT) = do ∀x, i.e. ∏ι is feasible everywhere.	口
Theorem D.2. Let πn and πn+1 be successive policies generated be the policy iteration algorithm of
(SPI). Then V πn+1 ≥ Vπn.
Proof. Note that πn+1 and πn are both feasible solutions of the LP (SPI). Since πn+1 maximizes
Vπ over all feasible solutions, the result follows.	口
E Analytical Solution of the Update - Discrete Case
We follow the same procedure as (Chow et al., 2018, Section E.1) to convert the problem to its
Shannon entropy regularized version:
max π(.∣x)T (Q(x,.) + T log π(.∣x)),
π∈∆
s.t.	π(.∣x)TQD(x,.) + VD(x) — d(x) ≤ do,	(6)
where τ > 0 is a regularization constant. Consider the Lagrangian problem for optimization:
maxmaxΓx(π, λ) = π(.∣x)T(Q(x,.) + λQp(x,.) + Tlogπ(.∣x)) + λ(do + d(x) — V(x))
λ≥o π∈∆
From entropy-regularized literature (Neu et al., 2017), the inner λ-solution policy has the form:
π∖λ(.∣x) Y exp (- Q(X，.)+:QD(X，.))
15
Under review as a conference paper at ICLR 2020
We now need to solve for the optimal lagrange multiplier λ* at x.
max -T log-sum-exp (- Q(x,.) + JQD(x,.))
+ λ(d0 + d(x) - V D (x)),
where log-sum-exp(y) = log a exp(ya) is a convex function in y, and objective is a concave
function of λ. Using KKT conditions, the Vλ gives the solution:
(d0 + d(x) - V D(x)) -
Pa Qd (x, a)exp((- Q(x,a)+λQD(X⑷))
0
a exp(
Q(x,a)+λQ0(x,a)
T
Using parameterization of z = exp(-λ), the above condition can be written as polynomial equation
in z:
X (do + d(x) - VD(x) - Qd(x, a)).卜xp(-QxLa))) Zf，)= 0
a
The roots to this polynomial will give 0 ≤ z*(x) ≤ 1, using which one can find λ*(x) =
一log(z*(x)). The roots can be found using the Newton,s method. The final optimal policy of
the entropy-regularized process is then:
∏Γ (x exp (-Q(x, ∙) + λ*QD(x, ∙)
τ
F Extension of Safety Layer to Stochastic Policies with Gaussian
Paramterization
Consider stochastic gaussian policies parameterized by mean μ(x; θ) and standard-deviation σ(x; φ),
and the actions sampled have the form μ(x; θ) + σ(x; φ)e, where E 〜N(0, I) is the noise. Here,
< μ(χ; θ),σ(x; φ) > are both deterministic w.r.t. the parameters θ, φ and x, and as such both of
them together can be treated in the same way as deterministic policy (∏(χ) =< μ(χ), σ(x) 〉). The
actual action sampled and executed in the environment is still stochastic, but we have moved the
stochasticity fron the policy to the environment. This allows us to define and work with action-value
functions QD(x,μ∏(x),σ∏(x)). In this case, the corresponding projected actions have the form
μ0 + σ0E. The main objective of the safety layer (without the constraints) can be further simplified as:
arg min Ee 〜N(0,1) - ∣∣(μ0 + σ0e) - (μ∏ (x) + σ∏ (x)e)k2
μ0,σ0	L2
arg min Ee〜N(o,i)
μ0,σ0
2 k(μ0 - μ∏(X)) + ((σ'-σ∏(χ))e)k2
1	0	20	2	0	0
arg min AEe 〜N (o,i)	∣∣μ - μ∏ (x)k + k(σ -	σ∏	(x))e∣	+2 <μ一	μ∏ (x),	(σ	-	σ∏	(x))e >
μ0,σ0	2	'--------------V----------------}
=0,due to linearity of expectation,e〜N(0,I)
10	2
arg min - (∣∣μ- μ∏ (x)∣ + Ee 〜N (o,i)
μ0,σ0	2 1
∣(σ0 - σπ(x))E∣2
1
arg min 一
μ0,σ0 2
1
arg min -
μ0,σ0 2
f	∖
kμ0 - μ∏(x)k2 + k(σ0 - σ∏(x))k2 Ee〜N(0,i) [|k『]
'---------------------------------------{------}
=1,second moment of e
(kμ0 -μ∏(χ)k2 + k(σ0 -σ∏(X))k2)
As both μ∏(.; θ) and σ∏(.; φ) are modelled by independent set of parameters (different neural
networks, usually) we can solve each of the safety layer problem independently, w.r.t. only those
parameters.
16
Under review as a conference paper at ICLR 2020
G	Analytical Solution in Safety Layer
The proof is similar to the proof of the Proposition 1 of Dalal et al. (2018). We have the following
optimization problem:
arg min 1 k(μ - μ∏ (x)k2 ,
μ L2	一
s.t.	VD(X)- d(x) + QD(x,μ∏(x)) + (μ - μ∏(x))T(VQD(x,μ)∣μ=μ∏(χ)) ≤ do
As the objective function and constraints are convex, and the feasible solution, μ*, λ*, should satisfy
π
the KKT conditions. We define e(x) = (do + d(x) - VD(x) - QD(x,μ∏(x))), and gμ,D(x)=
VQD (x, u)∣u=μπ(χ). Thus, We can write the Lagrangian as:
L(μ, λ) = 2 k(μ - μ∏(X)『+ λ((μ - μ∏(X))Tgμ,D(X) - e(X))
From the KKT conditions, we get:
VμL = μ - μ∏ (x) + λgμ,D (X)= 0	(7)
(μ - μ∏(X))Tgμ,D(x) - e(X) = 0	(8)
From Eq. (7), we have:
μ* = μ∏ (x) - λ* (x) ∙ gμ,D (x)	(9)
Substituting Eq. (9) in Eq. (8), we get:
-λ*(X) ∙ gμ,D (X)T gμ,D (x) - e(X) = 0
λ* =	-e(X)
gμ,D (X)Tgμ,D (x)
When the constraints are satisfied (e(X) > 0), the λ should be inactive, and hence we have ()+
operator, that is 0 for negative values.
H	Details of Grid-World Experiments
H. 1 Architecture and Training details
We use one-hot encoding of the agent’s location in the grid as the observation, i.e. X is a binary vector
of dimension R12×12. The agent is trained for 200k episodes, and the current policy’s performance is
evaluated after every 1k episodes.
The same three layer neural network with the architecture is used for state encoding for all the
different the estimators. The feed-forward neural network has hidden layers of size 64, 64, 64, and
relu activations. For the state-action value based estimators, the last layer is a linear layer with 4
outputs, for each action. For value function based estimators the last layer is linear layer with a single
output.
We use Adam Optimizer for training all the estimators. A learning rate of 1e-3 was selected for all
the reward based estimators and a learning rate of 5e-4 was selected for all the cost based estimators.
The same range of learning rate parameters for considered for all estimators i.e. {1e-5, 5e-5, 1e-4,
5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1}.
We use n-step trajectory length in A2C with n = 4, i.e., trajectories of length n were collected and
the estimators were updated to used via the td-errors based on that. We use the number of parallel
agents 20 in all the experiments. The range of parameters considered was n ∈ {1, 4, 20}. The same
value of n was used for all the baselines.
17
Under review as a conference paper at ICLR 2020
Figure 4: MuJoCo Safety Environments
I Details of the MuJoCo Experiments
I.1	Environments Description
•	Point-Gather: The environment (Fig.4c) is taken from Achiam et al. (2017), where the
point mass agent gets a reward of +10.0 for collecting a green apple, and a cost of 1 for
collecting a red bomb. Two apples and eight bombs are spawned randomly at the start
of each episode. The constraints are defined over the nmber of bombs collected over the
episode. Episode horizon is 15 and threshold d0 = 4.
•	Safe-Cheetah: This environment (Fig.4b) is taken from Chow et al. (2019). A bi-pedal
agent (HalfCheetah-v0) is augmented with speed safety constraints. The agent gets the
reward based on the speed with which it runs, and the constrain is define on the speed to be
less than 1, i.e., it gets a constraint cost based on l[|v| > 1], where V is the velocity at the
state. The maximum length of the episode is 200 and the constraint threshold is d0 = 50.
•	Point-Circle: This environment (Fig.4a) is taken from Achiam et al. (2017). The point-
mass agent is rewarded for running along the circumference of a circle of radius 15 in
counter-clockwise direction, with the reward and cost function:
R(s)
vT[-y,χ]
1	+ | k[χ,y]k2 -15|
C(S) = l[|x| > 2.5],
where x, y are coordinates in the plane and v is the velocity. The length of the episode is 65
and the constraint threshold d0 = 10.0.
I.2	Network Architecture and training details
The architecture and the training procedure is based on the open-source implementations (Kostrikov,
2018). All the value based estimators use a network architecture of 2 hidden layers of size 200, 50
hidden units with tanh non-linearity, followed by a linear layer with single output. For the actor, we
model mean using a network architecture of 2 hidden layers of size 100, 50 hidden units with tanh
non-linearity, followed by a linear layer with dimensions of the action-space and tanh non-linearity.
For the Q(x, μ) we also a 2 layer neural network with 200, (50 + action-dimension) hidden units and
tanh non-linearity. We concatenate the mean in the second layer, and add a linear layer with single
output in the end.
Entropy regularization with β = 0.001 was used for all the experiments and the baselines. The
trajectory length for different environments. For PPO GAE with λ = 0.95 was used for every
algorithm. 20 parallel actors were used for every algorithm for each experiment. We searched the
trajectory length hyper-parameter in the range 5,20,100 for every environment. We used the trajectory
length of 1000 over which the samples are collected for PPO, for all environments. For the A2C
experiments, for SafeCheetah trajectory length of 5 is used and for the rest 20 is used.
18
Under review as a conference paper at ICLR 2020
We use Adam Optimizer for training all the estimators. The learning rate of the critic is always 0.5
the learning rate of the actor. For the cost estimators, the same learning rate was used for forward and
backward estimators. The same range of learning rate parameters for considered for all estimators i.e.
{1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1}.
I.3	Other details
As we mentioned in Sec. 7, due to exploration the agent can potentially end up being in an infeasible
policy space. To prevent that from happening a recovery policy (or safe-guard policy) (Achiam et al.,
2017; Chow et al., 2019) is used to recover back to the feasible policy space. We run the experiments
with and without the use of recovery policies (in the same procedure as the baselines), and chose the
run that performs the best. We noticed that, empirically, for our approach recovery policies are only
required for Point-Circle environments, as the agent has much more probability of being stuck in the
constraint space.
In order to take error due to function approximation into account, Achiam et al. (2017) use cost-
shaping to smooth out the sparse constraint, and Chow et al. (2019) use a relaxed threshold, i.e.
do ∙ (1 - δ), instead of do, where δ ∈ (0,1). We run experiments with δ = {0.0,0.2} for each
algorithms, and use the best among them. We found that empirically, only for Safe-Cheetah δ = 0.2
works better compared to δ = 0.0.
J Algorithm Details
J.1 n-step Synchronous SARSA
The algorithm for n-step Synchronous SARSA is similar to the n-step Asynchronous Q-learning of
Mnih et al. (2016), except that it uses SARSA instead of Q-learning, is synchronous, and instead
of greedy maximization step of -greedy we use (SPI). When working with discrete actions and
deterministic policies, this can be solved as part of the computation-graph itself. The algorithm is
presented in Alg. 1.
J.2 A2C
In Actor Critic (Konda & Tsitsiklis, 2000) algorithms, the parameterized policy (actor) is denoted by
π(a∣x; θ), and is updated to minimizing the following loss:
L(θ) = E[- log∏(at∣xt; θ)(r + YVπ(xt+ι - Vxt))]
The algorithm for A2C with Safety Layer given by Eq. (5) is similar to the Synchronous version of
Actor-Critic (Mnih et al., 2016), except that it has estimates for the costs and safety-layer. Note that
due to the projection property of the safety layer, it is possible to sample directly from the projected
mean. Also, as the projection is a result of vector products and max, it is differentiable and and
computed in-graph (via relu). The algorithm is presented in Alg. 2.
J.3 PPO
The PPO algorithm build on top of the Actor-Critic algorithm and is very similar to Algorithm 2. The
main difference is how the PPO loss for the actor is defined as:
LCLIP (θ) = E[min(ρt (θ)At, clip(ρt (θ), 1 - , 1 + )At)],
where the likelihood ration is ρt(θ) = ∏∏θ(甯[),with ∏oid being the policy parameters before the
update, < 1 is a hyper-parameters that ocontrols the clipping and At is the generalized advantage
estimator:
T-1
AtGAE(λ,γ) = X(λγ)kδtV+πk,
k=o
19
Under review as a conference paper at ICLR 2020
Algorithm 1 Synchronous n-step SARSA
Input: θ parameters for Q(x, .; θ), θD parameters for QD(x, .; θD),φD parameters for VD(x; φD);
π0 initial feasible policy.
for episode e ∈ 1, ..., M do
Add the initial state to the trajectory buffer T J {x0}
t J 1
while t < T do:
tstart J t
while t < t + n or t == T do
Select at using (SPI), execute at , observe xt+1 and reward rt and cost dt .
Add experiences to a buffer, i.e., τ J (at, rt, dt, xt+1).
tJt+1
end while
Calculate the next action for xt+1 using the current policy estimates, at+1
Bootstrap the targets:
RJ 0
Q(xt+1, at+1; θ)
if t == T
otherwise
RD J
ift ==T
QD(xt+1, at+1; θD) otherwise
0
ift ==0
art-1; φD) otherwise
. Calculate the targets for the transitions in buffer
for i ∈ {t - 1, . . . , tstart } do
RJri+γR
RD J di + γRD
Accumulate the gradients wrt θ, θD :
d0jdθ+d (R - Q(Mai θ))2
dθ J dθ +-----τττ----
∂θ
∂(RD- QD(xi , ai ; θD))2
dθD J dθD +------ττ7------
∂θD
end for
for i ∈ {tstart, . . . , t} do
RD J di + γRD
Accumulate the gradients wrt φD :
d, ʃ d , Id(RD — VD(Xi； φD))2
dφD J dφD +---------------
∂φD
end for
Do synchronous batch update with the accumulated gradients to update θ, θD , φD using
dθ, dθD, dφD.
end while
Empty the trajectory buffer, τ
end for
20
Under review as a conference paper at ICLR 2020
Algorithm 2 Synchronous A2C with Safety Layer
Input: θ parameters for ∏(x; θ), φ the parameters for V(x; φ), Θd parameters for QD(x, μ; Θd),
φD parameters for VD (X； φD);
for episode e ∈ 1, ..., M do
Add the initial state to the trajectory buffer τ J {x0}
tJ1
while t < T do:
tstart J t
while t < t + n or t == T do
Select at using sampling from the projected mean μt via the safety layer Eq.(5), execute
at, observe xt+1 and reward rt and cost dt.
Add experiences to a buffer, i.e., T J (at, μt, rt, dt, xt+ι).
tJt+1
end while
Calculate the next mean for xt+ι using the current policy estimates, μt+ι
Bootstrap the targets:
0
R -
if t == T
V (xt+1 , at+1 ; φ) otherwise
0
RD —
if t == T
Qd (xt+ι ,μt+ι ； Θd ) otherwise
if t == 0
art-1； φD) otherwise
. Calculate the targets for the transitions in buffer
for i ∈ {t - 1, . . . , tstart } do
R J ri + γR
RD J di + γRD
Accumulate the gradients w.r.t. θ, φ, θD :
dθ J dθ + Vθ logπ(ai | Xi; θ)(R — V(xi； φ))
dφ J dφ +
∂(R - V(xiφ))2
dθg J— dθg +
∂φ
∂(Rd - Qd(χi,μi; Θd))2
∂θD
end for
for i ∈ {tstart, . . . , t} do
RD J di + γRD
Accumulate the gradients wrt φD :
dφD J dφD +
∂(RD - VD(Xi； φD))2
∂φD
end for
Do synchronous batch update with the accumulated gradients to update θ, φ, θD , φD using
dθ, dφ, dθD , dφD.
end while
Empty the trajectory buffer, τ
end for
21
Under review as a conference paper at ICLR 2020
where T is the maxmimum number of timestamps in an episode trajectory, and δj denotes the TD
error at j . The value function is updated using the γλ-returns from the GAE:
L(φ) = E[(Vπ(x; φ) - (Vπ(x; φoid) + At))2].
Similar to the the forward value estimates the backward value estimates are defined in the similar
sense. One way to think of it is to assume the trajectories are reversed and we are doing the regular
GAE estimation for the value functions.
The GAE updates for the regular value function can be seen in the λ-operator form as:
Tλπvπ = (I - γλP π)-1(rπ +γPπvπ - vπ) +vπ.
In similar spirit it can be shown that the λ-operator for SARSA has the form:
Tλπqπ =(I-λγPπ)-1(Tπqπ -qπ)+qπ,
where (Tπqπ -qπ) denotes the TD error. Thus, the GAE estimates can be applied for the Q-functions
in the similar form, i.e.
T-1
BtGAE(λ,γ) = X (λγ)kδtQ+πDk,
k=0
L(Θd) = E[(QD(x, a； Θd) — (QDD(x, a； θ0d) + Bt))2].
22