Under review as a conference paper at ICLR 2020
Sensible Adversarial Learning
Anonymous authors
Paper under double-blind review
Ab stract
The trade-off between robustness and standard accuracy has been consistently re-
ported in the machine learning literature. Although the problem has been widely
studied to understand and explain this trade-off, the problem seems to remain as
an open problem. In this paper, motivated by the fact that the high dimensional
distribution is poorly represented by limited data samples, we introduce sensi-
ble adversarial learning and demonstrate the synergistic effect between pursuits
of natural accuracy and robustness. Specifically, we define a sensible adversary
which is useful for learning a defense model and keeping a high natural accuracy
simultaneously. We theoretically establish that the Bayes rule is the most robust
multi-class classifier with the 0-1 loss under sensible adversarial learning. We
propose a novel and efficient algorithm that trains a robust model with sensible
adversarial examples, without a significant drop in natural accuracy. Our model
on CIFAR10 yields state-of-the-art results against various attacks with perturba-
tions restricted to '∞ with E = 8/255, e.g., the robust accuracy 57.23% against
PGD attacks as well as the natural accuracy 91.51%.
1	Introduction
With many impressive successes of deep learning, there are a multitude of applications of deep
neural networks (DNNs) that permeate in our everyday life. As DNNs are applied in security-
critical systems such as malware detection, face identification, and autonomous driving, robustness
of DNNs against adversarial attacks, i.e., the intently perturbed inputs to fool the system, has become
an important research topic (Szegedy et al., 2013; Papernot et al., 2016; Biggio et al., 2013).
One of the most widely studied classes of adversarial perturbations is `p -norm constrained adver-
sarial perturbations (Szegedy et al., 2013). Madry et al. (2017) formalize the adversarial learning
against this class of perturbations as a minimization problem of adversarial risk defined in a follow-
ing way. Let (X, y) ∈ X × Y be from some unknown distribution PX,Y . Given a loss function
` : Y × Y → R and a constraint constant E > 0, the adversarial robust risk is
Rrob(f) = EX,Y max `(f (X +δ),y) .
kδkp≤
(1)
Many adversarial learning methods can be interpreted as empirical minimization of (1) (Goodfellow
et al., 2014; Kurakin et al., 2016b; Ruitong Huang & Szepesvari, 2015; Madry et al., 2017). For
this optimization problem, Madry et al. (2017) propose to train a robust model with the augmented
data generated by the projected gradient descent method (PGD). On this adversarial training, they
make two important observations. First, it costs natural accuracy. A network trained with adversarial
examples tends to have a lower natural accuracy than a naturally trained network. This trade-off is
observed even with a small training E. Second, the adversarial training requires a larger model ca-
pacity than the natural training does. If the model capacity is only sufficient for the natural learning,
the adversarial training can converge to a constant function.
For a large E, the optimization problem of (1) itself may pose the trade-off. For instance, Tsipras
et al. (2018) show an example of an inherent tension between pursuits of accuracy and robustness
when E is large enough to change the true class. For a smaller E, however, the formulation in (1) does
not explicitly pose any conflict between the pursuit of robustness and accuracy. Note that Rrob (f)
is an upper bound of the standard risk of f . A perfectly robust model f with Rrob (f) = 0 is
also perfectly accurate for natural learning. If the perfect classifier exists in a given model class,
the trade-off may be caused by the large sample complexity of adversarially robust generalization
1
Under review as a conference paper at ICLR 2020
(Schmidt et al., 2018; Yin et al., 2018; Stutz et al., 2019). Without sufficiently large amount of data,
the empirical minimization of (1) may result in a large standard risk by converging to a model of a
poor robust risk. On the other hand, if robust learning converges to a constant function, it cannot
achieve natural accuracy. In this sense, to resolve the trade-off problem, we may need to deal with
the increased requirement on the model capacity.
In this paper, we propose a novel framework, called sensible adversary, in order to overcome the
trade-off between natural accuracy and robustness. In particular, we restrict adversarial perturbations
not to cross the Bayes decision boundary besides the -ball constraint, so that the perturbation ball
is adaptively modified for every single data point. Our main contributions are:
•	Under the framework of sensible adversary, the pursuit of robustness and accuracy given an
enough model capacity can align with each other, i.e., there is no trade-off. We theoretically
establish the Bayes rule is most robust against the sensible adversary. If the Bayes decision
boundary can be far from data manifolds at least by , our pursuit of sensible robustness
does not cost any adversarial robust risk.
•	We propose an efficient algorithm for sensible adversarial training , which utilizes sensible
adversaries in the absence of the true Bayes rule. This sensible adversarial training enjoys
robustness without a significant drop of natural accuracy. Furthermore, the algorithm is not
sensitive to the model capacity. When insufficient model capacity is given, our algorithm
does not collapse to a constant function. Instead, it trains a model as robust as possible.
•	We experimentally demonstrate that sensible adversarial training enables to stably learn
a robust and accurate model. In particular, on CIFAR10, we achieve 91.51% natural test
accuracy and 57.23% robust test accuracy against '∞ PGD attacks constrained to E =
8/255. To the best of our knowledge, there is no approaches known to achieve natural
accuracy more than 90%, while achieving more than 55% of robust accuracy against PGD
attacks of E = 8/255. Moreover, no previous approaches pursuing robustness against this
attack achieved the natural accuracy more than .
1.1	Related work
Madry et al. (2017) formalize adversarial learning as a mini-max problem given perturbation restric-
tion, and theoretically and empirically established the feasibility of the optimization. Our sensible
adversary redefines the set of perturbation in the inner maximization problem on which their the-
oretical result is directly applicable. Tsipras et al. (2018) investigate the possible source of robust
trade-off. The key idea is that when there are features that are useful for natural classification but
vulnerable to adversarial perturbations, a robust model would abandon these features because other-
wise all of these features can adversarially move to promote incorrect prediction. Our work explores
the possibility of learning a robust model while not allowing such collective adversarial migration.
While a class change by adversarial examples typically has been prevented by using a small E, Sug-
gala et al. (2018) explicitly ignore an adversarial perturbation that crosses the decision boundary
of a Bayes rule. Zhang et al. (2019) also investigate the Bayes decision boundary to resolve the
trade-off problem. They search for a model f having a small weighted sum of the natural risk and
a probability that an adversarial example can cross the decision boundary of f. Gilmer et al. (2018)
show that in high dimensional setting, even small test error can imply the existence of adversarial
examples for most of data points. Our effort to prioritize natural accuracy to find a robust model is
consistent to the view in Gilmer et al. (2018). More related work will be presented in Appendix A.
1.2	Notation
Let F denote the class of functions represented by DNNs with a fixed architecture. An opti-
mal robust model w.r.t. PX,Y is denoted by frob = arg minf ∈F Rrob(f). Denote the stan-
dard risk w.r.t PX,Y by Rstd(f) = EX,Y [`(f (X), y)] and its optimal natural model by fstd =
arg min f ∈f Rstd(f). Let Pχ,γ = Pχ,γ ∣χ×γ denote a restricted distribution of Pχ,γ on a
subset X × Y ⊂ X × Y. Likewise, denote the standard risk w.r.t PX,Y by Rstd(f) and the
robust risk w.r.t PX,Y by Rrob(f). For a set A, the E-neighborhood in `p-norm is defined as
B(A, E) = {y|ky - xkp ≤ E, x ∈ A}, and the interior is denoted by int(A). Denote the E-
2
Under review as a conference paper at ICLR 2020
neighborhood of the decision boundary of f by DB(f, ) = {x ∃x0 ∈ B(x, ) s.t. f(x) 6= f(x0)}.
Let Pf,y (x) denote the predicted probability of the label of X being y by a model f.
2	Adversarial learning may help standard learning
In this section, we use a toy example to investigate the synergistic effect between the pursuit of
robustness and natural accuracy.
Example: We can easily find an example of trade-off wherever an -ball attack can cross the
manifold boundary between classes. For example, consider a two-dimensional random vector
X = (X1,X2) on X = (0,1) X (0,1) with a binary class Y 〜 Ber(P), where p > 0.5. Let
the conditional distribution be (X1,X2)∣Y = 0 ~ Unif ((0, ɪ) × (0,1)) and (X1,X2)∣Y = 1 ~
Unif ((2, 1) × (0,1)). Then the Bayes Rule is fB (x) = Sign(XI 一 0.5), and it is a perfect classifier,
in that Rstd(fB) = 0 with ` as the 0-1 loss. IfP > 0.5, the robust classifier against -ball attacks is
frob(X) = sign(X1 一 (0.5 一 )). Its decision boundary is deviated by 一 from that of the Bayes rule,
and this deviation costs natural accuracy by Rstd(frob) 一 Rstd(f) = (1 一 P). However, when the
data points reside in a high-dimensional space, the samples are too sparse to represent the underlying
true distribution. To take this phenomenon into account, consider a distribution Pχ,γ = Pχ,γ ∣χ×γ
on a subset X × Y ⊂ X × Y. Assume that we only observe data generated from PX,Y, and the
training and test sets do not provide any information on Xc × Y. Now by applying this support re-
striction to the example above, we demonstrate how adversarial learning dramatically changes from
harming to benefiting natural accuracy.
The Cheese hole distribution: For the above example, We restrict the support on X = ∪3=1 ∪3=ι
Hij, where Hij = ((αɑ, 3α) + 2α(i-1)) × ((αa, 3α) + 2α(j -1)), α = 6, and j = 1, 2, 3. Therefore
the sampling support X comprises of nine small squares equally spaced by a. This is illustrated in
Figure 1 (a). Among all classifiers which predict the same as fB on X, the worst one, denoted by
产,predicts exact opposite on X ∈ Xc as illustrated in Figure 1 (b). For this worst case, the true
standard risk w.r.t Pχ,γ is 4 , although the standard risk w.r.t. Pχ,γ is zero.
We argue that pursuing robustness can mitigate this discrepancy. Among minimizers of Rrob, let
fob be the worst classifier in that it predicts incorrectly outside X as depicted in Figure 1 (c). On
7-1 / τ5 ∖	J∙⅛	1	1 1	.1	1	∙ Γ∙	.	Λ	r /	∖ I z-> T-	,
B(X, e), fob should correctly classify, except on Ae = {(xι, x2)∣0.5 一 E ≤ xι ≤ 0.5} ∩ B(X, e).
In this situation, the inaccuracy of f% is compensated by its increased accuracy on B(X, E) \ Ae.
Moreover, the composition of Rrob(f b*) is particularly interesting, and it is easy to show that
Rrob(产)=P(产*(Y) = X,X ∈ DBfB,E)) + 3/4,
where 3/4 is from the standard inaccuracy on Xc. The major part of Rrob(fB*) comes from the
inaccuracy of fB* outside of the sampling support X. Therefore by simply reducing this in-
accuracy on Xc, we can lower the robust risk by 3/4. Another interesting observation is that
Be = {(X1, X2)|0.5 一 E < X1 < 0.5 + E} is the only area where the Bayes rule fB has its ro-
bust risk. Note that Rrob (fB) = P(X ∈ Be, fB (X) = Y) = 2E which is greater than the optimal
robust risk. The reason why fB is not most robust is simply because it is accurate on Be . This
prevents fB from being a robust function under the current adversarial robustness framework.
These observations have two implications. First, the robust pursuit can help to increase natural
accuracy outside ofX where the samples are poorly representative. Second, by accurately correcting
the model outside of X in a standard sense, the robust risk can be significantly decreased. This is
where the pursuit of robustness and accuracy coincides.
Toward respecting the original data structure: What if we regard the adversarial robust risk near
the class border as reasonable gullibility? This corresponds to pursuit of robustness only against
adversarial examples which do not cross between the class manifolds, i.e., as long as this does not
harm the natural accuracy on X . We call this robustness as sensible robustness, which respects the
structure represented by data. In this example, the sensible robustness can increase both robustness
and natural accuracy. Let fK denote the worst case sensibly robust classifier w.r.t Pχ,γ. The
3
Under review as a conference paper at ICLR 2020
□□□
□□□
□ Q□
↑x2πmm
口口口
z v X1	X1	,、 X1	z .λ X1
(a)	(b)	(c)	(d)
Figure 1: Cheese holes distribution. (a) The outer square is the support of the underlying true
distribution PX,Y , but the sampling is restricted on the small squares X × Y with distribution PX,Y .
(b) The worst case naturally optimal model w.r.t. Pχ,γ. (C) The worst case robustly optimal model
w.r.t. Pχ,γ. (d) The worst case sensibly robust model w.r.t. Pχ,γ.
decision boundary of fs should not deviate to the left on B(X, e) no matter how large e is, as
depicted in Figure 1 (d). Note that when e ≥ α∕2,于温 is the unique mmιmιzer of Rrob(f) as
B(X, e) covers X.
3 Sensible adversarial robustness
In this section, we introduce a sensible adversary framework. Consider a general multi-class case
with the 0-1 loss, where Y = [K]. Assume the model capacity is enough so that the Bayes rule
fB ∈ F. We consider `p-norm constrained adversarial attacks, where p ∈ {0, 1, ..., ∞}.
Definition 1. (sensible adversarial example) Fora classifier f, let Sx,(f) = {z ∈ X|kz - xkp ≤
e, f(z) = y}. Then the sensible adversarial example of (x, y) w.r.t f is defined as
〜∫χ,	if f B(X) = y
x = ʌ arg max '(f (z),y),	otherwise.	(2)
[z∈Sx,e (f B)
Definition 2. (sensible robustness) Let the sensible adversarial loss be `rob e(f, x, y) = '(f (X),y)
where X is a sensible adversarial example as defined above. Then the sensible robust risk of a model
f is defined by
Rrob (f) = EPX,Y h`rob, (f,X,Y)] = Epχ∕'(f(X),Y)].
We call its minimizer as a sensibly robust model w.r.t PX,Y and denote by frsob , i.e.,
frsob = argminRsrob(f).
f∈F
(3)
Remark 1. Intuitively, a sensible adversarial example is an adversarial example restricted not to
cross the decision boundary of the Bayes rule. In addition, a sensible adversary does not perturb a
data point that the Bayes rule incorrectly classifies. Therefore, it is natural to expect that pursuing
sensible robustness would not cost natural accuracy, and the following theorem confirms it.
Theorem 1.	Let RStd denote the minimum standard risk which is Rstd(fB). Then we have
Rsob(fB) = RStd. Furthermore, fB is the unique minimizer of RSob(f) among f ∈ F.
Theorem 2.	Let SX, be an e-ball centered at X. Then for any f ∈ F and for any set A ⊂
X\DB(fB,e),
P∃	X0	∈	SX,	s.t. fB(X0)	6=	Y,X ∈ A ≤ P∃ X0	∈	SX,	s.t. f(X0)	6= Y,X ∈ A.
Remark 2. According to the Theorem 1, a sensibly robust model w.r.t PX,Y is the Bayes rule,
i.e., frsob = fB . This sensible robustness costs adversarial robustness since fB may have a larger
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Sensible adversarial training for '∞ norm restriction
1:	Input: Initialized f = fθ, c ∈ (0, 1), step number K, step sizes η1, η2, data Xa(0d)v = X
2:	repeat
3:	for i = 1, ..., m, s.t. f (xi,adv) = yi
4:	for k = 1, ..., K
5:	Xkadv — πB(Xi,e)SlSign(Vx'(f (X(k-dV)),yi))+ x(kadv), 口： the Projection oPerator
6：	if'f,χ(k)dv,yi) > log C
7:	(sensible reversion) xi(,Kad)v = xi(,ka-dv1)
8:	break
9:	θ - θ - η Pm=I vθ'(f, x(KdvMi)Im
10:	until training converged
adversarial robust risk comPared with frob, a direct minimizer of (1). However, Theorem 2 shows
fB is equally or even more robust than frob excePt on a certain area. In Particular, Theorem 2
shows that fB is the most robust model excePt on DB(fB, ). Therefore, fB is most robust almost
everywhere if the decision boundary offB can lie outside ofB(X, ), so that X \ DB(fB, ) = X.
For examPle, this haPPens when each class has its own suPPort aPart from each other by at least 2.
The next theorem shows that even when we only have data from PX,Y restricted on X × Y , we can
find frsob, the oPtimal function w.r.t. PX,Y .
Theorem 3. Let Ae = {f ∈ F ∣ Pf(χ) = fB(X) ∀χ ∈ Sχ,e(fB)) = 1} and TR%(f)=
EPX γ ['rob,e(f, X，Y)]. Then, for any e > 0, RSob(f) is only minimized by any f ∈ Ae. Further-
more, if B(X, E) ⊃ X, fB is the unique minimizer of TR"(f).
Remark 3. It is interesting to comPare our work with that of Suggala et al. (2018). We notice there
are some critical differences between these two works. Given Y ∈ {-1, 1}, Suggala et al. (2018)
define an adversarial risk as
Radv (f) = E[
max
g(x)=g(x+δ),kδk≤e
'(f (X + δx), g(X))- '(f (X), g(X))]∙
Unlike our definition of sensible adversary, if g is a Bayes rule fB, their adversary tries to increase
the loss w.r.t. not y but a deterministic function of X. Therefore, for making fB being an oPtimal
robust model, their objective should always have an additional term, e.g., Rnat (f) + λRadv (f) for
0 < λ < ∞ because an f s.t. f(X) 6= g(X) w.P. 1 can minimize Radv (f). Unlike their aPProach,
our sensible adversarial risk (3) can alone be oPtimized making the Bayes rule as the oPtimal model.
4 Algorithm
A transition from theory to algorithm Poses two main challenges. First, the 0-1 loss function in the
theory is hard to oPtimize. Therefore, as a common Practice, we use the cross-entroPy loss. Second,
fB on the entire sPace is Practically unavailable. We note that a model that Performs well on natural
data can be a nice approximation of fB on the restricted support X XY. Therefore, We generalize
sensible adversary in (2) to utilize a general loss function and a reference model that substitutes fB .
Definition 3. (generalized sensible adversarial example) Consider a loss function `. For a classi-
fier f and C ∈ (θ, 1], let Sx,e(f) = {z ∈ X∣∣∣z - XIlp ≤ 3 '(f(z),y) ≤ log 1} ∪ {x}. Thengivena
reference model fr, the sensible adversarial example of f for (X, y) is defined as
1x,
arg max `(f (z), y),
z∈Sx,(fr)
if fr(X) 6= y
otherWise.
(4)
If ' is the cross-entropy loss, the condition '(f(x), y) ≤ log C is equivalent to Pfr,y(x) ≥ c. In
binary case With c = 0.5, this requires the perturbed examples not to cross the decision boundary of
fr, and for general c, not to reach to a vicinity of the boundary.
5
Under review as a conference paper at ICLR 2020
Table 1: The test result on natural examples and '∞-attacks for CIFAR10 (E = 8/255). The PGD
attacks are generated with 20 random restarts and counted the worst case only.
	NAT	CW40	DeepFool	FGSM	LBFGS	MIFGSM	PGD100
SENSE	91.51	67.01	78.89	72.72	85.94	68.87	57.23
TRADE	84.92	62.19	61.38	61.06	81.58	57.95	54.72
Table 2: The transfer attack results between the TRADE and SENSE model on CIFAR10. The
'∞ PGD40 and MIFGSM attacks are generated. The subscripts of the column names denote the
generating model. The denominator and numerator in each cell are the number of adversarial attacks
and correct predictions respectively (E = 8/255).
Defence model	PGDSENSE	PGDTRADE	MIFGSMSENSE	MIFGSMT RADE
TRADE	7831/10000	5655/10000	1584/3113	5888/10000
SENSE	6499/10000	6961/10000	6887/10000	2916/4112
In our algorithm we set fr as a naturally trained model ora current model. Given a fr, the implemen-
tation of sensible adversarial attacks is straightforward. For a correctly classified natural example by
fr, we add perturbations in the similar way to the PGD method (Madry et al., 2017). The difference
is that during the K-step of PGD iterations, once the loss of a currently generated example exceeds
log 1, We reverse it back to the previous step and break the iteration. This requires no additional
forward- or backward-propagation, compared with the PGD method. The proposed algorithm is in
Algorithm 1 for the '∞-norm and in Algorithm 2 in Appendix C for the other 'p-norms.
The definition of the sensible adversary in (4) divides the data into three subsets: Afr = {x|fr(x) 6=
y or Sχ,e(fr) = {x}}, Bfr = {χ∣Sχ,e(fr) = SxQ Sχ,e(fr) = {χ}},and Cfr = X\(Af ∪ Bf).
Therefore, our algorithms generate a sensible adversarial example xS|fr = f in three different Ways:
(1) Xs = X for X ∈ Af, (2) Xs = Xp for X ∈ Bf, and (3) Xs = Xp for X ∈ Cf, where Xp is a
full K-step PGD attack. Therefore, sensible adversarial training inherently involves a set selection
mechanism. The sensible adversarial loss `s(f, X, y) can be written as
'(f,X,y)1x∈Af + '(f,Xs,y)1x∈Bf + '(f,Xp,y)1x∈Cf
='(f, x, y)1'(f ,x,y)>log C + '(f, x , y)1xp=xs , '(f,x,,y)≤log 1 + '(f, XJ y)1'(f ,æp ,y)≤log C .	(5)
When c ≥ 0.5, this optimization problem has a strong motivation to increase |Cf |, i.e., the number
of sensible adversarial attacks that are identical to full PGD attacks. Because Af = {x∣'(∕(x), y) >
log 1} for C ≥ 0.5, the loss on the natural stage Af is always greater than the loss of the sensibly
reversed stage Bf and the full PGD stage Cf. Furthermore, the loss for X ∈ Bf is always ap-
proximately log 1 by adaptive sensible perturbations. Therefore, paradoxically, the smallest loss is
c
only achievable by full PGD examples. In other words, sensible adversarial training penalizes when
X 6∈ Cf . We note that the optimization problem has a smooth landscape; Although the data points
may jump around between Af, Bf, and Cf, there is no obvious discontinuity in the loss. Therefore,
during the training, 's (f, x, y) smoothly slides down, making both '(f, x, y) and '(f, Xp, y) smaller
than log 1. This enables to learn a naturally and adversarially accurate model. More discussion on
the stability of our algorithms are presented in Appendix D.
5	Experiments
5.1 EXPERIMENT 1: ROBUSTNESS AGAINST E-BALL ATTACKS
CIFAR10 We train WideResNet-34-10 (He et al., 2016) with sensible adversarial examples of
E = 8/255 and c = 0.7 with the step number K = 10 and size η1 = E/5. We attack our model
with various white-box attacks with perturbations restricted to '∞ with E = 8/255. We compare
the performance with TRADE using the same architecture (Zhang et al., 2019), which is know to
be robust and accurate. The result is in Table 1. Our model achieves 91.51% natural accuracy. This
is 3.7% drop in natural accuracy from 95.2%, which is an accuracy that a naturally trained model
can achieve (Madry et al., 2017). With this architecture, Madry et al. (2017) achieve 47.04% robust
6
Under review as a conference paper at ICLR 2020
Table 3: MNIST: test results of our models on natural examples and '∞ based attacks (e = 0.3).
Defence model	Natural	PGD500	C&W40
SENSE	99.51	91.74	96.02
TRADE	99.48	93.30	96.90
Figure 2: The natural and robust accuracies of our models for the varying parameter c.
accuracy against PGD20 attacks and 87.3% natural accuracy. As a black-box attack, we try transfer
attacks between the TRADE and SENSE model by the '∞ based PGD and MIFGSM, which are
known to be effective for transfer attack (e = 8/255). We obtain adversarial examples by applying
PGD and MIFGSM on a generating model, and then use the examples to attack a defense model.
The result is in Table 2. Overall we observe that our model outperforms both the TRADE and Madry
model. In particular, sensible adversary achieves more than 55% of robust accuracy against PGD
attacks of e = 8/255. This performance is consistent to the test margin distribution in Figure 8, and
this is discussed in detail in Appendix E. For PGD100, we conduct random 20 restarts and count
only the worst case. The step number 100 and step size 2/255 of the PGD attacks are justified by the
plot in Figure 11 in Appendix H.
MNIST We consider a CNN model with three convolutional layers followed by a fully connected
linear layer, which is the same architecture in (Zhang et al., 2019). We train an MNIST model with
sensible adversarial examples of e = 0.3 and c = 0.5 with the step number K = 10 and size
η1 = 0.05. The robust test result in Table 3 shows the comparable performance of our model. The
step size of the PGD500 attack is 0.01, and the serenity check on the PGD attack is in Appendix H.
We conduct 100 random restarts and count only the worst case for each test example.
5.2 Experiment 2: Sensitivity analysis
MNIST: We perform the sensibility analysis to understand the effect ofc and the model capacity for
two reasons. First, the sensible training prevents full PGD perturbations on an example until the loss
on it becomes less than log 1, which could hardly happen for a model with a small capacity. Second,
as Madry et al. (2017) point out, when the model capacity is insufficient for adversarial learning, the
model collapses to a constant function. We are interested in a range of c that keeps sensible learning
from collapsing. Therefore, we consider a sequence of CNNs of the increasing number of kernels
similar to Madry et al. (2017), where we denote the capacity by q ∈ {1, 2, 3, 4, 5}. The details of
the model architectures are presented in Appendix H. When we train them with natural examples,
the networks of capacity 1 and 2 achieve about 95% and 97% accuracy, whereas the networks of the
other capacities achieve more than 99%. When trained with regular PGD examples, the networks
with capacity 1,2, and 3 collapse. Therefore, capacity 3 is enough only for natural learning, and
capacities 1 and 2 are possibly insufficient even for natural learning.
7
Under review as a conference paper at ICLR 2020
capacity 5
capacity 5
AI-SUBP
Figure 3: The prediction margins at convergence of capacity 5 on the test set. The natural margin of
a model f at (x, y) is logpf,y (x) - maxyθ=y logpfy (x). The adversarial margin is calculated by
logPf,y (Xp) - maxyo=y logpf,yo (Xp), where Xp is a full PGD attack.
For each capacity q, we train the models with sensible adversarial examples on MNIST with the
hyperparameters C ∈ {0,0.1,…，0.9,1}. Note that sensible adversarial training is identical to
natural training when c = 1, and to adversarial learning without any perturbations for incorrectly
classified natural examples when c = 0. Figure 2 shows the natural and robust accuracy against
PGD-40 attacks with varying c for each capacity. In general, the natural accuracy tends to increase
as c increases while robustness decreases. For capacities 3, 4 and 5, the accuracies are almost
insensitive to varying c. On the other hand, the tendency is most distinct in capacity 1. In this case,
the network obtains best robustness when c = 0.5, which is the least loss bound among c ≥ 0.5, the
range of c with stable learning property. Even for capacity 1, however, the sensible learning does
not collapse except when c = 0. We also see that c = 0.5 is least sensitive to the model capacity.
Note that capacities 2 and 3 do not collapse even with c = 0. When c = 0, the only difference of
the sensible adversarial learning from the regular PGD training is that the sensible learning requires
robustness only for the correctly classified natural examples. When c = 0, at the convergence of the
models of capacities 1 and 2, about 5% of the data points are allowed to be free from the perturbation.
By paying only the 5% of robust training accuracy, the sensible learning avoids collapsing and
obtains about 80% of robust accuracy and 90% of natural accuracy.
Intuitively, an insufficient model capacity or locally close class manifolds can make a virtual deci-
sion boundary that is inevitable to keep a nice natural performance. In the algorithm, the sensible
reversion prevents adversarial examples from crossing this boundary. This effectively reduces the
requirement of the model capacity posed by the regular adversarial learning. The sensible reversion
also allows a robust model to have larger margins than the PGD trained model for the majority of
the dataset as in Figure 3. The PGD trained model has majority adversarial margins as positive.
Instead, it has much smaller natural margins than the naturally trained model. The SENSE model
with c = 0.5 has comparably large adversarial and natural margins. Instead, the number of data
points with negative adversarial margins is larger than that of the PGD trained model. As c increases
to 0.9, this model has much smaller and more negative adversarial margins. On the other hand, its
two types of margins are generally even larger. This phenomenon is consistent to the decreasing
robustness in Figure 2 for capacity 5. In general, for a fixed capacity, increasing c increases the
natural and adversarial margins of the majority of the data, while it also increases the portion of data
of negative adversarial margins.
The cost for this highly confident prediction is the robustness near the decision boundary at con-
vergence, i.e., the portion of the data points in the natural and sensibly reversed stage. When we
consider the margin on the training set, we find that there is a linear relationship between the portion
of the data points having negative adversarial margins and the test accuracy. In practice, such a por-
tion can be an indicator about the robustness of the model or the sufficiency of the model capacity,
without directly testing the model.
8
Under review as a conference paper at ICLR 2020
Table 4: The sensitivity of C on CIFAR models of WideResNet
CIFAR WideResNet	c=0.0	c=0.3	c=0.5	c=0.6	c=0.7	c=0.8
Natural data	82.88	86.76	90.42	90.87	91.51	92.35
PGD100	43.70	46.90	50.95	55.90	57.80	55.60
Table 5: The sensitivity of C on CIFAR models with CNNs
CIFAR CNNs	c=0.1	c=0.5	c=0.9	natural training
NAT	66.26	75.70	82.02	80.85
PGD40	26.67	20.26	3.95	0.00
CIFAR: We train train WideResNet-34-10 with several different c values. Except that for c = 0.8
we stopped learning at 120 epoch, other models are trained for 300 epochs. We report the adversarial
accuracy against and PGD100 attacks with a step size η1 = 2/255 for the first 2000 test examples,
with random 20 restarts. The results are in Table 4.
Interestingly, while the natural accuracy positively correlated to the C value, the robustness does not
show clear negative correlation to C. Rather, as C become closer to 0.7, more robust result the model
shows. As we see that C = 0.8 has better robustness than C ≤ 0.5, in CIFAR, the main reason of the
observed trade-off could attribute to the influential adversarial perturbations.
For the CIFAR dataset, we also try to train a robust model with a small capacity. We intentionally
choose the CNNs model that is used in Experiment 1 for MNIST. This model can be not enough
for adversarial training on CIFAR. We report the results in Table 5. Given the serious lack of model
capacity, the decrease in natural accuracy of SENSE models compared with the naturally trained
model is not serious, while achieving better robust accuracy numbers. Note that none of our models
collapse, and therefore the possibility of sensible adversarial training is not sensitive to the choice
of C even in the lack of model capacity.
6 Concluding remarks
In this paper, we proposed a sensible adversary which is useful for learning a defense model, keep-
ing a high natural accuracy simultaneously. We theoretically establish that the Bayes rule is most
robust under the framework of sensible adversarial learning. Our learning algorithm is efficient and
stable, and not sensitive to the choice of the main hyperparameter C. Also, C has a clear meaning as
the lowest prediction-probability bound. Our empirical experiments yield state-of-the-art results of
adversarial learning on the CIFAR10 and MNIST datasets. In addition, we showed that the sensible
approach can effectively deal with the lack of model capacity. This is because by paying a robust
accuracy on a certain area, the algorithm protects the model from being collapsed by influential
adversarial examples. Furthermore, the sensible adversarial learning trains a model to have high
prediction margins on both natural and adversarial examples.
We now mention several future directions for research on sensible adversary. One remaining the-
oretical problem is to develop generalization error bounds for sensible adversary learning, so that
we can theoretically justify our empirical performance. In fact, in this work, we did not tackle the
lack of sample size. In particular, as our algorithm tends to produce a model with large natural and
adversarial prediction margins, in the lack of sample size, it is not clear if this large margins are
always beneficial. Therefore, there is much remaining work to be done to theoretically understand
the high margin tendency of the models trained with sensible adversarial examples with relation to
generalization.
9
Under review as a conference paper at ICLR 2020
References
BattiSta Biggio, Igino Corona, Davide MaiorCa, Blaine Nelson, Nedim SrndiC, Pavel Laskov, Giorgio GiaCinto,
and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on
machine learning and knowledge discovery in databases, pp. 387-402. Springer, 2013.
Sebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints.
arXiv preprint arXiv:1805.10204, 2018.
Gavin Weiguang Ding, Kry Yik Chau Lui, Xiaomeng Jin, Luyu Wang, and Ruitong Huang. On the sensitivity
of adversarial robustness to input data distributions. arXiv preprint arXiv:1902.08336, 2019a.
Gavin Weiguang Ding, Luyu Wang, and Xiaomeng Jin. AdverTorch v0.1: An adversarial robustness toolbox
based on pytorch. arXiv preprint arXiv:1902.07623, 2019b.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting ad-
versarial attacks with momentum. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 9185-9193, 2018.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian
Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report,
Citeseer, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint
arXiv:1607.02533, 2016a.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint
arXiv:1611.01236, 2016b.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [Online]. Avail-
able: http://yann. lecun. com/exdb/mnist, 2:18, 2010.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 2574-2582, 2016.
Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi,
Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian Molloy, and Ben Edwards. Adversarial robustness
toolbox v0.8.0. CoRR, 1807.01069, 2018. URL https://arxiv.org/pdf/1807.01069.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami.
The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and
Privacy (EuroS&P), pp. 372-387. IEEE, 2016.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox v0.8.0: A python toolbox to benchmark the
robustness of machine learning models. CoRR, abs/1707.04131, 2017. URL http://arxiv.org/abs/
1707.04131.
Dale Schuurmans Ruitong Huang, Bing Xu and Csaba Szepesvari. Learning with a strong adversary. arXiv
preprint arXiv:1511.03034, 2015.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially
robust generalization requires more data. In Advances in Neural Information Processing Systems, pp. 5014-
5026, 2018.
David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6976-6987, 2019.
10
Under review as a conference paper at ICLR 2020
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the cost of
accuracy?-a comprehensive study on the robustness of18 deep image classification models. In Proceedings
of the European Conference on Computer Vision (ECCV), pp. 631-648, 2018.
Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, and Pradeep Ravikumar. Revisiting adversarial risk.
arXiv preprint arXiv:1806.02924, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. In 2016 International Joint
Conference on Neural Networks (IJCNN), pp. 426-433. IEEE, 2016.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness
may be at odds with accuracy. stat, 1050:11, 2018.
Dong Yin, Kannan Ramchandran, and Peter Bartlett. Rademacher complexity for adversarially robust general-
ization. arXiv preprint arXiv:1810.11914, 2018.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoreti-
cally principled trade-off between robustness and accuracy. arXiv preprint arXiv:1901.08573, 2019.
11
Under review as a conference paper at ICLR 2020
Appendices
A	Additional related work
Bubeck et al. (2018) conjecture that learning a robust model is information theoretically possible but computa-
tionally intractable. They introduce an example which is not robustly learnable in polynomial time. Our view
is consistent with Bubeck et al. (2018) in that a robust model exists, but we search for an efficient algorithm
to estimate a robust model in a reasonable time. Su et al. (2018) compared various naturally trained models
on ImageNet, and found that the trade-off varies among different model architectures. Also, they empirically
discovered that more accurate models tend to be less robust when the models are trained with natural examples.
Stutz et al. (2019) demonstrate that given a large training set, adversarial training can produce a robust model
that is as accurate as a naturally trained model. Our work to maximize the synergistic effect between natural
and adversarial accuracy is consistent to their demonstration. Also, they show that most of PGD attacks are off-
manifold of the original data, and by on-manifold adversarial training, the natural accuracy can be improved.
c
If we see the restricted space X as the data manifold, and Xc as off-manifold, our sensible framework aligns
with their view, given the data manifolds of different classes are separated by at least 2. Kurakin et al. (2016b)
suggest adversarial learning that trains with data randomly divided into two parts, a natural and adversarial
set. We divide data in a data adaptive way into three parts including a sensibly reversed adversarial set. The
relationship between their approach and our algorithm is discussed more in Appendix F.
B Proofs of Theorems
To save the space, We use η(k∣x) to denote P(Y = k|X = x).
Theorem 1.	(Restated) Let R；td denote the minimum standard risk which is RStd(fB). Then we have
Rrob(fB ) = RStd. Furthermore, f* * B is the unique minimizer of Rrsob (f) among f ∈ F.
Proof.
RSob(fB ) = P(fB(X)= Y)
=P(fB(X) 6=Y)+P(fB(X) =Y,and ∃x0 ∈ SX,(fB)s.t.fB(x0) 6=Y)
= P(fB(X) 6= Y) + 0,	by the definition of SX,(fB)
= Rstd(fB) =
Rstd.
It is obvious that fB is a minimizer of Rsrob(f) because RsStd is a lower bound of Rrsob (f) for any f ∈ F.
Now we show fB is the unique minimizer of Rrsob (f).
Rrsob(f) = P(fB(X) 6=Y,f(X) 6=Y)+P(fB(X) =Y,∃x0 ∈ SX,(fB) s.t. f(x0) 6=Y)
K
X P(fB (X) 6=k,f(X) 6=k,Y=k)+P(fB(X) =k,∃x0 ∈ SX,(fB)s.t.f(x0) 6=k,Y=k)
k=1
K
=	P(fB(X) 6= k, f(X) 6=k,Y=k|X =X)
+ P(fB (X) = k, ∃X0 ∈ Sx,(fB) s.t. f(X0) 6= k,Y = k|X = X)dP(X)
K
=〉:I IfB(X) = k,f(x) = kn(k|X) + 1f B (x) = k,∃x0∈Sχ,e(f B ) s.t. f(x0) = kn(k|X)dP(X)
k=1 X
RStdf) + X 1 IfB (χ) = k (I∃x0∈Sχ,e(fB ) s.t. f (χ0) = k - 1f (x) = k)n(k|X)dP(X)
k=1 X
(6)
The last equality is by1fB(x)6=k,f(x)6=k = (1	-1fB(x)=k)1f(x)6=k.
The first term RStd (f) is uniquely minimized by the Bayes rule fB . The second term is always non-negative,
and is zero when f = fB . Therefore, RSrob (f) is uniquely minimized by fB .
□
12
Under review as a conference paper at ICLR 2020
Theorem 2.	(Restated) Let SX, be an -ball centered at X. Then for any f ∈ F and for any set A ⊂
X\DB(fB,),
P ∃ x0 ∈ SX, s.t. fB(x0) 6= Y,X	∈ A	≤ P ∃ x0 ∈	SX, s.t. f(x0) 6= Y,X ∈ A	.
Proof. For any set A ⊂	X\DB(fB, ), P(∃x0	∈ SX,	s.t. fB(x0)	6= Y,X ∈ A) = P(fB(X)	6=	Y,X	∈	A).
Note that on any subset B ⊂ X, the Bayes rule has the least error probability. Therefore, P(fB (X) 6= Y, X ∈
A) ≤ P(f (X) 6= Y, X	∈ A) ≤ P(∃x0 ∈ SX,	s.t. f (x0) 6= Y, X	∈ A). The last inequality is	trivial	because
if f (X) = Y ⇒∃x0 ∈	Sχ,e St. f(x0) = Y.'	□
Theorem 3. (Restated) Let Ae = {f ∈ F∣P(f(x) = fB(x), ∀x ∈ Sχ,e(fB)) = l} and RR%(f)=
EPX V ['rob,e(f, X，Y)]∙ Then, for any e > 0, R；0b(f) is only minimized by any f ∈ Ae. Furthermore, if
Β(X,c) ⊃ X, f B isthe Uniqueminimizerof TRS°b(f).
Proof. The sensible risk of f w.r.t. the restricted distribution corresponding to (6) is
R sob(f ) = R std(f ) + X /1f B (χ) = k(1∃χ0∈sx,e(f B ) s.t. f (χ0 ) = k - 1f (x) = k )n(k|X)dp(X)
(7)
K
R std(f) + I yZ 1f B (x) = k(1∃X0∈Sχ,e(f B ) s.t. f (x0 ) = fB(x) - 1f (x) = fB(x)
JX k = 1
)η(f B (x)∣x)dP(x)

Rstd⑺+ ∕~ (I∃χ0∈Sχ (fB ) s.t.
X
B
f(x0)6=fB(x) - 1f(x)6=fB(x))η(f (X)|X)dP(X)
(8)
because P(Y = k|X = X)= P(Y = k|X = x) for X ∈ X. The minimum of R£b(f) is achieved by f B
*	B	*
with the minimum value R = Rstd(f B) + 0. Therefore, any function f that Rstd(f) > R cannot achieve
s
the minimum of Rrsob(f) because the term (1∃x0∈Sx,(fB) s.t. f(x0)6=fB(x) - 1f(x)6=fB(x)) in (8) is always
non-negative. Therefore, only functions in A = {f ∈ F∣P(f (X) = f B (X)) = 1} need to be considered as
s
possible minimizers of Rrsob (f).
Note that Ae ⊂ A. By the definition, we know that f ∈ A \ Ae if and only if
i)P(f(X ) = f B (X)) = 1
ii)P(f(x) = f B (x), ∀X ∈ Sχ,e(f B)) < 1
Therefore, for f ∈ A\Ae, ∃A ⊂ X s.t. P(X ∈ A) > 0 and ∃x0 ∈ Sχ,e(fB) for∀x ∈ A s.t. f (x0) = fB (x0).
s*
For this f, the equation in (8) can be written as Rrsob (f) = R* +α for some α ≥ 0. Now we show that α > 0.
Note that by the definition of the Bayes rule, P(y = fB(x)∣X = x) ≥ K. Otherwise, 1 = PK=I P(y =
k|X = x) ≤ KP(y = f B (x)|X = x) < 1, which is contradict. Then, for the f ∈ A\Ae and A ⊂ X that
are described above,
(1∃x0 ∈Sx, (fB ) s.t. f(x0)6=fB (x) - 1f (x)6=f B (x) )η(f (x)|x)dP(x)
≥ K L (I∃X0∈Sχ,e(fB ) s.t. f(χ0) = f B (χ) - 1f (χ) = fB(χ))dP(X)
=K	~Q∃X0∈Sχ,e(fB ) s.t. f(x0) = f B (x))dP(X) by f ∈ A
X
1	P(A)
≥ K Jʌ(I∃X0∈Sχ,e(fB ) s.t. f (x0) = fB(x))dP(X) =	> 0.
Therefore, α > 0. Note that for any f ∈ Ae, the second term in (8) is zero by the definition of Ae. Therefore,
first result of the theorem is proved. Furthermore, for e such that Β(X, c) ⊃ X, Ae = {f B }. Therefore, when
Β(X, e) ⊃ X, f B is the unique minimizer of R£b(f).	□
Theorem 4. Let A = {f ∈ F∣P(f(X) = f B (X)) = 1}, and take a reference function fr ∈ A. Consider
extended sensible adversarial examples, with c < 1 and ' as the 0-1 loss. Then, for any e > 0, RS°b(f |fr)=
EP ['(f (X|f , Y)] is uniquely minimized by fr.
PX,Y	r
13
Under review as a conference paper at ICLR 2020
Theorem 4 says that for fr ∈ A, which behaves as the Bayes rule fB on the restricted support, the correspond-
ing sensible adversarial risk EPX γ ['(f (X | fr, Y)] is minimized only by fr. This implies that if We do not have
cc
any information about the Bayes rule on Xc, the sensibly optimal model W.r.t. PX,Y can be arbitrary on Xc
although this optimal model is the Bayes rule on X. Our algorithm deals with this arbitrariness by searching
for a better reference function in each iteration. As a current model is used as a reference function, i.e., the
estimation of the defense model and the reference model is identical, the algorithm essentially pursues sensi-
bleness on X and robustness on X c of the trained model. Note that on X , sensibleness a sufficient condition
for natural accuracy.
Proof. By using the same way to derive (6) and (8) and noting that f (x) = f B (x) on X, we get
RR Sob(flfr) = RR std(f) + XL 1f B (X) = k(l∃χθ∈Sχ,f ) s.t. f(χ0) = k - 1f (x) = k )η(k∣χ)dP(χ)
B
= Rstd (f) +	(1∃x0∈Sx(fr) s.t. f(x0)6=fB(x) - 1f(x)6=fB (x))η(f (x)|x)dP(x)	(9)
JX
Because fr(x) = fB(X) on X, RRstd(fr) = RRstd(fB). This implies fr minimizes RR“(flfr) because
RRstd(f B) is the minimum of RRStd(f) for all f ∈ F and the second term in (9) is non-negative. This implies
Bs
any function f s.t.	RStd (f) > RStd (fB) cannot achieve the minimum of RrSob(f|fr). Therefore, as a
S
minimizer of RrSob(f|fr), we only need to consider f ∈ A. Note that 1f (x)6=f B (x) = 0 for f ∈ A on X.
Therefore, by letting RRStd be TRstd(f B), the equation in (9) can be written as
Rrob(f|fr )=R Std + 1 1∃χθ∈Sχt(fr ) S.t.f(χ0 ) = f B (x)η(f B (x)∣x)dP(x)
JX	,
=RStd+L
1∃x0 ∈Sx, (fr) S.t.
(10)
Note that η(fB (x)|x) is positive on X. Therefore, for f ∈ A to minimize (10), it must satisfy that P(∃x0 ∈
SX,(fr) s.t. f(x0) 6= fr (x0) = 0. This essentially says f should be fr.
□
C Algorithm
Algorithm 2 Sensible adversarial training for `p norm restriction
Input: Initialized f = fθ, c ∈ (0, 1), step number and sizes K, η1, η2, data Xa(0d)v = X
2: repeat
for i = 1, ..., m, s.t. f (xi,adv) = yi
4:	for k = 1, ..., K
(k) Jn / JE ▽”'(f(Xi,adv)),yi)	, (k-1八 π. th nmiPPtinn CnerntCr
xi,adv J nBp(Xi,e)(η1 kVx'(f(χikadV)),yi)kp + xi,adv ), ":眈 PrOjeCtion OPerator
6:	if '(f,x(,k)dv,yi) > logC
(sensible reversion) xi(,Kad)v = xi(,ka-dv1)
8:	break
θ Jθ - η Pm=I vθ '(f, X(Kdv, yi)/m
10: until training converged
Algorithm 2 is a straightforward extension of Algorithm 1 from '∞ norm to 'p norm. In addition, we present
another way to realize the sensible reversion. After stepping back when the loss exceeds the threshold, we can
add a random noise as the next step instead of just breaking the iteration as in Algorithm 1 and 2. Considering
the nature of (mini-) batch leaning, this random noise does not add much computational cost because until
every loss exceeds the threshold, the for loop would keep calculating the forward and backward loop with fixed
perturbation for the reversed examples. This algorithm for '∞ norm is presented in Algorithm 3, of which the
objective function is also (5). Algorithm 3 also reverses the adversarial example when the loss is greater than
log C. The only difference is that instead of breaking the iteration after the reversion, it considers a random
noise as the next step. This can lead to more effective search for the local loss maximizer in the ball x, (f) in
definition 4. The extension of Algorithm 3 to `p norm is straight forward, and thus omitted.
14
Under review as a conference paper at ICLR 2020
Algorithm 3 Sensible adversarial training for '∞ norm restriction
3:
6:
9:
12:
Input: Initialized f = fθ, c ∈ (0, 1), step number K, step sizes η1, η2, data Xa(0d)v = X
repeat
for i = 1, ..., m, s.t. f (xi,adv) = yi
for k = 1, ..., K
Xi — ∏B(XiQ(Step(k) + x(kad1)), ∏: the projection operator
if '(f,Xi,yi) ≤ log C
(k)
xi,adv = xi
Step(k+1) = ηιsign(Vχ'(f (Xika)dv ),yi))
else
X(k)	= X(k-1)
Xi,adv = Xi,adv
Stepi(k+1) = random noise
θ J θ - η Pm=ι vθ'(f, X(Kdv, yi)/m
until training converged
D	The landscape for Sensible robust optimization
Note that when the original examples are incorrectly classified, full PGD examples can be very influential in
regular adversarial training. Ifwe do not add any perturbations on these potentially influential examples and add
full PGD perturbations on the other examples, the resultant examples are equivalent to the sensible adversarial
examples when c = 0. Experiment 1 shows this small change keeps the models from collapsing, demonstrating
how influential the PGD perturbations on the incorrectly classified natural examples. However, an adversarial
training with the sensible adversarial examples with c = 0, the empirical loss can largely fluctuate during the
training; Once an incorrectly classified natural example becomes correctly classified, its full PGD attack can
pose a sudden large gradient for the model update. Then, when it is incorrectly classified again, the loss on it
suddenly reduces to the natural loss that is distinctly smaller than the large loss value on its full PGD attack.
Therefore, whenever an example changes its state between a correctly and incorrectly classified example, i.e.,
the full PGD and natural stage, the corresponding loss can fluctuate making the learning unstable.
However, if we add sensible reversion step, particularly if c ≥ 0.5, this fluctuation does not occur. Between
the two stages, the sensibly reversed stage provides a kind of cushion between the natural and full PGD stage,
preventing the sudden change in the loss value as described. Until Sx, (f) = Sx,, the sensibly adversarial
perturbation for X is adapted to make the loss of the current function approximately equal to log C. As the
abstraction in Figure 4, it is like to have a virtually extended area at pf,y (x) = C on the loss function that
is locally flat on {Xs∣Sχ,e(f) = Sχ,e}, i.e., on a set of Xs that are in the sensibly reversed stage. However,
in spite of the existence of such a flat loss area, the model still can learn with Xs in sensibly reversed stage.
This is obvious because the cross-entropy loss has its non-zero gradient when it is log 1. Interestingly, when
the model is updated in a way to decrease the loss of the previous Xs, the new sensible perturbation is again
adapted to have the loss approximately equal to log C. Therefore, the course of training directs the model to
have sensible adversarial examples in the full PGD stage. Furthermore, as long as c > 0, even when Xs is in a
sensibly reversed stage, it still has perturbation in an adapted -ball. This helps to obtain a robustness although
it is not on a full ball.
Note that for c ≥ 0.5, the learning is very stable because only natural examples can overpower the training;
The large loss values are only achievable by sensible attack in the natural stage. Any single full PGD attack
cannot dominate the next update of the training because the loss function has relatively small gradients on the
full PGD stage. Therefore, our algorithms not only effectively ignore any influential full PGD attacks that may
overpower the next update but also train a model that allows as many sensible attacks to be full PGD attacks.
In other words, our algorithms can stably learn a robust model.
E	High margin property of Sensible adversarial training
In sensible adversarial learning, the natural accuracy clearly takes priority over the adversarial accuracy. The
perturbed example is not allowed to cross the decision boundary of the Bayes rule in (2) or to reach the vicinity
of the decision boundary of a reference function in (4). As mentioned, when the cross-entropy loss is used,
pf,y (x) ≥ c is a prerequisite for adding any adversarial perturbation on x. We note that this condition provides
15
Under review as a conference paper at ICLR 2020
An abstraction of the sensible adversarial loss
Natural
Sensibly revesed
Full PGD
Figure 4: An abstraction of the sensible adversarial loss when c ≥ 0.5. When x is in the sensibly
reversed stage for a current model f, the loss of Xs is approximately log C. Although the loss
is approximately the same while x stays in this stage, the model updates in a way to pushes the
sensible adversarial example to become a full PGD attack as the arrow.
pf,y(X)	≥ log 二
1 - Pfy(X) -	1 - C
a lower bound of the following natural margin as
M(f,x,y) = logp^f,y(x) - maxlogPf,y0 (x) ≥ log
y0 6=y
Therefore, the priority of natural accuracy hints that the learning will prevent the natural margin from being
sacrificed for the sake of adversarial robustness. We note that the natural margin of x is an upper bound of its
adversarial margin. Therefore, if a model cannot confidently predict a natural example x, neither can the model
confidently predict any adversarial examples of x.
Experiment 1 We investigate the margins of the models trained in Experiment 2, to understand the effect of
c. In Figure 5 we draw density plots of the margins on the test set for varying c for the fixed capacities. Overall,
we see that a larger c results in a larger adversarial margins and natural margins. In Figure 6, we also draw
the margins but for varying model capacity. In general, for a each c, a smaller capacity has more data points
of negative margins. However, for naturally trained models, i.e., the models with c = 1.0, a larger model has
smaller adversarial margins. This is consistent to the observation of Su et al. (2018) that accurate models tend
to be less robust when the models are trained with natural examples. On the other hand, although not displayed,
the plots corresponding to c = 0.9 are essentially similar to the plots in the second low. This implies even for
large c, our method is not like natural learning; The models trained with c = 0.9 still have larger natural and
adversarial margins.
Note that in Figure 3, the regular PGD model of capacity 5 has only few negative adversarial margins. Instead,
their natural margins significantly smaller than those of the natural models. In contrast, our model has negative
adversarial margins for a few more data points. Note that the majority of both natural and adversarial margins
of the our models are significantly larger than that of the regular PGD models. For capacity 1,2 and 3, the
regular PGD models collapse having small ”mean” of adversarial losses. On the contrary, our models deal with
the lack of model capacity by letting more portion of examples to have negative margins. As demonstrated by
Figure 5, in our methods the mean adversarial loss can be arbitrarily large. Instead, it maintains a large portion
of points having relatively large adversarial margins, i.e., being far from decision boundaries.
The flip side of the advantage of high margin is the possibility of over fitting. For c ≥ 0.5 in Figure 7, the best
robust accuracies are not achieved when capacity is 5 but when capacity is 3. On the contrary, the PGD method
achieves better robustness as the capacity increases. Therefore, there is a possibility of over-fitting problem that
arises form the high margin property of sensible adversarial learning.
16
Under review as a conference paper at ICLR 2020
0.05-
0.00-
0.06-
capacity 1
parameter c
capacity 1
Al-suəp
capacιty 3
0.025-
0.020-
0.015-
0.010-
0.005-
0.000-
capacιty 5
Natural margin
o.oo-
margin
capacity 3
0.04-
0.02-
0.00-
0.02-
0.01-
□ 0.5
Figure 5:	The prediction margins at convergence of the models in Experiment 1 on the test set.
Experiment 2 In Figure 8, we draw the plot of natural and adversarial margins on the testset of CIFAR10.
Compared with the distribution of the TRADE model, the natural margin of the SENSE model is large. For
the adversarial margins, the SENSE model has two clearly separated clusters; one is of negative margins and
the other is of positive margins. Instead, the positive margins are distributed on the larger values. We remark
that this phenomenon is consistent to the sensible idea to allow to be fooled near the decision boundary of the
Bayes rule. When the capacity is not enough, the concept of the decision boundary is not of the Bayes rule. Its
is projected to an inevitable boundary of a model with nice natural performance, caused by the lack of model
capacity. The portion of negative adversarial margin is a cost for sensibility and with this cost, the model can
obtain robustness as much as possible given a model capacity. We see the portion of adversarial margins of the
SENSE model in the negative area in Figure 8 is not small. This may imply the current model capacity and the
sample size for SENSE are not enough.
F Comparison with other methods
The sensible selection and reversion in our approach distinguish sensible learning from other approaches that
balance between the natural and adversarial accuracy. The objective function of our algorithm can be rewritten
as follows. For f ∈ F ,
1n
L(f) = n £'s (f,χi,yi)
n i=1
=M Rstd(f|Af)+
~ lRrob(f ∖Bf ) + lCfl RR rob(f ∖Cf ),
(11)
where `s (f, x, y) is defined in (5).
17
Under review as a conference paper at ICLR 2020
The margin distributions for the varying capacity
0	100	200
margin
capacity 口1口2口3口4 J 5
Figure 6:	The prediction margins at convergence of the models in Experiment 1 on the test set.
Kurakin et al. (2016b) suggest adversarial training that randomly divides data into two parts, a natural
and adversarial set. The objective function can be written as the following. For random index sets A and C s.t.
|A| + |C | = n and A ∩ C = φ,
L(f) = ∣A7-1λCτ [X '(f(xi),yi) + λ X '(f(Xi),yi)]
| | + | | i∈A	i∈C
=∣A7‰Rstd(f |A) + λ TAT‰Rrob(f |C)，	(12)
where Xi is an adversarial example of Xi.
Our approach is similar in that we also divide the data for different usage. However, we divide the data by not a
random but an adaptive way, so that `s(f, Xi, yi) ≥ `s (f, Xj, yj) for Xi ∈ Af and Xj ∈ Cf. Also, we have an
additional set other than a natural and fully adversarial set. This additional set, a sensibly reversed adversarial
set, plays an important role in allowing a data point smoothly changes its identity between a natural example and
a full adversarial example. Note that we do not have any weight controllers like λ in (12). The hyperparameter
c itself controls the importance of the natural accuracy in comparison to the adversarial accuracy.
Zhang et al. (2019) investigate the Bayes decision boundary to resolve the trade-off problem. They propose
TRADE, of which the objective function is for β > 0,
L(f) = E['(f(χ),y)+β v0max /(f(χ0),f(χ))].	(13)
X0∈B(X,)
This formulation shows several key differences between TRADE and SENSE. First, (13) uniformly restricts
the perturbation norm to for all data points, whereas (11) selects the sets for Af, Bf and Cf and restricts
18
Under review as a conference paper at ICLR 2020
Figure 7: Another visualization of Figure 2. The name of each panel denotes the hyperparameter c.
-Inf denotes the model trained with the PGD method.
Figure 8: Adversarial and natural prediction margins on CIFAR10 of the SENSE and TRADE
model. The margins are calculated by M(f,χ,y) = logPy(x) - maxyo=y logpy，(x) = Sy(x)-
maxy06=y sy0 (x), where s denote a score function of f, i.e., the output of the neural network.
19
Under review as a conference paper at ICLR 2020
Table 6: MNIST: test results of the our models on natural examples and '∞ based attacks.
Defence model	= 0.3	e = 0.33	= 0.36	e = 0.39
SENSE	96.46	92.89	83.15	63.51
TRADE	96.72	90.56	46.94	11.66
Table 7: CIFAR: test results of the our models on natural examples and '∞ based attacks.
Defence model	e= 10/255	e = 12/255	e= 14/255	e= 16/255
SENSE	62.63	60.39	58.05	55.09
TRADE	47.61	39.05	31.57	25.15
the perturbation in different ways. Second, the main parameter β in (13) controls the importance of the natural
accuracy in comparison to the smoothness of the model. However, the main parameter c in (11) directly
controls the lower bound of the natural and adversarial loss of the individual data. This is the lower bound
on the prediction probability c if ` is the cross entropy loss. Third, the term `(f (X0), f(X)) in (13) leads
the model to be smooth to all directions in the input space, but `s(f, x, y) in (11) leads to be close to one.
Therefore, intuitively, TRADE achieves robustness by obtaining smoothness of the model, whereas SENSE
achieves robustness by reformulating (1) in a way to promote high confidency of the robust prediction. This
may provide an intuition on the plot of the margin density in Figure 8. By this intuition, we apply PGD attacks
with larger perturbation of the training . We apply the attacks on our trained models and the TRADE model
by Zhang et al. (2019) for MNIST and CIFAR10. The results are in Table 6 and Table 7.
G Additional information about Cheese hole distribution
T-T	C / 、	.1	■	i 1 1-1 zlɔ / 孑* ∖	1 zlɔ / 孑B*∖ A 1.1	1 zlɔ / 孑* \ 1
Figure 9 (e) compares the worst-case standard risk Rstd(frob) and Rstd(fB ). Although Rstd(frob) does
not consistently decrease as e increases, it is always much smaller than Rstd(fBl. Therefore, pursuit of
robustness leads to a more naturally accurate classifier. Figure 9 (e) shows that, as e increases from e = α∕2 to
1, the standard risk of the robustly optimal model gradually increases whereas the sensibly robust model keeps
B B B	B ,B*	孑*	S	S *
zero risk. Figure 9 (f) demonstrates the robustness against e-ball attacks of fBc*t (black),于海(blue), and f，b
(red). Although sensibly robust models have large adversarial robustness increasing to 1 as e increases, this is
because more and more adversarial examples can cross the border, while the model keeps its decision boundary
consistent to the class border line. On the other hand, adversarially robust functions have constant robust risk
for e > 1/4. This is because the robust functions predict as y = 1 for every x ∈ X.
Standard Risk
Adversarial Risk
Figure 9: Cheese holes distribution. (e) and (f) The natural and robust risk when p = 0.55. The
black, blue, and red colors are the worst cases of naturally, adversarially, and sensibly robust func-
tions. (g) The robustly optimal model when > 0.25. (h) The sensibly robust model when > 1/12.
The sketch of the proof on the standard and adversarial robust risks in (e) and (f) in Figure 9
We first calculate the three classes of functions which minimize natural, adversarial robust, and sensibly robust
risk respectively, w.r.t PX,Y . Then for each class, we consider the worst case function from each class, in that
the function maximizes the standard risk w.r.t PX,Y . The corresponding standard risks are in Figure 9 (e).
20
Under review as a conference paper at ICLR 2020
Likewise, for each class, we consider the worst case function from each class, in that the function maximizes
the adversarial robust risk w.r.t PX,Y . The corresponding adversarial robust risks are in Figure 1 (f).
First, the minimizers of each risk w.r.t. PX,Y are as following.
1)	Let FB be a set of naturally optimal functions w.r.t PX,Y :
FB = {f ∈ F∣f(x) = Sign(XI — 0.5) for (x1,x2) ∈ X}
2)	Let Frob be a Set of sensibly optimal functions w.r.t Pχ,γ:
Frob = {f ∈ F|f (x) = Sign(XI — 0.5) for (xι, x2) ∈ B(X, e)}.
3)	Let Frob be a set of robustly optimal functions w.r.t PX,Y :
` , ______________, , , . _ , `
Frob = {f ∈ FIf(X) = g(x) for (X1,X2) ∈ B(X,c)},
where
g(X)
S Sign(XI — 0.5 + £)if e < α∕2 or e ≥ 3α∕4
[1(xi≥0.5-ε) — 1(xι ≤3α∕2 + ε) if α/2 ≤ e < 3α/4
(14)
Second, we consider the worst case standard risk w.r.t. PX,Y for each class above.
1)	maxf ∈Fb Rstd(f): Although P = 0.5, due to the symmetry of the shape of X, maxf ∈jfb Rstd(f) is the
area on X ∖X,the area outside the small nine squares.
2)	maXf∈Fs Rstd(f): For the same reason above, maxf ∈f Rstd(f) is the area on X \ B(X, e), the area
outside the small nine squares extended by e.
3)	maxf∈f b Rstd(f): When e < α∕2 or e ≥ 3α∕4, we consider the deviated line on B(X, e), and regard
the model outside B(X, e) as incorrect. The risk is calculated easily by using the fact that the risk of any
f ∈ Frob on is B(X, e) is 3 × mm(a + 2e, 2a) × (1 — p) × mm(2e, 0.5)/0.5. When α∕2 ≤ e < 3α∕4, since
B(X, e) covers X, the worst case functions are in a form of f(X) = Sign(X1 — 0.5 + c) for some c on the
entire X. For each e s.t. 3α∕2 + e < xι < 0.5 — e, it is easy to find the corresponding Cj
Last, for the worst case adversarial robust risk w.r.t. PX,Y, we can calculate the risks in a similar way to above.
H Additional information about Experiments
H.1	Experiment 1
We consider the MNIST and CIFAR-10 dataset (LeCun et al., 2010; Krizhevsky & Hinton, 2009).
Training For each dataset, we initialize our model by a naturally trained model. Then, we train the initialized
model with sensible adversarial examples with the specifications in Table 8.
Table 8: The learning specifications for the SENSE models in experiment 1
Dataset	I C	η1	K	c	I Initial η	Epoch
MNIST	I 0.3	0.05	10	0.5	I 0.01	500
CIfAR10	I ɪ I 255	旦× 2 255 × 10	10	0.7	I 0.1	300
Testing with white-box attacks For white-box attacks, we consider PGD (Madry et al., 2017), C&W
(Ding et al., 2019a), DeepFool (Moosavi-Dezfooli et al., 2016), FGSM (Kurakin et al., 2016a), LBFGS (Taba-
cof & Valle, 2016), and MIFGSM (Dong et al., 2018). In Experiment 1, we consider adversarial perturbations
with '∞-norm less than e, where e = 0.3 for the MNIST dataset and e = 8/255 for the CIFAR10 dataset.
We attack our models with the white-box attacks. We use the attacks implemented in Foolbox (Rauber et al.,
2017), Advertorch (Ding et al., 2019b), and Adversarial Robustness 360 Toolbox (ART) (Nicolae et al., 2018).
The attack specifications are in Table 9. The options that are not listed in the table are kept as default of the
attack generating functions.
21
Under review as a conference paper at ICLR 2020
Table 9: The white-box attack specifications. We denote the step size and step number by ηι and K.
Dataset	Attack	ηι	K	Python package	Function
MNIST	PGD500	0.01	500	Advertorch	LinfPGDAttack
MNIST	C&W40	0.01	40	ART	CarliniLInfMethod
CIfAR10	PGD100	ɪ 255	100	Advertorch	LinfPGDAttack
CIfAR10	C&W40	8 × 1 255 × 20	40	ART	CarliniLInfMethod
CIfAR10	DeepFool	default	default	Foolbox	DeepFoolLinfinityAttack
CIfAR10	FGSM	8 255	1	Advertorch	GradientSignAttack
CIfAR10	LBFGS	default	default	Foolbox	LBFGSAttack
CIfAR10	MIFGSM	ɪ × ι 255 × 40.001	40	Foolbox	MomentumIterativeAttack (distance=Linfinity, return_early=False)
We set the step size for MIFGSM as slightly smaller than 端 X 40 in order to keep the adversarial example
from having the perturbation norm greater than = 8/255. For the performance of TRADE on C&W40, we
apply the C&W40 attack with the same specifications in Table 9. For the other results of TRADE in Table 1,
we refer to Zhang et al. (2019). For PGD attacks, we draw accuracy and loss plots for increasing step numbers
in Figure 10 and Figure 11. They show that for the chosen step sizes, the chosen step numbers are enough to
generate proper local maximizer of the PGD objective function. Particularly, for CIFAR, we use the step size
2/255 rather than X 2/255 as it is more efficient.
PGD attack serenity check We test with 100 random restarts, and for the step size 0.01, the step number
500 seem enough by Figure 10.
MNIST with PGD for the first IOO examples
PGD loss
PGD accuracy
O 250	500	750 IOOO
Figure 10: The convergence check for the PGD attacks on the MNIST model. We used a step size
0.01. We can see that K = 500 is enough to achieve the lowest point by counting the worst case of
random restarts.
Testing with black-box attacks We attack our models with PGD40 and MIFGSM swith the specifications
in Table 9. As the Foolbox implementation for MIGSM only returns the successful attacks on the generating
model, We only apply these attacks on the defense model. We note that the argument return_early of MIFGSM
is set to False as in Table 9. For TRADE, we use the models by Zhang et al. (2019) for both MNIST and
CIFAR10.
22
Under review as a conference paper at ICLR 2020
CIFAR10 with PGD for the first 100 examples
Figure 11: The convergence check for the PGD attacks on the CIFAR model. We used a step size
0.01 and for CIFAR, 2/255. We can see that K = 100 is enough to achieve the lowest point by
counting the worst case of random restarts.
H.2 Experiment 2
Model Architecture We conduct Experiment 2 on the MNIST dataset (LeCun et al., 2010). We consider
a sequence of CNNs with the increasing number of kernels. A network of capacity q has two convolutional
layers with 2(d-1) and 2d filters respectively, followed by a fully connected linear layer of 2(d+4) units. Each
layer is activated by ReLU. Each convolutional layer is followed by 2 × 2 a max-pooling layer. The size of all
convolutional filters is 5 × 5.
With a similar sequence of CNNs, Madry et al. (2017) investigate the model behavior when the capacity in-
creases. They have capacity scale 1,2,4,8 and 16. In their experiment, capacity scale 1 and 2 collapse. our
capacities 2 and 3 are comparable to the capacity scale 1 and 2 by Madry et al. (2017). Likewise, our capacities
4 and 5 are comparable to their capacity scale 4 and 81. Therefore, our result, which shows the PGD models of
capacity 1,2 and 3 collapse, is consistent to the result by Madry et al. (2017).
Training We train the sequence of MNIST models with sensibly adversarial example with = 0.3, η1 =
0.05 and K = 10 for varying C ∈ {0.0, 0.1, •一,0.9}. The initial learning rate η is 0.01, and We train for 500
epochs. When training the PGD models, we use the same hyperparameters except c.
Testing The MNIST models are tested with '∞ PGD attacks of e = 0.3 with the step number K = 40 and
step size η1 = 0.01. We generate the attacks by using a Python package Advertorch by Ding et al. (2019b).
H.3 Additional Experiment for Table 6 and Table 7
We conduct our additional experiment on the MNIST and CIFAR-10 dataset (LeCun et al., 2010; Krizhevsky
& Hinton, 2009). For each dataset, we consider the SENSE model trained in Experiment 1 and the TRADE
model by Zhang et al. (2019). We note that for each dataset, the TRADE and SENSE model share the same
architecture. On theses models, we apply the PGD attacks with perturbations larger than the training . We
generate the PGD attacks, using a Python package Advertorch by Ding et al. (2019b) with the following attack
specifications.
MNIST Let €o = 0.3, which is the training e for each model. For δ ∈ {1.1,1.2,1.3}, we generate the '∞
PGD attacks of = δ0 with the step number K = 40 × δ and step size η1 = 0.01.
1 Our models could be slightly smaller than the counterparts by Madry et al. (2017) because our max-pooling
layers do not apply any padding.
23
Under review as a conference paper at ICLR 2020
CIFAR-10 Let 0 = 8/255, which is the training for each model. For δ ∈ { 180，12,184, 186 },we generate
the '∞ PGD attacks of e = δs with the step number K = 40 × δ step size nɪ = s/20.
24