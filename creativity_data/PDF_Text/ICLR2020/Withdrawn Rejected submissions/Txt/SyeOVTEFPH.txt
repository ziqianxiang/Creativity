Under review as a conference paper at ICLR 2020
Instance adaptive adversarial training:
Improved accuracy tradeoffs in neural nets
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial training is by far the most successful strategy for improving robust-
ness of neural networks to adversarial attacks. Despite its success as a defense
mechanism, adversarial training fails to generalize well to unperturbed test set.
We hypothesize that this poor generalization is a consequence of adversarial train-
ing with uniform perturbation radius around every training sample. Samples close
to decision boundary can be morphed into a different class under a small pertur-
bation budget, and enforcing large margins around these samples produce poor
decision boundaries that generalize poorly. Motivated by this hypothesis, we pro-
Pose instance adaptive adversarial training - a technique that enforces SamPle-
specific perturbation margins around every training sample. We show that using
our aPProach, test accuracy on unPerturbed samPles imProve with a marginal droP
in robustness. Extensive exPeriments on CIFAR-10, CIFAR-100 and Imagenet
datasets demonstrate the effectiveness of our ProPosed aPProach.
1	Introduction
A key challenge when dePloying neural networks in safety-critical aPPlications is their Poor stability
to inPut Perturbations. Extremely tiny Perturbations to network inPuts may be imPercePtible to the
human eye, and yet cause major changes to outPuts. One of the most effective and widely used
methods for hardening networks to small Perturbations is “adversarial training” (Madry et al., 2018),
in which a network is trained using adversarially Perturbed samPles with a fixed Perturbation size.
By doing so, adversarial training tyPically tries to enforce that the outPut of a neural network remains
nearly constant within an `p ball of every training inPut.
DesPite its ability to increase robustness, adversarial training suffers from Poor accuracy on clean
(natural) test inPuts. The droP in clean accuracy can be as high as 10% on CIFAR-10, and 15% on
Imagenet (Madry et al., 2018; Xie et al., 2019), making robust models undesirable in some industrial
settings. The consistently Poor Performance of robust models on clean data has lead to the line of
thought that there may be a fundamental trade-off between robustness and accuracy (Zhang et al.,
2019; TsiPras et al., 2019), and recent theoretical results characterized this tradeoff (Fawzi et al.,
2018; Shafahi et al., 2018; Mahloujifar et al., 2019).
In this work, we aim to understand and oPtimize the tradeoff between robustness and clean accu-
racy. More concretely, our objective is to imProve the clean accuracy of adversarial training for a
chosen level of adversarial robustness. Our method is insPired by the observation that the constraints
enforced by adversarial training are infeasible; for commonly used values of , it is not Possible to
achieve label consistency within an -ball of each inPut image because the balls around images of
different classes overlaP. This is illustrated on the left of Figure 1, which shows that the -ball around
a “bird” (from the CIFAR-10 training set) contains images of class “deer” (that do not aPPear in the
training set). If adversarial training were successful at enforcing label stability in an = 8 ball
around the “bird” training image, doing so would come at the unavoidable cost of misclassifying
the nearby “deer” images that come along at test time. At the same time, when training images lie
far from the decision boundary (eg., the deer image on the right in Fig 1), it is Possible to enforce
stability with large with no comPromise in clean accuracy. When adversarial training on CIFAR-
10, we see that = 8 is too large for some images, causing accuracy loss, while being unnecessarily
small for others, leading to sub-oPtimal robustness.
1
Under review as a conference paper at ICLR 2020
Figure 1: Overview of instance adaptive adversarial training. Samples close to the decision boundary
(bird on the left) have nearby samples from a different class (deer) within a small Lp ball, making
the constraints imposed by PGD-8 / PGD-16 adversarial training infeasible. Samples far from the
decision boundary (deer on the right) can withstand large perturbations well beyond = 8. Our
adaptive adversarial training correctly assigns the perturbation radius (shown in dotted line) so that
samples within each Lp ball maintain the same class.
The above observation naturally motivates adversarial training with instance adaptive perturbation
radii that are customized to each training image. By choosing larger robustness radii at locations
where class manifolds are far apart, and smaller radii at locations where class manifolds are close
together, we get high adversarial robustness where possible while minimizing the clean accuracy
loss that comes from enforcing overly-stringent constraints on images that lie near class bound-
aries. As a result, instance adaptive training significantly improves the tradeoff between accuracy
and robustness, breaking through the pareto frontier achieved by standard adversarial training. Ad-
ditionally, we show that the learned instance-specific perturbation radii are interpretable; samples
with small radii are often ambiguous and have nearby images of another class, while images with
large radii have unambiguous class labels that are difficult to manipulate.
Parallel to our work, we found that Ding et al. (2018) uses adaptive margins in a max-margin frame-
work for adversarial training. Their work focuses on improving the adversarial robustness, which
differs from our goal of understanding and improving the robustness-accuracy tradeoff. Moreover,
our algorithm for choosing adaptive margins significantly differs from that of Ding et al. (2018).
2	Background
Adversarial attacks are data items containing small perturbations that cause misclassification in neu-
ral network classifiers (Szegedy et al., 2014). Popular methods for crafting attacks include the fast
gradient sign method (FGSM) (Goodfellow et al., 2015) which is a one-step gradient attack, pro-
jected gradient descent (PGD) (Madry et al., 2018) which is a multi-step extension of FGSM, the
C/W attack (Carlini & Wagner, 2017), DeepFool (Moosavi-Dezfooli et al., 2016), and many more.
All these methods use the gradient of the loss function with respect to inputs to construct addi-
tive perturbations with a norm-constraint. Alternative attack metrics include spatial transformer
attacks (Xiao et al., 2018), attacks based on Wasserstein distance in pixel space (Wong et al., 2019),
etc.
Defending against adversarial attacks is a crucial problem in machine learning. Many early de-
fenses (Buckman et al., 2018; Samangouei et al., 2018; Dhillon et al., 2018), were broken by strong
attacks. Fortunately, adversarially training is one defense strategy that remains fairly resistant to
most existing attacks.
2
Under review as a conference paper at ICLR 2020
Let D = {(xi, yi)}in=1 denote the set of training samples in the input dataset. In this paper, we focus
on classification problems, hence, yi ∈ {1, 2, . . . Nc}, where Nc denotes the number of classes. Let
fθ(x) : Rc×m×n → RNc denote a neural network model parameterized by θ. Classifiers are often
trained by minimizing the cross entropy loss given by
minN χ	-yi[ιog(fθ(xi))]
(xi,yi)〜D
where % is the one-hot vector corresponding to the label y%. In adversarial training, instead of
optimizing the neural network over the clean training set, we use the adversarially perturbed training
set. Mathematically, this can be written as the following min-max problem
min
θ
1
max —
kδik∞≤N
E	-yri[iog(fθ(xi) + δi)]
(xi,yi)〜D
(1)
This problem is solved by an alternating stochastic method that takes minimization steps for θ,
followed by maximization steps that approximately solve the inner problem using k steps of PGD.
For more details, refer to Madry et al. (2018).
Algorithm 1 Adaptive adversarial training algorithm
Require: Niter: Number of training iterations, Nwarm: Warmup period
Require: P GDk (x, y, ) : Function to generate PGD-k adversarial samples with norm-bound
Require: w : used in warmup
1:	for t in 1 : Niter do
2:	Sample a batch of training samples {(xi, yi)}Nbatch 〜D
3:	if t < Nwarm then
4:	i = w
5:	else
6:	Choose i using Alg 2
7:	end if
8:	xiadv = P GD(xi, yi,	i)
9:	S+ = {i|f (xi) is correctly classified as yi}
10:	S- = {i|f(xi) is incorrectly classified as yi}
11:	minθ Nbatch [Pi∈S+ LcLsgdV ,yi) + Pi∈S- Lcls(xi,yi)i
12:	end for
3 Instance Adaptive Adversarial Training
To remedy the shortcomings of uniform perturbation radius in adversarial training (Section 1), we
propose Instance Adaptive Adversarial Training (IAAT), which solves the following optimization:
min max
θ kδik∞<i
N χ	-yi[log(fθ(xi)+δi)]
(χi,yi)〜D
(2)
Like vanilla adversarial training, we solve this by sampling mini-batches of images {xi}, crafting
adversarial perturbations {δi} of size at most {i}, and then updating the network model using the
perturbed images.
The proposed algorithm is distinctive in that it uses a different i for each image xi . Ideally, we
would choose each i to be as large as possible without finding images of a different class within
the i -ball around xi . Since we have no a-priori knowledge of what this radius is, we use a simple
heuristic to update i after each epoch. After crafting a perturbation forxi, we check if the perturbed
image was a successful adversarial example. If PGD succeeded in finding an image with a different
class label, then Ei is too big, so We replace Ei J Ei - γ. If PGD failed, then We set Ei J Ei + Y.
Since the network is randomly initialized at the start of training, random predictions are made, and
this causes {Ei} to shrink rapidly. For this reason, We begin With a Warmup period of a feW (usually
10 epochs for CIFAR-10/100) epochs Where adversarial training is performed using uniform E for
every sample. After the Warmup period ends, We perform instance adaptive adversarial training.
A detailed training algorithm is provided in Alg. 1.
3
Under review as a conference paper at ICLR 2020
s-dluDS
Cos
I⅛U3J (9IU-(3 a~κDP<)
s-dEDS s-dEDS sa-dLUDS
PaqLnJJad P3cn3d paq,ln3d
(a) Samples from bottom 1%
Figure 2: Visualizing training samples and their perturbations. The left panel shows samples that
are assigned small (displayed below images) during adaptive training. These images are close to
class boundaries, and change class when perturbed with ≥ 8. The right panel show images that are
assigned large . These lie far from the decision boundary, and retain class information even with
very large perturbations. All live in the range [0, 255]
(b) Samples from top 1%
Algorithm 2 selection algorithm
Require: i: Sample index, j : Epoch index
Require: β: Smoothing constant, γ: Discretization for search.
1:	Set 1
= mem[j- 1,i] +γ
2:	Set 2 = mem [j - 1, i]
3:	Set 3 = mem[j - 1,i] -γ
4:	if fθ (P GDk (xi, yi, 1)) predicts as yi then
5:	Set i = 1
6:	else if fθ (P GDk (xi, yi, 2)) predicts as yi then
7:	Set i = 2
8:	else
9:	Set i = 3
10:	end if
11:	6i ― (1 - β)
mem[j - 1, i] + βi
12： Update Ememj,i] — G
13: Return i
4 Experiments
To evaluate the robustness and generalization of our models, we report the following metrics: (1) test
accuracy of unperturbed (natural) test samples, (2) adversarial accuracy of white-box PGD attacks,
(3) adversarial accuracy of transfer attacks and (4) accuracy of test samples under common image
corruptions (Hendrycks & Dietterich, 2019). Following the protocol introduced in Hendrycks &
Dietterich (2019), we do not train our models on any image corruptions.
4.1	CIFAR
On CIFAR-10 and CIFAR-100 datasets, we perform experiments on Resnet-18 and WideRenset-32-
10 models following (Madry et al., 2018; Zhang et al., 2019). All models are trained on PGD-10
attacks i.e., 10 steps of PGD iterations are used for crafting adversarial attacks during training.
In the whitebox setting, models are evaluated on: (1) PGD-10 attacks with 5 random restarts, (2)
PGD-100 attacks with 5 random restarts, and (3) PGD-1000 attacks with 2 random restarts. For
4
Under review as a conference paper at ICLR 2020
(％ U-) AUe-IrDUe -PJrQBN
6	9	12	15	18	21	24	27
Adversarial accuracy (in %)
(a) CIFAR-10	(b) CIFAR-100
Figure 3: Tradeoffs between accuracy and robustness: Each blue dot denotes an adversarially trained
model with a different used at training (Training is marked next to blue dots). Models trained
using instance adaptive adversarial training are shown in red. Adaptive training breaks through the
Pareto frontier achieved by plain adversarial training with a fixed . For all models, adversarial
accuracy is reported on PGD-1000 attacks with a fixed test = 8.
transfer attacks, an independent copy of the model is trained using the same training algorithm and
hyper-parameter settings, and PGD-1000 adversarial attacks with 2 random restarts are crafted on
the surrogate model. For image corruptions, following (Hendrycks & Dietterich, 2019), we report
average classification accuracy on 19 image corruptions.
Beating the robustness-accuracy tradeoff: In adversarial training, the perturbation radius is
a hyper-parameter. Training models with varying produces a robustness-accuracy tradeoff curve -
models with small training achieve better natural accuracy and poor adversarial robustness, while
models trained on large have improved robustness and poor natural accuracy. To generate this
tradeoff, we perform adversarial training with in the range {1, 2, . . . 8}. Instance adaptive adver-
sarial training is then compared with respect to this tradeoff curve in Fig. 3a, 3b. Two versions of
IAAT are reported - with and without a warmup phase. In both versions, we clearly achieve an im-
provement over the accuracy-robustness tradeoff. Use of the warmup phase helps retain robustness
with a drop in natural accuracy compared to its no-warmup counterpart.
Clean accuracy improves for a fixed level of robustness: On CIFAR-10, as shown in Table. 1,
we observe that our instance adaptive adversarial training algorithm achieves similar adversarial ro-
bustness as the adversarial training baseline. However, the accuracy on clean test samples increases
by 4.06% for Resnet-18 and 4.49% for WideResnet-32-10. We also observe that the adaptive train-
ing algorithm improves robustness to unseen image corruptions. This points to an improvement in
overall generalization ability of the network. On CIFAR-100 (Table. 2), the performance gain in
natural test accuracy further increases - 8.79% for Resnet-18, and 9.22% for Wideresnet-32-10. The
adversarial robustness drop is marginal.
Maintaining performance over a range of test : Next, we plot adversarial robustness over
a sweep of values used to craft attacks at test time. Fig. 4a, 4b shows an adversarial training
baseline with = 8 performs well at high regimes and poorly at low regimes. On the other hand,
adversarial training with = 2 has a reverse effect, performing well at low and poorly at high
regimes. Our instance adaptive training algorithm maintains good performance over all regimes,
achieving slightly less performance than the = 2 model for small test , and dominating all models
for larger test .
Interpretability of : We find that the values of i chosen by our adaptive algorithm correlate
well with our own human concept of class ambiguity. Figure 2 (and Figure 6 in Appendix B) shows
that a sampling of images that receive small i contains many ambiguous images, and these images
are perturbed into a (visually) different class using = 16. In contrast, images that receive a large i
have a visually definite class, and are not substantially altered by an = 16 perturbation.
5
Under review as a conference paper at ICLR 2020
Table 1: Robustness experiments on CIFAR-10. PGD attacks are generated with = 8. PGD10 and
PGD100 attacks are generated with 5 random restarts, while PGD1000 attacks are generated with 2
random restarts
Method	Natural acc. (in %)	Whitebox acc. (in %)			Transfer (in %) acc. (PGDi00θ)	Corruption acc. (in %)
		PGD10 I	PGDi0θ I PGDιo00			
Resnet-18						
Clean	94.21	0.02	0.00	0.00	3.03	72.71
Adversarial	83.20	43.79	42.30	42.36	59.80	73.73
IAAT	87.26	43.08	41.16	41.16	59.87	78.82
WideResnet 32-10						
Clean	95.50	0.05	0.00	0.00	5.02	78.35
Adversarial	86.85	46.86	44.82	44.84	62.77	77.99
IAAT	91.34	48.53	46.50	46.54	58.20	83.13
Table 2: Robustness experiments on CIFAR-100. PGD attacks are generated with = 8. PGD10
and PGD100 attacks are generated with 5 random restarts. PGD1000 attacks are generated with 2
random restarts
Method	Natural acc. (in %)	Whitebox acc. (in %)			Transfer acc. (in %) PGDi000
		PGD10 I PGDιo0		I PGDi000	
Resnet-18					
Clean	74.88	0.02	0.00	0.01	1.81
Adversarial	55.11	20.69	19.68	19.91	35.57
IAAT	63.90	18.50	17.10	17.11	35.74
	WdeResnet 32-10						
Clean	79.91	0.01	0.00	0.00	1.20
Adversarial	59.58	26.24	25.47	25.49	38.10
IAAT	68.80	26.17	24.22	24.36	35.18
Robustness to other attacks: While our instance adaptive algorithm is trained on PGD attacks,
we are interested to see if the trained model improves robustness on other adversarial attacks.
As shown in Table. 3, IAAT achieves similar level of robustness as adversarial training on other
gradient-based attacks, while improving the natural accuracy.
Figure 4: Plot of adversarial robustness over a sweep of test
6
Under review as a conference paper at ICLR 2020
Table 3: Robustness results on other attacks for models trained using PGD-10 for WRN-32-10 model
on CIFAR-10 dataset. Accuracies are reported in %
Algorithm	Natural acc.	PGD-1000	DeePFool	MIFGSM	CW40
Adversarial training	8685	-44.84-	-65.28-	-54.66-	55.62
	IAAT	91.34	46.54	66.58	53.99	56.80
Table 4: Robustness experiments on Imagenet. All adversarial attacks are generated with PGD-1000.
(↑) indicates higher numbers are better, while (J) indicates lower numbers are better
Method	Natural acc. (in %) (↑)	Whitebox acc. (in %) (↑)				Corruption mCE Q)
		E = 4I	E = 5 * * 8	E=12	E = 16	
Resnet-50						
Clean training	75.80	0.64	0.18	0.00	0.00	76.69
Adversarial training	50.99	50.89	49.11	44.71	35.82	95.48
	IAAT	62.71	61.52	54.63	39.90	22.72	85.21
Resnet-101						
Clean training	77.10	0.83	0.12	0.00	0.00	70.37
Adversarial training	55.42	55.11	53.07	48.35	39.08	91.45
	IAAT	65.29	63.83	56.62	41.51	23.91	79.52
Resnet-152						
Clean training	77.60	0.57	0.08	0.00	0.00	69.27
Adversarial training	57.26	56.77	54.75	49.86	40.40	89.31
	IAAT	67.44	65.97	59.28	45.01	27.85	78.53
4.2 Imagenet
Following the protocol introduced in Xie et al. (2019), we attack Imagenet models using random
targeted attacks instead of untargeted attacks as done in previous experiments. During training,
adversarial attacks are generated using 30 steps of PGD. As a baseline, we use adversarial training
with a fixed of 16/255. This is the setting used in Xie et al. (2019). Adversarial training on
Imagenet is computationally intensive. To make training practical, we use distributed training with
synchronized SGD on 64/128 GPUs. More implementation details can be found in Appendix E.
At test time, we evaluate the models on clean test samples and on whitebox adversarial attacks with
= {4, 8, 12, 16}. PGD-1000 attacks are used. Additionally, we also report normalized mean
corruption error (mCE), an evaluation metric introduced in Hendrycks & Dietterich (2019) to test
the robustness of neural networks to image corruptions. This metric reports mean classification
error of different image corruptions averaged over varying levels of degradation. Note that while
accuracies are reported for natural and adversarial robustness, mCE reports classification errors, so
lower numbers are better.
Our experimental results are reported in Table. 4. We observe a huge drop in natural accuracy for
adversarial training (25%, 22% and 20% drop for Resnet-50, 101 and 152 respectively). Adaptive
adversarial training significantly improves the natural accuracy - we obtain a consistent performance
gain of 10+% on all three models over the adversarial training baseline. On whitebox attacks, IAAT
outperforms the adversarial training baseline on low regimes, however a drop of 13% is observed
at high ’s ( = 16). On the corruption dataset, our model consistently outperforms adversarial
training.
5 Ablation experiments
5.1 Effect of warmup
Recall from Section 3 that during warmup, adversarial training is performed with uniform norm-
bound constraints. Once the warmup phase ends, we switch to instance adaptive training. From
7
Under review as a conference paper at ICLR 2020
Table 5: Ablation: Effect of warmup on CIFAR-10
Method	Natural acc. (%)	Whitebox acc. (in %)			Transfer acc.(%) PGD1000	Corruption acc. (in %)
		PGD10	PGD100	PGD1000		
Resnet-18						
IAAT (no warm)	89.62	40.55	38.15	38.08	58.89	81.10
IAAr (Warm)	87.26	43.08	41.16	41.16	59.87	78.82
	WideResnet32-10							
IAAT (no warm)	92.62	45.12	41.08	41.11	53.08	84.92
IAAT (Warm)	90.67	48.53	46.50	46.54	58.20	83.13
Table 6: Ablation: Effect of warmup on CIFAR-100
Method	Natural acc. (in %)	Whitebox acc. (in %)			Transfer acc.(%) PGD1000
		PGD10	PGD100 I PGD1000		
Resnet-18					
Adaptive (no warm)	68.34	14.76	13.29	13.30	32.39
Adaptive (warm)	63.90	18.50	17.10	17.11	35.74
	WideResnet32-10						
Adaptive (no warm)	75.48	18.14	13.78	13.71	24.00
Adaptive (warm)	68.80	26.17	24.22	24.36	35.18
Table 7: Ablation: Comparison of IAAT with exact line search. Accuracies are reported in % for
Resnet-18 model trained on CIFAR-10 dataset.
Algorithm	Natural acc.	PGD-10	PGD-1000
Full line search	88:67	43.26	-41.37-
IAAT	87.26	43.08	41.16
Table 5 and 6, we observe that when warmup is used, adversarial robustness improves with a small
drop in natural accuracy, with more improvements observed in CIFAR-100. However, as shown in
Fig. 3a and 3b, both these settings improve the accuracy-robustness tradeoff.
5.2 Other heuristics
We are interested in estimating instance-specific perturbation radius i such that predictions are con-
sistent within the chosen i-ball. To obtain an exact estimate of such an i, we can perform a line
search as follows: Given a discretization η and a maximum perturbation radius max , generate PGD
attacks with radii {in}；max/n. Choose the desired Ei as the maximum in for which the prediction re-
mains consistent as that of the ground-truth label. We compare the performance of exact line search
with that of IAAT in Table 7. We observe that exact line search marginally improves compared to
IAAT. However, exact line search is computationally expensive as it requires performing Emax/n
additional PGD computations, whereas IAAT requires only 2.
6 Conclusion
In this work, we focus on improving the robustness-accuracy tradeoff in adversarial training. We
first show that realizable robustness is a sample-specific attribute: samples close to the decision
boundary can only achieve robustness within a small E ball, as they contain samples from a different
class beyond this radius. On the other hand samples far from the decision boundary can be robust
on a relatively large perturbation radius. Motivated by this observation, we develop instance adap-
tive adversarial training, in which label consistency constraints are imposed within sample-specific
perturbation radii, which are in-turn estimated. Our proposed algorithm has empirically been shown
to improve the robustness-accuracy tradeoff in CIFAR-10, CIFAR-100 and Imagenet datasets.
8
Under review as a conference paper at ICLR 2020
References
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=S18Su--CW.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017,
pp. 39-57, 2017.
Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna,
Zachary C. Lipton, and Animashree Anandkumar. Stochastic activation pruning for robust
adversarial defense. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=H1uR4GZRZ.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin adversar-
ial (mma) training: Direct input space margin maximization through adversarial training. arXiv
preprint arXiv:1812.02637, 2018.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. In
Advances in Neural Information Processing Systems, pp. 1178-1187, 2018.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/
abs/1412.6572.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=HJz6tiCqYm.
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training:
Achieving robust neural networks without sacrificing accuracy. CoRR, abs/1906.06784, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration
in robust learning: Evasion and poisoning attacks from concentration of measure. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4536-4543, 2019.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2574-2582, 2016.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers
against adversarial attacks using generative models. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=BkJ3ibb0-.
Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial
examples inevitable? arXiv preprint arXiv:1809.02104, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In 7th International Conference on Learning Repre-
sentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
9
Under review as a conference paper at ICLR 2020
Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected
Sinkhorn iterations. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, Proceedings of Machine Learning Re-
search, pp. 6808-6817. PMLR, 2019.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially trans-
formed adversarial examples. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=HyydRMZC-.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L. Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7472-7482, Long Beach,
California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/
v97/zhang19p.html.
10
Under review as a conference paper at ICLR 2020
Table 8: Comparison with Mixup
Method	Natural acc. (in %)	Whitebox acc. (in %)			Transfer attack (in %) PGD1000
		PGD10	PGD100	PGD1000	
Resnet-18					
Mixup	89.47	42.60	38.42	38.49	59.48
IAAT	87.26	43.08	41.16	41.16		59.87	
	WideResnet32-10						
Mixup	92.57	45.01	36.6	36.44	63.57
IAAT	90.67	48.53	46.50	46.54		58.20	
A	Appendix
A.1 Comparison with Mixup
A recent paper that addresses the problem of improving natural accuracy in adversarial training is
mixup adversarial training (Lamb et al., 2019), where adversarially trained models are optimized
using mixup loss instead of the standard cross-entropy loss. In this paper, natural accuracy was
shown to improve with no drop in adversarial robustness. However, the robustness experiments
were not evaluated on strong attacks (experiments were reported only on PGD-20). We compare our
implementation of mixup adversarial training with IAAT on stronger attacks in Table. 8. We observe
that while natural accuracy improves for mixup, drop in adversarial accuracy is much higher than
IAAT.
B S ample visualization
A visualization of samples from CIFAR-10 dataset with the corresponding value assigned by IAAT
is shown in Figure. 5. We observe that samples for which low ’s are assigned are visually confusing
(eg., top row of Figure. 5), while samples with high distinctively belong to one class.
In addition, we also show more visualizations of samples near decision boundary which contain sam-
Ples from a different class within a fixed '∞ ball in Figure. 6. The infeasibility of label consistency
constraints within the commonly used perturbation radius of '∞ = 8 is apparent in this visualiza-
tion. Our algorithm effectively chooses an aPProPriate that retains label information within the
chosen radius.
B.1 VISUALIZING PROGRESS
Next, we visualize the evolution of over epochs in adaptive adversarial training. A plot showing
the average growth, along with the progress of 3 randomly picked samples are shown in Fig. 7a
and 7b. We observe that average converges to around 11, which is higher than the default setting of
= 8 used in adversarial training. Also, each sample has a different profile - for some, increases
well beyond the commonly use radius of = 8, while for others, it converges below it. In addition,
a plot showing the histogram of ’s at different snapshots of training is shown in Fig. 8. We observe
an increase in spread of the histogram as the training progresses.
C Imagenet sweep over PGD iterations
Testing against a strong adversary is crucial to assess the true robustness of a model. A popular
practice in adversarial robustness community is to attack models using PGD with many attack iter-
ations (Xie et al., 2019). So, we test our instance adaptive adversarially trained models on a sweep
of PGD iterations for a fixed level. Following (Xie et al., 2019), we perform the sweep upto 2000
attack steps fixing = 16. The resulting plot is shown in Figure. 9. For all three Resnet models, we
observe a saturation in adversarial robustness beyond 500 attack iterations.
11
Under review as a conference paper at ICLR 2020
Figure 5: Visualizing training samples with their corresponding perturbation. All live in the range
[0, 255]
12
Under review as a conference paper at ICLR 2020
Figure 6: Visualizations of samples for which low ’s are assigned by instance adaptive adversarial
training. These samples are close to the decision boundary and change class when perturbed with
≥ 8. Perturbing them with assigned by IAAT retains the class information.
Figure 7: Visualizing progress of instance adaptive adversarial trianing. Plot on the left shows
average of samples over epochs, while the plot on the right shows progress of three randomly
chosen samples.
Figure 8: Histogram of of training samples at different training epochs
13
Under review as a conference paper at ICLR 2020
Figure 9: Imagenet robustness of IAAT over the number of PGD iterations
Table 9: Sensitivity of IAAT performance to hyperparameters β and γ. Models are trained on
CIFAR-10 dataset using Wideresnet-32-10.
Y	β	Natural accuracy (in %)	Adversarial accuracy (in %)
0.00375 * 255	0.05	92710	457T3
0.00375 * 255	0.1	90.27	46.32
0.00375 * 255	0.2	89.73	47.53
0.0075 * 255	0.05	92.15	46.34
0.0075 * 255	0.1	91.34	48.53
0.0075 * 255	0.2	89.95	48.72
0.011 *255	0.05	90.47	46.11
0.011 *255	0.1	90.52	46.17
0.011 *255	0.2		89.99			46.32	
D Sensitivity analysis
As shown in Alg. 2, IAAT algorithm has two hyper-parameters - smoothing constant β and dis-
cretization γ . In this section, we perform a sensitivity analysis of natural and robust accuracies by
varying these hyper-parameters. Results are reported in Table. 9. We observe that the algorithm
is not too sensistive to the choice of hyper-parameters. But the best performance is obtained for
γ = 1.9 and β = 0.1.
E Implementation details
E.1 CIFAR
On CIFAR-10 and CIFAR-100 datasets, our implementation follows the standard adversarial train-
ing setting used in Madry et al. (2018). During training, adversarial examples are generated using
PGD-10 attacks, which are then used to update the model. All hyperparameters we used are tabu-
lated in Table. 10.
14
Under review as a conference paper at ICLR 2020
Table 10: Hyper-parameters for experiments on CIFAR-10 and CIFAR-100
Hyperparameters	Resnet-18	WideReSnet-32-10
Optimizer	SGD =	SGD
Start learning rate	0.1	0.1
Weight decay	0.0002	0.0005
Number of epochs trained	200	110
Learning rate annealing	Step decay	Step decay
Learning rate decay steps	[80,140,170]	[70, 90,100]
Learning rate decay factor	0.1	0.2
Batch size	128	128
Warmup period	5 epochs	10 epochs
E used in warmup (Ew)	8	8
Discretization γ	1.9	1.9
Exponential averaging factor β	0.1		0.1	
Attack parameters during training		
Attack steps	10	10
Attack E (for adv. training only)	8	8
Attack learning rate	2/255	2/255
Table 11: Hyper-parameters for experiments on Imagenet
Hyperparameters	Imagenet
Optimizer Start learning rate Weight decay Number of epochs trained Learning rate annealing Learning rate decay steps Learning rate decay factor Batch size	SGD 0.1 X (effective batch size / 256) 0.0001 110 Step decay with LR warmup [35, 70, 95] 0.1 	32 Per GPU	
Warmup period E used in warmup (Ew) Discretization γ Exponential averaging factor β	30 epochs 16 4 	0.1	
Attack parameters during training
Attack steps	30
Attack E (for adv. training only)	16
Attack learning rate		1/255	
E.2 Imagenet
For Imagenet implementation, we mimic the setting used in Xie et al. (2019). During training,
adversaries are generated with PGD-30 attacks. This is computationally expensive as every training
update is followed by 30 backprop iterations to generate the adversarial attack. To make training
feasible, we perform distributed training using synchronized SGD updates on 64 / 128 GPUs. We
follow the training recipe introduced in Goyal et al. (2017) for large batch training. Also, during
training, adversarial attacks are generated with FP-16 precision. However, in test phase, we use
FP-32.
We further use two more tricks to speed-up instance adaptive adversarial training: (1) A weaker
attacker(PGD-10) is used in the algorithm for selecting (Alg. 2). (2) After i is selected per Alg. 2,
We clip it with a lower-bound i.e., Ei J max(6i,七此).cib = 4 was used in our experiments.
Hyperparameters used in our experiments are reported in Table 11. All our models were trained on
PyTorch.
15
Under review as a conference paper at ICLR 2020
Table 12: Training time for Imagenet experiments
Model	NUmberofGPUs used	Training time
Resnet-50	64	92 hrs =
Resnet-101	128	78 hrs
Resnet-152		128		94 hrs
Resnet-50 model was trained on 64 Nvidia V100 GPUs, while Resnet-101 and Resnet-152 models
were trained on 128 GPUs. Time taken for instance adaptive adversarial training for all models is
reported in Table. 12.
16