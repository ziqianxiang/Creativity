Under review as a conference paper at ICLR 2020
EnsembleNet: A novel architecture for In-
CREMENTAL LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks are used in many state-of-the-art systems for machine per-
ception. Once a network is trained to do a specific task, it cannot be easily trained
to do new tasks as it leads to catastrophic forgetting of the previously learned
tasks.We propose here a novel architecture called EnsembleNet that accommo-
dates for newer classes of data without having to retrain previously trained sub-
models. The novelty of our model lies in the fact that only a small portion of the
network has to be retrained which makes it computational efficient and also re-
sults in higher performance compared to other architectures in the literature. We
demonstrated our model on MNIST Handwritten digits, MNIST Fashion and CI-
FAR10 datasets. The proposed architecture was benchmarked against other mod-
els in the literature on Ωnew, Ωbase, Ωaiι metrics for MNIST- Handwritten dataset.
The experimental results show that Ensemble Net on overall outperformed every
other model in the literature.
1	Introduction
Many times in the real world setting, it becomes essential for an existing neural network to learn a
new class. For example: A Deep Neural Network (DNN) that is trained to recognize cats might have
to be trained to recognize a new species of cat. However, current DNNs are not capable of meeting
the need. Whenever a DNN is trained on a new subset of data, the weights that are responsible for
retaining the old information are disturbed. This leads to forgetting of the old data that it learnt
and this is called as Catastrophic Forgetting. Human brains are very good at this and make it look
easy, but emulating this behaviour in machine learning models has proven to be a challenging task.
To solve this problem, many techniques have been proposed in the past . Most of them include
freezing the weights that are important for the previous data (Fernando et al. (2017)) (Kirkpatrick
et al. (2017)) or retraining the entire neural network on the previous data. Retraining the entire model
on entire previous data is a computationally intensive task and might require weeks of retraining in
few cases. It would be computationally very efficient if we can retrain only a small portion of the
model instead of the entire neural network.
In this paper we proposed a model that will address this issue. The proposed architecture performs
rehearsal of the previous data on only a small portion of the entire model. The proposed architecture
contains two layers of neural networks and only the neural network that is present in the final layer
is exposed to rehearsal. We have shown that this partial rehearsal technique is sufficient to achieve
state of the art performance on Ωbase, Ωnew, Ω0id metrics Kemker et al. (2018) where Ωbase is the
ability of a model to retain the base knowledge, Ωnew is the ability of a model to retain the new
knowledge and Ωaiι is the overall knowledge retention capacity of a model.
As a consequence of the proposed models architecture, it can be trained on a distributed computing
network in an asynchronous manner on heterogeneous systems. Many models and architectures
have been proposed earlier which train a neural network using a distributed computing system Dean
et al. (2012). However, all of the proposed methods involve data transfer between the systems in the
distributed network. To ensure smooth data transfer between systems, it is highly essential that all
the systems in the network are synchronised and are online. This requirement reduces the robustness
of those methods. However, our proposed architecture does not require any data transfer between
systems. This allows all the systems in the network to be asynchronous. The training will not be
affected even if one system goes offline.
1
Under review as a conference paper at ICLR 2020
2	Related Work
Many architectures have been proposed in the past which are capable of incremental learning. Some
of the previously proposed architectures utilize Ensembling methods while other architectures pro-
pose using specialized architectures. For example in Polikar et al. (2001) and He et al. (2011)
ensemble methods were used to achieve incremental learning. They are capable of learning new
data that is flowing into the system as well as learning new classes incrementally. Connolly et al.
(2008), Polikar et al. (2001), Gabrys & Bargiela (2000), Fritzke (1995), Bouchachia (2006) were
proposed which do not use any ensembling techniques. These algorithms have smaller memory
footprint compared to ensemble methods but are incapable of incrementally learning new classes.
Our proposed architecture is similar to a stacking method which can learn new incoming data as
well as incrementally learn new classes.Apart from incremental learning capabilities the proposed
architecture can be trained in a distributed manner in an asynchronous fashion on heterogeneous
computers in parallel. Some work has already been done in trying to achieve distributed training of
machine learning models like the two algorithms proposed in Dean et al. (2012) where the neural
network is split to smaller parts and then trained in parallel. Some of the existing distributed systems
infrastructure like Map Reduce Dean & Ghemawat (2008), Graph Lab Low et al. (2012) has been
utilized by the machine learning community to implement their algorithms. But none of the proposed
distributed training methods are capable of incremental learning. The novelty of our architecture lies
in the fact that our model is capable of incremental learning new classes with minimal rehearsal and
also being capable of trained in a distributed manner.
3	Proposed architecture
The architecture consists of two levels of classifiers. In the first level, there are k classifiers where
k is the number of classes in the data that the model has been trained on till now. Each classifier in
the first layer is trained to recognize datapoints that belong to only one class. If they see a datapoint
that they are not trained to recognize then they output 0. In the second level there is only one
classifier and it is called as the final classifier. The final classifier takes input from all the classifiers
in level 1 and predicts the class of the given input sample point. The proposed architecture is similar
to StackingC architecture Seewald (2002). However, StackingC architecture doesnot append new
classifier to the first layer of classifiers in-order to learn new classes incrementally. Another key
difference between our architecture and StackingC is that unlike StackingC, in our architecture the
final classifier takes in the confidence of the classifier and not the predicted output of the level 1
classifier.
The architecture proposed is built using an iterative al-
gorithm where each new class is added in a one-vs-rest
fashion. For adding a class ’K’, the training data is split
into positive and negative data, where positive points cor-
respond to class K points. The negative data corresponds
to points of all other classes than K. The negative data is
only a subset of points from each of the other classes.
The algorithm is presented in [reference to algorithm]
table. The Algorithm 1 describes the training procedure
of the proposed architecture. The training procedure takes
a data point xi where i is the class to which the datapoint
belongs. If there is already a classifier li in the first level
of the EnsembleNet that is trained to recognize datapoints
belonging to class i, then the new datapoint is fed to li as
a training point. If the label is not covered, then a new
model is built and added to the net. The training data for
this new classifier consists ofXi, where Xi is the set of all
the new points belonging to class i and Xi-1,i-2,i-3.. is
the subset of points belonging to all the previous classes.
Here, Xi is considered as positive class and Xi-1,i-2,i-3..
Figure 1: Architecture of EnsembleNet
considered as negative class. Finally, the
2
Under review as a conference paper at ICLR 2020
entire training data is rehearsed to the final classifier by freezing the classifiers present in the first
level and only retraining the final classifier.
Algorithm 1 EnsembleNet - Training algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
XN // New data set
X // Entire data set
xi // Datapoint belonging to class i
I // Set of all classes that EnsembleNet already knows
li // Classifier trained to recognize i class points
l(x) // Opinion of classifier l on datapoint x.
L // Set of all classifiers in Level 1
Θ // Opinion vector
FC // Final classifier
for xi in XN do
if i 6∈ I then
A New classifier l is created
l	is trained on with i class as positive points and subset of rest classes as negative points
L	= L ∪ li
else
li	is trained on xi
end if
end for
for x in X do
Θ = {}
for l in L do
Θ= Θ∪l(x)
end for
FC is trained on Θ
end for
Algorithm 2 EnsembleNet - Prediction algorithm
1:	XT // Test data
2:	for xi in XT do
3:	Θ =	{}
4:	for l	in L	do
5:	Θ	= Θ	∪	l(x)
6:	end for
7:	prediction = FC (Θ)
8:	print(prediction)
9:	end for
In the prediction phase, a given input point x is passed through each of the existing classifiers in
layer 1.[Algorithm 2]. A classifier in the first level outputs it’s confidence only when it recognizes
the point. This is called as opinion of the classifier. For every point, opinion of all the classifiers in
the first level is gathered and is passed to the final classifier for classification.
3.1	Vector space transformation of datapoints
Whenever datapoints are projected in a vector space, they are usually represented in the n-
dimensional feature vector space (Figure 2.a), where x1 , x2 , x3 ... are features. When these dat-
apoints are passed to the level 1 of the EnsembleNet a feature transformation takes place and the
points are now projected on a different vector space where the vectors are the confidence of a point
belonging to a particular class. We can call this new vector space as class vector space. There are
two possible cases that could occur. In the first case, only one classifier from layer 1 recognises the
point and the point will lie along one axis in the class vector (Figure 2.b). If all the points in the
dataset fall under case 1, then the final classifier can be a single perceptron. But this is not the case
3
Under review as a conference paper at ICLR 2020
(a) Datapoints projected in the feature vector space
Figure 2: Comparision of vector space representations
(b) Datapoints projected in the class vector space
(Ideal case)
Figure 3: Datapoints projected in the class vector space (Non - Ideal case)



always. Many times, for a given datapoint, more than one classifier in layer 1 is triggered and the
points donot lie along any axis as shown in Figure 3. Hence, a multi-layer perceptron was used to
perform the final classification task.
3.2	Training time of EnsembleNet
The architecture of EnsembleNet allows it to be trained on an asynchronous distributed system
with ease. All the classifiers in the first level are independent of each other and can be trained
independently. Hence, parallelizing the training of the first level of classifiers by training each
classifier on one system in a distributed network will greatly reduce the training time. If one classifier
is trainined on one sub-system of a distributed system, then the training time (T) will be
T = max(Tl1, Tl2, Tl3..Tlk) + TFC	(1)
where Tlk is the time consumed by a system to train the lk classifier on one sub-system in the
distributed system and TFC is the time taken to train the final classifier. If the distributed system
contains only n nodes, then equation 1 can be re-written as
T = max(T1,T2,T3..Tn) + TFC	(2)
where T1 is the time spent by the first node in the system on training the first classifier and T2
is the time spent by the second node in the system on training the second classifier and so on. n
denotes the total number of nodes in the distributed system. The proposed architecture’s training
4
Under review as a conference paper at ICLR 2020
time was benchmarked against a standard CNN and standard CNN with rehearsal on Multi task
learning experiment. The results have been tabulated in the Experimental results section of the
paper in Table 4. The total time taken by the model to train on the base dataset is tabulated under the
TB column and the total time to train on the new dataset is tabulated under the TN column. Total
training time of the model is represented as TF .The time required by the CNN on rehearsing the
base dataset have been added to TB . The experimental results demonstrate that EnsembleNet trains
faster compared to a traditional CNN and CNN using rehearsal while demonstrating higher data
retention capacity. A thorough mathematical analysis regarding training time and error analysis of
the proposed architecture was performed and made available in the supplementary material section.
4	Experimental Setup
4.1	Dataset Description
4.1.1	MNIST-Handwritten Digits
MNIST Handwritten digits is a classic dataset in Machine
learning and contains grayscale images of handwritten
digits with 10 classes. The images are 70,000 in number
and have a dimensions of 28x28 each. 60,000 training
samples were selected as train data and 10,000 training
samples were selected as test data.
4.1.2	CIFAR 1 0
CIFAR10 consists of 50,000 training samples and 10,000
test samples where each point belongs to one of 10
mutually-exclusive object categories. Each image has a
dimension of 32x32.
4.1.3	MNIST- FASHION
Figure 4: Parallel training of level 1
classifiers on a distributed system
Apart from Handwritten and CIFAR10, the architecture
was also tested on MNIST Fashion dataset. The dataset
has 70,000 gray-scale images of fashion products belong-
ing to 10 categories. The dimesions of each image in the
dataset is 28x28. 60,000 images were chosen as training
data and 10,000 images were chosen as test data.
	Handwritten digits	Fashion	CIFAR10
Classification Task	Grayscale Image	Grayscale Image	RGB Image
Classes	10	10	10
Image Shape	28x28	28x28	-32x32-
Train Samples	60,000	60,000	-50,000-
Test Samples	10,000	10,000	10,000 一
Table 1: Dataset Specifications
4.2	Models Evaluated
The proposed architecture was evaluated against the fol-
lowing architectures: 1) Standard Multi-layer Perceptron 2) MLP with Elastic Weight Consolidation
3)GeppNet 4)Fixed Expansion layer architecture and 5) GeppNet+STM on MNIST - Handwritten
dataset. The benchmarking performance of each model on MNIST Handwritten dataset was taken
from Kemker et al. (2018). The experiment that was proposed there was replicated using the same
dataset without any changes. The evaluation of the Ωbase, Ωnew and 0。〃 were based on the ɑideai
that were described in the paper.
5
Under review as a conference paper at ICLR 2020
4.2.1	S tandard Multi-Layer Perceptron
A standard Multi Layer Perceptron was used as a baseline by Kemker et al. (2018). A hyperparam-
eter search for the number of units per hidden layer (32-4,096) and number of hidden layers (2-3)
was done to obtain the optimal model for the given task. This model was tested for incremental class
learning by declaring the number of classes initially and then adding new sessions of data.
4.2.2	Elastic Weight Consolidation
In EWC, the weights that are important for remembering the previous data are disturbed compara-
tively less when new data is being added to the model. This is achieved by using a modified loss
function
L(θ) = Lt(θ) + Pi2Fi(θi - θA,i)2
F is the Fisher information matrix which is used to constrain the weights that are important for
retaining previously learned data. A detailed description of the loss function can be found at Kirk-
patrick et al. (2017)
4.2.3	GeppNet and GeppNet+STM
GeppNet and GeppNet+STM Gepperth & Karaoguz (2016) are biologically inspired networks that
deploy rehearsal to mitigate forgetting of previous data. GeppNet consists of a SOM layer and a
Linear regression classifier. The SOM layer is updated only when the new training sample has been
determined as novel by the model. The novelty of the datapoint is determined using the confidence
measure. When GeppNet+STM detects a novel point, it stores it in the STM (Short Term Memory).
It replays the samples in the STM during a sleep phase. GeppNet and GeppNet STM store the pre-
vious data however, in GeppNet+STM a training example is only replayed if the model is uncertain
about the prediction.
4.2.4	Fixed Expansion Layer
FEL Coop et al. (2013) is a two layer Multi layer perceptron with the second layer being bigger
than the first layer. The weights in the second layer are sparse and remain fixed during the training.
Only some of the units in the first layer are updated because only a subset of the FEL layer units are
allowed to have non-zero output to the final classification layer. Moreover, only a subset of units in
the FEL layer are connected to the units in the first layer.
4.3	Experimental Results
4.3.1	Incremental Class learning
The experiment that was performed was Incremental class learning, where new classes are added to
the model iteratively and the ability of the model to retain the base knowledge, new knowledge and
the overall knowledge is measured. For all the three datasets, classes from 0 to 4 were considered as
base knowledge and classes 5 to 9 were added iteratively to the model.
For MNIST-Handwritten digits dataset, although MLP and FEL showed better capability to retain
new data, they both performed poorly when their ability to retain Base data and Overall data were
measured. Elastic weight consolidation technique demonstrated higher capability of retaining base
knowledge, but the weight consolidation prevented the model from learning new data effectively.
GeppNet and GeppNet+STM performed better compared to other models, but our model outper-
formed both the models in all the metrics. EnsembleNet showed significantly higher ability to retain
new knowledge ComaPared to the GePPNet variants. The Geometric mean of ΩBase, Ωaii and Ωnew
has been calculated and is represented as Ωg. EnsembleNet outperformed all other models in Ωg
as well.
The model was also tested on MNIST-Fashion dataset and CIFAR10. 0-4 classes were considered as
the base knowledge and 5-9 classes were incrementally added in successive sessions.The ProPosed
6
Under review as a conference paper at ICLR 2020
Model	Ωnew	ωBase	Ωaii	Ωg
MLP	1.000	0.60	0.181	0.477
EWC 一	0.001	1.000	0.133	0.0510
GeppNet	0.824	0.960	0.922	0.900
FEL	1.000	0.451	0.439	0.5828
GePPNet+STM	0.599	0.919	0.824	0.768
Ours	0.973	0.979	0.980	0.977
Table 2: Incremental class learning results on MNIST Handwritten digits dataset. The benchmarking
results were taken from Kemker et al. (2018)
Dataset	Cbase	◎new	Ωall
MNIST-Fashion-	0.943	0.941	0.963
CIFAR10	0.9231	0.999	0.967
Table 3: Incremental class learning results on MNIST-Fashion and CIFAR10 datasets
model showed similar performance on Fashion and CIFAR10 dataset. The αideal was set as 92%
and 67.3% respectively. It has to be observed that EnsembleNet showed values greater than 0.92 for
all Ω values on all the tested datasets.
4.3.2	Multi modal Incremental Learning
The experiment evaluates the ability of a model to learn multiple tasks sequentially. In this experi-
ment, the model is first trained on one dataset and then it is expected to learn a new dataset. The first
data on which the model has been trained is considered as the base dataset and the second dataset
on which the model is trained is considered as the new dataset. MNIST Handwritten digits and
MNIST Fashion datasets were used for the experiment. In one iteration, Handwritten digits dataset
was trained as the base dataset and Fashion dataset was trained as the new dataset. In the second
iteration, Fashion dataset was considered as the base dataset and Handwritten digits dataset was
considerd as the new dataset. The model’s behaviour has been benchmarked against standard Con-
volutional Neural Network Krizhevsky et al. (2012) and a standard Convolutional Neural Network
that was allowed to rehearse the base dataset after being trained on the new dataset.
Model	Base Dataset	New Dataset	Ωnew	Ωbase	Ωall	∏B1	PNI	∏⅛∏
CNN	MNIST	Fashion	1.000	0.0013	0.5114	60s	^60s^	120s
CNN+rehearsal	MNIST	Fashion	0.758	0.9945	0.9314	120s	^60s^	180s
CNN	Fashion	-MNIST-	0.9937	0.2535	0.6787	60s	^60s^	120s
CNN+rehearsal	Fashion	-MnIst-	0.013	0.9262	0.521	120s	~60s~	180s
Ours	MNIST	Fashion	0.907	0.9728	0.985	-6s-	6s	T12T
Ours	Fashion	MNIST 一	0.9648	0.9272	0.9842	6s	6s	^H2T
Table 4: Multi modal incremental learning results on MNIST-Fashion and MNIST-Handwritten
digits datasets
The experimental results show that the proposed architecture has better capacity to retain the base
knowledge and as well as learn new knowledge. Four variations of experiments were conducted on
CNN and the CNN was allowed to rehearse the base knowledge in two experiments. Even after re-
hearsal, the standard CNN failed to show results that were comparable to our proposed architecture.
The architecture showed an overall of higher performance on Ωaiι and Ωnew metrics.
7
Under review as a conference paper at ICLR 2020
5	discussion
The working philosophy of EnsembleNet can be understood by understanding the Stability-
Plasticity Dilemma Abraham & Robins (2005). The stability-plasticity dilemma basically states that,
if a neural network’s weights are disturbed easily when it is trained on new data, then the weights
that are responsible for retention of old memories are distorted and the ability of the network to
retain and recall the old data is lost. However, if the neural network’s weights remain unchanged
even when it is trained on new data, then it will fail to learn the new data. In the former case, the
neural network can be considered as being too plastic and in the later case, the neural network can
be considered as being too stable. The standard Multi-layer perceptron and Convolutional Neural
networks are good examples of neural architectures that are too plastic in nature. The experimental
results on Multi-modal experiments clearly show that CNNs are capable of learning new data very
well, however their capacity for retaining base knowledge is low. Neural Networks that incorporate
partial or total weight freezing perform poorly on learning new data as their architecture is too sta-
ble and crucial weight updations donot happen. Examples of architectures that utilize partial or total
weight freezing are Fixed expansion layer network and Elastic weight consolidation networks.
The high performance of EnsembleNet can be explained by EnsembleNet’s ability to keep a por-
tion of it’s network stable while leaving other portion of it’s network to be plastic. The classifiers
in the first level remain un-affected when the model is trained on new data. This protects the crucial
weights in the network that are responsible for retaining old data. However, the weights in the newly
added classifier and the final classifier are subjected to change which helps in the learning of new
data. The lack of updation of weights in the level 1 makes EnsembleNet stable while rapid updation
of weights in the final classifier and the new classifier makes it plastic. This inherent nature of re-
maining stable while having the flexibility to be plastic explains the high performance that has been
observed in the experiments.
6	Conclusions
In this paper, we proposed a novel architecture which is capable of incrementally learning new object
classes. We demonstrated that EnsembleNet is capable of retaining both recently learned data and
newly learned data by only retraining a small portion of the entire network. In addition we showed
that EnsembleNet can be trained much faster than traditional neural networks on a distributed system
in an asynchronous manner. Future work will include exploration of ways to avoid storing of entire
training data. Pseudorehearsal will be incorporated into the EnsembleNet architecture to make it
more memory efficient. We are also planning to test the proposed architecture on datasets with
higher number of classes like CIFAR100 and CUB200.
References
Wickliffe C Abraham and Anthony Robins. Memory retention-the synaptic stability versus plastic-
ity dilemma. Trends in neurosciences, 28(2):73-78, 2005.
Abdelhamid Bouchachia. Incremental learning by decomposition. In 2006 5th International Con-
ference on Machine Learning and Applications (ICMLA’06), pp. 63-68. IEEE, 2006.
Jean-FranCoiS Connolly, Eric Granger, and Robert Sabourin. Supervised incremental learning with
the fuzzy artmap neural network. In IAPR Workshop on Artificial Neural Networks in Pattern
Recognition, pp. 66-77. Springer, 2008.
Robert Coop, Aaron Mishtal, and Itamar Arel. Ensemble learning in fixed expansion layer net-
works for mitigating catastrophic forgetting. IEEE transactions on neural networks and learning
systems, 24(10):1623-1634, 2013.
Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on large clusters. Com-
munications of the ACM, 51(1):107-113, 2008.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in
neural information processing systems, pp. 1223-1231, 2012.
8
Under review as a conference paper at ICLR 2020
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super
neural networks. arXiv preprint arXiv:1701.08734, 2017.
Bernd Fritzke. A growing neural gas network learns topologies. In Advances in neural information
processing systems, pp. 625-632, 1995.
Bogdan Gabrys and Andrzej Bargiela. General fuzzy min-max neural network for clustering and
classification. IEEE transactions on neural networks, 11(3):769-783, 2000.
Alexander Gepperth and Cem Karaoguz. A bio-inspired incremental learning architecture for ap-
plied perceptual problems. Cognitive Computation, 8(5):924-934, 2016.
Haibo He, Sheng Chen, Kang Li, and Xin Xu. Incremental learning from stream data. IEEE
Transactions on Neural Networks, 22(12):1901-1914, 2011.
Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes, and Christopher Kanan. Mea-
suring catastrophic forgetting in neural networks. In Thirty-second AAAI conference on artificial
intelligence, 2018.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yucheng Low, Danny Bickson, Joseph Gonzalez, Carlos Guestrin, Aapo Kyrola, and Joseph M
Hellerstein. Distributed graphlab: a framework for machine learning and data mining in the
cloud. Proceedings of the VLDB Endowment, 5(8):716-727, 2012.
Robi Polikar, Lalita Upda, Satish S Upda, and Vasant Honavar. Learn++: An incremental learning
algorithm for supervised neural networks. IEEE transactions on systems, man, and cybernetics,
part C (applications and reviews), 31(4):497-508, 2001.
Alexander K Seewald. How to make stacking better and faster while also taking care of an unknown
weakness. In Proceedings of the nineteenth international conference on machine learning, pp.
554-561. Morgan Kaufmann Publishers Inc., 2002.
9
Under review as a conference paper at ICLR 2020
7	Supplementary material
7.1	Training time relationship between and ordinary network
In order to deduce the training time relationship between and Ordinary network, we need to make
certain assumptions that are general and sensible in practice.
Assumptions or Control settings
•	Training time of each of the sub-modules is proportional to the size of its corresponding
input training data Ti 8 |Si|
•	Each of the sub-models have similar complexities and take same training time, i.e.
Ti — |Si|-T_
Tj =两 → Ti = α *|Si|
•	Comparisons are drawn against a single neural network of multi class prediction type
•	The training time of the final layer (say, Tf) of the is proportional to the number of input
classes
•	In a given scenario of comparison between Ordinary network and , we can assume Tf = c,
some constant value.
In case of the parallel training scenario, the effective time required for training the non-final layer is,
Tb = max(T1, . . . , Tn)
Let Smax ⊆ S be the partition corresponding to the sub-module that took the maximum time. Let
|Smax = β * |S|.
The total time required to train the is therefore shown in (Equation 3).
Ten = Tb + Tf = Tmax + c	(3)
The total time required to train the ordinary network is (Equation 4).
Tord = γ * |S |	(4)
Now, the relationship between Tord and Ten is shown in (Lemma 7.1).
Let us consider the ratio of training times of and Ordinary network,
Ten
Tord
Ten
Tord
Tmax + c
Tord
_ α * |Smax | + C
=β*^
α * β * lSmax | + C
Y * |S|
10
Under review as a conference paper at ICLR 2020
When sub-module and the ordinary network are of similar architectures,
	γ≈β
. When all the sub-modules are similar,
β≈α
	Ten _ α * B * |Smax | + C - =	：—：	 Tord	Y * |S| α * α * |S| + c =α *|S|
We require that,	.Ten < 1 Tord
, which is implied by (Equation 5).
	|S| * α2 + c ——→ --!	 < 1 α * |S|	< c -→=α+而 <1 —→= α2 — α + -- < 0 |S| α I⑸
If sub-module and ordinary are of similar architectures, then Ten ≤ Tord
Upon enforcing (Equation refeqn:enet-ord-axiom1) and due to (Lemmas 7.1.1,7.1.2,7.1.3), we can
conclude that Ten ≤ Tord .
7.1.1	REAL VALUED α
We required that, α is a real value, which means the quantity under square root to be positive
(Equation 6).
When the number of training points in the data set are at least 4 times larger than the number of
classes, then each sub-module is non-trivially trainable.
Refer to (Equation 5) and consider the quantity inside the square root.
	c 1-4*西 ≥0 c 一 1 ≥ 4 * 西 -→ |S | ≥ 4 * c |S| ≥ 4*c	(6)
11
Under review as a conference paper at ICLR 2020
7.1.2	Positive valued α
We require,
ɑ ≥ 0
The condition α ≥ 0 is satisfied all valid values of number of classes and sizes of training set of
points.
Refer to (Equation 5) and consider the minimum value of α.
2	, c
α -a + 西
< 0
1 -
—→ a ∈ [-
1-- 4 * ⅛ 1 ± J1 + 4 * 卤
-2	,	2
1- 1--4 *⅛
a = --ɪ------
2
≥0
Consider 1 M1-4∖SL ≥ 0,
1- J1 -4 * 看
2
≥0
1 - 4 * 工
∣s I
C
T 1 - 4 * 西
—> -4 * c—
∣s I
≤ 1
≤ 1
≤ 0
Therefore, (∀c, ∣S∣) : a ≥ 0.
7.1.3 Handling the upper bound, a ≤ 1
The condition a ≤ 1 is satisfied all valid values of number of classes and sizes of training set of
points.
Refer to (Equation 5) and consider the maximum value of a.
We required that,
a≤1
a≤1
—→
1 + J1 - 4 * 高
2
≤1
-1 + ʌ/1 - 4 * ∣⅜ ≤2
C
C
1 - 4 * 两 ≤ 1 T-4 * 两 ≤
0
This is again true, (∀c, ∣S∣) : a ≤ 1.
12
Under review as a conference paper at ICLR 2020
7.2
Estimating relationship between training and testing error rates
Figure 5: Relationship between sub-module error ξ and overall error δ for n = 10
Figure 6: Relationship between sub-module error ξ and overall error δ for n = 1, 2
13