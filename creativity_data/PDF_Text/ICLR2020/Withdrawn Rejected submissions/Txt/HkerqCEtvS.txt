Under review as a conference paper at ICLR 2020
Stochastic Gradient Methods with
Block Diagonal Matrix Adaptation
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient approaches that automatically adjust the learning rate on a per-
feature basis have been very popular for training deep networks. This rich class
of algorithms includes Adagrad, RMSprop, Adam, and recent extensions. All
these algorithms have adopted diagonal matrix adaptation, due to the prohibitive
computational burden of manipulating full matrices in high-dimensions. In this
paper, we show that block-diagonal matrix adaptation can be a practical and power-
ful solution that can effectively utilize structural characteristics of deep learning
architectures to significantly improve convergence and out-of-sample generaliza-
tion. We present AdaBlock, a general framework for block-diagonal matrix
adaption via coordinate grouping, which includes counterparts of the aforemen-
tioned algorithms. We prove its convergence in non-convex optimization and
provide generalization error bounds, highlighting benefits compared to diagonal
versions. In addition, we propose two techniques enriching the AdaBlock family:
i) an efficient spectrum-clipping scheme that benefits from superior generalization
performance of Sgd and ii) a randomized layer-wise block diagonal adaptation
scheme to further reduce computational cost. Extensive experiments show that
AdaBlock achieves state-of-the-art results on several deep learning tasks, and
can outperform adaptive diagonal methods, vanilla Sgd, as well as a modified
version of full-matrix adaptation proposed very recently.
1	Introduction
Stochastic gradient descent (Sgd, Robbins & Monro (1951)) is a dominant approach for training
large-scale machine learning models such as deep networks. At each iteration of this iterative method,
the model parameters are updated in the opposite direction of the gradient of the objective function
typically evaluated on a mini-batch, with step size controlled by a learning rate. While vanilla SGD
uses a common learning rate across coordinates (possibly varying across time), several adaptive
learning rate algorithms have been developed that scale the gradient coordinates by square roots of
some form of average of the squared values of past gradients coordinates. The first key approach
in this class, Adagrad (Duchi et al., 2011; McMahan & Streeter, 2010), uses a per-coordinate
learning rate based on squared past gradients, and has been found to outperform vanilla Sgd on sparse
data. However, in non-convex dense settings where gradients are dense, performance is degraded,
since the learning rate shrinks too rapidly due to the accumulation of all past squared gradient in its
denominator. To address this issue, variants of Adagrad have been proposed that use the exponential
moving average (EMA) of past squared gradients to essentially restrict the window of accumulated
gradients to only few recent ones. Examples of such methods include Adadelta (Zeiler, 2012),
RMSPROP (Tieleman & Hinton, 2012), ADAM (Kingma & Ba, 2015), and NADAM (Dozat, 2016).
Despite their popularity and great success in some applications, the above EMA-based adaptive
approaches have raised several concerns. Wilson et al. (2017) studied their out-of-sample gener-
alization and observed that on several popular deep learning models their generalization is worse
than vanilla Sgd. Recently, Reddi et al. (2018) showed that they may not converge to the optimum
1
Under review as a conference paper at ICLR 2020
(or critical point) even in simple convex settings with constant minibatch size, and noted that the
effective learning rate of EMA methods can increase fairly quickly while for convergence it should
decrease or at least have a controlled increase over iterations. AMSGrad, proposed in Reddi et al.
(2018) to fix this issue, did not yield conclusive improvements in terms of generalization ability.
To simultaneously benefit from the generalization ability of vanilla Sgd and the fast training of
adaptive approaches, Luo et al. (2019) recently proposed AdaB ound and AMSBound as variants
of Adam and AMSGrad, which employ dynamic bounds on learning rates to guard against extreme
learning rates. Chen et al. (2019) introduced AdaFom that only add momentum to the first moment
estimate while using the same second moment estimate as AdaGrad. Zaheer et al. (2018) showed
that increasing minibatch sizes enables convergence of Adam, and proposed Yogi which employs
additive adaptive updates to prevent informative gradients from being forgotten too quickly. Yu et al.
(2017) considers a variant of diagonal adaptation where, for each neural network layer, the gradients
are normalized by the `2 norm of the layer’s gradients.
We note that all the aforementioned adaptive algorithms deal with adaptation in a limited way, namely
they only employ diagonal information of Gradient of Outer-Product (gtgtT where gt is the stochastic
gradient at time t, a.k.a. GOP). Though initially discussed in Duchi et al. (2011), full matrix adaptation
has been mostly ignored due to its prohibitive computational overhead in high-dimensions. The
only exception is the GGT algorithm Agarwal et al. (2019); it uses a modified version of full-matrix
AdaGrad with exponentially attenuated gradient history as in Adam, but truncated to a small
window parameter so the preconditioning matrix becomes low rank thereby computing its inverse
square root effectively. Lafond et al. (2017) proposes a block diagonal structure in the context of
natural gradients which always requires a probabilistic model.
Contributions. In this paper, we propose an extended form of SGD learning with block-diagonal
matrix adaptation that can better utilize the structural characteristics of deep learning architectures.
We also show that it can be a practical and powerful solution, which can actually outperform vanilla
Sgd and achieve state-of-the-art results on several deep learning tasks. More specifically, the main
contributions of this paper are as follows:
•	We provide an SGD framework with block diagonal matrix adaptation via coordinate grouping,
AdaBlock. This framework takes advantage of richer information on interactions across
different gradient coordinates, while relaxing the expensive computational cost of full matrix
adaptation in large-scale problems. In addition, we introduce several grouping strategies that
are practically useful for deep learning problems.
•	We provide the first convergence analysis of our framework in the non-convex setting, and
highlight difference and benefits compared with diagonal versions. We investigate how the
block sizes affect the convergence theoretically and empirically. Moreover, we provide insights
on why AdaBlock can improve generalization compared to usual diagonal approaches.
•	We introduce spectrum-clipping, a non-trivial extension of Luo et al. (2019), to further boost
the generalization ability of AdaBlock. Spectrum-clipping allows the block diagonal matrix
to become a constant multiple of the identity matrix in the latter part of training, similarly to
vanilla Sgd. In addition, we propose RAdaBlock, a randomized layer-wise AdaBlock
scheme, to further reduce computational cost.
•	We evaluate the training and generalization ability of our approaches on popular deep learning
tasks. Our experiments reveal that block diagonal methods perform better than diagonal
approaches, even for small grouping sizes, and can also outperform vanilla Sgd and the
modified version of full-matrix adaptation GGT.
Notation. For a vector x, kxkp denotes the vector p-norm, and kxk is kxk2 if not specified. For
a matrix A, |||A|||p indicates the matrix p-norm for matrix A, and λ(A) returns a eigenvalue list
(spectrum) of A. λmin(A) and λmax(A) denote the minimum and maximum eigenvalue of A
respectively. The function Clip(x, a, b) represents clipping x element-wise with the interval I = [a, b].
Lastly, log |A| denotes the log-determinant of a matrix A.
2
Under review as a conference paper at ICLR 2020
OOooQ) 9008)0。。。Q)
(a) input-neuron (b) output-neuron (c) partially group
Image
(d) filter-wise group
Figure 1: Examples of coordinate grouping. The weights with same color belong to the same group.
2 Adaptive Gradient Methods with Block Diagonal Matrix
Adaptations via Coordinate Partitioning
In the context of stochastic optimization, Duchi et al. (2011) proposed a full-matrix variant of
AdaGrad. This version employs a preconditioner which exploits first-order information only, via
the sum of outer products of past gradients:
gt =	▽/(Xt),	Gt	=	Gt-I	+ gtgt	,	xt+1 =	Xt-	αt(G1/2	+ δI)	1gt	(I)
where gt is a stochastic gradient at time t, αt is a step-size, and δ is a small constant for numerical
stability. Duchi et al. (2011) presented regret bounds for (1) in the convex setting. However, this
approach is quite expensive due to Gt1/2 term, so they proposed to only use the diagonal entries of
Gt. Popular adaptive methods for training deep models such as RMSPROP/ADAM are based on such
diagonal adaptation. Their general form and designs of the 2nd momentum are given in the appendix.
Duchi et al. (2011) also discussed the case where full-matrix adaptation can converge faster than its
popular diagonal counterpart. Motivated by this, we first check through atoy MLP experiment whether
preconditioning with exact GOP (1) can be more effective even in the deep learning context. Our
experiment shows that one can achieve faster convergence and better objective values by considering
the interaction between gradient coordinates (1). Details are provided in appendix due to space
constraint. The caveat here is that using full GOP adaptation in real deep learning optimization
problems is computationally intractable due to the square root operator in (1). Nevertheless, is the
best choice to simply use diagonal approximation given the available computation budget? What if
we can afford to pay a little bit more for our computations?
Main Algorithm: Adaptive SGD with Block Diag-
onal Adaptation (AdaBlock)
We address the above question and provide a family of
adaptive Sgd bridging exact GOP adaptation and its
diagonal approximation, via coordinate partitioning.
Given a coordinate partition, we simply ignore the
interactions of coordinates between different groups.
For instance, given a gradient g ∈ R6, one example
of constructing block diagonal matrices via coordi-
nate partitioning is g = (g1,g2,g3,g4,g5, g6 ) →
l-{z-} |--{----} l{z}
G1	G2	G3
[gG1 gGT1 | 0 | 0; 0 | gG2 gGT2 | 0; 0 | 0 | gG3 gGT3] where
Gi represents each group and gG denotes the collec-
Algorithm 1 ADABLOCK: Adaptive Gradient
Methods with Block Diagonal Matrix Adaptation
Input: Stepsize αt, initial point x1 ∈ Rd, and
{β1,t }tT=1 ∈ [0, 1). The function Ht designs
r
Vt with dynamic size of r blocks, {Vt,[j] }jr=1.
Initialize: m0 = 0, Vb0 = 0.
Require: Coordinate partition P .
for t = 1, 2, . . . , T do
Draw a minibatch sample ξt from P
gt — Nf(Xt)
mt — βι,t mt-ι + (1 — βι,t)gt
for j = 1, 2, . . . , r do
Vt,[j] - Ht(gι,[j], ∙ ∙ ∙ ,gt,[j]; P)
tion of entries corresponding to group Gi . Both exact
GOP and diagonal approximation are special cases
of our family. Exploring the use of block-diagonal
end for
xt+1 - xt - αt (Vbt1/2 + δI)-1mt
end for
matrices was suggested as future work in Duchi et al. (2011), and our work therefore provides an
in-depth study of this proposal. Our main algorithm, Algorithm 1, formalizes our approach for a
total r groups where each group Gi has a size of ni for i ∈ [r]. The Algorithm 1 can handle arbitrary
coordinate grouping with appropriate reordering of entries, and groups of unequal sizes.
3
Under review as a conference paper at ICLR 2020
Effect of grouping on optimization. Fig-
ure 1 shows some grouping examples
in the context of deep learning models:
grouping the weights with the same color
in a network can approximate the exact
GOP matrix with a block diagonal ma-
trix of several small full matrices. To see
which grouping could be more effective in
terms of optimization, we revisit our MLP
toy example. Figure 2-(a,b) show the loss
landscapes for different grouping strate-
gies (weights other than shown are fixed
as true model values). We can see that the
loss landscape when grouping weights in
the same layer has a much more dynamic
curvature than when grouping weights in
different layers. In this context, we expect
(a) From the same layer
(b) From different layers
(c) RMSPROP-Diag
Figure 2: Top: Loss surface for different groupings;
Bottom: optimization histories on the loss surface (a).
(d) RMSPROP-Group
that a block-diagonal preconditioner is effective in terms of optimization and illustrate this empirically
by comparing the grouping version for the loss landscape with dynamic curvature (Figure 2-(a)), and
its diagonal counterpart. To figure out the effect of the block-diagonal structure only, we compare
both approaches using RMSprop which does not consider the 1st-order momentum. Figure 2-(c,d)
show the optimization histories. The block diagonal extension of RMSprop converges to a stationary
point in fewer steps than usual RMSprop and shows a more stable trajectory.
3 Analysis on Block Diagonal Matrix Adaptations
3.1	Convergence in Non-convex Optimization
In this section, we provide a theoretical analysis on the convergence of Algorithm 1. Towards this,
We consider the form of non-convex optimization problem, min .f (x) := Eξ〜P [f (x; ξ)] where X
is an optimization variable and ξ is a random variable representing randomly selected data sample
from training data S. As in other works for non-convex optimization such as (Ghadimi & Lan, 2013;
2016), We study the convergence to “stationarity” and hence derive upper bounds of ∣∣Vf (x)k2 by
Algorithm 1, under the following mild conditions:
Assumption 1. (a) f is differentiable, L-smooth, and lower bounded. (b) At time t, the algorithm
can access a noisy gradient. We assume the true gradient Vf (Xt) and noisy gradient gt are both
bounded, i.e. ∣∣Vf (Xt)∣2, ∣∣gt∣∣2 ≤ G for all t. (C) The noisy gradient gt is unbiased and the noise
is independent, i.e. gt = Vf(Xt) + ζt where E[ζt] = 0 and ζt ⊥⊥ ζs for t 6= s. (d) The sequence of
-1/2
β1,t ∈ [0, 1), t ∈ [T] in Algorithm 1 is non-increasing. (e) ∣αtVt	mt∣2 ≤ D for some D > 0.
Note that the condition (a) is a key assumption in general non-convex optimization analysis, and
(b)-(d) are standard ones in the line of Work on stochastic gradient based solvers such as Chen et al.
-1/2
(2019)	. The last condition (e) states that the final step vector αt Vt	mt should be finite, Which is
also a mild condition. We are noW ready to state our first theorem.
Theorem 1. Let Qt ：= |Ilat-IVbt-1/2 —。游-1/% = maxj∈[r]{∣∣αt-ιVt-i/j] - at立-j]/%} for
Algorithm 1 which measures the maximum difference in effective spectrums over all diagonal blocks
Vbt jj] and Yt := λmin (at Vt-1/2). Then, under Assumption 1, Algorithm 1 is guaranteed to yield
E C1
Term A
.}1.
miTn ∣Vf(Xt)∣2 ≤
t∈[T]
^^^{
T
XatVbt-1/2gt2+
t=1
Term B
ʌ
T
T-1
C2	Qt + C3 P Qt2 + C4
t=2	t=2
T
P γt
t=1
,s√T)
—S2(T)
(2)
4
Under review as a conference paper at ICLR 2020
TermA
O O
21
20000	40000
Iterations. T
Term B
≡l⅛⅛:且一"
20000
Iterations, T
Loa-determinant
(_n6-—_2_6-)嘱
20000	40000
Iterations. T
40000
Figure 3: Empirical studies with block diagonal extension of ADAM with stepsize αt = 10-3.
where C1, C2, and C3 are constants independent of problem dimension d and the number of iterations
T, and C4 is a constant independent of T.
Note that while Theorem 1 is generally applicable to any Adam-type block diagonal matrix adapta-
tions, the effect of block size b is implicitly represented in (2). As we will see below, the terms here
can be further simplified for some cases depending on algorithmic details.
•	Sufficient Condition for Convergence. From Theorem 1, we can see that s1(T) = o s2(T)
provides a sufficient condition for convergence. For example, ADAGRAD with α= = ɑ∕√7 satisfies
sι(T) = O (log ∣Vbτ | + log T + 1) and s2(T) = Ω(√T), so sι(T) = 0(s2(T)). In contrast,
RMSPROP/Adam with at = α satisfies sι(T) = O (log ∣V⅞ | + T + 1) and s2(T) = Ω(√T), so
s1(T) 6= o(s2 (T)) (see Appendix for details).
•	Comparison to Prior Analysis. The special case of (2) for a diagonal case (b = 1) provides
the convergence bound of standard Adam-type diagonal adaptations which was previously studied
in (Chen et al., 2019). However, our bound is strictly tighter than that of (Chen et al., 2019).
Specifically, the Term B in (Chen et al., 2019) involves kat-ι∕^v't-ι - αt∕√Vtkι while ours is
l∣at-ι∕PVt-I - at/√vtk∞.
•	Convergence of AdaGrad. We can instantiate our theorem for block diagonal extensions of
AdaGrad/AdaFom (Chen et al., 2019; Zou & Shen, 2018) where the benefit of using block
diagonal adaptation is explicit:
Corollary 1. (ADAGRAD/AdaFom) Consider block diagonal version of ADAGRAD/AdaFom
with at = a∕√t under Assumption 1 (in case of ADAGRAD, βι = 0). Then, they
achieve sι(T) = O (log | VbT | + log T +	∣{zz}	) and s2(T) = Ω(√T), hence we have
From Term A	From Term B and others
mint∈[T] E[∣Nf (Xt )『]=O (log | VT ∣∕√T + log T ∕√T + 1∕√T). Moreover, for any coordinate
partition it is guaranteed that log |VT | decreases as a block size b increases.
•	Convergence of EMA-based Algorithms. Now, we consider popular EMA-based algorithms
such as RMSPRoP/ADAM, i.e., the design function Ht in Algorithm 1 constructs Vt as Vt =
β2Vbt-1 + (1 - β2)gtgtT with β2 ∈ [0, 1). For the convergence guarantee of this family, we need
the following matrix Λt := at-1Vbt--11/2 - atVbt-1/2 should be positive semidefinite which is a
generalized version of Γt := at-ι∕√Vt-1 - at∕√Vt ≥ 0 in previous analysis (Reddi et al., 2018;
Chen et al., 2019). From Proposition 1 in Appendix E, we can expect that the Term A/Term B of
RMSprop/Adam also have similar dynamics as those of AdaGrad. For empirical studies, we
design a simple experiment with MLP 784-100-10 on MNIST dataset. We optimize the network
parameters via block diagonal extension of ADAM with constant β1,t = 0.9 and β2 = 0.999. Figure
3 illustrates the Term A/Term B/Log-determinant for stepsize at = 10-3. In Figure 3, both the Term
A and log |VT| decreases as a block size b increases, which corroborates Proposition 1.
3.2 Uniform Stability and Generalization Error Bounds of Algorithm 1
The generalization error of a randomized algorithm A (e.g., SGD) on training data S is defined as
gen := ES,A RS(A(S)) - R(A(S)) where RS and R denote empirical risk and population risk
5
Under review as a conference paper at ICLR 2020
respectively. Hardt et al. (2015) show that an -uniformly stable algorithm satisfies ∣gen∣ ≤ where
-uniform stability is defined as follows:
Definition 1. (Hardt et al., 2015) Let S, S0 ∈ Zn be two datasets of size n that differ in only one exam-
ple. The Algorithm A is said to be -uniformly stable if supz∈DEA f(A(S); z) - f(A(S0); z) ≤.
In order to bound the generalization error using the result of (Hardt et al., 2015), it would suffice
under a Lipschitz continuity as in our Assumption 1-(b) to show that EA kθ - θ0k2 is bounded
since supz∈D EA f(A(S); z) - f(A(S0); z) ≤ GEA kθ - θ0k2. Here, we consider an EMA-based
design function Ht and defer the result of ADAGRAD to Appendix.
Theorem 2. (EMA-BASED) For αt = α and β1,t = 0, we have the following recurrence relation,
EaT+』≤ n√ι-βr hqgdVT)+qg(vbT)]+«(1 - 1 JT
1	t 1	. .1	1 仆	1 T^7∙∕ 1	r 11	1 ∙ . I ,1	. • . •
where t0 denotes the time when Vt and Vt0 becomes full-rank with the quantities
g(VbT ) :=
t0(l-β2 )G2
^δ2
一 --	. ʌ . . .ʌ -，一 、- -1 .
+ E[log |VbT| — log |Vt01] + d(T - to) log 卷,and
X---------------------}	〜
Term C
JT := GPT=1 E[∣KVt1/2 + δI)-1∣∣∣2 + |西1/2 + δI)-1g] + LPw E[∣∣∣(Vt01/2 + δI)-%∆t].
'-----------7------------} '--------------7------------}
Term D	Term D
'-----------V-------------}
Term D
In the quantities g(VT) and JT, we remark the Term C/Term D since only those terms depend on the
block sizes. Therefore, we investigate the dynamics of Term C/Term D as we will see below.
•	Dynamics of Term C/Term D. In Theorem 2, the Term C
is smallest when b = d as in Corollary 1 while the Term D is
smallest for b = 1 since maxi Aii ≤ λmax(A) for any matrix
A ∈ S++. By the way, the Term D can be bounded as IIl(Vt01/2 +
δI)-1∣∣2 ≤ 1∕δ which is independent of T. For empirical studies,
we revisit our experiment with MLP 784-100-10 on MNIST
dataset. The Figure 4 shows that ɑ∣∣∣(Vbt01/2 + δI)-1∣∣2 converges
to ɑ∕δ (Here, α∕δ = 10-3∕10-4 = 10) regardless of block sizes
and the difference among block sizes is negligible compared to
log ∣VbT ∣ (see Figure 3). As a result, the Term C is dominant, so
we can expect that E[∆t] grows slower for b > 1 than b = 1.
Figure 4: Term D
•	Large δ Improves Generalization. Zaheer et al. (2018) suggest that one should use large δ such
as 10-3 to improve generalization but with only empirical studies. In Theorem 2, it can be seen
clearly that the growth rate of ∆t is slower for large δ, which in result improves generalization.
4 Improving Generalization and Computational Cost
Interpolating SGD via Spectrum-Clipping. It has been shown in Wilson et al. (2017) that adap-
tive methods are better than vanilla Sgd in the early stage but get worse as the learning process
matures. To address this, Keskar & Socher (2017) suggest training networks with Adam at the
beginning and switching to SGD later. Luo et al. (2019) propose methods ADABOUND/AMSBound
which clip the effective learning rate at∕(√Vt + e) of ADAM by decreasing sequence of intervals
It = [ηl(t), ηu(t)] every iteration which converges to some point, thereby resembling SGD in the end.
However, this type of extension is not obvious in our framework due to the absence of effective learning
rate in our case. Instead, we observe that the spectral property is important in our analysis (In Theorem
1 and 2: both convergence and generalization depend on log ∣VbT ∣). Motivated on them, we propose a
spectrum-clipping scheme which clips the spectrum of 8(匕1/2 + δI)-1 by decreasing sequence of
intervals. For spectrum-clipping, we use the following modified update rule in Algorithm 1 after con-
structing Vt:⑴ Ut, ∑ 1/2二一 SVD(Vt1/2), (ii) ∑-1/2 J Clip(λ(αt(∑ 1/2 + δI)-1), %(t), λu(t)),
and (iii) xt+1 J Xt — UT∑ JtlUtmt∙ We schedule the sizes of clipping intervals converging to a
single point uniformly over the spectrum so that 乜(匕1/2 + δI)-1 can be easily computed in the
form of constant times identity matrix and effectively behaves like vanilla Sgd.
6
Under review as a conference paper at ICLR 2020
LeNet-S-Caffe
"τ I τ <.
-----
Ooooo
Illll
SSO~∣ 6U_U」J_
25
50
75	100
Epochs
97o
LeNet-S-Caffe
'9'8
9 9
(％) Aejn4th
25	50	75
Epochs
100
AdsBlock-IO ------ Add Block-25 ------ Adam ------- GGT
Figure 5: Results on MNIST classification.
sso-j6u≡s∙Jl
(a) Point Cloud Classification on DeepSets
(b) MIG score on β-TCVAE
Figure 6: Results on point cloud classification and β-TCVAE.
Randomized Layer-wise AdaBlock (RAdaBlock). To further reduce the computational cost,
we propose a randomized update scheme. At each iteration, we select l layers at random to be updated
via AdaBlock, while the remaining layers are updated via the usual diagonal approximation. By
bridging block-diagonal and diagonal adaptation, we wish to combine the advantages of both ap-
proaches: fast per-iteration time of diagonal adaptations and faster convergence/better generalization
of AdaBlock. Additional considerations on computations and memory are provided in Appendix.
5	Experiments
We consider three sets of experiments. The first shows the differences between block-diagonal and
diagonal versions. The second investigates whether block diagonal matrix adaptation can achieve
state-of-the-art performance on benchmark architecture/dataset for various important deep learning
problems. The third evaluates RadaBlock, which aims at further reducing computational cost. For
the first set, we do not consider the spectrum-clipping or randomized update in Section 4 to clearly
assess the effect of coordinate grouping. In Algorithm 1, coordinate grouping can be done in a number
of ways. Given our insight that grouping weights in the same layer could be more effective, we
consider Figure 1-(c) with grouping 10 or 25 weight parameters connected to input-neuron for dense
layer and filter-wise grouping for convolutional layers in Figure 1-(d). In all our experiments, we use
block diagonal version of Adam. Details on settings/hyperparameters are provided in Appendix C.
Investigating Grouping Effect. We investigate the effect of coordinate grouping on (i) MNIST
classification, (ii) Point cloud classification on DeepSets, and (iii) Deep variational autoencoder.
MNIST Classification. We consider a simple LeNet-5 network. We use 128 mini-batch size and train
networks with 100 epochs. As Figure 5-(a) illustrates the results, the learning curve looks similar in
the early stage of training, but AdaBlock converges without oscillations in the latter part of training,
which corroborates the effect of block sizes in Theorem 1 and 2. The generalization of AdaBlock
also becomes more stable than diagonal variant and GGT, and overall superior across epochs.
7
Under review as a conference paper at ICLR 2020
0.04
DenseNet-BC-100-12
QQQ
Ooo
SSCrl 6u,ll
DenseNet-BC-100-12
O O
7 6
(％) A2-lnvl
DenSeNet-BC-Ioo-12
(％) Au2-lnuuvtt°,l
300	150	225	300
Epochs
—GGT --------- AdaBIock-CIip
0	75	150	225	300 “θ 75	150	225
Epochs	Epochs
--- SGD ------ Adam ------ AdaBound ----- AMSBound
S
85.0
CT
c
—4 5
m TQ
F
4.0
0
LSTM
20	40
Epochs
Figure 7: Results on CIFAR classification.
3-Layer Deep LSTM
Γ - i
-8-64
4 4 4
(& Xgd-JgdJ.) 60-
20	40
Epochs
SN-GAN
5 5
7 6
3J0s UoQd①u-
0	10000	20000	30000
WaII-CIockTime (sec)
---RadaBlock --- Adam
(b) Inception score on SN-GANs
60
----SGD --------- Adam -------- AdaBound --------- AMSBound --------- GGT AdaBIock-CIip
(a) Training loss/test perplexity on 3-Layer deep LSTM
Figure 8: Results on language models and generative adversarial nets.
Point Cloud Classification. We evaluate ADABLOCK on classifying point-cloud representation of a
subset of ShapeNet objects (Chang et al., 2015), called ModelNet40 (Wu et al., 2015). This dataset
consists of 40 classes of 3-dimensional objects. Each object is represented as a point cloud which we
treat as a set of n particles in R3. For this task, we employ DeepSets (Zaheer et al., 2017) architecture
and follow their settings with n = 1000. Figure 6-(a) shows that the learning curve has a similar
behavior, but AdaBlock outperforms Adam for any block size in terms of generalization.
Deep Variational Autoencoder. We conduct experiments on a very recent variant of VAE called
β-TCVAE (Chen et al., 2018). The goal of this model is to make the encoder q(z|x) give disentangled
representation z of input images x by additionally forcing q(z) = q(z|x)p(x)dx to be factorized,
which can be achieved by giving heavier penalty on total correlation. We evaluate our optimizer
with the Mutual Information Gap (MIG) score they proposed, to measure disentanglement of the
latent code. Following implementation in (Chen et al., 2018), we use convolutional encoder-decoder
for β-TCVAE on 3D faces dataset (Paysan et al., 2009). Figure 6-(b) illustrates the results over 10
random simulations with 95% confidence intervals. ADABLOCK outperforms diagonal version with
a smaller variance except at β ∈ {9, 10}, and we can achieve the best performance at β = 5.
Improving Performance with Spectrum-Clipping. We demonstrate the superiority of our algo-
rithms using more complex benchmark architecture/dataset for two popular tasks in deep learning:
image classification and language modeling. For both tasks, vanilla Sgd with proper learning rate
scheduling has enjoyed state-of-the-art performance. Therefore, we compare algorithms using our
spectrum-clipping methods that can exploit higher generalization ability of vanilla Sgd.
CIFAR Classification. We conduct experiments using DenseNet architecture (Huang et al., 2017).
Figure 7 illustrates our results on CIFAR-100 dataset, and the figure for CIFAR-10 is in appendix.
In both cases, the training speed of AdaBlock at the early stage is similar or slightly slower, but
we can arrive at the state-of-the-art generalization performance in the end among all comparison
algorithms. Specifically, we can achieve great improvement in generalization about 0.5%.
8
Under review as a conference paper at ICLR 2020
Language Models. We use recurrent networks (Zaremba et al., 2014), base architectures still fre-
quently used today for language modeling. While (Zaremba et al., 2014) uses only two layers
maximum, we add one more layer to consider more complex and deeper networks. To consider
similar model capacity as (Zaremba et al., 2014), we use 500 hidden units on each layer. Based on this
architecture, we build a word-level language model using 3-layer LSTM (Hochreiter & Schmidhuber,
1997) on Penn TreeBank (PTB) dataset (Marcus et al., 1994). Figure 8-(a) shows the experimental
results: the optimizer with spectrum-clipping of AdaBlock outperforms all the other algorithms
w.r.t. learning curve. It achieves similar perplexity as GGT and outperforms the other methods.
Reducing Computational Cost with RAdaBlock. We consider generative adversarial nets (a.k.a.
GANs). Since it is well-known that training GANs generally takes a lot of time, it is reasonable
to evaluate RAdaBlock on this task. We choose recently proposed spectral normalization GANs
(a.k.a. SN-GANs) which control the Lipschitz constant of the discriminator (Miyato et al., 2018),
thereby stabilizing the training procedure. SN-GANs are still generally used as baselines, so we
use a CIFAR-10 dataset on standard CNN architecture and the inception score (Salimans et al.,
2016) for quantitative assessment. For RadaBlock, at each iteration, we update two layers via
AdaB lock chosen randomly, one for the generator and one for the discriminator. Figure 8-(b)
depicts inception score vs. wall-clock time for each method. RAdaBlock achieves higher inception
score in wall-clock time. We conjecture that a block diagonal approximation has a regularization
effect and leave this as an open question for future work.
6	Concluding Remarks
We proposed AdaBlock, a general adaptive gradient framework that approximates exact GOP with
block diagonal matrices via coordinate grouping, to effectively utilize structural characteristics of deep
learning architectures. We analyzed convergence and generalization for our approach, highlighting
benefits compared to its popular diagonal counterpart, and confirmed our findings theoretically
and empirically. We also proposed a spectrum-clipping extension which achieved state-of-the-art
generalization performance on popular deep learning tasks and a randomized approach to further
reduce computational cost. As future work, we plan to explore strategies for (i) setting the clipping
parameters in spectrum-clipping, to strike the best balance between training speed and generalization
ability, and (ii) selecting the layers that would benefit most from block-diagonal adaptation at each
iteration in RadaBlock.
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efficient full-matrix adaptive regularization. In International Conference on Machine Learning,
pp.102-110, 2019.
Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,
Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d
model repository. arXiv preprint arXiv:1512.03012, 2015.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems, pp.
2610-2620, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In International Conference on Learning Representation
(ICLR), 2019.
Timothy Dozat. Incorporating nesterov momentum into adam. ICLR Workshop, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Journal of Machine Learning Research (JMLR), 2011.
9
Under review as a conference paper at ICLR 2020
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex
optimization. Machine Learning, 69(2-3):169-192, 2007.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university Press, 2012.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, PP. 4700-4708, 2017.
Nitish Shirish Keskar and Richard Socher. ImProving generalization Performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In International
Conference on Learning Representation (ICLR), 2015.
Jean Lafond, Nicolas Vasilache, and Leon Bottou. Diagonal rescaling for neural networks. arXiv
preprint arXiv:1705.09319, 2017.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. AdaPtive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson,
Karen Katz, and Britta Schasberger. The Penn treebank: annotating Predicate argument structure.
In Proceedings of the workshop on Human Language Technology, PP. 114-119. Association for
ComPutational Linguistics, 1994.
H Brendan McMahan and Matthew Streeter. AdaPtive bound oPtimization for online convex oPti-
mization. In Conference on Computational Learning Theory (COLT), 2010.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. SPectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=B1QRgziT-.
Vinod Nair and Geoffrey E Hinton. Rectified linear units imProve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), PP. 807-814,
2010.
Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face
model for Pose and illumination invariant face recognition. In 2009 Sixth IEEE International
Conference on Advanced Video and Signal Based Surveillance, PP. 296-301. Ieee, 2009.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representation (ICLR), 2018.
Herbert Robbins and Sutton Monro. A stochastic aPProximation method. The annals of mathematical
statistics, PP. 400-407, 1951.
10
Under review as a conference paper at ICLR 2020
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems, pp.
2234-2242, 2016.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems (NIPS), 2017.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1912-1920, 2015.
Adams Wei Yu, Lei Huang, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell. Block-
normalized gradient method: An empirical study for training deep neural network. arXiv preprint
arXiv:1707.04822, 2017.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 9793-
9803, 2018.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329, 2014.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Shuai Zheng and James T Kwok. Blockwise adaptivity: Faster training and better generalization in
deep learning. arXiv preprint arXiv:1905.09899, 2019.
Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural
networks. arXiv preprint arXiv:1808.03408, 2018.
11
Under review as a conference paper at ICLR 2020
Appendix
A	Toy MLP example: Full GOP adaptation vs. Diagonal
APPROXIMATION
We consider a structured MLP (two nodes in two hidden layers followed by single output). For hidden units, we
use ReLU activation (Nair & Hinton, 2010) and the sigmoid unit for the binary output. We generate n = 10 i.i.d.
observations: Xi 〜N(0, I2) and yi from this two layered MLP given Xi. The results of our toy experiment are
depicted in Figure 9.
0 2 4
--
)ssoL gniniarT( 01gol
0 2 4
--
)ssoL gniniarT( 01gol
0	250	500
The number of epochs
-0.2-
-0.4-
-0.6-
ADAGrAD-DiAg
---ADaGraD-FULL
8
.
0
-
)ssoL gniniarT( 01gol
0	250	500
The number of epochs
0	250	500
The number of epochs
Figure 9: Comparison of AdaGrad diagonal version and full matrix version varying the minibatch
size M.
B	Computations and memory considerations
Table 1: Test error (%) for CIFAR dataset.
	SGD	Adam	Ada- Bound	Ams- B ound	GGT	Ada- Block- Clip
CIFAR-10	4.51	6.07	4.78	4.77	6.33	4.34
CIFAR-100	22.27	26.51	22.5	22.52	22.17	21.7
Table 2: Average time (sec) per iteration for CIFAR-100 experiments.
SGD	Adam	Ada- B ound	Ams- B ound	GGT	Ada- Block	Ada- Block- Clip
0.080	0.108	0.126	0.130	0.166	0.226	0.251
Compared with full matrix adaptation, working with a block diagonal matrix is computationally more efficient as
it allows for decoupling computations with respect to each small full sub-matrix. In Algorithm 1, the procedures
for constructing the block diagonal matrix and for updating parameters for each block by computing the “inverse”
square root of each sub-matrix can be done in a parallel manner. As the group size increases, the block diagonal
matrix becomes closer to the full matrix, resulting in greater computational cost. Therefore, we consider small
group size for our numerical experiments. For instance, on CIFAR-100 dataset, the average time per iteration of
AdaBlock methods is at most twice that of their diagonal counterparts, but this comes with a great advantage
since AdaBlock shows significant improvement in generalization. In comparison the average time per iteration
of GGT Agarwal et al. (2019) (the modified version of full-matrix Adagrad) is 1.5 times that of Adam but the
generalization of GGT is worse (see Table 1 and 2). Moreover, the wall-clock time performance of AdaBlock
can be improved using RAdaBlock. As can be seen in Figure 8, RAdaBlock achieves higher inception
score in wall-clock time.
12
Under review as a conference paper at ICLR 2020
In terms of memory, our method is more efficient than GGT Agarwal et al. (2019) (the modified version of
full-matrix ADAGRAD). For example, consider models with a total of d parameters. For Algorithm 1, assume
that Vbt is a block diagonal matrix with r sub-matrices, and each block has size b × b (so, br = d). Also, assume
that the truncated window size for GGT is w. GGT needs a memory size of O(wd), and our algorithm requires
O(rb2) = O(bd). We consider small group size b = 10 or 25 for our experiments while the recommended
window size of GGT is 200 (Agarwal et al., 2019). Therefore, our algorithm is more memory-efficient and the
benefit is more pronounced as the number of model parameters d is large, which is the case in popular deep
learning models/architectures.
C	Hyperparameters and Additional Experimental Results
We use the recommended step size or tune it in the range [10-4 , 102] for all comparison algorithms. For
ADAM based algorithms, we use default decay parameters (β1, β2) = (0.9, 0.999). For a diagonal version
of ADAM variant algorithm, we choose numerical stability parameter = 10-3 since the larger value of
can improve the generalization performance as discussed in (Zaheer et al., 2018). For spectrum-clipping in
Section 4, We use the same intervals λι(t) = α*(1 — (1-β；"+]) and λu(t) = α*(1 +(1-^)t) as in Luo
etal.(2019). For Y and a* in clipping bound functions, We consider Y ∈ {0.0001, 0.0005, 0.001} and choose
ɑ* ∈ {αSGD, 5αSGD, 10osgd} where Osgd is the best-performing initial learning rate for vanilla Sgd (These
hyperparameter candidates are based on the empirical studies in Luo et al. (2019)). As in Luo et al. (2019),
our results are also not sensitive to choice of Y and α*. With these hyperparameters, we consider maximum
300 epochs training time, and mini-batch size or learning rate scheduling are introduced in each experiment
description. Our Algorithm 2 requires SVD procedures to compute the square root of a block diagonal matrix.
We apply SVD efficiently to all small sub-matrices simultaneously through batch mode of SVD.
MNIST Classification. We consider the following LeNet-5 network architecture, 20C5 - MP2 - 50C5 -
MP2 - 500FC - softmax. Designing for corroborating our theoretical results, we employ the numerical stability
parameter δ = = 10-4.
Deep Sets. For point cloud classification, we follow the same settings in Zaheer et al. (2017) and use the same
architecture in Appendix H of Zaheer et al. (2017). Also, we use the author’s implementation only replacing the
optimizer with ADABLOCK. As they use = 10-3 for ADAM, we also employ the same value as δ = 10-3 in
Algorithm 1 for fair comparison.
β -TCVAE. For experiments on generative models β-TCVAE, we use the author’s implementation only
replacing the Adam optimizer with our B lock-Adam. We use convolutional networks for encoder-decoder
they consider in Chen et al. (2018) and mini-batch size 2048. For generative models, the small value for ADAM
works better than large value in our experiences, so we employ δ = = 10-8.
CIFAR classification. According to experiment settings in (Huang et al., 2017), we use mini-batch size 64
and consider maximum 300 epochs. Also, we use a step-decay learning rate scheduling in which the learning
rate is divided by 10 at 50% and 75% of the total number of training epochs. With this setting, vanilla SGD
with a momentum factor 0.9 performs best with initial learning rate α* = 0.1, so we use this value for our
bound functions of spectrum-clipping, λl(t) and λu(t). As recommended in Zaheer et al. (2018), we employ
δ = = 10-3 for better generalization.
Language Models. In this experiment, we use a dev-decay learning rate scheduling Wilson et al. (2017)
where we reduce learning rate by a constant factor if the model does not attain a new best validation performance
at each epoch as in Zaremba et al. (2014). Under this setting, vanilla Sgd performs best when the initial learning
rate α* = 5. As recommended in Zaheer et al. (2018), we employ δ = e = 10-3 for better generalization.
SN-GANs. Since the Adam is a classical optimizer (than vanilla Sgd) for training GANs with tuned
decaying parameters (β1, β2). As parameter configuration in Miyato et al. (2018) (Table 1 in Section 4), we
set (β1 , β2) = (0.5, 0.999) with stepsize α = 0.002. The number of updates on discriminator ndis per single
update on generator is set to 1. For RAdaBlock, we update two layers via AdaBlock randomly at each
iteration, one for the generator and one for the discriminator. As noted in β-TCVAE, the small works better in
our experiences, so we employ δ = = 10-8.
13
Under review as a conference paper at ICLR 2020
ss<r∣ mc-c-SF
25
50	75	100
The number of epochs
2 5 8
Ooo
111
ss<r∣ mc-c-SF
LeNet-5-Caffe
25	50	75
The number of epochs
——RMsprop-Group-IO ——RM Sprop-G roup-25	——RMSprop-DIag
100
sso~∣ 6uws,l.L
LeNet-S-Caffe
25
50	75	100
The number of epochs
——RMSprop-Group-10 ——RM Sprop-G roup-25 ——RMSprop-DIag
0.00
0
Figure 10: Training loss/test accuracy for MLP/LeNet-5-Caffe with block diagonal RMSprop.
SGD
Adam-Diag ------ AdaBound ------ AMSBound
0.02
DenseNet-BC-100-12 (CIFAR-IO)
225	300
The number of epochs
GGT ------ Adam-Group-Clip
≥1 99
E
n
U
< 98
Figure 11: Training curve and test accuracy for DenseNet-BC-100-12 on CIFAR-10 dataset.
D	General Frameworks
Algorithm 2 Adaptive Gradient Methods with Block Diagonal Matrix Adaptations via Grouping
Input: Stepsize αt, initial point x1 ∈ Rd, β1 ∈ [0, 1), and the function Ht which designs Vbt.
Initialize: m0 = 0, Vb0 = 0, and t = 0.
Assumption: We have r blocks with each size n × n and nι +---+ n = d, and βι,t ≥ β1,t+1
fort = 1,2,. . . ,T do
Draw a minibatch sample ξt from P
offset - 0
Gt — 0
gt J ▽/(Xt)
mt J β1,tmt-1 + (I - β1,t)gt
for each group index j = 1, 2, . . . , r do
gt(j) J gt[offset : offset + nj]
Gt[offset: offset + n7-, offset: offset + n7-] J g(j) (g(j))T
offset J offset + nj
end for
^
Vt J Ht(Gι, ∙∙∙ ,Gt)
1/2
xt+1 J xt - αt(Vt	+ δI)-1mt
end for
We provide the general frameworks of adaptive gradient methods with exact full matrix adaptations. The
Algorithm 3 and 4 represent the general framework for each case. We can identify algorithms according to the
functions ht (Table 3) and Ht (Table 4) which determine the dynamics of vbt and Vbt respectively. Also, the
Algorithm 2 is a detail version of the Algorithm 1.
14
Under review as a conference paper at ICLR 2020
Algorithm 3 General Adaptive Gradient Methods
approximating gtgtT via DIAGONAL Matrix
Input: Initial point x1 ∈ Rd, stepsize {αt}tT=1,
decay parameters β1,t , β2 ∈ [0, 1], and > 0.
Initialize: m0 = 0, vb0 = 0.
for t = 1, 2, . . . , T do
Draw a minibatch sample ξt from P
gt Jkf (xt ξt)
Gt J diag(gtgT)
mt J β1,tmt-1 + (1 - β1,t)gt
Vt J ht(Gι, G2,..., Gt)
xt+1 J Xt - αtmt∕(√Vt + E)
end for
Output: xb.
Algorithm 4 General Adaptive Gradient Methods
with the exact gtgtT (FULL Matrix)
Input: Initial point x1 ∈ Rd, stepsize {αt}tT=1,
decay parameters β1,t, β2 ∈ [0, 1], and δ > 0.
Initialize: m0 = 0, Vb0 = 0.
for t = 1, 2, . . . , T do
Draw a minibatch sample ξt from P
gt J kf(xt; ξt)
Gt J gtgtT
mt J β1,tmt-1 + (1 - β1,t)gt
^
Vbt J Ht(G1, G2, . . . , Gt)
Xt+1 J Xt - at(Vbt1/2 + δI)-1mt
end for
Output: Xb.
Table 3: Variants of diagonal matrix adaptations
βι,t bt^^	β1,t = 0	β1,t = β1
1	Sgd	-
(1/t) PLI g2	AdaGrad	AdaFom
β2vt-1 + (I - β2)g2	RMSProp	Adam
vt = β2vt-1 + (1 - β2 )gt2, vbt = max{vbt-1,vt}	-	AMSGRAD
Table 4: Variants of full matrix adaptations
∖∖βι,t Vr^	β1,t =0	β1,t = β1
^ 	匕=I	Sgd	-
Vt = T PT=I gtgT	AdaGrad	AdaFom
^	^	^E Vt = β2Vbt-1 + (I- lβ2)gtgl	RMSPROP	Adam
Vt = Ut ∑tUT,		
R = Ut max{Σt-1, ∑t}UT	-	AMSGRAD
E	Details for Convergence Analysis in Section 3.1
E.1 Proofs for Corollary 1
Before moving onto proofs, we need the following technical lemma
Lemma 1. (Lemma 12 in Hazan et al. (2007)) For positive definite matrices A and B, the following inequality
holds
Tr(AT(A - B)) ≤ log |A| - log |B|
For AdaGrad, We have αt = ɑ∕√t and Vt = ∣ P；=i gigT ：= ∣Gbt. First, We bound the Term A/Term B to
find the s1 (T).
15
Under review as a conference paper at ICLR 2020
The Term A is
T
X "F2
t = 1
t0	T
X gtZT/2gtk2+ X	gt%τ∕2gtk2
t = 1	t = t0 + 1
、	一、.一	J

Since the first term in RHS is independent of T, so we only need to bound Ti. The quantity Ti can be bound as
T
Ti = X k«tVt-1/2gtk2
t=t0+1
T
=α2 X ɪ kVtT%tk2
t=t0+1
一1
=α2 X Tr(Vt Wgtgt)
t=t0+1
T
≤ α2 X [log I Vbt I - log I -ηr-Vt-1 I ]
t=t0+1
T
=α2 X [log ∣ Vt ∣ - log i Vt-1 i + d log ^-ɪ]
t=t0+1
=α2(log IVr | - log ∣Vt01 + d log T) = O (log IVr | + log T)
Next, we bound the Term B. Similarly to the Term A, the Term B can be splitted as follows
TT
X Qt = XI I I at-1Vt-1/2 - atVt-1/2 ∣ ∣ ∣ 2
t=2	t=2
t0	T
XI I I αt-1 k 1/2-"T/2 III 2 + X III at-1Vt-1/2 -αtVr2∣∣∣ ,
t = 2	t = t0 + 1
、	-、z-	J
玉
Also, in this case, the first term in RHS is independent of T, so we only bound T2. The T2 can be bound as
X IIIαt-1VtZ1/2 - αtVt-1/2 ∣ ∣ ∣ 2 = α X ∣∣∣ ɪ Vt-1/2 - √t V”2 ∣∣∣ 2
t=t0 + 1	t=t0+1 V	Vt
T
=α X III G-T- G-1/2III 2
t=t0+1
T
≤ α X Tr(G-T- G-")
t=t0+1
=«(Tr(Gt；1/2) - Tr(GT"))
≤ αTr(Gt01/2) = O(1)
The remaining term is PT-1 Qt which can be derived from Term B with slight modifications.
∑ Q2 = ∑∣ I I αt-1Vt-1/2 - αtVt-1/2 ∣ ∣ ∣ 2
t=2	t=2
t0	T — 1
=XI I I at-1Vt—1/2 - αtVt-1/2 ∣ ∣ ∣ 2 + X IIIat-1Vt—1/2-atVtT/2||| :
t = 2	t = t0 + 1
、	-	-	J

16
Under review as a conference paper at ICLR 2020
Similarly to the Term B, we can bound T3 with a little modification as
T-1	T-1
X M-IVt-1/2 -αtVt-1∕2 = α2 X M-1/2- G-1 /2
t=t0+1	t=t0+1
T-1
=α2 X Tr((G-T - G-1/2)2)
t=t0+1
T-1
≤α2 X Tr(Gbt--11 - Gbt-1
t=t0+1
=α2Tr(Gt01 - GT-ι)
≤ α2Tr(Gt01)
Therefore, We have si(T) = O(log ∣Vτ∣ + logT +1). Lastly, We should bound the LHS of 2.
T
E X at(Vf (xt),Vt-1/2Vf (xt))
t=1
T2
≥ E XγtVf(xt)
t=1
T
≥ E tm∈[iTn]{kVf (xt)k2} X γt
NoW, We bound the sum of γt .
Yt = λmin (αt^Vt-1/2) = αλmin(G-1/2)
α
^^t
P kgτk2
τ=1
≥
≥
α
√TG∞
From above observations, We have
T
Xγt
t=1
T
≥ X √TG∞
α√T
^G∞
Therefore, we have Ω (s2 (T)) = Ω(√T). Finally, we conclude that
minjvf(χt)n2=o( log∣vT∣√Tog T+1
Now, we need a following lemma for relation between block sizes and convergence.
Lemma 2. (Fischer’s inequality, Theorem 7.8.5 in Horn & Johnson (2012)) For a positive definite matrix
A ∈ Rn×n, let B ∈ Rk×k and C ∈ Rn-k×n-k be top left corner of A and bottom right corner of A
respectively. Then, det(A) ≤ det(B) det(C) holds.
This lemma says that log |VT | becomes larger as a block size b decreases.
E.2 Proposition for RMSprop/Adam
Now, we can obtain intuition on the dynamics of Term A/Term B for EMA-based algorithms with the following
proposition.
Proposition 1. For block diagonal extensions of RMSPROP/ADAM, we set exponentially decaying stepsize
at = α(√β2)t-1. Then, the Term A depends on the log 1VT | - log 1Vt01 and the Term B is upper bounded with
constant for any block sizes. Here, t0 is the time when Vbt becomes full-rank.
17
Under review as a conference paper at ICLR 2020
Proof. First, we will bound the Term A.
T	t0	T
EgtVtT/2gtk2 = EgtVtT/2gtk2+ E gt%τ∕2gtk2
t = 1	t = 1	t = t0 + 1
The first term in RHS is independent of T, so we only have to bound the term Ti. The Term Ti can be bound as
follows
TT
Ti = X gtVr1/2gtk2= X g(√^)tτ%τ∕2gt∣∣2
t=t0 + 1	t=t0+1
T	T	T
≤ X kαVtT2gtk2 = α2 X	kVtT2gtk2 = α2 X Tr(VtTgtgT)
t=t0 + 1	t=t0+1	t=t0+1
& T
≤ 1 _ β X [log ∣½∣ - log ∣β2½-1∣]
—02 t=t0 + 1 L
八2	T
=1 _ β2 X [log ∣Vt∣- log MTI+ dlog ɪ]
t=t0+1
α2	/	, ^ ,	, ^ ,	_ 、	1、
=Tj~~^^(log IVr ∣ - log ∣Vt01 +d(T - to)log-)
1 — β2 ∖、7/	β2 )
Dependent on T
Summing up all the terms, we have
T	t0	2
X gtVt-1∕2gt k2 ≤ X gtVt-1∕2gt k2 + —r (log IVT ∣-log ∣% ∣ +d(T - to) log ɪ)
M	匕	1 - β2、|----V-------'	β2)
t=1	t=1	Dependent on T
Now, we derive the bound for the Term B.
TT
X Qt = XM-IVt-1/2-。心勺2
t=2	t=2
t0	T
=Xl I I at-1Vt-1/2 -atVt-1/2 I I I 2 + X III αt-1Vt-1/2 -。海-1/2||| 2
t = 2	t = t0 + 1
As in the Term A, the first term in RHS is independent of T, and we can bound the second term as
TT
X III αt-1Vt-1/2 - αtZT/2III 2 = αXIII (√β2)t-2½-1/2 - (√β2)t-1½-1/2 I II 2
t = t0 + 1	t = 2
T	I
=α X (Pβ2)t-1 I I I √⅛Vt-1/2-VtT/2III 2
t=t0+1	”β2
T
=α X (Pβ2)t-1 I I I (β2½-1)-1/2 -VtT/21
t=t0 + 1
T
2
≤ α X (√β2)t-1 Tr((β2优-1)T/2- ZT/2)
t=t0+1
T
=α X Tr((√β2)t-2Vt-Y2 - (√β2)t-1½-1/2)
t=t0+1
=α(Tr((√β2)t0-1%-1/2) - Tr((√β2)TTV-1/2)
≤ a(√β2)t0-1Tr(½-1/2)
Therefore, the bound for the Term B is independent of T .
T	t0
X Qt ≤ X III ɑt-1½-1/2 - atRT/2III 2 + α(√β2)t0-1Tr(½-1/2) = O(1)
t=2	t=2
18
Under review as a conference paper at ICLR 2020
As a result, we can expect that the Term A is related to the log-determinant of VbT and the Term B is upper
bounded as constant as in AdaGrad.	口
F Proofs of Main Theorems
We study the following minimization problem,
min f (x) := Eξ [f (x; ξ)]
under the assumption 1. The parameter x is an optimization variable, and ξ is a random variable representing
randomly selected data sample from D. We study the convergence analysis of the Algorithm 1. For analysis in
stochastic convex optimization, one can refer to Duchi et al. (2011). For analysis in non-convex optimization
with full matrix adaptations, we follow the arguments in the paper Chen et al. (2019). As we will show, the
convergence of the adaptive full matrix adaptations depends on the changes of effective spectrum while the
diagonal counterpart depends on the changes of effective stepsize. We assume that Vbt-1/2 means pseudo-inverse
1/2
of Vt	if it is not full-rank. Note that, our proof can be applied to exact full matrix adaptations, Algorithm 1
F.1 Technical Lemmas for Theorem 1
Lemma 3. Consider the sequence
β1,t
Zt = Xt + -——-Z—(Xt - Xt-1)
1 - β1,t
Then, the following holds true
zt+1 - zt
β1,t + 1
βι,t
1 - β1,t+1	1 - β1,t
β
atVt-1/2mt - ι β1,t	atVbt-1/2 - αt-1Vt-1/2	mt-1 - atVt-1/2gt
1 - β1,t
—
—
Proof. By our update rule, we can derive
-1/2
Xt+1 - Xt = - αtVt	mt
= -atVbt-1/2 (β1,tmt-1 + (1 - β1,t)gt)
= -atβ1,tVt-1∕2mt-1 - at(1 - β1,t)Vt-1∕2gt
(=) -αtβ1,tVt-1/2
—
a— VtI-I(Xt- xt-1))- αt(1 - e1，t)Vt-1/2gt
旦βι,t(Vt-1 优-1)1/2(Xt - xt-1)- αt(1 - βι,t)Z-1∕2gt
αt-1
β1,t(Xt- Xt-I)+β1,t	急(Vt-IVt-I)1/2- Id	(Xt- Xt-I) - αt(I- β1,t沱-1∕2gt
(iii)
β1,t (xt - xt-1) - β1,t	αtVbt
-1/2 - αt-ιVt-1/2 mt-ι - αt(1 - βι,t)Vj"'2gt
^
The reasoning follows
(i)	By definition of mt .
(ii)	Since Xt = Xt-1 — at-1Vb-1/2mt-1, We Can solve as mt-1 =------ Vb 1/2(Xt — Xt-1).
t-1	αt-1 t-1
(iii)	Similarly to (ii), We can have Vbt1-/12 (Xt - Xt-1)/at-1 = -mt-1.
Since Xt+1 - Xt = (1 - β1,t)Xt+1 + β1,t(Xt+1 - Xt) - (1 - β1,t)Xt, We can further derive by combining the
above,
(1 - β1,t)Xt+1 + β1,t(Xt+1 - Xt)
=(1 - e1,t)Xt + e1,t(Xt - Xt-I) - β1,t ( atVbt 1/2 - αt-1Vt-1/2 )mt-1 - at(1 - β1,t)^bt 1/2gt
19
Under review as a conference paper at ICLR 2020
By dividing both sides by 1 - β1,t,
xt+1 + 1 β1,t (Xt+ 1 - Xt)
1 - β1,t
=Xt +	β1,t— (Xt - Xt-ι) -	β1,t- αatVtτ∕2 - at-iVt-1/2!mt-i - atVt-1/2gt
1 - β1,t	1 - β1,t
Define the sequence
Zt = Xt + ~ 彳 (Xt - Xt-I)
1 - β1,t
Then, we obtain
zt+1 = zt +
β1,t + 1
1 - β1,t+1
ι⅛ (Xt+1- Xt)
—
—
1⅛ αtVtτ2 - αt-1Vt-11/2)mt-1 - αtVtT2gt
—
β1,t+1
1 - β1,t+1
β1,t	-1/2
1-而	αtVt	mt
—
—
1⅛	“-，mt-ι-αt VtT0
By putting zt to the left hand side, we can get desired relations.
□
Lemma 4. Suppose that the assumptions in Theorem 1 hold, then
6
E[f(zt+1) - f(z1)] ≤ XTi
i=1
where
T1 = -E
T2 = -E
T3 = -E
T4 = E
T5 = E
T6 = E
XX *Vf (Zi), 1-β7-biZT/2 - αi-ιV-1/2) mi-)
XX α(vf (Zi), ViT/2gi)#
XX S (I -1⅛)一μ
XX 2L
i=1
β1,i+1
1 - β1,i+1
ι⅛ )—“|2#
X 2Ll 1⅛ 32-α"
—
t
XX 2L。£
—
i1
Proof. By L-Lipschitz continuous gradients, we get the following quadratic upper bound,
L2
f(Zt+1) ≤ f(Zt) +〈▽/(Zt),zt+ι - Zti + — IlZt+ι - ZtIl
Let dt = Zt+1 - Zt . The lemma 1 yields
dt
β1,t + 1
1 - βι,t+ι
β1,t ) αt V-1/2mt ——^t- α αt VT/2
1 - βι,t α 11 t 1 - βι,t Itt
αt-1 Vt--11/2 mt-1 -αt Vt-1/2gt
—
—
20
Under review as a conference paper at ICLR 2020
Combining with Lipschitz continuous gradients, we have
-t
E[f(zt+1) - f (zι)] = E X f(zi+ι) - f(zi)
_ i=1	.
.t、	L
≤ E X〈▽〃&),&)+ 2Mik2
-E
-E
-E
+E
X kf (Zi), τ-βr- QViT/2 - oi-ι%-Y j mi-1)
X OiyfS),「％)]
X S (r⅛ - r⅛ 2/2m)]
With ∣∣α + b + c∣∣2 ≤ 3(kα∣∣2 + k"∣2 + kc∣∣2), we can finally bound by
6
E[f(zt+1) -f(zι)] ≤ X Ti
i=1
T1 + T2 + T3 + E
Lemma 5. Suppose that the assumptions in Theorem 1 hold, Ti can be bound as
TI ≤ G2Γ⅛EIX
1/2
-Oi-iVi-1/2
—
—
□
Proof. From the definition of quantity Ti,
T1
-E
X (vf (Zi),
i=1
r⅛ ET/2 -"E2
(i)
≤
(ii)
≤
(iii)
≤
t
X gf(Zi)∣2
i=1
τ⅛ E
「2	βi
1 - βι
r⅛	―2 -"iVi-1/2
t
X Wf(Zi)k2∣∣αi%τ/2 -αi-i%-1/2
i=1
t
'E X αiVi-
i=1
““I Vi-[U
)mi-i)]
mi-1l2]
∣∣∣ 2kmt-1k2]
E
The reasoning follows
(i)	By Cauchy-Schwarz inequality.
(ii)	For a matrix norm, we have IlAxll2 ≤ ∣∣∣A∣∣∣2 ∣∣x∣∣2. Also, /3： . = 1/],一1 ≤ J① 一1
(iii)	By definition of mt, we have mt = β1,tmt-1 + (1 — βι,t)gt. Therefore, we use a triangle inequality
by IlmtIl2 ≤ β1,t∣∣mt-1∣∣2 + (1 - β1)∣∣gt∣∣2 ≤ (βι,t + 1 - β1,t)max{∣∣mt-1∣∣2, ∣∣gt∣∣2}. Since we
have mo = 0 and ∣∣gt ∣∣ ≤ G, we also have ∣∣mt ∣∣ ≤ G by the mathematical induction.
□
Lemma 6. Suppose that the assumptions in Theorem 1 hold, then T3 can be bound as
T3 ≤
—
β1,t+1
1 - β1,t+1
(G2 + D2)
21
Under review as a conference paper at ICLR 2020
Proof. By the definition of T3 ,
T3
-E
XX (vf (Zi),
i=1
β1,i+1
1 - β1,i+1	1 - β1,i
αi%τ∕2m)]
(i)
≤
(ii)
≤
t
X
i=1
t
X
i=1
t
X
i=1
β1,i+1
1 - β1,i+1
Bi*
1 - Bl,i
2 (kVf (zi)k2 + kai ViT/2mi『)
β1,i+1
1 - β1,i+1
β1,i
1 - β1,i 1 - β1,i+1	2
%
1 - β1,i
β1,i+1	1
(iii)
≤
β1
β1,t + 1
1 - β1	1 - β1,t+1
The reasoning follows
(i)
Use CauChy-SChWarz inequality and ab ≤ 11 (a2 + b2) for a, b ≥ 0.
(ii)
By our assumptions on bounded gradients and bounded final step vectors.
(iii)
The sum over i = 1 to T can be done by telescoping.
Lemma 7. Suppose that the assumptions in Theorem 1 hold, T4 can be bound as
T4 ≤
βι
1 - βι
β1,t+1	∖2D1
1 - β1,t+J
E
E
—
—
%
—
—
—
—
Proof. By the definition of T4 ,
3LT4
3L
(i)
≤
(ii)
≤
(iii)
≤
β1,i+1
1 - β1,i+1
Γ⅛ )22"]
The reasoning follows
(i)
(ii)
(iii)
t
X
i=1
t
i=1
βι
1 - βι
β1,i+1
一 β1,i+1
βl,i
1 - β1,i
2D2
βι
1 - βι
β1,t + 1	ʌ X ( β1,i+1
1 - β1,t+1 × = 1- - β1,i+1
β1,t+1	∖2D1
1 - β1,t+J
β1,i	∖d2
1 - 0i"
From our assumptions on final step vector ||aiV/i-1/2mik2 ≤ D.
We use the relation β1 ≥ β1,t ≤ β1,t+1.
By telescoping sum, we can get the final result.
Lemma 8. Suppose that the assumptions in Theorem 1 hold, T5 can be bound as
ɪ T5 ≤ ( τβλir )2
3L	1 - β1
G2E	αiVbi-
i=2
E
E
—
—
—
—
—
t
^
j-El2#
□
□
22
Under review as a conference paper at ICLR 2020
Proof. By the definition of T5,
(i)
≤
(ii)
≤
The reasoning follows
t
X
i=2
t
X
i=2
d⅛ "2 "1VZ1/2
β1,i
1 - β1,i
β1	ʌ2
1 - βι)
αi忆-1/2 - αi-ι忆-1/2	k
i=2
『1
mi-1H
lmi-ιk2j
1/2--11
(i)	By the matrix norm inequality, we use IlAxll2 ≤ IllAIll2∣∣x∣∣2.
(ii)	We can obtain the result using βι ≥ βι,t ≥ β1,t+1.
Lemma 9. Suppose that the assumptions in Theorem 1 hold, The quantity T2 can be bound as
T2 ≤ L2(占)2T8+ L2(占)2T≡+ 2ElX 皿兄-1/匕『1
+ 2G2E X	aiVi-1/2 - ai-1Vi-1/2
i=2
HUM」
X α (vf(χi), ViT/2Vf(xi),]
Proof. First, note that,
Zi - Xi =	β1,i— (Xi - Xi-1)
1 - β1,i
By the definition of T2 and z1 = X1 , we have
τ⅛ “启1/20-1
T2 = -E
-E
X ai ∕vf (zi),
i=1	∖
X ai (vf (xi),
i=1	∖
ViT/2q)1
f>-1∕2
匕	Qi
X α(vf (Zi) - Vf(Xi),Vt1/2q)]
The second term can be bounded as
-E
X αi (Vf(Zi) -Vf(Xi) ,V
i=1
-1/2用
(i)
≤E
X 2 kvf(zi) - Vf(Xi) k2 + 2 giViT∕2Qik2
(ii)
≤
_ i=1
L2	1
5 T, + 2 E
t
X k—2gik2
i=1	.
3LT5
3L
-E
E
E
t
2
2
G2E E α兄-
t
^
—
□
E
^
(i) is due to Cauchy-Schwarz inequality and ab ≤ ∣a2 + ∣b2 for a, b ≥ 0. (ii) is as follows:
By L-Lipschitz continuous gradients, we have
kVf(zi) -Vf(Xi)Il ≤ LkZi- Xik= L∣∣31忆1/20-1
Il 1 — β1,t
Let T7 be
T = E
X «T⅛"E2mi-ι∣∣2
i=1	1 i	.
23
Under review as a conference paper at ICLR 2020
We should bound the quantity T7, by the definition of mt, we have
i	i
mi = X [( ∏ βι,∣)(i - βι,k)gk]
k=1	l=k+1
Plugging mi-ι into TV yields
t
TLE XBi⅛…1"|
(i)
<
I⅛ )2e
⅛ )2e
t
X
i=2
t
X
i=2
i-1 i-1
ai-iVi-1/2 X[( ∏ β1,1)(1 - β1,k)gfc]
k=1	l=k+1
i-1	i-1
X31匕-1/2[( ∏ β1,1)(1-β1,k)gfc]
k=1	l=k+1
(ii)
<
(1⅛ )2
t	i-1	i-1
X X akV^2[( ∏ β1,ι)(1 - β1,fc)gfc]
i=2 k = 1	l = k+1
T8
β1
1 - β1
t	i-1	i-1
X X(αi%τ∕2 - «kVfc-1/2)[( ∏ β1,ι)(1-β1,k)gk]
i i=2 k = 1	l = k+1
+ 2
2
2
E
J
(i) is by β1 ≥ β1,t and (ii) is by We use the fact (a + b) < 2(∣∣α∣∣2 + ∣∣b∣∣2) in (i). We first bound T⅛ as below
t	i-1	i-1
T8 = E X X α"T∕2[( ∏ β1,ι) (1 - β1,fc)gfc]
_ i=2 k = 1	l = k + 1
2
E
t d /i-1	i-1	∖
XX XaVkT2[( ∏ β1,1)(1-β1,k)gk]
i=2 j = 1 ∖k = 1	l = k + 1	)
j」
E
td
[X X
i-1 i-1	i-1	i-1
XX(akVk-1/2gk).( ∏ β1,ι)(1-β1,k )(αp/τ∕2 9p) ( ∏
k=1p=1	j l=k+1	j q=p+1
β1,q) (1 - β1,p)
)]
<E
XX (X 广国-1-，(俄-1-呜｛(αkHT2gk)2+ (αp/T2gp)2))
E
t d i-1	2 i-1
XX(X(*T)(ak 4-1/2gk )j X^Lp))]
<
t d i-1
⅛E[ X X X(βi-1-k )("-1/2gk)
2
j
τ⅛ E
t-1 d t	2
XX X (βi-1-k)(akVkT2gk).
_ k = 1j = 1i=k + 1	乙
t-1 d
XX"/2gk ).
k=1 j=1	j
=(i⅛ )2e[ Xh ViTF2
1	i=1
24
Under review as a conference paper at ICLR 2020
For the Tg bound, we have
i-1 i-1
Tg = E
X X[( ∏ βι,ι)(1-βι,k)](aiVi-1/2 - αkV-1/2)gk
i=2
t
i=2
t-1
X
i=1
k=1	l=k+1
i-1 i-1
X[( ∏ β1,ι)(1-β1,k)] OiVT
∖k=1	l=k+1
1/2 - αkVk-"
≤ G2E
≤ G2E
≤ G2
i
∏ M)]
l = k+1
β1-k
t-1 i
XX
i=1	k=1
t-1	i
XX
i=1	k=1
1
1 - β1
ai%T/2 - ak Vr1/2
必匕-1/2 - akV-1/2
βi-k X	αιVι-1/2
l = k+1
t t-1
)2(i⅛)2E X αiVi-
LI	√-9
i=2
2.
2-)
)1
-I]
2]
MkM)
2
2
t
≤ E E
≤ E
kgk ∣∣2
2
i
—
Then, the remaining term is
E
t
X
i=1
αi(vf(χi),VΓ1∕2gi
+]
To find the upper bound for this term, we reparameterize gt = Nf (Xt) + δt with E[δt] = 0, and we have
E
E
E
X α(Vf(Xi),VΓ1∕2g)]
X αi(Vf (Xi),VΓ1"(Vf(Xi) + δi),]
X α(vf(Xi),VΓ1/2 Vf(Xi))] + [X α(vf(Xi),VΓ1∕2δ)]
For the second term of last equation,
E
(i)
≥
X αi*Nf(xi),V-1∕2δi + ]
X {“Xi), (aiV-1/2 - 31匕-1/2)用
X {“Xi), (aiV-1/2 - 31匕-1/2)用
X {“Xi), (aiV-1/2 - 31匕-1/2)& ) ]
+ E X αi-1 (Vf(Xi),V-1∕2δ)]
+ E O1 ^Vf(X1 ),¼-1/25^|
+E
O1Nf(XI)T V-1/261
2T"1]]
E
E
E
The reasoning is as follows:
(i) The conditional expectation E [匕11/2小鼠,％-1] = 0 since the %-1 only depends on the noise
variables ξ1, •…,ξi-1 and δi depends on ξi with E[ξk] = 0 for all k ∈ {1, 2,…，i}. Therefore, they
are independent.
25
Under review as a conference paper at ICLR 2020
Further, we have
E ⅛∕v∕(xi),(αi½-iz2
.i=2 ∖
∑∣^V∕(xi), (¾½-172 -ai-ιVΓ∖^δi
(≥-E⅛ ∣∣v∕m∣∣2∣∣ (ML -^T)⅛∣∣2
≥ -2G⅛ £
_ i = 2
Therefore, we can bound the first term
-E
t
Σ
i=l
aiNf(xi'),VΓ1∕2Vf(xi')
□
Lemma 10. (Lemma 6.8 in Chen et al. (2019)) Foya% ≤ 0, /3 ∈ [0,1), and bi = J21=ι 厂卜 £；=卜十1 aι› we
have
26
Under review as a conference paper at ICLR 2020
F.2 Proof of Theorem 1
Proof. We combine the above lemmas to bound
6
E[f(zt+1) — f (z1)] ≤ X Ti
i=1
≤ G2 -β1- E
一 1 — β1
t
X	αiV-
i=2
1/2—-11
βι
β1,t+1
1 1 — β1	1 — β1,t+1
^^^^
β1
-TC
β1,t+1
1 — β1	1 — β1,t+1
T
2D2
+(-⅛ )2
G2E E αiVi-
i=2
1/2-i/211

+
—
✓
+
I
—
✓
t
一 t 3
+ E £ 2 L aiVi
i=1
--
1/2 gi∣2]
τ⅛
+ 2G2E X	OiVT1/2 — «i-1^-1/2
i=2
L"小丁211
|
t

)#
t
-E X ai(vf(Xi)，VT1/2Vf(Xi)
|
i=1
+ L2( A )2
{z
T2
2
E
βj
-{z-
t-1
XzT2gi∣∣
i=1
✓
+G2( -⅛ N 占
2
-t-1
)E X αiV-
2
1/2 - ai-1Vi-1/2
i=2
™{z
T2
2 J
)
一
t
1二 …
+e - X giVT1/2gik
2
I
i=1
T22
✓
27
Under review as a conference paper at ICLR 2020
By merging similar terms, we can have
E[f(zt+1)- f(zι)] ≤
+ 2G2 E
t
X
i=2
^	1/2
-MfU
+ 3L +1+L2( 1⅛)2(T⅛)2 E
+ 1+l2( 1⅛)2( 1⅛)2 (1⅛
t
X"-1%i∣∣
i=1	.
-t-1
)2G2E X αV
+
β1
-E
1 - βι	1 "(G2 + D2) +
X Oiyfg ),V-"vf (Xi)U
—
i=2
βι	β1,t+1
—
j"2∣1
1 — βι 1 — βι,t+ι
2D2+2G2EaTju
We define constants Ci, C2, and C3 as
Ci = 3 L + 1 + L2 (1⅛ )2( T⅛ )2
C2 = G2 -β1- +2G2
1 - β1
1 + L2(!⅛)2(T⅛)2 (T⅛)2g2
By rearranging terms, we obtain
E
X Oi (
i=1
Rf(Xi),Vi-1∕2 Rf(Xi)
X 川限L/2城 + C2 X IaiViT/2 - αi-ιVi-1/2
i=1	i=2	2
aiV-1/2 -αi-ι Vi-1" I ]
t-1
+ C3 X
i=2
+
βi
1 - βι
β1,t+1	)2D2
1 - β1,t+1 /
—
β1,t+1
1 - β1,t+1
+ 2G2 E
≤E
X 川限2-1/2城 + C2 X IaiVr1/2 - ai-iVC，
i=1	i=2
t-1
+ C3 X
i=2
山/2"1 Vi-1/2||2|
+(r⅛)(G2+D2)+( τ⅛ )2D2+2G2EBaIVIj2 III J
Finally, we can get
E
U
≤E
X Oi (
t=1
Rf (xt ),V-1/2Rf(Xt)
■ T
Ci X
t=1
atV-1/2gi
^^^~>^^^^^^^—
TermA
2 + C2 X| I I atVt-1/2 - at-iVt-1/2
， U--------------------V------------
Term B
T-i
+ C3 X
2	t=2
atVr1/2 - at-iVd/2 I ] + C4
with constants
C4
)2d2 +2G2 E
28
Under review as a conference paper at ICLR 2020
with almost same constant for the diagonal version. Lastly, we have γt = λmin (αt Vbt-1/2), so
E Xα(vf(xt),V
t=1
-1/2Vf(xt) ) ≥ E
T
X γt∣∣vf (χt)∣∣
t=1
^
T
≥ min E[kVf (χt)k2] X Yt
t∈[T]	t=1
Therefore, we finally have
Term A
C1
min [∣Nf(Xt)k2] ≤
T
XHbtT2gt ∣∣
t=1
Term B
zA{
T	T-1
+ C2	Qt + C3 P Qt
t=2	t=2
T
Xγt
t=1
+ C4
sι(T)
S2(T )
E
□
F.3 Proof of Theorem 2
For generalization error bounds, we refer the following references (Hardt et al., 2015; Zheng & Kwok, 2019).
Since We have bounded gradient ∣∣gt∣∣2 ≤ G and ∣∣Vf(x)∣∣2 ≤ G, We also have G-LiPschitz condition.
Therefore, we obtain the following relation
sup EA[f (A(S); z) - f (A(S0); z)] ≤GEA[∣A(S)-A(S0)∣2]
z
= GEA[∣θ - θ0∣2]
Therefore, We only have to bound the term ∆t := ∣θ - θ0∣2. From noW, We denote θ := A(S) and θ0 := A(S0).
We assume αt = α and β1,t = 0 for all t ∈ [T].
θT+1 = θT - αT (VbT1/2 + δI)-1mT
T
=θι- X αt(Vt1/2 + δI)-1gt
t=1
T
=θι — X αt(Vl1/ + δI)-1Vf (θt; Zit)
t=1
Where zik is the selected examPle at iteration k. Then, We can bound
E[∆T+1] = E[∣θT +1 - θT0 +1 ∣2]
TT
E θι — X at(Vt112 + δI厂 1 Vf (θt; Zit) — θ1 + X at (V01/2 + δI)-1Vf (θ0; z0,)11
T
≤ E[∣θ1 — θ10 ∣2] +XαtE
t=1
(VL12 + δI)-1Vf (θt; Zit) -(V0112 + δI)-1Vf(θ0; z0t)∣∣ j
T
XαtE
t=1
(Z1/2 + δI)-1Vf (θt; Zit)-(V01/2 + δI)-1Vf (θ0; z0t)
29
Under review as a conference paper at ICLR 2020
The probability of Zik = Zik is 1 — 1 /n. Then,
E (b1/2 + δI)-1Vf (θt; Zit) — (V'1/2 + δI厂1▽/(θ0; ZQu
≤ nE[k(½1/2 + δI)-1Vf (θt; Zit)M] + nE[k(V'1/2 + δI)-1Vf (θt; Zit)M]
+ (1 — 1 )E[k(Vt1/2 + δ)-1Vf (θt; Zit) — (V'1/2 + δI)-1Vf (θt; Zit)II2]
≤ 1 E[k(Vt1/2 + δI)-1Vf (θt; Zit)II2] +1 E[k(V'1/2 + δI)-1 Vf(θt; Zit)II2]
n -------------------------------/ n ----------------------------/
{z
T1
{z*
T2
+ (1 — 1) E[k(Vt1/2 + δI)-1Vf (θt; Zit) — (V'1/2 + δI)-1Vf (θt; Zit)II2]
∖ n / 、一	--	-
T3
+ (1 — 1) E[k(V'1/2 + δI)-1Vf (θt; Zit) — (V'1/2 + δI)-1Vf (θt; Zit)II2]
∖ n / ।_____________________________________________________________/
T4
Let t0 denote the time when M and VO becomes full-rank. Then, we can bound T1 as
T
X atE[I(Vt1/2 + δI)-1Vf (θt; Zit)II2]
t = 1
≤ √T ∖X «2E[I(Vt1/2 + δI )-1Vf (θt; Zit)II 2]
t=1
t0	T
√Tt X «2E[I(Vt1/2 + δI )-1Vf (θt; Zit)II2] + X a2E[Tr((Vt1/2 + δI )-2Vf(θt; Zit)Vf(θt; Zit)T)]
∖ t = 1	t = t0 + 1
t0
≤√TtX «2E[I(Vt1/2 + δI)-1Vf (θt; Zit)II2] + X a2E[Tr(½-1Vf (θt; Zit)Vf (θt; Zit))]
t=1
t0
t=t0+1
≤ √TtX α2E[I函1/2 + δI)-1Vf (θt; Zit)II2]
t=1
ɑ√T
√1 一 户2 ∖
t0	1
(1 — β2) £e[I(Z1/2 + δI)-1Vf(θt; Zit)II2] + E[logdet(Vτ) — logdet(优0)] + d(T — to)log —
t=1	β2
ɑ√T
01
(1 — β2) £e[|MI)-1Vf (θt; Zit)II2] + E[logdet(Vτ) — logdet(Vt0)] + d(T — t0)log β
t=1	β2
ɑ√T
√r-W
∕t0(1 — β2)G2
V	δ2
+d(T - t0)log V
≤
≤
T
T
+ E
In the same way, we can bound T2 as
X atE[I(V01/2 + δI )-1Vf(θ0; Z0t)M] ≤ √∣==t t0(I ；?2)。2 + e] log 科]+ d(T — t0)log ɪ
For notational convenience, we set the function g as
g(VT ) = √⅛ ＞产——T7—
Now, we can easily bound T3 and T as
E[I(Vt1/2 + δI)-1Vf (θt; Zit) — (V，1/2 + δI)-1Vf (θt; Zit)Il2] ≤ GE[|||(Vt1/2 + δI)-1 — (V01/2 + δI)-1∣∣∣2]
≤ GE[ I I I (Vt1/2 + δI )-1 III 2 + III M1/2 + δI )-1 III 2]
30
Under review as a conference paper at ICLR 2020
and
E[k(V, 1/2 + δI)-1Vf (θt; Zit) -(V01/2 + δI)-1Vf (θ0; Zit)M] ≤ LE[∣∣∣(V,1/2 + δI)-%2∆t]
from IlAxll2 ≤ ∣∣∣A∣∣∣2∣∣x∣∣2 and Lipschitz continuous gradients. Combining all the terms, we finally have
E [△「+ι] ≤ n√√Tβ2 Iqg(Vr)+ q g(V)j+ α(1 - n )jt
where
g(Vr) = t0(I ；?2)。2 + E[log 跖 ∣-log ∣V,∣,] + d(T - t0) log 1
TermC
and
rr
Jr = GX E[ ∣∣∣ 常/2 + δI)-1 ∣∣∣ 2 +1∣∣ (V01/2 + δI)-1 ∣∣∣ 2 ] + LX E[VM1/2 + δI)-1 ∣∣ 2∆t]
t=1	1---'Z--} |{Z}	t = 1	|	7^z^	/
Term D	Term D	Term D
F.4 Generalization Bounds for ADAGRAD
The main difference with EMA-BASED algorithms is the way of bounding T1. For AdaGrad, we set
at = a∕√t and Vt = t Pt=1 gigT =： 1 Gt as in Corollary 1.
0
Let to denote the time when Vt and V0 becomes full-rank. Then, we can bound T1 for AdaGrad as
r
X αtE[k(½1/2 + δI)-1Vf (θt; Zit)M]
t=1
≤√T t
r
X a2E[||(Vt1/2 + δI)-1Vf (θt; Zit)II2]
t=1
t0	T
√Tt X a2E[k(Vt1/2 + δI )-1Vf (θt; Zit)Il2]+ X a2E[Tr((½1/2 + δI )-2Vf(θt; Zit)Vf (θt; Zit)T)]
t=1	t=t0+1
t0	T
≤√Tt X a2E[k(Vt1/2 + δI)-1Vf (θt; Zit)II2] + X a2E[Tr(Z-1Vf(θt; Zit)Vf (θt; %))]
t=1	t=t0+1
≤√T
∖
t0
£a2E[||(Vt1/2 + δI)-1Vf(θt; Zit)II2] + a2E[log ∣ VT∣ - log 花°∣ + dlog -]
t = 1	0
t0
l0	T
≤ a√Tλ fE[∣∣(δI )-1Vf (θt; Zit)II2] + E[ log IVT ∣-log ∣ ¾ ∣ + d log -]
t=1	t0
≤ a√Ty toδEL + E[log ∣ VT∣ - log ∣ Vt0∣ + dlog -]
We can similarly bound the term -2 only replacing Vt with V0. Also, the remaining terms T3 and T4 can be
bound in the same way as in section F.3. Therefore, we have
where
t G2	T
g(VT) =	+ E[]og ∣ VT ∣ - log ∣ Vt0∣] + dlog —
TermC
and
TT
Jt = G X E [ ∣∣∣ 常/2 + δI)-1 ∣∣∣ 2 + ∣∣∣ (V01/2 + δI)-1 ∣∣∣ 2 ] + L X E [V(Vt01/2 + δI)-1 ∣∣ 2,∆t]
t=1	|	V	, |	V	,	t = 1	|	7^z^	/
Term D	Term D	Term D
31