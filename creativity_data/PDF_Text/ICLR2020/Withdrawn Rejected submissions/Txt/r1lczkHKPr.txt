Under review as a conference paper at ICLR 2020
Off-policy Multi-step Q-learning
Anonymous authors
Paper under double-blind review
Ab stract
In the past few years, off-policy reinforcement learning methods have shown
promising results in their application for robot control. Deep Q-learning, how-
ever, still suffers from poor data-efficiency which is limiting with regard to real-
world applications. We follow the idea of multi-step TD-learning to enhance
data-efficiency while remaining off-policy by proposing two novel Temporal-
Difference formulations: (1) Truncated Q-functions which represent the return for
the first n steps ofa target-policy rollout w.r.t. the full action-value and (2) Shifted
Q-functions, acting as the farsighted return after this truncated rollout. We prove
that the combination of these short- and long-term predictions is a representation
of the full return, leading to the Composite Q-learning algorithm. We show the
efficacy of Composite Q-learning in the tabular case and compare our approach
in the function-approximation setting with TD3, Model-based Value Expansion
and TD3(∆), which we introduce as an off-policy variant of TD(∆). We show
on three simulated robot tasks that Composite TD3 outperforms TD3 as well as
state-of-the-art off-policy multi-step approaches in terms of data-efficiency.
1	Introduction
In recent years, Q-learning (Watkins and Dayan, 1992) has achieved major successes in a broad
range of areas by employing deep neural networks (Mnih et al., 2015; Silver et al., 2018; Lillicrap
et al., 2016), including environments of higher complexity (Riedmiller et al., 2018) and even in
first real world applications (Haarnoja et al., 2019). Due to its off-policy update, Q-learning can
leverage transitions collected by any policy which makes it more data-efficient compared to on-
policy methods. Deep Q-learning, however, still has a very high demand for data samples which is
limiting with regard to robot applications.
One reason for the low data-efficiency is the long temporal horizon the reward signal has to propa-
gate through. Data-efficiency of on-policy Temporal-Difference methods can be enhanced by the use
of n-step returns, where a Monte Carlo rollout of length n is combined with a bootstrap of the value
function. To employ n-step returns in an off-policy setting, subtrajectories of the exploratory policy
have to be stored. These stored multi-step returns, however, will differ from the true value of the
target-policy. In order to benefit from n-step data, the replay buffer has to be restricted in size or n
has to be set to a small value to keep the samples close to the target-policy (Barth-Maron et al., 2018;
Hessel et al., 2018). To avoid these problems, a dynamics model can be used for imaginary roll-
outs, the so-called Model-based Value Expansion (MVE) (Feinberg et al., 2018). Alternatively, the
full return can be composed of value functions with increasing discount, an approach called TD(∆)
(Romoff et al., 2019). In this work, we define a model-free Temporal-Difference formulation which
follows the idea of multi-step learning while remaining off-policy.
Our contributions are threefold. First, we introduce the Composite Q-learning algorithm. For its
formulation, we define Truncated Q-functions, representing the return for the first n steps of a
target-policy rollout w.r.t. the full action-value. In addition, we introduce Shifted Q-functions which
represent the farsighted return after this truncated rollout. Both are then combined in a mutual re-
cursive definition of the Q-function for the final algorithm. Second, we evaluate MVE within TD3,
leading to MVE-TD3. And third, we introduce TD3(∆), an extension of TD(∆) to deep Q-learning.
We discuss related work in Section 2, describe the theoretical background in Section 3 and define
Composite Q-learning, MVE-TD3 and TD3(∆) in Section 4. By breaking down the long-term re-
turn into a composition of several short-term predictions, our method increases data-efficency which
1
Under review as a conference paper at ICLR 2020
we show in the tabular case and for three simulated robot tasks in Section 5. We then conclude in
Section 6.
2	Related Work
In order to correct for the deviation from the current target-policy, several methods suggest the use
of Importance Sampling (Precup et al., 2000; 2001; Munos et al., 2016). Approaches based on
Importance Sampling, however, can come with a vast increase in variance or can be of high cost and
are not easily applicable to deterministic policies.
One way to remain off-policy in multi-step Q-learning is to get the Monte Carlo rollout on the basis
of the current target-policy applied to a learned dynamics model (Feinberg et al., 2018; Buckman
et al., 2018). Due to accumulating errors of single-step models, this can lead to severe stability
issues in the Q-update, which is also the conclusion of Feinberg et al. (2018). The authors suggest to
average all intermediate i-step returns to smoothen out the model error, the so-called TD-k trick. In
contrast to our work, Feinberg et al. assume to have access to the true reward function. In order to
balance the length of the model-assisted rollout, Buckman et al. (2018) couple it with an uncertainty
estimate from value function and model ensembles. We alleviate the problem of accumulating error
by estimating a multi-step dynamics model implicitly via consecutive bootstrapping.
Most related to our approach is TD(∆) (Romoff et al., 2019). Romoff et al. formalize a Bellman-
operator over the differences between value functions of increasing discount values. Their approach
is on-policy and can therefore benefit directly from n-step returns. We extend TD(∆) to the off-
policy case below. Q-learning can also be extended to a SARSA-like tree-backup called Q(σ) (Asis
et al., 2018; Hernandez-Garcia and Sutton, 2018). However, it is an open question how to adjust
this idea to continuous action-spaces and deterministic policies. In the Hybrid Reward Architecture
(HRA), van Seijen et al. (2017) suggest a decomposition of the reward and the estimation of value
functions for each part of this decomposition which are then combined as an approximation of the
full return. HRA addresses the problem of complex rewards and is thus complementary to our
work focusing on long time scales. Concurrently to our work, Asis et al. (2019) introduced Fixed
Horizon TD-learning, formalizing action-value functions for different horizons in time. In contrast
to our work, however, Asis et al. define a consecutive bootstrapping formulation w.r.t. the target
policies of the different truncated horizons and not the full return. A first general description of
using intermediate predictions in Temporal Differences to restrict predictions to a fixed temporal
horizon has been introduced in (Sutton, 1988).
3	Background
We consider tasks modelled as Markov decision processes (MDP), where an agent executes action
at ∈ A in some state st ∈ S following its stochastic policy π. According to the dynamics model
M of the environment, the agent transitions into some state st+1 ∈ S and receives scalar reward rt .
The agent aims at maximizing the expected long-term return:
-T-1	-
Rn(St)= Eaj≥t〜∏,Sj>t〜M E YTIrjst ,	⑴
j	=t
where T is the (possibly infinite) temporal horizon of the MDP andγ	∈ [0, 1] the discount factor.
It therefore tries to find ∏*, s.t. Rn ≥ Rn for all ∏. If the model of the environment is unknown,
model-free methods based on the Bellman Optimality Equation over the so-called action-value can
be used:
T-1
Q∏ (st, at) = Eaj>t~n,Sj>t~M〉： Yj rrj st, at .	⑵
j	=t
In the following, We abbreviate Eaj∙>t〜∏,sj∙>t〜M[∙∣st,at] by Et,∏,M[∙]. One popular represen-
tative of continuous model-free reinforcement learning is the Deep Deterministic Policy Gradi-
ent algorithm (DDPG) (Lillicrap et al., 2016). In DDPG, actor μ is a deterministic mapping
from states to actions, μ : S → A, representing the actions that maximize the critic Qμ, i.e.
2
Under review as a conference paper at ICLR 2020
Figure 1: (a) Structure of Composite Q-learning. Target networks are omitted for visibility. Qi
denotes the Truncated and Shifted Q-functions at step i and Q the Composite Q-function. In-
coming edges yield the targets for the corresponding heads. Edges denoted by γ are discounted.
(b) Architecture of the Composite Q-network used in our experiments in Section 5.2.
μ(st) = argmaxa Qμ(st, a). Q and μ are estimated by function approximators Q(∙, ∙∣Θq) and
μ(∙∣θμ), parameterized by θQ and θμ. The critic is optimized on the mean squared error between
predictions Q(Sj, aj∣θQ) and targets yj = r7- + γQ0(sj+ι, μ0(sj+1 ∣θμ )∣Θq ), where Q0 and μ0 are
target networks, parameterized by θQ and θμ . The parameters of μ are optimized following the
deterministic policy gradient theorem (Silver et al., 2014):
Vθμ Jm X VaQ(s,a∣θQ)∣s=Sj ,a=μ(Sj |e“)V©” μ(s∣θμ),	⑶
j
and the parameters of the target networks are updated according to:
θQ0 J (1 — T)θQ0 + τθQ and θμ J (1 — T)θμ0 + τθμ,	(4)
with τ ∈ [0, 1].
TD3 (Fujimoto et al., 2018) adds three adjustments to vanilla DDPG. First, the minimum prediction
of two distinct critics is taken for target calculation to alleviate overestimation bias, an approach be-
longing to the family of Double Q-learning algorithms (van Hasselt et al., 2016). Second, Gaussian
smoothing is applied to the target-policy, addressing the variance in updates. Third, actor and target
networks are updated every d-th gradient step of the critic, to account for the problem of moving
targets.
4	Off-policy Multi-step Q-learning
In this section, we introduce the Composite Q-learning algorithm, an off-policy multi-step reinforce-
ment learning method to enhance data-efficiency, along with MVE-TD3 and TD3(∆) as baselines.
4.1	Composite Q-learning
The main motivation behind this work is the assumption that learning values on short time scales
can be achieved faster than for the full temporal horizon of a task, which can be prohibitively long.
As Murphy (2005) shows for the fixed-batch fitted Q-iteration case, the number of samples needed
to achieve a certain generalization error is exponential in the horizon of the MDP. As discussed
in Jin et al. (2018), Q-learning with UCB-exploration has total regret polynomial in the horizon
of the MDP and exponential with -greedy exploration (Kearns and Singh, 2002). We thus argue
that truncated horizons for a fixed MDP translate to lower sample complexity of value-estimation.
Building upon this idea, we estimate the return of n-step rollouts of the target-policy via Truncated
Q-functions which we then combine to the full return with model-free Shifted Q-functions, an ap-
proach we call Composite Q-learning, while remaining purely off-policy. Since these quantities
cannot be estimated directly from single-step transitions, we introduce a consecutive bootstrapping
3
Under review as a conference paper at ICLR 2020
Et,π,M
scheme based on intermediate predictions. For an overview, see Figure 1a. The full algorithm is in
the appendix. Code based on the implementation of TD31 can be found in the supplementary.
4.1.1	Truncated Q-functions
In order to formalize the off-policy estimation of n-step returns, assume that n (T - 1) and that
(T - 1 - t) mod n = 0 for task horizon T. We make use of the following observation:
Qπ(st, at) = Et,∏,M [rt + γrt+ι + Y2rt+2 + Y3rt+3 H-+ YTTrT-i]
t+n-1	t+2n-1
X Y fj) + Y n ( X Yj-(t+n)rj I
j=t	j=t+n	(5)
(T-1
X Yj-(T-n)rj
j=T-n
That is, we can define the action-value as the combination of partial sums of length n. We can
then define the Truncated Q-function as Qπn(st, at) = Et,π,M[Ptj+=nt -1 Yj-trj], which we plug into
Equation (5):
Qn(St, at) = Et,∏,M[Qn(st, at) + YnQn(St+n,&+八)+ …+ YT-nQ∏(sT-n, aT-n)].	(6)
Theorem 1. Let Q1π (st, at) = rt be the one-step Truncated Q-function and Qiπ>1(st, at) =
rt + YEt,π,M[Qiπ-1(St+1, at+1)] the i-step Truncated Q-function. Then Qiπ (St, at) represents the
truncated return Qiπ (St, at) = Et,π, M [Ptj+=it-1 Yj-trj ].
Following Theorem 1 (the proof can be found in the supplementary), we approximate Qπn (St, at)
off-policy via consecutive bootstrapping. Let QTr(∙, ∙∣θQ r) denote a function approximator with
parameters θQTr and n outputs, subsequently called heads, estimating Qiπ . Each output QiTr boot-
straps from the prediction of the preceding head, with the first approximating the immediate reward
function. The targets are therefore given by:
yTrι = TjandyTri>ι = rj+ YQT-I(Sj+ι,μ0(sj+ιlθμ0)lθQT-1),	⑺
where μ0 corresponds to the actor maximizing the full Q-value as defined in Section 3. That is,
QTr represents evaluations of μ at different stages of truncation and y；；<n serve as intermediate
predictions to get yjT,rn . We then only use QTnr, which implements the full n-step return, as the first
part of the composition of the Q-target. Please note that in order to estimate Equation (6), the
dynamics model would be needed to get st+c∙n of a rollout starting in st. In the following, We
describe an approach to achieve an estimation of Equation (6) model-free.
4.1.2	Shifted Q-functions
To get an estimation for the remainder of the rollout Qn∞ = Et,∏,M[ynQ(st+n, at+n)] after n
steps, we use a consecutive bootstrapping formulation of the Q-prediction.
Theorem 2. Let Q∏∞(st, at) = Et,∏,M[YQπ(st+ι, at+ι)] be the one-step Shifted Q-function and
Q∏>L∞(st,at) = Et,∏,M[YQ∏τ∞(st+ι,at+ι)] the i-step Shifted Q-function. Then Q∏∞(st,at)
represents the shifted return Q∏∞(st,at) = Et,∏,M[YiQπ(st+i, at+i)].
Again, the proof of Theorem 2 can be found in the supplementary. Let QSh(∙, ∙∣θQSh) denote the
function approximator estimating the Shifted Q-function Q^∞, parameterized by θQ . We can
shift the Q-prediction by bootstrapping without taking the immediate reward into account, so as to
skip the first n rewards of a target-policy rollout. The Shifted Q-targets for heads QiSh therefore
become:
yShι = yq0 (Sj+ι,μ0(sj+ιlθμ0)lθQj and yjh>ι = yqs-0i (sj+ι, μ0(sj+ι lθμ0) lθQSh01 )∙⑻
1https://github.com/sfujim/TD3
4
Under review as a conference paper at ICLR 2020
4.1.3	Composition
Following the definitions of Truncated and Shifted Q-functions, we can compose the full return.
Theorem 3. Let Qπn (st, at) = Et,π,M [Ptj+=nt -1 γj-trj] be the truncated return and
QM∞(st,at) = Et,∏,M[γnQ(st+n,at+n)] the shifted return. Then QK(St,at) = Qn(st,at) +
Qn∞(st, at) represents thefull return, i.e. Qn(st, at) = Et,∏,M[Pj∞=tγj-trj].
The incorporation of truncated returns breaks down the time scale of the long-term prediction by
the Shifted Q-function. For details, see the proof of Theorem 3 in the supplementary. We can thus
define the Composite Q-target as:
yj = rj + Y(QnrO (Sj + 1,μ0(Sj + 1∣θμ0 )∣θQn0)+ Qnh0(sj+I,μ0(sj+1 ∣θμ0 )∣θQn)),	⑼
approximated by Q(∙, ∙∣θj) with parameters θj. Since We have true reward r, We include it in
the target. Please note, that shifting the action-value in time imposes a bottleneck, since it relies on
the estimation of the full action-value. We therefore jointly estimate QTr, QSh and Q with function
approximator QC(∙, ∙∣θjC). Let SEC denote the squared error between targets yj and predictions
QC (sj, aj ∣θjC). See Figure 1b for the detailed architecture of the Composite Q-network.
Each pair QTr+QSh | ι≤i≤n is a complete approximation of the true Q-value. The circular dependency
can lead to stability issues, due to the amplification of propagated errors. We add a regularization
term to the loss penalizing the deviation between the prediction of Q and the n different Q-pairs to
keep estimates in a narrow range. It is implemented as the weighted mean squared error, i.e. the loss
function becomes:
L = ；1 XX bEC + β1 XX (Q(Sj,aj ∣ΘQ) - (QTr(sj, aj∣θQTr) + QSh(Sj,aj ∣θQSh))^) , (10)
for batch of size m and with β being the regularization weight. Actor μ is then updated on Q.
4.2	TD3(∆) AND MVE-TD3
Most related to Composite Q-learning are TD(∆) (Romoff et al., 2019) and MVE-DDPG (Feinberg
et al., 2018). In this section, we describe how to extend TD(∆) to an off-policy setting and how to
combine MVE and TD3.
TD3(∆) Another way to divide the value function into multiple time scales is TD(∆) (Romoff
et al., 2019). To this point, it has only been applied in an on-policy setting. In favor of comparability,
we extend TD(∆) to Q-learning, yielding TD3(∆). The main idea of TD(∆) is the combination of
different value functions corresponding to increasing discount values. Let γ∆ denote a fixed ordered
sequence of increasing discount values, i.e. Yδ = (γ1,γ2,..., Yk )> ∣γi>1>γi-1. We can then define
delta functions Wi as:
W1 = Qγ1 and Wi>1 = Qγi - Qγi-1 .	(11)
Let Qδ(∙, ∙∣Θqδ) denote the function approximator estimating Qγι≤i≤k. Based on the derivations
in (Romoff et al., 2019), the targets for Q-learning can be formalized as:
yj,ι = rj + Y1QY1 ⑶+ι,μ0(sj+ι∣θμj∣θQγ1) and
yj,i>1 = (Yi-Yi-l)QYi-ι (Sj+1 ,μ0(Sj + 1∣θμθ )∣θQγi-1)+ Yi W∕(Sj+ι,μ0(Sj+ι∣θμ0 )∣θWi),
which can then be used in any Q-learning algorithm. The authors suggest the use of n-step targets
within TD(∆) which is not easily applicable in an off-policy setting. In our experiments, we there-
fore compare our approach to single-step TD3(∆). The algorithm can be found in the appendix.
MVE-TD3 We also apply Model-based Value Expansion within TD3, subsequently called MVE-
TD3. We add Gaussian policy smoothing to the rollout of the model. In contrast to Feinberg et al.
(2018), however, we do not assume to have knowledge about the reward function. Our model
therefore approximates both, dynamics and reward.
5
Under review as a conference paper at ICLR 2020
5	Experimental Results
We evaluate Composite Q-learning in both, the tabular setting and the actor-critic method TD3.
5.1	Tabular Composite Q-learning
To analyze the effect of incorporating short-term prediction QTnr in the Q-update, we apply Com-
posite Q-learning in the tabular case to the MDP of horizon K given in Figure 2a. We compare it
to vanilla Q-learning, as well as multi-step Q-learning based on subtrajectories of the exploratory
policy and imaginary rollouts of the target-policy with the true model of the MDP.
c,
c
a, -1
a, -1
a, -1
c,-3”二
sK-3
a, -
■ Composite Q-learnmg
■ Multi-step Q-learning w/ True Model
■ On-policy Multi-step Q-learning
■ Q-learning
0 5 0 5
112 2
- - - -
S) IOJnIBA占
2	4
Updates	×106
c, —3/
c,
a, -100
==S) IOJ SenIBAd pφsqs PUB psBωunu
2
Updates
4
×106
(a)
(b)
Figure 2: (a) In this MDP of horizon K, the agent ought to arrive at terminal state sK-1 using
actions {a, b, c}. The initial state is s0 and the optimal policy is given in red. (b) Mean results and
two standard deviations over 10 runs on the MDP with a horizon ofK = 20. The left plot depicts the
value of s0 and action a as estimated by the different approaches over time. Dashed lines indicate
convergence to the optimal policy. The predicted Truncated Q-values for state s0 and action a with
horizons 1 to 4, denoted by Tr1 , . . . , Tr4 , and predicted Shifted Q-values for state s0 and action a,
denoted by Sh1, . . . , Sh4, are to the right. Dotted lines indicate the true optimal respective Q-values.
Results for K = 20 are depicted in Figure 2b. All approaches update the Q-function with a learning
rate of 10-3 on the same fixed batch of 103 episodes with a percentage of 10% non-optimal transi-
tions. For the multi-step approaches, we set rollout length n = 4. Since there is no generalization
among states in the tabular setting, we update the Shifted Q-function with a learning rate of 10-2
and the Truncated Q-functions with a learning rate of 10-3. An evaluation of different learning
rates for the Shifted Q-functions is depicted in Figure 3. We compare Composite Q-learning and
vanilla Q-learning to Shifted Q-learning, which corresponds to Q-learning with a one-step shifted
target (without approximate n-step returns from a Truncated Q-function). The results show that
shifting the value in time alone is slowing down convergence. Precisely, Shifted Q-learning with
a learning rate of 1.0 is equivalent to vanilla Q-learning, the same holds for Composite Q-learning
with a learning rate of 10-3 for the Shifted Q-function. The counterpart with different learning rates
for the Truncated Q-functions, while keeping the learning rates for the full Q-estimate and Shifted
Q-functions fixed, can be seen in Figure 4. While there is improvement in convergence using a
larger learning rate for the Truncated Q-functions, the results show higher variance and less benefit
than the Shifted Q-functions in Figure 3. The results suggest that the interplay between short- and
long-term predictions yields the most benefit if the learning rate for the Shifted Q-functions can be
set to a higher value. We believe this to be due to two possible reasons. The higher learning rates for
the Truncated Q-functions might lead to overfitting to the shorter horizons (given that it represents
an easier learning problem compared to the full return), whereas the Shifted Q-functions allow for
6
Under review as a conference paper at ICLR 2020
■	Composite Q-learning	(10-3)	■	Composite Q-learning (100)	■	Shifted Q-learning	(10T)
■	Composite Q-learning	(10-2)	■	Shifted Q-learning (10-3)	■	Shifted Q-learning	(100)
■	Composite Q-learning	(10T)	■	Shifted Q-learning (10-2)	■	Q-learning
) O 5202δ
--TT
(6S) IoJ 3nIA
0	1	2	3	4	5
Updates	×106
0	1	2	3	4	5
Updates	×106
Uo 520
一--T
(S) IOJ 3nIA,
0	1	2	3	4	5
Updates	×106
UO 520
一--T
(S) IOJ 3nIA,
0	1	2	3	4	5
Updates	×106
Figure 3: Results of four individual runs on the MDP with a horizon of K = 20 for Composite and
Shifted Q-learning, with different learning rates for the Shifted Q-function (denoted by the numbers
in parentheses). The learning rates for the full Q-function and for the Truncated Q-functions are set
to 10-3 in all experiments.
higher learning rates, since they only have to consider the variability of the distribution over next
states leading to decreased variance in their targets. In the experiments in Section 5.2, we show that
this can also be achieved by generalization as in the given architecture in Figure 1b. The erroneous
updates of on-policy multi-step Q-learning lead to convergence to a wrong action-value which is
underlining the importance of truly off-policy learning. However, if the true model can be used for
the rollout, this can be really effective. The difference in convergence speed between Q-learning
and Composite Q-learning is highly significant (P-ValUe of 4 ∙ 10-8 according to a t-test) and grows
with increasing horizon, as shown in Table 1. This is in line with the findings of Jin et al. (2018)
who establish a connection between the horizon of an MDP and the sample complexity of ValUe
estimation.
Table 1: Comparison of conVergence speed between tabUlar Q-learning and tabUlar Composite Q-
learning for exemplary rUns on the MDP giVen in FigUre 2a with n = 4.
Horizon K	10	20	50	100
Speed up to Q-learning	11%	44%	57%	66%
5.2 Composite Q-learning with Function Approximation
We eValUate TD3, Composite TD3, MVE-TD3 and TD3(∆) on three robot simUlation tasks of
OpenAI Gym (Brockman et al., 2016) based on MUJoCo (TodoroV et al., 2012): Walker2d-V2,
Ant-V2 and Hopper-V2. A VisUalization of the enVironments is depicted in FigUre 5.
Parameter Setting OUr main focUs is on the analysis of the strUctUre of Q-fUnctions and not to
achieVe maximUm possible performance. We therefore keep the main parameters the same across
all approaches (the Underlying algorithm was TD3 in all cases), since this woUld lead to another
source of potential differences otherwise. Learning rate (10-3), target update (5 ∙ 10-3) and actor
setting (two hidden layers with 400 and 300 neUrons and ReLU actiVation) are the same as in the
default setting of TD3. We use Gaussian exploration noise with σ = 0.15. The critic in Composite
Q-learning consists of four layers with 500 neurons and leaky ReLU actiVation, the critic in all other
7
Under review as a conference paper at ICLR 2020
■	Composite Q-learning (10-3)
■	Composite Q-learning (10-2)
(o=S) IOJ 3nI∙,A,σ
■ Composite Q-learning (10T)
■ Composite Q-learning (100)
I Q-learning
-20
-201∙∙∙.............∙∙R .	..........
0	1	2	3	4	5
Updates	×106
0
0	1	2	3	4	5
Updates	×106
(o6S) IOJ 3nI∙,Ad
5 0 5 0
-112
- - -
0	1	2	3	4	5
Updates	×106
0
(o6S) IOJ 3nI∙,Ad
5 0 5 0
-112
- - -
0	1	2	3	4	5
Updates	×106
Figure 4:	Results of four individual runs on the MDP with a horizon of K = 20 for Composite
Q-learning, with different learning rates for the Truncated Q-function (denoted by the numbers in
parentheses). The learning rates for the full Q-function and for the Shifted Q-functions are set to
10-3 in all experiments.
Figure 5:	Visualization of Walker2d-v2 (left), Ant-v2 (middle) and Hopper-v2 (right).
approaches and the model in MVE-TD3, with a learning rate of 10-3, has two layers. However, we
also evaluate TD3 and TD3(∆) with the same critic setting as Composite TD3, see Figure 8 in the
supplementary. This corresponds to the most stable setting we could find for TD3 with the given
subset of MuJoCo tasks. For Composite TD3, we set n = 50 and β = 10-4 for Walker2d-v2 and
HoPPer-V2 and β = 5 ∙ 10-5 for Ant-V2. For TD3(∆), We use the Y-SchedUle as suggested by the
authors, i.e. γι = 0 and γi>ι = γi-2+1, with an upper limit of 0.99 (Romoff et al., 2019). For
MVE-TD3, We use a rollout length of 3, as described in Feinberg et al. (2018).
Data Efficiency We choose the area under the learning curVe as performance measure, which is
a common way to eValuate data-efficiency and learning stability (see e.g. Hessel et al. (2018)).
Since Vanilla TD3 can haVe seVere stability issues for a small number of runs, we compare median
and interquartile ranges (IQR). As listed in Table 2, Composite TD3 outperforms TD3, as well as
state-of-the-art multi-step approaches, in terms of learning speed throughout training. After the
considered time frame of 4 ∙ 105 transitions, Composite TD3 has a 19%, 11% and 19% larger area
under the median learning curVe, in comparison to Vanilla TD3. As depicted in Figure 6, MVE-TD3
is highly sensitiVe w.r.t. accumulating errors in reward and state prediction, eVen when applying
the TD-k trick. HoweVer, if dynamics and reward are estimated with high accuracy, model-based
rollouts can be Very effectiVe, as the results show for Ant-V2. Based on our experiments, TD3(∆)
seems to be less sensitiVe than MVE-TD3, albeit also less efficient than Composite TD3. The
maximum return found by all approaches within 4 ∙ 105 transitions can be seen in Table 3.
8
Under review as a conference paper at ICLR 2020
Walker2d-v2
Ant-v2
HoPPer-v2
ComPosite TD3
TD3
MVE-TD3
TD3(∆)
UIns上
Composite TD3 ■ MVE-TD3
TD3
TD3(∆)
4000
3000
2000
1000-
0
Composite TD3
TD3
MVE-TD3
TD3(∆)
3000-
2000-
1000-
0-
3000
2000
1000-
■ Q	■ Tr40 , Tr20 ・ Tr5
■ Tr50 ■ Tr30 ■ Tr10 ■ Tri
Jo=H,ClI
■ Q	■ T⅛ ■ Tr20 ・ Tr5
■ Tr50 ■ Tr30 ■ Tr10 ■ Tri
2.0
1.5
1.0
0.5
0.0
Q	■	T⅛	・ Tr20	・	T⅛
Tr50	■	T⅛o	■ Tr10	■	TrI
0	12	3	4
Transitions	×105
0
Figure 6: Results (top) and TD-errors (bottom) for Walker2d-v2 (left), Ant-v2 (middle) and
Hopper-v2 (right). The plots show median and interquartile ranges over 11 training runs, each rep-
resenting mean evaluation performance over 100 initial states. The lower plots show TD-errors over
time for the different horizons of the Truncated Q-function (denoted by Trh for horizon h) as well
as the TD-errors for the complete Q-estimate. Please note that TD-error here means the deviation
from the associated target.
Table 2: Normalized area under the median learning curve and IQRs over 11 training runs.
Samples	Method	Walker2d-v2	Ant-v2	Hopper-v2
2 ∙ 105	TD3	82% (-19,+15)	88% (-20, +19)	84% (-29, +28)
	Composite TD3	100% (-11, +15)	100% (-11, +8)	100% (-20, +50)
	MVE-TD3	50% (-15, +14)	82% (-5, +10)	86% (-17, +20)
	TD3(∆)	89% (-26, +34)	97% (-6, +8)	74% (-22, +24)
3 ∙ 105	TD3	79% (-19,+14)	88% (-16, +13)	78% (-28, +21)
	Composite TD3	100% (-10, +12)	100% (-9, +8)	100% (-19, +34)
	MVE-TD3	44% (-16, +12)	86% (-6, +8)	57% (-13, +18)
	TD3(∆)	87% (-21, +28)	94% (-6, +8)	72% (-26, +27)
4 ∙ 105	TD3	81%(-20,+11)	89% (-14, +11)	81% (-25, +19)
	Composite TD3	100% (-10, +10)	100% (-9, +7)	100% (-18, +27)
	MVE-TD3	44% (-16, +12)	88% (-8, +11)	44% (-11, +16)
	TD3(∆)	88% (-16, +22)	95% (-6, +7)	80% (-27, +22)
TD-error Analysis To test our intuition stated in the motivation, we refer to the lower row of
Figure 6 which shows the TD-errors for the different stages of truncation, as well as the TD-errors
for the complete Q-estimate. All Truncated Q-estimates have lower TD-error throughout learning
and it can be seen that the TD-error is consistently higher for longer horizons. This reaffirms our
9
Under review as a conference paper at ICLR 2020
Table 3: Maximum return of the median learning curve and IQRs over 11 training runs within the
considered time frame of 4 ∙ 105 samples.
Environment	TD3	Composite TD3	MVE-TD3	TD3(∆)
Walker2d-v2	3962 (-887, +187)	4391 (-414, +239)	2050 (-703, +524)	4101 (-70, +304)
Ant-v2	2933 (-229, +291)	3172 (-255, +250)	3019 (-322, +625)	3043 (-171, +199)
Hopper-v2	2888 (-460, +169)	3044 (-459, +103)	1081 (-244, +234)	3047 (-474, +172)
hypothesis that learning is easier on shorter horizons. The Composite Q-target therefore reflects the
true action-value faster which is beneficial for Q-learning.
3a:InQ»XIrau
β31y•Zr
XImsH
n
・ β = 10-5	■ β = 10-4 ■ β = 10-3
4000-
3000-
2000-
1000-
0-
0	12	3	4
Transitions	× 105
1.0∙
0.8∙
06
04
10—5	5 ∙ 10-5	10-4	5 ∙ 10-4	10-3
β
Figure 7: Results (top) and normalized area under the learning curve (bottom) for Composite TD3
in the Walker2d-v2 environment with different truncation horizons n (left) and different regulariza-
tion weights β (right). The plots show median and interquartile ranges over 11 training runs, each
representing mean evaluation performance over 100 initial states.
Sensitivity to Hyperparameters Lastly, we evaluate the influence of n and β, exemplary for the
Walker2d-v2 environment. The results can be seen in Figure 7. Shorter truncation horizons can
lead to faster convergence, but the variance increases. If n is rather large, however, it can slow
learning down. The same holds if the regularization weight β is set to a high value. On the other
hand, regularization is needed to keep the predictions in a narrow range, since it can lead to stability
issues, otherwise.
6 Conclusion
We introduced Composite Q-learning, an off-policy learning method that divides the long-term value
into smaller time scales. It combines Truncated Q-functions acting on a short horizon with Shifted
Q-functions for the remainder of the rollout. We analyzed the efficacy of Composite Q-learning in
the tabular case and showed that the benefit of short-term predictions increases with growing task
horizon. We further evaluated MVE-TD3 and introduced TD3(∆), an off-policy variant of TD(∆).
We showed on three simulated robot tasks that Composite TD3 outperforms vanilla TD3 by 19%,
11% and 19% in terms of area under the median learning curve. The given results provide evidence
that Composite TD3 enhances data-efficiency compared to other approaches in off-policy multi-step
learning.
Going forward, the uncertainty estimate based on the variance of the Composite Q-network could
be of benefit in both, update calculation and exploration. We further leave the application of the
truncated formulation of other quantities such as state change or auxiliary costs as future work.
10
Under review as a conference paper at ICLR 2020
References
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations,
ICLR, 2016.
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom van de
Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving
sparse reward tasks from scratch. In Jennifer Dy and Andreas Krause, editors, Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pages 4344-4353. PMLR, 10-15 Jul 2018.
Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning
to walk via deep reinforcement learning. In Proceedings of Robotics: Science and Systems, 2019.
Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB,
Alistair Muldal, Nicolas Heess, and Timothy P. Lillicrap. Distributed distributional deterministic
policy gradients. In 6th International Conference on Learning Representations, ICLR, 2018.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. In Sheila A. McIlraith and Kilian Q. Weinberger,
editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),
the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artificial Intelligence (EAAI-18), pages 3215-3222. AAAI Press,
2018.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine. Model-based value expansion for efficient model-free reinforcement learning. CoRR,
abs/1803.00101, 2018. URL http://arxiv.org/abs/1803.00101.
Joshua Romoff, Peter Henderson, Ahmed Touati, Yann Ollivier, Emma Brunskill, and Joelle Pineau.
Separating value functions across time-scales. CoRR, abs/1902.01883, 2019. URL http://
arxiv.org/abs/1902.01883.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In Pat Langley, editor, Proceedings of the 17th International Conference on Machine
Learning ICML, pages 759-766. Morgan Kaufmann, 2000.
Doina Precup, Richard S. Sutton, and Sanjoy Dasgupta. Off-policy temporal difference learning with
function approximation. In Carla E. Brodley and Andrea Pohoreckyj Danyluk, editors, Proceed-
ings of the 18th International Conference on Machine Learning ICML, pages 417-424. Morgan
Kaufmann, 2001.
Remi Munos, Tom StePleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient off-
policy reinforcement learning. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29,
pages 1046-1054, 2016.
11
Under review as a conference paper at ICLR 2020
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efficient reinforcement learning with stochastic ensemble value expansion. In Samy Bengio,
Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, NicoIo Cesa-Bianchi, and Roman Gar-
nett, editors, Advances in Neural Information Processing Systems 31, NeurIPS, pages 8234-8244,
2018.
Kristopher De Asis, J. Fernando Hernandez-Garcia, G. Zacharias Holland, and Richard S. Sutton.
Multi-step reinforcement learning: A unifying algorithm. In Sheila A. McIlraith and Kilian Q.
Weinberger, editors, Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pages
2902-2909. AAAI Press, 2018.
J. Fernando Hernandez-Garcia and Richard S. Sutton. Understanding multi-step deep rein-
forcement learning: A systematic study of the DQN target. Deep Reinforcement Learn-
ing Workshop (NeurIPS 2018), 2018. URL https://sites.google.com/view/
deep-rl-workshop-nips-2018/home#h.p_7z_mbUjm5DxN.
Harm van Seijen, Mehdi Fatemi, Romain Laroche, Joshua Romoff, Tavian Barnes, and Jeffrey
Tsang. Hybrid reward architecture for reinforcement learning. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems, pages 5392-5402, 2017.
Kristopher De Asis, Alan Chan, Silviu Pitis, Richard S. Sutton, and Daniel Graves. Fixed-horizon
temporal difference methods for stable reinforcement learning. CoRR, abs/1909.03906, 2019.
URL http://arxiv.org/abs/1909.03906.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference
on Machine Learning, ICML, volume 32 of JMLR Workshop and Conference Proceedings, pages
387-395, 2014.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th
International Conference on Machine Learning, ICML, volume 80 of Proceedings of Machine
Learning Research, pages 1582-1591. PMLR, 2018.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Dale Schuurmans and Michael P. Wellman, editors, Proceedings of the 30th AAAI
Conference on Artificial Intelligence, pages 2094-2100. AAAI Press, 2016.
Susan A. Murphy. A generalization error for q-learning. J. Mach. Learn. Res., 6:1073-1097, 2005.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is q-learning provably effi-
cient? In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-
Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems, pages 4868-4878, 2018.
Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time.
Machine Learning, 49(2-3):209-232, 2002.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/
abs/1606.01540.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, pages 5026-
5033. IEEE, 2012.
12
Under review as a conference paper at ICLR 2020
A Algorithm for Composite TD3
A detailed description of Composite DDPG is given in Algorithm 1, where the adjustments of TD3
are omitted for simplicity. In order to transform Algorithm 1 to its TD3-equivalent, Gaussian policy
smoothing has to be added to all targets in Line 8, as well as taking the minimum prediction of two
distinct critics for each target. Furthermore, actor and target networks have to be updated with delay.
1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 1: Composite DDPG
initialize critic QC, actor μ and targets QC0, μ0
initialize replay buffer R
for episode = 1..E do
get initial state s1
for t = 1..T do
apply action at = μ(st∣θμ) + ξ, where ξ 〜N(0, σ)
observe st+1 and rt and save transition (st, at, st+1 , rt) in R
calculate targets:
yjT,r1 = rj
yTi>ι = rj + YQTr0 I(Sj+ι,μ0(Sj+ιlθμjlθQi-I)
yShι = YQO(Sj+1,μ0(Sj+P" )lθQ )
yShi>ι =γQS"ι(Sj+1,μ0(Sj+ιlθμO)lθQi-1)
yQ = rj + γ(Qn0(Sj+1,μ0(Sj+1∣θμ)∣θQn0) + Qnho(Sj+ι,μ0(Sj+ι∣θμθ)∣θQn ))
yjC = hyjQ, yjT,r1, yjT,ri>1, yjS,h1, yjS,hi>1i
update QC on minibatch b of size m from R according to Equation (10)
update μ on Q
adjust parameters of QC0 and μ0
B	ALGORITHM FOR TD3(∆)
A detailed description of DDPG(∆) is given in Algorithm 2. The authors suggest the use of n-step
samples which is not easily applicable in an off-policy setting. In our experiments, we therefore
compare our approach to single-step TD3(∆). To transform DDPG(∆) to TD3(∆), the adjustments
as described in Appendix A have to be applied analogously.
Algorithm 2: DDPG(∆)
initialize critic Qδ, actor μ and targets Qδ, μ0
initialize replay buffer R
set discount values Y∆ = (Y0 , Y1 , . . . , Yk)|
for epiSode = 1..E do
get initial state S1
for t = 1..T do
apply action at = μ(St∣θμ) + ξ, where ξ 〜N(0, σ)
observe St+1 and rt and save transition (St , at , St+1 , rt) in R
calculate targets:
yγ,ι = rj + yiqYi (Sj+ι,μ0(Sj+ιlθμ0 )lθQγ1)
yγ,i>ι = (Yi-Yi-I)QYi- (Sj+ι,μ0(Sj+ιlθμ0 )lθQYiT)
+γiW∕(Sj+ι,μ0(Sj+ι∣θμ0 )∣θwi)
update Q∆ on minibatch b of size m from R
update μ on QYk
adjust parameters of Qδ and μ0
13
Under review as a conference paper at ICLR 2020
C	Truncated Q-functions
Theorem 1. Let Q1π (st, at) = rt be the one-step Truncated Q-function and Qiπ>1(st, at) =
rt + γEt,π,M [Qiπ-1(st+1, at+1)] the i-step Truncated Q-function. Then Qiπ (st, at) represents the
truncated return Qiπ (st, at) = Et,π,M [Ptj+=it-1 γj-trj].
Proof. Proof by induction. Q1π(st, at) = rt by definition. The theorem follows from induction step:
Qi (st, at) = rt + γEt,π,M Qiπ-1(st+1,at+1)
(t+1) + (i-1)-1
=rt + YEt,∏,M	X	Y j-(t+1) Tj
j=(t+1)
t+i-1
=rt + YEt,∏,M	X Yj-(t+1)rj
j=(t+1)
t+i-1
=rt + Et,∏,m	E Y j-t Tj
j=(t+1)
t+i-1
=Et,∏,M X YTrj .
j=t
□
D Shifted Q-functions
Theorem 2.	Let Q∏∞(st, at) = Et,∏,M[γQπ(st+ι, at+ι)] be the one-step Shifted Q-function and
Q∏>L∞(st,at) = Et,∏,M[γQ∏τ∞(st+ι,at+ι)] the i-step Shifted Q-function. Then Q∏∞(st,at)
represents the shifted return Q∏∞(st,at) = Et,∏,M[γiQπ(st+i,at+i)].
Proof. Proof by induction. Q∏∞(st,at) = Et,∏,M[γQπ(st+ι,at+ι)] by definition. The theorem
follows from induction step:
QΠ∞(St, at) = Et,∏,M [γQi —L∞(st+1, at+1)
= Et,π,M [Y(YiTQn(St+1+i-1, at+1+i-I))]
=Et,n,M [Y(YiTQn(St+i,at+i川
= Et,π,M [Y Qπ (St+i, at+i) .
□
E Resolving the Mutual Recursion
Theorem 3.	Let Qnn(St, at) = Et,n,M [Ptj+=nt -1 Yj-trj] be the truncated return and
Qn∞(st,at) = Et,n,M[YnQ(st+n,at+n)] the shifted return. Then QK(St,at) = Qn(st,at) +
Qn∞(st, at) represents thefull return, i.e. Qn(st, at) = Et,∏,M[Pj∞=tYj-trj].
14
Under review as a conference paper at ICLR 2020
Proof.
Qn(St, at) = Qn(St, at)+q<∞(St, at)
Et,π,M
Et,π,M
Et,π,M
Et,π,M
t+n-1
X γj-trj +γnQπ(St+n,at+n)
j=t
t+n- 1	]
γj-trj + γn Qπn (St+
n, at+n) + Qn-.∞(st+n, at+n)J I
t+n-1	t+2n-1
X γj-trj +γn	X γj-t-nrj + γnQπ(St+2n, at+2n)
j=t	j=t+n
t+n—1	t+2n—1
X γj-trj + X γj-trj +γ2nQπ(St+2n,at+2n)I
j=t	j=t+n
t+2n-1
Et,π,M	X γj-trj + γ2nQπ(St+2n, at+2n)
j=t
Repeating this process then gives Qπ(St, at) = Et,π,M[Pj∞=tγj-trj].
□
Due to the composite structure, the Shifted Q-function represents the long-term sum of partial re-
turns provided by the Truncated Q-function, as opposed to single reward values.
F TD3 AND TD3(∆) WITH LARGER NETWORKS
To further analyze the impact of the Composite Q-learning structure, we evaluate TD3 and TD3(∆)
with the same number of parameters for the critic, as depicted in Figure 8. The approaches do not
seem to make use of the additional capacity and reveal a worse performance in the given time frame.
Walker2d-v2
Figure 8: Results for TD3 and TD3(∆) with the same number of parameters for the critic as Com-
posite TD3, i.e. four layers with 500 neurons. The plots show median and interquartile ranges over
11 training runs, each representing mean evaluation performance over 100 initial states. We denote
approaches using a critic with four layers by 4L.
Ant-v2
HoPPer-v2
・ TD3(∆)
■ TD3
15
Under review as a conference paper at ICLR 2020
G Shallow Architecture
In order to analyze importance of the multi-layered structure reflecting the temporal dependencies
between short- and long-term predictions, we compare the architecture used in our experiments
to a shallow architecture which includes all predictions as outputs of the last hidden layer for the
Walker2d-v2 environment in Figure 9. Based on the results in Appendix F, we reduced the amount
of hidden layers to two.
UIn"H
3000-
2000-
ComPosite TD3
Shallow Composite TD3
4000-
0	2	4
Transitions ×105
fully connected (500, leaky ReLU)
fully connected (500, leaky ReLU)
(a)	(b)
Figure 9: (a) Results for a shallow architecture of the Composite Q-network for the Walker2d-v2
environment. The plots show median and interquartile ranges over 11 training runs, each represent-
ing mean evaluation performance over 100 initial states. (b) Architecture of the shallow Composite
Q-network.
H Individual Comparisons
For clarity, we provide individual comparisons between Composite TD3 and all baselines.
Walker2d-v2
・ Composite TD3
・ TD3
Ant-v2
・ Composite TD3
・ TD3
UJnsrt
0	2	4
Transitions	× 105
0	2	4
Transitions × 105
Figure 10: Results for Composite TD3 and TD3 as in Figure 6.
Hopper-v2
■ Composite TD3
■ TD3
Transitions × 105
16
Under review as a conference paper at ICLR 2020
Walker2d-v2
Ant-v2
4000
3000
2000
1000
0
4000
3000
2000
1000
0
Composite TD3	・ Composite TD3
MVE-TD3	・ MVE-TD3
Figure 11: Results for Composite TD3 and MVE-TD3 as in Figure 6.
Walker2d-v2
Ant-v2
Composite TD3
TD3(∆)
Figure 12: Results for Composite TD3 and TD3(∆) as in Figure 6.
Hopper-v2
Hopper-v2
Composite TD3	■ Composite TD3
TD3(∆)	■ TD3(∆)
0	2	4	0	2	4
Transitions × 105	Transitions	× 105
17