Under review as a conference paper at ICLR 2020
A Mechanism of Implicit Regularization in
Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Despite a lot of theoretical efforts, very little is known about mechanisms of im-
plicit regularization by which the low complexity contributes to generalization in
deep learning. In particular, causality between the generalization performance,
implicit regularization and nonlinearity of activation functions is one of the ba-
sic mysteries of deep neural networks (DNNs). In this work, we introduce a
novel technique for DNNs called “random walk analysis” and reveal a mech-
anism of the implicit regularization caused by nonlinearity of ReLU activation.
Surprisingly, our theoretical results suggest that the learned DNNs interpolate al-
most linearly between data points, which leads to the low complexity solutions
in the over-parameterized regime. As a result, we prove that stochastic gradient
descent can learn a class of continuously differentiable functions with generaliza-
tion bounds of the order of O(n-2) (n: the number of samples). Furthermore, our
analysis is independent of the kernel methods, including neural tangent kernels.
1 Introduction
Deep Neural Networks (DNNs) have demonstrated dominating performance in numerous machine
learning tasks, and it shows great generalization performance in the over-parameterized regime.
Theoretically, mechanisms of implicit regularization, which is considered as an important factor of
such generalization performance in over-parameterized DNNs, still remain unknown (Zhang et al.,
2016; Neyshabur, 2017). In the over-parameterized regime, recent studies report that generalization
bounds for DNNs can be obtained by replacing the neural network with its linear approximation
model with respect to weight parameters at initialization. Most of these studies rely on the connec-
tion between deep learning and neural tangent kernels (NTKs) (Daniely et al., 2016; Jacot et al.,
2018; Arora et al., 2019b), which characterizes the dynamics of network outputs throughout gra-
dient descent training in the infinite width limit. However, a source of implicit regularization in
over-parameterized DNNs has not been identified. Recent empirical and theoretical results indicate
that generalization performance and implicit regularization of over-parameterized DNNs cannot be
captured by NTK analysis (Wei et al., 2018; Allen-Zhu & Li, 2019; Woodworth et al., 2019; Geiger
et al., 2019). Understanding how implicit regularization properly controls the superfluous expres-
sive power of over-parameterized DNNs gives us new insights into the theoretical analysis in deep
learning. This leads to the first question:
Question 1. What kind of low complexity is caused by implicit regularization in deep learning?
Linear networks without activation functions are important subject, and there are a number of theo-
retical works on the implicit regularization in over-parameterized neural networks mainly focusing
on linear models (Ji & Telgarsky, 2018; Gidel et al., 2019; Arora et al., 2019a). In contrast, whole
properties of over-parameterized DNNs that may result from nonlinearity of activation functions
cannot be captured by the approximated linear models, specifically, the kernel regression predictor
using the NTK. However, some mechanisms of the implicit regularization can depend on nonlinear-
ity. This leads to the next question:
Question 2. Can we identify a mechanism of implicit regularization that depends on nonlinearity
of activation functions?
Now for optimization algorithms, the training dynamics of full batch gradient descent (GD) is better
understood although GD is too expensive for most applications and one often uses stochastic gra-
1
Under review as a conference paper at ICLR 2020
Figure 1: Random walks having a step size distributed according to N (0, 2/m) (m = 103) after
α steps. The smaller the number of steps α is, the more straight. For visibility, we transformed the
random walk sequence {yji=。into {zi}i=o defined by Zi := J - y。)- (i∕α)(yα - y0).
dient decent (SGD) instead. In most cases, the authors used GD to derive their results by the NTK
analysis. Recent work showed that without any structural assumptions about the data distribution,
two-layer or three-layer over-parameterized networks trained by SGD can learn C∞-class functions
(Allen-Zhu et al., 2018a; Arora et al., 2019c). However, it is not clear the relation between gener-
alization and implicit regularization for DNNs optimized by SGD. Towards this end, the following
question is also unsolved:
Question 3. Is it possible to obtain provable generalization bounds based on the implicit regular-
ization for DNNs optimized by SGD?
To answer these questions, we introduce a novel analysis for DNNs and characterize a mechanism
of implicit regularization that caused by nonlinearity of ReLU activation. Our results indicate that
the DNNs (trained by SGD) interpolate almost linearly between data points, which leads to the
low complexity solutions in the over-parameterized regime. Accordingly, we prove that SGD with
random initialization can learn a class of continuously differentiable functions with generalization
error bounds of the order of O(n-2) (n: the number of samples), which is independent of the NTK
analysis.
In order to introduce our analysis for implicit regularization, let us focus on the DNN output on a
one-dimensional linear path between training points. We define it as x(s) := (1 - s)x(p) + sx(q)
(s ∈ [0, 1]), where x(p) and x(q) are training points. The corresponding function of the DNN with
ReLU (ReLU DNN) is continuous piecewise linear on the linear path x(s). In the hidden layer, the
corresponding function gl(x) of each unit in the l-th layer is also continuous piecewise linear on the
linear path x(s), that is, a composite function (gl ◦ x)(s) is continuous piecewise linear. Since DNN
nonlinearity is linked to breakpoints, which is caused by ReLU activation, the set of the breakpoints
plays a key role in the network behavior. These kind of breakpoints are also known as kinks or
knots (Steinwart, 2019). Since (gl ◦ x)(s) is continuous piecewise linear, the gradient of (gl ◦ x)(s)
has gaps at the breakpoints, which we call gradient gaps. We focus on gradient gaps with respect
to the parameter s.1 Our key finding is that the gradient gaps are a constant multiple of independent
Gaussian random variables according to N(0, 2/m), where m is the number of units in the hidden
layer. Since the gradient of the unit (gl ◦ x)(s) is the sum of the gradient gaps, it is a “Gaussian
random walk”2 (see Step 2 in the proof of Lemma 3). The relation between the step size variance and
the number of steps determines the Gaussian random walk behavior. In the corresponding function
of the DNN, the step size variance is the variance of weights, and the number of steps is the number
of breakpoints. In a regime where ( the number of steps × the step size variance) ≤ O(1), the
Gaussian random walk strolls little from the origin with high probability (see Figure 1). Our results
show that the ReLU DNN is in the same regime and “simplicity” of the gradient depends crucially on
the number of breakpoints. Hanin & Rolnick (2019) proves that the average number of breakpoints
1 It is worth pointing out that in this paper, each gradient is a derivative not with respect to weight parameters
but with respect to input parameters.
2 If {Xi}α=ι are independent Gaussian random variables, then {Yi}α=ι, where Yi := Xi + X[ + …+ Xi,
is called a Gaussian random walk.
2
Under review as a conference paper at ICLR 2020
is linear in the number of hidden units at initialization, and we also give a proof of an upper bound
on the number of breakpoints even after training (see §3.1).
In this work, we prove a priori generalization estimates, which is independent of the posterior data
distribution, by analyzing the behavior of DNNs on the linear path x(s). In other words, we show
that in Theorem 2, the difference between the network output function and its linear interpolation is
evaluated only from the amount of the weight change. We note that in Theorem 2, we dose not use
the properties of the trained neural networks. Our key analysis is that the gradient of unit (gl ◦ x)(s)
is a “Gaussian random walk” and approximately equal to a straight line on the linear path x(s),
which we call random walk analysis. Our technique is based on a priori estimates that variation
of the gradients between data points is extremely small and depends essentially on the value of the
number of breakpoints times the variance of weights.
Difference from NTK. Interestingly, our analysis idea is different from other previous work based
on NTK. Our findings are some novel aspects as follows:
•	The NTK is defined using the gradient of the DNN output with respect to weight param-
eter space. In contrast, the linear approximation (Lemma 3 in this paper) is defined using
the gradient of the DNN output with respect to input parameter space. In other words,
the variables to be differentiated are different.
•	The random walk analysis indicates that over-parameterized ReLU DNNs interpolate al-
most linearly between the data points. For ReLU activation, since the NTK kernel mapping
is not LiPschitz but 1/2-Holder, it is difficult to obtain such a result in the NTK analysis
without a tradeoff between smoothness and approximation (Bietti & Mairal, 2019).
Our Contributions. In this work, we consider an L-layer over-Parameterized ReLU neural net-
work with l2 regression task using SGD from random initialization. We show the constitutive rela-
tion between imPlicit regularization and generalization, which enables us to Provide new insights on
the role of imPlicit regularization in deeP learning. Our main contributions are as follows:
•	Our random walk analysis Provides a priori estimates of low comPlexity in over-
Parameterized deeP neural networks, and directly indicates that the unit outPut between
data Points is ProPerly controlled by weight initialization and SGD to keeP connecting the
data Points almost straight, which is one of the underlying mechanisms of imPlicit regular-
ization in the over-Parameterized regime.
•	Our result suggests that imPlicit regularization is attributed to the nonlinearity of ReLU
DNN, which is indicated by the fact that variance of weight and the number of breakPoints
determine the Gaussian random walk behavior.
•	We also Prove that in one-dimensional inPut case, SGD with random initialization can learn
C1-class functions. Our generalization estimates are based on the imPlicit regularization.
2	Preliminaries and Notation
Notation. For n ∈ N, we let [n] = {1, 2,..., n}. We use N(μ, σ2) to denote the Gaussian
distribution of mean μ and variance σ2. We use ∣∣vk2 to denote the Euclidean norm of a vector v,
use kv kF to denote the Frobenius norm of a vector v. When z is a sub-Gaussian random variable,
we let ∣z∣ψ2 = inf {t > 0 | E[exp(z2/t2)] ≤ 2} to denote sub-Gaussian norm (Vershynin, 2018).
For a vector v, we denote by [v]i or vi the i-th element of v. For a matrix M, we denote by [M]i,j
or Mi,j the entry in the i-th row and j-th column of M, and we denote by [M]i the i-th row vector
of M. We denote by 1{E} the indicator function for the event E. ReLU activation is given by
φ(x) = max{0, x}, and for a vector a ∈ Rn, we define φ(a) = φ(a1), φ(a2), . . . , φ(an) .
Network structure. In this work, an L-layer fully-connected feed-forward ReLU neural network
with m units in each hidden layer is given by f : Rd → Rc. For inPut x ∈ Rd, unit outPut gl(x) of
each layer l ∈ [L] and network outPut f(x) are given by the following functions:
g0(x) := Ax,	gl (x) := Wlφ(gl-1(x)) (l ∈ [L]),	f(x) := Bφ(gL(x)),	(1)
3
Under review as a conference paper at ICLR 2020
where A ∈ Rm×d, Wl ∈ Rm×m (l ∈ L), and B ∈ Rc×m are weight matrices. We assume the
following Gaussian initialization: A%,j 〜N(0, 2/m), Wij 〜N(0, 2/m), and Bi,j 〜N(0,2/c).
For input x and weight matrices -W→ := (W1, W2, . . . , WL), the network output f(x) is also
denoted by f (-W→, x). In this work, we only update weights in -W→ and leave A and B at the random
initialization. For input x ∈ Rd and l ∈ [L] , we denote by Gl (x) a diagonal matrix, which
represents the activation pattern of the l-th layer, which we call an indicator matrix. More precisely,
we define the i-th diagonal element as Gl(x) i,i := 1{gl(x)≥0} . Since the ReLU activation is
positive homogeneous, we obtain the following equality: gl (x) = WlGl-1 (x)gl-1 (x).
Dataset and loss function. The data are generated from an unknown distribution D over (x, y) ∈
Rd × Rc, where x is the input data point and y is the label associated with this data point. We
assume without loss of generality that for each input x = (x1, . . . , xd), using additional coordinates
(xd+1, xd+2), the replacement input x00 := (x1, . . . , xd, xd+1, xd+2) is normalized so that kx00k2 =
1 and its last coordinate Xd+2 = 1/√2.3 * We also use X to denote the replacement input x00 ∈ Rd+2.
The training data Z := {(x(1) , y(1) ), . . . , (x(n), y(n))} is given as n i.i.d. samples from D. We
define the minimum distance of the training data: δ := min{kx(i) -x(j)k2 : ∀i,j ∈ [n], i 6= j} > 0.
For the l2 regression loss '(y^, y) := 2 ∣∣τ^ - yk2 and a subset of training data Z(T) ⊂ Z, we define
our regression objective as follows: LZ(T)(W) := E(x,y)〜z(t)['(f (W, x), y)].
Stochastic gradient descent with Gaussian initialization. We use mini-batch SGD to train
the network with a constant learning rate η > 0, a batch size b and iteration number T . Let
W(0) := (W1,(0), W2,(0), . . . , WL,(0)), A, B be weight matrices generated from the above Gaus-
sian initialization. Suppose we start at W(0) and for each l ∈ [L] and t = 0, 1, . . . , T - 1,
Wl,(t+1) = Wl,(t) - η ∙VwiLz(t) (W(t)),	(2)
where Z(t) ⊂ Z is a mini-batch of size b. For input x, each layer l ∈ [L] and each step t ∈ [T],
we denote by gl,(t) (x) the unit output, f(t) (x) the network output, Wl,(t) the weight matrix, and
Gl,(t)(x) the indicator matrix.
In the above setting, recent paper (Allen-Zhu et al., 2018b) shows that SGD can allow an over-
parameterized multi-layer network to attain arbitrarily low training error as follows:
Theorem 1 (Convergence of SGD (Allen-Zhu et al., 2018b)). For any ε ∈ (0, 1], δ ∈
(0,O(1∕L)] and b ∈ [n], let m ≥ ω (PoMn片厂^) , η ：= θ (PoFyRL⅛m) , T =
Θ (poly(n,Lδ2log m log (nloεm)) , and W(0),A,B are at random initialization. Then, it satisfies
with probability at least 1 — e-Q(Iog2 m) over randomness of Z⑴,...Z(T):
LZ(W) ≤ ε,	∣Wl,(t) - Wl，(O)IIF ≤ O (λl√gm),	∣∣[Wl,(t)]i - [Wl,(0)]i∣∣2 ≤ O (λlomm)
(∀t ∈ [T]), where
λ:
n3.5√c
δ√b .
Our results on generalization also crucially depend on this analysis.
3	Random Walk Analysis
In this work, we consider the behavior of the unit output in each layer on a one-dimensional linear
path between two data points, which we denote by x(s) (s ∈ [0, 1]). In this section, for notational
simplicity, we drop the superscript with respect to (t), which is the t-th iteration.
3Without loss of generality, one can rescale and assume ∣∣x∣∣2 ≤ 1/Vz2 for every input x. Again, without
loss of generality, one can pad each X by an additional coordinate xd+ι to ensure kx0∣∣2 = 1/vz2. Finally,
without loss of generality, one can pad each x0 by an additional coordinate xd+2 to ensure kx00 k2 = 1. This
last coordinate xd+2 = 1 / vz2 is equivalent to introducing a (random) bias term and this procedure is described
in Allen-Zhu et al. (2018a).
4
Under review as a conference paper at ICLR 2020
Definition 3.1 (One-dimensional Linear Path). For each pair of data points x(p) and x(q) (p 6= q),
We define x(s) := (1 - s) x(p) + s x(q), s ∈ [0, 1], and denote by v := x(q) - x(p) the direction
vector.
Note that the unit output gil(x(s)) and the network output fi(x(s)) are continuous piecewise linear
functions on s ∈ [0, 1], and the network output can be expressed as follows:
f(x(s)) = BGL(X(S))WLGLT(x(S))WLT …G1(χ(s))w1G0(χ(s))Aχ(s).	(3)
Note that gil(x(s)) and fi(x(s)) have linear approximations at the point s = 0 as follows4:
H(S)= gi(x(0)) + s ∙ -dgi(x(s))	, fi(s) ：= fi(x(0)) + s ∙ -dfi(X(S))	.	(4)
dS	s=0	dS	s=0
We prove that these piecewise linear functions are almost straight, which contributes to the gener-
alization of NN. In other words, the unit output gil(X(S)) can be approximated by a linear function
with high accuracy. Moreover, the small difference between g[(x(s)) and gi(s) indicates the low
complexity of the unit output.
Theorem 2 (A Priori Estimates for Implicit Regularization). Under the same setting as Theorem 1,
with probability at least 1 一 e-Q(Iog2m), for ^very X(P), x(q) (p, q ∈ [n], P = q), t ∈ [T], I ∈ [L],
i ∈ [m] andj ∈ [c], we have
sup
0≤s≤1
gil,(t) (X(S)) 一
l,(t)
gi
OsupJff) (X(S)) 一 f(t)(S)I ≤ O (log √c m) ∣∣vk2.
(5)
(6)
3.1	Intuition Behind Implicit Regularization
To prove Theorem 2, we introduce our key analysis that the gradient of gl,(t) (X(S)) is “Gaussian
random walk” and nearly equal to a straight line on the linear path X(S), which we call random
walk analysis. For simplicity, we explain the outline of the proof for the initial state of the network
(i.e. t = 0). All proofs are given in the Supplementary Material (including 0 ≤ t ≤ T).
To state our key Lemma, We define the following function: gl(S) := Wlφ(gl-1(S)), which is an
analogue of the unit output gl(x(S)) = Wlφ(gl-1(x(S))). In other words, gl (S) is the one in which
gl-1 (x(s)) is replaced by ^lT(S). Next, Lemma 3 shows that each gl can be well approximated by
a linear function gi(S) with high probability.
Lemma 3 (Linear Approximation Analogue). With probability at least 1 一 e-Q(Iog2 m), for ^v^ry
X(p), X(q) (p, q ∈ [n], p 6= q), t ∈ [T] and l ∈ [L], we have
OsUpJg严(S)- ∕t)(s)∣ ≤ O(lo√mm)kv∣2, (∀i ∈ H),	⑺
OsupJf(t)(S) - f(t)(S)∣ ≤ O (l0√√cm) kv∣2, (∀j ∈ [c]).	(8)
The purpose of this subsection is to give an intuitive explanation of the proof of Lemma 3, which
can be divided into two steps. The first step gives estimates of the number of breakpoints. We
show that the number of breakpoints of the piecewise linear function gl (s) is less than or equal to
the number of units in the layer (i.e. m).
The second step gives estimates of gradient gaps of ggil (S). We show that the gradient gaps are
independent Gaussian random variables and the gradient of ggl (S) is the sum of the gradient gaps,
which indicates that the gradient is a “Gaussian random walk”. Note that in this setting, the number
of breakpoints is equal to the total number of steps of the random walk.
4 Although the components of gl(s), f(s) are non-differentiable at breakpoints, with probability 1, the start
point s = 0 is not a breakpoint.
5
Under review as a conference paper at ICLR 2020
Step 1. By the definition of gl(s), We may write
m
9il(s) = XWl]i,j Φ(gj-1(s)).	⑼
j=1
Note that the input to the above φ, that is gj-1(s), is a linear function on S ∈ [0,1]. For each
j ∈ [m], the linear equation OjI-I(S) = 0 has at most one solution (S = s*). If the solution S = s*
satisfies 0 < s* < 1, then g/(S) has a breakpoint at S = s*. This shows that the number of
breakpoints of g/(S)is bounded by the number of the linear equations {gj-1(S)= 0}j∈[m], which
is equal to m. We denote by 0 < si < s2 < •…< Sα < 1 breakpoints of gJ(S), where α is the
number of breakpoints. For β ∈ [α], we set an open interval of breakpoints Iβ := (Sβ, Sβ+1).
Step 2. To give an intuition that the gradient of gi(s) - gi(S) is “Gaussian random walk”, we fix
some notation. For β ∈ [ɑ], we define the gradient Vβ:
Ve := d (gi(S)- W(S)) L .
s∈Iβ
(10)
Note that gi(s) is a linear function, and there is no breakpoints for gi(s) on Ie. Note also that since
gi(s) - gil(s) is linear on Ie, the gradient is constant on Ie. For β ∈ [ɑ],wedefinea gradient gap:
Xe := Ve - Ve-ι, and we have the following estimate with probability at least 1 - e-Q(Iog2 m):
log m
Xe = Sωe where ωg 〜N(0, 2/m), |S| ≤ —^^kvk2.
m
(11)
Note that ωe is an element of the weight matriXWl. Thus, {Xe} are independent Gaussian random
variables, and Ve is the sum ofXe, that is
e
Ve=XXγ, (β∈ [α]).
γ=1
(12)
This shows that the gradient Ve is a “Gaussian random walk” and depends essentially on the num-
ber of breakpoints α and the variance of weights 2/m. Using the randomness ofWl and general
Hoeffding’s inequality (Vershynin, 2018), we have
(13)
According to Step 1, the number of breakpoints α is less than or equal to m. Thus, Ve is bounded
by lo√√mm ∣∣vk2 with probability at least 1 - e-Q(Iog2 m). Therefore, with the same probability, we
have IddS(gi(s) - gi(s)) ∣ ≤ l°√mmkvk2, (∀s ∈ [0,1]). Thus, integrating this inequality from S = 0
to S = 1 shows the estimates of (7) in Lemma 3. Note that the proof of (8) in Lemma 3 is idential to
the above proof, eXcept the fact that each entry of B follows from N(0, 2/c) insted of N(0, 2/m).
Note that as illustrated in Figure 1, for sufficiently large m, the gradient Ve can be made arbitrarily
small, which means that the networks interpolate almost linearly between the data points.
3.2	Proof Theorem 2
The purpose of this subsection is to give the proof of Theorem 2. We proceed by induction on the
layer l. Note that since g0(x(s)) = Ax(S) is linear and g0(s) = g0(x(s)), g1(s) is identically
equal to g1 (x(S)) = W1φ(Ax(S)). Thus, the case l = 1 is true. Assume the theorem holds for any
layer l = k, and let us prove it for l = k + 1. Using the triangle inequality, we have
∣gk+1(x(s)) - gk + 1(s)∣ ≤ ∣gk+1(x(s))-犷1(s)∣ + ∣gk+1(s)-犷1(s)∣ .	(14)
6
Under review as a conference paper at ICLR 2020
By Proposition 11.3 of (Allen-Zhu et al., 2018b), for any y,y ∈ Rm, there exists a diagonal matrix
D ∈ Rm×m such that |Dj,j| ≤ 1 and φ(y) - φ(y) = D(y - y). Thus, for the first term on the
RHS, we may write
∣gk+1(x(s)) - gk+1(s)∣ = ∣[Wk+1Φ(gk(X(S)))]i - [Wk+1φ伍k(s))]J	(15)
m
=∣ [Wk+1D(gk(X(S))- gk(s))]i∣ = ∣ X[Wk]i,ρDρ,ρ(gk(x(s))-磅(s))∣	(16)
ρ=1
≤ Ilgk (X(S))- gk (S)Il 2 √m .	(I 7)
In the last inequality, we use general Hoeffding’s inequality (Vershynin, 2018), and this inequality
holds with probability at least 1 - e-Q(Iog2 m). We can now apply our induction hypothesis to the
above estimate. Thus, applying (7) in Lemma 3 to the second term on the RHS of (14) shows the
estimates of (5) in Theorem 2. Next, Using the triangle inequality for fj(t), we have
∣f(t)(x(s)) - f(t)(s)∣ ≤ ∣f(t)(x(s)) - fjtt(s)∖ + ∣f(t)(s) - f(t)(s)∣ .	(18)
In a similar way, using the randomness of B (recall each entry of B follows from N (0, 2/c)), with
probability at least 1 - e-Q(log2 m), We have
印(X(S))- f(t)(S)I ≤ ι∣gL(X(S))- gL(s)∣ι2 rm log m ≤ O( lo√m)kvk2
(19)
In the last inequality, we use (7) in Lemma 3. Thus, applying (8) in Lemma 3 to the second term on
the RHS of (18) shows the estimates of (6) in Theorem 2. We complete the proof.
4 Generalization
In this section, considering the mechanism of implicit regularization revealed by random walk anal-
ysis, we provide a priori estimates for the generalization performance of over-parameterized deep
neural networks, in an l2 regression task on a one-dimensional input-space. Since the network is
over-parameterized, the expressive power of the network is rich enough to considerably overfit the
data. Nevertheless, it is known empirically that properly initialized over-parameterized deep neural
networks can achieve the good generalization performance while fitting all training data. We esti-
mate the low complexity of over-parameterized deep networks and show that the error between a
trained over-parameterized neural network and the target function can be uniformly bounded by an
arbitrarily small positive number.
Setting. We propose a new type of generalization bounds (Theorem 4), and prove this theorem
in models with one dimensional input: d = 1. In the following, we restrict ourselves to a one-
dimensional regression task on an interval [0, ν] ⊂ R. The training dataset { x(i), y(i) }i∈[n] is
given as n i.i.d. samples from some unknown distribution D. We assume that the corresponding
target function for the regression task f* : [0,ν] → Rc is a C1-class function. This implies
that y(i) = f*(x(i)). Without loss of generality, we may assume that the input data {x(i)}i∈[n]
follow a uniform distribution on [0, ν], and after relabeling, we may assume that the data points
{x(1), . . . , x(n)} are ordered by index: 0 < x(1) < •…< x(n) < V. We define (X(O),y(O)):=
(0, f *(X(O))), (x(n+1),y(n+1)) := (ν, f *(ν)) and T := supk∈[n+i] (x(k) - x(k-1)). We denote by
f : [0, ν] → R the linear interpolation of the data points {(x(i),f *(x(i)))}i∈[n].
A priori generalization bounds. Now, we introduce a novel approach for a priori generalization
bounds, which is based on random walk analysis in the over-parameterized regime. This can be
interpreted as the significant expressive power of over-parameterized neural networks is controlled
by implicit regularization.
7
Under review as a conference paper at ICLR 2020
Theorem 4. Suppose f *(x) is a C1 - classfunction on [0, ν]. Under the same setting as Theorem 1,
for δ ∈ (0,1 /2] ,then with probability at least 1 一 (δ + e-Q(Iog2 m)),we have
(20)
Proof sketch of generalization. The purpose of this paragraph is to give an intuitive explanation of
the proof of Theorem 4 for one-dimensional output c = 1. All proofs are given in the Supplementary
Material.
To estimate E(x,y)〜D [' f(T)(x), y)] , we evaluate ∣∣f(T)(x) 一 f *(x) [^,whiehwe may writeas
∣∣f(T)(X)- f*(x)∣∣2 ≤ ∣f(T)(x)- f(x)∖ + ∖f(x) - f*(x)∣	(21)
The first term of the RHS of (21) represents an error between the trained network output f(T) (x)
and the piecewise linear function f (x). We can use the results of random walk analysis to evaluate
this critical term as follows. Theorem 2 provides that the linear approximation error of the network
output, which we denote by εf(T), is small in each interval [x(k-1), x(k)], and hence in each interval
[χ(kτ),χ(k)], the difference between f(T)(x) and f(χ) falls within the error ε,(T). This suggests
that the network output between adjacent training points is properly controlled by weight initial-
ization and SGD to keep connecting the points almost straight, which results in low complexity of
over-parameterized neural networks. This statement can be extended to the interval [0, ν]. Fix ε > 0,
and suppose m ≥ O (T6/ε3). Then with high probability at least 1 一 e-Q(Iog2 m), we have
sup
x∈[0,ν]
∖f(T)(x) - f(x)
(22)
The second term of RHS of (21) is the error of the piecewise linear approximation of f*(x) by
f(x) on the interval [0, ν]. Note that the error can be reduced by increasing n. Hence, for any fixed
ε > 0, there exists δ > 0 such that if n ≥ O (√⅛), with probability at least 1 一 δ, we have
supx∈[o,ν] |f (x) 一 f *(x)∣ ≤，ε.Thus, from (21), for m and n sufficiently large, the error between
the trained network output and the target function is uniformly bounded by ε on the interval [0, ν]
with probability at least 1 一 (e-Q(Iog2 m) + δ). This gives a priori estimates for the generalization
performance of over-parameterized neural networks.
5	Related Work
Implicit regularization in neural networks has recently become an active area of research in ma-
chine learning. A number of works have focused on the behavior of gradient descent on over-
parameterized neural networks (Neyshabur et al., 2014; Lin et al., 2016; Zhang et al., 2016; Soudry
et al., 2018; Rahaman et al., 2018). In order to get a handle on implicit regularization in deep neu-
ral networks, the majority of theoretical attention has been devoted to linear neural networks (Ji &
Telgarsky, 2018; Gidel et al., 2019; Arora et al., 2019a).
Many works try to explain generalization of over-parameterized neural networks. Recent works have
shown that on sufficiently over-parameterized neural networks, the learning dynamics of gradient
descent are governed by the NTK (Daniely et al., 2016; Jacot et al., 2018; Du et al., 2018; Allen-
Zhu et al., 2018b; Lee et al., 2019; Arora et al., 2019b). In these settings, the implicit regularization
and the generalization error of the resulting network can be analyzed via NTK and the reproducing
kernel Hilbert space (RKHS) (Bietti & Mairal, 2019; Nakkiran et al., 2019). To extend it to SGD,
Hayou et al. (2019) introduce a stochastic differential equation dependent on the NTK. Closely
related work Cao & Gu (2019) showed that the expected 0-1 loss of a wide enough ReLU network
trained with SGD and random initialization can be bounded by the training loss ofa random feature
model induced by the network gradient at initialization.
This contrasts with other recent results that show a provable separation between the generalization
error obtained by neural networks and kernel methods (Wei et al., 2018; Allen-Zhu et al., 2018a;
8
Under review as a conference paper at ICLR 2020
Allen-Zhu & Li, 2019). Several papers suggest that training deep models with gradient descent can
behave differently from kernel methods, and have much richer implicit regularization (Chizat et al.,
2019; Woodworth et al., 2019; Yehudai & Shamir, 2019; Geiger et al., 2019).
6	Conclusion
In this work, probability estimates for the network output behavior (i.e. random walk analysis) pro-
vide a priori generalization estimates for l2 regression problems. We prove that even after training,
network gradients between the data points are approximately Gaussian random walks, and the vari-
ation of the gradients between the data points is extremely small and depends essentially on the
number of breakpoints and the variance of weights. To the best of our knowledge, this paper is the
first to show a mechanism of implicit regularization and to prove the generalization bounds by using
the implicit regularization for deep (three or more layer) neural networks with ReLU activation. As
a result, we also show that over-parameterized deep neural networks can learn C 1 - class functions.
Importantly, our analysis is independent of the kernel generalization analysis, and the generalization
bounds are different from the NTK inductive bias of the RKHS norm.
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019b.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019c.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint
arXiv:1905.12173, 2019.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. arXiv preprint arXiv:1905.13210, 2019.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
arXiv preprint arXiv:1812.07956, 2019.
Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: volume II:
general theory and structure. Springer Science & Business Media, 2007.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy
learning in deep neural networks: an empirical study. arXiv preprint arXiv:1906.08034, 2019.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in deep linear neural networks. arXiv preprint arXiv:1904.13262, 2019.
9
Under review as a conference paper at ICLR 2020
Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. arXiv preprint
arXiv:1901.09021, 2019.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Training dynamics of deep networks using
stochastic gradient descent via neural tangent kernel. arXiv preprint arXiv:1905.13654, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco. Generalization properties and implicit
regularization for multiple passes sgm. In International Conference on Machine Learning, pp.
2340-2348, 2016.
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred
Zhang, and Boaz Barak. Sgd on neural networks learns functions of increasing complexity. arXiv
preprint arXiv:1905.11604, 2019.
Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953,
2017.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht,
Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint
arXiv:1806.08734, 2018.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70),
2018.
Ingo Steinwart. A sober look at neural network initializations. arXiv preprint arXiv:1903.11482,
2019.
Roman Vershynin. High-dimensional probability: an introduction with applications in data sci-
ence. Number 47 in Cambridge series in statistical and probabilistic mathematics. Cambridge
University Press, Cambridge, 2018. ISBN 978-1-108-41519-4.
Colin Wei, Qiang Lee, Jason D.and Liu, and Tengyu Ma. Regularization matters: Generalization
and optimization of neural nets v.s. their induced kernel. arXiv preprint arXiv:1810.05369, 2018.
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and
deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. arXiv preprint arXiv:1904.00687, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
10
Under review as a conference paper at ICLR 2020
A Proofs
A.1 Proof of Lemma 3
Lemma 3 (reshown, see §3.1). With probability at least 1 一 e-Q(Iog2m) ,foreVery X(P), X⑷(p, q ∈
[n], p 6= q), t ∈ [T] and l ∈ [L], we have
OsUpJgi,(t)(s) - ∕t)(s)∣ ≤ O ( o√mm) I∣vk2, (∀i ∈ [m]),	(23)
OsupJf(t)(s)- fj(t)(s)∣ ≤ O (l0√2cm)∣∣v∣∣2, (∀j ∈ [c]).	(24)
Proof. We prove Lemma 3 for a fixed l ∈ [L] and t ∈ [T] every pair of data points X(p), X(q)
(p, q ∈ [n], p 6= q), because we can apply union bound at the end.
Recall the definition gl,(t) (x) (the unit output), Gl,(t)(x) (the indicator matrix function) and X(s) =
(1 一 s)X(p) + sX(q), s ∈ [0, 1] (a linearly interpolating path between data points X(p), X(q)), we have
gl,(t)(x(s)) = Wl,⑴Gl-1,⑴(X(S))WlT,⑴…w1,(t)G0,(t)(x(s))Ax(s).	(25)
Using the definition of e l,(t)(s) and g l,(t)(s), We also have
e 3(S) = Wl，(t)GlT,⑴(x(0))Wl-1,⑴∙∙∙ W1，(t)Go，(t)(x(0))Ax(s),	(26)
(27)
g l，(t)(s) = Wl,(t)φ(e l-1,(t)(s))	(28)
=Wl，(t)GlT，(t)(s)WlT，(t)Gl-2,⑴(χ(0))Wl-2,⑶∙∙∙WIf)GO,⑶(χ(0))Aχ(s),
(29)
where Gel-1,(t)(S) is an indicator diagonal matrix for S ∈ [0, 1] as follows:
[Gel-1,(t)(S)]i,i := 1ngel-1,(t)(s)≥0o,	[Gel-1,(t)(S)]i,j := 0 (i 6= j).	(30)
We begin by proving that the number of breakpoints of the function gj(t)(s) in the interval [0.1] is
bounded by m. For fixed i ∈ [m], we have
gil,(t)(s) = [Wl,㈤φ(e l-1,㈤(s))]i	(31)
m
= X[Wl,(t)]i,j φ(gejl-1,(t)(S))	(32)
j=1
m
= X[Wl,(t)]i,j gejl-1,(t)(S)1ngel-1,(t)(s)≥0o	(33)
j=1	j
Note that the input to the above φ, that is gejl-1,(t) (S), is a linear function on S ∈ [0, 1]. For each
j ∈ [m], the linear equation e7-l-1,(t)(s) = 0 has at most one solution S = s*.5 If the solution S = s*
satisfies inequality 0 < s* < 1, then gj(t)(s) has a breakpoint at S = s*. This shows that the
number of breakpoints of gj(t) (s) is bounded by the number of the linear equations
gejl-1,(t)(S) =0 (j ∈ [m]),	(34)
which is clearly equal to m.
We denote by 0 < si < S2 < …< Sa < 1 all breakpoints of g/,(e)(S), where α is the number of
breakpoints. For notational simplicity, we set S0 = 0 and Sα+1 = 1. Note that, with probability 1,
5Otherwise, the linear function is identically zero (i.e. gejl-1,(t) (s) ≡ 0), which does not affect the number
of breakpoints.
11
Under review as a conference paper at ICLR 2020
breakpoints are all distinct from each other. Each breakpoint sβ (β ∈ [α]) corresponds to a linear
equation gejl-1,(t)(s) = 0 for some j ∈ [m]. In other words, for each breakpoint sβ (β ∈ [α]), there
exists a unique element j ∈ [m] such that gejl-1,(t) (s) = 0. Therefore, for s = sβ, we set j = jβ
(β ∈ [α]) then we have gejl-1,(t) (sβ) = 0.
It is easy to verify dSx(s) = x(q) 一 X(P) =: v. Therefore,
d

dSg
l,(t)(s) = Wl,⑴Gl-1,㈤(χ(0))Wl-1,㈤Gl-2,㈤(χ(0))Wl-2,⑴…W1,⑴G0,⑴(χ(0))Av,
(35)
-dg l,⑴(S) = Wl,⑴Gl-1,㈤(S)WlT,㈤Gl-2,⑴(χ(0))Wl-2,⑴...W1,㈤G0,⑴(χ(0))Av.
s	(36)
This implies,
d-g l,㈤(S)- -de l,⑴(S)	(37)
dS	dS
=Wl,㈤(Gl-1,⑴(S) - Gl-1,⑴(x(0))) Wl-1,⑴Gl-2,⑴(x(0))…W1,⑴G0,㈤(x(0))Av.
(38)
Note that from the definition of the indicator matrix, [Gl-1,(t) (x(0))]i,i = 1{[gl-1,(t)(x(0))] ≥0},
and from the definition of the linear approximation, ge l-1,(t) (0) = gl-1,(t) (x(0)), which says that
Ge l-1,(t) (0) equals Gl-1,(t) (x(0)).
For each i ∈ [m], we may write
-dgil,(t)(S) - d⅛l,(t)(S) = XX [Wl,(t)]i,j M(t)v]jλj,
dS	dS
j=1
(39)
where M(t) is a matrix:
M⑶：=Wl-1,⑴Gl-2,⑴(χ(0))Wl-2,⑶... W1,(t)G0,⑴(χ(0))A	(40)
and λj is the difference of indicator functions:
e
λj := 1ngejl-1,(t)(s)≥0o - 1ngejl-1,(t)(0)≥0o.	(41)
For β ∈ [α], we set an open interval of breakpoints Iβ := (Sβ, Sβ+1), and recall the notation of the
gradient V β
Ve ：= dS G产⑹-∕t)(S))[，	(42)
and the gradient gap xβ := Vβ - Vβ-1. Note that since there is not a breakpoint in Iβ, the gradient
Vβ is constant in Iβ .
Hence, for β ∈ [α] and S ∈ Iβ, we may write
d⅛l,㈤(S)- d⅛l,(t)(S) = X xγ.	(43)
SS
γ=1
Note that the gradient gap xγ (γ ∈ [α]) can be rewritten as
XY = [Wl,⑴]ijγ M㈤v]jγ ZY,	(44)
where ζY is a difference of indicator functions:
ζY := 1ngejl-1,(t)(dγ)≥0o - 1ngejl-1,(t)(dγ-1)≥0o,	∀ dY-1 ∈ IY-1 and ∀dY ∈ IY.	(45)
12
Under review as a conference paper at ICLR 2020
Note that ζγ is independent of the choice of dγ-1 and dγ. Note also that since the linear function
gejl-1,(t)(s) switches the sign at s = sγ, we have ζeγ = ±1.
We begin by proving the upper bound on Pγβ=1 xγ . For notational simplicity, we use Wl to denote
Wl,(0), and for l ∈ [L] we set
dWl := Wl,(t) - Wl,(0) = Wl,(t) - Wl,	(46)
then we may write for γ ∈ [α],
[M(t)v]jγ = [Wl-1,⑴Gl-2,⑴(χ(0))Wl-2,⑶…Wut)GO,⑴(χ(0))Av]jγ	(47)
m
= X[Wl-1,(t)]jγ,ρ[Nv]ρ	(48)
ρ=1
mm
= X[Wl-1]jγ,ρ[Nv]ρ + X[dWl-1]jγ,ρ[Nv]ρ	(49)
ρ=1	ρ=1
=: hγ + dhγ ,	(50)
where N := Gl-2,㈤(x(0))Wl-2,㈤…W1，⑴G0，㈤(x(0))A.
Therefore, we have
xγ = ζeγ[Wl,(t)]i,jγ [M(t)v]jγ	(51)
=Zγ ([Wl]ijγ + [dWl]i,jγ) (hγ + dhγ)	~	(52)
= ζeγ[Wl]i,jγhγ + ζeγ[dWl]i,jγhγ + ζeγ[Wl]i,jγdhγ + ζeγ[dWl]i,jγdhγ	(53)
=: x(γ0) + x(γ1) + x(γ2) + x(γ3).	(54)
In order to estimate Nv, we use the following estimate (see Lemma 7.1 and Claim 11.2 in (Allen-
Zhu et al., 2018b)):
Lemma. If ε ∈ (0,1], with probability at least 1 一 e-3mε /L), for a fixed vector Z and l ∈ [L],
we have
∣∣Gl,(t)(x(0))Wl,(t) …W1,(t)G0,(t)(χ(0))Azk2 ≤ (1 + ε)kz∣∣2.	(55)
This Lemma implies that with probability at least 1 一 e-Q(m/L), We have
∣∣Nv∣2 = ||Gl-2，(t)(x(0))Wl-2，(t)…W1,(t)G0,(t)(χ(o))Avk2 ≤ 2∣v∣2.	(56)
We will estimate the 1st term x(γ0). Conditioning on this event (56) happens, using the randomness
of Wl-1 and general Hoeffding’s inequality (Vershynin, 2018), for each fixed vector v, we have
Cτ2
P {lhγ | >τ} ≤ P (- (2∕m)kNv∣2) .	(57)
Choose T := lo√mm ∣∣Nv∣2. Then, with probability at least 1 ― e-Q(IOg2 m), We have
∣hγ| ≤ l0gm∣Nv∣2.	(58)
m
We set u(γ0) := ζeγ hγ and u(0) = (u(10), . . . , u(β0)). Note that since ζeγ = ±1, we have u(γ0) =
∣Zγ hγ∣ ≤ lo√mm IlNvk 2. The set of the gradient gaps {xY0)} = {uY0)[Wl]ijγ} are a sequence of in-
dependent Gaussian random variables. Hence, Pγβ=1 x(γ0) is a “Gaussian random walk” (12). Thus,
using the randomness ofWl, conditioning on the above event (58), fixing any v, with probability at
13
Under review as a conference paper at ICLR 2020
least 1 - e-Q(log2m), we have
β
XχY0) ≤ l√mku(0)k2
γm
γ=1
Therefore, we have the estimate of x(γ0).
Next, we will estimate the 2nd term x(γ1). By definition, we may write
ββ
X x(γ1) = Xζeγ[dWl]i,jγhγ.
γ=1	γ=1
Note that we use Theorem 1 (the convergence theorem) to write
k[dWl]ik2 = k[Wl,(t)]i - [Wl]ik2 ≤ O 卜Iomm)
Thus, applying the estimate of hγ to eq. (58), we have
β
X x(γ1)
γ=1
≤ -ɪ kvk2 ∙ √β k[dWl]ik2
m
≤ Cλ-og2mkvk2 ≤ -o√mkvk2.
mm
(59)
(60)
(61)
(62)
(63)
(64)
(65)
(66)
The last inequality uses λ ≤ o(√m), which is indicated by the result of Theorem 1, therefore We
have the estimate of x(γ1).
(2)
Next, we will estimate the 3rd term xγ . By definition, we may write
ββ
Xx(γ2)=Xζeγ[Wl]i,jγdhγ.
γ=1	γ=1
(67)
We set u(γ2) := ζeγ dhγ and u(2) := (u(12), . . . , u(β2)). Using the randomness of Wl and general
Hoeffding’s inequality (Vershynin, 2018), for each fixed vector v, we have
P(Xuγ2)M,jγ >τ) ≤ 2exp (-ku⑵k2 maCTk[wl"∣ψ2!	(68)
Cmτ2
≤ P V 2∣∣u(2)k2)	(69)
Note that
m
dhγ = X[dWl-1]jγ ,ρ [Nv]ρ ,	(70)
ρ=1
14
Under review as a conference paper at ICLR 2020
and choose τ :
l√gmm ku⑵ ∣∣2, with probability 1 - e-Q(IOg2 m), We have
β
X x(γ2)
γ=1
≤ lW ku-12 ≤ 等{X (X [dWjjγ,")}2
k[dWl-1 jγk2kvk2)	≤ l√mkdWl-1kFkv∣2
m
(71)
(72)
(73)
log2 m
≤ F kvk2.
The last inequality uses λ ≤ o(√m) and the result of Theorem 1:
∣∣dWl-1kF = kWl-1,㈤一WlTkF ≤ O 卜l√gm) .	(74)
Therefore, we have the estimate of x(γ2).
(3)
Next, we Will estimate the last term xγ ∖ Note that since Z7 = ±1, we have ∣Zγ| = 1, and Condition-
ing on this event (56) happens, we may write
β
Xx(γ3)
γ=1
β
ζeγ [dWl]i,jγ dhγ
γ=1
≤
≤
≤
k[dWl]ik2 {X (X[dWl-1jγ,ρ[Nv]ρ!
β	1/2
k[dWl]ik2	X k[dWl-1]jγ k22kvk22
k[dWl]ik2kdWl-lkFkvk2 ≤ l0g2mkvk2.
m
(75)
(76)
(77)
(78)
Putting this all together, with probability 1 - e-∙(IOg2m), We have
dsgil(S)- dsgil(s) = X XY ≤ 4lc√m kvk2.	(79)
ss	m
γ=1
Therefore, we have
Igil(S)-eil(S)I ≤Z dsgil(S)- dseil(S) ds	(80)
≤ 4sl0g2m∣v∣2	(O ≤∀s ≤ 1).	(81)
m
This finishes the proof of (7) in Lemma 3. Finally, the proof of (8) in Lemma 3 is idential to the
above proof, except the fact that each entry of B follows from N (0, 2/c) insted ofN(0, 2/m). We
complete the proof.
□
A.2 Proof of Generalization Theorem
Theorem 2 (reshown, see §3). Under the same setting as Theorem 1, with probability at least
1 - e-Q(log2m) ,for every X(P), x(q) (p, q ∈ [n] P = q), t ∈ [T ], l ∈ [L], i ∈ [m] and j ∈ [c], we
15
Under review as a conference paper at ICLR 2020
have
sup
0≤s≤1
gil,(t)(x(s)) -
l,(t)
gi
OsupJff) (X(S)) - f(t)(s)l ≤ O (log √c m) kvk2.
(82)
(83)
In order to give provable guarantees for the generalization performance of over-parameterized deep
networks in the l2 regression task on the one-dimensional input-space, we apply the above Theorem
2 to bound the error between the trained network outputf(T) (x) and the piecewise linear function
f (x). Note that ifwe apply Theorem 1 to this case (one-dimensional models), simple preprocessing
is needed: It just consider a mapping [0, ν] → S+1 (a half circle).
S+1 = {x = (x1, x2) ∈ R2 : kxk2 = 1, x2 ≥ 1}
It is apparent that there exists a C∞ diffeomorphism from [0, ν] to S+1 . Under this setting, the
input from S+1 is the two-dimension. More precisely, the input needs another axis of coordinates to
express the biases term. This procedure is written in the subsection of Dataset and loss function in
the paper p.4. As a result, we deal with not the case of d = 1 but the case of d = 3 in Theorem 1.
Lemma 5. Under the same setting as Theorem 1 and 2, for δ ∈ (0, 1/2], then with probability at
least 1 - (δ + e-Q(IOg2 m)), we have
sup
x∈[0,ν]
(X)- fj (X)I ≤ O
logL+1 m ν
m1/6	nδ
(84)
Proof. Without loss of generality, We may assume that the entry of the weight matrix Bij 〜
N(0, 2/c), where C = Θ (m1/3).6
For k ∈ [n + 1], we denote by |Ik| the length of the interval Ik = X(k-1), X(k) . We set c = c =
mi/3 in Theorem 2, With probability at least 1 一 e-Q(log2m), We have
sup
x∈Ik
(T)(χ) -F(T)(χ)∣ ≤ c.log m S SUD ∣χ(k) - x(k-1)I I = c.log_____mT (85)
j (X)	fj	(X)I	≤C	mi/6	(;[U+ι]∣x	X ∣	=	Cj	m1/6	/	(85)
where Cj is a constant, and T := supk∈[n+χ |x(k) — x(k-1)∣. Here we set ε := Cj logL+6m T.
Note that fjτ)(x(kT)) = f (x(kT)) and fjτ)(x(k)) = f (x(k)), by eq. (85) we have
If(T)(x(kT))- fj(x(k-D)∣ ≤ ε and	If(T)(x(k)) - fj(x(k))∣ ≤ ε.
Note that the piecewise linear function f linearly connects (χ(k-1),f∙(x(k-1)))
(x(k),f∙(x(k))). Thus, the following inequality holds.
(86)
and
SUp If(T)(x) - fj (x)∣ ≤ ε
x∈Ik
We use eq. (85) and eq. (87) to see that
sup Ifj(T )(x) - fj (x)∣ ≤ SUp IfjT )(x) - f(T )(x)∣ + SUp fτ )(x) - fj (x“ ≤ 2ε.
x∈Ik	x∈Ik	x∈Ik
Note that ε is not dependent on k . Thus, we have
sup IfT)(x) - fj(x)∣ ≤ 2ε.
x∈[0,ν]
(87)
(88)
(89)
6 For each output y = (y1,y2, ∙∙∙ ,yc),an additional coordinates (yc+ι, yc+2 ,...,yC) = (0, 0,..., 0) can
always be padded to the output. We did not try hard to improve the exponent 1/3.
16
Under review as a conference paper at ICLR 2020
Next, we evaluate T = supk∈[n+1] x(k) - x(k-1) . Recall that the training dataset {x(k)}k∈[n] is
generated from the uniform distribution in the interval [0, ν]. We denote by μ := n/v the density of
training samples. In this setting, it is well known that for every r > 0 (r < ν), the number of train-
ing samples in the interval [0, r] follows the Poisson distribution with mean μr, then the sequence
of inter-sample distances is independent and identically distributed exponential random variables
having mean 1∕μ (= V) (Daley & Vere-Jones, 2007). Define ∆ to be the above exponential distri-
bution having mean v∕n, which is the inter-sample distance. Then, using Markov,s inequality, for
τ > 0, we have
P[∆ ≥ τ]≤ E[∆l =上
τ nτ
Thus, We can choose T :=券,which implies that with probability at least 1 - δ, we have
T≤ W
nδ
Combining this with eq. (89), with probability at least 1 - (δ + e-Q(Iog2 m)), we have
I NT)z、	/ / J . W logL+1 mʃ / M logL+1 m V
xSU0,V]lfj (X)- fj(X)1 ≤ (C mi/6 T ≤ Cj mi/6 nδ.
We complete the proof of the Lemma 5.
(90)
(91)
(92)
□
ɪ .    / c	r*∕∖∙ Cl ι r . ■	Γr> 1 J ? / ∖ ■ .1 ι ■	■	ι . ■ r
Lemma 6. Suppose f*(x) is a C1 -classfunction on [0, V] and f (x) is the linear interpolation of
the data points {(x(i),∕*(x(i))) }就时 For δ ∈ (0,1/2], with probability at least 1 一 δ, we have
XsZJfj (χ)-fj*(x)∣≤ O (nδ).	(93)
Proof. We use fj"(χ) to denote the first derivative of fj (x). Since fj is a C1-class function on the
compact set [0, v], fj0 is a bounded function. Thus, we can define S(k) and S as follows:
S(k) := sup ∣∣fjj0(x)∣∣ , S := sup S(k).	(94)
x∈Ik	k∈[n+1]
Since fj(χ) and fj(x) are continuous functions, there exists h(k) (0 ≤ h(k) ≤ x(k) - x(kT)) such
that
sup |f；(X)- fj (x)∣ = |f；(x(k-1) + hk) - fj (X(I) + hk )∣.	(95)
x∈Ik
By the mean value theorem, there exists a real number c(k) ∈ Ik such that,
fj (X(kT) + h(k)) = fj0(Jk))h(k) + fj(x(kτ)).	(96)
Note that by the definition of fj, we have f；(x(kT)) = fj(x(k-1)), therefore, we also have that
∣fj (X(k-1) + h(k)) - fj (X(I) + h(k))| = |f；(x(k-1) + h(k)) - f；(x(kT))- fj0(Jk))h(k)|
(97)
x(k-1)+h(k)
≤	||fjj0(X) - fjj0(c(k))|| dX	(98)
≤ 2h(k)S(k).	(99)
By considering whole interval [0, v], we have
sup f∙ (x) - fj (x) | = sup (SUP|f；(X)-fj (x)|) ≤ 2TS	(100)
x∈[0,ν]	k∈[n+1] x∈Ik
Now we apply the estimate of T from eq. (91). We may therefore rewrite eq. (100) as
SUPJfj (x) - f；(X)| ≤ O (nVδ) .	(101)
We complete the proof of Lemma 6.
□
17
Under review as a conference paper at ICLR 2020
Proof of Theorem 4 (Generalization)
Proof. From eq.(89) and eq.(93), with probability at least 1 - (δ + e-Q(Iog2 m)),we have for every
j = 1,2,…，c,
j) (x) - f；(x)| ≤ I fj) (X)- fj (x)∖ + ∖fj (x) - fj(x)∖
≤ C-ν (ι + loCm!	(∀x ∈ [0,ν]).
nδ	m1/6
Using the triangle inequality, this gives
' f )(χ),y) = 1 ∣∣f(T)(x) - f*(χ)∣∣2 = 1XXMT)(x) - f；(x)||2
≤ 1C02c⅛ (l + loCm!2	(∀x ∈ [0,ν]).
2	n2 δ2	m1/6
where C0 := maxj=ι,…,c Cj.
Thus, we have the following estimate:
With probability at least 1 - (δ + e-Q(log2 m)), we have
E(x,y)〜D h' (f(T)(x),yj = 1 / 1||f(T)(x) - f*(x)|| dx
≤ 1C02c⅛ (l + l°gL+⅛m!2.
2	n2 δ2	m1/6
We complete the proof.
(102)
(103)
□
18