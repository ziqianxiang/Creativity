Under review as a conference paper at ICLR 2020
Why Learning of Large-Scale Neural Net-
works Behaves Like Convex Optimization
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we present some theoretical work to explain why simple gradient
descent methods are so successful in solving non-convex optimization problems in
learning large-scale neural networks (NN). After introducing a mathematical tool
called canonical space, we have proved that the objective functions in learning NNs
are convex in the canonical model space. We further elucidate that the gradients
between the original NN model space and the canonical space are related by a
pointwise linear transformation, which is represented by the so-called disparity
matrix. Furthermore, we have proved that gradient descent methods surely converge
to a global minimum of zero loss provided that the disparity matrices maintain full
rank. If this full-rank condition holds, the learning of NNs behaves in the same
way as normal convex optimization. At last, we have shown that the chance to
have singular disparity matrices is extremely slim in large NNs. In particular, when
over-parameterized NNs are randomly initialized, the gradient decent algorithms
converge to a global minimum of zero loss in probability.
1	Introduction
In the past decade, deep learning methods have been successfully applied to many challenging real-
world applications, ranging from speech recognition to image classification and to machine translation,
and more. These successes largely rely on learning a very large neural network from a plenty of
labelled training samples. It is well known that the objective functions of nonlinear neural networks
are non-convex, and even non-smooth when the popular ReLU activation functions are used. The
traditional optimization theory has regarded this type of high-dimensional non-convex optimization
problem as infeasible to solve (Blum & Rivest, 1992; Auer et al., 1995) and any gradient-based
first-order local search methods do not converge to any global optimum in probability (Nesterov,
2004). However, the deep learning practices in the past few years have seriously challenged what
is suggested by the optimization theory. No matter what structures are used in a large scale neural
network, either feed-forward or recurrent, either convolutional or fully-connected, either ReLU or
sigmoid, the simple first-order methods such as stochastic gradient descent and its variants can
consistently converge to a global minimum of zero loss no matter what type of labelled training
samples are used. At present, one of the biggest mysteries in deep learning is how to explain why the
learning of NNs is unexpectedly easy to solve. Plenty of empirical evidence accumulated over the
past years in various domains has strongly suggested that there must be some fundamental reasons
in theory to guarantee such consistent convergence to global minimum in learning of large-scale
neural networks. Recently, lots of empirical work (Goodfellow et al., 2014; Zhang et al., 2015) and
theoretical analysis (Baldi & Hornik, 1989; Choromanska et al., 2014; Livni et al., 2014; Kawaguchi,
2016; Safran & Shamir, 2016; Soudry & Carmon, 2016; Nguyen & Hein, 2017; Chizat & Bach,
2018; Safran & Shamir, 2018; Du et al., 2018b;a; Allen-Zhu et al., 2018; Zou et al., 2018) have been
reported to tackle this question from many different aspects.
In this paper, we present a novel theoretical analysis to uncover the mystery behind the learning of
neural networks. Comparing with all previous theoretical work, such as Choromanska et al. (2014);
Kawaguchi (2016); Soudry & Carmon (2016); Du et al. (2018b); Zou et al. (2018), we use some
unique mathematical tools, like canonical model space and Fourier analysis, to derive theoretical
proofs under a very general setting without any unrealistic assumption on the model structure and
data distribution. Unlike many math-heavy treatments in the literature, our method is technically
concise and conceptually intuitive so that it leads to an intelligible sufficient condition for such
1
Under review as a conference paper at ICLR 2020
consistent convergence to occur. Our theoretical results have also well explained many common
practices widely followed by deep learning practitioners.
2	Problem Formulation
In this work, we study model estimation problems under the standard machine learning set-
ting. Given a finite training set of T samples of input and output pairs, denoted as DT =
{(x1,y1), (x2,y2),…，(XT, yτ)}, the goal is to learn a model from input to output over the entire
feature space f : x → y (x ∈ RK, y ∈ R), which will be used to predict future inputs. In all practical
applications, the input X normally lies in a constrained region in RK, which can always be normalized
into a unit hypercube, denoted as UK , [0, 1]K. Without losing generality, we may formulate the
above learning problem as to search for the optimal function f(X) within the function class L1(UK)
to minimize a loss functional measured in DT, where L1(UK) denotes all bounded absolutely inte-
grable functions defined in [0, 1]K. The loss function is computed as empirical errors accumulated
over all training samples in DT. The empirical error at each sample, (Xt, yt), is usually measured by
a convex loss function l (yt, f (Xt)), which penalizes mis-classification errors and rewards correct
classifications. In other words, a meaningful loss function l(y, y0) always satisfies:
l(y, y0) ⇒	=0	(y	=	y00)	.	(1)
,	> 0	(y	6=	y0 )
Obviously, the popular loss measures, such as mean squared error, cross-entropy, always satisfy the
condition in eq.(1) and are clearly convex functions with respect to its second argument. Therefore,
we may use a general notation to represent this learning problem as follows:
T
f = arg min、Q(f∣Dτ) = arg ma、X l (yt,f (xt))	⑵
f∈L1(UK)	f∈L1(UK) t=1
Next, let’s consider how to parametarize the function space L1(UK) to make the above optimization
computationally feasible.
2.1	Literal Model Space
Based on the universal approximation theorems in Cybenko (1989); Hornik et al. (1989); Hornik
(1991), any function in L1(UK) can be approximated up to arbitrary precision by a well-structured
neural network of sufficiently large model size. Here, we use ΛM to represent the set of all well-
structured neural networks using M free model parameters, and all weight parameters of each neural
network are denoted as a vector w (w ∈ RM). Obviously, if M is made sufficiently large, ΛM
is a good choice to parameterize the function space L1 (UK) because given any bounded function
f(X) ∈ L1(UK), there exists at least one set of model parameters in ΛM, denoted as w, to make the
corresponding neural network approximate f(X) up to arbitrary precision. In this case, the function
represented by the underlying neural network is denoted as fw(X). Furthermore, if M is finite and
all model parameters are bounded, every function represented by each possible neural network in
ΛM belongs to L1(UK). As a result, once we pre-determine the neural network structure of model
size M = |w| (assume that M is sufficiently large), the functional minimization problem in eq.(2)
can be simplified into an equivalent parameter optimization problem as follows:
w* = arg min
w∈RM
Q(fw|DN ) = arg WmRM
T
Xl (yt,fw(Xt)) .
t=1
(3)
However, due to the weight-space symmetries and network redundancy, the mapping from ΛM to
L1(UK) is surjective but not injective. For any function f ∈ L1(UK), there always exist many
different w in ΛM to make neural networks represent f equally well. In this work, the space of
neural networks, ΛM, is called literal model space of L1(UK) because it only satisfies the existence
requirement but not the uniqueness one. It is the non-uniqueness of ΛM that makes it extremely
difficult to directly conduct the theoretical analysis for the optimization problem in eq.(3).
2
Under review as a conference paper at ICLR 2020
2.2	Canonical Model Space
Here, let’s consider the so-called canonical model space for L1(UK). A model space is called to be
canonical if it satisfies both existence and uniqueness requirements. In other words, for every function
f ∈ L1(UK), there exists exactly one unique model in the canonical model space to represent f.
Moreover, every model in the canonical model space corresponds to a unique function in L1 (UK).
Therefore, the mapping between L1(UK) and its canonical model space is bijective. For L1(UK),
the multivariate Fourier series naturally form such a canonical model space for L1 (UK).
As in Pinsky (2002), given any function f(x) ∈ L1(UK), we may compute its multivariate Fourier
coefficients as follows:
θk
/…/
x∈UK
f (x) e-2πik∙xdx (∀k ∈ ZK)
(4)
where i = √-1 and θk ∈ C, and k = [k1,k2,…,kK] is a tuple of K integers, denoting the
K -dimensional index of each Fourier coefficient. All Fourier coefficients of a function f(x) can be
arranged as an infinite sequence, i.e. θ = {θk | k ∈ ZK}, which in turn can be viewed as a point in a
Hilbert space with an infinite number of dimensions. This Hilbert space is denoted as Θ. For the
notational simplicity, we may represent eq.(4) by a generic mapping from any f(x) ∈ L1(UK) to a
θ ∈ Θ as: θ := Ff(x))∙
According to the Fourier theorem, for any function f(x) ∈ L1 (UK), these Fourier coefficients may
be used to perfectly reconstruct f(x) by summing the following Fourier series, which converges
everywhere in x ∈ UK :
f (x) = X Ok e2πik∙x := FT (χ∣θ) ∙	(5)
k∈ZK
Because both eq.(4) and eq.(5) converge for any function f(x) ∈ L1 (UK), therefore, Θ is a canonical
model space of L1(x). Every function in f(x) can be uniquely represented by all of its Fourier
coefficients θ in Θ.
According to the Riemann-Lebesgue lemma (pp.18 in Pinsky (2002)), the Fourier coefficients θk
decay from both sides in every dimension of k, i.e., ∣θk∣ → 0 as the absolute value of any dimension
of k goes to infinity, max |k| → ∞. As a result, the Fourier serie in eq.(5) can be truncated into a
partial sum centered at the origin. Given any small number > 0, for any function f(x) ∈ L1(UK),
we can always choose a finite number of the most significant Fourier coefficients located in the center,
which are denoted as Ne := {k | |ki| ≤ Ni, ∙∙∙ , |kK| ≤ NK}. The cardinality of Ne is denoted as
N = |N|. The truncated Fourier coefficients can also be arranged onto an N -dimensional vector,
θe = {θk | k ∈ Ne}, which may be viewed as a point in a complete normed linear space with N
dimensions. This finite-dimensional normed linear space is thus denoted as Θe . Moreover, we may
use these truncated coefficients in Ne to form a partial sum to approximate the original function f(x)
up to the precision 1- 2 :
f(x)= X θk eik∙x	(6)
k∈N
where f(x) is an infinitely differentiable function in UK, approximating f (x) up to 1 - e2 in the L2
norm,i.e. R …Rχ∈uK kf (X) - f(X)k2dx ≤ e2.
In summary, for any function f(x) ∈ L1(UK), we may calculate its Fourier coefficients as in eq.(4)
to represent it uniquely in the infinite-dimensional canonical space Θ as θ = F (f (x)) where θ ∈ Θ.
Furthermore, for the computational convenience, we may approximate Θ with a finite-dimension
space up to any arbitrary precision. For a given tolerance error , we may truncate θ by leaving out
all insignificant coefficients and construct θe of N dimensions as above. This process is conveniently
denoted as a mapping: θe = Fe (f (x)). On the other hand, θe may be used to construct f(x) based
on the partial sum in eq. (6), ι.e. f (x) = F 1(x∣θe). As shown above, f (x) approximates the
original function f(x) up to the precision 1 - 2, denoted as f(x) = f(x) + O(). Here, we call
this finite dimensional space Θe as an -precision canonical model space of L1 (UK). Obviously,
the approximation error 2 can be made arbitrarily small so as to make Θe approach Θ as much as
possible.
3
Under review as a conference paper at ICLR 2020
Figure 1: An illustration to show how literal vs. canonical model spaces are related when neural
networks are used to approximate function class L1 (UK).
2.3	Literal vs. Canonical Model Space
For any neural network in the literal model space ΛM, assuming its model parameter is w(0) , it
forms a function fw(0) (x) between its input and output. As we know fw(0) (x) ∈ L1 (UK), we can
easily find its corresponding representation in the canonical model space with infinite dimensions via
θ(0) = F (fw(0) (x)), where θ(0) ∈ Θ. Similarly, we may truncate θ(0) to find its representation in
an -precision canonical model space with N dimensions as θ(0) = F(fw(0) (x)), where θ(0) ∈ Θ.
As shown in Figure 1, the mapping from ΛM to Θ is surjective but not injective while the mapping
from Θ to L1 (UK) is bijective.
3	Main Results
3.1	Learning in Canonical Space
Let’s consider the original learning problem eq.(2) in the canonical model space. Since canonical
model spaces normally have nice structures, i.e., every f (x) is uniquely represented as a linear
combination of some fixed orthogonal base functions. The functional minimization in eq.(2) turns to
be an optimization problem to determine the unknown coefficients in a linear combination as follows:
T
θ* = arg min Q(θ∣DT) = arg min ^X l (yi, FT(Xi | θ))
∈	∈	t=1
(7)
This learning problem turns out to be strikingly easy in the canonical model space.
Theorem 1 The objective function Q(f |DT) in eq.(2), when constructed using a convex loss measure
l(∙), is a convex function in the canonical model space. Ifthe dimensionality of the canonical space is
not less than the number of training samples in DT, the global minimum achieves zero loss.
Proof: Given an arbitrarily small tolerance error ≥ 0, any function f(X) ∈ L1 (UK) can be
uniquely mapped to a point in an canonical space Θ as: θ = F (f (X)). Meanwhile, the function
f(X) may be approximated by θ = {k | k ∈ N} with the partial sum of Fourier series: fθ (X) =
Pk∈N θk e2πik∙χ = Pk∈N θkηk(x), where ηk(x) = e2πik'x. If We constrain to optimize eq.(2)
in the canonical space Θ, the objective function is represented as:
TT
Q(fθe |DT) = X l(yt,fθe (Xt)) = X l I yt, X θkηk(Xt) )∙
t=1	t=1	k∈N
Since the loss measure l(∙) itself is a convex function of its second argument, and the argument is
a linear combination of all parameters θk , therefore, the right-hand side of the above equation is a
4
Under review as a conference paper at ICLR 2020
convex function of its parameter θe. When mean squared error is used for l(∙), the above procedure is
the same as the well-known least square method.
Due to eq.(1), if a global minimum θ* achieves zero loss, it must satisfy: l(yi,f(Xi)) =
l (yi, Pk∈N θkηk(xi)) = 0 (i = 1,2, ∙ ∙ ∙ , T), which are equivalent to a system of T linear
equations: Pfc∈Ne θkηk(xi) = y (i = 1, 2,…，T).
Obviously, if the dimensionality of the canonical model space is not less than the number of point-
wise distinct (and noncontradictory) training samples, T, we have more free variables in θ* than the
total number of linear equations. Therefore, there exists at least one solution to jointly satisfy these
independent equations, which also achieves the zero loss at the same time. If the dimensionality of
the canonical model space is equal to the number of point-wise distinct training samples, there exists
a unique global minimum that achieves zero loss.
Corollary 1 Q(f |DT) in eq.(2) is a convex functional in L1 (UK).
Proof: For any two functions, f1, f2 ∈ L1(UK), we may map them into the canonical model space
Θ to find their representations as: θ1 = F(f1) and θ2 = F(f2). As shown in eq.(5), both f1 and
f2 can be represented as a linear function of θι and θ2. Since l(∙) is a convex function, for any
0 ≤ ε ≤ 1, it is trivial to show that Q(ε f + (1 — ε) f2∣Dτ) ≤ εQ(fι∣Dτ) + (1 — ε) Q(f2∣Dτ).
Therefore, Q(f |DT) is a convex functional in L1 (UK).
3.2	Going Back to Literal Space from Canonical Space
As shown above, learning in the canonical model space is a standard convex optimization problem.
However, the direct learning in the canonical model space may be computationally prohibitive in
high-dimensional canonical spaces. At present, the common practice in machine learning is still to
learn neural networks in the literal model space. Here we will look at how the canonical space may
help to understand the learning behaviours of neural networks in the literal space.
The learning of neural networks mainly uses the first-order methods, which solely rely on the gradients
of the objective function. We will first investigate how the gradients in the literal model space are
related to the gradients in the canonical model space. Given model parameters w of a neural network,
which may be viewed as a point in the literal space ΛM with M = |w| dimensions, the objective
function at w is denoted as Q(fw|DT). If the training set DT has T point-wise distint training
samples, we consider an -precision canonical space, Θ . The cardinality of Θ, denoted as N,
is chosen to satisfy two conditions: i) N is not smaller than T; ii) N is large enough to make the
truncation error sufficiently small. In this way, for any w in ΛM, it may be mapped to this canonical
space as θe = Fe(fw(x)) (θe ∈ Θe). Obviously, Q(fw|Dn) = Q(θ∣Dτ) + O(e). However, the
residual error term may be ignored since it can be made arbitrarily small when we choose a large
enough N . Moreover, due to N ≥ T, the global minimum of the objective function achieves zero
loss in this canonical space. In this work, we consider the learning problem of over-parameterized
neural networks, where the dimensionality of the literal space, M = |w|, is much larger than the
dimensionality of this canonical space, namely M N .
Based on the chain rule, we have:
VwQ(fw∣Dτ) = VθeQ(fθe |Dt) Vwθe = ▽或Q(fθe |Dt) PwFe(fw(x)
which can be represented as the following matrix format:
-∂Q -
∂wι
∂Q
∂Wm
∂Q
,dwM-I M×1
F (∂fw(x)∖
e	∂w1
.
.
.
多(dfw(X) ʌ
Fe〈 ∂Wm J
.
.
.
((dfw(X) A
Fe〈 ∂WM J
-∂Q -
∂θι
∂Q
∂θn
(8)
M×N
∂Q
∂θNJ
N×1
where the M × N matrix is called disparity matrix, denoted as H(w). The disparity matrix H(w)
is composed of M sets of Fourier coefficients (computed over the input X) of partial derivatives of
5
Under review as a conference paper at ICLR 2020
the neural network function with respect to each of its weights. When we form the m-th row of
the disparity matrix, we first compute the partial derivative of the neural network function fw(x)
with respect to m-th parameter, wm, in the neural netowork. This partial derivative, fW,(x), is still a
∂wm
function of input x. Then we first apply Fourier series in eq.(5) to it and then truncate to keep the
total N most significant Fourier coefficients. These N coefficients are aligned to be placed in the
m-th row of the disparity matrix H(w). This process is repeated for every parameter of the neural
network so as to fill up all rows in the M × N matrix. Obviously, the disparity matrix depends on
w because when w take different values, these function derivatives are normally different and so
are their Fourier coefficients. For any given neural network w, this mapping can be represented as a
compact matrix format:
网 QL ×ι=hH(W)L ×n[v%qL ×ι∙	⑼
From the above, we can see that the gradient in the literal space is related to its corresponding
gradient in the canonical space through a linear transformation at every W, which is represented by
the disparity matrix H(W). The disparity matrix varies when W moves from one model to another in
the literal space. As a result, we say that the gradient in the literal space is linked to its corresponding
gradient in the canonical space via a point-wise linear transformation.
Lemma 1 Assume the used neural network is sufficiently large (M ≥ N). If w* is a stationary point
of the objectivefunction Q(fw∖Dτ) and the corresponding disparity matrix H(w*) has full rank at
w*, then w* is a global minimum of Q(fw |DT).
Proof: If w* is a stationary point, it implies that the gradient vanishes at w* in the literal model
space, i.e., vwQ(w*)	= 0. Substituting this into eq.(9), we have
hH(w*)iM×NhvθQ(θ)iN×1 = 0	(10)
because M ≥ N and H(w*) has the full rank (N), the only solution to the above equations in the
canonical space is vθQ(θ*) = 0. In other words, the corresponding model θ* in the canonical
space is also a stationary point. Since Q(θ) is a convex function in the canonical space, this implies
the corresponding model θ* is a global minimum. Due to the fact that the objective functions are
equal across two spaces when we map w* to θ*, i.e., Q(w*) = Q(θ*), we conclude that w* is also
a global minimum in the literal space.
Lemma 2 If w(0) is a stationary point of Q(fw∖DT), but the corresponding disparity matrix
H(w(0)) does not have full rank at w(0), then w(0) may be a local minimum or saddle point
or global minimum of Q(fw∖DT).
Proof: The disparity matrix is an M × N matrix and we still assume M ≥ N. If H(w(0))
degenerates and does not have full rank at w(0) , it is possible to have zero or some nonzero so-
lutions in the canonical space to the system of under-specified equations in eq.(10). For the zero
solution vθQ(θ(0)) = 0, the corresponding model of θ(0) in literal space is a global minimum
because the objective function is convex in the canonical space. However, for the non-zero solutions,
Vθe Q(8(0)) = 0, the corresponding model of 星0) in literal space is certainly not a global minimum
since the gradient does not vanish in the canonical space. They may corresponds to some bad local
minimum or saddle points in the literal space since the gradient vanishes only in the literal space.
As we know that neural networks are directly learned in the literal space as in eq.(3). We normally use
some fairly simple first-order local search methods to solve this non-convex optimization problem.
These methods include gradient descent (GD) and more computationally efficient stochastic gradient
descent (SGD) algorithms. As shown in Algorithm 1, these algorithms operate in a pretty simple
fashion: it starts from a random model and the model is iteratively updated with the currently
computed gradient VWQ(w(k)) and some preset step sizes, hk (k = 0,1,2,…),which are usually
decaying.
The optimization in eq.(3) is clearly non-convex in the literal space for any nonlinear neural networks.
However, due to the fact that both literal and canonical spaces are complete and the gradients in both
6
Under review as a conference paper at ICLR 2020
Algorithm 1 Stochastic gradient descent (SGD) to learn neural networks in the literal space
randomly initialize w* * * * * * * * * * * * * * (0), and set k = 0
for epoch = 1 to L do
for each minibatch in training set DT do
w(k+1) - w(k) 一 hkVwQ(w(k))
k J k +1
end for
end for
spaces are linked through a point-wise linear transformation, in the following, we will theoretically
prove that the gradient descent methods can solve this non-convex problem efficiently in the literal
space in a similar way as solving normal convex optimization problems. As long as some minor
conditions hold, the gradient descent methods surely converge to a global minimum even in the literal
space.
Theorem 2 In the gradient descent methods in Algorithm 1, assume neural network is large enough
(M ≥ N), if the initial model, W(O), and the step sizes, hk (k = 0,1, ∙∙∙), are chosen as such to
ensure the disparity matrix H(w) maintains full rank at every w(k) (k = 0,1, ∙…),then it Surely
converges to a global minimum of zero loss. Moreover, the trajectory of Q(Wg) (k = 0,1, •…)
behaves in the same as those in typical convex optimization problems.
Proof: In non-convex optimization problems, as long as the objective function satisfies the Liptschitz
condition and the step sizes are small enough at each step, the gradient descent in Algorithm 1 is
guaranteed to converge to a stationary point, namely kVwQ(w(k))k → 0 as k → ∞ (Nesterov,
2004). As long as H(w(k)) maintains full rank, according to Lemma 1, w(k) approaches a global
minimum, i.e. Q(w(k)) → 0 as k → ∞.
Moreover, as shown in eq.(9), we have VWQ(w(k)) = H(w(k)) VθQ(θ(k)) for k = 0,1,…，where
θ(k) denotes the corresponding representation of w(k) in the canonical space. As VwQ(w(k)) → 0,
H(w(k)) normally does not approach 0. Let us show this by contradiction. Assume H(w(k)) → 0,
which means that every row of H(w(k)) approaches 0 at the same time. As we know, m-th row of
H(w) corresponds to the Fourier series coefficients of the function fw(x). If the Fourier coefficients
are all approaching zero, it means that the function itself approaches 0 as well, i.e., dfW(x) = 0. It
means the current neural network function fW(x) is irrelevant to the weight wm. We may simply
remove the link associated to wm without changing fW(x). Because all rows of H(w) approaches
0, we may remove all links of all weights from the neural network without changing fW(x). In all
network structures used for artificial neural networks, after all weights are removed, there usually
does not exist any link from input to output. As a result, as long as H(w(k)) → 0, the current fW(x)
approaches a constant function in the entire input range UK . This normally will not happen if a
suitable initial model is chosen. Therefore, we have H(w(k)) 9 0 for all k = 0,1, ∙∙∙. If all H(w(k))
maintain full rank at the same time, as VWQ(w(k)) → 0, we will surely have Vθ Q(θ(k)) → 0. The
trajectory of θ(k) will converge towards a global minimum in the convex loss suface in the canonical
space. Since the objective functions are equal across two spaces when we map w(k) to θ(k), i.e.,
Q(w(k)) = Q(θ(k)), then the trajectory of Q(w(k)) will go down steadily in the literal space until it
converges to the global minimum.
3.3 WHEN DO THE DISPARITY MATRIX H(w) DEGENERATE ?
As shown above, the success of gradient descent learning in the literal space largely depends on the
rank of the disparity matrix H(w). Here we will study under what conditions the disparity matrix
H(w) may degenerate into a singular matrix. As we have seen before, the disparity matrix H(w)
is a quantity solely depending on the structure and model parameters of neural network, w, and it
has nothing to do with the training data and the loss function. Since its columns are derived from
orthogonal Fourier bases, its column vectors remain linearly independent unless its row vectors
degenerate. As an M × N matrix (M ≥ N), H(w) may degenerate in two different ways: i) some
7
Under review as a conference paper at ICLR 2020
rows vanish into zeros; ii) some rows are coordinated to become linearly dependent. In the following,
we will look at how each of these cases may actually occur in actual neural networks.
3.3.1	Dead neurons
In a neural network, if all in-coming connection weights to a neuron are chosen in such a way as
to make this neuron remain inactive to the entire input range UK, e.g., choosing small connection
weights but a very negative bias for a ReLU node. In this case, for any x ∈ UK, this neuron does
not generate any output. It becomes a dead neuron in the network. Obviously, for any connection
weight W to a dead neuron, dfw(x) = 0. Asa result, the corresponding row of W in H(W) is all zeros.
Therefore, dead neurons lead to zero rows in H(w) and they may reduce the rank of H(w). The
bad thing is that the gradients of all weights to a dead neuron are always zero. In other words, the
gradient descent methods can not save any dead neurons once they become dead. In a very large
neural network, if a large number of neurons are dead, the model may not have enough capacity to
universally approximate any function in L1(UK), this invalidates the assumption of completeness
from the literal space to the canonical space. Obviously, the learning of this neural network may not
be able to converge to a global minimum once a large number of neurons are dead.
3.3.2	Duplicated or coordinated neurons
If many neurons in a neural network are coordinated in such a way that their corresponding rows in
the disparity matrix H(W) become linearly dependent, this may reduce the rank of H(W) as well.
However, in a large nonlinear neural network, the chance of such linear dependency is very small
except the case of duplicated neurons. Two neurons are called to be duplicated only if they are
connected to the same input nodes and the same output nodes and these input and output connection
weights happen to be identical for these two neurons. Obviously, two duplicated neurons responds
in the same way for any input x so that one of them becomes redundant. The rows of H(W)
corresponding to duplicated neurons are the same so that its rank is reduced. Similar to dead neurons,
the duplicated neurons will have the same gradients for their weights. As a result, once two neurons
become duplicated, they will remain as duplicated hereafter in gradient descent algorithms. Like
dead neurons, if there are a large number of duplicated neurons in a neural network, this may affect
the model capacity for universal approximation. As a result, the learning may not be able to converge
to a global minimum.
3.3.3	Initial conditions are the key
In an over-parameterized neural network (assume M N), the disparity matrix H(W) becomes
singular only after at least M - N neurons become dead or duplicated or coordinated, each of
which essentially indicates all neural network parameters happen to satisfy an equality constraint. If
the initial model W(0) is randomly selected in such a way that all hyperplanes corresponding to all
neurons intersect the input space UK from the center as much as possible. The initial disparity matrix
is nonsingular in probability one. When we use the gradient descent algorithms in Algorithm 1 to
update the model, the chance to derive any new dead or duplicated neurons is extremely slim because
it is unlikely for all parameters to simultaneously satisfy a large number of equality constraints.
This is intuitively similar to the case where a large number of random points are generated in a
high-dimensional space, the chance for all points to happen to lie in a hyper-plane is sufficiently
small. Therefore, when an over-parameterized neural network is randomly initialized, the gradient
descent algorithm in Algorithm 1 will converge to a global minimum of zero loss in probability one.
4 Final Remarks
In this paper, we have presented some theoretical analysis to explain why gradient descents can
effectively solve non-convex optimization problems in learning large-scale neural networks. We
make use of a novel mathematical tool called canonical space, derived from Fourier analysis. As
we have shown, the fundamental reason to unravel this mystery is the completeness of model space
for the function class L1(UK) when neural networks are over-parameterized. The same technique
can be easily extended to other function classes, such as linear functions (Baldi & Hornik, 1989;
Kawaguchi, 2016; Haeffele & Vidal, 2015), analytic functions and band-limited functions (Jiang,
8
Under review as a conference paper at ICLR 2020
2019). As another future work, we may further quantitively characterize the disparity matrix H(w)
to derive the convergence rate in learning large neural networks.
References
Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In preprint arXiv:1811.03962, 2018.
P. Auer, M. Herbster, and M.K. Warmuth. Exponentially many local minima for single neurons. In
Proc. of Advances in Neural Information Processing Systems 8 (NIPS), 1995.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural Networks, 2:53-58, 1989.
A. L. Blum and R. L. Rivest. Training a 3-node neural network is NP-complete. Neural Networks, 5
(1):117-127, 1992.
L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models
using optimal transport. In Proceedings of the 32nd Conference on Neural Information Processing
Systems (NeurIPS), 2018.
A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of
multilayer networks. In preprint arXiv:1412.0233, 2014.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals, and Systems, 2:303-314, 1989.
S. Du, J. D. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. In preprint arXiv:1811.03804, 2018a.
S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. In preprint arXiv:1810.02054, 2018b.
I. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization
problems. In preprint arXiv:1412.6544, 2014.
B. D. Haeffele and R. Vidal. Global optimality in tensor factorization, deep learning, and beyond. In
preprint arXiv:1506.07540, 2015.
K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4:
251-257, 1991.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approxi-
mators. Neural Networks, 2(5):359-366, 1989.
H. Jiang. A new perspective on machine learning: How to do perfect supervised learning. In preprint
arXiv:1901.02046, 2019.
K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems 29 (NIPS 2016), 2016.
R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational efficiency of training neural
networks. In preprint arXiv:1410.1141, 2014.
Y. Nesterov. Introductory Lectures on Convex Programming. Springer, 2004.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proceedings of the
34th International Conference on Machine Learning, 2017.
M. A. Pinsky. Introduction to Fourier Analysis and Wavelets. Brooks/Cole Thomson Learning, 2002.
I. Safran and O. Shamir. On the quality of the initial basin in overspecified neural networks. In
Proceedings of the 33rd International Conference on Machine Learning, 2016.
9
Under review as a conference paper at ICLR 2020
I. Safran and O. Shamir. Spurious local minima are common in two-layer ReLU neural networks. In
Proceedings of the 35th International Conference on Machine Learning, 2018.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. In preprint arXiv:1605.08361, 2016.
S. Zhang, H. Jiang, S. Wei, and L. Dai. Rectified linear neural networks with tied-scalar regularization
for LVCSR. In Proc. of Interspeech, 2015.
D. Zou, Y. Cao, D. Zhou, and Q. Gu. Stochastic gradient descent optimizes over-parameterized deep
ReLU networks. In preprint arXiv:1811.08888, 2018.
10