Under review as a conference paper at ICLR 2020
Disentangling Trainability and
Generalization in Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
A fundamental goal in deep learning is the characterization of trainability and
generalization of neural networks as a function of their architecture and hyper-
parameters. In this paper, we discuss these challenging issues in the context of
wide neural networks at large depths where we will see that the situation simpli-
fies considerably. To do this, we leverage recent advances that have separately
shown: (1) that in the wide network limit, random networks before training are
Gaussian Processes governed by a kernel known as the Neural Network Gaussian
Process (NNGP) kernel, (2) that at large depths the spectrum of the NNGP kernel
simplifies considerably and becomes “weakly data-dependent”, and (3) that gra-
dient descent training of wide neural networks is described by a kernel called the
Neural Tangent Kernel (NTK) that is related to the NNGP. Here we show that in
the large depth limit the spectrum of the NTK simplifies in much the same way
as that of the NNGP kernel. By analyzing this spectrum, we arrive at a precise
characterization of trainability and a necessary condition for generalization across
a range of architectures including Fully Connected Networks (FCNs) and Con-
volutional Neural Networks (CNNs). In particular, we find that there are large
regions of hyperparameter space where networks can only memorize the training
set in the sense they reach perfect training accuracy but completely fail to gener-
alize outside the training set, in contrast with several recent results. By comparing
CNNs with- and without-global average pooling, we show that CNNs without av-
erage pooling have very nearly identical learning dynamics to FCNs while CNNs
with pooling contain a correction that alters its generalization performance. We
perform a thorough empirical investigation of these theoretical results and finding
excellent agreement on real datasets.
1	Introduction
Machine learning models based on deep neural networks have attained state-of-the-art performance
across a dizzying array of tasks including vision (Cubuk et al., 2019), speech recognition (Park
et al., 2019), machine translation (Bahdanau et al., 2014), chemical property prediction Gilmer et al.
(2017), diagnosing medical conditions Raghu et al. (2019), and playing games Silver et al. (2018).
Historically, the rampant success of deep learning models has lacked a sturdy theoretical foundation;
architectures, hyperparameters, and learning algorithms are more often than not selected by brute
force search Bergstra & Bengio (2012) and heuristics Glorot & Bengio (2010). Recently, signifi-
cant theoretical progress has been made on several fronts that have shown promise in making neural
network design more systematic. In particular, in the infinite width (or channel) limit, the distribu-
tion of functions induced by neural networks with random weights and biases has been precisely
characterized before, during, and after training.
The study of infinite networks dates back to seminal work by Neal (1994) who showed that the
distribution of functions given by single hidden-layer networks with random weights and biases in
the infinite-width limit are Gaussian Processes (GPs). Recently, there has been renewed interest in
studying random, infinite, networks starting with concurrent work on “conjugate kernels” (Daniely
et al., 2016; Daniely, 2017) and “mean-field theory” (Poole et al., 2016; Schoenholz et al., 2017).
The former set of papers argued that the empirical covariance matrix of pre-activations became
deterministic in the infinite-width limit and called this the conjugate kernel of the network while
the latter papers studied the properties of these limiting kernels along with the kernel describing
1
Under review as a conference paper at ICLR 2020
distribution of gradients. In particular, it was shown that the spectrum of the conjugate kernel of
wide fully-connected networks approached a well-defined, data-independent, limit when the depth
exceeds a certain scale, ξ. Networks with tanh-nonlinearities (among other bounded activations)
exhibit a phase transition between two limiting spectral distributions of the conjugate kernel as a
function of their hyperparameters with ξ diverging at the transition. It was additionally hypothesized
that networks were un-trainable when the conjugate kernel was sufficiently close to its limit.
Since then this analysis has been pushed to a wide range for architectures such as convolutions (Xiao
et al., 2018), recurrent networks (Chen et al., 2018; Gilboa et al., 2019), networks with residual
connections (Yang & Schoenholz, 2017), networks with quantized activations (Blumenfeld et al.,
2019), the spectrum of the fisher (Karakida et al., 2018), a range of activation functions Hayou et al.
(2018), and batch normalization (Yang et al., 2019). In each case, it was observed that the spectra
of the kernels correlated strongly with whether or not the architectures were trainable. While these
papers studied the properties of the conjugate kernels, especially the spectrum in the large-depth
limit, a branch of concurrent work made a stronger statement: that many networks converge to
Gaussian Processes as their width becomes large (Lee et al., 2018; Matthews et al., 2018; Novak
et al., 2019b; Yang, 2019). In this case, the Conjugate Kernel was referred to as the Neural Network
Gaussian Process (NNGP) kernel.
Together this work offered a significant advance to our understanding of wide neural networks;
however, this theoretical progress was limited to networks at initialization or after Bayesian posterior
estimation and provided no link to gradient descent. Moreover, there was some preliminary evidence
that suggested the situation might be more nuanced than the qualitative link between the NNGP
spectrum and trainability might suggest. For example, Philipp et al. (2017) showed that deep fully-
connected tanh-networks could be trained after the kernel reached its large-depth, data-independent,
limit but that these networks did not generalize to unseen data.
In the last year, significant theoretical clarity has been reached regarding the relationship between the
GP prior and the distribution following gradient descent. In particular, Jacot et al. (2018) along with
followup work (Lee et al., 2019; Chizat et al., 2019) showed that the distribution of functions induced
by gradient descent for infinite-width networks is a Gaussian Process with a particular compositional
kernel known as the Neural Tangent Kernel (NTK). In addition to characterizing the distribution over
functions following gradient descent in the wide network limit, the learning dynamics can be solved
analytically throughout optimization.
In this paper, we leverage these developments and revisit the relationship between architecture,
hyperparameters, trainability, and generalization in the large-depth limit for a variety of neural net-
works. In particular, we make the following contributions:
1.	We compute the large-depth asymptotics of several quantities related to trainability, includ-
ing the largest eigenvalue of the NTK, λmaχ, and the condition number K = λmaχ∕λmi∩,
where λmin is the smallest eigenvalue; see Table 1.
2.	We introduce the residual predictor ∆(l), namely the difference between the finite depth
and infinite depth NTK predictions, which is related to the model’s ability to generalize:
the network fails to generalize if ∆(l) is too small.
3.	We show that the ordered and chaotic phases identified in Poole et al. (2016) lead to
markedly different limiting spectra of the NTK, which further indicates that, as a func-
tion of depth, the optimal learning rates ought to decay exponentially, linearly and remain
roughly a constant in the chaotic, order-to-chaos and ordered phases, respectively.
4.	We examine the differences in the above quantities for fully-connected networks (FCNs)
and convolutional networks (CNNs) with and without pooling and precisely characterize
the effect of pooling to these quantities.
5.	We provide substantial experimental evidence supporting these claims, includes experi-
ments that densely vary the hyperparameters of FCNs and CNNs with and without pooling.
Together these results provide a complete, analytically tractable, and dataset-independent theory
for learning in very deep and wide networks. In addition to being interesting in its own right our
theory provides a strong test of the NTK theory. Finally, our results provides clarity regarding the
observation that for linear networks the learning rate must be decreased linearly in the depth of the
2
Under review as a conference paper at ICLR 2020
FC/CNN-F, CNN-P
NTK	Ordered χ1 < 1	Critical χ1 = 1	Chaotic χd > 1
λ(l) max	mq* - mO(lχl1)	md+2 lq* + mO(1)	O(x1)∕d
λ(l) rest	O(lχ1)∕d	3"l + d O(1)	o(χd)∕d
κ(l)	dmq*O(χ-1 /l)	md+2 + dmO(l-1)	→1
∆(l)	O(lχ1)∕d	dO(l-d)	dO(l(Xc*/Xd)l)
Table 1: Evolution of the NTK spectra and ∆(l). The NTKs of FCN and CNN-F are essentially the
same and the scaling of λ(ml)ax, λ(rle)st, κ(l), and ∆(l) for these networks is written in black. Corrections
to these quantities due to the addition of an average pooling layer with window size d is written in
blue.
network Saxe et al. (2013). Here, we note that this is true only for networks that are initialized
critically, i.e. on the order-to-chaos phase boundary.
2	Background
We summarize recent developments in the study of wide random networks. We will keep our dis-
cussion relatively informal; see (Lee et al., 2018; Matthews et al., 2018; Novak et al., 2019b) for a
more rigorous version of these arguments. To simplify this discussion and as a warmup for the main
text, we will consider the case of FCNs. Consider a fully-connected network of depth L where each
layer has a width N (l) and an activation function φ : R → R. In this work we will take φ = erf
however, most of the results will hold for a wide range of non-linearities though specifics - such
as the phase diagram - can vary substantially. For simplicity, we will take the width of the hidden
layers to infinity sequentially: N(1) → ∞, . . . , N (L-1) → ∞. The network is parameterized by
weights and biases that We take to be randomly initialized with wj), b(l) ~ N(0,1) along with
hyperparameters, σw and σb that set the scale of the weights and biases. Letting the pre-activations
in layer l due to an input x be given by zi(l)(x), the network is then described by the recursion,
z(l+1)(x) = √σww X Wjl+1)φ(zjl)(x)) + σbb(l+1)	0 ≤ l ≤ L - 1.	(1)
Notice that as N(l) → ∞, the sum ends up being over a large number of random variables
and we can invoke the central limit theorem to conclude that the {zi(l+1)}i∈[N] are i.i.d. Gaus-
sian with zero mean. Given a dataset of m points, the distribution over pre-activations can
therefore be described completely by the covariance matrix between neurons in different inputs
K(l) (x, x0) = E[zi(l) (x)zi(l) (x0)]. Inspecting Equation 75, we see that K(l+1) (x, x0) can be com-
puted in terms of K(l) (x, x0) as
K(l+1)(X,χ0) = σW E(z,z0)~N (0,K(l)(x,χ0))[φ(Z)φ(Z0)] + σb ≡ σW T(K(I)(X,χ0)) + σ2 .⑵
for T, an appropriately defined operator from the space of positive semi-definite matrices to itself.
Equation 2 describes a dynamical system on positive semi-definite matrices K(X, X0). It was
shown in Poole et al. (2016) that fixed points, K* (χ,χ0), of these dynamics exist such that
liml→∞ K(I)(X,x0) = K*(x,x0) with K*(x,x0) = q*[δχ,χo + c*(1 一 δχ,χo)] independent of the
inputs X and X0 . The values of q* and c* are determined by the hyperparameters, σw and σb . How-
ever Equation 2 admits multiple fixed points (e.g. c* = 0, 1) and the stability of these fixed points
plays a significant role in determining the properties of the network. Generically, there are large
regions of the (σw, σb ) plane in which the fixed-point structure is constant punctuated by curves,
called phase transitions, where the structure changes.
3
Under review as a conference paper at ICLR 2020
The rate at which K(χ, χ0) approaches or departs K (x, x0) can be determined by expanding Equa-
tion 2 about its fixed point, δK(x, x0) = K(x, x0) 一 K*(x, x0) to find1
δK(l+1)(x,x0) ≈ σWT(K*(x,x0))δK(l)(x,x0)	(3)
which exhibits exponential convergence to - or divergence from - the fixed-point as δK(l)(x, x0)〜
χ(x, x0)l where χ(x, x0) = σWT(K*(x, x0)). Since K*(x, x0) does not depend on X or x0 it follows
that χ(χ, χ0) will take on a single value, χc*, whenever X = x0. If χc* > 1 then the fixed point is
unstable and, as discussed above, there will be another fixed point that becomes stable, if χc* < 1
then the fixed point is stable, and if χc* = 1 then the hyperparameters lie at a phase transition.
As was shown in Poole et al. (2016), there is always a fixed-point at C = 1 whose stability is
determined by χ1 . This defines the order-to-chaos transition. Note, that χc* can be used to define
a depth-scale, ξc* = 一1/ log(χc* ) that describes the number of layers over which K(l) approaches
K*.
This provides a precise characterization of the NNGP kernel at large depths. As discussed above, re-
cent work (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019) has connected the prior described
by the NNGP with the result of gradient descent training using a quantity called the NTK. To con-
struct the NTK, suppose we enumerate all the parameters in the fully-connected network described
above by θα. The finite width NTK is defined by ΘΘ(x,x0) = J(X)J(x0)T where Jia(X) = ∂θα ZL(X)
is the Jacobian evaluated at a point X. The main result in Jacot et al. (2018) was to show that in the
infinite-width limit, the NTK converges to a deterministic kernel Θ and remains constant over the
course of training. As such, at a time t during gradient descent training with an MSE loss, the
expected outputs of an infinitely wide network, μt(∕) = E[zL(x)], evolve as
μt(Xtrain) = (Id - e-ηθt*"aint)Yrain	(4)
μt(Xtest) = Θtest,trai∏Θta1n,train(Id 一 e-ηθtrain,∙raint)Y^rain	(5)
for train and test points respectively; see Section 2 in Lee et al. (2019). Here Θtest, train denotes
the NTK between the test inputs Xtest and training inputs Xtrain and Θtrain, train is defined similarly.
Since Θ converges to Θ, the gradient flow dynamics of real network also converge to the dynamics
described by Equation 4 and Equation 5 (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019; Yang,
2019; Arora et al., 2019; Huang & Yau, 2019). As the training time, t tends to infinity we note that
these equations reduce to μ(Xtrain) = Yrain and μ(Xtest) = Θtest,trainΘtra1n,trainY^rain. Consequently
we call the linear operator
P(Θ) ≡ Θtest, train Θt-ra1in, train	(6)
the “mean predictor” or “predictor” for short. In addition to showing that the NTK describes net-
works during gradient descent, Jacot et al. (2018) showed that the NTK could be computed in closed
form in terms of T, T, and the NNGP as,
Θ(l+1)(X, X0) = K(l+1)(X, x0) + σWT(K(I))(x, X0)Θ(l)(X, x0) .	(7)
where Θ(l) is the NTK for the pre-activations at layer-l.
3	Metrics for Trainability and Generalization at Large Depth
We begin by discussing the interplay between the conditioning of Θtrain,train and the trainability of
wide networks. We can write Equation 4 in terms of the spectrum of Θtrain,train letting Θtrain,train =
UTDU as,
μt(Xtrain)i = (Id 一 e-ηλit)Yrain,i	(8)
where λi are the eigenvalues of Θtrajn,trajn and μt (Xtrain) = Uμt (Xtrain), Yrain = UYrain are the
mean prediction and the labels respectively written in the eigenbasis of Θtrain,train. If we order the
eigenvalues such that λo ≥ ∙∙∙ ≥ Xm then it has been hypothesized in e.g. Lee et al. (2019) that the
maximum feasible learning rate scales as η 〜2∕λ° as we verify empirically in section 4. Plugging
1More precisely, one needs to consider the Jacobian ofT as an operator from positive semi-definite matrices
to positive semi-definite matrices. We refer the readers to Section B of Xiao et al. (2018) for more details.
4
Under review as a conference paper at ICLR 2020
this scaling for η into Equation 8 we see that the smallest eigenvalue will converge exponentially at
a rate given by κ = λM /λ0 the conditioning number. It follows that if the conditioning number of
the NTK associated with a neural network diverges then it will become untrainable and so we use
κ as a metric for trainability. We will see that at large depths, the spectrum of Θtrain,train typically
features a single large eigenvalue, λmax, and then a gap that is large compared with the rest of the
spectrum. We therefore will often refer to a typical eigenvalue in the bulk as λrest and approximate
the condition number as K = λmaχ∕λrest.
In the large-depth limit We will see that Θ(l) converges to Θ* independent of the data distribution.
In this case Θ[st,train will be a rank-1 constant matrix. As such, the mean prediction defined by
Equation 5 completely fails to generalize. We define the finite depth correction to the infinite depth
predictor2,
△⑷YTrain ≡ (P(Θ(I))- P(Θ*)) YTrain.	(9)
By the triangle inequality, the generalization error is lower bounded by
kP(Θ(I))Yrain- KeStk2 ≥ kP(Θ*)Yrain — KeStk2 — k∆(I)Yraink2 ∙	(⑼
∣∣P(Θ*)Yτain — KeStIl2 is a constant independent of the test inputs and Equation 10 is large if
∣∣∆(l)YTraink2 is too small. Therefore, a necessary condition for the network to generalize is that
there exists some ρ > 0 such that
k∆(I)YTraink2 ≥ P∣P(Θ*)Y^rai∏ — KeStk2 .	(11)
As such, we use △(l) as a metric for generalization in this paper.
Our goal is therefore to characterize the evolution of the two metrics κ(l) and △(l) in l. We follow
the methodology outlined in Schoenholz et al. (2017); Xiao et al. (2018) to explore the spectrum of
the NTK as a function of depth. We will use this to make precise predictions relating trainability and
generalization to the hyperparameters (σw, σb, l). Our main results are summarized in Table 1 which
describes the evolution of λ(ml)ax (the largest eigenvalue of Θ(l)), λ(rle)st (the remaining eigenvalues),
κ(l), and △(l) in three different phases (ordered, chaotic, and the phase transition) and their depen-
dence on m, the size of the training set, the choices of architectures: FCN, CNN-F (convolution with
flattening) and CNN-P (convolution with pooling), and size, d, of the window in the pooling layer
(which we always take to be the penultimate layer).
We give a brief derivation of these results in Section 4 followed by a more detailed discussion in
the appendix. However, it is useful to first give a qualitative overview of the phenomenology. In the
ordered phase,入《\乂 → m(q* 一 lχ1) and λ([St → lχ1. At large depths since χι < 1 it follows that
K(I) → mq*/(lχ1) and so the condition number diverges exponentially quickly. Thus, in the ordered
phase we expect networks not to be trainable (or, specifically, the rate at which they learn will grow
exponentially in their depth). The predictor scales as lχl1 which goes to zero at the same rate as the
divergence of K(l) ; thus, in the ordered phase networks fail to train and generalize simultaneously.
By contrast in the chaotic phase we see that there is no gap between λ(ml)ax and λr(le)st and networks
become perfectly conditioned and are trainable everywhere. However, in this regime we see that
the predictor scales as l(χc*/χι)l. Since, by definition, in the chaotic phase χc* < 1 and χι > 1
it follows that ∆(l) → 0 over a depth ξ = -1/ log(χc* /χι). In the chaotic phase networks fail to
generalize at a finite depth but remain trainable indefinitely. Finally, notice that introducing pooling
modestly augments the depth over which networks can generalize in the chaotic phase but reduces
the depth in the ordered phase. We will explore all of these predictions in detail in section 5.
4	Large-Depth Asymptotics of the NNGP and NTK
We now give a brief derivation of the results in table 1. To simplify the notation we will discuss
fully-connected networks and then extend the results to CNNs with pooling (CNN-P) and without
pooling (CNN-F). Details of these two cases can be found in the appendix. We will focus on the
2If Θ(l) diverges to infinity, we define P(Θ*) = limι→∞ P(Θ(l)). If。；劭底加 is singular, we will add a
diagonal regularizer σId into Θtrain,train.
5
Under review as a conference paper at ICLR 2020
NTK here since Schoenholz et al. (2017); Xiao et al. (2018) contains a detailed description of the
NNGP in this case. As in sec. 2, we will be concerned with the fixed points of Θ as well as the
linearization of Equation 7 about its fixed point. Recall that the fixed point structure is invariant
within a phase so it suffices to consider the ordered phase, the chaotic phase, and the critical line
separately. In cases where a stable fixed point exists, we will describe how Θ converges to the fixed
point. We will see that in the chaotic phase and on the critical line, Θ has no stable fixed point and in
that case we will describe its divergence. As above, in each case the fixed points of Θ have a simple
structure with Θ* = p*((1 - c*)Id + C*11T). To simplify the forthcoming analysis, without a loss
of generality, We assume the inputs are normalized to have variance q* 3. As such, We can treat T
and T, restricted on {K⑴}ι, as a point-wise functions, since
T(K)(X,x0)= Eφ(u)φ(v),	(u,v)T 〜N(0, K(X*x0) K(χ*x0) ).	(12)
Since the off-diagonal elements approach the same fixed point at the same rate, we use qa(lb) ≡
K(l)(x, x0) and P(alb) ≡ Θ(l)(x, x0) to denote any off diagonal entry of K(l) and Θ(l) respectively. We
will similarly use qa*b and P*ab to denote the limits, liml→∞ qa(lb) = qa*b = c*q* and liml→∞ P(alb) =
Pab = C*P*. Using the above notation, Equation 7 and Equation 2 become
qa(lb+1) = σw2 T(qa(lb)) +σb2
q(l+1) = q*
p(a+1=qab+ι)+σw 亍相)Pa
p(l+1) = q* + σWT(q*) P(Il,
(13)
(14)
where P(l) ≡ Θ(l)(x, x) and q(l) = K(l) (x, x). In what follows, we split the discussion into three
parts according to the values of χι ≡ σ?T(q*) recalling that in Poole et al. (2016); Schoenholz
et al. (2017) it was shown that χ1 controls the fixed point structure.
4.1	THE CHAOTIC PHASE χ1 > 1:
The chaotic phase is so-named because qa*b/q* < 1 so that similar inputs become more uncorrelated
as they pass through the network. In this phase, the diagonal entries of Θ(l) grow exponentially and
the off-diagonal entries converge to a fixed value. Indeed, Equation 14 implies,
P(l+1) = q* + χ1P(l)
P(l) = q*
χl1+1 - 1
χ1 - 1
(15)
which diverges exponentially. To find the limit of the off-diagonal terms, define Xc = σ Tgab)
which was shown to control convergence of the qa(lb) and is always less than 1 (Schoenholz et al.,
2017; Xiao et al., 2018). Let l → ∞ in Equation 13, we find that
Pab=金—< ∞.
(16)
The rate of convergence of P*ab is O(lχlc) (see Section A in the appendix). Since the diagonal terms
diverge and the off-diagonal terms are finite it follows that in very deep networks in the chaotic
phase, (P(l))-1Θ(l) → Id. Thus, in the chaotic phase, the spectrum of the NTK for very deep
networks approaches the diverging constant multiplying the identity. From Equation 4 this implies
that optimization in the chaotic phase should be easy since κ(l) → 1 (provided numerical precision
issues from the prefactor do not become problematic). However, computing the mean prediction on
test points and noticing that P(Θ*)Ytrain = 0 we find (see Section B for the derivation),
∆(l)Ytrain=P(Θ(l))Ytrain≈ (P(l))-1O(lχlc)Ytrain → 0.	(17)
It follows that in the chaotic phase the networks predictions on unseen data to converge to 0 expo-
nentially quickly in the depth. Since Equation 17 decays like O(l(P(l))-1χlc), we expect the network
fails to generalize after O(ξ*) layers, where ξ* = -1/(log χc - logχ1) 3 4.
3It has been observed in previous works Poole et al. (2016); Schoenholz et al. (2017) that the diagonals
converge much faster than the off-diagonals for tanh- or erf- networks.
4For simplicity, we ignore the polynomial correction in l.
6
Under review as a conference paper at ICLR 2020
In summary, for wide networks, in the chaotic phase as the depth increases optimization becomes
increasingly easy but the generalization performance degrades and eventually the network fails com-
Pletely away from the training set after O(ξ*) layers. Therefore, in the chaotic phase, deep network
memorizes the training data. We will confirm this prediction for both kernel prediction and neural
network training in the experimental results; see Fig 3.
4.2	THE Ordered Phase χι = σ3T(q*) < 1:
The ordered phase is defined by the stable fixed point with qafe∕q* = 1; in this case, disparate
inputs will end up converging to the same output at the end of the network. In the ordered phase,
Equation 14 implies that all the diagonal entries of Θ converge to the same value,
p(l) = q* χ1+l -l 1	-→→ p* = q*	< ∞	(18)
χ1 - 1	1 - χ1
However, as with the NNGP kernel, the off-diagonal terms of the NTK, p(alb) , will also converge
to the value on the diagonal, p*. It follows that the limiting kernels have the form Θ* = p* 11T
and K* = q* 11T. Thus, the limiting kernels are highly singular and feature only one non-zero
eigenvalue. Since the limit is singular, we must linearize the dynamics about the fixed point to gain
insight into the limiting behavior of the network. To compute the corrections, let
(alb) =	qa(lb)	-	qa*b	δa(lb)	= p(alb)	- p*ab	(19)
(l) =	q(l)	-	q*	δ(l)	= p(l)	- p*	(20)
The diagonal correction can be obtained directly from Equation 18 and we find that (l) = 0 and
δ(l) = ιχ1-q*. To compute correction of the off-diagonals, we linearize the equation around the
1-χ1
fixed point to find that asymptotically (see Section A),
4b ≈ X胤)	δab ≈ χ1,?+ l (1 + X2Pab) ea,	(21)
where X2 = σω2 T (q*). While the NNGP and NTK feature the same exponential rate of convergence
set by X1, we see that terms in the off-diagonal terms of the NTK feature polynomial corrections.
Θ(l) has (approximately) two eigenspaces. The first eigenspace comes from the single non-zero
eigenvalue at the fixed point and it is very close to the DC mode (i.e. all entries of the eigenvector
are equal to 1) with eigenvalue
*
λ(^aχ ≈ (m - 1)(p* - δ% + (p* - δ(I)) → mp* = —q-	(22)
1 - X1
i.e. is the sum of one row, where m is the size of the dataset. The second eigenspace comes
from lifting the degenerate zero-modes when l < ∞ and it has dimension (m - 1) with eigenvalue
λr(le)st≈-δ(l)+δa(lb) = O(lXl1)→ 0, which goes to zero exponentially over depth l. The eigenvalues
of K(l) have a similar distribution with λ(ml)ax ≈ mq* - (m - 1)(alb) and λ(rle)st = O(Xl1). Thus the
conditioning number, κ(l), of both Θ(l) and K(l) diverges exponentially as O(X1-ll-1) and O(X1-l)
respectively. As discussed above, there is a polynomial correction in the conditioning number of the
NTK that slightly improves its conditioning. Since Θ* is singular, we insert a diagonal regularization
term σId into Θtrain, train of the linear predictor Equation 6, where σ is a positive constant independent
from l and X1. We find ∆(l) = Oσ(lXl1); see Section B for the derivation. In summary, in the
ordered phase, ξ1 = -1∕ log X1 (for simplicity, we ignore the polynomial correction) governs both
trainability and generalizability of the predictor.
4.3	The Critical Line χι = σ3T(q*) = 1
On the critical line both the diagonal and the off-diagonal terms of Θ(l) diverge linearly in the depth
while K(l) converges to q*11T. From Equation 14 we see immediately that the diagonal terms are
given by q(l) = q* and p(l) = lq*. To compute the correction of the off-diagonals, we keep the
7
Under review as a conference paper at ICLR 2020
(a) Chaotic
(b) Ordered	(c) Critical: FCN and CNN-F
(d) Critical: CNN-P
(e) Critical: CNN-F and CNN-P
(f) Ordered: Dropout
Figure 1: Condition numbers of NTKs and their rate of convergence. Different colors represent
images of different size. For example, in the yellow “12-6” , “12” represents the size of the dataset
and “6” represents the dimension number (6 * 6 * 3 for FCNand (6,6,3) for CNN) (a) In the chaotic
phase, κ(l) converges to 1 for all architectures. (b) We plot χl1κ(l), confirming κ explodes with rate
χl1 /l in the ordered phase. In (c) and (d), the dashed lines representing the condition number κ(l)
and solid lines the ratio between first and second eigenvalues. We see that, on the order-to-chaos
transition, these two numbers converge to m++2 and dm+2 (horizontal lines) for FC/CNN-F and
CNN-P respectively. In (e), we plot rates of convergence for CNN-P (solid) and CNN-F (dashed),
confirming that pooling slows down the convergence of κ(l) by a factor of d. (f) Adding dropout to
the penultimate layer prevents κ(l) from divergence in the ordered phase. The legends indicate the
rate of the mask with ρ = 1 meaning keeping all activations. Horizontal lines are the limit of κ(l)
computed in Equation 85 (here m = 20 for all curves.)
definition of (alb) unchanged but define δa(lb) slightly differently to the above as δab = Pa - lq* to
take into account the linear divergence at large depths. Taylor expanding to second order we find,
Aa = - X2 1 + o(1)，现=-2lq*+ O(I)
(23)
Thus for large l, Θ(l) has the following formP(I) = lq* andPa) = 1 lq* + O(1). AS in the ordered
phase, for large l it follows that Θ(l) essentially has two eigenspaces: one has dimension one and
the other has dimension (m - ) with
》m)ax = (m +32)q* l + mO(1),	λ(eSt = 3q*l + O(1)	(24)
and the condition number K(I) = m++2 + mO(l-1) → m+2 as l → ∞. Unlike the chaotic and
ordered phases, κ(l) converges with rate O(l-1). The K(l) has λ(ml)ax = mq* + mO(l-1) and λr(le)st ≈
χ2l-1 and the condition number K(I) diverges linearly with slope mχ2∕2. A similar calculation
gives ∆(l) = O(l-1) on the critical line. In summary, κ(l) converges to a finite number and the
network ought to be trainable for arbitrary depth but the residual predictor ∆(l) decays polynomially,
explaining why critically initialized networks with thousands of layers could still generalize (Xiao
et al., 2018).
8
Under review as a conference paper at ICLR 2020
deptħlo
depth?。
M4	M*	10,	W1	10*	M1	M4	W	M1	W1	M*	M1
peep
Figure 2: Maximal learning rate can be calculated via the λmax. y-axis: accuracy and x-axis:
multiples of ηtheory. Each point on the solid (dashed) lines represents the best training (test) accuracy
throughout training of one configuration. From blue to purple to red, (σω , σb) is moving from the
order phase to the chaotic phase. ρ = 1 is the theoretical prediction.
4.4	Remarks
We end this section with a couple remarks. (1) The above theory holds for CNNs; see Section D.
In the large depth setting, the NTK of CNNs without pooling is essentially the same as the NTK
of FCNs. (2) In the ordered phase, Adding a dropout layer could significantly improve trainability
of a network. For example, adding dropout to the penultimate layer, the condition number κ(l) will
converge to a finite number rather than diverge exponentially; see (f) in Figure 1 and Equation 85 in
the appendix.
5 Experiments
In this section, we provide empirical results to support the theoretical results in Section 4. Figure 1
is generated using synthetic data and all other plots are generated using CIFAR-10 with MSE as the
loss function.
Evolution of κ(l) (Figure 1). We randomly sample inputs with shapes (m, k2 × 3) for FCN and
(m, k, k, 3) for CNN-F/CNN-P, where m ∈ {12, 20} and k ∈ {6, 10}. We compute the exact
NTK with activation function Erf using the Neural Tangents library (Novak et al., 2019a). We see
excellent agreement between the theoretical calculation of κ(l) in Section 4 (summarized in Table 1)
and the experimental results Figure 1.
Maximum Learning Rates (Figure 2 top). In practice, given a set of hyper-parameters of a net-
work, knowing the range of feasible learning rates is extremely valuable. As discussed above,
in the infinite width setting, Equation 4 implies the maximal convergent learning rate is given by
ηtheory ≡ 2∕λ(ll)aχ. We argue that ηtheory is a good prediction for the maximal convergent learning
rate for wide network. To test this statement, we apply SGD to train a collection of fully-connected
networks on CIFAR-10 using 1k training samples with the following configurations: (1) width:
2048 (2) σb = 0.43 fixed, (3) depths: l = 5, 10, 20, 40, (4) 10 different values of σω moving from
the ordered phase (blue) to the chaotic phase (red) (5) 10 different learning rates η = ρηtheory, with
ρ ∈ [10-1, 101]. Overall, we see excellent agreement for depths less or equal to 20 and reasonable
good agreement for depth 40. We point out that the degradation of the agreement for larger depth
may due to the fact that the finite width NTK becomes more stochastic as the ratio between depth
and width increases (Hanin & Nica, 2019). Note that Table 1 tells that, as depth increases, ηtheory
should decays exponentially and linearly in the chaotic and critical phases resp. and remain roughly
a constant in the ordered phase.
Trainability vs Generalization (Figure 3 top). Our theoretical result suggests that in the deep
chaotic regime (χl1 is large) training becomes easier but the network can not generalize. On the other
hand, the network can generalize but training becomes much more difficult as one moves towards
the deep ordered region because κ(l) blows up exponentially. To confirm this claim, we conduct an
experiment using 16k training samples from CIFAR-10 with 20 × 20 different (σω, l) configurations.
We train each network using SGD with batch size b = 1024 and learning rate η = 0.3ηtheory. Deep
in the chaotic phase we see that all configurations reach perfect training accuracy but the network
9
Under review as a conference paper at ICLR 2020
-0.96
-0.84
-0.72
- 0.8
0.48
-0.36
-0.24
一 0.12
0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
威
0450
0405
0360
0315
0370
0225
0.1S0
0.135
OMO
∣-Q5
-。.38
M
，。
8
SQ
49
39
2。
1«
M 1。	1.5 ZQ 2.S S" 3.S	4。
-MlS
-β≡2S
OlSO
-«139


Figure 3: Top: training (left) and test accuracy of FCN using SGD. Bottom: test accuracy of CNN-P,
CNN-F and the difference. In the blue strip, CNN-F significantly outperforms CNN-P, due to the
fact that pooling increases the spectra gap by a factor of d.
completely fails to generalize in the sense test accuracy approaches 10%. However, in the ordered
phase although the training accuracy degrades, generalization improves. The network eventually
becomes Untrainable after O(ξι) layers. In both phases We see that the depth scales, ξι and ξ*
respectively, perfectly capture the transition from generalizing to overfitting.
CNN-P v.s. CNN-F: spatial correction (Figure 3 bottom). We compUte the test accUracy Using the
analytic NTK predictor EqUation 5, Which corresponds to the test accUracy of ensemble of gradient
descent trained neUral netWorks taking the Width to infinity. We choose 1k training points, fix σb2,
and choose 20 × 20 different (σω, l) configUrations. We plot the test performance of CNN-P and
CNN-F and the performance difference in Fig 3. Remarkably, the performance of both CNN-P and
CNN-F are captured by ξι = -1/log(χι) in the ordered phase and by ξ* = -1∕(log ξc - log ξι) in
the chaotic phase. We see that the test performance difference betWeen CNN-P and CNN-F exhibits
a region in the ordered phase (a blue strip) Where CNN-F outperforms CNN-P by a large margin.
This performance difference is due to the correction term d as predicted by the ∆(l)-roW of Table 1.
6 Conclusion and Future work
In this Work, We identify several quantities (λmax, λrest, κ, and ∆(l)) related to the spectrum of the
NTK that control trainability and generalization of deep netWorks. We offer a precise characteriza-
tion of these quantities and provide substantial experimental evidence supporting theoretical results.
In future Work, We Would like to extend our frameWork to other architectures, e.g., ResNet (With
batch-norm), attention model. Understanding the implication of the sub-Fourier modes in the NTK
to the test performance of CNN is also an important research direction.
References
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation With an infinitely Wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research,13(Feb):281-305, 2012.
10
Under review as a conference paper at ICLR 2020
Yaniv Blumenfeld, Dar Gilboa, and Daniel Soudry. A mean field theory of quantized deep networks:
The quantization-depth trade-off. arXiv preprint arXiv:1906.00771, 2019.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field
theory of RNNs: Gating enables signal propagation in recurrent neural networks. In International
Conference on Machine Learning, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
2019.
Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment:
Learning augmentation strategies from data. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems 30. 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, 2016.
Dar Gilboa, Bo Chang, Minmin Chen, Greg Yang, Samuel S. Schoenholz, Ed H. Chi, and Jef-
frey Pennington. Dynamical isometry and a mean field theory of lstms and grus. CoRR,
abs/1901.08987, 2019. URL http://arxiv.org/abs/1901.08987.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference
on Machine Learning - Volume 70, IcMl’17, pp. 1263-1272. JMLR.org, 2017. URL http:
//dl.acm.org/citation.cfm?id=3305381.3305512.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In International Conference on Artificial Intelligence and Statistics, pp. 249-256, 2010.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv
preprint arXiv:1909.05989, 2019.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the selection of initialization and activa-
tion function for deep neural networks. arXiv preprint arXiv:1805.08266, 2018.
Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hier-
archy. arXiv preprint arXiv:1909.08156, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems 31. 2018.
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of fisher information in
deep neural networks: mean field approach. arXiv preprint arXiv:1806.01316, 2018.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-
dickstein. Deep neural networks as gaussian processes. In International Conference on Learning
Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Confer-
ence on Learning Representations, 4 2018. URL https://openreview.net/forum?id=
H1-nGgWC-.
Radford M. Neal. Priors for infinite networks (tech. rep. no. crg-tr-94-1). University of Toronto,
1994.
11
Under review as a conference paper at ICLR 2020
Roman Novak, Lechao Xiao Jaehoon Lee, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Neu-
ral tangents: Fast and easy infinite neural networks in python, 2019a. URL http://github.
com/google/neural-tangents.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abo-
lafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are gaussian processes. In International Conference on Learning Representations,
2019b.
Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and
Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.
arXiv preprint arXiv:1904.08779, 2019.
George Philipp, Dawn Song, and Jaime G Carbonell. The exploding gradient problem demystified-
definition, prevalence, impact, origin, tradeoffs, and solutions. arXiv preprint arXiv:1712.05577,
2017.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-
nential expressivity in deep neural networks through transient chaos. In Advances In Neural
Information Processing Systems,pp. 3360-3368, 2016.
Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding
transfer learning with applications to medical imaging. arXiv preprint arXiv:1902.07208, 2019.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Si-
monyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess,
shogi, and go through self-play. Science, 362(6419):1140-1144, 2018. ISSN 0036-8075. doi:
10.1126/science.aar6404. URL https://science.sciencemag.org/content/362/
6419/1140.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convo-
lutional neural networks. In International Conference on Machine Learning, 2018.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances
in Neural Information Processing Systems. 2017.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S Schoenholz. A
mean field theory of batch normalization. arXiv preprint arXiv:1902.08129, 2019.
12
Under review as a conference paper at ICLR 2020
A Signal propagation of NNGP and NTK
Recall that
qa(lb+1) = σw2T(qa(lb)) +σb2
q(l+1) = q*
p(a+1=qaιb+1)+σw 亍础)Pa
(25)
P(I+1) = q* + σWT(q*) P(I),	(26)
A.1 Correction of the off-diagonals in the chaotic/ordered phase
Applying Taylor’s expansion to the first equation of 25 gives
qa*b + (alb+1) = σω2 T (qa*b + (alb)) + σb2
=σω τ (qtab)+σ+σ 中⑹/用+。(4的
= qab+σω 于%思+。(―
WithXc =叱T(q*J, We have
(l+1)
ab
χcab
Similarly, applying Taylor’s expansion to the second equation of 25 gives
δa(lb+1) ≈ (1 +χc,2P*ab)(alb+1) +χcδa(lb)
・	C 二，，、一.	.	一.
where χc,2 =。方^T(q*b). This implies
(l)
ab
(0)
ab
χc,2
χc
(a0b)
(27)
(28)
(29)
(30)
(31)
(32)
(33)



Note that δa(lb) contains a polynomial correction term and decays like lχlc . The correction to the fixed
points in the ordered phase could be obtained using the same calculation:
(l)
ab
χl1
(0)
ab
，ab ≈ xiR?+1 (ι+χ2P*) -a?
(34)
(35)

A.2 Correction of the off-diagonals on the critical line.
We have χ1 = 1 on the critical line. We need to expand the first equation of 25 to the second order
√l+1)—冲,ɪ °(l)2 JOr#3、
Cab = Cab + 2 χ2eab + O(Cab )
Here we assume T has a continuous third derivative. The above equation implies
(l)
ab
21	1
-X2 7 + o( 7).
Then
δalb+1) = qj) - q* + σ T(q* + eaib>alb - lq*
≈ C<ab1 + (XI + X2C(ab + 2X3(Ca?)2)(Iq* + ^) - lq*
≈ ea+1)+(ι+χ2caι)δaab+iq*χ2cab)+2 χ3 (CalJ))2lq*
Plugging Equation 37 into the above equation gives
δab = - 2 Iq* + O(1)
(36)
(37)
(38)
(39)
(40)
(41)
13
Under review as a conference paper at ICLR 2020
A.3 Relu
We only consider the critical initialization σω2 = 2 and σb2 = 0. Using the equations in Appendix C
of (Lee et al., 2019) gives
σ2 T(1-e) = 1-e +2√2 e3/2 + O(e5/2)	(42)
3π
and taking the derivative w.r.t.
4 T(1-e) = 1-√2 e1/2 + O(e3/2)	(43)
ωπ
This is enough to conclude (similar to the above calculation)
≡alb) = ( √2)2l-2 + o(l-2)	(44)
δalb) = - 31 + O(1).	(45)
Recall that q(l) = 1 and p(l) = l for Relu network with σω2 = 2. and σb = 0. Therefore
λmax = m +3 l + mO ⑴，λ(eSt = 31 + O ⑴ KQ)= m +3 + mO(1∕l)∙	(46)
A.4 Residual Relu
We consider the following “continuum” residual network
x(t+dt) = X㈤ + (dt)1/2 (Wφ(x㈤)+ b)	(47)
where t denotes the number of layer and dt > 0 is sufficiently small and W and b are the weights
and biases. We also set σω2 = 2 (i.e. E[WWT] = 2Id) and σb2 = 0 (i.e. b = 0). The NNGP and
NTK have the following form
K(t+dt) = K(t) + 2dtT (K(t))	(48)
Θ(t+dt) = Θ㈤ + dtK⑴+ 2dtT(K⑴)Θ⑴	(49)
Taking the limit dt → 0 gives
K(t) =2T (K(t))	(50)
Θ⑴=K⑴ + 2T(K^)Θ⑶	(51)
Using the fact that q0 = 1 (i.e. the inputs have unit variance), we can compute the diagonal terms
q(t) = et and p(t) = tet. Let qa(tb) = etc(atb), applying the above fractional Taylor expansion to T and
z-j- 1
T, we have
卷=- Tl)3 +O((1i% 5)	(52)
Solving this gives
qat) = (1- 9∏21-2 + o(t-2))et	(53)
P「11 + O(1))et.	(54)
Thus the condition number is m/3 + 1. This is the same as the non-residual Relu case discussed
above.
B ASYMPTOTIC OF ∆(l)
To keep the notation simple, we denote Xd = Xtrain, Yd = Ytrain, Θtd = Θtest,train, Θdd = Θtrain, train.
Recall that
△(IM = (P(Θ(I))- P(Θ*)) Yd = (θ(d (θdld)-1 - Θ* (eld)-] Yd	(55)
We split our calculation into three parts.
14
Under review as a conference paper at ICLR 2020
B.1 Chaotic phase
In this case the diagonal p(l) diverges exponentially and the off-diagonals p(alb) converges to a
bounded constantpafe. Thus P(Θ*)Yd = 0. We expand Θ(l) about its fixe point
∆(l)Yd = Θt(dl) Θ(dld)-1Yd	(56)
=(θtd + O(δa?)) G(I)Id+ PIb(IIT - Id) + O(δab))-1 Yd	(57)
=(P(I))T (θ着	+ O(δab))	(Id-	PI(11t -Id) +	O(δa?/P(I)))	Yd	(58)
=(P(I))T (θ着	+ O(δab))	(Id-	PI(11t -Id) +	O(δa?/P(I)))	Yd	(59)
=(P(I))T (θ(δ% + 0或/P(I))) Yd	(60)
In the last equation, We have used the fact 11tYd = 0 and Θ*dYd = 0 since Yd is balanced (i.e.
containing the same number of positive (+1) and negative (-1) labels.) Therefore
△⑷YdO((P(I))Tδa?) = O(i(χc∕χι)1).	(61)
B.2 Order-to-chaos phase
Note that in this phase, both the diagonals and the off-diagonals diverge linearly. In this case
li 1 Θ(I) — 11 1T li 1 Θ(I) — B — 2Id 11 .1T	62、
1→∞ 石θtd = 31t1d 1→∞ 历θdd = B ≡ 3Id+ 31d1d	(62)
Here We use 1d to denote the all ‘1’ (column) vector With length equal to the number of training
points in Xd and 1t is defined similarly. Note that the constant matrix B is invertible. By Equation 41
p(e(I))=3 (谭啸)(5 θdld)
=3 (1t1T + O(1∕lq*)) (B + O(1∕lq*))-1
=3 (1t1T + O(1∕lq*))(B-1 + O(1∕lq*))
= 3itiT B-1 + O(i∕iq*)
=lim P(Θ(I)) + O(1∕lq*)
l→∞
Thus
△(l)Yd =O(1∕lq∙)
Ordered Phase In this case Θ* is a rank one matrix. We add a diagonal regularization term and
defined
Pσ(Θ) = Θtd (Θdd + σId)-1	(69)
Where σ > 0 is a positive constant independent of the hyper-parameters (σw , σb, l). Let Bσ =
Θ* + σId. Then
(63)
(64)
(65)
(66)
(67)
(68)
Pσ(Θ(l))	= (θtd +。磷))(Bσ + O(δab) ))-1	(70)
	=Θ*B-1 + Oσ 成)	(71)
	=Pσ (Θ*) + Oσ (δ%	(72)
15
Under review as a conference paper at ICLR 2020
C Dropout
In this section, we investigate the effect of adding a dropout layer to the penultimate layer. Let
0 < ρ ≤ 1 and γj(L) (x) be iid random variables
(L)	1,	with probability ρ
γj	x 0,	with probability 1 - ρ.
For 0 ≤ l ≤ L - 1,
z(l+1)(x) = √σ=) X Wjl+1)Φ(Zjl)(x))+ σbbil+1)
and for the output layer,
N(L)
Z(L+1)(X) = p√wL) X WjL+1)。(ZjL)(X))YjL)(X) + σbb(L+1)
(73)
(74)
(75)
where Wi(jl) and bi(l) are iid Gaussians N(0, 1). Since no dropout is applied in the first L layers,
the NNGP kernel K(l) and Θ(l) can be computed using Equation 2 and Equation 7. Let Kρ(L+1) and
Θ(ρL+1) denote the NNGP and NTK of the (L+1)-th layer. Note that when ρ = 1, K1(L+1) = K(L+1)
and Θ(1L+1) = Θ(L+1) . We will compute the correction induced by ρ < 1. The fact
E[ (L)(X) (L)(X0)] = ρ2, if (j, X) 6= (i,X0)	(76)
E[γj (X)γi (X)] = ρ, if (j,X)=(i,X0)	(76)
implies that the NNGP kernel Kρ(L+1) (Schoenholz et al., 2017) is
(W T(K(L)(X,x0))+ σ2, if X = x0
Kρ(L+1)(X,X0)≡E[Zi(L+1)(X)Zi(L+1)(X0)] =	(77)
[ρσwT(K(L)(x,x))+σ if X=X0.
Now we compute the NTK Θ(ρL+1), which is a sum of two terms
Θ(ρL+1)(X,X0) = E
∂z(L+I)(X) ∂∂z(L+1)(X0)!T
∂θ(L+1)	∂θ(L+1)
+E
∂z(L+I)(X) ∂∂z(L+1)(X0) !T
∂θ(≤L)	∂θ(≤L)
(78)
Here θ(L+1) denote the parameters in the (L + 1) layer, namely, Wi(jL+1) and bi(L+1) and θ(≤L) the
remaining parameters. Note that the first term is in Equation 78 is equal to Kρ(L+1)(X, X0). Using the
chain rule, the second term is equal to
_σω_E 'X w(L+I)W(L+Dφ(z(L)(X))〃(L)(X)φ(z(L)(XO))、(L)(XO)dzj__(x) ddzk_(X)!
p2N(L) E	2 Wj Wik	φ(Zj(X))Yj(X)φ(zk	(X	))γj	(X )	∂θ(≤L)	I	∂θ(≤L)
j,k=1
(79)
σω2 雨 N^ \( (L)/ ʌʌ (L)/、口(L)，0、、(L)，0∖dzj')(X) (dzj')(XO八	z2m
=ρ2NωL) E T φ(Zj J(X))YjJ(X)φ(ZjJ(X ))γj J(X ) ∂θ(≤L) ( ∂θ(≤L) J	(80)
=σ2E，jL)(X)YjL)(X0)i E[φ(ZjL)(X))φ(ZjL)(X0))]E jL) (⅛⅛?!	(81)
(σ22 T(K(L)(X,xo))Θ(L)(X,xo) if X = xo
=	(82)
(Pσ±T(K(L)(X,x))Θ(L)(X,x) if x = xo .
16
Under review as a conference paper at ICLR 2020
In sum, we see that dropout only modifies the diagonal
(θPL+1)(x, x0) = Θ(L+1)(x, x0)
[θPL+1)(x,x) = P Θ(L+1)(x,x) + (1 - 1∕ρ)σ2
In the ordered phase, we see
lim θPL)(x,x0) = p*,	lim θPL)(x,x) = ɪp* + (1 — 4)σ2
L→∞	L→∞	ρ	ρ
and the condition number
].	(L) (m - I)p* + Pp* + (I - P)σb	mp*	ɪ 1
lim κ∖	+ I
L→∞ P	(P-1)(p*-σ2)	(P-1)(p*-2) +
(83)
(84)
(85)
D Convolutions
General setup. For simplicity of presentation we consider 1D convolutional networks with circular
padding as in Xiao et al. (2018). We will see that this reduces to the fully-connected case introduced
above if the image size is set to one and as such we will see that many of the same concepts and
equations carry over schematically from the fully-connected case. The theory of two-dimensional
convolutions proceeds identically but with more indices.
Random weights and biases. The parameters of the network are the convolutional filters and biases,
ωj β and μi, respectively, with outgoing (incoming) channel index i (j) and filter relative spatial
location β ∈ [±k] ≡ {-k, . . . , 0, . . . , k}.5 As above, we will assume a Gaussian prior on both the
filter weights and biases,
Wij,β =	/, ω == ωij,β	bi = σbμi,	ωij,β, μi 〜N(0, I) (86)
(2k + 1)nl
As above, σω2 and σb2 are hyperparameters that control the variance of the weights and biases respec-
tively. nl is the number of channels (filters) in layer l, 2k + 1 is the filter size.
Inputs, pre-activations, and activations. Let X denote a set of input images. The network has
activations yl(x) and pre-activations zl(x) for each input image x ∈ X ⊂ Rn0d, with input channel
count n0 ∈ N, number of pixels d ∈ N, where
xiα	l = 0	n k
yi,α(X)=	φ (zl-1(χ))	l> 0 ,	zi,α(X) ≡ ΣL Wij,βyj,α+β(X) + bi.	(87)
i,α	j=1 β=-k
φ : R → R is a point-wise activation function. Since we assume circular padding for all the
convolutional layers, the spacial size d remains constant throughout the networks until the readout
layer.
For each l > 0, as min{n1 . . . , nl-1} → ∞, for each i ∈ N, the pre-activation converges in distri-
bution to d-dimensional Gaussian with mean 0 and covariance matrix K(l), which can be computed
recursively (Novak et al., 2019b; Xiao et al., 2018)
K(l+1) = AoT(K(I)) = (AoT )l+1(K0)	(88)
Here K(l) = [K(αl,)α0 (X, X0)]α,α0 ∈[d],x,x0 ∈X, T is a non-linear transformation related to its fully-
connected counterpart, and A a convolution coupled with a shift term acting on Xd × Xd PSD
matrices
[T(K)]α,α0 (x,x0) = IEu〜N(0,K) [φ(uɑ(x)) Φ (Ua(x0))]	(89)
[A (K)]a,a0 (x, x0) = σ2 + σω X 2k1+∣ [K]a+β,a0 + β (x, x') ∙	(90)
________________________________ β
5We will use Roman letters to index channels and Greek letters for spatial location. We use letters i, j, i0 , j0,
etc to denote channel indices, α, α0, etc to denote spatial indices and β, β0, etc for filter indices.
17
Under review as a conference paper at ICLR 2020
D.1 The Neural Tangent Kernel
To understand how the neural tangent kernel evolves with depth, we define the NTK of the l-th
hidden layer to be ΘΘ(l)
θθ α)α0 (X,xO) = vθ≤l zi,α(X)Vθ≤l zi,α0 (XO)	(91)
where θ≤l denotes all of the parameters in layers at-or-above the l’th layer. It does not matter which
channel index i is used because as the number of channels approach infinity, this kernel will also
converge in distribution to a deterministic kernel Θ(l+1) (Yang, 2019), which can also be computed
recursively in a similar manner to the NTK for fully-connected networks as,
Θ(l+1) = K(l+1) + A。(T(K(I)) Θ Θ(l)) - σ2,	(92)
where T is given by Equation 89 with φ replaced by its derivative φO. We will also normalize the
variance of the inputs to q* and hence treat T and T as PointWise functions. We Will only present
the treatment in the chaotic phase to showcase how to deal with the operator A. The treatment of
other phases are similar. Note that the diagonal entries of K(l) and Θ(l) are exactly the same as
the fully-connected setting. We only need to consider the off-diagonal terms. Letting l → ∞ in
Equation 92 we see that all the off-diagonal terms also converge . Let A = σ-2(A 一 σ2), be the
normalized convolution operator. Note that A does not mix terms from different diagonals and it
suffices to handle each off-diagonal separately. Let (alb) and δa(lb) denote the correction of the j-th
diagonal of K(l) and Θ(l) to the fixed points. Linearizing Equation 88 and Equation 92 gives
≡al+1) ≈ XcAealb)	(93)
sat+1) ≈ XcAd+1)+F Pabealb)+διab).	网
χc
Next let {ρa}a be the eigenvalues of A and e，？ a and δalb+1) be the projection of €a” and 6)? onto
the α-th eigenvector of A. Then for each α,
e(alb+,α1) ≈ (ραXc)(l+1)e(a0b),α	(95)
δab,α ≈ ραχc(ealb+c1)+Xc2 Pabeaza+δalbQ)	(96)
Xc
Which gives
δalb) ≈ PaXceab,a, δ^,a ≈ PIOXc δ^,a + l (1 + 芋Pab) e^a	(97)
Therefore, the correction Θ(l) - Θ* propagates independently through different Fourier modes.
In each mode, up to a scaling factor Pa, the correction is the same as the correction of its FC
counterpart. Since the subdominant modes (With |Pa| < 1) decay exponentially faster than the
dominant mode (With Pa = 1), for large depth, the NTK of CNN is essentially the same as that of
its FC counterpart.
D.2 The effect of pooling and flattening of CNNs
With the bulk of the theory in hand, We noW turn our attention to CNNs. We shoW in the appendix
that the dominant mode in CNNs behaves exactly like the fully-connected case, hoWever We Will see
that the readout can significantly affect the spectrum. The NNGP and NTK of the l-th hidden layer
CNN are 4D tensors K(al,)a0 (X, XO) and Θ(al,)a0 (X, XO), Where α, αO ∈ [d] ≡ [0, 1, . . . , d - 1] denote the
pixel locations. To perform tasks like image classification or regression, “flattening” and “pooling”
(more precisely, global average pooling) are tWo popular readout strategies that transform the last
convolution layer into the logit layer. The former strategy “flattens” an image of size (d, N) into a
vector in RdN and stacks a fully-connected layer on top. The latter projects the (d, N) image into
a vector of dimension N via averaging out the spatial dimension and then stacks a fully-connected
layer on top. The actions of “flattening” and “pooling” on the image correspond to computing the
18
Under review as a conference paper at ICLR 2020
mean of the trace and the mean of the pixel-to-pixel covariance on the NNGP/NTK, respectively,
i.e.,
θfl2tten (X,x1 = d X θ0l)α(X,x0), θP'0ol (X,xO) = ( X。几 (x,x')	(98)
α∈[d]	α,α0 ∈[d]
where Θ(flla)tten (Θ(plo)ol) denotes the NTK right after flattening (pooling) the last convolution. We will
also use Θf(cl) to denote the NTK of FC. Kfl(la)tten and K(plo)ol are defined similarly.
As discussed above, in the large depth setting, all the diagonals Θ(αl,)α(X, X) = p(l) (since the in-
puts are normalized to have variance q* for each pixel) and similar to θf?, all the off-diagonals
Θ?) a(x, x0) are almost equal (in the sense they have the same order of correction to Pab if exists.)
Without loss of generality, we assume all off-diagonals are the same and equal to p(alb) (the leading
correction of qa(lb) for CNN and FCN are of the same order.) Applying flattening and pooling, the
NTKs become
θflιatten(χ, XO)=d x θα)α(χ, XO)=1χ=χoOp(I)+1χ=χo Pa,	(99)
α
θP'0oi(x,XO)=d12 X θ2)ao(X,xO)=d 1χ=χ0 (P(I)- pαb)+Palb)	(IOO)
α,α0
respectively. As we can see, Θf(tl) is essentially the same as its FCN counterpart Θf(cl) up to sub-
dominant Fourier modes which decay exponentially faster than the dominant Fourier modes. There-
fore the spectrum properties of Θf(tl) and Θf(cl) are essentially the same for large l.
However, pooling alters the NTK/NNGP spectrum in an interesting way. On the critical line, asymp-
totically,入马；乂 ≈ (md + 2)q*l∕(3d) and 1也 ≈ 2q*l∕(3d), and Ka) = mfd+2 + mdO(l-1). Here
we use blue color to indicate the changes of such quantities against their Θ(flla)tten counterpart. Thus
pooling decreases λ(rle)st roughly by a factor of d and increases the condition number by a factor of d
comparing to flattening. In the chaotic phase, pooling does not change the off-diagonals qa(lb) = O(1)
but does slow down the growth of the diagonals by a factor of d, i.e. P(I) = O(χ1∕d). This SUg-
gests, in the chaotic phase, there exists a transient regime of depths, where CNN-F hardly perform
while CNN-P performs well. In the ordered phase, the pooling does not affect λ(ml)ax much but does
decrease λrest by a factor of d and the condition number grows approximately like dχ1-l, d times
bigger than its flattening and fully-connected network counterparts. This suggests the existence of a
transient regime of depths, in which CNN-F outperforms CNN-P. This might be surprising because
it is commonly believed CNN-P usually outperforms CNN-F.
19