Under review as a conference paper at ICLR 2020

STABILITY  AND  CONVERGENCE  THEORY  FOR  LEARN-
ING  RESNET: A FULL  CHARACTERIZATION

Anonymous authors

Paper under double-blind review

ABSTRACT

ResNet structure has achieved great success since its debut.  In this paper, we
study the stability of learning ResNet. Specifically, we consider the ResNet block
hl  =  φ(hl−₁ + τ  · g(hl−₁)) where φ(·) is ReLU activation and τ  is√a  scalar.

We show that for standard initialization used in practice,  τ  =  1/Ω(   L) is a

sharp value in characterizing the stability of forward/backward process of ResNet,
where L is the number of residual blocks. Specifically, stability is guaranteed for

1

τ  ≤ 1/Ω(   L) while conversely forward process explodes when τ  > L− 2 +c for a

positive const√ant c. Moreover, if ResNet is properly over-parameterized, we show

for τ  ≤ 1/Ω˜(   L) gradient descent is guaranteed to find the global minima ¹, which

significantly enlarges the range of τ      1/Ω˜(L) that admits global convergence in

previous work. We also demonstrate that the over-parameterization requirement
of ResNet only weakly depends on the depth, which corroborates the a√dvantage

of ResNet over vanilla feedforward network. Empirically, with τ  ≤ 1/   L, deep

ResNet√can be easily trained even without normalization layer. Moreover, adding

τ  = 1/   L can also improve the performance of ResNet with normalization layer.

1    INTRODUCTION

Residual Network (ResNet) has achieved great success in computer vision tasks since the seminal
paper (He et al., 2016). Moreover, the ResNet structure has also been extended to natural language
processing and achieved the state-of-the-art performance (Vaswani et al., 2017; Devlin et al., 
2018).
In      this paper, we study the forward/backward stability and convergence theory of learning 
ResNet.

Specifically, we consider the ResNet with the following residual block,

hl = φ(hl−₁ + τ W lhl−₁),                                                     (1)

where φ( ) is the ReLU activation, hl is the output of layer l, W l is the parameter of layer l and 
τ
is a scale factor on the parametric branch in a residual block.  We note that standard 
initialization
schemes, e.g., the Kaiming’s initialization, are designed to keep the forward and backward variance
constant when passing through one layer.  However, things become different for ResNet.  If W l
adopts Kaiming’s initialization, a small τ is necessary for a stable forward process of ResNet, 
because
the output explodes in expectation for τ  = 1 when L gets large. On the other side, a limit form of
Euler’s constant indicates that τ  = 1/Ω(L) is sufficient for the forward stability as shown in 
previous
work (Allen-Zhu et al., 2019; Du et al., 2019). It is natural to ask

“Are there other values of τ that can guarantee the stability of ResNet with arbitrary depth?”

We target the above question and unveil that τ  = 1/√L is a sharp value in terms of characterizing
the stability of forward/backward process of ResNet with a non-asymptotic analysis. Specifically,

1

stability is guaranteed for τ  ≤ 1/Ω(   L). Conversely, for τ  > L− 2 +c the network output grows 
at

least with rate Lᶜ in expectation, which implies forward/backward explosion for large L.

One step further, based on the stability argument, we show that if the network is properly over-

param√eterized, gradient descent is guaranteed to find global minima for training ResNet with τ  ≤

1/Ω˜(   L), where the range of τ is significantly enlarged compared to the result in Allen-Zhu et 
al.

1We use Ω˜(·) to hide logarithmic factor.

1


Under review as a conference paper at ICLR 2020

(2018b); Du et al. (2018) with τ       1/Ω(L).  Over-parameterization has been recently used as a
hammer to tackle the optimization property (Allen-Zhu et al., 2018b; Du et al., 2018; Zou et al.,
2018; Zou and Gu, 2019) of neural network. It considers the case when the neural network is very
wide at each layer, which enables the theoretical analysis via the statistical concentration 
property of
the parameter matrix. We show that the over-parameterization requirement of ResNet only weakly
depends on the depth, which justifies the advantage of ResNet over feedforward network.  Our
contribution can be summarized as follows.

We establish a non-asymptotic analysis showing that τ  = 1/√L is tight in the order sense
for characterizing the stability of ResNet.

For τ      1/Ω(√L log m), we establish the convergence of gradient descent to global minima

for learning over-parameterized ResNet with arbitrary depth.

The key step to prove o√ur first claim is a new bound of the spectral norm of the forw√ard process 
for

ResNet with τ  ≤ 1/Ω(   L). This bound is a bit surprising as a natural bound (1 + 1/   L)L 
explodes.

We use martingale theory to characterize the l√argest possible change after multiple residual 
mappings,

which is shown to be bounded given τ  < 1/   L. This technique may be of independent interest for
other problems.

The idea for proving global convergence is as follows.  First we establish the forward/backward
stability at the initialization and derive the gradient upper/lower bounds by utilizing the 
statistical
concentration of random matrix.  Then we show the gradient bounds do not change much after
perturbation as long as the perturbation is relatively small. Finally by properly choosing the step 
size,
gradient descent update indeed produces small enough perturbation. Besides the forward/backward
stability, the key step to show our global convergence is a new gradient upper bound which is 
tighter
than previous works by exploiting the scaling factor τ .  This new upper bound enables the weak
depth-dependent argument of learning ResNet.

We note that although the global convergence of gradient descent for learning ResNet has been
established by Allen-Zhu et al. (2018b); Du et al. (2018) for τ  =  1/Ω˜(L), if we train practical

ResNet with τ  = 1/L, the performance is largely worse than ResNet with normalization layer that is
used in practice. In contrast, our stability and co√nvergence theory can serve as a guide for 
practice.

Empirically, we demonstrate that with τ  = 1/   L, ResNet can be effectively trained without the

normalization layers. Moreover, even with normalization layer, deep ResNet does not perform well
in prac√tice.  We illustrate the reason from the stability perspective and demonstrate that adding

τ  = 1/   L on top of the normalization layer can obtain considerable performance improvement.

1.1    RELATED WORKS

A  concu√rrent  work  Arpit  et  al.  (2019)  considers  a  similar  residual  mapping  and  
suggests  that

τ   =  1/   L  can  keep  the  norm  of  the  forward/backward  pass  roughly  constant  in  
expectation.

In sharp contrast, we consider the standard initialization scheme used in practice and prov√ide 
rigorous

non-asymptotic analysis for the stability for the forward/backward pass with τ  = 1/Ω(   L). Zhang

et al. (2019a) demonstrates that ResNet can b√e trained without normalization layer if the first 
mapping

in a residual block is initializ√ed  down by 1/   L and the last mapping is initialized to 0. In 
contrast,

our result shows that τ  = 1/   L is sufficient for training a standard ResNet without 
normalization

layer and achieve better empirical performance than Zhang et al. (2019a). Zhang et al. (2019b) 
argues
that a small τ can give robust ResNet from a partial differential equation interpretation of ResNet 
but
does not provide how small τ should be. Moreover, our paper is also related to the work that have
studied the benefit of ResNet (Veit et al., 2016; Zhang et al., 2018; Hardt and Ma, 2016; Orhan and
Pitkow, 2018).

Our paper is closely related to recent work on over-parameterization/neural tanget kernel (NTK)
technique.  The NTK approach considers the case when the neural network is very wide (often
more neurons than samples) at each layer and the training iterates fall into a small region near the
initialization. Jacot et al. (2018); Allen-Zhu et al. (2018b); Du et al. (2018); Chizat and Bach 
(2018);
Zou et al. (2018); Zou and Gu (2019); Arora et al. (2019a); Oymak and Soltanolkotabi (2019) showed
that gradient descent converges linearly to the global minima for training over-parameterized deep
neural network from the optimization perspective.  Brutzkus et al. (2017); Li and Liang (2018);

2


Under review as a conference paper at ICLR 2020

Allen-Zhu et al. (2018a); Arora et al. (2019b); Cao and Gu (2019); Neyshabur et al. (2019) establish
the generalization properties of over-parameterized neural network.  On the other side, Ghorbani
et al. (2019); Chizat et al. (2019); Yehudai and Shamir (2019); Allen-Zhu and Li (2019) discuss the
limitation of the NTK approach in characterizing the behavior of neural network. The “limitation”
lies     in that kernel approach cannot approximate a single ReLU neuron efficiently and the 
empirical
generalization gap between the NTK approach and neural network, which does not underrate our
contribution as we focus on the convergence behavior of gradient descent. Moreover, our stability
result     on the range of τ holds beyond the NTK regime.

1.2    PAPER ORGANIZATION

The rest of this paper is organized as follows. Section 2 introduces the model and notations. 
Section 3
gives a non-asymptotic analysis on the forward/backward stability of ResNet for a full range of τ .
Section 4 presents the global convergence of gradient descent for training over-parameterized 
ResNet,
including the proof roadmap. Section 5 gives some experiments that support our theory. Finally, we
conclude in Section 6.

2    PRELIMINARIES

There are many residual network models since the seminal paper (He et al., 2016). Here we study a
simple ResNet model with Kaiming’s initialization (He et al., 2016) is described as follows²,

•  Input layer: h₀ = φ(Ax);

•  L − 1 residual layers: hl = φ(hl−₁ + τ W lhl−₁);

•  A fully-connected layer: hL = φ(W LhL−₁);

•  Output layer: y = BhL;

•  Initialization: A ∈ Rm×p, B ∈ Rd×m and W l ∈ Rm×m for l = [L] where entries of A, B
and W l are independently sampled from N(0,  ² ), N(0, ² ) and N(0,  ² ), respectively;

where φ( ) is the ReLU activation function φ( ) := max  0,    .  Specifically, we assume the input
dimension is p and hence x     Rp, the intermediate layers have the same width m, and hence hl    Rm
for l  =  0, 1, ..., L, and the output has dimension d and hence y      Rd.  Denote the values 
before
activation by g₀  = Ax, gl = hl  ₁ + τ W lhl  ₁ for l  = 1, 2, ..., L     1 and gL = W LhL  ₁.  Use
hi,l and gi,l to denote the value of hl and gl, respectively, when the input vector is xi, and Di,l 
the
diagonal sign matrix where [Di,l]k,k = 1{₍gi,l)k ≥0}.

We  introduce  a  notation  −W→  :=  (W ₁, W ₂, . . . , W L)  to  represent  all  the  trainable  
parameters.

Throughout the paper, we use ǁvǁ to denote the l₂ norm of the vector v. We further use ǁM ǁ₂ and

ǁM ǁF to denote the spectral norm and the Frobenius norm of the matrix M , respectively. Denote

ǁW ǁ₂ := maxl∈[L] ǁW lǁ₂ and ǁW [L−₁]ǁ₂ := maxl∈[L−₁] ǁW lǁ₂.

The training data set is {(xi, yi∗)}n    , where xi is the feature vector and yi∗  is the target 
signal for all

i = 1, ..., n. We consider the objective function is

n

F (W ) :=      Fi(W ),    where  Fi(W ) := l(Bhi,L, yi∗),

i=1

where l( ) is the loss function that measures the discrepancy between the the network output and
the target signal. The model is trained by running the gradient descent algorithm. Though ReLU is
nonsmooth, we abuse the word “gradient” to represent the value computed through back-propagation.

3    FORWARD  AND  BACKWARD  STABILITY  OF  RESNET

In this section, we establish the stability of training ResNet.  We show that when τ       1/Ω(√L)

the forward and backward pass is bounded at the initialization and after small perturbation. On the

² The same ResNet model has been used in Allen-Zhu et al. (2018b) and Du et al. (2018). Here, we 
borrow
notations from Allen-Zhu et al. (2018b).

3


Under review as a conference paper at ICLR 2020

converse side, if τ  > L−⁰.⁵⁺ᶜ for any positive constant c, the output norm grows at least 
polynomial
with depth. The stability result has good correspondence with empirical observation. Moreover, it
forms the basis for establishing the global convergence in Section 4.

3.1    FORWARD PROCESS IS BOUNDED FOR τ  ≤ 1/Ω(√L)

We first give a non-asymptotic bound on the spectral norm of the forward process at initialization.
Theorem  1.  Suppose  that  −W→(0),  A  are  randomly  generated  as  in  the  initialization  
step,  and
D₀,         . . . , DL are diagonal matrices such that ǁDlǁ₂  ≤  1 for all l  ∈  [L] and Dl is 
deterministic

given {W      :  a  ≤  l}.  If τ  ≤  1/Ω(   L), then there exists some small constant c such that 
with
probability at least 1 − L² · exp(−Ω(mc²)) over the initialization randomness we have for any b > 
a,

¨Db .I + τ W ⁽⁰⁾Σ Db−₁ · · · Dₐ .I + τ W ⁽⁰⁾Σ¨   ≤ 1 + c.                          (2)

ǁ(I + τ W ⁽⁰⁾) · · · (I + τ W ⁽⁰⁾)ǁ ≤ (1 + √1   )L explodes. Here the intuition is that the 
cross-product

term concentrates on the mean 0 because of the independent randomness of matrices W ⁽⁰⁾. Moreover,
we note that to guarantee Theorem 1 holds for all training samples [n], we take the union bound and
the probability becomes 1     nL²   exp(   Ω(mc²)). Next we give a rigorous argument based on the
martingale sequence.

Proof Outline.  Suppose we have ǁhₐ−₁ǁ = 1.  Then we abuse notations gl = hl−₁ + τ W ⁽⁰⁾hl−₁

and hl = Dlgl for a ≤ l ≤ b, and we have


₂        ǁhbǁ2                 ǁhₐǁ2

₂         ǁgbǁ2

ǁgₐǁ2                      2


ǁhbǁ

=

ǁhb−1

ǁ2  · · · ǁh

a−1

ǁ2 ǁhₐ−₁ǁ

≤  ǁh

b−1

ǁ2  · · · ǁh

a−1

ǁ2 ǁhₐ−₁ǁ  .

Taking logarithm at both side, we have

log ǁh  ǁ2  ≤ Σ log ∆ ,         where ∆  :=      g        .

2

		

If let h˜l−1  :=    ʰl−1    , then we obtain that

log ∆l = log .1 + 2τ .h˜l−1, W ⁽⁰⁾h˜l−1Σ + τ ²ǁW ⁽⁰⁾h˜l−1ǁ2Σ ≤ 2τ .h˜l−1, W ⁽⁰⁾h˜l−1Σ + τ ²ǁW 
⁽⁰⁾h˜l−1ǁ2,

where the inequality is because log(1 + x) < x for x > −1. Let ξl := 2τ .h˜l−1, W ⁽⁰⁾h˜l−1Σ and

ζ  := τ ²ǁW ⁽⁰⁾h˜      ǁ2, for given h˜      , ξ  ∼ N .0, ⁸τ2 Σ, ζ  ∼  2τ 2 χ² .

Without rigor, we could say Σb      ξ   ∼  N .0, ⁸⁽ᵇ−ᵃ⁾τ2 Σ and Σb      ζ   ∼  2(b−a)τ 2 χ² .  
Hence we

have Σb      log ∆l ≤ c with probability at least 1 − exp(−Ω(m)c²). Taking ε-net argument, we can

establish the spectral norm bound for all vector hₐ  ₁. Let a and b vary from 1 to L     1 and 
taking

the union bound gives the claim.

We next show that the output norm at each layer is close to 1.

Theorem 2.  Suppose τ      1/Ω(√L). There exists some small constant c such that with probability
at least 1     O(nL)   e−Ω⁽ᵐ·ᵐⁱⁿ⁽ᶜ,ᶜ2 ))  over the randomness of A     Rm×p and −W→(0)        
(Rm×m)L
the following holds

∀i ∈ [n], l ∈ {0, 1, . . . , L} :    hi,l      ∈ [1 − c, 1 + c].                                 
(3)

The proof is relegated to Appendix C.1. We note that Theorem 2 is much stronger compared with the
result in Allen-Zhu et al. (2018b) which is only showed for the case τ  = 1/Ω(L log m). Moreover

the above two lemmas also holds for −W→ that is within the neighborhood of −W→(0)  and the result 
is

presented in Appendix C.2.

In the sequel, c is treated as a fixed constant, e.g. c = 0.1, which may be hidden in the Ω() or 
O()

notation.

4


Under review as a conference paper at ICLR 2020

3.2    BACKWARD PROCESS IS BOUNDED FOR τ  ≤ 1/Ω(√L)

For ResNet, the gradient with respect to the parameter is computed through back-propagation, e.g.,

∂W l = ∂hl · hT   , where ∂· represents the gradient of the objective with respect to ·. Therefore, 
the

gradient upper bound is guaranteed if hl and ∂hl are bounded across layers and iterations. We next
show the backward process is bounded for each individual sample.

Theorem 3.  With probability at least 1 − (nL) · exp(−Ω(m)) over the randomness of −W→(0), A, B,
it satisfies for every l ∈ [L − 1], every i ∈ [n], and every −W→ with ǁ−W→ − −W→(0)ǁ₂ ≤ ω for ω ∈ 
[0, 1],

ǁ∇      F (−W→)ǁ2   ≤ O . Fi(−W→) × τ ²mΣ ,         ǁ∇       F (−W→)ǁ2   ≤ O . Fi(−W→) × mΣ .    
(4)

The full proof is relegated to Appendix D.1. Here we give an outline.

Proof Outline.  The argument is based on the bounded forward/backward process at −W→  and the
back-propagation formula. For each i ∈ [n] and l ∈ [L − 1], i.e., the residual layers, we have

ǁ∇W Fi(W−→)ǁF = τ .Di,l(I + τ W l₊₁)T · · · Di,L−₁W T Di,LBT (Bhi,L − y∗)Σ hT −₁


l                                                                                                   
                              L

≤ O(τ √m/d)√Fi(−W→),

i          i,l

where the last inequality is due to that the forward/backward process is bounded for all the −W→ 
such
that ǁ−W→ − −W→(0)ǁ₂ ≤ ω (Lemma 3 in Appendix).

This gradient upper bound indicates that the gradient of residual layers could be much (τ  < 1) 
smaller
than the usual feedfoward layer.

3.3    A CONVERSE RESULT FOR τ  > 1/Ω(√L)

We have built the stability of the forward/backward process fo√r τ  ≤ 1/Ω(√L). We next establish a

converse result showing that if τ is slightly larger than 1/Ω(   L), the network output norm√grows

uncontrollably as the depth L increases, which justifies the tightness of the value τ  = 1/Ω(   L) 
for
arbitrary L.

Theorem 4.  For the ResNet defined and initialized as in Section 2, if τ  ≥ L− 2 +c, then in 
expectation

EǁhLǁ2  > L²ᶜ.                                                             (5)

Proof.  The proof is relegated to the supplemental material in Appendix G.

This indicates the value of τ  = 1/Ω(√L) is tight for characterizing the forward stability of 
learning
ResNet.  We note that the theoretical results in Section 3 hold for very mild condition i.e., high
probabiltiy is obtained when m  >  Ω(log(nL)).  In the next section, we will show that gradient
descent is guaranteed to find the g√lobal minima for training ResNet if the network is properly

over-parameterized when τ  ≤ 1/Ω(   L).

Up to now, we have provided a full characterization of the ResNet stability in terms of the value of
τ .  Next, we take one step further towards bridging the theory to practice.  We study whether this
theoretical result has any practical guide. In practice, instead of using τ , the normalization 
layers, e.g.
batch normalization (BN) and layer normalization (LN), are used to control the forward/backward
stability. One firs√t guide is that ResNet can be effectively trained even without normalization 
layer if

choosing τ  = 1/   L, as shown in the experiments in Section 5.

Our experiments demonstrate that the training performance of ResNet with τ  is on par with the
ResNet with BN for all depths {20, 32, 56, 110, 1202} on CIFAR10 classification task.  The test
performance is a bit complicated: ResNets with τ drop 1 ∼ 2 points compared to ResNet with BN
for depths {20, 32, 56, 110} , which may be attributed to the regularization effect of BN (Zhang et 
al.,

5


Under review as a conference paper at ICLR 2020

2019a); for depth 1202, ResNet with τ is slightly better than ResNet with BN. This motivates us to
study the potential benefit of combining BN and τ .

We observe that although the residual block output norm of ResNet with BN does not increase
exponentially, it still increases as traversing through layers. We give an heuristic estimation on 
how
the   residual block output norm grows for ResNet with BN. Take the image classification task as an

example, the input is normalized across channel and batch. Therefore, for (g˜l)k = BN ((W lhl  ₁)k 
),
we assume that Var[(g˜l)k] = 1 for all k = 1, ..., m and l = 0, 1, ..., L     1. We further assume 
the
independence of each (g˜l)k. Then, we have the following estimation.

Claim 1.  The output for ResNet with BN grows with EǁhLǁ2   ≈ nmL, where hL = [hL₁,     , hLn].

The calculation can be adapted from the p√roof of Theorem 4. This indicates th√at the output norm 
of

ResNet with BN grows roughly at the rate    L. We propose that adding τ  = 1/   L on top of BN can

keep the output norm roughly constant across layers, which produces considerable performance gain
especially for deep ResNet.

4    GRADIENT  DESCENT  CONVERGES  TO  GLOBAL  MINIMA  FOR  LEARNING

OVER-PARAMETERIZED  RESNET

In this section, we establish that gradi√ent descent can converge to global minima for learning 
over-

parameterized ResNet with τ  ≤ 1/Ω(   L). Compared to the recent work (Allen-Zhu et al., 2018b),

our result enlarges the region of τ that admits the global convergence of gradient descent. 
Moreover,
our result also theoretically justifies the advantage of ResNet over vanilla feedforward network in
terms of facilitating the convergence of gradient descent. Before stating the theorem, we introduce 
a
common assumption on the training data (Allen-Zhu et al., 2018b; Zou and Gu, 2019; Oymak and
Soltanolkotabi, 2019).

Assumption 1 (training data).  For any xi, it holds that ǁxiǁ = 1 and (xi)p = 1/√2. For every pair

i, j ∈ [n], we assume ǁxi − xjǁ ≥ δ.

We further assume that the loss function l( ,  ) is quadratic and the objective function for 
individual
sample becomes Fi(−W→) :=  ¹ ǁBhi,L − yi∗ǁ2.

Theorem 5.  Suppose that the ResNet is defined as in Section 2 with τ  ≤  1/Ω(√L log m) and

training data satisfy Assumption 1.  If the network width m  ≥  Ω(n⁸L⁷δ−⁴d log² m), then with

probability at least 1 − exp(−Ω(log² m)), gradient descent with learning rate η = Θ(  ᵈ  ) finds a

point F (−W→) ≤ ε in T  = Ω(n²δ−¹ log ⁿ ˡᵒᵍ2 m ) iterations.

Proof.  The proof is deferred to Appendix F.

This t√heorem establishes the linear convergence of gradient descent for learning ResNet with τ  ≤

1/Ω(   L log m). Compared with Allen-Zhu et al. (2018b); Du e√t al. (2018), our result 
significantly

enlarges the range of τ  (from τ      1/Ω(L log m) to τ      1/Ω(   L log m)) that can guarantee the
global convergence of training over-parameterized ResNet.

Moreover, we argue that even for τ  =  1/Ω(√L log m), the depth dependence for learning over-

parameterized ResNet is smaller than that for learning feedforward network, which is also better 
than
the bound for ResNet given in previous work (Allen-Zhu et al., 2018b). This theoretically justifies
the advantage of ResNet over vanilla feedforward network in terms of facilitating the convergence of
gradient descent. For the case of τ      1/Ω(L log m), the depth dependence might be further reduced
at   the cost of adding the order on n. We believe that the depth dependence of ResNet is due to the
limit of bounding techniques when handling nonsmooth activation. Better bounding technique or
smooth activation√function may help remove such dependence. We leave this for future study. For the

case of τ  > 1/Ω(   L), the global convergence cannot be guaranteed because of the instability of 
the

forward process for extremely large depth as shown in Section 3.3.

6


Under review as a conference paper at ICLR 2020

4.1    PROOF SKETCH AND MAIN CHALLENGES

Though the objective is nonconvex and nonsmooth from the first sight, it admits good optimization
property, which can guarantee the convergence of gradient descent, at the initialization and along 
the
optimization path. Specifically, we establish three properties in the neighborhood of 
initialization:
gradient  is upper bounded, gradient is lower bounded, i.e., the gradient is large when the 
objective is
large, and the objective satisfies certain smooth property.

The gradient upper and lower bounds for each individual sample have been proved based on the
stability of forward/backward process at the initialization stage and after small perturbation in 
Section

3.2. Then following Allen-Zhu et al. (2018b); Zou and Gu (2019), we argue the sum of individual
gradients also being large. The gradient lower bound is presented as Theorem 7 in Appendix D.2.

The  next  challenge  is  to  establish  the  objective  smoothness.    We  present  an  informal  
semi-
smoothness³  result.   Interestingly,  from  this  result,  we  can  illustrate  the  advantage  of 
 learning
ResNet: the convergence of learning ResNet only depends on the network depth weakly, echoing the
empirical evidence that deep ResNet is much easier to train than deep feedforward network.

Theorem 6 (Informal semi-smoothness result).  Let ω < 1 and τ ²L ≤ 1. With high probability, we

have for every −→˘   ∈ (Rm×m)L with ǁW˘ L −W ⁽⁰⁾ǁ₂ ≤ ω and ǁW˘ l −W ⁽⁰⁾ǁ₂ ≤ τω for l ∈ [L−1],

and for every −W→′  ∈ (Rm×m)L with ǁW ′Lǁ₂ ≤ ω and ǁW l′ ǁ₂ ≤ τω for l ∈ [L − 1], we have


−→˘

−→′

−→˘

−→˘

−→′

. nm Σ

−→′  ₂

.. mnLω²/3

4/3 Σ

−→′

√    −→˘

We note that apart from the second-order term (the third term on the right hand side) in classical
smoothness, the semi-smoothness has a first-order term (the last term on the right hand side). One
can      see that as m becomes large in the over-parameterization regime the effect of the 
first-order
term becomes small comparing to the second-order term.  Interestingly, if one replaces W ′L  and
W ′l  with the gradient upper bounds in Theorem 3, only the first-order term depends on L while
the second-order term, which is dominant when m is large, is depth-independent. We note that the
first-order term is the only source where the depth dependence in Theorem 5 comes from, which
renders the depth dependence is weak for learning ResNet when m is large, in contrast with the case
of learning deep feedforward network (Allen-Zhu et al., 2018b).

5    EMPIRICAL  STUDY

In this section, we present some experiments to verify our theory. We first demonstrate that τ  = 
1/√L
is a sharp value in determining the trainability of deep ResNet. We then show that practical ResNet
with τ  can be efficiently trained even without normalization layer.  We finally show for ResNet
with normalization, that adding τ also achieve considerable performance gain for both CIFAR and
ImageNet tasks.

5.1    THEORETICAL VERIFICATION

We train feedforward fully-connected neural networks and ResNets with different values of τ , and
compare their convergence behaviors.  The feedforward model adopts the same architecture and
initialization scheme as the ResNet model except the skip connection (see Section 2). The models
are generated with width m  =  128 and depth L        3, 10, 30, 100, 500, 1000   and .  We conduct
experiments with τ  =  ¹ ,    ¹   ,    ¹     and show how it affects the training performance.  We 
use

L   L0.5     L0.25

MNIST dataset (LeCun et al., 1998) and do the classification task. We train the model with SGD ⁴

and the size of minibatch is 256. The learning rate lr is set as lr = 0.001 without heavily tuning.

Experiment results. We plot the training curves of feedforward network and ResNet with varying
depths and widths in Figure 1.  We see that both τ  =  ¹  and τ  =     ¹    are able to train very 
deep

L                    L0.5

ResNets successfully. However, when τ  =     ¹    , the training loss explodes for models with 
depth 30

³The smoothness is compromised from the usual sense because of the non-smoothness of ReLU.

⁴GD exhibits the same phenomenon. We use SGD due to the expensive per-iteration cost of GD.

7


Under review as a conference paper at ICLR 2020


and more. This indicates that the bound τ  =   ¹

L

is sharp for learning ResNet with arbitrary depth.

Moreover the convergence of ResNets with τ  =  ¹  and τ  =     ¹    do not depend on the depth much

L                   L0.5

while training feedforward network becomes harder as the depth increases, which verifies our theory
of weak dependence on depth for learning ResNet.


2.4

2.2

2.0

1.8

1.6

1.4

1.2

1.0

0.8

0.6

0.4

0.2

0.0

depth_3
depth_10
depth_30
depth_100
depth_500
depth_1000

0           5000       10000     15000     20000

training steps

depth_3
depth_10
depth_30
depth_100
depth_500
depth_1000

0           5000       10000     15000     20000

training steps

depth_3
depth_10
depth_30
depth_100
depth_500
depth_1000

0           5000       10000     15000     20000

training steps

depth_3
depth_10
depth_30
depth_100
depth_500
depth_1000

0           5000       10000     15000     20000

training steps

Figure 1: Training curves for feedforward network and ResNets with τ  =  ¹ ,    ¹    and    ¹     
(from


left to right).

L   L0.5

L0.25

5.2    LEARN RESNET WITH τ

We use more experiments to demonstrate that ResNet can be trained efficiently with τ even when
there is not normalization layer.

In this section we conduct experiments on CIFAR10/100 datasets. We use the ResNet models (He
et al., 2016) with the same hyperparameters but remove all the normalization layers. Motivated by
Zhang et al. (2019a), we add learnable scalar bias at the in√put of each convolution layer. 
Moreover,

we treat τ  as a learnable parameter with initialization 1/   L.  Following Zhang et al. (2019a), 
the

learning rate of scalar parameters is divided by 10.  All blocks share the same τ  and we use the
averaged gradient to update τ . Figure 2 shows the training/validation curves.


2.00

CIFAR10

CIFAR10

4.0

CIFAR100

CIFAR100


1.75

1.50

1.25

1.00

0.75

0.50

0.25

0.00

depth_20

        

depth_32

          depth_56

          depth_110

depth_1202

0           50         100        150        200

Epoch

90

80

70

60                                           depth_20

          depth_32

50                                           depth_56

40                                           depth_110

          depth_1202

30

0           50         100        150        200

Epoch

3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.0

depth_20

        

depth_32

          depth_56

          depth_110

depth_1202

0           50         100        150        200

Epoch

70

60

50

40

          depth_20

30                                           depth_32

          depth_56

20                                           depth_110

10                                           depth_1202

0           50         100        150        200

Epoch


Figure 2: Experiment results on CIFAR10/100 with τ initialized as   ¹

L

. All normalization layers are


removed.

With τ initialized as   ¹

L

, we can easily train ResNet without batch normalization (BN) even for very

deep networks. We note that Zhang et al. (2019a) has also proposed a way to train ResNet without
normalization layers by doing specific initialization strategy (scaling down the first weight matrix
and zeroing the last weight matrix inside all residual blocks). We compare the performances of our τ
ResNet strategy and the Fixup scheme in Table 1. From Table 1, we see that scaling down the output
of residual block has better performance than scaling down the initialization. Moreover, the Fixup
scheme fails to converge 2 out of 5 runs for training ResNet1202 while there is no failure case for
our τ  ResNet. This indicates that the Fixup scheme could be unstable for extremely deep ResNet.
The possible reason is that τ can stabilize both the forward pass and the backward pass while 
scaling
down initialization only stabilizes the forward pass.

We also conduct experiments on Transformer (Vaswani et al., 2017) for machine translation task
and compare our τ and the Fixup scheme Zhang et al. (2019a). Transformer uses multiple residual
connections in its basic building block. Therefore multiplying τ after each residual connection can
also stabilize its training process. The results is relegated to Appendix H.

8


Under review as a conference paper at ICLR 2020


Dataset

CIFAR 10

Depth
20

32

56

110

1202

ResNet + FixupInit
8.72(±0.26)

7.99(±0.24)

7.45(±0.37)

7.24(±0.12)

7.83(±0.18)

ResNet + τ

8.39(±0.11)

7.68(±0.10)

7.07(±0.16)

6.52(±0.20)

6.08(±0.21)

Table 1: Top1 validation error on CIFAR10. Numbers are average of 5 runs except ResNet1202 +
Fixup (standard deviations are given inside the bracket).


Epoch=0

18                   ResNet1202+BN

16                   ResNet1202+BN+

            ResNet1202+

14

            ResNet1202

12

10

8

6

4

2

0

0               50             100           150           200

number of blocks

Epoch=50

18                                         ResNet1202+BN

16                                         ResNet1202+BN+

            ResNet1202+

14

12

10

8

6

4

2

0

0               50             100           150           200

number of blocks

Epoch=150

18                                         ResNet1202+BN

16                                         ResNet1202+BN+

            ResNet1202+

14

12

10

8

6

4

2

0

0               50             100           150           200

number of blocks

Figure 3: Output norm of ResNet1202 over layers at different epochs. X axis is the index of block. Y
axis is the output norm ratio compared to the first block.

5.3    ADD τ  ON TOP OF NORMALIZATION

In this section, we investigate the property of ResNet model with normalization layer and 
empirically
demonstrate that adding a τ on top of normalization can achieve even better performance.

We first illustrate how the output norm of each residual block grows for ResNet1202 with BN (He
et al., 2016; Ioffe and Szegedy, 2015√) in Figure 3. We see that at epoch 0 (initialization stage), 
the

output norm grows almost at the rate    l as we estimate in Claim 1. After training, the estimation 
in

Claim 1 is not as accurate as the initialization because the independence assumption does not hold
after training.  Nonetheless, by adding a τ  on top of BN, the output norm keeps roughly constant
across the forward layers and across the training epochs.

We next verify that adding τ can further improve the performance over original models.

Experiments on image classification.  We use the standard classification datasets:  CIFAR10/100
and ImageNet.  Our models and hyperparameters are the same as in He et al. (2016).  The only


modification is multiplying a fixed τ  =   ¹

L

at the output of each residual block (right before the

residual addition). For ResNet1202, we do not use small learning rate to warm up the training. The
validation errors on CIFAR10/100 are illustrated in Figure 4, where all numbers are averaged over
five runs. We note that the benefit of τ becomes larger when the network is deeper. Without τ , the
performance of ResNet1202 on CIFAR10 is worse than ResNet110 as shown in He et al. (2016). As
depth increases, the norm grow and imbalance over layers hurts the performance, which covers up
the benefit of adding layers. As shown in Figure 4, ResNets+BN+τ keep the output norm roughly
constant and make good use of the network depth.

The ResNet model for ImageNet has different numbers of residual blocks in each stage, and we


choose L to represent the largest number of blocks over all stages. We choose τ  =   ²

L

for ImageNet

dataset. All models are trained for 200 epochs with learning rate divided by 10 every 60 epochs. The
other hyperparameters are the same as in He et al. (2016).

Table 2 shows the results on ImageNet.  We can see that by just adding a τ  on top of BN we can
achieve considerable performance gain. We note that a trick in Goyal et al. (2017) that initializes 
the
scaling factor γ of the last BN of each residual block to 0, shares the same spirit as a small τ 
here,
but does not have principled justification.

9


Under review as a conference paper at ICLR 2020


ResNet+BN
ResNet+BN+

8

6

4

2

0

20           32           56          110        1202

depth

30

25

20

15

10

5              ResNet+BN
ResNet+BN+

0

20           32           56          110        1202

depth

Figure 4: Top1 validation error on CIFAR10/CIFAR100 dataset. The benefit of τ becomes larger as
network goes deeper.


Model
ResNet50

ResNet101
ResNet152

Method

+BN

+BN+τ

+BN

+BN+τ

+BN

+BN+τ

Validation Error (Top1)
23.6

23.0

22.0

21.4

21.7

20.9

Table 2: Validation error on ImageNet dataset.

6    CONCLUSION

In this paper, we provide a√non-asymptotic analysis on the forward/backward stability for ResNet,

which unveils that τ  = 1/   L is a sharp value in terms of characterizing the stability. 
Furthermore,

when the network is properly over-para√meterized, we show that gradient descent finds global minima

for  training  ResNet  with  τ   ≤  1/Ω(   L log m)  which  greatly  improves  over  previous  work 
 of

τ  ≤ 1/Ω(L log m). We also bridge theoretical understanding an√d practical guide of ResNet 
structure.

We empirically verify the efficacy of the suggestion τ  =  1/   L for ResNet with/without batch

normalization. One interesting future direction is to bypass the semi-smooth argument and give a
sharp dependence on the depth and the number of training samples.

10


Under review as a conference paper at ICLR 2020

REFERENCES

Zeyuan Allen-Zhu and Yuanzhi Li.  What can resnet learn efficiently, going beyond kernels?  In 
Advances in
Neural Information Processing Systems (NeurIPS), 2019.

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized 
neural
networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via 
over-parameterization.

arXiv preprint arXiv:1811.03962, 2018b.

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural 
networks.
In Advances in Neural Information Processing Systems (NeurIPS), 2019.

Sanjeev Arora,  Simon S Du,  Wei Hu,  Zhiyuan Li,  Ruslan Salakhutdinov,  and Ruosong Wang.   On 
exact
computation with an infinitely wide neural net.  In Advances in Neural Information Processing 
Systems
(NeurIPS), 2019a.

Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of 
optimization and
generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 
2019b.

Devansh Arpit, Victor Campos, and Yoshua Bengio. How to initialize your network? robust 
initialization for
weightnorm & resnets. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

Alon Brutzkus,  Amir Globerson,  Eran Malach,  and Shai Shalev-Shwartz.   Sgd learns 
over-parameterized
networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 
2017.

Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning 
over-parameterized deep
relu networks. arXiv preprint arXiv:1902.01384, 2019.

Le´na¨ıc Chizat and Francis Bach. On the global convergence of gradient descent for 
over-parameterized models
using optimal transport. In Advances in Neural Information Processing Systems 31. 2018.

Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. 
In Advances
in Neural Information Processing Systems (NeurIPS), 2019.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep 
bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global 
minima of
deep neural networks. arXiv preprint arXiv:1811.03804, 2018.

Simon S. Du,  Xiyu Zhai,  Barnabas Poczos,  and Aarti Singh.   Gradient descent provably optimizes 
over-
parameterized neural networks. In International Conference on Learning Representations, 2019.

Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy 
training of
two-layers neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew 
Tulloch,
Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv 
preprint
arXiv:1706.02677, 2017.

Moritz Hardt and Tengyu Ma.  Identity matters in deep learning.  In International Conference on 
Learning
Representations, 2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image 
recognition. In

The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.

Sergey Ioffe and Christian Szegedy.  Batch normalization:  Accelerating deep network training by 
reducing
internal covariate shift. In International Conference on Machine Learning (ICML), pages 448–456, 
2015.

Arthur Jacot, Franck Gabriel, and Cle´ment Hongler. Neural tangent kernel: Convergence and 
generalization in
neural networks. In Advances in neural information processing systems, pages 8571–8580, 2018.

Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model 
selection. Annals
of Statistics, pages 1302–1338, 2000.

Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to 
document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

11


Under review as a conference paper at ICLR 2020

Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient 
descent on
structured data. In Advances in Neural Information Processing Systems, pages 8168–8177, 2018.

Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro.  The role of 
over-
parametrization in generalization of neural networks. In International Conference on Learning 
Representa-
tions, 2019.

A Emin Orhan and Xaq Pitkow.  Skip connections eliminate singularities.  In International 
Conference on
Learning Representations (ICLR), 2018.

Samet Oymak and Mahdi Soltanolkotabi.  Overparameterized nonlinear learning: Gradient descent takes 
the
shortest path?  In International Conference on Machine Learning (ICML), 2019.

Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm 
usually
takes polynomial time. Journal of the ACM (JACM), 51(3):385–463, 2004.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz 
Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing 
systems, 2017.

Andreas Veit, Michael J Wilber, and Serge Belongie.  Residual networks behave like ensembles of 
relatively
shallow networks. In Advances in Neural Information Processing Systems (NIPS), pages 550–558, 2016.

R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed Sensing, 
Theory
and Applications, pages 210 – 268, 2012.

Gilad Yehudai and Ohad Shamir.  On the power and limitations of random features for understanding 
neural
networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without 
normalization.
In International Conference on Learning Representations (ICLR), 2019a.

Huishuai Zhang, Wei Chen, and Tie-Yan Liu. On the local hessian in back-propagation. In Advances in 
Neural
Information Processing Systems (NeurIPS), pages 6521–6531. 2018.

Jingfeng Zhang, Bo Han, Laura Wynter, Kian Hsiang Low, and Mohan Kankanhalli. Towards robust 
resnet: A
small step but a giant leap. In International Joint Conferences on Artificial Intelligence (IJCAI), 
2019b.

Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural 
networks. In

Advances in Neural Information Processing Systems (NeurIPS), 2019.

Difan  Zou,  Yuan  Cao,  Dongruo  Zhou,  and  Quanquan  Gu.    Stochastic  gradient  descent  
optimizes  over-
parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.

12


Under review as a conference paper at ICLR 2020

A    USEFUL  LEMMAS

First we list several useful bounds on Gaussian distribution.

Lemma 1.  Suppose X ∼ N(0, σ²), then


P{|X| ≤ x} ≥ 1 − exp .    x

2

− 2σ2      ,

(6)

P{|X| ≤ x} ≤ . 2 x.                                                                  (7)

Another bound is on the spectral norm of random matrix (Vershynin, 2012, Corollary 5.35).

Lemma 2.  Let A ∈ RN ×n, and entries of A are independent standard Gaussian random variables.

Then for every t ≥ 0, with probability at least 1 − exp(−t²/2) one has

smₐₓ(A) ≤ √N + √n + t,                                                    (8)

where smₐₓ(A) are the largest singular value of A.

B    SPECTRAL  NORM  BOUND  AT  INITIALIZATION

Next we present a spectral norm bound related to the forward process of ResNet with τ .

Theorem  1.  Suppose  that  −W→(0),  A  are  randomly  generated  as  in  the  initialization  
step,  and
D₀, . . . , DL are diagonal matrices such that ǁDlǁ₂  ≤  1 for all l  ∈  [L] and Dl is deterministic
given {W      :  a  ≤  l}.  If τ  ≤  1/Ω(   L), then there exists some small constant c such that 
with
probability at least 1 − L² · exp(−Ω(mc²)) over the initialization randomness we have for any b > 
a,

¨Db .I + τ W ⁽⁰⁾Σ Db−₁ · · · Dₐ .I + τ W ⁽⁰⁾Σ¨   ≤ 1 + c.                          (2)

Proof.  We first show for any vector hₐ  ₁  with   hₐ  ₁    =  1, we have   hb       1 + c with high
probability, where

hb = Db(I + τ W ⁽⁰⁾)Db−₁ · · · Dₐ(I + τ W ⁽⁰⁾)hₐ−₁.                              (9)

Using notations gl = hl  ₁ + τ Wlhl  ₁ introduced in Section 2, we have   gl        hl  . Thus we 
have
the following


₂        ǁhbǁ2

ǁhₐǁ2

₂         ǁgbǁ2

ǁgₐǁ2                      2


ǁhbǁ

=

ǁhb−1

ǁ2  · · · ǁh

a−1

ǁ2 ǁhₐ−₁ǁ

≤  ǁh

b−1

ǁ2  · · · ǁh

a−1

ǁ2 ǁhₐ−₁ǁ  .

Taking logarithm at both side, we have

log ǁh  ǁ2  ≤ Σ log ∆ ,         where ∆  :=      g        .                              (10)

2

		

If let h˜       :=    ʰl−1    , then we obtain that

ǁhl−₁ǁ

log ∆l = log .1 + 2τ .h˜l−1, W ⁽⁰⁾h˜l−1Σ + τ ²ǁW ⁽⁰⁾h˜l−1ǁ2Σ

≤ 2τ .h˜l−1, W ⁽⁰⁾h˜l−1Σ + τ ²ǁW ⁽⁰⁾h˜l−1ǁ2,

w.here  the  inequaliΣty  is  due  to  the  fact  log(1  +  x)   ≤    x  for  all  x   >   −1.    
Le.t  ξl   :=Σ

						   	 

ζ  ∼  2τ 2 χ² .


We see that

b

P

l=a

log ∆l ≥ cΣ ≤ P

b
l=a

ξ  ≥  c Σ + P

b
l=a

ζ  ≥  c Σ

.                         (11)

13


Under review as a conference paper at ICLR 2020

Next we bound terms on the right hand side one by one. For the first term we have


b

P

l=a

ξ  ≥  c Σ

= P .

exp

.   Σl=a

ξlΣ

≥ exp

. λc ΣΣ

≤ E Σ

exp

.   Σl=a

ξl −

λc ΣΣ

,      (12)

where λ is any positive number and the last inequality uses the Markov’s inequality. Moreover,


E Σexp

.   Σl=a

ξlΣΣ

= E Σ

exp .λ

b−1

l=a

ξlΣ

E [exp (λξb)]  Fb−₁Σ


= exp

. 8τ ²λ² Σ E

Σexp .λ

b−1

l=a

ξlΣΣ

(13)


Hence we obtain

= · · · = exp

. 8τ ²λ²(b − a + 1) Σ


b

P

l=a

ξ  ≥  c Σ

≤ exp

mc²

− 128τ 2(b − a + 1)

= exp

.−Ω

mc²

τ 2(b − a + 1)

(14)


by choosing λ =    ᵐᶜ   and τ  ≤ 1/Ω(√L). Due to the symmetry of Σb

ξl, the conclusion can be

generalized to the quantity | Σb      ξl|.

Then, for the second term, we first present a concentration inequality for the general χ²   
distribution

X (Laurent and Massart, 2000, Lemma 1)


Then for Σb

2 

P (|X − m| ≥ u) ≤ e− 4m .                                                   (15)

ζl, by applying the above concentration inequality and Jensen’s inequality, we have


b

P

l=a

ζ  ≥  c Σ

= E ΣP

Σ

b

l=a

. 

ζl  ≥  2  Fb−1ΣΣ

b   1

2

c          2                                           ΣΣ


≤ E   P

ζb +

 

l=a

ζl − 2τ

(b − a + 1 + 1)  ≥      − 2τ

(b − a + 1) Fb−₁


≤ E ΣP

.|ζb − 2τ

2           c                      2

| ≥  2 − (4L − 2)τ

b−1

−

k=a

ζk  Fb−1ΣΣ


= E ΣP

m

2τ 2

ζb − m  ≥

m

2τ 2

c

2 − (4L − 2)τ   −

b−1

k=a

ζk Σ  Fb−1ΣΣ


           m   . δ

	

b−1

Σ2


Σ      .     m

. c2                   2      4

2ΣΣΣ


= exp

.−Ω

. mc2 ΣΣ

(16)


Combining equation 16, equation 14 and the condition τ  ≤

1

Ω(    L)

, we obtain ǁhbǁ ≤ 1 + c with

probability at least 1     exp(   Ω( ᵐᶜ2 )), where we use approximation log(1 + c)      c for small 
c

to simplify the expression. Taking ϵ-net over all m-dimensional vectors of hₐ  ₁, with probability

1     exp(   Ω(mc²)) the inequality 2 holds for a fixed a and b with 1     a     b < L. Taking a 
union
bound over a and b, the conclusion is proved.

14


Under review as a conference paper at ICLR 2020

The ϵ-net argument is as follows. We have proved for a given unit vector hₐ−₁, ǁhbǁ > 1 + c with

probability at most exp(−Ω( ᵐᶜ2 )) for a small constant c. Let N   be an ε-net over the unit ball 
in


Rm  with

τ 2 L

s

ᵐ           ᵐ.  Taking the union


ϵ = 1/11, then we have the cardinality |Ns|  ≤  (1 + 2/ϵ)

bound over all vectors hₐ−₁ in the net Ns, we obtain

< (23)


P       max

ha−1 ∈Nе

ǁhbǁ > 1 + cΣ

≤ (1 + 2/ϵ)

· exp

.−Ω

mc²
τ 2L

.       .    mc2                         ΣΣ

By choosing the coefficient of τ  appropriately, we can make Ω( ᵐᶜ2 ) > log 23 and then the right


hand side can be written as exp

5.3), we have

.−m

       

mc²
τ 2 L

τ 2 L

. Based on the result (Vershynin, 2012, Lemma


¨Db .I + τ W ⁽⁰⁾Σ Db−₁ · · · Dₐ .I + τ W ⁽⁰⁾Σ¨   ≤ (1 − ϵ)−¹    max    ǁhbǁ₂ = 1.1    max

ǁhbǁ₂.

We complete the ϵ-net argument for the spectral norm bound by introducing a new constant c.

C    BOUNDED  FORWARD/BACKWARD  PROCESS

C.1    PROOF AT INITIALIZATION

Theorem 2.  Suppose τ      1/Ω(√L). There exists some small constant c such that with probability
at least 1     O(nL)   e−Ω⁽ᵐ·ᵐⁱⁿ⁽ᶜ,ᶜ ⁾⁾ over the randomness of A     Rm×p and W ⁽⁰⁾     (Rm×m)L

the following holds

∀i ∈ [n], l ∈ {0, 1, . . . , L} :  ¨hi,l ¨ ∈ [1 − c, 1 + c].                                 (3)

Proof.  We ignore the subscript (0) for simplicity. The upper bound of ǁhi,lǁ can be easily achieved
by the proof of Theorem 1. Now, we give the lower bound of ǁhi,lǁ. First we have


ǁhi,l

ǁ = ǁh

i,0

ǁǁhi,₁ǁ · · ·   ǁhi,lǁ

.                                            (17)


Then we see

ǁhi,₀ǁ

ǁhi,l−1ǁ

l                               2


log ǁh

i,l

ǁ   = log ǁh

i,0

₂ +       log   ǁhi,ₐǁ
ǁhi,a−1ǁ2


= log ǁhi,₀ǁ2  + Σ

log

.1 +

ǁhi,a

ǁ2  − ǁh

i,a−1

ǁ2 Σ

(18)


a=1
l

2                                        2

a

ǁhi,a−1ǁ2

a=1


where ∆ₐ

:=  ǁhi,aǁ   −ǁhi,a−1 ǁ

ǁhi,a−1 ǁ2

and the last inequality uses the relation log(1 + x) ≥ x − x². We

next give a lower bound on ∆ₐ. Let S be the set {k : k ∈ [m] and (hi,ₐ−₁)k + (W ₐhi,ₐ−₁)k > 0}.

15


Under review as a conference paper at ICLR 2020

We have that

m

∆   =         1        Σ Σ(h        )² + 2τ (h        )  (W   h        )   + (τ W   h        )²Σ −  
       1        Σ(h        )²

	

= −        1        Σ(h        )² +        1        Σ τ ²(W   h        )² +        2        Σ τ (h  
      )  (W   h        )


ǁhi,a−1ǁ2

k∈/S
m

i,a−1   k

ǁhi,a−1ǁ2

k∈S

a   i,a−1   k

m

ǁhi,a−1ǁ2

k∈S

i,a−1   k

a   i,a−1   k


       1              (τ W

hi,a   1   2

k=1

ahi,a−1

)² +        2        τ       (h
hi,a   1   2

k=1

i,a−1)k (W

ahi,a−1)k

ǁτ W ahi,a−1ǁ2         2τ ⟨hi,a−1, W ahi,a−1⟩


=

ǁhi,a−1

ǁ2         +

ǁhi,a−1

ǁ2                 ,                                                                    (19)

where  the  inequality  is  due  to  the  fact  that  for  k   ∈/   S,  |(hi,ₐ−₁)k|   <   |(τ W 
ₐhi,ₐ−₁)k|  and


(hi,a−1)k (W

ahi,a−1)k

≤  0.   We let ξₐ

:=  2τ (hi,a−1 ,W ahi,a−1 ⟩   and ζ

ǁhi,a−1 ǁ2                               a

:=  ǁτ W ahi,a−1 ǁ   ,  then

ǁhi,a−1 ǁ2


∆   ≥ ξ

− ζ  . We note that given h

, ξ   ∼ N .0, ⁸τ2 Σ and ζ

∼  2τ 2 χ² . Due to equation 14


and equation 16, we have

P

b

 l=a

ξl  ≥  2

≤ 2 exp

.− 128τ

mc²

2(b − a + 1)

(20)


b

P

l=a

ζ  ≥  c Σ

≤ exp

.−Ω

. mc2 ΣΣ

Then for any c > 0, and τ          ¹     , we have

Ω(    L)


l

P

a=1

∆ₐ ≤ −cΣ = P

l

a=1

∆ₐ ≤ −c,

aΣ=1

ξ   ≥ − c Σ + P

l

a=1

∆ₐ ≤ −c,

aΣ=1

ξ   ≤ − c Σ


l

≤ P

a=1

c
ζₐ ≥  2

l

+ P

a=1

c
ξₐ  ≤ − 2

= e−Ω(c  m).

(21)

We can derive a similar result that P .Σl       ∆   ≥ cΣ ≤ e−Ω⁽ᶜ2 m).  Let a = b in equation 20, we


obtain that for a single ∆ₐ,

In addition, we see that

P (|∆ₐ| ≥ c) ≤ 2e−Ω⁽Lᵐᶜ ⁾                                                    (22)


l

P

a=1

∆²  ≥ cΣ ≤

aΣ=1

P .∆²  ≥  c

=

a=1

P .|∆ₐ| ≥

. c Σ

≤ 2le−Ω⁽−ᵐᶜ⁾.         (23)

Thus, similar to the equation 21, we obtain

.Σl                                  Σ                                   2


which results in

P

a=1

∆ₐ − ∆²  ≤ −c

≤ 2Le−Ω(−m min{c,c  }),                               (24)


P .log ǁhi,lǁ2  ≤ −cΣ ≤ P .

log ǁhi,₀ǁ2  +

aΣ=1

.∆ₐ − ∆²Σ

≤ −cΣ

≤ 2Le−Ω(min{c,c  }m).    (25)

Then we get the conclusion.

16


Under review as a conference paper at ICLR 2020

C.2    LEMMAS AND PROOFS AFTER PERTURBATION

We use −W→(0)  to denote the weight matrices at initialization and use −W→′  to denote the 
perturbation

matrices.  Let −W→  =  −W→(0)  + −W→′.  We define h⁽⁰⁾  =  φ((I + τ W ⁽⁰⁾)h⁽⁰⁾   ) and hi,l  =  
φ((I +


i,l

l        i,l−1

τ W l)hi,l−₁) for l ∈ [L − 1], and h⁽⁰⁾  = φ(W ⁽⁰⁾h⁽⁰⁾     ) and hi,L = φ(W Lhi,L−₁). Furthermore,

(0)                  ′                                        (0)

let h′    := hi,l − h     and D    := Di,l − D    . Then the spectral norm bound after perturbation 
is


i,l

as follows.

i,l

i,l

i,l

Lemma  3.  Suppose  that  −W→(0),  A  are  randomly  generated  as  in  the  initialization  step,  
and

D′0′, . . . , D′L′   are diagonal matrices such that ǁD′l′ǁ₂  ≤ 1 for all l ∈ [L] and Dl′′  is 
deterministic

given {W ⁽⁰⁾ : a ≤ l}, and W ′ , . . . , W ′   ∈ Rm×m are perturbation matrices with ǁW ′ǁ₂ < τω

for all l     [L     1] for some ω < 1. Then with probability at least 1     (L)   exp(   Ω(m)) 
over the
initialization randomness we have

ǁ(I + τ W ⁽⁰⁾ + τ W ′ )D′′     · · · D′′(I + τ W ⁽⁰⁾ + τ W ′ )ǁ₂ ≤ O(1).                (26)

Proof.  This proof is based on the result of Theorem 1. From Theorem 1, we know for any 1     a
b < L

ǁ(I + τ W ⁽⁰⁾)D′′     · · · D′′(I + τ W ⁽⁰⁾)ǁ₂ ≤ 1 + c.

b          b−1               a                   a

Then we have

(0)                  ′          ′′                      ′′                          (0)             
     ′

ǁ(I + τ W      + τ W  )D      · · · D  (I + τ W      + τ W   )ǁ₂


b−Σa+1 .b − a + 1Σ .

τ ǁW ′ǁΣj

(1 + c)ʲ⁺¹ ≤ (1 + c) · (1 + (1 + c)τ ²)ᵇ−ᵃ⁺¹ ≤ O(1 + c),

j=0

due to the assumption ǁW ′lǁ ≤ τω for l ∈ [L − 1] and ω < 1, τ  ≤ 1/Ω(√L).

We also have small changes on the output vector of each layer after perturbation.

Lemma 4.  Suppose for ω ≤ O(1), τ ²L ≤ 1, ǁW ′Lǁ₂ ≤ ω and ǁW l′ ǁ₂ ≤ τω for l ∈ [L − 1]. Then

with probability at least 1 − exp(−Ω(mω2/3 )), the following bounds on h′    and D′    hold for all

i,l

i ∈ [n] and all l ∈ [L − 1],


ǁh′i,l

ǁ ≤ O(τ ²Lω),  ǁD′i,lǁ₀

≤ O .m(ωτ L)2/3 Σ ,  ǁh′

ǁ ≤ O(ω),  ǁD′i,Lǁ₀

≤ O .mω2/3 Σ .

Proof.  Fixing  i  and  ignoring  the  subscript  in  i,  by  Claim  8.2  in  Allen-Zhu  et  al.  
(2018b),  for

l ∈ [L − 1], there exists D′l′  such that |(D′l′)k,k| ≤ 1 and

h′  = D′′ .(I + τ W l + τ W ′)hl−₁ − (I + τ W l)h⁽⁰⁾ Σ

= D′′ .(I + τ W l + τ W ′)h′      + τ W ′h⁽⁰⁾ Σ

= D′l′(I + τ W l + τ W ′l)D′l′   1(I + τ W l−₁ + τ W ′l−   )h′


+ τ D′′(I + τ W l + τ W ′)D′′    W ′

h(0)

+ τ D′′W ′ h(0)


l

= · · ·

l

l       l−1

l−1

l−2

l       l   l−1

a   a

a=1

We claim that

ǁh′lǁ ≤ O(τ ²Lω)                                                          (28)

due to the fact ǁD′l′ǁ₂  ≤  1 and the assumption ǁW l′ ǁ₂  ≤  τω for l  ∈  [L − 1].  This implies 
that
ǁh′i,lǁ, ǁgi′,lǁ  ≤  O(τ ²Lω) for all l  ∈  [L − 1] and for all i with probability at least 1 − 
O(nL) ·
exp(−Ω(m)). One step further, we have ǁh′Lǁ, ǁgL′  ǁ ≤ O(ω).

17


Under review as a conference paper at ICLR 2020

As for the sparsity ǁD′ǁ₀, we have ǁD′ǁ₀ ≤ O(m(ωτ L)2/3 ) for every l = [L − 1] and ǁD′  ǁ₀ ≤

l                               l                                                                   
                                L

O(mω2/3 ).

The argument is as follows (adapt from the Claim 5.3 in Allen-Zhu et al. (2018b)).


We first study the case for l ∈ [L − 1]. We observe that if (D′l)j,j

|(g′)j| > |(g⁽⁰⁾)j|.

0 one must have


We note that

(0)

l

)j  =  (h

(0)

l−1

+ τ W

(0)

l

l

(0)

l−1

l

)j  ∼  N

.(h

(0)

l−1

)j,

2τ 2   h(0)    2

l−1

m

ξ  ≤  2

¹   be a

parameter to be chosen later. Let S₁ ⊆ [m] be a index set satisfying S₁ := {j : |(g⁽⁰⁾)j| ≤ ξτ }. We
have P{|(g    )j| ≤ ξτ } ≤ O(ξ    m) for each j ∈ [m]. By Chernoff bound, with probability at least
1 − exp(−Ω(m³/²ξ)) we have


|S₁| ≤ O(ξm

3/2).

Let S₂  := {j  : j  ∈/  S₁,  and (D′l)j,j  /= 0}.  Then for j  ∈  S₂, we have |(gl′)j|  > ξτ .  As 
we have
proved that ǁgl′ǁ ≤ O(τ ²Lω), we have

ǁgl′ǁ2                                2      2

|S₂| ≤  (ξτ )2   = O((ωτ L)  /ξ  ).

Choosing ξ  to minimize |S₁| + |S₂|,  we have ξ  =  (ωτ L)2/3 /√m and consequently,  ǁD′ǁ₀  ≤

O(m(ωτ L)2/3 ). Similarly, we have ǁD′  ǁ₀ ≤ O(mω2/3 ).

We next prove that the norm of a sparse vector after the ResNet mapping.

Lemma 5.  If s  ≥  Ω(d/ log m) and τ  ≤  1/Ω(√L), then for all i  ∈  [n] and a  ∈  [L] and for all

s-sparse vectors u ∈  Rm  and for all v  ∈  Rd, the following bound holds with probability at least


1 − (nL) · exp(−Ω(s log m))

T

s log m             Σ

|v   BDi,LW LDi,L−₁(I + τ W L−₁) · · · Di,ₐ(I + τ W ₐ)u| ≤ O         √d     ǁuǁǁvǁ    ,    (29)

where Di,ₐ is diagonal matrix with value 0 or 1 and it is independent of W b for any b ∈ (a, L].

Proof.  For any fixed vector u ∈  Rm, ǁDi,LW LDi,L−₁(I + τ W L−₁) · · · Di,ₐ(I + τ W ₐ)uǁ  ≤

1.1ǁuǁ holds with probability at least 1 − exp(−Ω(m)) (over the randomness of W l, l ∈ [L]).

On the above event,  for a fixed vector v  ∈  Rd  and any fixed W l for l  ∈  [L],  the randomness
only comes from B, then vT BDi,LW LDi,L−₁(I + τ W L−₁)√· ·  · Di,ₐ(I + τ W ₐ)u is a Gaussian

variable with mean 0 and variance no larger than O(ǁuǁ · ǁvǁ/   d). Hence

P.|vT BDi,LW LDi,L−₁(I + τ W L−₁) · · · Di,ₐ(I + τ W ₐ)u| ≥ √s log m · Ω(ǁuǁǁvǁ/√d)Σ

Take ϵ-net over all s-sparse vectors of u and all d-dimensional vectors of v, if s      Ω(d/ log m)
then with probability 1     exp(   Ω(s log m)) the claim holds for all s-sparse vectors of u and all
d-dimensional vectors of v. Further taking the union bound over all i     [n] and a     [L], the 
lemma is
proved.

D    GRADIENT  LOWER/UPPER  BOUNDS  AND  THEIR  PROOFS

Because the gradient is pathological and data-dependent, in order to build bound on the gradient, we
need to consider all possible point and all cases of data. Hence we first introduce an arbitrary 
loss
vector and then the gradient bound can be obtained by taking a union bound.

18


Under review as a conference paper at ICLR 2020

We define the BP−W→,i(v, ·) operator.   It back-propagates a vector v  to the ·  which could be the
intermediate output hl or the parameter W l at the specific layer l using the forward propagation 
state
of input i through the network with parameter −W→. Specifically,


BP−W→,i(v, hl) := (I + τ W l₊₁)

Di,l₊₁ · · · (I + τ W L−₁)

Di,L−1W T Di,LBT v,


BP−→

(v, W l) := τ .Di,l(I + τ W l₊₁)T · · · (I + τ W L−₁)T Di,L−₁W T Di,LBT vΣ hT

∀l ∈ [L − 1],


W ,i

BP−W→,i

(v, W L) := .Di,LBT vΣ hT      .

L                          i,l−1

Moreover, we introduce

n

BP−W→(→−v , W l) :=        BP−W→,i(vi, W l)    ∀l ∈ [L],

i=1

where  →−v   is  composed  of  n  vectors  vi  for  i  ∈   [n].   If  vi  is  the  error  signal  
of  input  i,  then

∇W l Fi(−W→) = BP−W→,i(Bhi,L − yi∗, W l).

D.1    GRADIENT UPPER BOUND

Theorem 3.  With probability at least 1 − (nL) · exp(−Ω(m)) over the randomness of −W→(0), A, B,
it satisfies for every l ∈ [L − 1], every i ∈ [n], and every −W→ with ǁ−W→ − −W→(0)ǁ₂ ≤ ω for ω ∈ 
[0, 1],

ǁ∇      F (−W→)ǁ2   ≤ O . Fi(−W→) × τ ²mΣ ,         ǁ∇       F (−W→)ǁ2   ≤ O . Fi(−W→) × mΣ .    
(4)


Proof.  For each i ∈ [n], we have

¨BP−W→(vi, W L)¨F  = ¨Di,L .B   viΣ h

¨   = ¨Di,L .BT viΣ¨ ¨hT  −₁

¨ ≤ O(√m/d)ǁviǁ.

Similarly, we have for l ∈ [L − 1],

¨BP−→(vi, W l)¨   = τ .Di,l(I + τ W l₊₁)T · · · (I + τ W L−₁)T Di,L−₁W T Di,LBT viΣ hT


W                  F

≤ O(τ √m/d)ǁviǁ.

L                            i,l−1

The above upper bounds hold for the initialization −W→(0)  because of Lemma 1 and Lemma 2. They
also hold for all the −W→ such that ǁ−W→ − −W→(0)ǁ₂ ≤ ω due to Lemma 4 and Lemma 3.

Finally, taking ϵ   net over all possible vectors →−v  = (v  , . . . , v  )     (Rd)ⁿ, we prove 
that the above
bounds holds for all →−v . In particular, we can now plug in the choice of vi = Bhi,L    yi∗  and 
obtain
the desired bounds on the true gradients.

D.2    GRADIENT LOWER BOUND

Theorem  7.  Let  ω  =  O .    δ3/2       Σ.   With  probability  at  least  1 − exp(−Ω(mω2/3 ))  
over  the

randomness of −W→(0), A, B, it satisfies for every −W→ with ǁ−W→ − −W→(0)ǁ₂ ≤ ω,


ǁ∇W L

F (−W→)ǁ2

Ω    F (−W→)     m    .                                        (30)

dn/δ

This gradient lower bound on ǁ∇W  F (−W→)ǁ2   acts like the gradient dominance condition (Zou and

Gu, 2019; Allen-Zhu et al., 2018b) except that our range on ω does not depend on the depth L.

Proof.  The gradient lower-bound at the initialization is given in (Allen-Zhu et al., 2018b, Section
6.2) and (Zou and Gu, 2019, Lemma 4.1) via the smoothed analysis (Spielman and Teng, 2004): with

19


Under review as a conference paper at ICLR 2020

high probability the gradient is lower-bounded, although the worst case it might be 0. We adopt the
same proof for (Zou and Gu, 2019, Lemma 4.1) based on two preconditioned results Theorem 2 and
Lemma 6. We shall not repeat it here.

Now suppose that we have ǁ∇W L F (−W→     )ǁ    ≥ Ω .             × mΣ. We next bound the change of

(0)      2                   F (−W→(0))

the gradient after perturbing the parameter. Recall that


BP−→

(→−v , W L) − BP−→(→−v , W L) = Σ .(vT BD⁽⁰⁾)T (h⁽⁰⁾

)T  − (vT BDi,L)T (hi,L−1)T Σ


W (0)

W                                                 i

i=1

i,L

i,L−1                  i

By Lemma 4 and Lemma 5, we know,

i           i,L          i

Furthermore, we know

T

i

By Theorem 2 and Lemma 4, we have

(0)                                                                               (0)

ǁhi,L−1ǁ ≤ 1.01    and     ǁhi,L−₁ − hi,L−1ǁ ≤ O(ω).

Combing the above bounds together, we have

ǁBP−W→₍₀₎ (→−v , W L) − BP−W→(→−v , W L)ǁ    ≤ nǁ v ǁ   · O(.mω    /d + ω√m/d)   ≤ nǁ v ǁ   · O .   
 ω

2                →−   2                          2/3                                       2        
       →−   2              m   2/3 Σ

Hence the gradient lower bound still holds for −W→ given ω < O . δ3/2 Σ.

Finally, taking ϵ   net over all possible vectors →−v  = (v  , . . . , v  )     (Rd)ⁿ, we prove 
that the above
gradient lower bound holds for all →−v . In particular, we can now plug in the choice of vi = Bhi,L 
  yi∗
and   it implies our desired bounds on the true gradients.

The gradient lower bound requires the following property.

Lemma 6.  For any δ and any pair (xi, xj) satisfying ǁxi − xjǁ₂  ≥  δ, then ǁhi,l − hj,√lǁ  ≥  Ω(δ)


holds for all l ∈ [L] with probability at least 1−O(n²L)·exp(−Ω(log² m)) for τ  ≤ 1/Ω(

and m ≥ Ω(τ ²L²δ−²).

L log m)

The proof of Lemma 6 follows that of (Allen-Zhu et al., 2018b, Appendix C.1) given the condition
that m ≥ Ω(τ ²L²δ−²).

E    SEMI-SMOOTHNESS  FOR τ ≤ 1/Ω(√L)

With the help of Theorem 3 and several improvements, we can obtain a tighter bound on the semi-
smoothness condition of the objective function.


Theorem 8.  Let

Σ   ..        d      Σ3/2 Σ

Σ and −→

be at random initialization and

τ ²L ≤ 1. With probability at least 1 − exp(−Ω(mω2/3 )) over the randomness of −W→(0), A, B, we

have for every −→˘   ∈ (Rm×m)L with ǁW˘ L −W ⁽⁰⁾ǁ₂ ≤ ω and ǁW˘ l −W ⁽⁰⁾ǁ₂ ≤ τω for l ∈ [L−1],

and for every −W→′  ∈ (Rm×m)L with ǁW ′Lǁ₂ ≤ ω and ǁW l′ ǁ₂ ≤ τω for l ∈ [L − 1], we have


−→˘

−→′

−→˘

−→˘

−→′

nm    −→′  ₂

F (W + W  ) ≤F (W ) + ⟨∇F (W ), W  ⟩ + O(  d  )ǁW  ǁF


mnLω2/3

+ O

· (τ L)

Σ ǁ−W→′ǁF

.F (W ).                   (31)

Before going to the proof of the theorem, we introduce a lemma.

20


Under review as a conference paper at ICLR 2020

Lemma 7.  There exist diagonal matrices D′i′,l   ∈  Rm×m  with entries in [-1,1] such that ∀i  ∈

[n], ∀l ∈ [L − 1],

l

hi,l − h˘i,l  =       (D˘ i,l + Di′′,l)(I + τ W˘ l) · · · (I + τ W˘ a+1)(D˘ i,a + D′i′,a)τ W 
′ahi,ₐ−₁,     (32)

a=1


and

hi,L − h˘i,L  =(D˘ i,L + D′i′,L)W ′Lhi,L−1

L−1

+       (D˘ i,L + D′i′,L)W˘ L · · · (I + τ W˘ a+1)(D˘ i,a + D′i′,a)τ W a′  hi,ₐ−₁.      (33)

a=1

Furthermore, we then have ∀l ∈ [L − 1], ǁhi,l − h˘i,lǁ ≤ O(τ ²Lω), ǁD′′  ǁ₀ ≤ O(m(ωτ L)2/3 ), and


ǁhi,L − h˘i,Lǁ ≤ O((1 + τ

√L)ǁW ′ǁF ), ǁD′i′,L

ǁ₀ ≤ O(mω2/3 ) and

i,l

ǁBhi,L − Bh˘i,Lǁ ≤ O(    m/d)ǁW ′ǁF

hold with probability 1 − exp(−Ω(mω2/3 )) given ǁW ′  ǁ₂ ≤ ω, ǁW ′ǁ₂ ≤ τω for l ∈ [L − 1] and

L                         l

ω ≤ O(1), τ    L ≤ 1.

Proof.  The proof can adapt from the proof of Claim 8.2 in Allen-Zhu et al. (2018b) and the proof of
Lemma 4.

Proof of Theorem 8.  First of all, we know that lo˘ssi := Bh˘i,L − yi∗


1

ǁBh

− y∗ǁ2  =  1 ǁlo˘ss

+ B(h

− h˘

)ǁ2


and

=  1 ǁlo˘ss ǁ2  + l ˘  T B(h

n

− h˘

1

i,L) + 2 ǁB(h

i,L

− h˘

i,L

)ǁ2,        (34)

l                                                              i

i=1
n


Then,

∇W L

F (−W→) =       (lossT BDi,L)T (hi,l−₁)T .                                                          
   (36)

i=1


−→˘

−→′

−→˘

−→˘

−→′


F (W + W  ) − F (W ) − ⟨∇F (W ), W  ⟩

= −⟨∇     −→˘     −→′⟩ + 1 Σ ǁBh     − y∗ǁ2  − ǁBh˘

− y∗ǁ2


F (W ), W

L

2

i=1

i,L          i

n

i,L          i


= − Σ⟨∇

−→˘

′⟩ + Σ   ˘  T

− h˘

1

) +    ǁB(h

− h˘

)ǁ2


n

(a)

=

2

ǁB(hi,L − h˘i,L)ǁ2  + Σ

T

lossi  B

.(D˘ i,L + D′i′,L)W ′Lhi,L−1  − (D˘ i,L)W ′Lh˘i,L−1Σ

i=1                                                       i=1


L−1

+

T

lossi  B

.(D˘ i,L + D′i′,L)W˘ L · · · (I + τ W˘ l+1)(D˘ i,l + D′i′,l)τ W l′ hi,l−₁

i=1  l=1

− D˘ i,LW˘ L · · · (I + τ W˘ l+1)D˘ i,lW ′l(τ h˘i,l−1)  ,                                           
                          (37)
where (a) is due to Lemma 7.

21


Under review as a conference paper at ICLR 2020

We next bound the RHS of equation 37. We first use Lemma 7 to get

ǁB(hi,L − h˘i,L)ǁ ≤ O(    m/d)ǁW ′ǁF .                                        (38)
Next we calculate that for l = L,


˘  T    .  ˘

′′               ′

˘            ′  ˘         Σ  

	


loss   B

(Di,L + Di,L)W Lhi,L−1  − (Di,L)W Lhi,L−1


≤    ˘

.   ′′

W ′  h

Σ       

T    . ˘

W ′  (h

− h˘

)Σ  .          (39)


˘  T      .   ′′            ′

Σ         . √mω2/3 Σ     ˘                ′


loss   B

Di,LW Lhi,L−1

  ≤ O

√d        ǁlossiǁ · ǁW Lhi,L−₁ǁ

. √mω2/3 Σ     ˘                ′

where the last inequality is due to ǁhi,L−₁ǁ ≤ O(1). For the second term, by Lemma 7 we have


˘  T    . ˘          ′

˘          Σ 


loss   B

Di,LW L(hi,L−1  − hi,L−1)


≤ ǁlo˘ssiǁ · ¨BD˘ i,L¨

· ǁW ′Lǁ2ǁhi,L−1  − h˘i,L−1ǁ


≤ ǁlo˘ssiǁ · O

ω√m

√d

· ǁW ′Lǁ₂,                                                         (41)

where the last inequality is due to the assumption   W ′L   2        ω. Similarly for l     [L     
1], we ignore
the index i for simplicity.


L−1

  l=1

T

loss

B(D˘ L + D′L′ )W˘ L · · · (I + τ W˘ l+1)(D˘ l + Dl′′) − BD˘ LW˘ L · · · (I + τ W˘  l+1)D˘ l

ΣW ′l(τhl−₁) 


L−1

=

l=1

T

loss

BD′L′ W˘ L(DL−₁ + D′L′−₁)(I + τ W˘  L−1) · · · (Dl + D′l′)(τ W ′lhl−₁) 


L−1 L−1

+

l=1   a=l
L−1

T

loss

T

BD˘ LW˘  L · · · (I + τ W˘ a+1)D′ₐ′(I + τ W˘ a) · · · (Dl + D′l′)(τ W ′lhl−₁) 

 

We next bound the terms in equation 42 one by one. For the first term, by Lemma 5 and Lemma 7,
we have


L−1

  l=1

T

loss

BD′L′ W˘ L(DL−₁ + D′L′ −1)(I + τ W˘ L−1) · · · (Dl + D′l′)(τ W ′lhl−₁) 

. √mω2/3 Σ ¨      ¨     L   1                                                                       
                                                                                    ¨


(a)

≤  O

√mω2/3

√d

· ǁlo˘ssǁ · τ

√LǁW ′L−1:1ǁF ,                                                                          (43)

where    (a)    is   due    to    the    similar    argument    in    the   proof    Lemma    7    
and    the   fact
W˘ L(DL−₁ + D′L′ −1)(I + τ W˘ L−1) · · · (Dl + D′l′)    =  O(1) and ǁhl−₁ǁ  =  O(1) holds with
high probability.

22


Under review as a conference paper at ICLR 2020

We have similar bound for the second term of equation 42


L−1 L−1

  l=1   a=l

T

loss

BD˘ LW˘ L · · · (I + τ W˘ a+1)D′a′(I + τ W˘ a) · · · (Dl + D′l′)(τ W ′lhl−₁) 


. √m(ωτ L)2/3 Σ

. √m(ωτ L)2/3 Σ

· ǁlo˘ssǁ · τ

˘

L−1

a=1
3/2

aǁW ₐ:₁ǁF


≤ O             √d

· ǁlossǁ · τ L

ǁW L−₁:₁ǁF .                                                         (44)

For the last term in equation 42, we have

L−1           T                          


≤ ǁlo˘ssǁ · O

.√m/dΣ ·

L−1

l=1

ǁW ′lǁ₂ · τ ³Lω

≤ ǁlo˘ssǁ · O .√m/dΣ · ǁW ′L−1:1ǁF · (τ ²L)³/²,                                          (45)

where is the last inequality is due to the bound on ǁhl−₁ − h˘l−1ǁ₂ in Lemma 7. Hence


equation 42 ≤ O

. √m(ωτ L)2/3 Σ

· ǁlo˘ssǁ · τ L

3/2

ǁW L−1:1ǁF


≤ O .

(τ L)4/3

√mLω2/3

√d

· ǁlo˘ssǁ · ǁW ′L−1:1ǁF

.                      (46)

Having all the above together and using triangle inequality, we have the result.

Proposition  1  (Proposition  8.3  in  in  Allen-Zhu  et  al.  (2018b)).  Given  vectors  a, b  ∈  
Rm  and

D  ∈  Rm×m  the diagonal matrix where Dk,k  =  1ₐ ≥₀.   Then,  there exists a diagonal matrix

D′′ ∈ Rm×m with

•  |Dk,k + D′k′,k | ≤ 1 and |D′k′,k | ≤ 1 for every k ∈ [m],

•  D′k′,k  /= 0 only when 1ₐk ≥0  /= 1bk ≥0,

•  φ(a) − φ(b) = (D + D′′)(a − b).

Proof of Lemma 7.  Fixing index i and ignoring the subscript in i for simplicity, by Proposition 1, 
for
each l ∈ [L − 1] there exists a D′l′  such that |(D′l′)k,k| ≤ 1 and

hl − h˘l  = φ((I + τ W˘ l + τ W ′l)hl−₁) − φ((I + τ W˘  l)h˘l−1)

= (D˘ l + D′l′) .(I + τ W˘ l + τ W l′ )hl−₁ − (I + τ W˘  l)h˘l−1Σ

= (D˘ l + D′l′)(I + τ W˘ l)(hl−₁ − h˘l−1) + (D˘ l + D′l′)τ W l′ hl−₁

l

=       (D˘ l + D′l′)(I + τ W˘ l) · · · (I + τ W˘ a+1)(D˘ a + D′a′)τ W a′  hₐ−₁

a=1

Then we have following properties.  For l  ∈  [L − 1], ǁhl − h˘lǁ  ≤  O(τ ²Lω).  This is because
ǁ(D˘ l  + D′l′)(I + τ W˘  l) · · · (I + τ W˘ a+1)(D˘ a  + D′a′)ǁ  ≤  1.1 from Lemma 3; ǁhₐ−₁ǁ  ≤  
O(1)
from Theorem 2; and the assumption ǁW ′lǁ₂ ≤ τω for l ∈ [L − 1].

23


Under review as a conference paper at ICLR 2020


To have a tighter bound on ǁhL − h˘Lǁ, let us introduce W ′′ := Σl

(D˘ l + D′′)(I + τ W˘ l) · · · (I +


It is easy to get

ǁ[τhl−1

, τhT

l−2

, ..., τhT ]T ǁ₂ = ‚u,τ 2

L

l−1
a=0

L

ǁhₐǁ2  ≤ τ

0

√L · O(1),

where the inequality is because of ǁhₐ−₁ǁ ≤ O(1) from Theorem 2. Next, we have


¨ΣW ′′, W ′′

, ..., W ′′Σ¨   = ¨ΣW ′′, W ′′

, ..., W ′′ΣT ¨


l         l−1

1      2                     l

‚uΣl

l−1

1

‚uΣl

where the second inequality is from the definition of spectral norm, the third inequality is 
because of

ǁ(D˘ l + D′l′)(I + τ W˘ l) · · · (I + τ W˘  a+1)(D˘ a + D′a′)ǁ ≤ 1.1 from Lemma 3.

Hence we have ǁhL − h˘Lǁ ≤ O .(1 + τ √L)ǁW ′ǁF Σ = O .ǁW ′ǁF Σ because of the assumption

For l ∈ [L], ǁD′′ǁ₀ ≤ O(mω2/3 ). This is because (D′′)k,k is non-zero only at coordinates k where

(g˘l)k and (gl)k have opposite signs, where it holds either (D⁽⁰⁾)k,k  =/      (D˘ l)k,k or 
(D⁽⁰⁾)k,k

l                                                l

(Dl)k,k. Therefore by Lemma 4, we have ǁD′′ǁ₀ ≤ O(m(ωτ L)2/3 ) if ǁW ′ǁ₂ ≤ τω.

F    PROOF  FOR  THEOREM  5

Theorem 5.  Suppose that the ResNet is defined as in Section 2 with τ  ≤  1/Ω(√L log m) and
training data satisfy Assumption 1.  If the network width m  ≥  Ω(n⁸L⁷δ−⁴d log² m), then with

probability at least 1 − exp(−Ω(log² m)), gradient descent with learning rate η = Θ(   ᵈ  ) finds a

point F (−W→) ≤ ε in T  = Ω(n²δ−¹ log ⁿ ˡᵒᵍ2 m ) iterations.

F.1    CONVERGENCE RESULT FOR GD

Proof.  Using Theorem 2 we have ǁh⁽⁰⁾ǁ₂ ≤ 1.1 and then using the randomness of B, it is easy to
show that ǁBh⁽⁰⁾ −y∗ǁ2  ≤ O(log² m) with probability at least 1 −exp(−Ω(log² m)), and therefore


i,L         i

F (−W→(0)) ≤ O(n log² m).                                                   (49)

Assume that for every t = 0, 1, . . . , T − 1, the following holds,


(t)

(0)

∆     .   δ3/2     Σ

(t)               (0)

ǁW l    − W l    ǁF ≤ τω.                                                                (51)

We shall prove the convergence of GD under the assumption equation 50 holds, so that previous
statements can be applied. At the end, we shall verify that equation 50 is indeed satisfied.

Letting ∇t = ∇F (−W→(t)), we calculate that

F (−W→(t+1)) ≤ F (−W→(t)) − ηǁ∇tǁ2   + O(η²nm/d)ǁ∇tǁ2  +


η.F (−W→(t)) · O

.. mnLω2/3

(τ L)4/3 Σ

F

· ǁ∇  ǁ

≤ .1 − Ω . ηδm ΣΣ F (−W→(t)),                                                    (52)

24


Under review as a conference paper at ICLR 2020

where the first inequality uses Theorem 4, the second inequality uses the gradient upper bound in
Theorem 3 and the last inequality uses the gradient lower bound in Theorem 7 and the choice of

η  =  O(d/(mn)) and the assumption on ω equation 50.  That is, after T  =  Ω(   ᵈⁿ ) log ⁿ ˡᵒᵍ2 m


iterations F (−W→(T )) ≤ ϵ.

ηδm                 s

We need to verify for each t, equation 50 holds.   Here we use a result from (Zou and Gu, 2019,


Lemma 4.2) that states ǁW ⁽ᵗ⁾ − W ⁽⁰⁾ǁ

≤ O(. n2 d log m ).

To guarantee the iterates fall into the region given by ω  equation 50,  we obtain a bound m
n⁸δ−⁴dL⁷ log² m.

G    TIGHTNESS  OF τ = 1/√L AND  THE  PROOF  OF  THEOREM  4

Theorem 4.  For the ResNet defined and initialized as in Section 2, if τ  ≥ L− 2 +c, then in 
expectation

EǁhLǁ   > L   .                                                             (5)

Proof.  By induction we can show for any k ∈ [m] and l ∈ [L − 1],


(hl)k ≥ φ

l

a=1

(τ W ₐhₐ−₁)k Σ

.                                              (53)

It is easy to verify (h₁)k = φ ((h₀)k + (τ W ₁h₀)k) ≥ φ ((τ W ₁h₀)k) because of (h₀)k ≥ 0.

k Σ

Then assume (hl)k ≥ φ .Σl       (τ W ₐhₐ−₁)    , we show it holds for l + 1.


(hl₊₁)k = φ ((hl)k + (τ W l₊₁hl)k) ≥ φ .φ

l

a=1

(τ W aha−1)kΣ

+ (τ W l₊₁hl)kΣ ≥ φ

l+1

a=1

(τ W aha−1)kΣ ,

where the last inequality can be shown by case study.


Next we can compute the mean and variance of Σl

(τ W aha−1)k

by taking iterative conditioning.


We have

E

a=1

(τ W aha−1)k

= 0,     E

l
a=1

2

(τ W ₐhₐ−₁)k        =

2     l

m

a=1

Eǁhₐ−₁ǁ2.          (54)

Moreover, (τ W ₐhₐ  ₁)k are jointly Gaussian for all a with mean 0 because W ₐ’s are drawn from
independent Gaussian distributions.  We use l  =  2 as an example to illustrate the conclusion, it
can be generalized to other l.  Assume that h₀ is fixed.  First it is easy to verify that (τ W 
₁h₀)k is
Gaussian variable with mean 0 and (τ W ₂h₁)k W ₁ is also Gaussian variable with mean 0. Hence

[(τ W ₁h₀)k, (τ W ₂h₁)k]  follows  jointly  Gaussian  with  mean  vector  [0, 0].   Thus  (τ W 
₁h₀)k +

(τ W ₂h₁)k is Gaussian with mean 0. By induction, we have Σl      (τ W ₐhₐ−₁)k is Gaussian with


mean 0. Then we have

m

φ

k=1

l
a=1

(τ W aha−1)k

ΣΣ₂

1

=           E

2

k=1

l
a=1

2

(τ W aha−1)k


Σ τ 2 Σl

E Σǁhₐ−₁ǁ2Σ         Σ

where the first step is due to equation 53,  the second step is due to the symmetry of Gaussian
distribution and the third step is due to equation 54. Since (hl)k = φ ((hl−₁)k + (W lhl−₁)k ), we

can show E(hl)²  ≥ (hl−₁)² given hl−₁ by numerical integral of Gaussian variable over an interval.


k

Hence we have E

ǁhlǁ2

k

≥  Eǁh

l−1

ǁ2   ≥  · · ·  ≥  Eǁh₀ǁ2   =  1 by iteratively taking conditional
25


Under review as a conference paper at ICLR 2020


2.4

2.2

2.0

1.8

1.6

1.4

1.2

1.0

0.8

0.6

0.4

0.2

0.0

depth_1000
depth_5000

          depth_10000

2.4

2.2

2.0

1.8

1.6

1.4

1.2

1.0

0.8

0.6

0.4

0.2

0.0

depth_1000
depth_5000

          depth_10000


0        5000    10000   15000   20000   25000

training steps

0        5000    10000   15000   20000   25000

training steps

Figure 5: ResNet with 1000/5000/10000 layers. The width is 8 for the left figure and 16 for the 
right.

expectation. Then combined with equation 55 and the choice of τ  = L− 2 +c, we have EǁhL−₁ǁ2  ≥
L²ᶜ.   Because of the random Gaussian initialization of W L and hL  =  φ(W LhL−₁),  we have
EǁhLǁ2  = ǁhL−₁ǁ2. Thus, the claim is proved.

H    MORE  EMPIRICAL  STUDIES


Dataset
IWSLT DE-EN

WMT EN-DE

Model

Transformer (Vaswani et al., 2017)
Fixup (Zhang et al. (2019a))
Transformer + τ

Transformer (Vaswani et al., 2017)
Fixup (Zhang et al. (2019a))
Transformer + τ

BLEU
34.8

35.0

35.6

28.4

28.1

29.1

Table 3: Experiment results on machine translation task (Higher is better).

In this section, we train Transformer with τ  but no normalization layer.  We conduct experiments
on two standard machine translation tasks:  IWSLT DE-EN and WMT EN-DE. We modify the
model by multiplying a fixed τ right after each residual addition and removing all the normalization
layers. We do not use scalar bias in our Transformer+τ model. For IWSLT DE-EN, we adopt the
Transformer-base model and set τ  = 0.5. For WMT EN-DE, we use Transformer-big model and set
τ                                       = 0.3. Dropout is set as 0.5 for base model. For big model, 
we set attention dropout and activation
dropout as 0.1. Other hyperparameters are the same as Vaswani et al. (2017). For Fixup, we use the
implementation by Zhang et al. (2019a). All models are trained for 150 epochs with 25k batch size.
We      average the checkpoints of the last 10 epochs and evaluate the BLEU score. The scores are 
shown
in Table 3.

I    EXTREME  DEEP  RESNET

Per  reviewer’s  request,  we  plot  the  training  curves  of  extremely  deep  ResNets  with  the 
 width
m        8, 16   and the depth L        1000, 5000, 10000   in Figure 5.  We set τ  =     ¹   .  We 
see that
even for ResNet with depth 10000 and width 8, the training curve although fluctuates, manages to
converge. This reflects the depth dependency of ResNet is weak, echoing our stability result.

26

