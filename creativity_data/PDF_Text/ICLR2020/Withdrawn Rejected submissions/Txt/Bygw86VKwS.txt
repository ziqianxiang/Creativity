Under review as a conference paper at ICLR 2020
Fault Tolerant Reinforcement Learning via A
Markov Game of Control and Stopping
Anonymous authors
Paper under double-blind review
Ab stract
Recently, there has been a surge in interest in safe and robust techniques within
reinforcement learning (RL). Current notions of risk in RL fail to capture the
potential for systemic failures such as abrupt stoppages from system failures or
surpassing of safety thresholds and the appropriate responsive controls in such
instances. We propose a novel approach to fault-tolerance within RL in which the
controller learns a policy can cope with adversarial attacks and random stoppages
that lead to failures of the system subcomponents. The results of the paper also
cover fault-tolerant (FT) control so that the controller learns to avoid states that carry
risk of system failures. By demonstrating that the class of problems is represented
by a variant of SGs, we prove the existence of a solution which is a unique fixed
point equilibrium of the game and characterise the optimal controller behaviour.
We then introduce a value function approximation algorithm that converges to the
solution through simulation in unknown environments.
1	Introduction
Reinforcement learning (RL) provides the promise of adaptive agents being able to discover solutions
merely through repeated interaction with their environment. RL has been deployed in a number of
real-world settings in which, using RL, an adaptive agent learns to perform complex tasks, often in
environments shared by human beings. Large scale factory industrial applications, traffic light control
(Arel et al., 2010), robotics (Deisenroth et al., 2013) and autonomous vehicles (Shalev-Shwartz et al.,
2016) are notable examples of settings to which RL methods have been applied.
Numerous automated systems are however, susceptible to failures and unanticipated outcomes.
Moreover, many real-world systems amenable to RL suffer the potential for random stoppages and
abrupt failures; actuator faults, failing mechanical system components, sensor failures are few such
examples. In these settings, executing preprogrammed behaviours or policies that have been trained
in idealised simulated environments can prove vastly inadequate for the task of ensuring the safe
execution of tasks. Consequently, in the presence of such occurrences, the deployment of RL agents
introduces a risk of catastrophic outcomes whenever the agent is required to act so as to avoid adverse
outcomes in unseen conditions. The important question of how to control the system in a way that is
both robust against systemic faults and, minimises the risk of faults or damage therefore arises.
In response to the need to produce RL algorithms that execute tasks with safety guarantees, a
significant amount of focus has recently been placed on safe execution, robust control and risk-
minimisation (Garcia and Fernandez, 2015). Examples include Hm control (Morimoto and Doya,
2001), coherent risk, conditional value at risk (Tamar et al., 2015). In general, these methods introduce
an objective1 defined with an expectation measure that either penalises actions that lead to greater
uncertainty or embeds a more pessimistic view of the world (for example, by biasing the transition
predictions towards less desirable states). In both cases, the resulting policies act more cautiously
over the horizon of the problem as compared to policies trained with a standard objective function.
Despite the recent focus on safe methods within RL, the question of how to train an RL agent that
can cope with random failures remains unaddressed. In particular, at present the question of how to
produce an RL policy that can cope with an abrupt failure of some system subcomponent has received
1With a Lagrangian approach, constraints are captured in the construction of the Lagrangian.
1
Under review as a conference paper at ICLR 2020
no systematic treatment. Similarly, the task of addressing how to produce RL policies that account
for the risk of states in which such failures occur has not been addressed.
In this paper, we for the first time produce a method that learns optimal policies in response to
random and adversarial systems attacks that lead to stoppages of system (sub)components that may
produce adverse events. Our method works by introducing an adversary that seeks to determine
a stopping criterion to stop the system at states that lead to the worst possible (overall) outcomes
for the controller. Using a game-theoretic construction, we then show how a policy that is robust
against adversarial attacks that lead to abrupt failure can be learned by an adaptive agent using an
RL updating method. In particular, the introduction of an adversary that performs attacks at states
that lead to worst outcomes generates experiences for the adaptive RL agent to learn a best-response
policy against such scenarios.
To tackle this problem, we construct a novel two-player stochastic game (SG) in which one of the
players, the controller, is delegated the task of learning to modify the system dynamics through its
actions that maximise its payoff and an adversary or ‘stopper’ that enacts a strategy that stops the
system in such a way that maximises the controller’s costs. This produces a framework that finds
optimal policies that are robust against stoppages at times that pose the greatest risk of catastrophe.
The main contribution of the paper is to perform the first systematic treatment of the problem of robust
control under worst-case failures. In particular, we perform a formal analysis of the game between
the controller and the stopper. Our main results are centered around a minimax proof that establishes
the existence of a value of the game. This is necessary for simulating the stopping action to induce
fault-tolerance. Although minimax proofs are well-known in game theory (Shapley, 1953; Maitra and
Parthasarathy, 1970; Filar et al., 1991), replacing a player’s action set with stopping rules necessitates
a minimax proof (which now relies on a construction of open sets) which markedly differs to the
standard methods within game theory. Additionally, crucial to our analysis is the characterisation of
the adversary optimal stopping rule (Theorem 3).
Our results tackle optimal stopping problems (OSPs) under worst-case transitions. OSPs are a
subclass of optimal stochastic control (OSC) problems in which the goal is to determine a criterion
for stopping at a time that maximises some state-dependent payoff (Peskir and Shiryaev, 2006).
The framework is developed through a series of theoretical results: first, we establish the existence of
a value of the game which characterises the payoff for the saddle point equilibrium (SPE). Second, we
prove a contraction mapping property of a Bellman operator of the game and that the value is a unique
fixed point of the operator. Third, we prove the existence and characterise the optimal stopping time.
We then prove an equivalence between the game of control and stopping and worst-case OSPs and
show that the fixed point solution of the game solves the OSP.
Finally, using an approximate dynamic programming method, we develop a simulation-based iterative
scheme that computes the optimal controls. The method applies in settings in which neither the
system dynamics nor the reward function are known. Hence, the agent need only observe its realised
rewards by interacting with the environment.
1.1	Related Work
At present, the coverage of FT within RL is limited. In (Zhang and Gao, 2018) RL is applied to tackle
systems in which faults might occur and subsequently incur a large cost. Similarly, RL is applied
to a problem in (Yasuda et al., 2006) in which an RL method for Bayesian discrimination which is
used to segment the state and action spaces. Unlike these methods in which infrequent faults from
the environment generate negative feedback, our method introduces an adversary that performs the
task of simulating high-cost stoppages (hence, modelling faults) that induce an FT trained policy.
A relevant framework is a two-player optimal stopping game (Dynkin game) in which each player
chooses one of two actions; to stop the game or continue (Dynkin, 1967). Dynkin games have
generated a vast literature since the setting requires a markedly different analysis from standard SG
theory. In the case with one stopper and one controller such as we are concerned with, the minimax
proof requires a novel construction using open sets to cope with the stopping problem for the minimax
result. Presently, the study of optimal control that combines control and stopping is limited to a few
studies e.g. (Chancelier et al., 2002). Similarly, games of control and stopping have been analysed in
continuous-time (Bayraktar et al., 2011; Baghery et al., 2013; Mguni, 2018). In these analyses, all
2
Under review as a conference paper at ICLR 2020
aspects of the environment are known and in general, solving these problems requires computing
analytic solutions to non-linear partial differential equations which are often analytically insoluble
and whose solutions can only be approximated numerically at very low dimensions.
Current iterative methods in OSPs (and approximated dynamic programming methods e.g. (Bertsekas,
2008)) in unknown environments are restricted to risk-neutral settings (Tsitsiklis and Van Roy, 1999)
— introducing a notion of risk (generated adversarially) adds considerable difficulty as it requires
generalisation to an SG involving a controller and stopper which alters the proofs throughout. In
particular, the solution concept is now an SG SPE, the existence of which must be established. As we
show, our framework provides an iterative method of solving OSPs with worst-case transitions in
unknown environments and hence, generalises existing OSP analyses to incorporate a notion of risk.
Organisation
The paper is organised as follows: we firstly give a formal description of the FT RL problem we
tackle and the OSP with worst-case transitions and give a concrete example to illustrate an application
of the problem. In Sec. 2, we introduce the underlying SG framework which we use within the
main theoretical analysis which we perform in Sec. 3. Lastly, in Sec. 4, we develop an approximate
dynamic programming approach that enables the optimal controls to be computed through simulation,
followed by some concluding remarks.
We now describe the main problem with which we are concerned that is, FT RL. We later prove an
equivalence between the OSPs under worst-case transitions and the FT RL problem and characterise
the solution of each problem.
1.2	Fault-Tolerant Reinforcement Learning
We concern ourselves with finding a control policy that copes with abrupt system stoppages and
failures at the worst possible states. Unlike standard methods in RL and game theory that have fixed
time horizons (or purely random exit times) in the following, the process is stopped by a fictitious
adversary that uses a stopping strategy or rule to decide when to stop given its state observations. In
order to generate an FT control, we simulate the adversary’s action whilst the controller determines
its optimal policy. This as we show, induces a form of control that is an FT best-response control.
A formal description is as follows: an agent exercises actions that influence the sequence of states
visited by the system. At each state, the agent receives a reward which is dependent on the state and
the chosen action. The agent’s actions are selected by a policy π : S × A → [0, 1] — a map from the
set of states S and the set of actions A to a probability. We assume that the action set is a discrete
compact set and that the agent’s policy π is drawn from a compact policy set Π. The horizon of the
problem is T ∈ N × {∞}. However, at any given point τS ≤ T the system may stop (randomly)
and the problem terminates where TS 〜f ({0,..., T}) is a measurable, random exit time and f is
some distribution on {0, . . . , T}. If after k ≤ T time steps the system stops, the agent incurs a cost
of G(Sk) and the process terminates.
For any s ∈ S and for any π ∈ Π, the agent’s performance function is given by:
τS∧T
JTS,π[s] = E E YtR(St,at) + YYTS∧TG(sτs八T) so = s,at 〜∏,τs 〜f({0,...,T}) , (1)
t=0
where a ∧ b := min{a, b}, E is taken w.r.t. the transition function P. The performance function (1)
consists of a reward function R : S × A → R which quantifies the agent’s immediate reward when
the system transitions from one state to the next, a bequest function G : S → R which quantifies
the penalty incurred by the agent when the system is stopped and Y ∈ [0, 1[, a discount factor. We
assume R and G are bounded and measurable.
The FT control problem which we tackle is one in which the controller acts both with concern
for abrupt system failures and stoppages. In particular, the analysis is performed in sympathy
with addressing the problem of how the controller should act in two scenarios — the first involves
acting in environments that are susceptible to adversarial attacks or random stoppages in high costs
states. Such situations are often produced in various real-world scenarios such as engine failures in
autonomous vehicles, network power failures and digital (communication) networks attacks. The
second scenario involves a controller that seeks to avoid system states that yield a high likelihood
3
Under review as a conference paper at ICLR 2020
of systemic (subcomponent) failure. Examples of this case include an agent that seeks to avoid
performing tasks that increase the risk of some system failure, for example increasing stress that
results in component failure or breakages within robotics.
To produce a control that is robust in these scenarios, it is firstly necessary to determine a stopping
rule that stops the system at states that incur the highest overall costs. Applying this stopping rule to
the system subsequently induces a response by the controller that is robust against systemic faults at
states in which stopping inflicts the greatest overall costs. This necessitates a formalism that combines
an OSP to determine an optimal (adversarial) stopping rule and secondly, a RL problem. Hence,
problem we consider is the following:
Find (k,∏) ∈ V X Π and J^π s.th.
max I min Jk,π [s] ) = Jk,π [s],	∀s ∈ S,	(2)
π∈Π k∈V
where the minimisation is taken pointwise and V is a set of stochastic processes of the form
V : Ω → T where T ⊆ {0,1,2 ...} is a set of stopping times.
Hereon, we employ the following shorthand R(s, a) ≡ Rsa for any s ∈ S, a ∈ A.
The dual objective (2) consists of finding both a stopping rule that minimises J and an optimal policy
that maximises J. By considering the tasks as being delegated to two individual players, the problem
becomes an SG between a controller that seeks to maximise J by manipulating state visitations
through its actions and an adversarial stopper that chooses a stopping rule to stop the process in order
to minimise J. We later consider a setting in which neither player has up-front knowledge of the
transition model or objective function but each only observes their realised rewards.
The results of this paper also tackle OSPs under a worst-case transitions — problems in which the
goal is to find a stopping rule T under the adverse non-linear expectation EP := min EPn s.th.
π∈Π
k∧T
X γtR(st, at) + γk∧T G(sk∧T ) .	(3)
t=0
Here, the agent seeks to find an optimal stopping time in a problem in which the system transitions
according to an adversarial (worst-case) probability measure.
1.3	Example: Control with random actuator failure
To elucidate the ideas, we now provide a concrete practical example namely that of actuator failure
within RL applications.
Consider an adaptive learner, for example a robot that uses a set of actuators to perform actions. Given
full operability of its set of actuators, the agent’s actions are determined by a policy π : S × A → [0, 1]
which maps from the state space S and the set of actions A to a probability. In many systems, there
exists some risk of actuator failure at which point the agent thereafter can affect the state transitions
by operating only a subset of its actuators. In this instance, the agent’s can only execute actions
drawn from a subset of its action space A ⊂ A and hence, the agent is now restricted to policies of
the form ∏partiai : S × A → [0,1]——thereafter its expected return is given by the value function
V πpartial (this plays the role of the bequest function G in (1)). In order to perform robustly against
actuator failure, it is therefore necessary to consider a set of stopping times T ⊆ {0, 1, 2, . . .} and a
stopping criterion T : Ω → T which determines the worst states for the agent,s functionality to be
impaired so that it can only use some subset of its set of actuators.
The problem involves finding a pair (T,Π) ∈ V × Π 一—a stopping time and control policy s.th.
min ^max E [hπ0,k0(s)i ) = E [Hπ,τ(s)] ;	∀s ∈ S,
where s := so,at 〜π0 and Hπ,k (s) := Pik=∞ YtR(st, at) + γkk∧∞ Vπpartial (Sk∧∞). Hence the role
of the adversary is to determine and execute the stopping action T that leads to the greatest reduction
in the controller,s overall payoff. The controller in turn learns to execute the policy ∏ which involves
T ∈ arg maxEP
k∈V
4
Under review as a conference paper at ICLR 2020
playing a policy ∏partiai ∈ arg max Vπpartial after the adversary has executed its stopping action.
The resulting policy ∏ is hence robust against actuator failure at the worst possible states.
Embedded within problem (4) is an interdependence between the actions of the players — that is, the
solution to the problem is jointly determined by the actions of both players and their responses to
each other. The appropriate framework to tackle this problem is therefore an SG (Shapley, 1953).
2	Discrete-Time Stochastic Games of control and stopping
In this setting, the state of the system is determined by a stochastic process {st|t = 0, 1, 2, . . .}
whose values are drawn from a state space S ⊆ Rp for some p ∈ N. The state space is defined
on a probability space (Ω, B,P), where Ω is the sample space, B is the set of events and P is
a map from events to probabilities. We denote by F = (Fn)n≥0 the filtration over (Ω, B,P)
which is an increasing family of σ-algebras generated by the random variables s1, s2,   We
operate in a Hilbert space V of real-valued functions on L2, i.e. a complete2 vector space which
we equip with a norm k ∙ k : V → R>o X {0} given by ∣∣f kμ := A/E* [f 2(s)] and its inner product
hf, fTiμ := Eμ [f (s)fT(s)] where μ : B(Rn) → [0,1] is a probability measure. The problem
occurs over a time interval {0, . . . K} where K ∈ N × {∞} is the time horizon. A stopping time
is defined as a random variable T : Ω → {0,..., K} for which {ω ∈ Ω∣τ(ω) ≤ t} ∈ Ft for
any t ∈ {0, . . . , K} — this says that given the information generated by the state process, we can
determine if the stopping criterion has occurred.
An SG is an augmented Markov decision process which proceeds by two players tacking actions that
jointly manipulate the transitions of a system over K rounds which may be infinite. At each round,
the players receive some immediate reward or cost which is a function of the players’ joint actions.
The framework is zero-sum so that a reward for player I simultaneously represents a cost for player II.
Formally, a two-player zero-sum SG is a 6-tuple hS, Ai∈{1,2}, P, R, γi where S = {s1, s2, . . . , sn}
is a setofn ∈ N states, Ai is an action set for each player i ∈ {1, 2}. The map P : S × A1 × A2 × S →
[0, 1] is a Markov transition probability matrix i.e. P(s0; s, a1, a2) is the probability of the state s0
being the next state given the system is in state s and actions a1 ∈ A 1 and a2 ∈ A2 are applied by
player I and player II (resp.). The function R : S × A1 × A2 is the one-step reward for player I and
represents one-step cost for player II when player I takes action a1 ∈ A 1 and player II takes action
a2 ∈ A2 and γ ∈ [0, 1[ is a discount factor. The goal of each player is to maximise its expected
cumulative return — since the game is antagonistic, the total expected reward received by player I
which we denote by J, represents a total expected cost for player II.
Denote by Πi, the space of strategies for each player i ∈ {1, 2} . For SGs with Markovian transition
dynamics, we can safely dispense with path dependencies in the space of strategies.3 Consequently,
w.log. we restrict ourselves to the class of behavioural strategies that depend only on the current state
and round, namely Markov strategies, hence for each player i, the strategy space Πi consists of
strategies of the form πi : S × Ai → [0, 1]. It is well-known that for SGs, an equilibrium exists in
Markov strategies even when the opponent can draw from non-Markovian strategies (Hill, 1979).
In SGs, it is usual to consider the case A1 = A2 so that the players’ actions are drawn from the same
set. We depart from this model and consider a game in which player II can choose a strategy which
determines a time to stop the process contained within the set T ⊆ {0, 1, 2, . . .} which consists of
F - measurable stopping times. In this setting, player I can manipulate the system dynamics by
taking actions drawn from A 1 (we hereon use A) and at each point, player II can decide to intervene
to stop the game.
Let us define by val+ [J] := min max Jk,π the upper value function and by val- [J] :=
k∈V π∈Π
maxmin Jk,π, the lower value function. The upper (lower) value function represents the mini-
π∈Π k∈V
mum payoff that player I (player II) can guarantee itself irrespective of the actions of the opponent.
2A vector space is complete if it contains the limit points of all its Cauchy sequences.
3There are some exceptions for games with payoff structures not considered here for example, limiting
average (Ergodic) payoffs (Blackwell and Ferguson, 1968).
5
Under review as a conference paper at ICLR 2020
The value of the game exists if we can commute the max and min operators:
val- [J] = max min Jk,π = min max Jk,π = val+ [J].	(4)
π∈Π k∈V	k∈V π∈Π
We denote the value by J? := val+ [J] =va「[J] and denote by (k, ∏) ∈ V X Π the pair that
τ^
satisfies Jk,π ≡ J?. The value, should it exist, is the minimum payoff each player can guarantee
itself under the equilibrium strategy. In general, the functions val+ [J] and val- [J] may not coincide.
Should J? exist, it constitutes an SPE of the game in which neither player can improve their payoff by
playing some other control — an analogous concept to a Nash equilibrium for the case of two-player
zero-sum games. Thus the central task to establish an equilibrium involves unambiguously assigning
a value to the game, that is proving the existence of J? .
3	Main Analysis
In this section, we present the key results and perform the main analysis of the paper. Our first task is
to prove the existence of a value of the game. This establishes a fixed or stable point which describes
the equilibrium policies enacted by each player. Crucially, the equilibrium describes the maximum
payoff that the controller can expect in an environment that is subject to adversarial attacks that stop
the system or some subcomponent. Unlike standard SGs with two controllers, introducing a stopping
criterion requires an alternative analysis in which i) an equilibrium with Markov strategies in which
one of the players uses a stopping criterion is determined and ii) the stopping criterion is characterised.
It is well-known that introducing a stopping action to one of the players alters the analysis of SGs the
standard methods of which cannot be directly applied (c.f. Dynkin games (Dynkin, 1967)).
Our second task is to perform an analysis that enables us to construct an approximate dynamic
programming method. This enables the value function to be computed through simulation. This,
as we show in Sec. 4, underpins a simulation-based scheme that is suitable for settings in which
the transition model and reward function is a priori unknown. Lastly, we construct an equivalence
between robust OSPs and games of control and stopping. We defer some of the proofs to the appendix.
Our results develop the theory of risk within RL to cover instances in which the agent has concern
the process at a catastrophic system state. Consequently, we develop the theory of SGs to cover
games of control and stopping when neither player has up-front environment knowledge. We prove
an equivalence between robust OSPs and games of control and stopping and demonstrate how each
problem can be solved in unknown environments.
A central task is to prove that the Bellman operator for the game is a contraction mapping. Thereafter,
we prove convergence to the unique value. Consider a Borel measurable function which is absolutely
integrable w.r.t. the transition kernel P- then E [J[s0]∣Ft] = JS J[s0]P嘉0, where P^, ≡ P(S0; s, a) is
the probability of the state s0 being the next state given the action a ∈ A and the current state is s . In
this paper, we denote by (P J)(s) := S J[s0]Psads0 .
We now introduce the operator of the game which is of central importance:
TJ[s] := min max Rsa +γ X Psas0 Jτ,π[s0],G(s) ,
a∈	s0 ∈S
∀s ∈ S
(5)
The operator T enables the game to be broken down into a sequence of sub minimax problems. It will
later play a crucial role in establishing a value iterative method for computing the value of the game.
We now briefly discuss strategies. A player strategy is a map from the opponent’s policy set to the
player’s own policy set. In general, in two player games the player who performs an action first
employs the use of a strategy. Typically, this allows the player to increase its rewards since their
action is now a function of the other player’s later decisions. Markov controls use only information
about the current state and duration of the game rather than using information about the opponent’s
decisions or the game history. Seemingly, limiting the analysis to Markov controls in the current
game may restrict the abilities of the players to perform optimally.
Our first result however proves the existence of the value in Markov controls:
Theorem 1.
val+ [J] = val- [J] ≡ J?.	(6)
6
Under review as a conference paper at ICLR 2020
Theorem 1 establishes the existence of the game which permits commuting the max and min
operators of the objective (2). Crucially, the theorem secures the existence of an equilibrium pair
(τ, ∏) ∈ V X Π, where ∏ ∈ Π is the controller,s optimal Markov policy when it faces adversarial
attacks that stop the system. Additionally, Theorem 1 establishes the existence of a given by J?, the
computation of which, is the subject of the next section.
We can now establish the optimal strategies for each player. To this end, we now define best-response
strategies which shall be useful for further characterising the equilibrium:
Definition 1. The set of best-response (BR) strategies for player I against the stopping time τ ∈ V
(BR strategies for player II against the control policy π ∈ Π) is defined by:
π ∈ argmax E[Jτ,π'[s]] (resp.,T ∈ argmin E[Jτ',π[s]]),	∀s ∈ S.	(7)
π0∈Π	τ0∈V
The question of computing the value of the game remains. To this end, we now prove that repeatedly
applying T produces a sequence that converges to the value. In particular, the game has a fixed point
property which is stated in the following:
Theorem 2.	1. The sequence (TnJ)n∞=0 converges (in L2).
2.	There exists a unique function J? ∈ L2 s.th.
J? = TJ? and lim TnJ = J?.	(8)
n→∞
Theorem 2 establishes the existence of a fixed point of T and that the fixed point coincides with the
value of the game. Crucially, it suggests that J? can be computed by an iterative application of the
Bellman operator which underpins a value iterative method. We study this aspect in Sec. 4 where we
develop an iterative scheme for computing J? .
Definition 2. The pair (T,Π) ∈ V × Π is an SPE iff:
Jτ,π [s] = max Jτ,π [s] = min Jτ,π [s],	∀s ∈ S.	(9)
π∈Π	τ∈V
An SPE therefore defines a strategic configuration in which both players play their BR strategies.
With reference to the FT RL problem, an SPE describes a scenario in which the controller optimally
responds against stoppages at the set of states that inflict the greatest costs to the controller. In
particular, we will demonstrate that ∏ ∈ Π is a BR to a system that undergoes adversarial attacks.
Proposition 1. The pair (T,Π) ∈ V × Π consists ofBR strategies and constitutes an SPE.
By Prop. 1, when the pair (τ, ∏) is played, each player executes its BR strategy. The strategic
response then induces FT behaviour by the controller. We now turn to the existence and characterising
the optimal stopping time for player II. The following result establishes its existence.
Theorem 3.	There exists an F-measurable stopping time:
T = min {k ∈ TlG(Sk) ≤ minmaxJv,π[sk]} , a.s.
The theorem characterises and establishes the existence of the player II optimal stopping time which,
when executed by the adversary, induces an FT control by the controller.
Having shown the existence of the optimal stopping time τ?, by Theorem 3 and Theorem 1, we find:
Theorem 4. Let T be the player II optimal stopping time defined in (3) and let T? be the optimal
stopping timefor the robust OSP(Cf (3)) then T? = T.
Theorem 4	establishes an equivalence between the robust OSP and the SG of control and stopping
hence, any method that computes T for the SG yields a solution to the robust OSP.
4	Simulation-Based Value Iteration
We now develop a simulation-based value-iterative scheme. We show that the method produces
an iterative sequence that converges to the value of the game from which the optimal controls can
be extracted. The method is suitable for environments in which the transition model and reward
functions are not known to either player.
7
Under review as a conference paper at ICLR 2020
The fixed point property of the game established in Theorem 2 immediately suggests a solution
method for finding the value. In particular, we may seek to solve the fixed point equation (FPE)
J? = T J? . Direct approaches at solving the FPE are not generally fruitful as closed solutions are
typically unavailable. To compute the value function, we develop an iterative method that tunes
weights of a set of basis functions {φk : Rp → R|k ∈ 1, 2, . . . D} to approximate J? through
simulated system trajectories and associated costs. Algorithms of this type were first introduced by
Watkins (Watkins and Dayan, 1992) as an approximate dynamic programming method and have since
been augmented to cover various settings. Therefore the following can be considered as a generalised
Q-learning algorithm for zero-sum controller stopper games.
Let us denote by Φr := PjD=1 r(j)φj an operator representation of the basis expansion. The
algorithm is initialised with weight vector r0 = (r0(1), . . . , r0(P))0 ∈ Rd. Then as the trajectory
{st|t = 0, 1, 2, . . .} is simulated, the algorithm produces an updated series of vectors {rt|t =
0, 1, 2, . . .} by the update:
rt+1 = rt + γφ(st) max Rsat + γmin{(φrt)(st+1), G(st+1)} - (φrt)(st) .
Theorem 5 demonstrates that the method converges to an approximation of J?. We provide a bound
for the approximation error in terms of the basis choice.
We define the function Q? which the algorithm approximates by:
Q?(s) = max Ra + γPJ?[s],	∀s ∈ S	(10)
a∈A s
We later show that Q? serves to approximate the value J? . In particular, we show that the algorithm
generates a sequence of weights rn that converge to a vector r? and that Φr? , in turn approximates
Q? . To complete the connection, we provide a bound between the outcome of the game when the
players use controls generated by the algorithm.
We introduce our player II stopping criterion which now takes the form:
T = min{t∣G(st) ≤ Q*(st)}.	(11)
Let us define a orthogonal projection Π and the function F by the following:
ΠQ := arg min kQ — Qk, FQ := max Ra + YP min{G,Q}.	(12)
Q ∈{ Φr I r ∈ Rp }	a ∈ A
We now state the main results of the section:
Theorem 5.	rn converges to r? where r? is the unique solution: ΠF(Φr?) = Φr?.
The following results provide approximation bounds when employing the projection Π:
Theorem 6.	Let T = min {k ∈ VlG(Sk) ≤ (Φr*)(sk)∣, then thefollowing hold:
kΦr? — Q?k ≤ (p1- Y2)Tk∏Q*- Q?k,	(13)
E [J? - JT，n] ≤ 2[(1— γ)P1-^72『linQ? — Q*k∙	(14)
Hence the error bound in approximation of J? is determined by the goodness of the projection.
Theorem 5 and Theorem 6 thus enable the FT RL problem to be solved by way of simulating the
behaviour of the environment and using the update rule (10) to approximate the value function.
Applying the stopping rule in (11), by Theorem 6 and Theorem 2, means the pair (T, ∏) is generated
where the policy ∏ approximates the policy ∏ which is FT against adversarial stoppages and faults.
Conclusion
In this paper, we tackled the problem of fault-tolerance within RL in which the controller seeks to
obtain a control that is robust against catastrophic failures. To formally characterise the optimal
behaviour, we constructed a new discrete-time SG of control and stopping. We established the
existence of an equilibrium value then, using a contraction mapping argument, showed that the game
can be solved by iterative application of a Bellman operator and constructed an approximate dynamic
programming algorithm so that the game can be solved by simulation.
8
Under review as a conference paper at ICLR 2020
References
Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning 8, 3-4 (1992),
279-292.
Pieter Abbeel, Adam Coates, and Andrew Y Ng. 2010. Autonomous helicopter aerobatics through
apprenticeship learning. The International Journal of Robotics Research 29, 13 (2010), 1608-1639.
Toshiyuki Yasuda, Kazuhiro Ohkura, and Kanji Ueda. 2006. A homogeneous mobile robot team that
is fault-tolerant. Advanced Engineering Informatics 20, 3 (2006), 301-311.
Dapeng Zhang and Zhiwei Gao. 2018. RL-based fault-tolerant control with application to flux cored
wire system. Measurement and Control 51, 7-8 (2018), 349-359.
Enhancing R&D in science-based industry: An optimal stopping model for drug discovery.
International Journal of Project Management 27, 8 (2009), 754-764.
Jerzy A Filar, Todd A Schultz, Frank Thuijsman, and OJ Vrieze. 1991. Nonlinear programming and
stationary equilibria in SGs. Mathematical Programming 50, 1-3 (1991), 227-237.
A Maitra and T Parthasarathy. 1970. On SGs. Journal of Optimization Theory and Applications 5, 4
(1970), 289-300.
Itamar Arel, Cong Liu, T Urbanik, and AG Kohls. 2010. RL-based multi-agent system for network
traffic signal control. IET Intelligent Transport Systems 4, 2 (2010), 128-135.
Fouzia Baghery, Sven Haadem, Bernt 0ksendal, and Isabelle Turpin. 2013. Optimal stopping and
stochastic control differential games for jump diffusions. Stochastics An International Journal of
Probability and Stochastic Processes 85, 1 (2013), 85-97.
Erhan Bayraktar, Xueying Hu, and Virginia R Young. 2011. Minimizing the probability of lifetime
ruin under stochastic volatility. Insurance: Mathematics and Economics 49, 2 (2011), 194-206.
Dimitri P Bertsekas. 2008. Approximate dynamic programming. (2008).
David Blackwell and Tom S Ferguson. 1968. The big match. The Annals of Mathematical Statistics
39, 1 (1968), 159-163.
Peter Carr and Dilip B Madan. 2005. A note on sufficient conditions for no arbitrage. Finance
Research Letters 2, 3 (2005), 125-130.
Jean-Philippe Chancelier, Bernt 0ksendal, and Agnes Sulem. 2002. Combined stochastic control and
optimal stopping, and application to numerical approximation of combined stochastic and impulse
control. 237, 0 (2002), 149-172.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. 2013. A survey on policy search for
robotics. Foundations and TrendsR in Robotics 2, 1-2 (2013), 1-142.
EB Dynkin. 1967. Game variant of a problem on optimal stopping. In Soviet Math. Dokl., Vol. 10.
270-274.
Javier Garcia and Fernando Fernandez. 2012. Safe exploration of state and action spaces in RL.
Journal of Artificial Intelligence Research 45 (2012), 515-564.
Javier Garcia and Fernando Fernandez. 2015. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research 16, 1 (2015), 1437-1480.
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale
Doshi-Velez, and Leo Anthony Celi. 2019. Guidelines for RL in healthcare. Nature medicine 25, 1
(2019), 16-18.
Feng Guo, Carl R Chen, and Ying Sophie Huang. 2011. Markets contagion during financial crisis: A
regime-switching approach. International Review of Economics & Finance 20, 1 (2011), 95-109.
Theodore Preston Hill. 1979. On the existence of good Markov strategies. Trans. Amer. Math. Soc.
247 (1979), 157-176.
9
Under review as a conference paper at ICLR 2020
Christopher Jennison and Bruce W Turnbull. 2013. Interim monitoring of clinical trials: Decision
theory, dynamic programming and optimal stopping. Kuwait Journal of Science 40, 2 (2013).
Ying Jiao and HUyen Pham. 2011. Optimal investment with counterparty risk: a default-density
model approach. Finance and Stochastics 15, 4 (2011), 725-753.
Ioannis Karatzas and William Sudderth. 2006. SGs of control and stopping for a linear diffusion. In
Random Walk, Sequential Analysis And Related Topics: A Festschrift in Honor of Yuan-Shih Chow.
World Scientific, 100-117.
Thomas Kruse and Philipp Strack. 2015. Optimal stopping with private information. Journal of
Economic Theory 159 (2015), 702-727.
David Mguni. 2018. A Viscosity Approach to Stochastic Differential Games of Control and Stopping
Involving Impulsive Control. arXiv preprint arXiv:1803.11432 (2018).
Jun Morimoto and Kenji Doya. 2001. Robust RL. In Advances in Neural Information Processing
Systems. 1061-1067.
Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. 2007. Algorithmic game theory.
Cambridge University Press.
Goran Peskir and Albert Shiryaev. 2006. Optimal stopping and free-boundary problems. Springer.
HUyen Pham. 1997. Optimal stopping, free boundary, and American option in a jump-diffusion
model. Applied Mathematics and Optimization 35, 2 (1997), 145-164.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. 2016. Safe, multi-agent, RL for
autonomous driving. arXiv preprint arXiv:1610.03295 (2016).
Lloyd S Shapley. 1953. SGs. Proceedings of the national academy of sciences 39, 10 (1953),
1095-1100.
Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. 2015. Policy gradient for
coherent risk measures. In Advances in Neural Information Processing Systems. 1468-1476.
John N Tsitsiklis and Benjamin Van Roy. 1999. Optimal stopping of Markov processes: Hilbert
space theory, approximation algorithms, and an application to pricing high-dimensional financial
derivatives. IEEE Trans. Automat. Control 44, 10 (1999), 1840-1851.
Ka-Fai Cedric Yiu. 2004. Optimal portfolios under a value-at-risk constraint. Journal of Economic
Dynamics and Control 28, 7 (2004), 1317-1334.
Virginia R Young. 2004. Optimal investment strategy to minimize the probability of lifetime ruin.
North American Actuarial Journal 8, 4 (2004), 106-126.
Guozhen Zhao and Wen Chen. 2009. Enhancing R&D in science-based industry: An optimal stopping
model for drug discovery. International Journal of Project Management 27, 8 (2009), 754-764.
10
Under review as a conference paper at ICLR 2020
Appendix
Assumptions
Our results are built under the following assumptions:
Assumption A.1. Stationarity: the expectations E are taken w.r.t. a stationary distribution so that for
any measurable function f we have E [f (s)] = E [f (Sk)] for any k ≥ 0 where S := s0.
Assumption A.2. Ergodicity: i) Any invariant random variable of the state process is P-almost
SUrely (P-a.s.) a constant.
Assumption A.3. Markovian transition dynamics: the transition probability function P satisfies the
following equality: P(sk+ι ∈ A|Fk) = P(sk+ι,A) for any A ∈ B(Rp).
Assumption A.4. The constituent functions {R, G} in J are square integrable: that is, R,G ∈ L2(μ).
Additional Lemmata
We begin the analysis with some preliminary lemmata and definitions which are useful for proving
the main results.
Definition A.1. An operator T :V → V is said to be a contraction w.r.ta norm k ∙ k if there exists
a constant c ∈ [0, 1[ s.th for any V1, V2 ∈ V we have that:
kTV1-TV2k ≤ ckV1 -V2k.	(15)
Definition A.2. An operator T : V → V is non-expansive if ∀V1 , V2 ∈ V we have:
kTV1-TV2k ≤ kV1 - V2k.	(16)
Definition A.3. The residual of a vector V ∈ V w.r.t the operator T : V → V is:
T(V):=kTV -Vk.	(17)
Lemma A.1. Define val+ [f] := minb∈B maxa∈A f(a, b) and define
val- [f] := maxa∈A minb∈B f(a, b), then for any b ∈ B we have that for any f, g ∈ L and for any
c ∈ R>0:
max f(a, b) - maxg(a, b) ≤ c =⇒ val- [f] - val- [g] ≤ c.
a∈A	a∈A
Lemma A.2. For any f, g, h ∈ L and for any c ∈ R>0 we have that:
kf - gk ≤ c =⇒ kmin{f, h} - min{g, h}k ≤ c.
Lemma A.3. Let the functions f, g, h ∈ L then
kmax{f,h} - max{g, h}k ≤ kf -gk.	(18)
The following lemma, whose proof is deferred is a required result for proving the contraction mapping
property of the operator T .
Lemma A.4. The probability transition kernel P is non-expansive, that is:
kPV1-PV2k ≤ kV1 -V2k.	(19)
The following estimates provide bounds on the value J? which we use later in the development of
the iterative algorithm. We defer the proof of the results to the appendix.
Proposition A.1. The operator T in (5) is a contraction.
11
Under review as a conference paper at ICLR 2020
Lemma A.5. Let T : V → V be a contraction mapping in ∣∣ ∙ k and let J ? be a fixed point so that
TJ? = J? then there exists a constant c ∈ [0, 1[ s.th:
∣J? - J∣ ≤ (1 - c)-1T(J).	(20)
Lemma A.6. Let T1 : V → V , T2 : V → V be contraction mappings and suppose there exists
vectors J1?, J2? s.th T1 J1? = J1? and T2 J2? = J2? (i.e. J1?, J2? are fixed points w.r.t T1 and T2
respectively) then ∃c1, c2 ∈ [0, 1[ s.th:
kJ? - J?k ≤ (1 -{cι ∧ C2})T M (J) - y(J)).
Lemma A.7. The operator T satisfies the following:
1.	(Monotonicity) For any J1, J2 ∈ L2 s.th. J1(s) ≤ J2(s) then TJ1 ≤ TJ2.
2.	(Constant shift) Let I(s) ≡ 1 be the unit function, then for any J ∈ L2 and for any scalar
α ∈ R, T satisfies T(J + αI)(s) = TJ(s) + αI (s).
Proof of Results
Proof of Lemma A.1. We begin by noting the following inequality for any f : V × V → R, g :
V × V → R s.th. f, g ∈ L we have that for all b ∈ V:
max f(a, b) - max g(a, b) ≤ max |f(a, b) - g(a, b)| .	(21)
a∈V	a∈V	a∈V
From (21) we can straightforwardly derive the fact that for any b ∈ V :
min f(a, b) - min g(a, b) ≤ max |f (a, b) - g(a, b)| ,	(22)
a∈V	a∈V	a∈V
(this can be seen by negating each of the functions in (21) and using the properties of the max
operator).
Assume that for any b ∈ V the following inequality holds:
max |f (a, b) - g(a, b)| ≤ c	(23)
a∈V
Since (22) holds for any b ∈ V and, by (21), we have in particular that
max min f(a, b) - max min g(a, b)
b∈V a∈V	b∈V a∈V
≤ max min f (a, b) - min g(a, b)
≤ max max |f(a, b) - g(a, b)| ≤ c,	(24)
whenever (23) holds which gives the required result.	□
Lemma A.2 and Lemma A.3 are given without proof but can be straightforwardly checked.
Proof of Lemma A.4. The proof is standard, we give the details for the sake of completion. Indeed,
using the Tonelli-Fubini theorem and the iterated law of expectations, we have that:
kPJk2 = E (PJ)2[s0]
=E ([E [J [s1]∣so])2i ≤ E[E[J 2[s1]∣s0]] = EJ 2[si]] = ∣J ∣2,
where We have used Jensen's inequality to generate the inequality. This completes the proof. □
Proof of Proposition A.1. We wish to prove that:
∣TJ - TJkn ≤ YkJ -3.	(25)
12
Under review as a conference paper at ICLR 2020
Firstly, we observe that:
max
a∈A
I Ra+Y X Pas，JT,n[s ι,G(sk)} - (m∈χ Rr++Y X Pss,Jπ[si,G(sk)})
≤ Ymaχ EPssO (JT±[sI - J∏-i[s 1)	≤ YJT-I-JS-Jl ,
a∈A
s，∈ S
using Cauchy-Schwartz (and that Y ∈ [0, 1[) and (30). The result follows after applying Lemma A.2
and Lemma A.3.	□
Proof of Lemma A.5. The proof follows almost immediately from the triangle inequality, indeed for
any J ∈ L2 :
kJ? - Jk = kTJ ? - Jk ≤ Y kJ? - Jk + kTJ - Jk,	(26)
where we have added and subtracted TJ to produce the inequality. The result then follows after
inserting the definition of ET(J).	□
Proof of Lemma A.6. The proof follows directly from Lemma A.5. Indeed, we observe that for any
J ∈ L2 we have
kJ?- J?k ≤kJ? - J k + kJ? - J k,	(27)
where we have added and subtracted J to produce the inequality. The result then follows from Lemma
A.5.	□
Proof of Lemma A.7. Part 2 immediately follows from the properties of the max and min operators.
It remains only to prove part 1.
We seek to prove that for any S ∈ S, if J ≤ J then
min [ max Ra+Y X P北，Jτ,π[s ι, G(ST) ∖
TJIa	S0 ∈ S	J
-Iminlm∈χ Ra + Y X P北，Jn [s 1,g(ST) ∖ ≤ 0
T ∈j a∈	S0 ∈ S	J
We begin by firstly making the following observations:
1.	For any x, y, h ∈ V
x ≤ y =⇒ min{x, h} ≤ min{y, h}.
2.	For any f , g, h ∈ L2
(28)
(29)
max f(x) - max g(x) ≤ max |f (x) - g(x)| .	(30)
Assume that J ≤ J, then We observe that:
max IRa + Y X PasOJt,π[sl) - max IRa + Y X Pss,Jπ[sl)	(31)
a I s0 ∈ S	JaI	s0 ∈ S	J
≤ Y max {X PssO (Jt,π[s'] - Jπ[s 1)}
=Y ((PJ)-(PJ)) ≤ J - J ≤ 0,
Where We have used (30) in the penultimate line. The result immediately folloWs after applying
(29).	□
13
Under review as a conference paper at ICLR 2020
Proof of Theorem 1. We begin by noting the following inequality holds:
val+ [J] = min max E[Jτ,π [s]] ≥ max min E[J τ,π [s]] = val- [J].	(32)
τ∈T π∈Π	π∈Π τ∈T
The inequality follows by noticing Jk,π ≤ max Jk,π and thereafter applying the mink∈T and
π∈Π
maxπ∈Π operators.
The proof can now be settled by reversing the inequality in (32). To begin, choose a sequence
of open intervals {Dm}∞=ι s.th. for each m = 1,2,... Dm is compact and Dm ⊃ Dm+ι and
[0,T] = ∩∞=1Dm and define Td(m) := infk∈Dm E[Jk,π[so]].
We now observe that:
τD (m)
E[Jτ,π[s]]=max E	X Yt(R(St,at) + G0d(m)))
π∈Π t=0
τD (m)
X γt(R(St, at) + G(SτD (m)))
t=τ
τD (m)
≥E JτD(m),π[S] - E X γt(R(St, at) + G(SτD(m)))
t=τ
τD (m)
≥ E JτD(m),π[S] - X γt E[R(St, at)] +E G(SτD(m))
t=τ
τD (m)
≥ E JτD(m),π[s]] — X Yt (E[∣R(S0,∙)∣] + E [∣G(so)∣])
t=τ
1	τ-τD (m)
E IJTD(m)，n[s]J + YTD(m)+1 ——Y---C
lim infE[JτD(m),π[S]] + lim
m→∞
m→∞
YTD (m)+ι I-Yf (m) 一
.	1 - Y 一
c≥E[Jτ,π[S]],
where we have used the stationarity property and, in the limit m → ∞ and, in the last line we used
the FatoU lemma. The constant C is given by C := (E[R(s0, •)] + E[G(so)]) ∈ L.
Hence, we now find that
E[Jτ,π[s]] ≥ E[JT，n[s]].	(33)
Now since (33) holds ∀π ∈ Π we find that:
E[Jτ,π [s]] ≥ max E[JT，n [s]].	(34)
π∈Π
Lastly, applying min operator we observe that:
E[Jτ,π [s]] ≥ min max E[Jτ,π [s]] = val+[J].	(35)
T∈T π∈Π
It now remains to show the reverse ineqUality holds:
E[Jτ,π [s]] ≤ max min E[Jτ,π [s]] = val- [J].	(36)
π∈Π T∈T
Indeed, we observe that
∞
E[JT，n[s]] ≤ min E [Jττ∧mπ[s]] + E X Yt (∣R(st,at)∣ + ∣G(st)∣)	(37)
t=m
≤ lim min E [Jτ八m,π [s]] + c(m)	(38)
m→∞ T∈T
= min E [Jτ,π [s]] ≤ max min E [Jτ,π [s]],	(39)
since Y ∈ [0,1[,where c(m) := -Y-γ(E[∣R(so, ∙)∣] + E[∣G(so)∣]) (using the stationarity of the state
process) and where we have Used LebesgUe’s Dominated Convergence Theorem in the penUltimate
step.
14
Under review as a conference paper at ICLR 2020
Hence, by (39) we have that:
E [Jτ,π[s]] ≤ max min E [Jτ,π[s]] = va「[J].	(40)
π∈Π τ∈T
Hence putting (35) and (40) together gives:
val- [J] = max min E [J τ,π [s]]
≥ E[Jτ,π[s]] ≥ min max E[Jτ,π [s]] = val+[J].	(41)
τ∈T π∈Π
After combining (41) with (32) We deduce the thesis.	□
Proof of Prop. 1. The proposition follows from the fact that if either player plays a Markov strategy
then their opponent,s best-response is a Markov strategy. Moreover, T is a BR strategy for player 2
(recall Definition 3). Moreover, by Theorem 1 (commuting the max and min operators) we observe
that ∏ is a BR strategy for player 1.	□
ProofofTheorem 2. Par 1: We note that the contraction property of T (c.f. Prop. A.1) allows us to
demonstrate that the game has a unique fixed point to which a sequence (TnJ)n∞=0 converges (in L2).
In particular, by Prop. 1 we have that kT2J - TJk ≤ γkTJ - Jk which proves that the sequence
(Tn J)n∞=0 converges to a fixed point.
Part 2: We observe that the fixed point is unique since if ∃J, M ∈ L2 s.th. TJ = J and TM = M
we find that ∣∣M - Jk = ∣∣TM - TJk = YkM - J∣∣, so that M = J (since Y ∈ [0,1[) which gives
the desired result.
Adopting notions in dynamic programming, denote by:
Tn J[s] = min max E
τ∈Tπ0,π1 ,...,πn-1
{n-1∧τ}
X	YtR(st, at) + YnJ(sn∧τ)
t=0
We begin the proof by invoking similar reasoning as (37) - (38) to deduce that:
n
E [J T，n [s]] ≤ min E [Jτ ∧ n，n [s]] + ɪɪ^e,
where C := (E[∣R(s0, ∙)∣] + E[∣G(s0)∣]). Hence,
TnJ[s] ≤ max min E [Jτ,π [s]] +
π∈Π τ∈T
Yn
----C
1-Y
J?[s]+
Yn
--------C.
1 - Y
By analogous reasoning we can deduce that:
TnJ[s] ≥ min max E [Jτ,π [s]] -
τ∈T π∈Π
Yn
J?[s]-
Yn
--------C.
1 - Y
Putting (42) and (43) together implies:
J?[s]-
Yn
1-Y
c ≤ TnJ[s] ≤ J?[s]+
Yn
1---c.
1-Y
(42)
(43)
(44)
-------C
1 - Y
By Lemma A.7, i.e. invoking the monotonicity and constant shift properties of T, we can apply T to
(44) and preserve the inequalities to give:
Yn	Yn
TJ*[s] - J-l~C≤ ≤ Tn+1J[s] ≤ TJ?[s] + l1~Cc-	(45)
After taking the limit in (45) and, using the sandwich theorem of calculus, we deduce the result. □
Proof of Theorem 3. For any m ∈ N we have that:
∞
max J τ,π [s]≥ max Jτ∧m,π[s] - X Ytmax (|R(st, at)| + |G(st)|) .	(46)
t=m
15
Under review as a conference paper at ICLR 2020
We now apply the min operator to both sides of (46) which gives:
∞
min max J τ,π [s] ≥ min max Jτ∧m,π[s] - X γtmax (|R(st, at)| + |G(st)|) .
τ∈T π∈Π	τ∈T π∈Π	π∈Π
t=m
After taking expectations, we find that:
E min max Jτ,π [s]
τ∈T π∈Π
≥ E min max Jτ∧m,π [s]
τ∈T π∈Π
∞
- XγtE max (|R(st,at)| + |G(st)|)
π∈Π
t=m
(47)
(48)
Now by Jensen’s inequality and, using the stationarity of the state process (recall the expectation is
taken under π) we have that:
E max (|R(st, at)| + |G(st)|)
π∈Π
≥ max E [(∣R(st, at)| + ∣G(st)∣)] = E[∣R(so, ∙)∣]+ E[∣G(so)∣].	(49)
π∈Π
By standard arguments of dynamic programming, the value of the game with horizon n can be
obtained from n iterations of the dynamic recursion; in particular, we have that:
min max Jτ∧m,π [s] = TmG(s).	(50)
τ∈T π∈Π
Inserting (49) and (50) into (48) gives:
E min max J τ,π [s] ≥ E [T m G(s)] - c(m)
τ∈T π∈Π
=lim [E[TmG(s)] - c(m)]= E Jτ,π[s]] ,	(51)
m→∞
m
where c(m) := 1l-(E[∣R(so, ∙)∣] + E[∣G(so)∣]) so that lim c(m) = 0. Hence, we find that:
1-γ	m→∞
E [Jτ,π [s]] ≤ E min max Jτ,π[s] ,	(52)
We deduce the result after noting that GJ) = Jτ, ∙ [sτ ] by definition of G.	□
The proofs of the results in Sec. 4 are constructed in a similar fashion that in (Bertsekas, 2008)
(approximate dynamic programming). However, the analysis incorporates some important departures
due to the need to accommodate the actions of two players that operate antagonistically.
We now prove the first of the two results of Sec. 4.
ProofofTheorem 5. We firstly notice the construction of T given by
T = min{t∣G(st) ≤ Q?},	(53)
is sensible since we observe that
min{t|G(st) ≤ J?}
= min{t|G(st) ≤ min{G(st), Q?(st)}
= min{t|G(st) ≤ Q?}.
ReSUlt 1
Step 1 Our first step is to prove the following bound:
∣∣FQ - FQll ≤ γ∣∣Q - Q∣∣.	(54)
16
Under review as a conference paper at ICLR 2020
Proof.
max Ra + YPmin{G, Q}- (max Ra + YPmin{G, Q}) ∣∣
a∈A	s	a∈A s
=γ∣∣Pmin{G,Q} - Pmin{G,Q}∣∣
≤ Y ∣∣min {G,Q} - min {G, Q}∣∣
≤ γ∣∣Q - Q∣∣.
which is the required result.	□
Step2
Our next task is to prove that the quantity Q? is a fixed point of F and hence We can apply the
operator F to achieve the approximation of the value.
Proof. Using the definition of T (c.f. (13) we find that:
J? = TJ? o max Rs + YPJ
a∈A s
= max Rsa + YP min max Rsa + YP J, G
^⇒
Q? = max Rsa + YP min {Q?, G}
a∈A	s
^⇒
Q? = FQ?.
□
Step 3
WenoW prove that the operator ΠF is a contraction on Q, that is the following inequality holds:
∣∣∏FQ - ∏FQ∣∣ ≤ Y∣∣Q - Q∣∣.
Proof. The proof follows straightforwardly by the properties of a projection mapping:
∣∣∏FQ - ∏FQ∣∣ ≤ ∣∣FQ - FQ∣∣ ≤ γ∣∣Q - Q∣∣.
□
Step4
kΦr? - Q?k ≤ -7=1=≡ k∏Q? - Q?k .	(55)
1 - Y 2
The result is proven using the orthogonality of the (orthogonal) projection and by the Pythagorean
theorem. Indeed, we have that:
Proof.
kΦr? - Q?k2 = ∣∣Φr? - ∏Q*k2 + k∏Q? - Q*∣∣2
=k∏FΦr? - ∏Q*k2 + k∏Q? - Q?k2
=k∏FΦr? - ∏Q*k2 + k∏Q? - Q?k2
≤ γ2kΦr? - Q?k2 + k∏Q? - Q?k2.
Hence, we find that
kΦr? - Q?k ≤	k∏Q? - Q?k,
1 - Y 2
which is the required result.
□
17
Under review as a conference paper at ICLR 2020
ResUlt 2
2
E [J?[s]] — E [Jτ,π[s]] ≤ -一ʌ λ——21 k∏Q? — Q?k.	(56)
[(1 - γ) 1 - γ2]
Proof. The proof by Jensen’s ineqUality, stationarity and the non-expansive property of P . In
particUlar, we have
E[J*[s]] - E [JT，n [s]]
=E [PJ?[s]] - E [PJT，n [s]]
≤ ∣E [PJ?[s]] — E[PJT，n [s]]∣
≤kPj — PJτ,π k.	(57)
Inserting the definitions of Q? and Q into (57) then gives:
E[J?[s]]- E[JT，n[s]] ≤ 1 kQ? - Qk.	(58)
It remains therefore to place a boUnd on the term kQ? — Q. We observe that by the triangle ineqUality
and the fixed point properties of F on Q and F on Q we have
~ ~
kQ? - Qk ≤ kQ? - F(Φr*)k + kQ - F(Φr*)k	(59)
≤ Y {kQ* - Φr*k + kQ - Φr*k}	(60)
≤ Y {2kQ? - Φr*k + kQ? - Qk} .	(61)
So that
kQ? - Qk ≤ 12-γkQ? - Φr*k∙	(62)
The result then follows after substituting the result of step 4 (55).	□
Let Us now define the following qUantity:
HQ(s)∙= (G(s)ifG(S) ≤ ®r?)⑸	(63)
HQ(s) := Q(s) otherwise,	(63)
and
FQ := max Ra + γPHQ.	(64)
a∈A	s
Step 5
IlFQ - FQ∣∣ ≤ Y∣∣Q - Q∣∣	(65)
Proof.
∣∣FQ - FQH =回 Ra+YPHQ - (m∈xRa+YPHQ) ∣∣
=Y ∣∣PHQ - PHQ ∣∣
≤ Y∣∣HQ - HQ∣∣
=Y ∣∣min{G, Q} - min{G, Q} ∣∣
≤ y∣∣Q - Q∣∣.
We now prove that Q = max Ra + YPJπ,τ is a fixed point.
a∈A	s
HQ = H fmaχRa + )PJπ,τ)
18
Under review as a conference paper at ICLR 2020
GG(S)	if G(S) ≤ (Φr?)(S)
]maxRa + γPJ7π" otherwise
a∈A s
J n，T
□
Let us now define the following quantity:
S(z, r) := φ(S) max Ra + γ min {(Φr)(y), G(y)} - (Φr)(S) .
a∈A s
Additionally, We define S by the following:
s(z,r) := E [s(zo,r)].
The components of S(z, r) are then given by:
Sk ≡ E φk(S0) max Rsa + γ min {(φr)(S0), G(S0)} - (φr)(S0)	.
a∈A
We now observe that Sk can be described in terms of an inner product. Indeed, using the iterated law
of expectations we have that
Sk ≡ E Φk(S0) max Rsa + γ min {(Φr)(S0), G(S0)} - (Φr)(S0)
= E Φk(S0) max Rsa + γE [min {(Φr)(S0), G(S0)} |S0] - (Φr)(S0)
= E Φk(S0) max Rsa + γPmin{(Φr)(S0), G(S0)} - (Φr)(S0)
= hΦk,F(Φr)-F(Φr)i.
□
Proof of Theorem 6. Step 5 enables us to use classic arguments for approximate dynamic program-
ming. In particular, following step 5, Theorem 6 follows directly from Theorem 2 in (Tsitsiklis &
Van Roy, 1999) with only a minor adjustment in substituting the max operator with min.	□
19