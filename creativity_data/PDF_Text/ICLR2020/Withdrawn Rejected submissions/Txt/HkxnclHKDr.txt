Under review as a conference paper at ICLR 2020
Provab le Representation Learning for Imita-
tion Learning via Bi-level Optimization
Anonymous authors
Paper under double-blind review
Ab stract
A common strategy in modern learning systems is to learn a representation which
is useful for many tasks, a.k.a. representation learning. We study this strategy in
the imitation learning setting for Markov decision processes (MDPs) where mul-
tiple experts’ trajectories are available. We formulate representation learning as
a bi-level optimization problem where the “outer” optimization tries to learn the
joint representation and the “inner” optimization encodes the imitation learning
setup and tries to learn task-specific parameters. We instantiate this framework for
the imitation learning settings of behavior cloning and observation-alone. Theo-
retically, we provably show using our framework that representation learning can
reduce the sample complexity of imitation learning in both settings. We also pro-
vide proof-of-concept experiments to verify our theoretical findings.
1	Introduction
Humans can often learn from experts quickly and with a few demonstrations and we would like
our artificial agents to do the same. However, even for simple imitation learning tasks, the current
state-of-the-art methods require thousand of demonstrations. Humans do not learn new skills from
scratch. We can summarize learned skills, distill them and build a common ground, a.k.a, represen-
tation that is useful for learning future skills. Can we build an agent to do the same?
The current paper studies how to apply representation learning to imitation learning. Specifically,
we want to build an agent that is able learn a representation from multiple experts’ demonstrations,
where the experts aim to solve different Markov decision processes (MDPs) that share the same
state and action spaces but can differ in the transition and reward functions. The agent can use this
representation to reduce the number of demonstrations required for a new imitation learning task.
While several methods have been proposed (Duan et al., 2017; Finn et al., 2017b; James et al., 2018)
to build agents that can adapt quickly to new tasks, none of them, to our knowledge, give provable
guarantees showing the benefit of using past experience. Furthermore, they do not focus on learning
a representation. See Section 2 for more discussions.
In this paper, we propose a framework to formulate this problem and analyze the statistical gains of
representation learning. The main idea is to use bi-level optimization formulation where the “outer”
optimization tries to learn the joint representation and the “inner” optimization encodes the imitation
learning setup and tries to learn task-specific parameters. In particular, the inner optimization is
flexible enough to allow the agent to interact with the environment. This framework allows us to do
a rigorous analysis to show provable benefits of representation learning for imitation learning. With
this framework at hand, we make the following concrete contributions:
•	We first instantiate our framework in the setting where the agent can observe experts’ actions
and tries to find a policy that matches the expert’s policy, a.k.a, behavior cloning. This setting
can be viewed as a straightforward extension of multi-task representation learning for supervised
learning (Maurer et al., 2016). We show in this setting that with sufficient number of experts
(possibly optimizing for different reward functions), the agent can learn a representation that
provably reduces the sample complexity for a new target imitation learning task.
•	Next, we consider a more challenging setting where the agent cannot observe experts’ actions
but only their states, a.k.a., the observation-alone setting. We set the inner optimization as a min-
max problem inspired by Sun et al. (2019). Notably, this min-max problem requires the agent to
interact with the environment to collect samples. We again show that with sufficient number of
1
Under review as a conference paper at ICLR 2020
experts, the agent can learn a representation that provably reduces the sample complexity for a
target task where the agent cannot observe actions from either source experts or the target expert.
•	We conduct experiments to verify our theoretical insights by learning a representation from multi-
ple tasks using our framework and testing it using both behavior cloning and policy optimization.
In these settings, we observe that by learning representations the agent can learn a good policy
with fewer samples than needed to learn a policy from scratch.
The key contribution is to connect existing literature on multi-task representation learning that deals
with supervised learning (Maurer et al., 2016) to single task imitation learning methods with guar-
antees (Syed & Schapire, 2010; Ross et al., 2011; Sun et al., 2019). To our knowledge, this is the
first work showing such guarantees for general losses that are not necessarily convex.
2	Related Work
Representation learning has shown its great power in various domains. See Bengio et al. (2013)
for a survey. Theoretically, Maurer et al. (2016) gave analysis showing representation can provably
reduce the sample complexity in the multi-task supervised learning setting. Recently, Arora et al.
(2019) analyzed the benefit of representation learning via contrastive learning. These papers all build
representations for the agent / learner. We remark that researchers also try to build representations
about the environment / physical world (Wu et al., 2017).
Imitation learning can help with sample efficiency of many problems (Ross & Bagnell, 2010; Sun
et al., 2017; DaUme et al., 2009; Chang et al., 2015; Pan et al., 2018). Most existing work Con-
sider the setting where the learner can observe expert’s action. A general strategy is use supervised
learning to learn a policy that maps the state to action that matches expert’s behaviors. The most
straightforward one is behavior cloning (PomerleaU, 1991), which we also stUdy in oUr paper. More
advanced approaches have also been proposed (Ross et al., 2011; Ross & Bagnell, 2014; SUn et al.,
2018). These approaches, inclUding behavior cloning, often enjoy soUnd theoretical gUarantees in
the single task case. OUr paper extends the theoretical gUarantees of behavior cloning to the mUlti-
task representation learning setting.
This paper also considers a more challenging setting, imitation learning from observation alone.
ThoUgh some model-based methods have been proposed (Torabi et al., 2018; Edwards et al., 2018),
these methods lack theoretical gUarantees. Another line of work learns a policy that minimizes the
difference between the state distribUtions indUced by it and the expert policy, Under a certain distri-
bUtional metric (Ho & Ermon, 2016). SUn et al. (2019) gave a theoretical analysis to characterize
the sample complexity of this approach and oUr method for this setting is inspired by their approach.
A line of work Uses meta-learning for imitation learning (DUan et al., 2017; Finn et al., 2017b; James
et al., 2018). OUr work is different from theirs as we want to explicitly learn a representation that is
UsefUl across all tasks whereas these work try to learn a meta-algorithm that can qUickly adapt to a
new task. For example, Finn et al. (2017b) Used a gradient based method for adaptation. Recently
RaghU et al. (2019) argUed that most of the power of MAML (Finn et al., 2017a) like approaches
comes from learning a shared representation.
On the theoretical side of meta-learning and mUlti-task learning, Baxter (2000) performed the first
theoretical analysis and gave sample complexity boUnds Using covering nUmbers. BUllins et al.
(2019) provides an efficient algorithm that generalizes to new Unseen tasks, bUt for linear repre-
sentations. Another recent line of work analyzes gradient based meta-learning methods, similar to
MAML (Finn et al., 2017a). Existing work on the sample complexity and regret of these meth-
ods (Denevi et al., 2019; Finn et al., 2019; Khodak et al., 2019) show gUarantees for convex losses
by leveraging tools from online convex optimization. In contrast, oUr analysis works for arbitrary
fUnction classes and the boUnds depend on the gaUssian averages of these classes. Recent work
(Rajeswaran et al., 2019) Uses a bi-level optimization framework for meta-learning and improves
compUtation (not statistical) aspects of meta-learning throUgh implicit differentiation.
3	Preliminaries
Markov Decision Processes (MDPs): Let M = (S , A, P, C, ν) be an MDP, where S is the state
space, A is the finite action space with |A| = K, H ∈ Z+ is the planning horizon, P : S × A →
2
Under review as a conference paper at ICLR 2020
4	(S) is the transition function, C : S × A → R is the cost function and ν ∈ 4(S) is the initial
state distribution. We assume that cost is bounded by 1, i.e. C(s, a) ≤ 1, ∀s ∈ S, a ∈ A. This is a
standard regularity condition used in many theoretical reinforcement learning work. A (stochastic)
policy is defined as π = (π1, . . . , πH), where πh : S → 4(A) prescribes a distribution over action
for each state at level h ∈ [H]. For a stationary policy, We have ∏ι = •…=∏h = ∏. A policy
π induces a random trajectory si, aι, s2,a2,..., SH, a，H where si 〜 ν,aι 〜 ∏ι(s), s2 〜 Psι,aι
etc. Let νhπ denote the distribution over S induced at level h by policy π. The value function
Vhπ : S → R is defined as
H
Vn(Sh)= E EC(Si,ai) | ai 〜∏i(si),Si+ι	si,ai
and the state-action function Qn(Sh,ah is defined as Qn(Sh,ah = E§h+i〜尸小…[Vhπ(sh+i)].
The goal is to learn a policy π that minimizes the expected cost J(π) = Esi〜V Vn(si). We define
the Bellman operator at level h for any policy π as Γhn : RS → RS, where for S ∈ S and g ∈ RS,
(Γ∏g)(s) := Ea〜∏h(s),s0〜Ps,a[g(s0)]	(1)
Multi-task Imitation learning: We formally describe the problem we want to study. We as-
sume there are multiple tasks (MDPS) sampled i.i.d. from a distribution η. A task μ 〜η is an
MDP Mμ = (S, A, H, Pμ, Cμ, Vμ) all tasks share everything except the cost function, initial state
distribution and transition function. For simplicity of presentation, we will assume a common tran-
sition function P for all tasks; proofs remain exactly the same even otherwise. For every task μ,
∏μ = (∏ι,μ, ...,∏Hμ) is an expert policy that the learner has access to in the form of trajectories
induced by that policy. The trajectories may or may not contain expert’s actions. These correspond
to two settings that we discuss in more detail in Section 5 and Section 6. The distributions of states
induced by this policy at different levels are denoted by {νjμ,...,νH *} and the average state dis-
H
tribution as	νμ	= H P	νh *.	We define	Vh= *	to be the value function of	∏μ	and	Jμ	to be the
h=i
expected cost function for task μ. We will drop the subscript μ whenever the task at hand is clear
from context. Of interest is also the special case where the expert policy ∏μ is stationary.
Representation learning: In this work, we wish to learn policies from a function class of the form
Π = F ◦ Φ, where Φ ⊆ {φ : S → Rd | kφ(S)k2 ≤ R} is a class of bounded norm representation
functions mapping states to vectors and F ⊆ {f : Rd → ∆(A)} is a class of functions mapping
state representations to distribution over actions. We will be using linear functions, i.e. F = {x →
softmax(W x) | W ∈ RK×d, kW kF ≤ 1}. We denote a policy parametrized by φ ∈ Φ and f ∈ F
by πφ,f, where πφ,f (a|s) = f (φ(s))a. In some cases, we may also use the policy πφ,f (a|s)=
I{a = argmaxf(φ(S))a0}1. Denote Πφ = {πφ,f : f ∈ F} to be the class of policies that use φ as
a0∈A
the representation function.
Given demonstrations from expert policies for T tasks sampled independently from η, we wish to
first learn representation functions (φi, . . . , φH) so that we can use a few demonstrations from an
expert policy π* for new task μ 〜η and learn a policy π = (∏ι,..., ∏h) that uses the learned
representations, i.e. ∏h ∈ Πφh, such that has average cost of π is not too far away from π*. In the
case of stationary policies, we need to learn a single φ by using tasks and learn π ∈ Πφ for a new
task. The hope is that data from multiple tasks can be used to learn a complicated function φ ∈ Φ
first, thus requiring only a few samples for a new task to learn a linear policy from the class Πφ .
Gaussian complexity: As in Maurer et al. (2016), we measure the complexity of a function class
H ⊆ {h : X → Rd} on a set X = (Xi, . . . , Xn) ∈ Xn by using the following Gaussian average
G(H(X)) = E
d,n
sup	γij hi (Xj) | Xj
h∈H i=i
j=i
(2)
where γij are independent standard normal variables. Bartlett & Mendelson (2003) also used Gaus-
sian averages to show some generalization bounds.
1Break ties in any way
3
Under review as a conference paper at ICLR 2020
4	B i-level Optimization Framework
In this section we introduce our framework and give a high-level description of the conditions under
which this framework gives us statistical guarantees. Our main idea is to phrase learning represen-
tations for imitation learning as the following bi-level optimization
minL(φ) := E min 'μ(π)	(3)
φ∈Φ	μ〜η∏∈Πφ
Here 'μ is the inner loss function that penalizes π being different from ∏μ for the task μ. In general,
one can use any loss 'μ that is used for single task imitation learning, e.g. for the behavioral cloning
setting (cf. Section 5), 'μ is a classification like loss that penalizes the mismatch between predictions
by ∏* and ∏, while for the observation-alone setting (cf. Section 6) it is some measure of distance
between the state visitation distributions induced by ∏ and ∏*. The outer loss function is over the
representation φ. The use of bi-level optimization framework naturally enforces policies in the inner
optimization to share the same representation.
While Equation 3 is formulated in terms of the distribution η, in practice we only have access to
few samples for T tasks; let x(1),..., X(T) denote samples from tasks μ(1),..., μ(τ) sampled i.i.d.
from η. We thus learn the representation φ by minimizing empirical version L of Equation 3.
TT
L(Φ) = T X mφ 'x(i) (∏) = T X 'x(i) (∏φ,χ(i))
i=1 π∈	i=1
where 'x is the empirical loss on samples X and ∏φ,x = arg min∏∈∏φ 'x(∏) corresponds to a task
specific policy that uses a fixed representation φ. Our goal then is to show that for a new task μ 〜η,
^
the policy ∏φ,x learned by using samples X from the task μ has low expected cost Jμ, i.e.,
Informal Theorem 4.1. With high probability over the sampling of train task data and with suffi-
cient number of tasks and samples per task,
-,^
E E Jμ(πφ, ) - E Jμ(∏μ) is small
μ〜η X	μ〜η
At a high level, in order to prove such a theorem for a particular choice of 'μ, we would need to
prove the following three properties about 'μ and 'x:
1.	'x(∏) concentrates to 'μ(∏) simultaneously for all ∏ ∈ Πφ (for a fixed φ), with sample complex-
ity depending on some complexity measure of Πφ rather than being polynomial in |S|;
2.	a small value of 'μ(∏) implies a small value for Jμ(∏) - Jμ(∏μ);
3.	if φ and φ0 induce “similar” representations then min∏∈∏Φ 'μ(π) and min∏∈∏φθ 'μ(π) are close.
The first property ensures that learning a policy for a single task by fixing the representation is
sample efficient, thus making representation learning a useful problem to solve. The second property
ensures that matching the behavior of the expert as measured by the loss 'μ ensures low average cost
i.e., 'μ is meaningful for the average cost; any standard imitation learning loss will satisfy this. The
third property is specific to representation learning and requires 'μ to use representations in a smooth
way. This ensures that the empirical loss for T tasks is a good estimate for the average loss on tasks
sampled from η. We prove these three properties for the cases where 'μ is the either behavioral
cloning loss or observation-alone loss, with natural choices for the empirical loss 'x. However the
general proof recipe can be used for potentially many other settings and loss functions.
In the next section, we will describe representation learning for behavioral cloning as an instantiation
of the above framework and describe the various components of the framework. Furthermore we
will describe the results and give a proof sketch to show how the aforementioned properties help
us show our final guarantees. The guarantees for this setting follow almost directly from results
in Maurer et al. (2016) and Ross et al. (2011). Later in Section 6 we describe the same for the
observations alone setting which is more non-trivial.
5 Representation Learning for B ehavioral Cloning
Choice of 'μ: We first specify the inner loss function in the bi-level optimization framework. In
the single task setting, the goal of behavioral cloning (Syed & Schapire, 2010; Ross et al., 2011)
4
Under review as a conference paper at ICLR 2020
is to use expert trajectories of the form τ = (s1, a1, . . . , sH, aH) to learn a stationary policy2 3 that
tries to mimic the decisions of the expert policy on the states visited by the expert. For a task μ,
this reduces to a supervised classification problem that minimizes a surrogate to the following loss
'μ-ι(∏) = Es〜叱。〜∏[(s)I{∏(s) = a}. We abuse notation and denote this distribution over (s, a)
for task μ as μ; so (s, a)〜μ is the same as S 〜ν*, a 〜∏μ(s). Prior work (Syed & Schapire, 2010;
Ross et al.,2011) have shown that a small value of 'μ-1 (∏) implies a small difference J(∏) - J(∏*).
Thus for our setting, We choose 'μ to be of the following form
'μ(π) =	E	'(π(s),a) = E	'(π(s),a)	(4)
s 〜νμ ,a^πμ (S)	(s,a)〜μ
where ` is any surrogate to 0-1 loss I{a 6= arg maxπ(s)a0} that is Lipschitz in φ(s). In this work
a0∈A
we consider the logistic loss '(π(s), a) = - log(n(s)。).
Learning φ from samples: Given expert trajectories for T tasks μ(I),..., μ(τ) we construct a
dataset X = {x(1),..., X(T)}, where x(t) = {(sj, aj)}n=ι 〜(μ(t))n is the dataset for task t.
Details of the dataset construction are provided in Section C.1. Let S denote the set of states {Stj }.
Instantiating our framework, we learn a good representation by solving φ = arg min L(φ), where
φ∈Φ
1 T	1 n	1 T	(t)
L(φ) := τ Σ min 一 £'(n(Sj),aj)=于 E min `	(π)
T	π∈Πφ n j j	T	π∈Πφ
t=1 j =1	t=1
where 'x is loss on samples X = {(sj, a,)}n=ι defined as 'x(π) = ɪ P；=i '(∏(sj), aj).
(5)
Evaluating representation φ: A learned representation φ is tested on a new task μ 〜η as follows:
^	^
draw samples X 〜μn using trajectories from ∏μ and solve πφ,x = arg min 'x(π). Does πφ,x have
π∈Πφ
^
expected cost Jμ(∏φ,x) not much larger than Jμ(∏*)? The following theorem answers this question.
We make the following two assumptions to prove the theorem.
Assumption 5.1. The expert policy ∏μ is deterministic for ev^ry μ ∈ SuPPort(η).
Assumption 5.2 (Policy realizability). There is a representation φ* ∈ Φ such that for every μ ∈
SuPPort(η), ∏μ ∈ Πφ such that ∏μ(s)∏* (s)3 ≥ 1 一 γ, ∀s ∈ S for some Y < 1/2.
The first assumption holds if ∏μ is aiming to maximize some cost function. The second assumption
is for representation learning to make sense: we need to assume the existence of a common repre-
sentation φ* that can approximate all expert policies and Y measures this expressiveness of Φ. Now
we present our first main result.
Theorem 5.1. Let φ ∈ arg min L(φ). Under Assumptions 5.1,5.2, with probability 1 - δ over the
φ∈Φ
sampling of dataset X, we have
E E Jμ(∏φ,x) - E Jμ(∏μ) ≤ H2(2γ + egen)
μ〜η X〜μn	μ〜η
where egen = C G(φ√n)) + C R√√K + C0 ʌz∕ln(4Zδ),for some small constants c, C, C0.
To gain intuition for what the above bound means, we give a PAC-style guarantee for the special
case where the class of representation functions Φ is finite. This follows directly from the above
theorem and the use of Massart’s lemma.
Corollary 5.1. In the same setting as Theorem 5.1, suppose Φ is finite. If number of tasks satis-
fies T ≥cι max{-.尹包)
H 呼/，) }, and number of samples (expert trajectories) per task
satisfies n ≥ c? H，RiK for small constants ci, c2, then with probability 1 一 δ,

E E Jμ(∏φ,x) - E Jμ(∏μ) ≤ H2Y + e
μ〜η X〜μn	μ〜η
2We can easily extend the theory to non-stationary policies
3We abuse notation and use π* (S) instead of arg max π* (s)a
5
Under review as a conference paper at ICLR 2020
Discussion: The above bound says that as long as we have enough tasks to learn a representation
from Φ and sufficient samples per task to learn a linear policy, the learned policy will have small
average cost on a new task from η. The first term H2γ is small if the representation class Φ is
expressive enough to approximate the expert policies (see Assumption 5.2). The results says that
if We have access to data from T = O (H R ]og(lφD) tasks sampled from η, We can use them to
learn a representation such that for a new task we only need n = O (H4R2K) samples (expert
demonstrations) to learn a linear policy With good performance. In contrast, Without access to tasks,
we would need n = O (max { H，R2)og(|回), H4R2K }) samples from the task to learn a good
policy π ∈ Π from scratch. Thus if the complexity of the representation function class Φ is much
more than number of actions (log(∣Φ∣)》K in this case), then multi-task representation learning
might be much more sample efficient4. Note that the dependence of sample complexity on H comes
from the error propagation when going from 'μ to Jμ∖ this is also observed in single task imitation
learning (Ross et al., 2011; Sun et al., 2019).
We give a proof sketch for Theorem 5.1 below, while the full proof is deferred to Appendix A.
5.1 Proof sketch
The proof has two main steps. In the first step we bound the error due to use of samples. The
policy ∏φ,x that is learned on samples X 〜 μ/ is evaluated on the distribution μ and the average loss
incurred by representation φ across tasks is L(φ) = E E 'μ(∏φ,x).
μ〜η X〜μn
On the other hand, if the learner had complete access to the distribution η and distributions μ for ev-
ery task, then the loss minimizer would be φ* = arg minφ∈φ L(φ), where L(φ) := E min 'μ(π).
μ〜η π∈Πφ
ɪ τ ∙	1 ,	Tl r	.	1 ∕CCY X∙∖	,1	11	∙	1	,	?
Using results from Maurer et al. (2016), we can prove the following about φ
Lemma 5.2. With probability 1 - δ over the choice of X, φ ∈ arg min L(φ) satisfies
φ∈Φ
ιa	G(φ({sj})) ɪ 0R√K 00 rln(1∕δ)
L⑷ ≤ minL⑷+c t √n + C F+C V ~τ~
The proof of this lemma is provided in the appendix for completeness.
The second step of the proof is connecting the loss L(φ) and the average cost Jμ of the policies
induced by φ for tasks μ 〜η. This can obtained by using the connection between the surrogate 0-1
loss 'μ and the cost Jμ that has been established in prior work (Ross et al., 2011; Syed & Schapire,
2010). The following lemma uses the result for deterministic expert policies from Ross et al. (2011).
Lemma 5.3. Given a representation φ with L(φ) ≤ e. Let X 〜μn be SamPksfor a new task μ 〜η.
Let πφ,x be the policy learned by behavioral cloning on the samples, then under Assumption 5.1
E E J"(∏φ,x) - E Jμ(∏μ) ≤ H2e
μ〜η X〜μn	μ〜η
This suggests that making L small is good enough. A simple implication of Assumption 5.2 that
minφ∈Φ L(φ) ≤ L(φ*) ≤ γ, along with the above two lemmas completes the proof.
6 Representation Learning for Observation-Alone Setting
Now we consider the setting where we cannot observe experts’ actions but only their states. As in
Sun et al. (2019), we also solve a problem at each level; consider a level h ∈ [H].
Choice of 琮： Let ∏μ = {∏j*, ∙ ∙ ∙, ∏H ,μ} be the sequence of expert policies (possibly stochastic)
at different levels for the task μ. Let ν[* be the distribution induced on the states at level h by
the expert policy ∏μ. The goal in imitation learning with observations alone (Sun et al., 2019) is
4These statements are qualitative since we are comparing upper bounds.
6
Under review as a conference paper at ICLR 2020
to learn a policy π = (∏ι,..., ∏h) that matches the distributions Vn with Vh for every h, w.r.t. a
discriminator class G5 6 that contains the true value functions %*,..., VH and is approximately closed
under the Bellman operator of π*. Instead, in this work we learn π that matches the distributions
∏h ∙ Vh and V力十1for every h w.r.t. to a class G ⊆ {g : S → R, ∣g∣∞ ≤ 1} that contains the value
functions and has a stronger Bellman operator closure property. For every task μ,琮 is defined as
'h (∏)=maχ[ E E	g(s) - E	g(s)]
g∈G S〜νh,μ a〜∏(S)	S〜VUl,μ
S 〜Ps,a
= max[ E E	Kπ(a∣s)g(s) - E g(s)]
g∈G s~νh,μ a〜U(A)	后〜哪+ 1,μ
S 〜Ps,a
(6)
where we rewrite 'μ by importance sampling in the second equation; this will be useful to get
an empirical estimate. While our definition of 'μ differs slightly from the one used in Sun et al.
(2019), using similar techniques, we will show that small values for 'μ(∏h) for every h ∈ [H] will
ensure that the policy π = (∏ι,..., ∏H) will have expected cost Jμ(∏) close to Jμ(∏μ). We abuse
notation, and for a task μ we denote μ = (μι,..., μH) where μh is the distribution of (s, a, S, s)
used in 琮；thus (s, a, S, s)〜 μh is equivalent to S 〜 Vh平,a 〜U (A), S 〜 PS,a, W 〜 V 工+],*.
Learning φh from samples: We assume, 1) access to 2n expert trajectories for T independent
train tasks, 2) ability to reset the environment at any state S and sample from the transition P(∙∣s, a)
for any a ∈ A. The second condition is satisfied in many problems equipped with simulators.
Using the sampled trajectories for the T tasks {μ(1),... ,μ(τ)} and doing some interaction with
environment, we get the following dataset X = {X1, . . . , XH} where Xh is the dataset for level
h. Specifically, Xh = {xh1),..., XHT)} where Xhi = {(sj, aj, Sj, sj)}n=ι 〜(μ(i))n. Additionally
we denote Sh = {sij}iT=,n1,j=1 to be all the s-states in Xh, SSh and Ss h are similarly defined. Details
about how this dataset is constructed from expert trajectories and interactions with environment is
provided in Section C.2. We learn the representation φh = argminLh(φ), where
φ∈Φ
1T	1n	1T
Lh(φ) =亍 E min max - ɪ2[Kπ(aj|sj)g(Sj)- g(sj)]=斤 E min 'h)(n)
T	π∈Πφ g∈G n	j j j	j T	π∈Πφ
i=1	j =1	i=1
1n
where for dataset X = {(sj∙, aj∙, Sj, Sj)}n=ι, 'X(∏) ：= max 1 E [K∏(aj|sj)g(Sj) - g(sj)]. Note
g∈G j=1
that because of the maxg∈g,会 is no longer an unbiased estimator of 'μ when X 〜μn. However
we can still show generalization bounds.
Evaluating representations φι,..., φH: Learned representations are tested on a new task μ 〜
η as follows: get samples X = (xι,..., XH)6 for all levels using trajectories from ∏μ, where
^^
Xh 〜μ⅛. For each level h, learn πφh,xh = argmin∏∈∏φ 'hh(∏) and consider the policy πφ,x =
^^^
(πφ1,x1 , . . . , πφH,xH). Before presenting the guarantee for πφ,x, we introduce a notion of Bellman
error that will show up in our results. For a policy π = (π1, . . . , πH) and an expert policy πh =
(π1h, . . . , πHh ), we define the inherent Bellman error

π
be ：
max max min E
h∈[H] g∈G g0∈G S 〜(Vh+νh )/2
[∣g0(s) - (Γ∏g)(s)∣]
(7)
We make the following two assumptions for the subsequent theorem. These are standard assump-
tions in theoretical reinforcement learning literature.
Assumption 6.1 (Value function realizability). Vh* ∈ G for every h ∈ [H], μ ∈ support(η).
Assumption 6.2 (Policy realizability). There are representations φh,..., φH ∈ Φ such that ∏h μ ∈
Πφh for every h ∈ [H], μ ∈ SuPPort(η).
Now we present our main theorem for the observation-alone setting.
5If G contains all bounded functions, then it reduces to minimizing TV between Vn and Vh.
6 Note that we do not need the datasets xh at different levels to be independent of each other
7
Under review as a conference paper at ICLR 2020
mi. ....--/Y	r,7 一	∙ τ ι, I ∖ rτ 1 λ	,♦	∕τ∕c ♦"	1	1 ∙ι∙. -t C
Theorem 6.1. Let φh ∈ arg min Lh(φ). Under Assumptions 6.1,6.2, with probability 1 - δ over
φ∈Φ
sampling of X = (X1, . . . , XH), we have
H
^
^
^
E E J (π , )— E J (πμ) ≤ 52(2H - 2h + 1)egen,h + O(H 卜巳已
μ〜η X	μ〜η	;一J
h=1
.	φ	Γ Φ,X1	,	,	,
where be = E E[bπe ] is the average inherent Bellman error and
μ〜η x
KG(Φ(Sh))
CgenL O	T 匕 + E E.
T n n	μ〜η X〜μn
KG(G(Sh)) 1 G(G (Sh))
RK √K	∕ln(H∕δ)
+	+ V -ʃ
n
n
We again give a PAC-style guarantee for the special case where the class of representation functions
Φ and value function class G are finite. It follows from the above theorem and Massart’s lemma.
Corollary 6.1. In the setting of Theorem 6.1, suppose Φ, G are finite. If number of tasks satisfies
T ≥ c1
n≥ c2
max
max
H H4R2K2 log(∣Φ∣)
I 目
H4K2 log(∣G∣)
T2
H ln2H∕δ) }, and number of samples (trajectories) per task satisfies
H4R2K3 } for small constants ci, c2, then with probability 1 一 δ,
{
.	2	.	. .	r> . A
E E J(∏φ,x) - E J(∏μ) ≤ O(H2)eφe + e.
μ〜η x	μ〜η
Discussion: As in the previous section, the number of samples required for a new task after learn-
ing a representation is independent of the class Φ but depends only on the value function class G
and number of actions. Thus representation learning is very useful when the class Φ is much more
complicated than G, i.e. R2 log(∣Φ∣)》max{log(∣G∣), R2K}. In the above bounds, eφe is a Bell-
man error term. This type of error terms occur commonly in the analysis of policy iteration type
algorithms (Munos, 2005; Munos & Szepesvari, 2008). We remark that unlike in SUn et al. (2019),
our Bellman error is based on the Bellman operator of the learned policy rather than the optimal
policy. Le et al. (2019) used a similar notion that they call inherent Bellman evaluation error.
The proof of Theorem 6.1 follows a similar outline to that of behavioral cloning. However we cannot
use the results from Maurer et al. (2016) directly since we are solving a min-max game for each task.
We provide the proof in Appendix B.
7 Experiments
In this section we present experimental results on the DirectedSwimmer environment (modified from
the Swimmer environment from OpenAI gym (Brockman et al., 2016)) with Todorov et al. (2012)
simulator and a NoisyCombinationLock environment designed by ourself. These experiments have
two aims: 1) verify the benefit of representation learning predicted by our theory, 2) test the power
of representations learned via our framework in a broader context: we learn a policy for a new task
by using the representation and doing policy optimization instead of imitation learning. In our ex-
periments we learn representations using Equation 5. Experiment details are deferred to Section D.
Our method: Given access to a dataset X = {(stj, atj)}jn=1 of n state-action pairs each for T tasks,
We learn a φ according to Equation 8. For any new task We learn a linear policy ∏ from the class Πφ.
1T1n
φ=arg min fι,mfn ∈f TEn ∑- log(πφ,ft 电jj
t=1 j=1
(8)
Baseline: For a task we learn a policy π from the class Π without learning a representation first.
Verification of theory: In Figure 1 we verify our theoretical findings. On the left, we test on the
DirectedSwimmer environment and report the logistic loss on the validation, which measures how
close the trained policy is to the target expert policy. We find that learning representations, even with
a few experts, can significantly reduce the sample complexity. On the right, we report the average
8
Under review as a conference paper at ICLR 2020
DirectedSwimmer
NoisyCombinationLock
5 OU
ɪ 1
Iun+-ɪə,l
Figure 1:	Experiments for verifying theory. Left: validation loss on DirectedSwimmer. Right:
average return on NoisyCombinationLock
PPO on DirectedSwimmer
----baseline -
一 1 expert ^
—2 expert
——4 expert
----8 expert
----16 expert
Figure 2:	Experiments on policy Optimization with representation trained by imitation learning Left:
average return on the DirectedSwimmer. Right: average return on the NoisyCombinationLock.
reward of the trained policies on the environment. Here we see a different phenomenon: when the
number of experts is small (4 or 16), the baseline method can beat policies trained using represen-
tation learning, though the baseline method requires more samples to do so. When the number of
experts is large (64), we see the policy trained using representation learning can significantly out-
perform the baseline method. This behavior is expected as when the number of experts is small, we
may learn a sub-optimal representation and because we fix this representation for training the policy,
more samples for the test task cannot make this policy better, whereas more samples always make
the baseline method better. Nevertheless, when the number of experts is large, we can significantly
reduce the sample complexity. With 60 samples, the base line method is still far behind the policy
trained using representation learning with 64 experts.
Policy optimization with representations trained by imitation learning: We next test the utility
of representations learned via our framework for RL. After training a representation, we use a simpli-
fied proximal policy optimization method that learns a linear policy over the learned representation.
Results are reported in Figure 2. For DirectedSwimmer and NoisyCombinationLock, we observe
a common pattern. When the number of experts to learn the representation is small, the baseline
method enjoys better performance than the policies trained using representation learning. As the
number of experts to learn the representation increases, we see the policy trained using represen-
tation learning can initially outperform baseline, sometime significantly. However, unsurprisingly,
the baseline method performs very well with a large number of samples, since it is allowed to learn
a representation from scratch. This experiment suggests that representations trained via imitation
learning can be useful beyond imitation learning, especially when the target task has few samples.
8 Conclusion
The current paper proposes a bi-level optimization framework to formulate and analyze representa-
tion learning for imitation learning using multiple demonstrators. Theoretical guarantees are pro-
vided to justify the statistical benefit of representation learning. Some preliminary experiments
verify the effectiveness of the proposed framework. In particular, in experiments, we find the rep-
resentation learned via imitation learning is also useful for policy optimization in the reinforcement
learning setting. We believe it is an interesting theoretical question to explain this phenomenon. Ad-
ditionally, extending this bi-level optimization framework to incorporate methods beyond imitation
learning is an interesting future direction.
9
Under review as a conference paper at ICLR 2020
References
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.
A theoretical analysis of contrastive unsupervised representation learning. In Proceedings of the
36th International Conference on Machine Learning, 2019.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 2003.
Jonathan Baxter. A model of inductive bias learning. J. Artif. Int. Res., 2000.
Y. Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new per-
spectives. IEEE transactions on pattern analysis and machine intelligence, 08 2013.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Brian Bullins, Elad Hazan, Adam Kalai, and Roi Livni. Generalize across tasks: Efficient algo-
rithms for linear representation learning. In Proceedings of the 30th International Conference on
Algorithmic Learning Theory, 2019.
Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, III, and John Langford.
Learning to search better than your teacher. In Proceedings of the 32nd International Confer-
ence on International Conference on Machine Learning - Volume 37, ICML’15. JMLR.org, 2015.
Hal Daume, Iii, John Langford, and Daniel Marcu. Search-based structured prediction. Mach.
Learn., 2009.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In Proceedings of the 36th International
Conference on Machine Learning, 2019.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya
Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances
in Neural Information Processing Systems 30. 2017.
Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Lee Isbell. Imitating latent
policies from observation. arXiv preprint arXiv:1805.07914, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
2017a.
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imita-
tion learning via meta-learning. 09 2017b.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In
Proceedings of the 36th International Conference on Machine Learning, 2019.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NIPS, 2016.
Stephen James, Michael Bloesch, and Andrew Davison. Task-embedded control networks for few-
shot imitation learning. 10 2018.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-
learning methods. arXiv preprint arXiv:1906.02717, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
10
Under review as a conference paper at ICLR 2020
Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Pro-
Ceedings of the 36th International Conference on Machine Learning, pp. 3703-3712, 2019.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. The Journal of Machine Learning Research, 17(1):2853-2884, 2016.
Remi Munos. Error bounds for approximate value iteration. In Proceedings of the 20th National
Conference on Artificial Intelligence - Volume 2, AAAI’05. AAAI Press, 2005.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. J. Mach. Learn.
Res., 2008.
Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos Theodorou,
and Byron Boots. Agile autonomous driving using end-to-end deep imitation learning. In Pro-
ceedings of Robotics: Science and Systems, 2018.
D. A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural
Computation, 3, 1991.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.
Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit
gradients. arXiv preprint arXiv:1906.02717, 2019.
StePhane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings ofthe
thirteenth international conference on artificial intelligence and statistics, pp. 661-668, 2010.
Stephane Ross and J. Andrew Bagnell. Reinforcement and imitation learning via interactive no-
regret learning. arXiv preprint arXiv:1406.5979, 2014.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, 2011.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and
the geometric picture. IEEE Transactions on Information Theory, 63(2):853-884, 2017.
Wen Sun, J. Andrew Bagnell, and Byron Boots. Truncated horizon policy search: Combining
reinforcement learning and imitation learning. arXiv preprint arXiv:1805.11240, 2018.
Wen Sun, Anirudh Vemula, Byron Boots, and J Andrew Bagnell. Provably efficient imitation learn-
ing from observation alone. arXiv preprint arXiv:1905.10948, 2019.
Umar Syed and Robert E Schapire. A reduction from apprenticeship learning to classification. In
Advances in Neural Information Processing Systems 23, pp. 2253-2261. 2010.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. pp. 5026-5033. IEEE, 2012. URL http://dblp.uni-trier.de/db/conf/iros/
iros2012.html#TodorovET12.
Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In IJCAI,
2018.
Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, and Joshua B. Tenenbaum. Learning to see
physics via visual de-animation. In NIPS, 2017.
11
Under review as a conference paper at ICLR 2020
A Proofs for B ehavioral Cloning
We prove Theorem 5.1 in this section by proving Lemma 5.2,5.3. In this section, we abuse notation
and define 'μ(φ, f) := 'μ(∏φ,f), where 'μ is defined in Equation 4. Let fφ
arg min 'x(φ, f) be
the optimal task specific parameter for task μ by fixing representation φ. Thus by our definitions in
^ ^-	^ ^φ
Section 5, we get πφ,x = πφ,fx . We assume w.l.o.g. that A = [K]. Remember that ` : 4(A) × A →
R is defined as `(v, a) = - log(va) for some v ∈ RK and va is the coordinate corresponding to
action a ∈ A = [K]. We define a new function class and loss function that will be useful for our
proofs
F0 = {x→ Wx | W ∈ RK×d, kWkF ≤ 1}
(9)
'0(v, a) = — log(softmax(v)a), V ∈ RK, a ∈ A	(10)
We basically offloaded the burden of computing softmax from the class F to the loss `. We can
convert any function f0 ∈ F0 to one in F by transforming it to softmax(f 0).
We now proceed to proving the lemmas
Proof of Lemma 5.2. We can then rewrite the various loss functions from Section 5 as follows
1T 1n
L(。)= T XmF0 - X '0(f(Φ(s)),a)
i=1	j=1
L(φ) = E min E `0 (f (φ(s)), a)
μ〜η f ∈F0 (s,a)〜μ
φ
L(φ)= EEE '0(f0χ(φ(s)),a)
μ〜η X〜μn (s,a)〜μ
φ
where f0* ∈ arg min f o∈f? 'x(φ, Softmax(f0)). It is easy to show that both ' (∙, a) '0(f0(∙), ∙) are
2-lipschitz in their arguments for every a ∈ A and f0 ∈ F0 . Using a slightly modified version of
Theorem 2(i) from Maurer et al. (2016), we get that for φ ∈ arg minφ∈Φ L(φ), with probability at
least - — δ over the choice ofX
u uu-/ɜ/ɪ ∕ow	E kφ(s)k2	ι~~.	7T77∖
『以、∙ 2fι2 . 2√2πG(φ(S))	kz	μ μ~η,(S,a)~*	∕8log(40
L(φ) - min L(φ) ≤ ————+ √2∏Q SUP ∖ ------------------ + ∖ ——斤——
φ∈Φ	T n	φ∈Φ	n	T
L(φ) - min L(φ) ≤ cG≡+ CQR + c00∕0g≡	(11)
φ∈Φ	T n	n	T
n,K
where Q0 = SuP aE SuP P Yij f 0(yi)j. First We discuss why We need a modified
y∈Rdn \{0} y f∈F 0 i=1,j=1
version of their theorem. Our setting differs from the setting for Theorem 2 from Maurer et al.
(2016) in the following ways
• F0 is a class of vector valued function in our case, whereas in Maurer et al. (2016) it is assumed
to contain scalar valued. The only place in the proof of the theorem where this shows up is in the
definition of Q0, which we have updated accordingly.
• Maurer et al. (2016) assumes that '0(∙, a) is 1-lipschitz for every a ∈ A and that f0(∙) is L lipschitz
for every f0 ∈ F0. However the only properties that are used in the proof of Theorem 16 are that
'0(∙,a) is 1-lipschitz and that '0(f0(∙), a) is L-lipschitz for every a ∈ A, which is exactly the
property that we have. Hence their proof follows through for our setting as well.
n,K
LemmaA.1. Q0:= sup 击 E sup P Yij f(yi)j≤ √K
y∈Rdn∖{0}	f ∈F0 i=1,j = 1
12
Under review as a conference paper at ICLR 2020
Proof.
1	n,K
Q0 =	sup η-∏-E sup	Yijf'5j
y∈Rdn∖{0} kyk f ∈F0 i=fj=ι
1	n,K
sup η-∏-E sup £ YijhWj ,yii
y∈Rdn∖{0} kyk kWkF≤1 i=1j=ι
1	Kn
SuP	∣mγe SuP ΣShW∙,Eγijyi
y∈Rdn∖{0} kyk kWkF≤1 j=1	i=1
≤
sup
y∈Rdn∖{0}
1 uu K
≡ EtjX
n
Yijyi
i=1
sup
y∈Rdn∖{0}
1 uK
≡ tx E
Yijyi
i=1
1u
SuP	η-[rʌ
y∈Rdn∖{0} kyk
Kn	ι
χχ kyi k2 =廊 pKhF = √K
where we use Jensen’s inequality and linearity of expectation for the first inequality and properties
of standard normal gaussian variables for the equality after that.	□
Plugging in Lemma A.1 into Equation 11 completes the proof.
□
We now proceed to prove the next lemma.
ProofofLemma 5.3. Suppose L(φ) = E E 'μ(∏φ,x) ≤ ∈. Consider a task μ 〜η and samples
μ〜η X〜μn
X 〜 μn and let c*(x) = 'μ(∏φ,x) so that L(φ) = E E ∈μ(x). Since ∏μ is deterministic, we get
μ〜η X〜μn
E E I{a = πμ(S)} = E [1 - πφ,x(s)∏*(s)]
S〜νμ a〜∏φ,χ	S〜νμ	μ
≤ SE *[-log(I-(I- π°,x(S)埠(S)))]
=E [- log(πφ,x(s)∏*(s))] = e“(X)
s~ν*	μ
where we use the fact that x ≤ - log(1 - x) for x < 1. for the first inequality. Thus by using
Theorem 2.1 from Ross et al. (2011), we get that Jμ(∏φ,x)-Jμ (∏*) ≤ H 2∈μ(x). Taking expectation
w.r.t. μ 〜η and X 〜μn completes the proof.	□
Proof of Theorem 5.1. By using Assumption 5.2, we first get that
L(<n= μEη∏mΠn* S1Vμ - lθg(π(s)∏* (S))
≤ E E -lθg(∏μ(s)π* (S))
μ〜η S〜ν*	μ
≤ E E - log(1 - Y) ≤ 2Y
μ〜η S〜ν*
where in the last step we used - log(1 - x) ≤ 2x for x < 1/2. Hence from Lemma 5.2 we get
L(φ) ≤ 2γ + Cgen,h, which combining with Lemma 5.3 gives the desired result.	□
13
Under review as a conference paper at ICLR 2020
B Proofs for Observation-Alone
Before proving Theorem 6.1, we introduce the following loss functions, as we did in the proof sketch
for the behavioral cloning setting. We again abuse notation and define 'μ(φ, f) := 'μ(∏φ,f), where
'μ is defined in Equation 6. Let fφ = arg min 'x(Φ, f) be the optimal task specific parameter for
f∈F
task μ by fixing representation φ. As before, we define the following
Lh(Φh) = E E 'μ(Φ,fφh)
μ〜η X〜μn
ʌ
ʌ
We first show a guarantee on the performance of representations (φι,..., Φh) as measured by the
functions Li,..., LH.
Theorem B.1. With probability at least 1 - δ in the draw ofX = (X(1), . . . , X(H)), ∀h ∈ [H]
Lh(φh) ≤ min Lh(φ)+ cEgen,h(φ) + Cegen,h(F, G) + c0 ∖ -' J '
φ∈Φ	T
where gen,h (Φ)
KGT√√Sh)) and egen,h (F, G )= E E	[ KG(*)) + GGnsh^] + R√√K
JVn	μ〜η X〜μn L	J n
We then connect the losses Lh to the expected cost on the tasks.
Theorem B.2. Consider representations (φι,..., Φh ) with L h(φh) ≤ Eh. Let X = (xι,..., XH)
be samples at different levels for a newly sampled task μ 〜 η such that Xh 〜 μ^. Let ∏φ,x =
(πφ1,X1, . . . , πφH,XH) be policies learned using the samples, then under Assumption 6.1,
H
E E J (πφ,x) - E
μ〜η X
μ〜η
J(∏μ) ≤ ∑(2H - 2h +1)Ch + O(H2)eφe
where Ebφe = E E[Ebπeφ,x] is the average inherent Bellman error.
μ〜η X
It is easy to show that under Assumption 6.2, minφ∈Φ Lh (φ) = 0 for every h ∈ [H]. Thus from
(F, G )+c00 产睥
0

Theorem B.1, we get that Lh(φh) ≤ gen,h, where gen,h = gen,h(Φ)+gen,h
T	1 ♦ EI	τ-> C	. 1	.	r 7 T	1	.	.1	/`
Invoking Theorem B.2 on the representations {φh} completes the proof.
B.1 Proof of Theorem B.1
Before proving the theorem, we discuss important lemmas. In yet another abuse of no-
tation, We define 'μ(φ, f, g) =
n
1 P [Kπφ,f (ajlsj)g(sjj) - g(sj)].
j=1
,a,S,S)〜μh∖Kπφf ⑷s)g ⑸-g⑸] and 'h(O,f,g)
Let m x(Φ) = min max %X(φ,f,g) =丽。,度,0夕), m “,x(。)= max 'μ(φ,fφ,g), mμ(φ) =
f∈F g∈G	g∈G
min max 'μ(φ, f, g). Notethat Lh(φ) = E m(φ), L h (φ) = E E m *,x(Φ). Define the distri-
f ∈F g∈G	μ〜η	μ〜η X〜μn
bution Ph where X 〜Ph is the same as μ 〜η and then X 〜μ力
Lemma B.3.
Lemma B.4.
Lemma B.5.
For every O ∈ Φ and h ∈ [H],
E E SUP SUP Px(Φ, f, g) — 'μ(Φ, f, g)] ≤ Egen,h(F, G)
μ〜η X〜μn f ∈F g∈G L	」
With probability 1 - δ, for every O ∈ Φ,
Lh(φ) — E mx(φ) ≤ Egen,h(F, G)
X〜Ph
With probability 1 - δ, for every O ∈ Φ,
E mX(O)- 1 X mx(i) (φ) ≤ egen,h(φ)
X 〜Ph	T
i
log(1)
T
14
Under review as a conference paper at ICLR 2020
We prove these lemmas later. First We prove Theorem B.1 using them. If φh = arg min Lh(φ), then
φ∈Φ
Lh(Bh) - Lh(Oh)
+ (T X m x® (φh) - T X m x® (φh)
ii
+ (1 X mx(i) (φh) - E mχ(φh)
∖ T	X〜Ph
+ E [ E mx(φh) - mμ(φh)]
μ〜η X〜μn
≤ 2gen,h(F, G) + gen,h (Φ)
where for the first part we use Lemma B.4, second part we use Lemma B.5, third part is upper
l log	l lo lo l lθg( 1 )
bounded by 0 by optimality of φh, fourth is upper bounded by O(V Tδ ) by Hoeffding S inequal-
ity and fifth is bounded by the following argument: let fφ, gφ = arg min arg max 'μ(φ, f, g)
...... ʌ.,.,
E mχ(φh) = E minmax'h(φh,f, g)
X〜μn	X〜μn f ∈F g∈G
≤ E max信(。*产,9)= E纵。焉产涉)
X〜μn g∈G	X〜μn
≤ 'μ(Φh,fφh,g)+ S,h(F, G)
≤ 'μ(φh, fφh ,gφh ) + Cgen,hIF, G)= mμ(Bh) + ^gen,h(F, G)
where the second inequality uses Lemma B.3.
B.2 Proof of Theorem B.2
Consider a task μ. For simplicity of notation, we use ∏h instead n°h,Xh, ∏ instead of πφ,x. Let Vn
and Vh be the state distributions at level h induced by πφ,h and π* respectively. Let
h(xh) = max E [ E	g(s0) -	E	g(s0)]
g∈G s~哪 a~∏h	a「“h
S 〜Ps,a	S0〜Ps,a
be the loss of policy πh at level h. By definition, h = E E h(x). Using Lemma C.1 from Sun
μ〜η X〜μh
et al. (2019), we have
HH
J(∏φ,x) - J(∏μ) = X δh = X E h
h=1	h=1 S Vh
E 0	Vhh+1(s0)-	E 0	Vhh+1(s0)
a〜∏h(∙∣s),s 〜Ps,a	a〜π式∙∣s),s 〜Ps,a
Observe that
∆h =	E」 E 0	Vh+ι(s0)-	E 0	Vh+ι(s0)]
s~νh α~∏h(∙∣s),s ~Ps,a	α~π式∙∣s),s ~Ps,a
≤ max E [ E	g(s0) - E	g(s0)]+
g∈G S〜IVh α~∏h(∙∣s),s0~Ps,a	α~π式∙∣s),s0~Ps,a
max[ E Γhπg(s)	- E	Γhπg(s)]	+ [ E ΓhhVhh+1	(s)	- E ΓhhVhh+1 (s)]
g∈G S〜Vh	S〜Vh	S〜Vh	S〜Vh
≤	h(xh) + max[ E Γhπg(s) - E Γhπg(s)] + max[ E g(s) - E g(s)]
g∈G S 〜Vh	S 〜Vh	g∈G S 〜Vh	S 〜Vh
15
Under review as a conference paper at ICLR 2020
Lemma B.6. Defining ∆h =max | E g(s) - E g(s)∣, we have
g∈G S 〜Vh	S 〜Vh
max[ E rhg(s) - E rhg(s)] ≤ & + 2嚷
g∈G s~νh	s~"h
Using the above lemma, we get ∆h ≤ S(Xh) + 2∆h + 2噎.We now bound ∆h
∆h = max E E	g(sr) — E g(s)
g∈G S〜vf_i α〜πh-1	S〜Vh
h - 1	0 c	h
S 〜Ps,α
max
g∈G
E E g(s,)-
S〜Vh η a〜πh-1
h-1	0 D
S 〜Ps,a
E E g(s)
S 〜Vh 1 a 〜πh-1
-S0 〜Ps,a
+ max
g∈G
E E g(s0)-
S 〜Vh 1 a 〜πh-1
-SjPS,a
E g(s)
S〜Vh
max E Γh-ig(s0) - E Γh-ig(s) + S-I(Xh-1)
g∈G S 〜Vh-I	S 〜Vh-I
≤ ∆h-1 + 2嚷 + Eh-I(Xh-1)
ThUS ∆h ≤ 2(h - 1)噎 + Ei：h-i(xi：h-i) and so ∆h ≤ ei：h(xi：h) + Ch-1 (xi：h-i) + (4h - 2)e".
This implies that
H	H
J(∏φ,χ) - J(π*) = X ∆h ≤ X(2H - 2h +1)Eh(Xh) + O(H2)≤φ
h=i	h=i
Taking expectation wrt μ 〜η and X 〜μn completes the proof.
B.3 Proofs OF Lemmas
ProofofLemma B.3. Again we define F0 as in Equation 9. Let '(v,α, β,a) = KSOftmaX(v)0α-
β, andlet 染(φ, f',g)=染(φ, SOftmaX(f0),g) = E	'(f'(φ(S)),g(W),g⑸,α) for f∈ ∈
(S,a,S,S)〜μ h
F0 and similarly define //χ(φ, f0,g) = %X(Φ, SOftmaX(f0), g). Notice that '(∙,α, β,a) is 2K-
lipschitz, '(v, ∙, β, a) is K-lipschitz and '(v, α, ∙, a) is 1-lipschitz, Using Theorem 8(i) from Maurer
et al. (2016), we get that
E E sup sup
μ 〜n X〜μn f ∈fg∈G
卜X(φ,f,g)-琛(φ,f,g)]
E E sup sup
μ 〜n X〜μn f 0∈F0 g∈G
KX (φ,f0,g)―染(Φ,f',g)]
师 EX G('(F0(φ(sh)), G(Sh), G (Sh), a))
n
2√ΠKG(F0(φ(sh)), a) + E E	" √2∏KG(G (Sh)) + √2∏G(G (Sh))
n	μ 〜n X〜μn	n	n
≤ CRKVK + c0 E E	ΓKG(G(Sh)) + G(G(Sh))一
ʌ/n	μ 〜n X〜μn	n	n
≤ Egen,h(F, G)
where we used lipschitzness and Slepian,s lemma for second inequality and a similar computation
to Lemma A.1 for the third.	□
ProofofLemma B.4.
Lh(φ) - E mx(Φ) = E E mm,x(≠) - E E mx(Φ)
________ -	_  _ l Tl ' '	_  . l Tl
X〜Ph
μ〜η X〜μn
μ〜η X〜μn
=E E max 琛(φ,f,g)- m^ eχ(φ,fφ,g)
μ〜η X〜μn g∈G	g∈G
≤ E E max[琛(φ,f,g)-詹(φ,f,g)]
μ〜η X〜μn g∈G
16
Under review as a conference paper at ICLR 2020
≤ EE maχmax['μ(φ, f, g) - %h(φ,f,g)]
μ〜η X〜μn f ∈F g∈G
≤ gen,h(F, G)
where We use the definition of Lh, obviousness for the first inequality and Lemma B.3 for the
last.	□
Proof of Lemma B.5. We wil be using Slepian’s lemma
Lemma B.7 (Slepian’s lemma). Let {X}s∈S and {Y }s∈S be zero mean Gaussian processes such
that
E(Xs -Xt)2 ≤ E(Ys -Yt)2,∀s,t ∈ S
Then
E sup Xs ≤ E sup Ys
s∈S	s∈S
Using Theorem 8(ii) from Maurer et al. (2016), we get that
SUp	E mX(φ) - 1 X mχ(i) (φ) ≤ √2πG(S) + J9ln(2∕S)
φ∈Φ X〜Ph	T	T	V	2T
(12)
where S = {(m^(φ)xι,..., m^(φ)χτ) : φ ∈ Φ}. We bound the Gaussian average of S using Slepian's
lemma. Define two Gaussian processes indexed by Φ as
2K
Xφ = Σ Yim(O)χ(i) and Yφ = √n∑γijkφ(sij)k
ii
For X = {(sj,aj, Sj, Sj)}, consider 2 representations φ and φ0,
(m(φ)x- m(φ0)x)2 = (minmaxex(φ, f,g) - minmax詹(。0,/,9))2
f∈F g∈G	f∈F g∈G
≤ ( SUp 澹(φ, f,g)- ɑh(θ',f,g)l)2
f ∈F,g∈G
2
sUp
f∈F,g∈G
n x[Kπφ,f(叼同 )g(sj) - Kπφ0,f (aj |sj )g(sj)]
j
2
K2	SUp
f ∈F,g∈G
1 X(f (Φ(Sj Xaj- f(Φ'(Sj ))aj)g岛)
j
≤ -- sup X (f (φ(sj ))aj - f (φ(sj ))aj )2
n f∈F j
4K2	4K2
≤	X |O(sj ) - φ (Sj )| =	X(φ(sj )k - φ (Sj )k )
j	j,k
where we prove the first inequality later, second inequality comes from g being upper bounded by 1
and by Cauchy-Schwartz inequality, third inequality comes from the 2-lipschitzness of f.
E(Xφ - Xφ0) = E(m(φ)χ(i) - m(φ0)χ(i))2
i
≤ 4K2 X(φ(sj)k - φ0(sj)k)2 = E(Yφ - Yφo)2
i,j,k
Thus by Slepian’s lemma, we get
2K
G(S) = EsupXφ ≤ Esup Yφ = —G(Φ({sj}))
φ∈Φ	φ∈Φ	n
17
Under review as a conference paper at ICLR 2020
Plugging this into Equation 12 completes the proof. To prove the first inequality above, notice that
miFm∈Gc 蓊@ f，g)- miFm绘欲"f，g)='h(φ, f，g)-" 0，g0)
≤ 信应尸,。〃)-信(Φ0,f0,go)
≤ 信应广,。〃)-信(。0,/0,900)
.ʌ _ , ʌ _ , ..
≤ Sup l'h(Φ,f,g) -'h(Φ,f,g)l
f ∈F,g∈G
1 A	.	1	. . 1	/J-V /If \	♦	/J-V / I ! f \ ,
By symmetry, We also get that min max 'h(φ, f, g) -min max 'h(φ0, f, g) ≤
f ∈F g∈G	f∈F g ∈G
ʌ ..
'h(φ0,f,g)∣.
.ʌ . 、
sup l'h(Φ,f,g)-
f∈F,g∈G
□
ProofofLemma B.6. Let g = arg max E Γ∏g(s) — E Γ∏g(s) and g0 = arg min |g 一
g∈G ∖s 〜Vn	S 〜Vh	J	g∈G
rh 引(Vn+νh)∕2.
maxf E Γ∏g(s) - E Γ∏g(s)∖ = E [γ∏g(s) - E Γ∏g(s)
g∈G ∖s〜Vh	S〜Vh	)	S〜Vh	S〜Vh
≤∣ E g0(s) - E g0(s)∣ + | E [g0(s) - Γ∏g(s)]∣ + | E [g0(s)-Γ∏g(s)]∣
S〜Vh	S〜Vh	S〜Vh	S〜Vh
≤ max | E g(s) - E g(s)∣ + 2 E	[∣g0(s) - Γ∏g(s)]∣]
g∈G S〜Vh	S〜'Vh	S〜(Vh+Vh)/2
≤ ∆h + 2bπe
□
C Data Set Collection Details
C.1 Dataset from trajectories
Given n expert trajectories for a task μ, for each trajectory T = (si, a`,..., SH, aH) we can sample
an h 〜U([H]) and select the pair (sh,ah) from that trajectory7. This gives US n i.i.d. pairs
{(sj,aj)}n=ι for the task μ. We collect this for T tasks and get datasets X(I),..., X(T).
C.2 Dataset from trajectories and interaction
Given 2n expert trajectories for a task μ, we use first n trajectories to get independent samples from
the distributions νj*,..., VH# respectively for the S states in the dataset. Using the next n trajecto-
ries, we get samples from ν0,μ,..., νH-ι,* for the S states in the dataset, and for each such state we
uniformly sample an action a from A and then get a state S from PS,a by resetting the environment to
S and playing action a. We collect this for T tasks and get datasets X(i) = {X(1i), . . . , X(Hi)} for every
i ∈ [T], where each dataset X(hi) a set of n tuples obtained level h. Rearranging, we can construct
the datasets Xh = {X(h1), . . . , X(hT)}.
D Experiment Details
For the policy optimization experiments, we use 4 random seeds to evaluate our algorithm. We
show the results for 1 test environment as the results for other test environments are also showing
the algorithm works but the magnitude of reward might be different, so we do not average the
numbers over different test environments.
7In practice one can use all pairs from all trajectories, even though the samples are not strictly i.i.d.
18
Under review as a conference paper at ICLR 2020
Figure 3: The total rewards by different algorithms in DirectedSwimmer.
Experiment Setup We first describe the construction of the NoisyCombinationLock environment.
The state space is R40 . Each state s is in the form of [sreal , snoise], while sreal ∈ R20 is either a one-
hot vector or a zero vector, and snoise ∈ R20 is sampled from N(0, I). The action space is discrete
and has size 2. For each MDP, We have a sequence of actions a* ∈ [2]20. This is the sequence of
optimal actions. We use different a* to define different environments. The transition model is that:
If sreal = ei for some i and the action is ai*, then sr0eal = ei+1 and We’ll get reWard 1. OtherWise s0real
Will be all zero and the reWard is 0. snoise Will alWays be sampled from the Gaussian distribution.
Note that once sreal is all zero, it Will not change and the reWard Will alWays be 0. The maximum
horiozn is set to 20 and therefore, the optimal policy has return 20. The initial sreal is alWays e1.
The representation has dimension of 10. We limit the function φ to be a linear mapping from R40 to
R10. Although the dimension of representation is smaller than the number of states, there still exists
a linear mapping from states to representation such that We can find a linear optimal policy. For each
expert, We collect 200 state-action pairs to train the representation φ. The trajectories are generated
by the optimal policy.
When training the policy using an RL algorithm, to reduce the impact of initialization, the last full
connected layer is initialized to 0. We use the PPO (Schulman et al., 2017) algorithm to train our
policy With code from DhariWal et al. (2017).
DirectedSwimmer A DirectedSWimmer environment is the same as SWimmer in OpenAI Gym
(Brockman et al., 2016), except the folloWing: the reWard function is parametrized by a direction d
With kdk = 1, and is defined as the traveled distance along the direction d. For each task, We sample
a random direction. The state space is still R8. The original action space in SWimmer is R2, and We
discretize the action space, such that each entry can be only one of {-1, -0.5, 0, 0.5, 1}. We also
reduce the maximum horizon from 1000 to 100. We trained the experts for 1 million steps by PPO
to make sure it converges.
The function φ We use has tWo fully connected layers and tWo ReLU layers. The number of hidden
units is 100, so is the dimension of representation. We also include the total reWards that each algo-
rithm can get in Figure 3. Note even though the baseline has a high validation loss, its performance
can be quite good. This does not indicate a failure of representation learning, but it shoWs that loWer
logistic loss does not alWays imply higher reWard.
Optimization All optimization, including training φ, π and behavior cloning baseline, is done by
Adam (Kingma & Ba, 2014) With learning rate 0.001 until it converges. To solve equation 8, We
build a joint loss over φ and all f’s in each task,
1Tn
L(φ,fl, . . . ,fτ ) = nT XX - log(∏φ,ft (sj)aj).	(13)
n	t=1 j =1
Then We minimize L(φ, f1, . . . , fT) and obtain the optimal φ.
19