Under review as a conference paper at ICLR 2020

ONLINE   META-CRITIC   LEARNING   FOR   OFF-POLICY

ACTOR-CRITIC  METHODS

Anonymous authors

Paper under double-blind review

ABSTRACT

Off-Policy Actor-Critic (Off-PAC) methods have proven successful in a variety of
continuous control tasks.  Normally, the critic’s action-value function is updated
using temporal-difference, and the critic in turn provides a loss for the actor that
trains it to take actions with higher expected return. In this paper, we introduce a
novel and flexible meta-critic that observes the learning process and meta-learns
an additional loss for the actor that accelerates and improves actor-critic learning.
Compared to the vanilla critic, the meta-critic network is explicitly trained to ac-
celerate the learning process; and compared to existing meta-learning algorithms,
meta-critic is rapidly learned online for a single task, rather than slowly over a
family of tasks.  Crucially, our meta-critic framework is designed for off-policy
based  learners,  which  currently  provide  state-of-the-art  reinforcement  learning
sample efficiency.  We demonstrate that online meta-critic learning leads to im-
provements in a variety of continuous control environments when combined with
contemporary Off-PAC methods DDPG, TD3 and the state-of-the-art SAC.

1    INTRODUCTION

Off-policy  Actor-Critic  (Off-PAC)  methods  are  currently  central  in  deep  reinforcement  
learning
(RL) research due to their greater sample efficiency compared to on-policy alternatives.  On-policy
requires new trajectories to be collected for each update to the policy, and is expensive as the 
number
of gradient steps and samples per step increases with task-complexity even for contemporary TRPO
(Schulman  et  al.,  2015),  PPO  (Schulman  et  al.,  2017)  and  A3C  (Mnih  et  al.,  2016)  
algorithms.
Off-policy methods, such as DDPG (Lillicrap et al., 2016), TD3 (Fujimoto et al., 2018) and SAC
(Haarnoja et al., 2018b) achieve greater sample efficiency due to their ability to learn from 
randomly
sampled historical transitions without a time sequence requirement, thus making better use of past
experience.  Their critic estimates the action-value (Q-value) function using a differentiable func-
tion approximator, and the actor updates its policy parameters in the direction of the approximate
action-value gradient.  Briefly, the critic provides a loss to guide the actor, and is trained in 
turn to
estimate the environmental action-value under the current policy via temporal-difference learning
(Sutton et al., 2009). In all these cases the learning algorithm itself is hand-crafted and fixed.

Recently meta-learning, or “learning-to-learn” has become topical as a paradigm to accelerate RL
by learning aspects of the learning strategy, for example, through learning fast adaptation 
strategies
(Finn et al., 2017; Rakelly et al., 2019; Riemer et al., 2019), exploration strategies (Gupta et 
al.,
2018), optimization strategies (Duan et al., 2016b), losses (Houthooft et al., 2018), 
hyperparameters
(Xu et al., 2018; Veeriah et al., 2019),  and intrinsic rewards (Zheng et al., 2018).   However,  
the
majority of these works perform meta-learning on a family of tasks or environments and amortize
this huge cost by deploying the trained strategy for fast learning on a new task.

In this paper we introduce a novel meta-critic network to enhance existing Off-PAC learning frame-
works.   The meta-critic is used alongside the vanilla critic to provide a loss to guide the actor’s
learning.  However compared to the vanilla critic, the meta-critic is explicitly (meta)-trained to 
ac-
celerate the learning process rather than merely estimate the action-value function.   Overall,  the
actor is trained by gradients provided by both critic and meta-critic losses, the critic is trained 
by
temporal-difference as usual, and the meta-critic is trained to generate maximum learning perfor-
mance improvements in the actor.  In our framework, both the critic and meta-critic use randomly
sampled off-policy transitions for efficient and effective Off-PAC learning, providing superior 
sam-

1


Under review as a conference paper at ICLR 2020

ple efficiency compared to existing on-policy meta-learners.  Furthermore, we demonstrate that our
meta-critic can be successfully learned online within a single task. This is in contrast to the 
currently
widely used meta-learning research paradigm – where entire task families are required to provide
enough data for meta-learning, and to provide new tasks to amortize the huge cost of meta-learning.

Essentially our framework meta-learns an auxiliary loss function, which can be seen as an intrinsic
motivation towards optimum learning progress (Oudeyer & Kaplan, 2009). As analogously observed
in several recent meta-learning studies (Franceschi et al., 2018), our loss-learning can be 
formalized
as a bi-level optimization problem with the upper level being meta-critic learning, and lower level
being  conventional  learning.   We  solve  this  joint  optimization  by  iteratively  updating  
the  meta-
critic and base learner online while solving a single task.  Our strategy is thus related to the 
meta-
loss learning in EPG (Houthooft et al., 2018), but learned online rather than offline, and 
integrated
with Off-PAC rather than their on-policy policy-gradient learning.  The most related prior work is
LIRPG (Zheng et al., 2018), which meta-learns an intrinsic reward online.  However, their intrinsic
reward  just  provides  a  helpful  scalar  offset  to  the  environmental  reward  for  on-policy  
trajectory
optimization via policy-gradient (Sutton et al., 2000). In contrast our meta-critic provides a loss 
for
direct actor optimization just based on sampled transitions, and thus achieves dramatically better
sample efficiency than LIRPG reward learning in practice.  We evaluate our framework on several
contemporary continuous control benchmarks and demonstrate that online meta-critic learning can
be integrated with and improve a selection of contemporary Off-PAC algorithms including DDPG,
TD3 and SAC.

2    BACKGROUND  AND  RELATED  WORK

Policy-Gradient  (PG)  Methods.   On-policy  methods  usually  update  actor  parameters  in  the  
di-
rection of greater cumulative reward.   However,  on-policy methods need to interact with the en-
vironment in a sequential manner to accumulate rewards and the expected reward is generally not
differentiable due to environment dynamics.  Even exploiting tricks like importance sampling and
improved application of A2C (Zheng et al., 2018), the use of full trajectories is less effective 
than
off-policy transitions, as the trajectory needs a series of continuous transitions in time.  
Off-policy
actor-critic architectures aim to provide greater sample efficiency by reusing past experience (pre-
viously collected transitions).  DDPG (Lillicrap et al., 2016) borrows two main ideas from Deep Q
Networks (Mnih et al., 2013; 2015):  a big replay buffer and a target Q network to give consistent
targets during temporal-difference backups. TD3 (Twin Delayed Deep Deterministic policy gradient
algorithm) (Fujimoto et al., 2018) develops a variant of Double Q-learning by taking the minimum
value between a pair of critics to limit over-estimation.  SAC (Soft Actor-Critic) (Haarnoja et al.,
2018a;b) proposes a maximum entropy RL framework where its stochastic actor aims to simulta-
neously maximize expected action-value and entropy.  The latest version of SAC (Haarnoja et al.,
2018b) also includes the “the minimum value between both critics” idea in its implementation.

Meta Learning for RL. Meta-learning (a.k.a.  learning to learn) (Santoro et al., 2016; Finn et al.,
2017) has received a resurgence in interest recently due to its potential to improve learning 
perfor-
mance, and especially sample-efficiency in RL (Gupta et al., 2018). Several studies learn optimizers
that provide policy updates with respect to known loss or reward functions (Andrychowicz et al.,
2016; Duan et al., 2016b; Meier et al., 2018). A few studies learn hyperparameters (Xu et al., 2018;
Veeriah et al., 2019), loss functions (Houthooft et al., 2018; Sung et al., 2017) or rewards (Zheng
et al., 2018) that steer the learning of standard optimizers.  Our meta-critic framework is in the 
cat-
egory of loss-function meta-learning, but unlike most of these we are able to meta-learn the loss
function online in parallel to learning a single extrinsic task rather.  No costly offline learning 
on
a task family is required as in Houthooft et al. (2018); Sung et al. (2017).  Most current Meta-RL
methods are based on on-policy policy-gradient, limiting their sample efficiency. For example, while
LIRPG (Zheng et al., 2018) is one of the rare prior works to attempt online meta-learning, it is in-
effective in practice due to only providing a scalar reward increment rather than a loss for direct
optimization. A few meta-RL studies have begun to address off-policy RL, for conventional offline
multi-task meta-learning (Rakelly et al., 2019) and for optimising transfer vs forgetting in 
continual
learning of multiple tasks (Riemer et al., 2019).  The contribution of our Meta-Critic is to enhance
state-of-the-art Off-PAC RL with single-task online meta-learning.

Loss Learning. Loss learning has been exploited in ‘learning to teach’ (Wu et al., 2018) and surro-
gate loss learning (Huang et al., 2019; Grabocka et al., 2019) where a teacher network predicts the

2


Under review as a conference paper at ICLR 2020

parameters of a manually designed loss in supervised learning. In contrast our meta-critic is 
itself a
differentiable loss, and is designed for use in reinforcement learning. Other applications learn 
losses
that improve model robustness to out of distribution samples (Li et al., 2019; Balaji et al., 2018).
Our loss learning architecture is related to Li et al. (2019), but designed for accelerating 
single-task
Off-PAC RL rather than improving robustness in multi-domain supervised learning.

3    METHODOLOGY

We aim to learn a meta-critic that provides an auxiliary loss Lᵃᵘˣ to assist the actor’s learning 
of a
task.  The auxiliary loss parameters ω are optimized in a meta-learning process.  The vanilla critic
Lᵐᵃⁱⁿ and meta-critic Lᵃᵘˣ losses train the actor πφ off-policy via stochastic gradient descent.

3.1    REVIEW OF OFF-POLICY ACTOR-CRITIC RL

Reinforcement  learning  involves  an  agent  interacting  with  the  environment  E.   At  each  
time  t,
the  agent  receives  an  observation  st,  takes  a  (possibly  stochastic)  action  at based  on  
its  policy
π  :                ,  and  receives  a  scalar  reward  rt and  new  state  of  the  environment  
st₊₁.   We  call
(st, at, rt, st₊₁) as a single point transition.  The objective of RL is to find the optimal policy 
πφ,
which maximizes the expected cumulative return J.

In on-policy RL, J is defined as the discounted episodic return based on a sequential trajectory 
over

horizon H: (s₀, a₀, r₀, · · · , sH , aH , rH ). J = Er ,s ∼E,ₐ ∼π ΣΣH    γᵗrtΣ. In the usual 
implemen-

tation of A2C, r is represented by a surrogate state-value V (st) from its critic.  Since J  is 
only a
scalar value, the gradient of J  with respect to policy parameters φ has to be optimized under the
policy gradient theorem (Sutton et al., 2000): ∇φJ(φ) = E [J ∇φ log πφ(at|st)].

In off-policy RL (e.g., DDPG, TD3, SAC) which is our focus in this paper, parameterized policies
πφ can be directly updated by defining the actor loss in terms of the expected return J(φ) and 
taking
its  gradient     φJ(φ),  where  J(φ)  depends  on  the  action-value  Qθ(st, at).   The  main  
loss  Lᵐᵃⁱⁿ
provided by the vanilla critic is thus

Lᵐᵃⁱⁿ = −J(φ) = −Es∼p  Qθ(s, a)|ₐ₌π  ₍s₎,                                       (1)

where we follow the notation in TD3 and SAC that φ and θ denote actors and critics respectively.

The main loss is calculated by a mini-batch of transitions randomly sampled from the replay buffer.
The actor’s policy network is updated as ∆φ  =  α   φLᵐᵃⁱⁿ,  following the critic’s gradient to in-
crease the likelihood of actions that achieve a higher Q-value. Meanwhile, the critic uses 
Q-learning
updates to estimate the action-value function:

θ ← arg min (Qθ(st, at) − rt − γQθ(st₊₁, π(st₊₁))².                               (2)

3.2    ALGORITHM OVERVIEW

Our meta-learning goal is to train an auxiliary meta-critic network Lᵃᵘˣ that in turn enhances actor
learning.  Specifically, it should lead to the actor φ having improved performance on the main task
Lᵐᵃⁱⁿ when following gradients provided by the meta-critic as well as those provided by the main
task. This can be seen as a bi-level optimization problem (Franceschi et al., 2018; Rajeswaran et 
al.,
2019) of the form:


ω = argmin Lᵐᵉᵗᵃ(dvₐl; φ∗)

ω

s.t.  φ∗ = argmin (Lᵐᵃⁱⁿ(dtrn; φ) + Lᵃᵘˣ(dtrn; φ)),

(3)

φ

where we can assume Lᵐᵉᵗᵃ( ) = Lᵐᵃⁱⁿ( ) for now. Here the lower-level optimization trains the actor
φ to minimize both the main task and meta-critic-provided losses on some training samples.  The
upper-level optimization further requires the meta-critic ω to have produced a learned actor φ∗ that
minimizes a meta-loss that measures the actor’s main task performance on a second set of validation

3


Under review as a conference paper at ICLR 2020

Algorithm 1 Online Meta-Critic Learning for Off-PAC RL

φ, θ, ω,                                                            // Initialize actor, critic, 
meta-critic and buffer

for each iteration do

for each environment step do

at ∼ πφ(at|st)                                        // Select action according to the current 
policy

st₊₁ ∼ p(st₊₁|st, at), rt                    // Observe reward rt and new state st₊₁

endDf←or D ∪ {(st, at, rt, st₊₁)}              // Store the transition in the replay buffer

for each gradient step do

θ      θ     λ   θJQ(θ)                               // Update the critic parameters

meta-train:

Sample mini-batch dtrn from D

Lᵐᵃⁱⁿ ← Eqs. (1),  (8) or (9)                // Main actor loss

Lᵃᵘˣ ← Eqs. (6) or (7)                          // Auxiliary actor loss from meta-critic


ω

φold

= φ − α∇φ

Lᵐᵃⁱⁿ                            // Update actor according to vanilla critic only


φnew  = φold − α∇φLaux

// Update actor according to meta-critic


meta-test:

Sample mini-batch dvₐl from
Lᵐᵉᵗᵃ(dvₐl; φₒld, φnₑw)      Eq. (5)
meta-optimization

// Meta-loss: Did meta-critic improve performance?

φ ← φ − η(∇φLᵐᵃⁱⁿ + ∇φLᵃᵘˣ)            // Update actor parameters


ω      ω     η

end for
end for=0

ω

ω Lmeta

// Update meta-critic parameters

samples, after being trained by the meta-critic.  Note that in principle the lower-level 
optimization
could purely rely on Lᵃᵘˣ  analogously to the procedure in EPG (Houthooft et al., 2018),  but we
find that optimizing their linear combination greatly increases learning stability and speed.  Eq. 
(3)
is satisfied when the meta-critic successfully improves the actor’s performance on the main task as
measured by meta-loss. Note that the vanilla critic update is also in the lower loop, but as it 
updates
as usual, so we focus on the actor and meta-critic optimization for simplicity of exposition.

In this setup the meta-critic is a neural network hω(dtrn; φ) that takes as input some 
featurisation of
the actor φ and the states and actions in dtrn.  This auxiliary neural network must produce a scalar
output, which we can then treat as a loss Lᵃᵘˣ  := hω, and must be differentiable with respect to φ.
We next discuss the overall optimization flow, and discuss the specific meta-critic architecture 
later.

Meta-Optimization  Flow.   To  optimize  Eq.  (3),
we iteratively update the meta-critic parameters ω
(upper-level) and actor and vanilla-critic parame-
ters φ and θ  (lower-level).  At each iteration,  we
perform:  (i) Meta-train:  Sample a mini-batch of
transitions and putatively update policy φ accord-
ing to the main Lᵐᵃⁱⁿ and meta-critic Lᵃᵘˣ losses.

(ii) Meta-test: Sample another mini-batch of tran-
sitions to evaluate the performance of the updated
policy according to Lᵐᵉᵗᵃ. (iii) Meta-optimization:
Update the meta-critic parameters ω to maximize
the performance on the validation batch, and per-
form the real actor update according to both losses.
In this way the meta-critic is trained online and in
parallel to the actor so that they co-evolve.   Fig-
ure   1 and Algorithm 1 summarize the process and


the details of each step are explained next.

Updating Actor Parameters (φ).    During meta-
train, we randomly sample a mini-batch of transi-
tions dtrn = {(si, ai, ri, si₊₁)} with batch size N

from the replay buffer D. We then update the pol-

4

Figure 1:  Meta-critic for Off-PAC. The agent uses
data  sampled  from  the  replay  buffer  during  meta-
train and meta-test.   Actor parameters are first up-
dated using only vanilla critic, or both vanilla- and
meta-critic.  Meta-critic parameters are updated by
the meta-loss.


Under review as a conference paper at ICLR 2020


icy using both losses as:  φ

main                                             aux

=  φ − η                    − η      ω                 .  We also compute a separate


new

∂φ                          ∂φ


update φ

old

= φ − η ∂Lmain(dtrn)  that only makes use of the vanilla loss. If the meta-critic provided

a beneficial source of loss, φnₑw should be a better parameter than φ, and in particular it should 
be
a better parameter than φₒld. We will use this comparison in the next meta-test step.

Updating  Meta-Critic  Parameters  (ω).   To  train  the  meta-critic  network,  we  sample  
another

mini-batch  of  transitions:  dvₐl  =  {(sᵛᵃˡ, aᵛᵃˡ, rᵛᵃˡ, sᵛᵃˡ  )} with  batch  size  M .   The  
use  of  a  val-

idation  batch  for  bi-level  meta-optimization  (Franceschi  et  al.,  2018;  Rajeswaran  et  
al.,  2019)
ensures  the  meta-learned  component  does  not  overfit.   Since  our  framework  is  off-policy, 
 this
does not incur any sample-efficiency cost.   The meta-critic is then updated by a meta loss ω
argmin Lᵐᵉᵗᵃ(dvₐl; φnₑw), which could in principle be the same as the main loss Lᵐᵉᵗᵃ  =  Lᵐᵃⁱⁿ.

ω

However, we find it helpful for optimization efficiency to optimize the (monotonically related) 
dif-

ference between the updates with- and without meta-critic’s input. Specifically, we use

Lᵐᵉᵗᵃ = tanh(Lᵐᵃⁱⁿ(dvₐl; φnₑw) − Lᵐᵃⁱⁿ(dvₐl; φₒld)),                               (4)
which is simply a re-centering and re-scaling of Lᵐᵃⁱⁿ. This leads to

ω ← argmin tanh(Lᵐᵃⁱⁿ(dvₐl; φnₑw) − Lᵐᵃⁱⁿ(dvₐl; φₒld)).                           (5)

Note that here the updated actor φnₑw has dependence on the feedback given by meta-critic ω and
φₒld does  not.   Thus  only  the  first  term  is  optimized  for  ω.   In  his  setup  the  
Lᵐᵃⁱⁿ(dvₐl; φnₑw)
term should obtain high reward/low loss on the validation batch and the latter provides a baseline,
analogous to the baseline commonly used to accelerate and stabilize policy-gradient RL. The use of
tanh reflects the idea of diminishing marginal utility, and ensures that the meta-loss range is 
always
nicely distributed in [   1, 1]. In essence, the meta-loss is for the agent to ask itself the 
question based
on the validation batch, “Did meta-critic improve the performance?”, and adjusts the parameters of
meta-critic accordingly.

Designing Meta-Critic (hω).   The meta-critic network hω implements the auxiliary loss for the
actor.  The design-space for hω has several requirements:  (i) Its input must depend on the policy
parameters φ, because this auxiliary loss is also used to update policy network. (ii) It should be 
per-
mutation invariant to transitions in dtrn, i.e., it should not make a difference if we feed the 
randomly
sampled transitions indexed [1,2,3] or [3,2,1]. The most naive way to achieve (i) is given in 
MetaReg
(Balaji et al., 2018) which meta-learns a parameter regularizer: hω(φ) =     i ωi φi .  Although 
this

form of hω acts directly on φ, it does not exploit state information, and introduces a large number

of parameters as φ, and then hω may be a high-dimensional neural network.  Therefore, we design
a more efficient and effective form of hω that also meets both of these requirements.  Similar to
the feature extractor in supervised learning, the actor needs to analyse and extract information 
from
states for decision-making.  We assume the policy network can be represented as πφ(s) = πˆ(π¯(s))
and decomposed into the feature extraction π¯φ and decision-making πˆφ (i.e., the last layer of the 
full
policy network) modules. Thus the output of the penultimate layer of full policy network is just the
output of feature extraction π¯φ(s), and such output of feature jointly encodes φ and s.  Given this
encoding, we implement hw(dtrn; φ) as a three-layer multi-layer perceptron (MLP) whose input is
the extracted feature from π¯φ(s). Here we consider two designs for meta-critic (hω): using our 
joint
feature alone (Eq. (6)) or augmenting the joint feature with states and actions (Eq. (7)):

N

 1  Σ

(i)        h   (d     ; φ) =             MLP  (π¯  (s )),                                     (6)


w     trn

N

i=1

N

ω     φ    i

 1  Σ

(ii)        h   (d     ; φ) =             MLP  (π¯  (s ), s , a ).                               
(7)

hω is to work out the auxiliary loss based on such batch-wise set-embdedding (Zaheer et al., 2017)
of our joint actor-state feature.  That is to say, dtrn is a randomly sampled mini-batch transitions
from the replay buffer, and then the s (and a) of the transitions are inputted to the hω network in
a permutation invariant way, and finally we can obtain the auxiliary loss for this batch dtrn.  
Here,
our design of Eq. (7) also includes the cues features in LIRPG and EPG where si and ai are used as
the input of their learned reward and loss respectively. We set a softplus activation to the final 
layer
of hω, following the idea in TD3 that the vanilla critic may over-estimate and so the introduction 
of
a non-negative actor auxiliary loss can  mitigate such over-estimation.  Moreover, we point out 
that

5


Under review as a conference paper at ICLR 2020

only si (and ai) from dtrn are used when calculating Lᵐᵃⁱⁿ and Lᵃᵘˣ for the actor, while si, ai, ri

and si₊₁ are all used for optimizing the vanilla critic.

Implementation on DDPG, TD3 and SAC. Our meta-critic module can be incorporated in the
main Off-PAC methods DDPG, TD3 and SAC. In our framework, these algorithms differ only in
their definitions of Lᵐᵃⁱⁿ, and the meta-critic implementation is otherwise exactly the same for 
each.
Further implementation details can be found in the supplementary material.

TD3 (Fujimoto et al., 2018) borrows the Double Q-learning idea and use the minimum value between
both critics to make unbiased value estimations. At the same time, computational cost is obtained by
using a single actor optimized with respect to Qθ . Thus the corresponding Lᵐᵃⁱⁿ for actor becomes:


Lmain  = −Es∼p

Qθ (s, a)|ₐ₌π  ₍s₎.                                              (8)

In  SAC,  two  key  ingredients  are  considered  for  the  actor:   maximizing  the  policy  
entropy  and
automatic temperature hyper-parameter regulation.   At the same time,  the latest version of SAC
(Haarnoja et al., 2018b) also draws lessons from “taking the minimum value between both critics”.
The Lᵐᵃⁱⁿ for SAC actor is:

Lᵐᵃⁱⁿ = Es∼p  [α log (πφ(a|s)) − Qθ(s, a)|ₐ₌π  ₍s₎].                                (9)

4    EXPERIMENTS  AND  EVALUATION

The goal of our experimental evaluation is to demonstrate the versatility of our meta-critic module
in integration with several prior Off-PAC algorithms, and its efficacy in improving their respective
performance.   We  use  the  open-source  implementations  of  DDPG,  TD3  and  SAC  algorithms  as
our baselines,  and denote their enhancements by meta-critic as DDPG-MC, TD3-MC, SAC-MC
respectively.   All  -MC  agents  have  both  their  built-in  vanilla  critic,  and  the  
meta-critic  that  we
propose. We take Eq. (6) as the default meta-critic architecture hω, and we compare the alternative 
in
the later ablation study. For our implementation of meta-critic, we use a three-layer neural network
with an input dimension of π¯ (300 in DDPG and TD3, 256 in SAC), two hidden feed-forward layers
of 100 hidden nodes each, and ReLU non-linearity between layers.

We evaluate the methods on a suite of seven MuJoCo continuous control tasks (Todorov et al., 2012)
interfaced through OpenAI Gym (Brockman et al., 2016) and HalfCheetah and Ant (Duan et al.,
2016a) in rllab. We use the latest V2 tasks instead of V1 used in TD3 and the old implementation of
SAC (Haarnoja et al., 2018a) without any modification to their original environment or reward.

Implementation Details. For DDPG, we use the open-source implementation “OurDDPG” ¹ which
is  the  re-tuned  version  of  DDPG  implemented  in  Fujimoto  et  al.  (2018)  with  the  same  
hyper-
parameters of the actor and critic.   For TD3 and SAC, we use the open-source implementations
of     TD3 ² and SAC ³. In each case we integrate our meta-critic with learning rate 0.001. The 
specific
pseudo-codes can be found in the supplementary material.

4.1    EVALUATION OF META-CRITIC OFF-PAC LEARNING

DDPG  Figure  2  shows  the  learning  curves  of  DDPG  and  DDPG-MC.  The  experimental  results
corresponding to each task are averaged over 5 random seeds (trials) and network initialisations,
and  the  standard  deviation  confidence  intervals  are  represented  as  shaded  regions  over  
the  time
steps. Following Fujimoto et al. (2018), curves are uniformly smoothed (window size 30) for clarity.
We run the gym-MuJoCo experiments for 1-10 million depen ding on to environment,  and rllab
experiments for 3 million steps.  Every 1000 steps we evaluate our policy over 10 episodes with no
exploration noise.

From the learning curves in Figure 2, we can see that DDPG-MC generally outperforms the corre-
sponding DDPG baseline in terms of the learning speed and asymptotic performance. Furthermore,
it usually has smaller variance.   The summary results for all nine tasks in terms of max average
return are given in Table 1.  We selected the six tasks shown in Figure 2 for plotting, because the

¹https://github.com/sfujim/TD3/blob/master/OurDDPG.py
²https://github.com/sfujim/TD3/blob/master/TD3.py

³https://github.com/pranz24/pytorch-soft-actor-critic

6


Under review as a conference paper at ICLR 2020


10000

8000

6000

4000

2000

0

HalfCheetah-v2

DDPG
DDPG-MC

2000

1750

1500

1250

1000

750

500

250

0

DDPG
DDPG-MC

Hopper-v2

3500

3000

2500

2000

1500

1000

500

0

DDPG
DDPG-MC

Walker2d-v2


0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)


4000

3000

2000

1000

0

1000

DDPG
DDPG-MC

Ant-v2

6000

5000

4000

3000

2000

1000

0

HalfCheetah (rllab)

DDPG
DDPG-MC

2500

2000

1500

1000

500

0

DDPG
DDPG-MC

Ant (rllab)


0         2         4         6         8        10

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

Figure 2: Learning curve mean and standard-deviation of vanilla DDPG and meta-critic enhanced 
DDPG-MC
for continuous control tasks.

Table 1: Max Average Return over 5 trials over all time steps. Max value for each task is bolded.


Environment
HalfCheetah
Hopper
Walker2d

Ant
Reacher
InvPend
InvDouPend

HalfCheetah(rllab)
Ant(rllab)

DDPG     DDPG-MC

8440.2        10187.5

2097.5          3253.6

2920.1          3753.7

2375.4          3661.1

-3.6              -3.7

1000.0          1000.0

9307.5          9326.5

5860.8          6254.6

2300.8          2721.1

TD3        TD3-MC

12735.7      15064.0

3807.0        3854.3

5942.7        5955.5

5914.8        6280.0

-3.0             -2.9

1000.0        1000.0

9357.4        9358.8

8029.6        8552.1

3672.6        4776.8

SAC        SAC-MC

16651.8       16815.9

3610.6         3738.4

6398.8         7164.9

6954.4         7204.3

-2.8              -2.7

1000.0         1000.0

9359.6         9359.6

10011.0       10597.0

8014.8         8353.8

PPO       PPO-LIRPG

2061.5           1882.6

3762.0           2750.0

4432.6           3652.9

684.2              23.6

-6.08              -7.53

988.2             971.6

7266.0           6974.9

-                     -

-                     -

other MuJoCo tasks “Reacher”, “InvertedPendulum” and “InvertedDoublePendulum” have an envi-
ronmental reward upper bound which all methods reach quickly without obvious difference between
them. Table 1 shows that DDPG-MC provides consistently higher max return for the tasks without
upper bounds.

TD3 and SAC Figure 3 reports the learning curves for TD3.  For some tasks vanilla TD3 perfor-
mance declines in the long run,  while our TD3-MC shows improved stability with much higher
asymptotic performance.   Generally speaking,  the learning curves show that TD3-MC providing
comparable or better learning performance in each case, while Table 1 shows the clear improvement
in         the max average return. Figure 4 report the learning curves of SAC. Note that we use the 
most re-
cent update of SAC (Haarnoja et al., 2018b), which can be regarded as the combination SAC+TD3.
Although this SAC+TD3 is arguably the strongest existing method, SAC-MC still gives a clear boost
on    the asymptotic performance for several of the tasks.

Comparison vs PPO-LIRPG Intrinsic Reward Learning for PPO (Zheng et al., 2018) is the  most
related  method  to  our  work  in  performing  online  single-task  meta-learning  of  an  
auxiliary  re-
ward/loss via a neural network.  The original PPO-LIRPG study evaluated on a modified environ-
ment with hidden rewards. Here we apply it to the standard unmodified learning tasks that we aim to
improve. The results in Table 1 demonstrate that: (i) In this conventional setting, PPO-LIRPG wors-
ens rather than improves basic PPO performance.  (ii) Overall Off-PAC methods generally perform
better than on-policy PPO for most environments. This shows the importance of our meta-learning
contribution to the off-policy setting.  In general Meta-Critic is preferred compared to PPO-LIRPG
because  the  latter  only  provides  a  scalar  reward  bonus  only  influences  the  policy  
indirectly  via
policy-gradient updates, while Meta-Critic provides a direct loss.

Summary Table 1 and Figure 5 summarize all the results in terms of max average return. We can see
that SAC-MC always performs best; the Meta-Critic-enhanced methods are generally comparable

7


Under review as a conference paper at ICLR 2020


16000

14000

12000

10000

8000

6000

4000

2000

0

HalfCheetah-v2

TD3
TD3-MC

3500

3000

2500

2000

1500

1000

500

0

Hopper-v2

TD3
TD3-MC

5000

4000

3000

2000

1000

0

Walker2d-v2

TD3
TD3-MC


0         2         4         6         8        10

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)


6000

5000

4000

3000

2000

1000

0

Ant-v2

TD3
TD3-MC

8000

6000

4000

2000

0

HalfCheetah (rllab)

TD3
TD3-MC

4000

3000

2000

1000

0

TD3
TD3-MC

Ant (rllab)


0         2         4         6         8        10

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

Figure 3:  Learning curve mean and standard-deviation of vanilla TD3 and meta-critic enhanced 
TD3-MC for
continuous control tasks.


16000

14000

12000

10000

8000

6000

4000

2000

0

HalfCheetah-v2

SAC
SAC-MC

3000

2500

2000

1500

1000

500

Hopper-v2

SAC
SAC-MC

7000

6000

5000

4000

3000

2000

1000

0

Walker2d-v2

SAC
SAC-MC


0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

0         2         4         6         8        10

Time Steps (1e6)


7000

6000

5000

4000

3000

2000

1000

0

Ant-v2

SAC
SAC-MC

10000

8000

6000

4000

2000

0

HalfCheetah (rllab)

SAC
SAC-MC

8000

7000

6000

5000

4000

3000

2000

1000

0

Ant (rllab)

SAC
SAC-MC


0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

Figure 4: Learning curve mean and standard-deviation of vanilla SAC and meta-critic enhanced SAC-MC 
for
continuous control tasks.

or better than their corresponding vanilla alternatives; and Meta-Critic usually provides improved
variance in return compared to the baselines.

4.2    FURTHER ANALYSIS

Loss Analysis. To analyse the learning dynamics of our algorithm, we take Walker2d as an example.
Figure 6 reports the main loss Lᵐᵃⁱⁿ curve of actor and the loss curves of hω (i.e., Lᵃᵘˣ) and 
Lᵐᵉᵗᵃ over


ω

5 trials for SAC. We can see that: (i) SAC-MC shows faster convergence to a lower value of L

main,

demonstrating the auxiliary loss’s ability to accelerate learning.  Unlike supervised learning, 
where
the vanilla loss is, e.g., cross-entropy vs ground-truth labels. The Lᵐᵃⁱⁿ for actors in RL is 
provided
by the critic which is also learned, so the plot also encompasses convergence of the critic.  (ii) 
The
meta-loss (which corresponds to the success of the meta-critic in improving actor learning) 
fluctuates
throughout, reflecting the exploration process in RL. But it is generally negative, confirming that 
the
auxiliary-trained actor generally improves on the vanilla actor at each iteration.  (iii) The 
auxiliary
loss  converges smoothly under the supervision of the meta-loss.

Ablation on hω design. We also run Walker2d experiments with alternative hω designs as in Eq. (7)
or MetaReg (Balaji et al., 2018) format (input actor parameters directly).  As shown in Table 2, we
record the max average return and sum average return (regarded as the area under the average reward
curve) of all evaluations during all time steps.  Eq. (7) achieves the highest max average return 
and

8


Under review as a conference paper at ICLR 2020


16000

14000

12000

10000

8000

HalfCheetah-v2

4000

3000

2000

Hopper-v2

7000

6000

5000

4000

3000

Walker2d-v2


6000

4000

2000

1000

0

2000

1000

0


7000

6000

5000

4000

3000

2000

1000

0

Ant-v2

11000

10000

9000

8000

7000

6000

5000

HalfCheetah(rllab)

Ant(rllab)

9000

8000

7000

6000

5000

4000

3000

2000

Figure 5: Box plots of the Max Average Return over 5 trials of all time steps.


100

200

300

400

500

600

700

SAC
SAC-MC

0           2           4           6           8          10

Time steps (1e6)

0.95

0.90

0.85

0.80

0.75

0.70

0.0      0.5      1.0      1.5      2.0      2.5      3.0

Time steps (1e6)

0.02

0.01

0.00

0.01

0.02

0.03

0           2           4           6           8          10

Time steps (1e6)


(a) Main loss of actor

(b) Auxiliary loss of actor
Figure 6: Loss analysis of our algorithm.

(c) Meta-loss

our default hω (Eq. (6)) attains the highest mean average return. We can also see some improvement
for hω(φ) using MetaReg format, but the huge number (73484) of parameters is expensive. Overall,
all meta-critic module designs provides at least a small improvement on vanilla SAC.

Ablation on baseline in meta-loss.  In Eq. (5), we use Lᵐᵃⁱⁿ(dvₐl; φₒld) as a baseline to improve
numerical stability of the gradient update. To evaluate this design, we remove the φₒld baseline and
optimize ω      argmin tanh(Lᵐᵃⁱⁿ(dvₐl; φnₑw)).  The last column in Table 2 shows that this barely

ω

improves on vanilla SAC, validating our design choice to use a baseline.

Table 2:  Max and Sum Average Return over 5 trials of all time steps under different designs of 
meta-critic
(aux-loss) and meta-loss. Max value in each row is bolded.

SAC                                           Lmeta  : φnew − φold                                  
        Lmeta  : φnew
hω(π¯φ)               hω(π¯φ, s, a)                 hω(φ)                     hω(π¯φ)

Max Average Return     6398.8 ± 289.2     7164.9 ± 151.3     7423.8 ± 780.2     6644.3 ± 1815.6     
6456.1 ± 424.8

Sum Average Return         53,695,678             61,672,039            57,364,405             
58,875,184              52,446,717

5    CONCLUSION

We present Meta-Critic, an auxiliary critic module for Off-PAC methods that can be meta-learned
online during single task learning.   The meta-critic is trained to generate gradients that improve
the actor’s learning performance over time, and leads to long run performance gains in continuous
control.  The meta-critic module can be flexibly incorporated into various contemporary Off-PAC
methods to boost performance.  In future work,  we plan to apply the meta-critic to conventional
offline meta-learning with multi-task and multi-domain RL.

9


Under review as a conference paper at ICLR 2020

REFERENCES

Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas.  Learning to learn by gradient descent by gradient
descent. In NIPS, 2016.

Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.  Metareg:  Towards domain gener-
alization using meta-regularization. In NeurIPS, 2018.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. In arXiv preprint arXiv:1606.01540, 2016.

Yan  Duan,  Xi  Chen,  Rein  Houthooft,  John  Schulman,  and  Pieter  Abbeel.   Benchmarking  deep
reinforcement learning for continuous control. In ICML, 2016a.

Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel.  Rl ²: Fast
reinforcement  learning  via  slow  reinforcement  learning.   In  arXiv  preprint  
arXiv:1611.02779,
2016b.

Chelsea Finn, Pieter Abbeel, and Sergey Levine.  Model-agnostic meta-learning for fast adaptation
of deep networks. In ICML, 2017.

Luca  Franceschi,   Paolo  Frasconi,   Saverio  Salzo,   Riccardo  Grazzi,   and  Massimilano  
Pontil.
Bilevel  programming  for  hyperparameter  optimization  and  meta-learning.    In  arXiv  preprint
arXiv:1806.04910, 2018.

Scott Fujimoto, Herke van Hoof, and David Meger.   Addressing function approximation error in
actor-critic methods. In ICML, 2018.

Josif Grabocka, Randolf Scholz, and Lars Schmidt-Thieme.   Learning surrogate losses.   In arXiv
preprint arXiv:1905.10108, 2019.

Abhishek  Gupta,  Russell  Mendonca,  YuXuan  Liu,  Pieter  Abbeel,  and  Sergey  Levine.    Meta-
reinforcement learning of structured exploration strategies. In NeurIPS, 2018.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.   Soft actor-critic:  Off-policy
maximum  entropy  deep  reinforcement  learning  with  a  stochastic  actor.     In  arXiv  preprint
arXiv:1801.01290, 2018a.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine.  Soft actor-critic algo-
rithms and applications. In arXiv preprint arXiv:1812.05905, 2018b.

Rein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie, Filip Wolski, Jonathan Ho, and
Pieter Abbeel. Evolved policy gradients. In NeurIPS, 2018.

Chen Huang, Shuangfei Zhai, Walter Talbott, Miguel A´ ngel Bautista, Shih-Yu Sun, Carlos Guestrin,
and Josh Susskind. Addressing the loss-metric mismatch with adaptive loss alignment. In ICML,
2019.

Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M Hospedales.  Feature-critic networks for het-
erogeneous domain generalization. In ICML, 2019.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR,
2016.

Franziska Meier, Daniel Kappler, and Stefan Schaal. Online learning of a memory for learning rates.
In ICRA, 2018.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller.  Playing atari with deep reinforcement learning.  In arXiv preprint
arXiv:1312.5602, 2013.

10


Under review as a conference paper at ICLR 2020

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare,  Alex  Graves,  Martin  Riedmiller,  Andreas  K  Fidjeland,  Georg  Ostrovski,  Stig  
Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518:529–533, 2015.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu.  Asynchronous methods for deep reinforcement
learning. In ICML, 2016.

Pierre-Yves Oudeyer and Frederic Kaplan.  What is intrinsic motivation?  a typology of computa-
tional approaches. Frontiers in neurorobotics, 1:6, 2009.

Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine.  Meta-learning with implicit
gradients. In NeurIPS, 2019.

Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine.  Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In ICML, 2019.

Matthew  Riemer,  Ignacio  Cases,  Robert  Ajemian,  Miao  Liu,  Irina  Rish,  Yuhai  Tu,  and  
Gerald
Tesauro.  Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. In ICLR, 2019.

Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In ICML, 2016.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.  Trust region
policy optimization. In ICML, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.  Proximal policy
optimization algorithms. In arXiv preprint arXiv:1707.06347, 2017.

Flood Sung,  Li Zhang,  Tao Xiang,  Timothy Hospedales,  and Yongxin Yang.   Learning to learn:
meta-critic networks for sample efficient learning. In arXiv preprint arXiv:1706.09529, 2017.

Richard S Sutton,  David A McAllester,  Satinder P Singh,  and Yishay Mansour.   Policy gradient
methods for reinforcement learning with function approximation. In NIPS, 2000.

Richard  S  Sutton,  Hamid  Reza  Maei,  Doina  Precup,  Shalabh  Bhatnagar,  David  Silver,  Csaba
Szepesvri, and Eric Wiewiora.   Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In ICML, 2009.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IROS, 2012.

Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Richard Lewis, Janarthanan Rajendran, Junhyuk Oh,
Hado van Hasselt, David Silver, and Satinder Singh.  Discovery of useful questions as auxiliary
tasks. In NeurIPS, 2019.

Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, and Tie-Yan Liu. Learning to
teach with dynamic loss functions. In NeurIPS, 2018.

Zhongwen  Xu,  Hado  van  Hasselt,  and  David  Silver.   Meta-gradient  reinforcement  learning.   
In

NeurIPS, 2018.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola.  Deep sets.  In Advances in Neural Information Processing Systems 30.
2017.

Zeyu Zheng, Junhyuk Oh, and Satinder Singh.   On learning intrinsic rewards for policy gradient
methods. In NeurIPS, 2018.

11


Under review as a conference paper at ICLR 2020

Supplementary Information

6    ALGORITHMS  OF  META-CRITIC  FOR  DDPG, TD3 AND  SAC

We incorporate our Meta-Critic to the implementation of vanilla DDPG, TD3 and SAC, following
their original implementations.

Algorithm 2 DDPG-MC algorithm

Initialize critic Q(s, a θ), actor π(s φ) and auxiliary loss network hω
Initialize target network Q′ and π′ with weights θ'         θ, φ'         φ
Initialize replay buffer

for episode = 1, ..., M do

Initialize a random process      for action exploration
Receive initial observation state s₁

for t = 1, ..., T do

Select action at = π(st φ) +    t according to the current policy and exploration noise
Execute action at, observe reward rt and new state st₊₁

Store transition (st, at, rt, st₊₁) in R

Sample a random mini-batch of N transitions (si, ai, ri, si₊₁) from R


Set yi = ri + γQ′(si₊₁, π′(si₊₁ φ' ) θ' )
Update critic by minimizing the loss: L = N −¹
meta-train:

Σi(yi − Q(si, ai|θ))²

Calculate the old actor weights using the main actor loss:

∇φLᵐᵃⁱⁿ = −N −¹ Σ ∇ₐQ(s, a|θ)|s₌s ,ₐ₌π₍s₎∇φπ(s|φ)|s₌s

i

φold  = φ − α∇φLmain

Calculate the new actor weights using the auxiliary actor loss:


meta-test:

ω                                                                                                   
       i

i

φnew  = φold − α∇φLaux

Sample a random mini-batch of N sᵛᵃˡ from

Calculate the meta-loss using the meta-test sampled transitions:


Lmeta

= tanh(L

main

(s, a|θ)|s=sᵛᵃˡ,a=π(s|φnew ) − L

main

(s, a|θ)|s=sᵛᵃˡ,a=π(s|φₒld))

meta-optimization: Update the weight of actor and meta-critic network:

φ ← φ − η(∇φLᵐᵃⁱⁿ + ∇φLᵃᵘˣ)


Update the target networks:

end for
end for=0

ω ← ω − η∇ωLᵐᵉᵗᵃ

θ'  ← τθ + (1 − τ )θ'

φ'  ← τφ + (1 − τ )φ'

12


Under review as a conference paper at ICLR 2020

Algorithm 3 TD3-MC algorithm

Initialize critics Qθ1 , Qθ2 , actor πφ and auxiliary loss network hω

Initialize target networks θ1′        θ₁, θ2′        θ₂, φ′     φ

Initialize replay buffer

for t = 1 to T  do

Select action with exploration noise a      πφ(s) + ϵ, ϵ          (0, σ) and observe reward r and
new state s′

Store transition tuple (s, a, r, s′) in B

Sample mini-batch of N transitions (s, a, r, s′) from B

a˜ ← πφ' (s′) + ϵ,     ϵ ∼ clip(N (0, σ˜), −c, c)

y ← r + γ mini₌₁,₂ Qθi' (s′, a˜)


Update critics θi     arg minθi

if t mod d then

N −¹ Σ(y − Qθ (s, a))²


aux

Σ1                          φ

Calculate the old actor weights using the main actor loss: φₒld = φ − α∇φLᵐᵃⁱⁿ

Calculate the new actor weights using the auxiliary actor loss: φnₑw = φₒld − α∇φLᵃᵘˣ

meta-test:

Sample mini-batch of N sᵛᵃˡ from

Calculate the meta-loss using the meta-test sampled transitions:


Lmeta  = tanh(Lmain(sval, a θ1) a=π(sval) φ

meta-optimization:

Update the actor and meta-critic:

φ ← φ − η(∇φLᵐᵃⁱⁿ + ∇φLᵃᵘˣ)

new

) − Lmain(sval, a|θ1)|a=π(sval)|φ

old))


ω ← ω − η∇ω

ω

Lmeta

Update target networks:
θi′       τθi + (1     τ )θi′
φ′     τφ + (1     τ )φ′

end if

end for=0

13


Under review as a conference paper at ICLR 2020

Algorithm 4 SAC-MC algorithm

θ₁, θ₂, φ, ω                                                      // Initialize parameters

θ¯ ← θ₁, θ¯2 ← θ₂                                // Initialize target network weights


Dfor←eaØch iteration do

for each environment step do

// Initialize an empty replay pool

at ∼ πφ(at|st)                                        // Sample action from the policy

st₊₁ ∼ p(st₊₁|st, at)                             // Sample transition from the environment

endDf←or D ∪ {(st, at, r(st, at), st₊₁)}     // Store the transition in the replay pool

for each gradient step do

θi     θi    λQ   θiJQ(θi) for i       1, 2   // Update the Q-function parameters

meta-train :


∇φLmain  = N −1 Σt

∇ₐ[α log (πφ(a|s)) − Qθ(s, a)|s₌st,a=π(s)]∇φπφ(s)|s₌st


main

∇φLaux  = ∇φhω  = N −1 Σ

// Calculate old weights of the actor

∇φMLPω(π¯φ(s))|s₌s

φnₑw = φₒld    α   φLω                               // Calculate new weights of the actor

meta-test:


Lmeta  = tanh(Lmain(s, a|θ)|s=sval ,a=π(s|φ

new

) − Lmain(s, a|θ)|s=sval ,a=π(s|φ

old))


meta-optimization:

// Calculate meta-loss

φ ← φ − η(∇φLᵐᵃⁱⁿ + ∇φLᵃᵘˣ)            // Update the actor parameters


ω ← ω − η∇ω

ω

Lmeta

// Update the meta-critic parameters

α      α     λ   αJ(α)                                // Adjust temperature

θ¯i        τθi + (1     τ )θ¯i for i       1, 2        // Update target network weights

end for
end for =0

14


Under review as a conference paper at ICLR 2020

7    AVERAGE  REWARDS  ON  OTHER  TASKS  AND  PPO-LIRPG EXPERIMENTS


Reacher-v2

4

5

6

7

8

DDPG
DDPG-MC

1000

900

800

700

600

500

400

300

InvertedPendulum-v2

DDPG
DDPG-MC

9000

8000

7000

6000

5000

4000

InvertedDoublePendulum-v2

DDPG
DDPG-MC


0.0       0.2       0.4       0.6       0.8       1.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

Figure 7: Learning curve mean and standard-deviation of vanilla DDPG and meta-critic enhanced 
DDPG-MC
for MuJoCo tasks with upper reward bound.


4.0

4.5

5.0

5.5

6.0

6.5

7.0

7.5

Reacher-v2

TD3
TD3-MC

1000

900

800

700

600

500

400

300

InvertedPendulum-v2

TD3
TD3-MC

9000

8000

7000

6000

5000

4000

InvertedDoublePendulum-v2

TD3
TD3-MC


0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

Figure 8:  Learning curve mean and standard-deviation of vanilla TD3 and meta-critic enhanced 
TD3-MC for
MuJoCo tasks with upper reward bound.


3.5

4.0

4.5

5.0

5.5

6.0

6.5

7.0

Reacher-v2

         SAC

SAC-MC

1000

800

600

400

200

InvertedPendulum-v2

         SAC

SAC-MC

9000

8000

7000

6000

5000

InvertedDoublePendulum-v2

         SAC

SAC-MC


0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

0.0      0.2      0.4      0.6      0.8      1.0

Time Steps (1e6)

Figure 9: Learning curve mean and standard-deviation of vanilla SAC and meta-critic enhanced SAC-MC 
for
MuJoCo tasks with upper reward bound.


2000

1500

1000

500

0

HalfCheetah-v2

PPO

4000

3500

3000

2500

2000

1500

1000

500

Hopper-v2

PPO

4000

3000

2000

1000

Walker2d-v2

PPO

PPO-LIRPG

600

400

200

0

200

Ant-v2

PPO

PPO-LIRPG


500

       PPO-LIRPG

0           2           4           6           8          10

Time Steps (1e6)

PPO-LIRPG

0

0           2           4           6           8          10

Time Steps (1e6)

0

0           2           4           6           8          10

Time Steps (1e6)

400

0           2           4           6           8          10

Time Steps (1e6)


Reacher-v2

0

20

40

60

80

PPO

1000

800

600

400

200

InvertedPendulum-v2

PPO

7000

6000

5000

4000

3000

2000

1000

InvertedDoublePendulum-v2

PPO


100

       PPO-LIRPG

0         2         4         6         8        10

Time Steps (1e6)

PPO-LIRPG

0

0         2         4         6         8        10

Time Steps (1e6)

PPO-LIRPG

0

0         2         4         6         8        10

Time Steps (1e6)

Figure 10: Learning curve mean and standard-deviation of PPO and PPO-LIRPG for continuous control 
tasks.

15


Under review as a conference paper at ICLR 2020


Reacher-v2

2

3

4

5

1000

990

980

InvertedPendulum-v2

9000

8500

InvertedDoublePendulum-v2


6

970

7

960

8

9                                                                                         950

8000

7500

7000

Figure 11:  Box plots of the Max Average Return over 5 trials of all time steps for MuJoCo tasks 
with upper
reward bound.

8    FURTHER  ANALYSIS

8.1    META-CRITIC COMPUTATION

In  terms  of  computation  requirement,  meta-critic  takes  around  15-30%  more  time  per  
iteration,
depending on the base algorithm.  This is primarily attributable to the cost of evaluating the meta-
loss Lᵐᵉᵗᵃ, and hence Lᵐᵃⁱⁿ.

To investigate whether the benefit of meta-critic comes solely the additional compute expenditure,
we perform an additional experiment where we increase the compute applied by the baselines to a
corresponding degree. Specifically, if meta-critic takes K% more time than the baseline, then we re-
run the baseline with K% more update steps iteration.  This provides the baseline more mini-batch
samples while controlling the number of environment interactions.  Examples in Figure 12 shows
that increasing the number of update steps does not have a straightforward link to performance.
For  DDPG,  Walker2d-v2  performance  increases  with  more  steps,  but  stills  performs  worse  
than
Meta-Critic.  Meanwhile, for HalfCheetah, the extra iterations dramatically exacerbates the drop in
performance that the baseline already experiences after around 1.5 million steps.  Overall, there is
no consistent impact of providing the baseline more iterations, and Meta-Critic’s consistently good
performance can not be simply replicated by a corresponding increase in gradient steps taken by the
baseline.


8000

6000

4000

2000

0

HalfCheetah (rllab)

TD3
TD3-MC

TD3_moreupdates

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

3500

3000

2500

2000

1500

1000

500

0

Walker2d-v2

DDPG
DDPG-MC

DDPG_moreupdates

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

Figure 12:  Experiment controlling for compute time per method.   Assigning more update itera-
tions to the baselines so their running speed matches Meta-Critic.  Left:  Learning curves of TD3
HalfCheetah (rllab). Right: Learning curves of DDPG Walker2d-v2.

8.2    ADDITIONAL ENVIRONMENTS

In order to investigate the impact of meta-critic on harder environments,  we evaluated SAC and
SAC-MC on TORCS and Humanoid(rllab). The results in Figure 13 show that meta-critic provides
a clear margin of performance improvement in these more challenging environments.

16


Under review as a conference paper at ICLR 2020


30000

25000

20000

15000

10000

5000

0

SAC
SAC-MC

0        20       40       60       80      100

Time Steps(1e3)

10000

8000

6000

4000

2000

0

SAC
SAC-MC

0.0    0.5    1.0    1.5    2.0    2.5    3.0

Time Steps (1e6)

Figure 13:  Learning curves of SAC and SAC-MC on the TORCS driving (Left) and Humanoid
(Right) environments

.

17

