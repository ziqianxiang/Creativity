Under review as a conference paper at ICLR 2020
Learning to Anneal and Prune Proximity
Graphs for Similarity Search
Anonymous authors
Paper under double-blind review
Ab stract
This paper studies similarity search, which is a crucial enabler of many feature
vector-based applications. The problem of similarity search has been extensively
studied in the machine learning community. Recent advances of proximity graphs
have achieved outstanding performance through exploiting the navigability of the
underlying graph structure. In this work, we introduce the annealable proximity
graph (APG) method to learn and reshape proximity graphs for efficiency and
effective similarity search. APG makes proximity graph edges annealable, which
can be effectively trained with a stochastic optimization algorithm. APG identifies
important edges that best preserve graph navigability and prune inferior edges
without drastically changing graph properties. Experimental results show that
APG achieves state-of-the-art results not only by producing proximity graphs with
less number of edges but also speeding up the search time by 20-40% across
different datasets with almost no loss of accuracy.
1	Introduction
Similarity search (nearest neighbor search) is an integral and indispensable task in many machine
learning applications, such as non-parametric classification/regression, computer vision, informa-
tion retrieval, and language modeling. Recently, it has been demonstrated that it is possible to
build a vector search engine to support semantic search (Chen et al., 2018; Sullivan, 2018; Wang
et al., 2018a; Johnson et al., 2017), which leverages high-quality neural ranking models (Nogueira
& Cho, 2019; Xiong et al., 2017; Zamani et al., 2018a) to encode both natural language query and
documents into dense continuous feature vectors and performs similarity search to retrieve relevant
documents with vast data volumes (e.g., based on Euclidean distance). This approach has demon-
strated significant relevance gains in a wide range of applications and outperforms existing term
matching baselines, such as web search (Huang et al., 2013; Zamani et al., 2018b), question and
answering (Yu et al., 2014), ad-hoc retrieval (Mitra et al., 2017; Dehghani et al., 2017; Guo et al.,
2016), mobile search (Aliannejadi et al., 2018), and product search (Van Gysel et al., 2016).
The efficiency and effectiveness of the similarity search approaches have become a problem of
great interest, due to the widespread commercial value and the exciting prospect. Recent ad-
vance of proximity graphs has demonstrated great potential for fast and accurate nearest neighbor
retrieval (Malkov & Yashunin, 2016; Fu et al., 2019), and the empirical performance of proximity
graphs outperforms existing tree-based (Bentley, 1975; Beckmann et al., 1990; Yianilos, 1993; Muja
& Lowe, 2014), locality sensitive hashing-based (Gionis et al., 1999), and product quantization-
based methods (Jegou et al., 2011; Ge et al., 2013; Norouzi & Fleet, 2013; Lempitsky, 2012; Kalan-
tidis & Avrithis, 2014) by a large margin. Proximity graphs exploit the navigability of graph struc-
tures, which the search process relies on to converge and achieve good efficiency. In practice, that of-
ten results in dense connectivity and large memory consumption because they need to have sufficient
edges to maintain specific graph properties, which is a major limitation of this class of approaches.
We wish to improve the efficiency of similarity search. In this paper, we address the following
research question: can we learn to prune edges of a proximity graph while still being accurate to
find nearest neighbors? Specifically, the pruned proximity graph should be more efficient than the
state-of-the-art proximity graphs with comparable accuracy. Before providing a definite answer to
the question, we briefly review the findings in percolation theory that motivates our research.
1
Under review as a conference paper at ICLR 2020
Percolation describes the phase transition of a physical system when one or more of its properties
change abruptly after a slight change in controlling variables (e.g., temperature, pressure, or oth-
ers) (Broadbent & Hammersley, 1957). Prototypical percolation processes include water turning
into ice or steam, and the spontaneous emergence of magnetization and superconductivity in metals.
Percolation theory mathematically models these physical systems as complex networks and phase
transition as a dramatic change of the properties of network connections. We believe that if we can
model edge importance as the robustness of proximity graphs to the removal of the edges between
vertices, we can produce a proximity graph with less number of edges without dramatically changing
the navigability of the graph.
We present Annealable Proximity Graph (APG), for simplifying proximity graphs. In particular, We
make the following contributions:
•	We introduce the annealable proximity graph and summarize its key characteristics.
•	To learn edge importance, we present a percolation inspired method for identifying impor-
tant edges and introduce a domain-specific loss derived from search distance errors.
•	Our formulation makes it possible to leverage a stochastic optimization algorithm to opti-
mize the objective and prune edges with low importance.
•	We prove the convergence of our optimization process and a theoretical guarantee of the
search quality of the pruned graph.
This approach is unique compared with previous proximity graph algorithms, where most of them
only exploit the structure of the underlying index instead of learning from query distribution to
reshape proximity graphs. We provide a detailed empirical analysis of our approach. Experimental
results show that our approach reduces the number of edges of state-of-the-art proximity graphs
significantly by 50% while also speeds up the search time by 20-40% across different datasets with
almost no loss of accuracy.
2	Related Work
In this section, we review the main ideas from the existing work that is relevant to our approach.
Approximate nearest neighbor search (ANN). The problem of similarity search has been exten-
sively studied in the literature of ANN algorithms, which trade the guarantee of exactness against
high-efficiency improvement. Some representative methods include tree structure-based (Bent-
ley, 1975; Beckmann et al., 1990; Yianilos, 1993; Muja & Lowe, 2014), locality sensitive hash-
ing (LSH)-based (Gionis et al., 1999), product quantization (PQ)-based (Jegou et al., 2011; Ge
et al., 2013; Norouzi & Fleet, 2013; Lempitsky, 2012; Kalantidis & Avrithis, 2014), and nearest
neighbor graph-based (Hajebi et al., 2011; Fu & Cai, 2016) approaches. Although some of these
methods, such as LSH, have strong theoretical performance guarantee even in the worst case (Indyk
& Motwani, 1998), recent advances of the proximity graphs have demonstrated logarithmic search
complexity and outperformed prior approaches by a large margin (Malkov & Yashunin, 2016; Douze
et al., 2018; Fu et al., 2019; Li et al., 2019).
Proximity graphs. A proximity graph exploits the closeness relationship among feature vectors
to support similarity search. In particular, let V = {vi ∈ RD |i = 1, ..., N} be a database of vectors,
a proximity graph G(V, E) is a directed graph, where each vertex corresponds to one of the vectors
v and the whole graph achieves great local connectivity (as in a lattice graph) combined with a small
graph diameter (as in a random graph) (Malkov et al., 2014; Malkov & Yashunin, 2016; Fu et al.,
2019). Such a graph exhibits strong navigability and enables quick search with an N -greedy best-
first search algorithm. During the search, a candidate queue of size L is used to determine the trade-
off between the searching time and accuracy. Recent studies look into optimizing proximity graphs
with product quantization (Douze et al., 2018; Baranchuk et al., 2018). However, these approaches
often suffer from a considerable amount of recall loss on large datasets because quantization errors
tend to be large on dense continuous feature vectors (e.g., generated by neural networks).
Learning to prune. Pruning is a common method to derive sparse neural networks and reduce
their heavy inference cost (Han et al., 2015a;b). These methods annihilate the non-important weights
2
Under review as a conference paper at ICLR 2020
through the introduction of an L0 or L1 regularizer to the loss function. Many of them remove
weights and keep important weights to best preserve the accuracy. Neural network pruning can also
be viewed as an architecture search technique, where the network is viewed as a computational graph
where the vertices denote the computation nodes and the edges represent the flow of tensors (Wang
et al., 2018b; Liu et al., 2019). To the best of our knowledge, learning to prune has not yet been
applied to the task of proximity graphs for similarity search. However, it appears to be a natural fit
for restructuring the proximity graphs to improve similarity search.
3	Challenges
The navigability of proximity graphs comes as a result of approximating monotonicity graphs, e.g.,
Delaunay graphs (Lee & Schachter, 1980). According to the graph monotonicity theory (Dearholt
et al., 1988), a monotonicity graph has a strong guarantee to find the exact nearest neighbor by
following a monotonic path with 1-greedy search (Fu et al., 2019). However, monotonicity graphs
in high dimensional space quickly become almost fully connected, and search in fully connected
graphs would be infeasible, due to the out-degree explosion problem (Hajebi et al., 2011). To address
the problem, proximity graphs limit each node to connect to only a number of R neighbors, aiming
to minimize the loss of graph monotonicity while still letting greedy search be effective.
However, several challenges remain. The correct choice of R is not so obvious. R cannot be too
small, because then the graph tends to lose too much monotonicity and search can frequently get
stuck at non-global local minima, hurting accuracy. A sufficiently large R is often required to
reach high accuracy, but it also increases the number of edges significantly and decreases efficiency:
(1) It ubiquitously raises the connections of both ”hubs” (i.e., nodes in dense areas) and nodes in
sparse areas; and (2) it makes the graph more densely connected with many vertices sharing a lot of
common neighbors, increasing unnecessary distance computations. Ideally, each vertex should have
a different R that best preserves graph monotonicity. The problem is beyond selecting a good value
for R and seems to require more fundamental changes to existing proximity graphs.
4	Methods
In this section, we propose the annealable proximity graph (APG), which supports the exploration
of edge heterogeneity in proximity graphs and learning to prune such graphs with an eye towards
getting to the nearest neighbor as quickly as possible with minimal loss of accuracy.
4.1	Overview
APG starts with augmenting a pre-built proximity graph with a learnable weight associated with each
edge, representing its importance to preserve graph monotonicity, and a keep probability (§§ 4.2).
The weight is updated in an iterative manner according to a learning rule.
APG employs a multi-step learning process, as illustrated in Fig. 1. At initialization of APG, all
edges have equal weights. Since no edge heterogeneity has been learned so far, applying pruning
at this stage could lead to premature decisions. Therefore, we start with performing a “warm-up”
iteration over the edge weights only, without optimizations, i.e., as the grace-period.
Once this warm-up ends, we introduce a systematic and principled approach to optimize the edge
keep probability distribution of the APG (§§ 4.4). In particular, APG models edge importance as the
robustness of graph monotonicity to edge removal and defines an objective function that reflects the
destruction of graph monotonicity, based on relative search distance errors (§§ 4.3). It then generates
a sequence of randomized subgraphs through a sampling policy to learn edge importance and uses a
predefined annealing schedule to optimize the objective function.
The process ends once we meet a stopping criterion. After this step, APG marks low weight edges
as less important and perform a hard pruning to remove inferior edges, as shown in Fig. 2.
3
Under review as a conference paper at ICLR 2020
proximity graph.
4.2	Annealable Proximity Graph (APG)
Given a proximity graph G(V, E), an annealable proximity graph G* (V, E) is obtained by augment-
ing each edge e ∈ E with a weight variable we, and a keep probability function p : E → (0, 1) such
that p(e) ≡ pe indicates the keep probability of e ∈ E. That is, an independent Bernoulli random
variable Re, where P(Re = 1) = pe, P(Re = 0) = 1 - pe is assigned to each e.
Intuitively, pe should be: (i) monotonically increasing as we increases; and (ii) limwe→+∞ pe = 1,
and limwe→-∞ pe = 0. More importantly, it is desirable to have we initialized to similar values
for all edges, allowing each edge to have an equal probability of consideration when there is little
information about edge importance. As the optimization process continues, pe should converge into
a degenerated distribution that allows identifying a subset of removable edges that do not signifi-
cantly change graph properties. Moreover, we introduce an additional parameter, the temperature
T ∈ (0, ∞), which smooths the probabilities pe as following. If T → ∞ (at the beginning), the
probabilities pe converges uniformly to the same value regardless of edge e; on the other hand, if
T → 0 (at the end), the probabilities pe converges to either 1 or 0, for important and not important
edges, respectively. To satisfy the above conditions, we introduce the following function p:
Pe (T )= 1 + expl 中
(1)
where μ is a normalizing factor to keep E Pe(T) = C a constant.
e∈E
4.3	Robustness of Graph Monotonicity
To efficiently find nearest neighbors, all the previous algorithms try to exploit the proximity graph
structure of V vectors by letting each vertex connect a number of R neighbors. However, some
connections could be more crucial for preserving graph monotonicity, which is important for search
efficiency, while the rest is less important. How do we identify those important edges?
One naive approach is to keep those edges that appear as part of the shortest path from the en-
try vertex to the ground truth nearest neighbor for a query. Such an approach significantly suffers
overfitting: non-shortest-path edges may still contain possibly relevant closeness relation for unseen
queries on the test query set. Another possibility is to treat all checked edges during the search
process as important. However, the checked and taken edges are then not differentiated, and some
checked but not taken edges may even hurt accuracy by misleading the route. Both of these results
are undesirable. As § 3 mentioned, proximity graphs rely on the approximation of graph mono-
tonicity to converge and achieve their efficiency. Can we identify important edges based on their
robustness to preserve graph monotonicity?
Figure 3: A high-level depiction of learning edge importance for proximity graphs.
4
Under review as a conference paper at ICLR 2020
Identifying important edges. Inspired by the phase transition in percolation (Broadbent & Ham-
mersley, 1957; Callaway et al., 2000), we introduce a method for identifying edge importance in
proximity graphs. In particular, as Fig. 3 shown, for a given G*, We randomly delete each edge
e with probability 1 - pe (remember that pe denotes the keep probability of e), independently of
all other edges, and we denote the resulting random graph by G (V, E \ F), where F is the set
of deleted edges. For any query q, if we can find the exact nearest neighbor in G but not in G
under a search budget, then we treat the edge hops along the search path of q in G, including those
erroneous paths, to be important for preserving the robustness of graph monotonicity, because it is
their deletion that causes the failure to find the nearest neighbor in G.
Relative distance errors. Once we get the set of edge hops for a query q (let us denote it as
Hq), we update the weight wh of h ∈ Hq to increase the importance of those edges, based on how
far off the found candidate is relative to the true nearest neighbor, similar to the Teacher Forcing
scheme (Williams & Zipser, 1989). In particular, assume G answers the query q by returning a point
p, and G answers q with a found candidate p0 , we define the weight update as:
∆wh = ( ¾p0,4- 1) ∙ η, ∀h ∈ Hq
δhp, qi
(2)
where δ(∙, ) represents the distance between two vertices and η is the learning rate. It measures the
delta of searching q’s nearest neighbor in a subgraph G versus in the full graph G by removing
F edges. The update is designed based on the following intuition: when the returned candidate p0
in G is the exact nearest neighbor as q returned by G, then the deleted edges are less important
and the relative distance error is 0. Otherwise, the larger δhp0,qi in comparison to δhp, qi, the more
importance Hq indicates, and the edge weights of Hq should increase more.
Objective. Based on Eqn. 2, we propose to learn the keep probability distribution of APG while
minimizing the relative distance error over a learning set Q. In particular, the learning objective
function is defined as:
minimize ɪ- ^X E
h∈Hq
∆wh
(3)
Based on the learned keep probability, we identify a subgraph G that is robust to the deletion of a
subset of edges F with minimal loss of graph monotonicity.
4.4	Optimization
The main objective of the optimization is to construct a proper learning framework to optimize the
objective defined in Eq 3. Similar to bagged ensemble learning (Bengio et al., 2017), one approach
is to initialize a set of graphs, each with a subset of edges randomly deleted. Then it individually
learns edge importance on each subgraph and finally combines learned weights. However, bag-
ging ignores the dependency among the individual subgraphs, and the edge importance does in fact
depend on the chosen subgraph. In this paper, we introduce a learning algorithm, inspired by sim-
ulated annealing (Ingber, 1993) and stochastic optimization process (Bottou & Bousquet, 2007),
where we probabilistically generate a sequence of sampled subgraphs to learn the important edges
that preserve graph monotonicity. The benefit is that it gradually discovers important edges while
also allowing edges with lower keep probability to be sampled to demonstrate their values.
Generating a sequence of random subgraphs. For a given G*, we generate a sequence of ran-
domized subgraphs: GA ⑴ → G>(2) → ∙∙∙ → GA(K), which correspond to K optimization
steps. At each step k, the keep probability of each edge is computed from the weight of the previous
step w(k - 1). The subgraph GA (k) is obtained via sampling edges from G* by randomly picking
a set of E(k)(|E(k)| ≤ |E|) edges based on this keep probability. The new weight w(k) is then
obtained through learning to minimize the relative distance errors measured on GA (k). As a strin-
gent control, the expected number of edges to be selected by the algorithm is |E(k)| = dλ(k) ∙ |E |],
where 0 < λ(k) ≤ 1 denotes the sampling ratio at iteration k and is governed by a sampling policy.
5
Under review as a conference paper at ICLR 2020
Gradual sampling policy. The sampling policy λ(k) decides how many edges a sampled sub-
graph G (k) has. We found that the simplest solution to having a fixed value λ(k) ≡ 1 - σ works
reasonably well in most cases, where σ is the final pruning ratio (the fraction of pruned edges). In
this paper, inspired by the formula suggested in (Zhu & Gupta, 2018), we define λ(k) as:
λ(k) = 1 - σ + (λ(0) + σ - 1) (l - K)
(4)
where c ∈ {1, 3} and λ(0) is the initial sampling rate. Take σ = 0.5 and λ(0) = 1 as an example,
intuitively, this policy allows to select more edges for exploration in the beginning and gradually
becomes more selective as the optimization is close to the end.
Binomial weight normalization. The expectation of the number of edges of a random subgraph
G (k) follows a Poisson binomial distribution: It is the sum of Bernoulli random variables Re, e =
1, ..., |E|, each taking on values 0 and 1 with probabilities 1-pe andpe, respectively. However, since
∆w ≥ 0 in Eq 2, how to expect the sampled graph G (k) to have |E(k)| edges given that increased
weights also increase the sum of Re ? To address this issue, we do a binomial normalization to
adjust the edge weights at each iteration by adding a normalizing factor μ(k) (as in Eq 1) to all edge
weights so that the sum of Re equals to |E(k)|:
|E(k)| ≡ E XRe =Xpe(T)= X
-we + μ(k)	(5)
,e∈E	e	e∈E	e∈E 1 + e T
where μ(k) is calculated through a binary search of the computed sum of probabilities, which has a
time complexity of O(∣E∣∙ log(max(w(k)) — min(w(k))).
Annealing schedule. To balance exploration and exploitation of edge importance, our approach
includes an annealing schedule Φ(k) to determine the temperature T, which is updated along the
iterations. The schedule choice is dominated by a trade-off. On one hand, fast temperature decay
simplifies the optimization objective and reduces the complexity, assisting the edge weights to con-
verge, since inferior edges quickly have their keep probability driven to 0 and are excluded from
subsequent optimizations. However, premature decisions could lead to a sub-optimal keep proba-
bility distribution. This suggests that we should choose a slow temperature decay, which provides
more opportunities for testing random subgraphs during the pre-convergence phase, but also may
find a better subgraph during the convergence phase.
The solution we propose is based on observations explored in simulated annealing, which share
a similar trade-off between the continuity of the optimization process and the time required for
deriving the solution (Ingber, 1993). Among the many alternatives, we choose the exponential
schedule, which has been shown to be effective in multiple other tasks, e.g. (Kirkpatrick et al.,
1983; Nourani & Andresen, 1998),
Φ(k) = To ∙ βk	(6)
This schedule starts with a relatively high temperature T0 and decays fast with a decay factor β .
Hard pruning. The optimization process ends when it meets a stopping criterion. In our current
implementation, we use a simple criterion that stops after a given number of iterations. Once the
optimization finishes, we re-rank edge weight and prune less important edges according to a desired
pruning ratio σ . While pruning, we avoid deleting bridges that disconnect the graph. After prun-
ing, we add a minimal number of edges to keep the graph strongly connected through connectivity
augmentation (Hsu et al., 2017). The overall learning algorithm is given in Algorithm 1.
Convergence and correctness proof. We demonstrate the effectiveness of our approach empir-
ically in § 5 and provide a theoretically derived proof of the convergence and correctness of our
algorithm in Theorem A.1 (Appendix A).
6
Under review as a conference paper at ICLR 2020
Algorithm 1
APG learning algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
Input: Unpruned proximity graph G(V, E), learning set Q, candidate queue length L.
Output: Pruned graph G(V, E \ F).
Parameters: Learning rate η, starting temperature T0, decay factor β, pruning ratio σ, max
iteration K
Init: T J To,k J 0
Convert G(V, E) to G* * (V, E), W J 0
Update w according to a warm up run
while k ≤ K do
λ J^1 - σ + (λ(0) + σ - 1)(1 - KY
Normalize W s.t. |E(k)| = dλ(k) ∙ |E|] ≡ P ——/ 工+“(%八
e∈E 1+exp( T )
Randomly sample a subgraph G (k)(V, E(k))
for q in Q do
p,一 Search(GA (k), q,L)
p, Hq J search(G, q, L)
if p 6= p0 then
for h in Hq do 0
δWh J(冷-1) ∙ η
Wh J Wh + ∆Wh
T J To ∙ βk
Shuffle Q
Remove |F| = σ ∙ |E| lowest ranking edges from G*
Convert G* (V, E \ F) to G(V, E \ F)
5 Evaluation
5.1 Methodology
Datasets. We evaluate APG on three publicly available datasets, which are widely used as the
similarity search benchmarks:
• SIFT1M is a classical dataset containing 128-dimensional SIFT descriptors (Jegou et al., 2011).
It consists 1,000,000 base vectors, 100,000 learning vectors, and 10,000 testing vectors.
• Deep1M is a random 1,000,000 subset of one billion of 96-dimensional vectors produced by
CNN (Babenko & Lempitsky, 2016). We sample 100,000 vectors from the provided 350M learn
set as the learning set. For testing, we take the original 10,000 queries.
• GloVe is a collection of 200-dimensional word embedding vectors from Twitter data (Pennington
et al., 2014). We randomly sample from the original 1,193,514 vectors to get base, learning, and
testing sets, each containing 1,000,000, 100,000, and 10,000 vectors, respectively.
For each dataset, we use one of the state-of-the-art approaches, Hierarchical Navigable Small World
graph (Malkov & Yashunin, 2016), to build the proximity graph using the base set, which we refer
to as PG (proximity graph). We use the learning set to learn and prune the bottom layer of HNSW
and use the testing set for final evaluation. We do not prune the edges in the upper layers of HNSW
since there are much fewer edges in those layers. We set hyperparameters as T0 = 1, K = 20,
β = 0.8, η = 0.1, and σ = 0.5. The training time is 77, 40, and 128 minutes for SIFT1M, Deep1M,
and GloVe, respectively.
Setup. All the experiments were done on a 64-bit Linux Ubuntu 16.04 server with Intel Xeon CPU
E5-2650 v4 @ 2.20GHz processor.
Implementations. APG and the learning algorithm are implemented in C++. Subgraph sampling
is implemented by using a binary mask map, which is of the same size as the number of edges, to
determines edges that are kept in the sampled subgraph. The weights that are masked in the sampled
subgraph do not get updated during the weight update phase.
7
Under review as a conference paper at ICLR 2020
Evaluation metrics. To measure the pruning effectiveness, we report the number of edges before
and after pruning and the search time. We measure the accuracy by calculating the rate of queries
for which the exact nearest neighbor is found. Since it is essential to be both fast and with high
accuracy, we focus on the high accuracy range.
5.2	Experiment Results
In this section we evaluate the proposed method by comparing the following schemes for each
dataset:
•	Original PG. We construct a proximity graph over the base vectors using the HNSW algo-
rithm.
•	Ours. Our main algorithm as described in § 4. We use the same initial R as PG and set
the pruning ratio to σ = 0.5, which forces half of the edges to be useful for preserving the
graph properties.
•	PG + sampling. As a heuristic baseline, we uniformly sample half of the edges from PG
to remove without iteration optimizations and annealing edge weights.
•	Sparse-PG. Unlike ours, this approach directly constructs a proximity graph with only half
edges to begin with. Hence all the edges still have equal importance.
Table 1 shows the accuracy, the edge counts, the search time for all three datasets. To compare the
performance of baselines and our approach apples to apples, we perform controlled experiments to
keep all approaches to reach the same accuracy target in order to compare the edge reduction rate and
the search time improvement. Given an accuracy target (e.g., 0.99 for SIFT1M, 0.94 for Deep1M,
and 0.83 for GloVE), we vary L to find the minimum latency to reach the desired accuracy.
Overall, compared to the original proximity graph, ours reduces the number of edges by 50%, mak-
ing PG more memory efficient. On SIFT1M and GloVe1M, ours further makes the search 26.1% and
1.8% faster, respectively. This is because pruning simplifies the proximity graph while preserving
the graph monotonicity, so that a query still gets to the nearest neighbor but is faster by avoiding
checking less important edges. On Deep1M, the fastest search time is achieved by the unmodified
PG, because when the target accuracy is really high (e.g., 0.99), there are much fewer redundant
edges. And for this particular dataset, after pruning 50% edges, ours needs to search a little bit more
to reach the same level of accuracy.
Both PG + subsampling and sparse-PG have a similar number of edges like ours. However, reducing
the number of edges alone does not necessarily lead to better search efficiency. If the search takes a
fixed number of hops to reach the nearest neighbor, then pruning edges will lead to less number of
vectors being checked. However, for proximity graphs, the search stop condition is (1) greedy search
until reaching a local optimum and (2) the search time budget has exhausted, so the search may end
up searching the same amount of edges even after pruning edges. Since subsampling does not take
into graph monotonicity into account, it leads to poor search efficiency either because a query needs
to take a detour to find the nearest neighbor or it may not even find the nearest neighbor if the graph
becomes disconnected. In fact, randomly removing edges is so destructive that GloVe1M cannot
get the desired 0.83 accuracy even if the search time has increased from 0.55ms to 1.45ms. On
the other hand, the sparse-PG directly restricts the number of edges of each node during the graph
construction phase. However, as described in § 3, a smaller R hurts the connectivity of the proximity
graph, and search can easily get stuck at local minimum. In order to reach the same accuracy, this
approach needs to search more, which causes the degradation of search time. As a result, APG
makes the search 25-74.2% faster compared to these two approaches.
Impact of pruning ratio σ. We evaluate the impact of different pruning ratio σ. Fig.4a shows that,
as σ increases, the number of edges left in the pruned graph linearly decreases, which is expected
because our algorithm enables accurate control of pruning a given number of edges.
Fig.4b evaluates the impact on accuracy varying σ, under different L = 20, 50, 100. The accuracy
remains almost on par as the unpruned proximity graph when the pruning ratio is equal or less than
50%. It starts to drop significantly once the pruning ratio is beyond 50%. The results suggest that
our approach prunes a large number of edges while still being able to let queries find their nearest
8
Under review as a conference paper at ICLR 2020
Dataset	Configuration	ReCall@1	#Edges	LatenCy(ms)	Edge reduction rate	Search time improvement
SIFTIM	Original PG1	0.993	40.3M	0.23	50.0%	26!%
	PG + sampling	0.992	20.2M	0.43	0.0%	60.5%
	-SParse-PG-	0.991-	20.2M	0.66	0.0% 一	74.2% 一
	Ours	0.993	20.2M	0TT7		
DeepIM	Original PG	0.947	43.2M	0.1	50.0%	-4.0%
	PG + sampling	0.945	21.6M	0TT6	0.0%	35.0%
	-Sparse-PG-	0.943	21.6M	0TT4	0.0% 一	25.7% 一
	Ours	0.943	21.6M	0.104		
GloVeIM	Original PG1	0.831-	20.6M	0.55	50.0%	18%
	PG + sampling	0.814	10.3M	145	0.0%	62.8%
	-Sparse-PG-	-0:83-	10.3M	0.74	0.0% 一	27.0% 一
	Ours	0.833	10.3M	0.54 一		
Table 1: Comparison between APG and different baselines over SIFT1M, Deep1M, and GloVe.
neighbors, as long as the proximity graph has not entered a phase transition where it starts to quickly
lose graph monotonicity.
To demonstrate the impact of the pruning ratio on search efficiency, we present Fig.4c and Fig.4d,
which illustrate the distance computations and search time under different σ. The results show that
larger σ often leads to fewer distance computations and shorter search time, with varying L. This
is expected, because APG makes the proximity graph sparser and saves both distance computations
and search time by avoiding checking less important edges.
(a)	(b)	(c)	(d)
Figure 4: Impact of different pruning ratio. (a) #Edges v.s. pruning ratio; (b) Accuracy v.s. pruning
ratio under different L; (3) and (4) Distance computations and search time under different σ.
Impact on degree distribution. To see how pruning affects the graph topology, Fig.6 in Ap-
pendix B shows the frequency distributions of in-degree and out-degree for all nodes before and
after pruning (σ = 0.5). We found that the out-degree tends to be smoothed out to have a power-law
distribution after the pruning, and the in-degree remains to have a binomial distribution but with
only a slight shift after the pruning.
Internal of APG learning process. To reveal the internal learning process, Fig. 7 in the Appendix
demonstrates the snapshot of the keep probability distribution at different iterations. The distribu-
tion starts with a relatively high temperature and high sampling ratio λ, so that most edges having an
equal probability being selected (e.g., 0.8-1). Then as the iteration goes on, the temperature decays
by following the annealing schedule Φ(k) (Eqn. 6) and the sampling ratio follows the sampling pol-
icy λ(k) (Eqn. 4). As a result, those edges that are less important for preserving graph monotonicity
have their probability gradually reduced. This process keeps going until the distribution becomes
very biased: most of the edge probability are distributed around the two peaks. This is expected
because as Theorem A.1 shows, the optimization eventually converge with the joint distribution of
Re(T) for all edges e being equal for T → ∞ and sparsely supported for T → 0.
5.3	Comparison with Existing Methods
We include the comparison between APG and two state-of-the-art proximity graphs: (1)
HNSW (Malkov & Yashunin, 2016) and (2) NSG (Navigable Spread-out Graph) (Fu et al., 2019).
9
Under review as a conference paper at ICLR 2020
We further report the comparison group results from the corresponding sparse counterparts: (3)
HNSW-sparse and (4) NSG-sparse, both of which have a similar number of edges like ours. We also
provide a comparison to (5) HNSW with random pruning 1.
We first consider the compromise between the search time and accuracy. Fig. 5 illustrates the
accuracy-latency tradeoff between APG and the other configurations. We observe that on SIFT1M,
APG is reaches the highest accuracy (0.995) when having the same search time budget (e.g., 0.2ms)
and is the fastest to reach the same accuracy (e.g., 0.99), which indicates that by pruning redundant
edges APG makes graph similarity search faster. On the other hand, compared to the sparse coun-
terparts, APG delivers better accuracy-latency tradeoff than both HNSW-sparse and NSG-sparse.
APG is 34% and 66% faster than HNSW-sparse and NSG-sparse to reach the same accuracy (0.98)
and achieves the highest accuracy under the same search time budget (e.g., <0.3ms), which indi-
cates it is preferable to first build a proximity graph with a large number of edges and then prune
to obtain a simplified one than directly build a proximity graph with size comparable to the pruned
graph. Random pruning has the worst performance, because it considers neither the graph structure
nor edge importance. For Deep1M and GloVe, APG outperforms HNSW-sparse, NSG-sparse, and
HNSW-rand. Compared to HNSW and NSG, APG achieves almost on-par accuracy-latency trade-
off, but it has 50% less number of edges than HNSW and NSG (Fig. 5d) and therefore is much more
memory efficient.
(a) SIFT1M
(b) Deep1M
Figure 5: Comparison to existing approaches and alternatives.
6 Concluding Remarks
Proximity graph is an important data structure for building large scale vector search engine of
many machine learning systems. It is crucial to make it answer queries with low latency, low
memory cost, and high accuracy. To the best of our knowledge, this is the first work on prox-
imity graphs that demonstrate that we can learn to anneal and prune proximity graphs without
losing much accuracy. This has several benefits: pruned edges reduce the memory cost; and the
pruned proximity graphs perform similarity search 21-41% faster than existing and the State-Of-
the-art approaches with minimal loss of accuracy. The cost is a small investment on learning and
optimization time. We open-sourced the code at https://drive.google.com/open?id=
15vGhNS0O9l-zPAbdPAxwxIzV558jjGeQ.
1APG is built with R = 64 and a pruning ratio σ = 0.5. (1) and (2) are built with R = 64, (3) and (4) are
built with R = 32. (5) is built with R = 64 but with 50% of edges randomly deleted.
10
Under review as a conference paper at ICLR 2020
References
Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. Target Apps Selec-
tion: Towards a Unified Search Framework for Mobile Devices. In SIGIR 2018, pp. 2l5-224,
2018.
Artem Babenko and Victor S. Lempitsky. Efficient Indexing of Billion-Scale Datasets of Deep
Descriptors. In CVPR 2016, pp. 2055-2063, 2016.
Dmitry Baranchuk, Artem Babenko, and Yury Malkov. Revisiting the Inverted Indices for Billion-
Scale Approximate Nearest Neighbors. In ECCV 2018, pp. 209-224, 2018.
Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger. The R*-Tree: An
Efficient and Robust Access Method for Points and Rectangles. In SIGMOD 1990, pp. 322-331,
1990.
Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning, volume 1. Citeseer, 2017.
Jon Louis Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Com-
munications of the ACM, 18(9):509-517, September 1975. ISSN 0001-0782.
Leon BottoU and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In Advances in Neu-
ral Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on
Neural Information Processing Systems, pp. 161-168, 2007.
Simon R Broadbent and John M Hammersley. Percolation processes: I. Crystals and mazes. In
Mathematical Proceedings of the Cambridge Philosophical Society, volume 53, pp. 629-641,
1957.
Duncan S Callaway, Mark EJ Newman, Steven H Strogatz, and Duncan J Watts. Network robustness
and fragility: Percolation on random graphs. Physical review letters, 85(25):5468, 2000.
Qi Chen, Haidong Wang, Mingqin Li, Gang Ren, Scarlett Li, Jeffery Zhu, Jason Li, Chuanjie Liu,
Lintao Zhang, and Jingdong Wang. SPTAG: A library for fast approximate nearest neighbor
search, 2018. URL https://github.com/Microsoft/SPTAG.
DW Dearholt, N Gonzales, and G Kurup. Monotonic search networks for computer vision databases.
In Twenty-Second Asilomar Conference on Signals, Systems and Computers, volume 2, pp. 548-
553, 1988.
Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. Neural
Ranking Models with Weak Supervision. In SIGIR 2017, pp. 65-74, 2017.
MatthijS Douze, Alexandre Sablayrolles, and Herve Jegou. Link and Code: Fast Indexing With
Graphs and Compact Regression Codes. In CVPR, 2018.
Cong Fu and Deng Cai. EFANNA : An Extremely Fast Approximate Nearest Neighbor Search
Algorithm Based on kNN Graph. CoRR, abs/1609.07228, 2016.
Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast Approximate Nearest Neighbor Search
with the Navigating Spreading-out Graph. In VLDB’19, 2019.
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized Product Quantization for Approximate
Nearest Neighbor Search. In CVPR 2013, 2013.
Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity Search in High Dimensions via
Hashing. In VLDB’99, pp. 518-529, 1999.
Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. A Deep Relevance Matching Model for
Ad-hoc Retrieval. In CIKM 2016, pp. 55-64, 2016.
Kiana Hajebi, Yasin Abbasi-Yadkori, Hossein Shahbazi, and Hong Zhang. Fast Approximate
Nearest-neighbor Search with K-nearest Neighbor Graph. In IJCAI’11, 2011.
11
Under review as a conference paper at ICLR 2020
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing Systems, pp. 1135-1143,
2015b.
D. Frank Hsu, Xiaojie Lan, Gabriel Miller, and David Baird. A Comparative Study of Algorithm
for Computing Strongly Connected Components. In DASC’17, 2017.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep
structured semantic models for web search using clickthrough data. In CIKM ’13, pp. 2333-2338,
2013.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,
pp. 604-613, 1998.
Lester Ingber. Simulated annealing: Practice versus theory. Mathematical and computer modelling,
18(11):29-57, 1993.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. In Product Quantization for Nearest Neighbor
Search, 2011.
Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with GPUs. CoRR,
abs/1702.08734, 2017. URL http://arxiv.org/abs/1702.08734.
Yannis Kalantidis and Yannis S. Avrithis. Locally Optimized Product Quantization for Approximate
Nearest Neighbor Search. In CVPR 2014, pp. 2329-2336, 2014.
Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.
Science, 220(4598):671-680, 1983.
D. T. Lee and Bruce J. Schachter. Two algorithms for constructing a Delaunay triangulation. Inter-
national Journal of Parallel Programming, 9(3):219-242, 1980.
Victor Lempitsky. The inverted multi-index. In CVPR ’12, pp. 3069-3076, 2012.
Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Wenjie Zhang, and Xuemin Lin. Approximate Nearest
Neighbor Search on High Dimensional Data - Experiments, Analyses, and Improvement. IEEE
Transactions on Knowledge and Data Engineering, 2019.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the Value
of Network Pruning. In 7th International Conference on Learning Representations, ICLR 2019,
2019.
Yury Malkov, Alexander Ponomarenko, Andrey Logvinov, and Vladimir Krylov. Approximate near-
est neighbor algorithm based on navigable small world graphs. Inf. Syst., 45:61-68, 2014.
Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using
Hierarchical Navigable Small World graphs. CoRR, arXiv preprint abs/1603.09320, 2016.
Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to Match using Local and Distributed
Representations of Text for Web Search. In WWW 2017, 2017.
Marius Muja and David G. Lowe. Scalable Nearest Neighbor Algorithms for High Dimensional
Data. TPAMI 2014, 36(11):2227-2240, 2014.
Rodrigo Nogueira and Kyunghyun Cho. Passage Re-ranking with BERT. CoRR, abs/1901.04085,
2019.
Mohammad Norouzi and David J. Fleet. Cartesian K-Means. In CVPR 2013, 2013.
Yaghout Nourani and Bjarne Andresen. A comparison of simulated annealing cooling strategies.
Journal of Physics A: Mathematical and General, 31(41):8373, 1998.
12
Under review as a conference paper at ICLR 2020
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global Vectors for Word
Representation. In EMNLP, pp. 1532-1543, 2014.
Danny Sullivan. Faq: All about the google rankbrain algorithm. 2018.
Christophe Van Gysel, Maarten de Rijke, and Evangelos Kanoulas. Learning Latent Vector Spaces
for Product Search. In CIKM ’16, pp. 165-174, 2016.
Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. Billion-
scale Commodity Embedding for E-commerce Recommendation in Alibaba. In KDD 2018, pp.
839-848, 2018a.
Mengdi Wang, Qing Zhang, Jun Yang, Xiaoyuan Cui, and Wei Lin. Graph-Adaptive Pruning for
Efficient Inference of Convolutional Neural Networks. CoRR, abs/1811.08589, 2018b.
Ronald J. Williams and David Zipser. A Learning Algorithm for Continually Running Fully Recur-
rent Neural Networks. Neural Computation, 1(2):270-280, 1989.
Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-End Neural
Ad-hoc Ranking with Kernel Pooling. In SIGIR 2017, pp. 55-64, 2017.
Peter N. Yianilos. Data Structures and Algorithms for Nearest Neighbor Search in General Metric
Spaces. In SODA ’93, pp. 311-321, 1993.
Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. Deep Learning for Answer
Sentence Selection. CoRR, abs/1412.1632, 2014.
Hamed Zamani, Mostafa Dehghani, W. Bruce Croft, Erik G. Learned-Miller, and Jaap Kamps. From
Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing.
In CIKM 2018, pp. 497-506, 2018a.
Hamed Zamani, Bhaskar Mitra, Xia Song, Nick Craswell, and Saurabh Tiwary. Neural Ranking
Models with Multiple Document Fields. In WSDM ’18, 2018b.
Michael Zhu and Suyog Gupta. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for
Model Compression. In ICLR 2018, 2018.
13
Under review as a conference paper at ICLR 2020
A Proof of Theorem
Theorem A.1. Let G0 (V, E0) be the original graph to prune, and let Q be the query set. Suppose
there exists a subset ofedges E* ⊆ Eo such that the average recall ofretrieving the nearest neighbor
in V for all queries q ∈ Q using edges in E* is 1. Let G(k) be the random graph at iteration k
running Algorithm 1, and let {Re (k)} be the family of Bernoulli random variables defined on the
edges of G(k), with P(Re(k) = 1) = Pe(k). Assume thatPe(ke) < 2 for e ∈ Eo\E* at some SteP
ke. Also assume	that T(k) ≥ T(k	+ 1) for all k. Ifσ = 1 - |E* |/|E0|,	then:
(1)	Ee∈E*	Pe(k + 1)	≥ Ee∈E* Pe(k) for all sufficiently	large	k.	In particular,
limk→∞ Pe∈E* Pe(k) exists.
(2)	If limk→∞ T(k) = 0, then limk→∞ pe(k) = 1for all e ∈ E*, and limk→∞ pe(k) = 0for
all e ∈ E0\E*.
(3)	With the condition in (2), suppose r(k) denotes the average expected recall of retrieving the
ground truth nearest neighbor ofa query q ∈ Q in a subgraph of G(k) randomly sampled
from the joint distribution {Re(k)}. Then limk→∞ r(k) = 1.
Theorem A.1. We start with showing (1). Note that the probability sum constraints
∑ Pe(k) +	∑ Pe(k) = (1 - σ)∣Eo∣ = |E*|
is satisfied for all k. It then suffices to show that
X pe(k + 1) ≤ X pe(k)
e∈E∖E*	e∈E∖E*
for sufficiently large k. Observe that we(k) remains a constant (= we(0)) for all k and all
e ∈ Eo∖E*, and that μ(k) monotonically decreases as k increases. We claim thatpe(k +1) ≤ Pe(k)
for all k ≥ ke. Together with the probability sum constraints this shows that Pe∈E* pe(k)
is monotonically increasing for k ≥ maxe ke. Since Pe∈E* pe(k) is bounded from above,
limk→∞ Pe∈E* pe(k) = supk Pe∈E* pe(k) exists.
For (2), since limk Tk = 0, observe that limk→∞ pe(k) → 0 for all e ∈ E0\E*. Thus the probability
sum constraint enforces that limk Pe∈E* pe(k) = |E* |. Recall that liminfk xk + lim supk yk ≥
lim infk (xk + yk) for any real sequences xk and yk. Now consider any e ∈ E*, we have
lim inf pe(k)
k→∞
≥
lim
k→∞
pe(k) - lim sup	pe0 (k)
e∈E*	k→∞ e0∈E*∖{e0}
≥
|E*|-
Σ
e0∈E* \{e0}
lim sup pe0 (k)
k→∞
≥
|E*|-	1
e0∈E* \{e0}
|E*|-(|E*| -1)= 1.
which implies that limk pe(k) = 1.
To prove (3), we first define the notation χ(q, E) to be the indicator function of whether the ground
truth nearest neighbor of a query q can be retrieved in G using a subset of edges E ⊆ E0 . More
precisely, χ(q, E) = 1 if the nearest neighbor of q can be retrieved using edges in E; and χ(q, E) =
0 otherwise. Note that the average recall at step k can be written as
Irk = Q X X χ(q, E)P(E is chosen from G(k)),
|Q| q∈Q E⊆E0
14
Under review as a conference paper at ICLR 2020
where the second summation is amongst all subsets of E⅛. Since each edge being chosen is inde-
pendent of other edges, we can expand the probability and it follows that
⅛ XX XaE) Y pe(k) ∏(1- Pe(k)).
q∈QECE0 ∖	e∈E	e∈E	)
Since we assumed that χ(q, E*) = 1 for all q ∈ Q, it follows that χ(q, E) = 1 for all E ⊇ E*.
Thus
Tk ≥ |Q| X X I ∏ PeM ∏ (1- Pe(k)) I .
q∈Q E*CECE0 ∖ e∈E	e∈E	I
Note that
E I ∏Pe(k)∏(1- Pe(k)))
E*CECE0 ∖e∈E	e∈E	)
X I ∏ Pe(k)	∏	Pe(k) ∏ (1	- Pe(k))
E*CECE0 ∖e∈E*	e∈E∖E*	e∈E
∏ Pe(k) X	I	∏	Pe(k) ∏ (1	- Pe(k))
e∈E*	E* CECE0	e∈E∖E*	e∈E
∏ pe(k)	X	P(E is chosen from E0∖E*)
e∈E*	E* CECE0
∏ Pe㈤.
Therefore
≥
lim inf Tk
k-∞
≥
≥
IimCinf |Q| X ∏ Pe⑻
q∈Q e∈E*
|Q| X lik→i∞f ∏ Pe⑻
q∈Q	e∈E*
|Q| X ∏ l⅛fPe⑻
q∈Q e∈E*
卷S j
This shows that limk→∞ Tk = 1 and completes the proof of the Theorem.
□
15
Under review as a conference paper at ICLR 2020
B Frequency Distribution of Degree
(a) In-degree	(b) Out-degree
Figure 6: Frequency distributions of in-degree and out-degree of proximity graph before and after
pruning over SIFT1M.
C Internal of APG Learning Process
Figure 7: Distribution of keep probability at different steps of the optimization.
16