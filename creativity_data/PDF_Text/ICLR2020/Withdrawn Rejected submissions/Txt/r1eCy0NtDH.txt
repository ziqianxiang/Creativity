Under review as a conference paper at ICLR 2020
Mean-field Behaviour of Neural Tangent
Kernel for Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Recent work by Jacot et al. (2018) has showed that training a neural network of
any kind with gradient descent in parameter space is equivalent to kernel gradient
descent in function space with respect to the Neural Tangent Kernel (NTK). Lee
et al. (2019) built on this result to show that the output of a neural network trained
using full batch gradient descent can be approximated by a linear model for wide
networks. In parallel, a recent line of studies (Schoenholz et al. (2017), Hayou
et al. (2019)) suggested that a special initialization known as the Edge of Chaos
leads to good performance. In this paper, we bridge the gap between these two
concepts and show the impact of the initialization and the activation function on
the NTK as the network depth becomes large. We provide experiments illustrating
our theoretical results.
1	Introduction
Deep neural networks have achieved state-of-the-art results on numerous tasks; see, e.g., Nguyen
& Hein (2018), Du et al. (2018b), Zhang et al. (2017). Although the loss function is not convex,
Gradient Descent (GD) methods are often used successfully to learn these models. It has been actually
recently shown that for certain overparameterized deep ReLU networks, GD converges to global
minima ((Du et al., 2018a)). Similar results have been obtained for Stochastic Gradient Descent
(SGD) ((Zou et al., 2018)).
The training dynamics of wide neural networks with GD is directly linked to kernel methods. Indeed,
Jacot et al. (2018) showed that training a neural network with full batch GD in parameter space is
equivalent to a functional GD i.e. a GD in a functional space with respect to a kernel called Neural
Tangent Kernel (NTK). Du et al. (2019) used a similar approach to prove that full batch GD converges
to global minima for shallow neural networks and Karakida et al. (2018) linked the Fisher Information
Matrix to the NTK and studied its spectral distribution for infinite width networks. The infinite
width limit for different architectures was studied by Yang (2019) who introduced a tensor formalism
that can express most of the computations in neural networks. Lee et al. (2019) studied a linear
approximation of the full batch GD dynamics based on the NTK and gave an method to approximate
the NTK for different architectures. Finally, Arora et al. (2019) gives an efficient algorithm to
compute exactly the NTK for convolutional architectures (Convolutional NTK or CNTK). In all of
these papers, authors studied only the effect of infinite width on the NTK. The aim of this paper is to
tackle the infinite depth limit.
In parallel, the impact of the initialization and activation function on the performance of wide
deep neural networks has been studied in Hayou et al. (2019), Lee et al. (2018), Schoenholz et al.
(2017), Yang & Schoenholz (2017). These works analyze the forward/backward propagation of
some quantities through the network at the initial step as a function of the initial parameters and the
activation function. They propose a set of parameters and activation functions so as to ensure a deep
propagation of the information at initialization. While experimental results in these papers suggest
that such selection also leads to overall better training procedures (i.e. beyond the initialization step),
it remains unexplained why this is the case. In this paper, we link the initialization hyper-parameters
and the activation function to the behaviour of the NTK which controls the training of DNNs, this
could potentially explain the good performance. We provide a comprehensive study of the impact
of the initialization and the activation function on the NTK and therefore on the resulting training
dynamics for wide and deep networks. In particular, we show that an initialization known as the Edge
1
Under review as a conference paper at ICLR 2020
of Chaos (Yang & Schoenholz, 2017) leads to better training dynamics and that a class of smooth
activation functions discussed in (Hayou et al., 2019) also improves the training dynamics compared
to ReLU-like activation functions (see also Clevert et al. (2016)). We illustrate these theoretical
results through simulations. All the proofs are detailed in the Supplementary Material which also
includes additional theoretical and experimental results.
2	Motivation and Related work
Neural Tangent Kernel
Recent work by Jacot et al. (2018) has shown that the training dynamics of neural networks are
captured by the Neural Tangent Kernel (NTK). In the infinite width limit (wide neural networks),
the NTK converges to a kernel that remains unchanged as the training time grows. While this is
only true in the infinite width limit, Lee et al. (2018) showed that a first order linear approximation
of the training dynamics (approximation of the NTK by its value at the initialization step) leads to
comparable performances for different architectures. More recently, Bietti & Mairal (2019) studied
the RKHS of the NTK for a two layers convolutional neural network with ReLU activation and
provided a spectral decomposition of the kernel, while in Arora et al. (2019), the authors propose an
algorithm to compute the NTK for convolutional neural networks. However, for finite width neural
networks, Arora et al. (2019) observed a gap between the performances of the linear model derived
from the NTK and the deep neural network, which is mainly due to the fact that the NTK changes
with time. To fill this gap, Huang & Yau (2019) studied the dynamics of the NTK as a function of the
training time for finite width neural networks and showed that the NTK dynamics follow an infinite
hierarchy of ordinary differential equations baptised Neural Tangent Hierarchy (NTH). In this paper,
we consider the limit of infinite width neural networks (mean-field approximation), and we study the
behaviour of the NTK as the network depth goes to infinity assuming the width is infinite.
Edge of Chaos and Activation Function
Recent works by Hayou et al. (2019) and Schoenholz et al. (2017) have shown that weight initialization
plays a crucial role in the training speed of deep neural networks (DNNs). In Schoenholz et al.
(2017), the authors demonstrate that only a special initialization can lead to good performance. This
initialization is known as the ’Edge of Chaos’ since it represents a transition between two phases : an
ordered phase and a chaotic phase. When the DNN is initialized on the ordered phase, the output
function of the DNN is constant almost everywhere, because the correlation of the outputs of two
different inputs converges to 1 as the number of layers becomes large. On the other hand, when the
DNN is initialized on the Chaotic phase, the output function is discontinuous almost everywhere as
the depth goes to infinity. In this case, the correlation between the outputs of two different inputs
converges to a value c such that |c| < 1, therefore, very close inputs may lead to very different outputs.
In Hayou et al. (2019), authors give a comprehensive analysis of the Edge of Chaos, and further show
that a certain class of smooth activation functions outperform the ReLU-like activation functions in
term of test accuracy on MNIST and CIFAR10.
Our contributions
In this paper, we bridge the gap between the two previous concepts of Neural Tangent Kernel and
Edge of Chaos Initialization for DNNs. More precisely, we study the impact of the Edge of Chaos
initialization and the activation function on the NTK as the depth L goes to infinity. Our main results
are :
1.	With an Initialization on the ordered/chaotic phase, the NTK converges exponentially to
a constant kernel with respect to the depth L, making the training impossible for DNNs
(Lemma 1 and Proposition 1)
2.	The Edge of Chaos initialization leads to an invertible NTK even in the infinite depth limit,
making the model trainable even for very large depths (Proposition 2)
3.	The Edge of Chaos initialization leads to a sub-exponential convergence rate of the NTK
to the limiting NTK (w.r.t to L), which means that the ’information’ carried by the NTK
propagates deeper compared to an initialization on the ordered/chaotic phase (Propositon 2)
2
Under review as a conference paper at ICLR 2020
4.	Using a certain class S of smooth activation functions can further slow this convergence,
making this class of activation functions more suitable for DNNs
5.	When adding Residual connections, we no longer need the initialisation on the Edge of
Chaos, and the convergence of the NTK to the limiting NTK is always at a polynomial rate
3	Neural Networks and Neural Tangent Kernel
3.1	Setup and notations
Consider a neural network model consisting of L layers (yl)1≤l≤L, with yl : Rnl-1 → Rnl, n0 = d
and let θ = (θl)1≤l≤L be the flattened vector of weights and bias indexed by the layer’s index and p
be the dimension of θ. Recall that θl has dimension nl + 1. The output f of the neural network is
given by some transformation s : RnL → Ro of the last layer yL(x); o being the dimension of the
output (e.g. number of classes for a classification problem). For any input x ∈ Rd, we thus have
f(x, θ) = s(yL(x)) ∈ Ro. As we train the model, θ changes with time t and we denote by θt the
value of θ at time t and ft(x) = f(x, θt) = (fj (x, θt), j ≤ o). Let D = (xi, yi)1≤i≤N be the data
set and let X = (xi)1≤i≤N, Y = (yj)1≤j≤N be the matrices of input and output respectively, with
dimension d × N and o × N. For any function g : Rd×o → Rk, k ≥ 1, we denote by g(X , Y) the
matrix (g(xi, yi))1≤i≤N of dimension k × N.
Jacot et al. (2018) studied the behaviour of the output of the neural network as a function of the
training time t when the network is trained using a gradient descent algorithm. Lee et al. (2019) built
on this result to linearize the training dynamics. We recall hereafter some of these results.
For a given θ, the empirical loss is given by L(θ) = N PN=I '(f (xi, θ), Iyi). The full batch GD
algorithm is given by
O	O	__ .,0.
%+ι = θt - ηVθL(Bt)	(1)
where η > 0 is the learning rate.
Let T > 0 be the training time and Ns = T∕η be the number of steps of the discrete GD equation 1.
The continuous time system equivalent to equation 1 with step ∆t = η is given by
dθt = -Vθ L(θt)dt
(2)
This differs from the result by Lee et al. (2019) since we use a discretization step of ∆t = η. It is
well known that this discretization scheme leads to an error of order O(η) (see Appendix). As in Lee
et al. (2019), Equation (2) can be re-written as
dθt = -NVθf(X,θt)TVz'(f(X,θt), Y)dt
where Vθf (X, θt) is a matrix of dimension oN X P and Vz'(f (X, θt), Y) is the flattened vector of
dimension oN constructed from the concatenation of the vectors Vz'(ζ, yi)∣z=f (xi,θt), i ≤ N. As a
result, the output function ft(x) satisfies the following ordinary differential equation
dft(x) = Vθ f(x,θt )dθt = - -N Vθ f (x,θt)Vθ f (X ,θt)T Vz '(ft (X), Y )dt ∈ Ro (3)
The Neural Tangent Kernel (NTK) KθL is defined as the o × o dimensional kernel satisfying: for all
x, x0 ∈ Rd,
KθLt(x,x0) = Vθf(x, θt)Vθf(x0, θt)T ∈ Ro×o
L
= XVθlf(x,θt)Vθlf(x0,θt)T.
l=1
(4)
We also define KθL (X, X) as the oN × oN matrix defined blockwise by
KθLt (x1, x1)
KL(X, X) =	KLt(X2,x1)
KθLt (xN, x1)
KL(x1,x2)… KL(X1,XN) ʌ
…	…KL (X2,XN)
• ∙ ∙	∙ ∙ ∙	∙ ∙ ∙
KL (XN ,X2)… KLt (XN ,XN ))
3
Under review as a conference paper at ICLR 2020
By applying equation 3 to the vector X, one obtains
dft(X ) = - N KLt (X, X Xz '(ft(X ),Y )dt,	(5)
meaning that for all j ≤ Ndft(Xj) = — NnKL (Xj, X)vz'(ft(X), Y)dt.
Infinite width dynamics : In the case of a fully connected feedforward neural network (FFNN)
of depth L and widths n1, n2, ..., nL, Jacot et al. (2018) proved that, with GD, the kernel KθL
converges to a kernel KL which depends only on L (number of layers) for all t < T when
n1, n2, ..., nL → ∞, where T is an upper bound on the training time, under the technical as-
sumption RT ||Vz '(ft(X, Y ))∣∣2dt < ∞ almost surely with respect to the initialization weights. The
infinite width limit of the training dynamics is given by
dft(X) = -NKl(x, X)Vz'(ft(X), Y)dt,	(6)
We note hereafter KL = Kl(X, X). As an example, with the quadratic loss '(ζ,y) = 2||z 一 y||2 3 4 5,
equation 6 is equivalent to
dft(X ) = 一 N K L(ft (X)-Y )dt,	(7)
which is a simple linear model that has a closed-form solution given by
ft(X) = e-NKLtfo(X) + (I -e-NKLt)Y.	(8)
For general input X ∈ Rd , we then have
ft(x) = fo(x) + KL(x, X)Kl(X, X)-1 (I - e-NKLt)(Y - fo(X))	(9)
Note that in order for ft(x) to be defined, KL must be invertible. Indeed, it turns out that training
with dynamics 6 is only possible if the NTK is invertible. We shed light on this behaviour in the
following Lemma.
Lemma 1 (Trainability of the Neural Network and Invertibility of the NTK). Assume f0(X) 6= Y.
L
Then with dynamics defined by equation 8, ||ft(X) - Y|| converges to 0 as t → ∞ if and only if KL
is non-singular.
Moreover, if KL is singular, there exists a constant C > 0 such that for all t > 0,
||ft(X)-Y|| ≥C
Lemma 1 shows that an invertible NTK is crucial for trainability. Since KθL = KL is constant w.r.t to
training time, it is completely determined at the initialization step. It is therefore intuitive to study the
impact of the initialization on the NTK, particularly as the number of layers L grows (Deep Neural
Networks), which is our focus in this paper. Another interesting aspect is the impact of the NTK on
the generalization error of the neural network model. To see this, if the NTK is constant for example
(i.e. there exists a constant δ such as KL(X, X0) = δ for all X 6= X0, this example is useful in the next
section), then the second part of ft(X) in equation 9 is constant w.r.t X. Therefore, the generalization
function ft(X) of the model 9 is entirely given by its value at time zero f0(X), which means that the
generalisation error Ex,y[||ft(X) - y||] remains of order O(1).
In the next section, we show that the initialization and the activation function have major impact on
the invertibility and ’expressivity’ of NTK. More precisely, we show that :
1. Under some constraints, the NTK KL (or a scaled version of the NTK) converges to a
limiting NTK K∞ as L goes to infinity (otherwise it diverges)
2. A special initialization known as the Edge of Chaos (EOC) leads to an invertible K∞ which
makes it useful for training DNNs
3. The EOC initialization gives a sub-exponential rate for this convergence (w.r.t L), which
means for the same depth L, the EOC gives ’richer’ limiting NTK, and therefore leading to
better generalization properties
4. The smoothness of the activation can further slow this convergence, leading to ’richer’
limiting NTK (the convergence to the limiting trivial kernel is slower)
5. Adding Residual connections leads to sub-exponential convergence rate for the NTK (w.r.t
to L) and we no longer need the Edge of Chaos
4
Under review as a conference paper at ICLR 2020
4 Impact of the Initialization and the Activation function on the
Neural Tangent Kernel
In this section we study the impact of the initialization and the activation function on the limiting NTK
for Fully-connected Feed-forward Neural Networks (FFNN). We prove that only an initialization on
the Edge of Chaos (EOC) leads to an invertible NTK for deep neural networks. All other initializations
will lead to a trivial non-invertible NTK. We also show that the smoothness of the activation function
plays a major role in the behaviour of NTK. To simplify notations, we restrict ourselvs to the case
s(x) = x and o = 1, since generalization to any function s and any nL is straightforward.
Consider a FFNN of depth L, widths (nl)1≤l≤L, weights wl and bias bl. For some input x ∈ Rd, the
forward propagation is given by
d	nl-1
yi1(x) = X wi1j xj +bi1, yil(x) = X wiljφ(yjl-1(x)) +bli, forl ≥ 2,	(10)
j=1	j=1
where φ is the activation function.
We initialize the model with Wj iid N(0, n^) and b 理 N(0, σ2), where N(μ, σ2) denotes the
normal distribution of mean μ and variance σ2. For some x, we denote by ql (x) the variance of yl(χ).
The convergence of ql(x) as l increases is studied in Lee et al. (2018), Schoenholz et al. (2017) and
Hayou et al. (2019). In particular, under weak regularity conditions they prove that ql(x) converges to
a point q(σb, σw) > 0 independent of x as l → ∞. Also the asymptotic behaviour of the correlations
between yl(x) and yl(x0) for any two inputs x and x0 is driven by (σb, σw); the authors define the
EOC as the set of parameters (σb, σw) such that σWE[φ0( yzq(σb, σw)Z)2] = 1 where Z 〜N(0,1).
Similarly the Ordered, resp. Chaotic, phase is defined by σWE[φ0(,q(σb, σ3)Z)2] < 1, resp.
σw2 E[φ0( √q(σb, σw)Z)2] > 1; more details are recalled in Section 2 of the supplementary material.
It turns out that the EOC plays also a crucial role on the NTK. Let us first define two classes of
activation functions.
Definition 1. Let φ : R → R be a measurable function. Then
1.	φ is said to be ReLU-like if there exist λ, β ∈ R such that φ(x) = λx for x > 0 and
φ(x) = βx for x ≤ 0.
2.	φ is said to be in S if φ(0) = 0, φ is twice differentiable, and there exist n ≥ 1, a
partition (Ai)1≤i≤n of R and infinitely differentiable functions g1, g2, ..., gn such that
φ(2) = Pin=1 1Aigi, where φ(2) is the second derivative of φ.
The class of ReLU-like activations includes ReLU and Leaky-ReLU, whereas the S class includes,
among others, Tanh, ELU and SiLU (Swish). The following proposition establishes that any initial-
ization on the Ordered or Chaotic phase, leads to a trivial limiting NTK as the number of layers L
becomes large.
Proposition 1 (Limiting Neural Tangent Kernel with Ordered/Chaotic Initialization). Let (σb, σw )
be either in the ordered or in the chaotic phase. Then, there exist λ, γ > 0 such that
sup |KL(x,x0) — λ∣ ≤ e-γL →l→∞ 0
x,x0 ∈Rd
As a result, as L goes to infinity, KL converges to a constant kernel K∞(x, x0) = λ for all x, x0 ∈ Rd.
The training is then impossible. Indeed, we have KL(X, X) ≈ λU where U is the matrix with
all elements equal to one, i.e. KL is at best degenerate and asymptotically (in L) non invertible,
rendering the training impossible by Lemma 1. We illustrate empirically this result in Section 5.
Recall that the (matrix) NTK for input data X is given by
L
KL (X, X) = Vθ f (X ,θt)Vθ f (X ,θt)T = X Vθ" (X ,θt)Vθι f (X ,θt)T
l=1
5
Under review as a conference paper at ICLR 2020
As shown in Schoenholz et al. (2017) and Hayou et al. (2019), an initialization on the EOC preserves
the norm of the gradient as it back-propagates through the network. This means that the terms
▽仇 f (X, θt)Vθι f (X ,θt )T are of the same order. Hence, it is more convenient to study the average
NTK (ANTK hereafter) given by KL/L. Note that the invertibility of the NTK is equivalent to that
of the ANTK. The next proposition shows that on the EOC, the ANTK converges to an invertible
kernel as L → ∞ at a sub-exponential rate. Moreover, by choosing an activation function in the
class S, we can slow the convergence of ANTK with respect to L, which means that, for the same
depth L, a smooth activation function from the class S leads to ’richer’ NTK which is crucial for the
generalization error of deep models as discussed in Section 3. This confirms the findings in (Hayou
et al., 2019).
Proposition 2 (Neural Tangent Kernel on the Edge of Chaos). Let φ be a non-linear activation
function and (σb, σw ) ∈ EOC.
1.	If φ is ReLU-like, then forall X ∈ Rd, K RVx = σw dx||——+ K LlX). Moreover, there exist
A, λ ∈ (0, 1) such that
KL(x, x0 )	σw2	0 A	0	σw2 ||x||kx0 k
SUp I -V^-λ谭∣∣χ∣∣∣∣χ01|| ≤ -, K∞(X,X0) = Wl3"(1-(1-λ)1χ=χ,)
X6=X0∈Rd	L	d	L	d
2.	If φ is in S, then, there exists q > 0 such that K LLX网X = q + K 片) → q. Moreover, there
exist B, C, λ ∈ (0, 1) such that
Blog(L)	i KL(x,x0)	i Clog(L)	0「门门	Nl 、
≤	≤ SUp 1	~	qλ 1 ≤	,	, K∞(x,x ) = q(1 - (I - λ) 1X = X0)
L	χ6=χ0∈Rd	L	L
Since 0 < λ < 1, on the EOC there exists a matrix J invertible such that KL(X, X) =L×J(1+o(1))
asL→ ∞. Hence, although the NTK grows linearly withL, it remains asymptotically invertible.
This makes the training possible for deep neural networks when initialized on the EOC, contrariwise
to an initialization on the Ordered/Chaotic phase, see Proposition 1). However the limiting kernels
K∞ carry (almost) no information on x, x0 and have therefore little expressive power. Interestingly
the convergence rate of the ANTK to K∞ is slow inL(O(L-1) for ReLU-like activation functions
and O(log(L)L-1) for activation functions of type S). This means that as L grows, the NTK remains
expressive compared to the Ordered/Chaotic phase case (exponential convergence rate). This is
particularly important for the generalization part (see equation 9). The log(L) gain obtained when
using smooth activation functions of type S means we can train deeper neural networks with this
kind of activation functions compared to the ReLU-like activation functions and could explain why
ELU and Tanh tend to perform better than ReLU and Leaky-ReLU (see Section 5).
Another important feature of deep neural network which is known to be highly influential is their
architecture. The next proposition shows that adding residual connections to a ReLU network leads
to a polynomial rate for wide range of initialization parameters.
Proposition 3 (Residual connections). Consider the following network architecture (FFNN with
residual connections)
nl-1
yil(x) = yil-1(x) + X wiljφ(yjl-1(x)) + bli, forl ≥ 2.	(11)
j=1
with initialization parameters σb = 0 and σw > 0 and φ is the ReLU activation function. Let KrLes be
the corresponding NTK. Thenfor all X ∈ Rd, Kres×XLχ) =与——+ O(yl) and there exists λ ∈ (0,1)
such that
sup iKres(Xs') - ∣∣x∣∣ ×∣∣x∣∣λι =O(LT),
X6=X0 ∈Rd αL × 2	d
where αl and γl are given by
•	if σw <	√2,	then	aL	= 1 and YL	= (1+：w /)L
•	if σw =	√2,	then	aL	= L and YL	= LT
6
Under review as a conference paper at ICLR 2020
(a) EOC
(b) Ordered phase	(c) FFNN with residual connections
Figure 1: Convergence rates for different initializations and architectures. (a) Edge of Chaos. (b)
Ordered phase. (c) Adding residual connections.
• if Ow > √, then Ql = (1+2w/ )L and YL = (1+σw/ )-l
Proposition 3 shows that the NTK of a ReLU FFNN with residual connections explodes exponentially
with respect to L. However, the normalised kernel KLes (x, x0)∕ql2l where X = x0 converges to a
limiting kernel similar to K∞ with a rate O(L-1) for all σw > 0. We say that residual networks ’live’
on the Edge of Chaos, i.e. no matter what the choice of σw is, the convergence rate of the NTK w.r.t
L is polynomial and there is no Ordered/Chaotic phase in this case. This could potentially explain
why residual networks perform better than FFNN (RELU) in many tasks when the initialization is
not on the EOC. We illustrate this result in section 5.
5	Experiments
In this section, we illustrate empirically the theoretical results obtained in the previous sections. We
first illustrate the results of Propositions 1, 2 and 3. Then, we confirm the impact of the EOC and
Activation function on the overall performance of the model (FFNN), on MNIST and CIFAR10
datasets.
5.1	CONVERGENCE RATE OF KL AS L GOES TO INFINITY
Propositions 1, 2 and 3 give theoretical convergence rates for quantities of the form IKL - K∞ |.
αL
We illustrate these results in Figure 1. Figure 1a shows a convergence rate approximately equal to
O(L-1) for ReLU and ELU. Recall that for ELU the exact rate is O(log(L)L-1) but one cannot
observe experimentally the logarithmic factor. However, ELU performs indeed better than ReLU (see
Table 1) which might be explained by this log(L) factor. Figure 1b demonstrates that this convergence
occurs at an exponential convergence rate in the Ordered phase for both ReLU and ELU, and Figure
1c the convergence rate in the case of FFNN with residual connections. As predicted by Proposition
3, the convergence rate O(L-1) is independent of the parameter σw.
5.2	Impact of the initialization and smoothness of the activation on the
OVERALL PERFORMANCE
We train FFNN of width 300 and depths L ∈ {200, 300} and width ∈ {200, 300} with SGD and
categorical cross-entropy loss. Training with full batch GD is practically impossible for DNNs, so we
use SGD instead (see Section D in the Appendix for more details about how the results extend to
SGD) with a batchsize of 64 and a learning rate 10-3 for L = 100 and 10-4 for L ∈ 200, 300 (this
learning rate was found by a grid search of exponential step size 10). For each activation function,
we use an initialization on the EOC when it exists, we add the symbol (EOC) after the activation
when this is satisfied. We use (σb,σw) = (0, √2) for ReLU, (θb,Ow) = (0.2,1.227) for ELU
and (σb, σw) = (0.2, 1.302) for Tanh. These values are all on the EOC (see Hayou et al. (2019)
for more details). Table 1 displays the test accuracy for different activation functions on MNIST
and CIFAR10 after 10 and 100 training epochs for depth 300 and width 300. Functions in class S
(ELU and Tanh) perform much better than ReLU-like activation functions (ReLU, Leaky-Relu-α
with α ∈ {0.01, 0.02, 0.03}). Even with Parametric ReLU (PReLU) where the parameter of the
leaky-ReLU is also learned by backpropagation, we obtain only a small improvement over ReLU. For
7
Under review as a conference paper at ICLR 2020
(a) (width,depth) = (200,100)
(b) (width,depth) = (200,200)
Figure 2: Test accuracy for different Activation Functions and (width, depth) on MNIST
Table 1: Test accuracy for a FFNN with width 300 and depth 300 for different activation functions on
MNIST and CIFAR10. We show test accuracy after 10 epochs and 100 epochs
Activation	MNIST		CIFAR10	
	Epoch 10	Epoch 100	Epoch 10	Epoch 100
ReLU (EOC)	46.53 ± 12.01	82.11 ± 4.51	20.38 ± 1.85	35.88 ± 0.6
LReLU0.01 (EOC)	48.10 ± 3.31	84.71 ± 3.39	22.62 ± 1.15	29.44 ± 4.14
LReLU0.02 (EOC)	49.09 ± 3.58	84.3. ± 3.98	18.62 ± 4.56	30.78 ± 6.33
LReLU0.03 (EOC)	50.94 ± 4.48	85.49 ± 2.71	21.19 ± 6.53	34.54 ± 2.32
PReLU	51.94 ± 5.51	87.49 ± 1.58	22.95 ± 3.57	36.13 ± 3.83
ELU (EOC)	91.63 ± 2.21	96.07 ± 0.13	33.81 ± 1.55	46.14 ± 1.49
Tanh (EOC)	91.16 ± 1.21	95.75 ± 0.27	32.37 ± 1.88	42.40 ± 1.13
Softplus	10.11 ± 0.09	10.13 ± 0.18	11.13 ± 0.15	11.09 ± 0.36
Sigmoid	9.85 ± 0.11	9.87 ± 0.10	10.65 ± 0.25	10.33 ± 0.17
activation functions that do not have an EOC, such as Softplus and Sigmoid, we use He initialization
for MNIST and Glorot initialization for CIFAR10 (see He et al. (2015) and Glorot & Bengio (2010)).
For Softplus and Sigmoid, the training algorithm is stuck at a low test accuracy 〜10% which is the
test accuracy of a uniform random classifier with 10 classes.
6	Conclusion and limitations
That the training dynamics of deep neural networks is equivalent to a Functional Gradient Descent
with respect to the Neural Tangent Kernel. In the infinite width limit, the NTK has a closed-form
expression. This approximation sheds light on how the NTK impacts the training dynamics: it
controls the training rate and the generalization function. Using this approximation for wide neural
networks (Mean-field approximation), we show that for an initialization in the Ordered/Chaotic
phase, NTK converges exponentially fast to a non-invertible kernel as the number of layers goes
to infinity, making training impossible. An initialization on the EOC leads to an invertible ANTK
(and NTK) even for an infinite number of layers: the convergence rate is O(L-1) for ReLU-like
activation functions and O(log(L)L-1) for a class of smooth activation functions.
However, recent findings showed that the infinite width approximation of the NTK does not fully
capture the dynamics of the training of DNNs. A recent line of work showed that the NTK for
finite width neural networks changes with time and might even be random (Chizat & Bach (2018),
Ghorbani et al. (2019), Huang & Yau (2019), Arora et al. (2019)). Therefore, we believe that the NTK
is a useful tool to partially understand wide deep neural networks (have insights on hyper-parameters
choices for example) and not a tool to train neural networks.
8
Under review as a conference paper at ICLR 2020
References
S. Arora, S.S. Du, W. Hu, Z. Li, R. Salakhutdinov, and R. Wang. On exact computation with an
infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
A. Bietti and J. Mairal. On the inductive bias of neural tangent kernels. arXiv Preprint
arXiv:1905.12173, 2019.
L. Chizat and F. Bach. A note on lazy training in supervised differentiable programming. arXiv
preprint arXiv:1812.07956, 2018.
D.A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponen-
tial linear units (elus). International Conference on Learning Representations, 2016.
S.S. Du, J.D. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. arXiv preprint arXiv:1811.03804, 2018a.
S.S. Du, J.D. Lee, Y. Tian, B. Poczos, and A Singh. Gradient descent learns one-hidden-layer CNN:
Don’t be afraid of spurious local minima. ICML, 2018b.
S.S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. ICLR, 2019.
B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Linearized two-layers neural networks in
high dimension. arXiv preprint arXiv:1904.12191, 2019.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
International Conference on Artificial Intelligence and Statistics, 2010.
S. Hayou, A. Doucet, and J. Rousseau. On the impact of the activation function on deep neural
networks training. ICML, 2019.
K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification. ICCV, 2015.
W. Hu, C. Junchi Li, L. Li, and J Liu. On the diffusion approximation of nonconvex stochastic
gradient descent. arXiv preprint arXiv:1705.07562, 2018.
J. Huang and H.T Yau. Dynamics of deep neural networks and neural tangent hierarchy. arXiv
preprint arXiv:1909.08156, 2019.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. 32nd Conference on Neural Information Processing Systems, 2018.
R. Karakida, S. Akaho, and S. Amari. Universal statistics of Fisher information in deep neural
networks: Mean field approach. arXiv preprint arXiv:1806.01316, 2018.
M. Kubo, R. Banno, H. Manabe, and M. Minoji. Implicit regularization in over-parameterized neural
networks. arXiv preprint arXiv:1903.01997, 2019.
J. Lee, Y. Bahri, R. Novak, S.S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural
networks as Gaussian processes. 6th International Conference on Learning Representations, 2018.
J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, J. Sohl-Dickstein, and J. Pennington. Wide neural networks
of any depth evolve as linear models under gradient descent. arXiv preprint arXiv:1902.06720,
2019.
D. Lei, Z. Sun, Y. Xiao, and W.Y. Wang. Implicit regularization of stochastic gradient descent in
natural language processing: Observations and implications. arXiv preprint arXiv:1811.00659,
2018.
Q. Li, C. Tai, and W E. Stochastic modified equations and adaptive stochastic gradient algorithms.
arXiv preprint arXiv:1511.06251, 2017.
Q. Nguyen and M. Hein. Optimization landscape and expressivity of deep CNNs. ICML, 2018.
9
Under review as a conference paper at ICLR 2020
S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th
International Conference on Learning Representations, 2017.
G. Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
G. Yang and S. Schoenholz. Mean field residual networks: On the edge of chaos. Advances in Neural
Information Processing Systems, 30:2869-2869, 2017.
C.	Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. arXiv preprint arXiv:1611.03530, 2017.
D.	Zou, Y. Cao, D. Zhou, and Q. Gu. Stochastic gradient descent optimizes over-parameterized deep
ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
10
Under review as a conference paper at ICLR 2020
We provide in Section A and Section B the proof of the theoretical results presented in the main
document. Section C provides additional theoretical results while Section ?? presents additional
experimental results.
A	Appendix: Proofs of Section 3: Neural Networks and Neural
Tangent Kernel
Lemma 1 (Trainability of the Neural Network and Invertibility of the NTK). Assume f0(X) 6= Y.
Then with dynamics (8), ||ft(X) — Y|| converges to 0 as t → ∞ if and only if K L is non-singular
Moreover, if K L is singular, there exists a constant C > 0 such that for all t > 0,
||ft(X)-Y|| ≥C
Proof. Assume fo(X) = Y. Let KL = QTDQ be the spectral decomposition of the empirical NTK;
i.e. Q is an orthogonal matrix and D is a diagonal matrix.
We have that e-令KKLt = QTe-令DtQ = QTDiag(e-dit)ι≤i≤oNQ where (di)1≤i≤0N are the
eigenvalues. We also have ||ft(X) - Y|| = ||e-NKLt(fo(X) - Y)||. Therefore, the equivalence
holds true.
Moreover, assume KL in singular. Let Zt = Q(ft(X) - Y)QT. We have that Zt = e-NDtZo.
Since D has at least one zero diagonal value, then there exists j ∈ {1, 2, ..., oN} such that for all t,
(Zt)j = (Z0)j, and we have
||ft(X)-Y||= ||Zt||
≥ |(Zt)j| = |(Z0)j|
□
Lemma 2 (Discretization Error for Full-Batch Gradient Descent). Assume Vθ L is C-lipsChitz, then
there exists C0 > 0 that depends only on C and T such that
sup	∣∣θtk - Gk || ≤ ηC0
k∈[0,T /η]
ɪʌ /` i - ，一 l^c ml	1 r∙ .1	.	♦	.	. 7i 久	τ .	_ Γr∖ ml	1
Proof. For t ∈ [0, T ], we define the stepwise constant system θt = θ∖t∕η∖. Let t ∈ [0, T ], we have
bt/nc-1
θt = θo - η E VθL(θk)
k=0
θ0
—
rt	〜	∕*t∕n
/ VθL(θs)ds + η /
0 0	J b*ct∕n-1
一 一 J 、一
Vθ L(θb*cs)ds
Therefore,
llθt - Gt||
≤ 八∣VθL(Θs)
0
__	. , -T . . . _
-VθL(θs)∣∣ds + η
t∕η
Jb*ct∕n-i
∣∣VθL(θb*cs)∣∣ds
≤C
Z t∣∣θs- θs∣∣ds
0
,	,	........... 一 J	_______ J	.
+ n(t/n- b*Ct/n)||veL(Gb*ct/n)|| + 楣口仁⑼*"/〃-。11
Moreover, for any k ∈ [0, b*cT /η], we have
..ʌ ∙ ∙ . . . ʌ ・ •
Ilθk - θo∣∣ ≤ (I + ηC)∣∣θk-ι - θo∣∣
≤ (1 + nC)T/nl∣θι - θoll
≤ eCTI∣θι-Θo∣∣
11
Under review as a conference paper at ICLR 2020
1	1	Ii t λ	, 尸« τ τ ∙	. 1 ∙	1 . .ι	∙.	.	入 1	1 ∙	m
where we have used log(1 + ηC) ≤ ηC. Using this result, there exists a constant C depending on T
and C such that
，	.	........... . , O	. . .	____.... , O	. . .	,	._.... , O ...	_ , ʌ	_	, .	_ , ʌ	_	...
η(t∕η - [*Ct∕η)∣∣VθL(θ[*ct∕η)|| + η∣∣VθL(θb*ct∕η-ι)∣∣ ≤ η(2∣∣VθL(θo)∣∣ + C∣∣θ[*ct∕η - θo∣∣ + C∣∣θ[*ct∕η-1 - θo∣∣)
≤ ηC
Now we have
llθt 一切|
≤CZt
0
∣∣θs - θs∣∣ds + ηC,
so we can conclude using Gronwall’s lemma.
□
B Proofs of Section 4: Impact of the Initialization and the
Activation function on the Neural Tangent Kernel
We first recall the results obtained in Lee et al. (2018), Schoenholz et al. (2017) and Hayou et al.
(2019) where the impact of the EOC (Edge of Chaos) on the initialization is studied. We also present
some results that we will be used below.
Consider a FFNN of depth L, widths (nl)1≤l≤L, weights wl and bias bl. For some input x ∈ Rd, the
forward propagation is given by
d	nl-1
yi1(x) = X wi1j xj +bi1,	yil(x) = X wiljφ(yjl-1(x)) +bli,	forl ≥ 2,	(12)
j=1	j=1
where φ is the activation function.
We initialize the model with Wj iid N(0, n^) and b iʌd N(0, σ2), where N(μ, σ2) denotes the
normal distribution of mean μ and variance σ2. For some x, we denote by ql (x) the variance of yl(χ).
In general, ql(x) converges to a point q(σb, σw) > 0 independent ofx as l → ∞. The EOC is defined
by the set of parameters (σb,σw) such that σWE[φ0(∖∕q(σb, σw)Z)2] = 1 where Z 〜 N(0,1).
Similarly the Ordered, resp. Chaotic, phase is defined by σWE[φ0(/q(σb, σw)Z)2] < 1, resp.
σWE[φ0('q(σb, σw)Z)2] > 1 (see Hayou et al. (2019) for more details). For two inputs x, x0 ∈ Rd,
define Σl (x, x0) = E[yl (x)yl(x0)] and let cl (x, x0) be the corresponding correlation. Let f be the
correlation function defined implicitly by cl+1 = f(cl). In the limit of infinitely wide networks, we
have the following results (Hayou et al. (2019)) :
•	ς1(x,x0) = σ2 + σW Ez 〜N (0,∑1-I)[φ(Z(X))φ(z(x0))].
•	There exist q,λ> 0 such that, for all supχ∈Rd ∣Σl(x, x) 一 q| ≤ e-λl.
•	On the Ordered phase, there exists γ > 0 such that supx,x0∈Rd |cl(x, x0) - 1| ≤ e-γl.
•	On the chaotic phase, there exist γ > 0 and c < 1 such that supx6=x0∈Rd |cl(x, x0) - c| ≤
e-γl.
2
•	For ReLU network on the EOC, we have that Σl(x, x) = -w-∣∣x∣∣2 for all l ≥ 1. Moreover,
we have
f(x) _= X +23∏2(1 — x)3/2 + O((1 — x)5/2)
•	In general, we have
σ σ σ2 + σW E[φ(√qZ1)φ(√qz (X))]
f (x)=------------------------
q
where Z(x) = xZi + √1 - x2Z2 and Z1,Z2 are iid standard Gaussian variables.
•	On the EOC, we have f0(1) = 1
•	If φ is k-times differentiable, then f is k-times differentiable and for all 1 ≤ j ≤ k, we have
f(j)(x) = σw2 qj-1E[φ(j)(Z1)φ(j)(Z(x))]
12
Under review as a conference paper at ICLR 2020
•	From Jacot et al. (2018), we have that
Kl(x, x0) = Kl-1(x, x0)Σl(x, x0) + Σl(x, x0).
where the definition of Σl (x, x0) is given in Proposition 1 below.
Definition 1. Let φ : R → R be a measurable function. Then
1.	φ is said to be ReLU-like if there exist λ, β ∈ R such that φ(x) = λx for x > 0 and
φ(x) = βx for x ≤ 0.
2.	φ is said to be in S if φ(0) = 0, φ is twice differentiable, and there exist n ≥ 1, a
partition (Ai)1≤i≤n of R and infinitely differentiable functions g1, g2, ..., gn such that
φ(2) = Pin=1 1Aigi, where φ(2) is the second derivative of φ.
The following two lemmas will be useful to prove the results of Section 3 in the main paper.
Lemma 2. Let (al) be a sequence of non-negative real numbers such that ∀l ≥ 0, al+1 ≤ αal+ke-βl,
where α ∈ (0, 1) and k, β > 0. Then there exists γ > 0 such that ∀l ≥ 0, l ≤ e-γl.
Proof. Using the inequality on al , we can easily see that
l-1
al ≤ a0αl + k X αj e-β(l-j)
j=0
≤ aoαl + k∣e-βl/2 + k∣al/2
where we divided the sum into two parts separated by index l/2 and upper-bounded each part. The
existence of Y is straightforward.
Proposition 1 (Limiting Neural Tangent Kernel with Ordered/Chaotic Initialization). Let (σb, σw)
be in the ordered or chaotic phase. Then, there exist λ, γ > 0 such that
sup |KL(x,x0) — λ∣ ≤ e-γL →l→∞ 0
x,x0 ∈Rd
Proof. From Jacot et al. (2018), we have that
Kl(x, x0) = Kl-1(x, x0)Σl(x, x0) + Σl(x, x0)
where Σ1(x,x0) = σ2 + σwxTx0 and Σl(x,x0) = σ2 + σWEf〜N(0,∑1-i)[φ(f(x))Φ(f(x0))] and
Σl(χ,χ0) = Ef〜N(o,∑ι-i)[φ0(f (x))φ0(f (x0))]. In the ordered/chaotic phase, Hayou et al. (2019)
showed that there exist k, γ,l0 > 0 and α ∈ (0, 1) such that for alll ≥ l0 we have
sup ∣Σl(x, x0) — k| ≤ e-γl
x,x0 ∈Rd
and
sup Σl(x,X) ≤ α.
x,x0∈Rd
Therefore we have for any l ≥ l0 and x, x0 ∈ Rd
Kl (x, x0) ≤ αK l-1 (x, x0) + k + e-γl.
Letting r = Kl(χ, χ0) — ɪ-ɑ, We have
rl ≤ αrl-1.
We can now conclude using Lemma 2.	□
Now, we show that the Initialization on the EOC leads to an invertible NTK even if the number of
layers L goes to infinity. We first prove two preliminary lemmas that will be useful for the proof of
the next proposition.
13
Under review as a conference paper at ICLR 2020
Lemma 3. Let (al), (bl), (λl) be three sequences of real numbers such that
	al = al-1λl + bl λl = 1 - α + O(LT) bl = q + O(l-1)
where α ∈ N*,β,q ∈ R+ and α > β 一 L
Then,	al =	q	+ O(l-min(1,β)) l 1+α
Proof. It is easy to see that there exists a constant G > 0 |al| ≤ G × l + |a0| for all l ≥ 0, therefore
(ai/l) is bounded. Now let Z = min(1, β) and r = a∣l. We have
	rl = rl-ι(I 一 J)(I 一 亍 + O(IT-β)) + : + O(I-2) = rl-ι(1- 1+α) + q + O(l-1-ζ).
Letting xι = Irl 一 l+α ∖, there is exist a constant M > 0 such that
	1+α	M Xl ≤ Xl-I(I	1-) + ∣ι+ζ.
Hence, we have
	1+α	1+α 1 Xl ≤χ0∏(1 —k-) + ME ∏ (1 ——)ki+ζ. k=1	k=1 j=k+1
By taking the logarithm of the first term in the right hand side and using the fact that Pk=11 ~ log(l),
we have	YY (1 - 中) 〜ι-1-α k=1
For the second part, observe that
	YY ∩	1 + α	(1 一 α 一 1)!	k! U (	j~~) =	l!	(k — α — 1)! j=k+1
and	(k 一 α — 1)! k1+ζ ~k→∞ k0 Z
so that,	XX	k	_〜XX k≈-ζ 乙Z (k 一 α 一 1)! k1+ζ 乙Z k=1	k=1 〜Z tα-ζdt 	ια-ζ+1 α — Z + 1
therefore,	X^ YlJ	1 + α 1	(1 - a - 1)! X^	k!	1 工 U (	T~)k1+ζ =	l!	A (k - α - 1)! k1+ζ k=1 j=k+1	k=1 	1——l-ζ α - Z + 1
We can now conclude using the fact that α > β 一 1.
□
14
Under review as a conference paper at ICLR 2020
We now introduce a different form of the previous Lemma that will be useful for other applications.
Lemma 4. Let (al), (bl), (λl) be three sequences of real numbers such that
al = al-1λl + bl
λι = 1 - α + κlog(l)+ O(l-ι-β)
l	l2
bl = q + O(l-1)
where α ∈ N*,β,q ∈ R+ and α>β — 1,β ≥ 1.
Then, there exists A, B > 0 such that
AIog(I) ≤ । a - q । ≤ B Iog(I)
l	~ 1 l	1 + α∣ - l
Proof. It is easy to see that there exists a constant G > 0 |al | ≤ G × l + |a0| for all l ≥ 0, therefore
(ai/l) is bounded. Let r = a∣l. We have
ri = ri-i(1 — 力(1 —亍 + κ * + O(LT)) + q + O(1-2)
=ri-i(1 — 中)+ ri-iκ 胃 + q + 0(1-2)
Let xi = ri — ι+α. It is clear that λ = 1 — α∕l + O(l-3/2). Therefore, using Lemma 3 with
β = 1∕2,we have r → ι+α. Thus, there exists κ1,κ2,M,l0 > 0 such that for all 1 ≥
l0
xi-1(1 —
1+α
)+ Ki Iog(I) — M ≤ Xl ≤ xi-1 (1 —
1+α
)+K2 * +
M
~P
l
l
Similarly to the proof of Lemma 3, it follows that
and
xi ≤ xi0
i
Y(1—
k=i0
1+α
ii
)+X Y(1
k=i0 j=k+1
xi ≥
x0
i
Y(1 —
k=0
1+α
ii
)+X Y(1
k=i0 j =k+1
Recall that we have
i
Y(1 —
k=1
1+α
and
i
Y (1
j=k+1
so that
k!
1 + a) K2 log(k) + M
k2
κ1 log(k) — M
k2
)〜l-1-α
(l — a — 1)! k!
l!
(k — α — 1)!
κ1 log(k) — M
(k — a — 1)! k2
〜k-∞
log(k)kα-1
k
k
—
1 + a)
j
—
—
k
j
1 + a)
j
Therefore, we obtain
i
X
k=1
k!	κ1 log(k) — M
(k — a — 1)! k2
i
X log(k)kα-1
k=1
Z log(t)tα-1dt
C1lα log(α)
〜
〜
〜
15
Under review as a conference paper at ICLR 2020
where C1 > 0 is a constant. Similarly, there exists a constant C2 > 0 such that
G k! K2log(k)+ M
g(k - α - 1)! -k----------------C2l log(α)
We conclude using the fact that (1-0~1)!〜l-1-α.
□
Proposition 2 (Neural Tangent Kernel on the Edge of Chaos). Let φ be a non-linear activation
function and (σb, σw ) ∈ EOC.
1.	If φ is ReLU-like, then forall X ∈ Rd, K R口X 二 σw dx||——+ K LlX). Moreover, there exist
A, λ ∈ (0, 1) such that
KL(x, x0)	σw2	0 A
SUP I—— -----------λ-TIIxIIIIx III ≤ 7
X6=X0∈Rd	L	d	L
2.	If φ is in S, then, there exist q > 0 such that K (LxXX = q + K LbX) → q. Moreover, there
exist B, C, λ ∈ (0, 1) such that
B log(L)
-L-
≤ suP
X6=X0 ∈Rd
K L(X, x0)	i∣ / C log(L)
—L	qN≤ -πr-
Proof. We use some results from Hayou et al. (2019) in this proof.
Let x, x0 ∈ Rdand cχ,χ0 = √≡!⅛⅛. Letγl := 1 - clX,X0 and f be the correlation
function defined by the recursive equation cl+1 = f(xl). From the preliminary results,
2
We know that Σl(x,x) = -W IIxII2 and that Kl(x, x0) = Kl-1(x, x0)Σl(x, x0) + Σl(x, x0).
This concludes the proof for KL(x, x). We denote S = 23∏2. From Hayou et al. (2019), we
have on the EOC γι+ι = γι - sγ3/2 + O(γ5/2) so that
Yι+11/2 =-1/ + O(Yι3∕2))T∕2 = γ-1∕2(1 + 同/ + O(γ3/2))
二γ-1/2 + 2 + O(γι).
Thus, as l goes to infinity
Yi+11/2 -Y-"
s
〜—
2
and by summing and equivalence of positive divergent series
Y-1/2 〜Sl
Moreover, since YI-I/2 - Y-1/2 = 2 + O(γι)= S + O(l-2), we have γ-1/2 = 2l + O(1).
Therefore, CXxO = 1 - 92∏2 + O(l-3).
we also have
f 0(x) = ɪ arcsin(x) + ɪ
=1 — —(1 - x)1∕2 + O((1 - x)5∕2).
π
Thus, it follows that
3
f0(cX,χ0) = 1 - l + O(l-2)
Moreover, qXι ,X0 = q + O(l-2) where q is the limiting variance of yι.
Using Lemma 3, we conclude that K (X，X ) = 4σdwIIχIIIIχ0II + O(l-1). Since cX,X0 is
bounded, this result is uniform in x, x0. Therefore, we can take the supremum over x, x0 ∈
Rd.
16
Under review as a conference paper at ICLR 2020
12.	We prove the result when φ(2) (x) = 1x<0g1 (x) + 1x≥0g2(x). The generalization to the
whole class is straightforward. Let f be the correlation function. We first show that for all
k≥3f(k)(x)
We have
(1-χ2)(k-2)∕2 gk (X) where gk ∈ C∞.
f 00(X) = σW qE[φ00 (TqZI)φ[√qU2(X))]
=σW qE[φ'0 (√qzi)1U2(x)<0g1(√qU2(x))] + σW qE[φ00(√qzi)1U2(x)>0g2(√qU2(x))].
Let G(X) = E[φ00(√qZ1)lu2(χ)<0g1(√qU2(x))] then
X1
G (X) = E[φ	(√qZ1)(Z1	— /]	2 Z2)δU2(x)=0	h 2 gl(√qU2(x))]
1	- X2	1	- X2
X
+ E[φ (VqZI)lU2(x)<0√q(ZI--/ I) Z 2) g 1 ( √qU2 (X))].
1 - X2
It is easy to see that G0(x) = √r-χ2Gi(x) where Gi ∈ C1. A similar analysis can
be applied to the second term of f00. We conclude that there exists g3 ∈ C∞ such that
f (3)(x) = √1l-χ2-g(X). We obtain the result by induction.
Since f(k) are potentially not defined at 1, we use the change of variable x = 1 - t2 to
obtain a Taylor expansion near 1. Simple algebra shows that the function t → f (1 - t2) has
a Taylor expansion near 0:
46
f (1 - t2) = 1 - t2f0(1) + ~2 f00⑴ + 石f ⑶(1) + O(t8).
Therefore,
f(X) = 1 + (X - 1)f0(1) + (ɪ-1^f00(1) + (1-Xɪf⑶⑴ + O((X-1)4).
Letting λl := 1 - cl, there exist α, β > 0 such that
λl+1 = λl - αλl2 - βλl3 + O(λl4)
therefore,
λl-+11 = λl-1(1 - αλl - βλ2 + O(λl3))-1
= λl-1 (1 + αλl + βλl2 + O(λl3))
= λl-1 + α + βλl + O(λl2 ).
By summing (divergent series), we have that λ-i 〜βl-. Therefore,
λl-+11 - λl-1 - α = βα-1l-1 + O(l-2)
By summing a second time, we obtain
λl-1 = αl + βα-1 log(l) + O(1)
so that λι = α-1l-i - α-1βIog^ + O(l-2).
Using the fact that f0(X) = 1 + (X - 1)f00(1) + O((X - 1)2), we have f 0 (clx,x0 ) =
1 一 2∣ + KIogQ + O(l-2). We can now conclude using Lemma 4. Using again the agrument
of the boundedness of c1x,x0, we can take the supremum.
□
17
Under review as a conference paper at ICLR 2020
Proposition 3. Consider the following network architecture (FFNN with residual connections)
nl-1
yil(x) = yil-1(x) + X wiljφ(yjl-1(x)) + bli,	forl ≥ 2.	(13)
j=1
with initialization parameters σb = 0 and σw > 0. Let KrLes be the corresponding NTK. For all
X ∈ Rd, KL×2Lx = σW llχl- ■+ O(LT)and there exists λ ∈ (0,1) such that
sup
x6=x0 ∈Rd
KLes (x, x0)
L × 2L
-σw≡×Kλ∣=O(LT)
Proof. We only give a sketch of the proof. A more rigorous proof can be easily done but it unecessary.
As in the feedforward without residual connections case, it is easy to see that KrLes satisfies the
following recursive equation
KIres(X, X0) = Kres(X，X0)(Σl(x, X0) ÷1)÷ ∑l(X, X0)
To see this, with FeedForward Neural Network, we have ( Jacot et al. (2018))
∂ l+1	∂ l+1	nl
*7 (x)( M S = X
:	:	j,j 0
W+w+0φ (Ij ∂yir(X)(∂yil (X))t ÷ IW
whereas for Residual Networks, we have an extra term
dylr+1(x)M+1(x'})t_ Mi	∂ylr	t X r+1 r+1 0( I) ∂ylr	∂y∖ ( W ,
k(X)(k(X)) =衍(X)(Wr(X)) ÷2-Wij WijO φ (yj)Wr(X)(即(X)) ÷ 1W
:	:	:	:	j,j0	:	:
where IW , IW0 are quantities that converge to 0 as nl grows (self-averaging). The additional term
results in an added term Kl(X, X0) in the formula of Kl+1 (X, X0).
We already now that Σl (x, x) = 1. Moreover, We have Σ1(x, x) = Σ1-1(x, x) ÷ σW/2Σ1-1(x, x)=
2 l 1 σ2	l
(1 ÷ σW/2)1 1 -dw ∣∣d∣∣. Depending on the value of σw, the behaviour of Kl(x, x0) changes. However,
from Hayou et al. (2019), we have that Σl(χ, x0) = 1 - βl-1 ÷ Ο(l-2). So by scaling with &l2L
and using Lemma 3, we conclude on the convergence rate of O(l-1). We can take the supremum as
the result of the boundedness of c1(X, X0).
□
C Impact of the Initialization the output function
In this section, we show how an initialization on the EOC impacts on the output function of the
neural network. More precisely, we show that it leads to a larger range compared to an initialization
in the Ordered/chaotic phase.
Let us start by a simple Lemma that compares the expectations of some smooth mapping with respect
to two different Gaussian vectors.
Lemma 5. Let X = (Xi)1≤i≤n, Y = (Yi)1≤i≤N be two centered Gaussian vectors in Rn. Let
g ∈ D2(Rn, R). Then we have
E[g(X)] -E[g(Y)] = 1 [ 1	X (EXiXj]-E[YiY∙])E[77⅞-(√Γ-^X÷√uY)]du (14)
0 1≤i,j,≤n	i j	∂Xi∂Xj
The result of Lemma 5 is valid when the second derivatives of g exist only in the distribution sense
(e.g. Dirac mass).
18
Under review as a conference paper at ICLR 2020
Proof. We define the function G on R by
G(t) = E[g(tX + pl - t2Y)]
we have that
G0(t) = XX E[(Xi - -1t== YO 会(tX + p1-t2Y)]
i=1	-	i
Moreover, it is easy to see that for any random vector Z in Rn we have E[Zig(Z)]
pn=ι cov(Xi, Xj)E[∂Xj(Z)], this yields
n	K	_____
Gg = t X (EXiXj] - E[YY∙])E[d-ɪ (tX + p1 -12Y)]
i,j=1	∂xi∂xj
We conclude by integrating G0(t) between 0 and 1.	口
Now let D = {(xi, zi) : 1 ≤ i ≤ N} be the datapoints. Using the same notations as in the previous
chapter, let yL(xi) denotes on the neurons of lth layer (the neurons are iid). Assume c1x ,x ≥ 0 for
all i, j ∈ [1, N] (this is almost always the case, but in general, we can re-scale the input data to satisfy
this assumption). We have the following result
Lemma 6. Let φ be a non ReLU-like activation function and (σb, σw ) ∈/ EOC. Then there
exists (σb,EOC, σw,EOC) ∈ EOC such that for any function g ∈ D2 such that for all i, j ∈
[|1, N|], a∕∂χ ∙ ≥ 0, there exist β > 0, Zn > 0 Such that
E[g(y(L(X))] ≤ E[g(yLrd(X))] - Le
Proof. Let (σb, σw) ∈/ EOC and q be the corresponding limiting variance. From the previous chapter,
it is easy to see that there exists σ0 > 0 such that σ0 + E[φ(√Z))j = q. Let (σb,Eoc, 0加再。。)=
(σo, 1/'E[φ0(√qZ)2]) ∈ EOC. There exists a constant κ > 0 (independent of N) such that for
alli,j ∈ [|1,N|],E[y(Loc(Xi)y(Loc(Xj )] ≤ E[yoLrd(Xi)yoLrd(Xj)] - κL-β where β = 1 for a smooth
activation functions in S and β = 2 for ReLU-like activation functions (see Hayou et al. (2019)). Let
λN = infi,j infu∈[0,i] E[∂⅛-(√1 - uX + √uY)]. Using Lemma 5, We have
E[yLoc(XiMoc(Xj)] ≤ E[y幺c(Xi)y%d(Xj)] - ∣KN2λNLT
□
As a simple application, using the function g(X) = QiN=1 1xi ≤ti, we have the following
Lemma 7. Let t1, t2, ..., tN ∈ R. Then there exist β > 0, ζN > 0 such that
ζ
p(yLoc(XI) ≤ tι,…,yLoc(χN) ≤ tN) ≤ p(yLTd(XI) ≤ tι,…,yθrd(XN) ≤ tN) - Le
as a Corollary, we have a generalized form of Slepian’s Lemma for y(Loc(X) and yoLrd(X).
Corollary 1 (Max Range and Min Range).
P(maxyLoc(xi) ≥t) ≥ P(maxyLrd(xi) ≥t) + Le
and
P(m.inyLoc(χi) ≤t) ≥ P(maxyLrd(Xi) ≤t) + Le
where β = 1 for activation functions of type S and β = 2 for ReLU-like activation functions.
19
Under review as a conference paper at ICLR 2020
D Training with SGD instead of GD
In this section, we extend the results of the NTK to the case of SGD. We use an approximation of the
SGD dynamics by a diffusion process. We assume implicitly the existence of the triplet (Ω, P, F)
where Ω is the probability space, P is a probability measure on Ω, and F is the natural filtration of
the Brownian motion. Under boundedness conditions, when using SGD, the gradient update can be
seen as a GD with a Gaussian noise (Hu et al., 2018; Li et al., 2017). More precisely, let S = o(N)
be the batchsize. The SGD update is given by
θt+ι = Gt- ηVθL(S) (θt),
(15)
where L(S) = S1 PS=I '(f (Xi, θ), yi) where (xi, yi)1≤i≤S is a randomly selected batch of size S.
Then for all θ
VθL(S)(θ) - VθL(θ) = X ZSS)(Vθ'(fθ(xi),yi) - Eo(θ)) - X BV'(fθ(Xi)Nyi)- E。⑻)
i	i=1
where Zi(S) = 1 if observation i belongs to the batch (Xj, yj),j ≤ S and equals 0 otherwise and
Eo(θ) = EoVv'(f(Xι,θ),Y1). We have
tr
Cov
(Vv'(fv(Xi),yi)- Eo(θ))
N
p
X
l=1
Var (∂'(fv (Xι),Y1)∕∂θι)
N
So that if S = o(N) and if
tr (Cov (VV '(f(X1,θ),Y1)))= o(S)
where Cov(∙) denotes the covariance matrix under Po. Then
VVL(S)(θ)-VvL(θ) = Z√≡+ oPo (S τ∕2)
S
where ZS (θ) converges in distribution (as S goes to infinity) to a Gaussian random vector with
covariance matrix Σ(θ) = Cov (VV'(f (Xι,θ), Y1)) and We have, neglecting the term op° (S-1/2),
η
θ^t+1 = θt - nVvL(θt) + √=^Z(θt).
(16)
We can in particular bound the difference between equation 16 and the continuous time SDE
approximation (see also Hu et al. (2018) and Li et al. (2017))
dθt
-VV L(θt)dt +
(17)
SGD updates can therefore be seen as a discretization of the previous SDE with time step ∆ = η,
and where Σ(θt) 2 is the square-root matrix of Σ(θt) = Cov (VV'(f(Xι, θt), Yι)) and (Wt)t≥o a
standard Brownian motion.
Since the dynamics of θt are described by an SDE, the dynamics of ft can also be described by an
SDE which can be obtained from ltð′s lemma.
Proposition 4. Under the dynamics of the SDE equation 17, the vector ft(X) is the solution of the
following SDE
dft(X) = [-ɪKVt(X，X)Vz'(ft(X),Y) + 1 ηΓt(X)]dt +ʌ/IVvf(X,θt)∑(θt)2dWt (18)
N	2S	S
where Γt(X) is the concatenated vector of (Γt(x) = (Tr(Σ(θt)2 V2fi(x, θt)Σ(θt)1 ))1≤i≤0)χ∈χ
and V2fi(X, θ) is the Hessian of fi (ith component of f) with respect to θ.
20
Under review as a conference paper at ICLR 2020
Proof. Since θt is a diffusion process, we can use Ito,s lemma to deduce how the randomness ProPa-
gates to ft. We denote by ft,i the ith coordinate of ft, i.e., for an input x, ft(x) = (ft,i(x))1≤i≤k.
Let i ∈ 1, ..., k, we have
dft,i(x) = Vθfi(x, θt)dθt + 1 η Tr(Σ(θt)1 V2fi(x,θt)Σ(θt)2)dt
2S
=[-Vθ ft,i(x)Vθ ft(X )Vz '(ft(X ),Y ) + 1 η Tr(∑(θt)1 V2fi(x,θt )∑(θt)1 )]dt
2S
+ SV Vθ fi(x,θt)Σ(θt)2 dWt
where V2fi(x, θt) is the hessian of fi with resPect to θ. Aggregating these equations with resPect to
i yields
dft(x) = [-ɪ Vθ ft(χ)Vθ ft(X )Vz '(ft(X), Y) + 1 η Γt(χ)]dt + ʌ ηV Vθ f (χ,θt)∑(θt)2 dWt
N	2S	S
where Γt(x) = (Tr(∑(θt)2Sfi(x, θt)∑(θt)2))ι≤i≤k.
Therefore, the dynamics of the vector ft(X ) is given by
dft(X) = [-ɪ Kθt(X, X )Vz '(ft(X ),Y) + 1 η Γt(X )]dt + ʌ ∕η Vθ f(X ,θt)∑(θt)2 dWt
N	2S	S
where Γt(X) is the concatenated vector of (Γt (x))x∈X.
□
With the quadratic loss '(z, y) = ɪ ||z 一 y||2, the SDE equation 18 is equivalent to
dft(X) = [一ɪKθt(X, X)(ft(X) -Y) + 1 ηΓt(X)]dt + ʌηVVθf (X,θt)Σ(θt)1 dWt.	(19)
N	2S	S
This is an Ornstein-Uhlenbeck Process (mean-reverting Process) with time dePendent Parameters.
The additional term Γt is due to the randomness of the mini-batch, it can be seen as a regularization
term and could Partly exPlain why SGD gives better generalization errors comPared to GD (Kubo
et al. (2019), Lei et al. (2018)).
Dynamics of ft for wide FeedForward neural networks :
In the case of a fully connected feedforward neural network (FFNN hereafter) of dePth L and widths
n1 , n2 , ..., nL , Jacot et al. (2018) Proved that, with GD, the kernel KθL converges to a kernel K L that
dePends only on L (number of layers) for all t < T when n1, n2, ..., nl → ∞, where T is an uPPer
bound on the training time, under the technical assumption RT ||Vz'(ft(X, Y))∣∣2dt < ∞ almost
surely with resPect to the initialization. For SGD, we assume that the convergence result of the NTK
holds true as well, this is illustrated empirically in figure 3 but we leave the theoretical proof for
future work. With this approximation, the dynamics of ft(X) for wide networks is given by
dft(X) = -ɪKL(ft(X) - Mt)dt + 1 ηVVθf (X,θt)Σ(θt)2dWt,
NS
where KL = Kl(X, X) and Mt = Y -嘎(KL)-Tt(X). This is an Ornstein-Uhlenbeck process
whose closed-form expression is given by
ft(X) = e-NKK fo(X) + (I - e-NKK )Y + At(X)	(20)
where At(X) = -2¾ Rt e-tNsKLΓs(X)ds + PSRt e-tNsKL Vf (X,θs)Σ(θs)2dW3; see sup-
plementary material for the proof. So for any (test) input x ∈ Rd, we have
ft(x) = fo(x) + KL(x, X)(KL)T(I - e-tNsKK )(Y - fo(X)) + Zt(x) + Rt(x),	(21)
where Rt(X) = PRt[KL(x, X)(KL)T(e-NKK - I)Vθf (X,θs) + Vθf (x, θs)]Σ(θs)2dWs
and Zt(x) = 2S [Rt Γs(x)ds + R0 K(x, X)(Kl)-1(I - e-(tNs)KL)Γs(X)ds].
21
Under review as a conference paper at ICLR 2020
(a) t=0
(b) t=100
Figure 3: Ratio KθL /KL for three randomly selected pairs from MNIST dataset as a function of
width for three training times t = 0, t = 100 and t = 1000 (training time is measured by SGD
updates)
(c) t=1000
Proof. Using the approximation of the NTK by KL as n1, n2, ..., nL → ∞, the dynamics of ft(X)
for wide networks are given by
dft(X) = — ɪ 宜 L(ft(X) — Mt)dt + ∖ ∕η Vθ f (X ,θt)∑(θt)2 dWt,
NS
To solve it, We use the change of variable At = eNKL ft(X). Using Ito's lemma, We have
dAt = NKKLAtdt + eN K dft(X)
=W 宜 Le N K L Mtdt +∖ Re N K L Vθ f (X ,θt)∑(θt)2 dWt
NS
By integrating, We conclude that
ft(X ) = e-N KK fo(X ) + 1 Z t K Le -N-S KK Msds + JI Zt e- tNs KK Vθ f (X ,θs)Σ(θs)2 dWs
N0	S0
We conclude for ft(X) using the fact that -N RtKLe (N S)KLMsds = (I 一 e-NNKL)Y 一
2S R0e-(Ns)KLrsds.
Recall that for any input x ∈ Rd ,
dft(X) = [-ɪKL(x, X)(ft(X) - Y) + 1 9Γt(x)]dt + ʌηVVθf(χ,θt)∑(θt)1 dWt
N	2S	S
To prove the expression of ft(x) for general x ∈ Rd, We substitute ft(X) by its value in the SDE of
ft(x) and integrate.
□
E	Additional Experiments : Impact of the parametrization
Figure 4 shoWs the impact of different parametrizations on the training dynamics. It shoWs that both
standard parametrization and NTK parametrization have similar behaviour on different initialization
regimes.
22
Under review as a conference paper at ICLR 2020
ntk-parametrization
standard_parametrization
u'(ŋ.lnɔuv 6u⊂ω二
Ordered phase
ntk-parametrization
standard-parametrization
epoch
(a) EOC
epoch
(b) Ordered phase
Figure 4: Impact of NTK parametrization on the Training
23