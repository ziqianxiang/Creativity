Under review as a conference paper at ICLR 2020
Generative Restricted Kernel Machines
Anonymous authors
Paper under double-blind review
Ab stract
We introduce a novel framework for generative models based on Restricted Kernel
Machines (RKMs) with multi-view generation and uncorrelated feature learning
capabilities, called Gen-RKM. To incorporate multi-view generation, this mech-
anism uses a shared representation of data from various views. The mechanism
is flexible to incorporate both kernel-based, (deep) neural network and Convo-
lutional based models within the same setting. To update the parameters of the
network, we propose a novel training procedure which jointly learns the features
and shared subspace representation. The latent variables are given by the eigen-
decomposition of the kernel matrix, where the mutual orthogonality of eigenvec-
tors represents uncorrelated features. Experiments demonstrate the potential of
the framework through qualitative and quantitative evaluation of generated sam-
ples on various standard datasets.
1 Introduction
In the past decade, interest in generative models has grown tremendously, finding applications in
multiple fields such as, generated art, on-demand video, image denoising (Vincent et al., 2010),
exploration in reinforcement learning (Florensa et al., 2018), collaborative filtering (Salakhutdinov
et al., 2007), inpainting (Yeh et al., 2017) and many more.
Some examples of graphical models based on a probabilistic framework with latent variables are
Variational Auto-Encoders (Kingma & Welling, 2014) and Restricted Boltzmann Machines (RBMs)
(Smolensky, 1986; Salakhutdinov & Hinton, 2009). More recently proposed models are based on
adversarial training such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and
its many variants. Furthermore, auto-regressive models such as Pixel Recurrent Neural Networks
(PixelRNNs) (Van Den Oord et al., 2016) model the conditional distribution of every individual
pixel given previous pixels. All these approaches have their own advantages and disadvantages. For
example, RBMs perform both learning and Bayesian inference in graphical models with latent vari-
ables. However, such probabilistic models must be properly normalized, which requires evaluating
intractable integrals over the space of all possible variable configurations (Salakhutdinov & Hinton,
2009). Currently GANs are considered as the state-of-the-art for generative modeling tasks, pro-
ducing high-quality images but are more difficult to train due to unstable training dynamics, unless
more sophisticated variants are applied.
Many datasets are comprised of different representations of the data, or views. Views can corre-
spond to different modalities such as sounds, images, videos, sequences of previous frames, etc.
Although each view could individually be used for learning tasks, exploiting information from all
views together could improve the learning quality (Pu et al., 2016; Liu & Tuzel, 2016; Chen & De-
noyer, 2017). Also, it is among the goals of the latent variable modelling to model the description
of data in terms of uncorrelated or independent components. Some classical examples are Indepen-
dent Component Analysis; Hidden Markov models (Rabiner & Juang, 1986); Probabilistic Principal
Component Analysis (PCA) (Tipping & Bishop, 1999); Gaussian-Process Latent variable model
(Lawrence, 2005) and factor analysis. Hence, when learning a latent space in generative models,
it becomes interesting to find a disentangled representation. Disentangled variables are generally
considered to contain interpretable information and reflect separate factors of variation in the data
for e.g. lighting conditions, style, colors, etc. The definition of disentanglement in the literature is
not precise, however many believe that a representation with statistically independent variables is a
good starting point (Schmidhuber, 1992; Ridgeway, 2016). Such representations extract informa-
tion into a compact form which makes it possible to generate samples with specific characteristics
1
Under review as a conference paper at ICLR 2020
(Chen et al., 2018; Bouchacourt et al., 2018; Tran et al., 2017; Chen et al., 2016). Additionally, these
representations have been found to generalize better and be more robust against adversarial attacks
(Alemi et al., 2017).
In this work, we propose an alternative generative mechanism based on the framework of Restricted
Kernel Machines (RKMs) (Suykens, 2017), called Generative RKM (Gen-RKM). RKMs yield a
representation of kernel methods with visible and hidden units establishing links between Kernel
PCA, Least-Squares Support Vector Machines (LS-SVM) (Suykens et al., 2002) and RBMs. This
framework has a similar energy form as RBMs, though there is a non-probabilistic training proce-
dure where the eigenvalue decomposition plays the role of normalization. Recently, Houthuys &
Suykens (2018) used this framework to develop tensor-based multi-view classification models and
Schreurs & Suykens (2018) showed how kernel PCA fits into this framework.
Contributions. 1) A novel multi-view generative model based on the RKM framework where mul-
tiple views of the data can be generated simultaneously. 2) Two methods are proposed for computing
the pre-image of the feature vectors: with the feature map explicitly known or unknown. We show
that the mechanism is flexible to incorporate both kernel-based, (deep) convolutional neural network
based models within the same setting. 3) When using explicit feature maps, we propose a training
algorithm that jointly performs the feature-selection and learns the common-subspace representation
in the same procedure. 4) Qualitative and quantitative experiments demonstrate that the model is
capable of generating good quality images of natural objects. Further experiments on multi-view
datasets exhibit the potential of the model. Thanks to the orthogonality of eigenvectors of the kernel
matrix, the learned latent variables are uncorrelated. This resembles a disentangled representation,
which makes it possible to generate data with specific characteristics.
This paper is organized as follows. In Section 2, we discuss the Gen-RKM training and generation
mechanism when multiple data sources are available. In Section 3, we explain how the model
incorporates both kernel methods and neural networks through the use of implicit and explicit feature
maps respectively. When the feature maps are defined by neural networks, the Gen-RKM algorithm
is explained in Section 4. In Section 5, we show experimental results of our model applied on
various public datasets. Section 6 concludes the paper along with directions towards the future
work. Additional supplementary materials are given in the Appendix A.
2 Generative Restricted Kernel Machines framework
The proposed Gen-RKM framework consists of two phases: a training phase and a generation phase
which occurs one after another.
2.1	Training
Similar to Energy-Based Models (EBMs, see LeCun et al. (2004) for details), the RKM objective
function captures dependencies between variables by associating a scalar energy to each configura-
tion of the variables. Learning consists of finding an energy function in which the observed con-
figurations of the variables are given lower energies than unobserved ones. Note that the schematic
representation, as shown in Figure 1 is similar to Discriminative RBMs (Larochelle & Bengio, 2008)
and the objective function Jt (defined below) has an energy form similar to RBMs with additional
regularization terms. The latent space dimension in the RKM setting has a similar interpretation
as the number of hidden units in a restricted Boltzmann machine, where in the specific case of the
RKM these hidden units are uncorrelated.
We assume a dataset D = {xi, yi}iN=1, with xi ∈ Rd, yi ∈ Rp comprising ofN data points. Here yi
may represent an additional view of xi, e.g., an additional image from a different angle, the caption
of an image or a class label. Starting from the RKM interpretation of Kernel PCA, which gives an
upper bound on the equality constrained Least-Squares Kernel PCA objective function (Suykens,
2017), and applying the feature-maps φ1 : Rd 7→ Rdf and φ2 : Rp 7→ Rpf to the input data points,
2
Under review as a conference paper at ICLR 2020
Figure 1: Gen-RKM schematic representation modeling a common subspace H between two data
sources X and Y. The φ1, φ2 are the feature maps (Fx and Fy represent the feature-spaces) corre-
sponding to the two data sources. While ψ1, ψ2 represent the pre-image maps. The interconnection
matrices U , V model dependencies between latent variables and the mapped data sources.
the training objective function Jt for generative RKM is given by1:
N	λη	η
Jt = E <-φι(xiYUhi- φ2(yi)τVhi + 2h>hi) + η Tr(U>U) + η Tr(V>V), (1)
i=1
where U ∈ Rdf ×s and V ∈ Rpf ×s are the unknown interaction matrices, and hi ∈ Rs are the
latent variables modeling a common subspace H between the two input spaces X and Y (see Figure
1). The derivation of this objective function is given in the Appendix A.1.
Given η1 > 0 and η2 > 0 as regularization parameters, the stationary points of Jt are given by:
(∂Jt =0 =⇒ 2hi = U τφι (xi) + V τφ2(yi), ∀i = 1,...,N
d J = 0	=⇒	U =	η11 PlN=I	φι(Xi)h>	⑵
[Jt = 0	=⇒	V =	η⅛ PN=I	φ2(yi)h>-
Substituting U and V in the first equation above, denoting Λ = diag{21, . . . , 2s} ∈ Rs×s with
s ≤ N, yields the following eigenvalue problem:
ɪKi + ɪK2 Hτ = HτΛ,	⑶
η1 η2
where H = h1, . . .,hN ∈ Rs×N withs ≤ N is the number of selected principal components and
K1 , K2 ∈ RN ×N are the kernel matrices corresponding to data sources2 * *. Based on Mercer’s theo-
rem (Mercer, 1909), positive-definite kernel functions k1 : Rd × Rd 7→ R, k2 : Rp × Rp 7→ R can
be defined such that k1(xi,xj) = hφ1(xi), φ1(xj)i, and k2(yi,yj) = hφ2(yi),φ2(yj)i, ∀i,j =
1, . . . , N forms the elements of corresponding kernel matrices. The feature maps φ1 and φ2 , map-
ping the input data to the high-dimensional feature space (possibly infinite) are implicitly defined
by kernel functions. Typical examples of such kernels are given by the Gaussian RBF kernel
k(xi, Xj) = e-kxi-xjk2^2σ)or the LaP山Cekemel k(xi, Xj) = e-kxi-xjk"σ just to name a few
(Scholkopf & Smola, 2001). However, one can also define explicit feature maps, still preserving the
Positive-definiteness of the kernel function by construction (Suykens et al., 2002).
2.2	Generation
In this section, we derive the equations for the generative mechanism. RKMs resembling energy-
based models, the inference consists in clamPing the value of observed variables and finding con-
figurations of the remaining variables that minimizes the energy (LeCun et al., 2004). Given the
1
1For convenience, it is assumed that all the feature vectors are centered in the feature sPace F using φ(x) :=
φ(x) -Nn PNL φ(χi). Otherwise, a centered kernel matrix could be obtained using Eq. 17 (APPendix A.4).
2While in the above section we have assumed that only two data sources (namely X and Y) are available
for learning, the above Procedure could be extended to multiPle data-sources. For the M views or data-sources,
this yields the training problem: [pM=ι η^K'[ H> = H>Λ.
3
Under review as a conference paper at ICLR 2020
learned interconnection matrices U and V , and a given latent variable h? , consider the following
objective function:
Jg = -φι(^ητUh? - φ2(y*)>Vh* + 2φι(x*)>φι(x?) + 202(y?)>02(y?),	(4)
with an additional regularization term on data sources. Here Jg denotes the objective function for
generation. The given latent variable h? can be the corresponding latent code of a training point, a
newly sampled hidden unit or a specifically determined one. Above cases correspond to generating
the reconstructed visible unit, generating a random new visible unit or exploring the latent space by
carefully selecting hidden units respectively. The stationary points of Jg are characterized by:
0 =⇒ φ1(x?) = Uh?,	5
0 =⇒ φ2 (y?) = Vh?.	()
Using U and V from Eq. 2, we obtain the generated feature vectors:
∂ J
J)
I ∂φ2 (y*)
Φι(χ?) = (η1ιXXXΦι(χi)h>) h?,	Φ2(y*) = (η12X。23加>)h?.	⑹
To obtain the generated data, one now needs to compute the inverse images of the feature maps
φι(∙) and φ2(∙) in the respective input spaces, i.e., solve the pre-image problem. We seek to find
the functions ψ1 : Rdf 7→ Rd and ψ2 : Rpf 7→ Rp corresponding to the two data-sources, such that
(ψ1 ◦ φ1)(x?) ≈ x? and (ψ2 ◦ φ2)(y?) ≈ y?, where φ1(x?) and φ2(y?) are calculated using Eq. 6.
When using kernel methods, explicit feature maps are not necessarily known. Commonly used
kernels such as the radial-basis function and polynomial kernels map the input data to a very high
dimensional feature space. Hence finding the pre-image, in general, is known tobean ill-conditioned
problem (Mika et al., 1999). However, various approximation techniques have been proposed (Bui
et al., 2019; Kwok & Tsang, 2003; Honeine & Richard, 2011; Weston et al., 2004) which could
be used to obtain the approximate pre-image X of φι(x?). In section 3.1, We employ one such
technique to demonstrate the applicability in our model, and consequently generate the multi-view
data. One could also define explicit pre-image maps. In section 3.2, We define parametric pre-image
maps and learn the parameters by minimizing the appropriately defined objective function. The next
section describes the above tWo pre-image methods for both cases, i.e., When the feature map is
explicitly knoWn or unknoWn, in greater detail.
3 Implicit & Explicit feature map
3.1	Implicit feature map
As noted in the previous section, since x? may not exist, we find an approximation X. A possible
technique is shoWn by Schreurs & Suykens (2018). Left multiplying Eq. 6by φ1(xi )τ and φ2(yi )τ,
∀i = 1, . . . , N , we obtain:
kx* = ɪ K1H τh?,	ky* = ɪ K2H τh?,
η1	η2
(7)
where, kχ* = [k(xι, x?),..., k(xN, x?)]> represents the similarities between φι(x?) and training
data points in the feature space, and K1 ∈ RN×N represents the centered kernel matrix of X .
Similar conventions follow for Y respectively. Using the kernel-smoother method (Hastie et al.,
2001), the pre-images are given by:
X = Ψι (φι(x?))
∙-v
Pj=I k1(Xj χ*)Xj
∙-v
Pn= ι kι(Xj, x? )
y = Ψ2 (Φ2 (y?))
∙-v
Pj=I k2(yj, y*)yj
∙-v
Pn=I ⅛2(yj, y?)
(8)
∙-v	∙-v
where k1(Xi , X?) and k2(yi , y?) are the scaled similarities (see Eq. 8) between 0 and 1 and nr the
number of closest points based on the similarity defined by kernels k1 and k2 .
4
Under review as a conference paper at ICLR 2020
3.2	Explicit Feature map
While Using an exPlicit featUre maP, Mercer’s theorem is still aPPlicable dUe to the Positive semi-
definiteness of the kernel fUnction by constrUction, thereby allowing the derivation of Eq. 3. In the
experiments, we use a set of (convolutional) neural networks as the feature maps φθ(∙). Another
(transposed convolutional) neural network is used for the pre-image map ψζ(∙) (Dumoulin & Visin,
2016). The network parameters {θ, ζ} are learned by minimizing the reconstruction errors defined
by L1(x, ψ1ζ1 (φ1θ1 (x))) and L2(y, ψ2ζ2 (φ2θ2 (y))). In our experiments, we use the mean-squared
errors LI(X,ψ1ζ1 (φ1θ1 (X))) = N PN=1 Ilxi - ψ1ζ1 (φ1θ1 (Xi))I∣2 and L2(y,ψ2ζ2 (φ2θ2 W)))=
N PN=1 ∣∣yi - ψ2ζ2 (Φ2θ2 (yi)) Il；, however, in principle, one can use any other loss appropriate to
the dataset. Here φ1θ1 (Xi) and φ2θ2 (yi) are computed from Eq. 6, i.e., the generated points in
feature space from the subspace H.
Adding the loss function directly into the objective function Jt is not suitable for minimization.
Instead, we use the stabilized objective function defined as Jstab = Jt + cstfbJ2, where Cstab ∈ R+
is the regularization constant (Suykens, 2017). This tends to push the objective function Jt towards
zero, which is also the case when substituting the solutions λi , hi back into Jt (see Appendix A.3
for details). The combined training objective is given by:
min	Jc = Jstab + cacc
θ1,θ2,Z1,Z2	+ 2N
L1 (xi, ψ1ζ1 (φ1θ1 (xi))) +L2(yi,ψ2ζ2(φ2θ2(yi)))	, (9)
where cacc ∈ R+ is a regularization constant to control the stability with reconstruction accuracy. In
this way, we combine feature-selection and subspace learning within the same training procedure.
There is also an intuitive connection between Gen-RKM and autoencoders. Namely, the properties
of kernel PCA resemble the objectives of the 3 variations of an autoencoder: standard (Kramer,
1991), VAE (Kingma & Welling, 2014) and β-VAE (Higgins et al., 2017). 1) Similar to an autoen-
coder, Gen-RKM minimizes the reconstruction error in the loss function (see Eq. 9), where kernel
PCA which acts as a denoiser (the information is compressed in the principal components). 2) By
interpreting kernel PCA within the LS-SVM setting (Suykens et al., 2002), the PCA analysis can
take the interpretation of a one-class modeling problem with zero target value around which one
maximizes the variance (Suykens et al., 2003). When choosing a good feature map, one expects
the latent variables to be normally distributed around zero. This property resembles the added reg-
ularization term in the objective of the VAE (Kingma & Welling, 2014), which is expressed as the
Kullback-Leibler divergence between the encoder’s distribution and a unit Gaussian as a prior on
the latent variables. 3) Kernel PCA gives uncorrelated components in feature space. While it was
already shown that PCA does not give a good disentangled representation for images (Eastwood
& Williams, 2018; Higgins et al., 2017). Hence by designing a good kernel (through appropriate
feature-maps) and doing kernel PCA, it is possible to get a disentangled representation for images
as we show on the example in Figure 5. The uncorrelated components enhances the interpretation
of the model.
4 The Gen-RKM Algorithm
Based on the previous analysis, we propose a novel algorithm, called the Gen-RKM algorithm,
combining kernel learning and generative models. We show that this procedure is efficient to train
and evaluate. It is also scalable to large datasets when using explicit feature maps. The training
procedure simultaneously involves feature selection, common-subspace learning and pre-image map
learning. This is achieved via an optimization procedure where one iteration involves an eigen-
decomposition of the kernel matrix which is composed of the features from various views (see Eq.
3). The latent variables are given by the eigenvectors, which are then passed via a pre-image map to
reconstruct the sample. Figure 1 shows a schematic representation of the algorithm when two data
sources are available.
Thanks to training in m mini-batches, this procedure is scalable to large datasets (sample size N)
with training time scaling SUPer-IinearIy with Tm = C mN-1, instead of Tk = cNY, where Y ≈ 3
for algorithms based on decomposition methods, with some proportionality constant c. The training
time coUld be fUrther redUced by comPUting the covariance matrix (size (df +pf)×(df +pf)) instead
5
Under review as a conference paper at ICLR 2020
of a kernel matrix (size m X m), when the sum of the dimensions of the feature-spaces is less than
the samples in mini-batch i.e. df + Pf ≤ m. While using neural networks as feature maps, df and
pf correspond to the number of neurons in the output layer, which are chosen as hyperparameters
by the practitioner. Eigendecomposition of this smaller covariance matrix would yield U and V
as eigenvectors (see Eq. 10 and Appendix A.2 for detailed derivation), where computing the hi
involves only matrix-multiplication which is readily parallelizable on modern GPUs:
-1 Φ Φ>
η φχφy
η2 φy φ>
Λ,
Φx := [φ1(x1), . . . , φ1(xN )] ,
Φy := [φ2 (y1), . . . , φ2 (yN )] .
(10)
U
V
U
V
Algorithm 1 Gen-RKM			
Input: {xi, yi}N=ι, ηι, η2, feature map φj(∙) - explicit or implicit via kernels kj(∙, ∙), for j ∈ {1, 2} Output: Generated data x? , y?			
1:	procedure TRAIN	1	procedure GENERATION
2:	if φj (∙) = Implicit then	2	Select h?
3:	Hyperparameters: kernel specific	3	if φj (∙) = Implicit then
4:	Solve Eq. 3	4	Hyperparameter: nr
5:	Select s principal components	5	Compute kχ*, ky* (Eq. 7)
6:	else if φj (∙) = Explicit then	6	Get x, y (Eq. 8)
7:	while not converged do	7	else if φj (∙) = Explicit then
8:	{x, y} — {Get mini-batch}	8	do steps 11-12
9:	φι(x) — x; φ2(y) - y	9	end if
10 11 12 13 14 15 16 17	:	do steps 4-5 {φ1(x),φ2(y)} — h (Eq. 6) {x, y} — {Ψ1(Φ1(x)),Ψ2(Φ2(y))} ∆θι X -Vθ1 Jc; ∆θ2 X -Vθ2 Jc ∆Zι X -Vζι Jc; ∆Z2 X -Vζ2 Jc :	end while :	end if : end procedure	10	: end procedure
5 Experiments
To demonstrate the applicability of the proposed framework and algorithm, we trained the Gen-
RKM model on a variety of datasets commonly used to evaluate generative models: MNIST (Le-
Cun & Cortes, 2010), Fashion-MNIST (Xiao et al., 2017), CIFAR-10 (Krizhevsky, 2009), CelebA
(Liu et al., 2015), Dsprites (Matthey et al., 2017) and Teapot (Eastwood & Williams, 2018). The
experiments were performed using both the implicit feature map defined by a Gaussian kernel and
parametric explicit feature maps defined by deep neural networks, either convolutional or fully con-
nected. As explained in Section 2, in case of kernel methods, training only involves constructing the
kernel matrix and solving the eigenvalue problem in Eq. 3. In our experiments, we fit a Gaussian
mixture model (GMM) with l components to the latent variables of the training set, and randomly
sample anew point h? for generating views using a kernel smoother. In case of explicit feature maps,
we define φ1θ1 and ψ1ζ1 as convolution and transposed-convolution neural networks, respectively
(Dumoulin & Visin, 2016); and φ2θ2 and ψ1ζ2 as fully-connected networks. The particular archi-
tecture details are outlined in Table 3 in the Appendix. The training procedure in case of explicitly
defined maps consists of minimizing Jc using the Adam optimizer (Kingma & Ba, 2014) to update
the weights and biases. To speed-up learning, we subdivided the datasets into m mini-batches, and
within each iteration of the optimizer, Eq. 3 is solved to update the value of H. Information on the
datasets and hyperparameters used for the experiments is given in Table 4 in the Appendix.
Generation:
Qualitative examples: Figure 2 shows the generated images using a convolutional neural network
and transposed-convolutional neural network as the feature map and pre-image map respectively.
The first column in yellow-boxes shows the training samples and the second column on the right
shows the reconstructed samples. The other images shown are generated by random sampling from
6
Under review as a conference paper at ICLR 2020
(b) Fashion-MNIST
j7 " g
6 4”
/ g 2 5
吊£夕O
(a) MNIST
(c) CIFAR-10
OOOGU /// /
OOCq • / / / / /
O 9 d , ∙ /< / / /
9sd ∕∙ / / /
"二二二m∙.: •，, .，•
55L二二 m∙，一•，一•
5 $ $ 5 S √- √- √∙ √ √
5 S 5 5 S √- √∙√T T
3 s55 3 ∙ :tth
55533 4∙√τ T H
(d) CelebA
(f) Bilinear interpolation: CelebA
(e) Bilinear interpolation: MNIST

Figure 2:	Generated samples from the model using CNN as explicit feature map in the kernel func-
tion. In (a), (b), (c), (d) the yellow boxes in the first column show training examples and the adjacent
boxes show the reconstructed samples. The other images (columns 3-6) are generated by random
sampling from the fitted distribution over the learned latent variables. (e) and (f) shows the generated
images through bilinear interpolations in the latent space.
Male
Bags under eye
Narrow eyes
Side buns
Mouth slightly open
No beard
Narrow eyes
smiling
Young
No beard
Narrow eyes
0’clock shadow
No beard
Big nose
Mouth slightly open
Oval face
Figure 3:	Multi-view generation on CelebA dataset showing images and attributes.
7
Under review as a conference paper at ICLR 2020
119873063425

(a) MNIST: Implicit feature maps with Gaussian kernel are used during training. For generation, the pre-images
are computed using the kernel-smoother method.
917646082137
"76qQ8W J<37
(b) MNIST: Explicit feature maps and the corresponding pre-image maps are defined by the Convolutional
Neural Networks.
horse bird horse dog cat plane horse bird car plane ship deer
(c) CIFAR-10: Explicit feature maps as Convolutional Neural Networks. Pre-images are computed using
Transposed CNNs.
Figure 4: Multi-view Generation (images and labels) on various datasets using implicit and explicit
feature maps.
a GMM over the learned latent variables. Notice that the reconstructed samples are of better quality
visually than the other images generated by random sampling. To elucidate that the model has not
merely memorized the training examples, we show the generated images via bilinear-interpolations
in the latent space in 2e and 2f.
Comparison: We compare the proposed model with the standard VAE (Kingma & Welling, 2014).
For a fair comparison, the models have the same encoder/decoder architecture, optimization param-
eters and are trained until convergence, where the details are given in Table 3. We evaluate the
performance qualitatively by comparing reconstruction and random sampling, the results are shown
in Figure 8 in the Appendix. In order to quantitatively assess the quality of the randomly gener-
ated samples, We use the Frechet Inception Distance (FID) introduced by HeUsel et al. (2017). The
results are reported in Table 1. Experiments were repeated for different latent-space dimensions
(hdim), and We observe empirically that FID scores are better for the Gen-RKM. This is confirmed
by the qualitative evaluation in Table 8, Where the VAE generates smoother images. An interesting
trend could be noted that as the dimension of latent-space is increased, VAE gets better at gener-
ating images Whereas the performance of Gen-RKM decreases slightly. This is attributed to the
eigendecomposition of the kernel matrix Whose eigenvalue spectrum decreases rapidly depicting
that most information is captured in feW principal components, While the rest is noise. The presence
of noise hinders the convergence of the model. It is therefore important to select the number of latent
variables proportionally to the size of the mini-batch and the corresponding spectrum of the kernel
matrix (the diversity Within a mini-batch affects the eigenvalue spectrum of the kernel matrix).
Table 1: FID Scores (Heusel et al., 2017) for randomly generated samples (smaller is better).
Dataset	Algorithm	FID score		
		hdim = 10	hdim = 30	hdim = 50
MNIST	Gen-RKM	-89.825-	130.497	131.696
	VAE	250	234.749	205.282
CelebA	Gen-RKM	103.299	-84.403-	-85121-
	VAE 一	286.039	245.738	225.783
Multi-view Generation: Figures 3 & 4 demonstrate the multi-vieW generative capabilities of the
model. In these datasets, labels or attributes are seen as another vieW of the image that provides
extra information. One-hot encoding of the labels Was used to train the model. Figure 4a shoWs the
generated images and labels When feature maps are only implicitly knoWn i.e. through a Gaussian
kernel. Figures 4b, 4c shoWs the same When using fully-connected netWorks as parametric functions
8
Under review as a conference paper at ICLR 2020
to encode and decode labels. We can see that both the generated image and the generated label
matches in most cases, albeit not all.
Figure 5: Exploring the learned uncorrelated-features by traversing along the eigenvectors. The first
column shows the scatter plot of latent variables using the top two principal components. The green
lines within, show the traversal in the latent space and the related rows show the corresponding
reconstructed images.
Table 2: Disentanglement Metric on DSprites and Teapot dataset with Lasso and Random Forest
regressor (Eastwood & Williams, 2018). For disentanglement and completeness higher score is
better, for informativeness, lower is better.
			Lasso			Random Forest		
	hdim	Algorithm	Disent.	Comple.	Inform.	Disent.	Comple.	Inform.
DSprites	10	Gen-RKM VAE β-VAE (β = 3)	0.30 0.11 0.53	-0!0- 0.09 0.18	-087- 0.17 0.18	0.12 0.73 0.58	-0Γ0- 0.54 0.36	-028- 0.06 0.06
	2	Gen-RKM VAE β-VAE (β = 3)	0.72 0.04 0.13	-071- 0.01 0.40	-064- 0.87 0.71	0.05 0.01 0.00	-0TT9- 0.13 0.26	-0:03- 0.11 0.09
Teapot	10	Gen-RKM VAE β-VAE (β = 3)	0.28 0.28 0.33	-0:23- 0.21 0.25	-039- 0.36 0.36	0.48 0.30 0.31	-0:39- 0.27 0.24	-0:19- 0.21 0.20
	5	Gen-RKM VAE β-VAE (β = 3)	0.22 0.16 0.31	-0:23- 0.14 0.25	-074- 0.66 0.68	0.08 0.11 0.13	-0:09- 0.14 0.15	-0:27- 0.28 0.29
Disentanglement:
Qualitative examples: The latent variables are uncorrelated, which gives an indication that the model
could resemble a disentangled representation. This is confirmed by the empirical evidence on Fig-
ure 5, where we explore the uncorrelated features learned by the models on the Dsprites and celebA
dataset. In our experiments, the Dsprites training dataset comprised of 32 × 32 positions of oval
and heart-shaped objects. The number of principal components chosen were 2 and the goal was
to findout whether traversing along the eigenvectors, corresponds to traversing the generated im-
9
Under review as a conference paper at ICLR 2020
age in one particular direction while preserving the shape of the object. Rows 1 and 2 of Figure 5
show the reconstructed images of an oval while moving along first and second principal component
respectively. Notice that the first and second components correspond to the y and x positions re-
spectively. Rows 3 and 4 show the same for hearts. On the celebA dataset, we train the Gen-RKM
with 15 components. Rows 5 and 6 shows the reconstructed images while traversing along the prin-
cipal components. When moving along the first component from left-to-right, the hair-color of the
women changes, while preserving the face structure. Whereas traversal along the second compo-
nent, transforms a man to woman while preserving the orientation. When the number of principal
components were 2 while training, the brightness and background light-source corresponds to the
two largest variances in the dataset. Also notice that, the reconstructed images are more blurry due
to the selection of less number of components to model H.
Comparison: To quantitatively assess disentanglement performance, we compare Gen-RKM with
VAE (Kingma & Welling, 2014) and beta-VAE (Higgins et al., 2017) on the Dsprites and Teapot
datasets (Eastwood & Williams, 2018). The models have the same encoder/decoder architecture, op-
timization parameters and are trained until convergence, where the details are given in Table 3. The
performance is measured using the proposed framework3 of Eastwood & Williams (2018), which
gives 3 measures: disentanglement, completeness and informativeness. The results are depicted in
Table 2. Gen-RKM has good performance on the Dsprites dataset when the latent space dimension
is equal to 2. This is expected as the number of disentangled generating factors in the dataset is
also equal to 2, hence there are no noisy components in the kernel PCA hindering the convergence.
The opposite happens in the case hdim = 10, where noisy component are present. The above is
confirmed by the Relative Importance Matrix on Figure 6 in the Appendix, where the 2 generating
factors are well separated in the latent space of the Gen-RKM. For the Teapot dataset, Gen-RKM
has good performance when hdim = 10. More components are needed to capture all variations in
the dataset, where the number of generating factors is now equal to 5. In the other cases, Gen-RKM
has a performance comparable to the others.
6 Conclusion and future work
The paper proposes a novel framework, called Gen-RKM, for generative models based on RKMs
with extensions to multi-view generation and learning uncorrelated representations. This allows for
a mechanism where the feature map can be implicitly defined using kernel functions or explicitly
by (deep) neural network based methods. When using kernel functions, the training consists of
only solving an eigenvalue problem. In the case of a (convolutional) neural network based explicit
feature map, we used (transposed) networks as the pre-image functions. Consequently, a training
procedure was proposed which involves joint feature-selection and subspace learning. Thanks to
training in mini-batches and capability of working with covariance matrices, the training is scalable
to large datasets. Experiments on benchmark datasets illustrate the merit of the proposed framework
for generation quality as well as disentanglement. Extensions of this work consists of adapting the
model to more advanced multi-view datatsets involving speech, images and texts; further analysis
on other feature maps, pre-image methods, loss-functions and uncorrelated feature learning. Finally,
this paper has demonstrated the applicability of the Gen-RKM framework, suggesting new research
directions to be worth exploring.
3Code and dataset available at https://github.com/cianeastwood/qedr
10
Under review as a conference paper at ICLR 2020
References
Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy. Deep variational information bottleneck.
In ICLR, 2017.
Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder:
Learning disentangled representations from grouped observations. In Thirty-Second AAAI Con-
ference on Artificial Intelligence, 2018.
Anh Tuan Bui, Joon-Ku Im, Daniel W. Apley, and George C. Runger. Projection-Free Kernel Prin-
cipal Component Analysis for Denoising. Neurocomputing, 2019. ISSN 0925-2312.
Mickael Chen and LUdovic Denoyer. Multi-view generative adversarial networks. In Joint Euro-
pean Conference on Machine Learning and Knowledge Discovery in Databases, pp. 175-188.
Springer, 2017.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems, pp.
2610-2620, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning. arXiv
preprint arXiv:1603.07285, 2016.
Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=By-7dz-AZ.
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic Goal Generation for
Reinforcement Learning Agents. In Proceedings of the 35th International Conference on Ma-
chine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1515-1528, Stock-
holmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural In-
formation Processing Systems 27: Annual Conference on Neural Information Processing Systems
2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672-2680, 2014.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.
Springer New York Inc., New York, NY, USA, 2001.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Proceed-
ings of the 31st International Conference on Neural Information Processing Systems, NIPS’17,
pp. 6629-6640, USA, 2017. Curran Associates Inc. ISBN 978-1-5108-6096-4. URL http:
//dl.acm.org/citation.cfm?id=3295222.3295408.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a
constrained variational framework. ICLR, 2(5):6, 2017.
Paul Honeine and Cedric Richard. Preimage Problem in Kernel-Based Machine Learning. IEEE
Signal Processing Magazine, 28(2):77-88, March 2011. ISSN 1053-5888.
Lynn Houthuys and Johan A K Suykens. Tensor learning in multi-view kernel PCA . In 27th
International Conference on Artificial Neural Networks ICANN, Rhodes, Greece, volume 11140,
pp. 205-215, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
11
Under review as a conference paper at ICLR 2020
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Mark A Kramer. Nonlinear principal component analysis using autoassociative neural networks.
AIChE journal, 37(2):233-243,1991.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009.
James T. Kwok and Ivor Wai-Hung Tsang. The pre-image problem in kernel methods. IEEE Trans-
actions on Neural Networks, 15:1517-1525, 2003.
Hugo Larochelle and Yoshua Bengio. Classification using discriminative restricted Boltzmann ma-
chines. In Proceedings of the 25th International Conference on Machine Learning - ICML ’08,
pp. 536-543, Helsinki, Finland, 2008. ACM Press. ISBN 978-1-60558-205-4.
Neil Lawrence. Probabilistic non-linear principal component analysis with gaussian process latent
variable models. JMLR, 6:1783-1816, December 2005. ISSN 1532-4435. URL http://dl.
acm.org/citation.cfm?id=1046920.1194904.
Yann LeCun and Corinna Cortes.	MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/, 2010. URL http://yann.lecun.com/exdb/
mnist/.
Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004.,
volume 2, pp. II-97-104 Vol.2, 2004.
Ming-Yu Liu and Oncel Tuzel. Coupled Generative Adversarial Networks. In Advances in Neural
Information Processing Systems 29, pp. 469-477. Curran Associates, Inc., 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
James Mercer. Functions of Positive and Negative Type, and Their Connection the Theory of Integral
Equations. Philosophical Transactions of the Royal Society of London. Series A, Containing
Papers of a Mathematical or Physical Character, 209(441-458):415-446, January 1909.
Sebastian Mika, Bernhard SchOlkopf, Alex Smola, Klaus-Robert Muller, Matthias Scholz, and Gun-
nar Ratsch. Kernel PCA and De-noising in Feature Spaces. In Proceedings ofthe 1998 Conference
on Advances in Neural Information Processing Systems II, pp. 536-542. MIT Press, 1999.
Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence
Carin. Variational Autoencoder for Deep Learning of Images, Labels and Captions. NIPS’16, pp.
2360-2368. Curran Associates Inc., USA, 2016. ISBN 978-1-5108-3881-9.
Lawrence R Rabiner and Biing-Hwang Juang. An introduction to Hidden Markov models. IEEE
ASSP magazine, 3(1):4-16, 1986.
Karl Ridgeway. A survey of inductive biases for factorial representation-learning.	CoRR,
abs/1612.05299, 2016.
Ralph Tyrrell Rockafellar. Conjugate Duality and Optimization. SIAM, 1974.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep Boltzmann Machines. Proceedings of the 12th
International Conference on Artificial Intelligence and Statistics, Volume 5 of JMLR, 2009.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted Boltzmann machines for
collaborative filtering. In ICML ’07, pp. 791-798, Corvalis, Oregon, 2007. ACM Press.
12
Under review as a conference paper at ICLR 2020
Jurgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,
4(6):863-879,1992.
Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2001. ISBN
0262194759.
Joachim Schreurs and Johan A. K. Suykens. Generative Kernel PCA. In European Symposium
on Artificial Neural Networks, Computational Intelligence and Machine Learning , pp. 129-134,
2018.
Paul Smolensky. Parallel distributed processing: Explorations in the microstructure of cognition,
vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory,
pp. 194-281. MIT Press, Cambridge, MA, USA, 1986. ISBN 0-262-68053-X.
Johan A. K. Suykens. Deep Restricted Kernel Machines using Conjugate Feature Duality. Neural
Computation, 29(8):2123-2163, August 2017. ISSN 0899-7667, 1530-888X.
Johan A. K. Suykens, Tony Van Gestel, Jos De Brabanter, Bart De Moor, and Joos Vandewalle.
Least Squares Support Vector Machines. World Scientific, River Edge, NJ, January 2002. ISBN
978-981-238-151-4.
Johan A. K. Suykens, Tony Van Gestel, Joos Vandewalle, and Bart De Moor. A support vector ma-
chine formulation to PCA analysis and its kernel version. IEEE Transactions on neural networks,
14(2):447-450, 2003.
Michael E. Tipping and Chris M. Bishop. Probabilistic principal component analysis. Journal Of
The Royal Statistical Society, series B, 61(3):611-622, 1999.
Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled representation learning GAN for pose-invariant
face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 1415-1424, 2017.
Aaron Van Den Oord, Nal Kalchbrenner, and Koray KavukCuoglu. Pixel Recurrent Neural Net-
works. In Proceedings of the 33rd International Conference on International Conference on
Machine Learning - Volume 48, ICML’16, pp. 1747-1756. JMLR.org, 2016.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a
Local Denoising Criterion. Journal of Machine Learning Research, 11:3371-3408, 2010.
Jason Weston, Bernhard Scholkopf, and Gokhan H. Bakir. Learning to Find Pre-Images. In NIPS
16, pp. 449-456. 2004.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Bench-
marking Machine Learning Algorithms. 2017.
Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson,
and Minh N. Do. Semantic image inpainting with deep generative models. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), July 2017.
13
Under review as a conference paper at ICLR 2020
A Appendix
A. 1 Derivation of Gen-RKM objective function
Given D = {xi, yi}iN=1, where xi ∈ Rd, yi ∈ Rp and feature-map φ1 : Rd 7→ Rdf and φ2 : Rp 7→
Rpf , the Least-Squares Support Vector Machine (LS-SVM) formulation of Kernel PCA (Suykens
et al., 2002) for the two data sources can be written as:
η	η	1N
min η1	Tr(U>U) +	η2	Tr(VTV)-万	Xe>ei	πn
U,V,ei 2	2	2λ	i=1	(11)
s.t. ei = U>φ1(xi) + V> φ2(yi) ∀i = 1, . . . ,N,
where U ∈ Rd×s and V ∈ Rp×s are the interconnection matrices.
Using the notion of conjugate feature duality introduced in Suykens (2017), the error variables ei
are conjugated to latent variables hi using:
ɪe>e + λh>h ≥ e>h,	∀e, h ∈ Rs	(12)
2λ	2
which is also known as the Fenchel-Young inequality for the case of quadratic functions (Rockafel-
lar, 1974). By eliminating the variables ei from Eq. 11 and using Eq. 12, we obtain the Gen-RKM
training objective function:
Nλ
Jt = N 卜。13YUhi- φ2(yi)τVhi + 2h>hi) + η1 Tr(U>U) + η2 Tr(V>V). (13)
A.2 Kernel PCA in the primal
From Eq. 2, eliminating the variables hi yields the following:
1
ηι
NN
Xφ1(xi)φ1(xi)τU+Xφ1(xi)φ2(yi)τV =λU,
i=1	i=1
NN
—X Φ2(yi)Φ1(χi )τu + X Φ2(yi)Φ2(yi)τv = λV.
η2 i=1	i=1
(14)
Denote Φx := [φ1(x1), . . . , φ1 (xN)], Φy := [φ2 (y1), . . . , φ2 (yN)] and Λ = diag{λ1, . . . , λs } ∈
Rs×s with s ≤ N . Now, composing the above equations in matrix form, we get the following
eigen-decomposition problem:
一 η⅛ φχφ>
一η2 φy φ>
-1 Φ Φ>
ηi φxφy
-1 Φ Φτ
η2 φy φy J
Λ.
(15)
U
V
U
V
Here the size of the covariance matrix is (df + pf) × (df + pf). The latent variables hi can be
computed using Eq. 2, which simply involves matrix multiplications.
A.3 Stabilizing the objective function
Proposition 1. All stationary solutions for H,Λ in Eq. 3 of Jt lead to Jt = 0.
Proof. Let λi , hi are given by Eq. 3. Using Eq. 2 to substitute V and U in Eq. 1 yields:
N λ	1N	N
Jt(V, U, λ, H) = X -2h>hi + 71 Tr ( n2 X hiφ1(xi)> X φ1(xj)h>
(1 N	N	∖
滔 X hiφ2(yi)τ X φ2(yj )h>
η2 i=1	j=1
14
Under review as a conference paper at ICLR 2020
=X - λ h>h，+ η2τTY (η12 HKIH >)+ η22Tr (a HaH >
=X - λ h>hi+ 1Tr (H [ η1ι KI+ η12 K2] H)
From Eq. 3, we get:
N λ 1	N λ	λN
Jt(V, U, Λ, H) = E- 2 h>hi +2τr (HH >λ) = E- 2 h>hi + 2 £ h>hi = 0.
=
=
=
□
Proposition 2. Let J(x) : RN -→ R be a smooth function, for all x ∈ RN and for c ∈ R>0, define
c
J(X) := J(x) + 2 J(x)2. Assuming (1 + CJ(x)) = 0, then x is the stationary points of J(X) iff
x? is the stationary point for J(x).
Proof. Let x? be a stationary point of J(x), meaning that VJ(x?) = 0. The stationary points for
J(X) can be obtained from:
dJ = (VJ(x) + CJ(x)VJ(x)) = (1 + CJ(x)) VJ(x).	(16)
dx
It is easy to see from Eq. 2 that if X = x*, VJ(x*) = 0, we have that 里 ∣	= 0, meaning that all
_	dx lx*
the stationary points of J(x) are stationary points of J(X).
To show the other way, let x? be stationary point of J(X) i.e. VJ(X?) = 0. Assuming (1 +
CJ (x?)) 6= 0, then from Eq. 16 for all C ∈ R>0, we have
(1 + CJ(X?)) VJ(X?) =0,
implying that VJ(x?) = 0.	□
Based on the above propositions, we stabilize our original objective function Eq. 1 to keep it
bounded and hence is suitable for minimization with Gradient-descent methods. Without the re-
construction errors, the stabilized objective function is
min
U,V,hi
Jt + 2 Jt.
Denoting J = Jt + cs2ab J2. Since the derivatives of Jt are given by Eq. 2, the stationary points of
J are:
瑞= (1 + CstabJt) (— P=I Φι(xi)h> + ηιV) = 0
J = (1 + CstabJt) (- PN=I φ2(yi)h> + η2 U) = 0
∂j = (I + CstabJt) ( — V>φ1(Xi)- U>φ2(yi) + λhi) = 0
=⇒ V = n11 PN=I φ1(Xi)h>,
=⇒ U = η2 PN=I φ2(yi)h>,
=⇒ λhi = V>φ1(Xi)
+ U>φ2(yi),
assuming 1 + CstabJt = 0. Elimination of V and U yields	Ki + + K^ H > = H >Λ, which
is indeed the same solution for Cstab = 0 in Eq. 1 and Eq. 3.
A.4 Centering of kernel matrix
Centering of the kernel matrix is done by the following equation:
Kc = K - N-i11>K - N-iK11> + N-211>K11>,	(17)
where 1 denotes an N -dimensional vector of ones and K is either Ki or K2.
15
Under review as a conference paper at ICLR 2020
A.5 Architecture details
See Table 3 and 4 for details on model architectures, datasets and hyperparameters used in this
paper. The PyTorch library in Python was used as the programming language with a 8GB NVIDIA
QUADRO P4000 GPU.
Table 3: Details of model architectures used in the paper. All convolutions and transposed-
convolutions are with stride 2 and padding 1. Unless stated otherwise, the layers have Parametric-
RELU (α = 0.2) activation function, except the output layers of the pre-image maps which has
sigmoid activation function.
Dataset	Optimizer (Adam)	Architecture		
		X		Y
MNIST	1e-3	Input Feature-map (fm) Pre-image map Latent space dim.	28x28x1 Conv 32x4x4; Conv 64x4x4; FC 128 (Linear) reverse of fm	10 (One-hot encoding) FC 15, 20 (Linear) reverse of fm 500
Fashion -MNIST	1e-3	Input Feature-map Pre-image map (fm) Latent space dim.	28x28x1 Conv 32x4x4; 64x4x4; FC 128 (Linear) reverse of fm	10 (One-hot encoding) FC 15, 20 reverse of fm 100
CIFAR-10	1e-3	Input Feature-map (fm) Pre-image map Latent space dim.	32x32x3 Conv 64x4x4; Conv 128x4x4; FC 128 (Linear) reverse of fm	10 (One-hot encoding) FC 15, 20 reverse of fm 500
CelebA	1e-4	Input Feature-map (fm) Pre-image map Latent space dim.	64x64x3 Conv 32x4x4; Conv 64x4x4; Conv 128x4x4; Conv 256x4x4 ; FC 128 (Linear) reverse of fm	- - - 15
Dsprites	1e-4	Input Feature-map (fm) Pre-image map Latent space dim.	64x64x1 Conv 20x4x4; Conv 40x4x4; Conv 80x4x4; FC 128 (Linear) reverse of fm	- - - 2/10
Teapot	1e-4	Input Feature-map (fm) Pre-image map Latent space dim.	64x64x3 Conv 30x4x4; Conv 60x4x4; Conv 90x4x4; FC 128 (Linear) reverse of fm	- - - 5/10
16
Under review as a conference paper at ICLR 2020
Table 4: Datasets and hyperparameters used for the experiments. The bandwidth of the Gaussian
kernel for generation corresponds to the bandwidth that gave the best performance determined by
cross-validation on the MNIST classification problem.
Dataset	N	d	Nsubset	s	m	σ	nr	l
MNIST	60000	28 X 28	5000	500	50	1.3	4	10
Fashion-MNIST	60000	28 × 28	500	100	5	/	/	10
CIFAR-10	60000	32 × 32 × 3	500	500	5	/	/	10
CelebA	202599	128 × 128 × 3	500	15	5	/	/	20
Dsprites	737280	64 × 64	1024	2/10	5	/	/	/
Teapot	200000	64 × 64 × 3	1000	5/10	100	/	/	/
A.6 Bilinear Interpolation
Given four vectors h1, h2, h3 and h4 (reconstructed images from these vectors are shown at the
edges of Figs. 2e, 2f), the interpolated vector h? is given by:
h? = (1 - α)(1 - γ)h1 + α(1 - γ)h2 + γ(1 - α)h3 + γαh4, 0 ≤ α, γ ≤ 1.
This h? is then used in step 8 of the generation procedure of Gen-RKM algorithm (see Algorithm
1) to compute x?.
A.7 Visualizing the disentanglement metric
In this section we show the Hinton plots to visualize the disentaglement scores as shown in Table
2. Following the conventions of Eastwood & Williams (2018), z represents the ground-truth data
generating factors. Figs. 6 & 7 shows the Hinton plots on DSprites and Teapot datasets using Lasso
and Random Forest regressors for various algorithms. Here the square size indicates the magnitude
of the relative importance of the latent code hi in predicting zi .
17
Under review as a conference paper at ICLR 2020
Random Forest
β-VAE
β-VAE
Gen-RKM
Gen-RKM
VAE
Lasso
VAE
(a) hdim = 10
Gen-RKM
Lasso
VAE
β-VAE
Random Forest
Gen-RKM VAE β-VAE
Γ
(b) hdim = 2
Figure 6: Relative importance matrix as computed by Lasso and Random Forest regaressors on
DSprites dataset for hdim = {10, 2} against the underlying data generating factors zdim = {2}
corresponding to x, y positions of object.
Lasso
Gen-RKM VAE
I	Random Forest
β -VAE	Gen-RKM VAE
(b) hdim = 5
Figure 7: Relative importance matrix as computed by Lasso and Random Forest regaressors on
Teapot dataset for hdim = {10, 5} against the underlying data generating factors zdim = {5}
corresponding to azimuth, elevation and colors red, green and blue of the teapot object.
β-VAE
Z
18
Under review as a conference paper at ICLR 2020
A.8 Further empirical results
VAE
Gen-RKM
7 10。7?
F \ 7/
K q，oɔ/
s√√/
5 q57 g y
6qJ7/ y
，45¾75
243 J 75
F109 7夕
P \ 7/
s√√ofn/
4“ 夕 Ci
5qJ，
6q J 7揖，
p43 S 75
，43 Ln 75
Figure 8: Comparing Gen-RKM and standard VAE for reconstruction and generation quality. In re-
construction MNIST and reconstruction CelebA, uneven columns correspond to the original image,
even columns to the reconstructed image.
ISINn SUo=Onbsuoo°XlSINn UolaJouo°UIoPUBX vqoIo□ SUo=Onbsuoo°XVqoIo□ UoIaJouo°UIoPUEX
19