Under review as a conference paper at ICLR 2020
Training Neural Networks
for and by Interpolation
Anonymous authors
Paper under double-blind review
Ab stract
In modern supervised learning, many deep neural networks are able to interpolate
the data: the empirical loss can be driven to near zero on all samples simultaneously.
In this work, we explicitly exploit this interpolation property for the design of a
new optimization algorithm for deep learning. Specifically, we use it to compute an
adaptive learning-rate in closed form at each iteration. This results in the Adaptive
Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains
the main advantage of SGD which is a low computational cost per iteration. But
unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter
and does not require a decay schedule, which makes it considerably easier to tune.
We provide convergence guarantees of ALI-G in the stochastic convex setting.
Notably, all our convergence results tackle the realistic case where the interpolation
property is satisfied up to some tolerance. We provide experiments on a variety of
architectures and tasks: (i) learning a differentiable neural computer; (ii) training
a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the
SNLI data set; and (iv) training wide residual networks and densely connected
networks on the CIFAR data sets. ALI-G produces state-of-the-art results among
adaptive methods, and even yields comparable performance with SGD, which
requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to
implement in any standard deep learning framework and can be used as a drop-in
replacement in existing code.
1 Introduction
Training a deep neural network is a challenging optimization problem: it involves minimizing the
average of many high-dimensional non-convex functions. In practice, the main algorithms of choice
are Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951) and adaptive gradient methods
such as AdaGrad (Duchi et al., 2011) or Adam (Kingma & Ba, 2015). In recent work, the ability to
interpolate - i.e. to achieve near zero loss on all training samples simultaneously - has been used to
show convergence of SGD (Ma et al., 2018, Vaswani et al., 2019a, Zhou et al., 2019). This property is
usually satisfied in supervised deep learning because of the empirical success of over-parameterized
architectures. However, while the convergence analyses provide a better theoretical understanding of
SGD, they do not help improve its practical behavior.
In this work, we open a different line of enquiry, namely: can the interpolation property be used
to design a robust and efficient optimization algorithm for deep learning? In order to answer this
question, we begin by giving the following two desiderata of an optimization algorithm for deep
learning: (i) an inexpensive computational cost per iteration (typically a call to a stochastic first-order
oracle); and (ii) adaptive learning-rates that do not require a manually designed schedule.
We present ALI-G (Adaptive Learning-rates for Interpolation with Gradients), an algorithm that
takes advantage of interpolation by design and satisfies both properties mentioned above. Key to the
ALI-G algorithm are the following two ideas. First, an adaptive learning-rate can be computed for
the non-stochastic gradient direction when the minimum value of the objective function is known
(Polyak, 1969, Shor, 1985, BrannIUnd, 1995, Nedic & Bertsekas, 2001a;b). And second, one such
minimum value is usually approximately known for interpolating models: for instance, it is close
to zero for a model trained with the cross-entropy loss. By carefully combining these two ideas,
1
Under review as a conference paper at ICLR 2020
we create a stochastic algorithm that provably converges fast in the convex setting and that obtains
state-of-the-art results with neural networks.
Procedurally, ALI-G is close to many existing algorithms, such as Deep Frank-Wolfe (Berrada et al.,
2019), APROX (Asi & Duchi, 2019) and L4 (Rolinek & Martius, 2018). And yet uniquely, thanks to
its careful design and analysis, ALI-G enables accurate optimization of a wide class of deep neural
networks using only a single hyper-parameter that does not need to be decayed. This makes ALI-G
well-suited to the deep learning setting, where hyper-parameter tuning is widely regarded as an
onerous and time consuming task. Since ALI-G is easy to implement in any deep learning framework,
we believe that it can prove to be a practical and reliable optimization tool for deep learning.
Contributions. We summarize the contributions of this work as follows:
- We design an optimization algorithm that uses a single hyper-parameter for its learning-rate and
does need any decaying schedule. In contrast, the closely related aProx (Asi & Duchi, 2019) and
L4 (Rolinek & Martius, 2018) use respectively two and four hyper-parameters for their learning-rate.
- We provide convergence rates of ALI-G in various stochastic convex settings. Importantly, our
theoretical results take into account the error in the estimate of the minimum objective value. To
the best of our knowledge, our work is the first to establish convergence rates for interpolation with
approximate estimates.
- We demonstrate state-of-the-art results for ALI-G on learning a differentiable neural computer;
training variants of residual networks on the SVHN and CIFAR data sets; and training a Bi-LSTM on
the Stanford Natural Language Inference data set.
2 The Algorithm
2.1	Problem Setting
Loss Function. We consider a supervised learning task where the model is parameterized by
w ∈ Rp. Usually, the objective function can be expressed as an expectation over z ∈ Z, a random
variable indexing the samples of the training set:
f(w) , Ez∈Z['z(w)],	(1)
where each `z is the loss function associated with the sample z. We assume that each `z is non-
negative, which is the case for the large majority of loss functions used in machine learning. For
instance, suppose that the model is a deep neural network with weights w performing classification.
Then for each sample z, `z(w) can represent the cross-entropy loss, which is always non-negative.
Other non-negative loss functions include the structured or multi-class hinge loss, and the L1 or L2
loss functions for regression.
Regularization. It is often desirable to employ a regularization function φ in order to promote
generalization. In this work, we incorporate such regularization as a constraint on the feasible domain:
Ω = {w ∈ Rp : φ(w) ≤ r} for some value of r. In the deep learning setting, this will allow Us
to assume that the objective function can be driven close to zero without unrealistic assumptions
about the regularization. Our framework can handle any constraint set Ω on which Euclidean
projections are computationally efficient. This includes the feasible set induced by L2 regularization:
Ω = {w ∈ Rp : ∣∣wk2 ≤ r}, for which the projection is given by a simple rescaling of w. Finally,
note that if We do not wish to use any regularization, We define Ω = Rp and the corresponding
projection is the identity.
Problem Formulation. The learning task can be expressed as the problem (P) of finding a feasible
vector of parameters w? ∈ Ω that minimizes f:
w? ∈ argmin f (w).	(P)
w∈Ω
Also note that f? refers to the minimum value of f over Ω: f?，minw∈Ω f (w).
2.2	The Polyak Step-Size
Before outlining the ALI-G algorithm, we begin with a brief description of the Polyak step-size, from
which ALI-G draws some fundamental ideas.
2
Under review as a conference paper at ICLR 2020
Setting. We assume that f? is known and we use non-stochastic updates: at each iteration, the full
objective f and its derivative are evaluated. We denote by Vf (W) the first-order derivative of f at W
(e.g. Vf (w) can be a sub-gradient or the gradient). In addition, k ∙ ∣∣ is the standard Euclidean norm
in Rp, and ∏ω(w) is the Euclidean projection of the vector W ∈ Rp on the set Ω.
Polyak Step-Size. At time-step t, using the Polyak step-size (Polyak, 1969, Shor, 1985, Brannlund,
1995, Nedic & Bertsekas, 2001a;b) yields the following update:
wt+ι=πω (Wt-YtVf(Wt)), where Yt, fVw(W-)f?,
(2)
where We loosely define 0 = 0 for simplicity purposes.
Interpretation. It can be shown that Wt+1 lies on
the intersection between the linearization of f at Wt
and the horizontal plane z = f? (see Figure 1, more
details in Proposition 2 in the supplementary material).
Note that since f? is the minimum of f , the Polyak
step-size Yt is necessarily non-negative.
wt wt+1 w?
Figure 1: Illustration of the Polyak step-size
in 1D. In this case, and further assuming
that f? = 0, the algorithm coincides with the
Newton-Raphson method for finding roots of
a function.
Limitations. Equation (2) has two major short-
comings that prevent its applicability in a machine
learning setting. First, each update requires a full eval-
uation of f and its derivative. Stochastic extensions
have been proposed in NediC & Bertsekas (2001a;b),
but they still require frequent evaluations of f . This is
expensive in the large data setting, and even computa-
tionally infeasible when using massive data augmen-
tation. Second, when applying this method to the non-convex setting of deep neural networks, the
method sometimes fails to converge.
Therefore we would like to design an extension of the Polyak step-size that (i) is inexpensive to
compute in a stochastic setting (e.g. with a computational cost that is independent of the total number
of training samples), and (ii) converges in practice when used with deep neural networks. The next
section introduces the ALI-G algorithm, which achieves these two goals in the interpolation setting.
2.3	The ALI-G Algorithm
We now present the ALI-G algorithm. For this, we suppose that we are in an interpolation setting:
the model is assumed to be able to drive the loss function to near zero on all samples simultaneously.
Algorithm. The main steps of the ALI-G algorithm are provided in Algorithm 1. ALI-G iterates
over three operations until convergence. First, it computes a stochastic approximation of the learning
objective and its derivative (line 3). Second, it computes a step-size decay parameter Yt based on the
stochastic information (line 4). Third, it updates the parameters by moving in the negative derivative
direction by an amount specified by the step-size and projecting the resulting vector on to the feasible
region (line 5).
Algorithm 1 The ALI-G algorithm
Require: maximal learning-rate η, initial feasible wq ∈ Ω, small constant δ > 0
1:	t=0
2:	while not converged do
3：	Get 'zt (wt), V'zt (Wt) with zt drawn i.i.d.
4：	Yt=min n k"z(Wwt)…,η0
5:	Wt+1 = ∏Ω (Wt — YtV'zt (Wt))
6： t = t + 1
7: end while
3
Under review as a conference paper at ICLR 2020
Comparison with the Polyak Step-Size. There are three main differences to the update in equation
(2). First, each update only uses the loss `zt and its derivative rather than the full objective f and its
derivative. Second, the learning-rate γt is clipped to η, the maximal learning-rate hyper-parameter. We
emphasize that η remains constant throughout the iterations, therefore it is a single hyper-parameter
and does not need a schedule like SGD learning-rate. Third, the minimum f? has been replaced by
the lower-bound of 0. All these modifications will be justified in the next section.
The ALI-G∞ Variant. When ALI-G uses no maximal learning-rate, we refer to the algorithm as
ALI-G∞, since it is equivalent to use an infinite maximal learning-rate. Note that ALI-G∞ requires
no hyper-parameter for its step-size.
3	Justification and Analysis
3.1	Interpolation Enables Inexpensive Stochastic Updates
By definition, the interpolation setting gives f? = 0, which we used in ALI-G to simplify the formula
of the learning-rate γt . More subtly, the interpolation property also allows the updates to rely on the
stochastic estimate `zt (wt) rather than the exact but expensive f(wt). Intuitively, this is possible
because in the interpolation setting, each training sample can use its own learning-rate without
harming progress on the other ones. Recall that ALI-G∞ is the variant of ALI-G that uses no maximal
learning-rate. The following result formalizes the convergence guarantee of ALI-G∞ in the stochastic
convex setting:
Theorem 1 (Convex and Lipschitz). We assume that Ω is a convex set, and that for every Z ∈ Z, 'z
is convex and C -Lipschitz. Let w? be a solution of (P), and assume that the interpolation property
is approximately satisfied: ∀z ∈ Z, `z (w?) ≤ ε, for some interpolation tolerance ε ≥ 0. Then
ALI-G∞ applied to f satisfies:
f	τ+l X wt
t=0
C2
wo - w?k √C2 + δ
√T+1
(3)
In other words, by assuming interpolation, ALI-G provably converges while requiring only `zt (wt)
and V'zt (Wt) (stochastic estimation per sample) to compute its learning-rate. In contrast, the
Polyak step-size, which does not exploit interpolation, would require f (Wt) and Vf (Wt) to compute
the learning-rate (deterministic computation over all training samples). This constitutes a major
computational advantage of ALI-G over the usual Polyak step-size.
We emphasize that in Theorem 1, our careful analysis explicitly shows the dependency of the
convergence result on the interpolation tolerance ε. It is reassuring to note that convergence is exact
when the interpolation property is exactly satisfied (ε = 0).
In the supplementary material, we also establish convergence rates of O(1/T) for smooth convex
functions, and O(exp(-αT /8β)) for α-strongly convex and β-smooth functions. Similar results can
be proved when using a maximal learning-rate η: the convergence speed then remains unchanged
provided that η is large enough, and it is lowered when η is small. We refer the interested reader to
the supplementary for the formal results and their proofs.
3.2	A Maximal Learning-Rate Helps with Non-Convexity
The Polyak step-size may fail to converge when the objective is non-convex, as figure 2 illustrates: in
this (non-convex) setting, gradient descent with Polyak step-size oscillates between two symmetrical
points because its step-size is too large (details in the supplementary).
Indeed, we empirically find that non-convexity usually leads to overestimation of the Polyak step-size.
Intuitively, using a maximal learning-rate allows ALI-G to behave like constant step-size SGD in
non-convex regions where the Polyak step-size would be over-estimated, and to automatically use the
Polyak step-size once reaching a convex basin of convergence.
Importantly, using a maximal learning-rate can be seen as a very natural extension of SGD when
using a non-negative loss function:
4
Under review as a conference paper at ICLR 2020
Proposition 1. [Proximal Interpretation] Suppose that Ω = Rp and let δ = 0.We consider
the update performed by SGD: wS，D = Wt — nN % (Wt); and the update performed by ALI-G:
wa+⅛g = wt - YNCzt (Wt), Where Yt = min { JV⅛⅛⅛,n}
Then we have:
WSGD=argmpn n Nkw - wtk2+'zt (Wt) KJ (Wt)>(w - wt)o，
WA+1g = argmin { 21nIlW — Wtk2 + max {'zt (Wt) + N'zt (wt)>(w — Wt), 0} }
(4)
(5)
In other words, at each iteration, ALI-G solves a proxi-
mal problem in closed form in a similar way to SGD. In
both cases, the loss function `zt is locally approximated
by a first-order Taylor expansion at Wt . The difference
is that ALI-G also exploits the fact that `zt is non-
negative. This allows ALI-G to use a constant value for
η in the interpolation setting, while the learning-rate ηt
of SGD needs to be manually decayed.
It is currently an open question whether ALI-G can
be proved to converge in the stochastic non-convex
setting given a sufficiently small yet constant maximal
learning-rate η.
4	Related Work
Figure 2: A simple example where the
Polyak step-size oscillates due to non-
convexity. On this problem, ALI-G con-
verges whenever its maximal learning-rate
η is lower than 10.
Interpolation in Deep Learning. As mentioned in the introduction, recent works have successfully
exploited the interpolation assumption to prove convergence of SGD in the context of deep learning
(Ma et al., 2018, Vaswani et al., 2019a, Zhou et al., 2019). Such works are complementary to ours in
the sense that they provide a convergence analysis of an existing algorithm for deep learning.
Adaptive Gradient Methods. Similarly to ALI-G, adaptive gradient methods also rely on tuning
a single hyper-parameter, thereby providing a more pragmatic alternative to SGD that needs a
specification of the full learning-rate schedule. While the most popular ones are Adagrad (Duchi
et al., 2011), RMSPROP (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2015) and AMSGrad
(Reddi et al., 2018), there have been many other variants (Zeiler, 2012, Orabona & Pal, 2015, Defossez
& Bach, 2017, Levy, 2017, Mukkamala & Hein, 2017, Zheng & Kwok, 2017, Bernstein et al., 2018,
Chen & Gu, 2018, Shazeer & Stern, 2018, Zaheer et al., 2018, Chen et al., 2019, Loshchilov & Hutter,
2019, Luo et al., 2019). However, as pointed out in Wilson et al. (2017), adaptive gradient methods
tend to give poor generalization in supervised learning. In our experiments, the results provided by
ALI-G are significantly better than those obtained by the most popular adaptive gradient methods.
Adaptive Learning-Rate Algorithms. Vaswani et al. (2019b) show that one can use line search in
a stochastic setting for interpolating models while guaranteeing convergence. However, in contrast
to our work, the resulting algorithm requires more than one hyper-parameter (up to four), and the
line-search is not computed in closed form. Less closely related methods have proposed adaptive
learning-rates without using the minimum for the computation of the learning rate (Schaul et al., 2013,
Tan et al., 2016, Zhang et al., 2017, Baydin et al., 2018, Wu et al., 2018, Li & Orabona, 2019), but
they have not demonstrated competitiveness against SGD with a well-tuned hand-designed schedule.
L4 Algorithm. The L4 algorithm (Rolinek & Martius, 2018) also uses a modified version of the
Polyak step-size. However, the L4 algorithm computes an online estimate of f? rather than relying
on a fixed value. This requires three hyper-parameters, which are in practice sensitive to noise and
crucial for empirical convergence of the method. In addition, L4 does not come with convergence
guarantees. In contrast, by utilizing the interpolation property and a maximal learning-rate, our
method is able to (i) provide reliable and accurate minimization with only a single hyper-parameter,
and (ii) offer guarantees of convergence in the stochastic convex setting.
5
Under review as a conference paper at ICLR 2020
Frank-Wolfe Methods. The proximal interpretation in Proposition 1 allows us to draw additional
parallels to existing methods. In particular, the formula of the learning-rate γt may remind the reader
of the Frank-Wolfe algorithm (Frank & Wolfe, 1956) in some of its variants (Locatello et al., 2017),
or other dual methods (Lacoste-Julien et al., 2013, Shalev-Shwartz & Zhang, 2016). This is because
such methods solve in closed form the dual of problem (5), and problems in the form of (5) naturally
appear in dual coordinate ascent methods (Shalev-Shwartz & Zhang, 2016).
When no regularization is used, ALI-G and Deep Frank-Wolfe (DFW) (Berrada et al., 2019) are
procedurally identical algorithms. This is because in such a setting, one iteration of DFW also
amounts to solving (5) in closed-form - more generally, DFW is designed to train deep neural
networks by solving proximal linear support vector machine problems approximately. However,
we point out the two fundamental advantages of ALI-G over DFW: (i) ALI-G can handle arbitrary
(lower-bounded) loss functions, while DFW can only use convex piece-wise linear loss functions;
and (ii) as seen previously, ALI-G provides convergence guarantees in the convex setting.
SGD with Polyak’s Learning-Rate. Oberman & Prazeres (2019) extend the Polyak step-size to
rely on a stochastic estimation of the gradient V'zt (Wt) only, instead of the expensive deterministic
gradient Vf (wt). However, they still require to evaluate f (Wt), the objective function over the
entire training data set, in order to compute its learning-rate, which makes the method impractical. In
addition, since they do not do exploit the interpolation setting nor the fact that regularization can be
expressed as a constraint, they also require the optimal objective function value f? in advance.
APROX Algorithm. Asi & Duchi (2019) have recently introduced the APROX algorithm, a family
of proximal stochastic optimization algorithms for convex problems. Notably, the aProx “truncated
model” version is similar to ALI-G. However, there are four clear advantages of our work over (Asi
& Duchi, 2019) in the interpolation setting, in particular for training neural networks. First, our work
is the first to empirically demonstrate the applicability and usefulness of the algorithm on varied
modern deep learning tasks - most of our experiments use several orders of magnitude more data
and model parameters than the small-scale convex problems of (Asi & Duchi, 2019). Second, our
analysis and insights allow us to make more aggressive choices of learning rate than (Asi & Duchi,
2019). Indeed, Asi & Duchi (2019) assume that the maximal learning-rate is exponentially decaying,
even in the interpolating convex setting. In contrast, by avoiding the need for an exponential decay,
the learning-rate of ALI-G requires only one hyper-parameters instead of two for aProx. Third,
our analysis takes into account the interpolation tolerance ε ≥ 0 rather than unrealistically assuming
the perfect case ε = 0 (that would require infinite weights when using the cross-entropy loss for
instance). Fourth, our analysis proves fast convergence in function space rather than iterate space.
5	Experiments
We empirically compare ALI-G to the optimization algorithms most commonly used in deep learning.
Our experiments span a variety of architectures and tasks: (i) learning a differentiable neural computer;
(ii) training wide residual networks on SVHN; (iii) training a Bi-LSTM on the Stanford Natural
Language Inference data set; and (iv) training wide residual networks and densely connected networks
on the CIFAR data sets. Note that the tasks of training wide residual networks on SVHN and CIFAR-
100 are part of the DeepOBS benchmark Schneider et al. (2019), which aims at standardizing
baselines for deep learning optimizers. In particular, these tasks are among the most difficult ones
of the benchmark because the SGD baseline benefits from a manual schedule for the learning rate.
Despite this, ALI-G obtains competitive performance with SGD. In addition, ALI-G is the best
performing method with a single hyper-parameter on the difficult tasks of Bi-LSTM on SNLI and
ResNet variants on CIFAR.
The code to reproduce our results will be made publicly available. In the Tensorflow (Abadi et al.,
2015) experiment, we use the official and publicly available implementation of L41. In the PyTorch
(Paszke et al., 2017) experiments, we use our implementation of L4, which we unit-test against the
official Tensorflow implementation. In addition, we employ the official implementation of DFW2 and
1 https://github.com/martius- lab/l4- optimizer
2https://github.com/oval-group/dfw
6
Under review as a conference paper at ICLR 2020
we re-use their code for the experiments on SNLI and CIFAR. All experiments are performed either
on a 12-core CPU (differentiable neural computer) or on a single GPU (SVHN, SNLI, CIFAR).
5.1	Differentiable Neural Computers
Setting. The Differentiable Neural Computer (DNC) (Graves et al., 2016) is a recurrent neural
network that aims at performing computing tasks by learning from examples rather than by executing
an explicit program. In this case, the DNC learns to repeatedly copy a fixed size string given as input.
Although this learning task is relatively simple, the complex architecture of the DNC makes it an
interesting benchmark problem for optimization algorithms.
Methods. We use the official and publicly available implementation of DNC3. We vary the initial
learning rate as powers of ten between 10-4 and 104 for each method except for L4Adam and
L4Mom. For L4Adam and L4Mom, since the main hyper-parameter α is designed to lie in (0, 1),
we vary it between 0.05 and 0.095 with a step of 0.1. The gradient norm is clipped for all methods
except for ALI-G, L4Adam and L4Mom (as recommended by Rolinek & Martius (2018)).
L4Adam
L4Mom
AdaGrad
Adam
RMSProp
SGD
SGD w/ Mom
ALI-G
Main Step-Size Hyper-Parameter (α):
Maximal Learning-Rate (η):
1e-1	3e-1	1e+0	3e+0	1e+1	3e+1	1e+2	1e+3	1e+4	1e+5	1e+6	1e+7
Figure 3: Final objective function when training a Differentiable Neural Computer for 10k steps
(lower is better). The intensity of each cell is log-proportional to the value of the objective function
(darker is better). ALI-G obtains good performance for a very large range of η (10-1 ≤ η ≤ 106).
Results. We present the results in Figure 3. ALI-G provides accurate optimization for any η within
[10-1, 106], and is among the best performing methods by reaching an objective function of 4.10-8.
On this task, RMSProp, L4Adam and L4Mom also provide accurate and robust optimization. In
contrast to ALI-G and the L4 methods, the most commonly used algorithms such as SGD, SGD with
momentum and Adam are very sensitive to their main learning-rate hyper-parameter. Note that the
difference between well-performing methods is not significant here because these reach the numerical
precision limit of single-precision float numbers.
5.2	Wide Residual Networks on SVHN
Setting. The SVHN data set contains 73k training samples, 26k testing samples and 531k additional
easier samples. From the 73k difficult training examples, we select 6k samples for validation; we use
all remaining (both difficult and easy) examples for training, for a total of 598k samples. We train a
wide residual network 16-4 following Zagoruyko & Komodakis (2016).
Method. For SGD, we use the manual schedule for the learning rate of Zagoruyko & Komodakis
(2016). For L4Adam and L4Mom, we cross-validate the main learning-rate hyper-parameter α to
be in {0.0015, 0.015, 0.15} (0.15 is the value recommended by Rolinek & Martius (2018)). For
other methods, the learning rate hyper-parameter is tuned as a power of 10. The L2 regularization
is cross-validated in {0.0001, 0.0005} for all methods but ALI-G. For ALI-G, the regularization is
expressed as a constraint on the L2-norm of the parameters, and its maximal value is set to 50. SGD,
3https://github.com/deepmind/dnc
7
Under review as a conference paper at ICLR 2020
ALI-G and BPGrad use a Nesterov momentum of 0.9. All methods use a dropout rate of 0.4 and a
fixed budget of 160 epochs, following (Zagoruyko & Komodakis, 2016).
Results. The results are presented in Table 1. On this relatively easy task, most methods achieve
about 98% test accuracy. Despite the cross-validation, L4Mom does not converge on this task. Even
though SGD benefits from a hand-designed schedule, ALI-G and other adaptive methods obtain close
performance to it.
	W		B		
98.0	97.9	97.9 98.1	98.1	98.2	19.6 98.1	∣	98.3	98.4
Table 1: Test Accuracy (%) on SVHN. In red, SGD benefits from a hand-designed schedule for its
learning-rate. In black, adaptive methods, including ALI-G, have a single hyper-parameter for their
learning-rate. SGDt refers to the performance reported by Zagoruyko & Komodakis (2016).
5.3	BI-LSTM ON SNLI
Setting. We train a Bi-LSTM of 47M parameters on the Stanford Natural Language Inference
(SNLI) data set (Bowman et al., 2015). The SNLI data set consists in 570k pairs of sentences, with
each pair labeled as entailment, neutral or contradiction. This large scale data set is commonly used
as a pre-training corpus for transfer learning to many other natural language tasks where labeled data
is scarcer (ConneaU et al., 2017) - much like ImageNet is used for pre-training in computer vision.
We follow the protocol of Berrada et al. (2019); we also re-use their code and results for the baselines.
Method. For L4Adam and L4Mom, the main hyper-parameter α is cross-validated in {0.015, 0.15}
- compared to the recommended value of 0.15, this helped convergence and considerably improved
performance. The SGD algorithm benefits from a hand-designed schedule, where the learning-rate is
decreased by 5 when the validation accuracy does not improve. Other methods use adaptive learning-
rates and do not require such schedule. The value of the main hyper-parameter η is cross-validated as
a power of ten for the ALI-G algorithm and for previously reported adaptive methods. Following the
implementation by Conneau et al. (2017), no L2 regularization is used. The algorithms are evaluated
with the Cross-Entropy (CE) loss and the multi-class hinge loss (SVM), except for DFW which
is designed for SVMs only. For all optimization algorithms, the model is trained for 20 epochs,
following (Conneau et al., 2017).
Loss	W	§ *		备	§ 7*	—"2		Q*	
CE	83.8	84.5	84.2	83.6	-	83.3	83.7	84.6 84.8	84.7	84.5
SVM	84.6	85.0	85.1	84.2	85.2	82.5	83.2 84.7 85.2	85.2	-
Table 2: Test Accuracy (%) on SNLI. In red, SGD benefits from a hand-designed schedule for its
learning-rate. In black, adaptive methods have a single hyper-parameter for their learning-rate. In
blue, ALI-G∞ does not have any hyper-parameter for its learning-rate. With an SVM loss, DFW and
ALI-G are procedurally identical algorithms - but in contrast to DFW, ALI-G can also employ the
CE loss. Methods in the format X* re-use results from Berrada et al. (2019). SGDt is the result
from Conneau et al. (2017).
Results. We present the results in Table 2. ALI-G∞ is the only method that requires no hyper-
parameter for its learning-rate. Despite this, and the fact that SGD employs a learning-rate schedule
that has been hand designed for good validation performance, ALI-G∞ is still able to obtain results
that are competitive with SGD. Moreover, ALI-G, which requires a single hyper-parameter for the
learning-rate, outperforms all other methods for both the SVM and the CE loss functions.
8
Under review as a conference paper at ICLR 2020
5.4	Wide Residual Networks and Densely Connected Networks on CIFAR
Setting. We follow the methodology of Berrada et al. (2019); we also re-use their code and we
reproduce their results. We test two architectures: a Wide Residual Network (WRN) 40-4 (Zagoruyko
& Komodakis, 2016) and a bottleneck DenseNet (DN) 40-40 (Huang et al., 2017). We use 45k
samples for training and 5k for validation. The images are centered and normalized per channel. We
apply standard data augmentation with random horizontal flipping and random crops. AMSGrad
was selected in Berrada et al. (2019) because it was the best adaptive method on similar tasks,
outperforming in particular Adam and Adagrad. In addition to the baselines from Berrada et al.
(2019), we also provide the performance of L4Adam, L4Mom, AdamW (Loshchilov & Hutter, 2019)
and Yogi (Zaheer et al., 2018).
Method. All optimization methods employ the cross-entropy loss, except for the DFW algorithm,
which is designed to use an SVM loss. For DN and WRN respectively, SGD uses the manual
learning rate schedules from Huang et al. (2017) and Zagoruyko & Komodakis (2016). Following
Berrada et al. (2019), the batch-size is cross-validated in {64, 128, 256} for the DN architecture,
and {128, 256, 512} for the WRN architecture. For L4Adam and L4Mom, the learning-rate hyper-
parameter α is cross-validated in {0.015, 0.15}. For AMSGrad, AdamW, Yogi, DFW and ALI-G,
the learning-rate hyper-parameter η is cross-validated as a power of 10 (in practice η ∈ {0.1, 1} for
ALI-G). SGD, DFW and ALI-G use a Nesterov momentum of 0.9. Following Berrada et al. (2019),
for all methods but ALI-G and AdamW, the L2 regularization is cross-validated in {0.0001, 0.0005}
on the WRN architecture, and is set to 0.0001 for the DN architecture. For AdamW, the weight-decay
is cross-validated as a power of 10. For ALI-G, L2 regularization is expressed as a constraint on the
norm on the vector of parameters; its maximal value is set to 100 for the WRN models, 80 for DN on
CIFAR-10 and 75 for DN on CIFAR-100. For all optimization algorithms, the WRN model is trained
for 200 epochs and the DN model for 300 epochs, following respectively (Zagoruyko & Komodakis,
2016) and (Huang et al., 2017).
Results. We present the results in Table 3. In this setting again, ALI-G obtains competitive
performance with manually decayed SGD. ALI-G largely outperforms AMSGrad, AdamW and Yogi.
In addition, it significantly bridges the gap between DFW and SGD on CIFAR-10 with the WRN
model, and on CIFAR-100 with the DN one.
Data Set	Architecture			Q . q。0	S					
CIFAR-10	WRN	90.8	92.1	91.2	94.2	90.5	91.6	95.2	95.3	95.4
	DN	91.7	92.6	92.1	94.6	90.8	91.9	95.0	95.1	-
CIFAR-100	WRN	68.7	69.6	68.7	76.0	61.7	61.4	75.8	77.8	78.8
	DN	69.4	69.5	69.6	73.2	60.5	62.6	76.3	76.3	-
Table 3: Test Accuracy (%) on the CIFAR data sets. In red, SGD benefits from a hand-designed
schedule for its learning-rate. In black, adaptive methods, including ALI-G, have a single hyper-
Parameterfor their learning-rate. SGDt refers to the result from Zagoruyko & Komodakis (2016).
Each reported result is an average over three independent runs; the standard deviations are reported
in APPendix (they are at most 0.3 for ALI-G and SGD).
5.5	Training Performance
The experiments have so far focused on testing accuracy (except for the DNC task), because that is the
main metric driving practitioners’ choice of optimization algorithm. In this section, we empirically
assess the performance of ALI-G and its competitors in terms of training objective. In order to have
comparable objective functions, the L2 regularization is deactivated. We do not use dropout. The
learning-rate is selected as a power of ten for best final objective value, and the batch-size is set to its
default value. All methods use a fix budget of 160 epochs for WRN-SVHN, 200 epochs for WRN-
CIFAR and 300 epochs for DN-CIFAR, following (Zagoruyko & Komodakis, 2016) and (Huang
et al., 2017). The L4 methods diverge on CIFAR-100 in this setting. For clarity, we only display the
9
Under review as a conference paper at ICLR 2020
performance of SGD, Adam, Adagrad and ALI-G (DFW does not support the cross-entropy loss).
Here SGD uses a constant learning-rate to emphasize the need for adaptivity. Therefore all methods
use one hyper-parameter for their learning-rate. As can be seen, ALI-G provides better training
performance than the baseline algorithms on all tasks.
WRN-CIFAR10
DN-CIFAR10
10-1
10-2
10-3
10-4
10-5
0	80	160
DN-CIFAR100
---Adagrad
---Adam
——SGD
——ALI-G
Figure 4: Objective function over the epochs on CIFAR-10, CIFAR-100 and SVHN (smoothed with a
moving average over 5 epochs). On SVHN, ALI-G obtains similar performance to its competitors
and converges faster. On CIFAR-10 and CIFAR-100, which are more difficult tasks, ALI-G yields an
objective function that is an order of magnitude better than the baselines.
6	Discussion
We hope that the ALI-G algorithm is a helpful step towards efficient and reliable training of deep
neural networks. ALI-G is readily applicable to a broad range of applications in deep learning where
the model can interpolate the data. When that is not the case however, it would be interesting to
design new algorithms that adapt the minimum f? online while requiring few hyper-parameters. This
could be achieved for instance by building upon the works of Nedic & Bertsekas (2001b) and Rolinek
& Martius (2018).
References
MarHn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, GregS.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelione Man, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernandaegas Vi, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on
Heterogeneous Systems, 2015. Software available from tensorflow.org.
Hilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Convergence,
optimality, and adaptivity. SIAM Journal on Optimization, 2019.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood.
Online learning rate adaptation with hypergradient descent. International Conference on Learning
Representations, 2018.
10
Under review as a conference paper at ICLR 2020
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd:
Compressed optimisation for non-convex problems. International Conference on Machine Learn-
ing, 2018.
Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Deep Frank-Wolfe for neural network
optimization. International Conference on Learning Representations, 2019.
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated
corpus for learning natural language inference. Conference on Empirical Methods in Natural
Language Processing, 2015.
Ulf Brannlund. A generalized subgradient method with relaxation step. Mathematical Programming,
1995.
SebaStien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in
Machine Learning, 2015.
Jinghui Chen and Quanquan Gu. Padam: Closing the generalization gap of adaptive gradient methods
in training deep neural networks. arXiv preprint, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. International Conference on Learning Representations,
2019.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised
learning of universal sentence representations from natural language inference data. Conference
on Empirical Methods in Natural Language Processing, 2017.
Alexandre Defossez and Francis Bach. Adabatch: Efficient gradient aggregation rules for sequential
and parallel stochastic gradient methods. arXiv preprint, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 2011.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research
Logistics Quarterly, 1956.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
BarWinska, Sergio G6mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
Hybrid computing using a neural network with dynamic external memory. Nature, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. Conference on Computer Vision and Pattern Recognition, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, and Patrick Pletscher. Block-coordinate Frank-
Wolfe optimization for structural SVMs. International Conference on Machine Learning, 2013.
Kfir Levy. Online to offline conversions, universality and adaptive minibatch sizes. Neural Information
Processing Systems, 2017.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. International Conference on Artificial Intelligence and Statistics, 2019.
Francesco Locatello, Rajiv Khanna, Michael Tschannen, and Martin Jaggi. A unified optimization
view on generalized matching pursuit and frank-wolfe. International Conference on Artificial
Intelligence and Statistics, 2017.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. International
Conference on Learning Representations, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. International Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2020
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. International Conference on Machine
Learning, 2018.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of rmsprop and adagrad with logarithmic
regret bounds. International Conference on Machine Learning, 2017.
Angelia Nedic and Dimitri Bertsekas. Convergence rate of incremental subgradient algorithms.
Stochastic optimization: algorithms and applications, 2001a.
Angelia Nedic and Dimitri Bertsekas. Incremental subgradient methods for nondifferentiable opti-
mization. SIAM Journal on Optimization, 2001b.
Adam M Oberman and Mariana Prazeres. Stochastic gradient descent with polyak’s learning rate.
arXiv preprint, 2019.
Francesco Orabona and David Pal. Scale-free algorithms for online linear optimization. International
Conference on Algorithmic Learning Theory, 2015.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. NIPS Autodiff Workshop, 2017.
Boris Teodorovich Polyak. Minimization of unsmooth functionals. USSR Computational Mathematics
and Mathematical Physics, 1969.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond.
International Conference on Learning Representations, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, 1951.
Michal Rolinek and Georg Martius. L4: Practical loss-based stepsize adaptation for deep learning.
Neural Information Processing Systems, 2018.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. International Conference
on Machine Learning, 2013.
Frank Schneider, Lukas Balles, and Philipp Hennig. DeepOBS: A deep learning optimizer benchmark
suite. International Conference on Learning Representations, 2019.
Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. Mathematical Programming, 2016.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
International Conference on Machine Learning, 2018.
Naum Zuselevich Shor. Minimization methods for non-differentiable functions. Springer Series in
Computational Mathematics, 1985.
Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian. Barzilai-borwein step size for stochastic
gradient descent. Neural Information Processing Systems, 2016.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. International Conference on Artificial
Intelligence and Statistics, 2019a.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. arXiv
preprint, 2019b.
12
Under review as a conference paper at ICLR 2020
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. Neural Information Processing Systems,
2017.
Xiaoxia Wu, Rachel Ward, and Leon Bottou. WNGrad: Learn the learning rate in gradient descent.
arXiv preprint, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Confer-
ence, 2016.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. Neural Information Processing Systems, 2018.
Matthew Zeiler. ADADELTA: an adaptive learning rate method. arXiv preprint, 2012.
Ziming Zhang, Yuanwei Wu, and Guanghui Wang. Bpgrad: Towards global optimality in deep
learning via branch and pruning. Conference on Computer Vision and Pattern Recognition, 2017.
Shuai Zheng and James T Kwok. Follow the moving leader in deep learning. International Conference
on Machine Learning, 2017.
Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. Sgd converges to
global minimum in deep learning via star-convex path. International Conference on Learning
Representations, 2019.
13
Under review as a conference paper at ICLR 2020
A Local Interpretation of the Polyak Step-Size
Proposition 2. Suppose that the problem is unconstrained: Ω = Rp. Let wt+ι = Wt —
kfWtWf2 Vf(Wt). Then wt+ι VerifieS：
Wt+1 = argmin ∣∣w — Wtk SUbjeCt to: f(wt) + Vf(Wt)>(w — Wt) = f?,	(6)
w∈Rp
where we remind that f? is the minimum of f, and W 7→ f(Wt) + Vf (Wt)> (W — Wt) is the
linearization of f at Wt. In other words, Wt+1 is the closest point to Wt that lies on the hyper-plane
f(Wt) + Vf(Wt)>(W — Wt) = f?.
Proof : See Appendix D.1
B Convergence Results
Before we detail our convergence results, we introduce the notions of uniform lower bound and
ε-interpolation.
B.1 Notation
Intuitively, a uniform lower bound on the problem (P) is a lower bound on all loss functions `z on
their unconstrained domain Rp . We formalize this below:
Definition 1 (Uniform Lower Bound). We say that B is a uniform lower bound on (P) if:
B ≤ inf inf 'z(w).	(7)
----z∈Z w∈Rp ' ，
Note that in the main paper, we have used the special case B = 0 as the uniform lower bound. The
definition above makes B a useful statistic to analyze the behavior of each loss function 'z around
w?, in a uniform way (that is, independently of z). The quality of a uniform lower bound B can be
quantified by the notion of ε-interpolation:
Definition 2 (ε-Interpolation). Let B be a uniform lower bound on (P), w? be a solution of(P) and
ε ≥ 0 be a non-negative number Then we say that w? is an ε-interpolationfor((P), B) if:
∀z ∈ Z, 'z(w?) — B ≤ ε.
(8)
By taking the expectation over equation (8), we can see that if w? is an ε-interpolation for ((P), B),
then we immediately have: f? — ε ≤ B ≤ f?. In other words, B is also an approximation of f? by
below, and its quality is quantified by ε. We further note that f? does not satisfy the definition of a
uniform lower bound in the general case. However when f? actually is a uniform lower bound, for
any solution w? of (P), w? is a ε = 0-interpolation for ((P), f?).
In the general case where B may be different from 0, the ALI-G step-size can be defined as:
γt = min
'zt (Wt) — B
kV'zt (Wt)k2 + δ ,η
(9)
We now turn to our convergence results, which give convergence rates in three settings: convex and
Lipschitz functions, convex and smooth function, and strongly convex and smooth functions. In each
setting, we analyze three regimes which complement each other: no (infinite) maximal learning-rate,
large maximal learning-rate and small maximal learning-rate.
B.2 Lipschitz Convex Functions
Theorem 2.	[Convex and Lipschitz] We assume that X is a convex set, and that for every z ∈ Z,
'z is convex and C-Lipschitz. Let B be a uniform lower bound on (P) and w? be a solution of(P).
Further suppose that w? is an ε-interpolationfor((P), B). Then ALI-G∞ applied to f satisfies:

f
T
X wt
t=0
— f? ≤ ε
W0 - w?k√C2 + δ
√T+1
(10)
14
Under review as a conference paper at ICLR 2020
Proof : See Appendix D.3.
Theorem 3.	We assume that X is a convex set, and that for every z ∈ Z, `z is convex and C -Lipschitz.
Let w? be an ε-interpolation for ((P), B). Wefurther assume that η > δ. Then if we apply ALI-G
with a maximal learning-rate of η to f, we have:
11 1 X ! f k Iiwo - w?k2 ,	ε2	, S (C 2+δ)kwo—w?k2, r C2, ι
f(τ+1 ti t)-f* ≤ (η - ε )(T + 1) + W-II+ V	T+^ɪ	+ 川方 +
(11)
Proof : See Appendix D.4.
We note that for very large values of η (η → ∞), Theorem 3 gives the exact same result as Theorem
2. However when η is small, the convergence error of Theorem 3 is large. This is corrected in the
following result which is informative in the regime where η is small:
Theorem 4.	We assume that X is a convex set, and that for every z ∈ Z, `z is convex and C -Lipschitz.
Let w? be an ε-interpolationfor ((P), B). Then if we apply ALI-G with a maximal learning-rate of η
to f, we have:
f lτ+x X wt) - f? ≤kW大 + ε +「J— + ηεK. (12)
Proof : See Appendix D.5.
B.3 Smooth Convex Functions
We now tackle the convex and β-smooth case. Our proof techniques naturally produce the separation
η ≥ 2β and η ≤ 2β. Whenever η ≥ ɪ, the convergence result is exactly the same as when η → ∞.
When η ≤ 2β, the speed of convergence is limited by the value of η.
Theorem 5.	[Convex and Smooth] We assume that X is a convex set, and that for every z ∈ Z, `z is
convex and β-smooth. Let B be a uniform lower bound on (P) and w? be a solution of(P). Further
suppose that w? is an ε-interpolation for ((P), B), and that δ > 2βε. Then ALI-G∞ applied to f
satisfies:
3
f
,	δ	2β	Jwo - w?J2
f? ≤ β(ι - 2βε)+τ-2βε	T+ι
(13)
Proof : See Appendix D.6.
Theorem 6.	We assume that X is a convex set, and that for every z ∈ Z, `z is convex and β-smooth.
Let w? be an ε-interpolationfor ((P), B), and suppose that δ > 2βε. Further assume that η ≥ ɪ.
Then if we apply ALI-G with a maximal learning-rate of η to f, we have:
(τ⅛
f
_ , < δ	2β	Jwo - w?J2
- f? ≤ β(1 - 竿)+ T-2βε	T + 1
(14)
Proof : See Appendix D.7.
Theorem 7.	We assume that X is a convex set, and that for every z ∈ Z, `z is convex and β-smooth.
Let w? be an ε-interpolationfor ((P), B), and suppose that δ > 2βε. Further assume that η ≤ ɪ.
Then if we apply ALI-G with a maximal learning-rate of η to f, we have:
J 1	「ʌ	Jwo - w?J2	δ
f (τ∏∑0wt) -f? ≤	η(τ + 1) +2β +ε
(15)
Proof : See Appendix D.8.
15
Under review as a conference paper at ICLR 2020
B.4 Smooth and Strongly Convex Functions
Finally, we consider the α-strongly convex and β-smooth case. Again, our proof yields a natural
separation between η ≥ 表 and η ≤ 表.In a similar way to the β-smooth case, when η ≥ 泰,
Theorem 9 gives the exact same result as η → ∞. And when η ≤ ɪ, the rate of convergence given
by Theorem 10 is limited by the value of η.
Theorem 8.	[Strongly Convex and Smooth] We assume that X is a convex set, and that for every
Z ∈ Z, 'z is α-strongly ConVex and β-smooth. Let B be a uniform lower bound on (P) and w? be a
solution of(P). Further suppose that w? is an ε-interpolation for ((P), B), and that δ > 2βε. Then
ALI-G∞ applied to f satisfies:
f (WT+ι)- f? ≤β exp (-αT) kw0 - w*k2+2δ+(10 α+4 4α)ε (16)
In other words, f approximately converges to f? at a rate of O(exp(一αT∕8β)).
Proof : See Appendix D.9.
Theorem 9.	We assume that X is a convex set, and that for every z ∈ Z, `z is α-strongly convex
and β-smooth. Let w? be an ε-interPOlationfor ((P), B), and suppose that δ > 2βε. Further assume
that η ≥ 21g. Then ifwe applyALI-G with a maximal learning-rate of η to f, we have:
f (WT+i) 一f? ≤β exp (一 αT) kw0 一 w*k2+2δ+(l0 α+4 β2)ε (17)
Proof: See Appendix D.10.
Theorem 10.	We assume that X is a convex set, and that for every z ∈ Z, `z is α-strongly convex
and β-smooth. Let w? be an ε-interpolationfor ((P), B), and suppose that δ > 2βε. Further assume
that η ≤ 21g. Then ifwe applyALI-G with a maximal learning-rate of η to f, we have:
f(wT+1) 一 f? ≤ βexp
2	2δ	14εβ
kw0- w*k + Z + 丁.
(18)
Proof: See Appendix D.11.
C On the Need for a Maximal Learning-Rate for Non-Convexity
The Restricted Secant Inequality (RSI) is a milder assumption than convexity. It can be defined as
follows:
Definition 3. Let f : Rp → R be a lower-bounded differentiable function achieving its minimum at
w?. We say that f satisfies the RSI if there exists α > 0 such that:
∀w ∈ Rp, Vf(w)>(w — w?) ≥ α∣∣w — w*∣∣2.	(19)
The RSI is sometimes used to prove convergence of optimization algorithms without assuming
convexity (Vaswani et al., 2019b).
As we prove below, the Polyak step-size may not convergence under the RSI assumption, even in a
non-stochastic setting with the exact minimum known.
We introduce the function f : W ∈ [-53; 5] → w2 一 |w|3. We restrict our domain of study to [-3; 3]
for simplicity purposes - an extension to R can easily be constructed by extending f. We will first
show that f fulfills the RSI assumption, and then that it oscillates between two points for a well
chosen initialization.
Let us show that f satisfies the RSI with α = 5. FirSt we note that f achieves its minimum at w? = 0,
and that f(w?) = 0. In addition, we introduce the sign function σ(w), which is equal to 1 if w ≥ 0,
16
Under review as a conference paper at ICLR 2020
Figure 5: Illustration of the function f, which satisfies the RSI. When starting at w = -3/5, gradient
descent with the Polyak step-size oscillates between w = -3/5 and w = 3/5.
and -1 otherwise. Now let W ∈ [-3; ∣]. Then We have that:
Vf (w)(w — w?) — ɪ(w — w?)2, = (2w — 3σ(w)w2)(w — 0) — ɪ(w — 0)2,
=；w2 — 3σ(w)w3,
=3w2(— — σ(w)w),
5
≥ 0.
(20)
Now let us show that if we apply gradient descent with a Polyak step-size to f, with starting point
wo = -3, we obtain wι = ∣. This will prove oscillation of the iterates by symmetry of the problem.
Let wo = -5∣. Then we have f (wo) = 2∣  -27∣ =接.Furthermore, Vf (wo) = 2(-∣) + 3(29∣)=
-∣. Therefore:
f (wo) — 0
w1 = wo — (Vf(Wo))2Vf(W。)，
f(wo)
wo —
—3
7 十
—3
V + 5,
3
.
5
Vf(wo),
18
T2∣
"ɪ，
2∣
6
(21)
D Proofs
D.1 Proposition 2
Proposition 2. Suppose that the problem is unconstrained: Ω = Rp. Let wt+1 = Wt —
f⅞⅛⅜Vf(Wt). Then wt+1 Verfies:
Wt+1 = argmin kW — Wtk subject to: f(Wt) + Vf(Wt)>(W — Wt) = f?,	(6)
w∈Rp
where we remind that f? is the minimum of f, and W 7→ f(Wt) + Vf(Wt)>(W — Wt) is the
linearization of f at Wt. In other words, Wt+1 is the closest point to Wt that lies on the hyper-plane
f(Wt) + Vf(Wt)> (W — Wt) = f?.
Proof : First we show that wt+1 satisfies the linear equality constraint:
17
Under review as a conference paper at ICLR 2020
f(wt) + Vf (wt)>(wt+1 - Wt)
=f (Wt)+vf (Wt)> (-fVw⅛> Vf(Wt)),
=f (wt) - f(wt)+ f?
=f?.
(22)
Now let us show that it has a minimal distance to Wt .
We take W ∈ Rp a solution of the linear equality constraint, and We will show that kwt+ι 一 Wtk ≤
k W — Wt k. By definition, We have that W satisfies:
	f (Wt) + Vf(Wt)> (W - Wt) = f?	(23)
Now we can write:	kWt+1 - Wtk = kfV⅛⅛Vf(Wt)k, 二 f (Wt) - f? =kVf(Wt)k , _ ∣Vf(Wt)>(W - Wt)|	(24) =kVf(Wt)k, ≤ ||Vf(IWtf(W)- Wtk, (CaUChy-SChWarZ) =IIW - Wtk.
D.2 Theorem 1
Proposition 1. [Proximal Interpretation] Suppose that Ω = Rp and let δ = 0.We consider
the update performed by SGD: wS+D = Wt — ηtN'zt (Wt); and the update performed by ALI-G:
wA+I-G = Wt — YtN'zt (Wt), where Yt = min { g`'z(Wwt)2+δ ,η} ∙ Then we have:
wtS+GD1 = argmin
w∈Rp
(4)
wA+1G = argmin 1 1- l∣w - wtk2 + max {'zt (Wt) + NQzt (wt)>(w - wt), 0} 0.
w∈Rp	2η
(5)
Proof: We tackle the slightly more general case where B may be different from zero. In order to make the
notation simpler, we use dt，V'zt (Wt) and lt，'zt (Wt) — B.
First, let us consider dt = 0.
Then we choose Yt = 0 and it is clear that Wt+ι = Wt — ηγtdt = Wt is the optimal solution of problem
(5).
We now assume dt 6= 0.
We can successively re-write the proximal problem (5) as :
	min 1 — kW - Wtk2 + max ['z/Wt) + V'ζt (Wt)>(W - Wt), Bo ∖ , w∈Rp [ 2η	I min {金kW - Wtk2 + max {'zt (Wt) - B + V'zt (w∕>(w - Wt), 0o}, min {金 kw - Wtk2 + max [lt + d> (w - Wt), 0o}, min < -kw — Wtk2 + Ul subject to: U ≥ 0, U ≥ lt + d>(w — Wt)
18
Under review as a conference paper at ICLR 2020
min SuP i w∈Rp,υ μ,ν≥0 I	-∣w — Wt 12 + υ — μυ — V(U — It — d> (w — Wt)) ʃ
SuP min < ——∣∣w — wt∣∣2 + U — μυ — V(U — Lt — d>(w — wt)) > (strong duality) (25)
μ,ν≥o w∈Rp,υ I 2η	J
The inner problem is now smooth in W and υ. We write its KKT conditions:
	∂∙ —=0:	1 — μ — V = 0	(26) ∂ ∙	1 ,	.	_ ∂W = 0 ：	η(w — Wt) + Vdt = 0	(27)
We plug in these results and obtain:
	SuP 1 ɪIInVdt∣2 + V(It + d>(—η"dt)) > μ,ν≥01 2n	J st: μ + V = 1 suP n nV 2∣∣dt∣∣2 + Vlt — nV2∣d> ∣20 ν∈[0,1] I 2	J Sup n — nV2∣dt∣2 + Vlto	(28) ν∈[0,1] I 2	J
This is a one-dimensional quadratic problem in V. It can be solved in closed-form by finding the global
maximum of the quadratic objective, and projecting the solution on [0,1]. We have:
	∂∙	一	.. . ..9	,	一	一. ∂V = 0 ： —nV∣dtk + lt = 0	(29)
Since dt = 0 and η = 0, this gives the optimal solution:
	V=min{max{6,0} ,1}=min{6,1},	(30)
since lt,η, ∣dt∣2 ≥ 0.
Plugging this back in the KKT conditions, we obtain that the solution wt+ι of the primal problem can be
written as:	Wt+1 = Wt — nVdt, =Wt-n min{ n∣⅛ ,1}dt， =Wt—n min (⅛⅛⅛⅛, 1} v'zt (Wt),	(31) =Wt-min {⅛⅛⅛⅛ ,n}v'zt (Wt).
D.3 Theorem 2
Theorem 2. [Convex and Lipschitz] We assume that X is a convex set, and that for every z ∈ Z,
'z is convex and C-Lipschitz. Let B be a uniform lower bound on (P) and w? be a solution of(P).
Further suppose that w? is an ε-interPolationfor ((P), B). Then ALI-G∞ applied to f satisfies:
f(τ+ιX W) - f? ≤ε
W0 - w?k VZC2 + δ
√T+1
(10)
Proof:
We consider the update at time t, which we condition on the draw of Zt ∈ Z:
∣∣Wt + 1 — W?k2
19
Under review as a conference paper at ICLR 2020
=k∏Ω(wt - Yt▽'% (Wt)) - w*∣∣2
≤ ∣∣wt - 7tV'zt (wt) - w?k2	(∏Ω projection)
=IIWt - w*∣2 - 2γtV'zt(wt)>(wt - w?) + Y2∣∣V'zt(wt)∣∣2
=Ilwt- w?I2 - 2%v'zt (Wt)T(Wt- w?)+ Yt kVzt ((W))-2 B+δ ∣∣v'zt (Wt) I2
(definition of Yt)
≤ Ilwt - w?『-2Ytv'zt (Wt)T(Wt - w?) + Yt ∣:v'w2- B2 llv'zt (Wt)k2
llv'zt (Wt)Il
(because 'zt (Wt) — B ≥ 0 and δ ≥ 0)
≤ ∣∣wt - w?k2 - 2Yt('zt (wt) - 'zt (w?)) + Yt('zt (wt) - B)	(convexity of '%)
=IlWt - w*ll2 - Yt(('zt(Wt) - 'zt (w?)) - Gt (w?) - B))
=Iwt - w?『-∣∣v'zt(Wt)Il2 + δ (Bt(wt) -B)Czt(wt) -'zt(W?))
-('zt (Wt) - B)('zt (W?) - B))	(definition of Yt)
=Ilwt - w?『-nv`z (W±)∣∣2 + δ ((Czt(Wt) - 'zt(w?))('zt (Wt) - 'zt(w?))
+ (Czt (W?) - B)('zt (Wt) - Czt (W?)) - ('zt (Wt) - Czt (W?))('zt (W?) - B)
-(Czt (w?) - B)(Czt (w?) - B))
(we use twice Czt(Wt) - B = Czt(Wt) - Czt (w?) + Czt (w?) - B)
=Iwt - w?『-IIVCzt (Wt)Il2 + δ ((Czt(wt) - Czt (w?))2 -(Czt(w?)-B)2)
(middle terms cancel out)
=Iwt- w*∣∣2 - (Czt (Wt) - Czt (w?))2 + (Czt (w?) - B)2
i	t ?"	IvCzt(Wt)Il2 + δ + IvCzt(Wt)Il2 + δ
≤ Iwt- w?I2 - (Czt (Wt)-：z；(w?))2 + (Czt (w? - B)2 (0 ≤ IVCzt (Wt)I2 ≤ C)
C2 + δ	δ
≤ Iwt - w?I2 - (Czt(Wt)—上(w?)) + ε- (definition of ε)
C2 + δ	δ
We re-write this inequality as:
(Czt (wt) - Czt (w?))2 ≤ ε2 (C^ + 1) + (C2 + δ) (Iwt - w?I2 - ∣∣wt+ι - w?||2)
We can now use the Cauchy-Schwarz inequality to bound the sum over the iterations:
T	/ T	∖ 2
(T + 1) X (Czt (Wt) - Czt (w?))2 ≥ IXCzt (Wt) - Czt (w?) I
t = 0	∖t = 0	)
Therefore we can write:
(32)
(33)
(34)
(35)
≤ (T + 1)2 ε2 ( ɪ + 1 ) + (T + 1) (C2 + δ) (Iwo - W?12 - ∣∣wτ+ι - w?∣∣2),
≤ (T +1)2 ε2 -r + 1 + (T +1) (C2 + δ) Iwo - w?I2,
20
Under review as a conference paper at ICLR 2020
which yields:
T
X 'zt (Wt) - 'zt (W?) ≤
t = 0
+ (T + 1)(C2 + δ) kwo - w?k2,
I + D2ε2 (C2 + 1
≤ (T +1) ε
(36)
We can now take the expectation over the zt :
T
X f(wt) - f? ≤ (T +1)
t = 0
+ p/(T + 1) (C2 + δ) k w0 - w? k2 .
(37)
Dividing by T + 1 and exploiting convexity of f, We finally get:
(T⅛
-f? ≤ T+γ X f (wt) - f? (convexity of f)
+ 1 t = 0
≤ ε
(C2 + δ) ∣∣wo - w?k2
T + 1
(38)
D.4 Theorem 3
Theorem 3. We assume that X is a convex set, and that for every z ∈ Z, `z is convex and C -Lipschitz.
Let w? be an ε-interpolation for ((P), B). Wefurther assume that η > ε. Then if we apply ALI-G
with a maximal learning-rate of η to f, we have:
f (t+1 X W) - f? ≤
kw0 - w*k2 +	ε2	+ , I(C 2 + S)kw0 - w*k2 + α r C2 + 1
(η - ε)(T + 1) + δ(η - ε) + V	T + 1	+ V δ + .
(11)
Proof:
We consider the update at time t, which we condition on the draw of Zt ∈ Z:
I∣wt+1 一 w?k2
k∏Ω(Wt — YtV'zt (wt)) — W?k2
∣∣wt - γtV'zt(wt) - w?k2	(∏Ω projection)
∣∣wt 一 w?|2 一 2γtV'zt(wt)>(wt - w?) + γ2∣V'zt(wt)∣2
kwt - w*k2 - 2Ytv'zt (Wt)>(wt - w?) + Yt kVzt(Wt))-2B+δ kv'zt (Wt)k2
Il Y zt ∖w tJW `
`z	J	'zt (wt) - B 、
(because Yt ≤ ∣∣	(~~Vii2T7)
kv'zt(wt)∣∣2 + δ
kwt - w?k2 - 2γtv'zt(wt)>(wt - w?) + Yt IV，WtW-)B2 kv'zt(wt)k2
Il Y ZtlW t / 11
(because " (Wt) — B ≥ 0 and δ ≥ 0)
IlWt -
IlWt 一
Nk2 - 2γt('zt(wt) - 'zt (w?)) + γt('zt (wt) - B) (convexity of J)
‘？『-2Yt('zt(Wt) - 'zt (w?))+ Yt('zt (Wt) - 'zt (w?))+ Yt('zt (w?) - B)
IlWt - W?k2 一 γt('zt(wt) — 'zt(w?)) + γt('zt (w?) - B)
(39)
We now consider different cases, according to the value that Yt takes: Yt = kv'zt (WWt)-B+δ or Yt = η.
≤
≤
≤
≤
f
w
w
21
Under review as a conference paper at ICLR 2020
First, suppose that Yt = ∣∣ 金t (W：；-B+^ ∙ Then we can follow the proof of Theorem 2 to obtain:
I∣wt+1 - w?k2 ≤ ∣∣wt - w?k2 - ('zt(wtj-j(w?)) + ε-.	(40)
C 2 + δ	δ
Now suppose Yt = η and 'zt (wt) - '% (w?) ≤ 0. We can use Yt ≤ g'：：((Wt))[B+δ to write:
llwt+ι - w?k2 ≤ IlWt - w*∣∣2 - Yt(CZt (Wt) - 'zt(W?)) + Yt(CZt(W?) - B),
≤ kwt - w?k2 - ∣⅛t(w⅛¾(Czt(Wt) - 'zt(WQ)
+ ρ⅛⅛⅛⅛(CZt (WQ- B),
(41)
where the last inequality has used Yt ≤ ^^t(Wt)-B+δ, 'zt (Wt) — Cz： (w?) ≤ 0 and Cz： (w?)一
B ≥ 0. Therefore we are exactly in the same situation as the first case (where we used Yt =
kv⅛(Wtt))[B+δ), and thus we have again:
Il	∣∣2 , II	∣∣2	(CZt(Wt) - Czt(w?))2	£2
llwt+ι - w?k ≤ kwt - w?||----------------C2 + §-------+ y∙	(42)
Now suppose that Yt = η and CZt(Wt) — Czt (w?) ≥ 0. The inequality (39) gives:
∣∣wt+ι - w?|2
≤
≤
l∣wt - W?I2 - Yt(CZt(Wt) - CZt(w?)) + Yt(CZt (w?) - B),
l∣wt - w?|2 - η(CZt(wt) - Cz/w?)) + Yt(CZt(w?) - B),	(Yt = η)
∣∣wt - w?|2 - η(Czt (wt) - Czt (w?)) + Ytε. (definition of ε, Yt ≥ 0)
≤
kWt - W?『-η(CZt (Wt) - CZt(W?))+ C ∣V⅛t(Wt))-2BU ,
(because Yt ≤
CZt (Wt) - B ε ≥ 0)
IVCzt (Wt)Il2 + δ,	≥ )
≤
≤
l∣Wt - w?k2 - η(Czt (wt) - Czt(w?)) + CCZt(W)_B,	(∣VCzt(wt)∣2 ≥ 0)
Iwt- w?|2 - η(Czt (Wt)	-	Czt	(w?))+ CCZt(Wt)	- CZt (W§)+ CZt (W?)-B,
IlWt - w*∣∣2 - η(Czt (Wt)	-	CZt(W?)) + CCZt(Wt)	- CZt (W?)+ C ,
δ
(because CZt (w?) — B ≤ C)
(43)
kwt - w?『-(η - δ) (Czt(wt)- CZt(W?))+ %∙
We now introduce IT and Jt as follows:
TT , {t ∈	{0,-,T} ： Yt	= η andCZt(Wt)	-	CZt(W?)≥	0)	(44)
Jt = {0,…, T} \ TT
Then, by combining inequalities (40), (42) and (43), and using a telescopic sum, we obtain:
Il	∣∣2/Il	∣∣2 ,	(	(CZt (Wt) - CZt (w?))2	C2 ʌ
llwτ+ι - w?|| ≤ ∣∣w0 - w?|| +	(------C2 + §-------+ y )
t∈Jτ '+	,
+ X (- (η -1)(CZt(Wt) - Czt(W?))+ X)
t∈Iτ '	,
(45)
22
Under review as a conference paper at ICLR 2020
Using ∣∣wτ+ι — w? ∣∣2 ≥ 0, we obtain:
C?[ § X ('z/Wt) — 'zt (w?))2 + (η — ・) X Gt (Wt) — 'zt (w?)) ≤ l∣wo - w*ll2 + (T+ι) ε^
t∈Jτ	t∈Iτ
(46)
In particular, the inequality (46) gives that:
, 一、	一 2
(η — δ) X ('zt (Wt) - 'zt (w?)) ≤ IlWO — w*∣∣2 + (T + 1) y∙
t∈Iτ
(47)
Furthermore, for every t ∈ It , we have ('% (wt) — '% (w?)) ≥ 0, which yields (η — ∣∙) P ('% (wt)—
'zt (w?)) ≥ 0 since η > ∣. Thus the inequality (46) also gives:
t∈Iτ
1
C2 + δ
ε2
X (`zt (Wt) — `zt (w?))2 ≤ ι∣wo— w?k2 + (T+1) -ξ-∙
δ
(48)
Using the Cauchy-Schwarz inequality, we can further write:
(X 'zt (Wt) — 'zt (w?)) ≤ 1JT1 X (CZt (Wt) — 'zt (w*))2∙
t∈Jτ
t∈Jτ
Therefore we have:
X 'zt (Wt) — 'zt (W?) ≤ JIJT 1 X ('zt (Wt) — 'zt (w?))2 ,
t∈Jτ	V	t∈Jτ
≤ j∖Jt I (C2 + δ) (||wo — W? I2 + (T + 1) ε2 )∙
We can now put together inequalities (47) and (49) by writing:
T
X 'zt (Wt) - 'zt(w?)
t = 0
E 'zt (wt) — 'zt (w?) + E 'zt (wt) — 'zt (w?)
t∈Iτ
t∈Jτ
(49)
(50)
≤
1
n— δ
∣∣wo — w?k2 + (T + 1)
|Jt|(C2 + δ) (IlWO — w?||2 + (T + 1)ε2
≤
1
n— δ
∣∣wo — w?k2 + (T + 1)ε2) + j(T +1)(C2 + δ) (∣∣wo — w?k2 + (T + 1)≡2)
(51)
Dividing by T + 1 and taking the expectation, we obtain:
f
1
T + 1
X W) — f? ≤
t=o )
1
T + 1
T
X f (wt) — f?,	(f is convex)
t=0
≤
w?l2
ε2
lwo - -.？”	+
(η—ε )(t+1)+ δ(η—ε)
+ j(C2 + δ)
∣wo — w?l2 +ε2)
T + 1	+ δ /
≤
ε2
∣∣wo — w?k2
(η—ε )(t+1)+ δ(η—ε)
+/
(C2 + δ)∣wo - w?12
T + 1
+ ε
(52)
23
Under review as a conference paper at ICLR 2020
D.5 Theorem 4
Theorem 4. We assume that X is a convex set, and that for every z ∈ Z, `z is convex and C -Lipschitz.
Let w? be an ε-interpolationfor ((P), B). Then if we apply ALI-G with a maximal learning-rate of η
to f, we have:
1( 1 X ∖ f ∕w0 - w?k2 . . S(C2 + δ)lwo - w*l∣2	Ir2 X z19λ
f(T+1 ∑0wtJ-f?	≤	η(τ + 1)	+ε + V---------T+1----------+ ηε,C2	+	δ∙	(12)
Proof:
We consider the update at time t, which We condition on the draw of Zt ∈ Z. We re-use the inequality (39)
from the proof of Theorem 3:
I∣wt+1 - w?k2 ≤ l∣wt - w?k2 - γt('zt(wt) - 'zt(w?)) + γt('zt(w?) - B)	(53)
We consider again different cases, according to the value of Yt and the sign of 'zt (wt) — 'zt (w?).
Suppose that 'zt (wt) — 'zt (w?) ≤ 0. Then the inequality (53) gives:
I∣wt+1 - w?k2
≤ ∣∣wt - w?k2 - Yt('zt(wt) - 'zt(w?)) + Yt('zt (w?) - B),
≤ ∣∣wt - w?k2 - η('zt (wt) - 'zt(w?)) + Yt ('zt (w?) - B),
(because Yt ≤ η, 'zt (wt) - 'zt (w?) ≤ 0)
≤ IIWt	- w?k2 -	η('zt (wt) -	'zt(w?)) + Ytε,	(definition of ε, Yt	≥ 0)
≤ l∣wt	- w?k2 -	η('zt (wt) -	'zt(w?)) + ηε,	(Yt ≤ η, ε ≥ 0)
(54)
Now suppose '2七(Wt)— '2七(w?) ≥ 0 and Yt = η. Then the inequality (53) gives:
l∣wt+ι - w?k2 ≤ l∣wt - w?k2 - Yt('zt(wt) - 'zt(w?)) + Yt('zt(w?) - B),
=l∣wt -w?k2 -η('zt(wt)	-'zt(w?)) + η('zt(w?) -b),	(Yt = η)	(55)
≤ IlWt -	w?||2 - η('zt	(wt)	- 'zt(w?)) + ηε,	(definition of ε, η ≥ 0)
Finally, suppose that 'zt (wt) — 'zt (w?) ≥ 0 and Yt = kv^t(WWt)-B+δ ∙ Then the inequality (53)
gives:
∣∣wt+ι - w?||2
≤	∣∣wt	-	w?l2	- Yt('zt (wt)	-	'zt (w?)) + Yt('zt (w?) - B),
≤	l∣wt	-	w?l2	- Yt('zt(wt)	-	'zt(w?)) + η('zt(w?) -b),	(Yt ≤	η, 'zt(w?)	-B	≥	0)
≤	∣∣wt	-	w?l2	- Yt('zt(wt)	-	'zt (w?)) + ηε, (definition of ε,	η ≥	0)
=lwt	-	w*l2	- P⅛⅛⅛⅛δ('zt(Wt) - 'zt (W?)) + ηε,	(Yt	= W⅛⅛⅛⅛δ)
≤ Ilwt- w?『-('∣tx(Wt)(W3：??) + ηε,	('zt (Wt) - B ≥ 'zt (Wt)- 'zt(w?) ≥ 0)
≤ ∣wt- w?l2 - ('zt (Wt) '-3(w?))2 + ηε, (∣V'zt (Wt)I2 ≤ C2)
C2 + δ
(56)
We now introduce IT and JT as follows:
JT , {t ∈ {0,...,T} : Yt = IlVt(W))-2B+ δ and 'zt (Wt) - 'zt (W?) ≥ 0}
IT，{0,∙∙∙,T}∖Zτ
(57)
24
Under review as a conference paper at ICLR 2020
Then, by combining inequalities (54), (55) and (56), and using a telescopic sum, We obtain:
kwτ+ι - w?k2 ≤ kwo- w?k2 + X (- KWC-+zδ(W?)) + ηε)
t∈Jτ	+
+ X (-η('z (Wt) - 'zt (w?)) + ηε)
t∈Iτ
(58)
Using ∣∣wt+ι — w? ∣∣2 ≥ 0, we obtain:
C21+ § X ('zt(Wt) ―'zt(w?))2 + η X ('zt(Wt) ―'zt(w?)) ≤ Iwo 一 w?k2 + (T + 1)ηε (59)
t∈Jτ	t∈Iτ
We now take the expectation and obtain:
C21+ δ	X E [Gt (Wt) -	'zt (w*))2]	+ η X	(f(wt)- f?)	Wkwo-	w*k2 +	(T + 1)ηε	(60)
t∈Jτ	t∈Iτ
Since E[U]2 ≤ E[U2] for any real-valued random variable, we can write:
C21+ δ X (J(Wt) - f*)2 + η X (f(wt) - f*) ≤ kWo- w*∣2 +(T + 1)ηε
t∈Jτ	t∈Iτ
Since each f (Wt) — f* ≥ 0, the inequality (61) gives that:
η E (f(Wt) — f*) ≤ ∣∣Wo - w*∣2 + (T + ι)ηε,
t∈Iτ
and:
d K X (J(Wt) - f*)2 ≤ kwo- w*∣2 + (T + 1)ηε∙
C + δ M
Using the CaUChy-SChWarz inequality, we can further write:
(X f(wt) - f*)	≤ ∣Jτi X (f(wt) - f*)2.
∖t∈Jτ	J	t∈Jτ
Therefore we have:

E f(Wt)- f* ≤
t∈Jτ
IJtI ∑ (f(Wt) - f*)2,
t∈Jτ
(61)
(62)
(63)
(64)
(65)
≤ √∣æ|(C2 + δ) (kwo - w*k2 + (T + 1)ηε).
We can now put together inequalities (62) and (65) by writing:
T
X f (Wt) - f*
t = 0
=X f (wt) - f* + X f (wt) - f*
t∈iτ	t∈Jτ	(66)
≤ η	(kwo -	w*k2 + (T +	1)ηε)	+	p|JT∣(C2 + δ)(kwo 一 w*k2 + (T + 1)ηε)
≤ ：	(kwo -	w*k2 + (T +	1)ηε)	+	P(T + 1)(C2 + δ)(∣wo - w*∣2 + (T + 1)ηε)
25
Under review as a conference paper at ICLR 2020
Dividing by T + 1, We obtain:
f (片 XXX Wt) - f? ≤
1 T
T—1 E f (wt) - f?,	(f is convex)
+ 1 t = 0
kw0 - W?k2 , "S(C2 +δ∕ kw0 - w*k2 +
η(T + 1)	+ ε +fC + ")( T + 1	+
(67)
kw0 - w*k2	J
η(T + 1) + ε +V
(C2 + δ)kwo - w?k2
≤
≤
T + 1
+ Rz C2 + δ.
D.6 Theorem 5
Lemma 1. Let Z ∈ Z. Assume that 'z is convex, β-smooth and is lower-bounded on Rp by B ∈ R.
∀ W ∈ Rp, 'z(w)- B ≥ ɪ∣∣V'z(w)k2	(68)
2β
Proof: Let W ∈ Rp and suppose that 'z reaches its infimum at w ∈ (R ∪{-∞, +∞})p.
First, let us consider the case W ∈ Rp. Then by Lemma 3.5 ofBubeck (2015), we have:
'z(w) - 'z(w) ≤ V'z(w)>(w - w) - ɪ∣∣V'z(w) - V'z(w)k2,
1	β	(69)
=-而 kV'z (w)k2 (V'z (w) = 0).
2β
Therefore we can write:
1
'z(w) - B ≥ 'z(w) - 'z(w) ≥ —kV'z(w)k2.	(70)
2β
Now let us assume that W ∈ Rp. Then we can construct a sequence (Wk)k∈N ∈ (Rp)N that converges to
W. Since 'z and V'z are continuous functions (they are respectively convex and smooth), we have:
lim 'z(Wk) = inf 'z,
k→∞
lim V'z(Wk) = 0.
k→∞
(71)
Therefore the previous case gives the wanted result by using Wk in place of W and then taking the limit
k → ∞.
Lemma 2. Let Z ∈ Z. Assume that 'z is convex, β-smooth and is lower-bounded on Rp by B ∈ R.
Then we have:
∀W ∈ Rp,
'z(w)- B 〉J_____________δ
kV'z(w)∣∣2 + δ ≥ 2β - 4β2('z(w)-B))
(72)
Proof:
Let w ∈ Rp. We apply Lemma 1 and we write successively:
'z (w) - B 〉	'z (w) - B
kV'z (w)k2 + δ ≥ 2β('z (w) - B)+ δ ,
(Lemma 1)
'z (W) - B + 2β - 2β
2β('z (w) - B + 2β)，
1	2δβ
2β	2β('z (W)- B + 2β )
(73)
≥ 2β - 4β2('z (W) - B).	(δ ≥ O)
26
Under review as a conference paper at ICLR 2020
Theorem 5. [Convex and Smooth] We assume that X is a convex set, and that for every z ∈ Z, `z is
convex and β-smooth. Let B be a uniform lower bound on (P) and w? be a solution of(P). Further
suppose that w? is an ε-interpolation for ((P), B), and that δ > 2βε. Then ALI-G∞ applied to f
satisfies:
f 1 1 XW! - f ≤ δ + 2/ kw0 -w*k2	(13)
T T+ + 1 t=0 t) ∕? 一 β(1 - 2βε) + 1 -竽 T + 1 .	()
Proof:
We consider the update at time t, which We condition on the draw of Zt ∈ Z:
∣∣wt + 1 - W? k2
=k∏Ω(wt - γtV'zt(wt)) - W?k2
≤ IlWt - YtV'zt (wt) - w?k2 (∏Ω projection)
=∣∣Wt - W?k2 - 2γtV'zt(wt)>(wt - W?) + γ2∣V'zt(Wt)II2
=kwt - w*k2 - 2Ytv'zt (Wt)T(Wt - w?) + Yt kVzt (W))-2 B+δ kv'zt (Wt )k2
(definition of Yt)
≤ kWt - W*k2 - 2Ytv'zt (Wt)T(Wt - w*) + Yt 摩叱-ιB2 kv'zt (Wt) k2
Il Y zt ∖ v tJW
(because J (Wt) — B ≥ 0 and δ ≥ 0)
≤ ∣∣Wt - W?k2 - 2Yt('zt(Wt) - 'zt (w?)) + Yt('zt (Wt) - B) (convexity of J)
=∣∣Wt - W?k2 - 2Yt('zt (Wt) - 'zt (w? )) + Yt('zt (Wt) - 'zt (w?))+ Yt('zt(w?) - B)
=l∣Wt - W?k2 - Yt('zt (Wt) - 'zt(w?)) + Yt('zt(w?) - B)	(74)
We now lower bound Yt ('zt (wt) — 'zt (w?)) and upper bound Yt ('zt (w?) — b) individually.
We begin with Yt ('zt (wt) - 'zt (w?)), for which we distinguish two cases according to its sign:
Suppose ('zt (wt) - 'zt (w?)) ≥ 0. Then we can write:
Yt ('zt (Wt)- 'zt(w?))
='zt(wt) - B
=∣V'zt (Wt)k2 + δ
1	δ
('zt (Wt) - 'zt (w?)) , (definition of Yt)
陵1丽1¥
>- = >-
4β2('z(wt) - B)
-'zt (w?))-
-'zt (w?))-
('zt (wt) - 'zt (w?)) (Lemma 2, ('Zt(Wt) - 'zt (w?)) ≥ 0)
δ 'zt(Wt) - 'zt(w?)
铲('zt (Wt)- B)
4δ2	('zt (w?) ≥ B, ('zt (Wt) - 'zt (w?)) ≥ 0)
4β
(75)
—
27
Under review as a conference paper at ICLR 2020
Now suppose ('zt (wt) — 'zt (w?)) ≤ 0, and let Us show that the same result holds. We have:
〜='zt (Wt) — B		
Yt= ∣V'zt (wt)k2 + δ ≤	'zt (w*) — B ≤ ∣V'zt(wt)k2 + δ	(('zt (Wt) — 'zt (w*)) ≤ 0)	
≤	ε	 ≤ ∣V'zt(wt)k2 + δ	(definition of ε)	(76)
≤ ε (∣v'zt(wt)∣≥ δ ≤ 2β (δ ≥ 2βε)	0)	
Therefore we have:		
Yt ('zt (Wt) — 'zt (W*))		
≥ *('Zt(Wt) — 'zt (w*)) 2P	(('zt (wt) — 'zt (w?)) ≤ 0)	(77)
≥ 亲('zt (Wt) — 'zt (w*)) 2P	-4p2 (δ ≥ O)	
In conclusion, regardless of the sign, it always holds true that:
Yt ('zt (Wt) - 'zt (w?)) ≥ 21β ('zt (Wt) - 'zt (w?)) - 4β2	(78)
We now upper bound γt('zt (w?) — b):
Yt('zt (w?) -B)= ('zt (WW—t BW⅞(w?δ -B)，(definitionof Yt)
≤ ('zt (Wt) - B)FZt (W?) - B) , (kV'zt (Wt)k≥0)
δ	(79)
≤ ('zt (Wt) - 'zt (W?) + ε)ε ,	(definition of ε twice)
δ
=*(('Zt(Wt) — 'zt (w?)) + ~τ∙
δ	δ
Putting inequalities (74), (78) and (79) together, we obtain:
∣∣Wt+1 - w? k2 ≤ IIWt - W?k2 — 2β (OZt(Wt) — 'zt (w?)) + 4e2 + ε (('Zt(Wt) — 'zt (w?)) + ε^.
(80)
Therefore we have:
(2β — δ) ('zt (Wt) — 'zt (w?))—
≤ ∣∣wt — W?k2 — I∣wt+1 — W?k2.
(81)
By summing over t and taking the expectation over the zt, we obtain:
δ —ββ'	X	(f(wt)	— f(w?)	— δ	+4δ	ε ) ≤	kw0-w?|2—E	[|wT+1	—	w*k2] ≤ kw0-w?k2.
t=0	(82)
28
Under review as a conference paper at ICLR 2020
By assumption, We have that δ — 2βε > 0. Dividing by T + 1 and using the convexity of f, We finally
obtain:
f3 X Wt) - f? ≤
1 T
T+j ΣLf (Wt) -f?
(convexity of f),
(83)
2βδ δ2 + 4β2ε2	2βδ ∣∣W0 — w?k2
δ — 2βε	4β2δ+ δ — 2βε T+1
δ2 +4β2ε2	2βδ ∣∣W0 — w?|2
2β(δ — 2βε) + δ — 2βε ―T + 1 一,
≤	δ2 ≤ β(δ — 2βε)	,2βδ + δ — 2βε	∣∣W0 - w?k2 T + 1-,	(δ — 2βε ≥ 0)
_	δ	I 2β	∣∣W0 - w?k2	
=β(1 — 2βε)	+ 1T	T + 1-.	
D.7 Theorem 6
Theorem 6.	We assume that X is a convex set, and that for every z ∈ Z, `z is convex and β-smooth.
Let w? be an ε-interpolationfor((P), B), and suppose that δ > 2βε. Further assume that η ≥ 泰.
Then if we apply ALI-G with a maximal learning-rate of η to f, we have:
f
(τ+ι
f ≤ δ +	2β	kwo - w?k2
f? ≤ β(i -竿)+ 1-ψ	T + 1
(14)
Proof:
By using the fact that Yt ≤ g'zt(Wt))-B+δ rather than Yt = g'z：(Wt))-2二,We can see that the inequality
(74)	is still valid:	Nt	Nt
I∣wt+1 — w?k2 ≤ ∣∣Wt — w?k2 — Yt('zt(wt) — 'zt (w?)) + Yt('zt (w?) — b)	(84)
As previously, we lower bound γt('zt (wt) — 'zt (w?)) and upper bound γt('zt (w?) — b) individually.
We begin with Yt ('zt (wt) — 'zt (w?)). We remark that either Yt = g'：： ((Wt))-J or Yt = η.
Suppose Yt = kv'z (Wt)-B+看 and 'zt (wt) — 'zt (w?) ≥ 0. Then we are in the same condition as in
Theorem 5, and thus the inequality (78) holds true:
It(QZt(Wt) - 'zt(w?)) ≥ 2β ('zt (Wt) - 'zt(w?)) - 4e2 .	(85)
Now suppose Yt = η and 'zt (wt) — 'zt (w?) ≥ 0. Then we have:
Yt ('zt (wt) — 'zt (w? ))= η ('zt (wt) — 'zt (w?))
≥ η('zt (Wt) - 'zt (w?)) - 4δ2
4β
≥ 2β ('zt (Wt) - 'zt(w?)) - 4^2
(because η ≥ ɪ, 'zt (wt) — 'zt (w?) ≥ 0).	(86)
29
Under review as a conference paper at ICLR 2020
Now suppose	'zt (Wt)	-	'zt (w?)	≤ 0. By using	Yt	≤	kv⅛(W1-B+δ	instead of	Yt	=
kv^t (Wt)-B+δ，we can See that the inequality (76) is still valid, which gives:
Yt ≤ 2^	(87)
2β
We now use 'zt (Wt) — 'zt(w?) ≤ 0 to write:
Yt ('zt (Wt) - 'zt (w?)) ≥ 2β ('zt (Wt) - 'zt (w?)) (Czt(Wt) - 'zt (W?) ≤ O)
β	δ	(88)
≥ 2β ('zt (Wt) - 'zt (W?)) -场
In conclusion, in all cases, it holds true that:
It(QZt(Wt) - 'zt(W?)) ≥ 2e('zt(Wt) - 'zt(W?)) - 4β2	(89)
By using Yt ≤ kVzt (Wt))-B+看,We can remark that the inequality (79) holds true and gives:
γt ('zt (W?) - B) ≤ τ(('zt (Wt) - 'zt (W?)) + k.	(90)
δ	δ
We now put together inequalities (84), (89) and (90):
..	..q	,.	,,0	1 ,	,	、	,	、、 δ ε ,,	,	、	,	、、	ε2
kwt+ι -	W?k	≤ IlWt	- W?k	-	2β ('zt(Wt) -	'zt(W?)) +	4β2	+ δ(('Zt(Wt)	-	'zt (W?)) + y,
=kWt - W?k2 - (2β -1) ('zt(Wt) -'zt(W?)) + 4β2 + εδ^.
(91)
This is exactly the same result as in the inequality (81) from the proof of Theorem 5. Therefore the rest of
the proof of Theorem 5 follows and we obtain the desired result.
D.8 Theorem 7
Theorem 7.	We assume that X is a convex set, and that for every z ∈ Z, `z is convex and β-smooth.
Let w? be an ε-interpolationfor ((P), B), and suppose that δ > 2βε. Further assume that η ≤ *.
Then if we apply ALI-G with a maximal learning-rate of η to f, we have:
e 1 1	1X	ʌ	llw0 - w*l∣2	δ
f[τ+1 tξWtJ -f? ≤ η(τ + 1)	+2β + ε
(15)
Proof:
By using the fact that Yt ≤ kv'Z^W?)-% rather than Yt = 区'：：((Wt)-2B+δ, We Can see that the inequality
(74) is still valid:	Nt	Nt
∣∣Wt + 1 - W?k2 ≤ ∣∣Wt - W?k2 - Yt ('zt (Wt) - 'zt (w? )) + Yt ('zt (w?) - B)	(92)
As previously, we lower bound Yt ('zt (Wt) — 'zt (w?)) and upper bound Yt ('zt (w?) - b) individually.
We begin with Yt ('zt (Wt) - 'zt (w?)). We remark that either Yt =区'：：((Wt))-B+δ or Yt = η
30
Under review as a conference paper at ICLR 2020
Suppose Yt = kN(W)-B+ and 'zt (Wt) - 'zt (w?) ≥ 0. First we write:
Yt
='zt(wt) - B
=k'zt (Wt)II2 + δ
'zt (Wt) - B + 2β	2β
=--------------------------------------
k'zt (Wt)Il2 + δ k'zt (Wt)k2 + δ
kgzt (Wt) k2 I δ
≥	2β + 2β O	1
—k'zt (Wt)II2 + δ 那 k'zt (Wt)II2 + δ
_ 1 O	1
=2β -而 Mzt(Wt)II2 + O
(Lemma 1)
(93)
≥ η - 2 I'zt (Wt)I2 + O
(η ≤
Since 'zt (Wt) — 'zt (w?) ≥ 0, this yields:
YtCZt(Wt) - 'zt (W?)) ≥ (η -金 I'zt (W1)∣∣2 + O ) ('Zt(Wt) - 'zt (W?))
= η('zt (Wt) - 'zt (WQ)-捺 ⅛W⅛⅛⅛1	(94)
≥ η('z÷ (Wt) — 'z÷ (w?))-- CZt(Wt)- B	('z÷ (w?) ≥ B)
一八 ZtI tJ	ZtI ?〃	2β ∣∣'zt (Wt)I2 + O 'zt' ?7 - B
We now notice that since Yt = κ';((Wt)[B+δ, and Yt ≤ η, then necessarily g;：：((Wt)-B+δ ≤ η.
This gives:
Yt('zt (Wt) - 'zt(w?)) ≥ η('zt (Wt) - 'zt(w?)) - 2ηββ	(95)
Now suppose Yt = η and 'zt (Wt) — 'zt (w?) ≥ 0. Then we have:
Yt('zt(wt) - 'zt(w?)) = η('zt(wt) - 'zt(w?))
≥ η('zt (wt) - 'zt(w?)) - η∣.	(96)
Now suppose 'zt (wt) - 'zt (w?) ≤ 0. Since Yt ≤ η by definition, we have that:
Yt ('zt (Wt)- 'zt (w?)) ≥ η('zt (Wt)- 'zt (w?)) (CZt (Wt)- 'zt (w?) ≤ 0)
≥ η('zt (Wt) - 'zt (w?)) - ηβ∙
In conclusion, in all cases, it holds true that:
Yt('zt (Wt) - 'zt (w?)) ≥ η('zt (Wt) - 'zt(w?))-%
2β
(97)
(98)
We upper bound Yt ('zt (w?) — b) as follows:
Yt('zt (w?) - B) ≤ η('zt (w?) - B) ('zt (w?) ≥ B)
≤ ηε (definition of ε)
We combine inequalities (92), (98) and (99) and obtain:
Ilwt+1 - w?『≤ IlWt - w?||2 - η('zt (Wt) - 'zt(w?)) + η∣ + ηε∙
By taking the expectation and using a telescopic sum, we obtain:
T
0 ≤ I∣wτ +1 - w?I2 ≤ ∣∣W0 - w?I2 - X
t = 0
(η(f (Wt) - f?) + 1|+ 心.
(99)
(100)
(101)
31
UlIderreVieW as a COnferenCe PaPer at ICLR 2020
Re—arran--ng and USing rhe COnVeXiry Of j-we finalIy Obram
=WO — W1-一 2h
I T(T+1)
(102)
D∙9 THEoREM 8
LemmaFbry ɑm 为We hae Iha
= α=2 + =b=2≥*αlb=2 (103)
PrOOf: ThiSiS a Simpie apicarn Ofrhe ParalleIogram law- bur We give rhe PrOOf here for COmPleSneS
=⅜+ ==—T—=l亘2 + ==—τ=2 —ɪF + %-
J'=⅜+l2+ατb
"τ+=
≥ 0
Lemma 4∙ LsNmN- Asss∕ne Ihal FiS R—stnm-y Co≡wex and is lowey-bou^ded 0≡为七 bylrom⅛
such ZhRZinf gl≤∙ Jaddilion》suppose Ihql b2α∙ Then We hve.∙
∀w ∈冕
C(W) —B 1
<lc(w) = 2 + d I 第
(104)
ProOf:
Lef Wm 甩 and SUPPoSe fhafreaches ifs minimum af 恺 m 甩(This minimum exisfs because Of SfrOng
ConvexiryBy denirn Of SrrOng Convexiry-We have rha
<a m 甩)0a) ≥ 0W)-I- VeN(W)T (a — W) +一 a — W=2 (105)
We minimize rhe righanside OVer * WhiChVe 巴
<a m 区)a) ≥ £ W) + V0w)τ(a — W) +M — W=2w) —-vw)=2 (106)
2Q
ThUS by ChOoSg a =恺 and re—ordering》We Obtarhe following result: (a∙ka rhe pyak—Lojasiewicz
equayx
C(W)Ir!?) ≤τ<r (W=(107)
TherefOre We Can Writ^
l-( W) lllωl-( W) ll-(恺)+6 升=V1-(W)= 2 +6
= Ve(W)=2 +3 I =Ve(W)=2 二 I =Ve(W)=2 二
LrB + 6
We inqoduce rhe function G .∙ x e 国+ I 2口 " and We COmPUre i〔s derivaHV
+ d
I
汴(2 + 3) — 汴2 — 6
(2 + 3)2

I
(=≥ = ≥2s
TherefOre 铮 is monosnically5'CreaS5'g∙ AS a resulr- we have
< 2 m 定+"s≤ Hm 容s=v∙
1 2p
I
32
Under review as a conference paper at ICLR 2020
Therefore We have that:
a W'z(w)k2+ ε
∣∣V'z (w)k2 + δ
ψ(kv'z(w)k2)≤ 2α,
(111)
which concludes the proof.
Theorem 8.	[Strongly Convex and Smooth] We assume that X is a convex set, and that for every
Z ∈ Z, 'z is α-strongly ConVex and β-smooth. Let B be a uniform lower bound on (P) and w? be a
solution of(P). Further suppose that w? is an ε-interpolation for ((P), B), and that δ > 2βε. Then
ALI-G∞ applied to f satisfies:
f (WT +1) -	f?	≤ βexP	(-谒)	llw0	- w?k2	+----+ f10- + 44—2	)	ε∙	(16)
8β	α α	α2
In other words, f approximately converges to f? at a rate of O(exp(一αT∕8β)).
Proof:
We condition the update on Zt drawn at random. The beginning of the proof is identical to that of Theorem
5 (and in particular requires δ > 2βε). In addition, we remark that δ > 2βε ≥ 2αε, because it always
holds true that β ≥ α. Combining inequalities (74) and (78), we obtain:
..	1 . ..	1	1,“，、	”，、、 δ ,“，、	、
kwt + 1 - w*k ≤	kwt	-	w*k	-	2β ('zt (Wt)	- 'zt (w?))	+ 4β2	+ Yt ('zt	(W?) - B),
≤	l∣Wt	-	w*k2	-	21β ('zt (Wt)	- 'zt (w?))	+ 4β2	+ Ytε,	(definition of ε)
≤	kwt	-	w*k2	-	215('zt (Wt)	- 'zt(w?))	+ 4δ2	+ 2ε-.	(Lemma4)	(I12)
2 β	4β 2α
Let W(Zt) be the minimizer of 'zt on its unconstrained domain Rp (its existence is guaranteed by the strong
convexity property). Then we exploit strong convexity to lower bound the progress made:
'zt (Wt) - 'zt (W?) = 'zt	(Wt)	-	'zt (w(Zt))+ 'zt (w?)	- 'zt (W(Zt))-	2('zt(W?) -	'zt (w(Zt)))
≥ 'zt	(Wt)-	'zt (W(Zt)) + 'zt (w?)	- 'zt (W(Zt))-	2('zt (w?) -	B)
(because 'zt (w(Zt)) ≥ B)
≥ 'zt (Wt) - 'zt (w(Zt)) + 'zt (w?) - 'zt (W(Zt)) - 2ε (definition of ε)
≥ 2 kwt - W(Zt) k2+α kw? - W(Zt) k2 - 2ε (α-strong convexity)
≥ αkwt — w?k2 — 2ε (Lemma3)	(113)
We now combine inequalities (112) and (113):
kwt+ι- w?k2 ≤ (1 - 8β)kwt - w?k2+β+4e2+2α	(114)
We use a trivial induction over t and write:
kwt+1 - w?k2
ε δ ε
2ɑ
+2α
33
Under review as a conference paper at ICLR 2020
=(1- 8β) kw0 -w*k2 + 8β (ε +卷 + 2α).	(115)
In particular, We remark that the right hand-side of the equation is independent on Zt.
Given an arbitrary w ∈ Rp, we now wish to relate the distance kw-w? k2 to the function values f (w)-f (w?).
Since each 'z is α-strongly convex and β-smooth, so is f = Ez ['z]. We introduce w the minimizer of f
on its unconstrained domain Rp. Then we can write that for any W ∈ Rp:
f (w) - f (w?) ≤ f (w) - f (w), (f (w) ≤ f (w?))
≤ Vf (w)>(w — w) + β ∣∣w — wk2, (f is β-smooth)
=2 l∣w — wk2,	(Vf (w) = 0)
≤ β(kw - w?k2 + ∣∣w? 一 wk2),	(Lemma3)
≤ β∣w 一 w?k2 + 2β (f (w?) - f (w)) ,	(f is a-strongly convex)
≤ β∣w 一 w?|2 + 2β (f (w?) 一 B),	(B ≤ f (w) by definition)
≤ β∣w - w?k2 + 2βε, (definition of ε)	(116)
一	α
We combine the results to obtain the final result:
f (wt+1) 一f(w?) ≤ 创加+1一 w*k2 + 2βε
≤ β
β
即一 w?k2 + 8β (ε + 4β2 + 2α ))+2 β∣
即一 w*k2 + 8β(ε + 4β + 2α)+ 2 βε
β(1- -α
∖	8β
≤ β exp
∣∣wo 一 w?k2 + 2δ +(10β +
ɑ ∖ ɑ
∣∣wo — w?k2 + 2δ +(10β +
ɑ ∖ ɑ
ε
ε.
(117)
D.10 Theorem 9
Theorem 9.	We assume that X is a convex set, and that for every z ∈ Z, `z is α-strongly convex
and β-smooth. Let w? be an ε-interpolationfor ((P), B), and suppose that δ > 2βε. Further assume
that η ≥ 21β. Then ifwe applyALI-G with a maximal learning-rate of η to f, we have:
f (WT+1) - f? ≤β exp (-αT) kw0- w*k2+2δ+(10 α+4 β2)ε (17)
Proof : Re-using inequalities (84) and (89) from the proof of Theorem 6, we obtain:
kwt + 1 - w? k2 ≤ kwt 一 w?k2 ― 2β ('zt(Wt) ― 'zt (w?)) + 4β2 + γt('zt(w?) ― B).	(118)
This is exactly the same result as the first line of the inequality (112) in the proof of Theorem 8. Then the rest of
the proof is identical to the one of Theorem 8.
D.11 Theorem 10
Theorem 10.	We assume that X is a convex set, and that for every z ∈ Z, `z is α-strongly convex
and β-smooth. Let w? be an ε-interpolationfor ((P), B), and suppose that δ > 2βε. Further assume
34
Under review as a conference paper at ICLR 2020
that η ≤ 21β. Then ifwe applyALI-G with a maximal learning-rate of η to f, we have:
14εβ
α
2	2δ
kw0-w*k + Z +
f(wT+1) - f? ≤ βexp
(18)
Proof : Re-using inequalities (92	) and (98) from the proof of Theorem 7, we can write:
kwt+1 - w? k2 ≤ kwt - w? k	2 - η(Czt(Wt) - `zt (w?)) + 2ηβ+Yt(Czt(W?) - B),
≤ kwt - w? k	2 一 η('zt (wt) - 'zt (w?)) + 2ηδ + ηε (Yt ≤ η, 0 ≤ 'zt (w?) 一 B ≤ ε). (119)
Furthermore, the inequality (113)	gives:
)	'zt(wt) — 'zt(w?) ≥ 4∣∣Wt - w?k2 — 2ε	(120)
Therefore, we can write:
kwt+1 一 w	Qk2 ≤ Ilwt - w?k2 - α4ηIlwt - w?k2 + 2ηβ + 3ηε, 4	2β	(121) =(1 - *) Ilwt - w?k2 + 2ηβ + 3ηε∙
Then a trivial induction gives tha	:
∣wT +1 - w? ∣2 ≤	(1- 苧)kw0 -w*k2 + (ηδ + 3ηε)∑0 (1- αη),
≤	(1号)Tkw0-w*k2 + 偌 +3ηε)X(1号)t , t=0	(122) (1 - 24η)T kw0 - w*k2 + (2⅞ + 3ηε) 1- (11-0η), (1-α )T kw0 - w*k2+α2β+1α2ε.
We now re-use the inequality (11	6) to write:
f(wT+1)	-f* ≤ βkwT+1-w*k2+2ααε, αη T	2	2δ 14εβ ≤β (1- τ) kw0 - w*k + Z + 丁，	(123) ≤ βexp (-4ηT) kw0 - w*k2 + 2δ + 14εβ.
35
Under review as a conference paper at ICLR 2020
E Additional Experimental Details
E.1 Standard Deviation of CIFAR Results
Task	Optimizer	Avg	Std
DN10	ADAMW	92.6	0.08
DN10	ALIG	95.0	0.16
DN10	AMSGRAD	91.7	0.25
DN10	DFW	94.6	0.22
DN10	L4ADAM	90.8	0.09
DN10	L4MOM	91.9	0.17
DN10	SGD	95.1	0.21
DN10	YOGI	92.1	0.38
DN100	ADAMW	69.5	0.54
DN100	ALIG	76.3	0.14
DN100	AMSGRAD	69.4	0.41
DN100	DFW	73.2	0.29
DN100	L4ADAM	60.5	0.64
DN100	L4MOM	62.6	1.98
DN100	SGD	76.3	0.22
DN100	YOGI	69.6	0.34
WRN10	ADAMW	92.1	0.34
WRN10	ALIG	95.2	0.09
WRN10	AMSGRAD	90.8	0.31
WRN10	DFW	94.2	0.19
WRN10	L4ADAM	90.5	0.09
WRN10	L4MOM	91.6	0.24
WRN10	SGD	95.3	0.31
WRN10	YOGI	91.2	0.27
WRN100	ADAMW	69.6	0.51
WRN100	ALIG	75.8	0.29
WRN100	AMSGRAD	68.7	0.70
WRN100	DFW	76.0	0.24
WRN100	L4ADAM	61.7	2.17
WRN100	L4MOM	61.4	0.86
WRN100	SGD	77.8	0.13
WRN100	YOGI	68.7	0.47
Table 4: Test Accuracy (%) on CIFAR including standard deviations. Each experiment was run three
times.
36