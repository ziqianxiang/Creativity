Under review as a conference paper at ICLR 2020
Image Classification Through
Top-Down Image Pyramid Traversal
Anonymous authors
Paper under double-blind review
Ab stract
The available resolution in our visual world is extremely high, if not infinite. Ex-
isting CNNs can be applied in a fully convolutional way to images of arbitrary
resolution, but as the size of the input increases, they can not capture contextual
information. In addition, computational requirements scale linearly to the num-
ber of input pixels, and resources are allocated uniformly across the input, no
matter how informative different image regions are. We attempt to address these
problems by proposing a novel architecture that traverses an image pyramid in a
top-down fashion, while it uses a hard attention mechanism to selectively process
only the most informative image parts. We conduct experiments on MNIST and
ImageNet datasets, and we show that our models can significantly outperform fully
convolutional counterparts, when the resolution of the input is that big that the
receptive field of the baselines can not adequately cover the objects of interest.
Gains in performance come for less FLOPs, because of the selective processing
that we follow. Furthermore, our attention mechanism makes our predictions more
interpretable, and creates a trade-off between accuracy and complexity that can be
tuned both during training and testing time.
1	Introduction
Our visual world is very rich, and there is information of interest in an almost infinite number of
different scales. As a result, we would like our models to be able to process images of arbitrary
resolution, in order to capture visual information with arbitrary level of detail. This is possible with
existing CNN architectures, since we can use fully convolutional processing (Long et al. (2015)),
coupled with global pooling. However, global pooling ignores the spatial configuration of feature
maps, and the output essentially becomes a bag of features1. To demonstrate why this an important
problem, in Figure 1 (a) and (b) we provide an example of a simple CNN that is processing an image
in two different resolutions. In (a) we see that the receptive field of neurons from the second layer
suffices to cover half of the kid’s body, while in (b) the receptive field of the same neurons cover
area that corresponds to the size of a foot. This shows that as the input size increases, the final
representation becomes a bag of increasingly more local features, leading to the absence of coarse-
level information, and potentially harming performance. We call this phenomenon the receptive field
problem of fully convolutional processing.
An additional problem is that computational resources are allocated uniformly to all image regions,
no matter how important they are for the task at hand. For example, in Figure 1 (b), the same amount
of computation is dedicated to process both the left half of the image that contains the kid, and the
right half that is merely background. We also have to consider that computational complexity scales
linearly with the number of input pixels, and as a result, the bigger the size of the input, the more
resources are wasted on processing uninformative regions.
We attempt to resolve the aforementioned problems by proposing a novel architecture that traverses an
image pyramid in a top-down fashion, while it visits only the most informative regions along the way.
1There are forms of global pooling, like SPP (He et al. (2015)), that maintain spatial information in their
output. However, all kinds of global pooling are based on simple grouping operations, because their primary
goal is to reduce the spatial dimension of arbitrarily sized inputs. As a result, they cannot capture complex
non-linear relationships between the input features. In addition, methods like SPP use a predefined number of
bins, and consequently, they can only capture spatial relations in a limited number of different scales.
1
Under review as a conference paper at ICLR 2020
Figure 1: (a), (b) The receptive field problem of fully convolutional processing. A simple CNN
consisted of 2 convolutional layers (colored green), followed by a global pooling layer (colored red),
processes an image in two different resolutions. The shaded regions indicate the receptive fields of
neurons from different layers. As the resolution of the input increases, the final latent representation
becomes a bag of increasingly more local features, lacking coarse information. (c) A sketch of
our proposed architecture. The arrows on the left side of the image demonstrate how we focus on
image sub-regions in our top-down traversal, while the arrows on the right show how we combine the
extracted features in a bottom-up fashion.
In Figure 1 (c) we provide a simplified sketch of our approach. We start at level 1, where we process
the input image in low resolution, to get a coarse description of its content. The extracted features
(red cube) are used to select out of a predefined grid, the image regions that are worth processing in
higher resolution. This process constitutes a hard attention mechanism, and the arrows on the left
side of the image show how we extend processing to 2 additional levels. All extracted features are
combined together as denoted by the arrows on the right, to create the final image representation that
is used for classification (blue cube).
We evaluate our model on synthetic variations of MNIST (LeCun et al., 1998) and on ImageNet
(Deng et al., 2009), while we compare it against fully convolutional baselines. We show that when
the resolution of the input is that big, that the receptive field of the baseline2 covers a relatively
small portion of the object of interest, our network performs significantly better. We attribute this
behavior to the ability of our model to capture both contextual and local information by extracting
features from different pyramid levels, while the baselines suffer from the receptive field problem.
Gains in accuracy are achieved for less floating point operations (FLOPs) compared to the baselines,
due to the attention mechanism that we use. If we increase the number of attended image locations,
computational requirements increase, but the probability of making a correct prediction is expected
to increase as well. This is a trade-off between accuracy and computational complexity, that can
be tuned during training through regularization, and during testing by stopping processing on early
levels. Finally, by inspecting attended regions, we are able to get insights about the image parts that
our networks value the most, and to interpret the causes of missclassifications.
2	Related Work
Attention. Attention has been used very successfully in various problems (Bahdanau et al., 2014;
Xu et al., 2015; Larochelle & Hinton, 2010; Gregor et al., 2015; Denil et al., 2012). Most similar to
2When we refer to the receptive field of a CNN, we refer to the receptive field of the neurons that belong to
the last convolutional layer.
2
Under review as a conference paper at ICLR 2020
our work, are models that use recurrent neural networks to adaptively attend to a sequence of image
regions, called glimpses (Ba et al. (2014); Mnih et al. (2014); Eslami et al. (2016); Ba et al. (2016);
Ranzato (2014)). There are notable technical differences between such models and our approach.
However, the difference that we would like to emphasize, is that we model the image content as a
hierarchical structure, and we implicitly create a parsing tree (Zhu et al. (2007)), where each node
corresponds to an attended location, and edges connect image regions with sub-regions (an example
is provided in Appendix A.1). If we decide to store and reuse information, building such a tree
structure offers a number of potential benefits, e.g. efficient indexing. We consider this an important
direction to explore, but it is beyond the scope of the current paper.
Multi-scale representations. We identify four broad categories of multi-scale processing methods.
(1) Image pyramid methods extract multi-scale features by processing multi-scale inputs (Eigen et al.
(2014); Pinheiro & Collobert (2014); Najibi et al. (2018)). Our approach belongs to this category. (2)
Encoding schemes take advantage of the inherently hierarchical nature of deep neural nets, and reuse
features from different layers, since they contain information of different scale (Liu et al. (2016);
He et al. (2014); Chen et al. (2018)). (3) Encoding-Decoding schemes follow up the feed-forward
processing (encoding) with a decoder, that gradually recovers the spatial resolution of early feature
maps, by combining coarse with fine features (Lin et al. (2017); Ronneberger et al. (2015)). (4)
Spatial modules are incorporated into the feed forward processing, to alter the feature extraction
between layers (Yu & Koltun (2015); Chen et al. (2017); Wang et al. (2019)).
Computational efficiency. We separate existing methods on adjusting the computational cost of
deep neural networks, into four categories. (1) Compression methods aim to remove redundancy from
already trained models (LeCun et al. (1990); Hinton et al. (2015); Yu et al. (2018)). (2) Lightweight
design strategies are used to replace network components with computationally lighter counterparts
(Howard et al. (2017); Iandola et al. (2016); Rastegari et al. (2016); Jaderberg et al. (2014); Wang
et al. (2017)). (3) Partial computation methods selectively utilize parts of a network, creating paths
of computation with different cost (Huang et al. (2017); Wu et al. (2018); Larsson et al. (2016);
Figurnov et al. (2017); Zamir et al. (2017)). (4) Attention methods selectively process parts of the
input, based on their importance for the task at hand (Katharopoulos & Fleuret (2019); Ramapuram
et al. (2018); Levi & Ullman (2018)). This is the strategy that we follow in our architecture.
3	Architecture
We present our architecture by walking through the example in Figure 2, where we process an image
with original resolution of 128 × 128 px (1 in the top left corner). In the fist level, we downscale the
image to 32 × 32 px and pass it through the feature extraction module, in order to produce a feature
vector V1 that contains a coarse description of the original image. The feature extraction module
is a CNN that accepts inputs in a fixed resolution, that we call base resolution. We can provide V1
as direct input to the classification module in order to get a rapid prediction. If we end processing
here, our model is equivalent to a standard CNN operating on 32 × 32 px inputs. However, since the
original resolution of the image is 128 × 128 px, we can take advantage of the additional information
by moving to the second processing level.
In the second level, we feed the last feature map, F1, of the feature extraction module, to the location
module. The location module considers a number of candidate locations within the image described
by F1 , and predicts how important it is to process in higher detail each one of them. In this particular
example, the candidate locations form a 2 × 2 regular grid (2 ), and the location module yields 4
probabilities, that we use as parameters of 4 Bernoulli distributions in order to sample a hard attention
mask (3 ). Based on this mask, we crop the corresponding regions, and we pass them through the
feature extraction module, creating V21 and V23. If we want to stop at this processing level, we can
directly pass V21 and V23 through the aggregation module (skipping the merging module 4 ). The
aggregation module combines the features from individual image regions into a single feature vector
V1agg , that describes the original image solely based on fine information. This means that V1agg is
complementary to V1 , and both vectors are combined by the merging module, which integrates fine
information (V1agg) with its context (V1), creating a single comprehensive representation V10. Then,
V10 can be used for the final prediction.
3
Under review as a conference paper at ICLR 2020
① Input image (128 X 128 PX)
128 x 128 px
Figure 2: Three unrolled processing levels of our architecture. The network starts by processing
the coarsest scale (Feature Extraction Module), and uses the extracted features to decide where to
focus (Location Module). Features extracted from subsequent detail levels are combined together
(Aggregation Module), and then are used to enrich the features from the coarser scales (Merging
Module) before the final classification (Classification Module).
We can extend our processing to a third level, where F21 and F23 are fed to the location module to
create two binary masks. No locations are selected from the image patch described by V23 , and the
aggregation module only creates V2a1gg . Then, we start moving upwards for the final prediction. In
Appendix A.2 we provide additional details about the modules of our architecture. In Appendix A.3
we express the feature extraction process with a single recursive equation.
4	Training
Our model is not end-to-end differentiable, because of the Bernoulli sampling involved in the location
selection process. To overcome this problem, we use a variant of REINFORCE (Williams, 1992):
1	1 X dlogp(yi[li,χi,w)	i	dlogp(lilχi,w)]
LF = N∙M 工[----------------∂W--------+ λf (log汉yi|l ,xi,W)- b)---------∂W-------]	⑴
i=1
where N ∙ M is the number of images We use for each update, Xi is the ith image, yi is its label,
and w are the parameters of our model. p(li|xi, w) is the probability that the sequence of locations
li is attended for image xi, andp(yi|li, xi, w) is the probability of predicting the correct label after
attending li . The size of our original batch B is N, but in the derivation of (1) we approximate
with a Monte Carlo estimator of M samples, the expectation Pli p(li∣Xi,w) [dlogp(dWl 'xi'w) +
logp(yi∖li,Xi,w)dlogPdwIxi,w)] for each image Xi in B. Based on this, for simplicity we just
consider that our batch has size N ∙ M. b is a baseline that we use to reduce the variance of our
estimators, and λf is a weighting hyperparameter. The first term of LF allows us to update the
parameters in order to maximize the probability of each correct label. The second term allows us to
update the location selection process, according to the utility of attended locations for the prediction
of the correct labels. In Appendix A.4.1 we provide the exact derivation of learning rule (1).
We experimentally identified two problems related to (1). First, our model tends to attend all the
available locations, maximizing the computational cost. To regulate this, we add the following term:
NM	2
Rt = -λt2 (NM X p -Ct)	⑵
i=1
4
Under review as a conference paper at ICLR 2020
where plti approximates the expected number of attended locations for image xi base on li , and
quantity Nm PM=N Pti calculates the average expected number of attended locations per image in
our augmented batch of NM images. The purpose of Rt is to make the average number of attended
locations per image equal to ct , which is a hyperparameter selected according to our computational
cost requirements. λt is simply a weighting hyperparameter.
The second problem we identified while using learning rule (1), is that the learned attention policy
may not be diverse enough. In order to encourages exploration, we add the following term:
1g1	2
Rr = -λr-	~(Pk - Cr)
gk=12
(3)
where pk is the average probability of attending the kth out of the g candidate locations, that the
location module considers every time it is applied during the processing of our NM images. Rr
encourages the location module to attend with the same average probability cr , locations that are
placed at different regions inside the sampling grid. λr is a weighting hyperparameter. In Appendix
A.4.2 we provide additional details about terms (2) and (3). Our final learning rule is the following:
∂Rt	∂Rr
Lr = LF + ”--+ F-
∂w	∂w
(4)
Gradual Learning. The quality of the updates we make by using (4), depends on the quality of the
Monte Carlo estimators. When we increase the number of processing levels that our model can go
through, the number of location sequences that can be attended increases exponentially. Based on
this, if we allow our model to go through multiple processing levels, and we use a small number of
samples in order to have a moderate cost per update, we expect our estimators to have high variance.
To avoid this, we separate training into stages, where in the first stage we allow only 2 processing
levels, and at every subsequent stage an additional processing level is allowed. This way, we expect
the location module to gradually learn which image parts are the most informative, narrowing down
the location sequences that have a considerable probability to be attended at each training stage, and
allowing a small number of samples to provide acceptable estimators. We call this training strategy
gradual learning, and the learning rule that we use at each stage s, is the following:
∂Rts	∂Rrs
Ls=LF+房+房
(5)
where Lsr is equivalent to (4), with s superscripts indicating that the hyperparameters of each term
can be adjusted at each training stage. The maximum number of possible training stages depends on
the resolution of the training images.
Multi-Level Learning. By following the gradual learning paradigm, a typical behavior that we
observe is the following. After the first stage of training, our model is able to classify images
with satisfactory accuracy by going through 2 processing levels. After the second stage of training,
our model can go through 3 processing levels, and as we expect, accuracy increases since finer
information can be incorporated. However, if we force our model to stop processing after 2 levels,
the obtained accuracy is significantly lower than the one we were able to achieve after the first stage
of training. This is a behavior that we observe whenever we finish a new training stage, and it is an
important problem, because we would like to have a flexible model that can make accurate predictions
after each processing level. To achieve this, we introduce the following learning rule:
s
Lml = X λz ∙ Lz	(6)
z =0
where Lrz is learning rule (5) with z + 1 processing levels allowed for our model. Each λzs is a
hyperparameter that specifies the relative importance of making accurate predictions after processing
level z + 1, while we are at training stage s.
5 Experimental Evaluation
We experiment with two datasets, MNIST (LeCun et al., 1998) and ImageNet (Deng et al., 2009).
MNIST is a small dataset that we can easily modify in order to test different aspects of our models’
5
Under review as a conference paper at ICLR 2020
56 × 56 px
28 × 28 px
14 × 14
ISINW UlBld


Figure 3: Example images from our MNIST-based datasets, along with the attended locations of
M328 models. We blur the area outside the attended locations, because it is processed only in lower
resolution during the first processing level. This way we aim to get a better understanding of what
our models “see”.
behavior, e.g. the localization capabilities of our attention mechanism. ImageNet has over a million
training images, and allows us to evaluate our model on a large scale.
5.1	Experiments With MNIST
Data. MNIST is a dataset with images of handwritten digits that range from 0 to 9, leading to 10
different classes. All images are grayscale, and their size is 28 × 28 pixels. The dataset is split in
a training set of 55, 000 images, a validation set of 5, 000 images, and a test set of 10, 000 images.
We modify the MNIST images by placing each digit at a randomly selected location inside a black
canvas of size 56 × 56. We refer to this MNIST variation as plain MNIST, and we further modify
it to create noisy MNIST and textured MNIST. In the case of noisy MNIST, we add salt and pepper
noise by corrupting 10% of the pixels in every image, with a 0.5 ratio between black and white noise.
In the case of textured MNIST, we add textured background that is randomly selected from a large
image depicting grass. Example images are provided in the first column of Figure 3.
Models. We create 3 pairs of models {Mi, BLi}i3=1, each composed of a version Mi of our
architecture, and a corresponding fully convolutional baseline BLi for comparison. Ideally, we would
like the feature extraction processes of the models that we compare to be consisted of the same layers,
in order to eliminate important factors of performance variation like the type or the number of layers.
To this end, in every pair of models, the baseline BLi is equivalent to the feature extraction module
of Mi followed by the classification module, with 2 fully connected layers in between. The 2 fully
connected layers account for the aggregation and merging modules, which could be considered part
of the feature extraction process in Mi . We create 3 pairs of models because we want to study how
our architecture performs relatively to fully convolutional models with different receptive fields. To
achieve this, BL1 has 1 convolutional layer and receptive field 3 × 3 px, BL2 has 2 convolutional
layers and receptive field 8 × 8 px, and BL3 has 3 convolutional layers and receptive field 20 × 20 px.
We would like to note that all models {Mi}i3=1 have base resolution 14 × 14 px, and their location
modules consider 9 candidate locations which belong to a 3 × 3 regular grid with 50% overlap. In
Appendix A.5.1 we provide the exact architectures of all models.
Training. We describe how we train one pair of models (Mi, BLi) with one of the 3 MNIST-based
datasets that we created. The same procedure is repeated for all datasets, and for every pair of models.
The original resolution of our dataset is 56 × 56 px, and we rescale it to 2 additional resolutions that
6
Under review as a conference paper at ICLR 2020
(a) PIain MNIST
(b) Plain MNlST
(C) PIain MNIST
5000	10000
15000
500	1000	1500
0	5000	10000	15000	20000
(d) textured MNIST
(e) textured MNIST
(f) textured MNIST
O 5000 IOOOO 15000	20000
___________(g) noisy MNlST
Ooo
9 8 7
≡wo+->——A3e-r∞4
O 5000 IOOOO 15000	20000
KFLOPs per image
O	5000 IOOOO	15000
___________(h) noisy MNlST
908070
O	5000 IOOOO 15000
KFLOPs per image
0	500	1000	1500
___________(j) noisy MNIST
908070
O 500 IOOO 1500
KFLOPs per image
Figure 4: Experimental results on plain, textured and noisy MNIST. The differences in accuracy
between many models were very small, and as a result, in the provided graphs we report the average
of 20 different evaluations on the validation set, where each time we randomly change the positions
of the digits inside the images. For textured and noisy MNIST, we randomly change the background
and the noise pattern of each image as well.
are equal to 28 × 28 px, and 14 × 14 px (example images are provided in the first 3 columns of
Figure 3). We split our training procedure in 3 sessions, where at each session we train our models
with images of different resolution. In the first session we use images of size 14 × 14 px, and we
refer to the resulting models as BLi14 and Mi14 . We note that since the resolution of the input is
equal to the base resolution of Mi , our model can go through just 1 processing level and the location
module is not employed.
In our second training session we use images of resolution 28 × 28 px, and the increased resolution
of the input allows Mi to use the location module and extend processing to 2 processing levels. Based
on this, we are able to train Mi multiple times by assigning different values to hyperparameter ct (2),
resulting to models that attend a different average number of locations per image, and as a result, they
have different average computational cost and accuracy. We refer to the models from this training
session as BLi28 and Mi28,nc, where nc is the average number of locations that Mi attended on the
validation set, while trained with ct = c. We also define Mi28 as the set of all Mi28,nc models we
trained in this session. In the third training session we use images of resolution 56 × 56 px, and Mi
is able to go through 3 processing levels. Following our previous notation, we refer to the models of
this session as BLi56 and Mi56,nc, while we use Mi56 to denote the set of all Mi56,nc. In Appendix
A.5.2 we provide additional details about the training sessions, along with the exact hyperparameters
that we used to obtain the results that follow.
Results. In the first row of Figure 4, we present performance graphs for all models {Mi , BLi }i3=1
on plain MNIST. We note that the annotations under all models {Mi }i3=1 indicate the average number
of locations nc that we described in the training section. We start by examining Figure 4 (a), where
7
Under review as a conference paper at ICLR 2020
Figure 5: Two missclasication examples of model M328,2.2, that demonstrate the interpretability of
our models’ decisions because of the employed attention mechanism.
we depict the performance of models M3 and BL3 on images of different resolution. The overlapping
blue markers indicate models M314 and BL314, which achieve the same accuracy level. The green
markers denote models BL238 and M328,2.23, and as we expect, they achieve higher accuracy compared
to BL133 4 and M314, since they operate on images of higher resolution. The red markers denote
models M356 and BL356 , and by observing the performance of all M3 models, we see that as the
resolution of the input and the number of attended locations increases, accuracy increases as well,
which is the expected trade-off between computation and accuracy. This trade-off follows a form of
logarithmic curve, that saturates in models M356 .
In the last column of the first row of Fig. 3, we provide a representative example of how the attention
mechanism of M328,2.2 operates. We observe that the location module is capable of focusing on
the digit, and the generally good performance of the location module is reflected on the accuracy
of the model, which is over 99%. However, BL328 performs slightly better, and in an attempt to
28 2.2
understand why, in Fig. 5 we provide two example images which are missclassified by M3 , , but
are correctly classified by BL238 . In both examples, the attended locations partially cover the digits,
leading 5 to be interpreted as 1, and 3 to be interpreted as 7. Of course, we are not always able to
interpret the cause of a missclassification by inspecting the attended locations, but as the provided
examples show, we may be able to get important insights. Besides the performance of our attention
28 2.2
mechanism, we don’t expect M3 , to achieve higher accuracy compared to BL238. We remind that
the images provided to the models are of size 28 × 28 px, the digit within each image covers an area
of maximum size 14 × 14 px, and the receptive field of the baseline is 20 × 20 px. As a result, the
receptive field can perfectly cover the area of the digit, and the extracted features contain both coarse
and fine information. Consequently, our model doesn’t offer any particular advantage in terms of
feature extraction that could be translated in better accuracy. This is something that we expect to
change if the resolution of the input increases, or if the receptive field of the baseline gets smaller, as
in the cases of BL2 and BL1.
In Fig. 4 (b) we present the performance of models M2 and BL2 , and the main difference we observe
with (a), is that BL526 demonstrates lower accuracy compared to BL228 . We attribute this behavior to
the fact that the receptive field of BL256 covers less than 10% of the area occupied by each digit, and
as a result, BL526 is unable to extract coarse level features which are valuable for classification. Based
on this hypothesis, we are able to interpret the behavior of the models in (c) as well. We observe that
M128 and M156 significantly outperform BL218 and BL516, while BL516 demonstrates accuracy that is
even lower than that of BL114. We note that the receptive field of BL218 covers a little bit over 4.5%
of the area occupied by each digit, while for BL516 this percentage drops to approximately 1%.
In the second and third row of Figure 4, we present our results on textured and noisy MNIST
respectively. Our previous analysis applies to these results as well. In addition, we would like to note
that our attention mechanism is robust to textured background and salt and pepper noise, as we can
see in the respresentative examples provided in the last column of Fig. 3. In Appendix A.5.3 we
provide some additional remarks on the results reported in Fig. 4.
5.2	Experiments On ImageNet
Data. We use the ILSVRC 2012 version of ImageNet, which contains 1, 000 classes of natural
images. The training and validation sets contain 1, 281, 167 and 50, 000 images, respectively. The
average resolution of the original images is over 256 px per dimension. All images are color images,
3We don’t depict other M328 models that were trained with different ct values, because they don’t demonstrate
any significant changes in accuracy, and would reduce the clarity of our graphs.
8
Under review as a conference paper at ICLR 2020
504540353025
≡I—dol — A□ejn□□<
O	500	IOOO	1500	2000	2500
MFLOPs per image
Figure 6: Experimental results on ImageNet. In the y-axis we provide the top-1 accuracy on the
validation set, while in the x-axis we provide the required number of FLOPs (×106) per image.
but for simplicity, when we refer to resolution, we will drop the last dimension that corresponds to
the color channels.
Models. We create two pairs of models {Mi , BLi }i2=1 by following the same design principles
we presented in Section 5.1. Model BL1 has 3 convolutional layers and receptive field 18 × 18 px,
while BL2 has 4 convolutional layers and receptive field 38 × 38 px. Models {Mi }i2=1 have base
resolution 32 × 32 px, and their location modules consider 9 candidate locations which belong to a
3 × 3 regular grid with 50% overlap. In Appendix A.6.1 we provide the architectures of all models.
Training. We follow the training procedure we described in Section 5.1. The main difference is
that we rescale our images to 4 different resolutions {r × r|r ∈ {32, 64, 128, 256}}, resulting to 4
training sessions. We follow the notation we introduced in Section 5.1l, and we denote the models
that result from our first training session as Mi32 and BLi32 . We refer to the models that result from
the other 3 sessions as Mir,nc and BLir, while we use Mir to denote the set of all Mir,nc models that
are trained with different ct values. For the second training session we have r = 64, for the third
r = 128, and for the fourth r = 256, while i ∈ {1, 2}. Finally, we use multi-level learning rule (6) to
train models that are able to demonstrate high accuracy after each processing level. The resulting
models are denoted by {Miml}i2=1. In Appendix A.6.2 we provide additional training details.
Results. In Figure 6 we provide our experimental results. As in Figure 4, markers of different color
denote models which are trained with images of different resolution, while the annotations next to
markers that correspond to models {Mi}i2=1 indicate the average number of locations nc. In the first
row of Fig. 6, we depict the performance of models M2 and BL2 . First, we would like to note that
we observe the trade-off between accuracy and computatinal complexity that we have identified in
Fig. 4. When the number of required FLOPs increases by processing inputs of higher resolution, or
by attending a bigger number of locations, accuracy increases as well.
By inspecting the behavior of individual models, we observe that BL624 has comparable performance
to M264 models, while this is the case for BL2128 and M2128,14.5 models as well. However, the
behavior of models BL2256 and M2256 is different. Both M2256,32.8 and M2256,42 achieve higher
accuracy compared to BL2256 , even if they require much smaller number of FLOPs. Our explanation
is based on the receptive field problem that we relate to fully convolutional models. The receptive
field of BL264 covers around 35% of the image area, while this percentage drops close to 9% for
9
Under review as a conference paper at ICLR 2020
Figure 7: Examples of attended locations from a M1128 model. Image parts are blurred according to
the resolution they are processed.
BL2128, and to 2% for BL2256 . As a result, we assume that the features extracted by BL2256 lack
coarse information, and we expect this phenomenon to become even more intense for models BL1 ,
since they have smaller receptive field. Indeed, as we can see in the second row of Fig. 6, the
performance gap between M1 and BL1 models is bigger. M164, M1128 and M1256 achieve higher
accuracy compared to BL614, BL1128 and BL2156 respectively.
We would also like to comment on the behavior of models M2ml and M1ml that we provide in the
graphs of Fig. 6. We observe that both M2ml and M1ml are able to maintain comparable performance
to models M2r and M1r respectively, in all processing levels. This shows that we are able to adjust
the computational requirements of our models during testing time, by controlling the number of
processing levels that we allow them to go through. This is beneficial when we face constraints in the
available computational resources, but also when we are processing images which vary in difficultly.
Easier images can be classified after a few processing levels, while for harder ones we can extend
processing to more levels. Finally, in Figure 7 we provide examples of attended image locations.
6 Conclusion
We proposed a novel architecture that is able to process images of arbitrary resolution without
sacrificing spatial information, as it typically happens with fully convolutional processing. This is
achieved by approaching feature extraction as a top-down image pyramid traversal, that combines
information from multiple different scales. The employed attention mechanism allows us to adjust
the computational requirements of our models, by changing the number of locations they attend.
This way we can exploit the existing trade-off between computational complexity and accuracy.
Furthermore, by inspecting the image regions that our models attend, we are able to get important
insights about the causes of their decisions. Finally, there are multiple future research directions
that we would like to explore. These include the improvement of the localization capabilities of our
attention mechanism, and the application of our model to the problem of budgeted batch classification.
In addition, we would like our feature extraction process to become more adaptive, by allowing
already extracted features to affect the processing of image regions that are attended later on.
10
Under review as a conference paper at ICLR 2020
References
Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual
attention. arXiv preprint arXiv:1412.7755, 2014.
Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast
weights to attend to the recent past. In Advances in Neural Information Processing Systems, pp.
4331-4339, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully
connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848,
2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
Misha Denil, Loris Bazzani, Hugo Larochelle, and Nando de Freitas. Learning where to attend with
deep architectures for image tracking. Neural computation, 24(8):2151-2184, 2012.
David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a
multi-scale deep network. In Advances in neural information processing systems, pp. 2366-2374,
2014.
SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton,
et al. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in
Neural Information Processing Systems, pp. 3225-3233, 2016.
Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and
Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1039-1048, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: A recurrent
neural network for image generation. In International Conference on Machine Learning, pp.
1462-1471, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep
convolutional networks for visual recognition. In European conference on computer vision, pp.
346-361. Springer, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep
convolutional networks for visual recognition. IEEE transactions on pattern analysis and machine
intelligence, 37(9):1904-1916, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Andrew G Howard. Some improvements on deep convolutional neural network based image classifi-
cation. arXiv preprint arXiv:1312.5402, 2013.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
11
Under review as a conference paper at ICLR 2020
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q Wein-
berger. Multi-scale dense networks for resource efficient image classification. arXiv preprint
arXiv:1703.09844, 2017.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. SqUeezenet: Alexnet-level accuracy with 50x fewer parameters andj 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
Angelos Katharopoulos and Francois Fleuret. Processing megapixel images with deep attention-
sampling models. arXiv preprint arXiv:1905.03711, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Hugo Larochelle and Geoffrey E Hinton. Learning to combine foveal glimpses with a third-order
boltzmann machine. In Advances in neural information processing systems, pp. 1243-1251, 2010.
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks
without residuals. arXiv preprint arXiv:1605.07648, 2016.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598-605, 1990.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Hila Levi and Shimon Ullman. Efficient coarse-to-fine non-local module for the detection of small
objects. arXiv preprint arXiv:1811.12152, 2018.
Tsung-Yi Lin, Piotr Dollar, Ross B Girshick, Kaiming He, Bharath Hariharan, and Serge J Belongie.
Feature pyramid networks for object detection. In CVPR, volume 1, pp. 4, 2017.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and
Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision,
pp. 21-37. Springer, 2016.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431-3440, 2015.
Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In
Advances in neural information processing systems, pp. 2204-2212, 2014.
Mahyar Najibi, Bharat Singh, and Larry S Davis. Autofocus: Efficient multi-scale inference. arXiv
preprint arXiv:1812.01600, 2018.
Pedro HO Pinheiro and Ronan Collobert. Recurrent convolutional neural networks for scene labeling.
In 31st International Conference on Machine Learning (ICML), number EPFL-CONF-199822,
2014.
Jason Ramapuram, Maurits Diephuis, Russ Webb, and Alexandros Kalousis. Variational saccading:
Efficient inference for large resolution images. arXiv preprint arXiv:1812.03170, 2018.
Marc’Aurelio Ranzato. On learning where to look. arXiv preprint arXiv:1405.5488, 2014.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, pp. 525-542. Springer, 2016.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241. Springer, 2015.
12
Under review as a conference paper at ICLR 2020
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Huiyu Wang, Aniruddha Kembhavi, Ali Farhadi, Alan L Yuille, and Mohammad Rastegari. Elastic:
Improving cnns with dynamic scaling policies. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2258-2267, 2019.
Min Wang, Baoyuan Liu, and Hassan Foroosh. Factorized convolutional neural networks. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 545-553, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman,
and Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8817-8826, 2018.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048-2057, 2015.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv
preprint arXiv:1511.07122, 2015.
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao,
Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance score
propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 9194-9203, 2018.
Amir R Zamir, Te-Lin Wu, Lin Sun, William B Shen, Bertram E Shi, Jitendra Malik, and Silvio
Savarese. Feedback networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1308-1317, 2017.
Song-Chun Zhu, David Mumford, et al. A stochastic grammar of images. Foundations and Trends®
in Computer Graphics and Vision, 2(4):259-362, 2007.
13
Under review as a conference paper at ICLR 2020
Figure 8: The parsing tree that is implicitly created by our model according to the example of Figure
1 (c). Nodes correspond to the attended image locations, and edges relate each location with its
constituent parts.
A	Appendix
A.1 Implicit Parsing Tree
Following the example of Figure 1, in Figure 8 we provide the parsing tree that our model implicitly
creates.
A.2 Modules
Feature extraction module. It is a CNN that receives as input images of fixed resolution h × w × c,
and outputs feature vectors of fixed size 1 × f. In the example of Fig. 2, h = w = 32.
Location module. It receives as input feature maps of fixed size fh × fw × fc , and returns g
probabilities, where g corresponds to the number of cells in the grid of candidate locations. In the
example of Fig. 2, g = 4 since we are using a 2 × 2 grid.
Aggregation module. It receives g vectors of fixed size 1 × f, and outputs a vector of fixed size
1 × f . The g input vectors describe the image regions inside a k × k grid. Image regions that were not
selected by the location module, are described by zero vectors. The g input vectors are reorganized
into a 1 × k × k × f tensor, according to the spatial arrangement of the image regions they describe.
The tensor of the reorganized input vectors is used to produce the final output.
Merging module. It receives two vectors of fixed size 1 × f, concatenates them into a single vector
of size 1 × 2f, and outputs a vector of fixed size 1 × f.
Classification module. It receives as input a vector of fixed size 1 × f, and outputs logits of fixed
size 1 × c, where c is the number of classes. The logits are fed into a softmax layer to yield class
probabilities.
A.3 Equation Of Feature Extraction
The feature extraction process of our model can be described with the following recursive equation:
Vn=Vn ㊉ agg (Ioc({ Vm |m ∈ IVnD)	⑺
14
Under review as a conference paper at ICLR 2020
where ㊉ denotes the merging module, agg(∙) denotes the aggregation module, loc(∙) denotes the
outcome of the Bernoulli sampling that is based on the output of the location module, and lVn is the
set of indexes that denote the candidate locations related to the image region described by Vn . When
recursion ends, Vm0 = Vm ∀m.
A.4 Training
A.4. 1 Learning Rule Derivation
The REINFORCE rule naturally emerges if we optimize the log likelihood of the labels, while
considering the attended locations as latent variables (Ba et al., 2014). For a batch of N images, the
log likelihood is given by the following relation:
NN
log p(yi|xi, w) =	log	p(li|xi, w)p(yi|li, xi, w),	(8)
i=1	i=1	li
Nli g	i	li	i	li
P(Ii |xi, W) = Y Y P(Ij,k |xi,W)Uj,k ∙ (I- P(Ij,k |xi,W))I-Uj,k	(8a)
j=1 k=1
where xi is the ith image in the batch, yi is its label, and W are the parameters of our model. P(li |xi, W)
is the probability that the sequence of locations li is attended for image xi, and P(yi|li, xi, W) is the
probability of predicting the correct label after attending li . Equation 8 describes the log likelihood
of the labels in terms of all location sequences that could be attended. Equation 8a shows that
P(li |xi , W) is a product of Bernoulli distributions. Nli is the number of times the location module
is applied while sequence li is attended, and g is the number of candidate locations considered by
the location module. In the example of Figure 2, Nli = 3 and g = 4. llj,k is the kth out of the g
candidate locations, that are considered the jth time the location module is applied while attending
li. ulji,k ∈ {0, 1} is equal to 1 when location ljli,k is attended, and 0 when it is not. P(llji,k |xi, W) is
computed by the location module, ulji,k is the outcome of Bernoulli sampling, and P(yi|li, xi, W) is
computed by the classification module.
We use Jensen’s inequality in equation 8 to derive the following lower bound on the log likelihood:
NN
log P(yi|xi, W) ≥	P(li |xi, W) log P(yi|li, xi, W) = F	(9)
i=1	i=1 li
By maximizing the lower bound F, we expect to maximize the log likelihood. The update rule that
we use is the partial derivative of F with respect to W, normalized by the number of images in the
batch. We get:
1 ∂F
N ∂w
∂ log p(yi∣li,Xi,w)
∂w
+ logp(Vi∖li, Xi, W) dp(l∂Wi, w) i
⇒
1 ∂F 1 N	i	∂ log p(yi|li, xi, W)	i	∂ log p(li|xi, W)
N∂W = N££p(l lxi,w)[-----------------∂W--------+logp(yi|l,xi,w) —∂W—J (IO)
i=1 li
To derive equation 10 we used the log derivative trick. As we can see, for each image xi we need to
calculate an expectation according to p(li |xi, W). We approximate each expectation with a Monte
Carlo estimator of M samples:
1 ∂F
N ∂w
1 ∂F
N ∂w
N xx Mm xx hd log p(y∂Wm,xi,w)+log p(yl,Xi,w)
i=1	m=1
∂ log p(li,m∣Xi, W)
∂w
(11)
]
We get samples from p(li|xi, W) by repeating the processing of image xi. li,m is the sequence of
locations that is attended during the mth time we process image xi . In order to reduce the variance of
15
Under review as a conference paper at ICLR 2020
the estimators, we use the baseline technique from Xu et al. (2015). In particular, the baseline we use
is the exponential moving average of the log likelihood, and is updated after the processing of each
batch during training. Our baseline after the nth batch is the following:
NM
bn = 0.9 ∙ bn-1 +0.1 ∙ NM X logp(yn∣li,n, X，, W)	(12)
where xin is the ith image in the nth batch, yin is its label, and li,n is the corresponding attended
sequence of locations. Since we use M samples for the Monte Carlo estimator of each image, we
simply consider that our batch has size NM to simplify the notation. Our updated learning rule is the
following:
LF
1 X h∂logp(yi∣li,Xi,w)
NM 乙[ ∂W
i=1
+ λf(logp(yi∣li,xi, w) - b) HlogPdwIxi, W)]
(13)
For simplicity, we drop the indexes that indicate the batch we are processing. (13) is the learning rule
we presented in (1), and this concludes our derivation.
A.4.2 Regularization Terms
We provide the exact equations that we use to calculate quantities plti and pk in regularization terms
Rt (2) and Rr (3) respectively. We have:
Nli g
plt = X X p(lij,k|xi, w)	(14)
j=1 k=1
MN Nli
1i
Pk = PMN Nli ∑ Ep(Ij,k lxi,w)	(15)
Based on the notation we introduced in Appendix A.4.1, Plt approximates the expected number
of attended locations for image xi, by summing the probabilities from all Bernoulli distributions
considered during a single processing of xi under li. Pk computes the average probability of attending
the kth out of the g candidate locations, that the location module considers every time it is applied.
The average is calculated by considering all the times the location module is applied during the
processing of the N M images in our augmented batch. Finally, we would like to note that the values
of ct and cr are interdependent.
A.5 Experiments With MNIST
A.5. 1 Architectures
In Tables 1, 2 and 3, we provide the exact architectures of models (M3, BL3), (M2, BL2) and
(M1,BL1).
A.5.2 Training Details
We provide details about training one pair of models (Mi , BLi) with one of the 3 MNIST-based
datasets, and the exact same procedure applies to all pairs and to all datasets. In our first training
session we optimize the cross entropy loss to train BLi, and we use learning rule (4) for Mi . We
remind that because of the input resolution, Mi goes through only 1 processing level, and behaves
as a regular CNN which is consisted of the feature extraction module followed by the classification
module. As a result, the only term of learning rule (4) that we actually use, is the first term of LF,
and we end up optimizing the cross entropy of the labels. For both models we use the following
hyperparameters. We use learning rate 0.01 that drops by a factor of 0.2 after the completion of
80% and 90% of the total number of training steps. We train for 70 epochs, and we use batches of
size 128. We use the Adam optimizer (Kingma & Ba, 2014) with the default values of β1 = 0.9,
16
Under review as a conference paper at ICLR 2020
	Layer Type	Weights’ Size (w/o biases)	Stride			Padding	Activation	Output Size	Receptive Field Size	Params	FLOPs
	Input							14×14×1			
	Conv	3 × 3 × 1 × 32	1	×	1	SAME	Leaky ReLU	14 × 14 × 32	3×3	320	56, 488
	Max Pool	2×2×1	2	×	2	VALID		7 × 7 × 32	4×4		
Feature	Conv	3 × 3 × 32 × 64	1	×	1	SAME	Leaky ReLU	7 × 7 × 64	8×8	18, 496	903, 168
Extraction	Max Pool	3×3×1	2	×	2	VALID		3 × 3 × 64	12 × 12		
Module	Conv	3 × 3 × 64 × 64	1	×	1	SAME	Leaky ReLU	3 × 3 × 64	20 × 20	36, 928	331, 776
	GAP							1 × 1 × 64			
	Output							3 × 3 × 64,			
								1 × 64			
Location	Input							3 × 3 × 64			
Module	Conv	3 × 3 × 64 × 9	1	×	1	VALID	Logistic	1×1×9		5193	5184
	Output							1×9			
Aggregation Module	Input Conv	3 × 3 × 64 × 64	1	×	1	VALID	Leaky ReLU	3 × 3 × 64 1 × 1 × 64		36, 928	36, 864
	Output							1 × 64			
Merging Module	Input FC	128 × 64					Leaky ReLU	1 × 128 1 × 64		8, 256	8, 192
	Output							1 × 64			
Classification	Input							1 × 64			
Module	Linear	64 × 10						1 × 10		650	640
	Output							1 × 10			
	Input							N×N×1			
	Conv	3 × 3 × 1 × 32	1	×	1	SAME	Leaky ReLU	N × N × 32	3×3	320	288N 2
	Max Pool	2×2×1	2	×	2	VALID		N/2 × N/2 × 32	4×4		
	Conv	3 × 3 × 32 × 64	1	×	1	SAME	Leaky ReLU	N/2 × N/2 × 64	8×8	18, 496	4, 608N 2
Baseline	Max Pool	3×3×1	2	×	2	VALID		N/4 × N/4 × 32	12 × 12		2, 304N 2
CNN	Conv	3 × 3 × 64 × 64	1	×	1	SAME	Leaky ReLU	N/4 × N/4 × 64	20 × 20	36, 928	
	GAP							1 × 1 × 64			
	FC	64 × 64					Leaky ReLU	1 × 64		4, 160	4, 096
	FC	64 × 64					Leaky ReLU	1 × 64		4, 160	4, 096
	Linear	64 × 10						1 × 10		650	640
	Output							1 × 10			
Table 1: The architectures of models M3 and BL3 that we used in our experiments with MNIST.
“GAP” denotes a global average pooling layer. The variable output sizes of the baseline CNN are
approximately calculated to preserve the clarity of our tables. Based on these approximate output
sizes, the computation of the number of FLOPs in the corresponding layers is approximate as well.
β2 = 0.999 and = 10-8. We use xavier initialization (Glorot & Bengio, 2010) for the weights,
and zero initialization for the biases. For regularization purposes, we use the following form of data
augmentation. Every time we load an image, we randomly change the position of the digit inside the
black canvas, as well as the noise pattern and the background, in case we are using noisy MNIST or
textured MNIST. This data augmentation strategy and the aforementioned hyperparameter’s values,
are used in the other two training sessions as well.
In the second training session we again optimize the cross entropy loss for BLi , and we use learning
rule (4) for Mi . The resolution of the input allows Mi to go through 2 processing levels, and all terms
of (4) contribute to the updates of our models’ parameters. We would like to note that we stop the
gradients’ flow from the location module to the other modules, and as a result, the second term of (1),
as well as the regularization terms (2) and (3), affect only the parameters of the location module. We
do this to have better control on how the location module learns, since we experienced the problems
we described in Section 4. In the same context, we set λf = 10-6, λr = 10-7 and λt = 10-7, which
lead to very small updates at every training step. For our Monte Carlo estimators we use 2 samples.
These hyperparameter values are used in the third training session as well. The models Mi which are
reported in Figure 4 and result from this training session (green circles), are trained with ct = 2.
In the third training session Mi can go through 3 processing levels, and we can train it by using either
learning rule (4), or gradual learning (5). Gradual learning evolves in 2 stages, where in the first
stage Mi can go through 2 processing levels, while in the second stage it can go through 3. The first
training stage is equivalent to the previous training session, since there is an equivalence between
going through a different number of processing levels and processing inputs of different resolution.
Based on that, we can directly move to the second training stage of gradual learning, by initializing
the variables of Mi with the learned parameters from one of the Mi28 models that we already trained.
Gradual learning creates an imbalance in terms of how Mi and BLi are trained, since it evolves in
multiple stages. However, if we see gradual learning as a method of training models with images
of gradually increasing resolution, it can be applied to the baselines as well. Based on this, we can
17
Under review as a conference paper at ICLR 2020
	Layer Type	Weights’ Size (w/o biases)	Stride			Padding	Activation	Output Size	Receptive Field Size			Params	FLOPs
Feature Extraction Module	Input Conv Max Pool Conv GAP Output	3 × 3 × 1 × 32 2×2×1 3 × 3 × 32 × 64	1 2 1	× × ×	1 2 1	SAME VALID SAME	Leaky ReLU Leaky ReLU	14×14×1 14 × 14 × 32 7 × 7 × 32 7 × 7 × 64 1 × 1 × 64 7 × 7 × 64, 1 × 64	3 4 8	× × ×	3 4 8	320 18, 496	56, 488 903, 168
Location Module	Input Conv Output	7 × 7 × 64 × 9	1	×	1	VALID	Logistic	7 × 7 × 64 1×1×9 1×9				28, 233	28, 224
Aggregation Module	Input Conv Output	3 × 3 × 64 × 64	1	×	1	VALID	Leaky ReLU	3 × 3 × 64 1 × 1 × 64 1 × 64				36, 928	36, 864
Merging Module	Input FC Output	128 × 64					Leaky ReLU	1 × 128 1 × 64 1 × 64				8, 256	8, 192
Classification Module	Input Linear Output	64 × 10						1 × 64 1 × 10 1 × 10				650	640
Baseline CNN	Input Conv Max Pool Conv GAP FC FC Linear Output	3 × 3 × 1 × 32 2×2×1 3 × 3 × 32 × 64 64 × 64 64 × 64 64 × 10	1 2 1	× × ×	1 2 1	SAME VALID SAME	Leaky ReLU Leaky ReLU Leaky ReLU Leaky ReLU	N×N×1 N × N × 32 N/2 × N/2 × 32 N/2 × N/2 × 64 1 × 1 × 64 1 × 64 1 × 64 1 × 10 1 × 10	3 4 8	× × ×	3 4 8	320 18, 496 4, 160 4, 160 650	288N 2 4, 608N 2 4, 096 4, 096 640
Table 2: The architectures of models M2 and BL2 that we used in our experiments with MNIST.
“GAP” denotes a global average pooling layer. The variable output sizes of the baseline CNN are
approximately calculated to preserve the clarity of our tables. Based on these approximate output
sizes, the computation of the number of FLOPs in the corresponding layers is approximate as well.
	Layer Type	Weights’ Size (w/o biases)	Stride			Padding	Activation	Output Size	Receptive Field Size		Params	FLOPs
Feature Extraction Module	Input Conv Max Pool GAP Output	3 × 3 × 1 × 64 2×2×1	1 2	× ×	1 2	SAME VALID	Leaky ReLU	14×14×1 14 × 14 × 64 7 × 7 × 64 1 × 1 × 64 7 × 7 × 64, 1 × 64	3 4	×3 ×4	640	112896
Location Module	Input Conv Output	7 × 7 × 64 × 9	1	×	1	VALID	Logistic	7 × 7 × 64 1×1×9 1×9			28, 233	28, 224
Aggregation Module	Input Conv Output	3 × 3 × 64 × 64	1	×	1	VALID	Leaky ReLU	3 × 3 × 64 1 × 1 × 64 1 × 64			36, 928	36, 864
Merging Module	Input FC Output	128 × 64					Leaky ReLU	1 × 128 1 × 64 1 × 64			8, 256	8, 192
Classification Module	Input Linear Output	64 × 10						1 × 64 1 × 10 1 × 10			650	640
Baseline CNN	Input Conv Max Pool GAP FC FC Linear Output	3 × 3 × 1 × 64 2×2×1 64 × 64 64 × 64 64 × 10	1 2	× ×	1 2	SAME VALID	Leaky ReLU Leaky ReLU Leaky ReLU	N×N×1 N × N × 64 N/2 × N/2 × 64 1 × 1 × 64 1 × 64 1 × 64 1 × 10 1 × 10	3 4	×3 ×4	640 4, 160 4, 160 650	576N 2 4, 096 4, 096 640
Table 3: The architectures of models M1 and BL1 that we used in our experiments with MNIST.
“GAP” denotes a global average pooling layer. The variable output sizes of the baseline CNN are
approximately calculated to preserve the clarity of our tables. Based on these approximate output
sizes, the computation of the number of FLOPs in the corresponding layers is approximate as well.
initialize the variables of BLi with the learned parameters of BLi28 , and then apply our standard
optimization to the cross entropy loss.
18
Under review as a conference paper at ICLR 2020
In practice, we observe that baselines are almost always benefited by gradual learning, while in some
cases our models achieve higher accuracy when they are trained from scratch with learning rule (4).
In general, gradual learning and most modifications to the initial learning rule (11), resulted from our
experimentation with ImageNet, which is a much bigger and more diverse dataset of natural images.
Consequently, our results on the MNIST-based datasets remain almost unchanged even if we simplify
our training procedure, e.g. by excluding term (3) from our training rule. However, we kept our
training procedure consistent both on the MNIST-based datasets and on ImageNet, we experimented
both with gradual learning and with training from scratch at every session, and in the results of Figure
4 we report the best performing models. Finally, the models Mi which are reported in Figure 4 and
result from this training session (red circles), are trained with ct = 6 and ct = 12.
A.5.3 Additional Remarks On The Results
In Figure 4 (e), BL526 achieves higher accuracy compared to BL228, which wasn’t the case in (b). We
hypothesize that the fine information provided by the increased resolution, is valuable to disentangle
the digits from the distracting background, and outweighs the lack of coarse level features that stems
from the receptive field size of BL256 . In addition, the differences in accuracy between M1 and BL1
models in Fig. 4 (f), are considerably bigger compared to the ones recorded in (c). This shows that
our models are more robust to distracting textured background compared to the baselines.
In Fig. 4 (g), the accuracy of M356,6.1 is lower compared to M328,2.2. This is surprising, because
M356,6.1 is processing images of higher resolution, and it should be able to reach at least the same
level of accuracy as M328,2.2. Our explanation for this observation is based on the nature of the data.
As we can see in the example images that we provide in the last row of Figure 4, when the resolution
of an image is reduced, the noise is blurred out. As a result, when M356,6.1 is processing images of
higher resolution, it is processing more intense high frequency noise, and since it is using a limited
number of locations, its accuracy drops compared to M328,2.2. However, BL356 improves on BL328 in
terms of accuracy, and as a result, we expect that a M356 model with more attended locations should
be able to demonstrate accuracy at least at the level of M328,2.2. This is what is happening with model
M356,12.1.
This phenomenon is observed in Fig. 4 (h) and (j) as well, since M256,6.1 and M156,6.3 achieve lower
accuracy compared to M228,2.0 and M128,2.2 respectively. The explanation we provided adds a new
dimension to the understanding of our models, because so far we were treating fine information as
something that is by default beneficial for classification, while high frequency noise of any form may
require special consideration in our design and training choices.
A.6 Experiments On ImageNet
A.6.1 Architectures
In Tables 4 and 5, we provide the exact architectures of models (M2, BL2) and (M1, BL1).
A.6.2 Training Details
We provide details about training one pair of models (Mi , BLi). We make a distinction between
models M1 and M2 only when the process we follow, or the values of the hyperparameters that
we use, differ. In our first training session we optimize the cross entropy loss to train BLi , and
we use learning rule (4) for Mi . The base resolution of Mi matches the size of the input images
(32 × 32 px), and our model goes through only 1 processing level, without using the location module.
As a result, the only term of learning rule (4) that we actually use, is the first term of LF , and we end
up optimizing the cross entropy of the labels.
For both models we use the following hyperparameters. We use learning rate 0.001 that drops by a
factor of 0.2 after the completion of 80% and 90% of the total number of training steps. We train
for 200 epochs, and we use batches of size 128. We use the Adam optimizer with the default values
of β1 = 0.9, β2 = 0.999 and = 10-8. We use xavier initialization for the weights, and zero
initialization for the biases. For regularization purposes, we use data augmentation that is very similar
to the one used by Szegedy et al. (2015). In particular, given a training image, we get a random
19
Under review as a conference paper at ICLR 2020
	Layer Type	Weights’ Size (w/o biases)	Stride			Padding	Activation	Output Size	Receptive Field Size			Params	FLOPs
	Input							32 × 32 × 3					
	Conv	3 × 3 × 3 × 64	1	×	1	SAME	Leaky ReLU	32 × 32 × 64	3	×	3	1792	1, 769, 472
	Max Pool	2×2×1	2	×	2	VALID		16 × 16 × 64	4	×	4		
	Conv	3 × 3 × 64 × 128	1	×	1	SAME	Leaky ReLU	16 × 16 × 128	8	×	8	73, 856	18, 874, 368
Feature	Max Pool	2×2×1	2	×	2	VALID		8 × 8 × 128	10	×	10		
Extraction	Conv	3 × 3 × 128 × 256	1	×	1	SAME	Leaky ReLU	8 × 8 × 256	18	×	18	295, 168	18, 874, 368
Module	Max Pool	2×2×1	2	×	2	VALID		4 × 4 × 256	22	×	22		
	Conv	3 × 3 × 256 × 256	1	×	1	SAME	Leaky ReLU	4 × 4 × 256	38	×	38	590, 080	9, 437, 184
	GAP							1 × 1 × 256					
	Output							4 × 4 × 256,					
								1 × 256					
Location Module	Input Conv	4 × 4 × 256 × 9	1	×	1	VALID	Logistic	4 × 4 × 256 1×1×9				36, 873	36, 864
	Output							1×9					
Aggregation Module	Input Conv	3 × 3 × 256 × 256	1	×	1	VALID	Leaky ReLU	3 × 3 × 256 1 × 1 × 256				590, 080	589, 824
	Output							1 × 256					
Merging Module	Input FC	521 × 256					Leaky ReLU	1 × 512 1 × 256				131, 328	131, 072
	Output							1 × 256					
Classification Module	Input Linear	256 × 1, 000						1 × 256 1 × 10				256, 000	256, 000
	Output							1 × 10					
	Input							N×N×1					
	Conv	3 × 3 × 3 × 64	1	×	1	SAME	Leaky ReLU	N × N × 64	3	×	3	1792	1, 728N 2
	Max Pool	2×2×1	2	×	2	VALID		N/2 × N/2 × 64	4	×	4		
	Conv	3 × 3 × 64 × 128	1	×	1	SAME	Leaky ReLU	N/2 × N/2 × 128	8	×	8	73, 856	18, 432N2
	Max Pool	2×2×1	2	×	2	VALID		N/4 × N/4 × 128	10	×	10		
Baseline	Conv	3 × 3 × 128 × 256	1	×	1	SAME	Leaky ReLU	N/4 × N/4 × 256	18	×	18	295, 168	18, 432N2
CNN	Max Pool	2×2×1	2	×	2	VALID		N/8 × N/8 × 256	22	×	22		
	Conv	3 × 3 × 256 × 256	1	×	1	SAME	Leaky ReLU	N/8 × N/8 × 256	38	×	38	590, 080	9216N 2
	GAP							1 × 1 × 256					
	FC	256 × 256					Leaky ReLU	1 × 256				65, 792	65, 536
	FC	256 × 256					Leaky ReLU	1 × 256				65, 792	65, 536
	Linear	256 × 1, 000						1 × 1, 000				256, 000	256, 000
	Output							1 × 1, 000					
Table 4: The architectures of models M2 and BL2 that we used in our experiments on ImageNet.
“GAP” denotes a global average pooling layer. The variable output sizes of the baseline CNN are
approximately calculated to preserve the clarity of our tables. Based on these approximate output
sizes, the computation of the number of FLOPs in the corresponding layers is approximate as well.
crop that covers at least 85% of the image area, while it has an aspect ration between 0.5 and 2.0.
Since we provide inputs of fixed size to our networks, we resize the image crops appropriately. The
resizing is performed by randomly selecting between bilinear, nearest neighbor, bicubic, and area
interpolation. Also, we randomly flip the resized image crops horizontally, and we apply photometric
distortions according to Howard (2013). The final image values are scaled between -1 and 1. This
data augmentation strategy and the aforementioned hyperparameter’s values, are used in the other
training sessions as well, and in the stages of multi-level learning that we describe later.
In the second training session we again optimize the cross entropy loss for BLi , and we use learning
rule (4) for Mi . The resolution of the input allows Mi to go through 2 processing levels, and all terms
of (4) contribute to the updates of our models’ parameters. As we described in Appendix A.5.2, we
stop the gradients’ flow from the location module to the other modules. We set λf = 10-8, λr = 10-9
and λt = 10-9, and for our Monte Carlo estimators we use 2 samples. These hyperparameter values
are used in the two remaining training sessions as well. The models Mi64 which are reported in
Figure 6 are trained with ct ∈ {1.5, 4.5}.
In the third training session we use gradual learning (5) for Mi , by initializing its variables with the
learned parameters of a Mi64 model. The models M2128 which are reported in Figure 6 are trained
with ct ∈ {3.75, 7, 13.5, 24.75}, and models M1128 with ct ∈ {3.75, 8, 11, 13.5, 30}. We use gradual
learning for BLi as well, by initializing its parameters with those of BLi64 before we apply the
standard optimization to the cross entropy loss.
In the fourth training session we again use gradual learning for Mi, by initializing its variables with
the learned parameters of a Mi128 model. The models M2256 which are reported in Figure 6 are
trained with ct ∈ {22, 29, 39}, and models M1256 with ct ∈ {30, 35, 40}. As in the previous session,
we optimize the cross entropy loss to train BLi, and we initialize its parameters with those of BLi128.
20
Under review as a conference paper at ICLR 2020
	Layer Type	Weights’ Size (w/o biases)	Stride			Padding	Activation	Output Size	Receptive Field Size			Params	FLOPs
	Input							32 × 32 × 3					
	Conv	3 × 3 × 3 × 64	1	×	1	SAME	Leaky ReLU	32 × 32 × 64	3	×	3	1792	1, 769, 472
	Max Pool	2×2×1	2	×	2	VALID		16 × 16 × 64	4	×	4		
Feature	Conv	3 × 3 × 64 × 128	1	×	1	SAME	Leaky ReLU	16 × 16 × 128	8	×	8	73, 856	18, 874, 368
Extraction	Max Pool	2×2×1	2	×	2	VALID		8 × 8 × 128	10	×	10		
Module	Conv	3 × 3 × 128 × 256	1	×	1	SAME	Leaky ReLU	8 × 8 × 256	18	×	18	295, 168	18, 874, 368
	GAP							1 × 1 × 256					
	Output							8 × 8 × 256,					
								1 × 256					
Location	Input							8 × 8 × 256					
Module	Conv	8 × 8 × 256 × 9	1	×	1	VALID	Logistic	1×1×9				147, 465	147, 456
	Output							1×9					
Aggregation Module	Input Conv	3 × 3 × 256 × 256	1	×	1	VALID	Leaky ReLU	3 × 3 × 256 1 × 1 × 256				590, 080	589, 824
	Output							1 × 256					
Merging Module	Input FC	521 × 256					Leaky ReLU	1 × 512 1 × 256				131, 328	131, 072
	Output							1 × 256					
Classification	Input							1 × 256					
Module	Linear	256 × 1, 000						1 × 10				256, 000	256, 000
	Output							1 × 10					
	Input							N×N×1					1, 728N 2
	Conv	3 × 3 × 3 × 64	1	×	1	SAME	Leaky ReLU	N × N × 64	3	×	3	1792	
	Max Pool	2×2×1	2	×	2	VALID		N/2 × N/2 × 64	4	×	4		
	Conv	3 × 3 × 64 × 128	1	×	1	SAME	Leaky ReLU	N/2 × N/2 × 128	8	×	8	73, 856	18, 432N2
Baseline	Max Pool	2×2×1	2	×	2	VALID		N/4 × N/4 × 128	10	×	10		
CNN	Conv	3 × 3 × 128 × 256	1	×	1	SAME	Leaky ReLU	N/4 × N/4 × 256	18	×	18	295, 168	18, 432N2
	GAP							1 × 1 × 256					
	FC	256 × 256					Leaky ReLU	1 × 256				65, 792	65, 536
	FC	256 × 256					Leaky ReLU	1 × 256				65, 792	65, 536
	Linear	256 × 1, 000						1 × 1, 000				256, 000	256, 000
	Output							1 × 1, 000					
Table 5: The architectures of models M1 and BL1 that we used in our experiments on ImageNet.
“GAP” denotes a global average pooling layer. The variable output sizes of the baseline CNN are
approximately calculated to preserve the clarity of our tables. Based on these approximate output
sizes, the computation of the number of FLOPs in the corresponding layers is approximate as well.
Finally, we train {Mi}i2=1 with multi-level learning rule (6) in 3 stages of gradual learning. In the
first training stage (s = 1) we use images of resolution 64 × 64 and we set λ01 = λ11 = 1. This
means that we assign the same weight on correctly classifying images after the first and the second
processing level. The hyperparameters for learning rules Lr0 and Lr1 are set as we described before
(the same holds for the subsequent training stages), while for M2 we use ct1 = 3.2 and for M1 we
use ct1 = 2.9. In the second training stage (s = 2) we use images of resolution 128 × 128, and we
set λ02 = λ21 = λ22 = 1. For M2 we set ct1 = 3.2 and ct2 = 13.5, while for M1 we set ct1 = 2.9 and
ct2 = 11.3. In the third training stage (s = 3) we use images of resolution 256 × 256, and we set
λ30 = λ31 = λ32 = λ33 = 1. For M2 we set ct1 = 3.2, ct2 = 13.5 and ct3 = 40, while for M1 we set
ct1 = 2.9, ct2 = 11.3 and ct3 = 35.6. During these 3 stages of training, when we were moving to a
new training stage, we were initializing the variables of our models with the learned parameters from
the previous training stage.
21