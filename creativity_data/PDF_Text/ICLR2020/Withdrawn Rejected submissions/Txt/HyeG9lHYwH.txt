Under review as a conference paper at ICLR 2020
Compression without Quantization
Anonymous authors
Paper under double-blind review
Ab stract
Standard compression algorithms work by mapping an image to discrete code us-
ing an encoder from which the original image can be reconstructed through a de-
coder. This process, due to the quantization step, is inherently non-differentiable
so these algorithms must rely on approximate methods to train the encoder and de-
coder end-to-end. In this paper, we present an innovative framework for lossy im-
age compression which is able to circumvent the quantization step by relying on a
non-deterministic compression codec. The decoder maps the input image to a dis-
tribution in continuous space from which a sample can be encoded with expected
code length being the relative entropy to the encoding distribution, i.e. it is bits-
back efficient. The result is a principled, end-to-end differentiable compression
framework that can be straight-forwardly trained using standard gradient-based
optimizers. To showcase the efficiency of our method, we apply it to lossy im-
age compression by training Probabilistic Ladder Networks (PLNs) on the CLIC
2018 dataset and show that their rate-distortion curves on the Kodak dataset are
competitive with the state-of-the-art on low bitrates.
1	Introduction
The recent development of powerful generative models, such as Variational Auto-Encoders (VAEs)
and their hierarchical extensions, such as Probabilistic Ladder Networks (PLNs) (Kingma &
Welling, 2014; S0nderby et al., 2016; Higgins et al., 2017) has caused a great deal of interest in
their application to lossy compression, notably Balle et al. (2016); TheiS et al. (2017); RiPPeI &
BoUrdev (2017); Bane et al. (2018); Mentzer et al. (2018); Johnston et al. (2018). The benefit of
using these models as opposed to hand-crafted methods is that they can adapt to the statistics of their
inputs much better, and hence allow significant gains in compression rate at a given quality setting,
and in quality at a given compression rate. A second advantage is their easier adaptability to new
media formats, such as light-field cameras, 360° images, Virtual Reality (VR), video streaming, etc.
for which classical methods are not currently applicable, or would perform pathologically badly.
Lossy compression codecs usually perform a lossy transformation on the continuous representation
of the data, after which it is mapped to a discrete domain through an operation known as quan-
tization, such that entropy coding may be used to encode this transformed representation. This
continuous-to-discrete mapping will mean that quantization will necessarily have zero derivative
almost everywhere. Since the above mentioned generative models are usually trained using some
gradient-based technique, this means that they cannot be directly applied as compression models,
as quantization will destroy the learning signal, and hence a workaround is needed. A nice remedy
to this problem is to replace the quantities affected by quantization (or at least their derivatives) by
some smooth approximation during training, as is done in recent approaches, e.g. Balle et al. (2016);
Theis et al. (2017); Bane et al. (2018); Mentzer et al. (2018); Johnston et al. (2018).
Similarly to Balle et al. (2016); Theis et al. (2017); Balle et al. (2018), We aim to minimize the
rate-distortion of our model directly, but unlike them we replace entropy coding with a more gen-
eral coding technique that alloWs us to use probability densities for coding instead of probability
masses. This then alloWs us to forgo the quantization operation altogether and We get an end-to-end
differentiable model that can be optimized With standard gradient-based optimizers.
We select an appropriate generative model p(x, z) = p(x | z)p(z) With an approximate posterior
q(z | x), Where the latents z are going to serve as the transformed representation of x that We Wish
to compress. We shoW that - log p(x | z) corresponds to the distortion and KL [ q(z | x) || p(z) ]
1
Under review as a conference paper at ICLR 2020
corresponds to the rate of the model. Hence, optimizing our model for the rate-distortion is as
simple as minimizing - log p(x | z) + βKL [q(z | x) || p(z) ], where β is a scalar hyperparameter
controlling the compression rate.
To compress some data x with our trained model, instead of selecting z deterministically and
then coding it, we map x to the posterior q(z | x) and use it to code a sample from it, using an
adapted version of the importance sampling algorithm proposed by Havasi et al. (2018). First, set
K = KL [ q(z | x) || p(z) ] and stochastically draw independent samples (z1, . . . , zdexp{K}e ) from
p(z). Next, select Zc* for some 1 ≤ c* ≤ exp{K}. We can use the index c* as the compressed
representation of Zc*. Clearly, C can be coded in K nats, and it can also be shown that Zc* will
be a low-bias sample from q(z | x). As the efficiency of this algorithm is KL [ q(z | x) || p(z) ], we
will refer to it as relative entropy coding (REC), and note that REC algorithms achieve bits-back
efficiency (Hinton & Van Camp, 1993).
The contributions of this paper are as follows:
•	We show how any generative model p(x, z) = p(x | z)p(z) with approximate latent pos-
terior q(z | x) can be used for bits-back efficient lossy transform coding, using an adapted
version of Minimal Random Code Learning (MIRACLE, Havasi et al. (2018)).
•	We present the concept of relative entropy coding (REC) algorithms and attainable bounds
on their worst-case performance. We note their current intractability in high-dimensional
settings and propose an approximate solution that is tractable, though the rate-distortion
quality is impacted.
•	We demonstrate the effectiveness by applying our method to lossy image compression.
Concretely, We train a Probabilistic Ladder Network (S0nderby et al., 2θl6) on the CLIC
(2018) dataset and show that the rate-distortion curve of our method is competitive with the
state-of-the-art on the Kodak dataset (Eastman Kodak Company, 1999) on low bitrates.
2	Literature Review
2.1	Transform Coding
Neural network-based approaches lend themselves easiest to transform coding as a realisation of
lossy compression. In transform coding (Goyal, 2001), given some data x, it is first transformed us-
ing the analysis transform Z = fa(x) and then quantized Z = [z], where [∙] could denote rounding,
or any other quantizer in general. The distribution of this quantized representation is modelled by
some probability mass function P(z), which is used to encode Z using some entropy code. Note,
that this is different from the true distribution of Z, P(Z), which is entirely determined by the dis-
tribution of [fa(x)], where X 〜P(X). During decompression, once Z is decoded, the Synthesis
transform x0 = fs(Z) is applied to it to obtain a reconstruction of x.
Given some metric d(∙, ∙), the distortion of the codec is
Ex〜P(X) [d(x, X )] ,	(1)
and its rate is the expected codelength of the entropy code, which is
Ez〜P(^) [- log P(Z)i = Ex〜p(x) [- log P(Z)i .	(2)
The equality holds, since z is a deterministic transformation of x.
2.2	Bits-Back Efficiency
If instead of mapping some data we wish to compress x to a single deterministic point Z and entropy
coding it by using some prior p(z), we map it to an approximate posterior q(z | x), then we may
select a random representative z* from q(z | x) in the sense that z* has a short Minimum Description
Length (MDL, GrUnWaId et al. (2007)). Bits-back efficient methods are such that this MDL is
approximately KL [q(z | x) || p(z ]. An important distinction should be made in whether we wish to
communicate the distribution q(z | x) or a sample Z* drawn from it, as the communication problem
2
Under review as a conference paper at ICLR 2020
from which the MDL can be obtained, its precise value and the method required to obtain it are
different.
The concept originates in the work of Hinton & Van Camp (1993). Given data D and model weights
w, they derive the MDL of the posterior weight distribution q(w | D) of a Bayesian Neural Network
(BNN) with prior p(w) to be K = KL [q(w | D) || p(w) ]. However, they did not give a method
for achieving this, in fact, in their argument they merely show that in a longer message only K nats
are used to code the posterior.
In our setting, We are only interested in communicating a single sample z* from q(z | x). We assume
that we have access to a generative model p(x | z)p(z) and a posterior q(z | x). Then, the problem
We must ansWer is What is the minimum expected number of nats T [x : z] that is required to be
communicated, such that we could draw a sample Z 〜 q(z | x). This question was answered by
Harsha et al. (2007), Who proved the folloWing theorem:
Theorem 1 (Harsha et al., 2007) Let the setting be as described above, and assume that the coder
and the decoder share a common source of randomness (e.g. by using the same pseudo-random
number generator with the same seed), then the minimum expected number of nats T [x : z] that
need to be communicated in order for the decoder to be able to Sample a Z 〜q(z | x) can be
bounded in terms of the mutual information I [x : z] as
I[x : z] ≤ T[x : z] ≤ I[x : z] + 2log (I[x : z] + 1) + O(1).	(3)
They prove the above claim constructively, by giving a rejection sampler that achieves this. This,
to our knowledge, is the first example of what we call a single-shot relative entropy coding (REC)
algorithm, and we will examine it in further detail in Section 4. Bits-back efficiency comes from the
fact that I[x : z] = Ex〜p(x)[KL [ q(z | x) || p(z)]].
Relative entropy coding subsumes entropy coding in the sense that any entropy code is a relative
entropy code as well, since for a discrete z 〜P(Z) and the point-mass δ^ on Z,
-log P(Z)=KL[ δz || P(Z)].	(4)
See Havasi et al. (2018) for details on the derivation. In this sense, methods that use quantization
and then entropy coding are also bits-back efficient, but are restricted to point-masses for their latent
posteriors. Thus, one of the advantages of our method lies in that it allows a much wider class of
posteriors to be used.
2.3	Related Works
Minimal Random Code Learning (Havasi et al., 2018) is a powerful coding technique that allows us
to code a random sample from a continuous distribution, rather than coding a deterministic sample,
which requires quantization, as any fixed sample would have 0 density. They use their technique
to compress the weights of BNNs. The structure of the weight space is much different from the
structure of the latent space of a generative model trained for the compression of some medium.
Hence, we argue that their practical compression procedure is infeasible for our setting, the random
fixed-size block partitioning as well as the retraining between the coding of blocks is incompatible
with our setting as well as computationally infeasible.
Townsend et al. (2019) were the first to develop a bits-back efficient image compression algorithm
using VAEs, called Bits-back with ANS (BB-ANS). However, they develop a lossless compression
algorithm, and hence the performance their approach is not comparable to ours. The sense in which
their method is “bits-back” also differs from ours. Townsend et al. (2019) stick to the original bits-
back setting. Concretely, they show a clever way of utilizing the overhead of the method of Hinton
& Van Camp (1993) for efficient coding, our work on the other hand, avoids sending it in the first
place. Their coding efficiency collapses onto entropy efficiency when coding a single image and
bits-back efficiency is only achieved when coding larger batches.
Lately, there has been an explosion of interest in using powerful deep generative models for lossy im-
age compression, and thus all previous methods had to deal with the issue of the non-differentiability
of quantization. These approaches can generally be put into three different categories, depending on
how they avoid this problem. There are approaches that waive the paradigm of end-to-end training,
e.g. by not incorporating the feedback from the gradient of the rate term into the optimization of the
3
Under review as a conference paper at ICLR 2020
entropy coder, either by using inflexible distributions without any parameters (Toderici et al., 2017),
or by optimizing them post-training (Johnston et al., 2018; Rippel & Bourdev, 2017). The issue
with this, as pointed out in Balle et al. (2018) is that not training end-to-end might severly limit the
efficiency of these methods.
Another approach of getting aroud this issue has been to learn to generate samples from the image
distribution p(x), using a Generative Adversarial Network (GAN, Goodfellow et al. (2014)), by
learning the synthesis transform first and using an auxiliary discriminator. Then, fixing the discrimi-
nator, the analysis transform is trained in order to minimize the reconstruction error d(x, fs(fa(x))).
The noise provided fs during the initial GAN training should be a continous relaxation of quantiza-
tion noise, such as a uniform distribution (Balle et al., 2016).
Finally, there are methods that stick to Variational Auto-Encoders (VAEs, Kingma & Welling
(2014); Higgins et al. (2017)), optimize the weighted rate-distortion directly, and replace the non-
differentiable quantities with smooth approximations during training. Theis et al. (2017) introduce
compressive auto-encoders, and replace the derivative of the quantization operation with the iden-
tity function, and the rate of the discrete z by a differentiable upper bound which is used in the
loss. Closest to our work are the works of Bane et al. (2016; 2018); Minnen et al. (2018), who
all model the quantization noise as uniform noise centered on the quantized representation. Balle
et al. (2016) are the first to derive the connection between the weighted rate-distortion loss and the
β-Evidence Lower Bound (β-ELBO, Higgins et al. (2017)), though they only derive it for a smaller
class of distributions than we do, as well as through a different route, as we arrive at it through the
MDL principle (Grunwald et al., 2007). Crucially, these methods are still only equivalent to VAEs
during training, and the continuous relaxations are switched back to the original discrete operations
thereafter.
3	Compression without Quantization
Our method relies on two observations. First, we note that the sole purpose of quantization is that
once we have obtained a deterministic representation z, it has non-zero probability mass, and hence
entropy coding may be used to compress it. Hence, if we could replace entropy coding with a
method that could use a probability density p(z) instead of the mass P(z), then we could forgo the
quantization step and thus our training objective becomes end-to-end differentiable.
Second, as we have seen in Section 2.2, ifin addition have access to an approximate latent posterior
q(z | x), then it is enough to communicate approximately
I[x : z] = h[z]	-	h[z	| x]	= Ex〜p(χ)	[KL [ q(z	|	x)	|| p(z)]]	(5)
nats such that the decoder may obtain a sample Z 〜q(z | x). Applying f§ to Z could then be used
to reconstruct x.
These two could be combined by noting that our setting fulfils all premises required for both obser-
vations to hold. This enables the following development of a lossy transform codec:
Training After fixing an appropriate architecture for fa(x) = q(z | x) and fs such that the like-
lihood is p(x = fs(Z) | Z), we train our model using the β-Evidence Lower Bound (Higgins et al.,
2017):
Eχ,z〜q(z | x)p(x) [- logP(X | z)] + βEχ〜p(x) [KL [ q(z | x) || P(z) ]] .	(6)
Note, that if the distortion metric d(∙, ∙) is such, that exp{-d(x, x0)} can be normalized, then the
negative log-likelihood can be identified with the distortion D. Ex〜p(x) [KL [q(z | x) || p(z)]]=
I [x : z] can also be identified with the rate R. Hence, in this case the negative β-ELBO is equivalent
to training for the rate-distortion loss
L = D + βR,	(7)
which was our aim to train for in the first place. This gives us a new way to interpret β in Equation
6: it allows the model to converge on different points on its rate-distortion curve (see Bane et al.
(2016) for the full argument).
Note, that even if normalizing exp{-d(x, x0)} is intractable β can be adjusted in Equation 6 to
compensate for this unknown constant during training.
4
Under review as a conference paper at ICLR 2020
Compression and Decompression For given x that we wish to compress, we calculate q(z | x)
using the analysis transform. Then, We use relative entropy coding (REC) to code a sample Z 〜
q(z | x). The decoder can then re-obtain z and apply the synthesis transform to it to reconstruct x.
4 Relative Entropy Coding
As mentioned in the previous section, our method relies on the existence of relative entropy cod-
ing (REC) in order to be able to code the latent representation of a given input x With bits-back
efficiency. To our knoWledge, the first such algorithm Was developed by Harsha et al. (2007), as a
variant of rejection sampling. This algorithm is unfortunately intractable in more than a feW dimen-
sions, as it requires to keep track of probabilities across the entire range of z.
The simplest Way to extend the univariate algorithm to the multivariate setting is to note the inde-
pendence assumption for the dimensions of Z and thus concatenating univariate samples for each
dimension Will given an exact multivariate sample. Hence, We could use REC to encode the di-
mensions of Z individually, i.e. for each Z obtain some code Ci, and build an empirical distribution
P(c) over these. Then, We can encode each ci using entropy coding. The issue With this approach
is that While the upper bound in Theorem 1 is quite tight if We apply it to the Whole multivariate
distribution over Z, when applied to each of its dimensions zi, the O(1) cost is incurred for each
dimension, Which makes this approach very inefficient. Hence, We focus on multivariate REC.
4.1 Adaptively Grouped Importance Sampling
Algorithm 11mportance sampling algorithm proposed by Havasi et al. (2018)
Inputs:
P - Proposal distribution
Q - Target Distribution
hxi 〜Q | i ∈ Ni - Shared sequence of random draws from Q
procedure IMPORTANCE-SAMPLER(P) Q, g 〜Q | i ∈ Ni)
K J exp{KL[Q ]| P]}
Wi — log Q(Xi)	∀i = 1,...K
j — arg maxi Wi
return j, xj
end procedure
The basis of our approach is the approximate importance sampler proposed by Havasi et al. (2018).
For given x, it proceeds by letting K = KL [q(Z | x) || p(Z) ] and drawing S = eK samples
(z1, z2, . . . , zS) from p(Z). Then the index of the maximal importance weight
c
arg max
c∈{1,2,...S}
q(zc I χ)
P(Zc)
(8)
is used as the encoding of x. The decoder can recover Zc by drawing C samples from p(z) and
then can use it to reconstruct x, assuming it can generate the same sequence of samples. Clearly, by
virtue of the choice for S, C can be coded in KL [q(z ∣ x) ∣∣ P(Z) ] nats, and Havasi et al. (2018)
also show that Zc* has negligible bias. The algorithm is depicted in Algorithm 1, note that Havasi
et al. (2018) originally sample the softmax distribution of the Wi. We found it better to select samples
biased towards the mode by always selecting the most likely sample. Though now we do not need to
keep track of the probabilities over the entire range of Z, the run-time is still going to be exponential
in K, and hence becomes prohibitive for K larger than 15 - 20. In our experiments we had to deal
with KLs well above 105, and hence solving this issue is critical for the viability of our method.
To solve this issue, Havasi et al. (2018) randomly partition the vector of interest w into fixed size
blocks, in the hope that on average each individual block will have approximately some C nats of
total KL. They then code each the blocks sequentially. The concern is that the next block to be coded
is a suboptimal choice, i.e. its total KL is significantly higher or lower than the desired C nats. To
circumvent this problem, they introduce a blockwise KL penalty to the loss that forces each block
5
Under review as a conference paper at ICLR 2020
to have equal KL, and they intersperse coding a block with several rounds of retraining. They show
that this solves the suboptimal choice problem, and their coding procedure also does not impact the
quality of the coded w.
Unfortunately, this fix is not applicable in our case. Havasi et al. (2018) can use the retraining trick
to compensate for suboptimal block choices. However, if we hope to obtain a useful compression
codec, we should avoid extremely expensive operations to code a single item, such as performing
gradient descent for each individual input x to our algorithm.
To remedy these issues, instead of selecting fixed-size blocks randomly we impose a group size
constraint of G bits and total group KL constraint of B bits. Then, we partition the dimensions of
z such that each set in the partition (what we call a group) has size at most 2G and each group has
total KL at most K bits. The details are depicted in Algorithm 2.
Algorithm 2 Adaptively grouped importance sampler
Inputs:
K - Maximum individual KL allowed
G - Maximum group size
B - Bit budget per group
P - Proposal distribution
Q - Target Distribution
hxi 〜Q | i ∈ Ni - Shared sequence of random draws from Q
procedure ADAPTIVE-IMPORTANCE-SAMPLER(K, G, B, P,Q, (xi 〜Q | i ∈ Ni)
Γ J ()	. Initialize empty list of group sizes
ki J KL [Qi || Pi ] ∀i = 1,...N	. Get KLs for each dimension
Q0, P0, O J REMOVE-OUTLIERS({ki}iN=1, K)
γ J 0, k J 0	. Current group size and group KL
for i J 1, . . . dim(Q0 ) do
if k + kli > B or γ + 1 > G then
Append γ to Γ
k J kli, γ J 1
else
k J k + kli, γ J γ + 1
end if
end for
Append γ to Γ	. Append the last group size
S = (), I = (), g J 0	. Samples, sample codes and current group index
for γ in Γ do	. Now importance sample each group
i, s J IMPORTANCE-SAMPLER(Pg:g+Y, Qg：g+Y, hχ 〜Q | i ∈ Ni)
Append i to I, append s to S
gJg+γ
end for
return I, S, O
end procedure
While the concern of intractability due to high KL is resolved this way but we are faced with two
inefficiencies. First, for groups whose size is much smaller than 2G, dedicating G bits to coding the
group size is wasteful. Second, many groups will have size 2G but total KL much less then K, and
hence dedicating K bits to code the sample is wasteful. To solve these issues, we built an empirical
distribution over group sizes using our training dataset, which is tractable since we chose G to be
not too large (2 ≤ G ≤ 6), and then used arithmetic coding (Rissanen & Langdon, 1981) to code
the group sizes. This worked very effectively, to the extent that for appropriately selected G the cost
of communicating the group sizes is effectively negligible.
6
Under review as a conference paper at ICLR 2020
Output
Input
φ> ①己 S」.LL
Φ>Φ1 PUoɔ ① S
Arrows
Distributions
Blocks / Layers
Input / Output
Shared for posterior
and prior prediction
Output of source fed
to input of target
Posterior prediction
Image reconstruction
_ A Sample from source
fed to input of target
Output of source
predicts variance of target
Output of source
predicts mean of target
Figure 1: PLN network architecture. The blocks signal data transformations, the arrows signal the
flow of information. Block descriptions: Conv2D: 2D convolutions along the spatial dimensions,
where the W × H × C/S implies a W × H convolution kernel, with C target channels and S gives
the downsampling rate (given a preceding letter “d”) or the upsampling rate (given a preceding letter
“u”). If the slash is missing, it means that there is no up/downsampling. All convolutions operate in
same mode with zero-padding. GDN/IGDN: these are the non-linearities described in Balle et al.
(2016). Leaky ReLU: elementwise non-linearity defined as max{x, αx}, where we set α = 0.2.
Sigmoid: Elementwise non-linearity defined as 1+exp{-x}. We ran all experiments presented here
with N = 196, M = 128, F = 128, G = 24.
7
Under review as a conference paper at ICLR 2020
The relative entropy codes of the groups were also further compressed using Elias δ-coding (Elias,
1975). This is the main point of inefficiency of this method, as the empirical distribution of sample
indices has a much heavier tail than the distribution implied by δ-coding. Concretely, we found
that using δ-coding the resulting code lengths are approximately 1.6 - 1.8 times the theoretically
possible REC code lengths. However, building an empirical distribution over the sample indices and
using it for entropy coding is more challenging, as the number of possible indices for a ”useful” K
is between 215 ≤ 2K ≤ 220. This means that to obtain a good estimate of the tail mass, i.e. of
indices above 214 - 215, we need a lot of data, or some reasonable smoothing should be used, and
we leave this for future work.
5	Experiments
Figure 2: Demonstration of the effectiveness of the PLN’s conditional independence assumption
p(z) = p(z(1) | z(2))p(z(2)). a) Original image from the Kodak dataset. b) Randomly selected
channel of a latent sample Z(I)〜q(z⑴ | Z⑵,x), clearly there is a lot of structure present in the
sample. c) The same channel of the latent sample standardized according to p(z(1) | z(2)), i.e. we
Z Ui is displayed for each dimension, where the μ% and σ% are the means and standard deviations
of the dimensions of p(z(1) | Z(2)), respectively. We see that relative to the conditional prior, the
latent structure is effectively captured.
We applied our framework to lossy image compression. We followed the work of Balle et al. (2016)
and Balle et al. (2018). Not only do their results represent the current state-of-the-art, but their
work is also the closest to ours. In particular, as mentioned before, to our knowledge they were the
first to derive the connection of the weighted rate-distortion loss with the β-ELBO as the training
objective for a VAE. However, we stress that they derived this for a VAE whose latent distribution
was a continuous relaxation of the quantization error. Therefore, their results were restricted to a
narrow set of distributions, and neither did they make the connection to bits-back efficientcy. Most
notably though, they only used the continuous relaxation during the training of the model, thereafter
switching back to quantization and entropy coding, which, they show does not impact the predicted
performance, and hence confirming that their relaxation during training is reasonable. This is in
striking contrast with our method, in which not only do we allow any latent distributions for which
the KL can be calculated, but our architecture also remains the same after training.
Nonetheless, We based our architecture entirely on the one they present in Bane et al. (2018), appro-
priately modified for our setting. Most importantly, where they use i.i.d. uniform distributions for
the posterior and complicated non-parametric priors, we opted for Gaussians, a much more standard
8
Under review as a conference paper at ICLR 2020
choice for VAEs. We also opted for a similar hierarchical VAE structure, which in our case is a
Probabilistic Ladder Network (PLN, S0nderby et al. (2016)). The reason for this is as shown by
Balle et al. (2018) and also confirmed by us empirically, the full independence assumption in the la-
tent distribution ofa VAE is very limiting, as there is a lot of topological structure even on the latent
level. One solution to introduce a rich dependence structure is to introduce further latent variables,
conditioned upon which the rest are assumed to be independent (Bishop, 1998). Concretely, we
break up our generative model p(x, z) = p(x | z(1))p(z(1) | z(2))p(z(2)), and our approximate latent
posteriors now become q(z(2) | x) and q(z⑴ | x, z(2)) H q(z⑴ | x)p(z⑴ | z(2)). Further, our archi-
tecture also only uses convolutions and deconvolutions as non-linearities. This allows us to compress
arbitrary sized images, and the latent space can naturally grow with the dimensions of the images.
Full details of the architecture can be seen in Figure 1. A confirmation that the PLN’s conditional
independence structure efficiently removes the topological structure in the latent space on the first
stochastic level is shown in Figure 2. Note, that in order for us to code a latent sample Z 〜q(z | x),
We first code Z⑵ 〜q(z⑵ | x) using p(z⑵)and then code Z(I) | Z⑵ 〜q(z⑴ | x, Z⑵).
MS-SSIM comparison for kodim11
.05 0 5
20亿15.惬
(GP) Wωωφw
PSNR comparison for kodim11
≡mp) HNSQ
32.530.027.5
Theoretical
-→*- Actual
-→*- Balle et al.
JPEG
—Theoretical
—Actual
—Dalle et al.
JPEG
ωp) ≡-ωω-ω≡
Figure 3: Comparison of rate-distortion curves on kodim11.png andkodim21.png from the
Kodak dataset. MS-SSIM results are presented in decibels, where the conversion is done using the
formula -10 ∙ log10 (1 一 MS-SSIM(x, x0)) as is done by Bane et al. (2018). For the description of
the labels, see Section 5.
In our experiments, We compare the performance of our method with the method of Bane et al.
(2018) and JPEG as a baseline on their rate-distortion curves, using the Peak Signal-to-Noise Ra-
tio (PSNR) and Multiscale Structural Similarity Index (MS-SSIM) perceptual metrics (Huynh-Thu
& Ghanbari, 2008; Wang et al., 2003) as distortion distances. Our models were trained using
β ∈ {300, 600, 1200, 2400, 4800} and the Mean Absolute Error (MAE) loss. The reason for the
MAE loss is due to Zhao et al. (2015), who show that using it greatly improves image recon-
struction quality at no significant extra cost. The model of Balle et al. (2018) was trained using
λ ∈ {0.001, 0.003, 0.01, 0.03, 0.1, 0.3} and the Mean Squared Error (MSE) loss. Both were trained
on the CLIC (2018) training dataset, which allows a better comparison in performance. All models
9
Under review as a conference paper at ICLR 2020
were trained for 2 × 105 iterations with Adam (Kingma & Ba, 2014), using a learning rate of 10-4.
We stress, that for comparability, the architectures were made completely identical, except for the
latent distributions, which needed adaptation to our setting.
We report the rate-distortion curves on two images taken from the Kodak dataset in Figure 3. Further
comparison plots can be found in Appendix A. For our models, we report the theoretically optimal
rate-distortion, the rate of which for an image x can be calculated as KL[ q(z | x=x) || p(z) ]/log 2, and
the actual rate-distortion achieved by our proposed REC algorithm from Section 4.1. For the models
of Balle et al. (2018), We report the actual rate-distortion.
We see that our method is competitive with the method of Balle et al. (2018) on low bitrates and
starts tailing off more on higher bitrates.
6	Conclusion
We develop a lossy transform coding framework based on generative models equipped with an
approximate latent posterior and show that it is bits-back efficient (Hinton & Van Camp, 1993) and
can also be optimized end-to-end. We propose a relative entropy coding algorithm to achieves bits-
back efficiency, and we demonstrate the efficiency of our method by training a Probabilistic Ladder
Network on the CLIC (2018) dataset and show that it is competitive with the current state-of-the-art
on the Kodak dataset Eastman Kodak Company (1999).
While allowing for a much wider class of latent distributions, our methods relies on the existence
of efficient relative entropy coding algorithms for the selected distributions. Our REC algorithm
is based on the importance sampler proposed by Havasi et al. (2018), and while it is tractable and
works reasonably well on low bitrates, its rates are approximately 1.6 - 1.8 times higher than the
theoretically possible ones. Currently the run-time is also slower than other methods, it takes around
1-5 minutes to compress a reasonably sized image. Improving the rate factor and the run-time does
not seem too difficult a task, but since the focus of our work was to demonstrate the efficiency of
relative entropy coding, it is left for future work.
10
Under review as a conference paper at ICLR 2020
References
Johannes Balle, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.
International Conference on Learning Representations, 2016.
Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. In International Conference on Learning Represen-
tations, 2018. URL https://openreview.net/forum?id=rkcQFMZRb.
Christopher M Bishop. Latent variable models. In Learning in graphical models, pp. 371-403.
Springer, 1998.
CLIC. Workshop and challenge on learned image compression. https://www.compression.
cc, 2018. Accessed: 2019-03-25.
Eastman Kodak Company. Kodak lossless true color image suite. http://r0k.us/graphics/
kodak/, 1999.
Peter Elias. Universal codeword sets and representations of the integers. IEEE transactions on
information theory, 21(2):194-203, 1975.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Vivek K Goyal. Theoretical foundations of transform coding. IEEE Signal Processing Magazine,
18(5):9-21, 2001.
P.D. Grunwald, A. Grunwald, and J. Rissanen. The Minimum Description Length Principle.
Adaptive computation and machine learning. MIT Press, 2007. ISBN 9780262072816. URL
https://books.google.co.uk/books?id=mbU6T7oUrBgC.
Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communica-
tion complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational
Complexity (CCC’07), pp. 10-23. IEEE, 2007.
Marton Havasi, Robert Peharz, and Jose Miguel Hernandez-Lobato. Minimal random code learning:
Getting bits back from compressed model parameters. NIPS workshop on Compact Deep Neural
Networks with industrial application, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017.
Geoffrey Hinton and Drew Van Camp. Keeping neural networks simple by minimizing the descrip-
tion length of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning
Theory. Citeseer, 1993.
Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality as-
sessment. Electronics letters, 44(13):800-801, 2008.
Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung
Jin Hwang, Joel Shor, and George Toderici. Improved lossy image compression with priming and
spatially adaptive bit rates for recurrent networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 06 2018.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 12 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Conference
on Learning Representations, 2014.
11
Under review as a conference paper at ICLR 2020
Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Con-
ditional probability models for deep image compression. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 06 2018.
David Minnen, Johannes Balle, and George D Toderici. Joint autoregressive and hierarchical priors
for learned image compression. In Advances in Neural Information Processing Systems, pp.
10771-10780, 2018.
Oren Rippel and Lubomir Bourdev. Real-time adaptive image compression. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pp. 2922-2930. JMLR. org,
2017.
Jorma Rissanen and Glen Langdon. Universal modeling and coding. IEEE Transactions on Infor-
mation Theory, 27(1):12-23, 1981.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. HoW
to train deep variational autoencoders and probabilistic ladder networks. In 33rd International
Conference on Machine Learning (ICML 2016), 2016.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszar. Lossy image compression with
compressive autoencoders. International Conference on Learning Representations, 2017.
George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and
Michele Covell. Full resolution image compression with recurrent neural networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5306-5314, 2017.
James Townsend, Tom Bird, and David Barber. Practical lossless compression with latent variables
using bits back coding. arXiv preprint arXiv:1901.04866, 2019.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003,
volume 2, pp. 1398-1402. Ieee, 2003.
Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for neural networks for image
processing. arXiv preprint arXiv:1511.08861, 2015.
12
Under review as a conference paper at ICLR 2020
A Further Comparisons
Figure 4: Reconstruction comparison for kodim05.png. Top: original image. Bottom: Recon-
struction using our PLN with β = 600.
13
Under review as a conference paper at ICLR 2020
Figure 5: Reconstruction comparison for kodim08.png. Top: original image. Bottom: Recon-
struction using our PLN with β = 1200.
14
Under review as a conference paper at ICLR 2020
Figure 6: Reconstruction comparison for kodim13.png. Top: original image. Bottom: Recon-
struction using our PLN with β = 2400.
15
Under review as a conference paper at ICLR 2020
25.022.520.017.515.012.510.07.5
mp) Wωσ?Sw
mp) BNSd
50505050
-37.-35.-32.-30.R-25.-22.-20.
Theoretical
Actual
Balle et al.
JPEG
Figure 7: Rate-distortion curves for kodim05
Figure 8: Rate-distortion curves for kodim08
Figure 9: Rate-distortion curves for kodim13
22.520.017.515.012.510.07.5ao
ωp) WωωtΛw
16