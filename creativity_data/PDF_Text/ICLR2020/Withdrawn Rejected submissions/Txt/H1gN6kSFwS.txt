Under review as a conference paper at ICLR 2020
Learning Neural Causal Models from
Unknown Interventions
Anonymous authors
Paper under double-blind review
Ab stract
Meta-learning over a set of distributions can be interpreted as learning different
types of parameters corresponding to short-term vs long-term aspects of the mech-
anisms underlying the generation of data. These are respectively captured by
quickly-changing parameters and slowly-changing meta-parameters. We present a
new framework for meta-learning causal models where the relationship between
each variable and its parents is modeled by a neural network, modulated by struc-
tural meta-parameters which capture the overall topology of a directed graphical
model. Our approach avoids a discrete search over models in favour of a continu-
ous optimization procedure. We study a setting where interventional distributions
are induced as a result of a random intervention on a single unknown variable of
an unknown ground truth causal model, and the observations arising after such
an intervention constitute one meta-example. To disentangle the slow-changing
aspects of each conditional from the fast-changing adaptations to each intervention,
we parametrize the neural network into fast parameters and slow meta-parameters.
We introduce a meta-learning objective that favours solutions robust to frequent but
sparse interventional distribution change, and which generalize well to previously
unseen interventions. Optimizing this objective is shown experimentally to recover
the structure of the causal graph. Finally, we find that when the learner is unaware
of the intervention variable, it is able to infer that information, improving results
further and focusing the parameter and meta-parameter updates where needed.
1	Introduction
A major challenge of contemporary deep learning is to generalize well outside the assumptions of
independent and identically distributed data, when we care about generalization or fast adaptation
to distributions other than the main training distribution. For this purpose, we propose an approach
that starts by distinguishing between: (a) an underlying causal model, (b) observational distributions
derived from it, an (c) interventional distributions arising from interventions upon its variables,
whether they be known or unknown. Fortunately, these distinctions can be addressed by the paradigm
of Structural Causal Models (SCMs) (Pearl, 1995; Peters et al., 2017) and a wide body of associated
literature. Unlike other frameworks using SCMs we also account for interventions performed
by agents other than an experimenter. By treating the interventions of other agents as unknown
interventions that lead to changes in the underlying data distribution, the present work is a contribution
towards the use of a meta-learning for causal model induction.
Estimating the underlying causal structure from data is an open and challenging problem (Pearl,
2009; Imbens & Rubin, 2015). A lot of prior work has examined learning causal structure based
on observational data (Chickering, 2002; Tsamardinos et al., 2006; Goudet et al., 2017; Hauser &
Buhlmann, 2012; SPirtes et al., 2000; SUn et al., 2007; Zhang et al., 2012; ShimizU et al., 2006;
Hoyer et al., 2009; Daniusis et al., 2012; Budhathoki & Vreeken, 2017; Kalainathan et al., 2018).
However, many real-world datasets have an inherent distribUtional heterogeneity dUe to different
interventions to the variables comPosing the model. In these sitUations interventional aPProaches are
needed (e.g. Heckerman et al., 1995; CooPer & Yoo, 1999; HaUser & Buhlmann, 2012; Peters et al.,
2016; Rothenhausler et al., 2015; Ghassami et al., 2017). Established approaches for causal inference
are often either relying on restrictive assUmPtions or on conditional indePendence testing, which is
hard (Shah & Peters, 2018). Furthermore, most of these approaches either assume full knowledge of
the intervention or make strong assumptions about its form (Heinze-Deml et al., 2018).
1
Under review as a conference paper at ICLR 2020
However, in the real world, interventions are also not always performed by an experimenter. They
can be performed by other agents, or by environmental changes in ways that are unknown, or by
a naive learner (like a robot) which does not know precisely yet how its low-level actions change
high-level causal variables. In this paper, we look at the setting where interventions are unknown, and
our goal is to discover causal graphs given unknown-intervention samples. The challenging aspect of
this setting is to not only learn the causal graph structure, but also predict the intervention accurately.
In this setting, we need to make sure to: (1) avoid an exponential search over all possible DAGs, (2)
handle unknown interventions, (3) model the effect of interventions, and (4) model the underlying
causal structure.
One possibility for learning a causal structure (through SCM modelling) is to perform many ex-
periments in which one executes interventions. Thus, such interventions modify the effect of the
intervened upon variable from its parents in the corresponding DAG, which the model has to quickly
adapt to. We can make parallel connections to meta-learning, where the inner loop can be consid-
ered as fast adaptation to the distribution change, and outer loop can be considered as learning the
stationary meta-parameters of the model. For causal induction, one can consider each distribution
which arises as a result of an intervention as a meta-example, and use a meta-learning objective for
fast adaptation in response to an intervention. One can think of model parameters as being composed
of slow- and fast-changing parameters. The slow parameters are analogous to the meta-parameters in
meta-learning and are used for (1) intervention prediction in order to handle the unknown intervention
and for (2) modeling the underlying causal structure. On the other hand, the fast parameters are used
to model the effect of interventions. An explicit search over the exponentially-growing space of all
possible DAGs is avoided by modeling the conditionals of the structural causal model using function
approximators, with one neural network per variable. The belief over whether one node i is a direct
causal parent of another node j corresponds to a dropout probability for the i-th input of network j
(which predicts variable j given its parents). This cheaply represents all 2M2 possible model graphs,
with the graph search implicitly achieved by learning these dropout probabilities. We thus propose a
new method for fast adaptation and learning of neural causal models by framing the problem in a
meta-learning setting, similar to (Dasgupta et al., 2019; Bengio et al., 2019).
Our contributions Our key contributions can be summarized as follows:
•	Handle causal induction to the case where the variable on which a soft intervention took
place is not known by the learner, and show that better results can be obtained when the
learner attempts to infer that information and uses it to appropriately change parameters and
meta-parameters.
•	We bypass the issue of having to optimize over and represent an exponentially large set of
discrete causal graphs by learning an efficiently parametrized ensemble of SCMs,
•	Show that our algorithm correctly identifies the causal graph and use the learned graph for
generalization to an unseen environment.
2	Preliminaries
A Structural Causal Model (SCM) (Peters et al., 2017) over a finite number M of random variables
Xi is a set of structural assignments
Xi :=fi(Xpa(i,C),Ni) ,	∀i ∈ {0,...,M -1}	(1)
where Ni is jointly-independent noise and pa(i, C) is the set of parents (direct causes) of variable
i under configuration C of the SCM directed acyclic graph, i.e., C ∈ {0, 1}M×M, with cij = 1 if
node i has node j as a parent (equivalently, Xj ∈ Xpa(i,C); i.e. Xj is a direct cause of Xi). The n-th
power of an adjacency matrix, Cn, counts the number of length-n walks from node i to node j of the
graph in element cij. The trace of the n-th power of an adjacency matrix, Tr(Cn), counts the number
of length-n cycles in the graph. Causal structure learning is the recovery of the ground-truth C from
observational and interventional studies.
Functional and structural meta-parameters Let us consider the simplest SCMs, those with
M = 2 random variables (lets say random variables A and B). Only three DAGs exist relating
them; they are A → B, B → A or A⊥B . These can be represented as the following. If A causes
2
Under review as a conference paper at ICLR 2020
B, then the SCM between A and B can be represented as B = fθB (CBA ∙ A, ∈b), where CBA = 1,
EB 〜NB is a sample of an independent noise factor. If B causes A, then the SCM can be presented
as A = fθA (CAB ∙ B, ea), where CAB = 1, EA 〜NA is a sample of an independent noise factor. If
A and B are independent, then we have the same equations but we set CAB = 0 and CBA = 0.
We may now think of learning the structural causal model as learning the two probabilities of
CAB or CBA being 1 (versus 0), representing our belief in the causal relationship between A and
B. We can parametrize these probabilities differentiably using P (CAB = 1) = σ(γAB) and
P(CBA = 1) = σ(γBA), with Y real numbers and σ(x)= 阡匕. We can also parametrize
the structural equations fθA and fθB in a differentiable manner, using conditional probability tables
(CPTs) or neural networks.
Hence, the problem becomes simultaneously learning the structural meta-parameters γ and the
functional meta-parameters θ. Functional meta-parameters θ may be easily learned by maximum
likelihood or a proxy, and using backpropagation. However, the γ are more difficult to infer. Bengio
et al. (2019) propose to learn them by using observations of how the distribution changes sparsely,
with the transfer generalization (the adaptation rate after the sparse change) being the training
objective for γ. Simultaneously inferring both θ and γ is still more difficult. An M -variable SCM
over random variables Xi, i ∈ {0, . . . , M - 1} can induce a super-exponential number of adjacency
matrices C. The super-exponential explosion in the number of potential graph connectivity patterns
and the super-exponentially growing storage requirements of their defining conditional probability
tables make CPT-based parametrizations of the structural assignments fi increasingly unwieldy as M
scales. As shown below, neural networks with Cij -masked inputs can provide a more manageable
parametrization. For more background about different kinds of intervention we ask the reader to refer
Appendix A.3.
3	Proposed Framework: Meta Learning for Causal Induction
Our framework disentangles the slow-changing meta-parameters, which reflect the stationary prop-
erties discovered by the learner, and the fast-changing parameters, which adapt in response to
interventional changes in distribution. We consider two kinds of meta-parameters: the causal graph
structure γ and the model’s slow weights θslow, along with the meta-learning objective for both
of them. We also consider one kind of parameter: the model’s fast weights, θfast. We will call
θ = θslow + θfast the sum of the slow, stationary meta-parameters and the fast, adaptational parameters.
3.1	Task Description
Our task setup deviates from most common deep learning modeling setups. For the purposes of this
work, we restrict ourselves to inference of randomly-generated or manually-provided ground-truth
SCMs of M categorical random variables causally related via a DAG. The model is permitted to
see (1) data from the original ground-truth model, and (2) data from a modified ground-truth model
with a random intervention applied. In our experiments, at most one intervention is concurrently
performed. When an intervention is performed, a single node is randomly and uniformly chosen
among all M nodes, and its ground-truth distribution soft-intervened upon. The learner model is
aware of the samples having come from an intervention distribution, but is not aware of the identity
of the intervention node, and so must predict it. Each run of sampling steps under a given intervention
is referred to as an episode. The learner, over a large number of episodes, will experience all nodes
being intervened upon, and should be able to infer the SCM from these interventions.
3.2	Causal Induction as an Optimization Problem
We first explain how we mitigate the problem of searching in the super-exponential set of graph
structures. If there are M such variables, the strategy of considering all the possible structural
graphs as separate hypotheses is not feasible because it would require maintaining O(2M2 ) models
of the data. We note that we can cheaply choose any of the 2M2 possible DAGs through suitable
independent Bernoulli choices Cij associated with each edge i → j of the causal graph, i.e., sampling
all the Cij ’s independently. Then we only need to learn the M2 coefficients γij, and we implicitly
maintain a distribution over the 2M2 models corresponding to all the possible draws of Cij . Note that
a slight dependency between the Cij is induced if we require the causal graph to be acyclic (which
3
Under review as a conference paper at ICLR 2020
allows one to sample the X using ancestral sampling). To enforce that constraint it is not sufficient to
require CijCji = 0 (both cannot be 1). We deal with this problem with a regularizer acting on the Y's
in order to favour acyclic solutions (Zheng et al., 2018).
In our approach, each random vari-
able’s structural assignment is mod-
eled via Xi := fθi (Ci0 × X0,Ci1 ×
*
X1, ..., Cim × Xm,i), where fθi () is
a neural network (MLP) with parame-
ters θi, and Cij 〜 Bin(Sigmoid(Yij)).
Through this construction we can
frame the causal induction problem
as an optimization problem, with θ
optimized to maximize the likelihood
of data under the model but Y opti-
mized with respect to a meta-learning
objective arising from changes in
distribution because of interventions.
[O	0.088	0.090	1
0.894	0	0.045	→
0.973	0.116	0	J
Softmax
≡
0.03
0.97
A
OeV) →
1-hot sample A = [0,1]
1-hot sample B = [1,0]
1-hot sample C = [0,1]
Masking input values with edge presence configuration	MLPs
Figure 1: MLP Model Architecture for M = 3, N = 2 (fork3) SCM. The model
computes the conditional probabilities of A, B, C given their parents using a stack
of three independent MLPs. The MLP input layer uses an adjacency matrix sampled
from Ber(σ(γ)) as an input mask to force the model to make use only of parent
nodes to predict their child node.
There are a few benefits for learning
a parametrized ensemble of SCMs.
Such an ensemble is analogous to
an ensemble of neural nets differing
by their binary input dropout masks,
which select what variables are used as predictors of another variable.
3.3	Fast Adaptation by Meta-Learning
Fast and slow weights To disentangle an environment’s stable, unchanging properties (the causal
structure) from unstable, changing properties (the effects of an intervention), we proposed in §3 to
distinguish between the model’s functional meta-parameters θslow and parameters θfast. The sum of
these weights, θ = θslow + θfast, parametrizes the MLPs computing the conditionals Pi(Xi|Xpa(i) ; θi).
The fast weights and the slow weights terminology is drawn from Hinton & Plaut (1987). The
construction of θ as a sum of initial, slow weights plus zeroed, fast weight that are then allowed to
quickly adapt during a transfer episode is due to MAML (Finn et al., 2017). The ability to generalize
out-of-distribution by adapting to a transfer distribution can then be measured by the likelihood after
adapting the fast weights on transfer data.
Since an intervention is generally not persistent from one transfer distribution to another, the model’s
functional parameters (θfast) are reset after each episode of transfer distribution adaptation. The
meta-parameters (θslow, Y) are preserved, then updated after each episode. Inspired by Bengio et al.
(2019), the meta-objective for each meta-example over some intervention distribution Dint is the
following1 "meta-transfer" loss:
R = -EX 〜Dint[log EC 〜Ber(Y) Y Lc,i (X ； θslow )]]	(2)
i
where X is an example sampled from the intervention distribution Dint , C is an adjacency matrix
drawn from our belief distribution (parametrized by Y) about graph structure configurations and
Lc,i(X) = P(Xi ∣Xpa(i,C) ； θslow)	(3)
is the likelihood of the i-th variable Xi of the sample X , when predicting it under the configuration
C from the set of its putative parents, Xpa(i,C) .
Structural Parameter Gradient Estimator Because a discrete Bernoulli random sampling pro-
cess is used to produce the configurations under which the log-likelihood of data samples is obtained,
we require a gradient estimator to propagate gradient through to the Y structural meta-parameters.
1 These equations differ from Bengio et al. (2019) in that the t indices and products were dropped, because
the computation is not online.
4
Under review as a conference paper at ICLR 2020
We adopt for this purpose the gradient estimate of Bengio et al. (2019) (but see footnote 1):
gij
Pk (σ(Yij I-CaLC式 X)
Pk LCki(X)
(4)
where the (k) superscript indicates the values obtained for the k-th draw of C. This gradient is
estimated solely with θslow because estimates employing θ have much greater variance.
Acyclic Constraint We add a regularization term to the loss term that discourages the model from
having length-2 cycles in the learned adjacency matrix. The regularizer term is
JDAG =	cosh(σ(γij)σ(γji)),	∀i,j ∈ {0, . . . , M-1}
i6=j
and is derived from Zheng et al. (2018). The details of the derivation are in the Appendix.
3.4	Model Description
Learner Model We model the M structural assignments Xi := fi(Xpa(i,C), Ni) (Eq. 1) of the
SCM as a setofM multi-layer perceptrons (MLPs), as in Bengio et al. (2019). The MLPs are identical
in shape but do not share any parameters, since they are modeling independent causal mechanisms.
Each possesses an input layer of M × N neurons (for M one-hot vectors of length N each), a single
hidden layer chosen arbitrarily to have max(4M, 4N) neurons with a LeakyReLU activation of
slope 0.1, and an output layer of N neurons representing the unnormalized log-probabilities of each
category. To force fi to rely exclusively on the direct ancestor set pa(i, C) under adjacency matrix C
(See Eqn. 2), the one-hot input vector Xj for variable Xi ’s MLP is masked by the Boolean element
cij. The functional parameters of the MLP are the set θ = {W0ihjn, B0ih, W1inh, B1in}.
An example of the multi-MLP architecture with M = 3 categorical variables of N = 2 categories is
shown in Figure 1.
Ground-Truth Model In our experiments, ground-truth SCM models are parametrized as a set of
MLPs of the same size as the learner models, thus avoid having to manually define the Conditional
Probability Tables (CPTs). Ground-truth models exist in two variants that differ mainly in initial-
ization: synthetic, where the θ are randomly initialized and the γ are either randomly-initialized or
pre-specified; and real-world, where the θ and γ are both initialized so as to closely replicate the
CPTs of given Bayesian networks.
3.5	Interventions
Soft interventions To execute an intervention on variable Xi , we reinitialize Xi ’s ground-truth
MLP parameters randomly while leaving other variables’ MLPs untouched. A copy of the old
parameters is saved, allowing the intervention to be undone by resetting the parameters back to their
original values.
Predicting interventions After an intervention on Xi , the gradients into the learned model’s γi
and the slow weights for the i-th conditional are false, because they do not bear the blame for Xi ’s
outcome (which lies with the intervener). We find that ignoring this issue considerably hurts or slows
down meta-learning, suggesting that we should try to infer on which variable the intervention took
place. For this purpose, we take advantage of the fact that the conditional likelihood of the intervened
variable tends to have a poorer relative likelihood under Dint, so we pick the variable with the greatest
deterioration in likelihood as our guess.
3.6	Training algorithm
The structural meta-parameters σ(γij) represent the belief in the hypothesis that node i has node j as
a direct causal parent. We may sample from this belief, obtaining different configurations (causal
structures) of the causal graph. We hypothesize that the correct configuration enables better adaptation
to a slight change in distribution, e.g. resulting from a soft intervention. Hence, we evaluate different
configurations under the transfer distribution; those giving a higher transfer likelihood under θslow get
5
Under review as a conference paper at ICLR 2020
a higher reward and their probability is increased. The functional (meta-)parameters are trained as
usual by gradient ascent on the log-likelihood. The details of the training algorithm is in Section A.2
in the Appendix.
Synthetic datasets use either a specified edge structure for γ, or randomly initialize γij such that it
is Boolean, strictly lower-triangular, and each row has an expected sum of 1-5 (and therefore each
node expects 1-5 direct ancestors). Real-world datasets, specified as CPTs, must first be converted or
approximated by a near-ground-truth MLP. We use the graph’s proper edge structure to initialize γ,
and learn θ by training individually each MLP in the set to replicate the correct pre-softmax logits. In
practice, excellent reproductions of the CPT can be achieved.
Stability of training Our model requires simultaneous training of both the structural and the
functional meta-parameters, but these are not independent and do influence each other, which leads
to instability in training. For example, if σ(γij ) ≈ 0 incorrectly, the i-th MLP does not learn to use
input Xj , and vice-versa, if the i-th MLP has not learned to properly use input Xj , this will favour
pushing σ(γij ) towards 0. To overcome this instability, we pretrain the model under observational
data (from the distribution of the data before interventions) using dropout on the inputs. This ensures
that the functional meta-parameters θslow are not too biased towards certain configurations of the
meta-parameters γ .
4	Related Work
The recovery of the underlying structural causal graph from observational and interventional data is
a fundamental problem (Pearl, 1995; 2009). Different approaches have been studied, score-based,
constraint-based and asymmetry-based methods. Score-based methods search through the space of
all possible directed acyclic graphs (DAGs) representing the causal structure based on some form of
scoring function for network structures (Chickering, 2002; Tsamardinos et al., 2006; Goudet et al.,
2017; Hauser & Buhlmann, 2012; Heckerman et al., 1995; Cooper & Yoo, 1999; Hauser & Buhlmann,
2012). Constraint-based methods (Spirtes et al., 2000; Sun et al., 2007; Zhang et al., 2012) infer the
DAG by analyzing the conditional independence of data. Eaton & Murphy (2007b) use dynamic
programming techniques to accelerate Markov Chain Monte Carlo (MCMC) sampling in a Bayesian
approach to structure learning for discrete variable DAGs. Asymmetry-based methods (Shimizu et al.,
2006; Hoyer et al., 2009; Daniusis et al., 2012; Budhathoki & Vreeken, 2017; Mitrovic et al., 2018)
assume asymmetry between cause and effect in the data and try to use this information to estimate the
causal structure. Recently Peters et al. (2016); Ghassami et al. (2017) proposed to exploit invariance
across different environments to infer causal structure, but are difficult to scale to large graphs due to
the necessary iteration over the super-exponential set of possible graphs.
For interventional data, itis often assumed that the models have access to full intervention information,
which is rare in the real world. Rothenhausler et al. (2015) have investigated the case of additive
shift interventions, while Eaton & Murphy (2007a) have examined the situation where the targets of
experimental interventions are imperfect or uncertain. This is different from our setting where the
intervention is unknown to start with and is assumed to arise from other agents and the environment.
Learning based methods have been proposed (Guyon, a;b; Lopez-Paz et al., 2015) and there also exist
recent approaches using the generalization ability of neural networks to learn causal signals from
purely observational data (Kalainathan et al., 2018; Goudet et al., 2018). Neural network methods
equipped with learned masks, such as (Ivanov et al., 2018; Li et al., 2019; Yoon et al., 2018; Douglas
et al., 2017), exist in the literature, but only a few (Kalainathan et al., 2018) have been adapted to
causal inference. This last work is, however, tailored for causal inference on continuous variables
and from observations only. Adapting it to a discrete-variable setting is made difficult by its use of a
Generative Adversarial Network (GAN) Goodfellow et al. (2014) framework.
Turning now to meta-learning, Dasgupta et al. (2019) have used it to learn to make predictions under
interventions. However, their approach does not induce a causal graph, neither explicitly nor via
decoding. Thus, it cannot be used for general causal discovery, but only to make predictions of variable
values. Most similar to our work, Bengio et al. (2019) proposes a meta-learning framework for
learning causal models from interventional data. However, the proposed method (Bengio et al., 2019)
explicitly models every possible set of parents for every child variable and attempts to distinguish the
best among them. Because there are combinatorially-many such parent sets, the method cannot scale
beyond trivial graphs. In our work, we bypass this restriction by modeling the edge between any 2
variables as a dropout probability and hence our model only scales quadratically with the graph size.
6
Under review as a conference paper at ICLR 2020
5	Experimental setup and results
Our experiments aim to evaluate the proposed method to recover the correct causal structure and
which elements of the method matter. We first evaluate our model on a synthetic dataset where we
have control over the number of variables and causal edges in the ground-truth SCM. This allows
us to verify after learning with what accuracy we recover the individual decisions about cij and
understand the performance of our algorithm under various conditions. We then evaluate our method
on real world datasets collected from the BnLearn dataset repository, and show that the proposed
approach recovers the true causal structure. We then show that the trained models can correctly
predict the consequences of previously unseen interventions on the rest of the graph. We also perform
ablations showing how important is each component of the model.
5.1	Synthetic Datasets
∙ ∙ ∙ ∙ ∙ ∙ ∙ ∙ ∙
•	. ∙ ∙ ∙ . ∙ ∙ ∙ ； . ∙ ∙
•	∙■ ∙ ∙
•	∙ ∙ ∙ ∙ ∙ ∙ ∙ ■
Figure 2: Learned edges at three different stages of training. Left: Chain graph with 4 variables. Right: Fully-connected DAG graph with 4
variables.
We first evaluate the model’s performance on several randomly-initialized SCMs with specific,
representative graph structures. For M = 3-variable DAGs, we consider every possible connected
graph: chain3, fork3, collider3 and confounder3 (See Fig. 7 in appendix). They exhibit
every graph sub-structure that can exist in larger graphs, and must be mastered before tackling larger
graphs. Since the number of possible DAGs grows super-exponentially with the number of variables,
for M > 3 up to 8 a selection of representative and edge-case graphs are chosen. The chainM and
fullM graphs (M = 3-8) are the minimally and maximally connected M-variable graphs, while the
remaining graphs are randomly generated with a varying sparsity level (1-4 expected number of
parents per node). The details of the setup can be found in Appendix A.5.
Results The model can successfully recover the correct edge structure for all synthetic graphs con-
sidered. The learning curves plotting the average cross-entropy (CE) loss for the learned edges against
the ground-truth model for M = 3 are shown in Figure 7. The fully-connected confounder3
graph is particularly easy to learn, but all 3-variable graphs are learned perfectly. Plots of the same
process on 4- through 8-variable graphs were similarly encouraging, with all models converging to
a negligible loss. The results are, however, sensitive to some hyperparameters, notably the DAG
penalty and the sparsity penalty.
5.2 Real-World Datasets: BnLearn
The Bayesian Network Repository is a collection of commonly-
used causal Bayesian networks from the literature, suitable for
Bayesian and causal learning benchmarks. We evaluate our
model on the Earthquake (Korb & Nicholson, 2010), Cancer
(Korb & Nicholson, 2010) and Asia (Lauritzen & Spiegelhalter,
1988) datasets (M =5, 5 and 8-variables respectively, max- Figure 3: Earthquake: Learned edges at three dif-
imum 2 parents per node) in the BnLearn dataset repository. ferent stages of training.
The ground-truth SCM is given for each dataset, and the functional parameters are represented as
conditional probability tables (CPTs). We learn a near-ground-truth MLP from the dataset’s CPT and
use it as the ground-truth data generator. We also insert a (greater than 1) temperature factor in order
to increase the likelihood of sampling some very rare events in the CPTs. Details of the setup can be
found in Appendix A.5.1.
Results The model can successfully recover the correct edge
structure for all BnLearn graphs considered up to and including
8-variable Asia. Figures 3 and 4 illustrate what the model
has learned at several stages of learning. In these figures, the
σ(γij ) and cij adjacency matrix elements are plotted as colored
squares and dots. Off-diagonal terms are unknown, and appear
yellow.
Figure 4: Asia: Learned edges at three different
stages of training.
7
Under review as a conference paper at ICLR 2020

—chain3
一fork3
—collider3
一confounder3
5K	10K	15K
Steps (#)
—Chain4
-Chain5
-Chain6
—Chain7
-Chain3
—fork3
— confounder3
10K	15K	20K	25K	30K	35K 40K	45K
Steps (#)
5K 10K	15K	20K	25K	30K
Steps (#)


Figure 5: Left: Cross entropy (CE) for edge probability between learned and ground-truth graphs for all 3-variable SCMs. Error bars are ±1σ
over PRNG seeds 1-5. Middle: Edge CE loss for the chain graph with 4-7 variables. Right: Edge CE loss for 3-variable graphs with no
dropout during pretraining, showing the importance of this dropout.
1.0
0.8
0.6
0.4
0.2
0.0IK	5K	10K	15K	^20K
Steps (#)
1.0
0.8
0.6
0.4
0.2
—chain3, LDAG
-	-ChaIn3, no LDAG
—fork3, LDAG
-	-fork3, no LDAG
一collider3, LDAG
-	-collider3, no LDAG
—	confounder3, LDAG
-	-confounder3, no LDAG
三一y 二二
0.0IK	5K	10K	15K	^20K
Steps (#)
Figure 6: Ablations study results on all possible 3 variable graphs. Both graphs show the cross-entropy loss on learned vs ground-truth
edges over training time. Left: Models that infer the intervention (prediction, bold) vs models that have knowledge of the true intervention
(ground truth, long dash) vs models that use no knowledge of the intervention at all (no prediction, short dash). Result suggests inferring the
intervention works almost as well as knowing the true intervention. Right: Comparisons of model trained with and without DAG regularizer
(LDAG), showing that DAG regularizer helps convergence.
The color of the squares indicates belief in the presence or
absence of an edge (σ(γij )), while the color of the dot indicates the ground truth cij . Red indicates
(belief in) an edge; Blue indicates (belief in) the absence of an edge. Yellow indicates maximum
uncertainty At the beginning of training, the main diagonal is a priori known to be clear of edges,
and therefore is solid blue.
As training progresses, the beliefs approach the ground truth, which visually appears as the squares
converging towards the color of the dot within them. When they coincide, the dot vanishes. An
erroneous belief stands out as a red dot on blue square or, vice-versa, a blue dot on red square.
Because we pre-sort the nodes in BnLearn datasets so that they are in topological order, the model
must learn a lower triangle. This corresponds to a completely blue uppper triangle. Anti-causal
violations are easily recognizable as non-blue squares in the upper triangle.
Baseline comparisons We compared our method to ICP (Peters et al., 2016) and Eaton & Murphy
(2007a). Eaton & Murphy (2007a) handles uncertain interventions and Peters et al. (2016) handles
unknown interventions. However, neither attempt to predict the intervention.
Table 1: Baseline comparisons: Cross entropy (lower is better) for edge probability on learned and ground-truth edges on Asia graph.
compared to to Peters et al. (2016), (Eaton & Murphy, 2007a) and (Zheng et al., 2018)
Our method (Eaton & Murphy, 2007a) (Peters et al., 2016) (Zheng et al., 2018)
0.0	0.0	10.7	3.1
Importance of Dropout To perform initial pretraining for an observational distribution, sampling
adjacency matrices is required. One may be tempted to make these “fully-connected” (all-ones except
for a zero diagonal), to give the MLP maximum freedom to learn any potential causal relations
itself. We demonstrate that pretraining cannot be carried out this way, and that it is necessary to
“drop out” each edge (with probability 0.5 in our experiments) during pre-training of the conditional
distributions of the SCM. We attempt to recover the previously-recoverable graphs chain3, fork3
and confounder3 without dropout, but fail to do so, as shown in Figure 5.
Generalization to Previously Unseen Interventions It is often argued that learning approaches
based on prediction do not necessarily yield models that generalize to unseen experiments, since they
do not explicitly model changes through interventions - in contrast causal models use the concept
8
Under review as a conference paper at ICLR 2020
Table 2: Evaluating the consequences of a previously unseen intervention: (test log-likelihood under intervention)
	fork3	chain3	confounder3	collider3
Our Model	-0.4502	-0.3801	-0.2819	-0.4677
Baseline	-0.5036	-0.4562	-0.3628	-0.5082
of interventions to explicitly model changing environments and hold thus the promise to work even
under distributional shifts (Pearl, 2009; SchOIkoPf et al., 2012; Peters et al., 2017).
To test the robustness of causal modelling to previously unseen interventions (new values for an
intervened variable), we evaluate a well-trained causal model against a non-causal variant model
where all cij = 1, i 6= j . In both cases, an intervention is Performed, and the models, with knowledge
of the intervention, are asked to Predict the rest. For this PurPose, a batch of samPles X are drawn
from Dint and their average log-likelihoods are comPuted and contrasted. The intervention variable’s
contribution to the log-likelihood is ignored.
For all 3-variable graPhs (chain3, fork3, collider3, confounder3), the causal model
attributes higher log-likelihood to the intervention distribution’s samPles than the non-causal variant,
thereby demonstrating causal models’ suPerior generalization ability in transfer tasks. Table 2 collects
these results.
Importance of Inference After the inter-
vention has been Performed, the learner
draws data samPles from the intervention
distribution and comPutes the Per-variable
average log-Probability under samPled ad-
jacency matrices. The variable consistently
Producing the least-likely outPuts is Pre-
Table 3: Intervention Prediction Accuracy: (identify on which variable the
intervention took Place)
3 variables	4 variables	5 variables	8 variables
95 %	90 %	81 %	63 %
dicted to be the intervention node. ExPeriments over all 3-variable DAGs show that this Prediction
mechanism functions well in Practice, yielding far above-random accuracy in correctly Predicting the
intervention node (Table 3), the model Performance droPPed significantly without the Predication
(Figure 6 Left) and is comParable to having the ground-truth intervention (Figure 6 Right).
Effect of DAG Regularizer: To Promote the acyclicity of C, we include a DAG regularizer . This
significantly imProves cross-entroPy of the solution (wrt. ground truth DAG) on all 3-variable graPhs,
as illustrated in Figure 6, and was therefore included in all exPeriments with M > 3 variables.
6 Conclusion
In this work, we introduced a framework for fast adaPtation and slow learning of neural causal
models. We demonstrate through exPeriments that the PrinciPle of oPtimizing an out-of-distribution
meta-learning objective enables the learner to recover the causal graPh structure for graPhs with more
than two variables. To achieve this we introduce an efficient Parametrization of the belief regarding
the underlying graPh structure, imPlemented as an adaPtive form of droPout on the inPuts of MLPs
comPuting the conditionals of the model. This relies on Pre-training the conditionals using agnostic
beliefs and by aPProximately inferring on which variable the intervention took Place. We believe that
our aPProach of treating seemingly observational data as being derived from an environment with
agents executing interventions could rePresent an imPortant change in modelling PersPective with
deePer imPlications.
9
Under review as a conference paper at ICLR 2020
References
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sebastien Lachapelle, OleXa Bilaniuk,
Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal
mechanisms. arXiv preprint arXiv:1901.10912, 2019.
Kailash Budhathoki and Jilles Vreeken. Causal inference by stochastic compleXity. arXiv:1702.06776,
2017.
David MaXwell Chickering. Optimal structure identification with greedy search. Journal of machine
learning research, 3(Nov):507-554, 2002.
Gregory F. Cooper and Changwon Yoo. Causal Discovery from a MiXture of EXperimental and
Observational Data. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial
Intelligence, UAΓ99,pp.116-125, San Francisco, CA, USA, 1999.
Povilas Daniusis, Dominik Janzing, Joris Mooij, Jakob Zscheischler, Bastian Steudel, Kun Zhang,
and Bernhard Scholkopf. Inferring deterministic causal relations. arXiv preprint arXiv:1203.3475,
2012.
Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward
Hughes, Peter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal Reasoning from
Meta-reinforcement Learning. arXiv preprint arXiv:1901.08162, 2019.
Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas, Chris Hart, Adam Baker,
Maneesh Sahani, Yura Perov, and Saurabh Johri. A universal marginalizer for amortized inference
in generative models. arXiv preprint arXiv:1711.00695, 2017.
Daniel Eaton and Kevin Murphy. EXact bayesian structure learning from uncertain interventions. In
Artificial Intelligence and Statistics,pp. 107-114, 2007a.
Daniel Eaton and Kevin Murphy. Bayesian structure learning using dynamic programming and
MCMC. In Uncertainty in Artificial Intelligence, pp. 101-108, 2007b.
Frederick Eberhardt, Clark Glymour, and Richard Scheines. On the number of eXperiments sufficient
and in the worst case necessary to identify all causal relations among n variables. arXiv preprint
arXiv:1207.1389, 2012.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume
70, ICML’17, pp. 1126-1135. JMLR.org, 2017. URL http://dl.acm.org/citation.
cfm?id=3305381.3305498.
AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Kun Zhang. Learning causal
structures using regression invariance. In Advances in Neural Information Processing Systems, pp.
3011-3021, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz, and
Michele Sebag. Causal generative neural networks. arXivpreprint arXiv:1711.08936, 2017.
Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz, and
Michele Sebag. Learning functional causal models with generative neural networks. In Explainable
and Interpretable Models in Computer Vision and Machine Learning, pp. 39-80. Springer, 2018.
Isabelle Guyon. Cause-effect pairs kaggle competition, 2013. URL https://www. kaggle. com/c/cause-
effect-pairs, pp. 165, a.
Isabelle Guyon. Chalearn fast causation coefficient challenge, 2014. URL https://www. codalab.
org/competitions/1381, pp. 165, b.
10
Under review as a conference paper at ICLR 2020
Alain HaUser and Peter Buhlmann. Characterization and greedy learning of interventional markov
equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 13(Aug):
2409-2464, 2012.
David Heckerman, Dan Geiger, and David M Chickering. Learning bayesian networks: The combi-
nation of knowledge and statistical data. Machine learning, 20(3):197-243, 1995.
Christina Heinze-Deml, Marloes H MaathUis, and Nicolai MeinshaUsen. CaUsal strUctUre learning.
Annual Review of Statistics and Its Application, 5:371-391, 2018.
Geoffrey E Hinton and David C PlaUt. Using fast weights to deblUr old memories. In Proceedings of
the ninth annual conference of the Cognitive Science Society, pp. 177-186, 1987.
Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear
caUsal discovery with additive noise models. In Advances in neural information processing systems,
pp. 689-696, 2009.
GUido W Imbens and Donald B RUbin. Causal inference in statistics, social, and biomedical sciences.
Cambridge University Press, 2015.
Oleg Ivanov, Michael FigUrnov, and Dmitry Vetrov. Variational aUtoencoder with arbitrary condition-
ing. arXiv preprint arXiv:1806.02382, 2018.
Diviyan Kalainathan, Olivier Goudet, Isabelle Guyon, David Lopez-Paz, and Michele Sebag. Sam:
StrUctUral agnostic model, caUsal discovery and penalized adversarial learning. arXiv preprint
arXiv:1803.04929, 2018.
Kevin B Korb and Ann E Nicholson. Bayesian artificial intelligence. CRC press, 2010.
Steffen L Lauritzen and David J Spiegelhalter. Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society: Series
B (Methodological), 50(2):157-194, 1988.
Yang Li, Shoaib Akbar, and Junier B Oliva. Flow models for arbitrary conditional likelihoods. arXiv
preprint arXiv=1909.06319, 2019.
David Lopez-Paz, Krikamol Muandet, Bernhard Scholkopf, and Iliya Tolstikhin. Towards a learning
theory of cause-effect inference. In International Conference on Machine Learning, pp. 1452-1461,
2015.
Jovana Mitrovic, Dino Sejdinovic, and Yee Whye Teh. Causal inference via kernel deviance measures.
In Advances in Neural Information Processing Systems, pp. 6986-6994, 2018.
Judea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669-688, 1995.
Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant
prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 78(5):947-1012, 2016.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference: foundations
and learning algorithms. MIT press, 2017.
Dominik Rothenhausler, Christina Heinze, Jonas Peters, and Nicolai Meinshausen. Backshift:
Learning causal cyclic graphs from unknown shift interventions. In Advances in Neural Information
Processing Systems, pp. 1513-1521, 2015.
Bernhard Scholkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.
On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012.
Rajen D Shah and Jonas Peters. The hardness of conditional independence testing and the generalised
covariance measure. arXiv preprint arXiv:1804.07203, 2018.
11
Under review as a conference paper at ICLR 2020
Shohei Shimizu, Patrik O Hoyer, AaPo Hyvarinen, and Antti Kerminen. A linear non-gaussian
acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030,
2006.
Peter SPirtes, Clark N Glymour, Richard Scheines, David Heckerman, ChristoPher Meek, Gregory
CooPer, and Thomas Richardson. Causation, prediction, and search. MIT Press, 2000.
Xiaohai Sun, Dominik Janzing, Bernhard Scholkopf, and Kenji Fukumizu. A kernel-based causal
learning algorithm. In Proceedings of the 24th international conference on Machine learning, PP.
855-862. ACM, 2007.
Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing bayesian
network structure learning algorithm. Machine learning, 65(1):31-78, 2006.
Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. Gain: Missing data imputation using
generative adversarial nets. arXiv preprint arXiv:1806.02920, 2018.
Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Kernel-based conditional
independence test and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.
Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous
optimization for structure learning. In Advances in Neural Information Processing Systems, pp.
9472-9483, 2018.
12
Under review as a conference paper at ICLR 2020
A Appendix
A. 1 DAG Penalty Derivation
Recall, from Zheng et al. (2018):
Theorem 1. A matrix W ∈ Rd×d is a DAG if and only if
h(W) = Tr(eW ◦W) -d= 0
For the two-variable (d = 2) graph with adjacency matrix
W
0
σ(w21)
σ(w12)
0
we have
∞ An
Tr(exp(A)) = TrEl
n=0 n!
∞
Tr(exp(W ◦ W)) = Tr X -J
n!
n=0
0	σ2 (w12)	n
σ2(w21)	0
There can only be even- or odd-length paths in a graph. Because, in a two-variable graph with no
self-edges, all even-length paths are cycles and none of the odd-length paths are, we have
2k
}
TrXX 1	(	0 σ2(wi2)
V (2k)! ∖ σ2(w21)	0
'------------------{z------------
0
∞
2X
k=0
k
σ2k (w12)σ2k (w21)
(2k)!
= cosh(σ(w12)σ(w21))
A pairwise generalization to multinode graphs over all i 6= j is:
JDAG =	cosh(σ(wij )σ(wji))
i6=j
13
Under review as a conference paper at ICLR 2020
A.2 Training Algorithm
In this section, we describe the training algorithm in detail.
Algorithm 1 Training Algorithm
1:	procedure TRAINING(Categorical Distribution D, with M nodes and N categories)
2:	Let i an integer from 0 to M - 1
3:
4:	for kpretrain steps do	. Pretraining Loop
5:	X 〜D	. Sample data from D
6:	C 〜Ber(σ(γ))	. Sample config from structure distribution
7:	L = - log P (x|c)	. Compute log-probability of data given config
8:	θslow - Adam(θslow, VθL)	. Update θslow
9:
10:
11:
12:
13:
14:
15:
16:
17:
for kintervention steps do
I_N — randint(0, M — 1)
Dint := D with intervention on node I_N
if predicting intervention then
Li — 0 ∀i
for kpredict steps do
X 〜Dint
C〜 Ber(σ(γ))
Li ― Li + — log Pi(x∣Ci; θslow)∀i
. Interventions Loop
. Prediction Loop
. Draw batch of data from D
. Draw config from structure distribution
. Accumulate NLL for every node i separately
18:	I_N — argmax(Li)
19:	gammagrads, logregrets = [], []
20:	for kepisode steps do	. Transfer Episode Adaptation Loop
21:	X 〜Dint
22:	gammagrad, logregret = 0, 0
23:	for kcfg steps do	. Configurations Loop
24:	C〜 Ber(σ(γ))
25:	Li = — log Pi(X|Ci; θslow) ∀i
26:	gammagrad += σ(γ) — C	. Accumulate γ gradient
27:	logregret += P Li	. Accumulate regret
	i6=I_N
28:	gammagrads.append(gammagrad)
29:	logregrets.append(logregret)
30:	J — λMaxEnt LMaxEnt (Y) + λSparse LSParSe(Y) + Xdag LDAG(Y)	. RegularizerS
31:	VY — VY J + P gammagradskijlogregrets. Softmax (0) ki	. γ gradient estimator k
32:	γ — Adam(γ, VY)	. Update Y
A.3 Preliminaries
Interventions In a purely-observational setting, it is known that causal graphs can be distinguished
only up to a Markov equivalence class. In order to identify the true causal graph intervention data is
needed (Eberhardt et al., 2012). Several types of common interventions may be available (Eaton &
Murphy, 2007a). These are: No intervention: only observational data is obtained from the ground
truth causal model. Hard/perfect: the value of a single or several variables is fixed and then ancestral
sampling is performed on the other variables. Soft/imperfect: the conditional distribution of the
variable on which the intervention is performed is changed. Uncertain: the learner is not sure of
which variable exactly the intervention affected directly. Here we make use of soft interventions for
several reasons: First, they include hard interventions as a limiting case and hence are more general.
Second, in many real-world scenarios, it is more difficult to perform a hard intervention compared to
a soft one. We also deal with a special case of uncertain interventions, where the variable selected for
intervention is random and unknown. We call these unidentified or unknown interventions.
Causal sufficiency The inability to distinguish which causal graph, within a Markov equivalence
class, is the correct one in the purely-observational setting is called the identifiability problem. In
our setting, all variables are observed (there are no latent confounders) and all interventions are
14
Under review as a conference paper at ICLR 2020
random and independent. Hence, within our setting the true causal graph is always identifiable in
principle (Eberhardt et al., 2012; Heinze-Deml et al., 2018). We consider here situations where a
single variable is randomly selected and intervened upon with a soft or imprecise intervention, its
identity is unknown and must be inferred.
A.4 Experimental setup
For all datasets, the weight parameters for the learned model is initialized randomly. In order to not
bias the structural parameters, all γ is initialized to 0.5 in the beginning of training.
A.5 Synthetic data
SCM with n variables is modeled by n feedforward neural networks (MLPs) as described in section
3.1. For simplicity, we assume use an acyclic causal graph such that we could easily sample from
it. Hence, given any pair of random variables A and B, either A -→ B, B -→ A or A and B are
independent.
The MLP representing the ground-truth SCM has its weights θ initialized use orthogonal initialization
with gain 2.5 and the biases are initialized using a uniform initialization between -1.1 and 1.1, which
was empirically found to yield "interesting" yet learnable random SCMs.
Chain3
fork3
Collider3
ConfoUnder3
Figure 7: Left: Every possible 3-variable connected DAG. Right: Cross entropy for edge probability between learned and ground-truth SCM
for all 3-variable SCMs.
A.5.1 BnLearn data repository
The repo contains many datasets with various sizes and structures modeling different variables. We
evaluate our model on 3 of the datasets in the repo, namely the Earthquake (Korb & Nicholson,
2010), Cancer (Korb & Nicholson, 2010) and Asia (Lauritzen & Spiegelhalter, 1988) datasets. The
ground-truth model structure for the Cancer (Korb & Nicholson, 2010) and Earthquake (Korb &
Nicholson, 2010) datasets are shown in Figure 8. Note that even though the structure for the 2 datasets
seems to be the same, the conditional probability tables (CPTs) for these 2 datasets are very different
and hence results in different structured causal models (SCMs) for the 2 datasets.
Figure 8: Left: Ground Truth SCM for Cancer. Middle: Groundtruth SCM for Earthquake. Right: Groundtruth SCM for Asia.
15
Under review as a conference paper at ICLR 2020
A.5.2 Training ground-truth data generator
Because a CPT is capable of representing any distribution, and MLPs are strictly less powerful in
this respect, it may not be possible to learn perfectly the distribution with our MLP learner model.
We therefore train a near-ground-truth MLP to replicate as closely as possible the CPT’s probability
table, and then use this trained MLP are the ground-truth SCM data generator.
Training is by 1000 iterations of full-batch gradient descent with learning rate 0.001 and momentum
0.9, with all possible parent values masked with the ground-truth vector γi . The objective is to
minimize mean squared error between the MLP’s logits and the log-probability as drawn from the
CPT. If the CPT contains a zero, it is approximated by a logit of -100.
Given that some of the CPTs contain very unlikely events, we have found it necessary to add a
temperature parameter in order to make them more frequent. The near-ground-truth MLP model’s
logit outputs are divided by the temperature before being used for sampling. Temperatures above
1 result in more uniform distributions for all causal variables; Temperatures below 1 result in less
uniform, sharper distributions that peak around the most likely value. We find empirically that a
temperature of about 2 is required for our BnLearn benchmarks.
A.6 Effect of Sparsity
We use a L1 regularizer on the structure parameters γ to encourage a sparse representation of edges in
the causal graph. In order to better understand the effect of the L1 regularizer, we conducted ablation
studies on the L1 regularizer. It seems that the regularizer has an small effect on rate of converges
and that the model converges faster with the regularizer. This is shown in Figure 9
Figure 9: Effect of Sparsity: On 5 variable, 6 variable and 8 variable Nodes
A.7 Effect of Temperature
As noted in section A.5.1, we have introduced a temperature hyperparameter in order to encourage
the groundtruth model to generate some very rare events in the conditional probability tables (CPTs)
more frequently. We run ablation studies to understand the importance of the temperature term. A
temperature of 1 corresponds to no changes to the underlying CPTs. As shown in Figure 12, for the
Cancer (Korb & Nicholson, 2010) dataset, a temperature of 2 improves the accuracy of causal graph
recovery.
Figure 10: Cross entropy for edge probability between learned and ground-truth SCM for Cancer at varying temperatures.
16
Under review as a conference paper at ICLR 2020
Figure 11: Cross entropy for edge probability between learned and ground-truth SCM. Left: The Earthquake dataset with 6 variables. Right:
The Asia dataset with 8 variables
)stib gva( yportnE-ssorC ammaG
Figure 12: Left: SCM for cross5 graph. Right: Cross entropy for edge probability between learned and ground-truth SCM for chain5
and cross5, comparing the learning process with prediction of edges to ground-truth intervention
17