Under review as a conference paper at ICLR 2020
Low-Rank Training of Deep Neural Networks
for Emerging Memory Technology
Anonymous authors
Paper under double-blind review
Ab stract
The recent success of neural networks for solving difficult decision tasks has
incentivized incorporating smart decision making “at the edge.” However, this
work has traditionally focused on neural network inference, rather than training, due
to memory and compute limitations, especially in emerging non-volatile memory
systems, where writes are energetically costly and reduce lifespan. Yet, the ability
to train at the edge is becoming increasingly important as it enables real-time
adaptability to device drift and environmental variation, user customization, and
federated learning across devices. In this work, we address two key challenges
for training on edge devices with non-volatile memory: low write density and
low auxiliary memory. We present a low-rank training scheme that addresses
these challenges while maintaining computational efficiency. We then demonstrate
the technique on a representative convolutional neural network across several
adaptation problems, where it out-performs standard SGD both in accuracy and in
number of weight writes.
1	Introduction
Deep neural networks have shown remarkable performance on a variety of challenging inference
tasks. As the energy efficiency of deep-learning inference accelerators improves, some models are
now being deployed directly to edge devices to take advantage of increased privacy, reduced network
bandwidth, and lower inference latency. Despite edge deployment, training happens predominately
in the cloud. This limits the privacy advantages of running models on-device and results in static
models that do not adapt to evolving data distributions in the field.
Efforts aimed at on-device training address some of these challenges. Federated learning aims to
keep data on-device by training models in a distributed fashion (Konecny et al., 2016). On-device
model customization has been achieved by techniques such as weight-imprinting (Qi et al., 2018),
or by retraining limited sets of layers. On-chip training has also been demonstrated for handling
hardware imperfections (Zhang et al., 2017; Gonugondla et al., 2018). Despite this progress with
small models, on-chip training of larger models is bottlenecked by the limited memory size and
compute horsepower of edge processors.
Emerging non-volatile (NVM) memories such as resistive random access memory (RRAM) have
shown great promise for energy and area-efficient inference (Yu, 2018). However, on-chip training
requires a large number of writes to the memory, and RRAM writes cost significantly more energy
than reads (e.g., 10.9 pJ/bit versus 1.76 pJ/bit (Wu et al., 2019)). Additionally, RRAM endurance is
on the order of 106 writes (Grossi et al., 2019), shortening the lifetime of a device due to memory
writes for on-chip training.
In this paper, we present an online training scheme amenable to NVM memories to enable next
generation edge devices. Our contributions are (1) an algorithm called Streaming Kronecker Sum
Approximation (SKS), and its analysis, which addresses the two key challenges of low write density
and low auxiliary memory; (2) two techniques “gradient max-norm” and “streaming batch norm” to
help training specifically in the online setting; (3) a suite of adaptation experiments to demonstrate
the advantages of our approach.
1
Under review as a conference paper at ICLR 2020
2	Related Work
Efficient training for resistive arrays. Several works have aimed at improving the efficiency of
training algorithms on resistive arrays. Of the three weight-computations required in training (forward,
backprop, and weight update), weight updates are the hardest to parallelize using the array structure.
Stochastic weight updates (Gokmen & Vlasov, 2016) allow programming of all cells in a crossbar at
once, as opposed to row/column-wise updating. Online Manhattan rule updating (Zamanidoost et al.,
2015) can also be used to update all the weights at once. Several works have proposed new memory
structures to improve the efficiency of training (Soudry et al., 2015; Ambrogio et al., 2018). The
number of writes has also been quantified in the context of chip-in-the-loop training (Yu et al., 2016).
Distributed gradient descent. Distributed training in the data center is another problem that suffers
from expensive weight updates. Here, the model is replicated onto many compute nodes and in each
training iteration, the mini-batch is split across the nodes to compute gradients. The distributed gradi-
ents are then accumulated on a central node that computes the updated weights and broadcasts them.
These systems can be limited by communication bandwidth, and compressed gradient techniques (Aji
& Heafield, 2017) have therefore been developed. In Lin et al. (2017), the gradients are accumulated
over multiple training iterations on each compute node and only gradients that exceed a threshold are
communicated back to the central node. In the context of on-chip training with NVM, this method
helps reduce the number of weight updates. However, the gradient accumulator requires as much
memory as the weights themselves, which negates the density benefits of NVM.
Low-Rank Training. Our work draws heavily from previous low-rank training schemes that have
largely been developed for use in recurrent neural networks to uncouple the training memory require-
ments from the number of time steps inherent to the standard truncated backpropagation through time
(TBPTT) training algorithm. Algorithms developed since then to address the memory problem in-
clude Real-Time Recurrent Learning (RTRL) (Williams & Zipser, 1989), Unbiased Online Recurrent
Optimization (UORO) (Tallec & Ollivier, 2017), Kronecker Factored RTRL (KF-RTRL) (Mujika
et al., 2018), and Optimal Kronecker Sums (OK) (Benzing et al., 2019). These latter few techniques
rely on the weight gradients in a weight-vector product looking like a sum of outer products (i.e.,
Kronecker sums) of input vectors with backpropagated errors. Instead of storing a growing number
of these sums, they can be approximated with a low-rank representation involving fewer sums.
3	Training Non-Volatile Memory
The meat of most deep learning systems are many weight matrix - activation vector products W ∙ a.
Fully-connected (dense) layers use them explicitly: a['] = σ (W[']a[`-1] + b[`]) for layer ', where
σ is a non-linear activation function (more details are discussed in detail in Appendix C.1). Recurrent
neural networks use one or many matrix-vector products per recurrent cell. Convolutional layers can
also be interpreted in terms of matrix-vector products by unrolling the input feature map into strided
convolution-kernel-size slices. Then, each matrix-vector product takes one such input slice and maps
it to all channels of the corresponding output pixel (more details are discussed in Appendix C.2).
The ubiquity of matrix-vector products allows us to adapt the techniques discussed in “Low-Rank
Training” of Section 2 to other network architectures. Instead of reducing the memory across time
steps, we can reduce the memory across training samples in the case of a traditional feedforward
neural network. However, in traditional training (e.g., on a GPU), this technique does not confer
advantages. Traditional training platforms often have ample memory to store a batch of activations
and backpropagated gradients, and the weight updates ∆W can be applied directly to the weights
W once they are computed, allowing temporary activation memory to be deleted. The benefits of
low-rank training only become apparent when looking at the challenges of proposed NVM devices:
Low write density (LWD). In NVM, writing to weights at every sample is costly in energy, time,
and endurance. These concerns are exacerbated in multilevel cells, which require several steps of an
iterative write-verify cycle to program the desired level. We therefore want to minimize the number
of writes to NVM.
Low auxiliary memory (LAM). NVM is the densest form of memory. In 40nm technology, RRAM
1T-1R bitcells @ 0.085 um2 (Chou et al., 2018) are 2.8x smaller than 6T SRAM cells @ 0.242 um2
(TSMC, 2019). Therefore, NVM should be used to store the memory-intensive weights. By the same
2
Under review as a conference paper at ICLR 2020
token, no other on-chip memory should come close to the size of the on-chip NVM. In particular, if
our b-bit NVM stores a weight matrix of size no × ni, we should use at most r(ni + no)b auxiliary
non-NVM memory, where r is a small constant. Despite these space limitations, the reason we might
opt to use auxiliary (large, high endurance, low energy) memory is because there are places where
writes are frequent, violating LWD if we were to use NVM.
In the traditional minibatch SGD setting with batch size B, an upper limit on the write density per
cell per sample is easily seen: 1/B. However, to store such a batch of updates without intermediate
writes to NVM would require auxiliary memory proportional to B . Therefore, a trade-off becomes
apparent. If B is reduced, LAM is satisfied at the cost of LWD. If B is raised, LWD is satisfied at the
cost of LAM. Using low-rank training techniques, the auxiliary memory requirements are decoupled
from the batch size, allowing us to increase B while satisfying both LWD and LAM1. Additionally,
because the low-rank representation uses so little memory, a larger bitwidth can be used, potentially
allowing for gradient accumulation in a way that is not possible with low bitwidth NVM weights. In
the next section, we elaborate on the low-rank training method.
4	Low-Rank Training Method
Let z(i) = W a(i) + b be the standard affine transformation building block of some larger network,
e.g., yp(i) = fpost(z(i)) and a(i) = fpre (x(i)) with prediction loss L(yp(i),yt(i)), where (x(i),yt(i))
is the ith training sample pair. Then weight gradient VwL(i) = dz(i) (a(i))> = dz(i) 0 a(i) where
dz(i) = Vz(i) L(i). A minibatch SGD weight update accumulates this gradient over B samples:
∆W = -η PiB=1 dz(i) 0 a(i) for learning rate η.
For a rank-r training scheme, approximate the sum PiB=1 dz(i)0a(i) by iteratively updating two rank-
r matrices L ∈ Rno×r, R ∈ Rni×r with each new outer product: LR1 J rankReduce(LR1 +
dz(i) 0 a(i)). Therefore, at each sample, We convert the rank-q = r + 1 system LR> + dz(i) 0 a(i)
into the rank-r LR>. In the next sections, we discuss how to compute rankReduce.
4.1	Optimal Kronecker Sum Approximation (OK)
One option for rankReduce(X) to convert from rank q = r + 1 X to rank r is a minimum error
estimator, which is implemented by selecting the top r components of a singular value decompo-
sition (SVD) of X. However, a naive implementation is computationally infeasible and biased:
E[rankReduce(X)] 6= X. Benzing et al. (2019) solves these problems by proposing a minimum
variance unbiased estimator for rankReduce, which they call the OK algorithm2.
The OK algorithm can be understood in two key steps: first, an efficient method of computing the
SVD of a Kronecker sum; second, a method of splitting the singular value matrix Σ into two rank-r
matrices whose outer product is a minimum-variance, unbiased estimate of Σ. Details can be found
in their paper, however we include a high-level explanation in Sections 4.1.1 and 4.1.2 to aid our
discussions. Note that our variable notation differs from Benzing et al. (2019).
4.1.1	Efficient SVD of Kronecker Sums
Let L = [L, dz(i)] and R = [R, a(i)] sothat LR> = LR> + dz(i) 0a(i). Recall that rankReduce
should turn rank-q LR> into an updated rank-r LR>.
QR-factorize L = QLRL and R = QRRR where QL ∈ Rno×q, QR ∈ Rni×q are orthogonal
so that LR> = QL(RLRR>)QR>. Let C = RLRR> ∈ Rq×q. Then we can find the SVD of
C = UC ΣVC> in O(q3) time (Cline & Dhillon, 2006), making it computationally feasible on small
devices. Now we have:
1This can alternately be achieved by sub-sampling the training data by r/B where r is the OK rank. The
purpose of using a low-rank estimate is that for the same memory cost, it is significantly more informational
than the sub-sampled data, allowing for faster training convergence.
2Their target application differs slightly in that they handle matrix - vector Kronecker sums rather than vector
- vector Kronecker sums.
3
Under review as a conference paper at ICLR 2020
LR> = QL(UCΣVC>)QR> = (QLUC)Σ(QRVC)>	(1)
which gives the SVD of LR> since QLUC and QRVC are orthogonal and Σ is diagonal. This SVD
computation has a time complexity of O((ni+no+q)q2) and a space complexity of O((ni+no+q)q).
4.1.2	MINIMUM VARIANCE, UNBIASED ESTIMATE OF Σ
In Benzing et al. (2019), it is shown that the problem of finding a rank-r minimum variance unbiased
estimator of LR> can be reduced to the problem of finding a rank-r minimum variance unbiased
estimator of Σ and plugging it in to (1).
Further, it is shown that such an optimal approximator for Σ = diag(σ1 , σ2, . . . , σq), where σ1 ≥
σ2 ≥ ∙∙∙ ≥ σq will involve keeping the m 一 1 largest singular values and mixing the smaller singular
values σm, . . . , σq within their (k + 1) × (k + 1) submatrix with m, k defined below. Let:
k = q - m
q
m = min i s.t. (q 一 i)σi ≤	σj
j=i
q
s1 =	σi
i=m
Note that ||x0||2 = 1. Let X ∈ R(k+1)×(k) be orthogonal such that its left nullspace is the span of
x0. Then XX> = I 一 x0x0>. Now, let s ∈ {一1, 1}(k+1)×1 be uniform random signs and define:
Xs = (sX:,1,...,sX:,k)
Z = Vs1 ∙ Xs
∑L = ∑R = diag (√σ1,..., √σm-1, Z)	(2)
where is an element-wise product. Then ΣLΣR> = Σ is a minimum variance, unbiased3 rank-r
approximation of Σ. Plugging Σ into (1),
LR> = (QLUC)∑(QrVc)> ≈ (QLUC)∑(QrVc)> = (QLUC∑LKQRVCΣR)τ	(3)
_ ~ ____________二 __________ —	_ ~ __________二 _________ —
Thus, L = QLUCΣL ∈ Rno×r and R = QRVCΣR ∈ Rni×r gives us a minimum variance,
unbiased, rank-r approximation LRτ .
4.2	Streaming Kronecker Sum Approximation (SKS)
Although the standalone OK algorithm presented by Benzing et al. (2019) has good asymptotic
computational complexity, our vector-vector outer product sum use case permits further optimizations.
In this section we present these optimizations, and we refer readers to the explicit implementation
called Streaming Kronecker Sum Approximation (SKS) in Algorithm 1 of Appendix A.
4.2.1	MAINTAIN ORTHOGONAL QL, QR
The main optimization is a method of avoiding recomputing the QR factorization of L and R at
every step. Instead, we keep track of orthogonal matrices QL , QR, and weightings cx such that
L = QL ∙ diag(√CX)[2 and R = QR ∙ diag(√CX)[v]. Upon receiving a new sample, a single inner
loop of the numerically-stable modified Gram-Schmidt (MGS) algorithm (Bjθrck, 1967) can be
used to update QL and QR. The orthogonal basis coefficients cL = QLτdz(i) and cR = QRτa(i)
computed during MGS can be used to find the new value of C = cLcRτ + diag(cx).
3The fact that it is unbiased: E[Σ] = Σ can be easily verified.
4
Under review as a conference paper at ICLR 2020
After computing ΣL = ΣR in (2), we can orthogonalize these matrices into ΣL = ΣR = QxRx .
Then from (3), we have LR> = (QLUCQx)(RxRx)(QRVCQχ)>. With this formulation, we can
maintain orthogonality in QL , QR by setting:
Ql 一 QLUCQx	Qr 一 QrVcQx	Cx 一 diag(RxR>)
These matrix multiplies require O((ni +no)q2) multiplications, so this optimization does not improve
asymptotic complexity bounds. This optimization may nonetheless be practically significant since
matrix multiplies are easy to parallelize and would typically not be the bottleneck of the computation
compared to Gram-Schmidt. The next section discusses how to orthogonalize ΣL efficiently and why
(Rx Rx> ) is diagonal.
4.2.2	ORTHOGONALIZATION OF ΣL
Orthogonalization of ΣL is relatively straightforward. From (2), the columns of ΣL are orthogonal
since Z is orthogonal. However, they do not have unit norm. We can therefore pull out the norm into
a separate diagonal matrix Rx with diagonal elements √cX:
Cx= = (√σi,..., √σm-1,
q-m+1 times
4.2.3	FINDING ORTHONORMAL BASIS X
We generated X by finding an orthonormal basis that was orthogonal to a vector x0 so that we
could have XX> = I - x0x0>. An efficient method of producing this basis is through Householder
matrices (x0, X) = I - 2 vv>/||v||2 where v = x0 - e(1) and (x0, X) is a k + 1 × k + 1 matrix
with first column x0 and remaining columns X (Householder, 1958; user1551, 2013).
4.2.4	Efficiency Comparisons to Standard Approach
The OK/SKS methods require O((ni + no + q)q2) operations per sample and O(ninoq) operations
after collecting B samples, giving an amortized cost of O((ni + no + q)q2 + ninoq/B) operations
per sample. Meanwhile, a standard approach expands the Kronecker sum at each sample, costing
O(nino) operations per sample. If q B, ni, no then the low rank method is superior to minibatch
SGD in both memory and computational cost.
5	Convex Convergence
SKS introduces variance into the gradient estimates, so here we analyze the implications for online
convex convergence. We analyze the case of strongly convex loss landscapes ft(wt) for flattened
weight vector wt and online sample t. In Appendix B, we show that with inverse squareroot learning
rate, when the loss landscape Hessians satisfy 0 Y CI W V2 f t(wt) and under constraint (4) for
the size of gradient errors εt, where w* is the optimal offline weight vector, the online regret
(5) is sublinear in the number of online steps T. We can approximate ∣∣ε∣∣ and show that convex
convergence is likely when (6) is satisfied in the biased, zero-variance case (equivalent to raw SVD,
i.e., not applying Section 4.1.2), or when (7) is satisfied in the unbiased, minimum-variance case.
c	TT
I∣εt∣∣≤ 2I∣wt - w*||	(4)	R(T) = Xft(wt)- Xft(w*)	⑸
t=1	t=1
c2	B	c2
≤ 4 l∣wt - w*l∣2	(6)	∑σ(t,i)σqt,i) ≤ W ||wt - w*||2	⑺
i=1	4	i=1	8
Equations (6, 7) suggest conditions under which fast convergence may be more or less likely and also
point to methods for improving convergence. We discuss these in more detail in Appendix B.3.
5
Under review as a conference paper at ICLR 2020
5.1	Convergence Experiments
We validate (4) with several linear regression experiments on a static input batch X ∈ R1024×100
and target Yt ∈ R256×100. In Figure 1(a), Gaussian noise at different strengths (represented by
different colors) is added to the true batch gradients at each update step. Notice that convergence
slows significantly to the right of the dashed lines, which is the region where (4) no longer holds4.
In Figure 1(b), we validate Equations (4, 6, 7) by testing the SVD and SKS cases with rank r = 10.
In these particular experiments, SKS adds too much variance, causing it to operate to the right of the
dashed lines. However, both SVD and SKS can be seen to reduce their variance as training progresses.
In the case of SVD, it is able to continue training as it tracks the right dashed line.
(a) True gradients with artificial noise
Figure 1: In both plots, the solid line with markers plots the loss vs. gradient error variance (LHS of
(4)) across 50 steps of SGD for several different setups. The left dashed line represents the RHS of
(4) and the right dashed line is the RHS with C instead of c.
(b) SVD/SKS gradients over learning rates
6	Implementation Details
Quantization. The NN is quantized in both the forward and backward directions with uniform
power-of-2 quantization, where the clipping ranges are fixed at the start of training5. Weights are
quantized to 8 bits between -1 and 1, biases to 16 bits between -8 and 8, activations to 8 bits between
0 and 2, and gradients to 8 bits between -1 and 1. Both the weights W and weight updates ∆W
are quantized to the same LSB so that weights cannot be used for accumulation beyond the fixed
quantization dynamic range. This is in contrast to using high bitwidth (Zhou et al., 2016; Banner
et al., 2018) or floating point accumulators. See Appendix D for more details on quantization.
Gradient Max-Norming. State-of-the-art methods in training, such as Adam (Kingma & Ba, 2014),
use auxiliary memory per parameter to normalize the gradients. Unfortunately, we lack the memory
budget to support these additional variables, especially if they must be updated every sample6.
Instead, we propose dividing each gradient tensor by the maximum absolute value of its elements.
This stabilizes the range of gradients across samples. See Appendix E for more details on gradient
max-norming. In the experiments, we refer to this method as “max-norm” (opposite “no-norm”).
Streaming Batch Normalization. Batch normalization (Ioffe & Szegedy, 2015) is a powerful
technique for improving training performance which has been suggested to work by smoothing
the loss landscape (Santurkar et al., 2018). We hypothesize that this may be especially helpful
when parameters are quantized as in our case. However, in the online setting, we receive samples
one-at-a-time rather than in batches. We therefore propose a streaming batch norm that uses moving
average statistics rather than batch statistics as described in detail in Appendix F.
4As discussed in Appendix B.1, B < ni, so we substitute C in c∕2∖∖wt — w*|| with the minimum non-zero
Eigenvalue of the Hessian C when plotting the RHS of (4).
5 Future work might look into how to change these clipping ranges, but this is beyond the scope of this paper.
6SKS could potentially approximate Adam. SKS on a2, dz2, a, dz allows for a low-rank approximation of
the variance of the gradients, however, this is unlikely to work well because of numerical stability (e.g., estimated
variances might be negative).
6
Under review as a conference paper at ICLR 2020
7	Experiments
7.1	Adaptation Experiments
To test the effectiveness of SKS, experiments are performed on a representative CNN with four
3 × 3 convolution layers and two fully-connected layers. We generate “offline” and “online” datasets
based on MNIST (see Appendix G), including one in which the statistical distribution shifts every
10k images. We then optimize an online SGD and rank-4 SKS model for fair comparison (see
Appendix H). To see the importance of different training techniques, we run several ablations in
Appendix I. Finally, we compare these different training schemes in different environments, meant to
model real life. In these hypothetical scenarios, a model is first trained on the offline training set, and
is then deployed to a number of devices at the edge that make supervised predictions (they make a
prediction, then are told what the correct prediction would have been).
We present results on four hypothetical scenarios. First, a control case where both exter-
nal/environment and internal/NVM drift statistics are exactly the same as during offline training.
Second, a case where the input image statistical distribution shifts every 10k samples, selecting from
augmentations such as spatial transforms and background gradients (see Section G). Third and fourth
are cases where the NVM drifts from the programmed values, roughly modeling NVM memory
degradation. In the third case, Gaussian noise is applied to the weights as if each weight was a
single multi-level memory cell whose analog value drifted in a Brownian way. In the fourth case,
random bit flips are applied as if each weight was represented by b memory cells (see Appendix G
for details). For each hypothetical scenario, we plot five different training schemes: pure quantized
inference (no training), bias-only training, standard SGD training, SKS training, and SKS training
with max-normed gradients. In SGD training and for training biases, parameters are updated at every
step in an online fashion. These are seen as different colored curves in Figure 2.
1.00
0.98
›
0 0.96
轲94
1.0
、0.8
U 0 6
---Inference
---Bias-Only
---SGD/No-Norm
——SKS/No-Norm
——SKS/Max-Norm
SW_」M # xe≡
100 .....101 ~1~...102~1~....103~1~....104~1~...105
Sample
(a) Original Dataset
20000	40000	60 00	80000	100000
Sample
100	101	102	103	104	105
Sample
(c) Analog / Gaussian Weight Drift
(b) Distribution Shifts
100	101	102	103	104	105
Sample
(d) Digital / Bit-Flip Weight Drift
Figure 2: Adaptation of various training schemes over four different training environments (a) to
(d). In each training environment, the top plot shows the exponential moving averages (0.999) of the
per-sample online accuracy of the five training schemes, while the bottom plot shows the maximum
number of updates applied to any given convolution or fully-connected kernel memory cell. For the
distribution shifts in (b), the enabled augmentations at each contiguous 10k samples is shown (CD =
class distribution, ST = spatial transforms, BG = background gradients, WN = white noise).
Inference does best in the control case, but does poorly in adaptation experiments. SGD doesn’t
improve significantly on bias-only training, likely because SGD cannot accumulate gradients less
than a weight LSB. SKS, on the other hand, shows significant improvement, especially after several
7
Under review as a conference paper at ICLR 2020
thousand samples in the weight drift cases. Additionally, SKS shows about three orders of magnitude
improvement compared to SGD in the worst case number of weight updates. Much of this reduction
is due to the convolutions, where updates are applied at each pixel. However, reduction in fully-
connected writes is still important because of potential energy savings. SKS/max-norm performs best
in terms of accuracy across all environments and has similar weight update cost to SKS/no-norm.
7.2	Transfer Learning and Algorithm Comparisons
To test the broader applicability of low rank training techniques, we run several experiments on
ImageNet with ResNet-34 (Deng et al., 2009; He et al., 2016), a potentially realistic target for dense
NVM inference on-chip. For ImageNet-size images, updating the low-rank approximation at each
pixel quickly becomes infeasible, both because of the single-threaded nature of the algorithm, and
because of the increased variance of the estimate at larger batch sizes. Instead, we focus on training the
final layer weights (1000 × 512). ResNet-34 weights are initialized to those from Paszke et al. (2017)
and the convolution layers are used to generate feature vectors for 10k ImageNet training images7,
which are quantized and fed to a one-layer quantized8 neural network. To speed up experiments, the
layer weights are initialized to the pretrain weights, modulated by random noise that causes inference
top-1 accuracy to fall to 52.7% ± 0.9%. In Table 1, we see that the unbiased SKS has the strongest
recovery accuracies, although biased SVD also does quite well. The high-variance UORO and true
SGD have weak or non-existent recoveries.
Table 1: Accuracy recovery beyond inference (%, mean with standard deviation from 5 random seeds)
between different algorithms (all with max-norm; effective batch size B = 100 if applicable), tested
at different ranks (r), and learning rates (η). Optimal learning rates are bolded.
Algorithm	η r	0.003	0.010	0.030	0.100	0.300
SGD	-	+0.3 ± 0.2	+0.3 ± 0.2	+0.3 ± 0.2	+0.9 ± 0.2	-3.9 ± 0.8
UORO	1	+0.4 ± 0.2	+0.3 ± 0.4	-1.8 ± 0.9	-7.6 ± 1.6	-31.7 ± 1.6
SVD	1	+1.9 ± 0.2	+5.8 ± 1.0	-3.4 ± 1.0	-19.4 ± 0.9	-40.7 ± 1.1
	2	+1.4 ± 0.4	+6.5 ± 0.7	+6.3 ± 0.6	-5.2 ± 0.9	-36.3 ± 0.9
	4	+1.3 ± 0.4	+6.5 ± 0.7	+5.2 ± 0.8	-3.3 ± 1.0	-33.8 ± 0.8
	8	+1.4 ± 0.3	+5.6 ± 0.8	+4.3 ± 0.9	-2.4 ± 1.0	-32.8 ± 0.9
SKS	1	+0.3 ± 0.2	+0.3 ± 0.2	-0.7 ± 0.4	-2.7 ± 1.7	-26.5 ± 2.6
	2	+0.3 ± 0.2	+0.4 ± 0.3	-0.1 ± 0.4	+1.3 ± 0.9	-12.9 ± 1.1
	4	+0.4 ± 0.2	+0.6 ± 0.2	+1.9 ± 0.3	+8.0 ± 1.1	-5.1 ± 1.1
	8	+0.4 ± 0.2	+1.1 ± 0.2	+3.3 ± 0.7	+4.8 ± 1.5	-15.8 ± 1.7
8	Conclusion
We demonstrated the potential for SKS to solve the major challenges facing online training on
NVM-based edge devices: low write density and low auxiliary memory. SKS is a computationally-
efficient, memory-light algorithm capable of decoupling batch size from auxiliary memory, allowing
larger effective batch sizes, and consequently lower write densities. Additionally, we noted that
SKS may allow for training under severe weight quantization constraints as rudimentary gradient
accumulations are handled by the L, R matrices, which can have high bitwidths (as opposed to
SGD, which may squash small gradients to 0). We found expressions for when SKS might have
better convergence properties. Across a variety of online adaptation problems and a large-scale
transfer learning demonstration, SKS was shown to match or exceed the performance of SGD while
using a small fraction of the number of updates. Finally, we suspect that these techniques could
be applied to a broader range of problems. Auxiliary memory minimization may be analogous
to communication minimization in training strategies such as federated learning, where gradient
compression is important.
7The decision to use training data is deliberate, however experiments on out-of-sample images, such as Recht
et al. (2019) show similar behavior.
8Quantization ranges are chosen to optimize accuracy and are different from those in Section 7.1.
8
Under review as a conference paper at ICLR 2020
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. arXiv
preprint arXiv:1704.05021, 2017.
Stefano Ambrogio, Pritish Narayanan, Hsinyu Tsai, Robert M. Shelby, Irem Boybat, Carmelo
di Nolfo, Severin Sidler, Massimo Giordano, Martina Bodini, Nathan C. P. Farinha, Benjamin
Killeen, Christina Cheng, Yassine Jaoudi, and Geoffrey W. Burr. Equivalent-accuracy accelerated
neural-network training using analogue memory. Nature, 558(7708):60-67, June 2018. ISSN
1476-4687. doi: 10.1038/s41586-018-0180-5.
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of
neural networks. In Advances in Neural Information Processing Systems, pp. 5145-5153, 2018.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Frederik Benzing, Marcelo Matheus Gauy, Asier Mujika, Anders Martinsson, and Angelika Steger.
Optimal Kronecker-sum approximation of real time recurrent learning. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 604-613, Long Beach,
California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/
benzing19a.html.
Ake Bjorck. Solving linear least squares problems by gram-schmidt orthogonalization. BrTNUmerical
Mathematics, 7(1):1-21, 1967.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
C. Chou, Z. Lin, P. Tseng, C. Li, C. Chang, W. Chen, Y. Chih, and T. J. Chang. An N40 256K×44
embedded RRAM macro with SL-precharge SA and low-voltage current limiter to improve read
and write performance. In 2018 rEEE rnternational Solid - State Circuits Conference - (rSSCC),
pp. 478-480, February 2018. doi: 10.1109/ISSCC.2018.8310392.
Alan Kaylor Cline and Inderjit S Dhillon. Computation of the singular value decomposition, 2006.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
M. Ernestus. Elastic transformation of an image in python. https://gist.github.com/
erniejunior/601cdf56d2b424757de5, 2016.
Tayfun Gokmen and Yurii Vlasov. Acceleration of Deep Neural Network Training with Resistive
Cross-Point Devices. Frontiers in Neuroscience, 10, July 2016. ISSN 1662-453X. doi: 10.3389/
fnins.2016.00333.
S.	K. Gonugondla, M. Kang, and N. R. Shanbhag. A Variation-Tolerant In-Memory Machine
Learning Classifier via On-Chip Training. rEEE Journal of Solid-State Circuits, 53(11):3163-3173,
November 2018. doi: 10.1109/JSSC.2018.2867275.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
A. Grossi, E. Vianello, M. M. Sabry, M. Barlas, L. Grenouillet, J. Coignus, E. Beigne, T. Wu, B. Q.
Le, M. K. Wootters, C. Zambelli, E. Nowak, and S. Mitra. Resistive ram endurance: Array-level
characterization and correction techniques targeting deep learning applications. rEEE Transactions
on Electron Devices, 66(3):1281-1288, March 2019. doi: 10.1109/TED.2019.2894387.
S Haykin. Adaptive filter theory, ser. always learning, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the rEEE international
conference on computer vision, pp. 1026-1034, 2015.
9
Under review as a conference paper at ICLR 2020
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Alston S Householder. Unitary triangularization of a nonsymmetric matrix. Journal of the ACM
(JACM), 5(4):339-342, 1958.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jakub Konecny, H. Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimization:
Distributed machine learning for on-device intelligence. CoRR, abs/1610.02527, 2016. URL
http://arxiv.org/abs/1610.02527.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression:
Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887,
2017.
Fuyun Ling, Dimitris Manolakis, and John Proakis. A recursive modified gram-schmidt algorithm
for least-squares estimation. IEEE transactions on acoustics, speech, and signal processing, 34(4):
829-836, 1986.
Asier Mujika, Florian Meier, and Angelika Steger. Approximating real-time recurrent learning with
random kronecker factors. In Advances in Neural Information Processing Systems, pp. 6594-6603,
2018.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Hang Qi, Matthew Brown, and David G. Lowe. Low-Shot Learning with Imprinted Weights. In 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5822-5830, Salt Lake
City, UT, June 2018. IEEE. ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.00610.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019.
Jimmy SJ Ren and Li Xu. On vectorization of deep convolutional neural networks for vision tasks.
In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? In Advances in Neural Information Processing Systems, pp. 2483-2493,
2018.
Patrice Y. Simard, Dave Steinkraus, and John Platt. Best practices for convolutional
neural networks applied to visual document analysis. Institute of Electrical and Elec-
tronics Engineers, Inc., August 2003. URL https://www.microsoft.com/en-
us/research/publication/best-practices-for-convolutional-neural-
networks-applied-to-visual-document- analysis/.
D. Soudry, D. Di Castro, A. Gal, A. Kolodny, and S. Kvatinsky. Memristor-Based Multilayer Neural
Networks With Online Gradient Descent Training. IEEE Transactions on Neural Networks and
Learning Systems, 26(10):2408-2421, October 2015. doi: 10.1109/TNNLS.2014.2383395.
Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization. arXiv preprint
arXiv:1702.05043, 2017.
10
Under review as a conference paper at ICLR 2020
TSMC. 40nm Technology - Taiwan Semiconductor Manufacturing Company Limited.
https://www.tsmc.com/english/dedicatedFoundry/technology/40nm.htm, 2019.
user1551. Rotation matrix in arbitrary dimension to align vector. Mathematics
Stack Exchange, 2013. URL https://math.stackexchange.com/q/525587.
URL:https://math.stackexchange.com/q/525587 (version: 2013-10-14).
Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. Neural computation, 1(2):270-280, 1989.
T.	F. Wu, B. Q. Le, R. Radway, A. Bartolo, W. Hwang, S. Jeong, H. Li, P. Tandon, E. Vianello, P. Vivet,
E. Nowak, M. K. Wootters, H. . P. Wong, M. M. S. Aly, E. Beigne, and S. Mitra. 14.3 a 43pj/cycle
non-volatile microcontroller with 4.7s shutdown/wake-up integrating 2.3-bit/cell resistive ram and
resilience techniques. In 2019 IEEE International Solid- State Circuits Conference - (ISSCC), pp.
226-228, Feb 2019. doi: 10.1109/ISSCC.2019.8662402.
S. Yu, Z. Li, P. Chen, H. Wu, B. Gao, D. Wang, W. Wu, and H. Qian. Binary neural network with 16
Mb RRAM macro chip for classification and online training. In 2016 IEEE International Electron
Devices Meeting (IEDM), pp. 16.2.1-16.2.4, December 2016. doi: 10.1109/IEDM.2016.7838429.
Shimeng Yu. Neuro-inspired computing with emerging nonvolatile memorys. Proceedings of the
IEEE, 106(2):260-285, 2018.
E. Zamanidoost, F. M. Bayat, D. Strukov, and I. Kataeva. Manhattan rule training for memristive
crossbar circuit pattern classifiers. In 2015 IEEE 9th International Symposium on Intelligent Signal
Processing (WISP) Proceedings, pp. 1-6, May 2015. doi: 10.1109/WISP.2015.7139171.
J. Zhang, Z. Wang, and N. Verma. In-Memory Computation of a Machine-Learning Classifier in
a Standard 6T SRAM Array. IEEE Journal of Solid-State Circuits, 52(4):915-924, April 2017.
ISSN 0018-9200. doi: 10.1109/JSSC.2016.2642198.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 928-936,
2003.
11
Under review as a conference paper at ICLR 2020
A SKS Algorithm
Algorithm 1 Streaming Kronecker Sum Approximation
State: QL ∈ Rno×q; QR ∈ Rni×q; cx ∈ Rq×1
Input: dz(i) ∈ Rno×1; a(i) ∈ Rni×1 fori ∈ [1,B]
for i = 1 . . . B do
{Modified Gram-Schmidt.}
Cl, CR — 0q×1
for j = 1 . . . r do
CL,j J QLj ∙ dZ(i;	dZ(i J dZ(i - CL,j ∙ QL,j
CRj J QR,j ∙ a⑴；	a⑴ J a⑷-CL,j ∙ Qlj
end for
CL,q J ||dz(i)||;	QL,q J dz(i)/CL,q
CR,q J ||a(i)||;	QR,q J a(i)/CR,q
{Generate C and find its SVD.}
C J CLCR> + diag(Cx)
UC ∙ diag(σ) ∙ VC> J SVD(C)
{Minimum-variance unbiased estimator for Σ.}
m J min j s.t. (q 一 j)σj ≤ Pq=j σ
q
s1 J	σi,	k J q - m
i=m
V J p1 - k/s1 ∙ σ[m:] - e⑴
s J {-1, 1}(k+1)×1 {Ind. uniform random signs.}
Xs J(I + (s Θ v)(v∕vι)>)[2.] {Householder.}
{QR-factorization of Σ l.}
Qx J I0 X0	∈ Rq×r
Cx J (σ1, . . . , σm-1, s1∕k, . . . , s1∕k)
'-------------------------{-----}
q-m+1 times
{Update the first r columns of QL , QR.}
QLH J Ql ∙ UC Qx
QRH J Qr ∙ Vc ∙ Qx
end for
r Z-X	. i' 1 TΓ ŋ 1 X—7 C TΓ τ∖-Γ 、
{Compute final L, R where VwL ≈ LR 1 .}
L J (Ql ∙diag(√Cx))"]
R J (QR ∙diag(√Cx)) [:r]
B	Convex Convergence
In this section we will attempt to bound the regret (defined below) of an SGD algorithm using noisy
SKS estimates g = g + ε in the convex setting, where g are the true gradients and ε are the errors
introduced by the low rank SKS approximation. Here, g is a vector of size N and can be thought of
as a flattened/ConCatenated version of the gradient tensors (e.g., N = % ∙ n。).
Our proof follows the proof in Zinkevich (2003). We define F as the convex feasible set (valid
settings for our weight tensors) and assume that F is bounded with D = maxw,v∈F||w 一 v|| being
the maximum distance between two elements of F. Further, assume a batch t of B samples out of T
total batches corresponds to a loss landscape ft(wt) that is strongly convex in weight parameters
wt, so there are positive constants C ≥ C > 0 such that CI V2f t(wt) CI for all t (Boyd
& Vandenberghe, 2004, Section 9.3). We define regret as R(T) = PT=Ift(Wt) 一 PT=Ift(w*)
where w* = argminw PT=I f t(w) (i.e., it is an optimal offline minimizer of f1,…，fT).
12
Under review as a conference paper at ICLR 2020
The gradients seen during SGD are gt = V/t(wt) and we assume they are bounded by G =
maxw∈F,t∈[1,τ ]||V/t(w)∣∣. We also assume errors are bounded by E = maxt∈[i,τ] ∣∣εt∣∣. Therefore,
maxt∈[i,τ]∣∣gt∣∣ ≤ maxt∈[i,τ]∣∣gt∣∣ + ∣∣εt∣∣ ≤ G + E by the triangle inequality.
Theorem 1. Assume SKS-based SGD is applied with learning rate ηt = 1∕√t. Then, under the
additional constraintgtl ∙ (wtι — w*) — c∣∣wt — w*∣∣∣ ≤ gt ∙ (wt — w*), we have sublinear regret:
R(T) ≤ D2∣√T +(G + E)∣ (√T — 1/2)
Proof. From strong convexity cI W V∣/t(wt) W CI for all t,
f t(w) + gt ∙ (v — w) + C∣∣v — w∣∣∣ ≤ ft(v) for all v	(8)
In particular, if we consider V = w* and rearrange,
f t(w) — f t(w*) ≤ gt ∙ (w — w*) — 2 ∣∣w — w*∣∣∣
ft(w) — ft(w*) ≤ gt ∙ (wt — w*)	(9)
Consider a gradient update wt+1 = PF(Wt — ηtgt), where Pf projects the update back to F. Then,
∣∣wt+1 — w*∣∣∣ = ∣∣P(wt — ηtgt) — w*∣∣∣ ≤ ∣∣wt — ηtgt — w*∣∣∣
=∣∣wt — w* ∣∣∣ — 2ηt(Wt — w*) ∙ gt + η∣∣∣gt∣∣∣
≤ ∣∣wt — w* ∣∣∣ — 2ηt(wt — w*) ∙ gt + η∣(G + E)∣
gt ∙ (wt — w*) ≤ ɪ (∣∣wt — w*Hl —∣∣wt+1 — w*∣∣∣) + η(G + E)∣	(10)
2ηt	2
From (9, 10),
ft(wt) — ft(w*) ≤ *(∣∣wt — w*∣H — ∣∣wt+1 — w*∣∣∣) + η2t(G + E)∣	(11)
We now bound the regret:
T
R(T ) = Xft(wt) — f t(w*)]
t=1
ɪ「1	一	(n.	一
≤ X 2-	(∣∣wt	— w*∣H	— ∣∣wt+1	— w*∣∣∣)	+ η2t(G +	E)∣
t-1 L2ηt	2
∣∣w1 — w*∣∣∣	∣∣wτ+1 — w*∣∣∣
-----------------------------
2ηι	2ητ
∣∣wt — w*∣∣∣ +
∣∣wt — w*∣∣∣ +
(G + E)∣ X
—2— TrIt
t=1
≤⅛*i+ι X( η
D∣ +
(G + E )∣
2
T
X ηt
t=1
D∣	1 G C
'2ηι + 2 t= (ηt
(G + E)∣ X
丁鼻ηt
D∣	(G + E)∣ X
布+ ^^^ηt
(12)
13
Under review as a conference paper at ICLR 2020
If ηt = 1 /√t, then PT=I ηt ≤ 2√T - 1 (Zinkevich, 2003), so from (12),
R(T) ≤ D22√T +(G + E)2 (√T - 1/2)	(13)
□
This is a sublinear regret and therefore, average regret R(T)/T is bounded above by 0 in the limit as
T → ∞. To achieve this result, We constrainedgt ∙ (wt - w*) - C||wt - w*∣∣2 ≤ gt ∙ (wt - w*).
We now examine sufficient conditions for this inequality to be satisfied.
gt ∙ (Wt - w*) - C||wt - w*∣∣2 ≤ gt ∙ (Wt - w*)
εt ∙ (w* - wt) ≤ C∣∣wt- w*∣∣2	(14)
Since εt ∙ (w* - wt) ≤ ∣∣εt∣∣∙ ||w* - wt∣∣ by Cauchy-Schwarz, it is sufficient for:
∣∣εt∣∣∙∣∣w*-wtll≤ CIWt-w」2
I∣εt∣∣≤ CIIwt- W*||	(15)
B.1	Considerations for Rank Deficient Hessians
In the preceding proof, we assumed C > 0. However, it is common for this to not hold. For
example, in linear regression, where C = λmin(XX>) for sample input X ∈ R(ni×B) (Haykin,
2014, Chapter 4.3), if B < n then C = 0. We can modify (15) to handle this case. Let c be the
minimum non-zero Eigenvalue of XX> and let W ∈ RB represent W ∈ RN in the Eigenbasis of
XX>. Then (15) becomes:
C
l∣εt∣∣≤ G||Wt - W*||
(16)
B.2	Estimates for the SKS Error
We can estimate the SKS error IIεtII in both the biased and unbiased cases. For the biased, zero-
variance case, we get rid of the lowest singular value (out of q singular values) as we see each sample.
Thus, at a given sample i, the error is σ(i) ∙ QL' ∙ QR)q and the average squared error is (1/N)σ(i)2.
We can treat this as a per-element variance. If these smallest singular components are uncorrelated
from sample to sample, then the variances add:
σ2 ≈ N XM))2	(17)
i=1
For the unbiased, minimum-variance case, Theorem A.4 from Benzing et al. (2019) states that the
minimum variance is s12/k + s2 where s1 = Piq=m σi, s2 = Piq=m σi2, and k, m are as defined in
Section 4.1.2. Since m is chosen to minimize variance, we can upper bound the variance by choosing
m = r and therefore k = 1, s1 = σr + σq, and s2 = σr2 + σq2. Empirically, this tends to be a
good approximation. Then, the average per-element variance added at sample i is approximately
(2/N) σr(i) σq(i) . Assuming errors between samples are uncorrelated, this leads to a total variance:
i=1
(18)
14
Under review as a conference paper at ICLR 2020
Foreithercase, ∣∣ε∣∣2 ≈ Nσ∣. For the t-th batch and i-th sample, we denote σ(t,i) as the q-th singular
value. For simplicity, we focus on the biased, zero-variance case (the unbiased case is similar). From
(15), an approximately sufficient condition for sublinear-regret convergence is:
XX (σ(t,i))2 ≤ c42 ∣∣wt - w*||2	(19)
i=1	4
B.3	Discussion on Convergence
Equation (19) suggests that as Wt → w*, the constraints for achieving sublinear-regret convergence
become more difficult to maintain. However, in practice this may be highly problem-dependent as
the σq will also tend to decrease near optimal solutions. To get a better sense of the behavior of the
left-hand side of (19), suppose that:
B	2B	B
X 卜严)≈ X Si(Gt))2 ≤ X Si(Gt))2 = ∣∣Gt∣∣F
i=1	i=q	i=1
where Gt = Vwtft(Wt) ∈ RSOxni) are the matrix weight Wt gradients at batch t and || ∙ ||f is a
Frobenius norm. We therefore expect both the left (proportional to ||Gt||2F) and the right (proportional
to ∣∣wt 一 w*||2) of (19) to decrease during training as wt → w*. This behavior is in fact what is
seen in Figure 1(b). If achieving convergence is found to be difficult, (19) provides some insight for
convergence improvement methods.
One solution is to reduce batch size B to satisfy the inequality as necessary. This minimizes the
weight updates during more repetitive parts of training while allowing dense weight updates (possibly
approaching standard SGD with small batch sizes) during more challenging parts of training.
Another solution is to reduce σq . One way to do this is to increase the rank r so that the spectral
energy of the updates are spread across more singular components. There may be alternate approaches
based on conditioning the inputs to shape the distribution of singular values in a beneficial way.
A third method is to focus on c, the lower bound on curvature of the convex loss functions. Perhaps
a technique such as weight regularization can increase c by adding constant curvature in all Eigen-
directions of the loss function Hessian (although this may also increase the LHS of (19)). Alternatively,
perhaps low-curvature Eigen-directions are less important for loss minimization, allowing us to raise
the c that we effectively care about. This latter approach requires no particular action on our part,
except the recognition that fast convergence may only be guaranteed for high-curvature directions.
This is exemplified in Figure 1(b), where we can see SVD track the curve for C more so than c.
Finally, we note that this analysis focuses solely on the errors introduced by a floating-point version
of SKS. Quantization noise can add additional error into the εt term. We expect this to add a constant
offset to the LHS of (19). For a weight LSB ∆, quantization noise has variance ∆2∕12, so We desire:
N∆2 + XX Wm)2 ≤ c2I∣wt - W*||2	(20)
i=1
C Kronecker Sums in Neural Network Layers
C.1 Dense Layer
A dense or fully-connected layer transforms an input a ∈ Rni ×1 to an intermediate Z = W ∙ a + b
to an output y = σ(z) ∈ RnO×1 where σ is a non-linear activation function. Gradients of the loss
function with respect to the weight parameters can be found as:
VWL = (VzL) Θ (VWz) = dz 0 a
、{z} 、~{{z}
dz	a>
(21)
15
Under review as a conference paper at ICLR 2020
which is exactly the per-sample Kronecker sum update we saw in linear regression. Thus, at every
training sample, We can add (dz(i) 0 a(i)) to our low rank estimate with SKS.
C.2 Convolutional Layer
A convolutional layer transforms an input feature map A ∈ Rhin ×win ×cin to an intermediate feature
map Z = Wkern * A + b ∈ RhOut×wout×cout through a 2D convolution * with weight kernel
Wkern ∈ Rcout×kh×kw×cin. Then it computes an output feature map y = σ(z) where σ is a
non-linear activation function.
Convolutions can be interpreted as matrix multiplications through the im2col operation which
converts the input feature map A into a matrix Acol ∈ R(hout wout)×(kh kw cin) where the ith row is a
flattened version of the sub-tensor of a which is dotted with Wkern to produce the ith pixel of the
output feature map (Ren & Xu, 2015). We can multiply Acol by a flattened version of the kernel,
W ∈ Rcout×(khhwcin) to perform the Wkern * A convolution operation with a matrix multiplication.
Under the matrix multiplication interpretation, weight gradients can be represented as:
hout wout
VW L =(VZcOlL) Θ (VW Z )=	X dZ>oi,i 0 A^,i	(22)
、	{	' | {z }	i=ι
dZ>	Acol	i=1
col
which is the same as houtwout Kronecker sum updates. Thus, at every output pixel j of every training
sample i, we can add (dZc(oi)l>,j 0 A(cio)l>,j) to our low rank estimate with SKS.
Note that while we already save an impressive factor of B/q in memory when computing gradients
for the dense layer, we save a much larger factor of Bhoutwout/q in memory when computing
gradients for the convolution layers, making the low rank training technique even more crucial here.
However, some care must be taken when considering activation memory for convolutions. For
compute-constrained edge devices, image dimensions may be small and result in minimal intermediate
feature map memory requirements. However, if image dimensions grow substantially, activation
memory could dominate compared to weight storage. Clever dataflow strategies may provide a way
to reduce intermediate activation storage even when performing backpropagation9.
D	Hardware Quantization Model
In a real device, operations are expected to be performed in fixed point arithmetic. Therefore, all of
our training experiments are conducted with quantization in the loop. Our model for quantization
is shown in Figure 3. The green arrows describe the forward computation. Ignoring quantization
for a moment, we would have a' = ReLU (α'W' * a'-1 + b'), where * can represent either a
convolution or a matrix multiply depending on the layer type and α' is the closest power-of-2 to He
initialization (He et al., 2015). For quantization, we rely on four basic quantizers: Qw, Qb, Qa, Qg,
which describe weight quantization, bias and intermediate accumulator quantization, activation
quantization, and gradient quantization, respectively. All quantizers use fixed clipping ranges as
depicted and quantize uniformly within those ranges to the specified bitwidths.
In the backward pass, follow the orange arrows from δ'. Backpropagation follows standard back-
propagation rules including using the straight-through estimator (Bengio et al., 2013) for quantizer
gradients. However, because we want to perform training on edge devices, these gradients must
themselves be quantized. The first place this happens is after passing backward through the ReLU
derivitive. The other two places are before feeding back into the network parameters W', b', so
that W', b' cannot be used to accumulate values smaller than their LSB. Finally, instead of deriving
△ W' from a backward pass through the * operator, the SKS method is used.
9For example, one could compute just a sliding window of rows of every feature map, discarding earlier
rows as later rows are computed, resulting in a square-root reduction of activation memory. To incorporate
backpropagation, compute the forward pass once fully, then compute the forward pass again, as well as the
backward pass using the sliding window approach in both directions.
16
Under review as a conference paper at ICLR 2020
Figure 3: Signal flow graph for a forward and backward quantized convolutional or dense layer.
SKS collects a`-1, dz' for many samples before computing the approximate ∆W'. It accumulates
information in two low rank matrices L, R which are themselves quantized to 16 bits with clipping
ranges determined dynamically by the max absolute value of elements in each matrix. While SKS
accumulates for B samples, leading to a factor of B reduction in the rate of updates to W', b' is
updated at every sample. This is feasible in hardware because b` is small enough to be stored in more
expensive forms of memory that have superior endurance and write power performance.
Because of the coarse weight LSB size, weight gradients may be consistently quantized to 0, prevent-
ing them from accumulating. To combat this, we only apply an update if a minimum update density
ρmin = 0.01 would be achieved, otherwise we continue accumulating samples in L and R, which
have much higher bitwidths. When an update does finally happen, the “effective batch size” will be a
multiple of B and we increase the learning rate correspondingly. In the literature, a linear scaling
rule is suggested (see Goyal et al. (2017)), however we empirically find square-root scaling works
better (see Appendix H).
E Gradient Max-Norming
Figure 4: Maximum magnitude of weight gradients versus training step for standard SGD on a CNN
trained on MNIST.
Figure 4 plots the magnitude of gradients seen in a weight tensor over training steps. One apparent
property of these gradients is that they have a large dynamic range, making them difficult to quantize.
Even when looking at just the spikes, they assume a wide range of magnitudes. One potential
method of dealing with this dynamic range is to scale tensors so that their max absolute element is 1
(similar to a per-tensor AdaMax (Kingma & Ba, 2014) or Range Batch-Norm (Banner et al., 2018)
applied to gradients). Optimizers such as Adam, which normalize by gradient variance, provide
a justification for why this sort of scaling might work well, although they work at a per-element
17
Under review as a conference paper at ICLR 2020
rather than per-tensor level. We choose max-norming rather than variance-based norming because
the former is easier computational and potentially more ammenable to quantization. However, a
problem with the approach of normalizing tensors independently at each sample is that noise might be
magnified during regions of quiet as seen in the Figure. What we therefore propose is normalization
by the maximum of both the current max element and a moving average of the max element.
Explicitly, max-norm takes two parameters - a decay factor β = 0.999 and a gradient floor ε = 10-4
and keeps two state variables - the number of evaluations k := 0 and the current maximum moving
average xmv := ε. Then for a given input x, max-norm modifies its internal state and returns xnorm :
k
xmax
xmv
xmv
xnorm
k+1
max(|x|) + ε
xmv
+ (I - β) ∙
xmv
1 - βk
x
max(xmax,
xmv
xmax
β ∙
)
F Streaming Batch Normalization
Standard batch normalization (Ioffe & Szegedy, 2015) normalizes a tensor X along some axes, then
applies a trainable affine transformation. For each slice X of X that is normalized independently:
Y = Y ∙
+β
where μb, σb are mean and standard deviation statistics of a minibatch and γ,β are trainable affine
transformation parameters.
In our case, we do not have the memory to hold a batch of samples at a time and must compute μb, σb
in an online fashion. To see how this works, suppose We knew the statistics of each sample μi, σi
for i = 1 . . . B in a batch of B samples. For simplicity, assume the ith sample is a vector Xi,: ∈ Rn
containing elements Xi,j . Then:
1
μb = Bf μi	(23)
i=1
Bn	B	B
σb = B X n X Xij- μb = B X(σi+ μ)- μb = B X σi	(24)
i=1 j=1	i=1	i=1
In other words, the batch variance is not equal to the average of the sample variances. However, if we
keep track of the sum-of-square values of samples σi + μ2, then We can compute σb2 as in (24). We
keep track of two state variables: μs, Sqs which we update as μs := μs + μi and Sqs := Sqs + σi + μ2
for each sample i. After B samples, we divide both state variables by B and apply (23, 24) to get the
desired batch statistics. Unfortunately, in an online setting, all samples prior to the last one in a given
batch will only see statistics generated from a portion of the batch, resulting in noisier estimates of
μb, σb.
In streaming batch norm, we alter the above formula slightly. Notice that in online training, only the
most recently viewed sample is used for training, so there is no reason to weight different samples of
a given batch equally. Therefore we can use an exponential moving average instead of a true average
to track μs, sqs. Specifically, let:
μs ：= η ∙ μs + (1 - η) ∙ μi
Sqs := η ∙ Sqs +(I - η) ∙ (σ2 + μ2)
18
Under review as a conference paper at ICLR 2020
If we set η = 1 - 1/B, a weighting of 1/B is seen on the current sample, just as in standard averages
with a batch of size B, but now all samples receive similarly clean batch statistic estimates, not just
the last few samples in a batch.
G	Online Dataset
For our experiments, we construct a dataset comprising an offline training, validation, and test set, as
well as an online training set. Specifically, we start with the standard MNIST dataset of LeCun et al.
(1998) and split the 60k training images into partitions of size 9k, 1k, and 50k. Elastic transforms
(Simard et al., 2003; Ernestus, 2016) are used to augment each of these partitions to 50k offline
training samples, 10k offline validation samples, and 100k online training samples, respectively.
Elastic transforms are also applied to the 10k MNIST test images to generate the offline test samples.
The source images for the 100k online training samples are randomly drawn with replacement, so
there is a certain amount of data leakage in that an online algorithm may be graded on an image that
has been generated from the same image a previous sample it has trained on has been generated from.
This is intentional and is meant to mimic a real-life scenario where a deployed device is likely to see
a restrictive and repetitive set of training samples. Our experiments include comparisons to standard
SGD to show that SKS’s improvement is not merely due to overfitting the source images.
From the online training set, we also generate a “distribution shift” dataset by applying unique
additional augmentations to every contiguous 10k samples of the 100k online training samples. Four
types of augmentations are explored. Class distribution clustering biases training samples belonging
to similar classes to have similar indices. For example, the first thousand images may be primarily
“0”s and “3”s, whereas the next thousand might have many “5”s. Spatial transforms rotate, scale, and
shift images by random amounts. Background gradients both scale the contrast of the images and
apply black-white gradients across the image. Finally, white noise is random Gaussian noise added to
each pixel. Figure 5 shows some representative examples of what these augmentations look like. The
augmentations are meant to mimic different external environments an edge devices might need to
adapt to.
ΞΞ□EiElEIE]QE]Ei
DDUDUHΠUnα
SHQHQBBEaQQ
ξξħξπbssqħ
ΞESE3EiEI□□□3Ξ
SRIQgSEIQQEE
QQQBQQaiSUQ
SHanaEIQQEiIB
πsqqqdeieħπ
ξqŋei□□eiξħd
(a)	Spatial Transforms
ΞQDΞBE1EIE]E]Ξ
ΠDDDBΠBaBB
ΞBΞBQBBBBH
Q□DE3DQ□□E1ZI
■ E1SHBBSBBE3
QQQQQQrjaISIl 1
■BBBBQQHBH
ΞΞH□□QDH□D
(b)	Background Grads
国目0国El❾股国0圜
HHDQginDDPn
0❸色&理目图El国E3
ħqsξξei≡bħs
El团数用目目bEIEIEl
sħξ≡ξħsbh^
段修固固日墨嚼麴口已
QRranEIElQQElB
oħξq□□ob^b
Ξ^HEIE]QDO0D
(C) White Noise
Figure 5:	Samples of different types of distribution shift augmentations.
In addition to distribution shift for testing adaptation, we also look at internal statistiCal shift of
weights in two ways - analog and digital. For analog weight drift, We apply independent additive
Gaussian noise to each weight every d = 10 steps with σ = σ0 /，1M/d where σ0 = 10 and re-clip
the weights between -1 and 1. This Can be interpreted as eaCh Cell having a Gaussian Cumulative error
with σ = σ0 after 1M steps. For digital weight drift, we apply independent binary random flips to
the weight matrix bits every d steps with probability p = p0/(1M/d) where p0 = 10. This can be
interpreted as each cell flipping an average of p0 times over 1M steps. Note that in real life, σ0 , p0
depend on a host of issues such as the environmental conditions of the device (temperature, humidity,
etc), as well as the rate of seeing training samples.
19
Under review as a conference paper at ICLR 2020
H Hyperparameter S election
In order to compare standard SGD with the SKS approach, we sweep the learning rates of both to
optimize accuracy. In Figure 6, we compare accuracies across a range of learning rates for four
different cases: SGD or SKS with or without max-norming gradients. Optimal accuracies are found
when learning rate is around 0.01 for all cases. For most experiments, 8b weights, activations, and
gradients, and 16b biases are used. Experiments similar to those in Section I are used to select some
of the hyperparameters related to the SKS method in particular. In most experiments, rank-4 SKS
with batch sizes of 10 (for convolution layers) or 100 (for fully-connected layers) are used. Additional
details can be found in the supplemental code.
SGD/Max-Norm
1.0
0.4
-0.2
-0.8
-1.4-
-2.0-
-2.6
-3.2
-3.8
-4.4
-5.0
9%
9%
9%
7%
77%
29%
10%
7%
7%
SKS/No-Norm
1.2
0.6
0.0
-0.6
-1.2
-1.8
-2.4
-3.0
1.0 1.5 2.0 2.5 3.0
Log10 FC Batch Size
-1.0
0.8
0.6
0.4
0.2
0.0
-OI Jo OOES"，) >uro⅛uu<
Log10 FC Batch Size
Figure 6:	The left two heat maps are used to select the base / standard SGD learning rate. The right
two heat maps are used to select the SKS learning rate using the optimal SGD learning rate for bias
training from the previous sweeps. For the SKS sweeps, the learning rate is scaled proportional to
the square-root of the batch size B . This results in an approximately constant optimal learning rate
across batch size, especially for the max-norm case. Accuracy is reported averaged over the last 500
samples from a 10k portion of the online training set, trained from scratch.
I Additional Studies
In Figure 7, rank and weight bitwidth is swept for SKS with gradient max-norming. As expected,
training accuracy improves with both higher SKS rank and bitwidth. In dense NVM applications,
higher bitwidths may be achievable, allowing for corresponding reductions in the SKS rank and
therefore, reductions in the auxiliary memory requirements.
In Table 2, biased (zero-variance) and unbiased (low-variance) versions of SKS are compared.
Accuracy improvements are generally seen moving from biased to unbiased SKS although the pattern
differs between the no-norm and max-norm cases. In the no-norm case, a significant improvement is
seen favoring unbiased SKS for fully-connected layers. In the max-norm case, the choice of biased
or unbiased SKS has only a minor impact on accuracy. It might be expected that as the number
of accumulated samples for a given pseduobatch increases, lower variance would be increasingly
important at the expense of bias. For our network, this implies convolutions, which receive updates at
every pixel of an output feature map, would preferentially have biased SKS, while the fully-connected
layer would preferentially be unbiased. This hypothesis is supported by the no-norm experiments,
but not by the max-norm experiments.
In Table 3, several ablations are performed on SKS with max-norm. Most notably, weight training is
found to be extremely important for accuracy as bias-only training shows a ≈ 15 - 30% accuracy hit
depending on whether max-norming is used. Streaming batch norm is also found to be quite helpful,
especially in the no-norm case.
Now, we explain the κth ablation. In Section 4.1.1, we found the SVD of a small matrix C
and its singular values σ1, . . . , σq. This allows us to easily find the condition number of C as
K(C) = σι∕σq. We suspect high condition numbers provide relatively useless update information
akin to noise, especially in the presence of L, R quantization. Therefore, we prefer not to update
L, R on samples whose condition number exceeds threshold κth . We can avoid performing an actual
SVD (saving computation) by noting that C is often nearly diagonal, leading to the approximation
20
Under review as a conference paper at ICLR 2020
7 6 5 4 3 2
Iμp一M±g-M,⊂6a∕∖Λ
53% 59% 67% 69% 72% 75% 74% 76% 80% 76%
56% 65% 68% 72% 74% 79% 75% 79% 79% 76%
55% 64% 64% 69% 74% 74% 76% 74% 77% 77%
51% 58% 61% 62% 68% 69% 75% 76% 77% 78%
45% 50% 55% 59% 54% 63% 63% 64% 70% 70%
40% 42% 40% 44% 43% 45% 49% 52% 52% 49%
12% 9% 31% 26% 10% 12% 9% 9% 9% 10%
12% 12% 13% 13% 12% 12% 14% 13% 14% 12%
1.0
0.8式
M-
O
O
0.6^
S
P
0.4;
0.2 y
123456789	10
SKS/Max-Norm Rank
0.0
8
1
Figure 7:	Accuracy across a variety of SKS ranks and weight bitwidths, showing the expected trends
of increasing accuracy with rank and bitwidth. Accuracy is calculated by averaging the accuracy
on the last 500 samples from a 2k portion of the training data. For bitwidths of 1 and 2, mid-rise
quantization is used (e.g., 1 bit quantizes values to -0.5 and 0.5 instead of -1 and 0).
Table 2: Importance of unbiased SVD. Accuracy is calculated from the last 500 samples of 10k
samples trained from scratch. Mean and unbiased standard deviation are calculated from five runs of
different random seeds.
Conv SKS	FC SKS	Accuracy (no-norm)	Accuracy (max-norm)
Biased	Biased	79.7% ± 1.1%	82.7% ± 1.3%
Biased	Unbiased	83.0% ± 0.9%	82.4% ± 1.2%
Unbiased	Biased	77.7% ± 1.5%	84.6% ± 2.0%
Unbiased	Unbiased	81.0% ± 0.9%	83.6% ± 2.5%
κ(C) ≈ C1,1/Cq,q. Empirically, this rough heuristic works well to reduce computation load while
having minor impact on accuracy. In Table 3, κth = 108 does not appear to ubiquitously improve on
the default κth = 100, despite being ≈ 2× slower to compute.
Table 3: Miscellaneous selected ablations. Accuracy is calculated from the last 500 samples of 10k
samples trained from scratch. Mean and unbiased standard deviation are calculated from five runs of
different random seeds.
Modified Condition
baseline (no modifications)
bias-only training
no streaming batch norm
no bias training
κth = 108 instead of 100
Accuracy (no-norm) Accuracy (max-norm)
80.2% ± 1.0%
51.8% ± 3.2%
68.2% ± 1.9%
81.3% ± 1.0%
79.8% ± 1.4%
83.0% ± 1.1%
68.6% ± 1.4%
81.8% ± 1.3%
83.0% ± 1.4%
84.2% ± 1.4%
21