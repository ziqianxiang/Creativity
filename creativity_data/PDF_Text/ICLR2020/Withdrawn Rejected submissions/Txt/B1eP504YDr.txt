Under review as a conference paper at ICLR 2020
Independence-aware Advantage Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Most of existing advantage function estimation methods in reinforcement learn-
ing suffer from the problem of high variance, which scales unfavorably with the
time horizon. To address this challenge, we propose to identify the independence
property between current action and future states in environments, which can be
further leveraged to effectively reduce the variance of the advantage estimation. In
particular, the recognized independence property can be naturally utilized to con-
struct a novel importance sampling advantage estimator with close-to-zero vari-
ance even when the Monte-Carlo return signal yields a large variance. To further
remove the risk of the high variance introduced by the new estimator, we combine
it with existing Monte-Carlo estimator via a reward decomposition model learned
by minimizing the estimation variance. Experiments demonstrate that our method
achieves higher sample efficiency compared with existing advantage estimation
methods in complex environments.
1	Introduction
Policy gradient method (Sutton et al., 2000) and its variants have demonstrated their success in solv-
ing a variety of sequential decision making tasks, such as games (Mnih et al., 2016) and continuous
control (Lillicrap et al., 2015). The large variance associated with vanilla policy gradient estimator
has prompted a series of previous works to use advantage function estimation, due to its variance-
minimized form (Bhatnagar et al., 2008), to get a stable policy gradient estimation (Mnih et al.,
2016; Schulman et al., 2015a;b; 2017). For a policy π and a state-action pair (s, a), all these works
estimate the advantage function Aπ(s, a) by subtracting an estimate of the value function Vπ(s)
from the estimate of Q-value Qπ(s, a). The estimation of Qπ (s, a) or Vπ(s) typically involves a
discounted sum of future rewards, which still suffers from the high variance especially when facing
the long time horizon.
Meanwhile, in many real-world reinforcement learning applications, we observe that not all future
rewards have dependency with the current action. For example, consider a simple multi-round game
where at the end of each round of this game, the agent will be assigned a reward, representing
whether it wins this round. An episode of the whole game consists of multiple independent rounds.
In this example, an action in the current round will not affect the rewards in future rounds, and not
all rewards received in future states do contribute to the advantage function of the current action.
However, most of existing RL methods (Sutton et al., 2000; Mnih et al., 2013; Schulman et al.,
2015b) sum all future rewards to evaluate each action without considering their dependency. By
identifying the independence between current action and future states in the environment, we are
able to take advantage of such independence to reduce the variance of advantage estimation.
In this paper, we propose Independence-aware Advantage Estimation (IAE), an algorithm that can
identify and utilize the independence property between current action and future states. We first
introduce a novel advantage estimator that can utilize the independence property by importance sam-
pling. The estimator formalizes a dependency factor Cπ , representing the contribution level of each
future reward to advantage function estimation. For those states with no dependency on the current
action, there will be a close-to-zero dependency factor Cπ , and the importance sampling estimator
can reduce the variance of advantage estimation by ignoring the rewards on these states. For those
states with a large dependency factor, the importance sampling estimator will potentially increase
variance. In order to take advantage of variance reduction caused by small Cπ while removing the
risk of increased variance by large Cπ , we further combine existing Monte-Carlo estimator with
the proposed estimator by decomposing the reward into two estimators and learning the optimal
1
Under review as a conference paper at ICLR 2020
decomposition by minimizing the corresponding estimation variance. Ideally, when facing states
with zero dependency on the current action, our model can learn to distribute all the reward into the
importance sampling estimator, where the reward can be ignored; when those states yield extremely
large Cπ , our model can learn to distribute part of rewards into the Monte-Carlo estimator to reduce
the potential high variance caused by importance sampling. Details of our method are described in
Section 3, 4 and 5.
Empirically, we show that our estimated advantage function is closer to ground-truth advantage
function Aπ than existing advantage estimation methods such as Monte-Carlo and Generalized Ad-
vantage Estimation (Schulman et al., 2015b). We also test IAE advantage estimation in policy
optimization algorithms, showing that our method outperforms other advantage estimation methods
in sample efficiency. Results of our experiments are reported in Section 8.
Our contributions can be summarized as follows:
•	As far as we know, we are the first to explore and utilize the independence property between
current action and future states in environments to improve advantage estimation. The
independence property can help us ignore the unnecessary high variance parts in Monte-
Carlo estimator which do not contribute to advantage function.
•	We propose a practical advantage estimation method to identify and utilize the indepen-
dence property in environments, which achieves better performance than other advantage
estimation methods in both tabular settings and function approximation settings.
2	Background
2.1	Notations & Problem Settings
We consider a finite-horizon Markov Decision Process defined by (S, A, P, R, ρ0 , γ, T ), where S is
the set of states, A is the finite set of actions, P : S × A × S → R denotes the transition probability,
R : S × A → R denotes the reward function, ρ0 : S → R denotes the distribution of initial state
S0, γ ∈ (0, 1] is the discount factor, T is the total time steps. We denote St, At, Rt as the random
variable of state, action, reward at time t, and τt := (St, At, Rt, St+1, ..., ST, AT, RT) as trajectory
starting from time t.
We denote π : S × A → R as a stochastic policy, and use the notation of Qπ(st, at), Vπ(st),
Aπ(st, at) as state-action value function, state value function and advantage function respectively. In
the following discussions, we will recognize (st, at) as a constant state-action pair whose advantage
function needed to be estimated.
2.2	Advantage Function Estimators
Monte-Carlo estimator AMC of advantage function An (st,at) is formalized below:
T-t
AMC ：= -Vθ(St) + X YkRt+k, where Tt 〜Pn(τt∣st,at).
k=0
Here Vθ (St) denotes the function approximator of value function Vπ (St). We use Tt 〜 Pπ (τt∣st, at)
to denote that trajectory τt is generated by policy π from st , at .
Some previous work focuses on reducing the variance of AMC at the cost of introducing bias (SchUl-
man et al., 2015b), by using the n-step TD estimator and GAE estimator of advantage function
Aπ(St, at):
{n-1
-Vθ(St)+XγkRt+k+γnVθ(St+n), ifn<T-t
k=0
AMC,	if n ≥ T - t
∞
AGAE ：= (1 — λ) X λnATD(n+1), WhereTt 〜Pπ(Tt厢,电).
n=0
2
Under review as a conference paper at ICLR 2020
3 Utilizing Independence Property in Advantage Estimation
In many cases, we can utilize the independence between current action and future states to avoid
unnecessary parts of variance in the Monte-Carlo estimators. Consider the example where we have a
current state st whose advantage functions with respect to all actions are needed to be estimated. For
a set of st+k which can be reached from st, we have independence property such that the probability
Pπ(st+k|st, at) is a constant with respect to at. Although the Monte-Carlo return estimator from
st+k may have large variance, it is clear that the return after reaching st+k gives no contribution to
Aπ (st, at) in this case.
In this section, we propose anew advantage estimator based on importance sampling, which removes
the variance in Monte-Carlo return estimator after st+k by utilizing independence property, exactly
as we described above. In later discussions, we will name the proposed estimator as importance
sampling advantage estimator.
By importance sampling approach, we present our way to derive Aπ (st, at) into a form which
utilizes independence property:
T-t
An(St, at) = ETt〜Pπ(τt∣st,at)	γkRt+k
k=0
T-t
-ETt〜P∏(Ttlst)	EYkRt+k
k=0
Eτt~Pπ(τt∣st)
T-t
X γkRt+k(
k=0
P^(St+k1At+k∖st1αt)
P π (St+k,At+k∣st)
- 1)
(1)
To briefly summarize our derivation, we perform importance sampling in every future time t + k,
estimating the discounted reward γkRt+k in distribution Pπ(St+k, At+k ∖st, at) by sampling on
distribution Pπ(St+k, At+k∖st). Itis worth noting that when k ≥ 1, At+k is independently sampled
by St+k, and we are able to omit At+k in the probability ratio.
For the simplicity of discussion, we will use the following definition:
Ck (st, at, st+k , at+k)
Pπ(st+k, at+k∖st, at)
-----7--------：--7- - 1
Pπ(st+k, at+k∖st)
(2)
where we call Ckπ(st, at, st+k, at+k) the dependency factor, since the value captures how taking
a specific action at changes the probability of reaching a future state-action pair (st+k, at+k).
It is clear from equation 1 and equation 2 that future state pair st+k that has dependency factor
Ckπ(st, at, st+k, at+k) close to 0 has small contribution to Aπ(st, at), which further demonstrates
that the rewards from independent future states do not contribute to advantage estimation, even if
Monte-Carlo return signal has high variance. In practice, we face the challenge to estimate the
dependency factor Cπ by data samples. We propose a novel modeling method and a temporal dif-
ference training strategy to solve this problem, which is detailed in Section 5.
4	Optimal Combination with Monte-Carlo estimator
The advantage estimation method proposed in section 3 nicely deal with those future rewards which
are independent on current action, since we have dependency factor close to zero and further ignore
those rewards. However, the importance sampling advantage estimator may badly deal with those
rewards with large dependency factor, which can increase the variance in estimation. To illustrate,
consider we have st whose advantage functions with respect to all actions are needed tobe estimated.
The Monte-Carlo return starting from st following π is close to a constant q, while there is a large
gap between P π (St+k ∖st, at) and P π (St+k ∖st). This dependent case can cause high variance in
importance sampling advantage estimator, even when Monte-Carlo estimation has low variance.
To deal with the potential high variance problem, we seek to find the optimal combination between
the proposed importance sampling estimator and Monte-Carlo estimator. There have been some
previous works (Grathwohl et al., 2017) focusing on combining two estimators by optimizing a
control variate, producing an estimator with less variance. Inspired by that, we decompose the
reward into two estimators with a reward decomposition model, and learn the reward decomposition
model by minimizing estimation variance.
The following theorem demonstrates our derivation to combine two estimators:
3
Under review as a conference paper at ICLR 2020
EI_ . _ _ _ Λ C	A /	I	∖	1	A -	1 1 ∙1 ∙. 1 ∙ . ∙1 . ∙	τ-7
Theorem 1. Suppose rt+k 〜R(rt+k∣st, at, τt+k), where R is any probability distribution. Then
T-t
An(St,at) = ETt 〜P ∏(τt∣st,at) EYk (Rt + k - rt + k )
k=0
T-t
-ETt 〜P π(τt∣st)	£Yk (Rt + k - rt + k )
k=0
T-t
+ ETt 〜Pπ (Tt |st)〉: Y r t + k Ck (St, at, St + k , At + k)
k=0
(3)
The proof of theorem 1 is provided in Appendix A.1. The sum of all the terms containing rt+k in
equation 3 constructs a control variate with zero expectations, while the expectation of rest of terms
containing Rt+k is the value of advantage function Aπ(st, at). From equation 3, we find that rt+k
determines the way in which rewards are divided into two estimators. If rt+k is close to 0, rewards
are divided into the Monte-Carlo estimator; if rt+k is close to Rt+k, rewards are divided into the
importance sampling advantage estimator. We parameterize rt+k as rt+k,ψ, seeking to optimize ψ
to gain an advantage estimator with minimized variance. In practice, we represent rt+k,ψ by a neural
network rψ(st, at, St+k, At+k, Rt+k, k), whose architecture is shown by Figure 6 in the appendix.
For simplicity, we will use Jψ(τt) and Iψ(τt, at) to denote two random variables inside of expecta-
tion in equation 3, which is written by:
T-t	T-t
Jψ (Tt) := X Yk (Rt+k - rt+k); Iψ (τt,at) := X Ykrt+kCπ (st,at,St+k,At+k)	(4)
k=0	k=0
We use Varψ(st, at) to denote the variance of advantage estimator derived from equation 3. Though
we are not able to write the closed-form of variance in advantage estimation since we don’t know
how three sampling processes in equation 3 are correlated, we are able to derive the variance upper
bound of possible estimators by equation 5, whose proof is shown in Appendix A.2.
Varψ (st,at) ≤ 3 IVarTt 〜Pn (τt∣st,at) jψ (τ^t)] +VarTt 〜P∏(τt∣st) jψ (Tt)] +Varτt 〜Pn (τt∣st) [1ψ (Tt ,at)]]
(5)
Based on equation 5, we further derive the upper bound of variance which is friendly to optimize:
Eat〜π(at |st) Varψ (St, at) ≤ 6VarTt〜Pn (Tt |st)
[Jψ (Tt)]
+ 3Eat~n(at|st)VarTt~Pn (τt∣st)
[Iψ (τt,at)]
(6)
We seek to optimize ψ to minimize the variance upper bound shown in equation 6. To achieve this,
we firstly use two value function approximators Vw1 (st),Iw2 (st, at) to approximate the expectation
of two estimators respectively:
VwI (St) ≈ ETt〜Pn(τt∣st)
[Jψ (Tt)] ； Iw2 (St,at) ≈ ETt 〜P n (Tt∣st) IΨ( (τt,at)]
(7)
To approximate the expectation, we use SGD to minimize the mean squared error. The parameter
update process can be finally expressed as follows, where αw represents the step-size:
w
w
0π
,1 = wι + α∙w(Jψ(Tt) - VWi (St))VWI VWi (st),Tt 〜P (τ∕st)
0π
,2 = W2 + αw(Iψ (τt,at) 一 Iw2 (st, at))Vw2 Iw2 (st, at),Tt 〜P (τt∣St)
(8)
(9)
If we assume that Vw1 (St) and Iw2 (St, at) accurately represent the expectation, then we get the
gradient to optimize ψ as follows, in order to minimize the upper bound of Eat〜∏(at ∣st) Varψ (st, at)：
Vψ (6VarTt 〜P n (τt∣st) [Jψ (Tt)] +3Eat 〜∏(αt∣st)Varτt 〜P ∏ (τt |st) [1ψ (τt,at)])
=3ETt〜Pn(Tt∣st) 2Vψ(Jψ(τt) - VWi (st))2 + E∏(at∣st)Vψ(Iψ(τt, at) - Iw2 (st,at))2
at
(10)
We can use SGD to optimize ψ expressed by equation 11, where αψ represents the step-size：
0
ψ = ψ + αψ 2(Jψ (Tt ) - VWi (st ))Vψ Jψ (Tt )
+ X∏(at∣st)(Iψ(τt, at) - Iw2(st, at))VψIψ(τt, at)] ,Tt 〜PK(τt∣st)	(11)
at
4
Under review as a conference paper at ICLR 2020
Here we will illustrate why performing gradient descent on ψ leads to an advantage estimation
with lower variance. For a single rt+k,ψ, the gradient component in the first term of equa-
tion 11 pushes the rt+k,ψ to change towards the direction to the mean total return; meanwhile,
the second term of equation 11 counteracts the gradient in the first term, preventing rt+k,ψ from
constructing a constant return by the restriction in variance of importance sampling estimator
PkT=-0t γkrt+k,ψ Ckπ (st, at, St+k). When we have the independence property in environments (i.e.
the value of Ckπ is close to zero), the counteraction effect in the second term will disappear. With
the gradient in the first term, rt+k,ψ will be rapidly optimized towards the mean total return, making
the variance of advantage estimator to dramatically decrease along the training process.
By replacing the expectation terms in equation 3 by function approximators, the form of
independence-aware advantage estimator is given by:
AtAE := Jψ (Tt) - VwI (st) + Iw2 (st,at), WhereTt 〜Pπ (τt∣st,aj	(12)
5 Dependency Factor Estimation
The final challenge in our method is to estimate the dependency factor Cπ , Which is crucial to make
the advantage estimator loW-biased. In this section, We Will introduce our modeling and training
method, Which is able to give accurate dependency factor estimation in experiments.
It is hard to estimate the transition probability in equation 2 because of the high dimensionality of
state space. Here We derive the ratio betWeen tWo transition probability into a form Which can be
represented by an action classifier by equation 13, Whose proof is shoWn in Appendix A.3.
Pπ(st+k,at+k∣st,at)	Pπ(at∣St,St+k,at+k)	Pπ(at∣st, st+k).一
-----=---------i——∖- =--------7——：  ------=-----------：  ---,if k ≥ 1
Pπ(st+k, at+k∣St)	∏(at∣St)	∏(at∣St)	一
(13)
In practice, We use an action classification model Pφ(at |st, st+k, k) to represent Pπ (at |st, st+k), us-
PφStlst,st + k,2
∏(at∣st)
ing Cφ(st, at, st+k)
to give an approximation of Cπ. We call Pφ(at |st, st+k, k)
dependency model in later discussions.
Similar to the derivation in previous Work (Liu et al., 2018), We shoW that the dependency model
can be learned in an temporal difference manner:
Pπ(at∣st,st+k2 ) = Est+k1 〜Pn(st+k1∣st+k2,st) [Pπ(at∣st,st+kι)], if k2 >kι ≥ 1.	(14)
Detailed proof of equation 14 is provided in Appendix A.4. Although the case is different from
the common cases in temporal difference learning, it is clear that the samples of distribution
P π (st+k1 |st+k2, st) can be collected by directly rolling out policy π, then We are able to train
model Pφ by minimizing temporal difference error. We use a mixture of temporal difference tar-
get and Monte-Carlo target to train model Cφ, Which is detailed in Appendix B. Empirically, We
demonstrate the effectiveness of our approach to accurately estimate dependency factor in section
8.2.
6	Algorithm
In this section, We present crucial algorithms details of the method presented in Section 3, 4 and 5.
6.1	Model Dependency and Pseudo-Code
We illustrate the general frameWork of our algorithm shoWn by Figure 1. The model of dependency
factor Cπ , lying in the basic part of our algorithm, can be directly trained by samples of current
policy π . TWo value functions Iw1 and Vw2 are trained by SGD on MSE loss, shoWn in equation 8
and equation 9, given the the reWard decomposition rψ as input. The reWard decomposition model
rψ is also trained by SGD shoWn in equation 11, given the output of Iw1 and Vw2. The pseudo-code
of our algorithm is provided by Algorithm 1.
5
Under review as a conference paper at ICLR 2020
Figure 1: Model Dependency Graph.
Algorithm 1 Policy Optimization Algorithm by Independence-aware Advantage Estimation
1:	Initialize policy parameters θ, dependency model parameters φ, decomposition model parame-
ters ψ, and value function parameters w1 , w2 .
2:	for each iteration do
3:	Sample trajectory τ with policy πθ
4:	Update φ by temporal difference learning
5:	Update w1, w2 to minimize mean squared error by equation 8 and equation 9
6:	Update ψ to minimize variance upper bound by equation 11
7:	Compute advantage estimation AtAE at each time-step t by equation 12
8:	Update θ by Proximal Policy Optimization
9:	end for
6.2	Computational Complexity
There might be concerns about computational complexity for training dependency model and reward
decomposition model, since the training requires O(T 2) number of (st, st+k) pairs as training sam-
ples. We apply two techniques, trajectory truncation and block-wise training, enabling our algorithm
to have a comparable computational complexity with PPO algorithm.
Trajectory Truncation. We truncate the trajectory into slices of 128 timesteps similar to the ap-
proach in PPO algorithm. We apply our method into the truncated trajectories by considering a
reward Qθ (st , at) at the last step. The reward decomposition model also considers the decomposi-
tion over the final reward Qθ (st , at).
Block-wise Training. When training dependency model and reward decomposition model, we
provide a total of 1282 numbers of (st, st+k) pairs, and accumulate gradient for every valid
(st, st+k) pair. Only 128 × 2 times of forward and backward passes in CNN feature extractor
are computed though we deal with a squared number of training data, making our method computa-
tionally efficient. Our method requires less than 3 times training time compared to PPO algorithm.
7	Related Work
Policy gradient (Sutton et al., 2000) provides the basic form to optimize a parameterized policy in
expected returns. Generalized Advantage Estimation (Kimura et al., 2000; Schulman et al., 2015b)
replaces Monte-Carlo estimator by the mixture of N-step temporal difference estimator, reducing the
variance of policy gradient estimator while introducing bias. Another series of previous works focus
on how to optimize parameterized policy without focusing on the selection of estimator. Among
these works, TRPO (Schulman et al., 2015a) and PPO (Schulman et al., 2017) are part of the recent
works that reaches state-of-the-art performance on a variety of tasks.
6
Under review as a conference paper at ICLR 2020
Some of previous works have shown that RL algorithm tends to be unstable when planning horizon
is long (Jiang et al., 2015). Another series of works (Francois-Lavet et al., 2015; XU et al., 2018)
focus on how to select discount factor γ to improve the performance of RL algorithm. Our method
sUggests another solUtion to the problem in RL, since IAE leads to sUbstantial variance redUction in
retUrn signal even if the planning horizon is long, which improves the performance while keeping
the long planning horizon.
OUr method performs gradient descent on estimation variance to improve the estimator as train-
ing proceeds. This method has been Used in recent works on varioUs applications. One previoUs
work (Hanna et al., 2017) focUses on optimizing a behavioUr policy to minimize the variance of
off-policy valUe estimation; another previoUs work (Grathwohl et al., 2017) focUses on getting the
optimal variance balance between REINFORCE estimator and reparameterized gradient estimator
by minimizing estimation variance.
In the problem of density ratio estimation, oUr method is similar to one previoUs work (LiU et al.,
2018) which transforms the density ratio estimation problem into an temporal difference learning
problem. OUr method is different that we focUs on estimating the dependency between cUrrent
actions and fUtUre states in a fixed policy, while this previoUs work focUses on estimating the ratio
of the fUtUre state reaching probability between two different policies.
8	Experiments
In oUr experiments, we provide empirical resUlts to answer the following qUestions:
•	Can oUr dependency model training method in section 5 precisely estimates the dependency
factor Cπ , and captUres the independence property in environments?
•	Can oUr method Utilize the independence property to redUce the variance in advantage
estimation, and fUrther give more accUrate advantage estimation than other advantage esti-
mation methods?
•	Can IAE improve the overall performance of policy optimization algorithm, for instance,
PPO algorithm?
To answer the first qUestion, we train the dependency model by method in section 5 and compare the
prediction with groUnd-trUth valUe, proving the capacity of oUr training method to model dependency
factor Cπ. This part of resUlts are detailed in section 8.2.
For the second qUestion, we show that IAE method gives advantage estimation with less variance in
tabUlar settings, and redUces valUe fUnction training error in fUnction approximation settings. In the
Pixel Grid World environment, we fUrther show that oUr method gives advantage estimation closer
to groUnd-trUth advantage fUnction than MC and GAE method Under cosine similarity metric. This
part of resUlts are detailed in section 8.3. To demonstrate how IAE method Utilize independence
property, we also provide case stUdies in Appendix C.3 to visUalize the effect of variance redUction
by low dependency factor in Pixel Grid World environment.
For the last qUestion, we provide training cUrves in section 8.4 in Pixel Grid Worlds. Compared with
PPO algorithm with Monte-Carlo advantage estimation and generalized advantage estimation, IAE
method makes policy optimization process more sample-efficient.
8.1	Environment Settings
We perform experiment on two types of environments: finite-state MDPs and Pixel Grid World.
Finite-state MDP settings. To evalUate the qUality of advantage estimation of oUr method in tab-
Ular cases, we constrUct different 3-state MDPs with different transition probability and reward
fUnctions. We categorize state transition probability settings into connected settings and isolated
settings, and categorize reward settings into high-variance settings and low-variance settings. De-
tailed descriptions of transition probability and reward settings are provided in Appendix C.1.
Pixel Grid World Environment. To evalUate oUr method in fUnction approximation settings, we
bUild Pixel Grid World environment where observations are provided by high-dimensional images.
7
Under review as a conference paper at ICLR 2020
(a)
Figure 2: Results on dependency factor modeling. (a): The top and bottom image respectively illus-
trate st and st+k in the case study, and here we set k to be 7. (b): Dashed lines show the true dis-
tribution of Pπ (at∣st, st+k); solid lines show the dependency model prediction Pφ (at∣st, st+k, k);
x-axis represents the number of training iterations of dependency model. Four different colors rep-
resent four different actions. Blue line shows the KL divergence between predicted distribution and
true distribution. (c): The blue line shows the mean KL divergence between Pπ(at∣st, st+k) and
Pφ (at |st, st+k, k) over the dataset of random (St, st+k) pairs, averaged in 10 runs.
	Connected-low	Isolated-low	Connected-high	Isolated-high
MC	0.63	面	6.21	16.58
IS	1.43	8.68	1.45	8.63
IAE	0.59	0.60	0.68	0.71
Table 1: Standard derivation of various estimators in different transition probability settings (con-
nected and isolated) and different reward settings (low-variance and high-variance).
As illustrated in Figure 2a, the blue square represents the position of the agent and the yellow square
represents the position of current goal. The agent gets positive reward for reaching the goal. To
make the problem harder, the environment will do periodical reset multiple times in an episode,
by which the agent and the goal are randomly repositioned. We use two different reward settings:
per-step punishment setting and no punishment setting, which is detailed in Appendix C.1.
The hyper-parameters and network architectures we use in our experiments are presented in Ap-
pendix C.2.
8.2	Dependency Factor Modeling
In this section, we investigate our estimation of dependency factor Cπ , and show the general simi-
larity between our estimation and ground-truth Cπ . We train our model Cφ by data generated by a
fixed uniform random policy π . Figure 2a and 2b show the case where the dependency is precisely
captured: given the future state st+k shown in Figure 2a, the model Pφ (at |st, st+k, k) correctly
predicts down and right action that more likely lead current state st to future state st+k . We also
build a dataset consisting of 300 random (st , st+k) pairs, where k is uniformly sampled from 1
to 30. We evaluate the mean KL divergence between ground-truth Pπ (at |st, st+k) and prediction
Pφ (at |st, st+k, k) averaged in 10 runs, as shown in Figure 2c. The mean KL divergence decrease to
a relatively small value during training, showing that the dependency model Pφ leads to a generally
precise estimation of dependency factor Cπ .
8.3	The variance and accuracy of independence-aware advantage estimation
We evaluate the variance of IAE estimator on a variety of finite-state MDP settings. We train
r(st, st+k) for 10000 episodes and then test the advantage estimator by performing advantage es-
timation multiple times to get the estimation variance. We compare the variance of IAE estimator
with Monte-Carlo advantage estimator (MC) and importance sampling advantage estimator (IS). In
this experiment, we use the precise value of dependency factor for IS and IAE estimator. Table 1
8
Under review as a conference paper at ICLR 2020
Figure 3: (a): Mean squared error of two value functions Vw1 and Iw2 averaged in 10 runs on Pixel
Grid World. Green and red lines show the MSE of Vw1 and Iw2 respectively, when rψ is fixed; blue
and orange lines show the MSE of Vw1 and Iw2 respectively, when rψ is trained by our method.
(b): The cosine similarity between advantage estimation and ground-truth advantage function. We
compare IAE estimation, Monte-Carlo estimation and GAE estimation.
(a)
Figure 4: Overall performance curve. Figure (a) and (b) respectively show the training curve on
Pixel Grid World environment in per-step punishment setting and no punishment setting, averaged
in 10 random seeds. In per-step punishment setting agent gets negative rewards before reaching
goals, while in no punishment setting agent gets no reward before reaching goals.
(b)
demonstrates the standard derivation of advantage estimation. In both environments suitable for MC
estimation and ones suitable for IS estimation, our method gives estimation with less variance than
both MC and IS method. In some cases, IAE estimation dramatically reduces the variance of both
MC and IS estimation.
On function approximation settings, we show that our method dramatically reduces the mean
squared error in training value function approximators, as shown in Figure 3a. We initialize re-
ward decomposition model rψ to be zero for all inputs, which constructs a precise Monte-Carlo
advantage estimator initially, and compare the value function training error with or without training
rψ . When the reward decomposition model rψ is fixed, the loss of value function training keeps to
be high because of the stochasticity of Monte-Carlo return signal, while in our method, the rψ makes
reward to be distributed into importance sampling advantage estimator, making the mean squared
error of value function to reduce. In Figure 3b, we show that IAE estimation has much higher cosine
similarity to ground-truth advantage function, compared with Monte-Carlo and GAE estimation.
Besides, we investigate how independence property reduces the variance of estimation by visualizing
the reward decomposition in Pixel Grid World Environment, which is detailed in Appendix C.3.
8.4	Performance of Policy Optimization
We run Proximal Policy Optimization algorithm with IAE advantage estimation method, and com-
pare the result to PPO algorithm with Monte-Carlo and GAE advantage estimation. Figure 4 shows
9
Under review as a conference paper at ICLR 2020
the result on two different reward settings. Compared with Monte-Carlo and GAE estimation, IAE
estimation makes the policy improvement process more sample-efficient.
9 Conclusions
In this work, we addressed the large variance problem in advantage estimation for policy gradient
methods. We proposed a novel advantage estimation method by importance sampling, which iden-
tifies and utilizes the independence property, reducing the variance by ignoring those independent
rewards. We further combined the proposed estimator with Monte-Carlo estimator in the optimal
way, making the final IAE estimator to have low variance in general cases. The effectiveness of our
method can be verified on Pixel-input environments compared with previous advantage estimation
methods such as Monte-Carlo and Generalized Advantage Estimation.
There are a few directions to explore in the future. First, we only considered problems with discrete
actions in this work, and we will extend our method to problems with continuous actions. Second,
we will test our algorithm on more reinforcement learning environments, such as Atari games and
continuous control tasks.
References
Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental nat-
Ural actor-critic algorithms. In Advances in neural information processing Systems, pp. 105-112,
2008.
Vincent Frangois-Lavet, Raphael Fonteneau, and Damien Ernst. HoW to discount deep reinforce-
ment learning: Towards new dynamic strategies. arXiv preprint arXiv:1512.02011, 2015.
Will GrathWohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint
arXiv:1711.00123, 2017.
Josiah P Hanna, Philip S Thomas, Peter Stone, and Scott Niekum. Data-efficient policy evaluation
through behavior policy search. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1394-1403. JMLR. org, 2017.
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard LeWis. The dependence of effective planning
horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous
Agents and Multiagent Systems, pp. 1181-1189. International Foundation for Autonomous Agents
and Multiagent Systems, 2015.
Hajime Kimura, Shigenobu Kobayashi, et al. An analysis of actor-critic algorithms using eligibility
traces: reinforcement learning With imperfect value functions. Journal of Japanese Society for
Artificial Intelligence, 15(2):267-275, 2000.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control With deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari With deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937, 2016.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015a.
10
Under review as a conference paper at ICLR 2020
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems,pp. 1057-1063, 2000.
Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. arXiv
preprint arXiv:1805.09801, 2018.
11
Under review as a conference paper at ICLR 2020
A Proofs
A.1 Proof of Unbiasedness
In this section, we show the proof of theorem 1, which demonstrates that any choice of rt+k will not
bias the combined estimator as long as rt+k is a function of st, at and τt+k .
Theorem 1. Suppose rt+k 〜R(rt+k∣st, at, τt+k), where R is any probability distribution. Then
T-t
A (st, at) = Eτt~Pπ(τt∣st,at)	HCY (Rt + k - rt + k)
k=0
T-t
-ETt 〜Pπ(τt∣st)	£ γk (Rt + k - rt + k )
k=0
T-t
+ Eτt~Pπ (τt∣st)〉： γ rt + kCk (st ,at ,St + k , At + k)
k=0
(1)
where Ckπ (st, at, st+k, at+k)
Pπ(st+k, at+k |st, at)
-----；----------；-7. - 1,
Pπ (st+k, at+k |st)
Proof. Denote r(st, at, τt+k) = E [rt+k|st, at,τt+k].
Since Aπ (st, at) can be written by
T-t
A∏ (st, at) = ETt 〜P π(τt∣st,at)	γkRt+k
k=0
We only need to prove that
T-t
-Eτt~Pπ (τt∣st)	γkRt+k
k=0
T-t
ETt~Pπ(τt∣st)〉： Y rt+kCk (st, at, St+k, At+k)
k=0
T-t
Eτt~Pπ (τt∖st,at)	Y rt+k
k=0
We have derivation that
T-t
-ETt 〜Pn (τt∖st)	EYk rt+k
k=0
T-t
ETt ~Pπ(τt∖st)〉： Y rt+k Ck (st, at, St+k, At+k )
k=0
T-t
+ ETt〜Pπ (τt∖st)	Σ Ykrt+k
k=0
T-t
ETt 〜Pn (Tt∖st) EY k rt+k (1 + Ck (st, at, St+k ,At+k ))
k=0
(2)
T-t
X SX	YkETt+k~Pπ(Tt+k∖st+k,at+k) [r(st, at, τt+k )] (1 + Cn (st, at, st+k , at+k ))P∏ (st+k , at+k |st)dst+k
k=0 S at+k
χt ZSX Y k ETt+k 〜P ∏ (Tt+k∖st+k,ɑt+k) [r(st,at,τt+k )] PPStsat)Pπ (st+k ,at+k lst)dst+k
T-t
X ： X X ： Y ETt+k~Pπ (Tt+k∖st+k,ɑt+k) [r(st, at, τt+k )] P (st+k, at+k lst, at)dst+k
k=0 S at+k
T -t
Eτt^^Pπ (Tt∖st,αt)	Y rt+k
k=0
which proves equation 2 and further proves theorem 1.
□
12
Under review as a conference paper at ICLR 2020
A.2 Upper B ound of Variance
In this section, we show that the if we use the sum of three random variable in equation 3 as the
advantage estimator, the variance of advantage estimation is bounded by three times the sum of the
variance of estimation individual random variable inside expectation in equation 3. We begin with
the following lemma:
Lemma 1. Suppose X1,X2,…,Xn are random variables. Then
n
Var X Xi
i=1
n
≤ nXVar[Xi].
i=1
Proof.
n
Var X Xi
i=1
n
= XVar[Xi] +2 X Cov[Xi, Xj]
i=1	1≤i<j≤n
n	_____________
≤ X Var[Xi]+2 X ,Var[Xi]Var[Xj]
i=1	1≤i<j≤n
Then we have
n
n
n	Var[Xi] - Var	Xi
i=1
n
i=1
≥ (n — 1) X Var[Xi] - 2 X	,Var[Xi]Var[Xj]
1≤i<j ≤n
[Xi] - √Var[Xj]) ≥0,
which proves lemma 1.
□
From lemma 1, it is clear that the variance of advantage estimation is bounded by three times the
sum of three individual variances shown by equation 5.
A.3 Action probability form for Dependency Factor
In this section, we give detailed proof of equation 13 in the main body, which is written by:
Pπ(st+k,at+k∣st,at)	Pπ(at∣St,St+k,at+k)	Pπ(at∣st, st+k).一
-----=--------i——x- =----------7——：  -----=-----------：  --,if k ≥ 1
Pπ(st+k, at+k∣St)	∏(at∣St)	∏(at∣St)	一
(3)
Proof. We separately prove the first and the second equation. The first equation is proved by:
Pπ(st+k, at+k|st, at)	Pπ(st+k, at+k, st, at)	PF(St)
--~:---------：—：— = ----------:----：---- • —~:--------------
Pπ(st+k,at+k∣St)	Pπ(st, at)	Pπ(st+k, at+k, St)
Pπ(st+k,at+k,st,at)	Pπ(st)
------:-----------：-  :----------
Pπ(st+k, at+k, st)	Pπ(st, at)
P∏ (at |st, st+k, at+k)
∏(at∣st)	.
13
Under review as a conference paper at ICLR 2020
The second equation is proved by:
P (at |st, st+k, at+k)
P∏ (st, at, st+k, at+k )
Pπ (st, st+k, at+k )
Pπ(st, at, st+k)Pπ(at+k∣st, at, st+k)
Pπ (st, st+k )Pπ (at+k∣st, st+k)
P ∏ (st, at, st+k )π(at+k |st+k )
Pπ (st,st+k)∏(at+k lst+k)
Pπ (st ,at ,st+k)
Pπ (st, st+k)
P (at|st, st+k).
□
Thus, it is proved that at+k can be abandoned from the probability condition when k ≥ 1. When
k = 0, the conditional probability can be directly given by a closed form solution:
Pπ(At = a|st, st+k, at+k) =	0,, aa 6== aatt++kk
A.4 Dependency Factor Estimation
We prove that the dependency factor can be learned in an inverse temporal difference manner in this
section, which is formulated by equation 14 in the main body:
P (at |st, st+k2 ) = Est+kι 〜Pπ (st+kι ∣st+k2 ,st)P (at |st, st+kι ), if k2 > k1 ≥ 1.
Proof.
ESt + kl ~Ρπ (st + kι ∣St + k2 ,St)P (at | st, st+kl )
Pπ (st+k1 |st+k2
S
, st)P (at|st, st+k1 )dst+k1
/ P"(St+ k1,st + k2 ,st) P πSt,st,st+kι) d
Js	Pπ (st + k2 ,st)	Pπ (st, st + kl )	St+k1
S
Pπ(at, st, st+kl)
Pπ(St+k2, st)
P (st+k2 |st+k1 , st )dst+k1
s
P∏ (at, st, st+kι )
Pπ (st+k2 ,st)
P (st+k2 |st+k1 )dst+k1
Pπ(at, st, st+k1, st+k2)
JS	Pn (st+k2 ,st) — dst+k1
P"(at, st, st+k2 )
Pπ (St+k2 , st)
P (at |st, st+k2 )
□
B	Temporal Difference Learning of Dependency Model
We use a mixture of temporal difference and Monte-Carlo training target to train dependency model.
For a trajectory sample T = {si, ai, ri}T=ι, We provide training target for Pφ(at∣st, st+k, k) for
every valid (t, k) pair, and use cross-entropy loss to update φ. For the Monte-carlo training target,
We directly use one-hot probability distribution of action at as the ground-truth; for the temporal
difference training target, we use distribution of Pφ(at∣st, st+k-ι, k - 1) as the ground-truth. We
mix the tWo training target With 0.85k Weight on Monte-Carlo target and 1 - 0.85k Weight on
temporal difference target to achieve the optimal bias-variance tradeoff.
14
Under review as a conference paper at ICLR 2020
Figure 5: Observation in Pixel Grid World.
C Experiment
C.1 Environment Settings
Finite-state MDP settings. We construct different 3-state MDPs with different transition proba-
bility and reward functions. We categorize transition probability settings into two types:
•	Connected settings, where there is high mutual reaching probability between every state
pair, causing the n-step transition probability to converge rapidly into stationary distribu-
tion. We use the following transition matrix for connected settings:
0.2 0.5 0.3
P = 0.5 0.3 0.2
0.2 0.2 0.6
•	Isolated settings, where there is low mutual reaching probability between some state pairs,
causing the n-step transition probability to converge slowly into stationary distribution. We
use the following transition matrix for isolated settings:
0.9	0.05 0.05
P = 0.45 0.1 0.45
0.05 0.05	0.9
In connected settings, the n-step transition probability is very close into stationary distribution, re-
sulting in a small dependency factor in future rewards. Thus, importance sampling estimator will
have lower variance in connected settings than in isolated settings.
We also categorize reward settings into two types:
•	High-variance settings, where the variance of Monte-Carlo return signal is large compared
to mean total return. We use the following reward function for high variance settings:
R=[3 2 1]
•	Low-variance settings, where the variance of Monte-Carlo return signal is small compared
to mean total return. We use the following reward function for low variance settings:
R= [2.1 2 1.9]
It is clear that Monte-Carlo advantage estimator will have larger variance in high-variance settings
than in low-variance settings.
Pixel Grid World environment. We build Pixel Grid World environment where observations are
provided by high-dimensional images. As shown in Figure 5, the blue square represents the position
of the agent and the yellow square represents the position of current goal. The size of grid world is
8 × 8, and observations are provided by 128 × 128 × 3 RGB pixels. There are 4 actions in Pixel
Grid World, making the agent to move toward 4 different directions. We use two reward settings in
experiments:
•	Per-step punishment setting, where agent gets r = -0.03 reward in every step before
reaching goal, and r = 0 reward in every step after reaching goal. Agent gets r = 1 reward
when reaching goal for the first time.
•	No punishment setting, where agent gets r = 1 reward when reaching goal for the first
time, and gets r = 0 reward otherwise.
15
Under review as a conference paper at ICLR 2020
Figure 6: Network architecture for dependency model and reward decomposition model.
To make the problem harder, the environment will do periodical reset multiple times in an episode,
by which the agent and the goal are randomly repositioned. The reset period is set to be 30 steps,
and an episode contains 5 rounds.
C.2 Hyper-parameter Settings and Network Architecture
We use γ = 0.999 for all experiments in policy optimization. For GAE estimator, we use the
default settings λ = 0.95 in all experiments. We use 8 parallel environments to gather data, use
the trajectory truncation length of 128, and train each truncated trajectory with 5 epochs. We use
a global Adam optimizer for training both dependency model and reward decomposition model,
and set learning rate to be 2.5 X 10-4. For initialization, We use orthogonal initialization with Vz2
scale for all layers, except that we initialize the last weight matrix of D to be zeros. For training
Pφ(at∣st, st+k, k), We weight the MC loss by 0.85k and TD loss by 1 — 0.85k, and use the mixture
loss to train dependency model. For PPO algorithm, we use the same hyper-parameter settings with
OpenAI Baseline.
We use the network architecture shown in Figure 6 for dependency model and reward decomposition
model. We use the same CNN feature extractor settings as the settings in the OpenAI Baseline.
For time embedding, we store embedding vector of 128 dimensions for each k ∈ {1, 2,…，128}
separately.
16
Under review as a conference paper at ICLR 2020
(b)
Figure 7: Case study on reward decomposition model. We fix a state-action pair (st , at) and visu-
alize reward decomposition for every future reward. The x-axis represents the interval from current
state st to future states st+k , green line represents the dependency factor of future states, blue and
orange lines are future rewards that assigned to Monte-Carlo estimator and importance sampling
estimator respectively, and red lines are the estimation by importance sampling estimator on each
future time. Black dashed lines represent periodical resets.
C.3 Case Study on Reward Decomposition Model
In Pixel Grid World environment, we fix a stochastic policy and train model rψ to minimize esti-
mation variance. When the model rψ is near convergence, we visualize reward decomposition for
every future reward after a state-action pair (st , at). These results are shown in Figure 7. It is clear
that all the independent rewards after reset are assigned to importance sampling estimator, which
is multiplied by a small dependency factor and barely contribute to advantage estimation. By con-
trast, rewards before reset are assigned to the Monte-Carlo estimator, which contribute to advantage
estimation.
17