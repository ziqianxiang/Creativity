Under review as a conference paper at ICLR 2020
Semi-Implicit Back Propagation
Anonymous authors
Paper under double-blind review
Ab stract
Neural network has attracted great attention for a long time and many researcher-
s are devoted to improve the effectiveness of neural network training algorithms.
Though stochastic gradient descent (SGD) and other explicit gradient-based meth-
ods are widely adopted, there are still many challenges such as gradient vanishing
and small step sizes, which leads to slow convergence and instability of SGD
algorithms. Motivated by error back propagation (BP) and proximal methods,
we propose a semi-implicit back propagation method for neural network train-
ing. Similar to BP, the difference on the neurons are propagated in a backward
fashion and the parameters are updated with proximal mapping. The implicit up-
date for both hidden neurons and parameters allows to choose large step size in
the training algorithm. Finally, we also show that any fixed point of convergent
sequences produced by this algorithm is a stationary point of the objective loss
function. The experiments on both MNIST and CIFAR-10 demonstrate that the
proposed semi-implicit BP algorithm leads to better performance in terms of both
loss decreasing and training/validation accuracy, compared to SGD and a similar
algorithm ProxBP.
1	Introduction
Along with the rapid development of computer hardware, neural network methods have achieved
enormous success in divers application fields, such as computer vision (Krizhevsky et al., 2012),
speech recognition (Hinton et al., 2012; Sainath et al., 2013), nature language process (Collobert
et al., 2011) and so on. The key ingredient of neuron network methods amounts to solve a highly
non-convex optimization problem. The most basic and popular algorithm is stochastic gradient
descent (SGD) (Robbins & Monro, 1951), especially in the form of ”error” back propagation (BP)
(Rumelhart et al., 1986) that leads to high efficiency for training deep neural networks. Since then
many variants of gradient based methods have been proposed, such as Adagrad(Duchi et al., 2011),
Nesterov momentum (Sutskever et al., 2013), Adam (Kingma & Ba, 2014) and AMSGrad (Reddi
et al., 2019). Recently extensive research are also dedicated to develop second-order algorithms, for
example NeWton method (Orr & Muller, 2003) and L-BFGS (Le et al., 2011).
It is well known that the convergence of explicit gradient descent type approaches require sufficient-
ly small step size. For example, for a loss function With Lipschitz continuous gradient, the stepsize
should be in the range of (0, 2/L) for L being the Lipschitz constant, Which is in general extremely
big for real datasets. Another difficulties in gradient descent approaches is to propagate the ”error”
deeply due to nonlinear activation functions, Which is commonly knoWn as gradient vanishing. To
overcome these problems, implicit updates are more attractive. In (Frerix et al., 2018), proximal
back propagation, namely ProxBP, Was proposed to utilize the proximal method for the Weight up-
dating. Alternative approach is to reformulate the training problem as a sequence of constrained
optimization by introducing the constraints on Weights and hidden neurons at each layer. Block co-
ordinate descent methods (Carreira-Perpinan & Wang, 2014; Zhang & Brand, 2017) Were proposed
and analyzed to solve this constrained formulation With square loss functions. Along this line, the
Alternating direction method of multipliers (ADMM) (Taylor et al., 2016; Zhang et al., 2016) Were
also proposed With extra dual variables updating.
Motivated by proposing implicit Weight updates to overcome small step sizes and vanishing gradient
problems in SGD, We propose a semi-implicit scheme, Which has similar form as ”error” back
propagation through neurons, While the parameters are updated through optimization at each layer. It
can be shoWn that any fixed point of the sequence generated by the scheme is a stationary point of the
1
Under review as a conference paper at ICLR 2020
objective loss function. In contrast to explicit gradient descent methods, the proposed method allows
to choose large step sizes and leads to a better training performance per epoch. The performance
is also stable with respect to the choice of stepsizes. Compared to the implicit method ProxBP, the
proposed scheme only updates the neurons after the activation and the error is updated in a more
implicit way, for which better training and validation performances are achieved in the experiments
on both MNIST and CIFAR-10.
2	Notations
Given input-output data pairs (X, Y ), we consider a N -layer feed-forward fullly connected neural
network as shown in Figure 1. Here, the parameters from the i-th layer to the (i + 1)-th layer are the
X	W-→	G2	-→	F2	W-→	G3	…GN-1	-→	FN-WN-→N-iFn
Figure 1: N-layer Neural Network
weight matrix Wi and bias bi , and σ is a non-linear activation function, such as sigmod or ReLU.
We denote the neuron vector at i-th layer before activation as Gi , and the neurons after activation as
Fi, i.e. F1 = X,
Gi+1 = Wi Fi + bi ;	(1)
Fi+1 = σ(Gi+1)
for i = 1,…，N - 1. We note that in general at the last layer, there is no non-linear activation
function and FN = GN . For ease of notation, we can use an activation function σ as identity. The
generic training model aims to solve the following minimization problem:
min J(θ; X, Y):= L(FN, Y)	(2)
θ
where θ denotes the collective parameter set {Wi, bi}iN=-11 and L is some loss function.
3	Semi-Implicit back-propagation method
In the following, we first present the classic back propagation (BP) algorithm for an easier introduc-
tion of the proposed semi-implicit method.
3.1	Back probation method
The widely used BP method Rumelhart et al. (1986) is based on gradient descent algorithm:
θk+ι = θk - ηd(θk； x,y)
η ∂θ
(3)
where η > 0 is the stepsize. The main idea of BP algorithm is to use an efficient error propagation
scheme on the hidden neurons for computing the partial derivatives of the network parameters at
each layer. The so-called ”error” signal 5n ≡ ∂J at the last level is propagated to the hidden
neurons δi ≡ J using the chain rule. In fact for a square loss function, the gradient at the last layer
Sn = dL∂FN,y) = FN — y is indeed an error. The propagation from δi+ι to δ% for i = N - 1, ∙ ∙ ∙ , 1
is then calculated as
δ ∂J _ ∂Gi+ι ∂J ∂J
i : ∂Fi = ∂Fi ∂Gi+ι ∂Fi+ι
WT(∂σ(Gi+ι)
i( ∂Gi+ι
δi+1).
(4)
2
Under review as a conference paper at ICLR 2020
And the partial derivative to Wi can be computed as
∂J =∂Gi+ι ∂Fi+ι ∂J	∂σ(Gi+ι)	T
∂Wi := ^∂W^∂Gi+1 ∂F+ =( ∂Gi+ι 0 i+1) i
(5)
At k-th iteration, after a forward update of the neurons Fik by (1) using the current parameters sets
{Wk, bkk},i = 1,…，N — 1, we can compute the ”error” signal at each neurons sequentially from
the i + 1 level to i level by (4) and the parameters Wik+1 is updated according to the gradient at the
point Wik computed by (5).
3.2	Semi-implicit updates
Compared to the BP method, we propose to update the hidden neurons and the parameters sets at
each layer in an implicit way.
At the iteration k, given the current estimate θk : {Wik, bik}, we first update the neuron Fik and Gik
in a feedforward fashion as BP method, by using (1) for i = 1,…，N - 1. For the backward stage,
we start with updating neuron FN at the the last layer using the gradient descent:
δN = dL(FN，Y), FN 1 = FN - ηδN.	(6)
.	k+ k	k .	Lk+1 .	Tr7 T	IlF	1 .	ICl1	.
For i = N 一 1, ∙∙∙ , 1, given 工+J, the parameters Wi, b are updated by solving the following
optimization problem (once)
Wik+1 = argminkσ(WiFik + b) - Fk+2 kF + 3口用 一 WikkF
Wi	2
bk+1 = argminkσ(Wik+1Fik + bi) -工臂 kF + 21∣% - bfkF
bi	2
(7)
where λ > 0 is a parameter that is corresponding to stepsize. This update of parameters is related
to using an implicit gradient based on so-called proximal mapping. Taking Wi as an example, the
optimality in (7) gives
Wik+1 = Wik - 1 Vf(Wik+1)
λ
Wheref(Wi) = kσ(WiFk + b) -Fi%2 k2F. Compared to a direct gradient descent step, this update
is unconditionally stable for any stepsize 1∕λ. We note that proximal mapping was previously
proposed for training neural network as ProxBP in (Frerix et al., 2018). However the update of
the parameter sets is different as ProxBP uses Gi+1 for the data fitting at each layer. The two
subproblems at each layer can be solved by a nonlinear conjugate gradient method.
After the update of Wik+1 and bik+1, we need to update the hidden neuron Fi. As classical BP, we
first consider the gradient at Fik as
∂J _ ∂Fi+ι ∂J	∂Gk+ι ∂Fi+ι ∂J
----；------7---7-- - ----7-----7-----7-.
∂Fk	∂Fik ∂Fi+ι	∂Fik ∂Gk+ι ∂F+ι
(8)
It can be seen that the partial derivative d∂F+1 := Wk. Different from BP and ProxBP, we use the
newly updated Wik+1 instead of Wik to compute the error:
δk := (Wik+1)τ ∙ (∂σ(Glk+ι) Θ Si+1), Fk+1 = Fik -涧	(9)
By this formula, the difference can be propagated from the level N to 1. At the last level i = 1,
we only need to update W1k+1 and b1k+1 as F1 = X. The overall semi-implicit back propagation
method is summarized in Algorithm 1. For large scale training problem, the back propagation is
used in the form of stochastic gradient descent (SGD) using a small set of samples. The proposed
semi-implicit method can be easily extended to stochastic version by replacing (X, Y) by a batch
set (Xmini, Ymini) at each iteration in Algorithm 1.
3
Under review as a conference paper at ICLR 2020
Algorithm 1 Semi-implicit back propagation
Input: Current parameters θk = {Wik , bik }
// Forward pass
F1 = X
for i = 1 to N - 1 do
Gik+1 = Wik Fik + bi
Fik+1 = σ(Gik+1)
end for
// Update on the hidden neurons and the parameters
Update on FNk
for i = N - 1 to 2 do
Implicit update on Wik , bik
Error propagation and update on Fk → Fk+ 2
end for
Implicit update on W1k , b1k
Output: New parameters θk+1 = {Wik+1 , bik+1}
3.3	Fixed points of Semi-implicit method
The follow proposition indicates that any fixed point of the iteration is a stationary point of the
objective energy function.
Proposition 1 Assume that L and the activation functions σ are continuously differentiable. If
θk k→∞ θ* for θ* = {Wit, b } ,then θ* is a stationary point of the energy function J (θ; X, Y).
Proof Due to the forward update, { Wik, bkk } k→∞ { Wi*, b*} infers that { Fik, Gk } k→∞ { F* ,G*
where G* = W* F*_r + b* and G* = σ(F* for i = 1,…，N. At the last layer, the neuron FN is
updated with gradient descent:
Take a limit, we have
Now we show
FN2 = FN - η"∂⅛Y)
∂FN
lim FN 22
k→∞
lim FNk - η
k→∞ N
* ∂J
FN - η∂F*
∂FN
∂L(Fn,Y )
∂FN
klim Fr 2 = F*-η∂J*;
→∞	i
∂J _
∂W* =0
(10)
(11)
(12)
(13)
for i = N, •…，2 using mathematical induction. The first equation is shown for i = N .By the
optimality of equation 7, we have
2(Wk+1 - Wik) = [∂σ(Wk+1Fk + b) Θ (F^ - σ(Wk+1Fk + W))](Fik)T	(14)
4
Under review as a conference paper at ICLR 2020
Let k → ∞, we obtain
0 = [∂σ(Wi*F* + 增 Θ (F+ι - ηdF^ - σ(Wi*F* + b")](F*)T
∂Fi+1
∂J
=[∂σ(G↑+ι) Θ (-ηdF~~)](F*)T
∂ Fi+1
∂J
=-η[∂σ(G*+ι) Θ d-](F*)T
∂Fi+1
=_ ∂G+1 ∂F+1 J
=η AW,* ∂G*+ι ∂Fi+ι
∂J
=-η∂W**
It is easy to see that the limit of Fk+ 2 for i = N 一 1, ∙ ∙ ∙ , 1 is:
(15)
lim Fik+2
k→∞
lim Fik - (Wik+1)T [∂σ(Gik+1) Θ (Fik+1
k→∞
F*- η(Wi*)T [∂σ(G*+ι) Θ dF*~ ]
∂Fi+1
∂Gi+ι ∂Fi+1 J
i η ∂Fi* ∂G*+ι ∂Fi+ι
* ∂J
F, - η∂F*
一 Fk+2)]
(16)
With mathematical induction We obtain that ∂∂J = 0.

4	Numerical experiments
In this section, We Will compare the performance of BP, ProxBP Frerix et al. (2018) and the pro-
posed semi-implicit BP using MNIST and CIFAT-10 datasets. All the experiments are performed on
MATLAB With a 12-core CPU and the same netWork setting and initializations are used for a fair
comparison. We use softmax cross-entropy for the loss function L and ReLU for the activation func-
tion, as usually chosen in classification problems. For the linear CG used in ProxBP and nonlinear
CG in semi-implicit BP, the iterations number is set as 5. Finally the Weights and bias are initialized
by normal distribution With average 0 and standard deviation 0.01.
In MNIST experiments, the training set contains 55000 samples and the rest are included in the
validation set. For CIFAR-10, a set of 45000 samples is used as training set and the rest as valida-
tion set. The training process are performed 5 times, and Table 1 shoWs the average training and
validation accuracy achieved by SGD, Semi-implicit BP With different learning rates η (for SGD)
and 1∕λ for semi-implicit BP on MNIST. It can be seen that after 2 epoch, semi-implicit BP method
already achieves high accuracy as high as 99% and the performance is very stable With respect to
different stepsize choices, While SGD fails for some choices of stepsize η. For ProxBP, We present
the results With η = 1 as the best performance is achieved With this set of parameters. The highest
accuracies are marked in bold in each column, and We can see that semi-implicit BP achieves the
highest training and test accuracy than BP and ProxBP.
In Figure 2 and 3, We shoW the performance of the three methods per epoch for MNIST and CIFAR-
10. For the CIFAR-10 experiment, We train a 3072 × 2000 × 500 × 10 neural netWork and the rest
settings are the same as MNIST dataset experiments. For both datasets, We choose the step size
η = 0.1 for semi-implicit BP and ProxBP, While a smaller one to guarantee SGD achieves a best
better performance. The evolution of training loss and training accuracy shoWs that the performance
of ProxBP is not as good as SGD for MNIST While it is better than SGD on CIFAR-10. For both
experiments, the performance of semi-implicit BP method leads to the fastest convergence. The im-
provement on the validation accuracy also demonstrates that the proposed semi-implicit BP method
also generalize Well in a comparison to the other tWo methods.
5
Under review as a conference paper at ICLR 2020
MNIST(784 X 500 X 10厂	Training/validation accuracy				
learning rates	100	10	1	0.1	0.01
SGD, η	0.0985	0.1123	0.0980	0.9487	0.8885
	0.1002	0.1126	0.0924	0.9492	0.8988
ProXBP (λ = 1,η)	0.9239	0.9349	0.9390	0.9032	0.8415
	0.9344	0.9374	0.9494	0.9244	0.8832
ProXBP (η = 1,1∕λ)	0.9420 0.9486	0.9444 0.9554	0.9383 0.9494	0.9171 0.9344	0.8743 0.9064
Semi-implicit (λ = 1,η)	0.9780	0.9801	0.9904	0.9737	0.9104
	0.9710	0.9724	0.9800	0.9748	0.9338
Semi-implicit	0.9765	0.9752	0.9735	0.9598	0.9206
(η = 0.1, ι∕λ)		0.9736	0.9778	0.9738	0.9672	0.9394
Table 1: Training and validation accuracy with different step sizes for 2 epoch.
Figure 2: MNIST. Batch size 100
Sso-J6u⊂raJJ.
Figure 3: CIFAR-10. Batch size 100
6
Under review as a conference paper at ICLR 2020
5	Conclusion
We proposed a novel optimization scheme in order to overcome the difficulties of small stepsize and
vanishing gradient in training neural networks. The computation of new scheme is in the spirit of
error back propagation, with an implicit updates on the parameters sets and semi-implicit updates
on the hidden neurons. The experiments on both MNIST and CIFAR-10 show that the proposed
semi-implicit back propagation has better performance per epoch compared to SGD and ProxBP.
It is demonstrated in the experiment that larger step sizes can be adopted without losing stability
and performance. It can be also seend that the proposed scheme is flexible and some regularization
can be easily integrated if needed. The fixed points of the scheme are shown to be stationary points
of the objective loss function and further rigorous theoretical convergence will be explored in an
ongoing work.
References
Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In
Artificial Intelligence and Statistics, pp.10-19, 2014.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuk-
sa. Natural language processing (almost) from scratch. Journal of Machine Learning Research,
12(1):2493-2537, 2011.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7):257-269, 2011.
T. Frerix, T. Mollenhoff, M. Moeller, and D. Cremers. Proximal backpropagation. In International
Conference on Learning Representations (ICLR), 2018.
Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel Rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, and Tara N. Sainath. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine, 29(6):82-97, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In International Conference on Neural Information Processing Systems,
2012.
Quoc V Le, Jiquan Ngiam, Adam Coates, Abhik Lahiri, Bobby Prochnow, and Andrew Y Ng. On
optimization methods for deep learning. In Proceedings of the 28th International Conference on
International Conference on Machine Learning, pp. 265-272. Omnipress, 2011.
Genevieve B Orr and KIaUs-Robert Muller. Neural networks: tricks of the trade. Springer, 2003.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical
Statistics, 22(3):400-407, 1951.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature, 323(3):533-536, 1986.
Tara N. Sainath, A. R. Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. Deep convolutional
neural networks for lvcsr. In IEEE International Conference on Acoustics, 2013.
I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
in deep learning. In International Conference on International Conference on Machine Learning,
2013.
7
Under review as a conference paper at ICLR 2020
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable admm approach. In International conference on
machine learning,pp. 2722-2731, 2016.
Ziming Zhang and Matthew Brand. Convergent block coordinate descent for training tikhonov
regularized deep neural networks. In Advances in Neural Information Processing Systems, pp.
1721-1730, 2017.
Ziming Zhang, Yuting Chen, and Venkatesh Saligrama. Efficient training of very deep neural net-
works for supervised hashing. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1487-1495, 2016.
8