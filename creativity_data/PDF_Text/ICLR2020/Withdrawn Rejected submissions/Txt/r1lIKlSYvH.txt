Under review as a conference paper at ICLR 2020
The Usual Suspects ? Reassessing Blame for
VAE Posterior Collapse
Anonymous authors
Paper under double-blind review
Ab stract
In narrow asymptotic settings Gaussian VAE models of continuous data have been
shown to possess global optima aligned with ground-truth distributions. Even so,
it is well known that poor solutions whereby the latent posterior collapses to an
uninformative prior are sometimes obtained in practice. However, contrary to
conventional wisdom that largely assigns blame for this phenomena on the undue
influence of KL-divergence regularization, we will argue that posterior collapse
is, at least in part, a direct consequence of bad local minima inherent to the loss
surface of deep autoencoder networks. In particular, we prove that even small
nonlinear perturbations of affine VAE decoder models can produce such minima,
and in deeper models, analogous minima can force the VAE to behave like an
aggressive truncation operator, provably discarding information along all latent
dimensions in certain circumstances. Regardless, the underlying message here
is not meant to undercut valuable existing explanations of posterior collapse, but
rather, to refine the discussion and elucidate alternative risk factors that may have
been previously underappreciated.
1 Introduction
The variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) represents a
powerful generative model of data points that are assumed to possess some complex yet unknown
latent structure. This assumption is instantiated via the marginalized distribution
Pθ (X) = p Pθ (x∣z)p(z)dz,
(1)
which forms the basis of prevailing VAE models. Here z ∈ Rκ is a collection of unobservable
latent factors of variation that, when drawn from the prior p(z), are colloquially said to generate an
observed data point X ∈ Rd through the conditional distribution pθ(x|z). The latter is controlled
by parameters θ that can, at least conceptually speaking, be optimized by maximum likelihood over
pθ(X) given available training examples.
In particular, assuming n training points X = [X(1), . . . , X(n)], maximum likelihood estimation is
tantamount to minimizing the negative log-likelihood expression § Pi - log [pθ (x(i))]. Proceed-
ing further, because the marginalization over z in (1) is often intractable, the VAE instead minimizes
a convenient variational upper bound given by L(θ, φ) ,
nn
n1 X {-Eqφ(z∣X(i)) [log Pθ (X(i)|z) ] + KL%φ(zlx ⑴ ||p(z)]} ≥ 1 X - log [pθ (XCi))卜
i=1
i=1
(2)
with equality iff qφ(z∣x(i)) = pθ (z|X(i)) for all i. The additional parameters φ govern the shape
of the variational distribution qφ(z∣x) that is designed to approximate the true but often intractable
latent posterior pθ(z|x).
The VAE energy from (2) is composed of two terms, a data-fitting loss that borrows the basic struc-
ture of an autoencoder (AE), and a KL-divergence-based regularization factor. The former incen-
tivizes assigning high probability to latent codes z that facilitate accurate reconstructions of each
x(i). In fact, if qφ(z∣x) is a Dirac delta function, this term is exactly equivalent to a deterministic
AE with data reconstruction loss defined by - log pθ (X|z). Overall, it is because of this association
that qφ(z∣x) is generally referred to as the encoder distribution, while pθ (x|z) denotes the decoder
1
Under review as a conference paper at ICLR 2020
distribution. Additionally, the KL regularize] KL [qφ(z∣x)∣∣p(z)] pushes the encoder distribution
towards the prior without violating the variational bound.
For continuous data, which will be our primary focus herein, it is typical to assume that
p(z) = N(z∣0, I), pθ (x|z) = N(χ∣μχ,YI), and qφ (z|x) = N(z|〃z, ∑z),	⑶
where γ > 0 is a scalar variance parameter, while the Gaussian moments 模度 ≡ 模化(z; θ), μ% ≡
μz (x; φ), and Σzz ≡ diag[σz (x; φ)]2 are computed via feedforward neural network layers. The
encoder network parameterized by φ takes X as an input and outputs μ% and ∑z. Similarly the
decoder network parameterized by θ converts a latent code Z into μχ. Given these assumptions, the
generic VAE objective from (2) can be refined to
n
L(θ,φ) = 1X{%(z∣χ(i)) [1 kχ(i)-μχ(z;θ)k2]
i=1
+ dlog γ + σz x(i); φ
(4)
22,
2
2
ɔ ∣∣2 - log diag [σz (X⑴;φ)] + 帆(x(i); φ)∣∣
excluding an inconsequential factor of 1/2. This expression can be optimized over using SGD and
a simple reparameterization strategy (Kingma & Welling, 2014; Rezende et al., 2014) to produce
parameter estimates {θ*,φ*}. Among other things, new samples approximating the training data
can then be generated via the ancestral process Znew 〜N(z|0, I) and Xnew 〜pθ* (x∣znew).
Although it has been argued that global minima of (4) may correspond with the optimal recovery
of ground truth distributions in certain asymptotic settings (Dai & Wipf, 2019), it is well known
that in practice, VAE models are at risk of converging to degenerate solutions where, for example,
it may be that qφ (Z|X) = p(Z). This phenomena, commonly referred to as VAE posterior collapse
(He et al., 2019; Razavi et al., 2019), has been acknowledged and analyzed from a variety of dif-
ferent perspectives as we detail in Section 2. That being said, we would argue that there remains
lingering ambiguity regarding the different types and respective causes of posterior collapse. Con-
sequently, Section 3 provides a useful taxonomy that will serve to contextualize our main technical
contributions. These include the following:
•	Building upon existing analysis of affine VAE decoder models, in Section 4 we prove that even
arbitrarily small nonlinear activations can introduce suboptimal local minima exhibiting posterior
collapse.
•	We demonstrate in Section 5 that if the encoder/decoder networks are incapable of sufficiently
reducing the VAE reconstruction errors, even in a deterministic setting with no KL-divergence
regularizer, there will exist an implicit lower bound on the optimal value of γ . Moreover, we
prove that if this γ is sufficiently large, the VAE will behave like an aggressive thresholding
operator, enforcing exact posterior collapse, i.e., qφ (Z|X) = p(Z).
•	Based on these observations, we present experiments in Section 6 establishing that as network
depth/capacity is increased, even for deterministic AE models with no regularization, reconstruc-
tion errors become worse. This bounds the effective VAE trade-off parameter γ such that posterior
collapse is essentially inevitable. Collectively then, we provide convincing evidence that poste-
rior collapse is, at least in certain settings, the fault of deep AE local minima, and need not be
exclusively a consequence of usual suspects such as the KL-divergence term.
We conclude in Section 7 with practical take-home messages, and motivate the search for improved
AE architectures and training regimes that might be leveraged by analogous VAE models.
2	Recent Work and the Usual Suspects for Instigating Collapse
Posterior collapse under various guises is one of the most frequently addressed topics related to
VAE performance. Depending on the context, arguably the most common and seemingly trans-
parent suspect for causing collapse is the KL regularization factor that is obviously minimized by
qφ(z|x) = p(z). This perception has inspired various countermeasures, including heuristic anneal-
ing of the KL penalty or KL warm-start (Bowman et al., 2015; Huang et al., 2018; S0nderby et al.,
2016), tighter bounds on the log-likelihood (Burda et al., 2015; Rezende & Mohamed, 2015), more
2
Under review as a conference paper at ICLR 2020
complex priors (Bauer & Mnih, 2018; Tomczak & Welling, 2018), modified decoder architectures
(Cai et al., 2017; Dieng et al., 2018; Yeung et al., 2017), or efforts to explicitly disallow the prior
from ever equaling the variational distribution (Razavi et al., 2019). Thus far though, most published
results do not indicate success generating high-resolution images, and in the majority of cases, eval-
uations are limited to small images and/or relatively shallow networks. This suggests that there may
be more nuance involved in pinpointing the causes and potential remedies of posterior collapse. One
notable exception though is the BIVA model from (Maal0e et al., 2019), which employs a bidirec-
tional hierarchy of latent variables, in part to combat posterior collapse. While improvements in
NLL scores have been demonstrated with BIVA using relatively deep encoder/decoders, this model
is significantly more complex and difficult to analyze.
On the analysis side, there have been various efforts to explicitly characterize posterior collapse in
restricted settings. For example, Lucas et al. (2019) demonstrate that if γ is fixed to a sufficiently
large value, then a VAE energy function with an affine decoder mean will have minima that over-
prune latent dimensions. A related linearized approximation to the VAE objective is analyzed in
(Rolinek et al., 2019); however, collapsed latent dimensions are excluded and it remains somewhat
unclear how the surrogate objective relates to the original. Posterior collapse has also been associ-
ated with data-dependent decoder covariance networks Σx(z; θ) 6= γI (Mattei & Frellsen, 2018),
which allows for degenerate solutions assigning infinite density to a single data point and a diffuse,
collapsed density everywhere else. Finally, from the perspective of training dynamics, (He et al.,
2019) argue that a lagging inference network can also lead to posterior collapse.
3	Taxonomy of Posterior Collapse
Although there is now a vast literature on the various potential causes of posterior collapse, there
remains ambiguity as to exactly what this phenomena is referring to. In this regard, we believe that
it is critical to differentiate five subtle yet quite distinct scenarios that could reasonably fall under
the generic rubric of posterior collapse:
(i)	Latent dimensions of z that are not needed for providing good reconstructions of the training
data are set to the prior, meaning qφ(zj |x) ≈ p(zj) = N(0,1) at any superfluous dimension
j. Along other dimensions σZ will be near zero and μ% will provide a usable predictive signal
leading to accurate reconstructions of the training data. This case can actually be viewed as
a desirable form of selective posterior collapse that, as argued in (Dai & Wipf, 2019), is a
necessary (albeit not sufficient) condition for generating good samples.
(ii)	The decoder variance γ is not learned but fixed to a large value1 such that the KL term from
(2) is overly dominant, forcing most or all dimensions of z to follow the prior N(0, 1). In
this scenario, the actual global optimum of the VAE energy (conditioned on γ being fixed)
will lead to deleterious posterior collapse and the model reconstructions of the training data
will be poor. In fact, even the original marginal log-likelihood can potentially default to a
trivial/useless solution if γ is fixed too large, assigning a small marginal likelihood to the
training data, provably so in the affine case (Lucas et al., 2019).
(iii)	As mentioned previously, if the Gaussian decoder covariance is learned as a separate network
structure (instead of simply Σx (z; θ) = γI), there can exist degenerate solutions that assign
infinite density to a single data point and a diffuse, isotropic Gaussian elsewhere (Mattei &
Frellsen, 2018). This implies that (4) can be unbounded from below at what amounts to a
posterior collapsed solution and bad reconstructions almost everywhere.
(iv)	When powerful non-Gaussian decoders are used, and in particular those that can parameter-
ize complex distributions regardless of the value of z (e.g., PixelCNN-based (Van den Oord
et al., 2016)), it is possible for the VAE to assign high-probability to the training data even if
qφ(z∣x) = p(z) (Alemi et al., 2017; Bowman et al., 2015; Chen et al., 2016). This category
of posterior collapse is quite distinct from categories (ii) and (iii) above in that, although the
reconstructions are similarly poor, the associated NLL scores can still be good.
(v)	The previous four categories of posterior collapse can all be directly associated with emergent
properties of the VAE global minimum under various modeling conditions. In contrast, a
fifth type of collapse exists that is the explicit progeny of bad VAE local minima. More
1Or equivalently, a KL scaling parameter such as used by the β-VAE (Higgins et al., 2017) is set too large.
3
Under review as a conference paper at ICLR 2020
specifically, as we will argue shortly, when deeper encoder/decoder networks are used, the
risk of converging to bad, overregularized solutions increases.
The remainder of this paper will primarily focus on category (v), with brief mention of the other
types for comparison purposes where appropriate. Our rationale for this selection bias is that, un-
like the others, category (i) collapse is actually advantageous and hence need not be mitigated. In
contrast, while category (ii) is undesirable, it be can be avoided by learning γ . As for category (iii),
this represents an unavoidable consequence of models with flexible decoder covariances capable of
detecting outliers (Dai et al., 2019). In fact, even simpler inlier/outlier decomposition models such
as robust PCA are inevitably at risk for this phenomena (Candes et al., 2011). Regardless, when
Σz (x; θ) = γI this problem goes away. And finally, we do not address category (iv) in depth sim-
ply because it is unrelated to the canonical Gaussian VAE models of continuous data that we have
chosen to examine herein. Regardless, it is still worthwhile to explicitly differentiate these five types
and bare them in mind when considering attempts to both explain and improve VAE models.
4	Insights from Simplified Cases
Because different categories of posterior collapse can be impacted by different global/local minima
structures, a useful starting point is a restricted setting whereby we can comprehensively characterize
all such minima. For this purpose, we first consider a VAE model with the decoder network set to
an affine function. As is often assumed in practice, we choose Σx = γI, where γ > 0 is a scalar
parameter within the parameter set θ. In contrast, for the mean function We choose 模化= WXz + bχ
for some weight matrix Wx and bias vector bx . The encoder can be arbitrarily complex (although
the optimal structure can be shown to be affine as well).
Given these simplifications, and assuming the training data has r ≥ κ nonzero singular values, it
has been demonstrated that at any global optima, the columns of Wx will correspond with the first
κ principal components of X provided that we simultaneously learn γ or set it to the optimal value
(which is available in closed form) (Dai et al., 2019; Lucas et al., 2019; Tipping & Bishop, 1999).
Additionally, it has also be shown that no spurious, suboptimal local minima will exist. Note also
that if r < κ the same basic conclusions still apply; however, Wx will only have r nonzero columns,
each corresponding with a different principal component of the data. The unused latent dimensions
will satisfy qφ(z∣x) = N(0, I), which represents the canonical form of the benign category (i)
posterior collapse. Collectively, these results imply that if we converge to any local minima of
the VAE energy, we will obtain the best possible linear approximation to the data using a minimal
number of latent dimensions, and malignant posterior collapse is not an issue, i.e., categories (ii)-(v)
will not arise.
Even so, if instead of learning γ, we choose a fixed value that is larger than any of the significant
singular values of XX>, then category (ii) posterior collapse can be inadvertently introduced. More
specifically, let rγ denote the number of such singular values that are smaller than some fixed Y
value. Then along K - Eγ latent dimensions qφ(z∣x) = N(0, I), and the corresponding columns of
Wx will be set to zero at the global optima (conditioned on this fixed γ), regardless of whether or
not these dimensions are necessary for accurately reconstructing the data. And it has been argued
that the risk of this type of posterior collapse at a conditionally-optimal global minimum will likely
be inherited by deeper models as well (Lucas et al., 2019), although learning γ can ameliorate this
problem.
Of course when we move to more complex architectures, the risk of bad local minima or other
suboptimal stationary points becomes a new potential concern, and it is not clear that the affine
case described above contributes to reliable, predictive intuitions. To illustrate this point, we will
now demonstrate that the introduction ofan arbitrarily small nonlinearity can nonetheless produce a
pernicious local minimum that exhibits category (v) posterior collapse. For this purpose, we assume
the decoder mean function
μχ = πα (WXz) + bx, with πα(u) , sign(u) (|u| - α)+ , α ≥ 0.	(5)
The function πα is nothing more than a soft-threshold operator as is commonly used in neural net-
work architectures designed to reflect unfolded iterative algorithms for representation learning (Gre-
gor & LeCun, 2010; Sprechmann et al., 2015). In the present context though, we choose this non-
linearity largely because it allows (5) to reflect arbitrarily small perturbations away from a strictly
4
Under review as a conference paper at ICLR 2020
affine model, and indeed if α = 0 the exact affine model is recovered. Collectively, these specifica-
tions lead to the parameterization θ = { Wχ, bχ, γ} and φ = {μZi), σZi)}n=ι and energy (excluding
irrelevant scale factors and constants) given by
n2
L(θ,φ) = X Eqφ(z∣χ(i)) IIpi)-∏α (W xz)-bxll2	(6)
i=1	2
+d log Y+∣∣σzi)∣∣2 - log diag hσzi)i 2 + ∣∣μzi)∣∣2
where μZi) and σZi) denote arbitrary encoder moments for data point i (this is consistent with the
assumption ofan arbitrarily complex encoder as used in previous analysis of affine decoder models).
Now define Y，煮 Pi ∣∣x(i) - Xk2, With X，ɪ Pi x(i). We then have the following result:
Proposition 4.1 For any α > 0, there will always exist data sets X such that (6) has a global
minimum that perfectly reconstructs the training data, but also a bad local minimum characterized
by
qφ(z∣x) = N(z∣0,I) and pθ(X)= N(x|x, γI).	(7)
Hence the moment we allow for nonlinear (or more precisely, non-affine) decoders there can exist
a poor local minimum, across all parameters including a learnable γ, that exhibits category (v)
posterior collapse.2 In other words, no predictive information about X passes through the latent
space, and a useless/non-informative distribution pθ(X) emerges that is incapable of assigning high
probability to the data (except obviously in the trivial degenerate case where all the data points are
equal to the empirical mean XY). We will next investigate the degree to which such concerns can
influence behavior in arbitrarily deep architectures.
5	Extrapolating to Practical Deep Architectures
Previously we have demonstrated the possibility of local minima aligned with category (v) posterior
collapse the moment we allow for decoders that deviate ever so slightly from an affine model. But
nuanced counterexamples designed for proving technical results notwithstanding, it is reasonable to
examine what realistic factors are largely responsible for leading optimization trajectories towards
such potential bad local solutions. For example, is it merely the strength of the KL regularization
term, and if so, why can we not just use KL warm-start to navigate around such points? In this section
we will elucidate a deceptively simple, alternative risk factor that will be corroborated empirically
in Section 6.
From the outset, we should mention that with deep encoder/decoder architectures commonly used in
practice, a stationary point can more-or-less always exist at solutions exhibiting posterior collapse.
As a representative and ubiquitous example, please see Appendix A.4. But of course without further
details, this type of stationary point could conceivably manifest as a saddle point (stable or unstable),
a local maximum, or a local minimum. For the strictly affine decoder model mentioned in Section
4, there will only be a harmless unstable saddle point at any collapsed solution (the Hessian has
negative eigenvalues). In contrast, for the special nonlinear case elucidated via Proposition 4.1 we
can instead have a bad local minima. We will now argue that as the depth of common feedforward
architectures increases, the risk of converging to category (v)-like solutions with most or all latent
dimensions stuck at bad stationary points can also increase.
Somewhat orthogonal to existing explanations of posterior collapse, our basis for this argument is
not directly related to the VAE KL-divergence term. Instead, we consider a deceptively simple
yet potentially influential alternative: Unregularized, deterministic AE models can have bad local
solutions with high reconstruction errors when sufficiently deep. This in turn can directly translate
to category (v) posterior collapse when training a corresponding VAE model with a matching deep
architecture. Moreover, to the extent that this is true, KL warm-start or related countermeasures
2This result mirrors related efforts examining linear DNNs, where it has been previously demonstrated that
under certain conditions, all local minima are globally optimal (Kawaguchi, 2016), while small nonlinearities
can induce bad local optima (Yun et al., 2019). However, the loss surface of these models is completely different
from a VAE, and hence we view Proposition 4.1 as a complementary result.
5
Under review as a conference paper at ICLR 2020
will likely be ineffective in avoiding such suboptimal minima. We will next examine these claims in
greater depth followed by a discussion of practical implications.
5.1	From Deeper Architectures to Inevitable Posterior Collapse
Consider the deterministic AE model formed by composing the encoder mean μχ ≡ μχ (∙; θ)
and decoder mean μ% ≡ μ% (∙; φ) networks from a VAE model, i.e., reconstructions X are com-
puted via X = 模境[μ2 (x; φ); θ]. We then train this AE to minimize the squared-error loss
nd Pn=IllX⑶ -X(i)∣∣ , producing parameters {θae,φae}. Analogously, the corresponding VAE
trained to minimize (4) arrives at a parameter set denoted {θvae, φvae}. In this scenario, it will
typically follow that
n	2n
ndX ∣∣x(i) - μχ [μz (χ(i);φae);θae] ∣∣2 ≤ ndX%vae(zix(i))[kx(" - μχ(Z;θVae) ∣∣2]，
(8)
meaning that the deterministic AE reconstruction error will generally be smaller than the stochastic
VAE version. Note that if σ z2 → 0, the VAE defaults to the same deterministic encoder as the AE
and hence will have identical representational capacity; however, the KL regularization prevents
this from happening, and any σz2 > 0 can only make the reconstructions worse.3 Likewise, the
KL penalty factor ∣∣μ∣k2 can further restrict the effective capacity and increase the reconstruction
error of the training data. Beyond these intuitive arguments, we have never empirically found a case
where (8) does not hold (see Section 6 for examples).
We next define the set
Sε ,卜,φ : nd X∣∣x(i)- X (i)∣∣2 ≤ ε}	(9)
for any > 0. Now suppose that the chosen encoder/decoder architecture is such that with
high probability, achievable optimization trajectories (e.g., via SGD or related) lead to parameters
{θae, φae} ∈/ Sε, i.e., Prob ({θae, φae} ∈ Sε) ≈ 0. It then follows that the optimal VAE noise vari-
ance denoted γ*, when conditioned on PraCtiCally-achievable values for other network parameters,
will satisfy
n
γ* = nd X Eqφvae (z∣X(i)) [kX(i)- μx (Z； θVae) II2] ≥ ε	(IO)
i=1
The equality in (10) can be confirmed by simply differentiating the VAE cost w.r.t. γ and equating
to zero, while the inequality comes from (8) and the fact that {θae, φae} ∈/ Sε .
From inspection of the VAE energy from (4), it is readily apparent that larger values of γ will
discount the data-fitting term and therefore place greater emphasis on the KL divergence. Since the
latter is minimized when the latent posterior equals the prior, we might expect that whenever ε and
therefore Y* is increased per (10), We are at a greater risk of nearing collapsed solutions. But the
nature of this approach is not at all transparent, and yet this subtlety has important implications for
understanding the VAE loss surface in regions at risk of posterior collapse.
For example, one plausible hypothesis is that only as γ* → ∞ do we risk full category (v) collapse.
If this were the case, we might have less cause for alarm since the reconstruction error and by
association γ* will typically be bounded from above at any local minimizer. However, we will now
demonstrate that even finite values can exactly collapse the posterior. In formally showing this, it is
helpful to introduce a slightly narrower but nonetheless representative class of VAE models.
Specifically, let f (μz ,σ ,θ, X⑴)，Eqφ(z^(i)) [∣X(i) - 〃x (z； θ) ∣∣2], i.e., the VAE data
term evaluated at a single data point without the 1∕γ scale factor. We then define a well-
behaved VAE as a model with energy function (4) designed such that Vμzf (μ, σ%, θ, X(i)) and
Vσz f (μz ,σz ,θ, X⑺) are Lipschitz continuous gradients for all i. Furthermore, we specify a non-
degenerate decoder as any μχ(z; θ = θ) with θ set to a 京 value such that Vσzf μzz, σ%, G, X(i)) ≥
3Except potentially in certain contrived adversarial conditions that do not represent practical regimes.
6
Under review as a conference paper at ICLR 2020
c for some constant c > 0 that can be arbitrarily small. This ensures that f is an increasing func-
tion of σz , a quite natural stipulation given that increasing the encoder variance will generally only
serve to corrupt the reconstruction, unless of course the decoder is completely blocking the signal
from the encoder. In the latter degenerate situation, it would follow that Vμzf ^μz, σz, θ, x(i))=
Vσzf μz,, σz, θ, x(i)) = 0, which is more-or-less tantamount to category (V) posterior collapse.
Based on these definitions, we can now present the following:
Proposition 5.1 For any well-behaved VAE With arbitrary, non-degenerate decoder μχ(z; θ = θ),
there Will always exist a γ < ∞ such that the trivial solution μχ(z; θ = θ) = X and q®(z|x)=
p(z ) will have lower cost.
Around any evaluation point, the sufficient condition we applied to demonstrate posterior collapse
(see proof details) can also be achieved with some γ00 < γ0 if we allow for partial collapse, i.e.,
q@* (Zj |x) = p(zj) along some but not all latent dimensions j ∈ {1,..., κ}. Overall, the analysis
loosely suggests that the number of dimensions vulnerable to exact collapse will increase monoton-
ically with γ .
Proposition 5.1 also provides evidence that the VAE behaves like a strict thresholding operator, com-
pletely shutting off latent dimensions using a finite value for γ. This is analogous to the distinction
between using the `1 versus `2 norm for solving regularized regression problems of the standard
form minu kx - Auk22 + γη(u), where A is a design matrix and η is a penalty function. When η
is the `1 norm, some or all elements of u can be pruned to exactly zero with a sufficiently large but
finite γ Zhao & Yu (2006). In contrast, when the `2 norm is applied, the coefficients will be shrunk
to smaller values but never pushed all the way to zero unless γ → ∞.
5.2	Practical Implications
In aggregate then, if the AE base model displays unavoidably high reconstruction errors, this implic-
itly constrains the corresponding VAE model to have a large optimal γ value, which can potentially
lead to undesirable posterior collapse per Proposition 5.1. In Section 6 we will demonstrate empiri-
cally that training unregularized AE models can become increasingly difficult and prone to bad local
minima (or at least bad stable stationary points) as the depth increases; and this difficulty can persist
even with counter-measures such as skip connections. Therefore, from this vantage point we would
argue that it is the AE base architecture that is effectively the guilty party when it comes to category
(v) posterior collapse.
The perspective described above also helps to explain why heuristics like KL warm-start are not
always useful for improving VAE performance. With the standard Gaussian model (4) considered
herein, KL warm-start amounts to adopting a pre-defined schedule for incrementally increasing
γ starting from a small initial value, the motivation being that a small γ will steer optimization
trajectories away from overregularized solutions and posterior collapse.
However, regardless of how arbitrarily small γmaybefixedat any point during this process, the VAE
reconstructions are not likely to be better than the analogous deterministic AE (which is roughly
equivalent to forcing γ = 0 within the present context). This implies that there can exist an implicit
Y* as computed by (10) that can be significantly larger such that, even if KL warm-start is used,
the optimization trajectory may well lead to a collapsed posterior stationary point that has this γ*
as the optimal value in terms of minimizing the VAE cost with other parameters fixed. Note that if
full posterior collapse does occur, the gradient from the KL term will equal zero and hence, to be
at a stationary point it must be that the data term gradient is also zero. In such situations, varying γ
manually will not impact the gradient balance anyway.
6	Empirical Assessments
In this section we empirically demonstrate the existence of bad AE local minima with high recon-
struction errors at increasing depth, as well as the association between these bad minima and immi-
nent VAE posterior collapse. For this purpose, we first train fully connected AE and VAE models
with 1, 2, 4, 6, 8 and 10 hidden layers on the Fashion-MNIST dataset (Xiao et al., 2017). Each
hidden layer is 512-dimensional and followed by ReLU activations (see Appendix A.1 for further
7
Under review as a conference paper at ICLR 2020
JOJJmeJBnbs Csz
2	4	6
Depth
I I I I
JOJJmeJBnbs Csz
2	3	4	5
Layer / Spatial Scale
4 2
WMZ -wVN-*Ekβ≡ &£•»«
3	4
LayersZResBIock
Figure 1: Reconstruction errors for various encoder/decoder models of varying complexity. Left:
Fully connected networks with different depths trained on Fashion-MNIST. Middle: Convolution
networks with increasing depth/# of spatial scales trained on Cifar100. Right: Averaged AE results
from residual networks with varying number of residual blocks and block depth trained on SVHN,
Cifar10, Cifar100 and CelebA. In all plots, once the encoder/decoder complexity is sufficiently high,
the reconstruction errors begin to increase.
details). The reconstruction error is shown in Figure 1(left). As the depth of the network increases,
the reconstruction error of the AE model first decreases because of the increased capacity. However,
when the network becomes too deep, the error starts to increase, indicating convergence to a bad
local minima (or at least stable stationary point/plateau) that is unrelated to KL-divergence regular-
ization. The reconstruction error of a VAE model is always worse than that of the corresponding
AE model as expected. Moreover, while KL warm-start/annealing can help to improve the VAE
reconstructions to some extent, performance is still worse than the AE as expected.
We next train AE and VAE models using a more complex convolutional network on Cifar100
data (Krizhevsky & Hinton, 2009). At each spatial scale, we use 1 to 5 convolution layers fol-
lowed by ReLU activations. We also apply 2 × 2 max pooling to downsample the feature maps to a
smaller spatial scale in the encoder and use a transposed convolution layer to upscale the feature map
in the decoder. The reconstruction errors are shown in Figure 1(middle). Again, the trend is similar
to the fully-connected network results. See Appendix A.1 for an additional ImageNet example.
It has been argued in the past that skip connections can increase the mutual information between ob-
servations x(i) and the inferred latent variables z (Dieng et al., 2018), reducing the risk of posterior
collapse. And it is well-known that ResNet architectures based on skip connections can improve
performance on numerous recognition tasks (He et al., 2016). To this end, we train a number of AE
models using ResNet-inspired encoder/decoder architectures on multiple datasets including Cifar10,
Cifar100, SVHN and CelebA. Similar to the convolution network structure from above, we use 1,
2, and 4 residual blocks within each spatial scale. Inside each block, we apply 2 to 5 convolution
layers. For aggregate comparison purposes, we normalize the reconstruction error obtained on each
dataset by dividing it with the corresponding error produced by the most shallow network structure
(1 residual block with 2 convolution layers). We then average the normalized reconstruction errors
over all four datasets. The average normalized errors are shown in Figure 1(right), where we observe
that adding more convolution layers inside each residual block can increase the reconstruction error
when the network is too deep. Moreover, adding more residual blocks can also lead to higher recon-
struction errors. And empirical results obtained using different datasets and networks architectures,
beyond the conditions of Figure 1, also show a general trend of increased reconstruction error once
the effective depth is sufficiently deep.
We emphasize that in all these models, as the network complexity/depth increases, the simpler mod-
els are always contained within the capacity of the larger ones. Therefore, because the reconstruction
error on the training data is becoming worse, it must be the case that the AE is becoming stuck at
bad local minima or plateaus. Again since the AE reconstruction error serves as a probable lower
bound for that of the VAE model, a deeper VAE model will likely suffer the same problem, only
exacerbated by the KL-divergence term in the form of posterior collapse. This implies that there will
be more σz values moving closer to 1 as the VAE model becomes deeper; similarly μ% values will
push towards 0. The corresponding dimensions will encode no information and become completely
useless.
8
Under review as a conference paper at ICLR 2020
2.5
2
1.5
1
0.5
0
0	0.2	0.4	0.6	0.8	1	1.2	0	0.2	0.4	0.6	0.8	1	1.2	0	0.2	0.4	0.6	0.8	1	1.2
<z	<z	<z
Figure 2: Histogram of σz values as VAE encoder/decoder network depth is varied. There are 2, 4
and 5 convolution layers in each spatial scale from left to right. As depth increases, the reconstruc-
tion error grows and more σz values are near 1, indicative of impending posterior collapse.
To help corroborate this association between bad AE local minima and VAE posterior collapse, we
plot histograms of VAE σz values as network depth is varied in Figure 2. The models are trained
on CelebA and the number of convolution layers in each spatial scale is 2, 4 and 5 from left to right.
As the depth increases, the reconstruction error becomes larger and there are more σz near 1.
7 Discussion
In this work we have emphasized the previously-underappreciated role of bad local minima in trap-
ping VAE models at posterior collapsed solutions. Unlike affine decoder models whereby all local
minima are provably global, Proposition 4.1 stipulates that even infinitesimal nonlinear perturba-
tions can introduce suboptimal local minima characterized by deleterious posterior collapse. Fur-
thermore, we have demonstrated that the risk of converging to such a suboptimal minima increases
with decoder depth. In particular, we outline the following practically-likely pathway to posterior
collapse:
1.	Deeper AE architectures are essential for modeling high-fidelity images or similar, and yet
counter-intuitively, increasing AE depth can actually produce larger reconstruction errors on the
training data because of bad local minima (with or without skip connections). An analogous VAE
model with the same architecture will likely produce even worse reconstructions because of the
additional KL regularization term, which is not designed to steer optimization trajectories away
from poor reconstructions.
2.	At any such bad local minima, the value of γ will necessarily be large, i.e., if it is not large, we
cannot be at a local minimum.
3.	But because of the thresholding behavior of the VAE as quantified by Proposition 5.1, as γ be-
comes larger there is an increased risk of exact posterior collapse along excessive latent dimen-
sions. And complete collapse along all dimensions will occur for some finite γ sufficiently large.
Furthermore, explicitly forcing γ to be small does not fix this problem, since in some sense the
implicit Y* is still large as discussed in Section 5.2.
While we believe that this message is interesting in and of itself, there are nonetheless several
practically-relevant implications. For example, complex hierarchical VAEs like BIVA notwithstand-
ing, skip connections and KL warm-start have modest ability to steer optimization trajectories to-
wards good solutions; however, this underappreciated limitation will not generally manifest until
networks are sufficiently deep as we have considered. Fortunately, any advances or insights gleaned
from developing deeper unregularized AEs, e.g., better AE architectures, training procedures, or
initializations (Li & Nguyen, 2019), could likely be adapted to reduce the risk of posterior collapse
in corresponding VAE models.
In closing, we should also mention that, although this work has focused on Gaussian VAE mod-
els, many of the insights translate into broader non-Gaussian regimes. For example, a variety of
recent VAE enhancements involve replacing the fixed Gaussian latent-space prior p(z) with a pa-
rameterized non-Gaussian alternative (Bauer & Mnih, 2019; Tomczak & Welling, 2018). This type
of modification provides greater flexibility in modeling the aggregated posterior in the latent space,
which is useful for generating better samples (Makhzani et al., 2016). However, it does not immu-
nize VAEs against the bad local minima introduced by deep decoders, and good reconstructions are
required by models using Gaussian or non-Gaussian priors alike. Therefore, our analysis herein still
applies in much the same way.
9
Under review as a conference paper at ICLR 2020
References
A. Alemi, B. Poole, I. Fischer, J. Dillon, R. Saurous, and K. Murphy. Fixing a broken ELBO. arXiv
preprint arXiv:1711.00464, 2017.
M. Bauer and A. Mnih. Resampled priors for variational autoencoders. arXiv preprint
arXiv:1810.11428, 2018.
M. Bauer and A. Mnih. Resampled priors for variational autoencoders. International Conference
on Artificial Intelligence and Statistics, 2019.
S. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Jozefowicz, and S. Bengio. Generating sentences from
a continuous space. arXiv preprint arXiv:1511.06349, 2015.
Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519, 2015.
L. Cai, H. Gao, and S. Ji. Multi-stage variational auto-encoders for coarse-to-fine image generation.
arXiv preprint arXiv:1705.07202, 2017.
E. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 58(2), 2011.
X. Chen, D. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Schulman, I. Sutskever, and P. Abbeel.
Variational lossy autoencoder. arXiv preprint arXiv:1611.02731, 2016.
B. Dai and D. Wipf. Diagnosing and enhancing VAE models. International Conference on Learning
Representations, 2019.
B. Dai, Y. Wang, J. Aston, G. Hua, and D. Wipf. Hidden talents of the variational autoencoder.
arXiv preprint arXiv:1706.05148, 2019.
A. Dieng, Y. Kim, A. Rush, and D. Blei. Avoiding latent variable collapse with generative skip
models. arXiv preprint arXiv:1807.04863, 2018.
K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. International Conference
on Machine Learning, 2010.
J.	He, D. Spokoyny, G. Neubig, and T. Berg-Kirkpatrick. Lagging inference networks and poste-
rior collapse in variational autoencoders. International Conference on Learning Representations,
2019.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR, 2016.
I.	Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, , and A. Lerchner.
β-vae: Learning basic visual concepts with a constrained variational framework. International
Conference on Learning Representations, 2017.
C. Huang, S. Tan, A. Lacoste, and A. Courville. Improving explorability in variational inference
with annealed variational objectives. Advances in Neural Information Processing Systems, 2018.
K. Kawaguchi. Deep learning without poor local minima. Advances in Neural Information Process-
ing Systems, 2016.
D. Kingma and M. Welling. Auto-encoding variational Bayes. International Conference on Learn-
ing Representations, 2014.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical
report, Citeseer, 2009.
P. Li and P.M. Nguyen. On random deep weight-tied autoencoders: Exact asymptotic analysis, phase
transitions, and implications to training. International Conference on Learning Representations,
2019.
J.	Lucas, G. Tucker, R. Grosse, and M. Norouzi. Understanding posterior collapse in generative
latent variable models. International Conference on Learning Representations, Workshop Paper,
2019.
10
Under review as a conference paper at ICLR 2020
L.	Maal0e, M. Fraccaro, V. Lievin, and O. Winther. BIVA: A very deep hierarchy of latent variables
for generative modeling. arXiv preprint arXiv:1902.02102, 2019.
A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders. arXiv
preprint arXiv:1511.05644, 2016.
P.A. Mattei and J. Frellsen. Leveraging the exact likelihood of deep latent variable models. Advances
in Neural Information Processing Systems, 2018.
E. Orjebin. A recursive formula for the moments of a truncated univariate normal distribu-
tion. 2014. URL https://people.smp.uq.edu.au/YoniNazarathy/teaching_
projects/studentWork/EricOrjebin_TruncatedNormalMoments.pdf.
A. Razavi, A. Oord, B. Poole, and O. Vinyals. Preventing posterior collapse with δ-VAEs. Interna-
tional Conference on Learning Representations, 2019.
D. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint
arXiv:1505.05770, 2015.
D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
in deep generative models. International Conference on Machine Learning, 2014.
M. Rolinek, D. Zietlow, and G. Martius. Variational autoencoders pursue PCA directions (by acci-
dent). 2019.
C. S0nderby, T. Raiko, L. Maal0e, S. S0nderby, and O. Winther. HoW to train deep variational
autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282, 2016.
P. Sprechmann, A.M. Bronstein, and G. Sapiro. Learning efficient sparse and loW rank models.
IEEE Trans. Pattern Analysis and Machine Intelligence, 37(9), 2015.
M. Tipping and C. Bishop. Probabilistic principal component analysis. J. Royal Statistical Society,
SeriesB ,61(3):611-622, 1999.
J. Tomczak and M. Welling. VAE With a VampPrior. International Conference on Artificial Intelli-
gence and Statistics, 2018.
A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves, and K. Kavukcuoglu. Con-
ditional image generation With PixelCNN decoders. Advances in Neural Information Processing
Systems, 2016.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
S. Yeung, A. Kannan, Y. Dauphin, and L. Fei-Fei. Tackling over-pruning in variational autoencoders.
arXiv preprint arXiv:1706.03643, 2017.
C. Yun, S. Sra, and A. Jadbabaie. Small nonlinearities in activation functions create bad local minima
in neural netWorks. International Conference on Learning Representations, 2019.
P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine learning research,
7:2541-2563, 2006.
A Appendix
A. 1 Network Structure, Experimental Settings, and Additional ImageNet
Results
Three different kinds of netWork structures are used in the experiments: fully connected netWorks,
convolution netWorks, and residual netWorks. For all these structures, We set the dimension of the
latent variable z to 64. We then describe the netWork details accordingly.
11
Under review as a conference paper at ICLR 2020
Fully Connected Netowrk: This experiment is only applied on the simple Fashion-MNIST dataset,
which contains 60000 28 × 28 black-and-while images. These images are first flattened to a 784
dimensional vector. Both the encoder and decoder have multiple number of 512-dimensional hidden
layers, each followed by ReLU activations.
Convolution Netowrk: The original images are either 32 × 32 × 3 (Cifar10, Cifar100 and SVHN)
or 64 × 64 × 3 (CelebA and ImageNet). In the encoder, we use a multiple number (denoted as t)
of 3 × 3 convolution layers for each spatial scale. Each convolution layer is followed by a ReLU
activation. Then we use a 2 × 2 max pooling to downsample the feature map to a smaller spatial
scale. The number of channels is doubled when the spatial scale is halved. We use 64 channels when
the spatial scale is 32 × 32. When the spatial scale reaches 4 × 4 (there should be 512 channels in
this feature map), we use an average pooling to transform the feature map to a vector, which is then
transformed into the latent variable using a fully connected layer. In the decoder, the latent variable
is first transformed to a 4096-dimensional vector using a fully connected layer and then reshaped to
2 × 2 × 1024. Again in each spatial scale, we use 1 transpose convolution layer to upscale the feature
map and halve the number of channels followed by t - 1 convolution layers. Each convolution and
transpose convolution layer is followed by a ReLU activation layer. When the spatial scale reaches
that of the original image, we use a convolution layer to transofrm the feature map to 3 channels.
Residual Network: The network structure of the residual network is similar to that of a convo-
lution network described above. We simply replace the convolution layer with a residual block.
Inside the residual block, we use different numbers of convolution numbers. (The typical number of
convolution layers inside a residual block is 2 or 3. In our experiments, we try 2, 3, 4 and 5.)
Training Details: All the experiments with different network structures and datasets are trained
in the same procedure. We use the Adam optimization method and the default optimizer hyper
parameters in Tensorflow. The batch size is 64 and we train the model for 250K iterations. The
initial learning rate is 0.0002 and it is halved every 100K iterations.
Additional Results on ImageNet: We also show the reconstruction error for convolution networks
with increasing depth trained on ImageNet in Figure 3. The trend is the same as that in Figure 1.
Figure 3: Reconstruction error for Convolution networks with increasing depth/# of spatial scales
trained on ImageNet.
A.2 Proof of Proposition 4.1
While the following analysis could in principle be extended to more complex datasets, for our pur-
poses it is sufficient to consider the following simplified case for ease of exposition. Specifically, we
assume that n > 1, d > κ, set d = 2, n = 2, κ = 1, and x(1) = (1, 1), x(2) = (-1, -1).
Additionally, we will use the following basic facts about the Gaussian tail. Note that (12)-(13) below
follow from integration by parts; see Orjebin (2014).
12
Under review as a conference paper at ICLR 2020
Lemma A.1 Let E 〜N (0,1), A > 0; φ(x), Φ(x) be the pdf and Cdfofthe standard normal distri-
bution, respectively. Then
1 - Φ(A) ≤ e-A2/2,	(11)
E[E1{>A}] = φ(A),	(12)
E[E21{>A}] = 1 - Φ(A) + Aφ(A).	(13)
A.2.1 Suboptimality of (7)
Under the specificed conditions, the energy from (7) has a value of nd. Thus to show that it is not the
global minimum, it suffices to show that the following VAE, parameterized by δ, has energy → -∞
as δ → 0:
μz1 = 1,μZ2) = -1,
Wx = (α + 1,α + 1), bx = 0,
σz(1) = σz(2) = δ,
Y = EN(ε∣0,1)2(I- πα((α + I)(I + δε)))2∙
This follows because, given the stated parameters, we have that
2
L(θ, φ) = ^X(I + 2 log EN(ε∣0,1)2(I- πα((α + 1)(1 + δE)))2 - 2 log δ + δ2 + I)
i=1
2
= X(Θ(1)+ 2 log EN(ε∣0,1) (I - πα(α +1 + (α + 1)δε))2 - 2 log δ)
i=1
≤(i)4 log δ + Θ(1).
(i)	holds when δ < a+ɪ； to See this, denote X := α +1 + (α + 1)(δε). Then
EN(ε∣0,1) (I- πα(X))2
=Eε[(1 — ∏α(x))21{χ≥α}]+ Eε[(1 — ∏α (x))2 1{|x| <α}]+ Eε[(1 — ∏α (x))2 1{χ<-α}]
≤ Eε[(1 - (X - α))2] + P(|X| < α) + Eε((1 - X - α)21{x<-α}) .
、--------{-------} 、-----V----} 、-----------V------------}
(a)	(b)	(c)
In the RHS above (a) = [(α + 1)δ]2; using (11)-(13) we then have
(b)	< P(X < α) = P (ε < (0⅛) ≤ exp (- 2[(α+i)δ]2
(c)	< Eε((2α + (α + 1)δε)21{x<α})
-1
= I	(2α + (α + 1)δε)2 -^=e-ε2/2 dε
-∞	2π
-1
< [	(4ɑ2 + [(α + 1)δε]2)  e-ε2^2dε
-∞	2π
<	4α2 + ((α + 1)δ)2 1 +
exp (-2[(α + 1)δ]2
when δ < α++ι. ThUs
Iirn EN(ε∣0,1) (I - πα(X))2 = 1
δ→0	[(α +1)δ]2	,
and
lim{log EN(ε∣0,i)(1 - ∏α(x))2 - 2 log δ} = 2log(α + 1),
or
2logE(1 - πα(X))2 = 4logδ+ Θ(1),
and we can see (i) holds.
13
Under review as a conference paper at ICLR 2020
A.2.2 Local Optimality of (7)
We will now show that at (7), the Hessian of the energy has structure
	(Wx)	(bx)	(σ(i),μZi))	(γ)
(Wx)	0	0	0	0
(bx)	0	21 γ	0	0
-(i),μZi))	0	0	(p.d.)	0
(γ)	0	0	0	(p.d.)
where p.d. means the corresponding submatrix is positive definite and independent of other param-
eters. While the Hessian is 0 in the subspace of W x , we can show that for VAEs that are only
different from (7) by W x , the gradient always points back to (7). Thus (7) is a strict local minima.
First we compute the Hessian matrix block-wise. We will identify Wx ∈ R2×1 with the vector
(Wj)2=1, and use the shorthand notations x(i) = (Xji))2=» bχ = (bj)2=i, z(i) = μZi) + σZi)ε,
where ε ~ N(0,1) (recall that z(i) is a scalar in this proof).
1.	The second-order derivatives involving Wx can be expressed as
黑=-2 XXEε[(∏α(WjZ(W)) ∙ (xji) - ∏α(WjZ⑴)-bj)],	(14)
∂Wj	γ
j	i=1
and therefore all second-order derivatives involving Wj will have the form
E[πα0 (WjZ(i))F1 + πα00(WjZ(i))F2],	(15)
where F1 , F2 are some arbitrary functions that are finite at (7). Since πα0 (0) = πα00 (0) =
Wj = 0, the above always evaluates to 0 at W x = 0.
2.	For second-order derivatives involving bx, we have
and
∂L
∂bχ
-T Eε[x(i)-πα(W x'-bx]
∂ 2L	2		
d(bXp =	—	J- γ	,	
∂2 L	2	∂L	0,
∂γ∂bχ	一 γ	∂V	
(since Wx = 0);
∂2L	∂2L
and a ∂)Lb and	∂)L ⑸ will also have the form of (15), thus both equal 0 at WX = 0.
3.	Next consider second-order derivatives involving μZi) or σki). Since the KL part of the
energy, PZi KL(qφ(z∣x(i))∣p(z)), only depends on μZi) and σf), and have p.d. Hes-
sian at (7) independent of other parameters, it suffices to calculate the derivatives of the
reconstruction error part, denoted as Lrecon. Since
∂Lecon = - XEeh(Xji)-∏α(Wjz(i)) -bj)Wj∏α(Wjz(i))i ,
∂L(con = - XEeh(Xji)- ∏α(WjZSi)Wje∏α(WjZ⑴)i ,
all second-order derivatives will have the form of (15), and equal 0 at Wx = 0.
4.	For Y, we can calculate that ∂2L∕∂γ2 = 4∕γ2 > 0 at (7).
Now, consider VAE parameters that are only different from (7) in Wχ. Plugging bχ = X, μZi)
0, σk(i) = 1 into (14), we have
14
Under review as a conference paper at ICLR 2020
∂L 2 n
而=-∑Eε[(∏α(Wjε)ε) ∙ (-∏α(Wjε))].
∂Wj	γ
j	i=1
As (∏α(Wjε)ε) ∙ (-∏ɑ(Wjε)) ≤ 0 always holds, We can see that the gradient points back to (7).
This concludes our proof of (7) being a strict local minima.
A.3 Proof of Proposition 5.1
We begin by assuming an arbitrarily complex encoder for convenience. This allows us to remove
the encoder-sponsored amortized inference and instead optimize independent parameters μZi) and
σ(zi) separately for each data point. Later we will show that this capacity assumption can be dropped
and the main result still holds.
We next define
>	>>	>	>>
mz，(μZ1)) ,..., ("Zn))	∈ RKn and Sz，(σ?I)) ,..., (σZn))	∈ Rκn, (16)
which are nothing more than the concatenation of all of the decoder means and variances from
each data point into the respective column vectors. It is also useful to decompose the assumed
non-degenerate decoder parameters via
θ ≡ [ψ,w], ψ ，θ∖w,	(17)
where W ∈ [0,1] is a scalar such that μχ (z; θ) ≡ μχ (wz; ψ). Note that we can always repa-
rameterize an existing deep architecture to extract such a latent scaling factor which we can then
hypothetically optimize separately while holding the remaining parameters ψ fixed. Finally, with
slight abuse of notation, we may then define the function
f (wmz,wSz) ,	(18)
nn
X f (μzi), σzi), [ψ,w], x(i)) ≡ X en(z∣μZi),diag[σZi)i2) [kx(i) -μx wzzψ ψ k2].
This is basically just the original function f summed over all training points, with ψ fixed at the
corresponding values extracted from θ while w serves as a free scaling parameter on the decoder.
Based on the assumption of Lipschitz continuous gradients, we can always create the upper bound
f(u, V) ≤ f(u, V)	(19)
+ (U - U)> Vuf (u, V)Iu=U + L llu - Uk2 + (V - V)> Vvf (u, V)IV=V + L Ilv - vk2,
where L is the Lipschitz constant of the gradients and we have adopted u , wmz and V , wσz
to simplify notation. Equality occurs at the evaluation point {u, v} = {U, V}. However, this bound
does not account for the fact that we know Vvf (U, V) ≥ 0 (i.e., f (U, V) is increasing w.r.t. V) and
that V ≥ 0. Given these assumptions, we can produce the refined upper bound
fub (U, V) ≥ f(U,V),	(20)
where fub (U, V) ,
nd
f (u,V) + (U — u)> Vuf(U,v)|u=u + L2 ku — Uk2+Xg",vj, Vvjf(U,V)Ivj =%) (21)
j=1
and the function g : R3 → R is defined as
{(ν — V) δ + L (v	— v)2	if V ≥ V — L and	{v,	v, δ}	≥ 0,
嚏	if v<V — L and	{v,	V, δ}	≥ 0,	(22)
∞	otherwise.
15
Under review as a conference paper at ICLR 2020
Given that
V - L = arg min h(v - V) δ + LL (V - V)2] and -L = min h(v - V) δ + LL (V - V)2],
(23)
the function g is basically just setting all values of (V - V) δ + LL ∣∣v - V∣∣2 with negative slope to
δ2
the minimum -L. ThiS change is possible while retaining an upper bound because f (u, V) is non-
decreasing in v by stated assumption. Additionally, g is set to infinity for all V < 0 to enforce
non-negatively.
While it may be possible to proceed further using fub, we find it useful to consider a final modifica-
tion. Specifically, we define the approximation
fappr (u, V) ≈ fub (U, V),	(24)
where f appr (u, v) ,
nd
f(u, V) + (U - u)> Vuf(U, V)Iu=U + LL ku - u k2 + X gappr (Vj ,Vj, Vvjf(U, V)Ivj =Vj)
j=1
(25)
and
f	-2L + 2LV2V2	if V - "I ≥ 0 and {v, v, δ} ≥ 0,
gappr (v, V, δ)	, f	(LV2	- SV)+ (V -	LL) V2	if V - L < 0 and {v,V,S}≥ 0,	(26)
(	∞	otherwise.
While slightly cumbersome to write out, gappr has a simple interpretation. By construction, we have
that
min gappr (v, V, δ) = gappr (0, V, δ) = min g (v, V,δ) = g (0, V, δ)	(27)
vv
and	gappr (V, V,δ) = g (V, V, δ) = 0.	(28)
At other points, gappr is just a simple quadratic interpolation but without any factor that is linear
in V. And removal of this linear term, while retaining (27) and (27) will be useful for the analysis
that follows below. Note also that although f appr (U, V) is no longer a strict bound on f (U, V), it
will nonetheless still be an upper bound whenever Vj ∈ {0, Vj } for all j which will ultimately be
sufficient for our purposes.
We now consider optimizing the function
happr (mz, Sz ,w)，1 f appr (Wmz ,wsz) + X ∣∣μZi)∣∣ + ∣∣σZi)∣∣ - log Idiag [σZ]	. (29)
i=1	2	2
If we define L (mz , sz, w) as the VAE cost from (4) under the current parameterization, then by
design it follows that
happr (m z, Sz, W) = L (m z, Sz, W)	(30)
and
happr(mz,sz,W) ≥ L (mz,sz,W)	(31)
whenever Wσj ∈ {0, WSσSj} for allj. Therefore ifwe find such a solution {m0z, s0z, W0} that satisfies
this condition and has happr(m0z, s0z, W0) < happr(mS z, sSz, WS), it necessitates that L(m0z, s0z, W0) <
L(mS z, sSz, WS) as well. This then ensures that {mS z, Ssz, WS} cannot be a local minimum.
We now examine the function happr more closely. After a few algebraic manipulations and exclud-
ing irrelevant constants, we have that
happr(mz,sz,W)≡
nd
X{Y [wmz,j Vujf (u, V)Iuj =Wmz j + LL (w2mz,j - 2wmzjWmZj) + Cjw2s2,j]
j=1	,
+ m2,j + Szj- log s2,j},	(32)
16
Under review as a conference paper at ICLR 2020
where cj is the coefficient on the v2 term from (26). After rearranging terms, optimizing out mz and
sz, and discarding constants, we can then obtain (with slight abuse of notation) the reduced function
nd
happr(W) , X --^2+ + log(γ + Cjw2),
γ+	βw2
j=1
(33)
where β，LL and yj ,2 IlwmZj- 1 Vujf(U,V)Iuj=WmZJ∣2. Note that yj must be bounded
since L = 04 and w ∈ [0,1], Vujf (u, v)∣u =Wm ≤ L, and m are all bounded. The latter is im-
plicitly bounded because the VAE KL term prevents infinite encoder mean functions. Furthermore,
cj must be strictly greater than zero per the definition of a non-degenerate decoder; this guarantees
that
gappr wsj3 ,但,Jjf(U V)lvj=wsj > 9。PPp (0,wgj , Vvjf (U, v)Ivj=Wsj
which is only possible with cj > 0. Proceeding further, because
(34)
Vw2 happr (W)=XX ((γ⅛+Y⅛ )，	(35)
we observe that if γ is increased sufficiently large, the first term will always be smaller than the
second since β and all yj are bounded, and cj > 0 ∀j . So there can never be a point whereby
VW2 haPPr (w) = 0 when γ = γ0 sufficiently large. Therefore the minimum in this situation occurs
on the boundary where w2 = 0. And finally, if w2 = 0, then the optimal mz and sz is determined
solely by the KL term, and hence they are set according to the prior. Moreover, the decoder has no
signal from the encoder and is therefore optimized by simply setting μχ(0; ψ) to the mean X for
all i.4 5 Additionally, none of this analysis requires and arbitrarily complex encoder; the exact same
results hold as long as the encoder can output a 0 for means and 1 for the variances.
Note also that if we proceed through the above analysis using w ∈ Rκ as parameterizing a
separate wj scaling factor for each latent dimension j ∈ {1, . . . , κ}, then a smaller γ value would
generally force partial collapse. In other words, we could enforce nonzero gradients of haPPr (w)
along the indices of each latent dimension separately. This loosely criteria would then lead to
qΦ* (zj |x) = p(zj) along some but not all latent dimensions as stated in the main text below
Proposition 5.1.
A.4 Representative Stationary Point Exhibiting Posterior Collapse in Deep
VAE Models
Here we provide an example of a stationary point that exhibits posterior collapse with an arbitrary
deep encoder/decoder architecture. This example is representative of many other possible cases.
Assume both encoder and decoder mean functions 模化 and μ%, as well as the diagonal encoder
covariance function Σz = diag[σz2 ], are computed by standard deep neural networks, with layers
composed of linear weights followed by element-wise nonlinear activations (the decoder covariance
satisfies Σx = γI as before). We denote the weight matrix from the first layer of the decoder mean
network as W： , while wl j refers to the corresponding j-th column. Assuming P layers, We
μ X	μ x ,
denote Wμz and W黑 as weights from the last layers of the encoder networks producing μ% and
log σ2 respectively, with j-th rows defined as w∖= j. and wρσ2 夕.We then characterize the following
key stationary point:	z
Proposition A.2 If w;^,j = (w/^ j∙)> = (w：2 j)	= 0 for any j ∈ {1,2,...,κ}, then the
gradients of(4) with respect to w1 j, WP j, and WP2 ∙ are all equal to zero.
μx ,	μ Z j	σ Z ,j
4 L = 0 would violate the stipulated conditions for a non-degenerate decoder since it would imply that no
signal from z could pass through the decoder. And of course if L = 0, we would already be at a solution
exhibiting posterior collapse.
5We are assuming here that the decoder has sufficient capacity to model any constant value, e.g., the output
layer has a bias term.
17
Under review as a conference paper at ICLR 2020
If the stated weights are zero along dimension j, then obviously it must be that qφ(zj |x) = p(zj),
i.e., a collapsed dimension for better or worse. The proof is straightforward; we provide the details
below for completeness.
Proof: First we remind that the variational upper bound is defined in (2). We define L(x; θ, φ) as
the loss at a data point x, i.e.
L(x; θ, Φ) = -Eqφ(z∣x) [logPθ(x|z)] + KL [qφ(z∣x)∣∣p(z)].	(36)
The total loss is the integration of L(x; θ, φ) over x. Further more, we denote Lkl (x; θ) and
Lgen(x; θ, φ) as the KL loss and the generation loss at x respectively, i.e.
κ
Lkl(χ; φ) = KL[qφ(z∣χ)l∣p(z)] = X KL [qφ(z∕χ)l∣p(zj)],
i=1
1κ
=2 £ (μ2,j + σZ,j- log σ2,j- 1)	(37)
j=1
Lgen(X； Φ,θ) = -Eqφ(z∣χ) [logPθ (x|z)].	(38)
The second equality in (37) holds because the covariance of qφ(z∣x) and p(z) are both diagonal. The
last encoder layer and the first decoder layer are denoted as hρ and h∖. If w∖=小 =0, w：2 肌 =0,
then we have	z
μz,j=wμz,j∙he = 0,	σZ,j = eχp (wσ2,j∙) = 1,	q(zjIx)=N(O,I).
The gradient of μz,j and σz,j from Lki (x; φ) becomes
∂Lki(x； φ)	∂Lki(x; φ)	-ι n
~^μτ="z,j=0,	= 1-σz,j=0.
So the gradient of w/7 小 and w：2 肌 from Lkl is
∂Lkl (x; φ)	∂Lkl(x; φ)
d WPzj
dμz,j
∂Lkl(x; φ) = ∂ Lkl(x; φ) hρ> =o
∂w",	= 2σz,j ∙ ∂σz,j he =0.
Now we consider the gradient from Lgen (x; θ, φ). We have
-∂logpθ(x∣z) = -∂logpθ(x∣z) ∂hd
dz	—	∂ hd	dz
Since
hd = act (X w1μx∙j Zj
where act(∙) is the activation function, We can obtain
Ihd = act0 (Xwμχ,∙jzj) wwj = 0.
Plugging this back into (43) gives
-∂ log Pθ (X∣z) = 0
dzj
(39)
(40)
(41)
(42)
(43)
(44)
(45)
(46)
According to the chain rule, we have
∂Lgen(x; θ,φ) _ „
∂wμzj∙	=E
Z 〜qφ(z∣x)
-∂logpθ(x∣z) ∂zj . = = 0
(47)
18
Under review as a conference paper at ICLR 2020
∂Lgen(x; θ, φ)
∂W2 , — = E"(ZIx)
σZ,j∙
-∂logpθ(x|z) Hz： 1 = 0
(48)
After combining these two equations with (41) and (42) and then integrating over x, we have
∂L(θ,Φ) _0
⅛^ = ,
∂L(θ,φ) _0
dwj =.
(49)
(50)
Then We consider the gradient with respect to w',j. Since wμx,-j is part of θ, it only receives
gradient from Lgen(x; θ, φ). So we do not need to consider the KL loss. If wl j = 0, h∖ =
μ	μx,
EK=I WLjZj is not related to Zj. So pθ(x|z) = pθ(x∣z-j), where z-j represents Z without the
j-th dimension. The gradient of w[,j is
∂Lgen(x; θ, φ) ∂wl j μx , j	e EZ~q(Z∣x)	-∂ log pθ (x|z) ∂w1 j μχ , j	_		=EZ~q(Z∣x)	-∂ log pθ (x|z) 一 〔一砒I―Zj		(51)
	=Ez-j~q(Z-j∣x) = EZ-i~q(Z-i∣x)		Ezj~N (O,1) --∂ log pθ (x ∂hΓ	-∂ log pθ (x∣z-j) 一 〔一殖一Zj |Z j ) 	Ezj~N (0,i)[zj]		= 0.	
The integration over x should also be 0. So we obtain
∂L(θ; φ)	0
∂wl ∙j	.
μx , j
(52)
19