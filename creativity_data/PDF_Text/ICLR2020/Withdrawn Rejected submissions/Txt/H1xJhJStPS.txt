Under review as a conference paper at ICLR 2020

EQUILIBRIUM  PROPAGATION  WITH

CONTINUAL  WEIGHT  UPDATES

Anonymous authors

Paper under double-blind review

ABSTRACT

Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learn-
ing and Neuroscience, by computing gradients closely matching those of Back-
propagation Through Time (BPTT), but with a learning rule local in space. Given
an     input x and associated target y, EP proceeds in two phases: in the first phase
neurons evolve freely towards a first steady state; in the second phase output neu-
rons are nudged towards y until they reach a second steady state.  However, in
existing implementations of EP, the learning rule is not local in time: the weight
update is performed after the dynamics of the second phase have converged and
requires information of the first phase that is no longer available physically.  In
this work, we propose a version of EP named Continual Equilibrium Propagation
(C-EP) where neuron and synapse dynamics occur simultaneously throughout the
second phase, so that the weight update becomes local in time.  Such a learning
rule local both in space and time opens the possibility of an extremely energy
efficient hardware implementation of EP. We prove theoretically that, provided
the learning rates are sufficiently small, at each time step of the second phase the
dynamics of neurons and synapses follow the gradients of the loss given by BPTT
(Theorem 1). We demonstrate training with C-EP on MNIST and generalize C-EP
to neural networks where neurons are connected by asymmetric connections. We
show through experiments that the more the network updates follows the gradients
of BPTT, the best it performs in terms of training. These results bring EP a step
closer to biology by better complying with hardware constraints while maintaining
its intimate link with backpropagation.

1    INTRODUCTION

A motivation for deep learning is that a few simple principles may explain animal intelligence
and allow us to build intelligent machines, and learning paradigms must be at the heart of such
principles, creating a synergy between neuroscience and Artificial Intelligence (AI) research. In 
the
deep learning approach to AI (LeCun et al., 2015), backpropagation thrives as the most powerful
algorithm for training artificial neural networks. Unfortunately, its implementation on conventional
computer or dedicated hardware consumes more energy than the brain by several orders of magnitude
(Strubell et al., 2019). One path towards reducing the gap between brains and machines in terms of
power consumption is by investigating alternative learning paradigms relying on locally available
information, which would allow radically different hardware implementations: such local learning
rules  could be used for the development of extremely energy efficient learning-capable hardware.
Investigating such bioplausible learning schemes with real-world applicability is therefore of 
interest
not     only for neuroscience, but also for developing neuromorphic computing hardware that takes
inspiration from information-encoding, dynamics and topology of the brain to reach fast and energy
efficient AI (Ambrogio et al., 2018; Romera et al., 2018). In these regards, Equilibrium Propagation
(EP) is an alternative style of computation for estimating error gradients that presents significant
advantages (Scellier and Bengio, 2017).

EP belongs to the family of contrastive Hebbian learning (CHL) algorithms (Ackley et al., 1985;
Movellan, 1991; Hinton, 2002) and therefore benefits from an important feature of these algorithms:
neural dynamics and synaptic updates depend solely on information that is locally available. As a
CHL algorithm, EP applies to convergent RNNs, i.e. RNNs that are fed by a static input and converge
to a steady state.  Training such a convergent RNN consists in adjusting the weights so that the

1


Under review as a conference paper at ICLR 2020

steady state corresponding to an input x produces output values close to associated targets y. CHL
algorithms proceed in two phases: in the first phase, neurons evolve freely without external 
influence
and settle to a (first) steady state; in the second phase, the values of output neurons are 
influenced by
the target y and the neurons settle to a second steady state. CHL weight updates consist in a 
Hebbian
rule strengthening the connections between co-activated neurons at the first steady state, and an
anti-Hebbian rule with opposite effect at the second steady state. A difference between Equilibrium
Propagation and standard CHL algorithms is that output neurons are not clamped in the second phase
but elastically pulled towards the target y.

A second key property of EP is that, unlike CHL and other related algorithms, it is intimately 
linked
to backpropagation.  It has been shown that synaptic updates in EP follow gradients of recurrent
backpropagation (RBP) (Scellier and Bengio, 2019) and backpropagation through time (BPTT)
(Ernoult         et al., 2019). This makes it especially attractive to bridge the gap between 
neural networks
developed by neuroscientists, neuromorphic researchers and deep learning researchers.

Nevertheless, the bioplausibility of EP still undergoes two major limitations. First, although EP is
local in space, it is non-local in time.  In all existing implementations of EP the weight update is
performed after the dynamics of the second phase have converged, when the first steady state is
no longer physically available. Thus the first steady state has to be artificially stored. Second, 
the
network dynamics have to derive from a primitive function, which is equivalent to the requirement of
symmetric weights in the Hopfield model. These two requirements are biologically unrealistic and
also hinder the development of efficient EP computing hardware.

In this work, we propose an alternative implementation of EP (called C-EP) which features temporal
locality, by enabling synaptic dynamics to occur throughout the second phase, simultaneously with
neural dynamics. We then address the second issue by adapting C-EP to systems having asymmetric
synaptic connections, taking inspiration from Scellier et al. (2018) ; we call this modified version
C-VF.

More specifically, the contributions of the current paper are the following:

We introduce Continual Equilibrium Propagation (C-EP, Section 3.1-3.2), a new version of
EP with continual weight updates: the weights of the network are adjusted continually in
the second phase of training using local information in space and time. Neuron steady states
do not need to be stored after the first phase, in contrast with standard EP where a global
weight update is performed at the end of the second phase.  Like standard EP, the C-EP
algorithm applies to networks whose synaptic connections between neurons are assumed to
be symmetric and tied.

We show mathematically that, provided that the changes in synaptic strengths are sufficiently
slow (i.e.  the learning rates are sufficiently small), at each time step of the second phase
the dynamics of neurons and synapses follow the gradients of the loss obtained with BPTT
(Theorem  1  and  Fig.  2,  Section  3.3).   We  call  this  property  the  Gradient  Descending
Dynamics (GDD) property,  for consistency with the terminology used in Ernoult et al.
(2019).

We demonstrate training with C-EP on MNIST, with accuracy approaching the one obtained
with standard EP (Section 4.2).

Finally, we adapt our C-EP algorithm to the more bio-realistic situation of a neural network
with asymmetric connections between neurons. We call this modified version C-VF as it is
inspired by the Vector Field method proposed in Scellier et al. (2018). We demonstrate this
approach on MNIST, and show numerically that the training performance is correlated with
the satisfaction of Gradient Descending Dynamics (Section 4.3).

For completeness, we also show how the Recurrent Backpropagation (RBP) algorithm of Almeida
(1987); Pineda (1987) relates to C-EP, EP and BPTT. We illustrate the equivalence of these four
algorithms on a simple analytical model (Fig. 3) and we develop their relationship in Appendix A.

2    BACKGROUND: CONVERGENT  RNNS  AND  EQUILIBRIUM  PROPAGATION

Convergent RNNs With Static Input.    We consider the supervised setting, where we want to
predict a target y given an input x. The model is a recurrent neural network (RNN) parametrized by 
θ

2


Under review as a conference paper at ICLR 2020

and evolving according to the dynamics:

st₊₁ = F (x, st, θ) .                                                          (1)

F  is the transition function of the system. Assuming convergence of the dynamics before time step

T , we have sT = s∗ where s∗ is the steady state of the network characterized by

s∗ = F (x, s∗, θ) .                                                            (2)

The number of timesteps T  is a hyperparameter that we choose large enough so that sT = s   for the
current value of θ. The goal is to optimize the parameter θ in order to minimize a loss:

L∗ = l (s∗, y) .                                                              (3)

Algorithms that optimize the loss    ∗ for RNNs include Backpropagation Through Time (BPTT) and
the Recurrent Backpropagation (RBP) algorithm of Almeida (1987); Pineda (1987), presented in
Appendix B.

Equilibrium  Propagation  (EP).    EP  (Scellier  and  Bengio,  2017)  is  a  learning  algorithm  
that
computes the gradient of    ∗ in the particular case where the transition function F derives from
a scalar function Φ, i.e.  with F  of the form F (x, s, θ)  =  ∂Φ (x, s, θ).  The algorithm 
consists in
two phases (see Alg. 1 of Fig. 1). During the first phase, the network follows a sequence of states
s₁, s₂, s₃ . . . and converges to a steady state denoted s  .  In the second phase, an extra term β 
 ∂A

pertubs the dynamics of the neurons (where β  > 0 is a scalar hyperparameter): starting from the
steady state sβ = s∗, the network follows a second sequence of states sβ, sβ, sβ . . . and 
converges to

a new steady state denoted sβ. Scellier and Bengio (2017) have shown that the gradient of the loss

∗                                                      β

L∗

can be estimated based on the two steady states s∗ and s∗ . Specifically, in the limit β → 0,

1 . ∂Φ .x, sβ, θΣ − ∂Φ (x, s  , θ)Σ → − ∂L∗ .                                    (4)

3    EQUILIBRIUM  PROPAGATION  WITH  CONTINUAL  WEIGHT  UPDATES  (C-EP)

This section presents the main theoretical contributions of this paper. We introduce a new algorithm
to optimize    ∗ (Eq. 3):  a new version of EP with continual parameter updates that we call C-EP.
Unlike typical machine learning algorithms (such as BPTT, RBP and EP) in which the weight updates
occur after all the other computations in the system are performed, our algorithm offers a mechanism
in which the weights are updated continuously as the states of the neurons change.

3.1    FROM EP TO C-EP: AN INTUITION BEHIND CONTINUAL WEIGHT UPDATES

The key idea to understand how to go from EP to C-EP is that the gradient of EP appearing in Eq. (4)
reads as the following telescopic sum:


1 . ∂Φ .x, sβ, θΣ − ∂Φ (x, s  , θ)Σ = Σ∞   1 . ∂Φ .x, sβ, θΣ − ∂Φ .x, sβ

, θΣΣ .      (5)

`         global paramet˛e¸r gradient in EP              x

t=1

parameter gradient at time t in C-EP

In Eq. (5) we have used that sβ = s   and sβ → sβ as t → ∞. Here lies the very intuition of 
continual

updates motivating this work; instead of keeping the weights fixed throughout the second phase and

updating them at the end of the second phase based on the steady states s   and sβ, as in EP (Alg. 
1 of

Fig. 1), the idea of the C-EP algorithm is to update the weights at each∗time t of the second phase


between two consecutive states sβ

t−1

and sβ (Alg. 2 of Fig. 1). One key difference in C-EP compared

to EP though, is that, in the second phase, the weight update at time step t influences the neural 
states
at time step t + 1 in a nontrivial way, as illustrated in the computational graph of Fig. 2. In the 
next
subsection we define C-EP using notations that explicitly show this dependency.

3


Under review as a conference paper at ICLR 2020


Algorithm 1 EP                                                  

Input: x, y, θ, β, η.

Output: θ.

1:  s₀ ← 0                                     d First Phase

Algorithm 2 C-EP (with simplified notations)      

Input: x, y, θ, β, η.

Output: θ.

1:  s₀ ← 0                                        d First Phase


2:  repeat

3:         st₊₁ ← ∂Φ (x, st, θ)

2:  repeat

3:         st₊₁ ← ∂Φ (x, st, θ)


4:  until st = s∗

5:  Store s∗

4:  until st

β

∂s

= s∗


6:  sβ ← s∗                        d Second Phase

5:  s₀      s                                   d Second Phase

6:  repeat


7:  repeat              

Σ           .       Σ

7:         sβ

← ∂Φ .x, sβ, θΣ − β ∂A .sβ, yΣ

	


9:  until sβ = sβ

.     .       Σ          .    ΣΣ


11:  θ ← θ + η . ∂Φ .sβ, θΣ − ∂Φ (s  , θ)Σ

10:  until sβ and θ are converged.

Figure 1: Left. Pseudo-code of EP. This is the version of EP for discrete-time dynamics introduced
in Ernoult et al. (2019). Right. Pseudo-code of C-EP with simplified notations (see section 3.2 for 
a
formal definition of C-EP). Difference between EP and C-EP. In EP, one global parameter update
is performed at the end of the second phase ; in C-EP, parameter updates are performed throughout
the second phase. Eq. 5 shows that the continual updates of C-EP add up to the global update of EP.

3.2    DESCRIPTION OF THE C-EP ALGORITHM

The first phase of C-EP is the same as that of EP (see Fig. 1).  In the second phase of C-EP the
parameter variable is regarded as another dynamic variable θt that evolves with time t along with
st.   The  dynamics  of  st and  θt in  the  second  phase  of  C-EP  depend  on  the  values  of  
the  two
hyperparameters β (the hyperparameter of influence) and η (the learning rate), therefore we write 
sβ,η
and θβ,η to show explicitly this dependence. With now both the neurons and the synapses evolving in

the second phase, the dynamic variables sβ,η and θβ,η start from sβ,η = s∗ and θβ,η = θ and follow:


  sβ,η

=  ∂Φ .x, sβ,η, θβ,ηΣ − β  ∂l .sβ,η, yΣ ,

	

		


∀t ≥ 0 :

    β,η

β,η

η . ∂Φ .

β,η

β,η Σ

∂Φ .

β,η

β,η ΣΣ

(6)

The difference in C-EP compared to EP is that the value of the parameter used to update sβ,η  in

Eq. (6) is the current θβ,η, not θ. Provided the learning rate η is small enough, i.e. the synapses 
are
slow compared to the neurons, this effect is weak.  Intuitively, in the limit η       0, the 
parameter

changes are negligible so that θβ,η can be approximated by its initial value θβ,η = θ.  Under this

t                                                                                                   
     0

approximation, the dynamics of sβ,η in C-EP and the dynamics of sβ in EP are the same. See Fig. 3

t                                                               t

for a simple example, and Appendix A.3 for a proof in the general case.

3.3    GRADIENT-DESCENDING DYNAMICS (GDD)

Now we prove that, provided the hyperparameter β and the learning rate η are small enough, the
dynamics of the neurons and the weights given by Eq. (6) follow the gradients of BPTT (Theorem 1
and Fig. 2). For a formal statement of this property, we define the normalized (continual) updates 
of
C-EP, as well as the gradients of the loss L = l (sT , y) after T  time steps, computed with BPTT:

4


Under review as a conference paper at ICLR 2020

Figure 2: Gradient-Descending Dynamics (GDD, Theorem 1). In the second phase of Continual
Equilibrium Prop (C-EP), the dynamics of neurons and synapses descend the gradients of BPTT, i.e.

∆C−EP(t) =        BPTT(t). The colors illustrate when corresponding computations are realized in
C-EP and BPTT. Top left. 1ˢᵗ phase of C-EP with static input x and target y. The final state sT is 
the
steady state s∗. Bottom left. Backprop through time (BPTT). Bottom right. 2ⁿᵈ phase of C-EP. The
starting state sβ,η is the final state of the forward-time pass, i.e. the steady state s∗.

  ∆C−EP(β, η, t) =  1 .sβ,η − sβ,ηΣ ,             ∇BPTT(t) =     ∂L   ,

			

  ∆C−EP(β, η, t) =  1 .θβ,η − θβ,ηΣ ,             ∇BPTT(t) =     ∂L   .

More details about L and the gradients ∇BPTT(t) and ∇BPTT(t) are provided in Appendix B. Note

that injecting Eq. (6) in Eq. (7), the normalized updates of C-EP consequently read:

∆C−EP(β, η, t) =  1 . ∂Φ .x, sβ,η , θβ,ηΣ − ∂Φ .x, sβ,η, θβ,ηΣΣ ,                    (8)

which corresponds to the parameter gradient at time t, defined informally in Eq. (5). The following
result makes this statement more formal.

Theorem 1 (GDD Property).  Let s₀, s₁, . . . , sT  be the convergent sequence of states and denote
s   = sT the steady state. Further assume that there exists some step K where 0 < K      T  such 
that
s   = sT = sT  ₁ = . . . sT  K. Then, in the limit η      0 and β      0, the first K normalized 
updates
in the second phase of C-EP are equal to the negatives of the first K gradients of BPTT, i.e.

  lim   lim  ∆C−EP(β, η, t) = −∇BPTT(t),

	

C−EP                                BPTT

  lim   lim  ∆         (β, η, t) = −∇        (t).


Theorem 1 rewrites sβ,η

≈ sβ,η − β    ∂L

and θβ,η  ≈ θβ,η − η    ∂L

, showing that in the second


t+1         t

ˆ

∂sT −t

t+1          t

∂θT −t

phase of C-EP, neurons and synapses descend the gradients of the loss     obtained with BPTT, with

the hyperparameters β and η playing the role of learning rates for sβ,η and θβ,η, respectively. 
Fig. 3

t                t

illustrates Theorem 1 with a simple dynamical system for which the normalized updates ∆C−EP and
the gradients ∇BPTT  are analytically tractable - see Appendix  C for derivation details.

5


Under review as a conference paper at ICLR 2020

Figure 3: Illustration of Theorem 1 on a simple model. The variables s and θ are scalars, the first
phase equation is st₊₁ =  ¹ (st + θ), the steady state is denoted s∗ and the loss is L∗  =  ¹ s².  
For

completeness, we also include the corresponding gradients of Recurrent Backpropagation (RBP)

and the normalized updates of EP, denoted    RBP and ∆EP respectively. The equivalence between
C-EP, EP, RBP and BPTT holds in the general setting: see Appendix A for a thorough study of their
relationship.

4    NUMERICAL  EXPERIMENTS

In this section, we validate our continual version of Equilibrium Propagation against training on 
the
MNIST data set with two models. The first model is a vanilla RNN with tied and symmetric weights:
the dynamics of this model approximately derive from a primitive function, which allows training
with C-EP. The second model is a Discrete-Time RNN with untied and asymmetric weights, which is
therefore closer to biology. We train this second model with a modified version of C-EP which we
call C-VF (Continual Vector Field) as it is inspired from the algorithm with Vector-Field dynamics
of Scellier et al. (2018).  Ernoult et al. (2019) showed with simulations the intuitive result 
that, if
a model is such that the normalized updates of EP ‘match’ the gradients of BPTT (i.e. if they are
approximately equal), then the model trained with EP performs as well as the model trained with
BPTT. Along the same lines, we show in this work that the more the EP normalized updates follow
the gradients of BPTT before training, the best is the resulting training performance. We choose to
implement C-EP and C-VF on vanilla RNNs to accelerate simulations (Ernoult et al., 2019).

4.1    MODELS: VANILLA RNNS WITH SYMMETRIC AND ASYMMETRIC WEIGHTS

Vanilla RNN with symmetric weights trained by C-EP.    The first phase dynamics is defined as:

st₊₁ = σ (W · st + Wₓ · x) ,                                                 (10)

where σ is an activation function, W  is a symmetric weight matrix connecting the layers s and Wₓ is
a matrix connecting the input x to the layers s. Although the dynamics are not directly defined in
terms of a primitive function, note that st₊₁ ≈ ∂Φ (st, W ) with Φ(s, W ) =  ¹ sT · W · s if we 
ignore

the activation function σ. Following Eq. (6) and Eq. (7), we define the normalized updates of this

model as:


s                               β      t+1         t                   W

β      t+1

t+1         t

t

(11)

Note that this model applies to any topology as long as existing connections have symmetric values:
this includes deep networks with any number of layers – see Appendix E for detailed descriptions
of the models used in the experiments. More explicitly, for a network whose layers of neurons are
s⁰, s¹, ..., sN , with Wn,n₊₁ connecting the layers sⁿ⁺¹ and sⁿ in both directions, the 
corresponding
primitive function is Φ =     n(sⁿ)T  Wn,n₊₁   sn₊₁ + sNT    Wₓ   x - see Appendix E.1 for more

details.

6


Under review as a conference paper at ICLR 2020

Vanilla RNN with asymmetric weights trained by C-VF.    In this model, the dynamics in the
first  phase  is  the  same  as  Eq.  (10)  but  now  the  weight  matrix  W  is  no  longer  
assumed  to  be
symmetric, i.e. the reciprocal connections between neurons are not constrained. In this setting the

weight dynamics in the second phase is replaced by a version for asymmetric weights:  W β,η  =


W β,η + η sβ,ηT

· .sβ,η  − sβ,η

Σ, so that the normalized updates are equal to:

t+1

∆C−VF(β, η, t) =  1 .sβ,η − sβ,ηΣ ,         ∆C−VF(β, η, t) =  1 sβ,ηT  · .sβ,η − sβ,ηΣ .      (12)

Like the previous model, the vanilla RNN with asymmetric weights also applies to deep networks
with any number of layers. Although in C-VF the dynamics of the weights is not one of the form of
Eq. (6) that derives from a primitive function, the (bioplausible) normalized weight updates of Eq. 
(12)
can approximately follow the gradients of BPTT, provided that the values of reciprocal connections
are not too dissimilar: this is illustrated in Fig. 5 (as well as in Fig. 12 and Fig. 13 of 
Appendix E.6)
and proved in Appendix D.2. This property motivates the following training experiments.

Error (%)                 T       K     Epochs
Test             Train

EP-1h          2.00 ± 0.13      (0.20)      30      10         30

EP-2h          1.95 ± 0.10      (0.14)     100     20         50

C-EP-1h     2.28 ± 0.16    (0.41)      40      15        100

C-EP-2h     2.44 ± 0.14    (0.31)     100     20        150     

C-VF-1h     2.43 ± 0.08    (0.77)      40      15        100

C-VF-2h     2.97 ± 0.19    (1.58)     100     20        150     

Figure 4:  Left: Training results on MNIST with EP, C-EP and C-VF. "#h" stands for the number of
hidden layers. We indicate over 5 trials the mean and standard deviation for the test error (mean 
train
error in parenthesis). T  (resp. K) is the number of iterations in the 1ˢᵗ (resp. 2ⁿᵈ) phase. For 
C-VF
results, the initial angle between forward (θf ) and backward (θb) weights is Ψ(θf , θb) = 0◦. 
Right:

Test error rate on MNIST achieved by C-VF as a function of the initial Ψ(θf , θb).

4.2    C-EP TRAINING EXPERIMENTS

Experiments are performed with multi-layered vanilla RNNs (with symmetric weights) on MNIST.
The table of Fig. 4.1 presents the results obtained with C-EP training benchmarked against standard
EP training (Ernoult et al., 2019) - see Appendix E for model details and Appendix F.1 for training
conditions.  Although the test error of C-EP approaches that of EP, we observe a degradation in
accuracy. This is because although Theorem 1 guarantees Gradient Descending Dynamics (GDD) in
the limit of infinitely small learning rates, in practice we have to strike a balance between 
having a
learning rate that is small enough to ensure this condition but not too small to observe 
convergence

within a reasonable number of epochs. As seen on Fig. 5 (b), the finite learning rate η of continual
updates leads to ∆C−EP(β, η, t) curves splitting apart from the        BPTT(t) curves.  As seen per
Fig.  5 (a), this effect is emphasized with the depth: before training, angles between the 
normalized
updates of C-EP and the gradients of BPTT reach 50 degrees for two hidden layers.  The deeper

the network, the more difficult it is for the C-EP dynamics to follow the gradients provided by
BPTT. As an evidence, we show in Appendix F.2 that when we use extremely small learning rates

throughout the second phase (θ ← θ + ηtiny∆C−EP) and rescale up the resulting total weight update


(θ ← θ − ∆θ

tot

+    η   ∆θ

tiny

θ

tₒt), we recover standard EP results.

4.3    CONTINUAL VECTOR FIELD (C-VF) TRAINING EXPERIMENTS

Depending on whether the updates occur continuously during the second phase and the system obey
general dynamics with untied forward and backward weights, we can span a large range of deviations
from the ideal conditions of Theorem 1. Fig. 5 (b) qualitatively depicts these deviations with a 
model
for which the normalized updates of EP match the gradients of BPTT (EP) ; with continual weight

7


Under review as a conference paper at ICLR 2020

updates, the normalized updates and gradients start splitting apart (C-EP), and even more so if the
weights are untied (C-VF).

Protocol.    In order to create these deviations from Theorem 1 and study the consequences in terms
of training, we proceed as follows. For each C-VF simulations, we tune the initial angle between
forward weights (θf ) and backward weights (θb) between 0 and 180◦. We denote this angle Ψ(θf , θb)

- see Appendix F.1 for the angle definition and the angle tuning technique employed.  For each of

these weight initialization, we compute the angle between the total normalized update provided by
C-VF, i.e.  ∆C−VF(β, η, tot)  =      K−¹ ∆C−VF(β, η, t) and the total gradient provided by BPTT,


angle

.   C−VF  tot

t=0 BPTT  tot  Σ. Finally for each weight initialization, we perform training

in the discrete-time setting of Ernoult et al. (2019) - see Appendix E.4 for model details.   We
proceed in the same way for EP and C-EP simulations, computing          EP  tot          BPTT  tot  
  and
C−EP  tot          BPTT  tot    before training. We use the generic notation      tot   to denote 
the

total normalized update. This procedure yields (x, y) data points with x = Ψ  ∆(tot),       
BPTT(tot)

and y = test error, which are reported on Fig. 5 (a) - see Appendix F.1 for the full table of 
results.

Results.    Fig. 5 (a) shows the test error achieved on MNIST by EP, C-EP on the vanilla RNN with
symmetric weights and C-VF on the vanilla RNN with asymmetric weights for different number of
hidden layers as a function of the angle Ψ  ∆(tot),       BPTT(tot)   before training. This 
graphical
representation spreads the algorithms between EP which best satisfies the GDD property (leftmost
point in green at ∼ 20◦) to C-VF which satisfies the less the GDD property (rightmost points in red 
and

orange at ∼ 100  ). As expected., high angles between gradienΣts of C-VF and BPTT lead to high 
error

Fig. 5 shows the same data but focusing only on results generated by initial weight angles lying 
below

90◦, i.e. Ψ(θf , θb) = {0◦, 22.5◦, 45◦, 67.5◦, 90◦}. From standard EP wi.th one hidden layer to 
CΣ-VF

does not exceed 5.05% on average. This result confirms the importance of proper weight 
initialization
when weights are untied, also discussed in other context (Lillicrap et al., 2016).  When the initial
weight angle is of 0◦, the impact of untying the weights on classification accuracy remains 
constrained,
as shown in table of Fig. 4.1. Upon untying the forward and backward weights, the test error 
increases
by ∼ 0.2% with one hidden layer and by ∼ 0.5% with two hidden layers compared to standard C-EP.

5    DISCUSSION

Equilibrium Propagation is an algorithm that leverages the dynamical nature of neurons to compute
weight gradients through the physics of the neural network. C-EP embraces simultaneous synapse and
neuron dynamics, resolving the initial need of artificial memory units for storing the neuron values
between different phases.  The C-EP framework preserves the equivalence with Backpropagation
Through Time:  in the limit of sufficiently slow synaptic dynamics (i.e.  small learning rates), the
system satisfies Gradient Descending Dynamics (Theorem 1).  Our experimental results confirm
this theorem.  When training our vanilla RNN with symmetric weights with C-EP while ensuring
convergence in 100 epochs, a modest reduction in MNIST accuracy is seen with regards to standard
EP.     This accuracy reduction can be eliminated by using smaller learning rates and rescaling up 
the
total weight update at the end of the second phase (Appendix F.2). On top of extending the theory of
Ernoult et al. (2019), Theorem 1 also appears to provide a statistically robust tool for C-EP based
learning.  Our experimental results show as in Ernoult et al. (2019) that, for a given network with
specified neuron and synapse dynamics, the more the updates of Equilibrium Propagation follow
the gradients provided by Backpropagation Through Time before training (in terms of angle in this
work), the better this network can learn.

Our C-EP and C-VF algorithms exhibit features reminiscent of biology. C-VF extends C-EP training
to RNNs with asymmetric weights between neurons, as is the case in biology. Its learning rule, local
in space and time, is furthermore closely acquainted to Spike Timing Dependent Plasticity (STDP), a
learning rule widely studied in neuroscience, inferred in vitro and in vivo from neural recordings 
in
the hippocampus (Dan and Poo, 2004). In STDP, the synaptic strength is modulated by the relative
timings of pre and post synaptic spikes within a precise time window (Bi and Poo, 1998; 2001).

8


Under review as a conference paper at ICLR 2020

Figure 5:  Three versions of EP: standard Equilibrium Propagation (EP), Continual Equilibrium
Propagation (C-EP) and Continual Vector Field EP (C-VF). #-h denotes the number of hidden
layers.  (a): test error rate on MNIST as a function of the initial angle Ψ between the total 
normalized
update of EP and the total gradient of BPTT. (b): Dashed and continuous lines respectively represent
the normalized updates ∆θ(t) (i.e. ∆EP(t), ∆C−EP(t), ∆C−VF(t)) and the gradients −∇BPTT(t).

Each randomly selected synapse corresponds to one color.   While dashed and continuous lines

coincide for standard EP, they split apart upon untying the weights and using continual updates.

Strikingly, the same rule that we use for C-VF learning can approximate STDP correlations in a
rate-based formulation, as shown through numerical experiments by Bengio et al. (2015). From this
viewpoint our work brings EP a step closer to biology.  However, C-EP and C-VF do not aim at
being models of biological learning per se, in that it would account for how the brain works or how
animals learn, for which Reinforcement Learning might be a more suited learning paradigm. The
core motivation of this work is to propose a fully local implementation of EP, in particular to 
foster
its hardware implementation.

When computed on a standard computer, due to the use of small learning rates to mimic analog
dynamics within a finite number of epochs, training our models with C-EP and C-VF entail long
simulation times. With a Titan RTX GPU, training a fully connected architecture on MNIST takes 2
hours 39 mins with 1 hidden layer and 10 hours 49 mins with 2 hidden layers. On the other hand,
C-EP and C-VF might be particularly efficient in terms of speed and energy consumption when
operated on neuromorphic hardware that employs analog device physics (Ambrogio et al., 2018;
Romera et al., 2018).  To this purpose, our work can provide an engineering guidance to map our

algorithm onto a neuromorphic system.  Fig. 5 (a) shows that hyperparameters should be tuned
so that before training, C-EP updates stay within 90◦ of the gradients provided by BPTT. More
concretely in practice, it amounts to tune the degree of symmetry of the dynamics, for instance the
angle between forward and backward weights - see Fig. 4.1. Our work is one step towards bridging
Equilibrium Propagation with neuromorphic computing and thereby energy efficient implementations

of gradient-based learning algorithms.

REFERENCES

D. H. Ackley, G. E. Hinton, and T. J. Sejnowski.  A learning algorithm for boltzmann machines.

Cognitive science, 9(1):147–169, 1985.

L. B. Almeida.  A learning rule for asynchronous perceptrons with feedback in a combinatorial
environment. volume 2, pages 609–618, San Diego 1987, 1987. IEEE, New York.

9


Under review as a conference paper at ICLR 2020

S. Ambrogio, P. Narayanan, H. Tsai, R. M. Shelby, I. Boybat, C. Nolfo, S. Sidler, M. Giordano,

M. Bodini, N. C. Farinha, et al.  Equivalent-accuracy accelerated neural-network training using
analogue memory. Nature, 558(7708):60, 2018.

Y. Bengio, T. Mesnard, A. Fischer, S. Zhang, and Y. Wu. Stdp as presynaptic activity times rate of
change of postsynaptic activity. arXiv preprint arXiv:1509.05936, 2015.

G.-q. Bi and M.-m. Poo.  Synaptic modifications in cultured hippocampal neurons:  dependence
on spike timing, synaptic strength, and postsynaptic cell type.  Journal of neuroscience, 18(24):
10464–10472, 1998.

G.-q. Bi and M.-m. Poo.  Synaptic modification by correlated activity: Hebb’s postulate revisited.

Annual review of neuroscience, 24(1):139–166, 2001.

Y. Dan and M.-m. Poo. Spike timing-dependent plasticity of neural circuits. Neuron, 44(1):23–30,
2004.

M. Ernoult, J. Grollier, D. Querlioz, Y. Bengio, and B. Scellier. Updates of equilibrium prop match
gradients of backprop through time in an rnn with static input. arXiv preprint arXiv:1905.13633,
2019.

J. A. Hertz. Introduction to the theory of neural computation. CRC Press, 2018.

G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural computation,
14(8):1771–1800, 2002.

Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436, 2015.

T. P. Lillicrap, D. Cownden, D. B. Tweed, and C. J. Akerman. Random synaptic feedback weights
support error backpropagation for deep learning. Nature communications, 7:13276, 2016.

J. R. Movellan.  Contrastive hebbian learning in the continuous hopfield model.  In Connectionist
models, pages 10–17. Elsevier, 1991.

F. J. Pineda. Generalization of back-propagation to recurrent neural networks. 59:2229–2232, 1987.

M. Romera, P. Talatchian, S. Tsunegi, F. A. Araujo, V. Cros, P. Bortolotti, J. Trastoy, K. 
Yakushiji,

A. Fukushima, H. Kubota, et al. Vowel recognition with four coupled spin-torque nano-oscillators.

Nature, 563(7730):230, 2018.

B. Scellier and Y. Bengio. Equilibrium propagation: Bridging the gap between energy-based models
and backpropagation. Frontiers in computational neuroscience, 11, 2017.

B. Scellier and Y. Bengio. Equivalence of equilibrium propagation and recurrent backpropagation.

Neural computation, 31(2):312–329, 2019.

B. Scellier, A. Goyal, J. Binas, T. Mesnard, and Y. Bengio. Generalization of equilibrium 
propagation
to vector field dynamics. arXiv preprint arXiv:1808.04873, 2018.

E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in nlp.

arXiv preprint arXiv:1906.02243, 2019.

10


Under review as a conference paper at ICLR 2020

Appendix

A    PROOF  OF  THEOREM  1

In this appendix, we prove Theorem 1, which we recall here.

Theorem 1 (GDD Property).  Let s₀, s₁, . . . , sT  be the convergent sequence of states and denote
s   = sT the steady state. Further assume that there exists some step K where 0 < K      T  such 
that
s   = sT = sT  ₁ = . . . sT  K. Then, in the limit η      0 and β      0, the first K normalized 
updates
in the second phase of C-EP are equal to the negatives of the first K gradients of BPTT, i.e.

  lim   lim  ∆C−EP(β, η, t) = −∇BPTT(t),

	

C−EP                                BPTT

  lim   lim  ∆         (β, η, t) = −∇        (t).

A.1    A SPECTRUM OF FOUR COMPUTATIONALLY EQUIVALENT LEARNING ALGORITHMS

Proving Theorem 1 amounts to prove the equivalence of C-EP and BPTT. In fact we can prove the
equivalence of four algorithms, which all compute the gradient of the loss:

1.  Backpropagation Through Time (BPTT), presented in Section B.2,

2.  Recurrent Backpropagation (RBP), presented in Section B.3,

3.  Equilibrium Propagation (EP), presented in Section 2,

4.  Equilibrium Propagation with Continual Weight Updates (C-EP), introduced in Section 3.

In this spectrum of algorithms, BPTT is the most practical algorithm to date from the point of view
of machine learning, but also the less biologically realistic. In contrast, C-EP is the most 
realistic
in terms of implementation in biological systems, while it is to date the least practical and least
efficient for conventional machine learning (computations on standard Von-Neumann hardware are
considerably slower due to repeated parameter updates, requiring memory access at each time-step of
the  second phase).

A.2    SKETCH OF THE PROOF

Theorem 1 can be proved in three phases, using the following three lemmas.

Lemma 2 (Equivalence of C-EP and EP).  In the limit of small learning rate,  i.e.   η        0,  the
(normalized) updates of C-EP are equal to those of EP:

      lim       ∆C−EP(β, η, t) = ∆EP(β, t),

	

  η→0 (η>0)       θ                                   θ

Lemma 3 (Equivalence of EP and RBP).  Assume that the transition function derives from a prim-
itive function,  i.e.   that F  is of the form F (x, s, θ)  =  ∂Φ (x, s, θ).   Then,  in the limit 
of small
hyperparameter β, the normalized updates of EP are equal to the gradients of RBP:

       lim       ∆EP(β, t) = −∇RBP(t),

	


∀t ≥ 0 :

(14)

lim       ∆EP(β, t) = −∇RBP(t).

Lemma 4 (Equivalence of BPTT and RBP).  In the setting with static input x, suppose that the network
has reached the steady state s   after T     K steps, i.e. sT  K = sT  K₊₁ =       = sT  ₁ = sT = s 
 .
Then the first K gradients of BPTT are equal to the first K gradient of RBP, i.e.

.  ∇BPTT(t) = ∇RBP(t),

∇θ      (t) = ∇θ    (t).

Proofs of the Lemmas can be found in the following places:

11


Under review as a conference paper at ICLR 2020

The link between BPTT and RBP (Lemma 2) is known since the late 1980s and can be
found e.g. in Hertz (2018). We also prove it here in Appendix B.

•  Lemma 3 was proved in Scellier and Bengio (2019) in the setting of real-time dynamics.

•  Lemma 4 is the new ingredient contributed here, and we prove it in Appendix A.3.
Also a direct proof of the equivalence of EP and BPTT was derived in Ernoult et al. (2019).

A.3    EQUIVALENCE OF C-EP AND EP

First, recall the dynamics of C-EP in the second phase: starting from sβ,η = s∗ and θβ,η = θ we 
have


  sβ,η

0                            0

=  ∂Φ .x, sβ,η, θβ,ηΣ − β  ∂l .sβ,η, yΣ ,

	

		


∀t ≥ 0 :



β,η

β,η

η . ∂Φ .

β,η

β,η Σ

∂Φ .

β,η

β,η ΣΣ

(16)


∀t ≥ 0 :



C−EP

1 .  β,η

β,η Σ


∆θ         (β, η, t) =  η

We also recall the dynamics of EP in the second phase:

θt+1 − θt        .

sβ = s      and    sβ    =  ∂Φ .x, sβ, θΣ − β  ∂l .sβ, yΣ ,                           (18)

as well as the normalized updates of EP, as defined in Ernoult et al. (2019):


  ∆EP(β, t) =  1 .sβ

− sβΣ ,

		


∀t ≥ 0 :          EP



1 . ∂Φ .     β

Σ     ∂Φ .

β     ΣΣ

(19)

Lemma 2 (Equivalence of C-EP and EP).  In the limit of small learning rate,  i.e.   η        0,  
the

(normalized) updates of C-EP are equal to those of EP:

      lim       ∆C−EP(β, η, t) = ∆EP(β, t),

	

  η→0 (η>0)       θ                                   θ

Proof of Lemma 2.  We want to compute the limits of ∆C−EP(β, η, t) and ∆C−EP(β, η, t) as η → 0

with η > 0. First of all, note that under mild assumptions – which we made here – of regularity on

the functions Φ and l (e.g. continuous differentiability), for fixed t and β, the quantities sβ,η 
and θβ,η

t                t

are continuous as functions of η ; this is straightforward from the form of Eq. (16). As a 
consequence,

∆C−EP(β, η, t) is a continuous function of η, which implies in particular that

lim       ∆C−EP(β, η, t) = ∆C−EP(β, 0, t).                                   (20)

s                                   s

η→0 (η>0)

Now, taking η = 0 in the bottom equation of Eq. (16) yields the recurrence relation θβ,⁰  = θβ,⁰, 
so

t+1         t

that θβ,⁰ = θβ,⁰ = θ for every t. Injecting θβ,⁰ = θ in the top equation of Eq. (16) yields for 
sβ,⁰ the

t             0                                                            t                        
                                                                     t

same recurrence relation as that of sβ (Eq. 18), so that sβ,⁰ = sβ for every t. Therefore, for η = 
0,


we have

t                                     t             t

∆C−EP(β, 0, t) =  1 .sβ,⁰  − sβ,⁰Σ =  1 .sβ    − sβΣ = ∆EP(β, t).                 (21)

It follows from Eq. (20) and Eq. (21) that

lim       ∆C−EP(β, η, t) = ∆EP(β, t).                                       (22)

s                                   s

η→0 (η>0)

12


Under review as a conference paper at ICLR 2020

Now let us compute limη→₀ ₍η>₀₎  ∆C−EP(β, η, t). Using Eq. (16), we have

∆C−EP(β, η, t) =  1 .θβ,η − θβ,ηΣ                                                              (23)

=  1 . ∂Φ .x, sβ,η , θβ,ηΣ − ∂Φ .x, sβ,η, θβ,ηΣΣ .                  (24)

Similarly as before, for fixed t, ∂Φ .x, sβ,η, θβ,ηΣ is a continuous function of η. Therefore

lim       ∆C−EP(β, η, t) =  1 . ∂Φ .x, sβ,⁰ , θβ,⁰Σ − ∂Φ .x, sβ,⁰, θβ,⁰ΣΣ                 (25)

=  1 . ∂Φ .x, sβ   , θΣ − ∂Φ .x, sβ, θΣΣ = ∆EP(β, t).        (26)

A consequence of Lemma 2 is that the total update of C-EP matches the total update of EP in the
limit of small η, so that we retrieve the standard EP learning rule of Eq. (4). More explicitly, 
after K
steps in the second phase and starting from θβ,η = θ₀:


K

t=0
K−1

t+1         t

θ                                                                θ

t=0

K−1

≈   Σ η∆EP(β, t)        (Lemma 2)


η40   t=0
K−1

1 . ∂Φ

	

∂Φ                    Σ

	

=  η . ∂Φ(x, sβ , θ  ) − ∂Φ(x, s  , θ  )Σ

B    EQUIVALENCE  OF  BPTT AND  RBP

In  this  section,  we  recall  Backprop  Through  Time  (BPTT)  and  the  Almeida-Pineda  Recurrent
Backprop (RBP) algorithm, which can both be used to optimize the loss    ∗ of Eq. 3. Historically,
BPTT and RBP were invented separately around the same time. RBP was introduced at a time when
convergent RNNs (such as the one studied in this paper) were popular. Nowadays, convergent RNNs

are less popular ; in the field of deep learning, RNNs are almost exclusively used for tasks that 
deal
with sequential data and BPTT is the algorithm of choice to train such RNNs. Here, we present RBP
in a way that it can be seen as a particular case of BPTT.

B.1    SKETCH OF THE PROOF OF LEMMA 4

Lemma 4, which we recall here, is a consequence of Proposition 5 and Definition 6 below.

Lemma 4 (Equivalence of BPTT and RBP).  In the setting with static input x, suppose that the network
has reached the steady state s   after T     K steps, i.e. sT  K = sT  K₊₁ =       = sT  ₁ = sT = s 
 .
Then the first K gradients of BPTT are equal to the first K gradient of RBP, i.e.

.  ∇BPTT(t) = ∇RBP(t),

∇θ      (t) = ∇θ    (t).

However, in general, the gradients    BPTT(t) of BPTT and the gradients    RBP(t) of RBP are not
equal for t > K. This is because BPTT and RBP compute the gradients of different loss functions:

13


Under review as a conference paper at ICLR 2020

•  BPTT computes the gradient of the loss after T  time steps, i.e. L = l (sT , y),

•  RBP computes the gradients of the loss at the steady state, i.e. L∗ = l (s∗, y).

B.2    BACKPROPAGATION THROUGH TIME (BPTT)

Backpropagation Through Time (BPTT) is the standard method to train RNNs and can also be used
to train the kind of convergent RNNs that we study in this paper. To this end, we consider the cost 
of
the state sT after T  time steps, denoted     = l (sT , y), and we substitute the loss after T  
time steps
as a proxy for the loss at the steady state    ∗ = l (s  , y). The gradients of     can then be 
computed
with BPTT.

To do this, we recall some of the inner working mechanisms of BPTT. Eq. (1) rewrites in the form
st₊₁ = F (x, st, θt₊₁ = θ), where θt denotes the parameter of the model at time step t, the value
θ being shared across all time steps. This way of rewriting Eq. (1) enables us to define the partial
derivative  ∂L as the sensitivity of the loss L with respect to θt when θ₁, . . . θt−₁, θt₊₁, . . . 
θT remain
fixed (set to the value θ). With these notations, the gradient ∂L reads as the sum:

∂L =  ∂L  +  ∂L  + · · · +  ∂L .                                              (27)


∂θ       ∂θ₁

∂θ₂

∂θT

BPTT computes the ’full’ gradient ∂L by first computing the partial derivatives  ∂L and  ∂L 
iteratively,

∂θ                                                                              ∂st              
∂θt

backward in time, using the chain rule of differentiation. In this work, we denote the gradients 
that
BPTT computes:

  ∇BPTT(t) =     ∂L   ,

	

  ∇BPTT(t) =     ∂L   .

Proposition 5 (Gradients of BPTT).  The gradients ∇BPTT(t) and ∇BPTT(t) satisfy the recurrence

s                            θ

relationship

BPTT              ∂l

∇s      (0) =  ∂s (sT , y) ,                                                     (29)


BPTT              ∂F

∀t = 1, 2, . . . , T,         ∇        (t) =        (x, s

, θ)T · ∇BPTT(t − 1),               (30)


BPTT              ∂F

∀t = 1, 2, . . . , T,         ∇        (t) =        (x, s

, θ)T · ∇BPTT(t − 1).               (31)

B.3    FROM BACKPROP THROUGH TIME (BPTT) TO RECURRENT BACKPROP (RBP)

In general,  to apply BPTT, it is necessary to store in memory the history of past hidden states

s₁, s₂, . . . , sT in order to compute the gradients ∇BPTT(t) and ∇BPTT(t) as in Eq. 30-31. 
However,

in our specific setting with static input x, if the network has reached the steady state s∗ after T 
− K
steps, i.e. if sT −K = sT −K₊₁ = · · · = sT −₁ = sT = s∗, then we see that, in order to compute the
first K gradients of BPTT, all one needs to know is ∂F (x, s∗, θ) and ∂F (x, s∗, θ). To this end, 
all

one needs to keep in memory is the steady state s∗. In this particular setting, it is not necessary 
to
store the past hidden states sT , sT −₁, . . . , sT −K since they are all equal to s∗.

The Almeida-Pineda algorithm (a.k.a.  Recurrent Backpropagation, or RBP for short), which was
invented independently by Almeida (1987) and Pineda (1987), relies on this property to compute
the gradients of the loss L∗ using only the steady state s∗. Similarly to BPTT, it computes 
quantities

∇RBP(t) and ∇RBP(t), which we call ‘gradients of RBP’, iteratively for t = 0, 1, 2, . . .

14


Under review as a conference paper at ICLR 2020

Definition 6 (Gradients of RBP).  The gradients ∇RBP(t) and ∇RBP(t) are defined and computed


iteratively as follows:

s                         θ

RBP              ∂l

∇s    (0) =  ∂s (s∗, y) ,                                                    (32)

∀t ≥ 0,         ∇RBP(t + 1) =  ∂F  (x, s  , θ)T · ∇RBP(t),                           (33)

∀t ≥ 0,         ∇RBP(t + 1) =  ∂F  (x, s  , θ)T · ∇RBP(t).                           (34)

Unlike in BPTT where keeping the history of past hidden states is necessary to compute (or ‘back-
propagate’) the gradients, in RBP Eq. 33-34 show that it is sufficient to keep in memory the steady
state        s   only in order to iterate the computation of the gradients. RBP is more memory 
efficient than
BPTT.


Algorithm 3 BPTT                                             

Input: x, y, θ.

Output: θ.

1:  s₀         0

2:  for t = 0 to T     1 do
3:         st₊₁      F (x, st, θ)
4:  end for

Algorithm 4 RBP                                                  

Input: x, y, θ.

Output: θ.

1:  s₀         0

2:  repeat

3:         st₊₁ ← F (x, st, θ)

4:  until st = s∗ ∂A


BPTT               ∂A

5:  ∇RBP(0) ←      (s∗, y)


5:      s         (0)           (sT , y)

6:  for t = 1 to T  do

7:         ∇  (t) ← ∂F  (x, sT −t, θ)T · ∇  (t − 1)

s                    ∂s

6:  repeat

7:         ∇RBP(t) ← ∂F  (x, s∗, θ)T · ∇RBP(t − 1)

8:         ∇  (t) ← ∂F  (x, sT −t, θ)T · ∇  (t − 1)     8:         ∇RBP(t) ← ∂F  (x, s∗, θ)T · 
∇RBP(t − 1)


9:  end for

		

9:  until ∇RBP(t) = 0.

		


10:  ∇BPTT(tot) ←

T −1 ∇BPTT(t)

θ

10:  ∇RBP

← Σ∞

∇RBP

Figure 6: Left. Pseudo-code of BPTT. The gradients ∇(t) denote the gradients ∇BPTT(t) of BPTT.

Right.   Pseudo-code of RBP. Difference between BPTT and RBP. In BPTT, the state sT −t is
required to compute ∂F (x, sT −t, θ) and ∂F (x, sT −t, θ) ; thus it is necessary to store in memory 
the

sequence of states s₁, s₂, . . . , sT . In contrast, in RBP, only the steady state s∗ is required 
to compute

∂F  (x, s∗, θ) and ∂F (x, s∗, θ) ; it is not necessary to store the past states of the network.

B.4    WHAT ‘GRADIENTS’ ARE THE GRADIENTS OF RBP?

In this subsection we motivate the name of ‘gradients’ for the quantities ∇RBP(t) and ∇RBP(t) by

proving that they are the gradients of    ∗ in the sense of Proposition 7 below.  They are also the
gradients of what we call the ‘projected cost function’ (Proposition 8), using the terminology of
Scellier and Bengio (2019).

Proposition 7 (RBP Optimizes L∗).  The total gradient computed by the RBP algorithm is the
gradient of the loss L∗ = l (s∗, y), i.e.

Σ∞  ∇RBP(t) =  ∂L∗ .                                                       (35)

∇RBP(t) and ∇RBP(t) can also be expressed as gradients of Lt = l (st, y), the cost after t time

steps. In the terminology of Scellier and Bengio (2019),    t was named the projected cost. For t = 
0,

₀ is simply the cost of the initial state s₀. For t > 0,    t is the cost of the state projected a 
duration t

in the future.

Proposition 8 (Gradients of RBP are Gradients of the Projected Cost).  The ‘RBP gradients’ ∇RBP(t)

and ∇RBP(t) can be expressed as gradients of the projected cost:


∀t ≥ 0,         ∇RBP(t) =  ∂Lt           ,         ∇RBP(t) =  ∂Lt  

(36)


s                    ∂s₀  

θ

s0 =s∗

15

∂θ₀  

s0 =s∗


Under review as a conference paper at ICLR 2020

where the initial state s₀ is the steady state s∗.

Proof of Proposition 7.  First of all, by Definition 6 (Eq. 32-34) it is straightforward to see 
that


∀t ≥ 0,         ∇s    (t) =

. ∂F  (x, s  , θ)

TΣt

∂l

· ∂s (s∗, y) ,                                         (37)


∀t ≥ 1,         ∇θ    (t) =

Second, recall that the loss L∗ is

∂F

∂θ  (x, s∗, θ)

∗

· . ∂F  (x, s  , θ)

TΣt−1

∂l

· ∂s (s∗, y) .          (38)


where

L   = l (s∗, y) ,                                                            (39)

s∗ = F (x, s∗, θ) .                                                          (40)

By the chain rule of differentiation, the gradient of L∗ (Eq. 39) is

∂L∗  =  ∂l (s  , y) · ∂s∗ .                                                    (41)

In order to compute ∂ˢ∗ , we differentiate the steady state condition (Eq. 40) with respect to θ, 
which


yields

∂s∗  =  ∂F  (x, s  , θ) · ∂s∗  + ∂F  (x, s  , θ) .                                    (42)


Rearranging  the  terms,  and  using  the  Taylor  expansion  (Id − A)−¹   =   Σ∞

Aᵗ  with  A   =


∂F  (x, s∗, θ), we get

∂s∗ =

∂θ

∂F

Id −  ∂s  (x, s∗, θ)

Σ−1

∂F

·  ∂θ  (x, s∗, θ)                                  (43)


Therefore

∞    ∂F

=            ∂s         ∗

t=0

ᵗ   ∂F

·  ∂θ  (x, s∗, θ) .                                     (44)


∂L∗  =  ∂l (s  , y) · ∂s∗

(45)


∂θ        ∂s     ∗

Σ ∂l

∂θ

. ∂F

Σt    ∂F


=

t=0

∞

∂s (s∗, y) ·

∂s  (x, s∗, θ)

·  ∂θ  (x, s∗, θ)                          (46)

= Σ ∇RBP(t).                                                                                   (47)

t=0

Proof of Proposition 8.  By the chain rule of differentiation we have

∂Lt+1  =  ∂F  (x, s  , θ)T · ∂Lt+1 .                                            (48)


∂s₀

∂s         ⁰

∂s₁

Evaluation this expression for s₀ = s∗ we get


∂Lt+1  

=  ∂F  (x, s  , θ)T ·  ∂Lt+1  

.                                 (49)


∂Lt+1  

=  ∂Lt+1  

=  ∂Lt  

(50)


∂s₁

 

 s0 =s∗

∂s₁

 s1 =s∗

∂s0  s0 =s∗

0

 

16


Under review as a conference paper at ICLR 2020

C    ILLUSTRATING  THE  EQUIVALENCE  OF  THE  FOUR  ALGORITHMS  ON  AN
ANALYTICALLY  TRACTABLE  MODEL

Model.    To illustrate the equivalence of the four algorithms (BPTT, RBP, EP and CEP), we study a
simple model with scalar variable s and scalar parameter θ:


s   = 0,         s

1

=     (s

+ θ) ,         L∗ =  1 s²,                                 (51)

where s   is the steady state of the dynamics (it is easy to see that the solution is s    =  θ).  
The
dynamics rewrites st₊₁ = F (st, θ) with the transition function F (s, θ) =  ¹ (s + θ), and the loss
rewrites    ∗ = l (s∗) with the cost function l(s) =  ¹ s².  Furthermore, a primitive function of 
the
system ¹ is Φ(s, θ)  =  ¹ (s + θ)².  This model has no practical application ; it is only meant for

pedagogical purpose.

Backpropagation Through Time (BPTT).    With BPTT, an important point is that we approximate
the steady state s∗ by the state after T  time steps sT , and we approximate L∗ (the loss at the 
steady
state) by the loss after T  time steps L = l (sT ).

In order to compute (i.e. ‘backpropagate’) the gradients of BPTT, Proposition 5 tells us that we 
need
to compute  ∂A (sT ) = sT , ∂F (st, θ) =  ¹ and ∂F (st, θ) =  ¹ . We get


∂s                         ∂s

2            ∂θ

BPTT

2

sT                 BPTT               sT

∀t = 0, 1, . . . , T − 1,         ∇s      (t) =  2t ,         ∇θ      (t) =  2t+1 .                
(52)

Recurrent Backpropagation (RBP).    Similarly, to compute the gradients of RBP, Definition 6
tells us that we need to compute  ∂A (s∗) = s∗, ∂F (s∗, θ) =  ¹ and ∂F (s∗, θ) =  ¹ . We have

∀t ≥ 0,         ∇RBP(t) =  s∗ ,         ∇RBP(t) =    s∗   .                              (53)

The state after T  time steps in BPTT converges to the steady state s   as T            , therefore 
the
gradients of BPTT converge to the gradients of RBP. Also notice that the steady state of the 
dynamics
is s∗ = θ.

Equilibrium  Propagation  (EP).    Following  the  equations  governing  the  second  phase  of  EP
(Fig. 1), we have:


sβ = θ,         sβ

= . 1 −   Σ  β        1

(54)


This linear dynamical system can be solved analytically:

     θ      .           . 1

ΣtΣ

Notice that sβ → θ as β → 0 ; for small values of the hyperparameter β, the trajectory in the second
phase is close to the steady state s∗ = θ.

Using Eq. 19, it follows that the normalized updates of EP are

EP                        θ                t                 EP                          θ          
        t


∀t ≥ 0,         ∆s    (β, t) = − 2t  (1 − 2β)  ,         ∆θ    (β, t) = − 2t+1  (1 − 2β)

.        (56)

Notice again that the normalized updates of EP converge to the gradients of RBP as β → 0.

Continual Equilibrium Propagation (C-EP).    The system of equations governing the system is:


.   β,η

  sβ,η

= . 1 − βΣ sβ,η

1  β,η ,


t     0 :

θ      = θ,                           

β,η

β,η

η  .  β,η

β,η Σ

17


Under review as a conference paper at ICLR 2020

First, rearranging the terms in the second equation, we get


It follows that

η      t+1         t

2β      t+1         t

∆C−EP(β, η, t) =  1 ∆C−EP(β, η, t).                                           (59)

θ                               2    s

Therefore, all we need to do is to compute ∆C−EP(β, η, t). Second, by iterating the second equation


over all indices from t = 0 to t − 1 we get

θβ,η = θ +   η

.sβ,η − s  Σ .                                                 (60)

Using s∗ = θ and plugging this into the first equation we get


sβ,η

= . 1 − β +   η  Σ sβ,η + . 1 −  η  Σ θ.                                   (61)

Solving this linear dynamical system, and using the initial condition sβ,η = θ we get


sβ,η =           θ          Σ1 −  η

 

+ 2β

. 1 Σt .

1 − 2β +

η  ΣtΣ

(62)


Finally:

ᵗ               η  + 2β

2β              2

θ  .

2β

η  Σt

18


Under review as a conference paper at ICLR 2020

D    COMPLEMENT  ON  GRADIENT-DESCENDING  DYNAMICS  (GDD)

Step-by-step equivalence of the dynamics of EP and gradient computation in BPTT was shown in
Ernoult et al. (2019) and was refered to as the Gradient-Descending Updates (GDU) property.  In
this appendix, we first explain the connection between the GDD property of this work and the GDU
property of Ernoult et al. (2019). Then we prove another version of the GDD property (Theorem 9
below), more general than Theorem 1.

D.1    GRADIENT-DESCENDING UPDATES (GDU) OF ERNOULT ET AL. (2019)

The GDU property of Ernoult et al. (2019) states that the (normalized) updates of EP are equal to 
the
gradients of BPTT. Similarly, the Gradient-Descending Dynamics (GDD) property of this work states
that the normalized updates of C-EP are equal to the gradients of BPTT. The difference between the
GDU property and the GDD property is that the term ‘update’ has slightly different meanings in the
contexts of EP and C-EP. In C-EP, the ‘updates’ are the effective updates by which the neuron and
synapses are being dynamically updated throughout the second phase. In contrast in EP, the ‘updates’
are effectively performed at the end of the second phase.

D.2    A GENERALISATION OF THE GDD PROPERTY

The Gradient Descending Dynamics property (GDD, Theorem 1) states that,  when the system
dynamics derive from a primitive function, i.e. when the transition function F is of the form F  =  
∂Φ ,
then the normalized updates of C-EP match the gradients provided by BPTT. Remarkably, even in
the case of the C-VF dynamics that do not derive from a primitive function Φ, Fig. 5 shows that the
biologically realistic update rule of C-VF follows well the gradients of BPTT. More illustrations of
this property are shown on Fig. 12 and Fig. 13. In this section we give a theoretical justification 
for
this fact by proving a more general result than Theorem 1.

First, recall the dynamics of the C-VF model. In the first phase:

st₊₁ = σ (W · st) ,                                                         (64)

where σ is an activation function and W  is a square weight matrix. In the second phase, starting 
from

sβ,η = s∗ and W β,η = W , the dynamics read:


0                               0

  sβ,η

= σ .W β,η · sβ,η Σ − β  ∂l .sβ,η Σ ,

	 		


∀t ≥ 0 :

  W β,η  = W β,η +  η sβ,ηT · .sβ,η  − sβ,η Σ .                         (65)

Now let us define the transition function F (s, W ) = σ(W   s), so that the dynamics of the first 
phase

rewrites

st₊₁ = F (st, W ) .                                                         (66)

As for the second phase, notice that  ∂F (s, W )  =  σ′(W  · s) · s, so that if we ignore the 
factor


σ′(W · s), Eq. (65) rewrites

  sβ,η

= F .sβ,η , W β,η Σ − β  ∂l .sβ,η Σ ,

	


∀t ≥ 0 :

t+1

      β,η

t

β,η

t

η  ∂F

∂s

β,η

t

β,η ΣT

.  β,η

β,η Σ

(67)

Now, recall the definition of the normalized updates of C-VF, as well as the gradients of the loss

L = l (sT , y) after T  time steps, computed with BPTT:

  ∆C−VF(β, η, t) =  1 .sβ,η − sβ,ηΣ ,                 ∇BPTT(t) =     ∂L   ,

			

  ∆C−VF(β, η, t) =  1 .W β,η − W β,ηΣ ,             ∇BPTT(t) =      ∂L    .

The loss L and the gradients ∇BPTT(t) and ∇BPTT(t) are defined formally in Appendix B.2.

19


Under review as a conference paper at ICLR 2020

Theorem 9 (Generalisation of the GDD Property).  Let s₀, s₁, . . . , sT be the convergent sequence
of states and denote s∗ = sT the steady state. Further assume that there exists some step K where
0  <  K  ≤ T  such that s∗ =  sT  =  sT −₁  =  . . . sT −K.  Finally, assume that the Jacobian of 
the
transition function at the steady state is symmetric, i.e.  ∂F (s∗, W ) =  ∂F (s∗, W )T. Then, in 
the

limit η      0 and β      0, the first K normalized updates of C-VF follow the the first K 
gradients of

BPTT, i.e.

  lim  lim ∆C−VF(β, η, t) = −∇BPTT(t),

		

  lim  lim ∆C−VF(β, η, t) = −∇BPTT(t).

β→0 η→0

A few remarks need to be made:

1.  Observe that

∂F (s, W ) = σ′(W   s)   W T.                                         (70)

∂s

Ignoring the factor σ′(W   s), we see that if W  is symmetric then the Jacobian of F  is also
symmetric, in which case the conditions of Theorem 9 are met.

2.  Theorem 1 is a special case of Theorem 9. To see why, notice that if the transition function

F  is of the form F (s, W ) =  ∂Φ (s, W ), then


∂F

(s, W ) =

∂s

∂²Φ

∂s2  (s, W ) =

∂F

(s, W )

∂s

(71)

In this case the extra assumption in Theorem 9 is automatically satisfied.

D.3    PROOF OF THEOREM 9

Theorem 9 is a consequence of Proposition 5 (Appendix B.2), which we recall here, and Lemma 10
below.

Proposition 5 (Gradients of BPTT).  The gradients ∇BPTT(t) and ∇BPTT(t) satisfy the recurrence


relationship

s                            θ

BPTT              ∂l

∇s      (0) =  ∂s (sT , y) ,                                                     (29)


BPTT              ∂F

∀t = 1, 2, . . . , T,         ∇        (t) =        (x, s

, θ)T · ∇BPTT(t − 1),               (30)


BPTT              ∂F

∀t = 1, 2, . . . , T,         ∇        (t) =        (x, s

, θ)T · ∇BPTT(t − 1).               (31)

Lemma 10 (Updates of C-VF).  Define the (normalized) neural and weight updates of C-VF in the
limit η → 0 and β → 0:

  ∆C−VF(t) =  lim  lim ∆C−VF(β, η, t),

	


θ

They satisfy the recurrence relationship

β→0 η→0

∆C−VF(0) = − ∂l (s  , y) ,                                                 (73)

∀t ≥ 0,         ∆C−VF(t + 1) =  ∂F  (x, s  , θ) · ∆C−VF(t),                           (74)

∀t ≥ 0,         ∆C−VF(t + 1) =  ∂F  (x, s  , θ)T · ∆C−VF(t).                         (75)

The proof of Lemma 10 is similar to the one provided in Ernoult et al. (2019).

20


Under review as a conference paper at ICLR 2020

E    MODELS

In this section, we describe the C-EP and C-VF algorithms when implemented on multi-layered
models, with tied weights and untied weights respectively. In the fully connected layered 
architecture
model, the neurons are only connected between two consecutive layers (no skip-layer connections and
no lateral connections within a layer). We denote neurons of the n-th layer as sⁿ with n     [0, N  
   1],
where N is the number of hidden layers. Layers are labelled in a backward fashion: n = 0 labels
the output layer, n = 1 the first hidden layer starting from the output layer, and n = N     1 the 
last
hidden layer (before the input layer).  Thus, there are N  hidden layers in total.  Fig. 7 shows 
this
architecture with N  = 2. Each model are presented here in a "real-time" and "discrete-time" 
settings
For    each model we lay out the equations of the neuron and synapse dynamics, we demonstrate the
GDD property and we specify in which part of the main text they are used.

We present in this order:

1.  Discrete-Time RNN with symmetric weights trained with EP,

2.  Discrete-Time RNN with symmetric weights trained with C-EP,

3.  Real-Time RNN with symmetric weights trained with C-EP,

4.  Discrete-Time RNN with asymmetric weights trained with C-VF,

5.  Real-Time RNN with asymmetric weights trained with C-VF.

Our ‘Discrete-Time RNN model’ is also commonly called ‘vanilla RNN’, and it is refered to as the
‘prototypical model’ in Ernoult et al. (2019). Our ‘Real-Time RNN model with symmetric weights’
is also commonly called ‘continuous Hopfield model’ and is refered to as the ‘energy-based model’
in Ernoult et al. (2019).

Demonstrating the Gradient Descending Dynamics (GDD) property (Theorem 1) on MNIST.
For this experiment,  we consider the 784-512-. . . -512-10 network architecture,  with 784 input
neurons,  10 ouput neurons,  and 512 neurons per hidden layer.   The activation function used is
σ(x)  = tanh(x). The experiment consists of the following: we take a random MNIST sample (of
size 1     784) and its associated target (of size 1     10). For a given value of the 
time-discretization
parameter ϵ, we perform the first phase for T  steps. Then, we perform on the one hand BPTT over K
steps (to compute the gradients    BPTT), on the other hand C-EP (or C-VF) over K steps for given

values of β and η (to compute the normalized updates ∆C−EP or ∆C−VF) and compare the gradients
and normalized updates provided by the two algorithms. Precise values of the hyperparameters ϵ, T ,
K, β and η are given in Tab. E.6.

Figure 7: Fully connected layered architecture with N  = 2 hidden layers.

E.1    DISCRETE-TIME RNN WITH SYMMETRIC WEIGHTS TRAINED WITH EP

Context of use.    This model is used for training experiments in Section 4.2 and Table 4.1.

Equations with N  =  2.    We consider the layered architecture of Fig. 7, where s⁰ denotes the
output layer, and the feedback connections are constrained to be the transpose of the feedforward
connections, i.e. Wnn−₁ = WnT  1n. In the discrete-time setting of EP, the dynamics of the first 
phase

21


Under review as a conference paper at ICLR 2020


are defined as:

  s⁰    = σ .W₀₁ · s¹Σ ,

			

t+1           .           t           01      tΣ


  s0,β

= σ .W₀₁ · s¹,βΣ + β ϵ  .y − s⁰,βΣ ,

		


∀t ∈ [0, K] :   s¹,β

= σ .W₁₂ · s²,β + W T · s⁰,βΣ ,

(77)


  s2,β

= σ .W₂ₓ · x + W T · s¹,βΣ .

Φ  x, s², s¹, s⁰   = (s⁰)T · W₀₁ · s¹ + (s¹)T · W₁₂ · s² + (s²)T · W₂ₓ · x.             (78)
We can compute, for example:


∂Φ

∂s1  = W₁₂

· s² + W0T1 · s⁰.                                                  (79)

Comparing Eq. (76) and Eq. (79), and ignoring the activation function σ, we can see that


s¹ ≈  ∂Φ  .x, s²

, s¹

, s⁰

Σ .                                               (80)


t

And similarly for the layers s⁰ and s².

∂s1

t−1

t−1

t−1

According to the definition of ∆C−EP in Eq. (19), for every layer and every t ∈ [0, K]:


  ∆EP

(β, t) =  ¹ .s²,β  · xT − s²,β · xTΣ ,

				


  ∆EP  (β, t) =  ¹ .s¹,β  · s²,βT  − s¹,β · s²,βT Σ ,

(81)

  ∆EP  (β, t) =  ¹ .s⁰,β  · s¹,βT  − s⁰,β · s¹,βT Σ .

Simplifying the equations with N  = 2.    To go from our multi-layered architecture to the more
general model presented in section 4.1. we define the state s of the network as the concatenation of
all the layers’ states, i.e. s = (s², s¹, s⁰)T and we define the weight matrices W  and Wₓ as:


0       W1T2          0

W  =     W₁₂      0       W0T1      ,         Wₓ =
0       W₀₁          0

Note that Eq. (76) and Eq. (78) can be vectorized into:

W2x

0       .                               (82)

0

st₊₁ = σ(W · st + Wₓ · x),                                                         (83)


Φ =  1 sT · W ·

1

s +    s

2

· Wₓ

· x.                                           (84)

Generalizing the equations for any N .    For a general architecture with a given N , the dynamics
of the first phase are defined as:

  s⁰    = σ .W₀₁ · s¹Σ

 	 


t+1           .

t

n+1            T

n−1Σ


∀t ∈ [0, T ] :

n

t+1

= σ   Wnn+1 · st       + Wn−1n · stΣ

∀n ∈ [1, N − 1]

(85)

and those of the second phase as:

22


Under review as a conference paper at ICLR 2020


0,β

   t+1

= σ .W₀₁ · s¹,βΣ + β(y − s⁰,β)

		


∀t ∈ [0, K] :   sⁿ,β  = σ .Wnn₊₁ · sⁿ⁺¹,β + W T

· sⁿ−¹,βΣ        ∀n ∈ [1, N − 1]

(86)


  sN,β = σ .WN,ₓ · x + W T

· sN −1,β Σ ,


t+1

where y denotes the target. Defining:

N −1N      t

N −1

Φ(x, s   , . . . , s  ) =         s     · Wnn₊₁ · s       + s    · WN,ₓ · x,                      
(87)

n=0

ignoring the activation function σ, Eq. (85) rewrites:


n
t+1

∂Φ

≈ ∂sn (x, s   , . . . , s  )        ∀n ∈ [1, N − 1]                                 (88)

According to the definition of ∆C−EP in Eq. (19), for every layer Wnn₊₁ and every t ∈ [0, K]:


  ∆EP

(β, t) =  ¹ .sN,β · xT − sN,β · xTΣ ,

				


  ∆EP

(β, t) =  ¹ .sⁿ,β · sⁿ⁺¹,βT  − sⁿ,β · sⁿ⁺¹,βT Σ        ∀n ∈ [0, N − 1]          (89)

Defining s = (sN , sN−¹, . . . , s⁰)T and:

     0            WNT   1N                 0              0         0                            
  


W  =                                                                   . . .

N −2N −1

 ,         Wₓ =     .

 ,        (90)

.

01                                          0

0                  0                    0            W₀₁          0

Eq. (85) and Eq. (87) can also be vectorized into:

st₊₁ = σ(W · st + Wₓ · x)                                                 (91)


Φ(x, s, W, Wₓ

) =  1 sT · W ·

1

s +    s

2

· Wₓ

· x.                                  (92)

Thereafter we introduce the other models in this general case.

E.2    DISCRETE-TIME RNN WITH SYMMETRIC WEIGHTS TRAINED WITH C-EP

Context of use.    This model is used for training experiments in Section 4.2 and Table 4.1.

Equations.    Recall that we consider the layered architecture of Fig. 7, where s⁰ denotes the 
output
layer. Just like in the discrete-time setting of EP, the dynamics of the first phase are defined 
as:

  s⁰    = σ .W₀₁ · s¹Σ

	


t+1           .

t

n+1            T

n−1Σ


∀t ∈ [0, T ] :

n

t+1

= σ   Wnn+1 · st       + Wn−1n · stΣ

∀n ∈ [1, N − 1]

(93)


t+1  = σ

WN,x · x + WNT

23

1N  · st


Under review as a conference paper at ICLR 2020

Again, as in EP, the feedback connections are constrained to be the transpose of the feedforward
connections, i.e. Wnn−₁ = WnT  1n. In the second phase the dynamics reads:


  s0,β,η

= σ(W₀₁ · s¹,β,η) + β  .y − s⁰,β,ηΣ


∀t ∈ [0, K] :

t+1                   .              t

n−1n      t Σ


sN,β,η

= σ   WN,ₓ · x + W T

· sN −1,β,η


t+1



N −1N      t

As usual, y denotes the target. Since Eq. (93) and Eq. (85) are the same, the equations describing 
the

C-EP model can also be written in a vectorized block-wise fashion, as in Eq. (91) and Eq. (92). We

can consequently define the C-EP model in Section 4.1 per Eq. (10).

According to the definitions of Eq. (6) and Eq. (7), for every layer Wnn₊₁ and every t ∈ [0, K]:

  ∆C−EP  (β, t) =  1  .sn,β,η · sn+1,β,ηT  − sn,β,η · sn+1,β,ηT Σ        ∀n ∈ [0, N − 1]        
(95)

E.3    REAL-TIME RNN WITH SYMMETRIC WEIGHTS TRAINED WITH C-EP

Context of use.    This model has not been used in this work. We only introduce it for completeness

with respect to Ernoult et al. (2019).

Equations.    For this model, the primitive function is defined as:


Φ .s⁰, s¹, . . . , sN−¹Σ =

1

2 (1−ϵ)

N

n=0

||s  ||

Σ+ϵ

N −1

n=0

σ(sⁿ)·Wnn₊₁·σ(sⁿ⁺¹)+σ(sN )·WN,ₓ·σ(x)

(96)

so that the equations of motion read:


  s0

= (1 − ϵ)s⁰ + ϵW₀₁ · σ(s¹)


t+1                            t

t   .  n+1Σ        T

n−1


  sN

= (1 − ϵ)sN + ϵ(WN,ₓ · σ (x) + W T

· σ(sN−¹))


  s0,β,η

= (1 − ϵ)s⁰,β,η + ϵW₀₁ · σ(s¹,β,η) + βϵ(y − s⁰,β,η(t))

   t+1                                t                                  t

  st+1         = (1 − ϵ)st          + ϵ(WN,ₓ · σ (x) + WN −1N  · σ(st              ))

where ϵ is a time-discretization parameter and y denotes the target.

According the definition of the C-EP dynamics (Eq. (6)), the definition of ∆C−EP (Eq. (7)) and the

explicit form of Φ (Eq. 96), for all time step t ∈ [0, K], we have:

  ∆C−EP (β, η, t)    =  ¹ .σ .sⁿ,β,ηΣ · σ .sⁿ⁺¹,β,ηΣT − σ .sⁿ,β,ηΣ · σ .sⁿ⁺¹,β,ηΣTΣ        ∀n ∈ 
[0, N − 1]

E.4    DISCRETE-TIME RNN WITH ASYMMETRIC WEIGHTS TRAINED WITH C-VF

Context of use.    This model is used for training experiments in Section 4.3 and Table 4.1.

24


Under review as a conference paper at ICLR 2020

Equations.    Recall that we consider the layered architecture of Fig. 7, where s⁰ denotes the 
output
layer. The dynamics of the first phase in C-VF are defined as:


∀t ∈ [0, T ] :

0

t+1

sn

= σ(W₀₁ · s¹)

= σ(Wnn₊₁ · sⁿ⁺¹ + Wnn−₁ · sⁿ−¹)        ∀n ∈ [1, N − 1]

(98)


t+1               .              t

N − tΣ

Here, note the difference with EP and C-EP: the feedforward and feedback connections are uncon-
strained. In the second phase of C-VF:


  s0,β,η

= σ(W₀₁ · s¹,β,η) + βϵ(y − s⁰,β,η)

	

t+1                                   t                                t


  sn,β,η

= σ(Wnn+1 · sn+1,β,η + Wnn−1 · sn−1,β,η )        ∀n ∈ [1, N − 1],


st+1         = σ



WN,x · x + WNN −1 · st

As usual y denotes the target. Note that Eq. (98) can also be in a vectorized block-wise fashion as

Eq. (91) with s = (s⁰, s¹, . . . , sN−¹)T and provided that we define W  and Wₓ as:


     0            WNN−₁             0              0         0   

		

W      


W  =          0          WN −2N −1                 0

.

0    ,         Wₓ =     .

 ,       (100)

     0                  0                      .              0       W₁₀                         
    0

0                  0                    0            W₀₁          0

For all layers Wnn₊₁ and Wn₊₁n, and every t ∈ [0, K], we define:


     WN,x

β     t+1             t

			

  ∆C−VF (β, η, t)    =  1 (sn+1,β,η − sn+1,β,η ) · sn,β,ηT

E.5    REAL-TIME RNN WITH ASYMMETRIC WEIGHTS TRAINED WITH C-VF

Context of use.    This model is used to generate Fig. 5 - see Table E.6 for precise 
hyperparameters.

Equations.    For this model, the dynamics of the first phase are defined as:

  s⁰    = (1 − ϵ)s⁰ + ϵW₀₁ · σ .s¹Σ

		

	

where ϵ is the time-discretization parameter.  Again,  as in the discre-time version of C-VF, the
feedforward and feedback connections Wnn  ₁ and Wn  ₁n are unconstrained. In the second phase,
the dynamics reads:


t+1



.                .            Σ                     .            ΣΣ

		


  sn,β,η  = (1 − ϵ)sn,β,η + ϵ

Wnn+1 · σ

sn+1,β,η

+ Wnn−₁ · σ

sn−1,β,η

∀n ∈ [1, N − 1]


∀t ∈ [0, K] :



t+1

N,β,η

N,β,η         .

.  N −1,β,η ΣΣ


st+1      = (1 − ϵ)st          + ϵ



WN,ₓ · σ (x) + WNN−₁ · σ   st

where y denotes the target, as usual.  For every feedforward connection matrix Wnn₊₁ and every

feedback connection matrix Wn₊₁n, and for every time step t ∈ [0, K] in the second phase, we define

			

		

  ∆C−VF (β, η, t) =  1 .sn+1,β,η − sn+1,β,η Σ · σ .sn,β,η ΣT

25


Under review as a conference paper at ICLR 2020

E.6    FIGURES FOR THE GDD EXPERIMENTS

In the following figures, we show the effect of using continual updates with a finite learning rate 
in
terms of the ∆C−EP and       BPTT processes on different models introduced above. These figures
have been realized either in the discrete-time or continuous-time setting with the fully connected
layered architecture with one hidden layer on MNIST. Dashed an continuous lines respectively
represent the normalized updates ∆ and the gradients     BPTT.  Each randomly selected synapse

or neuron correspond to one color.  We add an s or θ index to specify whether we analyse neuron
or synapse updates and gradients. Each C-VF simulation has been realized with an angle between
forward and backward weights of 0 degrees (i.e.  Ψ(θf , θb)  =  0◦).  For each figure, left panels
demonstrate the GDD property with C-EP with η = 0 and the right panels show that, upon using
η          > 0, dashed and continuous lines start to split appart.

Table 1: Table of hyperparameters used to demonstrate Theorem 1.

Figure     Angle Ψ (◦)     Activation       T       K         β           ϵ            Learning 
rates
C-EP         5                  0                  tanh          800     80      0.01      0.08     
           0 − 0

C-VF         5                 45                 tanh          800     80      0.01      0.08      
          0 − 0

C-EP         5                  0                  tanh          800     80      0.01      0.08     
1.510−⁵ − 1.510−⁵

C-VF         5                 45                 tanh          800     80      0.01      0.08     
1.510−⁵ − 1.510−⁵

C-VF     14-15              0                  tanh          800     80     0.005     0.08          
      0 − 0

C-VF     14-15              0                  tanh          800     80     0.005     0.08       
2.10−⁵ − 2.10−⁵

C-VF     12-13              0                  tanh          150     10      0.01        −          
       0 − 0

C-VF     12-13              0                  tanh          150     10      0.01        −        
2.10−⁵ − 2.10−⁵

C-EP      10-11              0                  tanh          800     80      0.05      0.08        
        0 − 0

C-EP      10-11              0                  tanh          800     80      0.05      0.08       
2.10−⁵ − 2.10−⁵

C-EP        8-9                0                  tanh          150     10      0.01        −       
          0 − 0

C-EP        8-9                0                  tanh          150     10      0.01        −       
 2.10−⁵ − 2.10−⁵

26


Under review as a conference paper at ICLR 2020

Figure  8:  Discrete-Time  RNN  with  symmetric  weights.   Left:  ∆C−EP(t)  normalized  updates

(η = 0) and −∇BPTT(t) gradients. Right: ∆C−EP(t) normalized updates (η > 0) and −∇BPTT(t)

s                                                      s                                            
                                         s

gradients.

Figure  9:  Discrete-Time  RNN  with  symmetric  weights.   Left:  ∆C−EP(t)  normalized  updates
(η = 0) and −∇BPTT(t) gradients. Right: ∆C−EP(t) normalized updates (η > 0) and −∇BPTT(t)

θ                                                      θ                                            
                                         θ

gradients.

27


Under review as a conference paper at ICLR 2020

Figure 10:  Real-Time RNN with symmetric weights.  Left:  ∆C−EP(t) normalized updates (η  =

0) and −∇BPTT(t) gradients.  Right:  ∆C−EP(t) normalized updates (η  >  0) and −∇BPTT(t)

s                                                         s                                         
                                                 s

gradients.

Figure 11: Real-Time RNN with symmetric weights. Left∆C−EP(t) normalized updates (η = 0) and

−∇BPTT(t) gradients. Right: ∆C−EP(t) normalized updates (η > 0) and −∇BPTT(t) gradients.

28


Under review as a conference paper at ICLR 2020

Figure 12:  Discrete-Time RNN with asymmetric weights.  Left:  ∆C−VF(t) normalized updates

(η = 0) and −∇BPTT(t) gradients. Right: ∆C−VF(t) normalized updates (η > 0) and −∇BPTT(t)

s                                                      s                                            
                                         s

gradients.

Figure 13:  Discrete-Time RNN with asymmetric weights.  Left:  ∆C−VF(t) normalized updates
(η = 0) and −∇BPTT(t) gradients. Right: ∆C−VF(t) normalized updates (η > 0) and −∇BPTT(t)

θ                                                      θ                                            
                                         θ

gradients.

29


Under review as a conference paper at ICLR 2020

Figure 14: Real-Time RNN with asymmetric weights. Left: ∆C−VF(t) normalized updates (η =

0) and −∇BPTT(t) gradients.  Right:  ∆C−VF(t) normalized updates (η  >  0) and −∇BPTT(t)

s                                                         s                                         
                                                 s

gradients.

Figure 15: Real-Time RNN with asymmetric weights. Left: ∆C−VF(t) normalized updates (η =
0) and −∇BPTT(t) gradients.  Right:  ∆C−VF(t) normalized updates (η  >  0) and −∇BPTT(t)

θ                                                         θ                                         
                                                 θ

gradients.

30


Under review as a conference paper at ICLR 2020

F    EXPERIMENTAL  DETAILS

F.1    TRAINING EXPERIMENTS (TABLE 4.1)

Simulation framework.    Simulations have been carried out in Pytorch. The code has been attached
to the supplementary materials upon submitting this work on OpenReview. We have also attached a
readme.txt with a specification of all dependencies, packages, descriptions of the python files as 
well
as the commands to reproduce all the results presented in this paper.

Data set.    Training experiments were carried out on the MNIST data set. Training set and test set
include 60000 and 10000 samples respectively.

Optimization.    Optimization was performed using stochastic gradient descent with mini-batches of
size 20. For each simulation, weights were Glorot-initialized. No regularization technique was used
and we did not use the persistent trick of caching and reusing converged states for each data sample
between epochs as in Scellier and Bengio (2017).

Activation function.    For training, we used the activation function

1


σ(x) =

1 + exp(−4(x − 1/2))

.                                            (102)

Although it is a shifted and rescaled sigmoid function, we shall refer to this activation function 
as
‘sigmoid’.

Use of a randomized β.    The option ’Random β’ appearing in the detailed table of results (Table 3)
refers to the following procedure. During training, instead of using the same β accross 
mini-batches,
we only keep the same absolute value of β  and sample its sign from a Bernoulli distribution of
probability ¹ at each mini-batch iteration. This procedure was hinted at by Scellier and Bengio 
(2017)
to improve test error, and is used in our context to improve the model convergence for Continual
Equilibrium Propagation - appearing as C-EP and C-VF in Table 4.1 - training simulations.

Tuning the angle between forward and backward weights.    In Table 4.1, we investigate C-VF
initialized with different angles between the forward and backward weights - denoted as Ψ in Table 
4.1.
Denoting them respectively θf and θb, the angle κ between them is defined here as:

−₁              Tr(θf · θbT)              

 

where Tr denotes the trace, i.e. Tr(A) =     i Aii for any squared matrix A. To tune arbitrarily 
well
enough κ(θf , θb), the procedure is the following:  starting from θb  =  θf , i.e.  κ(θf , θb)  =  
0, we
can gradually increase the angle between θf and θb by flipping the sign of an arbitrary proportion
of components of θb. The more components have their sign flipped, the larger is the angle. More
formally, we write θb in the form θb = M (p) Ⓢ θf and we define:

Ψ(p) = κ(θf , M (p) Ⓢ θf ),                                                 (103)
where M (p) is a mask of binary random values {+1, -1} of the same dimension of θf : M (p) =    1

with probability p and M (p) = +1 with probability 1     p. Taking the cosine and the expectation of
Eq. (103), we obtain:

⟨cos(Ψ(p))⟩ = p × −Tr(θf  · θfT) + (1 − p) × Tr(θf  · θfT)


Tr(θf · θfT)

= 1 − 2p

31

Tr(θf · θfT)


Under review as a conference paper at ICLR 2020

Thus, the angle Ψ between θf and θf Ⓢ M (p) can be tuned by the choice of p through:

1

p(Ψ) =  2 (1 − ⟨cos(Ψ)⟩)                                                  (104)

Hyperparameter  search  for  EP.    We  distinguish  between  two  kinds  of  hyperparameters:  the
recurrent hyperparameters - i.e. T , K and β - and the learning rates. A first guess of the 
recurrent
hyperparameters T  and β  is found by plotting the ∆C−EP and     BPTT processes associated to
synapses and neurons to see qualitatively whether the theorem is approximately satisfied, and by
conjointly computing the proportions of synapses whose ∆C−EP processes have the same sign as its
BPTT processes. K can also be found out of the plots as the number of steps which are required
for the gradients to converge. Morever, plotting these processes reveal that gradients are vanishing
when going away from the output layer, i.e. they lose up to  10−¹ in magnitude when going from
a layer to the previous (i.e.  upstream) layer.  We subsequently initialized the learning rates with
increasing values going from the output layer to upstreams layers.  The typical range of learning
rates is [10−³, 10−¹], [10, 1000] for T , [2, 100] for K and [0.01, 1] for β.  Hyperparameters where
adjusted until having a train error the closest to zero. Finally, in order to obtain minimal 
recurrent
hyperparameters - i.e. smallest T  and K possible - we progressively decreased T  and K until the

train error increases again.

Table 2: Table of hyperparameters used for training. "C" and "VF" respectively denote "continual"
and "vector-field", "-#h" stands for the number of hidden layers. The sigmoid activation is defined
by Eq. (102).

Activation       T       K        β       Random β     Epochs               Learning rates
EP-1h           sigmoid        30      10      0.1          False             30                    
 0.08 − 0.04

EP-2h           sigmoid       100     20      0.5          False             50               0.2 − 
0.05 − 0.005

C-EP-1h       sigmoid        40      15      0.2          False            100                
0.0056 − 0.0028

C-EP-1h       sigmoid        40      15      0.2           True            100                
0.0056 − 0.0028

C-EP-2h       sigmoid       100     20      0.5          False            150         0.01 − 0.0018 
− 0.00018

C-VF-1h       sigmoid        40      15      0.2           True            100                
0.0076 − 0.0038

C-VF-2h       sigmoid       100     20     0.35          True            150        0.009 − 0.0016 
− 0.00016

32


Under review as a conference paper at ICLR 2020

Table 3:  Training results on MNIST with EP, C-EP and C-VF. "#h" stands for the number of hidden
layers. We indicate over five trials the mean and standard deviation for the test error, the mean 
error
in parenthesis for the train error. T  (resp. K) is the number of iterations in the first (resp. 
second)
phase.

Full table of results.    Since Table 4.1 does not show C-VF simulation results for all initial 
weight
angles, we provide below the full table of results, including those which were used to plot Fig. 5.

Initial Ψ(θf , θb) (◦)                 Error (%)                   T       K     Random β     
Epochs

Test               Train

EP-1h                       −                   2.00 ± 0.13        (0.20)       30      10          
 No               30

EP-2h                       −                   1.95 ± 0.10        (0.14)      100     20           
No               50

C-EP-1h                   −                   2.85 ± 0.18        (0.83)       40      15           
No              100

C-EP-1h                   −                   2.28 ± 0.16        (0.41)       40      15          
Yes             100

C-EP-2h                   −                   2.44 ± 0.14        (0.31)      100     20           
No              150

C-VF-1h                   0                     2.43 ± 0.08        (0.77)       40      15          
Yes             100

22.5                   2.38 ± 0.15        (0.74)       40      15          Yes             100

45                    2.37 ± 0.06        (0.78)       40      15          Yes             100

67.5                   2.48 ± 0.15        (0.81)       40      15          Yes             100

90                    2.46 ± 0.18        (0.78)       40      15          Yes             100

112.5                  4.51 ± 3.96        (2.92)       40      15          Yes             100

135                  86.61 ± 4.27      (88.51)      40      15          Yes             100

157.5                 91.08 ± 0.01      (90.98)      40      15          Yes             100

180                  92.82 ± 3.47      (92.71)      40      15          Yes             100

C-VF-2h                   0                     2.97 ± 0.19        (1.58)      100     20          
Yes             150

22.5                   3.54 ± 0.75        (2.70)      100     20          Yes             150

45                    3.78 ± 0.78        (2.86)      100     20          Yes             150

67.5                   4.59 ± 0.92        (4.68)      100     20          Yes             150

90                    5.05 ± 1.17        (4.81)      100     20          Yes             150

112.5                20.33 ± 13.03     (20.30)     100     20          Yes             150

135                 59.04 ± 17.97     (60.53)     100     20          Yes             150

157.5                77.90 ± 13.49     (78.04)     100     20          Yes             150

180                 74.17 ± 12.76     (74.05)     100     20          Yes             150

33


Under review as a conference paper at ICLR 2020

F.2    WHY C-EP DOES NOT PERFORM AS WELL AS STANDARD EP ?

We provide here further ground for the training performance degradation observed on the MNIST
task when implementing C-EP compared to standard EP. In practice, when training with C-EP, we
have to make a trade-off between:

1.  having a learning rate that is small enough so that C-EP normalized updates are subsequently
close enough to the gradients of BPTT (Theorem 1),

2.  having a learning rate that is large enough to ensure convergence within a reasonable number
of epochs.

In other words, the degradation of accuracy observed in the table of Fig. 4.1 is due to using a 
learning
rate that is too large to observe convergence within 100 epochs. To demonstrate this, we implement
Alg. 5 which simply consists in using a very small learning rate throughout the second phase 
(denoted
as ηtiny), and artificially rescaling the resulting weight update by a bigger learning rate 
(denoted as η).
Applying Alg. 5 to a fully connected layered architecture with one hidden layer, T  = 30, K = 10,
β = 0.1, yields 2.06     0.13% test error and 0.18     0.01% train error over 5 trials, where we 
indicate
mean and standard deviation. Similarly, applying Alg. 5 to a fully connected layered architecture 
with
two hidden layers, T  = 100, K  = 20, β  = 0.5, yields 1.89     0.22% test error and 0.02     0.02%
train error. These results are exactly the same as the one provided by standard EP - see Table 3.

Algorithm 5 Debugging procedure of C-EP

Input: x, y, θ, β, η, ηtiny = 10−⁵η.

Output: θ.

1:  s₀      0                                                                                       
                                d First Phase

2:  ∆θ      0                                                   d Temporary variable accumulating 
parameter updates

3:  repeat

4:         st₊₁      ∂Φ (x, st, θ)

5:  until st = s

β

0

7:  repeat

8:         sβ    ← ∂Φ .x, sβ, θΣ − β ∂A .sβ, yΣ


10:         θ ← θ + ηtiny  . ∂Φ .sβ

Σ − ∂Φ .sβ ΣΣ


11:         ∆θ ← ∆θ + ηtiny  . ∂Φ .sβ

Σ − ∂Φ .sβ ΣΣ

12:  until sβ and θ are converged.


t

13:  θ ← θ

∆θ +    η   ∆θ                                          d Rescale the total parameter update by    
η  

34


Under review as a conference paper at ICLR 2020

Figure 16: Train and test error achieved on MNIST with Continual Equilibrium Propagation (C-EP)
on the Discrete-Time RNN model with symmetric weights. Plain lines indicate mean, shaded zones
delimiting mean plus/minus standard deviation over 5 trials.  Left:  C-EP on the fully connected
layered  architecture  with  one  hidden  layer  (784-512-10)  without  beta  randomization.   
Middle:
C-EP on the fully connected layered architecture with one hidden layer (784-512-10) with beta
randomization.  Right:  C-EP on the fully connected layered architecture with two hidden layers
(784-512-512-10) without beta randomization.

35


Under review as a conference paper at ICLR 2020

Figure 17: Train and test error achieved on MNIST by Continual Vector Field Equilibrium Propagation
(C-VF) on the Discrete-Time RNN model with asymmetric weights with one hidden layer (784-512-
10) for different initialization for the angle between forward and backward weights (Ψ). Plain lines
indicate mean, shaded zones delimiting mean plus/minus standard deviation over 5 trials.

36


Under review as a conference paper at ICLR 2020

Figure 18: Train and test error achieved on MNIST by Continual Vector Field Equilibrium Propagation
(C-VF) on the vanilla RNN model with asymmetric weights with two hidden layers (784-512-512-10)
for different initialization for the angle between forward and backward weights (Ψ).  Plain lines
indicate mean, shaded zones delimiting mean plus/minus standard deviation over 5 trials.

37

