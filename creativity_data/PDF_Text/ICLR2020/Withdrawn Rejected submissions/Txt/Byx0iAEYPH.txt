Under review as a conference paper at ICLR 2020
Fully Polynomial-Time Randomized Approxi-
mation Schemes for Global Optimization of
High-Dimensional Minimax Concave Penalized
Generalized Linear Models
Anonymous authors
Paper under double-blind review
Ab stract
Global solutions to high-dimensional sparse estimation problems with a folded
concave penalty (FCP) have been shown to be statistically desirable but are
strongly NP-hard to compute which implies the non-existence of pseudo-
polynomial time global optimization schemes in the worst case. This paper shows
that, with high probability, a global solution to generalized linear models with
minimax concave penalty (MCP), a specific type of FCP, coincides with a station-
ary point characterized by the significant subspace second order necessary condi-
tions (S3ONC). Given that the desired S3ONC solution admits a fully polynomial-
time randomized approximation scheme (FPRAS), we are able to demonstrate the
existence of an FPRAS for this strongly NP-hard problem. We further demonstrate
two versions of the FPRAS for generating the desired S3ONC solutions. One fol-
lows the paradigm of an interior point trust region algorithm and the other is the
well-studied local linear approximation (LLA). Our analysis thus provides new
techniques for global optimization of certain NP-Hard problems and new insights
on the effectiveness of LLA.
1	Introduction
This paper concerns global optimization of a folded concave penalized formulation of high-
dimensional learning generalized linear models, which belongs to statistical/machine learning prob-
lems such that the number of dimensions (or number of fitting parameters) p is (much) larger than
the number of samples n. This type of problem has recently become very common in a variety of
engineering and scientific applications (Fan et al., 2014a; Fan & Li, 2006) including computational
biology, speech recognition and image processing (Huang et al., 2016; Algamal & Lee, 2015; Xing
et al., 2001; Palmer & Ostendorf, 2001; Bagaria et al., 2012; Paul et al., 2013). Globally minimal
solutions to such a nonconvex learning formulation have been shown effective to guarantee desirable
statistical performance in order to address high dimensionality (Zhang & Zhang, 2012). Nonethe-
less, generating a global solution admits no pseudo polynomial-time algorithm, unless “P = NP”;
Indeed, global optimality is shown strongly NP-hard to achieve by (Chen et al., 2017; Huo & Chen,
2010) while Chen et al. (2014) show similar results for several related problems in regularized min-
imization. In contrast to the existing pessimistic result, we derive herein a fully polynomial-time
randomized approximation scheme (FPRAS, as defined in 4) that theoretically ensures global mini-
mality to 2 with high probability.
Specifically, consider a high-dimensional generalized linear model (GLM) as follows. Let X =
(x1, ..., xn)| be the n×p design matrix with xi = (xi1, ..., xip)|, i = 1, ..., n, and Y = (y1, ..., yn)|
be the n-dimensional response vector. We will assume the design matrix X is fixed, while the mean
of the response is given by E[yi] = ψ0 (xi> βtrue) for some known link function ψ : Θ → <, where
Θ ⊆ < and βtrue = (β1true, ..., βptrue) is the unknown vector of true parameters of the model.
Such a setup can be seen as a generalization of linear regression models with the link function
allowing for nonlinear transformations that enable a more flexible approach to model estimation.
The high-dimensional regression problem is to estimate βtrue given knowledge of X, Y , and ψ in
the undesirable scenario where p n > 0. To that end, traditional statistical learning schemes
1
Under review as a conference paper at ICLR 2020
often resort to the following formulation:
nn
L(β) = X'(yi,xi,β) = — X[ψ(xlβ) - yix|e],
i=1	n i=1
(1)
which, according to traditional statistical theories, would result in over fitting in general under the
high-dimensional setting.
To resolve over fitting, modern statistical theories favor a modified formulation as below:
p
min Q(β) := LGe) + £ Pλ(IejD
β	j=1
(2)
where Pλ(∣ ∙ ∣) is sparsity-inducing regularization term that penalizes any nonzero dimensions in
the minimizer, and λ > 0 is a tuning parameter. Under the assumption that the true regression
parameter βtrue is sparse, a global optimizer to equation 2 has been shown effective to address over
fitting for many choices of specific regularization functions Pλ . Indeed, one of the most successful
choices of Pλ is the much studied Lasso-based regularized (TibShirani, 1996), aka, the 'ι(-norm)
penalty, which has been demonstrated to entail desirable statistical properties (Bickel et al., 2009;
Negahban et al., 2012). Another admirable property of the Lasso is that, especially when applied to
least squares linear regression, it yields an extremely tractable problem via a variety of algorithms
(Friedman et al., 2008; 2010). However, per (Zhao & Yu, 2006; Fan & Li, 2001), Lasso is not
selection consistent without a strong irrepresentable condition and may sometimes introduce non-
trivial estimation bias.
As a popular alternative to Lasso, the folded concave penalty (FCP) was first introduced by (Fan
& Li, 2001). There are mainstream examples of FCP functions, including the SCAD by (Fan
& Li, 200i) and MCP by (Zhang, 2010). This paper will focus on MCP, defined as Pλ(∣t∣)=
R0t Sλ-S)+ ds for some fixed parameter a > 0. In contrast to the Lasso, the FCP regulaizations
achieve variable selection consistency non-contingent on the irrepresentable condition and is demon-
strated to be unbiased (Fan &Li, 2001). Furthermore, (Zhang & Zhang, 2012) demonstrated that the
global solution to the FCP-regularized formulation leads to desirable recovery of the oracle solution.
Nonetheless, FCP problems are significantly harder to solve than Lasso, the new penalty term moves
the problem outside the realm of convex optimization, (Chen et al., 2017) even showed that any
estimation problem with convex loss and folded concave regularization to be strongly NP-hard,
ruling out the possibility of a pseudo-polynomial-time global optimization algorithm. (Liu et al.,
2016) were seemingly the first to propose a global approach to the problem called MIPGO which
reformulates the problem into a mixed integer program. Yet, the worst-case complexity of MIPGO
is in exponential time.
Perhaps for this reason, current literature tends to focus on local algorithms for the FCP-regularized
learning problems. The local quadratic approximation algorithm by (Fan & Li, 2001) is an exam-
ple of a majorization minimization algorithm, an approach which is also related to the local linear
approximation (LLA) algorithm proposed by (Zou & Li, 2008). LLA was further explored by (Fan
et al., 2014b) showing the oracle property can be obtained with high probability despite the local ap-
proach. (Mazumder et al., 2011; Fan & Lv, 2011) demonstrate coordinate optimization approaches
for FCP while (Wang et al., 2014) used an approximate regularization path-following algorithm to
obtain the optimal convergence rate to statistically desirable local solution. (Wang et al., 2013)
analyzed the CCCP algorithm and showed under certain conditions that it asymptotically finds the
oracle estimator. (Liu et al., 2017) took an algorithm agnostic approach by analyzing local solutions
satisfying second order KKT conditions and showed desirable statistical properties like recovering
the oracle solution and sparisty. These results discussed above primarily relate to FCP-regularized
linear regression, a special case of GLM where ψ is specifically the identity function. For analyses
which encompass GLM’s with FCP regularizers, (Fan &Lv, 2011) showed that GLM’s, even in ultra
high dimensional variable selection problems, have oracle properties when using FCP regularization
and demonstrated a coordinate optimization algorithm for finding local solutions. In the area ofM-
estimators, which is a further generalization of our estimation method beyond even GLMs, (Loh &
Wainwright, 2013; Loh et al., 2017a) showed that under certain conditions all local solutions must
be within statistical precision of the true parameter and its support while (Loh et al., 2017b) demon-
strate a two-step algorithm involving composite gradient descent to find a local solution. Bian &
2
Under review as a conference paper at ICLR 2020
Chen (2014) demonstrate a optimality conditions for a class of nonconvex optimization problems
using nonlipscitz regularization.
From the numerous results pertaining to local solution schemes above, our research question is why
local solutions are repetitively successful. In other words, are there certain geometric properties of
the learning formulation equation 2 with FCP that allow all local schemes to perform well indepen-
dent of the specific designs of the algorithmic procedures? Our answer to this question is affirma-
tive; we show herein that all local solutions within an efficiently achievable sub-level set are actually
globally optimal. Those local solutions are characterized by the significant subspace second-order
necessary conditions (S3ONC) provable admit FPRAS’s. The S3ONC are weaker conditions than
the standard second-order KKT conditions. As per this result, all S3ONC-guaranteeing algorithms
(which include a large spectrum of local algorithms) belong to the class of FPRAS’s for global op-
timization of the strongly NP-hard FCP-based learning problem. We subsequently develop theories
for two specific algorithms of this type: one gradient-based method and the other is the same as the
LLA.
It is worth noting that (Zhang, 2010) provides conditions to establish the uniqueness of local so-
lutions to FCP-based learning. When local solutions are unique, then any local optimization al-
gorithms would ensure global optimality. However, a few critical assumptions are necessary to
achieve the uniqueness result and, furthermore, many report numerical experiments, e.g., those in
(Fan et al., 2014b; Liu et al., 2017; 2016; Fan & Li, 2001) indicate the non-uniqueness of local so-
lutions, instead. In contrast, our results in this paper imposes only standard assumptions commonly
shared by a flexible set of high-dimensional GLMs and are applicable even if the local solutions are
non-unique. To our knowledge, this is the first geometric proof that global solutions coincide with
pseudo-polynomial-time computable local solutions in an FCP-based regression formulation with
high probability. The resulting algorithms are the first few FPRAS’s to this problem.
Two works with notable relations to our own are Liu et al. (2017) and Liu & Ye (2019). The first
applies a similar analytical framework to linear regression problems however our generalization to
GLMs adds significant flexibility and it was unknown for their result that the oracle solution implies
global optimality since it was only as of Zhang & Zhang (2012) that global optimality was known to
potentially imply the oracle solution. Further, it is nontrivial to extend their existing result to global
optimal results. (Liu & Ye, 2019) on the other hand is a more general setup than our own though
the tradeoff is that our rate is better and S3 ONC solutions do not ensure global optimality to the
in-sample training error for their setup.
The rest of this paper is organized as follows. Section 2 goes through specific problem assumptions
and explains the S3ONC. Section 3 contains our main result for global optimality and uses itto make
additional claims for LLA. Section 4 contains numerical results to verify our theoretical findings.
Section 5 provides concluding remarks.
In this paper We will use ∣∣∙ko to denote the number of nonzero entries, | ∙ | to denote the 'ι-norm if
the argument is a vector, or cardinality if the argument is a set, ∣∙∣ to denote the '2-norm, ∣∙∣maχ
to denote the maximum norm and 小鼠沦 to denote the absolute value of the entry with the smallest
magnitude. (•)+ is used equivalently to max(0, ∙). For any vector v, VQ is intended as (vj : j ∈ Q).
For any set Q, we denote the complement as Qc. In particular, let S be the true support set, that
is, S := {j : βjtrue 6= 0} and its complement is Sc . We occasionally use the term the “oracle
solution” to refer to the solution βoracle defined as βoracle ∈ arg min L(β). The oracle solution
β: βj = 0, ∀j∈s
is a hypothetical solution which assumes prior knowledge of the true support set S and thus can be
considered a theoretical benchmark.
2	Setups, preliminaries, and assumptions
2.1	setups and assumptions
Our analysis will focus on sparse GLMs that have a fixed design matrix and satisfy the following
assumptions:
(A1) Assume that
3
Under review as a conference paper at ICLR 2020
(i)	bu ≥ ψ00(x∣β) ≥ bl > 0 for all x∣β ∈ Θ;
(ii)	There exists K > 0 such that the design matrix satisfies 1 IlXjk 2 < K for all j ∈ [p].
Let the tuning parameter a in Pλ satisfy K < (bua)-1.
(A2)	The vector of residuals W ∈ <n such that W := y - E[y|X] is subgaussian(σ) which
means it satisfies that
P[|hW,v)| ≥ t] ≤ 2exp(-t2∕2σ2), for all V : ∣v∣ = 1 andany t > 0;
(A3)	There exists a sequence {rd ≥ 0 : d = 1, 2, ..., p} such that the following are satisfied:
(i)	For any d1, d2 : 1 ≤ d1 ≤ d2 ≤ p, we have rd1 ≥ rd2 ;
(ii)	There exists some p* : 2|S| ≤ p* ≤ P such that rp* > 0;
(iii)	For all d : 1 ≤ d ≤ p, it holds that n-1 kXβk2 ≥ rd kβk2 for any β ∈ <p : kβk0 ≤ d.
Remark 1. Part (i) of (A1) states that our link function is both strongly convex and continuously
differentiable; that is, the gradient being Lipschitz continuous. Many types of traditional GLM
problems satisfy this constraint including those for normal (linear regression), categorical (logistic
regression), binomial, gamma and Poisson distributions, although in some cases the mild assumption
on the boundedness of Θ has to be made. Even though the original domain of the link function can
be unbounded, one may still consider its bounded subset given that it contains the vector of true
parameters. Part (ii) of (A1) can be assumed without loss of generality by normalizing the design
matrix columns.
Remark 2. (A2) is a common assumption in the literature, for example (Negahban et al., 2012)
(Wang et al., 2013).
Remark 3. Both (i) and (A2) are satisfied by a number of GLM setups, one example is linear
regression. In such a setup, the response Y takes a gaussian distribution, while the gradient of the
link function (encoding E [Y |X]) is the identity. Note, it is difficult to have one without the other,
since the loss formulation 1 is simply a log-likelihood maximization applied to a distribution within
the class of exponential dispersion models (J0rgensen, 1987). Another classic example is logistic
regression which is used for a Bernoulli or binomial distributed Y along with a logit link function.
It should be noted that we treat the matrix X as fixed, so its generative distribution is not important
to the analysis outside of whether it satisfies the assumptions and constraints mentioned. In the
numerical experiments in Section 4, we use i.i.d. gaussian generation method since, as discussed
for Definition 1, it means our design matrices will satisfy (A3) with high probability.
Remark 4. Assumption (A3) can be understood to be a lower bound on the eigenvalues for principal
sub-matrices of X1X of dimension d X d for all d ∈ [p]. For every d : d ≤ p*, the lower bounds
are positive, meaning that the smallest eigenvalues of the d × d principal sub-matrices are assumed
positive.
According to (Liu et al., 2017), Assumption (A3), for certain parameters, is provably a weaker
condition than the restricted eigenvalue (RE) condition, as defined in Definition 1 below and first
introduced by (Bickel et al., 2009) as a plausible assumption to allow for the desired recovery quality
of Lasso. The RE condition is a common assumption in the high-dimensional learning literature,
such as (Zhang et al., 2014) and (Fan et al., 2014b).
Definition 1. (RE condition (Zhang et al., 2014)) The matrix X ∈ <n×p is said to satisfy the RE
12	2
condition if, for some 相 > 0, It holds that 1 ∣∣Xδ∣ ≥ 旌 ∣∣δ∣ for all δ ∈ U∣S^∣=s C(S) where
_ . , O..	, _	, _ ,	_ _	, _	,	, ,	, _	O. .	. _	, _	0..
C(S) := {δ := (δi) ∈ <p : ∣δs^c | ≤ 3∣δs^∣},δs^c := (δj : j ∈ Sc), and δ^ := (δj : j ∈ S).
Furthermore, the largest possible re is said to be the restricted eigenvalue constant ofX.
Random design matrices with with i.i.d. rows generated following subgaussian distributions as in
(A2) have been shown to satisfy the RE condition with high probability (Zhou, 2009) while propo-
sition 1 in Loh & Wainwright (2013) includes a proof that with high probability, restricted strong
convexity (RSC) is satisfied for a setup equivalent to our own. Note that satisfaction RSC implies
the RE condition above. Thus, within our setup, (A3) is also satisfied with high probability for our
setting.
2.2	Preliminaries on S3ONC
Our results focus on the S3ONC solutions, which has been formerly introduced by (Liu et al., 2017)
in the special case of high-dimensional linear regression as a relaxation of the standard second-order
4
Under review as a conference paper at ICLR 2020
KKT conditions. The definition of S3ONC depends on the notion of first order necessary conditions
(FONC) as below.
Definition 2 (FONC). A solution β* satisfies the first order necessary conditions (FONC) if
n
0 ∈ 1/nX [Ψ0(χ∣β*)-yi]Xi + (P；(lβj)∂(∣β*∣), 1 ≤ j ≤ P)	⑶
i=1
where ∂(| ∙ |) denotes the subdifferential of | ∙ |.
Definition 3 (S3ONC). A solution β* satisfies the significant subspace Second-Order necessary con-
dition (S3 ONC) ifit satisfies FONC andfor all j ∈ {j : βj = 0},
∂2Q(β)
(a% y2 β=β
≥0
(4)
if the second derivative exists.
Remark 5. The S3 ONC can be intuited as the second order necessary condition applied only to the
dimensions where βj 6= 0, i.e., the significant dimensions. Since the S3ONC is weaker than the stan-
dard second-order KKT conditions, any algorithm that guarantees the second-order KKT conditions
can be used to obtain an S3 ONC solution, by requiring a more stringent optimality condition, may
be slower than necessary. One specifically S3 ONC guaranteeing approach, presented in (Liu & Ye,
2019), utilizes an interior point trust region algorithm in order to guarantee an S3ON C solution in
polynomial time. This is the scheme which will be used later in Section 4.
3	Main results
We now present our theoretical results for global optimization of FCP penalized GLMs. All proofs
can be found in the appendix. We will make use of a short-hand notation:
βLasso ∈ argminL(β) + λ∣β∣.	(5)
Theorem 1. Suppose assumptions (A1), (A2), and (A3) with any p* : p* ≥ 2|S|. Let β* be
an arbitrary S3ONC solution to equation 2 with P； specified as the MCP. Assume that Q(β*) ≤
Q(βtrue) + Γ for an arbitrary Γ ≥ 0. (i) Let the sub-optimality gap satisfies Γ < Pλ (aλ) -
bσln (p* + 2√p*t + 2t);(ii) choose Pλ(aλ) > 2nb∣(I + 2√0 + 2tO) + n ।Sbl(P^-2s++；)+rM and
(iii) assume that the minimal signal strength satisfy
惬『min >{总(p* +2 即 +2t) + / min{勺 S1,PlQ)1S1+r
then the following two statements hold
(a)	β* is an oracle solution with probability at least 1 一 exp(-1 + p* ln(pe)) 一 exp(一(p* +
1)(t0 - lnp)) ∙ 1-exp(Tp-4[t0-lnP)).
1-exp(-t0 +ln p)
(b)	β* is both an oracle solution and an globally optimal solution to equation 2 with probability
1 - 2exp(-t + P* ln(崇))-2exp(-(p* + 1)(t0 - lnp)) •「芝-(P(-：雪-,P)).
Remark 6. Theorem 1 (especially in the second statement) is perhaps the first result that establishes
a set of conditions for any S3 ONC solution to be globally optimal with high probability. Further,
this result is algorithm independent which allows for greater flexibility compared to most existing
results as in (Loh et al., 2017b) and (Fan & Lv, 2011) which rely on a specific algorithm choice.
Remark 7. The second part follows quite easily from the first due to the uniqueness of βoracle as
well as the fact that βoPt must also be an S3 ONC solution. Thus by applying the first part of the
Theorem to βoPt we are able to show that both our arbitrary β * and βoPt coincide with the unique
βoracle.
5
Under review as a conference paper at ICLR 2020
Remark 8. The above constraints on Γ, Pλ (aλ) and kβStrue kmin may initially seem disparate but
can all be converted to constraints on the sample size n as is shown in Corollary 1 below. This is
possible because Γ can be bounded by some function of n-γ for some γ > 0. Given that, it can be
seen that the lesser side of inequalities (i),(ii) and (iii) go to 0 as n grows. Further discussion of
how this is achieved for Corollary 1 can be found in Remark 12.
Corollary 1. Assume ln P ≥ 1, b ≤ 1, and S ≥ 1. Let β* be an S3 ONC solution to equation 2.
Let assumptions (A1), (A2), and the RE condition as defined in Definition 1 hold. Assume that
Q(β*) ≤ Q(β Lasso) almost surely, where β Lasso is the optimal solution to the Lasso problem with
penalty coefficient λLasso = σ JnI-P/2 where Y ∈ [0,1] is an arbitrary scalar Let λ = 普2
and a ∈ [.8, 1). There exist problem independent constants C1 > 0,C2 > 0 and C2 > 0 such that if
n> max J C1,
bl
sσ2 lnp
2/Y
kβSruekmin b2r4
(6)
C2 s
2
1 —γ
Then β* is the global solution to 2 with probability at least 1 一 C4 exp(-C5snγ/2 lnP) 一
C6 exp(-C7bunγ/2 ln(p)) for problem independent constants C4, C5, C6 and C7
Remark 9. Corollary 1 indicates that for γ > 0, the global optimal solution coincides with com-
putable S3 ONC solution with overwhelming probability given that the sample size meets certain
requirements. It should specifically be noted that the relationship between n and P require only
Tn%=O(1), which ensures the applicability to the high-dimensional setting even if n《P.
Remark 10. (Liu & Ye, 2019) has derived a gradient-based algorithm that provably ensures an
S3 ONC solution at pseudo-polynomial-time complexity. When n is properly large, this pseudo-
polynomial-time algorithm enables a straightforward design ofan FPRAS for generating the global
optimal solution as follows.
FPRAS: A pseudo-polynomial-time algorithm that generates global optimum at high probability
Step 1. Initialize the parameters δ, λ,a,a,k = 0 and BLass by solving equation 5
Step 2. If Case 1: ∣β; | ∈ (0, aλ) for some j = 1,…,p, then choose an arbitrary ∣ ∈ {j : ∣βjk ∈
(0, aλ)} and solve
βk+1 ∈ argmin[VL(βk儿∙ β + Pλ(∣β∣)
β
s.t. (β — βk)2 ≤ δ2
and let βjk+1 = βjk ∀j 6= ι. Go to Step 3.
Else Case 2: If—βj / (0, aλ) for all j = 1, ...,p then for all j = 1,…,p:
— If βj =0 then βj+1 = a ∙ [∣[VL(βk )j| 一 λ1 ∙ Sign(-∖VL(βjk )j).
—If ∣βjk | ≥ aλ, then βjk+1 = βj 一 ^ ∙ [VL(βk )j. Go to Step 3.
Step 3. Algorithm stops if |B，| ∈ (0, aλ) and Ilek — βk+1∣∣ < δ. Otherwise, let k := k + 1 and go
to Step 2.
Remark 11. Here, the above algorithm has iteration complexity of O ((Q(BLaSs)一 Q(β°pt)) ∙
max{(1∕(2a) - bu/2)-1,2b-1,(1∕a - bu/2)-1} ∙ 1∕δ2) for any γ-accuracy S3ONC solution. In
this iteration complexity, all the quantities are verifiably upper bounded by a polynomial function
of dimensionality P and the desired accuracy 1∕γ. Furthermore, BLasso is a solution to a convex
problem, which be generated within polynomial time and the per-iteration problem admits a closed
form, whose complexity is strongly in polynomial-time. Therefor, this algorithm is an FPRAS in
generating an S3 ONC (global) solution where the term FPRAS is defined in Definition 4.
Definition 4. (FPRAS) Let f : L → <0+ be a function representing the optimal solution to a
problem. Let A be a probabilistic algorithm, which takes as an input an instance of the problem
x ∈ L, and a parameter > 0. We call A a fully polynomial randomized approximation scheme, if
it has the following properties
6
Under review as a conference paper at ICLR 2020
•	the algorithm returns a value within the required precision with probability at least 1/2.
P |A(x, ) - f (x)| ≤f(x) >1/2
•	the running time of A is polynomial in both the size of the input x and the precision 1/
Note that the specific probability bound is fairly arbitrary as long as it is a constant probability
bounded away from 1/2 (Vazirani, 2013).
Remark 12. We are able to remove Γ from the result by bounding the performance difference
between βtrue and β lasso using similar techniques as in Bickel et al. (2009). In order to use this
bound for our S3 ONC solution, we require that Q(β*) ≤ Q(BLasSo. However, this Can generally
be obtained by initializing any S3 ONC guaranteeing algorithm with β Lasso in a similar fashion to
(Fan et al., 2014b) for LLA. The FPRAS above follows the same initialization scheme.
Remark 13. The above specification of values for a, λ and λLasso can be thought of as examples
rather than strict requirements. A closer examination of the prooffor Corollary 1 will reveal that the
values for λ and λLasso can be chosen in a much more flexible fashion, though the corresponding
values ofC1 through C7 may be different for different combinations ofλ and λLasso.
The techniques used in the proof of Theorem 1 can be used to provide insights into other optimiza-
tion schemes. As an example, we can apply the same analysis to the state-of-the-art FCP-based
algorithm, LLA, using the framework in (Fan et al., 2014b) as a starting point.
LLA: local linear approximation.
Step 1. Set k = 0. Initialize the algorithm with β0 = βLasso, where βLasso is generated by solving
equation 5. Let N be the maximal iteration number.
Step 2. For all k = 1, ..., N, solve the following convex program to generate βk+1:
β k+1 ∈ arg min L(B) + X Pλ (IBkI) ∙ lβj |,
j ∈[p]
where Pλ0 is the first derivative of Pλ . Let k := k + 1.
We can show that in fact the LLA is another FPRAS that achieves the global optimal solution. The
proof of this can be found in the appendix.
Corollary 2. For problem equation 2. If ∣IBSruekmin	>	(a + 1)λ, λ >
max{ 3#：sosi/2, 4σ√s+2√st1+2tl, 2σ√s+2√st2+2t2} and the RE condition in Definition 1
bl re	bl (an/bu ) /	bl nre
holds, the following holds.
(a)	The LLA algorithm initialized with BLasso converges to the oracle solution in two iterations
with probability 1 - φ0 - φ1 - φ2, where
Φo := P(IIBLassO- Btruellmax >λ) ≤ 2pexp( TiLaa2nbua),
φι := P(IlOsc'n(Boracle) Il	≥ λ) ≤ (P)s exp(-tι) + 2exp(-¾⅛un),
p	max
φ2 := P(IlBSracleLin ≤ aλ) ≤ (*)s exp(-t2),
(b)	If in addition (A1) and (A2) holds, while the parameters of (a, λ) satisfy that
Pλ(aλ)	>	2nbl(I + 2√t4 + 2t4) + n l：l(P+¾∣S1+1)4)* and the and Pλ(aλ)	>
bσn (p* + 2√p*t3 + 2t3) and let the minimal signal strength satisfy kBSruekmin >
qr8σ2n (p* + 2√p*t3 + 2t3)+ G min{1∣ ∣S∣, Pι(aλ)∣S∣} then the LLA algorithm ini-
tialized by B Lasso converges to the global solution in two iterations with probability at least
1 - φ0 - φ1 - φ2 - φ3
7
Under review as a conference paper at ICLR 2020
where
φ3 := P(Boracle = BOpt) ≤ eχp(-t3 + P* ln(Ie))
+ exp(-(∣* + I)(t4 - lnp)) ∙
1 - exp(-(l -I*)(t4 - lnP))	⑺
1 — exp(-14 + ln p)	，
and t1 , t2 , t3 , t4 > 0 are arbitrary constants.
Remark 14. Since each iteration of the LLA solves a convex program, which can be done within
polynomial-time. When n is properly large, the above theorem then indicates that the LLA is another
FPRAS in globally optimizing the FCP-based nonconvex formulation.
4 Numerical experiments
4.1	Experimental setup
We focus our tests on sparse logistic regression. Our problem and data are implemented in a similar
way as (Fan et al., 2014b). We construct βtrue as below: Firstly, βStrue is constructed randomly by
choosing 10 elements of β and choosing the magnitude of each to be a uniform value within [1, 2].
Each value is chosen to be negative with probability .5. Then, the remaining entries βStrcue are set
to be 0. The design matrix X ∈ <n×p is constructed by generating n iterations of Xi 〜Np(0, Σ)
where Σ = (.5lj-j l)p×p. We then generate Y using a Bernoulli distribution where P(yi = 1)=
(1 + e-xlβtrue)-1. With this data, We train a logistic regression model by invoking Algorithm 1 in
solving equation 2 with MCP for S3ONC solutions initialized with Lasso implemented in Python 3.
The tuning parameters λ and a are obtained by cross validation following (Fan et al., 2014b).
We would like to ascertain whether our FCP classifier, obtained using S3ONC methods, is actually
the global optimal solution. We do this by taking each element of the FCP classifier and perturbing
it to find a new potential solution. Each element’s perturbation is independent and generated by a
N(0, 1/p1/2))-random variable. We then check if this perturbed classifier has better FCP regularized
performance on the training data than the FCP classifier. If not, we repeat until either a better solution
is found, or until 2000 perturbations have been tried.
Additionally, we compare our solution’s statistical performance to those of other popular regulariza-
tion methods. Using the data generation method above, we obtain two sets of data, both with 100
samples. One set is for training the model, and the other is the test set for out-of-sample tests. We
repeat the above process for 100 times to generate 100 training-and-test instances, each with 100
samples. We compare those trained using the method described above with Lasso solutions gener-
ated by the global minimizer to equation 5 and an estimator generated by solving equation 2 when
Pλ is substantiated by an `2 penalty. The Lasso and `2 classifiers are solved using the scikit learn
python library.
We compare the above estimators in terms of statistical performance for both '1 loss: ∣β* - βtrue∣
and `2 loss: kβ* - βtrue k.
4.2	Numerical results
Table 1: Percent of time FCP beat all perturbations
	n= 100 p= 500	n = 100 P = 1000	n= 100 P= 1500	n= 100 P=2000
% Best FCP	100%	100%	100%	100%
Table 1	contains the numbers from optimality analysis. This technique did not yield a single per-
turbed solution that could beat the FCP classifier obtained from the FPRAS in any of our thousands
of iterations.
Table 2	shows the numerical results for the statistical performance measurements. We show the two
performance measures for each of the three classifiers for tphree different problem types.
8
Under review as a conference paper at ICLR 2020
Table 2: Statistical performance of the four classifiers.
Classifier	Measure	n= 100, Mean	p= 1000 Std. dev	n = 100, Mean	p= 1500 Std. dev	n= 100, Mean	p=2000 Std. dev
MCP	`1 loss `2 loss	13.909907 4.108019	1.471911 0.320061	14.818059 4.304993	1.698191 0.374453	14.506226 4.489184	1.480686 0.399441
Lasso	`1 loss `2 loss	15.015975 4.3255	1.039529 0.25996	15.882654 4.397969	1.29422 0.326336	17.079414 4.433467	1.545309 0.362707
`2 Penalty	`1 loss `2 loss	22.211963 4.734209	0.791955 0.241683	26.026067 4.738025	0.966091 0.296726	28.485075 4.755959	0.993699 0.296746
As expected, the FCP classifier generally outperformed the lasso and `2 classifiers. The margins are
fairly thin between FCP and lasso, especially compared to the standard deviation. Other values of n
and p were tried but the results generally followed the same pattern.
As a result we tentatively conclude that our numerical results align with our theoretical results
though further testing of the global optimality probability would be valuable.
5 Conclusions
This paper investigates both the theoretical and empirical performance of FPRAS’s on MCP regu-
larized GLMs. Despite such a problem being strongly NP-Hard, we have shown two FPRAS that
achieve global optimality. To our knowledge this is the first probability bound for global optimiza-
tion of FCP regularized GLMs using an FPRAS. Further, the same technique can be used to extend
other results in order to obtain global optimization bounds for a wide variety of problems.
Though this paper focuses on GLMs, further exploration will focus on the question whether similar
results can be found for more general problem classes under weaker assumptions. High-dimensional
M-estimation problems could potentially be a future avenue of investigation.
References
Zakariya Yahya Algamal and Muhammad Hisyam Lee. Penalized logistic regression with the adap-
tive lasso for gene selection in high-dimensional cancer classification. Expert Systems with Ap-
plications, 42(23):9326-9332, 2015.
Anurag Bagaria, Victor Jaravine, YUanPeng J Huang, Gaetano T Montelione, and Peter Guntert.
Protein structure validation by generalized linear model root-mean-square deviation prediction.
Protein Science, 21(2):229-238, 2012.
Wei Bian and Xiaojun Chen. OPtimality conditions and comPlexity for non-liPschitz constrained
oPtimization Problems. Preprint, 2014.
Peter J Bickel, Yaacov Ritov, Alexandre B Tsybakov, et al. Simultaneous analysis of lasso and
dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009.
Xiaojun Chen, Dongdong Ge, Zizhuo Wang, and Yinyu Ye. ComPlexity of unconstrained l2-lP
minimization. Mathematical Programming, 143(1-2):371-383, 2014.
Yichen Chen, Dongdong Ge, Mengdi Wang, Zizhuo Wang, Yinyu Ye, and Hao Yin. Strong nP-
hardness for sParse oPtimization with concave Penalty functions. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, PP. 740-747. JMLR. org, 2017.
Jianqing Fan and Runze Li. Variable selection via nonconcave Penalized likelihood and its oracle
ProPerties. Journal of the American statistical Association, 96(456):1348-1360, 2001.
Jianqing Fan and Runze Li. Statistical challenges with high dimensionality: Feature selection in
knowledge discovery. arXiv preprint math/0602133, 2006.
9
Under review as a conference paper at ICLR 2020
Jianqing Fan and Jinchi Lv. Nonconcave penalized likelihood with np-dimensionality. IEEE Trans-
actions on Information Theory, 57(8):5467-5484, 2011.
Jianqing Fan, Fang Han, and Han Liu. Challenges of big data analysis. National science review, 1
(2):293-314, 2014a.
Jianqing Fan, Lingzhou Xue, and Hui Zou. Strong oracle optimality of folded concave penalized
estimation. Annals of statistics, 42(3):819, 2014b.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with
the graphical lasso. Biostatistics, 9(3):432-441, 2008.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear
models via coordinate descent. Journal of statistical software, 33(1):1, 2010.
Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012.
Hai-Hui Huang, Xiao-Ying Liu, and Yong Liang. Feature selection and cancer classification via
sparse logistic regression with the hybrid l1/2+ 2 regularization. PloS one, 11(5):e0149675, 2016.
Xiaoming Huo and Jie Chen. Complexity of penalized likelihood estimation. Journal of Statistical
Computation and Simulation, 80(7):747-759, 2010.
BentJ0rgensen. Exponential dispersion models. Journal of the Royal Statistical Society: Series B
(Methodological), 49(2):127-145, 1987.
Hongcheng Liu and Yinyu Ye. High-dimensional learning under approximate sparsity: A uni-
fying framework for nonsmooth learning and regularized neural networks. arXiv preprint
arXiv:1903.00616, 2019.
Hongcheng Liu, Tao Yao, and Runze Li. Global solutions to folded concave penalized nonconvex
learning. Annals of statistics, 44(2):629, 2016.
Hongcheng Liu, Tao Yao, Runze Li, and Yinyu Ye. Folded concave penalized sparse linear regres-
sion: sparsity, statistical performance, and algorithmic theory for local solutions. Mathematical
programming, 166(1-2):207-240, 2017.
Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and
algorithmic theory for local optima. In Advances in Neural Information Processing Systems, pp.
476-484, 2013.
Po-Ling Loh, Martin J Wainwright, et al. Support recovery without incoherence: A case for non-
convex regularization. The Annals of Statistics, 45(6):2455-2482, 2017a.
Po-Ling Loh et al. Statistical consistency and asymptotic normality for high-dimensional robust
m-estimators. The Annals of Statistics, 45(2):866-896, 2017b.
Rahul Mazumder, Jerome H Friedman, and Trevor Hastie. Sparsenet: Coordinate descent with
nonconvex penalties. Journal of the American Statistical Association, 106(495):1125-1138, 2011.
Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, Bin Yu, et al. A unified frame-
work for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical
Science, 27(4):538-557, 2012.
David D Palmer and Mari Ostendorf. Improving information extraction by modeling errors in speech
recognizer output. In Proceedings of the first international conference on Human language tech-
nology research, pp. 1-5. Association for Computational Linguistics, 2001.
Gregory Paul, Janick Cardinale, and Ivo F Sbalzarini. Coupling image restoration and segmentation:
a generalized linear model/bregman perspective. International journal of computer vision, 104
(1):69-93, 2013.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 267-288, 1996.
10
Under review as a conference paper at ICLR 2020
Vijay V Vazirani. Approximation algorithms. Springer Science & Business Media, 2013.
Lan Wang, Yongdai Kim, and Runze Li. Calibrating non-convex penalized regression in ultra-high
dimension. Annals of statistics, 41(5):2505, 2013.
Zhaoran Wang, Han Liu, and Tong Zhang. Optimal computational and statistical rates of conver-
gence for sparse nonconvex learning problems. Annals of statistics, 42(6):2164, 2014.
Eric P Xing, Michael I Jordan, Richard M Karp, et al. Feature selection for high-dimensional
genomic microarray data. In ICML, volume 1, pp. 601-608. Citeseer, 2001.
Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of
statistics, 38(2):894-942, 2010.
Cun-Hui Zhang and Tong Zhang. A general theory of concave regularization for high-dimensional
sparse estimation problems. Statistical Science, 27(4):576-593, 2012.
Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Lower bounds on the performance of
polynomial-time algorithms for sparse linear regression. In Conference on Learning Theory, pp.
921-948, 2014.
Peng Zhao and Bin Yu. On model selection consistency of lasso. Journal of Machine learning
research, 7(Nov):2541-2563, 2006.
Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. arXiv preprint
arXiv:0912.4045, 2009.
Hui Zou and Runze Li. One-step sparse estimates in nonconcave penalized likelihood models.
Annals of statistics, 36(4):1509, 2008.
A Appendix
The Appendix is organized as below: Section A.1 presents the proofs for the main results, Sections
A.2 and A.3 present central lemmata to be useful in Section A.1.
A. 1 Proof Of main results
A useful relationship in our proofs is that, for an S3ONC solution β* within {β* : Q(β*) ≤
Q(βtrue) + Γ} for any Γ ≥ 0, we have the following useful inequality under Assumption (A1):
2b^ kXδ*k2 -1WlXδ* + XPλ(∣β*∣) ≤ XPλ(∣βjrue∣) + Γ,	(8)
n	n	j∈S	j∈S
where δ* = β* - βtrue . This is obtained by invoking the strong convexity of ψ, which leads to
ψ(x∣β*) ≥ ψ(x∣βtrue) + ψ0(x∣βtrue)(x∣β* - x∣βtrue)+0.5 ∙ bι(x∣β* - x∣βtrue)2.
ProofofTheorem 1. First, given our assumption that (A1) holds, that (i) p* ≥ 2|S|, (ii) β* is
S3ONC satisfying Q(β*) ≤ Q(βtrue) + Γ for some Γ ≥ 0, and (iii) Pλ(aλ) > ^nb-(1 + 2√t0 +
2t0)+ σ ISb((P+2√1-+2t∣)+rbl, we can apply Lemma 5 withP = p*. This means that kβ* - βtrue∣∣o ≤
p* with probability at least 1 - exp(-(p* + 1)(t0 — lnp)) ∙ 1-eXp-p-^0鲁-,P)). From this, given
the additional assumption that (A3) holds, We can apply the second part of Lemma 4 with P = p*
to get that for any t > 0, 1 ∣∣X(β* - βtrue)『≤ 窸(p* + 2√p*t + 2t) + b8 min{λ2(∣S∣ -
kβ*ko)rp-1,Pλ(aλ)(∣S∣-∣∣β*ko) + Γ} holds with probability at least 1 - exp(-t + p* ln(P)).
Given that for 2 arbitrary sets A and B
11
Under review as a conference paper at ICLR 2020
P(A∩B) =P(B)P(A|B) = (1 - P(Bc))(1 - P(Ac|B))
= 1 -P(Ac|B) -P(Bc)+P(Bc)P(Ac|B)
= 1 -P(Ac|B) - P(Bc)(1 - P(Ac|B)) ≥ 1 -P(Ac|B) -P(Bc)
(9)
Therefor they hold simultaneous with probability at least 1 - exp(-t + pln(pe)) - exp(-(p* +
1)(t0 - lnP)) ∙ 1-exp(Tp-号岑-lnP)).	P
1-exp(-t0+lnp)	.
The same sequence of arguments can be used to show that βopt also satisfies ∣∣βopt - βtruek ≤ p*
and n ∣X(βopt - βtrue)k2 ≤ 僚(P* +2√P*i + 2t) + ⅞ min{λ2(∣S∣-kβ*ko)r-t1,Pλ(aλ)(|S|-
∣β* ∣0)+Γ} with the same probability. Using again the union bound and DeMorgan’s law, we say β*
and βopt satisfy the above conditions simultaneously with probability 1 - 2exp(-t + p* ln(Pe))-
2 exp(-(p* + 1)(t0 - lnp)) ∙ 1-æ1^-;--^+^-,P)). With this, our Γ assumption and our minimal
signal strength assumption, we can apply Lemma 6 to show that β* = βopt with probability at least
1 - 2exp(-t + p* ln(P)) - 2exp(-(P* + 1)(t0 - lnP)) ∙ 1-空空(-：雪-'P))
□
Proof of Corollary 1. First we need to bound Γ. In order to do this we use the lasso problem
Qlasso(β) = Pi∈N '(β,Xi,yi) + p λlasso∣βj| as well as the concavity of MCP over positive
j∈P
values to get the following 2 inequalities
Qlasso (β lasso ) ≤ Qlasso (βtrue )
X '(βjasso, Xi, yi) - '(βjrue, Xi, yi) ≤ X λlasso(∣βjrue∣-∣βjasso∣)
i∈N	j∈P
≤': λlasso ∣βl.asso - β^ttrue |
j∈P
(10)
and
EPλ(βjrue) - EPλ(βjasso) ≤ E Pλ (βjasso)(∣βjrue∣-∣βjasso∣) ≤ Eλ∣βjasso - βjrue∣
j∈P	j∈P	j∈P	j∈P
(11)
We also need 2 results from the proof for φo in Corollary 2 which shows that both ∣δSc | ≤ 3∣δS |
and bl 11X δ' 112 ≤ 3λlasso∣δS | conditional on A where δ' = βlasso — βtrue. Given our restricted
.	.	.	kxδ'k2 、	..	.	.
eigenvalue assumption " ` 2 ≥ re, this can be used to show
∙h	nkδ'k	-
∣δ'ι <41 δ'∣ <4"照『<4"叫2
lδ l≤ 4lδS l≤ 4'S 呵 ≤ 4 4s 呵
4√S ∣∣Xδ'∣∣2	4√S	3λlasso∣δS|	4√S	3λlasso√S∣∣δS∣∣
≤ ren-PSr ≤ F bll∣δSII	≤ 下e	bll∣δSII-
(12)
which means ∣δ'∣ ≤ 14；Sos With conditional on A which occurs with probability at least 1 -
2P exp( TS J'
Finally we are able to bound gamma by combining the above
12
Under review as a conference paper at ICLR 2020
Γ ≤ Q(β*) - Q(βtrue) ≤ Q(βlasso) - Q(βtrυe)	(13)
≤ X '(βjasso, Xi, yi) - X Pλ(βjasso) - X '(βjrue, Xi,	yi) - X Pλ(βjrue)]	(14)
i∈N	j∈P	i∈N	j∈P
≤ X(λlasso∣βjasso - βjrue∣ + λ∣βjasso - βjrue∣)	(15)
j∈P
lasso
≤ (λlasso + λ)∣δ'∣ ≤ (λlasso + λ)--------- (16)
blre
Next consider the conditions necessary to apply Theorem 1. We have assumptions (A1) and (A2)
and (A3) per our assumption that the RE condition holds combined with 7. That leaves the 3 re-
quirements on Γ, Pλ(aλ) and kβStruekmin. We will convert each of these to inequalities on n
Utilizing 16 and substituting λ = QQσ Jnnn^p and λlasso = eσy , where Q,e> 0 are arbitrary
constants, and setting p* = 4s, t = p?*n7/2 lnp, t0 = nγ/2 lnP We get the following
2
Pλ(aλ) > 丸 (1+2√+2t0)+
σ s(1 + 2√t0 + 2t0) + Γbl
bl(p* - 2s + 1)
n>
8 + 12e2 + 12eQ
blαQ2
C1/bl
(17)
(18)
Γ < Pλ(αλ) -
2
而(p*+2μ+
n>
20 + 12e2
αQ2
s
G
UeSrueLin > jrpσ^n (p* + 2Pp*t+2t)+ £ min{ 高 lSl,Pλ(aλ)lSl+r}
n>
(160 + 8Q2)
sσ2 lnp
(kβsruekmin r4sblre)2 一
2/7
C	sσ2lnp	-
.3 (kβSruekmi∕4sblre)2一
(19)
(20)
(21)
(22)
For some constants C1, C2 and C3
We can then apply Theorem 1 (conditional on A) substitute our values and simplify to get that β* is
the global solution with probability at least
1 - 2exp(-t + p* ln(pe)) - 2exp(-(p* + 1)(t0 - lnp)) ∙
p*
1 — exp(-(p — p*)(t0 — ln p))
1 - exp(-t0 + ln p)
p—p*
≥1 — 2 exp(-(n7/2 — 1)4s lnp) — 2 ^X exp(-(p* + k)(n7/2 — 1) lnp)
k=1
≥1 — 2exp(-(n7/2 — 1)4slnp) — 2exp(-[(4s + 1)(n7/2 — 1) — 1] lnp)
≥1 — C4 exp(-C5sn7/2 lnp)
(23)
We then use the same technique as in Theorem 1 to combine this number with the probability of A
to get the final non-conditional probability that β* is the global solution with probability at least
13
Under review as a conference paper at ICLR 2020
≥ 1 - C4 exp(-C5snY∕2 lnp)-2pexp( TWaE2nu
8σ2
≥ 1 - C4 exp(-C5snY∕2 lnp)-2exp(…叫2 - 8)lnP)	(24)
8
≥ 1 - C4 exp(-C5snγ∕2 ln P)-C6 exp(-C7bunγ∕2 ln(P))
For some constants C4 , C5 C6 and C7.
Note that these constants, as well as C1, C2 and C3, are dependent only on the value of a Q and , as
far as problem dependencies are concerned. Thus given that a Q and are chosen to be any positive
constant value, as in the statement of Corollary 1, C1 through C7 are problem independent, which
is the desired result.	□
Proof of Corollary 2. The first result is simply Corollary 2 in (Fan et al., 2014b). If we initialize the
LLA algorithm with β lasso, the solution to LASSO using λlasso as the LASSO constant, then the
LLA algorithm converges to the oracle solution in 2 iterations with probability 1 - φ0 - φ1 - φ2 .
However we still need to solve for the actual values of φ0 , φ1 , φ2 for GLM.
First consider φ0 = P (β lasso - βtrue max > a0 λ). Similar to Lemma B.1. in Bickel et al.
(2009), to bound this we will start by noticing that for the lasso penalized loss function Qlasso(β) =
Pi∈N l(β,xi,yi) + λlαsso Pj∈p ∣βj IWehavethat QlaSSo(βIaSSo ≤ Qlαsso(βtrue). If we then
let δ' = βlasso - βtrue we can use the same tactic as in the derivation of 8 to get 品 ∣∣Xδ'∣∣2 -
煮WlXδ' ≤ λlαsso Pj∈p ∣βjrue∣ - ∣βjasso∣, which can then be rearranged to get.
K ∣∣xδ'∣∣2 -1X ∣wlXj∣∣δj∣ ≤ λlasso X ∣β严-∣βjasso∣.
(25)
j∈P
j∈P
Next let A = Tj∈p{∣ 1W|Xj I ≤ λlasso∕2}. We can combine this with 25 to get that 黑 ∣∣xδ'∣∣2+
lasso	lasso true lasso	lasso true lasso	true lasso
λ	∕2	j∈P	Iβj	-	βj	I ≤ λ j∈P	Iβj	- βj	I + λ	j∈P	Iβj	I -	Iβj	I
conditional on A. From this notice that the right term goes to zero when βjtrue = 0 so we then have
bl ∣	` ∣2 lasso	lasso true lasso	lasso true true
that 2n	∣∣Xδ ∣∣ + λ	/2	I^j∈P	∖lβj	-	βj	| ≤ λ	j∈SeS	lβj	-	βj	| +	lβj	| -
∣βjαsso ∣. Using the triangle inequality and the definition of δ' we can simplify this to
⅛∣∣xδ'∣∣2 + 殍∖δ'∖ ≤ 2λlαsso∖δS∖
(26)
conditional on A. By relaxing different parts of the equation, this can be further simplified to both
bl ∣∣Xδ'∣∣2 ≤ 3λlasso∖δS∖ ≤ 3λlassos1∕2∖∖δS∖∖2 and ∖δSc ∖ ≤ 3∖δS∖. Note that the second of these
kXδ'k2
shows that δ' satisfies the constraint for the RE condition 1. Therefor we have that ”心" 2 ≥ re.
nkδ'k	e
If this is combined with the first of the two equations, we can get that n/2 ∣∣Xδ'∣∣ ≤ 3j；a；；；/2?
conditional on A.
Next, using this we can show that conditional on A we have that ∣∣δ'∣∣mαx ≤ ∣∣δ'∣∣ ≤
∣∣Xδ`∣∣2 /(∣∣δ`∣∣ nre) ≤ 33'；S1/2 < aoλ if λ > "；；；丁/2. This is the inverse of the con-
dition that defines	φo.	Thus, we can bound	φo	with	φo	≤ P(Ac)	=	P(Uj∈p ∖ 1W|Xj ∖	>
λlasso∕2) = P(Uj∈p ∖W TXj ∖/ kXj k > nλlasso∕(2 kXj k)) ≤ pP(∖hW,νi∖ > ⅛⅛) ≤
PP(∖{W, vi∖ > λ-------(啖Ua) / ) ≤ 2pexp -(λ 8σ2 nbua which uses both (AIXii) and (A2) as long
asλ> 3λ≡°Ser Per(A2).
Next consider φ1 = P(∣Osc'n(βorαcle∣	≥ aιλ)
Il P	11 max
14
Under review as a conference paper at ICLR 2020
φι =P(I∣OS≡'n(βoracle∣l	≥ aιλ)	(27)
p	max
=P(∃j ∈ P : |Oj'n(βoracle)∣ ≥ aιλ)	(28)
=P(∃j ∈ P : 11 X[ψ0(χ∣βoracle)χi,j — yiχi,j]| ≥ aιλ)	(29)
i∈N
=P(∃j ∈ P : |1 X[ψ0(x∣βoracle)xi,j - ψ0(x∣βtrue)xi,j + Wixij]| ≥ aιλ)	(30)
n i∈N
≤P(1 ∣Xjl(ψ0(Xβoracle) - ψ0(Xβtrue) + W)| ≥ aιλ)	(31)
≤P(1 ∣Xl(ψ0(Xβoracle) - ψ0(Xβtrue))∣ + |WlXj| ≥ aιλ)	(32)
≤P( 1 kXjk ∣∣ψ0(Xβoracle) - ψ0(Xβtrue)∣∣ + |WlXj| ≥ aιλ)	(33)
≤P(IM(Xeoracle) - ψ0(Xβtrue)∣∣ + |WlXjI/kXjk ≥ aιλ kXjk-1)	(34)
≤P(∣∣ψ0(Xβoracle) - ψ0(Xβtrue)∣∣ + IW lXj I/ kXj k ≥ (abunf/aiK	(35)
≤P(bu ∣∣Xβoracle - XetrUeIl + ∣Wlv∣ ≥ (abun)1%ιλ)	(36)
≤P(bu kXδok + |Wlv∣ ≥ (abun)1/2aiK)	(37)
where v ∈ <n is some vector with kvk = 1 as indicated in (A2) and δo = eoracle - e true .
From this, using Demorgan’s law and the union bound, we notice that P(A + B ≥ C) ≤ P(A ≥
C/2) + P(B ≥ C/2) which can be used to further simplify
φι ≤ P(bu kXδok + |Wlv∣ ≥ aιλ(abun)1/2)	(38)
≤ P(kXδok ≥ (1∕2)aιλ(an∕bu)1/2) + P(|Wlv∣ ≥ (1∕2)aιλ(abun)1/2)	(39)
We can then simplify both terms individually. For the first term, P(bu kXδok ≥
(1∕2)aιλ(abun)1∕2), given the fact that the oracle solution and true solution have the same SUP-
port, the oracle solution must be in the Γ = 0 level set of the true solution. Using simi-
lar arguments to Lemma 5, We have that 品 ∣∣Xδok/ ≤ 1WlXδo From here Lemma 2 can
be aPPlied since we know ∣eoracle - e true ∣0 ≤ s. With some simPlification this gives that
∣∣Xδok ≤ b2(maxsp：|Sp|=s ∣∣U^S∕W∣∣) Utilizing Lemma 3 with S in place of P shows that
P ImaXSp：|sp|=s 2 ∣∣USpW∣∣ ≥ 着σ√s + 2√st1 + 2ti ≤ (P)s exp(-ti). This is the first half
of φι as long as (1∕2)aιλ(an∕bu)1/2 ≥ 卷σ"S + 2√St + 2t which is equivalent to the as-
sumed condition λ ≥ 墨]：+：/^+：" Next, the second term can be easily bounded using (A2):
P(IWlv∣ ≥ (1∕2)αιλ(abun)1/2) ≤ 2exp(-¾bun)
Therefor φι ≤ (Pse)s exp(-tι) + 2 exp( -/abun)
Next consider φ2 = P (∣∣eSoracle ∣∣min ≤ aλ) First, given the assumption ∣∣etrue] ∣∣min > (a +
1)λ we can see that φ2	= P (∣∣eSoracle ∣∣min	≤ aλ) ≤	P (∣∣eSoracle -	eStrue ∣∣max > λ) ≤
P (∣∣eoracle - etrue ∣∣2 >	λ) = P (kδok2 > λ).	Next, since	we know that	the support of	eoracle
and e true is S, we know that IδSo I = 0 ≤ 3IδSo I which is the constraint for the RE condition.
kXδok2
Therefor we know that ：历。|12 ≥ re. With this and a similar line of argument as in φι we get
that φ/ ≤ P(MOk > λ) ≤ P(kXδok > λ√nre) ≤ P(b(maxsp：|sp|=s ∣∣USpW∣∣ > λ√nre) =
15
Under review as a conference paper at ICLR 2020
P(maxsp：|Sp|=s ∣∣USp W∣∣ > λbl ∖nre ≥ σ√s + 2√st2 + 2t7) ≤ (P)s exp(-t2) assuming that
λ bl√nre ≥ σps + 2√st + 2t which is equivalent to the condition λ ≥ il2"s+2√t +2t2
This, combined with the fact that for MCP, a0 = a1 = a2 = 1 shows the first result.
The second result can be seen by first noting all assumptions of Theorem 1 part 2 are satisfied,
where (A3) with r4s is implied by 7. Thus by using the same arguments as in Theorem 1 part 2
which shows that the oracle solution is unique and that the global solution is the oracle solution
with some probability, since the global solution is almost surely S3ONC with Γ = 0. If we use
t= t3 and t0 = t4 we get that the probability that the global solution is not the oracle solution as
φ3 ≤ exp(-t3 + pln(Pe)) + exp(-(p* + 1)(t4 — lnP)) ∙ 1-eX—eXP-P4+；：-；Pp) This combined
with the first result shows that the LLA algorithm converges to the global solution in 2 iterations
with probability 1 - φo - φι - φ2 - φ3 which is the second result.	□
A.2 Central lemmas and their proofs
Lemma 1. Let β* be a S3ONC solution to 2. If assumption (A1) holds, then
P[∣β*l∈ (0,aλ),∀j ∈{1, 2,...,p}] = 1.
Proof of Lemma 1. First, define events γj and δj as
γ. ：=[ ∂2Q(β)	≥ 0]
j \(响)2产=产* - j
δj := {∣β;∣ ∈ (0,aλ)}.
(40)
(41)
First, for any given j ∈ P, we solve for P [γj ∩ δj ] given our assumptions. We can start with
∂2Q(β) I
(dβjj2 lβ=β*
n
≥ 0 which gives us 1/n P ψ00 (x∣β*) x% + Pλ0(lβ*l) ≥ 0. We can rearrange this
i=1
nn
to get bu P x2,j ≥ P ψ00(x∣β*)x2,j ≥ -nP〈(∣β*∣) = n/a where We get the leftmost inequality
i=1	i=1
from assumption (A1) part (i) and the rightmost equality from the definition of MCP. More concisely
we have that bu kXj k2 ≥ n/a which contradicts (A1) part (ii). Therefor we know P[γj ∩ δj] = 0.
It should also be noted that P [γjc] = 0 since β* satisfies S3 ONC conditions. Thus, by applying
Demorgan’s law and then the union bound, it can be obtained that
0=P[γj∩δj]=1-P[γjc∪δjc] ≥ 1-P[γjc]-P[δjc]=1-P[δjc]=P[δj].	(42)
We can then apply this result to all indices to get that P[δj] = 0 for all j ∈ {1, 2, ..., p}, which is the
desired result.	□
Lemma 2. Consider an arbitrary S3ONC solution β* to 2 with MCP. Given the event that for some
integer P ： kβ* - βtrueko ≤ P,then |W lXδ*∣ ≤ (maxsp^∣=p ∣∣¾W∣∣) kXδ*k, a.s.
Where
("：={『Ij Sp
and USp ∈ <n×p is defined as in thefollowing Thin SVD: XSp = USp DSpVSp.
Proof. Denote δ* := (δ*) = β* - βtrue, SP := (j : δ* = 0) ⊆ P , 5S豆:=(δ* : j ∈ SP) and
XSp := (Xij : i ∈ N,j ∈ Sp). By assumption, We know that ∣∣δ*k0 ≤ |Sp| = p.
First decompose XSp using Thin SVD to get XSp = USPDSpVSp where USP ∈ <n×p.
Note that since and USfl USp = I we have that for any U ∈ <p we have ∣∣DSpVSPυ∣∣2 =
16
Under review as a conference paper at ICLR 2020
(DSpVSpWI(DSpVSM = UVSpDSUSpUSpDSpVSpn = UTXSpXSpU = IIXSpM2. therefor
we can obtain that
|WTχs*∣ = ∣wTXSp%| ≤ ∣∣WτuSp∣∣ ∣∣DSpV%%∣∣
=iUSpWMp δSp∣∣≤ (SpmaxJUSpW∣∣) g*k
(43)
Where
(USp)i,j := {Up,
if j ∈ Sp
else
□
Lemma 3. Consider an arbitrary S 3ONC solution β* to 2 with MCP. If(A2) holds, then for some
integerp ≤ p, PImaXSp：|Sp|=p ∣∣UpW∣∣ ≤ σ√ρ + 2√ρ⅛ + 2t] ≥ 1 - (pp)pexρ(-t). Where
(USp)i—{USp, j Sp
,	Ct se
and USp ∈ <n×p is defined as in the following Thin SVD: XSp = USp DSPVSp.
Proof. We attempt to bound (maxSp ： ∣ Sp | =p 11 U1 W11). Given that we now have W multiplied by a
.♦	1 T	/ʌ T .1 T	1	. ∖ '	T~T FTT EI r∙ . .1	. ∖ '	∖ '	∖ '
square matrix, we can apply Lemma 9. In the Lemma, let Σu = USP UT. The fact that Σu Σu = Σu
means that Σu is an idempotent matrix with ∣∣Σu∣ ≤ 1 and Tr(Σu) = rank(Σu) ≤ rank(Us25) ≤
rank(US 皆)≤ j5. Lemma 9 then states that that P ∣∣USj5 W∣∣ ≤ σ√p + 2√ρt + 2t ] ≥ 1-exp(-t).
From this we can show that
P
max
S5：|S5∣=ρ
W∣∣ ≤ σy∣p) + 2√pt + 2t
≥1 - (P) eχp(-t) ≥1 - (Pe)peχpf (44)
Where the first inequality can seen by noting that if ηk ∈ <k is a sequence of i.i.d random variables
and θ ∈ < is a scalar, by applying De Morgan,s Law and then using the union bound, it can be
obtainedthat P[maxfc∈κ ηk ≤ θ] = P[∩fceκ ηk ≤ θ] = 1-P[Sk∈κ ηs ≥ θ] ≥ I-PkeK P[ηk ≥
θ] = 1 - ∣K∣(1 - P[ηk ≤ θ]) which yields the same inequality as in 44.
This is the desired result.	□
Lemma 4. Consider an arbitrary S3ONC solution β* to 2 with MCP. Let Assumptions (A1) and
(A2) hold. Given the simultaneous occurrence of(i) the event that Q(β*) ≤ Q(βtrue) + Γ holds
for some Γ ≥ 0; (ii) the ^vent that for some integer P : ∣∣β* — βtrue∣0 ≤ P. Thenfor any t > 0,
1 |X (β *-βtrue)k2 ≤ 萧(p + 2√pt+2t) + 8 min{Pj∈s P； (∣β* ∣)∣β* ∣, Pλ(αλ)(∣S∣-∣β* ∣o) +
Γ} holds with probability at least 1 — exp(-1 + pln(Pe)).
If in addition (A3) holds with p* ≥ p, then 1 ∣∣X(β* — βtrue)∣2 ≤ 探(p+ 2√pt + 2t) +
b min{λ2(∣S∣ - ∣∣β*∣∣o)r-1, Pλ(aλ)(∣S∣ - ∣∣β*∣∣o) + Γ} holds where rp > 0 for any t > 0 with
probability at least 1 — exp(-1 + pln(Pe)).
Proof. First, denote δ* := (δ*) = β* - βtrue, SQ := (j : δ* =0) ⊆P , % := (δ* : j ∈ SQ) and
XSp := (Xij : i ∈ N, j ∈ Sq). By assumption, we know that ∣δ*∣o ≤ ∣Sq∣ = p. Further, let us
denote
17
Under review as a conference paper at ICLR 2020
T1 =min( fP1 (∣βW∣βjrue∣, fPλ (lβ*l)lβ* — βjrue I, Pλ(aλ)(∣S∣-kβ*ko) + r}. (45)
Ij∈S	j∈S
We now start to define the desired bound by applying the second part of Lemma 8. The result
simplified using the above definitions becomes
3 kXδ*k2 ≤ 1WlXδ* + T1,	a.s.	(46)
2n	n
Next, since all assumptions for Lemma 1 are satisfied, we can apply it to get
2bn kXδ*k2 ≤1 QmaUUSpW l)kXδ*k+T1	(47)
We can then complete the square, solving for √n ∣∣Xδ*k to get
1
(48)
(49)
where the last inequality holds due to the value inside the square root being larger than the term
outside. From here, squaring both sides gives us
1 kXδ*k2 ≤ b⅛ Qm3 叫)+ 8Ti.
(50)
Finally, by applying the second part of Lemma 1 we get
1 kXδ*k2 ≤ 4σn (p+2Ppt+2t) + 看 T1.	(51)
With probability at least 1 一 (Pe )p exp(-t). Thus by the definition of T1, the first result of the
lemma has been shown.
For the second part we look to bound the central term ofT1. We first notice (a) that since assumption
(A1) holds, Corollary 4 indicates that if βj = 0 ⇒ ∣β*∣ ≥ aλ for all j ∈ P; (b) that for this range
of βj, Pλ(∣βj∣) = 0; (C) that per the definition of MCP 0 ≤ P](∣β*∣) ≤ λ for any βj ∈ <. If We
combine these observations with 45 and the definition of δ*, we can see that T ≤ WPλ (∣βjiI)∣δ*I
≤ λ,|S| - ∣βSI。∙ ∣∣δ*∣. From this, given that assumption that, for this second result (A3) holds
withp ≥ p, and rp ≥ rp* ≥ 0 we can use (A3) part (iii) to show that T ≤ λp|S| -∣∣βS k。∙kXy.
Since this holds almost surely, it can then be combined with 47 to get
2⅛ kXδ*k2 ≤ 1 QmxJUSpWl) kXδ*k + λqlsl-kxSk。j√Xn=Γ.	(52)
We can then multiply by 2√n∕bι ∣∣Xδ* k to get
* kXδ*k ≤ b⅛ Spmax=P B¾ W B + b⅛ qSl-kxS ko.	(53)
18
Under review as a conference paper at ICLR 2020
We then square both sides and use the rule that (A + B)2 ≤ 2A2 + 2B2 to get
1 kXδ*k2 ≤ [bi√nSpmx=PIUpWl + b1√r≡qSl-kxSko
2
8	I I2	8λ2
≤ 而 Spmx=PIUpWl+ 西 (lSl-kxS k0).
Combining this with 50 yields that
1 kXδ*k2 ≤ b8n SpmxjUpW『+ 8 min £(ISl-kxS ko)，T1
(54)
(55)
(56)
Finally, by applying the second part of Lemma 1 and noting from (45) that T1 ≤ Pλ(aλ)(∣S∣ 一
kβ* k0) + Γ We see that
1 kXδ*k2 ≤ b8-σ2(p + 2PPt + 2t) + b8min! J(∣S∣-∣∣xSk°),Pλ(aλ)(∣S∣-kβ*k0)+r].
n	bl n	bl IrP	J
(57)
With probability at least 1 一 (Pe)p exp(-t) which is the desired result.	□
Lemma 5. Let Assumptions (A1) and (A2) hold. Consider a solution β* satisfying S3ONC of 2.
Assume that Q(β*) ≤ Q(βtrue)+Γ holdsfor an arbitrary Γ > 0. For any integerP : 2∣S∣ ≤ P ≤ P
ifthe penalty parameters (a, λ) satisfy that Pλ(aλ) > 2^ (1 + 2√t + 2t) +
for an arbitrary t > 0, then ∣∣β* - βtrue
lnP)) ∙
1-eχp(-(p-P)(t-ln P))
1—exp(-t+ln P)
k0
σn ∣S∣ (1+2 √t+2t)+r bι
bι(p+1-2∣S∣)
≤ P with probability at least 1 — exp(-(p + 1)(t —
Proof. We start from the useful inequality defined in 8
ɪ ∣Xδ*k2	- 1WlXδ*	+ XPλ(∣β*∣)	≤ XPλ(∣βjrue∣)	+ Γ,	(58)
n	n	j∈S	j∈S
where δ* = β* - βtrue. Next, conditioning on the fact (i) that β* is S3ONC, (ii) that all as-
sumptions for Corollary 4 are satisfied (which implies that Pλ(∖βj∖) ∈ {0,Pλ(aλ)}) and (iii) that
Pλ(∣βjrue∣) ≤ Pλ(aλ) we have that
，∣Xδ*k2 - 1 WlXδ* + kβ*∣0 ∙ Pλ(aλ) ≤ ∖S∖∙ Pλ(aλ)+Γ	(59)
2n	n
Now consider an event Ei := {∣β* - βtrueko = P + k} for an arbitrary integer k : 1 ≤ k ≤ P - P
Conditioning on this event, we may denote and S^+k ⊆ P such that δ* = 0 for all j ∈ S^+k. By
assumption we can ensure that ∖SP+k∖ = P + k. Also denote by Xsp+k = (Xij : i ∈ N,j ∈ S®+k)
and let δSp+k := (δj : j ∈ S_P+k). Note that conditional on Ei, the first part of Lemma 1 (using
P + k in place of P in Lemma 1) can be used to bound WlXδ* in 59. Additionally, by definition
∣∣βtrueko = ∖S∖ and conditional on Ei we can apply the substitution kβ*k0 ≥ P + k - ∖S∖. This
gives us
bι∣I XδT∣I2-ɪ
2 ii √n ii	√n
max ∣∣US W∣∣) 11 军
Sp+k：|Sp+k|=P+k Il Sp+k l∣7 ∣ √n
≤ -(P + k - 2∖S∖) ∙ pλ(aλ) + γ (6O)
In order for this equation to be feasible, we know that the quadratic formula must have real roots.
Therefor
19
Under review as a conference paper at ICLR 2020
max	I u√W
Sp+k：|Sp+k|=p+k Il	ʌ/n
2
-4[bι/2][(p+ k - 2|S|) ∙ Pλ(αλ) - Γ] ≥ 0	(61)
Now consier another event E2(t) ：= {max∣sj5+fc∣=p+fc ||U，十fcW|| ≤ σ√p + k ∙ √ 1 + 2册 + 2t}
for an arbitrary t > 0. Conditioning on the occurence of Ei ∩ E2(t) we can show, using first
E(t) and then 61, that 飞+k)∙ (1 + 2√t + 2t) ≥ ^max∣sp+fc∣=,p+fc "S舞W ) ≥ 2b，[(p +
k - 2|S|) ∙ Pχ(aλ) — Γ] almost surely, which contradicts with the assumption on the parameters
(a, λ). This can be seen starting from our original assumption that Pχ(aλ) > 2⅛ (1 + 2 Mt +
2t)+ *SS:⅞¾⅞+γ* ≥ 晶(1 + 2√t +2t)+ *;⅞+√¾∣)+γ* WeCanthenmUltiplyboth
(outer) sides by 2bι(p+ k - 2|S|) and rearrange to get σ2 (p + k) ∙ (1 + 2√t + 2t) < 2bι [(p- 2|S| 十
k) ∙ Pλ(aλ) - Γ]. Given this contradiction, we know that P [Ei ∩ E2(t)] = 0. Therefore, again using
the union bound combined with DeMorgan,s law we get that P [Ei ∩E2(t)] ≥ 1 - P [Ef] - P [E2(t)c]
which, can be simplified to
P [E2(t)c] ≥ P [Ei]	(62)
Since all assumptions of the Lemma 3 are satisfied, we can next use it to bound
P[E2(t)c]. By taking the compliment of the result in the second half of Lemma 1, we get,
forsome t0, that P maχsp+k：isp+k=(p+k)||U|p+kw∣∣ ≥σ/(p+乃+2，(力+k)，+20 ≤
(P+k)p+kexp(-10)	≤ pp+k exp(-10). If we then let t0	=	(p + k)t we get
PlmaXSp+k：|Sp+k∣=(P+k) IUp+kW|| ≥ σ√p+k ∙ √1 + 2√t + 2t]	≤ 产keχp(-(P + k)t).
Thus we have that pp+k exp(-(p + k)t) ≥ P[E2(t)c] which can be combined with 62 to show
pp+kexp(-(p + k)t ≥ P[∣ ∣ β* - βtrue∣∣ 0 = p+ k] ∀k ∈ Z : 1 ≤ k ≤ p -p. (63)
With this, we can solve for our desired value
p-p
P [∣∣β* -铲ueII0 ≤ P = 1 - P [∣∣β* -铲ueII0 ≥ p+1] =1 - EP [∣∣β* -铲 ueII0 = P + k]
k=1
P-P
≥1-	exp((p + k)(ln P — t))
k=1
1 - exp(-(p+ 1)(t - lnp)) ∙
1 — exp(-(p — p)(t — ln p))
1 - exp(-t + ln p)
(64)
Which is the desired result.
□
Lemma 6. Consider an arbitrary S3ONC solution β* to 2 with MCP. Let Assumptions (A1) and
(A3) with p* ≥ P hold. Assume the SatiSfaCtiOn of ∣∣β* — βtrue∣∣o ≤ P and Event Ea(P):=
{ n IlX (β*- βtrue)k2 ≤ 僚(p + 2√pt +2t) + b min(福(⑸-仍用田式。》)-。SHIβ*∣o) +
Γ}. Ifthe SUb-OPtimaIity gap satisfies Γ < Pλ(αλ)—焉(p + 2√pt + 2t). Ifthe minimum signal
strength satisfies ∣∣βlruekmin > ʌ/ 7^ ^p + 2√pt + 2t) + 7p^ min( λ∣ |S|, PA(aA)|S| +Γ} then
β* is the oracle solution to 2.
20
Under review as a conference paper at ICLR 2020
If in addition we have the satisfaction of ∣∣βopt 一 βtruek ≤ P and the event Eb(P):=
{1 ∣X (βopt- βtrue)k* 2 ≤ 鼠(P + 2√pt +2t) + bl min{卷(|S| - kβ*ko),P√aλ) ∙ (|S| -
kβ* ∣o) + Γ} then β* is both the oracle solution and the global solution to 2.
Proof. First, let Us denote β* 一 βtrue = δ*. We start by combining Ea(P) and (A3)iii, which is
possible due to our assumption ∣∣β* 一 βtrue∣o ≤ p. This gives us
8σ2 / L 8	8 λ2
pn (P+2Pρt+2t) + — min{—(ISl -Ile ι∣o),pλ(aλ) ∙(ISl-kβ IIO) 十 γ}
l n	l	rp
≥ 1 ∣Xδ*k2 ≥rp ∣δ*k2 a.s.
(65)
From here if we relax ∣S∣ 一 ∣∣βS k° to just ∣S∣, the definition of δ* and note that ∣∣δ*∣ ≥ ∣∣δ*∣∣, we
can obtain the following
8	λ2
+ rpbi min{ *|S1，piQ)1S|+r}
(66)
≥ ∣∣β* - βjrue∣∣ ≥ Iβjrue∣-Iβj∏,
almost surely. From this We can bound ∣β* ∣ using the square root term and ∣βjrue∣, so We know that
this we can obtain the inequality
+ rp8bι min{λplSl,pλ(aλ)lSl +r} > 0 then lβjl
> 0. From
kβs ko ≥ EIMrUeI-
j∈S
⅛n (p+2Ppt+2t) + rpbi min{ ⅛ lSl,Pλ(aλ)lsl+r} >0)
(67)
almost surely. We can then combine this with our minimum signal strength assumption to get that
kβS∣0 = |S| a.s.	(68)
We can combine this with equation 65, by focusing on the second part of the minimum term and
noting the right side is always positive to get
8σ2	8
b2n (p+2 ʌ/pt+2t) + — (-Pλ(aλ) IleSc ko+r) ≥ 0 a.s.	(69)
which can be simplified into
2
bn (P+2Ppt+2t) + γ ≥ Pλ(aλ) kβscko a.s.	(7o)
thus, it can be seen that if Pλ(aλ) > bσL (/+ 2√Pt + 2t) + Γ then 1 > kβSck0 = 0. This is
satisfied by the assumption that Pλ(aλ) 一 篇(P+ 2√Pt + 2t) > Γ.
Finally, because β* is an S3 *ONC solution, it has to satisfy FONC. Per 2, this means that β* ∈
arginf{ 1 Pi∈N'(β,Xi,yi) + Pj∈p P](∣β*∣)∣βjI : β ∈ <p}. Due to Corollary 4, we know that
the penalty term goes to 0 since either βj = 0 or P0(∣β*∣) = P0(∣aλ∣) = 0. Further we know that
βj = 0 for all j ∈ Sc. Threfore we know that
β* ∈ arg inf{— ^X '(β,Xi,yi): β ∈<p,βj = 0, ∀j ∈ Sc} a.s.	(71)
ni∈N
21
Under review as a conference paper at ICLR 2020
Given that the expression on the right is the definition of the oracle solution, we have shown the first
result.
Next, Consider βopt which is the global optimal solution to 2. Given that the S3ONC conditions
are necessary, βopt must be an S3ONC solution. With this fact and the assumption of Eb(p), We
have the same set of assumptions for βopt as we had for β*. Thus the same sequence of arguments
can be used to shoW that
β°pit ∈ arg inf {— ^X '(β,Xi,yi): β ∈ <p, βj =0, ∀j ∈ S c} a.s.	(72)
Finally, per the strict convexity of our loss function as implied by (A1) we can see that the infimum
of the above problem is unique. Therefor
β* =arginf{1 X '(β,xi,yi): β ∈<p,βj = 0,∀j ∈ SC} = βopt a.s.	(73)
i∈N
Which is the second result.
□
A.3 Additional lemmas
Lemma 7. The RE condition in 1 implies (A3) with rd§ ≥ r% > 0 andp* ≥ 4s.
Proof. As in Lemma 1 in (LiU et al., 2017).	□
Lemma 8. Let β* be a S3ONC solution to 2 Given (A1) and that Q(β*) ≤ Q(βtrue) + Γ holds
for some Γ ≥ 0 then
以 ∣∣Xδ*k2 - 1WlXδ*
2n U	U n
≤ min ∖ XPλ(lβ*l)∣βjrue∣, XPλ(∣β*l)∣β* — βjruel, Pλ(aλ)(∣S∣-∣β*∣∣0) + Γ 卜 a.s.
[j∈s	j∈s
(74)
np
Proof. First, we know that β* ∈ argmin{P '(β,xi,yi) + P P](∣β*∣)∣βj∣} because the KKT
β i=1	j=1
n
conditions are the same as FONC which β* satisfies. This gives us that P '(β*,χ%,yi) +
i=1
pnp
P P](∣β*∣)∣β*∣ ≤ P '(βtrue,Xi,yi) + P P](∣β*∣)∣βjrue∣. This can be used along the
j=1	i=1	j=1
same lines as the level set inequality in the derivation for 8 to get 品 ∣∣Xδ*∣2 一 1WlXδ* ≤
p
P Pλ (β*)(∣βjrue∣-∣β*l)
j=1
The first two terms of the min function are easily obtained from this. The last term can be obtained
from 8 by noting that due to Corollary 4, β* ∈/ (0, aλ) and that P](aλ) = P](β) ∀β ≥ aλ. This
gives us that 品 ∣∣Xδ*∣2 一 ɪ W | Xδ* ≤ P] (aλ)(S-∣β*ko)+Γ Which is the final term to complete
the desired result.
□
Lemma 9. COnsiderasubgaussian n-dimensional random vector W ∈ <n as defined in(A2). Then
for any V ∈ <n×n and ∑v = V1V then P [∣∣VIW∣∣ ≤ σ2 ∙ (Tr (∑v) + 2, Tr(∑2 )t + 2 ∣∣∑v ∣∣ t)] ≥
1 — exp(-1) for any t > 0 where Tr(∙) denotes the trace ofa matrix.
22
Under review as a conference paper at ICLR 2020
Proof. We apply Theorem 2.1 in (Hsu et al., 2012) where our W,V and Σv are equivalent to their
x, A and Σ. Note their expectation condition is equivalent to our (A2) with μ = E[W] = 0. This
gives us that for all t > 0
P[kVWk2 > σ2 ∙ (tr(∑v) + 2,tr(∑v)t + 2 k∑0k t)
+ tr(∑vμμl) ∙ (1 + 2(味£t)1/2)i ≤ exp(-t).
tr(Σv)
(75)
Given that μ = 0, the term involving tr(∑vμμl) goes to zero and therefor the statement in 9 can be
obtained by taking the complement of the probability bound.
□
23