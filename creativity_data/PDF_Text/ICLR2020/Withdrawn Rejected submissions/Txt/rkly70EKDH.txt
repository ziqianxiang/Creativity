Under review as a conference paper at ICLR 2020
Mildly Overparametrized Neural Nets can
Memorize Training Data Efficiently - REVI-
SION
Anonymous authors
Paper under double-blind review
Ab stract
It has been observed (Zhang et al., 2017) that deep neural networks can memorize:
they achieve 100% accuracy on training data. Recent theoretical results explained
such behavior in highly overparametrized regimes, where the number of neurons
in each layer is larger than the number of training samples. In this paper, we show
that neural networks can be trained to memorize training data perfectly in a mildly
overparametrized regime, where the number of parameters is just a constant factor
more than the number of training samples, and the number of neurons is much
smaller.
1	Introduction
In deep learning, highly non-convex objectives are optimized by simple algorithms such as stochastic
gradient descent. There has been many theoretical analysis for the optimization landscape of neural
networks(e.g., Brutzkus et al. (2017); Brutzkus & Globerson (2017); Ge et al. (2017b); Wang et al.
(2018)), but even very simple two-layer networks have spurious local optima(Safran & Shamir,
2018). In practice, it was observed that neural networks are able to fit the training data perfectly,
even when the data/labels are randomly corrupted(Zhang et al., 2017). Recently, a series of work
(Du et al. (2019); Allen-Zhu et al. (2019c); Chizat & Bach (2018); Jacot et al. (2018), see more
references in Section 1.2) developed a theory of neural tangent kernels (NTK) that explains the
success of training neural networks through overparametrization. Several results showed that if the
number of neurons at each layer is much larger than the number of training samples, networks of
different architectures (multilayer/recurrent) can all fit the training data perfectly.
However, if one considers the number of parameters required for the current theoretical analysis,
these networks are highly overparametrized. Consider fully connected networks for example. If a
two-layer network has a hidden layer with r neurons, the number of parameters is at least rd where
d is the dimension of the input. For deeper networks, if it has two consecutive hidden layers of size
r, then the number of parameters is at least r2 . All of the existing works require the number of
neurons r per-layer to be at least the number of training samples n (in fact, most of them require
r to be a polynomial of n). In these cases, the number of parameters can be at least nd or even
n2 for deeper networks -much larger than the number of training samples n. Therefore, a natural
question is whether neural networks can fit the training data in the mildly overparametrized regime -
where the number of parameters is only a constant factor larger than the number of training data. To
achieve this, one would want to use a small number of neurons in each layer - n/d for a two-layer
network and √n for a three-layer network. Yun et al. (2018) showed such networks have enough
capacity to memorize any training data. In this paper we show with polynomial activation functions,
simple optimization algorithms are guaranteed to find a solution that memorizes training data.
1.1	Our Results
In this paper, we give network architectures (with polynomial activations) such that every layer has
width much smaller than the number of training samples n, the total number of parameters is linear
in n, and simple optimization algorithms on such neural networks can fit any training data. We first
give a warm-up result that works when the number of training samples is roughly d2 (where d is the
input dimension).
1
Under review as a conference paper at ICLR 2020
Theorem 1 (Informal). Suppose there are n ≤ d+2 1 training samples in general position, there
exists a two-layer neural network with quadratic activations, such that the number of neurons in the
hidden layer is 2d+ 2, the total number of parameters is O(d2), and perturbed gradient descent can
fit the network to any output.
Here “in general position” will be formalized later as a deterministic condition that is true with
probability 1 for random inputs, see Theorem 4 for details.
In this case, the number of hidden neurons is only roughly the square root of the number of training
samples, so the weights for these neurons need to be trained carefully in order to fit the data. Our
analysis relies on an analysis of optimization landscape - we show that every local minimum for such
neural network must also be globally optimal (and has 0 training error). As a result, the algorithm
can converge from an arbitrary initialization.
Of course, the result above is limited as the number of training samples cannot be larger than O(d2).
We can extend the result to handle a larger number of training samples:
Theorem 2 (Informal). Suppose number of training samples n ≤ dp for some constant p, if the
training samples are in general position there exists a three-layer neural network with polynomial
activations, such that the number of neurons r in each layer is Op(√n), and perturbed gradient
descent on the weights of the middle layer can fit the network to any output.
Here Op considers p as a constants and hides constant factors that only depend on p. We consider
“in general position” in the smoothed analysis framework(Spielman & Teng, 2004) - given arbitrary
inputs xi ,X2,…，xn ∈ Rd,fixa perturbation radius √v, the actual inputs is Xj = Xj +Xj where Xj 〜
N(0,vI). The guarantee of training algorithm will depend inverse polynomially on the perturbation
v (note that the architecture - in particular the number of neurons - is independent of v). The formal
result is given in Theorem 5 in Section 4. Later we also give a deterministic condition for the inputs,
and prove a slightly weaker result (see Theorem 6).
1.2	Related Works
Optimization Landscape for Networks without Overparametrization Many works (Brutzkus
& Globerson, 2017; Tian, 2017; Li & Yuan, 2017; Soltanolkotabi, 2017; Zhong et al., 2017; Ge et al.,
2017b) analyzed the optimization landscape of 2-layer neural networks. However, these works either
work on a single neuron or have very strong assumptions on the input X (such as X is Gaussian). It is
also known that even with strong assumptions on input X gradient descent on the standard objective
can get stuck in spurious local minima when the network has more than a constant number of neurons
(Safran & Shamir, 2018).
Neural Tangent Kernel Many results in the framework of neural tangent kernel show that net-
works with different architecture can all memorize the training data, including two-layer (Du et al.,
2019), multi-layer(Du et al., 2018; Allen-Zhu et al., 2019c; Zou & Gu, 2019), recurrent neural net-
work(Allen-Zhu et al., 2019b). However, all of these works require the number of neurons in each
layer to be at least quadratic in the number of training samples. Oymak & Soltanolkotabi (2019)
improved the number of neurons required for two-layer networks, but their bound is still larger than
the number of training samples. There are also more works for NTK on generalization guarantees
(e.g., Allen-Zhu et al. (2019a)), fine-grained analysis for specific inputs(Arora et al., 2019b) and
empirical performances(Arora et al., 2019c), but they are not directly related to our results.
Representation Power of Neural Networks For standard neural networks with ReLU activations,
Yun et al. (2018) showed that networks of similar size as Theorem 2 can memorize any training data.
Their construction is delicate and it is not clear whether gradient descent can find such a solution.
Matrix Factorizations and Quadratic Activations Since the activation function for our two-
layer net is quadratic, training of the network is very similar to matrix factorization problem. Many
existing works analyzed the optimization landscape and implicit bias for problems related to matrix
factorization in various settings(Bhojanapalli et al., 2016; Ge et al., 2016; 2017a; Park et al., 2016;
Gunasekar et al., 2017; Li et al., 2018; Arora et al., 2019a). In this line of work, Du &Lee (2018) and
Soltanolkotabi et al. (2018) are the most similar to our two-layer result. Du & Lee (2018) showed
2
Under review as a conference paper at ICLR 2020
how gradient descent can learn a two-layer neural network that represents any positive semidefinite
matrix. However positive definite matrices cannot be used to memorize arbitrary data, and our
two-layer network can represent an arbitrary matrix. Soltanolkotabi et al. (2018) is very similar to
our two-layer result except they require the input to be Gaussian. Extending this condition to our
deterministic condition is crucial to our main (3-layer) result.
Existing works on mildly overparametrization There are several works on overparametrization
that does not fall exactly into the NTK regime. Brutzkus et al. (2017); Wang et al. (2018) works
for linear separable setting. However in this setting gradient descent also works even if the network
has only a single neuron (no overparametrization), so the results only show that overparametrization
does not hurt. Li & Liang (2018) requires an interesting but strong assumption on the input data.
Our work is different as we make mild assumptions on the data distribution and the network size is
necessary (up to constant factors) to achieve 0 training error without further assumptions.
Interpolating Methods Of course, simply memorizing the data may not be useful in machine
learning. However, recently several works(Belkin et al., 2018; 2019; Liang et al., 2019; Mei &
Montanari, 2019) showed that learning regimes that interpolate/memorize data can also have gener-
alization guarantees. Proving generalization for our architectures is an interesting open problem.
2	Preliminaries
In this section, we introduce notations, the two neural network architectures used for Theorem 1 and
2, and the perturbed gradient descent algorithm.
2.1	Notations
We use [n] to denote the set {1, 2, ..., n}. For a vector x, we use kxk2 to denote its `2 norm, and
sometimes kxk as a shorthand. For a matrix M, we use kMkF to denote its Frobenius norm, kMk
to denote its spectral norm. We will also use λi (M) and σi(M) to denote the i-th largest eigenvalue
and singular value of matrix M, and λmin(M), σmin (M) to denote the smallest eigenvalue/singular
value.
For the results of three-layer networks, our activation is going to be xp, where p is considered as a
small constant. We use Op(), Ωp() to hide factors that only depend on p.
For vectors χ,y ∈ Rd, the tensor product is denoted by (X 0 x) ∈ Rd2. We use X3P ∈ Rdp as a
shorthand for p-th power ofx in terms of tensor product. For two matrices M, N ∈ Rd1 ×d2, we use
M 0 N ∈ Rd12 ×d22 denote the Kronecker product of2 matrices.
2.2	Network Architectures
In this section, we introduce the neural net architectures we use. As we discussed, Theorem 1 uses
a two-layer network (see Figure 1 (a)) and Theorem 2 uses a three-layer network (see Figure 1 (b)).
Two-layer Neural Network For the two-layer neural network, suppose the input samples X are in
Rd, the hidden layer has r hidden neurons (for simplicity, we assume r is even, in Theorem 4 we
will show that r = 2d + 2 is enough). The activation function of the hidden layer is σ(X) = X2.
We use wi ∈ Rd to denote the input weight of hidden neuron i. These weight vectors are collected
as a weight matrix W = [w1, w2, . . . , wr] ∈ Rd×r. The output layer has only 1 neuron, and we use
ai ∈ R to denote the its input weight from hidden neuron i. There is no nonlinearity for the output
layer. For simplicity, We fix the parameters ai,i ∈ [r] in the way that a% = 1 for all 1 ≤ i ≤ 2
and ai = -1 for all 2 + 1 ≤ i ≤ r. Given X as the input, the output of the neural network is
y = Pir=1 ai (wiT X)2.
3
Under review as a conference paper at ICLR 2020
Input	Hidden	Output
layer	layer layer
Input	Hidden	Hidden	Output
layer	layer 1	layer 2	layer
(a) 2-layer neural net structure
(b) 3-layer neural net structure
Figure 1: Neural Network Architectures. The trained layer is in bold face. The activation function
after the trained parameters is x2(blue neurons). The activation function before the trained parame-
ters is xp (purple neurons).
If the training samples are {(xj , yj )}j≤n, we define the empirical risk of the neural network with
parameters W to be
f(W) = 4nX(Xai(wTXj)2-yJ .
Three-layer neural network For Theorem 2, we use a more complicated, three-layer neural net-
work. In this network, the first layer has a polynomial activation τ(x) = xp, and the next two layers
are the same as the two-layer network.
We use R = [r1, . . . , rk]T ∈ Rk×d to denote the weight parameter of the first layer. The first hidden
layer has k neurons with activation τ(x) = xp where p is the parameter in Theorem 2. Given input
x, the output of the first hidden layer is denoted as z, and satisfy zi = (riTxj)p.The second hidden
layer has r neurons (again we will later show r = 2k + 2 is enough). The weight matrix for second
layer is denoted as W = [w1, . . . , wr] ∈ Rk×r where each wi ∈ Rk is the weight for a neuron in
the second hidden layer. The activation for the second hidden layer is σ(x) = x2. The third layer
has weight a and is initialized the same way as before, where aι = a2 = •一=a//2 = 1, and
ar/2+1 = •一 =ar = -1. The final output y can be computed as y = P；=i ai(wf z)2.
Given inputs (x1, y1), ..., (xn, yn), suppose zi is the output of the first hidden layer for xi, the
empirical loss is defined as:
f (W) = 4n X (X ai(WTZj)2 - yj!.
Note that only the second-layer weight W is trainable. The first layer with weights R acts like a
random feature layer that maps xi’s into a new representation zi’s.
2.3	Second order stationary points and perturbed gradient descent
Gradient descent converges to a global optimum of a convex function. However, for non-convex
objectives, gradient descent is only guaranteed to converge into a first-order stationary point - a
point with 0 gradient, which can be a local/global optimum or a saddle point. Our result requires
any algorithm that can find a second-order stationary point - a point with 0 gradient and positive
definite Hessian. Many algorithms were known to achieve such guarantee(Ge et al., 2015; Sun et al.,
2015; Carmon et al., 2018; Agarwal et al., 2017; Jin et al., 2017a;b). As we require some additional
4
Under review as a conference paper at ICLR 2020
properties of the algorithm (see Section 3), we will adapt the Perturbed Gradient Descent(PGD, (Jin
et al., 2017a)). See Section B for a detailed description of the algorithm. Here we give the guarantee
of PGD that we need. The PGD algorithm requires the function and its gradient to be Lipschitz:
Definition 1 (Smoothness and Hessian Lipschitz). A differentiable function f (∙) is '-smooth if:
∀X1,X2, ||Vf (xi) - Vf(X2)|| ≤ '∣∣χι - X2∣∣.
A twice-differentiable function f (∙) is P-Hessian Lipschitz if:
∀X1,X2, ∣∣V2f(xι) - V2f (x2)∖∖ ≤ ρ∖∖xι - X2∣∣.
Under these assumptions, we will consider an approximation for exact second-order stationary point
as follows:
Definition 2 (ε-second-order stationary point). For a P-Hessian Lipschitzfunction f (∙), we say that
X is an ε-second-order stationary point if: ∖∖Vf (χ)∖∖ ≤ ε, and λma(V2f (x)) ≥ -√ρε.
Jin et al. (2017a) showed that PGD converges to an ε-second-order stationary point efficiently:
Theorem 3 (Convergence of PGD (Jin et al.(2017a))). Assume that f(∙) is '-smooth andP-Hessian
Lipschitz. Then there exists an absolute constant Cmax such that, for any δ > 0,ε ≤ 'ρ, ∆f ≥
f (xo) — f *, and constant C ≤ Cmax PGD(x0,', ρ, ε,c,δ, ∆f) will output an ε-SeCOnd-Order Sta-
tionary point with probability 1 - δ, and terminate in the following number of iterations:
o( fεf 1og4(号)).
3	Warm-up: Two-layer Net for Fitting Small Training Set
In this section, we show how the two-layer neural net in Section 2.2 trained with perturbed gradient
descent can fit any small training set (Theorem 1). Our result is based on a characterization of
optimization landscape: for small enough ε, every ε-second-order stationary point achieves near-
zero training error. We then combine such a result with PGD to show that simple algorithms can
always memorize the training data. Detailed proofs are deferred to Section D in the Appendix.
3.1	Optimization landscape of two -layer neural network
Recall that the two-layer network we consider has r-hidden units with bottom layer weights
w1, w2, ..., wr, and the weight for the top layer is set to ai = 1 for 1 ≤ i ≤ r/2, and ai = -1
for r/2 + 1 ≤ i ≤ r. For a set of input data {(x1, y1), (x2, y2), ..., (xn, yn)}, the objective function
is definedas f (W) = 4n Pn=I (Pr=I ai(WT Xj )2 - y )2.
With these definitions, we will that when a point is an approximate second-order stationary point (in
fact, we just need it to have an almost positive semidefinite Hessian) it must also have low loss:
Lemma 1 (Optimization Landscape). Given training data {(X1, y1), (X2, y2), ..., (Xn, yn)}, Sup-
pose the matrix X = [x?2,...,x之2] ∈ Rd2×n has full COlUmn rank and the smallest singular
value is at least σ. Also suppose that the number of hidden neurons satisfies r ≥ 2d + 2. Then if
λminV2f (W) ≥ -ε, thefunction value is bounded by f (W) ≤ n^.
Soltanolkotabi et al. (2018) gave a similar characterization of the landscape, except their theorem
requires Xi ’s to be Gaussians.
3.2	Optimizing the two-layer neural net
Given the property of the optimization landscape for f(W), it is natural to try to use PGD to find
a second-order stationary point. However, this is not enough since the function f does not have
bounded smoothness constant and Hessian Lipschitz constant, and without improved analysis PGD
is not guaranteed to converge in polynomial time. In order to control the Lipschitz parameters, we
5
Under review as a conference paper at ICLR 2020
note that these parameters are bounded when the norm of W is bounded (see Lemma 5 in appendix).
Therefore we add a small regularizer term to control the norm of W. More concretely, we optimize:
g(w ) = f(w) + 2 I∣W IlF.
We want to use this regularizer term to show that: 1. the optimization landscape is preserved: for
appropriate γ, any ε-second-order stationary point of g(W) will still give a small f(W); and 2.
During the training process of the 2-layer neural network, the norm of W is bounded, therefore
the smoothness and Hessian Lipschitz parameters are bounded. Then, the proof of Theorem 1 just
follows from the combination of Theorem 3 of PGD and the result of the geometric property.
The first step is simple as the regularizer only introduces a term γI to the Hessian, which increases
all the eigenvalues by γ. Therefore any ε-second-order stationary point of g(W) will also lead to
the fact that ∣λminV2f (W)∣ is small, and hence f (W) is small by Lemma 1.
For the second step, note that in order to show the training process using PGD will not escape
from the area {W : ||W ||2F ≤ Γ} with some Γ, it suffices to bound the function value g(W) by
YΓ∕2, which implies ∣∣ W∣∣F ≤ γ2g(W) ≤ Γ. To bound the function value We use properties of
PGD: for a gradient descent step, since the function is smooth in this region, the function value
always decreases; for a perturbation step, the function value can increase, but cannot increase by too
much. Using mathematical induction, we can show that the function value ofg is smaller than some
fixed value(related to the random initialization but not related to time t) and will not escape the set
{W: ||W||2F ≤ Γ} for appropriate Γ. Combining these analysis we have the following theorem:
Theorem 4 (Main theorem for 2-layer NN). Suppose the matrix X = [xf2,..., x®2] ∈ Rd2×n has
full column rank and the smallest singular value is at least σ. Also assume that we have ||xj ||2 ≤ B
and |yj | ≤ Y for all j ≤ n. We choose our width of neural network r ≥ 2d + 2 and we choose
P = (6B4p2(f (0) + 1)) (nd∕(σ2ε))1/4, Y = (σ2ε∕nd)1/2, and ' = max{(3B42(f(Y)+1) +
Y B2 + γ), 1}. Then there exists an absolute constant cmax such that, for any δ > 0, ∆ ≥ f(0) + 1,
and constant C ≤ Cmax, PGD(0, ', ρ, ε, c, δ, ∆) on W will output an parameter W * such that with
probability 1 一 δ, f (W *) ≤ ε when the algorithm terminates in thefollowing number of iterations:
O
B 8'(nd)5/2(f (0) + 1)2
Bnrd'∆(f (0) + 1)
σ5ε5/2
ε2δσ
4	Three-Layer Net for Fitting Larger Training Set
In this section, we show how a three-layer neural net can fit a larger training set (Theorem 2). The
main limitation of the two-layer architecture in the previous section is that the activation functions
are quadratic. Therefore, no matter the number neurons in the hidden layer, the whole network only
captures a quadratic function over the input, and cannot fit an arbitrary training set of size much
larger than d2 . On the other hand, if one replaces the quadratic activation with other functions, it is
known that even two-layer neural networks can have bad local minima(Safran & Shamir, 2018).
To address this problem, the three-layer neural net in this section uses the first-layer as a random
mapping of the input. The first layer is going to map inputs x/s into zi,s of dimension k (where
k = Θ(√n)). If Zi's satisfy the requirements of Theorem 4, then we can use the same arguments as
the previous section to show perturbed gradient descent can fit the training data.
We prove our main result in the smoothed analysis setting, which is a popular approach for going
beyond worst-case. Given any worst-case input {x1, x2, ..., xn }, in the smoothed analysis frame-
work, these inputs will first be slightly perturbed before given to the algorithm. More specifically, let
Xj = Xj + Xj, where Xj ∈ Rd is a random Gaussian vector following the distribution of N(0, vI).
Here the amount of perturbation is controlled by the variance v . The final running time for our algo-
rithm will depend inverse polynomially on v. Note that on the other hand, the network architecture
and the number of neurons/parameters in each layer does not depend on v.
Let {zι, Z2,…，Zn} denote the output of the first layer with (zj∙)i = (rf Xj)p(j = 1, 2,..., n), we
first show that {zj}’s satisfy the requirement of Theorem 4:
Lemma 2. Suppose k ≤ OpW) and (k+1) > n, let Xj = Xj + Xj be the perturbed input in
the smoothed analysis setting, where Xj 〜 N (0,vI), let {zι, z2,..., Zn} be the output of the first
6
Under review as a conference paper at ICLR 2020
layer on the perturbed input ((Zj)i = (rTXj)p). Let Z ∈ Rk2×n be the matrix whose j-th column
is equal to z；2, then with probability at least 1 一 δ, the smallest singular value of Z is at least
Ωp(vpδ4p∕n2p+1∕2k4p).
This lemma shows that the output of the first layer (zj ’s) satisfies the requirements of Theorem 4.
With this lemma, we can prove the main theorem of this section:
Theorem 5 (Main theorem for 3-layer NN). Suppose the original inputs satisfy kxj k2 ≤ 1, |yj | ≤ 1,
inputs Xj = Xj + Xj are perturbed by Xj 〜 N(0, vI), with probability 1 一 δ over the random
initialization, for k = 2d√n∖, perturbed gradient descent on the second layer weights achieves a
loss f (W*) ≤ e in Op(1) ∙⑺/以⑺ log4(n∕e) iterations.
Using different tools, we can also prove a similar result without the smoothed analysis setting:
Theorem 6. Suppose the matrix X = [X21p, ..., X2np] ∈ Rd2p×n has full column rank, and smallest
singular value at least σ. Choose k = Op(dp), with high probability perturbed gradient descent on
the second layer weights achieves a loss f (W*) ≤ e in Op(1) ∙ 肾声, log4(n∕e) iterations.
When the number of samples n is smaller than d2p∕(2p)!, one can choose k = Op(dp), in this
regime the result of Theorem 6 is close to Theorem 5. However, if n is just larger, say n = d2p , one
may need to choose k = Op(dp+1), which gives sub-optimal number of neurons and parameters.
Proof techniques Theorem 5 relies on Theorem 4 - it suffices to prove the two requirements in
in Theorem 4: (a) The norm of output from each input sample is bounded with high probability;
(b) The matrix formed by the tensor of each output (Matrix Z in Lemma 2) has a high-probability
positive smallest singular value lower bound. The proof of (a) is relatively simple as one only needs
standard concentration bounds. The condition (b) is stated in Lemma 2.
To prove Lemma 2, we need to show that the vectors z j2 are linearly independent (and well-
conditioned). We prove that by using a technique called leave-one-out distance, which is widely
used in random matrix theory (e.g., in Rudelson & Vershynin (2009)). Roughly speaking, leave-
one-out distance requires us to prove that for any fixed j, the vector z：2 is far from the span of all
the other z：2(l = j). We in fact show something stronger: for any fixed linear subspace (that is
independent of Xj) of dimension at most n _ 1, the distance between z：2 and this subspace is large.
This is done using anti-concentration inequalities for polynomials. Of course, there are additional
challenges in this approach, as the entries of zj： 2 are not independent.
to handle the additional dependencies. See details in Appendix E.
We use decoupling techniques
5	Experiments
In this section, we validate our theory using experiments. Detailed parameters of the experiments as
well as more result are deferred to Section A in Appendix.
Small Synthetic Example We first run gradient descent on a synthetic data-set, which fits into
the setting of Theorem 4. Our training set, including the samples and the labels, are generated from
a fixed normalized uniform distribution(random sample from a hypercube and then normalized to
have norm 1). As shown in Figure 2, simple gradient descent can already memorize the training set.
MNIST Experiment We also show how our architectures (two-layer and three-layer) can be used
to memorize MNIST. For MNIST, we use a squared loss between the network’s prediction and the
true label (which is an integer in {0, 1, ..., 9}). For the two-layer experiment, we use the original
MNIST dataset, with a small Gaussian perturbation added to the data to make sure the condition in
Theorem 4 is satisfied. For the three-layer experiment, we use PCA to project MNIST images to
100 dimensions (so the two-layer architecture will no longer be able to memorize the training set).
See Figure 3 for the results. In this part, we use ADAM as the optimizer to improve convergence
speed, but given the main result on optimization landscape the algorithm is flexible.
7
Under review as a conference paper at ICLR 2020
Figure 2: Training loss for random sample experiment
(a) Two-layer network with perturbation on input (b) Three-layer network on top 100 PCA direc-
tions
Figure 3: MNIST with original label
MNIST with random label We further test our results on MNIST with random labels to verify
that our result does not use any potential structure in the MNIST datasets. The setting is exactly the
same as before. As shown in Figure 4, the training loss can also converge.
(a) Two-layer network with perturbation on input (b) Three-layer network on top 100 PCA direc-
tions
Figure 4: MNIST with random label
6	Conclusion
In this paper, we showed that even a mildly overparametrized neural network can be trained to
memorize the training set efficiently. The number of neurons and parameters in our results are tight
(up to constant factors) and matches the bounds in Yun et al. (2018). There are several immediate
open problems, including generalizing our result to more standard activation functions and providing
generalization guarantees. More importantly, we believe that the mildly overparametrized regime is
more realistic and interesting compared to the highly overparametrized regime. We hope this work
would be a first step towards understanding the mildly overparametrized regime for deep learning.
8
Under review as a conference paper at ICLR 2020
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi-
mate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, pp.1195-1199. ACM, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In Advances in neural information processing systems, 2019b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019c.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019a.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019b.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019c.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for
classification and regression rules that interpolate. In Advances in Neural Information Processing
Systems, pp. 2300-2311, 2018.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contra-
dict statistical optimality? In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1611-1619, 2019.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for
low rank matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873-3881,
2016.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
605-614. JMLR. org, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Anthony Carbery and James Wright. Distributional and lq norm inequalities for polynomials over
convex bodies in rn. Mathematical research letters, 8(3):233-248, 2001.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex
optimization. SIAM Journal on Optimization, 28(2):1751-1772, 2018.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3036-3046, 2018.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with
quadratic activation. In International Conference on Machine Learning, pp. 1328-1337, 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
9
Under review as a conference paper at ICLR 2020
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In
Advances in Neural Information Processing Systems, pp. 2973-2981, 2016.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
unified geometric analysis. In International Conference on Machine Learning, 2017a.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017b.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1724-1732. JMLR. org, 2017a.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2-47, 2018.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the risk of minimum-norm interpolants
and restricted lower isometry of kernels. arXiv preprint arXiv:1908.10292, 2019.
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-of-
squares. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS),
pp. 438-446. IEEE, 2016.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674, 2019.
Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, and Sujay Sanghavi. Non-square
matrix sensing without spurious local minima via the burer-monteiro approach. arXiv preprint
arXiv:1609.03240, 2016.
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 62(12):1707-1739, 2009.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4430-4438, 2018.
10
Under review as a conference paper at ICLR 2020
Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information
Processing Systems, pp. 2007-2017, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. Journal of the ACM (JACM), 51(3):385-463, 2004.
Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint
arXiv:1510.06096, 2015.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge University Press, 2018.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. arXiv preprint arXiv:1808.04685, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight
analysis of memorization capacity. arXiv, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. arXiv preprint arXiv:1906.04688, 2019.
11
Under review as a conference paper at ICLR 2020
A	More Experiments and Detailed Experiment Setup
A.1 Experiments setup
In this section, we introduce the experiment setup in detail.
Small Synthetic Example We generate the dataset in the following way: We first set up a random
matrices X ∈ RN ×d(samples), where N is the number of samples, d is the input dimension and
Y ∈ RN (labels). Each entry in X or Y follows a uniform distribution with support [-1, 1]. Each
entry is independent from others. Then we normalize the dataset X such that each row in X has
norm 1, denote the normalized dataset as X = [Xi,...,Xn]t. Then We compute the smallest
singular value for the matrix [χf2,..., XN2]T, and we feed the normalized dataset X into the two-
layer netWork(Section 2.2) With r hidden neurons. We select all the parameters as shoWn in Theorem
4, and plot the function value for f (∙).
In our experiment for the small artificial random dataset, we choose N = 300, d = 100, and
r= 300.
MNIST experiments For MNIST, we use a squared loss between the network’s prediction and
the true label (which is an integer in {0, 1, ..., 9}).
For the first two-layer network structure, we first normalize the samples in MNIST dataset to have
norm 1. Then we set up a two-layer network with quadratic activation with r = 3000 hidden neurons
(note that although our theory suggests to choose r = 2d+2, having a larger r increases the number
of decreasing directions and helps optimization algorithms in practice). For these experiments, we
use Adam optimizer(Kingma & Ba, 2014) with batch size 128, initial learning rate 0.003, and decay
the learning rate by a factor of 0.3 every 15 epochs (we find that the learning rate-decay is crucial
for getting high accuracy).
We run the two-layer network in two settings, one for the original MNIST data, and one for the
MNIST data with a small Gaussian noise (0.01 standard deviation per coordinate). The perturbation
is added in order for the conditions in Theorem 4 to hold.
For the three-layer network structure, we first normalize the samples in MNIST dataset with norm
1. Then we do the PCA to project it into a 100-dimension subspace. We use D = [x1, . . . , xn] to
denote this dataset after PCA. Note that the original 2-layer network may not apply to this setting,
since now the matrix X = [xf2,..., X消 does not have full column rank(60000 > 1002). We then
add a small Gaussian perturbation to D 〜N(0, σ2) to the sample matrix D and denote the perturbed
matrix D = [Xi,.. .,Xn]. We then randomly select a matrix Q 〜N (0, σ2)k×d and compute the
random feature Zj = (QXj)2, where (∙)2 denote the element-wise square. Then we feed this sample
into the 2-layer neural network with hidden neuron d. Note that this is equivalent to our three-layer
network structure in Section 2.2. In our experiments, k = 750, r = 3000, σ1 = 0.05, σ2 = 0.15.
MNIST with random labels These experiments have exactly the same set-up as the original
MNIST experiments, except that the labels are replaced by a random number in {0,1,2,...,9}.
A.2 Experiment Results
In this section, we give detailed experiment results with bigger plots. For all the training loss graphs,
we record the training loss for every 5 iterations. Then for the ith recorded loss, we average the
recorded loss from i - 19th to ith and set it as the average loss at (5i)th iteration. Then we take the
logarithm on the loss and generated the training loss graphs.
Small Synthetic Example As we can see in Figure 5 the loss converges to 0 quickly.
MNIST experiments with original labels First we compare Figure 6 and Figure 7. In Figure 6,
we optimize the two-layer architecture with original input/labels. Here the loss decreases to a small
value (〜 0.1), but the decrease becomes slower afterwards. This is likely because for the matrix
X defined in Theorem 4, some of the directions have very small singular values, which makes it
12
Under review as a conference paper at ICLR 2020
Training loss
(SSen) 6cη (Sa-J)601
Figure 5: Synthetic Example
Training loss
Figure 6: Two-layer network on original MNIST
much harder to correctly optimize for those directions. In Figure 7, after adding the perturbation
the smallest singular value of the matrix X becomes better, and as we can see the loss decreases
geometrically to a very small value (< 1e - 5).
A surprising phenomenon is that even though we offer no generalization guarantees, the network
trained as in Figure 6 has an MSE error of 1.21 when tested on test set, which is much better than a
random guess (recall the range of labels is 0 to 9). This is likely due to some implicit regularization
effect (Gunasekar et al., 2017; Li et al., 2018).
For three-layer networks, in Figure 8 we can see even though we are using only the top 100 PCA
directions, the three-layer architecture can still drive the training error to a very low level.
13
Under review as a conference paper at ICLR 2020
Training loss
(S5cη)6σl
Figure 7: Two-layer network on MNIST, with noise std 0.01
(S5cη)601
Figure 8: Three-layer network with top 100 PCA directions of MNIST, 0.05 noise per direction
MNIST with random label When we try to fit random labels, the original MNIST input does not
work well. We believe this is again because there are many small singular values for the matrix X in
Theorem 4, so the data does not have enough effective dimensions fit random labels. The reason that
it was still able to fit the original labels to some extent (as in Figure 6) is likely because the original
label is correlated with some features of the input, so the original label is less likely to fall into the
subspace with smaller singular values. Similar phenomenon was found in Arora et al. (2019b).
Once we add perturbation, for two-layer networks we can fit the random label to very high accuracy,
as in Figure 9. The performance for three-layer network in Figure 10 is also similar to Figure 8.
14
Under review as a conference paper at ICLR 2020
Training loss
0	5000 10000 15000 20000 25000 30000 35000
Figure 9: Two-layer network on MNIST, with noise std 0.01, random labels
Figure 10: Three-layer network with top 100 PCA directions of MNIST, 0.05 noise per direction,
random labels
B Detailed Description of Perturbed Gradient Descent
In this section we give the pseudo-code of the Perturbed Gradient Descent algorithm as in Jin et al.
(2017a), see Algorithm 1. The algorithm is quite simple: it just runs the standard gradient descent,
except if the loss has not decreased for a long enough time, it adds a perturbation. The perturba-
tion allows the algorithm to escape saddle points. Note that we only use PGD algorithm to find
a second-order stationary point. Many other algorithms, including stochastic gradient descent and
accelerated gradient descent, are also known to find a second-order stationary point efficiently. All
these algorithms can be used for our analysis.
15
Under review as a conference paper at ICLR 2020
Algorithm 1 Perturbed Gradient Descent
Input: x0, `, ρ, ε, c, δ, ∆f.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
X J 3maχ{log (dlf) , 4}，n J C ,r J √χ⅞,gthres J 等,fhres J X√p "加6 J C2χ√ρε
tnoise J———tthres ——1
for t = 0, 1, . . . do
if "▽/(Xt) || ≤ gthres and t ——tnoise > tthres then
Xt J- xt,tnoise J- t
Xt J Xt + ξt, where ξt is drawn uniformly from B°(r).
end if
if t — tnoise = tthres and f (xt)——f 侬小) > ——fthres then
return xtnoise
end if
χt+ι J Xt 一 ηVf(Xt)
end for
C Gradient and Hessian of the Cost Function
Before we prove any of our main theorems, we first compute the gradient and Hessian of the func-
tions f (W) and g(W). In our training process, we need to compute the gradient of function g(W),
and in the analysis for the smoothness and Hessian Lipschitz constants, we need both the gradient
and Hessian.
Recall that given the samples and their corresponding labels {(Xj, yj)}j≤n, we define the cost func-
tion of the neural network with parameters W = [w1, . . ., wr] ∈ Rd×r,
f (W) = 4n X(X ai(wTXj)2 - yj!.
j=1 i=1
Given the above form of the cost function, we can write out the gradient and the hessian with respect
to W . We have the following gradient,
dffWW) =41n X2 (Xai(wTXj)2 ——yj) ∙ 2ak(WkXj)Xj
nr
=？ X (X &i(wT Xj )2 —— yj) Xj XT Wk.
and J*
∂wk1 ∂wk2
nr
等 X(X ai(wTXj)2
——yj) XjXT + 2ak1 ak2 X(xτWkI)(XTWk2)XjXT, if kι = k2
n j=1
2ak1 ak2 X(xTWkI)(XTWk2 )XjXT, if kι = k2
n	j=1
In the above computation, dfWW) is a column vector and ∂Wf∂W) is a square matrix whose differ-
ent rows means the derivative to elements in Wk2 and different columns represent the derivative to
elements in Wk1 . Then, given the above formula, we can write out the quadratic form of the hessian
with respect to the parameters Z = [z1, z2, . . . , zr] ∈ Rd×r,
V2f(W)(Z, Z)
r	nr
=X ZT (詈 X (X ai(WTχj)2 —— yjj XjxT 卜
16
Under review as a conference paper at ICLR 2020
+ x	wτ2 (2aknak2 X(XTWkI)(XTwk?)XjXT) wkι
1≤k1,k2 ≤r	j =1
X zkτ (a X (X ai(WTXj)2 - yj! XjXTjzk+ 2 X (X aiwτXjXTa)
In order to train this neural network in polynomial time, we need to add a small regularizer to the
original ocst function f(W). Let
g(w ) = f(w) + 2 I∣W IlF,
where γ is a constant. Then we can directly get the gradient and the hessian of g(W) from those of
f(W). We have
Vwk g(W)
VW g(w )(Z, Z)
2 - yj	xj xjT wk + γwk
ai(wiTxj )2 - yj	xj xjT	zk
+ 2 X(X aiwT Xj XT Zi) + Y I|Z||F .
For simplicity, we can use xjTWAWTxj - yj to denote (Pir=1 ai (wiT xj)2 - yj, where A is a
diagonal matrix with Aii = ai . Then we have
1n
VWg(W) =— E (XTWAWtXj - yj) XjXTWA + YW
n
j=1
2
+---
n
r
V2Wg(W)(Z,Z)=XzkT
k=1
zk
D	Omitted Proofs for Section 3
In this section, we will give a formal proof of Theorem 4. We will follow the proof sketch in Section
3. First in Section D.1 we prove Lemma 1 which gives the optimization landscape for the two-layer
neural network with large enough width;then in Section D.2 we will show that the training process
on the function with regularization will end in polynomial time.
D.1 Optimization landscape of two-layer neural net
In this part we will prove the optimization landscape(Lemma 1) of 2-layer neural network. First we
recall Lemma 1.
Lemma 1 (Optimization Landscape). Given training data {(X1, y1), (X2, y2), ..., (Xn, yn)}, Sup-
pose the matrix X = [x?2,...,x之2] ∈ Rd2 ×n has full column rank and the smallest singular
value is at least σ. Also suppose that the number of hidden neurons satisfies r ≥ 2d + 2. Then if
λminV2f (W) ≥ —ε,thefUnction value is bounded by f (W) ≤ Tnd2^∙
For simplicity, we will use δj(W) = Pir=1 ai(wiT Xj)2 - yj to denote the error of the output of the
neural network and the label yj. Consider the matrix M = 1 P；=i δj χj XT. To ShoW that every
17
Under review as a conference paper at ICLR 2020
ε-second-order stationary point W of f will have small function value f(W), we need the following
2 lemmas.
Generally speaking, the first lemma shows that, when the network is large enough, any point with
almost Semi-definite Hessian will lead to a small spectral norm of matrix M .
Lemma 3. When the number of the hidden neurons r ≥ 2d + 2, we have
λminV2f (W) = - max ∣λi(M)|,
i
where λminV2f(W) denotes the smallest eigenvalue of the matrix V2f(W) and λi (M) denotes the
i-th eigenvalue of the matrix M.
Proof. First note that the equation
λminV2f(W) = - max ∣λi(M)|
i
is equivalent to
min V2f(W)(Z, Z)
||Z||F=1
and we will give a proof of the equivalent form.
First, we show that
max |zTMz|,
||z||2=1
max |zTMz|.
||z||2=1
—
||Zm||iFn=1V2f(W)(Z,Z)
≥
—
Intuitively, this is because V2f(W) is the sum of two terms, one of them is always positive semidef-
inite, and the other term is equivalent to a weighted combination of the matrix M applied to different
columns of Z .
V2f(W)(Z, Z)
=X zT (a X (X ai(wT Xj )2 - yj! Xj XTjzk+n X (X aiwT XjXT zi)
rn
X ak ZTMzk + 2 X
k=1
r
≥	akzkT Mzk
k=1
r
j=1
Xr aiwiTxjxjTzi!2
≥ - EmaX |%(M)HIzk ||2
i
k=1
=—max ∣λi(M)| ∙ ∣∣Z∣∣F.
i
Then we have
min V2f(W)(Z,Z) ≥ min (— max ∣λi(M)∣∙∣∣Z∣∣F) = — max ∣λiM∣
||Z||F=1	||Z||F=1 i	i
max |zTMz |.
||z||2=1
—
For the other side, we show that
min V2f(W)(Z,Z) ≤ — max |zTMz |
||Z||F=1	||z||2=1
by showing that there exists Z, ||Z||F = 1 such that V2f(W)(Z, Z) = — max||z||2=1 |zTMz|.
First, let z0 = arg max||z||2=1 |zT M z|. Recall that for simplicity, we assume that r is an even
number and a® = 1 for all i ≤ 2 and a® = —1 for all i ≥ r++2. If zTMz0 < 0, there exists U ∈ Rr
such that
1. ||u||2 = 1,
18
Under review as a conference paper at ICLR 2020
2.	Ui = 0 for all i ≥ r++2,
3.	Pi=1 aiuiwi = 0,
since for constraints 2 and 3, they form a homogeneous linear system, and constraint 2 has r equa-
tions and constraint 3 has d equations. The total number of the variables is r and we have r > 2 + d
since we assume that r ≥ 2d + 2. Then there must exists r 6= 0 that satisfies constraints 2 and 3.
Then we normalize that u to have norm ||u||2 = 1.
Then, let Z = zoUT, We have ∣∣Z∣∣F = ∣∣zo∣∣2 ∙ ||u||2 = 1 and
r2
V2f (W )(Z, Z )=V a,k ZT Mzk + —
n
k=1
n
X
j=1
nr
aiUiwiTxj xjT z0
j=1	i=1
r2
au akukz0 MzO +---
n
k=1
=z0T M z0 + 2 XX (XX 0t Xj XT z0
j=1	i=1
- max |zTMz |,
||z||2=1
where the third equality comes from the fact that ∣∣u∣∣2 = P；=i U = 1, Ui = ° for all i > 2,
and Pir=1 aiUiwi = 0. The proof for the case when z0TMz0 > 0 is symmetric, except we use the
second half of the coordinates (where ai = -1).
□
The next step needs to connect the matrix M and the loss function. In particular, we will show that
if the spectral norm of M is small, the loss is also small.
Lemma 4. Suppose the matrix X = [xf2,..., X^2] ∈ Rd2×n hasfull column rank and the smallest
singular value is at least Q. Then if the Spectral norm of the matrix M = ɪ En=I δj Xj XT is upper
bounded by λ, the function value is bounded by
f(W) ≤
ndλ2
4σ2 .
Proof. We know that the function value f (W) = n Pn=I δj = ɪ ∣∣δ∣∣2, where δ ∈ Rn is the vector
whose j-th element is δj. Because X = [x?2,..., X^2] ∈ Rd2×n has full column rank and the
smallest singular value is at least σ, we know that for any v ∈ Rn,
∣∣Xv∣∣2 ≥ Qmin (X) ∙ ∣∣V∣∣2 ≥ Q∣∣V∣∣2∙
Since M = n Pn=I δjXjXT is a symmetric matrix, M has d real eigenvalues, and we use
λ1 , . . . , λd to denote these eigenvalues. Because we assume that the spectral norm of the matrix
M = n pn=ι δj Xj XT is upper bounded by λ, which means that ∣λi | ≤ λ for all 1 ≤ i ≤ d, and we
have
dd
||M||2F=X λi2 ≤X λ2 = dλ2.
Then we can conclude that
||M||2F
j=1
≥ -2σ2llδll2,
n2
where the second equation comes from the fact that reordering a matrix to a vector preserves the
Frobenius norm.
19
Under review as a conference paper at ICLR 2020
Then combining the previous argument, we have
f(W) = ∖∣δ∣∣2 ≤ -n2IIMIlF ≤ ndλ22.
4n	4σ2	4σ2
□
Lemma 1 follows immediately from Lemma 3 and Lemma 4.
D.2 Training guarantee of the two-layer neural net
Recall that in order to derive the time complexity for the training procedure, we add a regularizer to
the function f . More concretely,
g(w ) = f(w) + 2 I∣W IlF,
where γ is a constant that we choose in Theorem 4.
To analyze the running time of the PGD algorithm, we first bound the smoothness and Hessian
Lipschitz parameters when the Frobenius norm of W is bounded.
Lemma 5. In the set {W : ||W ||2F ≤ Γ}, if we have ||xj ||2 ≤ B and |yj | ≤ Y for all j ≤ n, then
1.	Vg(W) is (3B4Γ + YB2 + γ)-smooth.
2.	V2g(W) has 6B4Γ2 -LiPschitz Hessian.
Proof. We first figure out the smoothness constant. We have
IIVg(U) - Vg(V)IIF
1 n	1 n
=∣∣	^X	(XT U AU T Xj	—	yj)	Xj χT U A	+ YU^X (XT VAVT Xj	—	y7∙)	Xj XT VA — YV ∣∣f
nj =1	nj=1
nn
≤∣∣ 一 ^X (XT U AU t Xj — yj) Xj XT UA-^X (XT VAV t Xj- yj) Xj XT VA∣∣f + γ∣∣U — V ∣∣f .
n j=1	n j=1
Then we bound the first term, we have
1n	1n
Il X (XT UAUtXj — yj) Xj XT UA-------X (xTVAVtXj — yj) XjxJVA ∣∣ f
n j=1	n j=1
nn
=II 一 ^X (XT U AUT Xj — yj) Xj XT U A-^X (XT U AUT Xj — yj) Xj XT VA
n j=1	n j=1
nn
+— ^X (XT U AUT Xj — yj) Xj XT VA-^X (XT U AVT Xj — yj) Xj XT VA
n j=1	n j=1
1n	1n
+— ^X (XTUAVTXj — yj) XjXTVA------^X (XTVAVTXj — yj) XjXTVAIIf
n j=1	n j=1
1 n	1 n
≤II ^X (XT U AUT Xj — yj) Xj χjT U A^X (XT U AUT Xj — yj) Xj XT VAIIf
n j=1	n j=1
1	n	1 n
+	II ^X (XT U AUT Xj — yj) Xj XT VA^X (XT U AVT Xj — yj) Xj XT VAIIf
n j=1	n j=1
nn
+	II一 ^X (XTUAVTXj — yj) XjXTVA-^X (XTVAVTXj — yj) XjXTVAIIf.
n j=1	n j=1
20
Under review as a conference paper at ICLR 2020
The first term can be bounded by
1 n
||— ɪ2 (XT U AUT Xj — yj) Xj xJ U A —
1 n
一 ɪs (XTUAUTXj-%)XjxjTVA||f
j=ι
1 n
—ɪ2 (XTUAUTXj) XjxJVA||f
n j=i
1 n
+ ||- EyjXjxtUA - yjXjxtVA||f
n j=i
1 n
≤H- E (xJUAUtXj) XjxJ||f||(U - V)A||f + YB21|(U - V)A||f
j=i
≤B4Γ"U - V||f + YB2||U - V||f.
Similarly, we can show that
In	In
||— ^X (XTUAUtXj - yj) XjXTVA----^X (XTUAVTXj- yj) XjXTVA||f ≤ B4r||U-V||f,
j=ι
and
j=ι
1 二「	「	、	1 二—「	、	............
||— ^X (XTUAVtXj - yj) XjxJVA— ^X (XTVAVTXj- yj) XjXTVA||f ≤ B4r||U-V||f.
j=i
Then, we have
j=ι
||Vg(U )-Vg(V )||尸 ≤ (SB 4Γ + YB2 + γ)∣∣U - V ||F.
Then we bound the Hessian Lipschitz constant. We have
|V2g(U)(Z, Z)-V2g(V)(Z, Z)|
∖ j=i
rn
Zk + 2 X(X aiuτXjXTZi)	+ Y||Z||F
2 X(X αivτ Xj XT j	-Y||Z||F |
r
n
k = 1
j = 1
k=1
j=ι
1	3	-	E	E	E	E '	„
≤ — £ || (XT(UAUt - UAVt + UAVT - VAVT)叼)XjXT||尸|同||2
n j=ι
≤2B4Γ2||U - V||f|厮||2,
21
Under review as a conference paper at ICLR 2020
So we can bound the first term by
rn
X IzT (T X (XT(UAU T - VAV T)Xj) XjXTI zk |
k=1	j=1
r
≤X2B4Γ2IIU - V||fIIzkII2 = 2B4Γ2IIU - V∣∣F∣∣ζ∣∣F.
k=1
Then for the second term, note that
r
aiuiTXjXjTzi = hUA, XjXjTZi,
i=1
and we have
2	Xl (X	T T	X T T
—I 工 aiUi XjXj zi -	2_v aiVi XjXj zi	I
j=1	i=1	i=1
2n
=—ɪ2 IhUA, Xjx/Z〉2 -hVA, XjXjTZi21
n j=1
2n
=—£ Ih(U - V)A，XjXTZih(U + V)A, XjXTZiI
n j=1
2n
≤- EII(U - V)AIIFIIXjXTZIIfII(u + V)aIIfIIXjXTzIIf
n j=1
≤4B4Γ2IIU - VIIfIIZIIF,
where the first inequality comes from the Cauchy-Schwatz inequality. Combining with the previous
computation, we have
IV2g(U)(Z,Z) -V2g(V)(Z,Z)I ≤ 6B4Γ 1IIU - VIIfIIZIIF.
□
We also have the theorem showing the convergence result of Perturbed Gradient Descent(Algorithm
1).
Theorem 3 (Convergence of PGD (Jin et al.(2017a))). Assume that f (∙) is '-smooth andP-Hessian
Lipschitz. Then there exists an absolute constant C语 such that, for any δ > 0,ε ≤ 'ρ, ∆f ≥
f (xo) 一 f *, and constant C ≤ Cmx PGD(X0,', ρ, ε,c,δ, ∆f) will output an ε-second-order sta-
tionary point with probability 1 - δ, and terminate in the following number of iterations:
竿)).
Then based on the convergence result in Jin et al. (2017a) and the previous lemmas, we have the
following main theorem for 2-layer neural network with quadratic activation.
Theorem 4 (Main theorem for 2-layer NN). Suppose the matrix X = [x?2,..., X之2] ∈ Rd2 ×n has
full column rank and the smallest singular value is at least σ. Also assume that we have IIXj II2 ≤ B
and Iyj I ≤ Y for all j ≤ n. We choose our width of neural network r ≥ 2d + 2 and we choose
ρ = (6B4p2(f(0) + 1)) (nd∕(σ2ε))1/4, Y = (σ2ε∕nd) 1/2, and ' = max{(3B42(f(Y)+1) +
Y B2 + γ), 1}. Then there exists an absolute constant Cmax such that, for any δ > 0, ∆ ≥ f(0) + 1,
and constant C ≤ Cmax, P GD(0, `, P, ε, C, δ, ∆) on W will output an parameter W * such that with
probability 1 - δ, f(W*) ≤ ε when the algorithm terminates in the following number of iterations:
O
B 8'(nd)5/2(f (0) + 1)2
log4
Bnrd'∆(f (0) + 1)
ε2δσ
σ5ε5/2
22
Under review as a conference paper at ICLR 2020
Proof of Theorem 4. We first show that during the training process, if the constant c ≤ 1, the objec-
tive function value satisfies
3cε2
g(Wt) ≤ g(Wms) + ^4,
where we choose the smoothness constant ` ≥ 1 to be the smoothness for the region g(W) ≤
g(Wins) + 3c!r.
In the PGD algorithm (Algorithm 1), we say a point is in a perturbation phase, if t - tnoise <
tthres . A point xt is the beginning of a perturbation phase if it reaches line 5 of Algorithm 1 and a
perturbation is added to it.
We use induction to show that the following properties hold.
1.	If time t is not in the perturbation phase, then g(Wt) ≤ g(Wins).
2.	If time t is in a perturbation phase, then g(Wt) ≤ g(W⅛s) + ∣∣4'. Moreover, if t is the
beginning of a perturbation phase, then g(Wt) ≤ g(Wins).
First we show that at time t = 0, the property holds. If t = 0 is not the beginning of a perturbation
phase, then the inequality holds trivially by initialization. If t = 0 is the beginning of a perturbation
phase, then we know that g(W0) = g(Wins) from the definition of the algorithm, then
(1)
≤g(WD) + llξ0llF Rg(WD)IIF + 2 |Vg(W0 )llF
≤g(W0) + r ∙ gthres + 2r2
<g(Wo) + √cε ∙√cε +' √cε .√cε
≤g(W0)+ χ2`	X +2 X' X'
3cε2
=g(Wins) + 2χ电
Now we do the induction: assuming the two properties hold for time t, we will show that they also
hold at time t + 1. We break the proof into 3 cases:
Case 1: t + 1 is not in a perturbation phase. In this case, the algorithm does not add a perturbation
on Wt+1, and we have
g(Wt+ι )=g(Wt- ηyg(Wt))	(2)
`
<	g(Wt) - hηVg(Wt), Vg(Wt)i + 2IInVg(Wt)IIF
<	g(Wt) - 2IIVg(Wt)IIFII
<	g(Wt).
If t is not in a perturbation phase, then from the induction hypothesis, we have
g(Wt+1) < g(Wt) < g(Wins),
otherwise if t is in a perturbation phase, since t + 1 is not in a perturbation phase, t must be at the
end of the phase. By design of the algorithm we have:
g(Wt+l) < g(Wt) < g(Wtnoise) - f⅛res < g(Wins).
Case 2:	t + 1 is in a perturbation phase, but not at the beginning. Using the same reasoning as
equation 2, we know
g(Wt+1) < g(Wt) < g(Wins).
23
Under review as a conference paper at ICLR 2020
Case 3:	t + 1 is at the beginning of a perturbation phase. First we know that
g(Wt) ≤ g(Wins),
since t is either not in a perturbation phase of at the end of a perturbation phase, then we have
g(Wt+1) ≤ g(Wins). Same as the computation in equation 1, we have
3cε2
g(Wt+l) ≤ g(Wins) + κ .
2χ4'
This finishes the induction.
Since we choose ' ≥ 1, we can choose the other parameters such that g(Wt+ι) ≤ g(W⅛s) + 32χ2 ≤
g(Wins) + 1. Then since
g(w ) = f (W) + 2 l∣w IlF,
we know that during the training process, we have ∣∣W||F ≤ 2(g(Wins)+1). Since we train from
Wins = 0, we have ∣∣W||F ≤ 2f (；)+1). From Lemma 5, we know that
1.	Vg(W) is (3B42f (Y)+1) + YB2 + γ)-smooth.
2.	V2g(W) has 6B4^z∕2f(；)+1) -Lipschitz Hessian.
As we choose Y = (6B4√2(f (0) + 1))2/5 ∙ε2∕5, WeknOWthat P = (6B4√2(f (0) + 1))4/5 ∙ε-1/5
is an upper bound on the Lipschitz Hessian constant.
When PGD stops, we know that
λmin(V2g(W)) ≥ -√ρε = -(6B4√2(f (0) + 1))2/5 ∙ ε2/5,
and we have
λmin(V2f (W)) ≥ λmin(V2g(W))- Y ≥ -2(6B4√2(f (0) + 1))2/5 ∙ ε2/5.
From Lemma 3, We know that the spectral norm of matrix M is bounded by
2(6B4 √2(f (0) + 1))2∕5 ∙ ε2/5, and from Lemma 4, we know that
f(W) ≤
nd ∙ 4(6B4√2(f (0) + 1))4/5 ∙ ε4/5 _ nd ∙ (6B4√2(f (0) + 1))4/5 ∙ ε4/5
4σ2
σ2
The running time follows directly from the convergence theorem of Perturbed Gradient De-
scent(Theorem 3) and the previous argument that the training trajectory will not escape from the
set {W : ∣∣W∣∣F ≤ 2(g(WnS)+1)}.
Then, in order to get the error to be smaller than ε, we choose
ε0
σ2ε	5/4
nd	6B4 √2(f (0) + 1),
1
and the total running time should be
O
B 8'(nd)5/2(f (0) + 1)2
Bnrd'∆(f (0) + 1)
σ5ε5/2
ε2δσ
Besides, our parameter ρ and γ is chosen to be
1
P = (6B4√2(f (0) + 1))4/5 ∙ ε0T∕5 = (6B4√2(f (0) + 1))(段)，
and
Y = (6B4√2(f (0) + 1))2/5 ∙ ε02/5 =(夸丫
nd
□
24
Under review as a conference paper at ICLR 2020
E Omitted Proofs in Section 4
In this section, we give the proof of the main results of our three-layer neural network(Theorem 5
and 6). Our proof mostly uses leave-one-out distance to bound the smallest singular value of the
relevant matrices, which is a common approach in random matrix theory (e.g., in ). However, the
matrices we are interested in involves high order tensor powers that have many correlated entries, so
we need to rely on tools such as anti-concentration for polynomials in order to bound the leave-one-
out distance.
First in Section E.1, we introduce some more notations and definitions, and present some well-
known results that will help us present the proofs. In Section E.2, we proof Theorem 5 which focus
on the smoothed analysis setting. Finally in Section E.3 we prove Theorem 6 where we can give a
deterministic condition for the input.
E.1 Preliminaries
Representations of symmetric tensors Throughout this section, we use Tdp to denote the space
of p-th order tensors on d dimensions. That is, Td = (Rd)0p. A tensor T ∈ Td is symmetric if
T(i1, i2, ..., ip) = T (iπ(1) , iπ(2), ..., iπ(p) for every permutation π from [p] → [p]. We use Xdp to
denote the space of all symmetric tensors in Tdd. The dimension of Xdd is Ddd = d+dd-1 .
Let Xd = {x ∈ Xplkxk2 = l} be the Set of Unit tensors in Xd (as a sub-metric space of Td). For
Rd, let {ei∣i = 1,2 .…d} be its standard orthonormal basis. For simplicity of notation We use SP to
denote the group of bijections (permutations) [p] → [p], and Idd to denote the set of integer indices
d
Ip = {(i1,i2 …id) ∈ Nd| P ij = p}. We can make Xp isomorphic (as a vector space over R) to
j=1
Euclidean spaceRIp (∖Id| = D^bychoosingabasis 抬包於…id)∈ιp =	P ejb(i)%ej〃⑵ %
d Q ij ! σ∈Sp
j=1
…% ejσ(p) ∣(jι ◦ j2 ◦•••◦ jd) = (1(i1) ◦ 2(i2) ◦•••◦ d(id))} where (1(i1) ◦ 2(i2) ◦•••◦ d(id)) means
a length p string With i1 1’s, i2 2’s and so on, and let the isomorphism be φdd. We call the image of
a symmetric tensor through φdd its reduced vectorized form, and we can define a new norm on Xdd
with kxkrv = kφdd(x)k2.
Given the definition of reduced vectorized form and the norm ∣∣ ∙ ∣∣rv, we have the following lemma
that bridges between the norm ∣∣ ∙扃 and the original 2-norm.
Lemma 6. For any x ∈ Xnd,
kxk" ≥√1p! kxk2.
Proof. We can expand x as x = P xisi .Then ∣x∣rv =	P xi2	and ∣x∣2 = P	xi2 ∣si ∣22	as
i∈Inp	i∈Inp	i∈Inp
{si} are orthogonal. Notice that for i = (i1,i2 …in), ∣∣si∣∣2 = nd	≤ p!, and therefore
Q ij!
j=1
∣x∣2 ≤
JX x2P! = √p!kxkrv.
i∈Inp
□
ε-net Part of our proof uses ε-nets to do a covering argument. Here we give its definition.
Definition 3 (ε-Net). Given a metric space (X, d). A finite set N ⊆ P is called an ε-net for P ⊂ X
iffor every x ∈ P, there exists π(x) ∈ N such that d(x, π(x)) ≤ ε. The smallest cardinality ofan
ε-net for P is called the covering number: N(P, ε) = inf {|N | : N is an ε-net ofP}.
Then we introduce give an upper bound on the size of ε-net of a set K ⊆ Rd . First, we need the
definition of Minkowski sum
25
Under review as a conference paper at ICLR 2020
Definition 4 (Minkowski sum). Let A, B ⊆ Rd be 2 subsets of Rd, then the Minkowski sum is
defined as
A + B := {a + b : a ∈ A, b ∈ B}.
Then the covering number can be bounded by a volume argument. This is well-known, and the proof
can be found in Vershynin (2018)(Proposition 4.2.12 in Vershynin (2018)).
Proposition 1 (Covering number). Given a set K ⊆ Rd and the corresponding metric d(x, y) :=
kx - yk2. Suppose that ε > 0, and then we have
N(K,ε) ≤
|K + Bd(ε∕2)∣
∣Bd(ε∕2)∣
where ∣ ∙ ∣ denote the volume of the set.
Then with the help of the previous proposition, we can now bound the covering number of symmetric
tensors with unit length.
Lemma 7 (Covering number of Xp). There exists an ε-net of Xd with size O
i.e.
1+
N(Xp,ε) ≤ O
Proof. Recall that φd(∙) : Rdp → RDd is an bijection between the symmetric tensors in Rdp and
a vector in RDp. We first show that an √p -net for the image φd(Xp) implies an ε-net for the unit
symmetric tensor Xp
Suppose that the 品-net for the image φd(Xp) is denoted as N ⊂ φ1(Xp), and for any X ∈
φp(JXp), there exists ∏(χ) ∈ N such that ∣∣∏(χ) - χ∣∣2 ≤ *. Then We know that (Φd厂1 (N) is
p!
an ε-net for the unit symmetric tensors Xp because for any x0 ∈ Xp we have
kx0 - (φd)-1 (π(φp(x0)))k2 ≤Pp!kφd(χ0) - π(φd(x'))k2
≤pp! '
ε,
where the first inequality comes from Lemma 6.
Next, we bound the covering number for the set φd(Xp). First note that the set satisfies φd(Xp) ⊂
RDdp, and from Proposition 1, we have
ε	ε、	Φd (Xp ) + BDp (2√!)
N (φP(Xd), √) ≤ -D---------------------
'	ppj	BDd ( 2√!)
BDp (1)+ BDp (帚)
≤--------------------
BDp (帚)
where the first inequality comes from Proposition 1 and the second inequality comes from the fact
that ∣∣φd(χ)∣∣2 ≤ ∣∣χ∣∣2.	□
26
Under review as a conference paper at ICLR 2020
Leave-one-out Distance Another main ingredient in our proof is Leave-one-out distance. This is
a notion that is closely related to the smallest singular value, but usually much easier to compute
and bound. It has been widely used in random matrix theory, for example in Rudelson & Vershynin
(2009).
Definition 5 (Leave-one-out distance). For a set ofvectors V = {v1,v2 •…vn} ,their leave-one-out
distance is defined as
l(V) = min inf	kvi -	ajvjk2.
1≤i≤n a1,a2∙∙∙an∈R	(
j6=i
For a matrix M, its leave-one-out distance l(M) is the leave-one-out distance of its columns.
The leave-one-out distance is connected with the smallest singular value by the following lemma:
Lemma 8 (Leave-one-out distance and smallest singular value). For a matrix M ∈ Rm×n with
m ≥ n, let l(M) denote the leave-one-out distance for the columns ofM, and σmin(M) denote the
smallest singular value ofM, then
l(M)
√n
≤ σmin (M) ≤ l(M).
We give the proof for completeness.
Proof. For any x ∈ Rn\{0}, let r(x) = argmax |xi|, then |xr(x)| > 0 for x 6= 0.
i∈[n]
Because l(M) = min inf	kMxk2, We have
i∈[n] x∈Rn,xi=1
σmin(M )=χ∈inf ∖o kMXk2
kMX k2
k xχi ∣∣2
kMx0k2
kx0k2 .
min inf
i∈[n] x∈ Rn∖0,r (x)=i
min inf
i∈[n] x0∈Rn∖0,χi = 1
Because of the equations kx0 k2 ≥ |x0i| = 1 and kx0k2 = P xj ≤ √n∣χi∣ = √n, We have
j∈[n]
l(M ≤ σmin(M) ≤ l(M).
□
Anti-concentration To make use of the random Gaussian noise added in the smoothed analysis
setting, We rely on the folloWing anti-concentration result by Carbery & Wright (2001):
Proposition 2 (Anti-concentration (Carbery & Wright (2001))). For a multivariate polynomial
f (x) = f (x1,x2 …Xn) of degree P, let X 〜N(0,1)n follows the Standard normal distribution,
and Var[f] ≥ 1, then for any t ∈ R and ε > 0,
Pr[∣f (x) - t| ≤ ε] ≤ O(p)ε1∕p	(3)
x
Gaussian moments To apply the anti-concentration result, We need to give loWer bound of the
variance of a polynomial When the variables folloW standard Gaussian distribution N(0, 1). Next,
We Will shoW some definitions, propositions, and lemmas that Will help us to give loWer bound for
variance of polynomials.
Proposition 3 (Gaussian moments). if X 〜N(0,1) is a Gaussian variable, then for P ∈ N,
Ex [x2p] = jPp⅛ ≤ 2pp!; Eχ[x2p+1] = 0∙
Definition 6 (Hermite polynomials). In this paper, we use the normalized Hermite polynomials,
which are univariate polynomials which form an orthogonal polynomial basis under the normal
distribution. Specifically, they are defined by the following equality
Hn (X) = ⅛ (H!
27
Under review as a conference paper at ICLR 2020
The Hermite polynomials in the above definition forms a set of orthonormal basis of polynomials in
ni
the standard Normal distribution. For a polynomial f : Rn → R, let f(X) = P fiM Q Xjj and
i∈In≤p	j=1
n
f(X) = P fiH Q Hij (Xj) be its expansions in the basis of monomials and Hermite polynomials
i∈In≤p	j=1
p
respectively (Hk is the Hermite polynomial of order k). Let the index set In≤p =	Inj . We have
j=0
the following propositions. The propositions are well-known and easy to prove. We include the
proofs here for completeness.
Proposition 4. for i ∈ Inp, fiM
fiH
Proof. Consider i = (iι,i2 •…in) ∈ In in the monomial expansion, the coefficient for the mono-
ni
mial Mi = Q Xjj is fiM. In the Hermite expansion, since Hn(X) is an order-n polynomial, if the
j=1
n
term Q Hi0 (Xj) contain the monomial Mi, there must be i0j ≥ ij, and therefore for i ∈ Inp the only
j=1	j
n
term in the Hermite expansion that contains Mi is fiH Q Hij (Xj) (with Mi as its highest order
j=1
monomial). The coefficient for Xjj in Hij (Xj) is -ɪ=, and therefore fiM
ij!
fH	口
Proposition 5. For X ~ N(0,1)n, Eχ[f] = fH, Eχ[f2] = P (fH)2 (0n refers to the index
i∈In≤p
(0,0, o …0)∈ In).
Proof. Firstly, let W(X) = √= e-x2/2 be the PDF of N(0,1), then
∞
Z Hn(X)w(X)dX
-∞
dX
n -x2 /2
as a result of ⅛—→0 when X →±∞for n ≥0.BeSides,
∞
Hn(X)Hm(X)w(X)dX = δnm
-∞
for its well-known orthogonality in Guassian distribution (with δnm = I[n = m] as the Kronecker
function). Therefore,
∞
Ex [f] = X fiH Y	Hij (Xj )w(Xj)dXj
i∈In≤p	j∈[n]-∞
= X fiH Y I[ij =0]
i∈In≤p	j∈[n]
= fHn ,
∞
Ex [f2] =	X fiH fiH0 Y	Hij (Xj)Hi0j (Xj)w(Xj )dXj
i,i0∈In≤p	j∈[n]-∞
28
Under review as a conference paper at ICLR 2020
=	fiHfiH0	I[ij =i0j]
i,i0∈In≤p	j∈[n]
= X (fiH)2.
i∈In≤p
□
Then, we have the following lemma that lower bounds the variance of a polynomial with some
structure. Given the following lemma, we can apply the anti-concentration results in the proof of
Theorem 5 and 6.
Lemma 9 (Variance). Let f (x) = f (x1,x2 .…Xd) be a homogeneous multivariate polynomial of
degree P, then there is a Symmetric tensor M ∈ Xd that f (x) =(M, x0p). For all xo ∈ Rd, when
x 〜N(0,1)d,
Varx [f (x0 + x)] ≥ kM kr2v
Proof. We can view f(x0 + x) as a polynomial with respect to x and let fiM and fiH be the co-
efficients of its expansion in the monomial basis and Hermite polynomial basis respectively (with
variable x). It’s clear to see that (fiM |i ∈ Inp) is the reduced vectorized form of M. From the
Proposition 4 and 5, we have
Var[f (x0 + x)] =E[f (x0 + x)2] - E[f (x0 + x)]2
= X (fiH)2
i∈I≤p∖0n
≥X(fiH)2≥X(fiM)2
i∈Inp	i∈Inp
=kMkr2v.
□
We also need a variance bound for two sets of random variables
Lemma 10. Let f (x) = f (x1,x2 .…Xd) be a homogeneous multivariate polynomial of degree 2p,
then there is a symmetric tensor M ∈ Xn that f(x) = (M, x02p〉. For all uo,vo ∈ Rd, when
u, v 〜 N(0, Id), we have
Varu,v[(M, (uo + U产③(v0 + V产)]≥ ɪIIMk2v
(2p)!	rv
Proof. The proof is similar to Lemma 9. We can view hM,(uo + U产 0 (v0 + v)0pi as a degree-2p
polynomial g over 2d variables (u, v). Therefore by Lemma 9 the variance would be at least the
rv-norm of g. Note that every element (monomial in the expansion) in M corresponds to at least one
element in g, and the ratio of coefficient in the correspnding rv-basis is bounded by (2p)!, therefore
IlgIlrv ≥ (2p)!∣∣MIlrv, and the lemma follows from Lemma 9.	□
E.2 Proof of Theorem 5
In this section, we give the formal proof of Theorem 5. First recall the setting of Theorem 5: we add
a small independent Gaussian perturbation X 〜N(0, v)d on each sample x, and denote X = X + X.
The output of the first layer is {zj} where Zj (i) = (r>Xj)p.
Our goal is to prove that {zj}’s satisfy the conditions required by Theorem 4, in particular, the
matrix Z = [zf2,；., z，2] has full column rank and a bound on smallest singular value. To do that,
note that if We let X = [Xιf 2p, X2f 2p •…Xnf 2p] be the order-2p perturbed data matrix, and Q be a
matrix whose i-th row is equal to rfp, then We can write Z = (Q 0 Q)X.
We first show an auxiliary lemma which helps us to bound the smallest singular value of the output
matrix (Q 0 Q)X, and then we present our proof for Lemma 2.
29
Under review as a conference paper at ICLR 2020
Generally speaking, the proof of Lemma 2 consists of the lower bound of the Leave-one-out distance
by the anti-concentration property of polynomials and the use of Lemma 8 to bridge the Leave-one-
out distance and the smallest singular value.
Lemma 11. Let M be a k-dimensional subspace of the symmetric subspace of Xdp, and let ProjM
be the projection into M. For any X ∈ Rd with PertUbation X 〜N(0, v)d, X = X + x,
Pr {k ProjM X叫∣2 <
1/4 p
V2ε < O(p)ε1∕p.
Proof. Let mi, m2 …mk ∈ Xd be a set of orthonormal (in Td as a Euclidean space) ba-
sis that spans M, and each mi is symmetric.
Then IlProjMX叫∣2
JP hm-i2.
i=1
Let
kk
g(X)= P hmi, X0pi2 = h P mf2, X02pi, then g(X)is a homogeneous polynomial with order
i=1	i=1
2p. For any initial value x, if X = X + X, then √1v X= √1vX + √1vX is a vector where the random
part √1vX 〜N(0,1)n. Therefore by Lemma 9
Varx [g (√Vx)] ≥ k Xm"k2v ≥ (2p)!k Xmi32k2 = (2p)!.
Hence from Proposition 2 We know that, when X 〜N(0, VI),
Pr I kProjMXe)dk2 <
k
(2p)!
< ε2} ≤ O(p)ε1∕p.
□
Lemma 12. Let M be a k-dimensional subspace of the symmetric subspace ofXd2d, and let ProjM
be the projection into M. For any x, y ∈ Rd with pertubation X, y 〜N(0, v)d, X = X + X, and
y = y + y, there is
Pr {kPrOjM(Xop ③y0p)k2 < (((4p)!)2)	Vpε} <O(P)ε1∕2p.
Proof. The proof here is similar to that of Lemma 11. Let mι,m2 •一mk ∈ Xdp be a set of
orthonormal (in Td2p as a Euclidean space) basis that spans M, and each mi is symmetric. Then

IlProjM(X0p 名 y刃k2
k
P〈mi, (X0p ③ yop)i
i=1
k
Let g(x,y) = P (mi,(xop 0 yop)i2
i=1
kk
P mio2,	(Xop	0	yop	0	Xop	0 yop)i	=h	P	mi(), (Xo2p	0	yo2p)i	for some tensor	mi(),then
i=1	i=1
g(X) is a homogeneous polynomial with order 4p. Notice that Imi(2) I2 = Imio2I2 by a change of
coordinate. For any initial value X and y, if X = X + X and y = y + y, then √1v X and 表 y are vectors
where the random part √1vX, √1vy 〜N(0,1)n. Therefore by Lemma 10,
Varx,
30
Under review as a conference paper at ICLR 2020
k
(O)2.
Hence from Proposition 2 We know that, when X 〜N(0, VI),
Pr {∣∣ProjMHp ③ y«p)k2 < (ɪɪY" vpε}
Pr {∣(√Pτ g(餐 √v )l<ε2}≤ O(P)i.
□
Then we can show Lemma 2 as follows.
Lemma 2. Suppose k ≤ Op(dp) and (k+1) > n, let Xj = Xj + Xj be the perturbed input in
the smoothed analysis setting, where Xj 〜 N(0, vI), let {zι, z2,…，zn} be the output of the first
layer on the perturbed input ((Zj ) = (rTXj )p). Let Z ∈ Rk ×n be the matrix whose j-th column
is equal to z；2, then with probability at least 1 一 δ, the smallest singular value of Z is at least
Ωp(vpδ4p∕n2p+"k4p).
Actually, we show a more formal version which also states the dependency on p.
Lemma 13 (Smallest singular value for (QxQ)X with PertUbation). With Q being the k Xdp matrix
defined as Q = [r；p, r；p …r；p]T (r 〜N(0, I)), with pertubed X = [Xi；2p, X2；2p …Xn；2p]
(Xi = Xi + Xi), and with Z =(Q × Q)X, when Xi is drawn from i.i.d. Gaussian Distribution
D2P
N(0, vI), for 2ʌ/n ≤ k ≤	：2p、= Op(dp), with overall probability ≥ 1 — O(ρδ), the smallest
Dd ( P )
singular value
σmin (Z) ≥
- kDd(2P)∏("1) -n] V/4
[(4p)!]3
vp δ4p
n2p+ι∕2k4p
(4)
Proof. First, we show that with high probability, the projection of rows of Q x Q in the space of
degree 2p symmetric polynomials (in this proof we abuse the notation ProjX2P (Q x Q) to denote
d
the matrix with rows being the projection of rows of Q x Q onto the space in question) has rank
k2 := k+2 1 , and moreover give a bound on σk2 (ProjX 2P (Q x Q)).
We do this by bounding the leave one out distance of the rows ofProjX2P(Q x Q), note that we only
d
consider rows (i, j) as ProjX2P (ri；p x rj；p) where 1 ≤ i ≤ j ≤ k (this is because the (i, j) and
(j, i)-th row of ProjX2P (Q x Q) are clearly equal).
d
The main difficulty here is that different rows of ProjX2P (Q x Q) can be correlated. We solve this
problem using a technique similar to Ma et al. (2016).
For any 1 ≤ i ≤ j ≤ k, fix the randomness for rl where l 6= i, j. Consider the subspace S(i,j) :=
span{ProjX2P (rl；p x X；p), X ∈ Rd, l 6= i, j}. The dimension of this subspace is bounded by
k ∙ Dp ∙ (2p) (as there are (2ρ) ways to place P copies of r and P copies of x). Note that any other
row ofProjX2P(Q x Q) must be in this subspace.
d
Now by Lemma 12, we know that the projection of row (i,j) onto the orthogonal subspace of S(i,j)
D2P -kDP (2P)	1/4
has norm ( d ((4p)!d2 P ) ε with probability O(p)e1∕2p. Thus by union bound on all the rows,
with probability at least 1 一 O(Pδ), the leave-one-out distance is at least
MP .小心 JD2 - kDd(2p)γ∕4/ δ Yp
I(PrOjXdP (Q ③Q)) ≥(	((4p)!)2	J (("ij	,
31
Under review as a conference paper at ICLR 2020
l
and by Lemma 8 the minimal absolute singular value σmin(Projχ2p (Q 0 Q)) ≥ -
d
x2p (Q0Q)
(k+1)
Next, let V (Q 0 Q) be the rowspace of ProjX2p (Q 0 Q), which as we just showed has dimension
d
k+2 1 . We wish to show that the projections of columns ofX in V (Q0Q) have a large leave-one-out
distance, and thus (Q 0 Q)X has a large minimal singular value.
Actually for each i, the subspace (which for simplicity will be denoted as V-i(Q0Q)) ofV (Q0Q)
orthogonal to span{Xj2p∣j = i} has dimension (k+1)-n+1 almost surely, and therefore by Lemma
11 and union bound, with probability 1 一 O(p)τ 1/2pn = 1 一 O(pδ), for all i,
∙⅛ !1/4 vpτ,
kPv-i(Q0Q)(x产p)k2 = E [kPv-i(Q0Q)(x产p)k2∣{Xj |j = i
thus with probability 1 一 O(pδ), for any vector C ∈ Rn with ∣∣ck2 = 1, let i = argmax/c/,
∣ci* I ≥ √n, and
k(Q 0 Q)X Ck2	≥ σmin(ProjX2p Q 0 Q)Ici* |kProjv (QeQ)X , ∣∣2
σmin (ProjX2p QeQ)
Xd
√n
llProjV-i* (QeQ)(Xe2p)∣2
[D2p-kDP(2P)][(k+1)-n]
KW
1/4	vPδ4P
n2P+1/2k4P
≥
≥
And therefore we will get Lemma 13.
□
A minor requirement ofon zj’s is that they all have bounded norm. This is much easier to prove:
Lemma 14 (Norm upper bound for Qxep). Suppose that ∣∣Xj∣∣2 ≤ B for all j ∈ [n] and Xj =
Xj + Xj where Xj 〜N(0, VI). Same as the previous notation, Q = [r,p,..., rfp]T ∈ Rk×dp
Then with probability at least 1 -
δ
√2π ln((k+n)dδ-1 /2)(k+n)d
, for all i ∈ [n], we have
IIQXepI∣2 ≤ √k(2(B + 2/vdln((k + n)dδ-r2))Jdln((k + n)dS-1/2))p
Proof. First We have, for a standard normal random variable N 〜N(0,1), We have
、/2	χ2
Pr{∣N∣ ≥ x} ≤ ɪe-ɪ.
πX
Then, apply the union bound, we have with probability at least 1 -
all l ∈ [k], i ∈ d, j ∈ [n], ` ∈ d, δ < 1, we have
δ
√2π ln((k+n)dδ-1/2 )(k+n)d
, for
∣(rι)i∣ ≤ 2√ln((k + n)dδ-1/2), |(Xj)`l ≤ 2 Jv ln((k + n)dS-1/2).
Then for all j ∈ [n], we have
∣∣x∣∣2 ≤ ∣∣x∣∣2 + ∣∣X∣∣2 ≤ B + 2yjvdln((k + n)dδ-1/2).
If for all i ∈ [d], l ∈ [k], |(rj-)i| < 2pln((k + n)dδ-1/2), then for any X such that ∣∣X∣∣ ≤ B +
2，vdln((k + n)dδ-1/2) and any l ∈ [k], we have
∣((rι)ep)TXep∣ =∣(rT X)p∣
≤(I∣rι∣∣∙∣∣X∣∣)p
≤ (2(B + 2 Jvdln((k + n)dδ-1∕2)) Jdln((k + n)dδ-1/2)).
32
Under review as a conference paper at ICLR 2020
Then we have
IIQx0pII2 ≤ √k(2(B + 2，vdln((k + n)dδT∕2)),dln((k + n)dδ-1/2)) .
□
Then combined with the previous lemmas which lower bound the smallest singular value(Lemma
13) and upper bound the norm(Lemma 14) of the outputs of the random feature layer and Theorem
4, we have the following Theorem 5.
Theorem 5 (Main theorem for 3-layer NN). Suppose the original inputs satisfy kxj k2 ≤ 1, Iyj I ≤ 1,
inputs Xj = Xj + Xj are perturbed by Xj 〜 N(0, vI), with probability 1 一 δ over the random
initialization, for k = 2d√n], perturbed gradient descent on the second layer weights achieves a
loss f (W*) ≤ e in Op(1) ∙ "n∕v5∕2P) log4(n∕e) iterations.
Proof. From the above lemmas, we know that with respective probability 1-o(1)δ, after the random
featuring, the following happens:
1 σ ■ ((Q 0 Q)X) ≥ ([DdP-kDP(PMk++1)-n]ʌ / ___________vpδ4p___
1.	σmin((Q 0 Q)X ) ≥ 1	[(4p)!]3	I	p4pn2p+1∕2k4p
2.	kQX0p∣∣2 ≤ √k(2(B + IPvdln((k + n)dδ-1/2))√dln((k + n)dδ-1/2))p for all j ∈
[n].
Thereby considering the PGD algorithm on W, since the random featuring outputs [(rf Xj)p]=
Q[χ0p] has [(rTXj)2p] = (Q 0 Q)X, from Theorem 4, given the singular value condition and norm
condition above We obtain the result in the theorem.	□
E.3 Proof of Theorem 6
In this section, we show the proof of Theorem 6. In the setting of Theorem 6, we do not add
perturbation onto the samples, and the only randomness is the randomness of parameters in the
random feature layer.
Recall that Q ∈ Rk×dp is defined as Q = [rfp, r0p …r0p]T. We show that: when r is sampled
from i.i.d. Normal distribution N(0, 1)d and k is large enough, with high probability Q is robustly
full column rank. Let N and N be respectively an ε-net and a σ-net of Xp with size Zε and Zσ.
The following lemmas(Lemma 15, 16 and 17) apply the standard ε-net argument and lead to the
smallest singular value of matrix Q(Lemma 18). Then we will derive the smallest singular value for
the matrix (Q 0 Q)X (Lemma 19).
Note that unlike the Q matrix in the previous section, in this section the Q matrix is going to have
more rows than columns, so it has full column rank (restricted to the symmetry of Q). The Q matrix
in the previous section has full row rank. This is why we could not use the same approach to bound
the smallest singular value for Q.
Lemma 15. For some constant C, with probability at least 1 — Zε (CpnIP∖, for all C ∈ N, we
have
IlQCll2 ≥ J
p!
Proof. For any C ∈ Xp, by Lemma 6, k CkrV ≥ √p. Let f (r) = ctr0p, then f is a polynomial of
degree p with respect to r, and therefore by Lemma 9,
Var
r 〜N (0,1)d
[f(r)] ≥ kCkr2v
1
≥ p!.
33
Under review as a conference paper at ICLR 2020
∀ri : |f (ri)| <
Thus by Proposition 2,
r〜N‰)d{f("< √ }≤ O(Pw/p.
K
Therefore, as kQck22 = Pf(ri)2,
i=1
Pr	[∣3∣2 < 矶 ≤	Pr
ri ,r2∙∙∙rκ ~N (0,1)d	p!	r1,r2∙∙∙rκ ~N (0,l)d
≤ (O(P"k.
Therefore for some constant C, for each C ∈ Xp, with probability at most(Cpn1/p)k there
is IlQck2 < P. Thus by union bound this happens for all C ∈ N with probability at most
≤ Zε (Cpη1∕p)k, and thereby the proof is completed.	口
Lemma 16. For τ > 0, with probability 1 - O
kQck2 ≤ τ.
1/p
1
ke-2
2/p
, for each c ∈ Nσ,
Proof. For any c ∈ Xd
Pr {kQck2 >τ2} ≤ Pr	{∃i : |CT『浮1 > √∖
Q	r1,r2…rk~N(0,l)d	k J
≤k Pr	IIcTr0p∣ > τ- ∖ .
—r~N (0,1)d	k
Furthermore,
-nP0,i∕ IcT P> √k}≤ r~NP0,1)d{ kck2krkp > √k}
= r~Np0,1)d{krk2 > (A)：
Therefore for the σ-net Nσ , with a union bound we know with probability at least 1 -
O f(Zσ (√Tk)1/p ke-2(√)2 p),forall c ∈ Nσ, ∣∣Qc∣∣2 ≤ τ2.	口
Lemma 17. For σ <	1,	τ >	0,	with probability at least 1 一 O (Zσ	(√Tk)	/P	ke- 2( √⅛)	^, we
have for each c ∈ Xp, kQck2 ≤ ɪ-^ ∙
Proof. We first show that give Nσ, for each c ∈ Xp, We can find ci, c2,c3 … ∈ Nσ and
ai,a2,a3 ∙∙∙ ∈ R such that
c =	aici,
i≥1
and that a1 = 1, 0 ≤ ai ≤ σai-1 (i ≥ 2). Thus ai ≤ σi-1.
In fact, we can construct the sequence by induction. Let I : Xp → Nσ that
I(x) = argmin ky 一 xk2.
y∈Nσ
34
Under review as a conference paper at ICLR 2020
We take c1 = I(c), a1 = 1, and recursively
(
c
ai
i-1
c -	aj cj
j=1
ci = I
i-1
-	ajcj
j=1
ai
2
∖ /
By definition, for any C ∈ Xp, ∣∣c 一 I(c)∣∣2 ≤ σ, and therefore
1
C -	1 aj Cj
ai
一 Ci	≤ σ,
2
i
which shows that 0 ≤ ai+1 = kC 一 P aj Cj k2 ≤ σai , and by induction ai ≤ σ
j=1
We know from Lemma 16 that with probability at least 1 一 O
Ci ∈ Nσ, kQCi k2 ≤ τ, and therefore
1/p -1 (= ∖2p∖
ke 2 v√k√	1, for all
kQCk2 ≤	aikQCik2 ≤ σi-1τ
i≥1	i≥1
τ
1 — σ
□
Lemma 18 (least singular value of Q). If Q is the k X dp matrix defined as Q = [rfp, rfp …rfp]T
with ri drawn i.i.d. from Gaussian Distribution N (0, I), then there exists constant G0 > 0 that for
k = αpDdp (α > 1), with probability at least 1 一 o(1)δ, the rows ofQ will span Xdp, and for all
C ∈ XP,
δ	δ ((α-1)Dd)
kQck2 ≥ ω [Mpy≡≡"^GPinpD5⅞≡α-≡
Ωp
'δ ((α-⅛
p+1
k 2(α-1)
where Ωp is the big-Ω notation that treats P as a Constant
Proof. We show that with high probability, for all C ∈
this we will adopt an ε-net argument over all possible
First, we take the parameters
k
χp, kQck2 = P (H3p]Tc) is large. To do
i=1
C.
1
Q = 10, T
2 log
p
, and
δ∖ (a-11Dd)
ε = co----------ɪ-
(TpP√p!) α-1
1
for small constant co such that c°CpD (α-1)D《 1, and η
know that with probability at least
20ετ√p!. From Lemma 15 and 17, we
1 — Zε "n1/p)k — O 卜(尸)ke-2(√)2∕p
=1 — O (COaT)Dp2Ddckds) — o ( δ
q2l°g Zσk
=1 — o(1),
the following holds true:
1.	NCi ∈ Nε, k QCill 2 ≥ √p!;
35
Under review as a conference paper at ICLR 2020
2.	VC ∈ Xd,kQck2 ≤ 占=⅛.
Therefore for any c ∈ xd, let i* = argmin ∣∣c — ci∣∣2, we know
i:ci ∈Nε
kQck2 ≥kQci k2 - kQ(c - ci)k
≥ ɪ
^√P!
≥ ɪ
^√P!
η
- kc - ci k2 Q
一εl
2ε√P1
2
c — ci
kc—cik2
2
—2√p!,
and by definition we know that λmin(Q) ≥ 2√^. By lemma 7, with log Zσ = O(PlnPDd) ≤
□
G0p ln pDdp for some constant G0, this gives us the lemma.
Lemma 19 (Smallest singular value for (Q 0 Q)X without pertubation). With Q being the
k X dd matrix defined as Q = [rfp, rfp …rfp]T, X being the d2p X n matrix defined as
X = [xf2p,..., x^2p] ∈ Rd2p ×n ,and Z =(Q 0 Q)X, for k = αPDdd (α > 1), when r are
randomly drawn from i.i.d. Guassian distribution N(0, I), there exists constant G0 > 0 such that
with probability ≥ 1 — o(1)δ, the smallest singular value ofZ satisfies
/
σmin(Z) ≥ Ω
δ( (ɑ-" σmm(X)
__	2a	1
、(pd√p!) α-1 [k(G0PlnpDp)d)] (α-1)
Ωp
(δ⅛) σmm(X)	(5)
k(aT)	I
(where Ωp is the big-Ω notation that treats P as a constant). Furthermore, for k = Ω(∕Dd), with
high probability 1 — δ, σmin(Z) ≥ Ω( σmik(X)) (if δ is not exponentially small).
Proof. From Lemma 18, with probability ≥ 1 — o(1)δ, for all C ∈ Xd, IlQck2 ≥ △
/	(	1	!
δk (aT)DP
~	,	、. α /	，1	、
(pp√p!) α-1 (k(G0p lnPDd)P)) 2(a-I)
.Then, from linear algebra, We know for all S ∈ Xd 0 Xd，
k(Q 0 Q)sk2 ≥ ∆2. As Xdd ⊂ Xd 0 xp,
σmin(Q 0 Q)X =
inf k(Q0Q)Xuk2
u∈Rn,kuk2=1
=U∈Rn*2 = 1 k(Q 0 Q) ⅛ k2kXUk2 ≥ δ2 U∈RninUk2 = 1 kXuk2 =	X)
which gives us this lemma 19.
Besides the lower bound for the smallest singular value, we also need the following lemma to show
that with high probability, the norm is upper bounded.
Lemma 20 (Norm upper bound for Qx0p). Suppose that ∣∣Xi∣∣2 ≤ B for all i ∈ [n] ,and Q =
[rfp,..., rfp]T ∈ Rk×dp. Then with probability at least 1 —
have
√2π ln(kdδ-1 /2)kd
, for all i ∈ [n], we
||Qx泞∣∣2 ≤ √k (2Bqdln(kdδ-1∕2))d
Ω
∖
δ
□
Proof. First we have, for a standard normal random variable N 〜N(0,1), we have
√2	,2
Pr{∣N| ≥ x}≤ ɪe-r.
πx
Then, apply the union bound, we have
Pr {∃i ∈ [d],j ∈ [k], |(rj)i| ≥ 2,ln(kdδT∕2)} ≤kd
exp (—2 ln(kdδ-1/2))
√∏2√ln(kdδ-1/2)
36
Under review as a conference paper at ICLR 2020
_	δ
=P2π ln(kdδ-1∕2 *)kd∙
If for all i ∈ [d],j ∈ [k], ∣(r7-)i| < 2vzln(kd), then for any X such that ||x|| ≤ B and any ko ∈ [k],
we have
I ((rk0产p)Tx% =∣(rTox)p∣
≤(I∣rk0 l∣∙∣∣χ∣l)p
≤(2B/d ln(kdδ-1∕2))p.
Then We have	P
∣∣Qxe)P∣∣2 ≤ √k(2Bqdln(kdδ-1/2)).
□
Then, combining the previous lemmas and Theorem 4, We have the folloWing Theorem 6.
Theorem 6. Suppose the matrix X = [x21P, ..., x2nP] ∈ Rd2p×n has full column rank, and smallest
singular value at least σ. Choose k = OP(dP), with high probability perturbed gradient descent on
the second layer weights achieves a loss f (W*) ≤ e in Op(1) ∙ 肾声/^ log4(n∕e) iterations.
Proof. From the above lemmas, We knoW that With respective probability 1-o(1)δ, after the random
featuring, the folloWing happens:
1. There exists constant Go that σma((Q 0 Q)X) ≥
δ( (a -I)Dp) σmin(X)
/	∖	2α r	i 1
(pp√p!) α-1 [k(Gop lnPDp)p)] (α-1)
2. kQxepk2 ≤ √k(2B√dln(kdδ-1∕2))p forall j ∈ [n].
Thereby considering the PGD algorithm on W, since the random featuring outputs [(riTxj)P] =
Q[xjeP] has [(riTxj)2p] = (Q 0 Q)X, from Theorem 4, given the singular value condition and norm
condition above we obtain the result in the theorem.	□
37