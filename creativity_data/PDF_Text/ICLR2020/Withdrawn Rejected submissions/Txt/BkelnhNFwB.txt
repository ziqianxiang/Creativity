Under review as a conference paper at ICLR 2020
Generalization Puzzles in Deep Networks
Anonymous authors
Paper under double-blind review
Abstract
In the last few years, deep learning has been tremendously successful in many
applications. However, our theoretical understanding of deep learning, and
thus the ability of providing principled improvements, seems to lag behind.
A theoretical puzzle concerns the ability of deep networks to predict well
despite overparametrization and despite the lack of complexity control under
the form of explicit regularization. Do deep networks require a drastically
new theory of generalization? In particular, are there measurements based on
the training data that are predictive of the network performance on future
data? Here we show that when performance is measured appropriately,
the training performance is in fact predictive of expected performance,
consistently with classical machine learning theory and the presence of an
implicit regularization.
1 Introduction
Is it possible to decide the prediction performance of a deep network from its performance
in training - as it is typically the case for shallower classifiers such as kernel machines and
linear classifiers? Is there any relationship at all between training and test performances?
Figure 1a shows that when the network has more parameters than the size of the training set
- which is the standard regime for deep nets - the training classification error can be zero and
is very different from the testing error. This intriguing lack of generalization was recently
highlighted by the surprising and influential observation (Zhang et al. (2016)) that the same
network that predicts well on normally labeled data (CIFAR10), can fit randomly labeled
images with zero classification error in training while its test classification error is of course
at chance level, see Figure 1b. The riddle of large capacity and good predictive performance
led to many papers, with a variety of claims ranging from “This situation poses a conceptual
challenge to statistical learning theory as traditional measures of model complexity struggle
to explain the generalization ability of large artificial neural networks...” Zhang et al. (2016),
to various hypotheses about the role of flat minima Keskar et al. (2016); Dinh et al. (2017);
Chaudhari et al. (2016), about SGD Chaudhari & Soatto (2017); Zhang et al. (2017) and to
a number of other explanations (e.g. Belkin et al. (2018); Martin & Mahoney (2019)) for
such unusual properties of deep networks.
We start by defining some key concepts. We call “loss” the measure of performance of the
network f on a training set S = xι,y 1, •…,xn,yN. The most common loss optimized during
training for binary classification is the logistic loss L(f) = N EN=Iln(I + e-ynf(xn)). We
call classification “error” N EN=I H(—ynf (xn)), where y is binary and H is the Heaviside
function with H(-yf(x)) = 1 if -yf > 0 which correspond to wrong classification. There is
a close relation between the logistic loss and the classification error: the logistic loss is an
upper bound for the classification error. Thus minimizing the logistic loss implies minimizing
the classification error.
The criticism in papers such as Zhang et al. (2016) refers to the classification error. However,
training minimizes the logistic loss. As a first step it seems therefore natural to look at
whether logistic loss in training can be used as a proxy for the logistic loss at testing. The
second step follows from the following observation. The logistic loss can always be made
arbitrarily small for separable data (when f (xn)yn > 0, ∀n) by scaling up the value of f and
in fact it can be shown that the norm of the weights of f grows monotonically with time
1
Under review as a conference paper at ICLR 2020
(b)
Figure 1: (a) Generalization error (yel low, the difference between training and test classifi-
cation accuracy) in CIFAR10 and (b) generalization error in CIFAR10 with random labels.
The DNN, trained by minimizing the cross-entropy loss, is a 5-layer convolutional network
(i.e., no pooling) with 16 channels per hidden layer. The activation are ReLUs. The resulting
architecture has approximately 10000 parameters. SGD was used with batch size = 100 for
70 epochs for each point. Neither data augmentation nor regularization is performed. The
generalization error in classification seems to converge to zero for increasing size of the
training set. In these plots we use up to all the training examples available for CIFAR-10
(50K).
during gradient descent. Notice that multiplying f by any positive number does not affect
its sign and thus does not change its behavior at classification. The second step is then to
consider the logistic loss of the normalized network f with f = ρf, with P = P1 ...pκ where
ρi is the Frobenius norm of the weight matrix of layer i.
Now we show that by measuring the performance of a trained network on the training set in
terms of the quantities described above, classical generalization holds: the performance of
the network on the training set becomes a good qualitative proxy for the expected future
performance of the network. The precise procedure involves normalizing the output of the
2
Under review as a conference paper at ICLR 2020
trained network by dividing it by a single number - the product of the Frobenius norms of
the weight matrices at each layer - and then measuring the cross-entropy loss.
Figure 2a) shows the cross entropy test loss plotted versus the training loss for networks with
the same architecture trained with different initializations (this is a way to get networks with
different test performance). There is no clear relation between training and test cross-entropy
loss in the same way that there is not between classification error in training and testing.
Normalizing each network separately yields Figure 2b): now the test loss is tightly predicted
by the training loss. We emphasize that normalization does not change the classification
performance of the networks. In fact, since (binary) classification depends only on the sign of
f (x), it is not changed by normalization to any bal l. The figure suggests that the empirical
cross-entropy loss of f on the training set predicts rather well how networks will perform
on a test set. Impressively, the same normalization process correctly predicts the test loss
when the training is on randomly labeled data. This apparent lack of predictivity of the
training performance on randomly labeled data was at the core of Zhang et al. criticism
of machine learning theory (Zhang et al. (2016)). Figure 2 shows that it is in fact possible
to gauge just from the training performance that the network trained on randomly labeled
examples will have chance performance on a test set even if its classification error in training
is always zero. As shown in Figure 2 the data point corresponding to the randomly trained
network still satisfies approximately the same linear relationship, as explained by the classical
theory. Thus, while Figure 2 shows that the normalized train cross-entropy can predict the
performance of the of the normalized test cross-entropy, Figure 4 b shows that the normalized
test cross-entropy is approximately predictive of the relative test classification error. Thus
Figures 2 and 4 b together show that the normalized train cross-entropy loss in training
is approximately predictive of the test classification error in testing, at least in terms of
ranking between networks.
2 Interpretation in terms of classical learning theory
We define a deep network with K layers with the usual elementwise scalar activation functions
σ(Z) : R → R as the set of functions f (Wι,…,Wk; x) = σ(Wkσ(Wk-ι …σ(Wιx))),
where the input is x ∈ Rd, the weights are given by the matrices Wk , one per layer, with
matching dimensions. We use the symbol W as a shorthand for the set of Wk matrices
k = 1, •…,K. For simplicity we consider here the case of binary classification in which f
takes scalar values, implying that the last layer matrix WK is WK ∈ R1,Kl and the labels are
yn ∈ {-1, 1}. There are no biases apart form the input layer where the bias is instantiated
by one of the input dimensions being a constant. The activation function in this paper is
the ReLU activation. We denote the network as f = f (Wι, •…，Wk ; x) where x is the input
and the weight matrices Wk are the parameters. We ca
weights matrices, that is f = f (V1, ∙∙∙ , Vk ; x) with Vi =
ll f the network with normalized
Wi
WH.
Consider different asymptotic minima of the empirical loss obtained with the same network
architecture on the same training set by minimization of the cross-entropy loss with different
initial conditions (see Appendix). We obtain different test losses, depending on initial
conditions. The question is whether their test performance can be predicted from empirical
properties measured only on the training set.
CIFAR10, MNIST and CIFAR100 1 used in our experiments are multiclass problems. In
this theory section we discuss for simplicity the simpler case of binary classification. Exactly
the same steps in the theoretical arguments below apply to the cross-entropy loss (because
Equations 1,3 apply, see Appendix).
Consider the structure of the loss for a deep network. The “positive homogeneity” property
of ReLU networks implies the following property:
____ ________ 、 、
f (W1, ∙ ∙ ∙ ,Wk ； x) = P1 …PK f(x) = pf(x)
1	TTΛ	TT7	1 I I T ɪʒ- Il Λ	1
where Wk = PkWk and || Wk|| = 1 and Pι ∙∙∙ PK = P .
(1)
1 MNIST and CIFAR100 results are in the appendix
3
Under review as a conference paper at ICLR 2020
Training loss (UnnormaIlzed)
Training Loss (normalized)
Training Loss (unnormalized)	Training loss (normalized)
(C)	(d)
Figure 2: The plot in a) shows testing vs training cross-entropy loss for networks trained
on the same data sets but with different initializations. The graph in b) shows the testing
vs training loss for the same networks, normalized by dividing each weight by the Frobenius
norm of its layer. Notice that all points have zero classification error at training. The red
point in b) refers to a network trained on the same CIFAR-10 data set but with randomized
labels. It shows zero classification error at training and test error at chance level. The black
line is a square-loss regression of slope 1 with positive intercept. The blue line is the diagonal
at which training and test loss are equal. The networks are 3-layer convolutional networks.
The plots in c) and d) show similar experiments for ResNet-56, demonstrating that our
observations hold for state-of-the-art networks (testing errors ranging from 7% to 9%; on
this 10-classes classification chance performance is 90% error). We again emphasize that the
performance in classification of normalized and unnormalized networks is exactly the same.
The normalization in the case of ResNet was performed by using the norm of the output
of each network on one example from CIFAR-10 (randomly chosen), because we found it
computational ly difficult because of skip connections to directly evaluate the effective norm of
the residual layers. Further details of this experiment are in the Appendix.
This property is valid for layer-wise normalization under any norm. We emphasize again that
f (Wι,…，Wk ; x) and f(x) := f (Wι,…，W/k ; x) have the same classification performance
on any data set. Furthermore, during gradient descent on separable data - most data sets
are separable by overparametrized deep networks - it can be shown (see Poggio et al. (2018))
that the ρ factor continue to increase with time towards infinity, driving an exponential type
loss to zero without affecting the classification performance, which only depends on the sign
of yn f (xn)∀n. As we discussed earlier, it seems that different networks corresponding to
different empirical minimizers2 could be evaluated in terms of their normalized form f (x).
The intuition is similar to the linear case in which the classifier wT x depends on the unit
vector W= ∣w∣ while |w| diverges to infinity Soudry et al. (2017); Poggio et al. (2018).
2 In general an overparametrized network may have a large number of global minima, see for
instance Poggio & Liao (2017).
4
Under review as a conference paper at ICLR 2020
To assess f(x) we compute its logistic loss (which is the special case of cross-entropy in the
binary case)
N
L(f) = NN 工 ln(1+ e-ynf(xn)).
(2)
n=1
zʌ C	C	11 Ij / f /	r /" ∙	1	J 1	J 1	1	C f
Of course for separable data (ynf(xn) > 0, ∀n) the loss of f is larger than the loss of f since
the negative exponent is smaller. Figure 2 uses L2 for layer-wise normalization.
We are now ready to explain the results of Figure 2 in terms of classical machine learning
theory. A typical generalization bound that holds with probability at least (1 - δ), ∀f ∈ F
has the form Bousquet et al. (2003):
|L(f) - L(f )| ≤ cιRN(F) +
(3)
where L(f) = E[£(f (x), y)] is the expected loss, RN(F) is the empirical Rademacher average
of the class of functions F measuring its complexity; c1 , c2 are constants that reflect the
Lipschitz constant of the loss function and the architecture of the network.
We use the bound in Equation 3 and the key observation that the Rademacher complexity
satisfies the property,
R N (F) = PR N (F),	(4)
because of homogeneity of the networks. Then, the bound on the cross-entropy loss for the
unnormalized network gives
K
L (f) ≤ ∏ RN (F) +
k=1
(5)
since L(f) ≈ 0. Considering the corresponding bound for the cross-entropy loss of the
normalized network scaled with any desired scaling R gives
,~∙ ʌ , ^-. , ^-.
L (Rf) ≤ L( Rf) + RR N (F)+ C 2
(6)
In our experiments we find that RN (F) is small for the value of N of our datasets and RN (F)
is large. Equation 5 implies then that the unnormalized test loss will be quite different from
zero and thus different from the unnormalized train loss. On the other hand, in Equation 6
the terms RN(F) + C2 Jl；N) are small, implying that the normalized test loss is very close to
the normalized train loss. Thus Equation 6 with R = 1 shows that L(f) is bounded by L(f),
predicting a bound in terms of a linear relationship with slope one and a small offset between
L(f) and L(f). The prediction is verified experimentally (see Figure 2). Notice that both
homogeneity of the ReLU network and separability, that is very small cross-entropy loss,
are key assumptions in our argument. The first applies to networks with a linear or ReLU
activation but not to networks with other activation functions. The second is usually satisfied
by overparametrized networks. Thus Equation 6 shows that the training cross-entropy
loss is a very good proxy of the cross-entropy loss at testing, implying generalization for
a relatively small number of examples N . Notice that for all the networks in Figure 2,
classification performance at training is perfect and that scaling does not change the sign of
the networks outputs and therefore their behaviour in terms of classification. In particular,
Figure 2 shows that performance on the randomly labeled training set when measured for
the normalized network is bad (despite classification being at zero error) predicting correctly
bad performance on the test set. As we mentioned, Figure 4 b shows that there is a good
correlation between crossentropy loss and classification performance: empirically the ranking
between the different classifiers is mostly the same for crossentropy vs classification loss.
5
Under review as a conference paper at ICLR 2020
OL&VLDUO-lo-lij
Figure 3: Classification error in CIFAR-10 as a function of number of parameters for fixed
training set. The DNN is the same as in Figure 1. The classification error at test does not
increase here when increasing the number of parameters beyond the size of the training set .
3	Minimization of the cross-entropy loss implies minimization
of the classification error
Gradient descent techniques, such as stochastic gradient descent (SGD), are used to minimize
the cross-entropy loss in deep networks. This is a typical approach in machine learning:
minimize a convex surrogate of the 0 - 1 loss function used for binary classification. The
logistic loss is an upper bound to the binary classification error: thus minimization of the
loss implies minimization of the error, yielding a monotonic relation between the logistic loss
and the classification error (Figure 4 b). Thus L(f) is a much better bound on the value of
the classification error than L(f). Figure 4 c shows the test classification error R(f) as a
function of the product of the layerwise norms of the network. Notice that on this 10-classes
classification problem chance performance is 90% error. Its monotonic behavior, consistent
with previous reports Neyshabur et al. (2017); Liang et al. (2017), follows from Equation 5
and the fact that R(f) is upper-bounded by the expected cross-entropy loss L(f).
The linear behavior of a bound on the expected loss as a function of the empirical loss L(f) is
thus expected and it is a consequence of previous results Neyshabur et al. (2017); Liang et al.
(2017); Bartlett et al. (2017); Neyshabur et al. (2017) and of the upper bounds Equations 3
and 6. Lower bounds, however, are not available. As a consequence, the theory does not
guarantee that among two (normalized) networks, the one with lower cross-entropy loss in
training will always have a lower classification error at test. This difficulty is not specific to
deep networks. It is common to approaches using a surrogate loss function. The empirical
evidence however supports the claim that there is a roughly monotonic relationship between
training (and testing) loss of the normalized network and its expected classification error:
Figure 4b shows an approximately monotonic relation between normalized test cross-entropy
loss and test classification error.
4	Discussion
The linear relationship we found means that the generalization error of Equation 3 is smal l
once the complexity of the space of deep networks is “dialed-down” by normalization. It
also means that, as expected from the theory of uniform convergence, the generalization
gap decreases to zero for increasing size of the training set (see Figure 1). Thus there is
6
Under review as a conference paper at ICLR 2020
std. dev.
of initial
weights
2000	2500	30∞	3500	4000	4500
Product of Frobenius norms in unnormalized network
Figure 4: a) Test loss for the normalized networks vs the product of the unnormalized
Frobenius norms of the layers, b) test classification error as a function of the normalized test
loss and c) test classification error vs the product of the unnormalized Frobenius norms of
the layers. The network architecture is the same as in the top of Figure 2.
indeed asymptotic generalization - defined as training loss converging to test loss when the
number of training examples grows to infinity - in deep neural networks, when appropriately
measured.
The title in Zhang et al. (2016) “Understanding deep learning requires rethinking generaliza-
tion” seems to suggest that deep networks are so “magical” to be beyond the reach of existing
7
Under review as a conference paper at ICLR 2020
machine learning theory. This paper shows that this is not the case. On the other hand,
the generalization gap for the classification error and for the unnormalized cross-entropy is
expected to be small only for much larger N (N must be significantly larger than the number
of parameters). However, consistently with classical learning theory, the cross-entropy loss
at training predicts well the cross-entropy loss at test when the complexity of the function
space is reduced by appropriate normalization. For the normalized case with R = 1 this
happens in our data sets for a relatively “small” number N of training examples as shown
by the linear relationship of Figure 2.
The classical analysis of ERM algorithms studies their asymptotic behavior for the number
of data N going to infinity. In this limiting regime, N > W where W is the fixed number of
weights; consistency (informally the expected error of the empirical minimizer converges to
the best in the class) and generalization (the empirical error of the minimizer converges to
the expected error of the minimizer) are equivalent. This note implies that there is indeed
asymptotic generalization and consistency in deep networks. However, it has been shown that
in the case of linear regression, for instance with kernels, there are situations - depending
on the kernel and the data - in which there is simultaneously interpolation of the training
data and good expected error. This is typically when W > N and corresponds to the limit
for λ = 0 of regularization, that is the pseudoinverse. It is likely that deep nets may have a
similar regime, in which case the implicit regularization described here, with its asymptotic
generalization effect, is just an important prerequisite for a full explanation for W > N - as
it is the case for kernel machines under the square loss.
The results of this paper strongly suggested that the complexity of the normalized network is
controlled by the optimization process. In fact a satisfactory theory of the precise underlying
implicit regularization mechanism has now been proposed Soudry et al. (2017); Du et al.
(2018); Banburski et al. (2019); Shpigel Nacson et al. (2018).
As expected, the linear relationship we found holds in a robust way for networks with different
architectures, different data sets and different initializations.
Our observations, which are mostly relevant for theory, yield a recommendation for practi-
tioners: it is better to monitor during training the empirical “normalized” cross-entropy loss
instead of the unnormalized cross-entropy loss actually minimized. The former matters in
terms of stopping time and predicts test performance in terms of cross-entropy and ranking
of classification error. More significantly for the theory of Deep Learning, this paper confirms
that classical machine learning theory can describe how training performance is a proxy for
testing performance of deep networks.
8
Under review as a conference paper at ICLR 2020
A Multi-class classification extension
While the theoretical explanation in the main text applies to the case of binary classification, the
extension to multi-class case follows straightforwardly.
Recall some definitions for neural networks with multiple outputs. Let C be the number of classes
-the neural network is then a vector f (W; X) ∈ RC. The component fj (W; X) denotes the j-th
output. The dataset is again composed of examples xn ∈ Rd and labels are now yn ∈ [C]. Note
that nothing here changes in regards to homogeneity, and we again can define a normalized network
____ _______ . ~,
f (W1, ∙∙∙ , WK ； x) = ρ 1 ∙∙∙ PK f(x)
(7)
The main theoretical arguments depend on the generalization bounds of the form
| L (f) - L( f )| ≤ C1R N (F) + C 2
(8)
As the right hand side depends on the neural networks, which do not change in any substantial way,
all that remains is understanding the multi-class loss.
To transform the outputs of the network into probabilities, the Softmax function is used
e-fi (W;x)
Pi = ---κ;------------.
E 上 Ie - fj (W 口)
(9)
The cross-entropy loss is then defined simply as
N
L(f) =-工 log
n=1
(10)
It’s very easy to see that this reduces to the logistic loss in the binary case.
Classification now depends only on the margin ηn = fyn (W； X) - maxj=yn{fj (W； X)} - if ηn > 0
then the example is correctly classified. This means that, again, classification only cares about the
sign of the margin and not the normalization of the neural network.
One final property of note is that for separable data, the loss monotonically decreases with increasing
ρ. To see this, let us write αnj = fyn (W； X) - fj (W； X), which is a positive quantity in the separable
case.
Additionally define gn
-log (Ej=yn e-αnj]
-log (Ej=yn e- ρα j
which is clearly a
monotonic function of ρ if all αnj > 0. We can now rewrite the Cross-entropy loss as
N
L( f )=工 log(1 + e - gn),
n=1
(11)
which implies that we can drive the loss to 0 by increasing ρ → ∞.
B Details of Figure 2 in the main text
Top The top left graph shows testing vs training cross-entropy loss for networks trained on the
same data sets but with different initializations. The top right graph shows the testing vs training
loss for the same networks, normalized by dividing each weight by the Frobenius norm of its layer.
Notice that all points have zero classification error at training. The red point on the top right
refers to a network trained on the same CIFAR data set but with randomized labels. It shows zero
classification error at training and test error at chance level. The top line is a square-loss regression
of slope 1 with positive intercept. The bottom line is the diagonal at which training and test loss
are equal. The networks are 3-layer networks; the first layer is convolutional, 64 filters of size 5x5,
stride 2, padding 2, no bias, ReLU activation; the second layer is also convolutional, 64 filters of
size 5x5, stride 2, padding 2, no bias, ReLU activation; the third layer is fully connected, input size
64*8*8, output size 10, no bias, softmax activation. The training is on the CIFAR-10 dataset with
50k training examples, 10k testing examples. The network used for the point in red was trained
similarly except the testing set and training set labels were randomized.
No data augmentation was performed, but data were normalized to mean (0.4914, 0.4822, 0.4465)
and standard deviation (0.2023, 0.1994, 0.2010). Trained for 200 epochs with decreasing learning
rate (0.01, 0.01, 0.001, 0.0001) until training error reached 0 percent. The weights were initialized
9
Under review as a conference paper at ICLR 2020
Training loss (UnnormaIlzed)
Training Loss (normalized)
Training Loss (unnormalized)	Training loss (normalized)
(C)	(d)
Figure 2 in the main text
from a normal distribution with mean 0 and standard deviation in [0.0001, 0.0002, 0.0003, 0.0004,
0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.0016, 0.0032, 0.0064, 0.0128, 0.0256, 0.0512, 0.06,
0.07, 0.08, 0.09, 0.1, 0.1024, 0.11, 0.12, 0.14]. For the red point (trained on random labels), the
weights were initialized from a normal distribution with mean 0 and standard deviation 0.1.
Bottom The bottom graphs show similar experiments for ResNet-56 with 56 layers, demonstrating
that our observations hold for state-of-the-art networks (testing errors ranging from 7% to 9%; on
this 10-classes classification problem chance performance is 90% error). We again emphasize that
the performance in classification of normalized and unnormalized networks is exactly the same. The
normalization in the case of ResNet was performed by using the norm of the output of each network
on one example from CIFAR-10, because we found it computationally difficult to directly evaluate
the effective norm of the residual layers.
The networks were trained for 200 epochs with learning rate = 0.01 for the first 100 epochs and
learning rate = 0.001 for the second 100 epochs. SGD was used with batch size of 128 and shuffled
training data with random crop and horizontal flip data augmentation.
C Selecting minimizers with same training performance but
different test performance
First we start with a common observation: even when two networks have the same architecture, same
optimization meta parameters and same training loss, they usually have different test performances
(i.e. error and loss), because the stochastic nature of the minimization process leads to convergence
to different minima among the many existing in the loss landscape Poggio & Liao (2017); Poggio
et al. (2017; 2018).
With standard settings the differences are usually small (though significant, as shown later). We use
therefore two approaches to magnify the effect:
•	Initialize networks with different levels of “random pretraining”: the network is pretrained
for a specified number of epochs on “corrupted” training data — the labels of a portion of
the examples are swapped with each other in a random fashion.
10
Under review as a conference paper at ICLR 2020
•	Initialize the weights of the networks with different standard deviations of a diagonal
Gaussian distribution. As it turns out, different standard deviations yield different test
performance.
Similar techniques have been used previously Neyshabur et al. (2017); Liang et al. (2017). We show
the results of “random pretraining” with networks on CIFAR-10 (Figure 5) and CIFAR-100 (Figure
7) and initialization with different standard deviations on CIFAR-10 (Figure 6) and CIFAR-100
(Figure 8).
3.5
2.5
0.45
2 5
OL上<u--0 Uo SSO-I
0.5
-θ-Train
∙A—Test
0.25
0.2
0.15
0.1
0.05
-θ-Train
∙A—Test
3
1
0-⅛<H--o Uo ⅛tω
0®—θ©----------@-------@------©-------©
0	0.2	0.4	0.6	0.8	1
Pretraining Corrupted Percentage
0©—θ-®-
0	0.2
-®------0-------Θ--------Θ
0.4	0.6	0.8	1
Pretraining Corrupted Percentage
Figure 5:	Random Pretraining vs Generalization Performance on CIFAR-10: a 5-layer
ConvNet (described in section L.2) is pretrained on training data with partial ly “corrupted”
labels for 30 epochs. It is then trained on normal data for 80 epochs. Among the network
snapshots saved from all the epochs we pick a network that is closest to an arbitrarily (but
low enough) chosen reference training loss (0.006 here). The number on the x axis indicates
the percentage of labels that are swapped randomly. Loss is cross-entropy loss; error is
classification error. As pretraining data gets increasingly “corrupted”, the generalization
performance of the resultant model becomes increasingly worse, even though they have similar
training losses and the same zero classification error in training. Batch normalization (BN)
is used. After training, the means and standard deviations of BN are “absorbed” into the
network’s weights and biases. No data augmentation is performed.
11
Under review as a conference paper at ICLR 2020
OKLL一。UoSSo-I
0.5
oτtt<H--o uo」oxl山
0.1
—θ-Train
△ Test
O Q	Q Q Q
0	0.05	0.1	0.15	0.2
Standard Deviation in Weight Initialization
0.05 ^	I	I
―θ-Train
ʌ Test
0LQQQQ
0	0.05	0.1	0.15	0.2
Standard Deviation in Weight Initialization
Figure 6:	Standard Deviation in weight initialization vs generalization performance on
CIFAR-10: the network is initialized with weights of different standard deviations. The other
settings are the same as in Figure 5. As the norm of the initial weights becomes larger, the
generalization performance of the resulting model is worse, even though all models have the
same classification error in training.
12
Under review as a conference paper at ICLR 2020
Figure 7: Same as Figure 5, but on CIFAR-100.
oo-i<N=o UoSSo-I
Figure 8: Same as Figure 6, but on CIFAR-100.
0t7h<lloUOSSO-I
13
Under review as a conference paper at ICLR 2020
D Linear relations between training and test loss of
normalized networks
Our observations are that the linear relationship between train loss and test loss for normalized
networks hold in a robust way under several conditions:
•	Independence from Initialization: The linear relationship is independent of whether the
initialization is via pretraining on randomly labeled natural images or whether it is via
larger initialization, as shown by Figures 11 and 12.
•	Independence from Network Architecture: The linear relationship of the test loss and
train loss does not depend on the network architectures we tried. Figure 10, shows the
linear relationship for a 3 layer network without batch normalization while Figures 11, 12
show the linear relationship for a 5 layer network with batch normalization on CIFAR10.
Additional evidence can be found in Figures 9. Figures in the main text show the same
linear relationship for ResNet architectures.
•	Independence from Data Set: Figures 10, 11, 12, 9 show the linear relationship on CIFAR10
while Figures 17 and 18 show the linear relationship on CIFAR100.
•	Norm independence: Figures 9 show that the choice of Lp norm used for normalization
does not matter - as expected, see main text. The constants underlying the equivalence of
different norms depend on the dimensionality of the vector spaces.
14
Under review as a conference paper at ICLR 2020
8 6 4 2
4 4φ4
60.60.
(pəz=EE-ouu ∩ OMJən)」0t山 ISəl
0.75
0.75
0.010∙1	0.75
0.5
。。。。现。£瞎
O.(XK)1 02	■
0.1 *0.2
0.0 。荏 2
0.2
0.
0.0∞1
0αx>1
0.0001001
0.0(Λθ1
O-Wf
0.0001
0.001
0.38
2.284	2.286	2.288	2.29	2.292	2.294	2.296	2.298
Train Loss (Network Normalized)
2.3	2.302
2.302
2.3
(PeZ=ELU,ION X.IoMJΘN) SSO-IISΘJ.
2.286
2.284	2.286	2.288	2.29	2.292	2.294	2.296	2.298	2.3	2.302
Train Loss (Network Normalized)
Figure 9: Test loss/error vs training loss with all networks normalized layerwise by the L1
norm (divided by 100 to avoid numerical issues because the L1 norms are here very large).
The model was a 3 layer neural network described in section L.1.1 and was trained with
50K examples on CIFAR10. The networks were normalized after training each up to epoch
300 and thus different points on the plot correspond to different training losses. Figure 10
shows that the normalized network does not depend on the value of the training loss before
normalization. The slope and intercept of the line of best fit are 0.8358 and 0.3783 respectively.
The ordinary and adjusted R2 values are both 0.9998 while the root mean square (RMSE)
was 5.6567 × 10-5 . The numbers in the figure indicate the amount of corruption of random
labels used during the pretraining.
O0 Φ
o 0
0
• Normalization is independent of training loss: Figure 10 shows that networks with different
cross-entropy training losses (which are sufficiently small to guarantee zero classification
error), once normalized, show the same linear relationship between train loss and test loss.
15
Under review as a conference paper at ICLR 2020
E Higher Capacity leads to higher Test Error
One way to formalize the upper bound on classification error by the logistic loss is to consider the
excess classification risk R(f) — R*, where R(f) is the classification error associated with f and
R* is the Bayes error Bartlett et al. (2003). Let US call as before L(f) the expected cross-entropy
loss and L* the optimal expected cross entropy loss. Then the following bound holds for binary
classification in terms of the so-called ψ-transform of the logistic loss Q:
R(f) — R* ≤ ψ-1(L(f) — L*)
(12)
where the ψ function for the logistic is similar to the ψ for the exponential loss which is ψ(x) =
1 — √1 — x2. The key point here is that ψ-1 is monotonic: minimizing the logistic or cross-entropy
loss implies minimizing the classification error.
Our arguments imply that among two unnormalized minimizers of the exponential loss that achieve
the same given small empirical loss L = e, the mmimizer with higher product of the norms P 1,… ,PK
has the higher capacity and thus the highest expected loss L. Experiments support this claim, see
Figure 13. Notice the linear relationship of test loss with increasing capacity on the top right panels
of Figure 11, 12, 17, 18.
16
Under review as a conference paper at ICLR 2020
9 8 7 6
(pθNcβlLUOUU∩ X .IOΛ4ΘN) Sso-Ilse!-
4
0
(P®Z_C5E-ON WoMJθn) SSo-I ISθl
0.01	0.1
0
1
0.75
0.75
0.75 0.75
0.75
1.5
2
Train Loss (Network Unnormalized)
2.5
×10^3
2.302
2.288
2.286	2.288	2.29	2.292	2.294	2.296	2.298
Train Loss (Network Normalized)
2.3	2.302	2.304
Figure 10: Left: test loss vs training loss with all networks normalized layerwise by the
Frobenius norm. Right: test loss vs training loss with all unnormalized networks. The model
was a 3 layer neural network described in section L.1.1 and was trained with 50K examples
on CIFAR10. In this experiments the networks converged (and had zero train error) but not
to the same loss. All networks were trained for 300 epochs. The losses range approximately
from 1.5 × 10-4 to 2.5 × 10-3 . The numbers in the figure indicate the amount of corruption
of random labels used during pretraining. The slope and intercept of the line of best fit are
0.836 and 0.377 respectively. The ordinary and adjusted R2 values are both 0.9998 while the
root mean square (RMSE) is 4.7651 × 10-5 .
17
Under review as a conference paper at ICLR 2020
3.2
3
1
2.8
φ
N
2.6
,N
E
o
U
m
ω
S
o
2.4
2.2
S
2
1.8
0
0.01
1.6
5	5.2	5.4	5.6	5.8
0.6 0.8
0.4
6
0.2
0.1
6.2
Train Loss (Unormalized Net)
x 10-3
2.3014
2.3012
2.301
φ
N
2.3008
φ
,N
K
2.3006
o
2.3004
2.3002
2.3
2.2998
2.2992	2.2994	2.2996	2.2998	2.3	2.3002	2.3004	2.3006	2.3008	2.301
Train Loss (Normalized Net)
Figure 11: (Part 1) Random pretraining experiment with batch normalization on CIFAR-
10 using a 5 layer neural network as described in section L.2. The red numbers in the
figures indicate the percentages of “corrupted labels” used in pretraining. The green stars
in the figures indicate the precise locations of the points. Top: Training and test losses
of unnormalized networks: there is no apparent relationship. Bottom: under layerwise
normalization of the weights (using the Frobenius norm), there is a surprisingly good linear
relationship between training and testing losses, implying tight generalization. See next page
for the product of L2 norms and classification errors.
18
Under review as a conference paper at ICLR 2020
.2
3
3 .8
2
6422
222
)teN dezilamronU( ssoL tseT
1.6
4000	5000	6000	7000	8000	9000	10000
Product of L2 Norms of Weights from All Layers (Unormalized Net)
11000
0.4
.0.6
■
0.4
0.39
,o
0.38
P
E
0.37
N
0.36
P
φ
6
0.35
-⊂
O
0.34
0.2
0.33
0.32
Illl______0.1
0
0.01 ------
0.31
2.2992	2.2994	2.2996	2.2998	2.3	2.3002	2.3004	2.3006	2.3008	2.301
Train Loss (Normalized Net)
Figure 11:	(Part 2) Top the product of L2 norms from all layers of the network. We observe
a positive correlation between the norm of the weights and the testing loss. Bottom: under
layerwise normalization of the weights (using the Frobenius norm), the classification error
does not change.
19
Under review as a conference paper at ICLR 2020
876 543
)teN dezilamronU( ssoL tseT
RL
02.15
0.1
C c.0.05
0.o1	0.O01
1
0.025	0.03	0.035
0.04	0.045	0.05	0.055	0.06	0.065	0.07	0.075
Train Loss (Unormalized Net)
2.303
2.3025
2.302
2.3015
N
2.301
2.3005
O
2.2995
ω
Φ
2.299
2.2985
2.2980
2.297	2.298	2.299
2.3	2.301	2.302	2.303
Train Loss (Normalized Net)
Figure 12:	(Part 1) Same as Figure 11 but using different standard deviations for initialization
of the weights instead of “random pretraining”. The red numbers in the figures indicate the
standard deviations used in initializing weights. The “RL” point (initialized with standard
deviation 0.05) refers to training and testing on completely random labels.
20
Under review as a conference paper at ICLR 2020
876 543
)teN dezilamronU( ssoL tseT
0.001
0.05
0.1
0.15	0.2
1
2000	3000	4000	5000	6000	7000	8000	9000	10000	11000
Product of L2 Norms of Weights from All Layers (Unormalized Net)
0.9
0.8
0.7
E
E
0.6
A
q
P
φ
0.5
C
F
U
O
0.4
O
色
ω
Φ
I-
0.30
G00.01
0.05
0.1
0.15 0.2
0.2
2.297	2.298	2.299	2.3	2.301	2.302	2.303
Train Loss (Normalized Net)
Figure 12: (Part 2) Same as Figure 11 but using different standard deviations for initialization
of the weights instead of “random pretraining”. The red numbers in the figures indicate the
standard deviations used in initializing weights. The “RL” point (initialized with standard
deviation 0.05) refers to training and testing on completely random labels.
Figure 13	shows that when the capacity of a network (as measured by the product norm of the
layers) increases, so does the test error.
21
Under review as a conference paper at ICLR 2020
2 5 8 6 4 2 4
5s 4 4 4 4s
O Oooo
」0」」①IS①①uo_lZ=RJ①U①6
0.38 ----------------l-
0	0.5
1	1.5	2	2.5	3
product norm: Ilw ”...1叫11	×ιo4
Figure 13: Plot of test error Vs the product of the Frobenius norms of the layers ∣∣ W∣∣product =
“L=1 Il Wi Il - The model WaS a 3 layer neural network described in section L.1.1 and trained
with 50K examples on CIFAR10. The models were obtained by pretraining on random labels
and then fine tuning on natural labels. SGD without batch normalization was run on all
networks in this plot until each reached approximately 0.0044 ± 0.0001 cross-entropy loss
on the training data. Similar results can be found in Neyshabur et al. (2017); Liang et al.
(2017) .
22
Under review as a conference paper at ICLR 2020
0.52
0.5
6 0.48
(D
N
^ω
E
⅛-
0. 0.46
J
φ
Z 0.44
8
⅛-
LLl
ω
2 0.42
0.4
0.38
00(°
0	0
0.1
0.01
缠
0.75
0.75
0.75
0.5
0.1	(≡
0.00010.10.0205
0.0001 02	,
qQ10∙2
0.0000鹤.00.2
。.0。。1雷10.2
00011001T
(O科
0.0001001
0.0(0⅛01
0.0001
0.0001
0.001
2.284	2.286	2.288	2.29	2.292	2.294	2.296	2.298	2.3	2.302
0
Train Loss (Network Normalized)
Figure 14:	Test classification error Vs training Cross-entropy loss with all networks normalized
layerwise. The model was a 3 layer neural network and was trained with 50K examples on
CIFAR10. The empirical dependence of classification error on normalized cross-entropy loss
shown here is different from the predicted Bartlett et al. (2003) upper bound provided by ψ-1
(pers. comm. Y. Yao).
F Results on MNIST
This section shows figures replicating the main results on the MNIST data set. Figures 15 and 16
show that the linear relationship holds after normalization on the MNIST data set. Figure 16 shows
the linear relationship holds after adding the point trained only on random labels.
23
Under review as a conference paper at ICLR 2020
0.125
0.∞1
0.001
0.125

喻
0.00250.1
139876543
32222222c3
2 2222222
(peNCBBON⅛0MlθN) sso-jlsəl
2.22
2.21
2.2	2.22	2.24	2.26	2.28	2.3	2.32
Train Loss (Network Normalized)
Figure 15: The figure shows the cross-entropy loss on the test set vs the training loss for
networks normalized layerwise in terms of the Frobenius norm. The model was a 3 layer
neural network described in section L.1.2 and was trained with 50K examples on MNIST.
All networks were trained for 800 epochs. In this experiment the networks converged (and
had zero train error) but not to the same loss. The slope and intercept of the line of best fit
are 1.0075 and -0.0174 respectively. The ordinary and adjusted R2 values are both 1.0000
while the root mean square (RMSE) was 9.1093 × 10-4 . The makers indicate the size of the
standard deviation of the normal used for initialization.
350.3250.215
60.a
(PΦNC5E.! OUUnxN) SSO-Ilsəl
24
Under review as a conference paper at ICLR 2020
5 0 5 0
2 2 11
(PΘNC5E.! OULlnx.IOMleN) Sso-Jlsθl
o
0.∞2	0.004	0.006	0.008	0.01	0.012	0.014
Train Loss (Network Unnormalized)
2.31
2.3
2.29
Oq 7 6 5 4 3 2
2 2 2 2 2 2 Z
2 2 2 2 2 2 2
(pθNraB ON WOMJeN) sso-jisəh
2.21
2.2	2.22	2.24	2.26	2.28	2.3	2.32
Train Loss (Network Normalized)
Figure 16: The figure shows the cross-entropy loss on the test set vs the training loss for
networks normalized layerwise in terms of the Frobenius norm. The model was a 3 layer
neural network described in section L.1.1 and was trained with 50K examples on CIFAR10.
All networks were trained for 800 epochs. In this experiment the networks converged (and
had zero train error) but not to the same loss. The slope and intercept of the line of best fit
are 1.0083 and -0.0191 respectively. The ordinary and adjusted R2 values are both 1.0000
while the root mean square (RMSE) was 9.1093 × 10-5. The points labeled 1 were trained on
random labels; the training loss was estimated on the same randomly labeled data set. The
points marked with values less than 1 were only trained on natural labels and those makers
indicate the size of the standard deviation of the normal used for initialization.
25
Under review as a conference paper at ICLR 2020
G Results on CIFAR-100
This section is about replicating the main results on CIFAR-100. Figure 7 shows how different test
performance can be obtained with pretraining on random labels while Figure 8 shows that different
increasing initializations are also effective.
Figures 17 and 18 show that the linear relationship holds after normalization, regardless of whether
the training was done with pretraining on random labels or with large initialization.
6
5.8
5.6
5.4
N
P
5.2
.
"cC
5
O
4.8
ω
ω
o
4.6
ω
Φ
4.4
4.2
4
0.0675	0.068	0.0685	0.069	0.0695	0.07	0.0705	0.071	0.0715	0.072
Train Loss (Unormalized Net)
4.60495
4.6049
(ΦN PeZ=πsE.ION) SSollSφl
4.60485
4.6048
4.60475
4.6047
4.6045
4.60455
4.6046
4.60465
4.6047
4.60475
4.6048
Train Loss (Normalized Net)
Figure 17:	(Part 1) Same as Figure 11 but on CIFAR-100.
26
Under review as a conference paper at ICLR 2020
6
5.8
5.6
Test Error (Unchanged by Normalization)	Test Loss (Unormalized Net)
Oo	Oooo
5.4
5.2
5
4.8
4.6
4.4
4.2 -
0∙01
4 l-
0.2
0.1
2.4	2.6	2.8
0.7
0.64
0.8
0.4
3	3.2	3.4	3.6	3.8	4	4.2
Product of L2 Norms of Weights from All Layers (Unormalized Net)
×104
0.8
10.6
0.4
------------------------------------------------------0.2
0.1
0.01
0 -
4.6045	4.60455
4.6046	4.60465
4.6047	4.60475	4.6048
Train Loss (Normalized Net)
Figure 17: (Part 2) Same as Figure 11 but on CIFAR-100
27
Under review as a conference paper at ICLR 2020
4 2 08 6
)teN dezilamronU( ssoL tseT
2
0.06
0.08
0.1
0.12	0.14	0.16	0.18	0.2	0.22
Train Loss (Unormalized Net)
Test Loss (Normalized Net)
4.6052
4.6051
4.605
4.6049
4.6048
4.6047
4.6046
4.6045
4.6043	4.6044	4.6045	4.6046	4.6047	4.6048	4.6049	4.605	4.6051	4.6052
Train Loss (Normalized Net)
Figure 18:	(Part 1) Same as Figure 12 but on CIFAR-100.
28
Under review as a conference paper at ICLR 2020
64 2 086
)teN deZilamronU( ssoL tseT
2
1
2345678
Product of L2 Norms of Weights from All Layers (UnOrmaIiZed Net)	x 104
)noitaZilamroN yb degnahcnU( rorrE tseT
4.6043	4.6044	4.6045	4.6046	4.6047	4.6048	4.6049	4.605	4.6051	4.6052
Train Loss (Normalized Net)
Figure 18: (Part 2) Same as Figure 12 but on CIFAR-100.
H Results with ResNet architectures
This section is about replicating the main results with ResNets (see main text figures). To avoid
using the product of layerwise norms - whose form is not obvious for ResNets - We normalized the
different architectures using their output on the same single image, exploting the fact that all the
networks only differ in their initialization (for this reason we did not plot the RL point because this
shortcut to normalization does not apply in the case of a different training set with random labels).
I Generalization with N → ∞
While in this article we use the term generalization when the offset in the difference between training
and test losses is small, the technical definition of “generalization” just requires that the offset
decreases with increasing N. This means that for all f ∈ F, limN→∞ |L(f) — L(f)| → 0, since in
general we expect the Rademacher complexity to decrease as N increases. As Figure 19 shows,
|L(f) — L(f)∖ does in fact decrease with increasing N (notice that L(f);≈ L(f) around N = 50000
for normalization to the unit L2 ball of our network). These arguments do not depend on the specific
form of the Rademacher complexity. For deep networks several measures of complexity have been
29
Under review as a conference paper at ICLR 2020
proposed - mostly involving products of layer-wise normsAnthony & Bartlett (2002); Neyshabur
et al. (2017); Bartlett et al. (2017); Liang et al. (2017).
⅞OMΦN PeZ=EE-IOUn) OLcr<LL-O UO SSo-I
Number of Training Examples
(WOMΦN PeZ=EE-ION) OTH<LL-O Uo SSo-I
Number of Training Examples
(X-OM-əN PeZ=EE-ION) OTSH--O Uo SSo-I
Number of Training Examples
Figure 19: (Part 1) Top: Unnormalized cross-entropy loss in CIFAR10 for normal ly labeled
data. Middle: Cross-entropy loss for the normalized network for normal ly labeled data.
Bottom: Generalization cross-entropy loss (difference between training and testing loss)
for the normalized network for normal ly as a function of the number of data N . The
generalization loss converges to zero as a function of N but very slowly.
30
Under review as a conference paper at ICLR 2020
(⅛OMleN PeZl-IOU ∩oLlr<zlδUo SSo-I
2.3
2.295
2.29
2.285
2.28
103	104
Number of Training Examples
2.305
105
2.275
102
)krowteN dezilamroN( 01-RAFIC no ssoL
)krowteN dezilamroN( 01-RAFIC no sso
Figure 19: (Part 2) Top: Unnormalized cross-entropy loss in CIFAR10 for randomly labeled
data. Midd le: Cross-entropy loss for the normalized network for randomly labeled data.
Bottom: Generalization cross-entropy loss (difference between training and testing loss) for
the normalized network for randomly labeled data as a function of the number of data N .
The generalization loss converges to zero as a function of N but very slowly.
31
Under review as a conference paper at ICLR 2020
J Margin bounds
A typical margin bound for classification Bartlett & Shawe-Taylor (1998) is
| Lbinary (f) - Lsurr (f )| ≤ b 1 N ' ) + b 2 ∖	(13)
η	2N
where η is the margin, Lbinary (f) is the expected classification error, Lsurr (f) is the empirical loss
of a surrogate loss such as the logistic or the exponential. For a point X, the margin is η 〜yρf (X).
Since RN (F)〜P, the margin bound says that the classification error is bounded by ɪ that is by the
value of f on the “support vectors” — that is on the Xi,yi s.t arg minn ynf (Xn).
We have looked at data showing the test classification error versus the inverse of the margin. The
data are consistent with the margin bound in the sense that the error increases with the inverse of
the margin and can be bounded by a straight line with appropriate slope and intercept. Since the
bound does not directly provide slope and intercept, the data do not represent a very convincing
evidence.
K Numerical values of normalized loss
Why are all the cross-entropy loss values close to chance (e.g. ln 10 ≈ 2.3 for a 10 class data set) in
the plots for convnets — bit not for ResNets — showing the linear relationship? This is because most of
the (correct) outputs of the normalized neural networks are close to zero as shown by Figure 20. We
would expect the norm of the network to be appromimately bounded by |f(W; X)| / |W ||X| = |X|; the
data X is usually pre-processed to have mean 0 and a standard deviation of 1. In fact, for the MNIST
experiments, the average value f(X) of the most likely class according to the normalized neural
network is 0.026683 with a standard deviation 0.007144. This means that significant differences,
directly reflecting the predicted class of each point, are between 0.019539 and 0.033827. This in turn
implies that the exponentials in the cross-entropy loss are all very close to 1. Before normalization
(which of course does not affect the classification performance), the average value f(X) of the most
likely class was 60.564373 with standard deviation 16.214078.
32
Under review as a conference paper at ICLR 2020
0
0
400
200
Oooooo
Q Q O'0101010
8 6 4 2 0 8 6
u①FIbaJ=SlUnOO
0.01	0.02	0.03	0.04
f(x) value
0.05	0.06
Figure 20: Histogram of the values of f(x) for the most likely class of the layerwise normalized
neural network over the 50K images of the MNIST training set. The average value f (x) of
the most likely class according to the normalized neural network is 0.026683 with standard
deviation 0.007144.
L Deep Neural Network Architecture
L.1 Three layer network
L.1.1 Network with 24 filters
The model is a 3-layer convolutional ReLU network with the first two layers containing 24 filters of
size 5 by 5; the final layer is fully connected; only the first layer has biases. There is no pooling.
The network is overparametrized: it has 154, 464 parameters (compared to 50, 000 training examples).
L.1.2 Network with 34 filters
The model is the same 3-layer convolutional ReLU network as in section L.1.1 except it had 34
units.
The network was still overparametrized: it has 165, 784 parameters (compared to 50, 000 training
examples).
L.2 Five layer network
The model is a 5-layer convolutional ReLU network with (with no pooling). It has in the five layers
32, 64, 64, 128 filters of size 3 by 3; the final layer is fully connected; batch-normalization is used
during training.
The network is overparametrized with about 188, 810 parameters (compared to 50, 000 training
examples).
33
Under review as a conference paper at ICLR 2020
L.3 ResNet
The model for the experiments with ResNets was the 56-layer ResNet as detailed in He et al. (2015).
34
Under review as a conference paper at ICLR 2020
References
M. Anthony and P. Bartlett. Neural Network Learning - Theoretical Foundations. Cambridge
University Press, 2002.
Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Bob Liang, Jack Hidary, and
Tomaso Poggio. Theory III: Dynamics and Generalization in Deep Networks. arXiv e-prints, art.
arXiv:1903.04991, Mar 2019.
P. Bartlett and J. Shawe-Taylor. Generalization performance of support vector machine and other
patern classifiers. In ation performance of support vector machine B. Scholkopf, C. Burges and
other Patern classifiers (eds.), Advances in Kernel MethodS-Support Vector Learning. MIT press,
1998.
P. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks.
ArXiv e-prints, June 2017.
Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk
bounds. Technical report, 2003.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. CoRR, abs/1706.08498, 2017. URL http://arxiv.org/abs/1706.08498.
M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel
learning. ArXiv e-prints, Feb 2018.
O. Bousquet, S. Boucheron, and G. Lugosi. Introduction to statistical learning theory. pp. 169-207,
2003.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. CoRR, abs/1710.11029, 2017. URL http://arxiv.
org/abs/1710.11029.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing Gradient
Descent Into Wide Valleys. arXiv:1611.01838 [cs], November 2016. URL http://arxiv.org/
abs/1611.01838. arXiv: 1611.01838.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. CoRR, abs/1703.04933, 2017. URL http://arxiv.org/abs/1703.04933.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge-
neous models: Layers are automatically balanced. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Process-
ing Systems 31, pp. 384-395. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7321- algorithmic- regularization- in- learning- deep- homogeneous- models- layers- are- automatically- balanced.
pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. arXiv e-prints, art. arXiv:1512.03385, Dec 2015.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.
arXiv:1609.04836 [cs, math], September 2016. URL http://arxiv.org/abs/1609.04836. arXiv:
1609.04836.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry,
and complexity of neural networks. CoRR, abs/1711.01530, 2017. URL http://arxiv.org/abs/
1711.01530.
Charles H. Martin and Michael W. Mahoney. Traditional and heavy-tailed self regularization in
neural network models. CoRR, abs/1901.08276, 2019. URL http://arxiv.org/abs/1901.08276.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring
generalization in deep learning. arXiv:1706.08947, 2017.
T. Poggio and Q. Liao. Theory II: Landscape of the empirical risk in deep learning. arXiv:1703.09833,
CBMM Memo No. 066, 2017.
35
Under review as a conference paper at ICLR 2020
T. Poggio, Q. Liao, B. Miranda, L. Rosasco, X. Boix, J. Hidary, and H. Mhaskar. Theory of deep
learning III: explaining the non-overfitting puzzle. arXiv:1703.09833, CBMM Memo No. 073,
2017.
T. Poggio, Q. Liao, B. Miranda, A. Banburski, X. Boix, and J. Hidary. Theory IIIb: Generalization
in deep networks. arXiv:1703.09833, CBMM Memo No. 090, 2018.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic Gradient Descent on Separable
Data: Exact Convergence with a Fixed Learning Rate. arXiv e-prints, art. arXiv:1806.01796, Jun
2018.
D. Soudry, E. Hoffer, and N. Srebro. The Implicit Bias of Gradient Descent on Separable Data.
ArXiv e-prints, October 2017.
C. Zhang, Q. Liao, A. Rakhlin, K. Sridharan, B. Miranda, N.Golowich, and T. Poggio. Theory of
deep learning IIb: Optimization properties of SGD. CBMM Memo 072, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http:
//arxiv.org/abs/1611.03530.
36