Under review as a conference paper at ICLR 2020
A Uniform generalization bound for Genera-
tive Adversarial Networks
Anonymous authors
Paper under double-blind review
Ab stract
This paper focuses on the theoretical investigation of unsupervised generalization
theory of generative adversarial networks (GANs). We first formulate a more rea-
sonable definition of general error and generalization bounds for GANs. On top of
that, we establish a bound for generalization error with a fixed generator in a gen-
eral weight normalization context. Then, we obtain a width-independent bound by
applying 'p,q and spectral norm weight normalization. To better understand the
unsupervised model, GANs, we establish the generalization bound, which uni-
formly holds with respect to the choice of generators. Hence, we can explain how
the complexity of discriminators and generators contribute to generalization error.
For 'p,q and spectral weight normalization, we provide explicit guidance on how
to design parameters to train robust generators. Our numerical simulations also
verify that our generalization bound is reasonable.
1	Introduction
The generative adversarial network (GAN) (Goodfellow et al., 2014) is one of most powerful gen-
erative models for modeling complex high-dimensional tasks, such as image generation, dialogue
generation, and image impainting. Many variants of GANs (Ho & Ermon, 2016; Abadi & Andersen,
2016; Goodfellow et al., 2014; Li et al., 2017; Yu et al., 2018) have also been introduced to reinforce
the stability of training processes to obtain more realistic models.
A GAN consists of two neural networks: a discriminator and a generator. Literally, the generator
generates simulated data, while the discriminator tries to discriminate between simulated data and
real data. The training process of GANs is tantamount to a two-player game between a generator
and a discriminator. The main goal is to obtain a good generator, which is able to successfully
approximate the distribution of real data. We denote the distribution of real data and the generator-
induced distribution by Dreal and Dg, respectively. Our goal is to find a generator such that Dreal =
Dg. We revise the goal as d(Dreai, Dg) = 0, with a distribution distance d(∙, ∙). The Jensen-Shannon
(JS) divergence is implicitly used in Vanillar GANs (Goodfellow et al., 2014), and the 1-Wasserstein
distance is employed in WGANs (Arjovsky et al., 2017). Empirical experiments suggest that the
Wasserstein distance is a more sensible measure for differentiating probability measures supported
in low-dimensional manifolds.
The generalization properties of GANs are less explored in the literature, and some exceptions are
these works(Jiang et al., 2019; Arora et al., 2017; Bartlett et al., 2017; Zhang et al., 2017). Motivated
by the supervised learning context, where we say training to be generalized if the gap between the
training loss and the test loss is small, we can define the generalization for GANs in a similar way.
Concretely, generalization for GANs means that, the population distance between Dreal and Dg is
closed to the empirical distance between the empirical distributions of Dreal and Dg . Hence, we
define the gap between the population and empirical distance as the generalization error. Though
our ultimate goal is to minimize the former distance, the latter one is what we minimize in practice.
Given that our training process provides a small distance between empirical distributions, a small
generalization error indicates that the population distance is also small. In other words, a small
generalization error guarantees that the generated distribution is close to the real data distribution.
In fact, the training process of GANs is sample-dependent. In other words, the generator depends
on the training data sets, which are random samples from Dreal . The training process minimizes
1
Under review as a conference paper at ICLR 2020
τ /∙-r>	zrʌ ∖	1	zʌ	1	.	. 1	∙ ∙ ι ι ∙ . ∙ι	ι	EI ι ∙	ι
d(Dreal , Dg), where Dreal denotes the empirical distribution over samples. The deviation be-
tween Dreal and Dreal leads to the generalization error, i.e., the gap between d(Dreal , Dg) and
d⑦real, Dg). This motivates Us to establish a bound for generalization error, that is, the general-
ization bound. A tight generalization bound guarantees that the generalization error is small. The
highlights and main contributions of this article are summarized as follows:
•	We formulate new definitions for both generalization error and generalization bound, which
are more reasonable than the definitions in previous work (Arora et al., 2017; Jiang et al.,
2019).
•	We establish the generalization error bound in a general version, with a fixed generator. By
applying 'p,q weight normalization, we obtain a tighter bound.
•	We establish the generalization error bound, which uniformly holds over any choice of gen-
erator. Hence, we can explain how the complexity of the generator class and discriminator
class contribute to the generalization error.
•	Numerical experiments on Gaussian Mixture models verify that the theory of generalization
error bound is consistent with the numerical performance.
1.1	Related Work
Some previous works provide theoretical investigations of the generalization of GANs. Arora et al.
(2017) introduces a new metric for distributions called F -distance, and defines the generalization for
GANs based on this distance. On top of that, the paper shows that generalization does happen with
a moderate number of training examples (i.e., when the generator wins, the two distributions must
be close in F -distance). However, they analyze the generalization with a fixed generator. Hence,
the result is not guaranteed to hold uniformly across all generators. In this paper, we establish
generalization error bounds for both scenarios. When a generator is fixed, we provide a tighter
bound for generalization error than that in Arora et al. (2017).
Jiang et al. (2019) also established a bound for generalization error using spectral normalization of
GANs with a fixed generator. They show the advantages of spectrum control for generalization by
constraining discriminator class. By adopting the Rademacher Complexity, Bartlett et al. (2017)
yielded a bound of order O( ,d3k∕m), where d, k, m stand for the largest discriminator width, the
discriminator depth, and the training data size, respectively. Jiang et al. (2019) derived a bound
____________________ ɪ _______________________________________________________
of order O(,d2k∕m). In our work, We establish a bound of order O(dp* ,k/m) with 1/p* 6
1, d1 6 d, which is tighter than those from previous works. Moreover, we provide a more general
version of the bound for generalization error with a fixed generator, and the result in Jiang et al.
(2019) is a special case under the spectral weight normalization. To the best of our knowledge, we
are the first to establish a generalization bound for GANs that holds uniformly across all generators.
2	Preliminaries
We first introduce the formulation of the generalization bound for GANs. We use Dreal for the
real data distribution over Rd0 , and Nd(0, I) for a d-dimensional standard Gaussian distribution.
We define the sample set as S , {xi}im=1, where {xi}im=1 are i.i.d samples from Dreal and de-
note Z 〜Nio (0,I) as Gaussian noise. We denote the 'p,q-norm of a matrix A as IlAkp,q ，
q 1	，	，
(Pj(Pi Apj)P) q and the spectral norm as ∣∣A∣∣2 . We denote the conjugate of P as p*, with
\/p + 1/p = 1. For an arbitrary distribution μ, μ denotes the empirical distribution over a random
sample of size m from μ.
We let F = {f | f : Rd0 → [-1, 1]} denote the function class of discriminators, and G = {g | g :
Rl0 → Rd0} denote the function class of generators. Every generator g ∈ G induces a distribution
Dg: by applying g to a random sample Z 〜 Nlo (0, I), then we generate a sample g(z) from Dg. In
the context of GANs, both F and G are neural network function classes. Specifically, ∀ f ∈ F, x ∈
Rd0, f(x) = Tf,k1 + 1 ◦ σ◦ Tf,kι ◦…。σ◦ Tf, 1 ◦ x, where∀ 1 ≤ i ≤ kι + 1,Tf,i(u)，W%u+bf,i,
and σ(∙) is a ρ-Lipschitz active function. Note that, bf,i ∈ Rdi×1, Wf,i ∈ Rdi-1 ×di, where di
is the width of the ith layer of f, and k1 + 1 is the depth of f . For convenience, we introduce
2
Under review as a conference paper at ICLR 2020
Mf,i , (bf,i , Wf>,i)>. Similarly, we define the generator class as G , {g | g = Tg,k2+1 ◦ σ ◦
Tg,k2 ◦…。σ ◦ Tg,ι ◦ z, z ∈ Rl0}. Here, Tg,i(u)，W>iU + bg,i, 1 ≤ i ≤ k2 + 1. Note
that, bg,i ∈ Rli×1, Wg,i ∈ Rli-1 ×li, where li is the width of the ith layer of f, and k2 + 1 is
the depth of g. For neural network functions f ∈ F and g ∈ G, we parameterize them as fw , gv ,
respectively, where w, v are the weight parameters. We denote W and V as the parameter space of
F, G, respectively. We denote the Lipshchitz constant (with respect to the input x ∈ Rd0) of f as L.
Weight normalization. Weight normalization is an efficient regularization method for training
robust models. We introduce the 'p,q and spectral weight normalizations, and establish the general-
ization theory for such weight normalized neural networks.
Assume that, F is a neural network function class, which is parameterized as F = {fw|w =
(Mf,k1+1,..., Mf,ι), W ∈ W}. We define F with 'p,q weight normalization as ∣∣Mf,ikp,q 6
cF,i, i = 1, . . . , k1 + 1. In this context, we define the parameter norm as kw - w0 kp,q ,
Pk=+1 ∣Mf,i - Mf,ikp,q/cF,i.
For spectral weight normalization, we repeat the definition in Jiang et al. (2019)’s work. As-
sume that, F is a neural network function class without bias terms. It is parameterized as
F = {fw|W = (Wf,k1+1, . . . , Wf,1), W ∈ W}. We define F with spectral weight normaliza-
tion as ∣Wf,i ∣2 6 BF,i, i = 1, . . . , k1 + 1. In this context, we define the parameter norm as
kw - W0k2 , Pk=+1 ∣Wf,i - Wf,ik2∕BFM
For F and G, the parameter norm ∣∣ ∙ ∣∣ induces the metric parameter space (W, ∣∣∙∣∣) and (V, ∣ ∙ ∣∣),
respectively. Hence, We define the Lipschitz constants of f ∈ F and g ∈ G (with respect to ∣∣∙∣∣)as
Lf and LG, respectively.
GANs. According to the training process of GANs, we formulate the objective functions as:
min max E	[φ(f (x)] - E [φ(f (x))],
g∈G f ∈F X〜Dreal	X〜Dg
where φ(∙) : [-1,1] → R is a monotone Lφ-Lipschitz continuous function. The objective function
shows that, the discriminator f should give high values to X 〜Dreal and low values to X 〜Dg.
When Dreal , Dg are the same, f is expected to output 0.
The training process for GAN is tantamount to minimizing a specific distance, between Dg and
Dreal. To measure the distance between distributions, we consider a general distance. Let μ, V be
two distributions supported on Rd1 :
dF (μ,ν) , SuF xEμ[φ(f (X))] -
E [φ(f(X))].
X〜V
The objective function is equivalent to ming∈G dF (Dreal , Dg ). Without a loss of generality, we
take φ(x) ，X in our work. Therefore, our distribution distance can be revised as dF(μ, V)
SuF XJyf(X)I-
E [f (X)]. Though the distribution distance is similar to Wasserstein Distance
XJν
(Arjovsky et al., 2017), our discriminator function class F is not forced to be an 1-Lipschitz function
class. According to Arora et al. (2017)’s work, if we constrain the range of f to [0, 1] and utilize
the F -distance (Arora et al., 2017), the representation can be reduced to the original GAN and
WGAN by specific φ(∙). Thus, it can unify the JS divergence and the Wasserstein distance. Since
our generalization theory also holds in these cases, we omit repetitive discussions of F-distance.
Rademacher Complexity. The complexity or capacity of a network function class has a direct
effect on the generalization properties of a network. Since a GAN model consists of two network
structures, the complexities of F and G are the keys to further investigation of the generalization
properties of GANs. The definition of the Rademacher complexity is given as follows. If we as-
sume that F is a class of real value functions, and i is the Rademacher Random Variable, then the
empirical and expected Rademacher complexities are defined accordingly,
Rn,D (F)，ES〜Dn [rs(F)],
1n
R S (F) , Ee SuP — V" Gf(Zi)
f∈N n i=1
where 1, . . . , n are independent Rademacher random variables, i.e., P(i = 1) = P(i = -1)
1∕2.
3
Under review as a conference paper at ICLR 2020
3	GENERALIZATION ERROR B OUND WITH A FIXED g
We first introduce the definition of generalization error for GANs. In supervised learning, general-
ization error refers to the gap between the training error and the test error. However, in the context
of GANs, neither the training error nor the test error is well defined. This is because the discrimi-
nator f, which is the counterpart of the loss function, varies throughout the training process. In the
following, we provide a reasonable measure for the counterparts of the training error and the test
error for GANs using the distribution distance dF(∙, ∙). For a GAN model with the discriminator
f ∈ F and the generator g ∈ G, we define the training error as dF (Dreal, Dg) and the real error as
dF (Dreal, Dg), where Dreal is the empirical distribution over the sample set S. The generalization
error bound for GANs is defined as the difference between these two errors.
We compare our definition of the generalization error with that in Arora et al. (2017); Jiang et al.
(2019); Zhang et al. (2017). Arora et al. (2017) defined the training error as dF (Dreal, Dg), which is
related to the empirical distribution of Dg . In other words, noise samples are regarded as a training
set for GANs, while Dg is regarded as an unknown distribution. However, such a consideration is
not commensurate with the experiment process. Instead of being reinput in every iteration like the
training set S, a new noise sample set of size m is generated in every epoch. Thus, noise sample
sets are not equivalent to the training set S , since every noise sample sets is utilized only once.
The definition of the generalization bound in Jiang et al. (2019); Zhang et al. (2017) is slightly
different from our intuition. In these works, g is defined as the generator, which is obtained by the
training process, and g* is defined as the optimal generator for infg∈g dF(Dreaι, Dg). Then, they
define the generalization bound as dF(Dreaι, Dg) - dF(Dreaι, Dg*). However, in practise, exact
values for g * and Dreal are not accessible. Thus, the first term fails to represent the training error,
and the second term fails to represent the testing error.
It is more reasonable to assume that Dg is a known distribution for a fixed g. The process of
generating new noise samples in every epoch, is an empirical approximation of Dg , rather than a
simple “training noise” data collection process. From this perspective, our definition provides a
more reasonable theoretical measure of the generalization error for GANs.
Definition 3.1. For a GAN model with the discriminator f ∈ F and the generator g ∈ G, the
generalization error is defined as |dF (Dreal, Dg) - dF (Dreal, Dg)|. We say is a generalization
error bound if the following holds:
I 7	∕<T"∖	<T"∖ ∖	7	∕∙-r∖	<T-∖ ∖ I /
sup |dF (Dreal , Dg ) - dF (Dreal , Dg )| 6 ,
g∈G
where only relies on the parameter settings of F and G.
Intuitively, a low generalization error bound guarantees that the generalization error is low. Hence,
the discriminator successfully discriminates between real data and unseen data. In these cases, the
generator generates a distribution close to Dreal . An explicit generalization error bound provides
us with guidance to design a GAN for designing a GAN to adequately fit real data. The following
theorem provides an upper bound for the generalization error, with a fixed g .
Theorem 3.2. For any fixed g ∈ G, with a probability of at least 1 - δ over the choice of samples
S:
r
IdF (Dreal,Dg ) - &F (Dreal , Dg )| 6 2Rm,D“ai (F) +
log(1∕δ)
m
(1)
Though the result of Theorem 3.2 seems similar to the Theorem 3.1 in Zhang et al. (2017), our def-
inition of generalization error is different. Theorem 3.2 shows that, for a fixed g ∈ G, the bound for
the generalization error mainly depends on the complexity of F . Although the discriminator class
should be complex enough to discriminate between Dreal and Dg , a grossly complicated discrimi-
nator class generates extra generalization errors. Theorem 3.2 also shows the relationship between
Rm,Dreal (F) and the generalization error bound. For some specific neural network function classes,
such as the class of 'p,q weight normalized neural networks, We can compute the Rademacher com-
plexity to obtain an explicit upper bound for the generalization error. There are some previous works
of Rademacher Complexity bounds, including Golowich et al. (2018); Neyshabur et al. (2015); Chen
et al. (2019). We adopt Chen et al. (2019) to obtain a tighter and more general bound.
4
Under review as a conference paper at ICLR 2020
Corollary 3.3. Assume ∣∣xkp* 6 1, ∀x ∈ S For any fixed g ∈ G, with a probability ofat least 1 一 δ
over the choice of samples S, with 'p,q weight normalization:
IdF (Dreal, Dg ) — dF (DDreal, Dg )|
62 卜k1+N (2k1 +4"迹+ kY1 CF,iPdΓ- q] + dt rCp)+ r1°g≡,
where
k1+1 k1 + 1	「11]	1 k1+1	「1	1]
Sk+1 , X ( Y CF,ιρdlq] +)+ dΓ Y CF,1 Pdlkq]+ and
i=1 l=i	l=1
C( ) , (2 log(2d0)	p ∈ {1} ∪ (2, ∞),
Imin(p* - 1, 2iog(2d0))	P ∈ (1, 2].
1	_____
By utilizing the big O notation, Theorem 3.2 provides a bound of order O(d0 YkIlm), where
\/p 6 1 usually holds. Notice that, some previous works (Bartlett et al., 2017; Jiang et al., 2019)
provide bounds of order O(,心k\/m) and O(d，kI/rn), where d = max{di}k= ： Hence, our
bound is tighter than previous works.
Remark 3.4. A fair comparison between our bound and the bound in Arora et al. (2017) shows
our bound is tighter. We denote PF as the number of parameters of f ∈ F, and LF as the
Lipschitz constant with respect to the parameters of f ∈ F. We assess the two results un-
der our definition of generalization bound in Section 3. According to Arora et al. (2017) , if
m > 3PF log(Lj-PF/e)/e2, we have a probability of at least 1 — exp(-PF) over the choice of
C I 1 ∕<τ"∖	<τ~∖ ∖ 1 ∕∙-r∖	<τ-∖ ∖ I / /c r∏ι ■ ι ill	. ι r	ι ，，∖ ■ . ι r∏ι	n C
S, |dF (Dreal, Dg) 一 dF (Dreal, Dg)| 6 /2. This holds because the formula (6) in the Theorem B.2
of Arora et al. (2017) no longer contributes to the generalization bound. We convert our result into
a similarfashion by utilizing 1/m 6 e2∕(3PF log(LF-PF/e)) and δ = exp(—PF-). We obtain
|dF (Dreal, Dg ) — &F (Dreal, Dg )|
k	k1+1	[ J 1 ] ɪ
61 2(sk1+ι√(2kι +4)log2+ ɪɪ CF,iρdp q + dlf √C(P)) +
i=1
√3PF log(LF PF/I)
Since {cF,i}ik=1 1 can be constrained to small values by applying weight normalization, we can force
the following holds:
k1 + 1	[ J 1 ] ɪ
2(sk1+1 √(2fcι +4)log2+ ɪɪ CF,iρdp q + dlf √C(P))
i=1
6√PF ∙ (2t 3log( X Pk1+1-i Y dl p* q ]+cF,j PF∕e) - 1
log(LF-PF/e) ≈ log(PF√m) and PF are large numbers, which only depend on the structure of
F. Hence, there exits {cF,i}ik=1 1 that is small enough to satisfy the inequality above. In these cases,
our bound is tighter than Arora et al. (2017)’s bound. In fact, the weight normalization contracts
the range of network parameters, so the complexity of F is reduced, thereby leading to a tighter
generalization error bound.
Inspired by the probabilistic inequality in corollary 3.3, we formulate a hypothesis testing process
to judge whether a generator produces data with the same distribution as that of the real data. The
appendix contains the theory and experiments related to this novel hypothesis test on a toy dataset.
If We adopt ReLU (a homogeneous function) as an active function and apply the 'p,q weight normal-
ization, the bound for the generalization error can be further reduced to become width-independent.
Corollary 3.5. Under the same settings as Corollary 3.3, if 1/p + 1/q > 1, we adopt the ReLU
function as active function. Then, for any fixed g ∈ G, with a probability of at least 1 — δ over the
choice of samples S:
^F (Dreal, Dg ) - &F (Dreal, Dg )|
62 ((1+ d* )kY1CF ,l r (2k1 +4)log2 + kY1 CF,idt 百)+ rogP.
5
Under review as a conference paper at ICLR 2020
We can easily extend our results to cases with spectral weight normalization (Jiang et al., 2019).
Corollary 3.6. Let d = max{di}ik=1+1 1. Under the same settings as Corollary 3.3, for any fixed
g ∈ G, with a probability of at least 1 - δ over the choice of samples S with spectral weight
normalization:
IdF (Dreal, Dg ) — &F (Dreal, Dg )|
< 24(Qk=+1 Bf,i)dqkι log(2√dmkι Qk=+1 Bf,i)	8	rlog(1∕δ)
'	√m	√m	V m
Corollary 3.6 shows the advantage of applying spectral weight normalization. We constrain
Qik=1+1 1 BF,i such that it is small enough to force the first term to be small. Ifwe set Qik=1+1 1 BF,i = 1,
the conclusion is reduced to O{y∕d2kι∕m}. Experiments in Miyato et al. (2018) also show that,
spectral weight normalization render the discriminator more powerful for distinguishing between
generated data and real data. Hence, we can suffer less from model collapse. Since d depends on the
largest width of the network, the bound in spectral normalization is not actually width-independent.
For this reason, We prefer to utilize the 'p,q weight normalization to obtain a width-independent
bound.
4 Uniform Generalization Error b ound
We have established the generalization error upper bound with a fixed generator g, that is, g is
independent of the training process and the choice of sample set S . However, such a bound is
not a uniform bound for ∀ g ∈ G. For any fixed g, let S(g) denote the set of samples where the
inequality (1) in Theorem 3.2 holds, i.e., S(g) ，{S | S i^d Dmeaι, bound (1) holds with S}.
We emphasize the dependence of S(g) on g because this set varies as g changes. In other words,
different S (g) values lead to different probability levels, at which the bound holds. Hence, the
probability that (1) holds with S ∈ ∩g∈GS(g) is not guaranteed to be greater than 1 - δ. An upper
bound for the generalization error with a fixed g is tantamount to the generalization error bound
for neural networks in supervised learning. To further understand GANs, it is necessary for us to
establish a uniform bound for generalization error with varying g. The following theorem provides
a generalization bound for GANs.
Theorem 4.1. With a probability ofat least 1 一 2∣X∣∙ exp(-me2∕4) over the choice ofsamples S:
SuP |dF (Dreal, Dg ) - dF (DDreal, Dg )| 6 2Rm,Dreaι (F) + 3
g∈G
where X is a 2^Lq -net of V, and V is the parameter space of G.
Theorem 4.1 depicts the error that is contributed by G. Note that, ifG is complicated, then |X| can be
a large number. In other words, if we reduce the complexity of G by applying weight normalization,
the generalization error bound will consequentially decrease. In fact, weight normalization is an
approach for reducing the complexity of a function class and provides generalization error bound
control. For the 'p,q weight normalization, the next theorem provides an explicit expression of the
generalization error bound.
Corollary 4.2. With a probability of at least 1 一 δ over the choice of samples S:
— — 、 ,^ 一 、，
SuP |dF (Dreal, Dg ) 一 &F (Dreal, Dg )|
g∈G
.9 r	r (2kι+4)	log2, ʊ /表- q]+11^	rC(P)、,	r2PG1	於G	丁 丁 、
62(skι+1v	m +11 CFMdi	do V -mj + 2~m ∖og(6k2LLG),
where cδ satisfies
F2P.
lI---log(6k2LLG),
m
log(1∕δ) 6 (2δ - 1)PG log(6k2LLG) + PG log(cδ
sk1+1 and C(p) are given in theorem 3.3, and PG denotes the number of parameters in g ∈ G.
6
Under review as a conference paper at ICLR 2020
Notice that with the 'p,q weight normalization, We obtain an upper bound for L, LG, in a 'p,q norm
kι + 1	[— I]	[	[— 1 ]
version: L 6 Q cF,iρdip q ,Lg 6 Pi=1ρk2+1-i Qj=↑i ljp* q ^(,j.j. Hence, for an arbi-
i=1
trary weight normalized GAN, we calculate the generalization bound with PG, (F,i and (G,j. For
every δ, we calculate the right hand side of the constraint and pick a feasible and small (δ. Ac-
cording to theorem 4.2, the choice of g contributes to the second term. Since the bounds {(F,i}ik=1 1
for weight normalization can be artificially constrained to a small range, the first two terms can be
constrained to a small value. Since PG is determined by the network structure of g ∈ G and is
usually much larger than k22, the third term is dominate. Similarly, we obtain a width-independent
generalization bound by adopting ReLU functions and applying specific 'p,q weight normalization,
with 1∕p + 1∕q > 1 .
Our result can be extended to cases with spectral weight normalization.
Corollary 4.3. Assume kxk2 6 1 for x ∈ S. With a probability of at least 1 - δ over the choice of
samples S with spectral weight normalization:
k1+1	k	kι+1
SuP |dF(Dreal, Dg) - dF(DDreal, Dg)| 624	Y	Bf^、— log(2√dmkι	Y	BF,i)
g∈G	i=1	m	i=1
8u
+ √m+Cc t
P	k1+1	k2+1
m Iog(6k2 Y BF,i Y Bg,j),
i=1	j=i
where cδ satisfies
2	k1+1	k2+1
log(1∕δ) 6((2 - DPG log(6k2 Y BF,i Y Bg,j)
i=1	j=i
u 2P	k1+1	k2+1
+ PG log(cδt ~mG log(6k2 Y BF,i Y BGj)).
m	i=1	j=i
j=i
5 Numerical Experiments.
In this section, we illustrate some numerical experiments to verify that our generalization error
bound is consistent with numerical studies. We train Wasserstein generative adversarial networks
(WGANs) to learn a three-Gaussian Mixture distribution. The structure of the discriminator is a
three-layer (2× 50 FC)-ReLU-(50 × 50 FC)-ReLU-(50 × 50 FC)-ReLU-(50 × 1 FC) network, where
FC denotes a fully connected layer. The generator is a three-layer (2 × d FC)-ReLU-(d × 50 FC)-
ReLU-(50 × 50 FC)-ReLU-(50 × 2FC) network, where d takes value in {50, 70, 90, 110, 130}.
After the training process, we calculate the generalization error and generalization error bound. The
details of the experimental settings are included in the appendix.
In order to compute the generalization error with an empirical approach, we generate two data sets,
S train and S test , while the sample size of S test is much larger than that of S train . We regard
the empirical distribution over Stest as an approximation of Dreal . The noise is generated from
Z 〜 N (0, I), in the training process.
We compare the generalization error and generalization bound of the WGAN model. We adopt an
empirical approach to calculate dF (Dreal , Dg) and dF (Dreal, Dg). Since theorem 4.2 provides a
computable generalization bound with `2,2 weight normalization, for a trained WGAN, we can cal-
culate the generalization error and generalization bound. For each d ∈ {50, 70, 90, 110, 130}, we
repeat the following process 50 times to compute the generalization error and the average gener-
alization error. At the same time, we calculate the generalization error bound for each d and the
average value. Figure 1 displays two visualizations of the simulated results. The left panel shows a
visibly good generator and the right panel shows a visibly bad generator.
According to the previous discussions, generalization error is defined as |dF (Dreal , Dg) -
\
dF (Dreal, Dg)|. To compute dF (Dreal, Dg), dF (Dreal, Dg), we approximate the distribution dis-
tance dF (Dreal, Dg), dF (Dreal, Dg) by substituting Dreal, Dreal with the uniform distribution over
7
Under review as a conference paper at ICLR 2020
(a) A visibly good generator. (b) A visibly bad generator.
Figure 1:	The blue point cloud represents Strain , a sample from a Gaussian Mixture distribution. The red
points are {g(zi)}m=train ,while {zi}m=train Rd N2(0,I).
6 4 2 0 8
NNNNL
PUnog uo-ωz=al①U①6 ΦTOω⅛><
0.141	0.144	0.147	0.150
Average generalization Error
Figure 2:	Each dot on the graph represents the average generalization error and the generalization bound of
WGAN with d ∈ {50, 70, 90, 110, 130}.
Stest, Strain (denoted as DDtest, Dtrain), respectively. The detailed process is included in the ap-
pendix.
According to figure 2, there is a positive correlation between the generalization error and general-
ization bound. As the number of parameters in the generator increases, the generalization bound
increases. In other words, the experiment verifies that, our bound provides generalization error con-
trol. A low generalization bound guarantees that the generalization error is low, whereas a generator
with a large number of parameters introduces more error.
In fact, by applying 'p,q weight normalization to f, g with small CF,i,cg,j, we control the general-
ization bound so that it remains a small value. The number of parameters in the generator should
not be extremely large. So far, the experiment and our theory have explained how 'p,q weight nor-
malization and the parameter settings of f, g affect the generalization of GANs. Our generalization
bound provides explicit guidance on parameters designing to train GANs with small generalization
error. Thus, we can obtain robust generators.
6 Conclusion
In this paper, we establish the generalization theory for GANs and provide a more reasonable def-
inition of generalization error. We first establish a general bound for generalization error, with a
fixed generator. To further understand GANs, we establish the generalization error bound, which
uniformly holds over any choice of generators. Our numerical experiments on Gaussian Mixture
models verify that, our theory is consistent with the numerical studies. In the Appendix, we also
formulate a novel hypothesis testing procedure to judge whether the generated distribution equals
the distrbution of observed data. Notice that, in these high dimension cases, the ordinary statistical
approaches do not work well. Our hypothesis test is capable of discriminating between good and
bad generators. One interesting future research topic is to develop generalization error bounds for
autoencoder GANs with an additional encoder network.
8
Under review as a conference paper at ICLR 2020
References
Martin Abadi and David G. Andersen. Learning to protect communications with adversarial neu-
ral cryptography. CoRR, abs/1610.06918, 2016. URL http://arxiv.org/abs/1610.
06918.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In Proceedings of the 34th International Conference on
Machine Learning-Volume 70,pp. 224-232. JMLR. org, 2017.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. CoRR, abs/1706.08498, 2017. URL http://arxiv.org/abs/1706.
08498.
Quentin Berthet. Topics in statistical theory.
Hao Chen, Zhanfeng Mo, Zhouwang Yang, and Xiao Wang. Theoretical investigation of gen-
eralization bound for residual networks. In Proceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence, IJCAI-19, pp. 2081-2087. International Joint Con-
ferences on Artificial Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/288. URL
https://doi.org/10.24963/ijcai.2019/288.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neu-
ral networks. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings
of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Re-
search, pp. 297-299. PMLR, 06-09 Jul 2018. URL http://proceedings.mlr.press/
v75/golowich18a.html.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proceedings of the
27th International Conference on Neural Information Processing Systems - Volume 2, NIPS’14,
pp. 2672-2680, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/
citation.cfm?id=2969033.2969125.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. CoRR, abs/1606.03476,
2016. URL http://arxiv.org/abs/1606.03476.
Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On compu-
tation and generalization of generative adversarial networks under spectrum control. In Interna-
tional Conference on Learning Representations, 2019. URL https://openreview.net/
forum?id=rJNH6sAqY7.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jiwei Li, Will Monroe, Tianlin Shi, SebaStien Jean, Alan Ritter, and Dan Jurafsky. Adversarial
learning for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 2157-2169, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1230. URL https:
//www.aclweb.org/anthology/D17-1230.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. CoRR, abs/1802.05957, 2018. URL http://arxiv.org/
abs/1802.05957.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
9
Under review as a conference paper at ICLR 2020
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Generative image
inpainting with contextual attention. CoRR, abs/1801.07892, 2018. URL http://arxiv.
org/abs/1801.07892.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. CoRR, abs/1711.02771, 2017. URL http://arxiv.org/
abs/1711.02771.
A Proof details for main results
Proof of theorem 3.2
Proof.
sup E f(x) - E f ◦ g(z) - sup E f(x) - E f ◦ g(z)
f ∈F X 〜D	Z 〜N	f ∈F X 〜D	Z 〜N
6 sup E f(x) - E f ◦ g(z) - E f(x) - E f ◦ g(z)
f ∈F X〜D	Z〜N	X〜D	Z〜N
= sup E f (x) - E f (x)
f ∈F X 〜D	X〜D
Using the well-known estimation method for the generalization bound in supervised learning Mohri
et al. (2018), with a probability of at least 1 - δ over the choice of samples, we have
E f(x)- E f(x) 6 2Rm,Dreal (F)+^^
〜D	X〜D	V m
sup
f∈F
Proof of corollary 3.3
We adopt Chen et al. (2019) to obtain a tighter and more general bound of the Rademacher Com-
plexity:
Rm(F)
V	, /(2k1 + 4) lθg2	Y+	,[ 备-1 ] +
6skι + iy -----m-------- + 1 c cF,iPdi
i=1
dΓ H
m
Where
k1+ι k1+1	[」1 ]	ɪ k1+1	[ 工 _1
skι + 1 , X ( Π cF,lPdlp* q +) + d0* Π cF,lPdlP* q
i=1	l=i	l=1
C(p) , 2 log(2d0)
P	[min(p* - 1, 2log(2d0))
p = 1,
p> 1 .
□
Proof of theorem 4.1
LemmaA.1.	∀∣∣x∣∣p*	6 1,f ∈	F,	we have	f(x) 6	CF,加+1 Pk= ι	ρk1 + 1-i	Qk‰	d?*	q]+ CF,j.
Where ρ is the Lipschitz constant of the active function.
Proof. Define a series of variables {Z0, Z1, ..., Zk1} as
Z0 = kxkp*
and
Zl+1 = kfl+1 (x)kp* , l = 0, 1, ..., k1 ,
10
Under review as a conference paper at ICLR 2020
where fl is the combination of the first l layers. Then, we prove by induction that for l = 1, ..., k1,
l	l	11
Zl 6 X Pl + 1-iY djkq]+cF,j.
i=1	j=i
When l = 0, we have Zo = IlXkp* 6 1. For l = 1, 2,..., kι,
Zl =kfι(x)kp*
=kσ(Wf,l(X) +bf,l)kp*
6Pdlp* q ]+ (kWf,lkp,q kfl-1(X)kp* + kbf,lkp* )
6Pdlp* q" (CF,lzl-1 + CF,I)
l	l	11
6 X Pl+1-iY djp* q CF ,j
i=1	j=i
Finally, we have
f(X)
=kWf,k1+1fk1 (X)kp*
6kWf,k1+1kp* kfk1(X)kp*
k∖	[ _1_1 ]
6cf,k1+1 X ρk1+1-i Y djp* q CF,j
i=1	j=i
□
Lemma A.2. For generator class G, which is parameterized as the previous discussions, there is a
e/Lq -net X for parameter space V, with respect to the 'p,q parameter norm, such that
log |X | 6 PG log(3k2LG /),
where LG = CG,k2+1 Pk= 1 Pk2+1-i Q=. jp*- 1]+CG,j∙
Proof. For two generators gv , gv0	∈ G, v =	(Mg,k2+1, . . . , Mg,1), v0
(M0g,k2+1,..., M0g,1) ∈ V, we have
|gv(X) - gv0 (X)|
6lg{Mg,k2 + 1,...,Mg,1} - g{M0g,k2 + 1,Mg,k2 …，Mg,i}| +
+ |g{M0g,k2+1,...,M0g,2,Mg,1} - g{M0g,k2+1,...,M0g,1} |
VT k (δMg,k2 + 1)> kp,q	k k (δMgJ)> kp,q
6 Lq----------------------+ …+ LG----------------------
CG,k2+1	CG,1
k2+1
=LG X
i=1
k(∆bg,i , ∆W>i)>kp,q
cG,i
Notice that, we derive the second inequality by applying lemma A.1 on g.
Then, We focus on the CGLL^-nets of set {x ∈ Rli-1×li | ∣x∣p,q 6 cq,.}. According to lemma 1.3 of
Berthet, there exists a CG^ -net Xi thatsatiSfyies log |X.| 6 li-ι(li+1) log(3k2Lq/e). We construct
a e/Lq-net X of V, by taking X，{v = (Mg,k2+1,..., Mg,ι) ∈ V | Mg,. ∈X.,i = 1, ∙∙∙, k2 + 1}.
Hence, we have log |X| 6 PG log(3k2Lq/e).	□
Lemma A.3. For any generator g ∈ G,
IdF(Dreal, Dg) - ES [dF(DDreal, Dg)] ∣ 6 2Rm,Dreaι (F)
11
Under review as a conference paper at ICLR 2020
Proof.
dF (DreaI, Dg ) - ES [dʃ (Threat, Dg )] |
=sup [ E	f (x) - E f O g(z)] - ES sup [ E	f (x) - E f。g(z)]
f ∈F X〜DreaI	Z 〜N	f ∈F X 〜DreaI	Z 〜N
6Es SUP [	E	f (x) -	E f o g(z)] - sup	[	E f (x) - E f。g(z)]
f ∈F X〜DreaI	Z〜N	f ∈F X〜Dreal	Z〜N
≤Es sup [	E	f (x) -	E f o g(z)] - [	E	f (x) - E f。g(z)]
f ∈F	X 〜DreaI	Z〜N	X 〜Dreal	Z〜N
62Rm,Dreal 6
where we denote the noise distribution Nl0 (0,1) as N for short.	□
Lemma A.4. With F, G fixed, the following holds with a probability of at least 1 — 2δ over the
choice of samples S:
sup E sup [ E	f (x) - E f ◦ g(z)] - sup [ E	f (x) - E f ◦ g(z)]
g∈G S f ∈F X 〜Dreal	Z 〜N	f ∈F X 〜Dreal	Z 〜N
6 cδ √2Pg log(3k2LLG)
J	√m
where c$ satisfies
log(1∕δ) 6(C - 1)Pg log(3k2LLG) + PG log(cδ VZc2Pq log(3k?LLg)/m).
Proof. Let Si and Sj be two sample sets from real data, with #Si = #S[ = m. They differ by
exactly one element, which is denoted as Xi ∈ Si and Xi ∈ S1, respectively. For a fixed g, we have
sup [ E f (x) - E f O g(z)] - sup [ E f(x) - E f。g(z)]
f ∈F X 〜D	Z〜N	f ∈F X 〜D0	Z〜N
6 sup [ E f (x) - E f O g(z)] - [ E f (x) - E f。g(z)]
f ∈F X〜D	z~N	X〜D0	z~N
= SUP [ E. f (x) - E. f (x)]
f ∈F X〜D	X 〜D0
= SUPI — f (Xi) - f (Xi)) 1 6 -2-,
f ∈fIm	Im
where D, D0 stands for the uniform distribution over Si, S1. According to McDiarmid,s inequality,
it holds that:
PS ES sup [ E f (x) - E f o g(z)] - sup [ E f (x) - E f。g(z)]
_ f ∈F X〜D	Z 〜N	f ∈F X 〜D	Z 〜N
62 exp(-
€
> 2
where D stands for the uniform distribution over S. By following Lemma A.2, we let X bea ?；Lg -
net of V that satisfies log |X| 6 PG log(6k2LLG/e). Then, by a union bound over all g, whose
parameter belongs to X, we have:
PS sup ES sup [ E f (x) - E f。g(z)] - sup [ E f (x) - E f。g(z)]
.g∈X f∈F X〜D	Z〜N	f∈F X〜D	Z〜N
€
> 2
62 exp(PG log(6k2LLq/e)) exp(-
Take € = cδ√∣(6k2LLg), then the proof is finished.
□
12
Under review as a conference paper at ICLR 2020
Proof of theorem 4.1.
sup |dF (Dreal , Dg) - dF (Dreal , Dg)|
g∈G
6∣dF (Dreal, Dg ) - ES [”F (DreaI, Dg )]| + IES [”F (DreaI, Dg )] - [dF (Dreal, Dg )|
We complete our ultimate proof by combining lemma A.3 and A.4.	□
B Detailed experimental settings
B.1	Data set.
The training data set Strain is a point cloud in R2. We generate a data sample S accordingly:
1.	Set the size of three Gaussian mixtures as {mi}3=ι, (m1,m2, m3)〜PN(m : 3, 3, 3).
2.	Generate three Gaussian mixtures, respectively. S = S1 ∪ S2 ∪ S3, where ∀i, Si denotes
{{xj}jm=ι∣{xj}j=ι i七d N2(μi, ∑i), ∀j, ∣∣Xj ∣∣2 6 10}. We constrain the training data with
∀j, kxj k2 6 10 to simulate the assumption ofS without rejecting too much generated data.
a	τ ,	,1	∙	τ	∙	, 1	1 ∙	I 7 ∕zr> zr> ∖ τ ∕×r∖ zr> ∖ I
According to the previous discussions, the generalization error |dF (Dreal , Dg) - dF (Dreal , Dg)|
depends on Dreal, which is an unknown distribution. In order to compute the generalization error
by an empirical approach, we generate two data sets: Strain, Stest, with #S train #Stest . In
other words, we regard the uniform distribution over Stest as an approximation of Dreal . More
concretely, we generate S train with mtrain = 2 × 104 and S test with mtest = 2 × 105. We
set μ> = (-0.5,-1), μ> = (0.2,0), μ> = (-1,0), ∀ Σi = 0.05 ∙ I2. We generate noise as
Z 〜N (0, I) in the training process.
B.2	Network structure and parameters setting.
The structure of the discriminator is a three-layer(2 × 50 FC)-ReLU-(50 × 50 FC)-ReLU-(50 × 50
FC)-ReLU-(50 × 1 FC) network, where FC denotes a fully connected layer. The generator is a
three-layer(2 × d FC)-ReLU-(d × 50 FC)-ReLU-(50 × 50 FC)-ReLU-(50 × 2 FC) network, where
d takes value in {50, 70, 90, 110, 130}. For the generator, we apply the Xavier initialization with
gain = 1.3 to the weights, and we set the initial bias as 0.1. For the discriminator, we fill the
weights with random numbers from N(0, 0.022) and bias with 0.
We use the optimizer of Adam Algorithm Kingma & Ba (2014) with β = (0, 0.9) for both the
discriminator and generator. The leaning rate of both the discriminator and generator are set to 8e-4.
To apply `2,2 weight normalization to the discriminator, we add a `2,2 penalty, (Qi4=1 ∣Mf,i ∣2,2 -
1)2, to the loss function of WGANs. We train 150 epochs with a batch size of 1000 to train a
generator. Since there is no sufficient stopping criterion , we stop the training when the generated
point cloud visually resembles Strain without further variance.
(a) A visually good generator. (b) A visually bad generator.
Figure 3:	The blue points cloud represents S train , a sample from a Gaussian Mixture distribution. The red
points are {g(zi)}mtrain, while {zi}mrain i七d N2(0, I).
\
The process for computing dF (Dreal , Dg ), dF (Dreal , Dg)
is executed as follows:
13
Under review as a conference paper at ICLR 2020
1.
2.
3.
4.
Train a WGAN over S train and save the parameters of the optimal g.
Train a discriminator f to maximize Ex〜仇”"f (x)] - Ex〜Dg [f (x)] to obtain ftrain.
Train a discriminator f to maximize E^xr^^teSJf (x)] - Ex〜％ [f (x)] to obtain ftest.
\
We calculate dF (Dtrain, Dg) by following section 3.1.1. We obtain dF (Dreal, Dg) by
substituting ’train’ with ’test’.
C A NOVEL HYPOTHESIS TEST FOR H0 : Dreal = Dg .
For a given generator g ∈ G, we are insterested in whether this generator generates real data. To
answer this question, we formulate a hypothesis test.
Since our goal is to provide a hypothesis-testing process to figure out whether Dg = Dreal holds,
at a confidential level 1 - α, we state the null hypothesis as H0 : Dg = Dreal and the alternative
hypothesis as H1 : Dg 6= Dreal . With the distribution distance, we revise the null hypothesis as:
H0 : dF (Dreal , Dg) = 0. Intuitively, in the context of image-generating GANs, a visually bad
generator should give evidence to reject the null hypothesis and embrace the alternative hypothesis.
Assume that H0 holds, we have dF (Dreal, Dg) 6 |dF (Dreal, Dg) - dF (Dreal, Dg)|. According to
corollary 3.5, for a given g ∈ G, with probability at least 1 - α over the choice of S, it holds that:
dF (Dreal, Dg) 6 T, where T denotes the right hand side of corollary 3.5. Hence, dF (Dreal, Dg) >
T provides significant evidence to reject H0 at a 1 - α confidential level. In other words, for a given
g ∈ G, ifdF(Dreal, Dg) > T holds. This means that the artificial data is discriminable and g fails to
adequately fit the real data. Hence, to make a hypothesis testing conclusion, we compare the values
of dF①real, Dg) and T.
C.0.1 Hypothesis testing process.
1.	For a given g ∈ G,we train a discriminator f to maximize Ex〜DFαl [f (x)] - Ex〜Dg [f (x)].
Notice that, training such a discriminator is equivalent to training a WGAN with g fixed.
Hence, we obtain ftrain .
—■—
2.	For i ∈ {1,…，50}, we have dF(Dreal, Dg )i = Ex〜Dreal [ftrain (X)] - Ex〜Dg [ftrain (x)].
In each iterations, we generate an m-size noise sample set from Nl0 (0, I), to feed the
second term. Then, let dF(Dreal, Dg) = Pi=I dF(Dreaι, Dg)"50.
3.	WeSet cf,i as IlMftrain,ikp,q. For a given confidential level 1 - α, by plugging {Cf,i}k== ι
into the right hand side of corollary 3.5, we obtain T.
—■—
4.	If dF (Dreal , Dg ) > T, we reject H0 at a 1 - α level.
The hypothesis testing for H0 is an application of corollary 3.5. Instead of providing a statistic with
an explicit distribution, our hypothesis testing is based on a probabilistic bound for generalization
error with a fixed generator.
Notice that, there are some distribution-free methods to compare two distributions in a purely sta-
tistical way, such as the Kolmogorov-Smirnov test, Shapiro-Wilk test, Mann-Whitney U Test and
Bootstrap Methods. However, most of these methods are heavily based on some strong assump-
tions (i.e., normality, the conditions for central limit theorem and low dimension). Hence, these
methods do not work with some unusual and high dimensional distributions. In high dimensional
cases, corollary 3.5 guarantees that, our novel hypothesis testing has the capacity to discriminate be-
tween high dimensional distributions. Our numerical experiments show that, this hypothesis testing
is commensurate with our observations.
D More experimental results for our hypothesis test
The following experiment illustrates the hypothesis testing process in section 3.1.1. For a specific
generator, we convey the hypothesis process on 100 sample sets {Si}i1=001, where ∀Si is derived from
14
Under review as a conference paper at ICLR 2020
the same distribution of S train . If a generator is visually bad, the hypothesis process is expected to
reject H0 almost 100 times, at a 0.95 confidential level. In these cases, it is highly probable that the
generated distribution is not close to the real distribution. Notice that, even if the hypothesis process
embraces H0 , the generator is not guaranteed to fit the real distribution well. Instead, we should say,
the sample data is not rich enough to provide significant evidence to reject H0 at a high confidential
level. In fact, for a visually good generator, the hypothesis process fails to reject H0 , even with a
rich sample data set.
D.0. 1 HYPOTHESIS TESTING FOR H0 : Dreal = Dg.
0.280-
0.255-
ω 0.230-
g 0.205-
0.180-
0.155-
0.130-

6	25	50	75	100
Experiment order
loss
(b) For the previous visually bad generator.
(a) For the previous visually good generator.
Figure 4:	A representative result of the hypothesis test with the generators in figure 3. In every experiment, we
reject H0 if the training loss is larger than T.
We show a representative result in figure 4. Recall that, the hypothesis testing process reject H0
一'——	—'——	'
♦ r 7 //fʌ	zτ** ∖	、	m ι	c ,	7 /zrʌ	zτ** ∖ “i “	♦	♦	ι	ι m *、	*	*
if dF (Dreal , Dg)	>	T, where	we refer to	dF (Dreal , Dg)	as the training	loss	and T	as the test
statistic for H0 . Figure 4 suggests that, the hypothesis testing process embraces H0 with a visually
good generator, while it reject H0 with a visually bad one, at a 0.95 confidential level. In fact,
our experiments show that the conclusions from the hypothesis testing are commensurate with our
observation. In our toy model, H0 is rejected when the generated data forms a single clique (what
we call Model Collapse) or it obviously disjoints with the real data. Hence, our hypothesis test is
capable of discriminating between good and bad generators, with sufficient sample data.
T	.	1	11	J	1	C	7 ∕zA	八、 £ .	「	，1	,
In our experiments, an extremely small and negative value of gap dF (Dreal , Dg) - T implies that
the generator fits the real data very well and vice versa. For the previous bad generator in figure 3,
the gap is positive but small. This implies that, though the generator fails to fit the real data, it does
not violate the real data grossly.
We display some representative results for hypothesis test in section 4. The experimental settings
are the same as in previous discussions.
D.1 Discrimination
To verify that our hypothesis test shows discrimination in choice of good generators, we conduct
hypothesis test on 79 generators with the same training set Strain and d = 50.
In figure 6 and 7, we juxtapose the hypothesis test conclusion and the corresponding generator.
・	一∙^^'一
ClI ,1	, C	.	.	, L 八	八 S 7	/△	八、、后	7	.1	.	,
Recall that, for a given g, we reject H0 : Dreal = Dg if dF (Dreal, Dg) > T. Numerical experiments
show that, our hypothesis is capable to detect the visually bad generators, at a 0.95 confidential level.
Our hypothesis test is based on a probabilistic inequality, which depends on the artificially designed
sample set Strain and confidential level 1 - α. Thus, for visually bad generators, there may be some
rare acceptances of H0. In these cases, instead of stating ’the hypothesis test provided an incorrect
conclusion’, we should say ’the sample set Strain is not rich enough to provide evidence to reject
H0, at a 1 - α confidential level’. Such an example is included in 7.
D.2 Distribution deviation sensitivity
The numerical experiments also imply that, the hypothesis test can detect how Dg violates Dreal .
We provide an example in figure 5.
15
Under review as a conference paper at ICLR 2020
According to figure 5, the hypothesis test provides stable rejection of H0 with Si, i = 1, . . . , 100.
Moreover, there is an evident gap between the training loss and T. Recall that the gap between the
training loss and T of the bad generator in figure 3 is small and unstable. Obviously, the generator
in figure 5 is much worse than the previous ones. The result suggests that, our hypothesis test is
sensitive to the deviation between Dg, Dreal .
Concretely, for generators that fit the data very well (or very badly), the hypothesis test provides
a training loss which is much larger (or lower) than T ; for generators that perform modestly, the
hypothesis test provides a training loss that is close to T to show its hesitation. In latter cases, to
obtain an unambiguous conclusion, we can choose a richer sample set or set a larger confidential
level.
(a) An extremely bad generator, which is even (b) We convey the hypothesis test with Si , i =
worse than figure 3. Model collapse also occurs. 1, . . . , 100. All the experiments reject H0 at a 0.95 con-
fidential level.
Figure 5:	In such a extreme case, the values of training loss and T are stable.
16
Under review as a conference paper at ICLR 2020
(a) The hypothesis test result. For a fixed g, we reject H0 if the training loss is larger than T.
According to the figure, we reject H0 with the 1, 10, 12, 19, 25, 39 th generators.
(b) 39 generator examples in order. Notice that generators 1, 10, 12, 19, 25 and 39 are visually bad (or even
occur model collapse), which is commensurate with our hypothesis test conclusion.
Figure 6:	In every figure, the blue points cloud represents S train , a sample from a Gaussian Mixture distribu-
tion. Thered points are {g(zi)}mrain, while {zi}mtrain i七d N2(0,I).
17
Under review as a conference paper at ICLR 2020
O 5
.52
0.0.
<Dnra>
(a)	The hypothesis test result. For a fixed g, we reject H0 if the training loss is larger than T.
According to the figure, we reject H0 with the 9, 26 th generators.
(b)	40 generator examples in order. Notice that generator 9, 24 and 26 are visually bad (or even occur model
collapse).
Figure 7:	Though our hypothesis test fails to detect the 24 th generator, it shows that train loss is extremely
closed to T for the 24 th generator. This unusual acceptance is caused by randomness. In this case, the sample
set Strain is not rich enough to reject H0, at a 0.95 confidential level.
18