Under review as a conference paper at ICLR 2020
Stein Self-Repulsive Dynamics: B enefits From
Past Samples
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new Stein self-repulsive dynamics for obtaining diversified sam-
ples from intractable un-normalized distributions. Our idea is to introduce Stein
variational gradient as a repulsive force to push the samples of Langevin dynam-
ics away from the past trajectories. This simple idea allows us to significantly
decrease the auto-correlation in Langevin dynamics and hence increase the ef-
fective sample size. Importantly, as we establish in our theoretical analysis, the
asymptotic stationary distribution remains correct even with the addition of the
repulsive force, thanks to the special properties of the Stein variational gradient.
We perform extensive empirical studies of our new algorithm, showing that our
method yields much higher sample efficiency and better uncertainty estimation
than vanilla Langevin dynamics.
1	Introduction
Drawing samples from complex un-normalized distributions is one of the most basic problems in
statistics and machine learning, with broad applications to enormous research fields that rely on
probabilistic modeling. Over the past decades, large amounts of methods have been proposed for
approximate sampling, including both Markov Chain Monte Carlo (MCMC) (e.g., Brooks et al.,
2011) and variational inference (e.g., Wainwright et al., 2008).
MCMC works by simulating Markov chains whose stationary distributions match the distributions
of interest. Despite nice asymptotic theoretical properties, MCMC is widely criticized for its slow
convergence rate in practice. In difficult problems, the samples drawn from MCMC are often found
to have high auto-correlation across time, meaning that the Markov chains explore very slowly in
the configuration space. When this happens, the samples returned by MCMC only approximate a
small local region, and under-estimate the probability of the regions un-explored by the chain.
Stein variational gradient descent (SVGD) (Liu & Wang, 2016) is another type of approximation
sampling methods designed to overcome the limitation of MCMC. Instead of drawing random sam-
ples sequentially, SVGD evolves a pre-defined number of particles (or sample points) in parallel
with a special interacting particle system to match the distribution of interest by minimizing the KL
divergence. In SVGD, the particles interact with each other to simultaneously move towards the
high probability regions following the gradient direction, and also move away from each other due
to a special repulsive force. As a result, SVGD allows us to obtain diversified samples that correctly
represent the variation of the distribution of interest. SVGD has been found a promising tool for
solving difficult sampling problems in which diversity promotion is critical (e.g., Feng et al., 2017;
Haarnoja et al., 2017; Pu et al., 2017; Liu et al., 2017; Gong et al., 2019). Various extensions have
been developed (e.g., Han & Liu, 2018; Chen et al., 2018; Liu et al., 2019; Wang et al., 2019a).
However, one problem of SVGD is that it theoretically requires to run an infinite number of chains
in parallel in order to approximate the target distribution asymptotically (Liu, 2017). With a finite
number of particles, the fixed point of SVGD does still provide a prioritized, partial approximation
to the distribution in terms of the expectation of a special case of functions (Liu & Wang, 2018).
Nevertheless, it is still desirable to develop a variant of “single-chain SVGD”, which only requires
to run a single chain sequentially like MCMC to achieve the correct stationary distribution asymp-
totically in time, with no need to take the limit of infinite number of parallel particles.
1
Under review as a conference paper at ICLR 2020
In this work, we propose an example of single-chain SVGD by integrating the special repulsive
mechanism of SVGD with gradient-based MCMC such as Langevin dynamics. Our idea is to use
repulsive term of SVGD to enforce the samples in MCMC away from the past samples visited at
previous iterations. Such a new self-repulsive dynamics allows us to decrease the auto-correlation in
MCMC and hence increase the mixing rate, but still obtain the same stationary distribution thanks
to the special property of the SVGD repulsive mechanism.
We provide throughout theoretically analysis of our new method, establishing it asymptotic conver-
gence to the target distribution. As we show in the work, the analysis is highly non-trivial, because
our new self-repulsive dynamics is a non-linear, high-order Markov process. Empirically, we ex-
tensively evaluate our methods on an array of challenging sampling tasks, showing that our method
yields much better uncertainty estimation and larger effective sample size.
2	Background: Langevin dynamics and SVGD
In this section, we give a brief introduction on Langevin dynamics (Rossky et al., 1978) and Stein
Variational Gradient Descent (SVGD) (Liu & Wang, 2016), which we integrate teogehter to develop
our new self-repulsive dynamics for more efficient sampling.
Langevin dynamics Langevin dynamics is a basic gradient based MCMC method. For some
target distribution on Rd with density function ρ* (θ) 8 exp(-V(θ)), where V: Rd → R is the
potential function, the (Euler-discrerized) Langevin dynamics simulates a Markov chain with the
following rule:
θk+ι = θk - ηVV(θk) + p2nek,	ek 〜N(0, I),
where k denotes the number of iterations, {ek} are independent standard Gaussian noise, and η is a
step size parameter. It is well known that the limiting distribution of θk when k → ∞ approximates
the target distribution when η is sufficiently small.
Because the updates in Langevin dynamics are local and incremental, new points generated by the
dynamics is highly correlated to the past sample. As a result, we need to run Langevin dynamics
sufficiently long in order to obtain diverse samples.
Stein Variatinal Gradient Descent (SVGD) Different from Langevin dynamics, SVGD itera-
tively evolves a pre-defined number of particles in parallel. Starting from an initial set of particles
{θ0i : i = 1, ..., M }, SVGD updates the M particles in parallel by
θk+1 = θk+ ηg(θk; δM) ∀i = ι,...,M,
where the velocity field, which we denote by g(θk; $M), depends the empirical distribution of the
current set of particles δM :=吉 P^=ι δθj in the following way,
g(θk； $M) = Eθ〜δM -K(θ, θk)W(θ) + VθK(θ, θk).
k 、---------------------------{z-------} 、------{z----}
Confining Term Repulsive Term
Here δθ is the Dirac measure centered at θ, and hence E®〜§m [∙] denotes averaging on the particles.
k
The K(∙, ∙) is a positive definite kernel, such as the Gaussian RBF kernel, specified by users.
Note that g(θk; δM) consists of a confining term and repulsive term: the confining term pushes
particles to move towards high density region, and the repulsive term prevents the particles from
colliding with each other. It is the balance of these two terms that allows us to asymptotically
approximate the target distribution ρ*(θ) a exp(-V(θ)) at the fixed point, when the number of
particles goes to infinite. We refer the readers to Liu & Wang (2016); Liu (2017); Liu & Wang
(2018) for throughout theoretical justifications of SVGD. But a quick, informal way to justify the
SVGD update is through the Stein’s identity, which shows that the velocity field g(θ; ρ) equals zero
when P equals the true distribution ρ*, that is,
g(θ0; P*)	=	Eθ〜ρ*	[-K(θ, θ0)VV(θ) + V®K(θ,	θ0)]	=	0,	∀θ0	∈	Rd.	(1)
2
Under review as a conference paper at ICLR 2020
This shows that SVGD would converge if the particle distribution already forms a closed approxi-
mation to the target distribution p, meaning that the target distributions forms an (approximate) fixed
point of the update.
3 Stein Self-Repulsive Dynamics
In this work, we propose to integrate Langevin dynamics and SVGD to simultaneously decrease the
auto-correlation of Langevin dynamics and eliminate the need for running parallel chains in SVGD.
The idea is to use Stein repulsive force between the the current sample and the past samples, hence
forming a new self-avoiding dynamics with fast convergence speed.
Specifically, assume we run a single Markov chain like Langevin dynamics, where θk denotes the
particle at the k-th iteration. Denote by EM the empirical distribution of M samples taken from the
past iterations, i.e.,
1M
δM := MEδθk-jcη , Cn = C/η
where cη is a thinning factor, which scales inversely with the step size η, introduced to slim the
sequence of past samples. Compared with the δM in SVGD, which is averaged over M parallel
particles, δEkM is averaged across time over M past samples. Given this, our Stein self-repulsive
dynamics updates the sample via
θk+1 - θk + (-ηV (θk) + P2ηek) + ηαg(θk; δM),	⑵
'---------{z---------}	'------{z----}
Langevin	Stein Repulsive
in which the particle is updated with the typical Langevin gradient, plus a Stein repulsive force
against the samples from the previous iterations. α ≥ 0 is a parameter that controls the magnitude
of the Stein repulsive term. In this way, the particles are pushed away from the past samples, and
hence admits lower auto-correlation and faster convergence speed. Importantly, the addition of the
repulsive force does not impact the asymptotic stationary distribution, thanks to Stein’s identity in
(1). This is because when if the self-repulsive dynamics have converged to the target distribution
ρ*, SUCh that θk 〜ρ* for all k, the Stein self-repulsive term would equal to zero in expectation due
to Stein’s identity and hence does not introduce additional bias over Langevin dynamics. Rigorous
theoretical analysis of this idea is developed in Section 4.
Practical Algorithm Because δEkM is averaged across the past samples, it is necessary to introduce
a burn-in phase with the repulsive dynamics. Therefore, our overall procedure works as follows,
θk+1
(θk - ηVV(θk) + √2ηek,
ɪθk + η h-vV(θk) + αg(θk; EM)i + √2ηek,
k < MCη
k ≥ MCη .
(3)
It includes two phases. The first phase is the same as the Langevin dynamics which collects the
initial M samples used in the second phase while serves as a warm start. The repulsive gradient
update is introduced in the second phase to encourage the dynamics to visit the under-explored
density region. We call this particular instance of our algorithm Self-Repulsive Langevin dynamics
(SRLD), self-repulsive variants of more general dynamics is discussed in Section 5.
Remark Notice that, the first phase is introduced to collect the initial M samples. However, it’s
not really necessary to generate the initial M samples with Langevin dynamics. We can simply use
some other initialization distribution and get M initial samples from that distribution. In practice,
we find using Langevin dynamics to collect the initial samples is natural and it can also be viewed
as the burn-in phase before sampling, so we use (3) in all of the other experiments.
Remark The general idea of introducing self-repulsive terms inside MCMC or other iterative
algorithms is not new itself. For example, in molecular dynamics simulations, an algorithm called
metadynamics (Laio & Parrinello, 2002) has been widely used, in which the particles are repelled
away from the past samples in a way similar to our method, but with a typical repulsive function,
such as Pj D(θk,θk-jj), where D(∙, ∙) is any notation of dis-similarity. However, introducing
3
Under review as a conference paper at ICLR 2020
Figure 1:	An illustrative example showing the advantage of our Self-Repulsive Langevin dynamics.
With a set of initial examples locating on the left part of the target distribution (show in yellow),
Self-Repulsive Langevin dynamics is forced to explore the right part more frequently, yielding an
accurate approximation when combined with the initial samples. Langevin dynamics, however, does
not take the past samples into account and yields a poor overall approximation.
an arbitrary repulsive force would alter the stationary distribution of the dynamics, introducing a
harmful bias into the algorithm. The key highlight of our approach, as reflected by our theoretical
results in Section 4, is the unique property of the Stein repulsive term, that allows us to obtain the
correct stationary distribution even with the addition of the repulsive force.
Remark Recently, (Zhang et al., 2018) proposed a different combination of SVGD and Langevin
dynamics, in which the Langevin force is directly added to a set of particles that evolve in parallel
with SVGD. Using our terminology, their system is
θk +1 = θk + (-ηV(θk) + PVek) + ηαg(θk; δM), ek~N(0,I)	∀i = 1,...,M.
This is significantly different from our method on both motivation and practical algorithm. Their
algorithm still requires to simulate M chains of particles in parallel like SVGD, and was proposed
to obtain easier theoretical analysis than the deterministic dynamics of SVGD. Our work is instead
motivated by the practical need of decreasing the auto-correlation in Langevin dynamics, and avoid-
ing the need of running multiple chains in SVGD, and hence must be based on self-repulsion against
past samples along a single chain.
In another recent work, (Chen et al., 2018) proposed a π-SGLD method, which simulates the lin-
ear combination of the evalutionary partial differential equations of SVGD and Langevin dynamics
using discrete gradient flow with blob-based method. Their method is again motivated by the theo-
retical interest of discovering new categories of algorithms, and does not involve self-repulsive on a
single chain like our method.
An Illustrative Example Here we give an illustrative example to show the key advantage of our
self-repulsive dynamics. Assume that we want to sample from a bi-variate Normal distribution
shown in Figure 1. Unlike standard settings, we assume that we have already obtained some initial
samples (yellow dots in Figure 1) before running the dynamics. The initial samples are assumed
to concentrate on the left part of the target distribution as shown in Figure 1. In this extreme case,
since the left part of the distribution is over-explored by the initial samples, it is desirable to have
the subsequent new samples to concentrate more on the un-explored part of the distribution. How-
ever, standard Langevin dynamics does not take this into account, and hence yielding a bias overall
representation of the true distribution (see the left panel). With our self-repulsive dynamics, the new
4
Under review as a conference paper at ICLR 2020
Practical TheOrem4.3
Dynamics
P
#Particles
M → ∞
Discrete-Time
Mean-Field Limit
0
Theorem 4.2
Step size
η → 0
Continuous-Time
Mean-Field Limit
P
TheOrem 4. Stationary
D Dis tribution
P *
Figure 2:	Roadmap of the theoretical analysis. Theorem 4.3 shows the mean-field limit when M → ∞.
Theorem 4.2 bounds the time discretization error. And Theorem 4.1 shows that the limiting distribution of the
continuous-time mean field dynamics is the target distribution we want.
samples are forced to explore the un-explored region more frequently, allowing us to obtain a much
more accurate approximation when combining the new and initial samples.
4 Theoretical Analysis of S tein Self-Repulsive Dynamics
In this section, we provide theoretical analysis of the self-repulsive dynamics. We establish that
our self-repulsive dynamics converges to the correct target distribution asymptotically, in the limit
when M approaches to infinite and the step size η approaches to 0. This is a highly non-trivial
task, because the self-repulsive dynamics is a highly complex, non-linear and high order Markov
stochastic process. We attack this problem by breaking the proof into the following three steps
illustrated in Figure 2:
1)	At the limit of large particle sizes M → ∞ (called the mean field limit), we show that practical
dynamics in (3) is closely approximated by a discrete-time mean-field dynamics characterized by
(4) below.
2)	By further taking the limit of small step size η → 0+ (called the continuous-time limit), the
dynamics in (4) converges to a continuous-time mean-field dynamics characterized by (5) below.
3)	We show that the dynamics in (5) converges to the target distribution.
Remark As we mentioned in Section 3, we introduce the first phase to collect the initial M sam-
ples for the second phase, and our theoretical analysis follows this setting to make our theory as
close to the practice. However, the theoretical analysis can be generalized to the setting of drawing
M initial samples from some initialization distribution with almost identical argument.
Notations We use ∣∣∙k and(•, •)to represent the '2 vector norm and inner product respectively. The
Lipschitz norm and bounded Lipschitz norm of a function f are defined by kf kLip and kfkBL. The
KL divergence, Wasserstein-2 distance and Bounded Lipschitz distance between distribution ρ1, ρ2
are denoted as DKL[ρ1 ∣ ρ2], W2[ρ1, ρ2] andDBL[ρ1,ρ2] respectively.
4.1	Mean-Field and Continuous-Time Limits
To fix the notation, we denote by ρk := Law(θk) the distribution of θk at time k of the practical
self-repulsive dynamics (3), which we refer as the practical dynamics in the sequel, when the initial
particle θ0 is drawn from an initial continuous distribution ρ0 supported on Rd. Note that given ρ0,
the subsequent ρk can be recursively defined through dynamics (3). Due to the diffusion noise in
Langevin dynamics, all ρk are continuous distributions supported on Rd . We now introduce the
limit dynamics when we take the mean-field limit (M → +∞) and then the continuous-time limit
(η → 0+).
Discrete-Time Mean-Field Dynamics (M → +∞) In the limit of M → ∞, we show that
our practical dynamics is closely approximated by the following dynamics, in which the empirical
measures are replaced by the continuous distributions of the particles themselves.
θk+1
where PM = M
(θk - ηVV(θk) + √2ηek,	k ≤ Mcn
jθk + η h-VV(θk) + αg(θk, PM)i + √2ηek, k ≥ Mcn
PM=I Pk-jcn by Pk ：= Law(θk) the (smooth) distribution of θk at time-step
~,
(4)
k when the dynamics is initialized with θo 〜ρo = ρ0. Compared with the practical dynamics
5
Under review as a conference paper at ICLR 2020
in (3), the difference is that the empirical distribution δM is replaced by the smooth distribution
PM. Similar to the recursive definition of Pk following dynamics (3), Pk is also recursively defined
through dynamics (4), starting from po = ρo. As We show in Theorem 4.3, if the auto-correlation
of θk decays fast enough and M is sufficiently large, PM is well approximated by the empirical
distribution EM in (3), and further the two dynamics ((3) and (4)) converges to each other in the
sense that W2 [pk ,pk] → 0 as M → ∞ for any k. Note that in taking the limit of M, we need
to ensure that we run the dynamics for more than Mcη steps. Otherwise, SRLD degenerates to
Langevin dynamics as we stop the chain before we collect M samples.
Continuous-Time Mean-Field Dynamics (η → 0+) In the limit of zero step size (η → 0+), the
discrete-time mean field dynamics in (4) can be shown to converge to the following continuous-time
mean-field dynamics:
dθ = f-VV(θt)dt + dBt,	t ∈ [0, Mc)
t = I [-VV(θt) + αg(θk, pM)] dt + dBt, t ≥ Mc,
(5)
where PM := M PjM=I pt-jc(∙) and pt = LaW (θt) is the distribution of θt at a continuous time
point t with θo initialized by θo 〜po = po. We prove that (5) is closely approximated by (4) with
small step size in the sense that DKL [Pk k Pkn] → 0 as η → 0 in Theorem 4.2, and importantly, the
stationary distribution of (5) equals to the target distribution p*(θ) a exp(-V(θ)).
4.2	Assumptions
We first introduce the techinical assumptions used in our theoretical analysis.
Assumption 4.1. (RBF Kernel)
We use RBF kernel i.e. K(θ1, θ2) = exp(- kθ1 - θ2 k2 /σ) for some fixed 0 < σ < ∞.
We only assume the RBF kernel for the simplicity of our analysis. However, it is straightforward to
generalize our theory to other positive definite kernels.
Assumption 4.2. (V is dissipative and smooth)
Assume that hθ, -VV(θ)i ≤ b1 - a1 kθk2 and kVV(θ1) - VV (θ2)k ≤ b1 kθ1 -θ2k. We also
assume that kVV (0)k ≤ b1. Here a1 and b1 are some finite positive constant.
Assumption 4.3. (Regularity Condition)
Assume Eθ〜ρ° [∣∣θ∣k2] < 0. DefinePM = PM=I Pk-jcη/M, assume there exists B < ∞ such that
inf sup E g(θ; δEkM) - g(θ; PkM)2 > 0.
k≥Mcη kθk≤B
Assumption 4.4. (Strong-convexity)
Suppose that hVV(θ1) - VV(θ2), θ1 - θ2i ≥ L kθ1 - θ2k2 for a positive constant L.
Remark Assumption 4.2 is standard in the existing Langevin dynamics analysis (see Dalalyan
(2017); Raginsky et al. (2017) for example). Assumption 4.3 is a weak condition as it assumes that
the dynamics can not degenerate into one local mode and stop moving anymore. This assumption
is expected to be true when we have diffusion terms like the Gaussian noises in our self-repulsive
dynamics. Assumption 4.4 is a classical assumption on the existing Langevin dynamics analysis
with convex potential Dalalyan (2017); Durmus et al. (2019). Although being a bit strong, this
assumption broadly applies to posterior inference problem in the limit of big data, as the posterior
distribution converges to Gaussian distributions for large training set as shown by Bernstein-von
Mises theorem. It is technically possible to further generalize our results to the non-convex settings
with a refined analysis, which we leave as future work. This work focuses on the classic convex
setting for simplicity.
6
Under review as a conference paper at ICLR 2020
4.3	Main Theorems
All of the proof in this section can be found in Appendix B.5.
We first prove that the limiting distribution of the continuous-time mean field dynamics (5) is the
target distribution. This is achieved by writing dynamics (5) into the following (non-linear) partial
differential equation:
∂ . = ∫V∙ (-VVρt) + ∆ρt	t ∈ [0, Mc)
tpt= [v∙ [(-VV + αg(∙,ρM)) ρt] +∆ρt, t ≥ Mc.
Theorem 4.1.	(Stationary Distribution)
Given some finite M, c and α, and suppose that the limiting distribution of dynamics (5) exists. Then
the limit distribution is unique, and equals to ρ*(θ) H exp(-V(θ)).
We then give the upper bound on the discretization error, which can be characterized by analyzing
the KL divergence between Pk and pkη.
Theorem 4.2.	(Time Discretization Error)
Given some sufficiently small step size η and choose α < a2∕(2b1 + 4∕σ). Under assumption 4.1,
4.2, 4.3 and c” = c∕η. we havefor some constant C,
O O	(η + kη2)	k ≤ Mcn — 1
ι∈maxk} DKL [ρln	k ρl]	≤ jo	(η + Mcη	+ α2MceCa(kη-Mc)η2)	k ≥ Mcn.
With this theorem, we can know that if η is small enough, then the discretization error is small and
ρ approximates P closely. Next we give result on the mean field limit of M → ∞.
Theorem 4.3.	(Mean-Field Limit)
Under assumption 4.1, 4.2, 4.3, and 4.4, suppose that we choose α andη such that -(a1 -2αb1∕σ)+
ηbι < 0; 2αn(bi + 1) < 1；。2 — α(2b1 + 4) > 0; Then there exists a constant c?, such that when
L∕a ≥ c2 and we have
W2[ρk,Pk] = O (α2∕M + η2)	for k ≥ MCn,
and W2[ρk, Pk ] = 0 if k ≤ MCn 一 L
Remark Combine all the result, We have limk,M→∞,η→0+ DBL [ρk, ρ*] = 0, where thejoint limit
requires kη → ∞, exp(Ca2kη)η2 = o(1) and M = Y (1 + o(1)), with γ > 1. Notice that if
γ ≤ 1, the dynamics is degenerated to Langevin dynamics.
5 Extension to General Dynamics
Although we have focused on self-repulsive Langevin dynamics, our Stein self-repulsive idea can
be broadly combined with general gradient-based MCMC. Following Ma et al. (2015), we consider
the following general class of sampling dynamics for drawing samples from p(θ) H exp(-V(θ)):
dθt = —f (θ)dt + √2D(θ)dBt,
withf(θ) = [D(θ) + Q(θ)]VV (θ) - Γ(θ),
d∂
Γi(θ)=∑ 呵(Dij(θ)+ Qij(θ)).
where D is a positive semi-definite diffusion matrix that determines the strength of the Brownian
motion and Q is a skew-symmetric curl matrix that can represent the traverse effect (e.g. in Neal
et al., 2011; Ding et al., 2014). By adding the Stein repulsive force, we obtain the following general
self-repulsive dynamics
dθt
—f (θ)dt + √2D(θ)dBt,	t ∈ [0, MC)
—(f (θ) + αg(θt; PM)) dt + dBt,	t ≥ Mc
(6)
7
Under review as a conference paper at ICLR 2020
Figure 3: Sample quality of SRLD and Langevin dynamics for sampling the correlated 2D distribu-
tion. The auto-correlation is the averaged auto-correlation of the two dimensions.
where pt := Law(θt) is again the distribution of θt following (6) when initalized at θo 〜po. Similar
to the case of Langevin dynamics, this process also converges to the correct target distribution, and
can be simulated by practical dynamics similar to (3).
Theorem 5.1. (Stationary Distribution)
Given some finite M, c and α, and suppose that the limiting distribution of dynamics (6) exists. Then
the limiting distribution is unique and equals the target distribution p*(θ) H exp(-V(θ)).
6	Experiments
In this section, we evaluate the proposed method in various challenging problems, including sam-
pling the posteriors of Bayesian Neural Networks, and uncertainty estimation in Reinforcement
Learning. Our results show that our Self-Repulsive Langevin dynamics (SRLD) yields much higher
sample efficiency than vanilla Langevin dynamics. Our code is available along the submission.
6.1	Synthetic Experiment
A Correlated 2D Distribution This experiment aims to show how the repulsive gradient helps
explore the whole distribution. Following Ma et al. (2015), we compare the sampling efficiency on
the following correlated 2D distribution with density
p*([θ1,θ2]) H -θ4∕ιo - (4(θ2 +1.2)- θ2)2/2.
We compare the SRLD with vanilla Langevin dynamics, and evaluate the sample quality by Maxi-
mum Mean Discrepancy (MMD) (Gretton et al., 2012), Wasserstein-1 Distance and effective sample
size (ESS). Notice that the finite sample quality of gradient based MCMC method is highly related
to the step size. Compared with Langevin dynamics, we have an extra repulsive gradient and thus
we implicitly have larger step size. To rule out this effect, we set different step sizes of the two dy-
namics so that the gradient of the two dynamics has the same magnitude. In addition, to decrease the
influence of random noise, the two dynamics are set to have the same initialization and use the same
sequence of Gaussian noise. We collect the sample of every iteration. We repeat the experiment 20
times with different initialization and sequence of Gaussian noise.
Figure 3 summarizes the result with different metrics. We can see that SRLD have a signifi-
cantly smaller MMD and Wasserstein-1 Distance as well as a larger ESS compared with the vanilla
Langevin dynamics. Moreover, the introduced repulsive gradient creates a negative auto-correlation
between samples. Figure 4 shows a typical trajectory of the two sampling dynamics. We can see that
SRLD have a faster mixing rate than vanilla Langevin dynamics. Note that since we use the same
sequence of Gaussian noise for both algorithms, the difference is mainly due to the use of repulsive
gradient rather than the randomness.
Mixture of Gaussian Distribution We aim to show how the repulsive gradient helps the particle
escape from the local high density region by sampling the 2D mixture of Gaussian distribution using
SRLD and Langevin dynamics. The target density is set to be
P*(θ) H 1 exp (-∣∣θ - 1k2 /2)+ 1 exp (-∣∣θ + 1『/2), θ = [θ1,θ2]>,
8
Under review as a conference paper at ICLR 2020
Num of samples = 50
Num of samples = 100	Num of samples = 150	Num of samples = 200	Num of samples = 250
MMD
Num of samples
Figure 4: Sampling trajectory of the correlated 2D distribution.
Num of samples	Num of samples	Num of lags
Figure 5:	Sample quality and autocorrelation of the mixture distribution. The auto-correlation is the
averaged auto-correlation of the two dimensions.
where 1 = [1, 1]>. This target distribution have two mode at -1 and 1, and vanilla Langevin
dynamics can stuck in one mode while keeps the another mode under-explored (as the gradient
of energy function can dominate the update of samples). We use the same evaluation method, step
sizes, initialization and Gaussian noise as the previous experiment. We collect one sample every 100
iterations and the experiment is repeated for 20 times. Figure 5 shows that SRLD again consistently
outperforms the Langevin dynamics on all of the evaluation metrics.
6.2	Bayesian Neural Network
Bayesian Neural Network is one of the most important methods in Bayesian Deep Learning with
wide application in practice. Here we test the performance of SRLD on sampling the posterior of
Bayesian Neural Network on the UCI datasets (Dua & Graff, 2017). We assume the output is normal
distributed, with a two-layer neural network with 50 hidden units and tanh activation to predict the
mean of outputs. We set a Γ(1, 0.1) prior for the inverse output variance. All of the datasets are
randomly partitioned into 90% for training and 10% for testing. The results are averaged over 20
random trials. We set the mini-batch size to be 100 and the number of past samples M to be 10.
In all experiments, we use RBF kernel with bandwidth set by the median trick as suggested in Liu
& Wang (2016). For SVGD, we use the original implementation with 20 particles by Liu & Wang
(2016). We run 50000 iterations for each methods, and for LD and SRLD, the first 40000 iteration
is discarded as burn-in. We use a thinning factor of 5 = c/n = 100 and in total We collect 100
samples from the posterior distribution. For each dataset, we generate 3 extra data splits for tuning
the step size for each method.
Table 1 shoWs the average test RMSE and test log-likelihood and their standard deviation. The
method that has the best average performance is marked as boldface. We observe that a large portion
of the variance is due to the random partition of the dataset. Therefore, to shoW the statistical
significance, We use the matched pair t-test to test the statistical significance, mark the methods that
perform the best With a significance level of 0.05 With underlines. Note that the results of SRLD/LD
and SVGD is not very comparable, because SRLD/LD are single chain methods Which averages
across time, and SVGD is a multi-chain method that only use the results of the last iteration. We
9
Under review as a conference paper at ICLR 2020
Dataset	SVGD	Ave Test RMSE LD	SRLD	SVGD	Ave Test LL LD	SRLD
Boston	3.300 ± 0.142	3.342 ± 0.187	3.086 ± 0.181	-4.276 ± 0.217	-2.678 ± 0.092	-2.500 ± 0.054
Concrete	4.994 ± 0.171	4.908 ± 0.113	4.886 ± 0.108	-5.500 ± 0.398	-3.055 ± 0.035	-3.034 ± 0.031
Energy	0.428 ± 0.016	0.412 ± 0.016	0.395 ± 0.016	-0.781 ± 0.094	-0.543 ± 0.014	-0.476 ± 0.036
Naval	0.006 ± 0.000	0.006 ± 0.002	0.003 ± 0.000	3.056 ± 0.034	4.041 ± 0.030	4.186 ± 0.015
WineRed	0.655 ± 0.008	0.649 ± 0.009	0.639 ± 0.009	-1.040 ± 0.018	-1.004 ± 0.019	-0.970 ± 0.016
WineWhite	0.655 ± 0.008	0.692 ± 0.003	0.688 ± 0.003	-1.040 ± 0.019	-1.047 ± 0.004	-1.043 ± 0.004
Yacht	0.593 ± 0.071	0.597 ± 0.051	0.578 ± 0.054	-1.281 ± 0.279	-1.187 ± 0.307	-0.458 ± 0.036
Table 1: Averaged test RMSE and test log-likelihood on UCI datasets. Results are averaged over 20
trails. The boldface indicates the method has the best average performance and the underline marks
the methods that perform the best with a significance level of 0.05.
provide additional results in appendix that SRLD averaged on 20 particles (across time) can also
achieve similar or better results as SVGD with 20 (parallel) particles.
6.3	Contextual Bandits
We evaluate the quality of uncertainty estimation provided by our methods on several contextual
bandits problems. Uncertainty estimation is a key component of contextual bandits. If the agent
makes decisions with a poorly estimated uncertainty, the decisions will finally lead to catastrophic
failure through the feedback loops (Riquelme et al., 2018).
Though in principle all of the MCMC methods return the samples follow the true posterior ifwe can
run infinite MCMC steps, in practice we can only obtain finite samples as we only have finite time
to run the MCMC sampler. In this case, the auto-correlation issue can lead to the under-estimate
the uncertainty, which will cause the failure on all of the reinforcement learning problems that need
exploration.
We consider the posterior sampling (a.k.a Thompson sampling) algorithm with Bayesian neural
network as the function approximator. We follows the experimental setting from Riquelme et al.
(2018). The only difference is that we change the optimization of the objective (e.g. evidence lower
bound (ELBO) in variational inference methods) into running MCMC samplers. We set the step of
samplers equal to the optimization step, and use a thinning factor of 100. We compare the SRLD
with the Langevin dynamics on the Mushroom and Wheel bandits from (Riquelme et al., 2018), and
include SVGD as a baseline. For more detailed introduction and setup, see Appendix A.5.
The cumulative regret is shown in Table 2. SVGD is known to have the under-estimated uncertainty
for Bayesian neural network if particle number is limited (Wang et al., 2019b), and as a result, has
the worst performance among the three methods. SRLD is slightly better than vanilla Langevin
dynamics on the simple Mushroom bandits. On the much more harder Wheel bandits, SRLD is
significantly better than the vanilla Langevin dynamics, which shows the improving uncertainty
estimation of our methods within finite number of samples.
Dataset	SVGD	LD	SRLD
Mushroom	20.7 ± 2.0	4.28 ± 0.09	3.80 ± 0.16~~
Wheel	91.32 ± 0.17	38.07 ± 1.11	32.08 ± 0.75
Table 2: Cumulative Regrets on two bandits problem. Results are averaged over 10 trails. Boldface
indicates the methods with best performance and underline marks the best significant methods with
significant level 0.05.
7 Conclusion
We propose a Stein self-repulsive dynamics which applies Stein variational gradient to push samples
from MCMC dynamics away from its past trajectories. This allows us to significantly decrease the
auto-correlation of MCMC, increasing the sample efficiency for better estimation. The advantages
of our method are extensive studied both theoretical and empirical analysis in our work. In future
work, we plan to investigate the combination of our Stein self-repulsive idea with more general
MCMC procedures, and explore broader applications.
10
Under review as a conference paper at ICLR 2020
References
HZ An and FC Huang. The geometrical ergodicity of nonlinear autoregressive models. Statistica
Sinica,pp. 943-956,1996.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
in neural network. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pp. 1613-1622, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.
press/v37/blundell15.html.
Richard Bradley. Basic properties of strong mixing conditions. a survey and some open questions.
Probability Surveys, 2:107-144, 2005.
Richard C. Bradley. Introduction to strong mixing conditions. 2007.
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov chain Monte
Carlo. CRC press, 2011.
Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and TrendsR in Machine Learning, 5(1):1-122, 2012.
Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in
neural information processing systems, pp. 2249-2257, 2011.
Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified particle-
optimization framework for scalable Bayesian sampling. In Proceedings of the Thirty-Fourth
Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, Au-
gust 6-10, 2018, pp. 746-755, 2018.
Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):
651-676, 2017.
Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D. Skeel, and Hartmut Neven.
Bayesian sampling using stochastic gradient thermostats. In Advances in Neural Information
Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014,
December 8-13 2014, Montreal, Quebec, Canada, pp. 3203-3211, 2014.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Alain Durmus, Szymon Majewski, and Blazej Miasojedow. Analysis of langevin monte carlo via
convex optimization. Journal of Machine Learning Research, 20(73):1-46, 2019.
Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized stein variational
gradient descent. uncertainty in artificial intelligence, 2017.
Chengyue Gong, Jian Peng, and Qiang Liu. Quantile Stein variational gradient descent for batch
Bayesian optimization. In International Conference on Machine Learning, pp. 2347-2356, 2019.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. international conference on machine learning, pp. 1352-1361, 2017.
Jun Han and Qiang Liu. Stein variational gradient descent without gradient. In Uncertainty in
Artificial Intelligence, 2018.
Alessandro Laio and Michele Parrinello. Escaping free-energy minima. Proceedings of the National
Academy of Sciences, 99(20):12562-12566, 2002.
11
Under review as a conference paper at ICLR 2020
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and acceler-
ating particle-based variational inference. In International Conference on Machine Learning, pp.
4082-4092, 2019.
Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in neural information
processing systems, pp. 3118-3126, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances In Neural Information Processing Systems, pp. 2378-2386, 2016.
Qiang Liu and Dilin Wang. Stein variational gradient descent as moment matching. In Advances in
Neural Information Processing Systems, pp. 8854-8863, 2018.
Yang Liu, Qiang Liu, and Jian Peng. Stein variational policy gradient. uncertainty in artificial
intelligence, 2017.
Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc. In
Advances in Neural Information Processing Systems, pp. 2917-2925, 2015.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,
2(11):2, 2011.
Yunchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, and Lawrence Carin. Vae learn-
ing via stein variational gradient descent. neural information processing systems, pp. 4236-4245,
2017.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. In Proceedings of the 30th Conference
on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pp. 1674-1703,
2017. URL http://proceedings.mlr.press/v65/raginsky17a.html.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empir-
ical comparison of bayesian deep networks for thompson sampling. In 6th International Con-
ference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?
id=SyYe6k-CW.
Peter J Rossky, JD Doll, and HL Friedman. Brownian dynamics as smart monte carlo simulation.
The Journal of Chemical Physics, 69(10):4628-4633, 1978.
Jeff Schlimmer. Mushroom records drawn from the audubon society field guide to north american
mushrooms. GH Lincoff (Pres), New York, 1981.
William R. Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and TrendsR in Machine Learning, 1(1-2):1-305, 2008.
Dilin Wang, Ziyang Tang, Chandrajit Bajaj, and Qiang Liu. Stein variational gradient descent with
matrix-valued kernels. In Conference on Neural Information Processing Systems, 2019a.
Ziyu Wang, Tongzheng Ren, Jun Zhu, and Bo Zhang. Function space particle optimization for
bayesian neural networks. arXiv preprint arXiv:1902.09754, 2019b.
Jianyi Zhang, Ruiyi Zhang, and Changyou Chen. Stochastic particle-optimization sampling and the
non-asymptotic convergence theory. arXiv preprint arXiv:1809.01293, 2018.
12
Under review as a conference paper at ICLR 2020
A Experiment Details and Additional Experiment Result
A.1 Synthetic 2D Mixture of Gaussian Experiment
' 一
' 一
' 一

Repulsive Dynamics
丁
丁
ɪ





Langevin Dynamics
Num of samples = 100	Num of samples = 150	Num of samples : 200 Num of samples = 250
-	I	,
Num of samples = 300

Figure 6:	Sampling trajectory of the mixture of Gaussian.
To provide more evidence on the effectiveness of SRLD on escaping from local high density region,
we plot the sampling trajectory of SRLD and vanilla Langevin dynamics on the mixture of Gaussian
mentioned in Section 6.1. We can find that, when both of the methods obtain 200 samples, SRLD
have started to explore the second mode, while vanilla Langevin dynamics still stuck in the original
mode. When both of the methods have 250 examples, the vanilla Langevin dynamics just start to
explore the second mode, while our SRLD have already obtained several samples from the second
mode, which shows our methods effectiveness on escaping the local mode.
A.2 Synthetic higher dimensional Gaussian Experiment
To show the performance of SRLD in higher dimensional case with different value ofα, we addition-
ally considering the problem on sampling from Gaussian distribution with d = 100 and covariance
Σ = 0.5I. We run SRLD with α = 100, 50, 20, 10, 0 and the case α = 0 reduces to Langevin. We
collect 1 sample every 10 iterations. The other experiment setting is the same as the toy examples
in the main text. The results are summarized at Figure [7]. In this experiment, we set one SRLD
with an inappropriate α = 100. For this chain, the repulsive gradient gives strong repulsive force
and thus has the largest ESS and the fastest decay of autocorrelation. While the inappropriate value
α induces too much extra approximation error and thus its performance is not as good as these with
smaller α (see MMD and Wasserstein distance). This phenomenon matches our theoretical finding.
MMD
Num of samples
Wasserstein Distance
Num of samples
ESS
Num of lags
Figure 7:	Sample quality and autocorrelation of the higher dimensional Gaussian distribution. The
auto-correlation is the averaged auto-correlation of all dimensions.
13
Under review as a conference paper at ICLR 2020
MMD
Num of samples
Figure 8: Sample quality and autocorrelation of the higher dimensional mixture distribution. The
auto-correlation is the averaged auto-correlation of all dimensions.
A.3 Synthetic higher dimensional Mixture of Gaussian Experiment
We also consider sampling from the mixture of Gaussian with d = 20. The target density is set to
be
ρ*(θ) H 2 exp (—0.5 ∣∣θ — p2/d1 U ) + 2 exp (-0.5 ∣∣θ + p2∕d11∣
where θ = [θ1, ..., θ20]> and 1 = [1, ..., 1]>. And thus the mean of the two mixture component
is with distance 2√2. We run SRLD with α = 20,10,5,0 (and when α = 0, it reduces to LD).
The other experiment setting is the same as the low dimensional mixture Gaussian case. Figure [8]
summarizes the result. As shown in the figure, when α becomes larger, the repulsive forces helps
the sampler better explore the density region.
A.4 UCI Datasets
We show some additional experiment result on posterior inference on UCI datasets. As mentioned
in Section 6.2, the comparison between SVGD and SRLD is not direct as SVGD is a multiple-chain
method with fewer particles and SRLD is a single chain method with more samples. To show more
detailed comparison, we compare the SVGD with SRLD using the first 20, 40, 60, 80 and 100
samples, denoted as SRLD-n where n is the number of samples used. Table 3 shows the result of
averaged test RMSE and table 4 shows the result of averaged test loglikelihood. For SRLD with
different number of samples, the value is set to be boldface if it has better average performance than
SVGD. If it is statistical significant with significant level 0.05 using a matched pair t-test, we add an
underline on it.
Figure 9 and 10 give some visualized result on the comparison with Langevin dynamics and SRLD.
To rule out the variance of different splitting on the dataset, the errorbar is calculated based on the
difference between RMSE of SRLD and RMSE of Langevin dynamcis in 20 repeats (And similarily
for test log-likelihood). And we only applied the error bar on Langevin dynamics.
Dataset
SRLD-20
SRLD-40
AVe Test RMSE
SRLD-60	SRLD-80
SRLD-100
SVGD
Boston
Concrete
Energy
NaVal
WineRed
WineWhite
Yacht
3.236 ± 0.174
4.959 ± 0.109
0.422 ± 0.016
0.005 ± 0.001
0.654 ± 0.009
0.695 ± 0.003
0.616 ± 0.055
3.173 ± 0.176
4.921 ± 0.111
0.409 ± 0.016
0.004 ± 0.000
0.647 ± 0.009
0.692 ± 0.003
0.608 ± 0.052
3.130 ± 0.173
4.906 ± 0.109
0.405 ± 0.016
0.003 ± 0.000
0.644 ± 0.009
0.690 ± 0.003
0.597 ± 0.051
3.101 ± 0.179
4.891 ± 0.108
0.399 ± 0.016
0.003 ± 0.000
0.641 ± 0.009
0.689 ± 0.002
0.587 ± 0.054
3.086 ± 0.181
4.886 ± 0.108
0.395 ± 0.016
0.003 ± 0.000
0.639 ± 0.009
0.688 ± 0.003
0.578 ± 0.054
3.300 ± 0.142
4.994 ± 0.171
0.428 ± 0.016
0.006 ± 0.000
0.655 ± 0.008
0.655 ± 0.008
0.593 ± 0.071
Table 3: Comparing SRLD with different number of samples with SVGD on test RMSE. The results
are computed oVer 20 trials. For SRLD, the Value is set to be boldface if it has better aVerage
performance than SVGD. The Value if with underline if it is significantly better than SVGD with
significant leVel 0.05 using a matched pair t-test.
14
Under review as a conference paper at ICLR 2020
Dataset	SRLD-20	AVe TestLL				SVGD
		SRLD-40	SRLD-60	SRLD-80	SRLD-100	
Boston	-2.642 ± .088	-2.582 ± 0.084	-2.527 ± 0.612	-2.516 ± 0.062	-2.500 ± 0.054	-4.276 ± 0.217
Concrete	-3.084 ± 0.036	-3.061 ± 0.034	-3.050 ± 0.033	-3.040 ± 0.031	-3.034 ± 0.031	-5.500 ± 0.398
Energy	-0.580 ± 0.053	-0.536 ± 0.048	-0.522 ± 0.046	-0.504 ± 0.044	-0.476 ± 0.036	-0.781 ± 0.094
Naval	4.033 ± 0.230	4.100 ± 0.171	4.140 ± 0.015	4.167 ± 0.014	4.186 ± 0.015	3.056 ± 0.034
WineRed	-1.008 ± 0.019	-0.990 ± 0.017	-0.982 ± 0.016	-0.974 ± 0.016	-0.970 ± 0.016	-1.040 ± 0.018
WineWhite	-1.053 ± 0.004	-1.049 ± 0.004	-1.047 ± 0.004	-1.044 ± 0.004	-1.043 ± 0.004	-1.040 ± 0.019
Yacht	-1.160 ± 0.256	-0.650 ± 0.173	-0.556 ± 0.096	-0.465 ± 0.037	-0.458 ± 0.036	-1.281 ± 0.279
Table 4: Comparing SRLD with different number of samples with SVGD on test log-likelihood.
The results are computed over 20 trials. For SRLD, the value is set to be boldface if it has better
average performance than SVGD. The value if with underline if it is significantly better than SVGD
with significant level 0.05 using a matched pair t-test.
0.75
0.750
0.69
0.725
0.74
WineRed
0.68
0.67
0.66
0.65
0.64
0	20	40	60	80 100
Num of samples
WineWhite
0.73
0.72
0.71
0.70
0.69
0.575-1 ι
80 100
Num of samples
Yacht
0.700
0.675
0.650
0.625
0.600
0	20 40 60 80 100
Num of samples
Figure 9:	Comparison between SRLD and Langevin dynamics on test RMSE. The results are com-
puted based on 20 repeats. The error bar is calculated based on RMSE of SRLD - RMSE of Langevin
dynamics in 20 repeats to rule out the variance of different data splitting
A.5 Contextual Bandit
Contextual bandit is a class of online learning problems that can be viewed as a simple reinforcement
learning problem without transition. For a completely understanding of contextual bandit problems,
we refer the readers to the Chapter 4 of (Bubeck et al., 2012). Here we include the main idea for
completeness. In contextual bandit problems, the agent needs to find out the best action given some
observed context (a.k.a the optimal policy in reinforcement learning). Formally, we define S as
the context set and K as the number of action. Then we can concretely describe the contextual
bandit problems as follows: for each time-step t = 1,2, ∙∙∙ ,N, where N is some pre-defined time
horizon (and can be given to the agent), the environment provides a context st ∈ S to the agent,
then the agent should choose one action at ∈ {1,2,…，K} based on context st. The environment
will return a (stochastic) reward r(st, at) to the agent based on the context st and the action at that
similar to the reinforcement learning setting. And notice that, the agent can adjust the strategy at
each time-step, so that this kinds of problems are called “online” learning problem.
Solving the contextual bandit problems is equivalent to find some algorithms that can minimize the
pseudo-regret (Bubeck et al., 2012), which is defined as:
RN
max E
mS→{1,2,…，K}
NN
r(st, g(st)) -	r(st, at)
t=1	t=1
(7)
15
Under review as a conference paper at ICLR 2020
-0.5-
-0.6-
-0.7-
-0.8-
-0.9-
WineWhite
Num of samples
Figure 10:	Comparison between SRLD and Langevin dynamics on test log-likelihood. The results
are computed based on 20 repeats. The error bar is calculated based on log-likelihood of SRLD -
log-likelihood of Langevin dynamics in 20 repeats to rule out the variance of data splitting.
where ∏ denotes the deterministic mapping from the context set S to actions {1,2,…，K} (read-
ers can view π as a deterministic policy in reinforcement learning). Intuitively, this pseudo-regret
measures the difference of cumulative reward between the action sequence at and the best action
sequence π(st). Thus, an algorithm that can minimize the pseudo-regret (7) can also find the best π.
Posterior sampling (a.k.a. Thompson sampling; Thompson, 1933) is one of the classical yet success-
ful algorithms that can achieve the state-of-the-art performance in practice (Chapelle & Li, 2011). It
WorkS by first placing an user-specified prior ^0,a on the reward r(s, a), and each turn make decision
based on the posterior distribution and update it, i.e. update the posterior distribution μS,° with the
observation r(st-1, at-1) at time t - 1 where at-1 is selected with the posterior distribution: each
time, the action is selected with the following way:
at = arg max Ir(St,a),	Ir(St,a)〜μS α.
a∈{1,2,∙∙∙ ,K}
i.e., greedy select the action based on the sampled reward from the posterior, thus called “Posterior
Sampling”. Algorithm 1 summarizes the whole procedure of Posterior Sampling.
Algorithm 1 Posterior sampling for contextual bandits
Input: Prior distribution μ0。, time horizon N
for time t = 1, 2,…，N do
observe a new context st ∈ S,
sample the reward of each action r(st, a)〜μS0, a ∈ {1,2,…，K},
select action at = argmaXα∈{i,2,…，k} r(st, a) and get the reward r(st, at),
update the posterior μt+∖ with r(st,aj.
end for
Notice that all of the reinforcement learning problems face the exploration-exploitation dilemma, so
as the contextual bandit problem. Posterior sampling trade off the exploration and exploitation with
the uncertainty provided by the posterior distribution. So if the posterior uncertainty is not estimated
properly, posterior sampling will perform poorly. To see this, if we over-estimate the uncertainty,
we can explore too-much sub-optimal actions, while if we under-estimate the uncertainty, we can
16
Under review as a conference paper at ICLR 2020
fail to find the optimal actions. Thus, it is a good benchmark for evaluating the uncertainty provided
by different inference methods.
Here, we test the uncertainty provided by vanilla Langevin dynamics and Self-repulsive Langevin
dynamics on two of the benchmark contextual bandit problems suggested by (Riquelme et al., 2018),
called mushroom and wheel. One can read (Riquelme et al., 2018) to find the detail introduction of
this two contextual bandit problems. For completeness, we include it as follows:
Mushroom Mushroom bandit utilizes the data from Mushroom dataset (Schlimmer, 1981), which
includes different kinds of poisonous mushroom and safe mushroom with 22 attributes that can
indicate whether the mushroom is poisonous or not. Blundell et al. (2015) first introduced the
mushroom bandit by designing the following reward function: eating a safe mushroom will give
a +5 reward, while eating a poisonous mushroom will return a reward +5 and -35 with equal
chances. The agent can also choose not to eat the mushroom, which always yield a 0 reward. Same
to (Riquelme et al., 2018), we use 50000 instances in this problem.
Wheel To highlight the need for exploration, (Riquelme et al., 2018) designs the wheel bandit,
that can control the need of exploration with some “exploration parameter” δ ∈ (0, 1). The context
set S is the unit circle ksk2 ≤ 1 in R2 , and each turn the context st is uniformly sampled from S.
K = 5 possible actions are provided: the first action yields a constant reward r 〜N(μι,σ2); the
reward corresponding to other actions is determined by the provided context s:
•	For s ∈ S s.t. ksk2 ≤ δ, all of the four other actions return a suboptimal reward sampled
fromN(μ2,σ2) for μ2 < μι.
•	For s ∈ S s.t. ksk2 > δ, according to the quarter the context s is in, one of the four
actions becomes optimal. This optimal action gives a reward of N(μ3, σ2) for μ3	μ1,
and another three actions still yield the suboptimal reward N(μ2, σ2).
Following the setting from (Riquelme et al., 2018), we set μ1 = 1.2, μ2 = 1.0, and μ3 = 50.
When δ approaches 1, the inner circle ksk2 ≤ δ will dominate the unit circle and the first action
becomes the optimal for most of the context. Thus, inference methods with poorly estimated uncer-
tainty will continuously choose the suboptimal action a1 for all of the contexts without exploration.
This phenomenon have been confirmed in (Riquelme et al., 2018). In our experiments, as we want
to evaluate the quality of uncertainty provided by different methods, we set δ = 0.95, which is pretty
hard for existing inference methods as shown in (Riquelme et al., 2018), and use 50000 contexts for
evaluation.
Figure 11:	Visualization of the wheel bandit (δ = 0.95), taken from (Riquelme et al., 2018).
Experiment Setup Following (Riquelme et al., 2018), we use a feed-forward network with two
hidden layer of 100 units and ReLU activation. We use the same step-size and thinning factor
c/n = 100 for vanilla LangeVin dynamics and SRLD, and set M = 2θ, α = 1 on both of the
mushroom and wheel bandits. The update schedule is similar to (Riquelme et al., 2018), and we just
change the optimization step in stochastic variational inference methods into MCMC sampler step
and use a higher (2000) initial steps for burn-in like the warm-up of stochastic variational inference
methods in Riquelme et al. (2018). As this is an online posterior inference problem, we only use the
17
Under review as a conference paper at ICLR 2020
last 20 samples to give the prediction. Notice that, in the original implementation of Riquelme et al.
(2018), the authors only update a few steps with new observation after observing enough data, as
the posterior will gradually converge to the true reward distribution and little update is needed after
observing sufficient data. Similar to their implementation, after observing enough data, we only
collect one new sample with the new observation each time. For SVGD, we also use 20 particles to
make the comparison fair.
18
Under review as a conference paper at ICLR 2020
B The Detailed analysis of SRLD
B.1	Some additional notation
We use k ∙ k∞ to denote the '∞ vector norm and define the L∞ norm of a function f : Rd → R1
as kfkL . DTV denote the Total Variation distance between distribution ρ1, ρ2 respectively. Also,
as K is Rd × Rd → R1, we denote kKkL ,L = supθ1,θ2 K(θ1, θ2). For simplicity, we may
use kKk∞,∞ as kKkL ,L . In the appendix, we also use φ[ρ](θ) := g(θ; ρ), where g(θ; ρ) is
defined in the main text. For the clearance, We define ∏M,c∕η * Pk ：= PM, ∏M,c∕η * Pk ：= PM and
∏m,c * Pt ：= PM, where PM, PM and PM are defined in main text.
B.2	Geometric Ergodicity of SRLD
Before we start the proof of main theorems, we give the following theorem on the geometric er-
godicity of SRLD. It is noticeable that under this assumption, the practical dynamics follows an
(Mcln + 1)-order nonlinear autoregressive model when k ≥ Mc/η
θk+ι = ψ (θk,∙∙∙, θk-Mc∕η) + vz2nek,
where
f	1 M	]
ψ (θk,…,θk-Mc∕η) = θk + n - -VV (θk ) + αφ[ M ɪ2 δθk-jc∕η ](θk ) j ∙
Further, if we stack the parameter by Θk = [θk,…，θk-Mc∕η]> and define Ψ (Θk) =
[ψ> (Θk), Θ>]>, we have
Θk+1 =ψ (Θk)+PnEk,
where Ek = [ek>, 0>,..., 0>]>. In this way, we formulate Θk as a time homogeneous Markov
Chain. In the following analysis, we only analyze the second phase of SRLD given some initial
stacked particles θMc∕η-ι.
Theorem B.1. (Geometric Ergodicity) Under Assumption 4.1 and Assumption 4.2, suppose we
choose n and α such that
max (l - 2naι + η2bι + 2αnbι, 2αn(bi + 1)) < 1,
then the Markov Chain of Θk is stationary, geometrically ergodic, i.e.,for any Θ0 = θMc∕η-ι, we
have
DTV [Pk (∙, Θo) ,Π(∙)] ≤ Q (Θo) e-rk,
where r is some positive constant, Q(Θ0) is constant related to Θ0, Pk is the k-step Markov tran-
sition kernel and Π is the stationary distribution.
We defer the proof to Appendix B.5.1.
B.3 Moment Bound
Theorem B.2. (Moment Bound) Under assumption 4.2, suppose that we have Eθ〜ρ0 |网|2 < ∞;
and a2 — α ∣∣K∣∣∞(2bi + 4) > 0, we have
supEθ〜ρk ∣∣θ∣∣2 ∨ sup Eθ〜ρk kθ∣2 ∨ supEθ〜ρt kθ∣2
kk	t
≤Eθ 〜ρo kθk2 + -	——	2α ∣∣K∣∣	(2b I 2).
a2 -kκ∣∣l∞,l∞ ^σ - α kκ∣l∞,l∞ (2b1 + σ)
And by Lemma B.1, we thus have
sup Eθ〜PkllVV(θ)k2 ∨ sup Eθ〜PkIl VV(θ)∣∣2 ∨ SupEθ〜Zdl▽1(θ)∣∣2
kkt
≤biEθ 〜ρ0 kθk2 + a -kK kL∞J⅛b 二 1∣K ηL∞,L∞(2bi + 2) +1
19
Under review as a conference paper at ICLR 2020
The proof can be found at Appendix B.5.2.
B.4 Technical Lemma
Definition B.1. (α-mixing)
For any two σ-algebras A and B, the α-mixing coefficient is defined by
a(A, B)=	sup	|P (A ∩ B) - P (A) P (B)|.
A∈A,B∈B
Let (Xk, k ≥ 1) be a Sequence of real random variable defined on (Ω, A, P). This Sequence is
α-mixing if
α(n) := sup α (Mk, Gk+n) → 0, as n → ∞,
k≥1
where Mj := σ (Xi, i ≤ j) and Gj := σ (Xi, i ≥ j)forj ≥ 1. Alternatively, as shown by Theorem
4.4 of Bradley (2007)
α(n) ：=4 sup {
Cov (f, g)
kfkL∞ kgkL
; f ∈L∞(Mk), g ∈ L∞ (Gk+n)	.
∞
Definition B.2. (β-mixing)
For any two σ-algebras A and B, the α-mixing coefficient is defined by
1I J
β (A, B)：= suP 2	Σ ∣P (Ai ∩ Bj) — P (Ai) P(Bj)1,
where the supremum is taken over all pairs of finite partitions {Aι,...,A/} and {Bi,...,Bj } of Ω
such that Ai ∈ A and Bj ∈ Bfor each i, j. Let (Xk, k ≥ 1) be a sequence of real random variable
defined on (Ω, A, P). This Sequence is β-mixing if
β(n) := sup β (Mk, Gk+n) → 0, as n → ∞.
Proposition B.1. (β-mixing implies α-mixing)
For any two σ-algebras A and B,
α (A, B) ≤ 1 β (A, B).
This proposition can be found in Equation 1.11 of Bradley (2005).
Proposition B.2. A (strictly) stationary Markov Chain is geometric ergodicity if and only ifβ(n) →
0 at least exponentially fast as n → ∞.
This proposition is Theorem 3.7 of Bradley (2005).
Lemma B.1. By Assumption 4.2, we have ∣∣VV(θ)k ≤ bi (kθιk + 1) and ∣∣θ 一 ηVV(θ)k ≤
(1 — 2ηaι + η2bi) I∣θ∣2 + η2bi + 2ηbi.
Lemma B.2. (Some property of RBF Kernel) For RBF kernel with bandwidth σ, we have
∣K∣∞,∞ ≤ 1 and
∣K(θ0, θi) — K(θ0, θ2)k	≤ 卜-(BσLpkθi — Θ2k2
∣Vθo K (θ0, Θι)-Vθo K (θ0, θ2)k	≤	2 e-(Rσ (∙)∣∣	∣θι — Θ2k2 .
Lemma B.3. (Some property of Stein Operator)
For any distribution P such that Eθ〜P ∣∣VV(θ)∣ < ∞, we have
kΦ[ρ](∙)∣Lip ≤ 卜(RσhpEθ〜P ∣VV(θ)k+ 2e-(∙)2/σS卜，
kΦ[ρ](θ)k ≤ ∣Kk∞ Eθ0〜PkVV(θ0)k + 2 (∣θ0k + kθ∣)
≤ IiKk∞ bi+Eθ0〜P Q+bi) kθ0k + ιιθk.
20
Under review as a conference paper at ICLR 2020
Lemma B.4. (Bounded Lipschitz of Stein Operator) Given θ0, definege： (θ) ：= 6回，](θ)
K(θ0, θ)VV(θ0) + ViK(θ0, θ). ½e also denote φe,(θ) = [φe,j(θ),…,(^eo,d(θ)]>. We have
d
XIMi(θ)Rp
i=1
d
X"(ML∞
i=d
2 ∣∣VV(θ0)k2 卜-k,k2/] j +2d
Lip
2d
2
+ 2 卜-k,k3σ
L∞
2	kVV(θ0)k2 .
L∞
≤
≤
2
B.5 Proof of Main Theorems
B.5.1 Proof of Theorem B.1
The proof of this theorem is by verifying the condition of Theorem 3.2 of An & Huang (1996).
Suppose Θ = [θi,…，Θmc+i], where C = c∕η, We have
∣ψ (Θ)∣
θi + η [-VV(BI) + αφ[ M X δθι+jC ](θk Jl
M
θi — ηVV(θi) + M X e-kθl+jc-θl k2/b2 (θi - θi+jC) - e-kθl+jc-θlk3σVV(θι+jc)
j=1
≤
+
Ml
θi — ηVV(θι) + 2η- X e-kθl+jc-θ1k /σθi
σM	l
j=1	l
M
M X e-kθl+jc-θ1k%2 (-VV(θι+jc) — θι+jc)
M j=1	σ
≤
kθi — ηVV(θi)k + 2αη ∣Kk∞,∞ bi(1 + kθik)
M
+ M X kKk∞,∞ b1
σ j=1
(1 + (1 + 2)kθi+jC k)
(1)	4αη
≤ bi(1 + ~0η kKk∞,∞) + η2bi + 2ηbi
+ (1 - 2ηai + η2bi + -∑η kKk∞,∞ bi) kθik + ~∑η kKk∞,∞ (bi + I) ∙」“max ʃɑ kθi+jCk
σ	σ	i∈[M C +1]-{1}
≤ bi(I + 4αη kKk∞,∞) + η2bi +2ηbi
+ max (1 — 2ηai + η2bi + ~~ kKk∞,∞ bi, ~~ kKk∞,∞ (bi + 1)) , max kθi+jCIl ,
σ	σ	i∈[M C +i]
where (1) is by Lemma B.1. Thus, given the step size η, if we choose η, α such that
max (1 — 2ηai + η2bi + ~η-k kKk∞,∞ bi, ~σ kKk∞,∞ (bi + 1)) < 1,
then our dynamics is geometric ergodic.
B.5.2 Proof of Theorem B.2
Continuous-Time Mean Field Dynamics (5) Notice that as our dynamics has two phases and the
first phase can be viewed as an special case of the second phase by setting α = 0, here we only
analysis the second phase. Define Ut
sup Je ∣∣θs∣∣2,
s≤t
and thus
∂
∂tUt ≤ E <θt, —V(B) + αφ[πM,c * pt](θt)) ∨ 0.
21
Under review as a conference paper at ICLR 2020
Now We bound E ®,—V (θ) + αφ[∏M,c * pt](θt)):
E (θt, -V(θt) + αφ[∏M,c * ρt](θ>t))
≤bι- a2E∣∣θt∣∣2 + αE∣∣θt∣∣ ∣∣φ[∏M,c * ρt](θt)∣∣
≤bι - a2E∣∣θt∣∣2 + α kKk∞ E∣∣θt∣∣ Eθ，〜∏M,m kVV(θ0)k + 2 (设|| + |如|)
≤bι- a2E∣∣θt∣∣2 + α kKk∞ E∣∣θt∣∣Eθ0~∏M,c*pt ]b1(kθ0k + i) + 2(kθ0k + kθtk)]
=bι -	(a2	- kK k∞	-σ)	E ∣∣θt∣∣2 + α kK k∞ E	∣∣θt∣∣ Eθo-∏M,c*pt (Q + σ)	kθ0k+ b)
≤bι -	卜 2	-IlK k∞	~σ)	Ut2+α kK k∞	E ∣∣θt∣∣	Eθo-∏M,c*pt ((bi + σ) kθ0k+ bi)
≤bι -	(。2	-kK k∞	2α)	Ut2 + α kK k∞	(bi +	2) -M X UtUt-jc + α kK k∞ biUt
≤bi -	(a2	-kK k∞	2α)	Ut2 + α kK k∞	(bi +	2) Ut2 + α kK k∞ bi (Ut2 + 1)
≤ (bi + I)- (a2 - kKk∞ ^a - α kKk∞ (2bi + σ)) Ut2,
where (1) is by B.3. By the assumption that λ := a2 - kKk∞ 2α - a kKk∞ (2bi + 2) > 0, we
have
∂
∂tUt ≤ [(bi + I)- λUt ] ∨ 0.
By GronWall's inequality, We have Ut ≤ U2 + b^. (If ∂∂t Ut = 0，then Ut fix and this bound still
holds.) Notice that in the first phase, as α = 0, we have λ < a2 and thus this inequality also holds.
Discrete-Time Mean Field Dynamics (4) Similarly to the analysis of the continuous-time mean
field dynamics (5), we only give proof of the second phase. Define Uk
supjE ∣∣θs∣∣ , and thus
U2 - U2-i ≤ [2ηE (θk-i,-VV(a) + a^[∏M,c∕η * Pk](a))+ 2力]∨ 0.
By a similarly analysis, we have bound
E (θk-i, -VV(θk) + aφ[∏M,c∕η * Pk](θk))
≤ (bi + 1) - λUt2 ,
where λ = a2 - kK k∞,∞ 2α -α kK k∞,∞ (2bi +2) > 0. AndthUs Wehave
Uk2 - Uk2-i ≤ [2η [(bi + 1) - λUk2-i + 2η2 ∨0.
It gives that
U2 ≤红上户+U02.
λ
Practical Dynamics (3) The analysis of Practical Dynamics (3) is almost identical to that of the
discrete-time mean field dynamics (4) and thus is omitted here.
B.5.3 Proof of Theorem 4.1 and 5.1
Notice that the dynamics in Theorem 4.1 is special case of that in Theorem 5.1 and thus we only
prove Theorem 5.1 here. After some algebra, we can show that the continuity equation of dynamics
(6) is
∂tPt = V ∙ ([- (D(θ) + Q(θ)) VV(θ) + αφ[∏M,c * ρt](θt)] Pt + (D(θ) + Q(θ)) VPt).
22
Under review as a conference paper at ICLR 2020
Notice that the limiting distribution satisfies
0 C= ▽• ([- (D(θ) + Q(θ)) VV(θ) + αφ[∏M,c * ρ∞](θt)] ρ∞ + (D(θ) + Q(θ)) Vρ∞)
=V∙ ([- (D(θ) + Q(θ)) VV(θ) + αφ[ρ∞](θt)] ρ∞ + (D(θ) + Q(θ)) Vρ∞)
=V∙ ([- (D(θ) + Q(θ)) VV(θ)] ρ∞ + (D(θ) + Q(θ)) Vρ∞)
+ αV∙ (K * (Vρ∞ - VV(θ)ρ∞) ρ∞).
which implies that ρ∞ α exp(-V(θ)) is the stationary distribution.
B.5.4 Proof of Theorem 4.2
In the later proof we use cd to represent the quantity
^Eθ〜ρo kθk2 +
bi + 1 + η
a2—kκ k∞,∞ σ -α kK k∞,∞ (2bι+σ)
Recall that there are two dynamics: the continuous-time mean field dynamics (5) and the discretized
version discrete-time mean field Dynamics (4). Notice that here we couple the discrete-time mean
field dynamics with the continuous-time mean field system using the same initialization. Given
any T = nN, for any 0 ≤ t ≤ T, define t = [ηtCη. We introduce an another continuous-time
interpolation dynamics:
^	/-VV(θt) + dBt,	t ∈ [0, Mc)
θ+ =
- —VV(θt) + αφ[∏M,c * ρt](θt) + dBt, t ≥ Mc,
. 一 ,0 .
Pt = Law(θt),
θo = θo 〜po,
Notice that here we couples this interpolation dynamics with the same Brownian motion as that of
the dynamics of θt. By the definition of θt, at any tk := kη for some integrate k ∈ [N], θtk and
θk has the same distribution. Define pθ0 = Law(θt) conditioning on θo = θo and ρθ0 = Law(θt)
conditioning on θo = θ0. Followed by the argument of proving Lemma 2 in Dalalyan (2017), if
k ≥ M, we have
Dkl [ρθk0 k Pθ0]
1 / E∣∣-VV (θs) + αφ[∏M,c * PM®)+ VV (θs) — αφ[∏M,c * ρs](^^s)∣∣ ds
1 k-1	tj+1
4 jS L
E ∣∣-VV(θtj) + αφ[∏M,c * Ptj](θtj) + VV(θs) — αφ[∏M,c * ρs](θs)∣∣ ds
3 k-1 tj+1	∣
≤4 X Lj	E ∣∣
VV(θtj) -VV(θs)∣∣ ds
∣2
O
∣2
O
E ∣∣Φ[∏M,c * Ptj](θj) — Φ[∏M,c * Ps](θtj )∣∣ ds
∣2

E ∣∣Φ[∏M,c * Ps](θtj) — Φ[∏M,C * Ps](θs)∣∣ ds
=I1 + I2 + I3 .
We bound I1 , I2 and I3 separately.
Bounding I1 and I3 By the smoothness of VV, we have
∣vv (θtj) -vv (θs)∣∣2≤ b2∣θtj — θs∣∣2.
23
Under review as a conference paper at ICLR 2020
And by Lemma B.3 (Lipschitz of Stein Operator), we know that
kΦ[∏M,c * ps](θl) - Φ[∏M,c * Ps](θ2)k
≤ 卜(∙)”Lp廊―*PsW⑹k + 2ei"(Lp.
kθ1-θ2k2.
And by the Assumption 4.2 and that Ps as finite second moment, We have
kΦ[∏M,c * Ps](θl) — Φ[∏M,c * Ps](θ2)k
≤Ccdkθ1-θ2k2.
Combine the tWo bounds, We have
Ii + I3 ≤ 3C4cdχ I' tj+1 E 阿-θs∣∣2 ds.
4 j=0 tj
Notice that θt = θ^ + [-VV(θθt) + αφ[∏M,c * 力/(0上)](t -1) + Rtt dBs. ByltO's lemma, it implies
that
Il + I3 ≤ 3C4cdX I' tj+1 E 辰-θs∣∣2 ds
4	j=0 tj
≤ 3Ccd [tj+1
-4	Jtj
E∣∣-VV(θs) + αφ[∏M,c * ρs](θs)∣∣ (s - tj)2 +2d(s - tj) ds
k-1	2
=C(Cln X E∣∣-VV (θtj) + αφ[∏M,c * ρtj ](θtj )∣∣ + Ccddkh2.
j=0
By the assumption that E ∣∣θj ∣∣ is finite and θj = θj, E ∣∣θj ∣∣ is also finite, We have
E∣∣-VV(θt)+ αφ[∏M,c * Pt](θt)∣∣2
≤2E∣∣VV(θt)∣∣2 +2α2E∣∣φ[∏M,c * PtM)∣∣2
≤4b1+4bιE ∣∣θt∣∣ + 2α2E ((σ+bi) Eθ0 〜∏M,c*pt kθ0k + kθk)
≤c2dC.
Thus We conclude that
Ii + I3 ≤ CCd(Cdkn3 + dkη2).
Bounding I2
∣2
O
E ∣∣Φ[∏M,c * Ptj](θtj) - Φ[∏M,c * Ps](θtj)
=E
≤ M X E ∣∣φ[Ptj-Cl](θtj)-矶 Ps-Cl](θtj∙)∣∣
1	∣	∣2
=M X 用 ∣Eθ~ptj-cl φθj ⑹-Eθ~ps-cl φθj ⑻ ∣∣
l=i
1 M	d	2
=M EE% ΣS∣Eθ~ρj-ci φθj,i(θ)- Eθ~ps-ci φθj,i(θ)∣
l=i	i=i
Md	2
≤MχEθj X (∣∣φθj,i(∙)∣∣L∞ ∨ ∣∣φθtj,i(∙)∣Lp)DBL [力…Ps-Cl]
24
Under review as a conference paper at ICLR 2020
By Lemma B.4 and the Assumption 4.4 that V is at most quadratic growth and that Pt has finite
second moment, We have
Eθtj XX (Mθtj,i(ML∞∨Mθtj,iS∣Lp) =Eθtj XX (鼠,i(∙”L∞∨ 鼠,,SR) 2 ≤ 4d 2e-kθk2∕σθι	+4 卜-卜『/[|	E① σ	BL	BL	tj ≤C(d+c2d). Plug in the above estimation, we have I2 = 3α- X Z	E llφ[πM,c * Ptj](θG -矶πM 4 j=0 tj k-1 tj+1 1 M ≤ αC(d + Cd)E J	M y?DBL [ptj-cl, 1 k-1 1 M	tj+1 ≤ αC(d + Cd) E M	J	DKL [ptj-cl, definition	W (θtj)∣∣2 「,c * ps] (Otj) || ds Ps-ci] ds Os-cl] ds, Pinsker0 s
were te ast nequaty s ue to te reaton tat BL ≤ Overall Bound Combine all the estimation, we have k-1 1 M	tj+1 DKL [ρθk0 k PM ≤ α2C(d + Cd) X M XL	DKL [Pt k-1 1 M	η =α2C(d + Cd)二 M 二 / Dkl k tj Similar, if k ≤ Mc — 1, we have DKL [魔 k Pθ0 i =4 /"e∣∣vv (θs)-w(θ ≤ b2 XX /tj1 E l∣θtj- θs∣∣2 j=0 tj ≤ T XX E ∣∣vv (θtj)∣∣2+ j=0 b21 η3kC2d	dkb12 η2 ≤ 12 +^T^. Define Uk = sup	DKLhPθ0 k s∈ [tk ,tk+1 ]	TV ≤	KL . j-cl,Ps-ci] ds + CCd(Cdkn3 + dkη2) 铲),Pt( Y )+s ds + CCd(Cdkn3 + dkn2) s )∣∣∣ ds ds dkb21 n2 -^4- Pθ0 i,
25
Under review as a conference paper at ICLR 2020
and Uk = max Ul. We conclude that for k ≥ Mc, for any k0 ≤ k,
l∈{0,...,k}	η
k-1 1 M h
Uk0≤ α2C(d + Cd)三而三 / DKLu
ds + Ccd(Cdkn3 + dkη2)
(jη-cl ) , pt( jη-cl ) +s
k-1 1 M
≤ α2C(d + Cd) X M X ηu( jn-cι) + Ccd(Cdkrη3 + dkη2)
k-1
≤ α2C(d + cd)η ^X Uj + CCd (cdkη3 + dkη2).
j=0
For k < M, which is a simpler case, we have
Uk ≤ C (η3kcd + dkη2) < CMc (ηcd + d) η.
We bound the case when k ≥ Mc,
k-1
Uk ≤ α2C(d + cd)η ^X Uj + CCd (cdkη3 + dkη2).
j=0
Ifwe take η sufficiently small, such that c2d kη3 ≤ dkη2, we have
k-1
Uk ≤ a2C(d+ c2d)ηXUj + 2Ccd2 dkη2
j=0
k-1
≤ a2C(d+ c2d)ηX(Uj +η) .
j=0
Define η0 = a2C(d + c2d)η and we can choose η small enough such that η0 < 1/2 and η < 1/2.
Without loss of generality, we also assume η0 ≥ η and thus we have
k-1
Uk ≤ η0 X (Uj + η0) .
j=0
Also we assume Uk ≥ η0, otherwise we conclude that Uk < η0 . We thus have Uk ≤ q Pjk=-01 Uj ,
where q = 2η0. Suppose that UMc-ι = X ≤ CMc (ηcd + d) η and some algebra (which reduces to
Pascal’s triangle) shows that
Uk ≤ χq(i + q)k-M.
We conclude that Uk ≤ xq(1 + q)k-1. Notice that q = 2α2C(d + cd)η. Thus for any k ≥ Mc∕η,
Uk ≤ χq(l + q)k-Mηc
=xq(1 + q)(kn-MC)/n
=χq(ι + q)2(y2 C (d+ cdKkn - Mc) / q
≤ x2α2C(d + c2d)e2α2C(d+c2d)(kη-Mc)η
≤ CMca2 (ηcd + d (d + ,彳浮。2。3+Cd)^-Mc)η2,
for sufficiently small η. Combine the above two estimations, we have
(C (η3kcd + dkη2 + η)	k ≤ Μc∕η - 1
k 一 (CMCa2 (ηcd + d (d + cd)e2α2C(d+cd)(kn-Mc)η2 + Cη k ≥ Mc∕η ^
Notice that now We have Uk = max SuP DKL Pθ0+s k Pθ0 , which is a function of θ0.
l∈{0,...,k}s∈[0,η]
We then bound Uk
max sup DKL [piη+s k Pin]. Notice that the KL divergence has the
l∈{0,...,k}s∈[0,η]
following variational representation:
DKL[ρ1 k ρ2 ] = sup Eρ1 f - Eρ2ef ,
f
26
Under review as a conference paper at ICLR 2020
where the f is chosen in the set that Eρ1 f and Eρ2 ef exist. And thus we have
Dkl [ρiη+s Il PIn] = SuP
f
≤ Eθ0
〜P0
And thus Uk ≤ Uk. Also the inequality that
Uk =	max I SuP DKL [pin+s k PIn] ≥ max η DKL [pin k PIn ]
l∈{0,...,k}s∈[0,η]	l∈{0,...,k}
holds naturally by definition. We complete the proof.
B.5.5 Proof of Theorem 4.3
The constant h1 is defined as
h1
2
2 e-kθk1 2∕σ θι	∨∣∣e-k∙k2∕σ∣∣	∨
σ	BL
2 e-(∙)2/σ (∙)
σ
Lip
Now we start the proof. We couple the process of θk and θk by the same gaussian noise ek in every
iteration and same initialization θo = θo. For k ≤ Mc/n — 1, E ∣∣θk 一 0小 =0 and for k ≥ Mc/n
we have the following inequality,
E ∣θk+1 — θk+1 I - E ∣∣θk 一 θk I
=2nE Dθk 一 θk, -W(θk) + VV(θk))
1M
+2naE θΘ- - θk ,矶 M 工 δθk-jc∕η](Ok)- φ[πM,c∕n
M j=1
+n2E
1M
-VV (θk ) + αφ[ M y^δθk-jc∕η ](θk ) + VV (θk ) 一 αφ[πM,c∕n * pk ](θk )
M j=1
2
By the log-concavity, we have
E DΘk - Θk,-VV(Θk) + VV(Θk))
≤ - LE ∣∣θk - θk II ,
for some positive constant L. And also, as n is small, the last term on the right side of the equation
is small term. Thus our main target is to bound the second term. We decompose the second term on
the left side of the equation by
E

1M
θk - θk, φ[ M £通-” ](θk ) - φ[πM,c∕n
j=1
=E (θk - θk, φ[ M X δθk-jc∕η](θk ) - φ[πM,c∕n * Pk](θk ))
+E (θk - θk, φ[πM,c∕n * Pk](θk) - φ[πM,c∕n * Pk](θk))
+E (θk - θk, φ[πM,c∕n * pk](θk) - φ[πM,c∕n * Pk](θk))
=I1 + I2 + I3 .
We bound I1 , I2 and I3 independently.
27
Under review as a conference paper at ICLR 2020
Bounding I1 By Holder’s inequality,
Ii ≤ E B — θk
1M
φ[ M∑δθk-jc∕η ](θk ) - φ[πM,c∕η * Pk](θk )
j=i
≤ JE∣∣θk-θk∣∣2 t
1
φ[M Eδθk-jc∕η](θk) - φ[πM,c∕η * ρk](θk)	∙
j=1
We bound the second term on the right side of the inequality. Define
a2 = sup
k
E (。[焉 PM=I δθk-jc∕η ](θk ) - φ[πM,c∕η * Pk ](θk )(
sup E
kθk≤B
1 δθk-jc∕η](θ) - Φ[∏M,c∕η * Pk](θ)ll2
and by the regularity assumption we know that
SUp E
k
a2 ≤
∣∣φ岛 Pj=ι δθk-jc](θk) - φ[∏M,c∕η * Pk](θk)『
inf sup E
k kθk≤B
----------------------------------------------2 < ∞.
∣φ[M PM δθk-3c∕η](θ) - Φ[∏M,c∕η * Pk](θ)∣∣
Define φ[ MM PM=I δθk-jc∕η](θ) - φ[πM,c∕η * Pk ](θ) =。*岛 PM=I δθk-3c∕η ] and SinCe the Stein
operator is linear functional of the distribution, we have
1M
Eφ*[ M £ δθk-jc∕η ](θ) = 0，
j=i
given any θ. By Theorem B.1 that Θk iS geometriC ergodiCity and thuS iS β-mixing with exponen-
tially faSt deCay rate by PropoSition B.2. And by PropoSition B.1, we know that Θk iS alSo α-mixing
with exponentially faSt deCay rate. We have the following eStimation
1M
E 矶 MEδθk-jc∕η ](θk ) - φ[π
j=1
≤a2 sup E
kθk≤B
M,c∕η * Pk](θk)
M
≤ M2l lsupBE Xι∣φ*[δθt-kc∕η ](θ)ιι2
kθk≤B k=1
≤ CL
SUp EE <φ*[δθt-kc∕η](θ),φ*[δθf∕η](θ))
kθk≤B k6=j
Γe-rc(1 - e-rMc
1 - erc
+1 ,
for some positive constant r that characterize the decay rate of α mixing. Combine this two estima-
tions, we have
Ii ≤ …T aM [
+1 .
+"
+ M2
M
2
E
M
2
2
Bounding I2 By Holder’S inequality, we have
I2 ≤
JEM- θk∣∣2 qE∣∣Φ[∏M,c∕η
* Pk](θk) - φ[πM,c∕η * Pk](θk)∣∣2.
28
Under review as a conference paper at ICLR 2020
We bound the second term in the right side of the inequality.
e ∣φ∖πMtc2∕η * Pk](θk) - φ[πM,c∕η * ρk](θk)l∣2
=E
1M
M E φ[,[Pk-c∕∕r∣ ](θk ) - φ[ρk-jc∕η ](θk )]
M j=1
1M
≤M ɪ2E ∣lφ[ρk-jc∕η](θk) - φ[ρk-jc∕η](θk)l∣
1M
=M ∑Eθk l∣Eθ 〜Pk-jc/n φθk (θ) - Eθ 〜Pk-jc/n φθk (θ)l∣2
M j=1
Md
M X Eθk XIEθ
'〜Pk-jc/nφθk,i(θ) - Eθ〜Pk-jc/n φθk,i
4d
2 e-kθk% θι
σ
22
+ 4 卜 kτ kVV(θk)k2
BL	BL
Plug in the above estimation and by the relation that DBL ≤ W1 ≤ W2, we have
E kΦ[∏M,c * Pk](θk) - Φ[∏M,C * pk](θk)k2
4d
4d
2 e-kθk2∕σ θ1
σ
2
+ 4IeTHI2∕σ
BL
BL
1M
Eθk INV (θk )k	M EDBL [iρk-cj, ρk-cj ]
j=1
2
+ 4 ||e-k^12/CT
BL
BL
1
Eθk kW(θk)k2 M £w2 [ρk-cj,Pk-cj].
j=1
≤

2

≤
≤
2
2
M
M
And combined all the estimation and by the definition of Wasserstein-distance, we conclude that
I2 ≤	4d
≤	4d
2
2e-kθk2∕σ θι
σ
2
+ 4 1e-k∙k2∕σ
BL
1
||Bl Eθk kVV(θk)k2t M X W2 [Pk-cj,Pk-cj]
j=1
2
2e-kθk2∕σ θι
σ
BL
[ lvi
+ 4l∣e-k,k2∕σ∣lBL Eθk kVV (θk)k2t M X E∣∣θk
j=1
-cj - θk-cj ∣∣
M
2
M
Bounding I3 By Holder’s inequality,
I3 ≤
φ[πM,c∕η * ρk](θk) - φ[πM,c∕η * ρk](θk)∣∣
2
We bound the last term on the right side of the inequality. By assumption and Lemma B.3, we have
E ∣∣Φ[∏M,c∕η * Pk](θk) — Φ[∏M,c∕η * Pk](θk)∣∣
≤ 卜-(∙)2∕σ∣∣Li pEθ 〜PkkVV (θ)k + 2 e-O2∕σ (∙)
Lip
2
E∣θk - 0k『
29
Under review as a conference paper at ICLR 2020
And combine the estimation, we have
I3 ≤
卜-(Hσk pEθ 〜PkkVV(θ)k +
2 e-(Bσ (∙) E∣∣θk — θk
σ
Overall Bound
Combine all the results, we have the following bound: for k ≥ Mc,
E ∣∣θk + 1 — θk+'	— E ∣∣θk — θk∣∣
≤ — 2ηLE ∣∣θk — θk∣∣
+2ηα JE Mk-θk∣∣ √M
+2ηαc2t
1M
M X
j=1
E∣∣θk-jc∕η — θk-jc∕η∣∣ E∣∣θk — θk
〜
2
〜
2
+ 2ηαc3E ∣∣θk — θk ∣∣
+η2c4,
where
and
c2
4d
c1 = a2C
e-rc(1 — e-rMc)
1 — erc
+1 ,
2e-kθ"θι	+4∣∣e-k∙k3σ∣∣BL SUp Ee% ∣∣VV(θk)∣∣2,
σ	∣BL	k
C3= ∣∣e-(Bσ∣∣ sup Eθ〜PkkVV(θ)k + 2eY)24(∙)
Lip k	σ	Lip
c4 = sup E
k≥Mc∕η
1M
vv(θk) + αφ[M>2δθk-jc∕η](Ok)- vv(θk) — αφ[πM,c∕η * Pk](0k)
M j=1
Define Uk = 'E ∣∣θk - θk∣∣ and Uk = sup Ul, We have
U2+1 ≤ qUk2 + ~η~i=~Uk + η2c4,
M
Where q = (1 — 2η(L — αc2 — αc3)). By the assumption that α ≤ L/(c2 + c3), q < 1. NoW We
prove the bound of Uk by induction. We take the hypothesis that Uk2 ≤
2
30
Under review as a conference paper at ICLR 2020
and notice that the hypothesis holds for Uo = 0. By the hypothesis, we have
U2+ι ≤ q
(1 一 q)
+ η2卜4 +占
+
√M
q
(1 - q)2
(1-q)2
+(1 - q)嚼η Q+占)+(1 - q)η2 Q+
2√2c1 η (c4 + 丁L) + η2 (c4 + —^―
√M 1	1 - qj \	1 - q
+ 1 - q
(1-q)2
q
≤ q
+ 1 - q
(1-q)2
(1 - q)2
+(1 - q)2√M1 η 卜4+占)+(I- q)2η2 卜4+占)
(1-q)2
+(I - q)
(1-q)2
(2√M1 + (1 - q)η 卜4 + 1⅛))
(1-q)2
where the last second inequality holds by (1 - q)
of induction and we have, for any k,
≥ 1. Thus we complete the argument
u2 ≤
(瑞1 + (1 - q)η (c4 + 击))
(1-q)2
4η2 α2c1
-M-
≤2
(1 - q)
2a2c2	1
(L — αc2 — αc3)2 M
+ 4η2 (c4 + 2η(L - ac2 - ac3))2 .
And it implies that W2 [ρk, ρk] ≤ Uk ≤ Uk ≤(工_：02-彳03)2 Mr + 4η2 (c4 + 2η(L - αc2 - αc3))2.
B.6 PROOF OF TECHNICAL LEMMAS
B.6.1	PROOF OF LEMMA B.1
For the first part:
I” (θ)∣∣
≤kvv (θ) -VV (0)k + INV (0)k
≤bι (kθιk + 1).
q
2
31
Under review as a conference paper at ICLR 2020
For the second part:
kθ - nW(θ)k
=hθ - nW(θ), θ - nW(θ)i
=kθk2 + 2n (θ,-w (θ)i + n2kw(θ)k2
≤kθk2+2n-a1kθk2+b1+n2b1(1+kθk2)
=(1 - 2naι + n2bι) ∣∣θk2 + η2bι + 2nbι.
B.6.2 Proof of Lemma B.2
It is obvious that ∣K∣∞,∞ ≤ 1.
∣K(θ0,θ1)-K(θ0,θ2)∣
e-kθ0 -θι∣2∕σ — e-kθ0 -θ2∣2∕σ
≤ eY)2∕σLp∣θι — θ2∣2 .
And
∣∣VθoK(θ0, θι)-Vθ,K(θ0, θ2)k
=2e-kθ0-θlk24 (θ0 — θι) — 2e-kθ'-θzk24 (θ0 — θ2)
σ	σ
≤	2e一(.)2/<T(∙) kθι — Θ2k2 .
σ	Lip
B.6.3 Proof of Lemma B.3
For any distribution ρ such that ∣VθV(θ)∣ ρ(θ)dθ < ∞,
kφ[ρ](θι) — Φ[ρ](θ2)k
=∣Eθ〜P {— [K(θ, θι) — K(θ, θ2)] VV(θ) + ViK(θ, θι) — ViK(θ, θ2)}∣∣
≤ 卜-(BσLpEθ〜P ∣VV(θ)k kθi - Θ2k2
+ 2e-(∙)2/σ(∙)∣l, kθi - Θ2k2 .
For proving the second result, we notice that
kφ[ρ](θ)k = Eθ0〜P [K(θ0, θ)VV(θ0) + ViK(θ0, θ)]
≤∣K∣∞Eθo〜P	∣VV(θ0)k + 2(∣θ0k + kθ∣)
≤ kKk∞ bi+Eθ0~ρ	(*+bi) kθ'k + ιιθk
B.6.4 Proof of Lemma B.4
Given any θ0,
32
Under review as a conference paper at ICLR 2020
i=1
d
X
i=1
d
X
i=1
sup
Θ1=θ2
sup
θ1=θ2
d
≤2 X 黑2
d
+2 sup
i=1 θ1=θ2
忸 θ0,i(θ1) — φθ0,i(θ2)∣
2
Ilθ1 - θ2∣∣2
,θ,,i(θ1) - 4>θ',i(θ2)∣2
-1%-仇12
卜-kθ'-θ1∣bσ - e-kθ'-θ2k%)品 v (θ' )∣2
* θ2∣∣2
2e-kθ'-θ1k2∕σ(θι,i - θ/) - 2e-kθ'-θ2k”(θ2,i - θ/)∣2
∣θι- θ2∣2
For the first term on the right side of the inequality,
d
X sup
M θ1=θ2
e-kθ-θ1k2∕σ - e-kθ,-θ2k2∕σ)品v(θ/)∣2
d
E 焉v(θ)
i=1
2
sup
Θ1=θ2
∣θ1- θ2 ∣2
e-∣θ0-θ1k2∕σ - e-kθ0-θ2∣2
kθ1- θ2k2
=∣∣W(θ0)k2 ∣∣e-k∙k2∕σ
2
Lip
To bound the second term, by the symmetry of each coordinates, we have
匚	∣2e-kθ-θ1∣∣2∕σ(θι,i - θi) - 2e-kθ-θ2∣%(θι,i - θ/)∣2
sup	2
i=1 θ1=θ2	Ilθ1 - θ2∣2
=d
2 e-kθk2∕σ Θ1
σ
2
Lip
This finishes the first part of the lemma.
d
X∣M,i(θ)∣∣L∞
i=d
d
=X
i=d
d
≤ X 2
i=d
d
≤X2
i=d
σ
σ
eTθ'-θk2∕σ 22θi - 2θ0
2
2
L∞
≤2d
2
σ
2e-kθ,-θk2∕σ (θi
σ
2e-kθ,-θk2∕σ (θi
σ
-kθk2∕σ θ1
-θ)
-θ)
L∞
2
L∞
d
+ X 2
i=d
e-kθ'-θk2∕σ ' V (夕)
∂θi
+ 2 卜内q∣: IlVV(夕)∣2
U	U l∞>
2
+ 2∣e-k∙k2∕σ
Ll∞
∣∣2 ∣vv(θ')∣∣2.
"l∞
2
L∞
33