Under review as a conference paper at ICLR 2020
Fractional Graph Convolutional Networks
(FGCN) for Semi-Supervised Learning
Anonymous authors
Paper under double-blind review
Ab stract
Due to high utility in many applications, from social networks to blockchain to
power grids, deep learning on non-Euclidean objects such as graphs and mani-
folds continues to gain an ever increasing interest. Most currently available tech-
niques are based on the idea of performing a convolution operation in the spectral
domain with a suitably chosen nonlinear trainable filter and then approximating
the filter with finite order polynomials. However, such polynomial approxima-
tion approaches tend to be both non-robust to changes in the graph structure and
to capture primarily the global graph topology. In this paper we propose a new
Fractional Generalized Graph Convolutional Networks (FGCN) method for semi-
supervised learning, which casts the Levy Flights into random walks on graphs
and, as a result, allows to more accurately account for the intrinsic graph topology
and to substantially improve classification performance, especially for heteroge-
neous graphs.
1	Introduction
Adaptation of deep learning (DL) to graphs and other non-Euclidean objects has recently witnessed
an ever increasing interest, leading to the new subfield of geometric deep learning. In particular,
geometric deep learning is an emerging direction in machine learning which aims at generalizing the
concepts of deep learning for data in non-Euclidean spaces, e.g., graphs and manifolds, by bridging
the gap between graph theory and deep neural networks (see discussion by Bronstein et al., 2017;
Monti et al., 2017; Monti, 2019, and references therein).
Many such DL approaches for non-Euclidian objects are based on the idea of performing a convo-
lution operation in the spectral domain with a suitably chosen nonlinear trainable filter. As a result,
node features are mapped into some Euclidian space. Next, graph filters are approximated with
various finite order polynomials, e.g., Chebyshev polynomials (i.e., the ChebNet model family of
Defferrard et al., 2016; Kipf & Welling, 2017), Cayley transform (i.e., CayleyNet of Levie et al.,
2018) or the generalization of polynomial filters in a form of Auto-Regressive Moving Average
(ARMA) models (Bianchi et al., 2019). However, deep learning approaches based on approxima-
tion with finite order polynomials tend to be non-robust to even minor changes in the graph structure
and to largely disregard the local graph topology that often plays the critical role for learning on
heterogeneous graphs. In contrast, as noted by Bianchi et al. (2019), one of the primary benefits of
the ARMA filters over polynomial ones is that ARMA filters are not computed in the Fourier space
induced by a graph Laplacian, and as a result, ARMA filters are local in the node space and enable
to more flexibly and accurately capture the underlying graph topology.
In this paper we further advance this localized approach to DL on graphs and propose a frac-
tional generalized graph-based convolutional filter for semi-supervised learning, which casts the
Levy Flights into random walks on graphs. As a result, our new Fractional Generalized Graph
Convolutional Networks (FGCN) method allows to more accurately account for the intrinsic local
graph topology and to substantially improve classification performance, especially for heteroge-
neous graphs.
The key contributions of our paper can be summarized as follows:
1
Under review as a conference paper at ICLR 2020
•	we propose a new fractional generalized graph-based convolutional filter for semi-
supervised learning, which casts the Levy Flights into random walks on graphs and, as
a result, provides a more efficient exploration of the graph structure.
•	we develop a new Fractional Generalized Graph Convolutional Networks (FGCN) method
that substantially improves accuracy of node classification for graphs of smaller sizes and
with a lower number of node features.
•	the proposed architecture of FGCN uses two state-of-the-art operations - gated max-
average pooling and residual block. The architecture has been shown to significantly im-
prove the training convergence and model output stability.
2	Related Work
Many earlier semi-supervised learning approaches on graphs, e.g., Gaussian mixture models, co-
training, harmonic function, and label propagation, employed only the label information (i.e., la-
beled instances) to train models based on the smoothness assumption over the labels (Zhu & Gold-
berg, 2009) and largely disregarded the graph structure. To improve performance, several learning
methods on graphs propose to incorporate intrinsic “graph-based” information by designing a classi-
fying function via generalizing the normalized cut and adding smooth function with respect to the in-
trinsic structure (Zhou et al., 2004; Zhou & Burges, 2007). A recent approach of Avrachenkov et al.
(2012) proposes a generalized optimization framework through considering the above two methods
as particular cases. However, one of the major limitations to these graph-based semi-supervised
learning methods is disregarding important information contained in graph edges.
To address this disadvantage, Defferrard et al. (2016) propose a formulation of convolutional neural
networks (CNN) based on spectral graph theory - ChebNet, which employs approximation via finite
order polynomials and, in particular, is based in the Chebyshev expansion for fast filtering instead
of the expensive eigen-decomposition. Graph Convolutional Networks (GCN) of Kipf & Welling
(2017) simplifies ChebNet and further addresses the gradient vanishing problem and reduces the
number of optimization. Other related approaches to graph learning with deep neural networks in-
clude, for instance, mixture model networks (MoNet) (Monti et al., 2017), graph attention networks
(GAr) (Velickovic et al., 2017), graph convolutional recurrent networks (Seo et al., 2018), dual
graph convolutional networks (Zhuang & Ma, 2018), FastGCN (Chen et al., 2018), and simplified
version of GCN (Wu et al., 2019).
ro extend the success of GCN on undirected graphs to directed graphs, MotifNet of Monti et al.
(2018) replaces the normalized Laplacian with the motif Laplacian in a multivariate polynomial
filter, where the motifs information can help capture the network structure. Finally, the most recent
approach of Bianchi et al. (2019) provides more flexible responses than GCN by using parallel and
periodic concatenations of the convolutional kernel via the ARMA filter. As a result, the ARMA
approach which is applicable to both directed and undirected networks allows to more accurately
incorporate the underlying local graph structure into the graph learning process.
3	Methodology
Given a graph structure G = {V, E, W}, with V the set of nodes (N = |V|) and E ⊆ V × V the
set of edges between any two nodes. Let square matrix W represents the adjacency matrix of a
graph whose entries {ωij }1≤i,j≤N are the edge weight function such that each edge eij ∈ E has
a weight ωij . We call W the symmetric adjacency matrix (ωij 6= 0 if there is an edge between
vertex vi and vertex vj ) if we assume that graph G is undirected. In reality, however, undirected
graph is a simplified representation of complex networks, we also consider directed graph (in which
case W> 6= W) and we can easily transform the non-symmetric W to the symmetric one through
W0 = (W> + W)/2.
We take N × Q feature matrix X as the input to a semi-supervised learning algorithm where Q is the
number of different node features. Suppose we would like to classify N data points into K classes
(communities), we define N × K label matrix Y as:
2
Under review as a conference paper at ICLR 2020
Yik =	0
if vertex i is labelled as a class k,
otherwise.
(1)
Here We refer to each column Yk of matrix Y as a IabelingfUnction. Also define an N X K matrix
F and call its columns F∙k classification functions.
3.1	Graph signal processing
In graph signal processing, a real-symmetric matrix has N real eigenvalues and its N real eigen-
vectors form an orthonormal basis. Given the symmetric adjacency matrix W of a graph, let D be
the degree matrix Where dii = PjN=1 wij and denote L = U>ΛU the Standard Laplacian matrix,
Where Λ = diag(λ0, . . . , λN-1) and U = [u0, . . . , uN-1] is the matrix of eigenvectors.
In the folloWing, We Will revisit three popular semi-supervised learning methods - graph-based semi-
supervised learning, fractional graph-based semi-supervised learning, and graph convolutional net-
Works and gain neW insights for improving their modeling capabilities.
Graph-based semi-supervised learning. Graph-based semi-supervised learning (G-SSL) has re-
ceived much attention as an alternative approach to the population paradigm of supervised learning
in recent years. G-SSL develops a generalized optimization frameWork, Which has three particular
cases (i) the Standard Laplacian (SL); (ii) Normalized Laplacian (NL); (iii) PageRank (PR). The
optimization formulation With the folloWing expression:
min {2FTDσ-1 LDeTFk + μ(F∙k - Yk)TD2σ-1(F∙k - Yk)},	⑵
F
where μ is a regularization parameter. The minimization of the first term in Eq. 2 corresponds
to the idea that if tWo nodes are close in graph With respect to some metric, they should belong
to the same class; and by minimizing the second term we would like to bring the classification
function F∙k as close as possible to the labelling function Yk. The Eq. 2 allows US to obtain the
Standard Laplacian based formulation (σ = 1), the Normalized Laplacian formulation (σ = ɪ), and
PageRank formulation (σ = 0). The objective of the generalized optimization framework for G-SSL
is a convex function and the corresponding classification function:
Fk = (1 - α)(l - αD-σWDσ-1)-1 Yk,	(3)
where α = ɪ- and for k = 1,...,K.
2+μ
Tuning the parameter σ on the power of degree matrix D, we can obtain three mentioned above
particular semi-supervised learning methods:
•	SL method (σ = 1): F∙k = (1 一 α) (I - aD-1W) 1 Yk
•	NL method (σ = ɪ): F∙k = (1 — α)(I — αD-21 WD-1) 1 Yk
•	PR method (σ = 0): F∙k = (1 — α) (I — αWD-1) 1 YTk
From above formulations, the classification function F, i.e., the result of random walk process which
provides the connection to the probabilistic interpretation of G-SSL. The parameter α controls the
strength of the ground truth label matrix Y in the generalized optimization framework.
Fractional graph-based semi-supervised learning. To improve the classification performance
(in particular, fuzzy graphs and unbalanced labeled data) of G-SSL, fractional graph-based semi-
supervised learning (De Nigris et al., 2017) embeds Levy Flights into random walks on graphs
by constructing from powers of the Laplacian matrix, i.e., the Lγ operator. This operation can be
used to generate different transition probabilities (i.e., corresponding to stochastic adjacency matrix)
based on different Y values. Intuitively, embedding Levy Flights into random walks allows for better
capturing mixing properties (i.e., dependence) in the data. Based on a fractional Laplacian matrix,
0 < γ < 1, the anomalous (fractional) diffusion processes on networks can be constructed from
the spectra data and eigenvectors of the Laplacian matrix. The fractional powers of L allows Levy
random walks with long-range navigation on a network. For example, the long-range transitions
3
Under review as a conference paper at ICLR 2020
on a network can directly move node u and node v with the transition probability m(uγ→) v through a
random walker, where m(uγ→) v is an element in the fractional transition matrix M(γ) (see the graph
within dotted circle in Figure 1). The transition probability m(uγ→) v between any two nodes whose
geodesic distance is not infinite can be summarized as follow:
mU→V = δuv- Tuv,	(4)
kuγ
where δuv represents the Kronecker delta, ku(γ) denotes the fractional degree of the node u and
k(γ) ≡ (LY)uu. Eq. 4 provides transition probabilities for the Levy Flights. Unlike the standard
random walk, the Levy Flights can jump immediately over several hops in a graph. This feature
makes LeVy Flights a very good exploratory process. There is a price to pay for this: the typically
sparse transition probability matrix becomes non-sparse. We can mitigate non-sparsity by taking a
reasonable number of principal singular eigenvectors or limiting the number of terms in the Taylor
expansion. Through replacing the L operator with Lγ= U>ΛγU, the new optimization formulation
leaves us with the following expression:
min {2F∙TDγ-1LγDTF∙k + μ(F∙k - Yk)TDγσ-1(F∙k - Yk)},	⑸
F
where (Dγ)ii = (Lγ)ii.
Let 0 < γ < 1, the closed form solution for Eq. 5 can be obtain as fellow:
Fk = (1 — α)(l - αD-σ WY。71) — 1 Yk,	(6)
for k = 1, . . . , K . Therefore, we can conclude three particular fractional semi-supervised learning
mehtods like G-SSL:
•	Fractional SL method (σ = 1): F∙k = (1 - α)(I - αD-1Wγ) 1 Yk
•	Fractional NL method (σ = ɪ): F∙k = (1 - α)(I - αDγ2 WYDγ2)	Y∙k
•	Fractional PR method (σ = 0): F∙k = (1 - α) (I - αWγD-1) 1 Yk
3.2	Proposed Fractional Generalized Graph Convolutional Networks for
semi-supervised node classification
Although both G-SSL and fractional G-SSL achieve comparable and consistent (low variance) per-
formance on some datasets, e.g., Les Miserables, Wikipedia-math, and MNIST, these approaches
consider only the given adjacency matrix W and the label matrix Y without using the feature ma-
trix X. Such limitation is crucial especially when dealing with datasets that not only exhibit a
sophisticated topological graph structure but also provide node feature information, such as cita-
tion, biological, financial, and power grid networks. To address this drawback, through using the
feature matrix X instead of the label matrix Y and encoding the graph structure by using neural net-
work framework, many graph-based neural networks methods, e.g., graph convolutional networks
(GCN), have been recently proposed and have been shown to demonstrate very impressive advances
in semi-supervised learning performance on graphs. Next, we turn to discussing the new Fractional
Generalized Graph Convolutional Networks (FGCN) for semi-supervised node classification.
Fractional Generalized Graph Convolutional Networks (FGCN). The key idea behind our
proposed method is Fractional Generalized Sigma-based (FGS) filter gFGS (α, σ, γ) = (1 -
α) (I - aDγσWγDγ-1) 1 = (1 - α)(I - αL)-1. Let us insert Taylor series expansion into
the expression of the FGS filter and this has the advantage of avoiding the inverse computation. We
have the following expression:
gF GS (α, σ, γ)
∞i
(1 — α)(I + (αL)1 + (αL)2 + (aL)3 + •一 ) = (1 — α))： (αL),
i=0
(7)
4
Under review as a conference paper at ICLR 2020
where 0 < α ≤ 1, 0 ≤ σ ≤ 1, 0 < γ ≤ 1. We then obtain the general classification function by
multiplying the feature matrix X :
X = gFGS (α,σ,γ )X
=(1 - α)( |X} + aLX + α2L2X} + …)，	⑻
the first item the second item the third item
where, recursively,
0	00
(X )i = X + αL(X )i-ι,	(X )0 = X, (i = 1, 2,…).	(9)
Convolutional layer. During the training process, the convolutional model need to train the trainable
parameters (W, b) of the graph filter which can scan the given input feature matrix into a series of
feature maps with neurons. Thereby, we provide an implementation of Eq. 8 as a FGS convolutional
layer:
∞i
(1 - α) X 卜LyH(t)W(t) ʌ ,	(10)
where H(t+1) is the hidden layer output matrix of activations in the t-th layer and H⑼ = X, σ(∙) is
the adopted activation function, and Wt is the trainable weight in the t-th layer. Besides, we borrow
the concept of the parallel system in reliability theory to improve the consistency of our proposed
method. A parallel system is a configuration that the entire system works as long as not all involved
components in the system fail. Hence, the parallel system structure is more robust against noisy
inputs, compared to a single system structure.
Lemma 1. Let XFGS be the output matrix Pi from a pooling layer. Let U = {1, 2,..., N} be
(i)
a finite population such that each unit i, i ∈ U is associated with an output matrix XF GS, i =
1, . . . , N. Then
Var(XFGS )=(1 — N) S,	n<N,n ∈ Z+,	(11)
where S2 = Pi=1 XFGS - XFUGS ) /(N - 1).
Suppose there are n components in a parallel system, with the probability of non-failure PR(i) (where
i = 1,… ,n) in a parallel system, then the reliability of this parallel system PRS Can be obtained
with the following expression:
n
PRS = 1 - Y(1 - PRi) = 1 - (1 - PRI)) × (1 - pR2)) × …× (1 - PRn).	(12)
i=1
According to Lemma 11, the parallel system allows for enhancing stability and reducing estimation
variance UP to order of n (i.e., Var(XFGS) = O(S2∕n)). In this way, we establish both theoretical
and practical guarantees for our proposed model to reach stable over a large set of hyperparameters,
small datasets, and noisy labels based on this parallel implementation.
5
Under review as a conference paper at ICLR 2020
Figure 1: Illustration Fractional Generalized Graph Convolutional Network model. The input is the
feature matrix X and the graph within dotted circle represents embedding LeVy Flights into random
walks on graph (where Lγ is the Laplacian matrix L to a power γ). FGCN architecture consists of
three main components: (i) FGS conVolutional layer with parallel structure; (ii) gated max-aVerage
pooling layer; (iii) actiVation block for residual learning.
Pooling layer. When implementing the form of pooling operation to aggregate information from
the outputs of parallel FGS conVolutional layer, instead of using some popular pooling functions
such as max and aVerage pooling, we apply the state-of-art pooling operation - gated max-average
pooling (Lee et al., 2016) to capture the local and global information from all the nodes and graph
structure. The rationale behind the gated max-average pooling, is that it considers “responsiVe”
strategy (i.e., improVing translation inVariance and scale inVariance Via considering input in each
gating mask) based on the mixed max-average pooling equation. We haVe:
fgate(XFGS) = σ(W>XFGS) fmaχ(XFGS) +(1 — σ(W>XFGS)) favg(XFGS),	(13)
where W is the trainable weight matrix, XFGS is the output matrix from the parallel FGS conVolu-
tional layer after concatenation operation.
Residual building block. Inspired by residual learning, we apply a residual block by adding the
skip connection after the pooling layer. One of the adVantages of the residual learning is the identity
mapping which proVides a direct path for propagating information. When using the residual building
block, we adopt a similar scheme in He et al. (2016) to deal with the output of the pooling layer. Let
H(x) be an underlying mapping and we cast it as H(x) = F(x) + x, where F(x) is the residual
mapping defined by H(x) - x. In other words, it is easier to optimize the residual mapping F (x)
than optimizing the direct mapping H(x) and helps to aVoid the gradient Vanishing problem during
training. To this end, we use an exponential linear unit (ELU) in direct mapping and place a rectified
linear unit (ReLU) after addition in our model.
4	Experiments
4.1	Datasets
Undirected networks. Cora-ML (this Cora dataset consists of Machine Learning papers), Cite-
Seer and PubMed are three standard citation networks benchmark datasets used for semi-superVised
learning eValuation. In these citation networks, nodes represent publications, edges denote citation,
the input feature matrix are bag of words and label matrix contain the class label of publication. We
use the same data format as GCN, i.e., 20 labels per class in each citation network.
Directed networks. We eValuate our method on four directed networks - Cora, IEEE 118-bus sys-
tem (IEEE bus), Texas 2000-bus system (TX bus), and South Carolina 500-bus system (SC bus).
Unlike the small subset of the Cora-ML dataset (aboVe), Cora is extracted from original dataset
which contains more than 50, 000 computer science research papers, where nodes represent scien-
tific papers and directed edges between node u and node v (u → v) represent citation of paper v
in paper u. In addition to citation networks, we also test our method on power grid networks. For
6
Under review as a conference paper at ICLR 2020
IEEE 118-bus system, we consider a (unweighted-directed) graph as a model for the IEEE 118-
bus system where nodes represent units such as generators, loads and buses, and edges represent the
transmission lines. The input features of power grid network are generator active power upper bound
(PMAX) and real power demand (PD) obtained from MATPOWER case struct. For Texas 2000-bus
system and South Carolina 500-bus system, we treat them as weighted directed power grid networks.
In particular, We use total line charging susceptance (BR-B) as edge weight and input features are:
(i) real power demand (PD); (ii) reactive power demand (QD); (iii) voltage magnitude (VM); (iv)
voltage angle (VA); (v) base voltage (BASE-KV). On directed networks, we test them separately
from two scenarios: (i) on Cora, the number of features are decomposed into a low dimension (130
components) based on principal component analysis (PCA), the dataset is trained with 10% label
rate and evaluated in another 10% fraction whose labels are withheld; (ii) on IEEE 118-bus system,
Texas 2000-bus system, and South Carolina 500-bus system, 10% label rate for training set, 20%
for validation and 70% for test sets. The statistics of data we used in the experimental section are
summarized in Table 1.
Table 1: Dataset statistics.
Dataset	Vertices	Edges	Features	Classes	Label rate	Train/Valid/Test
Cora-ML	2,708	5,429	1,433	7	0.052	140/ 500/ 1,000
CiteSeer	3,327	4,732	3,703	6	0.036	120/500/ 1,000
PubMed	19,717	44,338	500	3	0.003	60/500/ 1,000
Cora	19,793	65,311	8,710	70	0.100	1,979 / 1,979 / 1,979
IEEE Bus	118	182	2	3	0.100	11/35/70
TX Bus	2,000	2,668	5	3	0.100	200 / 400 / 1,400
SC Bus	500	584	5	3	0.100	50/100/350
4.2	Setup and Baselines
Baselines. In this setting, on undirected networks, we compare FGCN with the state-of-the
art semi-supervised classification approaches including (i) using the label matrix as input: label
propagation (LP) (Zhu & Ghahramani, 2002); (ii) using the feature matrix as input: DeepWalk
(DW) (Perozzi et al., 2014), graph attention networks (GAT), GNN with ChebNet polynomials
filter (ChebNet), GCN, GNNs with convolutional ARMA filters (ARMA), Graph Markov Neural
Networks (GMNN) (Qu et al., 2019), and Large-Scale Learnable Graph Convolutional Networks
(LGCNs) (Gao et al., 2018). In addition, on undirected networks, we use MotifNet (using sym-
metric motif adjacency matrix instead of Normalized Laplacian matrix in multivariate polynomial
filters), ChebNet, GCN, ARMA, GMNN, and LGCNs as the benchmarks.
Training setting details. The training is done by using Adam optimizer with learning rate lr1 =
0.01 for undirected networks and lr2 = {0.1; 0.001} for directed networks. To prevent the model
from overfitting, we consider both adding dropout layer before two graph convolutional layers and
kernel regularizers ('2) in each layer. For undirected networks: we follow the experimental setup in
Kipf & Welling (2017) to set the parameters of baselines (GAT, ChebNet, GCN, ARMA, GMNN,
and LGCNs) by using two graph convolutional layers with 16 hidden units, `2 regularization term
with coefficient 5 × 10-4, and dropout probability pdrop. of 0.5 (for ARMA, except for number
of hidden units, the hyperparameters setting are significantly different from others). For directed
networks: we consider the MotifNet setting with using dropout rate pdrop. of 0.5, `2 regularization
term with coefficient 0.001, and the degree of multivariate polynomials k = 4 for MotifNet; for
other three baselines (i.e., ChebNet, GCN, ARMA, GMNN, and LGCNs), we also use a two-layer
network but with 16 hidden units, learning rate lr2 with 0.001, 0.5 dropout rate and `2 regularization
weight of 5 × 10-4. In the following, Table 2 displays the best hyperparameter configurations of
FGCN for each dataset by using standard grid search mechanism (the optimal kernel regularization
weight `2 always equal to 5 × 10-4).
7
Under review as a conference paper at ICLR 2020
Table 2: Hyperparameters setting of FGCN
Dataset	pdrop.	nh		α	σ	Y	nFGS	{#C[1];#C[2]}	φ(RB)
Cora-ML	0.75	32	0.6	0.50	1.000	7	{5;2}	ELU + ReLU
CiteSeer	0.75	32	0.6	0.53	1.000	7	{3;2}	ELU + ELU
PubMed	0.00	32	0.6	0.50	1.000	7	{5;2}	ELU + ReLU
Cora	0.00	128	0.4	1.00	1.000	15	{1;1}	ELU + ELU
IEEE Bus	0.00	128	0.1	0.20	0.004	40	{7;2}	ELU + ELU
TX Bus	0.00	128	0.3	0.10	0.100	14	{5;2}	ELU + ELU
SC Bus	0.00	128	0.2	0.10	0.100	20	{5;2}	ELU + ELU
* n[h1] denotes the number of hidden units in the first layer; #C[1] and #C[2] represents the number of statis-
tically independent parallel components in the first layer and the second layer respectively; nFGS denotes the
number of items in Taylor series polynomial expansion of FGS filter; φ(RB) represents the type of activation
functions in residual block structure.
5	Results
Performance comparison of semi-supervised node classification. Table 3 reports the average
accuracy delivered by FGCN and competing methods for fixed train/valid/test sizes (see Table 1).
The best performance for each dataset is marked in bold.
Table 3 shows that FGCN outperforms all competing approaches in all data sets, except for CiteSeer
(FGCN delivers the fouth best accuracy result), PubMed (FGCN delivers the second best accuracy
result), Cora (FGCN delivers the third best accuracy result after the LGCNs approach and the dif-
ference between FGCN and LGCNs for Cora is less than 0.05%). The improvement gain of FGCN
over the next most accurate method ranges from 0.75% (for undirected Cora-ML over GMNN) to
4.21% (for directed IEEE 118-Bus over GMNN). Remarkably, methods that are applicable both
to undirected and directed networks (i.e., ChebNet, GCN, ARMA, GMNN, and LGCNs) tend to
deliver noticeably lower accuracy results for a directed networks (especially on weighted-directed
networks), while the new FGCN method yields a more stable performance across both directed and
undirected networks. In turn, CiteSeer (unweighted-undirected), GMNN and LGCNs outperform
FGCN up to 1.7%. For the PubMed (unweighted-undirected), our FGCN model delivers marginally
better performance results than LGCNs, and GMNN has better performance than FGCN. Finally,
both GMNN and LGCNs outperform FGCN on CORA (unweighted-directed), but the delivered ac-
curacy results are very close. Based on the obtain results, the new FGCN approach tends to be the
most competitive and, hence, preferred node classification method for sparser networks with higher
label rates.
Furthermore, the IEEE 118-Bus data set is the smallest among the considered data (see Table 1),
and we might expect to observe lower accuracy results for this data set due to a limited training set.
However, the accuracy yielded by FGCN is among the highest ones across all data sets.
Table 3: Comparison of average accuracy (%) of semi-supervised classification approaches for both
undirected networks and directed networks.
Undirect networks	Directed networks
Method	Cora-ML	CiteSeer	PubMed	Cora	IEEE Bus	TX Bus	SC Bus
LP	68.70	46.32	^^65.92	-	-	-	-
DW	67.20	43.27	65.33	-	-	-	-
MotifNet	-	-	-	60.00	65.75	82.00	95.18
GAT	83.11	70.85	78.56	-	-	-	-
ChebNet	81.45	70.23	78.40	58.93	60.00	80.04	94.13
GCN	81.50	71.11	79.00	57.75	52.86	73.36	90.33
ARMA	82.80	72.30	78.80	58.99	70.55	81.20	94.33
GMNN	83.72	73.10	81.80	61.20	78.88	86.21	96.57
LGCNs	83.35	73.08	79.51	60.72	71.43	85.57	95.14
FGCN (ours)	84.35	71.89	79.56	60.70	82.20	87.74	97.61
8
Under review as a conference paper at ICLR 2020
Evaluation of FGCN-specific parameters. During grid search over three parameters (i.e., α, σ, and
γ), we find that: (i) the regularization parameter α which used to specify the relative importance of
a graph in clustering strongly relates to the probability of initial conditions for random walks when
the self-refreshing process works, and it strongly influences the network’s generalization ability and
node classification performance for all datasets; (ii) the free unifying parameter σ provides enough
flexibility to construct a canonical formulation of different graph-based semi-supervised methods.
Table 3 indicates that the optimal value of σ depends on both the types of networks (undirected and
directed) and label rate not on the size of network; (iii) the fractional power parameter γ substantially
impacts the accuracy of node classification for the small datasets (see e.g., Figure 2), however, no
similarly strong influence is found in the larger datasets.
Sensitivity analysis. In the sensitivity analysis setting, we have the ability to analyze the sen-
sitivity of the node classification accuracy to variation from three FGCN-specific parameters -
α ∈ {0.1,…，1}, σ ∈ {0,0.1,…，1}, and Y ∈ {0.001,0.01,0.1,1}. In this case, We only show
the results from sensitively analysis for FGCN model on IEEE 118-bus dataset. First, we perform
the parameter learning experiments on four scenarios with a fixed parameter γ . Figure 2 shows
that the accuracy substantially decreases when α is larger than 0.8, especially in γ equals to 0.001
and 0.01 (see Figure 2a, 2b). Setting γ = {0.1, 1}, we observe that the classification accuracy
nearly monotonic decreases while increasing α. Additionally, there is little difference of classifica-
tion accuracy between γ = 0.001 and γ = 0.01 and FGCN generally gives consistent and higher
accuracy when the α parameter is within the range of {0.1, 0.2, 0.3, 0.4}. We then explore the vari-
ation of accuracy based on tuning parameter Y within the range of [0.001,0.002, ∙∙∙ , 0.01] (setting
σ ∈ {0,0.1, ∙∙∙ , 1} at the same time), however, it is hard to obtain the optimal (σ, Y) combination
through gathering finite experimental results (100 runs) since some of the results are very close.
Therefore, we run the following experiments to demonstrate the impact evaluation of Y :
•	Step 1: Setting Y ∈ {0.001,0.002,…,0.1}.
•	Step 2: For each Y (i.e., fix the Y value in each experiment), we run our proposed model 100
times separately for σ from 0 to 1 by 0.01; Then we can obtain the 100 average accuracies
for each Y under fixed γ, that is, {AACσ=o, ACCσ=o 01, ∙∙∙ , AACσ=ι}[i], where i =
1,…，10.
•	Step 3: Fitting the Gaussian distribution to {AACσ=o, ACCσ=o 01,…,AACσ=ι}^[i (see
Figure 3b).
Similar to y, we can fit the Gaussian distribution to {AACγ=0.001,ACCγ=0.002, ∙…，AACγ=o.oι}[j]
by fixing σ, where j = 1, ∙∙∙ , 11 (see Figure 3a).
From Figure 3, we find that there exist larger difference between the shapes of approximate Gaussian
distributions by fixing the parameter σ than fixing the parameter Y which means the parameter σ is
more important factor in FGCN approach for small datasets.
6 Conclusion
In this paper we have proposed a new Fractional Generalized Graph Convolutional Networks
(FGCN) method to semi-supervised learning on graphs that enables to better capture the intrin-
sic local graph topology. The key idea behind our new approach is a new fractional generalized
graph-based convolutional filter which casts the Levy Flights into random walks on graphs and, as a
result, allows for a more efficient, accurate and robust exploration of the local graph structure which
often plays the key role in graph learning performance, especially for heterogeneous graphs.
Our numerical studies have indicated that the new FGCN method tends to outperform all competing
deep learning approaches on both unweighted-directed and unweighted-undirected graphs in all
considered data sets, except of Cora for which the difference between the best result delivered by
the GMNN and FGCN is less than 0.85%. The gain in learning accuracy of FGCN over the next
best competitor ranges from 0.75% to 4.21%, and the highest gain has been achieved for the IEEE
118-Bus data set which is the smallest among the considered data sets. Furthermore, in contrast to
the competing approaches, FGCN tends to deliver a more stable performance across directed and
undirected networks regardless of the label rate.
9
Under review as a conference paper at ICLR 2020
6.0 8∙0 z.0 9∙0 9.0 『0
Aoe」n。。" a6e,!aAα
Aoe」n。。" a6e,!aAa
6.0 8∙0 z.0 9∙0 9.0 『0
6∙0 8.0 z∙0 9.0.o.o
Aoe」n。。" a6e,!aAα
(c) γ = 0.1
6∙0 8.0 z∙0 9.0.o.o
Aoe」n。。" a6e,!aAa
(d) γ = 1.0
Figure 2: Accuracy of FGCN for PageRank, Normalized Laplacian, and Standard Laplacian, which
depends on the function of α and fixes the fractional power γ = {0.001 (a), 0.01 (b), 0.1 (c), 1.0 (d)}
ZL.0 80.0 寸0.0
A=SUea
0.65	0.70	0.75	0.80	0.85	0.90
Average accuracy: γ ∈ [0.001, 0,01]
(a) Density of accuracy depends on fixing σ
Figure 3: Generalized Gaussian density of accuracy of FGCN: (a) the solid curves represent three
particular scenarios in FGCN, where the red curve denotes the PR-based method (σ = 0), green
curve denotes the NL-based method (σ = 0.5), and blue curve denotes the SL-based method (σ =
1); the dotted curves represent common scenarios in FGCN model, that is, the generalized parameter
σ falls within the range of (0, 0.5) ∪ (0.5, 1). (b) the red, green, blue dotted curve represents the
scenario when setting the fractional parameter γ equals to 0.001, 0.005, and 0.01, respectively; The
solid curves represent scenarios where γ in the range of (0.001, 0.005) ∪ (0.005, 0.01).
ZL.0 80.0 寸0.0
SUea

0.65	0.70	0.75	0.80	0.85	0.90
Average accuracy: σ ∈ [0, 1 ]
(b) Density of accuracy depends on fixing Y
10
Under review as a conference paper at ICLR 2020
In the future we plan to advance the proposed FGCN technique to learning on multilayer networks
and to enhance graph learning process with topological information on the underlying deep neural
network.
References
K. Avrachenkov, A. Mishenin, P. Goncalves, and M. Sokol. Generalized optimization framework
for graph-based semi-supervised learning. In Proceedings of the 2012 SIAM International Con-
ference on Data Mining,pp. 966-974. SIAM, 2012.
Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks
with convolutional arma filters. arXiv preprint arXiv:1901.01343, 2019.
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning:
going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42, 2017.
J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via impor-
tance sampling. arXiv preprint arXiv:1801.10247, 2018.
S. De Nigris, E. Bautista, P. Abry, K. Avrachenkov, and P. Goncalves. Fractional graph-based semi-
supervised learning. In 25th EUSIPCO, pp. 356-360, 2017.
M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with
fast localized spectral filtering. In NIPS, pp. 3844-3852, 2016.
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional
networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 1416-1424. ACM, 2018.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Ernest J Henley and Hiromitsu Kumamoto. Reliability engineering and risk assessment, volume
568. Prentice-Hall Englewood Cliffs (NJ), 1981.
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In
ICLR, 2017.
Chen-Yu Lee, Patrick W Gallagher, and Zhuowen Tu. Generalizing pooling functions in convo-
lutional neural networks: Mixed, gated, and tree. In Artificial intelligence and statistics, pp.
464-472, 2016.
R. Levie, F. Monti, X. Bresson, and M. M. Bronstein. Cayleynets: Graph convolutional neural
networks with complex rational spectral filters. IEEE Transactions on Signal Processing, 67(1):
97-109, 2018.
F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric deep
learning on graphs and manifolds using mixture model cnns. In CVRPR, pp. 5115-5124, 2017.
F. Monti, K. Otness, and M. M. Bronstein. Motifnet: a motif-based graph convolutional network for
directed graphs. In 2018 IEEE DSW, pp. 225-228, 2018.
F. et al. Monti. Geometric deep learning, 2019. http://geometricdeeplearning.com/.
Andrzej S Nowak and Kevin R Collins. Reliability of structures. CRC Press, 2012.
Omer Ozturk and Narayanaswamy Balakrishnan. Constructing quantile confidence intervals using
extended simple random sample in finite populations. Statistics, pp. 1-15, 2019.
B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 701-710. ACM, 2014.
11
Under review as a conference paper at ICLR 2020
Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. arXiv preprint
arXiv:1905.06214, 2019.
Charles T Scott and Michael KohL Sampling with partial replacement and stratification. Forest
Science, 40(1):30-46, 1994.
Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson. Structured sequence modeling with graph
convolutional recurrent networks. In International Conference on Neural Information Processing,
pp. 362-373. Springer, 2018.
P. Velickovic, G. CUCUrUlL A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention net-
works. arXiv preprint arXiv:1710.10903, 2017.
Chak-KUen Wong and Malcolm C. Easton. An efficient method for weighted sampling withoUt
replacement. SIAM Journal on Computing, 9(1):111-113, 1980.
F. WU, T. Zhang, A. H. SoUza Jr, C. Fifty, T. YU, and K. Q. Weinberger. Simplifying graph convolU-
tional networks. arXiv preprint arXiv:1902.07153, 2019.
D. ZhoU and C. JC BUrges. Spectral clUstering and transdUctive learning with mUltiple views. In
ICML, pp. 1159-1166, 2007.
D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scholkopf. Learning with local and global
consistency. In NIPS, pp. 321-328, 2004.
X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002.
X. Zhu and A. B. Goldberg. Introduction to semi-supervised learning. Synthesis lectures on artificial
intelligence and machine learning, 3(1):1-130, 2009.
C. Zhuang and Q. Ma. Dual graph convolutional networks for graph-based semi-supervised clas-
sification. In Proceedings of the 2018 World Wide Web Conference, pp. 499-508. International
World Wide Web Conferences Steering Committee, 2018.
A Appendix
Proof of Lemma 1: The parallel structure (Henley & Kumamoto, 1981) is constructed in a sim-
ilar spirit as bagging of features in random forest and other ensemble learning methods. The key
rationale behind the parallel structure is to reduce variance and increase stability.
Let us first consider (11). Suppose that Υ is a random sample drawn without replacement from a
finite population U = {1, 2,...,N} according to a sampling design p (∙). Each unit i,i = 1,...,N
(i)
of U is associated with XFGS, i.e. the i-th output matrix from FGS convolution for new feature
matrix X%. Probability of choosing sample U is Pr(Y = U) = P(Y) > 0 for all U ∈ U and U = 0.
Let Z be the indicator variable such that
1,	if Xi is in the sample
0,	otherwise.
(14)
Hence, probability that the unit i, i = 1, . . . , N is selected, is given by πi = E(Zi), and proba-
bility that units i and j, i, j = 1, . . . , N are selected simultaneously is πij = E(ZiZj); πi and
πij are called first order inclusion probability and second order inclusion probability, respectively.
Since in the current paper, we consider a simple random sampling design of n units XF(iG) S without
replacement from U, πi = n/N and πij = n(n - 1)/N (N - 1).
(i)	N	(i)	(1)	(N)
Let XFGS =	i∈N XFGS/n =	i=1 ZiXFGS/n. Hence, given {XF GS , . . . , XFGS }, we find
that
N	X(i)	N X(i)	N	X(i)	N X(i)
EXFGS ] = X E [Zi]XFGS = X ∏iXFGS = X nxFGS- = X XFGS = XU
n	n	Nn	N
i=1	i=1	i=1	i=1
12
Under review as a conference paper at ICLR 2020
where XFGS is be the output matrix Pi from a pooling layer (See Figure 1), which implies that
XFGS is an unbiased estimator of population mean XFGS.
In turn,
Var(XFGS) = n2 V (X ZiXFGS
3 Cov
n2
XZiXF(iG) S,XZjXF(jG)S
FGS	FGS
i=1
j=1
1
n2
N	NN
X(XF(iG)S)2Var(Zi)+XXXF(iG)SXF(jG)SCov(Zi,Zj)
i=1	i=1 j6=i
NN
N(1- N X(χFiG S )2- XX XFG S XFG S
i=1 j 6=i
i=1
1
n2
N
NX(XF(iG)S)2
i=1
(N- 1)X(XF(iG)S)2 - X XF(iG) S
i=1
i=1
N2
X XF(iG) S
2+XN (XF(iG)S)2
i=1
N
N
N
1
N - 1
n
N
N
N
—
(15)
where S2 = PN=I (XFGS - XUGS) /(N - 1) and the factor (1 - n/N) is called the finite popu-
lation correction (FPC). Hence, the proposed PS structure decreases an original estimation variance
S by order of n. Note that we can consider other sampling designs p(∙), including, for example,
weighted sampling and p-extended simple random sampling with replacement (see discussion by
Wong & EaStOn,1980; Scott & Kohl,1994; Ozturk & Balakrishnan, 2019, and references therein).
The stability result of the parallel structure (Nowak & Collins, 2012) follows from verbatim appli-
cation of the probability bound on the intersection of independent events.
B Appendix
The general idea of graph-based semi-supervised learning (G-SSL) is based on two widely used
optimization frameworks. The first formulation, the Standard Laplacian based formulation (Zhou &
Burges, 2007) as follows:
)N N	N	I
XXwij kFi. - Fj.k + μXdi kFi. - Yi.k2,	(16)
where dii is (i, i)-element in degree matrix D and wij represents the edge weight for edge eij in ad-
jacency matrix W. For the second formulation, the Normalized Laplacian based formulation (Zhou
et al., 2004), is as follows:
min{X X Tl 袅-Pfe I +μ XkFi.- γif}	Cn)
The following lemma (Avrachenkov et al., 2012) asserts that the generalized optimization frame-
work, i.e., G-SSL, which has as particular cases the two above mentioned formulations:
13
Under review as a conference paper at ICLR 2020
Lemma 2. Let σ denote an alternative parameter on the power of degree matrix D whose entries
are the degrees dii; and let 0 ≤ σ ≤ 1. Then
(N N	N	)
XXWijw1Fi. -dσ-1FjJI2 + μXd2iσ-1 kFi. -Yi.k2	.
i=1 j=1	i=1	
The classification functions for the generalized semi-supervised learning are given by
F∙k = (1 - α) (I - aD-σWDσ-1)-1 Y∙k.
Proof: The objective function of the generalized semi-supervised learning framework (i.e.,
Lemma 2.) can be rewritten in the following matrix form:
KK
Q(F) = 2 X FTDσ-1LDσ-1F.k + μ X (F.k - Yk)t D2σ-1 (F.k - Y.k).
k=1	k=1
Given the first order optimality condition DF.k Q(F) = 0, we have
2FT (Dσ-1LDσ-1 + Dσ-1LTDσ-1) + 2μ (F.k - Y.k)T D2σ-1 = 0.
Multiplying the above expression from the right hand side by D-2σ+1 leads to
2FT (Dσ-1 (L + LT) D-σ) + 2μ (F.k - Y.k)T = 0.
Then, substituting L = D - W and rearranging the terms yields
F.T (2I - Dσ-1 (W + WT) D-σ + μi) - 〃Y：T = 0.
Since the resulting adjacency matrix W is a symmetric matrix (see transformation 3 in the main
body of the paper through replacing μ with α = 2/(2 + μ)), We obtain
FT = μVT (2I - 2Dσ-1WD-σ + μI)-1,
which concludes the proof.
14