Under review as a conference paper at ICLR 2020

GENERALIZED  TRANSFORMATION-BASED  GRADIENT

Anonymous authors

Paper under double-blind review

ABSTRACT

The reparameterization trick has become one of the most useful tools in the field of
variational inference.  However, the reparameterization trick is based on the stan-
dardization transformation which restricts the scope of application of this method
to distributions that have tractable inverse cumulative distribution functions or are
expressible as deterministic transformations of such distributions.  In this paper,
we generalized the reparameterization trick by allowing a general transformation.
Unlike other similar works, we develop the generalized transformation-based gra-
dient model formally and rigorously.  We discover that the proposed model is a
special case of control variate indicating that the proposed model can combine
the advantages of CV and generalized reparameterization. Based on the proposed
gradient model, we propose a new polynomial-based gradient estimator which has
better theoretical performance than the reparameterization trick under certain con-
dition and can be applied to a larger class of variational distributions. In studies of
synthetic and real data, we show that our proposed gradient estimator has a signif-
icantly lower gradient variance than other state-of-the-art methods thus enabling
a faster inference procedure.

1    INTRODUCTION

Most machine learning objective function can be rewritten in the form of an expectation:

L = Eqθ (z)[fθ(z)]                                                              (1)

where θ is a parameter vector. However, due to the intractability of the expectation, it’s often 
impos-
sible or too expensive to calculate the exact gradient w.r.t θ, therefore it’s inevitable to 
estimate the
gradient    θ   in practical applications. Stochastic optmization methods such as reparameterization
trick and score function methods have been widely applied to address the stochastic gradient esti-
mation problem. Many recent advances in large-scale machine learning tasks have been brought by
these stochastic optimization tricks.  Like in other stochastic optimzation related works, our paper
mainly focus on variational inference tasks.

The primary goal of variational inference (VI) task is to approximate the posterior distribution in
probabilistic  models  (Jordan  et  al.,  1999;  Wainwright  &  Jordan,  2008).   To  approximate  
the  in-
tractable posterior p(z x) with the joint probability distribution p(x, z) over observed data x  and
latent random variables z given, VI introduces a parameteric family of distribution qθ(z) and find
the best parameter θ by optimizing the Kullback-Leibler (KL) divergence DKL(q(z; θ)  p(z x)).
The performance of VI methods depends on the capacity of the parameteric family of distributions
(often measured by Rademacher complexity) and the ability of the optimizer.   In this paper,  our
method tries to introduce a better optimizer for a larger class of parameteric family of 
distributions.

The main idea of our work is to replace the parameter-independent transformation in reparame-
terization trick with generalized transformation and construct the generalized transformation-based
(G-TRANS) gradient with the velocity field which is related to the characteristic curve of the 
sublin-
ear   partial differential equation associated with the generalized transformation. Our gradient 
model
further generalizes the G-REP (Ruiz et al., 2016) and provides a more elegant and flexible way to
construct gradient estimators. We mainly make the following contributions:

1.  We develop a generalized transformation-based gradient model based on the velocity field
related to the generalized transformation and explicitly propose the unbiasedness constraint

1


Under review as a conference paper at ICLR 2020

on the G-TRANS gradient.  The proposed gradient model provides a more poweful and
flexible way to construct gradient estimators.

2.  We show that our model is a generalization of the score function method and the reparame-
terization trick. Our gradient model can reduce to the reparameterization trick by enforcing
a transport equation constraint on the velocity field. We also show our model’s connection
to control variate method.

3.  We propose a polynomial-based gradient estimator that cannot be induced by any other
existing generalized reparameterization gradient framework, and show its superiority over
similar works on several experiments.

The rest of this paper is organized as follows.   In Sec.2 we review the stochastic gradient varia-
tional inference (SGVI) and stochastic gradient estimators.  In Sec.3 we propose the generalized
transformation-based gradient. In Sec.4 we propose the polynomial-based G-TRANS gradient esti-
mator.                      In Sec.5 we study the performance of our gradient estimator on 
synthetic and real data.  In
Sec.6 we review the related works. In Sec.7 we conclude this paper and discuss future work.

2    STOCHASTIC  VARIATIONAL  INFERENCE

To   obtain   the   best   variational   parameter   θ,    rather   than   minimize   the   KL   
divergence
DKL(q(z; θ)  p(z x)), we usually choose to maximize the evidence lower bound (ELBO) (Jordan
et      al., 1999),

L(θ) = Eq₍z;θ₎[f (z)] + H[q(z; θ)]                                                (2)

where f (z) = log p(x, z) and H[q(z; θ)] = Eq₍z;θ₎[    log q(z; θ)].  The entropy term H[q(z; θ)] is
often assumed to be available analytically and usually omitted in the procedure of stochastic opti-

mization. This stochastic optimization problem is the basic setting for our method and experiments.
Without extra description, we only consider the simplified version of the ELBO:

L = Eq₍z;θ₎[f (z)]                                                              (3)

Generally, this expectation is intractable to compute, let alone its gradient.  Therefore, a common
stochastic optimization method for VI task is to construct a Monte Carlo estimator for the exact
gradient of the ELBO w.r.t θ. Among those gradient estimators, the score function method and the
reparamterization trick are most popular and widely applied.

Score function method.  The score function estimator, also called log-derivative trick or reinforce
Glynn (1990); Williams (1992) is a general way to obtain unbiased stochastic gradients of the ELBO
(Paisley et al., 2012; Ranganath et al., 2014; Mnih & Gregor, 2014). The simplest variant of the 
score
function gradient estimator is defined as:

∇θL = Eqθ (z)[f (z)∇θ log qθ(z)]                                                 (4)
and then we can build the Monte Carlo estimator by drawing samples from the variational distribu-
tion qθ(z) independently.

Although the score function method is very general, the resulting gradient estimator suffers from
high variance.  Therefore,  it’s necessary to apply variance reduction (VR) methods such as Rao-
Blackwellization (Casella & Robert, 1996) and control variates (Robert & Casella, 2013) in 
practice.

Reparameterization trick.  In reparameterization trick, we assume that there is an invertible and
continuously differentiable standardization function φ(z, θ) that can transform the variational dis-
tribution q(z; θ) into a distribution s(ρ) that don’t depend on the variational parameter θ as 
follows,

φ(z, θ) = ρ ∼ s(θ),       z = φ−¹(ρ)

Then the reparameterization trick can turn the computation of the gradient of the expectation into
the expectation of the gradient:

2


Under review as a conference paper at ICLR 2020

∇θEq ₍z₎[f (z)] = Es₍ρ₎[∇θf .φ−¹(ρ)Σ]

Although this reparameterization can be done for many commonly used distributions, such as the
Gaussian distribution, it’s hard to find appropriate standardization functions for a number of 
standard
distributions, such as Gamma, Beta or Dirichlet because the standardization functions will 
inevitably
involve special functions. On the other hand, though the reparameterization trick is not as 
generally
applicable as the score function method, it does result in a gradient estimator with lower 
variance.

3    THE  GENERALIZED  TRANSFORMATION-BASED  GRADIENT  MODEL

Define a random variable ρ by an invertible differentiable transformation ρ = φ(z, θ), where φ is
commonly called generalized standardization transformation (Ruiz et al., 2016) since it’s dependent
on the variational parameter θ.

Theorem 3.1.  Let θ be any component of the variational parameter θ, the probability density func-
tion of ρ be w(ρ, θ) and L = Eq₍z,θ₎[f (z)]. Then

(1)  ∂ʷ (ρ, θ)|ρ₌φ₍z,θ₎ =         ¹        ( ∂ q(z, θ) + ∇z · (q(z, θ)vθ(z, θ)))

∂θ                                     |∇zφ(z,θ)|   ∂θ

(2)  ∂L = Eq₍z,θ₎[ ∂ ˡᵒᵍ q⁽ᶻ,θ⁾ f (z) +     ¹     ∇z · (f (z)q(z, θ)vθ(z, θ))]


∂θ

where

∂θ                       q(z,θ)

vθ(z, θ) =  dz  = −(∇ φ(z, θ)) ∂φ(z, θ)

(5)

vθ(z, θ) is often referred to as velocity field, and the Equ.5 above is called velocity field 
equation.


constraint:

∂θ           ∂θ

Eqθ

 1

[ q  ∇z

· (fqθ

vθ)] = 0                                                        (6)

The proof details of the Theorem.3.1 are included in the Appendix.A.1.

We  refer  to  the  gradient   ∂L  with  vθ  satisfying  the  unbiasedness  constraint  as  
generalized

transformation-based  (G-TRANS)  gradient.   We  can  construct  the  G-TRANS  gradient  estimator
by choosing vθ of specific form.  In the following, we demonstrate that the score function gradient
and reparameterization gradient are special cases of our G-TRANS gradient model associating with
special velocity fields.

Remark.  The score function method is a special case of the G-TRANS model when f (z)q(z, θ)vθ =

const.

The standardization function φ doesn’t depend on the parameter θ when vθ =  0 according to the
velocity field equation (Equ.5).  Conversely, for any φ that doesn’t depend on θ, we have vθ  =

∂φ(z,θ)

−(∇zφ(z, θ))    ∂θ       =  0, thus the resulting gradient estimator has a same variance as the 
score

function estimator.

Remark.  The reparameterization trick is a special case when ∂ʷ (ρ, θ)|ρ₌φ₍z,θ₎ = 0, that’s to say


∂

∂θ q(z, θ) + ∇z

· (q(z, θ)v  ) = 0                                                 (7)

The detailed computation to obtain the transport equation (Equ.7) is included in the Appendix.A.1.
The  transport  equation  is  firstly  introduced  by  (Jankowiak  &  Obermeyer,  2018),  however,  
their
work derive this equation by an analog to the optimal transport theory.  In 1-dimensional case, for
any standardization distributions w(ρ) that doesn’t depend on the parameter θ, the variance of the
resulting gradient estimator is some constant (for fixed θ) determined by the unique 1-dimensional
solution of the transport equation.

3


Under review as a conference paper at ICLR 2020


Remark.   Let  g(z, θ)   =   −∇z · (f (z)q(z, θ)vθ(z, θ)),  then  ∂L

=   Eq(z,θ)[ ∂ log q(z,θ) f (z)  −

     1      g(z, θ)].     C(z, θ)   =        ¹     g(z, θ)  can  be  viewed  as  a  control  
variable  with  µC   =


q(z,θ)

E[C(z, θ)] = 0.

q(z,θ)

For the existence of the velocity field vθ and the generalized standardization transformation φ(z, 
θ),
g(z, θ)  must  satisfy  some  strong  differential  constraints  (Evans,  2010).   We  can  see  
that  the  G-
TRANS model is a special case of the control variate method with a complex differential structure.
This connection to CV means our gradient model can combine the advantages of CV and generalized
reparameterization.

Theorem.3.1 transforms the generalized unbiased reparameterization procedure into finding the ap-
propriate velocity field that satisfy the unbiasedness constraint.  It’s possible to apply 
variational
optimization theory to find the velocity field with the least estimate variance, however, the 
solution
to the Euler-Lagrange equation contains f (z) in the integrand which makes it impractical to use in
real-world model (See Appendix.A.2 for details).

By introducing the notion of velocity field, we provide a more elegant and flexible way to construct
gradient estimator without the need to compute the Jacobian matrix for a specific transformation.
In   the next section, we introduce a polynomial-based G-TRANS gradient estimator that cannot be
incorporated into any other existing generalized reparameterized gradient framework and is better
than the reparameterization gradient estimator theoretically.

4    THE  POLYNOMIAL-BASED  G-TRANS GRADIENT  ESTIMATOR

N

In this section, we always assume that the base distribution q(z, θ) can be factorized as       
qi(zi, θi)

i=1

where N is the dimension of the random variable z, θi is a slice of θ and θi share no component
with θj if i /= j. We consider an ad-hoc velocity field family:


with

ₐh(z, θ) = (−

f (z)

q(z, θ)

∂θ     dzi′)                                   (8)


δ (θ) = .1    θ is a component of θi

(9)

We always assume vθ   to be continuous which guarantees the existence of the solution to the veloc-

ity field equation. We verify in the Appendix.A.3 that vθ  (z, θ) satisfy the unbiasedness 
constraint

if h(z, θ) is bounded.

It’s  easy  to  see  that  the  gradient  estimator  that  results  from  vθ   is  more  general  
than  the  score

function  method or  reparameterization  trick  since they  are  two  special cases  when  h(z, θ)  
=  0

or h(z, θ)  =  f (z)  respectively.   In this paper,  we mainly consider a more special family of 
the


θ  (z, θ):

v       (z, θ) = (−

f (z) − Pn(z, θ)   δi(θ)   ∫  zi   ∂q(z′, θ)

		

dz′)                        (10)


where

Σk           Σ

ʲ        ʲ   is an polynomial of degree

.  A dual form of

vθ    (z, θ) is that vθ  (z, θ) = (− Pn(z,θ)   δi(θ)   ∫ zi   ∂q(z',θ) dz′), but their properties 
are similar (we

 	

here. We refer to vθ     as polynomial velocity field.

Proposition 4.1.  For distributions with analytical high order moments such as Gamma,  Beta or

Dirichlet distribution, the expectation Eq₍z,θ₎[Pk(z, θ) ∂  log q] has an analytical expression.

4


Under review as a conference paper at ICLR 2020

Proof.  Eq[Pk(z, θ) ∂  log q] =   ∂ Eq[Pk] − Eq[ ∂ Pk].  Both Pk(z, θ) and  ∂ Pk(z, θ) are polyno-

mials of random variable z. Therefore, for distribution with analytical high order moments, Eq[Pk]


and Eq[ ∂ Pk] both have analytical expressions, thus so does Eq₍z,θ₎[Pk(z, θ) ∂

log q].

∂θ                                                                                                  
                              ∂θ

With Proposition.4.1, we can write the G-TRANS gradient for the polynomial velocity field as:


∂L = E

Σ ∂ log q(z, θ) f (z) +      1      ∇

· (f (z)q(z, θ)v

(z, θ))Σ


= E         Σ

∂

Pk(z, θ)

log qΣ − E

Σ 1 Σ ∂(f − Pk)

δi(θ)

∫  zi   ∂q(z′, θ)

dz′

Σ    (11)


q(z,θ)                          ∂θ

q(z,θ)     q

i

∂zi

∂θ          ⁱ

Thus we can construct a G-TRANS gradient estimator based upon the polynomial velocity field with
a sample z˜ drawn from q(z, θ):


ˆ∂L = E

∂

[Pk(z, θ)

log q](z˜) −

Σ ∂(f − Pk)

z˜i   ∂q(z',θ)

δi(θ)            ∂θ     dz′

(12)


∂θ       q(z,θ)                       ∂θ

∂z                 q(z˜, θ)        ⁱ

i

The polynomial-based G-TRANS gradient estimator has a form close to control variate, thus cannot
be induced by any other existing generalized reparameterized gradient framework. In the following,
we show that the polynomial-based G-TRANS gradient estimator performs better than the reparam-
eterization gradient estimator under some condition.

Proposition  4.2.   If  Cov(     ¹     Λ(z, θ)  · ∇z(Pk),     ¹     Λ(z, θ)  · ∇z(2f  − Pk))   >   
0  where


Λ(z, θ)   =   (δ (θ) ∫ z1

q(z,θ)

∂q(z',θ) dz′ , ..., δ

(θ) ∫ zN

q(z,θ)

∂q(z',θ) dz′  ),  then  the  gradient  estimator  re-

sulted from polynomial velocity field has a smaller variance than the reparameterization gradient
estimator.


Proof.  Since Eq[Pk(z, θ) ∂

log q] can be resolved analytically, we have


∂ log q        1

	

∂f − P

∫ zi  q (z′, θ )dz′


then by reorganizing the expression Var(− ∂f  , zi  qi(z' ,θi)dz' ) − Var(− ∂f −P

, zi  qi(z' ,θi)dz' ),  we


can prove this proposition.

∂zi

qi(zi',θi)

∂zi

qi(zi',θi)

Remark.   As  an  example  about  how  to  choose  a  good  polynomial,  for  P₁(z, θ)   =   C₀(θ)  
+

ΣN                                                                                      , zi qi(zi' 
,θi)dzi'    ∂f   , zi qi(zi' ,θi)dzi'

	 

polynomial-based G-TRANS gradient estimator that is better than the reparameterization gradi-

ent estimator according to the Proposition.4.2.   And we can adjust the value of Ci(θ)  to obtain
better performance.

According  to  the  approximation  theory,  we  can  always  find  a  polynomial  Pk(z, θ)  that  
is  close
enough to f (z), and in this case, we can dramatically reduce the variance of the resulting gradient
estimator.   For  example,  within  the  convergence  radius,  we  can  choose  Pk(z, θ)  to  be  
the  k-th
degree Taylor polynomial of f (z) with the remainder  f (z)    Pk(z, θ)  being small. In the 
practical
situation, however, it’s often difficult to estimate the coefficients of the polynomial Pk(z, θ).  
And
when k is large, we need to estimate    (Nᵏ) coefficients which is almost impossible in real-world
applications. Therefore in the following experiments, we only consider k < 2.

5


Under review as a conference paper at ICLR 2020

5    EXPERIMENT

5.1    SYNTHETIC EXPERIMENT

In this section, we use a Dirichlet distribution to approximate the posterior distribution for a 
pro-
bilistic model which has a multinomial likelihood with a Dirichlet prior.   We use Gamma distri-


      z1      

     zK     


butions to simulate Dirichlet distributions.  If zi     Gamma(αi, 1), then (    K

j=1

, . . . ,    K

j=1

zj )  ∼

Dirichlet(α₁, . . . , αK). Then the problem we study here can be written as:


∫          z       Y

Gamma(z  ; α  , 1)dz                                (13)

Eq₍z,α₎[f (z)] =      f ( Σ  z  )                        k    k


i   i

with f (z) being the multinomial log-likelihood.

k=1

We  use  shape  parameter  α =  (α₁, . . . , αK)  to  parameterize  the  variational  Dirichlet  
distribu-
tion.  To construct polynomial-based G-TRANS gradient estimator for the factorized distribution


K

k=1

Gamma(zk; αk, 1),  we need an accurate and fast way to approximate the derivative of the

lower incomplete gamma function (part of the gamma CDF) w.r.t the shape parameter.

The lower incomplete gamma function γ(α, z) is a special function and does not admit analytical
expression for derivative w.r.t. the shape parameter. However, for small α and z, we have

∞            k   k+α


∂γ(α, z)  = Σ (−1)  z       (   ln z

+    (−1)   )                                  (14)

In practice, we take the first 200 terms from this power series. And the approximation error is 
smaller
than 10−⁹ when α < 5 and z < 20 with double precision floating point number.  For large α, we
use central finite difference to approximate the derivative.  This approximation scheme for lower
incomplete gamma function can also be used to construct polynomial-based G-TRANS gradient
estimator for distributions that can be simulated by the Gamma distribution such as Beta 
distribution

and Dirichlet distribution.

We follow the experiment setting in Naesseth et al. (2017).  Fig.1 shows the resulting variance of
the first component of the gradient based on samples simulated from a Dirichlet distribution with
K = 100 components, and gradients are computed with N = 100 trials.  We use P₁(z) = c   z to
construct the G-TRANS gradient estimator, and we assign 0.2,0 and    0.1 to c successively as α₁
increases.

Results.  From Fig.1 , we can see that the IRG (Figurnov et al., 2018) method and our G-TRANS
gradient estimator has obviously lower gradient variance than the RSVI (even with the shape aug-
mentation trick (Naesseth et al., 2017)) or G-REP (Ruiz et al., 2016) method. Further, our G-TRANS
gradient estimator outperforms the IRG method when α₁ is large though there is no obvious differ-
ence between these two methods when α₁ is small.

5.2    REAL WORLD DATASET

In this section, we study the performance of our G-TRANS gradient estimator on the Sparse Gamma
deep exponential family (DEF) model (Ranganath et al., 2015) with the Olivetti faces dataset that
consists of 64     64 gray-scale images of human faces in 8 bits. We follow the Sparse Gamma DEF
setting in Naesseth et al. (2017) where the DEF model is specified by:


A

n,k

∼ Gamma .αz, Σ

αz

A

k,k

A+1

n,k'

(15)


xn,d ∼ Poisson .Σ w⁰

1

n,k

k

6


Under review as a conference paper at ICLR 2020


5

10                                                                                                  
               G − REP

RSVI (B = 0)

4                                                                                                   
                                                                                                
RSVI (B = 10)

10

G − TRANS (C = 0.2, 0, − 0.1)

IRG

3

10

−0.50  1e7

−0.75

−1.00

2                                                                                                   
                                                                                                    
                                                                                                    
                                                           −1.25

10

1                                                                                                   
                                                                                                    
                                                                                                    
                                                           −1.50

10


0                                                                                                   
                                                                                                    
                                                                                                    
                                                           −1.75

10

−2.00

            ADVI

            BBVI


−1

10

−2.25

−2

10

            G − REP

            RSVI

            IRG

            G − TRANS(c = − 10)


1.25             1.50             1.75             2.00             2.25             2.50           
  2.75             3.00

α

−2.50   0

10

1                                              2

10                              10

3                                              4

10                              10

Time [s]


Figure  1:   Polynomial-bsaed  G-TRANS  gradi-
ent estimator (this paper) obtains lower variance
compared  to  IRG,  RSVI  and  G-REP.  The  es-
timated  variance  is  for  the  first  component  of
Dirichlet  approximation  to  a  multinomial  like-
lihood  with  uniform  Dirichlet  prior  (Naesseth
et  al.,  2017).   C  is  the  polynomial  coefficient,
B denotes shape augmentation (Figurnov et al.,
2018) and optimal concentration is α = 2.

Figure 2:  Polynomial-bsaed G-TRANS gradient
estimator  (this  paper)  achieves  better  accuracy
than ADVI, BBVI (Ranganath et al., 2014), G-
REP,  RSVI.  G-TRANS  is  faster  than  the  IRG
method with comparable accuracy.  This exper-
iment apply the sparse gamma DEF model to the
Olivetti faces dataset. dataset

Here n is the number of observations,  l is the layer number,  k denotes the k-th component in a

specific layer and d is the dimension of the output layer (layer 0).  zA    is local random 
variable,


A

k,k

'  is global weight that connects different layers like deep neural networks, and xn,d denotes the

set of observations.

We use the experiment setting in Naesseth et al. (2017).  αz is set to 0.1, all priors on the 
weights
are  set  to  Gamma(0.1, 0.3),  and  the  top-layer  local  variables  priors  are  set  to  
Gamma(0.1, 0.1).
The model consists of 3 layers, with 100, 40, and 15 components in each.  All variational Gamma
distributions are parameterized by the shape and mean.  For non-negative variational parameters θ,
the transfomration θ = log(1 + exp(ϑ)) is applied to avoid constrained optimization.

In this experiment, we use the step-size sequence ρⁿ proposed by Kucukelbir et al. (2017):

ρⁿ = η · n−¹/²⁺δ · (1 + √sn)−¹

sⁿ = t (gˆⁿ)² + (1 − t)sⁿ−¹                                                        (16)

δ =  10−¹⁶, t =  0.1, η =  0.75 is used in this experiment.  The best result of RSVI is reproduced
with B = 4 (Naesseth et al., 2017). We still use P₁(z) = c · z to construct the G-TRANS gradient
estimator and we use c = −10.0 for all time.

Results. From Fig.2, We can see that G-TRANS achieves significant improvements in the first 1000
runs and exceeds RSVI though with a slower initial improvement.  G-TRANS achieves obviously
better accuracy than ADVI, BBVI, G-REP and RSVI, and keeps improving the ELBO even after
75000 runs.  G-TRANS is faster than the IRG in early training stage which means G-TRANS has
a lower gradient variance.  However, this speed advantage of G-TRANS gradually decreases as the
step size goes down in the later training stage.

6    RELATED  WORK

There are already some lines of research focusing on extending the reparameterization trick to a
larger class of distributions.  The G-REP (Ruiz et al., 2016) generalizes the reparameterization 
gra-
dient by using a standardization transformation that allows the standardization distribution to 
depend

7


Under review as a conference paper at ICLR 2020

weakly on variational parameters.  Our gradient model gives a more elegant expression of the gen-
eralized reparameterized gradient than that of G-REP which decomposes the gradient as grₑp + gcₒr.
Different from G-REP, our model hides the transformation behind the velocity field thus the expen-
sive computation of the Jacobian matrix of the transformation is evaded.  And it’s more flexible to
construct gradient estimator with the velocity field than the very detailed transformation. The RSVI
(Naesseth et al., 2017) develops a similar generalized reparameterized gradient model with the tools
from rejection sampling literatures.  RSVI introduces a score function gradient term to compensate
the gap that is caused by employing the proposal distribution of a rejection sampler as a surrogate
distribution for reparameterization gradient, although the score function gradient term can often be
ignored in practice to reduce the gradient variance at the cost of small bias. Unlike RSVI, our 
gradi-
ent estimator can be constructed with deterministic procedure which avoids the additional stochas-
ticity introduced by the accept-reject steps thus lower gradient variance.  The path-wise derivative
(Jankowiak & Obermeyer, 2018) is closely related to our model. They obtain the transport equation
by an analog to the displacement of particles, while we derive the transport euqation for reparam-
eterization gradient by rigorous mathematical deduction.The path-wise gradient model can be seen
as a special case of our G-TRANS gradient model. Their work only focus on standard reparameter-
ization gradient while our model can admit generalized transformation-based gradient. The velocity
field used in their work must conform to the transport equation while we only require the velocity
field to satisfy the unbiasedness constraint.  The implicit reparameterization gradient (IRG) (Fig-
urnov et al., 2018) differentiates from the path-wise derivative only by adopting a different method
for multivariate distributions.

There are also some other works trying to address the limitations of standard reparameterization.
Graves  (2016)  applies  implicit  reparameterization  for  mixture  distributions  and  Knowles  
(2015)
uses approximations to the inverse CDF to derive gradient estimators. Both work involve expensive
computation that cannot be extended to large-scale variational inference.  Schulman et al. (2015)
expressed the gradient in a similar way to G-REP and automatically estimate the gradient in the
context of stochastic computation graphs, but their work is short of necessary details therefore 
cannot
be applied to general variational inference task directly. ADVI (Kucukelbir et al., 2017) transforms
the random variables such that their support are on the reals and then approximates transformed
random variables with Gaussian variational posteriors.  However, ADVI struggles to approximate
probability densities with singularities as noted by Ruiz et al. (2016).

7    CONCLUSION

We proposed a generalized transformation-based (G-TRANS) gradient model which extends the
reparameterization trick to a larger class of variational distributions.  Our gradient model hides 
the
details of transformation by introducing the velocity field and provides a flexible way to construct
gradient  estimators.   Based  on  the  proposed  gradient  model,  we  introduced  a  
polynomial-based
G-TRANS gradient estimator that cannot be induced by any other existing generalized reparameter-
ization gradient framework.  In practice, our gradient estimator provides a lower gradient variance
than other state-of-the-art methods, leading to a fast converging process.  For future work, We can
consider how to construct G-TRANS gradient estimators for distributions that don’t have analyt-
ical  high-order  moments.   We  can  also  utilize  the  results  from  the  approximation  theory 
 to  find
certain kinds of high-order polynomial functions that can approximate the test function effectively
with cheap computations for the coefficients. Constructing velocity fields with the optimal 
transport
theory is also a promising direction.

REFERENCES

George Casella and Christian P Robert. Rao-blackwellisation of sampling schemes. Biometrika, 83
(1):81–94, 1996.

Lawrence C. Evans.   Partial Differential Equations.   American Mathematical Society,  2 edition,
2010. ISBN 0821849743.

Mikhail Figurnov, Shakir Mohamed, and Andriy Mnih.  Implicit reparameterization gradients.  In

NeurIPS, pp. 439–450, 2018.

8


Under review as a conference paper at ICLR 2020

Peter W. Glynn.  Likelihood ratio gradient estimation for stochastic systems.  Commun. ACM, 33
(10):75–84, 1990.

Alex   Graves.      Stochastic   backpropagation   through   mixture   density   distributions.     
 CoRR,
abs/1607.05690, 2016.

Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives beyond the reparameterization trick. In

ICML, volume 80 of Proceedings of Machine Learning Research, pp. 2240–2249. PMLR, 2018.

Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction
to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.

David A Knowles.   Stochastic gradient variational bayes for gamma approximating distributions.

arXiv preprint arXiv:1509.01631, 2015.

Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei.  Automatic
differentiation variational inference. J. Mach. Learn. Res., 18:14:1–14:45, 2017.

Andriy Mnih and Karol Gregor.  Neural variational inference and learning in belief networks.  In
ICML, volume 32 of JMLR Workshop and Conference Proceedings, pp. 1791–1799. JMLR.org,
2014.

Christian A. Naesseth, Francisco J. R. Ruiz, Scott W. Linderman, and David M. Blei. Reparameter-
ization gradients through acceptance-rejection sampling algorithms.  In AISTATS, volume 54 of
Proceedings of Machine Learning Research, pp. 489–498. PMLR, 2017.

John W. Paisley, David M. Blei, and Michael I. Jordan. Variational bayesian inference with stochas-
tic search. In ICML. icml.cc / Omnipress, 2012.

Rajesh Ranganath, Sean Gerrish, and David M. Blei.  Black box variational inference.  In AISTATS,
volume 33 of JMLR Workshop and Conference Proceedings, pp. 814–822. JMLR.org, 2014.

Rajesh Ranganath, Linpeng Tang, Laurent Charlin, and David M. Blei.  Deep exponential families.
In AISTATS, volume 38 of JMLR Workshop and Conference Proceedings. JMLR.org, 2015.

Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science & Business
Media, 2013.

Francisco J. R. Ruiz, Michalis K. Titsias, and David M. Blei.  The generalized reparameterization
gradient. In NIPS, pp. 460–468, 2016.

John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel.  Gradient estimation using
stochastic computation graphs. In NIPS, pp. 3528–3536, 2015.

Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational
inference. Foundations and Trends in Machine Learning, 1(1-2):1–305, 2008.

Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8:229–256, 1992.

9


Under review as a conference paper at ICLR 2020

A    APPENDIX

A.1    PROOF OF THEOREM.3.1

We assume that transformed random variable ρ = φ(z, θ) is of the same dimension as z.  And we
assume that there exists ψ(ρ, θ) that satisfy the constraint z = ψ(φ(z, θ), θ).

Firstly, by the change-of-variable technique, we have

w(ρ, θ) = q(z, θ)|det(∇ρψ(ρ, θ))|                                  (17)
Take derivative w.r.t θ (any component of θ) at both sizes, we have

∂                  ∂                                           ∂


∂θ w(ρ, θ) =  ∂θ |det(∇ρψ(ρ, θ))| ∗ q(z, θ) + ∂θ q(z, θ) ∗ |det(∇ρψ(ρ, θ))|

∂z

+(∇zq(z, θ) · ∂θ ) ∗ |det(∇ρψ(ρ, θ))|

(18)

With the rule of determinant derivation, we have


∂

∂θ |det(∇ρ

ψ(ρ, θ))| = |det(∇ρ

ψ(ρ, θ))| ∗ tr((∇ρ

ψ(ρ, θ))−¹  ∂

∂θ

ρψ(ρ, θ))         (19)

Substitute the Equ.19 into Equ.18, we have

∂  w(ρ, θ) = |det(∇ρψ(ρ, θ))|(tr((∇ρψ(ρ, θ))−¹  ∂ ∇ρψ(ρ, θ)) ∗ q(z, θ)

∂θ                                                                                               ∂θ

+  ∂ q(z, θ) + (∇zq(z, θ) · ∂z ))                 

= |det(∇ρψ(ρ, θ))|(tr((∇ρψ(ρ, θ))−¹  ∂ ∇ρψ(ρ, θ)) ∗ q(z, θ)

+  ∂ q(z, θ) + (∇zq(z, θ) ·  ∂  ψ(ρ, θ))

= |det(∇ρψ(ρ, θ))|(tr(∇zφ(z, θ) ∂ ∇ρψ(ρ, θ)) ∗ q(z, θ)

+  ∂ q(z, θ) + (∇zq(z, θ) ·  ∂  ψ(ρ, θ))

Since we have ∇zψ(φ(z, θ), θ) = ∇ρψ(ρ, θ)|ρ₌φ₍z,θ₎∇zφ(z, θ), then

∂  w(φ(z, θ), θ) =             ¹            (tr(∇zφ(z, θ) ∂ ∇ρψ(ρ, θ)|ρ₌φ₍z,θ₎) ∗ q(z, θ)

∂θ                                  |det(∇zφ(z,θ))|                            ∂θ

+  ∂ q(z, θ) + (∇zq(z, θ) ·  ∂  ψ(ρ, θ)|ρ₌φ₍z,θ₎)

=             ¹            (tr( ∂ ∇ρψ(ρ, θ)|ρ₌φ₍z,θ₎∇zφ(z, θ)) ∗ q(z, θ)

|det(∇zφ(z,θ))|        ∂θ

+  ∂ q(z, θ) + (∇zq(z, θ) ·  ∂  ψ(ρ, θ)|ρ₌φ₍z,θ₎)

=             ¹            (tr( ∂ (∇ρψ(ρ, θ)|ρ₌φ₍z,θ₎∇zφ(z, θ))) ∗ q(z, θ)

|det(∇zφ(z,θ))|        ∂θ

+  ∂ q(z, θ) + (∇zq(z, θ) ·  ∂  ψ(ρ, θ)|ρ₌φ₍z,θ₎)

=             ¹            (tr( ∂ ∇zψ(φ(z, θ), θ)) ∗ q(z, θ)

|det(∇zφ(z,θ))|        ∂θ

+  ∂ q(z, θ) + (∇zq(z, θ) ·  ∂  ψ(ρ, θ)|ρ₌φ₍z,θ₎)

=             ¹            (tr(∇z ∂  ψ(ρ, θ)|ρ₌φ₍z,θ₎) ∗ q(z, θ)

|det(∇zφ(z,θ))|              ∂θ

+  ∂ q(z, θ) + (∇zq(z, θ) ·  ∂  ψ(ρ, θ)|ρ₌φ₍z,θ₎)

=             ¹            (tr(∇z(q(z, θ) ∂ ψ(ρ, θ)|ρ₌φ₍z,θ₎)) +  ∂ q(z, θ))

|det(∇zφ(z,θ))|                            ∂θ                                            ∂θ

=             ¹            ( ∂ q(z, θ) + ∇z · (q(z, θ) ∂ ψ(ρ, θ)|ρ₌φ₍z,θ₎))

|det(∇zφ(z,θ))|   ∂θ                                             ∂θ

Then let vθ(z, θ) =  ∂ ψ(ρ, θ)|ρ₌φ₍z,θ₎, we obtain the first conclusion of the Theorem.3.1.
As for the second part, we have

Ew₍ρ,θ₎[f (ψ(ρ, θ))] = Eq₍z,θ₎[f (z)]

Then

10


Under review as a conference paper at ICLR 2020


∂

∂θ Eq(z,θ)

∂

[f (z)] =  ∂θ E

∂

w(ρ,θ)

[f (ψ(ρ, θ))]

∂

(20)

∂


Since  ∂

=  ∂θ Ew₍ρ,θ₎[∇zf (z)|z₌ψ₍ρ,θ₎ ∂θ ψ(ρ, θ) + f (ψ(ρ, θ)) ∂θ log w(ρ, θ)]

log w(ρ, θ) =   ∂  w(ρ,θ)  =            ∂  w(ρ,θ)


∂θ

Then we have

∂θ                 

w(ρ,θ)

                    ∂θ                                

q(z,θ)|det(∇ρψ(ρ,θ))|

∂  Eq₍z,θ₎[f (z)] = Ew₍ρ,θ₎[∇zf (z)|z₌ψ₍ρ,θ₎  ∂  ψ(ρ, θ) + f (ψ(ρ, θ)) ∂  log w(ρ, θ)]

= Eq₍z,θ₎[∇zf (z) ∂ ψ(ρ, θ)|ρ₌φ₍z,θ₎ + f (z) ∂  log w(ρ, θ)|ρ₌φ₍z,θ₎]                               
               

= Eq₍z,θ₎[∇zf (z) ∂ ψ(ρ, θ)|ρ₌φ₍z,θ₎ +f (z)     ¹     ∇z ·(q(z, θ) ∂ ψ(ρ, θ)|ρ₌φ₍z,θ₎)+f (z)        
 ∂θ       ]


∂θ

∂q(z,θ)

q(z,θ)                               ∂θ

q(z,θ)

= Eq₍z,θ₎[f (z)         ∂θ        +     ¹     ∇z · (f (z)q(z, θ) ∂ ψ(ρ, θ)|ρ₌φ₍z,θ₎)]


q(z,θ)

∂q(z,θ)

q(z,θ)                                           ∂θ

= Eq₍z,θ₎[f (z)         ∂θ        +     ¹     ∇z · (f (z)q(z, θ)vθ)]

Thus we obtain the second part of the Theorem.3.1. Proof ends.

As a by-product, if we make   ∂ w(φ(z, θ), θ)  =  0 , we can obtain the transport equation for the


reparameterization trick:

∂

∂θ q(z, θ) + ∇z

∂

· (q(z, θ) ∂θ v  ) = 0                                            (21)

And   ∂ w(φ(z, θ), θ)  =  0 also means that the standardization distribution is independent with θ

which is the core of the reparameterization trick.

A.2    VARIANCE VARIATIONAL OPTIMIZATION

For  the  simplicity  of  the  proof,   we  only  consider  the  1-dimensional  here.      And  
denote

∂ ˡᵒᵍ q⁽ᶻ,θ⁾ f (z) +     ¹     ᵈ  (f (z)q(z, θ)vθ(z, θ)) as r(z, θ).

∂θ                       q(z,θ) dz

The variance of the G-TRANS gradient with N independent samples is defined as:

Var( ∂^L ) =  ¹ Var(r(z, θ)) =  ¹ (Eq₍z,θ₎[(r(z, θ))²] − (Eq₍z,θ₎[r(z, θ)])²)

∂q(z,θ)

where with the unbiased constraint, we have Eq₍z,θ₎[r(z, θ)] = Eq₍z,θ₎[f (z)         ∂θ       ] = 
const, so we

need to consider the term Eq₍z,θ₎[(rθ(z, θ))²] only.

∫  +∞      ∂q        1  d

	

According to the Euler–Lagrange equation, we have


∂q

[(f ∂θ  +

1  d (fqvθ))²q] =   d

∂q

[(f ∂θ  +

1  d (fqvθ))²q]                    (23)


dvθ

Simplify it, we have

q      q dz

dz dv′θ

q      q dz


Then we have

∂q

(f ∂θ  +

q

1  d

q dz

(fqvθ))

d

(fq) =

dz

∂q

(fq(f ∂θ  +
dz            q

1  d

q dz

(fqvθ)))                       (24)

11


Under review as a conference paper at ICLR 2020


which means

∂q

fq    ((f ∂θ  +
dz        q

1  d

q dz

(fqvθ))) = 0                                               (25)


Thus we have

∂q

(f ∂θ  +

q

1  d

q dz

(fqvθ)) = C(θ)                                                  (26)


vθ =

 1      ᶻ

fq

qC(θ) − f

∂q dz                                        (27)

∂θ

which is usually intractable in real world practice.

A.3    UNBIASEDNESS CONSTRAINT VERIFICATION

Here we verify that vθ  (z, θ) satisfy the unbiasedness constraint if h(z, θ) is bounded.  Let θi 
be

any component of θi, then


1

q ∇z · (qf vah)qdz =

1   ∂
q ∂zi

(−h(z, θ)

zi   ∂q(z′, θ)

∂θi

dzi′)qdz


=        ∂  (   h(z, θ)

∂zi

zi   ∂qi(zi′, θi)

∂θi

dzi′)dzi

Yj/=i

qj(zj, θj)dzj

(28)


∫  Yj/=i

qj(zj, θj)dzj

  ∂  (   h(z, θ)

∂zi

zi   ∂qi(zi′, θi)

∂θi

dzi′)dzi

If h(z, θ) is bounded, we have


  ∂

∂zi

(−h(z, θ)

zi   ∂qi(zi′, θi)

∂θi

dzi′)dzi =  −(h(z, θ)

zi   ∂qi(zi′, θi)

∂θi

+∞

dzi′)

−∞

= 0        (29)

Therefore, Eq  [  ¹  ∇z · (fqθvθi  )] = ∫  1 ∇z · (qf vθi  )qdz = 0.

A.4    DUAL POLYNOMIAL VELOCITY FIELD

If we take the dual polynomial velocity field vθ  in the G-TRANS framework, we can reach a dual

result to the Proposition.4.2:

Proposition  A.1.  If  Cov(Pk ∂ ˡᵒᵍ q⁽ᶻ,θ⁾ , (2f − Pk) ∂ ˡᵒᵍ q⁽ᶻ,θ⁾ )  >  0,  then  the  gradient  
estimator

resulted from dual polynomial velocity field has a smaller gradient variance than the score function
gradient estimator.

The proof is similar to that of Proposition.4.2.

12

