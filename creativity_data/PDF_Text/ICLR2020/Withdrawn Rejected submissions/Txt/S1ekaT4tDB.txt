Under review as a conference paper at ICLR 2020
Why Convolutional Networks Learn
Oriented Bandpass Filters: A Hypothesis
Anonymous authors
Paper under double-blind review
Ab stract
It has been repeatedly observed that convolutional architectures when applied to
image understanding tasks learn oriented bandpass filters. A standard explanation
of this result is that these filters reflect the structure of the images that they have
been exposed to during training: Natural images typically are locally composed
of oriented contours at various scales and oriented bandpass filters are matched
to such structure. The present paper offers an alternative explanation based not
on the structure of images, but rather on the structure of convolutional architec-
tures. In particular, complex exponentials are the eigenfunctions of convolution.
These eigenfunctions are defined globally; however, convolutional architectures
operate locally. To enforce locality, one can apply a windowing function to the
eigenfunctions, which leads to oriented bandpass filters as the natural operators
to be learned with convolutional architectures. From a representational point of
view, these filters allow for a local systematic way to characterize and operate on
an image or other signal.
1	Introduction
1.1	Motivation
Convolutional networks (ConvNets) in conjunction with deep learning have shown state-of-the-art
performance in application to computer vision, ranging across both classification, e.g., (Krizhevsky
et al., 2012; Tran et al., 2015; Ge et al., 2019) and regression, e.g., (Szegedy et al., 2013; Eigen &
Fergus, 2015; Zhou et al., 2017) tasks. However, understanding of how these systems achieve their
remarkable results lags behind their performance. This state of affairs is unsatisfying not only from
a scientific point of view, but also from an applications point of view. As these systems move beyond
the lab into real-world applications better theoretical understanding can help establish performance
bounds and increase confidence in deployment.
Visualization studies of filters that have been learned during training have been one of the key tools
marshaled to lend insight into the internal representations maintained by ConvNets in application to
computer vision, e.g., (Zeiler & Fergus, 2014; Yosinski et al., 2015; Mahendran & Vedaldi, 2015;
Shang et al., 2016; Feichtenhofer et al., 2018). Here, an interesting repeated observation is that
early layers in the studied networks tend to learn oriented bandpass filters, both in two image spatial
dimenstions, (x, y)>, in application to single image analysis as well as in three spatiotemporal
dimensions, (x, y, t)>, in application to video. An example is shown in Figure 1. Emergence of such
filters seems reasonable, because local orientation captures the first-order correlation structure of the
data, which provides a reasonable building block for inferring more complex structure (e.g., local
measurements of oriented structure can be assembled into intersections to capture corner structure,
etc.). Notably, however, more rigorous analyses of exactly why oriented bandpass filters might be
learned has been limited. This state of affairs motivates the current paper in its argument that the
analytic structure of ConvNets constrains them to learn oriented bandpass filters.
1.2	Related research
Visualization of receptive field profiles (i.e., pointspread functions (Lim, 1990)) of the convolutional
filters learned by contemporary ConvNets is a popular tool for providing insight into the image prop-
erties that are being represented by a network. A notable trend across these studies is that early layers
1
Under review as a conference paper at ICLR 2020
Figure 1: Visualization of pointspread functions (convolutional kernels) observed to be learned in
the early layers of ConvNets. Brightness corresponds to pointwise function values. The majority of
the plots show characteristics of oriented bandpass filters in two spatial dimensions, i.e., oscillating
values along one direction, while remaining relatively constant in the orthogonal direction, even
as there is an overall amplitude fall-off with distance from the center. The specific examples derive
from the early layers of a ResNet-50 architecture (He et al., 2016) trained on ImageNet (Russakovsky
et al., 2015).
appear to learn oriented bandpass filters in both two spatial dimensions, e.g., (Zeiler & Fergus, 2014;
Springenberg et al., 2015; Yosinski et al., 2015; Shang et al., 2016) as well as three spatiotemporal
dimensions, e.g., (Feichtenhofer et al., 2018). Indeed, earlier studies with architectures that also con-
strained their filters to be convolutional in nature, albeit using a Hebbian learning strategy (MacKay,
2003) rather than the currently dominant back-propagation approach (Rumelhart et al., 1986), also
yielded filters that visualized as having oriented bandpass filter characteristics (Linsker, 1986). In-
terestingly, biological vision systems also are known to show the presence of oriented bandpass
filters at their earlier layers of processing in visual cortex; see Hubel & Wiesel (1962) for pioneering
work along these lines and for more general review DeValois & DeValois (1988).
The presence of oriented bandpass filters in biological systems often has been attributed to their be-
ing well matched to the statistics of natural images (Field, 1987; Olshausen & Field, 1996; Karklin
& Lewicki, 2009; Simoncelli & Olshausen, 2001), e.g., the dominance of oriented contours at mul-
tiple scales. Similar arguments have been made regarding why such filters are learned by ConvNets.
Significantly, however, studies have shown that even when trained with images comprised of ran-
dom noise patterns, convolutional architectures still learn oriented bandpass filters (Linsker, 1986).
These later results suggest that the emergence of such filter tunings cannot be solely attributed to
systems being driven to learn filters that were matched to their training data.
Interestingly, some recent multilayer convolutional architectures have specified their earliest layers
to have oriented bandpass characteristics (Bruna & Mallat, 2013; Jacobsen et al., 2016; Hadji &
Wildes, 2017); indeed, some have specified such filters across all layers (Bruna & Mallat, 2013;
Hadji & Wildes, 2017). These design decisions have been variously motivated in terms of being well
matched to primitive image structure (Hadji & Wildes, 2017) or providing useful building blocks for
learning higher-order structures (Jacobsen et al., 2016) and capturing invariances (Bruna & Mallat,
2013). Other work has noted that purely mathematical considerations show that ConNets are well
suited to designs that capture multiscale, windowed spectra Bruna et al. (2016); however,it did not
explictly established the relationship to eigenfunctions of convolution nor offered an explanation for
why deep-learning yields oriented bandpass filters when applied to ConvNets.
In the light of previous research, the present work appears to be the first to offer an explanation
of why ConvNets learn oriented bandpass filters by appeal to the inherent properties of their archi-
tectures. By definition, the convolutional layers of a ConvNet are governed by the properties of
2
Under review as a conference paper at ICLR 2020
convolution. For present purposes, a key property is that the eigenfunctions of convolution are com-
plex exponentials. Imposing locality on the eigenfunctions leads to oriented bandpass filters, which
therefore are the appropriate filters to be learned by a ConvNet.
2	Analytic approach
This section details a novel explanation for why ConvNets learn oriented bandpass filters. The first
two subsections largely review standard material regarding linear systems theory (Oppenheim et al.,
1983) and related topics (Kaiser, 2011; Kusse & Westwig, 2006), but are necessary to motivate prop-
erly our explanation. The final subsection places the material in the specific context of ConvNets.
2.1	Eigenfunctions of convolution
Let L be a linear operator on a function space. The set of eigenfunctions φn associated with this
operator satisfy the condition (Kusse & Westwig, 2006)
Lφn = λnφn.
(1)
That is, the operator acts on the eigenfunctions simply via multiplication with a constant, λn , re-
ferred to as the eigenvalue. It sometimes also is useful to introduce a (positive definite) weighting
function, w, which leads to the corresponding constraint
Lφn = λnwφn.
(2)
For cases where any function in the space can be expanded as a linear sum of the eigenfunctions, it
is said that the collection of eigenfunctions form a complete set. Such a set provides a convenient
and canonical spanning representation.
Let x = (x1, x2, . . . , xn)>, a = (a1, a2, . . . , an)> and u = (u1, u2, . . . , un)> . For the space of
convolutions, with the convolution of two functions, f(x) and h(x) defined as
f (x) * h(x)
Z∞
∞
f(x - a)h(a) da
(3)
it is well known that functions of the form f(x) = eiu>x are eigenfunctions of convolution (Oppen-
heim et al., 1983), i.e.,
eiu>x
Z∞
∞
>
e-iu ah(a) da
(4)
with the equality achieved via appealing to eiu>(x-a) = eiu>xe-iu>a and subsequently factoring
eiu>x outside the integral as it is independent of a. The integral on the right hand side of (4),
Z∞
∞
>
e-iu ah(a) da,
(5)
is the eigenvalue, referred to as the modulation transfer function (MTF) in signal processing (Oppen-
heim et al., 1983). Noting that eiu>x = cos(u>x) + i sin(u>x) leads to the standard interpretation
of u in terms of frequency of the function (e.g., input signal).
Given the eigenfunctions of convolution are parameterized in terms of their frequencies, it is useful
to appeal to the Fourier transform of function f (x), where we use the form (Horn, 1986)
F(u)
Z∞
∞
f(x)e-iu>x dx,
(6)
because any convolution can be represented in terms of how it operates via simple multiplication of
the eigenvectors, (5), with the eigenfunctions, eiu>x, with u given by (6). Thus, this decomposition
provides a canonical way to decompose f(x) and explicate how a convolution operates on it.
3
Under review as a conference paper at ICLR 2020
2.2	Imposing locality
Understanding convolution purely in terms of its eigenvectors and eigenvalues provides only a global
representation of operations, as notions of signal locality, x, are lost in the global transformation to
the frequency domain, u. This state of affairs often is unsatisfactory from a representational point
of view because one wants to understand the structure of the signal (e.g., an image) on a more
local basis (e.g., one wants to detect objects as well as their image coordinates). This limitation
can be ameliorated by defining a windowed Fourier transform (Kaiser, 2011), as follows (Jahne &
Hausbecker, 2000).
Let w(x) be a windowing function that is positive valued, symmetric and monotonically decreasing
from its center so as to provide greatest emphasis at its center. A Windowed Fourier Transform
(WFT) of f(x) can then be defined as
F(uc, x; w)
∞>
f (a)w(a - x)e-iuc a da.
(7)
Making use of the symmetry constraint that we have enforced on the windowing function allows for
w(x) = w(-x) so that the WFT, (7), can be rewritten as
Z∞
f (a)w(x - a)eiuc (x-a)e-iuc x da,
∞
which has the form of a convolution
f (x) * (w(x)eiu>x)
iu>
with the inclusion of an additional phase component, e-iuc x.
(8)
(9)
To provide additional insight into the impact the WFT convolution, (9), has on the function, f (x),
it is useful to examine the pointspread function, w(x)eiuc>x, in the frequency domain by taking its
Fourier transform (6), i.e., calculate its MTF. We have
Z∞
∞
w(x)eiuc>xe-iu>x dx,
(10)
which via grouping by coefficients of x becomes
Z∞
∞
w(x)e-i(u-uc)>x dx.
(11)
Examination of (11) reveals that it is exactly the Fourier transform of the window function, cf.
(6), as shifted to the center frequencies, uc. Thus, operation of the WFT convolution, (9), on a
function, f(x), passes central frequency, uc, relatively unattenuated, while it suppresses those that
are further away from the central frequency according to the shape of the window function, w(x),
i.e., it operates as a bandpass filter. Thus, convolution with a bank of such filters with varying central
frequencies, uc, has exactly the desired result of providing localized measures of the frequency
content of the function f (x).
Returning to the pointspread function itself, w(x)eiuc>x, and recalling that eiu>x = cos(u>x) +
i sin(uc>x), it is seen that in the signal domain, x, the filter will oscillate along the direction of uc
while remaining relatively constant in the orthogonal direction, even as there is an overall amplitude
fall-off with distance from the center according to the shape of w(x), i.e., we have an oriented
bandpass filter.
As a specific example (Jahne & Hausbecker, 2000), taking w(x) to be an n-dimensional Gaussian-
like function, g(x; σ) = K e-kxk3σ2, With σ the standard deviation and K a scaling factor, yields an
n-dimensional Gabor-like filter,
g(x; σ)eiu>x = g(x; σ) (cos(u>x) + i sin(u>x)) ,	(12)
Which provides good joint localization of signal content in the signal and frequency domains Gabor
(1946). Indeed, visualization of these filters in tWo spatial dimensions (Figure 2) provides strik-
ingly similar appearance to those presented in Figure 1, if in an idealized form. In particular, their
4
Under review as a conference paper at ICLR 2020
Figure 2: Visualization of an analytically defined oriented bandpass filter (12). The left panel shows
the pointspread function corresponding to the odd symmetry (sin) component, while the right panel
shows its power in the frequency domain. Brightness corresponds to pointwise function values.
pointspread functions oscillate according to a frequency ∣∣Uck along the direction, ^^, while re-
maining relatively constant in the orthogonal direction, even as there is an overall amplitude fall-off
with distance from the center. In the frequency domain, they have peak power at uc with a fall-off
following a Gaussian-like shape with standard deviation, 1∕σ, that is the inverse of that used in
specifying the window, w(x). These observations hold because we already have seen, (11), that the
frequency domain representation of such a function is the Fourier transform of the window function,
w(x), shifted to the center frequencies, uc; furthermore, the Fourier tranform of a function of the
form g (x; σ) has a similar form, albeit with an inverse standard deviation Bracewell (1986).
2.3 Implications for ConvNets
Convolutions in ConvNets serve to filter the input signal to highlight its features according to the
learned pointspread functions (convolutional kernels). Thus, convolution with the oriented filters
shown in Figure 1 will serve to highlight aspects of an image that are correspondingly oriented
and at corresponding scales. The question at hand is, “Why did the ConvNet learn such filters?”
The previous parts of this section have reviewed the fact that complex exponentials of the form
eiu>x = cos(u>x) + i sin(u>x) are the eigenfunctions of convolution. Thus, such frequency de-
pendent functions similarly serve as the eigenfunctions of the convolutional operations in ConvNets.
In particular, this result is a basic property of the convolutional nature of the architecture, indepen-
dent of the input to the system. Thus, for any convolution in a ConvNet the frequency dependent
eigenfunctions, eiu>x, provide a systematic way to represent their input.
As with the general discussion of locality presented in Subsection 2.2, for the specifics of ConvNets
it also is of interest to be able to characterize and operate locally on a signal. At the level of convolu-
tion, such processing is realized via pointspread functions that operate as bandpass filters, (9). Like
any practical system, ConvNets will not capture a continuous range of bandpass characteristics, as
given by uc and the sampling will be limited by the number of filters the designer allows at each
layer, i.e., as a metaparameter of the system. Nevertheless, making use of these filters provides a
systematic approach to representing the input signal.
Overall, the very convolutional nature of ConvNets inherently constrain and even define the filters
that they learn, independent of their input or training. In particular, learning bandpass filters provides
a canonical way to represent and operate on their input, as these serve as the localized eigenfunc-
tions of convolution. As a ConvNet is exposed to more and more training data, its representation
is optimized by spanning as much of the data as it can. Within the realm of convolution, in which
ConvNet conv layers are defined, oriented bandpass filters provide the solution. They arise as the lo-
cality constrained eigenfunctions of convolution and thereby have potential to provide a span of any
input signal in a localized manner. Thus, ConvNets are optimized by learning exactly such filters.
Notably, since this explanation for why ConvNets learn oriented bandpass filters is independent of
training data, it can explain why such filters emerge even when the training data lacks such pattern
structure, including training on random input signals, e.g., (Linsker, 1986).
Two final implications suggest directions for future research. First, the analysis presented in this
paper has been targeted toward an explanation for why ConvNets learn oriented bandpass filters in
their early layers, as suggested in visualization studies discussed in Sec. 1.2. However, our analy-
sis of oriented bandpass filtes as the localized eigenfunctions of convolution is not specific to early
5
Under review as a conference paper at ICLR 2020
ConvNet layers, but rather applies to any convolutional layer. The result thereby raises the question
of whether deeper ConvNet layers also learn oriented bandpass filters. Here, empirical data is lack-
ing as visualization studies to date for any given layer concentrate on the combined result across
all previous layers, rather than the filtering characteristics at just that layer. Thus, interesting future
research in examining the filters at each layer in isolation from those at other layers is motivated by
the analysis presented in the current paper. Along these lines, it is interesting to note that certain
hand-crafted ConvNets already make use of the same set of oriented bandpass filters at all layers of
the architecture and do so to good advantage in their targeted tasks of single image and video texture
analysis (Bruna & Mallat, 2013; Hadji & Wildes, 2017).
A second direction for future work involves use of parameterized filters during learning. In par-
ticular, the presented theoretical motivation for learning of oriented bandpass operators as the con-
volutional filters in ConvNets suggests the possibility of constraining the filters to be of that form
in a learning-based framework. In such a framework, it would not be necessary for the training
process to learn the numerical values for each and every individual filter value (i.e., each filter tap),
but rather would merely need to learn a much smaller number of parameters, e.g., the values of the
center frequency, uc and the standard deviation σ associated with the Gabor-like filter derived above
(12). Such a constrained learning approach would require a much less intensive training procedure
(e.g. involving far less data) compared to learnig values fo all individual filter taps, owing to the
drastically reduced number of parameters that need to be estimated, even while being able to tune to
the specifics of the task that is being optimized.
3 Summary
Previous studies have demonstated that learned filters at the early layers of convolutional networks
visualize as oriented bandpass filters. This phenomenon typically is explained via appeal to natural
image statistics, i.e., natural images are dominated by oriented contours manifest across a variety
of scales and oriented bandpass filters are well matched to such structure. We have offered an
alternative explanation in terms of the structure of convolutional networks themselves. Given that
their convolutional layers necessarily operate within the space of convolutions, learning oriented
bandpass filters provides the system with the potential to span possible input, even while preserving
a notion of locality in the signal domain.
References
R. Bracewell. The Fourier Transform and Its Applications. McGraw-Hill, NY, NY, 1986.
J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Trans. Pattern Anal. Mach.
Intell.,35:1872-1886, 2013.
J. Bruna, S. Chintala, Y. LeCun, S. Piantino, A. Szlam, and M. Tygert. A matehatmical motivation
for complex-valued convolutional networks. Neural Computation, 28:815-825, 2016.
R. DeValois and K. DeValois. Spatial Vision. Oxford University Press, NY, NY, 1988.
D. Eigen and R. Fergus. Predicting depth, surface normals and sematic labels with a common muli-
scale convolutional architecture. In ICCV, 2015.
C.	Feichtenhofer, A. Pinz, R. Wildes, and A. Zisserman. What have we learned from deep represen-
tations for action recognition? In CVPR, 2018.
D.	Field. Relations between the statistics of natural images and the response properties of cortical
cells. Journal of the Optical Society of America A, 4:2379, 1987.
D. Gabor. Theory of communication. Journal of the Institute of Electrical Engineers, 93:429-457,
1946.
W. Ge, X. Lin, and Y. Yu. Weakly supervised complementary parts models for fine-grained image
classification from the bottom up. In CVPR, 2019.
I. Hadji and R. Wildes. A spatiotemporal oriented energy network for dynamic texture recognition.
In ICCV, 2017.
6
Under review as a conference paper at ICLR 2020
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
B. Horn. Robot Vision. MIT Press, Cambridge, MA, 1986.
D. H. Hubel and T. N. Wiesel. Receptive fields, binocular interaction and functional architecture in
the cat,s visual cortex. The Journal of Physiology, 160:106-154, 1962.
J H. Jacobsen, J V. Gemert, Z. Lou, and A W.M. Smeulders. Structured receptive fields in cnns. In
CVPR, 2016.
B. Jahne and H. Hausbecker. Computer Vision and Applications. Academic Press, San Diego, CA,
2000.
G. Kaiser. A Friendly Guide to Wavelets. Modern Birkhauser Classics, Switzerland, 2011.
Y. Karklin and M. Lewicki. Emergence of complex cell properties by learning to generalize in
natural scenes. Nature, 457:83-86, 2009.
A.	Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. In NIPS, 2012.
B.	Kusse and W. Westwig. Mathematical Physics. Wiley-VCH, Weinheim, FRG, 2006.
J. Lim. Two-Dimensional Signal and Image Processing. Prentice Hall, Upper Saddle River, NJ,
1990.
R. Linsker. From basic network principles to neural architecture. Proc. National Academy of Sci-
ences USA, 83:7508-7512, 8390-8394, 8779,8783, 1986.
D. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press,
Cambridge, UK, 2003.
A.	Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. In
CVPR, 2015.
B.	Olshausen and D. Field. Emergence of simple-cell field properties by learning a sparse code for
natural images. Nature, 381:607-609, 1996.
A. Oppenheim, A. Willsky, and I. Young. Signals and Systems. Prentice Hall, Upper Saddle River,
NJ, 1983.
D.	Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating errors.
Nature, 323:533-536, 1986.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge. IJCV,
2015.
W. Shang, K. Sohn, and H. Lee D. A. Enlitic. Understanding and improving convolutional neural
networks via concatenated rectified linear units. In ICML, 2016.
E.	Simoncelli and B. Olshausen. Natural image statistics and neural representation. Annual Review
of Neuroscience, 24:1193-1216, 2001.
J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convo-
lutional net. In ICLR Workshop, 2015.
C.	Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013.
D.	Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features with
3d convolutional networks. In ICCV, 2015.
J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding neural networks through
deep visualization. In ICML workshops, 2015.
M. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014.
T. Zhou, M. Brown, N. Snavely, and D. Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, 2017.
7