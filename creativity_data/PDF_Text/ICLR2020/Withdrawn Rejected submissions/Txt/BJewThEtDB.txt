Under review as a conference paper at ICLR 2020
Dynamical clustering of Time Series Data
using Multi-Decoder RNN Autoencoder
Anonymous authors
Paper under double-blind review
Ab stract
Clustering algorithms have wide applications and play an important role in data
analysis fields including time series data analysis. The performance of a cluster-
ing algorithm depends on the features extracted from the data. However, in time
series analysis, there has been a problem that the conventional methods based on
the signal shape are unstable for phase shift, amplitude and signal length varia-
tions. In this paper, we propose a new clustering algorithm focused on the dy-
namical system aspect of the signal using recurrent neural network and variational
Bayes method. Our experiments show that our proposed algorithm has a robust-
ness against above variations and boost the classification performance.
1	INTRODUCTION
The rapid progress of IoT technology has brought huge data in wide fields such as traffic, industries,
medical research and so on. Most of these data are gathered continuously and accumulated as time
series data, and the extraction of features from a time series have been studied intensively in recent
years. The difficulty of time series analysis is the variation of the signal in time which gives rise
to phase shift, compress/stretch and length variation. Many methods have been proposed to solve
these problems. Dynamic Time Warping (DTW) was designed to measure the distance between
warping signals (Rabiner & Juang, 1993). This method solved the compress/stretch problem by
applying a dynamic planning method. Fourier transfer or wavelet transfer can extract the features
based on the frequency components of signals. The phase shift independent features are obtained
by calculating the power spectrum of the transform result. In recent years, the recurrent neural
network (RNN), which has recursive neural network structure, has been widely used in time series
analysis (Elman, 1990; 1991). This recursive network structure makes it possible to retain the past
information of time series. Furthermore, this architecture enables us to apply this algorithm to
signals with different lengths. Although the methods mentioned above are effective solutions for the
compress/stretch, phase shift and signal length variation issues respectively, little has been studied
about these problems comprehensively.
Let us turn our attention to feature extraction again. Unsupervised learning using a neu-
ral network architecture autoencoder (AE) has been studied as a feature extraction method
(Hinton & Salakhutdinov, 2006; Vincent et al., 2008; Rifai et al., 2011). AE using RNN structure
(RNN-AE) has also been proposed (Srivastava et al., 2015) and it has been applied to real data such
as driving data (Dong et al., 2017) and others. RNN-AE can be also interpreted as the discrete dy-
namical system: chaotic behavior and the deterrent method have been studied from this point of
view (Zerroug et al., 2013; Laurent & von Brecht, 2016).
In this paper, we propose a new clustering algorithm for feature extraction focused on the dynamical
system aspect of RNN-AE. In order to achieve this, we employed a multi-decoder autoencoder
with multiple decoders to describe different dynamical systems. We also applied the variational
Bayes method (Attias, 1999; Ghahramani & Beal, 2001; Kaji & Watanabe, 2011) as the clustering
algorithm.
This paper is composed as follows: in Section 4, we explain AE from a dynamical system view,
then we define our model and from this, derive its learning algorithm. In Section 5, we describe the
application of our algorithm to an actual time series to show its robustness, including running two
experiments using periodic data and driving data. Finally we summarize our study and describe our
future work in Section 7.
1
Under review as a conference paper at ICLR 2020
2	Related work
A lot of excellent clustering/representation algorithms of data using AE have been studied so far
(Tschannen et al., 2018). Song et al. (2013) integrated the distance between data and centroids into
an objective function to obtain a cluster structure in the encoded data space. Pineau & Lelarge (2018)
proposed a generative model based on the variational autoencoder (VAE) (Kingma & Welling, 2014)
with a clustering structure as a prior distribution. Wang et al. (2019) achieved a high separability
clustering result by adding a regularization term for the orthogonality and balanced clusters of the
encoded data. These, however, are regularization methods of the objective function, and focused on
only the distribution of the encoded data.
They did not give the clustering policy based on the decoder structure, namely, the reconstruction
process of the data. From dynamical system point of view, one decoder of RNN-AE corresponds to
a single dynamics in the space of latent representation. Hence, it is natural to equip RNN-AE with
multiple decoders to implement multiple dynamics. Such an extension of RNN-AE, however, has
yet to be proposed in related works to the best of our knowledge.
3	Recurrent neural network and dynamical System
3.1	Recurrent neural network using unitary matrix
RNN is a neural network designed for time series data. The architecture of the main unit is called
cell, and mathematical expressions are shown in Fig. 1 and Eq. (1).
Figure 1: RNN Cell
Suppose we are given a time series,
X = ( X1, ∙∙∙ , X Xn,…,X N), Xn =(多 n,…，xtn,…，X),
Xn ∈ RD,n = 1, ∙∙∙ ,Ν,t = 1,…，T
where D denotes data dimension. RNN, unlike the usual feed-forward neural network, operates the
same transform matrix to the hidden valuable recursively,
zt = V ht-1 +	Uxt +	b,	ht = σ(zt),	(1)
where σ(∙)	is an activation function and zt, ht, b	∈	RL. This recursive	architecture makes it
possible to handle signals with different lengths, although it is vulnerable to the vanishing gradient
problem as with the deep neural network (DNN) (Elman, 1990; 1991). Long short-term memory
(LSTM) and gated recurrent unit (GRU) are widely known solutions to this problem (Gers et al.,
2000; Hochreiter & Schmidhuber, 1997; Cho et al., 2014). These methods have the extra mecha-
nism called a gate structure to control output scaling and retaining/forgetting of the signal informa-
tion. Though this mechanism works effectively in many application fields (Malhotra et al., 2015;
Rana, 2016), the architecture of network is relatively complicated. As an alternative simpler method
to solve this problem, the algorithm using a unitary matrix as the transfer matrix V was proposed
in recent years. Since the unitary matrix does not change the norm of the variable vector, we avoid
the vanishing gradient problem. In addition, the network architecture remains unchanged from the
original RNN.
In this paper, we focus on the dynamical system aspect of the original RNN. We employ the uni-
tary matrix type RNN to take advantage of this dynamical system structure. However, to imple-
ment the above method, we need to find the transform matrix V in the space of unitary matrices
U = {U (L) ∈ GL(L) U (LyU (L) = I}, where GL(L) is the set of complex-valued general lin-
ear matrices with size L × L and * means the adjoint matrix. Several methods to find the transform
2
Under review as a conference paper at ICLR 2020
matrix from the U has been reported so far (Pascanu et al., 2013; Jing et al., 2017; Wisdom et al.,
2016; Arjovsky et al., 2016; Jing et al., 2019). Here, we adopt the method proposed by Jing et al.
(2017).
3.2 RNN Autoencoder and dynamical system
The architecture of AE using RNN is shown in Fig. 2. AE is composed of an encoder unit and a
decoder unit. The parameters (Venc, Uenc, Vdec, Udec) are trained by minimizing kX - Xdec k2F =
PtT=1 kxt - xtdeck2, where X is the input data and Xdec is the decoded data.
The input data is recovered from only the encoded signal h using the matrix (Vdec, Udec), therefore
h is considered as the essential information of the input signal. When focusing on the transformation
Figure 2: Architecture of RNN Autoencoder
of the hidden variable, this recursive operation has the same structure of a discrete dynamical system
expression as described in the following equation:
ht = f(ht-1),
where f is given by Eq. (1). From this point of view, we can understand that RNN describes the
universal dynamical system structure which is common to the all input signals. 4
4 Derivation of Multi-Decoder RNN AE algorithm
In this section, we will give the architecture of the Multi-Decoder RNN AE (MDRA) and its learn-
ing algorithm. As we discussed in the previous section, RNN can extract the dynamical system
characteristics of the time series. In the case of the original RNN, the model expresses just one
dynamical system, hence all input data are recovered from the encoded result h by the same re-
covery rule. Therefore h is usually used as the feature value of the input data. In contrast, in
this paper, we focus on the transformation rule itself. For this purpose, we propose MDRA which
has multiple decoders to extract various dynamical system features. The architecture of MDRA is
shown in Fig. 3. Let Us PUt Wdec = (VdeC, UdeC) for k = 1,…，K, Wenc = (Venc, Uenc), and
W = (Wenc, WdeC,…,WKeC). We Will derive the learning algorithm to optimize the whole set of
Parameters W in the following section.
4.1 Decomposition of Free energy
We applied a clUstering method to derive the learning algorithm of MDRA. Many clUstering algo-
rithms have been proposed: here we employ the variational Bayes (VB) method, becaUse the VB
method enabled Us to adjUst the nUmber of clUsters by tUning the hyperparameters of a prior dis-
tribUtion. We first define free energy, which is negative marginal log-likelihood, by the following
eqUation,
F (X|W)
- log Z Z	YN XpW(Xn|yn,
n=1 yn
hn,β) p (yn∣α)
p(α)p(β)dαdβ,
(2)
where X is data tensor defined in Section 3 and W is parameter tensor of MDRA defined above.
Y = (y ι, y2,…,yN) is the set of latent variables each of which means an allocation for a decoder.
That is, yn = (ynι, ∙∙∙ , ynK)t ∈ RK, where ynk = 1 if Xn is allocated to the k-th decoder
3
Under review as a conference paper at ICLR 2020
Figure 3: Architecture of MDRA
and otherwise ynk = 0. pW(Xn|yn, hn, β) is the probability density function representation of
MDRA parametrized by tensor W, p(α) and p(β) are its prior distributions for a probability vector
α = (αι, ∙∙∙ , ακ) and a precision parameter β > 0. We applied the Gaussian mixture model as
our probabilistic model. Hence p(α) and p(β) were given by Dirichlet and gamma distributions
respectively which are the conjugate prior distributions of multinomial and Gaussian distributions.
These specific distributions are given as follows:
P W(X, Y, α,β ∣H )= P W(X | Y, H, β) p (Y ∣α) p(α) p (β),
P(α) =
Γ( θ 0 K)
Γ( θ0)K
K	λν0
ɪɪ αk 1, p(β) = 0^^βν0 leχp(—入oβ),
k=1	Γ(ν0 )
NN
p w(X ∣Y, H ,β ) = Y p W (X n ∣yn,hn, β ) , p ( Y|a ) = Y p(yn |a ) ,
n=1	n=1
K β∖ TnD	ynk	K
P W (X n ∣yn , hn,β ) = ∏ β	exp( 一β∣∣X n- g ( hn ∣ WdeC )∣∣ F )	,p( yn∣α ) = ɪɪ。产.
k=1	π	k=1
Here, θ0 > 0, ν0 > 0 and λ0 > 0 are hyperparameters and g(hn|Wdkec) = Xdnec,k denotes decoder
mapping of RNN from the encoded n-th data hn, H = (hι, ∙∙∙ , hN) and TnD is the total signal
dimension of input signal Xn including dimension of input data. To apply the variational Bayes
algorithm, we the derive the upper bound of the free energy by applying Jensen’s inequality,
F (X | W) = - log / / {γχ p w( X n∣yn, hn,β) p (yn∣α) j P (α) p (β) dadβ

-PW(XY, H, β)p(Yα)p(α)p(β)
-	q (Y) q (α) q (β)
≤ DΚL(q(Y)q(α)q(β)∣∣p(Y,α,β∣X)) + F(X|W)
N
=DKL(q(Y)q(α)q(β)kp(Y∣α)p(α)p(β)) - X Eq(yη)q(β)[logpW(Xn∣yn, hn,β)]
n=1
三 F(X I W),
where DKL (∙∣∣∙) is the KullbaCk-Leibler divergence. The upper bound F(X∣W) is called the vari-
ational free energy or (negated) evidence lower bound (ELBO). The variational free energy is min-
imized using the variational Bayes method under the fixed parameters W. Furthermore, it is also
minimized with respect to the parameters W by applying the RNN learning algorithm to the second
term of F(X ∣ W),
NN
-	Eq(yn)q(β) [logpW(Xnlyn, hn,β)] CX XX Eq(yn)q(β)
n=1	n=1
K
ynkβ kXn-g(hn|Wkdec)k2F
k=1
4
Under review as a conference paper at ICLR 2020
4.2 Minimization of the variational free energy
In this section, we derive the variational Bayes algorithm for MDRA to minimize the variational
free energy. We show the outline of the derivation below (for a detailed derivation, see Appendix
A). The general formula of the variational Bayes algorithm is given by
log q(Y) = Eq(α,β)[log pW(X, Y ,H,α,β)] +const.,
log q(α, β) = Eq(Y)[logpW(X,Y , H, α, β)] + const..
By applying the above equations to the above probabilistic models, we obtained the specific algo-
rithm shown in Appendix A.1. Then we minimize the following term using RNN algorithm:
NK
rnkkXn-g(hn|Wkdec)k2F ,
n=1 k=1
where rn,k = Eq(yn)[ynk] as detailed in Appendix A.2. We denote R = (r 1, ■■■ , tn), where
rn = (rnι, ■ ■■ ,rnK)T ∈ RK. From the above discussion, We finally obtained the following
MDRA algorithm. Fig. 4 describes the relation of the VB and RNN steps of MDRA algorithm.
Algorithm 1 MDRA
Input: X: set of input signals
Output: W: weight tensors, R: allocation weights, H : encoded signals
Set hyperparameters θ0, ν0, λ0 and the initial value ofW randomly.
repeat
Calculate W that minimizes the following value by RNN algorithm:
NK
rnkkXn-g(hn|Wkdec)k2F .
n=1 k=1
Calculate R = (rnk) by the algorithm VB part of MDRA (Algorithm 2).
until the difference of variational free energy F(X ∣ W) < Threshold * 5
SteplzVB part	Step2:RNN part
F(X∣W) ---------------λ η
(W：fix)	mκL(q(y)q(α)q(0)∣∣p(y,α,O∣X))
F(XIW) --------------- V」―L F(XW)
--------------F(X∣W,)
Figure 4: MDRA algorithm
5 Experiments
5.1	Periodic Signals
We first examined the basic performance of our algorithm using periodic signals. Periodic signals
are typical time series signals expressed by dynamical systems. Input signals have 2, 4, and 8 periods
respectively in 64 steps. Each signal is added a phase shift (maximum one period), amplitude varia-
tion (from 50% to 100% of the maximum amplitude), additional noise (maximum 2% of maximum
amplitude) and signal length variation (maximum 80% of the maximum signal length). Examples
of input data are illustrated in Fig. 5.
We compared RNN-AE to MDRA on its feature extraction performance using the above periodic
signals. Fig. 6 and Fig. 7 show the results of RNN-AE and MDRA respectively. The parameter
5
Under review as a conference paper at ICLR 2020
Figure 5: Examples of periodic signals
setting is listed in Table 3 in Appendix B. We used multi-dimensional scaling (MDS) as the dimen-
sion reduction method to visualize the distributions of features in Fig. 6 and Fig. 7. Fig. 6 shows
the distribution of the encoded data hn which is the initial value of the decoder unit in Fig. 2. We
found that RNN-AE can separate the input data into three regions corresponding to each frequency.
However each frequency region is distributed widely, therefore some part of the region overlap each
other.
Figure 6: Visualization of features extracted by RNN-AE (periodic signals)
Fig.7 shows the distributions of the encoded data hn and the clustering allocation weight rn ex-
tracted by MDRA. The distribution of rn , shown in the right figure of Fig. 7, is completely sepa-
rated into each frequency component without overlap. This result shows that the distribution of rn
as the feature extraction has robustness for a phase shift, amplitude and signal length variation. We
also show that MDRA can boost the classification accuracy using an actual driving data in the next
section.
5.2 Experiment of real driving data
We applied our algorithm to a driver identification problem. We use the data consisting of 3 drivers
signals when driving, including speed, acceleration, braking and steering angle signals.1 The input
signal was about 10 seconds differential data (128 steps), which was cut out from the original data
by a sliding window.2 The detailed information of the input data is shown in Table 1. We also show
samples of data (difference of acceleration) in Fig. 10 in Appendix C. The feature extraction results
by RNN-AE and MDRA are shown in Fig. 8. The parameter setting of this experiment is listed in
Table 4 in Appendix B. The left figure and middle figure are the distributions of hn of RNN-AE
1This data was created by HQL (Research Institute of Human Engineering for Quality Life: https://
www.hql.jp/ howhql/ spirit.html).
2We use only the data of which the maximum acceleration difference is more than a certain threshold.
6
Under review as a conference paper at ICLR 2020
Figure 7: Visualization of features extracted by MDRA (periodic signals) left: hn , right: rn
Table 1: Driving data specification
Training	Test	Driver	Signal length	Sampling pitch	Slide
44851	11213	3	128	—	0.1 sec.	4
and MDRA respectively, the right figure is the distribution of rn of MDRA. We can find different
trends in the distributions of the latent variable hn and rn of MDRA. The distribution of rn spreads
wider than that of hn . Table 2 shows the accuracy of the driver identification results using the above
Driver A
Driver B
Driver C
Driver
Driver B
Driver C
■0.03 -0.02 -0.01 0.00 0.01 0.02 0.03 0.04	.0.75 -0.50 -0.25 0.00 0.25 0.50 0.75
Figure 8: Extraction result of driving features
features as the input data. We adopt 10 folds cross variation to calculate the performance. The neural
network for the classification is composed of4 layers and illustrated in Fig. 11 in Appendix D. From
this result, we consider that the features extracted by MDRA are quite effective for the classification
of time series data.
6 Discussion
We verified the feature extraction peformance of the MDRA using actual time series data. In Sec-
tion 5.1, we saw that the periodic signals are completely classified by the frequency using clustering
weight rn. In this experiment, the average clustering weights, the elements of 焉 PN=I rn ∈ RK,
are (3.31e-01, 8.31e-47, 8.31e-47, 3.46e-01, 8.31e-47, 3.19e-01, 8.31e-47), with only three com-
ponents having effective weights. This weight narrowing-down is one of the advantages of VB
learning.
The left of Fig. 9 shows an enlarged view of around “freq 4” in Fig. 7 (right). We found
that the distribution of “freq 4” is in fact spread linearly. The right of Fig. 9 is the result
7
Under review as a conference paper at ICLR 2020
Table 2: Driving identification results
RNN-AE
MDRA
49.22%(+/- 0.74%)
49.30%(+/- 0.52%)
49.38%(+/- 0.71%)
49.10%(+/- 0.47%)
49.46%(+/- 0.55%)
Ave. 49.29
53.80%(+/- 0.46%)
53.77%(+/- 0.55%)
54.01%(+/- 0.50%)
52.78%(+/- 0.54%)
53.51%(+/- 0.63%)
Ave. 53.59
of the dimension reduction of features by t-Distributed Stochastic Neighbor Embedding (t-SNE)
(van der Maaten & Hinton, 2008). We found that each frequency data formed several spreading
clusters without overlapping.
As we saw earlier, the distribution of rn has a spreading distribution compared to that of hn . We in-
ferred that the spreading of the distribution rn was caused by extracting the diversity on the driving
scenes. In addition, the identification result shows that the combination of the features given by rn
and hn can improve the performance. Dong et al. (2017), which studied a driver identify algorithm
using the AE, proposed the minimization of the error integrating the reconstruction error of AE and
the classification error of deep neural network. This algorithm can avoid the over-fitting by using
unlabeled data whose data collection cost is smaller than labeled data. From these results, we can
expect that the MDRA can contribute to not only boost identification performance but also restrain
the over-fitting.
Figure 9: Left: Enlarged view of “freq 4” in Fig. 7 (right) Right: t-SNE result of rn
distribution
7	Conclusion
In this paper, we proposed a new algorithm MDRA that can extract dynamical system features of a
time series data. We conducted experiments using periodic signals and actual driving data to verify
the advantages of MDRA. The results show that our algorithm not only has robustness for the phase
shift, amplitude and signal length variation, but also can boost classification performance.
8	Future work
The phase transition phenomenon of the variational Bayes learning method, depending on the hy-
perparameters, has been reported in (Watanabe & Watanabe, 2006)．The hyperparameter setting of
the prior distribution has a great effect on the clustering result and classification performance. We
intend to undertake a detailed study of the relation between the feature extraction performance and
hyperparameter setting of the prior distributions in the future.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
Proceedings ofThe 33rd International Conference onMachine Learning (ICML) ,pp.1120-1128,
2016.
Hagai Attias. Inferring parameters and structure of latent variable models by variational Bayes.
In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI), pp.
21-30, 1999.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 1724-1734, 2014.
Weishan Dong, Ting Yuan, Kai Yang, Changsheng Li, and Shilei Zhang. Autoencoder regularized
network for driving style representation learning. In Proceedings ofthe Twenty-Sixth International
Joint Conference on Artificial Intelligence (IJCAI), pp. 1603-1609, 2017.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179-211, 1990.
Jeffrey L. Elman. Distributed representations, simple recurrent networks, and grammatical structure.
Machine Learning, 7(2):195-225, 1991.
Felix A. Gers, JUrgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction
with LSTM. Neural Computation, 12(10):2451-2471, 2000.
Zoubin Ghahramani and Matthew J. Beal. Graphical models and variational methods. In Advanced
Mean Field Methods Theory and Practice, pp. 161-177. MIT Press, 2001.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504-507, 2006.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin Soljacic. Tunable efficient unitary neural networks (EUNN) and their application to RNNs.
In Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 1733-
1741, 2017.
Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua
Bengio. Gated orthogonal recurrent units: On learning to forget. Neural Computation, 31(4):
765-783, 2019.
Daisuke Kaji and Sumio Watanabe. Two design methods of hyperparameters in variational Bayes
learning for Bernoulli mixtures. Neurocomputing, 74(11):2002-2007, 2011.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the
International Conference on Learning Representations (ICLR), 2014.
Thomas Laurent and James H. von Brecht. A recurrent neural network without chaos. CoRR,
abs/1612.06212, 2016.
Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, and Puneet Agarwal. Long short term memory
networks for anomaly detection in time series. In 23rd European Symposium on Artificial Neural
Networks, Computational Intelligence and Machine Learning (ESANN), pp. 89-94, 2015.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning (ICML), pp.
1310-1318, 2013.
Edouard Pineau and Marc Lelarge. Infocatvae: Representation learning with categorical variational
autoencoders. CoRR, abs/1806.08240, 2018.
9
Under review as a conference paper at ICLR 2020
Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of Speech Recognition. Prentice Hall,
Upper Saddle River, NJ, USA, 1993.
Rajib Rana. Gated recurrent unit (GRU) for emotion classification from noisy speech. CoRR,
abs/1612.07778, 2016.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-
encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International
Conference on Machine Learning (ICML), pp. 833-840, 2011.
Chunfeng Song, Feng Liu, Yongzhen Huang, Liang Wang, and Tieniu Tan. Auto-encoder based data
clustering. In Iberoamerican Congress on Pattern Recognition (CIARP), pp. 117-124, 2013.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video
representations using lstms. In Proceedings of the 32nd International Conference on Machine
Learning (ICML), pp. 843-852, 2015.
Michael Tschannen, Mario Lucic, and Olivier Bachem. Recent advances in autoencoder-based
representation learning. In Proceedings of Workshop on Bayesian Deep Learning (NeurIPS),
2018.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(Nov):2759-2605, 2008.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th International
Conference on Machine Learning (ICML), pp. 1096-1103, 2008.
Wei Wang, Dan Yang, Feiyu Chen, Yunsheng Pang, Sheng Huang, and Yongxin Ge. Clustering with
orthogonal autoencoder. IEEE Access, 7:62421-62432, 2019.
Kazuho Watanabe and Sumio Watanabe. Stochastic complexities of Gaussian mixtures in variational
Bayesian approximation. Journal of Machine Learning Research, 7(Apr):625-645, 2006.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity
unitary recurrent neural networks. In Advances in Neural Information Processing Systems 29
(NIPS), pp. 4880-4888. 2016.
Abdelhamid Zerroug, Labib S. Terrissa, and Alain Faure. Chaotic dynamical behavior of recurrent
neural network. Annual Review of Chaos Theory, Bifurcations and Dynamical Systems, 4:55-56,
2013.
10
Under review as a conference paper at ICLR 2020
A Derivation of Algorithm
A.1 Minimization algorithm of the variational free energy
Initially, we suppose that the posterior is expressed by q(Y , α, β) = q(Y )q(α, β). Then
log q(Y) = Eq(α,β)[log pW(X, Y ,H,α,β)] + const.
= Eq(α,β) [logPW(X∣Y, H, β)] + Eq(α,β)[logp(Y∣α)]
+ Eq(α,β)[log p(α)] + Eq(α,β)[log p(β)] + const.
=Eq(α)[lθgp( Y ∣α)] + Eq(α) [logP(α)]
+ Eq(β)[log PW(X|Y, H, β)] + Eq(β)[log P(β)] + const..
In addition,
NK
E q ( α) [log P ( Y ∣α )] = Eq(α) log	αkynk
n=1 k=1
NK
ΣΣynkEq(α) [logαk] ,
n=1 k=1
Eq(β) [logPW(X|Y, H, β)]
N K	TD
=E fyn Eq( β) -βkXn - g ( hnl WdeC ) k F + 子(log β — log ∏ ),
n=1 k=1
where TnD means total signal dimension. Therefore, we obtain
NK
log q(Y) = ΣΣ ynk log ρnk + const..
n=1 k=1
We here put
TD
log ρnk = E q(α) [log αk ]+ E q (β) -BIIX - g ( hnl W dec ) l∣F +-2-(IOg β - log π ) ∙	⑶
HenCe q(Y) (X QN=1 QK=1 Pnk, by putting rnk = PKnko U , We Obtain
k=1 ρnk
NK
q(Y) =	rnynkk.
n=1 k=1
Next We CalCulate log q(α, β),
log q(α, β) = Eq(Y)[log PW(X, Y, H, α, β)] + const.
=Eq(Y) [logP(Y∣∖α.)]+ logp(α)
+ Eq(Y)[log PW(X|Y, H, β)] + log P(β) + const..
Above equation Can be divided into the tWo terms inCluding α and β respeCtively,
logq(α) X Eq(Y)[logp(Y∣α)] + logp(α) + const.
NK	K
=	log αkEq(yn) [ynk] + (θ0 - 1) log αk + const..
n=1 k=1	k=1
Substituting E q (yn)[ ynk] = 1 ' q (ynk = 1) + 0 ' q (ynk = 0) = q (ynk = I) = rnk to the above
equation, We obtain
NK	K
log q(α) = ΣΣlogαkrnk + (θ0 - 1)	log αk + const..
n=1 k=1	k=1
11
Under review as a conference paper at ICLR 2020
On the other hand,
log q(β) = Eq(Y)[log pW(X|Y ,H,β)] + log p(β) + const.
N K	TD
=E Σ Eq(yn) [ ynk ] ∖-β "X - ( hnl W 忆)H F + Tr (唳 β —	∏ )
n=1 k=1
+ (ν0 - 1) log β + λβ + const..
卜 βXn - g ( hn∣ Wdec ) H F + Tr (log β — log ∏ )}
Similarly, by applying Eq(yn)[ynk] = rnk, we obtain
log q(β)
NK
= XX rnk
n=1 k=1
+ (ν0 - 1) log β + λβ + const..
We finally calculate log ρnk in Eq. (3).We first calculate log q(α) and log q(β),
NK
log q(β) = ΣΣ
n=1 k=1
—rnk [βχnn — g (hnl W kdcc) H F + Tr (log β — log ∏)
+ (ν0 — 1) log β — λ0β + const.
=βf + (Vo + 2)： TnD — l) log β — λβ + const.,
where we put f = P K=I P N=ι —rnkHXn — g (hnl W dec) Il F. In addition, putting X = λ o — f, V =
V 0 + 21 P N=1 Tn D，
q(β) = eβfβν0 +1 PN=1 TnD- 1 e-λ0β ∙ const.
λ	Vρ _	_
=e-λββν-1 ∙ const. = γ(v)βν- 1 e-λβ = Gamma(β∖ν, λ).
By using the expectations of β and logβ by gamma distribution Eq(β) [β] = Vλ-1, Eq(β) [log β] =
ψ(V) — log λ (Appendix A.3), we obtain
E q (β) —e IIx n - g ( hn∖W kdec ) Il F +	n-(log β — log π )
-HXn — g ( hn∖ Wdec ) H F VV-1 + TD ( ψ ( V) — log λ) — TD log ∏.
Similarly, q(α) turns out to be the Dirichlet distribution with parameters (θ1,…，θκ), and
Eq(α)[log ak] = ψ (Vk) — ψ (PK=1 Vk) is calculated by the same way in the general mixture model.
Therefore we finally obtain
log ρnk = ψ(θVk) — ψ	X θVk — HXn — g(hn ∖Wdkec)H2FVVλV-1
+ TD(ψ(V) — log V)- TD log ∏.
From the above results, the following variational Bayes algorithm is derived.
A.2 Minimization algorithm of the second term of the RNN term (The second
term of variational free energy)
In this section, we minimize —Eq(Y )q(β)
PnN=1logpW(Xn∖yn,hn,β)
to minimize the free en-
ergy Eq. (2) with respect to W. More specifically, we minimize
12
Under review as a conference paper at ICLR 2020
Algorithm 2 VB part of MDRA
Input: X: set of input signals
Output:R: allocation weight
Set hyperparameters θ0, ν0, λ0 and the number of iterations I
for i J 0 to I do
VB E-step:
log Pnk= ψ(θk ) - ψ (X θ) TXn -g (hnl Wdec ) H F V L
+ TnD (Ψ( V)- log X)-TnD log ∏
Pnk
Pk=1 ρnk
VB M-step:
N
Nk = X rnk , θVk = θ0 + Nk
n=1
KN	N
V = λ 0 +∑ fink HX n - g (hnl Wdec ) H F, V = V0 + 2 fTnD
k=1 n=1	n=1
end for
N
-	Eq(yn)q(β)
n=1
e-βkXn-g ( hnl Wkec) k F
N
-	Eq(yn)q(β)
n=1
K
ynk
k=1
(log β - log π) - βHXn - g(hn|Wdkec)H2F
NK
-	Eq(yn)[ynk]
n=1 k=1
TnDE(Eq(β)[iogβ] - log∏) - Eq(β)[β]HXn - g(hnlWd^ec)HF}
N K	N K	TD
=E q( β) [ β ] E fτnk HXn - g ( hnl Wdec ) H F -E f'nk Tnr (E q( β)睡 β ] - 唳 ∏ )
n=1 k=1	n=1 k=1
NK
8 E Σ>nkHX n - g ( hnl Wkec ) H F + 5.
n=1 k=1
where we used rnk = Eq(yn) [ynk]. We achieve this by applying RNN algorithm. From the above
discussion including Appendix A.1, we obtain the MDRA algorithm.
A.3 DERIVATION OF EGamma(β∣ν,λ) [log β]
By putting β = ex, we obtain x = logβ, dβ = exdx,
E Gamma( β∣ν,λ) [log β ]= r log β^^ 3“71"&3 = [ XM ( ©"厂一少入嘎切2
0	Γ(V)	Γ(V)
=Z x Γ( ) ex(ν-1)e-" exdx = Z x ； ) exν-λex dx.
We here use
dexν-λex = xexu-x(ex
dV
13
Under review as a conference paper at ICLR 2020
then the above equation is
EGamma(β∣ν,λ) [log β]= f	[-" dx = λy d [ e"…dx.
Γ(ν)dν	Γ(ν)dν
In addition, 0∞ xν-1e-λexdx is the normalization constant of gamma distribution, therefore it
equals to Γ(V)/λν. Hence We finally obtain
λν d Γ(ν)	λν Γ0(ν)λν - Γ(ν)λνlogλ
E G~( βiν,λ )[log β ] = Γ(V) dW = Γ(V)	R
=r(ν) — logλ = d logΓ(V) — logλ = ψ(V) — log λ.
Γ(ν )	dν
B Parameter setting
In this section, We shoW the parameter setting of the experiments in Section 5.
Table 3: Parameter setting (periodic signals)
	L	EUNN				VB				
		cap.	fft	cpx	K	θ0	V0	λ O
RNN-AE	^T6^	4	T	F				
MDRA	^16^				~τ~	5.0	^Γ0~	~0.Qr
Table 4: Parameter setting of Driving identification
	L	EUNN			VB			
		cap.	fft	CPx	K		V0	λ 0
RNN-AE	32	8	T	F				
MDRA	32					~0:83~	100.0	~0.0T
Here L is the dimension of hidden variable h, capacity, fft and cpx are parameters of EUNN
(Jing et al., 2017), K is the number of the decoders, θ0, V0, λ0 are hyperparameters of prior dis-
tributions.
C Example of Driving data
We applied our algorithm to the driving data in the Section 5.2. We used the differential signals of
speed, acceleration, braking and steering angle signals in the experiment. Fig. 10 shoWs examples
of acceleration signals.
Figure 10: Examples of driving data
14
Under review as a conference paper at ICLR 2020
D	Neural network for driver identification
We estimated the feature extraction performance of MDRA in the Section 5.2. We employed the
quite simple neural network in Fig. 11.
Figure 11: Fully connected neural network for classification
15