Under review as a conference paper at ICLR 2020
Provab le Benefits of Deep Hierarchical RL
Anonymous authors
Paper under double-blind review
Ab stract
Modern complex sequential decision-making problem often requires both low-
level policy and high-level planning. Deep hierarchical reinforcement learning
(Deep HRL) admits multi-layer abstractions which naturally model the policy
in a hierarchical manner, and it is believed that deep HRL can reduce the sam-
ple complexity compared to the standard RL frameworks. We initiate the study
of rigorously characterizing the complexity of Deep HRL. We present a model-
based optimistic algorithm which demonstrates that the complexity of learning
a near-optimal policy for deep HRL scales with the sum of number of states at
each abstraction layer whereas standard RL scales with the product of number of
states at each abstraction layer. Our algorithm achieves this goal by using the fact
that distinct high-level states have similar low-level structures, which allows an
efficient information exploitation and thus experiences from different high-level
state-action pairs can be generalized to unseen state-actions. Overall, our result
shows an exponential improvement using Deep HRL comparing to standard RL
framework.
1	Introduction
Reinforcement learning (RL) is a powerful tool to solve sequential decision making problems in vari-
ous domains, including computer games (Mnih et al., 2013), Go (Silver et al., 2016), robotics (Schul-
man et al., 2015). A particular feature in these successful applications of RL is that these tasks are
concrete enough to be solved by primitive actions and do not require high-level planning. Indeed,
when the problem is complex and requires high-level planning, directly applying RL algorithms
cannot solve the problem. An example is the Atari game Montezuma’s Revenge, in which the agent
needs to find keys, kills monsters, move to correct rooms, etc. This is notoriously difficulty problem
that requires more sophisticated high-level planning.
Hierarchical reinforcement learning (HRL) is powerful framework that explicitly incorporate high-
level planning. Roughly speaking, HRL divides the decision problem into multiple layers and each
layer has its own state space. States in higher layers represent more abstraction and thus higher layer
states are some time named meta-states. When number of layers of abstraction is large, we call this
framework, deep hierarchical reinforcement learning (deep HRL). In deep HRL, the agent makes
decisions by looking at states from all layers. The dependency on higher layer states represents
high-level planning. HRL has been successfully applied to many domains that require high-level
planning, including autonomous driving (Chen et al., 2018), recommendation system (Zhao et al.,
2019), robotics (Morimoto & Doya, 2001). Recently, extension to imitation learning has also been
studied (Le et al., 2018).
While being a practically powerful framework, theoretical understanding on HRL is still limited.
Can We Provably show the benefit of using HRL instead of naive RL? In particular, What can We
gain from multi-layer abstraction of deep HRL? Existing theories mostly focus on option RL setting,
Which transits from the upper layer to loWer layer When some stopping criterion is met (Fruit et al.,
2017). This is different from our setting requiring horizons in each layer to be the same, Which
is common in computer games and autonomous driving Moreover, the number of samples needed
in Fruit et al. (2017) is proportional to the total number of states and total number of actions. In our
setting, both the total number of states and number of actions can be exponentially large and hence
their algorithm becomes impractical.
We initiate the study of rigorously characterizing the complexity of deep HRL and explaining its
benefits compared to classical RL. We study the most basic form, tabular deep HRL in Which there
1
Under review as a conference paper at ICLR 2020
are total L-layers and each layer has its own state space s` for ' ∈ [L]. One can simply apply
classical RL algorithm on the enlarged state space S = Si ×S2 ×∙× Sl. The sample complexity
will however depend on the size of the enlarged states space |S| = ∏L=ι |S'|. In this paper, we
show because of the hierarchical structure, we can reduce the sample complexity exponentially,
from α poly(∏L |S'|) to α PL=I Poly(IS'|). We achieve this via a model-based algorithm which
carefully constructs confidence of the model in a hierarchical manner. We fully exploit the structure
that lower-level MDPs of different high-level states share the same transition probability, which
allows us to combine the information collected at different high-level states and use it to give an
accurate estimator of the model for all low-level MDPs. Due to this information aggregation, we are
able to improve the sample complexity bound. To our knowledge, this is the first theoretical result
quantifying the complexity of deep HRL and explain its benefit comparing to classical RL.
Organization. This paper is organized as follows. In Section 2 we discuss related work. In Sec-
tion 3, we review basic RL concepts and formalize deep HRL. In Section 4, we present our main
algorithm and present its theoretical guarantees. In Section 5, we give a proof sketch of our main
theorem. We conclude in Section 6 and defer some technical lemmas to appendix.
2	Related Work
We are going to provide several related literatures on tabular MDP and hierarchical reinforcement
learning in this section.
As for tabular MDP, many works focus on solving MDP with a simulator which can provide samples
of the next state and reward given the current state-action pair. These work includes Lattimore &
Hutter (2012); Azar et al. (2013); Sidford et al. (2018b;a); Agarwal et al. (2019). Since we do not
need to consider the balance between exploration and exploitation, this setting is easier than the
setting of minimizing the regret.
There are also a line of work on analysis of the regret bound in RL setting. Jaksch et al. (2010)
and Agrawal & Jia (2017) propose a model-based reinforcement learning algorithm, which esti-
mates the transition model using past samples and add a bonus to the estimation. Their algorithms
achieve regret bound O(√H4S2AT) and O(√H3S2AT) respectively. Later, the UCBVI algo-
rithm in Azar et al. (2017) adds bonus term to the Q function directly, and achieves the regret bound
O(√H2SAT), which matches the lower bound when the number of episode is sufficiently large.
Adopting the technique of variance reduction, the vUCQ algorithm in Kakade et al. (2018) im-
proves the lower order term in the regret bound. Jin et al. (2018) proposed a model-free Q-learning
algorithm is proved to achieve regret bound O(7H3SAT).
Hierarchical reinforcement learning Barto & Mahadevan (2003) are also broadly studied in Dayan
& Hinton (1993); Parr & Russell (1998); Sutton et al. (1999); Dietterich (2000); Stolle & Precup
(2002); Bacon et al. (2017); Florensa et al. (2017); Frans et al. (2017). The option framework, which
is studied in Sutton et al. (1999); Precup (2001), is another popular formulation used in hierarchical
RL. In Fruit et al. (2017), the regret analysis is carried on option reinforcement learning, but their
analysis only applies to the setting of option RL. To our current knowledge, there is no such work
analyzing the regret bound of multi-level hierarchical RL.
3	Preliminaries
3.1	Episodic Markov Decision Process
In this paper, we consider finite horizon Markov decision process (MDP). An MDP is specified by
a tuple (S, A, H, P, r), where S is the (possibly uncountable) state space, A is a finite action space,
H ∈ Z+ is a planning horizon, P : S × A → 4 (S) is the transition function, andr : S × A → [0, 1]
is the reward function. At each state s ∈ S, an agent is able to interact with the MDP by playing
an action a ∈ A. Once an action a is played on state s, the agent receives an immediate reward
r(s, a) ∈ [0, 1] 1, and the state transitions to next state s0 with probability P (s0 Is, a). Starting
1Here we only consider cases where rewards are in [0, 1], but it is easy to know that this result can generalize
to rewards in a different range using standard reduction Sidford et al. (2018a). We can also generalize our
2
Under review as a conference paper at ICLR 2020
from some initial state s1 ∈ S (draw from some distribution), the agent is able to play H steps (an
episode) and then the system resets to another initial state s1 sampled from the initial distribution.
For an MDP, our goal is to obtain an optimal (will be precise shortly) policy, π : S → A, which
is a function that maps each state to an action. If an agent always follows the action given by a
policy π, then it induces a random trajectory for an episode: s1 , a1 , r1, s2, a2, r2, . . . , sH, aH, rH
where ri = r(s1,a1), s2 〜 P(∙∣sι,αι), a2 〜 ∏(s2), etc. The value function and Q-function
at step h of a given policy is defined as Vhπ(s) = Eπ PhH0=h rh0 sh = s and Qhπ(s, a) =
Eπ PhH0=h sh = s, ah = a , where the expectation is over all sample trajectories. Then the op-
timal policy, ∏*, is defined to be the policy with largest VTr(si) for all si ∈ S. For any optimal
policy, its value and Q-function satisfy the following Bellman equation
∀s ∈	S,a	∈A,h	∈ [H] :	Qh(S,a)	=	r(S,a) + EsO〜P(∙∣s,a)Vh+i (SO),	V∕Γ (S)= max Qh(S,	a)
a∈A
and VH +i(s) = 0.	(1)
We consider the MDP problem in the online learning setting, where the probability transition is
unknown. However, our goal is still to collect the maximum amount reward, i.e., play a policy that
is comparable to the optimal one. Therefore, the agent needs to learn to play through trial and error,
i.e., improving the policy by learning from experiences. Suppose we allow the agent to play in total
K ≥ 1 episodes. For each episode, the agent is following a policy πk, which is computed based on
her experiences collected from episodes 1, 2, . . . , k - 1. To measure the performance of the agent,
we use the following standard regret formulation, which compares the reward collected by the agent
to the performance of an optimal policy.
K
R(K) = X (V1*(S1)- Vιπk(si)) .	(2)
k=i
Note that, if the agent learns nothing, We then expect R(K) 8 K. But if the agent is able to learn,
then the average regret, R(K)/K, which measures the average error per step, goes to 0 when K
becomes large. In the online MDP literature, model based algorithms (e.g. Jaksch et al. (2010))
achieves regret R(K) ≤ O(PH2∣S∣2∣A∣HK).
3.2	Deep Hierarchical MDP
In this section we introduce a special type of episodic MDPs, the hierarchical MDP (hMDP). If we
view them as just normal MDPs, then their state space size can be exponentially large. Formally,
each hMDP consists of L levels of episodic MDPS with the '-th level having planning horizon h`.
One can view the `-th level MDP as a subtask of the (`+ 1)-th level MDP. To transition between two
state in (` + 1)-th level, the agent needs to play an episode in `-th level MDP (the state transition
will be defined formally shortly). Therefore, the total planning horizon of the hierarchical MDP is
H = QL=I h'.
For each step h in the hMDP, we can represent it by a tuple (hi, ∙∙∙ ,hL), where h` ∈ [H'] is the
step of '-th level MDP for 1 ≤ ' ≤ L. We use H' = QL=' Hi to denote the effective horizon oflevel
',which represents the total number of actions in A' ∪∙∙∙∪ Al needed to be played in an episode
of the full hMDP. Note that, for each h = (hi,…，方工)∈ [1, H], we have h +1 = (h；…，hL) is
the immediate next lexicographical tuple of (hi,…，Al).
We now describe how the agent can interact with the full hMDP. In fact, in each step h, only an
action in one level can be played. This level is given by the function σ : [H] → [L], formally
defined as
σ(h) = arg max{h' = H'} + 1.
It characterizes the lowest level of MPDs which does not reach the last step in its horizon.
result to the setting where the reward is stochastic, since estimating the reward accurately requires much fewer
samples than estimating the transition.
3
Under review as a conference paper at ICLR 2020
Figure 1: Demonstration of Hierarchical MDP: 3 levels of MDP, level 1 (lowest level) has 3 states,
level 2 has 3 states, level 3 (highest level) has 4 states, the total number of states is 36.
To make it formal for the state-transition, We use s` to denote the set of states at level ', a` to
denote the set of actions at level '. To be convenient, we assume for every ' ∈ [L], for any h`-
length trajectory in the '-th level MDP, the last state always falls in S' ⊂ Si, which we call as
the endstates in level `. At step h of the full hMDP, the full state is described as a length-L tuple:
(si, ∙∙∙ ,sl). For any 1 ≤ ' < σ(h), we immediately have s` ∈ S' is an endstate of level '. Note
that the total number of states of the full MDP is QL=I |S'|, which is exponentially larger than the
average size of a level’s state space.
Now we define the transition. At step h of the full hMDP, the agent plays an action aσh(h) ∈ Aσ(h)
for the σ(h)-level MDP. The state of this MDP triggers a transition at level σ(h):
sσ(h)〜P ( । sσ(h) aσ(h) sσ(h)-1)
sh+1 P ∖ 1 sh	, ah	,sh
Note that the probability transition is determined by the state-action-ending-state tuple
(sσh(h), aσh(h), sσh(h)-1), instead of single state-action pair. Moreover, all the MDPs with level lower
than σ(h) will reset their state based on some initial distribution P0i:
sih+1 〜P0 (∙), ∀1 ≤ i ≤ σ(h) - 1,
and all the MDPs with level higher than σ(h) will keep their states unmoved.
For any given ` ∈ [L], we use E' to denote the state-action-ending-state tuple at level `:
E' = {(s', a', s'-1 ) | s' ∈ S', a' ∈ A', s'-1 ∈ S'0-1 }.
As for the reward, we use r(shh,…，sL, ah(h)) ∈ [0,1] to denote the immediate reward obtained
after executing the action ahσ(h). We illustrate the hierarchical MDP model in Figure 1.
An Example: Autonomous Driving. We here give a more concrete example. Suppose we want
our vehicle to reach the destination, while not hitting obstacles or crashing into another vehicles or
pedestrians. We use the following hierarchical MDP structure to formulate this problem.
4
Under review as a conference paper at ICLR 2020
Level 1 represents the status (e.g. position on the road, whether has an obstacle in the front) of the
vehicle, and the ending state represents whether the vehicle avoids all the obstacles, other vehicles,
pedestrians and arrives at the end of the road. Level 2 represents the road map, and the ending state
represents whether the vehicle reaches the desired position.
At each time step, if the vehicle does not reach the end state of level 1, that is, it still on the road and
not at a crossing, then the vehicle needs to decide whether speeding up, slowing down or dodging
the obstacle in the front. If the vehicle reaches the end state of level 1, that is, it arrives at the end
of a road, then it needs to decide whether going straight forward, turning left or turning right. This
process ends if and only if the vehicle reaches the desired position.
3.3	Deep Hierarchical Reinforcement Learning Objective
Suppose the environment is an hMDP. The hierarchical structure and the reward is known but the
transition models are not known. Similar to the classic RL setting, our agent needs to interact with
the unknown hMDP while being able to accumulate the amount of rewards comparable to an optimal
policy. Our goal is design an algorithm that minimizes the regret defined in Equation (2).
Since an hMDP is very special compared to a normal MDP, we redefine its related quantities here.
The policy π is a mapping from Si ×∙∙∙×Sl X [H] to Ai ∪∙∙∙ Al, where π(s1,…，sL,h) ∈ a`
if and only if σ(h) = ', s1 ∈ Sj,…，s'-1 ∈ S'7 and s' ∈ s`,…，sL ∈ Sl. Given a policy π,
and step h, the value function and Q function are again defined in Equation (1), but can be rewritten
as,
Qh (s1,…，sL； a) = r(s1,…，s'； a)
+ EgI〜P1,…,g'-1 〜P'-1,g'〜P(∙∣e)Vh+l(S1,…，g', s'+1 …，sL),
Vhr(s1,…，sL) = Qh(SI,…，sL； ∏(s1,…，sL,h))
VH+1(s1，…，SL)= 0, ∀s' ∈ S', 1 ≤ ' ≤ L.
Our objective is to find a policy ∏* such that the value function Vhr is maximized for all states and
steps in a horizon. We use Vh= and Qh to denote the optimal value function and optimal Q-function,
which is the value function and the Q-function when applying the optimal policy ∏*.
4	Algorithm
In this section, we will present a model-based hierarchical reinforcement learning algorithm, to-
gether with its regret bound analysis.
4.1	Model-based Hierarchical Reinforcement Learning Algorithm
To formally present our algorithm, we first explain the high-level ideas. Note that the full model size
is O Q'L=i |S' ||E' | , where |S' | is the number of states in level ` and |E' | is the number of state-
action-endstate tuples in level `. However, we notice that there are rich structures for the algorithm
to exploit: low-level MDPs corresponding to different high-level states share the same transition
model. Recall that, our eventual goal to learn the hierarchical model with number of samples much
less than Q'L=i |S' ||E' |. To achieve this goal, we group our samples obtained from transition models
by state-action-endstate tuple, and samples obtained from initial distributions by levels: even if the
samples are collected at a different high-level state-action pair, they are grouped to a same set as
long as they come from a same state-action-endstate pair. We then use the samples from a level to
estimate the MDP initial distribution corresponding to that level. In effect, to estimate all the MDPs
corresponding to a level accurately, we only need to visit this level a number of times proportional
to the size ofa single MDP in this level, which is far smaller than the model of all MDPs in this level
combined.
Next, we explain how we can deploy the algorithm in the online setting, where we can only visit
a state by following the trajectory of some policy. Initially, we have no knowledge of the MDPs
corresponding to each level, we just initialize each of them to an arbitrary MDP. Suppose we play
the whole game for K episodes, and for each episode, we play H = Q'L=i H' steps, where H'
5
Under review as a conference paper at ICLR 2020
is the horizon of an MDP in level `. Suppose at episode k ∈ [K] and step h ∈ [H], the transition
happens at level' (which means σ(h) = '). We denote the full state We observed as (Shk,…，$L k)
and the action we take as。力k.Then we collect data samples of the form (ehh k, shh+ι 卜),where
ehh,k = (Sh,k, ahh,k, sm-1) ∈ e` is a state-action-endstate tuple at level', and also data samples of the
form (i, sh+ι Q for every 1 ≤ i ≤ ' - 1. We add them to a buffer, Sh+1卜 to “九# corresponding
to state-action-endstate tuple eh,k, and Sih+1,k to Mi corresponding to level i. Here Mi and Neh,k
are multisets, i.e., their elements can be repeated. We use Neh,k to estimate probability transition
P(∙∣ehh k), and Mi to estimate P0(∙) respectively.
When a new episode k starts, we first do a model estimation based on all the samples collected and
partitioned. Using this estimated model, a value function and a Q-function is computed. However,
the estimation error always exists in the estimated model due to insufficient samples from certain
state-action-endstate tuples. To account for this, we estimate the model uncertainty based on con-
centration inequalities. Specifically, we add the uncertainties to our value function estimator, and
use the modified value function to play for the new episode. Note that doing so encourages explo-
ration on unexperienced state-action pairs. In fact, as we will show shortly, by using appropriate
uncertainty estimation, the model becomes more accurate if the algorithm makes a mistake (e.g.,
by playing a less-optimal action). With a pigeon hole principle argument, we can show that our
algorithm achieves a low regret bound.
Our model-based algorithm is formally presented in Algorithm 1. We denote our estimator of the
initial distribution at level ` as
P”#{SMM},	⑶
where #{s ∈ M'} and |Me| are the number of appearance of state S in buffer m` and the total
number of states in buffer m`, respectively. We also denote our estimator of transition distribution
at state-action-endstate tuple e as
Pk (s|e)
#{s ∈ Ne}
NeI
(4)
where #{S ∈ Ne} and |Ne| are the number of appearance of state S in buffer Ne and the total number
of states in buffer Ne, respectively. With this these estimators, we use dynamic programming to
solve for the Q-function and value functions as follows,
Qh(S1,…，SL; a) = r(s1,…，SL; a) + b(k, h, ', e)
十 %〜理,0,…,S〜户31 星〜Pk(∙∣e)Vh+1(s1,…，s', S'+1 …，SL),
Vhk(S1,…，SL) = min [ H, max [Qh(S1,…，SL； a)]],
a	a∈S'	J
(5)
where for 1 ≤ h ≤ H, ' = σ(h), e ∈ E' and VH+1(S1,…，SL) = 0. Here the bonus function
b(k, h, `, e) is used to estimate the uncertainty of the Q, V estimator, which are defined as follows:
b(k, h, ', e) = Hmin (l,产耳*巫画)
'-1
+ H min
i=1
8(|Si| + log(4L2∣Si∣∣E'∣k2∕δ))
. ~
(k- 1)Hsi
(6)
,
where δ is a constant between [0, 1] to be specified before, and n(k - 1, e) is the number of times
we encountered state-action-endstate tuple before k-horizon. These bonus functions bound the dif-
ference between the estimated Q-functions to the exact value (per-step) with high probability. For
episode k, our exploratory policy is then
πk(S1,…，SL, h) = arg max [Qh(S1,…，SL; a)].
α∈A' l	j
(7)
6
Under review as a conference paper at ICLR 2020
Algorithm 1 Model-based Algorithm for Hierarchical RL
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
Input: An MDP with hierarchical structure, δ
Initialize: m` = Ne =0 (elements repeatable) for every 1 ≤ ' ≤ L,e ∈ e`;
Initialize: b(k, h, `, e) as in formula (6)
for k = 1 : K do
Calculate Vhk, Qkh, πk with uses formula (5), (7).
for h = 1 : H do
Play action aσ(h) = argmaχa∈Aσ(h) IQh(Sh,k, …，sL,k ； a)]；
GetneXtstate (* sh+ι,k,…，sh+ι,k)；
for i = 1 : σ(h) do
PUt sh+ι,k intoM`;
Put sh+1,k into N(s' fc,a' fc,s'-ι).
h,k, h,k, h,k
Update
P' / ʌ _ #{s ∈ m'}	UV
Pk,o(s) =	M^	，	∀1 ≤ ' ≤ L，S ∈ S'，
Pk(SW= #{SN Ne} ,	∀1 ≤ ' ≤ L,e ∈E',s ∈S'.
|Ne |
4.2 Regret Bound for Algorithm 1
In this subsection we provide a formal guarantee for Algorithm 1. We present a proof sketch in the
neXt section, and the full proof is deferred to appendiX.
Theorem 4.1. Suppose we run Algorithm 1 for K ≥ 1 episodes on an hMDP. For k ∈ [K], let πk
be policy played by the algorithm in episode k. Then we have, with probability at least 1 - δ,
R(K ) = X O (HH'|E'| + HqKH ∙∣E'∣(∣S'∣ +log δ-1)).
'=1	×	)
where δ ∈ (0, 1) and R(K) is defined in Equation (2).
From this theorem, We observe that the regret bound only depends on PL= 1，|S'||Ee|, where ∣E'∣ =
∣S'∣∣A'∣∣S'-ι∣ (here ∣E'-∕ is the number of endstates at level ' - 1). Usually, the number of
actions and the number of endstates at a level are much smaller than the number of states and
can be viewed as constant. In this way our regret bound only depends on PL=ι ∣S'∣. It means
after K = ΩΩ (PL=I HqHp'∣E'∣∣S' ∣) episodes, the algorithms achieves a constant average regret
R(K)/K = O(1) (this is when the agent learns a meaningful amount of information). Let us
consider the full hMDP, whose state space size is QL=ι ∣S'∣. With a model based or model free
algorithm like Jaksch et al. (2010); Jin et al. (2019), the number of episodes needed would be K &
QL= ι ∣S'∣ to achieve a constant average regret. Note that QL= ι ∣S'∣ can be exponentially larger than
Poly(P'=ι ∣S'∣), therefore our algorithm achieves an exponential saving in the sample complexity
for RL.
5 Proof S ketch
The proof of Theorem 4.1 is divided into two parts. In the first part, we prove that with high
probability, the difference between empirical expectation and true expectation of the value function
can be bounded by the bonus b. The proof of this property involves estimation of the total variation
(TV) distance between a distribution on the state space S' of level ' and its empirical estimation
using n samples. This TV distance can be bounded by O( ∙∖∕∣S'∣∕η) with high probability.
The second part of proof tells that if the difference between empirical expectation and true expec-
tation of the value function can be bounded by the bonus b, then the estimator Qkh of Q function
7
Under review as a conference paper at ICLR 2020
is always an optimistic estimation to the true Q-function with high probability. That is, for every
si ∈ Si, We have Qh(s1,…，SL) ≥ Qh(s1, ∙∙∙ , sL). Then We can show that the regret can be
upper bounded by the sum of all bonuses along the sample path. Hence we can obtain the regret
bound by summing over all bonuses in each step and horizon. We notice that at level ` there are
only |E'| distributions we need to estimate, and each one is a distribution on s`. Therefore applying
Holder inequality we obtain the regret bound P3 O(，|S'||E'|K), where we put the dependence
on H and δ into O.
6 Conclusion
In this paper we prove the benefit of hierarchical reinforcement learning theoretically. We propose
a model-based hierarchical RL algorithm which achieves a regret bound that is exponentially better
than the naive RL algrorithm. To our knowledge, this is the first theoretical result demonstrating the
benefit of using deep hierarchical reinforcement learning. Below we list two future directions.
Deep Hierarchical Reenforcement Learning with Function Approximation The current work
focuses the most basic formulation, tabular RL: When state space is large, function approximation is
required for generalization across states. Recently, a line of work gave provably polynomial sample
complexity upper bound for RL with function approximation under various assumptions (Wen &
Van Roy, 2013; Du et al., 2019; Jiang et al., 2017; Yang & Wang, 2019; Jin et al., 2019). An
interesting direction is to combine our analysis with these results and obtain guarantees on deep
HRL with function approximation.
Deep Hierarchical Reenforcement Imitation Learning Imitation learning is another paradigm
where expert’s trajectories are available to the agent. Le et al. (2018) presented a framework to com-
bine hierarchical learning and imitation learning. However, there is no formal statistical guarantee.
We believe our analysis can be leveraged to understand deep hierarchical imitation learning too.
References
Alekh Agarwal, Sham Kakade, and Lin F Yang. On the optimality of sparse model-based planning
for markov decision processes. arXiv preprint arXiv:1906.03804, 2019.
Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret
bounds. In NIPS, 2017.
Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325-349, 2013.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. arXiv preprint arXiv:1703.05449, 2017.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Thirty-First AAAI
Conference on Artificial Intelligence, 2017.
Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning.
Discrete event dynamic systems, 13(1-2):41-77, 2003.
Jianyu Chen, Zining Wang, and Masayoshi Tomizuka. Deep hierarchical reinforcement learning for
autonomous driving with distinct behaviors. In 2018 IEEE Intelligent Vehicles Symposium (IV),
pp. 1239-1244. IEEE, 2018.
Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural infor-
mation processing systems, pp. 271-278, 1993.
Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decompo-
sition. Journal of artificial intelligence research, 13:227-303, 2000.
8
Under review as a conference paper at ICLR 2020
Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Lang-
ford. Provably efficient RL with rich observations via latent state decoding. arXiv preprint
arXiv:1901.09018, 2019.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical rein-
forcement learning. arXiv preprint arXiv:1704.03012, 2017.
Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared
hierarchies. arXiv preprint arXiv:1710.09767, 2017.
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Emma Brunskill. Regret minimization in mdps
with options without prior knowledge. In Advances in Neural Information Processing Systems,
pp.3166-3176, 2017.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal ofMachine Learning Research, 11(APr):1563-1600, 2010.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are PAC-learnable. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 1704-1713. JMLR. org, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. arXiv preprint arXiv:1907.05388, 2019.
Sham Kakade, Mengdi Wang, and Lin Yang. Variance reduction methods for sublinear reinforce-
ment learning. 02 2018.
Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In International Conference on
Algorithmic Learning Theory, pp. 320-334. Springer, 2012.
Hoang Minh Le, Nan Jiang, Alekh Agarwal, Miroslav Dudik, Yisong Yue, and Hal Daume. Hierar-
chical imitation and reinforcement learning. ArXiv, abs/1803.00590, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Jun Morimoto and Kenji Doya. Acquisition of stand-up behavior by a real robot using hierarchical
reinforcement learning. Robotics and Autonomous Systems, 36(1):37-51, 2001.
Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In Advances
in neural information processing systems, pp. 1043-1049, 1998.
Doina Precup. Temporal abstraction in reinforcement learning. 2001.
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region
policy optimization. 02 2015.
Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample
complexities for solving markov decision processes with a generative model. In Advances in
Neural Information Processing Systems, pp. 5186-5196, 2018a.
Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster
algorithms for solving markov decision processes. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms, pp. 770-787. Society for Industrial and Applied
Mathematics, 2018b.
David Silver, Aja Huang, Christopher Maddison, Arthur Guez, Laurent Sifre, George Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with
deep neural networks and tree search. Nature, 529:484-489, 01 2016. doi: 10.1038/nature16961.
9
Under review as a conference paper at ICLR 2020
Martin Stolle and Doina Precup. Learning options in reinforcement learning. In International
Symposium on abstraction, reformulation, and approximation, pp. 212-223. Springer, 2002.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-
211, 1999.
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. In-
equalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
2003.
Zheng Wen and Benjamin Van Roy. Efficient exploration and value function generalization in deter-
ministic systems. In Advances in Neural Information Processing Systems, pp. 3021-3029, 2013.
Lin F Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and
regret bound. arXiv preprint arXiv:1905.10389, 2019.
Dongyang Zhao, Liang Zhang, Bo Zhang, Lizhou Zheng, Yongjun Bao, and Weipeng Yan. Deep
hierarchical reinforcement learning based recommendations via multi-goals abstraction. arXiv
preprint arXiv:1903.09374, 2019.
10
Under review as a conference paper at ICLR 2020
A Proof of Theorem 4.1
A. 1 Notations for the Proof
We will specify some useful notations in the proof first.
To be convenient, We use Vh[K],Qh[K, a] to denote Vh(Skhj ∙∙∙ ,sLhL) and
Qh(Sk h, ∙∙∙ ,sLh； a). We use Vκ[k]0' to denote the ∣Sι∣ X … X |S'|-dimensional tensor
whose (xι,…，x')-element is Vh(X1,…，x', sk++ι,…，sL,h+ι). Given probability distribution
Pi(∙) (1 ≤ i ≤ ') over the state space Si, we use Pi to denote the operator over tensors:
Pi[V∏k]] = X P i(si)Vhι,∙∙∙ ,hL (∙,…，Si, ∙∙∙ , ∙,X',sk+h1+ι, ∙∙∙ ,sL,h+ι),	(8)
si∈S
which can be understood as taking the expectation on the i-th element. We can also define the tensor
product operator Pi1 0 Pi2, ∙∙∙0 Pij for different iι, ∙∙∙ ,j as the composite operator, where each
Pij is a operator of dimension |Sij |.
A.2 Proof of Theorem 4.1
We present the proof of Theorem 4.1 in this subsection. In the next we use ek,h to denote the
state-action-endstate pair (Slk,h, alk,h, Slk-,h1).
We first present a lemma indicating that with high probability, the difference between the empirical
expectation of the value function is bounded by the bonus.
Lemma A.1. With probability at least 1 - δ, we have
I [P1,0 0 …0 Pk-01 0 Pk(∙∣e) - P1,0 0 …Pk-01 0 Pk(∙∣e)i Vk+ι[k产I ≤ b(k, h,', e)
for every k〉1,1 ≤ h ≤ H, 1 ≤ ' ≤ L and e ∈ e`.
Proof. Given any k ≥ 1, 1 ≤ h ≤ H, 1 ≤ ` ≤ L and e = (Sl, al, Sl-1) ∈ Ek, we have the following
estimation of error between estimated transition model and true transition model.
∣ [Pl,。0 …0 Pk-1 0 Pk (∙∣e) - Pk,o 0 …P'-o10 Pk (∙∣e)i vk+1[k 产 I
k-1
≤ X ∣ [Pl,。0 …Pk,o …0 Pk- 0 Pk(∙∣e) - Pl,。0 …Pk,。…Pk- 0 Pk(∙∣e)] Vk+1[k产∣
i=1
+ I [p1,0 0 …P'-01 0 Pk(∙∣e) - P1,0 0 …P'-01 0 Pk(∙∣e)i Vk+1[k产I
k-1
=XI [[Pk,0 - Pk,oi 0 P1,0 0…Pk- 0Pk#…0 Pk-01 0 Pk (∙∣T Vk+1[k 产 I
i=1
+ I h[Pk(∙∣e) - Pk(∙∣e)i 0 P1,0 0 …Pk-01 - Pl,0 0 …P'-o1] Vk+1[k产I
k-1
≤ X I% - Pk,0∣∣1∣∣hp1,0 0…Pk-01 0 Pk+o1 …0 PkO 0 Pk (∙∣e)i vk+1[k]0k]
i=l	l	∞
+ ∣∣Pk(∙∣e) - Pk(∙∣e)∣∣1 ∣∣[P1,0 0 …Pk-01 - Pl,0 0 …P'-01i Vk+1[k产∣∣∞
k-l
≤ X ∣∣Pk,0 - Pk,0∣∣ J H + ∣∣Pk(∙∣e) - Pk(∙∣e)∣ι ∙ H
According to Theorem 2.1 in Weissman et al. (2003), for any 1 ≤ i ≤ `- 1, with probability at least
1 - δ we have
~-- --
ii
Pk,0 - Pk,0
8(|Si| + log δ-1)
, ,~
(k - 1)Hi
(9)
11
Under review as a conference paper at ICLR 2020
1	. 1 Γ∙ . . 1	. ∙ 11 7 1	1	7 τ~T	1 C ,1 ♦ ∙ . ∙ 1	,	. ∙	Γ∙ 1	1 ■
where we use the fact that till k horizons, we have kHi samples for the initial distribution of level i.
Similarly, with probability at least 1 - δ we have
~ ,. . ..
Pk (Ie)- Pk(Ie)
Ii ≤ /8(|s'|+logδ-1y
111 - y n(k 一 1, e)
where we use n(k - 1, e) to denote the number of appearance of state-action-endstate tuple till k - 1
horizons (including k 一 1 horizon). Replacing δ with δ∕(4L2∣Si∣∣E'∣k2) and applying union bound
on all 1 ≤ i ≤ `, we obtain
`-1
X ∣∣Pk,0 - Pk,0∣L ∙ H + ∣∣Pk(∙∣e) - Pk(∙∣e)∣ι ∙ H
≤H
(产鼻里…)+ H X min
8(∣Si∣ + log(4L2∣Si∣∣E'∣k2∕δ))
, , ~
(k 一 1)Hi
with probability at least 1 一 6/(4L|Si||&|k2). Therefore, noticing that
IhP1,0 乳…乳 Pk-乳 Pk(∙∣e) - P1,0 乳…隼1 乳 Pk(∙∣e)i Vh+ι[k产I ≤ H
if we choose
b(k,h,`,e)= H mm (1, JS'1+巾”9)
+ HXmin (l, S8(1Si1 + log(4Lg||E'|三),
⅛ 「V	(k 一 DHi	广
(10)
then we have
I [P1,0 乳…乳 P',01 乳 Pk(∙∣e) - P1,0 ㊈…Pk-乳 Pk(∙∣e)i Vh+ι[k产I ≤ b(k, h, ', e)
with probability at least 1 一 δ∕(2L∣E'∣k2).
Finally, we apply the union bound on all k ≥ 1, 1 ≤ ` ≤ L and e ∈ Ek ,
I [P1,0 乳…乳 Pk-乳 Pk(∙∣e) - P1,0 0…PkO 乳 Pk(∙∣e)i Vh+ι[k产I ≤ b(k, h,', e
holds for every k ≥ 1,1 ≤ ' ≤ L,e ∈ e` with probability at least 1 一 δ.	口
Next we present a lemma indicating that if the event in Lemma A.1 holds, then Vhk is always an
optimistic estimation to the true value function Vj^.
Lemma A.2. Suppose the event in Lemma A.1 holds, then
Vh [k] ≥琮同
holds for every 1 ≤ h ≤ H and 1 ≤ k ≤ K.
Proof. We will prove a stronger version of this lemma. That is, for every (s1, ∙∙∙ ,sL) ∈ Si ×∙∙∙×
SL, we have
Vk(SI,…，sL) ≥ VnS1,…，sL).
We will prove this result by induction. When h = H, since Vk = Vh= = 0, the inequality Vh [k] ≥
Vh= [k] already holds. Next we assume that this inequality holds for h + 1, and we consider the case
h. For any (s1,…，SL) ∈ Si X …X SL, ak ∈ a`, we let e = (sk, ak, sk-i). Suppose σ(h)= ',
and for every i < `, Sk, Si are all end-state of level i. According to the events in Lemma A.1, we
have
Qh(SI,…，sl; ak)= r(s1,…，sL,ah')+ [P1,0 % …%$0 ㊈Pk (忖]Vk+i(…，∙,sl+i,…，sL)
+ b(k, h, `, e)
≥ r(S1,…，sL,	ah`) +	hP1,o	%…%	Pk-1	% Pk(,Ie)]Vk+i(…，∙, sl+i,…，SL)
≥ r(S1, ∙∙∙	,SL,	ah) +	hP1,o	%∙∙∙%	Pk-1	% Pk (Ie)i	Vh+ι(∙∙∙, ∙, Sl+1, ∙∙∙	,SL)
= Q=h[k, akh`],
12
Under review as a conference paper at ICLR 2020
where the first inequality uses the event in Lemma A.1, and last inequality uses the fact that Vhk+1 ≥
Vh+1 and [Pi,o 乳…乳P',o1 0Pk(∙∣e)]
is a nonnegative operator. Therefore, we have
Vk(s1,…，sL)=min ( max Qjh(S',…，sL a'), H
[a'∈A'
≥ min < max Qh(SI,…，sL; a'), H > = Vh= (s1,…，SL)
一 I a'∈A'	J
This indicates that this lemma holds for h, which completes the induction. Hence for every 1 ≤ h ≤
H, this lemma holds.	□
Equipped with these two lemma, we are ready to prove Theorem 4.1.
Proof. Suppose the event in Lemma A.1 always holds (which will happen with probability at least
1 - δ), then we can calculate the value function.
Vh [k] - VhK [k] = Qh[k,ah'] - Qnk [k,ah`]
=[PI,。乳…乳 PkO 乳 Pk(∙∣ej,ι)[ Vh+ι[k尸 + b(k, h, ', ek,h)
-[p1,0 乳…FO XPk(∙∣ek,ι)i Vh+ι[k产
=[p1,0 X …XPk-0 XPk(归,，)](Vh+ι[k产-Vh+ι[k产)+ b(k,h,',ej,h)
+ [P1,0 X …X P- X Pk(∙∣ek,ι) - P1,0 X …Pk-01 X Pk(∙∣ek,ι)i Vh+ι[k产
=(Vh+ι[k]- Vh+ι[k])+ξh+ι,k + b(k, h,',ek,h)
+ [P1,0 X …X Pk-01 X Pk(∙∣ek,ι) - P1,0 X …Plk-0 X Pk(∙∣ek,ι)i Vh+ι[k产
≤ (Vh+ι[k] - Vh+ι[k]) + ξh+ι,k + 2b(k, h,', ek,h),
where the last inequality uses Lemma A.2, and we define ξh+1,k as follows:
ξh+ι,k = [p1,0 X …XPk-0 XPk(∙∖ek,ι)i (Vh+ι[k产-Vh+ι[k产)-(Vh+ι[k]-啸用).
Sum up this inequality for all 1 ≤ h ≤ H, and noticing that VHk = VHπk = 0, we get
HH
V1k[k] - V1πk [k] ≤ X ξh+1,k + 2 X b(k, h, σ(h), ek,h),
h=1	h=1
which indicates that
K	K	KH	KH
X V1=[k] - V1πk [k] ≤XV1k[k] -V1πk[k] ≤ XX
ξh+1,k +2 XX
b(k, h, σ(h), ek,h). (11)
k=1	k=1	k=1 h=1	k=1 h=1
As for the first term in the above , it is easy to see that ξh+1,k is a martingale difference sequence
with respect to h, k. Since every ξh,k is bounded by H = Hi …HL, according to Azuma-Hoeffding
inequality we have
K H
XXξh+i ≤ 4HPHKlogδ-i	(12)
k=1 h=1
with probability at least 1 - δ.
13
Under review as a conference paper at ICLR 2020
Next, we will analyze the second term in equation 11. According to formula equation 10, we have
K H
ΣΣb(k,h,σ(h),ek,h)
k=1h=1
H X X min (1, S8(1J(h)f°TL21S，(h)!%(h)1k20) )	(13)
公 h=1	[ V	n(k - 1,ek.h)	J
}
K H σ(h)-1
+HEE E min
k=1h=1 i=1
8(|&| +log(4L2∣Si∣∣Eσ(h)∣k2∕δ))
. ~
(k - 1)H.i
In the first summation of the above equation, the one involved with ,1∕n(k - 1, ek,h) appears
n(k, ek,h) - n(k — 1, ek,h) times. And given 1 ≤ ' ≤ L, there exists H'-1 - h` choices of h such
that σ(h) = '. For given ', there are ∣E'∣ choices of n(k, e) where e ∈ e`. Therefore, we have
KH
E E min
k = 1 h=1,σ(h)='
K
ΣΣ(n(k, e) - n(k -
k = 1 e∈E'
8(∣Sg∣+log(4L2∣S'∣∣E'∣k2∕δ))]
n(k - 1,ek,h)	ʃ
1, e)) min
8(∣Sg∣+log(4L2∣S'∣∣E'∣k2∕δ))]
n(k - 1, e)
K
E E	(n(k,e) - n(k - 1,e))
e∈E' k=1,n(k-1,e)≤H'
卜(∣S'∣+log(4L2∣S'∣∣E'∣k2∕δ))
V	n(k - 1, e)
K
+ E E	(n(k,e) - n(k - 1,e))
e∈E' k=1,n(k-1,e)>H'
≤ 2H'-1∣E`∣ + 2 ∙ E	E (n(k, e) - n(k - 1, e))	匹与T三
k = 1,n(k-1,e)>H` e∈E'	V	,
K	n(k,e)
≤EE	E
k = 1 e∈E' j=n(k-1,e) + 1
：8( ∣ s` ∣ +logR：2 ∣ S' ∣∣E' ∣ k2∕δ))
n(K,e) /
EE
e∈E' j=1 V
. _ .
2H` ∣ e` ∣ +
8( ∣ S' ∣ +log(4L2 ∣ S' ∣∣E' ∣ k2∕δ))
j
≤ 2H`∣E' ∣ + E 2,8( ∣S' ∣ +log(4L2 ∣S' ∣∣ E∣k2∕δ)) ∙ √n(K, e)
e∈E'
≤ 2H`∣E'∣ + 2√8( ∣S' ∣ +log(4L21 S' 11 E' ∣k2∕δ)) ∙ J∣E' ∣ ∙ E n(K, e)
V	e∈E'
=2H' ∣E' ∣ + 2√8( ∣ S' ∣ +log(4L21 S' 11 E' ∣ k2∕δ)) ∙√ ∣E' ∣ ∙ K(He - Hm)
=O (H'∣E'∣ + JkH' ∙∣ E'∣( ∣S'∣ +log尸)),
where the third inequality uses the fact that for any e ∈ E' we have n(k, e) - n(k - 1, e) ≤ H', and
the second last equation uses the fact that Pe∈% n(K, e) is the number of all possible state-action-
endstate pair appears up to K horizons, which is K times the number of m such that σ(h)= '.
14
Under review as a conference paper at ICLR 2020
Therefore, the first term in equation 13 has the following estimation
)
K H
H ^X ^X min
k=1h=1
8(∣Sσ(h)∣ + log(4L2∣Sσ(h)∣∣Eσ(h)∣k2∕δ))
n(k,%h)
=XO (HH'∣E'∣ + H JkH` ∙ ∣E'∣(∣S'∣ + logδ-1)).
'=1	×	)
As for the second term in equation 13, we have
K H σ(h)-1
HXX X min
k=1 h=1 i=1
K L H	2-1
8(∣Si∣ +log(4L2∣Si∣∣Eσ(h)∣k2∕δ))
, ,~
(k - 1)Hi
H	min
k=1 '=1 h=1,σ(h)=' i=1
KLL	H
H ∑∑ X	X	min
k=1 i=1 '=i+1 h=1,σ(h)='
8(∣S∣ +log(4L2∣S∣∣E'∣k2∕δ))
8(∣S∣ + log(4L2∣S∣∣E'∣k2∕δ))
. . ~
(k - 1)Hi
, , ~
(k - 1)Hi
)
)
)
KLL	H	I
≤ HXXX	X	min "∖
k=1 i=1 '=i+1 h=1,σ(h)='	I ∖
8(∣S∣ +log(4L2∣Si∣(p=ι∣Ej∣)k2∕δ))
. .. ~
(k - 1)Hi
K L	I
HXXHl min 1,t
k=1i=1	I	∖
8(∣S∣ +log(4L2∣Si∣(p=ι∣Ej∣)k2 ∕δ))
. .. ~
(k - 1)Hi
L	K L
≤HXHi + HXX Hi
i=1	k=1 i=1
8(∣S∣ +log(4L2∣Si∣(EL=1 吗 ∣)k2∕δ))
. ~
(k - 1)Hi
≤ XO (HHi + H JkHi(∣Si∣ + logδ-1)),
i=1
where in the last inequality we apply the Holder inequality. Combined previous two estimations
together, we obtain that
KH
XX
b(k, h, σ(h), ek,h)
≤ XO (HH'∣E'∣ + HJkH' ∙∣E'∣(∣S'∣ +logδ-1))
'=1	'	)
+ X O (HHi + H JkHi(∣Si∣ + log δ-1))
i=1
=X O(H JkH' ∙∣E'∣(∣S'∣ + log δ-1))
'=1	'	)
This equation, together with equation 11 and equation 12, indicates the regret bound
K
R(K) = X %*[k] - Vπk [k]
k=1
≤ XO (HH'∣E'∣ + HJKH` ∙ ∣E'∣(∣S'∣ +logδ-1)) + 4HPHKlogδ-1
'=1	×	)
=XO (HH'∣E'∣ + HJKH` ∙ ∣E'∣(∣S'∣ +logδ-1))
'=1	'	)
15
Under review as a conference paper at ICLR 2020
holds with probability at least 1 - 2δ. This completes the proof of Theorem 4.1.
□
16