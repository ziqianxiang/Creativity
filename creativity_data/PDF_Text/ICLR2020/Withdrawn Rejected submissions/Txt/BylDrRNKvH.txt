Under review as a conference paper at ICLR 2020
Understanding Attention Mechanisms
Anonymous authors
Paper under double-blind review
Ab stract
Attention mechanisms have advanced the state of the art in several machine learning
tasks. Despite significant empirical gains, there is a lack of theoretical analyses
on understanding their effectiveness. In this paper, we address this problem by
studying the landscape of population and empirical loss functions of attention-
based neural networks. Our results show that, under mild assumptions, every local
minimum of a two-layer attention model has low prediction error, and attention
models require lower sample complexity than models not employing attention.
Additionally for the popular self-attention, our theoretical results provide several
guidelines for designing attention mechanisms. Our findings are validated with
satisfactory experimental results on MNIST and IMDB reviews dataset.
1 Introduction
Significant research in machine learning has focused on designing network architectures for superior
performance, faster convergence and better generalization. Attention mechanisms are one such design
choice that is widely used in many natural language processing and computer vision tasks. Inspired
by human cognition, attention mechanisms advocate focusing on the relevant regions of input data to
solve a desired task rather than ingesting the entire input.
Several variants of attention mechanisms have been proposed, and they have advanced the state of the
art in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017), image
captioning (Xu et al., 2015), video captioning (Pu et al., 2018), visual question answering (Lu et al.,
2016), generative modeling (Zhang et al., 2018), etc. In computer vision, spatial/ spatio-temporal
attention masks are employed to focus only on the relevant regions of images/ video frames for the
underlying downstream task (Mnih et al., 2014). In natural language tasks, where input-output pairs
are sequential data, attention mechanisms focus on the most relevant elements in the input sequence
to predict each symbol of the output sequence. Hidden state representations of a recurrent neural
network are typically used to compute these attention masks. The most popular implementation of
this paradigm is self-attention (Vaswani et al., 2017), which uses correlation among the elements of
the input sequence to learn an attention mask.
Substantial empirical evidence demonstrating the effectiveness of attention mechanisms motivates
us to study the problem from a theoretical lens. In this work, we attempt to understand the loss
landscape of neural networks employing attention. Analyzing the loss landscape and optimization of
neural networks is an open area of research, and is a challenging problem even for two-layer neural
networks (Poggio & Liao, 2017; Rister & Rubin, 2017; Soudry & Hoffer, 2018; Zhou & Feng, 2017;
Mei et al., 2018b; Soltanolkotabi et al., 2017; Ge et al., 2017; Nguyen & Hein, 2017a; Arora et al.,
2018). Convergence of gradient descent for two-layer neural networks has been studied in (Allen-Zhu
et al., 2019; Mei et al., 2018b; Du et al., 2019). Ge et al. (2017) shows that there is no bad local
minima for two-layer neural nets under a specific loss landscape design. Unfortunately, these results
cannot directly be applied to attention mechanisms, as attention modifies the network structure and
introduces additional parameters which are jointly optimized with the model. To the best of our
knowledge, our work presents the first theoretical analysis on attention-based models.
Our main result shows that, under some mild conditions, every stationary point of attention models
achieve a low prediction error. We perform an asymptotic analysis where we show that expected
prediction on error goes to 0 as n → ∞. We also show that attention models achieve lower sample
complexity than the models not employing attention. We then discuss how the result can be extended
to recurrent attention and multi layer cases, and discuss the effect of regularization. In addition, we
1
Under review as a conference paper at ICLR 2020
show how attention further helps improve the loss landscape by studying three properties: number of
linear regions, flatness of local minima and small sample size training. We validate our theoretical
results with experiments on MNIST and IMDB reviews dataset.
2 Attention Models
Attention mechanisms are modules that help neural networks focus only on the relevant regions
of input data to make predictions. To study such behavior, we analyze different types of attention
models. We start with a naive global attention model. Then we analyze the most popular self-attention
model and discuss the extension to recurrent attention in Appendix. In the naive global attention
model, we consider a dataset D = {xi, yi}iN=1, xi ∈ Rp, yi ∈ R, where the output yi depends only
on certain regions of input xi, i.e., yi = f? (a? xi), where a? is an attention mask, and f?(.) is
the ground-truth function that is used to generate the dataset and the vector a? ∈ [0, 1]p. The set of
entries {ai?|ai? 6= 0} corresponds to the relevant region of the input, while the complementary set
{ai? |ai? = 0} corresponds to the irrelevant region.
We consider a two-layer ReLU-based neural network to approximate the function f *. The network
architecture consists of a linear layer followed by rectified linear units (ReLU) as a non-linearity
and a second linear layer. Denote the weights of the first layer by w(1), the weights of the second
layer by w(2), and the ReLU function by φ(∙). Then the response function for the input X can be
written as f(x) = w(2)T φ(hw(1), xi). We call the above function “baseline model" since it does
not employ any attention. To incorporate attention, we introduce the attention mask a as additional
neural network parameters. The attention model we use can be written as:
f(x) = w(2)T φ(hw(1), x	ai)	(1)
In this paper, we focus on the regression task which minimizes the following loss function:L =
E(χ,y)〜DIlf (x) — yk2. While we present analysis on the regression task, our theory can easily be
extended to classification tasks as well.
After a thorough analysis of global attention, we analyze a more practical self-attention setup,
which comes from the transformer model proposed in Vaswani et al. (2017). The input xi
(xi1 , . . . , xip ) ∈ Rt×p, where xij are t-dimensional vectors. Each xi corresponds to independent
sentences for i
sentence x. wQ
= 1, . . . , n, and xj ’s are the fixed dimensional vector embedding of each word in
,wK ∈ Rdq ×t are the query and key matrices, and wV ∈ Rdv ×t is the value matrix.
For each input xi, the key is calculated as: Ki = (wK xi)T ∈ Rp×dq ; For zth vector in the input,
the query vector is computed as: Qiz = (wQxiz)T ∈ R1×dq for z = 1, . . . ,p. The value matrix
V = wV xi ∈ Rdv ×p. Then the self-attention w.r.t to the zth vector in the input xi is computed as:
aSelf (Z)(Xz, wQ, WK) = Softmax( Q^Ei )
dk
(2)
for z = 1, . . . ,p. And aiself = (aiself(1), . . . , aiself(p)). This self-attention vector represents the
interaction between different words in each sentence. The value vector for each word in the sentence
z	z	se z	dv
Xi can be calculated as Vi = V ai ∈ R v . This value vector is then passed to a 2-layer MLP
parameterized by w(1) ∈ Rpdv ×d and w(2) ∈ Rd×1, resulting in the following generative model:
yi = w(2)?T φ(hw(1)?, vec(wV Xiaiself)i) + i	(3)
where vec(∙) represents the vectorization of a matrix. We also discuss the extension to recurrent
attention model and multi-layer self-attention model in Appendix due to the page limit.
Note that naive global attention is not widely used in practice, because here we assume the attention
mask is globally fixed for all points, but not a function of input. In real world application, the attention
weights depend on the input, such as the self-attention framework we just introduced. Despite the
limitation of naive global attention model, it is a fundamental building block of attention models, and
needed to be analyzed first for two following reasons.
First, this fixed attention shares the core idea of attention: There is a specific intrinsic structure in
data, and this intrinsic structure requires that we should assign different weights to different input
features accordingly. And this weight assigning strategy should be learned from data. In naive
2
Under review as a conference paper at ICLR 2020
global attention, attention weights a are parameters themselves; In the self-attention model, the
attention weights a depend on a function parameterized by value/key/query matrices, and we learn
these matrices as attention parameters. And for both global and self-attention, we jointly learn the
network weights(w(1),w(2)) and attention parameters(a in global attention, value/key/query matrices
in self-attention) at the same time. Therefore this naive global attention is a good starting point for
analyzing attention mechanisms.
Second, we can gain helpful insights on attention by analyzing the global attention case. The number
of non-zero elements of a in global attention represents both the size of attention parameters and
the sparsity level of attention. In the standard self-attention model, size of attention parameters
is determined by the size of value/key/query matrices. And the sparsity level is how many words
we allow one word to attend to. By studying the effect of this quantity, we can have a better
understanding of how the sparsity and parameter size of attention affect the model performance and
sample complexity. The detailed discussions can be found in Section 3.
3 Loss landscape analyses on attention models
In Section 3.1, we analyze the loss landscape for the the naive global attention model, which is
defined as
1n
α) min E X(W⑵T如W(I), Xi © ai - yi)2
w(1) ,w(2) ,a∈S 2n i=1
(4)
where W(I) ∈ Rp*d, w(2) ∈ Rd, a ∈ Rp, and S is the parameter space of a. Section 3.2 extends
the loss landscape analysis to the self-attention model with a = f (x). Section 3.3 discusses the
extension to recurrent attention model. Section 3.4 discusses the effect of regularization and how the
result can be extended to multi-layer networks.
To approximate the non-differentiable ReLU φ, we use the softplus activation function φτ (x), i.e.,
φτ(x) = 1log(1 + eτx)
τ
Note φτ converges to ReLU as τ → ∞ (Glorot et al., 2011). Theoretical results with φτ (x) will hold
for any arbitrarily large τ. For ease of notation, we still use φ(x) to denote the softplus function.
3.1	Asymptotic property of naive global attention model
We first analyze the asymptotic prediction error of local minimum with large sample size. Let the
covariance matrix of φ(W(1), x © a) be (Σφ)ij = cov(φi(hW(1), x © ai), φj (hW(1), x © ai)) and
h(x∙k) = x.k(a，k(w(2) © φ0(hw(1)T, x © a〉)，where x∙k represents kth feature in x, and a+ is the
attention mask for x∙k. φ0 (hw(1)T, X © a〉is the first order derivative with respect to the value in φ(∙)
and it belongs to Rd. Let u = (W(2)φ(hW(1), x © ai) - E(y|x)). Before proceeding, we introduce
several necessary assumptions:
(A1) xi are i.i.d with kxi k∞ < Cx for i = 1, 2, ..., n.
(A2) There exist C1, C2 such that kW(1) kF < C1 and kW(2) k2 < C2 for any W(1), W(2) in S.
(A3) The output y can be specified by the two-layer neural network up to an independent sub-
Gaussian error with variance σ2, i.e., there exists a set of parameter (a?, W(1)?, W(2)?), such that
yi = W⑵?Tφ({w⑴?, Xi © a?i) + Ci, where Ci 〜subG(0, C32) for i=1,2,...n, with Xi ⊥ ei. And
kE(φ(hW(1)?, xi © a?〉))k2 ≤ C4
(A4) kak0 ≤ s0 such that s0 ≤ p, which represents the sparsity of the attention model, and
0 ≤ ai ≤ 1 for any i = 1,…，p.
(A5) λmin(Σφ) ≥ Cφ, and when the estimation of φ is inaccurate, i.e, E(kφ(hW(1), X © a〉 -
φ?(hw⑴,x © a>)∣∣2) ≥ O(γ), then there exists a feature χ∙k and for the tth element of ht(χ∙k), it
satisfies sd(ht(x∙k)) = O(1) and cor(u, ht(x∙k)) = O(1) for W⑵ in the form of Σ-1r + o(γ).
(A6) E(W⑵φ(<W⑴,x © a〉)) 一 E(y) = o(γ∕√s0).
3
Under review as a conference paper at ICLR 2020
(A7) The sum of the weights kak1 = 1.
These assumptions are mild when the dimension of x is large. The justification of these assumptions
are provided in the appendices. Given these assumptions, we show in Theorem 1 that the sample
complexity required to converge to a good minimum is reduced with attention mechanism.
Theorem 1. Under (A1) to (A6), for any γ > 0, suppose
s20C12Cx2η2	s0η
n & 0 12	log(--)(pd + P + d)
γ2	γ
where η = C1C2Cχ. Then with probability tending to 1, any stationary point (a, W(1), W(2)) ofthe
objective function (4) satisfies the following prediction error bound:
E(W(2)Tφ(hW⑴，X Θ ai) — E(y∣x))2 . Y2
Remark: The sample complexity bound of Theorem 3 provides helpful insight for understanding
attention mechanisms. With the sparsity structure of attention mask a, attention mechanisms constrain
the parameters in a smaller space, thus reducing the variance and the covering number. This
leads to lower sample complexity compared to the baseline model not employing attention. It is
straightforward to calculate the sample complexity bound for the baseline model. To achieve the
same error bound, we substitute s0 with p in the bound, and this results in a much larger value. When
s0 is fixed, we can see up to a log term, prediction error γ is proportion to n-1/2, which is the optimal
rate of convergence in regression. This shows that the bound is tight in this aspect.
Next, we extend Theorem 1 to the attention model with an additional sum-to-one constraint (A7).
The discussion of the following Corollary 1 is provided in the appendix.
Corollary 1. Under (A1) to (A7), for any γ > 0, suppose n & CICjn log(η)(pd + P + d), where
η = C1C2Cχ. Then with probability tending to 1, anystationary point (a, W(1), W(2)) ofthe objective
function (4) satisfies thefollowingprediction error bound:E(W(2)Tφ(<W⑴，X Θ ai) — E(y∣x))2 .
γ2.
3.2	Asymptotic property of self-attention model
In this section, we extend our previous analysis to the self-attention model. In self-attention, the
attention mask is no more fixed globally, but instead a function of the input. We begin by analyzing a
self-attention model with a known attention function f (∙),in which the weight a is not optimized
together with W. Similar bound can be derived for the following model:
1n
,1min,	5-∑S(w⑵T0(〈W⑴，Xi θ f (x)i) - yi)2	⑸
w(1) ,w(2) ∈S 2n
i=1
Proposition 1. Under (A1) to (A6), suppose that a = f(X). For any γ > 0, given the sample
size n & SOCYx” log(干)(pd + d), where η = C1C2Cχ, with probability converging to 1, any
stationary point (W(1), W(2)) ofthe objectivefunction equation 5 satisfies that: E(W(2)T φ(<W(1), xΘ
a〉)- E(y∣x))2 . γ2.
Proposition 1 implies that if the self-attention mask can be precisely computed, global attention
results can be extended to self-attention ones. However, the function f (∙) is not necessary known,
and needs to be learnt in real world applications. Therefore the self-attention setup as we introduced
in section 2 is more desired in real-world setting. Denoting W = (W(1), W(2), WQ, WK, WV ), the
two-layer self-attention model can be estimated by:
1n
min— V"(W(2)Tφ(hW(1),vec(WvXiai ʃ)i) — yi)2	(6)
w 2n
i=1
We now introduce necessary assumptions for analyzing self-attention model.
(A8) There exist C5,C6 and C7 such that kWQkF ≤ C5, kWK kF ≤ C6, kWv kF ≤ C7.
4
Under review as a conference paper at ICLR 2020
(A9) The output y can be predicted by the two-layer network (3) with an independent sub-Gaussian
error with variance σ2, i.e, there exists a set of parameters (a? , w(1)? , w(2)?) such that yi =
W⑵?Tφ(hw(I)?, vec(wVXiaself(Z))i) + βi, where aself is calculated by (2);1〜subG(0, C42) for
i = 1, 2, ...n, with xi ⊥ i.
(A10) We assume (A5) and (A6) holds, substituting Xi Θ a with Vec(WVXiaself), and ht(x∙k)=
(Vec(WVXiaseef)),k(w⑵T)θ φ0(w⑴，Xi θ a)), where (Vec(WVXiaseef))/ j§ m k-th element
of the value matrices.
The assumption (A9) states that self-attention model can correctly predict the conditional mean
E(yi|Xi). Note that (A9) encompasses a more expressive class of models than (A3), which includes
the models used in practice such as the transformers. (A10) is parallel to (A5) and (A6). Under these
assumptions, we can obtain its sample complexity as given by following theorem:
Theorem 2. Under (A1), (A2), and (A8) to (A10), for any γ > 0, given the sample size:
n & η2C2Cx log( C5C6C7n )(pdvd + d + 2dqt + dvt)
γ2	γ
where η = C1C2Cχ, with probability tending to 1, any stationary point (W(1), W(2), WQ, WK) of
the objectivefunction (6) satisfies that: E(W(2)Tφ(<W(I), vec(WVXiaseef )〉)一 E(y∣X))2 . γ2
Remark: Theorem 2 shows that with the help of self-attention, we can achieve consistent prediction
under more expressive class of models (assumption (A9)) which considers the interactions between
vectors in data. It is worth pointing out that both global attention model and baseline model do not
have consistency for the class of models beyond the ones stated in (A3). In other words, consistent
prediction on the data distribution generated from equation 3 using baseline and global attention
models requires introducing larger parameter space, for example, using more layers of network
or more units in each layer. Self-attention model, on the other hand, achieves the more accurate
estimation by constraining the parameter space and input space. And parallel to the sparsity level s0
in Theorem 1, a proper choice of value/query/key matrices can help reduce sample complexity. If a
sparse attention(i.e. one word should attend to all words, but only some relevant words), the sample
complexity can also be further reduced similar with Theorem 1.
3.3	Extension to recurrent attention network
Sample complexity analysis can be extended to recurrent neural networks. This is included in the
appendix. The key messages from our analysis include: (1) A good design of recurrent framework
can help the network converge to a good stationary point with small sample complexity, and (2) An
arbitrarily complex framework increases the sample complexity. Since in real world, the optimal
recurrent framework is unknown, careful design choice has to be made for obtaining good sample
complexity.
3.4	Discussion: Regularization and beyond 2 layers
So far our analyses provide some theoretical justification on how attention mechanisms help learn
superior models. Furthermore, our analysis also suggests proper regularization is helpful in training
an attention model. An `1 regularization on attention weights and `2 regularization on network
weights are effective in reducing the sample complexity. We also find that imposing constraints and
regularization on network weights can help remove sharp minima, and keep flat minima with good
generalization. Detailed discussions on regularization are provided in Appendix Section D.1.
Also, Theorem 1 and 2 can be extended to multi-layer attention network, under the assumptions
parallel to (A8), (A9) and (A10): There exists a correct multi-layer self-attention network can specify
the model, and the bias and gradient with respect to network weights are not uncorrelated. Under
these assumptions, we provide sample complexity bound for multi-layer self-attention models. And
all the discussions and insights are applied to multi-layer models. Explicit assumptions, discussions
and theorems are provided in Appendix Section D.2. We avoid multi-layer setting in main context
because it leads to over-complicated derivations and assumption justifications, and will distract
readers from main idea of the paper. We believe two-layer models are representative enough to
provide theoretical evidence on why attention reduces sample complexity.
5
Under review as a conference paper at ICLR 2020
4 On improving the landscape structure
In this section, we further investigate three additional properties on how attention mechanisms
improve the landscape of neural networks. First, we show that in global attention model, attention
mechanisms reduce unnecessary number of linear regions and maintain a low approximation error;
Second, we show that flatness properties of minima are retained when attention mechanisms are used
for both global attention and self-attention. Furthermore, our analysis indicates that for attention
models, smaller sample size suffices to converge to good minima which generalize well in prediction.
Finally, we show that the perfect in-sample prediction on small sample size is also achieved in
attention networks for both global attention and self-attention.
4.1	On the number of linear regions
We first study how attention mechanisms affect the number of linear regions (Montufar et al., 2014)
in a wide two-layer network, where the number of units in the hidden layer is larger than the sparsity
of the attention mask matrix.
Theorem 3. Assume kak0 = s0, which is the sparsity of the mask matrix, and the number of units in
the hidden layer n1 > s0. Then the maximal number of linear regions of the function by a two-layer
fully connected neural network with ReLU activation function, is lower bounded by b nn1 C s0.
Remark: The theorem implies that when appro-
priate attention mechanism is used, the number
of linear regions reduces leading to a simpler
landscape, yet the approximation error remains
small. This leads to lower sample complexity
for achieving a desired prediction error. More
detailed discussion can be found in the appen-
dices. The result of Theorem 3 also applies to
the self-attention with different attention spar-
sity(i.e. allowing how many words we allow one
word to attend to).
4.2	On flatness/sharpness of minima
Figure 1: Number of linear regions in log scale v.s.
sparsity
Many recent works, such as Keskar et al. (2016),
argue that flatter local minima tend to generalize
well. However, in a recent study, Dinh et al.
(2017) observes that by scale transformation, the minima which are observationally equivalent,
can be arbitrarily sharp, and the operator norm of a Hessian matrix can also be arbitrarily large.
We will show that this fact also holds for the global attention mechanism, if no constraint on
parameter (a, W(1), W(2)) is imposed. Here We introduce the definition of e-flatness as in HoChreiter
& Schmidhuber (1997).
Definition 1. Given e > 0, a minimum θ, and loss L, C(L, θ, e) is the largest connected set
containing θ such that ∀θ0 ∈C(L,θ,e),L(θ0) ≤ L(θ) + e, and its volume is called the e-flatness.
In the folloWing Theorem, We analyze the flatness of stationary point for both naive global and
self-attention model.
Theorem 4. (a) Consider the two-layer ReLU neural network with naive global attention in Section
3.1:
yi = W(2)?T φ(hW(1)?, xi	a?i)
and a minimum θ = (a, W(1), W(2)) satisfying that a = 0, W(1) = 0, W(2) = 0. For any e > 0,
C(L, θ, e) has an infinite volume, and for any M > 0, we can find a stationary point such that the
largest eigenvalue of V2L(θ) is larger than M;
(b) Consider the two-layer ReLU neural network with self-attention mechanism as stated in
Section 3.2:
yi = W ⑵?T φ(<W(1)?, vec(WV Xi a；"’)》)
6
Under review as a conference paper at ICLR 2020
and a minimum θ = (W⑴，W⑵，wv, wQ, wκ) satisfying that Wi = 0 for i =⑴，(2), V, Q, K.
For any > 0, C(L, θ, ) has an infinite volume, and for any M > 0, we can find a stationary point
such that the largest eigenvalue of V2L(θ) is larger than M.
Theorem 4 indicates that property on flatness of minima is maintained when attention mechanism is
applied, and there exist good sharp minima, coinciding with the observation in Dinh et al. (2017).
However, there is no guarantee that all sharp minima are good in generalization. Revisiting our
analysis in Section 3, the restriction on the parameter space (see (A2)) help remove these sharp
minima. Specifically, (A2) provides an upper bound on the magnitude of (a, W(1), W(2)) and (A5)
bounds the magnitude of φ(hW(1), xi ai) from below. These constraints control the parameter
space and remove all sharp minima generated in Theorem 4 in which α1 or α2 goes to infinity. The
`2 bounds in (A2) can be achieved through a proper `2 regularization (See Section 3.4).
4.3 On small sample size
We conclude by studying the local minima of wide neural networks in small sample regime. (Nguyen
& Hein, 2017b) proved that a two-layer neural network model can always achieve perfect empirical
estimation error when the same size is small. Here, we extend this result for global and self-attention
model. The discussion of Theorem 5 is deferred to the appendices.
Theorem 5. (a) For naive global attention model, if rαnk(φ((W⑴，Xi Θ ai)i=1,2,..n) = n. Then
^very StatiOnarypoint (a, W(1), W(2)) ofthe objective function (4), is a global minimum;
(b) For self attention model, if rank(φ((W⑴,vec(WVXiaself )i)i=1,2,..n) = n. Then every
Stationary point (W(1), W(2), WV, WQ, WK) ofobjectfunction (6)isa global minimum.
5 Experiments
5.1	Sample complexity
5.1.1	Global Attention model
Theorem 1 proves that attention models require a lower sample complexity than baseline models,
i.e., attention models require fewer samples to achieve the same test error as baseline models. This
result is validated empirically in this experiment. To mimic the assumptions of Theorem 1, we
consider ground truth two-layer neural network G*(x) is formed using random weights as inputs.
The network G* maps the input vector X ∈ R256 to 10-dimensional output. The input vector X is
randomly sampled such that each element is drawn i.i.d from N(0,1). An attention mask a* is then
constructed with k randomly chosen elements as 1 and the rest as 0. The ground-truth labels are
generated from y = G*(X Θ a*).
Table 1: Experiments on sample complexity for global attention model. np denotes the number of
trainable parameters in each model. All results are averaged over 5 runs
Number of training samples	Test loss: baseline (np = 34186)	Test loss: attention (n = 34442)	Test loss: regularized attention (n = 34442)
10000	2.1063	0.5484	0.0143
14000	0.4109	0.0382	0.0107
16000	0.1811	0.0211	0.0100
18000	0.1072	0.0163	0.0122
20000	0.0769	0.0101	0.0098
50000	0.0511		0.0060			0.0072	
To test the sample complexity, we generate multiple datasets, each containing 10k, 14k, 16k, 18k,
20k and 50k unique samples respectively using the scheme mentioned in the previous section. A
common test set of 5000 samples is created to evaluate each of the models. A regression model
is then trained on each of these datasets. All models are trained with SGD optimizer with a fixed
learning rate of 10-3. Table 1 reports test errors for baseline and attention models at 400k iterations
7
Under review as a conference paper at ICLR 2020
as the number of training samples vary. We observe that attention models need fewer training samples
than baseline models to achieve a desired error. For instance, to attain the desired error 0.07, attention
models need 14000 samples, whereas baseline models need 20000 samples. We would like to point
out that improvements obtained by attention models is not because of increase in model parameters.
As shown in Table 1, the number of parameters in baseline and attention models are comparable.
Hence, the performance gain is solely due to the attention mechanism.
Regularization: In section 3.4, we discuss how regularizing attention vector helps obtain a better
attention model. To empirically validate this claim, we train a model with L1 regularization on the
attention vector. Same experimental setting as Section 5.1 is used, a regression model is trained
using a two-layer neural network. We add the L1 penalty on the attention vector to the objective:
Lreg = Pi |ai|. The results are also shown in Table 1. We observe that models trained with L1
regularization achieves better sample complexity than its unregularized counterpart. We also observe
the faster convergence when the models were regularized, as shown in the appendices.
5.1.2	Self-attention model
We extend the sample complexity experiments to self-attention model discussed in Section 3.2.
Since our model is tailored towards natural language tasks, we consider the problem of sentiment
classification on IMDB reviews dataset (Maas et al., 2011). Note that our analysis needs fixed length
sentences which hardly holds true in any NLP dataset. So, we zero-pad all our sentences to make
their length equal the maximum sentence length in the dataset (2142 for IMDB reviews). For every
input word, we first obtain their corresponding pre-trained GloVE embeddings (∈ R100) which is
then passed to the neural network. As a baseline model, we flatten the input to one large vector of
dimension 2142 × 100 and pass it to a 1-hidden layer MLP with 256 hidden units. For self-attention
model, we use wQ ∈ R100×100, wK ∈ R100×100, wV ∈ R100×2142. Once the attended features
are computed per equation 2, it is passed to a 1-hidden layer MLP with 256 hidden units as the
baseline model. All models were trained using Adam optimizer with learning rate 10-3. This was
the setting that gave the best performance among optimizer and learning rate configurations we tried.
A comparison of sample complexity of baseline model and the self-attention model is provided in
Table 2. We clearly observe that self-attention model requires low sample complexity to achieve the
same error as the baseline model. To test if improvements are obtained in attention model due to
increase in model parameters, we ran the baseline model with twice the number of parameters as the
self-attention model. Even with a large parameter size, baseline model performs poorly compared to
self-attention models.
Table 2: Experiments on sample complexity for self attention model. np denotes the number of
parameters used in each model. All results are averaged over 5 runs.
Number of training samples	Baseline (np = 57335913)	Test accuracy (in %) Baseline (np = 111744625)	Self-attention (np = 57365913)
875	63.38	64.38	70.52
1750	64.48	65.21	83.10
3500	63.51	64.32	86.14
5250	71.92	69.51	87.96
7000	75.49	76.32	87.10
8750	78.64	78.66	88.58
13125	79.97	79.72	88.98
17500	80.85	80.52	88.59
5.2 Convergence plot
This experiment studies the convergence of the empirical risk of the baseline and attention models.
A modified MNIST dataset called NoisyMNIST is constructed where the images of digits from the
MNIST dataset is embedded in noise as shown in Panel (a) of Figure 2. We consider the classification
task to predict the labels of the digit in each image. Since the ground truth label depends only on
certain regions of input, NoisyMNIST mimics the data generating process we consider in this paper.
8
Under review as a conference paper at ICLR 2020
Figure 2: Visualization of NoisyMNIST in (a) and convergence plots for baseline and attention
models on NoisyMNIST, including (b) plot of test loss and (c) plot of test accuracy over iterations.
For the baseline model, we train a two-layer neural network with 128 hidden units is using stochastic
gradient descent. For attention models, the input tensor is multiplied element-wise with a learned
attention mask a as in equation 1. The attended input is then passed to the two-layer network. The
convergence plots for baseline and attention models are plotted in Figure 2. We observe that the
attention model converges faster, and to a better minimum than the baseline model. Similar behavior
is observed for different learning rate and scale configurations as shown in the appendices.
5.3 Hessian plot
We study the Hessian matrix of the
loss surface to validate the loss land-
scape of attention models. The same
classification setup as the previous ex-
periment is considered. The follow-
ing two-layer neural network architec-
ture was employed: 576 → 16 → 10.
The Hessian matrix of loss landscape
about the computed minimum was, re-
spectively, computed for the baseline
and attention models, and their top k
sorted eigenvalues are plotted in Fig-
ure 3. Baseline models exhibit higher
eigenvalues than attention models, so
the loss landscape of attention mod-
els are flatter than the baseline mod-
els. Since flat landscapes lead to better
generalization, models with attention
Figure 3: Top 100 eigenvalues of the Hessian matrix for
baseline and attention models.
generalize better than models without attention as shown in Section 5.2.
6	Conclusions
In this paper, we study the loss landscape of two-layer neural networks on global and self attention
models, and show that attention mechanisms help reduce the sample complexity and achieve consistent
predictions in the large sample regime. Additionally, by analyzing the number of linear regions, the
loss landscape under small sample regime, and flatness of local minima, we demonstrate that attention
mechanisms produce a well behaved loss landscape that leads to a good minima. Extensive empirical
studies on NoisyMNIST dataset and IMDB reviews dataset validate our theoretical findings.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning via
Over-Parameterization. In Proceedings of the 36th International Conference on Machine Learning,
ICML’19, 2019.
9
Under review as a conference paper at ICLR 2020
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmdssan, Stockholm, Sweden, pp. 244-253, 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014.
Andrew R Barron and Jason M Klusowski. Approximation and estimation for high-dimensional deep
learning networks. arXiv preprint arXiv:1809.03090, 2018.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1019-1028, 2017.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Proceedings of the 36th International Conference on Machine
Learning, ICML ’19, 2019.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. CoRR, abs/1711.00501, 2017.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp.
315-323, 2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for
visual question answering. In NIPS, pp. 289-297, 2016.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 1412-1421. Association for Computational Linguistics, 2015.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Song Mei, Yu Bai, Andrea Montanari, et al. The landscape of empirical risk for nonconvex losses.
The Annals of Statistics, 46(6A):2747-2774, 2018a.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018b. ISSN 0027-8424. doi: 10.1073/pnas.1806579115.
Volodymyr Mnih, Nicolas Heess, Alex Graves, and koray kavukcuoglu. Recurrent models of visual
attention. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 27, pp. 2204-2212. Curran Associates, Inc.,
2014.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in neural information processing systems, pp.
2924-2932, 2014.
10
Under review as a conference paper at ICLR 2020
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings
of the 34th International Conference on Machine Learning (ICML 2017), Sydney, NSW, Australia,
6-11 August 2017,pp. 2603-2612, 2017a.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint
arXiv:1704.08045, 2017b.
Tomaso A. Poggio and Qianli Liao. Theory II: landscape of the empirical risk in deep learning.
CoRR, abs/1703.09833, 2017.
Yunchen Pu, Martin Renqiang Min, Zhe Gan, and Lawrence Carin. Adaptive feature abstraction
for translating video to text. In Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 7284-7291, 2018.
Blaine Rister and Daniel L. Rubin. Piecewise convexity of artificial neural networks. Neural Networks,
94:34-45, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. CoRR, abs/1707.04926, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer
neural networks, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, pp. 5998-6008, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference
on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2048-2057,
Lille, France, 07-09 Jul 2015. PMLR.
Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. CoRR, abs/1805.08318, 2018.
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Simple baseline
for visual question answering. arXiv preprint arXiv:1512.02167, 2015.
Pan Zhou and Jiashi Feng. The landscape of deep learning algorithms. CoRR, abs/1705.07038, 2017.
11
Under review as a conference paper at ICLR 2020
Appendices: Understanding Attention Mechanisms
In appendices, Section A presents the extensions of our analyses to recurrent attention model; Section
B provides detailed justification of our assumptions; Section C discusses the implications of theorem
results in more detail(Theorem 1, Corollary 1, Theorem 3 and Theorem 5 in order); Section D
discusses the regularization effect in attention networks and potential extensions beyond 2 layers
and to CNN/RNN. Finally We provide proofs and additional experiment results in section E and F
separately.
A Extension to recurrent attention model
Here we consider analyzing the representative recurrent attention framework in Bahdanau et al.
(2014). In the recurrent attention network, we still have each data point xi = (xi1 , . . . , xip) ∈
Rt×p, corresponding to p words with t-dimensional embedding. Then the generative model can be
represented as:
t
yi = w(2)T hw(1), Xa(xi)jxiji + i
j=1
Analogous to NLP setting, a(xi) is a unknown function mapping xi to a t-dimensional vector, where
a(xi)j represents the effect of the jth word in the sentence for point i. To simplify the model, we
use data features themselves as their annotation, then for time stamp k = 1, . . . , T, The recurrent
attention model estimates a(xi) as follows:
sk = f (sk-1 , ck-1); ekj = score(sk-1 , xij)
t
αkj = St―~	; Ck = ^X αkjXj
j=1 ekj	j=1
yk = w(2)T φ(hw(1), cki)
where score(∙) is the scoring function representing how well the inputs around position j and the
output at position i match. It can be dot product or MLP. And f (∙) is the function to update Sk.
Suppose the parameter set inside these two functions are wa and wf with number of parameters as
da and df accordingly. Here we show that when these two functions are expressive enough, recurrent
attention network will also have sample complexity bound parallel to previous sections. Here we
introduce necessary assumptions.
(A11) The output y can be predicted by the two-layer network with an independent sub-Gaussian
error with variance σ2, i.e, there exists a set of parameters (w(1)?, w(2)?) such that yi =
W⑵?Tφ(hw⑴？, pj=ι a(xi)jXji) + Ei, where Ei 〜subG(0, C2) for i = 1, 2,…n, with
xi ⊥ i.
(A12) Suppose (A5) holds when we substitute X Θ a with Pj=ι a(xi)jXj and ht(x∙k) =
a(xik)xijk(w(2)T)	φ0 (hw(1), xi ai)).
(A13) We assume kwak2 ≤ C8 and kwf k2 ≤ C9.
(A11) and (A13) are parallel to (A1) to (A5) in global attention case. They can be justified similar as
them, which is discussed in Section B of appendices.
Now we can provide following sample complexity bound extended from previous sections.
Theorem 6. Under (A1),(A2),(A11) to (A13), there exists a sufficient large T, for any γ > 0, suppose
n & CCCC* log(nC8C9)(td + d + dw + df)
γ2	γ
where η= C1C2Cx, such that if there exist stationary point(s), then with probability tending to 1,
any StatiOnary point (W(1), W(2), W f, Wa) satisfies the following prediction error bound:
t
E(W⑵φ((W⑴,X a(χi)jXji) - E(y|X))2 . γ2
j=1
12
Under review as a conference paper at ICLR 2020
Remark: This theorem provide a sample complexity bound for recurrent attention network. It holds
when such ’good stationary point’ exists. It also shows a trade-off between a complicated recurrent
attention network and the sample complexity bound. If f (∙) and a(∙) are properly selected, they will
be sufficient expressive to obtain good stationary points, and also the number of parameters dw and
df will not be too large. In this way. an ideal sample complexity bound to these good stationary
points can be achieved as theorem says. However, with a over complicated design in these functions,
the sample complexity bound will be large; With a over simple design, such good stationary points
don’t exist. It is parallel to a trade-off between approximation error and estimation error in learning
theory. The theory implies a good design of recurrent structure will help achieve an optimal sample
complexity in recurrent attention model.
B	Justification of Assumptions (A1)-(A6)
In this section, we discuss the rationality of Assumptions (A1) to (A6). Note that (A1) to (A6) are
required to prove the main result in Theorem 3 and they have also been studied by Keskar et al.
(2016); Dinh et al. (2017); Mei et al. (2018a;b); Nguyen & Hein (2017b). in what follows, we will
show in details that (A1)-(A6) are all reasonable assumptions.
First of all, (A1) and (A2) require upper bounds on the input xi and `2 bound for network weights. it
is a standard assumption in landscape analysis (Mei et al., 2018a;b), and also it is crucial to remove
sharp minima which may not generalize well (Keskar et al., 2016; Dinh et al., 2017).(see remark
after Theorem 4). These assumptions can be achieved through regularization.
(A3) requires that this two-layer network are rich enough to specify the condition mean E(yi|xi). it
has been studied that general bounded functions with a Fourier representation on [-1, 1] can be well
approximated by the defined two-layer network(Barron & Klusowski, 2018).
(A4) requires a sparse structure on a? ; otherwise, the model would be equivalent to the baseline
model, simply just choose attention masks all being 1. And kak∞ ≤ 1 requires the attention weight
ranges from 0 to 1.
(A5) is a technical condition for our analysis of stationary point. (A5) includes two parts. Both
of them hold naturally when dimensionality is large. Firstly, the lower eigenvalue bound assumes
φ(hw(1), xi ai) is not degenerated. if eigenvalue assumption is violated, the model is equivalent to
a network with fewer number of units in hidden layers, and we can study this equivalent degenerated
one instead; This assumption also guarantees us to remove sharp minima(Keskar et al., 2016; Dinh
et al., 2017), same as (A1) and (A2). secondly, (A5) assumes when φ(hw(1), xi ai) is not well
estimated, there exists an ’active feature’. The correlation between this feature and bias cannot be
cancelled by the direction of a specific linear combination of φ(hw(1) , xi ai). intuitively, this
assumption says that the correlation between ht(χ∙k) and E(y∣χ∙k) cannot be fully explained by
a fixed linear combination if there is some systematic bias in φ(hw(1) , xi ai). since there is
systematic bias, it is reasonable to assume this systematic bias cannot be uncorrelated with all the
directions spanned by ht(x). This correlation assumption between ht(x.j) and U is parallel to the
full column rank condition in (Nguyen & Hein, 2017b). Considering all active terms in ht(x∙k), they
span a larger space comparing to φ(hw(1), xi ai), considering d is a fixed dimension. Thus it is
a natural assumption if we have reasonable large dimensionality and the model doesn’t degenerate.
What’s more, with required large sample size, we can also straight forwardly evaluate this assumption
by checking empirical correlation, and avoid this type of bad minima through random initialization
and proper gradient descent type algorithm(Allen-Zhu et al., 2019). And it also will not affect the key
structure of our proof.
Specifically, for the sd(ht(x∙k)) = O(1) part, since ht(x∙k) = x∙ka%(w(2 ∙ φ0(hw⑴，Xi Θ a))t),
where φ0 (hw(1), xi ai)t are O(1) with positive probability otherwise the network always predicts
zero, therefore we have sd(ht(x∙k)) = O(1) as long as sd(x∙k) = O(1) and ak = O(1). For the
correlation assumption cor(u, ht(x∙k)) = O(1), if this assumption doesn,thold, kcor(u, h(xQ)∣∣2 =
o(1) for all k = 1, . . . ,p. By linear combination, for any vector z = (z1, . . . , zk) with bounded
`2 norm, we have cor(u, zT (x1 Θ a)((wt(2) Θ φ0 (hw(1), x Θ ai) = o(γ) for all t = 1, . . . , d. We
know for any W⑴ ∈ Rp×d, the term W(1)(x Θ a)((w(2) Θ φ((w(1), X Θ a))can be represented
as the combination of term zT (x1 Θ a)((wt(2) Θ φ0 (hw(1), x Θ ai). By the arbitrary choice of
13
Under review as a conference paper at ICLR 2020
our z, it means that all the directions of z are almost not correlated with u. When p and s0 are
high-dimensional, W⑴(X Θ a)((w(2) Θ φ,(hw⑴,X Θ a))spans all the directions asP → ∞, thus
there must be some direction is correlated with u. Therefore it is reasonable to assume a O(1)
correlation occurs.
(A6) assumes that we fit a model with expectation close to truth. We can always achieve this by
centralization.
With these assumptions, we just make sure we remove unnecessary minima, such that we can
concentrate on analyzing the behaviour of good stationary points of our interest. Assumptions have
no influence on the main idea of our following theorem: showing the required sample complexity to
approach a good minimum is reduced with attention mechanism.
Remark: The assumptions are also related to the question that when attention mechanism should
be applied in network. (Zhou et al., 2015) shows that attention sometimes can be badly used in
certain cases, which shares the similar philosophy with our analyses. For example, our theoretical
analyses depend on the assumption (A3), that is, the model can be correctly specified when attention
mechanism applies. This can be violated when all the variables are useful and they all need to be
included in the model. In this case, the model with attention will be inconsistent. It indicates that in
this case when assumptions are violated, neural networks can achieve the precise estimation only
through the over-parameterization of w(1).
C Discussions on Theorem results
C.1 Theorem 1: Comparison of sample complexity with baseline model
When we compare sample complexity of attention model versus baseline model, we say the key
difference will be that s0 is substituted with p, which can be a larger diverging dimensionality.
Constants C1 and C2 can be regarded remain same when we compare attention model with the
baseline for following reasons: (1) We assume same generative model, and the network size is the
same, thus the optimal weight is known to be same. To make sure trained network weight is on the
same scale with the optimal one, it is fair to keep there `2 bound constant same. (2) In this framework,
we study the effect when p diverges to ∞ as n → ∞. In this aspect, we don’t expect the weight
norm also diverges, since a diverging weight leads to overfitting. By imposing `2 regularizations on
weights, we can always control the upper bound of `2 norm. Therefore it is reasonable to assume its
norm is bounded by a sufficient large constant. By imposing `2 regularization on weights, we can
always control the upper bound of `2 norm by this sufficient large constant. (3) Even with overfitting,
C2 in baselines are expected to be even larger due to the overfitting effect(Ch.7.Goodfellow et al.
(2016). It will further explain why sample complexity of baseline is even larger comparing with
fixing C1 and C2. In the experiment result, we also observe that the `2 norm of weights from baseline
is larger or equal to the attention network.
C.2 Corollary 1: Sum-to-one global attention
Given the assumption (A7), we do not need s0 on the sample complexity bound for the sum-to-one
global attention model. However, if we rescale X properly, the result will be parallel to Theorem 1
result.
C.3 Theorem 3: Number of linear regions
In a sufficiently wide network, bn1 C Cs is much smaller than [nn1 Cp. Then, (n1 )s0 ≤ (n1 )p holds as
long as nι ≥ exp(Plog仁产 s0). Given PlogP-。。^s0
s is relatively small, the result still holds when n1 is larger than the order of P.
≤ p⅛ log P,since 昌 is close to P when
For illustration, the bounds are plotted in Figure 1 in paper. The red line is for baseline model with
P = 100, and others are attention model with different sparsity level s0 . In general we can see the
bound for attention model is smaller than that for baseline model. The implication is that, when
proper attention mechanism is applied, the approximation error remains small, and we can use a
14
Under review as a conference paper at ICLR 2020
simpler landscape structure with less number of linear regions to reduce the estimation error. This is
the why we can achieve specific prediction error rate with a smaller sample complexity.
C.4 Theorem 5: Small sample size result
In assumptions, rank(φ((W(1), Xi Θ a)')i=1,2,..n) = n is a mild assumption in a wide network with
over-parameterization. We can see that as long as we choose the number of units d to be larger than
n, the linear dependence of hw(1), xi Θ aii=1,2,..n holds with measure zero. In other words, almost
surely this matrix has full column rank n. Thus after the nonlinear activation, The full column rank
still holds almost surely. This assumption is similar to the condition in Theorem 3.8 of Nguyen &
Hein (2017b), where the number of units in some layer is larger than the sample size. When the
sample size is smaller than the number of units in the network, this theorem holds for the network
without attention. It has been proved by Nguyen & Hein (2017b) and Soudry & Carmon (2016) under
different conditions.
D Regularization effect and Multi-layer extensions
Our analyses provide some theoretical justification on how attention mechanisms help learn superior
models. The benefit mainly comes from that the attention weight a shrinks the whole parameter
space, while this space is still large enough to capture all the necessary information. Thus the gradient
and Hessian are more controllable in this space, and the landscape of loss function behaves better
compared to the baseline model. It will be shown again in Theorem 4 in the following section. This
fact holds for both global and self-attention model. Our theory further validates that as long as the
attention masks are learnt well, the performance is expected to improve. And this effect can be more
significant when the weight is sparse. We also find that imposing constraints and regularization on
network weights can also help remove sharp minima, and keep flat minima with good prediction.
These discussions can be found in D.1.
The main idea of Theorem 1 and 2 can also be extended to multi-layer networks under the assumption
that bias and gradients of weights are not uncorrelated. They can be found in D.2.
D. 1 Regularization
Our analyses indicate that it is worth considering approaches to control the bounds for the network
weight matrices and pursue more accurate estimation of attention weighted input space. Motivated by
this message, we suggest two possible regularization methods, which help to improve currently used
attention mechanisms.
•	`2 Regularization on w
It is known that in optimization, `2 regularization onw is equivalent to specific `2 bound
for w. For our two-layer model, if we choose regularization level properly, the `2 bound for
w(1), w(2)( also wQ,wK and wV in self-attention model) will match our assumption. In
practice, a proper regularization should be as large as possible, as long as a nice in-sample
prediction is still achieved. This is important for attention mechanisms to keep prediction
power while improving the landscape behavior of loss function.
•	`1 Regularization on a to achieve sparse attention weight
As we discussed, `1 regularization can help achieve sparsity and more precise estimation.
For our 2-layer model, imposing `1 regularization will not affect the analysis since we didn’t
use the gradient w.r.t a in the analysis. It means the same theoretical guarantee holds for
the regularized network. At the same time a will be more sparse, practically lead to a more
precise estimation, and more interpretable result.
This idea can also be adapted to self-attention model. In self-attention, we can add a
regularization to a = Softmax(	). With this regularization, only part of the value
matrices proceeds to the decoding procedure. In real world application such as in transformer,
the sparsity in ai corresponds to the case when there are some words in a sentence should
not attend each other. In this case, a sparse aself help us only focus attention between useful
words in a sentence, thus improve predition.
15
Under review as a conference paper at ICLR 2020
It is worth mentioning that, through our theorem, we can tell why `1 loss are more helpful
than `2 loss under sparsity assumption. Both `1 loss and `2 loss will control the magnitude
of attention mask, however, `1 loss can also help control sparsity level s0 , and we can
see that the sample complexity bound is proportion to s20 . Although `2 loss on attention
mask can also help reduce the sample complexity, its effect will only be reflected in the log
term, therefore it is less effective. Similarly in Theorem 2, if we assume the attention to be
sparse(i.e. one word should not attend to all words, but only some relevant words), then `1
loss can help further reduce the sample complexity.
Regularization is one experimental approach for estimating a sparse and precise a and w. If some
other methods can achieve this target, they are also the right directions to reduce sample complexity
and improve current attention structures.
D.2 Beyond two layers
Here we discuss how our results of Theorem 1 and 2 can be extended to multi-layer neural nets.
First we consider a D-layer network with naive global attention structure: f (x) =
(w(D)φ(w(D-1).. .φ(hw ⑴，X Θ a))). Denoting Vf (wk) = fx, under the assumption that
at least one gradient term Vf (Wk) for k = 1,..., D has correlation at rate O ⑴ with bias u(this
assumption is parallel to (A5)), we can show the expectation term Ex,y(VR(w(k))) ≥ O(γ). Then
using the similar -covering and uniform convergence technique as Theorem 1 goes, we can still
show attention mechanism leads to a smaller -covering number and tighter Hoeffding bound, thus
leading to a smaller sample complexity bound comparing to baseline model.
Then we consider a D-layer network with self-attention structure. We denote the kth self-attention
layer follows gk(xgk-1) = wk2 φ(hwk1 , wV xgk-1aselfi)), where xgk-1 is the output of (k - 1)th
self-attention layer, with wV ∈ Rdv ×t, xgk-1 ∈ Rt×dk-1, aself ∈ Rdk-1×dk-1 , wk1 ∈ Rdv ×qk and
wk2 ∈ Rdk ×qk. a is calcualted in the same way with two-layer self-attention network. Then we have
the final output f(x) = wD2φ(hwD1 ,vec(w^vXDTaSef))), where XDT = (gD-ι(…gι(x)),
and wD1 ∈ R× , wD2 ∈ R× . In this way, the network calculate self-attention D times and finally
produce the final prediction. It is worth mentioning that, To obtain a scalar prediction in regression
model, we flatten the value matrix of the last layer as same as the two-layer model. We still denote
u = (wD2 φ(hwD1 , vec(wV XgD-1aself ))) - E(y|X)). Then the necessary assumptions parallel to
(A2), (A9) and (A10) are as follows.
•	(A14) All weights wkj for k = 1, . . . , D and j = 1, 2 satisfy kwkj k2 ≤ C10. And we
assume the prediction is centered, i.e. E(u) = 0.
•	(A15) The output y can be predicted by the D-layer self-attention network with an in-
dependent sub-Gaussian error with variance σ2, i.e, there exists a set of parameters
(w(1)?, w(2)?) such that yi = wD2 φ(hwD1 , vec(wVXgD-1aself))) + i as defined, where
Ci〜 subG(0,C42) fori = 1, 2, ...n, with Xi ⊥ Ci.
•	(A16) There exists interger k and r such that k ∈ {1, . . . , D} and r ∈ {1, 2}, such that
cor(Vf (wkr), u) = O(1) and sd(Vf(wkr) = O(1).
Then we can have the following theorem of sample complexity bound of multi-layer self-attention
model:
Theorem 7. Under (A1) and (A14) to (A16), we assume the weight term wkr satisfies (A16) with
kVf (wkr)k2 ≤ ck. dself is the total number of parameters in all value, query, key matrices. Then
for any γ > 0, given the sample size:
k
n & log( 一 )(dself + ɪ2(dk + dv )qk )
γ	i=1
where η = C1C2Cχ, with probability tending to 1, any stationary point (W(1), W(2), WQ, WK) of
the objective function (6) satisfies that: E(f (X) - E(y|X))2 . γ2
16
Under review as a conference paper at ICLR 2020
Remark: Because multi-layer self-attention models include a large parameter set with complicated
gradients, the assumptions are not as intuitive as the two-layer model. But the main assumptions
are parallel, such that the bias ui cannot be uncorrelated with all possible directions. And this
assumption is reasonable considering the high-dimensionality nature of networks. The extension of
this multi-layer model is omitted in the main paper since it leads to over-complicated derivations
and complicated assumption discussion and will distract readers from main ideas of the paper. We
believe the two-layer attention model is representative enough to provide theoretical evidence on why
attention reduces sample complexity.
D.3 CNN and RNN
Our analyses are based on fully connected network, and may not be able to apply to more involved
task such as CNN and RNN directly. But we believe the key message of our analyses also provide
insights in analyzing CNN/RNN with attention: The attention mechanisms can help us effectively
shrink the parameter space, thus reducing most of the noise and unnecessary variability in training.
Thus the stationary solutions are more likely to generalize well. That’s why we start with analyzing
the naive global attention model and self-attention models, which can help inspire the analysis of
these complicated tasks. More detailed analysis/experiments with CNN/RNN are important future
work. To this point, our experiments aim at validating the theoretical analyses and explaining why
attention works in general.
E Proofs
E.1 Proof of Theorem 1
Proof. The proof is divided into two parts. Firstly, we study the landscape of population risk in part
(a), then we evaluate the convergence of empirical risk to the population risk in part (b).
a.	Landscape of population risk
We introduce necessary notations beforehand. To emphasize the role of x and y separately, here
we denote R(w(1) , w(2) , a) = Ey|x(Rn(w(1) , w(2) , a)), which is the expectation of the empiri-
Cal loss gradient with respect to y, treating X as random, and VR(w) as corresponding deriva-
tives. And We denote Ex(VR(W(1), w(2), a)) = Eχ,y(Rn(w(1), w(2), a)), which is the expectation
of the empirical loss function with expectation to both x and y . In our analysis, first we will
study Ex (VR(w(1), w(2), a)), then we analyze Ey|xRn(w(1), w(2), a). The motivation of using
Ey|xRn(w(1), w(2), a) comes from that it simplifies the part (b) analysis of empirical risk conver-
gence, since the randomness of x has been included in the population risk analysis, making advantage
that the noise is independent from predictors. In the proof, we may use o(γ) for vector/matrix case.
In these cases, it means that every element in vector/matrix is o(γ). In the proof, we will regard d as
an arbitrary large fixed value, not diverging with n.
We denote
u = (w(2)φ(hw(1), x ai) - E(y|x))	(7)
and ui as the version with specified sample index. Then the derivatives of population risk with
expection to y can be presented as follows:
1n
VR(W⑵)=—3^uiφ(hw⑴,Xi Θ ai)
i=1
1n	0
VR(W(I)) = — Eui(Xi Θ a)(w⑵ Θ φ ({w⑴,Xi Θ a〉))T
n i=1
1n	0
VR(a) = - Eui(Xi Θ (W(I)T(W⑵ Θ φ ({w⑴,Xi Θ a))))
n i=1
17
Under review as a conference paper at ICLR 2020
By (A3), we know that E(yi|xi) = w(2)?T φ(hw(1)?, xi a?i). Therefore when (a, w(1), w(2)) =
(a?, w(1)?, w(2)?), all the ui are zero, and all the gradients expectations are zero. Thus for any true
set of parameter (a?, w(1)?, w(2)?), they have zero gradient expectation automatically. And the key
of our proof is showing that with high probability, any parameter (a, w(1), w(2)) cannot be stationary
point if E(∣W(2)φ(htw⑴，X Θ a))一 E(y∣x)∣2) ≥ O(Y2), because their gradients w.r.t to w(2) or
w(1) must be bounded away from zero.
By our assumption (A2) and (A4), our parameters w(1),w(2) and a are inside the `2 balls
Bd(0, C1),Bp×d(0, C2) and Bp(0, s02). By Lemma 5.2 in Vershynin (2010), we know the -covering
number N1,N2,N3 for these three balls are upper bounded by:
N1 ≤ (3C1/)d, N2 ≤ (3C2/)pd,N3 ≤ (3s02/)p
Then we know 3-covering number for the union of all three parameters N3 satisfies that N3 ≤
N∈ι N∈2 Ne3. For the ease of notation, We denote θ = (a, w(1), w(2)). Let Θe = {θι,…，ΘnJ be
a corresponding cover with N3 elements. Then we can always find Θ such that for any feasible
θ, there exists j ∈ [N] such that max(kw((j1)) 一 w(1) k2, kw((j2)) 一 w(2) k2, ka(j) 一 ak2) ≤ . In this
proof, we use parenthesis subscription (j) to represent elements in the cover, to distinguish it from
other subscriptions.
By triangle inequality, we have for v = 1, 2:
kVR(w(V))k2 ≥ ∣∣VR(w(V))k2 一 ∣∣VR(W(V))- VR(Wj))k2	(8)
And in the following section, we prove that if E(u2) ≥ O(Y2), we must have ∣∣Eχ(VR(w(2) ))k2 ≥
O(γ) or there exists kth variable of x, such that the kth column of VR(w(1)) satisfies
∣Ex(VR(wk(1)))∣2 ≥ O(Y). Here the subscription k without parenthesis corresponds to kth feature.
First we consider VR(w(2)). By (A4), there are at most s0 nonzero elements in x Θ a, and we
denote ∣∣ ∙ ∣ι,active as the '1 norm on the elements with corresponding non-zero attention mask. Thus
by inequality between norms, we have:
E(∣φ(hw⑴,Xi Θ a))∣2) ≤ |x Θ a|max |w(1) |
1,actiV e ≤ √S0CχCι = O(√S0)
Then we derive
∣Ex(VR(w(2)))∣2 = ∣cov(u, φ(hw(1), X Θ ai)) + E(u)E(φ(hw(1), X Θ ai))∣2
≥ Ilcov(u, φ(hw⑴,x Θ a)))∣∣2 - o(γ∕√s0)O(√s0)
Therefore if ∣cov(u, φ(hw(1), Xi Θ ai))∣2 ≥ O(Y), we already have ∣E(VR(w(2)))∣2 ≥ O(Y).
Then we consider the case when ∣cov(u, φ(hw(1), Xi Θ ai))∣2 = o(Y). In this case, we denote
r = cov(E(y|X), φ(hw(1), X Θ ai)) ∈ Rd, and denote the covariance matrix for φ(hw(1), X Θ ai)
as:
(Σφ)ij = cov(φi (hw(1), X Θ ai), φj (hw(1), X Θ ai))
Then plugging into cov((w(2))T φ(hw(1), X Θ ai), φ(hw(1), X Θ ai)) = r + o(Y), using subtraction
and addition, we have:
Σφw(2) = r + o(Y)
With (A5), covariance matrix Σφ is invertible with smallest eigenvalue lower bounded, we have
w(2) = Σφ-1r + o(Y).
Next we argue the following term by contradiction:
E(∣φ(hw⑴,x Θ a)) - φ?(hw⑴,x Θ ai)k2) ≥ O(Y)	(9)
where φ?(hw(I), X Θ a)) represents the function corresponding to the true parameter set
(w(1)?, w(2)?, a?). If we have equation 9 violated, we know:
至Φi - ςφ?,=
E(φ(hw(1), x Θ ai)iφ(hw(1), x Θ ai)j) - E(φ(hw(1), x Θ ai)i)E(φ(hw(1), x Θ ai)j)
-(E(φ?(hw(1), x Θ a))iφ*(hw(D, X Θ a))j) - E(φ?(<w⑴,X Θ a))i)E(φ*(hw(D, X Θ a))j))
.。⑴(E(kφ(hw⑴,x Θ a))i - φ?(hw⑴,x Θ a)i∣2 + E(∣∣φ(<w⑴,X Θ a))j - φ?(hw⑴,X Θ a j∣2))
= o(Y)
18
Under review as a conference paper at ICLR 2020
where We use E(kφ(hw⑴，X Θ ai)k2) ≤ E(kφ?(hw⑴，X Θ ai)k2) + E(kφ(hw⑴，X Θ a))一
φ?(hw⑴，X θ ai)∣∣2) = O⑴,and We know φ?(hw⑴,X Θ a)) corresponding to true parameter set
is finite. Since we have derived w(2)? = ∑-1r, with lower bounded eigenvalue assumption, we can
derive:
Σ-1	-1	-1	-1	-1	-1	-1
φ 一 Σφ? = Σφ Σφ(Σφ 一 Σφ? ) = Σφ (I 一 (Σφ? + o(γ))Σφ? )
=ς-Io(Y)ς-?1 = O(Y)
We know if we have kcov(u, φ(hw(1), Xi Θ ai))k2 = o(γ), thus w(2) = Σφ-1r + o(γ). Therefore
we conclude w(2) = w(2)? + o(Y). Plugging back to the formula of u in equation 12, we have:
E(u2) = E(|w(2)?T φ(w(1)?, X Θ a) 一 w(2)T φ(hw(1), Xi Θ ai)|2)
≤ E(|w(2)?T φ(w(1)?, X Θ a) 一 w(2)?T φ(hw(1), Xi Θ ai)|2)
+ E(|w(2)?T φ(hw(1), Xi Θ ai) 一 w(2)T φ(hw(1), Xi Θ ai)|2) = o(Y2)
Here we conclude the contradiction. When condition equation 9 is violated, we derive E(u2) = o(Y2).
Therefore if E(u2) ≥ O(γ2), we must have E(kφ(hw⑴，Xi Θ a)) 一 φ*(<w⑴Xi Θ aik2) ≥ O(γ).
Now we are ready to study VR(w(1)). Recall in assumption (A5), we denote hk(x) = χ∙kak (w(2) Θ
φ0(hw⑴,Xi Θ a))), where x∙k represents kth feature in x, and a* is the attention weight for
χ∙k. And we have proved that E(kφ(hw⑴，Xi Θ a)) — φ*(<w⑴Xi Θ aik2) = O(γ). Thus by
(A5), we can find k,t such that sd(ht(x∙k)) = O(l) and Cor(u, ht(x∙k)) = O(1). Then we have
∣cov(u, ht(x∙k))| = sd(u)sd(ht(x∙k))∣cor(u, ht(x∙k))|. We know
sd(u) = pE(u2) - ∣E(u)∣2 = PO(Y2) — o(γ)2 = O(Y)
Therefore we have:
∣∣E(VR(wk1)))k2 ≥ ∣∣E(uht(x∙k))k2 = IIcov(u, ht(x∙k)))+ E(u)E(ht(x∙k))k2 = O(γ)
Here we conclude that we can always find k ∈ {1, . . . , p}, such that kE(VR(wk(1)))k2 ≥ O(Y).
With this conclusion, we move to bound the gradient term vi = uiφ(hw(1), Xi Θ a)) and zik =
ui(Xik Θ ak)(w(2)T) Θ φ0 (hw(1), Xi Θ a))). With any fixed parameter set, we can calculate:
∣vi∣22 . (C2∣φ(hw(1), Xi Θ a))∣2)2∣φ(hw(1), Xi Θ a))∣22 = s02C14C22Cx4
∣zik∣22.(C2∣φ(hw(1),XiΘa))∣2)2Cx2C22=s0C12C24Cx2
where we use again that there are at most s0 nonzero elements in X Θ a:
∣φ(hw⑴,Xi Θ a))∣∣2 ≤ max{∣x Θ a∣}∣∣w⑴∣1,active = √S0CχCι
From the last section, we know there exists a constant c such that ∣Ex(VR(w(2)))∣2 ≥ cY or
∣Ex(VR(wk(1)))∣2 ≥ cY for some constant c. Suppose ∣Ex(VR(w(2)))∣2 ≥ cY, and we denote
σ2 = s02C14C22Cx4, And we know ∣vi∣22 is bounded as we derived. Therefore we can use Hoeffding
bound on the `2 norm in direction of Ex(VR(w((j2)))), since we know the variance on this direction is
smaller than σ2. Denoting VR(w((j2))) as the gradient with respect to jth parameter set in -cover for
j∈{1,...,N}:
P(kVR(w(2)) — Ex(VR(W(2)))k2 ≥ cγ) . exp(-nc2Y2)
(j)	x	(j)	2	3	σ2
By union bound, we have:
P(∃j ∈ [Ne], ∣VR(wg))∣2 ≥ 23γ) . Ne exp(-ncσY2)
Secondly we analyze ∣VR(w(2)) 一 VR(w((j2)))∣2 term. Here we use ui to represent the prediction
error for ith instant with respect to parameter (a, w(1), w(2)), and use ui(j) to represent the term
19
Under review as a conference paper at ICLR 2020
with respect to the parameter from j th element in -cover set. By triangle inequality, we have:
2n
∣∣VR(w(2)) - VR(w(2))k2 ≤ —k f(uiφ(<w ⑴,Xi Θ a))- Ui(j)φ(hwj Xi Θ aj)i))∣∣2
n i=1
2n	n
.n (I E(Ui-UiCj))°(〈W(1), Xi θ ai)k2 + k Σ2ui(j) (φ(hw(1) , Xi θ ai) - φ(hwj Xi θ a(j)i))k2)
i=1	i=1
. s0CX2 C12C2
We choose E = 35g020?，and Plug back above results to equation 8, then at least with prob-
ability 1 一 O(Ne exp(-ncγ)), We have ∣∣VR(W⑵)k2 > 号.Therefore we can choose n &
Cσ22 log( s0CcC2Cx )(pd + P + d), such that Ne exp(-ncγ-) = o(1). Finally we can conclude that
with probability 1-0n(1), for any (a, w(1), w(2)) such that E(W(2)φ(hW⑴，xΘai)-E(y∣x))2 ≥ γ,
we have ∣∣VR(W⑵)∣∣2 > cγ.
If ∣∣Eχ(VR(WkI)))k2 = O(γ). Applying the same technique, we can show that with probability
1 — on(1), we have ∣∣VR(WkI))k2 > 号.
b.	Convergence of empirical risk
So far, we have shown that for population risk with respect to y, with high probability, all the parameter
sets with poor prediction in expectation, i.e E(|W⑵φ(hW⑴，X Θ ai) — E(y∣x)∣2) ≥ O(Y2), their
population risk gradient with expectation to y must be away from zero. Now we move forward to
show that empirical risk will converge to the popular risk, i.e. RRn(Wl → VR(w). Thus these
parameter sets cannot have zero empirical gradient. In aspect of three parameter sets, they can be
represented as:
1n
VRn(W⑵)一VR(W⑵)=-Zeiφ(hW⑴,Xi Θ a〉)
1n	0
VRn(W(I)) — VR(W(I)) = — y^6i(xi Θ a)(W(2) Θ φ (hW(1), Xi Θ a〉))T
i=1
1n	0
VRn(a) — VR(a) = — Eei(Xi Θ (W(I)T(W⑵ Θ φ (hW(1), Xi Θ a〉)))
n i=1
With (A3), we know that e% 〜subG(0, C2), thus n Pn=ι e = O(√n) by C.L.T, combin-
ing the bound for φ(hW(1), Xi Θ a〉) we have derived in last section, with sample size n &
Cσ22 log( s0CcC2Cx )(pd + p + d), conclude that with probability 1 — Οp(1):
kVRn(W⑵)一VR(W⑵)k2 ≤ CY	(10)
6
kVRn(Wk1)) —VR(Wk1))k2 ≤ CY	(11)
Recalling part (a), under the first case that w.h.p ∣∣VR(W(2))k2 ≥ 号 for any parameter
(a, w(1), w(2)) with ∣W(2)φ(hW⑴，X Θ a〉)— E(y∣X)∣2 ≥ γ. Combining this with (10), we
can conclude that for any positive constant Y > 0, with required sample size, with high probability
that ∣VRn(W(2))∣2 > 0, thus they cannot be stationary solution for our loss function. As we stated,
if we are in another case, we have w.h.p ∣∣VR(wk1) )∣2 ≥ c3- , we can use same techniques to show
w.h.p ∣VRn(W(2))∣2 > 0.
In other words, under our assumptions, all the stationary points (a, W⑴,W⑵)in our programming
satisfy the prediction error upper bound rate Y w.h.p.	□
20
Under review as a conference paper at ICLR 2020
E.2 Proof of Corollary 1
Proof. To extend the result from Theorem 3 to corollary 1, we simply substitute the `2 norm bound
kxi ak2, from s0Cx to Cx since kak2 ≤ kak1 = 1. All the other parts keep the same. Thus the
only difference is that We remove s° in the bound comparing with Theorem 3.	□
E.3 Proof of Proposition 1
Proof. This proposition is a direct result of Theorem 3. Since the assumptions for a still hold, all the
bounds apply. The only different is that since a is not optimized together, we don’t have to consider
the -cover number for a in the maximum operator. This leads to a slightly tighter sample complexity
bound in corollary 1 comparing with theorem 3.	□
E.4 Proof of Theorem 2
Proof. Similar with Theorem 1, we obtained a new -covering bound:
N1 ≤ (3C1/)d,N2 ≤ (3C2/)pdvd, N3 ≤ (3C5/)tdq,N4 ≤ (3C6/)tdq, N5 ≤ (3C7/)tdv,
where N = Πi5=1 Ni. And also, the new vi and zik terms are:
vi = uiφ(hw(1), vec(wV xiaiself)i)
Zik= Ui(WVxiasef )∙k(w(2)T) Θ φ0(hw⑴,vec(wVxiaself))))
where ui = yi - (w(2)T φ(hw(1), vec(wV xiaiself)). Under assumptions, using the same agrument
with Theorem 3, we can show there exists a constant C such that either ∣∣E(VR(w(2))k2 ≥ cγ or
there exist k ∈ 1,... ,p such that ∣E(VR(Wk1)))∣2 ≥ cγ.
In the case when ∣E(VR(w(2))∣2 ≥ cγ. we have new bound of ∣v∣22 with respect to x is upper
bounded by σ2 = p2t2C14C22C74CX4 , considering aself is normalized by softmax function for each
vector in set. Parallel to Theorem 3, by hoeffding bound and union bound, we have:
P(∃j ∈ [Ne], ∣Vj ∣2 ≤ 2Cγ) . N exp(-nc⅛)
3σ
Secondly we bound ∣VR(W(2)) - VR(W(2))j ∣2 term by subtraction and addition:
∣VR(W(2)) - VR(W(2))j∣2
1n
≤ /I X Uj φ(hw ⑴,vec(wVxiaself(Z))i) — Uij)φ(hwj1), vec(wVxiaself(Z))i)∣2
i=1
1n
.n(k E(Ui-UiGe(hW(),vec(w ©a，")))∣∣2
i=1
n
+ ∣ X Uij(φ(hW(1) , WV xi aiself(Z)) - Ui(j)φ(hWj(1), vec(W(Vj)xiais(ejl)f(Z))))∣2)
i=1
. ptC12C22C72Cx2
recalling that Ui(j ) and ai(j) are corresponding to the j th epsilon cover. We choose =
CY
3c2c2 c7cχ,
22
and combine the above results. Then at least with probability 1 — O(Ne exp(—n f7γ )), we have
∣VR(w⑵)∣2 > 3. Therefore we can choose n & ^j^ log(pC1 C2[5；C7Cx )(pdvd + d +2pdq),
22
such then Ne exp(-n) = o(1). Thus with this required sample complexity, we have
IlVR(W⑵)—E(VR(W⑵)∣∣2 ≤ 等
∣VR(wk1)) — E(VR(WkI))∣2 ≤ 等.
Finally we can conclude that with
E(W(2)φ(hW⑴，Vec(WVxiaself(Z)))i) -
. In the same way, we can in the case when
high probability, any parameter (a, W(1), W(2))
— E(y|x))2 ≥ γ, we have ∣VR(W(2))∣2 >
show
with
3 or
∣VR(WkI))∣2 > 3. Then following the same empirical risk convergence argument, we show
that with high probability they cannot be stationary point.
□
21
Under review as a conference paper at ICLR 2020
E.5 Proof of Theorem 3
Proof. First with kak0 = s0, we know all the inputs xi with corresponding ai = 0, will be inactive
in the network. We can omit all these inactive inputs. Then We split n units into s° group, with [nn1 C
number of units in each group, and discard the leftover units. s0 different groups correspond to s0
active inputs with non-zero attention weight.
Inside each group, for example in jth group, denoting q = [nn1 C, we choose the input weights and
biases for i = 1,2,…，q as:
h1 (x) = max{0, wj x},
h2(x) = max{0, 2wjx - 1},
hq (x) = max{0, 2wjx - (q - 1)}
here we assign wj to be a row vector with j th variable equal to 1 and all other entries to be 0. And in
the second layer, we choose w(2) = (w3,…，w3), where
w3 = (1, -1,1,…，(-1)q+1), corresponding to hi to hq in each group. Then the designed network
has q linear regions inside each group, giving by the intervals:
(-∞, 0],(0,1],(1, 2], ∙∙∙ , [q - 1, ∞)
Each of these intervals has a subset that is mapped by w3h(x) onto the interval (0,1).Montufar et al.
(2014) Therefore the total number of linear regions is lower bounded by [ nC s0.	□
E.6 Proof of Theorem 4
Proof. Here we define an (α1, α2) scale transformation such that:
Tα1,α2 : (a, w(1), w(2)) 7→ (α1a, α2w(1), (α1α2)-1w(2))
Then we know the jacobian determinant for Tα1,α2 is α1p-dα2pd-d. Let r > 0 such that B∞(r, θ) is in
C(L, θ, ) and has empty intersection with (a, w(1), w(2)) = 0. Since pd > d, we assign α2 → ∞,
such that the jacobian determinant goes to infinity, and the volume of C(L, θ, ) goes to infinity.
For the Hessian matrix, without loss of generality, we assume there is a positive diagonal element
δ > 0 in a. Therefore the Frobenius norm ∣∣V2L(Tα1,α2(θ))kF of
V2L(Tα1,α2(θ))
α1-1I 0 0	α2-1I	0 0	V2L(θ)	α1-1I 0	0 α2-1I	0 0
00	(α1α2)I		0	0	(α1α2)I
is lower bounded by α1-2δ. Further we apply the fact that the biggest eigenvalue of a symmetric
matrix X is larger than CkX∣∣f, and pick ɑι < ^PM, then we have the biggest eigenvalue of
V2L(Tα1,α2 (θ)) is larger than M. Therefore there exists a stationary point such that the operator
norm for Hessian is arbitrary large. Thus we finish proving part (a).
Then we consider part (b). Similar with part (a), we define an α scale transformation such
that:
Tα : (w(1), w(2)) 7→ (αw(1), α-1w(2))
And all the value,query and key matrices remain the same. Then we know the jacobian determinant
for Tα = α(pdv-1)d. Since pdvd ≥ d, as we assign α → ∞, such that the jacobian determinant goes
to infinity, and the volume of C(L, θ, ) goes to infinity.
22
Under review as a conference paper at ICLR 2020
For the Hessian matrix, we still assume a positive diagonal element δ > 0 in w(1) . Simi-
larly We have the FrobeniUs norm ∣∣V2L(Tɑ(θ))kF of
V2L(Tɑ(θ))
α-1I	0	0		α-1I	0	0
0	αI 0	V2L(θ)	0	αI 0
0	0I		0	0I
is lower bounded by α-2δ. When we choose sufficient small α, we have the biggest eigenvalue of
V2L(Tα1,α2 (θ)) is larger than any constant M. Therefore there exists a stationary point sUch that
the operator norm for Hessian is arbitrary large.	□
E.7 Proof of Theorem 5
Proof. For global attention in part (a), We start With calcUlating the gradient of the empirical
loss function VRn(w(2)), where Rn(W⑴,w(2), a) = ( Pn=ι(w(2)Tφ(<w(1), Xi Θ a))一 y乎.
Denoting ui = (w(2)T φ(hw(1), xi ai) - y). The derivatives can be presented as folloWs:
1n
VRn(w(2)) = — fuiφ(hwθ, Xi Θ a))
n i=1
1n	0
VRn(W⑴)=—TUi(Xi Θ a)(w⑵ Θ φ'(hw⑴,Xi Θ a)))T
n
i=1
1n	0
VRn(a) = — Eui(Xi Θ (W⑴(W⑵ Θ φ ({w⑴,Xi Θ a))))
n i=1
By assumption, rank(φ(hW(1), Xi Θ a))i=1,...,n) = n, thus solving the linear system, we must have
ui = 0 for any i = 1,2,..., n to satisfy that VRn(W(2)) = 0. Thus we know that the loss is exactly
zero inside sample. Thus it must be a global minimum.
Part (b) can be proved by substituting a to a in part (a). Here we only consider the deriva-
tives with respect to W(1) and W(2), they can be presented as follows:
1n
VRn(W(2)) = - yφφ uW(^1)wVιe(Wec^XiaXiaself)))
n i=1
1n	0
VRn(W(I)) = - ɪ2Ui(VeC(WaXiase /))(W⑵ Θ φ (<W(1), vec(WVXiase，))T
n i=1
By assumption, rank(φ(hW(1), vec(WaXiaiself)))i=1,...,n) = n, thus solving the linear system, we
must have Ui = 0 for any i = 1, 2,..., n to satisfy that VRn(W⑵)=0. Thus we know that the loss
is exactly zero inside sample. Thus it must be a global minimum.	□
E.8 Proof of Theorem 6
Proof. First, we obtained new -covering bound for the parameter set (W(2), W(1), Wf, Wa):
N1 ≤ (3C1/)d, N2 ≤ (3C2/)td, N3 ≤ (3C8/)da,N4 ≤ (3C9/)df,
And N ≤ Πi4=1 Ni Similar to Theorem 1, we denote
p
u = (W(2)T φ(hW(1), X a(Xi)Xij) 一 E(y|X))	(12)
j=1
and ui as the version with specified sample index. Then the derivatives of population risk with
expection to y can be presented as follows:
23
Under review as a conference paper at ICLR 2020
1n
VR(w(2)) = — y^uiφ(hw( ), Xi Θ a))
i=1
1n p	p
VR(w(1)) = - Eui(E 矶Xi)Xj)(w⑵ Θ φ'(<w(1),E a(xi)xj)))T
n i=1	j=1 j=1
And also, the new vi and zik terms are:
p
vi = uiφ(hwj(1),	a(Xi)Xiji)
j=1
p
zik = ui a(Xik)Xijk (w (2)T) Θ φ (hwj(1),	a(Xi)Xij i))
j=1
In the case when kE(VR(w(2))k2 ≥ cγ. we have new bound of kvk2 *2 with respect to X is upper
bounded by σ2 = t2C14C22C82C92CX4 with normalized attention weight. Same argument follows for
the case when kE(VR(w(1 ))kk2 ≥ cγ Then follow the same approach as Theorem 1 and 2, we
obtain the sample complexity bound:
)σ2	tC1C2C8C9Cχ
n & c2γ2 log(-------c3γ-----)(d + td + df + da)
□
E.9 Proof of Theorem 7
Proof. Under assumption (A1) and (A14), we know all input features and weights are bounded.
Therefore we know kVf (wkr)k2 is Lipschitz continuous function, and we denote its Lipschitz
constant Lk. For wkr, we can derive that:
1n
VR(Wkr ) = _ EuiVf(Wkr )
i=1
Under (A14) to (A16), if we have E(f	(X) - E(y|X))2 . γ2, then:
kE(VR(Wkr))k2&sd(f	(Wkr)k2)sd(u)cor(Vf	(Wkr),	u) & O(γ)
Then similar with Theorem 1 and 2, we construct an -cover over all parameters θ :=
(Wk1, Wk2, WV, WQ, WK), and we denote it as {θ1, . . ., θN} such that for any feasible parame-
ter, there exist j ∈ [N] such that the maximum `2 distance to θj is smaller than . By calculating the
number of parameters in all matrices in θ, we have
1k
Iog(Ne) = -O(dself + ɪ2(dk + dv )qlk )
i=1
Denoting VR(W(kjr)) as the gradient with respect to jth parameter set in -cover for j ∈ {1, . . .,	Ne}:
P(kVR(wj)) - Ex(VR(Wkjr))k2 ≥ cY) . eχp(-nc2-γ2)
By union bound, we have:
P(∃j ∈ [Ne], kVR(w(2))k2 ≥ 2cγ) . Ne exp(-n1)
24
Under review as a conference paper at ICLR 2020
Secondly We analyze ∣∣VR(wkr) 一 VR(Wj))|卜 term. As We have shown that the gradient is
Lipschitz continuous, thus we have:
k
∣VR(wkr) 一 VR(Wj))∣2 ≤ Lke(dseif + X(dk + d°)q®)
i=1
We choose e =*,then at least with probability 1-O(Ne exp(-n c2γ2 )),we have ∣∣VR(w(2))k2 >
3Lk	σ
导.Therefore we can choose n & log(Ck)(dseif + Pk=ι(dk + dv)qk), such that Ne exp(-nc-γ-)=
o(1). Finally we can conclude that with probability 1 一 on(1), for any (a, W(1), W(2)) such that
E(W⑵φ(hW⑴，X Θ ai) 一 E(y∣x))2 ≥ γ, we have ∣VR(w⑵)∣2 > Cγ. Then following the
convergence of empirical risk procedure of Theorem 1, we show with probability going to 1 such that
∣VRn(W(2))∣2 > 0 and all parameters with prediction error O(γ) cannot be stationary point as long
as n & log(Ck)(dseif + Pk=ι(dk + dvIqk). Thus we complete the proof.	口
F	Additional Numerical Experiments
F.1 Convergence plots for classification tasks on Noisy-MNIST dataset
We present additional results on convergence of 2一 layer neural networks on classification task
involving Noisy-MNIST dataset (Section 5.1 of main paper). This dataset is formed by embedding
an s × s digit image in a noisy image of size 48 × 48. We show convergence results for three
settings: s = {5, 8, 15}. In each setting, the learning rate of models are varied. The plots are shown
in Figures 4,5, and 6. We observe that in every setting, attention models converge faster than the
baseline models not employing attention.
F.2 Convergence plots for regression task
In this section, we present convergence plots for sample complexity experiments discussed in Section
5.3 of the main paper. Regression task is considered in this experiment. Convergence plots of baseline
model, attention model and regularized attention models are plotted for various sizes of the datsset
Ns as shown in Figure 7. We observe that for all Ns values, regualrized attention model converge
fastest followed by attention model which is then followed by baseline model. So, we conclude that
in addition to achieving improved sample complexity, attention models converge faster than models
not employing attention. Also, regularization helps speed up the model convergence.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning via
Over-Parameterization. In Proceedings of the 36th International Conference on Machine Learning,
ICML ’19, 2019.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmdssan, Stockholm, Sweden, pp. 244-253, 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014.
Andrew R Barron and Jason M Klusowski. Approximation and estimation for high-dimensional deep
learning networks. arXiv preprint arXiv:1809.03090, 2018.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1019-1028, 2017.
25
Under review as a conference paper at ICLR 2020
Learning rate: 0.001
Figure 4: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size s
一泼.εJ ‰⅛U3W‹
15
26
Under review as a conference paper at ICLR 2020
Learning rate: 0.005
Figure 5: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size s
Learning rate: 0.001
8
27
Under review as a conference paper at ICLR 2020
《泼,εJ ‰⅛U3W≤
Learning rate: 0.001
Figure 6: Convergence plots for classification tasks: Image size: 48 × 48, Digit patch size s
5
28
Under review as a conference paper at ICLR 2020
S
S
S
Figure 7: Convergence plots for regression task varying the number of samples Ns . "Baseline"
indicates 2- layer NN not using attention, "attention" denotes attention models, and "attention
regualrized" denotes attention models trained with L1 regularization
29
Under review as a conference paper at ICLR 2020
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Proceedings of the 36th International Conference on Machine
Learning, ICML ’19, 2019.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. CoRR, abs/1711.00501, 2017.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp.
315-323,2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
SePP Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deeP learning: Generalization gaP and sharP minima. arXiv
preprint arXiv:1609.04836, 2016.
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for
visual question answering. In NIPS, PP. 289-297, 2016.
Thang Luong, Hieu Pham, and ChristoPher D. Manning. Effective aPProaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, PP. 1412-1421. Association for ComPutational Linguistics, 2015.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and ChristoPher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, PP. 142-150,
Portland, Oregon, USA, June 2011. Association for ComPutational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Song Mei, Yu Bai, Andrea Montanari, et al. The landscaPe of emPirical risk for nonconvex losses.
The Annals of Statistics, 46(6A):2747-2774, 2018a.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscaPe of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018b. ISSN 0027-8424. doi: 10.1073/Pnas.1806579115.
Volodymyr Mnih, Nicolas Heess, Alex Graves, and koray kavukcuoglu. Recurrent models of visual
attention. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 27, PP. 2204-2212. Curran Associates, Inc.,
2014.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deeP neural networks. In Advances in neural information processing systems, PP.
2924-2932, 2014.
Quynh Nguyen and Matthias Hein. The loss surface of deeP and wide neural networks. In Proceedings
of the 34th International Conference on Machine Learning (ICML 2017), Sydney, NSW, Australia,
6-11 August 2017, PP. 2603-2612, 2017a.
Quynh Nguyen and Matthias Hein. The loss surface of deeP and wide neural networks. arXiv preprint
arXiv:1704.08045, 2017b.
Tomaso A. Poggio and Qianli Liao. Theory II: landscaPe of the emPirical risk in deeP learning.
CoRR, abs/1703.09833, 2017.
Yunchen Pu, Martin Renqiang Min, Zhe Gan, and Lawrence Carin. AdaPtive feature abstraction
for translating video to text. In Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, PP. 7284-7291, 2018.
Blaine Rister and Daniel L. Rubin. Piecewise convexity of artificial neural networks. Neural Networks,
94:34-45, 2017.
30
Under review as a conference paper at ICLR 2020
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. CoRR, abs/1707.04926, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer
neural networks, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems,pp. 5998-6008, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference
on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2048-2057,
Lille, France, 07-09 Jul 2015. PMLR.
Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. CoRR, abs/1805.08318, 2018.
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Simple baseline
for visual question answering. arXiv preprint arXiv:1512.02167, 2015.
Pan Zhou and Jiashi Feng. The landscape of deep learning algorithms. CoRR, abs/1705.07038, 2017.
31