Under review as a conference paper at ICLR 2020
On PAC-Bayes Bounds for Deep Neural
Networks using the Loss Curvature
Anonymous authors
Paper under double-blind review
Ab stract
We investigate PAC-Bayes bounds for deep neural networks through the closely
related IB-Lagrangian objective. We first propose to approximate the IB-
Lagrangian through a second order Taylor expansion of the randomized loss
around the minimum. For the case of Gaussian priors and posteriors with fixed
means and diagonal covariance, we are able to derive a lower bound to this approx-
imation that corresponds to an “invalid” PAC-Bayes prior (a prior that is training
set dependent). Our lower bound depends only on the flatness of the minimum,
and the distance between the prior and posterior means. Through a number of
experiments our lower bound implies easy and and hard cases where one can or
cannot prove generalization even through “cheating”. Motivated by this result, we
see that for the easy cases, a simple baseline closely matches our lower bound and
is sufficient to achieve a non-vacuous PAC-Bayes bound. Crucially the baseline
prior is centered on the random deep neural network initialization. This suggests
that a good prior mean choice is the main innovation in recent non-vacuous bounds
with further optimization of the PAC-Bayes bound through SGD having a com-
plementary role.
1	Introduction
Recently two works Dziugaite & Roy (2017) Zhou et al. (2018) have made significant progress in
proving generalization for deep neural networks. They manage to prove non-vacuous generalization
(b)
(a)
Figure 1: The importance of retaining the original minimum: A number of works have shown the
existence of an empirical correlation between generalization error and flatness in the loss landscape
around the deep neural network minimum μ0. Recently popular PAC-Bayes analyses can be seen
as a formal way of quantifying this flatness. However, moving forward from simple empirical cor-
relations, existing optimization based non-vacuous bounds compute implicitly a different minimum
μo and then evaluate the flatness around that minimum. By contrast We aim to estimate the flatness
and a related optimal posterior distribution around the original minimum μ0.
1
Under review as a conference paper at ICLR 2020
bounds for a simplified Mnist dataset Dziugaite & Roy (2017) and the Imagenet dataset respectively
Zhou et al. (2018). This stands in stark contrast with previous works Bartlett et al. (2017) Neyshabur
et al. (2017b) Golowich et al. (2017) which yield bounds that are vacuous by several orders of
magnitude and are usually motivated by simple empirical correlation with the generalization error.
Both works rely crucially on applying the PAC-Bayes framework McAllester (1999) which typi-
cally addresses the generalization error of stochastic classifiers. Given the randomized empirical
error Eθ〜q[L(Θ)], the randomized population error Eθ〜q[L(Θ)], a prior weight distribution P, a
posterior weight distribution Q and N training samples the PAC-Bayes bound gives a guarantee of
the form
θEQ[L(θ)] ≤ θEQ[L(θ)] + Ap(KL(Q||P) + B)/N,	(1)
with some probability 1 - δ where A and B are constants related to the derivation. The PAC-
Bayes bound models the complexity of a classifier as the KL-Divergence between a prior P and
a posterior Q weight distribution. Apart from yielding state of the art bounds for SVMs, a further
motivation for this framework, is it’s Bayesian nature i.e. is the existance of the prior P. Complexity
is not measured with respect to an arbitrary reference point, but as in the luckiness framework
Shawe-Taylor et al. (1998), with respect to a reference point that can potentially incorporate our
prior knowledge about good solutions to the classification problem, and can lead to possibly tighter
bounds.
In Dziugaite & Roy (2017) the authors model the weights as originating from a Gaussian posterior
distribution with diagonal covariance θ = μ + ξ Θ σ where ξ 〜 N(0, I). Then they optimize
directly the stochastic objective resulting from the PAC-Bayes bound, by approximating stochastic
quantities with MC sampling. Similarly in Zhou et al. (2018) the authorts first compress a DNN with
an off the shelf compession algorithm, removing redundant parameters and applying the PAC-Bayes
approach to the remaining weights. This can be seen as explicitly minimizing the length of a code
describing the DNN. While compression in Zhou et al. (2018) is done explicitly, Dziugaite & Roy
(2017) can also be seen under this light, the random variables form a variational code whose length
is explicitly minimized Blier & Ollivier (2018).
While these two works result in non-vacuous bounds they have a number of important limitations.
•	Importantly they provide generalization error guarantees for a different classifier than the
original. Compressing the neural network or modelling it as originating from a Gaussian
distribution whose mean is modified, results in finding a completely different point in the
parameter space than the original. The function that the new weights describe might or
might not be close to the original. This might seem like an academic problem. However we
argue that analyzing networks that typically result from vanilla SGD is equally important
potentially leading to better initialization and regularisation of SGD, as well as discovering
inherent limitions of our generalization proof toolbox.
•	The bounds provided by Dziugaite & Roy (2017)Zhou et al. (2018) are non-vacuous but
loose. What is the source of this looseness? Both methods rely in non-convex optimisation
that might have simply not converged properly. This is a particular problem in Dziugaite
& Roy (2017) where it is well known that VI techniques require laborious hyperparameter
tuning, something that has hindered significantly the wide applicability of Bayesian neural
networks Wu et al. (2018). Even when care is taken the resuls are widely considered sub-
optimal in terms of uncertainty estimation for prediction tasks and code length description
of the DNN Wu et al. (2018)Blier & Ollivier (2018). As such we argue that one stands to
benefit from circumventing these non-convex optimisation procedures when possible.
The PAC-Bayes bound can also be seen under the light of flat minima. Flat minima have been em-
pirically shown to correlate with better generalization Neyshabur et al. (2017a)Keskar et al. (2016).
The PAC-Bayes bound can be interpreted as balancing two terms, the randommized empirical loss
and the KL complexity term Neyshabur et al. (2017a). As the posterior can be arbitrarily chosen
increasing the variance of the noise added to the parameters will typically decrease the KL term
while increasing the empirical loss. If the neural network solution corresponds to a flat minimum,
2
Under review as a conference paper at ICLR 2020
more noise can be added to the parameters without affecting the empirical performance Neyshabur
et al. (2017a).
In this work we propose to tackle the above problems by using a second order Taylor expansion of
the loss around the minimum, in conjunction with the PAC-Bayes framework. The second order term
parameterized by the Hessian matrix corresponds to the curvature of the loss around the minimum.
In line with the works linking flat minima to better generalization Neyshabur et al. (2017a)Keskar
et al. (2016), we are effectively estimating the flatness of the minimum along all parameter dierec-
tions. Thus we are able to add more noise to the flat directions and less noise to the curved ones,
something that will typically decrease significantly the KL term in the PAC-Bayes framework while
ensuring that the loss of the stochastic classifier remains small. Crucially for the case of Gaussian
posteriors the optimal posterior covariance can be found in closed form, allowing us to circumvent
a non-convex optimization procedure by incuring an approximation penalty due to using a second
order approximation of the loss.
The resulting approximation is closely linked to the Laplace approximation in Bayesian statistics
Bishop (2006). It and similar approximations to the posterior have a appeared a number of times in
the literature of bayesian neural networks yielding good results in a number of tasks Maddox et al.
(2019); Khan et al. (2019); Ritter et al. (2018); Zhang et al. (2017); Khan et al. (2018). Similarly
second order approximations of the loss around a minimum have a long history in the literature of
DNN compression Dong et al. (2017); Wang et al. (2019); Peng et al. (2019); LeCun et al. (1990);
Hassibi & Stork (1993) often yielding state of the art results in parameter reduction.
Even though the resulting posterior is optimal with respect to our approximation, being able to
chose a data driven informative prior still leaves room for looseness. Importantly the prior in the
PAC-Bayes framework can depend on the data generating distribution but not on the training set
used to train the evaluated classifier. Workarounds include choosing data driven priors by training
a separate classifier on a different training set, or using the same training set but enforcing that the
training set is not too informative about each individual training signal. The later can be formalized
through the framework of differential privacy. Both of the above will usually involve some non-
convex optimisation procedure leaving again doubt as to the optimality of the classifier complexity
estimate. We show that for the case of Gaussian priors and posteriors with diagonal covariance we
can derive the optimal invalid prior covariance in closed form.
The invalid prior cannot be used to prove generalization, however it can be used as a sanity check
to see whether proving generalization is possible in principle. The optimal solution with respect
to both prior and posterior covariance results in a lower bound on a function closely related to the
PAC-Bayes bound (but not exactly equal given that we make a number of approximations). Through
experiments we find that, depending on the hardness of the dataset, one can find cases where the set
of feasible solutions implied from the lower bound and the set of non-vacuous PAC-Bayes bound
solutions don’t intersect. One is unable to prove generalization, even through choosing a prior in an
invalid manner.
A number of works Achille & Soatto (2018); Dziugaite & Roy (2017); Germain et al. (2016) have
noted the similarity between the stochastic PAC-Bayes objective and the objective
一，一―一、	—Γ ʌ ,	_____ 一
Ce (D; P, Q) = .E∕L(θ)] + βKL(QI∣P).
θ〜Q
(2)
For β = 1 this is known as the Evidence Lower Bound (ELBO) objective in Variational Inference
literature Kingma et al. (2015)Bishop (2006). In the Information Bottleneck framework Achille &
Soatto (2018)Tishby et al. (2000) it is know as the IB-Lagrangian. More recently the same objective
has been interpreted as the ”task complexity” Achille et al. (2019). Then β has the role of regulating
the amount of information in the randomized neural network Achille & Soatto (2018), smaller values
correspond to more information and potential to overfit. While the objectives 1 and 2 are not entirely
equivalent due to a square root term over the KL divergence in the PAC-Bayes case, we will be using
the IB formulation as removing the square root will ease our derivations. We note that Dziugaite &
Roy (2017) have used the two objectives intercheangably with no significant difference in results.
A number of preprints have appeared on Arxiv trying to link PAC-Bayes to flat minima Wang et al.
(2018); Li et al. (2019); Tsuzuku et al. (2019); Yang et al. (2019). These typically involve heuristic
choices for the optimal posterior, do not analyze the role of the prior, and focus on quantities that
simply correlate with the generalization error. Motivating vacuous generalization bounds on the
3
Under review as a conference paper at ICLR 2020
basis of empirical correlations with generalization error has been criticised in a number of recent
works Kawaguchi et al. (2017); Nagarajan & Kolter (2019b); Pitas et al. (2019).
2	Contributions
•	We propose to analyze a PAC-Bayes bound through a second order Taylor expansion of
the randomized empirical loss in the closely related IB-Lagrangian objective. We thus
pay an error resulting from the second order approximation in order to circumvent non-
convex optimisation procedures, which might require extensive hyperparameter tuning and
not converge properly.
•	We are able to find a lower bound to this second order taylor expansion that corresponds
to an invalid PAC-Bayes prior (the prior is training set dependent). Experimentally we find
cases where the set of feasible solutions implied by the lower bound does not intersect with
the set of non-vacuous PAC-Bayes accuracy-complexity pairs. This in turn implies easy
and hard cases were one can and cannot prove generalization using Gaussian priors and
posteriors with diagonal covariance.
•	While we rely on approximations, we see empirically that in easy cases one can find non-
vacuous generalization bounds using a very simple baseline where the prior is crucially
centered at the random DNN initialization. This in turn highlights the insightful prior mean
choice as the main innovation in previous non-vacous bound works Dziugaite & Roy (2017)
with bound optimization through SGD having a complementary role.
•	We motivate a layerwise method to optimise PAC-Bayes bounds with respect to the poste-
rior variance, reducing the required computations significantly.
3	PAC-Bayesian framework
We consider the hypothesis class HL realized by the feedforward neural network architecture of
depth L with coordinate-wise activation functions σ defined as the set of functions fθ : X →
Y (X ⊆ Rp,Y ⊆ RK) withfθ(x) = σ(σ(...σ(xT W0)W1)W2)..)WL) where θ ∈ ΘL ⊆ Rd is
a vectorization of the weights and ΘL = Rp×k1 × Rk1 ×k2 × ... × RkL ×K. Given the loss function
'(∙, ∙) We can define the population loss: L(θ) := E(x,y)〜P'(fθ(x), y) and given a training set of
N instances D = {(xj, y7-)}N=ι the empirical loss L(θ) ：=N PrLI '(fθ W), yi).
The PAC-Bayesian frameWork McAllester (1999) provides generalization error guarantees for ran-
domized classifiers draWn from a posterior distribution Q. We Will use the folloWing form of the
PAC-Bayes bound.
Theorem 3.1. (PAC-Bayesian theorem McAllester (1999)) For any data distribution over X × Y,
we have that the following bound holds with probability at least 1 - δ over random i.i.d. samples
D = {(xj, yj)}jN=1 of size N drawn form the data distribution:
E [L(θ)]≤ E [L(θ)] + SKL(QIPNn —.	⑶
θ-ql k θ ~ θ〜qL "，」V	2N
Here Q is an arbitrary ”posterior” distribution over parameter vectors, which may depend on the
sample D and on the prior P.
The frameWork models the complexity of the randomized classifier as the KL-Divergence betWeen
the posterior Q and a prior P. The prior P must be valid in that it cannot depend in any Way on the
training data. On the contrary the posterior Q can be chosen to be any arbitrary distribution. This
flexibility alloWs one to model deterministic neural netWorks as the mean of an arbitrary posterior
distribution, thus deriving results for a stochastic but closely related classifier.
The square root in 3 Will make subsequent calculations cumbersome. We Will therefore be analyzing
the similar objective
„ ʌ , ,
Ce (D; P, Q) = .E∕L(θ)] + βKL(QI∣P),
θ〜Q
(4)
Which is knoWn as the IB-Lagrangian Achille & Soatto (2018); Tishby et al. (2000). While the tWo
objectives are not exactly equivalent, We argue through experiments that the differences are minimal
and our results translate to the original PAC-Bayes bound 3.
4
Under review as a conference paper at ICLR 2020
4 Quadratic Approximation and Lower bound
The stochastic and non-convex objective 4 is difficult to analyze theoretically. As such we first
propose to expand the randomized loss using a Taylor expansion which will make the subsequent
analysis tractable. We get
„ ʌ , ,
Ce (D; P, Q) = .E∕L(θ)] + β KL(QI∣P)
θ〜Q
≈E
η〜Q0
η + 1 ητ Hn + o(llηll3)] + βκL(QI∣P)
(5)
≤ E ,[1 ηTHη] + βkl(QI∣p),
η〜Q0 2
where Q0 is a centered version of Q. We’ve made a number of assumptions. In the second line we
assumed that the loss L(θ) at the minimum is 0. In line 3 We assumed that the gradient ∂L(θ)∕∂θ
at the minimum is also 0, the term O( ∣∣n∣∣3) is negligible and that the loss is locally convex resulting
in the quadratic approximation being an upper bound. For a Well trained overparametrized DNN
assuming that the loss and the gradient at the minimum are zero is reasonable. Assuming local
convexity at the minimum is also often a reasonable assumption Sagun et al. (2017); Li et al. (2018).
Regarding the validity of the quadratic approximation We note that it often provides state of the
art results in DNN compression Dong et al. (2017); Wang et al. (2019); Peng et al. (2019); LeCun
et al. (1990); Hassibi & Stork (1993) being highly informative about parameter relevence. Finally
We Will be analyzing Gaussian posterior distributions With diagonal or close to diagonal covariance
Which can be assumed to sufficiently concentrate around the minimum making the local second
order approximation meaningful.
4.1	Optimal Posterior
We make the following modeling choices Q = N(μo, ∑o) and P = N(μι, λ∑ι) which are popular
in VI and PAC-Bayes literature. We can then shoW that the optimal posterior covariance of the above
objective for fixed prior and posterior means has a closed form solution.
Lemma 4.1. The convex optimization problem min∑° E [ 1 nτ Hιn] + βKL(Q∣∣P) where Q =
0 η〜Q0 2
N(μo, ∑o) and P = N(μι, λ∑ι) has the global minimum:
∑S = β(Hι + β ∑-1)-1,	(6)
λ
where Hl captures the curvature in the directions of the parameters, while Σ1 is a chosen prior
covariance.
This can been seen as a Laplace approximation Bishop (2006) to the posterior around the MAP
solution. In practice we will be using 6 by performing a grid search over the parameters β and λ.
Then we will try to find Pareto optimal pairs balancing the accuracy of the randomized classifier and
the KL complexity term.
4.2	Optimal Prior
The above solution is not optimal with respect to the prior covariance in that we have up to now
chosen it arbitrarily. Furthermore given that the choice of the random initialization as the prior mean
has been independently shown to result in much tighter bounds in a variety of settings Dziugaite &
Roy (2017); Nagarajan & Kolter (2019a) one would wish to isolate the effects of the prior mean on
the bound tightness from the prior covariance.
PAC-Bayesian theory allows one to choose an informative prior, however the prior can only depend
on the data generating distribution and not the training set. A number of previous works Parrado-
Hernandez et al. (2012); Catoni (2003); Ambroladze et al. (2007) have used this insight mainly on
simpler linear settings and usually by training a classifier on a separate training set and using the
5
Under review as a conference paper at ICLR 2020
Mnist2
Feasible
Non-Vacuous
Training Accuracy
Ez/(IL牙)1M>
(a)	(b)
Figure 2: Feasible solutions vs Non-Vacuous solutions: We merge the 10 classes in Mnist and Cifar
to create simpler 2 class problems. For different values of β We compute the optimal complexity
KL(Q∣∣P) + ln 2(N-1)
terms V -' ', ]N------δ- using 7, 8. We compute the accuracy of the stochastic classifier with
MC for 5 samples. We plot this loWer bound With the solid black line. All points above it are
feasible. We see that for the Mnist problem the two regions intersect suggesting that we might be
able to prove generalization using Gaussians with diagonal covariance. By contrast in the Cifar case
the two regions do not intersect suggesting that the prior and posterior means have a prohibitive
distance between them, and we cannot prove generalization with diagonal covariances.
result as a prior. Recently Dziugaite & Roy (2018) have proposed to use the original training set to
derive valid priors by imposing differential privacy constraints.
We ignore these concerns for the moment and optimize the prior covariance directly. The objective
is non-convex, however for the case of diagonal prior and posterior covariances we can find the
global minimum.
Lemma 4.2. The optimal prior and posterior for Ce (D; P, Q) = E [ 1 ητ Hl η] + βKL(Q∣∣P)
η〜Q0 2
with Q = N(μo, ∑o) and P = N(μι, λΣ1) and assuming that Σ-1 = Λ1 =
diag(Λ11, Λ21, ..., Λk1) and Hl = diag(h1l, h2l, ..., hkl) have:
λ" = 2β [ʌ∕h2l +	”: )2 - hil],
2β V	(μi0	μi1)
"21β [hil + " + (〃：-M)2 ]∙
(7)
(8)
where Hl encodes the local Curvature at the the minimum, μι CorresPonds to the random initializa-
tion (by design) of the DNN, and μo corresponds to the minimum obtained after optimization.
For our choice of Gaussian prior and posterior, the following is a lower bound on the IB-Lagrangian
under any Gaussian prior covariance:
min Ce(D; p, Q) & 5(X ail(μi0 - μiI)2 + β Xln( il + il)),	⑼
Σ0,Σ1	2 i	i	ail
where ail , ail (β, μi0, μi1, hil) = 2 [ Jhil + (μi0-1：、)2 - hil].
The above result is intuitively pleasing setting a lower bound to what we can achieve which de-
pends only on the initialization (by design), obtained minimum, curvature at the minimum and the
regularization parameter β. In particular the scaling factor λ has disappeared.
6
Under review as a conference paper at ICLR 2020
We now make some important notes about what one can and cannot prove using these results, by
stressing that the above result is a necessary but not a sufficient condition for generalization under
our prior and posterior modeling.
•	Given a deterministic deep neural network and it’s initialization (or other prior mean) one
can rule out being able to prove generalization using any choice of diagonal covariances
when modeling the priors and posteriors as multivariate Gaussians with fixed means. Mod-
eling with other distributions may give different results*.
•	One cannot prove generalization using this result, even in the case when the prior mean
is valid (only distribution dependent) and the feasible and non-vacuous sets intersect. One
still has to compute the prior and posterior covariances in a valid manner. As such a com-
putationally feasible region given finite data and computational resources as well as privacy
constraints, might be much smaller than the one we derive here.
We plot our lower bound for simplified 2 class Mnist and Cifar problems in Figure 2. We see that
while for the Mnist problem the feasible and non-vacuous regions intersect the same is not true for
the Cifar problem. What remains is to see if our results apply for the case of valid priors. First we
detail a number of computational issues in section 5.
5	Computational Aspects
We now present a number of computational and memory issues associated with the Hessian of a
modern deep neural network. There is ambiguity in the literature about the size of the Hessians that
can be computed exactly Kunstner et al. (2019). There have been few results in this area and the
main problem seems to be that the relevant computations are not well supported from common auto-
differentiation libraries, such as Tensorflow and Keras. However there is certainty on the fact that
storing and manipulating the full Hessian of a number of modern deep neural network architectures
would be infeasible as the matrix is of size H ∈ Rd×d where d is the number of parameters. As a
point of reference a dense uncompressed Numpy matrix for d = 50000 takes up 20GB of memory.
As such we detail in the next section a number of approximations that make both computing and
storing the Hessian feasible.
5.1	Approximating the Full Hessian
As noted in Kunstner et al. (2019) the generalized Gauss-Newton approximation of the Hessian
H(θ) coincides With the Fisher matrix F(θ) = Pn Epθ(y∣χn)[Vθ logpθ(y∣Xn)Ve logpθ(y|xn)T]
in modern deep learning architectures. While the Fisher matrix is difficult to compute exactly one
can compute an unbiased but noisy estimate as Martens & Grosse (2015)
F(θ) ≈ X[Vθ logPθ(yn∣Xn)VθlogPθ(yn∣Xn)τ],	(10)
n
where care must be taken to sample yn from the model predictive distribution yn 〜 pθ(y|xn).
Additionally we note that the interpretation of the outputs after the softmax as probabilities is not
well grounded theoretically Gal & Ghahramani (2016). Determining the true predictive distribution
requires MC sampling for example by taking multiple dropout samples Gal & Ghahramani (2016).
We now make two additional notes regarding computational aspects of the above. The approxi-
mation of the Hessian can be computed efficiently as the outer product of large but manageable
gradient vectors. The main computational burden after we approximate the Hessian, and given that
we choose a standard normal prior, is inverting a matrix of the form H + αI. This problem can be
tackled in a few different ways. The simplest would be to consider only the diagonal elements of H
and the resulting diagonal matrix can be efficiently inverted. However inversion of the full matrix
H + αI is also possible recursively using the Sherman-Morrison formula.
t While our result is a formal lower bound on what is achievable by 6, it,s applicability on direct minimiza-
tion of the IB-Lagrangian 4 depends on the tightness of the second order approximation.
7
Under review as a conference paper at ICLR 2020
Mnist2
Cifar2
Training Accuracy
Training Accuracy
(a)	(b)
Figure 3: Accuracy Vs Complexity for different bounds: We plot ʌ/ KL(QIP2+n δ and train-
ing accuracy (of the randomized classifier) for different architectures and datasets. Points to the right
of the dashed line correspond to non-vacuous pairs. All Mnist bounds are non-vacuous. All Cifar
bounds are vacuous. We are able to progressively get tighter bounds by using the diagonal Hessian
and then the full layerwise Hessian. The improvement is larger over the more difficult Cifar dataset.
Baseline
Diag Hessian
Full L Hessian
Further issues exist with computing the KL-Divergence of large multivariate Gaussians with non-
diagonal covariances in closed form which includes a determinant term that has to be computed with
the matrix determinant lemma, as well as sampling efficiently from these distributions. As such we
have used the diagonal variant of approximation 10 for our lower bound, but perform a layerwise
approximation of the Hessian for all other experiments. We detail this layerwise approximation in
the next section, and motivate it theoretically.
5.2	Layerwise approximation
For a specific case of the empirical loss we will now derive an upper bound on 4 which is more
suitable for optimization.
Lemma 5.1. Assumingthefollowingempiricalloss L(θ) = ||fe(X) — Y||f With X = [x0,…,XN]
and Y = [y0, ..., yN] the following is an upper bound on the IB Lagrangian given that we are at a
local minimum:
Ce(D;P,Q) . X Jxcij E0 [1 ητHijη] + βXKL(Qlj 13),	(ii)
l V j	η 〜Qlj	l,j
where l denotes different layers, j denotes the different neurons at each layer (we assume the same
number for simplicity), Hlj denotes the local Hessian, and Q0lj is a centered version of Qlj . The
local Hessian can be computed efficiently as Hlj = N PN=I zi-ιzi-ιT and zi-1 is the latent
representation input to layer l for signal i.
We see that we have managed to upper bound the empirical randomized loss by a scaled sum of
quadratic terms involving layerwise Hessian matrices and centered random noise vectors. Intuitively
we have reduced the complexity of our optimization problem simply by turning it into a number of
separate subproblems. The local Hessians can be computed efficiently from outer products of a
forward pass of the dataset. Apart from avoiding using backpropagation, breaking the Hessian into
subproblems in this manner allows us to move beyond the simplistic diagonal approximation. Im-
plicitly the Hessian now has a block diagonal structure and the blocks are small enough to be inverted
directly for the architectures used in this paper. For architectures with larger latent representations
the Sherman-Morrison formula can be used instead. While 5.1 holds for a specific loss which is
uncommon in practice, we have found empirically that solving the posterior optimization problem
in a layerwise manner gives good results with the more common loss functions.
8
Under review as a conference paper at ICLR 2020
6	Experiments
We now make a number of experiments on the simplified 2 class Mnist and Cifar datasets. Specifi-
cally we test the architecture
input → 300FC → 300FC → #classesFC → output
on Mnist and
input → 200FC → 200FC → #classesFC → output
on Cifar. We note that the Mnist architecture corresponds exactly to the architecture T-3002 p.7 in
Dziugaite & Roy (2017). We train each configuration to 100% accuracy and derive the layerwise
Hessians. We model the prior and posteriors as multivariate Gaussians centered at the initialization
and deterministic solution respectively. For the prior we choose the uninformative unit diagonal
covariance, scaled by the free parameter λ. The baseline posterior that we use has the same diagonal
covariance as the prior. For the baseline we perform a grid search over λ which increases the
complexity negligibly. For the optimized posterior we initially test a diagonal approximation of
the Hessian ”Diag Hessian” which results in an optimal diagonal covariance. We perform a grid
search for the parameters λ and β using formula 6 to derive candidates for the optimal posterior
covariance. For each point on the grid We calculate the empirical accuracy over the training set
. R” ，C-1∙	… ，1 C n „ n -	∕KL(Q∣∣P )+ln 2(N-I)
using Monte Carlo sampling and 5 samples, as well as the complexity term V -' '' 2n-δ一.
We then choose the Pareto optimal points from all candidates. We plot the results in Figure 3.
Interestingly we see that for the case of Mnist the baseline is tight with respect to our lower bound
and provides non-vacuous bounds. Therefore not much improvement can be achieved using the
Hessian approach. This implies a more careful interpretation of the results in Dziugaite & Roy
(2017). We see that non-vacuity can also be achieved as a result of the problem being very simple,
and the choice of the prior mean being the random initialization. The optimization techniques em-
ployed in Dziugaite & Roy (2017) should simply tighten the bound further, mainly by moving the
posterior mean closer to the prior mean (the random initialization). For the case of Cifar we see
that we can significantly tighten the bound. However we cannot manage to turn a vacuous bound
to a non-vacuous one in line with our lower bound. We also test a block-diagonal approximation
to the Hessian ”Full L Hessian” where each neuron of the network has it’s own block. Our non-
diagonal layerwise approximation however crude seems to improve significantly over the diagonal
case and slightly crosses our diagonal lower bound. This suggests that better approximations of the
Hessian as well as better prior means apart from the random initialization might be needed to prove
generalization in complex datasets and architectures.
7	Conclusion
We have presented a lower bound on an approximation of the IB-Lagrangian for the case of multi-
variate Gaussian priors and posteriors with diagonal covariance. This coincides with a lower bound
on a PAC-Bayesian generalization bound for an invalid (training set dependent) prior. For cases
where the feasible and non-vacuous regions intersect we have seen that it is possible to reach the
lower bound and achieve non-vacuous bounds by using valid non-informative priors. We have also
presented closed form solutions for the optimal posteriors given fixed means under our modeling
assumptions, and motivated theoretically breaking the estimation into layerwise subproblems. Cru-
cially all results depend on high quality estimates of the Hessian which remains an open topic of
research for large scale modern deep neural networks.
9
Under review as a conference paper at ICLR 2020
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018.
Alessandro Achille, Giovanni Paolini, Glen Mbeng, and Stefano Soatto. The information complexity
of learning tasks, their structure and their distance. arXiv preprint arXiv:1904.03292, 2019.
Amiran Ambroladze, Emilio Parrado-Hernandez, and John S ShaWe-taylor. Tighter pac-bayes
bounds. In Advances in neural information processing systems, pp. 9-16, 2007.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural netWorks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Leonard Blier and Yann Ollivier. The description length of deep learning models. In Advances in
Neural Information Processing Systems, pp. 2216-2226, 2018.
Olivier Catoni. A pac-bayesian approach to adaptive classification. preprint, 840, 2003.
Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural netWorks via layer-Wise
optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 4857-4867,
2017.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural netWorks With many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential
privacy. In Advances in Neural Information Processing Systems, pp. 8430-8441, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059,
2016.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory
meets bayesian inference. In Advances in Neural Information Processing Systems, pp. 1884-
1892, 2016.
Noah GoloWich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural netWorks. arXiv preprint arXiv:1712.06541, 2017.
Babak Hassibi and David G Stork. Second order derivatives for netWork pruning: Optimal brain
surgeon. In Advances in neural information processing systems, pp. 164-171, 1993.
Kenji KaWaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivas-
tava. Fast and scalable bayesian deep learning by Weight-perturbation in adam. arXiv preprint
arXiv:1806.04854, 2018.
Mohammad Emtiyaz Khan, Alexander Immer, Ehsan Abedi, and Maciej Korzepa. Approximate
inference turns deep netWorks into gaussian processes. arXiv preprint arXiv:1906.01930, 2019.
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
zation trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.
Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical fisher approxi-
mation. arXiv preprint arXiv:1905.12558, 2019.
10
Under review as a conference paper at ICLR 2020
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing Systems, pp. 598-605, 1990.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In Advances in Neural Information Processing Systems, pp. 6389-6399,
2018.
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based
analysis of sgd for deep nets: Dynamics and generalization. arXiv preprint arXiv:1907.10732,
2019.
Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A
simple baseline for bayesian uncertainty in deep learning. arXiv preprint arXiv:1902.02476, 2019.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
David A McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355-363, 1999.
Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from
initialization. arXiv preprint arXiv:1901.01672, 2019a.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generaliza-
tion in deep learning. arXiv preprint arXiv:1902.04742, 2019b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017b.
Emilio Parrado-Hernandez, Amiran Ambroladze, John ShaWe-Taylor, and Shiliang Sun. Pac-bayes
bounds with data dependent priors. Journal of Machine Learning Research, 13(Dec):3507-3531,
2012.
Hanyu Peng, Jiaxiang Wu, Shifeng Chen, and Junzhou Huang. Collaborative channel pruning for
deep netWorks. In International Conference on Machine Learning, pp. 5113-5122, 2019.
Konstantinos Pitas, Andreas Loukas, Mike Davies, and Pierre Vandergheynst. Some limitations of
norm based generalization bounds in deep neural netWorks. arXiv preprint arXiv:1905.09677,
2019.
Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for
overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, pp.
3738-3748, 2018.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of
the hessian of over-parametrized neural netWorks. arXiv preprint arXiv:1706.04454, 2017.
John ShaWe-Taylor, Peter L Bartlett, Robert C Williamson, and Martin Anthony. Structural risk
minimization over data-dependent hierarchies. IEEE transactions on Information Theory, 44(5):
1926-1940, 1998.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Normalized flat minima: Exploring scale
invariant definition of flat minima for neural netWorks using pac-bayesian analysis. arXiv preprint
arXiv:1901.04653, 2019.
Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning
in the kronecker-factored eigenbasis. arXiv preprint arXiv:1905.05934, 2019.
11
Under review as a conference paper at ICLR 2020
Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying generalization
properties in neural networks. arXiv preprint arXiv:1809.07402, 2018.
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, Jose MigUel Hernandez-Lobato,
and Alexander L Gaunt. Deterministic variational inference for robust bayesian neural networks.
arXiv preprint arXiv:1810.03958, 2018.
Jun Yang, Shengyang Sun, and Daniel M Roy. Fast-rate pac-bayes generalization bounds via shifted
rademacher processes. arXiv preprint arXiv:1908.07585, 2019.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as
variational inference. arXiv preprint arXiv:1712.02390, 2017.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous gen-
eralization bounds at the imagenet scale: a pac-bayesian compression approach. arXiv preprint
arXiv:1804.05862, 2018.
12
Under review as a conference paper at ICLR 2020
Appendix
A.	Additional Experiments
We test the architectures
input → 300FC → 300FC → #classesFC → output
on Mnist and
input → 200FC → 200FC → #classesFC → output
on Cifar. We conduct additional experiments on the original Cifar10 and Mnist10 datasets, as well as
Cifar5 and Mnist5 where we merge the 10 classes into 5. The results are consistent across datasets,
with more improvement when incorporating the Hessian for more difficult datasets.
Mnist2	Mnist5	MnistlO
SZ (巴一 N)T¾>
Baseline
Diag Hessian.
■ Full L Hessian
SZ (巴一 N)T¾>
S/(a=l¾τ¾Λ
⑶
Cifar2
O.O
.8.6.4.20
Cl.Il.Il.Cl.O-
至 Z/(d=l⅞τa>
(b)
Cifar5
1/(d=l⅞τ¾>
(c)
(d)	(e)	(f)
KL(QIIP)+ln 2(N-1)
Figure 4: Accuracy Vs Complexity for different bounds: We plot V -寺 δ- and tram-
ing accuracy (of the randomized classifier) for different architectures and datasets. Points to the right
of the dashed line correspond to non-vacuous pairs. All Mnist bounds are non-vacuous. All Cifar
bounds are vacuous. We are able to progressively get tighter bounds by using the diagonal Hessian
and then the full layerwise Hessian. The improvement is larger over the more difficult Cifar dataset.
13
Under review as a conference paper at ICLR 2020
B.	Proofs
Lemma 4.1. The convex optimization problem min∑° E [ 1 ητHιη] + βKL(Q∣∣P) where Q =
0 η〜Q0 2
N(μo, ∑o) and P = N(μι,λ∑ι) has the global minimum:
∑S = β(Hι + β ∑-1)-1,	(12)
λ
where Hl captures the curvature in the directions of the parameters, while Σ1 is a chosen prior
covariance.
Proof.
Ce(D;P,Q)= E ,[1 ητHιη]+ βKL(Q∣∣P)=
η〜Q0 2
E z[1tr(Hιηητ)] + βKL(Q∣∣P)=
η〜Q0 2
1tr(Hι E [ηηT]) + βKL(Q∣∣P)=	(13)
2	η 〜Q0
3tr(Hlςo) + ^l(tr(^rςi 1EO)- k + y(μo - μI)Tςi 1(μo - μI)
2	2λ	λ
det λΣ1
+Me^〉
The gradient with respect to Σ0 is
∂Cβ (D; P,Q)
∂∑o
[1 Hl + ɪ ∑-1 — β ∑-1].
2	2λ	2
(14)
Setting it to zero, we obtain the minimizer Σq = β(Hl + λ∑-1)-1.
Lemma 4.2. The optimal prior and posterior for Ce (D; P, Q) = E [ 1 ητ Hlη]+βKL(Q∣∣P) with
η 〜Q0
Q = N(μo, ∑o) and P = N(μι,λ∑ι) and assuming that Σ-1 = Λι = diag(Λ11, Λ21,…，ΛQ
and Hl = diag(h1l, h2l, ..., hkl) have:
AQI =2β["+(/-X1)2 — hi1],
(15)
AQ0 = 2β [hil + "+(/=1)2 ].
(16)
where Hl encodes the local curvature at the the minimum, μι corresponds to the random initializa-
tion (by design) of the DNN, and μo corresponds to the minimum obtained after optimization.
For our choice of Gaussian prior and posterior, the following is a lower bound on the IB-Lagrangian
under any Gaussian prior covariance:
^min Ce(D;p, Q) & 5(Xail(μio — μiι)2 + βXln( iι + iι)),	(17)
Σ0,Σ1	2 i	i	ail
Where ail , ail (β, μi0, μi1, hiι) = 2 [ Jhil + (μi00-μiι )2
— hil ].
□
14
Under review as a conference paper at ICLR 2020
Proof. Setting Λι = Σ-1 We can then see that the minimizer is equal to Σq = β(Hι + λΛι)-1.
Substituting ∑o = Σq in Ce(D; P, Q) We obtain:
Ce (D; P,Q)l∑0=∑o = E/1 ητ Hl η] + βKL(QI∣P )l∑0=∑o =
0 η 〜Q 2	0
Itr(Hl β(Hl + β AI)T) + β (tr(1 Aιβ(Hl + β AI)T)
2	λ	2λ	λ
+ λ(μo - μI)TAI(40 - μI) - k +ln
det λΛ1-1
det β(Hl + λ Aι)-1
)
=β tr(Hl (Hl + β AI)T) + β2 (tr(Aι(Hl + β AI)T))
2	λ	2λ	λ
β1	T
+ 2( + λ(μ0 - μ1) AI3。- μ1) - k + ln
det λΛ1-1
det β(Hι + λ Λι )-1
)
=β (tr((Hl + β Λ1)(H1 + β Λι)-1)
2λλ
γ(μo - μI)TAI(从。-μI) - k + ln
λ
det λΛ1-1
β1	T
= 77[+T(μ0 - μI) AI(40 - μI) + ln
2λ
det β(Hl + λ Aι)-1
det λA1-1
det β(Hι + λ Λι )-1
(18)
)
]
The above matrix equation 18 is difficult to deal With directly. We Will therefore use the common
diagonal approximation of the Hessian Which is more amenable to manipulation. Substituting A1 =
diag(Λ11, Λ21, ..., Λk1) and Hl = diag(h1l, h2l, ..., hkl) in the above expression We get
Ce(D;P)Q)l∑o=∑0 = 2(λX^ii(μio -μii)2 - Xln(苧) + Xln('il +))	(19)
i	ii
The above expression is easy to optimize. We see that the sole stationary point exists at
AQI = 2¼∕h2l + TlΓ~^2 )2 - hil].
2β V	(μi0 - μi1)
(20)
We noW turn to the original objective and calculate it’s second derivatives. For our diagonal approx-
imation the original objective turns into a sum of separable functions. We Will analyze the behavior
of one of them for simplicity. The result applies to all other functions in the sum.
h m. h h∖ L hil I L β ViO L β l L β(μi0 - μii) 1
Ce (D; P,Q) = χτ ViO + X2λVi7 2 2+ 々—2λ—V1
+ 2[X ln(λνii) - X In(Vi0)]
=X AiViO + X Biνi0 - X β + X Ci ɪ + DiX ln(λ%ι) - X ln(νio)]
i	i	Vi1	i 2 i	Vi1	i	i
(21)
where we have set Ai = hil, Bi = 2λ, Ci =队世钝^n1 , Di = 2. Denoting Cie(D； P, Q) one
function from this sum We calculate
15
Under review as a conference paper at ICLR 2020
	∂Ciβ(D; P,Q)	ZI l Bi 		 = Ai +		 ∂Vi0	V1i	Di	∂Ciβ (D; P, Q)	BiVi0 -	- 	二	-		 - Vi0	∂Vi1	Vi21	Ci』 -2	+ Vi21	Vi1	(22)
and	∂Ciβ (D; P, Q)	Di 	：	———7^— 一 d 2Vio	ViO	y；Pe) =2(BiVi0 + Ci) 与 ∂ Vi1	Vi1	Di 	 	 ViI	(23)
	∂Ciβ (D; Pq) ∂Vio∂Vii	Bi	∂Ciβ (D; P, Q)	Bi -	- 	二	二	-	 Vi21	∂Vi1 ∂Vi0	Vi21		(24)
We need to check whether the Hessian matrix is PSD so that the stationary point we found is a local
minimum and the function is convex. We do that by calculating whether all principal minors of the
Hessian are positive.
▽2Cie (νi0,Vii)=	VBi
L	ν2ι
2(BiViO + Ci) ν1τ - V2i
(25)
We see easily that det(D) > 0. While
det(V2Ciβ(νi0, Vii))
2(Bi νi0 + Ci )
(2CiDiViI -(DiViI- Bi%。)2)
(26)
1 1 β2∖ ((μio - μn)2	1
U砥彳八 一λ —ViI- 2
A first observation is that this determinant is not always positive and the function is not convex
everywhere. However we observe that it is not highly non convex either and the non convexity
mainly results from the function tending to infinity logarithmically on one of the boundaries. We
now check whether the sole stationary point is always a local minimum. We start by substituting
ν? = β(hii + λνL. )-1 in the multiplicand of 26 as the multiplier is positive by definition
det(V2Ciβ(VhViI)) = ~2^τβ2 ((“i0 ”) ViI- 1(νii - λ(% + λ；)-1)2)
Vi0 Vi1 2	λ	2 λ λ Vi1
_	1 β2 ((μio - μn)2	1, β,	%ιλ 、、2)
=H T (~λ -ViI- 2 (ViI- λ(hiiλVii + β)) )
—	1	β2 ((〃i0 - 〃i1 )2	_ VI (1 _ ( β ))2、
=Vio⅛ ɪ V λ	ViI- T( -(hiiλVn + β)) )
__	1	β2 ((即io - μn)2 _ Vii	hiiλVii	2、	(27)
=V*2V≡ ^2^ V λ	2^( hiiλVii + β) )
=	1	β2 ((〃io - μn)2 -	入2%哈	、
=Vi02Vf1 工 V λ	2(hilλVi1 + β)2 )
=	? 2 3 o∖∕A ∖	, n>2 (2(μi0 - μi1 )2(hilXVi1 + β)2 - λ3 hll VI)
ViO2Vfι2λ(hiiλVii + β)2
=	? 2”/八 ∖A-1 ,底、2 (2Ail(μi0 - μi1)2(hilλ + Ai1β)2 - λ3h2l)
Vi0 2λ(hilλΛi1 + β)
Where we substituted Vi1 = Λi-11 as this will make the calculations easier. We now show a useful
identity for λ?=金 [qh2ι + (μ-0‰2 - hil]
16
Under review as a conference paper at ICLR 2020
4βhii
(μio - μii)2
- 2hil J21'/—)'/
=4(2hii (hii-∖" +J⅛) + J⅛
4β2 y y V	(μio — μii)2 y	(μio — μii)2
hilλ λ (( 二	4βhi1	ʌ ι 2β
~β~ 2β IIhil- Vhi1 + (μio- μii)2 ) + (3。一μ,ι)2
(28)
λ
(μio - μii)2
We substitute Λɪ∖ = Λ？ in 27 and again develop only the multiplicand
det(V"β(v?0,v?J)= ? 2 Li …(2A?i(〃i° -μii)2(hiiλ + Λ"β)2 - λ3h2)
νi0 2λ(hiiλΛ?[	+ β)2
=Ai(2A?1 (μi0 - μi1)2(hi1λ + NIey - λ3h21)
=Ai(2A?1 (μi0 - μil)2(h2iλ2 + 2hilλNIe + (八?1)2万2) - λ3h2ι)
Ai(2^?1 (μio - μi1)2(h2ιλ2+2hiiM?1e+heλ (Mo，”#2 - “Je 2) - λ3h2l)
Ai(2A?1 (μi0 - μi1)2(h2lλ2 + hil》A?ie +
eλ hi
(μi0 - μii)2
) -λ3hi2l)
Ai(2A?1 (μi0 - μi1)2(h2lλ2 +
(μiβ=1)2 ) + 2k)2(μi0 - μi1)2hilλβ - λ3h2l)
Ai(2^?1 (μi0 - μi1)2(h2lλ2+(μiβ tμ∖)2)
+ 2(彳---------------72 - A?l) (μi0 - μi1)2hilλβ - λ3h2l)
e ∖(μi0 - μii)2	)
=Ai(2A?1 (μi0 - μi1)2(h2lλ2 +	---iʒ2 ) + 2λ3h2l - 2h2lλ2(μi0 - Mi1)2A?1 - λ3h2l)
(μi0 - μii)2
=Ai(2A?1 (μi0 - μi1)2(h2lλ2 + γ-^-	i^2 ) + λ3h2l - 2h2lλ2(μi0 - Mi1)2A?1)
(μi0 - μii)2
=Ai(2Λ?] βλ2hil + λ3h2l)
>0
(29)
where we have set Ai = ν,*22λ(hJ* Li2 > 0. We have used 28 in lines 4 and 7.
νi0 2λ(hil λ(Λi1 )	+β)2
Indeed the stationary point is always a local minimum. What remains is to show that there are no
other local minima at the boundaries of the domain. From 21 we see that we only need to evaluate
expressions of the form f (%0) = /0 - ln(%0) and g(νQ = 六 + ln(%0). By application of
L'HOpital's rule it,s easy to show that
lim Ciβ (νi0, νi1 ) = lim	Ciβ (νi0, νi1 )
νi0 →0	νi0 →+∞
νi1 =ct	νi1 =ct
= lim Ciβ(νi0, νi1) =	lim Ciβ(νi0, νi1) = +∞
νi0 =ct	νi0 =ct
νi1→0	νi1→+∞
(30)
this concludes the proof.
□
17
Under review as a conference paper at ICLR 2020
Lemma 5.1. Assuming the following empirical loss L(θ) = ∣∣fθ(X) - Y||f With X = [xo,…，XN]
and Y = [y0, ..., yN] the following is an upper bound on the IB Lagrangian given that we are at a
local minimum:
Ce (D; P,Q)
XIX Cl	Eo [2ηHη] + β XKL(QjIIPj),
l V j	η 〜Qlj	l,j
(31)
where l denotes different layers, j denotes the different neurons at each layer (we assume the same
number for simplicity), Hlj denotes the local Hessian, and Q0lj is a centered version of Qlj . The
local Hessian can be computed efficiently as Hl = NN PN=I zi-1 Zι-1T and z∣-ι is the latent
representation input to layer l for signal i.
Proof. We start by defining a layerwise empirical error Eι(θι) := NN PN=1 ∣∣WιZl-1 - z；∣∣2. One
can then easily show that L(θ) ≤ PL-IL jEι(θι) QL=ι+ι I∣θkIIf + JEl(Θl) Dong et al. (2017)
substituting this in the IB Lagrangian we get
Ce (D; P,Q)
„ ʌ , ..
E [L(θ)]+ βKL(QIIP)
θ〜Q
≤
≤
[X qE(θ) YY IIθkIIF + ,El(Θl)] + βKL(QIIP)
ι=L
L-L
Xι=L r
k=ι+L
L
θEQ[Eι(θι)] ∏
ι=k+L
. ʌ ..	/	-入^	,
E [IIΘιIIf]+ 人E [El(Θl)] + βKL(QIIP)
θ〜Q	θ θ〜Q
E
θ〜Q
≤
XcιrE [Eι(θι)] + βKL(QIIP)
ι=ι V"
(32)
≤
Lu
X cι ut
ι=L
E[
η〜Q0
)! η + 2ηHιη + O(I忻II3)] + βKL(QI∣P)

X CIrnEQ0[1 η Hιη]+ βKL(QIIP)
were in line 3 we use the linearity of expectation, Holder,s inequality due to the non-negativity
of the random variables, and Jensen’s inequality for the concave square root. In line 4 we hide
the Frobenius terms into constants Cι. Each error term Eι (θι ) is only multiplied with Frobenius
norm terms IIθιIIF from the deeper layers. Therefore one can start optimizing from the final layer
and proceed to the first while considering Cι as constant. In practice we will just consider all Cι
as unknown scaling factors. In line 5 we expand each Eι(θι) term using a Taylor expansion, and
subsequently ignore the first term as the DNN is assumed to be well trained and the first derivative
will be zero, while terms with order higher than 2 are unimportant. We also use Q0 to denote the
centered version of distribution Q.
Taking the first and second derivatives of the layerwise error with respect to Wι we get
NN
^∂wf = N X ∂W!iiw'ZlT - zιii2 = N X(WIzl-L - Zl)2Zl-L
ι	i=L ι	i=L
∂2Eι(θ)	_ 1 X Zi	Zi	T
∂Wι∂W(j,:)	N M ι-1 ι-l
(33)
(34)
(j,:)
Where the second derivative is with respect to any row Wι , of the weight matrix Wι . We see
that the full Hessian matrix Hι = *&：)then has a block diagonal structure where each block is
equal to Hl = 焉 PN=L Zi-LZi-LT. Each row W(j,') corresponds to a neuron of the layer and for
18
Under review as a conference paper at ICLR 2020
an appropriate choice of prior and posterior with block diagonal covariances it is easy to see that the
final form of expression 32 factorizes as
Ce (D; P,Q)
X ∕χ cij	EQo [2ητHijη] + β X KL(Qj ||Pj)
l j η lj	l,j
this completes the proof.
(35)
□
19