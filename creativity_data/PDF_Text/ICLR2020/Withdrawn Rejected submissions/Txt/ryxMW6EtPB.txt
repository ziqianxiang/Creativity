Under review as a conference paper at ICLR 2020
DG-GAN: the GAN with the duality gap
Anonymous authors
Paper under double-blind review
Ab stract
Generative Adversarial Networks (GANs) are powerful framework for modeling
complex and high dimensional data. The training of GANs is difficult because
the optimization is a min-max problem. This paper understands GANs from the
perspective of duality gap and shows that the duality gap can be a good metric
to evolution the difference between the true data distribution and the distribution
generated by generator. Training GANs using the duality gap can provide com-
petitive results. Furthermore, we establish the generalization error bound of the
duality gap to help design the neural network architecture and select the sample
size.
1	Introduction
In the past few years, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are im-
pactful because it has shown lots of great results for many AI tasks, such as image generation,
dialogue generation, and images inpainting (Abadi & G Andersen, 2016; Goodfellow, 2016; Ho &
Ermon, 2016). Differing from other unsupervised learning methods for model generation that con-
centrate on the hard optimization of the measure of distribution fit such as the maximum likelihood
method, GANs, which are a kind of methods of implicit models (Mohamed & Lakshminarayanan,
2017; Tran et al., 2017), can be seen as a game between two networks, the generator and the dis-
criminator. Training GANs will improve the two networks’ capability synchronously. Denote the
discriminator as f and a generator as g. The objective of GANs is
infsupV(f,g) = E [φ(f (x))] + E [φ(1 - f (g(x)))],	(1)
g f	X 〜Pdata	X 〜Pz
where pdata is the true data distribution and pz is the standard Gaussian distribution. Here, the
goal of f is to discriminate the difference between two distributions and the goal of g is to generate
a distribution with the Gaussian noise. Therefore the problem of GANs is a min-max problem.
The minimization problem is to search for the optimal discriminator f that can distinguish two
distributions as much as possible and the maximization problem is to find the optimal generator g
such that the discriminator can not find the difference. So the GAN is just like a game between these
two players. This is in general a challenging task to find the best solution because it may be not a
concave-convex min-max optimization. This means that the objective, denoted by V (f, g), may not
be a convex function when fixing f and not a concave function when fixing g.
The first major problem of GANs is how to measure the difference between the generated distribu-
tion and the true data distribution. It means that there is no an unanimous metric to represent the
difference between the true data distribution and the generated distribution (Borji, 2018). Different
metrics have achieved different performances on the different benchmark datasets, although many
state-of-the-art models can show similar results (Lucic et al., 2017). It is also difficult to know
whether the generated distribution is close to the true distribution, and this is often observed by hu-
man eyes. Another problem is the convergence of the training algorithm of GANs, especially the
global convergence. It means that if the original generator and discriminator are random, it is diffi-
cult to confirm that the generator and discriminator can converge to the ideal conclusion by training
with given data. So the existed algorithms should be heuristic or it can get a bad result even we
train the neural networks with lots of datasets. Although it can be proved that the generator and
discriminator can converge to the local Nash equilibrium under some strong assumptions (Martin
et al., 2017), many GAN algorithms can not converge globally (Gemp & Mahadeven, 2019),
In this paper, our main contributions are:
1
Under review as a conference paper at ICLR 2020
•	We propose a new metric of GANs and prove that the metric can be an upper bound of the
traditional metrics.
•	We establish a generalization error bound under the new metric and show that the empirical
metric can be viewed as the loss function for GANs.
•	We propose an new algorithm with the new metric which demonstrates better results than
state-of-the-art algorithms.
The remainder of this paper is organized as follows. In Section 2, some related work are reviewed.
Section 3 gives the new metric named duality gap that can be seen as an upper bound of traditional
metrics. In Section 4, we establish a generalization error bound under the new metric and show that
the empirical duality gap can be viewed as the loss function for GANs. Section 5 and 6 provide the
new algorithm and some experimental results. Finally, we give our conclusions and future work.
2	Related work
The problem of the GANs’ metric and convergence has been extensively explored over the past few
decades, and a substantial amount of work has been proposed in the categories of convergence and
new metric. The duality gap has ever been suggested by Grnarova et al. (2018). However, they
only take the original GAN (Goodfellow et al., 2014) into consideration. Theis et al. (2015) has
showed that even though the log-likelihood of the data can be seen as a loss function to train a
generative model and thus can be seen as a metric of GANs, it has severe limitations because it may
generate some low quality models with a high likelihood. Tolstikhin et al. (2017) proposed to use
the probability mass of the real data “covered” by the model distribution as a metric. They used
a kernel density estimation method to approximate the density of generated models’ distribution
and this metric is more interpretable than the likelihood, making it easier to assess the difference in
performance of the algorithms. One of the most famous metric of GANs is the inception score (IS)
(Salimans et al., 2016), which uses a pre-trained neural network (the Inception Net (Szegedy et al.,
2016) trained on the ImageNet (Deng et al., 2009)) to capture the desirable properties of generated
samples. It can measure the quality of the generated models and discriminability. There are some
modifications of IS such as (Martin et al., 2017; Gurumurthy et al., 2017) and so on. Furthermore,
Martin et al. (2017) proposed Frechet Inception Distance(FID) between two Gaussian distribution
for evaluating the quality of these models. However, even though these kinds of metrics can get
some good enough results on some samples, the Gaussian assumption is not always right and the
FID can not work well with the non-labeled datasets.
There are some other research concentrating on the metric to estimate the generated distribution.
Such as Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), which measures the dissim-
ilarity between two probability distributions using samples drawn independently from each other.
However, the MMD method’s computation complexity is the quadratic in the sample size, which is
difficult to train. Arora & Zhang (2017) proposed to use the birthday paradox test to evaluate GANs,
this test approximates the support size of a discrete distribution and can also be used to detect mode
collapse in GANs. Generative Adversarial Metric(GAM) is proposed by Jiwoong Im et al. (2016),
which means exchanging discriminators or generators of two GANs and then comparing the two
GANs by engaging them in a battle against each other. Image Retrieval Performance (Wang et al.,
2016) evaluates GANs with an image retrieval measure, the main idea of which is to examine the
badly modeled images. There are some research that view the GANs as a zero-sum game.Grnarova
et al. (2018) proposed the duality gap, but the paper only takes the log-likelihood into consideration.
Balduzzi et al. (2018) introduced the Hamiltonian mechanics in the games and designed an algo-
rithm that can converge to the Nash Equilibrium faster, and this method has showed some desirable
results if applying in GANs. Oliehoek et al. (2017) studied GANs from the view of game theory and
suggested an algorithm of training GANs to the Nash equilibrium. Grnarova et al. (2017) considered
the Nash equilibrium for semi-shallow GAN architectures and other more complex architectures.
3	The duality gap
In the section, we give the definition of the duality gap. Because the duality gap comes from game
theory, we give some knowledge of game theory at first.
2
Under review as a conference paper at ICLR 2020
Definition 3.1. (Game) A strategy game is a tuple < P, {Si}in=1, {ui}in=1 >, where P =
{p1 , ..., pn} is the players sets, Si is the set of pure strategies for player i and ui is i’s payoff
real-valued function defined on the pure strategy profiles’s set: S = S1 × ... × Sn
The key of the game theory is the Nash Equilibrium, which is a strategy profile such that no player
can change his payoff unilaterally.
Definition 3.2. (Nash Equilibrium) A Nash Equilibrium is a strategy profile < s1 , ..., si, ...sn >∈ S
s.t. ∀ < s1, ..., s0i, ..., sn >∈ S, we have ui(s1, ..., si, ..., sn) ≥ ui(s1, ..., s0i, ..., sn) for any player
i.
In this paper, we only discuss GANs with only two players, the game mentioned below are two-
players’ game.
Definition 3.3. (Zero-sum game)A zero-sum game is a game with the two payoff functions u1(s1, s2)
and u2(s1, s2) s.t. u1(s1, s2) + u2(s1, s2) = 0 for any (s1, s2) ∈ S
For a two-players’ zero-sum game, its equilibria also is called saddle point, which has some impor-
tant properties and has attracted lots of attentions. Because the saddle point is difficult to research,
this leads the difficulty of the GANs’ research. About the equilibria, we have the following theorem:
Theorem 3.1. In a zero-sum game, we have
sup inf ui(s1, s2) = inf supui(s1, s2) = v	(2)
s2 s1	s1 s2
where the v is called the value of the zero-sum game.
The strategy (s1, s2) ∈ S is called the maximin strategy. For these two players, they have different
maximin strategies. The player 1’s maximin strategy is sb1 such that sups2 ui(sb1, s2) = v and the
player 2’s maximin strategy is sb2 such that inf s1 u(s1, sb2) = v. Furthermore, if we combine the
two maximin strategies of these two players, we can achieve an equilibrium.
3.1	The duality gap of GANs
The traditional machine learning problem can be seen as an optimization problem. The objective
to be minimized is denoted by a loss function. However, because the GAN objective is a min-
max problem, it can be seen as the zero-sum game, with the 2 players being the generator and the
discriminator. We will introduce the duality gap metric, which can be used to estimate the ability of
the generators and the discriminators, and the relationship of duality gap and the classical metric-
F - distance when the generator’s and discriminator’s capacity are unbounded.
A zero-sum game comes from game theory, consisting of 2 players D(Discriminator) and
G(Generator) with their strategy-fields F and G. A function V : F × G → R is the utilities of
the 2 players. By selecting (f, g) ∈ F × G, the D’s utility is +V and the G’s utility is -V . The
goal of the 2 players is to maximize the worst case utility, which is
supinfV(f,g) & inf supV(f,g).
f∈F g∈G	g∈G f∈F
(3)
The strategy (f *,g*) ∈ F ×G is called (Pure) Equilibrium if it satisfies that
SupV (f")= infV (f *,g).	(4)
f∈F	g∈G
According to the above discussion, the GANs’ duality gap metric of the pure strategy can be defined.
Definition 3.4. (Duality Gap ofGANs) Given 2 strategy fields F and G, Strategy (f *,g*) ∈ F ×G,
a convex function φ, a true data distribution pdata, and a Gaussian distribution pz, the duality gap
of (f *,g*) is
DG(f*,g∖= SupV (f,g1-infV (f *,g)	(5)
f∈F	g∈G
Here the V (f, g ) is the the function that GANs concentrate on:
V(f,g)=	E [φ(f(x))] + E [φ(1 - f(g(x)))]	(6)
3
Under review as a conference paper at ICLR 2020
3.2 Duality gap as a metric
The traditional metric used in GANs is a kind of distance between two distribution, denoted by
F - distance.
Definition 3.5. (F - distance) Given a function space F = {f : Rd → R|f ∈ F ⇔ 1 - f ∈ F}.
A convex function φ, a distribution pdata, a Gaussian distribution pz and a generator g, then
dF ,φ(Pdata,Pg) = suP E [φf (X))] + E [φ(1 - f (X))] - 2φ(I).	⑺
f ∈F X〜Pdata	X〜Pg	2
So the F - distance can be written as
dF,φ (pdata, pg) = sup V (f, g),	(8)
f∈F
where V (f, g) has been defined in equation (4).
Remark 3.1. F - distance is a distance between two distributions: pdata andpg. Fora special case
when φ(X) = X andF = {f : Rd → R|Lf < ∞}, then the F - distance is Wasserstein-Distance,
where Lf is the Lipschitz constant of f.
The next theorem shows that the duality gap can be an upper bound of F - distance with the given
condition.
Theorem 3.2. Iffor any distribution P, ∃g ∈ G, s.t. g(z)〜P where Z 〜Pz that is a given Gaussian
distribution. Assuming {f : Rd → R|Lf < ∞} ⊂ F, then
supV(f,g*) - inf V (f *,g) ≥ SuPV (f,g*) - 2φ(1) ≥ 0	(9)
f∈F	g∈G	f∈F	2
Proof. Observe that
sup V(f, g*) - inf V(f *,g) ≥ suP V(f, g*) - 2φ(1) ⇔ inf V(f *,g) ≤ 2φ(1).	(10)
f∈F	g∈G	f∈F	2	g∈G	2
According the property of G,
inf V(f *,g) ≤ V(f *,g)Ipg=Pdata= E [Φ(f *(x)) + Φ(1- f *(x))] ≤ 2Φ(1),	(11)
g∈G	X 〜Pdata	2
where the second inequality comes from the property of F . Hence,
supV(f,g*)- 2φ(1) ≥ V(f,g*)f=1 - 2φ(1)=0.	(12)
f ∈F	2	2	2
□
The theorem above shows that if the discriminator and generator have unbounded capacities, the
F - distance can be a metric to discriminate the Pg and Pdata and the duality gap is an upper bound
of the F - distance.
4	The generalization error bound on the duality gap
Considering the training of GANs with the new metric, we first establish the generalization error
bound of the duality gap. The generalization error bound is the gap between the training error and
the test error. In general, the gap can be replaced by the empirical error and the population error
when assuming the test datasets are infinite. The generalization error bound in general depends the
sample size and the complexities of the function spaces of the discriminators and generators. So
establishing the generalization error bound can guide the design of these two neural networks and
select the sample size. The generalization error bound for vanilla GANs has been studied in the
literature. For example, spectral weight normalization (Miyato et al., 2018) is used to establish a
tight bound for GANs by (Jiang et al., 2019).
The generalization error bound of the unsupervised learning is always related to the complexity of
the function space. We use Rademacher complexity to characterize the capacity of the function
space. Because GANs have two function spaces F and G and the duality gap is related to these two
spaces, the complexities of F and G are the keys to establish the duality gap’s generalization bound.
4
Under review as a conference paper at ICLR 2020
Definition 4.1. (Rademacher Complexity) Given a function space F and a random sample X =
{xι,..., xn} where Xi 〜μ, then the empirical and the expected Rademacher Complexity are, re-
spectively,
1n
RX (F) = E[sup -y2eif (Xi)],	R n,μ(F) = E R X (F)],	(13)
e f ∈F n i=1	X〜μn
where the distribution of E =(印，…，Cn) satisfies that P(Ei = 1) = P(Ei = —1) = 11.
The generalization error bound of the duality gap concentrates on the gap between the population
duality gap denoted by DG and the empirical duality gap denoted by DG,
∙^^"∙^^*'''∙ . . . . ^ , . . ^ , ,
Dd(f*,g*> SupV(f,g1-infV(f*,g),	(14)
f∈F	g∈G
where
V (f,g)= XjL[φ(f (x))]+zEz [φ(1 — f (g(z)))]= X:X φf≡ ,(15)
and the xi are selected from observed data and the zi are sampled from a standard Gaussian distri-
bution.
Theorem 4.1. If the true data sample X and the Gaussian-distribution sample Z are bounded and
the bound is denoted by BX and BZ, and the ∃LF, LG s.t. ∀f ∈ F and g ∈ G, the Lipschitz
constant off is less than LF, and the Lipschitz constant ofg is less than LG. Then with probability
at least 1 — 3δ
,_ _ . ^ 一 ^ , 一 ^ , _.
|DG — DG| ≤4ρφRχ(F) + 2ρφLgRg*(z)(F) + 2ρφLFRZ(G)
+12ρφ LF BX
log 1
-2n--+ 12pφLF LG BZ
(16)
For GANs, the two players generator and discriminator are approximated by deep neural networks,
so the Rademacher Complexity is a function of the two neural networks’ parameter. Supposing
f ∈ F and g ∈ G, then the f and g can be written as the form of a composition of a sequence of
function, i.e.,
f = aH (MH (oh-i(Mh-i(…αι(Mι(∙))…)))),
g = bH (Nh0 (bH0-ι(NH0-ι(…bι(Nι(∙))…)))),
(17)
where ai and bi are activation functions, Mi and Ni are matrices. Assume that the Lipschitz con-
stants of ai and bi are less than 1. This is true for many popular activation functions such as ReLU.
We also assume ||Mi|| ≤ Bi and ||Ni|| ≤ Bi0 . Let df and dg denote the widths of these two
networks respectively.
Lemma 4.1. For the empirical Rademacher Complexity given above,
12Bx QH=ι Bi Jdf Hlog(2pfnHBχ 口3 Bi)
√n
12Bz QH0ι BiqdgH"2PdgmH0Bz QH； BD
√m
元	< 4	12Bz QH=I BiqdHlog(2pfmHBg*(Z)QH=IB
g (Z)	- m	√m
4
R X(F) ≤ n +
Rz(G) ≤ —+
m
(18)
This above theorem shows that the empirical Rademacher Complexity’s bound depends on these
two neural networks’ architectures, especially the width and the depth. When training GANs, we
generate a noise for every iteration, so we can claim that m n. Combining these two theorems,
we obtain
5
Under review as a conference paper at ICLR 2020
Theorem 4.2.
|DG- DdG| ≤
48pφBz QH=I Bi Jd22Hlog(2pdfnHBz QH=I Bi)
+12pΦLF BX ↑J 2n+o(n-2).
(19)
Based on (19), if the empirical duality gap DG(f *,g*) ≤ e, We can establish the population bound
of the F - distance such that
dF,φ(Pdata,Pg* ) ≤DG(f *,g*)
. . . -............. -, ..
≤∣DG(f*,g*)- Dd(f*,g*)∣ + DG(f*,g*)
48ρφBz QH=I Bi Jdf HlogQpfnHBz QH=I Bi)
≤	√n
+ 12pφLFBX { 2n + o(n-2) + e.
(20)
5 The Algorithm
According to the Sections 3 and 4, We knoW that the population duality gap is an upper bound of
F -distance and the gap betWeen population duality gap and empirical duality gap can be arbitrarily
small. Our theories imply that the empirical duality gap can be used as a loss function for training
GANs. Note that many classical algorithms use F -distance as the loss function. We develop a neW
algorithm using duality gap as the loss function. We focus on WGAN-GP, the loss function of Which
is
dF ,φ (Pdata,Pg* ) + λ E [(||Vxf (χ) ||2 - 1)2].	(21)
X〜Px L	」
Instead, our loss function is Written as
DG (f*,g*) + λ E [(IIVxf(X) ∣∣2 - 1)2i	(22)
X〜Px
where x = Ex +(1 — e)X, E 〜U(0,1), X 〜Pdata, X = G(z), Z 〜Pz. The details of the algorithm
is given in Algorithm 1. We call our method DG-GAN, the GAN With the duality gap.
6	Numerical Experiments
In order to test our method, We conduct experiments using the duality gap on some datasets such as
a toy dataset, MNIST, CIFAR-10, qnd so on. Then we compare our method DG-GAN with classic
GAN models such as WGAN and WGAN-GP. The experiment results show that there are significant
practical benefits to using our method over the traditional methods. There are two main benefits: (1)
DG-GAN provides a good metric suggesting the generator‘s convergence and sample’s quality. (2)
Our method using duality gap as loss function has faster rate of convergence.
We train DG-GANs on CIFAR-10, and compare our method with WGANs. Specifically, we adopt
a 4-layer CNN as the generator and a 3-layer CNN as the discriminator. In the following, λ is
10. Number of discriminator iterators per generator iterators is 5. We run 20K iterations in all
the experiments on CIFAR-10. Figure 1 shows the Wasserstein Distance on CIFAR-10 datasets
training with algorithm 1, And for quantitative assessment of our generated examples, we use
the inception score (Salimans et al. (2016)). Figure 2 shows the Inception score on CIFAR-10
datasets and Figure 3 shows the image generated after 20K iterations by the generator on CIFAR-10.
In addition to the inception scores of the two methods, we also calculate the FID (Frechet Inception
Distance) of them. For WGAN-GP, after 20K iteration’s training, the FID between generated distri-
bution and true distribution is 54.4, however for DG-GAN, it is 45.6. These observations, based on
IS and FID, show that DG-GAN can provide a better quality of generated samples.
6
Under review as a conference paper at ICLR 2020
Algorithm 1: Learning parameters for BPR
input :
sample real data X 〜Pdata；
latent variable Z 〜Pz;
a random number E 〜U [0,1];
output:
Generator parameter θ;
1 initialize the generator parameter θ and the discriminator parameter ω and Adam
parameterα = 0.0001,β1 = 0,β2 = 0.9;
2 while θ not convergence do
3 ω* = ω;
4 for t = 1, ..., ncritic do
5 for t = 1, ..., m do
6	X J gθ (Z) X J EX +(1 — E) X
L⑴ J fω (X) — fω(X)+ λ ((∣∣V^fω (X)) ∣∣2 — 1)2
7	end
8	ω J Adam (Vωm Pm=I L⑻,ω, α,β1,β2)
9 end
10	Sample a batch of latent variables {z(* i) }∖ 〜Pz
θ J Adam (Vθ* Pi=1 —fω (gθ (Z)) ,'θ,α,βι,β2);
ιι ω = ω*;
12	Sample a batch of latent variables {z(i) }∖ 〜Pz
θ J Adam (Vθ* Pm=1 — fω (gθ (Z)) ,θ,α,β1,β2);
13 for t = 1, ..., ncritic do
14	for t = 1, ..., m do
15	L⑻ J fω (X) — fω (x) + λ ((∣∣Vχfω (X)) ||2 — 1)2
16	end
17	ω J Adam (Vωm Pm=I L(i),ω, α,β1,β2)
18 end
19 end
7
Under review as a conference paper at ICLR 2020
Figure 1: Wasserstein Distance on CIFAR-10
Figure 2: Inception Score on CIFAR-10
(a) step 10000
(b) step 20000
Figure 3: result on CIFAR-10
7	Conclusion
In this paper, we introduce a new metric for GANs, which can bound the traditional metric under
several assumptions. We establish the generalization error bound of the new metric to help design
the neural networks and select the sample size. We call this new framework DG-GAN. We compare
the performance between DG-GANs and other classical GANs on benchmark datasets and DG-GAN
has demonstrated competitive performance.
There are several future research directions. The first is to extend DG-GANs to autoencoder GANs,
where we have an addition encoder network to learn the meaningful encoding. The second is to
develop a formal hypothesis testing procedure to test whether the generated sample and the observed
sample have the same distribution.
8
Under review as a conference paper at ICLR 2020
References
Martin Abadi and David G Andersen. Learning to protect communications with adversarial neural
cryptography. arXiv preprint arXiv:1610.06918, 2016.
Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv
preprint arXiv:1706.08224, 2017.
David Balduzzi, Sebastien Racaniere, James Maetens, Karl Jakob, Foerster Tuyls, and Graepel
Thore. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.
Ali Borji. Pros and cons of gan evaluation measures. arXiv preprint arXiv:1802.0344, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. CVPR, pp. 248-255, 2009.
S Feizi, C Suh, F Xia, and D Tes. Understanding gans: the lqg setting. 2018.
Ian Gemp and Sridhar Mahadeven. Global convergence to the equilibrium of gans using variational
inequalities. arXiv preprint arXiv:1808.01531, 2019.
Ian Goodfellow. Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirzal, Bing Xu, David Warde-Farley, Shejil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, pp. 723-773, (13)Mar 2012.
P Grnarova, K Y Levy, A Lucchi, T Hofmann, and A Krause. An online learning approach to
generative adversarial networks. CoRR, Vol abs/1706.03269., 2017.
Paulina Grnarova, Kfir Y Levy, Aurelien Lucchi, Nathanael Perradudin, Thomas Hofmann, and
Andreas Krause. Evaluating gans via duality gap. arXiv preprint arXiv:1811.05512, 2018.
Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu. Deligan: Genera-
tive adversarial networks for diverse and limited data. CVPR, pp. 4941-4949, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
Haoming Jiang, Zhehui Chen, Minshuo Chen, and Tuo Zhao. On computation and genralization
of generative adversarial networks under spectral control. International Conference on Learning
Representation, 2019.
Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, and Roland Memisevic. Generating images
with recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans
created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.
Heusel Martin, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in
Neural Information Processing Systems, pp. 6626-6637, 2017.
Taker Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral nornalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. NIPS
Workshop on Adversarial Training, 2017.
F Oliehoek, R Savani, J Gallego, E van der Pol, and R Gross. Beyond local nash equilibria for
adversarial networks. ArXiv e-prints, 2018.
9
Under review as a conference paper at ICLR 2020
F A Oliehoek, R Savani, J Gallego-Posada, E Van der Pol, E D De Jong, and R Gros. Gangs:
Generative adversarial network games. arXiv preprint arXiv:1712.00679., 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818-2826, 2016.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015., 2015.
Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Scholkopf. Adagan: Boosting generative models. In Advances in Neural Information Processing
Systems, pp. 5430-5439, 2017.
Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit model. arXiv
preprint arXiv:1702.08896, 2017.
Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial net-
works. arXiv preprint arXiv:1612.00991, 2016.
A The proof of theorems
A. 1 The proof of theorem 4.1
The theorem 4.1 gives the generalization error bound of the duality gap with the Rademacher Com-
plexity.
proof.
| Sup V(f, g*) - inf V(f *,g) - (sup V (f, g*) - inf V (f * ,g))∣
f∈F	g∈G	f∈F	g∈G
≤l SupV(f,g*) — SupVb(f,g*)l + | inf V(f *,g) — inf V(f *,g)∣
f∈F	f∈F	g∈G	g∈G
≤2(Sup E [φ(f (x))] - E [φ(f (x))])	(23)
f ∈F X〜Pdata	X〜Pdata
+(Sup E [φ(1-f(g*(x)))]- E [φ(1 - f(g*(x)))])
f ∈F X〜Pz	X〜Pz
+(Sup E [φ(1 - f*(g(x)))] - E [φ(1 - f *((x)))])
g∈G X〜Pz	X〜Pz
Let X = {xi,X2,…,Xi,…,Xn}, X0 = {xι,X2, ...,xi, ...Xn} and Pφ = ∣∣Φ∣∣Lip
| Sup E	[φ(f (x))] - E	[φ(f (x))]-
f ∈F X〜Pdata	X〜Pdata
- Sup E	[φ(f (x))] - E	[φ(f (x))]|
f EFX~Pdata	X 〜P0 data
≤1 sup ∣φ(f (xi)) — φ(f(xi))∣ ≤ 2pφLFBX
n f∈F	n
(24)
Using McDiarmid's inequality, with probability at least 1 一 三
Sup E	[φ(f (x))] - E	[φ(f (x))]
f ∈F X 〜Pdata	X 〜Pdata
≤ E [Sup E	[φ(f (x))] -
Pdata f ∈F X~Pdata
E
X 〜Pdata
[φ(f (x))]] + 2ρφLFBX
(25)
10
Under review as a conference paper at ICLR 2020
And use McDiarmid's inequality again, with probability at least 1 一 2
E[sup	E	[φ(f (x))] 一 E [φ(f (x))]]
f ∈F X〜Pdata	X〜Pdata
1n
≤2	E [一 sup	iφ(f(xi))]
χi~pdata,e n f ∈F i=ι
1n
≤2E[— supy^6iφ(f (xi))]+2ρφ sup |f(xi)一 f(xi)|
n f∈Fi=1	f,Xi,X0i
≤2ρφ E[- sup X Eif (xi)] + 2ρφ sup |f (xi) - f (χi)∣l
n f∈F i=1	f,Xi,X0i
log 2
2n
log 2
2n
(26)
=2ρφRbX (F) + 4ρφ LF BX
Here E = (E1 , E2, ..., En) and P (Ei = 1) = P (Ei = 一1) = 0.5
So with probability at least 1 一 δ
sup E	[φ(f (x))] 一 E	[φ(f (x))]
f ∈F X〜Pdata	X〜Pdata
≤2ρφRbX (F) + 6ρφ LF BX
log 2
2n
(27)
Similarly, with probability at least 1 一 δ
sup E [Φ(1 - f*(g(x)))] - E [Φ(1 - f*(g(x)))]
g∈G X〜Pz	X〜Pz
. ^ _ 一 一 一
≤2ρφ LFRZ (G) + 6ρφ LFLGBZ
SUP E [。(1一 f(g*(x)))] ― E [。(1一 f(g*(x)))]
f ∈F X 〜Pz	X 〜Pz
(28)
. ^ 一、 _ 一 一 一
≤2ρφ ∙ LgRζ(F) + 6ρφLFLGBZ
So, we get the next inequality with probability at least 1 一 3δ
._ _ . ^ ― ^ 一 ^ , _.
DG 一 DG| ≤ 4ρφRχ (F) + 2ρφ ∙ LgRZ (F) + 2ρφ ∙ LFRZ(G)
+ 12ρφ ∙ LfBX
log卷
2n + 12ρφ ∙ LF ∙ LGBZ
(29)
A.2 The proof of lemma 4.1
The lemma 4.1 gives the bound of the Rademacher Complexity
11
Under review as a conference paper at ICLR 2020
proof.
Ilf(χ)- f0(χ)ll∞
≤∣∣aH(Mh(aH-i(Mh-i(…a1(M1(x))…))))-0h(MH(&h-i(MH-i(∙∙∙aι(Ml(x))…))))∣∣2
≤llaH(MH(OH-1 (MH-I(...a1 (MI(X)).. J)))- aL(MH(aH-1 (MH-I (...a1 (MI(X)) .. J))) ll2
+||aH (MH0 (aH-1(MH-1(...a1(M1(x))...)))) - aH (MH0 (aH-1(MH0 -1(...a1(M10(x))...))))||2
H-1
≤∣∣Mh - MH∣∣2Bχ Y ∣∣Mi∣∣2 + ||MHI∣2∣∣aH-i(∙∙∙aι(Mι(x))∙∙∙) - aL-i(...aι(Ml(x))∙∙∙)∣∣2
i=1
≤∙∙∙
HH	H	H
≤ XBX Y IIMjl∣2∣∣Mi - M〕|2 ≤ XBX ∙ Y BiIMi- Mil∣2
i=1 j=1,j 6=i	i=1 j=1,j 6=i
(30)
For M = {M ∈ Rm×n : ∣∣M ∣∣2 ≤ Bi}, its'covering number N (M, e, ∣∣∙ ∣∣2) satisfy
N(M,e,∣H∣2 ) ≤ (1+ mW 匹 C)Bi )mn	(31)
Hence,
H
N(F ,e, I∣∙∣l∞) ≤ Y N (Mi, LBX QH——B, ∣∣∙∣∣2)	(32)
So, we have
N(F,e,∣HI∞) ≤ (1+ PHBX Qi=1 Bi )dfH	(33)
According to the relationship between Rademacher Complexity and covering number, we get
ʌ	4	12Bχ QH=ι Bi∖∕df Hlog(2pdfnHBχ QH=ι Bi)
RX(F) ≤ n + ——=	"√--------------------------
Similarly,
RR Z(G) ≤ m+
12Bz QH01 Bi JdgH0log(2pdgmH0Bz QH； Bi)
ʌ	4	12Bz QH=ι Bi，呼HlogRpfmHBz QH=ι By
Rg*(Z)(F) ≤ m +------------√m---------------
(35)
B S upplementary Experiments
B.1	Experiments on other datasets
B.1.1	Experiments on MNIST
We train the GANs using duality gap corresponding to WGAN-GP on MNIST. And compare
our method with the traditional methods WGAN-GP. Specifically, we adopt a 3-layers CNN as
the generator and a 3-layer CNN as the discriminator.In the subsection, λ is 10. Number of
discriminator iterators per generator iterators is 5. We take 100K iterations in all the experiments on
MNIST datasets.
Figure 4 shows the Wasserstein Distance on MNIST datasets and Figure 5 shows the image
generated after 100K iterations by the generator on MNIST datasets.
12
Under review as a conference paper at ICLR 2020
(a)	step 20000
Figure 4: Wasserstein Distance on MNIST
633350K64
tH5"3q 皿
3QA3"N M
X"2S5 夕
/5W5 八”
〃"心&VJ3
eysoisy⅛H
O272∕71”3
“5工77。必。
(b)	step 40000
(c)	step 60000
5**11OZW2
“判 7 W6/
b/M H/UI
iH3M mi
8"060d
blQ2SΛ∖W
47⅛O559ZU
四，"打6"
710”1 /牖4
6〃勿37M夕
Y"%*77
"75%S7<Q
(d) step 80000	(e) step 100000
Figure 5: result on MNIST datasets
B.1.2 Experiments on toy datasets
We train the the GANs using duality gap corresponding to WGAN-GP on three toy datasets with
increasing difficulty: (1) RING: a mixture of 8 Gaussians, (2) GRID: a mixture of 25 Gaussians,
(3)Swissroll. And compare our method with the traditional methods WGAN. Specifically, we adopt
a 4-layers ReLU- with 512 hidden units as the generator and a 4-layer ReLU- with 512 hidden units
as the discriminator. In the subsection, λ is 0.1. Number of discriminator iterators per generator
iterators is 5. We take 100K iterations in all the experiments on RING, 200K iterations on GRID
and 200K iterations on Swissroll.
Figure 6 shows the Wasserstein Distance on the above three toy datasets and Figure 7 shows
the image generated by the generator on the above three toy datasets. In the three figures, the yellow
points represents the true data and the green points represent the generated data.
13
Under review as a conference paper at ICLR 2020
(b) W-Distance on GRID
(c) W-Distance on Swissroll
(a) W-Distance on RING
Figure 6: W-Distance on toy datasets
(a) result on ring
(b) result on grid
Figure 7: result on toy datasets
(c) result on swissroll
B.2 Training GANs using DG corresponding to other GANs
Because for every traditional GANs which train GANs by minimizing F -distance, we can find a
duality gap corresponding to it. Thus, except the experiments in section 5, where the loss function
is the duality gap corresponding to WGAN-GP, we also take WGAN, in consideration. For WGAN,
we adopt a 4-layers CNN as the generator and a 3-layer CNN as the discriminator and the dataset is
CIFAR-10.
We take 10K iterations in the experiments on CIFAR-10 and compare their inception scores
and generated models. Figure 8 shows the inception score of WGAN and our method and Figure 9
shows their generated models:
14
Under review as a conference paper at ICLR 2020
Figure 8: Inception score on CIFAR-10
(a) WGAN’s generated images
(b) DG-GAN’s generated images
Figure 9: results on CIFAR-10
15