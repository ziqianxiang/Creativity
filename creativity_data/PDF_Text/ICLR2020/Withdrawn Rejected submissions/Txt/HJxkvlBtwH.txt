Under review as a conference paper at ICLR 2020
Certifying Neural Network Audio Classifiers
Anonymous authors
Paper under double-blind review
Ab stract
We present the first end-to-end verifier of audio classifiers. Compared to existing
methods, our approach enables analysis of both, the entire audio processing stage as
well as recurrent neural network architectures (e.g., LSTM). The audio processing
is verified using novel convex relaxations tailored to feature extraction operations
used in audio (e.g., Fast Fourier Transform) while recurrent architectures are
certified via a novel binary relaxation for the recurrent unit update. We show the
verifier scales to large networks while computing significantly tighter bounds than
existing methods for common audio classification benchmarks: on the challenging
Google Speech Commands dataset we certify 95% more inputs than the interval
approximation (only prior scalable method), for a perturbation of -90dB.
1 Introduction
Recent advances in deep learning have enabled replacement of traditional voice recognition systems
with a single neural network trained from data (Graves et al., 2013; Hannun et al., 2014; Amodei
et al., 2016). Wide adoption of these networks in consumer devices poses a threat to their safety when
exposed to a malicious adversary. Indeed, it was recently shown that an adversary can inject noise
unrecognizable to a human and force the network to misclassify (Szegedy et al., 2013; Goodfellow
et al., 2014; Zhang et al., 2017; Carlini & Wagner, 2018; Carlini et al., 2016; Qin et al., 2019;
Neekhara et al., 2019; Yang et al., 2019; Esmaeilpour et al., 2019), exposing a serious security flaw.
Ideally, when deploying an automated speech recognition system we would like to guarantee that the
system is robust against noise injected by an adversary. There has been substantial recent work on
certifying robustness of computer vision models (Katz et al., 2017; Ehlers, 2017; Bunel et al., 2018;
Ruan et al., 2018; Tjeng et al., 2019; Anderson et al., 2018; Wong et al., 2018; Dvijotham et al., 2018;
Raghunathan et al., 2018; Dvijotham et al., 2019; Weng et al., 2018; Zhang et al., 2018; Salman et al.,
2019; Gehr et al., 2018; Singh et al., 2018; 2019a; Wang et al., 2018; Singh et al., 2019b). However,
the audio domain poses unique challenges not addressed by prior certification work for vision.
Differences between audio and vision models Concretely, while an input to a vision model is a raw
image, audio models typically come with a complex preprocessing stage (that involves non-trivial
non-linear operations such as logarithm) which extracts relevant features from the signal. Additionally,
audio systems typically use recurrent architectures (Chiu et al., 2017) which computer vision verifiers
do not handle as they focus on fully-connected, convolutional and residual architectures.
This work We address both of these challenges and propose
an end-to-end verification method for neural network based
audio classifiers and an implementation of this method in
a system called DAC (Deep Audio Certifier). Our threat
model assumes an attacker can introduce a noise-based per-
turbation to the raw audio input signal. The goal then is to
certify that, for any signal that the attacker can produce, the
neural network classifies the signal to the correct label. We
perform verification of this property using the framework of
abstract interpretation (Gehr et al., 2018). At a high level,
the idea is to maintain an abstraction capturing all possible
behaviors of both the audio processing stage and the neural
network. The flow of DAC is shown in Fig. 1 where all
abstractions are dark blue shapes.
D
a
Figure 1: End-to-End Audio Certifica-
tion Flow using DAC.
S(T)[
1
Under review as a conference paper at ICLR 2020
Here, all possible signals an attacker can obtain are captured using an abstraction s(i) (a convex relax-
ation). This abstraction is then propagated through the audio processing stage (shown in green boxes).
The key components of this step are abstract transformers. For each audio processing operation (e.g.
FFT) we create an abstract transformer which receives an abstraction representing an approximation
of all possible inputs to the operation and outputs a new abstraction which approximates all possible
outputs of the operation. The result of the audio processing stage is the abstraction x(i) .
The shape x(i) is then used as input to the recurrent LSTM unit (light blue) which maintains an
abstraction of a hidden state h(i-1). LSTM consists of multiple operations and we create a custom
abstract transformer for each of those. The result of the transformers in LSTM is a new hidden state
h(i). If this was the last frame in the signal (meaning i = T), then hidden state h(T) is passed through
the fully connected layer of the neural network and, again using the abstract transformer, the final
abstract shape a is obtained at the output (at the right of Fig. 1). Finally, to certify the property we
check if each concrete output in the abstraction a classifies to the correct label (this is typically easy).
If this is true, the output of the network is correct for all inputs that the attacker can create.
Related work on RNN certification The work of (Ko et al., 2019) proposes the POPQORN verifier
for recurrent neural networks (RNN). We note that POPQORN does not handle the audio preprocess-
ing pipeline. Even though POPQORN cannot directly verify audio classifiers, their approximations
for LSTM non-linearities can be integrated in DAC. This results in ≈ 200× slowdown with small
decrease in the volume of the approximation. The massive slowdown makes their approximations un-
suitable for certifying audio classifiers. In contrast, using our custom abstract transformers for LSTM
non-linearities, DAC can precisely certify end-to-end robustness of challenging audio classifiers in
few minutes.
Our main contributions are:
1.	A novel and efficient method to certify robustness of neural network audio classifiers to
noise-based perturbations. The method is based on new abstract transformers which handle
non-linear operations used in both audio processing and recurrent architectures.
2.	An implementation of both verification and provably robust training in a system called
DAC. We evaluated DAC on common audio classification benchmarks, showing it scales to
realistic networks and is far more precise (97% to 2%) than the next best scalable method.
2	Background
We first define a threat model that we work with and then present all operations that are part of the
verification procedure, including audio processing (MFCC) and LSTM updates. We also discuss the
type of verification method we employ.
Threat model We follow the same attacker threat model as Carlini & Wagner (2018). The assumption
is that the attacker can add noise δ to the original signal s so to obtain a perturbed signal s0 = s + δ .
The measure of signal distortion are decibels (dB) defined as:
dB(s) = max20 ∙ logιo(∣Si|); dBs(δ) = dB(δ) — dB(s)
i
Note that the quieter the noise is, the smaller the decibel of perturbation dBs (δ) (it is usually a
negative value as it is quieter than the signal). We assume the attacker can generate noise δ such
that dBs(δ) < where is a constant defined by the threat model. Our goal is to verify whether the
neural network classifies s0 correctly for each small perturbation δ (as constrained above).
Mel-Frequency Cepstral Coefficients (MFCC) Though there have been number of works which
operate directly on the raw signal (Pascual et al., 2017; Sainath et al., 2015), Mel-Frequency Cepstrum
(MFC) is traditionally preferred for audio preprocessing in speech recognition systems e.g., Deep-
Speech (Hannun et al., 2014). The idea of MFC is to model non-linear human acoustic perception as
power spectrum filters based on certain frequencies, called Mel-frequencies. The final result of the
transformation is a vector of coefficients whose elements contain log-scaled values of filtered spectra,
one for every Mel-frequency. This resulting vector is a feature representation of the original signal
and can now be used in a downstream task such as audio classification.
Sahidullah & Saha (2012) presented an approach to represent MFCC computation using several matrix
operations which we integrate with our verification framework. Given T frames of audio signals of
2
Under review as a conference paper at ICLR 2020
the length N = 2k ,represented as matrix S = [s(1) …S(T )]tr ∈ RT ×N (tr for transpose), audio
preprocessing with MFCC is calculated using the following steps:
1.	Pre-emphasizing and Windowing: Y = S(IN - cpeIN+1)	H
Transform the signal with the pre-emphasizing and applying the Hamming window. Here,
IN+1 ∈ RN×N is the shifted diagonal identity matrix (Ii+,j1 = 1 if i + 1 = j, otherwise 0),
H ∈ RT×N is the Hamming window, and cpe is the pre-emphasizing constant.
2.	Power Spectrum of Fast Fourier Transform (FFT): Θ = (Y W)	(Y W)
Perform FFT on the windowed data and square it to get the real-value spectrum. We can
denote FFT on discrete domain (DFT) with the multiplication of Y and W ∈ CN×N/2.
3.	Filter Bank Log Energy: Ψ = log(ΘΛ)
Apply the Mel frequency filter bank to the power spectrum and get the log value of them.
Λ ∈ RNS×p is the filter bank given the number of filters p, and log is applied entry-wise.
4.	DCT(Discrete Cosine Transformation): X = ΨD
Perform DCT on the previous result. Again, this can be formulated using matrix multiplica-
tion. We use the resulting X = [x(1) … X(T)]tr as the input for the neural network.
Long-Short Term Memory (LSTM) LSTM architectures (Hochreiter & Schmidhuber, 1997) are a
key part in modern state-of-the-art speech recognition systems (Hannun et al., 2014; Amodei et al.,
2016). In our work, we consider the following definitions of updates in the LSTM unit.
f0(t) = [x(t),h(t-1)]Wf+bf	o(0t) = [x(t), h(t-1)]Wo + bo
婢=[X㈤,h(I)]Wi + bi	c0t) = [X⑴,h(T)] W? + bc
c(t) = σ(f0t)) © c(t-1) + σ(i0t)) © tanh(C0t))	h(t) = σ(o0t)) © tanh(c(t))
where [∙, ∙] is the horizontal concatenation two row vectors, W∙ and b∙ are kernel and bias of the cell,
respectively. At timestep t, vectors f0t), i0t), o0t), c0t) represent pre-activations of the forget, input,
and output gate, respectively, and the pre-calculation of cell state. Cell state c(t) and hidden state
h(t) computed at timestep t are propagated to the LSTM unit at the next timestep, thus allowing it to
maintain states. This recurrent architecture allows inputs with arbitrary length, making it especially
suited for audio inputs.
Robustness certification In this work, our goal will be to certify the robustness of an audio classifi-
cation pipeline (including the LSTM) to noise perturbations of the input. To build such a verifier,
we leverage the general method of abstract interpretation suggested by Cousot & Cousot (1977),
successfully employed by some of the most recent state-of-the-art verifiers of neural networks (Gehr
et al., 2018; Singh et al., 2019a). The basic idea here will be to propagate the possible perturbations
(captured by a convex region) through the operations of the entire audio pipeline and to then use the
final output region to certify the robustness property. The most challenging step of this approach is
defining efficient yet precise approximations of the non-linear operations (called abstract transformers)
used in the audio pipeline. In this work, we will introduce a number of such new abstract transformers
that handle the non-linear operations used in audio processing. Specifically, our over-approximations
will be expressed in the recent DeepPoly abstraction (Singh et al., 2019a) (a restricted form of
convex polyhedra) which aims to balance efficiency and precision. That is, the abstract transformer
of a non-linear function will take as input a convex polyhedra element expressible in DeepPoly and
output another polyhedra which over-approximates the behavior of the non-linear operation when
invoked with any concrete point inside the original polyhedra.
3	Overview of Verification Process
We now explain the workings of our verifier on a (toy) example from the point where sound enters
the processing stage to where the output of the classifier is certified as robust under any allowed
perturbation of the threat model. Our goal is to provide an intuitive understanding, formal details are
provided later.
Audio preprocessing Unlike in standard computer vision tasks where the input is fed directly to the
neural network, audio models typically first perform MFCC preprocessing to extract useful features
3
Under review as a conference paper at ICLR 2020
Figure 2: Robustness verification illustrated on a toy example. Each box shows the constraints
computed by the verifier after processing a given operation (shown on edges). Green color denotes
elements of the MFCC audio processing stage: the Fast Fourier Transform (FFT), the Filterbank
Transform (FB), and the Discrete Cosine Transform (DCT). Blue color denotes components of the
network: ReLU layer, LSTM and Fully Connected layer. Approximations computed with this work
are shown in the bottom row. The operations which the abstract transformers handle are shown next
to the downward edges. Edges without labels represent affine transforms.
from the signal. We represent all preprocessing operations as green boxes in Fig. 2. The calculation
follows steps described formally in Section 2, using the same notation for resulting vectors.
Fast Fourier transform The first operation is Fast Fourier Transform (FFT) of the pre-emphasized
input signal. It is a two-step process shown in dashed boxes in Fig. 2. We decompose it into affine
and square operations which transform signal s(t) into an intermediate representation θ(t) . Using our
novel and optimal abstract transformer for the square operation, formally defined in Section 4 and
visualized in Fig. 3b, we obtain linear bounds on θ(t).
Filterbank transform The next operation is Filterbank transform (FB) which consists of affine and
logarithm operations, shown in solid boxes in Fig. 2. Note that if the approximations from the square
transformer allows negative values, then the entire analysis will fail as the logarithm operation is
undefined for negative inputs. Our transformers are carefully designed to avoid this scenario. To
obtain bounds on the output of the logarithm we apply our novel and optimal logarithm transformer,
also formally described in Section 4 and visualized in Fig. 3a, and obtain linear upper and lower
bounds on the log energy of the filtered power spectrum, ψ(t). Logarithm operation is followed by
Discrete Cosine Transform (DCT) resulting in a vector x(t) which is then used as an input to the
fully connected layer of the neural network followed by a ReLU. Our analysis (detailed calculation in
Appendix C) produces X，)∈ [0.87, 5.56], x2t) ∈ [0.17,12.8]. Since all the values are positive, the
following ReLU has no effect on its input and thus we set x(t) = X(t). Using the back-substitution
technique we describe later, we derive x(1t) ∈ [0.87, 5.56], x(2t) ∈ [0.17, 12.83]. In the next paragraph
we describe bound propagation through LSTM in more detail.
LSTM bound propagation Here we provide a technical overview of our LSTM transformer, for-
malized in Section 5 and visualized in Fig. 4, by calculating the result of the transformation on the
first neuron of the LSTM hidden layer. We provide detailed mathematical basis of this process in
Appendix D. For our toy example, let the gates be updated as f1(t) = x(1t)/4, input gate i(1t) = 2h(2t-1),
output gate OIt) = hit-1) and cell state c(t) = hf-1) - χ2t)∕4. Also, assume the previous states
are bounded by: h(1t-1), h(2t-1), c(1t-1), c(2t-1) ∈ [0.90,1.00]. We now apply our σ(x) ∙ tanh(y) and
σ(x) ∙ y transformers to get bounds of the cell state CIt) = σ(f(t)) ∙ cf-1) + σ(if)) ∙ tanh(C，)).
0.18f(t) + 0.45 ≤ σ(f(t)超-1) ≤ 0.16f(t) + 0.58
0.03C1t) - 0.78 ≤ σ(i1t))∙ tanh(cf)) ≤ 0.39c1t) +0.28
Summing up the inequalities above results in c(1t) ∈ [-0.36, 1.46]. For the hidden state h(1t) =
σ(o1t)) ∙ tanh(c1t)), we again apply our abstract transformer and obtain
0.48c1t) - 0.08 ≤ σ(o1t)) ∙ tanh(cf)) ≤ 0.14cf) + 0.45
4
Under review as a conference paper at ICLR 2020
Figure 3: Our DeepPoly approximations of (a) the natural logarithm and (b) the square function. Blue
lines represent the valid (in case of square, non-negative) upper and lower bounds which minimize the
area between the planes under the given domain [lx , ux]. The red bound on (b) grants smaller area,
but contains the negative value, which occurs the analysis failure in the audio processing pipeline.
i	.e., h(1t) ∈ [-0.25, 0.65]. An analogous computation is performed for h(2t). For this example, we
assume the computation results in h(2t) = 0.
Robustness certification using DeepPoly The hidden states in the LSTM at the final timestep are
passed through a fully connected layer without activation. In our example we assume that logits after
the layer are obtained with the following formula: l1 = h(1t) + 0.24 and l2 = h(2t). These are shown
in the final box of Fig. 2. To certify that the neural network classifies the input to class 1, we need to
prove that l1 - l2 > 0. We now apply the back-substitution technique as in Singh et al. (2019a) upto
the input to the LSTM:
l1	- l2 = hit) - h2t) + 0.24 ≥ 0.48(0.18f(t) + 0.03c1t) - 0.33) - 0.08 + 0.24
=0.09f(t) + 0.01 胃) + 0.01 = 0.09(x1t)∕4) + 0.01(h1tτ) - x2t)∕4) + 0.01 ≥ 0.006
As this value is greater than 0, robustness is established. The process above, of replacing a variable
with its constraints, is called back-substitution. Here, we replaced l1 with h(1t) + 0.24, then h(1t) with
0.18f(t) + 0.03c1t) - 0.33 and So on. In Appendix C We show more detailed calculations to obtain
tighter bound 0.0375 by also back-substituting in the preprocessing pipeline. For our experiments in
Section 6, we tune the number of back-substitution steps to achieve a good tradeoff between speed
and precision. Note that robustness cannot be proved if one concretizes the expression above to an
interval, instead of performing back-substitution of linear bounds. For instance, if we concretize h(1t)
to [-0.25, 0.65], we obtain l1 - l2 ≥ h(1t) - h(2t) + 0.24 ≥ -0.01 which is imprecise and fails to
certify robustness. Note that if t is not the last timestep, the hidden state and the cell state are passed
to the next analysis timestep instead of computing the final output.
4	Audio Processing Transformers
As illustrated earlier, the first part of the verification process involves handling of the audio processing
stage (performed using MFCC). Here, most matrix multiplication parts can be captured exactly by
our abstraction, but MFCC also includes the square operation in the FFT and the logarithm operation
in the computation of energy from filterbanks. Thus, to handle these non-linear operations, we need
to create new abstract transformers, which we present next. To ensure a minimal loss of precision,
our abstract transformers minimize the area between the lower and upper bounds in the input-output
plane. This approach of minimizing the area has been used before for other transformers and has
been shown to be practically effective in Singh et al. (2018; 2019a).
We denote the set of all variables in the analysis as X. For an element x ∈ X, we denote the
functions corresponding to its linear lower and upper bound as xl and xu , respectively. These are
scalar functions defined on Rk where k is the number of other variables used to compute x. For every
element, we also maintain interval bounds, x ∈ [lx, ux]. For ease of explanation, we introduce the
log transformer followed by the square transformer.
5
Under review as a conference paper at ICLR 2020
Log abstract transformer The logarithm operation is an assignment y := log(x) where x, y ∈ X .
The output of this operation cannot be captured exactly and we need to carefully introduce an
approximation. Here, we first compute the minimum ly = log(lx ) and the maximum uy = log(ux),
which we use as interval approximation. We define linear lower and upper bound functions yl , yu :
R → R. Using concavity of the logarithm on any subset of the domain, the lower bound function is a
line connecting the points (lx , log(lx)) and (ux , log(ux)). The upper bound function is chosen as the
tangent line to the function minimizing the area between the lower and the upper bound. As a result,
we obtain the following bounds as depicted in Fig. 3a:
x - lx	ux	2x	ux + lx
yl(x)= logIx + —— log —	yu(x) = ――― - 1 + log—2—
ux-	lx	lx	ux + lx	2
Note that if lx ≤ 0, yl (x) =
exception when ux - lx < 10
-4
calculation caused by large denominator of x coefficient.
∞ since the function is not defined on that domain. We make an
to use interval bound for yl for avoiding the unstable floating point
Square abstract transformer The square operation is an assignment y := x2 where x, y ∈ X.
Similar to logarithm, this operation is non-linear and cannot be captured exactly in the abstraction.
We first compute the interval bounds of the output ly and uy , and set the minimum value ly to 0 when
0 ∈ [lx, ux] and min(lx2, u2x) otherwise. The maximum value uy is simply max(lx2, u2x).
Next, we define linear lower and upper bound functions yl , yu : R → R. Using the convexity of
the square function, we set the upper bound function yu to the linear function connecting (lx , lx2 )
and (ux, u2x). For the lower bound function yl, we have to be delicate: since x2 : R → R≥0, our yl
should also be greater or equal than 0 within any domain. With the principle of minimizing the area
between the bounds, we obtain the following bounds for cases as shown in Fig. 3b:
yl(x)
4lxx - 4lx2
4uxx - 4u2x
0
(ux + lx)x - ((ux + lx)/2)
0≤lx < ux/3
0 ≥ 3ux > lx
lx ≤ 0 ≤ ux
otherwise
y (x)	= (ux + lx)x - uxlx
5	LSTM Transformers
Most prior work on verification of neural networks focuses on feed-forward or convolutional ar-
chitectures whose operations consist of a sequence of affine transforms and activation functions.
However, to perform verification of recurrent architectures, one needs to handle the updates of the
recurrent unit. Following the equations updating the LSTM presented in Section 2, we can observe
that pre-activations of gates are affine transforms which can be captured exactly in our abstraction.
However, the operations updating the cell and hidden states are non-linear and require approximation.
Overall, We have three elementwise products - two are between a sigmoid and a tanh and one
between a sigmoid and the identity. A straightforward approach to handling such transforms would
be to concretize the polyhedra DeepPoly element to an interval and perform the multiplication using
intervals. However, this approach would lose all relations between inputs and incur precision loss
in future operations of the analysis. Instead, we design custom binary approximation transformers
specifically tailored to handle the elementwise product in the update of the recurrent unit.
Sigmoid Tanh abstract transformer We define elementwise multiplication between sigmoid and
tanh as an assignment z := σ(x) tanh(y), where x, y, z ∈ X. As before, our aim is to construct
linear bounds zl , zu . Unlike the previously defined DeepPoly transformers which take as input a
single abstract element, this transformer is the first which receives two abstract elements as operands.
Hence, the bound functions have the form zl , zu : R2 → R.
Let Z = f (x, y) = σ(x) ∙ tanh(y). Our goal is to bound this function between two planes such
that the volume between the planes is as small as possible (following our previous heuristic). We
first divide the computation into 3 cases based on the signs of ly and uy . To simplify notation, we
introduce the variable assignment in Table 1 so that we can reuse notation across all cases. We
first define an anchor point a: a point in the box [lx , ux] × [ly, uy] where function f attains the
max/min value defined upon the case. Also we need reference point r whereas the plane would meet
6
Under review as a conference paper at ICLR 2020
		Zu	zyu	zxl	zyl
ly ≥ 0	Φ	min	min	min	min
	a	ux, uy	Ux , uy	lx, ly	lx, ly
	r	lx, uy	ux, ly	Ux , ly	lx , Uy
ly < 0 ≤ uy	Φ	min	min	max	min
	a	ux, uy	Ux , uy	Ux , ly	Ux , ly
	r	lx, uy	lx, ly	lx, ly	lx , Uy
uy < 0	Φ	max	min	max	min
	a	lx, uy	lx , Uy	Ux , ly	Ux , ly
	r	ux, uy	lx, ly	lx, ly	Uy, Uy
Table 1: Variable reference for the σ(x) ∙ tanh(y)
approximation.
y
Figure 4: Visualization of the bounding
planes and σ(x) ∙ tanh(y) curve.
first among the four corners if it is tilted in given direction, other than the anchor. Our transformer
computes two candidates planes for the upper bound (zxu , zyu) and two for the lower bound (zxl , zyl ) as:
(ZTzl)X = Φ (Difg) Ka- f(r)) (X - ax) + f (a)
ax - rx
(zu∣zl)y = Φ (Dj f (a), f，-f C ) (y — ay) + f (a)
ay - ry
where Dif = dx, Djf = *, and Φ, a, r are chosen from Table 1. Finally, We choose bounds
from two candidates which minimize the volume between the planes. Fig. 4 is the visualization of
this result. We note that Sigmoid Identity transformer is handled in the same manner (we can
directly apply the same transformer by replacing f (x, y) with σ(x) ∙ y).
Theorem 1. Our Sigmoid Tanh transformer is optimal (it minimizes the volume between the
lower and upper plane) under the assumptions: (1) bounding planes are tangent to the curve at the
respective anchor points, (2) bounding planes are parallel to either x or y axis. Planes computed
using our transformer result in a volume strictly smaller than those computed using interval analysis.
We show the proof in Appendix B. Our assumptions are based on the following reasoning. The first
assumption is need so that our transformer produces bounds strictly better than interval analysis.
Unless zu passes through the point (uχ, Uy), the concrete upper bound would be larger than σ(uχ) ∙
tanh(uy), making it worse than an interval bound(analogously for the lower bound). The second
assumption enables computation of optimal planes in O(1) while without this assumption one needs
to solve a non-convex optimization problem which is not feasible at the scale of networks we consider.
6	Experiments
We now evaluate the effectiveness of DAC on several datasets and neural architectures. All our
transformers are implemented in C (for performance) and exposed to the verifier using a Python
interface. We will publicly release the datasets, trained networks and source code of DAC. Verification
is performed on a Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz using 6 cores for each instance.
Experimental setup We evaluate on audio classification benchmarks: Free Spoken Digit Dataset
(FSDD) (Jackson, 2019) and Google Speech Commands (GSC) (Warden, 2018). FSDD contains
2,000 samples of spoken digits while GSC is a more complex dataset containing 65,000 utterances of
30 short words spoken by thousands of different people. State-of-the-art accuracy on GSC is 96.9%
by de Andrade et al. (2018) using attention RNN, while our verified model achieves 89%.
Robustness evaluation Following Singh et al. (2019b), we define success rate of verification as the
ratio between certified samples and correctly predicted ones. We randomly shuffled the test data and
then, for every experiment, inferred labels one by one until the number of correctly classified samples
reached 100. We report the number of provably correct samples out of these 100 as our provability.
As a baseline, we consider a verification method based on interval analysis, which only derives
neuronwise lower and upper bounds using the previous layer’s bounds. We used networks with
7
Under review as a conference paper at ICLR 2020
(a) undefended FSDD, 98%
(b) defended FSDD, 95%
(c) undefended GSC, 89%.
Figure 5: Robustness evaluation on FSDD and GSC datasets, using DAC and Interval analysis.
h = 40 hidden neurons and p = 10 filters for both FSDD and GSC. Our results are shown in Fig. 5.
They suggest that DAC substantially outperforms interval analysis due to better approximations of
key functions in both audio preprocessing and neural network stages.
Effect of back-substitution depth In this experiment, we
vary the depth of back-substitution used in DAC and study
its effect on our performance. We run this experiment on
an FSDD network and report the results in Table 2. We
observe that increasing the back-substitution depth increases
the provability. This is due to the fact that we benefit from
cancellation of common terms in the propagated expressions,
as demonstrated in our example in Section 3. However, this
comes at a cost of exponential increase in runtime. Thus, we
choose to use depth 3 in order to get high verification rate
while still having reasonable verification speed.
Depth	Provability(%)	Run Time(s)
0	27	8.33
1	35	18.95
2	37	40.83
3	39	87.92
4	40	166.19
5	41	328.92
Table 2: Effect of back-substitution
depth on the performance.
Provable defense for audio classifiers We also for the first
time trained audio classifiers to be provably robust against noise-based perturbations. Our training
follows Mirman et al. (2018); Gowal et al. (2018) - We perturb the input signal and propagate interval
bounds through the audio processing and LSTM stages. To train, we combine standard loss with the
worst case loss obtained using interval propagation. The resulting network shown in Fig. 5b achieves
80% of provability for -80 dB even with the imprecise intervals, outperforming the undefended
network. Even though this network was specifically trained to be verifiable using intervals, DAC still
outperforms intervals and proves significantly more robustness properties. Also note that defended
network has lower accuracy of 95%, compared to the 98% accuracy of the baseline.
Experimental comparison with prior work POPQORN (Ko et al., 2019) also proposes a method
to certify recurrent neural networks by propagating linear bounds. One of the key differences with
our work is the approximation of σ(x) ∙ tanh(y) using linear bounds. We found that, in practice,
optimization approach used by POPQORN produces approximations of slightly smaller volume than
our LSTM transformer (although non-comparable). However, this smaller volume comes at a high
cost in runtime. We tried to integrate POPQORN bounds into our verification framework, however
we found it not feasible for our audio tasks as it increased our end-to-end runtime 200× on one of the
smaller networks. In contrast, DAC takes only 1-2 minutes on average to verify a single example
containing multiple frames. Indeed, our observation is consistent with the evaluation of POPQORN
in the paper where it takes hours on a GPU for verifying NLP and vision tasks only for a single frame.
7 Conclusion
We presented the first verifier for certifying audio classifiers. The key idea was to create abstract
transformers for non-linear operations used in the audio processing stage and the recurrent network.
These transformers compute an optimal (area-wise) approximation under assumptions representable
in the underlying convex relaxation and enable sound handling of the entire pipeline. Our evaluation
shows that DAC is practically effective and achieves high verification rates on different datasets.
8
Under review as a conference paper at ICLR 2020
References
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl
Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-
end speech recognition in english and mandarin. In International conference on machine learning,
pp.173-182, 2016.
Ross Anderson, Joey Huchette, Will Ma, Christian Tjand raatmadja, and Juan Pablo Vielma.
Strong mixed-integer programming formulations for trained neural networks. arXiv e-prints,
pp. arXiv:1811.01988, 2018.
Rudy Bunel, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, and M. Pawan Kumar. A unified
view of piecewise linear neural network verification. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), pp. 4795-4804, 2018.
Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text.
In 2018 IEEE Security and Privacy Workshops (SPW), pp. 1-7. IEEE, 2018.
Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David
Wagner, and Wenchao Zhou. Hidden voice commands. In 25th {USENIX} Security Symposium
({USENIX} Security 16), pp. 513-530, 2016.
Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng
Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly, Bo Li, Jan
Chorowski, and Michiel Bacchiani. State-of-the-art speech recognition with sequence-to-sequence
models. CoRR, abs/1712.01769, 2017. URL http://arxiv.org/abs/1712.01769.
Patrick Cousot and Radhia Cousot. Abstract interpretation: a unified lattice model for static analysis
of programs by construction or approximation of fixpoints. In Proceedings of the 4th ACM
SIGACT-SIGPLAN symposium on Principles of programming languages, pp. 238-252. ACM,
1977.
Douglas Coimbra de Andrade, Sabato Leo, Martin Loesener Da Silva Viana, and Christoph Bernkopf.
A neural attention model for speech command recognition. arXiv preprint arXiv:1808.08929,
2018.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli.
A dual approach to scalable verification of deep networks. In Proc. Uncertainty in Artificial
Intelligence (UAI), pp. 162-171, 2018.
Krishnamurthy (Dj) Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Push-
meet Kohli. Efficient neural network verification with exactness characterization. In Proc. Uncer-
tainty in Artificial Intelligence, UAI, pp. 164, 2019.
RUdiger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Automated
Technology for Verification and Analysis (ATVA), 2017.
Mohammad Esmaeilpour, Patrick Cardinal, and Alessandro Lameiras Koerich. A robust approach for
securing audio classification against adversarial attacks. arXiv preprint arXiv:1904.10990, 2019.
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In
2018 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation
for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing,
pp. 6645-6649. IEEE, 2013.
9
Under review as a conference paper at ICLR 2020
Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end
speech recognition. arXiv preprint arXiv:1412.5567, 2014.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.
Zohar Jackson. Free sPoken digit dataset. https://github.com/Jakobovski/
free- spoken- digit- dataset, 2019.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. ReluPlex: An efficient
smt solver for verifying deeP neural networks. In International Conference on Computer Aided
Verification, PP. 97-117. SPringer, 2017.
CY Ko, Z Lyu, TW Weng, L Daniel, N Wong, and D Lin. PoPqorn: Certifying robustness of recurrent
neural networks. In International Conference on Machine Learning (ICML), 2019.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interPretation for Provably
robust neural networks. In International Conference on Machine Learning, PP. 3575-3583, 2018.
Paarth Neekhara, Shehzeen Hussain, Prakhar Pandey, Shlomo Dubnov, Julian McAuley, and Farinaz
Koushanfar. Universal adversarial Perturbations for sPeech recognition systems. arXiv preprint
arXiv:1905.03828, 2019.
Santiago Pascual, Antonio Bonafonte, and Joan Serra. Segan: SPeech enhancement generative
adversarial network. arXiv preprint arXiv:1703.09452, 2017.
Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, and Colin Raffel. ImPercePtible, robust,
and targeted adversarial examPles for automatic sPeech recognition. In International Conference
on Machine Learning (ICML), 2019.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examPles. In Advances in Neural Information Processing Systems
(NeurIPS), PP. 10877-10887. 2018.
Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. Reachability analysis of deeP neural networks
with Provable guarantees. In Proc. International Joint Conference on Artificial Intelligence, IJCAI,
PP. 2651-2659, 2018.
Md Sahidullah and Goutam Saha. Design, analysis and exPerimental evaluation of block based
transformation in mfcc comPutation for sPeaker recognition. Speech Communication, 54(4):
543-565, 2012.
Tara N Sainath, Ron J Weiss, Andrew Senior, Kevin W Wilson, and Oriol Vinyals. Learning the
sPeech front-end with raw waveform cldnns. In Sixteenth Annual Conference of the International
Speech Communication Association, 2015.
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation
barrier to tight robustness verification of neural networks. CoRR, abs/1902.08722, 2019.
GagandeeP Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, PP.
10802-10813, 2018.
GagandeeP Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for
certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):41,
2019a.
GagandeeP Singh, Timon Gehr, Markus Puschel, and Martin Vechev. Boosting robustness certification
of neural networks. In International Conference on Learning Representations, 2019b.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing ProPerties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
10
Under review as a conference paper at ICLR 2020
Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations, (ICLR), 2019.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety
analysis of neural networks. In Advances in Neural Information Processing Systems, pp. 6367-
6377, 2018.
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint
arXiv:1804.03209, 2018.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and
Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In Proc.
International Conference on Machine Learning (ICML), volume 80, pp. 5276-5285, 2018.
Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In Proc. Neural Information Processing Systems (NeurIPS), pp. 8410-8419, 2018.
Zhuolin Yang, Bo Li, Pin-Yu Chen, and Dawn Song. Characterizing audio adversarial examples
using temporal dependency. 2019.
Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and Wenyuan Xu. Dolphi-
nattack: Inaudible voice commands. In Proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security, pp. 103-117. ACM, 2017.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In Advances in Neural Information
Processing Systems, pp. 4939-4948, 2018.
11
Under review as a conference paper at ICLR 2020
A SOUNDNESS PROOF OF SIGMOID TANH AB STRACT TRANSFORMER
Suppose x and y are the abstract variable bounded by [lx , ux], [ly , uy] respectively. This suggested
transformer finds the bounding planes of the function f (x, y) = σ(x) tanh(y). Let Dif = df and
Dj f=∂y.
1.	ly ≥ 0
Choose the upper bound of the function between zxu and zyu , where
Zu(x, y) = min (Dif(Ux, Uy), f(ux,uy) _ f(lx,uy)) (X — Ux) — f (ux, Uy)
ux - lx
zu(x,y) = min (Djf(u,Uy), f(UxUy _ f (UX,ly)) (y — Uy) — f(uχ,Uy)
Uy — ly
by the smaller volume under the each plane.
Claim 1: zxu(x,y) ≥ f(x, y) in (x, y) ∈ [lx,Ux] × [ly,Uy].
Let x1 < x2 ∈ [lx, Ux] and y1 < y2 ∈ [ly, Uy]. Then for any x, y, f(x, y1) < f(x, y2)
and f(x1, y) < f(x2, y). Thus, since zxu is independent to y, it is sufficient to show
Zu(X, ∙) ≥ f (X,Uy).
We can easily know that f(x, Uy) is concave at x ≥ 0 and convex at x ≤ 0 by the second
derivation of f .
(a)	Consider the case of Ux > 0. Let X0 be the x coordinate of the crossing of f(X, Uy) and
g(X) = Dif(Ux, Uy)(X — Ux) — f(Ux, Uy). From the convexity of f(X, Uy) in X < 0,
g(0) > f(0, Uy), and g0(X) > 0, there exists a single X0 < 0. If lx > X0, the slope
of zu(χ, ∙) becomes Dif (ux, Uy), and zu and g become identical. Hence, f (x, Uy) ≤
g(χ) = zu(χ, ∙). If lχ ≤ X0, the slope of zu(χ, ∙) becomes f(ux,uy)-f(lx,uy). Then
zu and g shares a point of UX but zu has smaller slope, so g(χ) ≤ zu(χ, ∙) in [lχ, Uχ].
Also from f (lχ,Uy) = Zu(IX, ∙) and f (0,Uy) < z^(0, ∙), f (x,Uy) ≤ zu(x, ∙). Thus,
the claim holds when Uχ > 0.
(b)	Another case of UX ≤ 0,x coefficient of ZX will always be "ux,?)-1"x,uy). Again,
by convexity of f (x, Uy), zX(x, ∙) ≥ f (x, Uy).
Hence zu(x, ∙) ≥ f(x,Uy) ≥ f (x,y) holds in (x,y) ∈ [lχ,Uχ] X [ly,Uy]. Claim proved.
With analogous steps, zyu can be shown to cover above the curve. Choosing the plane with
smaller volume underneath it allows to minimize the expected difference between the true
curve and the upper bound plane under the randomly chosen domain.
For the lower bound, choose the upper bound of the function between zχl and zyl, where
ZX (X, y) = min	(Dif(IX,	ly), f(lχ,ly] -U(UX,ly))	(x - lχ) -	f (lχ,	Iy)
Zy (x, y) = min	(Dj f (lχ,	ly), f (IX,ly )-U(lχ,uy))	(y - ly) -	f (lχ,	ly)
by the larger volume under the each plane.
Claim 2: Zχl (X, y) ≤ f(X, y) in (X, y) ∈ [lχ, Uχ] × [ly,Uy].
It is sufficient to show ZX(x, ∙) ≥ f (x,ly) since f (x, yι) ≤ f (x, y2) for yι ≤ y2.
(a)	Consider the case of lχ < 0. Let X0 be the x coordinate of the crossing of f(X, ly)
and g(X) = Dif(lχ, ly)(X - lχ) - f(lχ, ly). From the concavity of f(X, ly) in X > 0,
g(0) < f(0, ly), and g0(X) > 0, there exists a single X0 > 0. If lχ < X0, the slope
of ZX(x, ∙) becomes Dif (lχ,ly), and ZX and g become identical. Hence, f (x,ly) ≤
g(χ) = ZX(x, ∙). If lχ ≥ X0, the slope of ZX(x, ∙) becomes f(lx,ly)-u(ux,ly). Then
ZX and g shares a point of lχ but ZX has smaller slope, so g(χ) ≥ ZX(x, ∙) in [lχ, Uχ].
Also from f(Uχ, ly) = ZX(uχ, ∙) and f (0, Iy) > ZX(0, ∙), f(x, Iy) ≥ ZX(x, ∙). Thus,
the claim holds when Uχ < 0.
12
Under review as a conference paper at ICLR 2020
(b)	Another case of lχ ≥ 0,x coefficient of ZIx will always be f"x'lj)-f(ux'y). Again, by
convexity of f (x,ly), Zx(x, ∙) ≤ f (x,ly).
Hence Zx(x, ∙) ≤ f(χ,ly) ≥ f(χ,y) holds in (χ,y) ∈ [lx,ux] X [ly,uy]. Claim proved.
With analogous steps, Zyl can be shown to lie under the curve. Choosing the plane with
larger volume underneath it allows to minimize the expected difference between the true
curve and the lower bound plane under the randomly chosen domain.
2.	ly ≤ 0 ≤ uy
The proof of upper bounds will follow the same steps with the first case. zxu in this case
is exactly same as before, but since f(x, y) goes below 0 when y < 0, zyu has to anchor at
(lx, ly) instead of (ux, ly) since f(lx, ly) ≥ f(ux, ly) and convexity of f in the region. The
proof steps do not differ much from the previous proofs.
Again, the proof for lower bound is similar as before, but note that zxl needs to choose
maximum between the two slopes. This is due to the sign of the values. Since f (ux , ly) < 0
is the minimum in the region and it grows along x gets smaller, both Dif(ux, ly) and
f(ux"y)-f(lχ"y) are 院§ 山㊀口 zero.
ux -lx
3.	0 ≥ uy
We will not provide the proof since this case is symmetric to the first case.
B Proof of Theorem 1
Theorem 1. (copied) Our Sigmoid Tanh transformer is optimal (it minimizes the volume between
the lower and upper plane) under the assumptions: (1) bounding planes are tangent to the curve at
the respective anchor points, (2) bounding planes are parallel to either x or y axis. Planes computed
using our transformer result in a volume strictly smaller than those computed using interval analysis.
Proof. First note that Zxu, Zyu, Zxl, Zyl are sound bounds by Appendix A. Assume Zxu is chosen as the
upper bound. W.l.o.g., suppose there exists another sound candidate upper bound ζxu, satisfying (1)
and (2), with smaller volume underneath it than Zxu. Then ζxu would have larger absolute slope on
x than Zxu. However, the slope of Zxu is chosen as stiff as possible while remaining the soundness
(Appendix A to see in detail). Hence, ζxu would not be sound anymore with larger absolute coefficient.
Contradict to the existence of sound ZU. Other cases can be proved with similar logic as well. □
C Verification by Example
We assume the last sound frame s(t) consists of two elements s(1t) and s(2t) such that: s(1t) ∈ [0, 1] and
s(2t) ∈ [-2, -1]. These constraints capture a noise perturbation of the sound and are depicted in the
white box in the left part of Fig. 2. We describe analysis only at the last timestep t (the same process
is repeated for every timestep).
We note that the DeepPoly abstraction (which we build on in this work) maintains four constraints
for every element: a lower and upper constraints which are linear in terms of the previous elements
as well as two interval constraints. This abstraction is exact for affine transformations, however, to
handle non-linear operations, one has to create new abstract transformers. In our work we introduce
such transformers and formally describe their operation in the next sections. In what follows, we
show their effect on our running example. For the presentation below, our figure shows the linear
constraints obtained by the verifier, but to avoid visual clutter, we do not show the two interval
constraints (however, we do list them in the text).
Fast Fourier Transform The first operation in the audio processing stage is the Fast Fourier
Transform (FFT) of the pre-emphasized input signal. It is a two-step process shown in dashed
boxes in Fig. 2. The preemphasis is in fact an affine transform so we perform it jointly with
the affine transform in the FFT. As the composition of two affine transforms is again affine, this
13
Under review as a conference paper at ICLR 2020
amounts to a single affine transform on the input. In our example, the composed affine transform is
y1(t) = s(1t) - s(2t) and y2(t) = 2s(2t). Affine transforms can be captured exactly and here we obtain e.g.,
s(1t) - s(2t) ≤ y1(t) ≤ s(1t) - s(2t). We also obtain the interval bounds y1(t) ∈ [1, 3] and y2(t) ∈ [-4, -2].
The next step in the computation of FFT is elementwise square of y(t) . We denote this operation as
θ1(t) = (y1(t))2 and θ2(t) = (y2(t))2. The square operation is non-linear and cannot be captured exactly
which means that we need to decide how to lose precision. Here, we apply our new square abstract
transformer (formally defined in Section 4) which provides an optimal linear lower and upper bounds
of the square function in terms of area. After applying this transformer, we obtain the bounds:
4y1(t) - 4 ≤ θ1(t) ≤ 4y1(t) - 3	- 6y2(t) - 9 ≤ θ2(t) ≤ -6y2(t) - 8
The interval bounds are calculated by:
θ1(t) ≥ 4y1(t) - 4 = 4(s(1t) - s(2t)) -4 ≥ 4(0 - (-1)) - 4 = 0
θ1(t) ≤ 4y1(t) - 3 = 4(s(1t) - s(2t) ) - 3 ≤ 4(1 - (-2)) - 3 = 9
θ(t) ≥ -6y2t) - 9 = -6(2s2t)) - 9 ≥ -6(2 ∙ (-1)) - 9 = 3
θ(t) ≤ -6y2t) - 8 = -6(2s2t)) - 8 ≤ -6(2 ∙ (-2)) - 8=16
Hence, θ1(t) ∈ [0, 9] and θ2(t) ∈ [3, 16]. Note those bounds are not exact but sound; even y1(t) ∈ [1, 3]
and y2(t) ∈ [-4, -2], DAC calculates the concrete lower and upper bound with the expression to be
consistent with other transformers, so the bounds might not be exact as [1, 9] and [4, 16]. In practice,
FFT is followed by another affine transform which adds together the real and complex component of
each element, but we omit this for clarity as it is captured as before, without loss of precision.
Filterbanks Transform Our analysis continues with the computation of filter banks of the input,
(t)
shown in solid boxes in Fig. 2. The first step is an affine transform and in our example we use: ψ1 =
2θ(t) + θ2t) and 尉t = θ(t) + 3θ(t). Our abstraction is again exact here and additionally computes
interval bounds ψ5(t) ∈ [3,34] and ψ^t) ∈ [9, 57]. The final step in this transform is the elementwise
(t)	(t)	(t)	(t)
logarithm of the input. We denote the operation as ψ1 = log(ψ1 ) and ψ2 = log(ψ2 ). As this is
again a non-linear operation, we apply our new log transformer so to obtain:
0.0783ψ(t) + 0.8637 ≤ ψ(t) ≤ 0.054lψ(t) + 1.9178
0.0384ψ(t) + 1.8511 ≤ ≠2t) ≤ 0.0303ψ2t) + 2.4965
The interval bounds are calculated by:
ψ(t) ≥ 0.0783ψ(t) + 0.8637 = 0.0783(2θ(t) + θ2t)) + 0.8637
≥ 0.0783(2(4y1(t) - 4) + (-6y2(t) - 9)) + 0.8637
= 0.0783(2(4(s(1t) - s(2t)) - 4) + (-6(2s(2t)) - 9) + 0.8637
= 0.6264s(1t) - 1.566s(2t) - 0.4674 ≥ 1.0986
ψ(t) ≤ 0.0541ψ(t) + 1.9178 = 0.0541(2θ(t) + θgt)) + 1.9178
≤ 0.0541(2(4y1(t) - 3) + (-6y2(t) - 8)) + 1.9178
= 0.0541(2(4(s(1t) - s(2t)) - 3) + (-6(2s(2t)) - 8)) + 1.9178
= 0.4328s(1t) - 1.082s(2t) + 1.1604 ≤ 3.7572
14
Under review as a conference paper at ICLR 2020
ψ(t) ≥ 0.0384ψ2t) + 1.8511 = 0.0384(θ(t) + 3θgt)) + 1.8511
≥ 0.0384((4y1(t) -4) + 3(-6y2(t) - 9)) + 1.8511
= 0.0384((4(s(1t) - s(2t)) - 4) + 3(-6(2s(2t)) - 9)) + 1.8511
= 0.1536s(1t) - 1.536s(2t) + 0.6607 ≥ 2.1967
ψ(t) ≤ 0.0303ψ2t) + 2.4965 = 0.0303(θ(t) + 3θgt)) + 2.4965
≤ 0.0303((4y1(t) - 3) + 3(-6y2(t) - 8)) + 2.4965
= 0.0303((4(s(1t) - s(2t)) - 3) + 3(-6(2s(2t)) - 8)) + 2.4965
= 0.1212s(1t) - 1.212s(2t) + 1.6784 ≤ 4.2236
In other words, ψ1(t) ∈ [1.0986, 3.7572] and ψ2(t) ∈ [2.1967, 4.2236]. Again, these bounds are sound
since [log 3, log 34] ⊂ [1.0986, 3.7572], [log 9, log 57] ⊂ [2.1967, 4.2236].
Discrete Cosine Transform and ReLU After the Filterbanks Transform, the input is passed
through the Discrete Cosine Transform (DCT) followed by a Lifting operation. The analysis result
of these steps is then provided as an input to a fully connected (FC) layer followed by a ReLU
activation. To ease presentation, in our example we combine DCT, Lifting and FC layer in a single
affine transform: X，= ψ(t) 一 ψ(t) + 4 and xgt) = 4ψ(t) 一 ψ(t). We ShoW this transform in a
dotted box in Fig. 2. This is again captured exactly in our abstraction along with interval bounds
x1t) ∈ [0.875, 5.5605], X^ ∈ [0.1708,12.8321]. The affine transform is followed by a ReLU
activation XIt) = max(0, x1t)), x2t) = max(0,x2t)). In general, we use the ReLU transformer
defined in Singh et al. (2019a), which for this example produces: XIt) = XIt) and χ2t) = X；”.
LSTM analysis After the verifier completes the audio processing stage, its output is passed as
input to the LSTM cell. This cell also receives a hidden state and a cell state from the previ-
ous timestep (shown as a blue box in Fig. 2). In our example, we assume these are given as:
h(1t-1), h(2t-1), c(1t-1), c(2t-1) ∈ [0.9, 1]. We note that these elements usually have different interval
bounds, but for simplicity in our example we use the same intervals. We focus on the first neuron in
the LSTM and compute pre-activations in our example for the forget gate f1(t) = X(1t)/4, input gate
if) = 2h(2-1'), output gate of) = hf-1) and cell state c(t) = hf-1) - X；t)/4.
In order to update the cell state, we need to compute: cf) = σ(f(t)) ∙ c，-1) + σ(if)) ∙ tanh(cf)).
The left and right summands are computed using our new binary abstract transformers for σ(∕) ∙ y
and σ(X) ∙ tanh(y) expressions, respectively. Applying our abstract transformers on the summands
produces the following bounds:
0.1891f(t) + 0.4576 ≤ σ(f(t)) ∙ cf-1) ≤ 0.1596f(t) + 0.5787
0.0341c1t) — 0.7847 ≤ σ(if)) ∙ tanh(cf)) ≤ 0.3945c1t) + 0.2769
Summing up the above inequalities grants the upper and lower bounds of c(1t) in terms of x(t), h(t-1),
and c(t-1):
c1t) ≥ 0.1891f(t) + 0.0341C1t) — 0.3271
= 0.1891(X(1t)/4) + 0.0341(h(1t-1) 一 X(2t)/4) 一 0.3271
≥ 一0.3644
c1t) ≤ 0.1596f(t) + 0.3945C1t) + 0.8556
= 0.1596(X(1t)/4) + 0.3945(h(1t-1) 一 X(2t)/4) + 0.8556
≤ 1.4551
and get c(1t) ∈ [一0.3644, 1.4551]. We omit the back-substitution steps since we do not lose precision
over the affine transformations from x(t), and note that this calculation is based on the assumption
15
Under review as a conference paper at ICLR 2020
that the bounds of h(t) and c(t-1) are given constant. In practice, we recursively apply the process
for those vectors to express the bounds in terms of x(t), x(t-1), ∙∙∙.
The next hidden state is computed as hf) = σ(of)) ∙ tanh(cf)). We apply our abstract transformer
for the σ(x) ∙ tanh(y) expression, We obtain:
h(1t) ≥ 0.4906c(1t) - 0.0764
≥ 0.4906(0.1891f(t) + 0.0341?It) - 0.3271) - 0.0764
≥ -0.2551
h(1t) ≤ 0.1433c(1t) + 0.4471
≤ 0.1433(0.1596f1(t) + 0.3945c?(1t) + 0.8556) + 0.4471
≤ 0.6556
i.e., h(1t) ∈ [-0.2551, 0.6556]. An analogous computation is performed for h(2t). In this example We
Will assume h(2t) = 0 concretely.
Robustness certification using DeepPoly The hidden states in the LSTM at the final timestep are
passed through a fully connected layer Without activation. In our example We assume that logits after
the layer are obtained With the folloWing formula: l1 = h(1t) + 0.24 and l2 = h(2t). These are shoWn
in the final box of Fig. 2. To certify that the neural netWork classifies the input to class 1, We need to
prove that l1 - l2 > 0. We noW apply the same back-substitution technique as Singh et al. (2019a):
l1 - l2 = h(1t) - h(2t) + 0.24
≥ 0.4906(0.1891f1(t) + 0.0341c?(1t) - 0.3271) - 0.0764 + 0.24
= 0.0927f1(f) + 0.0167c?(1t) + 0.0031
= 0.0927(x(1t)/4) + 0.0167(h(1t-1) -x(2t)/4) + 0.0031
(t-1)	≥ 0.0231x(1t) - 0.0042x(2t) + 0.0181
(applying concrete bound to h1	for demonstration)	1	2
= 0.0231(ψ1(t) - ψ2(t) +4) - 0.0042(4ψ1(t) - ψ2(t)) + 0.0181
= 0.0063ψ1(t) - 0.0189ψ2(t) + 0.1105
haltthecomputationandpluginthebounds ≥ 0.0063 ∙ 1.0986 - 0.0189 ∙ 4.2236 + 0.1105
≥ 0.0375
For the purpose of demonstration, We stop here and plug in previously obtained interval bounds for
ψ(t). As this value is greater than 0, robustness is established.
D Mathematical Derivation of LSTM Transformer
LSTM cell receives not only the resulting neurons from the pre-processing stage but a hidden state
and a cell state from the previous timestep. To incorporate With all those incoming values, We extend
the scope of the LSTM equations defined in Section 2. Let d be the dimension of the input and h be
the dimension of states of the cell. To standardize the operations performed Within the LSTM cell,
We append 0d×d = [0]d×d and 0d = [0]d beloW to each kernel and bias respectively to express the
formula as
f0(t)	=	[x(t), h(t-1),	c(t-1)]Wf0	+b0f	o(0t)	= [x(t), h(t-1),	c(t-1)]Wo0	+b0o
i0t)	=	[x㈤,h(I),	C(I)] Wi0	+ bi	COt)	= [x㈤,h(I),	C(I)]Wt0	+	b,5
(1)
16
Under review as a conference paper at ICLR 2020
z
y
Figure 6: σ(x) ∙ tanh(y) boundings on [-30.7,107] X [-9.87, -0.02]. Orange plane is obtained
from POPQORN and blue plane is from DAC.
Then the cell state and hidden state computations can be rewritten as
C⑴=σ(f0t)) Θ c(t-1) + σ(i0t)) Θ tanh(COt))
= σ([x(t), h(t-1), c(t-1)]Wf0 +b0f)([x(t),h(t-1),c(t-1)][0h×(d+h),Ih]T)
+ σ([x⑴，h(t-1), MtT)]Wi' + bi) Θ tanh([x㈤,h(t-1), C(I)]W0 + 星)	⑵
h(t) = σ(o(0t)) Θ tanh(c(t))
= σ([x(t), h(t-1), C(t-1)]Wo0 + b0o) Θtanh(C(t))	(3)
As is described in detail in Section 5, f (x,y) = σ(x) ∙ tanh(y) and g(x,y) = σ(x) ∙ y can be
bounded with two planes regarding the operands’ lower and upper bounds. In other words, for any
given two real values, x, y ∈ R, x ∈ [lx , ux], y ∈ [ly , uy], our finding guarantees the planar bounds
of f and g with the coefficients built by linear combination of lx , ux , ly, uy. For a single neuron c(jt)
in C(t), the upper bound can be expressed from Eq. (2):
Cjt)= σ([x⑴,h(t-1), C(T)HWf]:,j + [bf]j) ∙ ([x(t), h(t-1), C(I)][0d+h, [Ih]j,:]T)
+ b([x(t), h(t-1), C(I)HWi‰+ %) ∙tanh([χ(t), h(t-1), C(T)HWQ+ [%]j)
≤ Au([x㈤,h(tT), C(tT)HWf]：,j + [bf]j) + Bu([x⑴,h(t-1), C(T)][0d+h, [Ih]j,：]T) + Cu
+ Du([x⑴,h(t-1), C(T)HWiI：j + [bi]j) + Eu([x(t), h(t-1), C(tT)HWQ + 闻j) + Fu
= x(t)wx + h(t-1)wh + C(t-1)wc + b	(4)
where Au, ∙ ∙ ∙ ,Fu are calculated real values by Section 5, and w∙ and b are the simplified linear
coefficients of each vector and bias term, respectively. We also can get the upper bound of h(jt) by
plugging Eq. (4) in Eq. (3), and similarly for the lower bounds of those states.
Note these linear bounds makes possible to recursively apply this step to get desired bounds, and can
be branched in any depth for the trade-off between the precision and the complexity.
E Comparison between DAC and POPQORN
In this subsection we provide in-depth comparison between our approach and POPQORN (Ko et al.,
2019) which provides another way to bound σ(x) ∙ tanh(y) and σ(x) ∙ y non-linearities. Before the
experiments, we discuss comparability between three methods to bound the non-linearities: interval,
DAC and POPQORN. Then, we perform three different experiments. First, we compare volume of
bounds produced by both POPQORN and DAC on a set of synthetic test cases. As these test cases
do not reflect the actual test cases which occur during the verification of audio benchmarks, in the
second set of experiments we compare the methods on a set of test cases from one of our benchmarks.
Finally, in the third experiment we plug in POPQORN bounds into our framework and show its
certification performance on the audio benchmarks.
17
Under review as a conference paper at ICLR 2020
DAC
POPQORN
0	0.2	0.4	0.6	0.8
Figure 7: Volume comparison with interval bounds with synthesized data. Excluded outliers.
∏~~l——I
0	0.2 0.4 0.6 0.8	1	1.2 1.4
-100dB certified
-80dB certified
-80dB uncertified
-60dB uncertified
Figure 8: Volume comparison with interval bounds with data from the working pipeline. Excluded
outliers.
Comparability of different bounding methods We say that methods A and B are pairwise non-
comparable if there exists an input for which method A produces tighter bound than method B, and
vice versa. Given this definition, POPQORN is non-comparable with our method. To demonstrate
this, in Fig. 6 we show a case where this behavior is manifested. Here, for y ≤ -1 POPQORN
(shown as orange plane) produces tighter bound than DAC (shown as blue plane). However, for the
other entire range of inputs where -10 ≤ y ≤ -2, POPQORN bounds are substantially worse than
our bounds. Further, those bounds are even worse than interval bounds and the overapproximation
error is not bounded. Contrary to POPQORN, our bounds are always strictly better than interval
bounds (this is proven in Theorem 1) and distance between the function and our planes is bounded.
Comparison on synthetic test cases In this experiment, we compare bounds produced by
POPQORN and DAC on a set of synthetic test cases, unrelated to the certification of audio classifiers.
Both methods take lx , ux and ly , uy as bounds for the input to the sigmoid and tanh, respectively.
We sample those inputs uniformly from [-2, 2] × [-2, 2] and compared the volume between the
curve σ(x) ∙ tanh(y) and bounding planes produced by both DAC and POPQORN. The volume
was computed using 1000 Monde Carlo samples in the set [lx , ux] × [ly , uy]. Since there are two
σ(x) ∙ tanh(y) and one σ(x) ∙ y calculation appearing in a single LSTM cell, We run the experiment
with such portion of data. In other words, in 67% of experiments we bound σ(x) ∙ tanh(y), and
in 33% we bound σ(x) ∙ y. We sampled 100 such inputs and compared the volumes obtained by
POPQORN and DAC with the volume obtained using interval.
The distribution of volumes is shown in Fig. 7. Here, 1 stands for the same volume as interval
bounds and values less than 1 indicate performance better than intervals. We conclude that, for this
experiment, POPQORN bounds produces smaller volume than our method - 0.2 compared to 0.37. In
terms of runtime, POPQORN takes 14.37 seconds on average while the bound calculation of DAC
finishes in few milliseconds.
Direct comparison on test cases from audio benchmarks The previous experiment may not
reflect the actual performance on audio benchmarks that we consider in this work. We uniformly
sampled the arguments lχ, uχ,ly, Uy that our transformer for σ(x) ∙ tanh(y) and σ(x) ∙ y was invoked
with. We distinguish between test cases corresponding to different perturbation ranges and then
further split it into certified and uncertified samples. For each of the cases, Fig. 8 shows the box plot
of the distributions of volumes produced by both POPQORN and DAC.
18
Under review as a conference paper at ICLR 2020
(a) undefended FSDD, 98%
Figure 9: Robustness evaluation on FSDD with error bars.
(b) defended FSDD, 95%
We found that, while overall POPQORN bounds work well in practice, they frequently produce bounds
less precise than interval bounds. The reason of this malfunctioning comes from the limitation of
gradient descent approach employed by POPQORN. The gradient of the function, which POPQORN
uses to search for the optimal bounds, is large when the inputs are distributed near 0. However, if
the inputs are far from the origin or are too close to each other, the function curve becomes almost
flat, gradients are close to zero and gradient descent has problems converging. Also, in other cases,
gradient descent is not guaranteed to find the bounds with minimum volume. Fig. 6 shows one of the
examples where POPQORN fails to find lower planar bound which produces minimum volume. On
the contrary, the resulting value of σ(x) ∙ tanh(y) is within [-1,1] regardless of the magnitude of
arguments which means that the error produced by intervals is bounded. As our planar bounds are
strictly better than intervals, regardless of input conditions, our error is also bounded.
Plugging in POPQORN bounds into DAC We also experimented with using POPQORN bounds
instead of our bounds in the pipeline and compared the final adversarial regions with those resulting
from DAC. As POPQORN is relatively slow (108 minutes per sample), we performed this experiment
only on the first 10 inputs with -80dB perturbation. Using their bounds results in 0 verified samples,
while DAC verifies 4 samples. We believe the reason here is the existence of many pathological cases
as described in the previous point where gradient descent used by POPQORN converges to suboptimal
solution which ends up being worse than interval bounds. These errors are further propagated through
each frame and resulting output can not be certified.
F	Sensitivity of Provability Metric
In our experiments, we followed the convention of provability measurement from Singh et al. (2019b).
Here, we also provide the result with error bars from 10 independent repetitions with randomly
permuted test set. For each repetition, we randomly permute the test set with the different seed and
collect the first 100 samples with the correct prediction under zero perturbation from the ordered set.
We then count the number of certified inputs from those samples to represent the provability under
the given constant . Fig. 9 shows that provabilities do not differ much from the reported results for
multiple experiments.
G Training with Provab le Defense with IBP
Here we give more details on our training procedure for provably defended network which follows
Mirman et al. (2018); Gowal et al. (2018). Let zLB() and zUB() be the resulting lower and upper
bounds for the final logit Z ∈ Rd under the perturbation size e, with the true label j ∈ {0, ∙∙∙ , d - 1}.
We define the worst-case logits Z(e) as
zi(e)
ziLB(e)
ziUB(e)
i=j
i 6= j
which corresponds to the worst concrete logits under the given input with the predefined perturbation
amount. Recall that we say the input is certified when this worst-case logits satisfy j = argmaxg Zi.
19
Under review as a conference paper at ICLR 2020
dB noise (log scale)
Figure 10: defended GSC, 82%.
params	FSDD	GSC
Frame size N	256	512
Frame step s	200	400
Number of filters p	10	10
Hidden neurons h	40	40
Learning rate	0.001	0.001
Batch size	5	50
Table 3: Model parameters used in the experiments.
Training loss L is a linear combination of the standard cross-entropy loss l(z, e(j)) and the worst-case
cross-entropy loss l(z, e(j)) where e(j) is target one-hot vector, i.e.,
L(t) = κ(t0)l(z, e(j)) + (1 - κ(t0))l(Z(e(t0)), e(j))
Note that we set up the κ and as the function of t0 . As in Gowal et al. (2018), gradual in-
crease/decrease of these parameters during the training was essential to get desired performance. We
set these functions with respect to the number of training epochs E as
1 - t/E t < E/2
1/2	otherwise
e(t) = -70 - (200 - 70)1-t/E.
Also, to track the training speed, we increase one by one t0 only if we are achieving 85% of standard
accuracy and 80% provability at the current setting. The models were built under the E = 60.
We also attach the missing result of defended GSC with 82% concrete accuracy in Fig. 10.
κ(t)
H Model Parameters Used in the Experiments
We perform the experiments with the same architecture with different model parameters for FSDD
and GSC. Table 3 shows the parameters of the models we use. Both defended and undefended
networks share the same parameters for each dataset.
20