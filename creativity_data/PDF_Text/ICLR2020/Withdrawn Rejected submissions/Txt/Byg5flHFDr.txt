Under review as a conference paper at ICLR 2020
EvoNet: A Neural Network for Predicting the
Evolution of Dynamic Graphs
Anonymous authors
Paper under double-blind review
Ab stract
Neural networks for structured data like graphs have been studied extensively in
recent years. To date, the bulk of research activity has focused mainly on static
graphs. However, most real-world networks are dynamic since their topology tends
to change over time. Predicting the evolution of dynamic graphs is a task of high
significance in the area of graph mining. Despite its practical importance, the
task has not been explored in depth so far, mainly due to its challenging nature.
In this paper, we propose a model that predicts the evolution of dynamic graphs.
Specifically, we use a graph neural network along with a recurrent architecture to
capture the temporal evolution patterns of dynamic graphs. Then, we employ a
generative model which predicts the topology of the graph at the next time step
and constructs a graph instance that corresponds to that topology. We evaluate the
proposed model on several artificial datasets following common network evolving
dynamics, as well as on real-world datasets. Results demonstrate the effectiveness
of the proposed model.
1	Introduction
Graph neural networks (GNNs) have emerged in recent years as an effective tool for analyzing
graph-structured data (Scarselli et al., 2008; Gilmer et al., 2017; Zhou et al., 2018; Wu et al., 2019).
These architectures bring the expressive power of deep learning into non-Euclidean data such as
graphs, and have demonstrated convincing performance in several graph mining tasks, including
graph classification (Morris et al., 2019), link prediction (Zhang & Chen, 2018), and community
detection (Bruna & Li, 2017; Chen et al., 2017). So far, GNNs have been mainly applied to tasks
that involve static graphs. However, most real-world networks are dynamic, i. e., nodes and edges
are added and removed over time. Despite the success of GNNs in various applications, it is still not
clear if these models are useful for learning from dynamic graphs. Although some models have been
applied to this type of data, most studies have focused on predicting a low-dimensional representation
(i. e., embedding) of the graph for the next time step (Li et al., 2016; 2017; Nguyen et al., 2018;
Goyal et al., 2018; Seo et al., 2018; Pareja et al., 2019). These representations can then be used
in downstream tasks (Li et al., 2016; Goyal et al., 2018; Meng et al., 2018; Pareja et al., 2019).
Predicting the topology of the graph is a task that has not been properly addressed yet.
Graph generation, another important task in graph mining, has attracted a lot of attention from the
deep learning community in recent years. The objective of this task is to generate graphs that exhibit
specific properties, e. g., degree distribution, node triangle participation, community structure etc.
Traditionally, graphs are generated based on some network generation model such as the Erdos-Renyi
model. These models focus on modeling one or more network properties, and neglect the others.
Neural network approaches, on the other hand, can better capture the properties of graphs since they
follow a supervised approach (You et al., 2018; Bojchevski et al., 2018; Grover et al., 2018). These
architectures minimize a loss function such as the reconstruction error of the adjacency matrix or the
value of a graph comparison algorithm.
Capitalizing on recent developments in neural networks for graph-structured data and graph genera-
tion, we propose in this paper, to the best of our knowledge, the first framework for predicting the
evolution of the topology of networks in time. The proposed framework can be viewed as an encoder-
decoder architecture. The “encoder” network takes a sequence of graphs as input and uses a GNN to
produce a low-dimensional representation for each one of these graphs. These representations capture
1
Under review as a conference paper at ICLR 2020
structural information about the input graphs. Then, it employs a recurrent architecture which predicts
a representation for the future instance of the graph. The “decoder” network corresponds to a graph
generation model which utilizes the predicted representation, and generates the topology of the graph
for the next time step. The proposed model is evaluated over a series of experiments on synthetic
and real-world datasets. To measure its effectiveness, the generated graphs need to be compared
with the corresponding ground-truth graph instances. To this end, we use the Weisfeiler-Lehman
subtree kernel which scales to very large graphs and has achieved state-of-the-art results on many
graph datasets (Shervashidze et al., 2011). The proposed model is compared against several baseline
methods. Results show that the proposed model is very competitive, and in most cases, outperforms
the competing methods.
The rest of this paper is organized as follows. Section 2 provides an overview of the related work and
elaborates our contribution. Section 3 introduces some preliminary concepts and definitions related to
the graph generation problem, followed by a detailed presentation of the components of the proposed
model. Section 4 evaluates the proposed model on several tasks. Finally, Section 5 concludes.
2	Related Work
Our work is related to random graph models. These models are very popular in graph theory and
network science. The Erdos-Renyi model (Erdos & R6nyi, 1960), the preferential attachment model
(Albert & Barabgsi, 2002), and the Kronecker graph model (Leskovec et al., 2010) are some typical
examples of such models. To predict how a graph structure will evolve over time, the values of the
parameters of these models can be estimated based on the corresponding values of the observed graph
instances, and then the estimated values can be passed on to these models to generate graphs.
Other work along a similar direction includes neural network models which combine GNNs with
RNNs (Seo et al., 2018; Manessi et al., 2017; Pareja et al., 2019). These models use GNNs to
extract features from a graph and RNNs for sequence learning from the extracted features. Other
similar approaches do not use GNNs, but they instead perform random walks or employ deep
autoencoders (Nguyen et al., 2018; Goyal et al., 2018). All these works focus on predicting how the
node representations or the graph representations will evolve over time. However, some applications
require predicting the topology of the graph, and not just its low-dimensional representation. The
proposed model constitutes the first step towards this objective.
3	EvoNet: A Neural Network for Predicting Graph Evolution
In this Section, we first introduce basic concepts from graph theory, and define our notation. We then
present EvoNet, the proposed framework for predicting the evolution of graphs. Since the proposed
model comprises of several components, we describe all these components in detail.
3.1	Preliminaries
Let G = (V, E) be an undirected, unweighted graph, where V is the set of nodes and E is the set
of edges. We will denote by n the number of vertices and by m the number of edges. We define a
permutation of the nodes of G as a bijective function π : V → V , under which any graph property
of G should be invariant. We are interested in the topology of a graph which is described by its
adjacency matrix Aπ ∈ Rn×n with a specific ordering of the nodes π1. Each entry of the adjacency
matrix is defined as Aj = l(∏(vj∏(vj))∈e where vi,vj ∈ V. In what follows, we consider the
“topology”, “structure” and “adjacency matrix” of a graph equivalent to each other.
In many real-world networks, besides the adjacency matrix that encodes connectivity information,
nodes and/or edges are annotated with a feature vector, which we denote as X ∈ Rn×d and L ∈
Rm×d, respectively. Hence, a graph object can be also written in the form of a triplet G = (A, X, L).
In this paper, we use this triplet to represent all graphs. If a graph does not contain node/edge
attributes, we assign attributes to it based on local properties (e. g., degree, k-core number, number of
triangles, etc).
1For simplicity, the ordering π will be omitted in what follows.
2
Under review as a conference paper at ICLR 2020
Figure 1: Illustration of the proposed architecture
An evolving network is a graph whose topology changes as a function of time. Interestingly, almost
all real-world networks evolve over time by adding and removing nodes and/or edges. For instance, in
social networks, people make and lose friends over time, while there are people who join the network
and others who leave the network. An evolving graph is a sequence of graphs {G0, G1, . . . , GT } where
Gt = (At, Xt, Et) represents the state of the evolving graph at time step t. It should be noted that not
only nodes and edges can evolve over time, but also node and edge attributes. However, in this paper,
we keep node and edge attributes fixed, and we allow only the node and edge sets of the graphs to
change as a function of time. The sequence can thus be written as {Gt = (At, X, E)}t∈[0,T] . We are
often interested in predicting what “comes next” in a sequence, based on data encountered in previous
time steps. In our setting, this is equivalent to predicting Gt based on the sequence {Gk }k<t. In
sequential modeling, we usually do not take into account the whole sequence, but only those instances
within a fixed small window of size w before Gt, which we denote as {Gt-w, Gt-w+1, . . . , Gt-1}.
We refer to these instances as the graph history. The problem is then to predict the topology of Gt
given its history.
3.2	Proposed Architecture
The proposed architecture is very similar to a typical sequence learning framework. The main
difference lies in the fact that instead of vectors, in our setting, the elements of the sequence
correspond to graphs. The combinatorial nature of graph-structured data increases the complexity
of the problem and calls for more sophisticated architectures than the ones employed in traditional
sequence learning tasks. Specifically, the proposed model consists of three components: (1) a graph
neural network (GNN) which generates a vector representation for each graph instance, (2) a recurrent
neural network (RNN) for sequential learning, and (3) a graph generation model for predicting the
graph topology at the next time step. This framework can also be viewed as an autoencoder-decoder
model. The first two components correspond to an encoder network which maps the sequence of
graphs into a sequence of vectors and predicts a representation for the next in the sequence graph. The
decoder network consists of the last component of the model, and transforms the above representation
into a graph. Figure 1 illustrates the proposed model. In what follows, we present the different
components of EvoNet in detail.
3.2.1	Graph Neural Network
Graph Neural Networks (GNNs) have recently emerged as a dominant paradigm for performing
machine learning tasks on graphs. Several GNN variants have been proposed in the past years. All
these models employ some message passing procedure to update node representations. Specifically,
each node updates its representation by aggregating the representations of its neighbors. After k
3
Under review as a conference paper at ICLR 2020
iterations of the message passing procedure, each node obtains a feature vector which captures the
structural information within its k-hop neighborhood. Then, GNNs compute a feature vector for the
entire graph using some permutation invariant readout function such as summing the representations
of all the nodes of the graph. As described below, the learning process can be divided into three
phases: (1) aggregation, (2) update, and (3) readout.
Aggregation In this phase, the network computes a message for each node of the graph. To compute
that message for a node, the network aggregates the representations of its neighbors. Formally, at
time t + 1, a message vector mtv+1 is computed from the representations of the neighbors N(v) of v:
mV+1 = AGGREGATEt+1({hW | W ∈N(v)})	(1)
where AGGRE GATE is a permutation invariant function. Furthermore, for the network to be end-to-
end trainable, this function needs to be differentiable. In our case, AGGRE GATE is a multi-layer
perceptron (MLP) followed by a sum function.
Update The new representation htv+1 of v is then computed by combining its current feature vector
htv with the message vector mtv+1 :
hV+1 = UPDATEt+1(hV ,mV+1)	(2)
The UPDATE function also needs to be differentiable. To combine the two feature vectors (i. e., htv
and mtv+1), we have employed the Gated Recurrent Unit proposed in (Li et al., 2015):
htv+1 = GRUt+1(htv, mtv+1)	(3)
Omitting biases for readability, we have:
rvt+1 = σ(WRt+1mtv+1 + URt+1htv)
zvt+1 = σ(WZt+1mtv+1 + UZt+1htv)	(4)
ht+1 = tanh(Wt+1mV+1 + Ut+1(rV+1 Θ htv))
htv+1 = (1 — zV+1) Θ hV + zV+1 Θ hV+1
where the W and U matrices are trainable weight matrices, σ is the sigmoid function, and rv and zv
are the parameters of the reset and update gates for a given node.
Readout The Aggregation and Update steps are repeated for T time steps. The emerging node repre-
sentations {hvT}v∈V are aggregated into a single vector which corresponds to the graph representation,
as follows:
hG = READOUT({hT | V ∈ V})	(5)
where READOUT is a differentiable and permutation invariant function. This vector captures
the topology of the input graph. To generate hG, we utilize Set2Set (Vinyals et al., 2015). Other
functions such as the sum function were also considered, but were found less effective in preliminary
experiments.
3.2.2	Recurrent Neural Networks
Given an input sequence of graphs, we use the GNN described above to generate a vector repre-
sentation for each graph in the sequence. Then, to process this sequence, we use a recurrent neural
network (RNN). RNNs use their internal state (i. e., memory) to preserve sequential information.
These networks exhibit temporal dynamic behavior, and can find correlations between sequential
events Specifically, an RNN processes the input sequence in a series of time steps (i. e., one for each
element in the sequence). For a given time step t, the hidden state ht at that time step is updated as:
ht+1 = f(ht, xt+1)	(6)
where f is a non-linear activation function. A generative RNN outputs a probability distribution over
the next element of the sequence given its current state ht. RNNs can be trained to predict the next el-
ement (e. g., graph) in the sequence, i. e., it can learn the conditional distribution p(Gt |G1, . . . , Gt-1).
In our implementation, we use a Long Short-Term Memory (LSTM) network that reads sequentially
the vectors {hGi}i∈[t-w,t-1] produced by the GNN, and generates a vector hGT that represents the
embedding of GT, i. e., the graph at the next time step. The embedding incorporates topological
information and will serve as input to the graph generation module. Along with the GNN component,
this architecture can be seen as a form of an encoder network. This network takes as input a sequence
of graphs and projects them into a low-dimensional space.
4
Under review as a conference paper at ICLR 2020
3.2.3	Graph Generation
To generate a graph that corresponds to the evolution of the current graph instance, we capitalize on
a recently-proposed framework for learning generative models of graphs (You et al., 2018). This
framework models a graph in an autoregressive manner (i. e., a sequence of additions of new nodes
and edges) to capture the complex joint probability of all nodes and edges in the graph. Formally,
given a node ordering π, it considers a graph G as a sequence of vectors:
π ππ	π
SG = (S1 , S2 , . . . , S|V | )	(7)
where Siπ = [a1,i, . . . , ai-1,i] ∈ {0, 1}i-1 is the adjacency vector between node π(i) and the nodes
preceding it ({π(1), . . . , π(i - 1)}). We adapt this framework to our supervised setting.
The objective of the generative model is to maximize the likelihood of the observed graphs of the
training set. Since a graph can be expressed as a sequence of adjacency vectors (given a node ordering),
We can consider instead the distribution p(Sπ; θ), which can be decomposed in an autoregressive
manner into the following product:
|V|	|V| i-1
p(Sπ; θ) = Y P 阴概:k<i,θ = YY p(^∏i∣a∏,ι<j ,s^k<i,θ)	⑻
i=1	i=1 j=1
This product can be parameterized by a neural network. Specifically, following (You et al., 2018),
we use a hierarchical RNN consisting of two levels: (1) the graph-level RNN which maintains the
state of the graph and generates new nodes and thus learns the distribution P(Sn ∣S∏+<i) and (2) the
edge-level RNN which generates links between each generated node and previously-generated nodes
and thus learns the distributionp(αji∣^u.jj<j). More formally, we have:
h0 = hGT
hi = RNN1(hi-1,Si-1)
m0,i = hi
mj,i = RNN2 (mj-1,i, aj-1,i)
P(a∏,i = I)= σ(mj,i)
(9)
&n，i ~ P
where hi is the state vector of the graph-level RNN (i. e., RNN1) that encodes the current state of the
graph sequence and is initialized by hGT, the predicted embedding of the graph at the next time step
T. The output of the graph-level RNN corresponds to the initial state of the edge-level RNN (i. e.,
RNN2). The resulting value is then squashed by a sigmoid function to produce the probability of
existence of an edge aj,i. In other words, the model learns the probability distribution of the existence
of edges and a graph can then be sampled from this distribution, which will serve as the predicted
topology for the next time step T .
To train the model, the cross-entropy loss between existence of each edge and its probability of
existence is minimized:
|V| i-1
L = XX aπ,i(1 - p(aπ,i = I)) +(I- aj,i)p(aj,i =I)	(IO)
i=1 j=1
Node ordering It should be mentioned that node ordering π has a large impact on the efficiency
of the above generative model. Note that a good ordering can help us avoid the exploration of all
possible node permutations in the sample space. Different strategies such as the Breadth-First-Search
ordering scheme can be employed to improve scalability (You et al., 2018). However, in our setting,
the nodes are distinguishable, i. e., node v of Gi and node v of Gi+1 correspond to the same entity.
Hence, we can impose an ordering onto the nodes of the first instance of our sequence of graphs, and
then utilize the same node ordering for the graphs of all subsequent time steps (we place new nodes
at the end of the ordering). 4
4 Experiments and Results
In this Section, we evaluate the performance of EvoNet on synthetic and real-world datasets for
predicting the evolution of graph topology, and we compare it against several baseline methods.
5
Under review as a conference paper at ICLR 2020
4.1	Datasets
We use both synthetic and real-world datasets. The synthetic datasets consist of sequences of graphs
where there is a specific pattern on how each graph emerges from the previous graph instance, i. e.,
add/remove some graph structure at each time step. The real-world datasets correspond to single
graphs whose nodes incorporate temporal information. We decompose these graphs into sequences
of snapshots based on their timestamps. We fix the length of the sequences to 1000 time steps. The
size of the graphs in each sequence ranges from tens of nodes to several thousand of nodes.
4.1.1	Synthetic Datasets
Path graph A path graph can be drawn such that all vertices and edges lie on a straight line. We
denote a path graph of n nodes as Pn . In other words, the path graph Pn is a tree with two nodes
of degree 1, and the other n - 2 nodes of degree 2. We consider two scenarios. In both cases the
first graph in the sequence is P3 . In the first scenario, at each time step, we add one new node to the
previous graph instance and we also add an edge between the new node and the last according to the
previous ordering node. The second scenario follows the same pattern, however, every three steps,
instead of adding a new node, we remove the first according to the previous ordering node (along
with its edge).
Cycle graph A cycle graph Cn is a graph on n nodes containing a single cycle through all the
nodes. Note that if we add an edge between the first and the last node of Pn, we obtain Cn . Similar
to the above case, we use C3 as the first graph in the sequence, and we again consider two scenarios.
In the first scenario, at each time step, we increase the size of the cycle, i. e., from Ci, we obtain Ci+1
by adding a new node and two edges, the first between the new node and the first according to the
previous ordering node and the second between the new node and the last according to the previous
ordering node. In the second scenario, every three steps, we remove the first according to the ordering
node (along with its edges), and we add an edge between the second and the last according to the
ordering nodes.
Ladder graph The ladder graph Ln is a planar graph with 2n vertices and 3n - 2 edges. It is the
cartesian product of two path graphs, as follows: Ln = Pn × P2 . As the name indicates, the ladder
graph Ln can be drawn as a ladder consisting of two rails and n rungs between them. We consider
the following scenario: at each time step, we attach one rung (P2) to the tail of the ladder (the two
nodes of the rung are connected to the two last according to the ordering nodes).
For all graphs, we set the attribute of each node equal to its degree, while we set the attribute of all
edges to the same value (e. g., to 1).
4.1.2	Real-World Datasets
Besides synthetic datasets, we also evaluate EvoNet on real-world datasets. These datasets contain
graphs derived from the Bitcoin transaction network, a who-trust-whom network of people who trade
using Bitcoin (Kumar et al., 2016; 2018). Due to the anonymity of Bitcoin users, platforms seek to
maintain a record of users’ reputation in Bitcoin trades to avoid fraudulent transactions. The nodes of
the network represent Bitcoin users, while an edge indicates that a trade has been executed between
its two endpoint users. Each edge is annotated with an integer between -10 and 10, which indicates
the rating of the one user given by the other user. The network data are collected separately from
two platforms: Bitcoin OTC2 and Bitcoin Alpha3. More details about these two datasets are given in
Table 1. For all graphs, we set the attribute of each node equal to the average rating that the user has
received from the rest of the community, and the attribute of each edge equal to the rating between its
two endpoint users.
4.2	Baselines
We compare EvoNet against several random graph models: (1) the Erdos-Renyi model (Erdos &
R6nyi, 1960), (2) the Stochastic Block model (Holland et al., 1983; Airoldi et al., 2008), (3) the
2https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html
3https://snap.stanford.edu/data/soc-sign-bitcoin-alpha.html
6
Under review as a conference paper at ICLR 2020
BTC-OTC
BTC-ALPHA
|V |	|E|	% POS.EDGES
5,881	35,592	89%
3, 783	24, 186	93%
Timespan
Begin	End
2010-11-8	2016-1-25
2010-11-8	2016-1-22
Table 1: Statistics of the two real-world datasets used in our experiments.
Size: Real vs. Predict
0	50	100	150	200	250	300
Graph Index
3000
Size: Real vs. Predict
160
; ] ]
》ou JOJaqEnU) əz-s UdeJ9
O IOO 200 300 400 500 600 700 800
Graph Index
Size: Real vs. Predict
n∞=060m,°
1 1
(səpou FEnVN-M
0	20	«	60	80	100 120 140
Graph Index
Figure 2:	Results of synthetic datasets. Comparison of graph size (path, ladder and cycle graphs from
left to right): predicted size (blue) VS. real size (orange).
Barabgsi-Albert model (Albert & Barabgsi, 2002), and (4) the Kronecker Graph model (Leskovec
et al., 2010). These are the traditional methods to study the topology evolution of temporal graphs,
by proposing a driven mechanism behind the evolution.
4.3	Evaluation Metric and Experimental Setup
In general, it is very challenging to measure the performance of a graph generative model since it
requires comparing two graphs to each other, a long-standing problem in mathematics and computer
science (Conte et al., 2004). We propose to use graph kernels to compare graphs to each other,
and thus to evaluate the quality of the generated graphs. Graph kernels have emerged as one of
the most effective tools for graph comparison in recent years (Nikolentzos et al., 2019). A graph
kernel is a symmetric positive semidefinite function which takes two graphs as input, and measures
their similarity. In our experiments, we employ the Weisfeiler-Lehman subtree kernel which counts
label-based subtree-patterns (Shervashidze et al., 2011). Note that we also normalize the kernel
values, and thus the emerging values lie between 0 and 1.
As previously mentioned, each dataset corresponds to a sequence of graphs where each sequence
represents the evolution of the topology of a single graph in 1000 time steps. We use the first 80% of
these graph instances for training and the rest of them serve as our test set. The window size w is
set equal to 10, which means that we feed 10 consecutive graph instances to the model and predict
the topology of the instance that directly follows the last of these 10 input instances. Each graph of
the test set along with its corresponding predicted graph is then passed on to the Weisfeiler-Lehman
subtree kernel which measures their similarity and thus the performance of the model.
The hyperparameters of EvoNet are chosen based on its performance on a validation set. The
parameters of the random graph models are set under the principle that the generated graphs need to
share similar properties with the ground-truth graphs. For instance, in the case of the ErdoS-Renyi
model, the probability of adding an edge between two nodes is set to some value such that the density
of the generated graph is identical to that of the ground-truth graph. However, since the model should
not have access to such information (e. g., density of the ground-truth graph), we use an MLP to
predict this property based on past data (i. e., the number of nodes and edges of the previous graph
instances). This is in par with how the proposed model computes the size of the graphs to be generated
(i. e., using also an MLP).
4.4	Results
Synthetic datasets Figure 2 illustrates the experimental results on the synthetic datasets. Since
the graph structures contained in the synthetic datasets are fairly simple, it is easy for the model to
7
Under review as a conference paper at ICLR 2020
Distnbution of Similarities between real and
?OTC}
5 0 5 0 5 0 5
7 5uo 7 5 2
SdUUeJSU- % JeqEnN
^0o^
Proposed
Erdos-Renyl
■ BarabasI-AIbert
Powerlaw-Cluster
■ SBM
KrOneCIcer-random
Kronecleer-fix
WVUenMWC- % =SqEnN
Distribution OfSimiIarities between real and
Proposed
・ EMos-RenyI
BarabasI-Albert
Poweriaw-Cluster
■ SBM
KroneCker-random
KroneCkeMX
∙,
Figure 3:	Similarity histograms on BTC-OTC (left) and BTC-Alpha (right) datasets. Blue one is the
result of EvoNet, which is compared against 6 random graph models.
Stat. MODE^^^^^^	BTC-OTC		BTC-ALPHA	
	Mean	90%ILE	Mean	90%ILE
TR	0.28	0.40	0.22	0.32
SBM	0.21	0.30	0.18	0.27
BA	0.35	0.48	0.23	0.28
POWER	0.35	0.48	0.23	0.28
Kron-Rand	0.62	0.64	0.44	0.47
Kron-Fix	0.21	0.23	0.08	0.11
EVONET	0.82	0.84	0.55	0.59
Table 2: Statistics on the similarity distribution of different models: ER stands for Erdos-Renyi model.
SBM stands for Stochastic Block Model. BA is Barabdsi-Albert Model. POWER is another model,
similar to the Barabdsi-Albert, that grows graphs with powerlaw degree distribution. Kron-Rand
represents the Kronecker Graph Model with learnable parameter while Kron-Fix represents the
Kronecker Graph Model with fixed parameters which depend on the initial graph.
generate graphs very similar to the ground-truth graphs (normalized kernel values > 0.9). Hence,
instead of reporting the kernel values, we compare the size of the predicted graphs against that of the
ground-truth graphs. The figures visualize the increase of graph size on real sequence (orange) and
predicted sequence (blue). For path graphs, in spite of small variance, we have an accurate prediction
on the graph size. For ladder graph, we observe a mismatch at the beginning of the sequence for
small size graphs but then a coincidence of the two lines on large size graphs. This mismatch on
small graphs may be due to a more complex structure in ladder graphs such as cycles, as supported by
the results of cycle graph on the right figure, where we completely mispredict the size of cycle graphs.
In fact, we fail to reconstruct the cycle structure in the prediction, with all the predicted graphs being
path graphs. This failure could be related to the limitations of GNN mentioned in Xu et al. (2018)
Real-World datasets Finally, we analyze the performance of our model on real datasets: the
Bitcoin-OTC and Bitcoin-Alpha. We obtain the similarities between each pair of real and predicted
graphs in the sequence and draw a histogram to illustrate the distribution of similarities. The results
are shown in Figure 3 respectively for the two datasets. Among all the traditional random graph
models, Kronecker graph model (with learnable parameter) performs the best, however on both
datasets, our proposed method EvoNet (in blue) outperforms tremendously all other methods, with an
average similarity of 0.82 on BTC-OTC dataset and 0.55 on BTC-Alpha dataset. Detailed statistics
can be found in Table 2.
Overall, our experiments demonstrate the advantage of EvoNet over the traditional random graph
models on predicting the evolution of dynamic graphs.
5 Conclusion
In this paper, we proposed EvoNet, a model that predicts the evolution of dynamic graphs, following
an encoder-decoder framework. We also proposed an evaluation methodology for this task which
capitalizes on the well-established family of graph kernels. Experiments show that the proposed
model outperforms traditional random graph methods on both synthetic and real-world datasets.
8
Under review as a conference paper at ICLR 2020
References
Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. Mixed membership
stochastic blockmodels. Journal ofmaChine learning research, 9(Sep):1981-2014, 2008.
Reka Albert and Albert-Ldszld Barabdsi. Statistical mechanics of complex networks. Reviews of
modern physics, 74(1):47, 2002.
Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zugner, and Stephan Gunnemann. Netgan:
Generating graphs via random walks. arXiv preprint arXiv:1803.00816, 2018.
Joan Bruna and X Li. Community detection with graph neural networks. stat, 1050:27, 2017.
Zhengdao Chen, Xiang Li, and Joan Bruna. Supervised community detection with line graph neural
networks. arXiv preprint arXiv:1705.08415, 2017.
Donatello Conte, Pasquale Foggia, Carlo Sansone, and Mario Vento. Thirty years of graph matching
in pattern recognition. International Journal of Pattern Recognition and Artificial Intelligence, 18
(03):265-298, 2004.
Paul Erdos and Alfred R6nyi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci,
5(1):17-60, 1960.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Palash Goyal, Nitin Kamra, Xinran He, and Yan Liu. Dyngem: Deep embedding method for dynamic
graphs. arXiv preprint arXiv:1805.11273, 2018.
Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs.
arXiv preprint arXiv:1803.10459, 2018.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First
steps. Social networks, 5(2):109-137, 1983.
Srijan Kumar, Francesca Spezzano, VS Subrahmanian, and Christos Faloutsos. Edge weight predic-
tion in weighted signed networks. In 2016 IEEE 16th International Conference on Data Mining
(ICDM), pp. 221-230. IEEE, 2016.
Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos Faloutsos, and VS Subrahmanian.
Rev2: Fraudulent user prediction in rating platforms. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining, pp. 333-341. ACM, 2018.
Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani.
Kronecker graphs: An approach to modeling networks. Journal of Machine Learning Research, 11
(Feb):985-1042, 2010.
Cheng Li, Xiaoxiao Guo, and Qiaozhu Mei. Deepgraph: Graph structure predicts network growth.
arXiv preprint arXiv:1610.06251, 2016.
Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. Attributed network
embedding for learning in a dynamic environment. In Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management, pp. 387-396. ACM, 2017.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Franco Manessi, Alessandro Rozza, and Mario Manzo. Dynamic graph convolutional networks.
arXiv preprint arXiv:1704.06199, 2017.
Changping Meng, S Chandra Mouli, Bruno Ribeiro, and Jennifer Neville. Subgraph pattern neural
networks for high-order graph evolution prediction. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
9
Under review as a conference paper at ICLR 2020
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In
Proceedings ofthe AAAI Conference on Artificial Intelligence, volume 33,pp. 4602-4609, 2019.
Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and Sungchul
Kim. Continuous-time dynamic network embeddings. In Companion Proceedings of the The Web
Conference 2018, pp. 969-976. International World Wide Web Conferences Steering Committee,
2018.
Giannis Nikolentzos, Giannis Siglidis, and Michalis Vazirgiannis. Graph Kernels: A Survey. arXiv
preprint arXiv:1904.12218, 2019.
Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi,
Tim Kaler, and Charles E Leisersen. Evolvegcn: Evolving graph convolutional networks for
dynamic graphs. arXiv preprint arXiv:1902.10191, 2019.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
Graph Neural Network Model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Youngjoo Seo, Michael Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence
modeling with graph convolutional recurrent networks. In International Conference on Neural
Information Processing, pp. 362-373. Springer, 2018.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):
2539-2561, 2011.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
arXiv preprint arXiv:1511.06391, 2015.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. Graphrnn: Generating
realistic graphs with deep auto-regressive models. arXiv preprint arXiv:1802.08773, 2018.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in
Neural Information Processing Systems, pp. 5165-5175, 2018.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural
networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018.
A Extra experiment results with synthetic datasets
A. 1 Graph size comparison
See Figure 4.
A.2 Histogram of similarities
See Figure 5.
A.3 Some examples of predicted graphs
See Figure 6, 7, 8, 9, 10, 11, respectively for Path graphs, Ladder graphs with small size, Ladder
graphs with large size, Cycle graphs, Path graphs with removal, Cycle graphs with adding extra
structures.
10
Under review as a conference paper at ICLR 2020
Path graph (VVlth removal) Size： Real vs. PredICt
500
Oooo
Oooo
4 3 2 1
(S3pou0」3qEnu) 3S llʤ
♦ , H ■ H ■
(S3poua-o-l,uqujnu)",z 访 qde-l°
(with extra structure) Size: Real vs. Predict
3000
0	200	400	600	800 1000 1200 1400 1600
Graph Index
0	200	400	600	800 1000 1200 1400 1600
Graph Index
0
Figure 4:	Comparison of graph size: predicted size (blue) VS. real size (orange). Left: Path graphs
with removal; Right: Cycle graphs with adding extra structures
Distribution of Similarities between real and
Distribution of Similarities between real and
250
sφuue-jsu- jo JaqEnN
200
Proposed
■ Erdos-Renyl
■ BarabasI-AJbert
■ PcwerIaw-CIuster
SBM
KrOneCker-random
KrOneCker-fix
150
hL
0 0.0 1 0.2
04  ......~0^6^
Similanty
^δT
0 5 0 5 0 5
5 2 0 7 5 2
111
suuesu-JO.IaqUJnN
Proposed
Erdos-Renyi
Barabasi-Albert
■ PowerIaw-CIuster
SBM
KroneCker-random
Kronecker-fi×
0.0	0.1	0.2	0.3	0.4	0.5
Similarity
(a)	(b)
Distnbution OfSimiIanties between real and predicted graphs (Circle)
u,uw8 64,2
suuesu- jo JaqEnN
-0.4	-0.2	0.0	0.2	0.4	0.6	0.8	1.0
Similarity
OTVUCfinC- % JBqEnN
Distribution of Similarities between real and predicted graphs (Path-Removal)
0.0	0.2
Proposed
■ Erdos-RenyI
Barsbasl-Albert
Poweriaw-Cluster
SBM
Kronecter-ra ndom
KroneCIffir-fix
0・4 _ .. .	0-6
Similarity
0.8
(c)	(d)
DlStnbUtIon OfSlmIlantleS between real and PredlCted graphs (CyCIe-Add)
Proposed
Erdos-Renyl
Barabasl-Albert
Poweriaw-Cluster
SBM
KroneCker-random
Kroπecker-flx
9ucm⅞c- ‰0 JBqUJnN
_.O3. ..	0.4
Similanty
(e)
Figure 5:	Similarity histograms on synthetic datasets. Blue one is the result of EvoNet, which is
compared against 6 random graph models. 5a: Path graphs; 5b: Ladder graphs; 5c: Cycle graphs; 5d:
Path graphs with removal; 5e: Cycle graphs with adding extra structures.
11
Under review as a conference paper at ICLR 2020
Figure 6: Some examples of predictions on Path datasets: the left column is the real graphs and the
right column is the predicted ones.
12
Under review as a conference paper at ICLR 2020
Real graphs ≡	Predicted graphs 受 5
Figure 7: Some examples of predictions on Ladder datasets (with small size graphs): the left column
is the real graphs and the right column is the predicted ones.
13
Under review as a conference paper at ICLR 2020
Figure 8: Some examples of predictions on Ladder datasets (with large size graphs): the left column
is the real graphs and the right column is the predicted ones.
14
Under review as a conference paper at ICLR 2020
Figure 9: Some examples of predictions on Cycle datasets: the left column is the real graphs and the
right column is the predicted ones.
15
Under review as a conference paper at ICLR 2020
Figure 10: Some examples of predictions on Path datasets with removal: the left column is the real
graphs and the right column is the predicted ones.
16
Under review as a conference paper at ICLR 2020
Figure 11: Some examples of predictions on Cycle datasets with adding extra structures: the left
column is the real graphs and the right column is the predicted ones.
17