Under review as a conference paper at ICLR 2020
Matrix Multilayer Perceptron
Anonymous authors
Paper under double-blind review
Ab stract
Models that output a vector of responses given some inputs, in the form of a
conditional mean vector, are at the core of machine learning. This includes neural
networks such as the multilayer perceptron (MLP). However, models that output a
symmetric positive definite (SPD) matrix of responses given inputs, in the form of
a conditional covariance function, are far less studied, especially within the context
of neural networks. Here, we introduce a new variant of the MLP, referred to as
the matrix MLP, that is specialized at learning SPD matrices. Our construction not
only respects the SPD constraint, but also makes explicit use of it. This translates
into a model which effectively performs the task of SPD matrix learning even
in scenarios where data are scarce. We present an application of the model in
heteroscedastic multivariate regression, including convincing performance on six
real-world datasets.
1	Introduction
For certain applications, it is desirable to construct a conditional covariance matrix as a function of
the input (the explanatory variable). The problem arises, for instance, in spatial (and spatio-temporal)
statistics, in relation to the heteroscedastic multivariate regression (e.g., Pourahmadi, 1999; Hoff &
Niu, 2012; Fox & Dunson, 2015), where we deal with multivariate response measurements for which
the typical assumption of homoscedasticity may not be suitable. In such cases, we require models
that estimate the covariance matrix that captures the spatial variations in correlations between the
elements of the response vector. The covariance matrix is a symmetric positive definite (SPD) matrix
which can be challenging to estimate due to its non-Euclidean geometry (Pennec et al., 2005). The
central problem that this work is concerned with is learning SPD matrices using neural networks.
To motivate our discussion, consider solving the problem of SPD matrix learning using a multilayer
perceptron (MLP) as an example of a fully connected neural network (e.g., Goodfellow et al., 2016).
To meet the SPD constraint, one would need to tailor the output layer of the MLP so that the estimated
covariance matrix satisfies the SPD requirement. One possible approach would be to use the Cholesky
decomposition. The main concern is that this approach does not take into account the non-Euclidean
geometry of the SPD matrices. Using empirical evaluations, we will show that the use of “wrong
geometry” results in the poor estimation of the SPD matrices in particular where data are scarce.
The primary objective here is to design a nonlinear architecture using neural networks that can
effectively perform the task of SPD matrix learning. More specifically, our main contribution is
to show how to alter the architecture of the MLP in such a way that it not only respects the SPD
constraint, but also makes an explicit use of it. We will achieve this by: 1) Explicitly taking the
non-Euclidean geometry of the underlying SPD manifolds (e.g., Pennec et al., 2005) into account by
designing a new loss function, and 2) by deriving a new backpropagation algorithm (Rumelhart et al.,
1986) that respects the SPD nature of the matrices. This new model will be referred to as the matrix
multilayer perceptron (mMLP)1. The mMLP makes use of positive-definite kernels to satisfy the SPD
requirement across all layers. Hence, it provides a natural way of enabling deep SPD matrix learning.
We take a step-by-step approach in the development of the model. We first develop a simplified
version of the resulting model that is designed for learning SPD matrices (Section 3). We then extend
this model into its most general form that can be used for joint estimation of the conditional mean
function and the conditional covariance function in a mean-covariance regression setting (Section 3.2).
An application of the model is discussed in the context of the heteroscedastic multivariate regression.
1An implementation of this work will be made available via GitHub.
1
Under review as a conference paper at ICLR 2020
2	Related Work
SPD manifold metric. Earlier approaches for analyzing SPD matrices relied on the Euclidean
space. But over the past decade, several studies suggest that non-Euclidean geometries such as the
Riemannian structure may be better suited (e.g., Arsigny et al., 2006; Pennec et al., 2005). In this
work, we consider the von Neumann divergence (e.g., Nielsen & Chuang, 2000) as our choice of the
SPD manifold metric which is related to the Riemannian geometry. Previously, Tsuda et al. (2005)
used this divergence in derivation of the matrix exponentiated gradients. Their work suggests its
effectiveness for measuring dissimilarities between positive definite (PD) matrices.
SPD manifold learning. There are multiple approaches towards the SPD matrix learning, via the
flattening of SPD manifolds through tangent space approximations (e.g., Oncel Tuzel, 2008; Fathy
et al., 2016), mapping them into reproducing kernel Hilbert spaces (Harandi et al., 2012; Minh et al.,
2014), or geometry-aware SPD matrix learning (Harandi et al., 2014). While these methods typically
employ shallow learning, the more recent line of research aims to design a deep architecture to
nonlinearly learn target SPD matrices (Ionescu et al., 2015; Huang & Gool, 2017; Masci et al., 2015;
Huang et al., 2018). Our method falls in this category but differs in the problem formulation. While
the previous methods address the problem where the input is an SPD matrix and the output is a vector,
we consider the reverse problem where the input is a matrix with an arbitrary size and the output is
an SPD matrix.
Backpropagation. Our extension of the matrix backpropagation differs from the one introduced by
Ionescu et al. (2015). In their work, the necessary partial derivatives are computed using a two-step
procedure consisting of first computing the functional that describes the variations of the upper layer
variables with respect to the variations of the lower layer variables, and then computing the partial
derivatives with respect to the lower layer variables using properties of the matrix inner product. In
contrast, we make use of the concept of α-derivatives (Magnus, 2010) and its favorable generalization
properties to derive a procedure which closely mimics the standard backpropagation.
3	Matrix Multilayer Perceptron
Preliminaries: Matrix α-derivative. Throughout this work we adopt the narrow definition of the
matrix derivatives known as the α-derivative (Magnus, 2010) in favor of the broad definition, the
ω-derivative. The reason for this is that the α-derivative has better generalization properties. This
choice turned out to be crucial in the derivation of the mMLP’s backpropagation routine which
involves derivatives of matrix functions w.r.t. the matrix of variables.
Definition: Let F be an m × n matrix function of an n × q matrix of variables X. The α-derivative
of F(X) is defined as (Magnus, 2010, Definition 2)
∂ vecF(X)
DXF := 布------^ττ>,	(I)
∂ (vecX)>
where DX F is an mp × nq matrix which contains all the partial derivatives such that each row
contains the partial derivatives of one function with respect to all variables, and each column contains
the partial derivatives of all functions with respect to one variable.
For convenience, the α-derivative' basic properties, including the product rule and the chain rule, are
summarized in Appendix B.
3.1	The Basic Form of the mMLP
Activation matrix function. Let Z = (z1, . . ., zd) denote a matrix of variables zi ∈ Rd.
The activation function K(Z) defines a matrix function in the form of [K(Z)]i,j = κ(zi, zj),
∀i, j ∈ {1, . . ., d}, where κ is some differentiable activation function outputting scalar values. In the
following, we restrict ourselves to the kernel functions which form PD activation matrix functions.
For numerical stability reasons, irrespective of the functional form of κ, we normalize the resulting
matrix. This can be achieved by enforcing the trace-one constraint, H(Z) = K(Z)/tr(K(Z)), where
H denotes a differentiable SPD activation matrix function of trace one. Without loss of generality,
throughout this work, we use the Mercer sigmoid kernel (Carrington et al., 2014) defined as
κ(zi,	zj) = tanh(αzi + β)	tanh(αzj + β),
(2)
2
Under review as a conference paper at ICLR 2020
where α and β denote the slope and the intercept, respectively. Furthermore, denotes the dot
product. In all experiments, we use default values of α = 1 and β = 0. The The α-derivative of the
Mercer sigmoid kernel is computed in Appendix E.
Model construction. Let X ∈ Rp1×p2 indicate the input matrix and Y ∈ Rd0×d0 indicate the
corresponding output matrix, an SPD matrix of trace one. The mMLP of L hidden layers is shown as
mMLP: X → Yb and constructed as
Yb = H(Z0),
Hl = H(Zl),
HL+1 = H(ZL+1),
Z0 =W0H1W0> +B0,
Zl = WlHl+1Wl> + Bl, ∀1 ≤l ≤ L,
ZL+1 = WL+1vecX(WL+1 1p1p2 ) + BL+1 .
(3)
The pair of Wl∈Rdl×dl+1,∀0 ≤l≤ L, and WL+1 ∈ RdL+1×p1p2 are the weight matrices,
Bl ∈Rdl ×dl , ∀0 ≤ l ≤ L+1, are the bias matrices, Zl ∈ Rdl ×dl , ∀0 ≤l ≤ L+1, are the latent input
matrices, and Hl ∈ Rdl ×dl , ∀1 ≤ l ≤ L + 1, are latent output SPD matrices of trace one.
Design choice. In the construction of (3), we have ensured that Hl are SPD matrices of trace
one across all layers as opposed to only at the output layer. The idea behind this design choice
is to propagate the nonlinearities introduced via the SPD activation matrix functions through all
layers. This design choice turned out to be more effective than the alternative, and arguably simpler,
design where the SPD requirement is met only at the output layer. We will discuss this further in
Section 5.1.3, where we also present an illustrative numerical example.
Loss function. We consider the normalized von Neumann divergence (e.g., Nielsen & Chuang,
2000), also commonly known as the quantum relative entropy (QRE), as the base for the loss function.
For two arbitrary SPD matrices of trace one, Φ and Φ, the normalized von Neumann divergence is
defined as:
Δqre (Φ∣∣Φ) = tr(ΦlogΦ — ΦlogΦ),
(4)
where log denotes the matrix logarithm (taking Φ as an example, it is computed using
logΦ = Vdiag(logλ)V>, where V and λ are the matrix of eigenvectors and the vector of eigenval-
ues from the eigendecomposition of Φ). The von Neumann divergence is asymmetric. However, it
can be symmetrized by using the fact that the von Neumann entropy of trace one follows the class of
generalized quadratic distances (Nielsen & Nock, 2007). Hence, we define the loss function as2
1
'qre(Y, Y) = 2(Δqre(Y∣∣Y) + Δqre(Y∣∣Y )).
(5)
Taking α-derivative of 'qre involves taking partial derivatives through the eigendeComPoSition. In
the following, We derive a method for analytically computing the derivative of 'qre.
The α-derivative of the symmetrized von Neumann divergence. For the loss function defined
in (5), using the a-derivative's product rule and chain rule (refer to Appendix B), we obtain
11
Dγ' = 2DYtr((Y - Y)logY) - qDYtr(YlogY),
where the above two terms are computed using
>
/
vec
_	, , ■^■	■^■ . .	■^■ . ~Γ . ~Γ
DY(tr((Y - Y)logY)) = (veC(IogY)>)> +
vec
z
|
z
DY (tr(Y logY)) = (vec(logY)>)>.
(6)
(7)
(8)
z
1×d20
2Note that there are multiple ways of symmetrizing the von Neumann divergence. Our choice in (5) resembles
to the α-Jensen-Shannon divergence for α = 1 (Nielsen, 2010).
3
Under review as a conference paper at ICLR 2020
∂
The remaining part in the computation of (6) is to evaluate ^d-logY, for all i,j ∈ {1,..., d0},
∂ Yij
which involves taking derivatives through the eigendecomposition. In the following, we take a similar
approach as in Papadopoulo & Lourakis (2000) to compute the necessary partial derivatives.
Let Yb = Υdiag(λ1, . . . , λd0)Υ> be the eigendecomposition. We can write
~d- IogY
∂Ybij
∂ ∂Ybij	YΛY> , where Λ	= diag(logλ1, .	..,logλd0),
∂Y	∂Λ ΛY> + Y;^Y>	∂Y>	
		+ YΛ———.	
∂Ybij	∂Ybij	∂Ybij	
(9)
By multiplying (9) from left and right by Υ> and Υ respectively, we obtain:
Υ> -Id-IogY Y = Υ>-dγΛ + 4λ + Λdγ>Y = Ξij(Υ)Λ + 4λ - ΛΞij(Υ), (10)
∂Yb ij	∂Ybij	∂Ybij	∂Yb ij	ij	∂Yb ij	ij
where We have defined Ξj(Y) = Y> -S- Y and used the fact that Ξj(Y) is an antisymmetric
∂Yij
matrix, Ξij(Y) + Ξi>j (Y) = 0, which in turn follows from the fact that Y is an orthonormal matrix,
∂Y>	∂Y
Y>Y = I do ⇒ k Y + Y> -r = Ξ> (Y) + Ξij (Y) = 0.
∂Yij	∂Yij
Taking the antisymmetric property of Ξij(Y) into account in (10), we obtain
∂
-logλk = YikYjk ,
∂Yij
Y Y +Y Y
Ξij (Yk) = YHl±YLYk,	∀l = k.
2(logλl - logλk)
(11)
(12)
(13)
It is notable that by construction, we do not have repeating eigenvalues, that is λk 6= λl , ∀k 6= l, so
there exists a unique solution to (13). Once Ξij (Υ) is computed, it follows that
∂Y	∂Y>
∂Yj = $(Y),	∂Y- = Tj(Y)Y>.
(14)
In summary, the necessary partial derivatives for computing (9) is given by (12) and (14). Once (9) is
computed for all i, j , we can evaluate (7) and ultimately evalaute (6).
Optimization. The remaining steps are feed-forward computation, backpropagation, and learning,
as in the standard MLP. However, here, the backpropagation requires taking derivatives with respect
to the matrix functions. These steps are described in Appendix C.
3.2	The General Form of the mMLP
We now discuss a general version of the mMLP which produces both a vector and an SPD matrix as
outputs. An important application of this model is in heteroscedastic multivariate regression which
we will discuss in Section 5.2.
Model construction. As before, let X ∈ Rp1 ×p2 denote the input matrix. The corresponding
outputs in this case are: an SPD matrix of trace one Y ∈ Rd0 ×d0 and y ∈ Rr0. The mMLP of L
) ∙ i i 1	∙ t	t -I HAlC rτ∙ r^ -O-τ i	t
hidden layers is denoted by mMLP: X→{y, Y} and constructed as:
∕^ y = Y 二	h(z0), H(Z0),	z0 = Z0 =	C0YA0h1 + b0, W0H1W0> +B0,
hl =	h(zl),	zl=	Cl Hl Al hl+1 + bl,	∀ 1 ≤ l ≤ L,
)Hι	= H(Zl),	Zl=	WlHl+1Wl> + Bl,	∀1 ≤ l ≤ L,
hL+1 = h(zL+l),		zL+1	= CL+1HL+1AL+11 + bL+1,
.Hl+i= H(Zl+i),		ZL+1	= WL+1vecX(WL+11p1p2 ) + BL+1
(15)
where	hl∈ Rrl , Hl∈ Rdl ×dl , ∀1 ≤ l ≤ L + 1,	zl, bl∈ Rrl,	Zl, Bl∈ Rdl×dl,
Cl∈Rrl ×dl ,∀0≤ l ≤ L+1, Al∈ Rdl ×rl+1, and Wl∈Rdl ×dl+1 , ∀0 ≤ l ≤ L. Just as in the
standard MLP, h is an activation function of choice, e.g., the hyperbolic tangent function.
4
Under review as a conference paper at ICLR 2020
Loss function. The loss function here needs to be designed with the specific application in mind.
In the case of the heteroscedastic multivariate regression, the loss can be defined based on the
log-likelihood. This will be discussed in Section 4.
Optimization. The remaining steps of feed-forward computation, backpropagation, and learning,
are all described in Appendix D.
4 The mMLP in Heteroscedastic Multivariate Regression
In this section, we discuss an application of the mMLP in relation to the heteroscedastic multivariate
regression, more specifically, the joint mean-covariance regression task.
Model construction. Let Dtrain = {y(i), x(i)}in=1 be our training dataset consisting of a set of
inputs xi ∈ Rdx and a set of responses yi ∈ Rdy . Consider the following multivariate regression
problem: y(i) = f (x(i)) + e(i), where f is a nonlinear function, and e(i) is the additive noise on the
i-th response measurement. The goal is estimation of the conditional mean function E[y | xj and the
conditional covariance function Var[y | x*] for an unseen input x* ∈ Dtest.
We consider two likelihood models based on our choices of the noise model, namely, the multivariate
Gaussian model and its generalization the multivariate power exponential model.
Multivariate Gaussian model. We first consider the noise model to follow a zero-mean multivariate
Gaussian (mG) distribution with a dense covariance matrix, that is ei 〜N(0, ∑i). Let ∑i = %Ωi,
where η = tr(∑i) and Ω% is a trace-one matrix. The noise model can accordingly be reformulated
as ei 〜Ntri(0, Ωi,5)where NtrI is referred to as the trace-one mG distribution3. Although these
two formulations of the mG distribution are identical, we find it easier to work with the latter. This is
because the output layer of the mMLP model in (15) operates under the trace-one constraint. It is
of course possible to drop the trace-one constraint from the output layer, but we would then also be
forced to use a different kernel function than the Mercer sigmoid kernel in that layer. Instead, we find
it easier to work with this reformulated mG distribution with a trace-one covariance matrix which
allows us to use the same choice of kernel function (2) across all layers.
Given ei 〜N(0, Ωi,ηi), the likelihood is defined as
'Ntri := log Ntri(y; μ, Ω,η),	mMLPθ : X → {μ, Ω,η},	(16)
where E[y | x] = μ, and Var [y | x] = ηΩ. The set θ includes all the neural network parameters.
Multivariate power exponential model. We next consider the noise model to follow a zero-mean
multivariate power exponential (mPE) distribution (e.g., G6mez et al., 1998) which is a generalized
variant of the mG distribution, that is e(i)〜E(0, ∑i,αi,βi) where ∑i is a symmetric real dispersion
matrix and the pair of αi ∈ R+ and βi ∈ R+ control the tail and the shape of the distribution. As a
special case, for α = 1 and β = 1, the mPE includes the mG distribution3 4.
As in the case of the mG distribution, we find it easier to work with a reformulated variant where
the dispersion matrix is of trace-one. Let ∑i = ηiΩi, where ηi = tr(∑i). The noise model can
accordingly be represented as e(i)〜 Etri(0, Ω%, αi, βi, ηi) where Eg is referred to as the trace-one
mPE distribution5.
Given e(i)〜Etri (0, Ωi, a%,βi, ηi), the likelihood is defined as:
'Etri ：= log Etri(y； μ, Ω,α,β,η), mMLPθ : X → {μ, Ω,α,β,η},	(17)
where E[y | x] = μ and Var[y | x] = ηαν(β)Ω, where V(β)
the neural network parameters.
21/e Γ( d⅜ )
d γ(2dβ )
The set θ includes all
Optimization. Finally, the optimization involves learning the neural network parameters θ by
maximizing the likelihoods given by (16) and (17).
3Refer to Appendix F.1 for the exact functional form of the trace-one mG distribution.
4Figure G.1.1 visualizes the probability density function of the distribution for selected values of α and β.
5Refer to Appendix G for the exact functional form of the trace-one mPE distribution and its basic properties.
5
Under review as a conference paper at ICLR 2020
covariance (SPD) matrices (20 X 20). (B, C) Estimated covariance matrices by the mMLP using
'qre, and using 'quad. (D) Estimated covariance matrices using the CholeSky-based MLP and 'quad.
5	Experiments
Experiments are divided into two parts. The first part is on the empirical validation of the mMLP
model in a supervised task of learning SPD matrices using synthetic data. The second part discusses
our results in heteroscedastic multivariate regression on real-world datasets.
5.1	SPD Matrix Learning
5.1.1	the Choice of Loss Function
Consider the problem of learning SPD matrices on synthetic data using the mMLP model of (3). The
objectives are to validate the model, and to evaluate the effect of the choice of the loss function on
the overall performance.
The following loss functions are considered for this analysis: The first candidate is the loss
function based on the normalized von Neumann divergence 'QRE(Y, Y) given by (5). The
'QRE is related to the Riemannian geometry. The second candidate is the quadratic loss,
'quad (Y, Y) = tr((Y - Y)(Y - Y)>), which is related to the Euclidean geometry.
Example 1. Consider the set Dtrain= {xi,Yi}in=tr1ain of inputs xi ∈ R20 and corresponding SPD
matrix outputs Yi ∈ Rd0×d0 which are in this case dense covariance matrices (refer to Appendix H.1
for details on the data generation). The goal is to estimate the covariance matrices Y associated
to the input vectors from the unseen test set Dtest = {xi}in=te1st . The training size is varied between
ntrain = {102, 104} samples. The analysis is carried out for d0 = {10, 20}. Two examples of the test
target outputs Yi for d0 =20 and ntrain = 102 are visualized in Figure 1-A.
The mMLP models (3) are trained using 3 layers (20 units per layer, dl = 20) under our two choices
of loss functions: 'QRE, and 'quad. All models share the same initialization and the only difference
here is the loss function. Refer to Appendix H.1 for additional details on the mMLP initialization
6. The performance is evaluated on the test set, ntest = 103, in terms of the losses as the error
measures, shown as EQRE and Equad. Table 1 summarizes the results of the evaluation (also refer
to Figure 1-B,C for the visual illustration). The key observation is that the quality of estimates
differs considerably depending on the choice of the loss function. The loss function 'QRE that takes
into account the geometry of the SPD matrices clearly outperforms the one based on the Euclidean
geometry, 'qaud. The advantage is more pronounced for small set of training data.
5.1.2	The mMLP vs. the MLP based on Cholesky decomposition
As we discussed in Section 1, one can take a heuristic approach and tailor the output layer ofa vanilla
MLP so that the resulting output matrix satisfies the SPD requirement.
For the sake of comparison, we solve the same task as in Example 1 using the standard MLP with 3
layers (200 units per layer) and with the quadratic loss. To meet the SPD requirement, we use the 6 *
6Here and in general throughout this section, special care has been made to minimize the effect of overfitting
through trying out various initializations and using early stopping.
6
Under review as a conference paper at ICLR 2020
Table 1: SPD matrix learning on synthetic dataset (Example 1).					
	d0 = 10, ntrain = 104	d0 = 20,ntrain = 104	d0 = 10, ntrain = 102	d0 = 20,	，ntrain = 102
Model/Loss	Equad	EQRE	Equad	EQRE	Equad	EQRE	Equad	EQRE
mMLP/'QRE	1.2 ×10-9 5.8×10-6	3.3× 10-8 7.7×10-5	6.3×10-6	1.1× 10-4	4.3× 10-	5 2.2×10-3
mMLP/'quad	2.3 × 10-4 2.1 × 10-2	2.1 × 10-3 3.9 × 10-2	3.7 × 10-2 6.6 × 10-1	6.0 × 10-	2	1.2
MLP/'quad	2.5 × 10-3 3.4 × 10-1	1.3 × 10-2 8.1 × 10-1	6.5 × 10-1 4.5	7.3 × 10-	1	6.4
Table 2: Shallow vs deep architecture, Example 2. Error is measured in terms of EQRE .
Model	d0	= 10		d0 =		20	
	L=2	L = 4	L = 6	L=2	L=	4	L=6
Deep architecture (3)	2×10-4	1×10-5	4× 10-5	5×10-3	3×10	-4	6 × 10-4
Shallow architecture (18)	4×10-3	5×10-3	4×10-2	8×10-2	6×10	-2	7×10-1
Cholesky decomposition at the output layer using the known result that for a SPD matrix there is a
unique lower triangular matrix, with ones as its diagonal entires, and a unique diagonal matrix with
positive diagonal entires. Table 1 summarizes the results of the evaluation. Overall the performance is
quite poor for small set of training data. As the size of training data grows, the performance improves
as expected (refer to Figure 1-D for the visual illustration).
5.1.3	Shallow vs Deep SPD Matrix Learning
The design of the mMLP model in (3) enables a mechanism for deep SPD matrix learning by
satisfying the SPD constraint across all input, hidden and output layers. The simpler approach would
be to consider the standard MLP architecture across input and hidden layers but make use of the
activation matrix functions only at the output layer to meet the SPD requirement:
(Y = H(Zo),	Zo = W0h1(W0l)> + Bo,
hl = h(zl),	zl = Wlhl+1 + bl,	1≤l≤L,	(18)
(hL+1 = h(zL+ι),	ZL+1 = WL+ιvecX + bL+1.
This amounts to a shallow design in the sense that it does not enable a mechanism for preserving the
SPD constraint across all layers during the learning.
The design in (3) allows nonlinearities to pass through layers via activation function matrices
imposing the SPD constraint, whereas in the shallow design, nonlinearities are propagated across
layers via activation functions without imposing any constraints. Our hypothesis is that the former
has advantage over the latter in that it captures complex dependencies which are important for the
SPD matrix learning. Below, we present a numerical example which indeed highlights the importance
of preserving the SPD constraint across all layers when learning the SPD matrix.
Example 2. Consider a similar experiment as in Example 1 for the case of ntrain = 102 and output
dimensions do = {10, 20}. We directly compare the performance of (3) against (18) under different
number of hidden layers L = {2, 4, 6}. For the mMLP model, the number of hidden units at each
layer is set to 20, and for the MLP model it is set to 200 units. The shallow design (18) uses the
hyperbolic tangent as the activation function h(∙). The same choice of the activation matrix function
H(∙), given by (2), is used for both models. We use 'qre as the choice of the loss function for
both models (refer to Appendix H.2 for additional details on the initialization). The performance is
evaluated in terms of EQRE .
Table 2 summarizes the results of the evaluation. Although the shallow design (18) performs relatively
well, it underperforms in comparison to (3). Given the limited number of training samples, arbitrarily
increasing the number of layers may not be necessarily advantageous, which is the case for both
models. However, in this regard, the design in (18) is more sensitive.
5.2	Experimental results on the multi-output regression datasets
In heteroscedastic multivariate regression, the central task is to estimate the conditional covariance
matrix that captures the spatial variations in correlations between the elements of the response vector.
The underlying hypothesis is that if a model can effectively capture the heteroscedasticity, then it will
provide better uncertainty quantification, and the estimation of the conditional mean response should
also improve, in comparison to the model that is build based on the homoscedasticity assumption.
7
Under review as a conference paper at ICLR 2020
Table 3: Summary of the regression methods used in the experiments.
Method		Model specifications
Acronym	Source	
mMLP-mG mMLP-mPE MLP	Section 4 Section 4 Goodfellow et al. (2016)	3 layers (rl = 200, dl = 20), Adam optimizer, activation functions asin Table 4 3 layers (rl = 200, dl = 20), Adam optimizer, activation functions asin Table 4 3 layers (200 units), ReLU activation, Adam optimizer, and default initialization of the scikit-learn1
GP	Rasmussen & Williams (2006)	RBF kernel and default initialization of the sckit-learn1
NBCR	Fox & Dunson (2015)	105 Gibbs iterations (discarded the first half), truncation of 5, and default initialization of the toolbox2.
1 SCikit-learn: https://scikit-learn.org/
2 NBCR: https://homes.cs.washington.edu/ ebfox/software/
Table 4: Choice of activation functions for the mMLP-mPE and mMLP-mG models in Table 3.
Model	Output layer					Hidden layers
	μ	logη	α	β	Ω	h	H
mMLP-mG	linear()	linear()	-	一	*	linear()	*
mMLP-mPE	linear()	linear()	0.5+Sigmoid()	0.5+Sigmoid()	*	linear()	*
* Trace-one MerCer Sigmoid: H(Z)= tr^K(ZZ))) , where [K(Z)]i,j = κ(zi, Zj), ∀i,j. The kernel function κ(∙, ∙) is given by (2).
Table 5: Datasets and regression results.
(a) Dataset specifications.
Dataset		dx	dy	ntrain	ntest
Name	Appendix				
oes10	H.3.1	298	16	50	352
edm	H.3.2	2	16	50	103
atp1d	H.3.3	411	6	100	236
atp7d	H.3.3	411	6	33	303
scm1d	H.3.4	280	16	1000	8802
scm20d	H.3.4	61	16	1000	8802
(b) Prediction errors in terms of the average RMSE
across 10 runs. The bold-faced numbers indicate the
statistical significance (p-value≤ 0.05).
Dataset	mMLP-mPE	mMLP-mG	MLP	GP	NBCR
oes10	0.51	052	0.59	0.97	1.2
edm	0.19	0.18	0.29	0.32	0.41
atp1d	0.49	0.58	0.61	0.89	large
atp7d	0.53	0.63	0.70	0.97	large
scm1d	0.69	0.65	0.73	0.82	1.5
scm20d	0.67	0.72	0.78	0.87	1.7
For this purpose, we compare our mMLP-based regression models against another heteroscedasti-
based mean-covariance regression model by Fox & Dunson (2015) and two homoscedastic-based
mean-regression models, namely, the MLP regressor and the Gaussian process (GP). The models
used in this experiment are summarized in Table 3.
Real-world datasets. We compare the performance of the heteroscedastic and homoscedastic
regression models on six real-world multi-output datasets. Key features of the datasets are summarized
in Table 5a. The performance is evaluated in terms of the root-mean-square error (RMSE) on test
sets, shown in Table 5b. The result suggests that the mMLP homoscedastic-based regression models
are capable of capturing dependencies between the output measurements which contributes to the
better estimation of the mean predictions.
6	Limitations and Future Work
The main limitation of the mMLP has to do with scalability to higher dimensions. The complexity
associated with computing the α-derivative of the von Neumann loss function (5) at the output layer is
O(d03). Taking the symmetric nature of the SPD matrices into account, the computational complexity
at the hidden layer l reduces to O(dl2).
Our implementation of the matrix backpropagation involves the use of multiple Kronecker products.
Although it facilitates the implementation, we would need access to the full Jacobian matrices
(dl2 × dl2). However, these matrices are in fact available in the form of sparse block matrices, which
means that it is possible to implement a memory-efficient computation of the tensor products without
the need to actually have access to the full matrices. Future work is needed in this direction.
We believe that there are many cases in which the mMLP can prove to be useful. An interesting
direction for future work is to investigate application of the model in the context of the conditional
density estimation within the framework of variational autoencoders.
7	Discussion
We introduced a new method to learn SPD matrices, referred to as the matrix multilayer perceptron
(mMLP). The mMLP takes the non-Euclidean geometry of the underlying SPD manifolds into
account by making use of the von Neumann divergence as the choice of the SPD manifold metric.
One key aspect of the mMLP is that it preserves the SPD constraint across all layers by exploiting
PD kernel functions and a backpropagation algorithm that respects the inherent SPD nature of the
matrices. We studied application of the model in the context of heteroscedastic multivariate regression.
We showed the effectiveness of the proposed model on multiple real-world datasets.
8
Under review as a conference paper at ICLR 2020
References
Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric means in a novel
vector space structure on symmetric positive-definite matrices. SIAM Journal of Matrix Analysis
Applications, 29(1):328-347, 2006.
Andre M. Carrington, Paul W. Fieguth, and Helen H. Chen. A new Mercer sigmoid kernel for clinical
data classification. In International Conference of the IEEE Engineering in Medicine and Biology
Society, 2014.
Mohammed E. Fathy, Azadeh Alavi, and Rama Chellappa. Discriminative log-Euclidean feature
learning for sparse representation-based recognition of faces from videos. In IJCAI, 2016.
Emily B. Fox and David B. Dunson. Bayesian nonparametric covariance regression. Journal of
Machine Learning Research, 16(12):2501-2542, 2015.
E G6mez, Miguel Gomez-Villegas, and J Marin. A multivariate generalization of the power expo-
nential family of distributions. Communications in Statistics-theory and Methods, 27(3):589-600,
1998.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
Mehrtash T. Harandi, Conrad Sanderson, Richard Hartley, and Brian C. Lovell. Sparse coding and
dictionary learning for symmetric positive definite matrices: A kernel approach. In ECCV, 2012.
Mehrtash T. Harandi, Mathieu Salzmann, and Richard Hartley. From manifold to manifold: Geometry-
aware dimensionality reduction for SPD matrices. In ECCV, 2014.
Peter D. Hoff and Xiaoyue Niu. A covariance regression model. Statistica Sinica, 22(2):729-753,
2012.
Zhiwu Huang and Luc Van Gool. A Riemannian network for SPD matrix learning. In AAAI, 2017.
Zhiwu Huang, Jiqing Wu, and Luc Van Gool. Building deep networks on Grassmann manifolds. In
AAAI, 2018.
Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation for deep
networks with structured layers. In ICCV, 2015.
A. Karalic and I. Bratko. First order regression. Machine Learning, 26(2-3):147-176, 1997.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In ICLR, 2014.
Jan R. Magnus. On the concept of matrix derivative. Journal of Multivariate Analysis, 101(9):
2200-2206, 2010.
Jonathan Masci, Davide Boscaini, Michael M. Bronstein, and Pierre Vandergheynst. Geodesic
convolutional neural networks on Riemannian manifolds. In ICCVW, 2015.
Ha Quang Minh, Marco San-Biagio, and Vittorio Murino. Log-Hilbert-Schmidt metric between
positive definite operators on Hilbert spaces. In NeurIPS, 2014.
Frank Nielsen. A family of statistical symmetric divergences based on Jensen’s inequality. arXiv,
2010.
Frank Nielsen and Richard Nock. On the centroids of symmetrized Bregman divergences. CoRR,
2007.
Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Information. Cam-
bridge University Press, 2000.
Peter Meer Oncel Tuzel, Fatih Porikli. Pedestrian detection via classification on Riemannian man-
ifolds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(10):1713-1727,
2008.
9
Under review as a conference paper at ICLR 2020
Theodore PaPadoPoUlo and Manolis I. A. Lourakis. Estimating the Jacobian of the singular value
decomposition: Theory and applications. In ECCV, 2000.
Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian framework for tensor comPuting.
International Journal ofComputer Vision, 66(1):41-66, 2005.
Mohsen Pourahmadi. Joint mean-covariance models with aPPlications to longitudinal data: Uncon-
strained Parameterisation. Biometrika, 86(3):677-690, 1999.
C.	E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. New York, NY,
USA, 2006.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning rePresentations by
back-ProPagating errors. Nature, 323(10):533-536, 1986.
Eleftherios SPyromitros-Xioufis, Grigorios Tsoumakas, William Groves, and Ioannis Vlahavas.
Multi-target regression via inPut sPace exPansion: treating targets as inPuts. Machine Learning,
104(1):55-98, 2016.
Koji Tsuda, Gunnar Ratsch, and Manfred K. Warmuth. Matrix exponentiated gradient updates for
on-line learning and Bregman Projection. Journal of Machine Learning Research, 6(1):995-1018,
2005.
10
Under review as a conference paper at ICLR 2020
A	Matrix notations
We use > for the transpose operator, tr(∙) for the trace operator, and det(∙) for the matrix determinant.
The symmetric part ofa square matrix B is denoted by sym(B) = (B + B>)/2. The Kronecker prod-
uct is denoted by ⑼ the Hadamard product by ◦, and the dot product by Θ. We use the vec-operator
for column-by-column stacking of a matrix A, shown as vecA ≡ vec(A). Let A be an m × n
matrix, the operator P(m,n) will then rearrange vecA to its matrix form: A = P(m,n)(vecA). For the
m × n dimensional matrix A, the commutation matrix is shown as K (m,n) which is the mn × mn
matrix that transforms vecA into vecA> as: K (m,n) vecA = vecA>. An m × m identity matrix is
shown as Im. If T(X) : Rd×d → R is a real-valued function on matrices, then VχT(X) denotes
the gradient with respect to the matrix X, VχT(X) = [∂∂Xɛ]2 j=]:d. The matrix logarithm and the
matrix exponential are written as logA and expA, respectively. The matrix exponential in the case of
symmetric matrices can be expressed using the eigenvalue decomposition as expA = V(expΛ)V>,
where V is an orthonormal matrix of eigenvectors and Λ is a diagonal matrix with the eigenvalues
on the diagonal. The matrix logarithm is the inverse of the matrix exponential if it exists. If A
is symmetric and strictly positive definite (PD) it is computed using logA = V(logΛ)V>, where
(logΛ)i,i = logΛi,i.
B The α-DERIVATIVE: Definition and properties
Definition Let F be an m × n matrix function of an n × q matrix of variables X. The α-derivative
of F(X) is defined as (Magnus, 2010, Definition 2)
DXF :
∂ VecF(X)
∂ (VecX)> ,
(19)
where DXF is an mp × nq matrix which contains all the partial derivatives such that each row
contains the partial derivatives of one function with respect to all variables, and each column contains
the partial derivatives of all functions with respect to one variable.
Product rule Let F (m × p) and G (p × r) be functions of X (n × q). Then the product rule for
the α-derivative is given by (Magnus, 2010)
DX(FG) = (G> 乳 Im)DχF + (Ir 乳 F)DχG.	(20)
Chain rule Let F (m × p) be differentiable at X (n × q), and G (l × r) be differentiable at
Y = F(X), then the composite function H(X) = G(F(X)) is differentiable at X, and
DXH = DYGDXF,
(21)
which expresses the chain rule for the α-derivative (Magnus, 2010).
C The basic case of the mMLP
C.1 Feedforward step
At the feed-forward computation, we compute and store the latent outputs Yb , Hl for all
l ∈ {L + 1, . . . , 1} using the current setting of the parameters, which are Wl, and Bl computed from
the learning step, Appendix C.3.
C.2 Backpropagation step
We first summarize the necessary α-derivatives for the backpropagation, and then write down the
backpropagation procedure accordingly.
Derivatives required for backpropagation The derivative of the activation matrix function de-
pends on the specific choice of kernel function, and in general it is computed readily from the
11
Under review as a conference paper at ICLR 2020
definition of α-derivative,
DZl H(Zl) = ： VeCH(R,	l ∈{0,...,L +1}.	(22)
∂ (vecZl )
For our specific choice of activation function, the Mercer Sigmoid kernel (2), it is computed in
Appendix E.
Via repeated use of the product rule of α-derivatives (20), we obtain
DWlZl =	(Wl 乳	Idl)(Hl+ι ㊈	Idl)	+ (Idl	㊈(WlHl+ι))K如g),l ∈	{0,...,L},	(23)
DWL+1Zl = (WL+1
1p1 p2 乳 IdL+I)((VeCX)T 乳 IdL+ι)
+ (IdL+ι ㊈(WL+ιvecX))(IdL+ι ㊈ I>1p2)K也十山「2)，(24)
DHl+ιZl = (Wl 0 Idl)(Idl+ι 0 Wl), l ∈ {0,..., L},	(25)
where K is the commutation matrix (refer to Appendix A for a summary of the matrix notation).
Backpropagation In the interest of simple expressions, let H0 ≡ Yb . Backpropagation to the
hidden layer l is computed recursively using the derivatives computed at the previous layer according
to
Dzl' =	DHl 'Dzl Hl,	∀l ∈ {0,. ..,L+ 1},	(26)
DWl' =	:DZl 'Dwl Zl,	∀l ∈ {0,. ..,L+ 1},	(27)
DHl + 1 '	=Dzl 'DHl+ι Zl,	∀l ∈ {0, . . . , L},	(28)
DBl' =	DZl',	∀l ∈ {0, . . . , L + 1}.	(29)
C.3 Learning step
Learning involves updating the weights Wl and the biases Bl using derivatives computed during the
backpropagation step. These are updated using derivatives Dwl' and Db" for a given learning rate
η as
Wl J Wl- ηP(dl,dl+ι)(Dwl'),	∀l ∈{0,...,L + 1},	(30)
Bl J Bl- ηP(dl,dl)(DBl'),	∀l ∈{0,...,L + 1},	(31)
where P is the rearrangement operator introduced in Appendix A.
D The general form of the mMLP
D.	1 Feedforward step
The forward path involves computing and storing both hl, yb and Hl, Yb using the current settings of
the parameters, for all l ∈ {0, . . . , L + 1}.
D.2 Backpropagation step
Most of the necessary derivatives are identical to the ones in Appendix C.2. However, there are
some additional derivatives needed which we will discuss in the following. We then write down the
backpropagation formula.
D.2. 1 Required derivatives for backpropagation
The derivative of the activation function depends on the choice of the function, and it is computed
using the definition of the α-derivative,
Dzlhl(zl) = d^⅛,	l ∈{0,...,L +1}.	(32)
l	∂ (zl)>
12
Under review as a conference paper at ICLR 2020
The other required derivatives are computed as
DAl Zl =	ClHl,	l ∈ {0, .	. . , L},	(33)
DAL+1Z	L+1 = (Cl+iHl+i)(1>l+ 1 氧 IdL+1 ),			(34)
DCl Zl =	二 (HlAh l + ι)> 乳 I ri,	l ∈ {0, .	. . , L},	(35)
DCL+1Z	L+1 = (HL+1AL+11rL+1)丁 0 IrL + ι ,			(36)
Dhl+1 Zl	= ClHlAl,	l ∈ {0, .	. . , L},	(37)
DHlZl =	((Alhl+1)>0Irl)(Idl 0 Cl),	l ∈ {0, .	. . , L},	(38)
DHL+1 Z	L+1 = ((AL+11L+1)> 0 I rL+1)(I dL+1 0 CL+1).			(39)
Backpropagation For simplicity of expressions, let h0 ≡ yb and H0 ≡ Yb . The derivatives are
recursively computed as
Dho' ≡	Dy '			(40)
Dho' ≡	DY' = Dz0 'Dγ Z0 + DY '			(41)
Dzl ` =	Dhl 'Dzl hl,	∀l ∈ {0, .	..,L+1},	(42)
Dhl + 1 '	=DZl 'Dhl + 1 zl,	∀l ∈ {0, . . . , L},		(43)
DZl' =	DHl 'Dzl Hl,	∀l ∈ {0, .	..,L+1},	(44)
Dhi + 1 '	= Dzl+1'DHl+ ιzl + 1 + DZl'DHl+ ι Zl,	∀l ∈ {0, . . . , L},		(45)
DAl' =	Dzl 'DAlzl,	∀l ∈ {0, .	. . , L + 1},	(46)
DCl' =	DZl 'Del zl,	∀l ∈ {0, .	. . , L + 1},	(47)
DWl' =	DZl 'DWl Zl,	∀l ∈ {0, .	. . , L + 1},	(48)
Dbl' = DZl',		∀l ∈ {0, .	. . , L + 1},	(49)
DBl' =	DZl',	∀l ∈ {0, .	. . , L + 1}.	(50)
D.3 Learning step
The learning step involves updating the weights and the biases which are computed using derivatives
computed from the backpropagation step. Update rules for Wl, and Bl are the same as the ones
given in Appendix C.3. The remaining parameters are learned in a similar fashion,
Al 一 Al — ηP(dl,rl + ι)(DAl'),	∀l ∈ {0, .	. . ,L+ 1},	(51)
Cl 一 Cl - ηP(rl,dl)(Del'),	∀l ∈ {0, .	. . ,L+ 1},	(52)
bl J bl - ηP(rl,1)(Dbl'),	∀l ∈ {0, .	. . ,L+ 1}.	(53)
E The α-DERIVATIVE of the Mercer sigmoid kernel
The α-derivative of the Mercer sigmoid kernel can be computed as
	(…	• • • ∂ κmn	•••			
DZH =		∂zi trK 1 - - }		,	∀ i, m, n ∈ {1,.	. . ,dl},	(54)
	v∙∙	{z 1×dl •••	•••			
where zi indicates the ith column of Z, trK ≡ trK(Z), κmn ≡ κ(zm, zn) as defined in (2), and
∂ κmn
∂zi trK
(α(1>f⅜° Mi)) ) ◦ (trK f (夫(Zm ◦ Zn)) - 25(&))，
(55)
wheref(Zi) := tanh(αZi - β).
13
Under review as c conference paper at ICLR 2020
(10.0,0.01) Ie-190
(IG0,0.5)
(1Q.0, IQ)
0.045
0.040
0.035
0.030
0.025
0.020
0.015
0.010
0.005
0.000
0.011
0.010
0.009
0.008
0.007
0.006
0.005
0.004
0.003
(10.0,2.0)	(10.0,10.0)
---∏ O-"	.	π 0.090
L % L、≡
I--	∙ ■ 0.00	- U ■ 0.000
Figure G.1: The probability density function of a trace-one mPE distribution, Etri (μ, M/η,η,ɑ,β)
for fixed values of μ, M, η = tr(M) and varying values of scale and shape parameters (α, β). When
α = 1 and β = 1, the density corresponds to the multivariate Gaussian distribution N(μ, M).
F TRACE-ONE MULTIVARIATE GAUSSIAN DISTRIBUTION
F.1 PROBABILITY DENSITY FUNCTION
For a d-dimensional random variable Q ∈ Rd, We define the trace-one Gaussian distribution according
to
Ntri(Q;…=2：.1 e-2 (j)")-1"),	(56)
where μ ∈ Rd is the mean, η ∈ R+ is the scale parameter, and Ω ∈ Rd×d is the trace-one covariance
matrix, tr(Ω) = 1.
F.2 The α-DERIVATIVES of the log-pdf
The α-derivatives of the trace-one Gaussian distribution’s log-pdf with respect to its parameters are
summarized as
DnlogNtri("； μ, Ω,η) = - 1(vec(Ω-1))> - ](Dnt)>,	(57)
Dnt = -vec((ηΩ)-1(Q - μ)(Q - μ)>Ω-i),	(58)
DμlogNtri(Q; μ, Ω, η) = (Q - μ)(ηΩ)-i,	(59)
DlognIogNtri(Q; μ, Ω,η) = -d + 2(Q - μ)>(ηΩ)-1(Q - μ).	(60)
G Trace-one multivariate power exponential (mPE) distribution
G. 1 Probability density function
For a random variable Q ∈ Rd, the trace-one mPE distribution can be expressed as
c(α,β)	1 (t(Q; μ,Ω)λβl
Etri(Q； μ, a,α,β,η)= ,J “ c∖∖i eχp-	,
(det(ηΩ)) 2	2∖	ɑη
d	(61)
c(α,β) = d β?2)a-d,	t(Q;μ, ω) ：=(Q - μ)>ω-I(Q - "), tr(a)= L
π 2 Γ( 2dβ )2 2β α d
Here, μ is the mean vector, and Ω is a d X d symmetric real dispersion matrix where tr(Ω) = 1.
The parameter η has the same role as in the trace-one Gaussian distribution. The pair of α ∈ R+
14
Under review as a conference paper at ICLR 2020
and β ∈ R+ control the tail and the shape of the distribution. As a special case, the mPE includes
the Gaussian distribution: For α = 1 and β = 1, the trace-one mPE distribution corresponds to the
trace-one multivariate Gaussian distribution. Figure G.1.1 visualizes the probability density function
of the distribution for selected values of α and β, for the case of d = 2.
Remark: Very large and very small values of α and β might be undesirable as, for one, they pose
numerical challenges. In practice, these parameters can be restricted within a range by choosing
suitable output activation functions. In all experiments in this paper, we choose to bound them
conservatively as: 0.5 ≤ α, β ≤ 1.5, using the sigmoid function.
G.2 Moments
Let E ∈ Rd and E 〜Etri (μ, Ω, ɑ, β, η). The mPE's mean vector and covariance matrix are Com-
puted from:
21/e Γ( d+2)
E[E] = μ,	V[E] = αην(β)Ω,	V(β) :=	d ： ,	(62)
dr( 2β)
where Γ(∙) denotes the gamma function.
G.3 The α-DERIVATIVES of the log-pdf
It is straightforward to take derivatives of the mPE’s log-pdf using the favorable generalization
properties of the a-derivative's chain and product rules. These are summarized as:
DclogEtri(E; μ, Ω, a, β, η) = - 1(vec(Ω->))	> 一 5^-*∕aη)β-iDΩt, 2aη ,	(63)
Dct = -(vec(Ω-1(E 一 μ)(E - μ)>Ω-1)>)>		(64)
β DμlogEtri (E; μ, Ω, a, β,η) =--- 2aη	(t∕aη)e-1Dμt,	(65)
Dμt = 一2(VeC((E — μ)>Ω-1)>)>,		(66)
DlognIogEtri(E; μ, Ω,a,β,η) = 一d +	Je- (t∕aη)eτ, 2aη	(67)
DalogEtri(E; μ, Ω,a,β,η) = -2d- +	^βt2 (t∕aη)e-i, 2ηa2	(68)
DeIogEtri (E; μ, Ω,a, β, η) = Delogc(a, β) -	-1(t∕aη)β log(t∕aη),	(69)
Delogc(a, β) = 1 + -d2(ψ(d∕2β) + log2). β	2β2		(70)
H Additional details on the experiments
H.1 Example 1
Data generation Let A ∈ Rd0 ×d0 be a matrix where each of its elements is generated from a
standard normal distribution. The matrix A is kept fixed. The ith class covariance Yi is computed
according to the following procedure: 1 2 3
1. Draw 104 samples from a known Gaussian distribution N(μi, ∑i) with a unique mean
μi ∈ Rd0 and a unique dense covariance matrix ∑i ∈ Rd0 ×d0.
2. Let tj be a random sample from this Gaussian. For this sample, compute yj = Atj . For all
104 samples, collect the results into y = {yj}104ι.
3. Compute the sample covariance of y and normalize the resulting covariance matrix to trace
one, that is Yi J cov(y)∕tr(cov(y)).
15
Under review as a conference paper at ICLR 2020
Initialization All models use the same batch size (equal to 5), the same choice of activation matrix
function, which is given by the Mercer sigmoid kernel (2), and the same optimizer (the Adam
optimizer (Kingma & Welling, 2014) with default settings).
H.2 Example 2
Data generation See the data generation procedure in Example 1.
Initialization Both models (18) and (3) use the same batch size (equal to 5), the same choice of
loss function (5), and the same optimizer (the Adam optimizer (Kingma & Welling, 2014) with
default settings). Both models use the same choice of the output activation matrix function, given
by the Mercer sigmoid kernel (2). The model in (18) uses the hyperbolic tangent as the activation
function across the hidden layers, while (3) makes use of the same choice of the activation matrix
function as in its output layer.
H.3 Multi- output regression datasets
H.3.1 oes10
The dataset oes10 was obtained from Spyromitros-Xioufis et al. (2016). The Occupational Em-
ployment Survey (OES) datasets contain records from the years of 2010 (OES10) of the annual
Occupational Employment Survey compiled by the US Bureau of Labor Statistics. As described in
(Spyromitros-Xioufis et al., 2016), "each row provides the estimated number of full-time equivalent
employees across many employment types for a specific metropolitan area". We selected the same 16
target variables as listed in (Spyromitros-Xioufis et al., 2016, Table 5). The remaining 298 variables
serve as the inputs. Data samples were randomly divided into training and test sets (refer to Table 5a).
H.3.2 edm
The dataset edm was obtained from Karalic & Bratko (1997). The electrical discharge machining
(EDM) dataset contain domain measurements in which the workpiece surface is machined by electrical
discharges occurring in the gap between two electrodes: the tool and the workpiece. Given the two
input variables, gap control and flow control, the aim here is to predict the other 16 target variables
representing mean values and deviations of the observed quantities of the considered machining
parameters. Data samples were randomly divided into training and test sets (refer to Table 5a).
H.3.3 atp1d and atp7d
The datasets atp1d and atp7d were obtained from (Spyromitros-Xioufis et al., 2016). The Air-
line Ticket Price (ATP) dataset includes the prediction of airline ticket prices. As described in
(Spyromitros-Xioufis et al., 2016), the target variables are either the next day price, atp1d, or
minimum price observed over the next seven days atp7d for 6 target flight preferences listed in
(Spyromitros-Xioufis et al., 2016, Table 5). There are 411 input variables in each case. The inputs for
each sample are values considered to be useful for prediction of the airline ticket prices for a specific
departure date, for example, the number of days between the observation date and the departure date,
or the boolean variables for day-of-the-week of the observation date. Data samples were randomly
divided into training and test sets (refer to Table 5a).
H.3.4 scm1d and scm20d
The datasets scm1d and scm20d were obtained from (Spyromitros-Xioufis et al., 2016). The Supply
Chain Management (SCM) datasets are derived from the Trading Agent Competition in Supply Chain
Management (TAC SCM) tournament from 2010. As described in (Spyromitros-Xioufis et al., 2016),
each row corresponds to an observation day in the tournament. There are 280 input variables in these
datasets which are observed prices for a specific tournament day. The datasets contain 16 regression
targets, where each target corresponds to the next day mean price scm1d or mean price for 20 days in
the future scm20d for each product (Spyromitros-Xioufis et al., 2016, Table 5). Data samples were
randomly divided into training and test sets (refer to Table 5a).
16