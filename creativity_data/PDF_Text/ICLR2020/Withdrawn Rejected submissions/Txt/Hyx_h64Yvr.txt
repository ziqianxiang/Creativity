Under review as a conference paper at ICLR 2020
Kronecker Attention Networks
Anonymous authors
Paper under double-blind review
Ab stract
Attention operators have been applied on both 1-D data like texts and higher-order
data such as images and videos. Use of attention operators on high-order data
requires flattening of the spatial or spatial-temporal dimensions into a vector, which
is assumed to follow a multivariate normal distribution. This not only incurs exces-
sive requirements on computational resources, but also fails to preserve structures
in data. In this work, we propose to avoid flattening by developing Kronecker atten-
tion operators (KAOs) that operate on high-order tensor data directly. KAOs lead
to dramatic reductions in computational resources. Moreover, we analyze KAOs
theoretically from a probabilistic perspective and point out that KAOs assume
the data follow matrix-variate normal distributions. Experimental results show
that KAOs reduce the amount of required computational resources by a factor of
hundreds, with larger factors for higher-dimensional and higher-order data. Results
also show that networks with KAOs outperform models without attention, while
achieving competitive performance as those with original attention operators.
1	Introduction
Deep learning networks with attention operators have demonstrated great capabilities of solving
challenging problems in various tasks such as computer vision (Xu et al., 2015; Lu et al., 2016),
natural language processing (Bahdanau et al., 2015; Vaswani et al., 2017), and network embed-
ding (Velickovic et al., 2017). Attention operators are capable of capturing long-range relationships
and brings significant performance boosts (Li et al., 2018; Malinowski et al., 2018). The application
scenarios of attention operators range from 1-D data like texts to high-order and high-dimensional
data such as images and videos. However, attention operators suffer from the excessive usage of
computational resources when applied on high-order or high-dimensional data. The memory and
computational cost increases dramatically with the increase of input orders and dimensions. This
prevents attention operators from being applied in broader scenarios. To address this limitation, some
studies focus on reducing spatial sizes of inputs such as down-sampling input data (Wang et al., 2018)
or attending selected part of data (Huang et al., 2018). However, such kind of methods inevitably
results in information and performance loss.
In this work, we propose novel and efficient attention operators, known as Kronecker attention
operators (KAOs), for high-order data, which avoid flattening and operate on high-order data directly.
Experimental results show that KAOs are as effective as original attention operators, while dramat-
ically reducing the amount of required computational resources. In particular, we employ KAOs
to design a family of efficient modules, leading to our compact deep models known as Kronecker
attention networks (KANets). KANets significantly outperform prior compact models on the image
classification task, with fewer parameters and less computational cost. We also investigate the
above problem from a probabilistic perspective. Specifically, regular attention operators flatten the
data and assume the flattened data follow multivariate normal distributions. This assumption not
only results in high computational cost and memory usage, but also fails to preserve the spatial or
spatial-temporal structures of data. Our KAOs, instead, use matrix-variate normal distributions to
model the data, where the Kronecker covariance structure is able to capture relationships among
spatial or spatial-temporal dimensions.
2	Background and Related Work
In this section, we describe the attention and related non-local operators.
1
Under review as a conference paper at ICLR 2020
2.1	Attention Operator
The inputs to an attention operator include a query matrix Q = [qι, q2,…，qm] ∈ Rd×m, a key
matrix K = [k1, k2, …, kn] ∈ Rd×n, and a value matrix V = [v1, v2, …, vn] ∈ Rp×n. The attention
operation computes the responses of a query vector qi by attending it to all key vectors in K and
uses the results to take a weighted sum over value vectors in V . The layer-wise forward-propagation
operation of an attention operator can be expressed as O = V softmax(K T Q). Matrix multiplication
between KT and Q results in a coefficient matrix E = KTQ, in which each element eij is calculated
by the inner product between kiT and qj . This coefficient matrix E computes similarity scores
between every query vector qi , and every key vector kj and is normalized by a column-wise softmax
operator to make every column sum to 1. The output O ∈ Rp×m is obtained by multiplying V with
the normalized E. In self-attention operators (Vaswani et al., 2017), we have Q = K = V . The
computational cost in attention operator is O(m × n × (d + p)). The memory required for storing the
intermediate coefficient matrix E is O(mn). If d = p and m = n, the time and space complexities
become O(m2 × d) and O(m2), respectively. There are several other ways to compute E from Q
and K, including Gaussian function, dot product, concatenation, and embedded Gaussian function. It
has been shown that dot product is the simplest but most effective one (Wang et al., 2018). Therefore,
we focus on the dot product similarity function in this work. In practice, we can first perform
separate linear transformations on each input matrix, resulting in the following attention operator:
O = W V V SoftmaX((W K K )T W QQ), where W V ∈ Rp'×p, W K ∈ Rd'×d, and WQ ∈ Rd'×d For
notational simplicity, we omit linear transformations in the following discussion.
2.2	Non-Local Operator
Non-local operators (Wang et al., 2018) ap- c∕7777∖	ffTΓTΓTfTfTf=
ply self-attention operators on higher-order data	_______.
such as images and videos. Taking 2-D data as 力 件书斗-	C
an example, the input to the non-local operator	匕匕匕匕/ 
is a third-order tensor X ∈ Rh×w×c, where h, w,
hw
w
and c denote the height, width, and number of Figure 1: Conversion of a third-order tensor into a
channels, respectively. The tensor is first con- matrix by unfolding along mode-3. In this example,
verted into a matrix X(3) ∈ Rc× w by unfolding a h × w × c tensor is unfolded into a c × hw matrix.
along mode-3 (Kolda & Bader, 2009), as illus-
trated in Figure 1. Then we perform the attention operation by setting Q = K = V = X(3) . The
output of the attention operator is converted back to a third-order tensor as the final output. One
practical challenge of the non-local operator is its excessive usage of computational resources. If
h = w, the computational cost of a 2-D non-local operator is O(h4 × c). The memory used to store the
coefficient matrix incurs O(h4) space complexity. The time and space complexities are prohibitively
high for high-dimensional and high-order data.
3	Kronecker Attention Networks
In this section, we describe our proposed Kronecker attention operators, which are efficient and
effective attention operators on high-order data.
3.1	Kronecker Attention Operators
We describe the Kronecker attention operators (KAO) in the context of self-attention on 2-D data,
but they can be easily generalized to generic attentions. In this case, the input to the `th layer is
a third-order tensor X(') ∈ Rh×w×c. We propose to use horizontal and lateral average matrices to
represent original mode-3 unfolding without much information loss. The horizontal average matrix
H and the lateral average matrix L are computed as
1h
H = - ∑ X(' ∈ Rw×c
h t1	i：：
i=1
-w
L = — ∑ X('' ∈ Rh×c
j: J j:
w=1
(1)
where X(' and Xj) are the horizontal and lateral slices (Kolda & Bader, 2009) of tensor X('',
respectively. We then form a matrix C by juxtaposing HT and LT as C = [HT, LT] ∈ Rc×(h+w'.
2
Under review as a conference paper at ICLR 2020
w
(a)
w
(b)
Outer
SUm
(c)
Figure 2: Illustrations of regular attention operator (a), KAOKV (b) and KAOQKV (c) on 2-D data. In
the regular attention operator (a), the input tensor is unfolded into a mode-3 matrix and fed into the
attention operator. The output of the attention operator is folded back to a tensor as the final output.
In KAOKV (b), we juxtapose the horizontal and lateral average matrices derived from the input tensor
as the key and value matrices. We keep the mode-3 unfolding of input tensor as the query matrix. In
KAOQKV (c), all three input matrices use the juxtaposition of two average matrices. In contrast to
KAOKV , we use an outer-sum operation to generate the output third-order tensor.
Based on the horizontal and lateral average matrices contained in C, we propose two Kronecker
attention operators (KAOs), i.e., KAOKV and KAOQKV . In KAOKV as shown in Figure 2 (b), we
use X(；) as the query matrix and C as the key and value matrices as
O = attn(X(3)，C, C) ∈ Rc×hw.	(2)
Note that the number of columns in O depends on the number of query vectors. Thus, we obtain
hw output vectors from the attention operation. Similar to the regular attention operator, O is folded
back to a third-order tensor Y(') ∈ Rh×w×c by considering the column vectors in O as mode-3 fibers
of Y('). KAOKV uses Y(') as the output of layer '. If h = w, the time and space complexities
of KAOKV are O(hw × c × (h + w)) = O(h3 × c) and O(hw × (h + w)) = O(h3), respectively.
Compared to the original local operator on 2-D data, KAOKV reduces time and space complexities
by a factor of h.
In order to reduce the time and space complexities further, we propose another operator known as
KAOQKV . In KAOQKV as shown in Figure 2(c), we use C as the query, key, and value matrices as
[H, L] = O = attn(C, C, C) ∈ Rc×(h+w).	(3)
~
hw
The final output tensor Y⑷ ∈ Rh×w×c is obtained as Kf) = HT 令 LT, where Hi： and Li： are the
ith rows of the corresponding matrices. That is, the ith frontal slice of Y(') is obtained by computing
the outer sum of the ith rows of H and L. If h = w, the time and space complexities of KAOQKV
are O((h + w) × c × (h + w)) = O(h2 × c) and O((h + w) × (h + w)) = O(h2), respectively. Thus,
the time and space complexities have been reduced by a factor of h2 as compared to the original
local operator, and by a factor of h as compared to KAOKV . Note that we do not consider linear
transformations in our description, but they can be applied to all three input matrices in KAOKV and
KAOQKV as shown in Figure 2.
3
Under review as a conference paper at ICLR 2020
3.2	Kronecker Attention Modules and Networks
Attention models have not been used in compact deep models to date, primarily due to their high
computational cost. In this section, we design a family of efficient Kronecker attention modules
based on KAOs as illustrated in Figure 3 in the appendix.
BaseModule: MobileNetV2 (Sandler et al., 2018) is mainly composed of bottleneck blocks with
inverted residuals. Each bottleneck block consists of three convolutional layers; those are, 1 × 1
convolutional layer, 3 × 3 depth-wise convolutional layer, and another 1 × 1 convolutional layer.
Suppose the expansion factor is r and stride is s. Given input X(') ∈ Rh×w×c for the 'th block, the
(`)	h
first 1 × 1 convolutional layer outputs rc feature maps X ∈ Rh×w×rc. The depth-wise convolutional
layer uses a stride of s and outputs rc feature maps X⑹ ∈ RS××w×rc. The last 1 × 1 convolutional
layer produces d feature maps Y(') ∈ RS× ww ×d. When S = 1 and C = d, a skip connection is added
between X(') and Y(').
BaseSkipModule: To facilitate feature reuse and gradient back-propagation in deep models, we
improve the BaseModule by adding a skip connection. Given input X('), we use an expansion factor
of r - 1 for the first 1 × 1 convolutional layer, instead of r as in BaseModule. We then concatenate the
(`)	h
output with the original input, resulting in X ∈ Rh×w×rc. The other parts of the BaseSkipModule
are the same as those of the BaseModule. Compared to the BaseModule, the BaseSkipModule
reduces the number of parameters by c × c and computational cost by h × w × c. It achieves better
feature reuse and gradient back-propagation.
AttnModule: We propose to add an attention operator into the BaseModule to enable the capture of
global features. We reduce the expansion factor of the BaseModule by 1 and add a new parallel path
with an attention operator that outputs c feature maps. Concretely, after the depth-wise convolutional
layer, the original path outputs XF ∈ RS×ww×(r-1)c. The attention operator, optionally followed
by an average pooling of stride S if S > 1, produces Xb') ∈ RS××w×c. Concatenating them gives
X⑹ ∈ RS ×^w×rc. The final 1 × 1 convolutional layer remains the same. Within the attention operator,
we only apply the linear transformation on the value matrix V to limit the number of parameters
and required computational resources. In this module, the original path acts as locality-based feature
extractors, while the new path with an attention operator computes global features. This enables the
module to incorporate both local and global information. Note that we can use any attention operator
in this module, including the regular attention operator and our KAOs.
AttnSkipModule: We propose to add an additional skip connection in the AttnModule. This
skip connection can always be added unless S > 1. The AttnSkipModule has the same amount of
parameters and computational cost as the AttnModule.
In the following sections, we perform some theoretical analysis on the proposed methods.
3.3	From Multivariate to Matrix-Variate Distributions
We analyze our solutions for attention operators on high-order data from a probabilistic perspective.
We take the non-local operator on 2-D data as an example. Formally, consider a self-attention operator
with Q = K = V = X(3), where X(3) ∈ Rc×hw is the mode-3 unfolding of a third-order input tensor
X ∈ Rh×w×c, as illustrated in Figure 1. The ith row of X(3)corresponds to Vec(XB)T ∈ Rhw,
where XM ∈ Rh×w denotes the ith frontal slice of X (Kolda & Bader, 2009), and vec(∙) denotes the
vectorization of a matrix by concatenating its columns (Gupta & Nagar, 2018).
The frontal slices X:：i, X:：2,..., Xlc ∈ Rh×w of X are usually known as C feature maps. In this view,
the mode-3 unfolding is equivalent to the vectorization of each feature map independently. It is worth
noting that, in addition to vec(∙), any other operation that transforms each feature map into a vector
leads to the same output from the non-local operator, as long as a corresponding reverse operation is
performed to fold the output into a tensor. This fact indicates that unfolding of X in local operators
ignores the structural information within each feature map, i.e., the relationships among rows and
columns. In addition, such unfolding results in excessive requirements on computational resources,
as explained in Section 2.2.
4
Under review as a conference paper at ICLR 2020
In the following discussions, We focus on one feature map X ∈ {X：：i, X:：2,..., X：：c} by assuming
feature maps are conditionally independent of each other, given feature maps of previous layers. This
assumption is shared by many deep learning techniques that process each feature map independently,
including the unfolding mentioned above, batch normalization, instance normalization (Ulyanov
et al., 2016), and pooling operations (LeCun et al., 1998). To view the problem above from a
probabilistic perspective (Ioffe & Szegedy, 2015; Ulyanov et al., 2016), the unfolding yields the
assumption that Vec(X) follows a multivariate normal distribution as Vec(X) Z Nhw (μ, Ω), where
μ ∈ Rhw and Ω ∈ Rhw×hw. Apparently, the multivariate normal distribution does not explicitly model
relationships among rows and columns in X . To address this limitation, we propose to model X
using a matrix-variate normal distribution (Gupta & Nagar, 2018), defined as below.
Definition 1. A random matrix A ∈ Rm×n is said to follow a matrix-variate normal distribution
MNm×n(M, Ω Θ Ψ) with mean matrix M ∈ Rm×n and covariance matrix Ω Θ Ψ, where Ω ∈
Rm×m A 0 and Ψ ∈ Rn×n A 0, if Vec(AT) Z Nmn(Vec(MT), Ω Θ Ψ). Here, Θ denotes the
Kronecker product (Van Loan, 2000; Graham, 2018).
The matrix-variate normal distribution has separate covariance matrices for rows and columns. They
interact through the Kronecker product to produce the covariance matrix for the original distribution.
Specifically, for two elements Xij and Xi'j' from different rows and columns in X, the relationship
between Xij and Xi'j' is modeled by the interactions between the ith and i'th rows and the jth and
j ′th columns. Therefore, the matrix-variate normal distribution can incorporate relationships among
rows and columns.
3.4	The Proposed Mean and Covariance Structures
In machine learning, (Kalaitzis et al., 2013) proposed to use the Kronecker sum to form covariance
matrices, instead of the Kronecker product. Based on the above observations and studies, we propose
to model X as X Z MNh×w (M, Ω ㊉ Ψ), where M ∈ Rh×w, Ω ∈ Rh×h A 0, Ψ ∈ Rw×w A 0,㊉
denotes the Kronecker sum (Kalaitzis et al., 2013), defined as Ω ㊉ Ψ = Ω Θ I[w] + I[h Θ Ψ, and
I[n] denotes an n × n identity matrix. Covariance matrices following the Kronecker sum structure
can still capture the relationships among rows and columns (Kalaitzis et al., 2013). It also follows
from (Allen & Tibshirani, 2010; Wang et al., 2017) that constraining the mean matrix M allows a
more direct modeling of the structural information within a feature map. Following these studies, we
assume X follows a variant of the matrix-variate normal distribution as
X z MNh×w(M, Ω ㊉ Ψ),
(4)
where the mean matrix M ∈ Rh×w is restricted to be the outer sum of two vectors, defined as
M = μ 令 U = μlTw] + 1[h]UT, where μ ∈ Rh, U ∈ Rw, and 1]m denotes an all-one vector of size n.
Under this model, the marginal distributions of rows and columns are both multivariate normal (Allen
& Tibshirani, 2010). Specifically, the ith row vector Xi： ∈ R1×w follows XT Z Nw (μi +ut, Ωɑ + Ψ),
and the jth column vector Xj ∈ Rh×1 follows Xj Z Nh(Ui + μ, Ψɑ + Ω). In the following discussion,
we assume that Ω and Ψ are diagonal, implying that any pair of variables in X are uncorrelated.
Note that, although the variables in X are independent, their covariance matrix still follows the
Kronecker covariance structure, thus capturing the relationships among rows and columns (Allen &
Tibshirani, 2010; Wang et al., 2017).
3.5	Main Technical Results
Let Xrow = (∑h=ι XT)/h ∈ Rw and XCol = (∑W=i Xj)/w ∈ Rh be the average of row and column
vectors, respectively. Under the assumption above, Xrow and XCol follow multivariate normal
distributions as
XrowZ Nw (μ+υ, Ω+Ψ)	⑸
XCol Z Nh(U + μ,------),	(6)
w
where μ = (∑h=ι μi)∕h, Ω = (∑h=ι Ωɑ)∕h, U = (∑j=ι Uj)/w, and Ψ = (∑W=i Ψjj)/w. Our main
technical results can be summarized in the following theorem.
5
Under review as a conference paper at ICLR 2020
Table 1: Comparisons between the regular attention operator, the regular attention operator with
a pooling operation (Wang et al., 2018), and our proposed KAOKV and KAOQKV in terms of the
number of parameters, MAdd, memory usage, and CPU inference time on data of different sizes.
The input sizes are given in the format of “batch size × spatial sizes × number of channels”. “Attn”
denotes the regular attention operator. “Attn+Pool” denotes the regular attention operator which
employs a 2 × 2 pooling operation on K and V input matrices to reduce required computational
resources._______________________________________________________________________________
Input	Operator	MAdd	Memory	Saving	Time	Speedup
	Attn	0.63m	5.2MB	0.00%	5.8ms	1.0×
2	Attn+Pool	0.16m	1.5MB	71.65%	2.0ms	3.0×
8× 14 ×8	KAOKV	0.09m	0.9MB	82.03%	1.7ms	3.5×
	KAOQKV	0.01m	0.3MB	95.06%	0.8ms	6.8×
	Attn	9.88m	^^79.9MB^^	0.00%	72.4ms	1.0×
2	Attn+Pool	2.47m	20.7MB	74.13%	20.9ms	3.5×
8 × 28 × 8	KAOKV	0.71m	6.5MB	91.88%	7.1ms	10.1×
	KAOQKV	0.05m	0.9MB	98.85%	1.7ms	40.9×
	Attn	157.55m	1,262.6MB	0.00%	1,541.1ms	1.0×
2	Attn+Pool	39.39m	318.7MB	74.76%	396.9ms	3.9×
8 × 56 × 8	KAOKV	5.62m	48.2MB	96.18%	49.6ms	31.1×
	KAOQKV	0.21m	3.4MB	99.73%	5.1ms	305.8×
Theorem 1. Given the multivariate normal distributions in Eqs.(5) and (6) with diagonal Ω and Ψ,
if (a) r1 , r2 , . . . , rh are independent and identically distributed (i.i.d.) random vectors that follow the
distribution in Eq. (5), (b) ci, c2,..., Cw arei.i.d. random vectors that follow the distribution in Eq. (6),
(c) ri, r2,..., rh and ci, c2,..., Cw are independent, we have X Z MN h×w (M, ψWω ㊉ ω+^ψ ),
1	-KT- Γ	~Λ,T , Γ	1 71 '^r	/	*	∖ . /— . —∖ T	. ∙ 1	∙ 7	,1
where X = [ri, r2,..., rh]T + [ci, C2,..., Cw ], M = (μ 令 U) + (μ + υ). In particular, if h = w, the
covariance matrix satisfies tr (ψWω ㊉ ΩΨ) = h tr (Ω ㊉ Ψ), where tr(∙) denotes matrix trace.
The proof of Theorem 1 can be found in the appendix. With certain normalization on X, we can
have μ + U = 0, resulting in M = μ 令 υ. As the trace of a covariance matrix measures the total
variation, Theorem 1 implies that X follows a matrix-variate normal distribution with the same mean
and scaled covariance as the distribution of X in Eq. (4). We build KAOs based on this conclusion
and the process to obtain X from X.
4	Experimental Studies
In this section, we evaluate our methods and networks on image classification and segmentation tasks.
4.1	Comparison of Computational Efficiency
According to the theoretical analysis in Section 3.1, our KAOs have efficiency advantages over
regular attention operators on high-order data, especially for inputs with large spatial sizes. We
conduct simulated experiments to evaluate the theoretical results. To reduce the influence of external
factors, we build networks composed of a single attention operator, and apply the TensorFlow profile
tool (Abadi et al., 2016) to report the multiply-adds (MAdd), required memory, and time consumed
on 2-D simulated data. For the simulated input data, we set the batch size and number of channels
both to 8, and test three spatial sizes; those are, 56 × 56, 28 × 28, and 14 × 14. The number of output
channels is also set to 8. Table 1 summarizes the comparison results. On simulated data of spatial
sizes 56 × 56, our KAOKV and KAOQKV achieve 31.1 and 305.8 times speedup, and 96.18% and
99.73% memory saving compared to the regular attention operator, respectively. Our proposed KAOs
show significant improvements over regular attention operators in terms of computational resources,
which is consistent with the theoretical analysis. In particular, the amount of improvement increases
as the spatial sizes increase. These results show that the proposed KAOs are efficient attention
operators on high-dimensional and high-order data.
6
Under review as a conference paper at ICLR 2020
4.2	Results on Image Classification
With the high efficiency of our KAOs, we have proposed several efficient Kronecker attention modules
for compact CNNs in Section 3.2. To further show the effectiveness of KAOs and the modules,
we build novel compact CNNs known as Kronecker attention networks (KANets). Following the
practices in (Wang et al., 2018), we apply these modules on inputs of spatial sizes 28 × 28, 14× 14, and
7 × 7. The detailed network architecture is described in Table 6 in the appendix due to space constraint.
We compare KANets with other CNNs on the ImageNet ILSVRC 2012 image classification dataset,
which serves as the benchmark for compact CNNs (Howard et al., 2017; Zhang et al., 2017; Gao
et al., 2018; Sandler et al., 2018). Details of the experimental setups are provided in the appendix.
The comparison results between our KANets
and other CNNs in terms of the top-1 accuracy,
number of parameters, and MAdd are reported
in Table 2. SqueezeNet (Iandola et al., 2016)
has the least number of parameters, but uses the
most MAdd and does not obtain competitive per-
formance as compared to other compact CNNs.
Among compact CNNs, MobileNetV2 (San-
dler et al., 2018) is the previous state-of-the-
art model, which achieves the best trade-off
between effectiveness and efficiency. Accord-
ing to the results, our KANets significantly out-
perform MobileNetV2 with 0.03 million fewer
parameters. Specifically, our KANetKV and
KANetQKV outperform MobileNetV2 by mar-
gins of 0.9% and 0.8%, respectively. More im-
portantly, our KANets has the least computa-
tional cost. These results demonstrate the effec-
tiveness and efficiency of our proposed KAOs.
Table 2: Comparisons between KANets and other
CNNs in terms of the top-1 accuracy on the Im-
ageNet validation set, the number of total pa-
rameters, and MAdd. We use KANetKV and
KANetQKV to denote KANets using KAOKV and
KAOQKV ,respectively.
Model	Top-1	Params	MAdd
VGG16	0.715	128m	15300m
AlexNet	0.572	60m	720m
SqueezeNet	0.575	1.3m	833m
MobileNetV1	0.706	4.2m	569m
ShuffleNet 1.5x	0.715	3.4m	292m
ChannelNet-v1	0.705	3.7m	407m
MobileNetV2	0.720	3.47m	300m
KANetKV (ours)	0.729	3.44m	288m
KANetQKV (ours)	0.728	3.44m	281m
The performance of KANets indicates that our proposed methods are promising, since we only make
small modifications to the architecture of MobileNetV2 to include KAOs. Compared to modules with
the regular convolutional layers only, our proposed modules with KAOs achieve better performance
without using excessive computational resources. Thus, our methods can be used widely for designing
compact deep models. Next, we show that our proposed KAOs are as effective as regular attention
operators.
4.3	Comparison with Regular Attention Operators
We perform experiments to compare our pro-
posed KAOs with regular attention operators.
We consider the regular attention operator and
the one with a pooling operation in (Wang et al.,
2018). For the attention operator with pooling
operation, the spatial sizes of the key matrix K
and value matrix V are reduced by 2 × 2 pooling
operations to save computation cost. To com-
pare these operators in fair settings, we replace
all KAOs in KANets with regular attention op-
erators and regular attention operators with a
pooling operation, denoted as AttnNet and At-
tnNet+Pool, respectively.
Table 3: Comparisons between KANets with
regular attention operators (denoted as AttnNet),
KANets with regular attention operators with a
pooling operation (denoted as AttnNet+Pool) and
KANets with KAOs in terms of the top-1 accuracy
on the ImageNet validation set, the number of total
parameters, and MAdd.
Model	Top-1	Params	MAdd
AttnNet	0.730	3.44m	365m
AttnNet+Pool	0.729	3.44m	300m
KANetKV	0.729	3.44m	288m
KANetQKV	0.728	3.44m	281m
The comparison results are summarized in Table 3. Note that all these models have the same number
of parameters. We can see that KANetKV and KANetQKV achieve similar performance as AttnNet
and AttnNet+Pool with dramatic reductions of computational cost. The results indicate that our
proposed KAOs are as effective as regular attention operators while being much more efficient. In
addition, our KAOs are better than regular attention operators that uses a pooling operation to increase
efficiency in (Wang et al., 2018).
7
Under review as a conference paper at ICLR 2020
4.4	Ablation Studies
To show how our KAOs benefit entire networks
in different settings, we conduct ablation stud-
ies on MobileNetV2 and KANetKV. For Mo-
bileNetV2, we replace BaseModules with Attn-
Modules as described in Section 3.2, resulting
in a new model denoted as MobileNetV2+KAO.
On the contrary, based on KANetKV , we replace
all AttnSkipModules by BaseModules. The re-
sulting model is denoted as KANet w/o KAO.
Table 4 reports the comparison results. By em-
ploying KAOKV , MobileNetV2+KAO gains a
performance boost of 0.6% with fewer parame-
Table 4: Comparisons among KANetKV ,
MobileNetV2, MobileNetV2 with KAOsKV
(MobileNetV2+KAOKV), and KANet without
KAO (KANet w/o KAO) in terms of the top-1
accuracy on the ImageNet validation set, the
number of total parameters, and MAdd.
Model	Top-1 Params MAdd
MobiIeNetV2	0.720 3.47m	300m
MobileNetV2+KAO	0.726	3.46m	298m
KANetKV	0.729	3.44m	288m
KANet w/o KAO	0.721	3.46m	298m
ters than MobileNetV2. On the other hand, KANetKV outperforms KANet w/o KAO by a margin of
0.8%, while KANet w/o KAO has more parameters than KANetKV . KANetKV achieves the best
performance while costing the least computational resources. The results indicate that our proposed
KAOs are effective and efficient, which is independent of specific network architectures.
4.5	Results on Image Segmentation
In order to show the efficiency and effective-
ness of our KAOs in broader application sce-
narios, we perform additional experiments on
image segmentation tasks using the PASCAL
2012 dataset (Everingham et al., 2010). With
the extra annotations provided by (Hariharan
et al., 2011), the augmented dataset contains
10,582 training, 1,449 validation, and 1,456 test-
ing images. Each pixel of the images is labeled
by one of 21 classes with 20 foreground classes
and 1 background class. We re-implement the
DeepLabV2 model (Chen et al., 2018) as our
baseline. Following (Wang & Ji, 2018), using
attention operators as the output layer, instead
Table 5: Comparisons among DeepLabV2,
DeepLabV2 with the regular attention operator
(DeepLabV2+Attn), DeepLabV2 with KAOKV
(DeepLabV2+KAOKV ), and DeepLabV2 with
KAOQKV (DeepLabV2+KAOQKV ) in terms of the
pixel-wise accuracy, and mean IOU on the PAS-
CAL VOC 2012 Vandation dataset.__________
Model	Accuracy	Mean IOU
DeePLabV2	0.944	75.1
DeePLabV2+Attn	0.947	76.3
DeePLabV2+KAOkv	0.946	75.9
DeePLabV2+KAOqkv	0.946	75.8
of atrous spatial pyramid pooling (ASPP), results in a significant performance improvement. In
our experiments, we replace ASPP with the regular attention operator and our proposed KAOs,
respectively, and compare the results. For all attention operators, linear transformations are applied
on Q, K, and V . Details of the experimental setups are provided in the appendix.
Table 5 shows the evaluation results in terms of pixel accuracy and mean intersection over union (IoU)
on the PASCAL VOC 2012 validation set. Clearly, models with attention operators outperform the
baseline model with ASPP. Compared with the regular attention operator, KAOs result in similar
pixel-wise accuracy but slightly lower mean IoU. From the pixel-wise accuracy, results indicate that
KAOs are as effective as the regular attention operator. The decrease in mean IoU may be caused by
the strong structural assumption behind KAOs. Overall, the experimental results demonstrate the
efficiency and effectiveness of our KAOs in broader application scenarios.
5	Conclusions
In this work, we propose Kronecker attention operators to address the practical challenge of applying
attention operators on high-order data. We investigate the problem from a probabilistic perspective
and use matrix-variate normal distributions with Kronecker covariance structure. Experimental
results show that our KAOs reduce the amount of required computational resources by a factor of
hundreds, with larger factors for higher-dimensional and higher-order data. We employ KAOs to
design a family of efficient modules, leading to our KANets. KANets significantly outperform the
previous state-of-the-art compact models on image classification tasks, with fewer parameters and
less computational cost.
8
Under review as a conference paper at ICLR 2020
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale
machine learning. In OSDI, volume 16, pp. 265-283, 2016.
Genevera I Allen and Robert Tibshirani. Transposable regularized covariance models with an
application to missing data imputation. The Annals of Applied Statistics, 4(2):764, 2010.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. International Conference on Learning Representations, 2015.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully
connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848,
2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchi-
cal Image Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2009.
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):
303-338, 2010.
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Channelnets: Compact and efficient con-
volutional neural networks via channel-wise convolutions. In Advances in Neural Information
Processing Systems, pp. 5203-5211, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, pp. 249-256, 2010.
Alexander Graham. Kronecker products and matrix calculus with applications. Courier Dover
Publications, 2018.
Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 2018.
Bharath Hariharan, Pablo Arbeiaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic
contours from inverse detectors. In Computer Vision (ICCV), 2011 IEEE International Conference
on, pp. 991-998. IEEE, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:
Criss-cross attention for semantic segmentation. arXiv preprint arXiv:1811.11721, 2018.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters andj 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Alfredo Kalaitzis, John Lafferty, Neil Lawrence, and Shuheng Zhou. The bigraphical lasso. In
International Conference on Machine Learning, pp. 1229-1237, 2013.
9
Under review as a conference paper at ICLR 2020
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):
455-500, 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong, and Liang Lin. Non-locally enhanced
encoder-decoder network for single image de-raining. In 2018 ACM Multimedia Conference on
Multimedia Conference, pp. 1056-1064. ACM, 2018.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
Wei Liu, Andrew Rabinovich, and Alexander C Berg. Parsenet: Looking wider to see better. arXiv
preprint arXiv:1506.04579, 2015.
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for
visual question answering. In Advances In Neural Information Processing Systems, pp. 289-297,
2016.
Mateusz Malinowski, Carl Doersch, Adam Santoro, and Peter Battaglia. Learning visual question
answering by bootstrapping hard attention. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-20, 2018.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 4510-4520. IEEE, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139-1147,
2013.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing
ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Charles F Van Loan. The ubiquitous kronecker product. Journal of computational and applied
mathematics, 123(1-2):85-100, 2000.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, pp. 5998-6008, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 4, 2018.
Zhengyang Wang and Shuiwang Ji. Smoothed dilated convolutions for improved dense prediction.
In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 2486-2495. ACM, 2018.
Zhengyang Wang, Hao Yuan, and Shuiwang Ji. Spatial variational auto-encoding via matrix-variate
normal distributions. arXiv preprint arXiv:1705.06821, 2017.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048-2057, 2015.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient
convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017.
10
Under review as a conference paper at ICLR 2020
Appendix
1	Experimental Setup for Image Classification
As a common practice on this dataset, we use the same data augmentation scheme in (He et al.,
2016). Specifically, during training, we scale each image to 256 × 256 and then randomly crop a
224 × 224 patch. During inference, the center-cropped patches are used. We train our KANets using
the same settings as MobileNetV2 (Sandler et al., 2018) with minor changes. We perform batch
normalization (Ioffe & Szegedy, 2015) on the coefficient matrices in KAOs to stabilize the training.
All trainable parameters are initialized with the Xavier initialization (Glorot & Bengio, 2010). We
use the standard stochastic gradient descent optimizer with a momentum of 0.9 (Sutskever et al.,
2013) to train models for 150 epochs in total. The initial learning rate is 0.1 and it decays by 0.1 at
the 80th, 105th, and 120th epoch. Dropout (Srivastava et al., 2014) with a keep rate of 0.8 is applied
after the global average pooling layer. We use 8 TITAN Xp GPUs and a batch size of 512 for training,
which takes about 1.5 days. Since labels of the test dataset are not available, we train our networks
on training dataset and report accuracies on the validation dataset.
2	Experimental Setup for Image S egmentation
We train all the models with randomly cropped patches of size 321 × 321 and a batch size of 8.
Data augmentation by randomly scaling the inputs for training is employed. We adopt the “poly”
learning rate policy (Liu et al., 2015) with power = 0.9, and set the initial learning rate to 0.00025.
Following DeepLabV2, we use the ResNet-101 model pre-trained on ImageNet (Deng et al., 2009)
and MS-COCO (Lin et al., 2014) for initialization. The models are then trained for 25,000 iterations
with a momentum of 0.9 and a weight decay of 0.0005. We perform no post-processing such as
conditional random fields and do not use multi-scale inputs due to limited GPU memory. All the
models are trained on the training set and evaluated on the validation set.
3	Illustration of Kronecker Attention Modules
IV X》X d∣
ConV 1 X 1
Stride = 1
OUtpUt d
Conv 1 x 1
Stride = 1
、BN + ReLU6 ,
DWConv 3 X 3
Stride = S
、BN + ReLU6 ,
KAO
Conv 1 X 1
Stride = 1
、BN + ReLU6 ,
DWConv 3 X 3
Stride = S
、BN + ReLU6
KAO
DWConv 3 x 3
Stride = S
、BN + ReLU6 ,
Conv 1 X1
Stride = 1
、BN + ReLU6 ,
Conv 1 x 1
Stride = 1
OUtpUt d
Conv 1 x1
Stride = 1
OUtpUt d
∣h X W X H
IV X W X M
1
1
(a)	(b)	(c)	(d)
Figure 3: Architectures of the BaseModule (a), BaseSkipModule (b), AttnModule (c), and AttnSkip-
Module (d) as described in Section 3.2. The skip connections indicated by single dashed paths are
not used when s > 1 or c ≠ d. Those indicated by double dashed paths are not used when s > 1.
4 The KANets Architecture
Table 6 describes the detailed KANets architecture. We use KAOs in every AttnSkipModule. In
KAOs, we use multi-head attention with 4 heads and concatenate results for output. The linear
11
Under review as a conference paper at ICLR 2020
Table 6: Details of the KANets architecture. Each line describes a sequence of operators in the format
of “input size / operator name / expansion rate r / number of output channels c / number of operators
in the sequence n / stride s”. “Conv2D” denotes the regular 2D convolutional layer. “AvgPool”
and “FC” denote the global average pooling layer and the fully-connected layer, respectively. All
depth-wise convolutions use the kernel size of 3 × 3. For multiple operators in a sequence denoted in
the same line, all operators produce c output channels. And the first operator applies the stride of s
while the following operators applies the stride of 1. k denotes the class number in the task.
Input	Operator	r	C	n	s
2242×3	Conv2D 3 × 3	-	32	1	2
1122×32	BaseSkipModule	1	16	1	1
1122×16	BaseSkipModule	6	24	2	2
562×24	BaseSkipModule	6	32	2	2
282×32	AttnSkipModule	6	32	1	1
282×32	BaseSkipModule	6	64	1	2
142×64	AttnSkipModule	6	64	3	1
142×64	AttnSkipModule	6	96	3	1
142×96	BaseSkipModule	6	160	1	2
72×160	AttnSkipModule	6	160	2	1
72×160	AttnSkipModule	6	320	1	1
72×320	Conv2D 1 × 1	-	1280	1	1
72×1280	AvgPool + FC	-	k	1	-
transformation is only performed on the value matrix V to limit the number of parameters and
computational resources.
5 Proof of Theorem 1
The fact that Ω and Ψ are diagonal implies independence in the case of multivariate normal distribu-
tions. Therefore, it follows from assumptions (a) and (b) that
[rι,r2,...,rh]T Z MNh×w
Ω+ 丁 Ω+Ψ∖
(Mr，I[h]区 ~^h~ )，
(7)
where Mr = μ + [υ, υ,..., υ]T = μ + 1[h]UT, and
[c1,c2, . . . , cw] Z MNh
×w
Ψ+ Ψ + Ω
(Mc,-
E I[w]),
(8)
where Mc = U + [μ, μ,..., μ] = U + μl∣τw].
Given assumption (c) and X = [ri, r2,..., rh]T + [ci, c2,..., Cw ], We have
-Ψ+ Ψ + Ω Ω + Ψ∖
X z MNh×w M,-㊉一-一,
wh
where M = Mr + Mc = (μ 令 υ) + (μ + U).
If h = w, we have
tr(Ω ® Ψ) = h (∑ Ωii + ∑ Ψjj),
(9)
(10)
12
Under review as a conference paper at ICLR 2020
and
tJΨ + Ω Ω + Ψ∖
wh
11丁、 1 —
tr (1 (Ω ㊉ Ψ) + 1 (Ψ + Ω))
(Σ Qi + Σ ψjj ) + h(ψ + ⑶
2(∑ Ωii + ∑ Ψjj)
2
—∙ tr(Ω ㊉ Ψ).
h
(11)
This completes the proof of the theorem.
References
All citations refer to the references in the main paper.
13