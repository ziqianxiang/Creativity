Under review as a conference paper at ICLR 2020
Recurrent Hierarchical Topic-Guided
Neural Language Models
Anonymous authors
Paper under double-blind review
Ab stract
To simultaneously capture syntax and global semantics from a text corpus, we pro-
pose a new larger-context recurrent neural network (RNN) based language model,
which extracts recurrent hierarchical semantic structure via a dynamic deep topic
model to guide natural language generation. Moving beyond a conventional RNN
based language model that ignores long-range word dependencies and sentence
order, the proposed model captures not only intra-sentence word dependencies, but
also temporal transitions between sentences and inter-sentence topic dependences.
For inference, we develop a hybrid of stochastic-gradient MCMC and recurrent
autoencoding variational Bayes. Experimental results on a variety of real-world
text corpora demonstrate that the proposed model not only outperforms state-of-
the-art larger-context RNN-based language models, but also learns interpretable
recurrent multilayer topics and generates diverse sentences and paragraphs that are
syntactically correct and semantically coherent.
1	Introduction
Both topic and language models are widely used for text analysis. Topic models, such as latent
Dirichlet allocation (LDA) (Blei et al., 2003; Griffiths & Steyvers, 2004; Hoffman et al., 2013) and
its nonparametric Bayesian generalizations (Teh et al., 2006; Zhou & Carin, 2015), are well suited
to extract document-level word concurrence patterns into latent topics from a text corpus. Their
modeling power has been further enhanced by introducing multilayer deep representation (Srivastava
et al., 2013; Mnih & Gregor, 2014; Gan et al., 2015; Zhou et al., 2016; Zhao et al., 2018; Zhang
et al., 2018). While having semantically meaningful latent representation, they typically treat each
document as a bag of words (BoW), ignoring word order (Griffiths et al., 2004; Wallach, 2006).
Language models have become key components of various natural language processing (NLP) tasks,
such as text summarization (Rush et al., 2015; Gehrmann et al., 2018), speech recognition (Mikolov
et al., 2010; Graves et al., 2013), machine translation (Sutskever et al., 2014; Cho et al., 2014), and
image captioning (Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015; Gan et al., 2017; Rennie
et al., 2017). The primary purpose of a language model is to capture the distribution of a word
sequence, commonly with a recurrent neural network (RNN) (Mikolov et al., 2011; Graves, 2013)
or a Transformer based neural network (Vaswani et al., 2017; Dai et al., 2019; Devlin et al., 2019;
Radford et al., 2018; 2019). In this paper, we focus on improving RNN-based language models that
often have much fewer parameters and are easier to perform end-to-end training.
While RNN-based language models do not ignore word order, they often assume that the sentences
of a document are independent to each other. This simplifies the modeling task to independently
assigning probabilities to individual sentences, ignoring their orders and document context (Tian &
Cho, 2016). Such language models may consequently fail to capture the long-range dependencies
and global semantic meaning of a document (Dieng et al., 2017; Wang et al., 2018). To relax the
sentence independence assumption in language modeling, Tian & Cho (2016) propose larger-context
language models that model the context of a sentence by representing its preceding sentences as
either a single or a sequence of BoW vectors, which are then fed directly into the sentence modeling
RNN. An alternative approach attracting significant recent interest is leveraging topic models to
improve RNN-based language models. Mikolov & Zweig (2012) use pre-trained topic model features
as an additional input to the RNN hidden states and/or output. Dieng et al. (2017); Ahn et al. (2017)
combine the predicted word distributions, given by both a topic model and a language model, under
variational autoencoder (Kingma & Welling, 2013). Lau et al. (2017) introduce an attention based
1
Under review as a conference paper at ICLR 2020
convolutional neural network to extract semantic topics, which are used to extend the RNN cell.
Wang et al. (2018) learn the global semantic coherence of a document via a neural topic model and
use the learned latent topics to build a mixture-of-experts language model. Wang et al. (2019) further
specify a Gaussian mixture model as the prior of the latent code in variational autoencoder, where
each mixture component corresponds to a topic.
While clearly improving the performance of the end task, these existing topic-guided methods still
have clear limitations. For example, they only utilize shallow topic models with only a single
stochastic hidden layer in their data generation process. Note several neural topic models use deep
neural networks to construct their variational encoders, but still use shallow generative models
(decoders) (Miao et al., 2017; Srivastava & Sutton, 2017). Another key limitation lies in ignoring the
sentence order, as they treat each document as a bag of sentences. Thus once the topic weight vector
learned from the document context is given, the task is often reduced to independently assigning
probabilities to individual sentences (Lau et al., 2017; Wang et al., 2018; 2019).
In this paper, as depicted in Fig. 1, we propose to use recurrent gamma belief network (rGBN) to guide
a stacked RNN for language modeling. We refer to the model as rGBN-RNN, which integrates rGBN
(Guo et al., 2018), a deep recurrent topic model, and stacked RNN (Graves, 2013; Chung et al., 2017),
a neural language model, into a novel larger-context RNN-based language model. It simultaneously
learns a deep recurrent topic model, extracting document-level multi-layer word concurrence patterns
and sequential topic weight vectors for sentences, and an expressive language model, capturing
both short- and long-range word sequential dependencies. For inference, we equip rGBN-RNN
(decoder) with a novel variational recurrent inference network (encoder), and train it end-to-end by
maximizing the evidence lower bound (ELBO). Different from the stacked RNN based language
model in Chung et al. (2017), which relies on three types of customized training operations (UPDATE,
COPY, FLUSH) to extract multi-scale structures, the language model in rGBN-RNN learns such
structures purely under the guidance of the temporally and hierarchically connected stochastic layers
of rGBN. The effectiveness of rGBN-RNN as a new larger-context language model is demonstrated
both quantitatively, with perplexity and BLEU scores, and qualitatively, with interpretable latent
structures and randomly generated sentences and paragraphs. Notably, rGBN-RNN can generate a
paragraph consisting of a sequence of semantically coherent sentences.
2	Recurrent hierarchical topic-guided language model
Denote a document of J sentences as D = (S1, S2, . . . , SJ), where Sj = (yj,1, . . . , yj,Tj ) consists
of Tj words from a vocabulary of size V . Conventional statistical language models often only focus
on the word sequence within a sentence. Assuming that the sentences of a document are independent
to each other, they often define P(D) ≈ QjJ=1 P (Sj) = QjJ=1 QtT=j 2 p (yj,t | yj,<t)p(yj,1) . RNN
based neural language models define the conditional probability of each word yj,t given all the
previous words yj,<t within the sentence Sj, through the softmax function of a hidden state hj,t, as
p (yj,t | yj,<t) = p (yj,t | hj,t) , hj,t = f (hj,<t, yj,t-1) ,	(1)
where f (∙) is a non-linear function typically defined as an RNN cell, such as long short-term memory
(LSTM) (Hochreiter & Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014).
These RNN-based statistical language models are typically applied only at the word level, without
exploiting the document context, and hence often fail to capture long-range dependencies. While
Dieng et al. (2017); Lau et al. (2017); Wang et al. (2018; 2019) remedy the issue by guiding the
language model with a topic model, they still treat a document as a bag of sentences, ignoring the
order of sentences, and lack the ability to extract hierarchical and recurrent topic structures.
We introduce rGBN-RNN, as depicted in Fig. 1(a), as a new larger-context language model. It
consists of two key components: (i) a hierarchical recurrent topic model (rGBN), and (ii) a stacked
RNN based language model. We use rGBN to capture both global semantics across documents
and long-range inter-sentence dependencies within a document, and use the language model to
learn the local syntactic relationships between the words within a sentence. Similar to Lau et al.
(2017); Wang et al. (2018), we represent a document as a sequence of sentence-context pairs as
({S1 , d1}, . . . , {SJ, dJ}), where dj ∈ ZV+c summarizes the document excluding Sj, specifically
(S1, ..., Sj-1, Sj+1, ..., SJ), into a BoW count vector, with Vc as the size of the vocabulary excluding
stop words. Note a naive way is to treat each sentence as a document, use a dynamic topic model
2
Under review as a conference paper at ICLR 2020
⑶
(b)
(c)
Figure 1: (a) The generative model of a three-hidden-layer rGBN-RNN, where the bottom part is the deep
recurrent topic model (rGBN), document contexts of consecutive sentences are used as observed data, and upper
is the language model. (b) Overview of the language model component, where input xj,t denotes the tth word in
jth sentence of a document, xj,t = yj,t-1, hlj,t is the hidden state of the stacked RNN at time step t, and θjl is
the topic weight vector of sentence j at layer l. (c) The overall architecture of the proposed model, including the
decoder (rGBN and language model) and encoder (variational recurrent inference), where the red arrows denote
the inference of latent topic weight vectors, black ones the data generation.
(Blei & Lafferty, 2006) to capture the temporal dependencies of the latent topic-weight vectors,
which is fed to the RNN to model the word sequence of the corresponding sentence. However, the
sentences are often too short to be well modeled by a topic model. In our setting, as dj summarizes
the document-level context of Sj , it is in general sufficiently long for topic modeling. Note during
testing, we redefine dj as the BoW vector summarizing only the preceding sentences, i.e., S1:j-1,
which will be further clarified when presenting experimental results.
2.1	Hierarchical recurrent topic model
Shown in Fig. 1 (a), to model the time-varying sentence-context count vectors dj in document D, the
generative process of the rGBN component, from the top to bottom hidden layers, is expressed as
θj 〜Gam (∏LθL-ι, τo),…，θj 〜Gam (Φl+1θj+1 + Πlθ", τ°),…，
θj 〜Gam (Φ2θj + Π1θ1-1, τo), dj 〜Pois (Φ1θ1),	(2)
where θjl ∈ R+Kl denotes the gamma distributed topic weight vectors of sentence j at layer l,
Πl ∈ R+Kl ×Kl the transition matrix of layer l that captures cross-topic temporal dependencies, Φl ∈
R+Kl-1 ×Kl the loading matrix at layer l, Kl the number of topics of layer l, and τ0 ∈ R+ a scaling
hyperparameter. At j = 1, θ∖ 〜Gam (Φl+1θ1+1,τo) for l = 1,...,L - 1 and θ1L 〜Gam (ν,τ0),
where ν = 1KL . Finally, Dirichlet priors are placed on the columns of Πl and Φl, i.e., πkl and φlk,
which not only makes the latent representation more identifiable and interpretable, but also facilitates
inference. The count vector dj can be factorized into the product of Φ1 and θj1 under the Poisson
likelihood. The shape parameters of θjl ∈ R+Kl can be factorized into the sum of Φl+1θjl+1, capturing
inter-layer hierarchical dependence, and Πlθjl -1, capturing intra-layer temporal dependence. rGBN
not only captures the document-level word occurrence patterns inside the training text corpus, but
also the sequential dependencies of the sentences inside a document. Note ignoring the recurrent
structure, rGBN will reduce to the gamma belief network (GBN) of Zhou et al. (2016), which can be
considered as a multi-stochastic-layer deep generalization of LDA (Cong et al., 2017a). If ignoring
its hierarchical structure (i.e., L = 1), rGBN reduces to Poisson-gamma dynamical systems (Schein
et al., 2016). We refer to the rGBN-RNN without its recurrent structure as GBN-RNN, which no
longer models sequential sentence dependencies; see Appendix A for more details.
3
Under review as a conference paper at ICLR 2020
2.2	Language model
Different from a conventional RNN-based language model, which predicts the next word only using
the preceding words within the sentence, we integrate the hierarchical recurrent topic weight vectors
θjl into the language model to predict the word sequence in the jth sentence. Our proposed language
model is built upon the stacked RNN proposed in Graves (2013); Chung et al. (2017), but with the
help of rGBN, it no longer requires specialized training heuristics to extract multi-scale structures.
As shown in Fig. 1 (b), to generate yj,t, the tth token of sentence j in a document, we construct the
hidden states hlj,t of the language model, from the bottom to top layers, as
h = [ LSTMword (hj,t-ι, We [xj,t]),	if l = 1,
hj,t =	LSTMlword
(hj,t-1,alj-,t1), ifL≥l>1,
(3)
where LSTMlword denotes the word-level LSTM at layer l, We ∈ RV are word embeddings to be
learned, and xj,t = yj,t-1. Note alj,t denotes the coupling vector, which combines the temporal topic
weight vectors θjl and hidden output of the word-level LSTM hlj,t-1 at each time step t.
Following Lau et al. (2017), a gating unit similar to GRU (Cho et al., 2014) combines θjl of sentence j
with its hidden state hlj,t of word-level LSTM at layer l and time t as alj,t = gl hlj,t, θjl . We
defer the details on gl to Appendix B. Denote Wo as a weight matrix with V rows and aj1,:tL as the
concatenation of alj,t across all layers; different from (1), the conditional probability of yj,t becomes
P ("t | yj,<t, θj) = Softmax (Woa1t
(4)
There are two main reasons for combining all the latent representations aj1,:tL for language modeling.
First, the latent representations exhibit different statistical properties at different stochastic layers of
rGBN-RNN, and hence are combined together to enhance their representation power. Second, having
“skip connections” from all hidden layers to the output one makes it easier to train the proposed
network, reducing the number of processing steps between the bottom of the network and the top and
hence mitigating the “vanishing gradient” problem (Graves, 2013).
To sum up, as depicted in Fig. 1 (a), the topic weight vector θjl of sentence j quantifies the topic usage
of its document context dj at layer l. It is further used as an additional feature of the language model
to guide the word generation inside sentence j , as shown in Fig. 1 (b). It is clear that rGBN-RNN has
two temporal structures: a deep recurrent topic model to extract the temporal topic weight vectors
from the sequential document contexts, and a language model to estimate the probability of each
sentence given its corresponding hierarchical topic weight vector. Characterizing the word-sentence-
document hierarchy to incorporate both intra- and inter-sentence information, rGBN-RNN learns
more coherent and interpretable topics and increases the generative power of the language model.
Distinct from existing topic-guided language models, the temporally related hierarchical topics of
rGBN exhibit different statistical properties across layers, which better guides language model to
improve its language generation ability.
2.3	Model likelihood and inference
For rGBN-RNN, given {Φl, Πl}lL=1, the marginal likelihood of the sequence of sentence-context
pairs ({s1, d1}, . . . , {sJ, dJ}) of document D is defined as
P(D∣{Φl, Πl}L=ι) = RQJ=1 {p (dj∣Φ1θj) [q3 P Iyjt ∖yj,<t, θ产)][口3 P (θj∣ej,τo)]} dθJ (5)
where elj := Φl+1θjl+1 + Πlθjl -1 . The inference task is to learn the parameters of both the topic
model and language model components. One naive solution is to alternate the training between these
two components in each iteration: First, the topic model is trained using a sampling based iterative
algorithm provided in Guo et al. (2018); Second, the language model is trained with maximum
likelihood estimation under a standard cross-entropy loss. While this naive solution can utilize readily
available inference algorithms for both rGBN and the language model, it may suffer from stability
and convergence issues. Moreover, the need to perform a sampling based iterative algorithm for
rGBN inside each iteration limits the scalability of the model for both training and testing.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Hybrid SG-MCMC and recurrent autoencoding variational inference for rGBN-RNN.
Set mini-batch size m and the number of layer L
Initialize encoder and neural language model parameter parameter Ω, and topic model parameter {Φl, Πl }L=ι
for iter = 1, 2, •一do
Randomly select a mini-batch of m documents consisting of J sentences to form a subset X
{di,1:J, si,1:J }i=1 ;
Draw random noise
{ei,j }m,Jr,L=ι 1=1 from uniform distribution;
Calculate Vω L (Ω, Φl, Πl; X, Eij) according to (6), and update Ω; Sample θl,j from ⑺ and (8) via C
to update {Πl}lL=1 and {Φl}lL=1, will be described in Appendix C;
end for
To this end, we introduce a variational recurrent inference network (encoder) to learn the latent
temporal topic weight vectors θ1J. Denoting Q = Q∫=1 QL=I q(θj | d≤j), the ELBO of the log
marginal likelihood shown in (5) can be constructed as
L = PJ=1 Eq [lnP (dj I Φ1θ1) + Pj 1 lnP M I jt。产)]-Pj=ι PL=ι EQ [n 渭U) ] ,(6)
which unites both the terms that are primarily responsible for training the recurrent hierarchical
topic model component, and terms for training the neural language model component. Similar to
Zhang et al. (2018), we define q(θjl I d≤j) = Weibull(kjl, λlj), a random sample from which can be
obtained by transforming standard uniform variables lj as
θj = λj( - ln(1-ej))1∕kj.	(7)
To capture the temporal dependencies between the topic weight vectors, both kjl and λlj , from the
bottom to top layers, can be expressed as
hss, = RNNsent (hj-1, h丁-1), kj = fk(hj,),λj = fλ (hj,l),	(8)
where hjs,0 = dj , hs0,l = 0, RNNlsent denotes the sentence-level recurrent encoder at layer l
implemented with a basic RNN cell, capturing the sequential relationship between sentences within
a document, hjs,l denotes the hidden state of RNNlsent, and superscript s in hjs,l denotes “sentence-
level RNN” used to distinguish the hidden state of language model in (3) . Note both fkl and fλl
are nonlinear functions mapping state hjs,l to the parameters of θjl , implemented with f(x) =
ln(1 + exp(Wx + b)).
Rather than finding a point estimate of the global parameters {Φl, Πl}lL=1 of the rGBN, we adopt a
hybrid inference algorithm by combining TLASGR-MCMC described in Cong et al. (2017a); Zhang
et al. (2018) and our proposed recurrent variational inference network. In other words, the global
parameters {Φl, Πl}lL=1 can be sampled with TLASGR-MCMC, while the parameters of the language
model and variational recurrent inference network, denoted by Ω, can be updated via stochastic
gradient descent (SGD) by maximizing the ELBO in (6). We describe a hybrid variational/sampling
inference for rGBN-RNN in Algorithm 1 and provide more details about sampling {Φl, Πl}lL=1 with
TLASGR-MCMC in Appendix C. We defer the details on model complexity to Appendix E.
To sum up, as shown in Fig. 1(c), the proposed rGBN-RNN works with a recurrent variational
autoencoder inference framework, which takes the document context of the j th sentence within a
document as input and learns hierarchical topic weight vectors θ:L that evolve sequentially with j.
The learned topic vectors in different layer are then used to reconstruct the document context input
and as an additional feature for the language model to generate the jth sentence.
3	Experimental results
We consider three publicly available corpora, including APNEWS, IMDB, and BNC. The links,
preprocessing steps, and summary statistics for them are deferred to Appendix D. We consider a
recurrent variational inference network for rGBN-RNN to infer θjl, as shown in Fig. 1(c), whose
number of hidden units in (8) are set the same as the number of topics in the corresponding layer.
5
Under review as a conference paper at ICLR 2020
Following Lau et al. (2017), word embeddings are pre-trained 300-dimension word2vec Google
News vectors (https://code.google.com/archive/p/word2vec/). Dropout with a rate of 0.4 is used to
the input of the stacked-RNN at each layer, i.e., alj,t or We [xj,t] in (3). The gradients are clipped if
the norm of the parameter vector exceeds 5. We use the Adam optimizer (Kingma & Ba, 2015) with
learning rate 10-3. The length of an input sentence is fixed to 30. We set the mini-batch size as 8,
number of training epochs as 5, and scaling hyperparameter τ0 as 1. Code in TensorFlow is provided.
3.1	Quantitative comparison
Perplexity: For fair comparison, we use standard language model perplexity as the evaluation met-
ric, by considering the following baselines: (i) a standard LSTM language model (Hochreiter &
Schmidhuber, 1997); (ii) LCLM (Tian & Cho, 2016), a larger-context language model that incorpo-
rates context from preceding sentences, which are treated as a bag of words; (iii) a standard LSTM
language model incorporating the topic information of a separately trained LDA (LDA+LSTM);
(iv) Topic-RNN (Dieng et al., 2017), a hybrid model rescoring the prediction of the next word by
incorporating the topic information through a linear transformation; (v) TDLM (Lau et al., 2017),
a joint learning framework which learns a convolutional based topic model and a language model
simultaneously. (vi) TCNLM (Wang et al., 2018), which extracts the global semantic coherence of a
document via a neural topic model, with the probability of each learned latent topic further adopted to
build a mixture-of-experts language model; (vii) TGVAE (Wang et al., 2019), combining a variational
auto-encoder based neural sequence model with a neural topic model; (viii) GBN-RNN, a simplified
rGBN-RNN that removes the recurrent structure of its rGBN component.
For rGBN-RNN, to ensure the information about the words in the jth sentence to be predicted is not
leaking through the sequential document context vectors at the testing stage, the input dj in (8) only
summarizes the preceding sentences S<j. For GBN-RNN, following TDLM (Lau et al., 2017) and
TCNLM (Wang et al., 2018), all the sentences in a document, excluding the one being predicted,
are used to obtain the BoW document context. As shown in Table 1, rGBN-RNN outperforms all
baselines, and the trend of improvement continues as its number of layers increases, indicating
the effectiveness of assimilating recurrent hierarchical topic information. rGBN-RNN consistently
outperforms GBN-RNN, suggesting the benefits of exploiting the sequential dependencies of the
sentence-contexts for language modeling. Moreover, comparing Table 1 and Table 4 of Appendix E
suggests rGBN-RNN, with its hierarchical and temporal topical guidance, achieves better performance
with fewer parameters than comparable RNN-based baselines.
Note that for language modeling, there has been significant recent interest in replacing RNNs with
Transformer (Vaswani et al., 2017), which consists of stacked multi-head self-attention modules, and
its variants (Dai et al., 2019; Devlin et al., 2019; Radford et al., 2018; 2019). While Transformer
based language models have been shown to be powerful in various natural language processing tasks,
they often have significantly more parameters, require much more training data, and take much longer
to train than RNN-based language models. For example, Transformer-XL with 12L and that with
24L (Dai et al., 2019), which improve Transformer to capture longer-range dependencies, have 41M
and 277M parameters, respectively, while the proposed rGBN-RNN with three stochastic hidden
layers has as few as 7.3M parameters, as shown in Table 4, when used for language modeling. From
a structural point-of-view, we consider the proposed rGBN-RNN as complementary to rather than
competing with Transformer based language models, and consider replacing RNN with Transformer
to construct rGBN guided Transformer as a promising future extension.
BLEU: Following Wang et al. (2019), we use test-BLEU to evaluate the quality of generated
sentences with a set of real test sentences as the reference, and self-BLEU to evaluate the diversity of
the generated sentences (Zhu et al., 2018). Given the global parameters of the deep recurrent topic
model (rGBN) and language model, we can generate the sentences by following the data generation
process of rGBN-RNN: we first generate topic weight vectors θjL randomly and then downward
propagate it through the rGBN as in (2) to generate θj<L. By assimilating the random draw topic
weight vectors with the hidden states of the language model in each layer depicted in (3), we generate
a corresponding sentence, where we start from a zero hidden state at each layer in the language model,
and sample words sequentially until the end-of-the-sentence symbol is generated. Comparisons of the
BLEU scores between different methods are shown in Fig. 2, using the benchmark tool in Texygen
(Zhu et al., 2018); We show below BLEU-3 and BLEU-4 for BNC and defer the analogous plots for
IMDB and APNEWS to Appendix G and H. Note we set the validation dataset as the ground-truth.
6
Under review as a conference paper at ICLR 2020
Table 1: Comparison of perplexity on three different datasets.
Model	LSTM Size	Topic Size	Perplexity		
			APNEWS	IMDB	BNC
LCLM (Tian & Cho, 2016)	600	—	-54.18	67.78	96.50
	900-900	—	50.63	67.86	87.77
LDA+LSTM (Lau et al., 2017)	600	100	-55.52	69.64	96.50
	900-900	100	50.75	63.04	87.77
TopicRNN (Dieng et al., 2017)	600	100	-54.54	67.83	93.57
	900-900	100	50.24	61.59	84.62
TDLM (Lau et al., 2017)	600	100	-52.75	63.45	85.99
	900-900	100	48.97	59.04	81.83
TCNLM (Wang et al., 2018)	600	100	-52.63	62.64	86.44
	900-900	100	47.81	56.38	80.14
TGVAE (Wang et al., 2019)	600	50	-48.73	57.11	87.86
	600	—	-64.13	72.14	102.89
basic-LSTM (Hochreiter & Schmidhuber, 1997)	900-900	—	58.89	66.47	94.23
	900-900-900	—	60.13	65.16	95.73
	600	100	-47.42	57.01	86.39
GBN-RNN	600-512	100-80	44.64	55.42	82.95
	600-512-256	100-80-50	44.35	54.53	80.25
	600	100	-46.35	55.76	81.94
rGBN-RNN	600-512	100-80	43.26	53.82	80.25
	600-512-256	100-80-50	42.71	51.36	79.13
0.35
∙LSTM3
0.30
0.20
0.15
io.25
∙LSTM3	
•GBN-RNN-2	•rGBN-RNN-1 CrGBN-RNN-2
∙GBN-RNNsGBN-RNN-3	•rGBN-RNN-3 Ground-Truth
∙TDLM	
0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65
test-BLEU
(a) BLEU-3
•GBN-RNN-1	OrGBN-RNN-2
•GBN-RNN-2	∙rGBN-RNN-3
.	Ground-Truth
•GBN-RNN-3
∙TDLM
0.18
0.16
§ 0.14
I012
0.10
0.08
0.06
0.10	0.15	0.20	0.25	0.30	0.35
test-BLEU
(b) BLEU-4
Figure 2: BLEU scores of different methods for BNC. x-axis denotes test-BLEU, y-axis self-BLEU, and a
better BLEU score would fall within the lower right corner.
the service employe e international union asked why	Cme group needs	tax	relief when	it	is	making huge amounts of	money	?
.	(b)
Figure 3: Visualization of the L2-norm of the hidden states of the language model of rGBN-RNN, shown in the
top-row, and that of GBN-RNN, shown in the bottom row.
For all datasets, it is clear that rGBN-RNN yields both higher test-BLEU and lower self-BLEU
scores than related methods do, indicating the stacked-RNN based language model in rGBN-RNN
generalizes well and does not suffer from mode collapse (i.e., low diversity).
3.2	Qualitative analysis
Hierarchical structure of language model: In Fig. 3, we visualize the hierarchical multi-scale
structures learned with the language model of rGBN-RNN and that of GBN-RNN, by visualizing the
L2-norm of the hidden states in each layer, while reading a sentence from the APNEWS validation set
as “the service employee international union asked why cme group needs tax relief when it is making
huge amounts of money?” As shown in Fig. 3(a), in the bottom hidden layer (h1), the L2 norm
sequence varies quickly from word to word, except within short phrases such “service employee”,
“international union,” and “tax relief,” suggesting layer h1 is in charge of capturing short-term local
dependencies. By contrast, in the top hidden layer (h3), the L2 norm sequence varies slowly and
exhibits semantic/syntactic meaningful long segments, such as “service employee international union,”
“asked why cme group needs tax relief,” “when it is,” and “making huge amounts of,” suggesting
that layer h3 is in charge of capturing long-range dependencies. Therefore, the language model in
7
Under review as a conference paper at ICLR 2020
48 generated sentence:
looming monday , the assembly added a
proposal to balance the budget medicaid
plan for raising the rate to $ 142 million ,
whereas years later , to $ 200 million .
57 generated sentence:
the last of the four companies
and the mississippi inter
national speedway was voted
to accept the proposal .
48
budget lawmakers
gov.revenue
vote proposal
community
legislation
57
lawmakers pay
proposal legislation
credit session
meeting gambling
75 generated sentence:
the state senate would give
lawmakers time to accept the
retirement payment.
Incgambling credit assets
medicaid investment
62 generated sentence:
the gambling and voting
department says it was a
chance of the game .
lawmaker proposal legislation
approval raising audit senate
48-57-75 generated sentence:
the proposal would give them a
pathway to citizenship for the year
before , but they don't have a chance
to participate in the elections .
68
rate rose
prices income
trade crisis
supply
management
48-57-62 generated sentence:
the arkansas senate has
purchased a $ 500 million state
bond for a proposed medicaid
expansion for a new york city .
35
democratic taxes
proposed
future state
spending association
administration
60
budget gov.
revenue vote
costs mayor
california
conservative
35 generated sentence:
the council on a group of
republican senators and the
governor 's joint public defenders ,
brandon baltimore and other state
and local officials , have
campaigned in chants of defending
and speaking with the defense
have been very confident .
60 generated sentence:
adrian on thursday issued an officer a
news release saying the two groups wil l
take more than $ 40,000 for contacts
with the private nonprofit .
11
budget revenue loan gains
treasury incentives profits
84
gov. vote months conservation
ballot reform fundraising
11 generated sentence:
the office of north dakota has been
offering a $ 22 mil lion bond to a $
68 million budget with the
proceedsfrom a escrow account.
48-60-11 generated sentence:
a new report shows the state
has been hit by a number of
shortcomings in jindal 's budget
proposal for the past decade .
84 generated sentence:
the u.s. sen. joe mccoy in the democratic
party says russ of the district , must take
the rest of the vote on the issues in the
first half of the year .
48-60-84 generated sentence:
it was partially the other in the republican
caucus that raised significant amounts of
spending last year and ended with cuts
from a previous government shutdown .
Figure 4:	Topics and their temporal trajectories inferred by a three-hidden-layer rGBN-RNN from the
APNEWS dataset, and the generated sentences under topic guidance (best viewed in color). Top words of each
topic at layer 3,2,1 are shown in orange, yellow and blue boxes respectively, and each sentence is shown in a
dotted line box labeled with the corresponding topic index. Sentences generated with a combination of topics in
different layers are at the bottom of the figure.
rGBN-RNN can allow more specific information to transmit through lower layers, while allowing
more general higher level information to transmit through higher layers. Our proposed model have the
ability to learn hierarchical structure of the sequence, despite without designing the multiscale RNNs
on purpose like Chung et al. (2017). We also visualize the language model of GBN-RNN in Fig. 3(b);
with much less smoothly time-evolved deeper layers, GBN-RNN fails to utilize its stacked RNN
structure as effectively as rGBN-RNN does. This suggests that the language model is much better
trained in rGBN-RNN than in GBN-RNN for capturing long-range temporal dependencies, which
helps explain why rGBN-RNN exhibits clearly boosted BLEU scores in comparison to GBN-RNN.
Hierarchical topics: We present an example topic hierarchy inferred by a three-layer rGBN-RNN
from APNEWS. In Fig. 4, we select a large-weighted topic at the top hidden layer and move down the
network to include any lower-layer topics connected to their ancestors with sufficiently large weights.
Horizontal arrows link temporally related topics at the same layer, while top-down arrows link
hierarchically related topics across layers. For example, topic 48 of layer 3 on “budget, lawmakers,
gov., revenue,” is related not only in hierarchy to topic 57 on “lawmakers, pay, proposal, legislation”
and topic 60 of the lower layer on “budget, gov., revenue, vote, costs, mayor,” but also in time to topic
35 of the same layer on “democratic, taxes, proposed, future, state.” Highly interpretable hierarchical
relationships between the topics at different layers, and temporal relationships between the topics at
the same layer are captured by rGBN-RNN, and the topics are often quite specific semantically at the
bottom layer while becoming increasingly more general when moving upwards.
Sentence generation under topic guidance: Given the learned rGBN-RNN, we can sample the
sentences both conditioning on a single topic of a certain layer and on a combination of the topics
from different layers. Shown in the dotted-line boxes in Fig. 4, most of the generated sentences
conditioned on a single topic or a combination of topics are highly related to the given topics in
terms of their semantical meanings but not necessarily in key words, indicating the language model is
successfully guided by the recurrent hierarchical topics. These observations suggest that rGBN-RNN
has successfully captured syntax and global semantics simultaneously for natural language generation.
Sentence/paragraph generation conditioning on a paragraph: Given the GBN-RNN and rGBN-
RNN learned on APNEWS, we further present the generated sentences conditioning on a paragraph,
as shown in Fig. 5. To randomly generate sentences, we encode the paragraph into a hierarchical
latent representation and then feed it into the stacked-RNN. Besides, we can generate a paragraph
with rGBN-RNN, using its recurrent inference network to encode the paragraph into a dynamic
hierarchical latent representation, which is fed into the language model to predict the word sequence
8
Under review as a conference paper at ICLR 2020
Document
O the senate sponsor (…)，a house committee last week removed photo ids issued by public colleges and universities from the measure
sponsored by republican rep. susan lynn, who said she agreed with the change . the house approved the bill on a 65-30 vote on monday
evening . but republican sen. bill ketron in a statement noted that the upper chamber overwhelmingly rejected efforts to take student
ids out of the bill when it passed 21-8 earlier this month . ketron said he would take the bill to conference committee if needed .
Generated Sentences with GBN-RNN
• if the house and senate agree, it will be the first time they 'll have to Seek their first meeting .
Othe proposal would also give lawmakers with more money to protect public safety , he said .
Generated Sentences with rGBN-RNN
O the proposal, which was introduced in the house on a vote on wednesday , has already passed the senate floor to the house .
O the city commission voted last week to approve the law, which would have allowed the council to approve the new bill.
Generated temporal Sentences with rGBN-RNN (Paragraph)
O senate president pro tem joe scarnati said the governor's office has never resolved the deadline for a vote in the house . the proposal is a new
measure version ofthe bill to enact a senate committee to approve the emergency manager's emergency license . the house gave the bill to
six weeks of testimony , but the vote now goes to the full house for consideration . jackson signed his paperwork wednesday with the
legislature .the proposal would also give lawmakers with more money to protect public safety , he said . "a spokesman for the federal
department of public safety says it has been selected for a special meeting for the state senate to investigate his proposed law . a new state
house committee has voted to approve a measure to let idaho join a national plan to ban private school systems at public schools . the
campaign also launched a website at the university of california , irvine, which are studyingthe current proposal .
Figure 5:	Examples of generated sentences and paragraph conditioned on a document from APNEWS (green
denotes novel words, blue the key words in document and generated sentences.)
in each sentence of the input paragraph. It is clear that both the proposed GBN-RNN and rGBN-RNN
can successfully capture the key textual information of the input paragraph, and generate diverse
realistic sentences. Interestingly, the rGBN-RNN can generate semantically coherent paragraphs,
incorporating contextual information both within and beyond the sentences. Note that with the
topics that extract the document-level word cooccurrence patterns, our proposed models can generate
semantically-meaningful words, which may not exist in the original document.
4 Conclusion
We propose a recurrent gamma belief network (rGBN) guided neural language modeling framework, a
novel method to learn a language model and a deep recurrent topic model simultaneously. For scalable
inference, we develop hybrid SG-MCMC and recurrent autoencoding variational inference, allowing
efficient end-to-end training. Experiments results conducted on real world corpora demonstrate that
the proposed models outperform a variety of shallow-topic-model-guided neural language models,
and effectively generate the sentences from the designated multi-level topics or noise, while inferring
interpretable hierarchical latent topic structure of document and hierarchical multiscale structures
of sequences. For future work, we plan to extend the proposed models to specific natural language
processing tasks, such as machine translation, image paragraph captioning, and text summarization.
Another promising extension is to replace the stacked-RNN in rGBN-RNN with Transformer, i.e.,
constructing an rGBN guided Transformer as a new larger-context neural language model.
References
Sungjin Ahn, Heeyoul Choi, Tanel Parnamaa, and Yoshua Bengio. A neural knowledge language
model. arXiv: Computation and Language, 2017.
D. M. Blei and J. D. Lafferty. Dynamic topic models. In ICML, 2006.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993-1022, 2003.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Fethi Bougares, and Yoshua Bengio.
Learning phrase representations using RNN encoder-decoder for statistical machine translation. In
Computer Science, 2014.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.
In ICLR, 2017.
Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou. Deep latent Dirichlet allocation with
topic-layer-adaptive stochastic gradient Riemannian MCMC. In ICML, 2017a.
9
Under review as a conference paper at ICLR 2020
Yulai Cong, Bo Chen, and Mingyuan Zhou. Fast simulation of hyperplane-truncated multivariate
normal distributions. Bayesian Anal., 12(4):1017-1037, 2017b.
BNC Consortium. The British National Corpus, version 3 (BNC XML Edition). http://www.
natcorp.ox.ac.uk, 2007.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. In ACL, 2019.
Jacob Devlin, Mingwei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In north american chapter of the association
for computational linguistics, pp. 4171-4186, 2019.
Adji B Dieng, Chong Wang, Jianfeng Gao, and John Paisley. TopicRNN: A recurrent neural network
with long-range semantic dependency. In ICLR, 2017.
Zhe Gan, Changyou Chen, Ricardo Henao, David Carlson, and Lawrence Carin. Scalable deep
Poisson factor analysis for topic modeling. In ICML, pp. 1823-1832, 2015.
Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin,
and Li Deng. Semantic compositional networks for visual captioning. In CVPR, pp. 1141-1150,
2017.
Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization. In
EMNLP, pp. 4098-4109, 2018.
Mark A Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo
methods. Journal of The Royal Statistical Society Series B-statistical Methodology, 73(2):123-214,
2011.
A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In
ICASSP, pp. 6645-6649, 2013.
Alex Graves. Generating sequences with recurrent neural networks. arXiv: Neural and Evolutionary
Computing, 2013.
T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of
Sciences, 101:5228-5235, 2004.
Thomas L Griffiths, Mark Steyvers, David M Blei, and Joshua B Tenenbaum. Integrating topics and
syntax. In NeurIPS, pp. 537-544, 2004.
Dandan Guo, Bo Chen, Hao Zhang, and Mingyuan Zhou. Deep Poisson gamma dynamical systems.
In NeurIPS, pp. 8451-8461, 2018.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference.
The Journal of Machine Learning Research, 14(1):1303-1347, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.
D Klein and Christopher D Manning. Accurate unlexicalized parsing. In Meeting of the Association
for Computational Linguistics, 2003.
Jey Han Lau, Timothy Baldwin, and Trevor Cohn. Topically driven neural language model. In
meeting of the association for computational linguistics, pp. 355-365, 2017.
C. Li, C. Chen, D. Carlson, and L. Carin. Preconditioned stochastic gradient Langevin dynamics for
deep neural networks. arXiv, 2015.
10
Under review as a conference paper at ICLR 2020
Y. Ma, T. Chen, and E. Fox. A complete recipe for stochastic gradient MCMC. In NIPS, pp.
2899-2907, 2015.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Huang Dan, Andrew Y. Ng, and Christopher Potts.
Learning Word Vectors for Sentiment Analysis. In Meeting of the Association for Computational
Linguistics: Human Language Technologies, 2011.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan L Yuille. Deep captioning
with multimodal recurrent neural networks (m-RNN). In ICLR, 2015.
Yishu Miao, Edward Grefenstette, and Phil Blunsom. Discovering discrete latent topics with neural
variational inference. In ICML, pp. 2410-2419, 2017.
Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.
In SLT, pp. 234-239, 2012.
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Interspeech, 2010.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions
of recurrent neural network language model. In ICASSP, pp. 5528-5531, 2011.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
ICML, pp. 1791-1799, 2014.
Sam Patterson and Yee Whye Teh. Stochastic gradient Riemannian Langevin dynamics on the
probability simplex. In NIPS, pp. 3102-3110, 2013.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. 2018.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical
sequence training for image captioning. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 7008-7024, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In ICML, pp. 1278-1286, 2014.
Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive
sentence summarization. In EMNLP, pp. 379-389, 2015.
Aaron Schein, Hanna Wallach, and Mingyuan Zhou. Poisson-gamma dynamical systems. In Neural
Information Processing Systems, 2016.
Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. In ICLR,
2017.
Nitish Srivastava, Ruslan Salakhutdinov, and Geoffrey E Hinton. Modeling documents with deep
Boltzmann machines. In Uncertainty in Artificial Intelligence, 2013.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical Dirichlet processes.
Publications of the American Statistical Association, 101(476):1566-1581, 2006.
Wang Tian and Kyunghyun Cho. Larger-context language modelling with recurrent neural network.
In Meeting of the Association for Computational Linguistics, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems,
pp. 6000-6010, 2017.
11
Under review as a conference paper at ICLR 2020
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. In CVPR, 2015.
Hanna M Wallach. Topic modeling: beyond bag-of-words. In ICML, pp. 977-984, 2006.
Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and
Lawrence Carin. Topic compositional neural language model. In AISTATS, pp. 356-365, 2018.
Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen,
and Lawrence Carin. Topic-guided variational autoencoders for text generation. In NAACL, 2019.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
ICML, pp. 681-688, 2011.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In ICML, pp. 2048-2057, 2015.
Hao Zhang, Bo Chen, Dandan Guo, and Mingyuan Zhou. WHAI: Weibull hybrid autoencoding
inference for deep topic modeling. In ICLR, 2018.
He Zhao, Lan Du, Wray Buntine, and Mingyuan Zhou. Dirichlet belief networks for topic structure
learning. In Neural Information Processing Systems, pp. 7955-7966, 2018.
Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 37(2):307-320, 2015.
Mingyuan Zhou, Yulai Cong, and Bo Chen. Augmentable gamma belief networks. J. Mach. Learn.
Res., 17(163):1-44, 2016.
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen:
A benchmarking platform for text generation models. SIGIR, 2018.
12
Under review as a conference paper at ICLR 2020
A	THE GBN-RNN
GBN-RNN: {y1:T, d} denotes a sentence-context pair, where d ∈ ZV+c represents the document-level
context as a word frequency count vector, the vth element of which counts the number of times the
vth word in the vocabulary appears in the document excluding sentence y1:T . The hierarchical model
of a L-hidden-layer GBN, from top to bottom, is expressed as
θL 〜Gam (r, cL+1), ..., θl 〜Gam (Φl+1θl+1,cl+1),...,
θ1 〜Gam (Φ2θ2, c2), d〜Pois (Φ1θ1),	(9)
The stacked-RNN based language model described in (3) is also used in GBN-RNN.
Statistical inference: To infer GBN-RNN, we consider a hybrid of stochastic gradient MCMC
(Welling & Teh, 2011; Patterson & Teh, 2013; Li et al., 2015; Ma et al., 2015; Cong et al., 2017a),
used for the GBN topics φlk, and auto-encoding variational inference (Kingma & Welling, 2013;
Rezende et al., 2014), used for the parameters of both the inference network (encoder) and RNN.
More specifically, GBN-RNN generalizes Weibull hybrid auto-encoding inference (WHAI) of Zhang
et al. (2018): it uses a deterministic-downward-stochastic-upward inference network to encode the
bag-of-words representation of d into the latent topic-weight variables θl across all hidden layers,
which are fed into not only GBN to reconstruct d, but also a stacked RNN in language model, as
shown in (3), to predict the word sequence in y1:T. The topics φlk can be sampled with topic-layer-
adaptive stochastic gradient Riemannian (TLASGR) MCMC, whose details can be found in Cong
et al. (2017a); Zhang et al. (2018), omitted here for brevity. Given the sampled topics φlk , the joint
marginal likelihood of {y1:T , d} is defined as
Yp Iyt I yi：t-i, θ1''L)][γp(θl I Φl+1θl+1)] dθLL. (10)
t=i	l=i
For efficient inference, an inference network as Q = QlL=1 q(θl I d, Φl+1θl+1) is used to provide a
ELBO of the log joint marginal likelihood as
q(θl | d,Φl + 1θl + 1)
ln p(θl | φl + 1θl + 1)
(11)
and the training is performed by maximizing Epdata(y1:T,d) [L(y1:T, d)]; following Zhang et al. (2018),
we define q(θl I d, Φl+1, θl+1) = Weibull(kl + Φl+1θl+1, λl), where both kl and λl are determin-
istically transformed from d using neural networks. Distinct from a usual variational auto-encoder
whose inference network has a pure bottom-up structure, the inference network here has a determistic-
UPWard-stoαchstic-downward ladder structure (Zhang et al., 2018).
p(yi：T ,d∣{Φl}ι) = /p(d∣Φ1θ1)
TL
L(y1:T, d) = Eq lnP (d| Φ1θ1) + P lnp (yt ∣ y1∙∙t-1, θ1:L) — P EQ
t=1	l=1
B The coupling vector
Following Lau et al. (2017), the alj,t = gl hlj,t, θjl can be imPlemented with a gating unit similar to
a GRU (Cho et al., 2014), describe as
zj,t = σ (Wz θj + Uz hj,t + bZ)
rj,t=σ (Wr θj+Ur hj,t+br)
hj,t = tanh (Whθj + Uh (rj,t Θ hj,t) + bh)
aj,t = (I-Zlj, J θ hj,t + zj,t θ hj,t
C SGMCMC FOR RGBN-RNN
To allow for scalable inference, we aPPly the toPic-layer-adaPtive stochastic gradient Riemannian
(TLASGR) MCMC algorithm described in Cong et al. (2017a); Zhang et al. (2018), which can be
used to samPle simPlex-constrained global Parameters Cong et al. (2017b) in a mini-batch based
manner. It imProves its samPling efficiency via the use of the Fisher information matrix (FIM)
13
Under review as a conference paper at ICLR 2020
Girolami & Calderhead (2011), with adaptive step-sizes for the latent factors and transition matrices
of different layers. In this section, we discuss how to update the global parameters {Φl , Πl }lL=1 of
rGBN in detail and give a complete one in Algorithm in 1.
Sample the auxiliary counts: This step is about the “backward” and “upward” pass. Let us denote
Zlkj = Pkι = 1	Zk ιkj,	Zlk,J +1 = 0, and	xkj,	=	dvj,	where	dj	=	{d1j,.., dvj,..,	dVcj }	is shown
in (2). Working backward for j = J, ..., 1 and upward for l = 1, ..., L, we draw
(AkIj,...,AkKlj)〜Multi fχkljl); PKi”),..., pKιkκf Ij ) ,	(13)
kl=1 φkkl klj	kl=1 φkkl klj
xk+1 〜CRTakj + Zlk,j+ι,τ0 (XK+11=1 娟 l1+1 θk+1ιj + XK=InkkI θk 1,j-1)].	(14)
Note that via the deep structure, the latent counts xlk+j 1 will be influenced by the effects from
both time j - 1 at layer l and time j at layer l + 1. With p1 := PkKl=1 πkl k θkl j-1 and p2 :=
PkKl+l+11=1 φlk+kl1+1θkl+l+11j, we can sample the latent counts at layer l and l + 1 by
(Xk+1,l,xk+1,l+1)〜Multi 卜k+1,pι∕(PI + P2),P2/(P1 + P20，	(I5)
and then draw
(Zι	Zι	) 〜Muhi χι+ι,ι.	πk 1θ1,jτ	πkKlθκl,j-1	∖	(16)
IZkIjI …IZkKlj)	Multi xkj ; PKl	ι Qi	,…，LKl	ι Z)(I)	.	(16)
kl=1 πkklθkl,j-1	kl=1 πkklθkl,j-1
In rGBN, the prior and the likelihood of {Φl}lL=1 is very similar with {Πl}lL=1, so we also apply the
TLASGR MCMC sampling algorithm on both of them conditioned on the auxiliary counts.
Sample the hierarchical components {Φl}lL=1: For φlk, the kth column of the loading matrix Φl
of layer l, its sampling can be efficiently realized as
(φk)η+1= (φk)n + P h(pAlk.+η0) - (pAlk.+ KiTnO) (φk)n]
k
+ N (0, Pn [diag(φk)n - (φk )n(φlk )T ])	,	(17)
where Pk is calculated using the estimated FIM, Akij∙ = PJ=I Akl 廿 Alk∙ = {A1j∙.,…，AKj∙}T
and Alk∙ = Pkl=ι Akj∙, Akkj comes from the augmented latent counts Al in (13), η0 denote the
prior of φk, and [∙]∠ denotes a simplex constraint.
Sample the transmission matrix {Πl}lL=1: For πkl , the kth column of the transition matrix Πl of
layer l, its sampling can be efficiently realized as
(πk)η+1 j(πk)η + M [(pZlk.+ ηlk) - (pZlk.+nlk) (πk)n]
+ N (0, M [diag(nk)n - (πk)n(πk)T] ) ,	(18)
where Mkl is calculated using the estimated FIM, Zklj∙ = PJ=I Zklkj, Zlk∙ = {Zlj∙,…，ZKlj∙}T
and Zlk∙ = Pkl = ι Zk j∙, ZkIkj comes from the augmented latent counts Zl in (16), and [.]∠ denotes
a simplex constraint, and ηlk denotes the prior of ∏k, more details about TLASGR-MCMC for our
proposed model can be found in Cong et al. (2017a).
D	Datasets
We consider three publicly available corpora1. APNEWS is a collection of Associated Press news
articles from 2009 to 2016, IMDB is a set of movie reviews collected by Maas et al. (2011), and BNC
1https://ibm.ent.box.com/s/ls61p8ovc1y87w45oa02zink2zl7l6z4
14
Under review as a conference paper at ICLR 2020
is the written portion of the British National Corpus (Consortium, 2007). Following the preprocessing
steps in Lau et al. (2017), we tokenize words and sentences using Stanford CoreNLP (Klein &
Manning, 2003), lowercase all word tokens, and filter out word tokens that occur less than 10 times.
For the topic model, we additionally exclude stopwords2 and the top 0.1% most frequent words. All
these corpora are partitioned into training, validation, and testing sets, whose summary statistics are
provided in Table 2 of the Appendix.
Table 2: Summary statistics for the datasets.
Dataset	Vocubalry		Training			Validation			Testing		
	LM	TM	Docs	Sents	Tokens	Docs	Sents	Tokens	Docs	Sents	Tokens
APNEWS	34231	32169	50K	0.8M	15M	~2K	33K	0.6M	~2K~~	32K	0.6M
IMDB	36009	34925	75K	1.1M	20M	12.5K	0.18M	0.3M	12.5K	0.18M	0.3M
BNC	43703	41552	15K	1M	18M	1K	57K	1M	1K	66K	1M
E Complexity of rGBN-RNN
The proposed rGBN-RNN consists of both language model and topic model components. For the
topic model component, there are the global parameters of rGBN (decoder), including {Φl , Πl}lL=1
in (2) , and the parameters of the variational recurrent inference network (encoder), consisting of
RNNlsent , fkl , and fλl in (8). The language model component is parameterized by LSTMlword in (3)
and the coupling vectors gl described in Appendix B. We summarize in Table 3 the complexity of
rGBN-RNN (ignoring all bias terms), where V denotes the vocabulary size of the language model,
E the dimension of word embedding vectors, Vc the size of the vocabulary of the topic model that
excludes stop words, Hlw the number of hidden units of the word-level LSTM at layer l (stacked-RNN
language model), Hls the number of hidden units of the sentence-level RNN at layer l (variational
recurrent inference network), and Kl the number of topics at layer l.
Table 4 further compares the number of parameters between various RNN-based language models,
where we follow the convention to ignore the word embedding layers. Some models in Table 1
are not included here, because we could not find sufficient information from their corresponding
papers or code to accurately calculate the number of model parameters. Note when used for language
generation at the testing stage, rGBN-RNN no longer needs its topics {Φl }, whose parameters are
hence not counted. Note the number of parameters of the topic model component is often dominated
by that of the language model component.
Table 3: Complexity of the three-layer rGBN-RNN.
Component	Language Model		Topic Model				
Param	LSTMwordin ⑶	gl inB	Φ in⑵	Πl in (2)	RNNsentin⑻	f in (8)	f in (8)
Layer1 Layer2 Layer3	O(4 × (E + Hw) × Hw) O(4 × (Hw + Hw) × Hw) O(4 × (Hw + Hw) × Hw)	O(3 × (Ki + Hw) × Hw) θ(3 × (K2 + Hw) × Hw) O(3 × (K3 + Hw) × Hw)	O(Vc × Ki) O(K1 × K2) O(K2 × K3)	O(K1 × Ki) O(K2 × K2) O(K3 × K3)	O((Vc + HS)× Hs) O((Hs + Hs) × Hs) O((H2 + Hs) × Hs)	O(Hs) O(Hs) O(HS)	O(K1 × HS) O(K2 × HS) O(K3 × HS)
Table 4: Comparison of the number of parameters of different models when used for language generation.
Model	LSTM Size	Topic Size	# LM Param	#TM Param	# All Param
TDLM (Lau et al., 2017)	600	100	3.35M	0.019M	3.37M
	900-900	100	13.38M	0.019M	13.40M
	600	—	2.16M	—	2.16M
basic-LSTM (Hochreiter & Schmidhuber, 1997)	900-900	—	10.80M	—	10.80M
	900-900-900	—	17.68M	—	17.68M
	600	100	3.40M	0.02M	3.42M
GBN-RNN	600-512	100-80	6.50M	0.04M	6.54M
	600-512-256	100-80-50	7.20M	0.05M	7.25M
	600	100	3.40M	0.03M	3.43M
rGBN-RNN	600-512	100-80	6.50M	0.06M	6.56M
	600-512-256	100-80-50	7.20M	0.07M	7.27M
2We use Mallet’s stopword list: https://github.com/mimno/Mallet/tree/master/stoplists.
15
Under review as a conference paper at ICLR 2020
F More experimental results on IMDB and BNC
13-1-26 generated sentence:
the plot and the acting Were
all so horrible and the script
was overly melodramatic.
the educational situation for the
debates , to the right as to the
implementation of the student
attitudes , and to the interests of all
existing social servicesto decide .
13-10-29 generated sentence:
the story is very interesting ,
and the movie is well crafted ,
it runs a couple of years and
that's what it's all about.
13-10-21 generated sentence:
nothing in this movie makes
sense in the beginning , and
the script for this movie is so
ridiculous.
Figure 6:	Topics and their temporal trajectories inferred by a three-hidden-layer rGBN-RNN from the IMDB
dataset, and the generated sentences under topic guidance (best viewed in color). Top words of each topic
are shown in orange, yellow and blue box, and each sentence is shown in a dotted line box labeled with the
corresponding topic index. Sentences generated with a combination of topics in different layers are at the bottom
of the figure.
15 generated sentence:
the advance of the educational
survey on agreement highlighted these
principles and many of the other major
obstacles to effective compliance .
members education welfare
central minister create reform
benefits reform development
association bill determination
details decision british, france
appraisal questionnaire
curriculum schools formal
.decisions training Primary J
society authorities contributions
formal election list voters
professional alliance
8 generated sentence:
a good example of the poor
would be lodge with the
educated counsellor if the lea
have no regional governors .
19
tax service
royal industry
air secretary
bsia church
agreements
schools education
welfare central
young curriculum
class formal
15
society members
education formal
appraisal curriculum
questionnaire
classroom minister
46
cent form general
included forces section
single environment
personal meeting
association
society formal authorities
contributions professional
73 generated sentence:
how We get to know what the need is for
the <unk> to consider in considering that
our right to think of the crucial outcome
which we were to give account of the
public and civil society .
7 generated sentence:
the school is considering what the
societythinksitis .
15-8-7 generatedsentence:
the school was found into the
property exchanges for further
subsistence changes , or the
existence of a deduction of added
discontent with the prime minister.
86 generated sentence:
on the reforms such as the extent to
which the administration might have
been divided by the recent enactment of
the alarm it had entrusted staff at all
politicians to allow a legal order on it .
15-8-86 generated sentence:
the educational situation for the
debates , to the right as to the
implementation of the student
attitudes , and to the interests of all
existing social services to decide .
21 generated sentence:
as a civil cabinet for the present
processing authority at the national
assembly , described the reforms as
not an opportunity to compel the
sovereign to appreciate .
15-73-21 generated sentence:
this election is said to have been
contested ofthe high islam that dragged
the government on the peril of the old
janata alliance and generally seen one-
sixth of voters in the european.
28 generatedsentence:
the government was to continue to
attempt to establish a meeting with
france , but it in turn of its
motivation must be lost away.
15-73-28 generated sentence:
the second part of the aid is to
demonstrate that the practical
strategy of the association is to be
negotiated by the general secretary
of the government.
Figure 7:	Topics and their temporal trajectories inferred by a three-hidden-layer rGBN-RNN from the BNC
dataset, and the generated sentences under topic guidance (best viewed in color). Top words of each topic
are shown in orange, yellow and blue box, and each sentence is shown in a dotted line box labeled with the
corresponding topic index. Sentences generated with a combination of topics in different layers are at the bottom
of the figure.
16
Under review as a conference paper at ICLR 2020
G BLEU SCORES FOR IMDB
∙LSTM3
0.50
0.45
H 0.40
S
§ 0.35
u
0.30
0.25
0.20	.	.	.	.	.
0.55	0.60	0.65	0.70	0.75	0.80	0.85
test-BLEU
TDLM
•rGBN-RNN-1
•GBN-RNN-2 QrGBN-RNN-2
.GBN-RNN-I
•GBN-RNN-3
βrGBN-RNN-3
Ground-Truth
0.275
0.250
0.225
0.200
0.175
0.150
0.125
0.100
∙LSTM3
•rGBN-RNN-1
•GBN-RNN-1
eGBN-RNN-2 rGBN-RNN-2
βGBN-RNN-3 ∙GBN-RNN-3
Ground-Truth
0.3	0.4	0.5	0.6	0.7
test-BLEU
Figure 8:	BLEU scores of different methods for IMDB. x-axis denotes test-BLEU, and y-axis self-BLEU. Left
panel is BLEU-3 and right is BLEU-4, and a better BLEU score would fall within the lower right corner, where
black point represents mean value and circles with different colors denote the elliptical surface of probability of
BLEU in a two-dimensional space.
H BLEU SCORES FOR APNEWS
• LSTM3
• LSTM3
-54∙3
Ooo
nτM*s
• GBN-RNN-I . rGBN-RNN-1
0.2
• rGBN-RNN-1
*θ≡⅛-2 IGBN-RNNa
TDLM - Ground-Truth
0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60
test-BLEU
0.10
0.05	.	.	.	.	.	.
0.200 0.225 0.250 0.275 0.300 0.325 0.350
test-BLEU
Figure 9:	BLEU scores of different methods for APNEWS. x-axis denotes test-BLEU, and y-axis self-BLEU.
Left panel is BLEU-3 and right is BLEU-4, and a better BLEU score would fall within the lower right corner,
where black point represents mean value and circles with different colors denote the elliptical surface of
probability of BLEU in a two-dimensional space.
17