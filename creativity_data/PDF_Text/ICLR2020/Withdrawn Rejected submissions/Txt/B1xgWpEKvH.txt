Under review as a conference paper at ICLR 2020
AdaSample: Adaptive Sampling of Hard
Positives for Descriptor Learning
Anonymous authors
Paper under double-blind review
Ab stract
Triplet loss is commonly used in descriptor learning, where the performance heav-
ily relies on mining triplets. Typical solution to that is first picking pairs of intra-
class patches (positives) from the dataset to form batches, and then selecting in-
batch negatives to construct triplets. For high-informativeness triplet collection,
researchers mainly focus on mining hard negatives in the second stage, while they
pay relatively less attention to constructing informative batches, i.e., matching
pairs are often randomly sampled from the dataset. To address this issue, we pro-
pose AdaSample, an adaptive and online batch sampler. Specifically, we sample
positives based on their informativeness, and formulate our hardness-aware posi-
tive mining pipeline within a novel maximum loss minimization training protocol.
The efficacy of the proposed method is demonstrated in several standard bench-
marks, in which it results in a significant and consistent performance gain on top
of the existing strong baselines. The source code will be released upon acceptance.
1	Introduction
Feature descriptors are widely used for finding correspondences (Bian et al., 2017; Yi et al., 2018)
across images, and hence allow for many computer vision applications, including structure-from-
motion (Agarwal et al., 2009), image retrieval (Philbin et al., 2010), and panorama stitching (Brown
& Lowe, 2007). Despite rich efforts made on that, the representation power of traditional descriptors,
like SIFT (Lowe, 2004), is limited. With the rapid development of deep learning, recent works (Han
et al., 2015; Zagoruyko & Komodakis, 2015; Yi et al., 2016; Tian et al., 2017) show that CNN-based
descriptors achieve better performance in terms of patch classification (Brown et al., 2011). Espe-
cially, HardNet (Mishchuk et al., 2017) demonstrates the generalization ability of deeply learned
descriptors in multiple vision tasks.
Learning descriptors from a large image patch collection is challenging. Other than network de-
sign and optimization, sampling informative pairs or triplets of patches is also critical. Mishchuk
et al. (2017) leverage the hardest-in-batch solution to construct high-quality triplets, making sig-
nificant progress on the performance and generalization ability. This demonstrates the importance
of triplet sampling in descriptor learning. However, current sampling strategies focus on sampling
hard triplets in batch level, while batches are randomly constructed from the dataset. We propose to
construct high-informativeness batches in a more principled manner to improve the performance.
The proposed method is nominated as AdaSample, which adaptively collects informative positive
pairs from the dataset for batch construction. The methodology is developed based on informa-
tiveness analysis, where we define informativeness of potential samples and estimate their optimal
sampling probabilities. Moreover, we propose a novel training protocol inspired by maximum loss
minimization (Shalev-Shwartz & Wexler, 2016) to boost the generalization ability of the descrip-
tor network. We make comprehensive evaluations and ablation studies of the proposed approach in
several standard benchmarks, in which the results clearly demonstrate the superiority of our method.
2	Related work
Local Descriptor Learning. Traditional handcrafted descriptors, e.g. SIFT (Lowe, 2004), extract
low-level texture information from image patches. Due to the low representation ability, recent re-
search has gradually been shifted to the learned descriptors. Han et al. (2015) propose a two-stage
Siamese architecture to extract features and measure patch similarity, which significantly improves
1
Under review as a conference paper at ICLR 2020
the performance and demonstrates the great potential of CNNs in descriptor learning. In (Zagoruyko
& Komodakis, 2015), another Siamese network is introduced to explore different network architec-
tures with a central-surround structure to boost the performance. Simo-Serra et al. (2015) mine hard
negatives and use a shallow architecture for the pairwise similarity.
Balntas et al. (2016) use a triplet-based architecture and show its superiority against the pair-based
one. Kumar et al. (2016) use a triplet network and a global loss function to separate the distribution
of matching and non-matching pairs. L2-Net (Tian et al., 2017) uses n matching pairs in each batch
to generate n(n - 1) negative samples and requires that the distance of matching pairs is minimum
in each row and column. HardNet (Mishchuk et al., 2017) mines the hardest-in-batch negatives and
significantly improves performance. Recently, SOSNet (Tian et al., 2019a) proposes a second order
similarity regularization term, and achieves more compact patch clusters in the feature space.
The triplet mining framework can generally be decomposed into two stages, i.e., batch construction
from the dataset and triplet generation within the mini-batch. Previous works mainly focus on
mining hard negatives in the second stage, while the first stage is often cursorily done by random
sampling. Therefore, we argue that their triplet sampling solutions are still sub-optimal. To address
this problem, we propose AdaSample to construct informative batches in the first place. Combined
with the hardest-in-batch solution in Mishchuk et al. (2017) to mine negatives within the mini-
batches, we formulate a complete and powerful triplet mining framework for descriptor learning.
Hard Negative Mining. Hard negative mining is widely used in object detection (Tian et al.,
2019b) and other vision tasks for collecting good training examples. FaceNet (Schroff et al., 2015)
proposes to sample semi-hard triplets within the batch. Wu et al. (2017) select training examples
based on their relative distances. Our sampling solution differs from them in that we analyze the
informativeness of the training data and ensure the most useful gradients for parameters update. Be-
sides, our method adaptively adjusts the hard level of the selected training samples with training.
Therefore, well-classified samples are filtered out, and the network is always fed with informative
triplets with suitable hard levels. Comprehensive results demonstrate consistent performance im-
provement contributed by our proposed approach.
3	Methodology
3.1	Problem Overview
Given a dataset that consists of N classes with each containing k patches, we decompose the triplet
generation into two stages. Firstly, we select n matching pairs (positives) to form a mini-batch,
where n is the batch size. This is done by proposed AdaSample, as introduced in Section 3.2.
Secondly, we mine the hardest-in-batch negatives for each matching pair, and use the triplet loss to
supervise the network training, as in Section 3.3. The overall solution is summarized in Section 3.4.
3.2	AdaSample
Previous works (Tian et al., 2017; Mishchuk et al., 2017) sample positives randomly to construct
batches, yielding a majority of similar matching pairs which can be easily discriminated by the net-
work. This reduces the overall “hardness” of the triplets. Motivated by the hardest-in-batch mining
strategy in Mishchuk et al. (2017), a naive solution is to select the most dissimilar matching pairs.
However, this has potential issues, i.e., the network may be trained with bias in favor of dissimi-
lar matching pairs, while other cases are less-considered. We validate this solution, nominated as
Hardpos, in experiments (Section 5.3).
A more principled solution is to sample positives based on their informativeness. Here we assume
that more informative pairs are those contributing more to the optimization, i.e., providing effec-
tive gradients for parameters update. Therefore, we quantify the informativeness of matching pairs
by measuring their contributing gradients during training. Moreover, we employ maximum loss
minimization (Shalev-Shwartz & Wexler, 2016) to improve the generalization ability of the learned
model. We then show that the resulting gradient estimator is an unbiased estimator of the true gradi-
ent. In the following, we introduce our derivation and elaborate theoretical justification in Section 4.
Informativeness Based Sampling. In the end-to-end learning literature, the training data con-
tribute to learning via gradients, so we measure the informativeness of training examples by analyz-
2
Under review as a conference paper at ICLR 2020
ing their resulting gradients. Generally, we consider the generic deep learning training framework,
in which each data-label pair contributes independent gradients. Let (xi , yi) be the ith data-label
pair of the training set, f (x; θ) be the model parameterized by θ, and L(∙, ∙) be a differentiable loss
function. The goal is to find the optimal model parameters θ* that minimize the average loss, i.e.,
1K
θ* = arg min K 二 L(f (xi； θ), yi),	(1)
where K denotes the number of training examples. Then we proceed our idea with the following
definition on the informativeness.
Definition 1. The informativeness of a training example (xi, yi) is quantified by how much it con-
tributes in the training iteration t, namely,
info(xi, yi):=11▽%L(f(xi； θ), yi)∣∣2.	(2)
At iteration t, let Pt = {pt1, . . . ,ptK} be the sampling probabilities of each datum in the training
set. More generally, We also re-weight each sample by Wt,…，WK. Let random variable It denote
the sampled index at iteration t, then It 〜 Pt, namely, P(It = i) = pt. We record the re-weighted
gradient induced by training sample (xi, yi) as
Gti= w"θtL(f (Xi； θ), yi).	(3)
For simplicity, we omit the superscript t when no ambiguity is made. By setting Wi = K—, we can
Kpi
make the gradient estimator Gi an unbiased estimator of the true gradient, i.e.,
1K
EIt〜Pt [Git] = Vθt K E L(f (xi； θ), yi).	(4)
K i=1
Without loss of generality, we use stochastic gradient descent (SGD) to update model parameters:
θt+ι = θt - ηtwIRθtL(f (XIt; θ), yIt) = θt - ηtGIt,	(5)
where η is the learning rate at iteration t. As the goal is to find optimal θ*, we define the expected
progress towards the optimum at each iteration as follows.
Definition 2. At iteration t, the expected parameter rectification Rt is defined as the expected re-
duction of the squared distance between the parameter θ and the optimum θ* after iteration t,
Rt ：= -EIt〜& [∣∣θt+ι - θ*∣∣2 -∣∣θt - θ*∣∣2] .	(6)
Generally, tens of thousands iterations are included in the training, so the empirical average parame-
ter rectification will converge to the average of Rt asymptotically. By maximizing Rt, we guarantee
the most progressive step towards parameters optimum at each iteration in the expectation sense.
Inspired by the greedy algorithm (Edmonds, 1971), we aim to maximize Rt at each iteration.
It can be shown that maximizing Rt is equivalent to minimizing tr(Var [GIt]), as shown in Theo-
rem 1. Under this umbrella, we show that the optimal sampling probability is proportional to the
per-sample gradient norm (a special case of Theorem 2). Therefore, the optimal sampling proba-
bility of each datum happens to be proportional to its informativeness. This property justifies our
definition of informativeness as the resulting gradient norm of each training example.
However, as the neural network has multiple layers with a large number of parameters, it is computa-
tionally prohibitive to calculate the full gradient norm. Instead, we prove that the matching distance
in the feature space is a good approximation to the informativeness1 in Section 4.2. Concretely, for
each class consisting of k patches {Xi : i = 1, . . . , k}, we first select a patch Xi0 randomly, which
serves as the anchor patch, and then sample a matching patch Xi with probability
Pi H d(Xi, Xio), for i = i0,	(7)
where Xi is the extracted descriptor of Xi, and d(∙, ∙) measures the discrepancy of the extracted
descriptors. See specific choices of d in Section 3.4.
1The approximation is up to a constant factor, which is insignificant as it will be offset by the learning rate.
The same reasoning applies to the approximation of gradients in Maximum Loss Minimization paragraph.
3
Under review as a conference paper at ICLR 2020
Maximum Loss Minimization. Minimizing the average loss may be sub-optimal because the
training tends to be overwhelmed by well-classified examples that may contribute noisy gradients
(Lin et al., 2017). On the contrary, well-classified examples can be adaptively filtered out by min-
imizing the maximum loss (Shalev-Shwartz & Wexler, 2016), which can further improve the gen-
eralization ability of models. However, directly minimizing the maximum loss may lead to insuffi-
cient usage of data and sensitivity to outliers, so we approximate the gradient of maximum loss by
Vθt K PK=1 Lα, in which α is sufficiently large. As GIt is used to update parameters, consider its
expectation
K
EIt~Pt [GIt] = EIt~Pt [wItVθtLIt] = XPiWiVθtLi.	(8)
i=1
To guarantee GIt to be an unbiased estimator of V θt 六 PK=1 Lf, it suffices to set
α
Piwi =不 Li ,	⑼
K
as in this case,
KK
EIt~Pt [GIt] = X KLCr-VLi = X KNetLl	(⑼
i=1 K	i=1 K
Following previous reasoning, we need minimize tr(Var [GIt]) given the constraints in Equation 9,
in order to step most progressively at each iteration. In Theorem 2, we show that the optimal sam-
pling probability and re-weighting scalar should be given by
Pi H LaT∣∣VθtLi∣∣2 and wi H ∣∣VθtLi||-1.	(11)
As previously claimed, we approximate the gradient norm via the matching distance in the feature
space. Besides, in our case, the loss function is hinge triplet loss (Equation 14), which is positively
(or even linearly) correlated with the matching distance squared. Therefore, we use matching dis-
tance squared as an approximation of the loss. Thus, the sampling probability and re-weighting
scalar are given by
Pi H d(xi, xi0)2C-1 and wi H d(xi, xi0)-1, for i 6= i0.	(12)
Moreover, for better approximation, it is preferable to adjust α adaptively, i.e., increase α with
training. Intuitively, when easy matching pairs have already been correctly classified, we focus
more on hard ones. A good indicator of the training progress is the average loss. As a result, instead
of predefining a sufficiently large a, We set 2α — 1 = λ/Lavg, where λ is a tunable hyperparameter
and Lavg is the moving average of history loss. Formally, we formulate our sampling probability
and re-weighting scalar as
1
Pi H d(xi, Xio ILavg and wi H d(xi, Xi°)	, fori = i0.	(13)
The exponent increases adaptively with training, so our sampling method is named as AdaSample.
3.3	Triplet generation by hardest-in-batch
AdaSample focuses on the batch construction stage; nevertheless, for a complete triplet mining
framework, we need to mine negatives from the mini-batch as well. Here, we adopt the hardest-
in-batch strategy in Mishchuk et al. (2017). Formally, given a mini-batch of n matching pairs
{(Xi, X +) : i = 1,..., n}, let (Xi, Xj) be the descriptors extracted from (Xi, X+) 2. For each
matching pair (Xi, X+), we select the non-matching patch which lies closest to one of the matching
patches in the feature space. Then, the Hinge Triplet (HT) loss is defined as follows:
Li = max {t + (dipos)2 -(dineg)2, 0},
dpos = d(Xi, X+),
min
j6=i
{min {d(Xi, Xj), d(X+, χ+)}},
(14)
where t denotes the margin. Incorporating the re-weighting scalar, we update the model parameters
via the gradient estimator Pin=1 wi Vθ Li .
2For clarity, (JX0,JX +) denotes the selected matching pairs, with different pairs belonging to different
classes. X denotes a generic patch in a specific class, where denotes the placeholder for the index.
4
Under review as a conference paper at ICLR 2020
3.4	Distance Metric
Euclidean distance is widely used in previous works (Tian et al., 2017; Mishchuk et al., 2017; Tian
et al., 2019a). However, as descriptors lie on the unit hypersphere in 128-dimensional space (Ap-
pendix C), it is more natural to adopt the geodesic distance of the embedded manifold. Therefore,
we adopt the angular distance (Deng et al., 2018) as follows:
d(Xι, X2) = arccos(Xι ∙ X2),	(15)
where ∙ denotes inner product. We nominate our loss function as Angular Hinge Triplet (AHT) loss,
which is demonstrated to result in consistent performance improvement (see Section 5.3).
Algorithm 1 summarizes the overall triplet generation framework. For each training iteration, we
first randomly pick n distinct classes from the dataset and extract descriptors for patches belonging
to these classes (Step 1, 2). Then, we randomly choose a patch as anchor from each selected class
(Step 4), and adopt AdaSample to select an informative matching patch (Step 5). With the generated
mini-batch, we mine hard negatives following Mishchuk et al. (2017) and compute Angular Hinge
Triplet (AHT) loss (Step 7). An illustration of our sampling pipeline can be found in Appendix B.
Algorithm 1 Pipeline of AdaSample framework.
Require:
Dataset of N classes with each containing k matching patches;
Moving average of history loss Lavg ;
Hyperparameter λ;
1:	Randomly select n distinct classes from the dataset without replacement;
2:	Extract descriptors of the patches belonging to the selected classes;
3:	for each selected class with k patches {Xi : i = 1, . . . , k} do
4:	Sample an anchor patch Xi0 randomly;
5:	Sample a matching patch Xi from the remaining patches with probabilities specified by Equa-
tion 13;
6:	end for
7:	With sampled positive pairs and their descriptors {(Xi, X+)}n=ι, compute Angular Triplet
Hinge loss by Equation 14;
8:	Backpropagate and update model parameters via Pn=ι WNθLi;
4 Theoretical Analysis
In this section, we complete the theoretical analysis of informativeness in Section 4.1, and prove that
the matching distance can serve as a good approximation of informativeness in Section 4.2.
4.1 Informativeness Formulation
Following notations in Section 3.2, we reformulate Rt (Equation 6), and give an equivalent condition
for maximizing Rt. The same conclusion can be found in Katharopoulos & Fleuret (2018).
Theorem 1. Let Rt, θ* and Gi be defined as in Equation 6, 1 and 3, respectively. Then, we have
Rt =	2ηt(θt	- θ*)TEit〜PtGIt]	-	η2Eit〜Pt	[Git]T EIt〜Pt	[Git]	-	n2tr(Var[Gi」).	(16)
Due to unbiasedness (Equation 4), the first two terms in Equation 16 is fixed, so maximizing Rt is
equivalent to minimizing tr(Var [GIt]). Theorem 2 specifies the optimal probabilities to minimize
the aforementioned trace under a more general assumption.
Theorem 2. Let Gi be defined in Equation 3 and suppose the sampled index It obeys distribution Pt.
Then, given the constraints PiWi = Ka La-1, tr(Var [GIt]) is minimized by the following optimal
sampling probabilities:
1K
Pi = Z La-1 ∣∣Vθt Li∣∣2, WheTeZ = ELa-1||V% Lj ∣∣2.	(17)
j=1
Note that in the special case of α = 1, the constraints degrade into PiWi = K and the optimal
sampling probabilities become Pi α ∣∣VθtLi ∣∣2. The proof of the above theorems can be found in
Appendix A.
5
Under review as a conference paper at ICLR 2020
4.2 Approximation of Informativeness
As mentioned in Section 3.2, the matching distance can serve as a good approximation of informa-
tiveness, up to a constant factor. We justify this here. For simplicity, we introduce some notations
for a L-layer multi-layer perceptron (MLP). Let θ(l) ∈ RMl ×Ml-1 be the weight matrix for layer
l and g(l) be a Lipschitz continuous activation function. Then the multi-layer perceptron can be
formulated as follows:
x(0) = x,
h(I) = θ(I)X(IT), forl = 1,…，L,
X(I)= g(I)(h(I)), forl = 1,…，L,
f(x; θ) = x(L),
θ={θ(1),...,θ(L)}.	(18)
Note that although our notations describe only MLPs without bias, our analysis holds for any affine
transformation followed by a Lipschitz continuous non-linearity. Therefore, our reasoning can nat-
urally extend to convolutional neural networks (CNNs). With
Γl(h(l)) = diag ng0(l)(h(1l)),..., g0(l)(h(Ml)l)o,
Π(I)= Γι(h(I))θT+ι …ΓL-i(h(LT))θLΓl(h(L)),	(19)
we have
∣∣Vθ,L(f(x;θ),y)∣∣2 = (∏(l)Vχ(L)L)(X(IT))T ? ≤ IIn(I)g||x('-'/k㈤L∣∣2. (20)
Various data preprocessing, weight initialization (Glorot & Bengio, 2010; He et al., 2015), and acti-
vation normalization (Ioffe & Szegedy, 2015; Lei Ba et al., 2016; Ulyanov et al., 2016) techniques
uniformise the activations of each layer across samples. Therefore, the variation of gradient norms
is mostly captured by the gradient of the loss function w.r.t. the output of neural networks,
info(X, y) = IIVθ L(f (X; θ), y)II2 ≈MkVx(L)Lk2,	(21)
where M is a constant, and M kVx(L) Lk2 serves as a precise approximation of the full gradient
norm. For simplicity, we consider hinge triplet loss (Equation 14) here. Then, the gradient norm
w.r.t. the descriptor of the matching patch is just twice the matching distance3,
kVx(L)Lk2=2dpos.	(22)
As a result, we get the conclusion that the matching distance is a good approximation to informa-
tiveness, up to a constant factor. Also, we empirically verify this in Section 5.3.
5	Experiments
We first compare with the state-of-the-art methods on two standard descriptor datasets: UBC Pho-
totour (Brown et al., 2011) and HPatches (Balntas et al., 2017). Then, we make comprehensive
ablation studies. See implementation details in Appendix C and more results on the ETH SfM
(Schonberger et al., 2017) dataset in Appendix D.
5.1	UBC Phototour
UBC Phototour (Brown et al., 2011), also known as Brown dataset, consists of three subsets: Liberty,
Notre Dame and Yosemite, with about 400K normalized 64 × 64 patches in each subset. Keypoints
were detected by DoG detector (Lowe, 2004) and verified by 3D model. Testing set consists of
100K matching and non-matching pairs for each sequence. For evaluation, models are trained on
one subset and tested on the other two. The metric is the false positive rate (FPR) at 95% true
positive recall. The evaluation results are reported in Table 1.
3This relation holds only when the hinge triplet loss is positive. Empirically, due to the relatively large
margin, the hinge loss never becomes zero.
6
Under review as a conference paper at ICLR 2020
Table 1: Patch classfication results on UBC Phototour dataset (Brown et al., 2011). The false positive
rate at 95% recall is reported. + indicates data augmentation and * indicates Positive generation.
Train	Notredame Yosemite Test	Liberty				Liberty Yosemite Notredame		Liberty Notredame Yosemite		Mean
SIFT (Lowe, 2004)		29.84			22.53		27.29	26.55
DeepDesc (Simo-Serra et al., 2015)		10.9			4.40		5.69	6.99
GeoDesc (Luo et al., 2018)		5.47			1.94		4.72	4.05
L2-Net (Tian et al., 2017)	3.64		5.29	1.15	1.62	4.43	3.30	3.24
CS-L2-Net (Tian et al., 2017)	2.55		4.24	0.87	1.39	3.81	2.84	2.61
HardNet (Mishchuk et al., 2017)	1.47		2.67	0.62	0.88	2.14	1.65	1.57
HardNet-GOR (Zhang et al.,_2017) _ _ -HardNet*	_ 1.72 一 1.80	—	2.89 _ 2.89 一	_ _0.63_ 一 ^0.68^	_ _ 0.91 _ — . 0.90 一	_ 2里 一 ^1.93^	_ _ j.59_ _ 一 一 171 一 一	_1.64 ^1765 -
AdaSample* (Ours)	1.64		2.62	0.61	0.88	1.92	1.46	1.52
L2-Net+ (Tian et al., 2017)	2.36		-470^^	0.72	1.29	2.57	1.71	2.23
CS-L2-Net+ (Tian et al., 2017)	1.71		3.87	0.56	1.09	2.07	1.30	1.76
HardNet+ (Mishchuk et al., 2017)	1.49		2.51	0.53	0.78	1.96	1.84	1.51
HardNet-GOR+ (Zhang et al., 2017)	1.48		2.43	0.51	0.78	1.76	1.53	1.41
DOAP+_(He et 生用用)		1.54		2.62 _	_ _0.43_	_ _ 0.87 _	_ _2.00_	_ _ J.21_ _	_1二45
-HardNet+* ----	一匚35		2.28 -	-^0.43^	--0.69 -	-^1.64^	--1.15^ -	"1726 -
AdaSample+* (Ours)	1.23		2.19	0.41	0.62	1.46	1.05	1.16
EIPatches Results
Patch Verification mAP [%]	ImagC Matching mAP [%]	Patch Retrieval mAP [%]
Figure 1: Evaluation results on HPatches dataset (Balntas et al., 2017). “-LIB” indicates descrip-
tors trained on Liberty subset of UBC Phototour dataset (Brown et al., 2011) and “-HP” indicates
descriptors trained on training set of HPatches (split ‘a’). Marker color indicates the level of geo-
metrical noises and marker type indicates the experimental setup. Inter and Intra indicate the
source of negative examples for the verification task. VIEWP and ILLUM indicate the sequence type
for the matching task.
Our method outperforms other approaches by a significant margin. We randomly flip and rotate by
90 degree for data augmentation, noted by +, for all methods. Besides, for our method, we also
generate positive patches by random rotation such that each class has 15 patches, noted by *. This
is because there are too few patches (two or three) corresponding to one class in UBC Phototour
dataset (Brown et al., 2011), which limits the capacity of our method. To analyze its effect, we also
conduct it for HardNet (Mishchuk et al., 2017), and observe inferior performance, indicating that
the performance improvement mainly comes from our adaptive sampling solution.
5.2	HPatches
HPatches (Balntas et al., 2017) consists of 116 sequences of 6 images. The dataset is split into two
parts: viewpoint - 59 sequences with significant viewpoint change and illumination - 57 sequences
with significant illumination change. According to the level of geometric noises, the patches can
be further divided into three groups: easy, hard and tough. There are three evaluation tasks: patch
verification, image matching and patch retrieval. Following standard evaluation protocols of the
dataset, we show results in Figure 1. It shows that our method outperforms other methods on all
three tasks when trained on either Liberty dataset (Brown et al., 2011) or HPatches training split.
7
Under review as a conference paper at ICLR 2020
5.3	Ablation Study
Informativeness Approximation. We empirically verify the conclusion in Section 4.2 that the
probability induced by matching distance is a good approximation to the one induced by informa-
tiveness (Figure 2(a)). Besides, the results show that the Pearson correlation is consistently greater
than 0.8 with training (Figure 2(b)), which indicates these probabilities have strong correlation with
each other statistically. It echoes our theoretical conclusion.
Prob Induced by informativeness
Figure 2: (a) Probabilities induced by informativeness and matching distance. (b) Pearson correla-
tion between probabilities and training epoches.
Impact of λ. We experiment with varying λ in AdaSample, which controls the overall “hardness”
of selected matching pairs. The larger λ is, the more likely hard matching pairs will be selected.
When λ = 0, our method degrades into random sampling and overall framework becomes HardNet
(Mishchuk et al., 2017), and as λ → +∞, the framework becomes Hardpos. Therefore, both
HardNet and Hardpos are special cases of our proposed AdaSample. Table 2 shows the results on
HPatches dataset (Balntas et al., 2017), where λ = 10 leads to the best results in most cases. It
demonstrates the advantages of our balanced sampling strategy against the hardest solution.
Table 2: Abaltion study results on HPatches (Balntas et al., 2017).
Task	Verification		Matching		Retrieval	
Loss	AHT	HT	AHT	HT	AHT	HT
λ=1	93.84	93.17	64.09	62.64	81.26	79.97
λ=2	94.72	94.56	66.04	65.92	83.58	83.34
λ=5	94.78	94.76	65.89	65.68	83.80	83.54
λ=10	94.78	94.60	65.46	65.37	83.98	83.62
λ= 20	94.60	94.69	64.56	64.84	83.56	83.69
λ —→ +^o	94.42	94.51	63.81	64.02	83.41	83.29
Distance metric and training speed. As shown in Table 2, our angular hinge triplet (AHT) loss
outperforms the commonly-used hinge triplet (HT) loss in most cases. Besides, our method can
speedup the network convergence, as non-informative matching pairs are adaptively filtered out.
See visualized learning trends in Appendix E.
6 Conclusion
This paper proposes AdaSample for descriptor learning, which adaptively samples hard positives
to construct informative mini-batches during training. We demonstrate the efficacy of our method
from both theoretical and empirical perspectives. Theoretically, we give a rigorous definition of
informativeness of potential training examples. Then, we reformulate the problem and derive a
tractable sampling probability expression (Equation 13) based on informativeness. Empirically,
we enjoy consistent performance gain on top of HardNet (Mishchuk et al., 2017) baseline when
evaluated on various task, including patch classification, patch verification, image matching, and
patch retrieval.
8
Under review as a conference paper at ICLR 2020
References
Sameer Agarwal, Noah Snavely, Ian Simon, Steven M Seitz, and Richard Szeliski. Building rome
in a day. In IEEE International Conference on Computer Vision (ICCV), pp. 72-79. IEEE, 2009.
Vassileios Balntas, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Learning local feature
descriptors with triplets and shallow convolutional neural networks. In British Machine Vision
Conference (BMVC), 2016.
Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krystian Mikolajczyk. HPatches: A benchmark
and evaluation of handcrafted and learned local descriptors. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR),pp. 5173-5182, 2017.
JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, and Ming-
Ming Cheng. GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Matthew Brown and David G Lowe. Automatic panoramic image stitching using invariant features.
International Journal on Computer Vision (IJCV),74(1):59-73, 2007.
Matthew Brown, Gang Hua, and Simon Winder. Discriminative learning of local image descriptors.
IEEE Transactions on Pattern Recognition and Machine Intelligence (PAMI), 33(1):43-57, 2011.
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive angular margin
loss for deep face recognition. arXiv preprint arXiv:1801.07698, 2018.
Jingming Dong and Stefano Soatto. Domain-size pooling in local descriptors: DSP-SIFT. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5097-5106, 2015.
Jack Edmonds. Matroids and the greedy algorithm. Mathematical programming, 1(1):127-136,
1971.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Sukthankar, and Alexander C Berg. Matchnet:
Unifying feature and metric learning for patch-based matching. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 3279-3286, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpass-
ing human-level performance on imagenet classification. In IEEE International Conference on
Computer Vision (ICCV), pp. 1026-1034, 2015.
Kun He, Yan Lu, and Stan Sclaroff. Local descriptors optimized for average precision. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 596-605, 2018.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. arXiv preprint arXiv:1803.00942, 2018.
BG Kumar, Gustavo Carneiro, Ian Reid, et al. Learning local image descriptors with deep siamese
and triplet convolutional networks by minimising global loss functions. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 5385-5394, 2016.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
TSUng-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object
detection. In IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988, 2017.
David G Lowe. Distinctive image features from scale-invariant keypoints. International Journal on
Computer Vision (IJCV), 60(2):91-110, 2004.
9
Under review as a conference paper at ICLR 2020
Zixin Luo, Tianwei Shen, Lei Zhou, Siyu Zhu, Runze Zhang, Yao Yao, Tian Fang, and Long Quan.
Geodesc: Learning local descriptors by integrating geometry constraints. In European Conference
on Computer Vision (ECCV),pp.168-183, 2018.
Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working hard to know your
neighbor’s margins: Local descriptor learning loss. In Neural Information Processing Systems
(NIPS),pp. 4826T837, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
James Philbin, Michael Isard, Josef Sivic, and Andrew Zisserman. Descriptor learning for efficient
retrieval. In European Conference on Computer Vision (ECCV), pp. 677-691. Springer, 2010.
Johannes L Schonberger, Hans Hardmeier, Torsten Sattler, and Marc Pollefeys. Comparative eval-
uation of hand-crafted and learned local features. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 1482-1491, 2017.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 815-823, 2015.
Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the maximal loss: How and why? In ICML,
2016.
E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative
learning of deep convolutional feature point descriptors. In IEEE International Conference on
Computer Vision (ICCV), pp. 118-126, Dec 2015.
Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and Francesc Moreno-
Noguer. Discriminative learning of deep convolutional feature point descriptors. In IEEE Inter-
national Conference on Computer Vision (ICCV), pp. 118-126, 2015.
Yurun Tian, Bin Fan, and Fuchao Wu. L2-Net: Deep learning of discriminative patch descriptor in
euclidean space. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
661-669, 2017.
Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second
order similarity regularization for local descriptor learning. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019a.
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object
detection. arXiv preprint arXiv:1904.01355, 2019b.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance Normalization: The missing
ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling matters in
deep embedding learning. In IEEE International Conference on Computer Vision (ICCV), pp.
2840-2848, 2017.
Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned invariant feature
transform. In European Conference on Computer Vision (ECCV), pp. 467-483. Springer, 2016.
Kwang Moo Yi, Eduard Trulls Fortuny, Yuki Ono, Vincent Lepetit, Mathieu Salzmann, and Pascal
Fua. Learning to find good correspondences. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.
Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional
neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
4353-4361, 2015.
Xu Zhang, Felix X Yu, Sanjiv Kumar, and Shih-Fu Chang. Learning spread-out local feature descrip-
tors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4595-4603,
2017.
10
Under review as a conference paper at ICLR 2020
A Proof of Theorems
Proof of Theorem 1.
Proof. We reformulate Rt as follows,
Rt = -EIt〜Pt [(θt+ι - θ*)T(θt+ι - θ*) - (θt - θ*)T(θt - θ*)]
=-EIt〜& [θT+1θt+1 - 2θT+ιθ* - θTθt + 2θTθ*]
=-EIt〜pt [e - ηtGIt)T(θt- ηtGIt) + 2ηtGTt θ* - θτθt]	(EqUatiOn 5)
=EIt〜Pt [2ηt(θt - θ*)TGIt - η2GTtGIt]
=2ηt(θt - θ*)TEIt〜PtGIt] - η2EIt-Pt[GIt]τEItit[GIt]- η2tr(Var[GIt]).	(23)
□
Proof of Theorem 2.
Proof. As GIt is an unbiased estimator of the true gradient (Equation 4), EIt 〜Pt [GIj is fixed in
our case, denoted by μ for short. By the linearity of trace and tr(μμτ) = ∣∣μ∣∣22,
tr(VarGItD =tr(EIt〜Pt [(GIt-μ)(GIt-μ)τ])
=tr(EIt~Pt [GIt GrJTt - μμT])
=EIt〜PtItr(GItGTt)] - tr(μμτ)
=EIt〜PthkGItk2i -∣∣μ∣∣2
K
=XPiw2 kVθtLik2 -I∣μ∣l2
i=1
α2 X L2α-2kV% Lik2
K2 i=1	Pi
-I∣μ∣l2.
(24)
Mathematically, given the constraints PiK=1 pi = 1, the aforementioned harmonic mean of
{p1 , . . . , pK } reaches its minimum when the probabilities satisfy
Pi «L?-1 kVθtLik2.	(25)
Divided by a normalization factor, We get the expression in Equation 17.	□
B S ampling Pipeline
Figure 3 illustrates our sampling pipeline and model architecture. The triplet generation consists
of tWo stages: i) matching pairs are sampled from the dataset to construct mini-batches, and ii)
hard negatives are mined from the mini-batch to form triplets. Our AdaSample focuses on sampling
informative matching pairs for mini-batch construction, and We folloW the hardest-in-batch strategy
in Mishchuk et al. (2017) for the second stage. We adopt the architecture of L2-Net (Tian et al.,
2017) to embed patches into 128-dimensional feature space.
C Implementation Details
We adopt the architecture of L2-Net (Tian et al., 2017) to embed local descriptors into the unit
hypersphere in 128-dimensional space. FolloWing prior Works (Tian et al., 2017; Mishchuk et al.,
2017), all patches are resized to 32 × 32 and normalized to zero per-patch mean and unit per-patch
variance. We train our model from scratch in PyTorch library (Paszke et al., 2017) using SGD
optimizer With initial learning rate η = 10, momentum 0.5 and Weight decay 0.0001. Batch size is
1024, margin t = 1 and λ = 10 unless otherWise specified. We generate 1, 000, 000 matching pairs
11
Under review as a conference paper at ICLR 2020
Figure 3: Training pipeline of AdaSample framework.
for each epoch and the total number of epochs is 90. The learning rate is divided by 10 at the end of
30, 60, 80 epochs.
We compare our method with both handcrafted and deep methods, including SIFT (Lowe, 2004),
DeepDesc (Simo-Serra et al., 2015), L2-Net (Tian et al., 2017), HardNet (Mishchuk et al., 2017),
HardNet with global orthogonal regularization (GOR) (Zhang et al., 2017), DOAP (He et al., 2018),
and GeoDesc (Luo et al., 2018).
Note that the training set of GeoDesc is not released, and the comparisons with it is unfair. Besides,
we argue that even more powerful descriptor learning framework can be established by combining
our method and SOSNet, as our method focuses on data sampling and batch construction, while
SOSNet on regularization. However, as the training code of SOSNet is not publicly available, we
leave the efficacy comparison and system combination in future work.
D	Results on ETH dataset
We evaluate the proposed descriptor on ETH SfM benchmark (Schonberger et al., 2017) to in-
vestigate its ability to reconstruct 3D models. We compare with both handcrafted algorithms
(SIFT (Lowe, 2004) and DSP-SIFT (Dong & Soatto, 2015)) and deep methods (L2-Net (Tian
et al., 2017) and GeoDesc (Luo et al., 2018)) as strong baselines. Following the standard proto-
cols in Schonberger et al. (2017), we show results in Table 3, where Reproj. Error indicates the
reconstruction accuracy, Track Length also indicates the quality of the reconstructed points, and
others stand for reconstruction density.
It is difficult to say which one (density or accuracy) is more important in 3D scene reconstruction.
Here, the results show that our method results in the best accuracy in reconstruction, and the recon-
structed points have better track lengths. This is especially clear in large dataset (the last two rows).
Although the reconstruction density of our approach is lower than other methods, we argue that our
method is as robust as them. For example, compared with SIFT (Lowe, 2004), which is the most
widely used descriptor in SfM problem, the registered of our method is consistently equal or higher.
Note that the density metrics should be considered with the accuracy. This is because there is no
guarantee that the reconstructed points (observations, and registered images) are correct. Here, we
find that other deep methods (L2-Net (Tian et al., 2017) and GeoDesc (Luo et al., 2018)) show higher
12
Under review as a conference paper at ICLR 2020
Table 3: SfM results on ETH dataset (Schonberger et al., 2017). Compared with handcrafted
methods (SIFT (Lowe, 2004) and DSP-SIFT (Dong & Soatto, 2015)), other deep descriptors (L2-
Net (Tian et al., 2017) and GeoDesc (Luo et al., 2018)) suffer in reconstruction accuracy (Reproj.
Error), although they get higher density. Here we show that deep descriptors (our method) can
achieve better accuracy of reconstruction against traditional methods.
	# Image		# Registered	# Sparse Points	# Observations	Track Length	Reproj. Error
Fountain	SIFT	11	11	14K	70K	4.79	0.39px
	DSP-SIFT		11	14K	71K	4.78	0.37px
	L2-Net		11	17K	83K	4.88	0.47px
	GeoDesc		11	16K	83K	5.00	0.47px
	AdaSample		11	14K	68K	4.78	0.38px
Herzjesu	SIFT	8	8	75K	31K	4.22	0.43px
	DSP-SIFT		8	7.7K	32K	4.22	0.45px
	L2-Net		8	9.5K	40K	4.24	0.51px
	GeoDesc		8	9.2K	40K	4.35	0.51px
	AdaSample		8	7.1K	30K	4.20	0.41px
South Building	SIFT	128	128	108K	653K	6.04	0.54px
	DSP-SIFT		128	112K	666K	5.91	0.58px
	L2-Net		128	170K	863K	5.07	0.63px
	GeoDesc		128	170K	887K	5.21	0.64px
	AdaSample		128	95K	603K	6.32	0.52px
Madrid Metropolis	SIFT	1344	500	116K	733K	6.32	0.60px
	DSP-SIFT		467	99K	649K	6.52	0.66px
	L2-Net		692	254K	1067K	4.20	0.69px
	GeoDesc		809	306K	1200K	3.91	0.66px
	AdaSample		500	75K	550K	7.33	0.57px
Gendarmenmarkt	SIFT	1463	1035	338K	1872K	5.95	0.69px
	DSP-SIFT		979	293K	1577K	5.38	0.74px
	L2-Net		1168	667K	2611K	3.91	0.73px
	GeoDesc		1208	779K	2903K	3.72	0.74px
	AdaSample		1051	209K	1430K	6.83	0.67px
density than traditional methods (SIFT (Lowe, 2004) and DSP-SIFT (Dong & Soatto, 2015)), but
suffer in accuracy. It is indeed arguable that which type of methods are better. Here, compared with
traditional methods, our method can achieve higher accuracy. This has important implications to
deep learning based descriptors for high-precision 3D reconstruction.
E	Comparis on of Convergence Speed
Figure 4 shows the training trend of our method and HardNet (Mishchuk et al., 2017). Models are
all trained and tested on HPatches (Balntas et al., 2017) using split ‘a’. By incorporating AdaSample
method, our model enjoys significantly better convergence speed.
Figure 4: Traning trend of AdaSample and HardNet (Mishchuk et al., 2017) on HPatches (Balntas
et al., 2017) dataset.
13