Under review as a conference paper at ICLR 2020
Trajectory representation learning
for Multi-Task NMRDPs planning
Anonymous authors
Paper under double-blind review
Ab stract
Expanding Non Markovian Reward Decision Processes (NMRDP) into Markov
Decision Processes (MDP) enables the use of state of the art Reinforcement Learn-
ing (RL) techniques to identify optimal policies. In this paper an approach to ex-
ploring NMRDPs and expanding them into MDPs, without the prior knowledge
of the reward structure, is proposed. The non Markovianity of the reward function
is disentangled under the assumption that sets of similar and dissimilar trajec-
tory batches can be sampled. More precisely, within the same batch, measuring
the similarity between any couple of trajectories is permitted, although comparing
trajectories from different batches is not possible. A modified version of the triplet
loss is optimised to construct a representation of the trajectories under which re-
wards become Markovian.
1	Introduction
One of the major objectives in reinforcement learning is to identify optimal policies of a given
Markov Decision Process (MDP), whose existence is typically ensured if the environment is Marko-
vian (Sutton et al., 1998). However, this assumption is usually not fulfilled in many real life prob-
lems. A first possible cause is “partial observability”, when the real state space is observable only
through a projection on a sub-space. For instance, consider a (cleaning) robot that uses the distance
from the walls in four directions as a state space. This representation of the environment does not
necessarily correspond to the exact location in the house as different places share the exact same
distances to walls. This setting is usually framed as a Partially Observed Markov Decision Process
(POMDP) (Shani et al., 2013; Cassandra, 1998; Murphy, 2000; Hausknecht & Stone, 2015; Zhu
et al., 2018). A second possible cause is that reward functions might depend on the whole trajectory.
Consider the example of a waiter/server robot whose state space is the current position in the coffee
shop and the current interaction with the clients (taking orders, getting and delivering beverages).
The objective is to train this robot to bring coffee when ordered. A possible reinforcement signal
is to reward the agent when it processes an order that was not satisfied in the past. As the states do
not include past requests, nor their fulfilment, the environment is not Markovian and the proposed
reward is a function of the trajectory up to the present. This setting is a Non-Markovian Reward
Decision Process (NMRDP)(BacchUs et al., 1997; ThiebaUx et al., 2006; Bakker, 2002; Bacchus
et al., 1996). Both POMDP and NMRDP are special cases of Non Markovian Decision Processes.
The second above example (waiter robot) is actUally a mUltiple task problem. Planning in this setting
is a challenging problem becaUse it reqUires more than optimally and independently solving each
task (Toro Icarte et al., 2018). It is possible that a global optimal policy for the whole seqUence of
tasks indUces sUb-optimal policies for each one of the sUb-tasks. Let Us take a naive example of an
agent bUilding a shelter which reqUires gathering wood and stones from nearby resoUrces. To gather
wood/stones optimally, the agent shoUld look for the nearest forest/qUarry. However, if it wants to
optimally gather all the resoUrces, it shoUld not jUst consider how close the forests or the qUarries
are, bUt how close they are to each other as it will have to walk from one to the other. Notice that
this example shares similarities with the k-server problem (Manasse et al., 1990).
In those cases, the natUre of the task is not encoded in the state space, so the Markov assUmption is
not verified. Casting the problems as an NMRDP reqUires Using a (stage by stage) reward fUnction
that encodes the tasks specifications. In the shelter bUilding scenario, the reward as a fUnction of
the states, while none of the tasks is achieved, is different from when stones or wood have been
collected. If it is possible to map the previoUsly observed states (the trajectory Up to the present)
1
Under review as a conference paper at ICLR 2020
into the current task, the problem can be expanded into a standard MDP, where it is known how to
identify optimal policies. Motivated by the shared similarities of the described problems, we shall
focus on learning to identify current tasks from the trajectory of visited states (Wilson et al., 2007).
Ideally, if annotated trajectory data sets are available, learning a trajectory representation function
that expands the NMRDP into an MDP would be a supervised task. However this is a strong as-
sumption. Indeed if a simple way of creating such data sets existed, it would be used to expand the
NMRDP’s state space. Another issue, is that given a set of tasks, there might exist multiple non
Markovian reward functions of interest. For instance, consider an agent whose goal is to maximise
customers satisfaction. In this case the reinforcement signals are the declared satisfaction of said
customers (in the form of a rating for example). However even if different customers expect the
same goals to be accomplished by the robot, their perception of the performance might differ.
This example can be generalised to the case where several experts (or simply users) are training a
robot to perform some tasks, possibly different from one expert to the other. They share a common
NMRDP environment with different reward/reinforcement functions as the set of tasks (or their ap-
preciations/preferences) might be different from one expert to the other. The same augmented MDP
can be used for all of their respective NMRDP as they share the same latent global set of tasks
(more or less whatever the robot can physically do). This training can be made by the expert inde-
pendently but it will certainly be faster and more efficient if made collectively. In these cases, in
order to identify the full set of tasks and annotate the trajectories, a central authority must be aware
of the reward function of each expert. This might be impossible due to tractability reasons, or pri-
vacy constraints. Fortunately, the experts can easily provide “similarity” between trajectories with
respect to the latent task, i.e., whether the robot was performing the same task or not at that time,
without specifying precisely the task. In the customer example, the evaluations given by the same
client can be used to compare if two trajectories are associated with the same task or not, but not
if those trajectories come from batches associated to two different customers. Indeed, establishing
comparisons between the tasks associated to subsets of the observed trajectories without actually
knowing the actual task is possible. As a consequence, we are going to assume that learning is done
using batches of trajectories, where we will have a task similarity measure within each batch.
In order to identify the tasks, standard techniques identify a latent state space in which policies
become Markovian, thus optimal ones exist. Unfortunately these approaches require domain spe-
Cific knowledge in the form of “relevant propositions” (Bacchus et al., 1997; ThiebaUx et al., 2006;
Bakker, 2002; Toro Icarte et al., 2018), either by estimating the hidden state or by constructing ad-
ditional features. For example, in the waiter robot case, received orders are the relevant propositions
that the agent needs to save in order to identify current tasks. Our major contribution in this paper
is a generic approach to discover a latent state space given a set of similar and different trajectories.
This assumption requires weaker domain specific knowledge.
Once the latent state space is available, multi-task and meta-learning related approaches provide ef-
ficient tools to identify optimal policies (Andrychowicz et al., 2017; Colas et al., 2019) or to learn a
general purpose policy from which learning a specific task’s optimal policy can be done in few steps
(Duan et al., 2017; Rakelly et al., 2019).
The remaining of the paper is organised as follows: we introduce the mathematical tools used in
this paper and we explain how they relate to our work in Section 2. We specify in Section 3 a
subset of NMRDPs that can be expanded with a Markovian latent space into MDPs and we provide
a possible structure of such expansion. In Section 4, we propose an algorithm to learn trajectory
representation functions that approximate the tasks’ latent state space and thus approximates the
proposed equivalent MDPs. Section 5 is dedicated to empirical results. We consider toy NMRDPs
grid-world environment (where we solve a multi-task problem using learned representations) and
a real-life data set of tourists visiting Salzburg (where we learn a trajectory representations that
clusters their paths according to their destinations).
2	Background
2.1	Non Markovian Reward Decision Process (NMRDP)
Both finite NMRDPs and MDPs are defined by a finite state space S, a finite action space A, a
transition probability P, a discount factor γ, and a reward function R. In NMRDPs, the reward
function maps trajectories into real values while in MDPs the reward function maps states into real
2
Under review as a conference paper at ICLR 2020
values. To the extent of our knowledge, existing methods for planning in NMRDP (Toro Icarte
et al., 2018; Bacchus et al., 1997; ThiebaUx et al., 2006) define an augmented state space to build an
equivalent MDP to the NMRDP. The most straightforward idea would be to define the augmented
state space S * as the set of all possible trajectories. However, the number of possible trajectories
grows exponentially with the horizon, which makes the numerical complexity intractable for state
of the art reinforcement learning techniques.
An established approach to expand the state space requires a set of domain-specific propositions.
Using temporal logical operator it combines these propositions to automatically construct a set of
temporal variables (Bacchus et al., 1997; ThiebaUx et al., 2006) that extends the state space to specify
an equivalent MDP. These methods have been successfully used to teach a reinforcement learning
(RL) agent multiple tasks (Toro Icarte et al., 2018). However, the downside is that domain spe-
cific propositions are required to disentangle the non Markovianity of the reward function. Other
approaches exploit the model free RL techniques by adding a Long Short Term Memory (LSTM)
layer to the policy network (Bakker, 2002). The agent policy becomes a function of the full trajec-
tory and the agent has enough information to find the optimal policy. In the tested toy examples of
POMDPs, the agent needs undesirable directed exploration techniques to have a good performance
(Bakker, 2002). Moreover, when dealing with similar NMRDPs with different reward functions, we
need to start the training from scratch when using this approach.
Motivated by the lack of approaches to expand the state space when relevant information about the
reward structure are not provided, we investigate the state expansion problem in NMRDPs.
2.2	Triplet Loss and Contrastive Learning
Learning useful representations of the data has been done in an array of challenging tasks. The
access to data in the form of similar and/or dissimilar observations enabled the development of
multiple efficient contrastive learning techniques (Mikolov et al., 2013; Schroff et al., 2015; Kilian
& Lawrence, 2009). Recent work (Arora et al., 2019) establishes a link between contrastive learning
and efficient linear separability of the data. The core idea is to find a representation in a metric space
so that similar samples have nearby representations and vice versa.
The triplet loss is a clever evaluation of this idea. Multiple formulations have been suggested in the
literature for both the loss definition and the batching procedure (Kilian & Lawrence, 2009; Arora
et al., 2019; Ding et al., 2015; Oh Song et al., 2016; Hermans et al., 2017). A recent success of this
approach lies in a new formulation (Hermans et al., 2017) that outperforms existing techniques in
the person re-identification task (i.e., representing pictures of the same person from different angles
as similar embedding vectors, and representing pictures of different persons as different embedding
vectors). The suggested batching (called P K Batch-hard) selects P classes (person identities) and
for each of these, K examples (images from different angles). Each of the PK images is selected
as an anchor, and each of the anchors is associated with the hardest positive (the same person’s
image from an angle which representation is the furthest to the anchor’s representation) and with
the hardest negative (a different person’s image which representation is the closest to the anchor’s).
This allows to build a batch of anchors, positives and negatives of size P K.
Formally, let X be the set of the PK sampled elements, with Xij being the ith sample of the
jth class, and fθ (Xij ) be a representation parameterised by θ in a metric space Rd where d is
the representation dimension. The metric of the representation space is denoted by D(u, v). The
proposed loss is:
PK
LBH(θ,X)=XX m+m≤aKxD(fθ(Xij),fθ(Xpj))- ≤Kmc≤inP c6=j D(fθ(Xij), fθ(Xpc))	. (1)
j=1 i=1	p	p ,	,
The margin m is a constraint on how far the negative samples should be from each other, and the
relu function [x]+ avoids correcting already well defined representations. The relu function can be
substituted by any smooth approximation such as log1p(.) defined as log1p(x) = log(1 + ex).
In multi-task problems, a representation that identifies the current task is sufficient to expand the
state space and build the equivalent MDP. Given trajectory batches with similar and dissimilar tasks,
the contrastive learning approaches, and particularly the triplet loss, lend themselves to construct
3
Under review as a conference paper at ICLR 2020
trajectory representations that translate the task similarity. In these approaches, no information
about the reward structure is required.
3	Latent space for Multi-Task NMRDPs
Given an NMRDP denoted by N = {O, A, P, R, γ} where O (for “observable”) is the state space,
and the reward is a function of the trajectory (ot)tt==0T, there always exists an equivalent MDP that
extends N (Bacchus et al., 1996).
Consider multi-task problems that do not suffer from partial observability. In other words, if the
NMRDP N is a formulation of the multi-task problem, then the reward function is Markovian in
the observables and some hidden goals (it depends on what task is being achieved and what current
state ot is being visited). This restriction implies that given the nature of the current task, the agent
is able to make optimal decisions based on the current observation. Many real life problems (like
the examples described in the introduction) satisfy this constraint naturally.
Recall that in an NMRDP, the reward is a function of the whole trajectory R((ot)tt==0T). However we
assume that given the mapping T from trajectories into the set of tasks H,
T : {((ot)t=T) ∀ T ∈ N} =Γ(O) →H,
the reward function R can be rewritten as:
R((ot)tt==0T)=R(oT,T((ot)tt==0T))=R(oT,hT) ∀T∈N.	(2)
In multi-task problems, the mapping T can be defined recursively using the recent observations and
tasks instead of using the whole trajectory. Consider the shelter building scenario, where the tasks
can be defined as “collecting the wood”, “collecting the stones” and “building the shelter”. If the
agents is currently “collecting the stones”, and given that it already “collected the wood”, it only
need to check the current state (to verify if it is currently in the quarry or not) to decide if we keep
“collecting the stones” or go on to “building the shelter”.
In these cases and depending on the nature of the problem, T is a bounded memory process in O
and H. Thus there exists a task horizon τT ≥ 1 and an observation horizon τO ≥ 0 such that:
T((ot)tt==0T)=T((oT-τ)ττ==τ0O,(hT-τ)ττ==τ1T) ∀T∈N.
To avoid cumbersome notations, we will restrict ourselves to the case where τO = 0 and τT = 1,
however, our results can be generalised to any values of τO and τT . In this case, T can be further
simplified in the following form:
T((ot)tt==0T) = T(oT, hT-1) = hT ∀ T ∈ N.	(3)
In other terms, the considered NMRDPs admit an equivalent MDP formulation
M* = {S *, A*, P *, R*,γ} whose state space is S * = H × O. By construction of the
equivalent MDP (Bacchus et al., 1996), and under the consideration of Equation 3, P * verifies the
following:
P*((ot+1, ht+1)|(ot, ht), at) = P (ot+1 |ot, at) × 1ht+1=T (ht,ot+1).
In other terms, if the MDP satisfies the following Equation 4, then M* is equivalent to N (Bacchus
et al., 1996):
f A*(ot,ht)	= A(ot)
R*(ot, ht)	= R(o0:t)	(4)
[P*((Ot+1,ht+I)I(Ot,hjaD = P (ot+ι∖ot, at) × 1ht+ι =T (ht,ot+ι)
As N satisfies Equation 2, the construction ofM* is straightforward once the mapping T is known.
3.1	Feature representation for the Latent space
Typical techniques to learn optimal policies in problems with large state space exploit feature func-
tions (Lillicrap et al., 2015; Mnih et al., 2015; 2013). This motivated learning an embedding
4
Under review as a conference paper at ICLR 2020
φ : Γ(O) → Rd where d is the dimension of the tasks’ feature space. As the goal is learning optimal
policies using classical RL techniques, it is important to evaluate an MDP M = {S, A, P, R, γ},
equivalent to N, such that S = Rd ×O ⊇ φ(Γ(O)) × O. In What follows We construct the MDP M
as a reformulation of M* by substituting ht = T(oo：t) with φ(o0：t) in Equation 3 and Equation 4.
We denote φt = φ(oo∙.t).
Equation 4 implies that the feature-based equivalent MDP must satisfy the following properties:
A(ot, φt)	= A(ot)
ʌ , _, ，_、
R(ot,φt)	= R(oo：t)	⑸
P ((Ot+1,φt+I)I(Ot,φja∕ = P (ot+1|ot, at) × 1φt+ι=φ(θ0M+ι)
As a consequence, the construction of A is straightforward from A. The reward (R) and the transi-
tions (P) are constructed based on
tasks, i.e. there exist a classifier C
Equation 4 and Equation 5 given any function φ that separate the
: Rd → H such that C ◦ φ = T:
z∙ ʌ ,	...
R R(ot,φt) = R"ot,C(φt)) = R*(ot,ht)
p^ p^((ot+1,φt+1)|(ot,φt),at) = P*((Ot+1,C(Ot+I))I(Ot,C(Ot)),at)
So from a practical point of view, the construction of an equivalent MDP to N boils down into
approximating any trajectory representation function O : Γ(O) → Rd, as long as O separates the
tasks.
If an agent is learning the optimal policy of N using any RL (Reinforcement Learning) setting,
it will receive both the rewards rt = R((Ot)tt==0T ) and the observations Ot over time. But as the
problem is non Markovian, the optimal policy can not be expressed using solely the observations Ot .
However if the same agent uses a trajectory representation function O, it can construct over time the
latent task feature φt. Using RL techniques, finding an optimal policy ∏* in the equivalent MDP MM
(with the extended state space (Ot, Ot)) is possible, and by extension (Bacchus et al., 1996), π* is an
optimal policy of N.
4	Approximation of a trajectory representation function
Without loss of generality, consider the case where the NMRDP admits n possible tasks:
H = {1, 2, .., n}. Let O : Γ(O) → Rd be a trajectory representation function that separates tasks
such that:
inf	I O(γ1) - O(γ2)I > sup I O(γ1) - O(γ2)I.	(6)
γ1,γ2,T (γ1 )6=T (γ2)	γ1,γ2,T(γ1)=T(γ2)
Let ei = arg min max	IO(γ) - ei I the centroid representations of each task. For a given
e∈Rd γ s.t.T (γ)=i
trajectory representation e ∈ Rd, the classifier C* (e) = arg min I e-eiI satisfies the following:
i∈H
C * ◦ φ(γ ) = T(Y) ∀γ ∈ Γ(O).	⑺
Such classifier can be approximated in an unsupervised manner using K-means algorithm (Kanungo
et al., 2002). Let C be such approximation. As a consequence, Equation 7 simplifies the problem
into finding any trajectory representation function f that satisfies Equation 6 for any two trajectories
γ1 , γ2 ∈ Γ(O). We propose in the remaining to approximate such function using deep neural
networks as a parametric family fθ and optimising the triplet loss introduced in Equation 1 with
respect to θ.
The obtained trajectory representation functions fθ* can either be used to reconstruct an approxima-
tion of M* (by evaluating C ◦ fθ* ) or to build an approximation of M (by using fθ* as a feature
function instead of O).
5
Under review as a conference paper at ICLR 2020
4.1	Architecture of the trajectory representation function
We have tested multiple architectures for the fθ function. The most promising architectures were
mostly inspired by the Long Short Term Memory (LSTM) cell architecture (Hochreiter & Schmid-
huber, 1997) and the Gated Recurrent Unit (GRU) cell architecture (Cho et al., 2014).
The architecture that we use in our implementation builds features from the observed states using
a Multi-Layer Perceptron (MLP) architecture (Rumelhart et al., 1985). The obtained features are
concatenated with the last latent state φpast, and fed to two parallel MLPs. The first one generates a
proposed latent state φnew, and the second generates a forget gate variable Z ∈ [0, 1] (Hochreiter &
Schmidhuber, 1997; Cho et al., 2014). The evaluated new latent state φnew is computed as follows:
φnew = Z × φnew + (1 - Z) × φpast
4.2	Optimisation of the trajectory representation function
Batches of comparable trajectories are used in order to learn a representation function fθ that sepa-
rates tasks. To construct these batches in practice, the only domain-knowledge required is the ability
to compare tasks. As mentioned before, the batches can eventually be collected from different ex-
perts working with different reward functions that share the same latent task space H.
Let (γij)i≤K, j ≤P ∈ Γ(O) be such that T (γij ) = T (γij ). Given a representation function fθ, the
representation φ(θ)ij ∈ Rd of the trajectory γij is constructed recursively with Equation 3:
φt = fθ(ot, φt-1).	(8)
In order to satisfy Equation 6, the batch hard loss function from Equation 1 is optimised with respect
to θ:
PK
L BH GY) = XX log1p	(m	+ max llφ(θ)j	-。⑻p||-	min	llφ(θ)j	-。⑻p||)⑼
p≤K	p≤K,c≤P,c6=j
j=1 i=1
Empirically, given the same computational power, using a special case of truncated back-propagation
through time (Puskorius & Feldkamp, 1994) (where we update the weights one time step at the
time after computing the representation of the whole trajectory) provided better performances. This
approach can be seen as an EM (Expectation-Maximisation) algorithm. In the expectation step (E),
the representation of the available data is evaluated under a fixed θ (i.e. φ(θ)ij is computed). In
the maximisation step (M), the recursive definition of task features in Equation 8 is used to define
Equation 10, a local version of Equation 9:
PK
LlBoHcal(θ, γ, θold) = X X log1p m + Lmax(γij, θ, θold) - Lmin(γij, θ, θold)	(10)
j	jj
Let oij be the last observation in the trajectory γij, and let φ(θ)ij the representation of the trajectory
γij without the last observation. We define Lmax(γij, θ, θold) and Lmin(γij, θ, θold) as follows:
L Lmax(Yi ,θ,θold) = maXp≤K ∣∣fθ (θj ,Φ(θ old )j ) - fθ (θp,Φ(θ old )p)||
.
I Lmin(Yi , θ, θ old) = minp≤K,c≤p,c=j ∣∣fθ Wi , φ(θ old) ) - fθ (耳,φ(θold )p)||
Given a step size α, the maximisation step (M) is a gradient descent with respect to θ:
θ…=θold - aVθLBH(θ,γ,θold).
j
Basically, optimising Equation 10 in the M-step, assumes that the previous representations φ(θ)ij is
an input data and does not propagate the gradient further than the current observation. The E-step,
re-evaluates the representations of the trajectory over time. By alternating these steps, we were able
to obtain the provided experimental results.
5	Experiments
As described before, building an equivalent MDP to an NMRDP N can be reduced to expanding
the state space with a trajectory representation that separates tasks. Experimentally, we establish the
6
Under review as a conference paper at ICLR 2020
ability to approximate such function in a simulated toy example and in a real life problem.
We used an NMRDP Grid-world problem where the agent needs to visit a sequence of particular
cells in order to be rewarded. The reward function is not Markovian because the state space is just
the coordinates of the current cell. Using PK batches from randomly generated trajectories, we
construct a representation function that separate the tasks and thus expands successfully the state
space. This is detailed in Section 5.1
For the real life example, we used the GPS tracks of tourists in Salzburg City, Austria. The tourists
can be seen as agents who are rewarded from certain “attractions” the first time they see them. So
this can be stated as an NMRDP problem. We build a representation function of the GPS tracks
that clusters trajectories going to the same place and separate those heading to different directions
in Section 5.2.
We also provide a github repository with our implementation of the solution (using TensorFlow)
along with the necessary modules to simulate the toy example and the data of the real life example.
In order to recreate the experimental results provided in what remains, you can access the contents on
the following Microsoft Azure blob (This link is anonymous and respects the double-blind reviewing
process, it will be substituted with a Github repository in the camera ready version)
5.1	Grid-World Problem
We run our approach on a Grid-World problems with varying grid dimensions. The state space O is
the grid cells, the action space A is the available directions (North, South, East, West) and staying
still. Transition probabilities P are naturally defined by the action space. We will consider a non
Markovian reward, with latent state structure H = {hi , i ∈ [0, n]}. Each of the latent states hi is
associated with visiting a particular “target cell” T(hi). If the target cell of hi is visited, N(hi)
defines the next task/latent state. Equation 11 formally defines this setting.
T(hi) = oi ∈ O
N(hi) = hj ∈ H
T(( )t=T) T( h ) N(hT-1)	if	oT	=	T(hT-1)
T ((ot)t=0)	= T(oT,hT-1)	= hT-1	if	oT	6=	T(hT-1)
(11)
R((ot)tt==0T) = e-|oT -T(hT)|
To simulate trajectories, we use a random policy to generate the action at each time step as this
ensures generating a wide variety of trajectory patterns. We exploit the hidden states to construct
the P K batches. P hidden states are sampled, and for each of them K trajectories are identified to
build the batch. Training uses the EM algorithm in order to optimise the representation function.
In order to evaluate a given representation function in this toy example, we plot, for a set of trajecto-
ries, the obtained representations φt ∈ Rd and we colour label the points with respect to the hidden
states ht ∈ H.
Figure 1 illustrates the results with 5 latent tasks, in a 5 × 5 grid. The graphs from left to right
represent the following:
•	A projection on the first two dimensions of the trajectories representations
•	An unsupervised dimension reduction using PCA (Principal Component Analysis)
•	A supervised dimension reduction using LDA (Linear Discriminant Analysis)
The obtained representations, when reduced to two dimensions, show clearly that trajectories have
been separated according to the associated task. Even when using unsupervised dimension reduc-
tion, we are able to spot clusters of representations that can be matched with different tasks. The
third plot (with LDA reduction) proves that the data in the original representation space (Rd) can be
separated linearly with respect to the tasks. We conclude that the construction of an n-class classi-
fier (5 classes in the illustrated example) is possible. Using Equation 7, the function fθ that satisfies
Equation 6 and a classifier, we construct an approximation of a task feature function φ ◦ T. There-
fore, an approximation of the equivalent MDP M* that expands the NMRDp
When optimising the trajectory representation function, an annealing procedure increased the con-
vergence speed and the quality of the results. A relaxed version of the batch hard triplet loss defined
in Equation 9 is used. This allows exploring easier batches in the early stages of learning and gener-
ating harder batches as the representation gets better at separating the tasks. This is discussed further
in Appendix A.
7
Under review as a conference paper at ICLR 2020
Figure 1: Evaluating the trajectory representation
Figure 2: DQN performances with and without using the trajectory representation
Deep Q-Networks (Mnih et al., 2013) are one of the widely used off-the-shelf approaches in RL.
The algorithm samples trajectories using an greedy policy with respect to the Q-function estimation
and optimises a loss function based on the Bellman equation. By using a different neural network
in the sampling steps than the one used for computing the parameter gradients, storing the recent
trajectories in a buffer replay memory, and annealing , the neural network converges to an unbiased
estimator of the optimal Q-function and thus identifies an optimal policy. In order to establish that
the obtained representation is valuable to learning the optimal policy, we compare the performances
of DQN-based agents with and without the use of the representation, and with the use a one-hot-
encoding of the actual task. The graph on the right in Figure 2 provides a comparison of the DQN
performances when using only the state features, the state features along with the learned trajectory
representation, and a recurrent version of the DQN (DRQN) (Hausknecht & Stone, 2015) provided
the history of state features and rewards. Both the learned representations and the LSTM within the
DRQN embed the trajectories in a latent space with the same dimension. In all experiments we used
the same horizon, annealing step and the same replay memory size. As expected, using the learned
representations enables the agent to outperform the settings where only the state features/rewards
where provided. This represents quantitative proof that the evaluated trajectory representations are
more useful to an agent learning downstream tasks than exploring previous rewards or previous ob-
servations within the policy network. The graph on the left side of Figure 2, illustrate a comparison
between the performances of a DQN-based agent when using either the learned representations or a
one hot encoding of the actual task. Both performances are comparable, confirming that the obtained
representations expended the NMRDP into an MDP. However, using the one-hot encoding of the ac-
tual task enables the agent to converge faster. This is a consequence of using an approximation of
the actual equivalent MDP. However with this comparison we establish that in such NMRDP scenar-
ios, having access to the task function is not a requirement. The ability to identify similar/different
trajectories with respect to the latent task is sufficient to converge to an optimal policy.
8
Under review as a conference paper at ICLR 2020
5.2	Salzburg City: tourists GPS tracks
The GPS tracking took place in Salzburg City, Austria. The participant of the data collection were
tourists (44) residing in a youth hostel next to the old town (Kellner & Egger, 2016). Given that
the collected trajectories reflect the behaviour of a first time visitor of the city, it is likely that there
exist a common latent task space. However, even though the actual mapping from trajectories to
tasks is unknown, it is possible to identify trajectories with similar latent behaviours. To construct
similarity batches from the data, we considered that each location where a tourist stays for more
than 2 minutes is a potential task (“attraction”). The 2 minutes threshold was chosen to reduce the
number of irrelevant stops. In the PK batches, the classes are P locations (stops), sparsely sampled.
For each location, K trajectories heading to the same vicinity were sampled. On figure 3a, we plot
the dataset of all the collected trajectories. The locations where the tourists stopped for more than 2
minutes are shown with circles. In figure 3b we plot an example of a PK trajectory batch that we
use for the optimisation. Training and implementation details are provided in Appendix B.
In this real life problem, trajectories are not annotated, so evaluating the separability of the tasks is
difficult if not impossible. As an alternative, we propose to qualitatively evaluate the obtained repre-
sentation function. Recall that our PK batch procedure implied that the destination of the tourist is
an indicator of the task it is accomplishing. A possible method to evaluate the performances of the
obtained representation, is to sample trajectories that have similar representation and verify if these
trajectories converge physically to a common area.
In Figure 4, trajectories with similar representations were illustrated in red. As expected, the ob-
tained representation clusters trajectories with similar latent tasks (destinations). This confirms that
the obtained representation succeeded in separating tasks and thus expanding the associated NM-
RDP into an MDP. Moreover, as the transformation clusters trajectories according to their destina-
tion, we can potentially exploit that to analyse the tourists’ patterns of visiting Salzburg.
(a) Measured trajectories
Figure 3: Tourists GPS trajectory data
(b) A sample PK batch
6	Conclusion
Expanding NMRDPs state spaces into minimal MDPs that share the same latent space structure is a
common practice to identify optimal policies in those contexts. In contrastive learning, looking for
separable representations with respect to a particular goal is also a common practice to simplify more
complex tasks. The principal novelty of our work is leveraging representation learning techniques
to bridge planning in non Markovian contexts with state of the art reinforcement learning techniques
without the need for extensive knowledge about the NMRDP reward. Gathering a set of comparable
9
Under review as a conference paper at ICLR 2020
Figure 4: Sampled trajectories with similar representations
trajectories with respect to the associated latent task requires a weaker domain specific knowledge
of the NMRDP problem than providing “relevant propositions” about the reward structure.
The fact that the obtained representation could be used to identify behavioural patterns of the tourists
(agents) motivates our future work to consider inverse reinforcement learning in Non Markovian
Decision Process.
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-
play. In Advances in Neural Information Processing Systems, pp. 5048-5058, 2017.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-
shi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint
arXiv:1902.09229, 2019.
Fahiem Bacchus, Craig Boutilier, and Adam Grove. Rewarding behaviors. In Proceedings of the
National Conference on Artificial Intelligence, pp. 1160-1167, 1996.
Fahiem Bacchus, Craig Boutilier, and Adam Grove. Structured solution methods for non-markovian
decision processes. In AAAI/IAAI, pp. 112-117. Citeseer, 1997.
Bram Bakker. Reinforcement learning with long short-term memory. In Advances in neural infor-
mation processing systems, pp. 1475-1482, 2002.
Anthony R Cassandra. A survey of pomdp applications. In Working notes of AAAI 1998 fall sympo-
sium on planning with partially observable Markov decision processes, volume 1724, 1998.
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Cedric Colas, Pierre-Yves Oudeyer, Olivier Sigaud, Pierre Fournier, and Mohamed Chetouani. Cu-
rious: Intrinsically motivated modular multi-goal reinforcement learning. In International Con-
ference on Machine Learning, pp. 1331-1340, 2019.
10
Under review as a conference paper at ICLR 2020
Shengyong Ding, Liang Lin, Guangrun Wang, and Hongyang Chao. Deep feature learning with
relative distance comparison for person re-identification. Pattern Recognition, 48(10):2993-3003,
2015.
Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya
Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances
in neural information processing systems, pp. 1087-1098, 2017.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In
2015 AAAI Fall Symposium Series, 2015.
Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-
identification. arXiv preprint arXiv:1703.07737, 2017.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and
Angela Y Wu. An efficient k-means clustering algorithm: Analysis and implementation. IEEE
Transactions on Pattern Analysis & Machine Intelligence, (7):881-892, 2002.
Lenka Kellner and Roman Egger. Tracking tourist spatial-temporal behavior in urban places, a
methodological overview and gps case study. In Information and Communication Technologies
in Tourism 2016, pp. 481-494. Springer, 2016.
Weinberger Kilian and Saul Lawrence. Distance metric learning for large margin nearest neighbor
classification. In JMLR,, pp. 10:207244, 2009.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Mark S Manasse, Lyle A McGeoch, and Daniel D Sleator. Competitive algorithms for server prob-
lems. Journal of Algorithms, 11(2):208-230, 1990.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Kevin P Murphy. A survey of pomdp solution techniques. environment, 2:X3, 2000.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted
structured feature embedding. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4004-4012, 2016.
GV Puskorius and LA Feldkamp. Truncated backpropagation through time and kalman filter training
for neurocontrol. In Proceedings of 1994 IEEE International Conference on Neural Networks
(ICNN’94), volume 4, pp. 2488-2493. IEEE, 1994.
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254,
2019.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations
by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive
Science, 1985.
11
Under review as a conference paper at ICLR 2020
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 815-823, 2015.
Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based pomdp solvers. Autonomous
Agents and Multi-Agent Systems, 27(1):1-51, 2013.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
Sylvie Thiebaux, Charles Gretton, John Slaney, David Price, and Froduald Kabanza. Decision-
theoretic planning with non-markovian rewards. Journal of Artificial Intelligence Research, 25:
17-74, 2006.
Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A McIlraith. Teaching
multiple tasks to an rl agent using ltl. In Proceedings of the 17th International Conference
on Autonomous Agents and MultiAgent Systems, pp. 452-461. International Foundation for Au-
tonomous Agents and Multiagent Systems, 2018.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine
learning, pp. 1015-1022. ACM, 2007.
Pengfei Zhu, Xin Li, Pascal Poupart, and Guanghui Miao. On improving deep reinforcement learn-
ing for pomdps. arXiv preprint arXiv:1804.06309, 2018.
A Annealing the batch difficulty over training
Using hard samples provides a lot of nuanced information to the gradient computations which helps
the generalisation property of the representation as it becomes harder to over fit. However, it is a re-
current issue (Hermans et al., 2017) that such sampling makes learning a lot harder in the beginning
of the training. In fact the representation is still erratic and does not benefit much from difficult ex-
amples. We anneal the complexity of the batches as the representation is getting better at separating
the tasks.
To circumvent this isue, we define a relaxed version of the batch hard loss defined in Equation 9
where we use the worst positive and negative sample per anchor with probability phard ∈ [0, 1] and
use a random positive and a random negative sample with probability 1 - phard .
Formally, Let P 〜 B(PK,phard) a random variable of PK binomial trials. We associate an anchor
γij to the trial Pij. A hard sample is used when Pij = 1 and a random sample is used when Pij 6= 1.
The relaxed loss is defined bellow in Equations 12
PK
LBHHr(θ,γ) = XXIogip (m + l∣φ(θ)j -φ(θ)jp(i,j)∖∖-∖∖φ(θ)j -φ(θ)n((j)ll),
(12)
where
p(i, j) :
arg maxp≤K ∖∖φ(θ)ij - φ(θ)jp∖∖ if Pij= 1
n(i, j), c(i, j) :
1,..,K}\{i}	if Pij= 0
arg minn≤K,c≤P,c6=j∖∖φ(θ)ij- φ(θ)cn∖∖
I ~U{1,..,K}, U{1,..,P}∖{j}
if	Pij= 1
if	Pij = 0
(13)
It is clear that LpBhHard=1 = LBH . However, as explained, optimising LBH is difficult from a random
initialisation. In what follows we develop formally an annealing procedure that increases phard as
the quality of the representation improves.
Let us denote by Pθ- and Pθ+ the probability distributions of distances between the representations
of negative and positive samples. Let (Lθ+, (σθ+)2) and (Lθ-, (σθ-)2) be the associated means and
12
Under review as a conference paper at ICLR 2020
variances. The average positive distance over a random batch of size N = PK, denoted by ¾^ (N),
is an unbiased estimator of the mean as
ι+(N) 〜N(L+J N)), ι-(N) 〜N(L-,( N)).
As a consequence, the probability of sampling an average value bigger than L+ - 3 √√N is roughly
0.9987 = 1 - 0.0013 ` 1 P(f+(N) > L+ - 3-√θ^). Let US denote by t+ the average positive
distance over the batch b. If batches were i.i.d. (which is not the case as θ evolves over time),
and if We denote by minb∈[i,B] l+ the minimum average distance between negative samples over B
batches, the following would hold:
P(°minJ+ > L+ - 3√√+) ` 0.9987B.
The same reasoning holds for maXb∈[i,B] l-:
P(^maX] I- <L- + 3√√θ=) ` 0.9987b .
Consider the events A+ = {minb∈[i,B] l+ < L+ - 3√√N} and A- = {maXb∈[i,B] I- > L- +
3√N}, the following property holds:
P(A+ ∪A-) ' (1 -0.9987B)2.
This leads to the following conclusion:
P Lθ+
> Lθ- +
min lI+ >
b∈[1,B] b
max Il-
b∈[1,B] b
> (1 - 0.9987B)2.
(14)
For B = 3000, we have that (1 - 0.9987B)2 ' 0.96. We start with a low hard sampling probability
and each time minb∈[1,B] Ilb+ > maxb∈[1,B] Ilb- we increase phard.
Figure 5 provides experimental analysis of the proposed procedure. In Figure 5a, we present the
representation evaluation of the same problem addressed in Section 5.1. The obtained representation
can be separated more easily. This allows us to conclude that obtained representation using the
annealing procedure is a better approximation. The training details are provided in Figure 5b. From
left to right we have illustrated the average loss value over the last used batches, the average distance
between positive and negative samples as well as minb∈[1,B] lIb+ and maxb∈[1,B] lI- and the value of
phard along the training. As the hard sampling probability increases, the loss value increases. This
is an expected behaviours as we are measuring a more difficult version of the loss. Starting off with
phard = 1 from a randomly initialised representation function did not converge in our toy example.
In Section 5, we optimised LpBhHard=.1 to provide trajectory representation results.
B GPS tracks training details
The collected GPS tracks required prepossessing in order to be used for the trajectory representation
learning. We constrained the observable space to a window around the city of Salzburg to avoid
outliers in the data (such as tourists wondering off to surrounding cities, in Figure 6a, we observed
that these trips render the data useless). The data kept for training are the GPS tracks with longitude
within [13.03, 13.06] and latitude within [47.77, 47.82].
The obtained measurement are noisy. To smooth trajectories, we apply a Kalman filter. We model
humans as a moving particle, characterised with X = [x,X,y,y], where X and y are the respective
longitude and latitude. As we want to reduce the noise of the measured positions, we can define the
observable as z = [x, y] and the observation matrix H as:
1000
0010	.
(15)
13
Under review as a conference paper at ICLR 2020
(a) Learned representation using annealed hard sampling probability
(b) Training evaluation of annealed hard sampling
Figure 5: Evaluating annealing
A linear approximation of the system dynamics is used to establish, for the kth observation, the
following:
Xk = Xk-1 + X k-1
X k = X k-1
yk = yk-ι + yk-ι
y = Uk-I
According to Equation 16, the transition matrix T is defined as follows:
1100
0100
0011	.
0001
(16)
(17)
Using the observation matrix from Equation 15 and the transition matrix from Equation 17, we apply
a Kalman filter to denoise the GPS tracks and smooth the trajectories. We used the implementation
from the python library pykalman. In Figure 6, we illustrate the GPS tracks with and without the
pre-processing operations. In Section 5.2, we use the data illustrated in Figure 6c.
B.1 Training hyper-parameters
When constructing the trajectory batches, we sampled P = 10 stops that are at least 5 × 10-3
far from each other1. For each stop, we sampled K = 64 trajectories heading, in the next 40
observations, to a point at worst 10-3 apart from the sampled stop. Figure ?? is a sample batch.
We run 5000 iteration to optimise the relaxed batch hard loss LpBhHard=.1, using the Adam routine from
TensorFlow with a learning rate of 10-3, a momentum of .95 and a minimum gradient of 0.01.
In Figure 7, we illustrated the training results. The representation is indeed learning to separate
trajectories that do not share the same destination. Therefore, we can conclude with more confidence
that our representation separates the latent tasks of the agents (tourists).
1For the sake of simplicity, the distance is evaluated as if the GPS coordinates were in an Euclidean plan.
14
Under review as a conference paper at ICLR 2020
Noisy trajectories
(a) Raw data
13.0
Longitude
(b) Constrained data
(c) Clean data
Figure 6: Pre-processing the data to smooth trajectories
LOSS value average	REPRESENTATION positive and negative distance average
Figure 7: Optimising the trajectory representation
15