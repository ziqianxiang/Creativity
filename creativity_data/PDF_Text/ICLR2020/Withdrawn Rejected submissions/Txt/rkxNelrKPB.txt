Under review as a conference paper at ICLR 2020
On Stochastic Sign Descent Methods
Anonymous authors
Paper under double-blind review
Ab stract
Various gradient compression schemes have been proposed to mitigate the com-
munication cost in distributed training of large scale machine learning models.
Sign-based methods, such as signSGD (Bernstein et al., 2018), have recently been
gaining popularity because of their simple compression rule and connection to
adaptive gradient methods, like ADAM. In this paper, we perform a general analy-
sis of sign-based methods for non-convex optimization. Our analysis is built on
intuitive bounds on success probabilities and does not rely on special noise distri-
butions nor on the boundedness of the variance of stochastic gradients. Extending
the theory to distributed setting within a parameter server framework, we assure
exponentially fast variance reduction with respect to number of nodes, maintaining
1-bit compression in both directions and using small mini-batch sizes. We validate
our theoretical findings experimentally.
1	Introduction
One of the key factors behind the success of modern machine learning models is the availability of
large amounts of training data (Bottou & Le Cun, 2003; Krizhevsky et al., 2012; Schmidhuber, 2015).
However, the state-of-the-art deep learning models deployed in industry typically rely on datasets
too large to fit the memory of a single computer, and hence the training data is typically split and
stored across a number of compute nodes capable of working in parallel. Training such models then
amounts to solving optimization problems of the form
M
minχ∈Rd f (x) :=焉 Σ fm(x),	(1)
m=1
where fm : Rd → R represents the non-convex loss of a deep learning model parameterized by
x ∈ Rd associated with data stored on node m.
Arguably, stochastic gradient descent (SGD) (Robbins & Monro, 1951; Vaswani et al., 2019; Qian
et al., 2019) in of its many variants (Kingma & Ba, 2015; Duchi et al., 2011; Schmidt et al., 2017;
Zeiler, 2012; Ghadimi & Lan, 2013) is the most popular algorithm for solving (1). In its basic
implementation, all workers m ∈ {1, 2, . . . , M} in parallel compute a random approximation
gm(χk) of Nfm(Xk), known as the stochastic gradient. These approximations are then sent to a
master node which performs the aggregation
M
g(xk) := MM P gm(χk).
m=1
The aggregated vector is subsequently broadcast back to the nodes, each of which performs an update
of the form
Xk+1 = Xk - Ykg(xk),
thus updating their local copies of the parameters of the model.
1.1	Gradient compression
Typically, communication of the local gradient estimators gm (Xk) to the master forms the bottleneck
of such a system (Seide et al., 2014; Zhang et al., 2017; Lin et al., 2018). In an attempt to alleviate
this communication bottleneck, a number of compression schemes for gradient updates have been
proposed and analyzed (Alistarh et al., 2017; Wang et al., 2018; Wen et al., 2017; Khirirat et al., 2018;
1
Under review as a conference paper at ICLR 2020
〜
Table 1: Summary of the theoretical results obtained in this work. O notation ignores logarithmic
factors and O* notation shows the rate to a neighbourhood of the solution.
	This work Theorem 1	This work Theorem 2	SGD (Sec. C) Theorem 6	(Bernstein et al., 2019) signSGD, Theorem 1
Step size γk ≡ γ	O* (K)	O* (K)	O* (K)	X
Step size ==γ0 	Y = √k+	O( √K)	O( √K)	O( √K)	X
Step size Y = O (√K)	O (√K)	O(√K)	O(√K)	O ( √K )
Can handle biased estimators?	✓	✓	X	X
Weak dependence on smoothness parameters?	d ✓ 1P Li i=1	d ✓ d P Li i=1	X max Li i=1	d ✓ d P Li i=1
				X
Weak noise	✓	✓	X	unimodal,
assumptions?	Pi > 2	Pi > 2	Ekgk2 ≤ C1	symmetric & Var[^i] ≤ σi	
Gradient norm used in theory	ρ-norm	ρ-norm	(squared) l2	A mix of l1 and l2 norms
Mishchenko et al., 2019). A compression scheme is a (possibly randomized) mapping Q : Rd → Rd,
applied by the nodes to gm(χk) (and possibly also by the master to aggregated update in situations
when broadcasting is expensive as well) in order to reduce the number of bits of the communicated
message.
Sign-based compression. Although most of the existing theory is limited to unbiased compression
schemes, i.e., on operators Q satisfying EQ(x) = x, biased schemes such as those based on
communicating signs of the update entries only often perform much better (Seide et al., 2014; Strom,
2015; Wen et al., 2017; Carlson et al., 2015; Balles & Hennig, 2018; Bernstein et al., 2018; 2019;
Zaheer et al., 2018; Liu et al., 2019). The simplest among these sign-based methods is signSGD (see
also Algorithm 1; Option 1), whose update direction is assembled from the component-wise signs of
the stochastic gradient.
Adaptive methods. While ADAM is one of the most popular adaptive optimization methods used
in deep learning (Kingma & Ba, 2015), there are issues with its convergence (Reddi et al., 2019)
and generalization (Wilson et al., 2017) properties. It was noted in Balles & Hennig (2018) that the
behaviour of ADAM is similar to a momentum version of signSGD. Connection between sign-based
and adaptive methods has long history, originating at least in Rprop (Riedmiller & Braun, 1993) and
RMSprop (Tieleman & Hinton, 2012). Therefore, investigating the behavior of signSGD can improve
our understanding on the convergence of adaptive methods such as ADAM.
1.2	Contributions
We now summarize the main contributions of this work. Our key results are summarized in Table 1.
1In fact, bounded variance assumption, being weaker than bounded second moment assumption, is stronger
(or, to be strict, more curtain) than SPB assumption in the sense of differential entropy, but not in the direct
sense. The entropy of probability distribution under the bounded variance assumption is bounded, while under
the SPB assumption it could be arbitrarily large. This observation is followed by the fact that for continuous
random variables, the Gaussian distribution has the maximum differential entropy for a given variance (see
https://en.wikipedia.org/wiki/Differential_entropy).
2
Under review as a conference paper at ICLR 2020
•	2 methods for 1-node setup. In the M = 1 case, we study two general classes of sign based
methods for minimizing a smooth non-convex function f . The first method has the standard form2
Xk+1 J Xk - Yk signg(xk),	(2)
while the second has a new form not considered in the literature before:
Xk+1 J argmin{f(xk),f(Xk — Yk Signg(xk))}.	(3)
•	Key novelty. The key novelty of our methods is in a substantial relaxation of the requirements that
need to be imposed on the gradient estimator g(xk) of the true gradient Vf (Xk). In sharp contrast
with existing approaches, We allow g(xk) to be biased. Remarkably, We only need one additional and
rather weak assumption on g(xk) for the methods to provably converge: we require the signs of the
entries of g(xk) to be equal to the signs of the entries of Vf (χk) with a probability strictly larger
than 1/2 (see Section 2; Assumption 1). We show through a counterexample (see Section 2.2) that
this assumption is necessary.
•	Geometry. As a byproduct of our analysis, we uncover a mixed l1-l2 geometry of sign descent
methods (see Section 3).
•	Convergence theory. We perform a complexity analysis of methods (2) and (3) (see Section 4.1;
Theorem 1). While our complexity bounds have the same O(1∕√K) dependence on the number of
iterations, they have a better dependence on the smoothness parameters associated with f . Theorem 1
is the first result on signSGD for non-convex functions which does not rely on mini-batching, and
which allows for step sizes independent of the total number of iterations K. Finally, Theorem 1 in
Bernstein et al. (2019) can be recovered from our general Theorem 1. Our bounds are cast in terms
of a novel norm-like function, which we call the ρ-norm, which is a weighted l1 norm with positive
variable weights.
•	Distributed setup. We extend our results to the distributed setting with arbitrary M (Section 4.2),
where we also consider sign-based compression of the aggregated gradients.
2	Success Probabilities and Gradient Noise
In this section we describe our key (and weak) assumption on the gradient estimator g(x) of the true
gradient Vf (X), and give an example which shows that without this assumption, method (2) can fail.
2.1	Success Probability B ounds
Assumption 1 (SPB: Success Probability Bounds). For any X ∈ Rd, we have access to an indepen-
dent (and not necessarily unbiased) estimator g(x) ofthe true gradient g(x) := Vf (x) that satisfies
pi(x) := Prob(Signgi(x) = signgi(x)) > 2, if gi(x) =0	(4)
for all x ∈ Rd and all i ∈ {1, 2, . . . , d}.
We will refer to the probabilities ρi as success probabilities. As we will see, they play a central
role in the convergence of sign based methods. We stress that Assumption 1 is the only assumption
on gradient noise in this paper. Moreover, we argue that it is reasonable to require from the sign
of stochastic gradient to show true gradient direction more likely than the opposite one. Extreme
cases of this assumption are the absence of gradient noise, in which case ρi = 1, and an overly noisy
stochastic gradient, in which case Pi ≈ 2.
Remark 1. Assumption 1 can be relaxed by replacing bounds (4) with
E [sign (gi(x) ∙ gi(x))] > 0, if gi(x)=0.
However, if Prob(signgi(x) = 0) = 0 (e.g. in the case of gi(x) has continuous distributions), then
these two bounds are identical.
2sign g is applied element-wise to the entries g1 , g2 , . . . , gd of g ∈ Rd. For t ∈ R we define sign t = 1 if
t > 0, sign t = 0 ift = 0, and sign t = -1 ift < 0.
3
Under review as a conference paper at ICLR 2020
Extension to stochastic sign oracle. Notice that We do not require ^ to be unbiased. Moreover, We
do not assume uniform boundedness of the variance, or of the second moment. This observation
alloWs to extend existing theory to more general sign-based methods With a stochastic sign oracle.
By a stochastic sign oracle We mean an oracle that takes xk ∈ Rd as an input, and outputs a random
vector Sk ∈ Rd with entries in ±1. However, for the sake of simplicity, in the rest of the paper We
will work with the signSGD formulation, i.e., we let Sk = sign g(xk).
2.2	A counterexample to signSGD
Here we analyze a counterexample to signSGD discussed in Karimireddy et al. (2019). Consider the
following least-squares problem with unique minimizer x* = (0,0):
min f(x) = 2 [haι,xi2 +ha2,xɪ2] , aι = [ -11+ε] S= [ 7胃
2
where ε ∈ (0,1) and stochastic gradient g(x) = Yhai,谓 =2(ai, Xiai with probabilities 1/2 for
i = 1, 2. Let us take any point from the line l = {(z1, z2) : z1 + z2 = 2} as initial point x0 for
the algorithm and notice that sign g(x) = ±(1, -1) for any X ∈ l. Therefore, signSGD with any
step-size sequence remains stuck along the line l, whereas the problem has a unique minimizer at the
origin.
We now investigate the cause of the divergence. In this counterexample, Assumption 1 is violated.
Indeed, note that
sign ^(x) = (-1)i Signhai, Xi [^l1]	with probabilities 11 for i = 1, 2.
By S := {x ∈ R2: haι, Xi ∙ ha1, xi > 0} = 0 denote the open cone of points having either an acute
or an obtuse angle with both ai’s. Then for any x ∈ S, the sign of the stochastic gradient is ±(1, -1)
with probabilities 1/2. Hence for any x ∈ S, we have low success probabilities:
pi(x) = Prob(Signgi(x) = signgi(x)) ≤ 1, i = 1, 2.
So, in this case we have an entire conic region with low success probabilities, which clearly violates
(4). Furthermore, if we take a point from the complement open cone Sc, then the sign of stochastic
gradient equals to the sign of gradient, which is perpendicular to the axis of S (thus in the next
step of the iteration we get closer to S). For example, if ha1, xi < 0 and ha2, xi > 0, then
sign ^(x) = (1, -1) with probability 1, in which case X — Y sign g(x) gets closer to low success
probability region S .
In summary, in this counterexample there is a conic region where the sign of the stochastic gradient
is useless (or behaves adversarially), and for any point outside that region, moving direction (which
is the opposite of the sign of gradient) leads toward that conic region.
2.3	Sufficient conditions for SPB
To justify our SPB assumption, we show that it holds under general assumptions on gradient noise.
Lemma 1 (see B.1). Assume that for any point x ∈ Rd, we have access to an independent and
unbiased estimator g(x) of the true gradient g(x). Assume further that each coordinate gi has
a unimodal and symmetric distribution with variance σi2 = σi2(x), 1 ≤ i ≤ d. Then ρi ≥
1 + 1 __________∣gi∣_____ > 1 if q∙ = 0
2 + 2 ∣gi∣ + √3σi > 2 if gi = 0.
Next, we remove the distribution condition and add a strong growth condition (Schmidt & Le Roux,
2013; Vaswani et al., 2019) together with fixed mini-batch size.
Lemma 2 (see B.2). Assume that for any point x ∈ Rd, we have access to an independent, unbiased
estimator ^(x) ofthe true gradient g(x), with coordinate-wise bounded variances σ2 (x) ≤ cg2 (x)
for some constant c. Then, choosing a mini-batch size T > 2c, we get Pi ≥ 1 一 c∕τ > 11, if gi = 0.
Finally, we give an adaptive condition on mini-batch size for the SPB assumption to hold.
Lemma 3 (see B.3). Assume that for any point x ∈ Rd we have access to an independent and
unbiased estimator ^(x) ofthe true gradient g(x). Let σ2 = σ2(x) be the variance and Vi = ν3(x)
be the 3th central moment of ^i(x), 1 ≤ i ≤ d. Then SPB assumption holds if mini-batch size
τ > 2min (σ2∕g2, ν3∕∣gi∣σ2}.
4
Under review as a conference paper at ICLR 2020
- 40	- 20	0	20	40
10
5
0
-5
-10
-10	-5	0	5	10
-4	-2	0	2	4
- 1.0	- 0.5	0.0	0.5	1.0
Figure 1: Contour plots of the l1,2 norm (5) at 4 different scales with fixed noise σ = 1.
3	A New “Norm” for Measuring the Size of the Gradients
In this section we introduce the concept of a norm-like function, which call ρ-norm, induced from
success probabilities. Used to measure gradients in our convergence rates, ρ-norm is a technical tool
enabling the analysis.
Definition 1 (ρ-norm). Let ρ := {ρi(x)}id=1 be the collection of probability functions from the SPB
assumption. We define the ρ-norm of gradient g(x) via kg(x)kρ := Pid=1(2ρi(x) - 1)|gi(x)|.
Note that ρ-norm is not a norm as it may not satisfy the triangle inequality. However, under SPB
assumption, ρ-norm is positive definite as it is a weighted l1 norm with positive (and variable) weights
2ρi(x) - 1 > 0. That is, kgkρ ≥ 0, and kgkρ = 0 if and only if g = 0. Under the assumptions
of Lemma 2, ρ-norm can be lower bounded by a weighted l1 norm with positive constant weights
1 - 2ci2 > 0: kgkρ = Pid=1(2ρi - 1)|gi| ≥ Pid=1(1 - 2ci2)|gi|. Under the assumptions of Lemma 1,
ρ-norm can be lower bounded by a mixture of the l1 and squared l2 norms:
d	d2
kgkρ = P(2Pi - 1)∣gi∣ ≥ P lgil+√σi := kgk11,2.	(5)
Note that l1,2-norm is again not a norm. However, it is positive definite, continuous and order
preserving, i.e., for any gk, g, g ∈ Rd We have: i) Ilgkl1,2 ≥ 0 and Ilgkl1,2 = 0 if and only if g = 0;
ii) gk → g (in l2 sense) implies ∣∣gkkl1,2 → ∣∣gkl1,2, and iii) 0 ≤ gi ≤ gi for any 1 ≤ i ≤ d implies
kgkl1,2 ≤ kgkl1,2. From these three properties it folloWs that kgk kl1,2 → 0 implies gk → 0. These
properties are important as We Will measure convergence rate in terms of the l1,2 norm in the case of
unimodal and symmetric noise assumption. To understand the nature of the l1,2 norm, consider the
following two cases when σi(x) ≤ c∣gi(x)∣ + c for some constants c, c ≥ 0. If the iterations are in
ε-neighbourhood of a minimizer x* with respect to the l∞ norm (i.e., maxι≤i≤d |gi | ≤ ε), then the
l1,2 norm is equivalent to scaled l2 norm squared: 0十容I)E十容W∣∣gk2 ≤ ∣∣gkl1,2 ≤ √∣kgk2∙On
the other hand, if iterations are away from a minimizer (i.e., min1≤i≤d |gi| ≥ L), then the l1,2-norm
is equivalent to scaled l1 norm: 1+√3(：+1/工)kgkι ≤ kgkl1,2 ≤〔+葭 kgkι∙ These equivalences are
visible in Figure 1, where we plot the level sets ofg 7→ kgkl1,2 at various distances from the origin.
Similar mixed norm observation was also noted in Bernstein et al. (2019).
4	Convergence Theory
Now we turn to our theoretical results of sign based methods. First we give our general convergence
results under the SPB assumption. Afterwards, we present convergence result in the distributed
setting under the unimodal and symmetric noise assumptions.
Throughout the paper we assume that f : Rd → R is lower bounded, i.e., f(x) ≥ f*, x ∈ Rd
and is L-smooth with some non-negative constants L = [L1, . . . , Ld]. That is, we assume that
f (y) ≤ f (x) + Nf(X), y - Xi + Pd=I L2i (yi - Xi)2 for all XJy ∈ Rd. We allow f to be nonconvex.
1
Let L := d ɪ^i Li and LmaX = maxi Li.
4.1	CONVERGENCE ANALYSIS FOR M = 1
We now state our convergence result for Algorithm 1 under the general SPB assumption.
5
Under review as a conference paper at ICLR 2020
Algorithm 1 SIGNSGD
1:	Input: step size Yk, current point Xk
2： gk J StochasticGradient(Xk)
3： Option 1： Xk+ι J Xk — Yk Signgk
4: Option 2： Xk+ι J argmin{f(xk),f (Xk — Yk signgk)}
Theorem 1 (Non-convex convergence of signSGD, see B.4). Under the SPB assumption, SignSGD
(Algorithm 1 with Option 1) with step sizes Yk = γo∕√k + 1 converges asfollows
mi<κEkVf(Xk)kρ ≤ √K [fxγ土 + Y0dL] + 吟*.	(6)
If Yk ≡ Y > 0, we get 1/K convergence to a neighbourhood of the solution:
K-1
K P EkVf(Xk)kρ ≤
k=0
f(x0)-f* + &
YK + 2 γ .
(7)
We now comment on the above result:
•	Generalization. Theorem 1 is the first general result on signSGD for non-convex functions
without mini-batching, and with step sizes independent of the total number of iterations K. Known
convergence results (Bernstein et al., 2018; 2019) on signSGD use mini-batches and/or step sizes
dependent on K. Moreover, they also use unbiasedness and unimodal symmetric noise assumptions,
which are stronger assumptions than our SPB assumption (see Lemma 1). Finally, Theorem 1 in
Bernstein et al. (2019) can be recovered from Theorem 1 (see Section D for the details).
•	Convergence rate. Rates (6) and (7) can be arbitrarily slow, depending on the probabilities ρi . This
is to be expected. At one extreme, if the gradient noise was completely random, i.e., if ρi ≡ 1/2, then
the ρ-norm would become identical zero for any gradient vector and rates would be trivial inequalities,
leading to divergence as in the counterexample. At other extreme, if there was no gradient noise, i.e.,
if Pi ≡ 1, then the ρ-norm would be just the l1 norm and from (6) We get the rate O(1/√K) with
respect to the l1 norm. However, if we know that ρi > 1/2, then we can ensure that the method will
eventually converge.
•	Geometry. The presence of the ρ-norm in these rates suggests that there is no particular geometry
(e.g., l1 or l2) associated with signSGD. Instead, the geometry is induced from the success probabili-
ties. For example, in the case of unbiased and unimodal symmetric noise, the geometry is described
by the mixture norm l1,2 .
•	Practicality. The rate (7) (as well as (30)) supports the common learning schedule practice of using
a constant step size for a period of time, and then halving the step-size and continuing this process.
For a reader interested in comparing Theorem 1 with a standard result for SGD, we state the standard
result in the Section C. We now state a general convergence rate for Algorithm 1 with Option 2.
Theorem 2 (see B.5). Under the SPB assumption, Algorithm 1 (Option 2) with step sizes Yk =
Yo∕√k + 1 converges as follows: KK PK-1 EIlVf (Xk )kρ ≤ √= f( (xγ-f + Yo"L] ∙ In the case
of constant step size Yk = Y > 0, the same rate as (7) is achieved.
Comparing Theorem 2 with Theorem 1, notice that a small modification in Algorithm 1 can remove
the log-dependent factor from (6); we then bound the average of past gradient norms instead of the
minimum. On the other hand, in a big data regime, function evaluations in Algorithm 1 (Option 2,
line 4) are infeasible. Clearly, Option 2 is useful only when one can afford function evaluations and
has rough estimates about the gradients (i.e., signs of stochastic gradients). This option should be
considered within the framework of derivative-free optimization.
4.2 Convergence Analys is in Distributed Setting
In this part we present the convergence result of distributed signSGD (Algorithm 2) with majority
vote introduced in Bernstein et al. (2018). Majority vote is considered within a parameter server
framework, where for each coordinate parameter server receives one sign from each node and sends
6
Under review as a conference paper at ICLR 2020
25	50	75	100 125 150 175 200
Iteration
SCO
400
300
20Q
100
25	50	75	1∞ 125 150 175 2∞
iteration
Figure 2: Experiments on distributed signSGD with majority vote using Rosenbrock function. Plots
show function values with respect to iterations averaged over 10 repetitions. Left plot used constant
step size γ = 0.02, right plot used variable step size with γ0 = 0.02. We set mini-batch size 1 and
used the same initial point. Dashed blue lines show the minimum value.
back the sign sent by the majority of nodes. Known convergence results (Bernstein et al., 2018;
2019) use O(K) mini-batch size as well as O(1/K) constant step size. In the sequel we remove this
limitations extending Theorem 1 to distributed training. In distributed setting the number of nodes M
get involved in geometry introducing new ρM -norm, which is defined by the regularized incomplete
beta function I (see B.6).
Algorithm 2 DISTRIBUTED SIGNSGD WITH MAJORITY VOTE
1:
2:
3:
4:
5:
6:
7:
8:
Input: step sizes {γk}, current point xk, # of nodes M
on each node
gm(χk) J StochasticGradient(Xk)
on server
pull sign gm(xk) from each node
push sign [pM=ι Sign gm(xk)] to each node
on each node
Xk+1 J Xk — Yk sign IPM=I signgm(xk)]
Definition 2 (PM-norm). Let M ≥ 1 be the number ofnodes and l = [M+1]. Define PM-norm of
gradient g(X) atX ∈ Rd as kg(X)kρM = Pid=1 (2I(ρi(X); l, l) - 1) |gi(X)|.
Now we can state the convergence rate of distributed signSGD with majority vote.
Theorem 3 (Non-convex convergence of distributed signSGD, see B.6). Under SPB assumption,
distributed signSGD (Algorithm 2) with step sizes Yk = γo/ʌ/k + 1 converges asfollows
mino≤k<κEkVf(XkXI。. ≤ √K [f(xθ)-fɪ + γ°dL] + 吟院.	(8)
For constant step sizes Yk ≡ Y > 0, we have convergence up to a level proportional to step size Y:
K-1
K P EkVf(Xk)kρM ≤
k=0
f(x0)-f * + &
YK + 2 Y
(9)
Variance Reduction. Using Hoeffding’s inequality, we show that kg(X)kρM → kg(X)k1 expo-
nentially fast as M → ∞: (1 — exp (—(2P(X)- 1)2l)) ∣∣g(x)kι ≤ ∣∣g(x)kρ. ≤ ∣∣g(x)kι, where
P(X) = min1≤i≤d Pi (X) > 1/2. Hence, in some sense, we have exponential variance reduction in
terms of number of nodes (see B.7).
Number of nodes. Notice that theoretically there is no difference between 2l-1 and 2l nodes, and this
in not a limitation of the analysis. Indeed, as itis shown in the proof, expected sign vector at the master
with M = 2l 一 1 nodes is the same as with M = 2l nodes: Esign(g(21) ∙ gi) = Esign(g(2lT) ∙ gi),
where g(M)is the sum of stochastic sign vectors aggregated from nodes. The intuition behind this
phenomenon is that majority vote with even number of nodes, e.g. M = 2l, fails to provide any sign
7
Under review as a conference paper at ICLR 2020
with little probability (it is the probability of half nodes voting for +1, and half nodes voting for
-1). However, if we remove one node, e.g. M = 2l - 1, then master receives one sign-vote less but
gets rid of that little probability of failing the vote (sum of odd number of ±1 cannot vanish). So,
somehow this two things cancel each other and we gain no improvement in expectation adding one
more node to parameter server framework with odd number of nodes.
5	Experiments
We verify our theoretical results experimentally using the MNIST dataset with feed-forward neural
network (FNN) and the well known Rosenbrock (non-convex) function with d = 10 variables:
f(x)=Pid=-11fi(x)=Pid=-11100(xi+1-xi2)2+(1-xi)2,	x∈Rd.	(10)
Stochastic formulation of minimization problem for Rosenbrock function is as follows: at any point
X ∈ Rd We have access to biased stochastic gradient g(x) = Nfi(x) + ξ, where index i is chosen
uniformly at random from {1,2,...,d - 1} and ξ 〜 N (0, V 2I) with ν > 0.
Figure 3: Comparison of signSGD and SGD on MNIST dataset with a fixed budget of gradient
communication (MB) using single hidden layer FNN. For each batch size, we first tune the constant
step size over logarithmic scale {10, 1, 0.1, 0.01, 0.001} and then fine tune it. Clearly, signSGD beats
SGD if we compare their accuracies against communication. As suggested by the theory (see Lemma
3) bigger mini-batch size increases the success probabilities ρi and thus improves the convergence.
Figure 2 illustrates the effect of multiple nodes in distributed training with majority vote. As we see
increasing the number of nodes improves the convergence rate. It also supports the claim that in
expectation there is no improvement from 2l - 1 nodes to 2l nodes.
Figure 4 shows the robustness of SPB assumption in the convergence rate (7) with constant step size.
We exploited four levels of noise in each column to demonstrate the correlation between success
probabilities and convergence rate. In the first experiment (first column) SPB assumption is violated
strongly and the corresponding rate shows divergence. In the second column, probabilities still
violating SPB assumption are close to the threshold and the rate shows oscillations. Next columns
show the improvement in rates when success probabilities are pushed to be close to 1.
8
Under review as a conference paper at ICLR 2020
,∣ιil
02	04 M
Qe Oe
Figure 4: Performance of signSGD with constant step size (γ = 0.25) under four different noise
levels (mini-batch size 1, 2, 5, 8) using Rosenbrock function. Each column represent a separate
experiment with function values, evolution of minimum success probabilities and the histogram
of success probabilities throughout the iteration process. Dashed blue line in the first row is the
minimum value. Dashed red lines in second and third rows are thresholds 1/2 of success probabilities.
The shaded area in first and second rows shows standard deviation obtained from ten repetitions.
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-
efficient SGD via gradient quantization and encoding. In Advances in Neural Information Process-
ing Systems 30, pp. 1709-1720, 2017.
Lukas Balles and Philipp Hennig. Dissecting Adam: The sign, magnitude and variance of stochastic
gradients. In Proceedings of the 35th International Conference on Machine Learning, pp. 404-413,
2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
signSGD: Compressed optimisation for non-convex problems. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80, pp. 560-569. PMLR, 2018.
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Animashree Anandkumar. signSGD
with majority vote is communication efficient and fault tolerant. In International Conference on
Learning Representations, 2019.
Leon Bottou and Yann Le Cun. Large scale online learning. In Advances in Neural Information
Processing Systems, 2003.
David Carlson, Volkan Cevher, and Lawrence Carin. Stochastic spectral descent for restricted boltz-
mann machines. In International Conference on Artificial Intelligence and Statistics (AISTATS),
pp. 111-119, 2015.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Journal of Machine Learning Research, pp. 2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. In SIAM Journal on Optimization, volume 23(4), pp. 2341-2368, 2013.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback
fixes SignSGD and other gradient compression schemes. In Proceedings of the 36th International
Conference on Machine Learning, volume 97, pp. 3252-3261, 2019.
9
Under review as a conference paper at ICLR 2020
Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with
compressed gradients. In arXiv preprint arXiv:1806.06573, 2018.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J. Dally. Deep gradient compression:
Reducing the communication bandwidth for distributed training. In International Conference on
Learning Representations, 2018.
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signSGD via zeroth-order oracle. In
International Conference on Learning Representations, 2019.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takdc, and Peter Richtdrik. Distributed learning
with compressed gradient differences. In arXiv preprint arXiv:1901.09269, 2019.
Xun Qian, Peter Richtdrik, Robert Mansel Gower, Alibek Sailanbayev, Nicolas Loizou, and Egor
Shulgin. SGD with arbitrary sampling: General analysis and improved rates. In International
Conference on Machine Learning, 2019.
Sashank Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
International Conference on Learning Representations, 2019.
Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning:
The Rprop algorithm. In IEEE International Conference on Neural Networks, pp. 586-591, 1993.
Herbert Robbins and Sutton Monro. A stochastic approximation method. In The Annals of Mathe-
matical Statistics, volume 22(3), pp. 400-407, 1951.
Jurgen Schmidhuber. Deep learning in neural networks: An overview. In Neural networks, volume 61,
pp. 85-117, 2015.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong
growth condition. In arXiv preprint arXiv:1308.6370, 2013.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. In Mathematical Programming, volume 162(1-2), pp. 83-112, 2017.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and
application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference
of the International Speech Communication Association, 2014.
Irina Shevtsova. On the absolute constants in the berry-esseen type inequalities for identically
distributed summands. In arXiv preprint arXiv:1111.6554, 2011.
Nikko Strom. Scalable distributed DNN training using commodity GPU cloud computing. In
Sixteenth Annual Conference of the International Speech Communication Association, 2015.
Tijmen Tieleman and Geoffrey E. Hinton. RMSprop. In Coursera: Neural Networks for Machine
Learning, Lecture 6.5, 2012.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-
parameterized models (and an accelerated perceptron). In Proceedings of the 22nd International
Conference on Artificial Intelligence and Statistics, PMLR, volume 89, 2019.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
doi: 10.1017/9781108231596.
10
Under review as a conference paper at ICLR 2020
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen
Wright. Atomo: Communication-efficient learning via atomic sparsification. In Advances in
Neural Information Processing Systems, 2018.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, pp.1509-1519, 2017.
Ashia Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 9815-
9825, 2018.
Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. In arXiv e-prints,
arXiv:1212.5701, 2012.
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear
models with end-to-end low precision, and a little bit of deep learning. In Proceedings of the 34th
International Conference on Machine Learning, volume 70, pp. 4035-4043, 2017.
11
Under review as a conference paper at ICLR 2020
Appendix: “On Stochastic Sign Descent
Methods”
A Extra Experiments
In this section we perform several additional experiments for further insights.
Figure 5: Performance of signSGD with variable step size (γ0 = 0.25) under four different noise
levels (mini-batch size 1, 2, 5, 7) using Rosenbrock function. As in the experiments of Figure 4 with
constant step size, these plots show the relationship between success probabilities and the convergence
rate (6). In low success probability regime (first and second columns) we observe oscillations, while
in high success probability regime (third and forth columns) oscillations are mitigated substantially.
12
Under review as a conference paper at ICLR 2020
iteration	Xratian
Figure 6: In this part of experiments we investigated convergence rate (7) to a neighborhood of the
solution. We fixed gradient noise level by setting mini-batch size 2 and altered the constant step size.
For the first column we set bigger step size γ = 0.25 to detect the divergence (as we slightly violated
SPB assumption). Then for the second and third columns we set γ = 0.1 and γ = 0.05 to expose the
convergence to a neighborhood of the minimizer. For the forth column we set even smaller step size
γ = 0.01 to observe a slower convergence.
σ=0
σ=0.3
σ=1.0
σ=2.0
Figure 7: Unit balls in l1,2 norm (5) with different noise levels.
13
Under review as a conference paper at ICLR 2020
B Proofs
B.1 Sufficient conditions for SPB: Proof of Lemma 1
Here we state the well-known Gauss’s inequality on unimodal distributions3.
Theorem 4 (Gauss’s inequality). Let X be a unimodal random variable with mode m, and let σm2 be
the expected value of (X - m)2. Then for any positive value of r,
Prob(|X - m| > r) ≤
4 (σm )2，
1 - ɪ工
√3 σm
if r ≥ √3 σm
otherwise
(
Applying this inequality on unimodal and symmetric distributions, direct algebraic manipulations
give the following bound:
Prob(∣X — μ∣ ≤ r) ≥
匕4 (r)2
√37σ σ，
if σ ≤ 空 ≥	丫/
otherwise — r∕σ + √3
where m = μ and σ21 = σ2 are the mean and variance of unimodal, symmetric random variable X,
and r ≥ 0. Now, using the assumption that each gi (x) has unimodal and symmetric distribution, we
apply this bound for X = gi(x), μ = gi(x), σ2 = σf(x) and get a bound for success probabilities
Prob(sign gi = sign gi)
PProb(gi ≥ 0),
[Prob(gi ≤ 0),
1-21-2
ifgi>0
if gi < 0
=11 + 2Prob(0 ≤ gi ≤ 2gi), if gi > 0
∖+ + 1 Prob(2gi ≤ gi ≤ 0), if gi < 0
=2 + 2Prob(l最-gi| ≤ |gi|)
≥ 1 + 1	㈤加，
_ 2	2 ∣gi∣∕σi + √3
= 1 + 1	∣gil
= 2 + 2 |gi| + √3σi
Improvment on Lemma 1 and l1,2 norm: The bound after Gauss inequality can be improved
including a second order term
11 - 4 (σ)2	ifσ ≤ √3	1
Prob(X - μ∣ ≤ r) ≥41 r9 rr, ,	Ir 12 ≥ 1 - τ———_, ,….
√1	√l -	[√3σ，	otherwise —	1 + r∕√3σ + (r∕√3σ)2
Indeed, letting Z := r∕√3σ ≥ 2∕3, We get 1
Otherwise, if 0 ≤ z ≤ 2/3, then z ≥ 1 -
tighter as
1+z+z2
≥ 1 - i+z；2 as it reduces to 23z2 - 4z - 4 ≥ 0.
as it reduces to 1 ≥ 1 - z3. The improvement is
二二1- J
Hence, continuing the proof of Lemma 1, we get
≤1-
1 + r∕√3σ + (r∕√3σ)2 .
Prob(Signgi = Signgi) ≥ 1 - 11 + w√3σi + (∣gi∣∕√3σi)
and we could have defined l1,2-norm in a bit more complicated form as
d
kgkl1,2 :=
i=1
1.....---------————-
1 + lgi∕√3σi + (lgi∕√3σi)2
|gi|.
3see https://en.wikipedia.org/wiki/Gauss%27s_inequality
1
1
1
2
14
Under review as a conference paper at ICLR 2020
B.2 Sufficient conditions for SPB: Proof of Lemma 2
Let g(τ) be the gradient estimator with mini-batch size T. It is known that the variance for g(τ) is
dropped by at least a factor of τ, i.e.
2
E[(g(τ) — gi)2] ≤ 士.
Hence, estimating the failure probabilities of sign g(T) when gi = 0, we have
Prob(signg(τ) = signgi) = Prob(Ig(T) - gi| = ∣g(τ)| + |gi|)
≤ Prob(∣g(τ) — giI ≥ |giI)
=Prob((O(TT- gi)2 ≥ g2
≤
E[(^(τ )-gi)2]
g2
σ
τg2,
which imples
σi2
Pi = Prob(signgi = signgi) ≥ 1----------2 ≥
τgi2
c
1
——
τ
B.3 Sufficient conditions for SPB: Proof of Lemma 3
We will split the derivation into three lemmas providing some intuition on the way. The first two
lemmas establish success probability bounds in terms of mini-batch size. Essentially, we present
two methods: one works well in the case of small randomness, while the other one in the case of
non-small randomness. In the third lemma, we combine those two bounds to get the condition on
mini-batch size ensuring SPB assumption.
Lemma 4. Let X1,X2,...,Xτ be i.i.d. random variables with non-zero mean μ := EXi = 0, finite
variance σ2 := E|Xi — μ∣2 < ∞. Thenfor any mini-batch size T ≥ 1
τ
Prob
sign 1X Xi
i=1
signμ) ≥ 1 — M.
T	τμ2
(11)
Proof. Without loss of generality, We assume μ > 0. Then, after some adjustments, the proof follows
from the Chebyshev’s inequality:
Prob
sign 1X Xi
i=1
sign μ j = Prob (—X: Xi > 0
=1 ------
τμ2
where in the last step we used independence of random variables X1, X2 , . . . , XT
Obviously, bound (11) is not optimal for big variance as it becomes a trivial inequality. In the case of
non-small randomness a better bound is achievable additionally assuming the finitness of 3th central
moment.
τ
□
15
Under review as a conference paper at ICLR 2020
Lemma 5. Let X1,X2,..., XT be i.i.d. random variables with non-zero mean μ := EXi = 0,
positive variance σ2 := E|Xi 一 μ∣2 > 0 and finite 3th central moment V3 := E|Xi 一 μ∣3 <
∞.
Then for any mini-batch size τ ≥ 1
τ
Prob
sign 1X Xi
i=1
sign μ ≥
1 + erf
(12)
2
where error function erf is defined as
erf(x) = √= / e-t2 dt, X ∈ R.
Proof. Again, without loss of generality, We may assume that μ > 0. Informally, the proof goes
as follows. As we have an average of i.i.d. random variables, we approximate it (in the sense of
distribution) by normal distribution using the Central Limit Theorem (CLT). Then we compute
success probabilities for normal distribution with the error function erf. Finally, we take into account
the approximation error in CLT, from which the third term with negative sign appears. More formally,
We apply Berry-Esseen inequality4 on the rate of approximation in CLT (Shevtsova, 2011):
Prob ( √- ^X(Xi — μ) > t J — Prob (N > t)
1 ν3
≤-------,
≤ 2 σ3√T,
t ∈ R,
where N 〜N(0,1) has the standard normal distribution. Setting t = -μ√τ∕σ, we get
Prob
1X Xi >0
i=1
1 ν3
≤ 2 σ3√τ.
(13)
It remains to compute the second probability using the cumulative distribution function of normal
distribuition and express it in terms of the error function:
τ
Prob
sign 1X Xi
sign μ
i=1
Prob
Xτ Xi > 0
i=1
(13)
≥
1 ν3
2 σ3√τ
□
Clearly, bound (12) is better than (11) when randomness is high. On the other hand, bound (12)
is not optimal for small randomness (σ ≈ 0). Indeed, one can show that in a small randomness
regime, while both variance σ2 and third moment V3 are small, the ration ν∕σ might blow up to
infinity producing trivial inequality. For instance, taking Xi 〜Bernoulli(p) and letting P → 1 gives
ν∕σ = O ((1 — p)-1/6). This behaviour stems from the fact that we are using CLT: less randomness
implies slower rate of approximation in CLT.
As a result of these two bounds on success probabilities, we conclude a condition on mini-batch size
for the SPB assumption to hold.
4see https://en.wikipedia.org/wiki/Berry-Esseen_theorem
16
Under review as a conference paper at ICLR 2020
Lemma 6. Let X1,X2,..., XT be i.i.d. random variables with non-zero mean μ = 0 and finite
variance σ2 < ∞. Then
τ
Prob
sign 1X χi
i=1
1	σ2	ν3
signμ > 2, if τ>2min①,∣μp
(14)
where ν3 is (possibly infinite) 3th central moment.
Proof. First, if σ = 0 then the lemma holds trivially. If ν = ∞, then it follows immediately from
Lemma 4. Assume both σ and ν are positive and finite.
In case of τ > 2σ2∕μ2 We apply Lemma 4 again. Consider the case T ≤ 2σ2∕μ2, which implies
μ√2τ ≤ 1. Itis easy to check that erf(x) is concave on [0,1] (in fact on [0, ∞)), therefore erf(x) ≥
erf(1)x for any X ∈ [0,1]. Setting X = √σ we get
f μμ√τλ ≥ erf⑴ μ√τ
V√2σJ ≥ √2	σ
which together with (12) gives
Prob sign
1X Xi
τ
i=1
—
signμ ≥2(ι+er√22乎
Hence, SPB assumption holds if
√2 ν3
T > -----------.
erf(1) μσ2
It remains to show that erf(1) > 1∕√2. Convexity of ex on X ∈ [—1,0] implies ex ≥ 1 + (1 — 1∕e)x
for any X ∈ [-1, 0]. Therefore
erf (1)
Z1e-t2
0
dt
— 1∕e)t2 dt
≥√∏ Z0(1-(1
2
> √4
71
9 > √2.
□
Lemma (3) follows from Lemma (6) applying it to i.i.d. data gi1(χ), gf(x),..., ^M (x).
B.4 Convergence Analysis: Proof of Theorem 1
First, from L-smoothness assumption we have
f (Xk+1 ) = f (xk — Yk sign gk )
dL
≤ f(xk) — hgk,Yk signgki +E_2(Yk signgk,i)
i=1
=f (Xk) — Yk hgk, sign gki + ɪγ2,
where gk = g(Xk), gk = g(Xk), gki is the i-th component of gk and L is the average value of Li's.
Taking conditional expectation given current iteration Xk gives
E[f(Xk+ι)∣Xk] ≤ f(Xk) — YkE[hgk, signgki] + dLY2.	(15)
17
Under review as a conference paper at ICLR 2020
Using the definition of success probabilities ρi we get
E[hgk, signgki] = hgk, E[signgk]i
d
=Egk,i ∙ E[sign gk,i] = E gk,i ∙ E[sign gk,i]
(16)
(17)
i=1
1≤i≤d
gk,i 6=0
gk,i (ρi(xk) sign gk,i + (1 - ρi(xk))(-signgk,i))
1≤i≤d
gk,i 6=o
(18)
1≤i≤d
gk,i 6=0
d
(2ρi (xk ) - 1)|gk,i | =	(2ρi (xk ) - 1)|gk,i | = kgk kρ .
(19)
i=1
Plugging this into (15) and taking full expectation, we get
Ekgkkρ ≤
E[f(xk)] - E[f (xk+ι)]	dL
----------T-----------+ T Yk.
γk
(20)
Therefore
KT	17 KT
X YkEkgkkρ≤ (f (xo) - f*) + τ X y2.
(21)
k=0
k=0
Now, in case of decreasing step sizes Yk = γo/√k +1
min Ekgkkρ ≤
o≤k<K
K-1
X
k=o
K-1
√⅛Ekgkkρ/ Xo
≤
≤
1
√K
1
√K
1
√K
f (xo) - f * , dL
—τ— + ɪ γo
Yo
f(xo) - f*
Yo
√k + 1
K-1	1
X ɪ
乙k + 1
k=o
+ YodL +
Yo
S-E + YodL
Yo
HLbg K
+
YodL log K
2	√K .
where we have used the following standard inequalities
K1
Ey； ≤ 2 + log K.
k
k=1
(22)
In the case of constant step size Yk = Y
1 K-1	1
K X Ekgkkρ ≤ YK

(f (xo) - f*) + dLY2K
f(x0)- f *
YK
dL
+ T γ.
18
Under review as a conference paper at ICLR 2020
B.5 Convergence Analysis: Proof of Theorem 2
Clearly, the iterations {xk}k≥0 of Algorithm 1 (Option 2) do not increase the function value in any
iteration, i.e. E[f (xk+1)|xk] ≤ f(xk). Continuing the proof of Theorem 1 from (20), we get
1 K-IE ll	1	K-1	E[f(xk)]	- E[f(xk+1)] dL
KK E Ekgkkp ≤	KK	E f(k)]	-,f(k+1)]	+ ZYk
k=0	k=0	γk
1	E[f (Xk)] - E[f (Xk+1)] A—；—j- l dL	γ0
=K 20--------Y0-------Gl+西 kξ T
k=0	k=0
K1
≤ 1	E[f (Xk)] - E[f (Xk +1)] + Y0dL
F ⅛	γ0	F
=f(xo) - E[f(xK)] + YodL
γ0 Vk	√K
V 1 f(xo)- f∖	,τ]
≤√KΓ^Γ~+Y叫，
where we have used the following inequality
The proof for constant step size is the same as in Theorem 1.
B.6 Convergence Analysis in Distributed Setting: Proof of Theorem 3
First, denote by I(p; a, b) the regularized incomplete beta function, which is defined as follows
I(p; a,b) = Bp^ = RitaT(1-'尸 dt
B(a,b)	Ro ta-1(1 - t)b-1dt
a, b> 0, p ∈ [0, 1].
(23)
The proof of Theorem 3 goes with the same steps as in Theorem 1, except the derivation (16)-(19) is
replaced by
E[hgk, sign^kM)i] = hgk, E[signgkM)]i
d
=X gk,i ∙ E [sign gkM)]
i=1
=X |gk,i| ∙ E hsign (⅛M) ∙ gk,，i
1≤i≤d
gk,i 6=o
= X |gk,i| (2I (ρi (Xk); l, l) - 1) = kgk kρM ,
1≤i≤d
gk,i 6=o
where we have used the following lemma.
Lemma 7. Assume that for some point X ∈ Rd and some coordinate i ∈ {1, 2, . . . , d}, master node
receives M independent stochastic signs signgm(x), m = 1,...,M oftrue gradient gi(x) = 0. Let
g(M)(x) be the sum ofstochastic signs aggregatedfrom nodes:
M
g(M) = X sign gm.
m=1
Then
E [sign 伍(M) ∙ gi) ] = 2I(Pi； l,l) — 1,	(24)
where l = [(M +1)/2] and ρi > 1/2 is the success probablity for coordinate i.
19
Under review as a conference paper at ICLR 2020
Proof. Denote by Sim the Bernoulli trial of node m corresponding to ith coordinate, where “success
is the sign match between stochastic gradient and gradient:
if sign gm = sign gi
otherwise
~ BemoUlli(ρi).
(25)
Since nodes have their own independent stochastic gradients and the objective fUnction (or dataset) is
shared, then master node receives i.i.d. trials Sim, which sUm Up to a binomial random variable Si :
M
Si := X Sm 〜Binomial(M,ρi).	(26)
m=1
First, let Us consider the case when there are odd nUmber of nodes, i.e. M = 2l - 1, l ≥ 1. In this
case, taking into accoUnt (25) and (26), we have
Prob 卜ign ^(M) =。) = 0,
P(M) ：=Prob 卜igngiM) = signgj = Prob(Si ≥ l),
1 — PiM) = Prob (Sign giM) = _ sign gj .
It is well known that cUmUlative distribUtion fUnction of binomial random variable can be expressed
with regUlarized incomplete beta fUnction:
Prob(Si ≥l) =I(Pi;l,M-l+1)=I(Pi;l,l).	(27)
Therefore,
E [sign (WM) ∙ gi)] = PiM) ∙1 + (I-P(M)) Y-I)
= 2Pi(M ) - 1
= 2Prob(Si ≥ l) - 1
= 2I(Pi; l, l) - 1.
In the case of even nUmber of nodes, i.e. M = 2l, l ≥ 1, there is a probability to fail the vote
Prob (Sign ^(m) = 0)> 0. However using (27) and properties of beta function5 gives
E [sign (^(2l) ∙ gi)] = Prob(Si ≥ l + 1) ∙ 1 + Prob(Si ≤ L- 1) ∙ (-1)
= I(Pi; l + 1, l) + I(Pi; l, l + 1) - 1
= 2I(Pi; l, l) - 1
=E [sign WjI) ∙ gi)].
This also shows that in expectation there is no difference between having 2l - 1 and 2l nodes.	口
B.7 Convergence Analysis in Distributed Setting: Variance reduction
Here we show exponential variance reduction in distributed setting in terms of number of nodes. We
first state the well-known Hoeffding’s inequality:
Theorem 5 (Hoeffding’s inequality for general bounded random variables; see (Vershynin, 2018),
Theorem 2.2.6). Let X1, X2, . . . , XM be independent random variables. Assume that Xm ∈
[Am , Bm ] for every m. Then, for any t > 0, we have
≤ exp
5see https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function
20
Under review as a conference paper at ICLR 2020
Define random variables Xim , m = 1, 2, . . . , M showing the missmatch between stochastic gradient
sign and full gradient sign from node m and coordinate i:
Xm := Γ-1, if sign^m = signgi
i 1, otherwise
(28)
Clearly EXim =1 - 2ρi and Hoeffding’s inequality gives
Prob
Xm - M(1 - 2ρi) ≥ t) ≤ eχp (-2M) , t > 0.
Choosing t = M (2ρi -1) > 0 (because of SPB assumption) yields
Prob
Xim ≥ 0
i
≤ exp (-2(2Pi- I)2M).
Using Lemma 24, we get
2I(ρi, l; l) - 1 = E [sign (g(M) ∙ g，] = 1 - Prob
Xim ≥ 0
≥ 1 - exp (-(2Pi - l)2l),
which provides the following estimate for PM -norm:
(1 - eχp (-(2ρ(x) - 1)2l)) ∣∣g(x)kι ≤ ∣∣g(x)kρM ≤ ∣∣g(x)kι,
where P(x) = min1≤i≤dPi(x) > 1/2.
C Convergence Result for S tandard SGD
For comparison, here we state and prove non-convex convergence rates of standard SGD with the
same step sizes.
Theorem 6 (Non-ConveX convergence of SGD). Let g be an unbiased estimator of the gradient Vf
and assume that E∣∣^∣∣2 ≤ C for some C > 0. Then SGD with step sizes Yk = γo/ʌ/k + 1 converges
as follows
•	π7∣∣v7 e( ∖∣∣2 /	1	f(xO)	- f * , 厂、T	.	YOCLmaX	log K	Cn∖
CmmI EkVf (Xk)∣2 ≤ "Tτ7------------+ YOCLmaX	+ ʒ--------7=^.	(29)
O≤k<K	2 K	YO	2	K
In the case of constant step size Yk ≡ Y > 0
ɪ IX EkVf (Xk)∣2 ≤ f (X0)- f * + CLmxγ.	(30)
K	YK	2
k=O
Proof. From L-smoothness assumption we have
E[f (xk+ι)∣Xk] = E[f (xk - Ykgk)∣Xk]
≤ f(xk) - E[hgk, Yk^ki] + LmxY2E[∣gkk2]
=f(xk) - Yk ∣gk k2 + Lmx Yk E[k^k k2].
Taking full eXpectation, using variance bound assumption, we have
E[f(xk+ι)] - E[f (Xk)] ≤ -Yk Ekgkkk + LmxYkC
Therefore
CL
YkEkgkkk ≤ E[f (Xk)] - E[f (Xk+ι)] + —max
21
Under review as a conference paper at ICLR 2020
Summing k = 0, 1, . . . , K - 1 gives
K-1	K-1
X YkEkgkk2 ≤ (f(χ0)-f*) + CLmax X Y.
k=0	k=0
Now, in case of decreasing step sizes Yk = γ0/√k + 1
K-1	K-1
0mi<κ Ekgk k2 ≤ X √fc Ekgk k2∕ X S
k=0	k=0
“ 1 f(xo)- f ɪ CLmax	K—	1 一
≤√K	Y0	+^γ°^ E
k=0
≤√⅛ [ Jfɪ + YoCLmax + YOCLmax log K]
K	Y0	2
1 ff (xo) - f 1 E L YoCLmax log K
=√k [	Yo	+ YOCLmT+√K.
where again we have used inequalities (22). In the case of constant step size Yk = Y
1 K^κll ∣∣2v 1 ∖fff 、 f*jCLmax 2/] f(xo) - f* ɪ CLmax
K TEkgkk2 ≤ YK Cf(XO)- f ) + ^-Y K = -YK— + ^-Y.
k=o	Y	Y
□
D Recovering Theorem 1 in (Bernstein et al., 2019) from
Theorem 1
To recover Theorem 1 in (Bernstein et al., 2019), first note that choosing a particular step size Y in (7)
yie S ⅛ X EkgkkP ≤《"(f 嗯-f*), with Y = ff.	(31)
K	K	dLK
k=o
Then, due to Lemma 1, under unbiasedness and unimodal symmetric noise assumption, we can lower
bound general ρ-norm by mixed l1,2 norm. Finally we further lower bound our l1,2 norm to obtain the
mixed norm used in Theorem 1 of Bernstein et al. (2019): let Hk = {1 ≤ i ≤ d: σi < √3∕2∣gk,i∣}
5产("-f' ≥√2K X Ekgkk
k=o
≥
≥
5 1 K-1
√K X Ekgkku-
5 1 X
√2 K k=o
d
X
i=1
g2
|gi| + √3σi
51
K-1
√2 K
k=o
2 X IgkM + √3 X /
5	5	σi
i∈Hk	i∈Hk
E
≥ K XE X igk,iι+X 誓
σ
k=O	i∈Hk	i∈Hk
E Stochastic signSGD
Our experiments and the counterexample show that signSGD might fail to converge in general. What
we proved is that SPB assumption is roughly a necessary and sufficient for general convergence.
There are several ways to overcome SPB assumption and make signSGD to work in general, e.g.
22
Under review as a conference paper at ICLR 2020
scaled version of signSGD With error feedback (Karimireddy et al., 2019). Here We to present a
simple Way of fixing this issue, Which is more natural to signSGD. The issue With signSGD is that
sign of stochastic gradient is biased, Which also complicates the analysis.
We define stochastic sign operator sign, Which unlike the deterministic sign operator is unbiased With
appropriate scaling factor.
dd
Definition 3 (Stochastic Sign). Define the stochastic sign operator sign : Rd → Rd as
with prob.
with prob.
22
∙2i^ ∙2i^
jg g- g
1-2 1-2
+-
1-2 1-2
1 ≤ i ≤ d,
j ~ CC	τ τ .1., T
and sign 0 = 0 with probability 1.
Furthermore, We define stochastic compression operator C : Rd → Rd as C(X) = ∣∣xk2 ∙ signx,
which compresses rd bits to r + d bits (r bits per one floating point number). Then for any unbiased
estimator g We get
E [C(^)] = E [E[C (^) ] g]]= E 卜 g∣2 (g + 2 备)一∣g∣2 (J - 2 备 U = E[g] = g,
Var[C(g)] = E [∣C(^) - g∣2] = E [∣∣C(g)∣∣2] - E [k^∣2] = (d - 1)Ekgk2∙
Using this relations, any analysis for SGD can be repeated for stochastic signSGD giving the same
convergence rate With less communication and With (d - 1) times Worse coefficients.
Another scaled version of signSGD investigated in Karimireddy et al. (2019) uses non-stochastic
compression operator C0 : Rd → Rd defined as C0(χ) = kχk1 sign x. It is shown (see Karimireddy
et al. (2019), Theorem II) to converge as
K X EkVf(Xk)k2 ≤ 2(f(X0K- f*) + γ⅛xC +4d(d - 1)γ2LmaχC,
k=0	γ
Where the error of current gradient compression is stored to be used in the next step. On the other
hand, adopting the analysis of Theorem 6 for the stochastic compression operator C, We get a bound
K1 £ EkVf(xk)k2 ≤ f⅛^ + —,
k=0	γ
Where no data needs to be stored. Furthermore, ignoring the factor 2 at the first term, later bound is
better if γ ≥ 1/8dLmax .
23