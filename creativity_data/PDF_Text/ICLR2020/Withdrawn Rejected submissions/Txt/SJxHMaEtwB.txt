Under review as a conference paper at ICLR 2020
Domain-invariant Learning using Adaptive
Filter Decomposition
Anonymous authors
Paper under double-blind review
Ab stract
Domain shifts are frequently encountered in real-world scenarios. In this paper,
we consider the problem of domain-invariant deep learning by explicitly model-
ing domain shifts with only a small amount of domain-specific parameters in a
Convolutional Neural Network (CNN). By exploiting the observation that a con-
volutional filter can be well approximated as a linear combination of a small set
of basis elements, we show for the first time, both empirically and theoretically,
that domain shifts can be effectively handled by decomposing a regular convo-
lutional layer into a domain-specific basis layer and a domain-shared basis coef-
ficient layer, while both remain convolutional. An input channel will now first
convolve spatially only with each respective domain-specific basis to “absorb”
domain variations, and then output channels are linearly combined using com-
mon basis coefficients trained to promote shared semantics across domains. We
use toy examples, rigorous analysis, and real-world examples to show the frame-
work’s effectiveness in cross-domain performance and domain adaptation. With
the proposed architecture, we need only a small set of basis elements to model
each additional domain, which brings a negligible amount of additional parame-
ters, typically a few hundred.
1 Introduction
Training supervised deep networks requires large amount of labeled training data; however, well-
trained deep networks often degrade dramatically on testing data from a significantly different do-
main. In real-world scenarios, such domain shifts are introduced by many factors, such as different
illumination, viewing angles, and resolutions. Research topics such as transfer learning and domain
adaptation are studied to promote invariant representations across domains with different levels of
availabilities of annotated data.
Recent efforts on learning cross-domain invariant representations using deep networks generally
fall into two categories. The first one is to learn a common network with constraints encouraging
invariant feature representations across different domains Long et al. (2015; 2016b); Tzeng et al.
(2014). The feature invariance is usually measured by feature statistics like Maximum Mean Dis-
crepancy (MMD), or feature discriminators using adversarial training Ganin et al. (2016). While
these methods introduce no additional model parameters, the effectiveness largely depends on the
degree of domain shifts. The other direction is to explicitly model domain specific characteris-
tics with a multi-stream network structure where different domains are modeled by corresponding
sub-networks at the cost of significant extra parameters and computations Rozantsev et al. (2018b).
Regularizations are imposed among sub-networks to encourage common semantics across domains.
In this paper, we model domain shifts through domain-adaptive filter decomposition (DAFD) with
layer branching. At a branched layer, we decompose each filter over a small set of domain-specific
basis elements to model intrinsic domain characteristics, while enforcing shared cross-domain coef-
ficients to align invariant semantics. A regular convolution is now decomposed into two steps. First,
a domain-specific basis convolves spatially only each individual input channel for shift “correction.”
Second, the “corrected” output channels are weighted summed using domain-shared basis coeffi-
cients (1×1 convolution) to promote common semantics. When domain shifts happen in space, we
rigorously prove that such layer-wise “correction” by the same spatial transform applied to bases
suffices to align the learned features.
Comparing to the existing subnetwork-based methods, the proposed method has several appealing
properties: First, only a very small amount of additional trainable parameters are introduced to
explicitly model each domain, i.e., domain-specific bases. The majority of the parameters in the
1
Under review as a conference paper at ICLR 2020
network remain shared across domains, and learned from abundant training data to effectively avoid
overfitting. Furthermore, the decomposed filters reduce the overall computations significantly com-
pared to previous works, where computation typically grows linearly with the number of domains.
We conduct extensive real-world face recognition, image classification, and segmentation experi-
ments, and observe that, with the proposed method, invariant representations across domains are
consistently achieved without compromising the performance of individual domain.
Our main contributions are summarized as follows:
•	We propose domain-invariant representation learning through bases decomposition with
layer branching, where domain-specific bases are learned to counter domain shifts, and
semantic alignments are enforced with cross-domain common basis coefficients.
•	We both theoretically prove and empirically observe that by stacking the bases-decomposed
branched layer, invariant representations across domains are achieved progressively.
•	The majority of network parameters remain shared across domains, which alleviates the
demand for massive annotated data from every domain, and introduces only a small amount
of additional computation and parameter overhead. Thus the proposed approach serves as
an efficient way for domain invariant learning.
Source Input ΞΞE1D□		Target Input
Conv layer		
ConV layer
(b) Basic Branching
Source Input Target Input
BBEinH ∙∙B■匿
Source Bases Target Bases
Cross-domain Coefficients
I SOUrce BaSeS ∣	∣ Target BaSeS ∣
Cross-domain Coefficients
FC layers + Classifier
(a) Regular CNN
(d) Feature space in (a)
(e) Feature space in (b)
FC layers + Classifier
(c) Branching with DAFD
(f) Feature space in (c)
Figure 1: Three candidate architectures considered for domain-invariant representation learning. In
(a), a set of common network parameters are trained to model both source and target domains. In
(b), the domain characteristics are explicitly modeled by two sets of convolutional filters in each
convolutional layer. Our approach is illustrated in (c) where domain-adaptive bases are learned to
“absorb” domain shifts, while the decomposition coefficients are shared across domains to promote
common semantics. The feature space of the three candidate architectures, MNIST → SVHN, are
visualized using t-SNE Maaten & Hinton (2008) in (d), (e), (f), respectively. The obtained cross-
domain invariance can be clearly observed in (f).
2	Related Work
Recent achievements on domain-invariant learning generally follow two directions. The first direc-
tion is learning a single network, where parameters are shared across domains, while the network
is encouraged to produce domain-invariant features by minimizing additional loss functions in the
network training Ganin et al. (2016); Long et al. (2015; 2016a;b); Tzeng et al. (2014). The Maxi-
mum Mean Discrepancy (MMD) Gretton et al. (2007), and MK-MMD Gretton et al. (2012) in Long
et al. (2015), are adopted as the discrepancy metric among domains. Beyond the first order statistic,
second-order statistics are utilized in Hoffman et al. (2014). Besides the hand-crafted distribution
distance metrics, Ganin et al. (2016); Tzeng et al. (2015); Long et al. (2018) resort to adversarial
training and achieve superior performances. Various distribution alignment methods, e.g., Wu et al.
(2019); Kumar et al. (2018), are proposed to improve the invariant feature learning. While effective
in certain scenarios, the performance of learning invariant features using a shared network is largely
constrained by the degree of domain shift as discussed in Rozantsev et al. (2018b). Meanwhile,
2
Under review as a conference paper at ICLR 2020
some recent works like Wu et al. (2019); Zhao et al. (2019) suggest important insights on whether
it is sufficient to do domain adaptation by invariant representation and small empirical source risk,
which shed light on exploring more effective alignment methods that are robust to common issues
like different marginal label distributions.
Another popular direction is modeling each domain explicitly using auxiliary network structures.
Based on the hypothesis that explicitly modeling what is unique to each domain can improve a
model’s ability to extract domain-invariant features, e.g., Bousmalis et al. (2016) proposes feature
representation by two components where domain similarities and shifts are modeled by a private
component and a shared component separately. A completely two-stream network structure with
no shared parameters is proposed in Rozantsev et al. (2018b), where auxiliary residual networks
are trained to adapt the layer parameters of the source domain to the target domain. Chang et al.
(2019) proposes attacking domain shifts by domain-specific batch normalization, which we believe
is compatible with the proposed domain-specific filter decomposition for better performance. An-
other popular direction for domain adaptation is to remap the input data between the source and the
target domain for domain adaptation Murez et al. (2017); Hu et al. (2018); Hoffman et al. (2018),
which is not included in the discussion since we are focusing on learning invariant feature space.
Finally, learning invariance is of relevance beyond domain adaptation, e.g., in the field of causal
inference Buhlmann (2018).
3	Domain-adaptive Filter Decomposition
A straightforward way to address domain shifts is to learn from multi-domain training data a single
network as in Figure 1(a). However, the lack of explicitly modelling of individual domains often re-
sults in unnecessary information loss and performance degradation as discussed in Rozantsev et al.
(2018b). Thus, we often simultaneously observe underfitting for domains with abundant training
data, and overfitting for domains with limited training. In this section, we start with a simplistic ped-
agogical formulation, domain-adaptive layer branching as in Figure 1(b), where domain shifts are
modeled by a respective branch of filters in a layer, one branch per domain. Each branch is learned
only from domain-specific data, while non-branched layers are learned from data from all domains.
We then propose to extend basic branching to bases-decomposed branching as in Figure 1(c), where
domain characteristics are modeled by domain-specific bases, and shared decomposition coefficients
are enforced to align cross-domain semantics.
3.1	Pedagogical Branching Formulation
We start with the simple-minded branching formulation in Figure 1(b). To model the domain-
specific characteristics, at the first several convolutional layers, we dedicate a separate branch to
each domain. Domain shifts are modeled by an independent set of convolutional filters in the
branch, trained respectively with errors propagated back from the loss functions of source and target
domains. For supervised learning, the loss function is the cross-entropy for each domain. For unsu-
pervised learning, the loss function for the target domain can be either the feature statistics loss or
the adversarial loss. The remaining layers are shared across domains. We assume one target domain
and one source domain in our discussion, while multiple domains are supported. Note that, though
we adopt the source vs. target naming convention in the domain adaptation literature, we address
here a general domain-invariant learning problem.
Domain-adaptive branching is simple and straightforward, however, it has the following drawbacks:
First, both the number of model parameters and computation are multiplied with the number of
domains. Second, with limited target domain training data, we can experience overfitting in deter-
mining a large amount of parameters dedicated to that domain. Third, no constraints are enforced
to encourage cross-domain shared semantics. We address these issues through layer branching with
the proposed bases decomposition.
3.2	Bases-decomposed Branching
To simultaneously counter domain shifts and enforce cross-domain shared semantics, we decompose
each convolutional filter in a branched layer into domain-specific bases, and cross-domain shared
coefficients, as illustrated in Figure 1(c).
3
Under review as a conference paper at ICLR 2020
(a) Decomposition over pre-fixed bases Qiu
et al. (2018).
Source Bases
(b) The proposed domain-adaptive filter decomposi-
tion.
Figure 2: Illustrations of the proposed domain-adaptive filter decomposition.
Filter Decomposition. A convolutional filter can be well approximated as a linear combination of
a small set of basis elements Qiu et al. (2018). In Figure 2(a), a convolutional filter W with a size of
L × L × C0 × C is decomposed into a truncated expansion with pre-fixed bases in the spatial domain.
The truncated bases ψ with a size L × L × K are predefined and remain fixed in training, where K is
a hyperparameter, usually a small number, e.g., K = 5, indicating the number of basis elements used
in each layer. The trainable parameters are thus reduced to the decomposition coefficients a with a
size of K × C0 × C. With bases decomposition, a reduction rate of K is achieved in both model
complexity and computation. This parameter saving is not the only virtue of basis decomposition,
such decomposition is natural to extract both invariance across domains and adaptation to specific
domains as detailed next.
In our approach, as shown in Figure 2(b), we decompose source and target domain filters over
domain-specific bases, with decomposition coefficients shared across domains. Specifically, at each
branched layer, the source domain filter Ws and target domain filter Wt of size L × L × C0 × C, are
decomposed into ψs × a and ψt × a, where ψs and ψt with a size of L × L × K are the domain-
adaptive bases for source and target domains, respectively; and a ∈ RK×C0×C denotes the common
decomposed coefficients shared across domains. Different from Qiu et al. (2018), the domain-
adaptive bases are independently learned from the corresponding domain data to model domain
shifts, and the shared decomposed coefficients are learned from the massive data from multiple
domains. Note that thanks to this proposed structure, only a small amount of additional parameters
is required here to model each additional domain, typically a few hundred.
With the above domain-adaptive filter decomposition, at each branched layer, a regular convolution
is now decomposed into two: First, a domain-specific basis convolves each individual input channel
for domain shift “correction.” Second, the “corrected” output channels are weighted summed using
domain-shared basis coefficients (1×1 convolution) to promote common semantics. A toy example
is presented in Figure A.1 for illustrating the intuition behind the reason why manipulating bases
alone can address domain shifts. We generate target domain data by applying two trivial operations
to source domain images: First, every 3× 3 non-overlapping patch in each image is locally rotated by
90o. Then, images are negated by multiplying with -1. Domain-invariant features are observed by
manipulating bases alone. We will rigorously prove in Section 4 why such layer-wise “correction”
aligns features across domains, and present real-world examples in the experiments.
Parameters and Computation Reduction. Suppose that both input and output features have the
same spatial resolution of W × W, in each forward pass in a regular convolutional layer, there
are totally W2 × C0 × C × (2L2 + 1) flops for each domain. While in our model, each domain
only introduces W 2 × C0 × 2K(L2 + C) flops, where K is the number of basis elements. For
parameters, there are totally D × C0 × C × L2 parameters in a regular convolutional layer where
D is the number of domains which is typically 2 in our case. In our model, each layer has only
K × (C0 × C+D × L2) parameters. Taking VGG-16 Simonyan & Zisserman (2014) as an example
with an input size of 224 × 224, a regular VGG-16 with branching, Fig 1(b) and Rozantsev et al.
(2018a;b), requires adding 14.71M parameters and 15.38G flops in convolutional layers to handle
each additional domain. With the here proposed method (Fig 1(c)), a VGG-16 only requires adding
702 parameters and 10.75G flops to handle one additional domain (K=6).
4 Provable Invariance with Adaptive Bases
In this section, we theoretically prove that the features produced by the source and target networks
from domain-transferred inputs can be aligned by the proposed framework of only adjusting multi-
4
Under review as a conference paper at ICLR 2020
layer bases, assuming a generative model of the source and target domain images via CNN. Since
convolutional generative networks are a rich class of models for domain transfer Hu et al. (2018);
Murez et al. (2017), our analysis provides a theoretical justification of the proposed approach. Di-
verse examples in the experiment section shows the applicability of the proposed approach is poten-
tially larger than what is proved here. All proofs are in the supplementary material.
Filter Transform via Basis Transform. Let ws and wt be the filters in the branched convolu-
tional layer for the source and target domains respectively, and similarly denote the source and
target bases by ψk,s and ψk,t. In the proposed bases decomposition DA architecture, the source
and target domain filters are linear combinations of the domain-specific basis elements with shared
decomposition coefficients, namely
ws(u) =	akψk,s(u), wt(u) =	akψk,t(u).
kk
Certain transforms of the filter can be implemented by only transforming the bases, including
(1)	A linear correspondence of filter values. Let λ : R → R be a linear mapping, by linearity,
ψk,s (u) → ψk,t(u) = λ(ψk,s (u))	applies ws(u) → wt(u) = λ(ws(u)).
E.g. the negation λ(ξ) = -λ(ξ), as shown in Figure A.1.
(2)	The transformation induced by a displacement of spatial variable, i.e., “spatial transform”
of filters, defined as Dτ w(u) = w(u - τ (u)), where τ : R2 → R2 is a differentiable
displacement field. Note that the dependence on spatial variable u in a filter is via the
bases, thus ψk,s → ψk,t = Dτ ψk,s applies ws → wt = Dτws .
If such filter adaptations are desired in the branching network, then it suffices to branch the bases
while keeping the coefficients ak shared, as implemented in the proposed architecture shown in
Figure 1(c). A fundamental question is thus how large is the class of possible domain shifts that
can be corrected by these “allowable” filter transforms. In the rest of the section, we show that if
the domain shifts in the images are induced from a generative CNN where the filters for source and
target differ by a sequence of allowable transforms, then the domain shift can be provably eliminated
by another sequence of filter transforms which can be implemented by bases branching only.
Correction of a Single Filter Transform. We first analyze the “symmetric” correction of one
filter spatial transform Dτ in one layer. The inclusion of linear correspondence transform is more
direct (see comments after Theorem 1). For technical reasons, we assume that the displacement field
T is a small distortion, namely ∣∣Vτ∣∣∞《 1, and then DT is invertible. Example includes rotation
by a small angle and a small factor rescaling (dilation).
For simplicity we only consider one input and output channel in each of the multiple convolutional
layers. The argument extends to multiple channels by modifying the boundedness condition of the
filters. Then the forward mapping in one convolutional layer can be written as y = σ(x * W + b),
where x is the input activation, y is the output, w is the filter, b is the constant bias, and σ is the
nonlinear activation function, e.g., ReLU. As we take a continuous formulation in the analysis, the
activations X and y are assumed to be smooth functions supported on domain Ω ⊂ R2, typically
Ω = [-1,1]2. The filter W is a function supported on 2jB, B being the unit disk, and 2j is layer
scale (diameter of filter patches) . The 1-norm ofa function is defined to be ∣x∣1 = R2 |x(u)|du.
Lemma 1. Suppose that the two filters W, f are supported on 2jwB and 2jfB respectively. σ :
R → R is non-expansive, Dτ is a spatial transform where τ is odd, i.e., τ (-u) = -τ(u), and
∣Vτ∣∞ < 1 .Then
∣σb(x * DτW) * f - σb(x * W) * Dτ-1f∣1
≤ 2∣Vτ∣∞∣w∣ιkf kι {(2jw + 2jf )∣Vx∣ι +4∣∣x∣∣ι},
where σb denotes the nonlinear function with the bias. The second term vanishes if (Id - τ) is a
rigid motion, e.g., rotation.
Provable Invariance under a Generative Model of Images. Stacking the approximate commut-
ing relation, Lemma 1, in multiple layers allows to correct a sequence of filter transforms in previous
convolutional layers by another sequence of “symmetric” ones. This means that ifwe impose a con-
volutional generative model on the source and target input images, and assume that the domain
transfer results from a sequence of spatial transforms of filters in the generative net, then by correct-
ing these filter transforms in the subsequent convolutional layers we can guarantee the recovery of
5
Under review as a conference paper at ICLR 2020
the same feature mapping. The key observation is that the filter transfers can be implemented by
bases transfer only.
We summarize the theoretical assumptions as follows:
(A1) The σ in any layer is non-expansive,
(A2) In the generative net (where layer is indexed by negative integers), wt(-l) = Dlws(-l),
where Dl = DTl, Tl is odd and ∣Vτι ∣∞ ≤ ε < 1 for all l = 1,…，L. The biases in the
target generative net are mildly adjusted accordingly due to technical reasons (to preserve
the “baseline output” from zero-input, c.f. the proof).
(A3) In the generative net, kws(-l) k1 ≤ 1 for all l, and so is wt(-l) = Dlws(-l). Same for the
feed-forward convolutional net taking the generated images as input, called “feature net”:
The source net filters have ∣∣w(l) k 1 ≤ 1 for l = 1, 2 ∙∙∙ ,and same with Dlws) which will
be set to be w(l). Also, wS-l) and w(l) are both supported on 2jl B for l = 1,…，L.
One can show that ∣DTw∣∣ι = ∣∣w∣ι when (Id 一 P) is a rigid motion, and generally ∣∣Dτw∣∣ι 一
IlwIl 1| ≤ c∣Vτ∣∞kwkι which is negligible when ε is small. Thus in (A3) the boundedness of the 1-
norm of the source and target filters imply one another exactly or approximately. The boundedness
of 1-norm of the filters preserves the non-expansiveness of the mapping from input to output in
a convolutional layer, and in practice is qualitatively preserved by normalization layers. Also, as
a typical setting, (A3) assumes that the scales jl in the generative net (the (-l)-th layer) and the
feature net (the l-th layer) are matched, which simplifies the analysis and can be relaxed.
Theorem 1. Suppose that Xs and Xt are source and target images generated by L-layer generative
CNN nets with source and target filters ws(-l), wt(-l) respectively from the common representation
h. Under (A1)-(A3), the output at the L-th layer of the target feature CNN from Xt, by setting
wt(l) = Dlws(l) in all layers which can be implemented by bases branching, approximates that of the
source feature CNN from Xsup to an error which is bounded in 1-norm by
4ε
(XL 2jl)∣Vh∣1+2L∣h∣
,
and the second term vanishes if (Id 一 τl) are rigid motions, e.g., rotation.
Note that typically jι ≤ ∙∙∙ ≤ jL and 2JL ≤ 1, and Pl 2jl is proportional to the diameter of Ω and
thus an O(1) constant. The result can be extended to allow a linear transform of the filter values,
namely Dw(u) = λ(DT w(u)), and correct the target filters ft to be λ-1(DTfs). In Lemma 1,
λ commutes with the convolution by linearity, and also with σ by adjusting the bias parameter if
needed. Unlike DT which becomes DT-1 when applying to f, λ remains as λ, thus λ-1 is needed in
the symmetric basis transfers to correct the domain shift. Proofs are in the supplementary material.
5 Experiments
In this section, we perform extensive experiments to evaluate the performance of the proposed
domain-adaptive filter decomposition. We start with the comparisons among the 3 architectures
listed in Figure 1 on two supervised tasks. To demonstrate the proposed framework as one princi-
pled way for domain-invariant learning, we then conduct a set of domain adaptation experiments.
There we show, by simply plugging the proposed domain filter decomposition into regular CNNs
used in existing domain adaptation methods, we consistently observe performance improvements,
which illustrates that the framework can be successfully applied with multiple leading deep network
architectures and very different datasets and applications.
5.1	Architecture Comparisons
We start with two supervised tasks performed on the three architectures listed in Figure 1, regular
CNN (A1), basic branching (A2), and branching with domain-adaptive filter decomposition (A3).
In each layer of the network with the proposed domain-adaptive filter decomposition, the different
domain features convolve spatially with the corresponding domain-specific bases first, then the out-
put features are linearly combined by the cross-domain basis coefficients simultaneously. For both
tasks, the networks are trained end-to-end with a summed loss for domains, and the domain-specific
bases are only updated by the error from the corresponding domain, while the basis coefficients are
updated by the joint error across domains.
6
Under review as a conference paper at ICLR 2020
Scales	Source domain			Target domain		
	0.1	0.05	0.005	0.1	0.05	0.005
A1	98.4	96.4	98.0	81.6	80.2	61.0
A2	99.2	98.6	97.6	81.4	78.4	49.6
A3	99.4	98.8	98.8	85.6	82.2	64.4
(a) Accuracy (%) on both domains for SuPer-
vised domain adaptation.
Methods	VIS Acc (%)	NIR Acc (%)	NIR+VIS (%)
A1	75.57	52.71	98.44
A2	94.46	87.50	98.58
A3	97.16	95.03	99.15
(b) Cross-domain simultaneous face recognition on NIR-
VIS-2.0.
Table 1: ComParisons on suPervised domain adaPtaion and cross-domain simultaneous face recog-
nition. A1, A2, and A3 corresPond to regular CNN, basic branching, and branching with domain-
adaPtive filter decomPosition shown in Figure 1, resPectively.
Supervised Domain Adaptation on Digits. The first task is suPervised domain adaPtation, where
we adoPt a challenging setting by using MNIST handwritten digit dataset as the source domain,
and SVHN as the target domain. We Perform a series of exPeriments by Progressively reducing the
annotated training data for the target domain. We start the comParisons at 10% of the target domain
labeled samPles, and end at 0.5% where only 366 labeled samPles are available for the target domain.
The results on test set for both domains are Presented in Table 3(a). It is clearly shown that when
training the target domain with small amount of data, a network with basic branching suffers from
overfitting to the target domain because of the large amount of domain sPecific Parameters. While
regular CNN generates well on target domain, the Performance on source domain degrades when the
number of target domain data is comParable. A network with the ProPosed domain-adaPtive filter
decomPosition significantly balances the learning of both the source and the target domain, and
achieves best accuracies on both domains regardless of the amount of annotated training data for the
target domain. The feature sPace of the three candidate architectures are visualized in Figure 1.
Supervised Simultaneous Cross-domain Face Recognition. To show the generality of the Pro-
Posed domain-adaPtive filter decomPosition for various tasks, we further Perform a suPervised cross-
domain (RGB and near infrared) simultaneous face recognition exPeriment in the suPPlementary
material Section B, and the quantitative comParisons are in Table 3(b), where we can clearly observe
that DAFD Performs suPeriorly even with a missing inPut domain. Note that A2 requires additional
14.71M Parameters over A1, while our method requires only 0.0007M as shown in Table A.1.
5.2	Experiments on S tandard Domain Adaptations
In this section, we Perform extensive exPeriments on unsuPervised domain adaPtation. Note that
the objective of the exPeriments in this section is not to validate the ProPosed domain-adaPtive filter
decomPosition as just another new method for domain adaPtation. Instead, since most of the state-of-
the-art domain adaPtation methods adoPt the regular CNN (A1) with comPletely shared Parameters
for domains, we show the comPatibility and the generality of the ProPosed domain-adaPting filter
decomPosition by Plugging it into underlying domain adaPtation methods, and evaluate the effec-
tiveness by retraining the networks using exactly the same setting and observing the Performance
imProvement over the underlying methods. Diverse real-world domain shifts including different
sensors, different image sources, and synthetic images, and aPPlications on both classification and
segmentation are examined in these exPeriments. Together with the exPeriments in the Previous
section, this further stresses the Plug-and-Play virtue of the ProPosed framework.
In Practise, instead of learning indePendent source and target domain bases, we learn the residual
between the source and the target domain bases. The residual is initialized by full zeros, and trained
by loss for encouraging invariant features in the underlying methods, e.g., the adversarial loss in
ADDA Tzeng et al. (2017). We consistently observe that this stabilizes the training and Promotes
faster convergence.
Digit Classification. We Perform exPeri- ments on three Public digits datasets: MNIST, USPS, and Street View House Numbers (SVHN), with three transfer tasks: USPS to MNIST (U → M), MNIST to USPS (M → U), and SVHN to MNIST (S → M). Classification	Table 2: Accuracy (%) on Digits for unsuPervised domain adaPtation. Methods	∣ M → U^^U → M	S → M	Avg				
	DANN ADDA CDAN+E	- 89.4 95.6	- 90.1 98.0	73.9 76.0 89.2	85.1 94.3
accuracy on the target domain test set samPles	ADADNDNA + DDAAFFDD ADDA + DAFD is adoPted as the metric for measuring the Per- CDAN+E + DAFD formance. We Perform domain-adaPtive domain decomPosition		92.0	95.2	82.1 (11.1% ↑)	89.8 91.4	94.8	82.9	89.7(5.5% ↑) 96.8	98.8	96.6	97.4(3.2% ↑) on state-of-the-art methods DANN			
7
Under review as a conference paper at ICLR 2020
Ganin et al. (2016), ADDA Tzeng et al. (2017), and CDAN+E Long et al. (2018). Quantitative
comparisons are presented in Table 2.
Office-31. Office-31 is one of the most widely used datasets for visual domain adaptation, which
has 4,652 images and 31 categories collected from three distinct domains: Amazon (A), Webcam
(W), and DSLR (D). We evaluate all methods on six transfer tasks A → W, D → W, W → D, A
→ D, D → A, and W → A. Two feature extractors, AlexNet Krizhevsky et al. (2012) and ResNet
He et al. (2016) are adopted for fair comparisons with underlying methods. Specifically, ImageNet
initialization are widely used for ResNet in the experiments with Office-31, and we consistently ob-
serve that initialization is important for the training on Office-31. Therefore, when training ResNet
based networks with domain-adaptive filter decomposition, we initialize the feature extractor using
parameters decomposed from ImageNet initialization. The quantitative comparisons are in Table 3.
Table 3: Accuracy (%) on Office-31 for unsupervised domain adaptation (AlexNet and ResNet).
	Method	A → W	D→W	W→D	A→D	D→A	W→A	Avg.
AlexNet	AleXNet (no adaptation)	61.6±0.5	95.4±0.3	99.0±0.2	63.8±0.5	51.1±0.6	49.8±0.4	70.1
	DANN Ganin et al. (2016)	73.0±0.5	96.4±0.3	99.2±0.3	72.3±0.3	53.4±0.4	51.2±0.5	74.3
	ADDATzengetal.(2017)	73.5±0.6	96.2±0.4	98.8±0.4	71.6±0.4	54.6±0.5	53.5±0.6	74.7
	DANN + DAFD	74.4±0.3	97.1±0.4	99.1±0.4	74.2±0.3	56.8±0.5	53.1±0.7	75.8 (2.3% ↑)
	ADDA + DAFD	77.2±0.5	97.9±0.4	98.5±0.2	73.2±0.4	55.4±0.6	57.8±0.5	76.7 (2.7% ↑)
ResNet-50 (no adaptation)	68.4±0.2	96.7±0.1	99.3±0.1	68.9±0.2	62.5±0.3	60.7±0.3	76.1
DANN Ganin et al. (2016)	82.0±0.4	96.9±0.2	99.1±0.1	79.7±0.4	68.2±0.4	67.4±0.5	82.2
ADDA Tzeng et al. (2017)	86.2±0.5	96.2±0.3	98.4±0.3	77.8±0.3	69.5±0.4	68.9±0.5	82.9
CDAN+E Long et al. (2018)	94.1 ±0.1	98.6±0.1	100.0±.0	92.9±0.2	71.0±0.3	69.3±0.3	87.7
DANN + DAFD	86.4±0.4	96.8±0.2	99.2±0.1	84.4±0.4	70.5±0.4	68.8±0.4	84.35 (2.3% ↑)
ADDA + DAFD	86.8±0.4	97.7±0.1	98.4±0.1	80.5±0.3	71.1±0.4	69.1±0.5	83.9 (1.2% ↑)
CDAN+E + DAFD	95.6±0.1	98.8±0.1	100.0±0.0	93.5±0.2	76.6±0.5	71.3±0.4	89.3 (1.8% ↑)
Image Segmentation. Beyond image classification tasks, we perform a challenging experiment
on image segmentation to demonstrate the generality of the proposed domain-adaptive filter decom-
position. We perform unsupervised adaptation from the GTA dataset Richter et al. (2016) (images
generated from video games) to the Cityscapes dataset Cordts et al. (2016) (real-world images),
which has a significant practical value considering the expensive cost on collecting annotations for
image segmentation in real-world scenarios. Two underlying methods FCNs in the wild Hoffman
et al. (2016) and AdaptSegNet Tsai et al. (2018) are adopted for comprehensive comparisons. Based
on the underlying methods, all the convolutional layers are decomposed using domain-adaptive fil-
ter decomposition, and all the transpose-convolutional layers are kept sharing by both domains. For
quantitative results in Table 4, We use intersection-over-union, i.e., IoU = tp+FP+fn , where TP,
FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively,
as the evaluation metric. As with the previous examples, our method improves all state-of-the-art
architectures. Qualitative results are shown in Figure A.2, and data samples are in Section C.1.
Table 4: Unsupervised DA for semantic segmentation: GTA → Cityscapes
Methods	IoU	Class-wide IoU																		
		/	然	X	参	Z	ɪ	/	y”	0	Z 砂	舲	奇_	/	亦	6	产	Z		
																				
No Adapt (VGG)	17.9	26.0	14.9	65.1	5.5	12.9	8.9	6.0	2.5	70.0	2.9	47.0	24.5	0.0	40.0	12.1	1.5	0.0	0.0	0.0
No Adapt (ResNet)	36.6	75.8	16.8	77.2	12.5	21.0	25.5	30.1	20.1	81.3	24.6	70.3	53.8	26.4	49.9	17.2	25.9	6.5	25.3	36.0
FCN WLD (VGG)	27.1	70.4	32.4	62.1	14.9	5.4	10.9	14.2	2.7	79.2	21.3	64.6	44.1	4.2	70.4	8.0	7.3	0.0	3.5	0.0
AdaptSegNet (VGG)	35.0	87.3	29.8	78.6	21.1	18.2	22.5	21.5	11.0	79.7	29.6	71.3	46.8	6.5	80.1	23.0	26.9	0.0	10.6	0.3
AdaptSegNet (ResNet)	42.4	86.5	36.0	79.9	23.4	23.3	23.9	35.2	14.8	83.4	33.3	75.6	58.5	27.6	73.7	32.5	35.4	3.9	30.1	28.1
FCN WLD + DAFD	32.7 (20.7% ↑)	76.4	36.7	68.8	17.6	5.8	11.1	13.9	2.9	80.0	24.4	69.1	47.5	4.3	74.4	14.1	6.3	0.0	2.1	0.0
AdaptSegNet (VGG) + DAFD	36.4(4.0% ↑)	86.7	35.3	78.8	22.8	14.5	23.9	21.9	18.2	82.1	32.2	66.8	49.6	10.1	81.2	19.6	27.1	1.1	11.4	4.2
AdaptSegNet (ResNet) + DAFD	45.0(6.1% ↑)	88.2	38.5	8.12	25.0	23.8	22.9	35.1	14.4	84.9	34.1	79.9	59.5	29.1	75.5	30.1	35.2	2.9	28.7	29.1
6 Conclusion
We proposed to perform domain-invariant learning through domain-adaptive filter decomposition.
To model domain shifts, convolutional filters in a deep convolutional network are decomposed over
domain-adaptive bases to counter domain shifts, and cross-domain basis coefficients are constrained
to unify common semantics. We present the intuitions of countering domain shifts by adapting bases
through toy examples, and further provide theoretical analysis. Extensive experiments on multiple
tasks validate that, by stacking domain-adaptive branched layers with filter decomposition, complex
domain shifts in real-world scenarios can be bridged to produce domain-invariant representation,
which are reflected by both experimental results and feature space visualizations, all this at virtual
no additional memory or computational cost when adding domains.
8
Under review as a conference paper at ICLR 2020
References
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Advances in Neural Information Processing Systems, pp. 343-
351, 2016.
Peter Buhlmann. Invariance, causality and robustness. arXiv preprint arXiv:1812.08233, 2018.
Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-specific
batch normalization for unsupervised domain adaptation. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp. 7354-7362, 2019.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic
urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3213-3223, 2016.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex J Smola. A ker-
nel method for the two-sample-problem. In Advances in Neural Information Processing Systems,
pp. 513-520, 2007.
Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil,
Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale two-sample
tests. In Advances in Neural Information Processing Systems, pp. 1205-1213, 2012.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick,
Trevor Darrell, and Kate Saenko. Lsda: Large scale detection through adaptation. In Advances in
Neural Information Processing Systems, pp. 3536-3544, 2014.
Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. FCNs in the wild: Pixel-level adver-
sarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,
and Trevor Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In International
Conference on Machine Learning, pp. 1994-2003, 2018.
Lanqing Hu, Meina Kan, Shiguang Shan, and Xilin Chen. Duplex generative aaversarial network for
unsupervised domain qdaptation. In IEEE Conference on Computer Vision and Pattern Recogni-
tion, pp. 1498-1507, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Abhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill Free-
man, and Gregory Wornell. Co-regularized alignment for unsupervised domain adaptation. In
Advances in Neural Information Processing Systems, pp. 9345-9356, 2018.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features
with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. arXiv preprint arXiv:1605.06636, 2016a.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136-
144, 2016b.
9
Under review as a conference paper at ICLR 2020
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 9(Nov):2579-2605, 2008.
Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, and Kyungnam Kim. Image to
image translation for domain adaptation. arXiv preprint arXiv:1712.00479, 13, 2017.
Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, and Guillermo Sapiro. DCFNet: Deep neural
network with decomposed convolutional filters. International Conference on Machine Learning,
2018.
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth
from computer games. In European Conference on Computer Vision, pp. 102-118. Springer,
2016.
Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. Beyond sharing weights for deep domain
adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018a.
Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. Residual parameter transfer for deep domain
adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, number CONF,
2018b.
Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised
domain adaptation. arXiv preprint arXiv:1702.08400, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan
Chandraker. Learning to adapt structured output space for semantic segmentation. In IEEE
Conference on Computer Vision and Pattern Recognition, pp. 7472-7481, 2018.
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across
domains and tasks. In IEEE International Conference on Computer Vision, pp. 4068-4076, 2015.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pp. 4,
2017.
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. In International Conference on Machine Learn-
ing, pp. 6872-6881, 2019.
Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Collaborative and adversarial network for
unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recogni-
tion, pp. 3801-3809, 2018.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532, 2019.
10
Under review as a conference paper at ICLR 2020
A Toy Experiment
Figure A.1: Visualization of the toy example. The two columns visualize the inputs, features, and
bases of the source domain and the target domain, respectively. Only the output feature in the first
channel of each convolutional layer is visualized for comparison. Domain invariant features, the last
row, are obtained by manually adapting source domain bases to generated target domain bases.
B	Supervised Cross-domain Face Recognition
Besides standard domain adaptation, the proposed domain-adaptive filter decomposition can be ex-
tended to general tasks that involves more than one visual domain. Here we demonstrate this by
performing experiments on supervised cross-domain face recognition. We adopt the NIR-VIS 2.0,
which consists of 17,580 NIR (near infrared) and VIS (visible light) face images of 725 subjects,
and perform cross-domain face recognition. We adopt VGG16 as the base network structure, branch
all the convolutional layers with the proposed domain-adaptive filter decomposition, and train the
network from scratch. In each convolutional layer, two bases are trained for modeling the NIR and
the VIS domain, respectively. Specifically, one VIS image and one NIR image are fed simulta-
neously to the network, and the feature vectors of both domains are averaged to produce the final
cross-domain feature, which is further fed into a linear classifier for classifying the identity. While
the training is conducted using both domains simultaneously, we test the network under three set-
tings including feeding single domain inputs only (VIS Acc and NIR Acc in Table 3(b)) and both
domain inputs (VIS+NIR Acc in Table 3(b)). Quantitative comparisons demonstrate that branching
with the proposed DAFD performs superiorly even with a missing input domain.
C Dataset Samples and Qualitative Results
C.1 Unsupervised DA for Image Segmentation
For the segmentation experiments in Section 5.5, we provide more qualitative results in Figure A.2,
and some dataset samples in Figure A.3.
D Computation and Parameters
In Table A.1, we provide comparisons on additional parameters and computation introduced by one
extra domain with and without the proposed domain-adaptive filter decomposition. The comparison
reveals that domain-adaptive filter decomposition not only delivers superior performances but also
saves both parameters and computation significantly.
11
Under review as a conference paper at ICLR 2020
(a) Target domain image.
(b) Before adaptation. (c) After adaptation. (d) Ground truth.
Figure A.2: Qualitative results for domain adaptation segmentation. The samples are randomly
selected from the validation subsets of Cityscapes.
(a) Source domain (GTA: video game images).
(b) Target domain (Cityscapes: real-world images).
Figure A.3: Dataset samples for segmentation experiments (video games → street views).
E	Proofs in Section 4
Proof of Lemma 1. We establish a few facts:
Fact 1. ∣Vτ∣∞ < 1 guarantees that, P := Id - T,
∣∣Jρ∣-1∣,∣∣Jρ-11-1| ≤ 4∣Vτ∣∞,	(A.1)
where Jf = det(Vf) denotes the determinate of the Jacobian matrix of the mapping f : R2 → R2.
The inequality can be verified by elementary calculation. When ρ is a rigid motion then the r.h.s of
equation A.1 is zero.
12
Under review as a conference paper at ICLR 2020
Table A.1: Comparisons on additional parameters and computation introduced by one extra domain.
Comparisons are performed on VGG-16, with 6 basis elements and the input size of 224 × 224.
Model	Regular VGG	VGG with DAFD
Parameters Flops	14.71M 15.38G	0.0007M 10.75G
Fact 2. ρ is invertible, and odd symmetry of τ implies that ρ and thus ρ-1 are odd, namely
-ρ-1(-u) = ρ-1(u).
Define
yι(u) ：= σb(x * DTW) * f (u)
Z	σb	Z
R2	R2
Z	σb	Z
R2	R2
x(u + v - z)w(ρ(z))dz f (-v)dv
x(u + V — ρ-1 (Z))w(Z)∣Jρ-1 (z)∣dz I f(-v)dv
and
yι(u) ：= I σb ( X x(u + v — ρ-1 (z))w(Z)dzj f(—v)dv.
R2	R2
We have that
∣yι(u) — yι(u)∣ ≤ / I |x(u + V — PT(Z))∣∣w(Z)∣ ||Jp-1| — 1∣ |f (—v)∣dzdv (by σb non-expansive)
R2 R2
≤ 4∣Vτ∣∞ [ I |x(u + v — PT(Z))∣∣w(Z)∣∣f(—v)|dZdv (by Fact 1)
R2 R2
and thus
kyι - yιkι ≤ 4|Vtl∞kχkιkwkikfkι.
When P is a rigid motion, y 1 = yι.
Also, let
y2 (u) := σb(x * w) * Dτ-1f(u)
(A.2)
and
Z	σb	Z
R2	R2
Z	σb	Z
R2	R2
x(u + v — z)w(z)dz ) f (—ρ-1(v))dv (by Fact 2)
x(u + ρ(V) — Ziw(ZldZ I f(—V)|Jρ(V)∣dV
y2(u) ：= I σb ( I x(u + ρ(V) — Z)w(Z)dZ j f (—V)dV.
R2	R2
Similar to the proof of equation A.2, one can verify that
I∣y2 - y2 k1 ≤ 4|Vt l∞kxk1 kwkikf k1,
and the bound is zero when P is a rigid motion.
It remains to bound kyi — y2k1. Note that by σb being non-expansive again
(A.3)
lyl(u) ― y2(u)| ≤
|x(u + v — ρ-1(Z)) — x(u + ρ(v) — Z)||w(Z)|dZ|f (—v)|dv.
(A.4)
We claim that
J	|x(u + V —	PT(Z))	—	x(u	+ p(v)	—	Z)∣du ≤ ∣Vτ∣∞2(2jw	+	2jf )kVxkι
uniformly for V and Z. If true, with equation A.4 it gives that
I |yi(u) — y2(u)∣du ≤∣Vτ∣∞2(2jw +2jf )kVxkιkwkιkfkι
R2
which proves the lemma together with equation A.2 and equation A.3.
(A.5)
13
Under review as a conference paper at ICLR 2020
Proof of equation A.5: We verify that for any fixed v, z,
/ |x(u + V 一 PT(Z)) 一 x(u + P(V) 一 z)∣du ≤ ∣∣Vx∣ι∣Vτ∣∞∣v 一 PT(Z)|,
by a direct calculation:
(A.6)
(Ihs) ≤ ||Vxki|(v - PT(Z)) -(P(V)-Z)I
=∣Vx∣ι∣τ (V)-T (PT(Z))|
≤ ∣Vχ∣ι∣Vτ∣∞∣v 一 PT(Z)|.
Then, combined with that V
1
1-∣Vτ ∣∞
2jw
≤ 22jw (τ (0)
∈ 2jfB thus |V| ≤ 2jf, and Z ∈ 2jwB and thus |P-1(Z)| ≤
0 by that T is odd, and then ∣τ(PT(Z))∣ ≤ ∣Vτ∣∞∣P-1 (z)|), the
r.h.s of equation A.6 ≤ 2(2jw + 2jf )∣Vτ ∣∞∣Vx∣ι, which proves equation A.5.
□
Proof of Theorem 1. We need a slightly generalized form of Lemma 1, which inserts multiple plain
convolutional layers between *w and *f, presented in Lemma 2.
Under the setting of the theorem, in the generative CNNs,
Xs = σ(…σ(h	* Ws-L) + bs-L))∙∙	• * w(-1) + bs-1))	(A.7)
Xt = σ(…σ(h	* w(-L) + b(-L))-	•* W(-1) + b(-1))	(A.8)
where Wt(l) and bt(l) are defined by, l =	-L, ∙∙∙ , -1,		
Wt(l) = DlWs(l),	X0l) * Wsl) + b(l)=	=X0l) * W(l) + bsl).	(A.9)
The notation X(l)stands for the l-th layer output in the target net from the input in the bottom ((-L)-
th) layer as x(-L) = h, x(0) = Xt, and XOl) for that from zero input in the bottom. In the feature
CNNs, the L-th layer outputs are
Fs = σ(…σ(Xs * W(I) + b,1))…* W(L) + bSL))	(A.10)
Ft = σ(…σ(Xt * w(1)+ b(1))…* w(L) + b(L))	(A.11)
where for l = 1,…，L,
Wt(l) = DlWs(l),	bt(l) = b(sl) .
The proof is by applying Lemma 2 recursively to the pair of layers indexed by l and -l, from l = 1
to L. Denote W(I) by w(l), then w(l) = Dlw(l), where D-l = Dl = DTl, l = 1,…，L. We also
denote b(sl) by b(l) and keep notation bt(l) for negative l.
First, l = 1, in the target net,
X(1) := σ(σ(X(-1) * Dιw(-1) + bt-1)) * D]W⑴ + b(1))
Use the centering xC-1) := X(-1) 一 X0^1), it can be written as
X(I) = σ(σ(χC-1) * Dιw(-1) + X0-1) * Dιw(-1) + b(-1)) * DIW(I) + b(1))	(A.12)
=σ(σ(Xc-I) * Dιw(-1) + (X0-1) * w(-1) + b(-1))) * D1w⑴ + b(1)) (by equation A.9)
(A.13)
Applying Lemma 2 (or Lemma 1 for this case), taking X0T) * W(T) + b(-1) as the effective "b”,
we have that (using the non-expansiveness of σ to take r outside the last σ)
X(I) = σ(σ(Xc-I) * w(-1) + X0-1) * w(-1) + b(-1)) * W⑴ + b(I)) + r(I)	(A.14)
=σ(σ(X(-1) * w(-1) + b(-1)) * W⑴ + b(I)) + r(I)	(A.15)
:=X(I) + r(I)	(A.16)
where, since W(-1), W(1) are supported on 2j1 B,
∣r(I)k1 ≤ 4ε{2jlkVXc-1)k1 + 2kXc-1)k1} .	(A.17)
Next,
X(2) := σ(X⑴ * D2W(2) + b(2))	(A.18)
=σ((X⑴ + r(1)) * D2W⑵ + b(2)) (by equation A.16)	(A.19)
=σ(X⑴ * 02W⑵ + b(2)) + r(1)0	(A.20)
14
Under review as a conference paper at ICLR 2020
where ∣∣r(1)0kι ≤ ∣∣r(1)∣∣ι and observe the same bound as equation A.17, since neither *w(2)
(Lemma 3(i)) nor applying σ with bias expands the 1-norm. Using the brief notation σl to denote
the non-linear mapping with biases b(l), consider
σ2(x(1) * D2w(2)) = σ2(σι(σ-ι(X(-1) * w(-1)) * w(1)) * D2w(2))
=σ2(σι(σ-ι(σ(X(-2) * D2w(-2) + b(-2)) * w(-1)) * W⑴)* D2w⑵)
=σ2(σι(σ-ι(σ(χC-2) * D2W(-2) + x0-2) * w(-2) + b(-2))
* w(-1)) * w(1)) * D2w(2)), (by equation A.9)
by Lemma 2, it equals (using the non-expansiveness of σ2 to take r(2) outside)
σ2(σ1 (σ-ι(σ(χC-2) * w(-2) + x0-2) * w(-2) + b(-2)) * w(-1)) * w(1)) * w(2)) + r(2)
=σ2(σι(σ-ι(σ(X(-2) * w(-2) + b(-2)) * w(-1)) * w(1)) * w(2)) + r(2)
:=x(2) + r(2)
where
kr⑵kι ≤ 4ε{2j2kVXC-2)kι + 2kχC-2)kι} .	(A.21)
Inserting back to equation A.20,
X⑵=X(2) + r(1)0 + r(2)
thus ∣∣X(2) 一 X(2) k 1 is bounded by the sum of equation A.17 and equation A.21.
Continue the process, X(l) denotes the l-th layer output in the source CNN (after l times correction
in the target CNN) by feeding X(T-I) from the (-l)-th layer, where X(T-I) is the output in the
(un-corrected) generative target CNN after the first (L 一 l) layers. By that X(-L) = x(-L) = h, and
that Ft = X(L), FS = x(L), repeating the argument L times gives that
L
∣Fs - Ftkι ≤ 4εX(2jl∣VXcT)∣ι + 2∣χC-l)∣ι),
l=1
and when (Id 一 ρl) are rigid motions, the 2nd term for each l vanishes.
We claim that
Claim 3. For l = -L, ∙∙∙ , -1, ∣VXcl)kι ≤ ∣Vh∣ι, and ∣Xcl)kι ≤ ∣h∣ι.
which suffices to prove the theorem.
Proof of Claim 3: No that in the bottom layer Xc-L) = X(-L) = h. For l = -L +1,…，—1,
IIXcl)IlI = IIX(I)- X0l)kι
=∣∣σl(X(IT) * w(l-1)) - σl(X0l) * w(l-1))∣ι
≤ ∣∣X(l-1) * Wy-I) - XOl) * w(l-1)kι (by that σl non-expansive)
≤ ∣∣X(l-1) - X0l)kι	(by that ∣w(lT)IlI ≤ 1 and Lemma 3(i))
=kXclτ)kι.
Recursing the inequality gives that ∣Xcl)∣∣ι ≤ Ilhkι∙ Similarly,
∣VXcl)kι = ∣VX(l)kι = TV[σl(X(l-1) * w(l-1))]
≤ TV[X(l^ -1) * Wt(l-1)]
(by that σl does not increase total variation)
= IlV(X(l-1) * W(lT))IlI
≤ 1~*°-1*] = IlVXclT) 11,	(by that ∣w(lT)IlI ≤ 1 and Lemma 3(ii))
and thus IIVXcOIi ≤ ∣∣Vh∣ι. This proves Claim3.	□
Lemma 2. Suppose filters W, fι, ∙∙∙ , fm, f satisfy that the 1-norm are all bounded by 1, and
W and f are supported on 2jB. The sequence of σl, denoting non-linear function with bias, for
l = 0,…,m are non-expansive. DT is a SPatial transform where T is odd and ∣Vτ∣∞ ≤ ε < 5.
Then
σm(…σ1(σ0(X * DTw) * fι)…* fm) * f
15
Under review as a conference paper at ICLR 2020
approximates
σm(…σ1(σ0(x * w) * fι)…* fm) * D-1f
up to an error whose 1-norm is bounded by
4ε {2j∣∣Vxkι + 2∣⅛},
and the second term vanishes if (Id - τ) is a rigid motion.
Proof of Lemma 2. The proof uses the same technique as in the proof of Lemma 1. Omitting sub-
script R2 in the integral, let
y1 (u)
yι(u)
∕σm(∕ …σι(∕σ°(∕x(u + Vi + …+ Vm + V - ρ-1(z))w(z)∣ Jρ-1 |dz)
f (-Vi)dVl) ∙∙∙ fm(-Vm)dVm)f (-V)dV,
∕σm(∕ …σι(∕σ°(∕x(u + Vi + …+ Vm + V — ρ-i(z))w(z)dz)
f (-Vi)dVi) .... fm(-Vm)dVm)f (-V)dV.
By Fact 1, that σj are all non-expansive and that the 1-norm of all the filters are bounded by 1,
/ |yi(u) - yι(u)∣du ≤ 4ε∣∣xkι.
Also,
y2(u)
y2(u)
Similarly,
/ σm(/ …σι(∕ σo(∕ x(u + vι +---+ Vm + ρ(v) - z)w(z)dz)
f(-Vl)dvi)…fm(-Vm)dVm)f (-V)| Jρ∣dv,
/ σm(/ …σι(∕ σo(∕ x(u + Vi +---+ Vm + P(V) - z)w(z)dz)
f (-vi)dvi)…fm(-Vm)dVm )f (-V)dV.
/ ∣y2(u) - y2(u)∣du ≤ 4ε∣∣xkι.
Same as before, with P being a rigid motion, kyi — yik and ∣∣y2 一 y2k are both zero.
It remains to bound ∣∣yι 一 y2 ∣i. Observe that
/ |yi (u) - y2(u)∣du ≤ /…/ dV∣f(-V)∣dVm∣f(-Vm)∣…dVi∣f (-Vi)∣dz∣w(z)∣
∕du∣x(u + Vi +------+ Vm + V - ρ-i(z)) - x(u + Vi +----+ Vm + P(V) - z)|,	(A.22)
and similarly as in proving Lemma 1, one can verify that for any fixed Vi,…，Vm,, v, z,
/ |x(u + Vi +----+ Vm + V - ρ-i(z)) - x(u + Vi +-----+ Vm + P(V) - z)∣du
≤ ∣Vx∣i VT∣∞∣V - ρ-i(z)∣ ≤ ε2(2j + 2j)∣∣Vx∣∣i.
Inserting back to equation A.22, and again by that the 1-norm of all the filters are bounded by 1, we
have that kyi - y2k1 ≤ 4ε2jkVx∣∣ι.
Lemma 3. Let x and w be smooth and compactly supported on R2, then
(i)	kx * wki ≤ kxki kwki .
(ii)	kV(x * w)ki ≤ kVxki kwki.
□
Proof of Lemma 3. For (i),
∣x * w∣i =	|	x(u - V)w(V)dV|du ≤
For (ii),	R2	R2
|x(u - V)||w(V)|dudV = ∣x∣i ∣w∣i .
∣V(x * w)∣i
R2
|Vu ( x(u -
R2
V)w(V)dV)|du
=	|	Vu x(u - V)w(V)dV|du
R2 R2
≤	|Vux(u - V)||w(V)|dudV
R2 R2
= ∣Vx∣i∣w∣i.
□
16