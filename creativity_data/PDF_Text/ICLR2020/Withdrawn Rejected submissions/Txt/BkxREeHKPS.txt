Under review as a conference paper at ICLR 2020
On the Parameterization of
Gaussian Mean Field Posteriors in
Bayesian Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Variational Bayesian Inference is a popular methodology for approximating poste-
rior distributions in Bayesian neural networks. Recent work developing this class
of methods has explored ever richer parameterizations of the approximate poste-
rior in the hope of improving performance. In contrast, here we share a curious
experimental finding that suggests instead restricting the variational distribution
to a more compact parameterization. For a variety of deep Bayesian neural net-
works trained using Gaussian mean-field variational inference, we find that the
posterior standard deviations consistently exhibit strong low-rank structure after
convergence. This means that by decomposing these variational parameters into a
low-rank factorization, we can make our variational approximation more compact
without decreasing the models’ performance. Furthermore, we find that such fac-
torized parameterizations improve the signal-to-noise ratio of stochastic gradient
estimates of the variational lower bound, resulting in faster convergence.
1	Introduction
Bayesian Neural Networks explicitly represent their parameter-uncertainty by forming a posterior
distribution over model parameters, instead of relying on a single point estimate for making predic-
tions, as is done in traditional deep learning. For neural network weights w, features x and labels
y, the posterior distribution p(w|x, y) is computed using Bayes’ rule, which multiplies the prior
distribution p(w) and data likelihood p(y|w, x) and renormalizes. When predicting with Bayesian
neural networks, we form an average over model predictions where each prediction is generated
using a set of parameters that is randomly sampled from the posterior distribution. Bayesian neural
networks are thus a type of ensembling, of which various types have proven highly effective in deep
learning (see e.g. Goodfellow et al., 2016, sec 7.11).
Besides offering improved predictive performance over single models, Bayesian ensembles are also
more robust because ensemble members will tend to make different predictions on hard examples
(Raftery et al., 2005). In addition, the diversity of the ensemble represents predictive uncertainty and
can be used for out-of-domain detection or other risk-sensitive applications (Ovadia et al., 2019).
Variational inference is a popular class of methods for approximating the posterior distribution
p(w|x, y), since the exact Bayes’ rule is often intractable to compute for models of practical in-
terest. This class of methods specifies a distribution qθ (w) of given parametric or functional form
as the posterior approximation, and optimizes the approximation by solving an optimization prob-
lem. In particular, we minimize the Kullback-Leibler (KL) divergence DKL between the variational
distribution qθ(w) and the true posterior distribution p(w|x, y), which is given by
Dkl[qθ(w)l∣p(w∣x, y)] = Eq log 丁θ(w)、= Eq log (、( qθ( ∣、.
p(w|x, y)	p(w)p(y|w, x)/p(y|x)
Here, we do not know the normalizing constant of the exact posterior p(y|x), but since this term
does not depend on w, we may ignore it for the purpose of optimizing our approximation q. We are
then left with what is called the negative Evidence Lower Bound (negative ELBO):
Lq = DKL [qθ (w)||p(w)] - Eq[p(y|w,x)].
1
Under review as a conference paper at ICLR 2020
In practice, the expectation of the likelihood p(y|w, x) with respect to q is usually not analytically
tractable and instead is estimated using Monte Carlo sampling:
1S
Eq[logp(y∣w,χ)] ≈ s ElOgp(y∣w(S),χ),	W(S)〜qe(w),
s=1
where the ELBO can then be optimized by differentiating this stochastic approximation with respect
to the variational parameters θ (Salimans et al., 2013; Kingma & Welling, 2013).
In Gaussian Mean Field Variational Infer-
ence (GMFVI), we choose the variational ap-
proximation to be a fully factorized Gaussian
distribution q = N(μq, Σq) with Wlij 〜
N(μlij, σlj), where l is a layer number, and
i and j are the row and column indices in
the layer’s weight matrix. Although this
type of approximation is considered to be
one of the simplest types of variational ap-
proximations, it already doubles the param-
eter count compared to deterministic neu-
ral networks. In addition, Bayesian neural
networks with mean-field Gaussian posterior
approximations often are harder to train than
deterministic neural networks because they
suffer from increased noise in stochastic gra-
dient estimates.
joint .
distribution
• K⅞y°
tied
parameters
Matrix-variate
Normal VB
• OUr method
hierarchical
VB
.normalizing
flows
flat
parameters
factorized
distribution
• Gaussian mean field
.structured
mean field
Figure 1: Approaches to variational Bayes on Bayesian
neural networks, ordered by i) whether they factorize the
variational distribution q, and ii) whether they tie the vari-
ational parameters.
Beyond mean-field variational inference, recent work on approximate Bayesian inference has ex-
plored ever richer parameterizations of the approximate posterior in the hope of improving the per-
formance of Bayesian neural networks (see Figure 1). In contrast, here we study a simpler, more
compactly parameterized variational approximation, which we show can also work well for a variety
of models. In particular we find that:
•	Converged posterior standard deviations under GMFVI consistently display strong low-
rank structure. This means that by decomposing these variational parameters into a low-
rank factorization, we can make our variational approximation more compact without de-
creasing our model’s performance.
•	Factorized parameterizations of posterior standard deviations are easier to train since they
improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower
bound. Using a more limited parameterization of the approximate posterior can thus lead to
faster convergence and a reduction in number of parameters compared to standard GMFVI.
2	Mean field posterior s tandard deviations naturally have
low-rank structure
In this section we show that the converged posterior standard deviations of Bayesian neural networks
trained using standard GMFVI consistently display strong low-rank structure. We also show that it
is possible to compress the learned posterior standard deviation matrix using a low-rank approxi-
mation without decreasing the network’s performance. We first briefly introduce the mathematical
notation for our GMFVI setting and the low-rank approximation that we explore. We then provide
experimental results that support the two main claims of this section.
To avoid any confusion among the readers, we would like to clarify that we use the terminology
“low-rank” in a particular context. While variational inference typically makes use of low-rank
decompositions to compactly represent the dense covariance of a Gaussian variational distribution
(see numerous references in Section 4), we investigate instead underlying low-rank structures within
the already diagonal covariance of a Gaussian fully-factorized variational distribution. We will
make this explanation more formal in the next section.
2
Under review as a conference paper at ICLR 2020
2.1 Methodology
To introduce the notation we consider layers that consist of a linear transformation followed by a
non-linearity f,
al = hlWl + bl,	hl+1 = f(al),	(1)
where Wl ∈ Rm×n, hl ∈ R1×m andbl, al, hl+1 ∈ R1×n. To simplify the notation in the following,
We drop the subscript l such that W = Wι, μq = μqi, Σq = Σqι and We focus on the kernel matrix
W for a single layer.
In GMFVI, (Blundell et al., 2015), We model the variational posterior as
mn
q(W) = N(μq, Σq)=YYq(wij),	with q(wij) = N (μij, σij),	⑵
i=1 j=1
where μq ∈ Rmn ×1 is the posterior mean vector, Σq ∈ Rmn ×mn is the diagonal posterior Co-
variance matrix. The Weights are then usually sampled using a reparametrization trick (Kingma &
Welling, 2013), i.e, for the s-th sample, We have
W(S) = μij + σije(s),	E 〜N(0, 1).	(3)
In practice, We often represent the posterior standard deviation parameters σij in the form of
a matrix A ∈ R+m×n. Note that We have the relationship Σq = diag(vec(A2)) Where the
elementWise-squared A is vectorized by stacking its columns, and then expanded as a diagonal
matrix into R+mn×mn .
In the sequel, We start by empirically studying the properties of the spectrum of matrices A post
training, While using standard Gaussian mean-field variational distributions. Interestingly, We ob-
serve that those matrices naturally exhibit a loW-rank structure (see Section 2.3 for the corresponding
experiments), i.e,
A≈ UVT	(4)
for some U ∈ Rm×k, V ∈ Rn×k and k a small value (e.g., 2 or 3). This observation motivates the
introduction of the folloWing variational family, Which We name k-tied Normal:
k-tied-N(W; μq, U, V) = N(μq, diag(Vec((UVT)2))),	(5)
Where the squaring of the matrix UVT is applied elementWise. Due to the tied parametrization of
the diagonal covariance matrix, We emphasize that this variational family is smaller—i.e., included
in—the standard Gaussian mean-field variational distribution family.
As formally discussed in Appendix B, the
matrix variate Gaussian distribution (Gupta
& Nagar, 2018), referred to as MN
and already used for variational inference
by Louizos & Welling (2016) and Sun et al.
(2017), is related to our k-tied Normal dis-
tribution With k = 1 When MN uses diag-
onal roW and column covariances. Interest-
ingly, We prove that for k ≥ 2, our k-tied
Normal distribution cannot be represented
Variational family	Parameters (total)
Multivariate Normal	i~mn (mn+1) mn H	k-2~-
diagonal Normal	mn + mn
MN	mn +汕产 + n(n户
MN (diagonal)	mn + m + n
k-tied Normal	mn + k(m + n)
Table 1: Number of variational parameters for a variational
family for a matrix W ∈ Rm×n. MN (diagonal) is from
Louizos & Welling (2016).
by any MN distribution.
This illustrates
the main difference of our approach from
the most closely related previous Work of Louizos & Welling (2016) (see also Figure 1). Further-
more, notice that diagonal covariance Σq repeatedly reuses the same elements of U and V, Which
results in parameter sharing across different Weights.
The total number of the standard deviation parameters in our method is k(m + n) from U and V,
compared to mn in the standard GMFVI parametrization from A. Given that in our experiments
the k is very loW (e.g. k = 2) this reduces the number of parameters from quadratic to linear in the
dimensions of the layer, see Table 1. More importantly, such parameter sharing across the Weights
leads to higher signal-to-noise ratio during training and thus faster convergence. We demonstrate this
phenomena in the next section. In the rest of this section, We Will first demonstrate that the standard
GMFVI methods already learn a loW-rank structure in the posterior standard deviation matrix A.
Furthermore, We Will shoW that replacing the full matrix A With its loW-rank approximation does
not reduce the predictive performance.
3
Under review as a conference paper at ICLR 2020
2.2	Experimental setting
Before providing the experimental results we briefly explain the key properties of the experimental
setting. We analyse three types of GMFVI Bayesian neural network models:
•	Multilayer Perceptron (MLP): a network of 3 dense layers and ReLu activations that we
train on the MNIST dataset (LeCun & Cortes, 2010). We use the last 10,000 examples of
the training set as a validation set.
•	Convolutional Neural Network (CNN): a LeNet architecture (LeCun et al., 1998) with 2
convolutional layers and 2 dense layers that we train on the CIFAR-100 dataset (Krizhevsky
et al., 2009b). We use the last 10,000 examples of the training set as a validation set.
•	Long Short-Term Memory (LSTM): a model that consists of an embedding and an LSTM
cell (Hochreiter & Schmidhuber, 1997), followed by a single unit dense layer. We train it
on an IMBD dataset (Maas et al., 2011), in which we use the last 5,000 examples of the
training set as a validation set.
•	Residual Convolutional Neural Network (ResNet): a ResNet-181 architecture (He et al.,
2016) trained on all 50,000 training examples of the CIFAR-10 dataset (Krizhevsky et al.,
2009a).
In each of the four models we use a mean-field Normal posterior and a Normal prior with a single
scalar standard deviation hyper-parameter for all the layers. We optimize the variational parameters
using an Adam optimizer (Kingma & Ba (2014)). For a more comprehensive explanation of the
experimental setup used in this section please refer to Appendix A.1. Finally, we highlight that
our experiments focus primarily on the comparison across a broad range of model types rather than
competing with the state-of-the-art results over the specifically used datasets. Therefore, we use
mainly small to medium models (MLP, CNN, LSTM) that are known to train well using the standard
GMFVI approach explored in this paper. Encouraged by the reviewers, we also briefly show that
our results can extend to larger models such as the ResNet-18 model. However, scaling GMFVI to
such larger model sizes is still a challenging research problem (Osawa et al., 2019).
2.3	Main experimental observation
Our main experimental observation is that the standard GMFVI learns posterior standard deviation
matrices that have a low-rank structure across different model types (MLP, CNN, LSTM, ResNet)
and layer types (dense and convolutional). To show this, we investigate the results of the SVD
decomposition of posterior standard deviation matrices for the four types of models trained until
ELBO convergence using GMFVI. While we evaluate the low-rank structure only for the dense
layers of the first three models (MLP, CNN and LSTM), we investigate also the low-rank structure
of the convolutional layers of the ResNet model.
Figure 2 shows the percentage of explained variance per singular value k of the SVD decomposition
of dense layers in the first three models. The percent of explained variance for the singular value
k is calculated as γk2/ Pi0 γi0, where γi0 are singular values. We observe that most of the variance
in the posterior standard deviation parameters is captured in the rank-1 approximation. However,
a more fine-grained analysis shows that a rank-2 approximation can encompass nearly all of the
remaining variance. Finally, we note that we do not observe the same behaviour for the posterior
mean parameters as we do for the posterior standard deviation parameters. Figure 9 in Appendix D
further supports this claim visually by comparing the heat maps of the full-rank posterior standard
deviations matrix with its rank-1 and rank-2 approximations. In particular, we observe that the rank-
2 approximation results in the heat-map looking visually very similar to the full-rank matrix. Finally,
Figure 3 illustrates that the low-rank structure is also visible in both the dense and convolutional
layers of the ResNet model. See Appendix C for more details. In the analysis of the experiments,
we use the shorthand SEM to refer to the standard error of the mean.
2.4	Low-rank approximation of variance matrices
1https://github.com/tensorflow/probability/blob/master/tensorflow
probability/examples/cifar10_bnn.py
4
Under review as a conference paper at ICLR 2020
τ-a-3γ-54
LoOooooo
a3u®-e> Psuo-dx9 jo4-lua3J9d
Figure 2: Explained variance per singular value from SVD of matrices of posterior means and posterior stan-
dard deviations for different layers of three types of models trained using standard GMFVI: MLP (left), CNN
(center), LSTM (right). Posterior standard deviations clearly display strong low-rank structure, with most of
the variance contained in the top few singular values, while this is not the case for posterior means.
MLP					CNN					LSTM			
Rank	-ELBO J	NLL J	Accuracy ↑	-ELBO J	NLL J	Accuracy ↑	-ELBO J	NLL J	Accuracy ↑
Full	0.431±0.0057	0.100±0.0034	97.6±0.15	3.83 0.020	2.23 0.017	42.1±0.49	0.536	058	0.493±0.0057	80.1±0.25
1	3.41±0.019	0.677±0.0040	93.6±0.25	4.33 0.021	2.30 0.016	41.7±0.49	0.687 ±0.0058	0.491±0.0056	80.0±0.25
2	0.456±0.0059	0.107±0.0033	97.6±0.15	3.88 0.020	2.24±0.017	42.2±0.49	0.621 ±0.0058	0.494±0.0057	80.1±0.25
3	0.450±0.0059	0.106±0.0033	97.6±0.15	3.86 0.020	2.24 0.017	42.1±0.49	0.595 ±0.0058	0.493±0.0056	80.1±0.25
ELBO and predictive performance, for three types of models. We report mean and SEM of each metric across
Table 2: Impact of low-rank approximation of the GMFVI-trained posterior standard deviation matrix on
100 models samples.
IO0
Motivated by the above observation, we show that it is
possible to replace the full-rank posterior standard devi-
ation matrix with its low-rank approximation without a
decrease in performance. Table 2 shows the comparison
of performance of models with different ranks of approxi-
mation to their posterior standard deviation matrix for the
MLP, CNN and LSTM models. Figure 3 contains analo-
gous results for the ResNet model. The results show that
the post-training approximation with ranks higher than 1
achieves predictive performance close to that of the full-
rank matrix for all the analyzed model and layer types.
This observation itself could be used as a form of post-
training network compression. Moreover, it gives rise to
further interesting exploration directions such as formu-
lating posteriors that exploit such a low rank structure.
In the next section we explore this particular direction
while focusing on the first three model types (MLP, CNN,
LSTM).
3 THE k-TIED
Normal Distribution: Exploiting
Low-Rank Parameter-Structure
in Mean Field Posteriors
♦
-1-j4 γ-5f
Oooooo
uelje> pu 一dxouxld
l*→ conv2d Stddevs~
*- * conv2d means
*→ conv2d_7 stddevs
*- * conv2<j_7 means
*—* dense stddevs
*- * dense means
W
Rank -ELBO J NLL J Accuracy ↑
Full	122.61±0.012-O.495±0.0080	835±0.37
1	12257±0.012-O658±0.0069	8T7±o.39
2	122.77±0.012	0.503±0.0080	83.2±0.37
3	122.67±0.012	0.501±0.0079	83.2±0.37
Figure 3: Unlike posterior means, the pos-
terior standard deviations of both dense
and convolutional layers in the ResNet
model trained using standard GMFVI dis-
play strong low-rank structure and can be
approximated without loss in predictive met-
rics. Top: Explained variance per singular
value of the matrices of converged posterior
means and standard deviations. Bottom: Im-
pact of post training low-rank approxima-
tion of the posterior standard deviation ma-
trices on model’s performance. We report
mean and SEM of each metric across 100
models samples.
In the previous section we have shown that it is possible to replace a full-rank matrix of posterior
standard deviations trained with GMFVI with this matrix’s low-rank approximation without de-
creasing the predictive performance. In this section we show that it is also possible to exploit this
observation during training time. We achieve this by exploiting our novel variational family, the
k-tied Normal distribution (see Section 2.1).
We show that using this distribution in the context of GMFVI in Bayesian neural networks allows
to reduce the number of network parameters and in some cases speed up model convergences while
maintaining the predictive performance of the standard parametrization of the GMFVI.
We start by recalling the definition of the k-tied Normal distribution:
k-tied-N(W; μq, U, V) = N(μq, diag(Vec((UVT)2)))
5
Under review as a conference paper at ICLR 2020
where the variational parameters are comprised of {μq, U, V}.
3.1	Experimental setting
We now introduce the experimental setting in which we evaluate the GMFVI variational posterior
parametrized by the k-tied Normal distribution. We assess the impact of the described posterior
in terms of predictive performance and reduction in the number of parameters for the same first
three model types (MLP, CNN, LSTM) and respective datasets (MNIST, CIFAR-100, IMDB) as
we used in the previous section. Additionally, we also analyse the impact of tying in the posterior
on the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound for the
CNN model as a representative example. Overall the experimental setup is very similar to the one
introduced in the previous section. Therefore, we highlight here only the key differences.
We apply the k-tied Normal variational posterior distribution to the layers which we analysed in the
previous section. More concretely, we use the k-tied Normal variational posterior for all the three
layers of the MLP model, the two dense layers of the CNN model and the LSTM cell’s kernel and
recurrent kernel. We initialize the parameters uik and vjk of the k-tied Normal distribution so that
after the outer-product operation the respective standard deviations σij have the same mean values
as we obtain in the standard GMFVI posterior parametrization. In the experiments for this section
we use KL annealing where we linearly scale-up the contribution of the KL term from a fraction
of its full value to its full contribution over the course of training. Finally, similarly to the previous
section, we picked the key hyper-parameters such as the KL annealing rate, a learning rate, a batch
size and a prior standard deviation based on the performance on respective validation sets. For more
information about the experimental details for this section please refer to Appendix A.2.
3.2	Experimental results
We first investigate the comparison of predictive performance of GMFVI Bayesian neural network
models trained using the k-tied Normal posterior distribution with the models trained using the
standard parametrization of the GMFVI. Figure 4 shows the results for the three model types on the
test splits of their respective datasets. We analyse variants of the k-tied Normal posterior distribution
with different levels of tying k. We observe that for k ≥ 2 the k-tied Normal posterior is able to
achieve the performance competitive with the standard GMFVI posterior parametrization, while
reducing the total number of model parameters. The benefits of using the k-tied Normal posterior
are the most visible for models where the dense layers with the k-tied Normal posterior constitute
for a significant portion of the total number of the model parameters (e.g. MLPs and CNNs with
dense layers for classification).
We further investigate the impact of the k-tied Normal posterior distribution on the signal-to-noise
ratio2 (SNR) of stochastic gradient estimates of the variational lower bound (ELBO). In particular,
we focus on the gradient SNR of the GMFVI posterior standard deviation parameters for which we
perform the tying. These parameters are either uik or vjk for the k-tied Normal posterior or σij for
the standard GMFVI parametrization, all optimized in their log forms for numerical stability. The
SNR results in the Figure 4 show a significant increase in the gradient SNR when using the k-tied
Normal posterior.
Consequently, we observe that the increase in the gradient SNR translates into faster convergence
of the negative ELBO objective in some of the analyzed models. Figure 5 shows the convergence
plots of validation negative ELBO for all the three model types. We observe that the impact of
the k-tied Normal posterior on convergence depends on the model type. As shown in Figure 4,
the impact on the MLP model is strong and consistent with the k-tied Normal posterior increasing
convergence speed compared to the standard GMFVI parameterization. For the LSTM model we
also observe a similar speed-up. However, for the CNN model the impact of the k-Normal posterior
on the ELBO convergence is much smaller. We hypothesize that this is due to the fact that we
use the k-tied Normal posterior for all the layers trained using GMFVI in the MLP and the LSTM
models, while in the CNN model we use the k-tied Normal posterior only for some of the GMFVI
2SNR for each gradient value is calculated as E[gb2]/Var[gb2], where gb is the gradient value for a single
parameter. The expectation E and variance V ar of the gradient values gb are calculated over a window of last
10 batches.
6
Under review as a conference paper at ICLR 2020
trained layers. More precisely, in the CNN model we use the k-tied Normal posterior only for the
two dense layers, while the two convolutional layers are trained using the standard parameterization
of the GMFVI. Finally, we point out that the k-tied Normal posterior does not increase the training
step time compared to the standard parameterization of the GMFVI. See Table 4 in Appendix D for
the support of this claim.
Model & Dataset	Rank k	-ELBO J	NLL J	Accuracy ↑	#Par. [k] J	Rank k	MNIST, M 1000	LP Dense 2, 5000	SNR at step 9000
MNIST, MLP	full	0.501	0.133:0.0040	96.8:0.18	957				
MNIST, MLP	1	0.539 :0.0063	0.155:0.0043	96.1:0.19	482	full	4.13	027	4.45:0.091	3.21:0.035
MNIST, MLP	2	0.520 :0.0063	0.129:0.0039	96.8:0.18	484	1	5840 190	158 8	5.3:0.20
MNIST, MLP	3	0.497 ±0≡0	0.120:0.0038	96.9:0.18	486	2 3	7500 240 7∩∩∩ γ1c	140:11 117 117:1.7	4.3:0.26 Λ J l^l .“I 4.1:0.20
CIFAR100, CNN	full	3.72 :0.018	2.16:0.016	43.9:0.50	4,405		/000 2/0		
CIFAR100, CNN	1	3.65 :0.017	2.12:0.015	45.5:0.50	2,262				
CIFAR100, CNN	2	3.76 :0.019	2.15:0.016	44.3:0.50	2,268		MNIST MTP -ETBO at Ster)		
CIFAR100, CNN	3	3.73 :0.018	2.13:0.016	44.3:0.50	2,273	Rank k	,	, -	at step 1000	5000	9000		
IMDB, LSTM	full	0.538 :0.0054	0.478:0.0052	79.5:0.26	2,823	full	42.16:0.070	26.52	15.39:0.016
IMDB, LSTM	1	0.592 :0.0041	0.512:0.0040	77.6:0.26	2,693	1	43.11:0.039	14.85	2.06:0.027
IMDB, LSTM	2	0.560 :0.0042	0.484:0.0041	78.2:0.26	2,694	2	42.74:0.090	13.97:0.023	1.82:0.017
IMDB, LSTM	3	0.550 :0.0051	0.491:0.0050	78.8:0.26	2,695	3	42.63:0.068	13.61:0.020	1.80:0.031
Figure 4: Left: impact of the k-tied Normal posterior on test ELBO, test predictive performance and number of
model parameters. Test performance is reported as a mean and SEM across 100 weights samples after training
each model for ≈300 epochs. Right top: mean gradient SNR in the Dense 2 layer of the MNIST MLP model
at increasing training steps for different ranks of tying k. We observe a similar increase in the SNR from tying
for the CNN and the LSTM models as for the MLP model shown here. We report mean and SEM across 3
training runs with different random seeds. Right bottom: Negative ELBO on the MNIST validation data set
at increasing training steps for different ranks of tying k. See also Figure 5, which shows negative ELBO
convergence plots for the all three models types.
Figure 5: Convergence of negative ELBO (lower is better) reported for validation dataset when training With
tied variational posterior standard deviations for MLP (left), CNN (center), and LSTM (right) with different
low-rank factorizations of the posterior standard deviation matrix. Full-rank is the standard parametrization of
the GMFVI.
4	Related work
The application of variational inference to neural networks dates back at least to Peterson (1987);
Hinton & Van Camp (1993). Many developments3 have followed those seminal research efforts,
in particular regarding (1) the expressiveness of the variational posterior distribution and (2) the
way the variational parameters themselves can be structured to lead to compact, easier-to-learn and
scalable formulations. We organize the discussion of this section around those two aspects, with a
specific focus on the Gaussian case.
Full Gaussian posterior. Because of their substantial memory and computational cost, Gaussian
variational distributions with full covariance matrices have been primarily applied to (generalized)
linear models and shallow neural networks (Jaakkola & Jordan, 1997; Barber & Bishop, 1998; Mar-
lin et al., 2011; Titsias & Ldzaro-Gredilla, 2014; Miller et al., 2017; Ong et al., 2018).
To represent the dense covariance matrix efficiently in terms of variational parameters, several
schemes have been proposed, including the sum of low-rank plus diagonal matrices (Barber &
Bishop, 1998; Seeger, 2000; Miller et al., 2017; Zhang et al., 2017; Ong et al., 2018), the Cholesky
decomposition (Challis & Barber, 2011) or by operating instead on the precision matrix (Tan &
Nott, 2018; Mishkin et al., 2018).
3We refer the interested readers to Zhang et al. (2018) for a recent review of variational inference.
7
Under review as a conference paper at ICLR 2020
Gaussian posterior with block-structured covariances. In the context of Bayesian neural net-
works, the layers represent a natural structure to be exploited by the covariance matrix. When assum-
ing independence across layers, the resulting covariance matrix exhibits a block-diagonal structure
that has been shown to be a well-performing simplification of the dense setting (Sun et al., 2017;
Zhang et al., 2017), with both memory and computational benefits.
Within each layer, the corresponding diagonal block of the covariance matrix can be represented by
a Kronecker product of two smaller matrices (Louizos & Welling, 2016; Sun et al., 2017), possibly
with a parametrization based on rotation matrices (Sun et al., 2017). Finally, using similar tech-
niques, Zhang et al. (2017) proposed to use a block tridiagonal structure that better approximates
the behavior of a dense covariance.
Fully factorized mean-field Gaussian posterior. A fully factorized Gaussian variational distri-
bution constitutes the simplest option for variational inference. The resulting covariance matrix
is diagonal and all underlying parameters are assumed to be independent. While the mean-field
assumption is known to have some limitations—e.g., underestimated variance of the posterior dis-
tribution (Turner & Sahani, 2011) and robustness issues (Giordano et al., 2018)—it leads to scalable
formulations, with already competitive performance, as for instance illustrated by the recent uncer-
tainty quantification benchmark of Ovadia et al. (2019).
Because of its simplicity and scalability, the fully-factorized Gaussian variational distribution has
been widely used for Bayesian neural networks (Graves, 2011; Ranganath et al., 2014; Blundell
et al., 2015; Hemgndez-Lobato & Adams, 2015; Zhang et al., 2017; Khan et al., 2018).
Our approach can be seen as an attempt to further reduce the number of parameters of the (already)
diagonal covariance matrix. Closest to our approach is the work of Louizos & Welling (2016).
Their matrix variate Gaussian distribution instantiated with the Kronecker product of the diagonal
row- and column-covariance matrices leads to a rank-1 tying of the posterior variances. In contrast,
we explore tying strategies beyond the rank-1 case, which we show to lead to better performance
(both in terms of ELBO and predictive metrics). Importantly, we further prove that tying strategies
with a rank greater than one cannot be represented in a matrix variate Gaussian distribution, thus
clearly departing from (Louizos & Welling, 2016) (see Appendix B for details).
Our approach can be also interpreted as a particular case of hierarchical variational inference (Ran-
ganath et al., 2016) where the prior on the variational parameters corresponds to a Dirac distribution,
non-zero only when a pre-specified low-rank tying relationship holds.
We close this related work section by mentioning the existence of other strategies to produce more
flexible approximate posteriors, e.g., normalizing flows (Rezende & Mohamed, 2015) and exten-
sions thereof (Louizos & Welling, 2017).
5	Conclusion
In this work we have shown that Bayesian Neural Networks trained with standard Gaussian Mean-
Field Variational Inference learn posterior standard deviation matrices that can be approximated with
little information loss by low-rank SVD decompositions. This suggests that richer parameterizations
of the variational posterior may not always be better, and that compact parameterizations can also
work well. We used this insight to propose a simple, yet effective variational posterior parametriza-
tion, which speeds up training and reduces the number of variational parameters without degrading
predictive performance on 3 different model types.
In future work, we hope to scale up variational inference with compactly parameterized approx-
imate posteriors to much larger models and more complex problems. For mean-field variational
inference to work well in that setting several challenges will likely need to be addressed (Osawa
et al., 2019); improving the signal-to-noise ratio of ELBO gradients using our compact variational
parameterizations may provide a piece of the puzzle.
8
Under review as a conference paper at ICLR 2020
References
David Barber and Christopher M Bishop. Ensemble learning for multi-layer networks. In Advances
in neural information processing systems, pp. 395-401, 1998.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Edward Challis and David Barber. Concave gaussian variational approximations for inference in
large-scale bayesian linear models. In Proceedings of the Fourteenth International Conference on
Artificial Intelligence and Statistics, pp. 199-207, 2011.
Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorflow distributions. arXiv
preprint arXiv:1711.10604, 2017.
Ryan Giordano, Tamara Broderick, and Michael I Jordan. Covariances, robustness and variational
bayes. The Journal of Machine Learning Research, 19(1):1981-2029, 2018.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Alex Graves. Practical variational inference for neural networks. In Advances in neural information
processing systems, pp. 2348-2356, 2011.
Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Jose MigUel Hemdndez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International Conference on Machine Learning, pp. 1861-
1869, 2015.
Geoffrey Hinton and Drew Van Camp. Keeping neUral networks simple by minimizing the descrip-
tion length of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning
Theory. Citeseer, 1993.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Tommi Jaakkola and Michael Jordan. A variational approach to bayesian logistic regression models
and their extensions. In Sixth International Workshop on Artificial Intelligence and Statistics,
volume 82, pp. 4, 1997.
Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivas-
tava. Fast and scalable bayesian deep learning by weight-perturbation in adam. arXiv preprint
arXiv:1806.04854, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009a.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009b.
9
Under review as a conference paper at ICLR 2020
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings ofthe IEEE, 86(11):2278-2324,1998.
Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, pp. 1708-1716, 2016.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 2218-2227. JMLR. org, 2017.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies-volume 1, pp. 142-150.
Association for Computational Linguistics, 2011.
Benjamin M Marlin, Mohammad Emtiyaz Khan, and Kevin P Murphy. Piecewise bounds for esti-
mating bernoulli-logistic latent gaussian models. In Proceedings of the International Conference
on Machine Learning, pp. 633-640, 2011.
Andrew C Miller, Nicholas J Foti, and Ryan P Adams. Variational boosting: Iteratively refining pos-
terior approximations. In Proceedings of the 34th International Conference on Machine Learning,
pp. 2420-2429. JMLR. org, 2017.
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and Mohammad Emtiyaz Khan.
Slang: Fast structured covariance approximations for bayesian deep learning with natural gradi-
ent. In Advances in Neural Information Processing Systems, pp. 6245-6255, 2018.
Victor M-H Ong, David J Nott, and Michael S Smith. Gaussian variational approximation with a
factor covariance structure. Journal of Computational and Graphical Statistics, 27(3):465-478,
2018.
Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E Turner, Rio
Yokota, and Mohammad Emtiyaz Khan. Practical deep learning with bayesian principles. arXiv
preprint arXiv:1906.02506, 2019.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V Dil-
lon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? eval-
uating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.
Carsten Peterson. A mean field theory learning algorithm for neural networks. Complex systems, 1:
995-1019, 1987.
Adrian E Raftery, Tilmann Gneiting, Fadoua Balabdaoui, and Michael Polakowski. Using bayesian
model averaging to calibrate forecast ensembles. Monthly weather review, 133(5):1155-1174,
2005.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
Intelligence and Statistics, pp. 814-822, 2014.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International
Conference on Machine Learning, pp. 324-333, 2016.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538, 2015.
Tim Salimans, David A Knowles, et al. Fixed-form variational posterior approximation through
stochastic linear regression. Bayesian Analysis, 8(4):837-882, 2013.
Matthias Seeger. Bayesian model selection for support vector machines, gaussian processes and
other kernel classifiers. In Advances in neural information processing systems, pp. 603-609,
2000.
10
Under review as a conference paper at ICLR 2020
Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty in
bayesian neural networks. In Artificial Intelligence and Statistics, pp. 1283-1292, 2017.
Linda SL Tan and David J Nott. Gaussian variational approximation with sparse precision matrices.
Statistics and Computing, 28(2):259-275, 2018.
Michalis Titsias and MigUeI Ldzaro-Gredilla. Doubly stochastic variational bayes for non-conjugate
inference. In International conference on machine learning, pp. 1971-1979, 2014.
Richard Turner and Maneesh Sahani. Two problems with variational expectation maximisation for
time-series models, pp. 109-130. Cambridge University Press, 2011.
Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in variational
inference. IEEE transactions on pattern analysis and machine intelligence, 2018.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as
variational inference. arXiv preprint arXiv:1712.02390, 2017.
A Additional Details
A.1 Experimental details for section 2
We provide here some of the experimental details that we skipped over in the main text of the Section
2 of the paper.
More architecture details:
•	The 3 layers of the MLP have number of units respectivelly 400, 400 and 10.
•	The 2 convolutional layers of the CNN have filters of sizes 32 and 64 and 2 dense layers
with the number of units 512 and 100.
•	The embedding and LSTM cell in the LSTM model are both of size 128.
•	In the MLPs and the CNN models we train both the kernel and bias weights using GMFVI.
In the LSTM model we train only the kernel weights using GMFVI and we trained the bias
weights using MAP estimation.
•	It is possible that the SVD decomposition of posterior standard deviation matrix will result
in a low-rank approximation that contains negative values. In such cases, we threshold the
minimum values of the resulting approximation at very low positive constant.
Details of the GMFVI optimisation:
•	In the MLPs and the CNN models we train both the kernel and bias weights using GMFVI.
In the LSTM model we train only the kernel weights using GMFVI and we trained the bias
weights using MAP estimation.
•	For each of the three models the GMFVI uses the standard reparametrization trick (Kingma
& Welling, 2013).
•	It is possible that the SVD decomposition of posterior standard deviation matrix will result
in a low-rank approximation that contains negative values. In such cases, we threshold the
minimum values of the resulting approximation at very low positive constant.
•	We initialize the variational posterior means using the standard He initialization (He et al.,
2015) and the posterior standard deviations using samples from N (0.01, 0.001).
•	We use a Normal prior N (0, σp) for all the weights and select the σp for each of the models
separately from a set of [0.2, 0.3] based on validation data perforfmance.
•	For optimization we used an Adam optimizer (Kingma & Ba (2014)), for which we picked
the optimal learning rate for each model from the set of [0.0001, 0.0003, 0.001, 0.003]
based on validation set performance.
Additional details:
11
Under review as a conference paper at ICLR 2020
•	It is possible that the SVD decomposition of posterior standard deviation matrix will result
in a low-rank approximation that contains negative values. In such cases, we threshold the
minimum values of the resulting approximation at very low positive constant.
A.2 Experimental details for section 3
We provide here some of the experimental details that we skipped over in the main text of the Section
3 of the paper:
•	For the k-tied Normal posteriors to work reliably we used KL annealing with a linearly
increasing scaling of the KL term during training. We select the best linear coefficient of
the KL annealing from 5 × 10-5 and 5 × 10-6 per batch and update it’s value every 100
batches.
•	We initalize the parameters uik and vjk so that after the outer-product operation the respec-
tive σij standard deviations have means at 0.01 before transforming to log-domain. That
means that in the log domain the parameters uik and vjk are initialized as 0.5(log(0.01) -
log(k)). We also add white noise N(0, 0.1) to the values of uik and vjk in the log domain
to break symmetry.
B	Proof of the Matrix Variate Normal Parameterization
In this section of the appendix, we formally explain the connections between the k-tied Normal
distribution and the matrix variate Gaussian distribution (Gupta & Nagar, 2018), referred to as MN .
Consider positive definite matrices Q ∈ Rr×r and P ∈ Rc×c and some arbitrary matrix M ∈ Rr×c .
We have by definition that W ∈ Rr×c 〜MN(M, Q, P) if and only if vec(W)〜N (Vec(M), PX
Q), where vec(∙) stacks the columns of a matrix and 0 is the Kronecker product
The MN has already been used for variational inference by Louizos & Welling (2016) and Sun
et al. (2017). In particular, Louizos & Welling (2016) consider the case where both P and Q are
restricted to be diagonal matrices. In that case, the resulting distribution corresponds to our k-tied
Normal distribution with k = 1 since
P 0 Q = diag(p) 0 diag(q) = diag(vec(qp>)).
Importantly, we prove below that, in the case where k ≥ 2, the k-tied Normal distribution cannot be
represented as a matrix variate Gauussian distribution.
Lemma B.1 (Rank-2 matrix and Kronecker product). Let B be a rank-2 matrix in Rr+×c. There do
not exist matrices Q ∈ Rr×r and P ∈ Rc×c such that
diag(vec(B)) = P 0 Q.
Proof. Let us introduce the shorthand D = diag(vec(B)). By construction, D is diagonal and has
its diagonal terms strictly positive (it is assumed that B ∈ Rr+×c, i.e., bij > 0 for all i, j).
We proceed by contradiction. Assume there exist Q ∈ Rr×r and P ∈ Rc×c such that D = P 0 Q.
This implies that all diagonal blocks of P 0 Q are themselves diagonal with strictly positive diagonal
terms. Thus, pjjQ is diagonal for all j ∈ {1, . . . , c}, which implies in turn that Q is diagonal, with
non-zero diagonal terms and pjj 6= 0. Moreover, since the off-diagonal blocks pij Q for i 6= j must
be zero and Q 6= 0, we have pij = 0 and P is also diagonal.
To summarize, if there exist Q ∈ Rr×r and P ∈ Rc×c such that D = P 0 Q, then it holds that
D = diag(p) 0 diag(q) with p ∈ Rc and q ∈ Rr. This last equality can be rewritten as bij = pjqi
for all i ∈ {1, . . . , r} andj ∈ {1, . . . , c}, or equivalently
B = qp>.
This leads to a contradiction since qp> has rank one while B is assumed to have rank two. □
Figure 6 provides an illustration of the difference between the k-tied Normal and the MN distribu-
tion.
12
Under review as a conference paper at ICLR 2020
Figure 6: Illustration of the difference between the k-tied Normal (green), the MN distribution (red), the
Gaussian mean field (blue) and the full Gaussian covariance (black) for a matrix of posterior standard devia-
tions for a layer of size m × n. k-tied Normal with k = 1 is equivalent to MN with diagonal row and column
covariance matrices (half-red, half-green circle). Our experiments show that the k = 1 fails to capture the per-
formance of the mean field. On the other hand, while the full/non-diagonal MN increases the expressiveness
of the posterior, it also increases the number of parameters. In contrast, k-tied Normal with k ≥ 2 not only
decreases the number of parameters, but also matches the predictive performance of the mean field.
-≡UUON p∙≡■士
*Mu-Ba.!。Ul
C Low-rank structure in posterior s tandard deviations of
CONVOLUTIONAL LAYERS
Encouraged by the reviewers, we analyze the applicability of our method to different layer types
and larger models. In particular, we investigate the low-rank structure of the posterior standard
deviations of both the dense layers and the convolutional layers in a ResNet-18 (He et al., 2016)
GMFVI Bayesian neural network trained on the CIFAR-10 dataset (Krizhevsky et al., 2009a). More
precisely, we use a publicly available model4 from the example repository of Tensorflow Probabil-
ity (Dillon et al., 2017). To perform the low-rank decomposition of the convolutional layers, we
flatten all the dimensions of the convolutional layers except the last dimension (e.g., a layer with
shape [3,3,512,512] is reshaped to [3 ∙ 3 ∙ 512,512]). See Figure 7 for the examples of the resulting
2-dimensional matrices. We then analyze the SVD spectrum of the posterior means and standard
deviations of such flattened convolutional layers. Interestingly, we observe a similar low-rank struc-
ture in the flattened convolutional layers as observed in the dense layers, see Figure 8. The low-rank
structure is the most visible for the last convolutional layer, which also contain the highest number
of parameters.
Importantly, note that after performing the low-rank approximation in the 2-dimensional space, we
can reshape the materialized 2-dimensional matrices back into the 4-dimensional form of a convo-
lutional layer. Table 3 shows that such a low-rank approximation in the convolutional layers of the
analyzed ResNet-18 model can be performed without a loss in the model’s predictive performance,
while halving the total number of parameters. These results suggest that our method can be applica-
ble not only to the different model types (MLP, CNN and LSTM), but also to a range of layer types
(dense and convolutional) in large and practically applicable models such as the ResNet-18 model.
Rank	-ELBO C	NLL C	Accuracy ↑	#Params C	%Params C
Full	122.61±0.012	0.495±0.0080	835±o.37	9,814,026	100.0
1	122.57±0.012	0.658±0.0069	81.7±0.39	4,929,711	50.2
2	122.77±0.012	0.503±0.0080	83.2±0.37	4,946,964	50.4
3	122.67±0.012	0.501±0.0079	83.2±0.37	4,964,217	50.6
Table 3: Impact of the low-rank approximation of the GMFVI-trained posterior standard deviations of a
ResNet-18 model on the model’s ELBO and predictive performance. We report mean and SEM of each metric
across 100 models samples.
4https://github.com/tensorflow/probability/blob/master/tensorflow
probability/examples/cifar10_bnn.py
13
Under review as a conference paper at ICLR 2020
警零 iH≡≡≡≡≡≡iR
，≡≡3a39≡粤=
-7^1852963074185296307
25<81013161821M2729323537404345超515456
fia≡≡≡≡
conv2d_flipout_7
H-H=¥,
-=≡-≡.
Re-Hi:译 Hm-IIEI≡
一 isi =≡w≡菱星i≡
NMI≡-=>=≡-i⅞≡=■运
COnV2 d_flipout_12
-0.195
-0.190
-0.185
-OlBO
-0.175

Figure 7:	Heat maps of the partially flattened posterior standard deviation tensors for the selected convolutional
layers of the ResNet-18 GMFVI BNN trained on CIFAR-10. The partially flattened posterior standard deviation
tensors of the convolutional layers display similar low-rank patterns that we observe for the dense layers.
conv2d flipout 2
8uw">su-"_dxeJ0-ue3J£
coπv2d flipout
COnV2d flipout 7
COnV2d flipout 12
2	4	6	8	10	2	4	6	8	10	2	4	6	8	10	2	4	6	8	10
Rank	Rank	Rank	Rank
Figure 8:	Explained variance per singular value from SVD of partially flattened tensors of posterior means
and posterior standard deviations for different convolutional layers of the ResNet-18 GMFVI BNN trained on
CIFAR-10. Posterior standard deviations clearly display strong low-rank structure, with most of the variance
contained in the top few singular values, while this is not the case for posterior means.
D Additional experimental results
Figure 9: Heat map of the posterior standard deviation matrix for the weights in the first dense layer of a LeNet
CNN trained using GMFVI on the CIFAR-100 dataset (left), as well as its rank-1 approximation (middle)
and rank-2 approximation (right). The rank-2 approximation looks visually similar to the full-rank matrix,
confirming our numerical results from Figure 2.
Training method	Train step time [ms] J
Point estimate	2.00 ±0.0064
Standard GMFVI	7.17±0.014
K-tied GMFVI	6.14±0.018
Table 4: Training step evaluation times for a simple model architecture with two dense layers5 for different
training methods. We report mean and SEM across a single training run. The k-tied Normal posterior does
not increase the train step evaluation times compared to the standard parameterization of the GMFVI posterior.
We expect this to hold more generally because the biggest additional operation per step when using the k-
tied Normal posterior is the UVT multiplication to materialize the matrix of posterior standard deviations A,
where U ∈ Rm×k , V ∈ Rn×k and k is a small value (e.g., 2 or 3). The time complexity of this operations
is O(kmn), which is usually negligible compared to the time complexity of data-weight matrix multiplication
O(bmn), where b is the batch size.
5E.g., as used in https://github.com/tensorflow/docs/blob/master/site/
en/tutorials/keras/classification.ipynb.	Our GMFVI implementation of this
model is available under https://colab.research.google.com/drive/14pqe_
VG5s49xlcXB-Jf8S9GoTFyjv4OF
14