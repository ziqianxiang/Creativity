Under review as a conference paper at ICLR 2020
ADAMT: A Stochastic	Optimization with
Trend Correction Scheme
Anonymous authors
Paper under double-blind review
Ab stract
Adam-typed optimizers, as a class of adaptive moment estimation methods with
the exponential moving average scheme, have been successfully used in many
applications of deep learning. Such methods are appealing for capability on large-
scale sparse datasets. On top of that, they are computationally efficient and insen-
sitive to the hyper-parameter settings. In this paper, we present a new framework
for adapting Adam-typed methods, namely AdamT. Instead of applying a simple
exponential weighted average, AdamT also includes the trend information when
updating the parameters with the adaptive step size and gradients. The newly
added term is expected to efficiently capture the non-horizontal moving patterns
on the cost surface, and thus converge more rapidly. We show empirically the
importance of the trend component, where AdamT outperforms the conventional
Adam method constantly in both convex and non-convex settings.
1	Introduction
Employing first order optimization methods, such as stochastic gradient descent (SGD), is a key of
solving large-scale problems. The classic gradient descent algorithm is widely used to update the
model parameters, denoted by x,
xt+1 = Xt-nVf (Xt),	(1)
where the gradient is denoted by Vf (Xt) and the step size by η. While the method has shown its
efficiency for many contemporary tasks, the adaptive variants of SGD outperform the vanilla SGD
methods on their rapid training time. Specifically, the step size η is substituted by an adaptive step
size η/√vt, and Vt is generated from the squared gradient [Vf (Xt)]2.
Several variants of the popular adaptive optimizers can be summarized into such common format.
These optimizers share gradients calculation and parameters updating functions, but specify dif-
ferent moving average schemes for calculating the parameter-wise adaptive learning rate Vt . For
example, AdaGrad (Duchi et al., 2011) takes the arithmetic average of historical squared gradients
[Vf(Xt)]2. Compared with the conventional momentum method, it adapts the learning rate to each
parameter to suit the sparse data structure, and thus gains a rapid convergence speed (Ruder, 2016).
Later, Tieleman & Hinton (2012) proposed RMSProp to reduce the aggressiveness of the decay rate
in AdaGrad. The method modifies Vt to the exponentially decayed squared gradients. Similar im-
plementations could also be found in ADADELTA (Zeiler, 2012). Instead of the squared gradients,
the method applies squared parameter updates to define the adaptive learning rate. As a result, each
update guarantees the same hypothetical units as the parameter. Later, Adam (Kingma & Ba, 2015)
modifies RMSProp with the idea from momentum methods (Qian, 1999). Except for the second
moment moving average, the new rule also replaces the gradient Vf(Xt) at the end of the Equa-
tion (1) to the first-moment estimation. The method has practically shown its superiority regarding
the converge speed and memory requirement. While the aforementioned methods are the most fa-
mous frameworks, there are also many variants for each of them. The examples include NAdam
(Dozat, 2016), AMSGrad (Reddi et al., 2018) and Adafom (Chen et al., 2019).
So far, the adaptive methods with exponential moving average gradients have gained great attention
with huge success in many deep learning tasks. However, it remains unsolved whether the simple
exponential smoothing results or the level information is sufficient in capturing the landscape of
the cost surface. When clear upward or downward pattern could be recognized within the moving
routine, it is suggested to add a trend term on top of the single level information.
1
Under review as a conference paper at ICLR 2020
In this paper, we modify the Adam rule with trend-corrected exponential smoothing schemes,
namely AdamT, to obtain the local minima with a faster speed. To the best of our knowledge, our
research is the first to apply the trend-corrected features on gradients scaling and parameters updat-
ing. It shall be emphasized that our framework is universally implementable for all adaptive update
methods that apply the exponential average term, including but not restricted to ADADELTA, RM-
SProp, AdaMAX and other well-recognized methods. For the sake of conciseness, in this specific
paper, we focus on Adam regarding rule modification and performance comparison.
Our contributions in this paper could be summarized in three-fold:
1.	We propose the notion of trend corrected exponential smoothing to modify the conven-
tional application of exponential moving average in optimizers with adaptive gradients.
Our AdamT method collaborates the trend information into the update rule of Adam.
2.	We show the conditions for the method to converge in convex settings. The regret bound is
in consistent to Adam at O(√T).
3.	We demonstrate AdamT’s convergence in both convex and non-convex settings. The per-
formance is compared with Adam, where AdamT shows clear superiority on both the train-
ing set and the test set, especially for non-convex problems.
For the remainder of the paper, we present the fundamental idea of Adam and Holt’s linear methods
in Section 2. In Section 3 and 4, we detail the update rules and experimental analysis, respectively.
In addition, Section 5 reviews recent developments of Adam-typed optimizers. While many of them
focus more on non-convex optimizations, there is a potential to incorporate our methods with such
frameworks and this extension is expected for future settings.
2	Preliminary
2.1	Adaptive Methods with Exponential Moving Averages
For adaptive gradient methods, we apply the update rule at step t + 1
η
xt+1 = Xt-----7=----m t.
Jvt + E
where mt is the gradient updates, and conventionally it is defined as the last gradient value Vf (xt).
To prevent zero division, a smoothing term E is included on the denominator.
In this paper, we focus our analysis and modifications on Adam. The method was initially proposed
by Kingma & Ba (2015), and quickly becomes one of the most popular optimizers in the last few
years. The adaptive step size is accelerated from the previous square gradients
vt = αvt-1 + (1 - α)[Vf(xt)]2.	(2)
In terms of the gradient mt, Adam takes the exponentially weighted average of all previous gradients
instead of solely relying on the last gradient Vft
mt = β1 mt-1 + (1 - β1) Vf(xt).	(3)
While the two moment estimates from Equations (2) & (3) could potentially counteract towards
zero, the series mt and Vt are considered for bias-correction. Formally, the rules are defined as:
m t
mt
1-β;
Vt
1-W.
2.2	Trend Corrected Exponential Smoothing
The idea of extracting a smoothed new point from all the previous information is called the exponen-
tial weighted moving average. Holt (2004) extended the method by including the trend behaviours
within the series, namely trend corrected exponential smoothing or Holt’s linear method. Consider
a time series {yt} for t = 1, 2, . Our target is to find the smoothing results at step t. We denote the
2
Under review as a conference paper at ICLR 2020
smoothing result as y\t+1|t. Holt’s linear method formulates the conditional forecasting by summing
two smoothing equations up
y\t+1|t = `t + bt
't = β('t-ι + bt-ι) + (I - β)yt
bt = γbt-i + (1 - γ)('t - 't-i).
For a new estimation, we first update the level term `t with the weighted average of the last obser-
vation yt and its estimation y\t|t-1. The trend term bt is updated simultaneously as the weighted
average of the estimated trend `t - `t-1 and its previous estimation bt-1. The smoothing parameters
for the level and the trend are denoted as α and β . Both values could be selected between 0 and 1.
Gardner Jr & McKenzie (1985) also suggested adding a damping factor φ, so that
y\t+1|t = `t + φbt
't = β('t-ι + φbt-ι) + (I - β)yt
bt = γφbt-i + (1 - Y)('t - 't-i).
The damped method is identical to Holt’s Linear method with φ = 1, and is the same as simple
exponential moving average method with φ = 0. When φ is positive, the parameter could be used
to control the significance of the trend component.
The damped trend methods are considerably popular for forecasting tasks (Hyndman & Athana-
sopoulos, 2018). Such methods inherent both level and trend information from historical series,
while stay flexible enough to adjust the influence of the trend term via φ. On top of that, involving
the damped factor could to some extend reduce the volatility of the smoothed line.
3	Methodology
We introduce our proposed algorithm AdamT, which is based on Adam (Kingma & Ba, 2015) with
added Holt’s linear trend information for both of the first moment estimate and the second raw
moment estimate. Specifically, we use trend-corrected exponential weighted moving averages in the
final parameter update step instead of the level-only estimates used in Adam.
3.1	Algorithm
Consider the gradient ofa stochastic objective function f(x) evaluated atT iterations as a time series
{Vf (xt)} for t = 1,2,...,T. According to the Holt,s linear trend method illustrated in Section 2.2,
We write two series {'m} and {bm} as the exponential weighted moving averages which estimate
the level and trend information of the first moment Vf(xt):
`tm = β1mt-1 + (1 -β1)Vf(xt)
bm = Y1Φ1bm-1 + (1 - γι)('m - '之ι)
mt = `tm + φ1btm
where β1 , γ1 and φ1 have the same functionality as explained in Section 2.2 and these are regarded
as hyperparameters in our algorithm. Equation (3.1) combines the level and the trend information of
first moment, which will be used for calculating the final update rule and the trend-corrected level
estimates. The procedures for the second raw moment Vf(xt) ◦ Vf(xt) is analogous:
`tv = β2vt-1 + (1 -β2)Vf(xt) ◦Vf(xt)
bv = Y2Φ2bV-i + (1 - Y2)('V - 'V-i)
vt = `tv + φ2btv
where the operation “◦” denotes an element-wise multiplication. The hyperparameters β2 , γ2 and
φ2 here share the same corresponding meanings as before. The moving averages {'V} and {bV}
estimate the level and trend of the second raw moment respectively. The term vt combines these
two information, which will be used in the calculations of final update rule and trend-corrected level
estimates of the second raw moment.
3
Under review as a conference paper at ICLR 2020
In our algorithm, We set the initial values of the series {'m}, {'V}, {bm} and {bV } to be zero vectors,
so that `m = 'V = bm = b0 = 0. The series {mt} and {vt}, as a result, are also initialized as zero
vectors. As observed in Kingma & Ba (2015), the exponential Weighted moving averages could
bias toWards zero, especially during the early training stage. We perform the bias correction for the
two level estimates {'}} and {'V} by following Kingma & Ba (2015). For the two trend estimates
{btm} and {btv}, We correct the bias in a different Way by taking into account the effect of damping
parameters (φ1, φ2). Thus, the bias-corrected version of the series {mt} and {vt} can be written as:
`m	+	(i - γ1Φ1)bm
1 - βt (I- YI)(I-(YIφIy)
`v	+	(1 - Y2φ2)bv
1 - βt	(I- Y2)(I-(Y2。2内
The justification for the two bias-corrected trend estimates is provided in Appendix A. Similar to
Adam, we consider the adaptive update rule with the bias-corrected first moment estimate and the
second raw moment estimate:
η
+1=	p≡n t
(4)
where is a positive tiny number added in the denominator to avoid zero-division case. Please
note that the series {mt} and {Vt} in AdamT are different from that of Adam. The two series are
trend-corrected (also bias-corrected) estimates of both moments.
The direction of the effective step ∆t = η ∙ mt/∖∕[V^t∖ (with E = 0) in the parameter space depends
on thejoint effect of the first moment level and trend estimates. In the update rule (4), we only care
about the magnitude of Vt by taking the absolute value and thus the ratio mt/∖∕∖V^t∖ can be seen as a
signal-to-noise ratio. Note that the effective step ∆t in our algorithm is also invariant to the scale of
the gradients. Specifically, re-scaling the gradients Vf (Xt) with a factor C will scale `m and bm by a
factor c, and will scale 'V and b by a factor c2. This results in scaling mt and Vt by a factor C and c2
respectively, and finally cancel out in the parameter update rule (c ∙ mt)/(，\c2 ∙ Vt∖) = mt/∖∕∖V^t∖.
Note that our proposed method AdamT has two extra computational steps, that is Equations (3.1) &
(3.1). However, the computational complexity of these two steps is almost linear in time. Therefore,
we can conclude that AdamT yields a superior performance compared with Adam (the results will
be shown in the experiment section) with a minimal additional computational cost.
In our algorithm, we set the hyperparameters β1, Y1, β2, Y2 according to the suggestion in Kingma &
Ba (2015). The smoothing parameters for the first moment estimates are set to 0.9, that is β1 = Y1 =
0.9, while the smoothing parameters for the second raw moment estimates are set to 0.999, that is
β2 = Y2 = 0.999. We empirically find that the good default values of the two damping parameters
can be set to φ1 = φ2 = 0.5. The pseudo-code of our AdamT is provided in Algorithm 1.
3.2 Regret Bound
+
We present the key results of convergence analysis for AdamT in this section and the details is
presented in Appendix B.
Theorem 3.1. Assume that the objective function ft has bounded gradients, that is kVft(x)k2 ≤ G,
kVft(x)k∞ ≤ G∞ for all x ∈ Rd. Assume that the distance between any xt produced by AdamT
is bounded, that is kxn - xm k2 ≤ D, kxm - xnk∞ ≤ D∞ for any m, n ∈ {1, 2, . . . , T}, and
β1, Y1, β2, Y2 ∈ [0, 1), φ1, φ2 ∈ [0, 1]. AdamT achieves the following guarantee for all T ≥ 1
D2	二,---------1	C C
R(T) ≤ 2-7Γ⅛ E PTvTi + 2[βι(1 - βι)(1 + φι(1 - Y1))2]G∞
2η(1 - β1) i=1	2
________D∞2(I - γ1φI)3	+	η (2 - βI)2	XX k k + dD2	(I - YI)β1	+ 1 - γ1φ1
(1 - βι)((1 - γι)(1 - (γ1φ1)τ))2_____________________________________________________2(1- βι) √-W 乙⑼皿2	2 2(1 - γι)(1 -员)
This result implies that AdamT has O(√T) regret bound given that Pd=I ∣∣g±τ,i∣∣2 ≤ dG∞√T and
Pd= 1 PTvTJ ≤ dCG∞ Tf for some positive constant C. Hence, we can prove that the average
regret of AdamT converges,
4
Under review as a conference paper at ICLR 2020
Algorithm 1 AdamT. The Adam optimizer modified with Holt’s Linear Trend method. Empirically
suggested default values for the hyperparameters are β1 = γ1 = 0.9, β2 = γ2 = 0.999, φ1 = φ2 =
0.5, η = 0.0001. All of the operations on vectors are element-wise.
1:	Require: η : Learning rate
2:	Require: β1, γ1 , β2, γ2 ∈ [0, 1): Smoothing hyperparameters
3:	Require: φ1, φ2 ∈ [0, 1]: Damping hyperparameters
4:	Require: f (x): Noisy objective function with parameters x
5:	Initialize: x1: Initial parameter values
6:	`m J 0: Initial first moment level estimate
7:	'V J 0: Initial second raw moment level estimate
8:	b0m J 0: Initial first moment trend estimate
9:	b0v J 0: Initial second raw moment trend estimate
10:	for t = 1, 2, . . . , T do
11:	`m J βιmt-i + (1- βι)Vf(xt)
12:	bm J YιΦιb2 1 + (1- Yι)('m — 砂ι)
13:	'V J β2vt-1 + (1 — β2)Vf (xt) ◦ Vf (xt)
14:	bV J Y2Φ2bV-i + (1 — Y2)('V - 'V-i)
15:	mt J 'm∕(1 — β1) + [(1 — Y1Φ1)bm]∕[(1	— γι)(1	— (γ1φ1)t)]
16:	Vt J 'V/(1 — β2) + [(1 ʃ Y2Φ2)bV]∕[(1 —	Y2)(1	—	(γ2Φ2)t)]
17:	Xt+1 J Xt — ηmt∕( VZivt| + e)	. e = 1e — 8
18:	end for
19:	return xt
Corollary 3.1.1. By following the assumptions in Theorem 3.1, for all T ≥ 1 AdamT achieves
翠=O( √⅛ )
This result follows immediately from Theorem 3.1 and thus limτ→∞ RTp) = 0.
4 Experiments
We evaluate the proposed algorithm AdamT on both convex and non-convex real-world optimiza-
tion problems with several popular types of machine learning models. The models we considered in
the experiments include logistic regression which has a well-known convex loss surface, and differ-
ent neural network models, including feedforward neural networks, convolutional neural networks
and variational autoencoder. Neural Networks with non-linear activation function typically have an
inherent non-convex loss surface which is more challenging for an optimization method.
We compare our method with Adam (Kingma & Ba, 2015) and demonstrate the effectiveness of the
trend information of the gradients infused in AdamT. The experiment results show that our method
has a converge faster to reach a better minimum point than Adam. The observation evidences that
the added trend information effectively helps AdamT to better capture the landscape of loss surface.
In the following experiments, we prepare the same set of initial values for the models, so that the
initial model losses (the loss value at epoch = 0) are identical for all the optimization methods.
In terms of the hyperparameters, all the smoothing parameters (β1, β2 in Adam and β1, β2, γ1, γ2
in AdamT) are set at their corresponding default values which are provided in Algorithm 1. The
damping factors (φ1 , φ2 ) and the learning rate η are tuned through a dense grid search to produce
the best results for both of the optimizers. All the experiments and optimizers are written in PyTorch.
4.1	Logistic Regression for Fashion-MNIST
We first evaluate AdamT on the logistic regression for multi-class classification problem with
Fashion-MNIST dataset (Xiao et al., 2017) which is a MNIST-like dataset of fashion products.
The dataset has 60, 000 training samples and 10, 000 testing samples. Each of them has 28 × 28
pixels. Each of the samples is classified into one of the 10 fashion products. The cross-entropy loss
5
Under review as a conference paper at ICLR 2020
0.004
0.003
D.002
D-OOl
D.000
。uuee*za8OJ CTC-C-EH
O 5	10	15	20	25	30	35	40	45
Epoch
Figure 1: Training loss difference (left) and test loss difference (right) of the logistic regression on
Fashion-MNIST dataset for classification task.
0.006
-0.004q
0.004
0.002
0.000
-0.002
。uuee5α^so*l 6u-y
5	10	15	20	25	30	35	40	45
Epoch
function has a well-behaved convex surface. The learning rate η for both Adam and AdamT is set to
be constant during the training procedure. We use minibatch training with size set to 128.
The results are reported in Figure 1. Since the superiority of our method over Adam is relatively
small in this experiment, the plot of loss value against epoch cannot visualize the difference. Instead,
we plot the loss difference of the two optimizers, which is (LossAdam - LossAdamT) against training
epoch. The difference above zero reflect the advantage of AdamT. Figure 1 indicates that AdamT
converges faster at the early training stage and constantly outperforms Adam during the rest of
the training phase, though the advantage is relatively small in this experiment. The AdamT gives
a similar (slightly better) performance on the test dataset as Adam. The loss surface of logistic
regression is convex and well-behaved so that the trend information of AdamT cannot further provide
much useful information for optimization, which results in a small advantage in this experiment.
4.2	Feedforward Neural Networks for SVHN
To investigate the performance on non-convex objective functions, we conduct the experiment with
feedforward neural networks on The Street View House Numbers (SVHN) dataset (Netzer et al.,
2011) for a digit classification problem. We pre-process this RGB image dataset into grayscale for
dimension reduction by taking the average across the channels for each pixel in the image. The
samples are 32 × 32 grayscale images. The neural network used in this experiment has two fully-
connected hidden layers, each of which has 1, 400 hidden units and ReLU activation function is used
for the two hidden layers. We use softmax cross-entropy loss function for training the model.
To evaluate the performance of the optimizers in noisy settings, we apply a stochastic regularization
method in the model for a separate experiment. Specifically, we include two dropout layers (Sri-
vastava et al., 2014), where one is applied between the two hidden layers and the other one is used
before the output layer. The dropout probability is set to 0.5 for both of the two dropout layers. In
the experiments, we use a constant learning rate η and minibatch training with size set to 128.
We examine the training and test loss of the models with and without dropout layers for the two
optimizers. According to Figure 2, we find that AdamT outperforms Adam significantly. In terms
of the training process, AdamT yields a faster convergence and reaches a better position than Adam
for the models, both with and without dropout layers. The superior performance of AdamT is also
shown in the test phase, which demonstrates that AdamT also has a better generalization ability
than Adam. For the model without dropout, our method performs on a par with Adam on the test
dataset. Comparing to logistic regression, the loss surface in this experiment becomes complex
and non-convex. The trend estimates of the gradients from AdamT can provide more meaningful
information of the landscape of the loss surface, and it encourages a better performance on AdamT.
4.3	Convolutional Neural Networks for CIFAR-10
Convolutional neural network (CNN) is the main workhorse for Computer Vision tasks. We train a
CNN model on standard CIFAR-10 dataset for a multi-class classification task. The dataset contains
6
Under review as a conference paper at ICLR 2020
20
j0
50
Ssol 6U-U-EJ.
5
3
O
3
E
W
-7
Figure 2: Training loss (left) and test loss (right) of the feedforward neural network on SVHN dataset
for a classification problem. The model architecture is fc1400-fc1400.
WWOJ ctc-⅛φh
w
5
3
5
2
E
5
1
W
O
1
w

Figure 3:	Training loss (left) and test loss (right) of the convolutional neural network on CIFAR-10
dataset for a classification task. The model architecture is c64-c64-fc600.
50, 000 training samples and 10, 000 test samples, and each sample is an RGB 32 × 32 image.
We pre-process the dataset by normalizing the pixel values to the range [-1, 1] for a more robust
training. The CNN model employed in this experiment is similar to the model used in Reddi et al.
(2018), which has the following architecture. There are 2 stages of alternating convolution and max
pooling layers. Each convolution layer has 64 channels and kernel size 5 × 5 with stride 1. Each
max pooling layer is applied with a kernel size of 2 × 2. After that, there is a fully-connected layer
with 600 hidden units and a dropout probability 0.5 followed by the output layer with 10 units. We
use ReLU for the activation function and softmax cross-entropy for the loss function. The model is
trained with a tuned constant learning rate and minibatch size 128 same as the previous experiments.
The experiment results are reported in Figure 3. We can observe that the proposed AdamT clearly
excels Adam on the training loss, and this superiority translates into a more significant advantage of
AdamT on the test loss, which again demonstrates a better generalization ability.
4.4 Deep Generative Models For MNIST
Variational Autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) is one of the most
popular deep generative models for density estimation and image generation. In this experiment,
we train a VAE model on the standard MNIST dataset which contains 60, 000 training samples
and 10, 000 test samples. Each sample is one 28 × 28 black-and-white image of the handwritten
digit. The VAE model used in this experiment exactly matches the architecture presented in Kingma
& Welling (2014): Gaussian encoder and Bernoulli decoder, both of which are implemented by
feedforward neural networks with single hidden layer and there are 500 hidden units in each hidden
layer. We employ the hyperbolic tangent activation function for the model and set the dimensionality
of the latent space as 20. We use the constant learning rate and set the minibatch size to 150.
7
Under review as a conference paper at ICLR 2020
BUUaJ⅝uaos^lujπc-c-EH
______________________________________________
86420246
--------
Oooooooo
- - -
E2⅝M08-3 6usaJ.
E
40
E
60
Figure 4:	Training ELBO difference (left) and test ELBO difference (right) of the variational au-
toencoder on MNIST dataset.
We examine the Evidence Lower Bound (ELBO) of the training and testing phases for the two
optimizers’ performance assessment. See Figure 4 for the experiment results. Due to the issue of
different scales, we plot the difference between the ELBOs produced by the two optimizers. Similar
to the first experiment, we plot the difference value (ELBOAdam - ELBOAdamT) against the epoch
for training and testing. We observe that our AdamT has a much faster convergence at the early stage
of training than Adam and constantly excels Adam during the rest of the training phase. The superior
performance of AdamT in this experiment also translates into a clear advantage in the testing phase.
5	Related Works
We consider the class of adaptive moment estimation methods with exponential moving average
scheme as Adam-type learning algorithms. The fundamental idea was proposed in Kingma & Ba
(2015) and quickly extended to many variants. Some examples include AdaMax (Kingma & Ba,
2015), Nadam (Dozat, 2016) and AdamW (Loshchilov & Hutter, 2019).
Despite the efficiency in practice, the conventional Adam-type methods fail to guarantee global con-
vergences. Reddi et al. (2018) discussed the problematic short-term memory of the gradients. For
the convex settings, they proposed AMSGrad that promises a global optimization with a compara-
ble performance. Except for some other recent studies for convex optimization (Xu et al., 2017;
Chen et al., 2018; Levy et al., 2018), several works developed optimization methods for non-convex
problems. Padam (Chen & Gu, 2018; Zhou et al., 2018) introduces a partial adaptive parameter p to
interpolate between SGD with momentum and Adam so that adjacent learning rates could decrease
smoothly. AdaUSM (Zou & Shen, 2018) appends the idea of unified momentum for non-decreasing
sequence of weights. AdaFom (Chen et al., 2019) obtains first-order stationary by taking simple
average on the second moment estimation. More conditions for pursuing global convergence were
summarized in Zou et al. (2019), basing on the currently successful variants.
6	Discussion and Conclusion
In this work, we have modified the scheme to calculate the adaptive step size from exponential
moving average to trend-corrected exponential smoothing. Empirical results demonstrate that our
method, AdamT, works well in practice and constantly beats the baseline method Adam.
We leave some potentials for future developments. First, although we focused primarily on ADAM
for theoretical and experimental analysis, we believe that similar ideas could also extend to other
adaptive gradient methods, such as RMSProp (Tieleman & Hinton, 2012) and AMSGrad (Reddi
et al., 2018). Also, this work, the same as the original ADAM method, relies on the theoretical
assumption of convex problems settings. We have demonstrated its computational ability on the
non-convex settings, and it is possible to extend the theoretical framework to non-convex scenarios.
Some potential candidates in the latest research are listed in Section 5.
8
Under review as a conference paper at ICLR 2020
References
Jinghui Chen and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in
training deep neural networks. preprint arXiv:1806.06763, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of ADAM-
type algorithms for non-convex optimization. In Proceedings of International Conference on
Learning Representation (ICLR), 2019.
Zaiyi Chen, Yi Xu, Enhong Chen, and Tianbao Yang. Sadagrad: Strongly adaptive stochastic gradi-
ent methods. In International Conference on Machine Learning, pp. 912-920, 2018.
Timothy Dozat. Incorporating Nesterov momentum into ADAM. In Proceedings of 4th Interna-
tional Conference on Learning Representations, Workshop Track, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Everette S Gardner Jr and ED McKenzie. Forecasting trends in time series. Management Science,
31(10):1237-1246, 1985.
Charles C Holt. Forecasting seasonals and trends by exponentially weighted moving averages.
International Journal of Forecasting, 20(1):5-10, 2004.
Rob J Hyndman and George Athanasopoulos. Forecasting: Principles and Practice. OTexts, 2018.
Diederik P Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. In Proceedings
of International Conference on Learning Representation (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of Interna-
tional Conference on Learning Representations, 2014.
Yehuda Kfir Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and
acceleration. In Advances in Neural Information Processing Systems, pp. 6500-6509, 2018.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of 4th
International Conference on Learning Representations, 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12
(1):145-151, 1999.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of ADAM and beyond. In
Proceedings of International Conference on Learning Representation (ICLR), 2018.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31st International Con-
ference on Machine Learning, 2014.
Sebastian Ruder. An overview of gradient descent optimization algorithms. preprint
arXiv:1609.04747, 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2):
26-31, 2012.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. preprint arXiv:1708.07747, 2017.
9
Under review as a conference paper at ICLR 2020
Yi Xu, Qihang Lin, and Tianbao Yang. Stochastic convex optimization: Faster local growth im-
plies faster global convergence. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70,pp. 3821-3830. JMLR. org, 2017.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. preprint arXiv:1212.5701, 2012.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of
adaptive gradient methods for nonconvex optimization. preprint arXiv:1808.05671, 2018.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning, pp. 928-936, 2003.
Fangyu Zou and Li Shen. On the convergence of weighted AdaGrad with momentum for training
deep neural networks. preprint arXiv:1808.03408, 2018.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for conver-
gences of ADAM and RMSProp. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 11127-11135, 2019.
10
Under review as a conference paper at ICLR 2020
A Bias Correction of the Trend Estimates
To conduct the bias correction for the two trend estimates {btm} and {btv }, we have to take into
account the effect of the corresponding damping factors. Here, we give the justification for the trend
estimate {btm } and the procedures for {btv } is analogous. Note that we can write the trend estimate
btm into the following compact summation form:
bm = γ1Φ1bm-1 + (i-γι)('m -C-I)
t
= (i-γι) X(γιΦι)t-i('m - `m-i)
i=1
To find how the expectation of the trend estimates btm relates to the expectation of the difference
between the level estimates at successive timesteps (`m - `m-i), We take the expectation for both
sides of the above equation:
t
E[bm] = E[(1 - γι) X(γιφι)t-i('m -C-1)]
i=1
t
= (1-Y1) X(γιΦι)t-iE[('m -C-1)]
i=1
t
=E[('m -C-I)](1 - Y1) X(γ1Φ1 尸 + Z
i=1
where ζ can be considered as a small constant, since the factor (γ1φ1)t-i will be tiny if the associated
expectation E[('m -'”1)] is too far away in the past in the case that E[('m -'”1)] is non-stationary.
If E[('m - `m-i)] is stationary, the constant Z will be zero. To further simplify the above equation,
we apply the formula for the sum of geometric sequence:
E[bm] = E[('m - `mj](i - %) (1-(γ1φl)t) + Z
t	t	t-	1 - γ1 φ1
This suggests that we can use the term (1 - γι)[1 - (γιφι)t]∕[1 - γιφι] to correct the bias and close
the discrepancy between the above two expectations at the presence of the damping factor φ1.
B Proof of Regret Bound
B.1 Backgrounds
We investigate the convergence of AdamT with regret minimization by following Zinkevich (2003).
Formally, the regret is defined as the difference between total cost and that of best point in hindsight
T
R(T ) = Xft(χt)- ft(χ*)],
t=1
where f (Xt) is the convex function with arguments Xt at iteration t and x* denotes the optimal set
of the parameters. For the sake of clarity, we denote f(xt) as ft.
For a vanishing average regret, Jensen,s inequality implies that the average decision f (XT) con-
verges to f(X*). In other words,
T
f (Xτ) - f (x*) ≤ T X [f (xt) - f (x*)]=牛
t=1
Below we list essential theorems and lemmas.
Definition B.1 (Convexity). Let K ⊆ Rd be a bounded convex and compact set in Euclidean space.
A function f : K 7→ R is convex if for any X, y ∈ K
∀α ∈ [0, 1],	f(αX + (1 - α)y) ≤ αf (X) + (1 - α)f (y).
11
Under review as a conference paper at ICLR 2020
We denote by D an upper bound on the diameter of K, then
x, y ∈ K,	kx - yk ≤ D
Definition B.2 (Subgradient). The set of all subgradients ofa function f at x, denoted ∂f (x), is the
set of all vectors u such that
f(y) ≥ f(x) +u>(y-x)
Suppose f is differentiable, for any existing gradient Vf (x), We have Vf (x) ∈ ∂f (x) and ∀y ∈ K
f (y) ≥ f(x) + Vf (x)>(y — x).
We denote G ≥ 0 and upper bound on the norm of the subgradients of f over K, i.e., kVf (x)k ≤ G
for all x ∈ K. In other Words, the function f is Lipschitz continuous With parameter G, that is, for
all x, y ∈ K
|f (x) -f(y)| ≤ Gkx-yk
BeloW are the three core lemmas that applied in proving the regret bound.
Lemma B.3 (Kingma & Ba (2015)). We define Vf1:t,i ∈ Rt as the vector at the ith dimension
of the gradients over all iterations till t, Vf1:t,i = [Vf1,i, Vf2,i, ..., Vft,i]. With lVft(xt)l2 ≤
G, lVft(xt)l∞ ≤ G∞ we have
T	Vf2
-tɪ ≤ 2G∞ llVf1：T,ik2
Lemma B.4. For β1,γ1,β2,γ2 ∈ [0,1) and φ1,φ2 ∈ [0,1] the series {rht,i} and {Vt,i} has the
following summation form
mt,i ≤
Vt,i ≤
(I - βI) + (I - YιφI)(I - βI)
,1- βt +	(1 - (γ1Φ1)t).
t
X Vfi .
i=1
(I -β2)+ (I - γ2φ2)(I 一尸2)X Vf2
,LT +	(1 - (γ2Φ2)t)	∖h
Proof. We start from Tmt,i. It can be shown that
mt-1 = `tm-1 +φ1btm-1
`tm= (1 - β1)Vft + β1ltm-1 + β1φ1btm-1
btm = (1 - β1)(1 - γ1)Vft - (1 - β1)(1 - γ1)ltm-1 - ((1 - β1)(1 - γ1) - φ1)btm-1
Substitute `tm,i and btm,i, we have
m t,i = 丁、%+∩—(I) -γ1φL)t)bm
1 - β1t t,i (1 - γ1)(1 - (γ1φ1)t) t,i
= (I - βι) . (I - γιφι)(I - βι) V f
=- ɪ-ɪ +	(1 — (Y1Φ1)t)	] f
「(1 — Bl)	f Q f[人、、(I- γ1φl)(1 - β1) ɑ fl Q 7n Y	V7f
+ 1 1 - βt	(BI(I	+ φ1))	+ —(1 - (γ1φ1)t)-(φ1	- (1	- β1)(2 -	Yl))_|	VftT,i
+ ɪr—CtT(BI((BI - φι(I -仇)(I - YI)) + φι)(BI - φι + (I - βι)(I - Yi)))
1 - B1
+ ^~∩Y1t^Λ tt ∖11 (BI(I - BI)(OI(YI - I) - I) - (φi - (1 - BI)(2 - YI))((I - BI)(I - YI) - φI))) Vft-2,i
(1 - (Y1φ1)t)
+…
≤
(I - BI) + (1 - YiφI)(I - BI)
.1 - Bt + (1 - (Y1Φ1)t).
t
X	Vfk,i
k=1
12
Under review as a conference paper at ICLR 2020
Similarly for Vt, We have
Vt,i ≤
(I - β2) + (I - 12φ2)(1 - β2)
,1 - βt	(1 - (Y2φ2)t)	.
t
X Vf2,i.
k=1
□
B.2 Proof of regret bound
Theorem B.5. Assume that the objectivefunction f hasboundedgradients, thatis ∣∣Vft(x)k2 ≤ G,
∣∣Vft(x)k∞ ≤ G∞ for all X ∈ Rd. Wefurther assume that the distance between any Xt produced
by AdamT is bounded, that is ∣xn - xm∣2 ≤ D, ∣xm - xn∣∞ ≤ D∞ for any m, n ∈ {1, 2, . . . , T},
and β1, γ1, β2, γ2 ∈ [0, 1), φ1, φ2 ∈ [0, 1]. The proposed AdamT achieves the following guarantee
for all T ≥ 1
R(T) ≤2η(D∞βι) X PTvTi + 2[β1(1 - βI)(I + 血(I-YI))2]G∞ + (1 - βι)((2(- -ιγ1φl^(7ιφι)T))2 D∞
+
η (2 - βι)2
2(1 - βι) √2^-β2
d
X llg1：T,ik2 + dD2
i=1
(1 一 Y1)β1 + 1 - γιφι
2(I- YI)(I- β1)
Proof. According to Theorem B.2, for any convex function f and x* ∈ K, We have
ft(Xt)- ft(χ*) ≤ Vft(Xt)>(xt - χ*)
Which implies the regret upper bound
TT
T
R(T) =	ft(Xt) -	ft(X*) ≤	Vft(Xt - X*).
From the update rules in Algorithm,
ηt
xt+ι = xt-----7= m t
√vt
=一2(`m |	(1- YMbm	ʌ
Xt	√vt V - βt	(1 - Y1)(1 - (γιφι)t)/
=	(βι(d 1 + φιb-1 + (1- βι)Vft(χt) +	(1- YMbm	A
t	√∖	1-βt	1 - βt	(1 - Yi)(1 - (Yιφι)t)/
Consider the ith dimension of the parameter vector at time t
(Xt+1,i - Xi ) =(Xt,i -
- Xi*)2 - 2ηt
mt,i I	*-	2( mt,i、2
^=(Xt,i - Xi) + ηt ( r~]
vzvt,i	√vt,i
=(Xt,i
2ηt (βι(4% + Φ1b-1)
—√∖	1-β
(1- βι)Vft(Xt)
1 - βt
+	(1 - γιφι)bm	ʌ (X∙-X*)
+ (1 - γ1)(1 - (γ1Φ1 )t)7( t,i i)
+
13
Under review as a conference paper at ICLR 2020
Rearrange the equation above will get
Vft,i(Xt,i - x*) = 2V((I - ββI)) ((Xt,i - X*)2 - (Xt+1,i - X*)2
—
βι(d 1 + φιb之ι) 1	(1- βt)(1- Yιφι)bm	ʌ (	_	ηt(* 1- βt)m2,i
1 - β1	(1 - β1)(1 - YI)(I-(Y1φ1 )t) J	t"	i 2(1 - βι)√^7
√Vt(1 - βt) 〃	*、2	(	*、2^ β1(C-1 + Φ1b立 1)一 、
2ηt (1 - β1)	((Xt,i	-	χi	) -	(Xt+1,i -	χi	) ) +	1 -	β]	(Xi	-	χt,i)
(I - βt)(I - Y1φ1)bm	* * _	)	ηt(I - βt)m2,i
+ (1-β1)(1-Y1)(1-(YiΦ1)t)(Xi -χt,i) + 2(1 -β1)√Vt^.
For any βι ≤ 1 and (1 - βtt) ≤ 1, we can get
vft,i(χt,i- W) ≤
2ηt(i- β)
((Xt,i - W)2 - (xt+1,i - W)2) +
+_______(1-γιφι)bm_______(χ* - χt.) +
(1 - βι)(1 - YI)(I-(Yιφι)t)'	,
βι(d ι + ΦιbD
—1—β— (Xi-Xt,i)
2
ηtm 2,i
2(1- βι)√t;.
Applying Young,s inequality on the second and the third term yields
β(d 1 + φ1b^1)
1 - βι
βι	(T * 一
(1 -βι)(Xi- Xt，i
≤ ' '1——-(x* — Xti)I2 +
≤ 2(1 - βj i P
(1 - Y1Φ1)bm
(i⅛(%+MI
2(1¾)(%+φ1*1)2,
(1 - β1)(I- YI)(I-(TLφIy)
1 - Yiφι	/ *
(1-βι)(1-γι)(Xi-Xt'i
≤____1 - γiφ1__(x* - x .)2 +
≤2(1-βι)(1-γι)(Xi	XtQ +
φι	bm	∖
(1 - βι )(1-Yt ”
1- γ1Φ1	(	bm	Y
2(1 - βι)(1 - YI) I(I-Yt)√
The regret bound can be derived by taking summation across all the dimensions for i = 1,2,...,d
and the convex functions for t = 1,2,...,T.
dT
R(T) ≤ XX V∕t,i(Xt,i -X*)
i=1 t=1
d
≤
i=1
1
2η(1- βι)
dT
(xι,i - X*)2√v1- + XX
i=1 t=2
1
2(1 - βι)
(XtL x"2
-1,i
,
T
+ X
t=1
1
2(1 - βι)
P1
R(Pm J 从时、2 j (1 - γ1φI)((I- γ1φ1)Sm))2
β1(4τ + φ1bt-1) +	((1-Y1)(1-(Y1Φ1 )T ))2
{z^
P 2
,
dT
+XX
η	m 2,i
2(1 - β1) √tj
一■―― ,
"{z
P 3
T
+X
t=1
1
2(1 - βι)
βι+⅛γφ1) (χ: - χt.i)2
{z^
P 4
,
14
Under review as a conference paper at ICLR 2020
According to the assumptions ∣∣xt - x*∣∣2 ≤ D, ∣∣xm - xnk∞ ≤ D∞ for any m, n ∈ 1, 2,...,T,
the above four parts in the inequality can be further upper bounded as follows respectively:
d	d T
P1 = X 2η(l⅛) (xι,i - W)2R + XX 2(T⅛(xt,i - *2
P2
≤
D∞
2η(1 - βι)
d
XpTvT7
i=1
T1
t=1 2(1- βι)
(βι('m-ι+φι bm-1)2 +
(1- γιφι)((1- Y1Φ1)(bm))2 ∖
((1-Y1)(1-(Y1Φ1)T ))2 )
T
X
t=1
β	(`m + φ bm )2 + X 1 (I - γ1φ1)((I - γ1φ1Χbm))2
2(1-βι)( t-1 + φ1 tT) + ⅛ 2(1- βι) ((1 - γ1)(1 - (γ1φ1)T))2
1T
≤ 2(尸1(I- β1)(1 + φ1 (I-YI))2)	Vft-1
t=1
+
__________(1 - Y1φ1)3________
2(1- βι)((1- γι)(1-(γιφι)T))2
T
X(btm)2
t=1
≤ 2(βi(1 - β1 )(1+ OK1 - γ I)) 2)G∞ + (1 - βι)((2- -ιγ1φ^ (Y1Φ1)T ))2 D∞
P3 _ X X η	m2,i ≤ η (2 - β1)2 X k k
? t= 2(1- β1) √vt^ - 2(1-员)√2-β2 "∣g1"2
dT
P 4 = XX----
⅛1 ⅛12(1-β1)
β1 +
1 — Y1φ1
1 - Y1
(1 - Y1)e1 + 1 - Y1φ1 X D2
≤	2(1-γ1)(1- β1) 士 2
dD22
(1 一 Y1)β1 + 1 — Y1φ1
2(1 - YI)(I- β1)
Hence, the final regret upper bound can be written as:
R(T)≤2η(D∞β1) XpTvTi + 1[β1(1 - β1)(1 + O1(1 - YI))2]G∞ + (1 - β1)((2(- -1γ1φ1^ (γ1 φ1)T))2 D∞
+
η (2 - β1 )2
2(1 - β1) √2-β2
d
X kg1：T,ik2 +dD2
i=1
(1 - Y1)β1 + 1 - Y1φ1
2(1 - YI)(I- β1)
□
15