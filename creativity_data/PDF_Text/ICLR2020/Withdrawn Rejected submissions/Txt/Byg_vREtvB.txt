Under review as a conference paper at ICLR 2020
Generalized Bayesian Posterior Expectation
Distillation for Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we present a general framework for distilling expectations with
respect to the Bayesian posterior distribution of a deep neural network, significantly
extending prior work on a method known as “Bayesian Dark Knowledge.” Our
generalized framework applies to the case of classification models and takes as input
the architecture of a “teacher” network, a general posterior expectation of interest,
and the architecture of a “student” network. The distillation method performs an
online compression of the selected posterior expectation using iteratively generated
Monte Carlo samples from the parameter posterior of the teacher model. We further
consider the problem of optimizing the student model architecture with respect to
an accuracy-speed-storage trade-off. We present experimental results investigating
multiple data sets, distillation targets, teacher model architectures, and approaches
to searching for student model architectures. We establish the key result that
distilling into a student model with an architecture that matches the teacher, as is
done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly,
we show that student architecture search methods can identify student models with
significantly improved performance.
1	Introduction
Deep learning models have shown promising results in the areas including computer vision, natural
language processing, speech recognition, and more (Krizhevsky et al., 2012; Graves et al., 2013a;b;
Huang et al., 2016; Devlin et al., 2018). However, existing point estimation-based training methods
for these models may result in predictive uncertainties that are not well calibrated, including the
occurrence of confident errors.
It is well-known that Bayesian inference can often provide more robust posterior predictive distribu-
tions in the classification setting compared to the use of point estimation-based training. However,
the integrals required to perform Bayesian inference in neural network models are also well-known to
be intractable. Monte Carlo methods provide one solution to representing neural network parameter
posteriors as ensembles of networks, but this can require large amounts of both storage and compute
time (Neal, 1996; Welling & Teh, 2011).
To help overcome these problems, Balan et al. (2015) introduced an interesting model training
method referred to as Bayesian Dark Knowledge. In the classification setting, Bayesian Dark
Knowledge attempts to compress the Bayesian posterior predictive distribution induced by the full
parameter posterior of a “teacher” network into a “student” network. The parameter posterior of the
teacher network is represented through a Monte Carlo ensemble of specific instances of the teacher
network (the teacher ensemble), and the analytically intractable posterior predictive distributions are
approximated as Monte Carlo averages over the output of the networks in the teacher ensemble. The
major advantage of this approach is that the computational complexity of prediction at test time is
drastically reduced compared to computing Monte Carlo averages over a large ensemble of networks.
As a result, methods of this type have the potential to be much better suited to learning models for
deployment in resource constrained settings.
In this paper, we present a Bayesian posterior distillation framework that generalizes the Bayesian
Dark Knowledge approach in several significant directions. The primary modeling and algorithmic
contributions of this work are: (1) we generalize the target of distillation in the classification case
from the posterior predictive distribution to general posterior expectations; (2) we generalize the
1
Under review as a conference paper at ICLR 2020
student architecture from being restricted to match the teacher architecture to being a free choice in
the distillation procedure.
The primary empirical contributions of this work are (1) evaluating the distillation of both the posterior
predictive distribution and expected posterior entropy across a range of models and data sets including
manipulations of data sets that increase posterior uncertainty; and (2) evaluating the impact of the
student model architecture on distillation performance including the investigation of sparsity-inducing
regularization and pruning for student model architecture optimization. The key empirical findings
are that (1) distilling into a student model that matches the architecture of the teacher, as in Balan
et al. (2015), can be sub-optimal; and (2) student architecture optimization methods can identify
significantly improved student models.
We note that the significance of generalizing distillation to arbitrary posterior expectations is that
it allows us to capture a wider range of useful statistics of the posterior that are of interest from
an uncertainty quantification perspective. As noted above, we focus on the case of distilling the
expected posterior entropy in addition to the posterior predictive distribution itself. When combined
with the entropy of the posterior predictive distribution, the expected posterior entropy enables
disentangling model uncertainty (epistemic uncertainty) from fundamental uncertainty due to class
overlap (aleatoric uncertainty). This distinction is extremely important in determining why predictions
are uncertain for a given data case. Indeed, the difference between these two terms is the basis for
the Bayesian active learning by disagreement (BALD) score used in active learning, which samples
instances with the goal of minimizing model uncertainty (Houlsby et al., 2011).
The remainder of this paper is organized as follows. In the next section, we begin by presenting
background material and related work in Section 2. In Section 3, we present the proposed framework
and associated Generalized Posterior Expectation Distillation (GPED) algorithm. In Section 4, we
present experiments and results. Additional details regarding data sets and experiments can be found
in Appendix A, with supplemental results included in Appendix B.
2	Background and Related Work
In this section we present background material on Bayesian inference for neural networks, and related
work on approximate inference, and model compression and pruning.
2.1	Bayesian Neural Networks
Let p(y|x, θ) represent the probability distribution induced by a deep neural network classifier over
classes y ∈ Y = {1, .., C} given feature vectors x ∈ RD. The most common way to fit a model
of this type given a data set D = {(xi, yi)|1 ≤ i ≤ N} is to use maximum conditional likelihood
estimation, or equivalently, cross entropy loss minimization (or their penalized or regularized variants).
However, when the volume of labeled data is low, there can be multiple advantages to considering
a full Bayesian treatment of the model. Instead of attempting to find the single (locally) optimal
parameter set θ* according to a given criterion, Bayesian inference uses Bayes rule to define the
posterior distribution p(θ∣D, θ0) over the unknown parameters θ given a prior distribution P(θ∣θ0)
with prior parameters θ0 as seen in Equation 1.
(θlD θo) _	p(D∣Θ)p(Θ∣Θ0)
P(I ,	) = R p(D∣θ)p(θ∣θ0)dθ
p(y∣x, D,θ0) = /p(y∣x, θ)p(θ∣D, θ0)dθ = Ep(θ∣D,θ0)[p(y∣x, θ)]
(1)
(2)
For prediction problems in machine learning, the quantity of interest is typically not the parameter
posterior itself, but the posterior predictive distribution p(y|x, D, θ0) obtained from it as seen in
Equation 2. The primary problem with applying Bayesian inference to neural network models is that
the distributions p(θ∣D, θ0) andp(y∣x, D, θ0) are not available in closed form, so approximations
are required, which we discuss in the next section.
2
Under review as a conference paper at ICLR 2020
2.2	Approximate Inference Methods for Bayesian Neural Networks
Most Bayesian inference approximations studied in the machine learning literature are based on
variational inference (VI) (Jordan et al., 1999) or Markov Chain Monte Carlo (MCMC) methods
(Neal, 1996; Welling & Teh, 2011). In VI, an auxiliary distribution qφ (θ) is defined to approximate
the true parameter posterior p(θ∣D, θ0). The variational parameters φ are selected to minimize the
KUllbaCk-Leibler(KL) divergence between q@(θ) and p(θ∣D, θ0). Hinton & Van Camp (1993) first
studied applying VI to neural networks. Graves (2011) later presented a method based on stochastic
VI with improved scalability. In the closely related family of expectation propagation (EP) methods
(Minka, 2001), Soudry et al. (2014) present an online EP algorithm for neural networks with the
flexibility of representing both continuous and discrete weights. Hernandez-Lobato & Adams (2015)
present the probabilistic backpropagation (PBP) algorithm for approximate Bayesian learning of
neural network models, which is an example of an assumed density filtering (ADF) algorithm that,
like VI and EP, generally relies on simplified posterior densities.
The main drawback of VB, EP, and ADF is that they all typically result in biased posterior estimates
for complex posterior distributions. MCMC methods provide an alternative family of sampling-based
posterior approximations that are unbiased, but are often computationally more expensive to use at
training time. MCMC methods allow for drawing a correlated sequence of samples θt 〜p(θ∣D, θ0)
from the parameter posterior. These samples can then be used to approximate the posterior predictive
distribution as a Monte Carlo average as shown in Equation 3.
1T
p(y∣χ, D,θ0) ≈ TEp(y∣χ, θt);	θt 〜p(θ∣D,θ0)	(3)
t=1
Neal (1996) addressed the problem of Bayesian inference in neural networks using Hamiltonian
Monte Carlo (HMC) to provide a set of posterior samples. A bottleneck with this method is that it
uses the full dataset when computing the gradient needed by HMC, which is problematic for larger
data sets. While this scalability problem has largely been solved by more recent methods such as
stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011), the problem of needing to
compute over a large set of samples when making predictions at test or deployment time remains.
Bayesian Dark Knowledge (Balan et al., 2015) is precisely aimed at reducing the test-time compu-
tational complexity of Monte Carlo-based approximations for neural networks. In particular, the
method uses SGLD to approximate the posterior distribution using a set of posterior parameter
samples. These samples can be thought of as an ensemble of neural network models with identical
architectures, but different parameter values. This posterior ensemble is used as the “teacher” in a
distillation process that trains a single “student” model to match the teacher ensemble’s posterior
predictive distribution (Hinton et al., 2015). The major advantage of this approach is that it can
drastically reduce the test time computational complexity of posterior predictive inference relative to
using a Monte Carlo average computed using many samples.
Finally, we note that with the advent of Generative Adversarial Networks (Goodfellow et al., 2014),
there has also been work on generative models for approximating posterior sampling. Wang et al.
(2018) and Henning et al. (2018) both propose methods for learning to generate samples that mimic
those produced by SGLD. However, while these approaches may provide a speed-up relative to
running SGLD itself, the resulting samples must still be used in a Monte Carlo average to compute a
posterior predictive distribution in the case of Bayesian neural networks. This is again a potentially
costly operation and is exactly the computation that Bayesian Dark Knowledge addresses.
2.3 Model Compression and Pruning
As noted above, the problem that Bayesian Dark Knowledge attempts to solve is reducing the test-time
computational complexity of using a Monte-Carlo posterior to make predictions. In this work, we are
particularly concerned with the issue of enabling test-time speed-storage-accuracy trade-offs. The
relevant background material includes methods for network compression and pruning.
Previous work has shown that overparameterised deep learning models tend to show much better
learnability. Further, it has also been shown that such overparameterised models rarely use their full
capacity and can often be pruned back substatially without significant loss of generality (Hassibi
et al., 1993; LeCun et al., 1989; Luo et al., 2017; Louizos et al., 2017; Frankle & Carbin, 2018;
3
Under review as a conference paper at ICLR 2020
Han et al., 2015; Zhang & Ou, 2018). Hassibi et al. (1993); LeCun et al. (1989) use the second
order derivatives of the objective function to guide pruning network connections. More recently, Han
et al. (2015) introduced a weight magnitude-based technique for pruning connections in deep neural
networks using simple thresholding. Guo et al. (2016); Jin et al. (2016); Han et al. (2016) introduce
thresholding methods which also support restoration of connections.
A related line of work includes pruning neurons/channels/filters instead of individual weights. Pruning
these components explicitly reduces the number of computations by making the networks smaller.
Group LASSO-based methods have the advantage of turning the pruning problem into a continuous
optimization problem with a sparsity-inducing regularizer. Zhang & Ou (2018); Alvarez & Salzmann
(2016); Wen et al. (2016); He et al. (2017) are some examples that use Group LASSO regularization at
their core. Similarly Louizos et al. (2017) use hierarchical priors to prune neurons instead of weights.
An advantage of these methods over ones which induce connection-based sparsity is that these
methods directly produce smaller networks after pruning (e.g., fewer units or channels) as opposed to
networks with sparse weight matrices. This makes it easier to realize the resulting computational
savings, even on platforms that do not directly support sparse matrix operations.
3	Proposed Framework
In this section, we describe our proposed framework for distilling general Bayesian posterior ex-
pectations for neural network classification models and discuss methods for enabling test-time
speed-storage-accuracy trade-offs for flexible deployment of the resulting models.
3.1	Generalized Posterior Expectations
There are many possible inferences of interest given a Bayesian parameter posterior P(θ∣D, θ0).
We consider the general case of inferences that take the form of posterior expectations as shown in
Equation 4 where g(y, x, θ) is an arbitrary function of y, x and θ.
Ep(θ∣d,θ0) [g(y, χ, θ)] = / g(y, χ, θ)p(θ∣D, θ0)dθ	(4)
Important examples of functions g(y, x, θ) include g(y, x, θ) = p(y|x, θ), which results in a pos-
terior expectation yielding the posterior predictive distribution p(y∣x, D, θ0), as used in Bayesian
Dark Knowledge; g(y, x, θ) = PC=Ip(y0∣x, θ) logp(y0∣x, θ), which yields the posterior predictive
entropy H(y|x, D,θ0)1; and g(y, x, θ) = p(y∣x, θ)(1 - p(y∣x, θ)), which results in the posterior
marginal variance σ1 2(y|x, D,θ0). While the posterior predictive distribution p(y∣x, D, θ0) is cer-
tainly the most important posterior inference from a predictive standpoint, the entropy and variance
are also important from the perspective of uncertainty quantification.
3.2	Generalized Posterior Expectation Distillation
Our goal is to learn to approximate posterior expectations Ep(θ∣d,θo)[g(y, x, θ)] under a given teacher
model architecture using a given student model architecture. The method that we propose takes
as input the teacher model p(y∣x, θ), the prior p(θ∣θ0), a labeled data set D, an unlabeled data set
D0, the function g(y, x, θ), a student model f (y, x∣φ), an online expectation estimator, and a loss
function '(∙, ∙) that measures the error of the approximation given by the student model f (y, x∣φ).
Similar to Balan et al. (2015), we propose an online distillation method based on the use of the SGLD
sampler. We describe all of the components of the framework in the sections below, and provide a
complete description of the resulting method in Algorithm 1.
SGLD Sampler: We define the prior distribution over the parameters p(θ∣θ0) to be a spherical
Gaussian distribution centered at μ = 0 with precision T (We thus have θ0 = [μ, T]). We define S
to be a minibatch of size M drawn from D . θt denotes the parameter set sampled for the teacher
model at sampling iteration t, while ηt denotes the step size for the teacher model at iteration t. The
Langevin noise is denoted by Zt 〜N(0, ηI). The sampling update for SGLD is given by:
1Note that the posterior predictive entropy represents the average entropy integrated over the parameter
posterior. It is not equal to the entropy of the posterior predictive distribution p(y|x, D, θ0) in general.
4
Under review as a conference paper at ICLR 2020
θt+ι J θt +^2t 卜 θ log p(θlθ0)+M X Vθ log P (yi|xi, θt) ʌ + Zt	(5)
i∈S
Distillation Procedure: For the distillation learning procedure, wemakeuseofa secondary unlabeled
data set D0 = {xi|1 ≤ i ≤ N0}. This data set could use feature vectors from the primary data set D,
or a larger data set. We note that due to autocorrelation in the sampled teacher model parameters θt,
we may not want to run a distillation update for every Monte Carlo sample drawn. We thus use two
different iteration indices: t for SGLD iterations and s for distillation iterations.
On every distillation step s, we sample a minibatch S0 from D0 of size M0 . For every data case i in
S0, We update an estimate gyis of the posterior expectation using the most recent parameter sample
θt, obtaining an updated estimate gyis+ι ≈ Ep(θ∣D,θ0) [g(y, x, θ)] (We discuss update schemes in the
next section). Next, We use the minibatch of examples S0 to update the student model. To do so, We
take a step in the gradient direction of the regularized empirical risk of the student model as shoWn
beloW Where αs is the student model learning rate, R(φ) is the regularizer, and λ is the regularization
hyper-parameter. We next discuss the estimation of the expectation targets gyis.
φs + 1 J φs + αs I Mj0 X X Nφ'(gyis+1,f(y, x∕φS)) + λVφR(φs) )	⑹
i∈S0 y∈Y
Expectation Estimation: Given an explicit collection of posterior samples θ1, ..., θs, the standard
Monte Carlo estimate of Ep(θ∣D∕) [g(y, x, θ)] is simply gyis = S Pj= g(y, xi,θj). However, this
estimator requires retaining the sequence of samples θ1, ..., θs, Which may not be feasible in terms of
storage cost. Instead, We consider the application of an online update function. We define mis to be
the count of the number of times data case i has been sampled up to and including distillation iteration
s. An online update function U(gyis, θt, mʤ) takes as input the current estimate of the expectation,
the current sample of the model parameters, and the number of times data case i has been sampled,
and produces an updated estimate of the expectation gyis+ι. Below, we define two different versions
of the function. Us (gyis,θt, mis), updates gyis using the current sample only, while Uo(gyis,θt, mis)
performs an online update equivalent to a full Monte Carlo average.
Us(gyis,θt,mis) = g(y, xi, θt)	(7)
Uo (gyis, θt, miS) =	(mis ∙ gyis + g(y, xi, θt))	(8)
mis+1
We note that both update functions provide unbiased estimates of Ep(θ∣D,θ0)[g(y, x, θ)] after a suitable
burn-in time B. The online update Uo() will generally result in much lower variance in the estimated
values of gyis, but it comes at the cost of needing to explicitly maintain the expectation estimates gyis
across learning iterations, increasing the storage cost of the algorithm. It is worthwhile noting that
the extra storage and computation cost required by Uo grows linearly in the size of the training set
for the student. By contrast, the fully stochastic update is memoryless in terms of past expectation
estimates, so the estimated expectations gyis do not need to be retained across iterations resulting in a
space savings.
General Algorithm and Special Cases: We show a complete description of the proposed method
in Algorithm 1. The algorithm takes as input the teacher model p(y|x, θ), the parameters of the prior
P(θ∣θ0), a labeled data set D, an unlabeled data set D0, the function g(y, x, θ), the student model
f (y, x∣φ), an online expectation estimator U(gyis,θt, mis), a loss function '(∙, ∙) that measures the
error of the approximation given by f (y, x∣φ), a regularization function R() and regularization
hyper-parameter λ, minibatch sizes M and M0, the thinning interval parameter H, the SGLD burn-in
time parameter B and step size schedules for the step sizes ηt and αs .
We note that the original Bayesian Dark Knowledge method is recoverable as a special case of
this framework via the the choices g(y, x, θ) = p(y|x, θ), `(p, q) = -p log(q), U = Us and
p(y|x, θ) = f(y, x, φ) (e.g., the architecture of the student is selected to match that of the teacher).
The original approach also uses a distillation data set D0 obtained from D by adding randomly
5
Under review as a conference paper at ICLR 2020
Algorithm 1 Generalized Posterior Expectation Distillation
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
procedure GPED(D, D0, p(y|x, θ), θ0, g, f, U, `, R, M, M 0, H, B, λ, {ηt}tT=1, {αs}sS=1 )
Initialize S = 0, φ0, θ0, gyi0 = 0, mi0 = 0,η0
for t = 0 to T do
Sample S from D with |S | = M
θt+ι J θt + η2t Ne logp(θlθ0) + N Pi∈s Vθ logP (yi|xi, θt》+ Zt
if mod (t, H) = 0 and t > B then
Sample S0 from D0 with |S0 | = M0
for i ∈ S 0 do
gyis + 1 j- U (gyis,θt, mis )
mis+1 J mis + 1
end for
φs + 1 J φs + αs (M Pi∈S 0 Py∈γ Vφ'(gyis+1,/ (y, χ∕φs)) + λN φR(φs)
sJs+1
end if
end for
end procedure
generated noise to instances from D on each distillation iteration, taking advantage of the fact that
the choice U = Us means that no aspect of the algorithm scales with |D0|.
Our general framework allows for other trade-offs, including reducing the variance in the esti-
mates of gyis at the cost of additional storage in proportion to |D0|. We also note that note
that the loss function `(p, q) = -p log(q) and the choice g(y, x, θ) = p(y|x, θ) are somewhat
of a special case when used together as even when the full stochastic expectation update Us is
used, the resulting distillation parameter gradient is unbiased. To distill posterior entropy, we set
g(y, χ,θ) = py∈γp(y∣χ,θ)logp(y∣χ,θ), U = Uo and '(h, h0) = |h — h0∣.
3.3 Model Compression and Pruning
One of the primary motivations for the original Bayesian Dark Knowledge approach is that it provides
an approximate inference framework that results in significant computational and storage savings
at test time. However, a drawback of the original approach is that the architecture of the student is
chosen to match that of the teacher. As we will show in Section 4, this will sometimes result in a
student network that has too little capacity to represent a particular posterior expectation accurately.
On the other hand, if we plan to deploy the student model in a low resource compute environment,
the teacher architecture may not meet the specified computational constraints. In either case, we need
a general approach for selecting an architecture for the student model.
To begin to explore this problem, we consider to basic approaches to choosing student model archi-
tectures that enable trading off test time inference speed and storage for accuracy. A helpful aspect of
the distillation process relative to a de novo architecture search problem is that the architecture of the
teacher model is available as a starting point. As a first approach, we consider wrapping the proposed
GPED algorithm with an explicit search over a set of student models that are “close” to the teacher.
Specifically, we consider a search space obtained by starting from the teacher model and applying a
width multiplier to the width of every fully connected layer and a kernel multiplier to the number of
kernels in every convolutional layer. While this search requires exponential time in the number of
layers, it provides a baseline for evaluating other methods.
As an alternative approach with better computational complexity, we leverage the regularization
function R(φ) included in the GPED framework to prune a large initial network using group '1∕'2
regularization (Zhang & Ou, 2018; Wen et al., 2016). To apply this approach, we first must partition
the parameters in the parameter vector φ across K groups Gk . The form of the regularizer is
R(Φ) = PK=ι (Pj∈Gfc φ2)1/2. As is well-established in the literature, this regularizer causes all
parameters in a group to go to zero simultaneously when they are not needed in a model. To use
it for model pruning for a unit in a fully connected layer, we collect all of that unit’s inputs into a
group. Similarly, we collect all of the incoming weights for a particular channel in a convolution layer
6
Under review as a conference paper at ICLR 2020
together into a group. If all incoming weights associated with a unit or a channel have magnitude
below a small threshold , we can explicitly remove them from the model, obtaining a more compact
architecture. We also fine-tune our models after pruning.
Finally, we note that any number of weight compressing, pruning, and architecture search methods
could be combined with the GPED framework. Our goal is not to exhaustively compare such methods,
but rather to demonstrate that GPED is sensitive to the choice of student model to highlight the need
for additional research on the problem of selecting student model architectures.
4	Experiments and Results
In this section, we present experiments and results evaluating the proposed approach using multiple
data sets, posterior expectations, teacher model architectures, student model architectures and basic
architecture search methods. We begin by providing an overview of the experimental protocols used.
4.1	Experimental Protocols
Data Sets: We use the MNIST (LeCun, 1998) and CIFAR10 (Krizhevsky et al., 2009) data sets as
base data sets in our experiments. In the case of MNIST, posterior predictive uncertainty is very
low, so we introduce two different modifications to explore the impact of uncertainty on distillation
performance. The first modification is simply to subsample the data. The second modification is
to introduce occlusions into the data set using randomly positioned square masks of different sizes,
resulting in masking rates from 0% to 86.2%. For CIFAR10, we only use sub-sampling. Full details
for both data sets and the manipulations applied can be found in Appendix A.1.
Models: We evaluate a total of three teacher models in this work: a three-layer fully connected
network (FCNN) for MNIST matching the architecture used by Balan et al. (2015), a four-layer
convolutional network for MNIST, and a five-layer convolutional network for CIFAR10. Full details
of the teacher model architectures are given in Appendix A.2. For exhaustive search for student
model architectures, we use the teacher model architectures as base models and search over a space
of layer width multipliers K1 and K2 that can be used to expand sets of layers in the teacher models.
A full description of the search space of student models can be found in Appendix A.2.
Distillation Procedures: We consider distilling both the posterior predictive distribution and the
posterior entropy, as described in the previous section. For the posterior predictive distribution,
we use the stochastic expectation estimator Us while for entropy we used the full online update
Uo. We allow B = 1000 burn-in iterations and total of T = 106 training iterations. The prior
hyper-parameters, learning rate schedules and other parameters vary by data set or distillation target
and are fully described in Appendix A.2.
4.2	Experiment 1: Distilling Posterior Expectations
For this experiment, we use the MNIST and CIFAR10 datasets without any subsampling or masking.
For each dataset and model, we consider separately distilling the posterior predictive distribution
and the posterior entropy. We fix the architecture of the student to match that of the teacher. To
evaluate the performance while distilling the posterior predictive distribution, we use the negative
log-likelihood (NLL) of the model on the test set. For evaluating the performance of distilling
posterior entropy, we use the mean absolute difference between the teacher ensemble’s entropy
estimate and the student model output on the test set. The results are given in Table 1. First, we
note that the FCNN NLL results on MNIST closely replicate the results in Balan et al. (2015), as
expected. We also note that the error in the entropy is low for both the FCNN and CNN architectures
on MNIST. However, the student model fails to match the NLL of the teacher on CIFAR10 and the
entropy MAE is also relatively high. In Experiment 2, we will investigate the effect of increasing
uncertainty on models applied to both data sets, while in Experiment 3 we will search for student
model architectures that improve performance.
7
Under review as a conference paper at ICLR 2020
Model & Dataset Teacher NLL Student NLL MAE (Entropy)
FCNN - MNIST	0.052	0.082	0.016
CNN - MNIST	0.022	0.053	0.016
CNN - CIFAR10	0.671	0.932	0.245
Table 1: Results of posterior distillation when the student architecture is fixed to match the teacher
architecture and the base data sets are used with no sub-sampling or occlusion.
2 10
ETlN
IOOOO 20000 30000 60000
No. of training samples
0.4
0.2
0.0
10000 20000 30000 60000
No. Oftraining samples
I02
< 0.0
W
Masking Rate
0.0
Il	- 0.03
J ∣∣∣ =；；；
■ Illlli Illlll - 0.51
10000 20000 30000 60000 — 0.8
No. Oftraining samples
(a)	(b)
(c)
5 0 5 0
.7.5.2O
0.0.0.0.
E -ΠN
(d)
(e)	(f)
Figure 1: Top Row: Distillation performance using CNNs on MNIST while varying data set size
and masking rate. (a) Test negative log likelihood of the teacher posterior predictive distribution. (b)
Difference in test negative log likelihood between student and teacher posterior predictive distribution
estimates. (c) Difference between teacher and student posterior entropy estimates on test data set.
Bottom Row: Distillation performance using CNNs on CIFAR10 while varying data set size. (d) Test
negative log likelihood of the teacher posterior predictive distribution. (e) Difference in test negative
log likelihood between student and teacher posterior predictive distribution estimates. (f) Difference
between teacher and student posterior entropy estimates on test data set. In the plots above, S denotes
the student and T denotes the teacher.
4.3	Experiment 2: Robustness to Uncertainty
This experiment builds on Experiment 1 by exploring methods for increasing posterior uncertainty on
MNIST (sub-sampling and masking) and CIFAR10 (sub-sampling). We consider the cross product of
four sub-sampling rates and six masking rates for MNIST and three sub-sampling rates for CIFAR10.
We consider the posterior predictive distribution and posterior entropy distillation targets. For the
posterior predictive distribution we report the negative log likelihood (NLL) of the teacher, and the
NLL gap between the teacher and student. For entropy, we report the mean absolute error between the
teacher ensemble and the student. All metrics are evaluated on held-out test data. We also restrict the
experiment to the case where the student architecture matches the teacher architecture, mirroring the
Bayesian Dark Knowledge approach. In Figure 1, we show the results for the convolutional models
on MNIST and CIFAR10 respectively. The FCNN results are similar to the CNN results on MNIST
and are shown in Figure 4 in Appendix B. In Appendix B, we also provide a performance comparison
between the Uo and Us estimators while distilling posterior expectations.
As expected, the the NLL of the teacher decreases as the data set size decreases. We observe
that changing the number of training samples has a similar effect on NLL gap for both CIFAR10
and MNIST. More specifically, for any fixed masking rate of MNIST (and zero masking rate for
CIFAR10), we can see that the NLL difference between the student and teacher decreases with
increasing training data. However, for MNIST we can see that the teacher NLL increases much more
rapidly as a function of the masking rate. Moreover, the gap between the teacher and student peaks
8
Under review as a conference paper at ICLR 2020
0.5
Student------- Teacher ・ individual Student
ClSəjj TlN
∖		∖ ∙					
					⅛ X		
0.2
0.0	0.5	1.0
1.5	2.0 2.5
FLOPS (×106)
(a)
3.0	3.5	4.0
0.2
0.0
----------Student--------- Teacher ∙ individual Student
0.5
4 3
O O
QSΦJJ TlN
0.5
1.0
1.5	2.0 2.5
FLOPS (×106)
3.0	3.5	4.0
0.5
0.0.
ClSəjj-J-JN
Student Teacher ∙ individual Student
0.2
0.00	0.05	0.10	0.15	0.20	0.25	0.30
Num. Parameters (×106)
(b)
Student Teacher ∙ individual Student
0.5
O O
ClSəjj --IN
0.2
0.00	0.05	0.10	0.15	0.2。	0.25	0.30
Num. Parameters (×106)
(c)	(d)
Figure 2: NLL-Storage-Computation tradeoff while using CNNs on MNIST with masking rate
29%. (a,b) Test negative log likelihood of posterior predictive distribution vs FLOPS found using
exhaustive search and group '1∕'2 with pruning. (c,d) Test negative log likelihood of posterior
predictive distribution vs storage found using exhaustive search and group `1 /`2 with pruning.
The optimal student model for this configuration is obtained with group '1∕'2 pruning. It has
approximately 6.6× the number of parameters and 6.4× the FLOPS of the base student model.
for moderate values of the masking rate. This fact is explained through the observation that when
the masking rate is low, posterior uncertainty is low, and distillation is relatively easy. On the other
hand, when the masking rate is high, the teacher essentially outputs the uniform distribution for every
example, which is very easy for the student to represent. As a result, the moderate values of the
masking rate result in the hardest distillation problem and thus the largest performance gap. For
varying masking rates, we see exactly the same trend for the gap in posterior entropy predictions on
MNIST. However, the gap for entropy prediction increases as a function of data set size for CIFAR10.
Finally, as we would expect, the performance of distillation using the Uo estimator is almost always
better than that of the Us estimator (refer Appendix B).
The key finding of this experiment is simply that the quality of the approximations provided by the
student model varies as a function of properties of the underlying data set. Indeed, restricting the
student architecture to match the teacher can sometimes result in significant performance gaps. In the
next experiment, we address the problem of searching for improved student model architectures.
4.4 Experiment 3: Towards Student Model Architecture Search
In this experiment, We compare the exhaustive search to the group '1∕'2 (group lasso) regularizer
combined with pruning. For the pruning approach, we start with the largest student model considered
under exhaustive search, and prune back from there using different regularization parameters λ,
leading to different student model architectures. We present results in terms of performance versus
computation time (estimated in FLOPS), as well as performance vs storage cost (estimated in number
of parameters). As performance measures for the posterior predictive distribution, we consider
accuracy and negative log likelihood. For entropy, we use mean absolute error. In all cases results are
reported on test data. We consider both fully connected and convolutional models.
Figure 2 shows results for negative the log likelihood (NLL) of the convolutional model on MNIST
with masking rate 29% and 60,000 training samples. We select this setting as illustrative of a difficult
case for posterior predictive distribution distillation. We plot NLL vs FLOPS and NLL vs storage for
all points encountered in each search. The solid blue line indicates the Pareto frontier.
9
Under review as a conference paper at ICLR 2020
First, we note that the baseline student model (with architecture matching the teacher) from Experi-
ment 2 on MNIST achieves an NLL of 0.469 at approximately 0.48 × 106 FLOPs and 0.03 × 106
parameters on this configuration of the data set. We can see that both methods for selecting student
architectures provide a highly significant improvement over the baseline student architectures. On
MNIST, the NLL is reduced to 0.30. Further, We can also see that the group '1∕'2 approach is able to
obtain much better NLL at the same computation and storage cost relative to the exhaustive search
method. Lastly, the group '1 /'2 method is able to obtain models on MNIST at less than 50% the
computational cost needed by the baseline model With only a small loss in performance. Results for
other models and distillation targets shoW similar trends and are presented in Appendix B. Additional
experimental details are given in Appendix A.2.
In summary, the key finding of this experiment is that the capacity of the student model has a
significant impact on the performance of the distillation procedure, and methods for optimizing the
student architecture are needed to achieve a desired speed-storage-accuracy trade-off.
5	Conclusions & Future Directions
We have presented a frameWork for distilling expectations With respect to the Bayesian posterior
distribution of a deep neural netWork that generalizes the Bayesian Dark KnoWledge approach in
several significant directions. Our results shoW that the performance of posterior distillation can be
highly sensitive to the architecture of the student model, but that basic architecture search methods
can help to identify student model architectures With improved speed-storage-accuracy trade-offs.
There are many directions for future Work including considering the distillation of a broader class
of posterior statistics including percentiles, assessing and developing more advanced student model
architecture search methods, and applying the frameWork to larger state-of-the-art models.
References
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep netWorks. In
Advances in Neural Information Processing Systems,pp. 2270-2278, 2016.
Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark
knoWledge. In Advances in Neural Information Processing Systems, pp. 3438-3446, 2015.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT, 2018.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural netWorks.
ArXiv, abs/1803.03635, 2018.
Ian GoodfelloW, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Alex Graves. Practical variational inference for neural netWorks. In Advances in neural information
processing systems, pp. 2348-2356, 2011.
Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition With deep
bidirectional lstm. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE
Workshop on, pp. 273-278. IEEE, 2013a.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition With deep recurrent
neural netWorks. In Acoustics, speech and signal processing (icassp), 2013 ieee international
conference on, pp. 6645-6649. IEEE, 2013b.
YiWen Guo, Anbang Yao, and Yurong Chen. Dynamic netWork surgery for efficient dnns. In NIPS,
2016.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both Weights and connections for
efficient neural netWorks. In NIPS, 2015.
10
Under review as a conference paper at ICLR 2020
Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John
Tran, and William J. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense
training flow. ArXiv, abs/1607.04381, 2016.
Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network
pruning. IEEEInternational Conference on Neural Networks,pp. 293-299 vol.1, 1993.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.
2017 IEEE International Conference on Computer Vision (ICCV), pp. 1398-1406, 2017.
Christian Henning, Johannes von Oswald, Joao Sacramento, Simone Carlo Surace, Jean-Pascal
Pfister, and Benjamin F Grewe. Approximating the predictive distribution via adversarially-trained
hypernetworks. In Bayesian Deep Learning Workshop, NeurIPS (Spotlight) 2018, 2018.
Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning
of bayesian neural networks. In International Conference on Machine Learning, pp. 1861-1869,
2015.
Geoffrey Hinton and Drew Van Camp. Keeping neural networks simple by minimizing the description
length of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory.
Citeseer, 1993.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Mate Lengyel. Bayesian active learning for
classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2261-2269, 2016.
Xiaojie Jin, Xiao-Tong Yuan, Jiashi Feng, and Shuicheng Yan. Training skinny deep neural networks
with iterative hard thresholding methods. ArXiv, abs/1607.05423, 2016.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In NIPS, 1989.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. ArXiv,
abs/1705.08665, 2017.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural
network compression. 2017 IEEE International Conference on Computer Vision (ICCV), pp.
5068-5076, 2017.
Thomas P Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the
Seventeenth conference on Uncertainty in artificial intelligence, pp. 362-369. Morgan Kaufmann
Publishers Inc., 2001.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg,
1996. ISBN 0387947248.
11
Under review as a conference paper at ICLR 2020
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of
multilayer neural networks with continuous or discrete weights. In Advances in Neural Information
Processing Systems,pp. 963-971, 2014.
Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger Grosse, and Richard Zemel. Adversarial
distillation of bayesian neural network posteriors. arXiv preprint arXiv:1806.10317, 2018.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688,
2011.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep
neural networks. In Advances in neural information processing systems, pp. 2074-2082, 2016.
Yichi Zhang and Zhijian Ou. Learning sparse structured ensembles with stochastic gradient mcmc
sampling and network pruning. 2018 IEEE 28th International Workshop on Machine Learning for
Signal Processing (MLSP), pp. 1-6, 2018.
A Datasets and Model Details
A. 1 Datasets
As noted earlier in the paper, the original empirical investigation of Bayesian Dark Knowledge for
classification focused on the MNIST data set (LeCun, 1998). However, the models fit to the MNIST
data set have very low posterior uncertainty and we argue that it is thus a poor benchmark for assessing
the performance of posterior distillation methods. In this section, we investigate two orthogonal
modifications of the standard MNIST data set to increase uncertainty: reducing the training set size
and masking regions of the input images. Our goal is to produce a range of benchmark problems
with varying posterior predictive uncertainty. We also use the CIFAR10 data set (Krizhevsky et al.,
2009) in our experiments and employ the same subsampling technique.
MNIST: The full MNIST dataset consists of 60,000 training images and 10,000 test images, each
of size 28 × 28, distributed among 10 classes LeCun (1998). As a first manipulation, we consider
sub-sampling the labeled training data to include 10,000, 20,000, 30,000 or all 60,000 data cases in
the primary data set D when performing posterior sampling for the teacher model. Importantly, we
use all 60,000 unlabeled training cases in the distillation data set D0 . This allows us de-couple the
impact of reduced labeled training data on posterior predictive distributions from the effect of the
amount of unlabeled data available for distillation.
As a second manipulation, we generate images with occlusions by randomly masking out parts of
each available training and test image. For generating such images, we randomly choose a square
m × m region (mask) and set the value for pixels in that region to 0. Thus, the masking rate for a
28 X 28 MNIST image corresponding to the mask of size m X m is given by r = 28×m. We illustrate
original and masked data in Figure 3. We consider a range of square masks resulting in masking rates
between 0% and 86.2%.
(b) Processed images
< a ι m
I，3 L
3 e I 7
(a) Original images
Figure 3: Example MNIST data after masking with m = 14.
CIFAR10: The full CIFAR10 dataset consists of 50,000 training images and 10,000 test images,
each of size 32 X 32 pixels. We sub-sample the data into a primary training sets D containing 10,000,
12
Under review as a conference paper at ICLR 2020
20,000, and 50,000 images. As with MNIST, the sub-sampling is limited to training the teacher
model only and we utilize all the 50,000 unlabeled training images in the distillation data set D0 .
A.2 Models
To demonstrate the generalizability of our methods to a range of model architectures, we run our
experiments with both fully-connected, and convolutional neural networks. We note that our goal
in this work is not to evaluate the GPED framework on state-of-the-art architectures, but rather to
provide illustrative results and establish methodology for assessing the impact of several factors
including the level of uncertainty and the architecture of the student model.
Teacher Models: We begin by defining the architectures used for the teacher model as follows:
1.	FCNN (MNIST): We use a 3-layer fully connected neural network. The architecture used
is: Input(784)-FC(400)-FC(400)-FC(output). This matches the architecture used by Balan
et al. (2015).
2.	CNN (MNIST): For a CNN, we use two consecutive sets of 2D convolution and max-
pooling layers, followed by two fully-connected layers. The architecture used is: In-
put(1, (28,28))-Conv(num_kernels=10, kerneLsize=4, Stride=1) - MaXPool(kerneLsize=2)-
Conv(num_kernels=20, kerneLSiZe=4, stride=1) - MaxPool(kerneLSiZe=2) - FC (80) - FC
(output).
3.	CNN (CIFAR10): Similar to the CNN architecture used for MNIST, we use two consec-
utive sets of 2D convolution and max-pooling layers followed by fully-connected layers.
Conv(num_kernels=16, kerneLSiZe=5) - MaXPool(kerneLSiZe=2) - Conv(num_kernels=32,
kerneISiZe=5) - MaxPool(kerneLSiZe=2) - FC(200) - FC (50) - FC (output).
In the architectures mentioned above, the “output” siZe will change depending on the eXpectation that
we’re diStilling. For claSSification, the output SiZe will be 10 for both dataSetS, while for the caSe of
entropy, it will be 1. We uSe ReLU non-linearitieS everywhere between the hidden layerS. For the
final output layer, softmax iS uSed for claSSification. In the caSe of entropy, we uSe an exponential
activiation to enSure poSitivity.
Student Models: The Student modelS uSed in our experimentS uSe the above mentioned architectureS
aS the baSe architecture. For explicitly Searching the Space of the Student modelS, we uSe a Set of width
multiplierS Starting from the teacher architecture. The Space of Student architectureS correSponding
to each teacher model defined earlier iS given below. The width multiplier valueS of K1 and K2 are
determined differently for each of the experimentS, and thuS will be mentioned in later SectionS.
1.	FCNN (MNIST): InPUt(784)-FC(400 ∙ KI)-FC(400 ∙ K2)-FC(output).
2.	CNN (MNIST): Input(1, (28,28))-Conv(num_kernels=[10 ∙ KJ, kerneLSiZe=4, stride=1)-
MaxPool(kerneLSiZe=2) - Conv(num_kernels= [20 ∙ KJ, kerneLSiZe=4, stride=1) - Max-
Pool(kernel_size=2) - fC ([80 ∙ KC) - FC (output).
3.	CNN (CIFAR10): Input(3, (32,32))-Conv(num_kernels=[16 ∙ KJ kerneLSiZe=5)
-MaxPool(kerneLSiZe=2) - Conv(num_kernels= [16 ∙ Kj, kerneLSiZe=5) - Max-
Pool(kernel-size=2) - FC ([200 ∙ K2C)- FC ([50 ∙ K2C)- FC (output).
Model and Distillation Hyper-Parameters: We run the distillation procedure using the following
hyperparameters: fixed teacher learning rate ηt = 4 × 10-6 for models on MNIST and ηt = 2 × 10-6
for models on CIFAR10, teacher prior precision τ = 10, initial student learning rate αs = 10-3,
student dropout rate p = 0.5 for fully-connected models on MNIST (and Zero otherwise), burn-in
iterations B = 1000, thinning interval H = 100 for distilling predictive means and H = 10 for
distilling entropy values, and total training iterations T = 106. For training the student model, we use
the Adam algorithm (instead of plain steepest descent as indicated in Algorithm 1) and set a learning
schedule for the student such that it halves its learning rate every 200 epochs for models on MNIST,
and every 400 epochs for models on CIFAR10. Also, note that we only apply the regulariZation
function R(φs) while doing Group '1∕'2 pruning. Otherwise, We use dropout as indicated before.
Hyper-parameters for Group '1 /'2 pruning experiments: For experiments involving group '1∕'2
regulariZer, the regulariZation strength values λ are chosen from a log-scale ranging from 10-8to10-3.
13
Under review as a conference paper at ICLR 2020
No. Oftraining samples	No. Oftraining samples
(a)	(b)
Masking Rate
・ 0.0
■ 0.03
■ 0.13
■ 0.29
■ 0.51
・ 0.8
No. Oftraining samples
(C)
Figure 4: Distillation performance using Fully-Connected Networks on MNIST while varying
data set size and masking rate. (a) Test negative log likelihood of the teacher posterior predictive
distribution. (b) Difference in test negative log likelihood between teacher and student posterior
predictive distribution estimates. (c) Difference between teacher and student posterior entropy
estimates on test data set.
When using Group '1∕'2 regularize], We do not use dropout for the student model. The number of
fine-tuning epochs for models on MNIST and CIFAR100 are 600 and 800 respectively. At the start of
fine-tuning, We also reinitialize the student learning rate αt = 10-4 for fully-connected models and
αt = 10-3 for convolutional models. The magnitude threshold for pruning is = 10-3.
B S upplemental Experiments and Results
Supplemental Results for Experiment 2: Robustness to Uncertainty In Figure 4, We demonstrate
the results of Experiment 2 (Section 4.3), on fully-connected netWorks for MNIST. Additionally, in
Tables [2-4], We provide a performance comparison betWeen Uo and Us estimators While distilling
posterior expectations for all model-data set combinations. We folloW the same experimental
configurations as in Experiment 2.
Supplemental Results for Experiment 3: Towards Student Model Architecture Search The
additional results from running Experiment 3 (Section 4.4) on different combinations of model type,
dataset, and performance metrics have been given in Figures[5 - 12].
14
Under review as a conference paper at ICLR 2020
Num. training samples	Masking rate	NLL (Teacher)	NLL (Student, Uo)	NLL (Student, Us)	MAE (Entropy, Uo)	MAE (Entropy, Us)
	0	0.048	0.214	0.218	0.025	0.030
	0.03	0.069	0.274	0.274	0.033	0.038
	0.13	0.161	0.509	0.509	0.058	0.069
10000	0.29	0.394	0.902	0.907	0.115	0.129
	0.51	1.099	1.615	1.630	0.194	0.170
	0.8	2.298	2.301	2.301	0.016	0.019
	0	0.034	0.126	0.126	0.020	0.021
	0.03	0.054	0.180	0.181	0.026	0.030
	0.13	0.123	0.342	0.344	0.053	0.066
20000	0.29	0.326	0.684	0.697	0.104	0.122
	0.51	1.050	1.369	1.378	0.145	0.150
	0.8	2.298	2.300	2.299	0.016	0.020
	0	0.028	0.084	0.086	0.017	0.019
	0.03	0.044	0.132	0.134	0.024	0.027
30000	0.13	0.106	0.292	0.294	0.051	0.061
	0.29	0.300	0.620	0.618	0.106	0.120
	0.51	1.044	1.307	1.308	0.130	0.141
	0.8	2.296	2.297	2.296	0.017	0.021
	0	0.022	0.053	0.053	0.016	0.017
	0.03	0.035	0.088	0.090	0.025	0.026
60000	0.13	0.090	0.219	0.221	0.049	0.058
	0.29	0.267	0.463	0.472	0.108	0.120
	0.51	1.024	1.184	1.187	0.118	0.127
	0.8	2.297	2.297	2.297	0.020	0.023
Table 2: Performance comparison between Uo and Us estimators for convolutional neural network
on MNIST. The NLL results correspond to the case of distilling the posterior predictive distribution
while the MAE on entropy results correspond to the case of distilling the expectation of predictive
entropy.
15
Under review as a conference paper at ICLR 2020
Num. training samples	Masking rate	NLL (Teacher)	NLL (Student, Uo)	NLL (Student, Us)	MAE (Entropy, Uo)	MAE (Entropy, Us)
	0	0.137	0.184	0.243	0.013	0.018
	0.03	0.180	0.233	0.300	0.018	0.023
10000	0.13	0.312	0.389	0.483	0.031	0.040
	0.29	0.556	0.637	0.760	0.059	0.089
	0.51	1.183	1.229	1.371	0.111	0.135
	0.8	2.103	2.111	2.129	0.023	0.019
	0	0.089	0.115	0.161	0.011	0.015
	0.03	0.131	0.165	0.220	0.014	0.021
20000	0.13	0.230	0.280	0.366	0.024	0.042
	0.29	0.452	0.510	0.607	0.049	0.104
	0.51	1.080	1.120	1.215	0.094	0.112
	0.8	2.104	2.108	2.117	0.019	0.021
	0	0.071	0.083	0.124	0.011	0.014
	0.03	0.107	0.129	0.180	0.015	0.021
30000	0.13	0.201	0.243	0.314	0.028	0.052
	0.29	0.414	0.459	0.555	0.062	0.105
	0.51	1.044	1.082	1.172	0.091	0.105
	0.8	2.089	2.092	2.101	0.022	0.023
	0	0.052	0.054	0.082	0.016	0.020
	0.03	0.081	0.094	0.133	0.023	0.034
60000	0.13	0.155	0.186	0.240	0.043	0.068
	0.29	0.360	0.398	0.471	0.086	0.109
	0.51	1.010	1.033	1.107	0.106	0.099
	0.8	2.088	2.089	2.094	0.021	0.022
Table 3: Performance comparison between Uo and Us estimators for fully-connected network on
MNIST. The NLL results correspond to the case of distilling the posterior predictive distribution
while the MAE on entropy results correspond to the case of distilling the expectation of predictive
entropy.
Num. training samples	NLL (Teacher)	NLL (Student, Uo)	NLL (Student, Us)	MAE (Entropy, Uo)	MAE (Entropy, Us)
10000	0.912	1.372	1.391	0.144	0.192
20000	0.798	1.184	1.179	0.210	0.231
50000	0.671	0.924	0.932	0.245	0.290
Table 4: Performance comparison between Uo and Us estimators for convolutional neural network on
CIFAR10. The NLL results correspond to the case of distilling the posterior predictive distribution
while the MAE on entropy results correspond to the case of distilling the expectation of predictive
entropy.
16
Under review as a conference paper at ICLR 2020
(a)	(b)
Num. Parameters (×106)
(c)
(d)
Num. Parameters (×106)
Figure 5: Accuracy-Storage-Computation tradeoff while using CNNs on MNIST with masking rate
29%. (a) Test accuracy using posterior predictive distribution vs FLOPS found using exhaustive
search. (b) Test accuracy using posterior predictive distribution vs FLOPS found using group
`1 /`2 with pruning. (c) Test accuracy using posterior predictive distribution vs storage found using
exhaustive search. (d) Test accuracy using posterior predictive distribution vs storage found using
group `1 /`2 with pruning. The optimal student model for this configuration is obtained with group
`1 /`2 pruning. It has approximately 6.6× the number of parameters and 6.4× the FLOPS of the base
student model.
17
Under review as a conference paper at ICLR 2020
FLOPS (×106)
FLOPS (×106)
(a)
Num. Parameters (×106)
(c)
Figure 6: Entropy Error-Storage-Computation tradeoff while using CNNs on MNIST with masking
rate 29%. (a) Test mean absolute error for posterior entropy vs FLOPS found using exhaustive
search. (b) Test mean absolute error for posterior entropy Vs FLOPS found using group '1∕'2
with pruning. (c) Test mean absolute error for posterior entropy vs storage found using exhaustive
search. (d) Test mean absolute error for posterior entropy VS storage found using group '1∕'2 with
pruning. The optimal student model for this configuration is obtained with group '1∕'2 pruning. It
has approximately 1.8× the number of parameters and 4.3× the FLOPS of the base student model.
(b)
Num. Parameters (×106)
(d)
18
Under review as a conference paper at ICLR 2020
(a)
(b)
(c)
Figure 7: Accuracy-Storage-Computation tradeoff while using Fully-connected networks on MNIST
with masking rate 29%. (a) Test accuracy using posterior predictive distribution vs FLOPS found
using exhaustive search. (b) Test accuracy using posterior predictive distribution vs FLOPS found
using group '1∕'2 with pruning. (C) Test accuracy using posterior predictive distribution Vs storage
found using exhaustive search. (d) Test accuracy using posterior predictive distribution vs storage
found using group '1∕'2 with pruning. The optimal student model for this configuration is obtained
with group '1∕'2 pruning. It has approximately 9.9× the number of parameters and 10× the FLOPS
of the base student model.
Num. Parameters (×106)
(d)
19
Under review as a conference paper at ICLR 2020
(a)
(b)
Num. Parameters (×106)	Num. Parameters (×106)
(c)	(d)
Figure 8: NLL-Storage-Computation tradeoff while using Fully-connected networks on MNIST with
masking rate 29%. (a) Test negative log likelihood of posterior predictive distribution vs FLOPS
found using exhaustive search. (b) Test negative log likelihood of posterior predictive distribution vs
FLOPS found using group '1 /'2 with pruning. (c) Test negative log likelihood of posterior predictive
distribution vs storage found using exhaustive search. (d) Test negative log likelihood of posterior
predictive distribution vs storage found using group '1/'2 with pruning. The optimal student model
for this configuration is obtained with group '1 /'2 pruning. It has approximately 9.9× the number of
parameters and 10× the FLOPS of the base student model.
20
Under review as a conference paper at ICLR 2020
(a)
(b)
Num. Parameters (×106)
(c)
Figure 9: Entropy Error-Storage-Computation tradeoff while using Fully-connected networks on
MNIST with masking rate 29%. (a) Test mean absolute error for posterior entropy vs FLOPS found
using exhaustive search. (b) Test mean absolute error for posterior entropy vs FLOPS found using
group `1 /`2 with pruning. (c) Test mean absolute error for posterior entropy vs storage found using
exhaustive search. (d) Test mean absolute error for posterior entropy vs storage found using group
'1∕'2 with pruning. The optimal student model for this configuration is obtained with group '1∕'2
pruning. It has approximately 4.2× the number of parameters and 4.2× the FLOPS of the base
student model.
Num. Parameters (×106)
(d)
21
Under review as a conference paper at ICLR 2020
1.2
φ ɪ-ʊ
t
J 0.9
N
0.8
0.7
0.0	2.5
1.1
----Student NLL (Test)
—Teacher NLL (Test)
• Individual Student
I I l IJ
1.2
5.0	7.5	10.0 12.5 15.0 17.5
FLOPS (×106)
0.7
0.0
0.9.8
loo
QSΦJJ -∏ N
2.5	5.0	7.5	10.0 12.5 15.0 17.5
FLOPS (×106)
(a)	(b)
1.2
出10
d 0.9
0.8
0.7
0.0	0.5	1.0	1.5
Num. Parameters (×106)
1.2
0.7
2.0	0.0
1.1
0.8
.0.9
1 O
(⅛①.L)TlN
0.5	1.0	1.5
Num. Parameters (×106)
2.0
(c)	(d)
Figure 10: NLL-Storage-Computation tradeoff while using CNNs on CIFAR10 with training set size
of 20,000 samples. (a) Test negative log likelihood of posterior predictive distribution vs FLOPS
found using exhaustive search. (b) Test negative log likelihood of posterior predictive distribution vs
FLOPS found using group '1 /'2 with pruning. (c) Test negative log likelihood of posterior predictive
distribution vs storage found using exhaustive search. (d) Test negative log likelihood of posterior
predictive distribution vs storage found using group '1/'2 with pruning. The optimal student model
for this configuration is obtained with group '1 /'2 pruning. It has approximately 4.7× the number of
parameters and 5.2× the FLOPS of the base student model.
22
Under review as a conference paper at ICLR 2020
0.750
0.725
0.625
0.600
g 0.700
& 0.675
S
5 0.650
FLOPS (×106)
0.750
0.725
0.625
0.600
g 0.700
& 0.675
S
5 0.650
(a)
(b)
Num. Parameters (×106)	Num. Parameters (×106)
(C)	(d)
Figure 11: Accuracy-Storage-Computation tradeoff while using CNNs on CIFAR10 with sub-
sampling training data to 20,000 samples. (a) Test accuracy using posterior predictive distribution vs
FLOPS found using exhaustive search. (b) Test accuracy using posterior predictive distribution vs
FLOPS found using group 'ι /'2 with pruning. (c) Test accuracy using posterior predictive distribution
vs storage found using exhaustive search. (d) Test accuracy using posterior predictive distribution vs
storage found using group `1 /`2 with pruning. The optimal student model for this configuration is
obtained with group '1 /'2 pruning. It has approximately 5.4× the number of parameters and 5.6×
the FLOPS of the base student model.
23
Under review as a conference paper at ICLR 2020
_ 0.250
φ
t 0-225
a. 0.200
ɑ 0.175
≥
h 0.150
c
出 0.125
z 0.100
FLOPS (×106)
ð 0.20
0.10
D 0.18
4-)
n
ɔ 0.16
O
&0.14
目 0.12
FLOPS (×106)
(a)
Num. Parameters (×106)
(c)
Figure 12: Entropy Error-Storage-Computation tradeoff while using CNNs on CIFAR10 with sub-
sampling training data to 20,000 samples. (a) Test mean absolute error for posterior entropy vs
FLOPS found using exhaustive search. (b) Test mean absolute error for posterior entropy vs FLOPS
found using group '1∕'2 with pruning. (c) Test mean absolute error for posterior entropy Vs storage
found using exhaustive search. (d) Test mean absolute error for posterior entropy vs storage found
using group '1∕'2 with pruning. The optimal student model for this configuration is obtained with
group '1∕'2 pruning. It has approximately 1.6× the number of parameters and 2.8× the FLOPS of
the base student model.
(b)
Num. Parameters (×106)
(d)
24