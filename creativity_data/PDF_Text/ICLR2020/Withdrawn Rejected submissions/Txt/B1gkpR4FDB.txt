Under review as a conference paper at ICLR 2020
Statistical Adaptive Stochastic Optimization
Anonymous authors
Paper under double-blind review
Ab stract
We investigate statistical methods for automatically scheduling the learning rate
(step size) in stochastic optimization. First, we consider a broad family of stochas-
tic optimization methods with constant hyperparameters (including the learning
rate and various forms of momentum) and derive a general necessary condition
for the resulting dynamics to be stationary. Based on this condition, we develop
a simple online statistical test to detect (non-)stationarity and use it to automati-
cally drop the learning rate by a constant factor whenever stationarity is detected.
Unlike in prior work, our stationarity condition and statistical test apply to differ-
ent algorithms without modification. Finally, we propose a smoothed stochastic
line-search method that can be used to warm up the optimization process before
the statistical test can be applied effectively. This removes the expensive trial and
error for setting a good initial learning rate. The combined method is highly au-
tonomous and it attains state-of-the-art training and testing performance in our
experiments on several deep learning tasks.
1	Introduction
We study adaptive stochastic optimization methods in the context of large-scale machine learning.
Specifically, we consider the stochastic optimization problem
minimize	F(x) , Eξ fξ (x),	(1)
where ξ is a random variable representing data sampled from some (unknown) probability distribu-
tion, x ∈ Rp represents the parameters of the machine learning model (e.g., the weight matrices in
a neural network), and fξ is the loss function associated with data ξ. The objective function F is the
expectation of fξ over the distribution of the data.
Many stochastic optimization methods for solving problem (1) can be written in the form of
xk+1 = xk - αkdk,	(2)
where dk is a (stochastic) search direction and αk > 0 is the step size or learning rate. In the
classical stochastic gradient descent (SGD) method,
dk= gk，Vfξk(Xk),	⑶
where ξk is a training example (or a mini-batch of examples) randomly generated at iteration k .
This method traces back to the seminal work of Robbins & Monro (1951), and it has become very
popular in machine learning (e.g., Bottou, 1998; Goodfellow et al., 2016). Many modifications of
SGD aim to improve its theoretical and practical performance. For example, Gupal & Bazhenov
(1972) studied a stochastic analog of the heavy-ball method (Polyak, 1964), where
dk = (1 - βk)gk + βkdk-1,	(4)
and βk ∈ [0, 1) is the momentum coefficient. Sutskever et al. (2013) proposed to use a stochastic
variant of Nesterov’s accelerated gradient method (Nesterov, 2004), where
dk =Vfξk(xk - αkβkdk-1) + βkdk-1.	(5)
Other recent variants include, e.g., Jain et al. (2018) and Ma & Yarats (2019).
Theoretical conditions for the asymptotic convergence of SGD are well studied, and they mostly
focus on polynomially decaying learning rates of the form αk = a/(b + k)c for some a, b > 0
1
Under review as a conference paper at ICLR 2020
and 1/2 < c ≤ 1 (e.g., Robbins & Monro, 1951; Polyak & Juditsky, 1992). Similar conditions
for the stochastic heavy-ball methods are also established (e.g., Gupal & Bazhenov, 1972; Polyak,
1977). However, these learning rate schedules still require significant hyperparameter tuning efforts
in modern machine learning practice.
Adaptive rules for adjusting the learning rate and other parameters on the fly have been developed
in both the stochastic optimization literature (e.g., Kesten, 1958; Mirzoakhmedov & Uryasev, 1983;
RUszczynski & Syski, 1983; 1984; 1986a;b; Delyon & Juditsky, 1993) and by the machine learning
community (e.g., Jacobs, 1988; Sutton, 1992; Schraudolph, 1999; Mahmood et al., 2012; Baydin
et al., 2018). More recently, adaptive algorithms that use diagonal scaling—replacing αk in (2) with
an adjustable diagonal matrix—have become very popular (Duchi et al., 2011; Tieleman & Hinton,
2012; Kingma & Ba, 2014, e.g.,). Despite these advances, costly hand-tuning efforts are still needed
to obtain good (generalization) performance in practice (Wilson et al., 2017).
In this paper, we focus on statistical methods for automatically scheduling the learning rate. This line
of work goes back to Kesten (1958), who used the change of sign of the inner products of consecutive
stochastic gradients as a statistical indicator of slow progress and as a trigger to decrease the learning
rate (see extensions in Delyon & Juditsky, 1993). Pflug (1983; 1988) considered the dynamics
of SGD with constant step size for minimizing convex quadratic functions and derived necessary
conditions for the resulting process to be stationary. Pflug (1983; 1990) also devised a sequential
statistical test to detect stationarity; when the test fires, the step size is decreased by a constant
factor. RUszczynski & Syski (1983) used online statistical tests to check if the present step size and
momentum constants satisfy certain optimality conditions and adjust them if the conditions are not
satisfied. However, Lang et al. (2019) found that these methods have limited success in machine
learning applications due to their reliance on quadratic approximation and/or strong assumptions on
the noise models.
Most recently, Yaida (2018) derived fluctuation-dissipation relations (necessary conditions) that
characterized the stationary behavior of stochastic gradient methods with constant learning rate and
momentum. These relations hold exactly for any stationary state (regardless of the loss function or
the noise model), so they can be very effective at detecting stationarity and can serve as a reliable
indicator of when to decrease the learning rate. Lang et al. (2019) devised a more rigorous statisti-
cal test for Yaida’s conditions and obtained robust and competitive performance on common deep
learning tasks.
Our contributions in this paper are threefold:
•	We consider a broad family of stochastic optimization methods with constant hyperparameters
(including the learning rate and various forms of momentum) and derive a more general neces-
sary condition (than Yaida’s) for the associated learning process to be stationary. Yaida (2018)
established a “master equation,” from which different stationarity conditions can be derived for
different variants of stochastic gradient methods (some of them may not admit a usable analyti-
cal form). We derive a simple “master condition” that works for different methods without any
change and holds exactly for any stationary process. Because of its conceptual and analytical
simplicity, it greatly simplifies implementation and deployment in software packages.
•	We develop a simple statistical test for checking stationarity based on our “master condition.” It
is a simple confidence interval test (checking if it contains zero), as opposed to the “equivalence
test” for Yaida’s condition used by Lang et al. (2019). This simple test is as robust as the
equivalence test and avoids the use of an additional hyperparameter.
•	A major challenge for almost all adaptive stochastic gradient methods is how to set the initial
learning rate appropriately without expensive trial and error. We propose a smoothed stochastic
line-search method that can be used to warm up the optimization process. Starting from any
safe, small learning rate, this method automatically increases it to a suitably large value for fast
initial convergence. Afterwards, the statistical test for stationarity can be applied to gradually
reduce the learning rate and obtain finer convergence.
Combining the three components above, we obtain a highly autonomous algorithm called SALSA
(Stochastic Approximation with Line-search and Statistical Adaptation). We conduct extensive ex-
periments on several deep learning tasks to test its training and testing perfromance as well as its
robustness to changes in the hyperparameter settings. SALSA matches the best performance of
hand-tuned methods across different deep learning tasks using default hyperparameters.
2
Under review as a conference paper at ICLR 2020
2	Neces sary conditions for stationarity
We consider general stochastic optimization methods in the form of (2) with a constant learning rate:
xk+1 = xk - αdk.	(6)
We assume that the stochastic search direction dk is generated with time-homogeneous dynamics;
In particular, any additional hyperparameters involved must be constant (not depending on k). As an
example, we consider the search directions generated by the family of Quasi-Hyperbolic Momentum
(QHM) methods (Ma & Yarats, 2019):
hk= (1 - β)gk + βhk-1,	(7)
dk = (1 - ν)gk + νhk,	()
where 0 ≤ β < 1 and 0 ≤ ν ≤ 1. With β = 0 or ν = 0, it recovers the SGD direction (3). With
ν = 1, it recovers the stochastic heavy-ball direction (4). With 0 < β = ν < 1, it is equivalent
to the	direction	with Nesterov momentum given in (5) (Sutskever	et al.,	2013; Gitman	et al.,	2019).
We assume that	the	dynamics of (6), e.g., driven by the stochastic gradients	gk through	(7), is	stable.
Stability regions of the hyperparameters in QHM are characterized by Gitman et al. (2019).
In addition, we assume that the stochastic process {xk }, as k → ∞, becomes stationary. Recall
that {xk} is (strongly) stationary if the joint distribution of any subset of the sequence is invariant
with respect to simultaneous shifts in the time index (e.g., Dembo, 2013, Section 3.2.3). As a direct
consequence, for any deterministic function φ : Rp → R, we have
Eπφ(xk+1) = Eπ φ(xk),	∀k,	(8)
where π denotes the stationary distribution of {xk}. If the test function φ is smooth, then we use
Taylor expansion to obtain
φ(xk+1) = φ(xk - adk) = φ(xk) - α(Vφ(xk),dk〉+ 哆(V2φ(xk)dk, dk)+ O(a3).
Taking expectations on both sides of the above equality and applying (8), we have
E∏ [<Vφ(xk),d" - 2(V2φ(xk)dk,dk)] = O(a2).	(9)
For an arbitrary smooth function φ, it is very hard in practice to characterize or approximate the
O(α2) term on the right-hand side. In addition, computing the Hessian-vector product V2φ(xk)dk
can be very costly. Therefore, we only consider simple quadratic functions for which the O(α2)
term in (9) vanishes. In particular, the choice of φ(x) = (1/2)kxk2 results in
EnKxk,dk〉- α kdkk2] =0.	(10)
This condition holds exactly for any stochastic optimization method of the form (6) if it reaches
stationarity. Indeed, weak stationarity is sufficient since φ is a quadratic function of the state (e.g.,
Dembo, 2013, Section 3.2.3). Beyond stationarity, it requires no specific assumption on the loss
function or noise model for the stochastic gradients. Moreover, it can be applied to different methods
without any change. We call this the master condition for stationarity.
Yaida (2018) focused on state perturbation along gk , Vfξk (xk) to obtain his “master equation”
Eπφ(xk-αgk)] =Eπφ(xk)],	∀k.
This equation can be combined with the actual state update equation (6) to obtain more specific
stationarity conditions for different algorithms. For example, for the stochastic heavy-ball method
with dk = (1 - β)gk + βdk-1, the master equation leads to (Yaida, 2018)
E∏ [〈xk,gk〉- 2挣kdkk2i=0.	(11)
It is a simple exercise to show that this is equivalent to our master condition (10). For the QHM
update (7), a more complex stationarity condition may also be derived, but it will still be equivalent
to (10). In practice, the single, simple master condition (10) is much more preferred.
If {xk} starts with a nonstationary distribution and converges to a stationary state, we have
lim E Kxk, dk) — 2∣∣dkk2] = 0.	(12)
k→∞	2
In the next section, we devise a simple statistical test to determine if the master condition fails to
hold. In this case, the learning process has not stalled, and we can continue with the same step size.
If we fail to detect non-stationarity (i.e., the dynamics may be approximately stationary), we would
like to reduce the learning rate α to allow for finer convergence.
3
Under review as a conference paper at ICLR 2020
Algorithm 1: SASA+: SASA with master condition and simple statistical testing
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
input: x0 ∈ Rp, α0 > 0, Nmin > 0, Ktest > 0, δ ∈ (0, 1), θ ∈ (0, 1), τ ∈ (0, 1)
α七—αo
ko — 0
for k = 0, ..., T - 1 do
Randomly sample ξk and compute stochastic search direction dk (e.g., using QHM)
xk+1 - Xk — adk
∆k 一 {xk ,dk〉— 2 kdkk2
N — dθ(k — ko)]
if N > Nmin and k mod Ktest == 0 then
(μN, &n) J sample mean and BM/OLBM variance of {∆k-N +1,..., ∆k}
if 0 ∈ μN ± t；-6/2 √N then
α J Ta
ko J k
end
end
16 end
17 output: xT (or the average of last epoch)
Table 1: List of hyperparameters of Algorithm 1
Parameter	Range	Explanation	Default value
Nmin	Z+	minimum number of samples for testing	min{1000, dn/be}
Ktest	Z+	period to perform statistical test	min{100, dn/be}
δ	(0, 1)	(1 — δ)-confidence interval	0.01 (99% confidence)
θ	(0, 1)	fraction of recent samples to keep (after reset)	1/4
τ	(0, 1)	learning rate drop factor	1/10
3	Statistical tests of (non-)stationarity
In order to test if the master condition (10) holds approximately, we collect the simple statistics
∆k，(Xk,dk) — 2 ∣∣dkk2 = (Xk + 1,dk) + 2 ∣∣dk k2.
Here the second expression for ∆k is obtained using the direct substitution xk+1 = Xk — adk, which
can be more convenient to implement if ∆k is collected after the state updates to xk+1.
In the language of statistical hypothesis testing (e.g., Lehmann & Romano, 2005), we make as our
null hypothesis that the dynamics (6) have reached a stationary distribution π. Ifwe have N samples
{∆k}, we know from equation (10) and the Markov chain CLT (see its application in Jones et al.
(20O6)) that as N → ∞, under the null hypothesis the mean statistic ∆ follows a normal distribution
with mean 0 and variance σ∆/√N. Our alternative hypothesis is that the dynamics (6) have not
reached stationarity. To test these hypotheses, we adopt the classical confidence interval test. More
specifically, we use the most recent N samples {∆k-N-1, . . . , ∆k} to compute the sample mean
μN and a variance estimator σN for σ∆. Then We can form the (1 — δ)-confidence interval
(μN — ωN, μN + ωN)
with half width
ωN = t
σN
(13)
.*
…√N,
where t↑-δ∕2 is the (1 — δ∕2) quantile of the Student,s t-distribution with degrees of freedom cor-
responding to that in the variance estimator σN. Because the sequence {∆k-N +ι,..., ∆k} are
highly correlated due to the underlying Markov dynamics, the classical formula for the sample vari-
ance (obtained by assuming i.i.d. samples) does not work for σN. We need to use more sophisticated
batch mean (BM) or overlapping batch mean (OLBM) variance estimators developed in the Markov
chain Monte Carlo literature (e.g., Jones et al., 2006; Flegal & Jones, 2010). See Lang et al. (2019)
for detailed explanation. We also list the formulas for computing BM and OLBM in Appendix A.
4
Under review as a conference paper at ICLR 2020
Algorithm 2: Smoothed Stochastic Line-Search (SSLS)
1
2
3
4
5
6
7
8
9
input: x0 ∈ Rp, α-1 > 0, sufficient decrease coefficient c ∈ (0, 1/2), line-search factors
ρinc ≥ 1, ρdec ∈ (0, 1), smoothing parameter γ ∈ [0, 1] and maximum LS count m
for k = 0, ..., T - 1 do
Sample ξk, compute gk J Vfξfc (Xk) and search direction dk (e.g., using QHM)
ηk J Pincak-1
while fξk (Xk — ηkgk) > fξk (Xk) - C ∙ ηkkgk k2 and ηk > P‰αk-ι do
I ηk J Pdecnk
end
αk J (1 - γ)αk-1 + γηk
Xk+1 J Xk - αkdk
10 end
11 output: XT
If the confidence interval in (13) contains 0, we fail to reject the null hypothesis that the learning
process is at stationarity, which means the learning rate should be decreased. Otherwise, we accept
the alternative hypothesis of non-stationarity and keep using the current constant learning rate.
Stochastic optimization methods equipped with such an adaptation scheme belong to the gen-
eral framework of SASA (Statistical Adaptive Stochastic Approximation) proposed by Lang et al.
(2019). Algorithm 1 is our new variant called SASA+. Table 1 lists its hyperparameters and their
default values (where n is total number of training examples and b is the mini-batch size).
Lang et al. (2019) focused on the stochastic heavy-ball method and devised a statistical test for the
condition (11). Another major difference is that they set non-stationarity as the null hypothesis and
stationarity as the alternative hypothesis (opposite to ours). This leads to a more complex “equiva-
lence test” that requires an additional hyperparameter. Our test is simpler and more straightforward,
and computationally as robust.
4	Smoothed stochastic line search
SASA+ can automatically decrease the learning rate to refine the last phase of the optimization
process, but it relies on an appropriate setting of the starting learning rate α0 for sufficient initial ex-
ploration. The appropriate initial learning rate varies substantially for different objective functions,
machine learning models, and training datasets, and setting it correctly without expensive trial and
error is a major challenge for all stochastic gradient methods, adaptive or not.
Several recent works (e.g., Schmidt et al., 2017; Vaswani et al., 2019) explore the use of classical
line-search techniques (e.g., Nocedal & Wright, 2006, Chapter 3) for solving stochastic optimization
problems with special structure. One of the main difficulties of applying line-search to stochastic
optimization is that the estimated step sizes may vary a lot from step to step and it may not capture
the appropriate step size for the average loss function. To overcome this difficulty, we propose a
smoothed stochastic line-search (SSLS) procedure listed in Algorithm 2.
During each step k, SSLS uses the classical Armijo line-search to find an appropriate step size ηk
for the randomly chosen function fξk (initialized by αk-1), then sets the overall learning rate to be
αk = (1 - γ)αk-1 + γηk,
where γ ∈ [0, 1]. When γ = 1, SSLS reduces to the stochastic Armijo line-search used by Vaswani
et al. (2019). A good choice is to set γ = b/n where n is the total number of training examples and
b is the mini-batch size. Suppose Pinc = 2 and ηk = 2αk-1 is accepted at step k, then
αk = (1 - γ)αk-1 + γ(2αk-1) = (1 + γ)αk-1.
If this happens at every iteration over one epoch (of dn/be iterations) and n b, then
ak+「n/b] =(1 + b/n)dn/beak ≈ e ∙ ak.	(14)
Therefore, the most aggressive growth of the learning rate is by a factor of e over one epoch. Such
a growing factor is reasonable for line search in deterministic optimization (e.g., Nesterov, 2013).
5
Under review as a conference paper at ICLR 2020
Algorithm 3: SALSA: SASA+ with warmup by SSLS
1	input: x0 ∈ Rp , number of steps for warmup Twu and total number of steps T > Twu
2	for k = 0, ..., Twu do
3	I Run SSLS (Algorithm 2)
4	end
5	for k = Twu + 1, ..., T do
6	I RUn SASA+ (Algorithm 1)
7	end
8	output: xT
Setting Y = ʌ/b/n leads to maximum growth of e2 over one epoch. A similar smoothing effect holds
for decreasing the learning rate as well. The smoothing scheme allows us to use standard increasing
and decreasing factors, such as ρinc = 2 and ρdec = 1/2. Without the smoothing scheme, Vaswani
et al. (2019) set ρinc = 2b/n to restrict dramatic growth of αk = ηk (equivalent to γ = 1 in SSLS).
However, the decreasing of αk = ηk can be excessive and premature, even with ρdec = 0.9.
Following the theoretical framework of Vaswani et al. (2019), it may be possible to show that SSLS
has similar convergence properties as stochastic Armijo line-search under the smooth and interpola-
tion assumptions, which we leave as a future research project. In this paper, we are mainly interested
in investigating its performance in practice. In particular, we use it on deep neural network models
with ReLU activations. Here the loss functions are non-smooth, and classical theoretical foundations
for line-search do not carry over. Nevertheless, we found SSLS to have robust performance in all of
our experiments. In order to handle the case of potential non-descent directions in the non-smooth
case, we exit the line search after a maximum of m tries by adding the condition ηk > ρdmec ηk for
staying in the line-search loop. By default, we set m = 10 together with ρdec = 1/2.
Finally, we combine SASA+ with SSLS to form Algorithm 3, which we call SALSA (Stochastic
Approximation with Line-search and Statistical Adaption). Without prior knowledge on the loss
function and training dataset, we start with a very small learning rate and use SSLS to gradually
increase it to be around a stationary value that is (automatically) customized to the problem. The
after Twu steps, we switch to SASA+ to gradually reduce the learning rate to settle down to a
(hopefully good) local minimum. We typically set the number of warm-up steps Twu to be equivalent
to a few tens of epochs. According to the calculation in (14), using 30 epochs would allow a potential
growth factor up to a reasonable fraction of e30 , which is sufficient for most applications. We could
avoid a warmup phase by running SASA+ and SLSS together until the first SASA+ drop, which
could only come after SLSS had reached a relatively stable value for αk , allowing the dynamics to
approach stationarity. However, we use a fixed warmup phase Twu in our experiments for simplicity.
Computational cost of SSLS. When we fix the number of training epochs, the wall-clock time of
Smoothed Stochastic Line Search (SSLS) is about 1.5 times of that of the plain stochastic optimiza-
tion algorithm. This 0.5 computational overhead is due to 2 function evaluations in each line search
step on average. Notice that the function evaluation is on the same minibatch and line search only
needs the function value without gradient. In SALSA, we switch from SSLS to SALSA at one-third
of the total training epochs. Therefore, the total computational overhead caused by using SSLS is
only 0.15 of the original total training time.
5	Experiments
We evaluate the performance of SASA+ (Algorithm 1), SSLS (Algorithm 2) and SALSA (Algo-
rithm 3) by conducting several experiments on two common deep learning datasets. We compare
SALSA to the following baselines: SGD with a hand-tuned constant-and-cut learning rate schedule
and Adam with a tuned warmup phase (e.g., Wilson et al., 2017; Lang et al., 2019). We also compare
SASA+ to the original SASA from Lang et al. (2019). We use weight decay in all experiments.
We use the default values in Table 1 for hyperparameters.
6
Under review as a conference paper at ICLR 2020
050
L
ssoUIUΓa!l
00
1ch
O
P
E
50
∙5。
O O
Figure 1: Comparison of different optimizers on CIFAR-10. All SASA, SASA+ and SALSA are
using SGD with momentum 0.9. SALSA starts from a small initial learning rate 0.01 while other
methods starts from an oracle maximal learning rate 1.0. SALSA witches from SSLS to SASA+ at
epoch 40.
CIFAR-10 We trained an 18-layer ResNet model (He et al., 2016) on CIFAR-10 (Krizhevsky &
Hinton, 2009) with random cropping and random horizontal flipping for data augmentation and
weight decay 0.0005. Row 1 of Figure 1 compares the best performance of each method. Here
SGM-hand uses α0 = 1.0 and β = 0.9 and drops α by a factor of 10 every 50 epochs. Adam has a
tuned global learning rate α0 and a “warmup” phase of50 epochs. With only a tuned α0 = 1e-4 (the
optimal value in our grid search of {1e-5 , 1e-4, 1e-3, 1e-2}), Adam is only able to reach around
93% test accuracy for this model. On the other hand, both SASA+ and SALSA are able to reach
similar performance with the hand-tuned SGD. While other methods starts from an oracle maximal
learning rate 1.0, SALSA starts from a small initial learning rate 0.01. The SSLS in the first stage of
SALSA is able to gradually grow the learning rate to a maximal value and then switches to SASA+
in the second stage.
Figure 2 shows the evolution of SASA+’s different statistics over the course of training the ResNet18
model on CIFAR-10 using the default parameters in Table 1. Between two jumps, the statistics ∆k
decays toward zero. As long as its confidence interval (13) does not overlap with 0, we are confident
that the process is not stationary yet and keep exploring with the current learning rate. Otherwise,
we decrease the learning rate and enter another cycle of approaching stationarity.
We give experiments that demonstrate the sensitivity of SASA+ to values around its defaults in
Figure 3. The top row shows that SASA+ is robust to the choice of the dropping factor τ. The larger
the τ is, the longer the process will stay between two drops, which can be also seen in Figure 2. The
middle row shows the effect of statistical parameter δ. Smaller δ leads to wider confidence interval,
and makes easier and thus earlier to fire the statistical test. The bottom row shows the effect of θ,
the fractions of samples to keep after reset. Smaller θ values lead to more frequent dropping, but do
not seem to impact the final performance.
Figure 4 shows the dynamics of SSLS and SALSA. In the upper row, SLSS increases the learning
rate in the initial phase, and then reaches a stable learning rate once the increases and decrease
balance out. With enough momentum (QHM β = ν = 0.9), the SSLS can further decrease the
learning rate automatically and achieve good training and testing performance. However, without
momentum (SGD), the stable learning rate gets too large, and its optimization performance is bad,
but the process is still stable and the learning rate does not blow up. In the lower row, SALSA
switches from SSLS to SASA+ when it reaches the maximal learning rate. Then even in the case
without momentum, SASA+ still works and finally gets as good performance as best hand-tuning
SGD. This shows that the SSLS is able to approximate the best hand-tuned maximal learning rate,
and the combination of SSLS and SASA+ is necessary to get final good optimization performance.
ImageNet We also test our algorithms on the large scale ImageNet dataset (Deng et al., 2009).
We use the ResNet18 architecture, random cropping and random horizontal flipping for data aug-
mentation and weight decay 0.0001. Row 1 of Figure 5 compares the performance of the dif-
ferent optimizers. Even with a tuned α0 = 1e-4 (the optimal value in our grid search of
{1e-5, 1e-4, 1e-3, 1e-2}) and allowed to have a long warmup phase (30 epochs), Adam failed
to match the generalization performance of SGM. On the other hand, both SASA+ and SALSA
were able to match the performance of hand-tuned SGM using the default values of its parameters.
Although both SASA+ and SALSA are using the NAG (equivalent to QHM (7) with β = ν = 0.9),
7
Under review as a conference paper at ICLR 2020

Figure 2: Statistics used in SASA+. Upper row: drop ratio τ = 1/2; lower row: τ = 1/10. The
left column shows the instantaneous value of ∆k . The middle column shows the confidence interval
of E[∆k] constructed by SASA+, which should contain 0 with high probability if the process is
stationary. The right column shows the variance estimated by different methods, where BM and
OLBM takes into the consideration of correlation in Markov chains and are more accurate.
J
Ooooo
0 9 8 7 6
1
(％) XDBJnDUB 4sI
£巴aUIUlBS
-/
0 5 0 5 0
0 7 5 2 0
L
SSOlu.IX
J
Ooooo
0 9 8 7 6
1
(％) AUBJnDUB 4sI
SSOlau'B's.IX
O 1
O Γ
1 O
£巴UIUlBS
Ooooo
0 9 8 7 6
1
(％) XDBJnDUB 4sI
O 1
O r
1 O
£巴UΠUB
∞
2
l
Figure 3:	Sensitivity analysis of SASA+ on CIFAR10, using β = 0.9 and ν = 1. The training loss,
test accuracy, and learning rate schedule for SASA+ using different values of τ (top row), δ (middle
row), and θ (bottom row) around the default values are shown.
they start with different initial learning rates and have quite different dynamics. SASA+(Nag) starts
with an initial learning rate of 1.0, while the SALSA(Nag) starts with a small learning rate 0.1 and
automatically determines the appropriate largest learning rate by using SSLS. As a consequence,
SASA+(Nag) reaches a stationary state much faster. Nevertheless, their final performances are
nearly the same, with SALSA(Nag) even slightly higher.
8
Under review as a conference paper at ICLR 2020
----SGDy= 0.05
----SGD γ= 0.0025
----QHM γ= 0.05
----QHM γ= 0.0025
IOlT-
O O - -
IIOC
1 ■
£巴uμuB
(％) AUBJnDUB 4soI
----SGD γ= 0.05
----SGD γ= 0.0025
----QHM γ= 0.05
----QHM γ= 0.0025
IOlT-
O O - -
IIOC
1 ■
£ 巴uμuB
Figure 4:	First row: SSLS. Second row: SALSA (warmup 40 epochs). Here, SGD with β =
0.0 and QHM with parameters β = 0.9 and ν = 0.9. SSLS is able to find a balance between
increasing and decreasing learning rate, and results in a stable large learning rate (SGD) or further
decreases the learning rate (QHM). In either case, switching to SASA+ (the lower row) gets good
final performance.
Row 2 of Figure 5 shows that SASA+ is simple and general enough to work with different stochas-
tic optimization methods with constant hyperparameters, i.e., SGM with β = 0.9, NAG, and the
recommended QHM setting (β = 0.999 and ν = 0.7). The dotted curves show the performance
of these methods with the best hand-tuned constant-and-cut learning rate schedule. The solid lines
show the automatic tuning by SASA+, which learns faster and finally achieves similar performance.
Figure 6 shows the sensitivity analysis of SSLS. From the left and middle panels, we can see that
the SSLS always converges to a large learning rate (1.806 in this case), which is comparable with
the best human-chosen value of 1.0. The right panel shows that the larger the sufficient descent
constant c is, the smaller the maximal learning rate is and the slower SSLS reaches to its maximal
learning rate. This is intuitive because larger c puts more limits on the SSLS to grow the learning
rate. Moreover, within 60 epochs, SSLS never decreases the learning rate again once it achieves a
large stable learning rate, so it is necessary to switch to SASA+ after SLSS reaches a stable learning
rate.
6 Conclusions
We presented SASA+, a simpler yet more powerful variant of the statistical adaptive stochastic ap-
proximation (SASA) method proposed by Lang et al. (2019). SASA+ is equipped with a single
statistical test for (non-)stationarity that works for a broad family of stochastic optimization meth-
ods without modification. This greatly simplifies its implementation and deployment in software
packages. While SASA+ focuses on how to automatically reducing the learning rate to obtain bet-
ter asymptotic convergence, we also propose a smoothed stochastic line-search method (SSLS) to
warm up the optimization process, thus removing the burden of expensive trial and error for setting
a good initial learning rate. The combined algorithm, SALSA, is highly autonomous and robust over
variations of its hyperparameters. Using the same default setting, SALSA obtained state-of-the-art
performance on several common deep learning models that is competitive with the best hand-tuned
optimizers.
References
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. On-
line learning rate adaptation with hypergradient descent. In Proceedings of the Sixth International
Conference on Learning Representations (ICLR), Vancouver, Canada, 2018.
9
Under review as a conference paper at ICLR 2020
6 4 2
SSOJ6UIU∙SJH
6 4 2
SSOJ6UIU∙SJI
O	50 IOO
Epoch
Oooo
7 6 5 4
累)穹JngV ⅛ΦF
-6-4-2
60
(求)穹JngV ⅛ΦF
F6F4N
5040
Figure 5: Upper: Comparison of different optimizers on ImageNet. All SASA runs use SGD with
momentum 0.9, while SASA+ and SALSA use NAG (QHM with β = ν = 0.9). SALSA starts
from a small initial learning rate 0.1 while other methods starts from an oracle maximal learning
rate 1.0. SALSA witches from SSLS to SASA+ at epoch 40. Lower: the first three curves are
stochastic optimization algorithms with hand-tuning learning rate, i.e., decrease every 30 epochs
as it is shown in the lower right panel. The last three curves are SASA+ combined with these 3
algorithms. SASA+ automatically adapt their learning rate (see the lower right panel), achieves
comparable and even slightly higher testing accuracy (see the lower middle panel).
əupj -g.silipəii PqOlU
Figure 6: Sensitivity analysis of SSLS around α0 = 0.01, γ = b/n = 0.0002, c = 0.1, with SGD
β = 0.9, ν = 1 on ImageNet.
Leon Bottou. Online algorithms and stochastic approximations. In David Saad (ed.), Online
Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998. URL
http://leon.bottou.org/papers/bottou-98x. Revised, Oct 2012.
B. Delyon and A. Juditsky. Accelerated stochastic approximation. SIAM Journal on Optimization,
3(4):868-881,1993.
Amir Dembo. Stochastic Processes. Lecture notes, Stanford University, 2013.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
James M Flegal and Galin L Jones. Batch means and spectral variance estimators in markov chain
monte carlo. The Annals of Statistics, 38(2):1034-1070, 2010.
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role momentum in
stochastic gradient methods. In Advances in Neural Information Processing Systems, volume 32,
2019.
10
Under review as a conference paper at ICLR 2020
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
A. M. Gupal and L. T. Bazhenov. A stochastic analog of the conjugate gradient method. Cybernetics,
8(1):138-140,1972.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual networks for image recog-
nition. In Proceedgins of the 29th IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770-778, 2016.
R. A. Jacobs. Increased rates of convergence through learning rate adaption. Neural Networks, 1:
295-307, 1988.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-
ing stochastic gradient descent for least squares regression. In Conference On Learning Theory,
pp. 545-604, 2018.
Galin L Jones, Murali Haran, Brian S Caffo, and Ronald Neath. Fixed-width output analysis for
markov chain monte carlo. Journal of the American Statistical Association, 101(476):1537-1547,
2006.
Harry Kesten. Accelerated stochastic approximation. Annals of Mathematical Statistics, 29(1):
41-59, 1958.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Hunter Lang, Pengchuan Zhang, and Lin Xiao. Using statistics to automate stochastic optimization.
In Advances in Neural Information Processing Systems, volume 32, 2019.
Erich L. Lehmann and Joseph P. Romano. Testing Statistical Hypotheses. Springer, 3rd edition,
2005.
Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and adam for deep learning. In Interna-
tional Conference on Learning Representations, 2019.
Ashique Rupam Mahmood, Richard S. Sutton, Thomas Degris, and Patrick M. Pilarski. Tuning-free
step-size adaption. In Proceedings of the IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 2121-2124, 2012.
F. Mirzoakhmedov and S. P. Uryasev. Adaptive step adjustment for a stochastic optimization algo-
rithm. Zh. Vychisl. Mat. Mat. Fiz., 23(6):1314-1325, 1983. [U.S.S.R. Comput. Math. Math. Phys.
23:6, 1983].
Y. Nesterov. Gradient methods for minimizing composite objective function. Mathematical Pro-
gramming, Series B, 140:125-161, 2013.
Yurii Nesterov. Introductory Lecture on Convex Optimization: A Basic Course. Kluwer Academic
Publishers, 2004.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, 2nd edition, 2006.
Georg Ch. Pflug. On the determination of the step size in stochastic quasigradient methods. Col-
laborative Paper CP-83-025, International Institute for Applied Systems Analysis (IIASA), Lax-
enburg, Austria, 1983.
Georg Ch. Pflug. Adaptive stepsize control in stochastic approximation algorithms. In Proceedings
of 8th IFAC Symposium on Identification and System Parameter Estimation, pp. 787-792, Beijing,
1988.
Georg Ch. Pflug. Non-asymptotic confidence bounds for stochastic approximation algorithms with
constant step size. MonatsheftefurMathematik, 110:297-314, 1990.
11
Under review as a conference paper at ICLR 2020
Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
Putational Mathematics and Mathematical Physics, 4(5):1-17,1964.
Boris T. Polyak. Comparison of the rates of convergence of one-step and multi-step optimization
algorithms in the presence of noise. Engineering Cybernetics, 15:6-10, 1977.
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and Optimization, 30(4):838-855, 1992.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400-407, 1951.
Andrzej RUszczynski and Wojciech Syski. Stochastic approximation method with gradient averag-
ing for unconstrained problems. IEEE Transactions on Automatic Control, 28(12):1097-1105,
1983.
Andrzej Ruszczynski and WojCieCh Syski. Stochastic approximation algorithm with gradient aver-
aging and on-line stepsize rUles. In J. Gertler and L. Keviczky (eds.), Proceedings of 9th IFAC
World Congress, pp. 1023-1027, Budapest, Hungary, 1984.
Andrzej RUSzCzynSki and Wojciech Syski. On convergence of the stochastic subgradient method
with on-line stepsize rules. Journal of Mathematical Analysis and Applications, 114:512-527,
1986a.
Andrzej RUSzCzynSki and Wojciech Syski. A method of aggregate stochastic subgradients with
on-line stepsize rules for convex stochastic programming problems. Mathematical Programming
Study, 28:113-131, 1986b.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Nicol N. Schraudolph. Local gain adaptation in stochastic gradient descent. In Proceedings of
Nineth International Conference on Artificial Neural Networks (ICANN), pp. 569-574, 1999.
David L Streiner. Unicorns do exist: A tutorial on proving the null hypothesis. The Canadian
Journal of Psychiatry, 48(11):756-761, 2003.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In Sanjoy Dasgupta and David McAllester (eds.), Pro-
ceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings
of Machine Learning Research, pp. 1139-1147, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.
Richard S. Sutton. Adapting bias by gradient descent: An incremental version of Delta-Bar-Delta. In
Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI’92), pp. 171-176.
The MIT Press, 1992.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julian. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Ad-
vances in Neural Information Processing Systems, volume 32, 2019.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
arXiv:1810.00004, 2018.
12
Under review as a conference paper at ICLR 2020
A MCMC variance estimators
Several estimators for the asymptotic variance of the history-average estimator of a Markov chain
have appeared in work on Markov Chain Monte Carlo (MCMC). Jones et al. (2006) gives a nice
example of such results. Here we simply list two common variance estimators, and we refer the
reader to that work and Lang et al. (2019) (from which we borrow notation) for appropriate context
and formality.
Batch means variance estimator. Let zZN be the history average estimator WithN samples, that
is, given samples {zi} from a Markov chain, ZN = N Pi=+N zi. Now form P batches from the N
samples, each of size q. Compute the “batch means” Zj = 1 Pi(=+jj^q-1 Zi for each batch j. Then
compute the batch means estimator using:
p-1
σN = -ɪv X(Zj- ZN )2.
N P-1j=0
(15)
The estimator is just the variance of the batch means around the history average ZN. This statistic
has P -1 degrees of freedom. As in Jones et al. (2006) and Lang et al. (2019), We take P = q = √N
when using this estimator.
Overlapping batch means variance estimator. The overlapping batch means (OLBM) estimator
Flegal & Jones (2010) has better asymptotic variance than the batch means estimator. The OLBM
estimator is conceptually the same, but it uses N - P + 1 overlapping batches of size P (rather than
disjoint batches) and has N - P degrees of freedom. It can be computed as:
Np
(N - p)(N - p +1)
N-p	p
X (ZN -1 x zj+i)2.
j=0	P i=1
(16)
B	The statistical tests in SASA and SASA+
As mentioned in our contributions, there are two main differences between SASA from Lang et al.
(2019) and SASA+. First, SASA+ has a much more general “master stationary condition” (10), so
it can be applied to any stochastic optimization method with constant hyperparameters (e.g., SGD,
stochastic heavy-ball, NAG, QHM, etc.), while the stationary condition in Yaida (2018) and SASA
proposed in Lang et al. (2019) (see Equation (11)) only applies to the stochastic heavy-ball method.
Second, the statistical tests used in SASA and SASA+ are quite different, as we elaborate below.
The difference starts from a conceptual change from SASA to SASA+. In SASA, one wants to con-
fidently detect stationarity. If stationarity is detected, one decreases the learning rate, and otherwise
keeps it the same. In SASA+, one wants to confidently detect non-stationarity. If non-stationarity
is detected, one keeps the learning rate the same, and otherwise decreases. This conceptual change
leads to a simpler and more rigorous statistical test in SASA+.
B.1	Equivalence test in SASA
To confidently detect stationarity, SASA has to set non-stationarity as null hypothesis and stationar-
ity as the alternative hypothesis. If one confidently rejects the null (non-stationarity), then one can
be confident that the process is stationarity. Instead of detecting stationarity, SASA simplifies to
only detect the necessary but not sufficient stationarity condition (11). Formally, the test in SASA
is
H0 : E[∆] 6= 0 vs. H1 : E[∆] = 0,	(17)
where samples of ∆, i.e., ∆k，(xk, dk) - α2Ildkk2, are collected along the training process. This
kind of test is called an equivalence test in statistics, see, e.g., Streiner (2003). There is no power1 in
the equivalence test (17), i.e., one cannot confidently reject the null hypothesis and prove stationarity
1In statistical hypothesis testing, power is the ability to reject the null hypothesis when it is false.
13
Under review as a conference paper at ICLR 2020
at all! Intuitively, even when the process is stationary, with only a finite number of (noisy) samples
{∆k}N= 0, the sample mean ∆n = 0 (with probability one) is always more likely to be the true
mean than the singleton 0. In other words, one can not deny that the process is probably infinitely
close to stationary but still non-stationary.
To gain power in the equivalence test, one needs to use domain knowledge to define an equivalence
interval. Formally, the true test in SASA is
Ho ： ∣E[∆]∣ > ZV vs. Hi : E[∆] ∈ [-Zν,Zν],	(18)
where ζν is the equivalence interval. In English, instead of the usual null hypothesis of not-equal-to-
zero in (17), now the null hypothesis is not-equal-to-zero by a margin ZV. In this case, when E[∆]'s
confidence interval is contained in the equivalence interval, i.e.,
卜 N - tLδ∕2 √NN, A N + tLδ∕2 /N] ⊂ [-ZpN，ZvN].	(19)
we are confident to reject the null hypothesis and to prove/accept H1 (the stationary condition is met
within an error tolerance). This is exactly the test in SASA Lang et al. (2019), see its Equation (10).
However, this equivalence test requires estimation of VN (that estimates the magnitude of ∆) and
an additional hyperparameter Z that controls the equivalence interval width. In SASA’s notation, the
equivalence width Z is denoted by δ .
Moreover, SASA makes the unjustified assumption that under their null hypothesis (H : ∣E[∆] | >
ZV), the Markov central limit theorem holds. Under their H0, the process is non-stationary, while the
construction of E[∆]'s confidence interval in Eqn. (19) relies on the (asymptotic) stationarity of the
Markov process. Therefore, despite the empirical success of SASA, its statistical test is intrinsically
flawed because of this testing setup.
B.2	Standard test in SASA+
In SASA+, we want to confidently detect non-stationarity. If non-stationarity is detected, one keeps
the learning rate the same, and otherwise one decreases it. This conceptual change naturally removes
the complication of the equivalence test and the intrinsic flaw in SASA.
To confidently detect non-stationarity, SASA+ sets stationarity as the null hypothesis and non-
stationarity as the alternative:
H0 : The process is staionary vs. H1 : The process is not staionary.	(20)
The master stationary condition E[∆] = 0 is a necessary condition for stationary of the process, and
thus confidently rejecting E[∆] = 0 is sufficient to confidently reject the null (stationarity) hypoth-
esis. Moreover, under this null hypothesis, the Central Limit Theorem of Markov Processes exactly
holds true (because the process is stationary), validating the construction of E[∆]'s confidence in-
terval. Therefore, the intrinsic flaw in SASA is naturally solved in SASA+.
Now we describe the test in SASA+. When the confidence interval of E[∆] does not contain 0, i.e.,
Gnt	3z
0 ∈ △ N - ti-δ∕2 √N, △ N + ti-δ∕2 √NJ ,	QI)
then we reject the null (stationary) hypothesis and keep the learning rate the same. Otherwise, we
decrease the learning rate. Notice that there is no additional hyperparameter Z .
B.3 The difference in practice
Although SASA and SASA+ are conceptually quite different, they both use the same confidence
interval (see (19) and (21)) for E[∆]. In practice, their difference is illustrated in Figure 7. Their
difference lies in Case (2) and (3). In Case (3), the process has not reached stationarity yet with high
probability, according to the confidence interval of E[∆] (Red). SASA+’s test is confident to reject
its (stationary) null hypothesis, so it keeps the learning rate the same. However, SASA decreases its
learning rate because it is confident that the stationary condition holds true within its error tolerance
(equivalence interval). In this case, SASA makes an error due to its relatively large equivalence
14
Under review as a conference paper at ICLR 2020
(1)	(2)	(3)	(4)
Equivalence	Confidence
Interval [-ζv, ζv]	InterVal of F[Δ]
Figure 7: Statistical tests in SASA and SASA+. Case (1): both SASA and SASA+ keep the learning
rate. Case (4): both SASA and SASA+ decrease the learning rate. Case (2): SASA keeps the
learning rate while SASA+ decreases. Case (3): SASA decreases the learning rate while SASA+
keeps.
interval. In Case (2), SASA+’s test is not confident to reject its (stationary) null hypothesis and so it
decreases its learning rate. On the contrary, SASA’s test is not confident to claim that the stationary
condition holds true within its error tolerance and thus keeps its learning rate. In this sense, SASA+
is more aggressive than SASA in decreasing learning rate.
In numerical experiments on the CIFAR10 and ImageNet datasets, the (width of) the equivalence
interval ζν is typically much smaller than the (width of) the confidence interval of E[∆], see Figure
5(a), 7(a) and 9(a) in Lang et al. (2019). Notice that in those figures, the yellow curve is ν instead
of ζν, and thus the equivalence interval is even smaller. Therefore, Case (3) happens very rarely
in those experiments, and this explains the reason why SASA does not make obvious mistakes
in decreasing the learning rate. In practice, Case (2) sometimes happens, and thus we can see that
SASA+ seems tobe slightly more aggressive at decreasing the learning rate than SASA. For example
in Figure 1, the black curve (SASA+) is faster at decreasing the learning rate than the green curve
(SASA).
15