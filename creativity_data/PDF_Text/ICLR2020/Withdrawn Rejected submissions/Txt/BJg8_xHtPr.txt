Under review as a conference paper at ICLR 2020
Unsupervised Object-Oriented 3D Scene
Representation and Rendering
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a probabilistic generative model, called ROOTS, for
unsupervised learning of object-oriented 3D-scene representation and rendering.
ROOTS bases on the Generative Query Network (GQN) framework. However,
unlike GQN, ROOTS provides independent, modular, and object-oriented decom-
position of the 3D scene representation. In ROOTS, the inferred object-oriented
representation is 3D in the sense that it is 3D-viewpoint invariant as the scene-level
representation of GQN is so. ROOTS also provides hierarchical object-oriented
representation: at 3D global-scene level and at 2D local-image level. In experi-
ments, we demonstrate on datasets of 3D rooms with multiple objects, the above
properties by focusing on its abilities for disentanglement, compositionality, trans-
ferability, and generalization. ROOTS achieves this without performance degra-
dation on generation quality in comparison to GQN.
1	Introduction
The shortcomings of contemporary deep learning such as interpretability, sample efficiency, ability
for reasoning and causal inference, transferability, and compositionality, are where the symbolic AI
has traditionally shown its strengths (Garnelo & Shanahan, 2019). Thus, one of the grand challenges
in machine learning has been to make deep learning embrace the benefits of symbolic representation
so that symbolic entities can emerge from high-dimensional observations such as visual scenes.
In particular, for learning from visual observations of the physical world, such representation should
consider the following criteria. First, it should focus on objects (and their relations) which are foun-
dational entities constructing the physical world. These can be considered as units on which we can
build a modular model. The modular nature also helps compositionality (Andreas et al., 2016) and
transferability (Kansky et al., 2017). Second, being three-dimensional (3D) is a decisive property
of the physical world. We humans, equipped with such 3D representation in our brain (Yamane
et al., 2008), can retain consistency on the identity of an object even if it is observed from different
viewpoints. Lastly, learning such representation should be unsupervised. Although there have been
remarkable advances in supervised methods to object perception (Redmon et al., 2016; Ren et al.,
2015; Long et al., 2015), the technology should advance toward unsupervised learning as we humans
do. This not only avoids expensive labeling efforts but also allows adaptability and flexibility to the
evolving goals of various downstream tasks because “objectness” itself can vary on the situation.
In this paper, we propose a probabilistic generative model that can learn, without supervision, object-
oriented 3D representation of a 3D scene from its partial 2D observations. We call the proposed
model ROOTS (Representation of Object-Oriented Three-dimensional Scenes). We base our model
on the framework of Generative Query Networks (GQN) (Eslami et al., 2018; Kumar et al., 2018).
However, unlike GQN which provides only a scene-level representation that encodes the whole 3D
scene into a single continuous vector, the scene representation of ROOTS is decomposed into object-
wise representations each of which is also an independent, modular, and 3D representation. Further,
ROOTS learns to model a background representation separately for the non-object part of the scene.
The object-oriented representation of ROOTS is more interpretable, composible, and transferable.
Besides, ROOTS provides the two-level hierarchy of the object-oriented representation: one for a
global 3D scene and another for local 2D images. This makes the model more interpretable and
provides more useful structure for downstream tasks. In experiments, we show the above abilities
of ROOTS on the 3D-Room dataset containing images of 3D rooms with several objects of different
1
Under review as a conference paper at ICLR 2020
colors and shapes. We also show that these new abilities are achieved without sacrificing generation
quality compared to GQN.
Our proposed problem and method are significantly different from existing works on visual 3D
learning although some of those partly tackle some of our challenges. First, our model learns fac-
torized object-oriented 3D representations which are independent and modular, from a scene con-
taining multiple objects with occlusion and partial observability rather than a single object. Second,
our method is unsupervised, not using any 3D structure annotation such as voxels, cloud points, or
meshes as well as bounding boxes or segmentation annotation. Third, our model is a probabilistic
generative model learning both representation and rendering with uncertainty modeling. Lastly, it is
trained end-to-end. In Section 4, we provide more discussion on the related works.
The main contributions are: (i) We propose, in the GQN framework, a new problem of learning
object-oriented 3D representations of a 3D scene containing multiple objects with occlusion and
partial observability in the challenging setting described above. (ii) We achieve this by proposing a
new probabilistic model and neural architecture. (iii) We demonstrate that our model enables various
new abilities such as compositionality and transferability while not losing generation quality.
2	Preliminary: Generative Query Networks
The generative query networks (GQN) (Eslami et al., 2018) is a probabilistic generative latent-
variable model providing a framework to learn a 3D representation ofa 3D scene. In this framework,
an agent navigating a scene i collects K images xik from 2D viewpoint vik . We refer this collection
to context observations Ci = {(xik, vik)}k=1,...,K. While GQN is trained on a set of scenes, in the
following, we omit the scene index i for brevity and discuss a single scene without loss of generality.
GQN learns scene representation z from context C. The learned representation z of GQN is a 3D-
viewpoint invariant representation of the scene in the sense that, given an arbitrary query viewpoint
vq , its corresponding 2D image xq can be generated from the representation.
In the GQN framework, there are two versions. The standard GQN model (Eslami et al., 2018)
uses the query viewpoint to generate representation whereas the Consistent GQN (CGQN) (Ku-
mar et al., 2018) uses the query after generating the scene representation in order to obtain query-
independent scene-level representation. Although we use CGQN as our base framework to obtain
query-independent scene-level representation, in the rest of the paper we use the abbreviation GQN
instead of CGQN to indicate the general GQN framework embracing both GQN and CGQN.
The generative process of GQN is written as follows: p(xq |vq, C) = p(xq|vq, z)p(z|C)dz. As
shown, GQN uses a conditional prior p(z|C) to learn scene representation z from context. To do
this, it first obtains a neural scene representation r from the representation network r = frepr-gqn(C)
which combines the encodings of (vk , xk) ∈ C in an order-invariant way such as sum or mean.
It then uses ConvDRAW (Gregor et al., 2016) to generate the scene latent variable z from scene
representation r by p(z|C) = Ql=1:L p(zl |z<l, r) = ConvDRAW(r) with L autoregressive rollout
steps. Due to intractability of the posterior distribution p(z|C, vq, xq), GQN uses variational infer-
ence for posterior approximation and the reparameterization trick (Kingma & Welling, 2013) for
backpropagation through stochastic variables. The objective is to maximize the following evidence
lower bound (ELBO) via gradient-based optimization.
logPθ(XqIvq,C) ≥ Eqφ(z∣C,vq,χq) [logPθ(xq | Vq, z)] - KL(qφ(z | C, Vq, Xq) ∣∣ Pθ(Z | C)).
Note that although in this paper we use a single target observation D = (xq, vq) for brevity, the
model is in general trained on a set of target observations D = {(xjq, Vjq)}j.
3	ROOTS: Representation of Object-Oriented 3D Scenes
3.1	Generative Process
The main difference of our model from GQN is that we have a 3D representation per object present
in the target 3D space while GQN has a single 3D representation compressing the whole 3D space
into a vector without object-level decomposition. We begin this modeling by introducing the num-
ber of objects M in the target space as a random variable. Then, we can write the representation
2
Under review as a conference paper at ICLR 2020
经
/PO NuLn-Ti
pS5P
Oliject
Gathering
Scene-Oliject ]
Latent Map
Figure 1: Overview of ROOTS. (A) The context observations are first fed into an encoder and obtain the
at for each
z4
z1
scene-volume feature-map. (B) The scene-object latent-map has a cell of zn = znpres , zpnos , znwh
spatial volume cell of the target environment. (C) After obtaining znpres , zpnos, we obtain znwhat
by finding
nwhat is not
object patches from context and then clustering them for each object. The resulting representation z
a 3D-representation. (D) Decoding process p(xq |s)p(s|z, vq). The representation is rearranged according to
query viewpoint vq by using f3D→2D (zpnos , vq) and then projected to a target 2D image. In (D), left-bottom
is an example of 3D full view. A projection camera is shown on the left corner. On the right, we perform 2
projection steps: (1) converting the coordinate w.r.t. the projection camera (bottom) and (2) projecting onto 2D
canvas (up).
prior of ROOTS as p(z, M|C) = p(M|C) QmM=1 p(z(m) |C). To implement such a model with a
variable number of objects, in AIR (Eslami et al., 2016), the authors proposed to use an RNN that
rolls out M steps, processing one object per step. However, according to our preliminary investiga-
tion (under review) and other works (Crawford & Pineau, 2019), it turned out that this approach is
computationally inefficient and shows severe performance degradation with growing M.
Object-Factorized Conditional Prior. To resolve this problem, in ROOTS we propose to process
objects in a spatially local and parallel way instead of sequential processing. This is done by first
introducing the scene-volume feature-map. The scene-volume feature-map is obtained by encoding
context C into a 3D tensor of N = (H×W ×L) cells. Each cell n ∈ {1, . . . , N} is then associated to
D-dimensional volume feature rn ∈ RD. Thus, the actual output of the encoder is a 4-dimensional
tensor r = frepr-scene(C). Each volume feature rn represents a local 3D space in the target 3D space
in a similar way that a feature vector in a 2D feature-map of CNN models a local area of an input
image. Note, however, that the introduction of the scene-volume feature-map is not the same as the
feature-map of 2D images because, unlike the CNN feature-map for images, the actual 3D target
space is not directly observable—it is only observed through a proxy of 2D images. For the detail
implementation of the encoder frepr-scene, refer to the Appendix A.3.
Given the scene-volume feature-map, for each volume cell n = 1, . . . , N but in parallel, we obtain
three latent variables (znpres, zpnos, znwhat) = zn from the 3D-object prior model p(zn|rn). We
refer this collection of object latent variables z = {zn}nN=1 to the scene-object latent-map as z
is generated from the scene-volume feature-map r. Here, znpres is a Bernoulli random variable
indicating whether an object is associated (present) to the volume cell or not, zpnos is a 3-dimensional
coordinate indicating the position of an object in the target 3D space, and znwhat is a representation
vector for the appearance of the object. We defer a more detail description of the 3D-object prior
model p(zn |rn ) to the next section. Note that in ROOTS we obtain znwhat as a 3D representation
which is invariant to 3D viewpoints. The position and appearance latents for cell n are defined only
when the cell has an associated object to represent, i.e., znpres = 1. From this modeling using scene-
volume feature-map, we can obtain M = Pn znpres and the previous prior model can be written as
follows: p(z, M | C) = p(M | C) QmM=1 p(z(m) | C) =
NN
pres
p(zn |rn)=	p(znpres |rn)	p(zpnos	|	rn)p(znwhat	|rn,zpnos)zn	.	(1)
n=1	n=1
In addition to allowing spatially parallel and local object processing, another key idea behind intro-
ducing a presence variable per volume cell is to reflect the inductive bias of physics: two objects
cannot co-exist at the same position. This helps remove the sequential object processing because
dealing with an object does not need to consider other objects if their features are from spatially
3
Under review as a conference paper at ICLR 2020
distant areas. Note also that the scene-volume feature-map is not to strictly partition the target 3D
space and that the presence variable represents the existence of the center position of an object not
the full volume of an object. Thus, information about an object can exist across neighboring cells.
Hierarchical Object-Oriented Representation. The object-oriented representations z = {zn}
provided by the above prior model is global in the sense that it contains all objects in the whole
target 3D space, independently to a query viewpoint. From this global representation and given a
query viewpoint, ROOTS generates a 2D image corresponding to a query viewpoint. This is done
first by learning the view-dependent representation of the target image. In a naive approach, this
may be done simply by learning a single vector representation p(zq|z, vq) but in this case, we lose
important information: the correspondence between a rendered object in the image and a global
object representation zn . That is, we cannot track from which object representation zn an object in
the image is rendered. In ROOTS, we resolve this problem by introducing local 2D-level object-
oriented representation layer. This local object-oriented and view-dependent representation allows
additional useful structure and more interpretability. This 2D local representation is similar to those
in AIR (Eslami et al., 2016) and SPAIR (Crawford & Pineau, 2019).
Specifically, for n for which znpres = 1, a local object representation sn is generated by conditioning
on the global representation set z and the query vq. Our local object representation model is written
as: p(s|z, vq) = QnN=1 p(sn|z, vq). Similar to the decomposition of zn, local object representa-
tion sn consists of (spnres, spnos, ssncale, snwhat). Here, spnres indicates whether an object n should be
rendered in the target image from the perspective of the query. Thus, even if an object exists in the
target 3D space, i.e., znpres = 1, spnres can be set to zero if that object should be invisible from the
query viewpoint. Similarly, spnos and ssncale represent respectively the position and scale of object n
in the image not in the 3D space, and snwhat represents the appearance to be rendered into the image
(thus not 3D invariant). For more details about how to obtain (spnres , spnos, ssncale, snwhat) from z and
vq, we describe in the next section. Given s = {sn }, we then render to the canvas to obtain the
target image p(xq |s). Combining all, the generative process of ROOTS is written as follows:
N
N
p(xq|vq,C)=
p(xq|s)	p(sn|z, vq)	p(zn|C) dzds.
(2)
n=1
See Figure 1 for the overview of the generation process.
3.2	Implementation Details
Global 3D-object prior. The 3D-object prior p(zn|rn) generates three latents (znpres, zpnos, znwhat)
as follows. It first obtains the presence latent from Znres 〜BemoUlli(fpres(rn)) and the 3-dimension
position latent from Znos 〜N(fpθs(rn), fpos(rn)). Using these two latents, We then obtain the
appearance latent znwhat . This process is divided into object gathering and object encoding.
For object gathering, for each context image xk we attend and crop a patch that corresponds to object
n. Specifically, we first notice that Using the deterministic camera-coordinate projection fUnction
f3D→2D (which we do not learn), we can project from the perspective of a context viewpoint vk, a
3D position zpnos in the global 3D coordinate system into a 2D position upko,ns in context image xk,
i.e., upko,ns = f3D→2D(zpnos, vk). If object n shoUld be invisible from the viewpoint vk, its projected
2D position upko,ns is oUt of the image xk and we do not crop a patch. For more details aboUt the
projection fUnction f3D→2D, refer to Wikipedia (2019). We also predict the boUnding box scale
uskc,nale = fscale(upko,ns , rn, vk). Given the center position and scale, we can crop a patch xkn ⊂ xk
Using the spatial transformer (Jaderberg et al., 2015). Applying this cropping to all context pairs
(vk, xk) ∈ C, we gather a set of object image-patches Xn = {xkn}kK=1 for all n with znpres = 1.
Given object image-patches Xn , obtaining object 3D-representation znwhat can be converted to a
GQN encoding problem. That is, we can simply consider Xn and its corresponding viewpoints as a
new object-level context Cn , and can rUn an order-invariant GQN representation network rnwhat =
frepr-obj(Cn) and then rUn ConvDRAW to obtain znwhat from rnwhat.
Local 2D-object prior. The intermediate prior p(sn|z, vq) generates (spnres, spnos, ssncale, snwhat).
This is done as follows. Similar to what we described in the above to obtain upko,ns, we Use
the coordinate projection fUnction f3D→2D to find the position of a global object zn in the
4
Under review as a conference paper at ICLR 2020
target image from the perspective of query vq, i.e., sqp,ons = f3D→2D(zpnos , vq). Thus, we
model the position as a deterministic variable. The scale sqsc,nale is also obtained similarly as
described in the above for Ukcnle, but With random sampling. Then, We predict Snres 〜
Bern(fpres(sqp,ons, sqs,cnale, zn, vq)). If object n should be visible in the 2D target image, i.e., spnres =
1, We generate the 2D appearance representation snwhat by using an object-level GQN decoder
based on ConvDRAW, i.e., snwhat = ConvDRAW(znwhat, vq). Finally, We have p(sn|z, vq) =
p(spnos|zpnos, vq)p(ssncale|spnos, znwhat, vq)p(spnres|spnos, ssncale, zn, vq).
Rendering to 2D Canvas. A main challenge in rendering the local representation s = {sn } into
the image canvas, i.e., p(xq|s), is to deal With occlusion. In ROOTS, this can easily be achieved by
noticing (i) that the coordinate conversion f3D→2D(zpnos , vq) actually converts a 3D-coordinate to
another 3D-coordinate and (ii) that then the last dimension of the converted coordinate system can
be interpreted as the orthogonal projection distance from the vieWpoint vq to the object’s position
zpnos. We can use this distance as object depth from the observer’s perspective. This alloWs us
to sort objects according to their depths and render each object accordingly Without occlusion. To
handle background, ROOTS has an independent module to infer the background separately at image-
level. We also found that learning an object-level mask along With the appearance latent snwhat helps
segmentation betWeen foreground and background as Well as generating less blurry images. More
details on related implementation are provided in Appendix A.4.
3.3	Learning and Inference
Due to the intractability of the posterior p(z, s|C, D) With D = {(vjq, xjq)}j the target vieWpoint-
image pairs, We train ROOTS using variational inference With the folloWing posterior approximation
qφ(z,s∣C,D) = qφ(z∣C,D)qφ(s∣z,C,D). To compute the gradient w.r.t. the continuous latent
variables such as the position and appearance, We use reparameterization trick and for the discrete
variables on the presence, We use a continuous relaxation using Gumbel-Softmax trick (Jang et al.,
2016). Other methods based on the REINFORCE algorithm (Williams, 1992; Tucker et al., 2017;
GrathWohl et al., 2017) can also be used. Implementation of the approximate posterior q(z|C, D)
and q(s|z, C, D) is made easier by sharing the parameters of q With that of conditional priorp(z|C):
We only need to provide additional data D. With D = (vq, xq) for simplicity, the objective is to
maximize the folloWing evidence loWer bound (ELBO): L(θ, φ; C, D) =
Es,z〜qφ [logPθ(xq|s) - KL[qφ(s∣z, Vq, C, D) ∣∣ p(s|z, Vq)]] - KL[q0(z∣C,D) ∣∣ pθ(z|C)]. (3)
Combining with Unconditioned Prior. One difficulty in using the conditional prior is the fact that,
because the prior is learned, We have less control in reflecting our prior knoWledge into the prior
distribution. In our experiments, it turns out that biasing the posteriors of some variables toWards
the values of our prior preference is helpful in stabilizing the model. To implement this, in our
training, We use the folloWing objective that has additional KL terms betWeen the posterior and
unconditioned prior.
L = L(θ, φ; C, D) + KL[qφ(zpos, sscale|C, D) ∣ N (0, I)]	(4)
+ γKL[qφ(zpres∣C,D) k Geom(ρ)] + YEZ〜q° [KL[qφ(spres∣zpres,C,D) ∣ Geom(ρ)]]()
Where zpres = {znpres}n and zpos , zscale, spres are defined similarly. The ρ and γ are hyperparam-
eters Weighting the auxiliary loss terms. We set them to 0.999 and 7 during training, respectively.
This auxiliary loss can be derived if We replace the conditional prior by the product of expert prior
p(z|C)p(z) divided by the posterior q(z|C).
4	Related Works
Although to our knoWledge there has been no previous Work on unsupervised and probabilistic
generative object-oriented representation learning for 3D scenes containing multiple objects, there
has been literature on its 2D problems. The first is the Attend, Infer, Repeat (AIR) model (Eslami
et al., 2016). AIR uses spatial transformer (Jaderberg et al., 2015) to crop an object patch and uses
an RNN to sequentially generate next object conditioning on the previous objects. In CraWford &
Pineau (2019), the authors shoWed that this RNN-based rollout is inefficient and can significantly
5
Under review as a conference paper at ICLR 2020
degrade performance as the number of objects increases. SPAIR is inspired by YOLO (Redmon
et al., 2016), but unlike YOLO, it does not require bounding box labels. The main limitation of
SPAIR is to infer the latents sequentially. Neural Expectation Maximization (NEM) (Greff et al.,
2017) considers the observed image as a pixel-level mixture of K objects and an image per object
is generated and combined according to the mixture probability. In Greff et al. (2019), the authors
proposed a more efficient version of NEM, called IODINE, using iterative inference (Marino et al.,
2018). In MONET (Burgess et al., 2019), an RNN drawing a scene component at each time step is
used. Unlike AIR and SPAIR, the representations in NEM, IODINE, and MONET do not explicitly
provide natural disentanglement like presence and pose per object.
There have been plenty amount of remarkable works about visual 3D learning from the computer
vision community. However, as pointed in Section 1, the problem setting and proposed model of
these works are different from ours in the sense that they are either (i) not decomposing object-wise
representations from a scene containing multiple objects (but working mostly on single object cases)
(Wu et al., 2016; Yan et al., 2016; Choy et al., 2016; Kar et al., 2017; Nguyen-Phuoc et al., 2019), (ii)
supervised approach (Huang et al., 2018; Tulsiani et al., 2018; Cheng et al., 2018; Shin et al., 2019;
Du et al., 2018), (iii) learn image generation (synthesis) of a 3D scene without scene-representation
(Sitzmann et al., 2019; Kato & Harada, 2019; Pinheiro et al., 2019; Tulsiani et al., 2017) or learn
scene-representation without learning rendering ability (Zhou et al., 2017; Yu & Wang, 2018), or
(iv) not end-to-end. Many of the more traditional works from 3D computer vision also relevant to
our work but many of them are not based on neural networks and not end-to-end trainable.
5	Experiments
We evaluate ROOTS quantitatively and qualitatively. We train ROOTS with the same hyperparame-
ters on all datasets. We first briefly describe the datasets. For more details of the dataset generation
and network architecture, refer to Appendix A.5 and Appendix A.4. We use MuJoCo, (Todorov
et al., 2012), to simulate 3D scenes. Specifically, we generate three different datasets of scenes with
1-3 objects, 2-4 objects and 3-5 objects, respectively. We set the image size to 64 × 64 × 3 pixels. For
each object, we randomly choose its position, shape, size, and color. Although we put all objects on
the floor, we still predict the 3-dimension coordinate values for zpnos because objects have different
sizes. We generate 60K different scenes for each dataset and split them into 50k for training, 5k for
validation, and 5k for testing. We use CGQN as the baseline implementation but in the rest of the
section use the abbreviation ‘GQN’ instead of ‘CGQN’ to indicate the general GQN framework.
5.1	Qualitative Evaluation
First, we compare the generations between ROOTS with GQN by visualizing several generated
samples from the same scene under the same set of query viewpoints. Note that the goal of this
experiment is to show that achieving the object-wise representation in ROOTS does not deterio-
rate its generation quality in comparison to GQN. As seen in Figure 2, ROOTS generates slightly
sharper object boundaries while GQN generates more blurry images. We believe that this is due
to the object-wise generation using segmentation masks. Then, to provide a further understanding
of the advantages of object-orientated representation, we visualize decomposed generations from
ROOTS, as shown in Figure 3.A. We can see that ROOTS can generate clean background, complete
objects, precise foreground, and detailed occlusion mask separately. Furthermore, to demonstrate
the viewpoint-invariant property of the global object representation znwhat, in Figure 3.B, we pro-
vide generations of two objects under different query viewpoints. We can see that ROOTS recovers
the object with pose corresponding to query viewpoints. GQN cannot provide such decomposed
generations because its representation is scene-level where objects are not decomposed.
Object-wise disentanglement. Generations from random viewpoints after changing object posi-
tions. In this section and the following sections, we use ROOTS trained on the 2-4 object dataset
for demonstration unless otherwise stated. To verify the disentanglement property of our object-
oriented representation learned by ROOTS, we carry out the experiment of arbitrarily modifying
zpnos of one object in a scene. As a good disentangled latent representation, this modification should
not affect the position, existence, and appearance of other objects concurrent in the scene while
handling occlusion properly. More importantly, we should be able to change either the x or y coor-
dinate independently and this change should be consistent across viewpoints. To demonstrate this,
6
Under review as a conference paper at ICLR 2020
ROOTS
GQN
ROOTS
GQN
、一口 •,一文ɪL二二卫二士工二.,止 U
工工工二ZI工工三三工二三二二工ZZ上
■下：Hrr7TK7-HHγ7 >->Γe"T
Figure 2:	Examples of generations from two different scenes together with ground truth placed in the first row
for each scene. ROOTS shows better generation on objects boundary and clearer occlusion, compared with
GQN, especially when there are more object in the scene.
Figure 3:	A: Generated examples for scene decomposition from three different scenes visualized as 2D images
captured by a query camera (viewpoint). For example, in the first group of visualization in A, ROOTS can
segment a scene into the foreground and background first and decompose foreground into each individual
object further, which are a green sphere, a blue cylinder, a partially observed cube and a yellow cube. We also
show the foreground occlusion mask, which is obtained via each object’s mask and the distance between the
object and query camera. B: Recovered generations of two objects from global 3D object-wise representation
under different query viewpoints. The recovered 2D projection have different pose when seen from different
viewpoints.
after changing one dimension of zpnos of the red cylinder, as shown in Figure 4, we feed this modified
Znos back to the model and follow the ROOTS generation model in the same way as We do during
testing. Generations under 4 different viewpoints are shown in Figure 4. We can see that ROOTS
has a strong ability to learn disentangled representation and occlusions have been handled well due
to this advantage.
Compositionality. As stated in earlier sections that a 3D scene can be decomposed into several
independent objects, another advantage coming with object-orientated representations is that a new
scene can be easily built up with selected object components. Thus, by simple combination, we
can build novel scenes. To demonstrate this, we first provide ROOTS with three sets of context
images from three different scenes and save the learned object representations z for each scene,
z = {zpnos, znwhat} for all n with zpnos = 1. Then we swap one object between the first two scenes
and add one additional object to the third scene, as shown in Figure 5. We see that the object
component learned by ROOTS is fully disentangled and can be reused to create new scenes. Also,
by adding one new object, we make a scene with 5 objects, which does not exist in the training
dataset. Another example shown a scene 9 objects is visualized in Figure 6, where we use ROOTS
trained on the 1-3 objects dataset. More details about this experiment can be found in Appendix
A.2.
Partial Observability. We know that the successful generation of ROOTS, even with only partial
observations provided, is coming from object gathering across viewpoints. This is because if an
object is invisible for one viewpoint, ROOTS can learn it from other viewpoints. Here, we want
7
Under review as a conference paper at ICLR 2020
Figure 4: Visualization of changing zpnos,x and zpnos,y of the red cylinder in a scene through generations from
4 different cameras. We also simulated the 3D scene in the center. Cameras are shown as an example. We put
walls for readers to better understand the relative height of cameras. There is no wall in the real dataset.
呷TV!≡κ壬HTILT尸EfIfFy巴
FkFFKj不为Hk方E*KKKF)・••
用FITFkrEP化EE耳KTlnk芦■■ ι∙
手TFSBK,学!35w7AR∙rT・司∙∙∙h
・0+
寿U±±T∏T5宾FWJ零EFKK齐■■ t∙∙
Figure 5:	TOP: Original generations from three different scenes, icons on the right side highlight objects in
each scene. Each column shows a 2D image captured by one camera. Bottom: Manipulated generations from
the same three scenes under the same set of cameras. We switch the green cylinder from scene 1 with the purple
sphere from scene 2 and we copy green sphere from scene 2 and put it into scene 3.
to push the partial observability to the extreme case, where one object is totally invisible for all
context viewpoints. In this case, ROOTS should not be able to correctly predict its existence, just
like humans observing the true physical world. To show this, we manually select some images from
a scene that has one object missing served as context and the rest served as targets for ROOTS to
generate. We show the results in Figure 7.
5.2 Quantitative Evaluation
NLL and MSE. In this section, we compare the quantitative results of ROOTS and GQN on the
negative log-likelihood (NLL) and the mean squared error (MSE) on the test dataset. The goal is to
show quantitatively that achieving the object-wise representation in ROOTS does not deteriorate its
generation quality in comparison to GQN. We approximate NLL using importance sampling with
K = 50 samples and report the image NLL normalized by the number of pixels in Table ??. Both
GQN and ROOTS are trained for 120 epochs on each dataset, making sure both ROOTS and GQN
have converged. We see that, although learning object-factorized representations, ROOTS achieves
NLL comparable to GQN. Due to object-wise representation, ROOTS recovers objects separately,
together with an independent background module, ROOTS performs better on MSE than GQN. This
benefit becomes clearer as the number of objects in the scene grows.
Table 1: Negative log-likelihood and mean squared error
Training set	1-3 objects		2-4 objects		3-5 objects	
Metrics	NLL	MSE	NLL	MSE	NLL	MSE
ROOTS	0.9199	15.16	0.9205	25.37	0.9213	29.82
GQN	0.9196	15.28	0.9201	26.86	0.9204	32.66
Object Detection Quality. In this section, we only provide precision and recall results as object
detection evaluation of ROOTS as GQN cannot detect objects. To estimate the true positive pre-
diction, we first filter out predictions that are too far away from any ground truth by applying a
8
Under review as a conference paper at ICLR 2020
汽宇F区再证可不片■国摩豆C七受F
Figure 6:	A novel scene consisted of 9 objects is composited by ROOTS trained on 1-3 objects dataset.
-F
Generations
Targets
Figure 7:	Left: Context images provided to ROOTS, we select images that do not have the green sphere
projected. Middle: Target images, from which we can see that there is one green sphere existed. Right:
Generations from ROOTS given context images from the left.
radius threshold. The distance is calculated as the euclidean distance between two center points.
Then we assign the predicted object to the ground truth object to which it has the nearest distance.
Multi-association is not allowed. We normalize the coordinate value to be in the range of [-1, 1]
for better interpretability of the applied radius threshold. Accordingly, the average object size is 0.3.
We provide the precision and recall result under different thresholds in Table ??. Besides precision
and recall, we also provide the counting accuracy of ROOTS.
Table 2: Precision and Recall
Training set	1-3 objects			2-4 objects			3-5 objects		
Threshold	0.1	0.15	0.35	0.1	0.15	0.35	0.1	0.15	0.35
Precision	76.82	91.56	98.68	66.76	86.07	98.29	66.64	86.09	96.65
Recall	75.56	90.15	97.40	65.53	84.57	95.93	66.91	86.47	97.10
Count Acc.		93.18			88.16			82.21	
6 Conclusion
We proposed ROOTS, a probabilistic generative model for unsupervised learning of 3D scene rep-
resentation and rendering. ROOTS can learn object-oriented interpretable and hierarchical 3D scene
representation. In experiments, we showed the generation, decomposition, and detection ability of
ROOTS. For compositionality and transferability, we also showed that, due to the factorization of
structured representation, new scenes can be easily built up by reusing components from 3D scenes.
Interesting future directions would be to learn the knowledge of the 3D world in a sequential manner
as we humans keep updating and inferring knowledge through time.
References
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 39-48,
2016.
Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt
Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and represen-
tation. arXiv preprint arXiv:1901.11390, 2019.
Ricson Cheng, Ziyan Wang, and Katerina Fragkiadaki. Geometry-aware recurrent neural networks
for active visual recognition. In Advances in Neural Information Processing Systems, pp. 5081-
5091, 2018.
9
Under review as a conference paper at ICLR 2020
Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A
unified approach for single and multi-view 3d object reconstruction. In European conference on
computer vision, pp. 628-644. Springer, 2016.
Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolu-
tional neural networks. In Proceedings of AAAI, 2019.
Yilun Du, Zhijian Liu, Hector Basevi, Ales Leonardis, Bill Freeman, Josh Tenenbaum, and Jia-
jun Wu. Learning to exploit stability for 3d scene parsing. In Advances in Neural Information
Processing Systems, pp. 1726-1736, 2018.
SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, and Geoffrey E
Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in
Neural Information Processing Systems, pp. 3225-3233, 2016.
SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Gar-
nelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, David P Reichert, Lars
Buesing, Theophane Weber, Oriol Vinyals, Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe
Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu, and Demis Hassabis. Neural scene
representation and rendering. Science, 360(6394):1204-1210, 2018.
Marta Garnelo and Murray Shanahan. Reconciling deep learning with symbolic artificial intelli-
gence: representing objects and relations. Current Opinion in Behavioral Sciences, 29:17-23,
2019.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint
arXiv:1711.00123, 2017.
Klaus Greff, Sjoerd van Steenkiste, and Jurgen Schmidhuber. Neural expectation maximization. In
Advances in Neural Information Processing Systems, pp. 6691-6701, 2017.
Klaus Greff, Raphael Lopez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran,
Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning
with iterative variational inference. arXiv preprint arXiv:1903.00450, 2019.
Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. In Advances In Neural Information Processing Systems, pp. 3549-3557,
2016.
Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Cooper-
ative holistic scene understanding: Unifying 3d object, layout, and camera pose estimation. In
Advances in Neural Information Processing Systems, pp. 207-218, 2018.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer
networks. In Advances in neural information processing systems, pp. 2017-2025, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Ken Kansky, Tom Silver, David A Mely, Mohamed Eldawy, Miguel Lazaro-Gredilla, Xinghua Lou,
Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-
shot transfer with a generative causal model of intuitive physics. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 1809-1818. JMLR. org, 2017.
Abhishek Kar, Christian Hane, and Jitendra Malik. Learning a multi-view stereo machine. In
Advances in neural information processing systems, pp. 365-376, 2017.
Hiroharu Kato and Tatsuya Harada. Learning view priors for single-view 3d reconstruction. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9778-
9787, 2019.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
10
Under review as a conference paper at ICLR 2020
Ananya Kumar, SM Eslami, Danilo J Rezende, Marta Garnelo, Fabio Viola, Edward Lockhart,
and Murray Shanahan. Consistent generative query networks. arXiv preprint arXiv:1807.02033,
2018.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431-3440, 2015.
Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. arXiv preprint
arXiv:1807.09356, 2018.
Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Holo-
gan: Unsupervised learning of 3d representations from natural images. arXiv preprint
arXiv:1904.01326, 2019.
Pedro O Pinheiro, Negar Rostamzadeh, and Sungjin Ahn. Domain-adaptive single-view 3d recon-
struction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7638-
7647, 2019.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,
real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 779-788, 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
pp. 91-99, 2015.
Daeyun Shin, Zhile Ren, Erik B Sudderth, and Charless C Fowlkes. 3d scene reconstruction with
multi-layer depth and epipolar transformers. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 2172-2182, 2019.
Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nieβner, Gordon Wetzstein, and Michael
Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2437-2446, 2019.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in
Neural Information Processing Systems, pp. 2627-2636, 2017.
Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Jitendra Malik. Multi-view supervision
for single-view reconstruction via differentiable ray consistency. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 2626-2634, 2017.
Shubham Tulsiani, Saurabh Gupta, David F Fouhey, Alexei A Efros, and Jitendra Malik. Factoring
shape, pose, and layout from the 2d image of a 3d scene. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 302-310, 2018.
Wikipedia. 3D projection — Wikipedia, the free encyclopedia, 2019. URL http://en.
wikipedia.org/w/index.php?title=3D%20projection&oldid=925559293.
[Online; accessed 14-November-2019].
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a prob-
abilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in
neural information processing systems, pp. 82-90, 2016.
11
Under review as a conference paper at ICLR 2020
Yukako Yamane, Eric T Carlson, Katherine C Bowman, Zhihong Wang, and Charles E Connor.
A neural code for three-dimensional object shape in macaque inferotemporal cortex. Nature
neuroscience, 11(11):1352, 2008.
Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and Honglak Lee. Perspective transformer nets:
Learning single-view 3d object reconstruction without 3d supervision. In Advances in Neural
Information Processing Systems,pp. 1696-1704, 2016.
Chong Yu and Young Wang. 3d-scene-gan: Three-dimensional scene reconstruction with generative
adversarial networks. In ICLR (Workshop), 2018.
Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth
and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1851-1858, 2017.
12
Under review as a conference paper at ICLR 2020
A	Appendix
A. 1 Generalization
In this section, we quantitatively analyze ROOTS’s generalization performance on tasks of genera-
tion and detection. For the task of generation, we provide a comparison between ROOTS and GQN.
Specifically, we train both ROOTS and GQN on our three datasets until full convergence. For each
model trained on one dataset, e.g 1-3 objects dataset, we test it on the other two datasets, e.g 2-4
objects dataset, and 3-5 objects dataset. For the task of detection, since GQN only provide scene-
level representation (detection is not possible), we only report detection performance of ROOTS.
Generation results (MSE) and detection results (precisin and recall) are shown in Table 3 and Table
4, respectively. Precision and recall are reported with the radius threshold set to 0.15 unit in Mu-
joco physic world. Here, we describe the trend we find from the experimental observations. When
trained on dataset with small (e.g., 1-3) number of objects and tested on large (e.g., 3-5) num-
ber of objects, both ROOTS and GQN do not generalize well. This is because, during testing, latent
variables are sampled from a learned prior. If during training, the model isn’t provided a chance to
see scenes with more objects, it is hard to generalize well. Also, we note that, compared with GQN,
ROOTS obtains larger MSE error. Together with observations in the following case, we hypothesize
this sensitivity comes from the discrete variable, presenting the existence of an object. While with
one vector representing the full scene, GQN has smoother performance. When trained on dataset
with a large number of objects while tested on a small number of objects, ROOTS and GQN
show a similar property in their performance. They both perform well. But, in this case, ROOTS
produces better generations results, which we believe is a benefit from the object-wise generation
and a separate background module. While in GQN all information are compressed into one vector.
The above conclusion of ROOTS is consistent with its performance on detection task. One inter-
esting finding during this experiment is that ROOTS trained on 3-5 objects dataset achieves better
results (lower MSE, higher precision, and recall) than trained on 1-3 objects dataset when testing on
1-3 objects dataset. A similar phenomenon is found when testing on 2-4 objects dataset. This is an
interesting finding that is worth more investigation.
Table 3: Quantitative generalization results on generation
Training set	1-3 Objects			2-4 Objects			3-5 Objects		
Testing set	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 Objects
GQN	15:57	27:49	43:57	1644	2677	3762	15:02	2373	3338
ROOTS	14.72	35.22	71:43	13.97	25:35	42:21	11:48	19:62	29:88
Table 4: Quantitative generalization results on object detection
Training set	1-3 Objects			2-4 Objects			3-5 Objects		
Testing set	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 Objects	1-3 Objects	2-4 Objects	3-5 Objects
Precision	9143	83:25	7181	9036	8655	80:83	9324	907Γ0	85:91
Recall	89:84	80:02	65:67	89:22	84:68	78:29	93:66	89:94	85:91
Count Acc:	93:07	81:04	57:91	93∙17	87:42	79:42	92:57	88:14	82:17
A.2 Composing with more objects
In this section, we provide one more example of compositionality. We first train ROOTS on 1-3
objects dataset. As in Figure 8, during testing, we collect object representations from 7 different
scenes and reuse them to composite a new scene with 9 objects. Additionally, the sampled center
position of each object and query viewpoints (represented as camera position, pitch, and roll) fed
into ROOTS are with respect to canonical coordinates. Note that, to predict the scale of objects in
2D projection correctly (e.g., for the same object, the larger the distance between it and viewpoint is,
the smaller that object should be in the 2D projection), ROOTS learns to infer local depth (position
translation) for each object given specific query viewpoint. This can be observed on the yellow
sphere, green cylinder and blue cube in the bottom row in Figure 8.
13
Under review as a conference paper at ICLR 2020
总司彳居港Wq不范用岸岸三；正M引
Figure 8: TOP: Target images taken by several query viewpoints from 9 different scenes with icons on the
right hand-side highlighting the objects in the scene. Bottom: New scene created with objects collected from
the above 9 scenes.
A.3 Details of component modules
We first introduce some important building-blocks for implementing our model in this section, and
then sketch the implementation steps using modules described here in the following section.
Scene Representation Network: We use Scene Representation Network to implement the order
invariant encoder at scene level, frepr-SCene(∙). This network is modified based on Representation Net-
work in GQN. We adjust kernel size and add a CNN3D layer to make sure the output conv-features
fit our needs, shown as in Figure 9. The scene representation network takes <image, viewpoint>
pair as input, output a conv-feature map. To implement the order invariant, we take the mean of
these outputs from the same scene.
64x64x3	22x22x256	22x22x128	8x8x(256+7)	4x4x128
2x2x256	1x1x256
3x1x3x256
Figure 9: Scene Representation Network
Object Representation Network: We use object Representation Network to implement order in-
variant encoder at object level, frepr-obj(∙). AS visualized in Figure 10, We design a branch to pass
low level feature to provide richer conv-features. The dimension d in the second input equals to the
summation of the dimension of {zpres, sscale, and spos }. The corresponding values are listed in
Table 5. The order invanriant is implemented in the same way as in Scene Representation Network.
Convolutional LSTM Cell: We use ConvLSTM as the RNN module in the ConvDRAW module.
In ConvLSTM, all fully-connected layers are substituted with convolutional layers. Its one updating
step is described as follows:
(hi+1, ci+1)《-COnVLSTM(Xi, hi, ci)
14
Under review as a conference paper at ICLR 2020
32x32x3
k=5 I
s=3	∖
p=2
1x1xd	8x8xd
Figure 10: Object Representation Network
where hi is the output of the cell and ci is the recurrent state of the ConvLSTM, xi is the input. Both
hi and ci are initialized with zeros at the i = 0 step.
ConvDRAW: We highlight one step ConvDRAW (denoted as l) used in ROOTS here for generative
process and inference process separately.
• Generative Process:
(hPl+1),eg+1)) — ConvLSTMθ (X(I), Z(I), hPl),cPl))
Z(I+1)〜StatisNet(hPl+1))
• Inference Process:
(hql+1), eql+1)) - ConvLSTMφ(y, x(l), hp) h'l), Cql))
z(l+1)〜StatisNet(hql+1))
(hPl+1), ePl+1)) — ConvLSTMθ(e(I), Z(I), h，CPl))
where xl is the input at the lth step, y is target reconstruction, z(l+1) is the sampled latent at the
lth step. We denote the prior module and posterior module with subscript p and q, respectively. θ
and φ are neural network parameters. The StatisNet is described in the following paragraph. In the
Renderer network, we replace the StatisNet with a deterministic Decoder.
Sufficient Statistic Networks: The Sufficient Statistic Networks will output sufficient statistics for
pre-defined distribution, e.g. μ and σ for GaUssian distribution, given inputs. We list the ConfigUra-
tion of all the Sufficient Statistics Networks in Table 5 used during generation. For znPos, znwhat and
ssnc,iale, we use auto-regressive scheme (ConvDraw) to learn the sufficient statistics. For znPres and
sqP,rnes, we use regular convolutional layers. In the third column, we give the kernel size of the first
convolutional layer, the remaining are Conv3D 1X1 layers. If the kernel size is 3, we have one zero
paddings, the stride is 1 to keep the spatial size unchanged. The ”Draw Rollouts” column shows the
number of DRAW steps we apply. The ”Concat” column specifies how we use sampled latent value
for the subsequent process, for example, concatenating zl, for l < L or only taking zL, where L is
the rollout steps.
GQN Implementation: We strictly follow the paper for implementation details. The only difference
is we enhance the decoder in the renderer by adding two more convolutional layers.
A.4 Implementation Details
Here, we give the details of the implementation of ROOTS. Details of component modules are
mentioned in Appendix A.3. We first outline the generation and inference process for one object,
indexed with n. Parallelizing it to multiple objects is straightforward.
15
Under review as a conference paper at ICLR 2020
Table 5: Configuration of Sufficient Statistic Networks
Latent Variable	Channel Numbers	K	Draw Rollouts	Concate
Znos	-[128,128, 64, 32, 3]-	丁	2	last step
DWhat 	Zn ,		-[128, 128,64, 32, 4]	ɪ	4	concatenate
sScαle 	sq,n		-[128, 128, 64, 32, 2]	ɪ	4	last step
pres 	zn		-[256, 256,128, 64,1]-	丁	-	-
GPreS 	Sq,n		[271,256,128, 64, 32,1]	ɪ	-	-
Zbg	[128,4]	丁	2	—	last step
Generation Process Superscript on ConvDRAW is used to indicate which variable the Con-
vDRAW module is responsible for. The Renderer module is implemented in the same way as
ConvDRAW with one difference that we do not model any variable in Renderer, making it a de-
terministic decoder. All the ConvDRAW modules have a hidden state size of 128. We use ST
denote spatial transformation process and ST-1 denotes the reversed process. K denote the index
set of context C and θ and φ are neural network parameters.
r J E frepr-sCene(Xk, Vk ,θ)		(Obtain scene-volume feature-map)	(5)
	k∈K		
ZPnoS	〜ConvDRAWPos(rn)	(Sample 3D position)	(6)
zPreS	〜CONVθ (rn)	(Sample global presence)	(7)
PoS uk,n	J f3D→2D (ZPnos , vk), k ∈ K	(Perspective projection)	(8)
Scale uk,n	〜ConvDRAWSCale(rn, Vk, UPon),k ∈ K	(Sample object scale)	(9)
Xk Xn	JST(Xk,[uPko,ns,uskc,nale]),k∈K	(Crop object patch)	(10)
what rn	J	frepr-obj (Xn , Vk ), k ∈ K	(Object level encoding)	(11)
	k∈K		
what Zn	〜ConvDRAWwhat (rwhat)	(Sample global what)	(12)
sPoS sq,n	J f3D→2D(znPos, Vq)	(Perspective projection)	(13)
Scale sq,n	〜ConvDRAWθcale(rn, SP3 vq)	(Sample object scale)	(14)
PreS sq,n	〜CONVθ(ZnrS rwhat, νq, S牌,SqCnle)	(Sample local presence)	(15)
xn,αn	J Rendererθ (Znwhat, Vq)	(Decode the object patch and object mask)	
			(16)
Xn	-sL(α × Xn,[spos, ss”)	(Generate object patch)	(17)
αqn	J ST-1(αqn, [SqnPos,Sqnscale])		(18)
αocc αq,n	J depthn × αqn == mnin(depthn × αqn)	(Obtain occlusion mask)	(19)
Xfg	-Xmocn × Xn × SPreS) n	(Generate foreground)	(20)
αqfg	- X(αqn × αqo,cnc × sqP,rnes) n	(Generate foreground mask)	(21)
Zbg	〜ConvDRAWθg(r)	(Sample background)	(22)
Xqg	- BgRenderer(Zbg , Vq)	(Decode background)	(23)
Xq	J Xfg +(I- afg) ×X qg	(Render generations)	(24)
Inference Process The inference modules are paralleled with generative modules. We only high-
light the different part below.
16
Under review as a conference paper at ICLR 2020
rq	—EfrePr-SCene(Xq, Vq, θ)	(Obtain scene-volume feature-map)	(25)
zpnos	q q 〜ConvDRAWpoS(rn, rn)	(Sample 3D position for object n)	(26)
pres zn	〜CONVφ(rn, rn)	(Sample global presence)	(27)
spos sq,n	一 f3D→2D(Znos, Vq)	(Perspective projection)	(28)
scale sq,n	〜COnvDRAWθCφle(rn, Tn SP工 vq)	(Sample object scale)	(29)
xqn	一 ST(Xq ,[Spos, S;")	(Crop object patch)	(30)
what rq,n	—X frepr-obj (Xn, Vq ) q q	(Encode object level context)	(31)
what zn	〜ConvDRAWw,φαt(rWhαt, rw,nnat)	(Sample global what)	(32)
pres sq,n	〜CONVφ(rwnat, rwhat, spoS, Sqnae vq)	(Sample local presence)	(33)
zbg	〜ConvDRAWθgφ(rq, r)	(Sample background)	(34)
A.5 Dataset Details
The sizes of objects are randomly chosen from 0.56 to 0.66 unit in the Mujoco physic world. Each
object is put on the floor of 3D space with a range of [-2, 2] along both x-axis and y-axis. We have
three different types of object, cube, sphere, and cylinder with 6 different colors. For each dataset,
we first randomly choose the number of objects in a scene, then randomly choose object type, color
and their positions (x and y coordinates). For each scene, we have 30 cameras put at a radius of 3,
pointing at a squared area located at the center, thus, the camera would not always look at the center
point. The pitch is randomly chosen from [-∏∕6., -n/7.] and the yaw is randomly chosen from
[-π, π]. During training, we randomly divide each scene into context set and query set, where the
number of <image, viewpoint> pair in context set is randomly chosen from a range of [10, 20] and
the remaining are served as query set.
17