Under review as a conference paper at ICLR 2020
Testing Robustness Against
Unforeseen Adversaries
Anonymous authors
Paper under double-blind review
Ab stract
Most existing defenses against adversarial attacks only consider robustness to Lp-
bounded distortions. In reality, the specific attack is rarely known in advance and
adversaries are free to modify images in ways which lie outside any fixed distor-
tion model; for example, image rotations lie outside the set of Lp-bounded dis-
tortions. In this work, we advocate measuring robustness against a much broader
range of unforeseen attacks, attacks whose precise form is unknown during de-
fense design.
We propose several new attacks and a methodology for evaluating a defense
against a diverse range of unforeseen distortions. First, we construct novel ad-
versarial JPEG, Fog, Gabor, and Snow distortions to simulate more diverse adver-
saries. We then introduce UAR, a summary metric that measures the robustness
of a defense against a given distortion. Using UAR to assess robustness against
existing and novel attacks, we perform an extensive study of adversarial robust-
ness. We find that evaluation against existing Lp attacks yields redundant informa-
tion which does not generalize to other attacks; we instead recommend evaluating
against our significantly more diverse set of attacks. We further find that adversar-
ial training against either one or multiple distortions fails to confer robustness to
attacks with other distortion types. These results underscore the need to evaluate
and study robustness against unforeseen distortions.
1	Introduction
Neural networks perform well on many benchmark tasks (He et al., 2016) yet can be fooled by
adversarial examples (Goodfellow et al., 2014) or inputs designed to subvert a given model. Ad-
versaries are usually assumed to be constrained by an L∞ budget (Goodfellow et al., 2014; Madry
et al., 2017; Xie et al., 2018), while other modifications such as adversarial geometric transforma-
tions, patches, and even 3D-printed objects have also been considered (Engstrom et al., 2017; Brown
et al., 2017; Athalye et al., 2017). However, most work on adversarial robustness assumes that the
adversary is fixed and known in advance. Defenses against adversarial attacks are often constructed
in view of this specific assumption (Madry et al., 2017).
In practice, adversaries can modify and adapt their attacks so that they are unforeseen. In this work,
we propose novel attacks which enable the diverse assessment of robustness to unforeseen attacks.
Our attacks are varied (§2) and qualitatively distinct from current attacks. We propose adversarial
JPEG, Fog, Gabor, and Snow attacks (sample images in Figure 1).
We propose an unforeseen attack evaluation methodology (§3) that involves evaluating a defense
against a diverse set of held-out distortions decoupled from the defense design. For a fixed, held-out
distortion, we then evaluate the defense against the distortion for a calibrated range of distortion
sizes whose strength is roughly comparable across distortions. For each fixed distortion, we sum-
marize the robustness of a defense against that distortion relative to a model adversarially trained
on that distortion, a measure we call UAR. We provide code and calibrations to easily evaluate a
defense against our suite of attacks at https://github.com/iclr-2020-submission/
advex- uar.
By applying our method to 87 adversarially trained models and 8 different distortion types (§4),
we find that existing defenses and evaluation practices have marked weaknesses. Our results show
1
Under review as a conference paper at ICLR 2020
Elastic
L∞	L2	L1
JPEG
Fog
Gabor
Figure 1: Attacked images (label “espresso maker”) against adversarially trained models with large
ε. Each of the adversarial images above are optimized to maximize the classification loss.
Snow
that existing defenses based on adversarial training do not generalize to unforeseen adversaries,
even when restricted to the 8 distortions in Figure 1. This adds to the mounting evidence that
achieving robustness against a single distortion type is insufficient to impart robustness to unforeseen
attacks (Jacobsen et al., 2019; Jordan et al., 2019; Tramer & Boneh, 2019).
Turning to evaluation, our results demonstrate that accuracy against different Lp distortions is highly
correlated relative to the other distortions we consider. This suggest that the common practice of
evaluating only against Lp distortions to test a model’s adversarial robustness can give a misleading
account. Our analysis demonstrates that our full suite of attacks adds substantive attack diversity
and gives a more complete picture of a model’s robustness to unforeseen attacks.
A natural approach is to defend against multiple distortion types simultaneously in the hope that
seeing a larger space of distortions provides greater transfer to unforeseen distortions. Unfortunately,
we find that defending against even two different distortion types via joint adversarial training is
difficult (§5). Specifically, joint adversarial training leads to overfitting at moderate distortion sizes.
In summary, we propose a metric UAR to assess robustness of defenses against unforeseen adver-
saries. We introduce a total of 4 novel attacks. We apply UAR to assess how robustness transfers to
existing attacks and our novel attacks. Our results demonstrate that existing defense and evaluation
methods do not generalize well to unforeseen attacks.
2	A set of diverse and novel adversarial attacks
We consider distortions (attacks) applied to an image x ∈ R3×224×224, represented as a vector of
RGB values. Let f : R3×224×224 → R100 be a model mapping images to logits1, and let `(f (x), y)
denote the cross-entropy loss. For an input x with true label y and a target class y0 6= y, our
adversarial attacks attempt to find x0 such that
1.	the attacked image x0 is obtained by applying a constrained distortion to x, and
2.	the loss `(f (x0), y0) is minimized (targeted attack).
Adversarial training (Goodfellow et al., 2014) is a strong defense baseline against a fixed attack
(Madry et al., 2017; Xie et al., 2018) which updates using an attacked image x0 instead of the clean
image x at each training iteration.
We consider 8 attacks: L∞ (Goodfellow et al., 2014), L2 (Szegedy et al., 2013; Carlini & Wagner,
2017), L1 (Chen et al., 2018), Elastic (Xiao et al., 2018), JPEG, Fog, Gabor, and Snow. We show
sample attacked images in Figure 1 and the corresponding distortions in Figure 2. The JPEG, Fog,
1We describe the attacks for ImageNet-100, but they can also be applied to CIFAR-10.
2
Under review as a conference paper at ICLR 2020
skcattA gnitsixE
L2 (1.3m, 4.8k, 99)	L1 (224k, 2.6k, 218)
Elastic (3.1m, 15.2k, 253)
L∞ (4.1m, 11.1k, 32)
JPEG (5.4m, 18.7k, 255)
Fog (4.1m, 13.2k, 89) Gabor (3.7m, 13.3k, 50)
Figure 2: Scaled pixel-level differences between original and attacked images for each attack (label
“espresso maker”). The L1, L2, and L∞ norms of the difference are shown after the attack name.
Our novel attacks display behavior which is qualitatively different from that of the Lp attacks. At-
tacked images are shown in Figure 1, and unscaled differences are shown in Figure 9, Appendix B.1.
Snow (11.3m, 32.0k, 255)
Gabor, and Snow attacks are new to this paper, and the L1 attack uses the Frank-Wolfe algorithm to
improve on previous L1 attacks. We now describe the attacks, whose distortion sizes are controlled
by a parameter ε. We clamp output pixel values to [0, 255].
Existing attacks. The Lp attacks with p ∈ {1, 2, ∞} modify an image x to an attacked image
x0 = x+δ. We optimize δ under the constraint k δ k p ≤ ε, where ∣∣∙kp is the Lp-norm on R3×224×224.
The Elastic attack warps the image by allowing distortions x0 = Flow(x, V ), where V :
{1, . . . , 224}2 → R2 is a vector field on pixel space, and Flow sets the value of pixel (i, j) to
the bilinearly interpolated original value at (i, j) + V (i, j). We construct V by smoothing a vector
field W by a Gaussian kernel (size 25 × 25, std. dev. 3 for a 224 × 224 image) and optimize W under
∣W (i, j)∣∞ ≤ ε for all i, j . This differs in details from Xiao et al. (2018) but is similar in spirit.
Novel attacks. As discussed in Shin & Song (2017) for defense, JPEG compression applies a
lossy linear transformation JPEG based on the discrete cosine transform to image space, followed
by quantization. The JPEG attack imposes the L∞-constraint ∣JPEG(x) - JPEG(x0)∣∞ ≤ ε on the
attacked image x0. We optimize z = JPEG(x0) and apply a right inverse of JPEG to obtain x0.
Our novel Fog, Gabor, and Snow attacks are ad-
versarial versions of non-adversarial distortions
proposed in the literature. Fog and Snow in-
troduce adversarially chosen partial occlusions
of the image resembling the effect of mist and
snowflakes, respectively; stochastic versions of
Fog and Snow appeared in Hendrycks & Diet-
terich (2019). Gabor superimposes adversarially
chosen additive Gabor noise (Lagae et al., 2009)
onto the image; a stochastic version appeared in
Original Initialization Optimized
Figure 3: Snow before and after optimization.
Co et al. (2019). These attacks work by optimizing a set of parameters controlling the distortion
over an L∞-bounded set. Specifically, values for the diamond-square algorithm, sparse noise, and
snowflake brightness (Figure 3) are chosen adversarially for Fog, Gabor, and Snow, respectively.
Optimization. To handle L∞ and L2 constraints, we use randomly-initialized projected gradient
descent (PGD), which optimizes the distortion δ by gradient descent and projection to the L∞ and
L2 balls (Madry et al., 2017). For L1 constraints, this projection is more difficult, and previous
Li attacks resort to heuristics (Chen et al., 2018; Tramer & Boneh, 2019). We use the randomly-
3
Under review as a conference paper at ICLR 2020
Figure 4: Accuracies ofL2 and Elastic attacks at different distortion sizes against a ResNet-50 model
adversarially trained against L2 at ε = 9600 on ImageNet-100. At small distortion sizes, the model
appears to defend well against Elastic, but large distortion sizes reveal a lack of transfer.
initialized Frank-Wolfe algorithm (Frank & Wolfe, 1956), which replaces projection by a simpler
optimization of a linear function at each step (pseudocode in Appendix B.2).
3	Motivation and description of our methodology
We now propose a method to assess robustness against unforeseen distortions, which relies on eval-
uating a defense against a diverse set of attacks that were not used when designing the defense. Our
method must address the following issues:
•	The range of distortion sizes must be wide enough to avoid the misleading behavior in which
robustness appears to transfer at low distortion sizes but not at high distortion sizes (Figure 4);
•	The set of attacks considered must be sufficiently diverse.
We first provide a method to calibrate distortion sizes and then use it to define a summary metric
that assesses the robustness of a defense against a specific unforeseen attack. Using this metric, we
are able to assess diversity and recommend a set of attacks to evaluate against.
Calibrate distortion size using adversarial training. As shown in Figure 4, the correlation
between adversarial robustness against different distortion types may look different for different
ranges of distortion sizes. It is therefore critical to evaluate on a wide enough range of distortion
size ε. We choose the minimum and maximum distortion sizes ε using the following principles;
sample images at εmin and εmax are shown in Figure 5b.
1.	The minimum distortion size εmin is the largest ε for which the adversarial validation ac-
curacy against an adversarially trained model is comparable to that of a model trained and
evaluated on unattacked data (for ImageNet-100, within 3 of 87).
2.	The maximum distortion size εmax is the smallest ε which either (a) yields images which
confuse humans when applied against adversarially trained models or (b) reduces accuracy
of adversarially trained models (ATA below) to below 25.
In practice, we select εmin and εmax according to these criteria from a sequence ofε which is geomet-
rically increasing with ratio 2. We choose to evaluate against adversarially trained models because
attacking against strong defenses is necessary to produce strong visual distortions (Figure 5a). We
introduce the constraint that humans recognize attacked images at εmax because we find cases for
L1 , Fog, and Snow where adversarially trained models maintain non-zero accuracy for distortion
sizes producing images incomprehensible to humans. An example for Snow is shown in Figure 5b.
UAR: an adversarial robustness metric. We measure a model’s robustness against a specific
distortion type by comparing it to adversarially trained models, which represent an approximate
ceiling on performance with prior knowledge of the distortion type. For distortion type A and size
ε, let the Adversarial Training Accuracy ATA(A, ε) be the best adversarial accuracy on the test
set that can be achieved by adversarially training a specific architecture (ResNet-50 for ImageNet-
100, ResNet-56 for CIFAR-10) against A.2 Even when evaluating a defense using an architecture
2As explained in Figure 13 (Appendix C.2), this usually requires training at distortion size ε0 > ε because
the typical distortion seen during adversarial training is sub-maximal.
4
Under review as a conference paper at ICLR 2020
clean vs clean vs ε 2
vs. ε = 8 vs. ε = 16 vs. ε = 32
(a) The L∞ attack at ε = 32 applied to undefended
model and models adversarially trained against L∞
at different distortion sizes. Attacking models trained
against larger ε produces greater visual distortion.
UinUnUln 日 nunXEn
UollJolSIClUoIlJolSICl
JPEG
Gabor
Snow
Gabor Snow
(b) The JPEG, Gabor, and Snow attacks applied to ad-
versarially trained models at εmin and εmax. Distortions
are almost imperceptible at εmin, but make the image
barely recognizable by humans at εmax.
Figure 5:	Varying distortion size against adversarially trained models reveals full attack strength.
other than ResNet-50 or ResNet-56, we recommend using the ATA values computed with these
architectures to allow for uniform comparisons.
Given a set of distortion sizes {ε1, . . . , εn}, we propose the summary metric UAR (Unforeseen
Attack Robustness) normalizing the accuracy of a model M against adversarial training accuracy:
UAR(A, M) := 100 ∙ (n X Acc(A, i®, M)) / (； X ATA(A, εj .	⑴
Here Acc(A, ε, M) is the accuracy of M against distortions of type A and magnitude ε. We expect
most UAR scores to be lower than 100 against held-out distortion types, as an UAR score greater than
100 means that a defense is outperforming an adversarially trained model on that distortion. The
normalizing factor in (1) is required to keep UAR scores roughly comparable between distortions, as
different distortions can have different strengths as measured by ATA at the chosen distortion sizes.
Having too many or too few εk values in a certain range may cause an attack to appear artificially
strong or weak because the functional relation between distortion size and attack strength (measured
by ATA) varies between attacks. To make UAR roughly comparable between distortions, we evaluate
at ε increasing geometrically from εmin to εmax by factors of 2 and take the subset of ε whose ATA
values have minimum '1-distance to the ATA values of the L∞ attack at geometrically increasing ε.
For example, when calibrating Elastic in Table 1, we start with εmin = 0.25 and εmax = 16 based
on our earlier criteria. We then compute the ATAs at the 7 geometrically increasing ε values ε ∈
{0.25, 0.5, 1, 2, 4, 8, 16}. We consider size-6 subsets of those ATA values, view them as vectors of
length 6 in decreasing order, and compute the `1 -distance between these vectors and the vector for
L∞ shown in the first row of Table 1. Finally, we select the ε values for Elastic in Table 1 as those
corresponding to the size-6 subset with minimum '1-distance to the vector for L∞.
For our 8 distortion types, we provide reference values of ATA(A, ε) on this calibrated range of 6
distortion sizes on ImageNet-100 (Table 1, §4) and CIFAR-10 (Table 3, Appendix C.3.2). This al-
lows UAR computation for a new defense using 6 adversarial evaluations and no adversarial training,
reducing computational cost from 192+ to 6 NVIDIA V100 GPU-hours on ImageNet-100.
Evaluate against diverse distortion types. Since robustness against different distortion types may
have low or no correlation (Figure 6b), measuring performance on different distortions is important
to avoid overfitting to a specific type, especially when a defense is constructed with it in mind (as
with adversarial training). Our results in §4 demonstrate that choosing appropriate distortion types to
evaluate against requires some care, as distortions such as L1, L2, and L∞ that may seem different
can actually have highly correlated scores against defenses (see Figure 6). We instead recommend
evaluation against our more diverse attacks, taking the L∞, L1, Elastic, Fog, and Snow attacks as a
starting point.
5
Under review as a conference paper at ICLR 2020
Table 1: Calibrated distortion sizes and ATA values for different distortion types on ImageNet-100.
ATA values for CIFAR-10 are shown in Table 3 (Appendix C.3.2).
AttaCk ει ε2 ε3	ε4	ε5	ε6	ATAI ATA2 ATA3 ATA4 ATA5 ATA6
L∞	1	2	4	8	16	32	84.6	82.1	76.2	66.9	40.1	12.9
L2	150	300	600	1200	2400	4800	85.0	83.5	79.6	72.6	59.1	19.9
L1	9562.5	19125	76500	153000	306000	612000	84.4	82.7	76.3	68.9	56.4	36.1
Elastic	0.250	0.500	2	4	8	16	85.9	83.2	78.1	75.6	57.0	22.5
JPEG	0.062	0.125	0.250	0.500	1	2	85.0	83.2	79.3	72.8	34.8	1.1
Fog	128	256	512	2048	4096	8192	85.8	83.8	79.0	68.4	67.9	64.7
Snow	0.062	0.125	0.250	2	4	8	84.0	81.1	77.7	65.6	59.5	41.2
Gabor	6.250	12.500	25	400	800	1600	84.0	79.8	79.8	66.2	44.7	14.6
4 UAR REVEALS THE NEED TO EVALUATE AGAINST MORE DIVERSE ATTACKS
We apply our methodology to the 8 attacks in §2 using models adversarially trained against these
attacks. Our results reveal that evaluating against the commonly used Lp-attacks gives highly corre-
lated information which does not generalize to other unforeseen attacks. Instead, they suggest that
evaluating on diverse attacks is necessary and identify a set of 5 attacks with low pairwise robustness
transfer which we suggest as a starting point when assessing robustness to unforeseen adversaries.
Dataset and model. We use two datasets: CIFAR-10 and ImageNet-100, the 100-class subset of
ImageNet-1K (Deng et al., 2009) containing every 10th class by WordNet ID order. We use ResNet-
56 for CIFAR-10 and ResNet-50 as implemented in torchvision for ImageNet-100 (He et al.,
2016). We give training hyperparameters in Appendix A.
Adversarial training and evaluation procedure. We construct hardened models using adversarial
training (Madry et al., 2017). To train against attack A, for each mini-batch of training images, we
select a uniform random (incorrect) target class for each image. For maximum distortion size ε, we
apply the targeted attack A to the current model with distortion size ε0 〜Uniform(0, ε) and update
the model with a step of stochastic gradient descent using only the resulting adversarial images (no
clean images). The random size scaling improves performance especially against smaller distortions.
We use 10 optimization steps for all attacks during training except for Elastic, where we use 30 steps
due to its more difficult optimization problem. When PGD is used, We use step size ε∕√steps, the
optimal scaling for non-smooth convex functions (Nemirovski & Yudin, 1978; 1983).
We adversarially train 87 models against the 8 attacks from §2 at the distortion sizes described in
§3 and evaluate them on the ImageNet-100 and CIFAR-10 validation sets against 200-step targeted
attacks with uniform random (incorrect) target class. This uses more steps for evaluation than train-
Normal Training -
L∞ ε = 32 -
L2 ε = 4800 -
Li ε = 612000 -
JPEG ε = 2 -
Elastic ε = 16 -
Fog ε = 8192 -
Gabor ε = 3200 -
Snow ε = 8 -
7	17	22	0	31	16	5	10
88	42	15	14	49	20	55	37
80、8 79 67 48 18 53 38
62	71 ∣89	56	43	18	47	31
65	70	54	92	40	19	52	31
23	25	11	1191	25	41	40
1	3	8	0	28	91	54	43
11	18	12	0	39	31	89	47
13	15	9	1	39	37	60	93
(b) Correlations between UAR scores in Figure 6a for
each attack (rows and columns). Correlation was com-
puted over adversarial defenses in Figure 6a trained
without knowledge of the attacks (6 total per pair).
Correlation of UAR scores
(a) UAR scores for adv. trained defenses (rows)
against attacks (columns) on ImageNet-100. See
Figure 12 for more ε values and Appendix C.3.2
for CIFAR-10 results.
Figure 6:	UAR scores demonstrate the need to evaluate against diverse attacks.
6
Under review as a conference paper at ICLR 2020
ing per best practices (Carlini et al., 2019). We use UAR to analyze the results in the remainder of
this section, directing the reader to Figures 10 and 11 (Appendix C.2) for exhaustive results and to
Appendix D for checks for robustness to random seed and number of attack steps.
Existing defense and evaluation methods do not generalize to unforeseen attacks. The many
low off-diagonal UAR scores in Figure 6a make clear that while adversarial training is a strong base-
line against a fixed distortion, it only rarely confers robustness to unforeseen distortions. Notably,
we were not able to achieve a high UAR against Fog except by directly adversarially training against
it. Despite the general lack of transfer in Figure 6a, the fairly strong transfer between the Lp -attacks
is consistent with recent progress in simultaneous robustness to them (Croce & Hein, 2019).
Figure 6b shows correlations between UAR scores of pairs of attacks A and A0 against defenses
adversarially trained without knowledge3 of A or A0 . The results demonstrate that defenses trained
without knowledge of Lp-attacks have highly correlated UAR scores against the different Lp attacks,
but this correlation does not extend to their evaluations against other attacks. This suggests that Lp -
evaluations offer limited diversity and may not generalize to other unforeseen attacks.
The L∞, L1, Elastic, Fog, and Snow attacks offer greater diversity. Our results on Lp-evaluation
suggest that more diverse attack evaluation is necessary for generalization to unforeseen attacks. As
the unexpected correlation between UAR scores against the pairs (Fog, Gabor) and (JPEG, L1) in
Figure 6b demonstrates, even attacks with very different distortions may have correlated behaviors.
Considering all attacks in Figure 6 together results in signficantly more diversity, which we suggest
for evaluation against unforeseen attacks. We suggest the 5 attacks (L∞, L1, Elastic, Fog, and Snow)
with low UAR against each other and low correlation between UAR scores as a good starting point.
5 Joint adversarial training: defending against two distortions
A natural idea to improve robustness against unforeseen adversaries is to adversarially train the same
model against two different types of distortions simultaneously, with the idea that this will cover a
larger portion of the space of distortions. We refer to this as joint adversarial training (Jordan et al.,
2θl9; Tramer & Boneh, 2019). For two attacks A and A0, at each training step, We compute the
attacked image under both A and A0 and backpropagate with respect to gradients induced by the
image with greater loss. This corresponds to the “max” loss described in Tramer & Boneh (2019).
We jointly train models for (L∞, L2), (L∞, L1), and (L∞, Elastic) using the same setup as before
Normal Training
7 17 22 0 31
Normal Training
7 17 22 0 31
L∞ ε =1, L2 ε = 300
L∞ ε = 2, L2 ε = 600
L∞ ε = 4, L2 ε = 1200
L∞ ε = 8, L2 ε = 2400
L∞ ε = 16, L2 ε = 4800
L∞ ε = 1, Li ε = 38250
L∞ ε = 2, Li ε = 76500
L∞ ε = 4, Li ε = 153000
L∞ ε = 8, Li ε = 306000
L∞ ε = 16, Li ε = 612000
Normal Training
L∞ ε = 1, Elastic ε = 0.5
L∞ ε = 2, Elastic ε = 1
L∞ ε = 4, Elastic ε = 2
L∞ ε = 8, Elastic ε = 4
L∞ ε = 16, Elastic ε = 8
7 17 22 0 31
50	60	43	27	41
63	73	53	41	43
73	81	64	53	45
80	87	74	62	48
82	88	79	67	48
47	61	58	28	41
48	61	66	31	41
51	63	72	35	40
44	56	62	26	36
45	50	35	26	30
43	52	39	24	45
56	62	43	35	54
68	71	46	41	63
35	42	28	10	65
69	54	27	27	43
Figure 7:	UAR scores for jointly adv. trained defenses (rows) against distortion types (columns).
Transfer for jointly trained models. Figure 7 reports UAR scores for jointly trained models using
ResNet-50 on ImageNet-100; full evaluation accuracies are in Figure 19 (Appendix E). Comparing
to Figure 6a and Figure 12 (Appendix E), we see that, relative to training against only L2, joint train-
ing against (L∞ , L2) slightly improves robustness against L1 without harming robustness against
other attacks. In contrast, training against (L∞, L1) is worse than either training against L1 or L∞
separately (except at small ε for L1). Training against (L∞, Elastic) also performs poorly.
Joint training and overfitting. Jointly trained models achieve high training accuracy but poor
validation accuracy (Figure 8) that fluctuates substantially for different random seeds (Table 4, Ap-
pendix E.2). Figure 8 shows the overfitting behavior for (L∞, Elastic): L∞ validation accuracy de-
creases significantly during training while training accuracy increases. This contrasts with standard
adversarial training (Figure 8), where validation accuracy levels off as training accuracy increases.
3We exclude defenses adversarially trained against A and A0 to ensure that attacks are unforeseen.
7
Under review as a conference paper at ICLR 2020
	
/W	l-π**^j--ʌ-j- Train, elastic ε = 4 t^	Train, L∞ ε = 8 Val, elastic ε = 4 Val, L ^∞ ε = 8
		
-		
一			Train, L∞ ε = 8 Val, L^∞ ε = 8
0	20	40	60	80
0	20	40	60	80
Epoch
Figure 8:	Left: train and validation curves for joint training against L∞, ε = 8 and Elastic, ε = 4,
Right: train and val curves for standard adversarial training for L∞, ε = 8. The joint validation
accuracy of L∞ decreases as training progresses, indicating overfitting.
Overfitting primarily occurs when training against large distortions. We successfully trained against
the (L∞ , L1) and (L∞, Elastic) pairs for small distortion sizes with accuracies comparable to but
slightly lower than observed in Figure 11 for training against each attack individually (Figure 18,
Appendix E). This agrees with behavior reported by Tramer & Boneh (2019) on CIFAR-10. Our
intuition is that harder training tasks (more diverse distortion types, larger ε) make overfitting more
likely. We briefly investigate the relation between overfitting and model capacity in Appendix E.3;
validation accuracy appears slightly increased for ResNet-101, but overfitting remains.
6 Discussion and related work
We have seen that robustness to one attack provides limited information about robustness to other
attacks, and moreover that adversarial training provides limited robustness to unforeseen attacks.
These results suggest a need to modify or move beyond adversarial training. While joint adversarial
training is one possible alternative, our results show it often leads to overfitting. Even ignoring this,
it is not clear that joint training would confer robustness to attacks outside of those trained against.
Evaluating robustness has proven difficult, necessitating detailed study of best practices even for a
single fixed attack (Papernot et al., 2017; Athalye et al., 2018). We build on these best practices by
showing how to choose and calibrate a diverse set of unforeseen attacks. Our work is a supplement to
existing practices, not a replacement-we strongly recommend following the guidelines in Papernot
et al. (2017) and Athalye et al. (2018) in addition to our recommendations.
Some caution is necessary when interpreting specific numeric results in our paper. Many previous
implementations of adversarial training fell prone to gradient masking (Papernot et al., 2017; En-
gstrom et al., 2018), with apparently successful training occurring only recently (Madry et al., 2017;
Xie et al., 2018). While evaluating with moderately many PGD steps (200) helps guard against this,
(Qian & Wegman, 2019) shows that an L∞-trained model that appeared robust against L2 actually
had substantially less robustness when evaluating with 106 PGD steps. If this effect is pervasive,
then there may be even less transfer between attacks than our current results suggest.
For evaluating against a fixed attack, DeepFool Moosavi-Dezfooli et al. (2015) and CLEVER Weng
et al. (2018) can be seen as existing alternatives to UAR. They work by estimating “empirical ro-
bustness”, which is the expected minimum ε needed to successfully attack an image. However, these
apply only to attacks which optimize over an Lp-ball of radius ε, and CLEVER can be susceptible
to gradient masking Goodfellow (2018). In addition, empirical robustness is equivalent to linearly
averaging accuracy over ε, which has smaller dynamic range than the geometric average in UAR.
Our results add to a growing line of evidence that evaluating against a single known attack type
provides a misleading picture of the robustness of a model (Sharma & Chen, 2017; Engstrom et al.,
2017; Jordan et al., 2019; Tramer & Boneh, 2019; Jacobsen et al., 2019). Going one step further,
we believe that robustness itself provides only a narrow window into model behavior; in addition to
robustness, we should seek to build a diverse toolbox for understanding machine learning models,
including visualization (Olah et al., 2018; Zhang & Zhu, 2019), disentanglement of relevant features
(Geirhos et al., 2018), and measurement of extrapolation to different datasets (Torralba & Efros,
2011) or the long tail of natural but unusual inputs (Hendrycks et al., 2019). Together, these windows
into model behavior can give us a clearer picture of how to make models reliable in the real world.
8
Under review as a conference paper at ICLR 2020
References
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. CoRR, abs/1707.07397, 2017. URL http://arxiv.org/abs/1707.07397.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.
Tom B. Brown, Dandelion Mane, AUrko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch.
CoRR, abs/1712.09665, 2017. URL http://arxiv.org/abs/1712.09665.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian J. Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial
robustness. CoRR, abs/1902.06705, 2019. URL http://arxiv.org/abs/1902.06705.
Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. EAD: Elastic-net attacks
to deep neural networks via adversarial examples. In Thirty-second AAAI conference on artificial
intelligence, 2018.
Kenneth T. Co, LUis Munoz-Gonzalez, and Emil C. Lupu. Sensitivity of deep convolutional net-
works to Gabor noise. CoRR, abs/1906.03455, 2019. URL http://arxiv.org/abs/
1906.03455.
Francesco Croce and Matthias Hein. Provable robustness against all adversarial lp-perturbations for
p ≥ 1. CoRR, abs/1905.11213, 2019. URL http://arxiv.org/abs/1905.11213.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. IEEE, 2009.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation suffice: Fooling CNNs with simple transformations. arXiv preprint
arXiv:1712.02779, 2017.
Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness
of adversarial logit pairing. arXiv preprint arXiv:1807.10272, 2018.
Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. Adversarial examples are a natural
consequence of test error in noise. arXiv preprint arXiv:1901.10513, 2019.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95-110, 1956.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias
improves accuracy and robustness. CoRR, abs/1811.12231, 2018. URL http://arxiv.org/
abs/1811.12231.
Ian Goodfellow. Gradient masking causes CLEVER to overestimate adversarial perturbation size.
arXiv preprint arXiv:1804.07870, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
9
Under review as a conference paper at ICLR 2020
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In International Conference on Learning Representations, 2019.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. arXiv preprint arXiv:1907.07174, 2019.
Jrn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tramr, and Nicolas Papernot. Ex-
ploiting excessive invariance caused by norm-bounded adversarial robustness, 2019.
Matt Jordan, Naren Manoj, Surbhi Goel, and Alexandros G. Dimakis. Quantifying perceptual dis-
tortion of adversarial examples. arXiv e-prints, art. arXiv:1902.08265, Feb 2019.
Ares Lagae, Sylvain Lefebvre, George Drettakis, and Philip Dutre. Procedural noise using sparse
Gabor convolution. ACMTrans. Graph., 28(3):54:1-54:10, July 2009. ISSN 0730-0301. doi: 10.
1145/1531326.1531360. URL http://doi.acm.org/10.1145/1531326.1531360.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple and
accurate method to fool deep neural networks. arXiv preprint arXiv:1511.04599, 2015.
Arkadi Nemirovski and D Yudin. On Cezari’s convergence of the steepest descent method for
approximating saddle point of convex-concave functions. In Soviet Math. Dokl, volume 19, pp.
258-269, 1978.
Arkadi Nemirovski and D Yudin. Problem Complexity and Method Efficiency in Optimization.
Intersci. Ser. Discrete Math. Wiley, New York, 1983.
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and
Alexander Mordvintsev. The building blocks of interpretability. Distill, 2018. doi: 10.23915/
distill.00010. https://distill.pub/2018/building-blocks.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519. ACM, 2017.
Haifeng Qian and Mark N. Wegman. L2-nonexpansive neural networks. In International Conference
on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?
id=ByxGSsR9FQ.
Yash Sharma and Pin-Yu Chen. Attacking the Madry defense model with L1-based adversarial
examples. arXiv e-prints, art. arXiv:1710.10733, Oct 2017.
Richard Shin and Dawn Song. JPEG-resistant adversarial images. In NIPS 2017 Workshop on
Machine Learning and Computer Security, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2011.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations.
arXiv e-prints, art. arXiv:1904.13000, Apr 2019.
Igor Vasiljevic, Ayan Chakrabarti, and Gregory Shakhnarovich. Examining the impact of blur on
recognition by convolutional networks. CoRR, abs/1611.05760, 2016. URL http://arxiv.
org/abs/1611.05760.
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach.
arXiv preprint arXiv:1801.10578, 2018.
10
Under review as a conference paper at ICLR 2020
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially trans-
formed adversarial examples. arXiv preprint arXiv:1801.02612, 2018.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. arXiv preprint arXiv:1812.03411, 2018.
Tianyuan Zhang and Zhanxing Zhu. Interpreting adversarially trained convolutional neural net-
works. In International Conference on Machine Learning (ICML), 2019.
11
Under review as a conference paper at ICLR 2020
A Training hyperparameters
For ImageNet-100, we trained on machines with 8 NVIDIA V100 GPUs using standard data aug-
mentation He et al. (2016). Following best practices for multi-GPU training Goyal et al. (2017),
we ran synchronized SGD for 90 epochs with batch size 32×8 and a learning rate schedule with 5
“warm-up” epochs and a decay at epochs 30, 60, and 80 by a factor of 10. Initial learning rate after
warm-up was 0.1, momentum was 0.9, and weight decay was 10-4. For CIFAR-10, we trained on a
single NVIDIA V100 GPU for 200 epochs with batch size 32, initial learning rate 0.1, momentum
0.9, and weight decay 10-4. We decayed the learning rate at epochs 100 and 150.
B	Further Attack Details
B.1	Further examples of attacks
We show the images corresponding to the ones in Figure 2, with the exception that they are not
scaled. The non-scaled images are shown in Figure 9.
B.2	L1 ATTACK
We chose to use the Frank-Wolfe algorithm for optimizing the L1 attack, as Projected Gradient
Descent would require projecting onto a truncated L1 ball, which is a complicated operation. In
contrast, Frank-Wolfe only requires optimizing linear functions g>x over a truncated L1 ball; this
can be done by sorting coordinates by the magnitude of g and moving the top k coordinates to the
boundary of their range (with k chosen by binary search). This is detailed in Algorithm 1.
C Full evaluation results
C.1 L1-JPEG AND L2-JPEG ATTACKS
We will present results with two additional versions of the JPEG attack which impose L1 or L2
constraints on the attack in JPEG-space instead of the L∞ constraint discussed in Section 2. To avoid
confusion, in this appendix, we denote the original JPEG attack by L∞-JPEG and these variants by
L1-JPEG and L2-JPEG, respectively. Comparing the L1-JPEG and L2-JPEG attacks in Figure 10,
skcattA gnitsixE
L2 (1.3m, 4.8k, 99)
L∞ (4.1m, 11.1k, 32)
JPEG (5.4m, 18.7k, 255)
Fog (4.1m, 13.2k, 89)
L1 (224k, 2.6k, 218)	Elastic (3.1m, 15.2k, 253)
Gabor (3.7m, 13.3k, 50) Snow (11.4m, 32.0k, 255)
Figure 9: Differences of the attacked images and original image for different attacks (label “espresso
maker”). The L1, L2, and L∞ norms of the difference are shown in parentheses. As shown, our
novel attacks display qualitatively different behavior and do not fall under the Lp threat model.
These differences are not scaled and are normalized so that no difference corresponds to white.
12
Under review as a conference paper at ICLR 2020
Algorithm 1 Pseudocode for the Frank-Wolfe algorithm for the L1 attack.
1:	Input: function f, initial input x ∈ [0, 1]d, L1 radius ρ, number of steps T.
2:	Output: approximate maximizer X of f over the truncated Li ball Bι(ρ; x) ∩ [0,1]d centered
at x.
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
x(0) J RandomInit(X)	. Random initialization
for t = 1, . . . , T do
g J Vf (χ(t-i))	. Obtain gradient
for k = 1, . . . , d do
sk J index of the coordinate of g by with kth largest norm
end for
Sk J {s1, . . . , sk}.
for i = 1, . . . , d do	. Compute move to boundary of [0, 1] for each coordinate.
if gi > 0 then
bi J 1 - Xi
else
bi J -Xi
end if
end for
Mk J Pi∈Sk |bi|	. Compute L1-perturbation of moving k largest coordinates.
k* J max{k | Mk ≤ ρ}	. Choose largest k satisfying Li constraint.
C ・	TTl	z-x	. ʌ	∙	∖	,1 TFIl
■ /»»• r> —	z√ rlc	Z f γΛτnτιιιrf2* ʃr* TnelYlTnlF71nc C l ʃr* rwαr 1^hα / . Ko 11
for i = 1, . . . , d do	. Compute X maximizing g X over the Li ball.
if i ∈ Sk* then
Xi J Xi + bi
else ifi = sk*+i then
Xi J Xi + (p — Mk* )sign(gi)
else
Xi J Xi
end if
end for
X(t) J (1 — 1 )X(t-i) + 1X	. Average X with previous iterates
end for
X J X(T)
Table 2: ATA values for Li-JPEG and L2-JPEG on ImageNet-100.
Attack	ε1	ε2	ε3	ε4	ε5	ε6	ATA1	ATA2	ATA3 ATA4 ATA5			ATA6
L2-JPEG 8	16	32	64	128	^256~~	84.8	82.5	78.9	72.3	47.5	3.4
L1 -JPEG 256	1024 4096		16384 65536		131072	84.8	81.8	76.2	67.1	46.4	41.8
we find that they have extremely similar results, so we omit Li-JPEG in the full analysis for brevity
and visibility. Calibration values for these attacks are shown in Table 2.
C.2 Full evaluation results and analysis for ImageNet- 1 00
We show the full results of all adversarial attacks against all adversarial defenses for ImageNet-100
in Figure 11. As described, the Lp attacks and defenses give highly correlated information on held-
out defenses and attacks respectively. Thus, we recommend evaluating on a wide range of distortion
types. Full UAR scores are also provided for ImageNet-100 in Figure 12.
We further show selected results in Figure 13. As shown, a wide range ofε is required to see the full
behavior.
13
Under review as a conference paper at ICLR 2020
Normal training -
70 0 0 0 0 0
50 0 0 0 0 0
π- 1.0
L∞ ε = 1 _	8610 0 0 0 0	■76	5 0	0	0	0|	
L∞ ε = 2 -	85 34 1 0 0 0	79 16 0		0	0	0∣	
L∞ ε = 4 -	8454 5 0 0 0	79 31 1		0	0	0∣	
L∞ ε = 8 -	79 43 6 0 0 0	74 33 2		0	0	0∣	
L∞ ε = 16 -	73 26 3 0 0 0	64 25 3		0	0	0∣	
L∞ ε = 32 -	68 7 1 0 0 0	56 23 4		0	0	0	
							
L2 ε = 150 -	85 2 0 0 0 0	.73	2 0	0	0	0	
L2 ε = 300 -	8517 0 0 0 0	∣78	7 0	0	0	0	
L2 ε = 600 -	85 49 2 0 0 0	82 24 0		0	0	0	-0.8
L2 ε = 1200 -	82 74 25 0 0 0	8156 4		0	0	0	
L2 ε = 2400 -	777457 7 0 0	77 68 25		1	0	0	
L2 ε = 4800 -	68676233 1 0	68 66 4912			2	3	
Attack (adversarial training)
Li ε = 9562.44 - Li ε = 19125 - Li ε = 38250.1 - Li ε = 76500 - Li ε = 153000 - Li ε = 306000 - Li ε = 612000 -	84 1 0 0 0 0 84 3 0 0 0 0 84 20 0 0 0 0 8448 4 0 0 0 806216 0 0 0 78 68 41 2 0 0 71675313 0 0		71| 1 0 0 0 0 78∣5 0 0 0 0 8118 0 0 0 0 8347 2 0 0 0 8071 12 0 0 0 78 74 36 1 0 0 716952 6 0 0	
L∞-JPEG ε = 0.03125 - L∞-JPEG ε = 0.0625 - L∞-JPEG ε = 0.125 - L∞-JPEG ε = 0.25 - L∞-JPEG ε = 0.5 - L∞-JPEG ε = 1 - L∞-JPEG ε = 2 - L∞-JPEG ε = 4 -				
	86 55 3 0 0 0 87 7418 0 0 0 868151 1 0 0 8482 7317 0 0 8180 7757 2 0 80 79 76 68 32 0 78 78 76 67 4517 76 75 72 59 30 8		8336 0000 8557 2 0 0 0 85 7214 0 0 0 84 78∣43 2 0 0 80 79 69 28 2 1 80 79 76 68 48 48 78 77 75 69 55 47 76 75 73 66 56 50	
L2-JPEG ε = 2- L2-JPEG ε = 4 - L2-JPEG ε = 8 - L2-JPEG ε = 16 - L2-JPEG ε = 32 - L2-JPEG ε = 64 - L2-JPEG ε = 128 - L2-JPEG ε = 256 -				
	86 23 0 0 0 0 86 56 3 0 0 0 86 76 24 0 0 0 868261 300 8482 76 29 0 0 8180 7965 4 0 78 77 76 72 39 0 78 77767247 3		8317 0000 8441 0000 85 64 3 0 0 0 85 78 24 0 0 0 8481∣57 3 0 0 8180 73 34 2 1 77 77 75673832 78 77 75 70 56 57	
LI-JPEG ε = 128
LI-JPEG ε = 256
LI-JPEG ε = 512
LI-JPEG ε = 1024
LI-JPEG ε = 2048
LI-JPEG ε = 4096
LI-JPEG ε = 8192
LI-JPEG ε = 16384
LI-JPEG ε = 32768
LI-JPEG ε = 65536
Li-JPEG ε = 131072
Elastic ε = 0.25
Elastic ε = 0.5
Elastic ε = 1
Elastic ε = 2
Elastic ε = 4
Elastic ε = 8
Elastic ε = 16
86 40 1 0 0 0
8661 6000
86 75 27 0 0 0
868157 300
8582 73 22 0 0
8382 7850 1 0
8180 7964 6 0
80 79 78 7017 0
78 77 76 70 22 0
75 74 7468 24 0
73 72716418 0
84 1 0 0 0 0
83 4 0 0 0 0
82 8 0 0 0 0
78 4 0 0 0 0
70 1 0 0 0 0
55 0 0 0 0 0
85 48 0 0 0 0
85 63 2 0 0 0
85 7510 0 0 0
86 80 34 0 0 0
848157 200
83827118 0 0
80 80 75 45 4 4
79 79 76621917
78 77 75653126
75 75 73 67 46 42
737271644137
62 1 0 0 0 0
67 3 0 0 0 0
67 6 0 0 0 0
68 9 0 0 0 0
61 7 0 0 0 0
45 3 0 0 0 0
32 1 0 0 0 0
/, %∙ c⅛ ⅞3 ʃv a
9幻幻幻S(I [,
Attack (evaluation)
Adversarial accuracy
-0.2
L 0.0
Figure 10: A comparison of L1-JPEG and L2-JPEG attacks.
14
Normal training - 困
1
5
	14 50	O O 2 O	O O
03 02 74 22 O 79 7976 59 6 74 74 73 67 34 71 70 69 6240			O O 1 B
			
B5Θ371 lθ
B4 0379 40
79 78 73 50
73 7163 30
B6B4 6610
05 04 77 34
04 03 79 54
79 7B 7043
73 70 59 26
68 6134 7
1
L2 ε-150
l2ε-300
L2 ε-600
£.2 ε-1200
⅛ £-2400
La e-4000
■7422 O O
B4B156 4 O
82 8174 28 O
77 7β74 56 6
6B 68 67 6128
E-9562.44 -『 1ε-19125 - J	71 7B	24 41	1 O 3 O	
ε-3θ250.1 - J ιε-76500 - J ε-153000 - J ε-306000 - £ ε-612000 -E	Bl 6211 O 02 7120 1 79 72 43 3 77725310 71 69 59 24			O O O O O O O O 10
				
U-JPEG £-0.03125 - J
La-JPEG £-0.0625 -，
J-JPEG ε-0.125 -,
U-JPEG ε-0.25 -]
U-JPEG ε≡ 0.5 - J
U-JPEGe-I- j
La-JPEGe-2 -E
75 2B 0047	1 O 3 O	O O O O
83 6814 O B3 7742 3 βθ 786617 79 77 68 27 77 7663 19		O O O O 1 O 1 O 10
		
69 6033 5
1
⅛JP∈Ge-2
L2-JPEG £-4
L2-JPEG ε"θ
L2-JPEG ε≡ 16
L2-JPEG £-32
L2-JPEG «-64
L3-JPEG ε-12θ
L2-JPEG E-256
6412 O .26 1 ■50 4 B3 7015 03 7844	O O O O 3 17 40 36	O O O O O O O O O O O O 3 O 2 O
Bl 79 67 7θ7672. 777671		
77 60 24 2 0
00 67 37 7 0
BO 73 53 IB 2
7B 74 62 35 7
77 75 60 50 21
77 75 68 48 17
04 7119 O
84 7943 2
03 βl6613
BO 797441
79 7074 50
7B 77 7347
B0 4∣ 2 O O
B3 59 B O O
■7424 1 O
848050 3 O
03 02 7010 O
01 00 76 49 4
7B 77 75 6316
77 77 75 6215
W
B3 765417 1	0	0	0	0	0	0
04 817041 θ	0	0	0	0	0	0
05 0379 6420	3	0	0	0	0	0
05 04 03 70 60 24	2	0	0	0	0
B4B3B3 Bl 75 57 22 3 0 0 0
01 0101 00 70 73 60 3410 2 1
77 77 77 77 76 75 72 67 55 38 32
78 78 77 77 77 75 7470 63 56 57
B2 5910 O O O O 02 6515 O O O O 03 6919 1 O O O 83 73 30 1 O O O ^H42 3 0 0 0 BO7654 9000 77 7562 19 IOO
77 75 6419 1 O O
Li-JPEG ε-12θ
Li-JPEGe-256
Li-JPEG ε-512
Li-JPEG ε-1024
Li-JPEG E-204B
Ll-JPEG ε-4096
Li-JPEG E-Θ192
Li-JPEG £- 163B4
Li-JPEG E-3276θ
Li-JPEG ε-65536
Li-JPEG ε -131072
Elastic E-0.25 -H
Elastic e≡0.5 - y
Elastic £-1-3
Elastic £-2-3
Elastic e≡4 - 5
Elastice-B- ɪ
Elastice ≡16 - !
D∣6612 O
B2 7534 1
B2 7955 5
BO 786414
797B7026
77 77 71 33
75 74 69 37
73 7165 29
■6311 O O
H7426 1 O
B4B051 3 O
04 Bl 67 13 O
03 Bl 75 34 ɪ
BO BO 76 40 3
79 79 76 50 B
7B 77 75 60 11
75 75 73 5913
73 72 69 5410
AdVerSaria- accuracy
UlIderreVieW as a COnferenCe PaPer at ICLR 2020
Fog 3128 -
Fog ε "256 -
Fog ε-512 -
Fog ε ≡ 1024 -
Fog ε ≡ 204B -
Fog ε≡4096 -
FOg £■ 8192 -
Fogε-163B4-
Fogε-3276θ-
FogE - 65536-
Gabor £-6.25
Gabor f-12.5
Gabor ε≡ 25
Gabor ε≡ 50
Gabore >100
Gabor £>200
Gabor ε≡ 400
Gaborc "BOO
GabOrg - 1600
Gabor ε> 3200
Snow £-0.03125
Snowε> 0.0625
Snowε> 0.125
Snow ε≡ 0.25
Snow ε-0.5
Snow e≡l
Snow e≡2
Snow £ - 4
Snow £■ B
Snowe-16
4 3 4 51013
7 2 5 4 4 4
2 2 2 2 3 3

I
5B15 1
5915 1
6016 1
5615 1
B3 69 33 3 O O O O O O
lB5 70 59 21 2 O O O O O
8682 745116 2 1111
06 04 70674316 5 2 1 1
04 03 79 7156 3416 7 3 1
79 7B 76 72 67 5B 46 2910 3
7070 70 6B6B 6B 65 57 3917
6465 6565 64 64 64 62 5333
56 57 57 57 57 57 55 514123
15150 5150 50 49 40 45 39 20
l∖∖∖∖∖
83 7438 2
02 73 37 3
01 7440 3
BO 72 39 4
BO 72 30 4
79 7142 6
7β 7043 9
77 69 4510
75 6847 13
Figure 11: Accuracy of adversarial attack (column) against adversarially trained model (row) on ImageNet-IOO.
Under review as a conference paper at ICLR 2020
Normal Training
7 17 22 0	0 31 16 5 10
Normal Training
7 17 22 0	0 31 16 5 10
Normal Training
7 17 22 0	0 31 16 5 10
L∞ ε = 1
46 54
37
24
21
40
29
29
25
L2 ε = 150
38
49
38
15
13
39
29
22
20
Li ε = 9562
26
40 43
5	6
37
22
14
16
L∞ ε = 2
60 64
42
36
30
42
29
41
31
L2 ε = 300
50
60
44
27
24
40
29
33
26
Li ε = 19125
33
47 49
12 14
39
23
21
20
L∞ ε = 4
72 74
48
45
37
44
27
53
37
L2 ε = 600
62
72
53
40
36
42
28
44
31
Li ε = 76500
50
63
70
34
35
41
24
38
27
L∞ ε = 8
83
72
42
42
32
47
23
60
41
L2 ε = 1200
73
82
65
54
49
46
26
54
37
Li ε = 153000
54
66
81
42
42
41
24
44
30
L∞ ε = 16
L∞ ε = 32
L∞-JPEG ε = 0.0625
L∞-JPEG ε = 0.125
L∞-JPEG ε = 0.25
L∞-JPEG ε = 0.5
L∞-JPEG ε = 1
L∞-JPEG ε = 2
Fog ε = 128
Fog ε = 256
Fog ε = 512
Fog ε = 2048
Fog ε = 4096
Fog ε = 8192
89
88
36
46
56
67
69
65
12
11
8
5
2
60
27
30
24
49
19
58
41
L2 ε = 2400
80
88
75
63
58
48
22
57
40
Li ε = 306000
59
70
87
51
50
43
21
48
33
42
15
14
11
49
20
55
37
L2 ε = 4800
80
88
79
67
63
48
18
53
38
Li ε = 612000
62
71
89
56
55
43
18
47
31
44 34
49 48
38
23
17
16
L2-JPEG ε = 8
37 46 36
49 50
38
23
17
17
Elastic ε = 0.25
21
32
30
4
3
41
24
14
18
52
38
63 59
39
22
27
20
L2-JPEG ε = 16
47 55
41
63 62
39
24
26
20
Elastic ε = 0.5
27
38
34
10
7
46
27
22
24
61
69
72
70
21
22
18
12
7
43
48
22
21
18
15
8
73 69
40
41
56
54
85 80
41
40
0
0
34
0
0
35
0
0
36
0
0
0
0
34
22
21
21
19
41
50
39
50
53
52
6
8
10
58
36
20
90
24
30
32
31
15
19
23
31
29 34
1
3
8
0
0
28
91
54 43
L2-JPEG ε = 32
L2-JPEG ε = 64
L2-JPEG ε = 128
L2-JPEG ε = 256
Gabor ε = 6.25
Gabor ε = 12.5
Gabor ε = 25
Gabor ε = 400
Gabor ε = 800
Gabor ε = 1600
57 63
67 73
74 77
72 76
17
28
11
20
7
15
46
53
59
61
26
22
18
10
18
11
19
72 74
84 84
90 93
96
1
0
0
0
0
0
14
14
0
0
0
0
40
41
43
43
39
40
39
39
39
24
23
21
21
30
32
29
25
27
36
48
53
53
46
59
64
68
73
25
31
36
36
30
36
39
36
37
12
19
14
0
0
39
29
82
40
Elastic ε = 2
Elastic ε = 4
Elastic ε = 8
Elastic ε = 16
Snow ε = 0.0625
Snow ε = 0.125
Snow ε = 0.25
Snow ε = 2
Snow ε = 4
Snow ε = 8
37
36
31
23
15
14
10
8
8
46
44
36
25
24
22
16
9
8
37
30
19
11
22
20
16
4
4
19
11
4
1
0
0
0
0
0
15
68
30
42
36
9
3
1
0
0
0
0
0
81
91
91
32
33
34
34
34
29
26
25
35
36
39
38
36
46
45
41
18
28
39
52
50
40
39
40
39
52
59
71
78
13
15
9
1
0
39
37
60
93
Figure 12:	UAR scores (multiplied by 100) for adv. trained defenses (rows) against distortion types
(columns) for ImageNet-100.
L∞-JPEG attacks against L∞-JPEG
L∞-attacks against L∞
Ooooo
8 6 4 2
Noe-Inooe -EeS,IOAPX/
0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	0	5	10	15	20	25	30
L∞-JPEG distortion size for adversarial training	L∞ distortion size for adversarial training
Figure 13:	Adversarial accuracies of attacks on adversarially trained models for different distortion
sizes on ImageNet-100. For a given attack ε, the best ε0 to train against satisfies ε0 > ε because the
random scaling of ε0 during adversarial training ensures that a typical distortion during adversarial
training has size smaller than ε0 .
C.3 Full evaluation results and analysis for CIFAR- 1 0
C.3.1 Full results for CIFAR 1 0
We show the results of adversarial attacks and defenses for CIFAR-10 in Figure 14. We experienced
difficulty training the L2 and L1 attacks at distortion sizes greater than those shown and have omitted
those runs, which we believe may be related to the small size of CIFAR-10 images.
C.3.2 ATAAND UAR FOR CIFAR- 1 0
The ε calibration procedure for CIFAR-10 was similar to that used for ImageNet-100. We started
with the perceptually small εmin values in Table 3 and increased ε geometrically with ratio 2 until
adversarial accuracy of an adversarially trained model dropped below 40. Note that this threshold
16
Under review as a conference paper at ICLR 2020
Table 3: Calibrated distortion sizes and ATA values for ResNet-56 on CIFAR-10
Attack	ε1	ε2	ε3	ε4	ε5	ε6	ATA1	ATA2	ATA3	ATA4	ATA5	ATA6
L∞	1	2	4	8	16	12^^	91.0	87.8	81.6	71.3	46.5	23.1
L2	40	80	160	320	640	2560	90.1	86.4	79.6	67.3	49.9	17.3
L1	195	390	780	1560 6240 24960 92.2				90.0	83.2	73.8	47.4	35.3
L∞-JPEG 0.03125 0.0625			0.125	0.25	0.5	1	89.7	87.0	83.1	78.6	69.7	35.4
L1-JPEG	2	8	64	256	512	1024	91.4	88.1	80.2	68.9	56.3	37.7
Elastic	0.125	0.25	0.5	1	2	8	87.4	81.3	72.1	58.2	45.4	27.8
is higher for CIFAR-10 because there are fewer classes. The resulting ATA and UAR values for
CIFAR10 are shown in Table 3 and Figure 15. We omitted calibration for the L2-JPEG attack
because we chose too small a range of ε for our initial training experiments, and we plan to address
this issue in the future.
D Robustness of our results
D. 1 Replication
We replicated our results for the first three rows of Figure 11 with different random seeds to see the
variation in our results. As shown in Figure 16, deviations in results are minor.
D.2 Convergence
We replicated the results in Figure 11 with 50 instead of 200 steps to see how the results changed
based on the number of steps in the attack. As shown in Figure 17, the deviations are minor.
E	Further Results for Joint Training
E.1 Full experimental results
We show the evaluation accuracies of jointly trained models in Figure 18.
We show all the attacks against the jointly adversarially trained defenses in Figure 19.
E.2 Dependence on random seed
In Table 4, we study the dependence of joint adversarial training to random seed. We find that at
large distortion sizes, joint training for certain pairs of distortions does not produce consistent results
over different random initializations.
Table 4: Train and val accuracies for joint adversarial training at large distortion are dependent on
seed. For train and val, ε0 is chosen uniformly at random between 0 and ε, and we used 10 steps for
L∞ and L1 and 30 steps for elastic. Single adversarial training baselines are also shown.
Training parameters (ResNet-50)	L∞ train	other train	L∞ val	other val
L∞ ε = 8, Elastic ε = 4, Seed 1	-90	89	35	74
L∞ ε = 8, Elastic ε = 4, Seed 2	89	90	47	44
L∞ ε = 8, Elastic ε = 4, Seed 3	90	89	29	63
L∞ε = 16,L1ε = 612000, Seed 1	-86	87	22	16
L∞ε = 16,L1ε = 612000, Seed 2	88	87	16	24
L∞ ε = 8	^"81	一	74	一
L∞ ε = 16	68	一	63	一
Elastic ε = 4	—	88	—	76
L1 ε = 612000	—	75	—	59
17
Under review as a conference paper at ICLR 2020
Table 5: Training and validation numbers for ResNet-101 and ResNet-50 for joint training against
L∞ , ε = 8 and elastic, ε = 4.
Training parameters		L∞ train		other train	L∞ val	other val
L∞ ε	8, Elastic ε =	4, ResNet-50 Seed 1	90	89	35	74
L∞ ε	8, Elastic ε =	4, ResNet-50 Seed 2	89	90	47	44
L∞ ε	8, Elastic ε =	4 ResNet-101	90	91	49	46
E.3 Overfitting and model capacity
As a first test to understand the relationship between model capacity and overfitting, we trained
ResNet-101 models using the same procedure as in Section 5. Briefly, overfitting still occurs, but
ResNet-101 achieves a few percentage points higher than ResNet-50.
We show the training curves in Figure 20 and the training and validation numbers in Table 5.
F Comparing adversarial distortions and common corruptions
“Common” visual corruptions such as (non-adversarial) fog, blur, or pixelation have emerged as
another avenue for measuring the robustness of computer vision models (Vasiljevic et al., 2016;
Hendrycks & Dietterich, 2019; Geirhos et al., 2018). Recent work suggests that robustness to such
common corruptions is linked to adversarial robustness and proposes corruption robustness as an
easily computed indicator of adversarial robustness (Ford et al., 2019). We consider this alternative
to our methodology by testing corruption robustness of our models on the ImageNet-C benchmark.
Experimental setup. We evaluate on the 100-class subset of the corruption robustness benchmark
ImageNet-C introduced in (Hendrycks & Dietterich, 2019) with the same classes as ImageNet-100,
which we call ImageNet-C-100. It is the ImageNet-100 validation set with 19 common corrup-
tions at 5 severities. We use the JPEG files available at https://github.com/hendrycks/
robustness. We show average accuracies by distortion type in Figure 21.
Adversarial training against small distortions increases corruption robustness. The first col-
umn of each block in Figure 21 shows that training against small adversarial distortions generally
increases average accuracy compared to an undefended model. However, training against larger
distortions often decreases average accuracy, largely due to the resulting decrease in clean accuracy.
Adversarial distortions and common corruptions can affect defenses differently. Our Lp -JPEG
and elastic attacks are adversarial versions of the corresponding common corruptions. While training
against adversarial JPEG at larger ε improves robustness against adversarial JPEG attacks (Figure 12
in Appendix C.2), Figure 21 shows that robustness against common JPEG corruptions decreases as
we adversarially train against JPEG at larger ε, though it remains better than for normally trained
models. Similarly, adversarial Elastic training at large ε begins to hurt robustness to its common
counterpart. This is likely because common corruptions are easier than adversarial distortions, hence
the increased robustness does not make up for the decreased clean accuracy.
G Attacks against undefended models
We show sample images of our attacks against undefended models trained in the normal way in
Figure 22.
18
Normal training
91 83 49

μy]
	93	■	89 79 37 1	O O
	93		91 86 64 12	O O
	91		90 88 78 40	1 O
	89		88 87 82 63	13 O
	83		83 82 80 71	39 2
BSl	84	I	83 82 77 66	47 23
90 86 73 44	9	O	OOO		83 50	3 0 0
90 87 78 54	17	1	OOO		87 69	14 O O
90 88 80 62	28	3	OOO		88 79	40 1 O
87 86 80 65	35	5	OOO		86 80	53 4 O
83 81 78 69	48	16	IOO		81 78	64 20 O
81 78 73 63	48	27	8 10		82 77	62 29 6
L2 ε = 10	92	■ 81 48	4 O	O O		91 89 80 42	3	3 5 5 3	1
L2 ε = 20	94	.88 67	13 O	O O		93 92 87 62	8	0 3 3 2	1
G £ = 40	93	H 90 81	42 1	O O		93 92 90 78	32	13 3 2	1
L2 ε = 80	92	H 90 86	66 13	O O		92 91 90 85	61	9 0 3 2	1
2 E = 160	90	.89 87	77 38	1 O		90 90 89 86	75	33 O O 1	O
2 ε = 320	87	.86 85 80 60		11 O		87 86 86 85 80		57 8 O O	O
2 6 = 640	80	H 79 79 77 68		34 1		80 80 80 79 77		67 32 O O	O
ε = 1280	73	H 73 73 71 66		49 14		73 73 73 73 71 66 50 15 O			O
ε = 2560 -	69	，69 68 67 63		49 33		69 68 68 68 67 62 50 32 17			4
ε = 5120	77	■ 76 746855		30 7		77 76 75 74 67 54 24 3 O			O
Lw-JPEGε = 0.03125
L00-JPEGe = 0.0625
L00-JPEGe = 0.125
L00-JPEGe = 0.25
L00-JPEGe = 0.5
U-JPEG ε = l
L2-JPEG £ = 0.0625
L2-JPEG ε = 0.125
L2-JPEG E = 0.25
L2-JPEGe = 0.5
L2-JPEG ε=l
L2-JPEG ε = 2
L2-JPEG E = 4	88
L2-JPEG ε = 8	85
	
Li-JPEG £=1-	93
Li-JPEG £ = 2-	93
LrJPEG ε = 4	91
Li-JPEG £ = 8-	91
LrJPEGε = 16 -	89
LrJPEGe = 32	88
LrJPEGe = 64	88
LrJPEGε = 128	86
LrJPEGe = 256	85
LrJPEG ε = 512	84
LrJPEG ε = 1024	82
74 23	O	OOO
77 30	O	OOO
86 58	7	OOO
88 71	20	OOO
86 69	21	OOO
87 80	51	5 0 0
86 81	61	15 O O
83 80	68 30 2 OI	
92 87 63 11 92 88 70 18	O 2 O 2
93 91 81 42	2 1
92 91 86 61	9 O
91 89 85 63	13 O
89 88 86 76	38 2
88 87 85 79	52 7
85 84 83 78	62 19
Elastice = 0.125
Elastic ε = 0.25
Elastic ε = 0.5
Elastic ε = 1
Elastic ε = 2
Elastic ε = 4
Elastic ε = 8
Elastic ε = 16
68 17 O O O
74 29 1 O O
55 10 O O O
40 4 O O O
38 5 O O O
39 6 O O O
44 9 O O O
67 34 7 1 O
48 49 45 39 25
79 37	IOOO
85 60	10 O O O
78 44	4 0 0 0
87∣75	34 1 O O
87 79	47 4 O O
85 79	54 8 O O
86 81	60 14 O O
84 81	64 20 O O
84 81	67 27 1 O
83 80	71 38 2 O
81 79	7140 3 O
92 89 74	26	1	1 3 ∙
92 90 83	50	5	1 4 4
90 87 77	40	2	O 2 ：
90 89 86	70	23	O 1 ：
89 88 86	76	37	1 1 ∙
87 87 85	77	46	4 O ：
87 87 85 79		53	7 O ：
86 85 84 79		58	12 O (
85 84 83 79		62	18 O (
84 83 82 79		67	28 O (
82 82 81 78		67	31 1 (
88 81 62 27	3	O	OOO			■	90 83 50 2	O	O
88 83 68 38	7	O	OOO				90 87 76 28	O	O
87 83 73 48	13	1	OOO				88 87 83 63	5	O
84 81 74 56	23	2	OOO				85 85 83 75	30	O
82 80 75 63	37	7	OOO				82 82 82 79	61	3
79 78 74 62	38	9	OOO				79 79 78 77	70 ■	35
									
86 70 35 5	O	O	OOO				50 4 O O	O	O
86 72 40 7	O	O	OOO				69 15 O O	O	O
88 78 52 15	O	O	OOO				83 48 2 O	O	O
89 82 63 25	2	O	OOO				89 76 27 O	O	O
88 82 66 31	3	O	OOO				90 85 62 7	O	O
87 84 73 46	11	O	OOO				89 87 79 44	1	O
86 84 77 56	22	2	OOO				87 87 84 70	15	O
84 82 77 63	34	5	OOO				85 84 83 78	48	O
									
87 77 50 13	O	O	O	O	O		71 21 O O	O	O
89 82 63 26	2	O	O	O	O		83 52 5 O	O	O
87 80 60 21	1	O	O	O	O		88 75 29 O	O	O
88 85 72 42	8	O	O	O	O		89 83 59 9	O	O
88 85 76 51	14	O	O	O	O		88 86 75 34	O	O
86 84 77 56	20	1	O	O	O		87 85 80 55	5	O
86 84 78 60	25	2	O	O	O		87 86 83 67	16	O
85 83 78 63	30	3	O	O	O		86 85 82 72	32	O
84 83 78 65	34	5	O	O	O		85 84 82 75	45	1
83 82 78 68	43	9	O	O	O		84 83 82 77	56	4
81 80 77 67	44 1	.1	O	O	O		82 82 81 76	59	9
92 90
90 88
87 85
82 80
80 78
75 73
68 66
60 59
82 50
81 54
79 57
76 59
70 48
66 47
59 42
53 39
92 92 91 88 72	22	O	O	■ 91 90 84	66 28 3	O	OOOO
92 92 91 90 81	41	1	O	∣92 91 86	73 39 7	O	OOOO
91 90 90 89 84	59	8	O	∣90 89 86	77 52 16	1	OOOO
88 88 88 87 83	66	16	O	.88 87 85	77 57 24	3	OOOO
83 83 83 82 80	71	33	1	■ 83 82 81 76 65 39		11	IOOO
84 84 84 83 79	66	36	8	.84 83 81 76 66 49		32 18 9 4 1	
							
92 91 89 79 37	1	2	3	■ 90 87 74	42 8 O	O	OOOO
93 93 91 86 55	4	O	1	■ 92 89 80	53 12 O	O	OOOO
93 92 92 89 76	26	O	1	■ 92 90 85 69 30 3		O	OOOO
92 92 91 90 85	58	6	O	∣91 90 87 78 51 14		1	OOOO
90 90 90 89 87	75	25	O	∣90 89 87 82 64 31		5	OOOO
86 86 86 86 85 81		56	6	∣86 86 85 83 76 58		24	3 0 0 0
79 79 80 79 79 77		69 30		.79 79 79 78 76 69		51	23 4 O O
73 73 73 73 72	72	67	54	■ 73 73 73 72 71 68 61			50 37 25 16
69 69 68 68 68 67 64 52				.68 68 68 68 67 64 58 49 40 35 32			
77 77 76 76 75 72 61 26				∣76 76 76 74 71 64 47 27 11 4 2			
							
93 92 91 84 50	4	O	2	■ 91 89 81	55 15 1	O	OOOO
93 93 91 86 62	11	O	1	■ 92 91 87	73 38 7	O	OOOO
93 92 89 76 35	2	O	1	■ 92 90 84 67 29 3		O	OOOO
92 91 86 64 19	1	O	1	■ 91 89 83	69 43 11	O	OOOO
92 91 85 61 20	1	O	O	■ 91 87 77 58 28 6		O	OOOO
92 90 84 61 19	1	O	O	■ 90 87 79	63 38 14	2	OOOO
91 90 84 62 22	2	O	O	∣89 86 77	57 30 10	2	OOOO
89 88 85 78 53	17	2	O	，88 86 82	71 52 28	10	2 0 0 0
47 48 48 49 49 48 45 35				.50 52 54	57 57 56 54 46 30 14 6		
							
91 91 91 90 87 70 17			O	■ 91 90 88 82 63 29		5	OOOO
90 90 90 90 89 84 56			5	■ 90 90 89 87 80 61 30			5 0 0 0
89 89 89 88 88 86 77 37				.89 88 88 87 85 78 63 32 5 O O			
86 86 86 85 85 84 80 63				.86 85 85 85 84 81 73 54 24 4 O			
83 83 83 82 82 82 80 74				.83 83 82 82 82 80 77 69 52 28 11			
80 80 80 79 79 79 77 73				∣80 80 80 79 79 78 76 74 69 62 53			
							
93 93 90 75 25	O	1	1	■ 91 86 71	36 4 O	O	OOOO
93 93 91 84 48	2	O	2	■ 91 88 78	51 11 O	O	OOOO
93 93 92 89 74	22	O	1	■ 92 90 85 67 28 3		O	OOOO
93 93 92 91 87 60		6	O	∣92 91 89 82 60 22		2	OOOO
92 91 91 91 88 80		39	1	∣91 91 90 87 78 54		17	IOOO
90 89 89 89 88 85		72 20		∣90 89 89 88 85 77		55	19 1 O O
88 88 88 88 87 86 81 56				.88 88 88 87 86 82		73	49 15 1 O
85 85 85 85 84 84 81 72				■ 85 85 85 84 83 82 79			69 47 16 3
							
93 93 91 86 58	6		1	.92 90 85 66 26 2		O	OOOO
93 92 91 89 79 35			2	∣92 91 89 81 57 17		1	OOOO
92 92 92 91 86 67			O	■ 92 91 90 87 76 45		9	OOOO
91 91 90 90 88 80			2	■ 91 90 90 88 83 69 37			5 0 0 0
89 89 89 89 88 84			21	.89 89 89 88 86 79 60 22 1 O O			
88 88 88 87 87 85			43	，88 87 87 87 85 82 71 47 12 O O			
88 88 88 87 87 85 80			58	.88 88 87 87 86 83 76 59 27 3 O			
86 86 86 86 85 85 81			66	∣86 86 86 85 84 83 79 69 46 15 2			
85 85 85 85 85 84 81			70	■ 85 85 85	85 84 83	80	73 59 36 13
84 84 84 84 84 83 81			74	.84 84 84 84 83 82 80 76 66 49 26			
82 82 82 82 82 81 79			73	■ 82 82 82 82 82 81 79 75 69 56 38			
							
92 91 87 69 24	2	O	O	.88 79 57	23 3000000		
							
90 89 86 71 32	3	O	O	，86 79 58	26 4000000		
							
87 86 84 74 45	9	O	O	，84 77 61	35 10 1 O O O O O		
							
81 81 80 74 54	20	2	O	.79 74 64	45 20 6 IOOOO		
							
78 78 77 70 50	20	3	O	.77 73 64	47 26 10	3	1000
74 74 73 66 49	21	4	O	∣72 69 61	46 27 11	4	IOOO
68 67 65 59 43	19	4	O	，65 62 54	40 24 11	4	IOOO
60 59 58 53 39	17	3	O	∣58 55 49	37 21 8	2	IOOO
67 11	O (
78 26	1 (
83 50	3 (
83 66	12 (
80 72	34 ■
80 66	22 :
86 60 5 O
87 78 33 O
85 81 62 8
81 79 72 40
79 76 71 58
74 72 68 58
68 67 63 53
61 60 58 49
37 19 11 7
45 33 23 10
42 34 28 15
39 29 23 18
AdVerSaria- accuracy
UlIderreVieW as a COnferenCe PaPer at ICLR 2020
Vk Vk IVM S S
Figure 14: Accuracy of adversarial attack (column) against adVersarially trained model (row) on CIFAR-10.
Under review as a conference paper at ICLR 2020
Normal Training -
17 16 48 5 25 3
Normal Training -
17 16 48 5 25 3
Normal
Training -
17 16 48 5 25 3
L∞ ε = 1
L∞ ε = 2
L∞ ε = 4
L∞ ε = 8
L∞ ε = 16
L∞ ε = 32
51	49	69	31	37	21
63	59	73	38	39	28
74	67	76	47	40	36
83	72	77	50	39	43
89	75	78	55	40	51
94	72	76	58	48	45
L? ε = 40 -
L2 ε = 80 -
L2 ε = 160 -
L2 ε = 320 -
L2 ε = 640 -
L2 ε = 2560 -
53 53 74 32 38 22
64 63 80 44 40 30
73 73 84 54 41 38
80 81 88 64 46 45
84 86 88 70 51 52
87 85 86 78 71 66
Li ε = 195 -
Li ε = 390 -
Li ε = 780 -
Li ε = 1560 -
Li ε = 6240 -
Li ε = 49920 -
36 38 70 19 34 12
40 41 79 24 39 13
26 26 79 15 37 9
18 15 80 10 37 7
17 13 77 10 36 10
49 47 61 47 51 29
L∞-JPEG ε = 0.03125 -
L∞-JPEG ε = 0.0625 -
L∞-JPEG ε = 0.125 -
L∞-JPEG ε = 0.25 -
L∞-JPEG ε = 0.5 -
L∞-JPEG ε = 1 -
LI-JPEG ε = 2 -
LI-JPEG ε = 8 -
LI-JPEG ε = 64 -
LI-JPEG ε = 256 -
LI-JPEG ε = 512 -
LI-JPEG ε = 1024-
Elastic ε = 0.125 -
Elastic ε = 0.25 -
Elastic ε = 0.5 -
Elastic ε = 1 -
Elastic ε = 2 -
Elastic ε = 8 -
40 37 63 19 24 41
41 38 65 23 25 53
43 40 65 28 27 64
47 41 57 33 29 75
49 35 51 32 29 89
45 31 37 27 25 86
Figure 15:	UAR scores on CIFAR-10. Displayed UAR scores are multiplied by 100 for clarity.
(M≡u一et--.Jes,Jgpe) *0s5
L∞ ε = 1
L∞ ε = 2
L∞ ε = 4
L∞ ε = 8
L∞ ε =16
L∞ ε = 32
L2 ε = 150
L2 ε = 300
L2 ε = 600
L2 ε = 1200
L2 ε = 2400
L2 ε = 4800
Li ε =
Li ε
Li ε =
Li ε
Li ε = 153000
Li ε = 306000
Li ε = 612000
184 70
13 0 0
85 81 50 2 0
83 82
74 23
80 79 77 59
74 74 73 67
6
34
185 81 47
85 83 71
84 83 78
18
47
80 78 73
74 72 64
46
35
180 66
81 72
80 74
18
29
72 62
61 48
23
10
84∣13
84 50
83 67
79 54
73 41
0
0
0
0
0
!71∣
71 70 69 63
40
8
69 62 35
6
0
0
40 22
2
0
0
0
65 9
0
0
0
82 54 3 0
84 74 21
0
77 76 71
56
22
1
69 69 66 61
45 14
170 22
0
0
0
0
∣84 77 13 1 0
0
6
2
9
5
4
4
77 76 74
69 68 67
77 43
80 60
81 70
80 75
77 72
73 68
29
47
51
52
3
10
56
61
10
15
1
4
0
0
6
27
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
84 74 33
84 79 54
83 80 66
81 79 72
78 76 70
73 72 67
77 76 73 48
69 69 67 60
21
40
45
46
1
5
220
0
0
0
0
81 68 5
82 76 19
83 79 44
81 80 65
78 78 75 67 40 4
73 73 71 66 47 12
8
32
0
4
0
0
0
0
0
0
0
1
38250.1
=76500
9562.44
=19125
) 79 33
) 82 65
I 83 78
? 81 79
84 80 56
82 80 73
ɑ § § § § §
84 81 33 3 0
84 83 53 12 0
83 83 71 41 5
81 81 76 63 24
0 0
8 0
3
28
41 1
68 15
0 0
1 0
、，V <⅛ ʌ
,.⅛> <V N <b ⅛
XG .9
§ <r <r <r∕</ 为
Attack (evaluation)
84 1
84 3
84 17
83 46
81 64
3
20
78 67
73 63
35
45
Adversarial accuracy
6 4
■ ■

4
4

0
2 0
0
6
0
0
0
0
0
0
0
0
0
1
0
2
5
3
0
0
0
0
1
5
6
2
0
0
0
0
0
0
0
0
序令令令
殳9五联总
1
7
0
0
8
4
0
0
0
0
0
0
0
0
0
0
0
0
2
8
0
0
0
0
0
0
0
0
0
0
0
0
Figure 16:	Replica of the first three block rows of Figure 11 with different random seeds. Deviations
in results are minor.
20
Normal training -
∣643710 1

LeE-T -		βffl70150	O	O
L^ε-2 - LeS.4 _ L^ε-B - i--ε-16-		■B5 BO 51 3 O .H4H2 74 24 O ,79 797659 7 ,74 7473 67 35		O O O 1
L-ε-32 -	71	■71 7069 634314∣		
05 02 50 3 O	O
B5Q371 21 O	O
04 B379 50 3	O
79 7β73 52 9	O
73 7164 32 4	O
68 60 35 7 O	O
∣81704310 1 BI 74 57 24 3 01 7664 39 9 73 65 53 31 9 59 462710 2	O O O O 1 O 1 O O O
∣39 24 9 3 0	OO
L3 ε-150
½ £-300
L2 ε-600
⅛ ε-1200
tjε≡ 2400
/.2ε-4B00
Ll ε-9562.44
I1 ε-19125
Li ε-3θ250.1
I1 £-76500
Lie-153000
lie-306000
i1ε-612000
U-JPEG ε-0.03125
U-JPEG £-0.0625
L.-JPEG ε-0.125
J-JPEGE=0.25
La-JPEGe-0.5
Le-JPEGc-I
Lw-JPEGe-Z
L2-JPEG £-2
L2-JPEG C-4
L2-JPEGe-B
L2-JPEG C-16
L2-JPEG ε≡ 32
L2-JPEGe-64
L2-JPEG E-12θ
L2-JPEGe-256
07
87
86
05
84
Bl
77
82 53 4 O O O
^≡23 OOO
04 Bl 50 4 O O
02 Bl 74 29 O O
77 7673 57 7 O
68 68 67 6128 1
B2 65 16 O	O	O
04 75 34 1	O	O
84 电 57 7	O	O
03 Bl 69 23	O	O
BO 79 70 39	2	O
7B 77 72 49	B	O
72 7168 57 22		1
B3 79 6121 1	O
84 817241 5	O
05 03 70 5010	1
04 03 017343	5
Bl BO 79 77 64 24	
79 79 7B 76 7149	
Li-JPEG E-12θ
Li-JPEG £-256
Li-JPEG ε-512
Li-JPEG £-1024
Li-JPEG ε-204B
Li-JPEG ε-4096
Li-JPEG ε-θl92
Li-JPEG ε- 163B4
Li-JPEG £-3276θ
Li-JPEG ε-65536
Li-JPEG E -131072
		
S3 7429 1 O B5 BO 50 4 O B4 Bl 67 14 O 03 B2 75 35 1 00 0075 49 3 79 79 76 5β 9		I
		
75 74 73 6015 ∣73 72 69 5411		
Elastice-0.25 -E
Elastic E-0.5 - ∏
Elastic f≡l - 5
Elastic t≡2 - ∏
Elastic f≡4 - J
Elastic^"B - J
Elastic C" 16-]
Fogε-12B - 1
Fog ε-256-1
Fog ε-512 -，
Fog ε ≡ 1024 - I
Fog ε"204B - J
Foge-4096-≡
FOg 38192 - j
Fogε-163β4- j
Fog 332768- 2
Foge-65536-G
Gabor £-6.25
Gabor ε-12.5
Gabor ε≡ 25
Gabor ε≡ 50
Gabor ε-100
Gabor C" 200
Gabor C" 400
Gabore∙800
Gabor ε≡ 1600
Gabor ε≡ 3200
Snow £-0.03125
Snowε> 0.0625
Snow ε ≡ 0.125
Snow ε≡ 0.25
Snow ε-0.5
Snow c≡l
Snow £ - 2
Snow f≡4
Sπowε≡B
Snow C" 16
6 9 0 0 9 1
IZl7 63 1
2 1 2 7 0 5
7 ≡ 0 7 6 4
4 4 3 0 3 5
8 8 8 8 7 6

05	01	51	I 3	O	O	O O O O O O O O O O 2 O
85	04	73	20	O	O	
04	04	BO	52	3	O	
82	02	Bl	74	2B	O	
77	77	76	74	58	B	
6B	6B	6B	67	63	36	
77 79 79 74 64 59	64 71 74 70 58 51	36 52 62 59 46 41		1 4 11 16 15 15	O O 1 4 5 7	O O O 1 1 2	O O O O O O	OOO OOO OOO OOO OOO 0 0。
■	■	■		■	■	■	■	
75 79 Bl Bl 77	55 69 76 BO 76	23 42 61 74 74		O 1 6 20 56	O O O 6 29	O O O O B	O O O O 1	OOO OOO OOO OOO OOO

			
			
			
			
			
2 51B46521
6 15 9 4
1 4 6 6 6
2 7 8 3 9
6 7 7 7 6
1 2 9 4 0
0 0 7 7 7
1 2 4112121
7 4 3 81411
12 3 4 4
3 7 9 5 1 6
3 4 5 6 6 5
7 3 6 3 6 1
6 7 7 7 6 6
O O 9 5 θ 3
8 8 7 7 6 6

Hββl9 1 O
卜27548 5 O
∣79 7663 20 ɪ
∣77 75 69 47 4
∣71 70 66 5418
8472 25 IOO
8479 49 5 0 0
04 02 69 23 1 O
04 02 77 51 5 O
00 79 77 64 20 1
70 77 75 6945 4
72 71 70 67 5617
74 5320 3 0 0 0 0 0 0
B0 6B4010 IOOOOO
02 77 59 26 5 O O O O O
B3 Bl 74 5319 3 0 0 0 0
B0 797B 704413 IOOO
78 7877 746630 9 1 0 0
71 7170 69 65 5025 7 1 1
84 7	2 32	2	O	O O	■	7 561	92000000		65 21 4 1 O	OOOOO		7B6124 4 O I
84 7	645	5	O	O O	■	655 2	02000000		7845 BlO	OOOOO		79 69 35 7 1 I
03 7	9 5B	12 O		O O		5 52 1	62000000		βl6720 2 O	OOOOO		79 73 5014 2 I
βl7	β67	24 2		O O	■	9451	21011101		Bl7645 6 O	OlOOO		7672 6026 5 ：
76 7	4 6B	39	5	1 O	■	134 I	9 110 0 10 1		76746119 1	OllOO		69 60 613B12 ：
68 6	7 63	49	12	10		127 I		I	68 67 6131 3	IiIii	I	60 59 554219 ι
82 67 22 1 O	O O
83 7132 2 O	O O
83 7543 4 O	O O
B2 70 53 B O	O O
79 75 50 14 O	O O
77 7462 23 1	O O
70 68 6135 4	OO
■3B 7 1
78 5516 2
7β6327 4
75 67 39 B
69 624314
≡15 3 1 0 0 0 0
7133 6 1 O O O O
70 5618 3 0 0 0 0
79 72 41 BlOOO
7β 73 50 16 3 1 1 O
77 7149 IB 4 111
		
B 6 Bl 51 06Q475	I 2 O 19 O	
		
		
		
		
		
∣βl 6114 62 66 IB		IOOO IOOO	
Θ3 70 B3 74	23 1 O O O 33 2 O O O		
B2 7544 4 BO 765610 77 7462 21 77 75 64 22			OOO IOO 2 0 0 IOO
			
6516 1	OOOO
1136 2	OOOO
竭 5711	OOOO
79 5014	OOOO
70 39 5	OOOO
5516 1	OOOO
30 4 O	OOoO
06 B5 7943 2 0 0	O
06 B5 03 63 BOO	O
06 05 04 76 31 O O	O
B6B5B5 02 59 4 O	O
05 05 0402 74 25 O	O
03 03 03 02 79 52 1	O
Bl Bl BO BO 7β 65 9	O
BO 79 79 79 78 7125	O
7B7β 70 77 76 7132	O
7575 757574 6940	1
7373 7373716636	1
B6B476 561β 2 O	O O	O O
B5B4B16Θ 35 5 O	O O	O O
05 05 03 75 5417 1	O O	O O
06 05 04 00 70 40 7	1 O	O O
04 04 03 02 76612θ	5 1	O O
B3B3B2B2 79 72 5322 4		1 O
Bl BO BO BO 79 76 6B49 22		6 3
BO 79 79 79 7B 77 73 64 4423 IB		
7B 77 77 77 77 75 73 67 5436 28		
75 75 75 75 74 74 72 68 6150 45		
73 73 73 7372 7169 65 5746 42		
Bl 6012 1
02 63 16 O
82 68 22 1
82 73 32 2
02 75 41 4
Bl 7652 B
79 75 5511
7B 75 59 14
77 746017
74 7158 17
72 69 54 14
,044	10 2000000
,349	13 2000000
,453	16 2000000
,662	23 4100000
,7 66	34 6100000
,6 69	4310 1 O O O O O
,570	5016 3 1 0 0 0 1
S 69 R 68 17 64 14 60	52 21 4 1 1 O 1 1 5325 6 2 1 1 0 1 52 24 7 2 1 1 1 1 47 21 5 1 1 O 1 1
AdVerSaria- accuracy
UlIderreVieW as a COnferenCe PaPer at ICLR 2020
■5315 1
77 57 22 2
7658 24 3
75 59 25 4
72 53 22 3
67 49 21 4
6444 19 4
TF		I		05	Ta	交	五	1	O	O	O	O	O
正				07	02	71	40	6	O	O	O	1	O
				86	04	BO	66	32	6		1	1	1
02	64			86	85	02	76	60			2	1	1
Bl	66			04	04	02	7B	70			12	3	1
74	60			79	79	7B	76	73			38	12	3
	i			70	70	70	70	69			61	45	24
				57	58	59	58	59			53	43	24
				51	51	52	51	51			47	40	29
03 73 37 3
02 7441 4
02 7441 4
02 7343 5
BO 7341 5
BO 72 42 7
79 7144 B
7θ 714511
76 704713
74 68 4816

5620 51000000
67 3βl0 3 1	0	0	0	0	0
75 6025 7 1	0	1	0	0	0
74 60 4414 3	1	1	1	0	0
70 613914 4	2	2	1	1	0
70 60 59 35 9	3	3	2	2	1
65 62 57 4110 8 8 5 3 2
65 65 63 50 42 25 23 20 1611
61 6159 55 46 33 3128 2417
Figure 17: Replica of Figure 11 with 50 steps instead of 200 at evaluation time. Deviations in results are minor.
Under review as a conference paper at ICLR 2020
L∞ attacking jointly trained (L∞,L2)
L∞ attacking jointly trained (L∞,Lι)
L∞ attacking jointly trained (L∞, Elastic)
L2 ε = 300
84
81
73
60
Li ε = 38250.1
82
75	71
51
Elastic ε = 0.5
83
81
73
58
L2 ε = 600
84
81
73
59
Li ε = 76500
81
69
32
55
Elastic ε = 1
81
78
72
58
L2 ε
1200
82
80
73
58
Li
ε = 153000
80
72
34
2
Elastic ε = 2
79
75
69
55
L2 ε = 2400
76
76
74
57
Li
ε = 306000
75
66
25
1
Elastic ε = 4
77
74
55
0
L2 ε = 4800 -
29
Li
ε = 612000 -
L∞ L∞ L∞ L∞ L∞
ε = 1 ε = 2 ε = 4 ε = 8ε = 16
L2 attacking jointly trained (L∞,L2)
Elastic ε = 8 -
L2 ε = 300
83	83	83	79
L2 ε = 600
78	79	79	76
L2 ε = 1200
68	69	68	69
L2 ε = 2400
49	49	48	48
L2 ε = 4800 -
19
l∞ l∞ l∞ l∞ l∞
ε = 1 ε = 2 ε = 4 ε = 8ε =16
Illl
l∞ l∞ l∞ l∞
ε = 1 ε = 2 ε = 4 ε = 8
Li attacking jointly trained (L∞,LJ
Li ε = 38250.1
77	76	70
60
Li ε = 76500
69	68	71
33
Li ε = 153000
Li ε = 306000
Li ε = 612000 -
48	56	50	53
19	17	8
4
Illl
l∞	l∞	l∞	l∞
ε = 1 ε = 2 ε = 4 ε =
l∞ l∞ l∞ l∞ l∞
ε = 1 ε = 2 ε = 4 ε = 8 ε = 16
Elastic attacking jointly trained (L∞, Elastic)
Figure 18: Evaluation accuracies of jointly trained models. Attack and training ε values are equal.
L∞ ε = 4, Li ε:
L∞ ε = 8, Li ε --
；=
；=
■=
■=
82 75 55
83
70
81 79 75
76 76 74 69
L∞ ε = 4, L2 ε ≡
L∞ ε = 8, L2 ε ≡
：1.01642
：2.03284
1200
2400
300
600
1
2
17
44
64
1
8
33
55
0
0
4
21
0
0
0
1
/ L2 ε:
i 2, L2 ε ≡
:=4, Elastic ε =
L∞ ε = 16, Elastic ε = 8
L∞ ε = 16, L2 ε = 4800
L∞ ε = 1, Elastic ε = 0.5
L∞ ε =16, Li ε = 4.06569
L∞ ε = 2, Elastic ε =
L∞ ε = 1, Li ε = 0.254106
L∞ ε =2, Li ε = 0.508211
;=8, Elastic ε = 4
■s 段.⅛
Figure 19: All attacks (columns) vs. jointly adversarially trained defense (rows).
69 68 68 66 60
42 12
48 42 27
12 3
79
80
78
61
68
72
71
46
41
55
56
22
9
22
30
5
1
2
7
1
0
0
0
0
0
0
0
0
0
0




Train, elastic ε = 4
Train, L∞ ε = 8
va∙^ Val, elastic ε = 4
Val, L∞ ε = 8
Figure 20: Train and validation curves for joint training against L∞, ε = 4 and elastic, ε = 8
using ResNet-101. As shown, the validation accuracies decrease as training progresses, indicating
overfitting.
22
Under review as a conference paper at ICLR 2020
Normal training ∣87 46
26 25 19 43 42 48 55 37 39 43 74 36 67 67 68 36 47 56 65
Normal training -87 46
26 25 19 43 42 48 55 37 39 43 74 36 67 67 68 36 47 56 65
L∞ ε = 1
L∞ ε = 2
L∞ ε = 4
L∞ ε = 8
L∞ ε = 16
L∞ ε = 32
L2 ε = 150
L2 ε = 300
L2 ε = 600
L2 ε = 1200
L2 ε = 2400
L2 ε = 4800
Li ε = 9562.44
Li ε = 19125
Li ε = 76500
Li ε = 153000
Li ε = 306000
Li ε = 612000
86	48	30	29	23	41	48	50	55	39	43	31	74	27	69	75	78	41	46	59	65
85	47	33	31	24	41	50	50	54	37	41	22	72	22	69	76	79	42	45	58	63
84	47	38	36	31	40	50	50	54	39	40	15	70	17	68	75	79	46	44	59	60
80	45	41	39	34	37	48	47	52	38	35	8	64	12	67	72	75	48	41	58	56
75	40	39	36	29	33	43	43	46	38	27	6	54	9	62	65	70	43	37	55	50
71 ■	39	32	30	21	36	46	45	48	34	25	9	50 ■	13	61 ■	65 ■	68	37 ■	39	53	47
87	. 48	. 31	. 30	. 24	. 43	. 48	. 49	. 55	. 38	. 42	. 33	74	. 29	69	75	. 77	42	. 47	. 58	. 65
85	48	34	33	27	44 50		50	56	37	42	27	73	26	69	76	79	44	48	59	63
84	48	35	33	29	42	53	52	56	38	41	20	71	22	69	77	79	43	46	59	61
82	47	40	38	34	42	52	51	55	37	38	12	67	17	69	75	78	48	45	60	58
77	44	42	41	38	40	50	49	52	36	31	8	60	12	65	70	73	50	43	58	52
68 ■	40 ■	43	41 ■	40 ■	38 ■	46	45 ■	47 ■	31 ■	23 ■	6	49 ■	9	59 ■	62 ■	66	49 ■	41 ■	53 ■	44
		. 33	32	30	45	46	50	55	36	42	. 36	74	. 33	68	74	. 74	43	48	59	. 65
86	49	37	35	34	45	50	50	56	35	40	34	73	32	68	75	74	45	49	60	64
84	48	36	35	32	43	52	50	55	34	38	24	70	26	67	77	75	46	46	59	60
81	45	38	36	35	40	49	49	53	31	35	21	66	24	64	73	70	46	44	58	56
79	40	39	37	37	31	39	42	44	28	29	15	60	18	58	67	62	47	34	54 51	
72	36	37	36	35	32	37	37	39	21	21	7	53	14	54	61	61	45	35	51	
L∞-JPEG ε = 0.03125 - 87 48
L∞-JPEG ε = 0.125 -86 51
L∞-JPEG ε = 0.25∣ 84 51
L∞-JPEG ε = 0.5∣ 81 51
L∞-JPEG ε = 1 -79 52
L∞-JPEG ε = 2 - 78 51
L2-JPEG ε = 2
L2-JPEG ε = 16
L2-JPEG ε = 32
L2-JPEG ε = 64
L2-JPEG ε = 128
L2-JPEG ε = 256
Elastic ε = 0.25, 87
Elastic ε = 1-85
Elastic ε = 2
Elastic ε = 4
Elastic ε = 8
Elastic ε = 16
14
Figure 21: Accuracies of defenses (rows) on ImageNet-C-100 corruptions (columns).
34 33	28	43	45	48	54	37	41	37	75	32	68	74	78	46	47	57	66
45 44	41	42	48	49	54	37	41	28	73	27	68	78	82	55	47	58	65
52 50	49	41	50	49	55	36	39	23	70	24	68	78	82	61	46	57	64
58 55	56	40	52	50	54	36	37	17	65	20	68	76	80	63	45	58	60
63 61	62	39	53	50	55	36	34	17	60	21	68	75	78	67	43	59	59
58 59	57	40	53	50	55	37	35	15	62	21	68	75	78	66	44	59	58
32 31	26	43	44	48	55	38	41	40	75	34	68	70
45 44	42	43	48	49	54	36	41	28	72	28	68	77
49 48	46	42	50	49	54	35	38	22	70	24	67	77
53 51	51	40	51	49	53	35	35	15	64	19	67	75
55 52	53	39	52	49	54	34	30	12	59	17	67	73
56 54	54	39	51	50	54	35	31	12	59	18	67	73
75 43
81 55
81 58
80 60
76 61
77 62
47 57 65
48 59 66
46 58 65
44 58 62
43 56 58
43 57 58
27 26	20	46	47	50	57	38	42	42	75	36	72	75	71	37	49	59	65
25 24	18	47	55	53	60	39	41	34	71	31	76	78	71	35	50	60	60
21 20	14	47	57	55	61	37	38	30	69	28	76	77	69	30	49	60	57
15 14	9	44	56	54	59	36	35	24	64	23	73	74	66	22	47	58	51
13 12	7	42	54	54	58	33	30	20	58	20	70	71	60	18	44	57	44
12 10	5	39	50	50	53	32	27	16	49	15	66	65	54	16	41	55	37
,4
Original	L∞ (2, 1)	L2 (600, 0)	L1 (38250, 5)	Elastic (1, 6)
JPEG (0.0625, 1)	Fog (512, 1)	Gabor (12.5, 3)	Snow (0.125, 7)
Figure 22: Adversarial attacks at low distortion sizes ε against an undefended model. The pair of
(ε, accuracy) is shown after the attack name. Visual differences between the original and attacked
images are imperceptible for L∞, L2, and JPEG, minor for L1, Elastic, and Gabor, and weather-
related for Fog and Snow.
23