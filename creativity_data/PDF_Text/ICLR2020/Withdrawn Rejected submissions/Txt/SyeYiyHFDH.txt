Under review as a conference paper at ICLR 2020
Convergence Analysis of a Momentum
Algorithm with Adaptive Step Size
for Nonconvex Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Although Adam is a very popular algorithm for optimizing the weights of neural
networks, it has been recently shown that it can diverge even in simple convex op-
timization examples. Several variants of Adam have been proposed to circumvent
this convergence issue. In this work, we study the Adam algorithm for smooth
nonconvex optimization under a boundedness assumption on the adaptive learning
rate. The bound on the adaptive step size depends on the Lipschitz constant of the
gradient of the objective function and provides safe theoretical adaptive step sizes.
Under this boundedness assumption, we show a novel first order convergence rate
result in both deterministic and stochastic contexts. Furthermore, we establish
convergence rates of the function value sequence using the KUrdyka-LojasieWicz
property.
1	Introduction
Consider the unconstrained optimization problem
xm∈iRnd f (x),
(1)
Where f : Rd → R is a differentiable map and d is an integer. Gradient descent is one of the
most classical algorithms to solve this problem. Since the seminal Work Robbins and Monro (1951),
its stochastic counterpart became one of the most popular algorithms to solve machine learning
problems (see Bottou et al. (2018) for a recent survey). Recently, a class of algorithms called adap-
tive algorithms Which are variants of stochastic gradient descent became very popular in machine
learning applications. Using a coordinate-Wise step size computed using past gradient information,
the step size is adapted to the function to optimize and does not folloW a predetermined step size
schedule. Among these adaptive algorithms, Adam (Kingma and Ba, 2015) is very popular for op-
timizing the Weights of neural netWorks. HoWever, recently, Reddi et al. (2018) exhibited a simple
convex stochastic optimization problem over a compact set Where Adam fails to converge because
of its short-term gradient memory. Moreover, they proposed an algorithm called Amsgrad to fix
the convergence issue of Adam . This Work opened the Way to the emergence of other variants
of Adam to overcome its convergence issues (see Section 3 for a detailed revieW). In this Work,
under a bounded step size assumption, We propose a theoretical analysis of Adam for nonconvex
optimization.
Contributions.
•	We establish a convergence rate for ADAM in the deterministic case for nonconvex optimization
under a bounded step size. This algorithm can be seen as a deterministic clipped version of Adam
Which guarantees safe theoretical step sizes. More precisely, We shoW a O(1/n) convergence rate
by introducing a suitable Lyapunov function.
•	We shoW a similar convergence result to Zaheer et al. (2018, Thm. 1) for nonconvex stochastic
optimization up to the limit of the variance of stochastic gradients under an almost surely bounded
step size. In comparison to the literature, We relax the hypothesis of the boundedness of the
gradients. We also improve the dependency of the convergence result on the dimension d of the
parameters.
1
Under review as a conference paper at ICLR 2020
•	We propose a convergence rate analysis of the objective function of the algorithm using the
KUrdyka-LojasieWicz (KE) property. To the best of our knowledge, this is the first time such
a result is established for an adaptive optimization algorithm.
2	A Momentum Algorithm with Adaptive Step Size
Notations. All operations between vectors of Rd are to read coordinatewise. The vector of ones of
Rd is denoted by 1. When a scalar is added to a vector, it is added to each one of its coordinates.
Inequalities are also to be read coordinatewise. If x ∈ Rd , x ≤ λ ∈ R means that each coordinate
of x is smaller than λ.
We investigate the following algorithm defined by two sequences (xn) and (pn) in Rd:
xn+1 = xn - an+1pn+1
[Pn+1 = Pn + b (▽/(Xn)- Pn)
(2)
where Vf (x) is the gradient of f at point x, (an) is a sequence of vectors in Rd with positive
coordinates, b is a positive real constant and x0 , p0 ∈ Rd .
Algorithm (2) includes the classical Heavy-ball method as a special case, but is much more general.
Indeed, we allow the sequence of step sizes (an) to be adaptive : an ∈ Rd may depend on the past
gradients gk := Vf(xk) and the iterates xk for k ≤ n. We stress that the step size an is a vector of
Rd and that the product an+1pn+1 in (2) is read componentwise (this is equivalent to the formulation
with a diagonal matrix preconditioner applied to the gradient (McMahan and Streeter, 2010; Gupta
et al., 2017; Agarwal et al., 2019)). We present in the following table how to recover some of the
famous algorithms with a vector step size formulation:
Algorithm	Effective step size an+ι			Momentum
SGD (Robbins andMonro, 1985)	an+1 ≡ a			b=1 (no momentum)
ADAGRAD (Duchi etal., 2011)	an+1= a (Pn=0 g2)-1∕2			b = 1
Rmsprop (Tieleman and Hinton, 2012)	an+1 = a	he + (C Pn=o(1 - c)n-ig2)1/2	-1	b = 1
ADAM (Kingma and Ba, 2015)	an+1 = a	he + (c Pn=o(1 - c)n-ig2)1/2	-1	0 ≤ b ≤ 1 (close to 0)
In particular, adam (Kingma and Ba, 2015) defined by the iterates :
J xn+1 = xn - e+√aVn+lpn+1
pn+1 = pn + b (Vf (xn ) - pn )	(3)
[vn+1 = vn + C (Vf(Xn) 2 - Vn)
for constants a ∈ R+, b, c ∈ [0, 1], can be seen as an instance of this algorithm by setting an =
,az- where the vector Vn, as defined above, is an exponential moving average of the gradient
+ vn
squared. For simplification, we omit bias correction steps for pn+1 and Vn+1.
We introduce the main assumption on the objective function which is standard in gradient-based
algorithms analysis.
Assumption 2.1. The mapping f : Rd → R satisfies the following
(i)	f is continuously differentiable and its gradient Vf is L-Lipschitz continuous,
(ii)	f is bounded from below, i.e., inf x∈Rd f(x) > -∞ .
3	Related Works
3.1	The Heavy-Ball Algorithm.
Adaptive algorithms as Heavy Ball. Thanks to its small per-iteration cost and its acceleration
properties (at least in the strongly convex case), the Heavy-ball method, also called gradient descent
2
Under review as a conference paper at ICLR 2020
with momentum, recently regained popularity in large-scale optimization (Sutskever et al., 2013).
This speeding up idea dates back to the sixties with the seminal work of Polyak (1964). In order to
tackle nonconvex optimization problems, Ochs et al. (2014) proposed iPiano, a generalization of the
well known heavy-ball in the form of a forward-backward splitting algorithm with an inertial force
for the sum of a smooth possibly nonconvex and a convex function. In the particular case of the
Heavy-ball method, this algorithm writes for two sequences of reals (αn) and (βn):
xn+1 = Xn - αnNf (xn) + Bn(xn - xn-1 ) .	(4)
We remark that Algorithm (2) can be written in a similar fashion by choosing step sizes αn = ban+1
and inertial parameters βn = (1 - b)an+1/an. Ochs et al. (2014) only consider the case where αn
and βn are real-valued. Moreover, the latter does not consider adaptive step sizes, i.e step sizes
depending on past gradient information. We can show some improvement with respect to Ochs
et al. (2014) with weaker convergence conditions in terms of the step size of the algorithm (see
Appendix B.5) while allowing adaptive vector-valued step sizes an (see Proposition A.1).
It is shown in Ochs et al. (2014) that the sequence of function values converges and that every limit
point is a critical point of the objective function. Moreover, supposing that the Lyapunov function
has the KE property at a cluster point, they show the finite length of the sequence of iterates and
its global convergence to a critical point of the objective function. Similar results are shown in Wu
and Li (2019) for a more general version than iPiano (Ochs et al., 2014) computing gradients at an
extrapolated iterate like in Nesterov’s acceleration.
Convergence rate. Ochs et al. (2014) determines a O(1/n) convergence rate (where n is the num-
ber of iterations of the algorithm) with respect to the proximal residual which boils down to the
gradient for noncomposite optimization. Furthermore, a recent work introduces a generalization of
the Heavy-ball method (and Nesterov’s acceleration) to constrained convex optimization in Banach
spaces and provides a non-asymptotic hamiltonian based analysis with O(1/n) convergence rate
(Diakonikolas and Jordan, 2019). In the same vein, in Section 4, we establish a similar conver-
gence result for an adaptive step size instead of a fixed predetermined step size schedule like in the
Heavy-ball algorithm (see Theorem 4.2).
Convergence rates under the Kurdyka-Lojasiewicz property. The KE property is a powerful
tool to analyze gradient-like methods. We elaborate on this property in Section 5. It is for example
possible to derive convergence rates assuming that the objective function satisfies this geometric
property. Indeed, some recent progress has been made to study convergence rates of the Heavy-ball
algorithm in the nonconvex setting. Ochs (2018) establishes local convergence rates for the iterates
and the function values sequences under the KE property. The convergence proof follows a gen-
eral method that is often used in non-convex optimization convergence theory. This framework was
used for gradient descent (Absil et al., 2005), for proximal gradient descent (see Attouch and Bolte
(2009) for an analysis with the Eojasiewicz inequality) and further generalized to a class of descent
methods called gradient-like descent algorithms (Attouch et al., 2013)(see also for ex. Bolte et al.
(2018, Appendix)). KE-based asymptotic convergence rates were established for constant Heavy-
ball parameters (Ochs, 2018). Asymptotic convergence rates based on the KE property were also
shown (Johnstone and Moulin, 2017) for a general algorithm solving nonconvex nonsmooth opti-
mization problems called Multi-step Inertial Forward-Backward splitting (Liang et al., 2016) which
has iPiano and Heavy-ball methods as special cases. In this work, step sizes and momentum pa-
rameter vary along the algorithm run and are not supposed constant. However, specific values are
chosen and consequently, their analysis does not encompass adaptive step sizes i.e. stepsizes that
can possibly depend on past gradient information. In the present work, we establish similar conver-
gence rates for methods such as adam under a bounded step size assumption (see Theorem 5.3).
We also mention Li et al. (2017) which analyzes the accelerated proximal gradient method for non-
convex programming (APGnc) and establishes convergence rates of the function value sequence by
exploiting the KE property. This algorithm is a descent method i.e. the function value sequence is
shown to decrease over time. In the present work, we analyze adaptive algorithms which are not
descent methods. Note that even Heavy-ball is not a descent method. Hence, our analysis requires
additional treatments to exploit the KE property : we introduce a suitable Lyapunov function which
is not the objective function.
3.2	Variants of Adam
3
4
Theoretical guarantees of variants of Adam . We list most of the existing variants of the Adam algorithms together with their theoretical convergence guarantees
in the following table. The gradient is supposed L-Iipschitz continuous in all the convergence results, g↑,-τ,i = [θι①。2①…)9τ,i]r∙
Algorithm	Effective step size an+ι					ɑn		Assumptions	Convergence Result
AMSGRAD ⑴,Adamnc ⑶ (Reddi et al., 2018)	a。 1 ⑴右九+1 = max(vn, (1 - cn)vn + CrIgV) (2)。九+1 = (1 — cn)vn + cn	1 - b1λn-1 or 1 - ^ɪ- n	cn ≡ cl ɪ (for AdamNC)	•	convex functions •	bounded gradients •	bounded feasible set ^ 1 /2 / i •	vT i — ɑ (AmsGrad) •	∑f=ι Hgld2 ≤ •	6ι < ʌ/eɪ(AdamNC)	Rτ∕T = O(^T∕T) ⅜ = O(1∕√T) (AdamNC)
Adam (De et al., 2018)	4∣∣g-∣∣2?7	1 3L(l-(l-fe)n-)2(η+2σ)2 e÷λMΓ ‰+l = (1 - c1)υn +	bn ≡ bl —1 —	. 一	— 2(7	cn ≡ Cl	• σ-bounded gradients • e = 2σ	∀τ? > 0 3n ≤ "bl"#-∕d)) 	s.t. IEll ≤ n	
Padam,AmsGrad (Zhou et al., 2018)	1 1 ɪ Vn % 卡土(AMSGRAD) 6n = max(‰-ι, (1 - c)vn-1 + Cg幻	bn≡b	cn = c	•	bounded gradients For Padam: ∙ p ∈ [0, ɪ] •	l-6< (1- "P •	∑f=ι Mi：NMI2 ≤ VdN AmsGrad: p = ɪ and 1 — δ < 1 — c	E[∣k∣∣2] = θ(ɪ±f+ =θ(ʌ/ɪ +告)(AMSGRAD) τ uniform r.v in (1, ∙ ∙ ∙ , N}
RMSPROP ⑴,yogi ⑶ (Zaheer et al., 2018)	αι (1)①八十1 = (1 - c)vn + cg^i (2)Vn =。九—-CSign(。九—-g^)	bn≡b	cn = c	•	G-bounded gradients •	Ql ≤	(Yogi) •	ɑɪ ≤ ɪ ∙ C ≤ ɪfɑɪ •	/-bounded variance	E[∣l^l∣2]=O(⅛+σ2) τ uniform r.v in (1, ∙ ∙ ∙ , N} 0(含)if minibatch Θ(N)
AMSGRAD ⑴，AdaFom(2) (Chen et al., 2019)	1 1 ⑴-Ori.+1 = max(-0ri,, (1 - CnWrI + CrIgV) 	⑵6-+I = (I _ 1)6_ + 袅))		non-increasing	cn ≡ Cl	• bounded gradients • 3c > 0 s.t. ∣pιji∣ ≥ c	M/I对2] = 0(»)
Generic Adam (Zou et al., 2019)	7Pn ‰+l = (1 — ɛn)^n +	9 n =√ι-(i-er	bn > b > 0	0 < Cn < 1 non-increasing Iim cn = c > b2	•	bounded gradients in expectation •	⅛1, ≤ -^= ≤ Cq dn dn non-increasing	E[∣∣gτ∣∣⅜]⅜ ≤ σ+σ,¾a^ τ uniform r.v in (1, ∙ ∙ ∙ , N}
ADABOUND ⑴，AMSBc)UND(2) (Luo et al., 2019)	%dip(忘，曲 S)，% (涌 ηι{n} non-decreasing to a* ηu(n) non-increasing to a* (1)①八+1 = (1 - c)vn + cg^i ⑶。九十1 = max(-υri,, (1 - c)vn + CgV)	1 - (1 - 6)λn--1 or 1 — n bn ≥b	cn = c	•	bounded gradients •	closed convex bounded feasible set •	1 — b < ʌ/l — c	Rτ∕T = O(1∕√T)
UnderreVieW as a ConferenCe PaPersICLR 2020
Under review as a conference paper at ICLR 2020
Discussion of theoretical results. The first type of convergence results uses the online optimization
framework which controls the convergence rate of the average regret. This framework was adopted
for AMSGRAD, ADAMNC (Reddi et al., 2018), ADABOUND and AMSBOUND (Luo et al., 2019).
In this setting, it is assumed that the feasible set containing the iterates is bounded by adding a
projection step to the algorithm if needed. We do not make such an assumption in our analysis.
(Reddi et al., 2018) establishes a regret bound in the convex setting. The second type of theoretical
results is based on the control of the norm of the (stochastic) gradients. We remark that some of these
results depend on the dimension of the parameters. Zhou et al. (2018) improves this dependency in
comparison to Chen et al. (2019). The convergence result in De et al. (2018) is established under
quite specific values of an+1, bn and . Zaheer et al. (2018) show a O(1/n) convergence rate for
an increasing mini-batch size. However, the proof is provided for RMSprop and seems difficult
to adapt to Adam which involves a momentum term. Indeed, unlike RMSProp, Adam does not
admit the objective function as a Lyapunov function. Although we assume boundedness of the
step size by Condition (7), We do not suppose that aι ≤ 聂(See table in Section 3.2) which can
impose a very small step size and result in a slow convergence. The step size assumption aι ≤ 亢
imposes a very small step size which may result in a slow convergence. We also remark that all the
available theoretical results assume boundedness of the (stochastic) gradients. We do not make such
an assumption. Furthermore, we do not add any decreasing 1 / √n factor in front of the adaptive step
size as it is considered in Reddi et al. (2018); Luo et al. (2019) and Chen et al. (2019). Although
constant hyperparameters b and c are used in practice, theoretical results are often established for
non constant bn and cn (Reddi et al., 2018; Luo et al., 2019). We also mention that most of the
theoretical bounds depend on the dimension of the parameter (Reddi et al., 2018; Zhou et al., 2018;
Chen et al., 2019; Zou et al., 2019; Luo et al., 2019).
Other variants of Adam . Recently, several other algorithms were proposed in the literature to en-
hance Adam . Although these algorithms lack theoretical guarantees, they present interesting ideas
and show good practical performance. For instance, AdaShift (Zhou et al., 2019) argues that the
convergence issue of Adam is due to its unbalanced step sizes. To solve this issue, they propose
to use temporally shifted gradients to compute the second moment estimate in order to decorrelate
it from the first moment estimate. Nadam (Dozat, 2016) incorporates Nesterov’s acceleration into
Adam in order to improve its speed of convergence. Moreover, originally motivated by variance
reduction, QHAdam (Ma and Yarats, 2019) replaces both Adam’s moment estimates by quasi-
hyperbolic terms and recovers Adam , Rmsprop and Nadam as particular cases (modulo the bias
correction). Guided by the same variance reduction principle, Radam (Liu et al., 2019) estimates
the variance of the effective step size of the algorithm and proposes a multiplicative variance correc-
tion to the update rule.
Step size bound. Perhaps, the closest idea to our algorithm is the recent ADABOUND (Luo et al.,
2019) which considers a dynamic learning rate bound. Luo et al. (2019) show that extremely small
and large learning rates can cause convergence issues to adam and exhibit empirical situations
where such an issue shows up. Inspired by the gradient clipping strategy proposed in Pascanu
et al. (2013) to tackle the problem of vanishing and exploding gradients in training recurrent neural
networks, Luo et al. (2019) apply clipping to the effective step size of the algorithm in order to
circumvent step size instability. More precisely, authors propose dynamic bounds on the learning
rate of adaptive methods such as Adam or AmsGrad to solve the problem of extreme learning rates
which can lead to poor performance. Initialized respectively at 0 and ∞, lower and upper bounds
both converge smoothly to a constant final step size following a predetermined formula defined by
the user. Consequently, the algorithm resembles an adaptive algorithm in the first iterations and
becomes progressively similar to a standard SGD algorithm. Our approach is different : we propose
a static bound on the adaptive learning rate which depends on the Lipschitz constant of the objective
function. This bound stems naturally from our theoretical derivations.
4	First Order Convergence Rate
4.1	Deterministic setting
Let (Hn)n≥o be a sequence defined for all n ∈ N by Hn := f (xn) + \(a。,pQ .
We further assume the following step size growth condition.
5
Under review as a conference paper at ICLR 2020
Assumption 4.1. There exists a > 0 s.t. an+ι ≤ 誓
Note that this assumption is satisfied for ADAM with α = √1 - C where C is the parameter in (3).
Unlike in AmsGrad (Reddi et al., 2018), the step size is not necessarily nonincreasing.
We provide a proof of the following key lemma in Appendix A.1.
Lemma 4.1. Let Assumptions 2.1 and 4.1 hold true. Then, for all n ∈ N, for all u ∈ R+,
Hn+1 ≤ Hn - han+lpn+l, An+1i - ] han+1 (Nf(Xn)- Pn)I2 , BIi ,	(5)
where An+ι = 1- an+L - |b - (ɪ - a)| - ⅛α and B :=1 - |b - (1 - α)lu-(1-α)
2	2u	2b	b
We state now one of the principal convergence results about Algorithm 2. In particular, we establish
a sublinear convergence rate for the minimum of the gradients norms until time n.
Theorem 4.2. Let Assumptions 2.1 and 4.1 hold true. Suppose that 1 - α < b ≤ 1. Let ε > 0 s.t.
asup ：= : (1 - (b-(2bOa))--⅛ɑ - ε) is nonnegative. Let δ > 0 s.t. for all n ∈ N,
δ ≤ an+1
≤ min
(6)
Then,
(i)	the sequence (Hn) is nonincreasing and Pn kpn k2 < ∞.
In particular, lim xn+1 - xn → 0 and lim Nf(xn) → 0 as n → +∞.
(ii)	For all n ≥ 1,	min0≤k≤n-1 kNf(xk)k2 ≤ n42 (H0 δinff + kpok2).
We provide some comments on this result.
Dimension dependence. Unlike most of the theoretical results for variants of ADAM as gathered in
Section 3.2, we remark that the bound (ii) does not depend on the dimension d of the parameter xk.
Comparison to gradient descent. A similar result holds for deterministic gradient descent. Ifγ is
a fix step size for gradient descent and there exist δ > 0,ε> 0 s.t. γ>δ and 1 一 YL > ε, then (
see Appendix B.6) for all n ≥ 1:
min
0≤k≤n-1
kNf(xk)k2≤
f (xo) - inf f ≤ f(xo) - inf f
nγ(1 - γL) — nδε
When p0 = 0 (this is the case for ADAM ), the bound in Theorem 4.2 coincides with the gradient
descent bound, up to the constant 4/b2. We mention however that ε for Algorithm (2) is defined by a
slightly more restrictive condition than for gradient descent : when b = 1, there is no momentum and
asup = = (1 - 2ε) < 2/L. Hence, under the boundedness of the effective step size, the algorithm
have a similar convergence guarantee to gradient descent. Remark that the step size bound almost
matches the classical 2/L upperbound on the step size of gradient descent. As itis already known for
gradient descent, a large step size, even if it is adaptive, can harm the convergence of the algorithm.
4.2	Stochastic setting
We establish a similar bound in the stochastic setting. Let (Ξ, S) denote a measurable space and
d ∈ N. Consider the problem of finding a local minimizer of the expectation F (x) := E(f (x, ξ))
w.r.t. x ∈ Rd, where f : Rd × Ξ → R is a measurable map and f( . , ξ) is a possibly nonconvex
function depending on some random variable ξ. The distribution of ξ is assumed unknown, but
revealed online by the observation of iid copies (ξn : n ≥ 1) of the r.v. ξ. For a fixed value of ξ,
the mapping x 7→ f(x, ξ) is supposed to be differentiable, and its gradient w.r.t. x is denoted by
Nf (x, ξ). We study a stochastic version of Algorithm (2) by replacing the deterministic gradient
Nf(xn) by Nf(xn, ξn+1).
Theorem 4.3. Let Assumption 2.1 (for F) and Assumption 4.1 hold true. Assume the following
bound on the variance in stochastic gradients: EkNf (x, ξ) - NF (x)k2 ≤ σ2 for all x ∈ Rd.
6
Under review as a conference paper at ICLR 2020
Suppose moreover that 1 - α < b ≤ 1. Let ε > 0 s.t. asup := L(1 - (b-(b-a))——1-α - ε) is
nonnegative. Let δ > 0 s.t. for all n ≥ 1, almost surely,
δ ≤ an+1
≤ min
(7)
Then,
EWF (XT )k2] ≤ nδ42α (H0-f + k√a0POk2 + F),
where x「is an iterate uniformly randomly chosen from {xo, ∙ ∙ ∙ ,Xn-1}.
The proof is defered to the appendix. In the special case where there is no momentum in the algo-
rithm (i.e. RMSProp) and assuming that the gradients are bounded, a similar convergence rate is
obtained in Zaheer et al. (2018, Thm. 1) (see Section 3.2).
5	Convergence RATE analysis under THE K匕 property
The KE inequality has been used to show the convergence of several first-order optimization methods
towards critical points (Attouch and Bolte, 2009; Attouch et al., 2010; 2013; Bolte et al., 2014;
Frankel et al., 2015; Li et al., 2017). In this section, we use a methodology exposed in Bolte et al.
(2018, Appendix) to show convergence rates based on the KE property. We modify it to encompass
momentum methods. Note that although this modification was initiated in ochs et al. (2014); ochs
(2018), we use a different separable Lyapunov function. The first part of the proof follows these
approaches and the second part follows the proof of Johnstone and Moulin (2017, Theorem 2).
Consider the function H : Rd × Rd → R defined for all z = (x, y) ∈ Rd × Rd by
H(Z) = H(χ,y) = f (χ) + 21b kyk2.	(8)
Notice that Hn = f (Xn) + J〈an,p» = H(xn, yn) where (yn)n∈N is a sequence defined for all
n ∈ N by yn = √αnPn.
Notations and definitions. If (E, d) is a metric space, z ∈ E and A is a non-empty subset ofE, we
use the notation d(z, A) :=inf{d(z, z0) : z0 ∈ A} . The set of critical points of the function H is
defined by crit H := {z ∈ R2d s.t. VH(Z) = 0}.
Definition 5.1. (set of limit points) The set of all limit points of (zk)k∈N initialized at zo is defined
by ω(zo) := {z ∈ R2d : ∃ an increasing sequence of integers (kj )j∈N s.t Zkj → Z as j → ∞}.
Assumption 5.1. f is coercive.
Lemma 5.1. (properties of the limit point set) Let (Zk)k∈N be the sequence defined for all k ∈ N
by Zk = (Xk,yk) where yk = √αkpk and (Xk,pk) is generated by Algorithm (2) from a starting
point Zo. Let Assumptions 2.1, 4.1 and 5.1 hold true. Assume that Condition (6) holds. Then,
(i)	ω(Zo) is a nonempty compact set.
(ii)	ω(Zo) ⊂ critH = critf × {0} .
(iii)	lim d(Zk,ω(Zo)) = 0.
k→+∞
(iv)	H is finite and constant on ω(Zo).
We introduce the KE inequality which is the key tool of our analysis. We refer to Bolte et al. (2010)
for an in-depth presentation of the KE property historically introduced by the fundamental works of
Eojasiewicz (1963) and Kurdyka (1998).
Define [α < H < β] := {Z ∈ R2d : α < H(Z) < β} . Let η > 0 and define
Φη := {夕 ∈ Co[0, η) ∩ C 1(0, η):2(0) = 0,2 concave and φ > 0}.
where Co [0, η) is the set of continuous functions on [0, η) and C1(0, η) is the set of continuously
differentiable functions on (0, η) .
Definition 5.2. (KE property, Bolte et al. (2018, Appendix)) A proper and lower semicontinuous
(l.s.c) function H : R2d → (-∞, +∞] has the KE property locally at Z ∈ domH if there exist
η > 0, φ ∈ Φη and a neighborhood U(Z) s.t. for all Z ∈ U(Z) ∩ [H(Z) < H < H(Z) + η]:
,(H(z)- H(Z)) IlVH(Z)k≥ 1.	(9)
7
Under review as a conference paper at ICLR 2020
When H(Z) = 0, We can rewrite Equation (9) as : k▽(夕◦ H)(z)k ≥ 1 for suitable Z points.
This means that H becomes sharp under a reparameterization of its values through the so-called
desingularizing function 夕.The function H is said to be a KE function if it has the KE property
at each point of the domain of its gradient. The KE property is satisfied by the broad class of
semialgebraic functions including most objective functions in real applications (k ∙ ∣∣p for P rational,
real polynomials, rank, etc.). (see Bolte et al. (2014, Appendix) for more examples). KE inequality
holds at any non critical point (see Attouch et al. (2010, Remark 3.2 (b))).
We introduce now a uniformized version of the KE property which will be useful for our analysis.
Lemma 5.2. (Uniformized KE property, Bolte et al. (2014, Lemma 6, P 478)) Let Ω be a compact
set and let H : R2d → (一∞, +∞] be a proper l.s.c function. Assume that H is constant on Ω and
satisfies the KE property at each point of Ω. Then, there exist ε > 0,η > 0 and 夕 ∈ ①n such that
for all Z ∈ Ω, for all Z ∈ {z ∈ Rd : d(z, Ω) < ε}∩ [H(z) < H < H(z) + η], one has
”(H(z)- H(Z))∣VH(z)k ≥ 1	(10)
Definition 5.3. (KE exponent) If 夕 can be chosen as 夕(S) = C sθ for some Z > 0 and θ ∈ (0,1] in
Definition 5.2, then we say that H has the KE property at zz with an exponent of θ 1. We say that H
is a KE function with an exponent θ if it has the same exponent θ at any zZ.
Furthermore, if H is a proper closed semialgebraic function, then H is a KE function with a suitable
exponent θ ∈ (0,1]. The slope of 夕 around the origin informs about the “flatness" of a function
around a point. Hence, the KE exponent allows to obtain convergence rates. In the light of this
remark, we state one of the main results of this work.
Theorem 5.3. (Convergence rates) Let (zk)k∈N be the sequence defined for all k ∈ N by zk =
(xk, yk) where yk = √akpk and (Xk,pk) is generated by Algorithm (2) from a starting point z0.
Let Assumptions 2.1, 4.1 and 5.1 hold true. Assume that Condition (6) holds. Suppose moreover
that H is a KE function with KE exponent θ. Denote by f (x*) the limit of the sequence (H(Zk ))k∈N
where x* is a critical point of f. Then, the following convergence rates hold:
(i)	If θ = 1, then f(xk) converges in a finite number of iterations.
(ii)	If 1/2 ≤ θ < 1, then f(xk) converges to f(x*) linearly i.e. there exist q ∈ (0, 1), C > 0
s.t. f(xk) - f(χ*) ≤ Cqk .
(iii)	If 0 <θ< 1/2 ,then f(xk) - f(x*) = O(k 2θ-1).
Sketch of the proof. The proof consists of two main steps. The first one is to show that the iterates
enter and stay in a region where the KE inequality holds. This is achieved using the properties of
the limit set (Lemma 5.1) and the uniformized KE property (Lemma 5.2). Then, the second step is
to exploit this inequality in order to derive the sought convergence results. We defer the complete
proof to Appendix B.3.
We introduce a lemma in order to make the KE assumption on the objective function f instead of
the auxiliary function H .
Lemma 5.4. Let f be a continuously differentiable function satisfying the KL property at xZ with an
exponent of θ ∈ (0, 1/2]. Then the function H defined in Equation (8) has also the KE property at
(xZ, 0) with an exponent of θ .
The following result derives a convergence rate on the objective function values under aKE assump-
tion on this same function instead of an assumption on the Lyapunov function H. The result is an
immediate consequence of Lemma 5.4 and Theorem 5.3.
Corollary 5.5. Let (zk)k∈N be the sequence defined for all k ∈ N by zk = (xk, yk) where yk =
√αkpk and (Xk,pk) is generated by Algorithm (2) from a starting point z°. Let Assumptions 2.1,
4.1 and 5.1 hold true. Assume that Condition (6) holds. Suppose moreover that f is a KE function
with KE exponent θ ∈ (0, 1/2). Denote by f(x*) the limit of the sequence (H(zk))k∈N where x*
is a critical point of f. Then f(xk) - f(x*) = O(k2θ-1).
1 α := 1 - θ is also defined as the KE exponent in other papers (Li and Pong, 2018).
8
Under review as a conference paper at ICLR 2020
References
P-A. Absil, R. Mahony, and B. Andrews. Convergence of the iterates of descent methods for analytic
cost functions. SIAM Journal on Optimization,16(2):531-547, 2005.
N. Agarwal, B. Bullins, X. Chen, E. Hazan, K. Singh, C. Zhang, and Y. Zhang. Efficient full-matrix
adaptive regularization. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings
of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pages 102-110, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/agarwal19b.html.
H. Attouch and J. Bolte. On the convergence of the proximal algorithm for nonsmooth functions
involving analytic features. Mathematical Programming, 116(1-2):5-16, 2009.
H. Attouch, J. Bolte, P. Redont, and A. Soubeyran. Proximal alternating minimization and projec-
tion methods for nonconvex problems: An approach based on the kurdyka-lojasiewicz inequality.
Mathematics of Operations Research, 35(2):438-457, 2010.
H. Attouch, J. Bolte, and B. F. Svaiter. Convergence of descent methods for semi-algebraic and
tame problems: proximal algorithms, forward-backward splitting, and regularized gauss-seidel
methods. Mathematical Programming, 137(1-2):91-129, 2013.
J. Bolte, A. Daniilidis, O. Ley, and L. Mazet. Characterizations of lojasiewicz inequalities: sub-
gradient flows, talweg, convexity. Transactions of the American Mathematical Society, 362(6):
3319-3363, 2010.
J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimization for nonconvex
and nonsmooth problems. Mathematical Programming, 146(1-2):459-494, 2014.
J. Bolte, S. Sabach, M. Teboulle, and Y. Vaisbourd. First order methods beyond convexity and
lipschitz gradient continuity with applications to quadratic inverse problems. SIAM Journal on
Optimization, 28(3):2131-2151, 2018.
L. Bottou, F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. Siam
Review, 60(2):223-311, 2018.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for
non-convex optimization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=H1x-x309tm.
S.	De, A. Mukherjee, and E. Ullah. Convergence guarantees for rmsprop and adam in non-convex
optimization and their comparison to nesterov acceleration on autoencoders. arXiv preprint
arXiv:1807.06766, 2018.
J. Diakonikolas and M. I. Jordan. Generalized momentum-based methods: A hamiltonian perspec-
tive. arXiv preprint arXiv:1906.00436, 2019.
T.	Dozat. Incorporating nesterov momentum into adam. 2016.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
P. Frankel, G. Garrigos, and J. Peypouquet. Splitting methods with variable metric for kurdyka-
lojasiewicz functions and general convergence rates. Journal of Optimization Theory and Appli-
cations, 165(3):874-900, 2015.
V. Gupta, T. Koren, and Y. Singer. A unified approach to adaptive regularization in online and
stochastic optimization. arXiv preprint arXiv:1706.06569, 2017.
P. R. Johnstone and P. Moulin. Convergence rates of inertial splitting schemes for nonconvex com-
posite optimization. In 2017 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 4716-4720. IEEE, 2017.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations, 2015.
9
Under review as a conference paper at ICLR 2020
K. Kurdyka. On gradients of functions definable in o-minimal structures. In Annales de l’institut
Fourier, volume 48, pages 769-783,1998.
G. Li and T. K. Pong. Calculus of the exponent of kurdyka-IojasieWicz inequality and its applica-
tions to linear convergence of first-order methods. Foundations of computational mathematics,
18(5):1199-1232, 2018.
QunWei Li, Yi Zhou, Yingbin Liang, and Pramod K Varshney. Convergence analysis of proximal
gradient With momentum for nonconvex optimization. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages 2111-2119. JMLR. org, 2017.
J. Liang, J. Fadili, and G. Peyre. A multi-step inertial forward-backward splitting method for non-
Convex optimization. In Advances in Neural Information Processing Systems, pages 4035T043,
2016.
L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive learning
rate and beyond. arXiv preprint arXiv:1908.03265, 2019.
S. Ecjasiewicz. Une PrOPriete topologique des sous-ensembles analytiques reels. Les equations aux
derivees partielles, 117:87-89, 1963.
L. Luo, Y. Xiong, and Y. Liu. Adaptive gradient methods with dynamic bound of learning rate. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=Bkg3g2R9FX.
J. Ma and D. Yarats. Quasi-hyperbolic momentum and adam for deep learning. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=S1fUpoR5FQ.
H. B. McMahan and M. J. Streeter. Adaptive bound optimization for online convex optimization. In
COLT, pages 244-256, 2010.
P. Ochs. Local convergence of the heavy-ball method and ipiano for non-convex optimization.
Journal of Optimization Theory and Applications, 177(1):153-180, 2018.
P. Ochs, Y. Chen, T. Brox, and T. Pock. ipiano: Inertial proximal algorithm for nonconvex optimiza-
tion. SIAMJournal on Imaging Sciences,7(2):1388-1419, 2014. doi: 10.1137/130942954. URL
https://doi.org/10.1137/130942954.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In
International conference on machine learning, pages 1310-1318, 2013.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1-17, 1964.
S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=ryQu7f-RZ.
H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statis-
tics, pages 400^07, 1951.
H.	Robbins and S. Monro. A stochastic approximation method. In Herbert Robbins Selected Papers,
pages 102-109. Springer, 1985.
I.	Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
in deep learning. In International conference on machine learning, pages 1139-1147, 2013.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. Coursera: Neural networks for machine learning, 4(2):26-31, 2012.
Z. Wu and M. Li. General inertial proximal gradient method for a class of nonconvex nonsmooth
optimization problems. Computational OPtimizatiOn and Applications, 73(1):129-158, 2019.
10
Under review as a conference paper at ICLR 2020
M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimiza-
tion. In Advances in Neural Information Processing Systems, pages 9793-9803, 2018.
D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the convergence of adaptive gradient methods
for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
Z. Zhou, Q. Zhang, G. Lu, H. Wang, W. Zhang, and Y. Yu. Adashift: Decorrelation and convergence
of adaptive learning rate methods. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=HkgTkhRcKQ.
F. Zou, L. Shen, Z. Jie, W. Zhang, and W. Liu. A sufficient condition for convergences of adam and
rmsprop. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 11127-11135, 2019.
A Appendix A
A.1 Proof of Lemma 4.1
Supposing that Vf is L-Lipschitz, using Taylor's expansion and the expression of Pn in the algo-
rithm, we obtain the following inequality:
f (Xn+1) ≤ f (Xn) - hVf (Xn) ,an+1pn+1i + ^ "2 kan+1 pn+1k	(II)
Moreover,
2b han+1,pn+1i - 2b han,pni = 2b han+1,pn+1 - Pn i + y han+1 - an,*〉.	(I2)
Observing that p2n+1 - p2n = -b2(Vf(Xn) - pn)2 + 2bpn+1 (Vf (Xn) - pn), we obtain after sim-
plification :
Hn+1 ≤ Hn +2 kan+lPn+1 k2 - ~2 han+1, (Vf(Xn)-Pn) 2i-han+lPn+1 ,Pn i +万 han+1 -an,pn.
(13)
Using again Pn = Pn+1 - b(Vf (Xn) -Pn), we replace Pn :
Hn+1 ≤ Hn + 2 kan+lPn+1 k2 - ~2 han+1, (Vf(Xn)- Pn)) 2〉
-han+1，Pn+1i + bhan+1Pn+1, Vf(Xn) - Pn)+ 赤 han+1 - an,Pni .
Under Assumption 4.1, we write: han+1 - an,P2ni ≤ (1 - α)han+1,Pni and using Pn = Pn+1 +
b2(Vf(Xn) -Pn)2 - 2bPn+1(Vf(Xn) -Pn), it holds that:
Hn+1 ≤ Hn - han+1,Pn+1i - ~2 han+1, (Vf(Xn)- Pn))2〉
+ 2 kan+1Pn+1k2 + (b - (1 - α))han+1Pn+1, Vf(Xn) - Pn)
1 - α	) b(1 - α)	)
+	2b- han+1,Pn+1i +	2---han+1, (Vf (Xn) - Pn)).
Using the classical inequality Xy ≤ χ2 + uy2, We have :
(b-(1-α))an+1Pn+1(Vf(Xn)-Pn) ≤
”(J -⑹ han+1,Pn+1i+ |b-(；- W1" han+1, (Vf (Xn)-Pn)2i.
2u	2
(14)
11
Under review as a conference paper at ICLR 2020
Hence, after using this inequality and rearranging the terms, we derive the following inequality:
Hn+1 ≤ Hn-han+1 P^1, 1 — " - ^^-^^ - 1^ i
— 2han+ι(Vf(xn) -Pn)2jl- ^-'- O)Iu - (1 - α)) 1).
This concludes the proof.
A.2 An additional proposition
Proposition A.1. Let Assumption 2.1 hold true. Suppose moreover that 1 - α < b ≤ 1. Let ε > 0
s.t. asup := LL(1 一 (b-(2baa))-⅛α 一 ε) is nonnegative. Let δ > 0 s.t. for all n ∈ N,
an+1 ≤ min
(asup, an).
Then, for all n ≥ 1,
Xhak+1, Vf(Xk)2) ≤ 4- (H0 - inff + hao,p0i
b2 α	ε
k=0
Proof. This is a consequence of Lemma 4.1. Conditions An+1 ≥ ε and B ≥ 0 write as follow :
b 一 (1 一 α)	1 一 α
+ 1 ≤ L V	2u	2b~
αb
一 ε and u ≤
b 一 (1 一 α)
We get the assumption made in the proposition by injecting the second condition into the first one
and adding the assumption an+1 ≤ α made in the lemma. Under this assumption, we sum over
an
0 ≤ k ≤ n 一 1 Equation (5), rearrange it and use An+1 ≥ ε, B ≥ 0 to obtain :
n-1
εhak+1 , p2k+1 ) ≤ H0 一 Hn ,
k=0
Then, observe that Hn ≥ f(xn) ≥ inf f. Therefore, we derive :
n-1
hak+1,p2k+1) ≤
k=0
Ho 一 inf f
ε
(15)
Moreover, from the Algorithm 2 second update rule, We get Vf (Xk) = 1bPk+ι 一 1-bPk. Hence, We
have for all k ≥ 0 :
Vf(Xk)2 ≤ 2 (庐pk+ι + --b2ɪPk) ≤ b2(Pk+1 + Pk) .
12
Under review as a conference paper at ICLR 2020
We deduce that :
n-1
Xhak+1, Vf(Xk)2i ≤
k=0
k=0
k=0
2 n-1	2 n-1
b Ehak+1,pk+ιi + b2α ΣShak ,pk i
k=0	k=0
2	n-1
b2α I 2 ΣShak ,pk i + han,pni + ha0,p0i
k=1
n
b4a Xhak ,pk i
k=0
≤ b4a (H0-εf + hao,p2i
A.3 Proof of Theorem 4.2
This is a consequence of Lemma 4.1. Conditions An+1 ≥ ε and B ≥ 0 write as follow :
an+1
≤ K1-
b- (1- α)	1- α
2U	2b~
αb
-ε and U ≤
b- (1- α)
≤
≤
≤
□
We get the assumption made in the proposition by injecting the second condition into the first one
and adding the assumption an+1 ≤ α made in the lemma. Under this assumption, we sum over
an
0 ≤ k ≤ n - 1 Equation (5), rearrange it and use An+1 ≥ ε, B ≥ 0 and ak+1 ≥ δ to obtain :
n-1
Xδεkpk+1k2 ≤H0 -Hn,
k=0
Then, observe that Hn ≥ f(xn) ≥ inff. Therefore, we derive :
n-1
X kpk+1k2 ≤
k=0
Ho - inf f
δε
(16)
Moreover, from the algorithm 2 second update rule, We get Vf (Xk) = bPk+ι — 1-bPk. Hence, We
have for all k ≥ 0 :
kVf(xk)k2 ≤ 2 (b2kpk+ιk2 + (ɪ-bɪ∣∣pkk2) ≤ b(kPk+ιk2 + kPkk2).
We deduce that :
X kVf (Xk)k2 ≤ b2 X(kPk+ιk2+kPkk2) = b
k=0	k=0
n-1
2XkPkk2+kPnk2+ kP0k2
k=1
n
≤ 4 x I-.
k=0
(17)
Finally, using Equations (16) and (17), We have :
n-1
min kVf(Xk)k2 ≤ — X kVf(xk)k2
0≤k≤n-1	n
k=0
≤ Y
nb2
Ho - inf f
δε
+ kP0k2
13
Under review as a conference paper at ICLR 2020
A.4 Proof of Theorem 4.3
The proof of this proposition mainly follows the same path as its deterministic counterpart. How-
ever, due to stochasticity, a residual term (the last term in Equation (18)) quantifying the difference
between the stochastic gradient estimate and the true gradient of the objective function (compare
Equation (18) to Lemma 4.1). Following the exact same steps of Appendix A.1, we obtain by re-
placing the deterministic gradient Vf (Xn) by its stochastic estimate Vf (xn, ξn+ι):
Hn+1 ≤ Hn-han+ip"l,1- T - ^^-^^ - 1^i
-2 han+1(Vf (xn, ξn+1) - Pn )2, (1 -  -----(I - α)) 1i
+ hVf(xn,ξn+1) -VF(xn),an+1pn+1i.	(18)
Using the classical inequality Xy ≤ x2 + y2 and the almost sure boundedness of the step size an+ι,
we get :
hVf (Xn,ξn+l) - VF(Xn),an+lPn+li ≤〈 1(Vf (Xn,ξn+l) — VF(Xn))2 + 1 夕看+1，。0+1〉
≤ -SUp kVf (Xn, ξn+1)- VF (Xn)k2 + ]〈厮+匕*+).
Therefore, taking the expectation and using the boundedness of the variance, we obtain from Equa-
tion (18) :
E[Hn+ι] - E[Hn] ≤ -E [han+ιpn+ι, 1 - an+L - |b-(； - a)| - 1-αil + asupσ2 .
2	2u	2b	2
Then, the proof follows the lines of Appendix A.2. Hence, we have
2
EHn+1] - E[Hn] ≤ -E [han+lPn+ι,ε1i] +-----H----
We sum these inequalities for k = 0,…，n — 1 and rearrange the terms to obtain
n-1
E	hak+1,p2k+1i
k=0
≤ Ho- inf f +
2
nasup Q
2ε
ε
Then following the derivations in Appendix A.2 using Vf (Xk, ξk+ι) = bPk+ι 一 1-bPk, We estab-
lish the following inequality
E Xhak+1, Vf(Xk ,ξk+1)2i ≤ b⅛ ( H° εinff + hao,p0i + na2f-).
k=0
Finally, we apply Jensen,s inequality to ∣∣ ∙ ∣∣2, we inject the assumption an+ι ≥ δ and we divide the
previous inequality by n to obtain the sought result
n-1
n X e [∣vf (Xk )k2]
k=°
≤	4	(H° - inf f
nδb2 α
2
+ ha°,P°i + 竺”
ε
14
Under review as a conference paper at ICLR 2020
B Appendix B
B.1 Conditions similar to ”gradient-like descent sequences” conditions
(B olte et al., 2018, Appendix)(Ochs et al., 2014)
Lemma B.1. Let (zk)k∈N be the sequence defined for all k ∈ N by zk = (xk, yk) where yk =
√akPk and (χk,Pk) is generated by Algorithm (2) from a starting point z°. Let Assumptions 2.1
and 4.1 hold true. Assume moreover that condition (6) holds. Then,
(i)
(ii)
(iii)
(sufficient decrease property) There exists a positive scalar ρ1 s.t. :
H (zk+1 ) - H(zk) ≤ -ρ1 kxk+1 - xk k2 ∀k ∈ N.
There exists a positive scalar ρ2 s.t. :
∣∣VH(zk+ι)k ≤ P2 (∣∣xk+ι - xkk + IIxk - xk-ιk) Vk ≥ 1.
(continuity condition) If Z is a limit point of a subsequence (Zkj) j∈N, then lim H(Zkj)
j	j→+∞	j
H⑸.
Remark 1. Conditions in Bolte et al. (2018, Appendix) corresponding to Lemma B.1 are more
general. Authors consider a nonsmooth objective function and introduce the Frechet subdifferential
instead of the gradient. For our purposes, this is sufficient.
Proof. (i) From Lemma 4.1 and theorem 4.2, we get for all k ∈ N:
H(Zk+I)-H(Zk) ≤ -εhak+1,pk + li ≤ -εhak+1, ( ⅛ + 1--) i ≤--kxk+1-xk『∙
-ak+1	asup
We set ρ1 :
ε
asup
(ii)	First, observe that for all k ∈ N
kVH(Zk+ι)k ≤ ∣Vf(xk+ι)∣ + b ∣yk+ιk.
(19)
Now, let us upperbound each one of these two terms. Recall that we can rewrite our algorithm
under a ”Heavy-ball”-like form as follows:
xk+1 = xk - αkVf(xk) + βk(xk - xk-1) ∀k ≥ 1.
where ak := bak+1 and βk = (1 - b) a0+1 are vectors.
On the one hand,
∣Vf(xk+ι)k2 ≤ 2 (kVf(xk+ι) - Vf(xk)k2 + ∣Vf(xk)k2)
≤ 2 (L2∣∣xk+1- xkk2 + ∣Vf(xk)∣2)
(L-Lipschitz continuity of the gradient)
Moreover,
kVf (Xk)『=；+ αk (Xk-Xk-0
≤2
2
Xk - Xk+1	+ 2
bak+1
「 L(Xk - Xk-1)
b ak
2
≤ b2δ2 kXk+1- Xkk2+ 2l'b2δ^ kXk - Xk-Ik2
≤ b2δ2 (kXk + 1 - Xk k2 + IlXk - Xk-1k2).
15
Under review as a conference paper at ICLR 2020
Hence,
kVf (xk+ι)k2 ≤ 2 (L2 kxk+1 - Xkk2 + kVf (xk)k2)
≤2
+ 1 - Xk k2 + b2δ2 kxk - xk-1 k
≤2
- Xkk2 + kXk - Xk-1k2) .
2
Therefore, the following inequality holds :
kVf(Xk+1)k ≤
2 (L2 + n§2 J(kxk+1 - Xk k + kxk - Xk-IlI) ∙
On the otherhand,
kyk+ιk = k√ak+ιPk+ιk = xk+1_Xk ≤ -≡ kxk+1 — xkk.
ak+1	δ
Finally, combining the inequalities for both terms in Equation (19), we obtain
kVH(Zk+ι)k ≤ ρ2(kXk+1 - Xkk + kXk - Xk-ιk) Vk ≥ 1.
with ρ2 := (q2 (L2 + b2δ2) + b√δ).
(iii)	This is a consequence of the continuity of H.
□
B.2	Proof of Lemma 5.1
(i)	By Theorem 4.2, the sequence (H(zn))n∈N is nonincreasing. Therefore, for all n ∈ N,
H(zn) ≤ H(z0) and hence zn ∈ {z : H(z) ≤ H(z0)} . Since f is coercive, H is also
coercive and its level sets are bounded. As a consequence, (zn)n∈N is bounded and there
exist z* ∈ Rd and a subsequence (zkj)j∈N s.t. Zkj → z* as j → ∞. Hence, ω(zo) = 0 .
Furthermore, ω(zo) = Tq∈N Uk≥q{zk } is compact as an intersection of compact sets.
(ii)	First, critH = critf	×	{0}	because	VH(z)	= (Vf (X), y/b)T .	Let	z*	∈	ω(z0).	Recall
that Xk+1 - Xk → 0 as k → ∞ by Theorem 4.2. We deduce from the second assertion
of Lemma B.1 that VH(zk) → 0 as k → ∞ . As z* ∈ ω(z0), there exists a subsequence
(zkj)j∈N converging to z*. Then, by Lipschitz continuity of VH, we get that VH(zkj ) →
VH(z*) as j → ∞ . Finally, VH(z*) = 0 since VH(zk) → 0 and (VH (zkj))j ∈N is a
subsequence of (VH (zn))n∈N .
(iii)	This point stems from the definition of limit points. Every subsequence of the sequence
(d(zk, ω(z0)))k∈N converges to zero as a consequence of the definition of ω(z0).
(iv)	The sequence (H(zn))n∈N is nonincreasing by Theorem 4.2. It is also bounded from below
because H(zk) ≥ f(Xk) ≥ inf f for all k ∈ N. Hence we can denote by l its limit. Let
W ∈ ω(zo). There there exists a subsequence (Zkj )j∈N converging to Z as j → ∞ . By
the third assertion of Lemma B.1, lim H(Zkj) = H(z). Hence this limit equals l since
j→+∞
(H(zn))n∈N converges towards l. Therefore, the restriction of H to ω(z0) equals l .
B.3	Proof of Theorem 5.3
The first step of this proof follows the same path as Bolte et al. (2018, Proof of Theorem 6.2,
Appendix). Since f is coercive, H is also coercive. The sequence (H(zk))k∈N is nonincreasing.
Hence, (zk) is bounded and there exists a subsequence (zkq)q∈N and zz ∈ R2d s.t. zkq → zz as
q → ∞ . Then, since (H (zk))k∈N is nonincreasing and lowerbounded by inf f, it is convergent and
we obtain by continuity of H,
lim H (zk ) = H (zz) .	(20)
k→+∞
16
Under review as a conference paper at ICLR 2020
If there exists k ∈ N s.t. H(Zk) = H(Z), then H(z^+ι) = H(Z) and by the first point of
Lemma B.1, χι+ι = Xk and then (Xk)k∈N is stationary and for all k ≥ k, H(Zk) = H(z) and
the results of the theorem hold in this case (note that W ∈ critH by Lemma 5.1). Therefore, we
can assume now that H(Z) < H(Zk)Vk > 0 since (H(Zk))k∈N is nonincreasing and Equation (20)
holds. One more time, from Equation (20), we have that for all η > 0, there exists k0 ∈ N s.t.
H(Zk) < H (ZZ) + η for all k > k0. From Lemma 5.1, we get d(Zk, ω(Z0)) → 0 as k → +∞ .
Hence, for all ε > 0, there exists k1 ∈ N s.t. d(Zk, ω(Z0)) < ε for all k > k1 . Moreover, ω(Z0) is a
nonempty compact set and H is finite and constant on it. Therefore, we can apply the uniformization
Lemma 5.2 with Ω = ω(Z0). Hence, for any k > l := max(k0, k∖), We get
φ'(H(Zk) — H(Z))2 kVH(Zk)k2 ≥ 1.	(21)
This completes the first step of the proof. In the second step, we follow the proof of Johnstone and
Moulin (2017, Theorem 2). Using Lemma B.1 .(i)-(ii), we can write for all k ≥ 1,
2ρ2
IIvh (Zk+1)k ≤ 2p2 (∣∣xk+1 - xk k + IIXk - Xk-IlI) ≤ -----(H(Zk-I)- H(Zk+1)) ∙
2	ρ1
Injecting the last inequality in Equation (21), we obtain for all k > k2 := max(l, 2),
2ρ2
R S(H(Zk)- H(Z))2 (H(Zk-2) - H(Zk)) ≥ 1.
ρ1
Now, use φ0(s) = Csθ-1 to derive the following for all k > k2:
[H(Zk-2) — H(Z)] - [H(Zk) - H(Z)] ≥ 鼻[H(Zk) - H(z)]2(1-θ).	(22)
2ρ22 cZ2
Let rk := H(Zk) - H(Z) and Ci =②乳2. Then, we can rewrite Equation (22) as
rk-2 - rk ≥ C1rk2(1-θ) ∀k > k2 .	(23)
We distinguish three different cases to obtain the sought results.
(i)	θ=1:
Suppose rk > 0 for all k > k2 . Then, since we know that rk → 0 by Equation (20), C1 must
be equal to 0. This is a contradiction. Therefore, there exist k3 ∈ N s.t. rk = 0 for all k > k3
(recall that (rk)k∈N is nonincreasing).
(ii)	θ ≥ 1:
ASrk → 0, there exists k4 ∈ N s.t. for all k ≥ k4, rk ≤ 1. Observe that 2(1 - θ) ≤ 1 and
hence rk-2 - rk ≥ C1rk for all k > k2 and then
rk ≤ (1 + C1)-1rk-2 ≤ (1+C1)-p1rk4 .	(24)
where pi := [k-2k4C . Notice that pi > k-k4-2. Thus, the linear convergence result follows.
Note also that if θ = 1/2, 2(1 - θ) = 1 and Equation (24) holds for all k > k2 .
(iii)	θ < 1:
Define the function h by h(t) = -D^t2θ-i where D > 0 is a constant. Then,
h(rk) - h(rk-2)
rk	rk-2
h0(t)dt = D	t2θ-2dt ≥ D (rk-2 - rk) rk2θ--22
rk-2	rk
We disentangle now two cases :
(a)	Suppose 2rk2θ--22 ≥ rk2θ-2 . Then, by Equation (23), we get
h(rk) - h(rk-2) = D (rk-2 一『k) r2θ-22 ≥ C2D .	(25)
17
Under review as a conference paper at ICLR 2020
(b)	Suppose now the opposite inequation 2rk2θ--22 < rk2θ-2 . We can suppose without loss
of generality that rk are all positive. Otherwise, if there exists p such that rp = 0, the
sequence (rk)k∈N will be stationary at 0 for all k ≥ p . Observe that 2θ-2 < 2θ- 1 < 0,
thus 2θ-2 > 0. As a consequence, We can write in this case r2θ-1 > qr2- where
2θ-1
q := 22θ-2 > 1. Therefore, using moreover that the sequence (rk)k∈N is nonincreasing
and 2θ - 1 < 0, we derive the following
h(r,∖ h(r,	D rr2θ-1	2θ-∏ D	11 n 2θ-1 D	1 n 2θ-1	._ C
h(r	k)-h(rk-2) =	1 - 2θ (rk -rk-2 )	> 1 - 2θ	(q-1)rk-2	> 1 - 2θ	(q-1)rk2	：= C2	.
(26)
Combining Equation (25) and Equation (26) yields h(rk) ≥ h(rk-2) + C3 where C3 :=
min(C2, C2D). Consequently, h(rk) ≥ h(rk-2p2) + P2 C3 where p2 := bk-k2C . We
deduce from this inequality that
h(rk) ≥ h(rk) - h(rk-2 p2) ≥ p2 C3 .
Therefore, rearranging this inequality using the definition of h, we obtain rk1-2θ ≤
1-D2θ(C3P2)-1. Then, sincep > k-k2-2,
rk ≤ C4 P卢 ≤ C4 (上”)总
where C4 :=
1
Ca (1-2θ)	2θ-1
D
We conclude the proof by observing that f (Xk) ≤ H(Zk) and recalling that Z ∈ critH.
B.4 Proof of Lemma 5.4
Since f has the KE property at X with an exponent θ ∈ (0,1/2], there exist c, ε and ν > 0 s.t.
kVf(x)k 1⅛ ≥ c(f(x)- f (X))	(27)
for all x ∈ Rd s.t. ||x 一 Xk ≤ ε and f (x) < f (x) + V where condition f (x) 一 f (x) is dropped
because Equation (27) holds trivially otherwise. Let Z = (x, y) ∈ R2d be s.t. 1x 一 Xk ≤ ε, 1y| ≤ ε
and H(X, 0) < H(x, y) < H(X, 0) + V. We assume that ε < b (ε can be shrunk if needed). We
have f (x) ≤ H(x, y) < H(X, 0) + V = f (X) + V. Hence Equation (27) holds for these x.
1
By concavity of u → U 2(1-θ) , we obtain
kVH(χ,y)k土 ≥ Co (kVf(χ)k熹 + Ib∣∣ 14θ)
--1
where Co := 2 2(1-θ) 1.
Hence, using Equation (27), we get
kVH(χ,y)k占 ≥ Co
(C (f (x) — f (X)) +
Observe now that ɪ-θ ≥ 2 and∣∣b∣∣ ≤ b ≤ 1. Therefore, ∣∣ b∣∣1-θ ≥ ∣∣y∕bk2.
Finally,
kVH(χ,y)k熹 ≥ Co (C(f(x) - f(X)) +b2bkyk2)
≥ Co min 卜,∣) (f (X)- f (X) + 2b版『)
=Co min (c, ∣) (H(x, y) - H(X, 0)).
This completes the proof.
18
Under review as a conference paper at ICLR 2020
B.5	Comparison to Ochs et al. (2014)
We recall the conditions satisfied by αn and βn in Ochs et al. (2014) in order to traduce them in
terms of the algorithm (2) at stake. Define :
δn =	L	- L -血	Yn	：= δn	-正.
n	an 2	2αn	Yn n 2a“
Conditions of Ochs et al. (2014) write: αn ≥ c1 βn ≥ 0 δn ≥ γn ≥ c2 where c1 , c2 are positive
constants and (δn) is monotonically decreasing.
One can remark that algorithm (2) can be written as (4) with step sizes αn = ban+1 and inertial
parameters βn = (1 - b) an+1. Conditions on these parameters can be expressed in terms of an
Supposing c2 = 0, the condition γn ≥ c2 is equivalent to
an+1 ≤	2
an	2 - b(2 - anL)
(28)
Note that the classical condition an ≤ 2/L shows up consequently. Moreover, the condition on (δn)
is equivalent to
1
an+1
3 - b 1	1 - b
2 an	2an-1
for n ≥ 1.
(29)
Note that we get rid of condition (29) while allowing adaptive step sizes an (see Proposition A.1).
B.6	A convergence result for gradient descent in the nonconvex setting.
Consider the gradient descent algorithm defined by : xk+i = Xk - YVf (Xk). Assume that γ > 0
and 1 - y2l > 0.
Supposing that Vf is L-Lipschitz, using Taylor’s expansion and regrouping the terms, we obtain
the following inequality:
f (Xk+1) ≤ f (Xk) - Y (1 - ^2-) kVf (XkXL
Then, we sum the inequalities for 0 ≤ k ≤ n - 1, lower bound the gradients norms in the sum by
their minimum and we obtain :
min
0≤k≤n-1
kVf (Xk )k22 ≤
f (xo) - inf f
nγ(1 -竽)
B.7	About THE KL-PROPERTY
A simple example for more intuition. For building intuition, let us consider the 1D function g
defined by g(X) = |X|p for p ≥ 2 as exposed in Johnstone and Moulin (2017). The function
夕(t) = t1/p is a desingUIarizing function. When g is flat around the origin (large p), gradient
methods are slower to converge. Therefore, a smaller KL exponent encodes a slower convergence
behavior near a critical point.
Remark 2. Definition 5.2 can be generalized to a nonsmooth function by introducing the notion of
the Frechet subdifferential (which is a set) and replacing the norm by the distance to a set.
19