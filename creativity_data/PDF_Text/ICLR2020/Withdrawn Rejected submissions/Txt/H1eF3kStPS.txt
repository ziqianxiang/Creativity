Under review as a conference paper at ICLR 2020
Redundancy-Free Computation Graphs for
Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks (GNNs) are based on repeated aggregations of information
across nodes’ neighbors in a graph. However, because common neighbors are
shared between different nodes, this leads to repeated and inefficient computations.
We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN
graph representation that explicitly avoids redundancy by managing intermediate
aggregation results hierarchically, and eliminating repeated computations and
unnecessary data transfers in GNN training and inference. We introduce an accurate
cost function to quantitatively evaluate the runtime performance of different HAGs
and use a novel search algorithm to find optimized HAGs. Experiments show
that the HAG representation significantly outperforms the standard GNN graph
representation by increasing the end-to-end training throughput by up to 2.8×
and reducing the aggregations and data transfers in GNN training by up to 6.3×
and 5.6×. Meanwhile, HAGs improve runtime performance by preserving GNN
computation, and maintain the original model accuracy for arbitrary GNNs.
1	Introduction
Graph neural networks (GNNs) have shown state-of-the-art performance across a number of tasks
with graph-structured data, such as social networks, molecule networks, and webpage graphs (Kipf
and Welling, 2016; Hamilton et al., 2017; Ying et al., 2018; Xu et al., 2019). GNNs use a recursive
neighborhood aggregation scheme — in a GNN layer, each node aggregates its neighbors’ activations
from the previous GNN layer and uses the aggregated value to update its own activations. The
activations of the final GNN layer are used for downstream prediction tasks, such as node classification,
graph classification, or link prediction.
Due to the clustering nature of real-world graphs, different nodes in a graph may share a number
of common neighbors. For example, in webpage graphs, different websites under the same domain
generally have a number of common links (i.e., neighbors). As another example, in recommender
systems, users in the same group may have interests in common items.
However, existing GNN representations do not capture these common neighbors in real-world graphs,
leading to redundant and unnecessary computation in both GNN training and inference. In particular,
most existing GNN models (Kipf and Welling, 2016; Xu et al., 2019; Wu et al., 2019) use the
full-batch training method that computes the activations for all nodes in each layer. Existing GNN
representations use a computation graph (referred to as GNN-graph) to define computation in a
GNN layer. The GNN-graph includes a tree structure for each node v in the input graph describing
how to compute v’s activations by aggregating the previous-layer activations of v’s neighbors.
Figure 1b shows the GNN-graph of the input graph in Figure 1a; for example, for node A, its
neighbor’s activations h(Bk-1), h(Ck-1) and h(Dk-1) from the layer k - 1 are aggregated to compute
new activations h(Ak) for the layer k (see the top portion of Figure 1b). The new activations of the
other nodes are computed similarly using the previous activations of their neighbors. Notice that
this representation results in redundant computation and data transfers. In this small example, both
{A, B } and {C, D} are aggregated twice. In wider and multi-layer GNNs, the redundancies in
existing GNN representations account for a significant fraction of all computation. For example, our
experiments show that in modern GNNs up to 84% of the aggregations are redundant and avoidable.
In this paper, we propose a new GNN representation called Hierarchically Aggregated computation
Graphs (HAGs). Figure 1c shows one possible HAG for the input graph in Figure 1a. HAGs are
1
Under review as a conference paper at ICLR 2020
(a) Input graph	(b) GNN-graph	(C) HAG
Figure 1: Comparison between a GNN-graph and an equivalent HAG. (a) Input graph; (b) 1-layer
GNN computation graph (GNN-graph); (c) HAG that avoids redundant computation. The GNN-
(k)
graph computes new activations hv by aggregating the previous-layer activations of v’s neighbors.
Because nodes in the input graph share common neighbors, the GNN-graph performs redundant
computation (e.g., both {A, B} and {C, D} are aggregated twice). (c) By identifying common
computational patterns, the HAG avoids repeated computation.
functionally equivalent to standard GNN-graphs (produce the same output), but represent common
neighbors across different nodes using aggregation hierarchies, which eliminates redundant com-
putation and unnecessary data transfers in both GNN training and inference. In addition, a HAG is
agnostic to any particular GNN model, and can be used to eliminate redundancy for arbitrary GNNs.
For a GNN-graph, there exist numerous equivalent HAGs with different aggregation hierarchies
and runtime performance. We introduce an accurate cost function to quantitatively estimate the
performance of different HAGs and develop a novel HAG search algorithm to automatically find
optimized HAGs.
Theoretically, we prove that the search algorithm finds HAGs with strong performance guarantees: (1)
for GNN models whose neighborhood aggregations require a specific ordering on a node’s neighbors,
the algorithm finds a globally optimal HAG under the cost function; and (2) for other GNN models,
the algorithm finds HAGs whose runtime performance is at least a (1 - 1/e) approximation (≈ 63%)
of globally optimal HAGs using the submodularity property (Mossel and Roch, 2007). Empirically,
the algorithm finds HAGs that lead to major improvements, reducing the runtime by up to 2.9×.
Our HAG abstraction maintains the predictive performance of GNNs but leads to much faster training
and inference. We evaluate the performance of HAGs on five real-world datasets and along three
dimensions: (a) end-to-end training and inference runtime; (b) number of aggregations; and (c)
size of data transfers. Experiments show that HAGs increase the end-to-end training and inference
performance by up to 2.8× and 2.9×, respectively. In addition, compared to GNN-graphs, HAGs
reduce the number of aggregations and the size of data transfers by up to 6.3× and 5.6×, respectively.
2	Related Work
Graph neural networks have been used to solve various real-world tasks with relational struc-
tures (Kipf and Welling, 2016; Hamilton et al., 2017; Ying et al., 2018; Xu et al., 2019). Fast-
GCN (Chen et al., 2018) and SGC (Wu et al., 2019) accelerate GNN training using importance
sampling and removing nonlinearilities. This paper solves an orthogonal problem: how to optimize
GNN efficiency while maintaining network accuracy. Our methods can be applied automatically to
eliminate redundancy for arbitrary GNN models.
Join-trees are a tree decomposition technique that maps a graph into a corresponding tree structure to
solve optimization problems on the graph, such as query optimization (Flum et al., 2002). Although
a join-tree provides a possible way to find optimal HAGs for a GNN-graph, its time complexity is
exponential in the treewidth of a graph (Arnborg et al., 1987), and real graphs tend to have very large
treewidths. For example, Adcock et al. (2016) shows that the treewidth of real-world social networks
grows linearly with the network size, making it infeasible to use join-trees to find optimal HAGs.
2
Under review as a conference paper at ICLR 2020
Table 1: Existing GNNs described in our abstraction. GraphSAGE-P and GraphSAGE-LSTM are the pooling and LSTM variants of GraphSAGE, respectively. σ and max indicate element-wise non-linear activation and max functions. For sequential AGGREGATE, vi denotes the i-th in-neighbor of node v .			
GNN	AGGREGATE({hUk-1) |u ∈N(v)})		UPDATE(a(vk), h(vk-1))
Set Aggregate			
GCN (Kipf and Welling, 2016) GIN (Xu et al., 2019)	(k) av (k) av	= Pu∈N(V) hUk-1) Pu∈N (v) hu	hVk)=σ(W(k) ∙ J」 hVk) = σ(W ∙ ((1 + Ak))hVk-D + aVk)))
Sequential Aggregate			
GCN-LSTM (Hamilton et al., 2017) N-ary Tree-LSTM (Tai et al., 2015)	(k) av (k) av	= LSTM(hV1-1),…,hVk-1)) =Tree-LSTM-Agg (hVIT),…,h⅞-1))	hVk) = σ(W(k) ∙ (aVk),hVkτ))) h(vk) = Tree-LSTM-Update(a(vk), h(vk-1) )
Computation reduction in DNNs. Several techniques have been proposed to reduce computation
in DNNs, including pruning weights (Han et al., 2015) and quantization (Han et al., 2016). These
techniques reduce computation at the cost of modifying networks, resulting in decreased accuracy (as
reported in these papers). By contrast, we propose a new GNN representation that accelerates GNN
training by eliminating redundancy in GNN-graphs while maintaining the original network accuracy.
3	Hierarchically Aggregated Computation Graphs (HAGs)
Existing GNN-graph representation. An
input graph G = (V, E) has nodes V and
edges E. For each node v ∈ V, N (v) denotes
the set of neighbors of v, and xv denotes the
input node features. A GNN iteratively learns
representations for individual nodes over the
entire graph through a number of GNN lay-
ers, as shown in Algorithm 1. The learned
activations of node v at layer k is h(vk) , and
we initialize h(v0) with xv . At the k-th layer,
a(vk) denotes the aggregated activations of v’s
Algorithm 1 An abstraction for GNNs. V is the set
of nodes in an input graph, and N (v) denotes the set
of neighbors for node v.
1:	h(v0) = xv , ∀v ∈ V
2:	for k = 1 to K do
3:	for v ∈ V do
4:	aVk) — AGGREGATE({hUk-1)∣u ∈ N(v)})
5:	hVk) — UPDATE(aVk),hVkτ))
6:	Goal: minimize L({h(vK) |v ∈ V})
neighbors, which is combined with h(vk-1) to compute an updated activation h(vk) . The learned node
activations of the final layer (i.e., h(vK)) are used for downstream learning tasks, and a GNN model
generally minimizes a loss function L that takes the final node activations as inputs (line 6).
Existing GNN models use a GNN computation graph (GNN-graph) to describe the computation in
each GNN layer, as shown in Figure 1b. For each node v in the input graph, the GNN-graph includes
an individual tree structure to define how to compute the activations h(vk) of node v by aggregating
the previous-layer activations of v’s neighbors (i.e., {h(uk-1) , u ∈ N (v)}). GNN-graphs are efficient
at expressing direct neighborhood relations between nodes, but are not capable of capturing common
neighbors across multiple nodes, leading to redundant computation in GNN training and inference.
3.1	HAG Representation
We propose a new graph representation called Hierarchically Aggregated computation Graphs
(HAGs) for GNNs. HAGs eliminate redundancy in the GNN-graph representation by hierarchically
managing and reusing intermediate aggregation results. A HAG G = (V, E) has nodes V = V ∪ VA
and edges E, where V is the set of nodes in the original graph, and VA is a new set of aggregation
nodes. Each aggregation node in VA represents the intermediate aggregations result for a subset
of nodes (i.e., aggregation on a subset of h(vk-1)) . For the HAG example in Figure 1c, the new
nodes AB and CD denote the aggregation results of {A, B} and {C, D}, respectively. A HAG can
contain a multi-level aggregation hierarchy. For example, Figure 1c can also have a third aggregation
node ABCD that depends on AB and CD. Similar to edges in GNN-graphs, an edge (u, v) in a HAG
denotes an aggregation relation — computing v’s activations requires aggregating u’s activations.
The standard GNN-graph representation can be considered as a special case in the HAG representation
with no intermediate aggregation nodes (i.e., VA = 0). Our HAG abstraction is general and applicable
to many existing GNN models. Table 1 shows how to use our abstraction to define existing GNNs,
which can be further divided into two categories.
3
Under review as a conference paper at ICLR 2020
•	Set Aggregate. Most GNNs assume the neighbors of a node have no ordering, and the aggrega-
tions are associative and commutative operations that are invariant to the order in which the aggre-
gations are performed. Examples include GCN with summation aggregations and GraphSAGE-P
with element-wise pooling aggregations (Table 1). Note that set aggregations in GNNs are designed
to be order invariant and thus can be performed in a hierarchical fashion as we do in HAGs.
•	Sequential Aggregate. Another class of GNNs require a specific ordering of a node’s neighbors
and the aggregations are not commutative. Examples include N -ary Tree-LSTM (Tai et al., 2015)
and the LSTM variant of GraphSAGE (Hamilton et al., 2017). HAGs can be applied in the case of
sequential aggregations as well. Rather than identifying common subsets of neighbors, we identify
the common prefixes of the sequence of aggregated nodes, which can then be reused among nodes.
We further define two properties for the aggregation nodes VA . First, for each v ∈ VA, abv denotes
its intermediate aggregation result, and N(v) denotes the in-neighbors of node v. To capture the
aggregation hierarchy in a HAG, we use a recursive function to define abv .
bav = AGGREGATE	hu	u ∈ V u ∈ Nb(v)	(1)
bau	u ∈ VA
Second, Cb(v) denotes the set of input activations h(uk-1) used to compute abv in the recursive procedure.
bav = AGGREGATE({h(uk-1) |u ∈ Cb(v)})	(2)
Intuitively, C(v) defines the coverage of node v in a HAG. For the HAG example in Figure 1c,
Cb(A) = {B, C, D} because h(Bk-1), h(Ck-1), and h(Dk-1) are used as inputs to compute h(Ak).
For a set Aggregate, C(∙) is an unordered set:
Cb(v) =	[	(ub	u ∈ V	(3)
C(u) u ∈ VA
u∈Nb (v)
For a sequential Aggregate, C(∙) isan ordered list:
O, .	/ O, . O, . ∖	. 一
Cb(v) = Cb(u1), ...,Cb(um)	(4)
where u1, ..., um are the ordered in-neighbors of v.
Algorithm 2 A GNN abstraction with HAGs. We
exclude layer index superscripts in abv to denote that
bav does not need to be memoized for back prop-
agation, and its memory can be reused across all
layers.
1:	h(v0) = xv , ∀v ∈ V
2:	for k = 1 to K do
3:	for v ∈ VA do
4:	compute bav using Equation (1)
5:	for v ∈ V do
6:	aVk) — Aggregate({bu |u ∈NV })
7:	hVk) — UPDATE(aVk),hVkτ))
3.2	GNNS WITH HAGS
We extend the GNN abstraction to make it
also applicable to HAGs, as shown in Algo-
rithm 2. The extension does not require any
modification to a GNN model, and the only
difference is how to compute neighborhood
aggregations (i.e., a(vk)) in each GNN layer.
Before the original aggregations, we add one
step that precomputes commonly used inter-
mediate aggregation results to reduce redun-
dant computation. In Algorithm 2, we first
compute bav for all aggregation nodes VA (line
3-4). This is performed recursively follow-
ing the aggregation hierarchy of the HAG. We
then compute the neighborhood aggregations
a(vk) (line 5-6) using the precomputed intermediate aggregations bav .
Memory overhead. Although Algorithm 2 includes new intermediate variables bav , the memory
overhead for storing abv is negligible since abv is not used for back propagation and can be saved in
a constant memory across all GNN layers. In the experiments, we show that HAGs can increase
the training throughput by 2.8×, while maintaining the original model accuracy at the cost of 0.1%
memory overhead to save abv . Meanwhile, storing a HAG representation requires less memory than
the original GNN-graph, as HAGs generally contain much fewer edges.
4
Under review as a conference paper at ICLR 2020
Equivalence between GNN-graphs and HAGs. We define a GNN-graph and a HAG to be equiv-
alent if they produce the exact same outputs and gradients for a GNN model. Formally, a GNN-graph
G and a HAG G are equivalent for a GNN model if (1) the GNN model outputs the same activations
(i.e., h(vk)) at each GNN layer, and (2) the GNN model computes the same gradients for all trainable
parameters in back propagation. Equivalent graphs guarantee the same predictive performance, and
therefore can be used interchangeably for both GNN training and inference. Theorem 1 provides a
simple yet efficient condition to check graph equivalence. We prove the theorem in the Appendix.
Theorem 1. A GNN-graph with nodes V and a HAG with nodes (V, VA) are equivalent if and only
f N(V) = C(V) for all V ∈ V, where N(V) is Vs neighbors in the input graph and C(∙) is defined in
Equation 3 and 4.
4 HAG Search Algorithm
For a GNN model and an input GNN-graph, there exists a large space of equivalent HAGs with the
same model accuracy but various runtime performance. Our goal is to explore the search space to
discover a HAG with optimized runtime performance. First, we define a realistic cost function to
quantitatively evaluate the runtime performance of different HAGs. Second, we introduce an efficient
search algorithm that finds an optimized HAG with the following guarantees:
•	For GNNs with sequential AGGREGATE, the HAG search algorithm can find globally optimal
HAGs under the cost function.
•	For GNNs with set AGGREGATE, finding an optimal HAG is NP-hard by a reduction from the
NP-hard maximum coverage problem (see Appendix for the proof). The search algorithm finds at
least a (1 - 1/e)-approximation of globally optimal HAGs based on the submodularity property.
4.1	Cost Function
We introduce a cost function that estimates the runtime performance of a HAG by measuring the
computation cost to perform one epoch of GNN training on the HAG.
The computation cost of a GNN model includes aggregating the neighbors of each node by calling
Aggregate and updating the activations of each node via Update, as shown in Algorithm 2. For a
GNN model M, we assume the cost of performing AGGREGATE on two elements is αM, and the
cost of computing an UPDATE is βM. In Algorithm 2, computing bav with |Nbv | neighbors requires
performing (|Nv| - 1) binary aggregations, whose cost is αM × (|Nv| - 1). Therefore, the total
computation cost of training a GNN model M on a HAG G is
CoSt(M, G)	= X αM(INv | -I) + X βM = αM QEI-IVAD + (βM - αM)IVI
v∈V ∪VA	v∈V
Since M is determined by the input graph, our goal is to minimize (㈤-IVaI).
4.2	Search Algorithm
We present a HAG search algorithm that finds a globally optimal HAG for GNNs with sequential Ag-
GREGATE and a (1 - 1/e)-approximation of globally optimal HAGs for GNNs with set AGGREGATE.
In addition to an input GNN-graph and a GNN model, the algorithm also takes a hyper-parameter
capacity, defining an upper limit on the number of intermediate aggregation nodes (i.e., IVAI).
Algorithm 3 shows the pseudocode of the HAG search algorithm. We start with an input GNN-graph,
and iteratively insert aggregation nodes into the current HAG to merge highly redundant aggregations
and remove unnecessary computation and data transfers.
In each iteration, we find a binary aggregation with the highest redundancy and insert a new aggrega-
tion node w in VA to represent the binary aggregation results (line 12-15). All nodes containing this
binary aggregation can directly use the output of w without recomputing the aggregation (line 16-18).
The search algorithm iteratively reduces the computation cost of the HAG by eliminating the most
redundant aggregation in each iteration. The redundancy scores are maintained in a heap structure.
5
Under review as a conference paper at ICLR 2020
Algorithm 3 A HAG search algorithm to automatically find an equivalent HAG for a GNN-graph
with optimized runtime performance. E and VA are the set of edges and aggregation nodes in the
HAG. REDUNDANCY(v1, v2, E) calculates the number of nodes aggregating both v1 and v2. Recall
that C(u) is an ordered list for sequential AGGREGATE (see Equation 4).
1:	Input: A GNN-graph G and a GNN model M.
2:	Output: An equivalent HAG with optimized performance
3:	function REDUNDANCY(v1 , v2 , Eb)
4:	if M has a set AGGREGATE then
5:	R = {u|(v1, u) ∈ E ∧ (v2, u) ∈ E}
6:	else
7:	R = {u|v1 = Cb(u)[1] ∧ v2 = Cb(u)[2]}
8:	return |R|
9:
10:	VA - 0, Eb JE
11:	while |VA | < capacity do
12:	(v1,v2) = arg maxv1,v2 REDUNDANCY(v1, v2, E)
13:	if REDUNDANCY(v1 , v2, Eb) > 1 then
14:	VA - VA + {w}	. where w is a new node
-一	^	^ 、	， 、
15:	E - E + (v1, w) + (v2, w)
16:	for u ∈ V do
.—	._ ,	O	O .
17:	if (v1 , u) ∈ Eb ∧ (v2 , u) ∈ Eb then
18:	E - E - (v1, u) - (v2, u) + (w, u)
19:	return (VA ∪ V , Eb)
For a GNN model with a sequential Aggregate, Theorem 2 shows that our search algorithm finds
an equivalent HAG with globally optimal computation cost. We prove the theorem in Appendix.
Theorem 2.	For any GNN-graph G = (V, E) and any GNN model M with a sequential AGGREGATE,
Algorithm 3 returns an equivalent HAG with globally minimum cost as long as capacity ≥ |E |.
For a GNN model with a set Aggregate, Theorem 3 shows that our search algorithm finds a HAG
that is at least a (1 - 1/e)-approximation of the globally optimal HAGs (see proof in Appendix).
Theorem 3.	For any GNN-graph G and GNN model M with a set AGGREGATE, Algorithm 3 gives
^
a (1 - 1/e)-approximation of globally optimal HAGs under the cost function. Formally, let G be the
HAG returned by Algorithm
3, and Gbo is
a globally optimal HAG under the capacity constraint,
cost(M, G) ≤ Ccost(M, G) + e——-cost(M, Go)
ee
Time and space complexity. Our HAG algorithm achieves low theoretical complexity and has
negligible runtime overhead. In particular, the overall time complexity of Algorithm 3 is O(capacity ×
|V| + |E| × log |V|), and the space complexity is O(capaccity × |V| + |E|) (see Appendix for the
proof). One key optimization is a heap data structure for maintaining the redundancy scores of the
highest O(|V |) node pairs. Finding the most redundant node pair over the entire graph thus only takes
O(1) time, and updating the redundancy scores (i.e., line 15 and 18) each takes O(log |V |) time.
5 Experiments
The HAG representation maintains the predictive performance of GNNs but has much better run-
time performance. This section evaluates the runtime performance of HAGs on five real-world
graph datasets. We evaluate HAGs along three dimensions: (a) end-to-end training and inference
performance; (b) number of aggregations; and (c) size of data transfers.
5.1	Implementation
Existing deep learning frameworks such as TensorFlow and PyTorch are designed for spatial data
structures (e.g., images and text), and have limited support for irregular data structures such as graphs.
6
Under review as a conference paper at ICLR 2020
As a result, GNN models in existing frameworks translate graph structures to sparse adjacent matrices
and use matrix operations to perform GNN training.
We implemented the following operations in TensorFlow r1.14 to support GNN training with HAGs.
First, graph_to_hag automatically transforms an input GNN-graph to an equivalent HAG with
optimized performance. Second, hag_aggregate takes a HAG and nodes' activations as inputs,
and computes the aggregated activations of all nodes. Finally, hag_aggregate_grad computes
the gradients of hag_aggregate for back propagation. Our implementation minimizes changes
to existing GNN programs: a GNN application can directly use all HAG optimizations by only
modifying a few lines of code.
5.2	Experimental Setup
Datasets. Table 2 summarizes the public
datasets used in our experiments. BZR is a
chemical compound dataset, where each node
is an atom and an edge is a chemical bond be-
tween two atoms (Kriege and Mutzel, 2012).
PPI contains a number of protein-protein inter-
action graphs, each of which corresponds to a
different human tissue (Zitnik and Leskovec,
2017). REDDIT is an online discussion forum
dataset, with each node being a Reddit post and
each edge being commenting relations. For both
Table 2: Datasets used in the experiments.
Name	I # Nodes J # Edges		
Node Classification			
BZR PPI REDDIT		6,519 56,944 232,965	137,734 1,612,348 114,615,892
Graph Classification			
IMDB COLLAB		19,502 372,474	197,806 12,288,900
PPI and REDDIT, we directly use prepossessed data from Hamilton et al. (2017). IMDB and COL-
LAB are two collaboration datasets for graph classification (Yanardag and Vishwanathan, 2015).
IMDB is a movie collaboration dataset, with each node representing an actor/actress, while COLLAB
is a scientific collaboration dataset, with each node representing a researcher.
All experiments were performed running TensorFlow r1.14 on NVIDIA Tesla V100 GPUs. Following
previous work (Kipf and Welling, 2016; Hamilton et al., 2017), each GNN model has two GNN
layers and one SoftMax layer. For graph classification datasets, each GNN model also includes
a mean-pooling layer to gather graph-level activations. For all experiments, we set the maximum
capacity of |VA| in a HAG to be |V |/4, which achieves high performance on real-world graphs.
5.3 End-to-End Performance
We first measure the per-epoch training time and
inference latency to run a 2-layer GCN model
on different graph datasets. We follow previous
work (Hamilton et al., 2017; Kriege and Mutzel,
2012; Yanardag and Vishwanathan, 2015) to
split the datasets into training/validation/testing
sets, and use the testing sets to measure the av-
erage inference latency.
Figure 2 compares the per-epoch training time
and inference latency between GNN-graphs and
HAGs. Compared to GNN-graphs, HAGs can
improve the training and inference performance
by up to 2.8× and 2.9×, respectively, while
maintaining the same network accuracy. We
note this improvement is achieved completely
automatically, and computing a HAG is inexpen-
sive. Thus, because the improvement is essen-
tially for free, we believe there is no reason not
to use HAGs in preference to GNN-graphs.
sdnpoods
nE≡-Ek
sdnpoods
ouuaJ°JU-
GNN-graph HAGI
O
5
O
5
PPI
REDDIT IMDB COLLAB Mean
Figure 2: End-to-end performance comparison be-
tween GNN-graphs and HAGs. We measure the
per-epoch training time and inference latency on a
2-layer GCN model with 16 hidden dimensions in
each layer. The performance numbers are normal-
ized by the GNN-graph numbers.
7
Under review as a conference paper at ICLR 2020
1.2r
1.0
0.8
I GNN-graph HAGI
1.2r
.2 i o
GNN-graph	HAG
ta 2。6
At。，
0.2
0.0
Li i i j 1
BZR
PPI REDDΓΓ IMDB COLLAB Mean
3 tθ 0.8-
≡s-
> 60.4
4 O)
to < 0.2
/。。
(a) Set Aggregations.	(b) Sequential Aggregations.
Figure 3: Comparing the number of aggregations and amount of data transfers between GPU threads
to perform aggregations (lower is better). The y-axes are normalized by GNN-graphs, and the last
column in each figure is the geometry mean over all datasets.
5.4	Aggregation Performance
We further compare the aggregation performance of GNN-graphs and HAGs on the following two
metrics: (1) the number of binary aggregations performed in each GNN layer; and (2) the size of
data transfers between GPU threads to perform the aggregations. Note that aggregating a neighbor’s
activations requires transferring the activations from GPU global memory to a thread’s local memory.
Figure 3 shows the comparison results. For GNNs with set aggregations, HAGs reduce the number
of aggregations by 1.5-6.3× and the size of data transfers by 1.3-5.6×. For GNNs with sequential
aggregations, HAGs reduce aggregations and data transfers by up to 1.8× and 1.9×, respectively.
Although the search algorithm finds a globally optimal HAG for sequential aggregations (Theorem 2)
and a (1 - 1/e)-approximation of globally optimal HAGs for set aggregations (Theorem 3), we
observe the performance improvement is more significant for set aggregations. Optimality for HAGs
with set aggregation involves more potential redundancy compared to sequential aggregations, due to
permutation invariance of set aggregation. Thus higher performance can be achieved with HAGs for
set aggregations, though optimal solutions are more difficult to compute.
It is also worth noting that the HAG search algorithm can find highly optimized HAGs even on very
sparse graphs. For example, on the COLLAB dataset with a graph density of 0.01%, our algorithm
reduces the number of aggregations and data transfers by 3.3× and 2.2×, respectively.
5.5	HAG Search Algorithm
We evaluate the performance of the HAG search algorithm.
Recall that the search algorithm uses a hyper-parameter
capacity to control the number of aggregation nodes in a
HAG. A larger capacity allows the algorithm to eliminate
more redundant aggregations and achieves lower cost.
Figure 4 shows the end-to-end GCN training time on the
COLLAB dataset using HAGs with different capacities. A
larger value of capacity can consistently improve the train-
ing performance, which indicates that the cost function
is an appropriate metric to evaluate and compare the per-
formance of different HAGs. By gradually increasing the
capacity, the search algorithm eventually finds a HAG with
〜100K aggregation nodes, which consume 6MB of mem-
ory (0.1% memory overhead) while improving the training
performance by 2.8×. In addition, the HAG search time
is negligible compared to the end-to-end training time.
Figure 4: End-to-end GCN training time
on the COLLAB dataset using HAGs
with different capacities. We train GCN
for a maximum of 350 epochs by follow-
ing prior work (Kipf and Welling, 2016).
Limitation. One limitation of the HAG graph representation is that it does not apply to attention-
based GNN models, such as GAT (Velickovic et al., 2017), which uses the attention scores (Vaswani
et al., 2017) between nodes to guide activation updates and results in limited redundancy that can be
optimized by HAG.
8
Under review as a conference paper at ICLR 2020
Conclusion. We introduce HAG, a new graph representation to eliminate redundancy in GNNs.
We propose a cost function to estimate the performance of different HAGs and use a search algorithm
to find optimized HAGs. We show that HAGs outperform existing GNN-graphs by improving the
end-to-end training performance and reducing the aggregations and data transfers in GNN training.
References
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems 30. PMLR, 2017.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Advances in Neural
Information Processing Systems, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019.
Felix Wu, Tianyi Zhang, Amauri H. Souza Jr., Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. CoRR, abs/1902.07153, 2019. URL http://arxiv.
org/abs/1902.07153.
Elchanan Mossel and Sebastien Roch. On the submodularity of influence in social networks. In
Proceedings ofthe thirty-ninth annual ACM symposium on Theory ofcomputing, pages 128-134.
ACM, 2007.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. CoRR, abs/1801.10247, 2018. URL http://arxiv.org/abs/1801.
10247.
Jorg Flum, Markus Frick, and Martin Grohe. Query evaluation via tree-decompositions. J. ACM,
2002.
Stefan Arnborg, Derek G. Corneil, and Andrzej Proskurowski. Complexity of finding embeddings in
a k-tree. SIAM J. Algebraic Discrete Methods, 1987.
Aaron B Adcock, Blair D Sullivan, and Michael W Mahoney. Tree decompositions and social graphs.
Internet Mathematics, 12(5), 2016.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections
for efficient neural networks. In Proceedings of the 28th International Conference on Neural
Information Processing Systems, NIPS, 2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. CoRR, 2016.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations
from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Nils Kriege and Petra Mutzel. Subgraph matching kernels for attributed graphs. arXiv preprint
arXiv:1206.6483, 2012.
Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. CoRR, abs/1707.04638, 2017.
Pinar Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, 2015.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
9
Under review as a conference paper at ICLR 2020
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing Systems, pages 5998-6008, 2017.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to
Algorithms, Third Edition. The MIT Press, 3rd edition, 2009.
A	Proof of Theorem 1
Proof. It is sufficient to prove that if N (v) = C(v) for all v ∈ V, then the GNN-graph G and the
(k)
HAG G generate the same outputs (i.e., hv ) for every GNN layer.
We prove this by induction. Assume a GNN-graph G and a HAG G generate the same outputs for the
(k-1)-th layer, we prove the two graphs produce the same outputs for the k-th GNN layer.
In Algorithm 2, bav is the aggregation results of node v, which is defined as
bav = AGGREGATE(h(uk-1) |u ∈ Cb(v))
= AGGREGATE(h(uk-1) |u ∈ N(v))
This proves that Algorithm 1 and Algorithm 2 compute the same a(vk). In addition, both algorithms
use the same UPDATE function that takes a(vk) and h(vk-1) as inputs and computes h(vk), which applies
that the two algorithms compute the same hVk).	□
B Proof of Theorem 2
Proof. Sequential aggregations require a specific ordering of a node’s neighbors. Let Nv denote the
ordered list of node v’s neighbors and L(vi) denote a list of the first i elements in Nv :
Lvi) = (Nv ⑴,Nv (2),…,Nv (i))
where Nv(i) is the i-th neighbor of node v.
L(vi) represents a necessary intermediate aggregation step for computing a(vk) (since sequential
aggregations are not commutative), and therefore any HAG must compute L(vi) as an intermediate
aggregation. Counting the number of distinct L(vi) (where v ∈ V and 1 < i ≤ |Nv |) provides a lower
bound on the number of aggregations any equivalent HAG must perform. Assuming Go is a globally
optimal HAG under the cost model, we have:
cost(M, Go) ≥ αM X lb + (βM - αM)∣V∣
where lb is the number of distinct L(vi) that must be computed by any equivalent HAG.
Assuming G is the output HAG of Algorithm 3, we prove that cost(M, G) = cost(M, Go) by using
contradiction. In the case cost(M, G) > cost(M, Go), G must perform more than lb aggregations.
Case 1.	One possible case is that G computes at least one aggregation that is not a prefix of any
Nv, indicating that G performs some useless aggregations, which contradicts with the fact that all
intermediate aggregations added to G must be used at least once.
Case 2.	The other possible case is that Gb computes the aggregation of some L(vi) multiple times.
However, in Algorithm 3, each iteration reduces the number of aggregations by at least 1, and there
are |E | aggregations initially. This implies there cannot be redundant aggregations after |E | iterations,
which contradicts with the precondition of Case 2.	□
C Proof of Theorem 3
Proof. The idea of the proof is to build a monotone submodular function Cormen et al. (2009) based
on the cost model.
10
Under review as a conference paper at ICLR 2020
For any GNN-graph G and an equivalent G, we define
f (Gb) = cost(M, G) - cost(M, Gb)	(5)
，∙ .^.
=αM(∣E∣-∣E∣ + |Va|)	(6)
where VA is the set of aggregation nodes in G, and E and E are the set of edges in G and G, respectively.
f(G) measures the number of aggregations that can be saved by using G for GNN training.
We begin by defining the subset relations between different HAGs. For two HAGs G and G0, we
define G ⊆ G0 iff VA is a subset of VA0 , where VA and VA0 are the aggregation nodes in G and G0 ,
respectively.
^
^ ^ . , ^,

Prove that f(G) is monotone. We show that for all G ⊆ G0, f(G) ≤ f(G0). This is true since
G ⊆ G0 indicates that G0 contains all aggregation nodes in G, which applies that G0 can at least save
the same number of aggregations as G .
G	八，”分、.	,. ʊɪ . J . C 八分一分，	1	..	1	"分,
Prove that f(G) is submodular. We show that for all G ⊆ G0 and any aggregation node n, f(G +
{n}) - f (Gb) ≥ f (Gb0 + {n}) - f(Gb0). This inequality holds because f(Gb+ {n}) - f (Gb) measures
the number of aggregations we can further save by adding aggregation n to the existing HAG, which
monotonically decreases as we add more aggregation nodes to the HAG.
Let Gb(i) denote the result HAG after the i-th iteration of Algorithm 3. Gb(i) includes exactly i
aggregation nodes. Let Gbo denote the optimal HAG under the cost model with k aggregation nodes.
We claim via induction that for 0 ≤ i ≤ k,
f(Gbo)-f(Gb(i)) ≤ (1 - 1/k)if (Gbo)
(7)
The base case is trivially true. In the i-th step, Algorithm 3 selects an aggregation node ai by
maximizing the marginal gain f (Gb(i) + ai) - f (Gb(i)). Observe that the remaining aggregation nodes
includes Gbo \ Gb(i-1), a set of at most k elements. The submodularity applies that
f (Go)- f (Gb(i-1)) ≤ X	f(G⑴ + a) - f (b⑺)
a∈bo∖b(i-1)
and this implies that the aggregation node ai has marginal value
f(Gb(i-1) + ai) - f(Gb(i-1))
≥ rχ⅛] X(f©i)+a)-f0>)
IGo \G | a∈bο∖b(i-1)
≥ 1(f (bo) - f (b(iτ)))
Assuming that Inequality 7 holds for Gb(i-1), we have
f(Go)- f(G(i)) = f(Go)- f(G(i-1))-(f (b(i)) - f(G(i-1)))
≤ f (Go) - f (G(i-1) - 1(f(Go) - f(G(j)))
k
= (1 - 1/k)(f (Gbo) - f (Gb(i-1)))
≤	(1 - 1/k)if(Gbo)
which proves Inequality 7. Therefore, we have
f(Gbo) - f(Gb(k)) ≤ (1 - 1/k)kf(Gbo) ≤ e-1f(Gbo)
By taking in the definition of f (∙), we have
cost(M, G) ≤ Ccost(M, G) + e——-cost(M, Go)
ee
□
11
Under review as a conference paper at ICLR 2020
wdnpφφds
0UC0k0‰C-
GNN-graph	HAG
wdnpφφdswdnpφφds
nc»c»EF 0UC0k0^C-
BZR
PPI REDDIT IMDB COLLAB Mean
(a) GIN.	(b) SGC.
Figure 5: End-to-end performance comparison between GNN-graphs and HAGs. We measure the
per-epoch training time and inference latency on a 2-layer GIN and a 2-layer SGC model. Both
models have 16 hidden dimensions in each layer. The performance numbers are normalized by the
GNN-graph numbers (higher is better).
D Complexity of the HAG Search Algorithm
Theorem 4. The overall time complexity of Algorithm 3 is O(capacity × |V| + |E| × log |V |), where
capacity is the upper bound on the number of aggregation nodes in a HAG.
Proof. We use a heap to maintain the redundancy score of each potential node pair and only update
the heap when we add and remove edges in E. Since the depth of the heap is at most O(log |V|) 1,
querying the most redundant binary aggregation and modifying E each takes O(log |V|) time.
First, we calculate the number of queries and updates to the heap structure:
•	The algorithm iteratively pull the most redundant binary aggregation from the heap and add
it to VA . Since the number of vertices in VA is smaller than capacity, the total number of
queries is O(capacity).
•	The algorithm inserts two new edges into E in line 16 and removes one edge from E in
line 19. Since line 16 can be invoked at most O(capacity) times, the total number of
invocations to line 19 is O(|E| + 2 × capacity). Therefore, the overall number of updates is
O(|E | + capacity).
Second, the enumeration over all vertices in V (line 17) involves time complexity of O(capacity × |V|).
Therefore, the overall time complexity of Algorithm 3 is
O capacity × |V| + (|E| + capacity) × log |V|
= O(capacity × |V| + |E| × log |V|)
□
E Performance Evaluation
We further evaluate the effectiveness of the HAG optimizations on two additional GNN models:
GIN Xu et al. (2019) and SGC Wu et al. (2019). The experimental setup is described in Section 5.2.
We measure the per-epoch training time and inference latency for GIN and SGC. We follow previous
work Xu et al. (2019); Wu et al. (2019) to split the datasets into training/validation/testing sets, and
use the testing sets to measure the average inference latency.
Figure 5 compares the performance between GNN-graphs and HAGs. Compared to GNN-graphs,
HAGs improve the training performance by up to 2.6× and 3.1× on GIN and SGC, while maintaining
the same network accuracy. This shows that the HAG optimization is general and applicable to
various GNN models.
1This is because there can be at most O(|V |2) node pairs.
12
Under review as a conference paper at ICLR 2020
0.75
0
5
9
O
9 5 8
080
O
Aue,mu4 ls
10	20	30	40
Time (minutes)
Figure 6: Time-to-accuracy comparison between HAG and GNN-graph for training a 2-layer GCN
model on the Reddit dataset.
F	Time-to-accuracy Evaluation
We compare the time-to-accuracy performance between HAG and GNN-graph. We train a 2-layer
GCN model (with 64 hidden dimensions in each layer) on the Reddit dataset until the test accuracy
exceeds 95%. We follow previous work Kipf and Welling (2016) to set all hyper-parameters and split
the dataset.
Figure 6 shows the results. Each dot indicates the training time and test accuracy of each epoch.
Training the GCN model using the HAG representation achieves the same training and test accuracy
at the end of each epoch. This is because the HAG optimization maintains the same results as the
original GNN-graph representation and therefore preserves the model accuracy. It takes 55 training
epochs to achieve a test accuracy of 95% for both HAG and GNN-graph, and HAG improves the
end-to-end training time by 1.8×.
13