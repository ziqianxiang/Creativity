Under review as a conference paper at ICLR 2020
Global Concavity and Optimization in a Class
of Dynamic Discrete Choice Models
Anonymous authors
Paper under double-blind review
Ab stract
Discrete choice models with unobserved heterogeneity are commonly used Econo-
metric models for dynamic Economic behavior which have been adopted in practice
to predict behavior of individuals and firms from schooling and job choices to
strategic decisions in market competition. These models feature optimizing agents
who choose among a finite set of options in a sequence of periods and receive
choice-specific payoffs that depend on both variables that are observed by the agent
and recorded in the data and variables that are only observed by the agent but
not recorded in the data. Existing work in Econometrics assumes that optimizing
agents are fully rational and requires finding a functional fixed point to find the
optimal policy. We show that in an important class of discrete choice models the
value function is globally concave in the policy. That means that simple algorithms
that do not require fixed point computation, such as the policy gradient algorithm,
globally converge to the optimal policy. This finding can both be used to relax be-
havioral assumption regarding the optimizing agents and to facilitate Econometric
analysis of dynamic behavior. In particular, we demonstrate significant computa-
tional advantages in using a simple implementation policy gradient algorithm over
existing “nested fixed point” algorithms used in Econometrics.
1	Introduction
Dynamic discrete choice model with unobserved heterogeneity is, arguably, the most popular model
that is currently used for Econometric analysis of dynamic behavior of individuals and firms in
Economics and Marketing (e.g. see surveys in Eckstein and WolPin (1989), Dube et al. (2002) Abbring
and Heckman (2007), Aguirregabiria and Mira (2010)). Even most recent Econometric papers on
single-agent dynamic decision-making use this setuP to showcase their results (e.g. Arcidiacono and
Miller, 2011; Aguirregabiria and Magesan, 2016; Muller and Reich, 2018).In this model, pioneered
in Rust (1987), the agent chooses between a discrete set of oPtions (tyPically 2) in a sequence of
discrete time periods to maximize the expected cumulative discounted payoff. The reward in each
period is a function of the state variable which follows a Markov process and is observed in the data
and also a function of an idiosyncratic random variable that is only observed by the agent but is not
reported in the data. The unobserved idiosyncratic component is designed to reflect heterogeneity of
agents that may value the same choice differently.
Despite significant empirical success in prediction of dynamic economic behavior under uncertainty,
dynamic discrete choice models frequently lead to seemingly unrealistic optimization problems that
economic agents need to solve. For instance, Hendel and Nevo (2006) features an elaborate functional
fixed point problem with constraints, which is computationally intensive, especially in continuous
state spaces, for consumers to buy laundry detergent in the supermarket. Common approach for this
functional fixed point problem is value function iteration (See Section 2.3 for more discussion).
At the same time, rich literature on Markov Decision Processes (cf. Sutton and Barto, 2018) have
developed several effective optimization algorithms, such as the policy gradient algorithm and its
variants, that do not require solving for a functional fixed point. However, the drawback of the policy
gradient is that the value function in a generic Markov Decision problem is not concave in the policy.
This means that gradient-based algorithms have no guarantees for global convergence for a generic
MDP. While for some specific and simple models where closed-form characterizations exist, the
1
Under review as a conference paper at ICLR 2020
convergence results are shown by model-specific technique which is hard to generalize (e.g. Fazel
et al., 2018, for linear quadratic regulator).
In this paper our main goal is to resolve the dichotomy in empirical social science literature that
the rationality of consumers requires for them to be able to solve the functional fixed point problem
which is computationally intensive. Our main theoretic contribution is the proof that, in the class of
dynamic discrete choice models with unobserved heterogeneity, the value function of the optimizing
agent is globally concave in the policy. This implies that a large set of policy gradient algorithms that
have a modest computational power requirement for the optimizing agents have a fast convergence
guarantee in our considered class of dynamic discrete choice models. The importance of this result is
twofold.
First, it gives a promise that seemingly complicated dynamic optimization problems faced by con-
sumers can be solved by relatively simple algorithms that do not require fixed point computation
or functional optimization. This means that the policy gradient-style methods have an important
behavioral interpretation. As a result, consumer behavior following policy gradient can serve as
a behavioral assumption for estimating consumer preferences from data which is more natural for
consumer choice settings than other assumptions that have been used in the past for estimation of
preferences (e.g. -regret learning in Nekipelov et al. (2015)). Second, more importantly, our result
showing fast convergence of the policy gradient algorithm makes it an attractive alternative to the
search for the functional fixed point in this class of problems. While the goal of the Econometric
analysis of the data from dynamically optimizing consumers is to estimate consumer preferences
by maximizing the likelihood function, it requires to sequentially solve the dynamic optimization
problem for each value of utility parameters along the parameter search path. Existing work in
Economics prescribes to use fixed point iterations for the value function to solve the dynamic opti-
mization problem (see Rust (1987), Aguirregabiria and Mira (2007)). The replacement of the fixed
point iterations with the policy gradient method significantly speeds up the maximization of the
likelihood function. This makes the policy gradient algorithm our recommended approach for use
in Econometric analysis, and establishes practical relevance of many newer reinforcement learning
algorithms from behavioral perspective for social sciences.
2	Preliminaries
In this section, we introduce the concepts of the Markov decision process (MDP) with choice-specific
payoff heterogeneity, the conditional choice probability (CCP) representation and the policy gradient
algorithm.
2.1	Markov decision process
A discrete-time Markov decision process (MDP) with choice-specific heterogeneity is defined as a
5-tuple (S, A, r, e, P, β), where S is compact convex state space with diam(S) ≤ S < ∞, A is the
set of actions, r : S × A → R+ is the reward function, such that r(s, a) is the immediate non-negative
reward for the state-action pair (s, a), are independent random variables, P is a Markov transition
model where where p(s0 |s, a) defines the transition density between state s and s0 under action a, and
β ∈ [0, 1) is the discount factor for future payoff. We assume that random variables are observed
by the optimizing agent and not recorded in the data. These variables reflect idiosyncratic differences
in preferences of different optimizing agents over choices. In the following discussion we refer to
these variables as “random choice-specific shocks."
In each period t = 1, 2, . . . , ∞, the nature realizes the current state st based on the Markov transition
P given the state-action pair (st-1, at-1) in the previous period t - 1, and the choice-specific shocks
t = {t,a }a∈A drawn i.i.d. from distribution . The optimizing agent chooses an action a ∈ A, and
her current period payoff is sum of the immediate reward and the choice-specific shock, i.e., r(s, a) +
t,a. Given initial state s1, the agent’s long-term payoff is E1,s2,2,... Pt∞=1 βt-1r(st, at) + t,at .
This expression makes it clear that random shocks play a crucial role in this model by allowing
us to define the ex ante value function of the optimizing agent which reflects the expected reward
from agent’s choices before the agent observes realization of t . When the distribution of shocks is
sufficiently smooth (differentiable), the corresponding ex ante value function is smooth (differentiable)
2
Under review as a conference paper at ICLR 2020
as well. This allows us to characterize the impact of agent’s policy on the expected value by
considering functional derivatives of the value function with respect to the policy.
In the remainder of the paper, we rely on the following assumptions.
Assumption 2.1. The state space S is compact in R and the action space A is binary, i.e., A = {0, 1}.
Assumption 2.2. For all states s, the immediate reward r(s, 0) for the state-action pair (s, 0) is zero
i.e., r(s, 0) = 0, and the immediate reward r(s, 1) for the state-action pair (s, 1) is bounded between
[Rmin , Rmax].
Assumption 2.3. Choice-specific shocks are Type I Extreme Value random variables with location
parameter 0 (cf. Hotz and Miller, 1993) which are independent over choices and time periods.
Assumption 2.1, 2.2, 2.3 are present in most of the papers on dynamic decision-making in economics,
marketing and finance, (e.g. DUbe et al., 2002; Aguirregabiria and Mira, 2010; Arcidiacono and
Miller, 2011; Aguirregabiria and Magesan, 2016; Muller and Reich, 2018)
The policy and the value function A stationary Markov policy is a function σ : S × RA → A
which maps the current state s and choice-specific shock to an action. In our further discussion we
will show that there is a natural more restricted definition of the set of all feasible policies in this
model.
Given any stationary Markov policy σ, the value function Vσ : S → R is a mapping from the initial
state to the long-term payoff under policy σ, i.e.,
∞
Vσ (SI)= Ee1,s2 4,…^etT {r(St,σ(St, Q)) + Q,σ(st,et)}.
Since the reward is non-negative and bounded, and the discount β ∈ [0, 1), value function Vσ is
well-defined and the optimal policy σ (i.e., Vσ(s) ≥ Vσ(S) for all policies σ and states s) exists.
Furthermore, the following Bellman equation holds
Vσ(s) = Ee [r(s,σ(s,e)) + 5s,e) + βEs，[K(s0)∣s, σ(s, e)]]	for all policies σ (1)
2.2	Conditional choice probability representation
Based on the Bellman equation (1) evaluated at the optimal policy, the optimal Conditional Choice
Probability δ(a∣s) (i.e., the probability of choosing action a given state S in the optimal policy σ) can
be defined as
6(a∣S) = Ee[1 {r(S, a) + Ea + βEs0 [Vσ(S )|s, a] ≥ r(S, a ) + 3, + βEs，[彩(S )|s, a ], ∀a }]
The optimal policy σ can, therefore, be equivalently characterized by threshold function Π(s, a)=
r(s, a) + βEs，[Vσ(s0)|s, a], such that the optimizing agent chooses action a* which maximizes the
sum of the threshold and the choice-specific shock, i.e., a* = argmaXɑ{Π(S, a) + Ea}. Similarly,
all non-optimal policies can be characterized by the corresponding threshold functions denoted π .
Under Assumption 2.3 the conditional choice probability δ can be explicitly expressed in terms of the
respective threshold π as (cf. Rust, 1996)
δ(a∣S) = exp(∏(S, a))/ (Pa,∈A exp(∏(S, a0))).
We note that this expression induces a one-to-one mapping from the thresholds to the conditional
choice probabilities. Therefore, all policies are fully characterized by their respective conditional
choice probabilities. For notational simplicity, since we consider the binary action space A = {0, 1},
and the reward r(S, 0) is normalized to 0 we denote the immediate reward r(S, 1) as r(S); denote the
conditional choice probability δ(0∣S) as 6(s); and denote ∏(s, 1) as ∏(s).
In the subsequent discussion given that the characterization of policy σ via its threshold is equivalent
to its characterization by conditional choice probability δ, we interchangeably refer to δ as the
“policy." Then we rewrite the Bellman equation for a given policy δ as
Vδ(S) =(1 - δ(S)) r(S) - δ(S) log (δ(S))
-(1 - δ(S))log(1 - δ(S)) + β Ee,s0 [vδ (S)(S0)H
(2)
3
Under review as a conference paper at ICLR 2020
Now we make two additional assumptions that are compatible with standard assumptions in the
Econometrics literature.
Assumption 2.4. For all states s ∈ S, the conditional distribution of the next period Markov
state p(∙∣s, 1) first-order stochastically dominates distribution p(∙∣s,0), i.e., for all s ∈ S,
Prso[s0≤ S∣s,1] ≤ P*[s0 ≤ ^∣s, 0].
Assumption 2.5. Under the optimal policy δ, the value function is non-decreasing in states, i.e.,
V¾(s) ≤ %(s0) for all s, s0 ∈ S s.t. S < s0.
Consider a myopic policy S(S) = (exp(r(s)) + 1)-1 which uses threshold π(s) = r(s). This policy
corresponds to agent optimizing the immediate reward without considering how current actions
impact future rewards. Under Assumption 2.4 and Assumption 2.5, the threshold for optimal policy
is at least the threshold of myopic policy, i.e., ∏(s) ≥ ∏(s). Hence, Lemma 2.1 holds.
Lemma 2.1. The optimal policy δ chooses action 0 with weakly lower probability than the myopic
policy δ in all states s ∈ S, i.e., δ(s) ≤ δ(s).
2.3 MDP in Economics and policy gradient
Our motivation in this paper comes from empirical work in Economics and Marketing where
optimizing agents are consumers or small firms who make dynamic decisions while observing
the current state s and the reward r(s, a) for their choice a. These agents often have limited
computational power making it difficult for them to solve the Bellman equation to find the optimal
policy. They also may have only sample access to the distribution of Markov transition which further
complicates the computation of the optimal policy. In this context we contrast the value function
iteration method which is based on solving the fixed point problem induced by the Bellman equation
and the policy gradient method.
Value function iteration In the value function iterations, e.g., discussed in Jaksch et al. (2010);
Haskell et al. (2016), the exact expectation in the Bellman equation (1) is replaced by an empirical
estimate and then functional iteration uses the empirical Bellman equation to find the fixed point, i.e.,
the optimal policy. Under certain assumptions on MDPs, one can establish convergence guarantees
for the value function iterations, e.g., Jaksch et al. (2010); Haskell et al. (2016). However, to run these
iterations may require significant computation power which may not be practical when optimizing
agents are consumers or small firms.
Policy gradient In contrast to value function iterations, policy gradient algorithm and its variations
are model-free sample-based methods. At a high level, policy gradient parametrizes policies {δθ}θ∈Θ
by θ ∈ Θ and computes the gradient of the value function with respect to the current policy δθ and
update the policy in the direction of the gradient, i.e., θ J θ + α Vθ 晞.Though the individuals
considered in the Economic MDP models may not compute the exact gradient with respect to a policy
due to having only sample access to the Markov transition, previous work has provided approaches to
produce an unbiased estimator of the gradient. For example, REINFORCE (Williams, 1992) updates
the policy by θ J θ + αR Vθ log(δθ(a∣s)) where R is the long-term payoff on path. Notice that this
updating rule is simple comparing with value function iteration. The caveat of the policy gradient
approach is the lack of its global convergence guarantee for a generic MDP. In this paper we show
that such guarantee can be provided for the specific class of MDPs that we consider.
3	Warm-up: local concavity of the value function at the optimal
POLICY
To understand the convergence of the policy gradient, in this section we introduce our main technique
and show that the concavity of the value function with respect to policies is satisfied in a fixed
neighborhood around the optimal policy. We rely on the special structure of the value function
induced by random shocks which essentially “smooth it" making it differentiable. We then use
Bellman equation (7) to compute strong FreChet functional derivatives of the value functions and
argue that the respective second derivative is negative at the optimal policy. We use this approach in
Section 4 to show the global concavity of the value function with respect to policies.
4
Under review as a conference paper at ICLR 2020
By ∆ we denote the convex compact set that contains all continuous functions δ : S → [0, 1] such that
0 ≤ δ(∙) ≤ S(∙). The Bellman equation (7) defines the functional Vδ(∙). Recall that Frechet derivative
of functional Vδ (∙), which maps bounded linear space ∆ into the space of all continuous bounded
functions of SLat a given δ(∙) is a bounded linear functional DVδ(∙) such that for all continuous h(∙)
with ∣∣hk2 ≤ H: Vδ+h(∙) - Vδ(∙) = DVδ(∙) h(∙) + 0(∣∣h∣∣2). When functional DVδ(∙) is also Frechet
differentiable, we refer to its FreChet derivative as the second FreChet derivative of functional Vδ (∙)
and denote it D2Vδ (∙).
Theorem 3.1. Value function Vδ is twice Freechet differentiable with respect to δ at the choice
probability δ corresponding to optimal policy and its Frechet derivative is negative at δ in all states s,
i.e., D2Vδ(s) ≤ 0.
We sketch the proof idea of Theorem 3.1 and defer its formal proof to Appendix A. Start with the
Bellman equation (7) of the value function, the Frechet derivative of the value function is the fixed
point of the following Bellman equation
DVδ(s) = (log(1 - δ(s)) - log(δ(s)) - r(s))	(3)
+ β (Es,[Vδ(s0)∣s, 0] - Es, [Vδ(s0)∣s, 1]) + βEeH [DVδ(s0)∣s],	()
and
D2Vδ(S) = -λ∕ V1 ɪ λ/ ʌʌ
δ(s)(1 - δ(s))	(4)
-2β(Es0[D⅛(s0)∣s, 1] - Es,[DVδ(s0)∣s, 0]) + βEs，[D2Vδ(s0)|s].
A necessary condition for its optimum yielding δ is DV⅛(s) = 0 for all states s. As a result, equation
(9) implies that its second FreChet derivative is negative for all states, i.e.,D2Vδ(s) ≤ 0.
The Bellman equation (9) of the second Frechet derivative suggests that D2Vδ (S) ≤ 0 for all states S
if
IV、、+2β(Es, [DVδ (s0)∣s, 1] - Es，[D⅛ (s0)∣s, 0]) ≥ 0	(5)
δ(S)(1 - δ(S))
The first term in the inequality (5) is always positive for all policies in ∆, but the second term can
be arbitrary small. In the next section, we will introduce a nature smoothness assumption on MDP
(i.e., Lipschitz MDP) and show that the local concavity can be extended to global concavity, which
implies that the policy gradient algorithm for our problem converges globally under this assumption.
4 Global concavity of the value function
In this section, we introduce the notion of the Lipschitz Markov decision process, and Lipschitz
policy space. We then restrict our attention to this subclass of MDPs. Our main result shows the
optimal policy belongs to the Lipschitz policy space and the policy gradient globally converges in
that space. We defer all the proofs of the results in this section to Appendix B.
4.1	Lipschitz markov decision process
Lipschitz Markov decision process has the property that for two state-action pairs that are close with
respect to Euclidean metric in S, their immediate rewards r and Markovian transition P should be
close with respect to the Kantorovich or L1 -Wasserstein metric. Kantorovich metric is, arguable,
the most common metric used used in the analysis of MDPs (cf. Hinderer, 2005; Rachelson and
Lagoudakis, 2010; Pirotta et al., 2015).
Definition 4.1 (Kantorovich metric). For any two probability measures p, q, the Kantorovich metric
between them is
K(p, q) = sup
f
ZX
fd(p - q) : f is 1-Lipschitz continuous
X
Definition 4.2 (Lipschitz MDP). A Markov decision process is (Lr, Lp)-Lipschitz if
∀S, S0 ∈ S	|r(S) - r(S0)| ≤ Lr |S - S0|
∀s,s0 ∈ S, a,a0 ∈ A	K(p(∙∣s, a),p(∙∣s0, a0)) ≤ Lp (|s — s0∣ 十 |a — a0∣)
5
Under review as a conference paper at ICLR 2020
4.2	Characterization of the optimal policy
Our result in Section 3, demonstrates that the second Frechet derivative of Vδ with respect to δ is
negative for a given policy δ when inequality (5) holds. To bound the second term of (5) from below,
i.e., Es0 [DVδ(s0)∣s, 0] - E§o [DVδ(s0)∣s, 1], it is sufficient to show that Frechet derivative DVδ(∙) is
Lipschitz-continuous. Even though we already assume that the Markov transition is Lipschitz, it is
still possible that DVδ is not Lipschitz: Bellman equation (8) for DVδ depends on policy δ(s) via
log(1 - δ(s)) - log(δ(s)), which can be non-Lipschitz in state s for general policies δ. Therefore,
to guarantee Lipschitzness of the Frechet derivative of the value function it is necessary to restrict
attention to the space of Lipschitz policies. In this subsection, we show that this restriction is
meaningful since the optimal policy is Lipschitz.
Theorem 4.1. Given (Lr, Lp)-Lipschitz MDP, the optimal policy δ satisfies
log
C- δ(s)
I δ(s)
s-
Stl
for all state S, St ∈ S where Rmax = maxs∈S r(S) is the maximum of the immediate reward r over
S.
4.3	Concavity of the value function with respect to Lipschitz policies
In this subsection, we present our main result showing the global concavity of the value function
for our specific class of Lipschitz MDPs with unobserved heterogeneity over the space of Lipschitz
policies.
Definition 4.3. Given (Lr, Lp)-Lipschitz MDP, define its Lipschitz policy space ∆ as
∆ = {δ : δ(s) ≤ S(S) ∀s ∈ S and
1	1--δ(s)∖	1 fl - δ(st)∖∣	(	2βRmaχLp∖ 1 tι t I
log()~ g("^ptJI≤(Lr+ɪ^JS-st∣ ∀s,st∈S卜
where δS is the myopic policy.
Theorem 4.1 and Lemma 2.1 imply that the optimal policy δ lies in this Lipschitz policy space ∆ for
any Lipschitz MDP.
Definition 4.4 (Condition for global convergence). We say that (Lr,Lp )-Lipschitz MDP satisfies the
sufficient condition for global convergence if
2βL	2 川 2βLp	(D 4 I	4βRmaxLp ʌ L	(exp(Rmin ) + 1)	小
2βLp <	1	and ---ɪ- 2L『+	一--U ≤	------~r~一-ɪ.	(6)
1 - 2βLp	1 -β	exp(Rmin)
Theorem 4.2.	Given (Lr,Lp)-Lipschitz MDP which satisfies the condition for global convergence
(6), value function Vδ is concave with respect to policy δ in the Lipschitz policy space ∆, i.e.,
D2Vδ (S) ≤ 0 for all S ∈ S, δ ∈ ∆.
4.4 The rate of global convergence of the policy gradient algorithm
In this subsection, we establish the rate of global convergence a simple version of the policy gradient
algorithm assuming oracle access to the Frechet derivative of the value function. While this analysis
provides only a theoretical guarantee, as discussed in Section 2.3, in practice the individuals are able
to produce an unbiased estimator of the exact gradient. As a result, the practical application of the
policy gradient algorithm would only need to adjust for the impact of stochastic noise in the estimator.
Since we assume that individuals know the immediate reward function r, the algorithm can be
initialized at the myopic policy δS with threshold πS(S) = r(S), which is in the Lipschitz policy space
∆. From Lemma 2.1 it follows that the myopic policy is pointwise in S greater than the optimal
policy, i.e., δ(s) ≤ δ(s). Consider policy δ with threshold π(s) = r(s) + ι-ββRmaX - 2Rmin∙ Note
that Bellman equation (7) implies that V(s) is between Rmmin and Rmax for all states s. Thus, policy
〜
〜
δ PointWiSe bounds the optimal policy δ from below, i.e., δ(s) ≤ δ(s). Our convergence rate result
applies to the policy gradient within the bounded Lipschitz policy set ∆.
6
Under review as a conference paper at ICLR 2020
Definition 4.5. Given (Lr, Lp)-Lipschitz MDP, define its bounded LiPschitz policy space ∆ as
∆∆ = {δ : δ(s) ≤ δ(s) ≤ S(S) ∀s ∈ S and
1	(1-δ(s)∖	1	(1-δ(st)∖∣	(	2βRmaχLp∖ , tl t I
l°g(	)~ g( "^PTjl≤ (Lr +	Js - St| ∀s,st ∈S 卜
For simplicity of notation, we introduce constants m and M which only depend onβ ,Rmin,Rmax,
Lr and Lp , whose exact expressions are deferred to the supplementary material for this paper.
Theorem 4.3.	Given a (Lr, Lp)-Lipschitz MDP, which satisfies the condition for global convergence
(6) and constants m and M defined above, for any step size α ≤ 吉,the policy gradient initialized at
the myopic PoIicy δ and updating as δ — αVδ Vδ in the bounded Lipschitz PoIicy space ∆∆ after k
iterations, it produces policy δ(k) satisfying
Vδ(s) - Vδ(k)(s) ≤
(1 — am)k
(exp(Rmin) + 1)2
at all s ∈ S.
5 Empirical application
To demonstrate the performance of the algorithm, we use the data from Rust (1987) which made the
standard benchmark for the Econometric analysis of MDPs. The paper estimates the cost associated
with maintaining and replacing bus engines using data from maintenance records from Madison
Metropolitan Bus City Company over the course of 10 years (December, 1974—May, 1985). The data
contains monthly observations on the mileage of each bus as well as the dates of major maintenance
events (such as bus engine replacement).
Rust (1987) assumes that the engine replacement decisions follow an optimal stopping policy derived
from solving a dynamic discrete choice model of the type that we described earlier. Using this
assumption and the data, he estimates the cost of operating a bus as a function of the running mileage
as well as the cost of replacing the bus engine. We use his estimates of the parameters of the return
function and the state transition probabilities (bus mileage) to demonstrate convergence of the gradient
descent algorithm.
In Rust (1987) the state st is the running total mileage of the bus accumulated by the end of period t.
The immediate reward is specified as a function of the running mileage as:
-RC + t1,	ifa= 1
r(st, a, θ1) =
-c(st, θ1 ) + t0, if a = 0
where RC is the cost of replacing the engine, c(st, θ1) is the cost of operating a bus that has st miles.
Following Rust (1987), we take c(st, θ1) = θ1st. Further, as in the original paper, we discretize the
mileage taking values in the range from 0 to 175 miles into an even grid of 2,571 intervals. Given the
observed monthly mileage, Rust (1987) assumes that transitions on the grid can only be of increments
0, 1, 2, 3 and 4. Therefore, transition process for discretized mileage is fully specified by just four
parameters θ2j = Pr[st+1 = st + j|st, a = 0], j = 0, 1, 2, 3. Table 1 describes parameter values
that we use directly from Rust (1987).
Table 1: parameter values in from Rust (1987).
Parameter	Value
RC	11.7257
θ1	0.001× 2.45569
(θ20, θ21, θ22, θ23)	(0.0937, 0.4475, 0.4459, 0.0127)
β	0.99
We use the gradient descent algorithm to update the policy threshold π : 1 + π ≥ 0 ⇒ a = 1,
where a = 1 denotes the decision to replace the engine. We set the learning rate using the RMSprop
method1.
1We use standard parameter values for RMSProp method: β = 0.1, ν = 0.001 and = 10-8. The
performance of the the method was very similar to that when we used ADAM to update the threshold values.
7
Under review as a conference paper at ICLR 2020
We use “the lazy projection" method to guarantee the search over Lipschitz policy space.
The policy space is parametrized by the vector of thresholds (π1 , . . . , πN) corresponding to
discretized state space (s1 , . . . , sN). It is initialized at the myopic policy, i.e. π1(0) =
u(s1),..., ∏N0) = u(sN). At step k the algorithm updates the thresholds to the value π(k*) =
π(k-1) - αDδ(k-i) V(Si)L(π(k-I))(1 - L(π(k-I))), where L(∙) is the logistic function and policy
δjk) = L(πjk-I)) for i,j = 1,... ,N. To make the“lazy projection" updated values π(k*) are ad-
justed to the closest monotone set of values π1(k) ≤ π2(k) ≤ . . . ≤ πN(k). The algorithm terminates at
step k where the norm maxi |D%(k)(si)| ≤ T for a given tolerance τ.2 The formal definition of lazy
projection can be found in Appendix C.
Figure 3 demonstrates convergence properties of our considered version of the policy gradient
algorithm. We used the “oracle" versions of the gradient and the value function that were obtained by
solving the corresponding Bellman equations. We initialized the algorithm using the myopic threshold
π(s) = -RC + c(s, θ1); with the convergence criterion set to be based on the value maxi ∣DVδ(Si)|3.
In the original model in Rust (1987), the discount factor used when estimating parameters of the cost
function was very close to 1. However, performance of the algorithm improves drastically when the
discount factor is reduced. This feature is closely related to the Hadamard stability of the solution of
the Bellman equation (e.g. observed in Bajari et al. (2013)) and is not algorithm-specific. In all of the
follow-up analysis by the same author (e.g. Rust (1996)) the discount factor is set to more moderate
values of .99 or .9 indicating that these performance issues were indeed observed with the settings in
Rust (1987). Figure 3 illustrates the performance of the algorithm for the case where the discount
factor is set to 0.992 3 4. For the same convergence criterion, the algorithm converges much faster.
Figure 1: Convergence of gradient descent, discount factor β = 0.99
Figure 2: Performance of the norm maxi ∣DVδ(Si)| and the second derivative maxi |D2Vδ(Si)|,
discount factor β = 0.99
2To optimize the performance of the method it is also possible to consider a mixed norm of the form
maxi ∣π(k) (Si) - π(k-1) (Si)| + λ maxi ∣DVδ(k) (si)∣∞ ≤ T for some calibrated weight λ. This choice would
control both the rate of decay of the gradient and the advancement of the algorithm in adjusting the thresholds.
3The particular tolerance value used was 0.03 for illustrative purposes.
4When we reduce the cost of replacing the engine along with the discount factor, which ensures that there is
significant variation in threshold values across states, convergence is improved even further
8
Under review as a conference paper at ICLR 2020
References
Abbring, J. H. and Heckman, J. J. (2007). Econometric evaluation of social programs, part iii:
Distributional treatment effects, dynamic treatment effects, dynamic discrete choice, and general
equilibrium policy evaluation. Handbook ofeconometrics, 6:5145-5303.
Aguirregabiria, V. and Magesan, A. (2016). Solution and estimation of dynamic discrete choice
structural models using euler equations. Available at SSRN 2860973.
Aguirregabiria, V. and Mira, P. (2007). Sequential estimation of dynamic discrete games. Economet-
rica, 75(1):1-53.
Aguirregabiria, V. and Mira, P. (2010). Dynamic discrete choice structural models: A survey. Journal
of Econometrics, 156(1):38-67.
Arcidiacono, P. and Miller, R. A. (2011). Conditional choice probability estimation of dynamic
discrete choice models with unobserved heterogeneity. Econometrica, 79(6):1823-1867.
Bajari, P., Hong, H., and Nekipelov, D. (2013). Game theory and econometrics: A survey of some
recent research. In Advances in economics and econometrics, 10th world congress, volume 3,
pages 3-52.
Bansal, N. and Gupta, A. (2017). Potential-function proofs for first-order methods. arXiv preprint
arXiv:1712.04581.
DUba J.-P., Chintagunta, P., Petrin, A., Bronnenberg, B., Goettler, R., Seetharaman, P., Sudhir,
K., Thomadsen, R., and Zhao, Y. (2002). Structural applications of the discrete choice model.
Marketing Letters, 13(3):207-220.
Dunford, N. and Schwartz, J. T. (1957). Linear Operators. Part 1: General Theory. New York
Interscience.
Eckstein, Z. and Wolpin, K. I. (1989). The specification and estimation of dynamic stochastic discrete
choice models: A survey. The Journal of Human Resources, 24(4):562-598.
Fazel, M., Ge, R., Kakade, S., and Mesbahi, M. (2018). Global convergence of policy gradient
methods for the linear quadratic regulator. In International Conference on Machine Learning,
pages 1466-1475.
Haskell, W. B., Jain, R., and Kalathil, D. (2016). Empirical dynamic programming. Mathematics of
Operations Research, 41(2):402-429.
Hendel, I. and Nevo, A. (2006). Measuring the implications of sales and consumer inventory behavior.
Econometrica, 74(6):1637-1673.
Hinderer, K. (2005). Lipschitz continuity of value functions in markovian decision processes.
Mathematical Methods of Operations Research, 62(1):3-22.
Hotz, V. J. and Miller, R. A. (1993). Conditional choice probabilities and the estimation of dynamic
models. The Review of Economic Studies, 60(3):497-529.
Jaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research, 11(Apr):1563-1600.
Muller, P. and Reich, G. (2018). Structural estimation using parametric mathematical programming
with equilibrium constraints and homotopy path continuation. Available at SSRN 3303999.
Nekipelov, D., Syrgkanis, V., and Tardos, E. (2015). Econometrics for learning agents. In Proceedings
of the Sixteenth ACM Conference on Economics and Computation, pages 1-18. ACM.
Pirotta, M., Restelli, M., and Bascetta, L. (2015). Policy gradient in lipschitz markov decision
processes. Machine Learning, 100(2-3):255-283.
Rachelson, E. and Lagoudakis, M. G. (2010). On the locality of action domination in sequential
decision making.
9
Under review as a conference paper at ICLR 2020
Rust, J. (1987). Optimal replacement of gmc bus engines: An empirical model of harold zurcher.
Econometrica: Journal ofthe Econometric Society, pages 999-1033.
Rust, J. (1996). Numerical dynamic programming in economics. Handbook of computational
economics, 1:619-729.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229-256.
Appendix
A Omitted proof for Theorem 3.1
Theorem 3.1. Value function Vδ is twice Freechet differentiable with respect to δ at the choice
probability δ corresponding to optimal policy and its Frechet derivative is negative at δ in all states s,
i.e., D2Vδ(s) ≤ 0.
Proof. We start with the Bellman equation of the value function.
Vδ(s) =(1 - δ(s)) r(s) - δ(s) log (δ(s))
- (1 - δ(s)) log(1 - δ(s)) + β E,s0 hVδ (s)(s0)si
(7)
First of all, note that in (7) the first three terms on the right hand side of the equation simple
nonlinear functions δ(∙) and thus the directional derivative with respect to δ(∙) can be taken as an
ordinary derivative with respect to δ as a parameter. Next note that if functional Jδ(∙) is directionally
differentiable with respect to δ and for all h(∙), 亲 Jδ+τh(∙)∣τ=o∕h(∙) is invariant, then J(∙) is
FreChet differentiable with respect to δ and the obove ratio is its FreChet derivative. As a result, the
FreChet derivative of simple functional (1 - δ(s)) r(s) - δ(s)log (δ(s)) - (1 - δ(s))log(1 - δ(s))
with respect to δ(∙) exists and equal to log(1 - δ(s)) - log(δ(s)) - r(s). This expression is itself a
Freechet-differentiable functional with Frechet derivative equal to -1∕(δ(s)(1 - δ(s))), meaning
that the original functional (1 - δ(s)) r(s) - δ(s) log (δ(s)) - (1 - δ(s)) log(1 - δ(s)) is twice
Frechet differentiable with the second Frechet derivative -1∕(δ(s)(1 - δ(s))). Whenever the state
transition is affected by the individual decision we need to consider decomposition of the conditional
expectation with respect to the future state:
Ee,s0M(s0)∣s] = (1 - δ(s)) EsoM(s0)∣s, 1] + δ(s) Es，M(s0)|s, 0].
Under standard technical conditions that allow the swap of the derivative and the integral
DEs，[⅛(s0)∣s] = (Es0[⅛(s0)∣s, 0] - Es，M(s0)|s, 1])+ Es，[D⅛(s0)∣s]
Thus, the Frechet derivative of the value function should be the fixed point of the following Bellman
equation
DVδ(s) = (log(1 - δ(s)) - log(δ(s)) - r(s))	8
+ β (Es，[Vδ(s0)∣s, 0] - Es，[Vδ(s0)∣s, 1]) + βEe,s，[DVδ(s0)∣s],	()
and
D2Vδ (S) = - δ(s)(1-δ(s))
-2β(Es∕[DVδ(s0)∣s, 1] - Es，[DVδ(s0)∣s, 0]) + βEs，[D2Vδ(s0)∣s].
(9)
Given that both these equations are Type II Fredholm integral equations for DVδ(∙) and D2Vδ(∙)
which have unique solutions whenever β < 1 that are bounded and continuous (see Dunford and
Schwartz (1957)) and, thus, unique solutions for both equations exist and Vδ(∙) is indeed Frechet-
differentiable. This means that the necessary condition for its optimum yielding δ is D%(s) = 0
for all states s. As a result, equation (9) implies that its second Frechet derivative is negative for all
states, i.e.,D2Vδ(s) ≤ 0.	□
10
Under review as a conference paper at ICLR 2020
B Omitted proofs in Section 4
B.1 OMITTED PROOF OF THEOREM 4.1
Theorem 4.1.	Given an (Lr, Lp)-Lipschitz MDP, the optimal policy δ satisfies
log
~ , .
1 - S(S)
~
S(S)
-log
1 - S(St)
~
S3)
/ ( T I 2∣3RmaXLP ʌ I t
≤ (Lr +	1-β JS -St
for all state s, St ∈ S where RmaX = maxs∈s γ(s) is the maximum of the immediate reward r over
S.
IΛ	∕' * . .1	. ∙	1	1 ∙	. < T-I √ < . 1	1' .1	1	1'	. ∙	♦	♦	I ʌ T 7- / ∖	∕∖
Proof. At the optimal policy δ, the FreChet derivative of the value function is zero, i.e., DVδ(s) = 0
for all state s. Therefore, from the Bellman equation (8) we establish that
log
~,.
1 - δ(s)
~
δ(s)
r(s) + β (Es' %(s0)∣s,1] - Es，%(s')∣s, 0])
Thus, for all states S, St ∈ S,
Es，M(S')∣s,α] - Es，M(S')∣st,α]∣
V	%(s')(p(s'∣s, α) - P(SlSt,α))ds'
J s，∈S
=Rmal Z	(t β)%(s0)(p(s0∣s,α) -P(SISt,a))ds0∣
1-β
s0∈S RmaX
≤RmaX sup I Z	f (s,)(p(s,∣s,a) - P(SlSt,a))ds0 |
1	- β ∣∣f∣∣L≤ι IjS,∈s	I
RmaxK(P(∙∣s,α),p(∙∣st,α)) ≤ RmaXLP ∣ S - St ∣
1-β	1-β
where we use upper bounds sups∈s %(s) ≤ Rmx and ∣∣ R-β) V⅞(s0)∣∣l ≤ 1. Thus,
log
~,.
1- S(s)
~
δ(s)
- log
(1- S(s0) !∣
I S(St) )∖
= Ir(S)- r(st) + β (Es，M(S0)∣s, 1] - Es，M(S0)∣s, 0])
-β (Es，%(s')∣st,1] -Es，M(S0)∣st, 0])∣
/ ( τ , 2βRmaXLp ∖ ∣ t ∣
≤(Lr +	1-β JS - St ∣
□
B.2 Omitted proof of Theorem 4.2
Theorem 4.2.	Given an (Lr, Lp)-Lipschitz MDP which satisfies the condition for global convergence
(6), the value function Vδ is concave with respect to policy S in the Lipschitz policy space ∆, i.e.,
D2Vδ(s) ≤ 0 for all S ∈ S, S ∈ ∆.
To show Theorem 4.2, we first introduce the following lemma establishing Lipschitz continuity of the
Frechet derivative of the value function.
Lemma B.1. Given a (Lr, Lp)-Lipschitz MDP, for all policies S in the the Lipschitz policy space
∆, the Frechet derivative of the respective value function DVδ (∙) is
2Lr +
46RmaxLp
1-β
1-2βLp
-Lipschitz
11
Under review as a conference paper at ICLR 2020
continuous, i.e.,for all states s, St ∈ S,
∕2L + 4βRmaχLp、
∣D%(s) — D⅛(st)∣ ≤ ( r 2 1-β	) ∣S-St
1 — 2βLp
Proof. We begin with the Bellman equation (8) for the Frechet derivative of value function.
DVδ(S) = log (1 δ(S)s)) — r(s)
+ β (Es，脑(s0)∣s, 0] — Es，[Vδ (s0)∣s, 1])+ β Ee,s, [D⅛ (s0)∣s]
We use the concept of the contraction mapping to prove the result of the Lemma.
Definition B.1. Let T : X → X be a mapping from a metric space X to itself,
•	T is a contraction mapping (with modulus γ ∈ [0, 1)) if ρ(T (x), T(y)) ≤ γρ(x, y) for all
x, y ∈ X, where ρ is a metric on X.
•	x is a fixed point ofT ifT (x) = x.
Lemma B.2. Suppose that X is a complete metric space and that T : X → X is a contraction
mapping with modulus γ. Then,
•	T has a unique fixed point x*.
•	If X 0 ⊆ X is a closed subset for which T (X 0) ⊆ X 0, then x* ∈ X 0.
Consider the contraction mapping Tδ(X)(S) = log (1-δ()S)
— r(S) + β (Es，[Vδ(s0)∣s, 0] —
Es，[Vδ(s0)∣s, 1]) + βEe,s，[x(s0)∣s], then the Bellman equation implies that DVδ is the fixed point of
contraction mapping Tδ. Since the Lipschitz continuity property forms a closed subset, by Lemma B.2,
it is sufficient to show for any LDV -Lipschitz continuous x, Tδ (x) is also LDV -Lipschitz continuous,
2L + 4βRmaxLp
where LDV = r ―61^ β——.Thus, consider states s, St ∈ S,
≤ log
1 — δ(s)∖	.
k)—log
1 - δ(st)
δ(st)
+ β ∣Es0[Vδ(s0)∣s, 0] — Es, [Vδ(s0)∣st, 0]∣
+ β∣Es0[Vδ(s0)∣s, 1] — Es, [Vδ(s0)∣st,l]∣
+ β ∣∣Es0 [x(s0)|s, 0] δ(s) - Es0 [x(s0)|st, 0] δ(st)∣∣
+ β∣Es0[x(s0)∣s, 1](1 — δ(s)) — Es, [x(s0)∣st, 1] (1 — δ(st))∣
where
iog(⅛1F)—log
1 - δ(st)
δ3)
2βRmaxLp
r(s) — r(st)∣ ≤ Lr ∣s — s
1-β
t∣
s — st
by the same calculation in the proof of Theorem 4.1, for a = 0, 1,
β∣Es,[Vδ(s0)∣s,a] — Es，[Vδ(s0)∣st,a]∣ ≤ (Lr + 2万：：彳力)∣s — st
1 — β
12
Under review as a conference paper at ICLR 2020
and
∣Es0[x(s0)∣s, 0] δ(s) - Es0 [x(sz)∣st, 0] δ(st)∣
I (	(δ(s) — 6(s，))x(s，)(p(s，|s, a) — p(s0∣s< ɑ))ds0
I JSyS
(δ(s) - δ(st))x(s0)
= LDV I /
IS ∈ ∈s
LDV
≤Ldv sup
kfkL≤1
[f	f(sO)(P(JBa)-P(SlsIa))ds0
I JS0∈s
=LdvK(p(∙∣5,a),p(∙∣5t,a)) ≤ LDVLP ∣s - s*∣
where we use the bound ∣ δ(s) - δ(s*) ∣ ≤ 1 and thus ∣∣(""-L^；"MS ' IlL ≤ 1. Similarly,
∣Eso[x(s0)∣s, 1](1 - δ(s)) - Es, [x(s0)∣st, 1] (1 - δ(st)) ∣ ≤ LDVLP ∣ s - st ∣
Combining all the bounds, we obtain that
IT(x)(s) - T(X)(St
≤	2Lr +
4βRmaχLp
+ 2βLDv
Substitution LDV
2Lr +
4βRmaxLp
1-β
1-2βLp
yields the statement of the Lemma.
1 - β
□
ProofofTheorem 4.2. From the Bellman equation (9), it is sufficient to show
W-IW ≥ 2β(ESO[DVδ(S')∣sH-ESO[DVδ(S%,0])
(10)
We bound both sides separately. Since the policy satisfies δ(s) ≤ S(S) for all states s, and S(S)
exp(，S))+i ≤ 1, the left hand side can be bounded from below as
1	>	1	>( exp(Rmin) +1)2
δ(s)(1 - δ(s)) ≥ S(S)(I - S(S)) ≥ -exp(Rmin)-
Meanwhile, the righthand side can be bounded from above by Lemma B.1. Let LDV
4 4 I 4βRmaxLP
2LT +	1-β
1-2βLp	,
2β ∣Es 0 [DVδ(s0)∣s, 1] - Es，[DVδ(s0)∣s, 0]∣
=2β
D	DVδ(s0)(p(s0∣s, 1) - P(Sls, 0))ds0
J s，∈S
=2β LDV
/
JS，∈S
DVδ (s0)
LDV
(P(SlS, 1) - p(s0∣s, 0))ds0
≤2βLDv sup
kfkL≤1
Z Z	f(s0)(p(s1s, I)-P(Sls,0))d
I JS，∈S
=2βLDVK(p(∙∣s, 1),p(∙∣s,0)) ≤ 1-β2βL(2Lr+ 4βRmaβLP
From the condition of global convergence
2βLP	(2L + 4βRmaXLP ) ≤ ( exp(Rmin) + 1)
1 - 2βLP Ir	1 - β ) —	exp(Rmin)
it follows that the inequality (10) is satisfied and the Bellman equation (9) implies that D2Vδ(s) ≤ 0
for all states S ∈ S.	□
13
Under review as a conference paper at ICLR 2020
B.3 Omitted proof of Theorem 4.3
For notation simplicity, we introduce notations m and M such that
1	((exp(Rmin) + 1)2 - 2βLp	(2L + 4βRmaχLp A
1 - β [	eχp(Rmin)	1 - 2βLp Ir 1 - 尸)
M =(T-J)2
(exp ( 1 —β RmaX - 2 Rmin) + 1)
(I- 8)eXp( 1R	- βR	)
expI 1-β 1 LmaX	2 ʃɛmin/
+ 2β (eχp (1-βRmaX - 2Rmin) + 1 尸产 RmaX - (I + 8)Rmin
Theorem 4.3. Given a (Lr, Lp)-Lipschitz MDP, which satisfies the condition for global convergence
(6) and constants m and M defined above, for any step size α ≤ 吉，the policy gradient initialized at
the myopic PoIiCy δ and updating as δ — αVδ Vδ in the bounded Lipschitz PoIiCy space ∆ after k
iterations, it produces policy δ(k) satisfying
- Vδk (S) ≤	(1-αm)k 2
δ ^ (eXp(Rmin) + 1)2
at all s ∈ S .
Our analysis follows the standard steps establishing convergence of the conventional gradient descent
algorithm which bounds the second Frechet derivative of the value function Vδ with respect to the
policy δ from above and from below by m and M respectively.
Lemma B.3. Given a (Lr, Lp)-Lipschitz MDP, which satisfies the condition for global convergence
(6), for all policies δ in the bounded Lipschitz policy space ∆, for all states S ∈ S, the SeCOnd Frechet
derivative of the value function Vδ with respect to the policy δ is upperbounded as
D2Vδ (S) ≤ -m.
Proof. The Bellman equation (9) implies that
max D2 Vδ(s) ≤ --- ( - min z , z ɪ——---
S δ( ) ≤ 1 - β V S δ(s)(1 - δ(s))
+ 2βmax(Eso[DVδ(s0)∣s, 0] - Es，[DVδ(s')∣s,1]))
By the same argument as in Theorem 4.2,
.	1	、(exp(Rmin) + 1)2
min≥
S δ(s)(1 - δ(s)) ≥	exp(Rmin)
max(Es0 [DVδ(s0)∣s, 1] - Es，[DVδ(s0)∣st, 0]) ≤ T^Lr(2Lr + 4βRmaxLp A
S	1 - 2βLp	1 - β
Thus, for all state S ∈ S,
D2Vδ(s) ≤ -m. □
Lemma B.4. Given a (Lr, Lp)-Lipschitz MDP, which satisfies the condition for global convergence,
for all policy δ in the bounded Lipschitz policy space ∆, for all state S ∈ S, the second derivative of
the value function Vδ with respect to the policy δ is is lowerbounded as
D2Vδ (S) ≥ -M.
Proof. The Bellman equation (9) implies that
min D2Vδ(S) ≥ --- ( - max z , z ɪ——---
s δ( ) ≥ 1 - β V s δ(s)(1 - δ(s))
+ 2β( minDVδ (S) — maxDVδ (S)))
14
Under review as a conference paper at ICLR 2020
By restricting policy to the bounded LiPschitz policy space ∆ We bound
1
1
max / 、/-------- ≤ max —------------- ≤
S δ(s)(1 - δ(s)) S δ(s)(1 - δ(s))
exp ( 1 —β RmaX - 2 Rmin) + 1)
exp( 1—βRmaX - 2Rmin)
Provided
Rmin
min Vδ(s) ≥ —2—
≥ min (log (⅛F
1	/1 - δ(s)'
≤ mαxlog( Wτ,
- r(s) = 0
- min r(s)
S
≤ exp (ɪ-eRmax - 2Rmin) - Rmin
it folloWs from Bellman equation (8) that
min DVδ (s) ≥
S
≥
maxDVδ(s) ≤
S
≤
1
1-β
β
1 - β
1
1-β
1
1-β
min (log (1 -(S)S)) - r(s)) + β(min Vδ(S) - max Vδ(S))
Rmin	RmaX
2	1-β
max (iog (1⅛F
- r(s) + β(max Vδ (s) - min Vδ (s))
exp (τ⅛βR maX - 2 Rmm) + 占 RmaX
2 + β R .
2 m Lmin
—
S
S
—
Thus, for all state S ∈ S,
D2Vδ(s) ≥ -M.	□
Proof of Theorem 4.3. The convergence rate guarantee folloWs from Lemma B.3 and Lemma B.4,
under the standard arguments for the gradient descent algorithm for m-strongly concave and M-
smooth (i.e., M-Lipschitz gradient) functions (cf. Bansal and Gupta, 2017).	□
C More results in Section 5
Algorithm 1 “Lazy projection”, (π1, . . . , πN): thresholds corresponding to discretized state space
(si,..., SN); L(∙): logistic function; policy δj = L(∏j); α: step size; τ: termination tolerance
1:	π(0) J u(si), ..., ∏N) J u(sn) // Initialize π(0) at the myopic policy
2:	while maxi ∣DVδ(k) ($i)| ≤ T do
3:	∏(k*) J ∏(k-1) - α Dδ(k-i) Vδ(k-i) (Si)L(∏(k-1))(1 一 L(π(k-1))) forall i ∈ [N]
4:	(∏(k),..., ∏(k)) J the closest monotone thresholds of (∏(k*),..., ∏(k*)) // Lazy projection
5:	return (π1(k), . . . , πN(k))
We list the convergence of gradient descent and its derivative, second derivative at smaller discount
factor β = 0.9.
15
Under review as a conference paper at ICLR 2020
Figure 3: Convergence of gradient descent, discount factor β = 0.9
Figure 4: Performance of the norm maxi ∣DVδ(Si)| and the second derivative maxi |D2Vδ(Si)|,
discount factor β = 0.9
16