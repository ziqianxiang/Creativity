Under review as a conference paper at ICLR 2020
Min-Max Optimization without Gradients:
Convergence and Applications to Adversar-
ial ML
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the problem of constrained robust (min-max) optimization in
a black-box setting, where the desired optimizer cannot access the gradients of the
objective function but may query its values. We present a principled optimization
framework, integrating a zeroth-order (ZO) gradient estimator with an alternating
projected stochastic gradient descent-ascent method, where the former only requires
a small number of function queries and the later needs just one-step descent/ascent
update. We show that the proposed framework, referred to as ZO-Min-Max, has
a sub-linear convergence rate under mild conditions and scales gracefully with
problem size. From an application side, we explore a promising connection between
black-box min-max optimization and black-box evasion and poisoning attacks in
adversarial machine learning (ML). Our empirical evaluations on these use cases
demonstrate the effectiveness of our approach and its scalability to dimensions that
prohibit using recent black-box solvers.
1	Introduction
In numerous real-world applications, one is faced with various forms of adversary that are not
accounted for by standard optimization algorithms. For instance, when training a machine learning
model on user-provided data, malicious users can carry out a data poisoning attack: providing false
data with the aim of corrupting the learned model (Steinhardt et al., 2017; Tran et al., 2018; Jagielski
et al., 2018). At inference time, malicious users can evade detection of multiple models in the form
of adversarial example attacks (Goodfellow et al., 2014; Liu et al., 2016; 2018a). Min-max (robust)
optimization is a natural framework to address adversarial (worst-case) robustness (Madry et al.,
2017b; Al-Dujaili et al., 2018b). It converts a standard minimization problem into a composition of
an inner maximization problem and an outer minimization problem.
Min-max optimization problems have been studied for multiple decades (Wald, 1945), and the
majority of the proposed methods assume access to first-order (FO) information, i.e. gradients, to find
or approximate robust solutions (Nesterov, 2007; Gidel et al., 2017; Hamedani et al., 2018; Qian et al.,
2019; Rafique et al., 2018; Sanjabi et al., 2018b; Lu et al., 2019; Nouiehed et al., 2019; Lu et al., 2019;
Jin et al., 2019). In this paper, we focus on design and analysis of black-box (gradient-free) min-max
optimization methods, where gradients are neither symbolically nor numerically available, or they
are tedious to compute (Conn et al., 2009). Our study is particularly motivated by the design of data
poisoning and evasion adversarial attacks from black-box machine learning (ML) or deep learning
(DL) systems, whose internal configuration and operating mechanism are unknown to adversaries.
The extension of min-max optimization from the FO domain to the gradient-free regime is challenging
since the solver suffers from uncertainties in both black-box objective functions and optimization
procedure and do not scale well to high-dimensional problems.
We develop a provable and unified black-box min-max stochastic optimization method by integrating
a query-efficient randomized zeroth-order (ZO) gradient estimator with a computation-efficient
alternating gradient descent-ascent framework, where the former requires a small number of function
queries to build a gradient estimate, and the latter needs just one-step descent/ascent update. Recently,
ZO optimization has attracted increasing attention in solving ML/DL problems. For example, ZO
optimization serves as a powerful and practical tool for generation of black-box adversarial examples
1
Under review as a conference paper at ICLR 2020
to evaluate the adversarial robustness of ML/DL models (Chen et al., 2017; Ilyas et al., 2018; Tu et al.,
2018; Ilyas et al., 2019). ZO optimization can also help to solve automated ML problems, where
the gradients with respect to ML pipeline configuration parameters are intractable (Aggarwal et al.,
2019). Furthermore, ZO optimization provides computationally-efficient alternatives of high-order
optimization methods for solving complex ML/DL tasks, e.g., robust training by leveraging input
gradient or curvature regularization (Finlay & Oberman, 2019; Moosavi-Dezfooli et al., 2019), model-
agnostic meta-learning (Fallah et al., 2019), network control and management (Chen & Giannakis,
2018), and data processing in high dimension (Liu et al., 2018b). Other recent applications include
generating model-agnostic contrastive explanations (Dhurandhar et al., 2019) and escaping saddle
points (Flokas et al., 2019). Current studies (Ghadimi & Lan, 2013; Nesterov & Spokoiny, 2015;
Duchi et al., 2015; Ghadimi et al., 2016; Shamir, 2017; Liu et al., 2019) suggested that ZO methods
typically agree with the iteration complexity of FO methods but encounter a slowdown factor up
to a small-degree polynomial of the problem dimensionality. To the best of our knowledge, it was
an open question whether any convergence rate analysis can be established for black-box min-max
optimization.
Contribution. We summarize our contributions as follows. (i) We first identify a class of black-box
attack and robust learning problems which turn out to be min-max black-box optimization problems.
(ii) We propose a scalable and principled framework (ZO-Min-Max) for solving constrained min-
max saddle point problems under both one-sided and two-sided black-box objective functions.
Here the one-sided setting refers to the scenario where only the outer minimization problem is
black-box. (iii) We provide a novel convergence analysis characterizing the number of objective
function evaluations required to attain locally robust solution to black-box min-max problems with
nonconvex outer minimization and strongly concave inner maximization. Our analysis handles
stochasticity in both objective function and ZO gradient estimator, and shows that ZO-Min-Max
yields O(1/T + 1/b + d/q) convergence rate, where T is number of iterations, b is mini-batch
size, q is number of random direction vectors used in ZO gradient estimation, and d is number
of optimization variables. (iv) We demonstrate the effectiveness of our proposal in practical data
poisoning and evasion attack generation problems.1
2	Related Work
FO min-max optimization. Gradient-based methods have been applied with celebrated success to
solve min-max problems such as robust learning (Qian et al., 2019), generative adversarial networks
(GANs) (Sanjabi et al., 2018a), adversarial training (Al-Dujaili et al., 2018b; Madry et al., 2017a),
and robust adversarial attack generation (Wang et al., 2019b). Some of FO methods are motivated by
theoretical justifications based on Danskin’s theorem (Danskin, 1966), which implies that the negative
of the gradient of the outer minimization problem at inner maximizer is a descent direction (Madry
et al., 2017a). Convergence analysis of other FO min-max methods has been studied under different
problem settings, e.g., (Lu et al., 2019; Qian et al., 2019; Rafique et al., 2018; Sanjabi et al., 2018b;
Nouiehed et al., 2019). It was shown in (Lu et al., 2019) that a deterministic FO min-max algorithm
has O(1/T) convergence rate. In (Qian et al., 2019; Rafique et al., 2018), stochastic FO min-max
methods have also been proposed, which yield the convergence rate in the order of O(1/√T) and
O(1/T 1/4), respectively. However, these works were restricted to unconstrained optimization at the
minimization side. In (Sanjabi et al., 2018b), noncovnex-concave min-max problems were studied,
but the proposed analysis requires solving the maximization problem only up to some small error.
In (Nouiehed et al., 2019), the O(1/T) convergence rate was proved for nonconvex-nonconcave
min-max problems under Polyak- LojasieWicz conditions. Different from the aforementioned FO
settings, ZO min-max stochastic optimization suffers randomness from both stochastic sampling in
objective function and ZO gradient estimation, and this randomness would be coupled in alternating
gradient descent-descent steps and thus make it more challenging in convergence analysis.
Gradient-free min-max optimization. In the black-box setup, coevolutionary algorithms were
used extensively to solve min-max problems (Herrmann, 1999; Schmiedlechner et al., 2018). However,
they may oscillate and never converge to a solution due to pathological behaviors such as focusing and
relativism (Watson & Pollack, 2001). Fixes to these issues have been proposed and analyzed—e.g.,
1Source code will be released.
2
Under review as a conference paper at ICLR 2020
asymmetric fitness (Jensen, 2003; Branke & Rosenbusch, 2008). In (Al-Dujaili et al., 2018c), the
authors employed an evolution strategy as an unbiased approximate for the descent direction of the
outer minimization problem and showed empirical gains over coevlutionary techniques, albeit without
any theoretical guarantees. Min-max black-box problems can also be addressed by resorting to direct
search and model-based descent and trust region methods (Audet & Hare, 2017; Larson et al., 2019;
Rios & Sahinidis, 2013). However, these methods lack convergence rate analysis and are difficult to
scale to high-dimensional problems. For example, the off-the-shelf model-based solver COBYLA
only supports problems with 216 variables at maximum in SciPy Python library (Jones et al., 2001),
which is even smaller than the size of a single ImageNet image. The recent work (Bogunovic et al.,
2018) proposed a robust Bayesian optimization (BO) algorithm and established a theoretical lower
bound on the required number of the min-max objective evaluations to find a near-optimal point.
However, BO approaches are often tailored to low-dimensional problems and its computational
complexity prohibits scalable application. From a game theory perspective, the min-max solution
for some problems correspond to the Nash equilibrium between the outer minimizer and the inner
maximizer, and hence black-box Nash equilibria solvers can be used (Picheny et al., 2019; Al-Dujaili
et al., 2018a). This setup, however, does not always hold in general. Our work contrasts with the
above lines of work in designing and analyzing black-box min-max techniques that are both scalable
and theoretically grounded.
3	Problem Setup
In this section, we define the black-box min-max problem and briefly motivate its applications. By
min-max, we mean that the problem is a composition of inner maximization and outer minimization
of the objective function f . By black-box, we mean that the objective function f is only accessible
via point-wise functional evaluations. Mathematically, we have
min max f(x, y)
x∈X y∈Y
(1)
where x and y are optimization variables, f is a differentiable objective function, and X ⊂ Rdx
and Y ⊂ Rdy are compact convex sets. For ease of notation, let dx = dy = d. In (1), the objective
function f could represent either a deterministic loss or stochastic loss f (x, y) = Eξ〜P [f (x, y; ξ)],
where ξ is a random variable following the distribution p. In this paper, we consider the stochastic
variant in (1).
We focus on two black-box scenarios in which gradients (or stochastic gradients under randomly
sampled ξ) of f w.r.t. x or y are not accessed.
(a)	One-sided black-box: f(x, y) is a white box w.r.t. y but a black box w.r.t. x.
(b)	Two-sided black-box: f(x, y) is a black box w.r.t. both x and y.
Motivation of setup (a) and (b). Both setups are well motivated from the design of black-box
adversarial attacks. The formulation of the one-sided black-box min-max problem corresponds to a
particular type of attack, known as black-box ensemble evasion attack, where the attacker generates
adversarial examples (i.e., crafted examples with slight perturbations for misclassification at the
testing phase) and optimizes its worst-case performance against an ensemble of black-box classifiers
and/or example classes. The formulation of two-sided black-box min-max problem represents
another type of attack at the training phase, known as black-box poisoning attack, where the attacker
deliberately influences the training data (by injecting poisoned samples) to manipulate the results of a
black-box predictive model.
Although problems of designing ensemble evasion attack (Liu et al., 2016; 2018a; Wang et al., 2019b)
and data poisoning attack (Jagielski et al., 2018; Wang et al., 2019a) have been studied in the literature,
most of them assumed that the adversary has the full knowledge of the target ML model, leading to an
impractical white-box attack setting. By contrast, we provide a solution to min-max attack generation
under black-box ML models. We refer readers to Section 6 for further discussion and demonstration
of our framework on these problems.
3
Under review as a conference paper at ICLR 2020
4	ZO-Min-Max: A Framework for Black-Box Min-Max
Optimization
Our interest is in a scalable and theoretically principled framework for black-box min-max problems
of the form (1). To this end, we first introduce a randomized gradient estimator that only requires
a few number of point-wise function evaluations. Based on that, we then propose a ZO alternating
projected gradient method to solve (1) under both one-sided and two-sided black-box setups.
Randomized gradient estimator. In the ZO setting, we adopt a randomized gradient estimator to
estimate the gradient of a function with the generic form h(x) := Eξ[h(x; ξ)] (Liu et al., 2019; Gao
et al., 2014),
4 -，、
▽ Xh(X)
1 X^ ^x d[h(x + μui; ξj) - h(X; ξj)]
bq j∈I =	μ
(2)
where d is number of variables, I denotes the mini-batch set of b i.i.d. stochastic samples {ξj}jb=1,
{ui}q=ι are q i.i.d. random direction vectors drawn uniformly from the unit sphere, and μ > 0
is a smoothing parameter. We note that the ZO gradient estimator (2) involves randomness from
both stochastic sampling w.r.t. ui as well as the random direction sampling w.r.t. ξj . It is known
from (Gao et al., 2014, Lemma2) that Vχh(x) provides an unbiased estimate of the gradient of
the smoothing function of h rather than the true gradient of h. Here the smoothing function of h is
defined by hμ(x) = Ev[h(x + μv)], where V follows the uniform distribution over the unit Euclidean
ball. Besides the bias, we provide an upper bound on the variance of (2) in Lemma 1.
Lemma 1. Suppose that for all ξ, h(x; ξ) has Lh Lipschitz continuous gradients and the gradient of
h(x; ξ) is upper bounded as kVxh(x; ξ)k22 ≤ η2 at x
∈ Rd Then E [VXh(x)] = Vχhμ(x),
E [忖Xh(X) - Vχhμ(x)k2] ≤ 筌 + 4dη2 + μLhd2 := σ2(Lh,μ,b,q,d),	(3)
where the expectation is taken over all randomness.
Proof: See Appendix A.2.
In Lemma 1, if We choose μ ≤ 1/√d, then the variance bound is given by O(1∕b + d/q). In our
problem setting (1), the ZO gradients Vxf(x, y) and Vyf(x, y) follow the generic form of (2) by
fixing y and letting h(∙) := f (∙, y) or by fixing X and letting h(∙) := f (x, ∙), respectively.
Algorithmic framework. To solve problem (1), we alternatingly perform ZO projected gradient
descent/ascent method for updating x and y. Specifically, for one-sided ZO min-max optimization,
the ZO projected gradient descent (ZO-PGD) over x yields
x(t) = ProjX (x(tτ) - AXf (x(tτ),y(t-1)
(4)
where t is the iteration index, Vxf denotes the ZO gradient estimate of f w.r.t. x, α > 0 is the
learning rate at the x-minimization step, and projX(a) signifies the projection of a onto X, given by
the solution to the problem minx∈X kx - ak22. For two-sided ZO min-max optimization, in addition
to (4), our update on y obeys the ZO projected gradient ascent (ZO-PGA)
y(t) =projγ 卜(t-1) +◎▽ yf ^x(t),y"-1))),
(5)
where β > 0 is the learning rate at the y-maximization step. The proposed method is named as
ZO-Min-Max; see Algorithm 1.
Why estimates gradient rather than distribution of function values? Besides ZO optimization
using random gradient estimates, the black-box min-max problem (1) can also be solved using the
Bayesian optimization (BO) approach, e.g., (Bogunovic et al., 2018; Al-Dujaili et al., 2018a). The
core idea of BO is to approximate the objective function as a Gaussian process (GP) learnt from the
history of function values at queried points. Based on GP, the solution to problem (1) is then updated
by maximizing a certain reward function, known as acquisition function. The advantage of BO is
4
Under review as a conference paper at ICLR 2020
its mild requirements on the setting of black-box problems, e.g., at the absence of differentiability.
However, BO usually does not scale beyond low-dimensional problems since learning the accurate
GP model and solving the acquisition problem takes intensive computation cost per iteration. By
contrast, our proposed method is more efficient, and mimics the first-order method by just using the
random gradient estimate (2) as the descent/ascent direction. In Figure A1, we compare ZO-Min-Max
with the BO based STABLEOPT algorithm proposed by (Bogunovic et al., 2018) through a toy
example shown in (Bogunovic et al., 2018, Sec. 5). As we can see, ZO-Min-Max not only achieves
more accurate solution but also requires less computation time. We refer readers to Appendix B for
details.
Technical challenges in convergence analysis.
The convergence analysis of ZO-Min-Max is
more challenging than the case of FO min-max
algorithms. Besides the inaccurate estimate of
the gradient, the stochasticity of the estimator
makes the convergence analysis sufficiently dif-
ferent from the FO deterministic case (Lu et al.,
2019; Qian et al., 2019), since the errors in min-
imization and maximization are coupled as the
algorithm proceeds.
Moreover, the conventioanl analysis of ZO op-
timization for single-objective problems cannot
directly be applied to ZO-Min-Max. Even at
the one-sided black-box setting, ZO-Min-Max
conducts alternating optimization using one-step
ZO-PGD and PGA with respect to x and y re-
spectively. This is different from a reduced ZO
optimization problem with respect to x only by
Algorithm 1 ZO-Min-Max to solve problem (1)
1:	Input: given x(0) and y(0), learning rates α
and β , the number of random directions q, and
the possible mini-batch size b for stochastic
optimization
2:	for t = 1, 2, . . . , T do
3:	x-step: perform ZO-PGD (4)
4:	y-step:
5:	if f(x(t), y) is black box w.r.t. y then
6:	perform ZO-PGA (5)
7:	else
8:	performPGA using Ny f (x(t), y(t-1))
as ascent direction in (5)
9:	end if
10:	end for
solving problem minx∈X h(x) := miny∈Y f(x, y), which requires the algorithm to obtain the solu-
tion to miny∈Y f(x, y) at a given x (when querying h(x) for a ZO gradient estimation). However,
this process is usually non-trivial or computationally intensive.
In particular, one key difficulty stems from the alternating algorithmic structure (namely, primal-dual
framework) as the problem is in the min-max form, which leads to opposite optimization directions
(minimization vs maximization) over variable x and y respectively. Even applying ZO optimization
only to one side, it needs to quantify the effect of ZO gradient estimation on the descent over both x
and y. We provide a detailed convergence analysis of ZO-Min-Max in the next section.
5	Convergence Analysis
We begin by elaborating on assumptions and notations used in analyzing the convergence of ZO-Min-
Max (Algorithm 1).
A1: In (1), f(x, y) is continuously differentiable, and is strongly concave w.r.t. y with parameter
Y > 0, namely, given X ∈ X, f(x, yι) ≤ f(x, y2) + Nyf (x, y2)τ(y1 — y2)— 2IM — y2k2 for
all points yι, y ∈ Y. And f is lower bounded by a finite number f * and has bounded gradients
kNxf(x, y; ξ)k ≤ η2 and IlNyf (x, y; ξ)k ≤ η2 for stochastic optimization with ξ 〜p. Here ∣∣ ∙ ∣∣
denotes the `2 norm. The constraint sets X , Y are convex and bounded with diameter R.
A2: f(x, y) has Lipschitz continuous gradients, i.e., there exists Lx, Ly > 0 such that ∣Nxf(x1, y)-
Nxf(x2, y)∣ ≤ Lx∣ x1 -x2 ∣ for ∀x1, x2 ∈ X, and ∣Nyf (x1, y) - Nyf (x2, y)∣ ≤
Ly∣ x1 -x2 ∣ and ∣Nyf(x,y1) - Nyf(x,y2)∣ ≤ Ly∣ y1 -y2 ∣ for∀y1,y2 ∈ Y.
We note that A1 and A2 are required for analyzing the convergence of ZO-Min-Max. They were
used even for the analysis of first-order min-max optimization methods (Lu et al., 2019; Nouiehed
et al., 2019) and first-order methods for nonconvex optimization with a single objective function
(Chen et al., 2019; Ward et al., 2019). In A1, the strongly concavity of f(x, y) with respect
to y holds for applications such as robust learning over multiple domains (Qian et al., 2019), and
adversarial attack generation that will be introduced in Section 6. In A2, the assumption of smoothness
5
Under review as a conference paper at ICLR 2020
(namely, Lipschitz continuous gradient) is required to quantify the descent of the alternating projected
stochastic gradient descent-ascent method. Even for single-objective non-convex optimization, e.g.,
(Chen et al., 2019; Bernstein et al., 2018), A2 is needed in analysis. For clarity, we also summarize
the problem and algorithmic parameters used in our convergence analysis in Table A1 of Appendix.
2019; Ghadimi
(6)
We measure the convergence of ZO-Min-Max by the proximal gradient (Lu et al.,
et al., 2016),
G(x, y)
(1∕α) (x —projχ(x —αVχf(x,y))) ^
(i∕β) (y -PrOjY(y +βvyf(x, y)))],
where (x, y) is a first-order stationary Point of (1) iff kG(x, y)k = 0.
In what follows, we delve into our convergence analysis. First, Lemma 2 shows the descent property
of ZO-PGD at the x-minimization step in Algorithm 1.
Lemma 2. (Descent lemma in minimization) Under A1-A2, let (x(t), y(t)) be a sequence generated
by Algorithm 1. When f(x, y) is black-box w.r.t. x, then we have following descent property w.r.t. x:
E[f (x(t+1),y(t))] ≤ E[f(x(t),y(t))] - (1 - L2x) Ek∆Xt+1)k2 + ασX + Lxμ2	(7)
where △Xt) := x(t) — x(t-1), and σ2 := σ2(Lx, μ, b, q, d) defined in (3).
Proof: See Appendix A.3.1.
It is clear from Lemma 2 that updating x leads to the reduced objective value when choosing a
small learning rate α. However, ZO gradient estimation brings in additional errors in terms of
ασ2 and Lxμ2, where the former is induced by the variance of gradient estimates in (3) and the
latter is originated from bounding the distance between f and its smoothing version; see (25) in
Appendix A.3.
Convergence rate of ZO-Min-Max by performing PGA. We next investigate the convergence of
ZO-Min-Max when FO PGA is used at the y-maximization step (Line 8 of Algorithm 1) for solving
one-sided black-box optimization problems.
Lemma 3. (Descent lemma in maximization) Under A1-A2, let (x(t), y(t)) be a sequence generated
by Algorithm 1 and define the potential function as
P (x(t), y(t), ∆(t)) = E[f (x(t), y(t))] + 4 + 4β2ILyy- 7βγ Ek∆t)k2,	(8)
where △(yt) := y(t) — y(t-1). When f(x, y) is black-box w.r.t. x and white-box w.r.t. y, then we
have the following descent property w.r.t. y:
P(x(t+1), y(t+1), ∆(yt+1)) ≤P(x(t+1), y(t), ∆(yt))
-(2β - 2LLy) Ek∆yt+1)k2 + (Y2β + β) LXEk∆Xt+1)k2,	(9)
Proof: See Appendix A.3.2.
It is shown from (9) that when β is small enough, then the term (1∕(2∣) — 2Ly/γ)Ek∆S+1)k2 will
give some descent of the potential function after performing PGA, while the last term in (9) will give
some ascent to the potential function. However, such a quantity will be compensated by the descent
of the objective function in the minimization step shown by Lemma 2. Combining Lemma 2 and
Lemma 3, we obtain the convergence rate of ZO-Min-Max in Theorem 1.
Theorem 1. Suppose that A1-A2 hold, the sequence (x(t), y(t)) over T iterations is generated
by Algorithm 1 in which learning rates satisfy β < 1/(4L2y) and α ≤ min{1/Lx, 1/(Lx/2 +
2LX∕(γ2β) + βLX/2)}. When f (x, y) isblack-boxw.r.t. X and white-box w.r.t. y, the convergence
rate of ZO-Min-Max under a uniformly and randomly picked (x(r), y(r)) from {(x(t), y(t))}tT=1 is
given by
EkG(X(r),y(r))k2 ≤ C (PI- f'- VR2) + W + cLx^	a。)
6
Under review as a conference paper at ICLR 2020
where Z is a constant independent on the parameters μ, b, q, d and T, Pt := P(x(t), y(t), ∆y))
given by (8), c = max{Lχ + 3∕α, 3∕β}, V = min{4 + 4β2Lyy 一 7βγ, 0}∕(2β2Y), σ2 is variance
bound of ZO gradient estimate given in (7), and f *, R, γ, Lx and Ly have been defined in A1-A2.
Proof: See Appendix A.3.3.
To better interpret Theorem 1, we begin by clarifying the parameters involved in our convergence rate
(10). First, the parameter ζ appears in the denominator of the derived convergence error. However, ζ
has a non-trivial lower bound given appropriate learning rates α and β (see Remark 1 that we will
show later). Second, the parameter c is inversely proportional to α and β. Thus, to guarantee the
constant effect of the ratio c∕ξ, it is better not to set these learning rates too small; see a specification
in Remark 1-2. Third, the parameter V is non-negative and appears in terms of -νR2, thus, it will
not make convergence rate worse. Fourth, P1 is the initial value of the potential function (8). By
setting an appropriate learning rate β (e.g., following Remark 2), P1 is then upper bounded by
a constant determined by the initial value of the objective function, the distance of the first two
updates, Lipschitz constant Ly and strongly concave parameter γ. We next provide Remarks 1-3 on
Theorem 1.
Remark 1. Recall that ζ = min{c1, c2} (Appendix B.2.3), where c1 = 1∕(2β) 一 2L2y∕γ and
c2 = 1 一 (L2x + YLχ + βLx). Given the fact that Lx and Ly are Lipschitz constants and Y is the
strongly concavity constant, a proper lower bound of ζ thus relies on the choice of the learning rates
α and β. By setting β ≤ 辰 and α ≤ 1∕(Lx + YLx + βLx), it is easy to verify that ci ≥ 2Ly and
c2 ≥ 导 + YLβχ + βLLx ≥ L2x + 2Lx. Thus, We obtain that Z ≥ min{2Ly, 2Lx + Lx}. Thisjustifies
that ζ has a non-trivial lower bound, which will not make the convergence error bound (10) vacuous
(although the bound has not been optimized over α and β).
Remark 2. It is not wise to set learning rates α and β to extremely small values since c is inversely
proportional to a and β. Thus, we typically choose β = ^j^ and α = 1∕(Lx + ∣LLx + βLx) in
Remark 1 to guarantee the constant effect of c∕Z.
Remark3. By setting μ ≤ min{ 1∕√d, 1∕√T}, we obtain σ2 = O(1∕b + d∕q) from Lemma 1, and
Theorem 1 implies that ZO-Min-Max yields O(1∕T + 1∕b + d∕q) convergence rate for one-sided
black-box optimization. Compared to the FO rate O(1∕T) (Lu et al., 2019; Sanjabi et al., 2018a),
ZO-Min-Max converges only to a neighborhood of stationary points with O(1∕T) rate, where the
size of the neighborhood is determined by the mini-batch size b and the number of random direction
vectors q used in ZO gradient estimation. It is also worth mentioning that such a stationary gap
may exist even in the FO/ZO projected stochastic gradient descent for solving single-objective
minimization problems (Ghadimi et al., 2016).
As shown in Remark 3, ZO-Min-Max could result in a stationary gap. A large mini-batch size b or
number of random direction vectors q can improve its iteration complexity. However, this requires
O(bq) times more function queries per iteration from (2). It implies the tradeoff between iteration
complexity and function query complexity in ZO optimization.
Convergence rate of ZO-Min-Max by performing ZO-PGA. We now focus on the convergence
analysis of ZO-Min-Max when ZO PGA is used at the y-maximization step (Line 6 of Algorithm 1)
for two-sided black-box optimization problems.
Lemma 4. (Descent lemma in maximization) Under A1-A2, let (x(t), y(t)) be a sequence generated
by Algorithm 1 and define the potential function as
4 + 4(3L2 + 2)β2 一 7βγ
P 0(χ(t), y(t), △州=E[f (x(t), y(t))] + —[	y β2γ_— Ek∆(t) k2.	(11)
When function f(x, y) is black-box w.r.t. both x and y, we have the following descent w.r.t. y:
P,(x(t+1), y(t+1), ∆yt+1)) ≤ P,(x(t+1), y⑷,∆yt)) - (2β - 6L2γ+4) Ek∆yt+1) k2
6LX	3βLX	(t+i)∣∣2 工 7β2γ2 + 28βγ +12 2 βγ + 4 222
+ " + — JEax	k +---------βγ2------σ + 7βvμdLy,	(12)
7
Under review as a conference paper at ICLR 2020
where σ22 := σ2(Ly, μ, b, q, d) given in (3).
Proof: See Appendix A.4.1.
Lemma 4 is analogous to Lemma 3 by taking into account the effect of ZO gradient estimate
Vyf (x, y) on the potential function (11). SUch an effect is characterized by the terms related
to σ∖ and μ2d2Ly in (12).
Theorem 2. Suppose that A1-A2 hold, the sequence (x(t), y(t)) over T iterations is generated by
Algorithm 1 in which learning rates satisfy β < γ∕(4(3Ly + 2)) and a ≤ min{Lχ, 1∕(Lχ∕2 +
6LX∕(γ2β) + 3βL2c∕2)}. When f (x, y) is black-box w.r.t. both X and y, the convergence rate of
ZO-Min-Max under a uniformly and randomly picked (x(r), y(r)) from {(x(t), y(t))}tT=1 is given by
EkG(X(r), y(r))k2 ≤ ζc0 PLf *T-"R +cα σχ + (筌+d2Ly) μ2 + (cb2 + 2)σy,
where Z0 is a ConStant independent on the parameters μ, b, q, d and T, P0 := P0(x(t), y(t), ∆yt))
in (11), C has been defined in (10), ν0 = mm{4+4(3Lβ +2)β -7βγ,", b1 = Lx + dl 气工的) and
b2 = 7β Y +Y8βγ+12, σx and σ2 have been introduced in (7) and (12), and f *, R, Y, Lx and Ly
have been defined in A1-A2.
Proof: See Appendix A.4.2.
Following the similar argument in Remark 1 of Theorem 1, one can choose proper learning rates α and
β to obtain valid lower bound on ζ0. However, different from Theorem 1, the convergence error shown
by Theorem 2 involves an additional error term related to σy2 and has worse dimension-dependence
on the term related to μ2. The latter yields a more restricted choice of the smoothing parameter μ:
We obtain O(1∕T + 1∕b + d∕q) convergence rate when μ ≤ 1∕(d√T).
6	Experiments
In this section, we evaluate the empirical performance of ZO-Min-Max on applications of adversarial
exploration: 1) design of black-box ensemble attack against two neural networks Inception-V3
(Szegedy et al., 2016) and ResNet-50 (He et al., 2016) under ImageNet (Deng et al., 2009), and 2)
design of black-box poisoning attack against a logistic regression model.
Black-box ensemble evasion attack via universal perturbation We consider the scenario in
which the attacker generates adversarial examples against an ensemble of multiple classifiers and/or
image classes (Liu et al., 2016; 2018a). More formally, let (z, l) denote a legitimate image z with the
true class label l, and z0 := z + X denote an adversarial example, where X signifies the adversarial
perturbation. Here the natural image z and the perturbed image z + X are normalized to [-0.5, 0.5]d.
Considering I classes of images (each group of images corresponding to the same class li is denoted
by Ωi) and J network models, the adversary is to find the universal perturbation X across I image
classes and J models. The proposed attack problem is given by
minimize maximize fι(x, W) := Pj=ι PI=ι [wj Fij (x； Ωi,li)] — λ∣∣w — 1/(IJ)k22,	(13)
x∈X	w∈W	j=	=
where X and w ∈ RIJ are optimization variables, and wij denotes the (i, j)th entry of w correspond-
ing to the importance weight of attacking image class i under neural network model j . In problem (13),
X denotes the perturbation constraint, e.g., X = {x ∣∣∣x∣∣∞ ≤ e, z + X ∈ [-0.5,0.5]d, ∀z ∈ ∪iΩi},
W = {w | ITW = 1, W ≥ 0}, Fij (x; Ωi, li) is the attack loss for attacking the set of images at
class li under model j, and λ > 0 is a regularization parameter. We note that {Fij} in (13) are
black-box functions w.r.t. X since the network models are blind to the adversary, which cannot
perform back-propagation to obtain gradients. By contrast, it is a white-box and strongly concave
function w.r.t. w once the function values of {Fij} are given. Thus, problem (13) belongs to the
one-sided black-box optimization problem.
In our experiments, we consider J = 2 for Inception-V3 and ResNet-50, and I = 2 for two classes,
each of which contains 20 images randomly selected from ImageNet (Deng et al., 2009). We also
8
Under review as a conference paper at ICLR 2020
specify the attack loss Fij in (13) as the C&W untargeted attack loss (Carlini & Wagner, 2017),
Fij (x; Ωi,li) = (1∕∣Ωi∣) T max{gj(Z + 乂)源一maxgj(Z + x)k, 0},
k6=li
z∈Ωi	十
(14)
where ∣Ωi | is the cardinality of the set Ωi, gj (Z + x)k denotes the prediction score of class k given
the input z + x using model j. In (13), we also set λ = 5. In Algorithm 1, we set α = 0.05, β =
0.01, q = 10 and μ = 5 X 10-3, and use the full batch of image samples in attack generation.
In experiment, we compare ZO-Min-Max with FO-Min-Max and ZO-Finite-Sum, where the former
is the FO counterpart of Algorithm 1, and the latter is ZO-PSGD (Ghadimi et al., 2016) to minimize
the finite-sum (average) loss rather than the worst-case (min-max) loss. The comparison with ZO-
Finite-Sum was motivated by the previous work on designing the adversarial perturbation against
model ensembles (Liu et al., 2018a) in which the averaging attack loss over multiple models was
considered. Note that although ZO-Finite-Sum consider a different loss function, it is a baseline from
the perspective of attack generation.
de6 XJeUocss
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Number of iterations lβ4
(a)
6 5 4 3 2 1 0
」'sd SS-Y3POE IPea」E
so- w-us<
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Number of iterations lβ*
(b)
6 5 4 3 2 1 0
」'sd SS-Y3POE IPea」E
so- w-us<
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Number of iterations w
(c)
Figure 1:	Convergence performance of ZO-Min-Max in design of black-box ensemble attack. a) Stationary gap
of ZO-Min-Max vs. FO-Min-Max, b) attack loss of using ZO-Min-Max vs. FO-Min-Max, and c) attack loss of
using ZO-Min-Max vs. ZO-Finite-Sum.
In Figure 1, we demonstrate the empirical convergence of ZO-Min-Max to solve problem (13) from
the stationary gap kG(x, y)k2 given in (6) and the attack loss Fij under each model-class pair. In
Figure 1-(a), the stationary gap decreases as the iteration increases, which is consistent with the
reduction in the attack loss at each MjCi. Here M and C represents network model and image class,
respectively. By comparing ZO-Min-Max with FO-Min-Max in Figure 1-(b), we see that the latter
yields faster convergence than the former. However, FO-Min-Max has to access the full knowledge on
the target neural network for computing the gradient of individual attack losses, yielding white-box
attack rather than black-box attack. In Figure 1-(c), We also compare ZO-Min-Max with ZO-Finite-
Sum, where the latter minimizes the average loss PjJ=1 PiI=1 Fij over all model-class combinations.
As we can see, our approach significantly improves the worst-case attack performance (corresponding
to M1C1). Here the worst case represents the most robust model-class pair against the attack. This
suggests that ZO-Min-Max takes into account different robustness levels of model-class pairs through
the design of importance weights w. This can also be evidenced from Figure A2 in Appendix:
M1C1 has the largest weight while M2C2 corresponds to the smallest weight. In Figure A3 of
Appendix, we further contrast the success or failure of attacking each image using the obtained
universal perturbation x with the attacking difficulty (in terms of required iterations for successful
adversarial example) of using per-image non-universal PGD attack (Madry et al., 2017b).
Black-box poisoning attack against logistic regression model Let D = {zi , ti}in=1 denote the
training dataset, among which n0 n samples are corrupted by a perturbation vector x, leading to
poisoned training data zi + x towards breaking the training process and thus the prediction accuracy.
The poisoning attack problem is then formulated as
maximize minimize f2(x, θ) := Ftr(x, θ; D0) + λkθk22,
kxk∞≤	θ
(15)
where x and θ are optimization variables, Ftr (x, θ; D0) denotes the training loss over model parame-
ters θ at the presence of data poison x, and λ > 0 is a regularization parameter. Note that problem
(15) can be written in the form of (1) with the objective function -f2(x, θ). Clearly, if Ftr is a convex
9
Under review as a conference paper at ICLR 2020
loss (e.g., logistic regression or linear regression (Jagielski et al., 2018)), then -f2 is strongly concave
in θ. Since the adversary has no knowledge on the training procedure and data, f2(x, θ) is a two-sided
black-box function. We provide more details on problem (15) in Appendix C. In Algorithm 1, unless
specified otherwise we choose b = 100, q = 5, α = 0.02, β = 0.05, and T = 50000. We report
the empirical results averaged over 10 independent trials with random initialization. We compare
our method with FO-Min-Max and the BO solver for robust optimization STABLEOPT (Bogunovic
et al., 2018) in the data poisoning example of a relatively small problem size.
---ZO-Min-Max: q=l
——ZO-Min-Max: q=5
—ZO-Min-Max: q=10
——ZO-Min-Max: q=20
——FO-Min-Max
3 2 1
de6 ΛJeuo-"κ
0.95
0.90
E 0.85
U 0.80
(O
σι -
2 0.75
品
H 0.70
0.65
0.95
0.90
«0.85
3 0 80
6
≡ 0.75
2
0.70
0.65
(c)	(b)	(c)
Figure 2:	Empirical performance of ZO-Min-Max in design of poisoning attack: a) stationary gap versus
iterations b) testing accuracy versus iterations (the shaded region represents variance of 10 random trials), and c)
testing accuracy versus data poisoning ratio.
In Figure 2, we present the convergence performance of ZO-Min-Max to generate the data poi-
soning attack and validate its attack performance in terms of testing accuracy of the logistic
regression model trained on the poisoned dataset. Unless specified otherwise, we set 15% poi-
soning ratio and λ = 10-3 for problem (15). We examine the sensitivity of the regulariza-
tion parameter λ in Figure A4. Figure 2-(a) shows the stationary gap defined in (6) obtained
by ZO-Min-Max under different number of random direction vectors while estimating gradi-
ents (2). As we can see, a moderate choice of q (e.g., q ≥ 5 in our example) is sufficient
to achieve near-optimal solution compared with FO-Min-Max. However, it suffers from a con-
vergence bias due to the presence of stochastic sampling,
Figure 2-(b) demonstrates the testing accuracy (against itera-
tions) of the model learnt from poisoned training data, where
the poisoning attack is generated by ZO-Min-Max (black-box
attack) and FO-Min-Max (white-box attack). As we can see,
ZO-Min-Max yields promising attacking performance compa-
rable to FO-Min-Max. We can also see that by contrast with the
testing accuracy of the clean model (94% without poison), the
poisoning attack eventually reduces the testing accuracy (below
70%). Furthermore, in Figure 2-(c), we present the testing ac-
curacy of the learnt model under different data poisoning ratios.
As we can see, only 5% poisoned training data can significantly
break the testing accuracy of a well-trained model. In Figure 3,
we compare ZO-Min-Max with STABLEOPT (Bogunovic et al.,
2018) in terms of testing accuracy versus computation time. Fol-
consistent with Theorem 1 and 2.
Figure 3: Comparison between ZO-
Min-Max and STABLEOPT on testing
accuracy versus optimization time.
lowing (Bogunovic et al., 2018), we present the best accuracy achieved up to the current time step.
We observe that STABLEOPT is has a poorer scalability while our method reaches a data poisoning
attack that induces much worse testing accuracy within 500 seconds.
7 Conclusion
This paper addresses black-box robust optimization problems given a finite number of function
evaluations. In particular, we present ZO-Min-Max: a framework of alternating, randomized gradient
estimation based ZO optimization algorithm to find a first-order stationary solution to the black-box
min-max problem. Under mild assumptions, ZO-Min-Max enjoys a sub-linear convergence rate. It
scales to dimensions that are infeasible for recent robust solvers based on Bayesian optimization.
Furthermore, we experimentally demonstrate the potential application of the framework on real-world
scenarios, viz. black-box evasion and data poisoning attacks.
10
Under review as a conference paper at ICLR 2020
References
Charu Aggarwal, Djallel Bouneffouf, Horst Samulowitz, Beat Buesser, Thanh Hoang, Udayan
Khurana, Sijia Liu, Tejaswini Pedapati, Parikshit Ram, Ambrish Rawat, et al. How can ai automate
end-to-end data science? arXiv preprint arXiv:1910.14436, 2019.
Abdullah Al-Dujaili, Erik Hemberg, and Una-May O’Reilly. Approximating nash equilibria for
black-box games: A bayesian optimization approach. arXiv preprint arXiv:1804.10586, 2018a.
Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and Una-May O’Reilly. Adversarial deep learning
for robust detection of binary encoded malware. In 2018 IEEE Security and Privacy Workshops
(SPW),pp. 76-82. IEEE, 2018b.
Abdullah Al-Dujaili, Shashank Srikant, Erik Hemberg, and Una-May O’Reilly. On the application
of danskin’s theorem to derivative-free minimax optimization. arXiv preprint arXiv:1805.06322,
2018c.
Charles Audet and Warren Hare. Derivative-free and blackbox optimization. Springer, 2017.
J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: compressed optimisation
for non-convex problems. ICML, 2018.
Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, and Volkan Cevher. Adversarially robust
optimization with gaussian processes. In Proc. of Advances in Neural Information Processing
Systems, pp. 5765-5775, 2018.
Jurgen Branke and Johanna Rosenbusch. New approaches to Coevolutionary worst-case optimization.
In International Conference on Parallel Problem Solving from Nature, pp. 144-153. Springer,
2008.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39-57. IEEE, 2017.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26. ACM,
2017.
Tianyi Chen and Georgios B Giannakis. Bandit convex optimization for scalable and dynamic IoT
management. IEEE Internet of Things Journal, 2018.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for
non-convex optimization. International Conference on Learning Representations, 2019.
A. R. Conn, K. Scheinberg, and L. N. Vicente. Introduction to derivative-free optimization, volume 8.
Siam, 2009.
John M Danskin. The theory of max-min, with applications. SIAM Journal on Applied Mathematics,
14(4):641-664, 1966.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pp. 248-255. IEEE, 2009.
Amit Dhurandhar, Tejaswini Pedapati, Avinash Balakrishnan, Pin-Yu Chen, Karthikeyan Shanmugam,
and Ruchir Puri. Model agnostic contrastive explanations for structured data. arXiv preprint
arXiv:1906.00117, 2019.
J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono. Optimal rates for zero-order convex
optimization: The power of two function evaluations. IEEE Transactions on Information Theory,
61(5):2788-2806, 2015.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-based
model-agnostic meta-learning algorithms. arXiv preprint arXiv:1908.10400, 2019.
11
Under review as a conference paper at ICLR 2020
Chris Finlay and Adam M Oberman. Scaleable input gradient regularization for adversarial robustness.
arXiv preprint arXiv:1905.11468, 2019.
Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Georgios Piliouras. Effi-
ciently avoiding saddle points with zero order methods: No gradients required. arXiv preprint
arXiv:1910.13021, 2019.
X. Gao, B. Jiang, and S. Zhang. On the information-adaptive variants of the ADMM: an iteration
complexity perspective. Optimization Online, 12, 2014.
S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic program-
ming. SIAMJournalon Optimization, 23(4):2341-2368, 2013.
S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex
stochastic composite optimization. Mathematical Programming, 155(1-2):267-305, 2016.
G. Gidel, T. Jebara, and S. Lacoste-Julien. Frank-Wolfe Algorithms for Saddle Point Problems.
In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,
volume 54, pp. 362-371. PMLR, 20-22 Apr 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
E. Y. Hamedani, A. Jalilzadeh, N. S. Aybat, and U. V. Shanbhag. Iteration complexity of randomized
primal-dual methods for convex-concave saddle point problems. arXiv preprint arXiv:1806.04118,
2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Jeffrey W Herrmann. A genetic algorithm for minimax optimization problems. In CEC, volume 2,
pp. 1099-1103. IEEE, 1999.
A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box adversarial attacks with limited queries and
information. arXiv preprint arXiv:1804.08598, 2018.
Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial
attacks with bandits and priors. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=BkMiWhR5K7.
Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li.
Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In
2018 IEEE Symposium on Security and Privacy (SP), pp. 19-35. IEEE, 2018.
Mikkel T Jensen. A new look at solving minimax problems with coevolutionary genetic algorithms.
In Metaheuristics: computer decision-making, pp. 369-384. Springer, 2003.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Minmax optimization: Stable limit points of
gradient descent ascent are locally optimal. arXiv preprint arXiv:1902.00618, 2019.
Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientific tools for Python,
2001. URL http://www.scipy.org/.
Jeffrey Larson, Matt Menickelly, and Stefan M Wild. Derivative-free optimization methods. Acta
Numerica, 28:287-404, 2019.
J. Liu, Weiming Zhang, and Nenghai Yu. Caad 2018: Iterative ensemble adversarial attack. arXiv
preprint arXiv:1811.03456, 2018a.
S. Liu, J. Chen, P.-Y. Chen, and A. O. Hero. Zeroth-order online admm: Convergence analysis and
applications. In Proceedings of the Twenty-First International Conference on Artificial Intelligence
and Statistics, volume 84, pp. 288-297, April 2018b.
12
Under review as a conference paper at ICLR 2020
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signSGD via zeroth-order oracle. In Proc.
of International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=BJe-DsC5Fm.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples
and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.
S. Lu, I. Tsaknakis, and M. Hong. Block alternating optimization for non-convex min-max prob-
lems: Algorithms and applications in signal processing and communications. In Proc. of IEEE
International Conference on Acoustics, Speech and Signal Processing, pp. 4754-4758, May 2019.
S. Lu, I. Tsaknakis, M. Hong, and Y. Chen. Hybrid block successive approximation for one-sided
non-convex min-max problems: Algorithms and applications. arXiv preprint arXiv:1902.08294,
2019.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant
to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017a.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017b.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Ro-
bustness via curvature regularization, and vice versa. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 9078-9086, 2019.
Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and related
problems. Mathematical Programming, 109(2-3):319-344, 2007.
Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Foundations
of Computational Mathematics, 2(17):527-566, 2015.
M. Nouiehed, M. Sanjabi, J. D. Lee, and M. Razaviyayn. Solving a class of non-convex min-max
games using iterative first order methods. arXiv preprint arXiv:1902.08297, 2019.
Victor Picheny, Mickael Binois, and Abderrahmane Habbal. A bayesian optimization approach to
find nash equilibria. Journal of Global Optimization, 73(1):171-192, 2019.
Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li. Robust optimization over
multiple domains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
pp. 4739-4746, 2019.
H. Rafique, M. Liu, Q. Lin, and T. Yang. Non-convex min-max optimization: Provable algorithms
and applications in machine learning. arXiv preprint arXiv:1810.02060, 2018.
Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms
and comparison of software implementations. Journal of Global Optimization, 56(3):1247-1293,
2013.
M. Sanjabi, J. Ba, M. Razaviyayn, and J. D. Lee. On the convergence and robustness of training
gans with regularized optimal transport. In Proceedings of the 32Nd International Conference on
Neural Information Processing Systems, pp. 7091-7101, 2018a.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and robustness
of training gans with regularized optimal transport. In Advances in Neural Information Processing
Systems, pp. 7091-7101, 2018b.
Tom Schmiedlechner, Abdullah Al-Dujaili, Erik Hemberg, and Una-May O’Reilly. Towards dis-
tributed coevolutionary gans. arXiv preprint arXiv:1807.08194, 2018.
O. Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point
feedback. Journal of Machine Learning Research, 18(52):1-11, 2017.
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks.
In Advances in neural information processing systems, pp. 3517-3529, 2017.
13
Under review as a conference paper at ICLR 2020
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In IEEE Conference on Computer Vision and
PatternRecognition (CVPR),pp. 2818-2826, 2016.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In Advances
in Neural Information Processing Systems, pp. 8000-8010, 2018.
C.-C. Tu, P. Ting, P.-Y. Chen, S. Liu, H. Zhang, J. Yi, C.-J. Hsieh, and S.-M. Cheng. Autozoom:
Autoencoder-based zeroth order optimization method for attacking black-box neural networks.
arXiv preprint arXiv:1805.11770, 2018.
Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of
Mathematics, pp. 265-280, 1945.
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. Neural
Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks, pp. 0, 2019a.
Jingkang Wang, Tianyun Zhang, Sijia Liu, Pin-Yu Chen, Jiacen Xu, Makan Fardad, and Bo Li.
Beyond adversarial training: Min-max optimization in adversarial attack and defense, 2019b.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex
landscapes. In Proceedings of the 36th International Conference on Machine Learning, pp.
6677-6686, 2019.
Richard A Watson and Jordan B Pollack. Coevolutionary dynamics in a minimal substrate. In
Proceedings of the 3rd Annual Conference on Genetic and Evolutionary Computation, pp. 702-
709. Morgan Kaufmann Publishers Inc., 2001.
14
Under review as a conference paper at ICLR 2020
Appendix
A Detailed Convergence Analysis
A. 1 Table of Parameters
In Table A1, we summarize the problem and algorithmic parameters used in our convergence analysis.
Table A1: Summary of problem and algorithmic parameters and their descriptions.
parameter	description
d	# of optimization variables
b	mini-batch size
q	# of random direction vectors used in ZO gradient estimation
α	learning rate for ZO-PGD
β	learning rate for ZO-PGA
CY	strongly concavity parameter of f (x, y) with respect to y
η	upper bound on the gradient norm, implying Lipschitz continuity
Lx, Ly	Lipschitz continuous gradient constant of f (x, y) with respect to X and y respectively
R	diameter of the compact convex set X or Y
f* .	lower bound on the function value, implying feasibility
σX, σ2	variance of ZO gradient estimator for variable X and y respectively
A.2 Proof of Lemma 1
Before going into the proof, let's review some preliminaries and give some definitions. Define hμ (x, ξ)
to be the smoothed version of h(x, ξ) and since ξ models a subsampling process over a finite number
of candidate functions, We can further have hμ(x)，Eξ[hμ(x, ξ)] and Vχhμ(x) = Eξ [Vχhμ(x, ξ)]
Recall that in the finite sum setting when ξj parameterizes the jth function, the gradient estimator is
given by
q
▽ Xh(X)= bq XX
j∈I i=1
d[h(x + μu* ξj) - h(x; ξj)]
--------------------Ui.
(16)
μ
where I is a set with b elements, containing the indices of functions selected for gradient evaluation.
From standard result of the zeroth order gradient estimator, we know
EI	忸 Ui	,i∈[q]	[V χh(x)i	IIi	=	EI	b X Vχfμ(x,	ξj)	= Vχhμ(x).
b j∈I
(17)
Now let’s go into the proof. First, we have
E UVXh(X)-Vχhμ(x)k2]
2
=EI	EUi,i∈[q]
≤2EI	Eui,i∈[q]
Vχh(x) - b X Vχfμ(x, ξj) + b X Vxfμ(x, ξj) - Vχhμ(x)
j∈I	j∈I
II
2
Vxh(x) - 1 X Vxfμ(x, ξj)	+
j ∈I	2
2
b X Vxo(X, ξp - Vxh“(X)
j∈I	2
I .
(18)
2
1-1	.1	1 t C- ∙ .∙	∙	= ©	7 /	∖ ∙	,1	r r-z/-ʌ	1∙	1	∙ ∙	1	1
Further, by definition, given I, Vxh(x) is the average ofZO gradient estimates under q i.i.d. random
directions, each of which has the mean b Pj∈I Vxfμ(x, ξj). Thus for the first term at the right-
15
Under review as a conference paper at ICLR 2020
hand-side (RHS) of the above inequality, we have
Eui,i∈[q]	I IlvXh(X)- b XVχfμ(x,ξj)
1
≤—
q
1
≤—
q
I
2d b x Vxf(χ, ξj)	+ μ2Lhd21
I bj∈I	I	2
(2dη2 + 号)	(19)
where the first inequality is by the standard bound of the variance of zeroth order estimator and the
second inequality is by the assumption that kVxh(x; ξ)k2 ≤ η2 andthus k 1 Pj∈ι Vxf (x, ξj)k2 ≤
η2.In addition, we have
2
EIl Eui,i∈[q]	b X Vxfμ(x, ξj) - Vxhμ(x)
I bj∈I
=EI 1| b X Vxfμ(x, ξj ) -Vxhμ(x)∣ j
=bEξ hkVxf*(X,a-Vxhμ(X)k2] ≤ -b
I
2
(20)
where the second equality is because ξj are i.i.d. draws from the same distribution as ξ and
E[Vxfμ(x,ξ)] = Vxhμ(x), the last inequality is because ∣∣Vxfμ(x,ξ)k2 ≤ η2 by assumption.
Substituting (19) and (20) into (18) finishes the proof.
A.3 Convergence Analysis of ZO-Min-Max by Performing PGA
In this section, we will provide the details of the proofs. Before proceeding, we have the following
illustration, which will be useful in the proof.
The order of taking expectation: Since iterates x(t) , y(t) , ∀t are random variables, we need to
define
F(t) = {x(t), y(t), x(t-1), y(t-1),..., x(1), y(1)}	(21)
as the history of the iterates. Throughout the theoretical analysis, taking expectation means that
we take expectation over random variable at the tth iteration conditioned on F (t-1) and then take
expectation over F (t-1) .
Subproblem: Also, it is worthy noting that performing (4) and (5) are equivalent to the following
optimization problem:
X⑴=min DVxf(x(tT), y(t-1)), X - X(I)E + ɪ ∣∣ X - x(tT) ∣∣2,	(22)
y⑴=maχ DVyf(x⑴,y(t-1)), y - y(t-1)E- &k y - y(t-1) ∣∣2.	(23)
When f(x, y) is white-box w.r.t. y, (23) becomes
y⑴=maχ DVyf(x⑴,y(tT)), y 7* -最k y - y(t-1) ∣∣2.	(24)
In the proof of ZO-Min-Max, we will use the optimality condition of these two problems to derive
the descent lemmas.
16
Under review as a conference paper at ICLR 2020
Relationship with smoothing function We denote by fμ,χ(x, y) the smoothing version of f w.r.t.
X with parameter μ > 0. The similar definition holds for fμ,y(x, y). By taking fμ,χ(x, y) as an
example, under A2 f and fμ,χ has the following relationship (Gao et al., 2014, Lemma4.1):
lf”,χ(X, y) - f (X, y))|	≤	Lxμ-	and	∣∣Vχfμ,χ(X, y)	-Vxf (X, y)k2	≤ μ d4 Lx,	(25)
lf”,y (X, y) - f (X, y))|	≤	Ly2μ-	and	kVyf“,y(X, y	-Vyf (x, y)∣2	≤ " Ly	∙	(26)
First, we will show the descent lemma in minimization as follows.
A.3.1 Proof of Lemma 2
Proof: Since f(X, y) has Lx Lipschtiz continuous gradients with respect to X, we have
fμ(χ(t+1),y⑴) ≤fμ(χ(t), y⑴) + hVxfμ(χ(t),y⑴),X(t+1) -X(t)i + Lxk X(t+I)-X㈤ k2
=fμ(x⑴,y⑴) + hVxf(χ(t), y⑴), χ(t+1) - χ(t)i + LXk X(t+I)-X⑴ k2
+ hVxfμ(χ㈤,y㈤)-Vxf(χ⑴,y㈤),X(t+I)-X⑴〉.	(27)
Recall that
X(t+1) = projX(X(t) - αVbxf(X(t), y(t))),	(28)
From the optimality condition of X-subproblem (22), we have
hVxf (X㈤，y㈤),X(t+1)- X㈤i ≤ -1 k X(t+1)- X㈤ k2.	(29)
α
Here we use the fact that the optimality condition of problem (22) at the solution X(t+1) yields
hVxf (χ(t), y(t)) + (χ(t+1) — χ(t))∕α, χ(t+1) -Xi ≤ 0 for any X ∈ X. By setting X = χ(t), we
obtain (29).
In addition, we define another iterate generated by Vxfμ(χ(t), y(t))
X(t+1) = projχ(χ(t) - αVxfμ(χ(t), y(t))).	(30)
Then, we can have
hVxfμ(χ(t), y(t)) - Vxf (χ(t), y(t)), X(t+1) -χ⑴)
= hVxfμ(χ(t), y(t)) - Vxf (χ(t), y(t)), χ(t+1) - χ(t) -(b(t+1) - X㈤)i
+ hVxfμ(χ(t), y(t)) - Vxf(χ⑴,y㈤), b(t+1) - X㈤i.	(31)
Due to the fact that Eu[Vxf (χ(t), y(t))] = Vxfμ(χ(t), y(t)), we further have
EuKVxfμ(χ(t), y(t)) - Vxf (χ⑴,y㈤),b(t+1) - X㈤i] = 0.	(32)
Finally, we also have
hVxfμ(χ(t), y(t)) - Vxf (χ(t), y(t)), X(t+1) - χ㈤-(X(t+1) - χ(t))i
≤2kVxfμ(χ(t), y(t)) - Vxf(χ(t), y(t))k2 + 2αk χ(t+1) - χ(t) -(b(t+1) - χ(t))k2
≤α∣Vxfμ(X㈤,y㈤) - Vxf(X㈤,y㈤)k2	(33)
where the first inequality is due to Young’s inequality, the second inequality is due to non-
expansiveness of the projection operator. Thus
EuKVxfμ(χ(t), y(t)) - Vxf (χ(t), y(t)), χ(t+1) - χ(t) -(X(t+1) - χ(t))i]
≤Eu[αkVxfμ(χ㈤,y㈤) - Vxf (X㈤,y㈤)k2] ≤ ασx	(34)
17
Under review as a conference paper at ICLR 2020
where σx2 := σ2(Lx, b, q, d) which was defined in (3).
Combining all above, we have
E[fμ(x(t+1), y⑴)]≤E[fμ(x⑴,y⑴)]-(1 - Lx) k x(t+I)-X⑴ ||2 + ασ2,	(35)
and we request α ≤ 1/Lx, which completes the proof.
2
Using ∣fμ,x(x, y) — f(x, y))| ≤ L2μμ-, we can get
E[f (X(t+1),y⑴)]-Lxμ ≤ Effμ(x(t+1),y⑴)]≤ E[f (x(t+1),y㈤)]+ 岑,(36)
so we are able to obtain from (3)
Eff (x(t+1), y(t))] ≤ Eff(x(t), y(t))] — (1 — Lx) k x(t+1) - x(t) k2 + ασ2 + Lχμ2.	(37)
Corollary 1.
E Df(x⑴,y(t-1))-Vfμ(X㈤,y(T)),y⑴-y(T)E ≤ 3端	(38)
σy2 := σ2(Ly, b, q, d) which was defined in (3).
Proof:
Define
歹⑴=projγ(y(t) - βVyfμ(X㈤,y(tT))),	(39)
we have
hVyfμ(X⑴,y(t-1)) - VXf(X⑴,y(t-1)), y(t) - y(t-1)i
=hVyfμ(x(t), y(t-1)) - Vyf (x(t), y(t-1)), y(t) - y(t-1) -(y(t) - y(t-1))i
+ hVy fμ(x(t), y(t-1))-V yf(x㈤,y(t-1)), y㈤-y(t-1)i.	(40)
Due to the fact that Eu[Vyf(x⑴,y(t-1))] = Vyf“(X⑴,y(t-1)), we further have
EufhVyfμ(x㈤,y(t-1)) - Vyf (X㈤,y(t-1)), e(t) - y(t-1)i] = 0.	(41)
Finally, we also have
EufhVyfμ(x(t), y(t-1)) - Vyf (x(t), y(t-1)), y(t) - y(t-1) -(y(t) - y(t-1))i]
≤Euff khVyfμ(x(t), yd) - Vyf(x㈤,y(t-1))k2 +	k y㈤- y(I)-(y㈤- y(T))k2]
≤EufβkVyf“(x㈤,y( j))-Vyf(X㈤,y( j))k2] ≤ 8σ	(42)
where σy2 := σ2(Ly, b, q, d) which was defined in (3).
Next, before showing the proof of Lemma 3, we need the following lemma to show the recurrence of
the size of the successive difference between two iterations.
Lemma 5. Under assumption 1, assume iterates x(t), y(t) generated by algorithm 1. When f (x(t), y)
is white-box, we have
WEk y(t+1) -y(t) k2- WEk y(t) -y(t-1) k2 ≤ 2LIEk x(t+1)- x(t) k2
+ 2Ek y(t+1) - y(t) k2
β
-(4 -邙)Ek y(t)-y(tτ) k2.
(43)
18
Under review as a conference paper at ICLR 2020
Proof: from the optimality condition of y-subproblem (24) at iteration t and t - 1, we have the
following two inequalities:
-hVyf (x(t+1), y⑴)-1(y(t+1) - y(t)), y(t+1) - y(t)i ≤0,
β
hVyf(x(t),y(t-1)) - 1(y(t) - y(t-1)),y(t+1) - y(t)i ≤0.
β
(44)
(45)
Adding the above inequalities, we can get
1 hv(t+1), y(t+1) - y(t)i ≤ DVyf(X(t+1), y⑴)-Vyf (X㈤,y㈤),y(t+1) - y(t))
+ DVyf (X(t), y(t)) - Vyf (X(t), y(t-1)), y(t+1) - y(t)E
where v(t+1) = y(t+1) - y(t) -(y(t) - y(t-1)).
According to the quadrilateral indentity, we know
DV(t+1), y(t+1) -y⑴E = 1 (k y(t+1) -y㈤ ∣∣2 + ∣∣ v(t+1) ∣∣2 - k y(t) - y(t-1)『)
Based on the definition of v(t+1), we substituting (47) into (46), which gives
2⅛k y(t+1) -y(t) k2 ≤21 ky(t) -y(T) k2 - 21.k v(t+1) k2
2β	2β	2β
+ DVyf(X(t+1),y(t)) - Vyf (X(t), y(t)), y(t+1) - y(t)E
+ DVyf(X(t),y(t)) - Vyf (X(t), y(t-1)), y(t+1) - y(t)E
(46)
(47)
(48)
≤)ɪ k y(t) -y(t-1) k2 + DVyf(X(t+1), y⑴)-Vyf (X㈤，y㈤)，y(t+1) -y⑴ E
+ βLy k y(t)-y(T) k2-γ k y(t)-y(T) k2
≤ 2βk y(t) -y(t-1) k2 + 2k y(t+1) -y(t) k2
L2
+ 2Xk x(t+1)- x(t) k2-"
k y(t) - y(t-1) k2
(49)
where in (a) we use the strong concavity of function f(X, y) in y (with parameter γ > 0) and
Young’s inequality, i.e.,
hVyf(X(t), y(t)) - Vyf (X(t), y(t-1)), y(t+1) - y(t)i
=hVyf(X(t), y(t)) - Vyf(X(t), y(t-1)), v(t+1) + y(t) - y(t-1)i
≤βLyk y(t) -y(t-1) k2 + 2βk V(t+1) k2 - γk y(t) -y(t-1) k2	(50)
and in (b) we apply the Young’s inequality, i.e.,
DVyf(X(t+1), y(t)) -Vy f(x ⑶,y㈤),y(t+1) - y⑴E ≤ Lx k x(t+1) -X㈤ k2 + 2 k y(t+1) - y㈤ ∣∣2.
(51)
Therefore, we have
1	1	L2
2βk y(t+1) -y(t) k2 ≤2βk y(t) - y(T) k2 + 由k x(t+1) -x(t) k2
+ 2k y(t+1)-y(t) k2
k y(t) - y(t-1) k2,	(52)
19
Under review as a conference paper at ICLR 2020
which implies
β2γk y(t+1) -y(t) k2 ≤β2γk y(t) - y(t-1) k2 + 笔k x(t+I)-X⑴ k2
+ 2Ily(t+1) -y(t) Il2
I y(t) - y(t-1) I2.	(53)
By taking the expectation on both sides of (53), we can get the results of Lemma 5.
Lemma 5 basically gives the recursion of ∣∣∆yt)∣2. It can be observed that term (4∕β 一 2L：/γ)∣∆yt)k
provides the descent of the recursion when β is small enough, which will take an important role in
the proof of Lemma 3 when we quantify the descent in maximization.
Then, we can quantify the descent of the objective value by the following descent lemma.
A.3.2 Proof of Lemma 3
Proof: let f0 (x(t+1), y(t+1)) = f(x(t+1), y(t+1)) - 1(y(t+1)) and 1(y) denote the indicator function
with respect to the constraint of y. From the optimality condition of sub-problem y in (23), we have
Vy f (x(t+1), y⑴)—1(y(t+1) — y(t)) — ξ(t+1) = 0
β
(54)
where ξ(t) denote the subgradient of 1(y(t)). Since function f0(x, y) is concave with respect to y,
we have
f 0(x(t+1), y(t+1)) — f 0(x(t+1), y(t)) ≤ hVyf (x(t+1), y(t)), y(t+1) — y(t)i — hξ(t), y(t+1) — y(t)i
(=) 1 k y
=βk y'
(t+1) — y(t) I2 — hξ(t) — ξ(t+1), y(t+1) — y(t)i
(t+1) — y(t) I2+ Vyf(x(t+1), y(t)) — Vyf (x(t), y(t-1)), y(t+1) — y(t)
β Dv(t+1),y(t+1) — y(t)E
—
(55)
：(t+1), y⑶)—1 (y(t+1) — y⑴).The last two terms of (55) is
where in (a) We use ξ(t+1) = Vyf (x'
the same as the RHS of (46). We can apply the similar steps from (48) to (49). To be more specific,
the derivations are shown as follows: First, we know
f0(χ(t+1),y(t+1))- f0(χ(t+1),y(t)) ≤ 1 Il y(t+1) — y(t) ∣∣2
β
+ DVyf(X(t+1), y(t)) — Vyf (x⑴,y(t-1)), y(t+1) — y(t))— 1 DV(t+1), y(t+1) — y㈤E . (56)
Then, we move term 1∕βhv(t+1), y(t+1) — y(t)i to RHS of (55) and have
f (x(t+1), y(t+1)) —f(x(t+1),y(t))
≤ 2⅛ k y(t+1) - y(t) k2 + 2⅛ k y(t) — y(tT) k2 - 2⅛ k v(t+1) ∣2
2β	2β	2β
+ DVyf (x(t+1), y(t)) — Vyf (x(t), y(t)), y(t+1) — y(t)E
+ DVyf (x(t), y(t)) — Vyf (x(t), y(t-1)), y(t+1) — y(t)E
≤2βIl y(t+1) - y(t) Il2 + DVyf(X(t+1), y(t)) — Vyf (X⑴,y(t)), y(t+1) - y(t))
+ βLyIl y(t) -y(tT) I2 —yI y(t) -y(tT) Il2
≤) 11y(t+1) - y(t) I2 + 2⅛Iy(t)-y(T)I2
β	2β
+ βLxI x(t+1) - x(t) I2 -(Y- βL2)I y(t) - y(tT) I2	(57)
20
Under review as a conference paper at ICLR 2020
where in (a) we use
hVyf (X(t+1), y(t)) -Vyf(X⑴,y㈤)i ≤ βLxk X(t+1) -x㈤ ∣∣2 + k y(t+1) - y(t) ∣∣2 (58)
which is different from (51); also y(t), y(t+1) ∈ Y so have f0(X(t+1), y(t+1)) = f(X(t+1), y(t+1))
and f0(X(t+1), y(t)) = f (X(t+1), y(t)).
Combing (53), we have
f (X(t+1)，y(t+1))+(W G
≤f (X(t+1), y(t))+(W + 2β)k
—
k y(t+1) - y(t)
(t) k2
k2
(59)
By taking the expectation on both sides of (53), we can get the results of Lemma 3.
Next, we use the following lemma to show the descent of the objective value after solving x-
subproblem by (4).
A.3.3 Proof of Theorem 1
Proof:
From Lemma 3, we know
Ef(X(M,y(t+1))]+f-⅛- + 2⅛E E[k y(t+1) -y㈤ k2]
β γ 2β
-4(1 - Ly) E[k y(t+1) -y(t) k2] ≤ E[f (X(t+1),y⑴)]
+ (β2γ + 21β) E[k y(t) -y(t-1) k2] - 4 (1 - Ly) E[k y(t) - y(t-1) k2]
—
E[k y(t+1) -y(t)	k2]	+ (!Lχ	+	βLx) E[k	X(t+1) -	X(t)	k2].	(60)
Combining Lemma 2, we have
E[f (χ(t+1),y(t+1))]+ (ɪ + 2⅛) E [k y(t+1) -y(t) k2]
β γ	2β
-4 (1 - Ly! E [k y(t+1) -y(t) k2] ≤ E[f (χ(t),y(t))]+ (忘 + t^) E [k y(t) -y(t-1) k2]
-4 (1 - Lŋ E [k y(t)-yJ) k2
E k y(t+1) - y(t) k2
(1 - (Lx + 2LX +
Ia 1 2	γ2β
'------------------
βLx)) E [k X(M-X⑴ k2i + ασ2 + Lxμ2.
(61)
{z
c2
—
}
If
γ
β< 丐 and
X	1
α < Lx + 2LX , βL,
2 + γ2β +	2
(62)
21
Under review as a conference paper at ICLR 2020
then we have that there exist positive constants c1 and c2 such that
P(x(t+1),y(t+1),∆(yt+1)) - P (x(t), y(t), ∆(yt))
≤ - cιE [k y(t+1) - y(t) k2i - c2E [k x(t+1) - x(t) ∣∣2i + ασ2 + L,μ2
≤-Z(E h∣ y(t+1)-y(t) k2i + E h∣ XU)-X㈤ k2]j +ασ2 + Lχμ2	(63)
where ζ = min{c1, c2}.
From (6), we can have
∣G(X(t), y(t))∣
≤1 k x(t+1) - x(t) k + 1 k x(t+I)-ProjX (x(t) -αVχf (x(t), y ⑴))∣ + 11∣ y(t+1) -y ⑴ k
αα	β
+ 1 k y(t+1) -projγ(y(t)+βVyf(X㈤,y(t))k
β
(≤'1 k x(t+1) - x(t) k
α
+ 1 kprojχ(x(t+1) -α(Vχf (x(t), y(t)) + 1(x(t+1) -x㈤))-ProjX(X㈤-αVχf (x㈤,y㈤))k
αα
+ 1 k y(t+1)-y(t) k
β
+ 11IprojY(y(t+1) +β(Vyf(x(t+1),y⑴)-1(y(t+1) -y(t))) -ProjY(y(t)+βVyf(X㈤,y㈤))k
ββ
≤αk x(t+I)-X⑴ k + kVyf(x(t+1),y⑴))-Vyf(x㈤,y㈤))k + 3k y(t+1) -y(t) k
≤) (3 + Lχ) k x(t+1) - x(t) k + 3 k y(t+1) - y(t) k
where in (a) we use x(t+1) = projX (x(t+1) -αVf(x(t+1), y(t)) - (x(t+1) - x(t))); in (b) we use
nonexpansiveness of the projection operator; in (c) we apply the Lipschitz continuous of function
f(x, y) with respect to x and y under assumption A2. Therefore, we can know that there exist a
constant C = max{Lχ + 3, 3} such that
kG(x(t),y(t))k2 ≤ c (k x(t+1) - x(t) k2 + k y(t+1) - y(t) k2 .	(64)
After applying the telescope sum on (63) and taking expectation over (64), we have
T
T XEkG(X㈤,y(t))k2 ≤ Z ( 1 T T +1 + ασX + Lχμ2) .	(65)
t=1
Recall from A1 that f ≥ f * and Y is bounded with diameter R, therefore, Pt given by (8) yields
min{4 + 4β2 L2 - 7βγ, 0}
Pt ≥ f * + —1——焉—R2,	∀t.	(66)
2β2γ
And let (x(r), y(r)) be uniformly and randomly picked from {(x(t), y(t))}tT=1, based on (65) and
(66), we obtain
Er[EkG(x(r),y(r))k2] = T XEkG(X(t),y(t))k2 ≤ C (P1- fT-νR2 + ασ2 + Lχμ2),
t=1
(67)
where recall that Z = min{c1,c2}, C = max{Lχ + 3, ∣} and V
min{4+4β2Ly-7βγ,0}
2β2γ
The proof is now complete.
22
Under review as a conference paper at ICLR 2020
A.4 Convergence Analysis of ZO-Min-Max by Performing ZO-PGA
Before showing the proof of Lemma 4, we first give the following lemma regarding to recursion of
the difference between two successive iterates of variable y.
Lemma 6. Under assumption 1, assume iterates x(t), y(t) generated by algorithm 1. When function
f (x(t) , y) is black-box, we have
β2γEk y(t+1) - y(t) k2 ≤β2γEk y(t) - y(t-1) ∣∣2 + 2Ek y(t+1) - y(t) ∣∣2
+6L2 Ek X(M-X⑴ k2 -(4
βγ2	β
4σy 3 3	ʌ	μ2d2Ly
+ ~∑~ (	+ 4β)+	*.
βγ γ	β2γ
-6LyT Ek y⑴-y J) k2
(68)
From the optimality condition of y-subproblem in (23) at iteration t and t - 1, we have
-gyf(χ(t+1),y(t)) - 1(y(t+1) -y(t)),y(t+1) -y(t))≤0,
χ(t), y(t-1)) - 1(y(t) -y(t-1)),y(t+1) -y(t)∖ ≤0.
β
(69)
(70)
Adding the above inequalities and applying the definition of v(t+1), we can get
βhv(t+1), y(t+1) - y(t)i ≤ Dyf (x(t+1), y(t)) - Vyf (X⑴,y(t)), y(t+1) - y(t))
β	×-----------------------------------------------}
~∙^^^^^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
I
+DVbyf(X(t),y(t)) - Vb yf (X(t), y(t-1)), y(t+1) - y(t)E .
X--------------------------------------------------------/
{z
II
(71)
Next, we will bound E[I] and E[II] separably as follows.
First, we give an upper bound of E[I] as the following,
E Vb yf (X(t+1), y(t)) - Vbyf(X(t), y(t)), y(t+1) - y(t)
≤2γEkVyf (x(t+1), y(t)) -Vyfμ,y(x(t+1), y⑴)k2 + 6Ek y(t+1) - y(t) k2
+ 2γEkVyfμ,y(x(t+1), y㈤)-Vyfμ,y(X⑴,y⑴)∣∣2 + 6Ek y(t+1) - y(t) k2
+ 2γEkVyfμ,y(x㈤,y㈤)-Vfy(x㈤,y㈤)k2 + 6Ek y(t+1) -y(t) k2
≤3σy + 3LxEk x(t+1) - X㈤ k2 + YEk y(t+1) - y㈤ ||2
γ	2γ	2
where Lemma 1 is used.
(72)
23
Under review as a conference paper at ICLR 2020
Second, we need to give an upper bound of E[II] as follows:
Wf (x(t), y(t)) - Vf (x⑴,y(t-1)), y(t+1) - y(t)i
=Wf (x(t),y(t)) — Vf (x(t),y(t-1)), v(t+1) + y(t) -y(t-1)i
= DVf (x(t), y(t)) - Vf (x(t), y(t-1)), y(t) - y(t-1)E
+ <Vf”,y (X ⑴,y(t)) -Vf (x(t), y(t)), y⑴-yj))
+ DVf (x(t), y(t)) - Vfμ,y (X⑴,y(t)), y(t) - y(t-1))
-<Vfμ,y (X⑴,y(tT))- Vf (X㈤,y(I)), y(t) - y(tT)E
-DVf(x⑴,y(tT))- Vf“,y(x(t), y(t-1)), y(t) - y(tT)E
+ hVb f (x(t), y(t)) - Vb f (x(t), y(t-1)), v(t+1)i.
Next, we take expectation on both sides of the above equality and obtain
EhVb f (x(t), y(t)) - Vb f (x(t), y(t-1)), y(t+1) - y(t)i
≤) 33βLy + β[ k y(t) -y(tT) k2 + ɪk v(t+1) k2 - γk y(t) - y(tT) ∣∣2
2	2β
μ2d2L2	9
+〒+4βσ2
where in (a) we use the fact that 1) γ-strong concavity of f with respect to y:
DVf (x(t), y(t)) -Vf(x(t),y(t-1)),y(t)-y(t-1)E≤ -γ∣ y(t) - y(t-1) ∣2;
and the facts that 2) smoothing property (26) and Young’s inequality
E <Vfμ,y(x(t),y(t)) - Vf(X㈤,y⑴),y(t) -y(t-1)) ≤ ^dL + β∣ y(t) -y(tT) ∣2;
8β	2
(73)
(74)
(75)
and the fact that 3) the ZO estimator is unbiased according to Lemma 1
E (Vf(x⑴,y(t)) - Vfμ,y(x㈤,y⑴),y⑴-y(T)) = 0;	(76)
and
E〈Vf“,y(x(t), y(tT))-Vf(X⑴,y(T)), y⑴-y(tT)E ≤ μdLy + β∣ y㈤-y(I) ∣2;
8β	2
(77)
and from Corollary 1 we have
E DVf (X⑴,y(tT))-Vf“,y(X⑴,y(t-1)), y(t)-y(tT)E ≤ βσy	(78)
and
EhVbf(x(t), y(t)) - Vbf(x(t), y(t-1)), v(t+1)i
≤3βE∣Vfμ,y(x㈤,y⑴)-Vf(x⑴,y⑴)k2 + 6β∣ v(t+1) ∣2
+ 3βE∣Vfμ,y(x⑴,y⑴)-Vfμ,y(x⑴,y(t-1))k2 + 6βk v(t+1) ∣2
+ 3βE∣Vfμ,y(X(t),y(tT))- Vf(X㈤,y( j))k2 + 6βk v(t+1) k2
≤3βσy + ɪk v(t+1) k2 + 3βLy k y(t) -y(tT) k2.	(79)
y	2β	2
24
Under review as a conference paper at ICLR 2020
Then, from (71), we can have
2⅛Ek y(t+1) - y(t) k2 ≤2⅛Ek y(t) -y(t-1) k2 - 2⅛EkV(t+1) k2
2β	2β	2β
+ 3σ2 + 3LxEk x(t+1) - x(t) k2 + YEk y(t+1) - y(t) k2
γ	2γ	2
+ DVb f (x(t), y(t)) - Vb f (x(t), y(t-1)), y(t+1) - y(t)E
≤2βEk y(t) - y(t-1) k2 + 2Ek y(t+1) - y(t) k2
+β Ek y(t) - y(t-1) k2
+ 3LyEk x(t+1)- X⑴ k2
2γ
3σ2	Q μ2d2L2
+ -Jy +4βσy2 + 三六,
γ	y 4β
(80)
which implies
β2γEk y(t+1) - y(t) k2 ≤β2γEk y(t) - y(t-1) ∣∣2 + IeEk y(t+1) - y(t) ||2
+βL2 Ek χ(t+I)-X⑴ k2 - (4
+4Y I，/)+卡.
6Lyl) Ek y⑴-yd k2
(81)
—
A.4. 1 Proof of Lemma 4
Proof: Similarly as A.3.2,let f0(x(t+1), y(t+1)) = f (x(t+1), y(t+1)) — 1(y(t+1)), 1(∙) denotes the
indicator function and ξ(t) denote the subgradient of 1(y(t)). Since function f0(x, y) is concave with
respect to y, we have
f0(χ(t+1), y(t+1)) - f (X(t+1), y(t)) ≤ hvf (χ(t+1), y㈤),y(t+1) - y(t)i - hξ(t), y(t+1) - y(t)i
(t+1) - y(t) k2 - hξ(t) - ξ(t+1), y(t+1) - y(t)i
=) 1 k y
=1k y'
(t+1) - y(t) k2 +(Vf (x(t+1), y⑴)-Vf(χ⑴,y(t-1)), y(t+1) -y(t))
—
β Dv(t+1),y(t+1) -y(t)E
where in (a) we use ξ(t+1) = Vf (x(t+1), y⑴)-ɪ(y(t+1) - y(t)). Then, we have
Ef(X(t+1), y(t+1)) - Ef(X(t+1), y⑴)+ 1 Dv(t+1), y(t+1) - y㈤E
≤ 1 k y(t+1) - y(t) k2 + DVf (x(t+1), y(t)) - Vf (x(t), y(t-1)), y(t+1) -y(t)).
Applying the steps from (73) to (80), we can have
Ef (x(t+1), y(t+1)) - Ef(x(t+1), y(t))
≤ 1 Ek y(t+1) - y(t) k2 + 1-Ek y(t) - y(t-1) ∣∣2
β	2β
+ 3β2LxEk XgI)-X㈤ k2 + 7βσy + μ24βLy
(82)
(83)
+β k y(t) - y(t-1) k2
25
Under review as a conference paper at ICLR 2020
where we use
E(Vyf(x(t+1), y(t)) - Vyf(x⑴,y⑴),y(t+1) -y㈤)
≤3βσ2 + 3βLxEk χ(t+1) - χ(t) k2 + 2βEk y(t+1) - y⑴ k2.
(84)
Combing (81), we have
Ef (x(t+1)，y(t+1))+(W TEk y(t+1)-y(t) k2 -
6Lyγ+4) Ek y(t+1) - y(t) k2
—
≤Ef (χ(t+1),y(t)) + (β2γ + 21β) Ek y(t) - y(t-1) k2 - 0 - 6Lγ+^j Ek y(t) -y(t-1) k2
—
+
1
2β
6Ly + 4 ) Ek y(t+1) -y(t) k2 +
+
Ek x(t+1) - x(t) k2.
μ2d2Ly 11 ι
4 + βγ
+(7β+βγ(3+7β)”.
(85)
—
γ
β

A.4.2 Proof of Theorem 2
Proof:
From (37), we know the “descent” of the minimization step, i.e., the changes from
P0(x(t), y(t), ∆(yt)) to P0 (x(t+1), y(t), ∆(yt)). Combining the “descent” of the maximization step by
Lemma 4 shown in (85), we can obtain the following:
P0(x(t+1), y(t+1), ∆(yt+1))
—

—
6L∣T E [k y(t+1)-y(t) k2i
4	-	,
{z
a1
(86)
When β, α satisfy the following conditions:
γ
β < 4(3L2 + 2), and
/	1
< Lx I 6LX 1 WT,
2 十γ2β十 2
(87)
α
we can conclude that there exist b1 , b2 > 0 such that
P0(x(t+1), y(t+1), ∆(yt+1))
≤P0(x(t), y(t), ∆(yt)) -a1E hk y(t+1) - y(t) k2i
-a2 [k x(t+1) -x⑴ ∣∣2i + bιμ2 + ασx2 + b2σy2
≤ - Z0E [k y(t+1) - y㈤ k2 + k x(t+1) - X⑴ k2] + bιμ2 + ασ2 + b2σ2
(88)
where ζ0 = min{a1, a2}.
26
Under review as a conference paper at ICLR 2020
From (6), we can have
EkG(x(t), y(t))k
≤1 Ek x(t+1) -X⑴ k + 1 Ek x(t+I)-ProjX(X⑴一αVχf(x⑴,y⑴))k
α	αX
+ 1 Ek y(t+1) — y(t) k + 1 Ek y(t+1) -ProjY(y(t) +βVyf(x㈤,y(t))k
ββ
≤)1 Ek X(M- X⑴ k + 1 Ek y(t+1)- y⑴ ||
αβ
+ 1 EkProjX(x(t+1) -α(Vχf(x(t), y(t)) + 1(x(t+1) - x(t))) - ProjX(X㈤-αVχf (X㈤,y(t)))k
αα
+ 1 EkProjY(y(t+1) +β(Vyf(x(t+1),y⑴)-1(y(t+1) -y(t))) - ProjY(y㈤+βVyf(x⑴,y(t)))k
ββ
≤ 3Ek x(t+I)-X⑴ k + EkVXf(X⑴,y㈤))-Vxf(X⑴,y⑴))k
α
+ 3Ek y(t+1) -y(t) k + EkVyf(x(t+1), y㈤)-Vyf(x⑴,y⑴)k
β
3
≤-Ek x(t+I)-X⑴ k + EkVxf(x⑴,y㈤))-Vxfμ,y(X⑴,y⑴))k
+ EkVxf“,y(x㈤,y㈤))-Vxf(X㈤,y(t)))k
+ -Ek y(t+1) - y(t) k + EkVyf (x(t+1), y㈤)-Vy fμy(x(t+1), y⑴)k
β
+ EkVyfμ,y (x(t+1), y(t)) - Vy fμ,y(Xt, y(t))k
+ EkVyfμ,y (X⑴,y⑴)-Vy f (X㈤,y(t))k
(≤) (- + Lx) Ek x(t+1) - x(t) k + -Ek y(t+1) - y(t) k + 2σ2 + μ2d2Ly
where in (a) we use the oPtimality condition of x(t)-subProblem; in (b) we use nonexPansiveness
of the Projection oPerator; in (c) we aPPly the LiPschitz continuous of function f(x, y) under
assumPtion A2.
Therefore, we can know that
E [kG(x(t), y(t))k2i ≤ C (k x(t+1) - x(t) k2 + k y(t+1) - y(t) k2) + 2σy + μ2d2L2.	(89)
After aPPlying the telescoPe sum on (88) and taking exPectation over (89), we have
T X E [kG(x(t), y(t))k2i ≤ Z PI-TT +1 + 号μ2 + W + Cb2σ2 + 2σy + 同正.(90)
t=1
Recall from A1 that f ≥ f * and Y is bounded with diameter R, therefore, Pt given by (11) yields
Pt ≥ f * +
min{4 + 4(3L2y + 2)β2 -7βγ,0}
(	β27
∀t.
(91)
And let (x(r), y(r)) be uniformly and randomly Picked from {(x(t), y(t))}tT=1, based on (91) and
(90), we obtain
T
Er [e [kG(x(r), y(r))k2]] = T XE [kG(X⑴,y㈤)k2]
t=1
C P1 - f * - ν0R2	Cb1	Cασ2	Cb2
≤ W P^fr——+ U μ2 + ― + σ + 2σ2 + μ2d2Ly	(92)
where recall that ζ0 = min{aι, a2}, C = max{Lx + 3, 3}, and V = mm{4+4(3Lβ产"-7βγ,0}.
The Proof is now comPlete.
27
Under review as a conference paper at ICLR 2020
B Toy Example in B ogunovic et al. (2018): ZO-Min-Max versus BO
We review the example in Bogunovic et al. (2018) as below,
maximize minimize f(x - δ) := -2(x1 - δ1)6 + 12.2(x1 - δ1)5 - 21.2(x1 - δ1)4
x∈C	kδk2≤0.5
-6.2(x1 - δ1) + 6.4(x1 - δ1)3 + 4.7(x1 - δ1)2 - (x2 - δ2)6
+11(x2 - δ2)5 - 43.3(x2 - δ2)4 + 10(x2 - δ2) + 74.8(x2 - δ2)3
-56.9(x2 - δ2)2 + 4.1(x1 - δ1)(x2 - δ2) + 0.1(x1 - δ1)2 (x2 - δ2)2
-0.4(x2 - δ2)2(x1 - δ1) - 0.4(x1 - δ1)2 (x2 - δ2),
(93)
where x ∈ R2, and C = {x1 ∈ (-0.95, 3.2), x2 ∈ (-0.45, 4.4)}.
Problem (93) can be equivalently transformed to the min-max setting consistent with ours
minimize maximize -f(x - δ).
x∈C	kδk2≤0.5
The optimality of solving problem (93) is measured by regret versus iteration t,
Regret(t) = minimizef (x* — δ) 一 minimizef (x(t) — δ),
kδk2≤0.5	kδk2≤0.5
(94)
(95)
Whereminimizekδ∣∣2≤0.5 f(x* — δ) = —4.33 and x* = [—0.195,0.284]T BogUnoVicetaL (2018).
In Figure A1, we compare the convergence performance and computation time of ZO-Min-Max with
the BO based approach STABLEOPT proposed in BogUnoVic et al. (2018). Here We choose the
same initial point for both ZO-Min-Max and STABLEOPT. And We set the same nUmber of fUnction
qUeries per iteration for ZO-Min-Max (With q = 1) and STABLEOPT. We recall from (2) that the
larger q is, the more qUeries ZO-Min-Max takes. In oUr experiments, We present the best achieVed
regret Up to time t and report the aVerage performance of each method oVer 5 random trials. As We
can see, ZO-Min-Max is more stable, With loWer regret and less rUnning time. Besides, as q becomes
larger, ZO-Min-Max has a faster conVergence rate. We remark that BO is sloW since learning the
accUrate GP model and solVing the acqUisition problem takes intensiVe compUtation cost.
(a)
(b)
Figure A1: Comparison of ZO-Min-Max against STABLEOPT BogUnoVic et al. (2018): a) ConVergence
performance; b) CompUtation time (seconds).
C Experiment Setup on Poisoning Attack
In oUr experiment, We generate a synthetic dataset that contains n = 1000 samples (zi, ti).
We randomly draW the featUre Vector zi ∈ R100 from N(0, I), and determine ti = 1 if
1/(1 + e-(za θ*+Vi)) > 0.5. Here We choose θ* = 1 as the ground-truth model parameters,
and νi ∈ N(0, 10-3) as random noise. We randomly split the generated dataset into the train-
ing dataset Dtr (70%) and the testing dataset Dte (30%). We specify our learning model as the
28
Under review as a conference paper at ICLR 2020
logistic regression model for binary classification. Thus, the loss function in problem (15) is
chosen as Ftr(x, θ; Dtr) := h(x, θ; Dtr,1) + h(0, θ; Dtr,2), where Dtr = Dtr,1 ∪ Dtr,2, Dtr,1 rep-
resents the subset of the training dataset that will be poisoned, |Dtr,1 |/|Dtr| denotes the poisoning
ratio, h(x, θ; D) = -(1∕∣D∣) f(zi,"D [ti log(h(x, θ; Zi)) + (1 - ti)log(1 - h(x, θ; Zi))], and
h(x, θ; zi) = 1/(1 + e-(ai+x)Tθ). In problem (15), we also set = 2 and λ = 10-3. In Algo-
rithm 1, unless specified otherwise we choose the the mini-batch size b = 100, the number of random
direction vectors q = 5, the learning rate α = 0.02 and β = 0.05, and the total number of iterations
T = 50000. We report the empirical results over 10 independent trials with random initialization.
D Additional Experiment Results
In Figure A2, we show how the importance weights w of individual attack losses are learnt during ZO-
Min-Max (vs. FO-Min-Max). We can see that ZO-Min-Max takes into account different robustness
levels of model-class pairs through the design of w.
0.15
0 5 0 5 0
4 3 3 2 2
a
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Number of iterations le4
Figure A2: Convergence of importance weights learnt from ZO-Min-Max vs. FO-Min-Max.
In Figure A3, we contrast the success or failure (marked by blue or red in the plot) of attacking each
image using the obtained universal perturbation x with the attacking difficulty (in terms of required
iterations for successful adversarial example) of using per-image non-universal PGD attack (Madry
et al., 2017b). We observe that the success rate of the ensemble universal attack is around 80% at each
model-class pair, where the failed cases (red cross markers) also need a large amount of iterations to
succeed at the case of per-image PGD attack. And images that are difficult to attack keep consistent
across models; see dash lines to associate the same images between two models in Figure A3.
IO0
“ failure in ensemble attack
• success in ensemble attack
5 4 3 2 1
Ooooo
Illll
SUoqaJl一0 JquJnN
MlCl
M2C1	M1C2	M2C2
Model-class pairs
Figure A3: Success or failure of our ensemble attack versus successful per-image PGD attack.
In Figure A4, we show the testing accuracy of the poisoned model as the regularization parameter λ
varies. We observe that the poisoned model accuracy could be improved as λ increases, e.g., λ = 1.
29
Under review as a conference paper at ICLR 2020
However, this leads to a decrease in clean model accuracy (below 90% at λ = 1). This implies a
robustness-accuracy tradeoff. If λ continues to increase, both the clean and poisoned accuracy will
decrease dramatically as the training loss in (15) is less optimized.
9-8-7-6
Oooo
Auajnuueσ,u=sωh-
0.5
10-4 IO_2 IO0 IO2
Lambda
Figure A4: Empirical performance of ZO-Min-Max in design of poisoning attack: Testing accuracy versus
regularization parameter λ.
30