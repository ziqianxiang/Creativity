Under review as a conference paper at ICLR 2020
Reinforcement Learning with
Probabilistically Complete Exploration
Anonymous authors
Paper under double-blind review
Ab stract
Balancing exploration and exploitation remains a key challenge in reinforcement
learning (RL). State-of-the-art RL algorithms suffer from high sample complexity,
particularly in the sparse reward case, where they can do no better than to explore in
all directions until the first positive rewards are found. To mitigate this, we propose
Rapidly Randomly-exploring Reinforcement Learning (R3L). We formulate explo-
ration as a search problem and leverage widely-used planning algorithms such as
Rapidly-exploring Random Tree (RRT) to find initial solutions. These solutions are
used as demonstrations to initialize a policy, then refined by a generic RL algorithm,
leading to faster and more stable convergence. We provide theoretical guarantees
of R3L exploration finding successful solutions, as well as bounds for its sampling
complexity. We experimentally demonstrate the method outperforms classic and
intrinsic exploration techniques, requiring only a fraction of exploration samples
and achieving better asymptotic performance.
1	Introduction
Reinforcement Learning (RL) studies how agents can learn a desired behaviour by simply using inter-
actions with an environment and a reinforcement signal. Central to RL is the long-standing problem
of balancing exploration and exploitation. Agents must first sufficiently explore the environment to
identify high-reward behaviours, before this knowledge can be exploited and refined to maximize
long-term rewards. Many recent RL successes have been obtained by relying on well-formed reward
signals, that provide rich gradient information to guide policy learning. However, designing such
informative rewards is challenging, and rewards are often highly specific to the particular task being
solved. Sparse rewards, which carry little or no information besides binary success or failure, are
much easier to design. This simplicity comes at a cost; most rewards are identical, so that there is
little gradient information to guide policy learning. In this setting, the sample complexity of simple
exploration strategies was shown to grow exponentially with state dimension in some cases (Osband
et al., 2016b). Intuition behind this phenomenon can be gained by inspecting Figure 1a: exploration
in regions where the return surface is flat leads to a random walk type search. This inefficient search
continues until non-zero gradients are found, which can then be followed to a local optimum.
Planning algorithms can achieve much better exploration performance than random walk by taking
search history into account (Lavalle, 1998). These techniques are also often guaranteed to find a
solution in finite time if one exists (Karaman & Frazzoli, 2011). In order to leverage the advantages
of these methods, we formulate RL exploration as a planning problem in the state space. Solutions
found by search algorithms are then used as demonstrations for RL algorithms, initializing them in
regions of policy parameter space where the return surface is not flat. Figure 1b shows the importance
of such good initialization; surface gradients can be followed, which greatly facilitates learning.
This paper brings the following contributions. We first formulate RL exploration as a planning
problem. This yields a simple and effective method for automatically generating demonstrations
without the need for an external expert, solving the planning problem by adapting the classic
Rapidly-exploring Random Tree algorithm (RRT) (Kuffner & LaValle, 2000). The demonstrations
are then used to initialize an RL policy, which can be refined with a classic RL method such
as TRPO (Schulman et al., 2015). We call the proposed method Rapidly Randomly-exploring
Reinforcement Learning (R3L)1, provide theoretical guarantees for finding successful solutions
1Code will be made available on Github.
1
Under review as a conference paper at ICLR 2020
(a)
(b)
Figure 1: Expected returns achieved by linear policy with 2 parameters on Sparse MountainCar
domain (background). Gradient is 0 in the dark blue area. Trajectories show the evolution of policy
parameters over 1000 iterations of TRPO, with 5 random seeds. Same colors indicate the same
random seeds. (a) Random-walk type behaviour observed when parameters are initialized using
Glorot initialization (Glorot & Bengio, 2010). (b) Convergence observed when parameters are
initialized in a region with gradients (1, 40).
and derive bounds for its sampling complexity. Experimentally, we demonstrate R3L improves
exploration and outperforms classic and recent exploration techniques, and requires only a fraction of
the samples while achieving better asymptotic performance. Lastly, we show that R3L lowers the
variance of policy gradient methods such as TRPO, and verify that initializing policies in regions
with rich gradient information makes them less sensitive to initial conditions and random seed.
The paper is structured as follows: Section 2 analyzes the limitations of classic RL exploration.
Section 3 describes R3L and provides theoretical exploration guarantees. Related work is discussed in
Section 4, followed by experimental results and comments in Section 5. Finally, Section 6 concludes
and gives directions for future work.
2	Sparse-Reward RL as Random Walk
Many recent RL methods are based on a policy gradient optimization scheme. This approach
optimizes policy parameters θ with gradient descent, using a loss function L(θ) (e.g. expected return)
and gradient g(θ) ≡ VθL(θ). Since computing L(θ) exactly is intractable, it is common to use
unbiased empirical estimators L(θ) and g(θ), estimated from samples acquired by executing the policy.
Optimization of θ then follows the common stochastic gradient descent (SGD) update-rule (Bottou,
2010; Robbins & Monro, 1951): θn+1 = θn - Cg (θn), where e is the learning rate.
The SGD update rule defines a discrete-time stochastic process (Mandt et al., 2017). Note that g
is the mean of nmb i.i.d. samples. Following the central limit theorem, the distribution over g is
approximately g(θ)〜N(g(θ), ηɪ^C(θ)), meaning g is an unbiased estimator of g with covariance
nɪ^C(θ). Consequently, the update rule can be rewritten as (Mandt et al., 2017):
θn+1 = θn -Cg(θn) + 七B∆W,	∆W 〜N(0, I).	(1)
Here we assume that C(θ) = C, i.e. approximately constant w.r.t. θ, and factorizes as C = BBT.
SGD is efficient for high-dimensional problems as it offers almost dimension independent convergence
rates (Nesterov, 2018). However, SGD requires non-zero gradients to guide the search towards the
optimum θ*, i.e. |g(θ)| > Cg, ∀θ = θ*, Cg ∈ R. In the case of sparse-reward RL problems, such as in
Figure 1, much of the loss surface is flat. This leads to inefficient exploration of parameter space Θ,
as the drift component in Eq. (1) g ≈ 0, turning the SGD to a random walk in Θ; ∆θ = ^ɪ^B∆W.
Random walk is guaranteed to wander to infinity when dimensionality dθ ≥ 3 (P6lya, 1921; Kakutani,
1944). However, the probability of it reaching a desired region in Θ, e.g. where g 6= 0, depends
heavily on problem specific parameters. The probability of θn ever reaching a sphere of radius r
centered at C such that kC- θ01∣ = R > r is (Dvoretzky & Erdos, 1951):
r dΘ -2
Pr{∣∣θn — Ck < r, for some n > 0}=(方)	< 1,	∀dθ ≥ 3.	(2)
2
Under review as a conference paper at ICLR 2020
In sparse RL problems r < R, thus the probability of reaching a desired region by random walk is
smaller than 1, i.e. there are no guarantees of finding any solution, even in infinite time. This is in
stark contrast with the R3L exploration paradigm, as discussed in Section 3.5.
3 R3L: Rapidly and Randomly-exploring Reinforcement Learning
R3L adapts RRT to the RL setting by formulating exploration as a planning problem in state space.
Unlike random walk, RRT encourages uniform coverage of the search space and is probabilistically
complete, i.e. guaranteed to find a solution (Kleinbort et al., 2019).
R3L is decomposed into three main steps: (i) exploration is first achieved using RRT to generate a
data-set of successful trajectories, described in Sections 3.2 and 3.3, (ii) successful trajectories are
converted to a policy using learning from demonstrations (Section 3.4), and (iii) the policy is refined
using classic RL methods.
3.1	Definitions
This work is based on the Markov Decision Process (MDP) framework, defined as a tuple
< S, A, T, R, γ >. S and Aare spaces of states s and actions a respectively. T : S ×A×S → [0, 1] is
a transition probability distribution so thatT(st, at, st+1) = p(st+1|st, at), where the subscript t indi-
cates the tth discrete timestep. R : S × A×S → R is a reward function defining rewards rt associated
with transitions (st, at, st+1). γ ∈ [0, 1) is a discount factor. Solving a MDP is equivalent to finding
the optimal policy ∏ maximizing the expected return J(π*) = Eτ,∏* [PH=o YtR(st, at, st+ι)] for
some time horizon H, where actions are chosen according to at = ∏* (St). Lastly, let S be a EU-
clidean space, equipped with the standard Euclidean distance metric with an associated norm denoted
by k∙∣∣. The space of valid states is denoted by F ⊆ S.
3.2	Exploration as a planning problem with RRT
The RRT algorithm (LaValle & Kuffner, 2001) provides a principled approach for planning in
problems that cannot be solved directly (e.g. using inverse kinematics), but where it is possible to
sample transitions. RRT builds a tree of valid transitions between states in F, grown from a root s0.
As such, the tree T maintains information over valid trajectories. The exploration problem is defined
by the pair (F, s0). In RL environments with a known goal set Fgoal ⊆ F (e.g. MountainCar), the
exploration problem is defined by (F, s0, Fgoal).
The RRT algorithm starts by sampling a random state srand ∈ S, used to encourage exploration in a
specific direction in the current iteration. This necessitates the first of two assumptions.
Assumption 1. Random states can be sampled uniformly from the MDP state space S.
Sampled states are not required to be valid, thus sampling a random state is typically equivalent to
trivially sampling a hyper-rectangle.
Then, the vertex snear = arg mins∈Tksrand - sneark is found. RRT attempts to expand the
tree T from snear toward srand by sampling an action a ∈ A according to a steering function
Υ : S × S → A. In many planning scenarios, Υ samples randomly from A. A forward step
snew = f (snear, a) is then simulated from snear using action a, where f is defined by the transition
dynamics. Being able to expand the tree from arbitrary snear relies on another assumption.
Assumption 2. The environment state can be set to a previously visited state s ∈ T.
Although this assumption largely limits the algorithm to simulators, it has previously been used
in Florensa et al. (2017); Nair et al. (2018); Ecoffet et al. (2019) for example; see discussion in
Section 6 on overcoming the limitation to simulated environments.
snew is added as a new vertex of T, alongside an edge (snear, snew) with edge information a. This
process repeats until a sampling budget k is exhausted or the goal set is reached (i.e. snew ∈ Fgoal).
Definition 1. A valid trajectory is a sequence τ = [s0, a0, s1, . . . , stτ-1, atτ-1, stτ] such that
(st, at, st+1) is a valid transition and st ∈ F, ∀t ∈ {0, 1, . . . , tτ}. Whenever a goal set is defined, a
successful valid trajectory end state satisfies stτ ∈ Fgoal.
3
Under review as a conference paper at ICLR 2020
Algorithm 1: R3L exploration
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Input: s0 , k: sampling budget
(optional) Fgoal , pg : goal sampling prob.
Output: τ : successful trajectory
Add root node s0 to T
for i = 1 : k do
Srand — sample random state Srand ∈ S
if U 〜U(0,1)≤ Pg then
I Srand J sample Srand from Fgoal
end
Snear J find nearest node to Srand in T
Figure 2: Example of R3L exploration on
sparse MountainCar. Green segments are
sampled transitions, executed in simulation.
A successful solution found by R3L is dis-
played in red. State dimensions are normal-
ized to [-1, 1].
a J sampleπl(Snear,
Srand - Snear)
Snew J execute a in state Snear
update πl with ({Snear , Snew - Snear}, a)
Add node Snew,a and edge (Snear,Snew) to T
end
τ J trajectory in T with max. cumulated reward
Once planning is finished, a successful valid trajectory can easily be generated from T by retrieving
all nodes between the leaf Sleaf ∈ Fgoal and the root. Because tree T is grown from valid transitions
between states in F, these trajectories are valid by construction.
3.3	R3L exploration: Adapting RRT to RL
RRT is not directly applicable to RL problems. This subsection presents the necessary adaptations to
the exploration phase of R3L, summed up in Algorithm 1. Figure 2 shows R3L’s typical exploration
behaviour.
Local policy learning: In classic planning problems, selecting actions extending Snear towards
Srand is easy, as these have a geometric interpretation or there is a known steering function. RL
state spaces do not benefit from the same geometric properties, and properly selecting actions can be
challenging. We solve this problem by defining a local policy πl , which models the distribution of
actions to transition from a state to a neighbouring goal state. Actions to extend a tree branch are
sampled as a 〜πl (Snear, Srand — Snear). We formulate the problem of learning πl as supervised
learning, where inputs are starting states St augmented with the difference St+1 - St , and targets are
actions. The model is learned using transition data collected from previous tree expansion iterations.
Results in this paper use Bayesian linear regression to represent πl, but any supervised learning model
can applied instead.
Unknown dynamics: RRT was designed for problems with known continuous dynamics f , but RL
features unknown discrete transition dynamics. In R3L, f is replaced with an environment interaction
from Snear, with selected action a, resulting in a new state Snew . Since (Snear , a, Snew) is a real
transition, it must be valid, and Snew can be added to the tree T.
Biasing search with Fgoal : Better exploration efficiency can be achieved if goal information is
available. Indeed, the RRT framework allows for biasing exploration towards Fgoal , often resulting in
faster exploration. This is achieved by sampling Srand from Fgoal instead of F with low probability
pg , while the rest of the iteration remains unchanged.
Since R3L uses RRT to explore, the algorithm is most suitable for RL problems that are fully
observable, exhibit sparse rewards and have continuous controls. R3L is applicable to other RL
problems, but may not perform as well as methods tailored to specific cases.
3.4	Policy initialization from R3L demonstrations
Upon completion, R3L exploration yields a successful trajectory τ , which may not be robust to
various starting conditions and/or stochastic transitions. Converting successful trajectories into a
policy is crucial to achieve robustness and enable further refinement with RL.
Policy initialization is applied to a set of successful trajectories τ = {τi }1N generated using N runs
of R3L exploration with different starting conditions. An imitation policy π0 is learned by supervised
4
Under review as a conference paper at ICLR 2020
learning on transitions from τ . Policy π0 is then refined using traditional RL algorithms like TRPO.
As shown in Figure 1, initializing policy parameters in the vicinity of a local optimum is crucial.
3.5	Exploration guarantees
The RL planning environment defines differential constraints of the form:
S = f (s(t),a(t)),	s(t) ∈ F, a(t) ∈ A.	(3)
Therefore, starting at s0, the trajectory τ can be generated by forward integrating Eq. (3) with
the applied actions. As with many RL problems, a(t) is time-discretized resulting in a piecewise
constant control function. This means τ is constructed of nτ segments of fixed time duration ∆t
such that the overall trajectory duration t「= n ∙ ∆t. Thus, a(t) is defined as a(t) = a% ∈ A where
t ∈ [(i - 1) ∙ ∆t, i ∙ ∆t) and 1 ≤ i ≤ n. Furthermore, as all transitions between states in T are
known, the trajectory return can be defined as Rτ = Ptn=τ0 γtR(st, at, st+1).
R3L explores in state-action/trajectory space instead of policy parameter space. Furthermore, it is an
effective exploration framework which provides probabilistic completeness (PC):
Definition 2. A probabilistically complete planner finds a feasible solution (if one exists) with a
probability approaching 1 in the limit of infinite samples.
With the aforementioned dynamic characteristics, we prove that R3L exploration under the RL setting
is PC. This is in stark contrast to the random walk exploration process, discussed in section 2, which
is not PC. We begin with the following theorem, a modification of Theorem 2 from Kleinbort et al.
(2019), which is applied to kinodynamic RRT where a goal set Fgoal is defined.
Theorem 1.	Suppose that there exists a valid trajectory τ from s0 to Fgoal as defined in definition 1,
with a corresponding piecewise constant control. The probability that R3L exploration fails to reach
Fgoal from s0 after k iterations is bounded by ae-bk, for some constants a, b > 0.
The proof, which is a modification of Theorem 2 from Kleinbort et al. (2019), can be found in
Appendix S2. It should be noted that R3L exploration does not require an explicit definition for Fgoal
in order to explore the space. While in some path planning variants of RRT, Fgoal is used to bias
sampling, the main purpose of Fgoal is to indicate that a solution has been found. Therefore, Fgoal
can be replaced by another implicit success criterion. In the RL setting, this can be replaced by a
return-related criterion.
Theorem 2.	Suppose that there exists a trajectory with a return Rτ ≥ R, R ∈ R. The probability
that R3L explorationfails to find a valid trajectoryfrom so with RT ≥ R after k iterations is bounded
by ae-bk, for some constants ^, b > 0.
Proof. The proof is straightforward. We augment each state in τ with the return to reach it from s0 :
s
0
n
sn
Rs
∀n = 1 : nτ ,
(4)
n
where Rsn= = Ptn=≤0nτ γtR(st, at, st+1). For consistency we modify the distance metric by simply
adding a reward distance metric. With the above change in notation, we modify the goal set to
FgRoLal = {(s, Rs)|s ∈ Fgoai,Rs ≥ R}, such that there is an explicit criterion for minimal return as
a goal. Consequently, the exploration problem can be written for the augmented representation as
(F, s0RL, FgRoLal), where s0RL = [s0, 0]>. Theorem 1 satisfies that R3L exploration can find a feasible
solution to this problem within finite time, i.e. PC, and therefore the probability of not reaching FgRoLal
bk
after k iterations is upper-bounded by the exponential term Qe bk, for some constants a,b > 0	■
We can now state our main result on the sampling complexity of the exploration process.
Theorem 3.	If trajectory exploration is probabilistically complete and satisfies an exponential
convergence bound, the expected sampling complexity is finite and bounded such that
E[k] ≤ —⅛,	(5)
4sinh2 b
where a,b > 0.
5
Under review as a conference paper at ICLR 2020
Proof. Theorem 2 provides an exponential bound for the probability the planner will fail in finding a
feasible path. Hence, we can compute a bound for the expected number of iterations needed to find a
solution, i.e. sampling complexity:
∞
E[k] ≤ X kae-bk
k=1
S ʌ de-bk
——a----τ-
上 db
k=1
, d X 6k d d 1
—a -ʌ ) e	——一a -ʌ -τ--
db y	dbeb — 1
k=1
a
4 sinh2
(6)

∖∞	^‰
where We used the relation Pk=I e bk ——^^1^.
It is worth noting that while the sample complexity is bounded, the above result implies that its
complexity varies according to problem-specific properties, which are encapsulated in the value of
^ and b. Intuitively, a depends on the scale of the problem. It grows as |FgR£ | becomes smaller or
as the length of the solution trajectory becomes longer. b depends on the probability of sampling
states that will expand the tree in the right direction. It therefore shrinks as the dimensionality of
S increases. We refer the reader to Appendix S2 for more details on the meaning of a, b and the
derivation of the tail bound in Theorem 1.
4 Related work
Exploration in RL has been extensively studied. Classic techniques typically rely on adding noise
to actions (Mnih et al., 2015; Schulman et al., 2015) or to policy parameters (Plappert et al., 2018).
However, these methods perform very poorly in settings with sparse rewards.
Intrinsic motivation tackles this problem by defining a new reward to direct exploration. Many
intrinsic reward definitions were proposed, based on information theory (Oudeyer & Kaplan, 2008),
state visit count (Lopes et al., 2012; Bellemare et al., 2016; Szita & LOrinCz, 2008; Fox et al., 2018),
value function posterior variance (Osband et al., 2016a; Morere & Ramos, 2018), or model prediction
error (Stadie et al., 2015; Pathak et al., 2017). Methods extending intrinsic motivation to continuous
state and action spaces were recently proposed (Houthooft et al., 2016; Morere & Ramos, 2018).
However, these approaches are less interpretable and offer no guarantees for the exploration of the
state space.
Offering exploration guarantees, Bayesian optimization was adapted to RL in Wilson et al. (2014),
to search over the space of policy parameters in problems with very few parameters. Recent work
extends the method to functional policy representations (Vien et al., 2018), but results are still limited
to toy problems and specific policy model classes.
Motion planning in robotics is predominantly addressed with sampling-based methods. This type of
approach offers a variety of methodologies for exploration and solution space representation (e.g.,
Probabilistic roadmaps (PRM) (Kavraki et al., 1996), Expansive space trees (ESP) (Hsu et al., 1997)
and Rapidly-exploring random tree (RRT) (Kuffner & LaValle, 2000)), which have shown excellent
performance in path planning in high-dimensional spaces under dynamic constraints (LaValle &
Kuffner, 2001; Hsu et al., 2002; Kavraki et al., 1996).
RL was previously combined with sampling-based planning to replace core elements of planning
algorithms, such as PRM’s point-to-point connection (Faust et al., 2018), local RRT steering func-
tion (Chiang et al., 2019) or RRT expansion policy (Chen et al., 2019). In contrast, the proposed
method bridges the gap in the opposite direction, employing a sampling-based planner to generate
demonstrations that kick-start RL algorithms and enhance their performance.
Accelerating RL by learning from demonstration was investigated in Niekum et al. (2015); Bojarski
et al. (2016); Torabi et al. (2018). However, these techniques rely on user-generated demonstra-
tions or a-priori knowledge of environment parameters. In contrast, R3L automatically generates
demonstrations, with no need of an external expert. 5
5	Experiments
In this section, we investigate (i) how learning a local policy πl and biasing search towards Fgoal with
probability pg affects R3L exploration, (ii) whether separating exploration from policy refinement
6
Under review as a conference paper at ICLR 2020
Table 1: Impact of learning local policy πl and biasing search towards Fgoal with probability pg on
R3L exploration. Results show the mean and standard deviation of successful trajectory length ∣τ|
and number of timesteps required, computed over 20 runs.
		Goal bias (Pg Learned ∏ι	= 0.05) Random ∏ι	Unbiased (Pg = 0)	
				Learned ∏ι	Random πl
MountainCar		84.75 ± 5.47	=	131.90 ± 17.91	86.75 ± 11.82	139.85 ± 18.79
	timesteps	895.65 ± 190.70	4303.80 ± 681.60	928.90 ± 204.0	4447.55 ± 417.10
Pendulum		73.10 ± 12.86	75.35 ± 14.50	-67.05 ± 15.30	77.90 ± 12.62
	timesteps	1108.65 ± 155.29	2171.95 ± 381.20	1221.35 ± 216.14	2349.20 ± 249.26
Acrobot	∣τ I	177.55 ± 20.66	-163.95 ± 19.19	173.5 ± 24.22	169.05 ± 17.07
	timesteps	15422.00 ± 2624.16	12675.20 ± 2652.39	15792.65 ± 3182.77	13133.55 ± 2060.51
Cartpole Swingup	∣τ∣	217.35 ± 53.09	-319.20 ± 58.78	235.70 ± 70.06	348.35 ± 80.09
	timesteps	17502.75 ± 13923.82	27186.70 ± 12246.32	23456.25 ± 16792.11	34482.20 ± 12034.27
Reacher	∣τ∣	32.70 ± 13.55	22.05 ± 8.81	32.25 ± 12.05	25.30 ± 11.02
	timesteps	1445.80 ± 1314.48	838.85 ± 846.94	1423.00 ± 1030.07	1034.55 ± 1208.67
is a viable and robust methodology in RL, (iii) whether R3L reduces the number of exploration
samples needed to find good policies, compared with methods using classic and intrinsic exploration,
and (iv) how R3L exploration can reduce the variance associated with policy gradient methods.
All experiments make use of the Garage (Duan et al., 2016) and Gym (Brockman et al., 2016)
frameworks. The experimental setup features the following tasks with sparse rewards: Cartpole
Swingup (S ⊆ R4 , A ⊆ R), MountainCar (S ⊆ R2 , A ⊆ R), Acrobot (S ⊆ R4 , A ⊆ R), Pendulum
(S ⊆ R2,A ⊆ R), Reacher (S ⊆ R6,A ⊆ R2)FetchReach (S ⊆ R13,A ⊆ R4), and Hand Reach
(S ⊆ R78 , A ⊆ R20). The exact environment and reward definitions are described in Appendix S3.
R3L exploration analysis We first analyze the exploration performance of R3L in a limited set of
RL environments, to determine the impact that learning policy πl has on exploration speed. We also
investigate whether R3L exploration is viable in environments where no goal information is available.
Table 1 shows the results of this analysis. Learning πl seems to greatly decrease the number of
exploration timesteps needed on most environments. However, it significantly increases the number
of timesteps on the acrobot and reacher environments. Results also suggest that learning πl helps
R3L to find shorter trajectories on the same environments, which is a desirable property in many RL
problems. Biasing R3L exploration towards the goal set Fgoal helps finding successful trajectories
faster, as well as reducing their length. However, R3L exploration without goal bias is still viable in
all cases. Although goal information is not given in the classic MDP framework, it is often available
in real-world problems and can be easily utilized by R3L. Lastly, successful trajectory lengths have
low variance, which suggests R3L finds consistent solutions.
Comparison to classic and intrinsic exploration on RL benchmarks We examine the rates at
which R3L learns to solve several RL benchmarks, and compare them with state-of-the-art RL
algorithms. Performance is measured in terms of undiscounted returns and aggregated over 10
random seeds, sampled at random for each environment. We focus on domains with sparse rewards,
which are notoriously difficult to explore for traditional RL methods. Our experiments focus on the
widely-used methods TRPO (Schulman et al., 2015) and DDPG (Lillicrap et al., 2015). R3L-TRPO
and R3L-DDPG are compared to the baseline algorithms with Gaussian action noise. As an additional
baseline we include VIME-TRPO (Houthooft et al., 2016). VIME is an exploration strategy based on
maximizing information gain about the agent’s belief of the environment dynamics. It is included to
show that R3L can improve on state-of-the-art exploration methods as well as naive ones, even though
the return surface for VIME-TRPO is no longer flat, unlike Figure 1. The exact experimental setup is
described in Appendix S3.2. The R3L exploration phase is first run to generate training trajectories
for all environments. The number of environment interactions during this phase is accounted for in
the results, displayed as an offset with a vertical dashed black line. The average performance achieved
by these trajectories is also reported as a guideline, with the exception of Cartpole Swingup where
doing so does not make sense. RL is then used to refine a policy pretrained with these trajectories.
Figure 3 shows the median and interquartile range for all methods. R3L is very competitive with
both vanilla and VIME baselines. It converges faster and achieves higher performance at the end of
the experiment. In most cases, the upper quartile for our method begins well above the minimum
return, indicating that R3L exploration and pre-training are able to produce successful though not
optimal policies. R3L-DDPG performance, for the majority of problems, starts significantly above the
minimum return, plunges due to the inherent instability of DDPG, but eventually recovers, indicating
that R3L pre-training can help mitigate the instability. It is worth noting that R3L’s lower quartile
is considerably higher than that of baselines. Indeed, for many of the baselines the lower quartile
7
Under review as a conference paper at ICLR 2020
0.0	0.5	1.0	1.5	2.0
Timesteps (1e6)
Pendulum
0	12	3
Timesteps (1e6)
Hand Reach
O 2	4	6	8	10
Timesteps (1e6)
Reacher
Fetch Reach (Zoomed)
4	6
Timesteps (1e6)
R3L-TRPO	---- TRPO
R3L-DDPG	---- DDPC
R3L demonstration ------ VIME-TRPO
Timesteps (1e6)
Fetch Reach
^ O	1	2	3	4	^ D.0	0.2	0.4	0.6	0.8	1.0
Timesteps (1 e6)	Timesteps (1 e6)
Figure 3: Results for classic control tasks, comparing our proposed method (R3L-TRPO/DDPG),
vanilla TRPO/DDPG, and VIME-TRPO. Trendlines are the medians and shaded areas are the
interquartile range, taken over 10 randomly chosen seeds. Also shown is the average undiscounted
return of successful trajectories generated with R3L exploration. The dashed offset at the start of
R3L-TRPO/DDPG reflects the number of timesteps spent on R3L exploration.
takes a long time to improve on the minimum return, and in some cases it never manages to do so
at all. This is a common problem in sparse reward RL, where there is no alternative but to search
the parameter space randomly until the first successful trajectory is found, as explained in Section 2.
While a few random seeds will by chance find a successful trajectory quickly (represented by the
quickly rising upper quartile), others take a long time (represented by the much slower rise of the
median and lower quartile). In other words, R3L-TRPO/DDPG is much more robust to random
policy initialization and to the random seed than standard RL methods. This is because R3L is able
to use automatically generated demonstrations to initialize RL policy parameters to a region with
informative return gradients.
6	Conclusion
We proposed Rapidly Randomly-exploring Reinforcement Learning (R3L), an exploration paradigm
for leveraging planing algorithms to automatically generate successful demonstrations, which can
be converted to policies then refined by classic RL methods. We provided theoretical guarantees
of R3L finding solutions, as well as sampling complexity bounds. Empirical results show that R3L
outperforms classic and intrinsic exploration techniques, requiring only a fraction of exploration
samples and achieving better asymptotic performance.
As future work, R3L could be extended to real-world problems by leveraging recent advances on
bridging the gap between simulation and reality (Peng et al., 2018). Respecting Assumption 2, a
policy would first be trained on a simulator and then transferred to the real-world. Exploration in
high-dimensional tasks is also challenging as stated in Theorem 3 and confirmed experimentally
by increased R3L exploration timesteps. Exploiting external prior knowledge and/or the structure
of the problem can benefit exploration in high-dimensional tasks, and help make R3L practical for
problems such as Atari games. Lastly, recent advances in RRT (Chiang et al., 2019) and learning
from demonstration (Torabi et al., 2018) could also improve R3L.
8
Under review as a conference paper at ICLR 2020
References
R. Arratia and L. Gordon. Tutorial on large deviations for the binomial distribution. Bulletin of
Mathematical Biology, 1989.
M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based
exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, 2016.
M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort,
U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Ziebaand. End to end learning for self-driving cars.
NIPS Deep Learning Symposium, 2016.
L.N. Bottou. Large-scale machine learning with stochastic gradient descent. In International
Conference on Computational Statistics, 2010.
G.	Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI
Gym, 2016.
B. Chen, B. Dai, and L. Song. Learning to plan via neural exploration-exploitation trees.
arXiv:1903.00070, 2019.
H.	Lewis Chiang, J. Hsu, M. Fiser, L. Tapia, and A. Faust. RL-RRT: Kinodynamic motion planning
via learning reachability estimators from RL policies. Robotics and Automation Letters, 4, 2019.
Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement
learning for continuous control. In International Conference on Machine Learning, 2016.
A. Dvoretzky and P. Erdos. Some problems on random walk in space. In Proceedings ofthe Second
Berkeley Symposium on Mathematical Statistics and Probability, 1951.
A. Ecoffet, J Huizinga, J Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach for
hard-exploration problems. arXiv:1901.10995, 2019.
A. Faust, K. Oslund, O. Ramirez, A. Francis, L. Tapia, M. Fiser, and J. Davidson. PRM-RL: Long-
range robotic navigation tasks by combining reinforcement learning and sampling-based planning.
In International Conference on Robotics and Automation, 2018.
C.	Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel. Reverse curriculum generation for
reinforcement learning. In Conference on Robot Learning, 2017.
L. Fox, L. Choshen, and Y. Loewenstein. DORA the explorer: Directed outreaching reinforcement
action-selection. In International Conference on Learning Representations, 2018.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In International Conference on Artificial Intelligence and Statistics, 2010.
R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational
information maximizing exploration. In Advances in Neural Information Processing Systems,
2016.
D.	Hsu, J.C. Latombe, and R. Motwani. Path planning in expansive configuration spaces. In
International Conference on Robotics and Automation, 1997.
D.	Hsu, R. Kindel, J.C. Latombe, and S. Rock. Randomized kinodynamic motion planning with
moving obstacles. The International Journal of Robotics Research, 2002.
S. Kakutani. On brownian motions in n-space. Proceedings of the Imperial Academy, 1944.
S. Karaman and E. Frazzoli. Sampling-based algorithms for optimal motion planning. The Interna-
tional Journal of Robotics Research, 2011.
L.	E. Kavraki, P. Svestka, J. C. Latombe, and M. H. Overmars. Probabilistic roadmaps for path
planning in high-dimensional configuration spaces. Transactions on Robotics and Automation,
1996.
9
Under review as a conference paper at ICLR 2020
M.	Kleinbort, K. Solovey, Z. Littlefield, K. E. Bekris, and D. Halperin. Probabilistic completeness of
RRT for geometric and kinodynamic planning with forward propagation. Robotics and Automation
Letters, 2019.
J.	J. Kuffner and S. M. LaValle. RRT-connect: An efficient approach to single-query path planning.
In International Conference on Robotics and Automation, 2000.
S.	M. Lavalle. Rapidly-exploring random trees: A new tool for path planning. Technical report,
Department of Computer Science. Iowa State University., 1998.
S.	M. LaValle and J. J. Kuffner. Randomized kinodynamic planning. The International Journal of
Robotics Research, 2001.
T.	P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, et al. Continuous control with deep reinforcement
learning. arXiv:1509.02971, 2015.
M. Lopes, T. Lang, M. Toussaint, and P.Y. Oudeyer. Exploration in model-based reinforcement
learning by empirically estimating learning progress. In Advances in Neural Information Processing
Systems, 2012.
S. Mandt, M. D. Hoffman, and D. M. Blei. Stochastic gradient descent as approximate bayesian
inference. The Journal of Machine Learning Research, 18, 2017.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 2015.
P. Morere and F. Ramos. Bayesian RL for goal-only rewards. In Conference on Robot Learning,
2018.
A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration
in reinforcement learning with demonstrations. In International Conference on Robotics and
Automation, 2018.
Y. Nesterov. Lectures on Convex Optimization. Springer, 2018.
S. Niekum, S. Osentoski, G. Konidaris, S. Chitta, et al. Learning grounded finite-state representations
from unstructured demonstrations. The International Journal of Robotics Research, 34, 2015.
I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped DQN. In
Advances in Neural Information Processing Systems, 2016a.
I. Osband, B. Van Roy, and Z. Wen. Generalization and exploration via randomized value functions.
In International Conference on Machine Learning, 2016b.
P. Y. Oudeyer and F. Kaplan. How can we define intrinsic motivation? In International Conference
on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems, 2008.
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised
prediction. In International Conference on Machine Learning, 2017.
X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control
with dynamics randomization. In International Conference on Robotics and Automation, 2018.
M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen, T. Asfour, P. Abbeel, and
M. Andrychowicz. Parameter space noise for exploration. In International Conference on Learning
Representations, 2018.
G.	P6lya. Uber eine aufgabe der Wahrscheinlichkeitsrechnung betreffend die irrfahrt im straβennetz.
Mathematische Annalen, 1921.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural
Information Processing Systems, 2008.
10
Under review as a conference paper at ICLR 2020
H.	Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics,
pp. 400-407, 1951.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
International Conference on Machine Learning, 2015.
B. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep
predictive models. arXiv:1507.00814, 2015.
I.	Szita and A. Lorincz. The many faces of optimism: a unifying approach. In International
Conference on Machine learning, 2008.
F. Torabi, G. Warnell, and P. Stone. Behavioral cloning from observation. In International Joint
Conference on Artificial Intelligence, 2018.
C. Urmson and R. Simmons. Approaches for heuristically biasing RRT growth. In International
Conference on Intelligent Robots and Systems, 2003.
N. A. Vien, H. Zimmermann, and M. Toussaint. Bayesian functional optimization. In AAAI
Conference on Artificial Intelligence, 2018.
A. Wilson, A. Fern, and P. Tadepalli. Using trajectory data to improve bayesian optimization for
reinforcement learning. The Journal of Machine Learning Research, 2014.
11
Under review as a conference paper at ICLR 2020
Reinforcement Learning with Probabilistically
Complete Exploration: Supplementary Material
S1 Appendix A: RRT algorithm psuedo-code
In this section, we provide pseudo-code of the classic RRT algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Algorithm S1: Rapidly-exploring Random Trees (RRT)
Input： Sinit
k: sampling budget
δt: Euler integration time interval
Output: T
T.init(sinit)
for i = 1 : k do
Srand J RANDOM_UNIFORM(S)
if srand ∈/ F then
I pass
end
Snear J arg mins∈T kSrand - Sneark	/* Find nearest vertex */
a J Υ(Snear, Srand)	/* Sample action */
Snew J Snear + ∆t ∙ f (Snear, a)	/* Propagate to new state, Eq. (3) */
if VALID_TRANSITION(Snear, Snew) then
T.add_vertex(Snew)
T.add_edge(Snear, * Snew )
end
end
S2	Appendix B: Proof of Theorem 1
This appendix proves Theorem 1 which shows that planning using RRT under differential constraints
is probabilistically complete. The following proof is a modification of Theorem 2 from Kleinbort et al.
(2019), where completeness of RRT in the RL setting is maintained without the need to explicitly
sample a duration for every action.
Equation 3 defines the environment’s differential constraints. In practice, Eq. (3) is approximated
by an Euler integration step. With the interval [0, tτ] divided into l >> nτ equal time intervals of
duration h with t「 = l ∙ h. Eq. (3) is then approximated by an Euler integration step, where the
transition between consecutive time steps is given by:
Sn+1 = Sn + f (Sn, an ) ∙ h, Sn, Sn+1 ∈ τ,
S.t. lim ∣∣Sn — T(n ∙ h)k = 0.	(SI)
l→∞,h→0
Furthermore, we define Br(S) as a ball with a radius r centered at S for any given state S ∈ S.
We assume that the planning environment is Lipschitz continuous in both state and action, constraining
the rate of change of Eq. (S1). Formally, there exists two positive constants Ks , Ka > 0, such that
∀S0 , S1 ∈ F, a0 , a1 ∈ A :
∣f(S0, a0) — f(S0, a1)∣ ≤ Ka∣a0 — a1∣,	(S2)
∣f(S0, a0) — f(S1, a0)∣ ≤ Ks∣S0 — S1∣.	(S3)
Lemma 1. For two trajectories	τ,	τ0,	where S0	=	τ(0) and S00	=	τ0 (0) such that ∣S0 —	S00∣	≤	δs
with δs a positive constant. Suppose that for each trajectory a piecewise constant action is applied,
so that Υ(t) = a and Υ0(t) = a0 is fixed during a time period T ≥ 0. Then ∣τ(T) — τ0(T)∣ ≤
eKsT δs + TKaeKsTlla — a0∣∣.
1
Under review as a conference paper at ICLR 2020
The proof for Lemma 1 is given in Lemma 2 of (Kleinbort et al., 2019). Intuitively, this bound is
derived from compounding worst-case divergence between τ and τ0 at every Euler step along T
which leads to an overall exponential dependence.
Using Lemma 1, we want to provide a lower bound on the probability of choosing an action that will
expand the tree successfully. We note that this scenario assumes that actions are drawn uniformly
from A, i.e. there is no steering function 2 * *. When better estimations of the steering function are
available, as described in 3.3, the performance of RRT significantly improves.
Definition 3. A trajectory τ is defined as δ-clear if for δclear > 0, Bδclear (τ (t)) ∈ F for all
t ∈ [0, tτ].
Lemma 2. Suppose that τ is a valid trajectory from τ (0) = s0 to τ (tτ ) = sgoal with a duration of
tτ and a clearance of δ. Without loss of generality, we assume that actions are fixed for all t ∈ [0, tτ],
such that Υ(t) = a ∈ A.
Suppose that RRT expands the tree from a state s00 ∈ B(κδ-) (s0) to a state s0goal, for any κ ∈ (0, 1]
and ∈ (0, κδ) we can define the following bound:
Z	κδ-eKstτ (κδ-e)
Pr[sgoal ∈ Bκδ (Sgoal))] ≥	K^ …
Here, Z∣s∣ = ∣Bι(∙)∣ is the Lebesgue measurefor a unit circle in S.
Proof. We denote τ0 a trajectory that starts from s00 and is expanded with an applied random action
arand. According to Lemma 1,
kτ(t)-τ0(t)k ≤ eKstδs+tKaeKstka-arandk ≤ eKst(κδ-E)+tKaeKstka-arandk,	∀t ∈ [0, tτ],
where δs ≤ κδ - E since s00 ∈ Bκδ-(s0). Now, we want to find ka - arand k such that the distance
between the goal points of these trajectories, i.e. in the worst-case scenario, is bounded:
eKstτ(κδ - E) + tτKaeKstτ ka - arand k < κδ.
After rearranging this formula, we can obtain a bound for ka - arand k:
κδ - eKstτ (κδ - E)
δ0 = k a - arandk <	χ^^K Kt	.
Assuming that arand is drawn out of a uniform distribution, the probability of choosing the proper
action is
κ	κδ-eKstτ (κδ-e)
4|S|	tτ KaeKstT
A
pa
(S4)
where Z∣s∣ is used to account for the degeneracy in action selection due to the dimensionality of S.
We note that E ∈ (0, κδ) guarantees Pa ≥ 0, thus a valid probability>	■
Equation S4 provides a lower bound for the probability of choosing the suitable action. The following
lemma provides a bound on the probability of randomly drawing a state that will expand the tree
toward the goal.
Lemma 3. Let s ∈ S be a state with clearance δ, i.e. Bδ (s) ∈ F. Suppose that for an RRT tree
T there exist a vertex V ∈ T such that V ∈ B2δ∕5(s). Following the definition in Section 3.2, we
denote snear ∈ T as the closest vertex to srand. Then, the probability that snear ∈ Bδ (s) is at least
瓦/5 (s)l∕∣s∣.
Proof. Let Srand ∈ Bδ∕5(s). Therefore the distance between Srand and V is upper-bounded by
ksrand - Vk ≤ 3δ ∕5. If there exists a vertex snear such that snear 6= V and ksrand - snear k ≤
Il srand - v k, then Snear ∈ B3δ∕5 (srand) ⊂ Bδ (S). Hence, by choosing Srand ∈ Bδ∕5 (S), we are
guaranteed Snear ∈ Bδ(s). AS Srand is drawn uniformly, the probability for Srand ∈ Bδ∕5(S is
∣Bδ∕5 (s)I∕∣s∣.	■
2The function steer : S × S → A returns an action asteer given two states srand and snear such that
asteer = arg mina∈AkSrand - (Snear + ∆t ∙ f(snear,a))k s.t. ∣∣∆t ∙ f(snear,a) k < 〃，for a prespecified
η > 0 (Karaman & Frazzoli, 2011).
2
Under review as a conference paper at ICLR 2020
We can now prove the main theorem.
Theorem 1. Suppose that there exists a valid trajectory τ from s0 to Fgoal as defined in definition 1,
with a corresponding piecewise constant control. The probability that RRT fails to reach Fgoal from
s0 after k iterations is bounded by ae-bk, for some constants a, b > 0.
Proof. Lemma 2 puts bound on the probability to find actions that expand the tree from one state
to another in a given time. As we assume that a valid trajectory exists, we can assume that the
probability defined in Lemma 2 is non-zero, i.e. pa > 0, hence:
κδ - eKs∆t(κδ - ) > 0,	(S5)
where we set κ = 2/5 and = 5-2 as was also done in (Kleinbort et al., 2019). We additionally
require that ∆t, which is typically defined as an RL environment parameter, is chosen accordingly so
to ensure that Eq.(S5) holds, i.e. Ks∆t < log (κδ⅛).
We cover τ with balls of radius δ = min{δgoal , δclear}, where Bδgoal ⊆ Fgoal. The balls are spaced
equally in time with the center of the ith ball is in Ci = T(∆t ∙ i), ∀i = 0 : m, where m = tr/∆t.
Therefore, c0 = s0 and cm = sgoal. We now examine the probability of RRT propagating along
τ. Suppose that there exists a vertex V ∈ B2δ∕5(ci), We need to bound the probability P that by
taking a random sample srand, there will be a vertex Snear ∈ Bδ(Ci) such that Snew ∈ B2δ∕5(ci+1).
Lemma 3 provides a lower bound for the probability that snear ∈ Bδ (ci), given that there exists
a vertex V ∈ B2δ∕5(c), of ∣Bδ∕5(s)∣∕∣S∣. Lemma 2 provide a lower bound for the probability of
Z	κδ-eKs δt (κδ — e)
choosing an an action from Snear to Snew of P ≡-------「甘血〜->。, where we have substituted
tτ with ∆t. Consequently,P ≥ (∣Bδ∕5(s)∣∙ ρ)∕∣S∣.
For RRT to recover τ, the transition between consecutive circles must be repeated m times. This
stochastic process can be described as a binomial distribution, where we perform k trials (randomly
choosing Srand), with m successes (transition between circles) and a transition success probability
P. The probability mass function of a binomial distribution is Pr(X = m) = Pr(m; k, P) =
mk Pm(1 - P)k-m. We use the cumulative distribution function (CDF) to represent the upper bound
for failure, i.e. the process was unable to perform m steps, which can be expressed as:
Pr(X < m) = mX-1 kPi(1 -P)k-i.	(S6)
i=0	i
Using Chernoff,s inequality we derive the tail bounds of the CDF when m ≤ P ∙ k:
Pr(X < m) ≤ exp
1 (kP - m)2
2p	k
exp ( 一 ɪkp + m 一
≤ eme-1 Pk = ae-bk.
(S7)
(S8)
(S9)
In the other case, where P < m∕k < 1, the upper bound is given by (Arratia & Gordon, 1989):
Pr(X < m) ≤ exp (—kD (m ∣∣ P))
(S10)
where D is the relative entropy such that
3
Under review as a conference paper at ICLR 2020
Rearranging D, we can rewrite S10 as follows:
Pr(X <m) ≤ exp (kk (m log (kp)+kkm log (H)))
m 1 m mW 1 71	∕1k m∖∖	1	1	∕1k mm
=exp km log — exp -k log -- exp m log ----
kp	1 k p	1 k p
exp -m log
- p)
exP -kl log(11≡∣
≤ exp -k log
≤ exp -k log
1-爸
1-p
0.5、
1 - p√
≤ e-kp = ae-bk
(S11)
(S12)
(S13)
(S14)
(S15)
(S16)
—
where (S14) is justified for worst-case scenario where p = m/k, (S15) uses the fact that p < m/k <
0.5, hence 1 k m/k > 0.5. The last step, (S16) is derived from the first term of the Taylor expansion
Oflog (1-p )= P∞=居.
As P and m are fixed and independent of k, We show that the expression for Pr(X < m) decays to
zero exponentially with k, therefore RRT is probabilistically complete.	■
It worth noting that as expected the failure probability Pr(X < m) depends on problem-specific
properties, which give rise to the values of a and b. Intuitively, a depends on the scale of the problem
such as volume of the goal set |FgRoLal | and how complex and long the solution needs to be, as evident
in Eq. (S9). More importantly, b depends on the probability p. Therefore, it is a function of the
dimensionality of S (through the probability of sampling srand) and other environment parameters
such as clearance (defined by δ) and dynamics (via Ks, Ka), as specified in Eq. (S4).
S3 Appendix C: experimental setup
All experiments were run using a single 2.2GHz core and a GeForce GTX 1080 Ti GPU.
S3.1	Environments
All environments are made available in supplementary code. Environments are based on Gym (Brock-
man et al., 2016), with modified sparse reward functions and state spaces. All environments emit a
k1 reward per timestep unless noted otherwise. The environments have been further changed from
Gym as follows:
•	CartPole Swingup- The state space S ⊆ R4 consists of states S = [x, θ, X, θ] where X is
cart position, X IS cart linear velocity, θ IS pole angle (measuring from the y-axιs) and θ pole
angular velocity. Actions A ⊆ R are force applied on the cart along the X-axis. The goal
space Fgoal is {s ∈ S | cos θ > 0.9}. Note that reaching the goal space does not terminate
an episode, but yields a reward of cos θ. Time horizon is H = 500. Reaching the bounds of
the rail does not cause failure but arrests the linear movement of the cart.
•	MountainCar- The state space S ⊆ R2 consists of states s = [X, θ] where X is car position
and X is car velocity. Actions A ⊆ R are force applied by the car engine. The goal space
Fgoal is {s ∈ S | X ≥ 0.45}. Time horizon is H = 200.
•	Acrobot- The state space S ⊆ R4 consists of states s
[θ0,θ1, θ0,θ1] where θ0,θ1 are the
angles of the joints (measuring from the y-axis and from the vector parallel to the 1st link,
respectively) and θ0 , θ1 are their angular velocities. Actions A ⊆ R are torque applied on
the 2nd joint. The goal space Fgoal is {s ∈ S | k cos θ0 k cos (θ0+ θ1) > 1.9}. In other
words, the set of states where the end of the second link is at a height y > 1.9. Time horizon
is H = 500.
4
Under review as a conference paper at ICLR 2020
Pendulum - The state space S ⊆ R2 consists of states S =[4 θ] where θ is thejoint angle
(measured from the y-axis) and θ is the joint angular velocity. Actions A ⊆ R are torque
applied on the joint. The goal space Fgoal is {s ∈ S | cos θ > 0.99}. Note that reaching
the goal space does not terminate an episode, but yields a reward of cos θ. Time horizon is
H = 100.
•	Reacher- The state space S ⊆ R6 consists of states S = [θ0,θ1,x, yθo, θ( where θ0,θι
are the angles of the joints, (x, y) are the coordinates of the target and θ0, θ1 are the joint
angular velocities. Actions A ⊆ R2 are torques applied at the 2 joints. The goal space Fgoal
is the set of states where the end-effector is within a distance of 0.01 from the target. Time
horizon is H = 50.
•	Fetch Reach- A high-dimensional robotic task where the state space S ⊆ R13 con-
sists of states S = [gripper_poS, f inger_poS, gripper_State, f inger_State, goal_poS]
where gripper_poS, gripper_vel are the Cartesian coordinates and velocities of the Fetch
robot’s gripper, f inger_State, f inger_vel are the two-dimensional position and veloc-
ity of the gripper fingers, and goal_poS are the Cartesian coordinates of the goal. Ac-
tions A ⊆ R4 are relative target positions of the gripper and fingers, which the Mu-
JoCo controller will try to achieve. The goal space Fgoal is the set of states where
the end-effector is within a distance of 0.05 from goal_poS. Time horizon is H = 50.
Note that this problem is harder than the original version in OpenAI Gym, as we only sample
goal_poS that are far from the gripper’s initial position.
•	Hand Reach- A high-dimensional robotic task where the state space S ⊆ R78 consists of
states S = [joint_poS, j oint_vel, f ingertip_poS, goal_poS] where joint_poS, j oint_vel
are the angles and angular velocities of the Shadow hand’s 24 joints, f ingertip_poS are
the Cartesian coordinates of the 5 fingertips, and goal_poS are the Cartesian coordinates of
the goal positions for each fingertip. Actions A ⊆ R20 are absolute target angles of the 20
controllable joints, which the MuJoCo controller will try to achieve. The goal space Fgoal is
the set of states where all fingertips are simultaneously within a distance of 0.02 from their
respective goals. Time horizon is H = 50.
S3.2	Experimental setup and hyper-parameter choices
All experiments feature a policy with 2 fully-connected hidden layers of 32 units each with tanh
activation, with the exception of Reacher, for which a policy network of 4 fully-connected hidden
layers of 128 units each with relu activation is used. For all environments we use a linear feature
baseline for TRPO.
Default values are used for most hyperparameters. A discount factor of γ = 0.99 is used in all
environments. For VIME, hyperparameters values reported in the original paper are used, and the
implementation published by the authors was used.
For TRPO, default hyperparameter values and implementation from Garage are used: KL divergence
constraint δ = 10-2, and Gaussian action noise N(0, 0.32).
In comparisons with VIME-TRPO and vanilla TRPO, the R3L goal sampling probability pg is set
to 0.05, as proposed in Urmson & Simmons (2003). Goal sets Fgoal are defined in Appendix S3.1
for each environment. In all experiments, the local policy πl learned during R3L exploration uses
Bayesian linear regression with prior precision 0.1 and noise precision 1.0, as well as 300 random
Fourier features (Rahimi & Recht, 2008) approximating a square exponential kernel with lengthscale
0.3.
5