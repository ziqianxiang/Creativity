Under review as a conference paper at ICLR 2020
Learning Deep-Latent Hierarchies by Stack-
ing Wasserstein Autoencoders
Anonymous authors
Paper under double-blind review
Abstract
Probabilistic models with hierarchical-latent-variable structures provide state-of-
the-art results amongst non-autoregressive, unsupervised density-based models.
However, the most common approach to training such models based on Variational
Autoencoders often fails to leverage deep-latent hierarchies; successful approaches
require complex inference and optimisation schemes. Optimal Transport is an
alternative, non-likelihood-based framework for training generative models with
appealing theoretical properties, in principle allowing easier training convergence
between distributions. In this work we propose a novel approach to training models
with deep-latent hierarchies based on Optimal Transport, without the need for
highly bespoke models and inference networks. We show that our method enables
the generative model to fully leverage its deep-latent hierarchy, and that in-so-doing,
itis more effective than the original Wasserstein Autoencoder with Maximum Mean
Discrepancy divergence.
1 Introduction
Probabilistic latent-variable modelling is widely applicable in machine learning as a method for
discovering structure directly from large, unlabelled datasets. Variational Autoencoders (VAEs)
(Kingma & Welling, 2014; Rezende et al., 2014) have proven to be effective for training generative
models parametrised by powerful neural networks, by mapping the data into a low-dimensional
embedding space. While allowing the training of expressive models, VAEs often fail when using
deeper hierarchies with several stochastic latent layers.
In fact, many of the most successful probabilistic latent-variable models use only one stochastic
latent layer. Auto-regressive models (LarOChelle & Murray, 2011; Van Den Oord et al., 2016b a;
Salimans et al., 2017), for example, produce state-of-the-art samples and likelihood scores. However,
auto-regressive models suffer from poor scalability to high-dimensional data. Flow-based models
(Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016; Kingma & Dhariwal, 2018)
are another class of generative models providing competitive sample quality and are able to scale to
higher-dimensional data, but they still lag behind auto-regressive models in terms of likelihood score.
Deep-latent-variable models are highly expressive models that aim to capture the structure of data in
a hierarchical manner, and could thus potentially compete with auto-regressive models for state-of-
the-art performance. However, they remain hard to train. Many explanations have been proposed for
this, from the use of dissimilarity measure directly in pixel space (Larsen et al., 2016) resulting in
poor sample quality, to the lack of efficient representation in the latent (Zhao et al., 2017), to simply
the poor expressiveness of the models used (Zha et al., 2017; Maal0e et al., 2019).
Recent works have tried to overcome the latter problem introducing latent skip connections in the
generative model and inference network (Bachman, 2016), sharing generative model and inference
network parameters (S0nderby et al., 2016), as well as bidirectional inference network (Maal0e et al.,
2019). Maal0e et al. (2019) managed to train very deep-hierarchical-latent models achieving near
state-of-the-art sample generations. However, in order to leverage their latent hierarchy (working in
the VAE framework), they needed both complex, tailored inference networks, and deterministic skip
connections in the generative model.
Optimal Transport (OT) (Villani, 2008) is a mathematical framework to compute distances between
distributions. Recently, it has been used as a training method for generative models (GeneVay et al.,
1
Under review as a conference paper at ICLR 2020
2018; Bousquet et al., 2017). Tolstikhin et al. (2018) introduced Wasserstein Autoencoders (WAEs),
where as with VAEs, an encoding distribution maps the data into a latent space, aiming to learn a
low-dimensional representation of the data. WAE is a non-likelihood based framework with appealing
topological properties (Arjovsky et al., 2017; Bousquet et al., 2017), in theory allowing for easier
training convergence between distributions. Gaujac et al. (2018) trained a two-latent-layer generative
model using WAE, showing promising results in the capacity of the WAE framework to leverage a
latent hierarchy in generative modelling.
Following these early successes, we propose to train deep-hierarchical-latent models using the
WAE framework, without the need for complex dependency paths in both the generative model and
inference network. As in the works of S0nderby et al. (2016) and Maal0e et al. (2019), We believe
that a deep-latent hierarchy offers the potential to improve generative models, if they could be trained
properly. In order to leverage the deep-latent hierarchy, we derive a novel objective function by
stacking WAEs, introducing an inference network at each level, and encoding the information up to
the deepest layer in a bottom-up way. For convenience, we refer to our method as StackedWAE.
Our contributions are two-fold: first, we introduce StackedWAE, a novel objective function based
on OT, designed specifically for generative modelling with latent hierarchies, and show that it is able
to fully leverage its hierarchy of latents. Second, we show that StackedWAE performs significantly
better when training hierarchical-latent models than the original WAE framework regularising the
latent with the Maximum Mean Discrepancy (MMD) divergence (Gretton et al., 2012).
2 Stacked WAE
StackedWAEs are probabilistic-latent-variable models with a deep hierarchy of latents. They can
be minimalistically simple in their inference and generative models, but are trained using OT in a
novel way. We start by defining the probabilistic models considered in this work, then introduce
OT, and finally discuss how to train probabilistic models with deep-latent hierarchies using OT, the
method that we refer to as StackedWAE.
Throughout this paper, we use calligraphic letters (e.g. X) for sets, capital letters (e.g. X) for random
variables, and lower case letters (e.g. x) for their values. We denote probability distributions with
capital letters (e.g. P (X)) and their densities with lower case letters (e.g. p(x)).
2.1	Generative models with deep hierarchies of latent variables
We will consider deep-generative models with Markovian hierarchies in their latent variables. Namely,
where each latent variable depends exclusively on the previous one. Denoting by PΘ the parametric
model with N latent layers, where Θ={θ1,...,θN}, we have:
pΘ(x)
/
Z1...ZN
Pθι (x∣Zl) Pθ2 (zi∣Z2) .∙∙PΘn (ZN-1∣ZN ) P(，N ) dz∖ ...dzN
(1)
where the data is X ∈Xand the latent variables are Zn ∈Zn and we chose p(zN )=
N(ZN; 0Zn , ZZN). The corresponding graphical model for N = 3 is given Figure 1a.
We will be using variational inference through the WAE framework of Tolstikhin et al. (2018),
introducing variational distributions, qΦ(Z1,...,ZN|x), to facilitate the evaluation of the integral in
Eq. (1), analogous to VAEs (Kingma & Welling, 2014； Rezende et al., 2014). It will be shown in
Section 2.3 that without loss of generality, qφ(zι,...,ZN |x) can have a Markovian latent hierarchy
when following the StackedWAE approach. That is,
w.l.o.g. with
qφ (zi,...,zn |x) staCk=Dwae qφι (zi|x) qφ2 (z2∣z1) ...qφN (ZN |zn-i)	(2)
where each qφN (Zi|Zi-1) is introduced iteratively by stacking WAEs at each latent layer. The
corresponding graphical model for N = 3 is given Figure 1b.
We focus on this simple Markovian latent-variable structure for the generative model as a proof
point for StackedWAE. This simple modelling setup is famously difficult to train, as is discussed
extensively in the VAE framework (see for example Burda et al. (2015); S0nderby et al. (2016); Zhao
et al. (2017)). The difficulty in training such models comes from the Markovian latent structure
2
Under review as a conference paper at ICLR 2020
3^ ʌ^2^ I^X
3 2 1
qqq
(a) Our gen. model (b) Our inf. model (c) Inf. without stacking (d) Alternative inf.
Figure 1: (a) Generative model (blue lines represent generative model parameters). (b) Inference
model used in StackedWAE. (c) Standard WAE; the generative model has only one latent with
prior p(z1) = p(z1 |z2)p(z2|z3)p(z3)dz1dz2dz3. (d) Inference model with skips connections and
parameter sharing with the generative model, as in S0nderby et al. (2016).
of the generative model; in particular, the difficulty of learning structure in the deeper latents.
This is because, to generate samples X 〜pθ(x), only the joint pθ(x, zι) is needed as pθ(x)=
JZI pθι (x∣zi)pθ (zι)dzι. Learning a smooth structure in each latent layer is not a strict requirement
for learning a good generative model, however, it is sought after if the latent is to be used downstream
or interpreted. We find empirically (see Section 3.1) that a better generative model is also achieved
when the latent hierarchy is well learnt all the way down.
S0nderby et al. (2016) sought to overcome the difficulty of learning a deep-latent hierarchy by using
deterministic bottom-up inference, followed by top-down inference that shared parameters with
the generative model. With additional optimisation tricks (e.g. KL annealing), their deeper-latent
distributions would still go unused for sufficiently deep-latent hierarchies (discussed in Maal0e et al.
(2019)). In order to get deeper hierarchies of latents, Maal0e et al. (2019) introduced additional
deterministic connections in the generative model as well as bidirectional inference network to
facilitate the deep information flow needed to ensure the usage of the deeper-latent layers.
While the approach in Maal0e et al. (2019) is well motivated and achieves excellent results, we choose
the OT framework for training deep-latent models due to its topological properties (see Arjovsky et al.
(2017)). Still, the standard WAE encounters the same difficulties as the VAE in learning deep-latent
hierarchies. We thus modify the original WAE objective, effectively stacking WAEs, to improve the
learning of both the generative model and the inference distribution throughout the latent hierarchy.
2.2	Wasserstein Autoencoders
The Kantorovich formulation of the OT between the true-but-unknown data distribution PD and the
model distribution PΘ, for a given cost function c, is defined by:
OTc(PD,PΘ)
inf
Γ∈P(PD,PΘ)
/
X×X
c(x, X) dΓ(x, X)
(3)
where P(PD,PΘ) is the space of all couplings ofPD and PΘ; namely, the space of joint distributions
Γ on X×Xwhose densities γ have marginals pD and pΘ :
)
P(PD,PΘ)
{r∣yX
Y(x, X) dx = PD(X)
and
γ(
X
X, X) dX = pθ(X)
(4)
In the WAE framework introduced by Tolstikhin et al. (2018), the space of couplings is constrained
to joint distributions Γ, with density γ, of the form:
Y(X,X) =	/
Z1 ...ZN
Pθ(X∣Zl,...,ZN ) qφι (zi,...,zn |x) PD (x) dzι ... dzN
(5)
where Qφ1 (Z1,...,ZN|X) plays the same role as the variational distribution in variational inference.
3
Under review as a conference paper at ICLR 2020
Marginalising over X in Eq. (5) automatically gives PD (x), however the second marginal constraint
in Eq. (4) (that over X giving pθ) is not guaranteed. Due to the Markovian structure of the generative
model, a sufficient condition for satisfying the second marginal constraint is (see Appendix A.1):
q qφι (zi∖x)PD (x)dx = /	Pθ2 (zi∣Z2) .∙∙PθN (ZN-1∣ZN)P(ZN )d，2 ∙∙∙dzN = PΘ2N(Zl)
X	Z2 ...ZN
(6)
Finally, to get the WAE objective of Tolstikhin et al. (2018), the marginal constraint of Eq. (6) is
relaxed using a Lagrange multiplier (see Appendix A.2):
Cc(PD,Pθ) =	Qφι喙 Ιx)
λι Dι( / Qφι (Zι∖χ)PD(x) dx, Pθ2:N (Zι))
(7)
+
/	/	c(x, X)Pθι (X∖zι) qφι (zι∖x)Pd(x)
X×X	Z1
dx dx dzι
where D1 is any divergence function and λ1 a relaxation parameter. Note that the inf is taken only
over qφ1 (z1 ∖x) instead of the full qφ1 (z1,...,zN∖x) because the expression does not depend on z>1.
While Eq. (7) is in-principle tractable (e.g. for Gaussian qφι (zι∖x) and sample-based divergence
function such as MMD), it only depends on the first latent Z1 . Thus it will learn only a good
approximation for PΘ (x, z1) = Pθ1 (x∖z1)P(z1), rather than a full hierarchy with each latent living on
a smooth manifold. We show empirically in Section 3.1 that Eq. (7) is indeed insufficient.
2.3 Stacking WAEs for deep-generative-latent variable modelling
In theory (e.g. λι → ∞), Eq. (7) does not depend on the choice of divergence Di. However, given
the set of approximations used, a divergence that takes into account the smoothness of the full stack
of latents will likely help the optimisation. We now show that by using the Wasserstein distance itself
for D1 , we can derive an objective that naturally pairs up inference and generation at every level in
the deep-latent hierarchy. After all, the divergence in Eq. (7) is between an aggregate distribution:
Qa1gg(Z1) d=ef
X
Qφ1(Z1∖x) PD(x) dx
(8)
from which We can only access samples, and an analytically-known distribution P82：N (Zi), which is
analgous to where we started with the Wasserstein distance between PD and the full PΘ .
Specifically, We choose Di in Eq. (7) to be the relaxed Wasserstein distance CcI, which following the
same arguments as before, requires the introduction ofa new variational distribution Qφ2 (Z2 ∖Zi):
Di(QIgg(Zi), Pez：N(ZI))=C 黑、λ2D2(Q岁(Z2), Pe3：N(Z2))	(9)
Qφ2 (Z2 |z1)
+
Z1 ×Z1	Z2
Ci(zι,Zi)Pθ2 (Zi∖z2) qφ2 (z2∖zi) qagg(zι) dzi dZi dz2
where the notation in Eq. (6) has been re-used forthe prior Pθ3in (Z2), and similarly for the aggregated
posterior, Q2gg(Z2) =f R^ Qφ2 (Z2∖zi)qagg(zi), as in Eq. (8). Just as before, Qφ2(Z2∖Zi) does not
need to provide a distribution over the Z>2 without loss of generality.
The divergence Di that arose in Eq. (7) between two distributions over Zi is thus mapped onto
the latent at the next level in the latent hierarchy, Z2, via Eq. (9). This process can be repeated by
using Wc2 again for D2 in Eq. (9) to get an expression that maps to Z3, requiring the introduction of
another variational distribution Qφ3 (Z3∖Z2). Repeating this process until we get to the final layer of
the hierarchical-latent-variable model gives the StackedWAE objective:
N
WSTACKEDWAE(PD,PΘ)=	inf	Y λi DN QaNgg(ZN),P(ZN)	(10)
QΦ(Z1,...,ZN|x)
N-i	n
+	X	Y	λi
n=0	i=i
LZL	Cn(Zn,zn) Pθn+ι
(Zn ∖zn+i ) qφn+ι (Zn+i ∖ Zn ) qn' (Zn ) dzndzndzn+i
4
Under review as a conference paper at ICLR 2020
where we denote (Z0,Z0,z0)=(X ,X,x) and we define the empty product Q0=1 λi d=ef 1. Each
Pθn is the nth layer of the generative model given in Eq. (1). The qφn's are the inference models
introduced each time a WAE is "stacked", which combine to make the overall S tac kedWAE
Markovian inference model given in Eq.(2), and the aggregated posterior distributions are defined as
Qa0gg d=ef PD
and
Qangg (Zn) d=ef
Zn-1
Qφn (Zn |zn-1 ) qn-1 (zn-1) dzn-1
(11)
Note that the S tackedWAE objective function in Eq. (10) still requires the specification of a
divergence at the highest latent layer DN , which we simply take to be the MMD as originally
proposed by Tolstikhin et al. (2018). Other choices can be made, as in Patrini et al. (2018), who
choose a Wasserstein distance computed using the Sinkhorn algorithm (Cuturi, 2013). While Patrini
et al. (2018) provide a theoretical justification for the minimisation of a Wasserstein distance in the
prior space, we found that it did not result in significant improvement and comes at an extra efficiency
cost. Similarly, one could choose different cost functions cn at each layer; for simplicity we take all
cost functions to be the squared Euclidean distance in their respective spaces.
3	Experiments
We now show how the StackedWAE approach of Section 2 can be used to train deep-latent
hierarchies without customizing the generative or inference models (e.g. skip connections, parameter
sharing) . We also show through explicit comparison that the S tackedWAE approach performs
significantly better than the standard WAE when training deep-hierarchical-latent models.
3.1 MNIST
Experimental setup
We trained a deep-hierarchical-latent variable model with N = 5 latent layers on raw (non-binarised)
MNIST (LeCun & Cortes, 2010). The latent layers have dimensions: dzι = 32, dz? = 16, dz3 = 8,
dZ4 =4and dZ5 =2. We chose Gaussian distributions for both the generative and inference models,
except for the bottom layer of the generative model, which we choose to be deterministic, as in
Tolstikhin et al. (2018). The mean and covariance matrices used are parametrised by fully connected
neural networks (see Appendix B.1 for details).
We choose the squared Euclidean distance cost function: Cn(zn, Zn) = ∣∣Zn - Znk22. Expectations
in Eq. (10) are computed analytically where possible, and otherwise with Monte Carlo sampling.
Learning a deep-latent hierarchy
Our results are shown in Figure 2, with the training curves in Figure 2a, samples from the generative
model in Figure 2b, and latent space interpolations in Figure 2c where digits are reconstructed
from points in Z5 taken evenly in a grid varying between ±2 standard deviations from the origin.
S tackedWAE generates compelling samples and learns a smooth manifold. Note that the choice
dZ5 = 2 allows for easy visualisation of the learned manifold, rather than being the optimal dimension
for capturing all of the variance in MNIST.
Figure 2c shows that StackedWAE manages to use all of its latent layers, capturing most of the
covariance structure of the data in the deepest-latent layer, which is something that VAE methods
struggle to accomplish (S0nderby et al., 2016). Figure 3a shows the encoded input images through the
latent layers, with corresponding digit labels coloured. We see through each layer that StackedWAE
leverages the full hierarchy in its latents, with structured manifolds learnt at each stochastic layer.
An advantage of deep-latent hierarchies is their capacity to capture information at different levels, aug-
menting a single-layer latent space. In Figure 3b, input images, shown in the bottom row, are encoded
and reconstructed for each latent layer. More specifically, the inputs are encoded up to the latent layer
i and reconstructed from the encoded zi using the generative model p(x|z1)p(z1|z2) ...p(zi-1 |zi).
Row i in Figure 3b shows the reconstruction obtained from encoding up to layer i. We can see that
each additional encoding layer moves slightly farther away from copying the input image as it moves
5
Under review as a conference paper at ICLR 2020
<0<2Z 22/////// / 1
OD<2N2N //--------------
。。。,222/771-------------
00OO<2,AZ 777Γ-77I
C G
A Q
。Co
6 O
s ð
3 3
3 3
8 S
4 q
y q
y q
√ q
√ q
H q
4夕<93/。93。3-2478
7864≤-γ 年y/Ya 7(/
cqFqoo∖∕q9 71 / 0<
67 7/ quZClq夕 ssə
,y /39 aoo870q∕a
3as"∙77F3-3 77FbG
7q7377ag夕—5304
•，CeoaF8 7 76。。--S
g2s∕08r∕F,I Vur 9
9。— 3 7。9 8002 夕∕q3
717SqfCGr53784，
24-709|7709口 5 7/
ZGGq∙3~63 1 9//0 72
U<o∕∕3q55o 7λ3 2
(a)
(c)
(b)
Figure 2: 5-layer StackedWAE. (a) Training curves; each term in Eq. (10) is shown throughout
training. (b) Model samples. (c) Z5 latent-space interpolations.
(a)
Qfr ʌv Λ7 ⅛z
0O0OD
77 7->o
Oo o< 09
子
6
D
ɔ
y
ʃ
Z g
g g
才ʃ
y ʃ
Z ∕z∕√r
33833
夕
CN
/
夕
A工‰xx
3 83a3
5 6 555
00666
。。jðð
05555
3 3 3 3 3
C 6 666
4 q "8"
, q q 9 土
7 7 7 7 7
夕夕?TL亍
2 3 2 ZN
夕 9 夕79
q 9÷
06 000
6 O 6 A 备
GXSS & G L
7
工
7
Z
夕
多
O
6
G
UUG.3Q
m7万，z







/
¥
(b)
Figure 3:	5-layer STAcKEDWAE. (a) Visualisations of latent spaces Zi. Each colour corresponds
to a digit label. dZ5 = 2 can be directly plotted; for higher dimensions We use UMAP (McInnes &
Healy, 2018) to plot a two dimensional representation. (b) Reconstructions for different encoding
layers. The bottom row is data; the ith row from the bottom is generated using the latent codes zi
which are from the ith encoding layer.
。J,i3。，夕夕Γ/ bβ2∕7
夕f§36675/33,75
Z 73 3 r"。，PJΘ∕∖90OG2
6、04£3XZ2+ 夕&563
YG/^965Λlq1√7›/
，工0d? 3z∕4 q&/.2a，
SgIGiqaN∙74y∕"6
彳夕 9S4 es∕GOo qc9 S
O6 ⅛*a,⅛9b∕“7u 34 3
4a/S 7，/q∕3s-sol
7"l∕3∖0 0-G/3/
Zto G u« α∕lt / ⅛> O 85Q S
0 24,^ 0,4√q 3γ36λx∙7
gl73α5√57JI72yk2
Γr∕ -6〜7rl∕√coA
5ɪ QJS <-c O¾mx∙^ox4sy
R2 CI7A / g，6G 7 I Z 2
ʃvovα,y∕∕a∕∕τ8i⅛*Q
6 7J3∕x-l3∕Ji 5offtt
/5^*7xd /夕 Mo 5, / q9a
7 O 1 RJ / q D'70'，9彳。夕
I⅛go3l8 7aoo3ce
6/ 7 6 S 3 卜> l∙7/ oʃðo
QZ 77$号〃依KJI 〉 473
606Gq∕2 36g1∕g2
3 / qz3 3∕?QCq 夕67
ʃ*VSTb grr，7Z74 3 3
。<74∙aa,oy60f5⅜
0。9 96 6 / /LUW33
τυ 40--2 Z g t，，S SFF
夕彳。0336Gm3559。
夕夕 3~⅛∙^yq55l I 3 To-
g⅛ollyy 夕 yyy∙7o0 D
‰x!33∕∕60∕ /9夕
33 7 755odc>c>ss33
??久277/ / q q W66
bo。。JJoo^^79ZN
co%77 ¥ y<Γ⅜k9夕//l7l
d N 33----q;F 夕/qq77
，Γ— Ii α/ α/ <5 CJ /J I/ Λτ> ðn
3 0 7 76 / G6oo0/3la
/to6 22//11，-TO0 9乡
(a)	(b)	(c)
Figure 4:	1-layer implicit-prior WAE. (a) Reconstructions (within pairs of rows, data is above with
the corresponding reconstructions below). (b) Model samples. (c) Z5 latent-space interpolations.
6
Under review as a conference paper at ICLR 2020
towards fitting the encoding into the 2-dimensional, unit-normal prior distribution. The dimension-
ality of the deeper-latent layers is a modelling choice which determines how much information is
preserved; this can be seen through the loss of information from deeper reconstructions in Figure [3b]
Indeed, in each layer, the encoder is asked to map the incoming encodings into a lower-dimensional
latent space, filtering the amount of information to keep and pass up to the deeper layer] Thus, if
there is a mismatch in the dimensions between the true underlying generative process of the data
and the chosen model, the encoder will have to project the encodings into lower-dimensional space,
losing information along the way]
Ablation study: StackedWAE versus standard WAE
In this section, we compare StackedWAE with the original WAE framework for training generative
models with deep-hierarchical latents] In particular, we train a WAE using the objective defined in
Eq. (7) and an inference distribution as in Eq. (2); the corresponding graphical model is shown in
Figure 1c. We use the same the experimental setup as outlined earlier in this section, and the same
parametrised networks.
The results are shown in Figure 4, with reconstructions in Figure 4a, generated samples in Figure 4b,
and deepest-latent interpolations in Figure 4c. The latter two should be compared directly with
Figure 2b and Figure 2c, respectively, for the STACKEDWAE. The quality of the generated samples
from the WAE is poor in comparison with that of the StackedWAE.
The lack of smooth interpolations in Figure 4c shows that almost no structure has been captured in
the deepest-latent layer. This is likely due to the fact that the standard WAE, with objective given
in Eq. (7), is independent of the deeper-latent inference distributions, thus reducing the smoothness
in the deeper layers. The relatively accurate reconstructions in Figure 4a indicate that the model
managed to capture most of the structure of the data in the first latent layer. This behaviour is similar
to that of the Markov HVAE as described in Zhao et al. (2017). They show that, for Markov HVAEs to
learn interpretable latents, one needs additional constraints beyond simply maximising the likelihood.
3.2 Street View House Numbers
We now show that StackedWAE is able to leverage a deep-latent hierarchy on more real-world
datasets, in particular Street View House Numbers (SVHN) (Netzer et al., 2011). We trained a
6-layer StackedWAE with inference networks and generative models at each latent layer taken to
be Gaussian distributions with mean and covariance functions parametrised by 3-layer ResNet-style
(Kaiming et al., 2015) neural networks. The details are given in Appendix B.2.
As before, We choose Cn(zn,	) = ∣∣zn - ≡nk22, and compute the expectations in Eq. (10) analyti-
cally whenever possible, otherwise using Monte Carlo sampling.
For simplicity, our architecture choices constrain the dimensionality on the latent space (details are
in Appendix B.2). The latent dimension is given by the size of the output feature map at that layer
times the number of these feature maps, leading to latent dimensions of: 16×16×1, 8×8×2, 4×4×4,
4×4×2, 4×4×1 and 2×2×2. These make for relatively high-dimensional latent spaces for the first few
latent layers. Rubenstein et al. (2018) observed that when training WAEs with high-dimensional
latent space, the variance of the inference network tends to collapse to 0. The authors argue that this
might come either from an optimisation issue or from the failing of the divergence used to regularise
the latents. Either way, the collapse to deterministic encoders results in poor sample quality as the
deterministic encoder is being asked to map the input manifold into a higher-dimensional space
than its intrinsic dimension. One solution proposed in Rubenstein et al. (2018) is to include in the
objective function Eq. (10) a regulariser that maintains a non-zero variance. As in Rubenstein et al.
(2018), we include a log-variance penalty term as given in Eq. (12):
N	dZi
Lpen = ∑>∑E | log ∑q [m]|
(12)
i=1	m=1
We find that λΣ = 10-(2+i) for i = 1,...,6 works well in our setting. This choice has been
motivated by the fact that the bigger the latent dimension, the more likely it is that dZ >dI , where
dI and dZ are the intrinsic dimension of the input and the latent dimension, respectively. Given our
7
Under review as a conference paper at ICLR 2020
Figure 5: 6-layer StackedWAE. (a) Model samples. (b) Points interpolations; the first and
last columns are actual data points. (C) Visualisations of latent spaces Zi, as in Figure 3a.(d)
Reconstructions for different encoding layers, as in Figure 3b.
latent-dimension choices (see Table 6b), the first latent layers will suffer more from the collapse to
deterministic encoder than the deeper ones with lower dimension.
Our results on SVHN are given Figure 5. Samples from the generative model are shown in Figure 5a.
Interpolations across the deepest-latent layer are shown in Figure 5b with the anchor data points in
the first and last columns; the interpolations are smooth as expected, though perhaps due to the choice
of low-dimensional deepest layers, the reconstructions (second and second-to-last columns) may fail
to match perfectly the anchor data points (first and last columns). The encoded latent space is shown
in Figure 5c for each latent layer, where the structure of SVHN can be seen; it is likely that the most
salient structure in these embeddings represents the background colour, rather than the central digit,
but that is to be expected from SVHN. Finally, Figure 5d shows the reconstructions of the data points
(along the bottom row) at each latent layer in the hierarchy. Similarly to the results for MNIST, we
can see that the deepest latent layer may not be large enough to enable high-fidelity reconstructions.
Our intention is to show that the hierarchy of latents can be learnt, which is clearly the case, rather
than to model SVHN perfectly, so we do not attempt to tune to the optimal latent dimensionality.
4 Conclusion
In this work we introduced a novel objective function for training generative models with deep hierar-
chies of latent variables using Optimal Transport. Our approach recursively applies the Wasserstein
distance as the regularisation divergence, allowing the stacking of WAEs for arbitrarily deep-latent
hierarchies. We showed that this approach enables the learning of smooth latent distributions even
in deep latent hierarchies, which otherwise requires extensive model design and tweaking of the
optimisation procedure to train. We also showed that our approach is significantly more effective at
learning smooth hierarchical latents than the standard WAE.
8
Under review as a conference paper at ICLR 2020
References
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International
Conference on Machine Learning, 2017.
P. Bachman. An architecture for deep, hierarchical generative models. In Advances in Neural
Information Processing Systems, 2016.
O. Bousquet, S. Gelly, I. Tolstikhin, C. J. Simon-Gabriel, and B. Scholkopf. From optimal transport
to generative modeling: the VEGAN cookbook. 2017.
Y. Burda, Grosse R., and R. Salakhutdinov. Importance weighted autoencoders. In International
Conference on Learning Representations, 2015.
M. Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In Advances in Neural
Information Processing Systems, 2013.
L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real NVP. In International
Conference on Learning Representations, 2016.
B. Gaujac, I. Feige, and D. Barber. Gaussian mixture models with Wasserstein distance.
arXiv:1806.04465, 2018.
A. Genevay, G. Peyre, and M. Cuturi. Learning generative models with Sinkhorn divergences. In
International Conference on Artificial Intelligence and Statistics, 2018.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In International
Conference on Artificial Intelligence and Statistics, 2011.
A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample test. In
Journal of Machine Learning Research, 2012.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning, 2015.
H. Kaiming, Z. Xiangyu, R. Shaoqing, and S. Jian. Deep residual learning for image recognition. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
D. P. Kingma and J. Ba. Adam: a method for stochastic optimization. In International Conference on
Learning Representations, 2015.
D. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances
in Neural Information Processing Systems, 2018.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on
Learning Representations, 2014.
D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved varia-
tional inference with inverse autoregressive flow. In Advances in Neural Information Processing
Systems, 2016.
H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In International
Conference on Artificial Intelligence and Statistics, 2011.
A. B. L. Larsen, K. S0nderby S., H. Larochelle, and O. Winther. Autoencoding beyond pixels using a
learned similarity metric. In International Conference on Machine Learning, 2016.
Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010.
L. Maal0e, M. Fraccaro, V. Lievin, and O. Winther. BIVA: a very deep hierarchy of latent variables
for generative modeling. In Advances in neural information processing systems, 2019.
L. McInnes and J. Healy. UMAP: uniform manifold approximation and projection for dimension
reduction. arXiv:1802.03426, 2018.
9
Under review as a conference paper at ICLR 2020
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural
images with unsupervised feature learning. In Advances in Neural Information Processing Systems
Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
G. Patrini, M. Carioni, P Forra S. Bhargav, M. Welling, R. Van Den Berg, T. GeneWein, and
F. Nielsen. Sinkhorn autoencoders. arXiv:1810.01118, 2018.
D. J. Rezende and S. Mohamed. Variational inference With normalizing floWs. In International
Conference on Machine Learning, 2015.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
in deep generative models. In International Conference on Machine Learning, 2014.
P. K. Rubenstein, B. Scholkopf, and I. Tolstikhin. On the latent space of Wasserstein auto-encoders.
In Workshop track - International Conference on Learning Representations, 2018.
T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma. Pixelcnn++: Improving the PixelCNN With
discretized logistic mixture likelihood and other modifications. In International Conference on
Learning Representations, 2017.
C. K. S0nderby, T. Raiko, L. Maal0e, S. K. S0nderby, and O. Winther. Ladder variational autoencoders.
In Advances in neural information processing systems, 2016.
I.	Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. In International
Conference on Learning Representations, 2018.
A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural netWorks. In
International Conference on Machine Learning, 2016a.
A. Van Den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu.
Conditional image generation With PixelCNN decoders. In Advances in Neural Information
Processing Systems, 2016b.
C. Villani. Optimal Transport: Old and New. Springer Berlin Heidelberg, 2008.
S. Zha, J. Song, and S. Ermon. ToWards deeper understanding of variational autoencoding models.
arxiv:1702.08658, 2017.
S. Zhao, J. Song, and S. Ermon. Learning hierarchical features from deep generative models. In
International Conference on Machine Learning, 2017.
10
Under review as a conference paper at ICLR 2020
A StackedWAE derivation details
A.1 Marginal constraint
The space of couplings Γ ∈ P (PD ,Pθ) that defines the OT distance in Eq. (3) is constrained
according to Eq. (4). WAE assumes ajoint density of the form given in Eq. (5), which automatically
satisfies the PD marginal constraint, but requires the further sufficient condition given in Eq. (6) in
order to satisfy the pθ marginal constraint. To see that Eq. (6) is indeed a sufficient condition for
the pΘ marginal constraint, note that from the Markovian assumption of the generative model (see
Eq. (1)), we can write Y as
∀(x, X) ∈ X X X,
γ(x,x) = /	Pθι (X∣Zl) qφι (zi,..., ZN |x) PD (x) dzi ... dzN
Z1 ...ZN
=/	Pθι (x|zi) qφι (zi|x)Pd(x) dzi
(13)
The constraint in Eq. (4) on the PD marginal is trivially true as the integral over the second variable
can be brought inside the integral over Z1, after which all the integrals simply integrate to unity
leaving PD .
The constraint on the second marginal is obtained by integrating Eq. (13) over the first variable,
∀x ∈ X,
I γ(x,X) dx
X
Pθι (X∣Zl)qφι (zi ∣x)pd(x)dzi dx
/	Pθι (x|zi) / qφι (zι∖x)pD(x)dxdzι
(14)
Thus, to satisfy Eq. (4), we need:
∀x ∈ X,
/ Pθ(x∖zι) / qφι(zι∖x)pD(x)dx dzi n=dPθ(x)
(15)
|
}
=/ Pθι (x|zi)p82：N(Zi) dzi
where the definition of the generative model from Eq. (1) was used and we introduced:
∀zi ∈ Zi ,
pθ2:N (ZI)
def
/
Z2...ZN
Pθ2(zi∖z2) ...PθN(zN-i∖zN)P(zN) dz2 ...dzN
(16)
To satisfy Eq. (15), one obvious sufficient condition on the aggregated posterior distribution Qlgg(Zi)
defined in Eq. (8) is Eq. (6), namely that
∀zi ∈ Zi, qfg(zi) = pΘ2：N(zi)	(17)
which is what we sought out to show. However, Eq. (17) is a sufficient condition, not a necessary one:
indeed Eq. (17) must only hold under JZl Pθ(X∣zi)dzi. So for example, if Pθ, (X∖zi) = Pθ, (X),
then Eq. (15) would boil down to a constraint only on the expectations of Qigg(Zi) and Pθ2,n (Zi).
11
Under review as a conference paper at ICLR 2020
A.2 WAE objective
Starting from the definition of the OT distance given in Eq.(3), and using the WAE approach with
density Y written as Eq. (13), we find:
OTc(Pd,Pθ) =	inf	/	c(x,X) dΓ(x,X)
Γ∈P(PD,PΘ) X×X
(18)
≤
inf	/	c(x,X)	/	pθι (X∣zι)qφι (zι∖x)p∏(x)dzι dxdx
Qφ1 (Z1,Z2,...Zn |X),	X×X	Z1
Jzi Pθ(xlz1)q1gg(z1)dz1= Jzi PΘ(XIzI)Pθ2:N (ZI)dz1
Given that the above does not depend on z>1, the inf can be written over Qφ1 (Z1∖X) rather than
the full Qφ1 (Z1,Z2,...Zn∖X). Replacing the constraint in the inf with the sufficient condition
according to Eq. (6), which amounts to replacing Eq.(15) with Eq. (17), we obtain:
OTc(PD,PΘ) ≤
inf
Qφi (Zι∣x),
QISg=Pθ2: N
I	c(x, x)	/
X×X	Z1
Pθι (x∖zι)qφι (zι ∖x)pd(x)dzι dxdX
(19)
Eq. (7) is then obtained by relaxing constraint in Eq. (19); replacing the hard constraint by a soft
constraint via a penalty term added to the objective, weighted by a λ1 :
Wcc(PD,PΘ)
inf
Qφ1 (z1 ∣x)
I	I c(x, X)
X×X	Z1
Pθι (X|zi) qφι (zi|x) PD (x) dz∖dxdX
(20)
+ λι Dι( Qagg(Z1), Pθ2:N (Zι))
where D1 is any divergence function between distributions on Z1.
B Experiments
B.1 MNIST experiments
We train a deep-hierarchical latent-variable model with N = 5 latent layers whose dimensions are
dZ1 = 32, dZ2 = 16, dZ3 =8, dZ4 =4and dZ5 =2, respectively. We parametrise the generative
and inference models as:
qφi (ZiIzi-I) = N (Zi; μq (Zi-I), ς? (Zi-I)) , i = 1,..∙, 5
Pθi(zi-i∣Zi) = N(Zi-1； μp(zi), Σp(zi)),	i = 2,..., 5	(21)
pθ1(x∖Z1)=δ x - fθ1(Z1)
For both the encoder and decoder, the mean and diagonal covariance functions μi, ∑i are fully-
connected networks with 2 same-size hidden layers (consider fθι as μp). For i = 1,2, 3,4,5, the
number of units is 2048, 1024, 512, 256, 128, respectively.
For the regularisation hyperparameters, we use Qn=ι λi = λec/dzn for n = 1,..., 4 (for each recon-
struction term in the objective), and Q5=1 λi = λmatch (for the final divergence term). We then perform
a grid search over the 25 pairs (λrec, λmatch) ∈ {0.01,0.05,0.1,0.5,1}③{10-4,10-3,10-2, 10-1,1}
and find the best result (smallest Eq. (10)) is obtained with (λrec, λmatch) = (0.05,10-4).
We choose the squared Euclidean distance as the cost function: Cn(Zn, Zn) = ∣∣zn - Znk22. The
expectations in Eq. (10) are computed analytically whenever possible, and with Monte Carlo sampling
otherwise.
We use batch normalisation (Ioffe & Szegedy, 2015) after each hidden fully-connected layer, followed
by a ReLU activation (Glorot et al., 2011). We train the models over 5,000 epochs using Adam
optimiser (Kingma & Ba, 2015) with default parameters and batch size of 128.
12
Under review as a conference paper at ICLR 2020
	Inputs		Layer i Filters dim. Resampling Output dim.
	:CoIIV.‘Block) I	×2	Layer 1 5 X 5 X 64	down / up	16x16x1
「八 D	∣⊂~~~~Γ7		~>	Layer 2	3×3×64	down / up	8X8X2 'onv∙ ReSamPlmgJ [Residual CoImeCtlOlIJ	J	r ɪ~：	I	Layer 3 3x3x64	down / up	4x4x4 (+/	 Layer 4	3x3x128	None	4x4x2 (BatChNoL+ReLu]	Layer5	3x3x128	None	4x4x1 J 二~	Layer 6	3×3×	128	down / up	2x2x2			
	[UollVOnnIon J I Outputs	(a)	(b)
Figure 6: (a) Residual network with 3 hidden convolutions. (b) Details of the architecture use in
Section 3.2.
B.2 SVHN experiment
SVHN setup
We train a 6-layers StackedWAE on the SVHN dataset using both the training dataset (73,257
digits) and the additional training dataset (531,131 digits). The mean and covariance functions of the
inference networks and generative models are parametrised by 3-layer ResNet-Style (Kaiming et al.,
2015) neural networks.
A M -layer residual network is composed of M - 1 convolutional blocks followed by a resampling
convolution, and a residual connection. The outputs of the two are added and a last operation (either
fully connected or convolution layer) is applied on the result. A convolutional block is composed of a
convolution layer followed by batch normalisation (IOfe & Szegedy, 2015) and a ReLU non-linearity
(Glorot et al., 2011). We also use batch normalisation and ReLU after the sum of the convolutional
blocks output and the residual connection. See Figure 6a for an example of a 3-layers residual
network with a last convolution operation.
When resampling, we use a convolution layer with stride 2 in both the skip connection and the
resampling covolution for the inference networks and a deconvolution layer with stride 2 in both
the skip connection and the resampling convolution in the generative models. If no resampling is
performed, then the resampling convolution is a simple convolution layer with stride 1 and the skip
connection performs the identity operation. The latent dimensions are then given by the dimensions
and the number of features in the last convolutional operation. More specifically, we use M =2
convolutional blocks with the dimensions of the filters specified in Table 6b. Networks in layers 1, 2,3
have 96 convolution filters while whose in layers 4, 5, 6 have 128 filters, each filters having the same
size within each residual network, and doubling (in the inference networks) or divide by 2 (in the
generative models) their number in the resampling convolution if any resampling is performed. The
latent layers 1, 2, 3 and 6 have a stride of 2 and we choose the number of features to be 1, 2, 4, 2, 1, 1
for the latent layers i = 1... 6 (see Table 6b for the full details).
For the regularisation hyperparameters, we use Qp=ι λi = λrep-1)/2+1 for P = 1,..., 5 (each
reconstruction term in the objective), and Q6=1 λi = λmatch (the final divergence term). The choice
for the reconstruction weights is motivated by the fact that the effective regularisation hyperparameters
scale exponentially. Thus, to avoid the collapse (or blowing up for λrec > 1) of the corresponding
reconstruction terms, we choose the weights to scale like O(λpec2). We found that (λrec, λmatch)=
10-1, 10-4 worked well with our experimental setup.
We train the models over 1000 epochs using Adam optimiser (Kingma & Ba, 2015) with default
parameters and batch size of 100.
13