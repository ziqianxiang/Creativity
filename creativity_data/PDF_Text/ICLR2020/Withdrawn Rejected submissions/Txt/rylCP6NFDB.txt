Under review as a conference paper at ICLR 2020
Hindsight Trust Region Policy Optimization
Anonymous authors
Paper under double-blind review
Ab stract
As reinforcement learning continues to drive machine intelligence beyond its con-
ventional boundary, unsubstantial practices in sparse reward environment severely
limit further applications in a broader range of advanced fields. Motivated by the
demand for an effective deep reinforcement learning algorithm that accommo-
dates sparse reward environment, this paper presents Hindsight Trust Region Pol-
icy Optimization (HTRPO), a method that efficiently utilizes interactions in sparse
reward conditions to optimize policies within trust region and, in the meantime,
maintains learning stability. Firstly, we theoretically adapt the TRPO objective
function, in the form of the expected return of the policy, to the distribution of
hindsight data generated from the alternative goals. Then, we apply Monte Carlo
with importance sampling to estimate KL-divergence between two policies, tak-
ing the hindsight data as input. Under the condition that the distributions are
sufficiently close, the KL-divergence is approximated by another f -divergence.
Such approximation results in the decrease of variance and alleviates the insta-
bility during policy update. Experimental results on both discrete and continu-
ous benchmark tasks demonstrate that HTRPO converges significantly faster than
previous policy gradient methods. It achieves effective performances and high
data-efficiency for training policies in sparse reward environments.
1	Introduction
Reinforcement Learning has been a heuristic approach confronting a great many real-world prob-
lems from playing complex strategic games (Mnih et al., 2015; Silver et al., 2016; Justesen et al.,
2019) to the precise control of robots(Levine et al., 2016; Mahler & Goldberg, 2017; Quillen et al.,
2018), in which policy gradient methods play very important roles(Sutton et al., 2000; Deisenroth
et al., 2013). Among them, the ones based on trust region including Trust Region Policy Opti-
mization (Schulman et al., 2015a) and Proximal Policy Optimization (Schulman et al., 2017) have
achieved stable and effective performances on several benchmark tasks. Later on, they have been
verified in a variety of applications including skill learning(Nagabandi et al., 2018), multi-agent
control(Gupta et al., 2017), imitation learning(Ho et al., 2016), and have been investigated further
to be combined with more advanced techniques(Nachum et al., 2017; Houthooft et al., 2016; Heess
et al., 2017).
One unresolved core issue in reinforcement learning is efficiently training the agent in sparse reward
environments, in which the agent is given a distinctively high feedback only upon reaching the
desired final goal state. On one hand, generalizing reinforcement learning methods to sparse reward
scenarios obviates designing delicate reward mechanism, which is known as reward shaping(Ng
et al., 1999); on the other hand, receiving rewards only when precisely reaching the final goal states
also guarantees that the agent can focus on the intended task itself without any deviation.
Despite the extensive use of policy gradient methods, they tend to be vulnerable when dealing with
sparse reward scenarios. Admittedly, policy gradient may work in simple and sufficiently rewarding
environments through massive random exploration. However, since it relies heavily on the expected
return, the chances in complex and sparsely rewarding scenarios become rather slim, which often
makes it unfeasible to converge to a policy by exploring randomly.
Recently, several works have been devoted to solving the problem of sparse reward, mainly applying
either hierarchical reinforcement learning (Kulkarni et al., 2016; Vezhnevets et al., 2017; Le et al.,
2018; Marino et al., 2019) or a hindsight methodology, including Hindsight Experience Replay
1
Under review as a conference paper at ICLR 2020
(Andrychowicz et al., 2017), Hindsight Policy Gradient (Rauber et al., 2019) and their extensions
(Fang et al., 2019; Levy et al., 2019). The idea of Hindsight Experience Replay(HER) is to regard the
ending states obtained through the interaction under current policy as alternative goals, and therefore
generate more effective training data comparing to that with only real goals. Such augmentation
overcomes the defects of random exploration and allows the agent to progressively move towards
intended goals. Itis proven to be promising when dealing with sparse reward reinforcement learning
problems.
For Hindsight Policy Gradient(HPG), it introduces hindsight to policy gradient approach and im-
proves sample efficiency in sparse reward environments. Yet, its learning curve for policy update
still oscillates considerably. Because it inherits the intrinsic high variance of policy gradient meth-
ods which has been widely studied in Schulman et al. (2015b), Gu et al. (2016) and Wu et al.
(2018). Furthermore, introducing hindsight to policy gradient methods would lead to greater vari-
ance (Rauber et al., 2019). Consequently, such exacerbation would cause obstructive instability
during the optimization process.
To design an advanced and efficient on-policy reinforcement learning algorithm with hindsight expe-
rience, the main problem is the contradiction between on-policy data needed by the training process
and the severely off-policy hindsight experience we can get. Moreover, for TRPO, one of the most
significant property is the approximated monotonic converging process. Therefore, how these ad-
vantages can be preserved when the agent is trained with hindsight data also remains unsolved.
In this paper, we propose a methodology called Hindsight Trust Region Policy Optimization
(HTRPO). Starting from TRPO, a hindsight form of policy optimization problem within trust region
is theoretically derived, which can be approximately solved with the Monte Carlo estimator using
severely off-policy hindsight experience data. HTRPO extends the effective and monotonically iter-
ative policy optimization procedure within trust region to accommodate sparse reward environments.
In HTRPO, both the objective function and the expectation of KL divergence between policies are
estimated using generated hindsight data instead of on-policy data. To overcome the high variance
and instability in KL divergence estimation, another f -divergence is applied to approximate KL
divergence, and both theoretically and practically, it is proved to be more efficient and stable.
We demonstrate that on several benchmark tasks, HTRPO can significantly improve the performance
and sample efficiency in sparse reward scenarios while maintains the learning stability. From the
experiments, we illustrate that HTRPO can be neatly applied to not only simple discrete tasks but
continuous environments as well. Besides, it is verified that HTRPO can be generalized to different
hyperparameter settings with little impact on performance level.
2	Preliminaries
Reinforcement Learning Formulation and Notation. Consider the standard infinite-horizon re-
inforcement learning formulation which can be defined by tuple (S, A, π, ρ0 , r, γ). S represents the
set of states and A denotes the set of actions. π : S → P(A) is a policy that represents an agent’s
behavior by mapping states to a probability distribution over actions. ρ0 denotes the distribution of
the initial state s0 . Reward function r : S → R defines the reward obtained from the environment
and γ ∈ (0, 1) is a discount factor. In this paper, the policy is a differentiable function regarding
parameter θ. We follow the standard formalism of state-action value function Q(s, a), state value
function V (s) and advantage function A(s, a) in Sutton & Barto (2018). We also adopt the defini-
tion of γ-discounted state visitation distribution as ρθ(s) = (1 - γ) Pt∞=0 γtP (st = s) (Ho et al.,
2016), in which the coefficient 1 - γ is added to keep the integration of ρθ (s) as 1. Correspondingly,
γ-discounted state-action visitation distribution (Ho et al., 2016), also known as occupancy measure
(Ho & Ermon, 2016), is defined as ρθ(s,a) = ρθ(S) X ∏θ(a|s), in which ∏θ(a|s) stands for the
policy under parameter θ.
Trust Region Policy Optimization(TRPO). Schulman et al. (2015a) proposes an iterative trust
region method that effectively optimizes policy by maximizing the per-iteration policy improvement.
The optimization problem proposed in TRPO can be formalized as follows:
max LT RP O(θ)	(1)
θ
2
Under review as a conference paper at ICLR 2020
s.t. E	[DκL(∏d(a∣s)∣∣∏θ(a|s))] ≤ C
S 〜Pj(S)
(2)
∞
in which Pd(S) = Et=O YtP(St = s). θ denotes the parameter of the new policy while θ is that of
the old one. Trajectory is represented by τ = s1, a1, s2, a2,   The objective function LTRPO(θ)
can be given out in the form of expeted return:
LTRPO(θ)
|S)
|S)
S, a)
(3)
Hindsight Policy Gradient(HPG). After generalizing the concept of hindsight, Rauber et al. (2019)
combines the idea with policy gradient methods. Though goal-conditioned reinforcement learning
has been explored for a long time and actively investigated in recent works(Peters & Schaal, 2008;
Schaul et al., 2015; Andrychowicz et al., 2017; Nachum et al., 2018; Held et al., 2018; Nair et al.,
2018; Veeriah et al., 2018), HPG firstly extends the idea of hindsight to goal-conditioned policy
gradient and shows that the policy gradient can be computed in expectation over all goals. The
goal-conditioned policy gradient is derived as follows:
X Vθ log πθ(at | St, g)Aθ(St, at, g)
(4)
Vθ η(θ) = E E
g T 〜Pθ(τ |g)
Then, by applying hindsight formulation, it rewrites goal-conditioned policy gradient with trajecto-
ries conditioned on some other goal g0 using importance sampling (Bishop, 2016) to improve sample
efficiency in sparse-reward scenarios.
In this paper, we propose an approach that introduces the idea of hindsight to TRPO, called Hind-
sight Trust Region Policy Optimization(HTRPO), aiming to further improve policy performance
and sample efficiency for reinforcement learning with sparse rewards. In Section 3 and Section
4, we demonstrate how to redesign the objective function and the constraints starting from TRPO
respectively.
3 Expected Return and Policy Gradients of HTRPO
In order to apply hindsight methodology, this section presents the main steps for the derivation of
HTRPO objective function. Starting from the original optimization problem in TRPO, the objective
function can be written in the following variant form:
L领=U(T) X Ytπθ⅛⅛jAθ(st,at)	⑸
The derivation process of this variant form is shown explicitly in Appendix A.1 and in Schulman
et al. (2015a).
Given the expression above, we consider the goal-conditioned objective function of TRPO as a
premise for hindsight formulation. Similar to equation 4, Lθd(θ) can be correspondingly given out in
the following form:
Lθ(θ = E E	X Y t nθ (at |St，g) Ad(St, at, gj]	⑹
θ g 卜〜PMT|g)[金	πd(atlst,g) θ	H
For the record, though it seems that equation 6 makes it possible for off-policy learning, it can be
used as the objective only when policy πθ is close to the old policy πθd, i.e. within the trust region.
Using severely off-policy data like hindsight experience will make the learning process diverge.
Therefore, importance sampling need to be integrated to correct the difference of the trajectory
distribution caused by changing the goal. Based on the goal-conditioned form of the objective
function, the following theorem gives out the hindsight objective function conditioned on some goal
g0 with the distribution correction derived from importance sampling.
Theorem 3.1 (HTRPO Objective Function). For the original goal g and an alternative goal g0 , the
object function of HTRPO Lθd(θ) is given by:
∞ /nʌ _ πr π7 「1-Γ πd(ak |sk ,gO) t πθ (at|st,gO) / /	/、]]	门、
Lθ(θ) = E E 〉/ I I γ i	√γ	7 i	八Ad(St, at, g )	,	⑺
θ	g0 卜〜Pθ (T |g) [t=0 M πd(ak lsk,g)	πd(atlst,g0) θ	JJ
3
Under review as a conference paper at ICLR 2020
in which, τ = s1 , a1 , s2, a2, ..., st, at.
Appendix A.2 presents an explicit proof on how the hindsight-form objective function derives from
equation 6. It will be solved under a KL divergence expectation constraint, which will be discussed
in detail in Section 4. Intuitively, equation 7 provides a way to compute the expected return in
terms of the advantage with new-goal-conditioned hindsight experiences which are generated from
interactions directed by old goals.
Naturally, Theorem 3.2 gives out the gradient of HTRPO objective function that will be applied to
solve the optimization problem. Detailed steps of computing the gradient is presented in Appendix
A.3.
Theorem 3.2 (Gradient of HTRPO Objective Function). For the original goal g and an alternative
goal g0, the gradient VθL^(θ) of HTRPO object function with respect to θ is given by the following
expression:
v7 ∞ /nʌ π-,	π-,	「π πθ(ak |sk ,gO) t vθ πθ (at|st,gO) / /	0、1]	0、
vθLθ(θ) = E E × Il	一I-----:Y ——Γ~∖——丁Ad(St,at,gj , ⑻
g0 卜〜Pθ(T|g) [t=0M πe(aklsk,g)	πθ(at∖st,gD	JJ
in which τ = s1, a1, s2, a2, ..., st, at.
4	Expectation of KL Divergence Estimation
This section firstly demonstrates some techniques, with strict proof, that can be used to estimate the
expectation of KL-divergence and further reduce the variance, and then presents how hindsight is
applied to the constraint function of TRPO.
In TRPO, the KL divergence expectation under ρθd(s) is estimated by averaging all the values of
KL divergence. When they are respectively conditioned on all states collected using the old policy,
this kind of estimation is exactly Monte Carlo estimation which is unbiased. However, when we
only have access to hindsight experience data, the state distribution may inevitably change and
the previous method for estimating the expectation of KL divergence is no longer valid. To solve
this problem, we firstly transform the KL divergence to an expectation under occupancy measure
Pθ(s,a) = Pθ(s) X ∏d(a∣s). It Can be estimated using collected state-action pair (s,a), whose
changed distribution can be corrected by importance sampling. Then, by making use of another
f -divergence, the variance of estimation is theoretically proved to be reduced so as to facilitating a
more stable training.
The constraint function in KL-divergence can be naturally converted to a logarithmic form. Ap-
pendix B.1 provides a more explicit version of this conversion.
Theorem 4.1 (Logarithmic Form of Constraint Function). Given two policies ∏d(a∣s) and ∏e(a|s),
the expectation of their KL-divergence over states S 〜Pd(S) is written as:
E	[DκL(∏d(a∣s)ll∏θ(a|s))] = E	[log∏d(a∣s) - log∏θ(a|s)]	(9)
s~Pθ(S)	s,a2pθ(s,a)
However, simply expanding the KL-divergence into logarithmic form still leaves several problems
unhandled. Firstly, the variance remains excessively high, which would cause considerable insta-
bility during the learning process. Secondly, current estimation of KL-divergence is of possible
negativity. If encountering negative expectation of KL-divergence, the learning process would result
in fatal instability.
The following Theorem 4.2 describes a technique to reduce the variance and Theorem 4.3 gives out
the strict proof for the decrease of variance.
Theorem 4.2	(Approximation of Constraint Function). For policy ∏d(a∣s) and ∏e(a|s), and for
η = ∏θ (a|s) 一 ∏d(a∣s),
2(log∏d(a∣s) ― log∏θ(a∣s))2
E	[log ∏d(a∣s) — log ∏θ (a|s)] = E
s,a〜Pθ(s,a	s,a〜Pθ(s,a ,
+ E	[o(η3)] .
s,a〜Pθ(s,a)
(10)
4
Under review as a conference paper at ICLR 2020
Theorem 4.2 demonstrates that when θ and θ is oflimited difference, the expectation of log ∏^(a∣s)-
log ∏θ (a|s) can be sufficiently estimated by the expectation of its square. The proof is provided in
Appendix B.2. In fact, Es,a〜p#,a)[2(logπ^(a∣s) - log∏θ(a|s))2] is the expectation of an f-
divergence, where f (x) = 2χ(log x)2. Noticeably, f (x) is a strictly convex function when X ∈
(e, ∞), and f (1) = 0.
Moreover, it is noteworthy that there are two corresponding major improvements through this kind of
estimation. Firstly, it is guaranteed to reduce the variance which leads to a more stable performance.
This merit will be explained in detail in Theorem 4.3. Another significant improvement is manifested
in the elimination of negative KL-divergence, since the estimation presents itself in the form of a
square which is always non-negative.
Theorem 4.3	(Variance of Constraint Function). For policy π^(a∣s) and ∏θ(a|s), let Var denotes the
variance of a variable. For any action a ∈ A and any state S ∈ S, when log π^(a∣s) - log ∏(a|s) ∈
[-0.5, 0.5], then
Var
s,a 〜Pg(S,a)
一(log∏d(a∣s) - log∏θ(a|s))2 -
2
≤ Var	[log∏j(a∣s) — log∏θ(a|s)].
s,a 〜Pg(S,a)
(11)
Theorem 4.3 illustrates that there is a decrease from the variance of log ∏^(a∣s) - log ∏θ (a|s) to the
variance of its square, and furthermore indicates that the variance is effectively reduced. The proof
is given in detail in Appendix B.3. In fact, the closer it is between θ and θ, the more the variance
decreases.
Based on Theorem 4.1 to Theorem 4.3, in this paper, we adopt the following form of constraint
condition:
E
s,a 〜Pg(S,a)
2(log∏θ(a∣s) - log∏θ(a|s))2
(12)
≤ .
In Theorem 4.4, we demonstrate that hindsight can also be introduced to the constraint function. The
proof follows the methodology similar to that in Section 3, and is deducted explicitly in Appendix
B.4.
Theorem 4.4	(HTRPO Constraint Function). For the original goal g and an alternative goal g0 , the
constraint between policy π^(a∣s) and policy ∏θ(a|s) is given by:
Eθ[	E ʌ[l XE Y:πθ5ak sk,g；γt(log∏g(at↑St,g0) - log∏θ(at∣St,g0))2]] ≤ e0.	(13)
g0 卜〜Pθ(τ|g) [2 t=0 k=ι πθ(ak lsk,g)	θ	H
in which e0 = ɪ--Y.
Theorem 4.4 implies the practicality of using hindsight data under condition g0 to estimate the ex-
pectation. From all illustration above, we give out the final form of the optimization problem for
HTRPO:
Trl	Trl	∖∞ T~Γ πQ(ak ∣sk,gO) t πθ (at|st,gO)ZI /	0、]]	/1 小
max E	E	T——TY^——XAa(St,at,g)∣ I	(14)
θ g0 卜〜pθ(t∣g) [t=0 k=1 πθ(ak|sk,g)	πθ(at∖st,gD	H
s.t. Eθ[ E ʌ[ɪ x∞ YY福，卜,g]Yt(log∏θ(at∣St,g') - log∏Θ(at∣St,g0))2]] ≤ e0. (15)
g0 卜〜pθ(t|g) [2 t=0 k=ι na(ak|sk,g)	H
The solving process for HTRPO optimization problem is explicitly demonstrated in Appendix C and
the complete algorithm procedure is included in Appendix D.
5	Experiments
This section demonstrates the validation of HTRPO on several sparse reward benchmark tasks1. The
design of our experiments aims to conduct an in-depth investigation in the following aspects:
1The source code and video can be found at https://github.com/HTRPOCODES/HTRPO.
5
Under review as a conference paper at ICLR 2020
(a) Bit Flipping
(d) Fetch
(c) Four Rooms
Figure 1: Demonstration of experimental environments
•	How is the effectiveness of HTRPO?
•	How does each component of HTRPO contribute to its effectiveness?
•	How is the performance of policy gradient methods trained with hindsight data in continu-
ous environments?
•	How sensitive is HTRPO to network architecture and some key parameters?
5.1	Experimental Settings
We implement HTRPO on a variety of reinforcement learning environments, including Bit Flipping,
Grid World and Fetch. Among them, Bit Flipping, Grid World, Fetch Reach and Fetch Push are
implemented as descrete-action environments while we also conduct continuous version of experi-
ments in Fetch Reach, Fetch Push and Fetch Slide. A glimpse of these environments is demonstrated
in Figure 1 while the detailed introductions are included in Appendix F.1. The reward mechanisms
are intentionally modified to sparse reward regulations. Besides, for continuous version of Fetch
experiments, we apply an additional policy entropy bonus to encourage more exploration.
For each trail of interaction, reward for the agent is set as the remaining number of time steps plus
one, and all goals during exploration are chosen uniformly at random for both training and evalua-
tion. During the training process, we terminate one episode either when the maximum number of
time steps has elapsed or when the goal state is reached. We evaluate agents’ performance by docu-
menting 10 learning trails in the form of average return and their corresponding standard deviation.
In Bit Flipping and Grid World environments, the network architecture is of two hidden layers, each
with 64 hyperbolic tangent units; in Fetch environment, for both discrete and continuous implemen-
tations, the network contains two 256-unit hidden layers. For all environments mentioned above,
we compare HTRPO with HPG (Rauber et al., 2019) and TRPO (Schulman et al., 2015a), which
are chosen as the baseline algorithms. Since HPG is never applied to continuous environments in
Rauber et al. (2019), we implement HPG to be adapted to continuous environments. Note that
the way we scale the time axis is significantly different from that in Rauber et al. (2019). Instead
of regarding a certain number of training batches as interval between evaluation steps, we directly
uses the accumulated time steps the agent takes while interacting with the environments throughout
episodes and batches.
Besides comparing with baselines, we also ablate each component of HTRPO to investigate how
significant it is for the final performance. To be specific, we adopt the “vanilla” estimation of KL-
divergence which we call “HTRPO with KL1” instead of the proposed one in Section 4; we also
observe the performance of our algorithm without weighted importance sampling, which is denoted
as “HTRPO without WIS” in this paper.
5.2	Comparative Analysis
In discrete environments, we test both the official version of HPG released in Rauber et al. (2019)
and our HPG implementation while for continuous environments of Fetch, we only test our HPG due
to the lack of surpport for continuous tasks in Rauber et al. (2019). We apply input normalization in
6
Under review as a conference paper at ICLR 2020
HTRPO M our HPG M official HPG -HTRPOwithKLI M HTRPO without WIS MTRPO
543210
nruteR egarevA
5
012345
Time steps (1e4)
(a) 8-Bit Flipping
01	234
Time steps (1e4)
(b) 16-Bit Flipping
012345
Time steps (1e4)
nruteR egarevA
01	2345
Time steps (1e4)
(d) Four Rooms
012345
Time steps (1e5)
(e) Discrete Fetch Reach
(c) Empty Maze
0	4	8	12	16	20
Time steps (1e5)
(f) Discrete Fetch Push
Figure 2:	Evaluation curves for discrete environments. The full lines represent the average evalua-
tion over 10 trails and the shaded regions represent the corresponding standard deviation.
HTRPO M our HPG -HTRPOwithKLI M HTRPO without WIS MTRPO
0	1	2	3	4	5	0	4	8	12	16	20	0	4	8	12	16	20
Time steps (1e5)	Time steps (1e5)	Time steps (1e5)
(a) Countinuous Fetch Reach	(b) Countinuous	Fetch Push	(c)	Countinuous	Fetch Slide
Figure 3:	Evaluation curves for continuous environments. The full lines represent the average eval-
uation over 10 trails and the shaded regions represent the corresponding standard deviation.
the continuous Fetch environments for better performance. However, for fair comparison with the
official HPG, we do not employ this trick in any of the discrete environments.
The evaluation curves for the trained policy are demonstrated in Figure 2 and 3 and the training
curves and success rate of these experiments are supplemented in Appendix F.3. Detailed settings of
hyperparameters for all experiments are listed in Appendix E. From results demonstrated in Rauber
et al. (2019), the officially released version of HPG eventually converges to similar performances
with that of HTRPO in discrete environments, but sometimes, unlike our HPG, it is still far from
converging under this time-step evaluation setting. This kind of distinction in converging speed
between our HPG and the official HPG may be caused by the reduction of noises, since we use TD-
error to update policies instead of the return corrected by importance sampling, which is adopted
in HPG. Thus, for the fairness of comparison, in the following analysis, we mainly compare the
properties between HTRPO and our HPG.
How is the effectiveness of HTRPO?
From the results we can see that in both discrete and continuous environments, HTRPO outper-
forms HPG significantly. Aside from assuring a good converging property, the sample efficiency
of HTRPO also exceed that of HPG, for it reaches a higher average return within less time in most
7
Under review as a conference paper at ICLR 2020
environments. As for TRPO, though it can converge in several simple tasks like Bit Flipping, Grid
World and continuous Fetch Reach, it remains incompetent in dealing with complex control tasks
including Fetch Push and Fetch Slide, in all of which HTRPO can learn a good policy. The reason is
that for TRPO, it is basically impossible to acquire a positive reward at the beginning of the training
in such environments, which makes the policy updates meaningless.
How does each component of HTRPO contribute to its effectiveness?
In both Figure 2 and Figure 3, “HTRPO with KL1” and “HTRPO without WIS” performs much
worse than the complete version of HTRPO. When we estimate the KL-divergence using the
“vanilla” KL-divergence defined as equation 9, it causes severe instability, which means that the
estimated KL-divergence can be negative with an unacceptable probability. Considering the prac-
ticality of the experiment, the corresponding iteration will be skipped without any updates of the
policy in this senario. Given the phenomenon stated above, the final performance of “HTRPO with
KL1” is much worse and more unstable in all environments. As for the study of Weighted Impor-
tance Sampling, it is widely known for significantly reducing the variance (Bishop, 2016), which
is once again proved by the results of “HTRPO without WIS”. Admittedly, we can see that the
performance of “HTRPO without WIS” matches the full version of HTRPO in several simple envi-
ronments in Figure 2 (a)-(d) and Figure 3 (a). However, for more complex environments like Fetch
Push and Fetch Slide, the variance is detrimentally larger than that in simple environments. In short,
the performance of “HTRPO without WIS” has a severe degradation comparing to the full version
of HTRPO.
How is the performance of policy gradient methods trained with hindsight data in continuous
environments?
As mentioned in Plappert et al. (2018), it still remains unexplored that to what extent the policy
gradient methods trained with hindsight data can solve continuous control tasks. In this section, we
will provide the answer. We implement HTRPO in continuous control tasks including Fetch Reach,
Fetch Push and Fetch Slide. HPG is tested as well for comparison. From the results, we can see
that with the help of input normalization, HPG can learn a valid policy in continuous control tasks.
Still, HTRPO performs much better than HPG in all three environments, benefiting from a faster and
more stable convergence. As illustrated in Figure 3, HTRPO eventually achieves an average success
rate of 92% for Fetch Push and 82.5% for Fetch Slide.
How sensitive is HTRPO to network architecture and some key parameters?
To study the sensitivity of HTRPO to different network architectures, we observe the performance of
HTRPO with different network settings. From the results demonstrated in Appendix F.2.1, HTRPO
achieves commendable performances with all three different network architectures while HPG only
converges under certain settings. As for the sensitivity of HTRPO to key parameters, we mainly ob-
serve the impact of different number of alternative goals. Based on the learning curves in Appendix
F.2.2, we can see that Hindishgt TRPO with more alternative goals achieves better converging speed.
6 Conclusion
We have extended the monotonically converging on-policy algorithm TRPO to accommodate sparse
reward environments by adopting the hindsight methodology. The optimization problem in TRPO
is scrupulously derived into hindsight formulation and, when the KL-divergence in the constraint
function is small enough, it can be tactfully approximated by another f -divergence in order to re-
duce estimation variance and improve learning stability. Experimental results on a variety of en-
vironments demonstrate effective performances of HTRPO, and validate its sample efficiency and
stable policy update quality in both discrete and continuous scenarios. Therefore, this work reveals
HTRPO’s vast potential in solving sparse reward reinforcement learning problems.
Acknowledgments
We greatly acknowledge all the fundings in support of this work.
8
Under review as a conference paper at ICLR 2020
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In
Advances in Neural Information Processing Systems, pp. 5048-5058, 2017.
C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer New York, 2016. ISBN 9781493938438.
S. Boyd, S.P. Boyd, L. Vandenberghe, and Cambridge University Press. Convex Optimization.
Berichte Uber Verteilte messysteme. Cambridge University Press, 2004. ISBN 9780521833783.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and TrendsR in Robotics, 2(1-2):1-142, 2013.
Meng Fang, Cheng Zhou, Bei Shi, Boqing Gong, Weitao Xi, Tianzhou Wang, Jia Xu, and Tong
Zhang. DHER: Hindsight experience replay for dynamic goals. In International Conference on
Learning Representations, 2019.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop:
Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016.
Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66-83. Springer, 2017.
Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez,
Ziyu Wang, SM Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich
environments. arXiv preprint arXiv:1707.02286, 2017.
David Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel. Automatic goal generation for
reinforcement learning agents. 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, pp. 4565-4573, 2016.
Jonathan Ho, Jayesh Gupta, and Stefano Ermon. Model-free imitation learning with policy opti-
mization. In International Conference on Machine Learning, pp. 2760-2769, 2016.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109-1117, 2016.
Niels Justesen, Philip Bontrager, Julian Togelius, and Sebastian Risi. Deep learning for video game
playing. IEEE Transactions on Games, 2019.
Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in
neural information processing systems, pp. 3675-3683, 2016.
Hoang Le, Nan Jiang, Alekh Agarwal, Miroslav Dudik, Yisong Yue, and Hal Daume, III. Hierar-
chical imitation and reinforcement learning. In Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2917-2926,
Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical reinforcement learning with hindsight.
In International Conference on Learning Representations, 2019.
Jeffrey Mahler and Ken Goldberg. Learning deep policies for robot bin picking by simulating robust
grasping sequences. In Conference on robot learning, pp. 515-524, 2017.
9
Under review as a conference paper at ICLR 2020
Kenneth Marino, Abhinav Gupta, Rob Fergus, and Arthur Szlam. Hierarchical RL using an ensem-
ble of proprioceptive periodic policies. In International Conference on Learning Representations,
2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy
trust region method for continuous control. arXiv preprint arXiv:1707.01891, 2017.
Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3303-3313,
2018.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics
for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), pp. 7559-7566. IEEE, 2018.
Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Vi-
sual reinforcement learning with imagined goals. In Advances in Neural Information Processing
Systems, pp. 9191-9200, 2018.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In ICML, volume 99, pp. 278-287, 1999.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21(4):682-697, 2008.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research. arXiv preprint
arXiv:1802.09464, 2018.
Michael JD Powell and J Swann. Weighted uniform samplinga monte carlo technique for reducing
variance. IMA Journal of Applied Mathematics, 2(3):228-236, 1966.
Deirdre Quillen, Eric Jang, Ofir Nachum, Chelsea Finn, Julian Ibarz, and Sergey Levine. Deep
reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of
off-policy methods. In 2018 IEEE International Conference on Robotics and Automation (ICRA),
pp. 6284-6291. IEEE, 2018.
PaUlo Rauber, Avinash Ummadisingu, Filipe Mutz, and Jurgen Schmidhuber. Hindsight policy
gradients. In International Conference on Learning Representations, 2019.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International Conference on Machine Learning, pp. 1312-1320, 2015.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Ricahrd S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Adaptive
Computation and Machine Learning series. MIT Press, 2018. ISBN 9780262352703.
10
Under review as a conference paper at ICLR 2020
Richard S. Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1):181-211,1999.
ISSN 0004-3702.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Vivek Veeriah, Junhyuk Oh, and Satinder Singh. Many-goals reinforcement learning. arXiv preprint
arXiv:1806.09605, 2018.
Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. FeUdal networks for hierarchical reinforcement learning. In
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceed-
ings of Machine Learning Research, pp. 3540-3549, International Convention Centre, Sydney,
Australia, 06-11 Aug 2017.
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade,
Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent
factorized baselines. arXiv preprint arXiv:1803.07246, 2018.
11
Under review as a conference paper at ICLR 2020
A Proof for Objective Function
A. 1 Deriviation from Equation 3 to Equation 5
With no influence to the optimal solution, We can multiply equation 3bya constant ι-1γ,
Ld(θ) = 士
_	1
1 - 7
∞
=X7t
t=0
Σ
s∈S
E
S 〜PG,a 〜nG(a|s)
||SS)) Aθd(S, a)
t∞=0γtP(st =s)
~Γ~
1-7
a/' Ad(S,a)
E
st^Pθ(st),at^∏θ'
"∏θ (at∣st)
(at∣st) l_nd(at|st)
Ad(St, at)
E
T 〜PG(T)
∞
Xγ
t=0
t ∏θ(at∣St)
∏d(at∣St)
Aθd(st , at )
(16)
A.2 Theorem 3.1
Theorem 3.1	(HTRPO Objective Function). For the original goal g and an alternative goal g0 , the
object function of HTRPO Lθd(θ) is given by:
Lθd(θ) = E
g0
E
T 〜Pθ (τ |g)
b Tl πθ(ak |sk ,g0) t πθ (at|st, g0) 4 /	0、1]
t=0 U ∏θ(ak ∣Sk,g)7 ∏⅛√) Ad(St, at, g)] J
(7)
in Which, τ = S1, a1, S2, a2, ..., St, at.
Proof. Starting from equation 6, for every time step t in the expectation, denote
so that
Lθd(θ, t) = E
g
T〜;
t πθ (at lst,g)
「Pθ(τ|g) [7 πd(at |St,g)
Aθd(st, at, g)
(17)
∞
Lθd(θ) = X Lθd(θ, t).
(18)
t0
Split every trajectory τ into τ1 and τ2 Where τ1 = S1, a1, S2, a2, ..., St, at and τ2 = St+1, at+1, ...,
then
Lθd(θ, t) = E
g	ti〜PG(TIIg) t2〜PG(T2|Ti，g).
'tπθ(atlst,g) λ /	ʌ
7∏⅛^) Ad(St,at,g)
For that 7t∏θ(at∣St,g) Ad(St, at, g) is independent from τ2 conditioned on τ1,
Lθd(θ, t) = E
g
E
τι〜Pd(TIIg) .
-tπθ(at|st,g) λ /	ʌ
7 ∏d(at∣st,g)Ad(St,at,g)
E [1]
, t2 〜PG(T2|T1,g)
(19)
Thus,
E
ti〜PG(TIIg) .
'tπθ(at|st,g) λ /	ʌ
7∏θ¼K^ Ad(St,at,g)
E
si:t,ɑi:t〜Pd(si：t,ai：t|g) _
'tπθ(at|St,g) λ /	ʌ
7∏⅛^) Ad(St,at,g)
(20)
∞
Lθd(θ) =XE
t=0 g
E
si：t，ai：t〜PG(si：t，ai：t|g) _
'tπθ(at|St,g) λ /	ʌ
7πθ¼K^) AiG
(21)
E
g
E
g
E
E
E
12
Under review as a conference paper at ICLR 2020
Following the techniques of importance sampling, the objective function can be rewritten in the form
of new goal g0 :
∞
L幽=X E0
E
si：t,ai：t 〜Pd(si：t,ai：t|g)
Pe(Si：t,aiMgO) tπθ(IMSt,g) ZI /	o∖
［而Y ∏θ⅛^)A …，g)
(22)
Furthermore, given that
t-1
p(S1:t, a1:t|g) = p(S1)p(at|St, g)	p(ak|Sk, g)p(Sk+1|Sk, ak),	(23)
k=1
after expanding the objective function and cancelling terms,
∞ 0_「旧	旧	π πe(ak|sk,g0) tπθ(at|st,g0) λ /	/、］］
Lθ(θ) =〉/ E	E	I I γ i	√γ	7 i	KAθ(st, at, g )
θ	t=0 g0 ［si：t,ai：t 〜PG(si：t,ai：t|g)［号 πθ(ak |Sk ,g)	πθ(atlst,g0) θ	JJ
(24)
A.3 Theorem 3.2
Theorem 3.2	(Gradient of HTRPO Objective Function). For the original goal g and an alternative
goal g0, the gradient VeLe(θ) of HTRPO object function with respect to θ is given by the following
expression:
VθLθe(θ) = E E
g0 T 〜Pθ(τ |g))
「π πe(ak|sk,g0) t vθπe(at|st,g0) λ /	o、］］
二 jɪ ∏e(ak∣sk ,g) Y ∏e(at∣st,gθ) Aθ(st, a g )
t=0 k=1
(8)
in which τ = S1, a1, S2, a2, ..., St, at.
Proof. Starting from equation 24, since that ∏e(at∣st, g0) is the only term relavant to θ, the corre-
sponding gradient of the objective function is computed by:
∞
VθLθe(θ) = X E
t=0 g0
E
si：t,ai：t 〜Pd(si：t,ai：t|g)
t
Y
k=1
∏d(ak ∣Sk,g0)
∏d(ak∣Sk ,g)
t Ve ∏θ (at∣st,g0)
πKat∣st,g0)
Aθe(St, at, g0)
))
(25)
13
Under review as a conference paper at ICLR 2020
B	Proof for Constraint Function
B.1 Theorem 4.1
Theorem 4.1 (Logarithmic Form of Constraint Function). Given two policies π^(a∣s) and ∏θ(a|s),
the expectation of their KL-divergence over states S 〜P石(S) is written as:
E	[DκL(%(a∣s)∣∣∏θ (a|s))] = E	[log %(a∣s)-log ∏θ (a|s)]	(9)
s~Pθ(s)	s,a2pθ(s,a)
Proof. Expand the expectation in equation 2 by the definition of KL-divergence,
E	[DκL(%(a∣s)∣∣∏θ (a|s))] = E E
s~Pθ(s)	s~pθ(s) [α~πMalS)
= E	[log %(a∣s)-log ∏θ (a|s)]	(26)
S 〜Pj(S),a 〜nj(a|s)
log πW
.g ∏θ(a∣s)J
B.2 Theorem 4.2
Lemma B.1 Given two distibutions p(x) and q(x), q(x) = p(x) + η(x), in which η(x)
variation of q(x) at p(x).
E [logp(x) - log q(x)] = E 2(logp(x) - log q(x))2 + E [o(η(x)3)]
Proof. Consider the second order Taylor expansion of log q(x) at p(x),
log q(x) = logP(X) + p(χyη(X) - 2p(1x)2 η(x)2 + ο(η(χ)3)
For the left side of equation 27,
E [log p(X) - log q(X)]
E [-P(X) η(χ) + 2P⅛n(x)2-0(n(X)3)
η(X) + 2p(1χ)2 η(x)2 - o(η(x)3))P(X) dx
/(2p1X)n(X)2 - P(X)o(n(X)3)) dX.
is the
(27)
(28)
(29)
For the first term on the right side of equation 27,
E 2(logP(X) - log q(X))2
2 E ](-P(X)η(') + 2P⅛n(X)2-o(n(X)3))2
1 E [p(X)2n(X)2 + o(n(X)3)
112	3
2 J (P(Xyη(X)2 + 0(η(X)3))P(X) dX
Z1	21	3
(2P(X) η(X)2 + 2 P(X)o(n(X)3)) dX
J(2 1 j(x)2 — P(X)o(η(X)3)) dX + JP(X)o(η(X)3) dX
E [log P(X) - log q(X)] + E [o(η(X)3)] .
(30)
Theorem 4.2	(Approximation of Constraint Function). For policy π^(a∣s) and ∏θ(a|s), and for
η = ∏θ(a|s) - ∏3(a∣s),
E	[log∏d(a∣s) - log∏θ(a|s)] = E
s,a〜Pθ(s,a	s,a〜Pθ(s,a)
2(log∏e(a∣s) - log∏θ(a∣s))2
+ E	[o(η3)] .
s,a〜Pθ(s,a)
(10)
14
UlIderreVieW as a COnferenCe PaPer at ICLR 2020
Proof BaSed on Lemma A∙L let--U 7Γ~4 一 S) and qsUTre (a--equation 27 results5-
equation 1。
B∙3 THEoREM 4∙3
Lemma B,2 FOr any random Variable Y m -075L
Var(Y2)Var(YL (31)
in WhiCh Var(Y) denotes the VarianCe Of γ∙
PyOof
Var(Y)I Var(Y2) H E(l) —≡y)一2 — E(y4) + 百(ye 2
HMy2) IE(W)T≡n2 — 百(飞)F 一
=H y2(l — y)(l + y)一— Ey(Il y)- Ey(I + YI
UCoV (y(l +YLy(IIy)L (32)
Deno-e XI(Y) = y(l + y) and X2(y) = y(l — y)∙ Then”
Var(Y) — Var(Y2) H COV(XLX2)
UE -XI(X2 — E(X2))一 (33)
There always exists 不 mthat SatiCefieS X2『)U IE(X2WheIl Y 」 et Xl(K) UFl
in WhiCh FIiSa COnStant，TheIl the equation Can be COnVerted by the fllg SteP 沼
Var(Y)I Var(Y2) H 里(XIl∙sl 二 X2 — E(X2))一 +∙sl 里 X2 — E(X2)一
“里(Xl t1)(X2—E(X2))一 (34)
ThU-WheIlY 口不"the two factors in equation 34“ (XI — Fl) and (X2 — IE(X2)) equal to 0
SimUItaneOUSly，
Alsit is easy to IIOtiCe that WheIl Y m -075L X- and XN are StriCtIy increasing With the increase
Of γ∙ Thu(XI — Fl) and (X2 — IE(X2)) ≡∙0 either both POSitiVe S both Hegativif not Zer0∙
Therefor 尸
Var(Y) — Var (Y2)0∙ (35)
Lemma B,3 FOr any random Variable Y”
Var(M) ≤ var(YL (36)
in WhiCh Var(Y) denotes the VarianCe Of γ∙
PrOof Apparently
一口育寻 ≥ 一 一fed. (37)
Jy Jy
CollSeqUentIy”
()≥(y∙ (38)
FOrthat
Var(Y) H E(y2) — (E(y)>(39)
We have
Var(M) ≤ Var(Y)∙ (36)
15
Under review as a conference paper at ICLR 2020
Theorem 4.3	(Variance of Constraint Function). For policy π^(a∣s) and ∏(a|s), let Var denotes
the variance of a variable. When log π^(a∣s) - log ∏θ(a|s) ∈ [-0.5,0.5], then
Var
s,a~ρg(S,a)
一(log∏d(a∣s) - log∏θ(a∣s))2 -
2
< Var	[log∏d(a∣s) - log∏θ(a|s)].
s,a~ρg(S,a)
(11)
Proof. let Y = | logπ^(a∣s) — log∏θ(a|s)|. GivenLemmaB.2,
Var	[| log %(a∣s)-log ∏θ (a∣s)∣2] ≤ Var [| log ∏^(a∣s) - log ∏θ (a∣s)∣]	(40)
s,a~Pθ(s,a)	s,a~Pθ(s,a)
Given Lemma B.3,
Var	[| log ∏j(a∣s)-log ∏ (a∣s)∣] ≤ Var	[log π^(a∣s) - log ∏ (a|s)] .	(41)
s,a~ρg(S,a)	s,a~ρg(S,a)
With the transitivity of inequality, combining equation 40 and equation 41, we know that
Var	[| log∏j(a∣s) - log∏(a∣s)∣2] ≤ Var	[log%(a∣s) - log∏(a|s)] .	(42)
s,a~ρG(S,a)	s,a~Pθ(s,a)
B.4 Theorem 4.4
Theorem 4.4	(HTRPO Constraint Function). For the original goal g and an alternative goal g0 , the
constraint between policy π^(a∣s) and policy ∏θ(a|s) is given by:
EE
g0	T ~pθ(τ |g)
2 XX YY π<aklsk,g)) γt(logπθ<at∣st,g0) - log ∏(a⅛∣s⅛, g0))2] ] ≤ J
2 t=o k=ι πθ(ak lsk,g)
(13)
in which 0
1-γ .
Proof. Starting from equation 9, the constraint condition is written as:
E	[log∏θ(a∣s) - log∏θ(a|s)] ≤ E
S,a~ρθG(S,a)
(43)
Given Theorem 4.2,
E	ɔ(log∏θ(a∣s) - log∏θ(a|s))2 ≤ e
S,a~ρθG(S,a)	2
(44)
Multiply the constraint function by a constant 11γ,
1
E
1 - γ S,a~ρθG(S,a)
2(log∏θ(a∣s) - log∏θ(a|s))2
Denote the constraint function as fθθ(θ),
fθ(θ) = τ~- X *t=0Y P(St = S) E	1(logπθ(als) - logπθ(a|s))2
1	- Y s∈s	ι-γ	a~nG(a|S)L2
∞
X Yt	E
t=0	St~Pj(St),at~nd(at|St)
2(log∏θ(at∣St) - log∏Θ(at∣St))2
1∞
E	t(log∏θ(at∣st) - log∏θ(at∣st))2
τ ~pθG(τ)	2 t=0
To write the constraint function in goal-conditioned form,
fθθ(θ) = E E
g τ ~pθG(τ |g)
1∞
5 E Yt(log∏θ(at∣st, g) - log∏θ(at∣st, g))2
2 t=0
(45)
(46)
(47)
E
≤-----
—1 - Y
16
Under review as a conference paper at ICLR 2020
In a similar way with the proof for Theorem 3.1, denote every time step of %(θ) as f石(θ, t), in other
words,
∞
fθ(θ) = X fθ(θ,t)
t=0
for trajectory τ1 = s1, a1, s2, a2, ..., st, at and τ2 = st+1, at+1, ...,
(48)
%(θ,t)= E	E
g τι〜Pd(TI Ig)
E	5γt(log∏d(at∣St,g) - log∏θ(at∣st,g))2
τ2〜Pd(T2 EM L2
For that 1 (log∏j(at∣st, g) - log∏θ(at∖st, g))2 is independent from τ2 conditioned on τ1,
f⅛(θ,t) = E
g
=E
g
E
T1 ^Pd(τιlg)
5 Yt(log ∏θ(at∖st,g) - log ∏θ (at∖st,g))2	E [1]
2	τ τ2〜Pd(T2 Ig)
(49)
E
τι ^Pd(τιlg)
5γt(log∏θ(αt∖st,g) - log∏θ(at∖st,g))2	.
(50)
Accordingly,
∞
%(θ)= X E
t=0 g
E
si：t,ai：t 〜Pd(si：t,ai：t|g)
5Yt(log∏θ(at∖st,g) - log∏θ(at∖st,g))2	.
(51)
Furthermore, by importance sampling, for a new goal g0 , the constraint can be converted to the
following form
f"E LE：τ∣g)[2XPHg))Yt(Iog%(atMgO)-logπθ(atMg0))； S)
in which T = s1, a1, s2,a2,…,st, at. Denote e0 = ι--γ. Based on equation 23, by expanding and
canceling terms, the constraint condition can be written as
EE
g0	τ 〜Pθ(τ ∣g)
2X YY “"；"∖sk,g]γt(log∏θ(at∖st,g0) - log∏θ(at∖st,g0))2]] ≤ 葭
2 t=0 k=1 πθθ(ak∖sk, g)	θ
(13)
17
Under review as a conference paper at ICLR 2020
C Solving Process for HTRPO
C.1 HTRPO Estimators
Based on the final form of HTRPO optimization problem, this section completes the feasibility of
this algorithm with estimators for the objective function and the KL-divergence constraint.
Given a dataset of trajectories and goals D = {τ(i), g(i)}iN=1, each trajectory τ(i) is obtained from
interacting with the environment under a goal g(i) . In order to generate hindsight experience, we
also need to sample a set of alternative goals G = {g0(i) }iN=g1 . The Monte Carlo estimation of
HTRPO optimization problem with dataset D can be derived as follows:
max
θ
N∞ t
1 xxx Y
g0∈G i=1 t=0 k=1
Vθ π mki)lski),gO)
πθ(aki lsk,g0)
AAaki),ski)
g0)
(53)
N ∞ t t	(i) (i) 0
s.t. λ XXX γ2- Y T (IK SYg:、(logπe(aki)lski),g0) - log∏(aki)lski),g0))2 ≤ J
λg0∈G i=ι t=0 L 2 k=ι πθ(ak |sk ,g(i))	」
(54)
in which λ = N ∙ Ng and g0 is supposed to follow uniform distribution. However, in experiments, We
follow the alternative goal sampling method of HPG (Rauber et al., 2019). As a result, the goals of
training data actually follow the distribution of alternative goals instead of uniform distribution, and
the objective and KL expectation will be estimated w.r.t. the alternative goal distribution. Therefore,
during the learning process, our algorithm is encouraging the agent to achieve the alternative goals.
Such mechanism serves as a mutual approach for all hindsight methods (Andrychowicz et al., 2017;
Rauber et al., 2019; Plappert et al., 2018), which can be seen as a merit, for the intention is to guide
the agent to achieve the alternative goals and then generalize to the original goals.
However, as discussed in Rauber et al. (2019), this kind of estimation may result in excessive vari-
ance, which leads to an unstable learning curve. In order to avoid instability, we adopt the technique
of weighted importance sampling introduced in Bishop (2016) and further convert the optimization
problem to the following form:
max
θ
N∞
1 xxx
g0∈G i=1 t=0
N∞
s.t∙1 xxx
g0∈G i=1 t=0
γt
Q	πMaki,lski),g0)
k=ι πβ(aki)lski) ,g(i))
PP Q	πj(aki)ιski),g0)
IMk=I ∏G(aki)∣s"g(i))
Q π0(aki)lski),g0)
k=ι πθKaki)lski),g(i))
2 PP Q πθKaki)lski),g0)
j=ik=i ∏夕(aki) ∣ski),g(i))
Y t vθ∏θ ⅛⅞ki),gO) AQki)
πθ(ak |sk ,g0)
s(ki)
gO)
(log 福(aki)kki),gO)- log π (aki)kki),gO)) 2
(55)
≤ O.
(56)
We provide an explicit solution method for the optimization problem above in Appendix C.2.
While introducing weighted importance sampling may cause a certain level of bias which is identical
to that of HPG (Rauber et al., 2019), the bias is to decrease in inverse ratio with regard to the
increase of the data theoretically (Powell & Swann, 1966). Given the limited resources, we need to
tradeoff between reducing bias and enlarging batch size. By picking a appropriate batch size, the
improvement of weighted importance sampling is well demonstrated in the experiments.
C.2 Solution Method for HTRPO
For the HTRPO optimization problem, briefly denote the optimization problem in expression 55 and
56 as:
max f(θ)
18
Under review as a conference paper at ICLR 2020
s.t. g(θ) ≤ 0.	(57)
T-,	1 ∙	. Zi ∙	. <	♦	1 FIlC ,1	. X	∙	.	.1	. ∙
For any policy parameter θ in the neighborhood of the parameter θ, approximate the optimization
problem with linear objective function and quadratic constraint:
max f(θ)+ Vθf (θ)(θ - θ)
θ
1
s.t. g(θ) + Vθg(θ)(θ	-	θ)	+	2(θ	- θ)TV2g(θ)(θ	- θ)	≤	e0.	(58)
Noticeably, g(θ) = 0 and Vθg(θ) = 0, which further simplifies the optimization problem to the
following form:
max f(θ)+ Vθf (θ)(θ - θ)
θ
s.t. 2(θ - θ)TV2g(θ)(θ - θ) ≤ 葭	(59)
Given a convex optimzation problem with a linear objective function under a quadratic constraint,
many well-practiced approaches can be taken to solve the problem analytically, among which we
adopt the Karush-Kuhn-Tucker(KKT) conditions (Boyd et al., 2004). For a Lagrangian multiplier
λ,
2(θ - θ)T ^ig(θ)(θ - θ) - e0 ≤ 0
λ≥0
λ[1(θ - θ)τV2g(θ)(θ - θ)-e0]=0
-Vθf(θ) + λV2g(θ)(θ - θ) = 0	(60)
Expressions in 60 form the KKT conditions of the optimization problem. Solving the KKT condi-
tions,	__________________________
θ = θ+J--------=----2^L---------h [V2g(θ)]-1Vθ f (θ)	(61)
V Vθf(θ)τ[V2g(θ)]-1Vθf(θ)L 即〃 八,
The policies, however, in this paper are in the form of a neural network, which makes it extremely
time-comsuming to compute the Hessian matrix. Thus, we compute [V2θg(θ)]-1Vθf(θ) with con-
jugate gradient algorithm by solving the following equation:
[v2 g(θ)]X = Vθ f(θ),
in which [Vθ2g(θ)]x can be practically calculated through the following expansion:
[V2g(θ)]χ = Vθ [(Vθ g(θ))τχ].
(62)
(63)
19
Under review as a conference paper at ICLR 2020
D Algorithm
Algorithm 1 Hindsight Trust Region Policy Optimization
Input:
Training batch size batchsize
Max number of training time steps Tmax
Policy θ
Q-function φ
Data Buffer Borigin with its max size equal to batchsize
Output:
Optimized Policy θ*
. Initialization
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
for iteration = 1 to Tmax /batchsize do
while Borigin is not full do
Sample a trajectory τ =
Borigin = Borigin ∩ τ;
end while
Sample alternative goals G
Btrain = 0;
for g0(i) in G do
{(st, at,rt, St+ι,g,∏θ(at∣st,g))}T=ι using current policy θ;
. Collecting data
{g0(i) }iN=g1 from achieved goals in Borigin;
for τ in Borigin do
for t = 0 to T do
Compute ∏(at∣st,g0(i));
Modify reward rt|g → rt|g0(i);
end for
T|g0(i) = {(st, at, rt|g0(i), st+1, g, πθ(at|st, g), πθ(at|st, g0(i)))}T=i；
Btrain = Btrain ∩ τ |g ;
end for
end for	. Generating training data
Use Btrain to optimize policy θ with objective 55 and constraint 56 following Section C.2;
Borigin = 0,
end for
return optimized policy θ*;
20
Under review as a conference paper at ICLR 2020
E Hyperparameters
E.1 Hyperparameters of Discrete Environments
Table 1: Hyperparameters of Discrete Environments
	FliPBit8	FliPBit16	EmPtyMaze
training time steps	5 X 104	5 × 104	5 × 104
batch size	128	256	256
cg damping	1e-2	1e-2	1e-3
reward decay	0.9	0.9	0.95
max KL step	1e-3	1e-3	1e-3
critic optimizer	Adam	Adam	Adam
critic learning rate	5e-4	5e-4	5e-4
critic updates per iteration	10	10	10
sampled goal number	∞	∞	∞
	FourRoom	FetchReach	FetchPush
training time steps	5 × 104	5 × 105	2 × 106
batch size	256	800	800
cg damping	1e-3	1e-3	1e-3
reward decay	0.95	0.98	0.98
max KL step	1e-3	3e-6	3e-6
critic optimizer	Adam	Adam	Adam
critic learning rate	5e-4	1e-4	1e-4
critic updates per iteration	10	10	10
sampled goal number	∞	30	30
E.2 Hyperparameters of Continuous Environments
Table 2: Hyperparameters of Continuous Environments
	FetchReach	FetchPush	FetchSlide
training time stePs	5 × 105	2 × 106	2 × 106
batch size	800	1600	3200
cg damPing	1e-3	1e-3	1e-3
reward decay	0.98	0.98	0.98
max KL steP	1e-5	1e-5	1e-5
entroPy weight	0	1e-4	1e-4
critic oPtimizer	Adam	Adam	Adam
critic learning rate	5e-4	5e-4	5e-4
critic uPdates Per iteration	20	20	20
samPled goal number	100	100	100
21
Under review as a conference paper at ICLR 2020
F Experiments
In this section, we provide a more comprehensive demonstration for the experiments of HTRPO.
In detail, section F.1 narrates a full introduction to each environment; section F.2 gives out the sen-
sitivity analysis of HTRPO including the performance under different network architectures and
different numbers of alternative goals, in which we strictly adopt the control variable method and
only the studied parameter is altered; section F.3 shows the supplementary materials of the experi-
mental data including learning curves and success rates during the training process. We finetune the
hyperparameters according to experience without hyperparameter search due to limited computing
resources.
F.1 Environments
k-Bit Flipping. In each episode of this experiment, two arrays of length k are generated. The first
array is initialized with all 0’s while the second one, usually regarded as the target array, is generated
randomly. At each time step, the agent is able to flip one bit of the first array from 0 to 1 or from 1
to 0. Once the first array is exactly the same with the target array, the agent reaches the goal state
and is then rewarded. The maximum number of time steps is k. In this experiment, we observe the
performance of HTRPO under conditions that k = 8 and that k = 16 respectively. The general
process of an 8-Bit Flipping task is demonstrated in Figure 1 (a).
Grid World. In this experiment, the agent starts at a position of an 11 × 11 grid with intransitable
obstacles, and is trying to reach another randomly chosen position in this grid. The agent is allowed
to move up, down, left and right at each time step. Moving into obstacles makes the agent remain in
its current position. States of this environment is represented by 2-dimensional integral coordinates
and the maximum number of time steps is 32. In Empty Maze environment, there is no impassable
obstacles other than the outer boundary, and the agent starts at the upper-left corner of the grid.
In Four Rooms environment (Sutton et al., 1999), walls separate the grid into 4 rooms, each with
access to its adjacent rooms through single openings. Example cases of Empty Maze and Four
Rooms environments adopted in this paper are demonstrated in Figure 1 (b) and (c).
Fetch. Fetch environment contains a 7-DoF Fetch robotic arm with a two-fingered parallel gripper
in simulation(Plappert et al., 2018). In Fetch Reach environment, a target position is randomly
chosen and the gripper of Fetch robotic arm needs to be moved upon it. In Fetch Push, the task
for the robotic arm is to push a randomly placed block towards the goal state, anther randomly
picked position, which is represented by a 3-dimensional Cartesian coordinate. In Fetch Slide, the
robotic arm needs to exert a force on the block for it to slide towrds a chosen goal at a certain
distance. A pictorial demonstration of this environment is shown in Figure 1 (d), in which the red
dot represents the goal position. For the discrete Fetch environment, detailed settings follow that in
Rauber et al. (2019); for the continuous version, the configurations of legal actions and states follow
that in Plappert et al. (2018). The maximum number of time steps is 50. As in Plappert et al. (2018),
we endure a 5cm target scope centered around the goal position for Fetch Reach and Fetch Push,
and the tolerance is set as 20cm for Fetch Slide as in Andrychowicz et al. (2017).
22
Under review as a conference paper at ICLR 2020
F.2 Sensitivity Analysis
F.2.1 Different Network Architectures
012345
Time steps (1e4)
(a) 8-Bit Flipping 1×16
■ HTRPO M our HPG - OfficialHPG
01	2345
Time steps (1e4)
(b) 8-Bit Flipping 2×64
54321
012345
Time steps (1e4)
(c) 8-Bit Flipping 2×256
nruteR egarevA
012345
Time steps (1e4)
(d) 16-Bit Flipping 1×16
01	2345
Time steps (1e4)
(e) 16-Bit Flipping 2×64
012345
Time steps (1e4)
(f) 16-Bit Flipping 2×256
012345
Time steps (1e4)
(g) Empty Maze 1×16
01	2345
Time steps (1e4)
(h) Empty Maze 2×64
012345
Time steps (1e4)
(i) Empty Maze 2×256
012345
Time steps (1e4)
(j) Four Rooms 1×16
012345
Time steps (1e4)
(k) Four Rooms 2×64
08642
012345
Time steps (1e4)
(l) Four Rooms 2×256
Figure 4:	Evaluation curves with other network structures. Horizontally, 3 figures in each line
illustrate the perfomances in one environment with different network architectures. Vertically, each
column illustrate the performances of one kind of network architecture in different environments.
In this experiment, we observe the performance of HTRPO with different network architectures.
Specially, we implement the proposed algorithm under 3 different network settings, i.e. networks
with a 16-unit layer, two 64-unit layers and two 256-unit layers respectively. For the record, all
parameters and other settings remain the same aside from the network architecture. As demonstrated
in Figure 4, each row shows the performance under different network architecture settings for each
environment. A general conclusion can be drawn that networks with more hidden layers and more
neurons help to speed up the convergence. However, one difference is that for HTRPO, it converges
23
Under review as a conference paper at ICLR 2020
quickly in all the settings, while for HPG, it converges much slower especially when the network
architecture is simple. We believe that the iteratively searching of optimal solution in the trust region
helps the network converge rapidly and is more robust to different network architecture.
F.2.2 Different number of Alternative Goals
In this experiment, how the number of alternative goals, as a key parameter, affects the performance
of HTRPO is studied. We conduct all the experiments, both discrete and continuous with different
number of alternative goals. For discrete environments, we set the number of alternative goals to be
10, 30, 100 and ∞ in turn. For continuous environments, we compare the performance under 10,
30, 100 alternative goals respectively. The evaluation curves are shown in 5. From the results, we
can see that in simple discrete environments, ∞ alternative goals produce the fastest convergence.
In complex and continuous environments, 30 and 100 alternative goals lead to comparatively good
performance. It is not hard to see that Hindishgt TRPO with more alternative goals achieves better
converging speed, which may be credited to the corresponding increase on training samples. This
is, to some extent, similar to data augmentation.
10B 10 alternative goals ■ 30 alternative goals 100 alternative goals ■ ∞ alternative goals
012345
Time steps (1e4)
(a) 8-Bit Flipping
01	2345
Time steps (1e4)
(b) 16-Bit Flipping
012345
Time steps (1e4)
(c) Empty Maze
nruteR egarevA
01	2345
Time steps (1e4)
(d) Four Rooms
012345
Time steps (1e5)
(e) Discrete Fetch Reach
0	4	8	12	16	20
Time steps (1e5)
(f) Discrete Fetch Push
00000
4321
nruteR egarevA
0	4	8	12	16	20
Time steps (1e5)
(h) Countinuous Fetch Push
0	4	8	12	16	20
Time steps (1e5)
(i) Countinuous Fetch Slide
012345
Time steps (1e5)
(g) Countinuous Fetch Reach
Figure 5:	Evaluation curves for different number of alternative goals: 8-Bit Flipping, 16-Bit Flip-
ping, Empty Maze, Four Rooms, Discrete Fetch Reach, Discrete Fetch Push, Continuous Fetch
Reach, Continuous Fetch Push and Contiuous Fetch Slide. The full lines represent the average
evaluation over 10 trails and the shaded regions represent the corresponding standard deviation.
24
Under review as a conference paper at ICLR 2020
F.3 Comprehensive Training Curves
F.3.1 Training Curves
nruteR egarevA
012345
Time steps (1e4)
(a) 8-Bit Flipping
HTRPO our HPG HTRPO With KL1 HTRPO without WIS
TRPO
012345
Time steps (1e4)
(c) Empty Maze
nruteR egarevA
(d) Four Rooms
01	2345
Time steps (1e4)
(b) 16-Bit Flipping
(f) Discrete Fetch Push
012345
Time steps (1e5)
(g) Countinuous Fetch Reach
0	4	8	12	16	20
Time steps (1e5)
(h) Countinuous Fetch Push
0	4	8	12	16	20
Time steps (1e5)
(i) Countinuous Fetch Slide
Figure 6:	Training curves for all environments: 8-Bit Flipping, 16-Bit Flipping, Empty Maze, Four
Rooms, Discrete Fetch Reach, Discrete Fetch Push, Continuous Fetch Reach, Continuous Fetch
Push and Contiuous Fetch Slide. The full lines represent the average evaluation over 10 trails and
the shaded regions represent the corresponding standard deviation.
25
Under review as a conference paper at ICLR 2020
F.3.2 Success Rate
In this section, we demonstrate the success rates of HTRPO during both evaluation and training.
For the record, the actions during the training process are sampled by the distribution output by the
network while during the evaluation process, we adopt a greedy strategy to choose the action by the
mean value of the distribution. Table 3 lists the success rates of Fetch Push and Fetch Slide during
evaluation, in which the ultimate values reflect the mean computed with 1000 test results in each
iteration. They are the only two environments listed for they are the most complex ones. Figure 7
illustrates the success rate curves during the training process.
Table 3: Evaluation success rate for Fetch Push and Fetch Slide
Fetch Push	Fetch Slide
Time step______________________________________________
our HPG (%) HTRPO (%) our HPG (%) HTRPO (%)
480k	56.4	63.2	36.1	46.4
960k	65.6	91.9	59.5	79.9
1920k	87.2	89.7	61.2	82.5
HTRPO our HPG HTRPO With KL1
012345
Time steps (1e4)
(a) 8-Bit Flipping
.0 .8 .6 .4 .2 0
1. 0. 0. 0. 0.
012345
Time steps (1e4)
(b) 16-Bit Flipping
HTRPO without WIS MTRPO
1.0
0.8
0.6
0.4
0.2
0
012345
Time steps (1e4)
(c) Empty Maze
etaR sseccuS
012345
Time steps (1e4)
(d) Four Rooms
012345
Time steps (1e5)
(e) Discrete Fetch Reach
0	4	8	12	16	20
Time steps (1e5)
(f) Discrete Fetch Push
1.0
0.8
0.6
0.4
0.2
0
≡fflH SS e 8ns
01	2345
Time steps (1e5)
(g) Countinuous Fetch Reach
1.0
0.8
0.6
0.4
0.2
0
0	4	8	12	16	20
Time steps (1e5)
(h) Countinuous Fetch Push
1.0
0.8
0.6
0.4
0.2
0
0	4	8	12	16	20
Time steps (1e5)
(i) Countinuous Fetch Slide
Figure 7:	Success Rate for all environments:8-Bit Flipping, 16-Bit Flipping, Empty Maze, Four
Rooms, Discrete Fetch Reach, Discrete Fetch Push, Continuous Fetch Reach, Continuous Fetch
Push and Contiuous Fetch Slide. The full lines represent the average evaluation over 10 trails and
the shaded regions represent the corresponding standard deviation.
26
Under review as a conference paper at ICLR 2020
F.4 Comparison with Dense Reward TRPO
Figure 8 demonstrates the success rate for HTRPO, TRPO with sparse reward and TRPO with
dense reward for Continuous Fetch Reach, Continuous Fetch Push and Contiuous Fetch Slide. The
performance of TRPO with dense rewards is similar to that of TRPO with sparse rewards: for
FetchReach, it converges much more slowly than HTRPO while for FetchPush and FetchSlide, it
doesnt work in the whole training process (2 million time steps). Similar conclusions can also be
found in some other literatures of Hindsight (Plappert et al., 2018). Therefore, it can be concluded
that HTRPO outperforms it significantly.
HTR HTRPO TRPO With Sparse Reward TRPO With Dense Reward
.0 .8 .6 .4 .2
1. 0. 0. 0. 0.
etaR sseccuS
01	2345
Time steps (1e5)
(a) Countinuous Fetch Reach
1.0
0.8
0.6
0.4
0.2
0
0	4	8	12	16	20
Time steps (1e5)
(b) Countinuous Fetch Push
.0 .8 .6 .4 .2
1. 0. 0. 0. 0.
0	4	8	12	16	20
Time steps (1e5)
(c) Countinuous Fetch Slide

Figure 8:	Success Rate for HTRPO, TRPO with Sparse Reward and TRPO with Dense Reward for
Continuous Fetch Reach, Continuous Fetch Push and Contiuous Fetch Slide. The full lines represent
the average evaluation over 10 trails and the shaded regions represent the corresponding standard
deviation.
27
Under review as a conference paper at ICLR 2020
F.5 Comparison OF Different KL Expectation Estimators
Figure 9 demonstrates the estimation results of KL divergence expectation in the training process.
From these data We can see that in the experiments, the approximation of equation 13 can signifi-
cantly reduce the variance of KL expectation estimation. Besides, the comparison of performance
between HTRPO and HTRPO with KL1 also shows the efficiency of this approximation, which
helps improve the final performance significantly. Both “HTRPO” and “HTRPO without WIS” use
the estimation method in equation 13 with one difference being that “HTRPO without WIS” doesn't
adopt weighted importance sampling. Thus, from Figure 9, we can see that “HTRPO" demonstrates
the least variance.
■ HTRPO -HTRPOwithKLl M HTRPO without WIS
(∙① L) ΦΟ⊂Φ^Φ≥Q
01	2345
Time steps (1e5)
(a) Countinuous Fetch Reach
10
8
6
4
2
0
0	4	8	12	16	20
Time steps (1e5)
(b) Countinuous Fetch Push
0	4	8	12	16	20
Time steps (1e5)
(c) Countinuous Fetch Slide
Figure 9:	Estimation of KL Divergence Expectation for different variants of HTRPO in Continuous
Fetch Reach, Continuous Fetch Push and Contiuous Fetch Slide. The full lines represent the average
evaluation over 10 trails and the shaded regions represent the corresponding standard deviation.
The curves for KL1 are comparatively lower than those of equation 13. Note that in TRPO, the
linear search mechanism adjust the updating step size according to the estimation of KL divergence
expectation. it sets a threshold to constrain the KL divergence. For those above the threshold, the
updating step size will be reduced to ensure that the estimation of KL divergence estimation falls
within the threshold. This explains why the curves for KL1 are comparatively lower. However, since
the estimation ofKL divergence expectation in HTRPO falls near the expected value, such step size
adjustment is rarely triggered. This benefits from the much lower variance of equation 13.
28