Under review as a conference paper at ICLR 2020
On Understanding Knowledge Graph
Representation
Anonymous authors
Paper under double-blind review
Ab stract
Many methods have been developed to represent knowledge graph data, which
implicitly exploit low-rank latent structure in the data to encode known information
and enable unknown facts to be inferred. To predict whether a relationship holds
between entities, their embeddings are typically compared in the latent space fol-
lowing a relation-specific mapping. Whilst link prediction has steadily improved,
the latent structure, and hence why such models capture semantic information,
remains unexplained. We build on recent theoretical interpretation of word embed-
dings to derive an explicit structure for the representations of relations between
entities. From this we explain properties and justify the relative performance of
leading knowledge graph representation methods for identifiable relation types,
including their often overlooked ability to make independent predictions.
1	Introduction
Knowledge graphs are large repositories of binary relations between words (or entities) in the form of
fact triples (subject, relation, object). Many models have been developed for learning representations
of entities and relations in knowledge graphs, such that known facts can be recalled and previously
unknown facts can be inferred, a task known as link prediction. Recent link prediction models (e.g.
Bordes et al., 2013; Trouillon et al., 2016; Balazevic et al., 2019b) learn entity representations, or
embeddings, of far lower dimensionality than the number of entities, by capturing latent structure in
the data. Relations are typically represented as a mapping from the embedding of a subject entity to
its related object entity embedding(s). Although the performance of knowledge graphlink prediction
models has steadily improved for nearly a decade, relatively little is understood of the low-rank latent
structure that underpins these models, which we address in this work.
We start by drawing a parallel between entity embeddings in knowledge graphs and unsupervised
word embeddings, as learned by algorithms such as Word2Vec (W2V) (Mikolov et al., 2013) and
GloVe (Pennington et al., 2014). We assume that words have latent features, e.g. meaning(s), tense,
grammatical type, that are innate and fixed, irrespective of what an embedding may capture (which
may be only a part, subject to the embedding method and/or the data source); and that this same latent
structure gives rise to patterns observed in the data, e.g. in word co-occurrence statistics and in which
words are related to which. As such, an understanding of the latent structure from one embedding
task (e.g. word embedding) might be useful to another (e.g. knowledge graph entity embedding).
Recent work (Allen & Hospedales, 2019; Allen et al., 2019) theoretically explains how semantic
properties are encoded in word embeddings that (approximately) factorise a matrix of word co-
occurrence pointwise mutual information (PMI), e.g. as is known for W2V (Levy & Goldberg, 2014).
Semantic relationships between words (specifically similarity, relatedness, paraphrase and analogy)
are proven to manifest as linear relationships between rows of the PMI matrix (subject to known
error terms), of which word embeddings can be considered low-rank projections. This explains why
similar words (e.g. synonyms) have similar embeddings; and embeddings of analogous word pairs
share a common “vector offset”.
Importantly, this insight allows us to identify geometric relationships between such word embeddings
necessary for other semantic relations to hold, such as those of knowledge graphs. These relation
conditions describe relation-specific mappings between entity embeddings, i.e. relation representa-
tions, providing a “blue-print” against which to consider knowledge graph representation models.
We find that various properties of knowledge graph representation models, including the relative
1
Under review as a conference paper at ICLR 2020
Table 1: Score functions of representative linear link prediction models. R ∈ Rde ×de and r ∈ Red are the
relation matrix and translation vector, W ∈ Rde ×dr ×de is the core tensor and bs, bo ∈ R are the entity biases.
Model		Linear SUbcategory	Score FUnction
TransE	(Bordes et al., 2013)	additive	-kes + r - eok22
DistMUlt	(Yang et al., 2015)	mUltiplicative (diagonal)	es>Reo
TUckER	(Balazevic et al., 2019b)	mUltiplicative	W ×1 es ×2 r ×3 eo
MURE	(Balazevic et al., 2019a)	mUltiplicative (diagonal) + additive	-kRes +r-eok22+ bs + bo
performance of leading link prediction models, accord with predictions based on these relation
conditions, suggesting a commonality to the latent structure learned in word embedding models and
knowledge graph representation models, despite the significant differences between their training
data and methodology. In summary, the key contributions of this work are:
•	to use recent understanding of PMI-based word embeddings to derive what a relation represen-
tation must achieve to map a subject word embedding to all related object word embeddings
(relation conditions), based on which relations can be categorised into three types;
•	to show that properties of knowledge graph models fit predictions made from relation conditions,
e.g. strength of a relation’s relatedness aspect is reflected in the eigenvalues of its relation matrix;
•	to show that the performance per relation of leading link prediction models corresponds to
the ability of the model’s architecture to meet the relation conditions of the relation’s type,
i.e. the better the architecture of a knowledge graph representation model aligns with the form
theoretically derived for PMI-based word embeddings, the better the model performs; and
•	noting how ranking metrics can be flawed, to provide novel insight into the prediction accuracy
per relation of recent knowledge graph models, an evaluation metric we recommend in future.
2	Background
Our work draws on knowledge graph representation and word embedding. Whilst related, these
tasks differ materially in their training data. The former is restricted to datasets crafted by hand or
automatically generated, the latter has the vast abundance of natural language text (e.g. Wikipedia).
2.1	Knowledge graph representation
Almost all recent knowledge graph models represent entities es, eo as vectors es, eo ∈ Rde of low
dimension (e.g. de = 200) relative to the number of entities ne (typically of order 104), and relations
as transformations in the latent space from subject entity embedding to object. These models are
distinguished by their score function, which defines (i) the form of the relation transformation, e.g.
matrix multiplication, vector addition; and (ii) how “closeness” between the transformed subject
embedding and an object embedding is evaluated, e.g. dot product, Euclidean distance. Score
functions can be non-linear (e.g. Dettmers et al. (2018)), or linear and sub-categorised as additive,
multiplicative or both. We focus on linear models due to their simplicity and strong performance at
link prediction (including state-of-the-art). Table 1 shows the score functions of competitive linear
models that span all linear sub-categories: TransE (Bordes et al., 2013), DistMult (Yang et al., 2015),
TUckER (Balazevic et al., 2019b) and MURE (Balazevic et al., 2019a).
Additive models typically use Euclidean distance and contain a relation-specific translation from a
(possibly transformed) sUbject to a (possibly transformed) object entity embedding. A generic additive
score fUnction is given by φ(es, r, eo) = -kRses+r-Roeok22+bs+bo = -ke(sr)+r-e(or)k22+bs+bo.
The simplest example is TransE for which Rs = Ro = I and bs = bo = 0. The score fUnction of MURE
has Ro = I and so combines mUltiplicative (Rs = R) and additive (r) components.
Multiplicative models have the generic score fUnction φ(es, r, eo) = es>Reo = he(sr) , eoi, i.e. a
bilinear product of the entity embeddings and a relation-specific matrix R. DistMUlt is a simple
example with diagonal R and so cannot model asymmetric relations (TroUillon et al., 2016). In
TUckER, each relation-specific R = W ×3 r is a linear combination of dr “prototype” relation
matrices in a core tensor W ∈ Rde×dr×de (where ×n denotes tensor prodUct along mode n), facilitating
multi-task learning across relations.
2
Under review as a conference paper at ICLR 2020
2.2	Word embedding
Algorithms such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) generate
succinct low-rank word embeddings that perform well on downstream tasks (Baroni et al., 2014).
Such models predict the context words (cj ) observed around each target word (wi) in a text corpus
using shallow neural networks. Whilst recent language models (e.g. Devlin et al. (2018); Peters et al.
(2018)) create impressive context-specific word embeddings, we focus on the former embeddings
since knowledge graph entities have no obvious context and, more importantly, they are interpretable.
Levy & Goldberg (2014) show that, for a dictionary of unique words D and embedding dimension
d《|D|, W2V's loss function is minimised when its weight matrices W, C ∈Rd×lDl (whose columns
are word embeddings wi , cj) factorise a word co-occurrence pointwise mutual information (PMI)
matrix, subject to a shift term (PMI(wi, Cj)=log PPwwiPjc))). This connects W2V to earlier count-
based embeddings and specifically to PMI, which has a long history in linguistic analysis (Turney &
Pantel, 2010). From its loss function, GloVe can be seen to perform a comparable factorisation.
Recent work shows why word embeddings that factorise such PMI matrix encode semantic word
relationships (Allen & Hospedales, 2019; Allen et al., 2019). The authors show that word embeddings
can be seen as low-rank projections of high dimensional PMI vectors (rows of the PMI matrix),
between which the semantic relationships of similarity, relatedness, paraphrase and analogy provably
manifest as linear geometric relationships (subject to defined error terms), which are then preserved,
under a sufficiently linear projection, between word embeddings. Thus similar words have similar
embeddings, and the embeddings of analogous word pairs share a common vector offset.
Specifically, the PMI vectors (px) of an analogy “man is to king as woman is to queen” satisfy
pQ-pW≈pK-pM because the difference between words associated with king and man (e.g. reign,
crown) mirrors that between queen and woman. This leads to a common difference between their
co-occurrence distributions (over all words), giving a common difference between their PMI vectors,
which projects to a common difference between embeddings. Any discrepancy in the mirroring of
word associations is shown to introduce error, weakening the analogy, as does a lack of statistical
independence within certain word pairs (see (Allen & Hospedales, 2019)). The common difference
in word co-occurrence distributions, e.g. the increased association with words {reign, crown, etc.},
can be interpreted semantically as a common change in context (context-shift) that transforms man to
king and woman to queen by adding a royal context. Under this interpretation, context can also be
subtracted, e.g. “king is to man as queen is to woman” (minus royal); or both, e.g. “boy is to king as
girl is to queen” (minus youth plus royal). Adding context can also be interpreted as specialisation,
and subtracting context as generalisation. This establishes a correspondence between common word
embedding vector offsets and semantic context-shifts.
Although the projection from PMI vectors to word embeddings preserves the relative relationships,
and thus the above semantic interpretability of common embedding differences, a direct interpretation
of dimensions themselves is obscured, not least because any embedding matrix can be arbitrarily
scaled/rotated if the other is inversely transformed.
3	Relationships between embeddings of related words
Our aim is to build on the understanding of PMI-based word embeddings (henceforth word embed-
dings), to identify what a knowledge graph relation representation needs to achieve to map all subject
word embeddings to all related object word embeddings. We note that if a semantic relation between
two words implies a particular geometric relationship between their embeddings, then the latter serves
as a necessary quantitative condition for the former to hold (a relation condition). Relation conditions
implicitly define a relation-specific mapping by which all subject embeddings are mapped to all
related object embedding(s), allowing related entities to be identified by a proximity measure (e.g.
Euclidean distance or dot product). Since this is the approach of many knowledge graph representa-
tion models, their performance can be contrasted with their ability to express mappings that satisfy
required relation conditions. For example, similarity and context-shift relations respectively imply
closeness and a relation-specific vector offset between embeddings (S2.2). Such relation conditions
can be tested for by respectively making no change to the subject entity or adding the relation-specific
offset, before measuring proximity with the object. We note that since relation conditions are not
necessarily sufficient, they do not guarantee a relation holds, i.e. false positives may arise.
3
Under review as a conference paper at ICLR 2020
(d) Context-shift
(e) Gen. context-shift
Figure 1: Relationships between PMI vectors of entities under different relation types. Shaded regions indicate
strong word associations (positive PMI values); red lines indicate relatedness; black lines denote context sets.
In general, the data from which knowledge graph embeddings are derived differs greatly to the
co-occurrence data used for word embeddings, and the latter would not be anticipated to be learned
by knowledge graph models. However, word embeddings provide a known solution (i.e. minimise
the loss function) of any knowledge graph model able to express the required mapping(s) derived
from relation conditions, where the loss function measures proximity between mapped entities.
The relation conditions for certain relation types (underlined) follow readily from S2.2:
•	Similarity: Semantically similar words induce similar distributions over the words they co-occur
with. Thus their PMI vectors (Fig 1a) and Word embeddings are similar.
•	Relatedness: The relatedness of two words can be defined in terms of the words with which both
co-occur similarly (S∈D), which define the nature of relatedness, e.g. milk and cheese are related by
S= {dairy, breakfast, ...}; and |S| reflects the strength of relatedness. Since PMI vector components
corresponding to S are similar (Fig 1b), embeddings of "S-related” words have Similar ComPonentS
in the SUbSPace VS that spans the projected PMI vector dimensions corresponding to S. The rank
of VS might be expected to reflect relatedness strength. In general, relatedness is a weaker, more
variable relation than similarity, its limiting case with S=D and rank(VS) =d.
•	Context-shift: In the context of word embeddings, analogy typically refers to relational similarity
(Turney, 2006; Gladkova et al., 2016). More specifically, the relations within analogies that give a
common VeCtOr OffSet between word embeddings require related words to have a common difference
between their distributions of co-occurring words, defined as a context-shifts (see S2.2). These
relations are strictly 1-to-1 and include an aspect of relatedness due to the word associations in
common (Fig 1d). A specialisation relation is a context-shift in which context is only added (Fig 1c).
•	Generalised context-shift: Context-shift relations are generalised to 1-to-many, many-to-1 and
many-to-many relations by letting the fully-specified added or subtracted context be one of a (relation-
specific) context set (Fig 1e), e.g. allowing an entity to be any colour or anything blue. The potential
scope and size of each context set means these relations can vary greatly. The limiting case for small
context sets has a single context in each, whereby the relation is an explicit context-shift (as above),
and the difference between embeddings is a known VeCtor offset. In the limiting case where context
sets are large, the added/subtracted context is so loosely defined that, in effect, only the relatedness
aspect of the relation and thus only the Common SUbSPaCe ComPonent of embeddings is known.
Link to set theory: Viewing PMI vectors as sets of word associations and taking intuition from
Fig 1, the above relations can be seen to reflect set operations: similarity as set equality; relatedness
as equality of a subset; and context-shift as the set difference equalling a relation-specific set. This
highlights how the relatedness aspect of a relation reflects features that must be common, and context-
shift reflects features that must differ. Whilst this mirrors an intuitive notion of “feature vectors”, we
emphasise that this is grounded in the co-occurrence statistics of PMI-based word embeddings.
3.1	Categorising knowledge graph relations
Analysing the relations of popular knowledge graph datasets with the above perspective, we find
that they comprise (i) a relatedness aspect that reflects a common theme (e.g. both entities are
animals or geographic terms); and (ii) specific word associations of the subject and/or object entities.
Specifically, relations appear to fall under a hierarchy of three relation types: highly related (R);
(generalised) specialisation (S); and (generalised) context-shift (C). As above, “generalised” indicates
that any added/subtracted contexts can be from a set. From Fig 1, type R relations can be seen as a
special case of S, which, in turn, is a special case of C. Type C is therefore a generalised case of all
considered relations. Whilst there are several other ways to classify relations (e.g. by their hierarchy,
4
Under review as a conference paper at ICLR 2020
Table 2: Categorisation of WN18RR relations.
Type	Relation	Examples (subject entity, object entity)
R	verb_group derivationally_related_form also_see	(trim_down_VB_1, cut_VB_35), (hatch_VB_1, incubate_VB_2) (lodge_VB_4, accommodation_NN_4), (question_NN_1, inquire_VB_1) (clean_JJ_1, tidy_JJ_1), (ram_VB_2, screw_VB_3)
S	hypernym instance_hypernym	(land_reform_NN_1, reform_NN_1), (prickle-weed_NN_1, herbaceous_plant_NN_1) (yellowstone_river_NN_1, river_NN_1), (leipzig_NN_1, urban_center_NN_1)
C	member_of_domain_usage member_of_domain_region member_meronym has_part synset_domain_topic_of	(colloquialism_NN_1, figure_VB_5), (plural_form_NN_1, authority_NN_2) (rome_NN_1, gladiator_NN_1), (usa_NN_1, multiple_voting_NN_1) (south_NN_2, sunshine_state_NN_1), (genus_carya_NN_1, pecan_tree_NN_1) (aircraft_NN_1, cabin_NN_3), (morocco_NN_1, atlas_mountains_NN_1) (quark_NN_1, physics_NN_1), (harmonize_VB_3, music_NN_4)
transitivity), by considering relation conditions, we delineate by the required mathematical form (and
complexity) of their representation. Table 2 shows a categorisation of the relations of the WN18RR
dataset (Dettmers et al., 2018), containing 11 relations between 40,943 entities.1 An explanation for
this assignment is in Appx. A and that for NELL-995 (Xiong et al., 2017) is in Appx. B. A review of
the FB15k-237 dataset (Toutanova et al., 2015) shows the vast majority of relations to be of type C
preventing a contrast between relation types being drawn, hence we do not consider that dataset.
3.2	Relations as mappings of embeddings
Given the relation conditions of a particular relation type, we can recognise mappings that meet them
and thus loss functions (that evaluate the proximity of mapped entity embeddings by dot product
or Euclidean distance) able to identify relations of that type between PMI-based word embeddings.
We then contrast these theoretically inspired loss functions (one per relation type) with those of
knowledge graph models (Table 1) and, on the outline assumption that a common low-rank latent
structure is exploited by both word embeddings and knowledge graph models, predict properties and
the relative performance of different knowledge graphs models for different relation types.
R: To evidence S-relatedness, both entity embeddings es , eo must be projected onto a subspace
VS , where their images are compared. Projection requires multiplication by a matrix Pr ∈ Rd×d
and cannot be achieved additively, except in the limiting case of similarity, when Pr = I or vector
r ≈ 0 is added. Comparison by dot product gives (Pres)>(Preo) = es>Pr>Preo = es>Mreo
(for a relation-specific symmetric Mr = Pr>Pr). Euclidean distance gives kPres - Preo k2 =
(es - eo)>Mr(es - eo) = kPr es k2 - 2es> Mr eo + kPr eo k2.
S/C: Evidencing these relations requires a test both for S-relatedness and for relation-entity-specific
embeddings component(s) (vrs, vro). This can be achieved by (i) multiplying both entity embeddings
by a relation-specific projection matrix Pr that projects onto the subspace that spans the low-rank
projection of dimensions corresponding to S, vrs and vro , (which tests for S-relatedness whilst
preserving any entity-specific embedding components); and (ii) adding a relation-specific vector
r = vro - vrs to the transformed subject entity embeddings. Comparison of the final transformed
entity embeddings by dot product equates to (Pres + r)>Preo; and by Euclidean distance to
kPres+r-Preok2= kPres +rk2 -2(Pres +r)>Preo + kPreok2 (cf MuRE: kRes+r-eok2).
Contrasting the above loss functions with those of knowledge graph models (Table 1), we make the
following predictions: (P1) the ability to learn the representation of a particular relation is expected
to reflect the complexity of its type (R>S>C), and whether all relation conditions (e.g. additive or
multiplicative interactions) can be met under a given model; (P2) relation matrices for relatedness
(type R) relations are highly symmetric; (P3) offset vectors for relatedness relations have low norm;
and (P4) as a proxy to the rank of VS, the eigenvalues of a relation matrix reflect a relation’s strength
of relatedness. To elaborate: P1 anticipates that additive-only models (e.g. TransE) are not suited to
identifying the relatedness aspect of relations (except in limiting cases of similarity, requiring a zero
vector); and multiplicative-only models (e.g. DistMult) should perform well on type R but are not
suited to identifying entity-specific features of type S/C, for which an asymmetric relation matrix in
TuckER may help compensate. Further, the loss function of MuRE closely resembles that derived for
type C relations (which generalise all others) and is thus expected to perform best overall.
1We omit the relation “similar_to” since its instances have no discernible structure, and only 3 occur in the
test set, all of which are the inverse of a training example and trivial to predict.
5
Under review as a conference paper at ICLR 2020
Table 3: Hits@10 per relation on WN18RR.
Relation Name	Type	%	#	Khs	Max/Avg Path		TransE	MuREI	DistMult	TuckER	MuRE
verb_group	R	1%	39	0.00	1	1.0	0.87	0.95	0.97	0.97	0.97
derivationally_related_form	R	34%	1074	0.04	1	1.0	0.93	0.96	0.96	0.96	0.97
also_see	R	2%	56	0.24	44	15.2	0.59	0.73	0.67	0.72	0.73
instance_hypernym	S	4%	122	1.00	3	1.0	0.22	0.52	0.47	0.53	0.54
synset_domain_topic_of	C	4%	114	0.99	3	1.1	0.19	0.43	0.42	0.45	0.53
member_of_domain_usage	C	1%	24	1.00	2	1.0	0.42	0.42	0.48	0.38	0.50
member_of_domain_region	C	1%	26	1.00	2	1.0	0.35	0.40	0.40	0.35	0.46
member_meronym	C	8%	253	1.00	10	3.9	0.04	0.38	0.30	0.39	0.39
has_part	C	6%	172	1.00	13	2.2	0.04	0.31	0.28	0.29	0.35
hypernym	S	40%	1251	0.99	18	4.5	0.02	0.20	0.19	0.20	0.28
all		100%	3134				0.38	0.52	0.51	0.53	0.57
Table 4: Hits@10 per relation on NELL-995.											
Relation Name	Type	%	#	Khs	Max/Avg Path		TransE	MuREI	DistMult	TuckER	MuRE
teamplaysagainstteam	R	2%	243	0.11	10	3.5	0.76	0.84	0.90	0.89	0.89
clothingtogowithclothing	R	1%	132	0.17	5	2.6	0.72	0.80	0.88	0.85	0.84
professionistypeofprofession	S	1%	143	0.38	7	2.5	0.37	0.55	0.62	0.65	0.66
animalistypeofanimal	S	1%	103	0.68	9	3.1	0.50	0.56	0.64	0.68	0.65
athleteplayssport	C	1%	113	1.00	1	1.0	0.54	0.58	0.58	0.60	0.64
chemicalistypeofchemical	S	1%	115	0.53	6	3.0	0.23	0.43	0.49	0.51	0.60
itemfoundinroom	C	2%	162	1.00	1	1.0	0.39	0.57	0.53	0.56	0.59
agentcollaborateswithagent	R	1%	119	0.51	14	4.7	0.44	0.58	0.64	0.61	0.58
bodypartcontainsbodypart	C	1%	103	0.60	7	3.2	0.30	0.38	0.54	0.58	0.55
atdate	C	10%	967	0.99	4	1.1	0.41	0.50	0.48	0.48	0.52
locationlocatedwithinlocation	C	2%	157	1.00	6	1.9	0.35	0.37	0.46	0.48	0.48
atlocation	C	1%	294	0.99	6	1.4	0.22	0.33	0.39	0.43	0.44
all		100%	20000				0.36	0.48	0.51	0.52	0.52
4 Comparing knowledge graph models
We test the predictions made on the basis of word embeddings by comparing the performance of
competitive knowledge graph models, TransE, DistMult, TuckER and MuRE (see S2), which entail
different forms of relation representation, on all WN18RR relations and a similar number of NELL-
995 relations (selected to represent each relation type). Since applying the logistic sigmoid to the
score function of TransE does not give a probabilistic interpretation comparable to other models,
we include MuREI, a constrained variant of MuRE with Rs = Ro = I, as a proxy to TransE for a
fairer comparison. Implementation details are included in Appx. D. For evaluation, we generate 2ne
evaluation triples for each test triple (for the number of entities ne) by fixing the subject entity es
and relation r and replacing the object entity eo with all possible entities and then keeping eo and r
fixed and varying es. The obtained scores are ranked to give the standard metric hits@10 (Bordes
et al., 2013), i.e. the fraction of times a true triple appears in the top 10 ranked evaluation triples.
4.1	Performance per relation type
Tables 3 and 4 report results (hits@10) for each relation and include the relation type and known
confounding influences: percentage of relation instances in the training and test sets (approximately
equal), number of instances in the test set, Krackhardt hierarchy score (see Appx. E) (Krackhardt,
2014; Balazevic et al., 2019a) and maximum and average shortest path between any two related
nodes. A further confounding effect is dependence between relations. Balazevic et al. (2019b) and
Lacroix et al. (2018) show that constraining the rank of relation representations benefits datasets
with many relations (particularly when the number of instances per relation is low) due to multi-task
learning, which is expected to benefit TuckER on the NELL-995 dataset (200 relations). Note that all
models have a comparable number of free parameters.
P1: As predicted, all models tend to perform best at type R relations, with a clear performance gap
to other relation types. Also, performance on type S relations appears higher in general than type
C. Additive-only models (TransE, MuREI) perform most poorly on average, in line with prediction
since all relation types involve a relatedness component. They achieve their best results on type R
relations, where the relation vector can be zero/small. Multiplicative-only DistMult performs well,
sometimes best, on type R relations, fitting expectation as it can fully represent those relations and
has no additional parameters that may overfit to noise (which may explain where MuRE performs
6
Under review as a conference paper at ICLR 2020
slightly worse). As expected, MuRE, performs best on average (particularly on WN18RR), and most
strongly on S and C type relations that require both multiplicative and additive components. The
comparable performance of TuckER on NELL-995 is explained by its ability for multi-task learning.
Other unexpected results also closely align with confounding factors, e.g. that all models perform
poorly on the hypernym relation, despite it having type S and a relative abundance of training data (40%
of all instances), might be explained by its hierarchical nature (Khs ≈ 1 and long paths). The same may
explain the reduced performance on type R relations also_see and agentcollaborateswithagent. As
found previously, none of the models considered are well suited to modelling hierarchical structures
(Balazevic et al., 2019a). We also note that the percentage of training instances of a relation does not
seem to correlate with its performance, as might typically be expected.
P2/P3: Table 5 shows the symmetry score (∈ [-1, 1] indicating perfect anti-symmetry to symmetry;
see Appx. F) for the relation matrix of TuckER and the norm of relation vectors of TransE, MuREI
and MuRE on the WN18RR dataset. As expected, type R relations have high symmetry, whereas both
other relation types have lower scores, fitting the expectation that TuckER compensates for having no
additive component. All additive models learn relation vectors of a noticeably lower norm for type R
relations (where, in the extreme, no additive component is required) than for types S and C.
Table 5: Relation matrix symmetry score [-1.1] for TuckER; and
relation vector norm for TransE, MuREI and MuRE (WN18RR).
Relation	Type	Symmetry Score TUCkER	Vector Norm		
			TransE	MUREI	MURE
verb_group	R	0.52	5.65	0.76	0.89
derivationally_related_form	R	0.54	2.98	0.45	0.69
also_see	R	0.50	7.20	0.97	0.97
instance_hypernym	S	0.13	18.26	2.98	1.88
member_of_domain_usage	C	0.10	11.24	3.18	1.88
member_of_domain_region	C	0.06	12.52	3.07	2.11
synset_domain_topic_of	C	0.12	23.29	2.65	1.52
member_meronym	C	0.12	4.97	1.91	1.97
has_part	C	0.13	6.44	1.69	1.25
hypernym	S	0.04	9.64	1.53	1.03
Figure 2: Eigenvalue magnitudes of
relation-specific matrices R for MuRE
by relation type (WN18RR).
P4: Fig 2 shows eigenvalue magnitudes (scaled relative to their largest and ordered) of the relation-
specific matrices R of MuRE, labelled by relation type. Predicted to reflect strength of a relation’s
relatedness, they should be highest for type R relations, as observed. For relation types S and C
the profiles are more varied, fitting the expectation that the relatedness of those types has greater
variability in both choice and size of S, i.e. in the nature and strength of relatedness.
In summary, the results support all predictions made based on the assumption that knowledge graph
models benefit from the same latent semantic structure as word embeddings and the relation conditions
theoretically derived from them. Our analysis identifies the best performing model per relation type:
multiplicative-only DistMult for type R, additive-multiplicative MuRE for types S/C; providing a
basis for dataset-dependent model selection. The per-relation insight into where models perform
poorly, e.g. on hierarchical or type C relations, can also be used to aid and direct future model design.
4.2	Knowledge graph model predictions
Even though in practice we want to know whether a particular triple is true or false, such independent
predictions are not commonly reported or evaluated. Despite many recent link prediction models
being able to independently predict the truth of each triple, it is common practice to report ranking-
based metrics, e.g. mean reciprocal rank, hits@k, which compare the prediction of a test triple to
those of all evaluation triples (see S4). Not only is this computationally costly, the evaluation is flawed
if entities are related to more than k others and does not evaluate a model’s ability to independently
predict whether “a is related to b”. We address this by considering actual model predictions.
Since for each relation there are ne2 possible entity-entity relationships, we sub-sample by computing
predictions for all (es , r, eo) triples only for each es , r pair seen in the test set. We split positive
predictions (σ(φ(es, r, eo)) > 0.5) between (i) training and test/validation instances (known truths);
and (ii) other, the truth of which is not known. Per relation, we then compute accuracy over the
training instances (train) and the test/validation instances (test); and the average number of other
truths predicted per es , r pair. Table 6 shows results for MuREI, DistMult, TuckER and MuRE. All
7
Under review as a conference paper at ICLR 2020
Table 6: Per relation prediction accuracy for MuREI (MI), (D)istMult, (T)uckER and (M)uRE (WN18RR).
Relation Name	#train	#test	Accuracy (train)				Accuracy (test)				# Other “True”			
			MI	D	T	M	MI	D	T	M	MI	D	T	M
verb_group	15	39	1.00	1.00	1.00	1.00	0.97	0.97	0.97	0.97	8.3	1.7	0.9	2.7
derivationally_related_form	1714	1127	1.00	1.00	1.00	1.00	0.96	0.94	0.95	0.95	8.8	0.5	0.6	1.7
also_see	95	61	1.00	1.00	1.00	1.00	0.64	0.64	0.61	0.59	7.9	1.6	0.9	1.9
instance_hypernym	52	122	1.00	1.00	1.00	1.00	0.32	0.32	0.23	0.43	1.3	0.4	0.3	0.9
member_of_domain_usage	545	43	0.98	1.00	1.00	1.00	0.02	0.00	0.02	0.00	1.5	0.6	0.0	0.3
member_of_domain_region	543	42	0.88	0.89	1.00	0.93	0.02	0.02	0.00	0.02	1.0	0.4	0.8	0.7
synset_domain_topic_of	13	115	1.00	1.00	1.00	1.00	0.42	0.10	0.14	0.47	0.7	0.6	0.1	0.2
member_meronym	1402	307	1.00	1.00	1.00	1.00	0.22	0.02	0.01	0.22	7.9	3.4	1.5	5.6
has_part	848	196	1.00	1.00	1.00	1.00	0.24	0.05	0.09	0.22	7.1	2.4	1.3	3.9
hypernym	57	1254	1.00	1.00	1.00	1.00	0.15	0.02	0.02	0.22	3.7	1.2	0.0	1.7
all	5284	3306	0.99	0.99	1.00	0.99	0.47	0.37	0.37	0.50	5.9	1.2	0.5	2.1
(a) derivationally_related_form (R)
(b) instance_hypernym (S)
(c) synset_domain_topic_of (C)
Figure 3: Histograms of MuRE predictions for an example WN18RR relation of each type, split into true
training, true test/validation and other instances.
models achieve almost perfect training accuracy. The additive-multiplicative MuRE gives best test
set performance, followed (surprisingly) closely by MuREI, with multiplicative models (DistMult
and TuckER) performing poorly on all but type R relations. Analysing a sample of “other” positive
predictions for a relation of each type (see Appx. G), we estimate that TuckER is relatively accurate
but pessimistic (〜0.3 correct of the 0.5 predictions ≈ 60%), MUREI is optimistic but inaccurate
(〜2.3 of 7.5≈ 31%), whereas MuRE is both optimistic and accurate (〜1.1of 1.5 ≈ 73%).
Fig 3 shows histograms of MuRE prediction probabilities for the same sample relations, split by
known truths (training and test/validation) and other instances. There is a clear distinction between
relation types: for type R, most train and test triples are classified correctly with high confidence; for
types S and C, an increasing majority of incorrect test predictions are far below the decision boundary,
i.e. the model is confidently incorrect. For relation types where the model is less accurate, fewer
positive predictions are made overall and the prediction distribution is more peaked towards zero.
This analysis probes further into the difficulty models have representing type S/C relations. Given the
additional insight provided and the benefit of standalone predictions, we recommend the inclusion of
predictive performance in future link prediction work.
5 Conclusion
Many models learn low-rank representations for knowledge graph link prediction, yet little is known
about the latent structure they learn. We build on recent understanding of PMI-based word embeddings
to theoretically establish what a relation representation must achieve to map a word embedding to
those it is related to for the relations of knowledge graphs (relation conditions). Such conditions
partition relations into three types and also provide a framework to assess loss functions of knowledge
graph models. Any model that satisfies a relation’s conditions can represent it if its entity embeddings
are set to PMI-based word embeddings, i.e. a solution is known to exist. Whilst knowledge graph
models do not learn the parameters of word embeddings, we show that the better a model’s architecture
satisfies a relation’s conditions, the better its link prediction performance, fitting the premise that
similar latent structure is exploited. Overall, we extend previous understanding of how semantic
relations are encoded in relationships between PMI-based word embeddings - generalising from a
limited set, e.g. similarity and analogy; we demonstrate commonality between the latent structure
learned by PMI-based word embeddings (e.g. W2V) and knowledge graph representation models; and
we provide novel insight into knowledge graph models by evaluating their predictive performance.
8
Under review as a conference paper at ICLR 2020
References
Carl Allen and Timothy Hospedales. Analogies Explained: Towards Understanding Word Embed-
dings. In International Conference on Machine Learning, 2019.
Carl Allen, Ivana Balazevic, and Timothy HosPedales. What the Vec? Towards Probabilistically
Grounded Embeddings. In Advances in Neural Information Processing Systems, 2019.
Ivana Balazevic, Carl Allen, and Timothy M Hospedales. Multi-relational PoinCare Graph Embed-
dings. In Advances in Neural Information Processing Systems, 2019a.
Ivana Balazevic, Carl Allen, and Timothy M Hospedales. TuckER: Tensor Factorization for Knowl-
edge Graph Completion. In Empirical Methods in Natural Language Processing, 2019b.
Marco Baroni, Georgiana Dinu, and Germdn Kruszewski. Don't Count, Predict! A Systematic
Comparison of Context-Counting vs. Context-Predicting Semantic Vectors. In Association for
Computational Linguistics, 2014.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information
Processing Systems, 2013.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M
Mitchell. Toward an Architecture for Never-ending Language Learning. In Association for the
Advancement of Artificial Intelligence, 2010.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D
Knowledge Graph Embeddings. In Association for the Advancement of Artificial Intelligence,
2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding. In North American Chapter of the
Association for Computational Linguistics, 2018.
Anna Gladkova, Aleksandr Drozd, and Satoshi Matsuoka. Analogy-based Detection of Morphological
and Semantic Relations With Word Embeddings: What Works and What Doesn’t. In NAACL
Student Research Workshop, 2016.
Lawrence J Hubert and Frank B Baker. Evaluating the Symmetry of a Proximity Matrix. Quality &
Quantity, 13(1):77-84, 1979.
Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International
Conference on Learning Representations, 2015.
David Krackhardt. Graph Theoretical Dimensions of Informal Organizations. In Computational
Organization Theory. Psychology Press, 2014.
Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical Tensor Decomposition for
Knowledge Base Completion. In International Conference on Machine Learning, 2018.
Omer Levy and Yoav Goldberg. Neural Word embedding as Implicit Matrix Factorization. In
Advances in Neural Information Processing Systems, 2014.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed Representations
of Words and Phrases and their Compositionality. In Advances in Neural Information Processing
Systems, 2013.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word
Representation. In Empirical Methods in Natural Language Processing, 2014.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep Contextualized Word Representations. In North American Chapter of the
Association for Computational Linguistics, 2018.
9
Under review as a conference paper at ICLR 2020
Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael
Gamon. Representing Text for Joint Embedding of Text and Knowledge Bases. In Empirical
Methods in Natural Language Processing, 2015.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex
Embeddings for Simple Link Prediction. In International Conference on Machine Learning, 2016.
Peter D Turney. Similarity of Semantic Relations. Computational Linguistics, 32(3):379-416, 2006.
Peter D Turney and Patrick Pantel. From Frequency to Meaning: Vector Space Models of Semantics.
Journal of Artificial Intelligence Research, 37:141-188, 2010.
Wenhan Xiong, Thien Hoang, and William Yang Wang. DeepPath: A Reinforcement Learning
Method for Knowledge Graph Reasoning. In Empirical Methods in Natural Language Processing,
2017.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding Entities and
Relations for Learning and Inference in Knowledge Bases. In International Conference on Learning
Representations, 2015.
10
Under review as a conference paper at ICLR 2020
A Categorising WordNet relations
Table 7 describes how each WN18RR relation was assigned to its respective category.
Table 7: Explanation for the WN18RR relation category assignment.
Type	Relation	Relatedness	Subject Specifics	Object Specifics
R	verb_group derivationally_related_form also_see	both verbs; similar in meaning different syntactic categories; semantically related semantically similar	- - -	- - -
S	hypernym instance_hypernym	semantically similar semantically similar	instance of the object instance of the object	- -
C	member_of_domain_usage member_of_domain_region member_meronym has_part synset_domain_topic_of	loosely semantically related (word usage features) loosely semantically related (regional features) semantically related semantically related semantically related	usage descriptor region descriptor collection of objects collection of objects broad feature set	broad feature set broad feature set part of the subject part of the subject domain descriptor
B Analysing NELL-995 relations
NELL-995: We categorise a random subsample of 12 relations from the NELL-995 dataset (Xiong
et al., 2017) containing 75,492 entities and 200 relations (a subset of NELL (Carlson et al., 2010)),
which span our identified relation types (see Table 8). Explanation for the relation category assignment
is shown in Table 9.
Table 8: Categorisation of NELL-995 relations.
Type	Relation	Examples (subject entity, object entity)
R	teamplaysagainstteam clothingtogowithclothing agentcollaborateswithagent	(sportsteam_rangers, sportsteam_mariners), (sportsteam_phillies, sportsteam_tampa_bay_rays) (clothing_shirts, clothing_trousers), (clothing_shoes, clothing_black_shirt) (musicartist_white_stripes, musicartist_jack_white), (politician_obama, politician_hillary_clinton)
S	professionistypeofprofession animalistypeofanimal chemicalistypeofchemical	(profession_trial_lawyers, profession_attorneys), (profession_engineers, profession_experts) (mammal_cats, mammal_small_animals), (bird_chickens, agriculturalproduct_livestock) (chemical_moisture, chemical_gas), (chemical_oxide, chemical_materials)
C	athleteplayssport itemfoundinroom bodypartcontainsbodypart atdate locationlocatedwithinlocation atlocation	(athlete_joe_smith, sport_baseball), (athlete_chris_cooley, sport_football) (bedroomitem_bed, room_den), (hallwayitem_refrigerator, visualizablething_kitchen_area) (bodypart_system002, braintissue_eyes), (bodypart_blood, bodypart_left_ventricle) (currency_scotland, date_n2009), (governmentorganization_wto, date_n2003) (city_medellin, country_colombia), (city_jackson, stateorprovince_wyoming) (city_ogunquin, stateorprovince_maine), (city_palmer_lake, stateorprovince_colorado)
Table 9: Explanation for the NELL-995 relation category assignment.
Type	Relation	Relatedness	Subject Specifics	Object Specifics
R	teamplaysagainstteam clothingtogowithclothing agentcollaborateswithagent	both sport teams both items of clothing that go together both people or companies; related industries	- - -	- - -
S	professionistypeofprofession animalistypeofanimal chemicalistypeofchemical	semantically related (both profession types) semantically related (both animals) semantically related (both chemicals)	instance of the object instance of the object instance of the object	- - -
C	athleteplayssport itemfoundinroom bodypartcontainsbodypart atdate locationlocatedwithinlocation atlocation	semantically related (sports features) semantically related (room/furniture features) emantically related (specific body part features) loosely semantically related (start date features) semantically related (geographical features) semantically related (geographical features)	athlete descriptor item descriptor collection of objects broad feature set part of the subject part of the subject	sport descriptor room descriptor part of the subject date descriptor collection of objects collection of objects
C S plitting the NELL-995 dataset
The test set of NELL-995 created by Xiong et al. (2017) contains only 10 out of 200 relations present
in the training set. To ensure a fair representation of all training set relations in the validation and test
sets, we create new validation and test set splits by combining the initial validation and test sets with
the training set and randomly selecting 20,000 triples each from the combined dataset.
11
Under review as a conference paper at ICLR 2020
D Implementation details
All algorithms are re-implemented in PyTorch with the Adam optimizer (Kingma & Ba, 2015) that
minimises binary cross-entropy loss, using hyper-parameters that work well for all models (learning
rate: 0.001, batch size: 128, number of negative samples: 50). Entity and relation embedding
dimensionality is set to de = d = 200 for all models except TuckER, for which dr = 30 (Balazevic
et al., 2019b).
E Krackhardt hierarchy score
The Krackhardt hierarchy score measures the proportion of node pairs (x, y) where there exists a
directed path x → y, but not y → x; and it takes a value of one for all directed acyclic graphs, and
zero for cycles and cliques (Krackhardt, 2014; Balazevic et al., 2019a).
Let M ∈ Rn×n be the binary reachability matrix of a directed graph G with n nodes, with Mi,j = 1
if there exists a directed path from node i to node j and 0 otherwise. The Krackhardt hierarchy score
of G is defined as:
Pi=1 Pj = 1 ɪ(Mij == 1 ∧ Mj,i == O)
Pn=I Pn=I I(Mij==1)
(1)
F Symmetry score
The symmetry score∈ [-1, 1] (Hubert & Baker, 1979) for a relation matrix R∈Rde×de is defined as:
PP RR- (PPi=j R )2
=乙 乙i=j RijRji	de(de-1)
PP	R2 _ (P Pi=j Ri )2
乙乙i=j Rij -	de(de-1)
where 1 indicates a symmetric and -1 an anti-symmetric matrix.
(2)
G	“Other” predicted facts
Tables 10 to 13 shows a sample of the unknown triples (i.e. those formed using the WN18RR
entities and relations, but not present in the dataset) for the derivationally_related form (R),
instance_hypernym (S) and synset_domain_topic_of (C) relations at a range of probabilities
(σ(φ(es, r, eo)) ≈ {0.4, 0.6, 0.8, 1}), as predicted by each model. True triples are indicated in
bold; instances where a model predicts an entity is related to itself are indicated in blue.
12
UnderreVieW as a ConferenCe PaPersICLR 2020
Table 10: "Other" facts as predicted by MuRE/.
Relation (Type)	σ(≠(es,r,eo)) ≈ 0.4	σ(φ(es,r, eo)) ≈ 0.6	σ(≠(es,r, eo)) ≈ 0.8	σ(≠(es,r,eo)) ≈ 1
derivationally_ related_form (R)	(equalizerJSIN_2, set_off_VB_5) (constellation JSIN_2, satellite_NN_3) (shrink_VB_3, subtraction_NN_2) (continue_VB_l0, proceed_VB_l) (SUPPort_VB_6, defend_VB_5) (shutter_NN_l, fill_up_VB_3) (yawning_NN_l, patellar_reflex_NN_l) (yaw_NN_l, spiral_VB_l) (stratum_NN_2, social_group_NN_l) (duel_VB_l, scuffle_NN_3)	(extrapolation	_1, maths_NN_l) (spread_VB_5, circularize_VB_3) (flauntJSIN_1, showing JSIN_2) (extrapolate_VB_3„ synthesis_NN_3) (strategist	machination J^N_1) (CrUSh_VB_4, grind_VB_2) (spike_VB_5, steady_VB_2) (licking_NN_l, vanquish_VB_l) (synthetical_JJ_l, synthes izer_NN_2) (realization J2, embodiment J^N_3)	(sewerJSIN_2, stitcherJSIN_1) (lard_VB_l, vegetable) (snuggle_NN_1, draw_close_VB_3) (train_VB_3, training_NN_1) (scratch_VB_3, skin_sensation_NN_l) (scheme_NN_5, Schematization	_1) (ordain_VB_3, vest_VB_l) (lie_VB_l, front_endJ^N_1) (tread_NN_l, step_NN_9) (register_NN_3, file_away_VB_l)	(trail^B_2, trail^B_2) (worship_VB_1, worship_VB_1) (steer_VB_l, steer_VB_l) (sort_out_yB_l, sort_out_yB_l) (makeJull_yB_l, makeJull_yB_l) (utilize_VB_l, utilize_VB_l) (geology_NN_l, geology JSIN_1) (zoology JSIN_2, zoology JSIN_2) (uranology_NN_l, uranology_NN_l) (travel_VB_l, travel_VB_l)
instance_ hypernym (S)-	(thomas_aquinas_NN_l, martyr_NN_2) (volcano_islands_NN_l, volcano_NN_2) (cape_horn_NN_l, urban_center_NN_l) (bergen_NN_l, national_capital_NN_l) (marshall_NN_2, generalship J^N_1) (nansen_NN_l, ve nturer_NN_2) (wisconsin_NN_2, state_capital_NN_l) (prussia_NN_l, stockade_NN_2) (de_mille_NN_l, dancing-master_NN_l) (aegean_sea_NN_l, aegean_island_NN_l)	(taiwan_NN_l, asian_nation_NN_l) (st._gregory_of_n._NN_l, canonization J∖INJI) (st._gregory_of_n._NN_l, saint_VB_2) (mccormick_NN_l, ftnd_VB_8) (St._gregory」JsINbishop JSlN_1) (richardJ}uckminsterJ^.J^N_l, technologicalJIJJ2) (thomas_aquinas_NN_l, archbishop_NN_l) (marshallNN2,general_officer_NN_l) (newman_NN_2, primateship_NN_l) (thomas_the_apostle_NN_l, sanctify_VB_1)	(prophets JSlN_1, gospels JSlN_1) (malcolm_x_NN_l, passive_resister_NN_l) (taiwan_NN_l, national_capitalJ>^N_1) (truth_NN_5, abolitionismJSIN_1) (thomas_aquinas_NN_l, saint_VB_2) (central_america_NN_l, s._am._nation_NN_l) (de_mille_NN_l, dance_VB_l) (st._gregory_i_NN_l, apostle_NN_3) (fertile_crescent_NN_l, asian_nation_NN_l) (robert_owen_NN_l, industry_NN_1)	(hels inki_NN_l, urban_center_NN_l) (mannheim_NN_l, stockade_NN_2) (nippon_NN_l, nippon_NN_l) (victor hugo NN 1, novel NN 1) (regiomontanus_NN_1, uranology_NN_l) (prophets_NN_1, book_NN_10) (thomas_aquinas_NN_l, churchJ^ather_NN_l) (woody_guthrie_1, minstrel_VB_l) (central america NN 1, c. am. ration NN 1) (aegean_sea_NN_l, island_NN_l)
synset_domain_ topic_of (O	(write_VB_8, tape_VB_3) (introvert	_1, scientific_disciplineJ^N_l) (libel_NN_l, slur_NN_2) (etymologizing_NN_l, law_NN_l) (temple_NN_4, place_of_worship_NN_l) (trial_impression_NN_l, proof_VB_l) (friend_of_the_courtJ^N_l, war_machine_NN_l) (multiv._analys is_NN_l, applied_math_NN_l) (sell_VB_l, transaction_NN_l) (draw_VB_6, represent_VB_9)	(draw_VB_6, creative_j)erson_NN_l) (suborder	taxonomic_group_NN_l) (draw_VB_6, draw_VB_6) (first_sacker JSlN_1, b allgame _NN_2) (alchemizemodify_VB_3) (sermon_NN_l, sermon_NN_l) (saint_VB_2, catholic_church_NN_l) (male_JJ_l, masculine JIJJ2) (fireSB^3, zoology_NN_2) (sell_VB_l, sell_VB_l)	(libel_NN_l, sully_VB_3) (relationship_NN_4, relationship_NN_4) (etymologizing_NN_l, Iingu istics_NN_l) (turn_VB_12, cultivation_NN_2) (brynhild_NN_l, mythologize_VB_2) (bry nh ild_NN_l, myth_NN_l) (ass ist_NN_2, am. _football_game_NN_1) (mitzvah_NN_2, human_acIivity_NN_1) (drive_NN_12, drive_VB_8) (relationship JSlN_4, biology JSlN_1)	(libel_NN_l, disparagement J∖IN_1) (roll-on_roIl-offJSINJL, transport JSlN_1) (prance_VB_4, equestrian_sport_NN_l) QibeiJSINtraducement_NN_l) (sellJVBj, selling_NN_lJ (trot_VB_2, rideJiorseback_VB_l) (prance_VB_4, rideJiorseback_VB_l) (galloprideJiorseback_VB_l) (brynhild_NN_l, mythology_NN_2) (drive_NN_12, badmintonJMN_1)

UnderreVieW as a ConferenCe PaPersICLR 2020
Table 11: "Other” facts as predicted by DistMult.
Relation (Type)	σ(φ(es,r, eo)) ≈ 0.4	σ(φ(es,r, eo)) ≈ 0.6	σ(φ(es,r,eo)) ≈ 0.8	σ(φ(es,r, eo)) ≈ 1
derivationally_ related_form (R)	(Stag_VB_3, undercover_work_1) (printJVB ji, publisher	) (crier_NN_3, PitChman	) (playJVB~26, tum_NN_10) (ColmtjV'B/, reciteJVB) (vividness_NN_2, imbue (sea^new_NN_l, IarusJVN_1) (alkali_NN_2, acidify_VBJ2) (see_VB_I7, understandJVB) (ShUnhedging_NN_2)	(dishJ^N_2, Ste心NN) (exposeJVB_3, show_NN_l) (system_NN_9, orderlinessJSIN_1) (spread_NN_4f strewJVB_1) (take_down_VB_2, put_VB_2) (wrestleJVB_4, wrestler JSllSl_1) (antotF._OFganiSm不N_1, epiphytic_JJJL) (duelJVBJ, slugfest_NN_1) (vocal_NN_2, rock_starJVN_l) (smelling_NN_lf scent	(shrink_NN_l, pedology JVN_1) (finish_VB _6, finishing_NN_2) (play JVB J26, playing_NN_3) (centralization_lf unite_VB_6) (existence_NN_l, livingJ^lN_3) (mouth_VB_3, sassing_NN_I) (constellation_NN_2, starJVN_1) (Print_VB_4, publishing_house_NN_l) (puzzleJVBJ2, secret_NN_3) (uranologyJVN_1,	(alliterateJVB_1, versifierJSllSl_1) (geolOgyJ^N_l, structural_JJ_5) (resectamputation	) (nutrition_NN_3, man_NN^) (saint JSllSl_3„ sanctify JVB (right_^elder_NN_l, leftfield_NN_l) (HSLVB_4, slope_NN_2) (lieutenancy_NN_1, captain_NN_I) (tread_1„ step_VB_7) (exenteration	_lf enucleate_VB_2)
instance_ hypernym (S)	(Wisconsin JSINJ2, urban_centerJSIN_1) (marshallJ2f lieutenant_general_NN_1) (abidjan_NN_I, cote_d'ivoire_NN_l) (worldjwar_i_NN_I, urban_center_NN_I) (st.j)aul_NN_2, evangelist_NN_2) (deep_south_NN_l, urban_center_NN_I) (nuptse_NN_I, urban_center_NN_I) (ticino_NN_I, urban_center_NN_I) (aegean_seaJVN_l, aegean_islandJVN_l) (cowpens_NN_I, war_of_am.^nd._NN_I)	(mississippi_river_NN_I, american_stateJSIN_l) (r._e._byrd_NN_I, commissioned_officerJSIN_l) (kobenhavn_NN_1, urban_center_NN_l) (the_gambia_NN_l, africa_NN_I) (tirich_mir_NN_I, urban_center_NN_I) (r._erd小N_1, militaiy_advisor_NN_l) (r._e._byrd_NN_I, aide-de-campJ^N_1) (tampa_bay_NN_I, urban_center_NN_I) (tidewater_regionJ^N_l, south_NN_I) (r. _e.^yrd_NN_l, exec utiv e_officer_NN_l)	(deep_south_NN_l, southJVN_l) (CaPital_of_gambia_NN_1, urban_center_NN_l) (southjwest_africa_NN_I, africa_NN_I) (brandenburg_NN_l, urban_center_NN_I) (sierra_nevada_NN_l, urban_center_NN_I) (malcolmu_NN_1, emancipationist不N_1) (north_platte_riverJVN_l, urban_center_NN_I) (oslo_NN_lf urban_center_NN_l) (zaire_river_NN_I, urban_center_NN_I) (transylvanian_alps_NN_I, urban_center_NN_I)	(helsinki_NN_l, urban_center_NN_l) (the_nazarene_NN_l, save_VB_7) (irish_capital_NN_l, urban_center_NN_l) (r. _erd小N_1, insp ector_gen eral_NN_l) (r._e._byrdJ^N_l, chief_of_staff_NN_l) (central_americaJSIN_l, c._am._nation_NN_I) (malcolm^)c_NN_I, environmentalistJSIN_l) (th e_nazaren e_NN_l› Christian (thomas_aquinas_NN_I, church^fatherJVN_l) (the_nazarene_NN_l, el_nino_NN_2)
synset_domain_ topic_of (C)	(limitationJSINtrammel_VB_2) (light_colonel_NN_l, colonel_NN_l) (nurse_VB_l, nursing_NN_l) (sermonJSIN_1, prophesy_VB_2) (libel_NN_l, practice_of_law_NN_l) (slugger_NN_l, baseball_player_NN_l) (rna_NN_lf chemistry	_1) (metrify_VB_l, versify_VB_l) (trialJ.mpressionJSIN_l, publish_VB_I)	(roll-on_roll-off_NN_lf transport_1) (hizb^it-tahrirJVN_l, asia_NN_l) (slugger_NN_lf s oftb all_gam e_NN_l) (sermon_NN_I, sermonize_VB_1) (drawJVB	drawer_NN_3) (turnyBJ2, PlOWKNJ) (assist_NN_2f softball_game_NN_l) (COilnCiN_2, assembly_NN_4) (throughput_1, turnout_NN_4)	(etymologizing_lf explanation	_1) (ferry—VB_3, travel_1) (public_prosecutor_NN_I, prosecute_VB_2) (alchemize_VB_l, modify_VB_3) (HbeLNNlt libel_VB_l) (turn^BJ2, ∕zZZjvδJ) (hitJ^NJ, hit VBJ) (fire _VB _3, Jlaming小N_1) (ring_NN^f, chemical_chain_NN_l)	matte_NN_2) (etymologizing_NN_I, derive_VB_3) (hole_o utJVB_1„ hole_NN_3) (relationship _4„ relation	) (drive_NN_12, badminton	) (etymologizing_NN_I, etymologize_VB_2) (matrix_algebra_NN_I, diagonalizationJSIN_1) (CabinetWOrk_NN_2, woodworking_NN_l) (cabinetworkJVN_2, bottom_VB_l)
(turn JVB_12„ plowman
(Iibidinal_energy_NN_1, charge_NN_9)
(cream_VB_l> cream_NN_2)
(cabinetworkJVN_2, UpholsterJVB_I)

UnderreVieW as a ConferenCe PaPersICLR 2020
Table 12: "Othel∙” facts as predicted by TuckER.
Relation (Type)	b（0（es，/，e。））上。-4	σ(φ(es,r, eo)) ≈ 0.6	σ(φ(es,r, eo)) ≈ 0.8	七 1
derivationally_ related_form (R)	(tymp anist_NN_l, gong_NN_2) (indicationJSlN_1, signalize_VB_2) (turn_over_VB_3, rotationJSlN_3) (date_VB_5, geological_dating_NN_l) (set_VB_23, emblem_NN_2) (tyro_NN_l, start_VB_5) (identification JSlN_1, name_VB_5) (stabber_NN_l, thrust_VB_5) (justification JSlN_1, apology_NN_2) (manufactureprevarication_NN_l)	(mash_NN_2, mill_VB_2) (walk_VB_9, ^j,mmerJ^rameJSlN_1) (use_VB_5, utility_NN_2) (music al instrumentJSlN _1, write_VB_6) '(lining_NN_3, yvrap_up_VB_l) (scrap_VB_2, struggle_NN_2) (tape_VB_3, tape_recorderJSlN_1) (vindicate _Vj Ustificatio n_NN_2) (IeachingJSlN_1, percolate_VB_3) (synchronize_VBsynchroscopeJSlN_1)	(take_chances_VB_1, venture_NN_l) (shutter_NN_l, fill_up_VB_3) (exit_NN_3, leave JVB_1) (tremblerJSlN_1, vibrate_VB_l) (mo tivatorJSIN_1, tripJVB_4) (SiIPPOrt_ VB_6, indorsementJSlN_1) (federateconfederation JSlN_1) (take_over_VB_6, return_NN_7) (yvait_on_VB_l, sUpporterJSlN_3) (denote_VB_3, promulgation JSlN _1)	(venturerJSlN^2, venturerJSlN) (dynamitist_NN_l, dynamitist_NN_l) (IoveJVBJ, lover_NN_2) (snuggle_NN_1, squeeze_VB_8) (departed mJ, die_VB_2) (position_VB_l, placementJSlN _1) (repentant_JJ_l, rep ent ant J_1) (tread_NN_l, step_VB_7) (stockist_NN_l, stockist_NN_l) (philan thropist_NN_l, philanthropist_NN_l)
instance_ hypernym (S) 一	(deep_south_NN_l, south_NN_l) (st. _j)aul_NN_2, organist_NN_l) (Helsinki JSlN_1, urban_center_NN_l) (malcolm_x_NN_l, emancipationist JSlN_1) (thomas_the_apostle_NN_l, ChUrdl_fa ther_NN」) (st. _gregory_of_n. _NN_1, SermonizerJSlN_1) (robert_owen_NN_l, movie_maker_NN_l) (theresa_NN_l, monk_NN_l) (st. _j)aul_NN_2, philosopher_1) (ibn-roshd_NN_l, pedagogue JSIN_1)	(thomas_aquinas_NN_l, bishop_NN_l) (irish_capital_NN_l, urban_center_NN_l) (thomas_the_apostle_NN_l, apostle_NN_2) (st. _p aul_NN_2, apostle_NN_3) (mccormick_NN_l, painter JSlN_1) (thomas_the_apostle_NN_l, troglodyteJSlN_1) (mccormick_NN_l, electric al _engineerJSlN_1) (mississippi_river_NN_l, american_state_NN_l)	(COWPenSJN_1, siege_NN_l) (mccormick_NN_l, armsJnanufacturer JSlN_1) (tho mas_ the_apostle_NN_l, evangelist JSlN) (mccormickJSIN_1, technologist JSlN_1) (st._gregory_i_NN_l, church_Jather_NN_l)	(r._e. _byrd_NN_l, siege_NN_l) (ShaWJ^N_3, wo men's.rightistqiN_1) (aegean_sea_NN_l, aegean_island_NN_l) (thomas_aquinas_NN_l, churchJ^ather_NN_l)
synset_domain_ topic_of (C)	(ro ll~on_ro U- off JSlN_1, motorcar JSlN_1) (libel_NN_l, legislature_NN_1) (roll-on_roIl-off JSlN_1, passenger_vehicIe_NN_1)	(drive JSlN_12, badmintonJSlN_1)		
LA
UnderreVieW as a ConferenCe PaPersICLR 2020
Table 13: "Othel∙” facts as predicted by MuRE.
	Relation (Type)	σ(≠(es,r,eo)) ≈ 0.4	σ(≠(es,r,eo)) ≈ 0.6	σ(≠(es,r,eo)) ≈ 0.8	σ(φ(es,r,eo)) ≈ 1
	derivationally_ related_form (R)	(surround_VB_1, wall_NN_l) (u npleasantJ[JJ[, unpalatableness JSlNJL) (love_VB_3, enjoyment_NNJ2) (magn itude_NN_l,	) (testify_VB_2, information J^N_1) (connect_VB_6, COnVergingJ^NJI) (connect_VB_6, connexionJSlN_4) (operate _VB _4, psyop_NN_l) (market_VB_l, trade_NN_4) (operate JVB_4, mission_NN_2)	(word_p icture_NN_l, sketch JVB (develop JVB JlO, adjustment_4) (gloss_VB_3, commentary_NN_l) (violate_VB_2, violation_NN_3) (suffocate_VB_l, strangler_tree_NN_l) (number^VB pointJVN_12) (develop _VB_10, organic process _NN_1) (plication_NN_l, twist_VB_4) (split_up_VB_3, separation JSlN) (plication_NN_l, wrinkle_VB_2)	(smeUingJ^N_l, wind_VB_4) (try_out_VB_l, somatic_cell_nuclear_transplantation_NN_l) (lighting_NN_4, set_pn JireJVBJL) (.Symphalangus_NN_1, one-half_NN_1) (justvalidity_NN_1) (reprove_VB_l, talking_to_NN_l) (sustainJVB_5, beamJVN_2) '(spring_NN_6, hurdle_VB_l) (spark_NN_l, scintillate_VB_1) (utility _NNJ2, functional_JJ_1)	(spoliation J^N, sack JVB Jl) (desire_NN_2, hopeJ2) (Stll用Ie”_3, whine_NN_l) (nasalization _NN_1, sou nd_p ut_VB_l) (tread_NN_l, step_VB_7) (yearn_VB_l, piningJVN_1) (unreliableness_NN_l, arbitrary_JJ_1) (travesty_NN_2, travesty) (spark_NN_l, sparkle_VB_1) (stockist_NN_l, stockist_NN_l)
S	instance_ hypernym (S) ^	(malcolm_x_NN_1, hipster_NN_1) (the_nazarene_NN_l, judaism_NN_2.) (old_line_stateJ\lN_l, river_NN_l) (r._e._byrdJ^N_1, commissioned_officer_NN_l) (south_kore a_NN_l, peninsula_NN_l) (st._gregory_pf_n. _NN_1, vicar_of_christ_NN_l) (nippon_NN_l, italian_region_NN_l) (robert_owen_NN_1, tycoon_NN_l) (mandalay_NN_1, national_capital_NN_1) (nan_ling_NN_l, urban_center_NN_l)	(central_americaJ^N_1, central_america_NN_1) (st. _gregory_i_1, ch urch_fatherJ\lN_l) (sOUthJcorea_NN_l, african_nation_NN_1) (malcolm_x J^N_1, passive_resister_NN_1) (malcolm_x J^N_1, birth-control_reformer_NN_1) (los_angeles_NN_l, port_NN_l) (great_lake,s_NN_l, Canadian province _NN_1) (transylvanian_alps_NN_1, urban_center_NN_l) (gettysburg_NN_2, siegeJVN_l) (wisconsinJVN_2, geographical_region_NN_l)	(theresa_NN_l, monk_NN_l) (nippon_NN_1, european_nation_NN_l) (great lakes NN 1, river NN 1) (r._e._byrd_NN_l, noncommissioned_officer_NN_l) (world_warJJ^N_l, pitched_battle_NN_l) (irish_capitalJVN_1, urban_centerJVN_l) (volcano_islands_NN_l, urban_center_NN_l) (nippon_NN_l, american_state_NN_1) (Helsinkiurban_centerJ^N_l) (cap ital_pf_gamb ia_NN_l, urban_centerJVN_l)	
	synset_domain_ topic_of (C)	(libel_NN_l, criminalJlaw_NN_1) (brynh ild_NN_l, mythology J^N_2) (slugger J^N_1, sport_NN_l) (seliyBJ, law_NN_l) (sem itic_deity_NN_l, mythology_NN_1) (nuclear_deterrence_NN_l, law_NN_l) (recep tion_NN_5, baseball_game_NN_l) (photosynthesisJSlN_1, chemistry_NN_l) (isolde_NN_l, parable_NN_l) (assistJVN_2, court_game_NN_l)	(write_VB_8, transcriptionJSlN) (temple_NN_4, mu.slifnism_NN_2) (assist_NNJ2, hockey_NN_1) (reIationship_NN_4, biology_NN_1) (apostle _NN_3, we stern _church _NN_1) (assist_NNJ2, SPOrtJSlND (trot_VB_2, equestrian_sport_NN_l) (maJVN_l, chemistry_NN_1) (assist_NNJ2, soccer_NN_l) (assist NNJ2, football game NN l)	(assist_NN_2, am. Joo tb all_game_NN_l) (drive_NN_12, co urt_game_NN_l) (sell_VB_l, offense_NN_3) (slugger_NN_l, softball_game J^N_1) (drive_NN_12, badminton J\!N_1)	