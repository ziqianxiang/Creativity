Under review as a conference paper at ICLR 2020
MIM: Mutual Information Machine
Anonymous authors
Paper under double-blind review
Ab stract
We introduce the Mutual Information Machine (MIM), an autoencoder framework
for learning joint distributions over observations and latent states. The model for-
mulation reflects two key design principles: 1) symmetry, to encourage the encoder
and decoder to learn different factorizations of the same underlying distribution;
and 2) mutual information, to encourage the learning of useful representations
for downstream tasks. The objective comprises the Jensen-Shannon divergence
between the encoding and decoding joint distributions, plus a mutual information
regularizer. We show that this can be bounded by a tractable cross-entropy loss
between the true model and a parameterized approximation, and relate this to max-
imum likelihood estimation and variational autoencoders. Experiments show that
MIM is capable of learning a latent representation with high mutual information,
and good unsupervised clustering, while providing NLL comparable to VAE (with
a sufficiently expressive architecture).
1	Introduction
Mutual information is a natural indicator of the quality of a learned representation (Hjelm et al.,
2019), along with other characteristics, such as the compositionality of latent factors that are expected
to be useful in downstream tasks, like transfer learning (Bengio et al., 2017). Mutual information is,
however, computationally difficult to estimate for continuous high-dimensional random variables. As
such, it can be hard to optimize when learning latent variable models (Belghazi et al., 2018; Chen
et al., 2016a).
This paper formulates a new class of probabilistic autoencoder model that is motivated by two
key design principles, namely, the maximization of mutual information, and the symmetry of the
encoder-decoder components. Symmetry captures our desire for both the encoder and decoder to
effectively and consistently model the underlying observation and latent domains. This is particularly
useful for downstream tasks in which either one or both of the encoder and decoder play a central
role. These properties are formulated in terms of the symmetric Jensen-Shannon Divergence between
the encoder and decoder, combined with an objective term to maximize mutual information. We refer
to the resulting model as the mutual information machine, or MIM.
We contrast MIM with models trained using (approximate) maximum likelihood, the canonical
example being the variational auto-encoder, or VAE (Kingma and Welling, 2013; Rezende et al., 2014).
The VAE comprises a probabilistic decoder and an approximate encoder, learned via optimization of
an evidence-based lower bound (ELBO) on the log marginal data distribution. In contrast to MIM it is
asymmetric in its formulation, and while often producing excellent representations, VAEs sometimes
produce pathological results in which the encoder, or approximate posterior, conveys relatively little
information between observations and latent states. This behavior, often referred to as posterior
collapse,results in low mutual information between observations and inferred latent states (Bowman
et al., 2015; Chen et al., 2016b; Razavi et al., 2019; van den Oord et al., 2016; 2017).
We formulate the MIM model, and a learning algorithm that minimizes an upper bound on the desired
loss. The resulting objective can be viewed as a symmetrized form of KL divergence, thereby closely
related to the asymmetric KL objective of the VAE. This also enables direct comparisons to the VAE
in terms of posterior collapse, mutual information, data log likelihood, and clustering. Experiments
show that MIM offers favourable mutual information, better clustering in the latent representation,
and similar reconstruction, at the expense of sampling quality and data log likelihood when compared
to VAE with the same architecture. We also demonstrate that for a sufficiently powerful architecture,
MIM can match sampling quality and log likelihood of a VAE with the same architecture.
1
Under review as a conference paper at ICLR 2020
2	Variational Autoencoders
VAE learning entails optimization of a variational lower bound on the log-marginal likelihood of
the data, log P(x), to estimate the parameters θ of an approximate posterior q®(z|x) over latent
states Z (i.e., the encoder) and a corresponding decoder, p®(x|z) (Kingma and Welling, 2013;
Rezende et al., 2014). A prior over the latent space, P(z), often assumed to be an isotropic Gaussian,
serves as a prior for q®(z|x) in the evidence-lower-bound (ELBO) on the marginal likelihood:
log P(x) ≥ Ez〜qθ(z∣x) [logPθ(x|z)] -DKL (qθ(z|x) IlP(z)). Here, we use the notation P(x)
and P(z ) to emphasize that these priors are given, and that we can draw random samples from them,
but not necessarily evaluate the log-likelihood of samples under them. In what follows we often refer
to them as anchors to further emphasize their role.
With amortized posterior inference, we take expectation over the observation distribution, P(x), to
obtain the VAE objective:
RVAE (θ) = Ex 〜P(X) [ Ez 〜qθ(z∣x) [log Pθ (x∣z)]-Dkl (qθ (z|x) k P (z))]
=Ex 〜P(x),z 〜qθ(z∣x) [log Pθ (x∣z)+log P (z) - log q® (z|x)] ,	(1)
Gradients of Eqn. (1) are estimated through MC sampling from q®(z|x) with reparameterization,
yielding unbiased low-variance gradient estimates (Kingma and Welling, 2013; Rezende et al., 2014).
VAEs are normally thought of as maximizing a lower bound on the data log-likelihood, however it
can also be expressed as minimizing the divergence between two joint distributions over x and z . To
see this, we first subtract log P(x) from (1), which does not change the gradients of the objective
with respect to θ. We then negate the result, as we will be performing minimization. This yields
LVAE (θ) = -RVAE (θ) + Ex〜P(x) [logP(x)] = DKL (q®(z∣x)P(x) kPθ(x∣z)P(Z)) . (2)
The VAE optimization is therefore equivalent to minimizing the KL divergence between an encoding
distribution q®(Z|x)P(x) and a decoding distribution p®(x|Z)P(Z).
3	S ymmetry and Mutual Information
Our goal is to find a consistent encoder-decoder pair, representing a joint distribution over the
observation and latent domains, with high mutual information between observations and latent
states. By consistent, we mean that the encoding and decoding distributions q®(Z|x) P(x) and
p®(x|Z) P(Z), define the same joint distribution. Effectively, we aim to estimate an undirected
graphical model with two valid factorizations. We note that consistency is achievable in the VAE
when the approximate posterior q®(Z|x) is capable of representing the posterior under the decoding
distributionp®(Z|x). In the general case, however, consistency is not usually achieved.
In contrast to the asymmetric divergence between encoding and decoding distributions in the VAE
objective (2), here we consider a symmetric measure, namely, the well-known Jensen-Shannon
divergence (JSD),
JSD(θ) = 2 (Dkl (pθ(x|z) P(Z) k MS) + DKL (q®(z|x) P(x) k MS) ) ,	(3)
where MS is an equally weighted mixture of the encoding and decoding distributions; i.e.,
MS = 2 (p®(x|z) P(z) + q®(z|x) P(x)).	(4)
In addition to encoder-decoder consistency, to learn useful latent representations we also want high
mutual information between x and Z. Indeed, the link between mutual information and representation
learning has been explored in recent work (Hjelm et al., 2019; Belghazi et al., 2018; Chen et al.,
2016a). Here, to emphasize high mutual information, we add a particular regularizer of the form
Rh(Θ) = 1 (H(p®(x|z) P(z)) + H(q®(z|x) P(x))) .	(5)
This is the average of the joint entropy over x and Z according to the encoding and decoding
distributions. This is related to mutual information by the identity H(x, Z) = H(x) + H(Z) -
I(x; Z). That is, minimizing joint entropy encourages the minimization of the marginal entropy and
maximization of the mutual information. In addition to encouraging high mutual information, one
can show that this particular regularizer has a deep connection to JSD and the entropy of MS, i.e.,
JSD(θ) + RH(θ) = H(MS) .	(6)
The derivation for Eqn. (6) is given in Appendix A.1.
2
Under review as a conference paper at ICLR 2020
4	Mutual Information Machine
Section 3 formulates a loss function (6) that reflects our desire for model symmetry and high mutual
information. This objective is difficult to optimize directly since we do not know how to evaluate
logP(x) in the general case (i.e., we do not have an exact closed-form expression for P(x)). As a
consequence, we introduce parameterized approximate priors, qθ(x) andpθ(z), to derive tractable
bounds on the penalized Jensen-Shannon divergence. This is similar in spirit to VAEs, which
introduce a parameterized approximate posterior. These parameterized priors, together with the
conditional encoder and decoder, qθ(z|x) and pθ(x|z), comprise a new pair ofjoint distributions,
qθ(x, Z) ≡ qθ (z|x) qθ (x), andpθ(x, Z) ≡ pθ (x|z)pθ (z).
These new joint distributions allow us to formulate a new, tractable loss that bounds H(MS); i.e.,
LCE(θ) ≡ H(MS, Mθ) = DKL (MS k Mθ) + H(MS) ≥ H(MS) ,
(7)
where Mθ = 1 (pe (x, Z) + qθ (x, Z)), and H(MS, Mθ) denotes the cross-entropy between MS
and Mθ . We refer to LCE as the cross-entropy loss. It aims to match the model prior distributions
to the anchors, while also minimizing H(MS). A key advantage of this formulation is that the
cross-entropy loss can be trained by Monte Carlo sampling from the anchor distributions with
reparameterization (Kingma and Welling, 2013; Rezende et al., 2014).
At this stage it might seem odd to introduce a parametric prior for P(Z). Indeed, setting it directly is
certainly an option. Nevertheless, in order to achieve consistency between pθ (x, Z) and qθ (x, Z) it
can be advantageous to allowpθ(Z) to vary. Essentially, we trade-off latent prior fidelity for increased
model consistency. We provide more insights about this in Appendix E.3.
One issue with LCE is that, while it will try to enforce consistency between the model and the
anchored distributions, i.e., pθ(x, Z) ≈ pθ(XIz)P(Z) and qθ(x, Z) ≈ q®(z∣x)P(x), it will not
directly try to achieve model consistency: pθ (x, Z) ≈ qθ(x, Z). To remedy this, we bound LCE using
Jensen’s inequality, i.e.,
Lmim (θ) ≡ ； (H(MS ,qθ(x, z)) + H(MS ,Pθ(x, z)) ) ≥ Lce(Θ).
(8)
Equation (8) gives us the loss function for the Mutual Information Machine (MIM). It is an average
of cross entropy terms between the mixture distribution MS and the model encoding and decoding
distributions respectively. To see that this encourages model consistency, it can be shown that LMIM
is equivalent to LCE plus a non-negative model consistency regularizer; i.e.,
LMIM(θ) = LCE(θ) + RMIM (θ) .
(9)
The non-negativity of RMIM is a simple consequence of LMIM(θ) ≥ LCE(θ) in (8). One can further
show (see Appendix A.2) that RMIM (θ) satisfies
RMIM(θ) = 2 (DKL (Ms k Pθ (x, z))+ DKL (MS k qθ (x, z)) ) -DKL (MS kMθ) .	(10)
One can conclude from (10) that the regularizer RMIM is zero only when the two joint model distri-
butions, qθ (x, Z) and pθ (x, Z), are identical under fair samples from the joint sample distribution
MS (x, Z). In practice we find that encouraging model consistency also helps stabilize learning.
To understand the MIM objective in greater depth, we find it helpful to express LMIM as a sum of
fundamental terms that provide some intuition for its expected behavior. In particular, as derived in
the Appendix A.3,
LMIM(θ) = Rh(Θ) + 1 (DKL (P(Z) k Pθ(Z)) + DKL (P(x) k qθ(x)))
+ 1 (DKL (qθ(Z∣x) P(x) kPθ(x, z)) + DKL (pθ(x|z) P(z) k qe(z, x)))	(11)
The first term in Eqn. (11) encourages high mutual information between observations and latent states.
The second shows that MIM directly encourages the model priors to match the anchor distributions.
Indeed, the KL term between the data anchor and the model prior is the maximum likelihood objective.
The third term encourages consistency between the model distributions and the anchored distributions,
in effect fitting the model decoder to samples drawn from the anchored encoder (cf. VAE), and,
via symmetry, fitting the model encoder to samples drawn from the anchored decoder (both with
3
Under review as a conference paper at ICLR 2020
reparameterization). As such, MIM can be seen as simultaneously training and distilling a model
distribution over the data into a latent variable model. The idea of distilling density models has been
used in other domains, e.g., for parallelizing auto-regressive models (Oord et al., 2017).
In summary, the MIM loss provides an upper bound on the joint entropy of the observation and latent
states under the mixture distribution MS: LMIM(θ) ≥ HMS (x) + HMS (z) - IMS (x; z). Through
the MIM loss and the introduction of the parameterized model distribution Mθ, we are pushing down
on the entropy of the anchored mixture distribution MS , which is the sum of marginal entropies
minus the mutual information. Minimizing the MIM bound yields consistency of the model encoder
and decoder, and high mutual information under MS between observations and latent states.
5	Experiments
In what follows we examine MIM empirically, with the VAE as a baseline. We consider synthetic
datasets and well-known image datasets, namely MNIST (LeCun et al., 1998), Fashion-MNIST (Xiao
et al., 2017a) and Omniglot (Lake et al., 2015). All models were trained using Adam optimizer
Kingma and Ba (2014) with a learning rate of 10-3, and a mini-batch size of 128. Following Alemi
et al. (2017), we anneal the loss to stabilize the optimization. To this end we linearly increase β from
0 to 1 in the following expression for a number of ’warm-up’ epochs:
1N
LMIM (θ; {xi, Zi}N=ι,β) = - 2Nf log (pθ (xi∣Zi) qθ (z"))+ β (log qθ (Xi) Pθ (Zi)) . (12)
2N i=1
Training continues until the loss (i.e., with β = 1) on a held-out validation set has not improved for
the same number of epochs as the warm-up steps (i.e., defined per experiment). We have found the
number of epochs to convergence of MIM learning to be between 2 to 5 times greater than a VAE
with the same architecture. A complete description of MIM learning is provided in Appendix C.
(Code is available from https://www.dropbox.com/s/idnls2layat77sj/MIM- master.zip?dl=1 ).
5.1	Posterior Collapse in Low Dimensional Data
Before turning to empirical results, it is useful to briefly revisit similarities and differences between
MIM and the canonical VAE formulation. To that end, one can show from Eqn. (1) that the VAE loss
can be expressed in a form that bears similarity to the MIM loss in Eqn. (8). In particular, following
the derivation in Appendix D,
LVAE = 1 ( H(MVAE, qθ(z|x) P(x)) + H(MVAE,Pθ(x|z) P(Z))) - HMVAE (z, x)	(13)
where MVAE(x, z) = q®(z|x) P(x), and the entropy in the last term is taken under the sample
distribution MSVAE. Like the MIM loss, the first term in Eqn. (13) is the average of two cross-entropy
terms, between a sample distribution and the encoding and decoding distributions. Unlike the MIM
loss, these terms are asymmetric as samples are drawn only from the encoding distribution. Further,
the last term in the VAE loss (13) is equivalent to -HMVAE (x) - HMVAE (Z) + IMVAE (x; Z),
and thus it will also encourage a reduction in the mutual information. We posit that this plays a
significant role in the phenomena often referred to as posterior collapse, in which the variance of the
variational posterior grows large and the latent embedding conveys relatively little information about
the observations (e.g., see (Chen et al., 2016b) and others).
To empirically support the expression in Eqn. (13), we consider experiments with synthetic data
comprising 2D observations x ∈ R2, with a 2D latent space, Z ∈ R2 . In 2D one can easily visualize
the model and measure quantitative properties of interest (e.g., mutual information). Fig. 1 depicts
data drawn from a Gaussian mixture model comprising five isotropic components with standard
deviation 0.25 (top), and latents from an isotropic standard Normal (bottom). The encoder and
decoder conditional distributions are Gaussian whose means and variances are regressed from the
input using two fully connected layers and tanh activation. The parameterized data prior, qθ(x), is
defined to be the marginal of the decoding distribution (see Eqn. (23) in Appendix C.1), and the
model priorpθ(Z) is defined to be the anchor, so the only model parameters are those of the encoder
and decoder (see Appendix C for details). We can therefore learn models with the MIM and VAE
objectives, but with the same architecture and parameterization. In this section we used a warm-up
4
Under review as a conference paper at ICLR 2020
VAE
MIM
VAE
MIM
VAE
MIM
Figure 1: VAE and MIM models with 2D inputs, a 2D latent space, and 5, 20 and 500 hidden units.
Top row: Black contours depict level sets of P (x); red dots are reconstructed test points. Bottom
row: Green contours are one standard deviation ellipses of qθ (z|x) for test points. Dashed black
circles depict one standard deviation of P (z). The VAE predictive variance remains high, regardless
of model expressiveness, an indication of various degrees of posterior collapse, while MIM produces
lower predictive variance and lower reconstruction errors, consistent with high mutual information.
(a) MI
(b) NLL
(c) Recon. Error
(d) Classif. (5-NN)
Figure 2: Test performance for MIM (blue) and VAE (red) for the 2D GMM data (cf. Fig. 1), all as
functions of the number of hidden units (on x-axis). Each plot shows the mean and standard deviation
of 10 experiments.
scheduler (Vaswani et al., 2017) with a warm-up of 3 steps, and with each epoch comprising 10000
samples. Training and test sets are drawn independently from the GMM.
Figure 1 depicts learned VAE and MIM models, with 5, 20 and 500 hidden units (from left to right)
to control model expressiveness. Mutual information and root-mean-squared test reconstruction error
are inset in the top row. For the weakest architecture (a), with 5 hidden units, VAE and MIM posterior
variances are similar to the prior (MIM in one dimension), a sign of posterior collapse. As the number
of hidden units increases (b,c), the VAE posterior variance remains large, preferring lower mutual
information while matching the aggregated posterior to the prior. The MIM encoder produces tight
posteriors, and yields higher mutual information and lower reconstruction errors.
To quantify this behavior Fig. 2 shows mutual information, the average negative log-likelihood
(NLL) of test points under the model qθ , the mean reconstruction error of test points, and 5-NN
classification performance1 (predicting which of 5 GMM components each test point was drawn
from), all as functions of the number of hidden units. Following Belghazi et al. (2018), we estimate
mutual information using the KSG estimator (Kraskov et al., 2004; Gao et al., 2016), based on 5-NN
neighborhoods. The auxiliary classification task provides a proxy for representation quality. Mutual
information, reconstruction RMSE, and classification accuracy for test data under the MIM model
are better than with VAE models, at the expense of poorer data log likelihoods for MIM. In this case
MIM learning finds near-invertible mappings with a sufficiently powerful architecture.
To further investigate MIM learning in low dimensional data, we project 784D images from Fashion-
MNIST (Xiao et al., 2017b) onto a 20D linear subspace using PCA (capturing 78.5% of total variance).
The training and validation sets had 50,000 and 10,000 images respectively. Fig. 3 summarizes the
results, with MIM producing high mutual information and classification accuracy, at all but very low
1We experimented with 1-NN,3-NN,5-NN,10-NN and found the results to be consistent.
5
Under review as a conference paper at ICLR 2020
(a) MI
(b) NLL
Figure 3: MIM (blue) and VAE (red) for 20D Fashion-MNIST, with latent dimension between 2 and
20. Plots depict mean and standard deviation of 10 experiments.
12	16	20
(c) Recon. Error
(d) Classif. (5-NN)
Table 1: High dimensional image data. Quantitative results based on 10 trials per condition.
Dataset	COnVHVAE (S)		ConvHVAE (VP)	
	MIM	VAE	MIM	VAE
FaShiOnMNIST	一 272.14 ± 0.64	225.40 ± 0.05	227.61 ± 0.34	224.77 ± 0.04
MNIST	126.85 ± 0.56	80.50 ± 0.05	82.73 ± 0.08	79.66 ± 0.06
Omniglot	141.81 ± 0.32	97.94 ± 0.29	104.10 ± 2.17	97.52 ± 0.16
	PiXeIHVAE (S)		PixelHVAE (VP)	[	
	A-MIM	VAE	A-MIM	VAE
FaShiOnMNIST	一 243.95 ± 0.47	224.65 ± 0.07	一 224.94 ± 0.34	224.02 ± 0.08
MNIST	114.96 ± 0.35	79.04 ± 0.05	79.04 ± 0.08	78.60 ± 0.04
Omniglot	126.12 ± 0.38	91.06 ± 0.14	91.82 ± 0.20	90.74 ± 0.15
(a) Test NLL (in nats). With a more powerful prior, MIM and VAE yield comparable results.
Dataset	ConvHVAE (S)		ConvHVAE (VP)		PixelHVAE (S)		PixelHVAE (VP)	
	MIM	VAE	MIM	VAE	A-MIM	VAE	A-MIM	VAE
FaShiOnMNIST	0.83	0.76	0.81	078^一	0.71	0.76	0.79	0.77^一
MNIST	0.97	0.92	0.97	0.92	0.95	0.86	0.96	0.81
(b) Test accuracy of 5-NN classifier. Standard deviations are well less than 0.1, and omitted from the table.
latent dimensions. MIM and VAE yield similar test reconstruction errors, with VAE having better
negative log likelihoods for test data.
We conclude that the VAE is prone to posterior collapse for a wide range of models’ expressiveness
and latent dimensionality, with latent embeddings exhibiting low mutual information. In contrast,
MIM was empirically robust to posterior collapse. In this regard, we note that several papers have
described ways to mitigate posterior collapse in VAE learning, e.g., by lower bounding, or annealing
the KL divergence term in the VAE objective (Alemi et al., 2017; Razavi et al., 2019), or by limiting
the expressiveness of the decoder (e.g., Chen et al. (2016b)). We posit that MIM does not suffer
from this problem as a consequence of the objective design principles that encourage high mutual
information between observations and the latent representation.
5.2	MIM Learning in High Dimensional Image Data
We next consider MIM and VAE learning with image data (Fashion-MNIST, MNIST, Omniglot).
Unfortunately, with high dimensional data we cannot reliably compute mutual information (Belghazi
et al., 2018). Instead, for model assessment we focus on log-likelihood, reconstruction, and the quality
of random samples. In doing so we also explore multiple architectures, including the top performing
models from Tomczak and Welling (2017), namely, convHVAE (L = 2) and PixelHVAE (L = 2), with
Standard (S) priors2, and VampPrior (VP) priors3. The VP pseudo-inputs are initialized with training
data samples. All the experiments below use the same experimental setup as in Tomczak and Welling
(2017), and the same latent dimensionality z ∈ R80 . Here we also demonstrate that a powerful prior
(e.g., PixelHVAE (VP)) allows MIM to learn models with competitive NLL performance.
2pθ(Z) = N(z; μ = 0, σ = I), a standard Normal distribution, where I is the identity matrix.
3pθ (Z) = -K PK=I qe (Z |uk), a mixture model of the encoder conditioned on optimized pseudo-inputs Uk.
6
Under review as a conference paper at ICLR 2020

EE[j
lil∏0 ππ□
VAE	A-MIM
Figure 4: MIM and VAE learning with the PixelHVAE (VP) architecture, applied to Fashion-MNIST,
MNIST, and Omniglot (left to right). The top three rows (from top to bottom) are test data samples,
VAE reconstruction, and A-MIM reconstruction. Bottom: random samples from VAE and A-MIM.

Sampling from an auto-regressive decoder (e.g., PixelHVAE) is very slow. During training, we
find that we can also learn effectively with a sampling distribution comprising just the encoding
distribution, i.e., P(x) qθ(z|x), rather than the mixture of the encoder and decoder. We refer to this
particular MIM variant as asymmetric MIM (or A-MIM). (For details, see Sec. C.3).
We report quantitative results in Table 1a for test negative log-likelihood (NLL). One can see that
VAE models result in better NLL, but with a rather small gap for the more expressive models (i.e.,
PixelHVAE (VP)). We also show qualitative results for the most expressive models (i.e., PixelHVAE).
Fig. 4 depicts reconstruction4 and sampling for Fashion-MNIST, MNIST, and Omniglot, for the top
performing model (PixelHVAE (VP)), with MIM and VAE being comparable. See Appendix F for
additional results.
The poor NLL and hence poor sampling for MIM with a weak prior model can be explained by the
tightly clustered latent representation (e.g., Fig. 1). A more expressive, learnable prior can capture
such clusters more accurately, and as such, also produces good samples (e.g., VampPrior). In other
words, while VAE opts for better NLL and sampling at the expense of lower mutual information,
MIM provides higher mutual information at the expense of the NLL for a weak prior, and comparable
NLL and sampling with more expressive priors.
5.3	Clustering and Classification
Finally, following Hjelm et al. (2019), we consider an auxiliary classification task as a further
measure of the quality of the learned representations. We opted for K-NN classification, being a
non-parametric method which relies on semantic clustering in latent space, without any additional
training. Given representations learned above in Sec. 5.2, a simple 5-NN classifier5 was applied to
test data to predict one of 10 classes for MNIST and Fashion-MNIST.
Table 1b shows that in all but one case, MIM yields more accurate classification results. We attribute
the performance difference to higher mutual information of MIM representations. Figure 5 provides a
qualitative visualization of the latent clustering, for which t-SNE (van der Maaten and Hinton, 2008))
was used to project the latent space down to 2D. One can see that MIM learning tends to cluster
classes in the latent representation more tightly, while VAE clusters are more diffuse and overlapping,
consistent with the results in Table 1b.
6	Related Work
Given the vast literature on generative models, here we only touch on the major bodies of work
related to MIM, beyond the strong connection to VAEs.
As mentioned above, mutual information, together with disentanglement, is considered to be a
cornerstone for useful representations (Belghazi et al., 2018; Hjelm et al., 2019). Normalizing flows
4Test data in the top row of Fig. 4 are binary, while reconstructions depict the probability of each pixel being
1, following Tomczak and Welling (2017)
5We omitted results for k ∈ {1, 3, 10} as we find them similar.
7
Under review as a conference paper at ICLR 2020
VAE
MIM
VAE
A-MIM
convHVAE (VP)
PixelHVAE (VP)
Figure 5: MIM and VAE z embedding for Fashion MNIST (top) and MNIST (botom).
(Rezende and Mohamed, 2015; Dinh et al., 2014; 2016; Kingma and Dhariwal, 2018; Ho et al., 2019)
directly maximizes mutual information by restricting the architecture to be invertible and tractable.
This, however, requires the latent dimension to be the same as the dimension of the observations
(i.e., no bottleneck). As a consequence, normalizing flows are not well suited to learning a concise
representation of high dimensional data (e.g., images). Here, MIM often yields mappings that are
approximately invertible, with high mutual information and low reconstruction errors.
Bornschein et al. (2015) share some of the same design principles as MIM, i.e., symmetry and
encode/decoder consistency. However, their formulation models the joint density in terms of the
geometric mean between the encoder and decoder, for which one must compute an expensive partition
function. Pu et al. (2017) focuses on minimizing symmetric KL, but must use an adversarial learning
procedure, while MIM can be minimized directly.
GANs (Goodfellow et al., 2014), which focus mainly on decoder properties, without a proper
inference model, have been shown to minimize JSD between the data anchor P (x) and the model
generative process qθ (x) (i.e., the marginal of the decoding distribution in MIM terms). In particular,
prior work recognizes the importance of symmetry in learning generative models with reference
to symmetric discriminators on x and z (Donahue et al., 2016; Dumoulin et al., 2017; Bang and
Shim, 2018). In contrast, here we target JSD between the joint encoding and decoding distributions,
together with a regularizer to encourage high mutual information.
7	Conclusions
We introduce a new representation learning framework, named the mutual information machine
(MIM), that defines a generative model which directly targets high mutual information (i.e., between
the observations and the latent representation), and symmetry (i.e., consistency of encoding and
decoding factorizations of the joint distribution). We derive a variational bound that enables the
maximizion of mutual information in the learned representation for high dimensional continuous data,
without the need to directly compute it. We then provide a possible explanation for the phenomena
of posterior collapse, and demonstrate that MIM does not suffer from it. Empirical comparisons
to VAEs show that MIM learning leads to higher mutual information and better clustering (and
classification) in the latent representation, given the same architecture and parametrization. In
addition, we show that MIM can provide reconstruction error similar to a deterministic auto-encoder,
when the dimensionality of the latent representation is equal to that of the observations. Such
behaviour can potentially allow approximate invertibility when the dimensionality differs, with a
stochastic mapping that is defined through consistency and high mutual information.
In future work, we intend to focus on utilizing the high mutual information mapping provided by
MIM, by exploiting the clustered latent representation to further improve the resulting generative
model.
8
Under review as a conference paper at ICLR 2020
References
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy.
An information-theoretic analysis of deep latent-variable models. CoRR, abs/1711.00464, 2017.
Duhyeon Bang and Hyunjung Shim. High quality bidirectional generative adversarial networks.
CoRR, abs/1805.10717, 2018.
Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. MINE:
Mutual information neural estimation. In ICML, 2018.
Emmanuel Bengio, Valentin Thomas, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently
controllable features. CoRR, abs/1703.07718, 2017.
Jorg Bornschein, Samira Shabanian, Asja Fischer, and YoshUa Bengio. Bidirectional helmholtz
machines. CoRR, abs/1506.03877, 2015.
Samuel R. Bowman, LUke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal J6zefowicz, and Samy Bengio.
Generating sentences from a continUoUs space. CoRR, abs/1511.06349, 2015.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable representation learning by information maximizing generative adversarial nets. In
NIPS, 2016a.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. CoRR, abs/1611.02731, 2016b.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components
estimation. arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP.
arXiv:1605.08803, 2016.
Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. CoRR,
abs/1605.09782, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky,
and Aaron Courville. Adversarially learned inference. ICLR, 2017.
Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying fixed k-nearest neighbor infor-
mation estimators. CoRR, abs/1604.03006, 2016. URL http://arxiv.org/abs/1604.
03006.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pages 2672-2680,
2014.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2019.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving
flow-based generative models with variational dequantization and architecture design. CoRR,
abs/1902.00275, 2019. URL http://arxiv.org/abs/1902.00275.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints,
art. arXiv:1412.6980, Dec 2014.
Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions.
In NIPS, 2018.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In ICLR, 2013.
Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improving variational inference with inverse autoregressive flow. In NIPS, 2016.
9
Under review as a conference paper at ICLR 2020
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Phys.
Rev. E, 69:066138, Jun 2004. doi: 10.1103/PhysRevE.69.066138. URL https://link.aps.
org/doi/10.1103/PhysRevE.69.066138.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-8, 2015.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proc. IEEE, 86(11):2278-2324, 1998.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. CoRR, abs/1611.00712, 2016.
Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
and George van den Driessche. Parallel wavenet: Fast high-fidelity speech synthesis. arXiv preprint
arXiv:1711.10433, 2017.
Yuchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, and Lawrence Carin.
Adversarial symmetric variational autoencoder. In Advances in Neural Information Processing
Systems, pages 4330-4339, 2017.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. In ICLR,
2018.
Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with
delta-vaes. CoRR, abs/1901.03416, 2019.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
ICML, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative Models. In ICML, 2014.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Proceedings of the 12th International
Conference on Neural Information Processing Systems, NIPS’99, pages 1057-1063, Cambridge,
MA, USA, 1999. MIT Press.
Jakub M. Tomczak and Max Welling. VAE with a vampprior. CoRR, abs/1705.07120, 2017. URL
http://arxiv.org/abs/1705.07120.
George Tucker, Andriy Mnih, Chris J. Maddison, and Jascha Sohl-Dickstein. REBAR: low-variance,
unbiased gradient estimates for discrete latent variable models. CoRR, abs/1703.07370, 2017.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
CoRR, abs/1601.06759, 2016.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.
CoRR, abs/1711.00937, 2017.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing high-dimensional data using t-sne.
Journal of Machine Learning Research, 9:2579-2605, 2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998-6008, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. CoRR, abs/1708.07747, 2017a. URL http://arxiv.org/abs/
1708.07747.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: A novel image dataset for benchmark-
ing machine learning algorithms, 2017b.
10
Under review as a conference paper at ICLR 2020
Appendix
Table of Contents
A	Derivations for MIM Formulation	11
A.1 Regularized JSD and Entropy .............................................. 12
A.2 MIM Consistency .......................................................... 12
A.3 MIM Loss Decomposition ................................................... 13
B	MIM in terms of Symmetric	KL Divergence	14
C	Learning	15
C.1 MIM Parametric Priors	.................................................. 16
C.2 Gradient Estimation ...................................................... 16
C.3 Training Time ............................................................ 17
D	Posterior Collapse in VAE	18
E	Additional Experiments	18
E.1 Entropy as Mutual Information Regularizer ................................ 19
E.2 Consistency regularizer in LMIM .......................................... 19
E.3 Parameterizing the Priors ................................................ 20
E.4 Effect of Consistency Regularizer on Optimization ........................ 21
F Additional Results	22
F.1 Reconstruction and Samples for MIM and A-MIM ............................. 22
F.2 Latent Embeddings for MIM and A-MIM ...................................... 25
A Derivations for MIM Formulation
(a)
(b)
(c)
Figure 6: MIM learning estimates two factorizations of a joint distribution: (a) encoding; (b) decoding
factorizations. (c) The estimated joint distribution.
In what follows we provide detailed derivations of key elements of the formulation in the paper,
namely, Equations (6), (9), and (11). We also consider the relation between MIM based on the
Jensen-Shannon divergence and the symmetric KL divergence.
11
Under review as a conference paper at ICLR 2020
A.1 Regularized JSD and Entropy
First we develop the relation in Equation (6), between Jensen-Shannon divergence of the encoder and
decoder, the average joint entropy of the encoder and decoder, and the joint entropy of the mixture
distribution MS .
The Jensen-Shannon divergence with respect to the encoding distribution q®(z|x) P(x) and the
decoding distribution p® (x|z) P(Z) is defined as
JSD(θ) = 1 (DKL (Pθ(x|z) P(Z) k Ms) + DKL (qθ(z|x) P(x) k MS))
=1 (H (pθ (x|z) P (z), MS) — H (pθ (x|z) P (Z))
+ H(q®(Z|x) P(x), MS) - H(q®(Z|x) P(x))
Where MS = 1 (p®(x|z) P(z) + q®(z|x) P(x)) is a mixture of the encoding and decoding distri-
butions. Adding on the regularizer Rh(Θ) = 2(H(p®(x|z) P(z)) + H(q®(z|x) P(x))) gives
JSD(θ) + Rh(Θ) = 1 (H(p®(x|z) P(z), Ms) + H(q®(z∣x) P(x), MS))
= H(MS)
A.2 MIM Consistency
Here we discuss in greater detail how the learning algorithm encourages consistency between the
encoder and decoder of a MIM model, beyond the fact that they are fit to the same sample distribution.
To this end we expand on several properties of the model and the optimization procedure.
A.2.1 MIM consistency regularizer
In what follows we derive the form of the MIM consistency regularizer, RMIM (θ), given in Equation
(9). Recall that We define M® = 1 (p®(x, z) + p® (x, z)). We can show that LMIM is equivalent to
LCE plus a regularizer by taking their difference.
RMIM (θ) = LMIM(θ) - LCE(θ)
2(H (Ms ,p® (x, z)) + H (Ms ,p® (x, Z)))- H (MS, M®)
2(Dkl (Ms k p® (x, Z)) + H (Ms ) + DKL (MS k p® (x, Z)) + H (MS))
-DKL(MSkM®)-H(MS)
1(Dkl (Ms k p®(x, z)) + DKL (MS k p®(x, Z)))- DKL (MS k M®)
where RMIM (θ) is non-negative, and is zero only when the encoding and decoding distributions
are consistent (i.e., they represent the same joint distribution). To prove that RMIM (θ) ≥ 0 we now
construct Equation (9) in terms of expectation over a joint distribution, which yields
Rmim(Θ) = 2(H(Ms,p®(x, z)) + H(MS,p®(x, Z)))- H(MS, M®)
=Eχ,z〜MS -2logp®(x, z) - 2logp®(x, Z) + log2(q® (x, z) + p® (x, z))
Ex,z 〜MS
-log，q® (x, Z) ∙ p® (x, Z) + log $(q® (x, Z) + p® (x, Z))
Ex,z 〜MS
≥0
where the inequality follows Jensen’s inequality, and equality holds only when q® (x, Z) =p® (x, Z)
(i.e., encoding and decoding distributions are consistent).
12
Under review as a conference paper at ICLR 2020
A.2.2 Self-Correcting Gradient
One important property of the optimization follows directly from the difference between the gradient
of the upper bound LMIM and the gradient of the cross-entropy loss LCE. By moving the gradient
operator into the expectation using reparametrization, one can express the gradient of LMIM (θ) in
terms of the gradient of log Mθ and the regularization term in Equation (9). That is, with some
manipulation one obtains
∂ (log qθ + log Pθ
∂θ V 2
∂ l	f(lθ + pθ∖	1 (pθ - 1) ∂θqθ + (pθ - 1)靠Pe
∂θ Og	)+ 2	qθ^+Pθ
(14)
which shows that for any data point where a gap qθ > pθ exists, the gradient applied to pθ grows
with the gap, while placing correspondingly less weight on the gradient applied to qθ . The opposite
is true when qθ < pθ . In both case this behaviour encourages consistency between the encoder and
decoder. Empirically, we find that the encoder and decoder become reasonably consistent early in the
optimization process.
A.2.3 Numerical Stability
Instead of optimizing an upper bound LMIM, one might consider a direct optimization of LCE. Earlier
we discussed the importance of the consistency regularizer in LMIM. Here we motivate the use of
LMIM from a numerical perspective point of view. In order to optimize LCE directly, one must convert
log qθ and log pθ to qθ and pθ . Unfortunately, this is has the potential to produce numerical errors,
especially with 32-bit floating-point precision on GPUs. While various tricks can reduce numerical
instability, we find that using the upper bound eliminates the problem while providing the additional
benefits outlined above.
A.2.4 Tractability
As mentioned earlier, there may be several ways to combine the encoder and decoder into a single
probabilistic model. One possibility we considered, as an alternative to Mθ in Equation (7), is
Me = 1 √qe Pe ,	(15)
β
where β = √ √qθ^pθ dx dz is the partition function, similar to Bornschein et al. (2015). One could
then define the objective to be the cross-entropy as above with a regularizer to encourage β to be
close to 1, and hence to encourage consistency between the encoder and decoder. This, however,
requires a good approximation to the partition function. Our choice of Mθ avoids the need for a good
value approximation by using reparameterization, which results in unbiased low-variance gradient,
independent of the accuracy of the approximation of the value.
A.3 MIM Loss Decomposition
Here we show how to break down the LMIM into the set of intuitive components given in Equation
(11). To this end, first note the definition of LMIM:
Lmim (θ) = 1(H(MS ,Pe (x, z)) + H(MS ,Pe (x, Z)))	(16)
We will focus on the first half of Equation (16) for now,
1H (MS ,Pe (x, Z)) = 1 (H(Pe (x|z) P (z),Pe (x, z)) + H(qe (z|x) P (x),Pe (x, Z)))	(17)
It will be more clear to write out the first term of Equation (17), 1H(Pe(x|z) P(z),Pe(x, Z)) in full
11
4H(Pe(x|z) P(z),Pe(x, Z)) = -4 J	Pe(x|z) P(z)log(Pe(x, z))dxdz
=-；/ Pe(x|z) P(Z)log(Pe(x∣Z))dxdZ
-；/ Pe(x|z) P(Z)log(Pe(Z))dxdZ
13
Under review as a conference paper at ICLR 2020
We then add and subtract 1H(P(z)), where
1H (P (Z))
-4J' P(Z)
x,z
log(P(Z))dxdZ
- 4 Lz
Ρθ (x|z) P (Z)Iog(P (z))dxdz
Writing out 4 H (pθ (x|z) P (z),pθ (x, Z)) + 1H (P (Z)) — 1H (P (Z)) and combining terms, We
obtain
4H(pθ(x|z) P(z),Pθ(x, z)) = H(pθ(x|z) P(z)) + DKL (P(Z) k Pθ(Z))	(18)
The second term in Equation (17) can then be rewritten as
1H(qθ(z|x) P(x),Pθ(x, z)) = 1DKL (qθ(z|x) P(x) k Pθ(x, z)) + 1H(qθ(z|x) P(x)) (19)
Combining Equations (18) and (19), we get the interpretable form for Equation 17, i.e.,
2H(MS,Pθ(x, z)) = 4(H(pθ(x|z) P(z)) + H(qθ(z|x) P(x)))
+ 4DKL (P(z) k Pθ(z)) + 4DKL (qθ(z|x) P(x) k Pθ(x, z))
=1Rh(Θ) + 1DKL (P(z) kPθ(z)) + 1DKL (qθ(z|x) P(x) k Pθ(x, Z))
(20)
We can use the same basic steps to derive an analogous expression for H(MS,pθ(x, Z)) in Equation
(16) and combine it with Equation (20) to get the final interpretable form:
Lmim(θ) = Rh(Θ) + "DKL (P(z) k Pθ(z)) + DKL (P(x) k qθ(x)))
+ 4 (DKL (qθ(z|x) P(x) k Pθ(x, z)) + DKL (pθ(x|z) P(z) k qθ(z, x)))
B MiM in terms of Symmetric KL Divergence
As discussed above, the VAE objective can be expressed as minimizing the KL divergence between
the joint anchored encoding and anchored decoding distributions. Below we consider a model
formulation using the symmetric KL divergence (SKL),
SKL(θ) = 2 ( Dkl (pθ(x|z) P(z) k qθ(z|x) P(x)) + DKL (qθ(z|x) P(x) k Pθ(x|z) P(z))),
the second term of which is the VAE objective. The mutual information regularizer RH (θ) given in
Equation (5) can be added to SKL to obtain a cross-entropy objective that looks similar to MiM:
1sKL(θ) + Rh(Θ) = 2 (H(Ms,Pθ(x|z) P(z)) + H(MS, qθ(z|x) P(x)))
When the model priors are equal to the anchors, this regularized SKL and MiM are equivalent. in
general, however, the MiM loss is not a bound on the regularized SKL.
in what follows we explore the relation between SKL and JSD. in Section A.1 we showed that the
Jensen-Shannon divergence can be written as
JSD(θ) = 2(H(pe(x|z) P(z), MS) - H(pθ(XIZ)P(z))
+ H(qθ(z|x) P(x), MS) - H(qθ(z|x) P(x)))
=2 (H(pθ(x|z) P(z), Ms) + H(qθ(z|x) P(x),MS)) - Rh(Θ)
14
Under review as a conference paper at ICLR 2020
Using Jensen’s inequality, we can bound JSD(θ) from above,
JSD(θ) ≤ 4 (H(pθ(x|z) P(Z)) + H(pθ(x|z) P(z), qθ(z|x) P(x))
+ H(qθ(z|x) P(χ)) + H(qθ(z|x) P(x),pθ(x|z) P(Z)) - RH(θ)
(21)
4(H(pθ(x|z) P(Z),qθ(z|x) P(x)) + H(qθ(z|x) P(x),Pθ(x|z) P(z))
+ 2RH(θ)	- RH(θ)
4 (DKL (pθ(x|z) P(z) k qθ(z|x) P(χ)) + DKL (qθ(z|x) P(x) k pθ(x|z) P(Z))
+ 4RH (θ)	- RH (θ)
=1 (DKL (pθ(x|z) P(z) k qθ(z|x) P(χ)) + DKL (qθ(z|x) P(χ) kpθ(x|z) P(Z)))
=1sKL(θ)
From Equation (21), if we add the regularizer RH (θ) and combine terms, we get
1sKL(θ) + Rh(Θ) = 2 (H(Ms, qθ(z|x) P(x)) + H(MS,Pθ(x|z) P(Z)))
Interestingly, we can write this in terms of KL divergence,
1sKL(θ) + Rh(Θ) = 2 (DKL (Ms	k	qθ(z|x) P(x)) + DKL	(MS kPθ(x|z) P(Z))) +	H(MS)
=1 (DKL (Ms	k	qθ(z|x) P(x)) + DKL	(MS k	Pθ(x|z) P(Z)))
+ JSD(θ) + RH(θ)
which gives the exact relation between JSD and SKL.
1sKL(θ) = 1 (DKL (Ms k qθ(z|x) P(x)) + DKL (MS kPθ(x|z) P(Z))) + JSD(θ)
=1 (DKL (MS k qθ(z|x) P(x)) + DKL (MS kPθ(x|z) P(Z)))
+ 1 (DKL (qθ(z|x) P(x) k MS) + Dkl (pθ(x|z) P(Z) k MS))
C Learning
Here we provide a detailed description of MIM learning, with algorithmic pseudo-code. In addition
we offer practical considerations regarding the choice of priors’ parameterization , and gradient
estimation. The empirical upper bound objective is expressed in terms of two cross-entropy terms (cf.
(8)). Given N fair samples, {xi, zi}iN=1 drawn from the anchored (sample) distribution, MS (x, z)
in (4), the empirical loss is
1N
LMIM (θ; {Xi, Zi}N=i) = - 2N	log qθ (Zi∣Xi) qθ (Xi)+lθg Pθ (Xi∣Zi) Pθ (Zi) ,	(22)
where samples from MS comprises equal numbers of points frompθ(x|z) P(z) and qe(z|x) P(x).
Samples from the anchors, P(x) and , P(Z), are treated as external observations; i.e., we assume we
can sample from them but not necessarily evaluate the density of points under the anchor distributions.
Algorithm 1 specifies the corresponding training procedure. The algorithm makes no assumptions on
the form of the parameterized distributions (e.g., discrete, or continuous). In practice, for gradient-
based optimization, we would like an unbiased gradient estimator without the need to accurately
15
Under review as a conference paper at ICLR 2020
approximate the full expectations per se (i.e., in the cross entropy terms). This is particularly important
when dealing with high dimensional data (e.g., images), where it is computationally expensive to
estimate the value of the expectation. We next discuss practical considerations for the continuous
case and the discrete case.
Algorithm 1 MIM learning of parameters θ
Require: Samples from anchors P(x), P(z)
1 2 3 4 5 6 7 8 9	while not converged do Ddec - {xi, Zi 〜Pe(XIZ)P(z)}Nι2 Denc J {Xj, Zj 〜qe(z|x)P(x)}N/2 D J Ddec U Denc #	See definition of LMIM in Eq. (22) LMIM (θ) ≈ LMIM (θ; D) #	Minimize loss ∆θ H -VeLMIM (θ; D) end while
C.1 MIM Parametric Priors
There are several effective ways to parameterize the priors. For the 1D experiments below we model
pθ(z) using linear mixtures of isotropic Gaussians. With complex, high dimensional data one might
also consider more powerful models (e.g., autoregressive, or flow-based priors). Unfortunately, the
use of complex models typically increases the required computational resources, and the training
and inference time. As an alternative we use for image data the vampprior Tomczak and Welling
(2017), which models the latent prior as a mixture of posteriors, i.e., pθ(z) = PK=I qθ(z|x = Uk)
with learnable pseudo-inputs {uk}kK=1. This is effective and allows one to avoid the need for
additional parameters (see Tomczak and Welling (2017) for details on vampprior’s effect over
gradient estimation).
Another useful model with high dimensional data, following Bornschein et al. (2015), is to define
qθ(x) as the marginal of the decoding distribution; i.e.,
qθ (x) =
Ez~Pθ (Z) [Pθ(x|z)] .	(23)
Like the vampprior, this entails no new parameters. It also helps to encourage consistency between
the encoding and decoding distributions. In addition it enables direct empirical comparisons of VAE
learning to MIM learning, because we can then use identical parameterizations and architectures
for both. During learning, when qθ(x) is defined as the marginal (23), we evaluate log qθ (x) with a
single sample and reparameterization. When z is drawn directly from the latent prior:
logqe(χ) = logEZ〜pθ(z) [pθ(x|z)] ≈ logpθ(x|z).
When z is drawn from the encoder, given a sample observation, we use importance sampling:
log qθ(x)
log E…“ax) pθ(X⑶篇占
≈ logPθ(x∣z)+logpe(z) - logqe(z|x)
Algorithm 2 provides algorithm details with the marginal prior.
C.2 Gradient Estimation
Optimization is performed through minibatch stochastic gradient descent. To ensure unbiased gradient
estimates of LMIM we use the reparameterization trick Kingma and Welling (2013); Rezende et al.
(2014) when taking expectation with respect to continuous encoder and decoder distributions, qe(z|x)
and pe(x|z). Reparameterization entails sampling an auxiliary variable e 〜p(e), with known p(e),
followed by a deterministic mapping from sample variates to the target random variable, that is
pe(z) = ge(e) and qe(z|x) = he(e, x) for prior and conditional distributions. In doing so we
assume p(e) is independent of the parameters θ. It then follows that
▽eEz〜qθ(z∣x) [fe(z)] = VeEe_p(e) [fe(he(e, x))] = Ee〜p© Vefe(he(e, x))]
16
Under review as a conference paper at ICLR 2020
Algorithm 2 MIM learning with marginal qθ (x)
Require: Samples from anchors P(x), P(z)
Require: Define q。(x) = Ez〜p®(Z) [pθ(x|z)]
1: while not converged do
2:	# Sample encoding distribution
3：	DenC - {Xi, Zi 〜q。(Z∣X)P (x)}N=ι
4:	# Compute objective, approximate log qθ (x) with 1 sample and importance sampling
5：	log qθ(Xi) ≈ logPθ(xi∣Zi) + logPθ(Zi) — log qθ(zi∣Xi)
6:	LMIM (θ; DenC){——N PN=I (log Pθ (Xi∣Zi) + log Pθ (Zi ))
7：	# Sample decoding distribution
8:	DdeC - {Xi, Zi 〜Pθ (XIz)P(z)}N=ι
9：	# Compute objective, approximate log qθ (x) with 1 sample
10:	log qθ(Xi) ≈ logpθ(XiIZi)
11：	LMIM (θ; DdeC){——2NN Pi=I (log qθ (Zi∣Xi) +2log Pθ (XiIzi) + log Pθ (Zi ))
12:	# Minimize loss
13：	∆θ (X -Vθ 2 (LMIM (θ; DdeC) + LMIM ⑹ DenC))
14: end while
where fθ (Z) is the loss function with parameters θ. It is common to let p() be standard normal,
e 〜N(0,1), and for z∣x to be Gaussian with mean μe(x) and standard deviation q。(x), in which
case Z = qθ(x) e + μe(x),A more generic exact density model can be learned by mapping a known
base distribution (e.g., Gaussian) to a target distribution with normalizing flows Dinh et al. (2014;
2016); Rezende and Mohamed (2015).
In the case of discrete distributions, e.g., with discrete data, reparameterization is not readily applica-
ble. There exist continuous relaxations that permit reparameterization (e.g., Maddison et al. (2016);
Tucker et al. (2017)), but current methods are rather involved in practice, and require adaptation of
the objective function or the optimization process. Here we simply use the REINFORCE algorithm
Sutton et al. (1999) for unbiased gradient estimates, as follows
Vθ Ez 〜qθ(z∣χ) [fθ (z)] = Ez 〜qθ(z∣χ) [Vθ fθ (z) + fθ (z)V。log qθ (z∣x)].
A detailed derivation follows the use of the relation below,
Vθqθ(ZIX) = qθ(ZIX)Vθlogqθ(ZIX)
in order to provide unbiased gradient estimates as follows
vθEz〜qθ(z∣x) [fθ(Z)]
Vθ
fθ(Z) qθ(ZIX) dZ
qθ(ZIX) Vθfθ(Z)dZ +
fθ(Z) Vθqθ(ZIX)dZ
qθ(ZIX) Vθfθ(Z)dZ +
fθ(Z) qθ(ZIX) Vθlogqθ(ZIX)dZ
Ez〜qθ(z∣x) [Vθfθ(z) + fθ(z)Vθlogqθ(z∣x)]
which facilitate the use of samples to approximate the integral.
C.3 Training Time
Training times of MIM models are comparable to training times for VAEs with comparable architec-
tures. One important difference concerns the time required for sampling from the decoder during
training. This is particularly significant for models like auto-regressive decoders (e.g., Kingma et al.
(2016)) for which sampling is very slow. In such cases, we find that we can also learn effectively with
a sampling distribution that only includes samples from the encoding distribution, i.e., P(X) qθ(ZIX),
rather than the mixture. We refer to this particular MIM variant as asymmetric MIM (or A-MIM).
We use it in Sec. 5.2 when working with the PixelHVAE architecture Kingma et al. (2016).
17
Under review as a conference paper at ICLR 2020
D	Posterior Collapse in VAE
Here we discuss a possible root cause for the observed phenomena of posterior collapse, and show
that VAE learning can be viewed as an asymmetric MIM learning with a regularizer that encourages
the appearance of the collapse. We further support that idea in the experiments in Section 5.1. As
discussed earlier, VAE learning entails maximization of a variational lower bound (ELBO) on the
log-marginal likelihood, or equivalently, given Equation (1), the VAE loss in terms of expectation
over a joint distribution:
-Ex〜P(x),z〜qθ(z|x) [logPθ(x∣z)+log P(Z) - log qθ(z|x)] .	(24)
To connect the loss in Equation (24) to MIM, we first add the expectation of log P(x), and scale the
loss by a factor of 2, to obtain
Ex 〜P(x),z 〜qθ (z∣x) -1 (log(pθ (XIz)P (Z)) +log(qθ (z∣x)P (x))) + log P (x) + log qθ (z|x) (25)
where P(x) is the data distribution, which is assumed to be independent of model parameters θ and
to exist almost everywhere (i.e., complementing P(z)). Importantly, because P(x) does not depend
on θ, the gradients of Eqs. (24) and (25) are identical UP to a multiple of 1, so they share the same
stationary points.
Combining IID samples from the data distribution, Xi 〜 P(x), with samples from the corresponding
variational posterior, zi 〜qθ (z∣xi), we obtain a joint sampling distribution; i.e.,
MSVAE(X, z) = P(X) qθ(zIX)
where MSVAE comprises the encoding distribution in MS . With it one can then rewrite the objective
in Equation (25) in terms of the cross-entropy between MSVAE and the parametric encoding and
decoding distributions; i.e.,
2(H(MVAE, Pθ(x|z) P(z)) + H(MVAE, qθ(z|x) P(x))) +
- HMSVAE (X) - HMSVAE (z) + IMSVAE (X; z) .	(26)
The sum of the last three terms in Equation (26) is the negative joint entropy -H(z, X) under the
sample distribution MSVAE .
Equations (1) and (26), the VAE objective and VAE as regularized cross entropy objective respectively,
define equivalent optimization problems, under the assumption that P(x) and samples X 〜P(x)
do not depend on the parameters θ, and that the optimization is gradient-based. Formally, the VAE
objectives (1) and (26) are equivalent up to a scalar multiple of 2 and an additive constant, namely,
HMSVAE (X).
Equation (26) is the average of two cross-entropy objectives ( i.e., between sample distribution MSVAE
and the model decoding and encoding distributions, respectively), along with a joint entropy term
(i.e., last three terms), which can be viewed as a regularizer that encourages a reduction in mutual
information and increased entropy in z and X. We note that Equation (26) is similar to the MIM
objective in Equation (8), but with a different sample distribution, where the priors are defined to
be the anchors, and with an additional regularizer. In other words, Equation (26) suggests that VAE
learning implicitly lowers mutual information. This runs contrary to the goal of learning useful latent
representations, and we posit that it is an underlying root cause for posterior collapse, wherein the
trained model show low mutual information which can be manifested as an encoder which matches
the prior, and thus provides weak information about the latent state (e.g., see (Chen et al., 2016b) and
others). We point the reader to Section E.1 for empirical evidence for the use of a joint entropy as a
mutual information regularizer.
E Additional Experiments
Here we provide additional experiments that further explore the characteristics of MIM learning.
18
Under review as a conference paper at ICLR 2020
VAE+H	MIM-H	VAE+H	MIM-H	VAE+H	MIM-H
(a) h ∈ R5	(b) h ∈ R20	(c) h ∈ R500
Figure 7: Effects of entropy as a mutual information regularizer in 2D x and 2D z synthetic problem.
VAE and MIM models with 2D inputs, a 2D latent space, and 5, 20 and 500 hidden units. Top row:
Black contours depict level sets of P (x); red dots are reconstructed test points. Bottom row: Green
contours are one standard deviation ellipses of qθ(z|x) for test points. Dashed black circles depict
one standard deviation of P(z). Here we added Hqθ (x, z) to VAE loss, and subtracted HMS (x, z)
from MIM loss, in order to demonstrate the effect of entropy on mutual information. Posterior
collapse in VAE is mitigated following the increased mutual information. MIM, on the other hand,
demonstrates a severe posterior collapse as a result of the reduced mutual information (i.e., posterior
matches prior over z almost perfectly). (see inset quantities).
(a) MI
(b) NLL
(c) Recon. Error
(d) Classif. (5-NN)
Figure 8: Effects of entropy as a mutual information regularizer in 2D x and 2D z synthetic problem.
Test performance for modified MIM (blue) and modified VAE (red) for the 2D GMM data with
(cf. Fig. 7), all as functions of the number of hidden units (on x-axis). Each plot shows the mean
and standard deviation of 10 experiments. Adding encoding entropy regularizer to VAE loss leads
to high mutual information (i.e., prevent posterior collapse), low reconstruction error, and better
classification accuracy. Subtracting sample entropy regularizer from MIM loss results in almost zero
mutual information (severe collapse), which leads to poor reconstruction error and classification
accuracy.
E.1 Entropy as Mutual Information Regularizer
Here we examine the use of entropy as a mutual information regularizer. We repeat the experiment in
Section 5.1 with added entropy regularizer. Figure 7 depicts the effects of an added Hqθ (x, z) to
the VAE loss, and a subtracted HMS (x, z) from MIM . The corresponding quantitative values are
presented in Figure 8. Adding the entropy regularizer leads to increased the mutual information, and
subtracting it results in a strong posterior collapse, which in turn is reflected in the reconstruction
quality. While such an experiment does not represent a valid probabilistic model, it supports our
use of entropy as a regularizer (cf. Eq. (5)) for JSD in order to define a consistent model with high
mutual information.
E.2 CONSISTENCY REGULARIZER IN LMIM
Here we explore properties of models for 1D x and z , learned with LMIM and LCE, the difference
being the model consistency regularizer RMIM (θ). All model priors and conditional likelihoods
(qθ(x), qθ(z|x), pθ(z), pθ(x|z)) are parameterized as 10-component Gaussian mixture models, and
19
Under review as a conference paper at ICLR 2020
Figure 9: We explore the influence of consistency regularizer Rθ . CE and MIM indicate the loss,
LCE or LMIM (the regularized objective), respectively. Top row shows anchor P (x) (dashed), prior
qθ(x) (red), and reconstruction distribution Xi 〜P(x) → Zi 〜qθ(z|xi) → Xi 〜pθ(x|zi) (green).
Bottom row mirrors the top row, with anchor P(z) (dotted), priorpθ(z) (blue), and reconstruction
distribution Zi 〜P(z) → Xi 〜pθ(x|zi) → Zi 〜qθ(z|x) (yellow). In (c-d) the reconstruction
is replaced with decoding from anchors Zi 〜P(z) → Xi 〜pθ(x|zi) (green), and encoding
xi 〜P(x) → Zi 〜qθ(z|x) (yellow). In (e-f) the reconstruction is replaced with decoding from
priors Zi 〜pθ(z) → Xi 〜pθ(x|zi) (green), and encoding Xi 〜qθ(x) → zi 〜qθ(z|x) (yellow).
While both models offers similar reconstruction (a-b), and similar consistency w.r.t. the anchors (c-d),
only MIM finds a consistent model (e-f). See text for details.
optimized during training. Means and variances for the conditional distributions were regressed with
2 fully connected layers (h ∈ R10) and a swish activation function Ramachandran et al. (2018).
Top and bottom rows in Fig. 9 depict distributions in observations and latent spaces respectively.
Dashed black curves are anchors, P(X) on top, and P(z) below (GMMs with up to 3 components).
Learned model priors, qθ (X) and pθ (z), are depicted as red (top) and blue (bottom) curves.
Green histograms in Fig. 9(a,b) depict reconstruction distributions, computed by passing fair samples
from P(X) through the encoder to z and then back through the decoder to X. Similarly the yellow
histograms shows samples from P(z) passed through the decoder and then back to the latent space.
For both losses these reconstruction histograms match the anchor priors well. In contrast, only the
priors that were learned with LCE loss approximates the anchor well, while the LMIM priors do not.
To better understand that, we consider two generative procedures: sampling from the anchors, and
sampling from the priors.
Anchor consistency is depicted in Fig. 9(c,d), where Green histograms are marginal distributions
over x from the anchored decoder (i.e., samples from P(z)pθ(x|z) ). Yellow are marginals over Z
from the anchored encoders P (X)qθ (z |X). One can see that both losses results in similar quality of
matching the corresponding opposite anchors.
Priors consistency is depicted in Fig. 9(e,f), where Green histograms are marginal distributions over
x from the model decoder pθ(z)pθ(x|z). Yellow depicts marginals over Z from the model encoder
qθ (X)qθ (z |X). Importantly, with LMIM the encoder and decoder are consistent; i.e., qθ (X) (red curve)
matches the decoder marginal, while pθ(z) (blue) matches the encoder marginal. The model trained
with LCE (i.e., without consistency prior) fails to learn a consistent encoder-decoder pair. We note
that in practice, with expressive enough priors, LMIM will be a tight bound for LCE.
E.3 Parameterizing the Priors
Here we explore the effect of parameterizing the latent and observed priors. A fundamental idea in
MIM is the concept of a single model, Mθ . As such, parameterizing a prior increases the global
expressiveness of the model Mθ. Fig. 10 depicts the utilization of the added expressiveness in order
to increase the consistency between the encoding and decoding model distribution, in addition to the
consistency of Mθ with MS .
20
Under review as a conference paper at ICLR 2020
(a) qθ (x), pθ (z)
(d) q(x), p(z)
Figure 10: MIM prior expressiveness. In this experiment we explore the effect of learning a prior,
where the priors q(x) and p(z) are normal Gaussian distributions. Top row shows anchor P(x)
(dashed), prior qθ(x) (red), and decoding distribution Zi 〜 pθ(z) → Xi 〜 pθ(x|zi) (green).
Bottom row mirrors the top row, with anchor P (z) (dotted), prior pθ (z) (blue), and encoding
distribution Xi 〜qθ(x) → z0 〜qθ(z|x) (yellow). AS can be seen, parameterizing priors affects
all learned distributions, supporting the notion of optimization of a single model Mθ . We point
that (a) demonstrates the best consistency between the priors and corresponding generated samples,
following the additional expressiveness.
E.4 Effect of Consistency Regularizer on Optimization
Here we explore whether a learned model with consistent encoding-decoding distributions (i.e.,
trained with LMIM) also constitutes an optimal solution of a CE objective (i.e., trained with LCE).
Results are depicted in Fig. 11. In order to distinguish between the effects of the optimization from
the consistency regularizer we initialize a MIM model by pre-training it with LCE loss followed by
LMIM training in Fig. 11(i), and vice verse in Fig. 11(ii). (a-b,e-f) All trained models in Fig. 11 exhibit
similarly good reconstruction (green matches dashed black). (c-d,g-h) However, only models that
were trained with LMIM exhibit encoding-decoding consistency (green matches red, yellow matches
blue). While it is clear that the optimization plays an important role (i.e., different initialization leads
to different local optimum), it is also clear that encoding-decoding consistency is not necessarily an
optimum of LCE, as depicted in a non-consistent model (h) which was initialized with a consistent
model (g). Not surprisingly, without the consistency regularizer training with LCE results in better fit
of priors to anchors (f) as it is utilizing the expressiveness of the parametric priors in matching the
sample distribution.
21
Under review as a conference paper at ICLR 2020
Prior Sampling
Reconstruction
(a) LCE	(b) LMIM	(c) LCE	(d) LMIM
(i) LCE =⇒ LMIM
(ii) LMIM =⇒ LCE
Figure 11: Effects of MIM consistency regularizer and optimization on encoding-decoding con-
sistency. (i) and (ii) differ in initialization order. Odd rows: anchor P (x) (dashed), prior
qθ (x) (red). Even rows: anchor P (z) (dotted), prior pθ (z) (blue). (a-b,e-f) Reconstruction
Xi 〜 P(x) → Zi 〜 qθ(z|xi) → Xi 〜 pθ(x|zi) (Xi green, Zi yellow). (c-d,g-h) Prior decod-
ing Zi 〜Pθ(Z) → Xi 〜Pθ(x|zi) (green), and prior encoding Xi 〜qθ(x) → Zi 〜qe(Z∣Xi)
(yellow). See text for details.
F Additional Results
Here we provide additional visualization of various MIM and VAE models which were not included
in the main body of the paper.
F.1 Reconstruction and Samples for MIM and A-MIM
In what follows we show samples and reconstruction for MIM (i.e., with convHVAE architecture),
and A-MIM (i.e., with PixelHVAE architecture). We demonstrate, again, that a powerful enough
encoder allows for generation of samples which are comparable to VAE samples.
22
Under review as a conference paper at ICLR 2020
Standard Prior
VampPrior Prior
(a) VAE (S)
(b) A-MIM (S)
(d) A-MIM (VP)
(c) VAE (VP)
Figure 12: MIM and VAE learning with PixelHVAE for Fashion MNIST. The top three rows (from
top to bottom) are test data samples, VAE reconstruction, A-MIM reconstruction. Bottom: random
samples from VAE and MIM. (c-d) We initialized all pseudo-inputs with training samples, and used
the same random seed for both models. As a result the samples order is similar.
JΠ□MHI∣IΠ□
□ππ□rι□≡n
□HHΠE1□I1IΓ]
□QEΞEUE∣1
ιli i□l ]∏∣i
[JOEEKD
1UHH□□□E1 □t3ΠΠS□y∣
□ΠΠΠΞΓ≡≡ m∣IΠΞQΞ
Standard Prior
VampPrior Prior
(a) VAE (S)
(b) MIM (S)
[iηΞΞΞΞ
□∏E≡nm□ iι∣mn□∣i∣πΞ
[]n∣u∣mu≡π aEΓoi∏≡π
FjIUEOEJL.IiI
EQEEJffiE
ΠΠO□UL1
ΠΠΠ^UD
(C) VAE (VP)
ri MLimmiiin
∏ [1ΞΞE≡ŋI∣ISΓJ
I ΠEE3BEEim
a P1ΠBΠKHSE]
(d) MIM (VP)
Figure 13: MIM and VAE learning with convHVAE for Fashion MNIST. The top three rows (from
top to bottom) are test data samples, VAE reConstruCtion, MIM reConstruCtion. Bottom: random
samples from VAE and MIM.
23
Under review as a conference paper at ICLR 2020
Standard Prior
VamPPrior Prior
ZmBLgg 上工 WMZ22

q / V t u ?。S
7乙/6 4/ 5穴C列。修
q/VKWR。&
ΞΞΞBDDBB
□■QB目ΞEJB
日Q□□目目 Gπ
□E]BΞ0 目 G□
0dΞa6OΞ目
H四SSQΞE3Ξ
目圈褥E9ΞΞS目
踏ħξbbħħe3
E10Qn□□E3D
DQB目ΞQΞ□
目 U□E9BΠO图
BHDQQBQΞ
ΞQQ❷BDBB
□□囱B□□口Q
O■口ΞSΠQQ
口nΞΞπ□E]Q
(a)	VAE (S)
(b)	A-MIM (S)
(c)	VAE (VP)
(d)	A-MIM (VP)
同h"qqħ□窗
Figure 14:	MIM and VAE learning with PixelHVAE for MNIST. ToP three rows are test data samPles,
followed by VAE and A-MIM reconstructions. Bottom: random samPles from VAE and MIM. (c-d)
We initialized all Pseudo-inPuts with training samPles, and used the same random seed for both
models. As a result the samPles order is similar.
Standard Prior	VamPPrior Prior
Zm上gg上工WMZ2g ZmZgg上工皂MZi2g
ΣZZΞΞZΞΞZΞΞΞ ΣZZΞΞZΞΞZZβΞ
~T~T~~~~~~~~7~~T~~~~~~~~
目I≡Ξ同BQ∙□
QbSQHHBH
目QBΞΞ口BB
BBBQ目Ξne
ΞΞSHSSQH
BSΞ'□B'BEB
E目EBB国ΠB
Sfehbsbo
ΞΞDBBB□目
SDBDSΠSΞ
目BDBQS目&
口□ΞBH同BH
日口ξbqħqq
□eo□□qħe]ξ
Q局Q目ΞOΞ"
SQDΞBBBΞ
(a) VAE (S)	(b) MIM (S)	(c) VAE (VP)	(d) MIM (VP)
Figure 15:	MIM and VAE learning with convHVAE for MNIST. ToP three rows are test data samPles,
followed by VAE and MIM reconstructions. Bottom: random samPles from VAE and MIM.
24
Under review as a conference paper at ICLR 2020
QE≡≡ΞDHΠH Ξ9ΞΞHΞDQQ ≡ΞSαQSHI≡ QQΞQΞQΞΞ
□□□Ξ□SΠ□ =EI 自图 E!阳明楼□E3EIES≡ΞI3 EKSΞΞ□□□□
Ξ□HΞ□ΞθΞ SS9QlSΞHε∣H Ξ!□ΞHE1HEI1I3
E3ΞΞ□ΞΞHEI EI 蜷程目喀网图丽 E1ΞQΞI3E3E3Ξ BHΞ□HΞ□□
BIHE3QI3E1ΞH /=EZ 湄 ESEl 旗国 OREl ®禄&同 团 E1HQQO^E1Q
■目国电目 EI 目 H HE9*3Ξ9HΞΞ □E3QΞSIQΞΞ ΞΞ□□Π□□□
BHHElΞΞEI∣a 阳 H 靓QI3ES≡IEI □Q□ΞDΞ≡B E19Ξ≡ΞQΞE1
QQQE39QE≡3 日固剧 ESES 阖 KlEI QGJEIII^!EIC3Q 回画& EI 窃明 ES ㈤
(a) VAE (S)	(b) A-MIM (S)	(C) VAE (VP)	(d) A-MIM (VP)
Figure 16:	MIM and VAE learning with PixelHVAE for Omniglot. Top three rows are test data
samples, followed by VAE and A-MIM reConstruCtions. Bottom: random samples from VAE and
MIM.
fflEIDΞHHElH fil≡B9HEΞΞEi QE1EIQE3HDΞ Ξ^HHQE∣[SH
QHΞE3EI≡ΞQ S3 图困的同国 ES 后 Ξ□B□ΞQQΞ tΞE3ΞQΞI3E3EI
QΞΞΞQ□ΞB1 ΞBBQΞSiQH ΞHΞΞQSE≡IS EJEIEIHEDEIEIH
ESΞΞSΞBΞES HHHQR!ΞHH ^SE3ΞQ□SD □ΞHΞΞ□ΠH
ΞΞHDΞi≡QΞ]羟想现蹄函矗里瞬 ΞΞQΞD≡aH aBBSaHSH
ElElElEiEHB国西得晶阳国第囹国目曲园EI国图E1KIE3国皿阿国囱西13国
a□ΠΞHHi3E! SES≡HHRHQ QHE3HBΞΞa S3ΞSElE8∏aD
HB9E1EI3E1H EiΞHB∣aKH□ EISQβSΞΞΞ DQEKSE≡E≡!SBI
(a) VAE (S)
(b) MIM (S)	(C) VAE (VP)	(d) MIM (VP)
Figure 17:	MIM and VAE learning with ConvHVAE for Omniglot. Top three rows are test data
samples, followed by VAE and MIM reConstruCtions. Bottom: random samples from VAE and MIM.
F.2 Latent Embeddings for MIM and A-MIM
In what follows we show additional t-SNE visualization of unsupervised Clustering in the latent
representation for MIM (i.e., with ConvHVAE arChiteCture), and A-MIM (i.e., with PixelHVAE
arChiteCture).
25
Under review as a conference paper at ICLR 2020
(a) VAE (S)
(b) MIM (S)
(c) VAE (VP)	(d) MIM (VP)
Figure 18: MIM and VAE z embedding for Fashion MNIST with convHVAE architecture.
(b) MIM (S)
(d) MIM (VP)
(a) VAE (S)
(c) VAE (VP)
Figure 19: MIM and VAE z embedding for MNIST with convHVAE architecture.
(d) A-MIM (VP)
(a) VAE (S)
(b) A-MIM (S)
(c) VAE (VP)
Figure 20: A-MIM and VAE z embedding for Fashion MNIST with PixelHVAE architecture.
(a) VAE (S)
(b) A-MIM (S)	(c) VAE (VP)	(d) A-MIM (VP)
Figure 21: A-MIM and VAE z embedding for MNIST with PixelHVAE architecture.
26